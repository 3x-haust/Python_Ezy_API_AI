{"repo_info": {"repo_name": "checkmk", "repo_owner": "Checkmk", "repo_url": "https://github.com/Checkmk/checkmk"}}
{"type": "test_file", "path": "agents/modules/windows/tests/integration/__init__.py", "content": "#!/usr/bin/env python3\n# Copyright (C) 2022 Checkmk GmbH - License: GNU General Public License v2\n# This file is part of Checkmk (https://checkmk.com). It is subject to the terms and\n# conditions defined in the file COPYING, which is part of this source code package.\n"}
{"type": "test_file", "path": "agents/wnx/tests/__init__.py", "content": "#!/usr/bin/env python3\n# Copyright (C) 2023 Checkmk GmbH - License: GNU General Public License v2\n# This file is part of Checkmk (https://checkmk.com). It is subject to the terms and\n# conditions defined in the file COPYING, which is part of this source code package.\n"}
{"type": "test_file", "path": "agents/modules/windows/tests/integration/test_scripts_execution.py", "content": "#!/usr/bin/env python3\n# Copyright (C) 2019 Checkmk GmbH - License: GNU General Public License v2\n# This file is part of Checkmk (https://checkmk.com). It is subject to the terms and\n# conditions defined in the file COPYING, which is part of this source code package.\n\nimport os\nimport subprocess\nfrom pathlib import Path\n\nimport pytest\n\n\ndef run_script(work_python: Path, *, script: Path) -> tuple[int, str, str]:\n    \"\"\"Returns exit code, stdout and stderr\"\"\"\n\n    exe = work_python / \".venv\" / \"Scripts\" / \"python.exe\"\n    # In fact we do not need asserting here, but in integration test is difficult to find\n    # source of error want to know/\n    assert exe.exists()\n    assert script.exists()\n\n    completed_process = subprocess.run(\n        [exe, script],\n        capture_output=True,\n        check=False,\n    )\n    return (\n        completed_process.returncode,\n        completed_process.stdout.decode(\"utf-8\"),\n        completed_process.stderr.decode(\"utf-8\"),\n    )\n\n\n@pytest.mark.parametrize(\n    \"script,expected_code,expected_pipe,expected_err\",\n    [\n        (\n            Path(\"..\\\\..\\\\..\\\\..\\\\..\\\\non-free\\\\cmk-update-agent\\\\cmk_update_agent.py\"),\n            1,\n            \"\",\n            \"Missing config file at .\\\\cmk-update-agent.cfg. Configuration\",\n        ),\n        (\n            Path(\"..\\\\..\\\\..\\\\..\\\\plugins\\\\mk_logwatch.py\"),\n            0,\n            \"<<<logwatch>>>\",\n            \"\",\n        ),\n        (\n            Path(\"..\\\\..\\\\..\\\\..\\\\plugins\\\\mk_jolokia.py\"),\n            0,\n            \"<<<jolokia_info:sep(0)>>>\",\n            \"\",\n        ),\n    ],\n)\ndef test_other_scripts(\n    python_to_test: Path, script: Path, expected_code: int, expected_pipe: str, expected_err: str\n) -> None:\n    pythons = python_to_test\n    for python_name in os.listdir(pythons):\n        # Call the script using deployed python as client does\n        ret, pipe, err = run_script(Path(python_to_test / python_name), script=script)\n\n        # Check results as client does\n        assert ret == expected_code\n        assert pipe.find(expected_pipe) != -1\n        assert err.find(expected_err) != -1\n"}
{"type": "test_file", "path": "agents/modules/windows/tests/integration/conftest.py", "content": "#!/usr/bin/env python3\n# Copyright (C) 2019 Checkmk GmbH - License: GNU General Public License v2\n# This file is part of Checkmk (https://checkmk.com). It is subject to the terms and\n# conditions defined in the file COPYING, which is part of this source code package.\n\nimport os\nimport shutil\nimport tempfile\nfrom pathlib import Path\nfrom subprocess import PIPE, Popen\n\nimport pytest\n\n\ndef pytest_addoption(parser):\n    parser.addoption(\"--expected_version\", action=\"store\", default=\"3.10.4\")\n\n\ntested_pythons = [\"python-3.cab\"]\n# I know this is not a best method to reach artifacts, but in Windows not so many options.\nartifact_location = Path(\"..\\\\..\\\\..\\\\..\\\\..\\\\artefacts\")\n\n\n@pytest.fixture(scope=\"session\", name=\"expected_version\")\ndef fixture_expected_version(pytestconfig):\n    return pytestconfig.getoption(\"expected_version\")\n\n\n# NOTE: unpacked python can work only in predefined folder(pyvenv.cfg  points on it).\n# We must do two things.\n# 1. Check that content of pyvenv.cfg is suitable to be used in agent(points on ProgramData).\n# 2. Change pyvenv.cfg so that we could test our scripts: replace ProgramData with temp\n# Above mentioned things must be done in fixture, while decompression process is quite expensive.\n# Also this give possibility to test literally everything\n@pytest.fixture(scope=\"session\", name=\"regression_data\")\ndef fixture_regression_data(expected_version):\n    return {\n        \"python-3.cab\": b\"\".join(\n            [\n                b\"home = C:\\\\ProgramData\\\\checkmk\\\\agent\\\\modules\\\\python-3\\r\\n\",\n                b\"version_info = \",\n                expected_version.encode(),\n                b\"\\r\\n\",\n                b\"include-system-site-packages = false\\r\\n\",\n            ]\n        ),\n    }\n\n\nclient_module_root = b\"C:\\\\ProgramData\\\\checkmk\\\\agent\\\\modules\\\\python-3\"\n\n\n@pytest.fixture(scope=\"session\", autouse=True, name=\"python_subdir\")\ndef fixture_python_subdir():\n    tmpdir = tempfile.mkdtemp()\n    subdir = os.path.join(tmpdir, \"modules\")\n    os.makedirs(subdir)\n    yield Path(subdir)\n    shutil.rmtree(tmpdir)\n\n\ndef run_proc(command: list[str], *, cwd: Path | None = None) -> None:\n    with Popen(command, stdout=PIPE, stderr=PIPE, cwd=cwd) as process:\n        pipe, err = process.communicate()\n        ret = process.wait()\n    assert ret == 0, (\n        f\"Code {ret}\\n\"\n        + f\"Pipe:\\n{pipe.decode('utf-8') if pipe else ''}\\n\"\n        + f\"Err\\n{err.decode('utf-8') if err else ''}\"\n    )\n\n\n@pytest.fixture(scope=\"session\", autouse=True)\ndef python_to_test(python_subdir, regression_data) -> Path:  # type: ignore[no-untyped-def]\n    \"\"\"This is quite complicated simulator to verify python module and prepare the module for\n    testing. During deployment every step will be validated, not because it is required(this method\n    also contradicts a bit to the TDD philosophy), but to prevent extremely strange errors during\n    testing phase.\n    Tasks:\n    1. Unpacks a python.\n    2. Validates pyvenv.cfg to be suitable.\n    3. Patch pyvenv.h for testing environment\n    4. Calls postinstall.cmd\n    5. Validates result.\n    \"\"\"\n\n    for python in [Path(x) for x in tested_pythons]:\n        python_file = artifact_location / python\n        assert python_file.exists()\n\n        # prepare target dir as client\n        work_dir = python_subdir / python.name\n        os.mkdir(work_dir)\n\n        # deploy python precisely as done by a client\n        run_proc([\"expand.exe\", f\"{python_file}\", \"-F:*\", f\"{work_dir}\"])\n        venv_cfg = work_dir / \".venv\" / \"pyvenv.cfg\"\n\n        # check that pyvenv.cfg is good for deployment.\n        with open(venv_cfg, \"rb\") as f:\n            assert f.read() == regression_data[str(python)]\n\n        # patch pyvenv.cfg to be valid for testing environment:\n        # tests do not use well known path in ProgramData/checkmk/agent\n        with open(venv_cfg, \"wb\") as f:\n            f.write(regression_data[str(python)].replace(client_module_root, bytes(work_dir)))\n\n        # apply postinstall step to prepare the python\n        run_proc([work_dir / \"postinstall.cmd\"], cwd=work_dir)\n        assert (work_dir / \"DLLs\").exists()\n\n    return python_subdir\n"}
{"type": "test_file", "path": "agents/wnx/tests/ap/conftest.py", "content": "#!/usr/bin/env python3\n# Copyright (C) 2019 Checkmk GmbH - License: GNU General Public License v2\n# This file is part of Checkmk (https://checkmk.com). It is subject to the terms and\n# conditions defined in the file COPYING, which is part of this source code package.\n\nimport sys\n\nimport agents.wnx.tests.testlib.utils as testlib\n\nsys.path.insert(0, str(testlib.get_git_root_path()))\n"}
{"type": "test_file", "path": "agents/wnx/tests/integration/__init__.py", "content": "#!/usr/bin/env python3\n# Copyright (C) 2023 Checkmk GmbH - License: GNU General Public License v2\n# This file is part of Checkmk (https://checkmk.com). It is subject to the terms and\n# conditions defined in the file COPYING, which is part of this source code package.\n"}
{"type": "test_file", "path": "agents/wnx/tests/ap/test_mk_logwatch_win.py", "content": "#!/usr/bin/env python3\n# Copyright (C) 2019 Checkmk GmbH - License: GNU General Public License v2\n# This file is part of Checkmk (https://checkmk.com). It is subject to the terms and\n# conditions defined in the file COPYING, which is part of this source code package.\n\n# fmt: off\n\nimport locale\nimport os\nimport re\nimport sys\nfrom collections.abc import Mapping\n\nimport pytest\n\nimport agents.plugins.mk_logwatch as lw\n\ntry:\n    from collections.abc import Sequence\nexcept ImportError:\n    pass  # only needed for type comments\n\n_SEP = os.sep.encode()\n_SEP_U = _SEP.decode(\"utf-8\")\n\n\ndef _oh_no():\n    return \"oh-no-ÐŸ\" if os.name == \"nt\" else b\"oh-no-\\x89\"\n\n\ndef _wat_bad():\n    return \"watÐŸÐ¸Ðº\" if os.name == \"nt\" else b\"wat\\xe2\\x80\\xbd\"\n\n\n# NOTE: Linux could use bytes and str paths, Windows only str\n# we need to have kind of API to provide different types for different OS's\n_OH_NO = _oh_no()\n_WAT_BAD = _wat_bad()\nif os.name == \"nt\":\n    _OH_NO_STR = _OH_NO\n    _WAT_BAD_STR = _WAT_BAD\n    _OH_NO_BYTES = _OH_NO.encode()\n    _WAT_BAD_BYTES = _WAT_BAD.encode()\nelse:\n    _OH_NO_STR = \"oh-no-\\uFFFD\"  # replace char\n    _WAT_BAD_STR = \"wat\\u203D\"  # actual interobang\n    _OH_NO_BYTES = _OH_NO\n    _WAT_BAD_BYTES = _WAT_BAD\n\n_TEST_CONFIG = \"\"\"\n\nGLOBAL OPTIONS\n ignore invalid options\n retention_period 42\n\nnot a cluster line\n\nCLUSTER duck\n 192.168.1.1\n 192.168.1.2\n\nCLUSTER empty\n\n/var/log/messages\n C Fail event detected on md device\n I mdadm.*: Rebuild.*event detected\n W mdadm\\\\[\n W ata.*hard resetting link\n W ata.*soft reset failed (.*FIS failed)\n W device-mapper: thin:.*reached low water mark\n C device-mapper: thin:.*no free space\n C Error: (.*)\n\n/var/log/auth.log\n W sshd.*Corrupted MAC on input\n\n\"c:\\\\a path\\\\with spaces\" \"d:\\\\another path\\\\with spaces\"\n I registered panic notifier\n C panic\n C Oops\n W generic protection rip\n W .*Unrecovered read error - auto reallocate failed\n\n/var/log/Ã¤umlaut.log\n W sshd.*Corrupted MAC on input\n\n/var/log/test_append.log\n C .*Error.*\n A .*more information.*\n A .*also important.*\n\"\"\"\n\n\n@pytest.fixture(name=\"parsed_config\", scope=\"module\")\ndef _get_parsed_config() -> (\n    tuple[lw.GlobalOptions, Sequence[lw.PatternConfigBlock], Sequence[lw.ClusterConfigBlock]]\n):\n    return lw.read_config(_TEST_CONFIG.split(\"\\n\"), files=[], debug=False)\n\n\ndef text_type():\n    if sys.version_info[0] == 2:\n        return unicode  # noqa: F821\n    return str\n\n\ndef binary_type():\n    if sys.version_info[0] == 2:\n        return str\n    return bytes\n\n\ndef ensure_text(s, encoding='utf-8', errors='strict'):\n    if isinstance(s, binary_type()):\n        return s.decode(encoding, errors)\n    if isinstance(s, text_type()):\n        return s\n    raise TypeError(\"not expecting type '%s'\" % type(s))\n\n\ndef ensure_binary(s, encoding='utf-8', errors='strict'):\n    if isinstance(s, text_type()):\n        return s.encode(encoding, errors)\n    if isinstance(s, binary_type()):\n        return s\n    raise TypeError(\"not expecting type '%s'\" % type(s))\n\n\ndef test_options_defaults() -> None:\n    opt = lw.Options()\n    for attribute in (\n        'encoding',\n        'maxfilesize',\n        'maxlines',\n        'maxtime',\n        'maxlinesize',\n        'regex',\n        'overflow',\n        'nocontext',\n        'maxcontextlines',\n        'maxoutputsize',\n        'skipconsecutiveduplicated',\n    ):\n        assert getattr(opt, attribute) == lw.Options.DEFAULTS[attribute]\n\n\n@pytest.mark.parametrize(\n    \"option_string, key, expected_value\",\n    [\n        (\"encoding=utf8\", 'encoding', 'utf8'),\n        (\"maxfilesize=42\", 'maxfilesize', 42),\n        (\"maxlines=23\", 'maxlines', 23),\n        (\"maxlinesize=13\", 'maxlinesize', 13),\n        (\"maxtime=0.25\", 'maxtime', 0.25),\n        (\"overflow=I\", 'overflow', 'I'),\n        (\"nocontext=tRuE\", 'nocontext', True),\n        (\"nocontext=FALse\", 'nocontext', False),\n        (\"maxcontextlines=17,23\", 'maxcontextlines', (17, 23)),\n        (\"fromstart=1\", 'fromstart', True),\n        (\"fromstart=yEs\", 'fromstart', True),\n        (\"fromstart=0\", 'fromstart', False),\n        (\"fromstart=no\", 'fromstart', False),\n        (\"maxoutputsize=1024\", 'maxoutputsize', 1024),\n        (\"skipconsecutiveduplicated=False\", 'skipconsecutiveduplicated', False),\n        (\"skipconsecutiveduplicated=True\", 'skipconsecutiveduplicated', True),\n    ],\n)\ndef test_options_setter(option_string: str, key: str, expected_value: object) -> None:\n    opt = lw.Options()\n    opt.set_opt(option_string)\n    actual_value = getattr(opt, key)\n    assert isinstance(actual_value, type(expected_value))\n    assert actual_value == expected_value\n\n\n@pytest.mark.parametrize(\n    \"option_string, expected_pattern, expected_flags\",\n    [\n        (\"regex=foobar\", 'foobar', re.UNICODE),\n        (\"iregex=foobar\", 'foobar', re.IGNORECASE | re.UNICODE),\n    ],\n)\ndef test_options_setter_regex(\n    option_string: str, expected_pattern: str, expected_flags: int\n) -> None:\n    opt = lw.Options()\n    opt.set_opt(option_string)\n    assert opt.regex.pattern == expected_pattern\n    assert opt.regex.flags == expected_flags\n\n\ndef test_get_config_files(tmpdir: str) -> None:\n    fake_config_dir = os.path.join(str(tmpdir), \"test\")\n    os.mkdir(fake_config_dir)\n\n    logwatch_d_dir = os.path.join(fake_config_dir, \"logwatch.d\")\n    os.mkdir(logwatch_d_dir)\n\n    with open(os.path.join(logwatch_d_dir, \"custom.cfg\"), mode=\"w\"):\n        expected = [\n            str(os.path.join(fake_config_dir, \"logwatch.cfg\")),\n            str(os.path.join(fake_config_dir, \"logwatch.d\", \"custom.cfg\")),\n        ]\n\n    assert lw.get_config_files(str(fake_config_dir)) == expected\n\n\ndef test_raise_no_config_lines() -> None:\n\n    # No config file at all available, raise in debug mode!\n    with pytest.raises(IOError):\n        lw.read_config([], files=[], debug=True)\n\n    # But it's ok without debug\n    lw.read_config([], files=[], debug=False)\n\n\ndef test_read_global_options(\n    parsed_config: tuple[\n        lw.GlobalOptions, Sequence[lw.PatternConfigBlock], Sequence[lw.ClusterConfigBlock]\n    ]\n) -> None:\n    global_options, _logfile_config, _cluster_config = parsed_config\n\n    assert isinstance(global_options, lw.GlobalOptions)\n    assert global_options.retention_period == 42\n\n\ndef test_read_config_cluster(\n    parsed_config: tuple[\n        lw.GlobalOptions, Sequence[lw.PatternConfigBlock], Sequence[lw.ClusterConfigBlock]\n    ]\n) -> None:\n    \"\"\"checks if the agent plugin parses the configuration appropriately.\"\"\"\n    _global_options, _logfile_config, c_config = parsed_config\n\n    assert len(c_config) == 2\n    assert isinstance(c_config[0], lw.ClusterConfigBlock)\n\n    assert c_config[0].name == \"duck\"\n    assert c_config[0].ips_or_subnets == ['192.168.1.1', '192.168.1.2']\n\n    assert c_config[1].name == \"empty\"\n    assert not c_config[1].ips_or_subnets\n\n\ndef test_read_config_logfiles(\n    parsed_config: tuple[\n        lw.GlobalOptions, Sequence[lw.PatternConfigBlock], Sequence[lw.ClusterConfigBlock]\n    ]\n) -> None:\n    \"\"\"checks if the agent plugin parses the configuration appropriately.\"\"\"\n\n    _global_options, l_config, _cluster_config = parsed_config\n\n    assert len(l_config) == 6\n    assert all(isinstance(lf, lw.PatternConfigBlock) for lf in l_config)\n\n    assert l_config[0].files == ['not', 'a', 'cluster', 'line']\n    assert not l_config[0].patterns\n\n    assert l_config[1].files == ['{}'.format(os.path.join(os.sep, \"var\", \"log\", \"messages\"))]\n    assert l_config[1].patterns == [\n        ('C', 'Fail event detected on md device', [], []),\n        ('I', 'mdadm.*: Rebuild.*event detected', [], []),\n        ('W', 'mdadm\\\\[', [], []),\n        ('W', 'ata.*hard resetting link', [], []),\n        ('W', 'ata.*soft reset failed (.*FIS failed)', [], []),\n        ('W', 'device-mapper: thin:.*reached low water mark', [], []),\n        ('C', 'device-mapper: thin:.*no free space', [], []),\n        ('C', 'Error: (.*)', [], []),\n    ]\n\n    assert l_config[2].files == ['{}'.format(os.path.join(os.sep, \"var\", \"log\", \"auth.log\"))]\n    assert l_config[2].patterns == [('W', 'sshd.*Corrupted MAC on input', [], [])]\n\n    assert l_config[3].files == ['c:\\\\a path\\\\with spaces', 'd:\\\\another path\\\\with spaces']\n    assert l_config[3].patterns == [\n        ('I', 'registered panic notifier', [], []),\n        ('C', 'panic', [], []),\n        ('C', 'Oops', [], []),\n        ('W', 'generic protection rip', [], []),\n        ('W', '.*Unrecovered read error - auto reallocate failed', [], []),\n    ]\n\n    assert l_config[4].files == ['{}'.format(os.path.join(_SEP_U, \"var\", \"log\", \"Ã¤umlaut.log\"))]\n    assert l_config[4].patterns == [('W', 'sshd.*Corrupted MAC on input', [], [])]\n\n    assert l_config[5].files == ['{}'.format(os.path.join(os.sep, \"var\", \"log\", \"test_append.log\"))]\n    assert l_config[5].patterns == [\n        ('C', '.*Error.*', ['.*more information.*', '.*also important.*'], [])\n    ]\n\n\n@pytest.mark.parametrize(\n    \"env_var, expected_status_filename\",\n    [\n        (\"192.168.2.1\", os.path.join(\"/path/to/config\", \"logwatch.state.192.168.2.1\")),\n        (\n            \"::ffff:192.168.2.1\",\n            os.path.join(\"/path/to/config\", \"logwatch.state.__ffff_192.168.2.1\"),\n        ),\n        (\"192.168.1.4\", os.path.join(\"/path/to/config\", \"logwatch.state.my_cluster\")),\n        (\n            \"1262:0:0:0:0:B03:1:AF18\",\n            os.path.join(\"/path/to/config\", \"logwatch.state.1262_0_0_0_0_B03_1_AF18\"),\n        ),\n        (\n            \"1762:0:0:0:0:B03:1:AF18\",\n            os.path.join(\"/path/to/config\", \"logwatch.state.another_cluster\"),\n        ),\n        (\"local\", os.path.join(\"/path/to/config\", \"logwatch.state.local\")),\n        (\"::ffff:192.168.1.2\", os.path.join(\"/path/to/config\", \"logwatch.state.my_cluster\")),\n    ],\n)\ndef test_get_status_filename(\n    env_var: str, expected_status_filename: str | bytes, monkeypatch: pytest.MonkeyPatch\n) -> None:\n    monkeypatch.setattr(lw, \"MK_VARDIR\", '/path/to/config')\n    fake_config = [\n        lw.ClusterConfigBlock(\n            \"my_cluster\",\n            ['192.168.1.1', '192.168.1.2', '192.168.1.3', '192.168.1.4'],\n        ),\n        lw.ClusterConfigBlock(\n            \"another_cluster\",\n            ['192.168.1.5', '192.168.1.6', '1762:0:0:0:0:B03:1:AF18'],\n        ),\n    ]\n\n    assert lw.get_status_filename(fake_config, env_var) == expected_status_filename\n\n\n@pytest.mark.parametrize(\n    \"state_data, state_dict\",\n    [\n        (\n            (\n                \"/var/log/messages|7767698|32455445\\n\"  #\n                \"/var/foo|42\\n\"  #\n                \"/var/test/x12134.log|12345\"\n            ),\n            {\n                '/var/log/messages': {\n                    \"file\": \"/var/log/messages\",\n                    \"offset\": 7767698,\n                    \"inode\": 32455445,\n                },\n                '/var/foo': {\n                    \"file\": \"/var/foo\",\n                    \"offset\": 42,\n                    \"inode\": -1,\n                },\n                '/var/test/x12134.log': {\n                    \"file\": \"/var/test/x12134.log\",\n                    \"offset\": 12345,\n                    \"inode\": -1,\n                },\n            },\n        ),\n        (\n            (\n                \"{'file': '/var/log/messages', 'offset': 7767698, 'inode': 32455445}\\n\"\n                \"{'file': '/var/foo', 'offset': 42, 'inode': -1}\\n\"\n                \"{'file': '/var/test/x12134.log', 'offset': 12345, 'inode': -1}\\n\"\n            ),\n            {\n                '/var/log/messages': {\n                    \"file\": \"/var/log/messages\",\n                    \"offset\": 7767698,\n                    \"inode\": 32455445,\n                },\n                '/var/foo': {\n                    \"file\": \"/var/foo\",\n                    \"offset\": 42,\n                    \"inode\": -1,\n                },\n                '/var/test/x12134.log': {\n                    \"file\": \"/var/test/x12134.log\",\n                    \"offset\": 12345,\n                    \"inode\": -1,\n                },\n            },\n        ),\n        (\n            \"{'file': 'I/am/a/byte/\\\\x89', 'offset': 23, 'inode': 42}\\n\",\n            {\n                'I/am/a/byte/\\x89': {\n                    \"file\": \"I/am/a/byte/\\x89\",\n                    \"offset\": 23,\n                    \"inode\": 42,\n                },\n            },\n        ),\n        (\n            \"{'file': u'I/am/unicode\\\\u203d', 'offset': 23, 'inode': 42}\\n\",\n            {\n                'I/am/unicode\\u203d': {\n                    \"file\": \"I/am/unicodeâ€½\",\n                    \"offset\": 23,\n                    \"inode\": 42,\n                },\n            },\n        ),\n    ],\n)\ndef test_state_load(\n    tmpdir: str | bytes, state_data: str, state_dict: Mapping[str, Mapping[str, object]]\n) -> None:\n    # setup for reading\n    file_path = os.path.join(str(tmpdir), \"logwatch.state.testcase\")\n\n    # In case the file is not created yet, read should not raise\n    state = lw.State(file_path).read()\n    assert not state._data\n\n    with open(file_path, \"wb\") as f:\n        f.write(state_data.encode(\"utf-8\"))\n\n    # loading and __getitem__\n    state = lw.State(file_path).read()\n    assert state._data == state_dict\n    for expected_data in state_dict.values():\n        key = expected_data['file']\n        assert isinstance(key, str)\n        assert state.get(key) == expected_data\n\n\n@pytest.mark.parametrize(\n    \"state_dict\",\n    [\n        {\n            '/var/log/messages': {\n                \"file\": \"/var/log/messages\",\n                \"offset\": 7767698,\n                \"inode\": 32455445,\n            },\n            '/var/foo': {\n                \"file\": \"/var/foo\",\n                \"offset\": 42,\n                \"inode\": -1,\n            },\n            '/var/test/x12134.log': {\n                \"file\": \"/var/test/x12134.log\",\n                \"offset\": 12345,\n                \"inode\": -1,\n            },\n        },\n    ],\n)\ndef test_state_write(tmpdir: str | bytes, state_dict: Mapping[str, Mapping[str, object]]) -> None:\n    # setup for writing\n    file_path = os.path.join(str(tmpdir), \"logwatch.state.testcase\")\n    state = lw.State(file_path)\n    assert not state._data\n\n    # writing\n    for data in state_dict.values():\n        key = data['file']\n        assert isinstance(key, str)\n        filestate = state.get(key)\n        # should work w/o setting 'file'\n        filestate['offset'] = data['offset']\n        filestate['inode'] = data['inode']\n    state.write()\n\n    read_state = lw.State(file_path).read()\n    assert read_state._data == state_dict\n\n\nSTAR_FILES = [\n    (b\"file.log\", \"file.log\"),\n    (b\"hard_link_to_file.log\", \"hard_link_to_file.log\"),\n    (b\"hard_linked_file.log\", \"hard_linked_file.log\"),\n    (_OH_NO_BYTES, _OH_NO_STR),\n    (b\"symlink_to_file.log\", \"symlink_to_file.log\"),\n    (_WAT_BAD_BYTES, _WAT_BAD_STR),\n]\n\n\ndef _fix_for_os(pairs: Sequence[tuple[bytes, str]]) -> list[tuple[bytes, str]]:\n    def symlink_in_windows(s: str) -> bool:\n        return os.name == \"nt\" and s.find(\"symlink\") != -1\n\n    return [(os.sep.encode() + b, os.sep + s) for b, s in pairs if not symlink_in_windows(s)]\n\n\ndef _cvt(path):\n    return os.path.normpath(path.decode(\"utf-8\", errors=\"replace\")) if os.name == \"nt\" else path\n\n\n# NOTE: helper for mypy\ndef _end_with(actual: str | bytes, *, expected: str | bytes) -> bool:\n    if isinstance(actual, str):\n        assert isinstance(expected, str)\n        return actual.endswith(expected)\n    assert isinstance(expected, bytes)\n    return actual.endswith(expected)\n\n\n@pytest.mark.parametrize(\n    \"pattern_suffix, file_suffixes\",\n    [\n        (\"/*\", _fix_for_os(STAR_FILES)),\n        (\"/**\", _fix_for_os(STAR_FILES)),\n        (\"/subdir/*\", [(b\"/subdir/symlink_to_file.log\", \"/subdir/symlink_to_file.log\")]),\n        (\n            \"/symlink_to_dir/*\",\n            [(b\"/symlink_to_dir/yet_another_file.log\", \"/symlink_to_dir/yet_another_file.log\")],\n        ),\n    ],\n)\ndef test_find_matching_logfiles(\n    fake_filesystem: str, pattern_suffix: str, file_suffixes: Sequence[tuple[bytes, str]]\n) -> None:\n    fake_fs_path_u = ensure_text(fake_filesystem)\n    fake_fs_path_b = bytes(fake_filesystem, \"utf-8\")\n    files = lw.find_matching_logfiles(fake_fs_path_u + pattern_suffix)\n    fake_fs_file_suffixes = [\n        (fake_fs_path_b + path[0], fake_fs_path_u + path[1]) for path in file_suffixes\n    ]\n\n    for actual, expected in zip(sorted(files), fake_fs_file_suffixes):\n        assert _end_with(actual[0], expected=_cvt(expected[0]))\n\n        assert isinstance(actual[1], text_type())\n        assert actual[1].startswith(fake_fs_path_u)\n        assert actual[1] == expected[1]\n\n\ndef test_ip_in_subnetwork() -> None:\n    assert lw.ip_in_subnetwork(\"192.168.1.1\", \"192.168.1.0/24\") is True\n    assert lw.ip_in_subnetwork(\"192.160.1.1\", \"192.168.1.0/24\") is False\n    assert (\n        lw.ip_in_subnetwork(\"1762:0:0:0:0:B03:1:AF18\", \"1762:0000:0000:0000:0000:0000:0000:0000/64\")\n        is True\n    )\n    assert (\n        lw.ip_in_subnetwork(\"1760:0:0:0:0:B03:1:AF18\", \"1762:0000:0000:0000:0000:0000:0000:0000/64\")\n        is False\n    )\n\n\n@pytest.mark.parametrize(\n    \"buff,encoding,position\",\n    [\n        (b'\\xFE\\xFF', 'utf_16_be', 2),\n        (b'\\xFF\\xFE', 'utf_16', 2),\n        (b'no encoding in this file!', locale.getpreferredencoding(), 0),\n    ],\n)\ndef test_log_lines_iter_encoding(\n    monkeypatch: pytest.MonkeyPatch, buff: bytes, encoding: str, position: int\n) -> None:\n    monkeypatch.setattr(os, 'open', lambda *_args: None)\n    monkeypatch.setattr(os, 'close', lambda *_args: None)\n    monkeypatch.setattr(os, 'read', lambda *_args: buff)\n    monkeypatch.setattr(os, 'lseek', lambda *_args: len(buff))\n    with lw.LogLinesIter('void', None) as log_iter:\n        assert log_iter._enc == encoding\n        assert log_iter.get_position() == position\n\n\ndef test_log_lines_iter() -> None:\n    txt_file = lw.__file__.rstrip('c')\n    with lw.LogLinesIter(txt_file, None) as log_iter:\n        log_iter.set_position(122)\n        assert log_iter.get_position() == 122\n\n        line = log_iter.next_line()\n        assert isinstance(line, text_type())\n        assert (\n            line\n            == \"# This file is part of Checkmk (https://checkmk.com). It is subject to the terms and\\n\"\n        )\n        assert log_iter.get_position() == 207\n\n        log_iter.push_back_line('TÃ¤ke this!')\n        assert log_iter.get_position() == 196\n        assert log_iter.next_line() == 'TÃ¤ke this!'\n\n        log_iter.skip_remaining()\n        assert log_iter.next_line() is None\n        assert log_iter.get_position() == os.stat(txt_file).st_size\n\n\ndef _latin_1_encoding():\n    return \"cp1252\" if os.name == \"nt\" else \"latin-1\"\n\n\n@pytest.mark.parametrize(\n    \"use_specific_encoding,lines,expected_result\",\n    [\n        # UTF-8 encoding works by default in Linux and must be selected in Windows\n        (\n            \"utf-8\" if os.name == \"nt\" else None,\n            [\n                b\"abc1\",\n                \"Ã¤bc2\".encode(),\n                b\"abc3\",\n            ],\n            [\n                \"abc1\\n\",\n                \"Ã¤bc2\\n\",\n                \"abc3\\n\",\n            ],\n        ),\n        # Replace characters that can not be decoded\n        (\n            \"utf-8\" if os.name == \"nt\" else None,\n            [\n                b\"abc1\",\n                \"Ã¤bc2\".encode(_latin_1_encoding()),\n                b\"abc3\",\n            ],\n            [\n                \"abc1\\n\",\n                \"\\ufffdbc2\\n\",\n                \"abc3\\n\",\n            ],\n        ),\n        # Set custom encoding\n        (\n            _latin_1_encoding(),\n            [\n                b\"abc1\",\n                \"Ã¤bc2\".encode(_latin_1_encoding()),\n                b\"abc3\",\n            ],\n            [\n                \"abc1\\n\",\n                \"Ã¤bc2\\n\",\n                \"abc3\\n\",\n            ],\n        ),\n    ],\n)\ndef test_non_ascii_line_processing(\n    tmpdir, monkeypatch, use_specific_encoding, lines, expected_result\n):\n    # Write test logfile first\n    log_path = os.path.join(str(tmpdir), \"testlog\")\n    with open(log_path, \"wb\") as f:\n        f.write(b\"\\n\".join(lines) + b\"\\n\")\n\n    # Now test processing\n    with lw.LogLinesIter(log_path, None) as log_iter:\n        if use_specific_encoding:\n            log_iter._enc = use_specific_encoding\n\n        result = []\n        while True:\n            l = log_iter.next_line()\n            if l is None:\n                break\n            result.append(l)\n\n        assert result == expected_result\n\n\ndef _linux_dataset_path(filename):\n    return os.path.join(os.path.dirname(__file__), \"datasets\", \"mk_logwatch\", filename)\n\n\ndef _windows_dataset_path(filename):\n    return os.path.join(\n        os.path.dirname(__file__),\n        \"..\",\n        \"..\",\n        \"..\",\n        \"..\",\n        \"tests\",\n        \"agent-plugin-unit\",\n        \"datasets\",\n        \"mk_logwatch\",\n        filename,\n    )\n\n\ndef _path_to_testfile(filename):\n    return (\n        _linux_dataset_path(filename)\n        if os.path.exists(_linux_dataset_path(filename))\n        else _windows_dataset_path(filename)\n    )\n\n\nclass MockStdout:\n    def isatty(self):\n        return False\n\n\n@pytest.mark.parametrize(\n    \"logfile, patterns, opt_raw, state, expected_output\",\n    [\n        (\n            __file__,\n            [\n                ('W', re.compile('^[^u]*W.*I mÃ¤tch Ã¶nly mysÃ©lf ðŸ§š', re.UNICODE), [], []),\n                ('I', re.compile('.*', re.UNICODE), [], []),\n            ],\n            {'nocontext': True},\n            {\n                'offset': 0,\n            },\n            [\n                \"[[[%s]]]\\n\" % __file__,\n                \"W                 ('W', re.compile('^[^u]*W.*I m\\xe4tch \\xf6nly mys\\xe9lf \\U0001f9da', re.UNICODE), [], []),\\n\",\n            ],\n        ),\n        (\n            __file__,\n            [\n                ('W', re.compile('I don\\'t match anything at all!', re.UNICODE), [], []),\n            ],\n            {},\n            {\n                'offset': 0,\n            },\n            [\n                \"[[[%s]]]\\n\" % __file__,\n            ],\n        ),\n        (\n            __file__,\n            [\n                ('W', re.compile('.*', re.UNICODE), [], []),\n            ],\n            {},\n            {},\n            [  # nothing for new files\n                \"[[[%s]]]\\n\" % __file__,\n            ],\n        ),\n        (\n            __file__,\n            [\n                ('C', re.compile('ðŸ‰', re.UNICODE), [], []),\n                ('I', re.compile('.*', re.UNICODE), [], []),\n            ],\n            {'nocontext': True},\n            {\n                'offset': 0,\n            },\n            [\n                \"[[[%s]]]\\n\" % __file__,\n                \"C                 ('C', re.compile('\\U0001f409', re.UNICODE), [], []),\\n\",\n            ],\n        ),\n        ('locked door', [], {}, {}, [\"[[[locked door:cannotopen]]]\\n\"]),\n        (\n            _path_to_testfile(\"test_append.log\"),\n            [\n                (\n                    'C',\n                    re.compile('.*Error.*'),\n                    [\n                        re.compile('.*more information.*'),\n                        re.compile('.*also important.*'),\n                    ],\n                    [],\n                ),\n            ],\n            {'nocontext': True},\n            {\n                'offset': 0,\n            },\n            [\n                '[[[%s]]]\\n' % _path_to_testfile(\"test_append.log\"),\n                'C Error: Everything down!\\x01more information: very useful\\x01also important: please inform admins\\n',\n            ],\n        ),\n    ],\n)\ndef test_process_logfile(monkeypatch, logfile, patterns, opt_raw, state, expected_output):\n\n    section = lw.LogfileSection((logfile, logfile))\n    section.options.values.update(opt_raw)\n    # in Windows default encoding, i.e., None means cp1252 and we cant change default easy\n    # we want to test utf-8 file, so please\n    if os.name == \"nt\":\n        section.options.values.update({\"encoding\": \"utf-8\"})\n    section._compiled_patterns = patterns\n\n    monkeypatch.setattr(sys, 'stdout', MockStdout())\n    header, warning_and_errors = lw.process_logfile(section, state, False)\n    output = [header] + warning_and_errors\n    assert output == expected_output\n    if len(output) > 1:\n        assert isinstance(state['offset'], int)\n        if logfile == __file__:\n            assert state['offset'] >= 15000  # about the size of this file\n\n\n@pytest.mark.parametrize(\n    \"input_lines, before, after, expected_output\",\n    [\n        ([], 2, 3, []),\n        (\n            [\"0\", \"1\", \"2\", \"C 3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"W 10\"],\n            2,\n            3,\n            [\"1\", \"2\", \"C 3\", \"4\", \"5\", \"6\", \"8\", \"9\", \"W 10\"],\n        ),\n        ([\"C 0\", \"1\", \"2\"], 12, 17, [\"C 0\", \"1\", \"2\"]),\n    ],\n)\ndef test_filter_maxcontextlines(\n    input_lines: list[str], before: int, after: int, expected_output: list[str]\n) -> None:\n    assert expected_output == list(lw._filter_maxcontextlines(input_lines, before, after))\n\n\n@pytest.mark.parametrize(\n    \"input_lines, nocontext, expected_output\",\n    [\n        ([], False, []),\n        ([], True, []),\n        ([\"ln\", \"ln2\", \"ln2\", \"ln2\", \"ln3\", \"ln3\", \"ln\"], True, [\"ln\", \"ln2\", \"ln3\", \"ln\"]),\n        (\n            [\"ln\", \"ln2\", \"ln2\", \"ln2\", \"ln3\", \"ln3\", \"ln\"],\n            False,\n            [\n                \"ln\",\n                \"ln2\",\n                \". [the above message was repeated 2 times]\\n\",\n                \"ln3\",\n                \". [the above message was repeated 1 times]\\n\",\n                \"ln\",\n            ],\n        ),\n        ([\"ln\", \"ln\"], False, [\"ln\", \". [the above message was repeated 1 times]\\n\"]),\n        ((str(i) for i in range(3)), False, [\"0\", \"1\", \"2\"]),\n    ],\n)\ndef test_filter_consecutive_duplicates(\n    input_lines: Sequence[str], nocontext: bool, expected_output: Sequence[str]\n) -> None:\n    assert expected_output == list(lw._filter_consecutive_duplicates(input_lines, nocontext))\n\n\n@pytest.fixture\ndef fake_filesystem(tmpdir):\n    root = [\n        # name     | type  | content/target\n        (\"file.log\", \"file\", None),\n        (_WAT_BAD, \"file\", None),\n        (_OH_NO, \"file\", None),\n        (\"symlink_to_file.log\", \"symlink\", \"symlinked_file.log\"),\n        (\n            \"subdir\",\n            \"dir\",\n            [\n                (\"symlink_to_file.log\", \"symlink\", \"another_symlinked_file.log\"),\n                (\n                    \"subsubdir\",\n                    \"dir\",\n                    [\n                        (\"yaf.log\", \"file\", None),\n                    ],\n                ),\n            ],\n        ),\n        (\"symlink_to_dir\", \"symlink\", \"symlinked_dir\"),\n        (\n            \"symlinked_dir\",\n            \"dir\",\n            [\n                (\"yet_another_file.log\", \"file\", None),\n            ],\n        ),\n        (\"hard_linked_file.log\", \"file\", None),\n        (\"hard_link_to_file.log\", \"hardlink\", \"hard_linked_file.log\"),\n    ]\n\n    def create_recursively(dirpath, name, type_, value):\n        obj_path = os.path.join(ensure_binary(dirpath), ensure_binary(name))\n\n        if type_ == \"file\":\n            with open(obj_path, 'w'):\n                pass\n            return\n\n        if type_ == \"dir\":\n            os.mkdir(obj_path)\n            for spec in value:\n                create_recursively(obj_path, *spec)\n            return\n\n        source = os.path.join(ensure_binary(dirpath), ensure_binary(value))\n        if type_ == \"symlink\":\n            if os.name != \"nt\":\n                os.symlink(source, obj_path)\n        else:\n            os.link(source, obj_path)\n\n    create_recursively(str(tmpdir), \"root\", \"dir\", root)\n\n    return os.path.join(str(tmpdir), \"root\")\n\n\ndef test_process_batches(tmpdir, mocker):\n    mocker.patch.object(lw, \"MK_VARDIR\", str(tmpdir))\n    lw.process_batches(\n        [lw.ensure_text_type(l) for l in [\"line1\", \"line2\"]],\n        \"batch_id\",\n        \"::remote\",\n        123,\n        456,\n    )\n    assert os.path.isfile(\n        os.path.join(\n            str(tmpdir),\n            \"logwatch-batches\",\n            \"__remote\" if os.name == \"nt\" else \"::remote\",\n            \"logwatch-batch-file-batch_id\",\n        )\n    )\n\n\ndef _get_file_info(tmp_path, file_name):\n    return lw.get_file_info(os.path.join(str(tmp_path), \"root\", file_name))\n\n\ndef test_get_uniq_id_one_file(fake_filesystem, tmpdir):\n    file_id, sz = _get_file_info(tmpdir, \"file.log\")\n    assert file_id > 1\n    assert sz == 0\n    assert (file_id, sz) == _get_file_info(tmpdir, \"file.log\")\n\n\ndef test_get_uniq_id_with_hard_link(fake_filesystem, tmpdir):\n    info = [\n        _get_file_info(tmpdir, f)\n        for f in (\"file.log\", \"hard_linked_file.log\", \"hard_link_to_file.log\")\n    ]\n    assert {s for (_, s) in info} == {0}\n    assert len({i for (i, _) in info}) == 2\n    assert info[0][0] != info[1][0]\n    assert info[1][0] == info[2][0]\n\n\ndef test_main(tmpdir, mocker):\n    mocker.patch.object(lw, \"MK_VARDIR\", str(tmpdir))\n    lw.main()\n"}
{"type": "test_file", "path": "agents/wnx/tests/files/scripts/combine_utf.py", "content": "#!/usr/bin/env python3\n# Copyright (C) 2019 Checkmk GmbH - License: GNU General Public License v2\n# This file is part of Checkmk (https://checkmk.com). It is subject to the terms and\n# conditions defined in the file COPYING, which is part of this source code package.\n\nimport argparse\nfrom typing import Final\n\nUTF_16_LE_BOM: Final = b\"\\xff\\xfe\"\nUTF_8_BOM: Final = b\"\\xef\\xbb\\xbf\"\n\n\ndef _offset_by_bom(data: bytes) -> int:\n    return 2 if data.startswith(UTF_16_LE_BOM) else 3 if data.startswith(UTF_8_BOM) else 0\n\n\ndef read_files(files: list[str]) -> bytes:\n    print(f\"Processing files: {', '.join(files)}\")\n    output = b\"\"\n    for file in files:\n        with open(file, \"rb\") as f:\n            file_data = f.read()\n            output += file_data[_offset_by_bom(file_data) :]\n    return output\n\n\ndef save_output(file: str, output: bytes) -> None:\n    print(f\"Saving to file: {file} {len(output)} bytes\")\n    with open(file, \"wb\") as f:\n        f.write(UTF_16_LE_BOM)\n        f.write(output)\n\n\ndef parse_args() -> tuple[str | None, list[str]]:\n    parser = argparse.ArgumentParser(\n        description=\"Concatenates files into UTF-16 BOM LE file with dropping BOM marks.\"\n    )\n    parser.add_argument(\"--output\", type=str, help=\"resulting data\")\n    parser.add_argument(\n        \"input_files\", metavar=\"file\", type=str, nargs=\"+\", help=\"files with UTF data\"\n    )\n    args = parser.parse_args()\n\n    return args.output, args.input_files\n\n\nif __name__ == \"__main__\":\n    output_file, input_files = parse_args()\n    all_data = read_files(input_files)\n    if output_file:\n        save_output(output_file, all_data)\n    else:\n        print(f\"Test run: gathered {len(all_data)}\")\n"}
{"type": "test_file", "path": "agents/wnx/tests/integration/conftest.py", "content": "#!/usr/bin/env python3\n# Copyright (C) 2019 Checkmk GmbH - License: GNU General Public License v2\n# This file is part of Checkmk (https://checkmk.com). It is subject to the terms and\n# conditions defined in the file COPYING, which is part of this source code package.\n\nimport os\nimport shutil\nfrom collections.abc import Generator\nfrom pathlib import Path\nfrom typing import Final, TypeVar\n\nimport pytest\nimport yaml\n\nfrom .utils import (\n    AGENT_EXE_NAME,\n    check_os,\n    create_legacy_pull_file,\n    create_protocol_file,\n    get_path_from_env,\n    INTEGRATION_PORT,\n    PYTHON_CAB_NAME,\n    YamlDict,\n)\n\nT = TypeVar(\"T\")\nYieldFixture = Generator[T, None, None]\n\n\ncheck_os()\n\n_DEFAULT_CONFIG: Final = \"\"\"\nglobal:\n  enabled: true\n  disabled_sections: wmi_webservices\n  logging:\n    debug: true\n  wmi_timeout: 10\n  port: {}\n\"\"\"\n\n_TEST_ENV_VAR: Final = \"WNX_INTEGRATION_BASE_DIR\"  # supplied by script\n_CHECKMK_GIT_ENV_VAR: Final = \"CHECKMK_GIT_DIR\"  # supplied by script\n_ARTIFACTS_ENV_VAR: Final = \"arte\"  # supplied by script\n_FACTORY_YAML_CONFIG: Final = \"check_mk.yml\"\n_CTL_EXE_NAME: Final = \"cmk-agent-ctl.exe\"\n\n\n@pytest.fixture(name=\"artifacts_dir\", scope=\"session\")\ndef artifacts_dir_fixture() -> Path:\n    p = get_path_from_env(_ARTIFACTS_ENV_VAR)\n    assert p.exists()\n    return p\n\n\n@pytest.fixture(name=\"main_dir\", scope=\"session\")\ndef main_dir_fixture() -> Path:\n    return get_path_from_env(_TEST_ENV_VAR)\n\n\n@pytest.fixture(name=\"git_dir\", scope=\"session\")\ndef git_dir_fixture(main_dir: Path) -> Path:\n    return get_path_from_env(_CHECKMK_GIT_ENV_VAR)\n\n\n@pytest.fixture(name=\"data_dir\", scope=\"session\")\ndef data_dir_fixture(main_dir: Path) -> Path:\n    return main_dir / \"test\" / \"data\"\n\n\n@pytest.fixture(name=\"module_dir\", scope=\"session\")\ndef module_dir_fixture(data_dir: Path) -> Path:\n    return data_dir / \"modules\" / \"python-3\"\n\n\n@pytest.fixture(name=\"root_dir\", scope=\"session\")\ndef root_dir_fixture(main_dir: Path) -> Path:\n    return main_dir / \"test\" / \"root\"\n\n\n@pytest.fixture(name=\"main_exe\", scope=\"session\")\ndef main_exe_fixture(root_dir: Path) -> Path:\n    return root_dir / AGENT_EXE_NAME\n\n\n@pytest.fixture(name=\"default_yaml_config\", scope=\"session\")\ndef default_yaml_config_fixture() -> YamlDict:\n    return yaml.safe_load(_DEFAULT_CONFIG.format(INTEGRATION_PORT))\n\n\n@pytest.fixture(autouse=True, scope=\"session\")\ndef setup_all(\n    main_dir: Path,\n    data_dir: Path,\n    artifacts_dir: Path,\n    root_dir: Path,\n) -> YieldFixture[None]:\n    shutil.rmtree(main_dir, ignore_errors=True)\n    os.makedirs(root_dir, exist_ok=True)\n    os.makedirs(data_dir, exist_ok=True)\n    os.makedirs(data_dir / \"bin\", exist_ok=True)\n    os.makedirs(data_dir / \"log\", exist_ok=True)\n    os.makedirs(data_dir / \"modules\" / \"python-3\", exist_ok=True)\n    os.makedirs(data_dir / \"plugins\", exist_ok=True)\n    os.makedirs(data_dir / \"install\" / \"modules\", exist_ok=True)\n    for f in [_FACTORY_YAML_CONFIG, AGENT_EXE_NAME, _CTL_EXE_NAME, PYTHON_CAB_NAME]:\n        shutil.copy(artifacts_dir / f, root_dir)\n    shutil.copy(root_dir / PYTHON_CAB_NAME, data_dir / \"install\" / \"modules\")\n    create_protocol_file(data_dir)\n    create_legacy_pull_file(data_dir)\n    yield\n"}
{"type": "test_file", "path": "agents/wnx/tests/integration/test_check_mk_cmd_line.py", "content": "#!/usr/bin/env python3\n# Copyright (C) 2019 Checkmk GmbH - License: GNU General Public License v2\n# This file is part of Checkmk (https://checkmk.com). It is subject to the terms and\n# conditions defined in the file COPYING, which is part of this source code package.\n\nfrom pathlib import Path\nfrom typing import Final\n\nimport pytest\n\nfrom .utils import run_agent, YamlDict\n\n\ndef test_check_mk_agent_cmd_line_help(\n    main_exe: Path,\n    default_yaml_config: YamlDict,\n    data_dir: Path,\n) -> None:\n    output = run_agent(\n        default_yaml_config,\n        param=\"help\",\n        main_exe=main_exe,\n        data_dir=data_dir,\n    )\n    assert output.ret_code == 0\n    assert output.stderr == \"\"\n    assert output.stdout.startswith(\"Normal Usage:\")\n    assert 100 > len(output.stdout.split(\"\\r\\n\")) > 50\n\n\ndef _splitter(line: str) -> dict[str, str]:\n    l = line.split(\"DIR=\")\n    return {l[0]: l[1]}\n\n\n_ENV_MAP: Final = {\n    \"MK_LOCALDIR\": \"local\",\n    \"MK_STATEDIR\": \"state\",\n    \"MK_PLUGINSDIR\": \"plugins\",\n    \"MK_TEMPDIR\": \"tmp\",\n    \"MK_LOGDIR\": \"log\",\n    \"MK_CONFDIR\": \"config\",\n    \"MK_SPOOLDIR\": \"spool\",\n    \"MK_INSTALLDIR\": \"install\",\n    \"MK_MSI_PATH\": \"update\",\n    \"MK_MODULESDIR\": \"modules\",\n}\n\n\ndef test_check_mk_agent_cmd_line_show_config(\n    main_exe: Path,\n    default_yaml_config: YamlDict,\n    data_dir: Path,\n) -> None:\n    output = run_agent(\n        default_yaml_config,\n        param=\"showconfig\",\n        main_exe=main_exe,\n        data_dir=data_dir,\n    )\n    assert output.ret_code == 0\n    assert output.stderr == \"\"\n    assert output.stdout.startswith(\"# Environment Variables:\")\n    result = output.stdout.split(\"\\r\\n\")\n    assert 200 > len(result) > 100\n    r = set(result)\n    assert {\"  disabled_sections: wmi_webservices\", \"  port: 25998\"}.issubset(r)\n\n    envs_set = {l[2:] for l in result if l.startswith(\"# MK_\")}\n    envs = {p[0]: p[1] for p in list(map(lambda x: x.split(\"=\"), envs_set))}\n    for k, v in _ENV_MAP.items():\n        assert envs[k] == f'\"{str(data_dir / v)}\"'\n\n\n@pytest.mark.parametrize(\n    \"command, code, starts_with\",\n    [\n        (\"trash\", 13, 'Provided Parameter \"trash\" is not allowed'),\n        (\"updater\", 1, \"\\r\\n\\tYou must install Agent Updater Python plugin to use the updater\"),\n    ],\n)\ndef test_check_mk_agent_cmd_line_bad(\n    main_exe: Path,\n    default_yaml_config: YamlDict,\n    data_dir: Path,\n    command: str,\n    code: int,\n    starts_with: str,\n) -> None:\n    output = run_agent(\n        default_yaml_config,\n        param=command,\n        main_exe=main_exe,\n        data_dir=data_dir,\n    )\n    assert output.ret_code == code\n    assert output.stderr == \"\"\n    assert output.stdout.startswith(starts_with)\n"}
{"type": "test_file", "path": "agents/wnx/tests/integration/test_check_mk_run.py", "content": "#!/usr/bin/env python3\n# Copyright (C) 2019 Checkmk GmbH - License: GNU General Public License v2\n# This file is part of Checkmk (https://checkmk.com). It is subject to the terms and\n# conditions defined in the file COPYING, which is part of this source code package.\n\nimport ast\nfrom collections.abc import Sequence\nfrom pathlib import Path\nfrom typing import Final\n\nimport pytest\n\nfrom .utils import CTL_STATUS_LINE, obtain_agent_data, ONLY_FROM_LINE, SECTION_COUNT, YamlDict\n\n\ndef _make_config(config: YamlDict, only_from: Sequence[str]) -> YamlDict:\n    if only_from:\n        config[\"global\"][\"only_from\"] = only_from\n    return config\n\n\ndef _get_ctl_status_line(data: Sequence[str]) -> dict[str, str | list | bool]:\n    s = data[CTL_STATUS_LINE].replace(\":false\", \":False\").replace(\":true\", \":True\")\n    return ast.literal_eval(s)\n\n\n_NOT_WMI_SECTIONS: Final = {\n    \"<<<wmi_cpuload:sep(124)>>>\",\n    \"<<<uptime>>>\",\n    \"<<<mem>>>\",\n    \"<<<df:sep(9)>>>\",\n    \"<<<fileinfo:sep(124)>>>\",\n    \"<<<logwatch>>>\",\n    \"<<<checkmk_agent_plugins_win:sep(0)>>>\",\n    \"<<<winperf_phydisk>>>\",\n    \"<<<winperf_if>>>\",\n    \"<<<winperf_processor>>>\",\n    \"<<<ps:sep(9)>>>\",\n    \"<<<services>>>\",\n}\n\n_WMI_SECTIONS: Final = {\n    \"<<<dotnet_clrmemory:sep(124)>>>\",\n    # \"<<<wmi_webservices:sep(124)>>>\", <--- not presented in CI VM\n}\n\n\n_INTERNAL_SECTIONS: Final = _NOT_WMI_SECTIONS.union(_WMI_SECTIONS)\n\n\n@pytest.mark.parametrize(\n    \"only_from, description\",\n    [\n        ([], \"default\"),\n        ([\"127.0.0.1\", \"::1\", \"10.1.2.3\"], \"custom\"),\n    ],\n)\ndef test_check_mk_base(\n    main_exe: Path,\n    default_yaml_config: YamlDict,\n    data_dir: Path,\n    only_from: list[str],\n    description: str,\n) -> None:\n    output = obtain_agent_data(\n        _make_config(default_yaml_config, only_from),\n        main_exe=main_exe,\n        data_dir=data_dir,\n    )\n    # correct value of only_from also guaranties that we are using own config\n    assert output[ONLY_FROM_LINE] == \"OnlyFrom: \" + \" \".join(only_from)\n\n    # NOTE. We validate the output only roughly: sections must be presented in correct order.\n    # Full validation may be achieved only using checkmk site and this is impossible.\n    # Details of a section are verified using unit-tests.\n    sections = [line for line in output if line[:3] == \"<<<\"]\n    assert sections[0] == \"<<<check_mk>>>\"\n    assert sections[1] == \"<<<cmk_agent_ctl_status:sep(0)>>>\"\n    assert sections.count(\"<<<>>>\") == 2\n    assert _INTERNAL_SECTIONS.issubset(set(sections)), (\n        f\"Missing sections: {_INTERNAL_SECTIONS.difference(set(sections))}\"\n    )\n    assert sections[-1] == \"<<<systemtime>>>\"\n    assert len(sections) == SECTION_COUNT\n\n    # Validate controller status is the expected one.\n    ctl_status = _get_ctl_status_line(output)\n    assert isinstance(ctl_status[\"version\"], str)\n    assert ctl_status[\"version\"].startswith(\"2.\")\n    assert ctl_status[\"agent_socket_operational\"] is True\n    assert ctl_status[\"ip_allowlist\"] == only_from\n    assert ctl_status[\"allow_legacy_pull\"] is True\n    assert ctl_status[\"connections\"] == []\n\n\n@pytest.fixture(name=\"config_no_wmi\")\ndef config_no_wmi_fixture(default_yaml_config: YamlDict) -> YamlDict:\n    default_yaml_config[\"global\"][\"wmi_timeout\"] = 0\n    return default_yaml_config\n\n\ndef test_check_mk_no_wmi(\n    main_exe: Path,\n    config_no_wmi: YamlDict,\n    data_dir: Path,\n) -> None:\n    output = obtain_agent_data(\n        config_no_wmi,\n        main_exe=main_exe,\n        data_dir=data_dir,\n    )\n    sections = [line for line in output if line[:3] == \"<<<\"]\n    assert sections[0] == \"<<<check_mk>>>\"\n    assert sections[1] == \"<<<cmk_agent_ctl_status:sep(0)>>>\"\n    assert sections.count(\"<<<>>>\") == 2\n    assert len(_WMI_SECTIONS.intersection(set(sections))) == 0\n    assert sections[-1] == \"<<<systemtime>>>\"\n"}
{"type": "test_file", "path": "agents/wnx/tests/integration/test_python_module.py", "content": "#!/usr/bin/env python3\n# Copyright (C) 2019 Checkmk GmbH - License: GNU General Public License v2\n# This file is part of Checkmk (https://checkmk.com). It is subject to the terms and\n# conditions defined in the file COPYING, which is part of this source code package.\nimport shutil\nfrom pathlib import Path\n\nimport pytest\n\nfrom .conftest import YieldFixture\nfrom .utils import (\n    CMK_UPDATER_CHECKMK_PY,\n    CMK_UPDATER_PY,\n    patch_venv_config,\n    postinstall_module,\n    run_agent,\n    unpack_modules,\n    YamlDict,\n)\n\n\n@pytest.fixture(name=\"unpack\", scope=\"module\")\ndef unpack_fixture(\n    root_dir: Path,\n    module_dir: Path,\n) -> YieldFixture[None]:\n    unpack_modules(root_dir, module_dir=module_dir)\n    yield\n    shutil.rmtree(module_dir)\n\n\ndef copy_cmk_updater(source_dir: Path, target_dir: Path) -> None:\n    shutil.copy(source_dir / CMK_UPDATER_PY, target_dir / CMK_UPDATER_CHECKMK_PY)\n\n\ndef test_python_module(\n    main_exe: Path,\n    default_yaml_config: YamlDict,\n    unpack: object,\n    module_dir: Path,\n    data_dir: Path,\n    git_dir: Path,\n) -> None:\n    assert postinstall_module(module_dir) == 0\n    assert (module_dir / \"DLLs\").exists()\n    patch_venv_config(module_dir)\n    output = run_agent(\n        default_yaml_config,\n        param=\"updater\",\n        main_exe=main_exe,\n        data_dir=data_dir,\n    )\n    assert output.ret_code == 1\n    assert output.stdout.startswith(\"\\r\\n\\tYou must install Agent Updater Python plugin\")\n    copy_cmk_updater(\n        git_dir / \"non-free\" / \"cmk-update-agent\",\n        data_dir / \"plugins\",\n    )\n    output = run_agent(\n        default_yaml_config,\n        param=\"updater\",\n        main_exe=main_exe,\n        data_dir=data_dir,\n    )\n    assert output.ret_code == 0\n    assert output.stderr.startswith(\"Missing config file\")\n    assert output.stdout.startswith(\"<<<cmk_update_agent_status:sep(0)>>>\")\n"}
{"type": "test_file", "path": "agents/wnx/tests/integration/utils.py", "content": "#!/usr/bin/env python3\n# Copyright (C) 2019 Checkmk GmbH - License: GNU General Public License v2\n# This file is part of Checkmk (https://checkmk.com). It is subject to the terms and\n# conditions defined in the file COPYING, which is part of this source code package.\n\nimport asyncio\nimport os\nimport platform\nimport subprocess\nimport sys\nimport time\nfrom collections.abc import Iterator\nfrom contextlib import contextmanager\nfrom pathlib import Path\nfrom typing import Any, Final, NamedTuple\n\nimport telnetlib3  # type: ignore[import-untyped]\nimport yaml\n\n# check_mk section, example of output\n# <<<check_mk>>>\n# Version: 2.3.0b1\n# BuildDate: Jan  5 2024\n# AgentOS: windows\n# OSName: Microsoft Windows 10 Pro\n# OSVersion: 10.0.19045\n# OSType: windows\n# Hostname: klapp-9999\n# Architecture: 64bit\n# Time: 2024-01-05T14:47:46+0100\n# WorkingDirectory: C:\\Program Files (x86)\\checkmk\\service\n# ConfigFile: C:\\Program Files (x86)\\checkmk\\service\\check_mk.yml\n# LocalConfigFile: C:\\ProgramData\\checkmk\\agent\\check_mk.user.yml\n# AgentDirectory: C:\\Program Files (x86)\\checkmk\\service\n# PluginsDirectory: C:\\ProgramData\\checkmk\\agent\\plugins\n# StateDirectory: C:\\ProgramData\\checkmk\\agent\\state\n# ConfigDirectory: C:\\ProgramData\\checkmk\\agent\\config\n# TempDirectory: C:\\ProgramData\\checkmk\\agent\\tmp\n# LogDirectory: C:\\ProgramData\\checkmk\\agent\\log\n# SpoolDirectory: C:\\ProgramData\\checkmk\\agent\\spool\n# LocalDirectory: C:\\ProgramData\\checkmk\\agent\\local\n# OnlyFrom:\n\n\nYamlDict = dict[str, dict[str, Any]]\nINTEGRATION_PORT: Final = 25998\nAGENT_EXE_NAME: Final = \"check_mk_agent.exe\"\n_HOST: Final = \"localhost\"\nUSER_YAML_CONFIG: Final = \"check_mk.user.yml\"\nSECTION_COUNT: Final = 18\nONLY_FROM_LINE: Final = 21\nCTL_STATUS_LINE: Final = ONLY_FROM_LINE + 2\nPYTHON_CAB_NAME: Final = \"python-3.cab\"\nCMK_UPDATER_PY: Final = \"cmk_update_agent.py\"\nCMK_UPDATER_CHECKMK_PY: Final = \"cmk_update_agent.checkmk.py\"\n\n\nclass ExeOutput(NamedTuple):\n    ret_code: int\n    stdout: str\n    stderr: str\n\n\ndef create_protocol_file(directory: Path) -> None:\n    # block  upgrading\n    protocol_dir = directory / \"config\"\n    try:\n        os.makedirs(protocol_dir)\n    except OSError as e:\n        print(f\"Probably folders exist: {e}\")\n\n    if not protocol_dir.exists():\n        print(f\"Directory {protocol_dir} doesn't exist, may be you have not enough rights\")\n        sys.exit(11)\n\n    protocol_file = protocol_dir / \"upgrade.protocol\"\n    with open(protocol_file, \"w\") as f:\n        f.write(\"Upgraded:\\n   time: '2019-05-20 18:21:53.164\")\n\n\ndef create_legacy_pull_file(directory: Path) -> None:\n    # we allow legacy communication: TLS testing is not for this test\n    allow_legacy_pull_file = directory / \"allow-legacy-pull\"\n    with open(allow_legacy_pull_file, \"w\") as f:\n        f.write(\"Created by integration tests\")\n\n\n_result = \"\"\n\n\nasync def _telnet_shell(reader: telnetlib3.TelnetReader, _: telnetlib3.TelnetWriter) -> None:\n    global _result\n    while True:\n        data = await reader.read(1024)\n        if not data:\n            break\n        _result += data\n\n\ndef _read_client_data(host: str, port: int) -> None:\n    loop = asyncio.get_event_loop()\n    coro = telnetlib3.open_connection(host, port, shell=_telnet_shell)\n    _, writer = loop.run_until_complete(coro)\n    loop.run_until_complete(writer.protocol.waiter_closed)\n\n\ndef _get_data_using_telnet(host: str, port: int) -> list[str]:\n    # overloaded CI Node may delay start/init of the agent process\n    # we must retry connection few times to avoid complaints\n    global _result\n    _result = \"\"\n    for _ in range(5):\n        try:\n            _read_client_data(host, port)\n            if _result:\n                return _result.splitlines()\n            time.sleep(2)\n        except Exception as _:\n            # print('No connect, waiting for agent')\n            time.sleep(2)\n\n    return []\n\n\ndef get_path_from_env(env: str) -> Path:\n    env_value = os.getenv(env)\n    assert env_value is not None\n    return Path(env_value)\n\n\ndef check_os() -> None:\n    assert platform.system() == \"Windows\"\n\n\n@contextmanager\ndef _write_config(work_config: YamlDict, data_dir: Path) -> Iterator[None]:\n    yaml_file = data_dir / USER_YAML_CONFIG\n    try:\n        with open(yaml_file, \"w\") as f:\n            ret = yaml.dump(work_config)\n            f.write(ret)\n        yield\n    finally:\n        yaml_file.unlink()\n\n\ndef obtain_agent_data(\n    work_config: YamlDict,\n    *,\n    main_exe: Path,\n    data_dir: Path,\n) -> list[str]:\n    with (\n        _write_config(work_config, data_dir),\n        subprocess.Popen(\n            [main_exe, \"exec\", \"-integration\"],\n            stdout=subprocess.PIPE,\n            stdin=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n        ) as p,\n    ):\n        try:\n            result = _get_data_using_telnet(_HOST, INTEGRATION_PORT)\n        finally:\n            # NOTE. we MUST kill both processes (as a _tree_!): we do not need it.\n            # Any graceful killing may require a lot of time and gives nothing to testing.\n            subprocess.call(\n                f'taskkill /F /T /FI \"pid eq {p.pid}\" /FI \"IMAGENAME eq {AGENT_EXE_NAME}\"'\n            )\n\n    return result\n\n\ndef run_agent(\n    work_config: YamlDict,\n    *,\n    param: str,\n    main_exe: Path,\n    data_dir: Path,\n) -> ExeOutput:\n    with (\n        _write_config(work_config, data_dir),\n        subprocess.Popen(\n            [main_exe, param],\n            stdout=subprocess.PIPE,\n            stdin=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n        ) as p,\n    ):\n        try:\n            stdout, stderr = p.communicate(timeout=10)\n            ret_code = p.returncode\n        finally:\n            # NOTE. we MUST kill both processes (as a _tree_!): we do not need it.\n            # Any graceful killing may require a lot of time and gives nothing to testing.\n            subprocess.call(\n                f'taskkill /F /T /FI \"pid eq {p.pid}\" /FI \"IMAGENAME eq {AGENT_EXE_NAME}\"'\n            )\n\n    return ExeOutput(ret_code, stdout=stdout.decode(), stderr=stderr.decode())\n\n\ndef unpack_modules(root_dir: Path, *, module_dir: Path) -> int:\n    # subprocess.call(f\"expand {root_dir / PYTHON_CAB_NAME } -F:* {data_dir / 'modules'/ 'python-3'}\")\n    with (\n        subprocess.Popen(\n            [\n                \"expand.exe\",\n                f\"{root_dir / PYTHON_CAB_NAME}\",\n                \"-F:*\",\n                f\"{module_dir}\",\n            ],\n            stdout=subprocess.PIPE,\n            stdin=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n        ) as p,\n    ):\n        _, __ = p.communicate(timeout=30)\n        return p.returncode\n\n\n@contextmanager\ndef _change_dir(to_dir: Path) -> Iterator[None]:\n    p = os.getcwd()\n    os.chdir(to_dir)\n    try:\n        yield\n    finally:\n        os.chdir(p)\n\n\ndef postinstall_module(module_dir: Path) -> int:\n    with (\n        _change_dir(module_dir),\n        subprocess.Popen(\n            [\n                \"postinstall.cmd\",\n            ],\n            stdout=subprocess.PIPE,\n            stdin=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n        ) as p,\n    ):\n        _, __ = p.communicate(timeout=30)\n        return p.returncode\n\n\ndef patch_venv_config(module_dir: Path) -> None:\n    pyvenv_cfg = module_dir / \".venv\" / \"pyvenv.cfg\"\n    with open(pyvenv_cfg, \"r+\") as in_file:\n        text = in_file.read()\n        text = text.replace(r\"C:\\ProgramData\\checkmk\\agent\\modules\\python-3\", f\"{module_dir}\")\n    with open(pyvenv_cfg, \"w\") as out_file:\n        out_file.write(text)\n"}
{"type": "test_file", "path": "agents/wnx/tests/regression/__init__.py", "content": "#!/usr/bin/env python3\n# Copyright (C) 2021 Checkmk GmbH - License: GNU General Public License v2\n# This file is part of Checkmk (https://checkmk.com). It is subject to the terms and\n# conditions defined in the file COPYING, which is part of this source code package.\n"}
{"type": "test_file", "path": "agents/wnx/tests/regression/_test_section_logfiles.py", "content": "#!/usr/bin/env python3\n# Copyright (C) 2019 Checkmk GmbH - License: GNU General Public License v2\n# This file is part of Checkmk (https://checkmk.com). It is subject to the terms and\n# conditions defined in the file COPYING, which is part of this source code package.\n\nimport os\nimport platform\nimport re\nimport sys\n\nimport pytest\n\nfrom .local import local_test, run_subprocess, test_dir, user_dir\n\n\n# ugly hacks to get to know the make_ini_config param and utf encoding in use:\nclass Globals:\n    config_param_in_use = None\n    utf_encoding = None\n    section = \"logfiles\"\n    alone = True\n    testlog1 = os.path.join(test_dir, \"test1.log\")\n    testlog2 = os.path.join(test_dir, \"test2.log\")\n    testentry1 = \"foobar\"\n    testentry2 = \"error\"\n    state_pattern = re.compile(\n        r\"^(?P<logfile>[^\\|]+)\\|(?P<inode>\\d+)\\|(?P<size>\\d+)\\|(?P<offset>\\d+)\"\n    )\n    fileid_pattern = re.compile(r\"^.*\\:\\s*(?P<fileid>0x[0-9a-fA-F]+)\")\n\n\ndef get_log_state(line):\n    m = Globals.state_pattern.match(line)\n    if m is None:\n        return None, (None, None, None)\n    return m.group(\"logfile\"), (int(m.group(\"inode\")), int(m.group(\"size\")), int(m.group(\"offset\")))\n\n\n# stat.st_inode remains 0 on Windows -> need to hack the fileid with fsutil\ndef get_fileid(filename):\n    cmd = [\"fsutil\", \"file\", \"queryfileid\", filename]\n    exit_code, stdout, stderr = run_subprocess(cmd)\n    if stdout:\n        sys.stdout.write(stdout)\n    if stderr:\n        sys.stderr.write(stderr)\n    assert exit_code == 0, \"'%s' failed\" % \" \".join(cmd)\n    m = Globals.fileid_pattern.match(stdout)\n    assert m is not None, \"'%s' does not match fileid pattern\" % stdout\n    return int(m.group(\"fileid\"), base=16)\n\n\ndef get_file_state(filename):\n    stat = os.stat(filename)\n    # stat.st_inode remains 0 on Windows -> need to hack the fileid with fsutil\n    return get_fileid(filename), stat.st_size, stat.st_size\n\n\ndef logtitle(log):\n    return re.escape(r\"[[[%s]]]\" % log)\n\n\n@pytest.fixture(name=\"testfile\")\ndef testfile_engine():\n    return os.path.basename(__file__)\n\n\n@pytest.fixture(\n    name=\"testconfig\",\n    params=[\n        (\"default\", True),\n        (\"from_start\", True),\n        (\"rotated\", True),\n        (\"nocontext\", True),\n        (\"default\", False),\n    ],\n    ids=[\n        \"default_alone\",\n        \"from_start_alone\",\n        \"rotated_alone\",\n        \"nocontext_alone\",\n        \"default_with_systemtime\",\n    ],\n)\ndef fixture_testconfig(request, make_ini_config):\n    Globals.alone = request.param[1]\n    if Globals.alone:\n        make_ini_config.set(\"global\", \"sections\", Globals.section)\n    else:\n        make_ini_config.set(\"global\", \"sections\", \"%s systemtime\" % Globals.section)\n    make_ini_config.set(\"global\", \"crash_debug\", \"yes\")\n    make_ini_config.add_section(Globals.section)\n    Globals.config_param_in_use = request.param[0]\n    tag = \"\" if request.param[0] == \"default\" else \"%s \" % request.param[0]\n    make_ini_config.set(\n        Globals.section, \"textfile\", f\"{tag}{Globals.testlog1}|{tag}{Globals.testlog2}\"\n    )\n    return make_ini_config\n\n\n@pytest.fixture(\n    name=\"testconfig_glob\",\n    params=[\n        \"no_glob\",\n        \"star_begin\",\n        \"star_end\",\n        \"star_middle\",\n        \"question_begin\",\n        \"question_end\",\n        \"question_middle\",\n    ],\n)\ndef fixture_testconfig_glob(request, testconfig):\n    entry = {\n        \"no_glob\": Globals.testentry2,\n        \"star_begin\": \"*\" + Globals.testentry2[2:],\n        \"star_end\": Globals.testentry2[:3] + \"*\",\n        \"star_middle\": Globals.testentry2[:2] + \"*\" + Globals.testentry2[-1:],\n        \"question_begin\": \"?\" + Globals.testentry2[1:],\n        \"question_end\": Globals.testentry2[:-1] + \"?\",\n        \"question_middle\": Globals.testentry2[:2] + \"?\" + Globals.testentry2[3:],\n    }[request.param]\n    testconfig.set(Globals.section, \"crit\", entry)\n    return testconfig\n\n\n@pytest.fixture(name=\"expected_output_no_statefile\")\ndef fixture_expected_output_no_statefile():\n    expected_output = [re.escape(r\"<<<logwatch>>>\"), logtitle(Globals.testlog1)]\n    if Globals.config_param_in_use == \"from_start\":\n        expected_output += [r\"\\. %s\" % Globals.testentry1, r\"C %s\" % Globals.testentry2]\n    expected_output.append(logtitle(Globals.testlog2))\n    if Globals.config_param_in_use == \"from_start\":\n        expected_output += [r\"\\. %s\" % Globals.testentry1, r\"C %s\" % Globals.testentry2]\n    if not Globals.alone:\n        expected_output += [re.escape(r\"<<<systemtime>>>\"), r\"\\d+\"]\n    return expected_output\n\n\n@pytest.fixture(name=\"expected_output_with_statefile\")\ndef fixture_expected_output_with_statefile():\n    expected_output = [re.escape(r\"<<<logwatch>>>\"), logtitle(Globals.testlog1)]\n    if Globals.config_param_in_use != \"nocontext\":\n        expected_output.append(r\"\\. %s\" % Globals.testentry1)\n    expected_output += [r\"C %s\" % Globals.testentry2, logtitle(Globals.testlog2)]\n    if Globals.config_param_in_use != \"nocontext\":\n        expected_output.append(r\"\\. %s\" % Globals.testentry1)\n    expected_output.append(r\"C %s\" % Globals.testentry2)\n    if not Globals.alone:\n        expected_output += [re.escape(r\"<<<systemtime>>>\"), r\"\\d+\"]\n    return expected_output\n\n\n@pytest.fixture\ndef no_statefile():\n    if platform.system() == \"Windows\":\n        try:\n            os.unlink(os.path.join(user_dir, \"state\", \"logstate.txt\"))\n        except OSError:\n            # logstate.txt may not exist if this is the first test to be run\n            pass\n    yield\n\n\n@pytest.fixture\ndef with_statefile():\n    if platform.system() == \"Windows\":\n        # simulate new log entries by setting file size & offset\n        # to 0 (utf-8) or 2 (utf-16)\n        filesize = 2 if Globals.utf_encoding == \"utf-16\" else 0\n        with open(os.path.join(user_dir, \"state\", \"logstate.txt\"), \"w\") as statefile:\n            for logfile in [Globals.testlog1, Globals.testlog2]:\n                fileid = get_fileid(logfile)\n                file_state = [str(item) for item in [logfile, fileid, filesize, filesize]]\n                statefile.write(\"%s\\r\\n\" % \"|\".join(file_state))\n    yield\n\n\n@pytest.fixture(autouse=True)\ndef verify_logstate():\n    yield\n    if platform.system() == \"Windows\":\n        expected_logstate = {\n            logfile: get_file_state(logfile) for logfile in [Globals.testlog1, Globals.testlog2]\n        }\n        with open(os.path.join(user_dir, \"state\", \"logstate.txt\")) as statefile:\n            actual_logstate = dict(get_log_state(line) for line in statefile)\n        for (expected_log, expected_state), (actual_log, actual_state) in zip(\n            sorted(expected_logstate.items()), sorted(actual_logstate.items())\n        ):\n            assert expected_log == actual_log\n            assert expected_state[0] == actual_state[0], (\n                \"expected file id for log '%s' is %d but actual file id %d\"\n                % (\n                    expected_log,\n                    expected_state[0],\n                    actual_state[0],\n                )\n            )\n            assert expected_state[1] == actual_state[1], (\n                \"expected file size for log '%s' is %d but actual file size %d\"\n                % (\n                    expected_log,\n                    expected_state[1],\n                    actual_state[1],\n                )\n            )\n            assert expected_state[2] == actual_state[2], (\n                \"expected offset for log '%s' is %d but actual offset number %d\"\n                % (\n                    expected_log,\n                    expected_state[2],\n                    actual_state[2],\n                )\n            )\n\n\n@pytest.fixture(params=[\"utf-8\", \"utf-16\"], autouse=True)\ndef manage_logfiles(request):\n    Globals.utf_encoding = request.param\n    if platform.system() == \"Windows\":\n        for log in [Globals.testlog1, Globals.testlog2]:\n            with open(log, \"w\", encoding=request.param) as logfile:\n                for entry in [str(Globals.testentry1), str(Globals.testentry2)]:\n                    logfile.write(\"%s\\r\\n\" % entry)\n    yield\n    if platform.system() == \"Windows\":\n        for log in [Globals.testlog1, Globals.testlog2]:\n            os.unlink(log)\n\n\n@pytest.mark.usefixtures(\"no_statefile\")\ndef test_section_logfiles__new_file(\n    request, testconfig_glob, expected_output_no_statefile, actual_output, testfile\n):\n    # request.node.name gives test name\n    local_test(expected_output_no_statefile, actual_output, testfile, request.node.name)\n\n\n@pytest.mark.usefixtures(\"with_statefile\")\ndef test_section_logfiles__new_entries_in_log(\n    request, testconfig_glob, expected_output_with_statefile, actual_output, testfile\n):\n    # request.node.name gives test name\n    local_test(expected_output_with_statefile, actual_output, testfile, request.node.name)\n"}
{"type": "test_file", "path": "agents/wnx/tests/regression/conftest.py", "content": "#!/usr/bin/env python3\n# Copyright (C) 2019 Checkmk GmbH - License: GNU General Public License v2\n# This file is part of Checkmk (https://checkmk.com). It is subject to the terms and\n# conditions defined in the file COPYING, which is part of this source code package.\n\nimport asyncio\nimport subprocess\nimport time\n\nimport pytest\nimport telnetlib3  # type: ignore[import-untyped]\nimport yaml\n\nfrom .local import DEFAULT_CONFIG, host, main_exe, port, run_agent, user_yaml_config\n\n\n@pytest.fixture\ndef make_yaml_config():\n    yml = yaml.safe_load(DEFAULT_CONFIG.format(port))\n    return yml\n\n\n@pytest.fixture(name=\"write_config\")\ndef write_config_engine(testconfig):\n    with open(user_yaml_config, \"w\") as yaml_file:\n        ret = yaml.dump(testconfig)\n        yaml_file.write(ret)\n    yield\n\n\n# Override this in test file(s) to insert a wait before contacting the agent.\n@pytest.fixture(name=\"wait_agent\")\ndef wait_agent_engine():\n    def inner():\n        return False\n\n    return inner\n\n\n_result = \"\"\n\n\nasync def _telnet_shell(reader: telnetlib3.TelnetReader, _: telnetlib3.TelnetWriter) -> None:\n    global _result\n    while True:\n        data = await reader.read(1024)\n        if not data:\n            break\n        _result += data\n\n\ndef _read_client_data(addr_host: str, addr_port: int) -> None:\n    loop = asyncio.get_event_loop()\n    coro = telnetlib3.open_connection(addr_host, addr_port, shell=_telnet_shell)\n    _, writer = loop.run_until_complete(coro)\n    loop.run_until_complete(writer.protocol.waiter_closed)\n\n\ndef _get_data_using_telnet(addr_host: str, addr_port: int) -> str:\n    # overloaded CI Node may delay start/init of the agent process\n    # we must retry connection few times to avoid complaints\n    global _result\n    _result = \"\"\n    for _ in range(5):\n        try:\n            _read_client_data(addr_host, addr_port)\n            if _result:\n                return _result\n            time.sleep(2)\n        except Exception:\n            # print('No connect, waiting for agent')\n            time.sleep(2)\n\n    return \"\"\n\n\n@pytest.fixture(name=\"actual_output\")\ndef actual_output_engine(write_config, wait_agent):\n    # Run agent and yield telnet output.\n    p = None\n    try:\n        p = run_agent(main_exe)\n        # Override wait_agent in tests to wait for async processes to start.\n        wait_agent()\n\n        yield _get_data_using_telnet(host, port).splitlines()\n    finally:\n        if p is not None:\n            p.terminate()\n\n            # hammer kill of the process, terminate may be too long\n            subprocess.call(\n                f'taskkill /F /FI \"pid eq {p.pid}\" /FI \"IMAGENAME eq check_mk_agent.exe\"'\n            )\n\n        # Possibly wait for async processes to stop.\n        wait_agent()\n"}
{"type": "test_file", "path": "agents/wnx/tests/regression/it_utils.py", "content": "#!/usr/bin/env python3\n# Copyright (C) 2019 Checkmk GmbH - License: GNU General Public License v2\n# This file is part of Checkmk (https://checkmk.com). It is subject to the terms and\n# conditions defined in the file COPYING, which is part of this source code package.\n\nimport os\nimport platform\nimport subprocess\nfrom collections.abc import Sequence\n\nimport pytest\n\n\ndef check_actual_input(name: str, lines: int, alone: bool, data: Sequence[str] | None) -> bool:\n    if data is None:\n        pytest.skip(f\"Section '{name}': Data is absent\")\n        return False\n\n    if not alone:\n        lines += 2\n\n    if len(data) < lines:\n        all_data = \"\\n\".join(data)\n        pytest.skip(f\"Section '{name}': Data is TOO short:\\n{all_data}\\n\")\n        return False\n\n    return True\n\n\ndef safe_binary_remove(binary_path):\n    try:\n        os.unlink(binary_path)\n    except OSError as os_error:\n        print(\"Error %s during file delete\" % os_error.errno)\n\n\ndef stop_ohm():\n    # stopping all\n    subprocess.call(\"taskkill /F /IM OpenhardwareMonitorCLI.exe\")\n    subprocess.call(\"net stop winring0_1_2_0\")\n\n\ndef remove_files(target_dir, binaries):\n    # removing all\n    for f in binaries:\n        safe_binary_remove(os.path.join(target_dir, f))\n\n\ndef make_dir(directory):\n    if not os.path.exists(directory):\n        os.mkdir(directory)\n\n\ndef check_os():\n    return platform.system() == \"Windows\"\n"}
{"type": "test_file", "path": "agents/wnx/tests/regression/local.py", "content": "#!/usr/bin/env python3\n# Copyright (C) 2019 Checkmk GmbH - License: GNU General Public License v2\n# This file is part of Checkmk (https://checkmk.com). It is subject to the terms and\n# conditions defined in the file COPYING, which is part of this source code package.\n\n\nimport os\nimport platform\nimport re\nimport subprocess\nimport sys\nfrom pathlib import Path\n\nimport yaml\n\nfrom . import it_utils\n\nDEFAULT_CONFIG = \"\"\"\nglobal:\n  enabled: true\n  logging:\n    debug: true\n  port: {}\n\"\"\"\n\nif not it_utils.check_os():\n    print(f\"Unsupported platform {platform.system()}\")\n    sys.exit(13)\n\n\ndef get_main_yaml_name(base_dir):\n    return os.path.join(base_dir, \"check_mk.yml\")\n\n\ndef get_user_yaml_name(base_dir):\n    return os.path.join(\n        base_dir,\n    )\n\n\ndef get_main_plugins_name(base_dir):\n    return os.path.join(base_dir, \"plugins\")\n\n\ndef create_protocol_file(on_dir):\n    # block  upgrading\n    protocol_dir = on_dir / \"config\"\n    try:\n        os.makedirs(protocol_dir)\n    except OSError as e:\n        print(f\"Probably folders exist: {e}\")\n\n    if not protocol_dir.exists():\n        print(f\"Directory {protocol_dir} doesnt exist, may be you have not enough rights\")\n        sys.exit(11)\n\n    protocol_file = protocol_dir / \"upgrade.protocol\"\n    with open(protocol_file, \"w\") as f:\n        f.write(\"Upgraded:\\n   time: '2019-05-20 18:21:53.164\")\n\n\ndef _get_path_from_env(env: str) -> Path:\n    env_value = os.getenv(env)\n    assert env_value is not None\n    return Path(env_value)\n\n\nport = 29998\nhost = \"localhost\"\nEXE_ENV_VAR = \"WNX_REGRESSION_BASE_DIR\"\nARTE_ENV_VAR = \"arte\"\n\nartifacts_dir = _get_path_from_env(ARTE_ENV_VAR)\ntest_dir = _get_path_from_env(EXE_ENV_VAR)\nif not artifacts_dir.exists():\n    print(f\"Artifacts Directory {artifacts_dir} doesnt exist\")\n    sys.exit(11)\n\nif not test_dir.exists():\n    print(f\"Test Directory {test_dir} doesnt exist\")\n    sys.exit(12)\n\n# root dir\nroot_dir = test_dir / \"test\" / \"root\"\nuser_dir = test_dir / \"test\" / \"data\"\n\ncreate_protocol_file(on_dir=user_dir)\n\n# names\nuser_yaml_config = user_dir / \"check_mk.user.yml\"\nmain_exe = test_dir / \"check_mk_agent.exe\"\n\n\n# environment variable set\ndef run_subprocess(cmd):\n    sys.stderr.write(\" \".join(str(cmd)) + \"\\n\")\n    with subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE) as p:\n        stdout, stderr = p.communicate(timeout=10)\n        return p.returncode, stdout, stderr\n\n\ndef run_agent(cmd):\n    return subprocess.Popen([cmd, \"exec\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n\n\ndef assert_subprocess(cmd):\n    exit_code, stdout_ret, stderr_ret = run_subprocess(cmd)\n\n    if stdout_ret:\n        sys.stdout.write(stdout_ret.decode(encoding=\"cp1252\"))\n\n    if stderr_ret:\n        sys.stderr.write(stderr_ret.decode(encoding=\"cp1252\"))\n\n    assert exit_code == 0, \"'%s' failed\" % \" \".join(cmd)\n\n\nclass DuplicateSectionError(Exception):\n    \"\"\"Raised when a section is multiply-created.\"\"\"\n\n    def __init__(self, section) -> None:  # type: ignore[no-untyped-def]\n        super().__init__(self, \"Section %r already exists\" % section)\n\n\nclass NoSectionError(Exception):\n    \"\"\"Raised when no section matches a requested option.\"\"\"\n\n    def __init__(self, section) -> None:  # type: ignore[no-untyped-def]\n        super().__init__(self, \"No section: %r\" % section)\n\n\nclass YamlWriter:\n    def __init__(self) -> None:\n        self._doc = None\n\n    def load_document(self, doc):\n        self._doc = yaml.safe_load(doc)\n\n\ndef local_test(\n    expected_output_from_agent,\n    actual_output_from_agent,\n    current_test,\n    test_name=None,\n    test_class=None,\n):\n    comparison_data = list(zip(expected_output_from_agent, actual_output_from_agent))\n    for expected, actual in comparison_data:\n        # Uncomment for debug prints:\n        # if re.match(expected, actual) is None:\n        #    print(\"ups: %s\" % actual)\n        #    print( 'DEBUG: actual output\\r\\n', '\\r\\n'.join(actual))\n        #    print('DEBUG: expected output\\r\\n', '\\r\\n'.join(expected))\n        # print(\"EXPECTED: %r\\n ACTUAL  : %r\\n\" % (expected, actual))\n\n        assert expected == actual or re.match(expected, actual) is not None, (\n            f\"\\nExpected '{expected!r}'\\nActual   '{actual!r}'\"\n        )\n    try:\n        assert len(actual_output_from_agent) >= len(expected_output_from_agent), (\n            \"actual output is shorter than expected:\\n\"\n            \"expected output:\\n%s\\nactual output:\\n%s\"\n            % (\"\\n\".join(expected_output_from_agent), \"\\n\".join(actual_output_from_agent))\n        )\n        assert len(actual_output_from_agent) <= len(expected_output_from_agent), (\n            \"actual output is longer than expected:\\n\"\n            \"expected output:\\n%s\\nactual output:\\n%s\"\n            % (\"\\n\".join(expected_output_from_agent), \"\\n\".join(actual_output_from_agent))\n        )\n    except TypeError:\n        # expected_output may be an iterator without len\n        assert len(actual_output_from_agent) > 0, \"Actual output was empty\"\n"}
{"type": "test_file", "path": "agents/wnx/tests/regression/test_section_check_mk.py", "content": "#!/usr/bin/env python3\n# Copyright (C) 2019 Checkmk GmbH - License: GNU General Public License v2\n# This file is part of Checkmk (https://checkmk.com). It is subject to the terms and\n# conditions defined in the file COPYING, which is part of this source code package.\n\nimport os\nimport re\n\nimport pytest\n\nfrom .local import get_main_yaml_name, get_user_yaml_name, local_test, root_dir, user_dir\n\n\nclass Globals:\n    section = \"check_mk\"\n    alone = True\n    output_file = \"agentoutput.txt\"\n    only_from: str | None = None\n    ipv4_to_ipv6 = {\"127.0.0.1\": \"0:0:0:0:0:ffff:7f00:1\", \"10.1.2.3\": \"0:0:0:0:0:ffff:a01:203\"}\n\n\n@pytest.fixture(name=\"testfile\")\ndef testfile_engine():\n    return os.path.basename(__file__)\n\n\n@pytest.fixture(name=\"testconfig\", params=[\"alone\", \"with_systemtime\"])\ndef testconfig_engine(request, make_yaml_config):\n    Globals.alone = request.param == \"alone\"\n    if Globals.alone:\n        make_yaml_config[\"global\"][\"sections\"] = Globals.section\n    else:\n        make_yaml_config[\"global\"][\"sections\"] = [Globals.section, \"systemtime\"]\n    return make_yaml_config\n\n\n@pytest.fixture(name=\"testconfig_host\")\ndef testconfig_host_engine(testconfig):\n    return testconfig\n\n\n@pytest.fixture(\n    name=\"testconfig_only_from\",\n    params=[None, \"127.0.0.1 10.1.2.3\"],\n    ids=[\"only_from=None\", \"only_from=127.0.0.1_10.1.2.3\"],\n)\ndef testconfig_only_from_engine(request, testconfig_host):\n    Globals.only_from = request.param\n    if request.param:\n        testconfig_host[\"global\"][\"only_from\"] = [\"127.0.0.1\", \"10.1.2.3\"]\n    else:\n        testconfig_host[\"global\"][\"only_from\"] = None\n    return testconfig_host\n\n\n# live example of valid output\n_EXAMPLE = \"\"\"\n<<<check_mk>>>\nVersion: 2.3.0-2024.03.15\nBuildDate: Mar 15 2024\nAgentOS: windows\nHostname: klapp-0336\nArchitecture: 64bit\nOSName: Microsoft Windows 10 Pro\nOSVersion: 10.0.19045\nOSType: windows\nTime: 2024-03-19T13:42:18+0100\nWorkingDirectory: c:\\\\dev\\\\shared\nConfigFile: c:\\\\dev\\\\shared\\\\check_mk.yml\nLocalConfigFile: C:\\\\ProgramData\\\\checkmk\\\\agent\\\\check_mk.user.yml\nAgentDirectory: c:\\\\dev\\\\shared\nPluginsDirectory: C:\\\\ProgramData\\\\checkmk\\\\agent\\\\plugins\nStateDirectory: C:\\\\ProgramData\\\\checkmk\\\\agent\\\\state\nConfigDirectory: C:\\\\ProgramData\\\\checkmk\\\\agent\\\\config\nTempDirectory: C:\\\\ProgramData\\\\checkmk\\\\agent\\\\tmp\nLogDirectory: C:\\\\ProgramData\\\\checkmk\\\\agent\\\\log\nSpoolDirectory: C:\\\\ProgramData\\\\checkmk\\\\agent\\\\spool\nLocalDirectory: C:\\\\ProgramData\\\\checkmk\\\\agent\\\\local\nOnlyFrom: 0.0.0.0/0\n<<<cmk_agent_ctl_status:sep(0)>>>:\n\"\"\"\n\n\ndef make_only_from_array(ipv4):\n    if ipv4 is None:\n        return None\n\n    addr_list = []\n\n    # not very pythonic, but other methods(reduce) overkill\n    for x in ipv4:\n        addr_list.append(x)\n        # addr_list.append(Globals.ipv4_to_ipv6[x])\n\n    return addr_list\n\n\n@pytest.fixture(name=\"expected_output\")\ndef expected_output_engine():\n    ipv4 = Globals.only_from.split() if Globals.only_from is not None else None\n    expected = [\n        r\"<<<%s>>>\" % Globals.section,\n        r\"Version: \\d+\\.\\d+\\.\\d+([bi]\\d+)?(p\\d+)?\",\n        r\"BuildDate: [A-Z][a-z]{2} (\\d{2}| \\d) \\d{4}\",\n        r\"AgentOS: windows\",\n        r\"Hostname: .+\",\n        r\"Architecture: \\d{2}bit\",\n        r\"OSName: Microsoft .+\",\n        r\"OSVersion: 10.+\",\n        r\"OSType: windows\",\n        r\"Time: 20\\d\\d-\\d\\d-\\d\\dT\\d\\d:\\d\\d:\\d\\d[\\+,-]\\d\\d\\d\\d\",\n        r\"WorkingDirectory: %s\" % (re.escape(os.getcwd())),\n        r\"ConfigFile: %s\" % (re.escape(get_main_yaml_name(root_dir))),\n        r\"LocalConfigFile: %s\" % (re.escape(get_user_yaml_name(user_dir))),\n        r\"AgentDirectory: %s\" % (re.escape(str(root_dir))),\n        r\"PluginsDirectory: %s\" % (re.escape(os.path.join(user_dir, \"plugins\"))),\n        r\"StateDirectory: %s\" % (re.escape(os.path.join(user_dir, \"state\"))),\n        r\"ConfigDirectory: %s\" % (re.escape(os.path.join(user_dir, \"config\"))),\n        r\"TempDirectory: %s\" % (re.escape(os.path.join(user_dir, \"tmp\"))),\n        r\"LogDirectory: %s\" % (re.escape(os.path.join(user_dir, \"log\"))),\n        r\"SpoolDirectory: %s\" % (re.escape(os.path.join(user_dir, \"spool\"))),\n        r\"LocalDirectory: %s\" % (re.escape(os.path.join(user_dir, \"local\"))),\n        (\n            r\"OnlyFrom: %s %s\" % tuple(make_only_from_array(ipv4))\n            if Globals.only_from\n            else r\"OnlyFrom: \"\n        ),\n        r\"<<<cmk_agent_ctl_status:sep(0)>>>\",\n    ]\n    if not Globals.alone:\n        expected += [re.escape(r\"<<<systemtime>>>\"), r\"\\d+\"]\n    return expected\n\n\ndef test_section_check_mk(  # type: ignore[no-untyped-def]\n    request, testconfig_only_from, expected_output, actual_output, testfile\n) -> None:\n    # request.node.name gives test name\n    local_test(expected_output, actual_output, testfile, request.node.name)\n"}
{"type": "test_file", "path": "agents/wnx/tests/regression/test_agent_start_parameters.py", "content": "#!/usr/bin/env python3\n# Copyright (C) 2019 Checkmk GmbH - License: GNU General Public License v2\n# This file is part of Checkmk (https://checkmk.com). It is subject to the terms and\n# conditions defined in the file COPYING, which is part of this source code package.\n\nimport os\nimport platform\nimport sys\n\nimport pytest\n\nfrom .local import local_test, main_exe, run_subprocess\n\n\nclass Globals:\n    param: list[str] = []\n\n\n@pytest.fixture(name=\"testfile\")\ndef testfile_engine():\n    return os.path.basename(__file__)\n\n\n@pytest.fixture(name=\"testconfig\")\ndef testconfig_engine(make_yaml_config):\n    if Globals.param[0] == \"showconfig\":\n        make_yaml_config[\"zzz\"] = {\n            \"enabled\": \"yes\",\n            \"timeout\": 60,\n        }\n        make_yaml_config[\"_xxx\"] = {\n            \"enabled\": \"yes\",\n            \"timeout\": 60,\n        }\n    return make_yaml_config\n\n\n@pytest.fixture(name=\"actual_output\")\ndef actual_output_engine(request, write_config):\n    if platform.system() == \"Windows\":\n        # Run agent and yield its output.\n        try:\n            exit_code, _stdout, _stderr = run_subprocess([str(main_exe)] + Globals.param)\n            if _stdout:\n                sys.stdout.write(_stdout.decode(encoding=\"cp1252\"))\n            if _stderr:\n                sys.stderr.write(_stderr.decode(encoding=\"cp1252\"))\n            expected_code = 13 if Globals.param[0] == \"bad\" else 0\n            assert expected_code == exit_code\n            # Usage is written to stderr, actual cmd output to stdout.\n            work_load = _stdout.decode(encoding=\"cp1252\")\n            yield work_load.splitlines()\n        finally:\n            pass\n    else:\n        # Not on Windows, test run remotely, nothing to be done.\n        yield\n\n\ndef output_usage():\n    return [\n        r\"Normal Usage:\",\n        r\"\\.?\",\n    ]\n\n\ndef output_bad_usage():\n    r = output_usage()\n    r.insert(0, r'Provided Parameter \"bad\" is not allowed')\n    return r\n\n\n@pytest.fixture(name=\"expected_output\")\ndef expected_output_engine(request, testconfig):\n    return {\n        \"version\": [r\"Check_MK Agent version \\d+\\.\\d+\\.\\d+([bi]\\d+)?(p\\d+)?\"],\n        \"showconfig\": [\n            r\"# Environment Variables:\",\n            r\"# MK_LOCALDIR=\\.?\",\n            r\"# MK_STATEDIR=\\.?\",\n            r\"# MK_PLUGINSDIR=\\.?\",\n            r\"# MK_TEMPDIR=\\.?\",\n            r\"# MK_LOGDIR=\\.?\",\n            r\"# MK_CONFDIR=\\.?\",\n            r\"# MK_SPOOLDIR=\\.?\",\n            r\"# MK_INSTALLDIR=\\.?\",\n            r\"# MK_MSI_PATH=\\.?\",\n            r\"# MK_MODULESDIR=\\.?\",\n            r\"# Loaded Config Files:\",\n            r\"# system: \\.?\",\n            r\"# bakery: \\.?\",\n            r\"# user  : \\.?\",\n            r\"\\.?\",\n            r\"\\.?\",\n            r\"\\.?\",\n            r\"\\.?\",\n            r\"\\.?\",\n            r\"\\.?\",\n            r\"\\.?\",\n            r\"\\.?\",\n            r\"\\.?\",\n            r\"\\.?\",\n            r\"\\.?\",\n            r\"\\.?\",\n            r\"\\.?\",\n            r\"\\.?\",\n            r\"\\.?\",\n            r\"\\.?\",\n            r\"\\.?\",\n            r\"\\.?\",\n            r\"\\.?\",\n            r\"\\.?\",\n            r\"\\.?\",\n            r\"\\.?\",\n            r\"\\.?\",\n            r\"\\.?\",\n            r\"\\.?\",\n            r\"\\.?\",\n            r\"\\.?\",\n            r\"\\.?\",\n            r\"\\.?\",\n            r\"\\.?\",\n            r\"\\.?\",\n            r\"\\.?\",\n            r\"\\.?\",\n            r\"\\.?\",\n            r\"\\.?\",\n            r\"\\.?\",\n        ],\n        \"help\": output_usage(),\n        \"bad\": output_bad_usage(),\n    }[Globals.param[0]]\n\n\n@pytest.fixture(\n    # Note: param 'adhoc' is tested in all section tests\n    #       params 'test' and 'debug' are tested in section check_mk tests\n    params=[[\"version\"], [\"showconfig\"], [\"help\"], [\"bad\"]],\n    ids=[\"version\", \"showconfig\", \"help\", \"bad\"],\n    autouse=True,\n)\ndef pre_test(request):\n    Globals.param = request.param\n    yield\n\n\n@pytest.fixture(autouse=True)\ndef post_test():\n    yield\n\n\ndef test_agent_start_parameters(  # type: ignore[no-untyped-def]\n    request, testconfig, expected_output, actual_output, testfile\n) -> None:\n    # request.node.name gives test name\n    expected_work = expected_output\n    if len(expected_work) < len(actual_output):\n        missing = len(actual_output) - len(expected_work)\n        expected_work.extend([r\"\\.?\" for i in range(missing)])\n    local_test(expected_output, actual_output, testfile, request.node.name)\n"}
{"type": "test_file", "path": "agents/wnx/tests/regression/test_section_eventlog.py", "content": "#!/usr/bin/env python3\n# Copyright (C) 2019 Checkmk GmbH - License: GNU General Public License v2\n# This file is part of Checkmk (https://checkmk.com). It is subject to the terms and\n# conditions defined in the file COPYING, which is part of this source code package.\n\nimport contextlib\nimport math\nimport os\nimport platform\nimport re\nimport winreg\nfrom itertools import chain, repeat\n\nimport pytest\nimport win32evtlog  # type: ignore[import-untyped]\n\nfrom .local import assert_subprocess, host, local_test, user_dir\n\n\nclass Globals:\n    local_statefile = \"eventstate.txt\"\n    state_pattern = re.compile(r\"^(?P<logtype>[^\\|]+)\\|(?P<record>\\d+)$\")\n    section = \"logwatch\"\n    alone = True\n    statedir = os.path.join(user_dir, \"state\")\n    statefile = \"eventstate_127_0_0_1.txt\"  # local test uses ipv4\n    testlog = \"Application\"\n    testsource = \"Test source\"\n    testeventtype = \"Warning\"\n    testdescription = \"Something might happen!\"\n    tolerance = 10\n    testids = [1, 2, 3]\n\n\ndef generate_logs():\n    if platform.system() == \"Windows\":\n        with winreg.OpenKey(  # type: ignore[attr-defined]\n            winreg.HKEY_LOCAL_MACHINE,  # type: ignore[attr-defined]\n            \"SYSTEM\\\\CurrentControlSet\\\\Services\\\\Eventlog\",\n        ) as key:\n            index = 0\n            while True:\n                try:\n                    yield winreg.EnumKey(key, index)  # type: ignore[attr-defined]\n                    index += 1\n\n                except OSError:\n                    break\n\n\n# Windows SSH agent and COM spoil tests by omitting occasional events to\n# Security and System logs. Ignore those logs as they are too unstable during a\n# test run.\nlogs = list(l for l in generate_logs() if l not in [\"Security\", \"System\"])\n\n\n@contextlib.contextmanager\ndef eventlog(logtype):\n    handle = win32evtlog.OpenEventLog(host, logtype)\n    try:\n        yield handle\n    finally:\n        win32evtlog.CloseEventLog(handle)\n\n\ndef get_last_record(logtype):\n    try:\n        with eventlog(logtype) as log_handle:\n            oldest = win32evtlog.GetOldestEventLogRecord(log_handle)\n            total = win32evtlog.GetNumberOfEventLogRecords(log_handle)\n            result = oldest + total - 1\n            return result if result >= 0 else 0\n    except Exception:\n        return 0\n\n\ndef get_log_state(line):\n    m = Globals.state_pattern.match(line)\n    if m is None:\n        return None, None\n    return m.group(\"logtype\"), int(m.group(\"record\"))\n\n\ndef logtitle(log):\n    return re.escape(r\"[[[\") + log + re.escape(r\"]]]\")\n\n\ndef create_event(eventid):\n    if platform.system() == \"Windows\":\n        cmd = [\n            \"eventcreate.exe\",\n            \"/l\",\n            Globals.testlog,\n            \"/t\",\n            Globals.testeventtype,\n            \"/so\",\n            Globals.testsource,\n            \"/id\",\n            \"%d\" % eventid,\n            \"/d\",\n            Globals.testdescription,\n        ]\n        assert_subprocess(cmd)\n\n\n@pytest.fixture\ndef create_events():\n    for i in Globals.testids:\n        create_event(i)\n\n\n@pytest.fixture(name=\"testfile\")\ndef testfile_engine():\n    return os.path.basename(__file__)\n\n\ndef setup_section(config, section, alone):\n    config[\"global\"][\"sections\"] = section if alone else [section, \"systemtime\"]\n    return config\n\n\n@pytest.fixture(name=\"testconfig_sections\", params=[\"alone\", \"with_systemtime\"])\ndef testconfig_sections_engine(request, make_yaml_config):\n    Globals.alone = request.param == \"alone\"\n    return setup_section(make_yaml_config, Globals.section, Globals.alone)\n\n\n@pytest.fixture(name=\"testconfig\", params=[\"yes\", \"no\"], ids=[\"vista_api=yes\", \"vista_api=no\"])\ndef testconfig_engine(request, testconfig_sections):\n    log_files = [{Globals.testlog: \"warn\"}, {\"Security\": \"off\"}, {\"System\": \"off\"}, {\"*\": \"off\"}]\n    testconfig_sections[Globals.section] = {\"vista_api\": request.param, \"logfile\": log_files}\n\n    return testconfig_sections\n\n\n@pytest.fixture(name=\"expected_output_no_events\")\ndef expected_output_no_events_engine():\n    if platform.system() != \"Windows\":\n        return None\n\n    expected = [re.escape(r\"<<<%s>>>\" % Globals.section), re.escape(r\"[[[Application]]]\")]\n    if not Globals.alone:\n        expected += [re.escape(r\"<<<systemtime>>>\"), r\"\\d+\"]\n    return expected\n\n\n@pytest.fixture(name=\"expected_output_application_events\")\ndef expected_output_application_events_engine():\n    if platform.system() != \"Windows\":\n        return None\n\n    split_index = logs.index(\"Application\") + 1\n    re_str = r\"|\".join(\n        [logtitle(l) for l in logs[split_index:]]\n        + [r\"[CWOu\\.] \\w{3} \\d{2} \\d{2}\\:\\d{2}:\\d{2} \\d+\\.\\d+ .+ .+\"]\n    )\n    if not Globals.alone:\n        re_str += r\"|\" + re.escape(r\"<<<systemtime>>>\") + r\"|\\d+\"\n    return chain(\n        [re.escape(r\"<<<%s>>>\" % Globals.section)],\n        [logtitle(l) for l in logs[:split_index]],\n        [\n            r\"W \\w{3} \\d{2} \\d{2}\\:\\d{2}:\\d{2} 0\\.%d %s %s\"\n            % (i, Globals.testsource.replace(\" \", \"_\"), Globals.testdescription)\n            for i in Globals.testids\n        ],\n        repeat(re_str),\n    )\n\n\ndef last_records():\n    if platform.system() != \"Windows\":\n        return None\n    return {logtype: get_last_record(logtype) for logtype in [\"Application\"]}\n\n\n@pytest.fixture\ndef no_statefile():\n    if platform.system() == \"Windows\":\n        try:\n            os.unlink(os.path.join(Globals.statedir, \"eventstate.txt\"))\n        except OSError:\n            # eventstate.txt may not exist if this is the first test to be run\n            pass\n    yield\n\n\n@pytest.fixture(params=[Globals.local_statefile, Globals.statefile])\ndef with_statefile(request):\n    if platform.system() == \"Windows\":\n        try:\n            os.mkdir(Globals.statedir)\n        except OSError:\n            pass  # Directory may already exist.\n        with open(os.path.join(Globals.statedir, request.param), \"w\") as statefile:\n            eventstate = {logtype: get_last_record(logtype) for logtype in logs}\n            for logtype, state in eventstate.items():\n                statefile.write(f\"{logtype}|{state}\\r\\n\")\n    yield\n\n\n@pytest.fixture(autouse=True)\ndef verify_eventstate():\n    yield\n    if platform.system() == \"Windows\":\n        expected_eventstate = last_records()\n        with open(os.path.join(Globals.statedir, Globals.statefile)) as statefile:\n            actual_eventstate = dict(get_log_state(line) for line in statefile)\n        for (expected_log, expected_state), (actual_log, actual_state) in zip(\n            sorted(expected_eventstate.items()), sorted(actual_eventstate.items())\n        ):\n            assert expected_log == actual_log\n            state_tolerance = 0 if expected_log == Globals.testlog else Globals.tolerance\n            assert math.fabs(expected_state - actual_state) <= state_tolerance, (\n                \"expected state for log '%s' is %d, actual state %d, \"\n                \"state_tolerance %d\"\n                % (\n                    expected_log,\n                    expected_state,\n                    actual_state,\n                    state_tolerance,\n                )\n            )\n\n\n# disabled tests\n@pytest.mark.usefixtures(\"no_statefile\")\ndef test_section_eventlog__no_statefile__no_events(\n    request, testconfig, expected_output_no_events, actual_output, testfile\n):\n    # request.node.name gives test name\n    local_test(expected_output_no_events, actual_output, testfile, request.node.name)\n\n\n@pytest.mark.usefixtures(\"with_statefile\", \"create_events\")\ndef test_section_eventlog__application_warnings(\n    request, testconfig, expected_output_application_events, actual_output, testfile\n):\n    # request.node.name gives test name\n    local_test(expected_output_application_events, actual_output, testfile, request.node.name)\n"}
{"type": "test_file", "path": "agents/wnx/tests/regression/test_section_df.py", "content": "#!/usr/bin/env python3\n# Copyright (C) 2019 Checkmk GmbH - License: GNU General Public License v2\n# This file is part of Checkmk (https://checkmk.com). It is subject to the terms and\n# conditions defined in the file COPYING, which is part of this source code package.\n\nimport os\nimport re\n\nimport pytest\n\nfrom .local import local_test\n\n\nclass Globals:\n    section = \"df\"\n    alone = True\n\n\n@pytest.fixture(name=\"testfile\")\ndef testfile_engine():\n    return os.path.basename(__file__)\n\n\n@pytest.fixture(name=\"testconfig\", params=[\"alone\", \"with_systemtime\"])\ndef testconfig_engine(request, make_yaml_config):\n    Globals.alone = request.param == \"alone\"\n    if Globals.alone:\n        make_yaml_config[\"global\"][\"sections\"] = Globals.section\n    else:\n        make_yaml_config[\"global\"][\"sections\"] = [Globals.section, \"systemtime\"]\n    return make_yaml_config\n\n\n@pytest.fixture(name=\"expected_output\")\ndef expected_output_engine():\n    drive = rf\"[A-Z]:{re.escape(os.sep)}\"\n    expected = [\n        re.escape(rf\"<<<{Globals.section}:sep(9)>>>\"),\n        rf\"({drive}.*|\\w+)\\t\\w*\\t\\d+\\t\\d+\\t\\d+\\t\\d{{1,3}}%\\t{drive}\",\n    ]\n    if not Globals.alone:\n        expected += [re.escape(r\"<<<systemtime>>>\"), r\"\\d+\"]\n    return expected\n\n\ndef test_section_df(  # type: ignore[no-untyped-def]\n    request, testconfig, expected_output, actual_output, testfile\n) -> None:\n    # request.node.name gives test name\n    result = actual_output\n    actual_output_len = len(result)\n    expected_output_len = len(expected_output)\n\n    # if we have length mismatch we have to extend expected output\n    # we will replicate expected strings depending from length mismatching\n    # the method is not elegant, but absolutely correct\n    for _ in range(expected_output_len, actual_output_len):\n        expected_output.insert(1, expected_output[1])  # [h][1][f] ->[h][1][1][f] -> ...\n\n    local_test(expected_output, actual_output, testfile, request.node.name)\n"}
{"type": "test_file", "path": "agents/wnx/tests/regression/test_section_dotnet_clrmemory.py", "content": "#!/usr/bin/env python3\n# Copyright (C) 2019 Checkmk GmbH - License: GNU General Public License v2\n# This file is part of Checkmk (https://checkmk.com). It is subject to the terms and\n# conditions defined in the file COPYING, which is part of this source code package.\n\nimport os\nimport re\nfrom itertools import chain, repeat\n\nimport pytest\n\nfrom . import it_utils\nfrom .local import local_test\n\n\nclass Globals:\n    section = \"dotnet_clrmemory\"\n    alone = True\n\n\n@pytest.fixture(name=\"testfile\")\ndef testfile_engine():\n    return os.path.basename(__file__)\n\n\n@pytest.fixture(name=\"testconfig\", params=[\"alone\", \"with_systemtime\"])\ndef testconfig_engine(request, make_yaml_config):\n    Globals.alone = request.param == \"alone\"\n    if Globals.alone:\n        make_yaml_config[\"global\"][\"sections\"] = Globals.section\n    else:\n        make_yaml_config[\"global\"][\"sections\"] = [Globals.section, \"systemtime\"]\n    return make_yaml_config\n\n\n@pytest.fixture(name=\"expected_output\")\ndef expected_output_engine():\n    base = [\n        re.escape(r\"<<<%s:sep(124)>>>\" % Globals.section),\n        (\n            r\"AllocatedBytesPersec,Caption,Description,FinalizationSurvivors,\"\n            r\"Frequency_Object,Frequency_PerfTime,Frequency_Sys100NS,Gen0heapsize,\"\n            r\"Gen0PromotedBytesPerSec,Gen1heapsize,Gen1PromotedBytesPerSec,\"\n            r\"Gen2heapsize,LargeObjectHeapsize,Name,NumberBytesinallHeaps,\"\n            r\"NumberGCHandles,NumberGen0Collections,NumberGen1Collections,\"\n            r\"NumberGen2Collections,NumberInducedGC,NumberofPinnedObjects,\"\n            r\"NumberofSinkBlocksinuse,NumberTotalcommittedBytes,\"\n            r\"NumberTotalreservedBytes,PercentTimeinGC,PercentTimeinGC_Base,\"\n            r\"ProcessID,PromotedFinalizationMemoryfromGen0,PromotedMemoryfromGen0,\"\n            r\"PromotedMemoryfromGen1,Timestamp_Object,Timestamp_PerfTime,\"\n            r\"Timestamp_Sys100NS\"\n        ).replace(\",\", \"\\\\|\"),\n    ]\n    re_str = (\n        r\"\\d+,,,\\d+,\\d+,\\d+,\\d+,\\d+,\\d+,\\d+,\\d+,\\d+,\\d+,\"\n        r\"[\\w\\#\\.]+,\\d+,\\d+,\\d+,\\d+,\\d+,\\d+,\\d+,\\d+,\\d+,\\d+,\"\n        r\"\\d+,\\d+,\\d+,\\d+,\\d+,\\d+,\\d+,\\d+,\\d+\"\n    ).replace(\",\", \"\\\\|\")\n    if not Globals.alone:\n        re_str += r\"|\" + re.escape(r\"<<<systemtime>>>\") + r\"|\\d+\"\n    return chain(base, repeat(re_str))\n\n\ndef test_section_dotnet_clrmemory(  # type: ignore[no-untyped-def]\n    request, testconfig, expected_output, actual_output, testfile\n) -> None:\n    # special case wmi may timeout\n    required_lines = 5\n    name = \"dotnet\"\n\n    if not it_utils.check_actual_input(name, required_lines, Globals.alone, actual_output):\n        return\n\n    local_test(expected_output, actual_output, testfile, request.node.name)\n"}
{"type": "test_file", "path": "agents/wnx/tests/regression/test_section_fileinfo.py", "content": "#!/usr/bin/env python3\n# Copyright (C) 2019 Checkmk GmbH - License: GNU General Public License v2\n# This file is part of Checkmk (https://checkmk.com). It is subject to the terms and\n# conditions defined in the file COPYING, which is part of this source code package.\n\nimport os\nimport platform\nimport re\nimport shutil\nfrom collections.abc import Sequence\n\nimport pytest\n\nfrom .local import local_test, user_dir\n\n\nclass TestPaths:\n    def __init__(self) -> None:\n        self.drive, _ = os.path.splitdrive(user_dir)\n\n    def tempdir1(self):\n        _, path = os.path.splitdrive(user_dir)\n        return os.path.join(self.drive, path, \"Testdir1\")\n\n    def tempdir2(self):\n        return os.path.join(self.tempdir1(), \"Testdir2\")\n\n    def tempfile1(self):\n        return os.path.join(self.tempdir1(), \"TestFile1\")\n\n    def tempfile2(self):\n        return os.path.join(self.tempdir1(), \"TestFile2\")\n\n    def tempfile3(self):\n        return os.path.join(self.tempdir2(), \"TestFile3\")\n\n    def missingfile(self):\n        return os.path.join(self.tempdir1(), \"foobar\")\n\n    def drive_upper(self):\n        self.drive = self.drive.upper()\n\n    def drive_lower(self):\n        self.drive = self.drive.lower()\n\n\nclass Globals:\n    section = \"fileinfo\"\n    alone = True\n    paths = TestPaths()\n\n\n@pytest.fixture(name=\"testfile\")\ndef testfile_engine():\n    return os.path.basename(__file__)\n\n\n@pytest.fixture(\n    name=\"testconfig_drive\",\n    params=[\"uppercase_drive\", \"lowercase_drive\"],\n)\ndef testconfig_drive_engine(request, make_yaml_config):\n    if request.param == \"uppercase_drive\":\n        Globals.paths.drive_upper()\n    else:\n        Globals.paths.drive_lower()\n    return make_yaml_config\n\n\n@pytest.fixture(\n    name=\"testconfig\",\n    params=[\n        (Globals.paths.tempdir1, \"**\", True),\n        (Globals.paths.tempdir2, \"Te*\", True),\n        (Globals.paths.tempdir2, \"Te*\", False),\n    ],\n    ids=[\"recursive_glob\", \"simple_glob_alone\", \"simple_glob_with_systemtime\"],\n)\ndef testconfig_engine(request, testconfig_drive):\n    if platform.system() != \"Windows\":\n        return None\n    Globals.alone = request.param[2]\n    Globals.alone = request.param == \"alone\"\n    if Globals.alone:\n        testconfig_drive[\"global\"][\"sections\"] = Globals.section\n    else:\n        testconfig_drive[\"global\"][\"sections\"] = [Globals.section, \"systemtime\"]\n\n    path_array = []\n    if request.param[0] != Globals.paths.tempdir1:\n        path_array.append(Globals.paths.tempfile1())\n        path_array.append(\n            os.path.join(\n                Globals.paths.tempdir1(), \"?\" + os.path.basename(Globals.paths.tempfile2())[1:]\n            )\n        )\n\n    path_array.append(os.path.join(request.param[0](), request.param[1]))\n    path_array.append(Globals.paths.missingfile())\n    testconfig_drive[Globals.section] = {\"path\": path_array}\n\n    return testconfig_drive\n\n\n@pytest.fixture(name=\"expected_output\")\ndef expected_output_engine():\n    if platform.system() != \"Windows\":\n        return None\n    # this variable is for a future release\n    _ = [\n        re.escape(r\"<<<%s:sep(124)>>>\" % Globals.section),\n        r\"\\d+\",\n        re.escape(r\"[[[header]]]\"),\n        re.escape(r\"name|status|size|time\"),\n        re.escape(r\"[[[content]]]\"),\n        re.escape(r\"%s|\" % Globals.paths.tempfile1()) + r\"ok\\|\" + r\"\\d+\\|\\d+\",\n        re.escape(r\"%s|\" % Globals.paths.tempfile2()) + r\"ok\\|\" + r\"\\d+\\|\\d+\",\n        re.escape(r\"%s|\" % Globals.paths.tempfile3()) + r\"ok\\|\" + r\"\\d+\\|\\d+\",\n        re.escape(r\"%s|\" % Globals.paths.missingfile()) + r\"missing\",\n    ]\n    expected_legacy = [\n        re.escape(r\"<<<%s:sep(124)>>>\" % Globals.section),\n        r\"\\d+\",\n        re.escape(r\"%s|\" % Globals.paths.tempfile1()) + r\"\\d+\\|\\d+\",\n        re.escape(r\"%s|\" % Globals.paths.tempfile2()) + r\"\\d+\\|\\d+\",\n        re.escape(r\"%s|\" % Globals.paths.tempfile3()) + r\"\\d+\\|\\d+\",\n        re.escape(r\"%s|missing|\" % Globals.paths.missingfile()) + r\"\\d+\",\n    ]\n    if not Globals.alone:\n        expected_legacy += [re.escape(r\"<<<systemtime>>>\"), r\"\\d+\"]\n    return expected_legacy\n\n\n@pytest.fixture\ndef use_testfiles():\n    if platform.system() == \"Windows\":\n        for d in [Globals.paths.tempdir1(), Globals.paths.tempdir2()]:\n            os.mkdir(d)\n        for f in [Globals.paths.tempfile1(), Globals.paths.tempfile2(), Globals.paths.tempfile3()]:\n            with open(f, \"w\") as handle:\n                handle.write(f)\n\n    yield\n\n    if platform.system() == \"Windows\":\n        for d in [Globals.paths.tempdir2(), Globals.paths.tempdir1()]:\n            shutil.rmtree(d)\n\n\n@pytest.mark.usefixtures(\"use_testfiles\")\ndef test_section_fileinfo(  # type: ignore[no-untyped-def]\n    request, testconfig, expected_output: Sequence[str] | None, actual_output, testfile\n) -> None:\n    # request.node.name gives test name\n    local_test(expected_output, actual_output, testfile, request.node.name)\n"}
{"type": "test_file", "path": "agents/wnx/tests/regression/test_section_plugin_group.py", "content": "#!/usr/bin/env python3\n# Copyright (C) 2019 Checkmk GmbH - License: GNU General Public License v2\n# This file is part of Checkmk (https://checkmk.com). It is subject to the terms and\n# conditions defined in the file COPYING, which is part of this source code package.\n\nimport os\nimport platform\nimport re\nimport shutil\nimport sys\nimport time\nfrom collections.abc import Iterator\nfrom itertools import chain, repeat\n\nimport pytest\n\nfrom .local import local_test, user_dir\n\n\nclass Globals:\n    executionmode = None\n    pluginname = \"\"\n    plugintype = None\n    suffixes = None\n    binaryplugin = \"monty.exe\"\n    alone = False\n\n\n@pytest.fixture(name=\"testfile\")\ndef testfile_engine():\n    return os.path.basename(__file__)\n\n\n@pytest.fixture(name=\"testconfig_suffixes\", params=[\"bat ps1\"], ids=[\"bat_ps1\"])\ndef testconfig_suffixes_engine(request, make_yaml_config):\n    Globals.suffixes = request.param\n    if request.param != \"default\":\n        make_yaml_config[\"global\"][\"execute\"] = request.param\n    return make_yaml_config\n\n\n@pytest.fixture(name=\"testconfig_sections\", params=[\"alone\", \"with_systemtime\"])\ndef testconfig_sections_engine(request, testconfig_suffixes):\n    Globals.alone = request.param == \"alone\"\n    if Globals.alone:\n        testconfig_suffixes[\"global\"][\"sections\"] = [Globals.plugintype]\n    else:\n        testconfig_suffixes[\"global\"][\"sections\"] = [Globals.plugintype, \"systemtime\"]\n    return testconfig_suffixes\n\n\n@pytest.fixture(name=\"testconfig\", params=[\"sync\", \"async\", \"async+cached\"])\ndef testconfig_engine(request, testconfig_sections):\n    Globals.executionmode = request.param\n    name = Globals.plugintype\n    if request.param == \"sync\":\n        testconfig_sections[name] = {\n            \"execution\": [\n                {\n                    \"pattern\": (\"$CUSTOM_PLUGINS_PATH$\\\\\" if name == \"plugins\" else \"\")\n                    + Globals.pluginname,\n                    \"run\": True,\n                    \"async\": False,\n                }\n            ]\n        }\n    else:\n        plugin_cfg = {\n            \"pattern\": (\"$CUSTOM_PLUGINS_PATH$\\\\\" if name == \"plugins\" else \"\")\n            + Globals.pluginname,\n            \"run\": True,\n            \"async\": True,\n            \"timeout\": 10,\n            \"retry_count\": 3,\n        }\n        if request.param == \"async+cached\":\n            plugin_cfg.update({\"cache_age\": 300})\n        testconfig_sections[name] = {\"execution\": [plugin_cfg]}\n    return testconfig_sections\n\n\n@pytest.fixture(name=\"expected_output\")\ndef expected_output_engine():\n    main_label = [\n        re.escape(r\"<<<%s>>>\" % (\"local:sep(0)\" if Globals.plugintype == \"local\" else \"\"))\n    ]\n\n    if Globals.suffixes == \"default\":\n        plugin_fixed = [\n            re.escape(r\"<<<\") + r\"monty_python\" + re.escape(r\">>>\"),\n            r\"Monty Python's Flying Circus\",\n        ]\n    else:\n        plugin_fixed = []\n\n    plugin_variadic: Iterator[str]\n    if Globals.pluginname == \"netstat_an.bat\":\n        plugin_fixed += [\n            re.escape(r\"<<<\")\n            + r\"win_netstat%s\"\n            % (r\":cached\\(\\d+,\\d+\\)\" if Globals.executionmode == \"async+cached\" else r\"\")\n            + re.escape(r\">>>\"),\n            r\"^$\",\n        ]\n        repeating_pattern = (\n            r\"^$\"\n            r\"|Aktive Verbindungen\"\n            r\"|Active Connections\"\n            r\"|\\s+Proto\\s+Lokale Adresse\\s+Remoteadresse\\s+Status\"\n            r\"|\\s+Proto\\s+Local Address\\s+Foreign Address\\s+State\"\n            r\"|\\s+TCP\\s+(\\d+\\.\\d+\\.\\d+\\.\\d+|\"\n            r\"\\[::\\d*\\]|\\[[0-9a-f]{,4}(:[0-9a-f]{,4})+(%\\d+)?\\]):\\d+\"\n            r\"\\s+(\\d+\\.\\d+\\.\\d+\\.\\d+|\\[::\\d*\\]|\"\n            r\"\\[[0-9a-f]{,4}(:[0-9a-f]{,4})+(%\\d+)?\\]):\\d+\"\n            r\"\\s+(ABH.REN|HERGESTELLT|WARTEND|SCHLIESSEN_WARTEN|SYN_GESENDET\"\n            r\"|LISTENING|ESTABLISHED|TIME_WAIT|CLOSE_WAIT|FIN_WAIT_\\d|SYN_SENT|LAST_ACK\"\n            r\"|SCHLIESSEND|SYN_RECEIVED|FIN_WARTEN_\\d|ZULETZT_ACK)\"\n            r\"|\\s+UDP\\s+\\d+\\.\\d+\\.\\d+\\.\\d+:\\d+\\s+\\*:\\*\"\n            r\"|\\-?\\d+( \\d+)+ [\\w\\(\\)]+\"\n        )\n        if Globals.plugintype == \"plugins\":\n            repeating_pattern += r\"|%s\" % re.escape(r\"<<<>>>\")\n        if not Globals.alone:\n            repeating_pattern += r\"|\" + re.escape(r\"<<<systemtime>>>\") + r\"|\\d+\"\n        plugin_variadic = repeat(repeating_pattern)\n    elif Globals.pluginname == \"wmic_if.bat\":\n        plugin_fixed += [\n            re.escape(r\"<<<\")\n            + r\"winperf_if_win32_networkadapter:sep\\(44\\)%s\"\n            % (r\":cached\\(\\d+,\\d+\\)\" if Globals.executionmode == \"async+cached\" else r\"\")\n            + re.escape(r\">>>\"),\n            r\"^$\",\n            r\"^$\",\n            r\"Node,MACAddress,Name,NetConnectionID,NetConnectionStatus,Speed\",\n        ]\n        re_variadic = r\"[^,]+,([0-9A-Fa-f]{2}(:[0-9A-Fa-f]{2}){5})?,[^,]+,[^,]*,\\d*,\\d*|^$\"\n        if Globals.plugintype == \"plugins\":\n            re_variadic += r\"|%s\" % re.escape(r\"<<<>>>\")\n        if not Globals.alone:\n            re_variadic += r\"|\" + re.escape(r\"<<<systemtime>>>\") + r\"|\\d+\"\n        plugin_variadic = repeat(re_variadic)\n    elif Globals.pluginname == \"windows_if.ps1\":\n        plugin_fixed += [\n            re.escape(r\"<<<\")\n            + r\"winperf_if_teaming:sep\\(9\\)%s\"\n            % (r\":cached\\(\\d+,\\d+\\)\" if Globals.executionmode == \"async+cached\" else r\"\")\n            + re.escape(r\">>>\"),\n            (\n                r\"Node\\s+MACAddress\\s+Name\\s+NetConnectionID\\s+NetConnectionStatus\"\n                r\"\\s+Speed\\s+GUID\"\n            ),\n            (\n                r\"[^\\t]+\\s+([0-9A-Fa-f]{2}(:[0-9A-Fa-f]{2}){5})?\\s+[^\\t]+\\s+[^\\t]*\"\n                r\"\\s+\\d*\\s+\\d*\\s+\\{[0-9A-F]+(\\-[0-9A-F]+)+\\}\"\n            ),\n        ]\n        plugin_variadic = iter(\n            [r\"%s\" % (\"\" if Globals.alone else r\"|\" + re.escape(r\"<<<systemtime>>>\") + r\"|\\d+\")]\n        )\n\n    return chain(main_label, plugin_fixed, plugin_variadic)\n\n\n@pytest.fixture(name=\"plugin_dir\", params=[\"plugins\", \"local\"])\ndef plugin_dir_engine(request):\n    Globals.plugintype = request.param\n    target_dir = os.path.join(user_dir, request.param)\n    return target_dir\n\n\n@pytest.fixture(name=\"manage_plugins\", params=[\"netstat_an.bat\", \"wmic_if.bat\"], autouse=True)\ndef manage_plugins_engine(request, plugin_dir):\n    Globals.pluginname = request.param\n    source_dir = os.path.join(os.path.dirname(os.path.dirname(__file__)), \"files\\\\regression\")\n\n    if not os.path.exists(plugin_dir):\n        os.mkdir(plugin_dir)\n\n    shutil.copy(os.path.join(source_dir, Globals.binaryplugin), plugin_dir)\n    shutil.copy(os.path.join(source_dir, request.param), plugin_dir)\n    yield\n    if platform.system() == \"Windows\":\n        for plugin in [request.param, Globals.binaryplugin]:\n            for _ in range(0, 5):\n                try:\n                    os.unlink(os.path.join(plugin_dir, plugin))\n                    break\n                except OSError as e:\n                    # For some reason, the exe plugin remains locked for a short\n                    # while every now and then. Just sleep 1 s and retry.\n                    sys.stderr.write(\"%s\\n\" % str(e))\n                    time.sleep(1)\n\n\ndef test_section_plugin_group(  # type: ignore[no-untyped-def]\n    request, testconfig, expected_output, actual_output, testfile\n) -> None:\n    # request.node.name gives test name\n    if Globals.executionmode == \"async+cached\" and Globals.plugintype == \"local\":\n        pytest.skip(\"This test is not conform with latest changes on Monitoring Site\")\n    local_test(expected_output, actual_output, testfile, request.node.name)\n"}
{"type": "test_file", "path": "agents/wnx/tests/regression/test_section_ps.py", "content": "#!/usr/bin/env python3\n# Copyright (C) 2019 Checkmk GmbH - License: GNU General Public License v2\n# This file is part of Checkmk (https://checkmk.com). It is subject to the terms and\n# conditions defined in the file COPYING, which is part of this source code package.\n\nimport os\nimport re\nfrom itertools import chain, repeat\n\nimport pytest\n\nfrom .local import local_test\n\n\nclass Globals:\n    section = \"ps\"\n    alone = True\n\n\n@pytest.fixture(name=\"testfile\")\ndef testfile_engine():\n    return os.path.basename(__file__)\n\n\n@pytest.fixture(name=\"testconfig\", params=[\"alone\", \"with_systemtime\"])\ndef testconfig_engine(request, make_yaml_config):\n    Globals.alone = request.param == \"alone\"\n    if Globals.alone:\n        make_yaml_config[\"global\"][\"sections\"] = Globals.section\n    else:\n        make_yaml_config[\"global\"][\"sections\"] = [Globals.section, \"systemtime\"]\n    return make_yaml_config\n\n\n@pytest.fixture(\n    name=\"testconfig_use_wmi\",\n    params=[\"yes\", \"no\"],\n    ids=[\"use_wmi=yes\", \"use_wmi=no\"],\n)\ndef testconfig_use_wmi_engine(request, testconfig):\n    testconfig[\"global\"][\"sections\"] = Globals.section\n    testconfig[Globals.section] = {\"enabled\": True, \"use_wmi\": request.param == \"yes\"}\n    return testconfig\n\n\n@pytest.fixture(\n    name=\"full_path_config\",\n    params=[\"yes\", \"no\"],\n    ids=[\"full_path=yes\", \"full_path=no\"],\n)\ndef full_path_config_engine(request, testconfig_use_wmi):\n    testconfig_use_wmi[Globals.section][\"full_path\"] = request.param\n    return testconfig_use_wmi\n\n\n@pytest.fixture(name=\"expected_output\")\ndef expected_output_engine():\n    # expected:\n    # *.exe, *.dll, System, System( Idle Process), Registry,Memory Compression\n    re_str = (\n        r\"\\([^,]+,\\d+,\\d+,\\d+,\\d+,\\d+,\\d+,\\d+,\\d+,\\d+,\\d+\\)\\s+\"\n        r\"(.+(\\.[Ee][Xx][Ee]|\\.[Dd][Ll][Ll]|\\.service)|System( Idle Process)?|Registry|Memory Compression|Secure System|vmmem|com.docker.service)\"\n    )\n    if not Globals.alone:\n        re_str += r\"|\" + re.escape(r\"<<<systemtime>>>\") + r\"|\\d+\"\n\n    # ***************************************\n    # method is not the best one, still works\n    # we have output:\n    # string_normal|string_for_systemtime\n    # ......\n    # string_normal|string_for_systemtime\n    # instead of:\n    # string_normal\n    # ......\n    # string_normal\n    # string_for_systemtime\n    # ***************************************\n    return chain([re.escape(r\"<<<ps:sep(9)>>>\")], repeat(re_str))\n\n\ndef test_section_ps(  # type: ignore[no-untyped-def]\n    request, full_path_config, expected_output, actual_output, testfile\n) -> None:\n    # request.node.name gives test name\n    local_test(expected_output, actual_output, testfile, request.node.name)\n"}
{"type": "test_file", "path": "agents/wnx/tests/regression/test_section_plugin_group_windows_if.py", "content": "#!/usr/bin/env python3\n# Copyright (C) 2019 Checkmk GmbH - License: GNU General Public License v2\n# This file is part of Checkmk (https://checkmk.com). It is subject to the terms and\n# conditions defined in the file COPYING, which is part of this source code package.\n\nimport os\nimport platform\nimport re\nimport shutil\nimport sys\nimport time\nfrom collections.abc import Iterator\nfrom itertools import chain, repeat\n\nimport pytest\n\nfrom .local import local_test, user_dir\n\n\nclass Globals:\n    executionmode: str | None = None\n    pluginname = \"\"\n    plugintype: str | None = None\n    suffixes: str | None = None\n    binaryplugin = \"monty.exe\"\n    alone = False\n\n\n@pytest.fixture(name=\"testfile\")\ndef testfile_engine():\n    return os.path.basename(__file__)\n\n\n@pytest.fixture(name=\"testconfig_suffixes\", params=[\"bat ps1\"], ids=[\"bat_ps1\"])\ndef testconfig_suffixes_engine(request, make_yaml_config):\n    Globals.suffixes = request.param\n    if request.param != \"default\":\n        make_yaml_config[\"global\"][\"execute\"] = request.param\n    return make_yaml_config\n\n\n@pytest.fixture(name=\"testconfig_sections\", params=[\"alone\", \"with_systemtime\"])\ndef testconfig_sections_engine(request, testconfig_suffixes):\n    Globals.alone = request.param == \"alone\"\n    if Globals.alone:\n        testconfig_suffixes[\"global\"][\"sections\"] = [Globals.plugintype]\n    else:\n        testconfig_suffixes[\"global\"][\"sections\"] = [Globals.plugintype, \"systemtime\"]\n    return testconfig_suffixes\n\n\n@pytest.fixture(name=\"testconfig\", params=[\"sync\", \"async\", \"async+cached\"])\ndef testconfig_engine(request, testconfig_sections):\n    Globals.executionmode = request.param\n    name = Globals.plugintype\n    if request.param == \"sync\":\n        testconfig_sections[name] = {\n            \"execution\": [\n                {\n                    \"pattern\": (\"$CUSTOM_PLUGINS_PATH$\\\\\" if name == \"plugins\" else \"\")\n                    + Globals.pluginname,\n                    \"run\": True,\n                    \"async\": False,\n                }\n            ]\n        }\n    else:\n        plugin_cfg = {\n            \"pattern\": (\"$CUSTOM_PLUGINS_PATH$\\\\\" if name == \"plugins\" else \"\")\n            + Globals.pluginname,\n            \"run\": True,\n            \"async\": True,\n            \"timeout\": 10,\n            \"retry_count\": 3,\n        }\n        if request.param == \"async+cached\":\n            plugin_cfg.update({\"cache_age\": 300})\n        testconfig_sections[name] = {\"execution\": [plugin_cfg]}\n    return testconfig_sections\n\n\n@pytest.fixture(name=\"expected_output\")\ndef expected_output_engine():\n    main_label = [\n        re.escape(r\"<<<%s>>>\" % (\"local:sep(0)\" if Globals.plugintype == \"local\" else \"\"))\n    ]\n\n    if Globals.suffixes == \"default\":\n        plugin_fixed = [\n            re.escape(r\"<<<\") + r\"monty_python\" + re.escape(r\">>>\"),\n            r\"Monty Python's Flying Circus\",\n        ]\n    else:\n        plugin_fixed = []\n\n    plugin_variadic: Iterator[str]\n    if Globals.pluginname == \"netstat_an.bat\":\n        plugin_fixed += [\n            re.escape(r\"<<<\")\n            + r\"win_netstat%s\"\n            % (r\":cached\\(\\d+,\\d+\\)\" if Globals.executionmode == \"async+cached\" else r\"\")\n            + re.escape(r\">>>\"),\n            r\"^$\",\n        ]\n        repeating_pattern = (\n            r\"^$\"\n            r\"|Aktive Verbindungen\"\n            r\"|Active Connections\"\n            r\"|\\s+Proto\\s+Lokale Adresse\\s+Remoteadresse\\s+Status\"\n            r\"|\\s+Proto\\s+Local Address\\s+Foreign Address\\s+State\"\n            r\"|\\s+TCP\\s+(\\d+\\.\\d+\\.\\d+\\.\\d+|\"\n            r\"\\[::\\d*\\]|\\[[0-9a-f]{,4}(:[0-9a-f]{,4})+(%\\d+)?\\]):\\d+\"\n            r\"\\s+(\\d+\\.\\d+\\.\\d+\\.\\d+|\\[::\\d*\\]|\"\n            r\"\\[[0-9a-f]{,4}(:[0-9a-f]{,4})+(%\\d+)?\\]):\\d+\"\n            r\"\\s+(ABH.REN|HERGESTELLT|WARTEND|SCHLIESSEN_WARTEN|SYN_GESENDET\"\n            r\"|LISTENING|ESTABLISHED|TIME_WAIT|CLOSE_WAIT|FIN_WAIT_\\d|SYN_SENT|LAST_ACK\"\n            r\"|SCHLIESSEND|FIN_WARTEN_\\d|ZULETZT_ACK)\"\n            r\"|\\s+UDP\\s+\\d+\\.\\d+\\.\\d+\\.\\d+:\\d+\\s+\\*:\\*\"\n            r\"|\\-?\\d+( \\d+)+ [\\w\\(\\)]+\"\n        )\n        if Globals.plugintype == \"plugins\":\n            repeating_pattern += r\"|%s\" % re.escape(r\"<<<>>>\")\n        if not Globals.alone:\n            repeating_pattern += r\"|\" + re.escape(r\"<<<systemtime>>>\") + r\"|\\d+\"\n        plugin_variadic = repeat(repeating_pattern)\n    elif Globals.pluginname == \"wmic_if.bat\":\n        plugin_fixed += [\n            re.escape(r\"<<<\")\n            + r\"winperf_if_win32_networkadapter:sep\\(44\\)%s\"\n            % (r\":cached\\(\\d+,\\d+\\)\" if Globals.executionmode == \"async+cached\" else r\"\")\n            + re.escape(r\">>>\"),\n            r\"^$\",\n            r\"^$\",\n            r\"Node,MACAddress,Name,NetConnectionID,NetConnectionStatus,Speed\",\n        ]\n        re_variadic = r\"[^,]+,([0-9A-Fa-f]{2}(:[0-9A-Fa-f]{2}){5})?,[^,]+,[^,]*,\\d*,\\d*|^$\"\n        if Globals.plugintype == \"plugins\":\n            re_variadic += r\"|%s\" % re.escape(r\"<<<>>>\")\n        if not Globals.alone:\n            re_variadic += r\"|\" + re.escape(r\"<<<systemtime>>>\") + r\"|\\d+\"\n        plugin_variadic = repeat(re_variadic)\n    elif Globals.pluginname == \"windows_if.ps1\":\n        plugin_fixed += [\n            re.escape(r\"<<<\")\n            + r\"winperf_if_teaming:sep\\(9\\)%s\"\n            % (r\":cached\\(\\d+,\\d+\\)\" if Globals.executionmode == \"async+cached\" else r\"\")\n            + re.escape(r\">>>\"),\n            (\n                r\"Node\\s+MACAddress\\s+Name\\s+NetConnectionID\\s+NetConnectionStatus\"\n                r\"\\s+Speed\\s+GUID\"\n            ),\n            (\n                r\"[^\\t]+\\s+([0-9A-Fa-f]{2}(:[0-9A-Fa-f]{2}){5})?\\s+[^\\t]+\\s+[^\\t]*\"\n                r\"\\s+\\d*\\s+\\d*\\s+\\{[0-9A-F]+(\\-[0-9A-F]+)+\\}\"\n            ),\n        ]\n        plugin_variadic = iter(\n            [r\"%s\" % (\"\" if Globals.alone else r\"|\" + re.escape(r\"<<<systemtime>>>\") + r\"|\\d+\")]\n        )\n\n    return chain(main_label, plugin_fixed, plugin_variadic)\n\n\n@pytest.fixture(name=\"plugin_dir\", params=[\"plugins\", \"local\"])\ndef plugin_dir_engine(request):\n    Globals.plugintype = request.param\n    target_dir = os.path.join(user_dir, request.param)\n    return target_dir\n\n\n@pytest.fixture(name=\"manage_plugins\", params=[\"windows_if.ps1\"], autouse=True)\ndef manage_plugins_engine(request, plugin_dir):\n    Globals.pluginname = request.param\n    source_dir = os.path.join(os.path.dirname(os.path.dirname(__file__)), \"files\\\\regression\")\n\n    if not os.path.exists(plugin_dir):\n        os.mkdir(plugin_dir)\n\n    shutil.copy(os.path.join(source_dir, Globals.binaryplugin), plugin_dir)\n    shutil.copy(os.path.join(source_dir, request.param), plugin_dir)\n    yield\n    if platform.system() == \"Windows\":\n        for plugin in [request.param, Globals.binaryplugin]:\n            for _ in range(0, 5):\n                try:\n                    os.unlink(os.path.join(plugin_dir, plugin))\n                    break\n                except OSError as e:\n                    # For some reason, the exe plugin remains locked for a short\n                    # while every now and then. Just sleep 1 s and retry.\n                    sys.stderr.write(\"%s\\n\" % str(e))\n                    time.sleep(1)\n\n\n@pytest.mark.skip(\"This test is not conform with latest changes on Monitoring Site\")\ndef test_section_plugin_windows_if(  # type: ignore[no-untyped-def]\n    request, testconfig, expected_output, actual_output, testfile\n) -> None:\n    # request.node.name gives test name\n    if Globals.executionmode == \"async+cached\" and Globals.plugintype == \"local\":\n        pytest.skip(\"This test is not conform with latest changes on Monitoring Site\")\n    local_test(expected_output, actual_output, testfile, request.node.name)\n"}
{"type": "test_file", "path": "agents/wnx/tests/regression/test_section_services.py", "content": "#!/usr/bin/env python3\n# Copyright (C) 2019 Checkmk GmbH - License: GNU General Public License v2\n# This file is part of Checkmk (https://checkmk.com). It is subject to the terms and\n# conditions defined in the file COPYING, which is part of this source code package.\n\nimport os\nimport re\nfrom itertools import chain, repeat\n\nimport pytest\n\nfrom .local import local_test\n\n\nclass Globals:\n    section = \"services\"\n    alone = True\n\n\n@pytest.fixture(name=\"testfile\")\ndef testfile_engine():\n    return os.path.basename(__file__)\n\n\n@pytest.fixture(name=\"testconfig\", params=[\"alone\", \"with_systemtime\"])\ndef testconfig_engine(request, make_yaml_config):\n    Globals.alone = request.param == \"alone\"\n    if Globals.alone:\n        make_yaml_config[\"global\"][\"sections\"] = Globals.section\n    else:\n        make_yaml_config[\"global\"][\"sections\"] = [Globals.section, \"systemtime\"]\n    return make_yaml_config\n\n\n@pytest.fixture(name=\"expected_output\")\ndef expected_output_engine():\n    re_str = (\n        r\"[\\(\\)$\\w\\.-]+ (unknown|continuing|pausing|paused|running|starting\"\n        r\"|stopping|stopped)/(invalid1|invalid2|invalid3|invalid4|auto\"\n        r\"|boot|demand|disabled|system|other) .+\"\n    )\n    if not Globals.alone:\n        re_str += r\"|\" + re.escape(r\"<<<systemtime>>>\") + r\"|\\d+\"\n    return chain([re.escape(r\"<<<%s>>>\" % Globals.section)], repeat(re_str))\n\n\ndef test_section_services(  # type: ignore[no-untyped-def]\n    request, testconfig, expected_output, actual_output, testfile\n) -> None:\n    # request.node.name gives test name\n    local_test(expected_output, actual_output, testfile, request.node.name)\n"}
{"type": "test_file", "path": "agents/wnx/tests/regression/test_section_systemtime.py", "content": "#!/usr/bin/env python3\n# Copyright (C) 2019 Checkmk GmbH - License: GNU General Public License v2\n# This file is part of Checkmk (https://checkmk.com). It is subject to the terms and\n# conditions defined in the file COPYING, which is part of this source code package.\n\nimport os\n\nimport pytest\n\nfrom .local import local_test\n\n\n@pytest.fixture(name=\"testfile\")\ndef testfile_engine():\n    return os.path.basename(__file__)\n\n\n@pytest.fixture(name=\"testconfig\")\ndef fixture_testconfig(make_yaml_config):\n    make_yaml_config[\"global\"][\"sections\"] = \"systemtime\"\n    return make_yaml_config\n\n\n@pytest.fixture(name=\"expected_output\")\ndef expected_output_engine():\n    return [r\"<<<systemtime>>>\", r\"\\d+\"]\n\n\ndef test_section_systemtime(  # type: ignore[no-untyped-def]\n    testconfig, expected_output, actual_output, testfile\n) -> None:\n    local_test(expected_output, actual_output, testfile)\n"}
{"type": "test_file", "path": "agents/wnx/tests/regression/test_section_spool.py", "content": "#!/usr/bin/env python3\n# Copyright (C) 2019 Checkmk GmbH - License: GNU General Public License v2\n# This file is part of Checkmk (https://checkmk.com). It is subject to the terms and\n# conditions defined in the file COPYING, which is part of this source code package.\n\nimport os\nimport platform\nimport re\n\nimport pytest\n\nfrom .local import local_test, user_dir\n\n\nclass Globals:\n    section = \"spool\"\n    alone = True\n    test_message = \"Test message\"\n    outdated = False\n\n\n@pytest.fixture(name=\"testfile\")\ndef testfile_engine():\n    return os.path.basename(__file__)\n\n\n@pytest.fixture(name=\"testconfig\", params=[\"alone\", \"with_systemtime\"])\ndef fixture_testconfig(request, make_yaml_config):\n    Globals.alone = request.param == \"alone\"\n    if Globals.alone:\n        make_yaml_config[\"global\"][\"sections\"] = Globals.section\n    else:\n        make_yaml_config[\"global\"][\"sections\"] = [Globals.section, \"systemtime\"]\n    return make_yaml_config\n\n\n@pytest.fixture(name=\"expected_output\")\ndef expected_output_engine():\n    expected = []\n    if not Globals.outdated:\n        expected += [r\"%s\" % Globals.test_message]\n    if not Globals.alone:\n        expected += [re.escape(r\"<<<systemtime>>>\"), r\"\\d+\"]\n    return expected\n\n\n@pytest.fixture(params=[\"yes\", \"no\"], ids=[\"outdated\", \"not_outdated\"], autouse=True)\ndef manage_spoolfile(request):\n    Globals.outdated = request.param == \"yes\"\n    testfile = \"0testfile\" if request.param == \"yes\" else \"testfile\"\n    filename = os.path.join(user_dir, \"spool\", testfile)\n    if platform.system() == \"Windows\":\n        spooldir = os.path.join(user_dir, \"spool\")\n        try:\n            os.mkdir(spooldir)\n        except OSError:\n            pass  # Directory may already exist.\n        with open(filename, \"w\") as f:\n            f.write(\"%s\" % Globals.test_message)\n        # Hack the modification time 2 s back in time\n        stat = os.stat(filename)\n        times = stat.st_atime, stat.st_mtime - 2\n        os.utime(filename, times)\n\n    yield\n\n    if platform.system() == \"Windows\":\n        os.unlink(filename)\n\n\ndef test_section_spool(  # type: ignore[no-untyped-def]\n    request, testconfig, expected_output, actual_output, testfile\n) -> None:\n    # request.node.name gives test name\n    local_test(expected_output, actual_output, testfile, request.node.name)\n"}
{"type": "test_file", "path": "agents/wnx/tests/regression/test_section_uptime.py", "content": "#!/usr/bin/env python3\n# Copyright (C) 2019 Checkmk GmbH - License: GNU General Public License v2\n# This file is part of Checkmk (https://checkmk.com). It is subject to the terms and\n# conditions defined in the file COPYING, which is part of this source code package.\n\nimport os\nimport re\n\nimport pytest\n\nfrom .local import local_test\n\n\nclass Globals:\n    section = \"uptime\"\n    alone = True\n\n\n@pytest.fixture(name=\"testfile\")\ndef testfile_engine():\n    return os.path.basename(__file__)\n\n\n@pytest.fixture(name=\"testconfig\", params=[\"alone\", \"with_systemtime\"])\ndef fixture_testconfig(request, make_yaml_config):\n    Globals.alone = request.param == \"alone\"\n    if Globals.alone:\n        make_yaml_config[\"global\"][\"sections\"] = Globals.section\n    else:\n        make_yaml_config[\"global\"][\"sections\"] = [Globals.section, \"systemtime\"]\n    return make_yaml_config\n\n\n@pytest.fixture(name=\"expected_output\")\ndef expected_output_engine():\n    expected = [r\"<<<%s>>>\" % Globals.section, r\"\\d+\"]\n    if not Globals.alone:\n        expected += [re.escape(r\"<<<systemtime>>>\"), r\"\\d+\"]\n    return expected\n\n\ndef test_section_uptime(  # type: ignore[no-untyped-def]\n    request, testconfig, expected_output, actual_output, testfile\n) -> None:\n    # request.node.name gives test name\n    local_test(expected_output, actual_output, testfile, request.node.name)\n"}
{"type": "test_file", "path": "agents/wnx/tests/regression/test_section_wmi_cpuload.py", "content": "#!/usr/bin/env python3\n# Copyright (C) 2019 Checkmk GmbH - License: GNU General Public License v2\n# This file is part of Checkmk (https://checkmk.com). It is subject to the terms and\n# conditions defined in the file COPYING, which is part of this source code package.\n\nimport os\nimport re\nfrom collections.abc import Sequence\nfrom typing import Any, AnyStr, Final\n\nimport pytest\n\nfrom . import it_utils\nfrom .local import local_test\n\n\nclass Globals:\n    section = \"wmi_cpuload\"\n    alone = True\n\n\nEXPECTED_SYSTEM_PERF_HEADER: Final[dict[str, str]] = {\n    \"use_wmi\": (\n        r\"AlignmentFixupsPersec,Caption,ContextSwitchesPersec,Description,\"\n        r\"ExceptionDispatchesPersec,FileControlBytesPersec,\"\n        r\"FileControlOperationsPersec,FileDataOperationsPersec,\"\n        r\"FileReadBytesPersec,FileReadOperationsPersec,FileWriteBytesPersec,\"\n        r\"FileWriteOperationsPersec,FloatingEmulationsPersec,Frequency_Object,\"\n        r\"Frequency_PerfTime,Frequency_Sys100NS,Name,\"\n        r\"PercentRegistryQuotaInUse,PercentRegistryQuotaInUse_Base,Processes,\"\n        r\"ProcessorQueueLength,SystemCallsPersec,SystemUpTime,Threads,\"\n        r\"Timestamp_Object,Timestamp_PerfTime,Timestamp_Sys100NS,WMIStatus\"\n    ).replace(\",\", \"\\\\|\"),\n    \"use_perf\": r\"Name,ProcessorQueueLength,Timestamp_PerfTime,Frequency_PerfTime,WMIStatus\".replace(\n        \",\", \"\\\\|\"\n    ),\n}\n\nEXPECTED_SYSTEM_PERF_DATA: Final[dict[str, str]] = {\n    \"use_wmi\": (\n        r\"\\d+,,\\d+,,\\d+,\\d+,\\d+,\\d+,\\d+,\\d+,\\d+,\\d+,\\d+,\\d+,\\d+,\\d+,,\\d+,\"\n        r\"\\d+,\\d+,\\d+,\\d+,\\d+,\\d+,\\d+,\\d+,\\d+,\\b(?:OK|Timeout)\\b\"\n    ).replace(\",\", \"\\\\|\"),\n    \"use_perf\": r\",\\d+,\\d+,\\d+,OK\".replace(\",\", \"\\\\|\"),\n}\n\nEXPECTED_COMPUTER_SYSTEM_HEADER: Final[dict[str, str]] = {\n    \"use_wmi\": (\n        r\"AdminPasswordStatus,AutomaticManagedPagefile,\"\n        r\"AutomaticResetBootOption,AutomaticResetCapability,BootOptionOnLimit,\"\n        r\"BootOptionOnWatchDog,BootROMSupported,BootStatus,BootupState,\"\n        r\"Caption,ChassisBootupState,ChassisSKUNumber,CreationClassName,\"\n        r\"CurrentTimeZone,DaylightInEffect,Description,DNSHostName,Domain,\"\n        r\"DomainRole,EnableDaylightSavingsTime,FrontPanelResetStatus,\"\n        r\"HypervisorPresent,InfraredSupported,InitialLoadInfo,InstallDate,\"\n        r\"KeyboardPasswordStatus,LastLoadInfo,Manufacturer,Model,Name,\"\n        r\"NameFormat,NetworkServerModeEnabled,NumberOfLogicalProcessors,\"\n        r\"NumberOfProcessors,OEMLogoBitmap,OEMStringArray,PartOfDomain,\"\n        r\"PauseAfterReset,PCSystemType,PCSystemTypeEx,\"\n        r\"PowerManagementCapabilities,PowerManagementSupported,\"\n        r\"PowerOnPasswordStatus,PowerState,PowerSupplyState,\"\n        r\"PrimaryOwnerContact,PrimaryOwnerName,ResetCapability,ResetCount,\"\n        r\"ResetLimit,Roles,Status,SupportContactDescription,SystemFamily,\"\n        r\"SystemSKUNumber,SystemStartupDelay,SystemStartupOptions,\"\n        r\"SystemStartupSetting,SystemType,ThermalState,TotalPhysicalMemory,\"\n        r\"UserName,WakeUpType,Workgroup,WMIStatus\"\n    ).replace(\",\", \"\\\\|\"),\n    \"use_perf\": r\"Name,NumberOfLogicalProcessors,NumberOfProcessors,WMIStatus\".replace(\",\", \"\\\\|\"),\n}\n\nEXPECTED_COMPUTER_SYSTEM_DATA: Final[dict[str, str]] = {\n    \"use_wmi\": (\n        r\"\\d+,\\d+,\\d+,\\d+,\\d*,\\d*,\\d+,[^,]*,[^,]+,[\\w-]+,\\d+,[^,]*,\\w+,\\d+,\\d+,\"\n        r\"[^,]+,[\\w-]+,[^,]+,\\d+,\\d+,\\d+,\\d+,\\d+,,,\\d+,,[^,]+(, [^,]+)?,[^,]+,\"\n        r\"[\\w-]+,,\\d+,\\d+,\\d+,,[^,]*,\\d+,\\-?\\d+,\\d+,\\d+,,,\\d+,\\d+,\\d+,,[^,]+,\"\n        r\"\\d+,\\d+,\\d+,[^,]+,\\w+,,[^,]*,[^,]*,,,,[^,]+,\\d+,\\d+,[^,]*,\\d+,\\w*,\\b(?:OK|Timeout)\\b\"\n    ).replace(\",\", \"\\\\|\"),\n    \"use_perf\": r\"[^,]*,\\d+,\\d+,OK\".replace(\",\", \"\\\\|\"),\n}\n\n\n@pytest.fixture(name=\"testfile\")\ndef testfile_engine() -> str:\n    return os.path.basename(__file__)\n\n\n@pytest.fixture(name=\"config_with_cpuload_method\", params=[\"use_wmi\", \"use_perf\"])\ndef change_config_cpuload_method(  # type: ignore[no-untyped-def]\n    request, make_yaml_config\n) -> dict[str, Any]:\n    make_yaml_config[\"global\"][\"cpuload_method\"] = request.param\n    return make_yaml_config\n\n\n@pytest.fixture(name=\"testconfig\", params=[\"alone\", \"with_systemtime\"])\ndef fixture_testconfig(  # type: ignore[no-untyped-def]\n    request, config_with_cpuload_method\n) -> dict[str, Any]:\n    Globals.alone = request.param == \"alone\"\n    config_with_cpuload_method[\"global\"][\"sections\"] = (\n        Globals.section if Globals.alone else [Globals.section, \"systemtime\"]\n    )\n    config_with_cpuload_method[\"global\"][\"wmi_timeout\"] = 10\n    return config_with_cpuload_method\n\n\n@pytest.fixture(name=\"expected\")\ndef expected_output_engine(testconfig) -> Sequence[str]:  # type: ignore[no-untyped-def]\n    method = testconfig[\"global\"][\"cpuload_method\"]\n    expected = [\n        re.escape(f\"<<<{Globals.section}:sep(124)>>>\"),\n        re.escape(\"[system_perf]\"),\n        EXPECTED_SYSTEM_PERF_HEADER[method],\n        EXPECTED_SYSTEM_PERF_DATA[method],\n        re.escape(\"[computer_system]\"),\n        EXPECTED_COMPUTER_SYSTEM_HEADER[method],\n        EXPECTED_COMPUTER_SYSTEM_DATA[method],\n    ]\n    if not Globals.alone:\n        expected += [re.escape(r\"<<<systemtime>>>\"), r\"\\d+\"]\n    return expected\n\n\ndef test_section_wmi_cpuload(  # type: ignore[no-untyped-def]\n    expected: Sequence[str], actual_output: Sequence[str], testfile: AnyStr\n):\n    required_lines = 7\n    name = \"cpu_load\"\n\n    if not it_utils.check_actual_input(name, required_lines, Globals.alone, actual_output):\n        return\n\n    local_test(expected, actual_output, testfile)\n"}
{"type": "test_file", "path": "agents/wnx/tests/regression/test_section_winperf.py", "content": "#!/usr/bin/env python3\n# Copyright (C) 2019 Checkmk GmbH - License: GNU General Public License v2\n# This file is part of Checkmk (https://checkmk.com). It is subject to the terms and\n# conditions defined in the file COPYING, which is part of this source code package.\n\nimport os\nimport re\nfrom itertools import repeat\n\nimport pytest\n\nfrom .local import local_test\n\n\nclass Globals:\n    section = \"winperf\"\n    alone = True\n\n\n@pytest.fixture(name=\"testfile\")\ndef testfile_engine():\n    return os.path.basename(__file__)\n\n\n@pytest.fixture(name=\"testconfig_sections\", params=[\"alone\", \"with_systemtime\"])\ndef fixture_testconfig_sections(request, make_yaml_config):\n    Globals.alone = request.param == \"alone\"\n    if Globals.alone:\n        make_yaml_config[\"global\"][\"sections\"] = Globals.section\n    else:\n        make_yaml_config[\"global\"][\"sections\"] = [Globals.section, \"systemtime\"]\n    return make_yaml_config\n\n\n@pytest.fixture(name=\"testconfig\", params=[\"System\", \"2\"], ids=[\"counter:System\", \"counter:2\"])\ndef fixture_testconfig(request, testconfig_sections):\n    testconfig_sections[Globals.section] = {\"counters\": [\"%s:test\" % request.param]}\n    return testconfig_sections\n\n\n@pytest.fixture(name=\"expected_output\")\ndef expected_output_engine():\n    re_str = (\n        r\"\\<\\<\\<winperf_(if|phydisk|processor|test)\\>\\>\\>\"\n        r\"|\\d+\\.\\d{2} \\d+ \\d+\"\n        r\"|\\d+ instances\\:( [^ ]+)+\"\n        r\"|\\-?\\d+( \\d+)+ [\\w\\(\\)]+\"\n        r\"|\\d\\d\\d[2|6]( .+)+ text\"\n    )\n    if not Globals.alone:\n        re_str += r\"|\" + re.escape(r\"<<<systemtime>>>\") + r\"|\\d+\"\n    return repeat(re_str)\n\n\ndef test_section_winperf(  # type: ignore[no-untyped-def]\n    request, testconfig, expected_output, actual_output, testfile\n) -> None:\n    # request.node.name gives test name\n    local_test(expected_output, actual_output, testfile, request.node.name)\n"}
{"type": "test_file", "path": "agents/wnx/tests/testlib/__init__.py", "content": "#!/usr/bin/env python3\n# Copyright (C) 2019 Checkmk GmbH - License: GNU General Public License v2\n# This file is part of Checkmk (https://checkmk.com). It is subject to the terms and\n# conditions defined in the file COPYING, which is part of this source code package.\n"}
{"type": "test_file", "path": "agents/wnx/tests/testlib/utils.py", "content": "#!/usr/bin/env python3\n# Copyright (C) 2019 Checkmk GmbH - License: GNU General Public License v2\n# This file is part of Checkmk (https://checkmk.com). It is subject to the terms and\n# conditions defined in the file COPYING, which is part of this source code package.\n\n\nimport os\nfrom pathlib import Path\nfrom typing import Final\n\n_CHECKMK_GIT_ENV_VAR: Final = \"CHECKMK_GIT_DIR\"\n\n\ndef get_path_from_env(env: str) -> Path:\n    env_value = os.getenv(env)\n    assert env_value is not None\n    return Path(env_value)\n\n\ndef get_git_root_path() -> Path:\n    return get_path_from_env(_CHECKMK_GIT_ENV_VAR)\n"}
{"type": "test_file", "path": "packages/cmk-agent-based/tests/cmk/agent_based/test_v2_namespace.py", "content": "#!/usr/bin/env python3\n# Copyright (C) 2020 Checkmk GmbH - License: GNU General Public License v2\n# This file is part of Checkmk (https://checkmk.com). It is subject to the terms and\n# conditions defined in the file COPYING, which is part of this source code package.\n\"\"\"\n+---------------------------------------------------------+\n|              Achtung Alles Lookenskeepers!              |\n|              =============================              |\n|                                                         |\n| The extend of the Check API is well documented, and the |\n| result of careful negotiation. It should not be changed |\n| light heartedly!                                        |\n+---------------------------------------------------------+\n\"\"\"\n\nfrom types import ModuleType\n\nfrom cmk.agent_based import v2\n\n\ndef _names(space: ModuleType) -> set[str]:\n    return {n for n in dir(space) if not n.startswith(\"_\")}\n\n\ndef test_v2() -> None:\n    expected = {\n        \"entry_point_prefixes\",\n        \"AgentParseFunction\",\n        \"AgentSection\",\n        \"SimpleSNMPSection\",\n        \"SNMPSection\",\n        \"CheckPlugin\",\n        \"InventoryPlugin\",\n        \"CheckResult\",\n        \"DiscoveryResult\",\n        \"HostLabelGenerator\",\n        \"InventoryResult\",\n        \"StringByteTable\",\n        \"StringTable\",\n        \"RuleSetType\",\n        \"Attributes\",\n        \"GetRateError\",\n        \"HostLabel\",\n        \"IgnoreResults\",\n        \"IgnoreResultsError\",\n        \"Metric\",\n        \"OIDBytes\",\n        \"OIDCached\",\n        \"OIDEnd\",\n        \"Result\",\n        \"SNMPTree\",\n        \"SNMPDetectSpecification\",\n        \"Service\",\n        \"ServiceLabel\",\n        \"State\",\n        \"TableRow\",\n        \"all_of\",\n        \"any_of\",\n        \"check_levels\",\n        \"NoLevelsT\",\n        \"FixedLevelsT\",\n        \"PredictiveLevelsT\",\n        \"LevelsT\",\n        \"clusterize\",\n        \"contains\",\n        \"endswith\",\n        \"equals\",\n        \"exists\",\n        \"get_average\",\n        \"get_rate\",\n        \"get_value_store\",\n        \"matches\",\n        \"not_contains\",\n        \"not_endswith\",\n        \"not_equals\",\n        \"not_exists\",\n        \"not_matches\",\n        \"not_startswith\",\n        \"render\",\n        \"startswith\",\n    }\n    assert _names(v2) == expected\n\n\ndef test_v2_render() -> None:\n    expected = {\n        \"bytes\",\n        \"date\",\n        \"datetime\",\n        \"disksize\",\n        \"filesize\",\n        \"frequency\",\n        \"iobandwidth\",\n        \"networkbandwidth\",\n        \"nicspeed\",\n        \"percent\",\n        \"timespan\",\n        \"time_offset\",\n    }\n    assert _names(v2.render) == expected\n\n\ndef test_v1_clusterize() -> None:\n    expected = {\"make_node_notice_results\"}\n    assert _names(v2.clusterize) == expected\n"}
{"type": "test_file", "path": "packages/cmk-agent-based/tests/cmk/agent_based/test_v1_namespace.py", "content": "#!/usr/bin/env python3\n# Copyright (C) 2020 Checkmk GmbH - License: GNU General Public License v2\n# This file is part of Checkmk (https://checkmk.com). It is subject to the terms and\n# conditions defined in the file COPYING, which is part of this source code package.\n\"\"\"\n+---------------------------------------------------------+\n|              Achtung Alles Lookenskeepers!              |\n|              =============================              |\n|                                                         |\n| The extend of the Check API is well documented, and the |\n| result of careful negotiation. It should not be changed |\n| light heartedly!                                        |\n+---------------------------------------------------------+\n\"\"\"\n\nfrom types import ModuleType\n\nfrom cmk.agent_based import v1\n\n\ndef _names(space: ModuleType) -> set[str]:\n    return {n for n in dir(space) if not n.startswith(\"_\")}\n\n\ndef test_v1() -> None:\n    expected = {\n        # value_store: not explicitly exposed here,\n        \"value_store\",\n        # register: only partially in this package, b/c that is not how we're doing things anymore.\n        \"register\",\n        \"Attributes\",\n        \"GetRateError\",\n        \"HostLabel\",\n        \"IgnoreResults\",\n        \"IgnoreResultsError\",\n        \"Metric\",\n        \"OIDBytes\",\n        \"OIDCached\",\n        \"OIDEnd\",\n        \"Result\",\n        \"SNMPTree\",\n        \"Service\",\n        \"ServiceLabel\",\n        \"State\",\n        \"TableRow\",\n        \"all_of\",\n        \"any_of\",\n        \"check_levels\",\n        \"check_levels_predictive\",\n        \"clusterize\",\n        \"contains\",\n        \"endswith\",\n        \"equals\",\n        \"exists\",\n        \"get_average\",\n        \"get_rate\",\n        \"get_value_store\",\n        \"matches\",\n        \"not_contains\",\n        \"not_endswith\",\n        \"not_equals\",\n        \"not_exists\",\n        \"not_matches\",\n        \"not_startswith\",\n        \"regex\",\n        \"render\",\n        \"startswith\",\n        \"type_defs\",\n    }\n    assert _names(v1) == expected\n\n\ndef test_v1_render() -> None:\n    expected = {\n        \"bytes\",\n        \"date\",\n        \"datetime\",\n        \"disksize\",\n        \"filesize\",\n        \"frequency\",\n        \"iobandwidth\",\n        \"networkbandwidth\",\n        \"nicspeed\",\n        \"percent\",\n        \"timespan\",\n    }\n    assert _names(v1.render) == expected\n\n\ndef test_v1_type_defs() -> None:\n    expected = {\n        \"CheckResult\",\n        \"DiscoveryResult\",\n        \"HostLabelGenerator\",\n        \"InventoryResult\",\n        \"StringByteTable\",\n        \"StringTable\",\n    }\n    assert _names(v1.type_defs) == expected\n\n\ndef test_v1_clusterize() -> None:\n    expected = {\"make_node_notice_results\"}\n    assert _names(v1.clusterize) == expected\n"}
{"type": "test_file", "path": "packages/cmk-agent-based/tests/cmk/agent_based/v1/test_checking_classes.py", "content": "#!/usr/bin/env python3\n# Copyright (C) 2019 Checkmk GmbH - License: GNU General Public License v2\n# This file is part of Checkmk (https://checkmk.com). It is subject to the terms and\n# conditions defined in the file COPYING, which is part of this source code package.\n\n\nfrom ast import literal_eval\nfrom collections.abc import Sequence\n\nimport pytest\n\nfrom cmk.agent_based.v1 import IgnoreResults, Metric, Result, Service, ServiceLabel, State\nfrom cmk.agent_based.v1._checking_classes import _EvalableFloat\n\n\ndef test_evalable_float() -> None:\n    inf = _EvalableFloat(\"inf\")\n    assert literal_eval(f\"{inf!r}\") == float(\"inf\")\n\n\ndef test_service_label() -> None:\n    # as far as the API is concerned, the only important thing ist that they\n    # exist, an can be created like this.\n    _ = ServiceLabel(\"from-home-office\", \"true\")\n\n\n@pytest.mark.parametrize(\n    \"item, parameters, labels\",\n    [\n        (4, None, None),\n        (None, (80, 90), None),\n        (None, None, [\"foo:bar\"]),\n    ],\n)\ndef test_service_invalid(item: object, parameters: object, labels: object) -> None:\n    with pytest.raises(TypeError):\n        _ = Service(item=item, parameters=parameters, labels=labels)  # type: ignore[arg-type]\n\n\ndef test_service_kwargs_only() -> None:\n    with pytest.raises(TypeError):\n        _ = Service(None)  # type: ignore[misc]\n\n\ndef test_service_features() -> None:\n    service = Service(\n        item=\"thingy\",\n        parameters={\"size\": 42},\n        labels=[ServiceLabel(\"test-thing\", \"true\")],\n    )\n\n    assert service.item == \"thingy\"\n    assert service.parameters == {\"size\": 42}\n    assert service.labels == [ServiceLabel(\"test-thing\", \"true\")]\n\n    assert repr(service) == (\n        \"Service(item='thingy', parameters={'size': 42},\"\n        \" labels=[ServiceLabel('test-thing', 'true')])\"\n    )\n\n    service = Service()\n    assert service.item is None\n    assert service.parameters == {}\n    assert service.labels == []\n    assert repr(service) == \"Service()\"\n\n    service_foo = Service(item=\"foo\")\n    assert repr(service_foo) == \"Service(item='foo')\"\n\n    assert service != service_foo\n\n\ndef test_state() -> None:\n    assert int(State.OK) == 0\n    assert int(State.WARN) == 1\n    assert int(State.CRIT) == 2\n    assert int(State.UNKNOWN) == 3\n\n    assert State.worst(State.WARN, State.UNKNOWN, State.CRIT) is State.CRIT\n    assert State.worst(State.OK, State.WARN, State.UNKNOWN) is State.UNKNOWN\n    assert State.worst(State.OK, State.WARN) is State.WARN\n    assert State.worst(State.OK) is State.OK\n    assert State.worst(State.OK, 3) is State.UNKNOWN\n\n    assert State(0) is State.OK\n    assert State(1) is State.WARN\n    assert State(2) is State.CRIT\n    assert State(3) is State.UNKNOWN\n\n    assert State[\"OK\"] is State.OK\n    assert State[\"WARN\"] is State.WARN\n    assert State[\"CRIT\"] is State.CRIT\n    assert State[\"UNKNOWN\"] is State.UNKNOWN\n\n    with pytest.raises(TypeError):\n        _ = State.OK < State.WARN  # type: ignore[operator]\n\n\n@pytest.mark.parametrize(\n    \"states, best_state\",\n    [\n        ((State.OK,), State.OK),\n        ((State.OK, State.WARN), State.OK),\n        ((State.OK, State.WARN, State.UNKNOWN), State.OK),\n        ((State.OK, State.WARN, State.UNKNOWN, State.CRIT), State.OK),\n        ((State.WARN,), State.WARN),\n        ((State.WARN, State.UNKNOWN), State.WARN),\n        ((State.WARN, State.UNKNOWN, State.CRIT), State.WARN),\n        ((State.UNKNOWN,), State.UNKNOWN),\n        ((State.UNKNOWN, State.CRIT), State.UNKNOWN),\n        ((State.CRIT,), State.CRIT),\n        ((0, 1, 2, 3, State.UNKNOWN), State.OK),\n    ],\n)\ndef test_best_state(\n    states: Sequence[State],\n    best_state: State,\n) -> None:\n    assert State.best(*states) is best_state\n\n\ndef test_metric_kwarg() -> None:\n    with pytest.raises(TypeError):\n        _ = Metric(\"universe\", 42, (23, 23))  # type: ignore[misc]\n\n\n@pytest.mark.parametrize(\n    \"name, value, levels, boundaries\",\n    [\n        (\"\", 7, None, None),\n        (\"name\", \"7\", (None, None), (None, None)),\n        (\"n me\", \"7\", (None, None), (None, None)),\n        (\"n=me\", \"7\", (None, None), (None, None)),\n        (\"name\", 7, (\"warn\", \"crit\"), (None, None)),\n        (\"name\", 7, (23, 42), (None, \"max\")),\n    ],\n)\ndef test_metric_invalid(name: object, value: object, levels: object, boundaries: object) -> None:\n    with pytest.raises(TypeError):\n        _ = Metric(name, value, levels=levels, boundaries=boundaries)  # type: ignore[arg-type]\n\n\ndef test_metric() -> None:\n    metric1 = Metric(\"reproduction_rate\", 1.0, levels=(2.4, 3.0), boundaries=(0, None))\n    metric2 = Metric(\"reproduction_rate\", 2.0, levels=(2.4, 3.0), boundaries=(0, None))\n    assert metric1.name == \"reproduction_rate\"\n    assert metric1.value == 1.0\n    assert metric1.levels == (2.4, 3.0)\n    assert metric1.boundaries == (0.0, None)\n\n    assert metric1 == metric1  # noqa: PLR0124\n    assert metric1 != metric2\n\n\n@pytest.mark.parametrize(\n    \"state_, summary, notice, details\",\n    [\n        (8, \"foo\", None, None),\n        (State.OK, b\"foo\", None, None),\n        (State.OK, \"newline is a \\no-no\", None, None),\n        (State.OK, \"\", \"\", \"details\"),  # either is required\n        (State.OK, None, None, \"details\"),  # either is required\n        (State.OK, \"these are\", \"mutually exclusive\", None),\n        (State.OK, \"summary\", None, {\"at the moment\": \"impossible\", \"someday\": \"maybe\"}),\n    ],\n)\ndef test_result_invalid(state_: object, summary: object, notice: object, details: object) -> None:\n    with pytest.raises((TypeError, ValueError)):\n        _: Result = Result(\n            state=state_,\n            summary=summary,\n            notice=notice,\n            details=details,\n        )  # type: ignore[call-overload]\n\n\n@pytest.mark.parametrize(\n    \"state_, summary, notice, details, expected_triple\",\n    [\n        (State.OK, \"summary\", None, \"details\", (State.OK, \"summary\", \"details\")),\n        (State.OK, \"summary\", None, None, (State.OK, \"summary\", \"summary\")),\n        (State.OK, None, \"notice\", \"details\", (State.OK, \"\", \"details\")),\n        (State.OK, None, \"notice\", None, (State.OK, \"\", \"notice\")),\n        (State.WARN, \"summary\", None, \"details\", (State.WARN, \"summary\", \"details\")),\n        (State.WARN, \"summary\", None, None, (State.WARN, \"summary\", \"summary\")),\n        (State.WARN, None, \"notice\", \"details\", (State.WARN, \"notice\", \"details\")),\n        (State.WARN, None, \"notice\", None, (State.WARN, \"notice\", \"notice\")),\n    ],\n)\ndef test_result(\n    state_: State,\n    summary: str | None,\n    notice: str | None,\n    details: str | None,\n    expected_triple: tuple[State, str, str],\n) -> None:\n    result: Result = Result(\n        state=state_,\n        summary=summary,\n        notice=notice,\n        details=details,\n    )  # type: ignore[call-overload]\n    assert (result.state, result.summary, result.details) == expected_triple\n    assert result != Result(state=state_, summary=\"a different summary\")\n\n\ndef test_ignore_results() -> None:\n    result1 = IgnoreResults()\n    result2 = IgnoreResults(\"Login to DB failed\")\n    assert repr(result1) == \"IgnoreResults('currently no results')\"\n    assert str(result2) == \"Login to DB failed\"\n    assert result1 != result2\n    assert result2 == IgnoreResults(\"Login to DB failed\")\n"}
{"type": "test_file", "path": "packages/cmk-agent-based/tests/cmk/agent_based/v1/__init__.py", "content": "#!/usr/bin/env python3\n# Copyright (C) 2023 Checkmk GmbH - License: GNU General Public License v2\n# This file is part of Checkmk (https://checkmk.com). It is subject to the terms and\n# conditions defined in the file COPYING, which is part of this source code package.\n"}
{"type": "test_file", "path": "packages/cmk-agent-based/tests/cmk/agent_based/v1/test_check_levels_predictive.py", "content": "#!/usr/bin/env python3\n# Copyright (C) 2021 Checkmk GmbH - License: GNU General Public License v2\n# This file is part of Checkmk (https://checkmk.com). It is subject to the terms and\n# conditions defined in the file COPYING, which is part of this source code package.\n\nimport time\nfrom pathlib import Path\n\nfrom cmk.agent_based.prediction_backend import PredictionInfo, PredictionParameters\nfrom cmk.agent_based.v1 import check_levels_predictive, Result\n\n_PredictiveLevels = tuple[\n    float | None, tuple[float | None, float | None, float | None, float | None]\n]\n\n\ndef _get_test_levels_v1(metric: str, template: str) -> dict[str, object]:\n    params = PredictionParameters(\n        horizon=90,\n        period=\"wday\",\n        levels=(\"absolute\", (23.0, 42.0)),\n    )\n    meta = PredictionInfo.make(metric, \"upper\", params, time.time())\n    return {\n        \"period\": params.period,\n        \"horizon\": params.horizon,\n        \"levels_upper\": params.levels,\n        \"__injected__\": {\n            \"predictions\": {hash(meta): (45.45, (23.0, 100))},\n            \"meta_file_path_template\": template,\n        },\n    }\n\n\ndef test_check_levels_predictive_default_render_func() -> None:\n    metric_name = \"my_test_metric\"\n    result = next(\n        check_levels_predictive(\n            42.42,\n            metric_name=metric_name,\n            levels=_get_test_levels_v1(metric_name, \"\"),\n        )\n    )\n\n    assert isinstance(result, Result)\n    assert result.summary.startswith(\"42.42\")\n\n\ndef test_check_levels_predictive_prediction_exists(tmpdir: Path) -> None:\n    metric_name = \"my_test_metric\"\n    template = str(tmpdir / \"testfile\")\n    _result = next(\n        check_levels_predictive(\n            42.42,\n            metric_name=metric_name,\n            levels=_get_test_levels_v1(metric_name, template),\n        )\n    )\n\n    assert not Path(template).exists()\n\n\ndef test_check_levels_predictive_prediction_not_found(tmpdir: Path) -> None:\n    other_metric_name = \"my_other_test_metric\"\n    metric_name = \"my_test_metric\"\n    template = str(tmpdir / \"testfile\")\n    _result = next(\n        check_levels_predictive(\n            42.42,\n            metric_name=other_metric_name,\n            levels=_get_test_levels_v1(metric_name, template),\n        )\n    )\n\n    assert Path(template).exists()\n"}
{"type": "test_file", "path": "packages/cmk-agent-based/tests/cmk/agent_based/v1/test_clusterize.py", "content": "#!/usr/bin/env python3\n# Copyright (C) 2019 Checkmk GmbH - License: GNU General Public License v2\n# This file is part of Checkmk (https://checkmk.com). It is subject to the terms and\n# conditions defined in the file COPYING, which is part of this source code package.\n# import pytest\n\nfrom collections.abc import Iterable\n\nfrom cmk.agent_based.v1 import IgnoreResults, IgnoreResultsError, Metric, Result, State\nfrom cmk.agent_based.v1.clusterize import make_node_notice_results\n\n_OK_RESULT = Result(state=State.OK, summary=\"I am fine\")\n\n_WARN_RESULT = Result(state=State.WARN, summary=\"Watch out\")\n\n\ndef _check_function_node(\n    test_results: Iterable[Result | Metric | IgnoreResults],\n) -> Iterable[Result | Metric | IgnoreResults]:\n    yield from test_results\n\n\ndef test_node_returns_nothing() -> None:\n    assert not list(make_node_notice_results(\"test_node\", _check_function_node(())))\n    assert not list(make_node_notice_results(\"test_node\", ()))\n\n\ndef test_node_raises() -> None:\n    def _check_node_raises() -> Iterable[IgnoreResults]:\n        yield from ()\n        raise IgnoreResultsError()\n\n    assert not list(make_node_notice_results(\"test_node\", _check_node_raises()))\n\n\ndef test_node_ignore_results() -> None:\n    node_results = _check_function_node((_OK_RESULT, IgnoreResults()))\n    assert not list(make_node_notice_results(\"test_node\", node_results))\n\n\ndef test_node_returns_metric() -> None:\n    node_results = _check_function_node((_OK_RESULT, Metric(\"panic\", 42)))\n    assert list(make_node_notice_results(\"test_node\", node_results)) == [\n        Result(state=State.OK, notice=\"[test_node]: I am fine\"),\n    ]\n\n\ndef test_node_returns_details_only() -> None:\n    node_results = _check_function_node((Result(state=State.OK, notice=\"This is detailed\"),))\n    assert list(make_node_notice_results(\"test_node\", node_results)) == [\n        Result(state=State.OK, notice=\"[test_node]: This is detailed\"),\n    ]\n\n\ndef test_node_returns_ok_and_warn() -> None:\n    node_results = _check_function_node((_OK_RESULT, _WARN_RESULT))\n    assert list(make_node_notice_results(\"test_node\", node_results)) == [\n        Result(state=State.OK, notice=\"[test_node]: I am fine\"),\n        Result(state=State.WARN, notice=\"[test_node]: Watch out\"),\n    ]\n\n\ndef test_node_mutliline() -> None:\n    node_results = (Result(state=State.WARN, notice=\"These\\nare\\nfour\\nlines\"),)\n    assert list(make_node_notice_results(\"test_node\", _check_function_node(node_results))) == [\n        Result(\n            state=State.WARN,\n            summary=\"[test_node]: These, are, four, lines\",\n            details=(\"[test_node]: These\\n[test_node]: are\\n[test_node]: four\\n[test_node]: lines\"),\n        ),\n    ]\n"}
{"type": "test_file", "path": "packages/cmk-agent-based/tests/cmk/agent_based/v1/test_render_api.py", "content": "#!/usr/bin/env python3\n# Copyright (C) 2019 Checkmk GmbH - License: GNU General Public License v2\n# This file is part of Checkmk (https://checkmk.com). It is subject to the terms and\n# conditions defined in the file COPYING, which is part of this source code package.\n\nimport time\n\nimport pytest\n\nfrom cmk.agent_based.v1 import render\n\n\n@pytest.mark.parametrize(\n    \"epoch, output\",\n    [\n        (0, \"1970-01-01\"),\n        (1587908220, \"2020-04-26\"),\n        (1587908220.0, \"2020-04-26\"),\n    ],\n)\ndef test_date(epoch: float | None, output: str, monkeypatch: pytest.MonkeyPatch) -> None:\n    monkeypatch.setattr(time, \"localtime\", time.gmtime)\n    assert output == render.date(epoch=epoch)\n\n\n@pytest.mark.parametrize(\n    \"epoch, output\",\n    [\n        (0, \"1970-01-01 00:00:00\"),\n        (1587908220, \"2020-04-26 13:37:00\"),\n        (1587908220.0, \"2020-04-26 13:37:00\"),\n    ],\n)\ndef test_datetime(epoch: float | None, output: str, monkeypatch: pytest.MonkeyPatch) -> None:\n    monkeypatch.setattr(time, \"localtime\", time.gmtime)\n    assert output == render.datetime(epoch=epoch)\n\n\n@pytest.mark.parametrize(\n    \"seconds, output\",\n    [\n        (0, \"0 seconds\"),\n        (0.00000001, \"10 nanoseconds\"),\n        (5.3991e-05, \"54 microseconds\"),\n        (0.1, \"100 milliseconds\"),\n        (22, \"22 seconds\"),\n        (158, \"2 minutes 38 seconds\"),\n        (98, \"1 minute 38 seconds\"),\n        (1234567, \"14 days 6 hours\"),\n        (31536001, \"1 year 0 days\"),\n    ],\n)\ndef test_timespan(seconds: float, output: str) -> None:\n    assert output == render.timespan(seconds=seconds)\n\n\ndef test_timespan_negative() -> None:\n    with pytest.raises(ValueError):\n        _ = render.timespan(seconds=-1.0)\n\n\n@pytest.mark.parametrize(\n    \"value, output\",\n    [\n        (0, 1),\n        (1, 1),\n        (45.123123, 2),\n        (1e3, 4),\n        (1e5, 6),\n        (-2, 1),\n    ],\n)\ndef test__digits_left(value: float, output: int) -> None:\n    assert output == render._digits_left(value)  # noqa: SLF001\n\n\n@pytest.mark.parametrize(\n    \"value, use_si_units, output\",\n    [\n        (0, True, (\"0.00\", \"B\")),\n        (1, True, (\"1.00\", \"B\")),\n        (101.123, True, (\"101\", \"B\")),\n        (101.623, True, (\"102\", \"B\")),\n        (1000.0, True, (\"1.00\", \"kB\")),\n        (10001.623, True, (\"10.0\", \"kB\")),\n        (100000.0, True, (\"100\", \"kB\")),\n        (0, False, (\"0.00\", \"B\")),\n        (-123.123, True, (\"-123\", \"B\")),\n    ],\n)\ndef test__auto_scale(value: float, use_si_units: bool, output: tuple[str, str]) -> None:\n    assert output == render._auto_scale(value, use_si_units)  # noqa: SLF001\n\n\n@pytest.mark.parametrize(\n    \"bytes_, output\",\n    [\n        (0, \"0 B\"),\n        (1, \"1 B\"),\n        (2345, \"2.35 kB\"),\n        (1024**2, \"1.05 MB\"),\n        (1000**2, \"1.00 MB\"),\n        (1234000, \"1.23 MB\"),\n        (12340006, \"12.3 MB\"),\n        (123400067, \"123 MB\"),\n        (1234000678, \"1.23 GB\"),\n        (-17408, \"-17.4 kB\"),\n    ],\n)\ndef test_disksize(bytes_: float, output: str) -> None:\n    assert output == render.disksize(bytes_)\n\n\n@pytest.mark.parametrize(\n    \"bytes_, output\",\n    [\n        (0, \"0 B\"),\n        (1, \"1 B\"),\n        (2345, \"2.29 KiB\"),\n        (1024**2, \"1.00 MiB\"),\n        (1000**2, \"977 KiB\"),\n        (1234000, \"1.18 MiB\"),\n        (12340006, \"11.8 MiB\"),\n        (123400067, \"118 MiB\"),\n        (1234000678, \"1.15 GiB\"),\n        (-17408, \"-17.0 KiB\"),\n    ],\n)\ndef test_bytes(bytes_: float, output: str) -> None:\n    assert output == render.bytes(bytes_)\n\n\n@pytest.mark.parametrize(\n    \"bytes_, output\",\n    [\n        (0, \"0 B\"),\n        (1, \"1 B\"),\n        (2345, \"2,345 B\"),\n        (1024**2, \"1,048,576 B\"),\n        (1000**2, \"1,000,000 B\"),\n        (600000, \"600,000 B\"),\n        (1234000678, \"1,234,000,678 B\"),\n        (-1234000678, \"-1,234,000,678 B\"),\n    ],\n)\ndef test_filesize(bytes_: float, output: str) -> None:\n    assert output == render.filesize(bytes_)\n\n\n@pytest.mark.parametrize(\n    \"octets_per_sec, output\",\n    [\n        (0, \"0 Bit/s\"),\n        (1, \"8 Bit/s\"),\n        (2345, \"18.8 kBit/s\"),\n        (1.25 * 10**5, \"1 MBit/s\"),\n        (1.25 * 10**6, \"10 MBit/s\"),\n        (1.25 * 10**7, \"100 MBit/s\"),\n        (1234000678, \"9.87 GBit/s\"),\n        (-1234000678, \"-9.87 GBit/s\"),\n    ],\n)\ndef test_nicspeed(octets_per_sec: float, output: str) -> None:\n    assert output == render.nicspeed(octets_per_sec)\n\n\n@pytest.mark.parametrize(\n    \"octets_per_sec, output\",\n    [\n        (0, \"0.00 Bit/s\"),\n        (1, \"8.00 Bit/s\"),\n        (2345, \"18.8 kBit/s\"),\n        (1.25 * 10**5, \"1.00 MBit/s\"),\n        (1.25 * 10**6, \"10.0 MBit/s\"),\n        (1.25 * 10**7, \"100 MBit/s\"),\n        (-1.25 * 10**7, \"-100 MBit/s\"),\n        (1234000678, \"9.87 GBit/s\"),\n        (-1234000678, \"-9.87 GBit/s\"),\n    ],\n)\ndef test_networkbandwitdh(octets_per_sec: float, output: str) -> None:\n    assert output == render.networkbandwidth(octets_per_sec)\n\n\n@pytest.mark.parametrize(\n    \"bytes_, output\",\n    [\n        (0, \"0.00 B/s\"),\n        (1, \"1.00 B/s\"),\n        (2345, \"2.35 kB/s\"),\n        (1024**2, \"1.05 MB/s\"),\n        (1000**2, \"1.00 MB/s\"),\n        (-(1000**2), \"-1.00 MB/s\"),\n        (1234000678, \"1.23 GB/s\"),\n        (-1234000678, \"-1.23 GB/s\"),\n    ],\n)\ndef test_iobandwidth(bytes_: float, output: str) -> None:\n    assert output == render.iobandwidth(bytes_)\n\n\n@pytest.mark.parametrize(\n    \"percentage, output\",\n    [\n        # 1. Die 0 selbst:\n        (0.0, \"0%\"),\n        # 2. Bereich ]0, 0.01[:\n        (0.000102, \"<0.01%\"),\n        # 3. Bereich [1 ... 99]:\n        # -> Ausgabe mit 2 Nachkommastellen\n        (1.0, \"1.00%\"),\n        (1.234, \"1.23%\"),\n        (10.80, \"10.80%\"),\n        (99.92, \"99.92%\"),\n        # 5. Bereich [100,\n        (100.01, \"100.01%\"),\n        (123.456, \"123.46%\"),\n    ],\n)\ndef test_percent(percentage: float, output: str) -> None:\n    assert output == render.percent(percentage)\n    # 6. Bereich kleiner 0:\n    #     negieren und \"-\" davorhÃ¤ngen\n    if abs(percentage) >= 0.01:\n        assert f\"-{output}\" == render.percent(-percentage)\n\n\n@pytest.mark.parametrize(\n    \"hz, output\",\n    [\n        (111, \"111 Hz\"),\n        (1112, \"1.11 kHz\"),\n        (111222, \"111 kHz\"),\n        (111222333, \"111 MHz\"),\n        (111222333444, \"111 GHz\"),\n    ],\n)\ndef test_frequency(hz: float, output: str) -> None:\n    assert output == render.frequency(hz)\n"}
{"type": "test_file", "path": "agents/wnx/tests/regression/test_section_wmi_webservices.py", "content": "#!/usr/bin/env python3\n# Copyright (C) 2019 Checkmk GmbH - License: GNU General Public License v2\n# This file is part of Checkmk (https://checkmk.com). It is subject to the terms and\n# conditions defined in the file COPYING, which is part of this source code package.\n\nimport os\nimport re\nfrom itertools import chain, repeat\n\nimport pytest\n\nfrom . import it_utils\nfrom .local import local_test\n\n\nclass Globals:\n    alone = True\n\n\n@pytest.fixture(name=\"testfile\")\ndef testfile_engine():\n    return os.path.basename(__file__)\n\n\n@pytest.fixture(\n    name=\"testconfig\",\n    params=[(\"wmi_webservices\", True), (\"wmi_webservices\", False)],\n    ids=[\"sections=wmi_webservices\", \"sections=wmi_webservices_systemtime\"],\n)\ndef testconfig_engine(request, make_yaml_config):\n    Globals.alone = request.param[1]\n    if Globals.alone:\n        make_yaml_config[\"global\"][\"sections\"] = request.param[0]\n    else:\n        make_yaml_config[\"global\"][\"sections\"] = [request.param[0], \"%s systemtime\"]\n    return make_yaml_config\n\n\n@pytest.fixture(name=\"expected_output\")\ndef expected_output_engine():\n    re_str = (\n        r\"^\\d+,\\d+,\\d+,\\d+,,\\d+,\\d+,\\d+,\\d+,\\d+,\\d+,\\d+,\\d+,\\d+,\\d+,\\d+,\\d+,\"\n        r\"\\d+,,\\d+,\\d+,\\d+,\\d+,\\d+,\\d+,\\d+,\\d+,\\d+,\\d+,\\d+,\\d+,\\d+,\\d+,\\d+,\"\n        r\"\\d+,\\d+,\\d+,\\d+,\\d+,\\d+,\\d+,[^,]+,\\d+,\\d+,\\d+,\\d+,\\d+,\\d+,\\d+,\"\n        r\"\\d+,\\d+,\\d+,\\d+,\\d+,\\d+,\\d+,\\d+,\\d+,\\d+,\\d+,\\d+,\\d+,\\d+,\\d+,\\d+,\"\n        r\"\\d+,\\d+,\\d+,\\d+,\\d+,\\d+,\\d+,\\d+,\\d+,\\d+,\\d+,\\d+,\\d+,\\d+,\\d+,\\d+,\"\n        r\"\\d+,\\d+,\\d+,\\d+,\\d+,\\d+,\\d+,\\d+,\\d+,\\d+,\\d+,\\d+,\\d+,\\d+,\\b(?:OK|Timeout)\\b\"\n    ).replace(\",\", \"\\\\|\")\n    if not Globals.alone:\n        re_str += r\"|\" + re.escape(r\"<<<systemtime>>>\") + r\"|\\d+\"\n    re_str += r\"$\"\n    return chain(\n        [\n            re.escape(r\"<<<wmi_webservices:sep(124)>>>\"),\n            (\n                r\"AnonymousUsersPersec,BytesReceivedPersec,BytesSentPersec,\"\n                r\"BytesTotalPersec,Caption,CGIRequestsPersec,\"\n                r\"ConnectionAttemptsPersec,CopyRequestsPersec,\"\n                r\"CurrentAnonymousUsers,CurrentBlockedAsyncIORequests,\"\n                r\"Currentblockedbandwidthbytes,\"\n                r\"CurrentCALcountforauthenticatedusers,\"\n                r\"CurrentCALcountforSSLconnections,CurrentCGIRequests,\"\n                r\"CurrentConnections,CurrentISAPIExtensionRequests,\"\n                r\"CurrentNonAnonymousUsers,DeleteRequestsPersec,Description,\"\n                r\"FilesPersec,FilesReceivedPersec,FilesSentPersec,\"\n                r\"Frequency_Object,Frequency_PerfTime,Frequency_Sys100NS,\"\n                r\"GetRequestsPersec,HeadRequestsPersec,\"\n                r\"ISAPIExtensionRequestsPersec,LockedErrorsPersec,\"\n                r\"LockRequestsPersec,LogonAttemptsPersec,MaximumAnonymousUsers,\"\n                r\"MaximumCALcountforauthenticatedusers,\"\n                r\"MaximumCALcountforSSLconnections,MaximumCGIRequests,\"\n                r\"MaximumConnections,MaximumISAPIExtensionRequests,\"\n                r\"MaximumNonAnonymousUsers,MeasuredAsyncIOBandwidthUsage,\"\n                r\"MkcolRequestsPersec,MoveRequestsPersec,Name,\"\n                r\"NonAnonymousUsersPersec,NotFoundErrorsPersec,\"\n                r\"OptionsRequestsPersec,OtherRequestMethodsPersec,\"\n                r\"PostRequestsPersec,PropfindRequestsPersec,\"\n                r\"ProppatchRequestsPersec,PutRequestsPersec,SearchRequestsPersec,\"\n                r\"ServiceUptime,Timestamp_Object,Timestamp_PerfTime,\"\n                r\"Timestamp_Sys100NS,TotalAllowedAsyncIORequests,\"\n                r\"TotalAnonymousUsers,TotalBlockedAsyncIORequests,\"\n                r\"Totalblockedbandwidthbytes,TotalBytesReceived,TotalBytesSent,\"\n                r\"TotalBytesTransferred,TotalCGIRequests,\"\n                r\"TotalConnectionAttemptsallinstances,TotalCopyRequests,\"\n                r\"TotalcountoffailedCALrequestsforauthenticatedusers,\"\n                r\"TotalcountoffailedCALrequestsforSSLconnections,\"\n                r\"TotalDeleteRequests,TotalFilesReceived,TotalFilesSent,\"\n                r\"TotalFilesTransferred,TotalGetRequests,TotalHeadRequests,\"\n                r\"TotalISAPIExtensionRequests,TotalLockedErrors,TotalLockRequests,\"\n                r\"TotalLogonAttempts,TotalMethodRequests,\"\n                r\"TotalMethodRequestsPersec,TotalMkcolRequests,\"\n                r\"TotalMoveRequests,TotalNonAnonymousUsers,TotalNotFoundErrors,\"\n                r\"TotalOptionsRequests,TotalOtherRequestMethods,TotalPostRequests,\"\n                r\"TotalPropfindRequests,TotalProppatchRequests,TotalPutRequests,\"\n                r\"TotalRejectedAsyncIORequests,TotalSearchRequests,\"\n                r\"TotalTraceRequests,TotalUnlockRequests,TraceRequestsPersec,\"\n                r\"UnlockRequestsPersec,WMIStatus\"\n            ).replace(\",\", \"\\\\|\"),\n        ],\n        repeat(re_str),\n    )\n\n\ndef test_section_wmi_webservices(  # type: ignore[no-untyped-def]\n    request, testconfig, expected_output, actual_output, testfile\n) -> None:\n    # special case wmi may timeout\n    required_lines = 3\n    name = \"webservices\"\n\n    if not it_utils.check_actual_input(name, required_lines, Globals.alone, actual_output):\n        return\n\n    local_test(expected_output, actual_output, testfile, request.node.name)\n"}
{"type": "test_file", "path": "packages/cmk-agent-based/tests/cmk/agent_based/v1/test_regex.py", "content": "#!/usr/bin/env python3\n# Copyright (C) 2019 Checkmk GmbH - License: GNU General Public License v2\n# This file is part of Checkmk (https://checkmk.com). It is subject to the terms and\n# conditions defined in the file COPYING, which is part of this source code package.\n\nfrom cmk.agent_based.v1 import regex\n\n\ndef test_regex() -> None:\n    \"\"\"\n    We use `re.compile`. This is just a sanity check.\"\"\"\n    assert regex(\"foo\").match(\"foobar\")\n"}
{"type": "test_file", "path": "packages/cmk-agent-based/tests/cmk/agent_based/v1/test_check_levels.py", "content": "#!/usr/bin/env python3\n# Copyright (C) 2019 Checkmk GmbH - License: GNU General Public License v2\n# This file is part of Checkmk (https://checkmk.com). It is subject to the terms and\n# conditions defined in the file COPYING, which is part of this source code package.\n\nimport math\nfrom collections.abc import Callable\n\nimport pytest\n\nfrom cmk.agent_based.v1 import check_levels as check_levels_v1\nfrom cmk.agent_based.v1 import Metric, render, Result, State\nfrom cmk.agent_based.v1._check_levels import _do_check_levels as do_check_levels\n\n\ndef _format_float(x: float) -> str:\n    return f\"{x:.1f}\"\n\n\ndef _format_meter(x: float) -> str:\n    return f\"{x:.1f} m\"\n\n\n@pytest.mark.parametrize(\n    \"value, levels_upper, levels_lower, render_func, result\",\n    [\n        (5, (3, 6), None, int, (State.WARN, \" (warn/crit at 3/6)\")),\n        (7, (3, 6), None, _format_meter, (State.CRIT, \" (warn/crit at 3.0 m/6.0 m)\")),\n        (7, (3, 6), None, _format_float, (State.CRIT, \" (warn/crit at 3.0/6.0)\")),\n        (2, (3, 6), (1, 0), int, (State.OK, \"\")),\n        (1, (3, 6), (1, 0), int, (State.OK, \"\")),\n        (0, (3, 6), (1, 0), int, (State.WARN, \" (warn/crit below 1/0)\")),\n        (-1, (3, 6), (1, 0), int, (State.CRIT, \" (warn/crit below 1/0)\")),\n    ],\n)\ndef test_boundaries(\n    value: float,\n    levels_upper: tuple[float, float] | None,\n    levels_lower: tuple[float, float] | None,\n    render_func: Callable[[float], str],\n    result: tuple[State, str],\n) -> None:\n    assert do_check_levels(value, levels_upper, levels_lower, render_func) == result\n\n\ndef test_check_levels_wo_levels() -> None:\n    assert list(check_levels_v1(5, metric_name=\"battery\", render_func=render.percent)) == [\n        Result(state=State.OK, summary=\"5.00%\"),\n        Metric(\"battery\", 5.0),\n    ]\n\n\ndef test_check_levels_ok_levels() -> None:\n    assert list(\n        check_levels_v1(\n            5, metric_name=\"battery\", render_func=render.percent, levels_upper=(100, 200)\n        )\n    ) == [\n        Result(state=State.OK, summary=\"5.00%\"),\n        Metric(\"battery\", 5.0, levels=(100.0, 200.0)),\n    ]\n\n\ndef test_check_levels_warn_levels() -> None:\n    def _format_years(x: float) -> str:\n        return f\"{x:.2f} years\"\n\n    assert list(\n        check_levels_v1(\n            6,\n            metric_name=\"disk\",\n            levels_upper=(4, 8),\n            render_func=_format_years,\n            label=\"Disk Age\",\n        )\n    ) == [\n        Result(\n            state=State.WARN,\n            summary=\"Disk Age: 6.00 years (warn/crit at 4.00 years/8.00 years)\",\n        ),\n        Metric(\"disk\", 6.0, levels=(4.0, 8.0)),\n    ]\n\n\ndef test_check_levels_boundaries() -> None:\n    def _format_ph(x: float) -> str:\n        return f\"pH {-math.log10(x):.1f}\"\n\n    assert list(\n        check_levels_v1(\n            5e-7,\n            metric_name=\"H_concentration\",\n            levels_upper=(4e-7, 8e-7),\n            levels_lower=(5e-8, 2e-8),\n            render_func=_format_ph,\n            label=\"Water acidity\",\n            boundaries=(0, None),\n        )\n    ) == [\n        Result(state=State.WARN, summary=\"Water acidity: pH 6.3 (warn/crit at pH 6.4/pH 6.1)\"),\n        Metric(\"H_concentration\", 5e-7, levels=(4e-7, 8e-7), boundaries=(0, None)),\n    ]\n"}
{"type": "test_file", "path": "packages/cmk-agent-based/tests/cmk/agent_based/v1/test_inventory_classes.py", "content": "#!/usr/bin/env python3\n# Copyright (C) 2019 Checkmk GmbH - License: GNU General Public License v2\n# This file is part of Checkmk (https://checkmk.com). It is subject to the terms and\n# conditions defined in the file COPYING, which is part of this source code package.\nimport pytest\n\nfrom cmk.agent_based.v1 import Attributes, TableRow\n\n\n@pytest.mark.parametrize(\"path\", [[\"a\", 23], (\"a\", \"b\")])\ndef test_common_raise_path_type(path: object) -> None:\n    with pytest.raises(TypeError):\n        _ = TableRow(path=path, key_columns={})  # type: ignore[arg-type]\n    with pytest.raises(TypeError):\n        _ = Attributes(path=path)  # type: ignore[arg-type]\n\n\ndef test_common_kwarg_only() -> None:\n    with pytest.raises(TypeError):\n        _ = Attributes([\"a\"])  # type: ignore[misc]\n    with pytest.raises(TypeError):\n        _ = TableRow([\"a\"], key_columns={\"ding\": \"dong\"})  # type: ignore[misc]\n\n\ndef test_atrributes_wrong_types() -> None:\n    with pytest.raises(TypeError):\n        _ = Attributes(\n            path=[\"software\", \"os\"],\n            inventory_attributes={\"version\": (42, 23)},  # type: ignore[dict-item]\n        )\n\n\ndef test_attributes_duplicate_keys() -> None:\n    with pytest.raises(ValueError):\n        _ = Attributes(\n            path=[\"software\", \"os\"],\n            inventory_attributes={\"version\": \"42\"},\n            status_attributes={\"version\": \"42\"},\n        )\n\n\ndef test_attributes_instanciated() -> None:\n    attr = Attributes(\n        path=[\"software\", \"os\"],\n        status_attributes={\"vendor\": \"emmentaler\"},\n        inventory_attributes={\"version\": \"42\"},\n    )\n\n    assert attr.path == [\"software\", \"os\"]\n    assert attr.status_attributes == {\"vendor\": \"emmentaler\"}\n    assert attr.inventory_attributes == {\"version\": \"42\"}\n    assert repr(attr) == (\n        \"Attributes(\"\n        \"path=['software', 'os'], \"\n        \"inventory_attributes={'version': '42'}, \"\n        \"status_attributes={'vendor': 'emmentaler'})\"\n    )\n\n    attr2 = Attributes(\n        path=[\"software\", \"os\"],\n        status_attributes={\"vendor\": \"camembert\"},\n        inventory_attributes={\"version\": \"42\"},\n    )\n    assert attr == attr  # noqa: PLR0124\n    assert attr2 != attr\n\n\ndef test_tablerow_missing_key_columns() -> None:\n    with pytest.raises(TypeError):\n        _ = TableRow(path=[\"hardware\"], key_columns=None)  # type: ignore[arg-type]\n        _ = TableRow(path=[\"hardware\"], key_columns={})\n\n\ndef test_tablerow_wrong_types() -> None:\n    with pytest.raises(TypeError):\n        _ = TableRow(path=[\"hardware\"], key_columns={23: 42})  # type: ignore[dict-item]\n\n\ndef test_tablerow_conflicting_keys() -> None:\n    with pytest.raises(ValueError):\n        _ = TableRow(\n            path=[\"hardware\"],\n            key_columns={\"foo\": \"bar\"},\n            status_columns={\"foo\": \"bar\"},\n        )\n\n\ndef test_tablerow_instanciated() -> None:\n    table_row = TableRow(\n        path=[\"software\", \"os\"],\n        key_columns={\"foo\": \"bar\"},\n        status_columns={\"packages\": 42},\n        inventory_columns={\"vendor\": \"emmentaler\"},\n    )\n\n    assert table_row.path == [\"software\", \"os\"]\n    assert table_row.key_columns == {\"foo\": \"bar\"}\n    assert table_row.status_columns == {\"packages\": 42}\n    assert table_row.inventory_columns == {\"vendor\": \"emmentaler\"}\n    assert repr(table_row) == (\n        \"TableRow(\"\n        \"path=['software', 'os'], \"\n        \"key_columns={'foo': 'bar'}, \"\n        \"inventory_columns={'vendor': 'emmentaler'}, \"\n        \"status_columns={'packages': 42})\"\n    )\n\n    table_row2 = TableRow(\n        path=[\"software\", \"os\"],\n        key_columns={\"foo\": \"bar\"},\n        status_columns={\"packages\": 42},\n        inventory_columns={\"vendor\": \"gorgonzola\"},\n    )\n    assert table_row == table_row  # noqa: PLR0124\n    assert table_row2 != table_row\n"}
{"type": "test_file", "path": "packages/cmk-agent-based/tests/cmk/agent_based/test_v0_unstable_legacy.py", "content": "#!/usr/bin/env python3\n# Copyright (C) 2019 Checkmk GmbH - License: GNU General Public License v2\n# This file is part of Checkmk (https://checkmk.com). It is subject to the terms and\n# conditions defined in the file COPYING, which is part of this source code package.\n\n\nimport math\nfrom collections.abc import Callable, Mapping\n\nimport pytest\n\nfrom cmk.agent_based.legacy.v0_unstable import _do_check_levels, check_levels, LegacyResult\n\n\ndef _rmeter(x: float | int) -> str:\n    return \"%.1f m\" % x\n\n\ndef _rint(x: float | int) -> str:\n    return str(int(x))\n\n\n@pytest.mark.parametrize(\n    \"value, levels, representation, result\",\n    [\n        (5, (3, 6), _rint, (1, \" (warn/crit at 3/6)\")),\n        (7, (3, 6), _rmeter, (2, \" (warn/crit at 3.0 m/6.0 m)\")),\n        (2, (3, 6, 1, 0), _rint, (0, \"\")),\n        (1, (3, 6, 1, 0), _rint, (0, \"\")),\n        (0, (3, 6, 1, 0), _rint, (1, \" (warn/crit below 1/0)\")),\n        (-1, (3, 6, 1, 0), _rint, (2, \" (warn/crit below 1/0)\")),\n    ],\n)\ndef test_boundaries(\n    value: int,\n    levels: tuple[int, int, int, int],\n    representation: Callable[[object], object],\n    result: tuple[int, str],\n) -> None:\n    assert _do_check_levels(value, levels, representation) == result\n\n\n@pytest.mark.parametrize(\n    \"value, dsname, params, kwargs, result\",\n    [\n        (\n            6,\n            \"disk\",\n            (4, 8),\n            {\"unit\": \"years\", \"infoname\": \"Disk Age\"},\n            (1, \"Disk Age: 6.00 years (warn/crit at 4.00 years/8.00 years)\", [(\"disk\", 6.0, 4, 8)]),\n        ),\n        (\n            5e-7,\n            \"H_concentration\",\n            (4e-7, 8e-7, 5e-8, 2e-8),\n            {\n                \"human_readable_func\": lambda x: \"pH %.1f\" % -math.log10(x),\n                \"infoname\": \"Water acidity\",\n            },\n            (\n                1,\n                \"Water acidity: pH 6.3 (warn/crit at pH 6.4/pH 6.1)\",\n                [(\"H_concentration\", 5e-7, 4e-7, 8e-7)],\n            ),\n        ),\n        (\n            5e-7,\n            \"H_concentration\",\n            (4e-7, 8e-7, 5e-8, 2e-8),\n            {\n                \"human_readable_func\": lambda x: \"pH %.1f\" % -math.log10(x),\n                \"unit\": \"??\",\n                \"infoname\": \"Water acidity\",\n            },\n            (\n                1,\n                \"Water acidity: pH 6.3 ?? (warn/crit at pH 6.4 ??/pH 6.1 ??)\",\n                [(\"H_concentration\", 5e-7, 4e-7, 8e-7)],\n            ),\n        ),\n    ],\n)\ndef test_check_levels(\n    value: float,\n    dsname: str | None,\n    params: None | tuple[float, ...],\n    kwargs: Mapping[str, object],\n    result: LegacyResult,\n) -> None:\n    assert check_levels(value, dsname, params, **kwargs) == result  # type: ignore[arg-type]\n"}
{"type": "test_file", "path": "agents/wnx/tests/regression/test_section_mrpe.py", "content": "#!/usr/bin/env python3\n# Copyright (C) 2019 Checkmk GmbH - License: GNU General Public License v2\n# This file is part of Checkmk (https://checkmk.com). It is subject to the terms and\n# conditions defined in the file COPYING, which is part of this source code package.\n\nimport os\nimport platform\nimport re\nimport shutil\n\nimport pytest\n\nfrom .local import local_test, user_dir\n\n\nclass Globals:\n    section = \"mrpe\"\n    alone = True\n    pluginname = \"check_crit.bat\"\n    param = \"foobar\"\n    checkname = \"Dummy\"\n    mrpedir = \"mrpe\"\n    includedir = \"test include\"  # space in directory name!\n    cfgfile = \"test.cfg\"\n    newline = -1\n\n\n@pytest.fixture(name=\"testfile\")\ndef testfile_engine():\n    return os.path.basename(__file__)\n\n\n@pytest.fixture(name=\"testconfig\", params=[\"alone\", \"with_systemtime\"])\ndef testconfig_engine(request, make_yaml_config):\n    Globals.alone = request.param == \"alone\"\n    if Globals.alone:\n        make_yaml_config[\"global\"][\"sections\"] = Globals.section\n    else:\n        make_yaml_config[\"global\"][\"sections\"] = [Globals.section, \"systemtime\"]\n\n    if Globals.newline < 0:\n        make_yaml_config[\"mrpe\"] = {\n            \"enabled\": \"yes\",\n            \"timeout\": 60,\n            \"config\": [\n                f\"check = {Globals.checkname} '{os.path.join(Globals.mrpedir, Globals.pluginname)}'\"\n            ],\n        }\n    else:\n        make_yaml_config[\"mrpe\"] = {\n            \"enabled\": \"yes\",\n            \"timeout\": 60,\n            \"config\": [\"include = '%s'\" % os.path.join(Globals.includedir, Globals.cfgfile)],\n        }\n    return make_yaml_config\n\n\n@pytest.fixture(name=\"expected_output\")\ndef expected_output_engine():\n    expected = [\n        re.escape(r\"<<<%s>>>\" % Globals.section),\n        rf\"\\({Globals.pluginname}\\) {Globals.checkname} 2 CRIT - This check is always critical\",\n    ]\n    if not Globals.alone:\n        expected += [re.escape(r\"<<<systemtime>>>\"), r\"\\d+\"]\n    return expected\n\n\n@pytest.fixture(\n    params=[-1, 0, 1, 2],\n    ids=[\n        \"direct\",\n        \"include_without_newline\",\n        \"include_with_newline\",\n        \"include_with_newline_forward_slash\",\n    ],\n    autouse=True,\n)\ndef manage_plugin(request):\n    Globals.newline = request.param\n    plugin_dir = Globals.mrpedir if Globals.newline < 0 else Globals.includedir\n    source_dir = os.path.join(os.path.dirname(os.path.dirname(__file__)), \"files\\\\regression\")\n    target_dir = os.path.join(user_dir, plugin_dir)\n    if not os.path.exists(target_dir):\n        os.mkdir(target_dir)\n\n    shutil.copy(os.path.join(source_dir, Globals.pluginname), target_dir)\n\n    # create config file\n    if Globals.newline >= 0:\n        with open(os.path.join(target_dir, Globals.cfgfile), \"wb\") as cfg:\n            path = os.path.join(target_dir, Globals.pluginname)\n            if Globals.newline == 2:\n                path = path.replace(\"\\\\\", \"/\")\n            cfg_line = \"check = {} '{}'{}\".format(\n                Globals.checkname,\n                path,\n                \"\\n\" if Globals.newline > 0 else \"\",\n            )\n            cfg.write(str.encode(cfg_line))\n    yield\n    if platform.system() == \"Windows\":\n        os.unlink(os.path.join(target_dir, Globals.pluginname))\n        if Globals.newline >= 0:\n            os.unlink(os.path.join(target_dir, Globals.cfgfile))\n\n\ndef test_section_mrpe(request, testconfig, expected_output, actual_output, testfile) -> None:  # type: ignore[no-untyped-def]\n    # request.node.name gives test name\n    local_test(expected_output, actual_output, testfile, request.node.name)\n"}
{"type": "test_file", "path": "agents/wnx/tests/regression/test_section_mem.py", "content": "#!/usr/bin/env python3\n# Copyright (C) 2019 Checkmk GmbH - License: GNU General Public License v2\n# This file is part of Checkmk (https://checkmk.com). It is subject to the terms and\n# conditions defined in the file COPYING, which is part of this source code package.\n\nimport os\nimport re\n\nimport pytest\n\nfrom .local import local_test\n\n\nclass Globals:\n    section = \"mem\"\n    alone = True\n\n\n@pytest.fixture(name=\"testfile\")\ndef testfile_engine():\n    return os.path.basename(__file__)\n\n\n@pytest.fixture(name=\"testconfig\", params=[\"alone\", \"with_systemtime\"])\ndef testconfig_engine(request, make_yaml_config):\n    Globals.alone = request.param == \"alone\"\n    if Globals.alone:\n        make_yaml_config[\"global\"][\"sections\"] = Globals.section\n    else:\n        make_yaml_config[\"global\"][\"sections\"] = [Globals.section, \"systemtime\"]\n    return make_yaml_config\n\n\n@pytest.fixture(name=\"expected_output\")\ndef expected_output_engine():\n    expected = [\n        r\"<<<mem>>>\",  #\n        r\"MemTotal:\\s+\\d+\\skB\",  #\n        r\"MemFree:\\s+\\d+\\skB\",  #\n        r\"SwapTotal:\\s+\\d+\\skB\",  #\n        r\"SwapFree:\\s+\\d+\\skB\",  #\n        r\"PageTotal:\\s+\\d+\\skB\",  #\n        r\"PageFree:\\s+\\d+\\skB\",  #\n        r\"VirtualTotal:\\s+\\d+\\skB\",  #\n        r\"VirtualFree:\\s+\\d+\\skB\",  #\n    ]\n    if not Globals.alone:\n        expected += [re.escape(r\"<<<systemtime>>>\"), r\"\\d+\"]\n    return expected\n\n\ndef test_section_mem(  # type: ignore[no-untyped-def]\n    request, testconfig, expected_output, actual_output, testfile\n) -> None:\n    # request.node.name gives test name\n    local_test(expected_output, actual_output, testfile, request.node.name)\n"}
{"type": "test_file", "path": "agents/wnx/tests/regression/test_section_openhardwaremonitor.py", "content": "#!/usr/bin/env python3\n# Copyright (C) 2019 Checkmk GmbH - License: GNU General Public License v2\n# This file is part of Checkmk (https://checkmk.com). It is subject to the terms and\n# conditions defined in the file COPYING, which is part of this source code package.\n\nimport os\nimport platform\nimport re\nimport shutil\nimport time\nfrom itertools import chain, repeat\n\nimport pytest\n\nfrom . import it_utils\nfrom .local import local_test, user_dir\n\n\nclass Globals:\n    it_utils.stop_ohm()\n    alone = True\n\n\n@pytest.fixture(name=\"testfile\")\ndef testfile_engine():\n    return os.path.basename(__file__)\n\n\n@pytest.fixture(\n    name=\"testconfig\",\n    params=[(\"openhardwaremonitor\", True), (\"openhardwaremonitor\", False)],\n    ids=[\"sections=openhardwaremonitor\", \"sections=openhardwaremonitor_systemtime\"],\n)\ndef testconfig_engine(request, make_yaml_config):\n    Globals.alone = request.param[1]\n    if Globals.alone:\n        make_yaml_config[\"global\"][\"sections\"] = request.param[0]\n    else:\n        make_yaml_config[\"global\"][\"sections\"] = [request.param[0], \"systemtime\"]\n    return make_yaml_config\n\n\n@pytest.fixture\ndef wait_agent():\n    def inner():\n        # Wait a little so that OpenHardwareMonitorCLI.exe starts properly.\n        time.sleep(10)\n        return True\n\n    return inner\n\n\n@pytest.fixture(name=\"expected_output\")\ndef expected_output_engine():\n    re_str = (\n        r\"^\\d+,[^,]+,(\\/\\w+)+,(Power|Clock|Load|Data|Temperature),\\d+\\.\\d{6},\\b(?:OK|Timeout)\\b\"\n    )\n    if not Globals.alone:\n        re_str += r\"|\" + re.escape(r\"<<<systemtime>>>\") + r\"|\\d+\"\n    re_str += r\"$\"\n    return chain(\n        [re.escape(r\"<<<openhardwaremonitor:sep(44)>>>\"), r\"Index,Name,Parent,SensorType,Value\"],\n        repeat(re_str),\n    )\n\n\n@pytest.fixture(autouse=True)\ndef manage_ohm_binaries():\n    if platform.system() == \"Windows\":\n        binaries = [\"OpenHardwareMonitorCLI.exe\", \"OpenHardwareMonitorLib.dll\"]\n\n        source_dir = os.path.join(os.path.dirname(os.path.dirname(__file__)), \"files\\\\ohm\\\\cli\")\n        target_dir = os.path.join(user_dir, \"bin\")\n\n        it_utils.make_dir(target_dir)\n\n        for f in binaries:\n            shutil.copy(os.path.join(source_dir, f), target_dir)\n    yield\n    if platform.system() == \"Windows\":\n        it_utils.stop_ohm()\n        binaries.append(\"OpenHardwareMonitorLib.sys\")\n        it_utils.remove_files(target_dir, binaries)\n\n\ndef test_section_openhardwaremonitor(  # type: ignore[no-untyped-def]\n    request, testconfig, expected_output, actual_output, testfile\n) -> None:\n    required_lines = 2\n    name = \"openhardwaremonitor\"\n\n    if not it_utils.check_actual_input(name, required_lines, Globals.alone, actual_output):\n        return\n\n    # request.node.name gives test name\n    local_test(expected_output, actual_output, testfile, request.node.name)\n"}
{"type": "source_file", "path": "agents/modules/windows/patch_pipfile.py", "content": "#!/usr/bin/env python3\n# Copyright (C) 2019 Checkmk GmbH - License: GNU General Public License v2\n# This file is part of Checkmk (https://checkmk.com). It is subject to the terms and\n# conditions defined in the file COPYING, which is part of this source code package.\n\n# simple script to patch linux Pipfile to Windows pipfile\n# Deprecated now\n\nimport sys\n\nfrom colorama import (  # type: ignore[import-untyped]\n    Fore,\n    init,\n    Style,\n)\n\ninit()\n\nerror_c = Style.BRIGHT + Fore.RED\nok_c = Style.BRIGHT + Fore.GREEN\ninfo_c = Style.BRIGHT + Fore.CYAN\n\nif len(sys.argv) < 2:\n    print(error_c + \"Missing arguments\")\n    sys.exit(1)\n\ntry:\n    # Read in the file\n    print(info_c + f\"Opening '{sys.argv[1]}'...\")\n    with open(sys.argv[1]) as f:\n        lines = f.readlines()\n\n    # Replace the target string\n    with open(sys.argv[1], \"w\") as f:\n        for l in lines:\n            if l.find(\"psycopg2 = \") == 0:\n                f.write('psycopg2 = \"*\" # windows need new version \\n')\n            elif l.find(\"pymssql = \") == 0:\n                f.write(\"# \" + l)\n            elif l.find(\"mysqlclient = \") == 0:\n                f.write(\"# \" + l)\n            else:\n                f.write(l)\n\n    print(ok_c + \"Finished\")\nexcept Exception as e:\n    print(error_c + f\"Exception is {e}\")\n"}
{"type": "source_file", "path": "agents/plugins/apache_status.py", "content": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n# Copyright (C) 2019 Checkmk GmbH - License: GNU General Public License v2\n# This file is part of Checkmk (https://checkmk.com). It is subject to the terms and\n# conditions defined in the file COPYING, which is part of this source code package.\n\n# Checkmk-Agent-Plugin - Apache Server Status\n#\n# Fetches the server-status page from detected or configured apache\n# processes to gather status information about this apache process.\n#\n# To make this agent plugin work you have to load the status_module\n# into your apache process. It is also needed to enable the \"server-status\"\n# handler below the URL \"/server-status\".\n#\n# By default this plugin tries to detect all locally running apache processes\n# and to monitor them. If this is not good for your environment you might\n# create an apache_status.cfg file in MK_CONFDIR and populate the servers\n# list to prevent executing the detection mechanism.\n#\n# It is also possible to override or extend the ssl_ports variable to make the\n# check contact other ports than 443 with HTTPS requests.\n\nimport contextlib\nimport os\nimport re\nimport sys\nfrom urllib.error import HTTPError, URLError\nfrom urllib.request import build_opener, HTTPSHandler, install_opener, Request, urlopen\n\n__version__ = \"2.5.0b1\"\n\nUSER_AGENT = \"checkmk-agent-apache_status-\" + __version__\n\nif sys.version_info < (2, 6):\n    sys.stderr.write(\"ERROR: Python 2.5 is not supported. Please use Python 2.6 or newer.\\n\")\n    sys.exit(1)\n\n\ndef urlopen_(request, timeout=5, cafile=None, context=None):\n    if sys.version_info[0] == 2:\n        scheme = request.get_type()\n    else:\n        scheme = request.type\n    if scheme not in [\"http\", \"https\"]:\n        raise ValueError(\"Scheme '%s' is not allowed\" % scheme)\n    return urlopen(  # nosec B310 # BNS:6b61d9\n        request, timeout=timeout, cafile=cafile, context=context\n    )\n\n\ndef get_config():\n    config_dir = os.getenv(\"MK_CONFDIR\", \"/etc/check_mk\")\n    config_file = config_dir + \"/apache_status.conf\"\n\n    if not os.path.exists(config_file):\n        config_file = config_dir + \"/apache_status.cfg\"\n\n    # None or tuple of ((proto, cacert), ipaddress, port, instance_name).\n    #  - proto is 'http' or 'https'\n    #  - cacert is a path to a CA certificate, or None\n    #  - port may be None\n    #  - instance_name may be the empty string\n    config = {\n        \"servers\": None,\n        \"ssl_ports\": [443],\n    }\n    if os.path.exists(config_file):\n        with open(config_file) as config_file_obj:\n            exec(config_file_obj.read(), config)  # nosec B102 # BNS:a29406\n    return config\n\n\ndef get_instance_name(host, port_nr, conf):\n    \"\"\"\n    Get Instance name either from config\n    or from detected sites\n    \"\"\"\n    search = \"%s:%s\" % (host, port_nr)\n    if search in conf[\"custom\"]:\n        return conf[\"custom\"][search]\n    if \"omd_sites\" in conf:\n        return conf[\"omd_sites\"].get(search, host)\n    return \"\"\n\n\ndef parse_address_and_port(address_and_port, ssl_ports):\n    \"\"\"\n    parse address:port section from netstat or ss\n    return scheme (http|https), address, port\n    \"\"\"\n    server_address, _server_port = address_and_port.rsplit(\":\", 1)\n    server_port = int(_server_port)\n\n    # Use localhost when listening globally\n    if server_address == \"0.0.0.0\":  # nosec B104 # BNS:537c43\n        server_address = \"127.0.0.1\"\n    elif server_address in (\"::\", \"*\", \"[::]\"):\n        server_address = \"[::1]\"\n    elif \":\" in server_address and server_address[0] != \"[\":\n        server_address = \"[%s]\" % server_address\n\n    # Switch protocol if port is SSL port. In case you use SSL on another\n    # port you would have to change/extend the ssl_port list\n    if server_port in ssl_ports:\n        scheme = \"https\"\n    else:\n        scheme = \"http\"\n\n    return scheme, server_address, server_port\n\n\ndef try_detect_servers(ssl_ports):\n    results = []\n\n    procs = [\n        \"apache2\",\n        \"httpd\",\n        \"httpd-prefork\",\n        \"httpd2-prefork\",\n        \"httpd2-worker\",\n        \"httpd-worker\",\n        \"httpd.worker\",\n        \"httpd-event\",\n        \"fcgi-pm\",\n    ]\n\n    #  ss lists parent and first level child processes\n    #  last process in line is the parent:\n    #    users:((\"apache2\",pid=123456,fd=3),...,(\"apache2\",pid=123,fd=3))\n    #  capture content of last brackets (...))\n    pattern = re.compile(r\"users:.*\\(([^\\(\\)]*?)\\)\\)$\")\n\n    for ss_line in os.popen(\"ss -tlnp 2>/dev/null\").readlines():\n        parts = ss_line.split()\n        # Skip lines with wrong format\n        if len(parts) < 6 or \"users:\" not in parts[5]:\n            continue\n\n        match = re.match(pattern, parts[5])\n        if match is None:\n            continue\n        proc_info = match.group(1)\n        proc, pid, _fd = proc_info.split(\",\")\n        proc = proc.replace('\"', \"\")\n        pid = pid.replace(\"pid=\", \"\")\n\n        # Skip unwanted processes\n        if proc not in procs:\n            continue\n\n        scheme, server_address, server_port = parse_address_and_port(parts[3], ssl_ports)\n\n        results.append((scheme, server_address, server_port))\n\n    if not results:\n        # if ss output was empty (maybe not installed), try netstat instead\n        # (plugin silently fails without any section output,\n        #  if neither netstat nor ss are installed.)\n\n        for netstat_line in os.popen(\"netstat -tlnp 2>/dev/null\").readlines():\n            parts = netstat_line.split()\n            # Skip lines with wrong format\n            if len(parts) < 7 or \"/\" not in parts[6]:\n                continue\n\n            pid, proc = parts[6].split(\"/\", 1)\n            to_replace = re.compile(\"^.*/\")\n            proc = to_replace.sub(\"\", proc)\n\n            # the pid/proc field length is limited to 19 chars. Thus in case of\n            # long PIDs, the process names are stripped of by that length.\n            # Workaround this problem here\n            stripped_procs = [p[: 19 - len(pid) - 1] for p in procs]\n\n            # Skip unwanted processes\n            if proc not in stripped_procs:\n                continue\n\n            scheme, server_address, server_port = parse_address_and_port(parts[3], ssl_ports)\n\n            results.append((scheme, server_address, server_port))\n\n    return results\n\n\ndef _unpack(config):\n    if isinstance(config, tuple):\n        if len(config) == 3:\n            # Append empty instance name.\n            config += (\"\",)\n        if not isinstance(config[0], tuple):\n            # Set cacert option.\n            config = ((config[0], None),) + config[1:]\n        return config + (\"server-status\",)\n    return (\n        (config[\"protocol\"], config.get(\"cafile\", None)),\n        config[\"address\"],\n        config[\"port\"],\n        config.get(\"instance\", \"\"),\n        config.get(\"page\", \"server-status\"),\n    )\n\n\ndef get_ssl_no_verify_context():\n    import ssl\n\n    context = ssl.create_default_context()\n    context.check_hostname = False\n    context.verify_mode = ssl.CERT_NONE\n    return context\n\n\ndef get_response_body(proto, cafile, address, portspec, page):\n    response = get_response(proto, cafile, address, portspec, page)\n    return response.read().decode(get_response_charset(response))\n\n\n# 'context' parameter was added to urlopen in python 3.5 / 2.7\ndef urlopen_with_ssl(request, timeout):\n    result = None\n    if (sys.version_info[0] == 3 and sys.version_info >= (3, 5)) or (\n        sys.version_info[0] == 2 and sys.version_info >= (2, 7)\n    ):\n        result = urlopen_(request, context=get_ssl_no_verify_context(), timeout=timeout)\n    else:\n        install_opener(build_opener(HTTPSHandler()))\n        result = urlopen_(request, timeout=timeout)\n    return result\n\n\ndef get_response(proto, cafile, address, portspec, page):\n    url = \"%s://%s%s/%s?auto\" % (proto, address, portspec, page)\n    request = Request(url, headers={\"Accept\": \"text/plain\", \"User-Agent\": USER_AGENT})\n    is_local = address in (\"127.0.0.1\", \"[::1]\", \"localhost\")\n    # Try to fetch the status page for each server\n    try:\n        if proto == \"https\" and cafile:\n            return urlopen_(request, cafile=cafile, timeout=5)\n        if proto == \"https\" and is_local:\n            return urlopen_with_ssl(request, timeout=5)\n        return urlopen_(request, timeout=5)\n    except URLError as exc:\n        if \"unknown protocol\" in str(exc):\n            # HACK: workaround misconfigurations where port 443 is used for\n            # serving non ssl secured http\n            url = \"http://%s%s/server-status?auto\" % (address, portspec)\n            return urlopen_(url, timeout=5)\n        raise\n\n\ndef get_response_charset(response):\n    if sys.version_info[0] == 2:\n        charset = response.headers.getparam(\"charset\")\n    else:\n        charset = response.info().get_content_charset()\n    return charset or \"utf-8\"\n\n\ndef get_instance_name_map(cfg):\n    instance_name_map = {\"custom\": cfg.get(\"CUSTOM_ADDRESS_OVERWRITE\", {})}\n    if cfg.get(\"ENABLE_OMD_SITE_DETECTION\") and os.path.exists(\"/usr/bin/omd\"):\n        for line in os.popen(\"omd sites\").readlines():\n            sitename = line.split()[0]\n            path = \"/opt/omd/sites/%s/etc/apache/listen-port.conf\" % sitename\n            instance_name_map.setdefault(\"omd_sites\", {})\n            with contextlib.suppress(PermissionError, FileNotFoundError):\n                with open(path) as site_cfg_handle:\n                    site_raw_conf = site_cfg_handle.readlines()\n                site_conf = site_raw_conf[-2].strip().split()[1]\n                instance_name_map[\"omd_sites\"][site_conf] = sitename\n    return instance_name_map\n\n\ndef main():\n    config = get_config()\n    servers = config[\"servers\"]\n    ssl_ports = config[\"ssl_ports\"]\n\n    if servers is None:\n        servers = try_detect_servers(ssl_ports)\n\n    if not servers:\n        return 0\n\n    sys.stdout.write(\"<<<apache_status:sep(124)>>>\\n\")\n    for server in servers:\n        (proto, cafile), address, port, name, page = _unpack(server)\n        portspec = \":%d\" % port if port else \"\"\n\n        try:\n            response_body = get_response_body(proto, cafile, address, portspec, page)\n            for line in response_body.split(\"\\n\"):\n                if not line or line.isspace():\n                    continue\n                if line.lstrip()[0] == \"<\":\n                    # Seems to be html output. Skip this server.\n                    break\n                if not name:\n                    name = get_instance_name(address, port, get_instance_name_map(config))\n                sys.stdout.write(\"%s|%s|%s|%s\\n\" % (address, port, name, line))\n        except HTTPError as exc:\n            sys.stderr.write(\"HTTP-Error (%s%s): %s %s\\n\" % (address, portspec, exc.code, exc))\n\n        except Exception as exc:\n            sys.stderr.write(\"Exception (%s%s): %s\\n\" % (address, portspec, exc))\n\n    return 0\n\n\nif __name__ == \"__main__\":\n    sys.exit(main())\n"}
{"type": "source_file", "path": "agents/plugins/isc_dhcpd.py", "content": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n# Copyright (C) 2019 Checkmk GmbH - License: GNU General Public License v2\n# This file is part of Checkmk (https://checkmk.com). It is subject to the terms and\n# conditions defined in the file COPYING, which is part of this source code package.\n\n__version__ = \"2.5.0b1\"\n\n# Monitor leases if ISC-DHCPD\nimport calendar\nimport os\nimport platform\nimport re\nimport sys\nimport time\n\nconf_file = None\nfor path in [\n    \"/etc/dhcpd.conf\",\n    \"/etc/dhcp/dhcpd.conf\",\n    \"/var/dhcpd/etc/dhcpd.conf\",\n    \"/usr/local/etc/dhcpd.conf\",\n]:\n    if os.path.exists(path):\n        conf_file = path\n        break\n\nleases_file = None\nfor path in [\n    \"/var/lib/dhcp/db/dhcpd.leases\",\n    \"/var/lib/dhcp/dhcpd.leases\",\n    \"/var/lib/dhcpd/dhcpd.leases\",  # CentOS\n    \"/var/dhcpd/var/db/dhcpd.leases\",  # OPNsense\n]:\n    if os.path.exists(path):\n        leases_file = path\n        break\n\n# If no configuration and leases are found, we assume that\n# no dhcpd is running.\nif not conf_file or not leases_file:\n    sys.exit(0)\n\n\ndef get_pid():\n    cmd = \"pidof dhcpd\"\n\n    if \"debian-10\" in platform.platform().lower():\n        # workaround for bug in sysvinit-utils in debian buster\n        # https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=926896\n        cmd = \"ps aux | grep -w [d]hcpd | awk {'printf (\\\"%s \\\", $2)'}\"\n\n    if \"freebsd\" in platform.platform().lower():\n        # workaround for freebsd\n        cmd = \"ps aux | grep -w \\\"[d]hcpd\\\" | awk '{print $2}'\"\n\n    # This produces a false warning in Bandit, claiming there was no failing test for this nosec.\n    # The warning is a bug in Bandit: https://github.com/PyCQA/bandit/issues/942\n    p = os.popen(cmd)  # nosec B605 # BNS:f6c1b9\n    return p.read().strip()\n\n\npidof_dhcpd = get_pid()\nsys.stdout.write(\"<<<isc_dhcpd>>>\\n[general]\\nPID: %s\\n\" % pidof_dhcpd)\n\nsys.stdout.write(\"[pools]\\n\")\n\n\ndef parse_config(filename):\n    with open(filename) as config_file:\n        for l in config_file:\n            line = l.strip()\n            if line.startswith(\"include\"):\n                regex_result = re.search(r'include\\s+\"(.*)\"', line)\n                if regex_result:\n                    included_file = regex_result.group(1)\n                parse_config(included_file)\n            elif line.startswith(\"range\"):\n                sys.stdout.write(line[5:].strip(\"\\t ;\") + \"\\n\")\n\n\nparse_config(conf_file)\n\n# lease 10.1.1.81 {\n#   starts 3 2015/09/09 11:42:20;\n#   ends 3 2015/09/09 19:42:20;\n#   tstp 3 2015/09/09 19:42:20;\n#   cltt 3 2015/09/09 11:42:20;\n#   binding state free;\n#   hardware ethernet a4:5e:60:de:1f:c3;\n#   uid \"\\001\\244^`\\336\\037\\303\";\n#   set ddns-txt = \"318c69bae8aeae6f8c723e96de933c7149\";\n#   set ddns-fwd-name = \"Sebastians-MBP.dhcp.mathias-kettner.de\";\n# }\n\nsys.stdout.write(\"[leases]\\n\")\nnow = time.time()\nip_address = None\nbinding_state = None\nseen_addresses = set()\nwith open(leases_file) as leases_file_obj:\n    for lease_line in leases_file_obj:\n        parts = lease_line.strip().rstrip(\";\").split()\n        if not parts:\n            continue\n\n        if parts[0] == \"lease\":\n            ip_address = parts[1]\n        elif parts[0] == \"ends\":\n            if parts[1] != \"never\":\n                ends_date_string = parts[2] + \" \" + parts[3]\n                ends_date = calendar.timegm(time.strptime(ends_date_string, \"%Y/%m/%d %H:%M:%S\"))\n                if ends_date < now:\n                    ip_address = None  # skip this address, this lease is outdated\n\n        elif parts[0] == \"binding\" and parts[1] == \"state\":\n            binding_state = parts[2]\n\n        elif parts[0] == \"}\":\n            if ip_address and binding_state == \"active\" and ip_address not in seen_addresses:\n                sys.stdout.write(\"%s\\n\" % ip_address)\n                seen_addresses.add(ip_address)\n            ip_address = None\n            binding_state = None\n"}
{"type": "source_file", "path": "agents/plugins/mk_ceph.py", "content": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n# Copyright (C) 2024 Checkmk GmbH - License: GNU General Public License v2\n# This file is part of Checkmk (https://checkmk.com). It is subject to the terms and\n# conditions defined in the file COPYING, which is part of this source code package.\n\nimport json\nimport os\nimport os.path\nimport socket\nimport sys\n\n__version__ = \"2.5.0b1\"\n\n\ntry:\n    import rados  # type: ignore[import-not-found]\nexcept ImportError:\n    pip = \"pip3\" if sys.version_info.major == 3 else \"pip\"\n    error = (\n        \"Error: mk_ceph requires the library 'rados'.\"\n        \" Please install it on the monitored system (%s install rados).\" % pip\n    )\n    sys.stdout.write(\"<<<cephstatus:sep(0)>>>\\n%s\\n\" % json.dumps({\"deployment_error\": error}))\n    sys.exit(0)\n\n\ndef _output_json_section(name, data):\n    sys.stdout.write(\"<<<%s:sep(0)>>>\\n%s\\n\" % (name, json.dumps(data)))\n\n\nclass RadosCMD(rados.Rados):  # type: ignore[misc]\n    def command_mon(self, cmd, params=None):\n        data = {\"prefix\": cmd, \"format\": \"json\"}\n        if params:\n            data.update(params)\n        return self.mon_command(json.dumps(data), b\"\", timeout=5)\n\n    def command_mgr(self, cmd):\n        return self.mgr_command(json.dumps({\"prefix\": cmd, \"format\": \"json\"}), b\"\", timeout=5)\n\n    def command_osd(self, osdid, cmd):\n        return self.osd_command(\n            osdid, json.dumps({\"prefix\": cmd, \"format\": \"json\"}), b\"\", timeout=5\n        )\n\n    def command_pg(self, pgid, cmd):\n        return self.pg_command(pgid, json.dumps({\"prefix\": cmd, \"format\": \"json\"}), b\"\", timeout=5)\n\n\ndef _load_plugin_config(mk_confdir: str) -> tuple[str, str]:\n    ceph_config = \"/etc/ceph/ceph.conf\"\n    ceph_client = \"client.admin\"\n\n    try:\n        with open(os.path.join(mk_confdir, \"ceph.cfg\"), \"r\") as config:\n            content = config.readlines()\n    except FileNotFoundError:\n        return ceph_config, ceph_client\n\n    for line in content:\n        if \"=\" not in line:\n            continue\n        key, value = line.strip().split(\"=\")\n        if key == \"CONFIG\":\n            ceph_config = value\n        if key == \"CLIENT\":\n            ceph_client = value\n\n    return ceph_config, ceph_client\n\n\ndef _make_bluefs_section(raw, hostname, fqdn, fsid):\n    # type: (str, str, str, str) -> tuple[dict, list]\n    localosds = []\n    out = {\"end\": {}}  # type: dict\n    for osd in json.loads(raw):\n        if osd.get(\"hostname\") in [hostname, fqdn]:\n            localosds.append(osd[\"id\"])\n            if \"container_hostname\" in osd:\n                adminsocket = \"/run/ceph/%s/ceph-osd.%d.asok\" % (fsid, osd[\"id\"])\n            else:\n                adminsocket = \"/run/ceph/ceph-osd.%d.asok\" % osd[\"id\"]\n            if os.path.exists(adminsocket):\n                chunks: list[bytes] = []\n                try:\n                    sock = socket.socket(socket.AF_UNIX, socket.SOCK_STREAM)\n                    sock.connect(adminsocket)\n                    sock.sendall(b'{\"prefix\": \"perf dump\"}\\n')\n                    sock.shutdown(socket.SHUT_WR)\n                    while len(chunks) == 0 or chunks[-1] != b\"\":\n                        chunks.append(sock.recv(4096))\n                    sock.close()\n                    chunks[0] = chunks[0][4:]\n                except Exception:\n                    chunks = [b'{\"bluefs\": {}}']\n                out[osd[\"id\"]] = {\"bluefs\": json.loads(b\"\".join(chunks))[\"bluefs\"]}\n    return out, localosds\n\n\ndef _make_osd_section(raw_df, raw_perf, localosds):\n    # type: (str, str, list) -> dict\n    osddf = json.loads(raw_df)\n    osdperf = json.loads(raw_perf)\n    osds = []\n    for osd in osddf[\"nodes\"]:\n        if osd[\"id\"] in localosds:\n            osds.append(osd)\n    perfs = []\n    if \"osd_perf_infos\" in osdperf:\n        for osd in osdperf[\"osd_perf_infos\"]:\n            if osd[\"id\"] in localosds:\n                perfs.append(osd)\n    if \"osdstats\" in osdperf and \"osd_perf_infos\" in osdperf[\"osdstats\"]:\n        for osd in osdperf[\"osdstats\"][\"osd_perf_infos\"]:\n            if osd[\"id\"] in localosds:\n                perfs.append(osd)\n\n    return {\"df\": {\"nodes\": osds}, \"perf\": {\"osd_perf_infos\": perfs}}\n\n\ndef main() -> int:\n    ceph_config, ceph_client = _load_plugin_config(os.environ[\"MK_CONFDIR\"])\n\n    cluster = RadosCMD(conffile=ceph_config, name=ceph_client)\n    cluster.connect()\n\n    hostname = socket.gethostname().split(\".\", 1)[0]\n    fqdn = socket.getfqdn()\n\n    res = cluster.command_mon(\"status\")\n    if res[0] != 0:\n        fsid = \"\"\n    else:\n        status = json.loads(res[1])\n        fsid = status.get(\"fsid\", \"\")\n        # only on MON hosts\n        mons = status.get(\"quorum_names\", [])\n        if hostname in mons or fqdn in mons:\n            _output_json_section(\"cephstatus\", status)\n\n            res = cluster.command_mon(\"df\", params={\"detail\": \"detail\"})\n            if res[0] == 0:\n                _output_json_section(\"cephdf\", json.loads(res[1]))\n\n    res = cluster.command_mon(\"osd metadata\")\n    if res[0] == 0:\n        section, localosds = _make_bluefs_section(res[1], hostname, fqdn, fsid)\n        _output_json_section(\"cephosdbluefs\", section)\n    else:\n        localosds = []\n\n    osddf_raw = cluster.command_mon(\"osd df\")\n    osdperf_raw = cluster.command_mon(\"osd perf\")\n    if osddf_raw[0] == 0 and osdperf_raw[0] == 0:\n        _output_json_section(\"cephosd\", _make_osd_section(osddf_raw[1], osdperf_raw[1], localosds))\n\n    return 0\n\n\nif __name__ == \"__main__\":\n    sys.exit(main())\n"}
{"type": "source_file", "path": "agents/plugins/mk_docker.py", "content": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n# Copyright (C) 2019 Checkmk GmbH - License: GNU General Public License v2\n# This file is part of Checkmk (https://checkmk.com). It is subject to the terms and\n# conditions defined in the file COPYING, which is part of this source code package.\nr\"\"\"Check_MK Agent Plug-in: mk_docker.py\n\nThis plugin is configured using an ini-style configuration file,\ni.e. a file with lines of the form 'key: value'.\nAt 'agents/cfg_examples/mk_docker.cfg' (relative to the check_mk\nsource code directory ) you should find some example configuration\nfiles. For more information on possible configurations refer to the\nfile docker.cfg in said directory.\nThe Python docker package is required to run this plugin.\nDepending on the system, there are different ways to install it.\nInstallation options:\n    - pip: pip install docker\n    - apt: apt install python3-docker  # Debian/Ubuntu\n\nThis plugin it will be called by the agent without any arguments.\n\"\"\"\n\nfrom __future__ import with_statement\n\n__version__ = \"2.5.0b1\"\n\n# NOTE: docker is available for python versions from 2.6 / 3.3\n\nimport argparse\nimport configparser\nimport functools\nimport json\nimport logging\nimport multiprocessing\nimport os\nimport pathlib\nimport struct\nimport sys\nimport time\n\n\ndef which(prg):\n    for path in os.environ[\"PATH\"].split(os.pathsep):\n        if os.path.isfile(os.path.join(path, prg)) and os.access(os.path.join(path, prg), os.X_OK):\n            return os.path.join(path, prg)\n    return None\n\n\n# The \"import docker\" checks below result in agent sections being created. This\n# is a way to end the plugin in case it is being executed on a non docker host\nif (\n    not os.path.isfile(\"/var/lib/docker\")\n    and not os.path.isfile(\"/var/run/docker\")\n    and not which(\"docker\")\n):\n    sys.stderr.write(\"mk_docker.py: Does not seem to be a docker host. Terminating.\\n\")\n    sys.exit(1)\n\ntry:\n    import docker  # type: ignore[import-untyped]\nexcept ImportError:\n    sys.stdout.write(\n        \"<<<docker_node_info:sep(124)>>>\\n\"\n        \"@docker_version_info|{}\\n\"\n        '{\"Critical\": \"Error: mk_docker requires the docker library.'\n        ' Please install it on the monitored system (%s install docker).\"}\\n'\n        % (\"pip3\" if sys.version_info.major == 3 else \"pip\")\n    )\n    sys.exit(0)\n\nif int(docker.__version__.split(\".\", 1)[0]) < 2:\n    sys.stdout.write(\n        \"<<<docker_node_info:sep(124)>>>\\n\"\n        \"@docker_version_info|{}\\n\"\n        '{\"Critical\": \"Error: mk_docker requires the docker library >= 2.0.0.'\n        ' Please install it on the monitored system (%s install docker).\"}\\n'\n        % (\"pip3\" if sys.version_info.major == 3 else \"pip\")\n    )\n    sys.exit(0)\n\nDEBUG = \"--debug\" in sys.argv[1:]\n\nVERSION = \"0.1\"\n\nDEFAULT_CFG_FILE = os.path.join(os.getenv(\"MK_CONFDIR\", \"\"), \"docker.cfg\")\n\nDEFAULT_CFG_SECTION = {\n    \"base_url\": \"unix://var/run/docker.sock\",\n    \"skip_sections\": \"\",\n    \"container_id\": \"short\",\n}\n\nLOGGER = logging.getLogger(__name__)\n\n\ndef parse_arguments(argv=None):\n    if argv is None:\n        argv = sys.argv[1:]\n\n    prog, descr, epilog = __doc__.split(\"\\n\\n\")\n    parser = argparse.ArgumentParser(prog=prog, description=descr, epilog=epilog)\n    parser.add_argument(\n        \"--debug\", action=\"store_true\", help=\"\"\"Debug mode: raise Python exceptions\"\"\"\n    )\n    parser.add_argument(\n        \"-v\",\n        \"--verbose\",\n        action=\"count\",\n        default=0,\n        help=\"\"\"Verbose mode (for even more output use -vvv)\"\"\",\n    )\n    parser.add_argument(\n        \"-c\",\n        \"--config-file\",\n        default=DEFAULT_CFG_FILE,\n        help=\"\"\"Read config file (default: $MK_CONFDIR/docker.cfg)\"\"\",\n    )\n\n    args = parser.parse_args(argv)\n\n    fmt = \"%%(levelname)5s: %s%%(message)s\"\n    if args.verbose == 0:\n        LOGGER.propagate = False\n    elif args.verbose == 1:\n        logging.basicConfig(level=logging.INFO, format=fmt % \"\")\n    else:\n        logging.basicConfig(level=logging.DEBUG, format=fmt % \"(line %(lineno)3d) \")\n    if args.verbose < 3:\n        logging.getLogger(\"urllib3\").setLevel(logging.WARNING)\n\n    LOGGER.debug(\"parsed args: %r\", args)\n    return args\n\n\ndef get_config(cfg_file):\n    config = configparser.ConfigParser(DEFAULT_CFG_SECTION)\n    LOGGER.debug(\"trying to read %r\", cfg_file)\n    files_read = config.read(cfg_file)\n    LOGGER.info(\"read configration file(s): %r\", files_read)\n    section_name = \"DOCKER\" if config.sections() else \"DEFAULT\"\n    conf_dict = dict(config.items(section_name))  # type: dict[str, str | tuple]\n    skip_sections = conf_dict.get(\"skip_sections\", \"\")\n    if isinstance(skip_sections, str):\n        skip_list = skip_sections.split(\",\")\n        conf_dict[\"skip_sections\"] = tuple(n.strip() for n in skip_list)\n\n    return conf_dict\n\n\nclass Section(list):\n    \"\"\"a very basic agent section class\"\"\"\n\n    _OUTPUT_LOCK = multiprocessing.Lock()\n\n    version_info = {\n        \"PluginVersion\": VERSION,\n        \"DockerPyVersion\": docker.__version__,\n    }\n\n    # Should we need to parallelize one day, change this to be\n    # more like the Section class in agent_azure, for instance\n    def __init__(self, name=None, piggytarget=None):\n        super().__init__()\n        if piggytarget is not None:\n            self.append(\"<<<<%s>>>>\" % piggytarget)\n        if name is not None:\n            self.append(\"<<<%s:sep(124)>>>\" % name)\n            version_json = json.dumps(Section.version_info)\n            self.append(\"@docker_version_info|%s\" % version_json)\n            self.append(\"<<<%s:sep(0)>>>\" % name)\n\n    def write(self):\n        if self[0].startswith(\"<<<<\"):\n            self.append(\"<<<<>>>>\")\n        with self._OUTPUT_LOCK:\n            for line in self:\n                sys.stdout.write(\"%s\\n\" % line)\n            sys.stdout.flush()\n\n\ndef report_exception_to_server(exc, location):\n    LOGGER.info(\"handling exception: %s\", exc)\n    msg = \"Plug-in exception in %s: %s\" % (location, exc)\n    sec = Section(\"docker_node_info\")\n    sec.append(json.dumps({\"Unknown\": msg}))\n    sec.write()\n\n\nclass ParallelDfCall:\n    \"\"\"handle parallel calls of super().df()\n\n    The Docker API will only allow one super().df() call at a time.\n    This leads to problems when the plugin is executed multiple times\n    in parallel.\n    \"\"\"\n\n    def __init__(self, call):\n        self._call = call\n        self._vardir = pathlib.Path(os.getenv(\"MK_VARDIR\", \"\"))\n        self._spool_file = self._vardir / \"mk_docker_df.spool\"\n        self._tmp_file_templ = \"mk_docker_df.tmp.%s\"\n        self._my_tmp_file = self._vardir / (self._tmp_file_templ % os.getpid())\n\n    def __call__(self):\n        try:\n            self._my_tmp_file.touch()\n            data = self._new_df_result()\n        except docker.errors.APIError as exc:\n            LOGGER.debug(\"df API call failed: %s\", exc)\n            data = self._spool_df_result()\n        else:\n            # the API call succeeded, no need for any tmp files\n            for file_ in self._iter_tmp_files():\n                self._unlink(file_)\n        finally:\n            # what ever happens: remove my tmp file\n            self._unlink(self._my_tmp_file)\n\n        return data\n\n    def _new_df_result(self):\n        data = self._call()\n        self._write_df_result(data)\n        return data\n\n    def _iter_tmp_files(self):\n        return self._vardir.glob(self._tmp_file_templ % \"*\")\n\n    @staticmethod\n    def _unlink(file_):\n        try:\n            file_.unlink()\n        except OSError:\n            pass\n\n    def _spool_df_result(self):\n        # check every 0.1 seconds\n        tick = 0.1\n        # if the df command takes more than 60 seconds, you probably want to\n        # execute the plugin asynchronously. This should cover a majority of cases.\n        timeout = 60\n        for _ in range(int(timeout / tick)):\n            time.sleep(tick)\n            if not any(self._iter_tmp_files()):\n                break\n\n        return self._read_df_result()\n\n    def _write_df_result(self, data):\n        with self._my_tmp_file.open(\"wb\") as file_:\n            file_.write(json.dumps(data).encode(\"utf-8\"))\n        try:\n            self._my_tmp_file.rename(self._spool_file)\n        except OSError:\n            # CMK-12642: It can happen that two df calls succeed almost at the same time. Then, one\n            # process attempts to move while the other one already deleted all temp files.\n            pass\n\n    def _read_df_result(self):\n        \"\"\"read from the spool file\n\n        Don't handle FileNotFound - the plugin can deal with it, and it's easier to debug.\n        \"\"\"\n        with self._spool_file.open() as file_:\n            return json.loads(file_.read())\n\n\nclass MKDockerClient(docker.DockerClient):  # type: ignore[misc]\n    \"\"\"a docker.DockerClient that caches containers and node info\"\"\"\n\n    API_VERSION = \"auto\"\n    _DEVICE_MAP_LOCK = multiprocessing.Lock()\n\n    def __init__(self, config):\n        super().__init__(config[\"base_url\"], version=MKDockerClient.API_VERSION)\n        all_containers = _robust_inspect(self, \"containers\")\n        if config[\"container_id\"] == \"name\":\n            self.all_containers = {c.attrs[\"Name\"].lstrip(\"/\"): c for c in all_containers}\n        elif config[\"container_id\"] == \"long\":\n            self.all_containers = {c.attrs[\"Id\"]: c for c in all_containers}\n        else:\n            self.all_containers = {c.attrs[\"Id\"][:12]: c for c in all_containers}\n        self._env = {\"REMOTE\": os.getenv(\"REMOTE\", \"\")}\n        self._container_stats = {}\n        self._device_map = None\n        self.node_info = self.info()\n\n        self._df_caller = ParallelDfCall(call=super().df)\n\n    def df(self):\n        return self._df_caller()\n\n    def device_map(self):\n        with self._DEVICE_MAP_LOCK:\n            if self._device_map is not None:\n                return self._device_map\n\n            self._device_map = {}\n            for device in os.listdir(\"/sys/block\"):\n                dev_path = \"/sys/block/%s/dev\" % device\n                if os.path.exists(dev_path):\n                    with open(dev_path) as handle:\n                        self._device_map[handle.read().strip()] = device\n\n        return self._device_map\n\n    @staticmethod\n    def iter_socket(sock, descriptor):\n        \"\"\"iterator to recv data from container socket\"\"\"\n        header = docker.utils.socket.read(sock, 8)\n        while header:\n            actual_descriptor, length = struct.unpack(\">BxxxL\", header)\n            while length:\n                data = docker.utils.socket.read(sock, length)\n                length -= len(data)\n                LOGGER.debug(\"Received data: %r\", data)\n                if actual_descriptor == descriptor:\n                    yield data.decode(\"UTF-8\")\n            header = docker.utils.socket.read(sock, 8)\n\n    def get_stdout(self, exec_return_val):\n        \"\"\"read stdout from container process\"\"\"\n        if isinstance(exec_return_val, tuple):\n            # it's a tuple since version 3.0.0\n            exit_code, sock = exec_return_val\n            if exit_code not in (0, None):\n                return \"\"\n        else:\n            sock = exec_return_val\n\n        return \"\".join(self.iter_socket(sock, 1))\n\n    def run_agent(self, container):\n        \"\"\"run checkmk agent in container\"\"\"\n        result = container.exec_run([\"check_mk_agent\"], environment=self._env, socket=True)\n        return self.get_stdout(result)\n\n    def get_container_stats(self, container_key):\n        \"\"\"return cached container stats\"\"\"\n        try:\n            return self._container_stats[container_key]\n        except KeyError:\n            pass\n\n        container = self.all_containers[container_key]\n        if not container.status == \"running\":\n            return self._container_stats.setdefault(container_key, None)\n\n        # We use the streaming mode here because it faciliates error handling. If a container is\n        # removed at exactly the same time when we query the stats, we get StopIteration in\n        # streaming mode. In non-streaming mode, the error type is version-dependent.\n        stats_generator = container.stats(stream=True, decode=True)\n        try:\n            next(stats_generator)  # we need to advance the generator by one to get useful data\n            stats = next(stats_generator)\n        except (\n            # container was removed in between collecting containers and here\n            docker.errors.NotFound,\n            # container is removed just now; it could be that under very old docker versions (eg.\n            # 1.31), there are other scenarios causing this exception (SUP-10974)\n            StopIteration,\n        ):\n            return self._container_stats.setdefault(container_key, None)\n\n        return self._container_stats.setdefault(container_key, stats)\n\n\ndef time_it(func):\n    \"\"\"Decorator to time the function\"\"\"\n\n    @functools.wraps(func)\n    def wrapped(*args, **kwargs):\n        before = time.time()\n        try:\n            return func(*args, **kwargs)\n        finally:\n            LOGGER.info(\"%r took %ss\", func.__name__, time.time() - before)\n\n    return wrapped\n\n\n@time_it\ndef set_version_info(client):\n    data = client.version()\n    LOGGER.debug(data)\n    Section.version_info[\"ApiVersion\"] = data.get(\"ApiVersion\")\n\n\n# .\n#   .--Sections------------------------------------------------------------.\n#   |                  ____            _   _                               |\n#   |                 / ___|  ___  ___| |_(_) ___  _ __  ___               |\n#   |                 \\___ \\ / _ \\/ __| __| |/ _ \\| '_ \\/ __|              |\n#   |                  ___) |  __/ (__| |_| | (_) | | | \\__ \\              |\n#   |                 |____/ \\___|\\___|\\__|_|\\___/|_| |_|___/              |\n#   |                                                                      |\n#   +----------------------------------------------------------------------+\n#   |                                                                      |\n#   '----------------------------------------------------------------------'\n\n\ndef is_disabled_section(config, section_name):\n    \"\"\"Skip the section, if configured to do so\"\"\"\n    if section_name in config[\"skip_sections\"]:\n        LOGGER.info(\"skipped section: %s\", section_name)\n        return True\n    return False\n\n\n@time_it\ndef section_node_info(client):\n    LOGGER.debug(client.node_info)\n    section = Section(\"docker_node_info\")\n    section.append(json.dumps(client.node_info))\n    section.write()\n\n\n@time_it\ndef section_node_disk_usage(client):\n    \"\"\"docker system df\"\"\"\n    section = Section(\"docker_node_disk_usage\")\n    try:\n        data = client.df()\n    except docker.errors.APIError as exc:\n        if DEBUG:\n            raise\n        section.write()\n        LOGGER.exception(exc)\n        return\n    LOGGER.debug(data)\n\n    def get_row(type_, instances, is_inactive, key=\"Size\"):\n        inactive = [i for i in instances if is_inactive(i)]\n        item_data = {\n            \"type\": type_,\n            \"size\": sum(i.get(key, 0) for i in instances),\n            \"reclaimable\": sum(i.get(key, 0) for i in inactive),\n            \"count\": len(instances),\n            \"active\": len(instances) - len(inactive),\n        }\n        return json.dumps(item_data)\n\n    # images:\n    images = data.get(\"Images\") or []\n    row = get_row(\"images\", images, lambda i: i[\"Containers\"] == 0)\n    section.append(row)\n\n    # containers:\n    containers = data.get(\"Containers\") or []\n    row = get_row(\"containers\", containers, lambda c: c[\"State\"] != \"running\", key=\"SizeRw\")\n    section.append(row)\n\n    # volumes\n    volumes = [v.get(\"UsageData\", {}) for v in data.get(\"Volumes\") or []]\n    if not any(-1 in v.values() for v in volumes):\n        row = get_row(\"volumes\", volumes, lambda v: v.get(\"RefCount\", 0) == 0)\n        section.append(row)\n\n    # build_cache:\n    build_cache = data.get(\"BuildCache\") or []\n    row = get_row(\"buildcache\", build_cache, lambda b: b.get(\"InUse\"))\n    section.append(row)\n\n    section.write()\n\n\ndef _robust_inspect(client, docker_object):\n    object_map = {\n        \"images\": {\n            \"api\": client.api.images,\n            \"getter\": client.images.get,\n            \"kwargs\": {},\n        },\n        \"containers\": {\n            \"api\": client.api.containers,\n            \"getter\": client.containers.get,\n            \"kwargs\": {\"all\": True},\n        },\n    }\n    if docker_object not in object_map:\n        raise RuntimeError(\"Unkown docker object: %s\" % docker_object)\n\n    api = object_map[docker_object][\"api\"]\n    getter = object_map[docker_object][\"getter\"]\n    kwargs = object_map[docker_object][\"kwargs\"]\n    # workaround instead of calling client.OBJECT.list() directly to be able to\n    # ignore errors when OBJECT was removed in between listing available OBJECT\n    # and getting detailed information about them\n    for response in api(**kwargs):\n        try:\n            yield getter(response[\"Id\"])\n        except docker.errors.NotFound:\n            pass\n\n\n@time_it\ndef section_node_images(client):\n    \"\"\"in subsections list [[[images]]] and [[[containers]]]\"\"\"\n    section = Section(\"docker_node_images\")\n\n    images = _robust_inspect(client, \"images\")\n    LOGGER.debug(images)\n    section.append(\"[[[images]]]\")\n    for image in images:\n        section.append(json.dumps(image.attrs))\n\n    LOGGER.debug(client.all_containers)\n    section.append(\"[[[containers]]]\")\n    for container in client.all_containers.values():\n        section.append(json.dumps(container.attrs))\n\n    section.write()\n\n\n@time_it\ndef section_node_network(client):\n    networks = client.networks.list(filters={\"driver\": \"bridge\"})\n    section = Section(\"docker_node_network\")\n    section += [json.dumps(n.attrs) for n in networks]\n    section.write()\n\n\ndef section_container_node_name(client, container_id):\n    node_name = client.node_info.get(\"Name\")\n    section = Section(\"docker_container_node_name\", piggytarget=container_id)\n    section.append(json.dumps({\"NodeName\": node_name}))\n    section.write()\n\n\ndef section_container_status(client, container_id):\n    container = client.all_containers[container_id]\n    status = container.attrs.get(\"State\", {})\n\n    healthcheck = container.attrs.get(\"Config\", {}).get(\"Healthcheck\")\n    if healthcheck:\n        status[\"Healthcheck\"] = healthcheck\n    restart_policy = container.attrs.get(\"HostConfig\", {}).get(\"RestartPolicy\")\n    if restart_policy:\n        status[\"RestartPolicy\"] = restart_policy\n\n    try:\n        status[\"ImageTags\"] = container.image.tags\n    except docker.errors.ImageNotFound:\n        # image has been removed while container is still running\n        pass\n    status[\"NodeName\"] = client.node_info.get(\"Name\")\n\n    section = Section(\"docker_container_status\", piggytarget=container_id)\n    section.append(json.dumps(status))\n    section.write()\n\n\ndef section_container_labels(client, container_id):\n    container = client.all_containers[container_id]\n    section = Section(\"docker_container_labels\", piggytarget=container_id)\n    section.append(json.dumps(container.labels))\n    section.write()\n\n\ndef section_container_network(client, container_id):\n    container = client.all_containers[container_id]\n    network = container.attrs.get(\"NetworkSettings\", {})\n    section = Section(\"docker_container_network\", piggytarget=container_id)\n    section.append(json.dumps(network))\n    section.write()\n\n\ndef _is_not_running_exception(exception):\n    return (\n        exception.response.status_code\n        in (\n            409,\n            500,  # Thrown by old docker versions: SUP-10974\n        )\n        and \"is not running\" in exception.explanation\n    )\n\n\ndef section_container_agent(client, container_id):\n    container = client.all_containers[container_id]\n    if container.status != \"running\":\n        return True\n    try:\n        result = client.run_agent(container)\n    except docker.errors.APIError as e:\n        # container was removed in between collecting containers and here\n        if _is_not_running_exception(e):\n            return True\n        raise e\n    success = \"<<<check_mk>>>\" in result\n    if success:\n        LOGGER.debug(\"running check_mk_agent in container %s: ok\", container_id)\n        section = Section(piggytarget=container_id)\n        section.append(result)\n        section.write()\n    else:\n        LOGGER.warning(\"running check_mk_agent in container %s failed: %s\", container_id, result)\n    return success\n\n\ndef section_container_mem(client, container_id):\n    stats = client.get_container_stats(container_id)\n    if stats is None:  # container not running\n        return\n    container_mem = stats[\"memory_stats\"]\n    section = Section(\"docker_container_mem\", piggytarget=container_id)\n    section.append(json.dumps(container_mem))\n    section.write()\n\n\ndef section_container_cpu(client, container_id):\n    stats = client.get_container_stats(container_id)\n    if stats is None:  # container not running\n        return\n    container_cpu = stats[\"cpu_stats\"]\n    section = Section(\"docker_container_cpu\", piggytarget=container_id)\n    section.append(json.dumps(container_cpu))\n    section.write()\n\n\ndef section_container_diskstat(client, container_id):\n    stats = client.get_container_stats(container_id)\n    if stats is None:  # container not running\n        return\n    container_blkio = stats[\"blkio_stats\"]\n    container_blkio[\"time\"] = time.time()\n    container_blkio[\"names\"] = client.device_map()\n    section = Section(\"docker_container_diskstat\", piggytarget=container_id)\n    section.append(json.dumps(container_blkio))\n    section.write()\n\n\nNODE_SECTIONS = (\n    (\"docker_node_info\", section_node_info),\n    (\"docker_node_disk_usage\", section_node_disk_usage),\n    (\"docker_node_images\", section_node_images),\n    (\"docker_node_network\", section_node_network),\n)\n\nCONTAINER_API_SECTIONS = (\n    (\"docker_container_node_name\", section_container_node_name),\n    (\"docker_container_status\", section_container_status),\n    (\"docker_container_labels\", section_container_labels),\n    (\"docker_container_network\", section_container_network),\n)\n\nCONTAINER_API_SECTIONS_NO_AGENT = (\n    (\"docker_container_mem\", section_container_mem),\n    (\"docker_container_cpu\", section_container_cpu),\n    (\"docker_container_diskstat\", section_container_diskstat),\n)\n\n\ndef call_node_sections(client, config):\n    for name, section in NODE_SECTIONS:\n        if is_disabled_section(config, name):\n            continue\n        try:\n            section(client)\n        except Exception as exc:\n            if DEBUG:\n                raise\n            # The section is already always written. Prevent duplicate @docker_version_info\n            if name != \"docker_node_info\":\n                write_empty_section(name)\n            report_exception_to_server(exc, section.__name__)\n\n\ndef write_empty_section(name, piggytarget=None):\n    Section(name, piggytarget).write()\n\n\ndef call_container_sections(client, config):\n    jobs = []\n    for container_id in client.all_containers:\n        job = multiprocessing.Process(\n            target=_call_single_containers_sections, args=(client, config, container_id)\n        )\n        job.start()\n        jobs.append(job)\n\n    for job in jobs:\n        job.join()\n\n\ndef _call_single_containers_sections(client, config, container_id):\n    LOGGER.info(\"container id: %s\", container_id)\n    for name, section in CONTAINER_API_SECTIONS:\n        if is_disabled_section(config, name):\n            continue\n        try:\n            section(client, container_id)\n        except Exception as exc:\n            if DEBUG:\n                raise\n            report_exception_to_server(exc, section.__name__)\n\n    agent_success = False\n    if not is_disabled_section(config, \"docker_container_agent\"):\n        try:\n            agent_success = section_container_agent(client, container_id)\n        except Exception as exc:\n            if DEBUG:\n                raise\n            report_exception_to_server(exc, \"section_container_agent\")\n    if agent_success:\n        return\n\n    for name, section in CONTAINER_API_SECTIONS_NO_AGENT:\n        if is_disabled_section(config, name):\n            continue\n        try:\n            section(client, container_id)\n        except Exception as exc:\n            if DEBUG:\n                raise\n            report_exception_to_server(exc, section.__name__)\n\n\n# .\n#   .--Main----------------------------------------------------------------.\n#   |                        __  __       _                                |\n#   |                       |  \\/  | __ _(_)_ __                           |\n#   |                       | |\\/| |/ _` | | '_ \\                          |\n#   |                       | |  | | (_| | | | | |                         |\n#   |                       |_|  |_|\\__,_|_|_| |_|                         |\n#   |                                                                      |\n#   +----------------------------------------------------------------------+\n#   |                                                                      |\n#   '----------------------------------------------------------------------'\n\n\ndef main():\n    args = parse_arguments()\n    config = get_config(args.config_file)\n\n    try:  # first calls by docker-daemon: report failure\n        client = MKDockerClient(config)\n    except Exception as exc:\n        if DEBUG:\n            raise\n        report_exception_to_server(exc, \"MKDockerClient.__init__\")\n        sys.exit(0)\n\n    set_version_info(client)\n\n    call_node_sections(client, config)\n\n    call_container_sections(client, config)\n\n\nif __name__ == \"__main__\":\n    main()\n"}
{"type": "source_file", "path": "agents/plugins/mk_filestats.py", "content": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n# Copyright (C) 2019 Checkmk GmbH - License: GNU General Public License v2\n# This file is part of Checkmk (https://checkmk.com). It is subject to the terms and\n# conditions defined in the file COPYING, which is part of this source code package.\nr\"\"\"Check_MK Agent Plugin: mk_filestats\n\nThis is a Check_MK Agent plugin. If configured, it will be called by the\nagent without any arguments.\n\nusage: mk_filestats [OPTIONS]\n\nOptions:\n    -h,  --help           Show this help message and exit\n    -v,  -vv              Increase verbosity\n    -c,  --config-file    Read config file\n                          (default: $MK_CONFDIR/filestats.cfg)\n\nDetails:\n\nThis plugin is configured using ini-style configuration files, i.e. a file with\nsections started by lines of the form '[Section Name]' and consisting of\n'key: value' (key-colon-space-value) lines.\n\nEvery section will be processed individually, and this processing can be\ndescribed by the following four phases:\n\n1. Input:\n    This phase will gather (iterators returning) the files the plugin will\n    initiallly be aware of.\n    Option keys concerning this phase a prefixed 'input_'. Currently, there\n    is only one option available (which therefore must be present):\n    * ``input_patterns'':\n      Here you can specify one or more *globbing* patterns. If more than one\n      pattern is provided, they will be splitted according to shell rules\n      (using shlex.split). Every matching file will be dealt with, every\n      matching folder will recursively searched for *all* files.\n2. Filtering:\n    This phase will filter the input files according to filters provided using\n    the option keys starting with 'filter_' of the corresponding configuration\n    section. The following are available (note that regex filters will allways\n    be applied before other types of filters)\n    * ``filter_regex: regular_expression''\n      Only further process a file, if its full path matches the given regular\n      expression. Everything following the characters 'filter_regex: ' will\n      considered one single regular expression.\n    * ``filter_regex_inverse: regular_expression''\n      Only further process a file, if its full path *does not* match the given\n      regular expression.\n    * ``filter_size: specification''\n      Only further process a file, if its size in bytes matches the provided\n      specification. The specification consists of one of the operators '>',\n      '<', '>=', '<=' and '==', directly followed by an integer.\n      E.g.: 'filter_size: <43008' will only match files smaller than 42 KB.\n    * ``filter_age: specification''\n      Only further process a file, if its age in seconds matches the filter.\n      See ``filter_size''.\n3. Grouping\n    It is possible to group files within a file group further into subgroups\n    using grouping criteria. The supported options are:\n    * ``grouping_regex: regular_expression''\n      Assign a file to a subgroup if its full path matches the given regular\n      expression.\n    A separate service is created for each subgroup, prefixed with its parent\n    group name (i.e. <parent group name> <subgroup name>). The order in which\n    subgroups and corresponding patterns are specified matters: rules are\n    processed in the given order.\n    Files that match the specified subgroups are shown as a separate service\n    and are excluded from the parent file group.\n4. Output\n    You can choose from three different ways the output will be aggregated:\n    * ``output: file_stats''\n      Output the full information for on every single file that is processed.\n      This is the default.\n    * ``output: count_only''\n      Only count the files matching all of the provided filters. Unless\n      required for the filtering operation, no stat call on the files is\n      made.\n    * ``output: extremes_only''\n      Only report the youngest, oldest, smallest, and biggest files. In case\n      checks only require this information, we can signifficantly reduce data.\n    * ``output: single_file''\n      Monitor a single file and send its metrics. If a input_pattern of a .cfg section\n      matches multiple files, the agent sents one subsection per file.\n\nYou should find an example configuration file at\n'../cfg_examples/filestats.cfg' relative to this file.\n\"\"\"\n\n__version__ = \"2.5.0b1\"\n\nimport collections\nimport configparser\nimport errno\nimport glob\nimport logging\nimport operator\nimport os\nimport re\nimport shlex\nimport sys\nimport time\n\n\ndef ensure_str(s):\n    if sys.version_info[0] >= 3:\n        if isinstance(s, bytes):\n            return s.decode(\"utf-8\")\n    elif isinstance(s, unicode):  # noqa: F821\n        return s.encode(\"utf-8\")\n    return s\n\n\ndef ensure_text(s):\n    if sys.version_info[0] >= 3:\n        if isinstance(s, bytes):\n            return s.decode(\"utf-8\")\n    elif isinstance(s, str):\n        return s.decode(\"utf-8\")\n    return s\n\n\nDEFAULT_CFG_FILE = os.path.join(os.getenv(\"MK_CONFDIR\", \"\"), \"filestats.cfg\")\n\nDEFAULT_CFG_SECTION = {\"output\": \"file_stats\", \"subgroups_delimiter\": \"@\"}\n\nFILTER_SPEC_PATTERN = re.compile(\"(?P<operator>[<>=]+)(?P<value>.+)\")\n\nLOGGER = logging.getLogger(__name__)\n\n\ndef parse_arguments(argv=None):\n    if argv is None:\n        argv = sys.argv[1:]\n\n    parsed_args = {}\n\n    if \"-h\" in argv or \"--help\" in argv:\n        sys.stderr.write(ensure_str(__doc__))\n        sys.exit(0)\n\n    if \"-v\" in argv or \"--verbose\" in argv:\n        logging.basicConfig(level=logging.INFO, format=\"%(levelname)s: %(message)s\")\n    elif \"-vv\" in argv or \"--verbose\" in argv:\n        logging.basicConfig(level=logging.DEBUG, format=\"%(levelname)s: %(lineno)s: %(message)s\")\n    else:\n        LOGGER.propagate = False\n\n    parsed_args[\"cfg_file\"] = DEFAULT_CFG_FILE\n    for opt in (\"-c\", \"--config-file\"):\n        if opt in argv:\n            try:\n                parsed_args[\"cfg_file\"] = argv[argv.index(opt) + 1]\n            except IndexError:\n                sys.stderr.write(\"missing value for option %r\\n\" % opt)\n                sys.exit(1)\n    return parsed_args\n\n\nclass FileStat:\n    \"\"\"Wrapper around os.stat\n\n    Only call os.stat once.\n    \"\"\"\n\n    @classmethod\n    def from_path(cls, raw_file_path, file_path):\n        LOGGER.debug(\"Creating FileStat(%r)\", raw_file_path)\n        try:\n            file_stat = os.stat(raw_file_path)\n        except OSError as exc:\n            # report on errors, regard failure as 'file'\n            stat_status = \"file vanished\" if exc.errno == errno.ENOENT else str(exc)\n            return cls(file_path, stat_status)\n\n        try:\n            size = int(file_stat.st_size)\n        except ValueError as exc:\n            stat_status = str(exc)\n            return cls(file_path, stat_status)\n\n        try:\n            m_time = int(file_stat.st_mtime)\n            age = int(time.time()) - m_time\n        except ValueError as exc:\n            stat_status = str(exc)\n            return cls(file_path, stat_status, size)\n\n        return cls(file_path, \"ok\", size, age, m_time)\n\n    def __init__(self, file_path, stat_status, size=None, age=None, m_time=None):\n        super().__init__()\n        self.file_path = file_path\n        self.stat_status = stat_status\n        self.size = size\n        self.age = age\n        self._m_time = m_time\n\n    def dumps(self):\n        data = {\n            \"type\": \"file\",\n            \"path\": self.file_path,\n            \"stat_status\": self.stat_status,\n            \"size\": self.size,\n            \"age\": self.age,\n            \"mtime\": self._m_time,\n        }\n        return repr(data)\n\n\n# .\n#   .--Input---------------------------------------------------------------.\n#   |                      ___                   _                         |\n#   |                     |_ _|_ __  _ __  _   _| |_                       |\n#   |                      | || '_ \\| '_ \\| | | | __|                      |\n#   |                      | || | | | |_) | |_| | |_                       |\n#   |                     |___|_| |_| .__/ \\__,_|\\__|                      |\n#   |                               |_|                                    |\n#   +----------------------------------------------------------------------+\n#   |                                                                      |\n#   '----------------------------------------------------------------------'\n\n\ndef _sanitize_path(raw_file_path):\n    # raw_file_path is the value returned by iglob. This value cannot typed meaningfully:\n    # * if the path is utf-8 decodable, python2: unicode\n    # * if the path is not utf-8 decodable, python2: str\n    # * python3: str, possibly with surrogates aka it can only be encoded again like so:\n    #   str_with_surrogates.encode('utf-8', 'surrogateescape')\n    if sys.version_info[0] >= 3:\n        return raw_file_path.encode(\"utf-8\", \"surrogateescape\").decode(\"utf-8\", \"replace\")\n    if isinstance(raw_file_path, unicode):  # type: ignore[name-defined] # noqa: F821\n        return raw_file_path\n    return raw_file_path.decode(\"utf-8\", \"replace\")\n\n\nclass PatternIterator:\n    \"\"\"Recursively iterate over all files\"\"\"\n\n    def __init__(self, pattern_list, filters):\n        super().__init__()\n        self._patterns = [os.path.abspath(os.path.expanduser(p)) for p in pattern_list]\n        self._regex_filters = [f for f in filters if isinstance(f, RegexFilter)]\n        self._numerical_filters = [f for f in filters if isinstance(f, AbstractNumericFilter)]\n\n    def _file_stats(self, raw_file_paths):\n        for raw_file_path in raw_file_paths:\n            file_path = _sanitize_path(raw_file_path)\n            if not all(f.matches(file_path) for f in self._regex_filters):\n                LOGGER.debug(\"File %r does not match any regex filter\", raw_file_path)\n                continue\n\n            file_stat = FileStat.from_path(raw_file_path, file_path)\n            if not all(f.matches(file_stat) for f in self._numerical_filters):\n                LOGGER.debug(\"File %r does not match any numerical filter\", raw_file_path)\n                continue\n\n            LOGGER.debug(\"File %s matches all regex and numerical filters\", raw_file_path)\n            yield file_stat\n\n    def __iter__(self):\n        for pattern in self._patterns:\n            LOGGER.info(\"processing pattern: %r\", pattern)\n            # pattern needs to be a unicode/python3 str. Otherwise things might go sour, for instance:\n            # If we pass \"*\".encode(\"utf-8\"), then a non-UTF-8 filesystem may no longer realize that\n            # b'\\x2A' refers to a wildcard. Instead iglob is responsible for conversion.\n            for item in glob.iglob(pattern):\n                if os.path.isdir(item):\n                    # equivalent to `find -type f`\n                    for currentpath, _folders, file_names in os.walk(item):\n                        for file_stat in self._file_stats(\n                            [os.path.join(currentpath, fn) for fn in file_names]\n                        ):\n                            yield file_stat\n                else:\n                    for file_stat in self._file_stats([item]):\n                        yield file_stat\n\n\ndef get_file_iterator(config):\n    \"\"\"get a FileStat iterator\"\"\"\n    input_specs = [(k[6:], v) for k, v in config.items() if k.startswith(\"input_\")]\n    if not input_specs:\n        raise ValueError(\"missing input definition\")\n    if len(input_specs) != 1:  # currently not supported\n        raise ValueError(\"multiple input definitions: %r\" % input_specs)\n    variety, spec_string = input_specs[0]\n    if variety != \"patterns\":\n        raise ValueError(\"unknown input type: %r\" % variety)\n    patterns = shlex.split(spec_string)\n    return PatternIterator(patterns, get_file_filters(config))\n\n\n# .\n#   .--Filtering-----------------------------------------------------------.\n#   |                _____ _ _ _            _                              |\n#   |               |  ___(_) | |_ ___ _ __(_)_ __   __ _                  |\n#   |               | |_  | | | __/ _ \\ '__| | '_ \\ / _` |                 |\n#   |               |  _| | | | ||  __/ |  | | | | | (_| |                 |\n#   |               |_|   |_|_|\\__\\___|_|  |_|_| |_|\\__, |                 |\n#   |                                               |___/                  |\n#   +----------------------------------------------------------------------+\n#   |                                                                      |\n#   '----------------------------------------------------------------------'\n\n\nCOMPARATORS = {\n    \"<\": operator.lt,\n    \"<=\": operator.le,\n    \">\": operator.gt,\n    \">=\": operator.ge,\n    \"==\": operator.eq,\n}\n\n\nclass AbstractNumericFilter:\n    \"\"\"Common code for filtering by comparing integers\"\"\"\n\n    def __init__(self, spec_string):\n        super().__init__()\n        match = FILTER_SPEC_PATTERN.match(spec_string)\n        if match is None:\n            raise ValueError(\"unable to parse filter spec: %r\" % spec_string)\n        spec = match.groupdict()\n        comp = COMPARATORS.get(spec[\"operator\"])\n        if comp is None:\n            raise ValueError(\"unknown operator for numeric filter: %r\" % spec[\"operator\"])\n        reference = int(spec[\"value\"])\n        self._matches_value = lambda actual: comp(int(actual), reference)\n\n    def matches(self, filestat):\n        raise NotImplementedError()\n\n\nclass SizeFilter(AbstractNumericFilter):\n    def matches(self, filestat):\n        \"\"\"apply AbstractNumericFilter ti file size\"\"\"\n        size = filestat.size\n        if size is not None and size != \"null\":\n            return self._matches_value(size)\n        # Don't return vanished files.\n        # Other cases are a problem, and should be included\n        return filestat.stat_status != \"file vanished\"\n\n\nclass AgeFilter(AbstractNumericFilter):\n    def matches(self, filestat):\n        \"\"\"apply AbstractNumericFilter ti file age\"\"\"\n        age = filestat.age\n        if age is not None and age != \"null\":\n            return self._matches_value(age)\n        # Don't return vanished files.\n        # Other cases are a problem, and should be included\n        return filestat.stat_status != \"file vanished\"\n\n\nclass RegexFilter:\n    def __init__(self, regex_pattern):\n        super().__init__()\n        LOGGER.debug(\"initializing with pattern: %r\", regex_pattern)\n        self._regex = re.compile(ensure_text(regex_pattern), re.UNICODE)\n\n    def matches(self, file_path):\n        return bool(self._regex.match(file_path))\n\n\nclass InverseRegexFilter(RegexFilter):\n    def matches(self, file_path):\n        return not self._regex.match(file_path)\n\n\ndef get_file_filters(config):\n    filter_specs = ((k[7:], v) for k, v in config.items() if k.startswith(\"filter_\"))\n\n    filters = []\n    for variety, spec_string in filter_specs:\n        LOGGER.debug(\"found filter spec: %r\", (variety, spec_string))\n        try:\n            filter_type = {\n                \"regex\": RegexFilter,\n                \"regex_inverse\": InverseRegexFilter,\n                \"size\": SizeFilter,\n                \"age\": AgeFilter,\n            }[variety]\n        except KeyError:\n            raise ValueError(\"unknown filter type: %r\" % variety)\n        filters.append(filter_type(spec_string))\n\n    # add regex filters first to save stat calls\n    return sorted(filters, key=lambda x: not isinstance(x, RegexFilter))\n\n\n# .\n#   .--Grouping------------------------------------------------------------.\n#   |               ____                       _                           |\n#   |              / ___|_ __ ___  _   _ _ __ (_)_ __   __ _               |\n#   |             | |  _| '__/ _ \\| | | | '_ \\| | '_ \\ / _` |              |\n#   |             | |_| | | | (_) | |_| | |_) | | | | | (_| |              |\n#   |              \\____|_|  \\___/ \\__,_| .__/|_|_| |_|\\__, |              |\n#   |                                   |_|            |___/               |\n#   +----------------------------------------------------------------------+\n#   |                                                                      |\n#   '----------------------------------------------------------------------'\n\n\ndef parse_grouping_config(\n    config,\n    raw_config_section_name,\n    options,\n    subgroups_delimiter,\n):\n    parent_group_name, child_group_name = raw_config_section_name.split(subgroups_delimiter, 1)\n\n    for option in options:\n        if option.startswith(\"grouping_\"):\n            grouping_type = option.split(\"_\", 1)[1]\n            grouping_rule = config.get(raw_config_section_name, option)\n\n    LOGGER.info(\"found subgroup: %s\", raw_config_section_name)\n    return parent_group_name, (\n        child_group_name,\n        {\n            \"type\": grouping_type,\n            \"rule\": grouping_rule,\n        },\n    )\n\n\ndef _grouping_construct_group_name(parent_group_name, child_group_name=\"\"):\n    \"\"\"allow the user to format the service name using '%s'.\n\n    >>> _grouping_construct_group_name('aard %s vark', 'banana')\n    'aard banana vark'\n\n    >>> _grouping_construct_group_name('aard %s vark %s %s', 'banana')\n    'aard banana vark %s %s'\n\n    >>> _grouping_construct_group_name('aard %s vark')\n    'aard %s vark'\n\n    >>> _grouping_construct_group_name('aard %s', '')\n    'aard %s'\n    \"\"\"\n\n    format_specifiers_count = parent_group_name.count(\"%s\")\n    if not format_specifiers_count:\n        return (\"%s %s\" % (parent_group_name, child_group_name)).strip()\n\n    if not child_group_name:\n        return parent_group_name\n\n    return (\n        parent_group_name % ((child_group_name,) + (\"%s\",) * (format_specifiers_count - 1))\n    ).strip()\n\n\ndef _get_matching_child_group(single_file, grouping_conditions):\n    for child_group_name, grouping_condition in grouping_conditions:\n        if re.match(grouping_condition[\"rule\"], single_file.file_path):\n            return child_group_name\n    return \"\"\n\n\ndef grouping_multiple_groups(config_section_name, files_iter, grouping_conditions):\n    \"\"\"create multiple groups per section if the agent is configured\n    for grouping. each group is shown as a separate service. if a file\n    does not belong to a group, it is added to the section.\"\"\"\n    parent_group_name = config_section_name\n    # Initalise dict with parent and child group because they should be in the section\n    # with 0 count if there are no files for them.\n    grouped_files = {\n        \"\": [],  # parent\n    }  # type: dict[str, list[FileStat]]\n    grouped_files.update({g[0]: [] for g in grouping_conditions})\n    for single_file in files_iter:\n        matching_child_group = _get_matching_child_group(single_file, grouping_conditions)\n        grouped_files[matching_child_group].append(single_file)\n\n    for matching_child_group, files in grouped_files.items():\n        yield _grouping_construct_group_name(parent_group_name, matching_child_group), files\n\n\ndef grouping_single_group(config_section_name, files_iter, _grouping_conditions):\n    \"\"\"create one single group per section\"\"\"\n    group_name = config_section_name\n    yield group_name, files_iter\n\n\ndef get_grouper(grouping_conditions):\n    if grouping_conditions:\n        return grouping_multiple_groups\n    return grouping_single_group\n\n\n# .\n#   .--Output--------------------------------------------------------------.\n#   |                    ___        _               _                      |\n#   |                   / _ \\ _   _| |_ _ __  _   _| |_                    |\n#   |                  | | | | | | | __| '_ \\| | | | __|                   |\n#   |                  | |_| | |_| | |_| |_) | |_| | |_                    |\n#   |                   \\___/ \\__,_|\\__| .__/ \\__,_|\\__|                   |\n#   |                                  |_|                                 |\n#   +----------------------------------------------------------------------+\n#   |                                                                      |\n#   '----------------------------------------------------------------------'\n\n\ndef output_aggregator_count_only(group_name, files_iter):\n    yield \"[[[count_only %s]]]\" % group_name\n    count = sum(1 for __ in files_iter)\n    yield repr({\"type\": \"summary\", \"count\": count})\n\n\ndef output_aggregator_file_stats(group_name, files_iter):\n    yield \"[[[file_stats %s]]]\" % group_name\n    count = 0\n    for count, filestat in enumerate(files_iter, 1):\n        yield filestat.dumps()\n    yield repr({\"type\": \"summary\", \"count\": count})\n\n\ndef output_aggregator_extremes_only(group_name, files_iter):\n    yield \"[[[extremes_only %s]]]\" % group_name\n\n    files = list(files_iter)\n    count = len(files)\n\n    if not count:\n        yield repr({\"type\": \"summary\", \"count\": count})\n        return\n\n    min_age = max_age = min_size = max_size = files[0]\n\n    for filestat in files[1:]:\n        if filestat.age is None:\n            continue\n\n        if min_age.age is None or filestat.age < min_age.age:\n            min_age = filestat\n        if max_age.age is None or filestat.age > max_age.age:\n            max_age = filestat\n        if min_size.size is None or filestat.size < min_size.size:\n            min_size = filestat\n        if max_size.size is None or filestat.size > max_size.size:\n            max_size = filestat\n\n    for extreme_file in set((min_age, max_age, min_size, max_size)):\n        yield extreme_file.dumps()\n    yield repr({\"type\": \"summary\", \"count\": count})\n\n\ndef output_aggregator_single_file(group_name, files_iter):\n    for lazy_file in files_iter:\n        count_format_specifiers = group_name.count(\"%s\")\n\n        if count_format_specifiers == 0:\n            subsection_name = group_name\n        else:\n            subsection_name = group_name % (\n                (lazy_file.file_path,) + ((\"%s\",) * (count_format_specifiers - 1))\n            )\n        yield \"[[[single_file %s]]]\" % subsection_name\n        yield lazy_file.dumps()\n\n\ndef get_output_aggregator(config):\n    output_spec = config.get(\"output\")\n    try:\n        return {\n            \"count_only\": output_aggregator_count_only,\n            \"extremes_only\": output_aggregator_extremes_only,\n            \"file_stats\": output_aggregator_file_stats,\n            \"single_file\": output_aggregator_single_file,\n        }[output_spec]\n    except KeyError:\n        raise ValueError(\"unknown 'output' spec: %r\" % output_spec)\n\n\ndef write_output(groups, output_aggregator):\n    for group_name, group_files_iter in groups:\n        for line in output_aggregator(group_name, group_files_iter):\n            sys.stdout.write(\"%s\\n\" % line)\n\n\n# .\n#   .--Main----------------------------------------------------------------.\n#   |                        __  __       _                                |\n#   |                       |  \\/  | __ _(_)_ __                           |\n#   |                       | |\\/| |/ _` | | '_ \\                          |\n#   |                       | |  | | (_| | | | | |                         |\n#   |                       |_|  |_|\\__,_|_|_| |_|                         |\n#   |                                                                      |\n#   +----------------------------------------------------------------------+\n#   |                                                                      |\n#   '----------------------------------------------------------------------'\n\n\ndef iter_config_section_dicts(cfg_file=None):\n    if cfg_file is None:\n        cfg_file = DEFAULT_CFG_FILE\n    # FIXME: Python 2.6 has no OrderedDict at all, it is only available in a separate ordereddict\n    # package, but we simply can't assume that this is installed on the client!\n    config = configparser.ConfigParser(\n        DEFAULT_CFG_SECTION,\n        dict_type=collections.OrderedDict,\n    )\n    LOGGER.debug(\"trying to read %r\", cfg_file)\n    files_read = config.read(cfg_file)\n    LOGGER.info(\"read configration file(s): %r\", files_read)\n\n    parsed_config = {}\n    for raw_cfg_section_name in config.sections():\n        options = config.options(raw_cfg_section_name)\n        subgroups_delimiter = config.get(raw_cfg_section_name, \"subgroups_delimiter\")\n        if subgroups_delimiter not in raw_cfg_section_name:\n            consolidated_cfg_section_name = raw_cfg_section_name\n            parsed_config[consolidated_cfg_section_name] = {\n                k: config.get(raw_cfg_section_name, k) for k in options\n            }\n            continue\n        parent_group_name, parsed_grouping_config = parse_grouping_config(\n            config,\n            raw_cfg_section_name,\n            options,\n            subgroups_delimiter,\n        )\n        consolidated_cfg_section_name = parent_group_name\n        # TODO: The below suppressions are due to the fact that typing the parsed config properly\n        # requires a more sophisticated type (perhaps a class) and a bigger refactoring to validate\n        # the options and dispatch immediately after parsing is complete.\n        parsed_config[consolidated_cfg_section_name].setdefault(\n            \"grouping\",\n            [],  # type: ignore[arg-type]\n        ).append(  # type: ignore[attr-defined]\n            parsed_grouping_config\n        )\n\n    for consolidated_cfg_section_name, parsed_option in parsed_config.items():\n        yield consolidated_cfg_section_name, parsed_option\n\n\ndef main():\n    args = parse_arguments()\n\n    sys.stdout.write(\"<<<filestats:sep(0)>>>\\n\")\n    for config_section_name, config in iter_config_section_dicts(args[\"cfg_file\"]):\n        # 1 input and\n        # 2 filtering\n        filtered_files = get_file_iterator(config)\n\n        # 3 grouping\n        grouping_conditions = config.get(\"grouping\")\n        grouper = get_grouper(grouping_conditions)\n        groups = grouper(config_section_name, filtered_files, grouping_conditions)\n\n        # 4 output\n        output_aggregator = get_output_aggregator(config)\n        write_output(groups, output_aggregator)\n\n\nif __name__ == \"__main__\":\n    main()\n"}
{"type": "source_file", "path": "agents/plugins/mk_jolokia.py", "content": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n# Copyright (C) 2019 Checkmk GmbH - License: GNU General Public License v2\n# This file is part of Checkmk (https://checkmk.com). It is subject to the terms and\n# conditions defined in the file COPYING, which is part of this source code package.\n\nimport io\nimport os\nimport re\nimport socket\nimport sys\nimport urllib.parse\n\n__version__ = \"2.5.0b1\"\n\nUSER_AGENT = \"checkmk-agent-mk_jolokia-\" + __version__\n\n# For Python 3 sys.stdout creates \\r\\n as newline for Windows.\n# Checkmk can't handle this therefore we rewrite sys.stdout to a new_stdout function.\n# If you want to use the old behaviour just use old_stdout.\nif sys.version_info[0] >= 3:\n    new_stdout = io.TextIOWrapper(\n        sys.stdout.buffer, newline=\"\\n\", encoding=sys.stdout.encoding, errors=sys.stdout.errors\n    )\n    old_stdout, sys.stdout = sys.stdout, new_stdout\n\n# Continue if typing cannot be imported, e.g. for running unit tests\ntry:\n    from typing import Any, TYPE_CHECKING  # noqa: F401\n\n    if TYPE_CHECKING:\n        from collections.abc import Callable  # noqa: F401\nexcept ImportError:\n    pass\n\ntry:\n    try:\n        import simplejson as json\n    except ImportError:\n        import json  # type: ignore[no-redef]\nexcept ImportError:\n    sys.stdout.write(\n        \"<<<jolokia_info>>>\\n\"\n        \"Error: mk_jolokia requires either the json or simplejson library.\"\n        \" Please either use a Python version that contains the json library or install the\"\n        \" simplejson library on the monitored system.\\n\"\n    )\n    sys.exit(1)\n\ntry:\n    import requests\n    from requests.auth import HTTPDigestAuth\n\n    # These days urllib3 would be included directly, but we leave it as it is for the moment\n    # for compatibility reasons - at least for the agent plugin here.\n    from requests.packages import urllib3  # type: ignore[attr-defined]\nexcept ImportError:\n    sys.stdout.write(\n        \"<<<jolokia_info>>>\\n\"\n        \"Error: mk_jolokia requires the requests library.\"\n        \" Please install it on the monitored system.\\n\"\n    )\n    sys.exit(1)\n\nVERBOSE = sys.argv.count(\"--verbose\") + sys.argv.count(\"-v\") + 2 * sys.argv.count(\"-vv\")\nDEBUG = sys.argv.count(\"--debug\")\n\nMBEAN_SECTIONS = {\n    \"jvm_threading\": (\"java.lang:type=Threading\",),\n    \"jvm_memory\": (\n        \"java.lang:type=Memory\",\n        \"java.lang:name=*,type=MemoryPool\",\n    ),\n    \"jvm_runtime\": (\"java.lang:type=Runtime/Uptime,Name\",),\n    \"jvm_garbagecollectors\": (\n        \"java.lang:name=*,type=GarbageCollector/CollectionCount,CollectionTime,Name\",\n    ),\n}  # type: dict[str, tuple[str, ...]]\n\nMBEAN_SECTIONS_SPECIFIC = {\n    \"tomcat\": {\n        \"jvm_threading\": (\n            \"*:name=*,type=ThreadPool/maxThreads,currentThreadCount,currentThreadsBusy/\",\n        ),\n    },\n}\n\nQUERY_SPECS_LEGACY = [\n    (\n        \"net.sf.ehcache:CacheManager=CacheManagerApplication*,*,type=CacheStatistics\",\n        \"OffHeapHits\",\n        \"\",\n        [],\n        True,\n    ),\n    (\n        \"net.sf.ehcache:CacheManager=CacheManagerApplication*,*,type=CacheStatistics\",\n        \"OnDiskHits\",\n        \"\",\n        [],\n        True,\n    ),\n    (\n        \"net.sf.ehcache:CacheManager=CacheManagerApplication*,*,type=CacheStatistics\",\n        \"InMemoryHitPercentage\",\n        \"\",\n        [],\n        True,\n    ),\n    (\n        \"net.sf.ehcache:CacheManager=CacheManagerApplication*,*,type=CacheStatistics\",\n        \"CacheMisses\",\n        \"\",\n        [],\n        True,\n    ),\n    (\n        \"net.sf.ehcache:CacheManager=CacheManagerApplication*,*,type=CacheStatistics\",\n        \"OnDiskHitPercentage\",\n        \"\",\n        [],\n        True,\n    ),\n    (\n        \"net.sf.ehcache:CacheManager=CacheManagerApplication*,*,type=CacheStatistics\",\n        \"MemoryStoreObjectCount\",\n        \"\",\n        [],\n        True,\n    ),\n    (\n        \"net.sf.ehcache:CacheManager=CacheManagerApplication*,*,type=CacheStatistics\",\n        \"DiskStoreObjectCount\",\n        \"\",\n        [],\n        True,\n    ),\n    (\n        \"net.sf.ehcache:CacheManager=CacheManagerApplication*,*,type=CacheStatistics\",\n        \"CacheMissPercentage\",\n        \"\",\n        [],\n        True,\n    ),\n    (\n        \"net.sf.ehcache:CacheManager=CacheManagerApplication*,*,type=CacheStatistics\",\n        \"CacheHitPercentage\",\n        \"\",\n        [],\n        True,\n    ),\n    (\n        \"net.sf.ehcache:CacheManager=CacheManagerApplication*,*,type=CacheStatistics\",\n        \"OffHeapHitPercentage\",\n        \"\",\n        [],\n        True,\n    ),\n    (\n        \"net.sf.ehcache:CacheManager=CacheManagerApplication*,*,type=CacheStatistics\",\n        \"InMemoryMisses\",\n        \"\",\n        [],\n        True,\n    ),\n    (\n        \"net.sf.ehcache:CacheManager=CacheManagerApplication*,*,type=CacheStatistics\",\n        \"OffHeapStoreObjectCount\",\n        \"\",\n        [],\n        True,\n    ),\n    (\n        \"net.sf.ehcache:CacheManager=CacheManagerApplication*,*,type=CacheStatistics\",\n        \"WriterQueueLength\",\n        \"\",\n        [],\n        True,\n    ),\n    (\n        \"net.sf.ehcache:CacheManager=CacheManagerApplication*,*,type=CacheStatistics\",\n        \"WriterMaxQueueSize\",\n        \"\",\n        [],\n        True,\n    ),\n    (\n        \"net.sf.ehcache:CacheManager=CacheManagerApplication*,*,type=CacheStatistics\",\n        \"OffHeapMisses\",\n        \"\",\n        [],\n        True,\n    ),\n    (\n        \"net.sf.ehcache:CacheManager=CacheManagerApplication*,*,type=CacheStatistics\",\n        \"InMemoryHits\",\n        \"\",\n        [],\n        True,\n    ),\n    (\n        \"net.sf.ehcache:CacheManager=CacheManagerApplication*,*,type=CacheStatistics\",\n        \"AssociatedCacheName\",\n        \"\",\n        [],\n        True,\n    ),\n    (\n        \"net.sf.ehcache:CacheManager=CacheManagerApplication*,*,type=CacheStatistics\",\n        \"ObjectCount\",\n        \"\",\n        [],\n        True,\n    ),\n    (\n        \"net.sf.ehcache:CacheManager=CacheManagerApplication*,*,type=CacheStatistics\",\n        \"OnDiskMisses\",\n        \"\",\n        [],\n        True,\n    ),\n    (\n        \"net.sf.ehcache:CacheManager=CacheManagerApplication*,*,type=CacheStatistics\",\n        \"CacheHits\",\n        \"\",\n        [],\n        True,\n    ),\n]  # type: list[tuple[str, str, str, list[object], bool]]\n\nQUERY_SPECS_SPECIFIC_LEGACY = {\n    \"weblogic\": [\n        (\"*:*\", \"CompletedRequestCount\", None, [\"ServerRuntime\"], False),\n        (\"*:*\", \"QueueLength\", None, [\"ServerRuntime\"], False),\n        (\"*:*\", \"StandbyThreadCount\", None, [\"ServerRuntime\"], False),\n        (\"*:*\", \"PendingUserRequestCount\", None, [\"ServerRuntime\"], False),\n        (\"*:Name=ThreadPoolRuntime,*\", \"ExecuteThreadTotalCount\", None, [\"ServerRuntime\"], False),\n        (\"*:*\", \"ExecuteThreadIdleCount\", None, [\"ServerRuntime\"], False),\n        (\"*:*\", \"HoggingThreadCount\", None, [\"ServerRuntime\"], False),\n        (\n            \"*:Type=WebAppComponentRuntime,*\",\n            \"OpenSessionsCurrentCount\",\n            None,\n            [\"ServerRuntime\", \"ApplicationRuntime\"],\n            False,\n        ),\n    ],\n    \"tomcat\": [\n        (\"*:type=Manager,*\", \"activeSessions,maxActiveSessions\", None, [\"path\", \"context\"], False),\n        (\"*:j2eeType=Servlet,name=default,*\", \"stateName\", None, [\"WebModule\"], False),\n        # Check not yet working\n        (\"*:j2eeType=Servlet,name=default,*\", \"requestCount\", None, [\"WebModule\"], False),\n        # too wide location for addressing the right info\n        # ( \"*:j2eeType=Servlet,*\", \"requestCount\", None, [ \"WebModule\" ] , False),\n    ],\n    \"jboss\": [\n        (\"*:type=Manager,*\", \"activeSessions,maxActiveSessions\", None, [\"path\", \"context\"], False),\n    ],\n}\n\nAVAILABLE_PRODUCTS = sorted(\n    set(QUERY_SPECS_SPECIFIC_LEGACY.keys()) | set(MBEAN_SECTIONS_SPECIFIC.keys())\n)\n\n# Default global configuration: key, value [, help]\nDEFAULT_CONFIG_TUPLES = (\n    (\"protocol\", \"http\", \"Protocol to use (http/https).\"),\n    (\"server\", \"localhost\", \"Host name or IP address of the Jolokia server.\"),\n    (\"port\", 8080, \"TCP Port of the Jolokia server.\"),\n    (\"suburi\", \"jolokia\", \"Path-component of the URI to query.\"),\n    (\"user\", \"monitoring\", \"Username to use for connecting.\"),\n    (\"password\", None, \"Password to use for connecting.\"),\n    (\"mode\", \"digest\", 'Authentication mode. Can be \"basic\", \"digest\" or \"https\".'),\n    (\"instance\", None, \"Name of the instance in the monitoring. Defaults to port.\"),\n    (\"verify\", None),\n    (\"client_cert\", None, \"Path to client cert for https authentication.\"),\n    (\"client_key\", None, \"Client cert secret for https authentication.\"),\n    (\"service_url\", None),\n    (\"service_user\", None),\n    (\"service_password\", None),\n    (\n        \"product\",\n        None,\n        \"Product description. Available: %s. If not provided,\"\n        \" we try to detect the product from the jolokia info section.\"\n        % \", \".join(AVAILABLE_PRODUCTS),\n    ),\n    (\"timeout\", 1.0, \"Connection/read timeout for requests.\"),\n    (\"custom_vars\", []),\n    # List of instances to monitor. Each instance is a dict where\n    # the global configuration values can be overridden.\n    (\"instances\", [{}]),\n)  # type: tuple[tuple[str | None | float | list[Any], ...], ...]\n\n\nclass SkipInstance(RuntimeError):\n    pass\n\n\nclass SkipMBean(RuntimeError):\n    pass\n\n\ndef get_default_config_dict():\n    return {elem[0]: elem[1] for elem in DEFAULT_CONFIG_TUPLES}\n\n\ndef write_section(name, iterable):\n    sys.stdout.write(\"<<<%s:sep(0)>>>\\n\" % name)\n    for line in iterable:\n        sys.stdout.write(chr(0).join(map(str, line)) + \"\\n\")\n\n\ndef cached(function):\n    cache = {}  # type: dict[str, Callable[[object], object]]\n\n    def cached_function(*args):\n        key = repr(args)\n        try:\n            return cache[key]\n        except KeyError:\n            return cache.setdefault(key, function(*args))\n\n    return cached_function\n\n\nclass JolokiaInstance:\n    # use this to filter headers whien recording via vcr trace\n    FILTER_SENSITIVE = {\"filter_headers\": [(\"authorization\", \"****\")]}\n\n    @staticmethod\n    def _sanitize_config(config):\n        instance = config.get(\"instance\")\n        err_msg = \"%s in configuration\"\n        if instance:\n            err_msg += \" for %s\" % instance\n\n        required_keys = set((\"protocol\", \"server\", \"port\", \"suburi\", \"timeout\"))\n        auth_mode = config.get(\"mode\")\n        if auth_mode in (\"digest\", \"basic\", \"basic_preemtive\"):\n            required_keys |= set((\"user\", \"password\"))\n        elif auth_mode == \"https\":\n            required_keys |= set((\"client_cert\", \"client_key\"))\n        if config.get(\"service_url\") is not None and config.get(\"service_user\") is not None:\n            required_keys.add(\"service_password\")\n        missing_keys = required_keys - set(config.keys())\n        if missing_keys:\n            raise ValueError(err_msg % (\"Missing key(s): %s\" % \", \".join(sorted(missing_keys))))\n\n        if not instance:\n            instance = str(config[\"port\"])\n        config[\"instance\"] = instance.replace(\" \", \"_\")\n\n        # port must be (or look like) an integer, timeout like float\n        for key, type_ in ((\"port\", int), (\"timeout\", float)):\n            val = config[key]\n            try:\n                config[key] = type_(val)\n            except ValueError:\n                raise ValueError(err_msg % (\"Invalid %s %r\" % (key, val)))\n\n        if config.get(\"server\") == \"use fqdn\":\n            config[\"server\"] = socket.getfqdn()\n\n        # if \"verify\" was not set to bool/string\n        if config.get(\"verify\") is None:\n            # handle legacy \"cert_path\"\n            cert_path = config.get(\"cert_path\")\n            if cert_path not in (\"_default\", None):\n                # The '_default' was the default value\n                # up to cmk version 1.5.0p8. It broke things.\n                config[\"verify\"] = cert_path\n            else:\n                # this is default, but be explicit\n                config[\"verify\"] = True\n\n        return config\n\n    def __init__(self, config, user_agent):\n        # type: (object, str) -> None\n        super().__init__()\n        self._config = self._sanitize_config(config)\n\n        self.name = self._config[\"instance\"]\n        self.product = self._config.get(\"product\")\n        self.custom_vars = self._config.get(\"custom_vars\", [])\n\n        self.base_url = self._get_base_url()\n        self.target = self._get_target()\n        self.post_config = {\"ignoreErrors\": \"true\"}\n        self._session = self._initialize_http_session(user_agent)\n\n    def _get_base_url(self):\n        return \"%s://%s:%d/%s/\" % (\n            self._config[\"protocol\"].strip(\"/\"),\n            self._config[\"server\"].strip(\"/\"),\n            self._config[\"port\"],\n            self._config[\"suburi\"],\n        )\n\n    def _get_target(self):\n        url = self._config.get(\"service_url\")\n        if url is None:\n            return {}\n        user = self._config.get(\"service_user\")\n        if user is None:\n            return {\"url\": url}\n        return {\n            \"url\": url,\n            \"user\": user,\n            \"password\": self._config[\"service_password\"],\n        }\n\n    def _initialize_http_session(self, user_agent):\n        # type: (str) -> requests.Session\n        session = requests.Session()\n        # Watch out: we must provide the verify keyword to every individual request call!\n        # Else it will be overwritten by the REQUESTS_CA_BUNDLE env variable\n        session.verify = self._config[\"verify\"]\n        if session.verify is False:\n            urllib3.disable_warnings(category=urllib3.exceptions.InsecureRequestWarning)\n        session.headers[\"User-Agent\"] = user_agent\n\n        auth_method = self._config.get(\"mode\")\n        if auth_method is None:\n            return session\n\n        # initialize authentication\n        if auth_method == \"https\":\n            session.cert = (\n                self._config[\"client_cert\"],\n                self._config[\"client_key\"],\n            )\n        elif auth_method == \"digest\":\n            session.auth = HTTPDigestAuth(\n                self._config[\"user\"],\n                self._config[\"password\"],\n            )\n        elif auth_method in (\"basic\", \"basic_preemptive\"):\n            session.auth = (\n                self._config[\"user\"],\n                self._config[\"password\"],\n            )\n        else:\n            raise NotImplementedError(\"Authentication method %r\" % auth_method)\n\n        return session\n\n    def get_post_data(self, path, function, use_target):\n        segments = re.split(r\"(?<!!)/\", path.strip(\"/\"))\n        segments[0] = segments[0].replace(\"!/\", \"/\")\n        # we may have one to three segments:\n        data = dict(zip((\"mbean\", \"attribute\", \"path\"), segments))\n\n        data[\"type\"] = function\n        if use_target and self.target:\n            data[\"target\"] = self.target\n        data[\"config\"] = self.post_config\n        return data\n\n    def post(self, data):\n        post_data = json.dumps(data)\n        if VERBOSE:\n            sys.stderr.write(\"\\nDEBUG: POST data: %r\\n\" % post_data)\n        try:\n            # Watch out: we must provide the verify keyword to every individual request call!\n            # Else it will be overwritten by the REQUESTS_CA_BUNDLE env variable\n            raw_response = self._session.post(\n                self.base_url,\n                data=post_data,\n                verify=self._session.verify,\n                timeout=self._config[\"timeout\"],\n            )\n        except requests.exceptions.ConnectionError:\n            if DEBUG:\n                raise\n            raise SkipInstance(\"Cannot connect to server at %s\" % self.base_url)\n        except Exception as exc:\n            if DEBUG:\n                raise\n            sys.stderr.write(\"ERROR: %s\\n\" % exc)\n            raise SkipMBean(exc)\n\n        return validate_response(raw_response)\n\n\ndef validate_response(raw):\n    \"\"\"return loaded response or raise exception\"\"\"\n    if VERBOSE > 1:\n        sys.stderr.write(\n            \"DEBUG: %r:\\n\"\n            \"DEBUG:   headers: %r\\n\"\n            \"DEBUG:   content: %r\\n\\n\" % (raw, raw.headers, raw.content)\n        )\n\n    # check the status of the http server\n    if not 200 <= raw.status_code < 300:\n        sys.stderr.write(\"ERROR: HTTP STATUS: %d\\n\" % raw.status_code)\n        # Unauthorized, Forbidden, Bad Gateway\n        if raw.status_code in (401, 403, 502):\n            raise SkipInstance(\"HTTP STATUS\", raw.status_code)\n        raise SkipMBean(\"HTTP STATUS\", raw.status_code)\n\n    response = raw.json()\n    # check the status of the jolokia response\n    if response.get(\"status\") != 200:\n        errmsg = response.get(\"error\", \"unkown error\")\n        sys.stderr.write(\"ERROR: JAVA: %s\\n\" % errmsg)\n        raise SkipMBean(\"JAVA\", errmsg)\n\n    if \"value\" not in response:\n        sys.stderr.write(\"ERROR: missing 'value': %r\\n\" % response)\n        raise SkipMBean(\"ERROR\", \"missing 'value'\")\n\n    if VERBOSE:\n        sys.stderr.write(\"\\nDEBUG: RESPONSE: %r\\n\" % response)\n\n    return response\n\n\ndef fetch_var(inst, function, path, use_target=False):\n    data = inst.get_post_data(path, function, use_target=use_target)\n    obj = inst.post(data)\n    return obj[\"value\"]\n\n\n# convert single values into lists of items in\n# case value is a 1-levelled or 2-levelled dict\ndef make_item_list(path, value, itemspec):\n    if not isinstance(value, dict):\n        if isinstance(value, str):\n            value = value.replace(r\"\\/\", \"/\")\n        return [(path, value)]\n\n    result = []\n    for key, subvalue in value.items():\n        # Handle filtering via itemspec\n        miss = False\n        while itemspec and \"=\" in itemspec[0]:\n            if itemspec[0] not in key:\n                miss = True\n                break\n            itemspec = itemspec[1:]\n        if miss:\n            continue\n        item = extract_item(key, itemspec)\n        if not item:\n            item = (key,)\n        result += make_item_list(path + item, subvalue, [])\n    return result\n\n\n# Example:\n# key = 'Catalina:host=localhost,path=\\\\/,type=Manager'\n# itemsepc = [ \"path\" ]\n# --> \"/\"\ndef extract_item(key, itemspec):\n    if not itemspec:\n        return ()\n\n    path = key.split(\":\", 1)[-1]\n    components = path.split(\",\")\n    comp_dict = dict(c.split(\"=\") for c in components if c.count(\"=\") == 1)\n\n    item = ()  # type: tuple[Any, ...]\n    for pathkey in itemspec:\n        if pathkey in comp_dict:\n            right = comp_dict[pathkey]\n            if \"/\" in right:\n                right = \"/\" + right.split(\"/\")[-1]\n            item = item + (right,)\n    return item\n\n\ndef fetch_metric(inst, path, title, itemspec, inst_add=None):\n    values = fetch_var(inst, \"read\", path, use_target=True)\n    item_list = make_item_list((), values, itemspec)\n\n    for subinstance, value in item_list:\n        if not subinstance and not title:\n            sys.stderr.write(\"INTERNAL ERROR: %s\\n\" % value)\n            continue\n\n        if \"threadStatus\" in subinstance or \"threadParam\" in subinstance:\n            continue\n\n        if len(subinstance) > 1:\n            instance_out = \",\".join((inst.name,) + subinstance[:-1])\n        elif inst_add is not None:\n            instance_out = \",\".join((inst.name, inst_add))\n        else:\n            instance_out = inst.name\n        instance_out = instance_out.replace(\" \", \"_\")\n\n        if title:\n            if subinstance:\n                title_out = title + \".\" + subinstance[-1]\n            else:\n                title_out = title\n        else:\n            title_out = subinstance[-1]\n\n        yield instance_out, title_out, value\n\n\n@cached\ndef _get_queries(do_search, inst, itemspec, title, path, mbean):\n    if not do_search:\n        return [(mbean + \"/\" + path, title, itemspec)]\n\n    try:\n        value = fetch_var(inst, \"search\", mbean)\n    except SkipMBean:\n        if DEBUG:\n            raise\n        return []\n\n    try:\n        paths = make_item_list((), value, \"\")[0][1]\n    except IndexError:\n        return []\n\n    return [\n        (\"%s/%s\" % (urllib.parse.quote(mbean_exp), path), path, itemspec) for mbean_exp in paths\n    ]\n\n\ndef _process_queries(inst, queries):\n    for mbean_path, title, itemspec in queries:\n        try:\n            for instance_out, title_out, value in fetch_metric(inst, mbean_path, title, itemspec):\n                yield instance_out, title_out, value\n        except (IOError, socket.timeout):\n            raise SkipInstance()\n        except SkipMBean:\n            continue\n        except Exception:\n            if DEBUG:\n                raise\n            continue\n\n\ndef query_instance(inst):\n    write_section(\"jolokia_info\", generate_jolokia_info(inst))\n\n    # now (after jolokia_info) we're sure about the product\n    specs_specific = QUERY_SPECS_SPECIFIC_LEGACY.get(inst.product, [])\n    write_section(\"jolokia_metrics\", generate_values(inst, specs_specific))\n    write_section(\"jolokia_metrics\", generate_values(inst, QUERY_SPECS_LEGACY))\n\n    sections_specific = MBEAN_SECTIONS_SPECIFIC.get(inst.product, {})\n    for section_name, mbeans in sections_specific.items():\n        write_section(\"jolokia_%s\" % section_name, generate_json(inst, mbeans))\n    for section_name, mbeans_tups in MBEAN_SECTIONS.items():\n        write_section(\"jolokia_%s\" % section_name, generate_json(inst, mbeans_tups))\n\n    write_section(\"jolokia_generic\", generate_values(inst, inst.custom_vars))\n\n\ndef _parse_fetched_data(data):\n    # type: (dict[str, Any]) -> tuple[str, str, str]\n    if \"details\" in data:\n        info = data[\"details\"]\n        # https://github.com/jolokia/jolokia/blob/2.0/src/documentation/manual/modules/ROOT/pages/jolokia_mbeans.adoc\n        product = info.get(\"server_product\", \"unknown\")\n        version = info.get(\"server_version\", \"unknown\")\n    else:  # jolokia version 1.7.2 or lower\n        # https://github.com/jolokia/jolokia/blob/v1.7.2/src/docbkx/protocol/version.xml\n        info = data.get(\"info\", {})\n        product = info.get(\"product\", \"unknown\")\n        version = info.get(\"version\", \"unknown\")\n    agentversion = data.get(\"agent\", \"unknown\")\n    return product, version, agentversion\n\n\ndef generate_jolokia_info(inst):\n    # Determine type of server\n    try:\n        data = fetch_var(inst, \"version\", \"\")\n    except (SkipInstance, SkipMBean) as exc:\n        yield inst.name, \"ERROR\", str(exc)\n        raise SkipInstance(exc)\n\n    product, version, agentversion = _parse_fetched_data(data)\n\n    if inst.product is not None:\n        product = inst.product\n    else:\n        inst.product = product\n\n    yield inst.name, product, version, agentversion\n\n\ndef generate_values(inst, var_list):\n    for var in var_list:\n        mbean, path, title, itemspec, do_search = var[:5]\n        value_type = var[5] if len(var) >= 6 else None\n\n        queries = _get_queries(do_search, inst, itemspec, title, path, mbean)\n\n        for item, title, value in _process_queries(inst, queries):\n            if value_type:\n                yield item, title, value, value_type\n            else:\n                yield item, title, value\n\n\ndef generate_json(inst, mbeans):\n    for mbean in mbeans:\n        try:\n            data = inst.get_post_data(mbean, \"read\", use_target=True)\n            obj = inst.post(data)\n            yield inst.name, mbean, json.dumps(obj[\"value\"])\n        except (IOError, socket.timeout):\n            raise SkipInstance()\n        except SkipMBean:\n            pass\n        except Exception:\n            if DEBUG:\n                raise\n\n\ndef yield_configured_instances(custom_config=None):\n    custom_config = load_config(custom_config)\n\n    # Generate list of instances to monitor. If the user has defined\n    # instances in his configuration, we will use this (a list of dicts).\n    individual_configs = custom_config.pop(\"instances\", [{}])\n    for cfg in individual_configs:\n        keys = set(cfg.keys()) | set(custom_config.keys())\n        conf_dict = dict((k, cfg.get(k, custom_config.get(k))) for k in keys)\n        if VERBOSE:\n            sys.stderr.write(\"DEBUG: configuration: %r\\n\" % conf_dict)\n        yield conf_dict\n\n\ndef load_config(custom_config):\n    if custom_config is None:\n        custom_config = get_default_config_dict()\n\n    conffile = os.path.join(os.getenv(\"MK_CONFDIR\", \"/etc/check_mk\"), \"jolokia.cfg\")\n    if os.path.exists(conffile):\n        with open(conffile) as conf:\n            exec(conf.read(), {}, custom_config)  # nosec B102 # BNS:a29406\n    return custom_config\n\n\ndef main(configs_iterable=None):\n    if configs_iterable is None:\n        configs_iterable = yield_configured_instances()\n\n    for config in configs_iterable:\n        instance = JolokiaInstance(config, USER_AGENT)\n        try:\n            query_instance(instance)\n        except SkipInstance:\n            pass\n\n\nif __name__ == \"__main__\":\n    main()\n"}
{"type": "source_file", "path": "agents/plugins/mk_logwatch.py", "content": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n# Copyright (C) 2019 Checkmk GmbH - License: GNU General Public License v2\n# This file is part of Checkmk (https://checkmk.com). It is subject to the terms and\n# conditions defined in the file COPYING, which is part of this source code package.\n\"\"\"mk_logwatch\nThis is the Check_MK Agent plugin. If configured it will be called by the\nagent without arguments.\n\nOptions:\n    -d               Debug mode: No saving of status.\n    -c CONFIG_FILE   Use this config file\n    -h               Show help.\n    --no_state       No state\n    -v               Verbose output for debugging purposes (no debug mode).\n\nYou should find an example configuration file at\n'../cfg_examples/logwatch.cfg' relative to this file.\n\"\"\"\n\nfrom __future__ import with_statement\n\n__version__ = \"2.5.0b1\"\n\nimport sys\n\nif sys.version_info < (2, 6):\n    sys.stderr.write(\"ERROR: Python 2.5 is not supported. Please use Python 2.6 or newer.\\n\")\n    sys.exit(1)\n\nimport ast\nimport binascii\nimport codecs\nimport glob\nimport io\nimport itertools\nimport locale\nimport logging\nimport os\nimport platform\nimport re\nimport shlex\nimport shutil\nimport socket\nimport time\n\ntry:\n    from collections.abc import (  # noqa: F401\n        Collection,\n        Iterable,\n        Iterator,\n        Sequence,\n    )\n    from typing import Any  # noqa: F401\nexcept ImportError:\n    # We need typing only for testing\n    pass\n\n\nDEFAULT_LOG_LEVEL = \".\"\n\nDUPLICATE_LINE_MESSAGE_FMT = \"[the above message was repeated %d times]\"\n\nMK_VARDIR = os.getenv(\"LOGWATCH_DIR\") or os.getenv(\"MK_VARDIR\") or os.getenv(\"MK_STATEDIR\") or \".\"\n\nMK_CONFDIR = os.getenv(\"LOGWATCH_DIR\") or os.getenv(\"MK_CONFDIR\") or \".\"\n\nREMOTE = (\n    os.getenv(\"REMOTE\")\n    or os.getenv(\"REMOTE_ADDR\")\n    or (\"local\" if sys.stdout.isatty() else \"remote-unknown\")\n)\n\nLOGGER = logging.getLogger(__name__)\n\nIPV4_REGEX = re.compile(r\"^(::ffff:|::ffff:0:|)(?:[0-9]{1,3}\\.){3}[0-9]{1,3}$\")\n\nIPV6_REGEX = re.compile(r\"^(?:[A-F0-9]{1,4}:){7}[A-F0-9]{1,4}$\")\n\nENCODINGS = (\n    (b\"\\xff\\xfe\", \"utf_16\"),\n    (b\"\\xfe\\xff\", \"utf_16_be\"),\n)\n\nCONFIG_ERROR_PREFIX = \"CANNOT READ CONFIG FILE: \"  # detected by check plug-in\n\nPY2 = sys.version_info[0] == 2\nPY3 = sys.version_info[0] == 3\nPY_GE_35 = PY3 and sys.version_info[1] >= 5\n\nif PY3:\n    text_type = str\n    binary_type = bytes\nelse:\n    text_type = unicode  # noqa: F821\n    binary_type = str\n\n\nif PY3:\n    # For Python 3 sys.stdout creates \\r\\n as newline for Windows.\n    # Checkmk can't handle this therefore we rewrite sys.stdout to a new_stdout function.\n    # If you want to use the old behaviour just use old_stdout.\n    new_stdout = io.TextIOWrapper(\n        sys.stdout.buffer,\n        newline=\"\\n\",\n        # Write out in utf-8, independently of any encodings preferred on the system. For Python 2,\n        # this is the case because we write str (aka encoded) to sys.stdout and we encode in UTF-8.\n        encoding=\"utf-8\",\n        errors=sys.stdout.errors,\n    )\n    old_stdout, sys.stdout = sys.stdout, new_stdout\n\n\n# Borrowed from six\ndef ensure_str(s, encoding=\"utf-8\", errors=\"strict\"):\n    # type: (text_type | binary_type, str, str) -> str\n    \"\"\"Coerce *s* to `str`.\n\n    For Python 2:\n      - `unicode` -> encoded to `str`\n      - `str` -> `str`\n\n    For Python 3:\n      - `str` -> `str`\n      - `bytes` -> decoded to `str`\n    \"\"\"\n    if not isinstance(s, (text_type, binary_type)):\n        raise TypeError(\"not expecting type '%s'\" % type(s))\n    if PY2 and isinstance(s, text_type):\n        s = s.encode(encoding, errors)\n    elif PY3 and isinstance(s, binary_type):\n        s = s.decode(encoding, errors)\n    return str(s)\n\n\ndef ensure_text_type(s, encoding=\"utf-8\", errors=\"strict\"):\n    # type: (text_type | binary_type, str, str) -> text_type\n    \"\"\"Coerce *s* to `text_type`.\n\n    For Python 2:\n      - `unicode` -> `unicode`\n      - `str` -> decoded to `unicode`\n\n    For Python 3:\n      - `str` -> `str`\n      - `bytes` -> decoded to `str`\n    \"\"\"\n    return s if isinstance(s, text_type) else s.decode(encoding, errors)\n\n\ndef int_to_escaped_char(char):\n    # type: (int) -> text_type\n    return ensure_text_type(\"\\\\x{:02x}\".format(char))\n\n\ndef bytestring_to_escaped_char(char):\n    # type: (binary_type) -> text_type\n    return ensure_text_type(\"\\\\x{:02x}\".format(ord(char)))\n\n\nif PY3:\n    escaped = int_to_escaped_char\nelse:\n    escaped = bytestring_to_escaped_char\n\nif PY_GE_35:\n    backslashreplace_decode = codecs.backslashreplace_errors\nelse:\n    # Python 2 and Python < 3.4 don't support decoding with \"backslashreplace\" error handler,\n    # but we need it to uniquely represent UNIX paths in monitoring.\n    def backslashreplace_decode(exception):\n        # type: (UnicodeError) -> tuple[text_type, int]\n\n        if not isinstance(exception, UnicodeDecodeError):\n            # We'll use this error handler only for decoding, as the original\n            # \"backslashreplace\" handler is capable of encoding in all Python versions.\n            raise exception\n\n        bytestring, start, end = exception.object, exception.start, exception.end\n\n        return (\n            ensure_text_type(\"\").join(escaped(c) for c in bytestring[start:end]),\n            end,\n        )\n\n\ncodecs.register_error(\"backslashreplace_decode\", backslashreplace_decode)\n\n\ndef init_logging(verbosity):\n    if verbosity == 0:\n        LOGGER.propagate = False\n        logging.basicConfig(level=logging.ERROR, format=\"%(levelname)s: %(message)s\")\n    elif verbosity == 1:\n        logging.basicConfig(level=logging.INFO, format=\"%(levelname)s: %(message)s\")\n    else:\n        logging.basicConfig(level=logging.DEBUG, format=\"%(levelname)s: %(lineno)s: %(message)s\")\n\n\nclass ArgsParser:\n    \"\"\"\n    Custom argument parsing.\n    (Neither use optparse which is Python 2.3 to 2.7 only.\n    Nor use argparse which is Python 2.7 onwards only.)\n    \"\"\"\n\n    def __init__(self, argv):\n        # type: (Sequence[str]) -> None\n        super().__init__()\n\n        if \"-h\" in argv:\n            sys.stderr.write(ensure_str(__doc__))\n            sys.exit(0)\n\n        self.verbosity = argv.count(\"-v\") + 2 * argv.count(\"-vv\")\n        self.config = argv[argv.index(\"-c\") + 1] if \"-c\" in argv else None\n        self.debug = \"-d\" in argv or \"--debug\" in argv\n        self.no_state = \"--no_state\" in argv\n\n\ndef get_status_filename(cluster_config, remote):\n    # type: (Sequence[ClusterConfigBlock], str) -> str\n    \"\"\"\n    Side effect:\n    - In case agent plugin is called with debug option set -> depends on global\n      LOGGER and stdout.\n\n    Determine the name of the state file dependent on ENV variable and config:\n    $REMOTE set, no cluster set or no ip match -> logwatch.state.<formatted-REMOTE>\n    $REMOTE set, cluster set and ip match      -> logwatch.state.<cluster-name>\n    $REMOTE not set and a tty                  -> logwatch.state.local\n    $REMOTE not set and not a tty              -> logwatch.state\n\n    $REMOTE is determined by the check_mk_agent and varies dependent on how the\n    check_mk_agent is accessed:\n    - telnet ($REMOTE_HOST): $REMOTE is in IPv6 notation. IPv4 is extended to IPv6\n                             notation e.g. ::ffff:127.0.0.1\n    - ssh ($SSH_CLIENT): $REMOTE is either in IPv4 or IPv6 notation dependent on the\n                         IP family of the remote host.\n\n    <formatted-REMOTE> is REMOTE with colons (:) replaced with underscores (_) for\n    IPv6 address, is to IPv6 notation extended address with colons (:) replaced with\n    underscores (_) for IPv4 address or is plain $REMOTE in case it does not match\n    an IPv4 or IPv6 address.\n    \"\"\"\n    remote_hostname = remote.replace(\":\", \"_\")\n    match = IPV4_REGEX.match(remote) or IPV6_REGEX.match(remote)\n    if not match:\n        LOGGER.debug(\"REMOTE %r neither IPv4 nor IPv6 address.\", remote)\n        return os.path.join(MK_VARDIR, \"logwatch.state.%s\" % remote_hostname)\n\n    remote_ip = match.group()\n    # in case of IPv4 extended to IPv6 get rid of prefix for ip match lookup\n    if remote_ip.startswith(\"::ffff:\"):\n        remote_ip = remote_ip[7:]\n\n    # In case cluster configured map ip to cluster name if configured.\n    # key \"name\" is mandatory and unique for cluster dicts\n    cluster_name = remote_hostname\n    for conf in cluster_config:\n        for ip_or_subnet in conf.ips_or_subnets:\n            if ip_in_subnetwork(remote_ip, ip_or_subnet):\n                # Cluster name may not contain whitespaces (must be provided from\n                # the WATO config as type ID or hostname).\n                cluster_name = conf.name\n                LOGGER.info(\"Matching cluster ip %s\", remote_ip)\n                LOGGER.info(\"Matching cluster name %s\", cluster_name)\n    status_filename = os.path.join(MK_VARDIR, \"logwatch.state.%s\" % cluster_name)\n    LOGGER.info(\"Status filename: %s\", status_filename)\n    return status_filename\n\n\ndef is_comment(line):\n    # type: (text_type) -> bool\n    return line.lstrip().startswith(\"#\")\n\n\ndef is_empty(line):\n    # type: (text_type) -> bool\n    return line.strip() == \"\"\n\n\ndef is_indented(line):\n    # type: (text_type) -> bool\n    return line.startswith(\" \")\n\n\ndef parse_filenames(line):\n    # type: (text_type) -> list[text_type]\n    if platform.system() == \"Windows\":\n        # we can't use pathlib: Python 2.5 has no pathlib\n        # to garantie that backslash is escaped\n        _processed_line = line.replace(\"\\\\\", \"/\")\n        _processed_line = os.path.normpath(_processed_line)\n        _processed_line = _processed_line.replace(\"\\\\\", \"\\\\\\\\\")\n        return shlex.split(_processed_line)\n\n    if sys.version_info[0] < 3:\n        return [x.decode(\"utf-8\") for x in shlex.split(line.encode(\"utf-8\"))]\n\n    return shlex.split(line)\n\n\ndef get_config_files(directory, config_file_arg=None):\n    # type: (str, str | None) -> list[str]\n    if config_file_arg is not None:\n        return [config_file_arg]\n\n    config_file_paths = []\n    config_file_paths.append(os.path.join(directory, \"logwatch.cfg\"))\n    # Add config file paths from a logwatch.d folder\n    for config_file in glob.glob(os.path.join(directory, \"logwatch.d\", \"*.cfg\")):\n        config_file_paths.append(config_file)\n    LOGGER.info(\"Configuration file paths: %r\", config_file_paths)\n    return config_file_paths\n\n\ndef iter_config_lines(files):\n    # type: (Iterable[str]) -> Iterator[text_type]\n    LOGGER.debug(\"Config files: %r\", files)\n\n    for file_ in files:\n        try:\n            with open(file_, \"rb\") as fid:\n                try:\n                    for line in fid:\n                        yield line.decode(\"utf-8\")\n                except UnicodeDecodeError:\n                    msg = \"Error reading file %r (please use utf-8 encoding!)\\n\" % file_\n                    sys.stdout.write(CONFIG_ERROR_PREFIX + msg)\n        except IOError:\n            pass\n\n\ndef consume_global_options_block(config_lines):\n    # type (list[text_type]) -> GlobalOptions\n    config_lines.pop(0)\n    options = GlobalOptions()\n\n    while config_lines and is_indented(config_lines[0]):\n        attr, value = config_lines.pop(0).split(None, 1)\n        if attr == \"retention_period\":\n            options.retention_period = int(value)\n\n    return options\n\n\ndef consume_cluster_definition(config_lines):\n    # type: (list[text_type]) -> ClusterConfigBlock\n    cluster_name = config_lines.pop(0)[8:].strip()  # e.g.: CLUSTER duck\n    ips_or_subnets = []\n    LOGGER.debug(\"new ClusterConfigBlock: %s\", cluster_name)\n\n    while config_lines and is_indented(config_lines[0]):\n        ips_or_subnets.append(config_lines.pop(0).strip())\n\n    return ClusterConfigBlock(cluster_name, ips_or_subnets)\n\n\ndef consume_logfile_definition(config_lines):\n    # type: (list[text_type]) -> PatternConfigBlock\n    cont_list = []\n    rewrite_list = []\n    filenames = parse_filenames(config_lines.pop(0))\n    patterns = []\n    LOGGER.debug(\"new PatternConfigBlock: %s\", filenames)\n\n    while config_lines and is_indented(config_lines[0]):\n        line = config_lines.pop(0)\n        level, raw_pattern = line.split(None, 1)\n\n        if level == \"A\":\n            cont_list.append(raw_pattern)\n\n        elif level == \"R\":\n            rewrite_list.append(raw_pattern)\n\n        elif level in (\"C\", \"W\", \"I\", \"O\"):\n            # New pattern for line matching => clear continuation and rewrite patterns\n            cont_list = []\n            rewrite_list = []\n            pattern = (level, raw_pattern, cont_list, rewrite_list)\n            patterns.append(pattern)\n            LOGGER.debug(\"pattern %s\", pattern)\n\n        else:\n            raise ValueError(\"Invalid level in pattern line %r\" % line)\n\n    return PatternConfigBlock(filenames, patterns)\n\n\ndef read_config(config_lines, files, debug=False):\n    # type: (Iterable[text_type], Iterable[str], bool) -> tuple[GlobalOptions, list[PatternConfigBlock], list[ClusterConfigBlock]]\n    \"\"\"\n    Read logwatch.cfg (patterns, cluster mapping, etc.).\n\n    Returns configuration as list. List elements are namedtuples.\n    Namedtuple either describes logile patterns and is PatternConfigBlock(files, patterns).\n    Or tuple describes optional cluster mapping and is ClusterConfigBlock(name, ips_or_subnets)\n    with ips as list of strings.\n    \"\"\"\n    config_lines = [l.rstrip() for l in config_lines if not is_comment(l) and not is_empty(l)]\n    if debug and not config_lines:\n        # We need at least one config file *with* content in one of the places:\n        # logwatch.d or MK_CONFDIR\n        raise IOError(\"Did not find any content in config files: %s\" % \", \".join(files))\n\n    logfiles_configs = []\n    cluster_configs = []\n    global_options = GlobalOptions()\n\n    # parsing has to consider the following possible lines:\n    # - comment lines (begin with #)\n    # - global options (block begins with \"GLOBAL OPTIONS\")\n    # - logfiles line (begin not with #, are not empty and do not contain CLUSTER)\n    # - cluster lines (begin with CLUSTER)\n    # - logfiles patterns (follow logfiles lines, begin with whitespace)\n    # - cluster ips or subnets (follow cluster lines, begin with whitespace)\n    # Needs to consider end of lines to append ips/subnets to clusters as well.\n\n    while config_lines:\n        first_line = config_lines[0]\n        if is_indented(first_line):\n            raise ValueError(\"Missing block definition for line %r\" % first_line)\n\n        if first_line.startswith(\"GLOBAL OPTIONS\"):\n            global_options = consume_global_options_block(config_lines)\n\n        if first_line.startswith(\"CLUSTER \"):\n            cluster_configs.append(consume_cluster_definition(config_lines))\n        else:\n            logfiles_configs.append(consume_logfile_definition(config_lines))\n\n    LOGGER.info(\"Logfiles configurations: %r\", logfiles_configs)\n    LOGGER.info(\"Optional cluster configurations: %r\", cluster_configs)\n    return global_options, logfiles_configs, cluster_configs\n\n\nclass State:\n    def __init__(self, filename):\n        # type: (str) -> None\n        super().__init__()\n        self.filename = filename\n        self._data = {}  # type: dict[text_type | binary_type, dict[str, Any]]\n\n    @staticmethod\n    def _load_line(line):\n        # type: (str) -> dict[str, Any]\n        try:\n            return ast.literal_eval(line)\n        except (NameError, SyntaxError, ValueError):\n            # Support status files with the following structure:\n            # /var/log/messages|7767698|32455445\n            # These were used prior to to 1.7.0i1\n            parts = line.split(\"|\")\n            filename, offset = parts[0], int(parts[1])\n            file_id = int(parts[2]) if len(parts) >= 3 else -1\n            return {\"file\": filename, \"offset\": offset, \"inode\": file_id}\n\n    def read(self):\n        # type: () -> State\n        \"\"\"Read state from file\n        Support state files with the following structure:\n        {'file': b'/var/log/messages', 'offset': 7767698, 'inode': 32455445}\n        \"\"\"\n        LOGGER.debug(\"Reading state file: %r\", self.filename)\n\n        if not os.path.exists(self.filename):\n            return self\n\n        with open(self.filename, \"rb\") as stat_fh:\n            for line in stat_fh:\n                line_data = self._load_line(ensure_text_type(line))\n                self._data[line_data[\"file\"]] = line_data\n\n        LOGGER.info(\"Read state: %r\", self._data)\n        return self\n\n    def write(self):\n        # type: () -> None\n        LOGGER.debug(\"Writing state: %r\", self._data)\n        LOGGER.debug(\"State filename: %r\", self.filename)\n\n        with open(self.filename, \"wb\") as stat_fh:\n            for data in self._data.values():\n                stat_fh.write(repr(data).encode(\"utf-8\") + b\"\\n\")\n\n    def get(self, key):\n        # type: (text_type | binary_type) -> dict[str, Any]\n        return self._data.setdefault(key, {\"file\": key})\n\n\nclass LogLinesIter:\n    # this is supposed to become a proper iterator.\n    # for now, we need a persistent buffer to fix things\n    BLOCKSIZE = 8192\n\n    def __init__(self, logfile, encoding):\n        super().__init__()\n        self._fd = os.open(logfile, os.O_RDONLY)\n        self._lines = []  # List[Text]\n        self._buffer = b\"\"\n        self._reached_end = False  # used for optimization only\n        self._enc = encoding or self._get_encoding()\n        self._nl = \"\\n\"\n        # for Windows we need a bit special processing. It is difficult to fit this processing\n        # in current architecture smoothly\n        self._utf16 = self._enc == \"utf_16\"\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, *exc_info):\n        self.close()\n        return False  # Do not swallow exceptions\n\n    def close(self):\n        os.close(self._fd)\n\n    def _get_encoding(self):\n        # In 1.5 this was only used when logwatch is executed on windows.\n        # On linux the log lines were not decoded at all.\n        #\n        # For 1.6 we want to follow the standard approach to decode things read\n        # from external sources as soon as possible. We also want to ensure that\n        # the output of this script is always UTF-8 encoded later.\n        #\n        # In case the current approach does not work out, then have a look here\n        # for possible more robust solutions:\n        # http://python-notes.curiousefficiency.org/en/latest/python3/text_file_processing.html\n        enc_bytes_len = max(len(bom) for bom, _enc in ENCODINGS)\n        self._buffer = os.read(self._fd, enc_bytes_len)\n        for bom, encoding in ENCODINGS:\n            if self._buffer.startswith(bom):\n                self._buffer = self._buffer[len(bom) :]\n                LOGGER.debug(\"Detected %r encoding by BOM\", encoding)\n                return encoding\n\n        pref_encoding = locale.getpreferredencoding()\n        encoding = (\n            \"utf_8\" if not pref_encoding or pref_encoding == \"ANSI_X3.4-1968\" else pref_encoding\n        )\n        LOGGER.debug(\"Locale Preferred encoding is %s, using %s\", pref_encoding, encoding)\n        return encoding\n\n    def _update_lines(self):\n        \"\"\"\n        Try to read more lines from file.\n        \"\"\"\n        binary_nl = self._nl.encode(self._enc)\n        while binary_nl not in self._buffer:\n            new_bytes = os.read(self._fd, LogLinesIter.BLOCKSIZE)\n            if not new_bytes:\n                break\n            self._buffer += new_bytes\n\n        # in case of decoding error, replace with U+FFFD REPLACEMENT CHARACTER\n        raw_lines = self._buffer.decode(self._enc, \"replace\").split(self._nl)\n        self._buffer = raw_lines.pop().encode(self._enc)  # unfinished line\n        self._lines.extend(l + self._nl for l in raw_lines)\n\n    def set_position(self, position):\n        if position is None:\n            return\n        self._buffer = b\"\"\n        self._lines = []\n        os.lseek(self._fd, position, os.SEEK_SET)\n\n    def get_position(self):\n        \"\"\"\n        Return the position where we want to continue next time\n        \"\"\"\n        pointer_pos = os.lseek(self._fd, 0, os.SEEK_CUR)\n        bytes_unused = sum((len(l.encode(self._enc)) for l in self._lines), len(self._buffer))\n        return pointer_pos - bytes_unused\n\n    def skip_remaining(self):\n        os.lseek(self._fd, 0, os.SEEK_END)\n        self._buffer = b\"\"\n        self._lines = []\n\n    def push_back_line(self, line):\n        self._lines.insert(0, line)\n\n    def next_line(self):\n        # type: () -> text_type | None\n        if self._reached_end:  # optimization only\n            return None\n\n        if not self._lines:\n            self._update_lines()\n\n        if self._lines:\n            return self._lines.pop(0)\n\n        self._reached_end = True\n        return None\n\n\ndef get_file_info(path):\n    stat = os.stat(path)\n    system = platform.system().lower()\n    if system == \"windows\":\n        return (stat.st_ctime_ns, stat.st_size)\n    if system in (\"linux\", \"aix\", \"sunos\"):\n        return (stat.st_ino, stat.st_size)\n\n    return (1, stat.st_size)\n\n\ndef get_formatted_line(line, level):\n    # type: (text_type, str) -> text_type\n    formatted_line = \"%s %s\" % (level, line)\n    if sys.stdout.isatty():\n        formatted_line = formatted_line.replace(\"\\1\", \"\\nCONT:\")\n    return formatted_line\n\n\ndef should_log_line_with_level(level, nocontext):\n    # type: (str, bool | None) -> bool\n    return not (nocontext and level == \".\")\n\n\ndef process_logfile(section, filestate, debug):\n    # type: (LogfileSection, dict[str, Any], object) -> tuple[text_type, list[text_type]]\n    \"\"\"\n    Returns tuple of (\n        logfile lines,\n        warning and/or error indicator,\n        warning and/or error lines,\n    ).\n    In case the file has never been seen before returns a list of logfile lines\n    and None in case the logfile cannot be opened.\n    \"\"\"\n    # TODO: Make use of the ContextManager feature of LogLinesIter\n    try:\n        log_iter = LogLinesIter(section.name_fs, section.options.encoding)\n    except OSError:\n        if debug:\n            raise\n        return \"[[[%s:cannotopen]]]\\n\" % section.name_write, []\n\n    try:\n        header = \"[[[%s]]]\\n\" % section.name_write\n\n        file_id, size = get_file_info(section.name_fs)\n        prev_file_id = filestate.get(\"inode\", -1)\n        filestate[\"inode\"] = file_id\n\n        # Look at which file offset we have finished scanning the logfile last time.\n        offset = filestate.get(\"offset\")\n        # Set the current pointer to the file end\n        filestate[\"offset\"] = size\n\n        # If we have never seen this file before, we do not want\n        # to make a fuss about ancient log messages... (unless configured to)\n        if offset is None and not (section.options.fromstart or debug):\n            return header, []\n\n        # If the inode of the logfile has changed it has appearently\n        # been started from new (logfile rotation). At least we must\n        # assume that. In some rare cases (restore of a backup, etc)\n        # we are wrong and resend old log messages\n        if prev_file_id >= 0 and file_id != prev_file_id:\n            offset = None\n\n        # Our previously stored offset is the current end ->\n        # no new lines in this file\n        if offset == size:\n            return header, []\n\n        # If our offset is beyond the current end, the logfile has been\n        # truncated or wrapped while keeping the same file_id. We assume\n        # that it contains all new data in that case and restart from\n        # beginning.\n        if offset is not None and offset > size:\n            offset = None\n\n        # now seek to offset where interesting data begins\n        log_iter.set_position(offset)\n\n        worst = -1\n        warnings_and_errors = []\n        lines_parsed = 0\n        start_time = time.time()\n\n        while True:\n            line = log_iter.next_line()\n            if line is None:\n                break  # End of file\n\n            # Handle option maxlinesize\n            if section.options.maxlinesize is not None and len(line) > section.options.maxlinesize:\n                line = line[: section.options.maxlinesize] + \"[TRUNCATED]\\n\"\n\n            lines_parsed += 1\n            # Check if maximum number of new log messages is exceeded\n            if section.options.maxlines is not None and lines_parsed > section.options.maxlines:\n                warnings_and_errors.append(\n                    \"%s Maximum number (%d) of new log messages exceeded.\\n\"\n                    % (\n                        section.options.overflow,\n                        section.options.maxlines,\n                    )\n                )\n                worst = max(worst, section.options.overflow_level)\n                log_iter.skip_remaining()\n                break\n\n            # Check if maximum processing time (per file) is exceeded. Check only\n            # every 100'th line in order to save system calls\n            if (\n                section.options.maxtime is not None\n                and lines_parsed % 100 == 10\n                and time.time() - start_time > section.options.maxtime\n            ):\n                warnings_and_errors.append(\n                    \"%s Maximum parsing time (%.1f sec) of this log file exceeded.\\n\"\n                    % (\n                        section.options.overflow,\n                        section.options.maxtime,\n                    )\n                )\n                worst = max(worst, section.options.overflow_level)\n                log_iter.skip_remaining()\n                break\n\n            level = DEFAULT_LOG_LEVEL\n            for lev, pattern, cont_patterns, replacements in section.compiled_patterns:\n                matches = pattern.search(line[:-1])\n                if matches:\n                    level = lev\n                    levelint = {\"C\": 2, \"W\": 1, \"O\": 0, \"I\": -1, \".\": -1}[lev]\n                    worst = max(levelint, worst)\n\n                    # TODO: the following for block should be a method of the iterator\n                    # Check for continuation lines\n                    for cont_pattern in cont_patterns:\n                        if isinstance(cont_pattern, int):  # add that many lines\n                            for _unused_x in range(cont_pattern):\n                                cont_line = log_iter.next_line()\n                                if cont_line is None:  # end of file\n                                    break\n                                line = line[:-1] + \"\\1\" + cont_line\n\n                        else:  # pattern is regex\n                            while True:\n                                cont_line = log_iter.next_line()\n                                if cont_line is None:  # end of file\n                                    break\n                                if cont_pattern.search(cont_line[:-1]):\n                                    line = line[:-1] + \"\\1\" + cont_line\n                                else:\n                                    log_iter.push_back_line(\n                                        cont_line\n                                    )  # sorry for stealing this line\n                                    break\n\n                    # Replacement\n                    for replace in replacements:\n                        line = replace.replace(\"\\\\0\", line.rstrip()) + \"\\n\"\n                        for num, group in enumerate(matches.groups()):\n                            if group is not None:\n                                line = line.replace(\"\\\\%d\" % (num + 1), group)\n\n                    break  # matching rule found and executed\n\n            if level == \"I\":\n                level = \".\"\n            if not should_log_line_with_level(level, section.options.nocontext):\n                continue\n\n            out_line = get_formatted_line(line[:-1], level)\n            warnings_and_errors.append(\"%s\\n\" % out_line)\n\n        new_offset = log_iter.get_position()\n    finally:\n        log_iter.close()\n\n    filestate[\"offset\"] = new_offset\n\n    # Handle option maxfilesize, regardless of warning or errors that have happened\n    if section.options.maxfilesize:\n        offset_wrap = new_offset // section.options.maxfilesize\n        if ((offset or 0) // section.options.maxfilesize) < offset_wrap:\n            warnings_and_errors.append(\n                \"W Maximum allowed logfile size (%d bytes) exceeded for the %dth time.\\n\"\n                % (section.options.maxfilesize, offset_wrap)\n            )\n\n    # output all lines if at least one warning, error or ok has been found\n    if worst > -1:\n        return header, warnings_and_errors\n    return header, []\n\n\nclass Options:\n    \"\"\"Options w.r.t. logfile patterns (not w.r.t. cluster mapping).\"\"\"\n\n    MAP_OVERFLOW = {\"C\": 2, \"W\": 1, \"I\": 0, \"O\": 0}  # case-insensitive, see set_opt\n    MAP_BOOL = {\"true\": True, \"false\": False, \"1\": True, \"0\": False, \"yes\": True, \"no\": False}\n    DEFAULTS = {\n        \"encoding\": None,\n        \"maxfilesize\": None,\n        \"maxlines\": None,\n        \"maxtime\": None,\n        \"maxlinesize\": None,\n        \"regex\": None,\n        \"overflow\": \"C\",\n        \"nocontext\": None,\n        \"maxcontextlines\": None,\n        \"maxoutputsize\": 500000,  # same as logwatch_max_filesize in check plug-in\n        \"fromstart\": False,\n        \"skipconsecutiveduplicated\": False,\n    }\n\n    def __init__(self):\n        # type: () -> None\n        self.values = {}  # type: dict\n\n    @property\n    def encoding(self):\n        return self._attr_or_default(\"encoding\")\n\n    @property\n    def maxfilesize(self):\n        return self._attr_or_default(\"maxfilesize\")\n\n    @property\n    def maxlines(self):\n        return self._attr_or_default(\"maxlines\")\n\n    @property\n    def maxtime(self):\n        return self._attr_or_default(\"maxtime\")\n\n    @property\n    def maxlinesize(self):\n        return self._attr_or_default(\"maxlinesize\")\n\n    @property\n    def regex(self):\n        return self._attr_or_default(\"regex\")\n\n    @property\n    def overflow(self):\n        return self._attr_or_default(\"overflow\")\n\n    @property\n    def nocontext(self):\n        # type: () -> bool | None\n        return self._attr_or_default(\"nocontext\")\n\n    @property\n    def maxcontextlines(self):\n        return self._attr_or_default(\"maxcontextlines\")\n\n    @property\n    def maxoutputsize(self):\n        return self._attr_or_default(\"maxoutputsize\")\n\n    @property\n    def fromstart(self):\n        return self._attr_or_default(\"fromstart\")\n\n    @property\n    def skipconsecutiveduplicated(self):\n        return self._attr_or_default(\"skipconsecutiveduplicated\")\n\n    def _attr_or_default(self, key):\n        if key in self.values:\n            return self.values[key]\n        return Options.DEFAULTS[key]\n\n    @property\n    def overflow_level(self):\n        return self.MAP_OVERFLOW[self.overflow]\n\n    def update(self, other):\n        self.values.update(other.values)\n\n    def set_opt(self, opt_str):\n        try:\n            key, value = opt_str.split(\"=\", 1)\n            if key == \"encoding\":\n                \"\".encode(value)  # make sure it's an encoding\n                self.values[key] = value\n            elif key in (\"maxlines\", \"maxlinesize\", \"maxfilesize\", \"maxoutputsize\"):\n                self.values[key] = int(value)\n            elif key in (\"maxtime\",):\n                self.values[key] = float(value)\n            elif key == \"overflow\":\n                if value.upper() not in Options.MAP_OVERFLOW:\n                    raise ValueError(\n                        \"Invalid overflow: %r (choose from %r)\"\n                        % (\n                            value,\n                            Options.MAP_OVERFLOW.keys(),\n                        )\n                    )\n                self.values[\"overflow\"] = value.upper()\n            elif key in (\"regex\", \"iregex\"):\n                flags = (re.IGNORECASE if key.startswith(\"i\") else 0) | re.UNICODE\n                self.values[\"regex\"] = re.compile(value, flags)\n            elif key in (\"nocontext\", \"fromstart\", \"skipconsecutiveduplicated\"):\n                if value.lower() not in Options.MAP_BOOL:\n                    raise ValueError(\n                        \"Invalid %s: %r (choose from %r)\"\n                        % (\n                            key,\n                            value,\n                            Options.MAP_BOOL.keys(),\n                        )\n                    )\n                self.values[key] = Options.MAP_BOOL[value.lower()]\n            elif key == \"maxcontextlines\":\n                before, after = (int(i) for i in value.split(\",\"))\n                self.values[key] = (before, after)\n            else:\n                raise ValueError(\"Invalid option: %r\" % opt_str)\n        except (ValueError, LookupError) as exc:\n            sys.stdout.write(\"INVALID CONFIGURATION: %s\\n\" % exc)\n            raise\n\n\nclass GlobalOptions:\n    def __init__(self):\n        super().__init__()\n        self.retention_period = 60\n\n\nclass PatternConfigBlock:\n    def __init__(self, files, patterns):\n        # type: (Sequence[text_type], Sequence[tuple[text_type, text_type, Sequence[text_type], Sequence[text_type]]]) -> None\n        super().__init__()\n        self.files = files\n        self.patterns = patterns\n        # First read all the options like 'maxlines=100' or 'maxtime=10'\n        self.options = Options()\n        for item in self.files:\n            if \"=\" in item:\n                self.options.set_opt(item)\n\n\nclass ClusterConfigBlock:\n    def __init__(self, name, ips_or_subnets):\n        # type: (text_type, Sequence[text_type]) -> None\n        super().__init__()\n        self.name = name\n        self.ips_or_subnets = ips_or_subnets\n\n\ndef find_matching_logfiles(glob_pattern):\n    # type: (text_type) -> list[tuple[text_type | binary_type, text_type]]\n    \"\"\"\n    Evaluate globbing pattern to a list of logfile IDs\n\n    Return a list of Tuples:\n     * one identifier for opening the file as used by os.open (byte str or unicode)\n     * one unicode str, safe for writing\n\n    Glob matching of hard linked, unbroken soft linked/symlinked files.\n    No tilde expansion is done, but *, ?, and character ranges expressed with []\n    will be correctly matched.\n\n    No support for recursive globs ** (supported beginning with Python3.5 only).\n\n    Hard linked dublicates of files are not filtered.\n    Soft links may not be detected properly dependent on the Python runtime\n    [Python Standard Lib, os.path.islink()].\n    \"\"\"\n    if platform.system() == \"Windows\":\n        # windows is the easy case:\n        # provide unicode, and let python deal with the rest\n        # (see https://www.python.org/dev/peps/pep-0277)\n        matches = list(glob.glob(glob_pattern))  # type: Iterable[text_type | binary_type]\n    else:\n        # we can't use glob on unicode, as it would try to re-decode matches with ascii\n        matches = glob.glob(glob_pattern.encode(\"utf8\"))\n\n    # skip dirs\n    file_refs = []\n    for match in matches:\n        if os.path.isdir(match):\n            continue\n\n        # match is bytes in Linux and unicode/str in Windows\n        match_readable = ensure_text_type(match, errors=\"backslashreplace_decode\")\n\n        file_refs.append((match, match_readable))\n\n    return file_refs\n\n\ndef _search_optimize_raw_pattern(raw_pattern):\n    # type: (text_type) -> text_type\n    \"\"\"return potentially stripped pattern for use with *search*\n\n    Stripping leading and trailing '.*' avoids catastrophic backtracking\n    when long log lines are being processed\n    \"\"\"\n    start_idx = 2 if raw_pattern.startswith(\".*\") else 0\n    end_idx = -2 if raw_pattern.endswith(\".*\") else None\n    return raw_pattern[start_idx:end_idx] or raw_pattern\n\n\ndef _compile_continuation_pattern(raw_pattern):\n    # type: (text_type) -> int | re.Pattern\n    try:\n        return int(raw_pattern)\n    except (ValueError, TypeError):\n        return re.compile(_search_optimize_raw_pattern(raw_pattern), re.UNICODE)\n\n\nclass LogfileSection:\n    def __init__(self, logfile_ref):\n        # type: (tuple[text_type | binary_type, text_type]) -> None\n        super().__init__()\n        self.name_fs = logfile_ref[0]\n        self.name_write = logfile_ref[1]\n        self.options = Options()\n        self.patterns = []  # type: list[tuple[text_type, text_type, Sequence[text_type], Sequence[text_type]]]\n        self._compiled_patterns = None  # type: list[tuple[text_type, re.Pattern, Sequence[re.Pattern | int], Sequence[text_type]]] | None\n\n    @property\n    def compiled_patterns(self):\n        # type: () -> list[tuple[text_type, re.Pattern, Sequence[re.Pattern | int], Sequence[text_type]]]\n        if self._compiled_patterns is not None:\n            return self._compiled_patterns\n\n        compiled_patterns = []  # type: list[tuple[text_type, re.Pattern, Sequence[re.Pattern | int], Sequence[text_type]]]\n        for level, raw_pattern, cont_list, rewrite_list in self.patterns:\n            if not rewrite_list:\n                # it does not matter what the matched group is in this case\n                raw_pattern = _search_optimize_raw_pattern(raw_pattern)\n            compiled = re.compile(raw_pattern, re.UNICODE)\n            cont_list_comp = [_compile_continuation_pattern(cp) for cp in cont_list]\n            compiled_patterns.append((level, compiled, cont_list_comp, rewrite_list))\n\n        self._compiled_patterns = compiled_patterns\n        return self._compiled_patterns\n\n\ndef parse_sections(logfiles_config):\n    # type: (Iterable[PatternConfigBlock]) -> tuple[list[LogfileSection], list[text_type]]\n    \"\"\"\n    Returns a list of LogfileSections and and a list of non-matching patterns.\n    \"\"\"\n    found_sections = {}  # type: dict[text_type | binary_type, LogfileSection]\n    non_matching_patterns = []\n\n    for cfg in logfiles_config:\n        # Then handle the file patterns\n        # The thing here is that the same file could match different patterns.\n        for glob_pattern in (f for f in cfg.files if \"=\" not in f):\n            logfile_refs = find_matching_logfiles(glob_pattern)\n            if cfg.options.regex is not None:\n                logfile_refs = [ref for ref in logfile_refs if cfg.options.regex.search(ref[1])]\n            if not logfile_refs:\n                non_matching_patterns.append(glob_pattern)\n            for logfile_ref in logfile_refs:\n                section = found_sections.setdefault(logfile_ref[0], LogfileSection(logfile_ref))\n                section.patterns.extend(cfg.patterns)\n                section.options.update(cfg.options)\n\n    logfile_sections = [found_sections[k] for k in sorted(found_sections)]\n\n    return logfile_sections, non_matching_patterns\n\n\ndef ip_in_subnetwork(ip_address, subnetwork):\n    \"\"\"\n    Accepts ip address as string e.g. \"10.80.1.1\" and CIDR notation as string e.g.\"10.80.1.0/24\".\n    Returns False in case of incompatible IP versions.\n\n    Implementation depends on Python2 and Python3 standard lib only.\n    \"\"\"\n    (ip_integer, version1) = _ip_to_integer(ip_address)\n    (ip_lower, ip_upper, version2) = _subnetwork_to_ip_range(subnetwork)\n    if version1 != version2:\n        return False\n    return ip_lower <= ip_integer <= ip_upper\n\n\ndef _ip_to_integer(ip_address):\n    \"\"\"\n    Raises ValueError in case of invalid IP address.\n    \"\"\"\n    # try parsing the IP address first as IPv4, then as IPv6\n    for version in (socket.AF_INET, socket.AF_INET6):\n        try:\n            ip_hex = socket.inet_pton(version, ip_address)\n        except socket.error:\n            continue\n        ip_integer = int(binascii.hexlify(ip_hex), 16)\n        return (ip_integer, 4 if version == socket.AF_INET else 6)\n    raise ValueError(\"invalid IP address: %r\" % ip_address)\n\n\ndef _subnetwork_to_ip_range(subnetwork):\n    \"\"\"\n    Convert subnetwork to a range of IP addresses\n\n    Raises ValueError in case of invalid subnetwork.\n    \"\"\"\n    if \"/\" not in subnetwork:\n        ip_integer, version = _ip_to_integer(subnetwork)\n        return ip_integer, ip_integer, version\n    network_prefix, netmask_len = subnetwork.split(\"/\", 1)\n    # try parsing the subnetwork first as IPv4, then as IPv6\n    for version, ip_len in ((socket.AF_INET, 32), (socket.AF_INET6, 128)):\n        try:\n            ip_hex = socket.inet_pton(version, network_prefix)\n        except socket.error:\n            continue\n        try:\n            suffix_mask = (1 << (ip_len - int(netmask_len))) - 1\n        except ValueError:  # netmask_len is too large or invalid\n            raise ValueError(\"invalid subnetwork: %r\" % subnetwork)\n        netmask = ((1 << ip_len) - 1) - suffix_mask\n        ip_lower = int(binascii.hexlify(ip_hex), 16) & netmask\n        ip_upper = ip_lower + suffix_mask\n        return (ip_lower, ip_upper, 4 if version == socket.AF_INET else 6)\n    raise ValueError(\"invalid subnetwork: %r\" % subnetwork)\n\n\ndef _filter_maxoutputsize(lines, maxoutputsize):\n    # type: (Iterable[text_type], int) -> Iterable[text_type]\n    \"\"\"Produce lines right *before* maxoutputsize is exceeded\"\"\"\n    bytecount = 0\n    for line in lines:\n        bytecount += len(line.encode(\"utf-8\"))\n        if bytecount > maxoutputsize:\n            break\n        yield line\n\n\ndef _filter_maxcontextlines(lines_list, before, after):\n    # type: (Sequence[text_type], int, int) -> Iterable[text_type]\n    \"\"\"Only produce lines from a limited context\n\n    Think of grep's -A and -B options\n    \"\"\"\n\n    n_lines = len(lines_list)\n    indices = iter(range(-before, n_lines))\n    context_end = -1\n    for idx in indices:\n        new_in_context_idx = idx + before\n        if new_in_context_idx < n_lines and context_end < n_lines:\n            new_in_context = lines_list[new_in_context_idx]\n            # if the line ahead is relevant, extend the context\n            if new_in_context.startswith((\"C\", \"W\", \"O\")):\n                context_end = new_in_context_idx + after\n        if 0 <= idx <= context_end:\n            yield lines_list[idx]\n\n\ndef _filter_consecutive_duplicates(lines, nocontext):\n    # type: (Iterable[text_type], bool | None) -> Iterable[text_type]\n    \"\"\"\n    Filters out consecutive duplicated lines and adds a context line (if nocontext=False) with the\n    number of removed lines for every chunk of removed lines\n    \"\"\"\n\n    lines = iter(lines)\n\n    counter = 0\n    current_line = next(lines, None)\n    next_line = None\n\n    while True:\n        if current_line is None:\n            return\n\n        next_line = next(lines, None)\n\n        if counter == 0:\n            yield current_line\n\n        if current_line == next_line:\n            counter += 1\n            continue\n\n        if counter > 0 and should_log_line_with_level(DEFAULT_LOG_LEVEL, nocontext):\n            unformatted_msg = DUPLICATE_LINE_MESSAGE_FMT % (counter)\n            duplicate_line_msg = get_formatted_line(unformatted_msg, DEFAULT_LOG_LEVEL)\n            yield \"%s\\n\" % duplicate_line_msg\n\n        counter = 0\n        current_line = next_line\n\n\ndef filter_output(lines, options):\n    # type: (Sequence[text_type], Options) -> Iterable[text_type]\n    lines_filtered = (\n        _filter_maxcontextlines(lines, *options.maxcontextlines)\n        if options.maxcontextlines\n        else lines\n    )\n\n    lines_filtered = _filter_maxoutputsize(lines_filtered, options.maxoutputsize)\n\n    if options.skipconsecutiveduplicated:\n        lines_filtered = _filter_consecutive_duplicates(lines_filtered, options.nocontext)\n\n    return lines_filtered\n\n\ndef _is_outdated_batch(batch_file, retention_period, now):\n    # type: (str, float, float) -> bool\n    return now - os.stat(batch_file).st_mtime > retention_period\n\n\ndef write_batch_file(lines, batch_id, batch_dir):\n    # type: (Iterable[str], str, str) -> None\n    with open(\n        os.path.join(batch_dir, \"logwatch-batch-file-%s\" % batch_id), \"w\", encoding=\"utf-8\"\n    ) as handle:\n        handle.writelines([ensure_text_type(l, errors=\"replace\") for l in lines])\n\n\ndef _ip_to_dir(ip_addr):\n    return ip_addr.replace(\":\", \"_\") if os.name == \"nt\" else ip_addr\n\n\ndef process_batches(current_batch, current_batch_id, remote, retention_period, now):\n    # type: (Collection[str], str, str, float, float) -> None\n    batch_dir = os.path.join(MK_VARDIR, \"logwatch-batches\", _ip_to_dir(remote))\n\n    try:\n        os.makedirs(batch_dir)\n    except OSError as os_err:\n        if os_err.errno != 17:  # 17 means exists\n            raise\n\n    pre_existing_batch_files = os.listdir(batch_dir)\n\n    write_batch_file(current_batch, current_batch_id, batch_dir)\n\n    sys.stdout.write(\"<<<logwatch>>>\\n\")\n    sys.stdout.writelines(current_batch)\n\n    for base_name in pre_existing_batch_files:\n        batch_file = os.path.join(batch_dir, base_name)\n        try:\n            if _is_outdated_batch(batch_file, retention_period, now):\n                os.unlink(batch_file)\n            else:\n                with open(batch_file, encoding=\"utf-8\", errors=\"replace\") as fh:\n                    sys.stdout.writelines([ensure_str(l) for l in fh])\n                continue\n        except EnvironmentError:\n            pass\n\n\ndef main(argv=None):\n    if argv is None:\n        argv = sys.argv\n\n    args = ArgsParser(argv)\n    init_logging(args.verbosity)\n    now = int(time.time())\n    batch_id = \"%s-%s\" % (now, \"\".join(\"%03d\" % int(b) for b in bytearray(os.urandom(16))))\n\n    try:\n        files = get_config_files(MK_CONFDIR, config_file_arg=args.config)\n        global_options, logfiles_config, cluster_config = read_config(\n            iter_config_lines(files), files, args.debug\n        )\n    except Exception as exc:\n        if args.debug:\n            raise\n        sys.stdout.write(\"<<<logwatch>>>\\n%s%s\\n\" % (CONFIG_ERROR_PREFIX, exc))\n        sys.exit(1)\n\n    status_filename = get_status_filename(cluster_config, REMOTE)\n    # Copy the last known state from the logwatch.state when there is no status_filename yet.\n    if not os.path.exists(status_filename) and os.path.exists(\"%s/logwatch.state\" % MK_VARDIR):\n        shutil.copy(\"%s/logwatch.state\" % MK_VARDIR, status_filename)\n\n    found_sections, non_matching_patterns = parse_sections(logfiles_config)\n\n    output = (\n        str(\n            \"[[[%s:missing]]]\\n\" % pattern\n            if sys.version_info[0] == 3\n            # Python 2.5/2.6 compatible solution\n            else (\"[[[%s:missing]]]\\n\" % pattern).encode(\"utf-8\")\n        )\n        for pattern in non_matching_patterns\n    )  # type: Iterable[str | text_type]\n\n    state = State(status_filename)\n    try:\n        state.read()\n    except Exception as exc:\n        if args.debug:\n            raise\n        # Simply ignore errors in the status file.  In case of a corrupted status file we simply\n        # begin with an empty status. That keeps the monitoring up and running - even if we might\n        # lose a message in the extreme case of a corrupted status file.\n        LOGGER.warning(\"Exception reading status file: %s\", str(exc))\n\n    for section in found_sections:\n        filestate = state.get(section.name_fs)\n        try:\n            header, log_lines = process_logfile(section, filestate, args.debug)\n            filtered_log_lines = filter_output(log_lines, section.options)\n        except Exception as exc:\n            if args.debug:\n                raise\n            LOGGER.debug(\"Exception when processing %r: %s\", section.name_fs, exc)\n\n        output = itertools.chain(\n            output,\n            [\n                header,\n                \"BATCH: %s\\n\" % batch_id,\n            ],\n            filtered_log_lines,\n        )\n\n    process_batches(\n        [ensure_str(l) for l in output],\n        batch_id,\n        REMOTE,\n        global_options.retention_period,\n        now,\n    )\n\n    if args.debug:\n        LOGGER.debug(\"State file not written (debug mode)\")\n        return\n    if not args.no_state:\n        state.write()\n\n\nif __name__ == \"__main__\":\n    main()\n"}
{"type": "source_file", "path": "agents/plugins/mk_mongodb.py", "content": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n# Copyright (C) 2019 Checkmk GmbH - License: GNU General Public License v2\n# This file is part of Checkmk (https://checkmk.com). It is subject to the terms and\n# conditions defined in the file COPYING, which is part of this source code package.\n\"\"\"Monitor MongoDB on Linux\n\nThis agent plugin creates various sections out of the MongoDB server status information.\nImportant: 1) If MongoDB runs as single instance the agent data is assigned\n              to the host same host where the plugin resides.\n\n           2) If MongoDB is deployed as replica set the agent data is piggybacked\n              to a different hostname, name after the replica set name.\n              You have to create a new host in the monitoring system matching the\n              replica set name, or use the piggyback translation rule to modify the\n              hostname according to your needs.\n\nIt is possible to run this script with pymongo 2.5.2 (the version provided by\ncentos 7) but you can not connect to a MongoDB 4.0 server with authentication.\nPymongo 2.5.2 uses the auth mechanism ``MONGODB-CR`` but this auth mechanism\nwas removed with MongoDB 4.0. If you want to use mk_mongodb.py with\nauthentication and a MongoDB server 4.0 you will have to use a more recent\nversion of pymongo (at least 2.8).\n\n\"\"\"\n\n__version__ = \"2.5.0b1\"\n\nimport argparse\nimport configparser\nimport json\nimport logging\nimport os\nimport sys\nimport time\nimport types\nfrom collections import defaultdict\nfrom urllib.parse import quote_plus\n\ntry:\n    from collections.abc import Iterable  # noqa: F401\n    from typing import Any  # noqa: F401\nexcept ImportError:\n    pass\n\n\ntry:\n    import pymongo\n    import pymongo.errors\n    from bson.json_util import dumps\nexcept ImportError:\n    sys.stdout.write(\"<<<mongodb_instance:sep(9)>>>\\n\")\n    sys.stdout.write(\n        \"error\\tpymongo library is not installed. Please install it on the monitored system \"\n        \"(for Python 3 use: 'pip3 install pymongo', for Python 2 use 'pip install pymongo')\\n\"\n    )\n    sys.exit(1)\n\n\nMK_VARDIR = os.environ.get(\"MK_VARDIR\")\nPYMONGO_VERSION = tuple(int(i) for i in pymongo.version.split(\".\"))\n\n\ndef get_database_info(client):\n    if isinstance(client.list_database_names, types.MethodType):\n        db_names = client.list_database_names()\n    elif isinstance(client.database_names, types.MethodType):\n        db_names = client.database_names()\n    else:\n        db_names = []\n\n    databases = defaultdict(dict)  # type: dict[str, dict[str, Any]]\n    for name in db_names:\n        database = client[name]\n        databases[name][\"collections\"] = list(get_collection_names(database))\n        databases[name][\"stats\"] = database.command(\"dbstats\")\n        databases[name][\"collstats\"] = {}\n        for collection in databases[name][\"collections\"]:\n            databases[name][\"collstats\"][collection] = database.command(\"collstats\", collection)\n    return databases\n\n\ndef get_collection_names(database):  # type:(pymongo.database.Database) -> Iterable[str]\n    if PYMONGO_VERSION <= (3, 6, 0):\n        collection_names = database.collection_names()  # type: list[str]\n    else:\n        collection_names = database.list_collection_names()\n\n    for collection_name in collection_names:\n        if \"viewOn\" in database[collection_name].options():\n            # we don't want to return views, as the command collstats can not be executed\n            continue\n        yield collection_name\n\n\ndef section_instance(server_status):\n    sys.stdout.write(\"<<<mongodb_instance:sep(9)>>>\\n\")\n    sys.stdout.write(\"version\\t%s\\n\" % server_status.get(\"version\", \"n/a\"))\n    sys.stdout.write(\"pid\\t%s\\n\" % server_status.get(\"pid\", \"n/a\"))\n\n    repl_info = server_status.get(\"repl\")\n    if not repl_info:\n        sys.stdout.write(\"mode\\tSingle Instance\\n\")\n\n    elif repl_info.get(\"isWritablePrimary\") or repl_info.get(\"ismaster\"):\n        sys.stdout.write(\"mode\\tPrimary\\n\")\n\n    elif repl_info.get(\"secondary\"):\n        sys.stdout.write(\"mode\\tSecondary\\n\")\n\n    else:\n        sys.stdout.write(\"mode\\tArbiter\\n\")\n\n    if repl_info and repl_info.get(\"me\"):\n        sys.stdout.write(\"address\\t%s\\n\" % repl_info.get(\"me\", \"n/a\"))\n\n\ndef section_flushing(server_status):\n    # key is depricated for MongoDB 4.0\n    flushing_info = server_status.get(\"backgroundFlushing\")\n    if flushing_info is None:\n        return\n    sys.stdout.write(\"<<<mongodb_flushing>>>\\n\")\n    sys.stdout.write(\"average_ms %s\\n\" % flushing_info.get(\"average_ms\", \"n/a\"))\n    sys.stdout.write(\"last_ms %s\\n\" % flushing_info.get(\"last_ms\", \"n/a\"))\n    sys.stdout.write(\"flushed %s\\n\" % flushing_info.get(\"flushes\", \"n/a\"))\n\n\ndef _write_section_replica(\n    primary,\n    secondary_actives=None,\n    secondary_passives=None,\n    arbiters=None,\n):\n    sys.stdout.write(\"<<<mongodb_replica:sep(0)>>>\\n\")\n    sys.stdout.write(\n        json.dumps(\n            {\n                \"primary\": primary,\n                \"secondaries\": {\n                    \"active\": secondary_actives or [],\n                    \"passive\": secondary_passives or [],\n                },\n                \"arbiters\": arbiters or [],\n            }\n        )\n        + \"\\n\"\n    )\n\n\ndef sections_replica(server_status):\n    repl_info = server_status.get(\"repl\")\n    if not repl_info:\n        return\n\n    def _remove_primary(primary, hosts):\n        if hosts is None:\n            return None\n        if primary is None:\n            return hosts\n        return list(set(hosts) - {primary})\n\n    primary = repl_info.get(\"primary\")\n    _write_section_replica(\n        primary,\n        secondary_actives=_remove_primary(primary, repl_info.get(\"hosts\")),\n        secondary_passives=repl_info.get(\"passives\"),\n        arbiters=repl_info.get(\"arbiters\"),\n    )\n\n\ndef sections_replica_set(client):\n    try:\n        rep_set_status = client.admin.command(\"replSetGetStatus\")\n    except pymongo.errors.OperationFailure:\n        LOGGER.debug(\n            \"Calling replSetGetStatus returned an error. \"\n            \"This might be ok if you have not configured replication on you mongodb server.\",\n            exc_info=True,\n        )\n        return\n\n    sys.stdout.write(\"<<<mongodb_replica_set:sep(9)>>>\\n\")\n    sys.stdout.write(\n        \"%s\\n\"\n        % json.dumps(\n            json.loads(dumps(rep_set_status)),\n            separators=(\",\", \":\"),\n        ),\n    )\n\n\ndef sections_replication_info(client, databases):\n    \"\"\"\n\n    :param client:\n    :param databases:\n    :return:\n    \"\"\"\n    if \"oplog.rs\" not in databases.get(\"local\", {}).get(\"collections\", {}):\n        # replication not detected\n        return\n\n    sys.stdout.write(\"<<<mongodb_replication_info:sep(9)>>>\\n\")\n    result_dict = _get_replication_info(client, databases)\n    sys.stdout.write(\"%s\\n\" % json.dumps(result_dict, separators=(\",\", \":\")))\n\n\ndef _get_replication_info(client, databases):\n    \"\"\"\n\n    :param client: mongdb client\n    :return: result\n    \"\"\"\n    oplog = databases.get(\"local\", {}).get(\"collstats\", {}).get(\"oplog.rs\", {})\n    result = {}\n\n    # this is basically \"db.getReplicationInfo()\" but it's not implemented in the python driver:\n    # https://jira.mongodb.org/browse/PYTHON-1717\n    # see also: https://gist.github.com/konstruktoid/bcb9daefab6beca67de833b5f547be91\n    # and: https://github.com/mongodb/mongo/blob/20d43f94ce5e943971904f65f8abff1e8b67521f/src/mongo/shell/db.js#L868-L933\n\n    # Returns the total size of the oplog in bytes\n    # This refers to the total amount of space allocated to the oplog rather than\n    # the current size of operations stored in the oplog.\n    if \"maxSize\" in oplog:\n        result[\"logSizeBytes\"] = oplog.get(\"maxSize\")\n    else:\n        return result\n\n    # Returns the total amount of space used by the oplog in bytes.\n    # This refers to the total amount of space currently used by operations stored in the oplog rather than\n    # the total amount of space allocated.\n    result[\"usedBytes\"] = oplog.get(\"size\", 0)\n\n    # Returns a timestamp for the first and last (i.e. earliest/latest) operation in the oplog.\n    # Compare this value to the last write operation issued against the server.\n    # Timestamp is time in seconds since epoch UTC\n    firstc = client.local.oplog.rs.find().sort([(\"$natural\", 1)]).limit(1)\n    lastc = client.local.oplog.rs.find().sort([(\"$natural\", -1)]).limit(1)\n    if firstc and lastc:\n        timestamp_first_operation = firstc.next().get(\"ts\", None)\n        timestamp_last_operation = lastc.next().get(\"ts\", None)\n        if timestamp_first_operation and timestamp_last_operation:\n            result[\"tFirst\"] = timestamp_first_operation.time\n            result[\"tLast\"] = timestamp_last_operation.time\n\n    result[\"now\"] = int(time.time())\n    return result\n\n\ndef section_cluster(client, databases):\n    \"\"\"\n    on router (mongos) node\n    1. get all databases\n    2. for each database, get all collections\n    3. get stats for each collection\n    5. get shards\n    6. get chunks and count chunks and jumbo chunks per shard\n    7. aggregate all into one conclusive dictionary\n    :param client: mongodb client\n    :param databases: database and collections statistic data as dictionary\n    \"\"\"\n    # check if we run on mongos (router) node\n    master_dict = client.admin.command(\"isMaster\")\n    if (\n        not (master_dict.get(\"isWritablePrimary\") or master_dict.get(\"ismaster\"))\n        or \"msg\" not in master_dict\n        or master_dict.get(\"msg\") != \"isdbgrid\"\n    ):\n        return\n\n    sys.stdout.write(\"<<<mongodb_cluster:sep(0)>>>\\n\")\n\n    # get balancer information\n    balancer_dict = _get_balancer_info(client)\n\n    # get cluster information for databases\n    databases_cluster_info = client.config.databases.find({}, {\"primary\": 1, \"partitioned\": 1})\n    _add_cluster_info(databases, databases_cluster_info)\n\n    # get chunksize\n    chunk_size_info = _get_chunk_size_information(client)\n\n    # get additional collection information\n    collections_dict = _get_collections_information(client)\n\n    # get all shards\n    shards_dict = _get_shards_information(client)\n\n    # get number of chunks per shard\n    chunks_dict = _count_chunks_per_shard(client, databases)\n\n    # aggregate all information in one dict\n    all_informations_dict = _aggregate_chunks_and_shards_info(\n        databases, chunks_dict, shards_dict, collections_dict, balancer_dict, chunk_size_info\n    )\n\n    sys.stdout.write(\"%s\\n\" % json.dumps(all_informations_dict, separators=(\",\", \":\")))\n\n\ndef _get_balancer_info(client):\n    \"\"\"\n    get information if balancer is enabled for cluster\n    get balancer statistics\n    :param client: mongdb client\n    :return: balancer status dictionary\n    \"\"\"\n    balancer_dict = {}\n\n    # check if balancer is enabled for cluster\n    settings = client[\"config\"][\"settings\"]\n    settings_dict = settings.find_one({\"_id\": \"balancer\"})\n    if settings_dict:\n        balancer_dict[\"balancer_enabled\"] = not settings_dict.get(\"stopped\")\n    else:\n        balancer_dict[\"balancer_enabled\"] = True\n\n    # get balancer status\n    status = client.admin.command(\"balancerStatus\")\n    _remove_keys(status, [\"$clusterTime\", \"operationTime\", \"ok\"])\n    balancer_dict.update(status)\n\n    return balancer_dict\n\n\ndef _add_cluster_info(databases, databases_cluster_info):\n    \"\"\"\n    add additional information for databases to main databases dictionary\n    :param databases: main database information dictionary\n    :param databases_cluster_info: additional information per database\n    \"\"\"\n    for database in databases_cluster_info:\n        database_name = database.get(\"_id\")\n        database.pop(\"_id\", None)\n        if database_name in databases:\n            databases.get(database_name).update(database)\n        else:\n            # add missing databases\n            databases[database_name] = database\n            databases.get(database_name).setdefault(\"collstats\", {})\n            databases.get(database_name).setdefault(\"collections\", [])\n\n\ndef _aggregate_chunks_and_shards_info(\n    databases_dict, chunks_dict, shards_dict, collections_dict, balancer_dict, settings_dict\n):\n    \"\"\"\n    generate one dictionary containing shards and chunks information per collection per database\n    :param databases_dict: dictionary with database and collections statistic details\n    :param chunks_dict: dictionary with number of chunks and jumps per shard\n    :param shards_dict: dictionary with information which shard is on which host\n    :return: dictionary with database, collections, shards and chunks information\n    \"\"\"\n    # remove system databases\n    _remove_keys(databases_dict, [\"admin\", \"config\"])\n\n    # chunks_dict: add info 'number_of_chunks' from chunks dict to collections statistic dictionary\n    for database_name in databases_dict:\n        for collection_name in databases_dict.get(database_name).get(\"collections\", []):\n            collection_info = collections_dict.get(database_name, {}).get(collection_name, {})\n            if collection_info:\n                databases_dict.get(database_name).get(\"collstats\").get(collection_name).update(\n                    collection_info\n                )\n            for shard_name in shards_dict:\n                chunks_info = (\n                    chunks_dict.get(database_name, {}).get(collection_name, {}).get(shard_name, {})\n                )\n                if chunks_info and shard_name in databases_dict.get(database_name).get(\n                    \"collstats\"\n                ).get(collection_name).get(\"shards\"):\n                    databases_dict.get(database_name).get(\"collstats\").get(collection_name).get(\n                        \"shards\"\n                    ).get(shard_name).update(chunks_info)\n\n    # remove irrelevant data\n    _lensing_data(databases_dict)\n\n    # shards_dict: add shard information to collections statistic dictionary\n    all_information_dict = {}\n    all_information_dict[\"databases\"] = databases_dict\n    all_information_dict[\"shards\"] = shards_dict\n    all_information_dict[\"balancer\"] = balancer_dict\n    all_information_dict[\"settings\"] = settings_dict\n\n    return all_information_dict\n\n\ndef _lensing_data(databases):\n    \"\"\"\n    removing data not needed for further processing\n    removing data that is not json conform (mongoDB extended json format)\n    :param databases: dictionary with databases, collections, shards information\n    :return: json convertible dictionary\n    \"\"\"\n    # clean up database data\n    for database_name in databases:\n        database = databases.get(database_name)\n        _remove_keys(database, [\"stats\"])\n\n        # clean up collections data\n        for collection_name in database.get(\"collstats\", {}):\n            collection = database.get(\"collstats\").get(collection_name)\n            # remove irrelevant data\n            _remove_keys(\n                collection,\n                [\n                    \"indexDetails\",\n                    \"wiredTiger\",\n                    \"operationTime\",\n                    \"lastCommittedOpTime\",\n                    \"$gleStats\",\n                    \"$configServerState\",\n                    \"$clusterTime\",\n                    \"indexSizes\",\n                ],\n            )\n\n            # clean up shards data\n            for shard_name in collection.get(\"shards\", {}):\n                shard = collection.get(\"shards\").get(shard_name)\n                # remove irrelevant data\n                _remove_keys(\n                    shard,\n                    [\n                        \"indexDetails\",\n                        \"wiredTiger\",\n                        \"operationTime\",\n                        \"lastCommittedOpTime\",\n                        \"$gleStats\",\n                        \"$configServerState\",\n                        \"$clusterTime\",\n                        \"indexSizes\",\n                    ],\n                )\n\n\ndef _get_chunk_size_information(client):\n    \"\"\"\n    chunk size default is 64MB. If the chunk size is changed, the changed value is in config.settings with id \"chunksize\".\n    example:\n    { \"_id\" : \"chunksize\", \"value\" : 64 }\n    value is in MB\n    :param client:\n    :return:\n    \"\"\"\n    chunk_size = 64 * 1024 * 1024\n    for setting in client.config.settings.find({\"_id\": \"chunksize\"}):\n        if \"value\" in setting:\n            chunk_size = int(setting.get(\"value\")) * 1024 * 1024\n    return {\"chunkSize\": chunk_size}\n\n\ndef _recursive_defaultdict():\n    return defaultdict(_recursive_defaultdict)\n\n\ndef _get_collections_information(client):\n    \"\"\"\n    get all documents from config collections\n    :param client: mongodb client\n    :return: dictionary with collections information\n    \"\"\"\n    collections_dict = _recursive_defaultdict()\n    for collection in client.config.collections.find(\n        {}, set([\"_id\", \"unique\", \"dropped\", \"noBalance\"])\n    ):\n        database_name, collection_name = _split_namespace(collection.get(\"_id\"))\n        collection.pop(\"_id\", None)\n        collections_dict[database_name][collection_name] = collection\n    return collections_dict\n\n\ndef _get_shards_information(client):\n    \"\"\"\n    get all documents from shards collection\n    :param client: mongodb client\n    :return: dictionary with shards information\n    \"\"\"\n    shard_dict = {}\n    for shard in client.config.shards.find():\n        shard_name = shard.get(\"_id\")\n        shard.pop(\"_id\", None)\n        shard_dict[shard_name] = shard\n    return shard_dict\n\n\ndef _count_chunks_per_shard(client, databases):\n    \"\"\"\n    count all chunks and jumbo chunks per shards\n    :param client: mongodb client\n    :return: dictionary with shards and sum of chunks and jumbo chunks\n    \"\"\"\n    chunks_dict = _recursive_defaultdict()\n\n    # initialize dictionary\n    # set default defaults for numberOfChunks and numberOfJumbos\n    for database_name in databases:\n        for collection_name in databases.get(database_name).get(\"collections\", {}):\n            for shard_name in (\n                databases.get(database_name)\n                .get(\"collstats\")\n                .get(collection_name, {})\n                .get(\"shards\", {})\n            ):\n                is_sharded = (\n                    databases.get(database_name)\n                    .get(\"collstats\")\n                    .get(collection_name, {})\n                    .get(\"sharded\", False)\n                )\n                if is_sharded:  # we count chunks below\n                    chunks_dict[database_name][collection_name][shard_name][\"numberOfChunks\"] = 0\n                else:  # unsharded => only 1 shard => nchunks = numberOfChunks (total number of chunks)\n                    chunks_dict[database_name][collection_name][shard_name][\"numberOfChunks\"] = (\n                        databases.get(database_name)\n                        .get(\"collstats\")\n                        .get(collection_name)\n                        .get(\"nchunks\", 0)\n                    )\n                chunks_dict[database_name][collection_name][shard_name][\"numberOfJumbos\"] = 0\n\n    chunks = client.config.chunks\n    chunks_list = chunks.find({}, set([\"ns\", \"shard\", \"jumbo\"]))\n    database_set = set()\n    for chunk in chunks_list:\n        # get database, collection and shard names\n        shard_name = chunk.get(\"shard\", None)\n        database_name, collection_name = _split_namespace(chunk.get(\"ns\"))\n\n        # if there are no chunk information for this database, continue\n        if database_name not in chunks_dict:\n            continue\n\n        # for later user\n        database_set.add(database_name)\n\n        # count number of chunks per shard\n        if chunks_dict:\n            chunks_dict.get(database_name).get(collection_name).get(shard_name)[\n                \"numberOfChunks\"\n            ] += 1\n\n        # count number of jumbo chunks per shard\n        if \"jumbo\" in chunk:\n            chunks_dict.get(database_name).get(collection_name).get(shard_name)[\n                \"numberOfJumbos\"\n            ] += 1\n\n    return chunks_dict\n\n\ndef _remove_keys(dictionary, list_of_keys):\n    \"\"\"\n    remove keys from dictionary\n    :param stats_dict:\n    :param list_of_keys:\n    :return:\n    \"\"\"\n    for key_to_delete in list_of_keys:\n        dictionary.pop(key_to_delete, None)\n\n\ndef _split_namespace(namespace):\n    \"\"\"\n    split namespace into database name and collection name\n    :param namespace:\n    :return:\n    \"\"\"\n    try:\n        names = namespace.split(\".\", 1)\n        if len(names) > 1:\n            return names[0], names[1]\n    except ValueError:\n        pass\n    except AttributeError:\n        pass\n    raise ValueError(\"error parsing namespace %s\" % namespace)\n\n\ndef section_locks(server_status):\n    sys.stdout.write(\"<<<mongodb_locks>>>\\n\")\n    global_lock_info = server_status.get(\"globalLock\")\n    if global_lock_info:\n        for what in [\"activeClients\", \"currentQueue\"]:\n            if what in global_lock_info:\n                for key, value in global_lock_info[what].items():\n                    sys.stdout.write(\"%s %s %s\\n\" % (what, key, value))\n\n\ndef section_by_keys(section_name, keys, server_status, output_key=False):\n    sys.stdout.write(\"<<<mongodb_%s>>>\\n\" % section_name)\n    for key in keys:\n        fmt = (\"%s \" % key if output_key else \"\") + \"%s %s\\n\"\n        for item in server_status.get(key, {}).items():\n            sys.stdout.write(fmt % item)\n\n\ndef section_collections(client, databases):\n    sys.stdout.write(\"<<<mongodb_collections:sep(9)>>>\\n\")\n    database_collection = databases.copy()\n    indexes_dict = _get_indexes_information(client, databases)\n\n    for database_name in database_collection:\n        database = database_collection.get(database_name)\n\n        # remove stats section\n        _remove_keys(database, [\"stats\"])\n\n        # clean up collections data\n        for collection_name in database.get(\"collstats\", {}):\n            collection = database.get(\"collstats\").get(collection_name)\n            # remove irrelevant data\n            _remove_keys(\n                collection,\n                [\n                    \"indexDetails\",\n                    \"wiredTiger\",\n                    \"operationTime\",\n                    \"lastCommittedOpTime\",\n                    \"$gleStats\",\n                    \"$configServerState\",\n                    \"$clusterTime\",\n                    \"shards\",\n                ],\n            )\n            if indexes_dict is None:\n                continue\n            collection[\"indexStats\"] = (\n                indexes_dict.get(database_name, {})\n                .get(collection_name, {})\n                .get(\n                    \"indexStats\",\n                    {},\n                )\n            )\n\n    sys.stdout.write(\n        \"%s\\n\"\n        % json.dumps(\n            json.loads(dumps(database_collection)),\n            separators=(\",\", \":\"),\n        ),\n    )\n    database_collection.clear()\n\n\ndef _get_indexes_information(client, databases):\n    \"\"\"\n    get all documents from shards collection\n    :param client: mongodb client\n    :return: dictionary with shards information\n    \"\"\"\n    indexes_dict = _recursive_defaultdict()\n    for database_name in databases:\n        database = databases.get(database_name)\n        for collection_name in database.get(\"collections\", []):\n            try:\n                # $indexStat only available since mongodb v. 3.2\n                indexes_dict[database_name][collection_name][\"indexStats\"] = client[database_name][\n                    collection_name\n                ].aggregate(\n                    [\n                        {\n                            \"$indexStats\": {},\n                        }\n                    ]\n                )\n            except pymongo.errors.OperationFailure:\n                LOGGER.debug(\"Could not access $indexStat\", exc_info=True)\n                return None\n\n    return indexes_dict\n\n\ndef get_timestamp(text):\n    \"\"\"parse timestamps like 'Nov  6 13:44:09.345' or '2015-10-17T05:35:24.234'\"\"\"\n    text = text.split(\".\")[0]\n    for pattern in [\"%a %b %d %H:%M:%S\", \"%Y-%m-%dT%H:%M:%S\"]:\n        try:\n            return time.mktime(time.strptime(text, pattern))\n        except ValueError:\n            pass\n    return None\n\n\ndef read_statefile(state_file):\n    try:\n        with open(state_file) as state_fd:\n            last_timestamp = int(state_fd.read())\n    except (IOError, ValueError):\n        return None, True\n\n    if time.localtime(last_timestamp).tm_year >= 2015:\n        return last_timestamp, False\n\n    # Note: there is no year information in these loglines\n    # As workaround we look at the creation date (year) of the last statefile\n    # If it differs and there are new messages we start from the beginning\n    statefile_year = time.localtime(os.stat(state_file).st_ctime).tm_year\n    output_all = time.localtime().tm_year != statefile_year\n    return last_timestamp, output_all\n\n\ndef update_statefile(state_file, startup_warnings):\n    lines = startup_warnings.get(\"log\")\n    if not lines:\n        return\n    timestamp = get_timestamp(lines[-1])\n    try:\n        with open(state_file, \"w\") as state_fd:\n            state_fd.write(\"%d\" % timestamp)\n    except (IOError, TypeError):\n        # TypeError: timestamp was None, but at least ctime is updated.\n        pass\n\n\ndef section_logwatch(client):\n    if not MK_VARDIR:\n        return\n\n    sys.stdout.write(\"<<<logwatch>>>\\n\")\n    sys.stdout.write(\"[[[MongoDB startupWarnings]]]\\n\")\n    startup_warnings = client.admin.command({\"getLog\": \"startupWarnings\"})\n\n    state_file = \"%s/mongodb.state\" % MK_VARDIR\n\n    last_timestamp, output_all = read_statefile(state_file)\n\n    for line in startup_warnings[\"log\"]:\n        state = \"C\"\n        state_index = line.find(\"]\") + 2\n        if len(line) == state_index or line[state_index:].startswith(\"**  \"):\n            state = \".\"\n\n        if \"** WARNING:\" in line:\n            state = \"W\"\n\n        if output_all or get_timestamp(line) > last_timestamp:\n            sys.stdout.write(\"%s %s\\n\" % (state, line))\n\n    update_statefile(state_file, startup_warnings)\n\n\nDEFAULT_CFG_FILE = os.path.join(os.getenv(\"MK_CONFDIR\", \"\"), \"mk_mongodb.cfg\")\n\nLOGGER = logging.getLogger(__name__)\n\n\ndef parse_arguments(argv):\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--debug\", action=\"store_true\", help=\"\"\"Debug mode: raise Python exceptions\"\"\"\n    )\n    parser.add_argument(\n        \"-v\",\n        \"--verbose\",\n        action=\"count\",\n        default=0,\n        help=\"\"\"Verbose mode (for even more output use -vvv)\"\"\",\n    )\n    parser.add_argument(\n        \"-c\",\n        \"--config-file\",\n        default=DEFAULT_CFG_FILE,\n        help=\"\"\"Read config file (default: %(default)s)\"\"\",\n    )\n\n    return parser.parse_args(argv)\n\n\ndef setup_logging(verbosity):\n    fmt = \"%%(levelname)5s: %s%%(message)s\"\n    if verbosity == 0:\n        logging.basicConfig(level=logging.WARNING, format=fmt % \"\")\n    elif verbosity == 1:\n        logging.basicConfig(level=logging.INFO, format=fmt % \"\")\n    else:\n        logging.basicConfig(level=logging.DEBUG, format=fmt % \"(line %(lineno)3d) \")\n\n\nclass MongoDBConfigParser(configparser.ConfigParser):\n    \"\"\"\n    Python2/Python3 compatibility layer for ConfigParser\n    \"\"\"\n\n    mongo_section = \"MONGODB\"\n\n    def read_from_filename(self, filename):\n        LOGGER.debug(\"trying to read %r\", filename)\n        if not os.path.exists(filename):\n            LOGGER.warning(\"config file %s does not exist!\", filename)\n        else:\n            with open(filename, \"r\") as cfg:\n                if sys.version_info[0] == 2:\n                    self.readfp(cfg)\n                else:\n                    self.read_file(cfg)\n            LOGGER.info(\"read configuration file %r\", filename)\n\n    def get_mongodb_bool(self, option, *, default=None):\n        if not self.has_option(self.mongo_section, option):\n            return default\n        return self.getboolean(self.mongo_section, option)\n\n    def get_mongodb_str(self, option, *, default=None):\n        if not self.has_option(self.mongo_section, option):\n            return default\n        return self.get(self.mongo_section, option)\n\n    def get_mongodb_int(self, option, *, default=None):\n        if not self.has_option(self.mongo_section, option):\n            return default\n        return self.getint(self.mongo_section, option)\n\n\nclass Config:\n    def __init__(self, config):\n        # type: (MongoDBConfigParser) -> None\n        self.tls_enable = config.get_mongodb_bool(\"tls_enable\")\n        self.tls_verify = config.get_mongodb_bool(\"tls_verify\")\n        self.tls_ca_file = config.get_mongodb_str(\"tls_ca_file\")\n        self.tls_cert_key_file = config.get_mongodb_str(\"tls_cert_key_file\")\n\n        self.auth_mechanism = config.get_mongodb_str(\"auth_mechanism\")\n        self.auth_source = config.get_mongodb_str(\"auth_source\")\n\n        self.host = config.get_mongodb_str(\"host\")\n        self.port = config.get_mongodb_int(\"port\")\n        self.username = config.get_mongodb_str(\"username\")\n        self.password = config.get_mongodb_str(\"password\")\n\n    def get_pymongo_config(self):\n        # type:() -> dict[str, str | bool]\n        \"\"\"\n        return config for latest pymongo (3.12.X)\n        \"\"\"\n        pymongo_config = {}\n        if self.username and self.auth_mechanism != \"MONGODB-X509\":\n            pymongo_config[\"username\"] = self.username\n            if self.password:\n                pymongo_config[\"password\"] = self.password\n\n        if self.tls_enable is not None:\n            pymongo_config[\"tls\"] = self.tls_enable\n            if self.tls_enable:\n                if self.tls_verify is not None:\n                    pymongo_config[\"tlsInsecure\"] = not self.tls_verify\n                if self.tls_ca_file is not None:\n                    pymongo_config[\"tlsCAFile\"] = self.tls_ca_file\n                if self.tls_cert_key_file is not None:\n                    pymongo_config[\"tlsCertificateKeyFile\"] = self.tls_cert_key_file\n        if self.auth_mechanism is not None:\n            pymongo_config[\"authMechanism\"] = self.auth_mechanism\n        if self.auth_source is not None and self.auth_mechanism != \"MONGODB-X509\":\n            pymongo_config[\"authSource\"] = self.auth_source\n        if self.host is not None:\n            pymongo_config[\"host\"] = self.host\n        if self.port is not None:\n            pymongo_config[\"port\"] = self.port\n\n        # Requests are distributed to secondaries, ref.\n        # https://www.mongodb.com/docs/manual/core/read-preference/\n        pymongo_config[\"read_preference\"] = pymongo.ReadPreference.SECONDARY\n\n        # The agent plugin is expected to run on each host, returing\n        # information from only that host.\n        # If directConnection is set to False (default), the plugin could also\n        # connect to a totally different host from where it is located.\n        if PYMONGO_VERSION >= (3, 11, 0):\n            # See 'Changes in Version 3.11.0' on\n            # https://pymongo.readthedocs.io/en/stable/changelog.html\n            pymongo_config[\"directConnection\"] = True\n\n        return pymongo_config\n\n\nclass PyMongoConfigTransformer:\n    def __init__(self, config):\n        # type:(Config) -> None\n        self._config = config\n\n    def transform(self, pymongo_config):\n        version_transforms = [\n            # apply the transform if the version of pymongo is lower than the\n            # tuple defined here. For the oldest pymongo version, multiple\n            # transforms will be executed.\n            ((3, 9, 0), self._transform_tls_to_ssl),\n            ((3, 5, 0), self._transform_credentials_to_uri),\n        ]\n\n        for version, transform_function in version_transforms:\n            if PYMONGO_VERSION < version:\n                pymongo_config = transform_function(pymongo_config)\n        return pymongo_config\n\n    def _transform_tls_to_ssl(self, pymongo_config):\n        # type:(dict[str, str | bool]) -> dict[str, str | bool]\n        if pymongo_config.get(\"tlsInsecure\") is True:\n            sys.stdout.write(\"<<<mongodb_instance:sep(9)>>>\\n\")\n            sys.stdout.write(\n                (\n                    \"error\\tCan not use option 'tls_verify = False' with this pymongo version %s.\"\n                    \"This option is only available with pymongo > 3.9.0\\n\"\n                )\n                % str(PYMONGO_VERSION)\n            )\n            sys.exit(3)\n        pymongo_config.pop(\"tlsInsecure\", None)\n\n        new_to_old = (\n            (\"tls\", \"ssl\"),\n            (\"tlsCAFile\", \"ssl_ca_certs\"),\n        )\n        for new_arg, old_arg in new_to_old:\n            if new_arg in pymongo_config:\n                pymongo_config[old_arg] = pymongo_config.pop(new_arg)\n        return pymongo_config\n\n    def _transform_credentials_to_uri(self, pymongo_config):\n        # type:(dict[str, str | bool]) -> dict[str, str | bool]\n        username = pymongo_config.pop(\"username\", None)\n        password = pymongo_config.pop(\"password\", None)\n        host = pymongo_config.pop(\"host\", \"localhost\")\n        port = pymongo_config.pop(\"port\", 27017)\n        if username is not None:\n            password_element = \"\"\n            if password is not None:\n                password_element = \":{}\".format(quote_plus(self._config.password))\n            uri = \"mongodb://{}{}@{}:{}\".format(\n                quote_plus(self._config.username), password_element, host, port\n            )\n        else:\n            uri = \"mongodb://{}:{}\".format(host, port)\n        pymongo_config[\"host\"] = uri\n        return pymongo_config\n\n\ndef main(argv=None):\n    if argv is None:\n        argv = sys.argv[1:]\n\n    args = parse_arguments(argv)\n    setup_logging(args.verbose)\n    LOGGER.debug(\"parsed args: %r\", args)\n    if LOGGER.isEnabledFor(logging.INFO):\n        LOGGER.info(\"python version: %s\", sys.version.replace(\"\\n\", \" \"))\n        LOGGER.info(\"pymongo version: %s\", PYMONGO_VERSION)\n        LOGGER.info(\"mk_mongodb version: %s\", __version__)\n\n    config_parser = MongoDBConfigParser()\n    config_parser.read_from_filename(os.path.abspath(args.config_file))\n    config = Config(config_parser)\n    pymongo_config = PyMongoConfigTransformer(config).transform(config.get_pymongo_config())\n\n    if LOGGER.isEnabledFor(logging.INFO):\n        LOGGER.info(\"pymongo configuration:\")\n        message = str(pymongo_config)\n        if config.password is not None:\n            message = message.replace(config.password, \"****\")\n            message = message.replace(quote_plus(config.password), \"****\")\n        LOGGER.info(message)\n\n    client = pymongo.MongoClient(**pymongo_config)  # type: pymongo.MongoClient\n    try:\n        # connecting is lazy, it might fail only now\n        server_status = client.admin.command(\"serverStatus\")  # type: dict\n    except (pymongo.errors.OperationFailure, pymongo.errors.ConnectionFailure) as e:\n        sys.stdout.write(\"<<<mongodb_instance:sep(9)>>>\\n\")\n        sys.stdout.write(\"error\\tFailed to connect\\n\")\n        # TLS issues are thrown as pymongo.errors.ServerSelectionTimeoutError\n        # (e.g. config with enabled TLS, but mongodb is plaintext only)\n        # Give the user some hints what the issue could be:\n        sys.stdout.write(\"details\\t%s\\n\" % str(e))\n        sys.exit(2)\n\n    section_instance(server_status)\n    repl_info = server_status.get(\"repl\")\n    if repl_info and not (repl_info.get(\"isWritablePrimary\") or repl_info.get(\"ismaster\")):\n        # this is a special case: replica set without master\n        # this is detected here\n        if \"primary\" in repl_info and not repl_info.get(\"primary\"):\n            _write_section_replica(None)\n        return\n\n    piggyhost = repl_info.get(\"setName\") if repl_info else None\n    if piggyhost:\n        sys.stdout.write(\"<<<<%s>>>>\\n\" % piggyhost)\n    try:\n        potentially_piggybacked_sections(client, server_status)\n    finally:\n        if piggyhost:\n            sys.stdout.write(\"<<<<>>>>\\n\")\n\n\ndef potentially_piggybacked_sections(client, server_status):\n    sections_replica(server_status)\n    sections_replica_set(client)\n    section_by_keys(\"asserts\", (\"asserts\",), server_status)\n    section_by_keys(\"connections\", (\"connections\",), server_status)\n    databases = get_database_info(client)\n    section_locks(server_status)\n    section_flushing(server_status)\n    section_by_keys(\"mem\", (\"mem\", \"extra_info\"), server_status)\n    section_by_keys(\"counters\", (\"opcounters\", \"opcountersRepl\"), server_status, output_key=True)\n    section_collections(client, databases)\n    section_cluster(client, databases)\n    sections_replication_info(client, databases)\n    section_logwatch(client)\n\n\nif __name__ == \"__main__\":\n    sys.exit(main())\n"}
{"type": "source_file", "path": "agents/plugins/mk_sap.py", "content": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n# Copyright (C) 2019 Checkmk GmbH - License: GNU General Public License v2\n# This file is part of Checkmk (https://checkmk.com). It is subject to the terms and\n# conditions defined in the file COPYING, which is part of this source code package.\n\n__version__ = \"2.5.0b1\"\n\n# This agent plugin has been built to collect information from SAP R/3 systems\n# using RFC calls. It needs the python module pyrfc.\n# pyrfc requires following libs (installable via pip3: Sphinx Cython setuptools-git\n# via apt or yum/dnf: gcc gcc-c++ python3-devel) (and the nwrfcsdk (can be downloaded from SAP\n# download portal) installed to be working. To install pyrfc export the SAPNWRFC_HOME:\n# \"export SAPNWRFC_HOME=/usr/sap/nwrfcsdk\". You can configure the agent plugin\n# using the configuration file /etc/check_mk/sap.cfg (a sample file can be\n# found in Checkmk git at agents/sap/sap.cfg) to tell it how to connect to\n# your SAP instance and which values you want to fetch from your system to be\n# forwarded to and checked by Checkmk.\n#\n#   Tested Versions:\n#   pyrfc:\n#   2.1.0, 2.4.0\n#   SAP nwrfc:\n#   750P_8-70002752\n\n# required params in sap.cfg:\n# 'ashost''sysnr' 'client' 'user' 'passwd' 'trace' 'loglevel' 'lang'\n#\n# create file /etc/ld.so.conf.d/pyrfc.conf and containing the path\n# \"/usr/sap/nwrfcsdk/lib\" and run \"ldconfig\" afterwards.\n\n\n#\n# During development the \"CCMS_Doku.pdf\" was really helpful.\n\nimport ast\nimport datetime\nimport fcntl\nimport fnmatch\nimport os\nimport sys\nimport time\n\ntry:\n    from typing import Any  # noqa: F401\nexcept ImportError:\n    pass\n\nif sys.version_info[:2] < (3, 5):\n    RecursionError = RuntimeError  # noqa: A001\n\n# #############################################################################\n\n# This sign is used to separate the path parts given in the config\nSEPARATOR = \"/\"\n\n# This are the different classes of monitoring objects which\n# can be found in the tree.\n#\n# Summarizs information from several subnodes\nMTE_SUMMARY = \"050\"\n# A monitoring object which has several subnodes which lead to the status\n# of this object. For example it is the \"CPU\" object on a host\nMTE_MON_OBJ = \"070\"\n# Contains performance information (which can be used to create graphs from)\nMTE_PERFORMANCE = \"100\"\n# Might contain several messages\nMTE_MSG_CONTAINER = \"101\"\n# Contains a single status message\nMTE_SINGLE_MSG = \"102\"\n# This is a long text label without status\nMTE_LONG_TXT = \"110\"\n# This is a short text label without status\nMTE_SHORT_TXT = \"111\"\n# Is a \"folder\" which has no own state, just computed by its children\nMTE_VIRTUAL = \"199\"\n\n# This map converts between the SAP color codes (key values) and the\n# nagios state codes and strings\nSTATE_VALUE_MAP = {\n    0: (0, \"OK\"),  # GRAY  (inactive or no current info available) -> OK\n    1: (0, \"OK\"),  # GREEN  -> OK\n    2: (1, \"WARN\"),  # YELLOW -> WARNING\n    3: (2, \"CRIT\"),  # RED    -> CRITICAL\n}\n\nSTATE_LOGWATCH_MAP = [\"O\", \"O\", \"W\", \"C\"]\n\n# Monitoring objects of these classes are skipped during processing\nSKIP_MTCLASSES = [\n    MTE_VIRTUAL,\n    MTE_SUMMARY,\n    MTE_MON_OBJ,\n    MTE_SHORT_TXT,\n    MTE_LONG_TXT,\n]\n\nMK_CONFDIR = os.getenv(\"MK_CONFDIR\") or \"/etc/check_mk\"\nMK_VARDIR = os.getenv(\"MK_VARDIR\") or \"/var/lib/check_mk_agent\"\n\nSTATE_FILE = MK_VARDIR + \"/sap.state\"\nstate_file_changed = False\n\n# #############################################################################\n\n# Settings to be used to connect to the SAP R/3 host.\nlocal_cfg = {\n    \"ashost\": \"localhost\",\n    \"sysnr\": \"00\",\n    \"client\": \"100\",\n    \"user\": \"\",\n    \"passwd\": \"\",\n    \"trace\": \"3\",\n    \"loglevel\": \"warn\",\n    # \"lang\": \"EN\",\n    # \"host_prefix\": \"FOOBAR_\",\n}\n\n# A list of strings, while the string must match the full path to one or\n# several monitor objects. We use unix shell patterns during matching, so\n# you can use several chars as placeholders:\n#\n# *      matches everything\n# ?      matches any single character\n# [seq]  matches any character in seq\n# [!seq] matches any character not in seq\n#\n# The * matches the whole following string and does not end on next \"/\".\n# For examples, take a look at the default config file (/etc/check_mk/sap.cfg).\nmonitor_paths = [\n    \"SAP CCMS Monitor Templates/Dialog Overview/*\",\n]\nmonitor_types = []  # type: list[str]\nconfig_file = MK_CONFDIR + \"/sap.cfg\"\n\ncfg = {}  # type: list[dict[Any, Any]] | dict[Any, Any]\nif os.path.exists(config_file):\n    with open(config_file) as opened_file:\n        exec(opened_file.read())  # nosec B102 # BNS:a29406\n    if isinstance(cfg, dict):\n        cfg = [cfg]\nelse:\n    cfg = [local_cfg]\n\n# Load the state file into memory\ntry:\n    with open(STATE_FILE) as opened_file:\n        states = ast.literal_eval(opened_file.read())\nexcept IOError:\n    states = {}\n\n# index of all logfiles which have been found in a run. This is used to\n# remove logfiles which are not available anymore from the states dict.\nlogfiles = []\n\n# #############################################################################\n\n#\n# HELPERS\n#\n\n\nclass SapError(Exception):\n    pass\n\n\ndef to_be_monitored(path, toplevel_match=False):\n    for rule in monitor_paths:\n        if toplevel_match and rule.count(\"/\") > 1:\n            rule = \"/\".join(rule.split(\"/\")[:2])\n\n        if fnmatch.fnmatch(path, rule):\n            return True\n    return False\n\n\ndef node_path(tree, node, path=\"\"):\n    if path:\n        path = node[\"MTNAMESHRT\"].rstrip() + SEPARATOR + path\n    else:\n        path = node[\"MTNAMESHRT\"].rstrip()\n\n    if node[\"ALPARINTRE\"] > 0:\n        parent_node = tree[node[\"ALPARINTRE\"] - 1]\n        return node_path(tree, parent_node, path)\n    return path\n\n\n#\n# API ACCESS FUNCTIONS\n#\n\n\ndef login(conn):\n    f = conn.call(\n        \"BAPI_XMI_LOGON\",\n        EXTCOMPANY=\"Checkmk GmbH\",\n        EXTPRODUCT=\"Check_MK SAP Agent\",\n        INTERFACE=\"XAL\",\n        VERSION=\"1.0\",\n    )\n    return f[\"SESSIONID\"]\n\n\ndef logout(conn):\n    conn.call(\"BAPI_XMI_LOGOFF\", INTERFACE=\"XAL\")\n\n\ndef mon_list(conn, cfg_entry):\n    f = conn.call(\"BAPI_SYSTEM_MON_GETLIST\", EXTERNAL_USER_NAME=cfg_entry[\"user\"])\n    l = []\n    for mon in f[\"MONITOR_NAMES\"]:\n        l.append((mon[\"MS_NAME\"].rstrip(), mon[\"MONI_NAME\"].rstrip()))\n    return l\n\n\n# def ms_list(cfg):\n#     f = conn.call(\"BAPI_SYSTEM_MS_GETLIST\", EXTERNAL_USER_NAME=cfg[\"user\"])\n#     l = []\n#     for ms in f[\"MONITOR_SETS\"]:\n#         l.append(ms[\"NAME\"].rstrip())\n#     return l\n\n\ndef mon_tree(conn, cfg_entry, ms_name, mon_name):\n    f = conn.call(\n        \"BAPI_SYSTEM_MON_GETTREE\",\n        EXTERNAL_USER_NAME=cfg_entry[\"user\"],\n        MONITOR_NAME={\n            \"MS_NAME\": ms_name,\n            \"MONI_NAME\": mon_name,\n        },\n    )\n    tree = f[\"TREE_NODES\"]\n    for node in tree:\n        try:\n            node[\"PATH\"] = ms_name + SEPARATOR + node_path(tree, node)\n        except RecursionError:\n            raise SapError(\n                (\n                    \"Could not calculate path, recursion limit reached. \"\n                    \"Reorganise your SAP data to get past this error. \"\n                    \"Element that causes this: {node}\"\n                ).format(node=node)\n            )\n    return tree\n\n\ndef tid(node):\n    return {\n        \"MTSYSID\": node[\"MTSYSID\"].strip(),\n        \"MTMCNAME\": node[\"MTMCNAME\"].strip(),\n        \"MTNUMRANGE\": node[\"MTNUMRANGE\"].strip(),\n        \"MTUID\": node[\"MTUID\"].strip(),\n        \"MTCLASS\": node[\"MTCLASS\"].strip(),\n        \"MTINDEX\": node[\"MTINDEX\"].strip(),\n        \"EXTINDEX\": node[\"EXTINDEX\"].strip(),\n    }\n\n\ndef mon_perfdata(conn, cfg_entry, node):\n    f = conn.call(\n        \"BAPI_SYSTEM_MTE_GETPERFCURVAL\", EXTERNAL_USER_NAME=cfg_entry[\"user\"], TID=tid(node)\n    )\n    value = f[\"CURRENT_VALUE\"][\"LASTPERVAL\"]\n\n    f = conn.call(\n        \"BAPI_SYSTEM_MTE_GETPERFPROP\", EXTERNAL_USER_NAME=cfg_entry[\"user\"], TID=tid(node)\n    )\n    if f[\"PROPERTIES\"][\"DECIMALS\"] != 0:\n        value = (value + 0.0) / 10 ** f[\"PROPERTIES\"][\"DECIMALS\"]\n    uom = f[\"PROPERTIES\"][\"VALUNIT\"].strip()\n\n    return value, uom\n\n\ndef mon_msg(conn, cfg_entry, node):\n    f = conn.call(\"BAPI_SYSTEM_MTE_GETSMVALUE\", EXTERNAL_USER_NAME=cfg_entry[\"user\"], TID=tid(node))\n    data = f[\"VALUE\"]\n    dt = parse_dt(data[\"SMSGDATE\"], data[\"SMSGTIME\"])\n    return (dt, data[\"MSG\"].strip())\n\n\ndef parse_dt(d, t):\n    d = d.strip()\n    t = t.strip()\n    if not d or not t:\n        return None\n    return datetime.datetime(*time.strptime(d + t, \"%Y%m%d%H%M%S\")[:6])\n\n\ndef mon_alerts(conn, cfg_entry, node):\n    f = conn.call(\"BAPI_SYSTEM_MTE_GETALERTS\", EXTERNAL_USER_NAME=cfg_entry[\"user\"], TID=tid(node))\n    return f[\"ALERTS\"]\n\n\ndef aid(alert):\n    return {\n        \"ALSYSID\": alert[\"ALSYSID\"],\n        \"MSEGNAME\": alert[\"MSEGNAME\"],\n        \"ALUNIQNUM\": alert[\"ALUNIQNUM\"],\n        \"ALINDEX\": alert[\"ALINDEX\"],\n        \"ALERTDATE\": alert[\"ALERTDATE\"],\n        \"ALERTTIME\": alert[\"ALERTTIME\"],\n    }\n\n\ndef alert_details(conn, cfg_entry, alert):\n    f = conn.call(\n        \"BAPI_SYSTEM_ALERT_GETDETAILS\", EXTERNAL_USER_NAME=cfg_entry[\"user\"], AID=aid(alert)\n    )\n    # prop  = f[\"PROPERTIES\"]\n    state = f[\"VALUE\"]\n    msg = f[\"XMI_EXT_MSG\"][\"MSG\"].strip()\n    return state, msg\n\n\ndef process_alerts(conn, cfg_entry, logs, ms_name, mon_name, node, alerts):\n    global state_file_changed\n\n    sid = node[\"MTSYSID\"].strip() or \"Other\"\n    context = node[\"MTMCNAME\"].strip() or \"Other\"\n    path = node[\"PATH\"]\n\n    # Use the sid as hostname for the logs\n    hostname = sid\n    logfile = context + \"/\" + path\n\n    logfiles.append((hostname, logfile))\n\n    logs.setdefault(sid, {})\n    logs[hostname][logfile] = []\n    newest_log_dt = None\n    for alert in alerts:\n        dt = parse_dt(alert[\"ALERTDATE\"], alert[\"ALERTTIME\"])\n\n        if (hostname, logfile) in states and states[(hostname, logfile)] >= dt:\n            continue  # skip log messages which are older than the last cached date\n\n        if not newest_log_dt or dt > newest_log_dt:\n            newest_log_dt = dt  # store the newest log of this run\n\n        alert_state, alert_msg = alert_details(conn, cfg_entry, alert)\n        # Format lines to \"logwatch\" format\n        logs[hostname][logfile].append(\n            \"%s %s %s\"\n            % (\n                STATE_LOGWATCH_MAP[alert_state[\"VALUE\"]],\n                dt.strftime(\"%Y-%m-%d %H:%M:%S\"),\n                alert_msg,\n            )\n        )\n\n    if newest_log_dt:\n        # Write newest log age to cache to prevent double processing of logs\n        states[(hostname, logfile)] = newest_log_dt\n        state_file_changed = True\n    return logs\n\n\ndef check(pyrfc, cfg_entry):\n    conn = pyrfc.Connection(\n        ashost=cfg_entry[\"ashost\"],\n        sysnr=cfg_entry[\"sysnr\"],\n        client=cfg_entry[\"client\"],\n        user=cfg_entry[\"user\"],\n        passwd=cfg_entry[\"passwd\"],\n        loglevel=cfg_entry[\"loglevel\"],\n        lang=cfg_entry[\"lang\"],\n    )\n    login(conn)\n\n    logs = {}  # type: dict[str, dict[str, list]]\n    sap_data = {}  # type: dict[str, list]\n\n    # This loop is used to collect all information from SAP\n    for ms_name, mon_name in mon_list(conn, cfg_entry):\n        path = ms_name + SEPARATOR + mon_name\n        if not to_be_monitored(path, True):\n            continue\n\n        tree = mon_tree(conn, cfg_entry, ms_name, mon_name)\n        for node in tree:\n            if not to_be_monitored(node[\"PATH\"]):\n                continue\n            # sys.stdout.write(\"%s\\n\" % node[\"PATH\"])\n\n            status_details = \"\"  # type: str | tuple[str, Any]\n            perfvalue = \"-\"\n            uom = \"-\"\n\n            # Use precalculated states\n            state = {\n                \"VALUE\": node[\"ACTUALVAL\"],\n                \"SEVERITY\": node[\"ACTUALSEV\"],\n            }\n\n            if state[\"VALUE\"] not in STATE_VALUE_MAP:\n                sys.stdout.write(\"UNHANDLED STATE VALUE\\n\")\n                sys.exit(1)\n\n            #\n            # Handle different object classes individually\n            # to get details about them\n            #\n\n            if monitor_types and node[\"MTCLASS\"] not in monitor_types:\n                continue  # Skip unwanted classes if class filtering is enabled\n\n            if node[\"MTCLASS\"] == MTE_PERFORMANCE:\n                perfvalue, this_uom = mon_perfdata(conn, cfg_entry, node)\n                uom = this_uom if this_uom else uom\n\n            elif node[\"MTCLASS\"] == MTE_SINGLE_MSG:\n                status_details = \"%s: %s\" % mon_msg(conn, cfg_entry, node)\n\n            elif node[\"MTCLASS\"] == MTE_MSG_CONTAINER:\n                alerts = mon_alerts(conn, cfg_entry, node)\n                logs = process_alerts(conn, cfg_entry, logs, ms_name, mon_name, node, alerts)\n                if len(alerts) > 0:\n                    last_alert = alerts[-1]\n                    dt = parse_dt(last_alert[\"ALERTDATE\"], last_alert[\"ALERTTIME\"])\n                    alert_state, alert_msg = alert_details(conn, cfg_entry, last_alert)\n                    last_msg = \"%s: %s - %s\" % (\n                        dt,\n                        STATE_VALUE_MAP[alert_state[\"VALUE\"]][1],\n                        alert_msg,\n                    )\n\n                    status_details = \"%d Messages, Last: %s\" % (len(alerts), last_msg)\n                else:\n                    status_details = \"The log is empty\"\n\n            elif node[\"MTCLASS\"] not in SKIP_MTCLASSES:\n                # Add an error to output on unhandled classes\n                status_details = \"UNHANDLED MTCLASS\", node[\"MTCLASS\"]\n\n            if node[\"MTCLASS\"] not in SKIP_MTCLASSES:\n                sid = node[\"MTSYSID\"].strip() or \"Other\"\n                context = node[\"MTMCNAME\"].strip() or \"Other\"\n                path = node[\"PATH\"]\n\n                sap_data.setdefault(sid, [])\n                sap_data[sid].append(\n                    \"%s\\t%d\\t%3d\\t%s\\t%s\\t%s\\t%s\"\n                    % (\n                        context,\n                        state[\"VALUE\"],\n                        state[\"SEVERITY\"],\n                        path,\n                        perfvalue,\n                        uom,\n                        status_details,\n                    )\n                )\n\n    for host, host_sap in sap_data.items():\n        sys.stdout.write(\"<<<<%s%s>>>>\\n\" % (cfg_entry.get(\"host_prefix\", \"\"), host))\n        sys.stdout.write(\"<<<sap:sep(9)>>>\\n\")\n        sys.stdout.write(\"%s\\n\" % \"\\n\".join(host_sap))\n    sys.stdout.write(\"<<<<>>>>\\n\")\n\n    for host, host_logs in logs.items():\n        sys.stdout.write(\"<<<<%s>>>>\\n\" % host)\n        sys.stdout.write(\"<<<logwatch>>>\\n\")\n        for log, lines in host_logs.items():\n            sys.stdout.write(\"[[[%s]]]\\n\" % log)\n            if lines:\n                sys.stdout.write(\"\\n\".join(lines) + \"\\n\")\n        sys.stdout.write(\"<<<<>>>>\\n\")\n\n    logout(conn)\n    conn.close()\n\n\ndef main():\n    global state_file_changed\n\n    try:\n        import pyrfc  # type: ignore[import-not-found]\n    except ImportError as e:\n        if \"No module named pyrfc\" in str(e):\n            sys.stderr.write(\"Missing the Python module pyrfc.\\n\")\n            sys.exit(1)\n        else:\n            raise\n\n    # It is possible to configure multiple SAP instances to monitor. Loop them all, but\n    # do not terminate when one connection failed\n    processed_all = True\n    try:\n        for entry in cfg:\n            try:\n                check(pyrfc, entry)\n                sys.stdout.write(\"<<<sap_state:sep(9)>>>\\n%s\\tOK\\n\" % entry[\"ashost\"])\n            except SapError as e:\n                sys.stderr.write(\"ERROR: %s\\n\" % e)\n                sys.stdout.write(\"<<<sap_state:sep(9)>>>\\n%s\\t%s\\n\" % (entry[\"ashost\"], e))\n                processed_all = False\n            except Exception as e:\n                sys.stderr.write(\"ERROR: Unhandled exception (%s)\\n\" % e)\n                sys.stdout.write(\n                    \"<<<sap_state:sep(9)>>>\\n%s\\tUnhandled exception (%s)\\n\" % (entry[\"ashost\"], e)\n                )\n                processed_all = False\n\n        # Now check whether or not an old logfile needs to be removed. This can only\n        # be done this way, when all hosts have been reached. Otherwise the cleanup\n        # is skipped.\n        if processed_all:\n            for key in states.keys():\n                if key not in logfiles:\n                    state_file_changed = True\n                    del states[key]\n\n        # Only write the state file once per run. And only when it has been changed\n        if state_file_changed:\n            new_file = STATE_FILE + \".new\"\n            state_fd = os.open(new_file, os.O_WRONLY | os.O_CREAT)\n            fcntl.flock(state_fd, fcntl.LOCK_EX)\n            os.write(state_fd, repr(states).encode(\"utf-8\"))\n            os.close(state_fd)\n            os.rename(STATE_FILE + \".new\", STATE_FILE)\n\n    except Exception as e:\n        sys.stderr.write(\"ERROR: Unhandled exception (%s)\\n\" % e)\n\n    sys.exit(0)\n\n\nif __name__ == \"__main__\":\n    main()\n"}
{"type": "source_file", "path": "agents/plugins/mk_postgres.py", "content": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n# Copyright (C) 2019 Checkmk GmbH - License: GNU General Public License v2\n# This file is part of Checkmk (https://checkmk.com). It is subject to the terms and\n# conditions defined in the file COPYING, which is part of this source code package.\nr\"\"\"Check_MK Agent Plugin: mk_postgres\n\nThis is a Check_MK Agent plugin. If configured, it will be called by the\nagent without any arguments.\n\nCan be configured with $MK_CONFDIR/postgres.cfg\nExample for postgres.cfg file:\n\n-----postgres.cfg-----------------------------------------\nDBUSER=postgres\nPG_BINARY_PATH=/usr/bin/psql\nINSTANCE=/home/postgres/db1.env:USER_NAME:/PATH/TO/.pgpass:\nINSTANCE=/home/postgres/db2.env:USER_NAME:/PATH/TO/.pgpass:\n----------------------------------------------------------\n\nExample of an environment file:\n\n-----/home/postgres/db1.env-----------------------------------------\nexport PGDATABASE=\"data\"\nexport PGPORT=\"5432\"\nexport PGVERSION=\"14\"\n----------------------------------------------------------\n\nInside of the environment file, only `PGPORT` is mandatory.\nIn case there is no `INSTANCE` specified by the postgres.cfg, then the plugin assumes defaults.\nFor example, the configuration\n\n-----postgres.cfg-----------------------------------------\nDBUSER=postgres\nPG_BINARY_PATH=/usr/bin/psql\n----------------------------------------------------------\n\nis equivalent to\n\n-----postgres.cfg-----------------------------------------\nDBUSER=postgres\nPG_BINARY_PATH=/usr/bin/psql\nINSTANCE=/home/postgres/does-not-exist.env:postgres::postgres\n----------------------------------------------------------\n\n-----/home/postgres/does-not-exist.env--------------------\nexport PGDATABASE=\"main\"\nexport PGPORT=\"5432\"\n----------------------------------------------------------\n\nThe only difference being `/home/postgres/does-not-exist.env` does not exist in the first setup.\nDifferent defaults are chosen for Windows.\n\"\"\"\n\n__version__ = \"2.5.0b1\"\n\nimport abc\nimport io\nimport logging\n\n# optparse exist in python2.6 up to python 3.8. Do not use argparse, because it will not run with python2.6\nimport optparse\nimport os\nimport platform\nimport re\nimport stat\nimport subprocess\nimport sys\nimport tempfile\n\ntry:\n    from collections.abc import Callable, Iterable, Sequence  # noqa: F401\n    from typing import Any  # noqa: F401\nexcept ImportError:\n    # We need typing only for testing\n    pass\n\n# For Python 3 sys.stdout creates \\r\\n as newline for Windows.\n# Checkmk can't handle this therefore we rewrite sys.stdout to a new_stdout function.\n# If you want to use the old behaviour just use old_stdout.\nif sys.version_info[0] >= 3:\n    new_stdout = io.TextIOWrapper(\n        sys.stdout.buffer, newline=\"\\n\", encoding=sys.stdout.encoding, errors=sys.stdout.errors\n    )\n    old_stdout, sys.stdout = sys.stdout, new_stdout\n\nOS = platform.system()\nIS_LINUX = OS == \"Linux\"\nIS_WINDOWS = OS == \"Windows\"\nLOGGER = logging.getLogger(__name__)\nLINUX_PROCESS_MATCH_PATTERNS = [\n    re.compile(pattern)\n    for pattern in [\n        \"(.*)bin/postgres(.*)\",\n        \"(.*)bin/postmaster(.*)\",\n        \"(.*)bin/edb-postgres(.*)\",\n        \"^[0-9]+ postgres(.*)\",\n        \"^[0-9]+ postmaster(.*)\",\n        \"^[0-9]+ edb-postgres(.*)\",\n    ]\n]\nWINDOWS_PROCESS_MATCH_PATTERNS = [\n    re.compile(pattern)\n    for pattern in [\n        r\"(.*)bin\\\\postgres(.*)\",\n        r\"(.*)bin\\\\postmaster(.*)\",\n        r\"(.*)bin\\\\edb-postgres(.*)\",\n    ]\n]\n\nif sys.version_info[0] >= 3:\n    UTF_8_NEWLINE_CHARS = re.compile(r\"[\\n\\r\\u2028\\u000B\\u0085\\u2028\\u2029]+\")\nelse:\n    UTF_8_NEWLINE_CHARS = re.compile(u\"[\\u000A\\u000D\\u2028\\u000B\\u0085\\u2028\\u2029]+\")  # fmt: skip\n\n\nclass OSNotImplementedError(NotImplementedError):\n    def __str__(self):\n        # type: () -> str\n        return \"The OS type ({}) is not yet implemented.\".format(platform.system())\n\n\nif IS_LINUX:\n    import resource\nelif IS_WINDOWS:\n    import time\nelse:\n    raise OSNotImplementedError\n\n\n# for compatibility with python 2.6\ndef subprocess_check_output(args):\n    return subprocess.Popen(args, stdout=subprocess.PIPE).communicate()[0]\n\n\n# Borrowed from six\ndef ensure_str(s):\n    if sys.version_info[0] >= 3:\n        if isinstance(s, bytes):\n            return s.decode(\"utf-8\")\n    elif isinstance(s, unicode):  # noqa: F821\n        return s.encode(\"utf-8\")\n    return s\n\n\nclass PostgresPsqlError(RuntimeError):\n    pass\n\n\nclass PostgresBase:\n    \"\"\"\n    Base class for x-plattform postgres queries\n    :param db_user: The postgres db user\n    :param instance: Pass an instance, in case of monitoring a server with multiple instances\n\n    All abstract methods must have individual implementation depending on the OS type\n    which runs postgres.\n    All non-abstract methods are meant to work on all OS types which were subclassed.\n    \"\"\"\n\n    __metaclass__ = abc.ABCMeta\n    _supported_pg_versions = [\"12\"]\n\n    def __init__(self, db_user, pg_binary_path, instance, process_match_patterns):\n        # type: (str, str | None, dict, Sequence[re.Pattern]) -> None\n        self.db_user = db_user\n        self.name = instance[\"name\"]\n        self.pg_user = instance[\"pg_user\"]\n        self.pg_port = instance[\"pg_port\"]\n        self.pg_database = instance[\"pg_database\"]\n        self.pg_passfile = instance.get(\"pg_passfile\", \"\")\n        self.pg_version = instance.get(\"pg_version\")\n        self.my_env = os.environ.copy()\n        pg_passfile = instance.get(\"pg_passfile\")\n        if pg_passfile:\n            self.my_env[\"PGPASSFILE\"] = pg_passfile\n        self.sep = os.sep\n        self.psql_binary_name = \"psql\"\n        if pg_binary_path is None:\n            self.psql_binary_path = self.get_psql_binary_path()\n        else:\n            self.psql_binary_path = pg_binary_path\n        self.psql_binary_dirname = self.get_psql_binary_dirname()\n        self.conn_time = \"\"  # For caching as conn_time and version are in one query\n        self.process_match_patterns = process_match_patterns\n\n    @abc.abstractmethod\n    def run_sql_as_db_user(\n        self, sql_cmd, extra_args=\"\", field_sep=\";\", quiet=True, rows_only=True, mixed_cmd=False\n    ):\n        # type: (str, str, str, bool, bool, bool) -> str\n        \"\"\"This method implements the system specific way to call the psql interface\"\"\"\n\n    @abc.abstractmethod\n    def get_psql_binary_path(self):\n        \"\"\"This method returns the system specific psql binary and its path\"\"\"\n\n    @abc.abstractmethod\n    def get_psql_binary_dirname(self):\n        \"\"\"This method returns the system specific psql binary and its path\"\"\"\n\n    @abc.abstractmethod\n    def get_instances(self):\n        \"\"\"Gets all instances\"\"\"\n\n    @abc.abstractmethod\n    def get_stats(self, databases):\n        \"\"\"Get the stats\"\"\"\n\n    @abc.abstractmethod\n    def get_version_and_connection_time(self):\n        \"\"\"Get the pg version and the time for the query connection\"\"\"\n\n    @abc.abstractmethod\n    def get_bloat(self, databases, numeric_version):\n        \"\"\"Get the db bloats\"\"\"\n\n    def get_databases(self):\n        \"\"\"Gets all non template databases\"\"\"\n        sql_cmd = \"SELECT datname FROM pg_database WHERE datistemplate = false;\"\n        out = self.run_sql_as_db_user(sql_cmd)\n        return out.replace(\"\\r\", \"\").split(\"\\n\")\n\n    def get_server_version(self):\n        \"\"\"Gets the server version\"\"\"\n        out = self.run_sql_as_db_user(\"SHOW server_version;\")\n        if out == \"\":\n            raise PostgresPsqlError(\"psql connection returned with no data\")\n        version_as_string = out.split()[0]\n        # Use Major and Minor version for float casting: \"12.6.4\" -> 12.6\n        return float(\".\".join(version_as_string.split(\".\")[0:2]))\n\n    def get_condition_vars(self, numeric_version):\n        \"\"\"Gets condition variables for other queries\"\"\"\n        if numeric_version > 9.2:\n            return \"state\", \"'idle'\"\n        return \"current_query\", \"'<IDLE>'\"\n\n    def get_connections(self):\n        \"\"\"Gets the the idle and active connections\"\"\"\n        connection_sql_cmd = (\n            \"SELECT datname, \"\n            \"(SELECT setting AS mc FROM pg_settings \"\n            \"WHERE name = 'max_connections') AS mc, \"\n            \"COUNT(state) FILTER (WHERE state='idle') AS idle, \"\n            \"COUNT(state) FILTER (WHERE state='active') AS active \"\n            \"FROM pg_stat_activity group by 1;\"\n        )\n\n        return self.run_sql_as_db_user(\n            connection_sql_cmd, rows_only=False, extra_args=\"-P footer=off\"\n        )\n\n    def get_sessions(self, row, idle):\n        \"\"\"Gets idle and open sessions\"\"\"\n        condition = \"%s = %s\" % (row, idle)\n\n        sql_cmd = (\n            \"SELECT %s, count(*) FROM pg_stat_activity WHERE %s IS NOT NULL GROUP BY (%s);\"\n        ) % (condition, row, condition)  # nosec B608 # BNS:fa3c6c\n\n        out = self.run_sql_as_db_user(\n            sql_cmd, quiet=False, extra_args=\"--variable ON_ERROR_STOP=1\", field_sep=\" \"\n        )\n\n        # line with number of idle sessions is sometimes missing on Postgres 8.x. This can lead\n        # to an altogether empty section and thus the check disappearing.\n        if not out.startswith(\"t\"):\n            out += \"\\nt 0\"\n        return out\n\n    def get_query_duration(self, numeric_version):\n        \"\"\"Gets the query duration\"\"\"\n        # Previously part of simple_queries\n\n        if numeric_version > 9.2:\n            querytime_sql_cmd = (\n                \"SELECT datname, datid, usename, client_addr, state AS state, \"\n                \"COALESCE(ROUND(EXTRACT(epoch FROM now()-query_start)),0) \"\n                \"AS seconds, pid, \"\n                \"query \"\n                \"AS current_query FROM pg_stat_activity \"\n                \"WHERE (query_start IS NOT NULL AND \"\n                \"(state NOT LIKE 'idle%' OR state IS NULL)) \"\n                \"ORDER BY query_start, pid DESC;\"\n            )\n\n        else:\n            querytime_sql_cmd = (\n                \"SELECT datname, datid, usename, client_addr, '' AS state,\"\n                \" COALESCE(ROUND(EXTRACT(epoch FROM now()-query_start)),0) \"\n                \"AS seconds, procpid as pid, query AS current_query \"\n                \"FROM pg_stat_activity WHERE \"\n                \"(query_start IS NOT NULL AND current_query NOT LIKE '<IDLE>%') \"\n                \"ORDER BY query_start, procpid DESC;\"\n            )\n\n        return self.run_sql_as_db_user(\n            querytime_sql_cmd, rows_only=False, extra_args=\"-P footer=off\"\n        )\n\n    def get_stat_database(self):\n        \"\"\"Gets the database stats\"\"\"\n        # Previously part of simple_queries\n        sql_cmd = (\n            \"SELECT datid, datname, numbackends, xact_commit, xact_rollback, blks_read, \"\n            \"blks_hit, tup_returned, tup_fetched, tup_inserted, tup_updated, tup_deleted, \"\n            \"pg_database_size(datname) AS datsize FROM pg_stat_database;\"\n        )\n        return self.run_sql_as_db_user(sql_cmd, rows_only=False, extra_args=\"-P footer=off\")\n\n    def get_locks(self):\n        \"\"\"Get the locks\"\"\"\n        # Previously part of simple_queries\n        sql_cmd = (\n            \"SELECT datname, granted, mode FROM pg_locks l RIGHT \"\n            \"JOIN pg_database d ON (d.oid=l.database) WHERE d.datallowconn;\"\n        )\n        return self.run_sql_as_db_user(sql_cmd, rows_only=False, extra_args=\"-P footer=off\")\n\n    def get_version(self):\n        \"\"\"Wrapper around get_version_conn_time\"\"\"\n        version, self.conn_time = self.get_version_and_connection_time()\n        return version\n\n    def get_connection_time(self):\n        \"\"\"\n        Wrapper around get_version_conn time.\n        Execute query only if conn_time wasn't already set\n        \"\"\"\n        if self.conn_time == \"\":\n            _, self.conn_time = self.get_version_and_connection_time()\n        return self.conn_time\n\n    def is_pg_ready(self):\n        \"\"\"Executes pg_isready.\n        pg_isready is a utility for checking the connection status of a PostgreSQL database server.\n        \"\"\"\n\n        out = subprocess_check_output(\n            [\"%s%spg_isready\" % (self.psql_binary_dirname, os.sep), \"-p\", self.pg_port],\n        )\n\n        sys.stdout.write(\"%s\\n\" % ensure_str(out))\n\n    def is_postgres_process(self, process):\n        # type: (str) -> bool\n        \"\"\"Determine whether a process is a PostgreSQL process.\n\n        Note that the relevant binaries are contained in PATH under Linux, so they\n        may or may not be called using the full path. Starting from PostgreSQL\n        verion >= 13, they are not called using the full path.\n\n        Examples:\n\n        1252 /usr/bin/postmaster -D /var/lib/pgsql/data\n        3148 postmaster -D /var/lib/pgsql/data\n        \"\"\"\n        return any(re.search(p, process) for p in self.process_match_patterns)\n\n    def execute_all_queries(self):\n        \"\"\"Executes all queries and writes the output formatted to stdout\"\"\"\n        instance = \"\\n[[[%s]]]\" % self.name\n\n        try:\n            databases = self.get_databases()\n            database_text = \"\\n[databases_start]\\n%s\\n[databases_end]\" % \"\\n\".join(databases)\n            version = self.get_server_version()\n            row, idle = self.get_condition_vars(version)\n        except PostgresPsqlError:\n            # if tcp connection to db instance failed variables are empty\n            databases = \"\"\n            database_text = \"\"\n            version = None\n            row, idle = \"\", \"\"\n\n        out = \"<<<postgres_instances>>>\"\n        out += instance\n        out += \"\\n%s\" % self.get_instances()\n        sys.stdout.write(\"%s\\n\" % out)\n\n        out = \"<<<postgres_sessions>>>\"\n        if row and idle:\n            out += instance\n            out += \"\\n%s\" % self.get_sessions(row, idle)\n            sys.stdout.write(\"%s\\n\" % out)\n\n        out = \"<<<postgres_stat_database:sep(59)>>>\"\n        out += instance\n        out += \"\\n%s\" % self.get_stat_database()\n        sys.stdout.write(\"%s\\n\" % out)\n\n        out = \"<<<postgres_locks:sep(59)>>>\"\n        if database_text:\n            out += instance\n            out += database_text\n            out += \"\\n%s\" % self.get_locks()\n            sys.stdout.write(\"%s\\n\" % out)\n\n        out = \"<<<postgres_query_duration:sep(59)>>>\"\n        if version:\n            out += instance\n            out += database_text\n            out += \"\\n%s\" % self.get_query_duration(version)\n            sys.stdout.write(\"%s\\n\" % out)\n\n        out = \"<<<postgres_connections:sep(59)>>>\"\n        if database_text:\n            out += instance\n            out += database_text\n            out += \"\\n%s\" % self.get_connections()\n            sys.stdout.write(\"%s\\n\" % out)\n\n        out = \"<<<postgres_stats:sep(59)>>>\"\n        if databases:\n            out += instance\n            out += database_text\n            out += \"\\n%s\" % self.get_stats(databases)\n            sys.stdout.write(\"%s\\n\" % out)\n\n        out = \"<<<postgres_version:sep(1)>>>\"\n        out += instance\n        out += \"\\n%s\" % self.get_version()\n        sys.stdout.write(\"%s\\n\" % out)\n\n        out = \"<<<postgres_conn_time>>>\"\n        out += instance\n        out += \"\\n%s\" % self.get_connection_time()\n        sys.stdout.write(\"%s\\n\" % out)\n\n        out = \"<<<postgres_bloat:sep(59)>>>\"\n        if databases and version:\n            out += instance\n            out += database_text\n            out += \"\\n%s\" % self.get_bloat(databases, version)\n            sys.stdout.write(\"%s\\n\" % out)\n\n\ndef _sanitize_sql_query(out):\n    # type: (bytes) -> str\n    utf_8_out = ensure_str(out)\n    # The sql queries may contain any char in `UTF_8_NEWLINE_CHARS`. However,\n    # Checkmk only knows how to handle `\\n`. Furthermore, `\\n` is always\n    # interpreted as a record break by Checkmk (see `parse_dbs`). This means\n    # that we have to remove all newline chars, before printing the section. We\n    # solve the issue in three steps.\n    # - Make Postgres return the NULL byte (instead of newlines). This achieved\n    #   by using the flag `-0`.\n    # - Remove all newlines from whatever Postgres returns. This is safe,\n    #   because of the first step.\n    # - Finally, turn the NULL bytes into linebreaks, so Checkmk interprets\n    #   them as record breaks.\n    utf_8_out_no_new_lines = UTF_8_NEWLINE_CHARS.sub(\" \", utf_8_out)\n    return utf_8_out_no_new_lines.replace(\"\\x00\", \"\\n\").rstrip()\n\n\nclass PostgresWin(PostgresBase):\n    def run_sql_as_db_user(\n        self, sql_cmd, extra_args=\"\", field_sep=\";\", quiet=True, rows_only=True, mixed_cmd=False\n    ):\n        # type: (str, str, str, bool | None, bool | None,bool | None) -> str\n        \"\"\"This method implements the system specific way to call the psql interface\"\"\"\n        extra_args += \" -U %s\" % self.pg_user\n        extra_args += \" -d %s\" % self.pg_database\n        extra_args += \" -p %s\" % self.pg_port\n\n        if quiet:\n            extra_args += \" -q\"\n        if rows_only:\n            extra_args += \" -t\"\n\n        if mixed_cmd:\n            cmd_str = 'cmd /c echo %s | cmd /c \"\"%s\" -X %s -A -0 -F\"%s\" -U %s\"' % (\n                sql_cmd,\n                self.psql_binary_path,\n                extra_args,\n                field_sep,\n                self.db_user,\n            )\n\n        else:\n            cmd_str = 'cmd /c \"\"%s\" -X %s -A -0 -F\"%s\" -U %s -c \"%s\"\" ' % (\n                self.psql_binary_path,\n                extra_args,\n                field_sep,\n                self.db_user,\n                sql_cmd,\n            )\n        proc = subprocess.Popen(\n            cmd_str,\n            env=self.my_env,\n            stdout=subprocess.PIPE,\n        )\n        out = proc.communicate()[0]\n        return _sanitize_sql_query(out)\n\n    @staticmethod\n    def _call_wmic_logicaldisk():\n        # type: () -> str\n        return ensure_str(\n            subprocess_check_output(\n                [\n                    \"wmic\",\n                    \"logicaldisk\",\n                    \"get\",\n                    \"deviceid\",\n                ]\n            )\n        )\n\n    @staticmethod\n    def _parse_wmic_logicaldisk(wmic_output):\n        # type: (str) -> Iterable[str]\n        for drive in wmic_output.replace(\"DeviceID\", \"\").split(\":\")[:-1]:\n            yield drive.strip()\n\n    @classmethod\n    def _logical_drives(cls):\n        # type: () -> Iterable[str]\n        for drive in cls._parse_wmic_logicaldisk(cls._call_wmic_logicaldisk()):\n            yield drive\n\n    def get_psql_binary_path(self):\n        # type: () -> str\n        \"\"\"This method returns the system specific psql interface binary as callable string\"\"\"\n        if self.pg_version is None:\n            # This is a fallback in case the user does not have any instances\n            # configured.\n            return self._default_psql_binary_path()\n        return self._psql_path(self.pg_version)\n\n    def _default_psql_binary_path(self):\n        # type: () -> str\n        for pg_version in self._supported_pg_versions:\n            try:\n                return self._psql_path(pg_version)\n            except IOError as e:\n                ioerr = e\n                continue\n        raise ioerr\n\n    def _psql_path(self, pg_version):\n        # type: (str) -> str\n\n        # TODO: Make this more clever...\n        for drive in self._logical_drives():\n            for program_path in [\n                \"Program Files\\\\PostgreSQL\",\n                \"Program Files (x86)\\\\PostgreSQL\",\n                \"PostgreSQL\",\n            ]:\n                psql_path = (\n                    \"{drive}:\\\\{program_path}\\\\{pg_version}\\\\bin\\\\{psql_binary_name}.exe\".format(\n                        drive=drive,\n                        program_path=program_path,\n                        pg_version=pg_version.split(\".\", 1)[\n                            0\n                        ],  # Only the major version is relevant\n                        psql_binary_name=self.psql_binary_name,\n                    )\n                )\n                if os.path.isfile(psql_path):\n                    return psql_path\n\n        raise IOError(\"Could not determine %s bin and its path.\" % self.psql_binary_name)\n\n    def get_psql_binary_dirname(self):\n        # type: () -> str\n        return self.psql_binary_path.rsplit(\"\\\\\", 1)[0]\n\n    def get_instances(self):\n        # type: () -> str\n        \"\"\"Gets all instances\"\"\"\n\n        taskslist = ensure_str(\n            subprocess_check_output(\n                [\"wmic\", \"process\", \"get\", \"processid,commandline\", \"/format:list\"]\n            )\n        ).split(\"\\r\\r\\n\\r\\r\\n\\r\\r\\n\")\n\n        out = \"\"\n        for task in taskslist:\n            task = task.lstrip().rstrip()\n            if len(task) == 0:\n                continue\n            cmd_line, PID = task.split(\"\\r\\r\\n\")\n            cmd_line = cmd_line.split(\"CommandLine=\")[1]\n            PID = PID.split(\"ProcessId=\")[1]\n            if self.is_postgres_process(cmd_line):\n                if task.find(self.name) != -1:\n                    out += \"%s %s\\n\" % (PID, cmd_line)\n        return out.rstrip()\n\n    def get_stats(self, databases):\n        # type: (list[str]) -> str\n        \"\"\"Get the stats\"\"\"\n        # The next query had to be slightly modified:\n        # As cmd.exe interprets > as redirect and we need <> as \"not equal\", this was changed to\n        # != as it has the same SQL implementation\n        sql_cmd_lastvacuum = (\n            \"SELECT \"\n            \"current_database() AS datname, nspname AS sname, \"\n            \"relname AS tname, CASE WHEN v IS NULL THEN -1 \"\n            \"ELSE round(extract(epoch FROM v)) END AS vtime, \"\n            \"CASE WHEN g IS NULL THEN -1 ELSE round(extract(epoch FROM g)) \"\n            \"END AS atime FROM (SELECT nspname, relname, \"\n            \"GREATEST(pg_stat_get_last_vacuum_time(c.oid), \"\n            \"pg_stat_get_last_autovacuum_time(c.oid)) AS v, \"\n            \"GREATEST(pg_stat_get_last_analyze_time(c.oid), \"\n            \"pg_stat_get_last_autoanalyze_time(c.oid)) AS g \"\n            \"FROM pg_class c, pg_namespace n WHERE relkind = 'r' \"\n            \"AND n.oid = c.relnamespace AND n.nspname != 'information_schema' \"\n            \"ORDER BY 3) AS foo;\"\n        )\n\n        query = \"\\\\pset footer off \\\\\\\\ BEGIN;SET statement_timeout=30000;COMMIT;\"\n\n        cur_rows_only = False\n        for cnt, database in enumerate(databases):\n            query = \"%s \\\\c %s \\\\\\\\ %s\" % (query, database, sql_cmd_lastvacuum)\n            if cnt == 0:\n                query = \"%s \\\\pset tuples_only on\" % query\n\n        return self.run_sql_as_db_user(query, mixed_cmd=True, rows_only=cur_rows_only)\n\n    def get_version_and_connection_time(self):\n        # type: () -> tuple[str, str]\n        \"\"\"Get the pg version and the time for the query connection\"\"\"\n        cmd = \"SELECT version() AS v\"\n\n        # TODO: Verify this time measurement\n        start_time = time.time()\n        out = self.run_sql_as_db_user(cmd)\n        diff = time.time() - start_time\n        return out, \"%.3f\" % diff\n\n    def get_bloat(self, databases, numeric_version):\n        # type: (list[Any], float) -> str\n        \"\"\"Get the db bloats\"\"\"\n        # Bloat index and tables\n        # Supports versions <9.0, >=9.0\n        # This huge query has been gratefully taken from Greg Sabino Mullane's check_postgres.pl\n        if numeric_version > 9.0:\n            # TODO: Reformat query in a more readable way\n            # Here as well: \"<\" and \">\" must be escaped. As we're using meta-command + SQL in one\n            # query, we need to use pipe. Due to Window's cmd behaviour, we need to escape those\n            # symbols with 3 (!) carets. See https://ss64.com/nt/syntax-redirection.html\n            bloat_query = (\n                \"SELECT current_database() AS db, \"\n                \"schemaname, tablename, reltuples::bigint \"\n                \"AS tups, relpages::bigint AS pages, otta, \"\n                \"ROUND(CASE WHEN sml.relpages=0 \"\n                \"OR sml.relpages=otta THEN 0.0 \"\n                \"ELSE (sml.relpages-otta::numeric)/sml.relpages END,3) AS tbloat, \"\n                \"CASE WHEN relpages ^^^< otta THEN 0 \"\n                \"ELSE relpages::bigint - otta END AS wastedpages, \"\n                \"CASE WHEN relpages ^^^< otta THEN 0 ELSE bs*(sml.relpages-otta)::bigint END \"\n                \"AS wastedbytes, CASE WHEN relpages ^^^< otta THEN 0 \"\n                \"ELSE (bs*(relpages-otta))::bigint END \"\n                \"AS wastedsize, iname, ituples::bigint AS itups, ipages::bigint \"\n                \"AS ipages, iotta, ROUND(CASE WHEN ipages=0 OR ipages^^^<=iotta THEN 0.0 \"\n                \"ELSE (ipages-iotta::numeric)/ipages END,3) AS ibloat, \"\n                \"CASE WHEN ipages ^^^< iotta THEN 0 ELSE ipages::bigint - iotta END \"\n                \"AS wastedipages, CASE WHEN ipages ^^^< iotta THEN 0 ELSE bs*(ipages-iotta) \"\n                \"END AS wastedibytes, CASE WHEN ipages ^^^< iotta THEN 0 \"\n                \"ELSE (bs*(ipages-iotta))::bigint END AS wastedisize, \"\n                \"CASE WHEN relpages ^^^< otta THEN CASE WHEN ipages ^^^< iotta THEN 0 \"\n                \"ELSE bs*(ipages-iotta::bigint) END ELSE CASE WHEN ipages ^^^< iotta \"\n                \"THEN bs*(relpages-otta::bigint) \"\n                \"ELSE bs*(relpages-otta::bigint + ipages-iotta::bigint) \"\n                \"END END AS totalwastedbytes \"\n                \"FROM ( SELECT nn.nspname AS schemaname, cc.relname AS tablename, \"\n                \"COALESCE(cc.reltuples,0) AS reltuples, COALESCE(cc.relpages,0) \"\n                \"AS relpages, COALESCE(bs,0) AS bs, \"\n                \"COALESCE(CEIL((cc.reltuples*((datahdr+ma- (CASE WHEN datahdr%ma=0 \"\n                \"THEN ma ELSE datahdr%ma END))+nullhdr2+4))/(bs-20::float)),0) \"\n                \"AS otta, COALESCE(c2.relname,'?') AS iname, COALESCE(c2.reltuples,0) \"\n                \"AS ituples, COALESCE(c2.relpages,0) \"\n                \"AS ipages, COALESCE(CEIL((c2.reltuples*(datahdr-12))/(bs-20::float)),0) \"\n                \"AS iotta FROM pg_class cc \"\n                \"JOIN pg_namespace nn ON cc.relnamespace = nn.oid \"\n                \"AND nn.nspname != 'information_schema' LEFT JOIN \"\n                \"( SELECT ma,bs,foo.nspname,foo.relname, \"\n                \"(datawidth+(hdr+ma-(case when hdr%ma=0 \"\n                \"THEN ma ELSE hdr%ma END)))::numeric AS datahdr, \"\n                \"(maxfracsum*(nullhdr+ma-(case when nullhdr%ma=0 THEN ma \"\n                \"ELSE nullhdr%ma END))) AS nullhdr2 \"\n                \"FROM ( SELECT ns.nspname, tbl.relname, hdr, ma, bs, \"\n                \"SUM((1-coalesce(null_frac,0))*coalesce(avg_width, 2048)) AS datawidth, \"\n                \"MAX(coalesce(null_frac,0)) AS maxfracsum, hdr+( SELECT 1+count(*)/8 \"\n                \"FROM pg_stats s2 WHERE null_frac != 0 AND s2.schemaname = ns.nspname \"\n                \"AND s2.tablename = tbl.relname ) AS nullhdr FROM pg_attribute att \"\n                \"JOIN pg_class tbl ON att.attrelid = tbl.oid JOIN pg_namespace ns \"\n                \"ON ns.oid = tbl.relnamespace LEFT JOIN pg_stats s \"\n                \"ON s.schemaname=ns.nspname AND s.tablename = tbl.relname AND \"\n                \"s.inherited=false AND s.attname=att.attname, \"\n                \"( SELECT (SELECT current_setting('block_size')::numeric) AS bs, CASE WHEN \"\n                \"SUBSTRING(SPLIT_PART(v, ' ', 2) FROM '#\\\\[0-9]+.[0-9]+#\\\\%' for '#') \"\n                \"IN ('8.0','8.1','8.2') THEN 27 ELSE 23 END AS hdr, CASE \"\n                \"WHEN v ~ 'mingw32' OR v ~ '64-bit' THEN 8 ELSE 4 END AS ma \"\n                \"FROM (SELECT version() AS v) AS foo ) AS constants WHERE att.attnum ^^^> 0 \"\n                \"AND tbl.relkind='r' GROUP BY 1,2,3,4,5 ) AS foo ) AS rs \"\n                \"ON cc.relname = rs.relname AND nn.nspname = rs.nspname LEFT \"\n                \"JOIN pg_index i ON indrelid = cc.oid LEFT JOIN pg_class c2 \"\n                \"ON c2.oid = i.indexrelid ) AS sml WHERE sml.relpages - otta ^^^> 0 \"\n                \"OR ipages - iotta ^^^> 10 ORDER BY totalwastedbytes DESC LIMIT 10;\"\n            )\n\n        else:\n            bloat_query = (\n                \"SELECT \"\n                \"current_database() AS db, schemaname, tablename, \"\n                \"reltuples::bigint AS tups, relpages::bigint AS pages, otta, \"\n                \"ROUND(CASE WHEN sml.relpages=0 OR sml.relpages=otta THEN 0.0 \"\n                \"ELSE (sml.relpages-otta::numeric)/sml.relpages END,3) AS tbloat, \"\n                \"CASE WHEN relpages ^^^< otta THEN 0 ELSE relpages::bigint - otta END \"\n                \"AS wastedpages, CASE WHEN relpages ^^^< otta THEN 0 \"\n                \"ELSE bs*(sml.relpages-otta)::bigint END AS wastedbytes, \"\n                \"CASE WHEN relpages ^^^< otta THEN '0 bytes'::text \"\n                \"ELSE (bs*(relpages-otta))::bigint || ' bytes' END AS wastedsize, \"\n                \"iname, ituples::bigint AS itups, ipages::bigint AS ipages, iotta, \"\n                \"ROUND(CASE WHEN ipages=0 OR ipages^^^<=iotta THEN 0.0 ELSE \"\n                \"(ipages-iotta::numeric)/ipages END,3) AS ibloat, \"\n                \"CASE WHEN ipages ^^^< iotta THEN 0 ELSE ipages::bigint - iotta END \"\n                \"AS wastedipages, CASE WHEN ipages ^^^< iotta THEN 0 \"\n                \"ELSE bs*(ipages-iotta) END AS wastedibytes, \"\n                \"CASE WHEN ipages ^^^< iotta THEN '0 bytes' ELSE \"\n                \"(bs*(ipages-iotta))::bigint || ' bytes' END AS wastedisize, CASE \"\n                \"WHEN relpages ^^^< otta THEN CASE WHEN ipages ^^^< iotta THEN 0 \"\n                \"ELSE bs*(ipages-iotta::bigint) END ELSE CASE WHEN ipages ^^^< iotta \"\n                \"THEN bs*(relpages-otta::bigint) \"\n                \"ELSE bs*(relpages-otta::bigint + ipages-iotta::bigint) END \"\n                \"END AS totalwastedbytes FROM (SELECT nn.nspname AS schemaname, \"\n                \"cc.relname AS tablename, COALESCE(cc.reltuples,0) AS reltuples, \"\n                \"COALESCE(cc.relpages,0) AS relpages, COALESCE(bs,0) AS bs, \"\n                \"COALESCE(CEIL((cc.reltuples*((datahdr+ma-(CASE WHEN datahdr%ma=0 \"\n                \"THEN ma ELSE datahdr%ma END))+nullhdr2+4))/(bs-20::float)),0) AS otta, \"\n                \"COALESCE(c2.relname,'?') AS iname, COALESCE(c2.reltuples,0) AS ituples, \"\n                \"COALESCE(c2.relpages,0) AS ipages, \"\n                \"COALESCE(CEIL((c2.reltuples*(datahdr-12))/(bs-20::float)),0) AS iotta \"\n                \"FROM pg_class cc JOIN pg_namespace nn ON cc.relnamespace = nn.oid \"\n                \"AND nn.nspname ^^^<^^^> 'information_schema' LEFT \"\n                \"JOIN(SELECT ma,bs,foo.nspname,foo.relname, \"\n                \"(datawidth+(hdr+ma-(case when hdr%ma=0 THEN ma \"\n                \"ELSE hdr%ma END)))::numeric AS datahdr, \"\n                \"(maxfracsum*(nullhdr+ma-(case when nullhdr%ma=0 THEN ma \"\n                \"ELSE nullhdr%ma END))) AS nullhdr2 \"\n                \"FROM (SELECT ns.nspname, tbl.relname, hdr, ma, bs, \"\n                \"SUM((1-coalesce(null_frac,0))*coalesce(avg_width, 2048)) \"\n                \"AS datawidth, MAX(coalesce(null_frac,0)) AS maxfracsum, hdr+(\"\n                \"SELECT 1+count(*)/8 FROM pg_stats s2 WHERE null_frac^^^<^^^>0 \"\n                \"AND s2.schemaname = ns.nspname AND s2.tablename = tbl.relname) \"\n                \"AS nullhdr FROM pg_attribute att JOIN pg_class tbl \"\n                \"ON att.attrelid = tbl.oid JOIN pg_namespace ns ON \"\n                \"ns.oid = tbl.relnamespace LEFT JOIN pg_stats s \"\n                \"ON s.schemaname=ns.nspname AND s.tablename = tbl.relname \"\n                \"AND s.attname=att.attname, (SELECT (\"\n                \"SELECT current_setting('block_size')::numeric) AS bs, CASE WHEN \"\n                \"SUBSTRING(SPLIT_PART(v, ' ', 2) FROM '#\\\"[0-9]+.[0-9]+#\\\"%' for '#') \"\n                \"IN ('8.0','8.1','8.2') THEN 27 ELSE 23 END AS hdr, CASE \"\n                \"WHEN v ~ 'mingw32' OR v ~ '64-bit' THEN 8 ELSE 4 END AS ma \"\n                \"FROM (SELECT version() AS v) AS foo) AS constants WHERE att.attnum ^^^> 0 \"\n                \"AND tbl.relkind='r' GROUP BY 1,2,3,4,5) AS foo) AS rs ON \"\n                \"cc.relname = rs.relname AND nn.nspname = rs.nspname LEFT JOIN pg_index i \"\n                \"ON indrelid = cc.oid LEFT JOIN pg_class c2 ON c2.oid = i.indexrelid) \"\n                \"AS sml WHERE sml.relpages - otta ^^^> 0 OR ipages - iotta ^^^> 10 ORDER \"\n                \"BY totalwastedbytes DESC LIMIT 10;\"\n            )\n\n        cur_rows_only = False\n        output = \"\"\n        for idx, database in enumerate(databases):\n            query = \"\\\\pset footer off \\\\\\\\ \\\\c %s \\\\\\\\ %s\" % (database, bloat_query)\n            if idx == 0:\n                query = \"%s \\\\pset tuples_only on\" % query\n            output += self.run_sql_as_db_user(query, mixed_cmd=True, rows_only=cur_rows_only)\n            cur_rows_only = True\n        return output\n\n\nclass PostgresLinux(PostgresBase):\n    def _run_sql_as_db_user(\n        self, sql_file_path, extra_args=\"\", field_sep=\";\", quiet=True, rows_only=True\n    ):\n        # type: (str, str, str, bool, bool) -> str\n        base_cmd_list = [\n            \"su\",\n            \"-\",\n            self.db_user,\n            \"-c\",\n            r\"\"\"PGPASSFILE=%s %s -X %s -A0 -F'%s' -f %s\"\"\",\n        ]\n        extra_args += \" -U %s\" % self.pg_user\n        extra_args += \" -d %s\" % self.pg_database\n        extra_args += \" -p %s\" % self.pg_port\n\n        if quiet:\n            extra_args += \" -q\"\n        if rows_only:\n            extra_args += \" -t\"\n\n        base_cmd_list[-1] = base_cmd_list[-1] % (\n            self.pg_passfile,\n            self.psql_binary_path,\n            extra_args,\n            field_sep,\n            sql_file_path,\n        )\n        proc = subprocess.Popen(base_cmd_list, env=self.my_env, stdout=subprocess.PIPE)\n        return _sanitize_sql_query(proc.communicate()[0])\n\n    def run_sql_as_db_user(\n        self, sql_cmd, extra_args=\"\", field_sep=\";\", quiet=True, rows_only=True, mixed_cmd=False\n    ):\n        # type: (str, str, str, bool, bool, bool) -> str\n        with tempfile.NamedTemporaryFile(delete=True) as tmp:\n            tmp.write(sql_cmd.encode(\"utf-8\"))\n            # set cursor to the beginning of the file\n            tmp.seek(0)\n            # We use 'psql ... -f <FILE_PATH>', the tmp file has to be readable to all users,\n            # ie. stat.S_IROTH\n            os.chmod(tmp.name, stat.S_IROTH)\n            return self._run_sql_as_db_user(\n                tmp.name,\n                extra_args=extra_args,\n                field_sep=field_sep,\n                quiet=quiet,\n                rows_only=rows_only,\n            )\n\n    def get_psql_binary_path(self):\n        # type: () -> str\n        \"\"\"If possible, do not use the binary from PATH directly. This could lead to a generic\n        binary that is not able to find the correct UNIX socket. See SUP-11729.\n        In case the user does not have any instances configured or if the assembled path does not\n        exist, fallback to the PATH location. See SUP-12878\"\"\"\n\n        if self.pg_version is None:\n            return self._default_psql_binary_path()\n\n        binary_path = \"/{pg_database}/{pg_version}/bin/{psql_binary_name}\".format(\n            pg_database=self.pg_database,\n            pg_version=self.pg_version,\n            psql_binary_name=self.psql_binary_name,\n        )\n\n        if not os.path.isfile(binary_path):\n            return self._default_psql_binary_path()\n        return binary_path\n\n    def _default_psql_binary_path(self):\n        # type: () -> str\n        proc = subprocess.Popen([\"which\", self.psql_binary_name], stdout=subprocess.PIPE)\n        out = ensure_str(proc.communicate()[0])\n\n        if proc.returncode != 0:\n            raise RuntimeError(\"Could not determine %s executable.\" % self.psql_binary_name)\n\n        return out.strip()\n\n    def get_psql_binary_dirname(self):\n        # type: () -> str\n        return self.psql_binary_path.rsplit(\"/\", 1)[0]\n\n    def _matches_main(self, proc):\n        # type: (str) -> bool\n        # the data directory for the instance \"main\" is not called \"main\" but \"data\" on some\n        # platforms\n        return self.name == \"main\" and \"data\" in proc\n\n    def _filter_instances(self, procs_list, proc_sensitive_filter):\n        # type: (list[str], Callable[[str], bool]) -> list[str]\n        return [\n            proc\n            for proc in procs_list\n            if self.is_postgres_process(proc)\n            and (proc_sensitive_filter(proc) or self._matches_main(proc))\n        ]\n\n    def get_instances(self):\n        # type: () -> str\n\n        procs_list = ensure_str(\n            subprocess_check_output([\"ps\", \"h\", \"-eo\", \"pid:1,command:1\"])\n        ).split(\"\\n\")\n\n        # trying to address setups in SUP-12878 (instance \"A01\" -> process \"A01\") and SUP-12539\n        # (instance \"epcomt\" -> process \"EPCOMT\") as well as possible future setups (containing\n        # instances where the names only differ in case (e.g. \"instance\" and \"INSTANCE\"))\n        procs = self._filter_instances(procs_list, proc_sensitive_filter=lambda p: self.name in p)\n        if not procs:\n            procs = self._filter_instances(\n                procs_list, proc_sensitive_filter=lambda p: self.name.lower() in p.lower()\n            )\n        out = \"\\n\".join(procs)\n        return out.rstrip()\n\n    def get_query_duration(self, numeric_version):\n        # type: (float) -> str\n        # Previously part of simple_queries\n\n        if numeric_version > 9.2:\n            querytime_sql_cmd = (\n                \"SELECT datname, datid, usename, client_addr, state AS state, \"\n                \"COALESCE(ROUND(EXTRACT(epoch FROM now()-query_start)),0) \"\n                \"AS seconds, pid, \"\n                \"query AS current_query FROM pg_stat_activity \"\n                \"WHERE (query_start IS NOT NULL AND \"\n                \"(state NOT LIKE 'idle%' OR state IS NULL)) \"\n                \"ORDER BY query_start, pid DESC;\"\n            )\n\n        else:\n            querytime_sql_cmd = (\n                \"SELECT datname, datid, usename, client_addr, '' AS state, \"\n                \"COALESCE(ROUND(EXTRACT(epoch FROM now()-query_start)),0) \"\n                \"AS seconds, procpid as pid, \"\n                \"query \"\n                \"AS current_query FROM pg_stat_activity WHERE \"\n                \"(query_start IS NOT NULL AND current_query NOT LIKE '<IDLE>%') \"\n                \"ORDER BY query_start, procpid DESC;\"\n            )\n\n        return self.run_sql_as_db_user(\n            querytime_sql_cmd, rows_only=False, extra_args=\"-P footer=off\"\n        )\n\n    def get_stats(self, databases):\n        # type: (list[str]) -> str\n        sql_cmd_lastvacuum = (\n            \"SELECT \"\n            \"current_database() AS datname, nspname AS sname, \"\n            \"relname AS tname, CASE WHEN v IS NULL THEN -1 \"\n            \"ELSE round(extract(epoch FROM v)) END AS vtime, \"\n            \"CASE WHEN g IS NULL THEN -1 ELSE round(extract(epoch FROM g)) \"\n            \"END AS atime FROM (SELECT nspname, relname, \"\n            \"GREATEST(pg_stat_get_last_vacuum_time(c.oid), \"\n            \"pg_stat_get_last_autovacuum_time(c.oid)) AS v, \"\n            \"GREATEST(pg_stat_get_last_analyze_time(c.oid), \"\n            \"pg_stat_get_last_autoanalyze_time(c.oid)) AS g \"\n            \"FROM pg_class c, pg_namespace n WHERE relkind = 'r' \"\n            \"AND n.oid = c.relnamespace AND n.nspname <> 'information_schema' \"\n            \"ORDER BY 3) AS foo;\"\n        )\n\n        query = \"\\\\pset footer off\\nBEGIN;\\nSET statement_timeout=30000;\\nCOMMIT;\"\n\n        cur_rows_only = False\n        for cnt, database in enumerate(databases):\n            query = \"%s\\n\\\\c %s\\n%s\" % (query, database, sql_cmd_lastvacuum)\n            if cnt == 0:\n                query = \"%s\\n\\\\pset tuples_only on\" % query\n\n        return self.run_sql_as_db_user(query, mixed_cmd=True, rows_only=cur_rows_only)\n\n    def get_version_and_connection_time(self):\n        # type: () -> tuple[str, str]\n        cmd = \"SELECT version() AS v\"\n        usage_start = resource.getrusage(resource.RUSAGE_CHILDREN)\n        out = self.run_sql_as_db_user(cmd)\n        usage_end = resource.getrusage(resource.RUSAGE_CHILDREN)\n\n        sys_time = usage_end.ru_stime - usage_start.ru_stime\n        usr_time = usage_end.ru_utime - usage_start.ru_utime\n        real = sys_time + usr_time\n\n        return out, \"%.3f\" % real\n\n    def get_bloat(self, databases, numeric_version):\n        # type: (list[Any], float) -> str\n        # Bloat index and tables\n        # Supports versions <9.0, >=9.0\n        # This huge query has been gratefully taken from Greg Sabino Mullane's check_postgres.pl\n        if numeric_version > 9.0:\n            # TODO: Reformat query in a more readable way\n            bloat_query = (\n                \"SELECT current_database() AS db, schemaname, tablename, reltuples::bigint \"\n                \"AS tups, relpages::bigint AS pages, otta, ROUND(CASE WHEN sml.relpages=0 \"\n                \"OR sml.relpages=otta THEN 0.0 \"\n                \"ELSE (sml.relpages-otta::numeric)/sml.relpages END,3) AS tbloat, \"\n                \"CASE WHEN relpages < otta THEN 0 \"\n                \"ELSE relpages::bigint - otta END AS wastedpages, \"\n                \"CASE WHEN relpages < otta THEN 0 ELSE bs*(sml.relpages-otta)::bigint END \"\n                \"AS wastedbytes, CASE WHEN relpages < otta THEN 0 \"\n                \"ELSE (bs*(relpages-otta))::bigint END \"\n                \"AS wastedsize, iname, ituples::bigint AS itups, ipages::bigint \"\n                \"AS ipages, iotta, ROUND(CASE WHEN ipages=0 OR ipages<=iotta THEN 0.0 \"\n                \"ELSE (ipages-iotta::numeric)/ipages END,3) AS ibloat, \"\n                \"CASE WHEN ipages < iotta THEN 0 ELSE ipages::bigint - iotta END \"\n                \"AS wastedipages, CASE WHEN ipages < iotta THEN 0 ELSE bs*(ipages-iotta) \"\n                \"END AS wastedibytes, CASE WHEN ipages < iotta THEN 0 \"\n                \"ELSE (bs*(ipages-iotta))::bigint END AS wastedisize, \"\n                \"CASE WHEN relpages < otta THEN CASE WHEN ipages < iotta THEN 0 \"\n                \"ELSE bs*(ipages-iotta::bigint) END ELSE CASE WHEN ipages < iotta \"\n                \"THEN bs*(relpages-otta::bigint) \"\n                \"ELSE bs*(relpages-otta::bigint + ipages-iotta::bigint) \"\n                \"END END AS totalwastedbytes \"\n                \"FROM ( SELECT nn.nspname AS schemaname, cc.relname AS tablename, \"\n                \"COALESCE(cc.reltuples,0) AS reltuples, COALESCE(cc.relpages,0) \"\n                \"AS relpages, COALESCE(bs,0) AS bs, \"\n                \"COALESCE(CEIL((cc.reltuples*((datahdr+ma- (CASE WHEN datahdr%ma=0 \"\n                \"THEN ma ELSE datahdr%ma END))+nullhdr2+4))/(bs-20::float)),0) \"\n                \"AS otta, COALESCE(c2.relname,'?') AS iname, COALESCE(c2.reltuples,0) \"\n                \"AS ituples, COALESCE(c2.relpages,0) \"\n                \"AS ipages, COALESCE(CEIL((c2.reltuples*(datahdr-12))/(bs-20::float)),0) \"\n                \"AS iotta FROM pg_class cc \"\n                \"JOIN pg_namespace nn ON cc.relnamespace = nn.oid \"\n                \"AND nn.nspname <> 'information_schema' LEFT JOIN \"\n                \"( SELECT ma,bs,foo.nspname,foo.relname, \"\n                \"(datawidth+(hdr+ma-(case when hdr%ma=0 \"\n                \"THEN ma ELSE hdr%ma END)))::numeric AS datahdr, \"\n                \"(maxfracsum*(nullhdr+ma-(case when nullhdr%ma=0 THEN ma \"\n                \"ELSE nullhdr%ma END))) AS nullhdr2 \"\n                \"FROM ( SELECT ns.nspname, tbl.relname, hdr, ma, bs, \"\n                \"SUM((1-coalesce(null_frac,0))*coalesce(avg_width, 2048)) AS datawidth, \"\n                \"MAX(coalesce(null_frac,0)) AS maxfracsum, hdr+( SELECT 1+count(*)/8 \"\n                \"FROM pg_stats s2 WHERE null_frac<>0 AND s2.schemaname = ns.nspname \"\n                \"AND s2.tablename = tbl.relname ) AS nullhdr FROM pg_attribute att \"\n                \"JOIN pg_class tbl ON att.attrelid = tbl.oid JOIN pg_namespace ns \"\n                \"ON ns.oid = tbl.relnamespace LEFT JOIN pg_stats s \"\n                \"ON s.schemaname=ns.nspname AND s.tablename = tbl.relname AND \"\n                \"s.inherited=false AND s.attname=att.attname, \"\n                \"( SELECT (SELECT current_setting('block_size')::numeric) AS bs, CASE WHEN \"\n                \"SUBSTRING(SPLIT_PART(v, ' ', 2) FROM '#\\\\[0-9]+.[0-9]+#\\\\%' for '#') \"\n                \"IN ('8.0','8.1','8.2') THEN 27 ELSE 23 END AS hdr, CASE \"\n                \"WHEN v ~ 'mingw32' OR v ~ '64-bit' THEN 8 ELSE 4 END AS ma \"\n                \"FROM (SELECT version() AS v) AS foo ) AS constants WHERE att.attnum > 0 \"\n                \"AND tbl.relkind='r' GROUP BY 1,2,3,4,5 ) AS foo ) AS rs \"\n                \"ON cc.relname = rs.relname AND nn.nspname = rs.nspname LEFT JOIN pg_index i \"\n                \"ON indrelid = cc.oid LEFT JOIN pg_class c2 ON c2.oid = i.indexrelid ) \"\n                \"AS sml WHERE sml.relpages - otta > 0 OR ipages - iotta > 10 ORDER \"\n                \"BY totalwastedbytes DESC LIMIT 10;\"\n            )\n        else:\n            bloat_query = (\n                \"SELECT \"\n                \"current_database() AS db, schemaname, tablename, \"\n                \"reltuples::bigint AS tups, relpages::bigint AS pages, otta, \"\n                \"ROUND(CASE WHEN sml.relpages=0 OR sml.relpages=otta THEN 0.0 \"\n                \"ELSE (sml.relpages-otta::numeric)/sml.relpages END,3) AS tbloat, \"\n                \"CASE WHEN relpages < otta THEN 0 ELSE relpages::bigint - otta END \"\n                \"AS wastedpages, CASE WHEN relpages < otta THEN 0 \"\n                \"ELSE bs*(sml.relpages-otta)::bigint END AS wastedbytes, \"\n                \"CASE WHEN relpages < otta THEN '0 bytes'::text \"\n                \"ELSE (bs*(relpages-otta))::bigint || ' bytes' END AS wastedsize, \"\n                \"iname, ituples::bigint AS itups, ipages::bigint AS ipages, iotta, \"\n                \"ROUND(CASE WHEN ipages=0 OR ipages<=iotta THEN 0.0 ELSE \"\n                \"(ipages-iotta::numeric)/ipages END,3) AS ibloat, \"\n                \"CASE WHEN ipages < iotta THEN 0 ELSE ipages::bigint - iotta END \"\n                \"AS wastedipages, CASE WHEN ipages < iotta THEN 0 \"\n                \"ELSE bs*(ipages-iotta) END AS wastedibytes, \"\n                \"CASE WHEN ipages < iotta THEN '0 bytes' ELSE \"\n                \"(bs*(ipages-iotta))::bigint || ' bytes' END AS wastedisize, CASE \"\n                \"WHEN relpages < otta THEN CASE WHEN ipages < iotta THEN 0 \"\n                \"ELSE bs*(ipages-iotta::bigint) END ELSE CASE WHEN ipages < iotta \"\n                \"THEN bs*(relpages-otta::bigint) \"\n                \"ELSE bs*(relpages-otta::bigint + ipages-iotta::bigint) END \"\n                \"END AS totalwastedbytes FROM (SELECT nn.nspname AS schemaname, \"\n                \"cc.relname AS tablename, COALESCE(cc.reltuples,0) AS reltuples, \"\n                \"COALESCE(cc.relpages,0) AS relpages, COALESCE(bs,0) AS bs, \"\n                \"COALESCE(CEIL((cc.reltuples*((datahdr+ma-(CASE WHEN datahdr%ma=0 \"\n                \"THEN ma ELSE datahdr%ma END))+nullhdr2+4))/(bs-20::float)),0) AS otta, \"\n                \"COALESCE(c2.relname,'?') AS iname, COALESCE(c2.reltuples,0) AS ituples, \"\n                \"COALESCE(c2.relpages,0) AS ipages, \"\n                \"COALESCE(CEIL((c2.reltuples*(datahdr-12))/(bs-20::float)),0) AS iotta \"\n                \"FROM pg_class cc JOIN pg_namespace nn ON cc.relnamespace = nn.oid \"\n                \"AND nn.nspname <> 'information_schema' LEFT \"\n                \"JOIN(SELECT ma,bs,foo.nspname,foo.relname, \"\n                \"(datawidth+(hdr+ma-(case when hdr%ma=0 THEN ma \"\n                \"ELSE hdr%ma END)))::numeric AS datahdr, \"\n                \"(maxfracsum*(nullhdr+ma-(case when nullhdr%ma=0 THEN ma \"\n                \"ELSE nullhdr%ma END))) AS nullhdr2 \"\n                \"FROM (SELECT ns.nspname, tbl.relname, hdr, ma, bs, \"\n                \"SUM((1-coalesce(null_frac,0))*coalesce(avg_width, 2048)) \"\n                \"AS datawidth, MAX(coalesce(null_frac,0)) AS maxfracsum, hdr+(\"\n                \"SELECT 1+count(*)/8 FROM pg_stats s2 WHERE null_frac<>0 \"\n                \"AND s2.schemaname = ns.nspname AND s2.tablename = tbl.relname) \"\n                \"AS nullhdr FROM pg_attribute att JOIN pg_class tbl \"\n                \"ON att.attrelid = tbl.oid JOIN pg_namespace ns ON \"\n                \"ns.oid = tbl.relnamespace LEFT JOIN pg_stats s \"\n                \"ON s.schemaname=ns.nspname AND s.tablename = tbl.relname \"\n                \"AND s.attname=att.attname, (SELECT (\"\n                \"SELECT current_setting('block_size')::numeric) AS bs, CASE WHEN \"\n                \"SUBSTRING(SPLIT_PART(v, ' ', 2) FROM '#\\\"[0-9]+.[0-9]+#\\\"%' for '#') \"\n                \"IN ('8.0','8.1','8.2') THEN 27 ELSE 23 END AS hdr, CASE \"\n                \"WHEN v ~ 'mingw32' OR v ~ '64-bit' THEN 8 ELSE 4 END AS ma \"\n                \"FROM (SELECT version() AS v) AS foo) AS constants WHERE att.attnum > 0 \"\n                \"AND tbl.relkind='r' GROUP BY 1,2,3,4,5) AS foo) AS rs ON \"\n                \"cc.relname = rs.relname AND nn.nspname = rs.nspname LEFT JOIN pg_index i \"\n                \"ON indrelid = cc.oid LEFT JOIN pg_class c2 ON c2.oid = i.indexrelid) \"\n                \"AS sml WHERE sml.relpages - otta > 0 OR ipages - iotta > 10 ORDER \"\n                \"BY totalwastedbytes DESC LIMIT 10;\"\n            )\n\n        query = \"\\\\pset footer off\"\n\n        cur_rows_only = False\n        for idx, database in enumerate(databases):\n            query = \"%s\\n\\\\c %s\\n%s\" % (query, database, bloat_query)\n            if idx == 0:\n                query = \"%s\\n\\\\pset tuples_only on\" % query\n\n        return self.run_sql_as_db_user(query, mixed_cmd=True, rows_only=cur_rows_only)\n\n\ndef postgres_factory(db_user, pg_binary_path, pg_instance):\n    # type: (str, str | None, dict[str, str | None]) -> PostgresBase\n    if IS_LINUX:\n        return PostgresLinux(db_user, pg_binary_path, pg_instance, LINUX_PROCESS_MATCH_PATTERNS)\n    if IS_WINDOWS:\n        return PostgresWin(db_user, pg_binary_path, pg_instance, WINDOWS_PROCESS_MATCH_PATTERNS)\n    raise OSNotImplementedError\n\n\ndef helper_factory():\n    # type: () -> Helpers\n    if IS_LINUX:\n        return LinuxHelpers()\n    if IS_WINDOWS:\n        return WindowsHelpers()\n    raise OSNotImplementedError\n\n\nclass Helpers:\n    \"\"\"\n    Base class for x-plattform postgres helper functions\n\n    All abstract methods must have individual implementation depending on the OS type\n    which runs postgres.\n    All non-abstract methods are meant to work on all OS types which were subclassed.\n    \"\"\"\n\n    __metaclass__ = abc.ABCMeta\n\n    @staticmethod\n    @abc.abstractmethod\n    def get_default_postgres_user():\n        pass\n\n    @staticmethod\n    @abc.abstractmethod\n    def get_default_path():\n        pass\n\n    @staticmethod\n    @abc.abstractmethod\n    def get_conf_sep():\n        pass\n\n    @staticmethod\n    @abc.abstractmethod\n    def get_default_db_name():\n        pass\n\n\nclass WindowsHelpers(Helpers):\n    @staticmethod\n    def get_default_postgres_user():\n        return \"postgres\"\n\n    @staticmethod\n    def get_default_path():\n        return \"c:\\\\ProgramData\\\\checkmk\\\\agent\\\\config\"\n\n    @staticmethod\n    def get_conf_sep():\n        return \"|\"\n\n    @staticmethod\n    def get_default_db_name():\n        return \"data\"\n\n\nclass LinuxHelpers(Helpers):\n    @staticmethod\n    def get_default_postgres_user():\n        for user_id in (\"pgsql\", \"postgres\"):\n            try:\n                proc = subprocess.Popen(\n                    [\"id\", user_id], stdout=subprocess.PIPE, stderr=subprocess.PIPE\n                )\n                proc.communicate()\n                if proc.returncode == 0:\n                    return user_id.rstrip()\n            except subprocess.CalledProcessError:\n                pass\n        LOGGER.warning('Could not determine postgres user, using \"postgres\" as default')\n        return \"postgres\"\n\n    @staticmethod\n    def get_default_path():\n        return \"/etc/check_mk\"\n\n    @staticmethod\n    def get_conf_sep():\n        return \":\"\n\n    @staticmethod\n    def get_default_db_name():\n        return \"main\"\n\n\ndef open_env_file(file_to_open):\n    \"\"\"Wrapper around built-in open to be able to monkeypatch through all python versions\"\"\"\n    return open(file_to_open).readlines()\n\n\ndef parse_env_file(env_file):\n    # type: (str) -> tuple[str, str, str | None]\n    pg_port = None  # mandatory in env_file\n    pg_database = \"postgres\"  # default value\n    pg_version = None\n\n    for line in open_env_file(env_file):\n        line = line.strip()\n        if not line or \"=\" not in line or line.startswith(\"#\"):\n            continue\n        if \"PGDATABASE=\" in line:\n            pg_database = re.sub(re.compile(\"#.*\"), \"\", line.split(\"=\")[-1]).strip()\n        elif \"PGPORT=\" in line:\n            pg_port = re.sub(re.compile(\"#.*\"), \"\", line.split(\"=\")[-1]).strip()\n        elif \"PGVERSION=\" in line:\n            pg_version = re.sub(re.compile(\"#.*\"), \"\", line.split(\"=\")[-1]).strip()\n\n    if pg_port is None:\n        raise ValueError(\"PGPORT is not specified in %s\" % env_file)\n    return pg_database, pg_port, pg_version\n\n\ndef _parse_INSTANCE_value(value, config_separator):\n    # type: (str, str) -> tuple[str, str, str, str]\n    keys = value.split(config_separator)\n    if len(keys) == 3:\n        # Old format (deprecated in Werk 16016), but we don't force updates unless there is\n        # a substantial benefit.\n        keys = keys + [\"\"]\n    env_file, pg_user, pg_passfile, instance_name = keys\n    env_file = env_file.strip()\n    return env_file, pg_user, pg_passfile, instance_name or env_file.split(os.sep)[-1].split(\".\")[0]\n\n\ndef parse_postgres_cfg(postgres_cfg, config_separator):\n    # type: (list[str], str) -> tuple[str, str | None, list[dict[str, str | None]]]\n    \"\"\"\n    Parser for Postgres config. x-Plattform compatible.\n    See comment at the beginning of this file for an example.\n    \"\"\"\n    dbuser = None\n    pg_binary_path = None\n    instances = []\n    for line in postgres_cfg:\n        if line.startswith(\"#\") or \"=\" not in line:\n            continue\n        line = line.strip()\n        key, value = line.split(\"=\")\n        if key == \"DBUSER\":\n            dbuser = value.rstrip()\n        if key == \"PG_BINARY_PATH\":\n            pg_binary_path = value.rstrip()\n        if key == \"INSTANCE\":\n            env_file, pg_user, pg_passfile, instance_name = _parse_INSTANCE_value(\n                value, config_separator\n            )\n            pg_database, pg_port, pg_version = parse_env_file(env_file)\n            instances.append(\n                {\n                    \"name\": instance_name.strip(),\n                    \"pg_user\": pg_user.strip(),\n                    \"pg_passfile\": pg_passfile.strip(),\n                    \"pg_database\": pg_database,\n                    \"pg_port\": pg_port,\n                    \"pg_version\": pg_version,\n                }\n            )\n    if dbuser is None:\n        raise ValueError(\"DBUSER must be specified in postgres.cfg\")\n    return dbuser, pg_binary_path, instances\n\n\ndef parse_arguments(argv):\n    parser = optparse.OptionParser()\n    parser.add_option(\"-v\", \"--verbose\", action=\"count\", default=0)\n    parser.add_option(\n        \"-t\",\n        \"--test-connection\",\n        default=False,\n        action=\"store_true\",\n        help=\"Test if postgres is ready\",\n    )\n    options, _ = parser.parse_args(argv)\n    return options\n\n\ndef main(argv=None):\n    # type: (list | None) -> int\n\n    helper = helper_factory()\n    if argv is None:\n        argv = sys.argv[1:]\n\n    opt = parse_arguments(argv)\n\n    logging.basicConfig(\n        format=\"%(levelname)s %(asctime)s %(name)s: %(message)s\",\n        datefmt=\"%Y-%m-%d %H:%M:%S\",\n        level={0: logging.WARN, 1: logging.INFO, 2: logging.DEBUG}.get(opt.verbose, logging.DEBUG),\n    )\n\n    instances = []  # type: list[dict[str, str | None]]\n    try:\n        postgres_cfg_path = os.path.join(\n            os.getenv(\"MK_CONFDIR\", helper.get_default_path()), \"postgres.cfg\"\n        )\n        with open(postgres_cfg_path) as opened_file:\n            postgres_cfg = opened_file.readlines()\n        postgres_cfg = [ensure_str(el) for el in postgres_cfg]\n        dbuser, pg_binary_path, instances = parse_postgres_cfg(postgres_cfg, helper.get_conf_sep())\n    except Exception:\n        _, e = sys.exc_info()[:2]  # python2 and python3 compatible exception logging\n        dbuser = helper.get_default_postgres_user()\n        pg_binary_path = None\n        LOGGER.debug(\"try_parse_config: exception: %s\", str(e))\n        LOGGER.debug('Using \"%s\" as default postgres user.', dbuser)\n\n    if not instances:\n        default_postgres_installation_parameters = {\n            # default database name of postgres installation\n            \"name\": helper.get_default_db_name(),\n            \"pg_user\": \"postgres\",\n            \"pg_database\": \"postgres\",\n            \"pg_port\": \"5432\",\n            # Assumption: if no pg_passfile is specified no password will be required.\n            # If a password is required but no pg_passfile is specified the process will\n            # interactivly prompt for a password.\n            \"pg_passfile\": \"\",\n        }\n        instances.append(default_postgres_installation_parameters)\n\n    for instance in instances:\n        postgres = postgres_factory(dbuser, pg_binary_path, instance)\n        if opt.test_connection:\n            postgres.is_pg_ready()\n            sys.exit(0)\n        postgres.execute_all_queries()\n    return 0\n\n\nif __name__ == \"__main__\":\n    main()\n"}
{"type": "source_file", "path": "agents/plugins/mtr.py", "content": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n# Copyright (C) 2019 Checkmk GmbH - License: GNU General Public License v2\n# This file is part of Checkmk (https://checkmk.com). It is subject to the terms and\n# conditions defined in the file COPYING, which is part of this source code package.\n\n__version__ = \"2.5.0b1\"\n\n# This plugin was sponsored by BenV. Thanks!\n# https://notes.benv.junerules.com/mtr/\n\n# Concept:\n# Read config mtr.cfg\n# For every host:\n# parse outstanding reports (and delete them)\n# If current time > last check + config(time)//300 start new mtr in background\n#    MTR results are stored in $VARDIR/mtr_${host}.report\n# return previous host data\n\ntry:\n    import configparser\nexcept ImportError:  # Python 2\n    import ConfigParser as configparser  # type: ignore[import-not-found,no-redef]\n\nimport glob\nimport os\nimport re\nimport subprocess\nimport sys\nimport time\n\ntry:\n    from typing import Any  # noqa: F401\nexcept ImportError:\n    pass\n\nmk_confdir = os.getenv(\"MK_CONFDIR\") or \"/etc/check_mk\"\nmk_vardir = os.getenv(\"MK_VARDIR\") or \"/var/lib/check_mk_agent\"\n\nconfig_filename = mk_confdir + \"/mtr.cfg\"\nconfig_dir = mk_confdir + \"/mtr.d/*.cfg\"\nstatus_filename = mk_vardir + \"/mtr.state\"\nreport_filepre = mk_vardir + \"/mtr.report.\"\n\ndebug = \"-d\" in sys.argv[2:] or \"--debug\" in sys.argv[1:]\n\n\ndef ensure_str(s):\n    if sys.version_info[0] >= 3:\n        if isinstance(s, bytes):\n            return s.decode(\"utf-8\")\n    elif isinstance(s, unicode):  # noqa: F821\n        return s.encode(\"utf-8\")\n    return s\n\n\ndef read_config():\n    default_options = {\n        \"type\": \"icmp\",\n        \"count\": \"10\",\n        \"force_ipv4\": \"0\",\n        \"force_ipv6\": \"0\",\n        \"size\": \"64\",\n        \"time\": \"0\",\n        \"dns\": \"0\",\n        \"port\": \"\",\n        \"address\": \"\",\n        \"interval\": \"\",\n        \"timeout\": \"\",\n    }\n    if not os.path.exists(config_filename):\n        if debug:\n            sys.stdout.write(\"Not configured, %s missing\\n\" % config_filename)\n        sys.exit(0)\n\n    cfg = configparser.ConfigParser(default_options)\n    # Let ConfigParser figure it out\n    for config_file in [config_filename] + glob.glob(config_dir):\n        try:\n            if not cfg.read(config_file):\n                sys.stdout.write(\"**ERROR** Failed to parse configuration file %s!\\n\" % config_file)\n        except Exception as e:\n            sys.stdout.write(\n                \"**ERROR** Failed to parse config file %s: %s\\n\" % (config_file, repr(e))\n            )\n\n    if len(cfg.sections()) == 0:\n        sys.stdout.write(\"**ERROR** Configuration defines no hosts!\\n\")\n        sys.exit(0)\n\n    return cfg\n\n\n# structure of statusfile\n# # HOST        |LASTTIME |HOPCOUNT|HOP1|Loss%|Snt|Last|Avg|Best|Wrst|StDev|HOP2|...|HOP8|...|StdDev\n# www.google.com|145122481|8|192.168.1.1|0.0%|10|32.6|3.6|0.3|32.6|10.2|192.168.0.1|...|9.8\ndef read_status():\n    current_status = {}  # type: dict[str, dict[str, Any]]\n    if not os.path.exists(status_filename):\n        return current_status\n    with open(status_filename) as opened_file:\n        for line in opened_file:\n            try:\n                parts = line.split(\"|\")\n                if len(parts) < 2:\n                    sys.stdout.write(\"**ERROR** (BUG) Status has less than 2 parts:\\n\")\n                    sys.stdout.write(\"%s\\n\" % parts)\n                    continue\n                host = parts[0]\n                lasttime = int(parts[1])\n                current_status[host] = {\"hops\": {}, \"lasttime\": lasttime}\n                hops = int(parts[2])\n                for i in range(0, hops):\n                    current_status[host][\"hops\"][i + 1] = {\n                        \"hopname\": parts[i * 8 + 3].rstrip(),\n                        \"loss\": parts[i * 8 + 4].rstrip(),\n                        \"snt\": parts[i * 8 + 5].rstrip(),\n                        \"last\": parts[i * 8 + 6].rstrip(),\n                        \"avg\": parts[i * 8 + 7].rstrip(),\n                        \"best\": parts[i * 8 + 8].rstrip(),\n                        \"wrst\": parts[i * 8 + 9].rstrip(),\n                        \"stddev\": parts[i * 8 + 10].rstrip(),\n                    }\n            except Exception as e:\n                sys.stdout.write(\n                    \"*ERROR** (BUG) Could not parse status line: %s, reason: %s\\n\" % (line, repr(e))\n                )\n    return current_status\n\n\ndef save_status(current_status):\n    with open(status_filename, \"w\") as f:\n        for host, hostdict in current_status.items():\n            hopnum = len(hostdict[\"hops\"].keys())\n            lastreport = hostdict[\"lasttime\"]\n            hoststring = \"%s|%s|%s\" % (host, lastreport, hopnum)\n            for hop in hostdict[\"hops\"].keys():\n                hi = hostdict[\"hops\"][hop]\n                hoststring += \"|%s|%s|%s|%s|%s|%s|%s|%s\" % (\n                    hi[\"hopname\"],\n                    hi[\"loss\"],\n                    hi[\"snt\"],\n                    hi[\"last\"],\n                    hi[\"avg\"],\n                    hi[\"best\"],\n                    hi[\"wrst\"],\n                    hi[\"stddev\"],\n                )\n            hoststring = hoststring.rstrip()\n            f.write(\"%s\\n\" % hoststring)\n\n\n_punct_re = re.compile(r'[\\t !\"#$%&\\'()*\\-/<=>?@\\[\\\\\\]^_`{|},.:]+')\n\n\ndef host_to_filename(host, delim=\"-\"):\n    # Get rid of gibberish chars, stolen from Django\n    \"\"\"Generates an slightly worse ASCII-only slug.\"\"\"\n    return ensure_str(delim).join(\n        word for word in _punct_re.split(ensure_str(host).lower()) if word\n    )\n\n\ndef check_mtr_pid(pid):\n    \"\"\"Check for the existence of a unix pid and if the process matches.\"\"\"\n    try:\n        os.kill(pid, 0)\n    except OSError:\n        return False  # process does no longer exist\n    pid_cmdline = \"/proc/%d/cmdline\" % pid\n    try:\n        return (\n            os.path.exists(pid_cmdline)\n            and \"mtr\\x00--report\\x00--report-wide\" in open(pid_cmdline).read()\n        )\n    except Exception:\n        return False  # any error\n\n\ndef parse_report(host, status):\n    reportfile = report_filepre + host_to_filename(host)\n    if not os.path.exists(reportfile):\n        if host not in status.keys():\n            # New host\n            status[host] = {\"hops\": {}, \"lasttime\": 0}\n        return\n\n    # 1451228358\n    # Start: Sun Dec 27 14:35:18 2015\n    # HOST: purple         Loss%   Snt   Last   Avg  Best  Wrst StDev\n    #  1.|-- 80.69.76.120    0.0%    10    0.3   0.4   0.3   0.6   0.0\n    #  2.|-- 80.249.209.100  0.0%    10    1.0   1.1   0.8   1.4   0.0\n    #  3.|-- 209.85.240.63   0.0%    10    1.3   1.7   1.1   3.6   0.5\n    #  4.|-- 209.85.253.242  0.0%    10    1.6   1.8   1.6   2.1   0.0\n    #  5.|-- 209.85.253.201  0.0%    10    4.8   5.0   4.8   5.4   0.0\n    #  6.|-- 216.239.56.6    0.0%    10    4.7   5.1   4.7   5.5   0.0\n    #  7.|-- ???            100.0    10    0.0   0.0   0.0   0.0   0.0\n    #  8.|-- 74.125.136.147  0.0%    10    4.5   4.6   4.3   5.2   0.0\n    # See if pidfile exists and if mtr is still running\n    if os.path.exists(reportfile + \".pid\"):\n        # See if it's running\n        try:\n            with open(reportfile + \".pid\", \"r\") as opened_file:\n                pid = int(opened_file.readline().rstrip())\n            if check_mtr_pid(pid):\n                # Still running, we're done.\n                if host not in status.keys():\n                    # New host\n                    status[host] = {\"hops\": {}, \"lasttime\": 0}\n                status[host][\"running\"] = True\n                return\n        except ValueError:\n            # Pid file is broken. Process probably crashed..\n            pass\n        # Done running, get rid of pid file\n        os.unlink(reportfile + \".pid\")\n\n    # Parse the existing report\n    with open(reportfile) as opened_file:\n        lines = opened_file.readlines()\n    if len(lines) < 3:\n        sys.stdout.write(\n            \"**ERROR** Report file %s has less than 3 lines, \"\n            \"expecting at least 1 hop! Throwing away invalid report\\n\" % reportfile\n        )\n        os.unlink(reportfile)\n        if host not in status.keys():\n            # New host\n            status[host] = {\"hops\": {}, \"lasttime\": 0}\n        return\n    status[host] = {\"hops\": {}, \"lasttime\": 0}\n\n    hopcount = 0\n    status[host][\"lasttime\"] = int(float(lines.pop(0)))\n    while len(lines) > 0 and not lines[0].startswith(\"HOST:\"):\n        lines.pop(0)\n    if len(lines) < 2:  # Not enough lines\n        return\n    try:\n        lines.pop(0)  # Get rid of HOST: header\n        hopline = re.compile(\n            r\"^\\s*\\d+\\.\"\n        )  # 10.|-- 129.250.2.147   0.0%    10  325.6 315.5 310.3 325.6   5.0\n        for line in lines:\n            if not hopline.match(line):\n                continue  # |  `|-- 129.250.2.159\n            hopcount += 1\n            parts = line.split()\n            if len(parts) < 8:\n                sys.stdout.write(\n                    \"**ERROR** Bug parsing host/hop, line has less than 8 parts: %s\\n\" % line\n                )\n                continue\n            status[host][\"hops\"][hopcount] = {\n                \"hopname\": parts[1],\n                \"loss\": parts[2],\n                \"snt\": parts[3],\n                \"last\": parts[4],\n                \"avg\": parts[5],\n                \"best\": parts[6],\n                \"wrst\": parts[7],\n                \"stddev\": parts[8],\n            }\n    except Exception as e:\n        sys.stdout.write(\n            \"**ERROR** Could not parse report file %s, \"\n            \"tossing away invalid data %s\\n\" % (reportfile, e)\n        )\n        del status[host]\n    os.unlink(reportfile)\n\n\ndef output_report(host, status):\n    hostdict = status.get(host)\n    if not hostdict:\n        return\n\n    hopnum = len(hostdict[\"hops\"].keys())\n    lastreport = hostdict[\"lasttime\"]\n    hoststring = \"%s|%s|%s\" % (host, lastreport, hopnum)\n    for hop in hostdict[\"hops\"].keys():\n        hi = hostdict[\"hops\"][hop]\n        hoststring += \"|%s|%s|%s|%s|%s|%s|%s|%s\" % (\n            hi[\"hopname\"],\n            hi[\"loss\"],\n            hi[\"snt\"],\n            hi[\"last\"],\n            hi[\"avg\"],\n            hi[\"best\"],\n            hi[\"wrst\"],\n            hi[\"stddev\"],\n        )\n    sys.stdout.write(\"%s\\n\" % hoststring)\n\n\ndef start_mtr(host, mtr_binary, config, status):\n    options = [mtr_binary, \"--report\", \"--report-wide\"]\n    pingtype = config.get(host, \"type\")\n    count = config.getint(host, \"count\")\n    ipv4 = config.getboolean(host, \"force_ipv4\")\n    ipv6 = config.getboolean(host, \"force_ipv6\")\n    size = config.getint(host, \"size\")\n    lasttime = config.getint(host, \"time\")\n    dns = config.getboolean(host, \"dns\")\n    port = config.get(host, \"port\")\n    address = config.get(host, \"address\")\n    interval = config.get(host, \"interval\")\n    timeout = config.get(host, \"timeout\")\n\n    if \"running\" in status[host].keys():\n        if debug:\n            sys.stdout.write(\"MTR for host still running, not restarting MTR!\\n\")\n        return\n\n    if time.time() - status[host][\"lasttime\"] < lasttime:\n        if debug:\n            sys.stdout.write(\n                \"%s - %s = %s is smaller than %s => mtr run not needed yet.\\n\"\n                % (\n                    time.time(),\n                    status[host][\"lasttime\"],\n                    time.time() - status[host][\"lasttime\"],\n                    lasttime,\n                )\n            )\n        return\n\n    pid = os.fork()\n    if pid > 0:\n        # Parent process, return and keep running\n        return\n\n    os.chdir(\"/\")\n    os.setsid()\n\n    # Close all fd except stdin,out,err\n    for fd in range(3, 256):\n        try:\n            os.close(fd)\n        except OSError:\n            pass\n\n    if pingtype == \"tcp\":\n        options.append(\"--tcp\")\n    if pingtype == \"udp\":\n        options.append(\"--udp\")\n    if port:\n        options.append(\"--port\")\n        options.append(str(port))\n    if ipv4:\n        options.append(\"-4\")\n    if ipv6:\n        options.append(\"-6\")\n    options.append(\"-s\")\n    options.append(str(size))\n    options.append(\"-c\")\n    options.append(str(count))\n    if not dns:\n        options.append(\"--no-dns\")\n    if address:\n        options.append(\"--address\")\n        options.append(str(address))\n    if interval:\n        options.append(\"-i\")\n        options.append(str(interval))\n    if timeout:\n        options.append(\"--timeout\")\n        options.append(str(timeout))\n\n    options.append(str(host))\n    if debug:\n        sys.stdout.write(\"Startin MTR: %s\\n\" % (\" \".join(options)))\n    reportfile = report_filepre + host_to_filename(host)\n    if os.path.exists(reportfile):\n        os.unlink(reportfile)\n    with open(reportfile, \"a+\") as report:\n        report.write(str(int(time.time())) + \"\\n\")\n        report.flush()\n        process = subprocess.Popen(options, stdout=report, stderr=report)\n    # Write pid to report.pid\n    with open(reportfile + \".pid\", \"w\") as pidfile:\n        pidfile.write(\"%d\\n\" % process.pid)\n        pidfile.flush()\n\n    os._exit(os.EX_OK)\n\n\ndef _is_exe(fpath):\n    # type: (str) -> bool\n    return os.path.isfile(fpath) and os.access(fpath, os.X_OK)\n\n\ndef _which(program):\n    fpath, _fname = os.path.split(program)\n    if fpath:\n        if _is_exe(program):\n            return program\n    else:\n        for path in os.environ[\"PATH\"].split(os.pathsep):\n            exe_file = os.path.join(path, program)\n            if _is_exe(exe_file):\n                return exe_file\n\n    return None\n\n\nif __name__ == \"__main__\":\n    # See if we have mtr\n    mtr_bin = _which(\"mtr\")\n    if mtr_bin is None:\n        if debug:\n            sys.stdout.write(\"Could not find mtr binary\\n\")\n        sys.exit(0)\n\n    # Parse config\n    sys.stdout.write(\"<<<mtr:sep(124)>>>\\n\")\n    conf = read_config()\n    stat = read_status()\n    for host_name in conf.sections():\n        # Parse outstanding report\n        parse_report(host_name, stat)\n        # Output last known values\n        output_report(host_name, stat)\n        # Start new if needed\n        start_mtr(host_name, mtr_bin, conf, stat)\n    save_status(stat)\n"}
{"type": "source_file", "path": "agents/plugins/plesk_backups.py", "content": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n# Copyright (C) 2019 Checkmk GmbH - License: GNU General Public License v2\n# This file is part of Checkmk (https://checkmk.com). It is subject to the terms and\n# conditions defined in the file COPYING, which is part of this source code package.\n\n__version__ = \"2.5.0b1\"\n\n# Monitors FTP backup spaces of plesk domains.\n# Data format\n#\n# <<<plesk_backups>>>\n# <domain> <age-of-newest-file> <size-of-newest-file> <total-size>\n\nimport datetime\nimport sys\nimport time\nfrom ftplib import FTP  # nosec B402 # BNS:97f639\n\ntry:\n    from typing import Any  # noqa: F401\nexcept ImportError:\n    pass\n\ntry:\n    import MySQLdb  # type: ignore[import-untyped]\nexcept ImportError as e:\n    sys.stdout.write(\n        \"<<<plesk_backups>>>\\n%s. Please install missing module via pip install <module>.\" % e\n    )\n    sys.exit(0)\n\n\ndef connect():\n    # Fix pylint issues in case MySQLdb is not present\n\n    try:\n        return MySQLdb.connect(\n            host=\"localhost\",\n            db=\"psa\",\n            user=\"admin\",\n            passwd=open(\"/etc/psa/.psa.shadow\").read().strip(),\n            charset=\"utf8\",\n        )\n    except MySQLdb.Error as exc:\n        sys.stderr.write(\"MySQL-Error %d: %s\\n\" % (exc.args[0], exc.args[1]))\n        sys.exit(1)\n\n\ndef get_domains():\n    cursor = db.cursor()\n    cursor2 = db.cursor()\n\n    cursor.execute(\"SELECT id, name FROM domains\")\n    domain_collection = {}\n    for this_domain_id, this_domain in cursor.fetchall():\n        cursor2.execute(\n            \"SELECT param, value FROM BackupsSettings WHERE id = %s AND type = 'domain'\",\n            (int(this_domain_id),),\n        )\n        params = dict(cursor2.fetchall())\n        domain_collection[this_domain] = params\n\n    cursor2.close()\n    cursor.close()\n    return domain_collection\n\n\n#\n# MAIN\n#\n\ndb = connect()\n\n# 1. Virtual Hosts / Domains auflisten\n# 2. Backupkonfiguration herausfinden\ndomains = get_domains()\n\n# 3. Per FTP verbinden\n#   4. Alter und GrÃ¶ÃŸe der neuesten Datei herausfinden\n#   5. GrÃ¶ÃŸe aller Dateien in Summe herausfinden\n#\n# 6. Neuer Monat?\n#   7. Auf FTP neues Verzeichnis anlegen: <kunde>_2012<monat>\n#   8. Konfiguration in Plesk anpassen\noutput = [\"<<<plesk_backups>>>\"]\nfor domain, p in domains.items():\n    try:\n        if not p:\n            output.append(\"%s 4\" % domain)  # Backup nicht konfiguriert\n            continue\n\n        ftp = FTP(  # nosec B321 # BNS:97f639\n            p[\"backup_ftp_settinghost\"],\n            p[\"backup_ftp_settinglogin\"],\n            p[\"backup_ftp_settingpassword\"],\n        )\n\n        # Zeilen holen\n        files = []  # type: list[str]\n        ftp.retrlines(\"LIST %s\" % p[\"backup_ftp_settingdirectory\"], callback=files.append)\n        # example line:\n        # -rw----r--   1 b091045  cust     13660160 Dec  3 01:50 bla_v8_bla-v8.bla0.net_1212030250.tar\n\n        # Zeilen formatieren\n        last_backup = None\n        backups = []\n        for line in files:\n            parts = line.split()\n            if parts[-1].endswith(\".tar\"):\n                dt = datetime.datetime(*time.strptime(parts[-1][-14:-4], \"%y%m%d%H%M\")[0:5])\n                backup = (parts[-1], dt, int(parts[-5]))\n\n                if not last_backup or dt > last_backup[1]:\n                    last_backup = backup\n                backups.append(backup)\n\n        if not backups:\n            output.append(\"%s 5\" % domain)  # Keine Sicherungen vorhanden\n            continue\n\n        # Get total size of all files on FTP\n        f = []  # type: list[Any]\n\n        def get_size(ftp_conn, base_dir, l=None):\n            if l and l.split()[-1] in [\".\", \"..\"]:\n                return 0\n\n            size = 0\n            if not l or l[0] == \"d\":\n                subdir = \"/\" + l.split()[-1] if l else \"\"\n                dir_files = []  # type: list[str]\n                ftp_conn.retrlines(\"LIST %s%s\" % (base_dir, subdir), callback=dir_files.append)\n                for ln in dir_files:\n                    size += get_size(ftp_conn, \"%s%s\" % (base_dir, subdir), ln)\n            else:\n                size += int(l.split()[-5])\n            return size\n\n        total_size = get_size(ftp, \"\")\n        if last_backup:\n            output.append(\n                \"%s 0 %s %d %d\"\n                % (domain, last_backup[1].strftime(\"%s\"), last_backup[2], total_size)\n            )\n\n    except Exception as e:\n        output.append(\"%s 2 %s\" % (domain, e))\n\n# Write cache and output\nsys.stdout.write(\"%s\\n\" % \"\\n\".join(output))\n"}
{"type": "source_file", "path": "agents/plugins/unitrends_replication.py", "content": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n# Copyright (C) 2019 Checkmk GmbH - License: GNU General Public License v2\n# This file is part of Checkmk (https://checkmk.com). It is subject to the terms and\n# conditions defined in the file COPYING, which is part of this source code package.\n\n__version__ = \"2.5.0b1\"\n\nimport sys\nimport time\nfrom urllib.request import urlopen\nfrom xml.dom import minidom\n\n# TODO: minicompat include internal impl details. But NodeList is only defined there for <3.11\nfrom xml.dom.minicompat import NodeList\n\nnow = int(time.time())\nstart = now - 24 * 60 * 60\nend = now\ndpu = 1\n\nurl = (\n    \"http://localhost/recoveryconsole/bpl/syncstatus.php?type=replicate&arguments=start:%s,end:%s&sid=%s&auth=1:\"\n    % (start, end, dpu)\n)\n\nxml = urlopen(url)  # nosec B310 # BNS:28af27\n\n\ndef _get_text(node: NodeList[minidom.Element]) -> str:\n    first = node.item(0)\n    if first is None:\n        raise ValueError(\"Node has no item\")\n    child = first.firstChild\n    if child is None or not isinstance(child, minidom.Text):\n        raise ValueError(\"Node has no text\")\n    return child.data\n\n\nsys.stdout.write(\"<<<unitrends_replication:sep(124)>>>\\n\")\ndom = minidom.parse(xml)\nfor item in dom.getElementsByTagName(\"SecureSyncStatus\"):\n    application_node = item.getElementsByTagName(\"Application\")\n    if application_node:\n        application = application_node[0].attributes[\"Name\"].value\n    else:\n        application = \"N/A\"\n    result = _get_text(item.getElementsByTagName(\"Result\"))\n    completed = _get_text(item.getElementsByTagName(\"Complete\"))\n    targetname = _get_text(item.getElementsByTagName(\"TargetName\"))\n    instancename = _get_text(item.getElementsByTagName(\"InstanceName\"))\n    sys.stdout.write(\n        \"%s|%s|%s|%s|%s\\n\" % (application, result, completed, targetname, instancename)\n    )\n"}
{"type": "source_file", "path": "buildscripts/docker_image_aliases/resolve.py", "content": "#!/usr/bin/env python3\n# Copyright (C) 2023 Checkmk GmbH - License: GNU General Public License v2\n# This file is part of Checkmk (https://checkmk.com). It is subject to the terms and\n# conditions defined in the file COPYING, which is part of this source code package.\n\n\"\"\"Maps a given Docker Image Alias name (e.g. IMAGE_CMK_BASE) to an unambiguous\nimage ID, defined in correspondingly named folders containing Dockerfiles.\nSo the mapping is SCM tracked and thus branch specific and reproducible.\"\"\"\n\nimport argparse\nimport sys\nfrom pathlib import Path\nfrom subprocess import run\n\n\ndef parse_arguments() -> argparse.Namespace:\n    parser = argparse.ArgumentParser(description=__doc__)\n\n    parser.add_argument(\"alias_name\")\n    parser.add_argument(\n        \"--check\",\n        action=\"store_true\",\n        help=\"Check whether the docker image is already locally available and print the resolved repo tag\",\n    )\n\n    return parser.parse_args()\n\n\ndef image_id(alias_name: str) -> str:\n    \"\"\"Basically returns the generated image id reported by `docker build` on a given\n    image alias folder. Matches against output with or without Docker build kit\"\"\"\n    try:\n        with open(Path(__file__).parent / alias_name / \"Dockerfile\") as dockerfile:\n            for line in dockerfile:\n                if \"FROM\" in line:\n                    return line.strip().split()[-1]\n    except FileNotFoundError:\n        pass\n\n    print(\n        f\"Docker image alias '{alias_name}' could not be resolved:\",\n        file=sys.stderr,\n    )\n    print(\n        \"Make sure the image alias exists, you're correctly logged into the registry\"\n        \" and the image exists on the registry.\",\n        file=sys.stderr,\n    )\n\n    print(\"INVALID_IMAGE_ID\")\n\n    raise SystemExit(1)\n\n\ndef extract_repo_tag(alias_name: str) -> str:\n    # Get the nexus repo tag via the meta.yml file. You're asking why not via docker image inspect?\n    # It seems that we don't always get the nexus repo tag via the field \"RepoTags\", so we go this way...\n    with open(Path(__file__).parent / alias_name / \"meta.yml\") as meta_file:\n        tag_lines = [line for line in meta_file.readlines() if \"tag:\" in line]\n        repo_tag = tag_lines and tag_lines[0].split()[1]\n        if not repo_tag:\n            raise SystemExit(f\"meta.yml of {alias_name} has no tag line\")\n        return repo_tag\n\n\ndef main() -> None:\n    args = parse_arguments()\n\n    alias_name = args.alias_name\n    image_id_value = image_id(alias_name)\n    repo_tag = extract_repo_tag(alias_name)\n\n    # The following is a workaround for images being deleted on Nexus.\n    # It should be replaced with a solution which allows to keep images forever without having\n    # to pull them regularly.\n\n    if args.check:\n        print(f\"Resolved repo tag: {repo_tag}\")\n        result = run([\"docker\", \"images\", \"-q\", repo_tag], capture_output=True, check=False)\n        if not result.stdout.splitlines():\n            print(\"Does not exist locally, might perform image pull as next step ...\")\n    else:\n        print(image_id_value)\n        if repo_tag:\n            # We need to pull also the tag, otherwise Nexus may delete those images\n            run([\"docker\", \"pull\", repo_tag], capture_output=True, check=False)\n\n\nif __name__ == \"__main__\":\n    main()\n"}
{"type": "source_file", "path": "agents/plugins/mk_tinkerforge.py", "content": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n# Copyright (C) 2019 Checkmk GmbH - License: GNU General Public License v2\n# This file is part of Checkmk (https://checkmk.com). It is subject to the terms and\n# conditions defined in the file COPYING, which is part of this source code package.\n\n__version__ = \"2.5.0b1\"\n\n###################################################\n# plugin to retrieve data from tinkerforge devices.\n#\n# please note that for this plugin to work, the tinkerforge api has to be installed\n#  (included in OMD, otherwise get it from http://download.tinkerforge.com/bindings/python/)\n# Also, if the tinkerforge device is connected directly to the computer via usb,\n# the brick deamon has to be installed and running: http://download.tinkerforge.com/tools/brickd/)\n#\n# This has been designed to also work as a special agent. In this case the following configuration\n# settings have to be provided on the command line\n\n#######################################################\n# sample configuration (/etc/check_mk/tinkerforge.cfg):\n#\n# host = \"localhost\"\n# port = 4223\n# segment_display_uid = \"abc\"         # uid of the sensor to display on the 7-segment display\n# segment_display_brightness = 2      # brightness of the 7-segment display (0-7)\n#\n# to find the uid of a sensor, either use brickv or run the plugin\n# manually. plugin output looks like this:\n#   temperature,Ab3d5F.a.xyz,2475\n# xyz is the uid you're looking for. It's always the last of the dot-separated sensor path\n# (Ab3d5F is the id of the master brick to which the sensor is connected, a is the port\n#  to which the sensor is connected)\n\n##################\n# developer notes:\n#\n# Support for individual bricklets has to be added in init_device_handlers.\n#  Currently the bricklets included in the Starter Kit: Server Room Monitoring are\n#  implemented\n\n# Don't have tinkerforge module during tests. So disable those checks\n\nimport hashlib\nimport os\nimport sys\nimport time\nfrom optparse import OptionParser\nfrom urllib.request import urlopen\n\n\ndef check_digest(data, expected):\n    digest = hashlib.sha256(data.read()).hexdigest()\n    if digest != expected:\n        raise ValueError(\"Failed to validate digest: expected: %s, got: %s.\" % (expected, digest))\n\n\ndef install():\n    dest = os.path.dirname(os.path.realpath(__file__))\n    sys.stdout.write(\"installing tinkerforge python api to %s\\n\" % dest)\n    if os.path.exists(os.path.join(dest, \"tinkerforge\")):\n        sys.stdout.write(\"already installed\\n\")\n        return 1\n\n    if sys.version_info[0] >= 3:\n        from io import BytesIO\n    else:\n        from cStringIO import StringIO as BytesIO\n    import shutil\n    from zipfile import ZipFile\n\n    url = \"https://download.tinkerforge.com/bindings/python/tinkerforge_python_bindings_2_1_30.zip\"\n    # sha256sum of the downloaded file. To update:\n    #   `curl -s \"https://download.tinkerforge.com/[new-version].zip | sha256sum`\n    download_digest = \"e735e0e53ad56e2c2919cf412f3ec28ec0997919eb556b20c27519a57fb7bad0\"\n\n    response = urlopen(url)  # nosec B310 # BNS:28af27\n    buf = BytesIO(response.read())\n    check_digest(buf, download_digest)\n\n    with ZipFile(buf) as z:\n        extract_files = [f for f in z.namelist() if f.startswith(\"source/tinkerforge\")]\n        z.extractall(dest, extract_files)\n\n    shutil.move(os.path.join(dest, \"source\", \"tinkerforge\"), os.path.join(dest, \"tinkerforge\"))\n    shutil.rmtree(os.path.join(dest, \"source\"))\n\n    return 0\n\n\nDEFAULT_SETTINGS = {\n    \"host\": \"localhost\",\n    \"port\": 4223,\n    \"segment_display_uid\": None,\n    \"segment_display_brightness\": 2,\n}\n\n# globals\nsegment_display_value = None\nsegment_display_unit = \"\"\nsegment_display = None\n\n\ndef id_to_string(identifier):\n    return \"%s.%s.%s\" % (identifier.connected_uid, identifier.position, identifier.uid)\n\n\ndef print_generic(settings, sensor_type, ident, factor, unit, *values):\n    if ident.uid == settings[\"segment_display_uid\"]:\n        global segment_display_value, segment_display_unit\n        segment_display_value = int(values[0] * factor)\n        segment_display_unit = unit\n    sys.stdout.write(\n        \"%s,%s,%s\\n\" % (sensor_type, id_to_string(ident), \",\".join([str(val) for val in values]))\n    )\n\n\ndef print_ambient_light(conn, settings, uid):\n    from tinkerforge.bricklet_ambient_light import (  # type: ignore[import-not-found]\n        BrickletAmbientLight,\n    )\n\n    br = BrickletAmbientLight(uid, conn)\n    print_generic(settings, \"ambient\", br.get_identity(), 0.01, \"L\", br.get_illuminance())\n\n\ndef print_ambient_light_v2(conn, settings, uid):\n    from tinkerforge.bricklet_ambient_light_v2 import (  # type: ignore[import-not-found]\n        BrickletAmbientLightV2,\n    )\n\n    br = BrickletAmbientLightV2(uid, conn)\n    print_generic(settings, \"ambient\", br.get_identity(), 0.01, \"L\", br.get_illuminance())\n\n\ndef print_temperature(conn, settings, uid):\n    from tinkerforge.bricklet_temperature import (  # type: ignore[import-not-found]\n        BrickletTemperature,\n    )\n\n    br = BrickletTemperature(uid, conn)\n    print_generic(\n        settings, \"temperature\", br.get_identity(), 0.01, \"\\N{DEGREE SIGN}C\", br.get_temperature()\n    )\n\n\ndef print_temperature_ext(conn, settings, uid):\n    from tinkerforge.bricklet_ptc import BrickletPTC  # type: ignore[import-not-found]\n\n    br = BrickletPTC(uid, conn)\n    print_generic(\n        settings,\n        \"temperature.ext\",\n        br.get_identity(),\n        0.01,\n        \"\\N{DEGREE SIGN}C\",\n        br.get_temperature(),\n    )\n\n\ndef print_humidity(conn, settings, uid):\n    from tinkerforge.bricklet_humidity import BrickletHumidity  # type: ignore[import-not-found]\n\n    br = BrickletHumidity(uid, conn)\n    print_generic(settings, \"humidity\", br.get_identity(), 0.1, \"RH\", br.get_humidity())\n\n\ndef print_master(conn, settings, uid):\n    from tinkerforge.brick_master import BrickMaster  # type: ignore[import-not-found]\n\n    br = BrickMaster(uid, conn)\n    print_generic(\n        settings,\n        \"master\",\n        br.get_identity(),\n        1.0,\n        \"\",\n        br.get_stack_voltage(),\n        br.get_stack_current(),\n        br.get_chip_temperature(),\n    )\n\n\ndef print_motion_detector(conn, settings, uid):\n    from tinkerforge.bricklet_motion_detector import (  # type: ignore[import-not-found]\n        BrickletMotionDetector,\n    )\n\n    br = BrickletMotionDetector(uid, conn)\n    print_generic(settings, \"motion\", br.get_identity(), 1.0, \"\", br.get_motion_detected())\n\n\ndef display_on_segment(conn, settings, text):\n    #        0x01\n    #       ______\n    #      |      |\n    # 0x20 |      | 0x02\n    #      |______|\n    #      | 0x40 |\n    # 0x10 |      | 0x04\n    #      |______|\n    #        0x08\n\n    CHARACTERS = {\n        \"0\": 0x3F,\n        \"1\": 0x06,\n        \"2\": 0x5B,\n        \"3\": 0x4F,\n        \"4\": 0x66,\n        \"5\": 0x6D,\n        \"6\": 0x7D,\n        \"7\": 0x07,\n        \"8\": 0x7F,\n        \"9\": 0x6F,\n        \"C\": 0x39,\n        \"H\": 0x74,\n        \"L\": 0x38,\n        \"R\": 0x50,\n        \"\\N{DEGREE SIGN}\": 0x63,\n    }\n\n    from tinkerforge.bricklet_segment_display_4x7 import (  # type: ignore[import-not-found]\n        BrickletSegmentDisplay4x7,\n    )\n\n    br = BrickletSegmentDisplay4x7(segment_display, conn)\n    segments = []  # type: list\n    for letter in text:\n        if len(segments) >= 4:\n            break\n        if letter in CHARACTERS:\n            segments.append(CHARACTERS[letter])\n\n    # align to the right\n    segments = [0] * (4 - len(segments)) + segments\n\n    br.set_segments(segments, settings[\"segment_display_brightness\"], False)\n\n\ndef init_device_handlers():\n    device_handlers = {}\n\n    # storing the dev_id is not necessary but may save a little time as otherwise the module\n    # needs to be imported just to find out this id. If the bricklet is present the module\n    # gets imported anyway of course\n    for dev_id, module_name, clazz, handler in [\n        (13, \"brick_master\", \"BrickMaster\", print_master),\n        (21, \"bricklet_ambient_light\", \"BrickletAmbientLight\", print_ambient_light),\n        (259, \"bricklet_ambient_light_v2\", \"BrickletAmbientLightV2\", print_ambient_light_v2),\n        (216, \"bricklet_temperature\", \"BrickletTemperature\", print_temperature),\n        (226, \"bricklet_ptc\", \"BrickletPTC\", print_temperature_ext),\n        (27, \"bricklet_humidity\", \"BrickletHumidity\", print_humidity),\n        (233, \"bricklet_motion_detector\", \"BrickletMotionDetector\", print_motion_detector),\n    ]:\n        if dev_id is not None:\n            device_handlers[dev_id] = handler\n        else:\n            module = __import__(\"tinkerforge.\" + module_name)\n            sub_module = module.__dict__[module_name]\n            device_handlers[sub_module.__dict__[clazz].DEVICE_IDENTIFIER] = handler\n\n    return device_handlers\n\n\ndef enumerate_callback(\n    conn,\n    device_handlers,\n    settings,\n    uid,\n    connected_uid,\n    position,\n    hardware_version,\n    firmware_version,\n    device_identifier,\n    enumeration_type,\n):\n    if device_identifier == 237:\n        global segment_display\n        segment_display = uid\n    elif device_identifier in device_handlers:\n        device_handlers[device_identifier](conn, settings, uid)\n\n\ndef read_config(env):\n    settings = DEFAULT_SETTINGS\n    cfg_path = os.path.join(os.getenv(\"MK_CONFDIR\", \"/etc/check_mk\"), \"tinkerforge.cfg\")\n\n    if os.path.isfile(cfg_path):\n        with open(cfg_path) as opened_file:\n            exec(opened_file.read(), settings, settings)  # nosec B102 # BNS:a29406\n    return settings\n\n\ndef main():\n    # host = \"localhost\"\n    # port = 4223\n    # segment_display_uid = \"abc\"         # uid of the sensor to display on the 7-segment display\n    # segment_display_brightness = 2      # brightness of the 7-segment display (0-7)\n\n    settings = read_config(os.environ)\n    parser = OptionParser()\n    parser.add_option(\n        \"--host\",\n        dest=\"host\",\n        default=settings[\"host\"],\n        help=\"host/ipaddress of the tinkerforge device\",\n        metavar=\"ADDRESS\",\n    )\n    parser.add_option(\n        \"--port\",\n        dest=\"port\",\n        default=settings[\"port\"],\n        type=int,\n        help=\"port of the tinkerforge device\",\n        metavar=\"PORT\",\n    )\n    parser.add_option(\n        \"--segment_display_uid\",\n        dest=\"uid\",\n        default=settings[\"segment_display_uid\"],\n        help=\"uid of the bricklet which will be displayed in the 7-segment display\",\n        metavar=\"UID\",\n    )\n    parser.add_option(\n        \"--segment_display_brightness\",\n        type=int,\n        dest=\"brightness\",\n        default=settings[\"segment_display_brightness\"],\n        help=\"brightness of the 7-segment display (0-7)\",\n    )\n    parser.add_option(\n        \"--install\",\n        action=\"store_true\",\n        help=\"install tinkerforge python api to same directory as the plugin\",\n    )\n\n    options = parser.parse_args()[0]\n\n    settings = {\n        \"host\": options.host,\n        \"port\": options.port,\n        \"segment_display_uid\": options.uid,\n        \"segment_display_brightness\": options.brightness,\n    }\n\n    if options.install:\n        return install()\n\n    try:\n        from tinkerforge.ip_connection import IPConnection  # type: ignore[import-not-found]\n    except ImportError:\n        sys.stdout.write(\"<<<tinkerforge:sep(44)>>>\\n\")\n        sys.stdout.write(\"master,0.0.0,tinkerforge api isn't installed\\n\")\n        return 1\n\n    conn = IPConnection()\n    conn.connect(settings[\"host\"], settings[\"port\"])\n\n    device_handlers = init_device_handlers()\n\n    try:\n        sys.stdout.write(\"<<<tinkerforge:sep(44)>>>\\n\")\n\n        conn.register_callback(\n            IPConnection.CALLBACK_ENUMERATE,\n            lambda uid,\n            connected_uid,\n            position,\n            hardware_version,\n            firmware_version,\n            device_identifier,\n            enumeration_type: enumerate_callback(\n                conn,\n                device_handlers,\n                settings,\n                uid,\n                connected_uid,\n                position,\n                hardware_version,\n                firmware_version,\n                device_identifier,\n                enumeration_type,\n            ),\n        )\n        conn.enumerate()\n\n        # bricklets respond asynchronously in callbacks and we have no way of knowing\n        # what bricklets to expect\n        time.sleep(0.1)\n\n        if segment_display is not None:\n            if segment_display_value is not None:\n                display_on_segment(\n                    conn, settings, \"%d%s\" % (segment_display_value, segment_display_unit)\n                )\n            else:\n                display_on_segment(conn, settings, \"\")\n    finally:\n        conn.disconnect()\n    return None\n\n\nif __name__ == \"__main__\":\n    main()\n"}
{"type": "source_file", "path": "agents/plugins/__init__.py", "content": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n# Copyright (C) 2019 Checkmk GmbH - License: GNU General Public License v2\n# This file is part of Checkmk (https://checkmk.com). It is subject to the terms and\n# conditions defined in the file COPYING, which is part of this source code package.\n\n# This file is only here to make pytest and mypy find the agent plugins.\n# It will not be shipped nor synced into the site. It must not contain code.\n"}
{"type": "source_file", "path": "agents/plugins/plesk_domains.py", "content": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n# Copyright (C) 2019 Checkmk GmbH - License: GNU General Public License v2\n# This file is part of Checkmk (https://checkmk.com). It is subject to the terms and\n# conditions defined in the file COPYING, which is part of this source code package.\n\n__version__ = \"2.5.0b1\"\n\n# Lists all domains configured in plesk\n#\n# <<<plesk_domains>>>\n# <domain>\n\nimport sys\n\ntry:\n    import MySQLdb  # type: ignore[import-untyped]\nexcept ImportError as e:\n    sys.stdout.write(\n        \"<<<plesk_domains>>>\\n%s. Please install missing module via pip install <module>.\" % e\n    )\n    sys.exit(0)\n\nwith open(\"/etc/psa/.psa.shadow\") as pwd_file:\n    pwd = pwd_file.read().strip()\ntry:\n    db = MySQLdb.connect(\n        host=\"localhost\",\n        db=\"psa\",\n        user=\"admin\",\n        passwd=pwd,\n        charset=\"utf8\",\n    )\nexcept MySQLdb.Error as e:\n    sys.stderr.write(\"MySQL-Error %d: %s\\n\" % (e.args[0], e.args[1]))\n    sys.exit(1)\n\ncursor = db.cursor()\ncursor.execute(\"SELECT name FROM domains\")\nsys.stdout.write(\"<<<plesk_domains>>>\\n\")\nsys.stdout.write(\"%s\\n\" % \"\\n\".join([d[0] for d in cursor.fetchall()]))\ncursor.close()\n"}
{"type": "source_file", "path": "buildscripts/scripts/__init__.py", "content": "#!/usr/bin/env python3\n# Copyright (C) 2024 Checkmk GmbH - License: GNU General Public License v2\n# This file is part of Checkmk (https://checkmk.com). It is subject to the terms and\n# conditions defined in the file COPYING, which is part of this source code package.\n"}
{"type": "source_file", "path": "agents/wnx/patch_windows_agent_version.py", "content": "#!/usr/bin/env python3\n# Copyright (C) 2019 Checkmk GmbH - License: GNU General Public License v2\n# This file is part of Checkmk (https://checkmk.com). It is subject to the terms and\n# conditions defined in the file COPYING, which is part of this source code package.\n\n# Simple python script to replace version in the file\n# first param is a file usually wnx\\include\\common\\wnx_version.h\n# second param is version without double quotes, for example, 2.0.0i1\n\nimport re\nimport sys\n\nprint(f\"Windows agent version to be set to '{sys.argv[1]}' in file '{sys.argv[2]}'\")\ncontent = \"\"\nwith open(sys.argv[1]) as f:\n    content = f.read()\n\nwith open(sys.argv[1], \"w\") as f:\n    pattern = r'(^#define CMK_WIN_AGENT_VERSION )(\"[^\"]*\")'\n    ret = re.sub(pattern, f'\\\\1\"{sys.argv[2]}\"', content)\n    f.write(ret)\n\nprint(\"Windows agent version has been set successfully\")\n"}
{"type": "source_file", "path": "agents/wnx/scripts/check_hashes.py", "content": "#!/usr/bin/env python3\n# Copyright (C) 2023 Checkmk GmbH - License: GNU General Public License v2\n# This file is part of Checkmk (https://checkmk.com). It is subject to the terms and\n# conditions defined in the file COPYING, which is part of this source code package.\n\n# Primitive python script to check that hash files is minimally correct\n\n# file content should look approximately as:\n#\n# check_mk_agent.msi 987227B23CA5A75FB8191F19D718E4348F748082AC16C83B48F99F3F2694913F\n# cmk-agent-ctl.exe AC3A70412218B6C97345E152D71CB03B11666A03CD7D8221AC63FAE5F8030972\n# OpenHardwareMonitorCLI.exe 652A088B5FA4D90C8B4FB4D68E649A80421552D3341B1E194C38D71825D87B43\n# OpenHardwareMonitorLib.dll 30A6C1ABDA911CE64315C5E6C94E6F0689B567E0346F73697D60FA09EA8F913C\n# check_mk_service32.exe 295ED26953165FEBC07174277572BC0E5BA7C2A3C6414B689C3A6DB69DF2D876\n# check_mk_service64.exe 2BED103B091822BEF9940E62C5AF34C1EA5B3B07466379CA5E44CB1810EFAFAF\n\nimport sys\n\nexpected_files = {\n    \"mk-sql.exe\",\n    \"check_mk_agent.msi\",\n    \"check_mk_service32.exe\",\n    \"check_mk_service64.exe\",\n    \"cmk-agent-ctl.exe\",\n    \"OpenHardwareMonitorCLI.exe\",\n    \"OpenHardwareMonitorLib.dll\",\n}\n\n\nwith open(sys.argv[1]) as inp:\n    lines = inp.readlines()\n    files = {l.split()[0] for l in lines}\n    if {f.lower() for f in expected_files} != {f.lower() for f in files}:\n        print(f\"MISSING FILE(S) ERROR: {files} not equal to expected {expected_files}\")\n        sys.exit(1)\n    hashes = {l.split()[1] for l in lines}\n    if len(hashes) != len(expected_files) or {len(h) for h in hashes} != {64}:\n        print(f\"WRONG COUNT ERROR: {len(hashes)}\")\n        sys.exit(1)\n\nsys.exit(0)\n"}
{"type": "source_file", "path": "agents/plugins/mk_inotify.py", "content": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n# Copyright (C) 2019 Checkmk GmbH - License: GNU General Public License v2\n# This file is part of Checkmk (https://checkmk.com). It is subject to the terms and\n# conditions defined in the file COPYING, which is part of this source code package.\n\n__version__ = \"2.5.0b1\"\n\nimport os\nimport signal\nimport sys\nimport time\n\ntry:\n    import configparser\nexcept ImportError:  # Python 2\n    import ConfigParser as configparser  # type: ignore[import-not-found,no-redef]\n\ntry:\n    from typing import Any  # noqa: F401\nexcept ImportError:\n    pass\n\ntry:\n    # TODO: We should probably ship this package.\n    import pyinotify  # type: ignore[import-not-found]\nexcept ImportError:\n    sys.stderr.write(\"Error: Python plugin pyinotify is not installed\\n\")\n    sys.exit(1)\n\n\ndef usage():\n    sys.stdout.write(\"Usage: mk_inotify [-g]\\n\")\n    sys.stdout.write(\"         -g: run in foreground\\n\\n\")\n\n\n# Available options:\n# -g: run in foreground\nopt_foreground = False\nif len(sys.argv) == 2 and sys.argv[1] == \"-g\":\n    opt_foreground = True\n\nmk_confdir = os.getenv(\"MK_CONFDIR\") or \"/etc/check_mk\"\nmk_vardir = os.getenv(\"MK_VARDIR\") or \"/var/lib/check_mk_agent\"\n\nconfig_filename = mk_confdir + \"/mk_inotify.cfg\"\nconfigured_paths = mk_vardir + \"/mk_inotify.configured\"\npid_filename = mk_vardir + \"/mk_inotify.pid\"\n\nconfig = configparser.ConfigParser({})\nif not os.path.exists(config_filename):\n    sys.exit(0)\nconfig_mtime = os.stat(config_filename).st_mtime\nconfig.read(config_filename)\n\n# Configurable in Agent Bakery\nheartbeat_timeout = config.getint(\"global\", \"heartbeat_timeout\")\nwrite_interval = config.getint(\"global\", \"write_interval\")\nmax_messages_per_interval = config.getint(\"global\", \"max_messages_per_interval\")\nstats_retention = config.getint(\"global\", \"stats_retention\")\nconfig.remove_section(\"global\")\n\n\ndef output_data():\n    sys.stdout.write(\"<<<inotify:sep(9)>>>\\n\")\n    if os.path.exists(configured_paths):\n        with open(configured_paths) as opened_conf_paths:\n            sys.stdout.write(opened_conf_paths.read())\n\n    now = time.time()\n    for dirpath, _unused_dirnames, filenames in os.walk(mk_vardir):\n        for filename in filenames:\n            if filename.startswith(\"mk_inotify.stats\"):\n                try:\n                    the_file = \"%s/%s\" % (dirpath, filename)\n                    filetime = os.stat(the_file).st_mtime\n                    file_age = now - filetime\n                    if file_age > 5:\n                        with open(the_file) as opened_the_file:\n                            sys.stdout.write(opened_the_file.read())\n                    if file_age > stats_retention:\n                        os.unlink(the_file)\n                except Exception:\n                    pass\n        break\n\n\n# Check if another mk_inotify process is already running\nif os.path.exists(pid_filename):\n    with open(pid_filename) as opened_file:\n        pid_str = opened_file.read()\n    proc_cmdline = \"/proc/%s/cmdline\" % pid_str\n    if os.path.exists(proc_cmdline):\n        with open(proc_cmdline) as opened_inner_file:\n            cmdline = opened_inner_file.read()\n        cmdline_tokens = cmdline.split(\"\\0\")\n        if \"mk_inotify\" in cmdline_tokens[1]:\n            # Another mk_notify process is already running..\n            # Simply output the current statistics and exit\n            output_data()\n\n            # The pidfile is also the heartbeat file for the running process\n            os.utime(pid_filename, None)\n            sys.exit(0)\n\n#   .--Fork----------------------------------------------------------------.\n#   |                         _____          _                             |\n#   |                        |  ___|__  _ __| | __                         |\n#   |                        | |_ / _ \\| '__| |/ /                         |\n#   |                        |  _| (_) | |  |   <                          |\n#   |                        |_|  \\___/|_|  |_|\\_\\                         |\n#   |                                                                      |\n#   +----------------------------------------------------------------------+\n#   Reaching this point means that no mk_inotify is currently running\n\nif not opt_foreground:\n    try:\n        pid = os.fork()\n        if pid > 0:\n            sys.exit(0)\n        # Decouple from parent environment\n        os.chdir(\"/\")\n        os.umask(0)\n        os.setsid()\n\n        # Close all fd\n        for fd in range(0, 256):\n            try:\n                os.close(fd)\n            except OSError:\n                pass\n    except Exception as e:\n        sys.stderr.write(\"Error forking mk_inotify: %s\" % e)\n\n    # Save pid of working process.\n    with open(pid_filename, \"w\") as opened_file:\n        opened_file.write(\"%d\" % os.getpid())\n# .\n#   .--Main----------------------------------------------------------------.\n#   |                        __  __       _                                |\n#   |                       |  \\/  | __ _(_)_ __                           |\n#   |                       | |\\/| |/ _` | | '_ \\                          |\n#   |                       | |  | | (_| | | | | |                         |\n#   |                       |_|  |_|\\__,_|_|_| |_|                         |\n#   |                                                                      |\n#   +----------------------------------------------------------------------+\n\n# Computed configuration\nfolder_configs = {}  # type: dict[str, dict[str, Any]]\n# Data to be written to disk\noutput = []  # type: list[str]\n\n\ndef get_watched_files():\n    files = set([])\n    for folder, attributes in folder_configs.items():\n        for filenames in attributes[\"monitor_files\"].values():\n            for filename in filenames:\n                files.add(\"configured\\tfile\\t%s/%s\" % (folder, filename))\n        if attributes.get(\"monitor_all\"):\n            files.add(\"configured\\tfolder\\t%s\" % (folder))\n    return files\n\n\ndef wakeup_handler(signum, frame):\n    global output\n    if output:\n        if opt_foreground:\n            sys.stdout.write(\"%s\\n\" % \"\\n\".join(output))\n            sys.stdout.write(\"%s\\n\" % \"\\n\".join(get_watched_files()))\n        else:\n            filename = \"mk_inotify.stats.%d\" % time.time()\n            with open(\"%s/%s\" % (mk_vardir, filename), \"w\") as stats_file:\n                stats_file.write(\"\\n\".join(output) + \"\\n\")\n        output = []\n\n    # Check if configuration has changed -> restart\n    if config_mtime != os.stat(config_filename).st_mtime:\n        os.execv(__file__, sys.argv)\n\n    # Exit on various instances\n    if not opt_foreground:\n        if not os.path.exists(pid_filename):  # pidfile is missing\n            sys.exit(0)\n        if time.time() - os.stat(pid_filename).st_mtime > heartbeat_timeout:  # heartbeat timeout\n            sys.exit(0)\n        with open(pid_filename) as opened_pid_file:\n            if os.getpid() != int(opened_pid_file.read()):  # pidfile differs\n                sys.exit(0)\n\n    update_watched_folders()\n    signal.alarm(write_interval)\n\n\ndef do_output(what, event):\n    if event.dir:\n        return  # Only monitor files\n\n    if len(output) > max_messages_per_interval:\n        last_message = \"warning\\tMaximum messages reached: %d per %d seconds\" % (\n            max_messages_per_interval,\n            write_interval,\n        )\n        if output[-1] != last_message:\n            output.append(last_message)\n        return\n\n    path = event.path\n    path_config = folder_configs.get(path)\n    if not path_config:\n        return  # shouldn't happen, maybe on subfolders (not supported)\n\n    filename = os.path.basename(event.pathname)\n    if what in path_config[\"monitor_all\"] or filename in path_config[\"monitor_files\"].get(what, []):\n        line = \"%d\\t%s\\t%s\" % (time.time(), what, event.pathname)\n        if map_events[what][1]:  # Check if filestats are enabled\n            try:\n                stats = os.stat(event.pathname)\n                line += \"\\t%d\\t%d\" % (stats.st_size, stats.st_mtime)\n            except Exception:\n                pass\n        output.append(line)\n        if opt_foreground:\n            sys.stdout.write(\"%s\\n\" % line)\n\n\nmap_events = {\n    # Mode     Mask                        Report_filestats (currently unused)\n    \"access\": (pyinotify.IN_ACCESS, False),\n    \"open\": (pyinotify.IN_OPEN, False),\n    \"create\": (pyinotify.IN_CREATE, False),\n    \"delete\": (pyinotify.IN_DELETE, False),\n    \"modify\": (pyinotify.IN_MODIFY, False),\n    \"movedto\": (pyinotify.IN_MOVED_TO, False),\n    \"movedfrom\": (pyinotify.IN_MOVED_FROM, False),\n    \"moveself\": (pyinotify.IN_MOVE_SELF, False),\n}\n\n\nclass NotifyEventHandler(pyinotify.ProcessEvent):  # type: ignore[misc]\n    def process_IN_MOVED_TO(self, event):\n        do_output(\"movedto\", event)\n\n    def process_IN_MOVED_FROM(self, event):\n        do_output(\"movedfrom\", event)\n\n    def process_IN_MOVE_SELF(self, event):\n        do_output(\"moveself\", event)\n\n    #    def process_IN_CLOSE_NOWRITE(self, event):\n    #        print \"CLOSE_NOWRITE event:\", event.pathname\n    #\n    #    def process_IN_CLOSE_WRITE(self, event):\n    #        print \"CLOSE_WRITE event:\", event.pathname\n\n    def process_IN_CREATE(self, event):\n        do_output(\"create\", event)\n\n    def process_IN_DELETE(self, event):\n        do_output(\"delete\", event)\n\n    def process_IN_MODIFY(self, event):\n        do_output(\"modify\", event)\n\n    def process_IN_OPEN(self, event):\n        do_output(\"open\", event)\n\n\n# Watch manager\nwm = pyinotify.WatchManager()\n\n\ndef update_watched_folders():\n    for folder, attributes in folder_configs.items():\n        if attributes.get(\"watch_descriptor\"):\n            if not wm.get_path(attributes[\"watch_descriptor\"].get(folder)):\n                del attributes[\"watch_descriptor\"]\n        elif os.path.exists(folder):\n            new_wd = wm.add_watch(folder, attributes[\"mask\"], rec=True)\n            if new_wd.get(folder) > 0:\n                attributes[\"watch_descriptor\"] = new_wd\n\n\ndef main():\n    # Read config\n\n    for section in config.sections():\n        section_tokens = section.split(\"|\")\n\n        folder = section_tokens[0]\n        folder_configs.setdefault(\n            folder,\n            {\"add_modes\": {}, \"del_modes\": {}, \"all_add_modes\": set([]), \"all_del_modes\": set([])},\n        )\n\n        files = None\n        if len(section_tokens) > 1:\n            files = set(section_tokens[1:])\n\n        add_modes = set([])\n        del_modes = set([])\n        for key, value in config.items(section):\n            if key in map_events:\n                if value == \"1\":\n                    add_modes.add(key)\n                else:\n                    del_modes.add(key)\n\n        if files:\n            for mode in add_modes:\n                folder_configs[folder][\"add_modes\"].setdefault(mode, set([]))\n                folder_configs[folder][\"add_modes\"][mode].update(files)\n            for mode in del_modes:\n                folder_configs[folder][\"del_modes\"].setdefault(mode, set([]))\n                folder_configs[folder][\"del_modes\"][mode].update(files)\n        else:\n            folder_configs[folder][\"all_add_modes\"].update(add_modes)\n            folder_configs[folder][\"all_del_modes\"].update(del_modes)\n\n    # Evaluate config\n    for folder, attributes in folder_configs.items():\n        required_modes = set([])\n        for mode in attributes[\"add_modes\"].keys():\n            if mode not in attributes[\"all_del_modes\"]:\n                required_modes.add(mode)\n\n        files_to_monitor = {}  # type: dict[str, set]\n        skip_modes = set([])\n        for mode in required_modes:\n            files_to_monitor.setdefault(mode, set([]))\n            files_to_monitor[mode].update(attributes[\"add_modes\"][mode])\n            files_to_monitor[mode] -= attributes[\"del_modes\"].get(mode, set([]))\n            if not files_to_monitor[mode]:\n                skip_modes.add(mode)\n\n        attributes[\"monitor_files\"] = files_to_monitor\n        attributes[\"monitor_all\"] = attributes[\"all_add_modes\"] - attributes[\"all_del_modes\"]\n        attributes[\"modes\"] = required_modes - skip_modes\n\n        # Determine mask\n        attributes[\"mask\"] = 0\n        for mode in attributes[\"modes\"]:\n            attributes[\"mask\"] |= map_events[mode][0]\n        for mode in attributes[\"monitor_all\"]:\n            attributes[\"mask\"] |= map_events[mode][0]\n\n    update_watched_folders()\n    if opt_foreground:\n        import pprint\n\n        sys.stdout.write(pprint.pformat(folder_configs))\n\n    # Save monitored file/folder information specified in mk_inotify.cfg\n    with open(configured_paths, \"w\") as opened_conf_paths:\n        opened_conf_paths.write(\"\\n\".join(get_watched_files()) + \"\\n\")\n\n    # Event handler\n    eh = NotifyEventHandler()\n    notifier = pyinotify.Notifier(wm, eh)\n\n    # Wake up every few seconds, check heartbeat and write data to disk\n    signal.signal(signal.SIGALRM, wakeup_handler)\n    signal.alarm(write_interval)\n\n    notifier.loop()\n\n\nif __name__ == \"__main__\":\n    main()\n"}
{"type": "source_file", "path": "agents/plugins/mk_sap_2.py", "content": "#!/usr/bin/env python2\n# -*- coding: utf-8 -*-\n# Copyright (C) 2019 Checkmk GmbH - License: GNU General Public License v2\n# This file is part of Checkmk (https://checkmk.com). It is subject to the terms and\n# conditions defined in the file COPYING, which is part of this source code package.\n# Do not test generated 2.x files\n# fmt: off\n# type: ignore\n\nfrom __future__ import division\nfrom __future__ import with_statement\nfrom __future__ import absolute_import\nfrom io import open\n__version__ = \"2.3.0b1\"\n\n# This agent plugin has been built to collect information from SAP R/3 systems\n# using RFC calls. It needs the python module sapnwrfc (available in Checkmk\n# git at agents/sap/sapnwrfc) and the nwrfcsdk (can be downloaded from SAP\n# download portal) installed to be working. You can configure the agent plugin\n# using the configuration file /etc/check_mk/sap.cfg (a sample file can be\n# found in Checkmk git at agents/sap/sap.cfg) to tell it how to connect to\n# your SAP instance and which values you want to fetch from your system to be\n# forwarded to and checked by Checkmk.\n#\n# This current agent has been developed and tested with:\n#   python-sapnwrfc-0.19\n#\n# During development the \"CCMS_Doku.pdf\" was really helpful.\n\n#   #################################\n#   #                               #\n#   #         N  O  T  E            #\n#   #                               #\n#   #################################\n#\n# This plugin will only work with python2 as sapnwrfc is not available\n# for python 3.\n# This plugin will be converted to python2 automatically, and a patch in\n# the bakery plugin will make sure only the python 2 version will be deployed\n# -- such that even on monitored hosts that provide both py3 and py2 the\n# py2 variant is executed.\n#\n# Make sure to remove said patch if the plugin is python 2/3 compatible!\n#\n\nimport ast\nimport datetime\nimport fcntl\nimport fnmatch\nimport os\nimport sys\nimport time\n\ntry:\n    from typing import Any\nexcept ImportError:\n    pass\n\nif sys.version_info[:2] < (3, 5):\n    RecursionError = RuntimeError\n\n# #############################################################################\n\n# This sign is used to separate the path parts given in the config\nSEPARATOR = \"/\"\n\n# This are the different classes of monitoring objects which\n# can be found in the tree.\n#\n# Summarizs information from several subnodes\nMTE_SUMMARY = \"050\"\n# A monitoring object which has several subnodes which lead to the status\n# of this object. For example it is the \"CPU\" object on a host\nMTE_MON_OBJ = \"070\"\n# Contains performance information (which can be used to create graphs from)\nMTE_PERFORMANCE = \"100\"\n# Might contain several messages\nMTE_MSG_CONTAINER = \"101\"\n# Contains a single status message\nMTE_SINGLE_MSG = \"102\"\n# This is a long text label without status\nMTE_LONG_TXT = \"110\"\n# This is a short text label without status\nMTE_SHORT_TXT = \"111\"\n# Is a \"folder\" which has no own state, just computed by its children\nMTE_VIRTUAL = \"199\"\n\n# This map converts between the SAP color codes (key values) and the\n# nagios state codes and strings\nSTATE_VALUE_MAP = {\n    0: (0, \"OK\"),  # GRAY  (inactive or no current info available) -> OK\n    1: (0, \"OK\"),  # GREEN  -> OK\n    2: (1, \"WARN\"),  # YELLOW -> WARNING\n    3: (2, \"CRIT\"),  # RED    -> CRITICAL\n}\n\nSTATE_LOGWATCH_MAP = [\"O\", \"O\", \"W\", \"C\"]\n\n# Monitoring objects of these classes are skipped during processing\nSKIP_MTCLASSES = [\n    MTE_VIRTUAL,\n    MTE_SUMMARY,\n    MTE_MON_OBJ,\n    MTE_SHORT_TXT,\n    MTE_LONG_TXT,\n]\n\nMK_CONFDIR = os.getenv(\"MK_CONFDIR\") or \"/etc/check_mk\"\nMK_VARDIR = os.getenv(\"MK_VARDIR\") or \"/var/lib/check_mk_agent\"\n\nSTATE_FILE = MK_VARDIR + \"/sap.state\"\nstate_file_changed = False\n\n# #############################################################################\n\n# Settings to be used to connect to the SAP R/3 host.\nlocal_cfg = {\n    \"ashost\": \"localhost\",\n    \"sysnr\": \"00\",\n    \"client\": \"100\",\n    \"user\": \"\",\n    \"passwd\": \"\",\n    \"trace\": \"3\",\n    \"loglevel\": \"warn\",\n    #'lang':     'EN',\n    #'host_prefix': 'FOOBAR_',\n}\n\n# A list of strings, while the string must match the full path to one or\n# several monitor objects. We use unix shell patterns during matching, so\n# you can use several chars as placeholders:\n#\n# *      matches everything\n# ?      matches any single character\n# [seq]  matches any character in seq\n# [!seq] matches any character not in seq\n#\n# The * matches the whole following string and does not end on next \"/\".\n# For examples, take a look at the default config file (/etc/check_mk/sap.cfg).\nmonitor_paths = [\n    \"SAP CCMS Monitor Templates/Dialog Overview/*\",\n]\nmonitor_types = []  # type: list[str]\nconfig_file = MK_CONFDIR + \"/sap.cfg\"\n\ncfg = {}  # type: list[dict[Any, Any]] | dict[Any, Any]\nif os.path.exists(config_file):\n    with open(config_file) as opened_file:\n        exec(opened_file.read())\n    if isinstance(cfg, dict):\n        cfg = [cfg]\nelse:\n    cfg = [local_cfg]\n\n# Load the state file into memory\ntry:\n    with open(STATE_FILE) as opened_file:\n        states = ast.literal_eval(opened_file.read())\nexcept IOError:\n    states = {}\n\n# index of all logfiles which have been found in a run. This is used to\n# remove logfiles which are not available anymore from the states dict.\nlogfiles = []\nconn = None\n\n# #############################################################################\n\n#\n# HELPERS\n#\n\n\nclass SapError(Exception):\n    pass\n\n\ndef to_be_monitored(path, toplevel_match=False):\n    for rule in monitor_paths:\n        if toplevel_match and rule.count(\"/\") > 1:\n            rule = \"/\".join(rule.split(\"/\")[:2])\n\n        if fnmatch.fnmatch(path, rule):\n            return True\n    return False\n\n\ndef node_path(tree, node, path=\"\"):\n    if path:\n        path = node[\"MTNAMESHRT\"].rstrip() + SEPARATOR + path\n    else:\n        path = node[\"MTNAMESHRT\"].rstrip()\n\n    if node[\"ALPARINTRE\"] > 0:\n        parent_node = tree[node[\"ALPARINTRE\"] - 1]\n        return node_path(tree, parent_node, path)\n    return path\n\n\n#\n# API ACCESS FUNCTIONS\n#\n\n\ndef query(what, params, debug=False):\n    if conn:\n        fd = conn.discover(what)\n\n    if debug:\n        sys.stdout.write(\"Name: %s Params: %s\\n\" % (fd.name, fd.handle.parameters))\n        sys.stdout.write(\"Given-Params: %s\\n\" % params)\n\n    f = fd.create_function_call()\n    for param_key, val in params.items():\n        getattr(f, param_key)(val)\n    f.invoke()\n\n    ret = f.RETURN.value\n    if ret[\"TYPE\"] == \"E\":\n        sys.stderr.write(\"ERROR: %s\\n\" % ret[\"MESSAGE\"].strip())\n\n    return f\n\n\ndef login():\n    f = query(\n        \"BAPI_XMI_LOGON\",\n        {\n            \"EXTCOMPANY\": \"Mathias Kettner GmbH\",\n            \"EXTPRODUCT\": \"Check_MK SAP Agent\",\n            \"INTERFACE\": \"XAL\",\n            \"VERSION\": \"1.0\",\n        },\n    )\n    # sys.stdout.write(\"%s\\n\" % f.RETURN)\n    return f.SESSIONID.value\n\n\ndef logout():\n    query(\n        \"BAPI_XMI_LOGOFF\",\n        {\n            \"INTERFACE\": \"XAL\",\n        },\n    )\n\n\ndef mon_list(cfg_entry):\n    f = query(\n        \"BAPI_SYSTEM_MON_GETLIST\",\n        {\n            \"EXTERNAL_USER_NAME\": cfg_entry[\"user\"],\n        },\n    )\n    l = []\n    for mon in f.MONITOR_NAMES.value:\n        l.append((mon[\"MS_NAME\"].rstrip(), mon[\"MONI_NAME\"].rstrip()))\n    return l\n\n\n# def ms_list( cfg ):\n#    f = query(\"BAPI_SYSTEM_MS_GETLIST\", {\n#        'EXTERNAL_USER_NAME': cfg['user'],\n#    })\n#    l = []\n#    for ms in f.MONITOR_SETS.value:\n#        l.append(ms['NAME'].rstrip())\n#    return l\n\n\ndef mon_tree(cfg_entry, ms_name, mon_name):\n    f = query(\n        \"BAPI_SYSTEM_MON_GETTREE\",\n        {\n            \"EXTERNAL_USER_NAME\": cfg_entry[\"user\"],\n            \"MONITOR_NAME\": {\"MS_NAME\": ms_name, \"MONI_NAME\": mon_name},\n        },\n    )\n    tree = f.TREE_NODES.value\n    for node in tree:\n        try:\n            node[\"PATH\"] = ms_name + SEPARATOR + node_path(tree, node)\n        except RecursionError:\n            raise SapError(\n                (\n                    \"Could not calculate path, recursion limit reached. \"\n                    \"Reorganise your SAP data to get past this error. \"\n                    \"Element that causes this: {node}\"\n                ).format(node=node)\n            )\n    return tree\n\n\ndef tid(node):\n    return {\n        \"MTSYSID\": node[\"MTSYSID\"].strip(),\n        \"MTMCNAME\": node[\"MTMCNAME\"].strip(),\n        \"MTNUMRANGE\": node[\"MTNUMRANGE\"].strip(),\n        \"MTUID\": node[\"MTUID\"].strip(),\n        \"MTCLASS\": node[\"MTCLASS\"].strip(),\n        \"MTINDEX\": node[\"MTINDEX\"].strip(),\n        \"EXTINDEX\": node[\"EXTINDEX\"].strip(),\n    }\n\n\ndef mon_perfdata(cfg_entry, node):\n    f = query(\n        \"BAPI_SYSTEM_MTE_GETPERFCURVAL\",\n        {\n            \"EXTERNAL_USER_NAME\": cfg_entry[\"user\"],\n            \"TID\": tid(node),\n        },\n    )\n    value = f.CURRENT_VALUE.value[\"LASTPERVAL\"]\n\n    f = query(\n        \"BAPI_SYSTEM_MTE_GETPERFPROP\",\n        {\n            \"EXTERNAL_USER_NAME\": cfg_entry[\"user\"],\n            \"TID\": tid(node),\n        },\n    )\n    if f.PROPERTIES.value[\"DECIMALS\"] != 0:\n        value = (value + 0.0) / 10 ** f.PROPERTIES.value[\"DECIMALS\"]\n    uom = f.PROPERTIES.value[\"VALUNIT\"].strip()\n\n    return value, uom\n\n\ndef mon_msg(cfg_entry, node):\n    f = query(\n        \"BAPI_SYSTEM_MTE_GETSMVALUE\",\n        {\n            \"EXTERNAL_USER_NAME\": cfg_entry[\"user\"],\n            \"TID\": tid(node),\n        },\n    )\n    data = f.VALUE.value\n    dt = parse_dt(data[\"SMSGDATE\"], data[\"SMSGTIME\"])\n    return (dt, data[\"MSG\"].strip())\n\n\ndef parse_dt(d, t):\n    d = d.strip()\n    t = t.strip()\n    if not d or not t:\n        return None\n    return datetime.datetime(*time.strptime(d + t, \"%Y%m%d%H%M%S\")[:6])\n\n\ndef mon_alerts(cfg_entry, node):\n    f = query(\n        \"BAPI_SYSTEM_MTE_GETALERTS\",\n        {\n            \"EXTERNAL_USER_NAME\": cfg_entry[\"user\"],\n            \"TID\": tid(node),\n        },\n    )\n    return f.ALERTS.value\n\n\ndef aid(alert):\n    return {\n        \"ALSYSID\": alert[\"ALSYSID\"],\n        \"MSEGNAME\": alert[\"MSEGNAME\"],\n        \"ALUNIQNUM\": alert[\"ALUNIQNUM\"],\n        \"ALINDEX\": alert[\"ALINDEX\"],\n        \"ALERTDATE\": alert[\"ALERTDATE\"],\n        \"ALERTTIME\": alert[\"ALERTTIME\"],\n    }\n\n\ndef alert_details(cfg_entry, alert):\n    f = query(\n        \"BAPI_SYSTEM_ALERT_GETDETAILS\",\n        {\n            \"EXTERNAL_USER_NAME\": cfg_entry[\"user\"],\n            \"AID\": aid(alert),\n        },\n    )\n    # prop  = f.PROPERTIES.value\n    state = f.VALUE.value\n    msg = f.XMI_EXT_MSG.value[\"MSG\"].strip()\n    return state, msg\n\n\ndef process_alerts(cfg_entry, logs, ms_name, mon_name, node, alerts):\n    global state_file_changed\n\n    sid = node[\"MTSYSID\"].strip() or \"Other\"\n    context = node[\"MTMCNAME\"].strip() or \"Other\"\n    path = node[\"PATH\"]\n\n    # Use the sid as hostname for the logs\n    hostname = sid\n    logfile = context + \"/\" + path\n\n    logfiles.append((hostname, logfile))\n\n    logs.setdefault(sid, {})\n    logs[hostname][logfile] = []\n    newest_log_dt = None\n    for alert in alerts:\n        dt = parse_dt(alert[\"ALERTDATE\"], alert[\"ALERTTIME\"])\n\n        if (hostname, logfile) in states and states[(hostname, logfile)] >= dt:\n            continue  # skip log messages which are older than the last cached date\n\n        if not newest_log_dt or dt > newest_log_dt:\n            newest_log_dt = dt  # store the newest log of this run\n\n        alert_state, alert_msg = alert_details(cfg_entry, alert)\n        # Format lines to \"logwatch\" format\n        logs[hostname][logfile].append(\n            \"%s %s %s\"\n            % (\n                STATE_LOGWATCH_MAP[alert_state[\"VALUE\"]],\n                dt.strftime(\"%Y-%m-%d %H:%M:%S\"),\n                alert_msg,\n            )\n        )\n\n    if newest_log_dt:\n        # Write newest log age to cache to prevent double processing of logs\n        states[(hostname, logfile)] = newest_log_dt\n        state_file_changed = True\n    return logs\n\n\ndef check(sapnwrfc, cfg_entry):\n    global conn\n    conn = sapnwrfc.base.rfc_connect(cfg_entry)\n    login()\n\n    logs = {}  # type: dict[str, dict[str, list]]\n    sap_data = {}  # type: dict[str, list]\n\n    # This loop is used to collect all information from SAP\n    for ms_name, mon_name in mon_list(cfg_entry):\n        path = ms_name + SEPARATOR + mon_name\n        if not to_be_monitored(path, True):\n            continue\n\n        tree = mon_tree(cfg_entry, ms_name, mon_name)\n        for node in tree:\n            if not to_be_monitored(node[\"PATH\"]):\n                continue\n            # sys.stdout.write(\"%s\\n\" % node[\"PATH\"])\n\n            status_details = \"\"  # type: str | tuple[str, Any]\n            perfvalue = \"-\"\n            uom = \"-\"\n\n            # Use precalculated states\n            state = {\n                \"VALUE\": node[\"ACTUALVAL\"],\n                \"SEVERITY\": node[\"ACTUALSEV\"],\n            }\n\n            if state[\"VALUE\"] not in STATE_VALUE_MAP:\n                sys.stdout.write(\"UNHANDLED STATE VALUE\\n\")\n                sys.exit(1)\n\n            #\n            # Handle different object classes individually\n            # to get details about them\n            #\n\n            if monitor_types and node[\"MTCLASS\"] not in monitor_types:\n                continue  # Skip unwanted classes if class filtering is enabled\n\n            if node[\"MTCLASS\"] == MTE_PERFORMANCE:\n                perfvalue, this_uom = mon_perfdata(cfg_entry, node)\n                uom = this_uom if this_uom else uom\n\n            elif node[\"MTCLASS\"] == MTE_SINGLE_MSG:\n                status_details = \"%s: %s\" % mon_msg(cfg_entry, node)\n\n            elif node[\"MTCLASS\"] == MTE_MSG_CONTAINER:\n                alerts = mon_alerts(cfg_entry, node)\n                logs = process_alerts(cfg_entry, logs, ms_name, mon_name, node, alerts)\n                if len(alerts) > 0:\n                    last_alert = alerts[-1]\n                    dt = parse_dt(last_alert[\"ALERTDATE\"], last_alert[\"ALERTTIME\"])\n                    alert_state, alert_msg = alert_details(cfg_entry, last_alert)\n                    last_msg = \"%s: %s - %s\" % (\n                        dt,\n                        STATE_VALUE_MAP[alert_state[\"VALUE\"]][1],\n                        alert_msg,\n                    )\n\n                    status_details = \"%d Messages, Last: %s\" % (len(alerts), last_msg)\n                else:\n                    status_details = \"The log is empty\"\n\n            elif node[\"MTCLASS\"] not in SKIP_MTCLASSES:\n                # Add an error to output on unhandled classes\n                status_details = \"UNHANDLED MTCLASS\", node[\"MTCLASS\"]\n\n            if node[\"MTCLASS\"] not in SKIP_MTCLASSES:\n                sid = node[\"MTSYSID\"].strip() or \"Other\"\n                context = node[\"MTMCNAME\"].strip() or \"Other\"\n                path = node[\"PATH\"]\n\n                sap_data.setdefault(sid, [])\n                sap_data[sid].append(\n                    \"%s\\t%d\\t%3d\\t%s\\t%s\\t%s\\t%s\"\n                    % (\n                        context,\n                        state[\"VALUE\"],\n                        state[\"SEVERITY\"],\n                        path,\n                        perfvalue,\n                        uom,\n                        status_details,\n                    )\n                )\n\n    for host, host_sap in sap_data.items():\n        sys.stdout.write(\"<<<<%s%s>>>>\\n\" % (cfg_entry.get(\"host_prefix\", \"\"), host))\n        sys.stdout.write(\"<<<sap:sep(9)>>>\\n\")\n        sys.stdout.write(\"%s\\n\" % \"\\n\".join(host_sap))\n    sys.stdout.write(\"<<<<>>>>\\n\")\n\n    for host, host_logs in logs.items():\n        sys.stdout.write(\"<<<<%s>>>>\\n\" % host)\n        sys.stdout.write(\"<<<logwatch>>>\\n\")\n        for log, lines in host_logs.items():\n            sys.stdout.write(\"[[[%s]]]\\n\" % log)\n            if lines:\n                sys.stdout.write(\"\\n\".join(lines) + \"\\n\")\n        sys.stdout.write(\"<<<<>>>>\\n\")\n\n    logout()\n    conn.close()\n\n\ndef main():\n    global state_file_changed\n\n    # sapnwrfc needs to know where the libs are located. During\n    # development the import failed, since the module did not\n    # find the libraries. So we preload the library to have it\n    # already loaded.\n    try:\n        import sapnwrfc  # type: ignore[import]\n    except ImportError, e:\n        if \"sapnwrfc.so\" in str(e):\n            sys.stderr.write(\n                \"Unable to find the library sapnwrfc.so. Maybe you need to put a file pointing to\\n\"\n                \"the sapnwrfc library directory into the /etc/ld.so.conf.d directory. For example\\n\"\n                \"create the file /etc/ld.so.conf.d/sapnwrfc.conf containing the path\\n\"\n                '\"/usr/sap/nwrfcsdk/lib\" and run \"ldconfig\" afterwards.\\n'\n            )\n            sys.exit(1)\n        elif \"No module named sapnwrfc\" in str(e):\n            sys.stderr.write(\"Missing the Python module sapnwfrc.\\n\")\n            sys.exit(1)\n        else:\n            raise\n\n    # It is possible to configure multiple SAP instances to monitor. Loop them all, but\n    # do not terminate when one connection failed\n    processed_all = True\n    try:\n        for entry in cfg:\n            try:\n                check(sapnwrfc, entry)\n                sys.stdout.write(\"<<<sap_state:sep(9)>>>\\n%s\\tOK\\n\" % entry[\"ashost\"])\n            except sapnwrfc.RFCCommunicationError, e:\n                sys.stderr.write(\"ERROR: Unable to connect (%s)\\n\" % e)\n                sys.stdout.write(\n                    \"<<<sap_state:sep(9)>>>\\n%s\\tUnable to connect (%s)\\n\" % (entry[\"ashost\"], e)\n                )\n                processed_all = False\n            except SapError, e:\n                sys.stderr.write(\"ERROR: %s\\n\" % e)\n                sys.stdout.write(\"<<<sap_state:sep(9)>>>\\n%s\\t%s\\n\" % (entry[\"ashost\"], e))\n                processed_all = False\n            except Exception, e:\n                sys.stderr.write(\"ERROR: Unhandled exception (%s)\\n\" % e)\n                sys.stdout.write(\n                    \"<<<sap_state:sep(9)>>>\\n%s\\tUnhandled exception (%s)\\n\" % (entry[\"ashost\"], e)\n                )\n                processed_all = False\n\n        # Now check whether or not an old logfile needs to be removed. This can only\n        # be done this way, when all hosts have been reached. Otherwise the cleanup\n        # is skipped.\n        if processed_all:\n            for key in states.keys():\n                if key not in logfiles:\n                    state_file_changed = True\n                    del states[key]\n\n        # Only write the state file once per run. And only when it has been changed\n        if state_file_changed:\n            new_file = STATE_FILE + \".new\"\n            state_fd = os.open(new_file, os.O_WRONLY | os.O_CREAT)\n            fcntl.flock(state_fd, fcntl.LOCK_EX)\n            os.write(state_fd, repr(states).encode(\"utf-8\"))\n            os.close(state_fd)\n            os.rename(STATE_FILE + \".new\", STATE_FILE)\n\n    except Exception, e:\n        sys.stderr.write(\"ERROR: Unhandled exception (%s)\\n\" % e)\n\n    sys.exit(0)\n\n\nif __name__ == \"__main__\":\n    main()\n"}
{"type": "source_file", "path": "agents/wnx/scripts/check_crlf.py", "content": "#!/usr/bin/env python3\n# Copyright (C) 2019 Checkmk GmbH - License: GNU General Public License v2\n# This file is part of Checkmk (https://checkmk.com). It is subject to the terms and\n# conditions defined in the file COPYING, which is part of this source code package.\n\n# Simple python script to find that file line ending is correctly encoded\n# Very Simple.\n\nimport sys\n\nwith open(\"install\\\\resources\\\\check_mk.user.yml\", \"rb\") as f:\n    content = f.read()\n    if content.count(b\"\\r\\n\") < 10:\n        sys.exit(1)\n\nsys.exit(0)\n"}
{"type": "source_file", "path": "buildscripts/scripts/assert_build_artifacts.py", "content": "#!/usr/bin/env python3\n# Copyright (C) 2023 Checkmk GmbH - License: GNU General Public License v2\n# This file is part of Checkmk (https://checkmk.com). It is subject to the terms and\n# conditions defined in the file COPYING, which is part of this source code package.\n\nimport sys\nfrom argparse import ArgumentParser\nfrom argparse import Namespace as Args\nfrom collections.abc import Iterator\nfrom pathlib import Path\n\nimport requests\n\nsys.path.insert(0, Path(__file__).parent.parent.parent.as_posix())\nfrom tests.testlib.package_manager import ABCPackageManager, code_name\n\nfrom cmk.ccc.version import Edition\n\nfrom buildscripts.scripts.lib.common import flatten, load_editions_file\nfrom buildscripts.scripts.lib.registry import (\n    Credentials,\n    DockerImage,\n    edition_to_registry,\n    get_credentials,\n    get_default_registries,\n    Registry,\n)\n\n\ndef hash_file(artifact_name: str) -> str:\n    return f\"{artifact_name}.hash\"\n\n\ndef build_source_artifacts(args: Args, loaded_yaml: dict) -> Iterator[tuple[str, bool]]:\n    for edition in loaded_yaml[\"editions\"]:\n        file_name = (\n            f\"check-mk-{edition}-{args.version}.{Edition.from_long_edition(edition).short}.tar.gz\"\n        )\n        internal_only = edition in loaded_yaml[\"internal_editions\"]\n        yield file_name, internal_only\n        yield hash_file(file_name), internal_only\n\n\ndef build_docker_artifacts(args: Args, loaded_yaml: dict) -> Iterator[tuple[str, bool]]:\n    for edition in loaded_yaml[\"editions\"]:\n        file_name = f\"check-mk-{edition}-docker-{args.version}.tar.gz\"\n        internal_only = edition in loaded_yaml[\"internal_editions\"]\n        yield file_name, internal_only\n        yield hash_file(file_name), internal_only\n\n\ndef build_docker_image_name_and_registry(\n    args: Args, loaded_yaml: dict, registries: list[Registry]\n) -> Iterator[tuple[DockerImage, str, Registry]]:\n    def build_folder(ed: str) -> str:\n        # TODO: Merge with build-cmk-container.py\n        match ed:\n            case \"raw\" | \"cloud\" | \"managed\":\n                return \"checkmk/\"\n            case \"enterprise\":\n                return f\"{ed}/\"\n            case \"saas\":\n                return \"\"\n            case _:\n                raise RuntimeError(f\"Unknown edition {ed}\")\n\n    for edition in loaded_yaml[\"editions\"]:\n        registry = edition_to_registry(edition, registries)\n        yield (\n            DockerImage(tag=args.version, image_name=f\"{build_folder(edition)}check-mk-{edition}\"),\n            edition,\n            registry,\n        )\n\n\ndef build_package_artifacts(args: Args, loaded_yaml: dict) -> Iterator[tuple[str, bool]]:\n    for edition in loaded_yaml[\"editions\"]:\n        for distro in flatten(loaded_yaml[\"editions\"][edition][args.use_case]):\n            package_name = ABCPackageManager.factory(code_name(distro)).package_name(\n                Edition.from_long_edition(edition), version=args.version\n            )\n            internal_only = (\n                distro in loaded_yaml.get(\"internal_distros\", [])\n                or edition in loaded_yaml[\"internal_editions\"]\n            )\n            yield package_name, internal_only\n            yield hash_file(package_name), internal_only\n\n\ndef file_exists_on_download_server(filename: str, version: str, credentials: Credentials) -> bool:\n    url = f\"https://download.checkmk.com/checkmk/{version}/{filename}\"\n    sys.stdout.write(f\"Checking for {url}...\")\n    if (\n        requests.head(\n            f\"https://download.checkmk.com/checkmk/{version}/{filename}\",\n            auth=(credentials.username, credentials.password),\n            timeout=10,\n        ).status_code\n        != 200\n    ):\n        sys.stdout.write(\" MISSING\\n\")\n        return False\n    sys.stdout.write(\" AVAILABLE\\n\")\n    return True\n\n\ndef assert_presence_on_download_server(\n    args: Args, internal_only: bool, artifact_name: str, credentials: Credentials\n) -> None:\n    if (\n        not file_exists_on_download_server(artifact_name, args.version, credentials)\n        != internal_only\n    ):\n        raise RuntimeError(\n            f\"{artifact_name} should {'not' if internal_only else ''} \"\n            \"be available on download server!\"\n        )\n\n\ndef assert_build_artifacts(args: Args, loaded_yaml: dict) -> None:\n    credentials = get_credentials()\n    registries = get_default_registries()\n\n    for artifact_name, internal_only in build_source_artifacts(args, loaded_yaml):\n        assert_presence_on_download_server(args, internal_only, artifact_name, credentials)\n\n    for artifact_name, internal_only in build_package_artifacts(args, loaded_yaml):\n        assert_presence_on_download_server(args, internal_only, artifact_name, credentials)\n\n    for artifact_name, internal_only in build_docker_artifacts(args, loaded_yaml):\n        assert_presence_on_download_server(args, internal_only, artifact_name, credentials)\n\n    for image_name, edition, registry in build_docker_image_name_and_registry(\n        args, loaded_yaml, registries\n    ):\n        if not registry.image_exists(image_name, edition):\n            raise RuntimeError(f\"{image_name} not found!\")\n\n    # cloud images\n    # TODO\n\n\ndef parse_arguments() -> Args:\n    parser = ArgumentParser()\n\n    parser.add_argument(\"--editions_file\", required=True)\n\n    subparsers = parser.add_subparsers(required=True, dest=\"command\")\n\n    sub_assert_build_artifacts = subparsers.add_parser(\"assert_build_artifacts\")\n    sub_assert_build_artifacts.set_defaults(func=assert_build_artifacts)\n    sub_assert_build_artifacts.add_argument(\"--version\", required=True, default=False)\n    sub_assert_build_artifacts.add_argument(\"--use_case\", required=False, default=\"release\")\n\n    return parser.parse_args()\n\n\ndef main() -> None:\n    args = parse_arguments()\n    args.func(args, load_editions_file(args.editions_file))\n\n\nif __name__ == \"__main__\":\n    main()\n"}
{"type": "source_file", "path": "agents/__init__.py", "content": "#!/usr/bin/env python3\n# Copyright (C) 2019 Checkmk GmbH - License: GNU General Public License v2\n# This file is part of Checkmk (https://checkmk.com). It is subject to the terms and\n# conditions defined in the file COPYING, which is part of this source code package.\n\n# This file is only here to make pytest and mypy find the agent plugins.\n# It will not be shipped nor synced into the site. It must not contain code.\n"}
{"type": "source_file", "path": "agents/plugins/nginx_status.py", "content": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n# Copyright (C) 2019 Checkmk GmbH - License: GNU General Public License v2\n# This file is part of Checkmk (https://checkmk.com). It is subject to the terms and\n# conditions defined in the file COPYING, which is part of this source code package.\n\n# Checkmk-Agent-Plug-in - Nginx Server Status\n#\n# Fetches the stub nginx_status page from detected or configured nginx\n# processes to gather status information about this process.\n#\n# Take a look at the check man page for details on how to configure this\n# plugin and check.\n#\n# By default this plugin tries to detect all locally running processes\n# and to monitor them. If this is not good for your environment you might\n# create an nginx_status.cfg file in MK_CONFDIR and populate the servers\n# list to prevent executing the detection mechanism.\n\nimport os\nimport re\nimport sys\nfrom urllib.error import HTTPError, URLError\nfrom urllib.request import Request, urlopen\n\n__version__ = \"2.5.0b1\"\n\nUSER_AGENT = \"checkmk-agent-nginx_status-\" + __version__\n\nif sys.version_info < (2, 6):\n    sys.stderr.write(\"ERROR: Python 2.5 is not supported. Please use Python 2.6 or newer.\\n\")\n    sys.exit(1)\n\nif sys.version_info[0] == 2:\n    import urllib2\n\n    urllib2.getproxies = lambda: {}\nelse:\n    import urllib\n\n    urllib.getproxies = lambda: {}  # type: ignore[attr-defined]\n\nPY2 = sys.version_info[0] == 2\nPY3 = sys.version_info[0] == 3\n\nif PY3:\n    text_type = str\n    binary_type = bytes\nelse:\n    text_type = unicode  # noqa: F821\n    binary_type = str\n\n\n# Borrowed from six\ndef ensure_str(s, encoding=\"utf-8\", errors=\"strict\"):\n    \"\"\"Coerce *s* to `str`.\n\n    For Python 2:\n      - `unicode` -> encoded to `str`\n      - `str` -> `str`\n\n    For Python 3:\n      - `str` -> `str`\n      - `bytes` -> decoded to `str`\n    \"\"\"\n    if not isinstance(s, (text_type, binary_type)):\n        raise TypeError(\"not expecting type '%s'\" % type(s))\n    if PY2 and isinstance(s, text_type):\n        s = s.encode(encoding, errors)\n    elif PY3 and isinstance(s, binary_type):\n        s = s.decode(encoding, errors)\n    return s\n\n\ndef try_detect_servers(ssl_ports):\n    pids = []\n    results = []\n    for netstat_line in os.popen(\"netstat -tlnp 2>/dev/null\").readlines():\n        parts = netstat_line.split()\n        # Skip lines with wrong format\n        if len(parts) < 7 or \"/\" not in parts[6]:\n            continue\n\n        pid, proc = parts[6].split(\"/\", 1)\n        to_replace = re.compile(\"^.*/\")\n        proc = to_replace.sub(\"\", proc)\n\n        procs = [\"nginx\", \"nginx:\", \"nginx.conf\"]\n        # the pid/proc field length is limited to 19 chars. Thus in case of\n        # long PIDs, the process names are stripped of by that length.\n        # Workaround this problem here\n        procs = [p[: 19 - len(pid) - 1] for p in procs]\n\n        # Skip unwanted processes\n        if proc not in procs:\n            continue\n\n        # Add only the first found port of a single server process\n        if pid in pids:\n            continue\n        pids.append(pid)\n\n        server_proto = \"http\"\n        server_address, _server_port = parts[3].rsplit(\":\", 1)\n        server_port = int(_server_port)\n\n        # Use localhost when listening globally\n        if server_address == \"0.0.0.0\":\n            server_address = \"127.0.0.1\"\n        elif server_address == \"::\":\n            server_address = \"::1\"\n\n        # Switch protocol if port is SSL port. In case you use SSL on another\n        # port you would have to change/extend the ssl_port list\n        if server_port in ssl_ports:\n            server_proto = \"https\"\n\n        results.append((server_proto, server_address, server_port))\n\n    return results\n\n\ndef main():\n    config_dir = os.getenv(\"MK_CONFDIR\", \"/etc/check_mk\")\n    config_file = config_dir + \"/nginx_status.cfg\"\n\n    config = {}  # type: dict\n    if os.path.exists(config_file):\n        with open(config_file) as open_config_file:\n            config_src = open_config_file.read()\n            exec(config_src, globals(), config)  # nosec B102 # BNS:a29406\n    # None or list of (proto, ipaddress, port) tuples.\n    # proto is 'http' or 'https'\n    servers = config.get(\"servers\")\n    ssl_ports = config.get(\"ssl_ports\", [443])\n\n    if servers is None:\n        servers = try_detect_servers(ssl_ports)\n\n    if not servers:\n        sys.exit(0)\n\n    sys.stdout.write(\"<<<nginx_status>>>\\n\")\n    for server in servers:\n        if isinstance(server, tuple):\n            proto, address, port = server\n            page = \"nginx_status\"\n        else:\n            proto = server[\"protocol\"]\n            address = server[\"address\"]\n            port = server[\"port\"]\n            page = server.get(\"page\", \"nginx_status\")\n\n        try:\n            if proto not in [\"http\", \"https\"]:\n                raise ValueError(\"Scheme '%s' is not allowed\" % proto)\n\n            url = \"%s://%s:%s/%s\" % (proto, address, port, page)\n            # Try to fetch the status page for each server\n            try:\n                request = Request(url, headers={\"Accept\": \"text/plain\", \"User-Agent\": USER_AGENT})\n                fd = urlopen(request)  # nosec B310 # BNS:6b61d9\n            except URLError as e:\n                if \"SSL23_GET_SERVER_HELLO:unknown protocol\" in str(e):\n                    # HACK: workaround misconfigurations where port 443 is used for\n                    # serving non ssl secured http\n                    url = \"http://%s:%s/%s\" % (address, port, page)\n                    fd = urlopen(url)  # nosec B310 # BNS:6b61d9\n                else:\n                    raise\n\n            for line in ensure_str(fd.read()).split(\"\\n\"):\n                if not line or line.isspace():\n                    continue\n                if line.lstrip()[0] == \"<\":\n                    # seems to be html output. Skip this server.\n                    break\n                sys.stdout.write(\"%s %s %s\\n\" % (address, port, line))\n        except HTTPError as e:\n            sys.stderr.write(\"HTTP-Error (%s:%d): %s %s\\n\" % (address, port, e.code, e))\n\n        except Exception as e:\n            sys.stderr.write(\"Exception (%s:%d): %s\\n\" % (address, port, e))\n\n\nif __name__ == \"__main__\":\n    main()\n"}
{"type": "source_file", "path": "buildscripts/scripts/build-cmk-container.py", "content": "#!/usr/bin/env python3\n# Copyright (C) 2023 Checkmk GmbH - License: GNU General Public License v2\n# This file is part of Checkmk (https://checkmk.com). It is subject to the terms and\n# conditions defined in the file COPYING, which is part of this source code package.\n\n\"\"\"Build the CMK container\n\nUsage:\n\nBuild .tar.gz file\nmay require the following env variables\n- DOCKER_USERNAME=carl.lama\n- DOCKER_PASSPHRASE=eatingHands\n\nscripts/run-uvenv python \\\nbuildscripts/scripts/build-cmk-container.py \\\n--branch=master \\\n--edition=enterprise \\\n--version=2023.10.17 \\\n--source_path=$PWD/download/2023.10.17 \\\n--action=build \\\n-vvvv\n\n(Down)load .tar.gz file\nmay require the following env variables\n- RELEASE_KEY=/path/to/id_rsa\n- INTERNAL_DEPLOY_PORT=42\n- INTERNAL_DEPLOY_DEST=user@some-domain.tld:/path/\n\nscripts/run-uvenv python \\\nbuildscripts/scripts/build-cmk-container.py \\\n--branch=2.2.0 \\\n--edition=enterprise \\\n--version=2.2.0p16 \\\n--version_rc_aware=2.2.0p16-rc3 \\\n--source_path=$PWD/download/2.2.0p16-rc3 \\\n--action=load \\\n-vvvv\n\"\"\"\n\nimport argparse\nimport gzip\nimport logging\nimport os\nimport re\nimport subprocess\nimport sys\nimport tarfile\nfrom pathlib import Path\nfrom shutil import rmtree\nfrom tempfile import mkdtemp\n\nimport docker  # type: ignore[import-untyped]\n\nsys.path.insert(0, Path(__file__).parent.parent.parent.as_posix())\nfrom buildscripts.scripts.lib.common import cwd, strtobool\n\n\ndef parse_arguments() -> argparse.Namespace:\n    parser = argparse.ArgumentParser(description=__doc__)\n    parser.add_argument(\n        \"-v\",\n        \"--verbose\",\n        action=\"count\",\n        default=0,\n        help=\"Verbose mode (for even more output use -vvvv)\",\n    )\n\n    parser.add_argument(\n        \"--branch\",\n        required=True,\n        help=\"Branch to build with, e.g. '2.1.0', 'master', 'sandbox-user.name-lower-chars-only'\",\n    )\n    parser.add_argument(\n        \"--edition\",\n        required=True,\n        choices=[\"raw\", \"enterprise\", \"managed\", \"cloud\", \"saas\"],\n        help=\"Checkmk edition to build\",\n    )\n    parser.add_argument(\"--version\", required=True, help=\"Version to build e.g. '2023.10.19'\")\n    parser.add_argument(\n        \"--source_path\", required=True, help=\"Full path to downloaded tar.gz and deb files\"\n    )\n    parser.add_argument(\n        \"--action\",\n        required=True,\n        choices=[\"build\", \"load\", \"push\", \"check_local\"],\n        help=\"Action to perform\",\n    )\n    parser.add_argument(\n        \"--version_rc_aware\",\n        required=parser.parse_known_args()[0].action in [\"load\", \"check_local\"],\n        help=\"RC aware version to load or check e.g. '2.2.0p16-rc3'\",\n    )\n\n    parser.add_argument(\n        \"--set_latest_tag\",\n        type=strtobool,\n        nargs=\"?\",\n        const=True,\n        default=False,\n        help=\"Flag to set/push 'latest' tag to build image\",\n    )\n    parser.add_argument(\n        \"--set_branch_latest_tag\",\n        type=strtobool,\n        nargs=\"?\",\n        const=True,\n        default=False,\n        help=\"Flag to set/push 'BRANCHNAME-latest' tag to build image\",\n    )\n    parser.add_argument(\n        \"--no_cache\",\n        type=strtobool,\n        nargs=\"?\",\n        const=True,\n        default=False,\n        help=\"Flag to build image without docker cache\",\n    )\n    parser.add_argument(\n        \"--image_cmk_base\", help=\"Custom CMK base image, defaults to checked in IMAGE_CMK_BASE\"\n    )\n\n    return parser.parse_args()\n\n\ndef run_cmd(\n    cmd: list[str] | str,\n    raise_exception: bool = True,\n    print_stdout: bool = True,\n) -> subprocess.CompletedProcess:\n    completed_process = subprocess.run(cmd, encoding=\"utf-8\", capture_output=True, check=False)\n    if raise_exception and completed_process.returncode != 0:\n        raise Exception(\n            f\"Failed to execute command '{' '.join(cmd)}' with: {completed_process.stdout}, {completed_process.stderr}\"\n        )\n\n    if print_stdout:\n        LOG.debug(completed_process.stdout.strip())\n\n    return completed_process\n\n\nlogging.basicConfig(\n    format=\"[%(asctime)s] [%(levelname)-8s] [%(funcName)-15s:%(lineno)4s] %(message)s\",\n    level=logging.WARNING,\n)\nLOG = logging.getLogger(__name__)\n\nbase_path = Path.cwd() / \"tmp\"\nbase_path.mkdir(parents=True, exist_ok=True)\ntmp_path = mkdtemp(dir=base_path, suffix=\".cmk-docker\")\nassert Path(tmp_path).exists()\n\ndocker_client = docker.from_env(timeout=1200)\n\n\ndef cleanup() -> None:\n    \"\"\"Cleanup\"\"\"\n    LOG.info(\"Remove temporary directory '%s'\", tmp_path)\n    rmtree(tmp_path)\n\n\ndef docker_tag(\n    args: argparse.Namespace,\n    version_tag: str,\n    registry: str,\n    folder: str,\n    target_version: str | None = None,\n) -> None:\n    \"\"\"Tag docker image\"\"\"\n    source_tag = f\"checkmk/check-mk-{args.edition}:{version_tag}\"\n    this_repository = f\"{registry}{folder}/check-mk-{args.edition}\"\n\n    if target_version is None:\n        target_version = version_tag\n\n    LOG.info(\"Creating tag ...\")\n    LOG.debug(\"target_version: %s\", target_version)\n    LOG.debug(\"args: %s\", args)\n    LOG.debug(\"version_tag: %s\", version_tag)\n    LOG.debug(\"registry: %s\", registry)\n    LOG.debug(\"folder: %s\", folder)\n    LOG.debug(\"target_version: %s\", target_version)\n\n    LOG.debug(\"Getting tagged image: %s\", source_tag)\n    image = docker_client.images.get(source_tag)\n    LOG.debug(\"this image: %s\", image)\n\n    LOG.debug(\"placing new tag, repo: %s, tag: %s\", this_repository, target_version)\n    image.tag(\n        repository=this_repository,\n        tag=target_version,\n    )\n    LOG.debug(\"Done\")\n\n    if args.set_branch_latest_tag:\n        LOG.info(\"Create '%s-latest' tag ...\", args.branch)\n        LOG.debug(\"placing new tag, repo: %s, tag: %s-latest\", this_repository, args.branch)\n        image.tag(\n            repository=this_repository,\n            tag=f\"{args.branch}-latest\",\n        )\n        LOG.debug(\"Done\")\n    else:\n        LOG.info(\"Create 'daily' tag ...\")\n        LOG.debug(\"placing new tag, repo: %s, tag: %s-daily\", this_repository, args.branch)\n        image.tag(\n            repository=this_repository,\n            tag=f\"{args.branch}-daily\",\n        )\n        LOG.debug(\"Done\")\n\n    if args.set_latest_tag:\n        LOG.info(\"Create 'latest' tag ...\")\n\n        LOG.debug(\"placing new tag, repo: %s, tag: latest\", this_repository)\n        image.tag(\n            repository=this_repository,\n            tag=\"latest\",\n        )\n        LOG.debug(\"Done\")\n\n    image.reload()\n    LOG.debug(\"Final image tags: %s\", image.tags)\n\n\ndef docker_login(registry: str, docker_username: str, docker_passphrase: str) -> None:\n    \"\"\"Log into a registry\"\"\"\n    LOG.info(\"Perform docker login to registry '%s' as user '%s' ...\", registry, docker_username)\n    docker_client.login(registry=registry, username=docker_username, password=docker_passphrase)\n\n\ndef docker_push(args: argparse.Namespace, version_tag: str, registry: str, folder: str) -> None:\n    \"\"\"Push images to a registry\"\"\"\n    this_repository = f\"{registry}{folder}/check-mk-{args.edition}\"\n\n    if \"-rc\" in version_tag:\n        LOG.info(\"%s was a release candidate, do a retagging before pushing\", version_tag)\n        version_tag = re.sub(\"-rc[0-9]*\", \"\", version_tag)\n        docker_tag(\n            args=args,\n            version_tag=version_tag,\n            registry=registry,\n            folder=folder,\n            target_version=version_tag,\n        )\n\n    docker_login(\n        registry=registry,\n        docker_username=os.environ.get(\"DOCKER_USERNAME\", \"\"),\n        docker_passphrase=os.environ.get(\"DOCKER_PASSPHRASE\", \"\"),\n    )\n\n    LOG.info(\"Pushing '%s' as '%s' ...\", this_repository, version_tag)\n    resp = docker_client.images.push(\n        repository=this_repository, tag=version_tag, stream=True, decode=True\n    )\n    for line in resp:\n        LOG.debug(line)\n        if \"error\" in line:\n            raise ValueError(f\"Some error occured during upload: {line}\")\n\n    if args.set_branch_latest_tag:\n        LOG.info(\"Pushing '%s' as '%s-latest' ...\", this_repository, args.branch)\n        resp = docker_client.images.push(\n            repository=this_repository, tag=f\"{args.branch}-latest\", stream=True, decode=True\n        )\n    else:\n        LOG.info(\"Pushing '%s' as '%s-daily' ...\", this_repository, args.branch)\n        resp = docker_client.images.push(\n            repository=this_repository, tag=f\"{args.branch}-daily\", stream=True, decode=True\n        )\n\n    for line in resp:\n        LOG.debug(line)\n        if \"error\" in line:\n            raise ValueError(f\"Some error occured during upload: {line}\")\n\n    if args.set_latest_tag:\n        LOG.info(\"Pushing '%s' as 'latest' ...\", this_repository)\n        resp = docker_client.images.push(\n            repository=this_repository, tag=\"latest\", stream=True, decode=True\n        )\n        for line in resp:\n            LOG.debug(line)\n            if \"error\" in line:\n                raise ValueError(f\"Some error occured during upload: {line}\")\n\n\ndef needed_packages(mk_file: str, output_file: str) -> None:\n    \"\"\"Extract needed packages from MK file\"\"\"\n    packages = []\n    with open(Path(mk_file).resolve()) as file:\n        lines = [line.rstrip() for line in file]\n        for line in lines:\n            this = re.findall(r\"^(OS_PACKAGES\\s*\\+=\\s*)(.*?)(?=#|$)\", line)\n            if len(this):\n                packages.append(this[0][-1].strip())\n\n    LOG.debug(\"Needed packages based on %s: %s\", mk_file, packages)\n    LOG.debug(\"Save needed-packages file to '%s'\", output_file)\n    with open(output_file, \"w\") as file:\n        file.write(\" \".join(packages))\n\n\ndef docker_load(args: argparse.Namespace, version_tag: str, registry: str, folder: str) -> None:\n    \"\"\"Load image from tar.gz file\"\"\"\n    tar_name = f\"check-mk-{args.edition}-docker-{args.version}.tar.gz\"\n    this_repository = f\"{registry}{folder}/check-mk-{args.edition}\"\n\n    with cwd(tmp_path):\n        LOG.debug(\"Now at: %s\", os.getcwd())\n        LOG.debug(\"Loading image '%s' ...\", tar_name)\n\n        with gzip.open(tar_name, \"rb\") as tar_ball:\n            loaded_image = docker_client.images.load(tar_ball)[0]\n\n    LOG.debug(\"Create '%s:%s' tag ...\", this_repository, version_tag)\n    loaded_image.tag(\n        repository=this_repository,\n        tag=version_tag,\n    )\n\n\ndef check_for_local_image(\n    args: argparse.Namespace, version_tag: str, registry: str, folder: str\n) -> bool:\n    \"\"\"Check whether image is locally available\"\"\"\n    image_name_with_tag = f\"{registry}{folder}/check-mk-{args.edition}:{version_tag}\"\n\n    try:\n        docker_client.images.get(image_name_with_tag)\n        LOG.info(\"%s locally available\", image_name_with_tag)\n        return True\n    except docker.errors.ImageNotFound:\n        LOG.info(\"%s not found locally, please pull or load it\", image_name_with_tag)\n        return False\n\n\ndef build_tar_gz(\n    args: argparse.Namespace, version_tag: str, docker_path: str, docker_repo_name: str\n) -> None:\n    \"\"\"Build the check-mk-EDITION-docker-VERSION.tar.gz file\"\"\"\n    if args.image_cmk_base in (None, \"\", \"None\", \"null\"):\n        # make it more ugly and less professional if possible, still to good to maintain\n        image_cmk_base = run_cmd(\n            cmd=[\n                f\"{Path(__file__).parent.parent}/docker_image_aliases/resolve.py\",\n                \"IMAGE_CMK_BASE\",\n            ]\n        ).stdout.strip()\n    else:\n        image_cmk_base = args.image_cmk_base\n\n    buildargs = {\n        \"CMK_VERSION\": args.version,\n        \"CMK_EDITION\": args.edition,\n        \"IMAGE_CMK_BASE\": image_cmk_base,\n    }\n    this_tag = f\"{docker_repo_name}/check-mk-{args.edition}:{version_tag}\"\n    tar_name = f\"check-mk-{args.edition}-docker-{args.version}.tar.gz\"\n\n    with cwd(docker_path):\n        LOG.debug(\"Now at: %s\", os.getcwd())\n        LOG.debug(\n            \"Building image '%s', tagged: '%s', buildargs: '%s', nocache: '%s' ...\",\n            docker_path,\n            this_tag,\n            buildargs,\n            args.no_cache,\n        )\n        image, build_logs = docker_client.images.build(\n            # Do not use the cache when set to True\n            nocache=args.no_cache,\n            buildargs=buildargs,\n            tag=this_tag,\n            path=docker_path,\n        )\n        LOG.debug(\"Built image: %s\", image)\n        for chunk in build_logs:\n            if \"stream\" in chunk:\n                for line in chunk[\"stream\"].splitlines():\n                    LOG.debug(line)\n\n        LOG.info(\"Creating Image-Tarball %s ...\", tar_name)\n        if \"-rc\" in version_tag:\n            LOG.info(\n                \"%s contains rc information, do a retagging before docker save with %s.\",\n                version_tag,\n                args.version,\n            )\n\n            # image.tag() is required to make image.save() work properly.\n            # See docs of image.save(chunk_size=2097152, named=False):\n            # If set to True, the first tag in the tags list will be used to identify the image.\n            # Alternatively, any element of the tags list can be used as an argument to use that specific tag as the saved identifier.\n            image.tag(\n                repository=f\"{docker_repo_name}/check-mk-{args.edition}\",\n                tag=f\"{args.version}\",\n            )\n            # reload this object from the server and update attrs\n            image.reload()\n            LOG.debug(\"Image tags after re-tagging: %s\", image.tags)\n            this_tag = f\"{docker_repo_name}/check-mk-{args.edition}:{args.version}\"\n            with gzip.open(tar_name, \"wb\") as tar_ball:\n                # image.save() can only take elements of the tags list of an image\n                # as new tags are appended to the list of tags, the \"oldest\" one would be used if nothing is specified by the named keyword\n                for chunk in image.save(named=this_tag):\n                    tar_ball.write(chunk)\n            LOG.debug(\n                (\n                    \"Remove image %s now, it will be loaded from tar.gz at a later point again, \"\n                    \"see CMK-16498\"\n                ),\n                this_tag,\n            )\n            docker_client.images.remove(image=this_tag)\n        else:\n            with gzip.open(tar_name, \"wb\") as tar_ball:\n                for chunk in image.save(named=this_tag):\n                    tar_ball.write(chunk)\n\n\ndef build_image(\n    args: argparse.Namespace,\n    registry: str,\n    folder: str,\n    version_tag: str,\n    suffix: str,\n    docker_repo_name: str = \"checkmk\",\n) -> None:\n    \"\"\"Build an image, create a tar ball and tag the image\"\"\"\n    docker_path = f\"{tmp_path}/check-mk-{args.edition}-{args.version}{suffix}/docker_image\"\n    docker_image_archive = f\"check-mk-{args.edition}-docker-{args.version}.tar.gz\"\n    pkg_name = f\"check-mk-{args.edition}-{args.version}\"\n    architecture = run_cmd(cmd=[\"dpkg\", \"--print-architecture\"]).stdout.strip()\n    pkg_file = f\"{pkg_name}_0.jammy_{architecture}.deb\"\n\n    LOG.debug(\"docker_path: %s\", docker_path)\n    LOG.debug(\"docker_image_archive: %s\", docker_image_archive)\n    LOG.debug(\"pkg_name: %s\", pkg_name)\n    LOG.debug(\"architecture: %s\", architecture)\n    LOG.debug(\"pkg_file: %s\", pkg_file)\n\n    LOG.info(\"Unpack source tar to %s\", tmp_path)\n    with tarfile.open(\n        name=f\"{args.source_path}/check-mk-{args.edition}-{args.version}{suffix}.tar.gz\",\n        mode=\"r:gz\",\n    ) as tar:\n        tar.extractall(tmp_path, filter=\"data\")\n\n    LOG.info(\"Copy debian package ...\")\n    run_cmd(cmd=[\"cp\", f\"{args.source_path}/{pkg_file}\", docker_path])\n\n    LOG.info(\"Building container image ...\")\n    if version_tag is None:\n        raise Exception(\"Required VERSION_TAG is not set.\")\n    needed_packages(\n        mk_file=\"omd/distros/UBUNTU_22.04.mk\", output_file=f\"{docker_path}/needed-packages\"\n    )\n\n    build_tar_gz(\n        args=args,\n        version_tag=version_tag,\n        docker_path=docker_path,\n        docker_repo_name=docker_repo_name,\n    )\n\n    LOG.info(\"Move Image-Tarball ...\")\n    run_cmd(cmd=[\"mv\", \"-v\", f\"{docker_path}/{docker_image_archive}\", args.source_path])\n\n    docker_tag(args=args, version_tag=version_tag, registry=registry, folder=folder)\n\n\ndef main() -> None:\n    args: argparse.Namespace = parse_arguments()\n\n    LOG_LEVELS = [\"DEBUG\", \"INFO\", \"WARNING\", \"ERROR\", \"CRITICAL\"][::-1]\n    LOG.setLevel(LOG_LEVELS[min(len(LOG_LEVELS) - 1, max(args.verbose, 0))])\n\n    LOG.debug(\"Docker version: %r\", docker_client.info()[\"ServerVersion\"])\n    LOG.debug(\"args: %s\", args)\n\n    # Default to our internal registry, set it to \"\" if you want push it to dockerhub\n    registry = os.environ.get(\"CHECKMK_REGISTRY\", \"registry.checkmk.com\")\n    # the leading \"/\" is needed to finally create a valid namespace\n    # all registry == \"\" have a different folder and/or all folder == \"\" have a different registry\n    # a full repo name shall not start with \"/\"\n    folder = f\"/{args.edition}\"  # known as namespace\n    match args.edition:\n        case \"raw\":\n            suffix = \".cre\"\n            registry = \"\"\n            folder = \"checkmk\"\n        case \"enterprise\":\n            suffix = \".cee\"\n        case \"managed\":\n            suffix = \".cme\"\n            registry = \"\"\n            folder = \"checkmk\"\n        case \"cloud\":\n            suffix = \".cce\"\n            registry = \"\"\n            folder = \"checkmk\"\n        case \"saas\":\n            suffix = \".cse\"\n            registry = \"artifacts.lan.tribe29.com:4000\"\n            folder = \"\"\n        case _:\n            raise Exception(f\"ERROR: Unknown edition '{args.edition}'\")\n\n    version_tag = Path(args.source_path).name\n\n    LOG.debug(\"tmp_path: %s\", tmp_path)\n    LOG.debug(\"version_tag: %s\", version_tag)\n    LOG.debug(\"registry: %s\", registry)\n    LOG.debug(\"suffix: %s\", suffix)\n    LOG.debug(\"base_path: %s\", base_path)\n\n    if os.environ.get(\"NEXUS_USERNAME\"):\n        docker_login(\n            registry=os.environ.get(\"DOCKER_REGISTRY\", \"\"),\n            docker_username=os.environ.get(\"NEXUS_USERNAME\", \"\"),\n            docker_passphrase=os.environ.get(\"NEXUS_PASSWORD\", \"\"),\n        )\n\n    match args.action:\n        case \"build\":\n            build_image(\n                args=args, registry=registry, folder=folder, version_tag=version_tag, suffix=suffix\n            )\n        case \"push\":\n            docker_push(args=args, registry=registry, folder=folder, version_tag=version_tag)\n        case \"load\":\n            if check_for_local_image(\n                args=args, registry=registry, folder=folder, version_tag=args.version\n            ):\n                return\n            LOG.info(\"Image not found locally, trying to download it ...\")\n            if release_key := os.environ.get(\"RELEASE_KEY\"):\n                internal_deploy_port = os.environ.get(\"INTERNAL_DEPLOY_PORT\")\n                internal_deploy_dest = os.environ.get(\"INTERNAL_DEPLOY_DEST\")\n                file_pattern = f\"check-mk-{args.edition}-docker-{args.version}.tar.gz\"\n                run_cmd(\n                    cmd=[\n                        \"rsync\",\n                        \"--recursive\",\n                        \"--links\",\n                        \"--perms\",\n                        \"--times\",\n                        \"--verbose\",\n                        \"-e\",\n                        f\"ssh -o StrictHostKeyChecking=no -i {release_key} -p {internal_deploy_port}\",\n                        f\"{internal_deploy_dest}/{args.version_rc_aware}/{file_pattern}\",\n                        f\"{tmp_path}/\",\n                    ]\n                )\n            else:\n                raise SystemExit(\n                    \"RELEASE_KEY not found in env, required to download image via rsync\"\n                )\n            docker_load(args=args, registry=registry, folder=folder, version_tag=args.version)\n        case \"check_local\":\n            if not check_for_local_image(\n                args=args, registry=registry, folder=folder, version_tag=args.version\n            ):\n                raise SystemExit(\"Image not found locally\")\n        case _:\n            raise Exception(\n                f\"Unknown action: {args.action}, should be prevented by argparse options\"\n            )\n\n\nif __name__ == \"__main__\":\n    try:\n        main()\n    except KeyboardInterrupt:\n        print(\"Feigling\")\n    except Exception as e:\n        raise e\n    finally:\n        cleanup()\n"}
{"type": "source_file", "path": "agents/wnx/__init__.py", "content": "#!/usr/bin/env python3\n# Copyright (C) 2022 Checkmk GmbH - License: GNU General Public License v2\n# This file is part of Checkmk (https://checkmk.com). It is subject to the terms and\n# conditions defined in the file COPYING, which is part of this source code package.\n"}
{"type": "source_file", "path": "buildscripts/docker_image_aliases/register.py", "content": "#!/usr/bin/env python3\n# Copyright (C) 2021 Checkmk GmbH - License: GNU General Public License v2\n# This file is part of Checkmk (https://checkmk.com). It is subject to the terms and\n# conditions defined in the file COPYING, which is part of this source code package.\n\n\"\"\"This script stores a given Docker image in our internal registry and creates a textual\ndocker image alias (in form of a Dockerfile file containing the provided data and a new SHA\nbased image name) referencing the remotly stored image via unique ID\n\nrun\n\n  >> docker login artifacts.lan.tribe29.com:4000 --username USER.NAME\n  >> ./register.py IMAGE_DEBIAN_DEFAULT debian:buster-slim\n\nto create an (internally used) Dockerfile inside `IMAGE_DEBIAN_DEFAULT/`:\n\nwhich can be used like this (Dockerfile example):\n\n  docker build --build-arg \"IMAGE_DEBIAN_DEFAULT=$(./resolve.py IMAGE_DEBIAN_DEFAULT)\" -t debian_example example\n\"\"\"\n\nimport json\nimport logging\nimport os\nimport shlex\nimport subprocess\nimport sys\nfrom collections.abc import Sequence\nfrom pathlib import Path\n\nimport docker  # type: ignore[import-untyped]\nimport yaml\n\nLOG = logging.getLogger(\"register-dia\")\nREGISTRY = \"artifacts.lan.tribe29.com:4000\"\n\n\ndef split_source_name(raw_source: str) -> tuple[str, str, list[str]]:\n    \"\"\"\n    >>> split_source_name(\"artifacts.lan.tribe29.com:4000/debian:latest\")\n    ('artifacts.lan.tribe29.com:4000', 'debian', ['latest'])\n    >>> split_source_name(\"debian:buster-slim\")\n    ('', 'debian', ['buster-slim'])\n    >>> split_source_name(\"debian\")\n    ('', 'debian', ['latest'])\n    >>> split_source_name(\"artifacts.lan.tribe29.com:4000/hadolint/hadolint\")\n    ('artifacts.lan.tribe29.com:4000/hadolint', 'hadolint', ['latest'])\n    \"\"\"\n    *registry_name, image_name = raw_source.split(\"/\")\n    base_name, *tags = image_name.split(\":\")\n    assert len(tags) <= 1\n    return \"/\".join(registry_name), base_name, tags if tags else [\"latest\"]\n\n\ndef cmd_result(cmd: str, cwd: str | None = None) -> Sequence[str]:\n    \"\"\"Run @cmd and return non-empty lines\"\"\"\n    return [\n        line\n        for line in subprocess.check_output(shlex.split(cmd), cwd=cwd, text=True).split(\"\\n\")\n        if line.strip()\n    ]\n\n\ndef commit_id(directory: str) -> str:\n    return cmd_result(\"git rev-parse --short HEAD\", cwd=directory)[0]\n\n\ndef cmk_branch(directory: str) -> str:\n    return min(\n        (\"master\", \"2.4.0\", \"2.3.0\", \"2.2.0\", \"2.1.0\", \"2.0.0\", \"1.6.0\", \"1.5.0\"),\n        key=lambda b: int(\n            cmd_result(f\" git rev-list --max-count=1000 --count HEAD...origin/{b}\", cwd=directory)[\n                0\n            ]\n        ),\n    )\n\n\ndef git_info() -> list[str]:\n    cwd = os.path.dirname(__file__)\n    a, b = cmk_branch(directory=cwd), commit_id(directory=cwd)\n    return [a, b]\n\n\ndef main() -> None:\n    logging.basicConfig(\n        level=logging.DEBUG if \"-v\" in sys.argv else logging.WARNING,\n        format=\"%(name)s %(levelname)s: %(message)s\",\n    )\n\n    alias_name, source_name = sys.argv[1], sys.argv[2]\n\n    client = docker.from_env(timeout=1200)\n    LOG.info(\"Docker version: %r\", client.info()[\"ServerVersion\"])\n\n    print(f\"pull image {source_name}\")\n    image = client.images.pull(source_name)\n\n    _source_registry, source_base_name, source_tags = split_source_name(source_name)\n\n    name_in_registry = (\n        f\"{REGISTRY}/{source_base_name}:{'-'.join(source_tags + ['image-alias'] + git_info())}\"\n    )\n\n    LOG.info(\"tag image as %s\", name_in_registry)\n    result = image.tag(name_in_registry)\n    assert result\n\n    print(f\"push image as {name_in_registry}\")\n    push_response = tuple(\n        map(\n            json.loads,\n            filter(lambda x: x.strip(), client.images.push(name_in_registry).split(\"\\n\")),\n        )\n    )\n\n    LOG.debug(push_response)\n\n    assert not any(\"error\" in parsed for parsed in push_response)\n\n    digest = next(parsed for parsed in push_response if \"aux\" in parsed)[\"aux\"][\"Digest\"]\n\n    remote_image_name = f\"{REGISTRY}/{source_base_name}@{digest}\"\n\n    LOG.info(\"pull sha %s (for verification)\", remote_image_name)\n    repulled = client.images.pull(remote_image_name)\n\n    new_digests = [d for d in repulled.attrs[\"RepoDigests\"] if d.startswith(REGISTRY)]\n    assert remote_image_name in new_digests\n\n    alias_dir = Path(os.path.dirname(__file__)) / alias_name\n    alias_dir.mkdir(parents=True, exist_ok=True)\n\n    print(f\"create new alias at {alias_dir.absolute()}\")\n    with open(alias_dir / \"Dockerfile\", \"w\") as dockerfile:\n        print(f\"FROM {remote_image_name}\", file=dockerfile)\n\n    with open(alias_dir / \"meta.yml\", \"w\") as metafile:\n        yaml.dump({\"source\": source_name, \"tag\": name_in_registry}, stream=metafile)\n\n\nif __name__ == \"__main__\":\n    main()\n"}
{"type": "source_file", "path": "buildscripts/scripts/lib/__init__.py", "content": "#!/usr/bin/env python3\n# Copyright (C) 2024 Checkmk GmbH - License: GNU General Public License v2\n# This file is part of Checkmk (https://checkmk.com). It is subject to the terms and\n# conditions defined in the file COPYING, which is part of this source code package.\n"}
{"type": "source_file", "path": "cmk/active_checks/check_cmk_inv.py", "content": "#!/usr/bin/env python3\n# Copyright (C) 2023 Checkmk GmbH - License: GNU General Public License v2\n# This file is part of Checkmk (https://checkmk.com). It is subject to the terms and\n# conditions defined in the file COPYING, which is part of this source code package.\n\nimport argparse\nimport logging\nimport sys\nfrom collections.abc import Sequence\nfrom contextlib import suppress\n\nimport cmk.ccc.debug\nfrom cmk.ccc.exceptions import OnError\n\nimport cmk.utils.cleanup\nimport cmk.utils.password_store\nimport cmk.utils.paths\nfrom cmk.utils.config_path import LATEST_CONFIG\nfrom cmk.utils.cpu_tracking import CPUTracker\nfrom cmk.utils.hostaddress import HostName\nfrom cmk.utils.log import console\n\nfrom cmk.fetchers import Mode as FetchMode\nfrom cmk.fetchers.filecache import FileCacheOptions\n\nfrom cmk.checkengine.checking import make_timing_results\nfrom cmk.checkengine.checkresults import ActiveCheckResult\nfrom cmk.checkengine.inventory import HWSWInventoryParameters\nfrom cmk.checkengine.parser import NO_SELECTION\nfrom cmk.checkengine.submitters import ServiceState\n\nimport cmk.base.api.agent_based.register as agent_based_register\nfrom cmk.base import config\nfrom cmk.base.api.agent_based.plugin_classes import AgentBasedPlugins\nfrom cmk.base.checkers import (\n    CMKFetcher,\n    CMKParser,\n    CMKSummarizer,\n    SectionPluginMapper,\n)\nfrom cmk.base.errorhandling import CheckResultErrorHandler\nfrom cmk.base.modes.check_mk import execute_active_check_inventory\n\n\ndef parse_arguments(argv: Sequence[str]) -> argparse.Namespace:\n    parser = argparse.ArgumentParser(\n        prog=\"check_cmk_inv\",\n        description=\"\"\"Check Checkmk HW/SW Inventory\"\"\",\n    )\n\n    parser.add_argument(\n        \"hostname\",\n        type=str,\n        metavar=\"HOSTNAME\",\n        help=\"Host for which the HW/SW Inventory is executed\",\n    )\n\n    parser.add_argument(\n        \"--inv-fail-status\",\n        type=int,\n        default=1,\n        help=\"State when HW/SW Inventory fails\",\n    )\n\n    parser.add_argument(\n        \"--hw-changes\",\n        type=int,\n        default=0,\n        help=\"State when hardware changes are detected\",\n    )\n\n    parser.add_argument(\n        \"--sw-changes\",\n        type=int,\n        default=0,\n        help=\"State when software packages info is missing\",\n    )\n\n    parser.add_argument(\n        \"--sw-missing\",\n        type=int,\n        default=0,\n        help=\"State when software packages info is missing\",\n    )\n\n    parser.add_argument(\n        \"--nw-changes\",\n        type=int,\n        default=0,\n        help=\"State when networking changes are detected\",\n    )\n\n    return parser.parse_args(argv)\n\n\ndef main(\n    argv: Sequence[str] | None = None,\n) -> int:\n    args = parse_arguments(argv or sys.argv[1:])\n    parameters = HWSWInventoryParameters(\n        hw_changes=args.hw_changes,\n        sw_changes=args.sw_changes,\n        sw_missing=args.sw_missing,\n        nw_changes=args.nw_changes,\n        fail_status=args.inv_fail_status,\n        status_data_inventory=False,\n    )\n\n    return inventory_as_check(parameters, args.hostname)\n\n\ndef inventory_as_check(parameters: HWSWInventoryParameters, hostname: HostName) -> ServiceState:\n    plugins = load_checks()\n    config_cache = config.load(\n        discovery_rulesets=agent_based_register.extract_known_discovery_rulesets(plugins)\n    ).config_cache\n    config_cache.ruleset_matcher.ruleset_optimizer.set_all_processed_hosts({hostname})\n    hosts_config = config.make_hosts_config()\n    file_cache_options = FileCacheOptions()\n\n    fetcher = CMKFetcher(\n        config_cache,\n        config_cache.fetcher_factory(config_cache.make_service_configurer(plugins.check_plugins)),\n        plugins,\n        file_cache_options=file_cache_options,\n        force_snmp_cache_refresh=False,\n        ip_address_of=config.ConfiguredIPLookup(\n            config_cache, error_handler=config.handle_ip_lookup_failure\n        ),\n        mode=FetchMode.INVENTORY,\n        on_error=OnError.RAISE,\n        selected_sections=NO_SELECTION,\n        simulation_mode=config.simulation_mode,\n        snmp_backend_override=None,\n        password_store_file=cmk.utils.password_store.core_password_store_path(LATEST_CONFIG),\n    )\n    parser = CMKParser(\n        config_cache.parser_factory(),\n        selected_sections=NO_SELECTION,\n        keep_outdated=file_cache_options.keep_outdated,\n        logger=logging.getLogger(\"cmk.base.inventory\"),\n    )\n    summarizer = CMKSummarizer(\n        hostname,\n        config_cache.summary_config,\n        override_non_ok_state=parameters.fail_status,\n    )\n    error_handler = CheckResultErrorHandler(\n        exit_spec=config_cache.exit_code_spec(hostname),\n        host_name=hostname,\n        service_name=\"Check_MK HW/SW Inventory\",\n        plugin_name=\"check_mk_active-cmk_inv\",\n        is_cluster=hostname in hosts_config.clusters,\n        snmp_backend=config_cache.get_snmp_backend(hostname),\n        keepalive=False,\n    )\n    check_results: Sequence[ActiveCheckResult] = []\n    with error_handler:\n        with CPUTracker(console.debug) as tracker:\n            check_results = execute_active_check_inventory(\n                hostname,\n                config_cache=config_cache,\n                hosts_config=hosts_config,\n                fetcher=fetcher,\n                parser=parser,\n                summarizer=summarizer,\n                section_plugins=SectionPluginMapper(\n                    {**plugins.agent_sections, **plugins.snmp_sections}\n                ),\n                inventory_plugins=plugins.inventory_plugins,\n                inventory_parameters=config_cache.inventory_parameters,\n                parameters=parameters,\n                raw_intervals_from_config=config_cache.inv_retention_intervals(hostname),\n            )\n        check_results = [\n            *check_results,\n            make_timing_results(\n                tracker.duration,\n                # FIXME: This is inconsistent with the other two calls.\n                (),  # nothing to add here, b/c fetching is triggered further down the call stack.\n                perfdata_with_times=config.check_mk_perfdata_with_times,\n            ),\n        ]\n\n    if error_handler.result is not None:\n        check_results = (error_handler.result,)\n\n    check_result = ActiveCheckResult.from_subresults(*check_results)\n    with suppress(IOError):\n        sys.stdout.write(check_result.as_text() + \"\\n\")\n        sys.stdout.flush()\n    return check_result.state\n\n\ndef load_checks() -> AgentBasedPlugins:\n    plugins = config.load_all_plugins(cmk.utils.paths.checks_dir)\n    if sys.stderr.isatty():\n        for error_msg in plugins.errors:\n            console.error(error_msg, file=sys.stderr)\n    return plugins\n"}
{"type": "source_file", "path": "buildscripts/scripts/lib/common.py", "content": "#!/usr/bin/env python3\n# Copyright (C) 2024 Checkmk GmbH - License: GNU General Public License v2\n# This file is part of Checkmk (https://checkmk.com). It is subject to the terms and\n# conditions defined in the file COPYING, which is part of this source code package.\n\"\"\"\nCommon parts of buildscripts scripts\n\"\"\"\n\nimport argparse\nfrom collections.abc import Iterable, Iterator\nfrom contextlib import contextmanager\nfrom os import chdir, getcwd\nfrom pathlib import Path\n\nimport yaml\n\n\ndef flatten(list_to_flatten: Iterable[Iterable[str] | str]) -> Iterable[str]:\n    # This is a workaround the fact that yaml cannot \"extend\" a predefined node which is a list:\n    # https://stackoverflow.com/questions/19502522/extend-an-array-in-yaml\n    return [h for elem in list_to_flatten for h in ([elem] if isinstance(elem, str) else elem)]\n\n\ndef strtobool(val: str | bool) -> bool:\n    \"\"\"Convert a string representation of truth to true (1) or false (0).\n    Raises ArgumentTypeError if 'val' is anything else.\n\n    distutils.util.strtobool() no longer part of the standard library in 3.12\n\n    https://github.com/python/cpython/blob/v3.11.2/Lib/distutils/util.py#L308\n    \"\"\"\n    if isinstance(val, bool):\n        return val\n    val = val.lower()\n    if val in (\"y\", \"yes\", \"t\", \"true\", \"on\", \"1\"):\n        return True\n    if val in (\"n\", \"no\", \"f\", \"false\", \"off\", \"0\"):\n        return False\n    raise argparse.ArgumentTypeError(\"Boolean value expected.\")\n\n\ndef load_editions_file(filename: str | Path) -> dict:\n    with open(filename) as editions_file:\n        return yaml.safe_load(editions_file)\n\n\n@contextmanager\ndef cwd(path: str | Path) -> Iterator[None]:\n    oldpwd = getcwd()\n    chdir(path)\n    try:\n        yield\n    finally:\n        chdir(oldpwd)\n"}
{"type": "source_file", "path": "cmk/base/api/agent_based/__init__.py", "content": "#!/usr/bin/env python3\n# Copyright (C) 2019 Checkmk GmbH - License: GNU General Public License v2\n# This file is part of Checkmk (https://checkmk.com). It is subject to the terms and\n# conditions defined in the file COPYING, which is part of this source code package.\n"}
{"type": "source_file", "path": "buildscripts/scripts/publish_cloud_images.py", "content": "#!/usr/bin/env python3\n# Copyright (C) 2023 Checkmk GmbH - License: GNU General Public License v2\n# This file is part of Checkmk (https://checkmk.com). It is subject to the terms and\n# conditions defined in the file COPYING, which is part of this source code package.\nimport abc\nimport argparse\nimport asyncio\nimport enum\nimport json\nimport os\nimport sys\nfrom typing import Final\n\nimport boto3\nfrom azure.identity import DefaultAzureCredential\nfrom azure.mgmt.compute import ComputeManagementClient\nfrom azure.mgmt.compute.models import (\n    GalleryArtifactVersionSource,\n    GalleryImageVersion,\n    GalleryImageVersionPublishingProfile,\n    GalleryImageVersionStorageProfile,\n    GalleryOSDiskImage,\n    TargetRegion,\n)\nfrom azure.mgmt.resource import ResourceManagementClient\nfrom msrest.polling import LROPoller\n\nsys.path.insert(0, os.path.join(os.path.dirname(__file__), \"..\", \"..\"))\nfrom cmk.ccc.version import _BaseVersion, ReleaseType, Version\n\n\ndef parse_arguments() -> argparse.Namespace:\n    parser = argparse.ArgumentParser(\n        description=\"This script is used for publishing new versions \"\n        \"for our AWS and Azure cloud images.\"\n    )\n    parser.add_argument(\n        \"--cloud-type\",\n        help=\"Choose here the type of the cloud\",\n        action=\"store\",\n        required=True,\n        choices=CloudPublisher.CLOUD_TYPES,\n    )\n    parser.add_argument(\n        \"--new-version\",\n        help=\"The new version which will be used for the update\",\n        action=\"store\",\n        required=True,\n    )\n    parser.add_argument(\n        \"--build-tag\",\n        help=\"The jenkins build tag to pass to the change sets for later identification\",\n        action=\"store\",\n        required=True,\n    )\n    parser.add_argument(\n        \"--image-name\",\n        help=\"The name of the cloud image which can be found in the cloud\",\n        action=\"store\",\n        required=True,\n    )\n    parser.add_argument(\n        \"--marketplace-scanner-arn\",\n        help=\"The arn of an aws role which can access our ami images\",\n        action=\"store\",\n        required=True,\n    )\n    parser.add_argument(\n        \"--product-id\",\n        help=\"The product id of the product which should receive a new version\",\n        action=\"store\",\n        required=True,\n    )\n    parser.add_argument(\n        \"--azure-subscription-id\",\n        help=\"Azure's subscription id\",\n        action=\"store\",\n        required=True,\n    )\n    parser.add_argument(\n        \"--azure-resource-group\",\n        help=\"Azure's resource group\",\n        action=\"store\",\n        required=True,\n    )\n    return parser.parse_args()\n\n\nclass CloudPublisher(abc.ABC):\n    # Currently, both AWS and Azure should long be finished after that time\n    SECONDS_TO_TIMEOUT_PUBLISH_PROCESS: Final = 3 * 60 * 60\n    CLOUD_TYPES: Final = [\"aws\", \"azure\"]\n    SECONDS_TO_WAIT_FOR_NEXT_STATUS: Final = 20\n\n    def __init__(self, version: Version, build_tag: str, image_name: str):\n        self.version = version\n        self.build_tag = build_tag\n        self.image_name = image_name\n\n    @abc.abstractmethod\n    async def publish(self): ...\n\n    @staticmethod\n    def build_release_notes_url(version: str) -> str:\n        \"\"\"\n        >>> CloudPublisher.build_release_notes_url(\"2.2.0p5\")\n        'https://forum.checkmk.com/t/release-checkmk-stable-release-2-2-0p5/'\n        \"\"\"\n        return (\n            f\"https://forum.checkmk.com/t/release-checkmk-stable-release-\"\n            f\"{version.replace('.', '-')}/\"\n        )\n\n\nclass AWSPublisher(CloudPublisher):\n    ENTITY_TYPE_WITH_VERSION = \"AmiProduct@1.0\"\n    CATALOG = \"AWSMarketplace\"\n\n    def __init__(\n        self,\n        version: Version,\n        build_tag: str,\n        image_name: str,\n        marketplace_scanner_arn: str,\n        product_id: str,\n    ):\n        super().__init__(version, build_tag, image_name)\n        self.client_ec2 = boto3.client(\"ec2\")\n        self.client_market = boto3.client(\"marketplace-catalog\")\n        self.aws_marketplace_scanner_arn = marketplace_scanner_arn\n        self.production_id = product_id\n\n    class ChangeTypes(enum.StrEnum):\n        ADD_DELIVERY_OPTIONS = \"AddDeliveryOptions\"  # for updating the version\n\n    async def publish(self) -> None:\n        image_id = self.get_ami_image_id()\n        update_details = {\n            \"Version\": {\n                \"VersionTitle\": str(self.version),\n                \"ReleaseNotes\": self.build_release_notes_url(str(self.version)),\n            },\n            \"DeliveryOptions\": [\n                {\n                    \"Details\": {\n                        \"AmiDeliveryOptionDetails\": {\n                            \"AmiSource\": {\n                                \"AmiId\": image_id,\n                                # This role must be able to read our ami images, see:\n                                # https://docs.aws.amazon.com/marketplace/latest/userguide/ami-single-ami-products.html#single-ami-marketplace-ami-access\n                                \"AccessRoleArn\": self.aws_marketplace_scanner_arn,\n                                \"UserName\": \"ubuntu\",\n                                \"OperatingSystemName\": \"UBUNTU\",\n                                # TODO: can we centralize this into editions.yml?\n                                \"OperatingSystemVersion\": \"22.04\",\n                            },\n                            \"UsageInstructions\": \"See the Checkmk manual for \"\n                            \"detailed usage instructions: \"\n                            \"https://docs.checkmk.com/latest/en/intro_gui.html\",\n                            \"RecommendedInstanceType\": \"c6a.large\",\n                            \"SecurityGroups\": [\n                                {\n                                    # ssh\n                                    \"IpProtocol\": \"tcp\",\n                                    \"FromPort\": 22,\n                                    \"ToPort\": 22,\n                                    \"IpRanges\": [\"0.0.0.0/0\"],\n                                },\n                                {\n                                    # https\n                                    \"IpProtocol\": \"tcp\",\n                                    \"FromPort\": 443,\n                                    \"ToPort\": 443,\n                                    \"IpRanges\": [\"0.0.0.0/0\"],\n                                },\n                                {\n                                    # agent registration\n                                    \"IpProtocol\": \"tcp\",\n                                    \"FromPort\": 8000,\n                                    \"ToPort\": 8000,\n                                    \"IpRanges\": [\"0.0.0.0/0\"],\n                                },\n                            ],\n                        }\n                    }\n                }\n            ],\n        }\n\n        print(f\"Starting change set for ami image {image_id} and version {self.version}\")\n        response = self.client_market.start_change_set(\n            Catalog=self.CATALOG,\n            ChangeSet=[\n                {\n                    \"ChangeType\": self.ChangeTypes.ADD_DELIVERY_OPTIONS,\n                    \"Entity\": {\n                        \"Type\": self.ENTITY_TYPE_WITH_VERSION,\n                        \"Identifier\": self.production_id,\n                    },\n                    \"Details\": json.dumps(update_details),\n                    \"ChangeName\": \"update\",\n                },\n            ],\n            ChangeSetName=f\"Add new version {self.version} by {self.build_tag}\",\n        )\n        await asyncio.wait_for(\n            self.update_successful(response[\"ChangeSetId\"]), self.SECONDS_TO_TIMEOUT_PUBLISH_PROCESS\n        )\n\n    def get_ami_image_id(self) -> str:\n        images = self.client_ec2.describe_images(\n            Filters=[\n                {\n                    \"Name\": \"name\",\n                    \"Values\": [self.image_name],\n                },\n            ],\n        )[\"Images\"]\n        assert len(images) == 1, (\n            f\"Cannot identify the correct image to publish, received the following: {images}\"\n        )\n        return images[0][\"ImageId\"]\n\n    async def update_successful(self, change_set_id: str) -> None:\n        while True:\n            response = self.client_market.describe_change_set(\n                Catalog=self.CATALOG,\n                ChangeSetId=change_set_id,\n            )\n            status = response[\"Status\"]\n            match status:\n                case \"PREPARING\" | \"APPLYING\":\n                    print(\n                        f\"Got {status=}... \"\n                        f\"sleeping for {self.SECONDS_TO_WAIT_FOR_NEXT_STATUS} seconds...\"\n                    )\n                    await asyncio.sleep(self.SECONDS_TO_WAIT_FOR_NEXT_STATUS)\n                case \"CANCELLED\" | \"FAILED\":\n                    raise RuntimeError(\n                        f\"The changeset {change_set_id} returned {status=}.\\n\"\n                        f\"The error was: {response['ChangeSet'][0]['ErrorDetailList']}\"\n                    )\n                case \"SUCCEEDED\":\n                    return\n\n\nclass AzurePublisher(CloudPublisher):\n    LOCATION = \"westeurope\"\n    STORAGE_ACCOUNT_TYPE = \"Standard_LRS\"\n    GALLERY_NAME = \"Marketplace_Publishing_Gallery\"\n\n    def __init__(\n        self,\n        version: Version,\n        build_tag: str,\n        image_name: str,\n        subscription_id: str,\n        resource_group: str,\n    ):\n        super().__init__(version, build_tag, image_name)\n        assert self.version is not None\n\n        credentials = DefaultAzureCredential()\n        self.subscription_id = subscription_id\n        self.resource_group = resource_group\n        # The image name is hardcoded, because we changing this for each new\n        # major or minor version would require going through the complete\n        # listing process again.\n        # The gallery ID is only visible internally and not visible by users.\n        # Use Checkmk_Cloud_Edition_2.2b5 for e.g. testing\n        self.gallery_image_name = \"Checkmk-Cloud-Edition-2.2\"\n        self.compute_client = ComputeManagementClient(\n            credentials,\n            self.subscription_id,\n        )\n        self.resource_client = ResourceManagementClient(\n            credentials,\n            self.subscription_id,\n        )\n\n    def get_azure_image_id(self) -> str:\n        resource_list = self.resource_client.resources.list_by_resource_group(\n            self.resource_group,\n            filter=f\"name eq '{self.image_name}'\",\n        )\n        first_id = next(resource_list).id\n        if another_match := next(resource_list, None):\n            raise RuntimeError(\n                f\"Cannot identify a unique azure image by using {self.image_name=}. \"\n                f\"Found also: {another_match}\"\n            )\n\n        return first_id\n\n    @staticmethod\n    def azure_compatible_version(version: Version) -> str:\n        \"\"\"\n        Yea, this is great... but azure doesn't accept our versioning schema\n        >>> AzurePublisher.azure_compatible_version(Version.from_str(\"2.2.0p5\"))\n        '2.2.5'\n        \"\"\"\n        assert isinstance(version.base, _BaseVersion)\n        return f\"{version.base.major}.{version.base.minor}.{version.release.value}\"\n\n    async def build_gallery_image(self):\n        image_id = self.get_azure_image_id()\n        print(f\"Creating new gallery image from {self.version=} by using {image_id=}\")\n        self.update_succesful(\n            self.compute_client.gallery_image_versions.begin_create_or_update(\n                resource_group_name=self.resource_group,\n                gallery_name=self.GALLERY_NAME,\n                gallery_image_name=self.gallery_image_name,\n                gallery_image_version_name=self.azure_compatible_version(self.version),\n                gallery_image_version=GalleryImageVersion(\n                    location=self.LOCATION,\n                    publishing_profile=GalleryImageVersionPublishingProfile(\n                        target_regions=[\n                            TargetRegion(name=self.LOCATION),\n                        ],\n                        storage_account_type=self.STORAGE_ACCOUNT_TYPE,\n                    ),\n                    storage_profile=GalleryImageVersionStorageProfile(\n                        source=GalleryArtifactVersionSource(\n                            id=image_id,\n                        ),\n                        os_disk_image=GalleryOSDiskImage(\n                            # Taken from previous images\n                            host_caching=\"ReadWrite\",\n                        ),\n                    ),\n                ),\n            ),\n        )\n\n    async def publish(self):\n        \"\"\"\n        Azure's update process has 2 steps:\n        * first, we need to create a gallery image from the VM image which was pushed by packer\n        * second, we need to add the new gallery image as technical configuration to our marketplace\n        offer\n        \"\"\"\n\n        await asyncio.wait_for(\n            self.build_gallery_image(),\n            self.SECONDS_TO_TIMEOUT_PUBLISH_PROCESS,\n        )\n\n        # TODO: Implement step #2\n\n    def update_succesful(self, poller: LROPoller) -> None:\n        while True:\n            result = poller.result(self.SECONDS_TO_WAIT_FOR_NEXT_STATUS)\n            assert isinstance(result, GalleryImageVersion)\n            if provisioning_state := result.provisioning_state:\n                print(f\"{provisioning_state=}\")\n                match provisioning_state:\n                    case \"Succeeded\":\n                        return\n                    case _:\n                        raise RuntimeError(f\"Poller returned {provisioning_state=}\")\n            print(\n                f\"Got no result yet... \"\n                f\"sleeping for {self.SECONDS_TO_WAIT_FOR_NEXT_STATUS} seconds...\"\n            )\n\n\ndef ensure_using_official_release(version: str) -> Version:\n    parsed_version = Version.from_str(version)\n    if parsed_version.release.release_type not in (ReleaseType.p, ReleaseType.na):\n        raise RuntimeError(\n            f\"We only want to publish official patch releases, got {parsed_version} instead.\"\n        )\n    return parsed_version\n\n\nif __name__ == \"__main__\":\n    args = parse_arguments()\n\n    new_version = ensure_using_official_release(args.new_version)\n    match args.cloud_type:\n        case \"aws\":\n            asyncio.run(\n                AWSPublisher(\n                    new_version,\n                    args.build_tag,\n                    args.image_name,\n                    args.marketplace_scanner_arn,\n                    args.product_id,\n                ).publish()\n            )\n        case \"azure\":\n            asyncio.run(\n                AzurePublisher(\n                    new_version,\n                    args.build_tag,\n                    args.image_name,\n                    args.azure_subscription_id,\n                    args.azure_resource_group,\n                ).publish()\n            )\n"}
{"type": "source_file", "path": "buildscripts/scripts/lib/registry.py", "content": "#!/usr/bin/env python3\n# Copyright (C) 2024 Checkmk GmbH - License: GNU General Public License v2\n# This file is part of Checkmk (https://checkmk.com). It is subject to the terms and\n# conditions defined in the file COPYING, which is part of this source code package.\n\"\"\"\nLibrary with container registry focussed functionality\n\"\"\"\n\nimport sys\nfrom collections.abc import Callable, Iterator, Sequence\nfrom contextlib import suppress\nfrom dataclasses import dataclass, field\nfrom datetime import date, timedelta\nfrom os import environ\nfrom pathlib import Path\nfrom typing import NamedTuple, Self, TypeAlias\nfrom urllib.parse import urlparse\n\nimport docker  # type: ignore[import-untyped]\nimport requests\n\nfrom cmk.ccc.version import BuildDate, ReleaseType, Version\n\nsys.path.insert(0, Path(__file__).parent.parent.parent.parent.as_posix())\nfrom tests.testlib.common.utils import (\n    get_cmk_download_credentials as _get_cmk_download_credentials,\n)\n\n# FQIN -> Fully Qualified Image Name\n# The format is: registryhost[:port]/repository/imagename[:tag]\nFQIN: TypeAlias = str\nDockerTag: TypeAlias = str\n\n\nclass Credentials(NamedTuple):\n    username: str\n    password: str\n\n\nclass DockerImage(NamedTuple):\n    image_name: str\n    tag: DockerTag\n\n    def full_name(self) -> FQIN:\n        return f\"{self.image_name}:{self.tag}\"\n\n    @classmethod\n    def from_str(cls, fqin: FQIN) -> Self:\n        image_name, tag = fqin.rsplit(\":\", 1)\n        return cls(image_name, tag)\n\n\n@dataclass\nclass Registry:\n    editions: Sequence[str]\n    url: str = field(init=False)\n    credentials: Credentials = field(init=False, repr=False)\n    client: docker.DockerClient = field(init=False)\n    image_exists: Callable[[DockerImage, str], bool] = field(init=False)\n    get_image_tags: Callable[[str], Iterator[DockerTag]] = field(init=False)\n\n    def delete_image(self, image: DockerImage) -> None:\n        tag = image.tag\n        image_name = image.image_name\n        headers = {}\n        docker_auth = None\n\n        if \"hub.docker.com\" in self.url:\n            # https://stackoverflow.com/questions/44209644/how-do-i-delete-a-docker-image-from-docker-hub-via-command-line\n            docker_hub_token = requests.post(\n                f\"{self.url}/v2/users/login/\",\n                json={\"username\": self.credentials.username, \"password\": self.credentials.password},\n                headers={\"Content-Type\": \"application/json\"},\n                timeout=30,\n            ).json()[\"token\"]\n            headers = {\n                \"Content-type\": \"application/json\",\n                \"Authorization\": f\"JWT {docker_hub_token}\",\n            }\n            url = f\"{self.url}/v2/repositories/{image_name}/tags/{tag}\"\n        else:\n            docker_auth = self.credentials\n            # https://wiki.lan.tribe29.com/books/how-to/page/release-withdraw-faq\n            url = f\"{self.url}/v2/{image_name}/manifests/{tag}\"\n            headers = {\"Accept\": \"application/vnd.docker.distribution.manifest.v2+json\"}\n            print(f\"Get digest of tag '{tag}' from '{url}'\")\n            response = requests.get(url, auth=docker_auth, headers=headers, timeout=30)\n            if response.status_code != 200:\n                raise RuntimeError(\n                    f\"Could not get digest of tag {tag} for image {image_name} with {url}: {response.status_code}\"\n                )\n            digest = response.headers.get(\"Docker-Content-Digest\")\n            assert digest, \"Did not receive Docker-Content-Digest. Headers: {response.headers}\"\n            print(f\"Digest of tag {tag} of image {image_name}: {digest}\")\n\n            url = f\"{self.url}/v2/{image_name}/manifests/{digest}\"\n            # https://registry.checkmk.com/v2/enterprise/check-mk-enterprise/manifests/<DIGEST>\n\n        # German Angst starts here :P\n        response = requests.delete(url, auth=docker_auth, headers=headers, timeout=30)\n\n        print(f\"Delete result for {image_name}:{tag}: {response.text}\")\n\n        if not response.ok:\n            raise RuntimeError(\n                f\"Could not delete image tag {tag} of {image_name} with {url}: {response.status_code}\"\n            )\n\n    def is_latest_image(self, image: DockerImage) -> bool:\n        \"\"\"\n        Check if the given image is also known by a `latest` tag.\n\n        This checks if the image is known as either the `latest` or has a\n        version-specific tag (e.g. `1.2.3-latest` or `4.5.6-daily`) applied to it.\n        If it is known as a latest version `True` will be returned.\n        \"\"\"\n        assert image.tag, f\"Expected image to have a tag, it has {image.tag!r}\"\n\n        for applied_tag in self.get_all_image_tags(image):\n            if applied_tag == \"latest\":\n                sys.stderr.write(f\"The image {image.full_name()} is tagged as 'latest'.\\n\")\n                return True\n\n            if applied_tag.endswith((\"-latest\", \"-daily\")):\n                sys.stderr.write(\n                    f\"The image {image.full_name()} is a latest of a specific version: {applied_tag}\\n\"\n                )\n                return True\n\n        return False\n\n    def get_all_image_tags(self, image: DockerImage) -> tuple[DockerTag]:\n        \"Get all tags applied to an image\"\n\n        image_from_registry = self.client.images.pull(image.image_name, tag=image.tag)\n        return tuple(image_from_registry.tags)\n\n    def list_images(self, image_name: str) -> Iterator[DockerImage]:\n        print(f\"Getting all images for {image_name} on {self.url}\")\n\n        for tag in self.get_image_tags(image_name):\n            yield DockerImage(image_name, tag)\n\n    def _get_image_tags_docker_hub(self, image: str) -> Iterator[DockerTag]:\n        \"\"\"\n        Get image tags for a specific image.\n\n        The image has the format `namespace/image_name`.\n        \"\"\"\n        session = requests.Session()\n        session.auth = (self.credentials.username, self.credentials.password)\n\n        namespace, image_name = image.split(\"/\")\n        # Current max for page_size is 100.\n        # We use the maximimum to minimize requests to Dockerhub.\n        next_url = (\n            f\"{self.url}/v2/namespaces/{namespace}/repositories/{image_name}/tags?page_size=100\"\n        )\n        while next_url:\n            response = session.get(\n                next_url,\n                timeout=30,\n            )\n            if not response.ok:\n                sys.stderr.write(f\"Request to {next_url} failed: {response.status_code}\")\n                break\n\n            json_response = response.json()\n\n            for tag in json_response[\"results\"]:\n                yield tag[\"name\"]\n\n            next_url = json_response.get(\"next\")\n\n    def _get_image_tags_enterprise(self, image: str) -> Iterator[DockerTag]:\n        \"\"\"\n        Get image tags for a specific image.\n\n        The image has the format `namespace/image_name`.\n        \"\"\"\n        yield from requests.get(\n            f\"{self.url}/v2/{image}/tags/list\",\n            auth=(self.credentials.username, self.credentials.password),\n            timeout=30,\n        ).json()[\"tags\"]\n\n    def image_exists_docker_hub(self, image: DockerImage, _edition: str) -> bool:\n        sys.stdout.write(f\"Test if {image.full_name()} is available...\")\n        with suppress(docker.errors.NotFound):\n            self.client.images.get_registry_data(image.full_name())\n            sys.stdout.write(\" OK\\n\")\n            return True\n        return False\n\n    def image_exists_and_can_be_pulled_enterprise(self, image: DockerImage, edition: str) -> bool:\n        if not self.image_exists_enterprise(image, edition):\n            return False\n        return self.image_can_be_pulled_enterprise(image, edition)\n\n    def image_exists_enterprise(self, image: DockerImage, _edition: str) -> bool:\n        url = f\"{self.url}/v2/{image.image_name}/tags/list\"\n        sys.stdout.write(f\"Test if {image.tag} can be found in {url}...\")\n        exists = (\n            image.tag\n            in requests.get(\n                url,\n                auth=(self.credentials.username, self.credentials.password),\n                timeout=30,\n            ).json()[\"tags\"]\n        )\n        if not exists:\n            sys.stdout.write(\" NO!\\n\")\n            return False\n        sys.stdout.write(\" OK\\n\")\n        return True\n\n    def image_can_be_pulled_enterprise(self, image: DockerImage, edition: str) -> bool:\n        repository = f\"{urlparse(self.url).netloc}/{edition}/check-mk-{edition}\"\n        sys.stdout.write(f\"Test if {image.tag} can be pulled from {repository}...\")\n        # Be sure we don't have the image locally... there is no force pull\n        with suppress(docker.errors.ImageNotFound):\n            self.client.images.remove(f\"{repository}:{image.tag}\")\n        try:\n            self.client.images.pull(\n                tag=image.tag,\n                repository=repository,\n            )\n        except docker.errors.APIError as e:\n            sys.stdout.write(f\" NO! Error was: {e}\\n\")\n            return False\n        sys.stdout.write(\" OK\\n\")\n        return True\n\n    def get_previous_release_tag(self, image: DockerImage, edition: str) -> str:\n        \"\"\"\n        Given an image tag we try to find the tag of the previous release\n\n        This will check for the existance of the tags and only return existing tags.\n\n        Example:\n        - 2.3.0p24 -> 2.3.0p23\n        - 2.3.0-2024.02.26 -> 2.3.0-2024.02.25\n        \"\"\"\n\n        current_version = Version.from_str(image.tag)\n        if current_version.release.is_unspecified():\n            raise ValueError(f\"{image.tag} misses build information (e.g. p12, 2024.04.03, b4).\")\n\n        def create_image(suffix: int | date) -> DockerImage:\n            match suffix:\n                case int():\n                    return DockerImage(\n                        image.image_name,\n                        f\"{current_version}{current_version.release.release_type.name}{suffix}\",\n                    )\n                case date():\n                    return DockerImage(\n                        image.image_name,\n                        f\"{current_version}-{suffix.year}.{suffix.month}.{suffix.day}\",\n                    )\n\n        def decrease_value(tag: int | date) -> int | date:\n            match tag:\n                case int():\n                    return tag - 1\n                case date():\n                    return tag - timedelta(days=1)\n\n        new_tag_value: int | date\n\n        # Create a new version tag that is lower than the previous one we have.\n        # Handling for dates and releases is similar, but with different startin points.\n        if current_version.release.release_type == ReleaseType.daily:\n            assert isinstance(current_version.release.value, BuildDate)\n\n            initial_release_date = date(\n                current_version.release.value.year,\n                current_version.release.value.month,\n                current_version.release.value.day,\n            )\n            new_tag_value = decrease_value(initial_release_date)\n        else:  # Non-daily patch release\n            assert isinstance(current_version.release.value, int)\n\n            new_tag_value = decrease_value(current_version.release.value)\n\n        new_image = create_image(new_tag_value)\n\n        # Limit the amount of attempts to find a replacement tag\n        retries = 10\n        for _try_number in range(1, retries + 1):\n            if self.image_exists(new_image, edition):\n                return new_image.tag\n\n            new_tag_value = decrease_value(new_tag_value)\n            new_image = create_image(new_tag_value)\n\n        raise RuntimeError(\n            f\"We have been unable to find an existing previous version for {image.tag} after {retries} attempts. Aborting.\"\n        )\n\n    def tag(self, source: FQIN, new_tag: DockerTag) -> None:\n        \"Create a tag for `source` with the tag given as `new_tag`\"\n\n        if not new_tag.strip():\n            raise ValueError(f\"Please supply a tag as the target version, not {new_tag.strip()!r}\")\n\n        # We want to make sure that we have the path to the registry included,\n        # so that we have a proper FQIN.\n        registry_addr = urlparse(self.url).netloc\n        if registry_addr not in source:\n            source = f\"{registry_addr}/{source}\"\n\n        source_docker_image = DockerImage.from_str(source)\n\n        image = self.client.images.get(source_docker_image.full_name())\n        image.tag(\n            repository=source_docker_image.image_name,\n            tag=new_tag,\n        )\n        # Reloading is required to have the new tag saved\n        image.reload()\n\n        # Push the changes to the registry\n        resp = self.client.images.push(\n            repository=source_docker_image.image_name,\n            tag=new_tag,\n            stream=True,\n            decode=True,\n        )\n        for line in resp:\n            print(line)\n\n    def __post_init__(self):\n        self.client = docker.client.from_env()\n        self.credentials = get_credentials()\n        match self.editions:\n            case [\"enterprise\"]:\n                self.url = \"https://registry.checkmk.com\"\n                # Asking why we're also pulling? -> CMK-14567\n                self.image_exists = self.image_exists_and_can_be_pulled_enterprise\n                self.get_image_tags = self._get_image_tags_enterprise\n                self.client.login(\n                    registry=self.url,\n                    username=self.credentials.username,\n                    password=self.credentials.password,\n                )\n            case [\"raw\", \"cloud\", \"managed\"]:\n                self.url = \"https://hub.docker.com/\"\n                self.image_exists = self.image_exists_docker_hub\n                self.get_image_tags = self._get_image_tags_docker_hub\n            case [\"saas\"]:\n                self.url = \"https://artifacts.lan.tribe29.com:4000\"\n                self.image_exists = self.image_exists_enterprise\n                # For nexus, d-intern is not authorized\n                self.credentials = Credentials(\n                    username=environ[\"NEXUS_USER\"],\n                    password=environ[\"NEXUS_PASSWORD\"],\n                )\n                # According to https://community.sonatype.com/t/api-get-latest-tags-of-all-docker-images/7837/3\n                # it looks like we can simply reuse how we talk to our enterprise repo\n                self.get_image_tags = self._get_image_tags_enterprise\n            case _:\n                raise RuntimeError(f\"Cannnot match editions to registry: {self.editions}\")\n\n\ndef get_credentials() -> Credentials:\n    return Credentials(*_get_cmk_download_credentials())\n\n\ndef get_default_registries() -> list[Registry]:\n    return [\n        Registry(\n            editions=[\"enterprise\"],\n        ),\n        Registry(\n            editions=[\"raw\", \"cloud\", \"managed\"],\n        ),\n        Registry(\n            editions=[\"saas\"],\n        ),\n    ]\n\n\ndef edition_to_registry(edition: str, registries: list[Registry]) -> Registry:\n    for registry in registries:\n        if edition in registry.editions:\n            return registry\n    raise RuntimeError(f\"Cannot determine registry for edition: {edition}!\")\n"}
{"type": "source_file", "path": "cmk/base/api/agent_based/cluster_mode.py", "content": "#!/usr/bin/env python3\n# Copyright (C) 2019 Checkmk GmbH - License: GNU General Public License v2\n# This file is part of Checkmk (https://checkmk.com). It is subject to the terms and\n# conditions defined in the file COPYING, which is part of this source code package.\n\"\"\"Compute the cluster check function from the plug-in and parameters.\"\"\"\n\nfrom collections import defaultdict\nfrom collections.abc import Callable, Iterable, Mapping, Sequence\nfrom functools import partial\nfrom typing import Any, Final, Literal, NamedTuple, Protocol\n\nfrom cmk.utils.hostaddress import HostName\n\nfrom cmk.checkengine.checking import ServiceID\nfrom cmk.checkengine.checkresults import state_markers\nfrom cmk.checkengine.value_store import ValueStoreManager\n\nfrom cmk.base.api.agent_based.plugin_classes import CheckPlugin\n\nfrom cmk.agent_based.v1 import IgnoreResults, IgnoreResultsError, Metric, Result, State\nfrom cmk.agent_based.v1.type_defs import CheckResult\n\n_Kwargs = Mapping[str, Any]\n\n_NON_SECTION_KEYS: Final = {\"item\", \"params\"}\n\n_INF = float(\"inf\")\n\nClusterMode = Literal[\"native\", \"failover\", \"worst\", \"best\"]\n\n\nclass Selector(Protocol):\n    def __call__(self, *a: State) -> State: ...\n\n\ndef _unfit_for_clustering(**_kw: object) -> CheckResult:\n    \"\"\"A cluster_check_function that displays a generic warning\"\"\"\n    yield Result(\n        state=State.UNKNOWN,\n        summary=(\n            \"This service does not implement a native cluster mode. Please change your \"\n            \"configuration using the rule 'Aggregation options for clustered services', \"\n            \"and select one of the other available aggregation modes.\"\n        ),\n    )\n\n\ndef get_cluster_check_function(\n    mode: ClusterMode,\n    clusterization_parameters: Mapping[str, Any],\n    *,\n    service_id: ServiceID,\n    plugin: CheckPlugin,\n    value_store_manager: ValueStoreManager,\n) -> Callable[..., Iterable[object]]:\n    if mode == \"native\":\n        return plugin.cluster_check_function or _unfit_for_clustering\n\n    executor = NodeCheckExecutor(\n        service_id=service_id,\n        value_store_manager=value_store_manager,\n    )\n\n    if mode == \"failover\":\n        return partial(\n            _cluster_check,\n            clusterization_parameters=clusterization_parameters,\n            executor=executor,\n            check_function=plugin.check_function,\n            label=\"active\",\n            selector=State.worst,\n            levels_additional_nodes_count=(1, _INF),\n            unpreferred_node_state=State.WARN,\n        )\n\n    if mode == \"worst\":\n        return partial(\n            _cluster_check,\n            clusterization_parameters=clusterization_parameters,\n            executor=executor,\n            check_function=plugin.check_function,\n            label=\"worst\",\n            selector=State.worst,\n            levels_additional_nodes_count=(_INF, _INF),\n            unpreferred_node_state=State.OK,\n        )\n\n    if mode == \"best\":\n        return partial(\n            _cluster_check,\n            clusterization_parameters=clusterization_parameters,\n            executor=executor,\n            check_function=plugin.check_function,\n            label=\"best\",\n            selector=State.best,\n            levels_additional_nodes_count=(_INF, _INF),\n            unpreferred_node_state=State.OK,\n        )\n\n    raise ValueError(mode)\n\n\ndef _cluster_check(\n    *,\n    clusterization_parameters: Mapping[str, Any],\n    executor: \"NodeCheckExecutor\",\n    check_function: Callable,\n    label: str,\n    selector: Selector,\n    levels_additional_nodes_count: tuple[float, float],\n    unpreferred_node_state: State,\n    **cluster_kwargs: Any,\n) -> CheckResult:\n    summarizer = Summarizer(\n        node_results=executor(check_function, cluster_kwargs),\n        label=label,\n        selector=selector,\n        preferred=clusterization_parameters.get(\"primary_node\"),\n        unpreferred_node_state=unpreferred_node_state,\n    )\n    if summarizer.is_empty():\n        return summarizer.raise_for_ignores()\n\n    yield from summarizer.primary_results()\n\n    yield from summarizer.secondary_results(\n        levels_additional_nodes_count=levels_additional_nodes_count\n    )\n\n    yield from summarizer.metrics(clusterization_parameters.get(\"metrics_node\"))\n    return None\n\n\nclass NodeResults(NamedTuple):\n    results: Mapping[HostName, Sequence[Result]]\n    metrics: Mapping[HostName, Sequence[Metric]]\n    ignore_results: Mapping[HostName, Sequence[IgnoreResults]]\n\n\nclass Summarizer:\n    def __init__(\n        self,\n        *,\n        node_results: NodeResults,\n        label: str,\n        selector: Selector,\n        preferred: HostName | None,\n        unpreferred_node_state: State,\n    ) -> None:\n        self._node_results = node_results\n        self._label = label.title()\n        self._selector = selector\n        self._preferred = preferred\n        self._unpreferred_node_state = unpreferred_node_state\n\n        selected_nodes = self._get_selected_nodes(node_results.results, selector)\n        # fallback: arbitrary, but comprehensible choice.\n        self._pivoting = preferred if preferred in selected_nodes else sorted(selected_nodes)[0]\n\n    @staticmethod\n    def _get_selected_nodes(\n        results_map: Mapping[HostName, Sequence[Result]],\n        selector: Selector,\n    ) -> set[HostName]:\n        \"\"\"Determine the best/worst nodes names\"\"\"\n        nodes_by_states = defaultdict(set)\n        for node, results in ((n, r) for n, r in results_map.items() if r):\n            nodes_by_states[State.worst(*(r.state for r in results))].add(node)\n\n        return nodes_by_states[selector(*nodes_by_states)] if nodes_by_states else set(results_map)\n\n    def is_empty(self) -> bool:\n        return not any(self._node_results.results.values())\n\n    def raise_for_ignores(self) -> None:\n        if msgs := [\n            f\"[{node}] {', '.join(str(i) for i in ign)}\"\n            for node, ign in self._node_results.ignore_results.items()\n            if ign\n        ]:\n            raise IgnoreResultsError(\", \".join(msgs))\n\n    def primary_results(self) -> Iterable[Result]:\n        if self._preferred is None or self._preferred == self._pivoting:\n            yield Result(state=State.OK, summary=f\"{self._label}: [{self._pivoting}]\")\n        else:\n            yield Result(\n                state=self._unpreferred_node_state,\n                summary=f\"{self._label}: [{self._pivoting}]\",\n                details=f\"{self._label}: [{self._pivoting}], Preferred node is [{self._preferred}]\",\n            )\n        yield from self._node_results.results[self._pivoting]\n\n    def secondary_results(\n        self,\n        *,\n        levels_additional_nodes_count: tuple[float, float],\n    ) -> Iterable[Result]:\n        secondary_nodes = sorted(\n            node\n            for node, results in self._node_results.results.items()\n            if node != self._pivoting and results\n        )\n        if not secondary_nodes:\n            return\n\n        yield Result(\n            state=self._secondary_nodes_state(secondary_nodes, levels_additional_nodes_count),\n            summary=f\"Additional results from: {', '.join(f'[{n}]' for n in secondary_nodes)}\",\n        )\n        yield from (\n            Result(\n                state=State.OK,\n                notice=r.summary if r.summary else r.details,\n                details=f\"{r.details}{state_markers[int(r.state)]}\",\n            )\n            for node in secondary_nodes\n            for r in self._node_results.results[node]\n        )\n\n    @staticmethod\n    def _secondary_nodes_state(\n        secondary_nodes: Sequence[str],\n        levels: tuple[float, float],\n    ) -> State:\n        count = len(secondary_nodes)\n        return State.CRIT if count >= levels[1] else State(count >= levels[0])\n\n    def metrics(self, node_name: HostName | None) -> CheckResult:\n        used_node = node_name or self._pivoting\n        if not (metrics := self._node_results.metrics.get(used_node, ())):\n            return\n\n        yield Result(\n            state=State.OK,\n            notice=f\"[{used_node}] Metrics: {', '.join(m.name for m in metrics)}\",\n        )\n        yield from metrics\n\n\nclass NodeCheckExecutor:\n    def __init__(\n        self,\n        *,\n        service_id: ServiceID,\n        value_store_manager: ValueStoreManager,\n    ) -> None:\n        self._service_id = service_id\n        self._value_store_manager = value_store_manager\n\n    def __call__(\n        self,\n        check_function: Callable[..., CheckResult],\n        cluster_kwargs: _Kwargs,\n    ) -> NodeResults:\n        \"\"\"Dispatch the check function results for all nodes\"\"\"\n        results = {}\n        metrics = {}\n        ignores = {}\n\n        for node, kwargs in self._iter_node_kwargs(cluster_kwargs):\n            elements = self._consume_checkresult(\n                node, check_function(**kwargs), self._value_store_manager\n            )\n            metrics[node] = [e for e in elements if isinstance(e, Metric)]\n            ignores[node] = [e for e in elements if isinstance(e, IgnoreResults)]\n            results[node] = [\n                self._add_node_name(e, node) for e in elements if isinstance(e, Result)\n            ]\n\n        return NodeResults(results, metrics, ignores)\n\n    def _iter_node_kwargs(self, cluster_kwargs: _Kwargs) -> Iterable[tuple[HostName, _Kwargs]]:\n        \"\"\"create kwargs for every nodes check function\"\"\"\n        section_names = set(cluster_kwargs) - _NON_SECTION_KEYS\n        all_nodes: set[HostName] = {\n            node for section_name in section_names for node in cluster_kwargs[section_name]\n        }\n        yield from (\n            (node, kwargs)\n            for node, kwargs in self._extract_node_kwargs(sorted(all_nodes), cluster_kwargs)\n            if self._contains_data(kwargs)\n        )\n\n    @staticmethod\n    def _extract_node_kwargs(\n        nodes: Iterable[HostName],\n        cluster_kwargs: _Kwargs,\n    ) -> Iterable[tuple[HostName, _Kwargs]]:\n        yield from (\n            (n, {k: v if k in _NON_SECTION_KEYS else v.get(n) for k, v in cluster_kwargs.items()})\n            for n in nodes\n        )\n\n    @staticmethod\n    def _contains_data(node_kwargs: _Kwargs) -> bool:\n        return any(v is not None for k, v in node_kwargs.items() if k not in _NON_SECTION_KEYS)\n\n    def _consume_checkresult(\n        self,\n        node: HostName,\n        result_generator: CheckResult,\n        value_store_manager: ValueStoreManager,\n    ) -> Sequence[Result | Metric | IgnoreResults]:\n        with value_store_manager.namespace(self._service_id, host_name=node):\n            try:\n                return list(result_generator)\n            except IgnoreResultsError as exc:\n                return [IgnoreResults(str(exc))]\n\n    @staticmethod\n    def _add_node_name(result: Result, node_name: str) -> Result:\n        return Result(\n            state=result.state,\n            summary=\"FAKE\",\n            details=\"\\n\".join(f\"[{node_name}]: {line}\" for line in result.details.splitlines()),\n        )._replace(summary=result.summary)\n"}
{"type": "source_file", "path": "buildscripts/scripts/collect_packages.py", "content": "#!/usr/bin/env python3\n# Copyright (C) 2024 Checkmk GmbH - License: GNU General Public License v2\n# This file is part of Checkmk (https://checkmk.com). It is subject to the terms and\n# conditions defined in the file COPYING, which is part of this source code package.\nimport argparse\nimport json\nimport sys\nfrom collections.abc import Iterator\nfrom dataclasses import asdict, dataclass, field\nfrom pathlib import Path\n\n\n@dataclass\nclass Package:\n    name: str\n    path: str\n    command_line: str\n    maintainers: list[str] = field(default_factory=lambda: [])\n    sec_vars: list[str] = field(default_factory=lambda: [])\n    dependencies: list[str] = field(default_factory=lambda: [])\n\n    def __post_init__(self):\n        # Fallback for now...\n        self.maintainers.append(\"team-ci@checkmk.com\")\n\n\ndef parse_arguments() -> argparse.Namespace:\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"packages_path\", nargs=\"+\", help=\"Paths to the packages to collect\")\n    return parser.parse_args()\n\n\ndef parse_package(meta_file: Path, name: str) -> Package:\n    with open(meta_file) as f:\n        data = {**json.load(f), \"name\": name, \"path\": str(meta_file.parent)}\n        return Package(**data)\n\n\ndef discover_packages(args: argparse.Namespace) -> Iterator[Package]:\n    for packages_path in args.packages_path:\n        for meta_file in Path(packages_path).rglob(\"ci.json\"):\n            try:\n                package = parse_package(meta_file, meta_file.parent.name)\n            except Exception as e:\n                sys.stderr.write(f\"Skipping {meta_file} as it has invalid meta data: {e}\\n\")\n                continue\n            yield package\n\n\ndef main():\n    args = parse_arguments()\n    print(json.dumps([asdict(p) for p in discover_packages(args)], indent=2))\n\n\nif __name__ == \"__main__\":\n    main()\n"}
{"type": "source_file", "path": "cmk/automations/__init__.py", "content": "#!/usr/bin/env python3\n# Copyright (C) 2019 Checkmk GmbH - License: GNU General Public License v2\n# This file is part of Checkmk (https://checkmk.com). It is subject to the terms and\n# conditions defined in the file COPYING, which is part of this source code package.\n"}
{"type": "source_file", "path": "cmk/active_checks/__init__.py", "content": "#!/usr/bin/env python3\n# Copyright (C) 2023 Checkmk GmbH - License: GNU General Public License v2\n# This file is part of Checkmk (https://checkmk.com). It is subject to the terms and\n# conditions defined in the file COPYING, which is part of this source code package.\n"}
{"type": "source_file", "path": "cmk/active_checks/check_traceroute.py", "content": "#!/usr/bin/env python3\n# Copyright (C) 2023 Checkmk GmbH - License: GNU General Public License v2\n# This file is part of Checkmk (https://checkmk.com). It is subject to the terms and\n# conditions defined in the file COPYING, which is part of this source code package.\n\n# This check does a traceroute to the specified target host\n# (usually $HOSTADDRESS$ itself) and checks which route(s) are\n# being taken. That way you can check if your preferred or\n# some alternative route in in place.\n# traceroute is expected to be in the search path and installed\n# with SUID root bit.\n\n# Example output from traceroute -n\n# traceroute to www.google.de (173.194.44.55), 30 hops max, 60 byte packets\n#  1  10.10.11.4  0.419 ms  0.444 ms  0.518 ms\n#  2  33.117.16.28  14.359 ms  14.371 ms  14.434 ms\n#  3  112.18.7.119  14.750 ms  14.765 ms  19.530 ms\n#  4  184.50.190.61  17.844 ms  17.865 ms  17.862 ms\n#  5  67.249.94.88  24.285 ms  78.527 ms  26.834 ms\n#  6  209.85.240.99  27.910 ms  27.420 ms  27.442 ms\n#  7  173.194.44.55  26.583 ms  20.410 ms  23.142 ms\n\n# Output without -n option:\n# traceroute to www.google.de (173.194.44.56), 30 hops max, 60 byte packets\n#  1  fritz.box (10.10.11.4)  0.570 ms  0.606 ms  0.677 ms\n#  2  foo-bar.x-online.net (33.117.16.28)  14.566 ms  14.580 ms  14.658 ms\n#  3  xu-2-3-0.rt-inxs-1.x-online.net (112.13.6.109)  18.214 ms  18.228 ms  18.221 ms\n#  4  * * *\n#  5  66.249.94.88 (66.249.94.88)  24.481 ms  24.498 ms  24.271 ms\n#  6  209.85.240.99 (209.85.240.99)  27.628 ms  21.605 ms  21.943 ms\n#  7  muc03s08-in-f24.1e100.net (173.194.44.56)  21.277 ms  22.236 ms  22.192 ms\n\n# Example output for IPv6\n# traceroute to ipv6.google.com (2404:6800:4004:80e::200e), 30 hops max, 80 byte packets\n#  1  2001:2e8:665:0:2:2:0:1 (2001:2e8:665:0:2:2:0:1)  0.082 ms  0.046 ms  0.044 ms\n#  2  2001:2e8:22:204::2 (2001:2e8:22:204::2)  0.893 ms  0.881 ms  0.961 ms\n#  3  * 2001:4860:0:1::1abd (2001:4860:0:1::1abd)  225.189 ms *\n#  4  2001:4860:0:1003::1 (2001:4860:0:1003::1)  3.052 ms  2.820 ms 2001:4860:0:1002::1 (2001:4860:0:1002::1)  1.501 ms\n#  5  nrt13s48-in-x0e.1e100.net (2404:6800:4004:80e::200e)  1.910 ms  1.828 ms  1.753 ms\n\n# It is also possible that for one hop several different answers appear:\n# 11 xe-0-0-1-0.co2-96c-1b.ntwk.msn.net (204.152.141.11)  174.185 ms xe-10-0-2-0.co1-96c-1a.ntwk.msn.net (207.46.40.94)  174.279 ms xe-0-0-1-0.co2-96c-1b.ntwk.msn.net (204.152.141.11)  174.444 ms\n\n# if DNS fails then it looks like this:\n#  5  66.249.94.88 (66.249.94.88)  24.481 ms  24.498 ms  24.271 ms\n#  6  209.85.240.99 (209.85.240.99)  27.628 ms  21.605 ms  21.943 ms\n\nfrom __future__ import annotations\n\nimport argparse\nimport enum\nimport os\nimport re\nimport subprocess\nimport sys\nfrom collections.abc import Iterable, Iterator, Sequence\nfrom dataclasses import dataclass\nfrom typing import assert_never, Protocol\n\n\ndef main(\n    argv: Sequence[str] | None = None,\n    routetracer: RoutetracerProto | None = None,\n) -> int:\n    exitcode, info, perf = _check_traceroute_main(\n        argv or sys.argv[1:],\n        routetracer or _TracerouteRoutertrace(),\n    )\n    _output_check_result(info, perf)\n    return exitcode\n\n\nclass RoutetracerProto(Protocol):\n    def __call__(\n        self,\n        target: str,\n        *,\n        use_dns: bool,\n        probe_method: ProbeMethod,\n        ip_address_family: IPAddressFamily,\n    ) -> Route: ...\n\n\nclass ProbeMethod(enum.Enum):\n    UDP = \"udp\"\n    ICMP = \"icmp\"\n    TCP = \"tcp\"\n\n\nclass IPAddressFamily(enum.Enum):\n    AUTO = \"auto\"\n    v4 = \"ipv4\"\n    v6 = \"ipv6\"\n\n\n@dataclass(frozen=True)\nclass Route:\n    routers: set[str]\n    n_hops: int\n    human_readable_route: str\n\n\ndef _output_check_result(s: str, perfdata: Iterable[tuple[str, int]] | None) -> None:\n    if perfdata:\n        perfdata_output_entries = [\n            \"{}={}\".format(p[0], \";\".join(map(str, p[1:]))) for p in perfdata\n        ]\n        s += \" | %s\" % \" \".join(perfdata_output_entries)\n    sys.stdout.write(\"%s\\n\" % s)\n\n\ndef _check_traceroute_main(\n    argv: Sequence[str],\n    routetracer: RoutetracerProto,\n) -> tuple[int, str, list[tuple[str, int]] | None]:\n    args = _parse_arguments(argv)\n\n    try:\n        return _check_route(\n            routetracer(\n                args.target,\n                use_dns=args.use_dns,\n                probe_method=args.probe_method,\n                ip_address_family=args.ip_address_family,\n            ),\n            routers_missing_warn=args.routers_missing_warn,\n            routers_missing_crit=args.routers_missing_crit,\n            routers_found_warn=args.routers_found_warn,\n            routers_found_crit=args.routers_found_crit,\n        )\n\n    except _RoutetracingError as e:\n        return 3, str(e), None\n\n    except Exception as e:\n        if args.debug:\n            raise\n        return 2, f\"Unhandled exception: {e}\", None\n\n\ndef _parse_arguments(argv: Sequence[str]) -> argparse.Namespace:\n    parser = argparse.ArgumentParser(\n        description=\"Check the route to a network host using the traceroute command.\",\n    )\n    parser.add_argument(\n        \"target\",\n        type=str,\n        metavar=\"TARGET\",\n        help=\"Can be specified either as an IP address or as a domain name.\",\n    )\n    parser.add_argument(\n        \"--routers_missing_warn\",\n        type=str,\n        nargs=\"*\",\n        metavar=\"ROUTER1 ROUTER2 ...\",\n        help=\"Report WARNING if any of these routers is not used.\",\n        default=[],\n    )\n    parser.add_argument(\n        \"--routers_missing_crit\",\n        type=str,\n        nargs=\"*\",\n        metavar=\"ROUTER1 ROUTER2 ...\",\n        help=\"Report CRITICAL if any of these routers is not used.\",\n        default=[],\n    )\n    parser.add_argument(\n        \"--routers_found_warn\",\n        type=str,\n        nargs=\"*\",\n        metavar=\"ROUTER1 ROUTER2 ...\",\n        help=\"Report WARNING if any of these routers is used.\",\n        default=[],\n    )\n    parser.add_argument(\n        \"--routers_found_crit\",\n        type=str,\n        nargs=\"*\",\n        metavar=\"ROUTER1 ROUTER2 ...\",\n        help=\"Report CRITICAL if any of these routers is used.\",\n        default=[],\n    )\n    parser.add_argument(\n        \"--ip_address_family\",\n        type=IPAddressFamily,\n        choices=IPAddressFamily,\n        default=IPAddressFamily.AUTO,\n        metavar=\"IP-ADDRESS-FAMILY\",\n        help=\"Explicitly force IPv4 or IPv6 traceouting. By default, the program will choose the \"\n        \"appropriate protocol automatically.\",\n    )\n    parser.add_argument(\n        \"--probe_method\",\n        type=ProbeMethod,\n        choices=ProbeMethod,\n        default=ProbeMethod.UDP,\n        metavar=\"PROBE-METHOD\",\n        help=\"Method used for tracerouting. By default, UDP datagrams are used.\",\n    )\n    parser.add_argument(\n        \"--use_dns\",\n        action=\"store_true\",\n        help=\"Use DNS to convert host names to IP addresses.\",\n    )\n    parser.add_argument(\n        \"--debug\",\n        action=\"store_true\",\n        help=\"Debug mode: let Python exceptions come through.\",\n    )\n    return parser.parse_args(argv)\n\n\nclass _RoutetracingError(Exception):\n    pass\n\n\ndef _check_route(\n    route: Route,\n    *,\n    routers_missing_warn: Iterable[str] = (),\n    routers_missing_crit: Iterable[str] = (),\n    routers_found_warn: Iterable[str] = (),\n    routers_found_crit: Iterable[str] = (),\n) -> tuple[int, str, list[tuple[str, int]]]:\n    missing_routers_warn = [\n        _mark_warning(router) for router in sorted(set(routers_missing_warn) - route.routers)\n    ]\n    missing_routers_crit = [\n        _mark_critical(router) for router in sorted(set(routers_missing_crit) - route.routers)\n    ]\n    found_routers_warn = [\n        _mark_warning(router) for router in sorted(set(routers_found_warn) & route.routers)\n    ]\n    found_routers_crit = [\n        _mark_critical(router) for router in sorted(set(routers_found_crit) & route.routers)\n    ]\n\n    return (\n        (\n            2\n            if any(missing_routers_crit + found_routers_crit)\n            else 1\n            if any(missing_routers_warn + found_routers_warn)\n            else 0\n        ),\n        f\"%d hop{'' if route.n_hops == 1 else 's'}, missing routers: %s, bad routers: %s\\n%s\"\n        % (\n            route.n_hops,\n            \", \".join(missing_routers_crit + missing_routers_warn) or \"none\",\n            \", \".join(found_routers_crit + found_routers_warn) or \"none\",\n            route.human_readable_route,\n        ),\n        [(\"hops\", route.n_hops)],\n    )\n\n\ndef _mark_warning(router: str) -> str:\n    return f\"{router}(!)\"\n\n\ndef _mark_critical(router: str) -> str:\n    return f\"{router}(!!)\"\n\n\nclass _TracerouteRoutertrace:\n    def __call__(\n        self,\n        target: str,\n        *,\n        use_dns: bool,\n        probe_method: ProbeMethod,\n        ip_address_family: IPAddressFamily,\n    ) -> Route:\n        traceroute_stdout = self._execute_traceroute(\n            target,\n            use_dns=use_dns,\n            probe_method=probe_method,\n            ip_address_family=ip_address_family,\n        )\n        router_lines = traceroute_stdout.splitlines()[1:]\n        return Route(\n            {router for line in router_lines for router in self._extract_routers_from_line(line)},\n            len(router_lines),\n            traceroute_stdout,\n        )\n\n    @staticmethod\n    def _execute_traceroute(\n        target: str,\n        *,\n        use_dns: bool,\n        probe_method: ProbeMethod,\n        ip_address_family: IPAddressFamily,\n    ) -> str:\n        cmd = [\"traceroute\", target]\n        if not use_dns:\n            cmd.append(\"-n\")\n\n        match probe_method:\n            case ProbeMethod.UDP:\n                pass\n            case ProbeMethod.ICMP:\n                cmd.append(\"-I\")\n            case ProbeMethod.TCP:\n                cmd.append(\"-T\")\n            case _:\n                assert_never(probe_method)\n\n        match ip_address_family:\n            case IPAddressFamily.AUTO:\n                pass\n            case IPAddressFamily.v4:\n                cmd.append(\"-4\")\n            case IPAddressFamily.v6:\n                cmd.append(\"-6\")\n            case _:\n                assert_never(ip_address_family)\n\n        completed_process = subprocess.run(\n            cmd,\n            capture_output=True,\n            encoding=\"utf8\",\n            check=False,\n            env={k: v for k, v in os.environ.items() if k != \"LANG\"},\n        )\n        if completed_process.returncode:\n            raise _RoutetracingError(f\"traceroute command failed: {completed_process.stderr}\")\n        return completed_process.stdout\n\n    @staticmethod\n    def _extract_routers_from_line(line: str) -> Iterator[str]:\n        \"\"\"\n        >>> list(_TracerouteRoutertrace._extract_routers_from_line('10  209.85.252.215  16.133 ms 108.170.238.61  12.731 ms 209.85.252.215  15.088 ms'))\n        ['209.85.252.215', '108.170.238.61', '209.85.252.215']\n        >>> list(_TracerouteRoutertrace._extract_routers_from_line(' 5  fra1.cc1.as48314.net (2a0a:51c1:0:4002::51)  231.003 ms !X  37.416 ms !X  230.950 ms !X 2001:4860:0:1::10d9  13.441 ms *'))\n        ['fra1.cc1.as48314.net', '2a0a:51c1:0:4002::51', '2001:4860:0:1::10d9']\n        >>> list(_TracerouteRoutertrace._extract_routers_from_line(' 7  * * *'))\n        []\n        \"\"\"\n        # drop asterisks, which mean no response from the router at this hop\n        line = line.replace(\"*\", \"\")\n        # drop round-trip times\n        line = re.sub(r\"[0-9]+(\\.[0-9]+)? ms\", \"\", line)\n        yield from (\n            part.lstrip(\"(\").rstrip(\")\")\n            for part in line.strip().split()[\n                # drop numbering\n                1:\n            ]\n            # drop additional information such as !X\n            if not part.startswith(\"!\")\n        )\n"}
{"type": "source_file", "path": "buildscripts/scripts/get_distros.py", "content": "#!/usr/bin/env python3\n# Copyright (C) 2023 Checkmk GmbH - License: GNU General Public License v2\n# This file is part of Checkmk (https://checkmk.com). It is subject to the terms and\n# conditions defined in the file COPYING, which is part of this source code package.\n\nimport sys\nfrom argparse import ArgumentParser\nfrom argparse import Namespace as Args\nfrom collections.abc import Iterable\nfrom pathlib import Path\n\nsys.path.insert(0, Path(__file__).parent.parent.parent.as_posix())\nfrom buildscripts.scripts.lib.common import flatten, load_editions_file\n\n\ndef print_internal_build_artifacts(args: Args, loaded_yaml: dict) -> None:\n    distros = flatten(loaded_yaml.get(\"internal_distros\", []))\n    editions = flatten(loaded_yaml[\"internal_editions\"])\n\n    if args.as_codename:\n        if diff := distros - loaded_yaml[\"distro_to_codename\"].keys():\n            raise Exception(\n                f\"{args.editions_file} is missing the distro code for the following distros: \"\n                f\"{diff}. Please add the corresponding distro code.\"\n            )\n        distros = [loaded_yaml[\"distro_to_codename\"][d] for d in distros]\n    if args.as_rsync_exclude_pattern:\n        exclude_elements = list(distros) + list(editions) + [\"omd\", \"bazel\"]\n        patterns = \",\".join([f\"'*{d}*'\" for d in exclude_elements])\n        if len(exclude_elements) > 1:\n            print(\"{\" + patterns + \"}\")  # this expands in bash\n        else:\n            print(patterns)\n        return\n\n    print(\" \".join(sorted(set(distros).union(set(editions)))))\n\n\ndef distros_for_use_case(edition_distros: dict, edition: str, use_case: str) -> Iterable[str]:\n    return sorted(\n        {\n            distro\n            for _edition, use_cases in edition_distros.items()\n            if edition in (_edition, \"all\")\n            for _use_case, distros in use_cases.items()\n            if use_case in (_use_case, \"all\")\n            for distro in flatten(distros)\n        }\n    )\n\n\ndef print_distros_for_use_case(args: Args, loaded_yaml: dict) -> None:\n    edition_distros = loaded_yaml[\"editions\"]\n    edition = args.edition or \"all\"\n    use_case = args.use_case or \"all\"\n    print(\" \".join(distros_for_use_case(edition_distros, edition, use_case)))\n\n\ndef print_editions(_args: Args, loaded_yaml: dict) -> None:\n    print(\" \".join(sorted(loaded_yaml[\"editions\"].keys())))\n\n\ndef test_distro_lists():\n    edition_distros = load_editions_file(Path(__file__).parent.parent.parent / \"editions.yml\")[\n        \"editions\"\n    ]\n\n    # fmt: off\n    assert distros_for_use_case(edition_distros, \"enterprise\", \"release\") == [\n        \"almalinux-9\",\n        \"cma-4\",\n        \"debian-11\", \"debian-12\",\n        \"sles-15sp3\", \"sles-15sp4\", \"sles-15sp5\", \"sles-15sp6\",\n        \"ubuntu-22.04\", \"ubuntu-24.04\",\n    ]\n    assert distros_for_use_case(edition_distros, \"enterprise\", \"daily\") == [\n        \"almalinux-9\",\n        \"cma-4\",\n        \"debian-11\", \"debian-12\",\n        \"sles-15sp3\", \"sles-15sp4\", \"sles-15sp5\", \"sles-15sp6\",\n        \"ubuntu-22.04\", \"ubuntu-23.10\", \"ubuntu-24.04\",\n    ]\n    assert distros_for_use_case(edition_distros, \"all\", \"all\") == [\n        \"almalinux-9\",\n        \"cma-4\",\n        \"debian-11\", \"debian-12\",\n        \"sles-15sp3\", \"sles-15sp4\", \"sles-15sp5\", \"sles-15sp6\",\n        \"ubuntu-22.04\", \"ubuntu-23.10\", \"ubuntu-24.04\"\n    ]\n    # fmt: on\n\n\ndef parse_arguments() -> Args:\n    parser = ArgumentParser()\n\n    parser.add_argument(\"--editions_file\", required=True)\n\n    subparsers = parser.add_subparsers(required=True, dest=\"command\")\n\n    all_distros_subparser = subparsers.add_parser(\n        \"all\", help=\"Print distros for all use case and all distros\"\n    )\n    all_distros_subparser.set_defaults(func=print_distros_for_use_case)\n    all_distros_subparser.add_argument(\"--edition\", default=\"all\")\n    all_distros_subparser.add_argument(\"--use_case\", default=\"all\")\n\n    use_cases_subparser = subparsers.add_parser(\"use_cases\", help=\"Print distros for use case\")\n    use_cases_subparser.set_defaults(func=print_distros_for_use_case)\n    use_cases_subparser.add_argument(\"--edition\", required=True)\n    use_cases_subparser.add_argument(\"--use_case\", required=True)\n\n    editions_subparser = subparsers.add_parser(\"editions\", help=\"Print all supported edtions\")\n    editions_subparser.set_defaults(func=print_editions)\n\n    internal_build_artifacts = subparsers.add_parser(\"internal_build_artifacts\")\n    internal_build_artifacts.set_defaults(func=print_internal_build_artifacts)\n    internal_build_artifacts.add_argument(\"--as-codename\", default=False, action=\"store_true\")\n    internal_build_artifacts.add_argument(\n        \"--as-rsync-exclude-pattern\", default=False, action=\"store_true\"\n    )\n\n    return parser.parse_args()\n\n\ndef main() -> None:\n    args = parse_arguments()\n    args.func(args, load_editions_file(args.editions_file))\n\n\nif __name__ == \"__main__\":\n    main()\n"}
{"type": "source_file", "path": "cmk/base/__init__.py", "content": "#!/usr/bin/env python3\n# Copyright (C) 2019 Checkmk GmbH - License: GNU General Public License v2\n# This file is part of Checkmk (https://checkmk.com). It is subject to the terms and\n# conditions defined in the file COPYING, which is part of this source code package.\n\n#\n#    This file must not contain any code.\n#\n#    It is only here to make our linters happy.\n#    It will not be deployed.\n#\n#    This folder is part of a namespace package, that can be shadowed/extended\n#    using the local/ hierarchy.\n#\n#    Do not change the following line, it is picked up by the build process:\n#    check_mk.make: do-not-deploy\n#\n"}
{"type": "source_file", "path": "cmk/automations/helper_api.py", "content": "#!/usr/bin/env python3\n# Copyright (C) 2025 Checkmk GmbH - License: GNU General Public License v2\n# This file is part of Checkmk (https://checkmk.com). It is subject to the terms and\n# conditions defined in the file COPYING, which is part of this source code package.\n\nfrom collections.abc import Sequence\n\nfrom pydantic import BaseModel\n\n\nclass AutomationPayload(BaseModel, frozen=True):\n    name: str\n    args: Sequence[str]\n    stdin: str\n    log_level: int\n\n\nclass AutomationResponse(BaseModel, frozen=True):\n    serialized_result_or_error_code: str | int\n    stdout: str\n    stderr: str\n"}
{"type": "source_file", "path": "cmk/automations/results.py", "content": "#!/usr/bin/env python3\n# Copyright (C) 2019 Checkmk GmbH - License: GNU General Public License v2\n# This file is part of Checkmk (https://checkmk.com). It is subject to the terms and\n# conditions defined in the file COPYING, which is part of this source code package.\n\nfrom __future__ import annotations\n\nimport json\nimport socket\nfrom abc import ABC, abstractmethod\nfrom ast import literal_eval\nfrom collections.abc import Mapping, Sequence\nfrom dataclasses import asdict, astuple, dataclass, field\nfrom typing import Any, Literal, TypedDict, TypeVar\n\nfrom cmk.ccc import version as cmk_version\nfrom cmk.ccc.plugin_registry import Registry\n\nfrom cmk.utils.agentdatatype import AgentRawData\nfrom cmk.utils.check_utils import ParametersTypeAlias\nfrom cmk.utils.config_warnings import ConfigurationWarnings\nfrom cmk.utils.hostaddress import HostAddress, HostName\nfrom cmk.utils.ip_lookup import IPStackConfig\nfrom cmk.utils.labels import HostLabel, HostLabelValueDict, Labels, LabelSources\nfrom cmk.utils.notify_types import NotifyAnalysisInfo, NotifyBulks\nfrom cmk.utils.rulesets.ruleset_matcher import RulesetName\nfrom cmk.utils.servicename import Item, ServiceName\n\nfrom cmk.checkengine.discovery import AutocheckEntry, CheckPreviewEntry\nfrom cmk.checkengine.discovery import DiscoveryResult as SingleHostDiscoveryResult\nfrom cmk.checkengine.legacy import LegacyCheckParameters\nfrom cmk.checkengine.parameters import TimespecificParameters\nfrom cmk.checkengine.submitters import ServiceDetails, ServiceState\n\nDiscoveredHostLabelsDict = dict[str, HostLabelValueDict]\n\n\nclass ResultTypeRegistry(Registry[type[\"ABCAutomationResult\"]]):\n    def plugin_name(self, instance: type[ABCAutomationResult]) -> str:\n        return instance.automation_call()\n\n\nresult_type_registry = ResultTypeRegistry()\n\n\nclass SerializedResult(str): ...\n\n\n_DeserializedType = TypeVar(\"_DeserializedType\", bound=\"ABCAutomationResult\")\n\n\n@dataclass\nclass ABCAutomationResult(ABC):\n    def serialize(\n        self,\n        for_cmk_version: cmk_version.Version,  # used to stay compatible with older central sites\n    ) -> SerializedResult:\n        return self._default_serialize()\n\n    @classmethod\n    def deserialize(\n        cls: type[_DeserializedType],\n        serialized_result: SerializedResult,\n    ) -> _DeserializedType:\n        return cls(*literal_eval(serialized_result))\n\n    @staticmethod\n    @abstractmethod\n    def automation_call() -> str: ...\n\n    def _default_serialize(self) -> SerializedResult:\n        return SerializedResult(repr(astuple(self)))\n\n\n@dataclass\nclass ServiceDiscoveryResult(ABCAutomationResult):\n    hosts: Mapping[HostName, SingleHostDiscoveryResult]\n\n    def _to_dict(self) -> Mapping[HostName, Mapping[str, Any]]:\n        return {k: asdict(v) for k, v in self.hosts.items()}\n\n    @staticmethod\n    def _from_dict(\n        serialized: Mapping[HostName, Mapping[str, Any]],\n    ) -> Mapping[HostName, SingleHostDiscoveryResult]:\n        return {k: SingleHostDiscoveryResult(**v) for k, v in serialized.items()}\n\n    def serialize(self, for_cmk_version: cmk_version.Version) -> SerializedResult:\n        return SerializedResult(repr(self._to_dict()))\n\n    @classmethod\n    def deserialize(cls, serialized_result: SerializedResult) -> ServiceDiscoveryResult:\n        return cls(cls._from_dict(literal_eval(serialized_result)))\n\n    @staticmethod\n    def automation_call() -> str:\n        return \"service-discovery\"\n\n\nresult_type_registry.register(ServiceDiscoveryResult)\n\n\n# Should be droped in 2.3\nclass DiscoveryPre22NameResult(ServiceDiscoveryResult):\n    @staticmethod\n    def automation_call() -> str:\n        return \"inventory\"\n\n\nresult_type_registry.register(DiscoveryPre22NameResult)\n\n\n@dataclass\nclass ServiceDiscoveryPreviewResult(ABCAutomationResult):\n    output: str\n    check_table: Sequence[CheckPreviewEntry]\n    nodes_check_table: Mapping[HostName, Sequence[CheckPreviewEntry]]\n    host_labels: DiscoveredHostLabelsDict\n    new_labels: DiscoveredHostLabelsDict\n    vanished_labels: DiscoveredHostLabelsDict\n    changed_labels: DiscoveredHostLabelsDict\n    labels_by_host: Mapping[HostName, Sequence[HostLabel]]\n    source_results: Mapping[str, tuple[int, str]]\n\n    def serialize(self, for_cmk_version: cmk_version.Version) -> SerializedResult:\n        if for_cmk_version < cmk_version.Version.from_str(\"2.4.0b1\"):\n            raw = asdict(self)\n            raw.pop(\"nodes_check_table\")\n            return SerializedResult(\n                repr(\n                    {\n                        **raw,\n                        \"labels_by_host\": {\n                            str(host_name): [label.serialize() for label in labels]\n                            for host_name, labels in self.labels_by_host.items()\n                        },\n                    }\n                )\n            )\n\n        return self._serialize_as_dict()\n\n    def _serialize_as_dict(self) -> SerializedResult:\n        raw = asdict(self)\n        return SerializedResult(\n            repr(\n                {\n                    **raw,\n                    \"labels_by_host\": {\n                        str(host_name): [label.serialize() for label in labels]\n                        for host_name, labels in self.labels_by_host.items()\n                    },\n                }\n            )\n        )\n\n    @classmethod\n    def deserialize(cls, serialized_result: SerializedResult) -> ServiceDiscoveryPreviewResult:\n        raw = literal_eval(serialized_result)\n        return cls(\n            output=raw[\"output\"],\n            check_table=[CheckPreviewEntry(**cpe) for cpe in raw[\"check_table\"]],\n            nodes_check_table={\n                HostName(h): [CheckPreviewEntry(**cpe) for cpe in entries]\n                for h, entries in raw[\"nodes_check_table\"].items()\n            },\n            host_labels=raw[\"host_labels\"],\n            new_labels=raw[\"new_labels\"],\n            vanished_labels=raw[\"vanished_labels\"],\n            changed_labels=raw[\"changed_labels\"],\n            labels_by_host={\n                HostName(raw_host_name): [\n                    HostLabel.deserialize(raw_label) for raw_label in raw_host_labels\n                ]\n                for raw_host_name, raw_host_labels in raw[\"labels_by_host\"].items()\n            },\n            source_results=raw[\"source_results\"],\n        )\n\n    @staticmethod\n    def automation_call() -> str:\n        return \"service-discovery-preview\"\n\n\nresult_type_registry.register(ServiceDiscoveryPreviewResult)\n\n\nclass SpecialAgentDiscoveryPreviewResult(ServiceDiscoveryPreviewResult):\n    @staticmethod\n    def automation_call() -> str:\n        return \"special-agent-discovery-preview\"\n\n\nresult_type_registry.register(SpecialAgentDiscoveryPreviewResult)\n\n\n@dataclass\nclass AutodiscoveryResult(ABCAutomationResult):\n    hosts: Mapping[HostName, SingleHostDiscoveryResult]\n    changes_activated: bool\n\n    def _hosts_to_dict(self) -> Mapping[HostName, Mapping[str, Any]]:\n        return {k: asdict(v) for k, v in self.hosts.items()}\n\n    @staticmethod\n    def _hosts_from_dict(\n        serialized: Mapping[HostName, Mapping[str, Any]],\n    ) -> Mapping[HostName, SingleHostDiscoveryResult]:\n        return {k: SingleHostDiscoveryResult(**v) for k, v in serialized.items()}\n\n    def serialize(self, for_cmk_version: cmk_version.Version) -> SerializedResult:\n        return SerializedResult(repr((self._hosts_to_dict(), self.changes_activated)))\n\n    @classmethod\n    def deserialize(cls, serialized_result: SerializedResult) -> AutodiscoveryResult:\n        hosts, changes_activated = literal_eval(serialized_result)\n        return cls(cls._hosts_from_dict(hosts), changes_activated)\n\n    @staticmethod\n    def automation_call() -> str:\n        return \"autodiscovery\"\n\n\nresult_type_registry.register(AutodiscoveryResult)\n\n\n@dataclass\nclass SetAutochecksV2Result(ABCAutomationResult):\n    @staticmethod\n    def automation_call() -> str:\n        return \"set-autochecks-v2\"\n\n\nresult_type_registry.register(SetAutochecksV2Result)\n\n\n@dataclass\nclass SetAutochecksInput:\n    discovered_host: HostName  # effective host, the one that is being discovered.\n    target_services: Mapping[\n        ServiceName, AutocheckEntry\n    ]  # the table of services we want to see on the discovered host\n    nodes_services: Mapping[\n        HostName, Mapping[ServiceName, AutocheckEntry]\n    ]  # the discovered services on all the nodes\n\n    @classmethod\n    def deserialize(cls, serialized_input: str) -> SetAutochecksInput:\n        raw = json.loads(serialized_input)\n        return cls(\n            discovered_host=HostName(raw[\"discovered_host\"]),\n            target_services={\n                ServiceName(n): AutocheckEntry.load(literal_eval(s))\n                for n, s in raw[\"target_services\"].items()\n            },\n            nodes_services={\n                HostName(k): {\n                    ServiceName(n): AutocheckEntry.load(literal_eval(s)) for n, s in v.items()\n                }\n                for k, v in raw[\"nodes_services\"].items()\n            },\n        )\n\n    def serialize(self) -> str:\n        return json.dumps(\n            {\n                \"discovered_host\": str(self.discovered_host),\n                \"target_services\": {n: repr(s.dump()) for n, s in self.target_services.items()},\n                \"nodes_services\": {\n                    str(k): {n: repr(s.dump()) for n, s in v.items()}\n                    for k, v in self.nodes_services.items()\n                },\n            }\n        )\n\n\n@dataclass\nclass UpdateHostLabelsResult(ABCAutomationResult):\n    @staticmethod\n    def automation_call() -> str:\n        return \"update-host-labels\"\n\n\nresult_type_registry.register(UpdateHostLabelsResult)\n\n\n@dataclass\nclass RenameHostsResult(ABCAutomationResult):\n    action_counts: Mapping[str, int]\n\n    @staticmethod\n    def automation_call() -> str:\n        return \"rename-hosts\"\n\n\nresult_type_registry.register(RenameHostsResult)\n\n\nclass ServiceInfo(TypedDict, total=False):\n    origin: str\n    checkgroup: RulesetName\n    checktype: str\n    item: Item\n    inv_parameters: LegacyCheckParameters\n    factory_settings: ParametersTypeAlias | None\n    parameters: TimespecificParameters | LegacyCheckParameters\n    command_line: str\n\n\n@dataclass\nclass AnalyseServiceResult(ABCAutomationResult):\n    service_info: ServiceInfo\n    labels: Labels\n    label_sources: LabelSources\n\n    def serialize(self, for_cmk_version: cmk_version.Version) -> SerializedResult:\n        if for_cmk_version >= cmk_version.Version.from_str(\"2.2.0i1\"):\n            return self._default_serialize()\n        previous_serialized: Mapping[str, object] = {\n            **self.service_info,\n            \"labels\": self.labels,\n            \"label_sources\": self.label_sources,\n        }\n        return SerializedResult(repr((previous_serialized,)))\n\n    @staticmethod\n    def automation_call() -> str:\n        return \"analyse-service\"\n\n\nresult_type_registry.register(AnalyseServiceResult)\n\n\n@dataclass\nclass GetServicesLabelsResult(ABCAutomationResult):\n    labels: Mapping[ServiceName, Labels]\n\n    @staticmethod\n    def automation_call() -> str:\n        return \"get-services-labels\"\n\n\nresult_type_registry.register(GetServicesLabelsResult)\n\n\n@dataclass\nclass GetServiceNameResult(ABCAutomationResult):\n    service_name: str\n\n    @staticmethod\n    def automation_call() -> str:\n        return \"get-service-name\"\n\n\nresult_type_registry.register(GetServiceNameResult)\n\n\n@dataclass\nclass AnalyseHostResult(ABCAutomationResult):\n    labels: Labels\n    label_sources: LabelSources\n\n    @staticmethod\n    def automation_call() -> str:\n        return \"analyse-host\"\n\n\nresult_type_registry.register(AnalyseHostResult)\n\n\n@dataclass\nclass AnalyzeHostRuleMatchesResult(ABCAutomationResult):\n    results: dict[str, list[object]]\n\n    @staticmethod\n    def automation_call() -> str:\n        return \"analyze-host-rule-matches\"\n\n\nresult_type_registry.register(AnalyzeHostRuleMatchesResult)\n\n\n@dataclass\nclass AnalyzeServiceRuleMatchesResult(ABCAutomationResult):\n    results: dict[str, list[object]]\n\n    @staticmethod\n    def automation_call() -> str:\n        return \"analyze-service-rule-matches\"\n\n\nresult_type_registry.register(AnalyzeServiceRuleMatchesResult)\n\n\n@dataclass\nclass DeleteHostsResult(ABCAutomationResult):\n    @staticmethod\n    def automation_call() -> str:\n        return \"delete-hosts\"\n\n\nresult_type_registry.register(DeleteHostsResult)\n\n\n@dataclass\nclass DeleteHostsKnownRemoteResult(ABCAutomationResult):\n    @staticmethod\n    def automation_call() -> str:\n        return \"delete-hosts-known-remote\"\n\n\nresult_type_registry.register(DeleteHostsKnownRemoteResult)\n\n\n@dataclass\nclass RestartResult(ABCAutomationResult):\n    config_warnings: ConfigurationWarnings\n\n    @staticmethod\n    def automation_call() -> str:\n        return \"restart\"\n\n\nresult_type_registry.register(RestartResult)\n\n\n@dataclass\nclass ReloadResult(RestartResult):\n    @staticmethod\n    def automation_call() -> str:\n        return \"reload\"\n\n\nresult_type_registry.register(ReloadResult)\n\n\n@dataclass\nclass GetConfigurationResult(ABCAutomationResult):\n    result: Mapping[str, Any]\n\n    @staticmethod\n    def automation_call() -> str:\n        return \"get-configuration\"\n\n\nresult_type_registry.register(GetConfigurationResult)\n\n\n@dataclass\nclass GetCheckInformationResult(ABCAutomationResult):\n    plugin_infos: Mapping[str, Mapping[str, Any]]\n\n    @staticmethod\n    def automation_call() -> str:\n        return \"get-check-information\"\n\n\nresult_type_registry.register(GetCheckInformationResult)\n\n\n@dataclass\nclass GetSectionInformationResult(ABCAutomationResult):\n    section_infos: Mapping[str, Mapping[str, str]]\n\n    @staticmethod\n    def automation_call() -> str:\n        return \"get-section-information\"\n\n\nresult_type_registry.register(GetSectionInformationResult)\n\n\n@dataclass(frozen=True)\nclass Gateway:\n    existing_gw_host_name: HostName | None\n    ip: HostAddress\n    dns_name: HostName | None\n\n\n@dataclass(frozen=True)\nclass GatewayResult:\n    gateway: Gateway | None\n    state: str\n    ping_fails: int\n    message: str\n\n\n@dataclass\nclass ScanParentsResult(ABCAutomationResult):\n    results: Sequence[GatewayResult]\n\n    @staticmethod\n    def automation_call() -> str:\n        return \"scan-parents\"\n\n    @classmethod\n    def deserialize(cls, serialized_result: SerializedResult) -> ScanParentsResult:\n        (serialized_results,) = literal_eval(serialized_result)\n        results = [\n            GatewayResult(\n                gateway=Gateway(*gw) if gw else None,\n                state=state,\n                ping_fails=ping_fails,\n                message=message,\n            )\n            for gw, state, ping_fails, message in serialized_results\n        ]\n        return cls(results=results)\n\n\nresult_type_registry.register(ScanParentsResult)\n\n\n@dataclass\nclass DiagSpecialAgentHostConfig:\n    host_name: HostName\n    host_alias: str\n    ip_address: HostAddress | None = None\n    ip_stack_config: IPStackConfig = IPStackConfig.NO_IP\n    host_attrs: Mapping[str, str] = field(default_factory=dict)\n    macros: Mapping[str, object] = field(default_factory=dict)\n    host_primary_family: Literal[\n        socket.AddressFamily.AF_INET,\n        socket.AddressFamily.AF_INET6,\n    ] = socket.AddressFamily.AF_INET\n    host_additional_addresses_ipv4: list[HostAddress] = field(default_factory=list)\n    host_additional_addresses_ipv6: list[HostAddress] = field(default_factory=list)\n\n    @classmethod\n    def deserialize(cls, serialized_input: str) -> DiagSpecialAgentHostConfig:\n        raw = json.loads(serialized_input)\n        deserialized = {\n            \"host_name\": HostName(raw[\"host_name\"]),\n            \"host_alias\": raw[\"host_alias\"],\n        }\n        if \"ip_address\" in raw:\n            deserialized[\"ip_address\"] = (\n                HostAddress(raw[\"ip_address\"]) if raw[\"ip_address\"] else None\n            )\n        if \"ip_stack_config\" in raw:\n            deserialized[\"ip_stack_config\"] = IPStackConfig(raw[\"ip_stack_config\"])\n        if \"host_attrs\" in raw:\n            deserialized[\"host_attrs\"] = raw[\"host_attrs\"]\n        if \"macros\" in raw:\n            deserialized[\"macros\"] = raw[\"macros\"]\n        if \"host_primary_family\" in raw:\n            deserialized[\"host_primary_family\"] = cls.deserialize_host_primary_family(\n                raw[\"host_primary_family\"]\n            )\n        if \"host_additional_addresses_ipv4\" in raw:\n            deserialized[\"host_additional_addresses_ipv4\"] = [\n                HostAddress(ip) for ip in raw[\"host_additional_addresses_ipv4\"]\n            ]\n        if \"host_additional_addresses_ipv6\" in raw:\n            deserialized[\"host_additional_addresses_ipv6\"] = [\n                HostAddress(ip) for ip in raw[\"host_additional_addresses_ipv6\"]\n            ]\n        return cls(**deserialized)\n\n    @staticmethod\n    def deserialize_host_primary_family(\n        raw: int,\n    ) -> Literal[\n        socket.AddressFamily.AF_INET,\n        socket.AddressFamily.AF_INET6,\n    ]:\n        address_family = socket.AddressFamily(raw)\n        if address_family is socket.AddressFamily.AF_INET:\n            return socket.AddressFamily.AF_INET\n        if address_family is socket.AddressFamily.AF_INET6:\n            return socket.AddressFamily.AF_INET6\n        raise ValueError(f\"Invalid address family: {address_family}\")\n\n    def serialize(self, _for_cmk_version: cmk_version.Version) -> str:\n        return json.dumps(\n            {\n                \"host_name\": str(self.host_name),\n                \"host_alias\": self.host_alias,\n                \"ip_address\": str(self.ip_address) if self.ip_address else None,\n                \"ip_stack_config\": self.ip_stack_config.value,\n                \"host_attrs\": self.host_attrs,\n                \"macros\": self.macros,\n                \"host_primary_family\": self.host_primary_family.value,\n                \"host_additional_addresses_ipv4\": [\n                    str(ip) for ip in self.host_additional_addresses_ipv4\n                ],\n                \"host_additional_addresses_ipv6\": [\n                    str(ip) for ip in self.host_additional_addresses_ipv6\n                ],\n            }\n        )\n\n\n@dataclass\nclass DiagSpecialAgentInput:\n    host_config: DiagSpecialAgentHostConfig\n    agent_name: str\n    params: Mapping[str, object]\n    passwords: Mapping[str, str]\n    http_proxies: Mapping[str, Mapping[str, str]] = field(default_factory=dict)\n    is_cmc: bool = True\n\n    @classmethod\n    def deserialize(cls, serialized_input: str) -> DiagSpecialAgentInput:\n        raw = json.loads(serialized_input)\n        deserialized = {\n            \"host_config\": DiagSpecialAgentHostConfig.deserialize(raw[\"host_config\"]),\n            \"agent_name\": raw[\"agent_name\"],\n            # TODO: at the moment there is no validation for params input possible\n            #  this could change when being able to use the formspec vue visitor for\n            #  (de)serialization in the future.\n            \"params\": literal_eval(raw[\"params\"]),\n            \"passwords\": raw[\"passwords\"],\n        }\n        if \"http_proxies\" in raw:\n            deserialized[\"http_proxies\"] = raw[\"http_proxies\"]\n        if \"is_cmc\" in raw:\n            deserialized[\"is_cmc\"] = raw[\"is_cmc\"]\n        return cls(**deserialized)\n\n    def serialize(self, _for_cmk_version: cmk_version.Version) -> str:\n        return json.dumps(\n            {\n                \"host_config\": self.host_config.serialize(_for_cmk_version),\n                \"agent_name\": self.agent_name,\n                \"params\": repr(self.params),\n                \"passwords\": self.passwords,\n                \"http_proxies\": self.http_proxies,\n                \"is_cmc\": self.is_cmc,\n            }\n        )\n\n\n@dataclass\nclass SpecialAgentResult:\n    return_code: int\n    response: str\n\n\n@dataclass\nclass DiagSpecialAgentResult(ABCAutomationResult):\n    results: Sequence[SpecialAgentResult]\n\n    @staticmethod\n    def automation_call() -> str:\n        return \"diag-special-agent\"\n\n    def serialize(self, for_cmk_version: cmk_version.Version) -> SerializedResult:\n        return SerializedResult(\n            json.dumps(\n                {\n                    \"results\": [asdict(r) for r in self.results],\n                }\n            )\n        )\n\n    @classmethod\n    def deserialize(cls, serialized_result: SerializedResult) -> DiagSpecialAgentResult:\n        raw = json.loads(serialized_result)\n        return cls(\n            results=[SpecialAgentResult(**r) for r in raw[\"results\"]],\n        )\n\n\nresult_type_registry.register(DiagSpecialAgentResult)\n\n\n@dataclass\nclass DiagHostResult(ABCAutomationResult):\n    return_code: int\n    response: str\n\n    @staticmethod\n    def automation_call() -> str:\n        return \"diag-host\"\n\n\nresult_type_registry.register(DiagHostResult)\n\n\n@dataclass\nclass ActiveCheckResult(ABCAutomationResult):\n    state: ServiceState | None\n    output: ServiceDetails\n\n    @staticmethod\n    def automation_call() -> str:\n        return \"active-check\"\n\n\nresult_type_registry.register(ActiveCheckResult)\n\n\n@dataclass\nclass UpdateDNSCacheResult(ABCAutomationResult):\n    n_updated: int\n    failed_hosts: Sequence[HostName]\n\n    @staticmethod\n    def automation_call() -> str:\n        return \"update-dns-cache\"\n\n\nresult_type_registry.register(UpdateDNSCacheResult)\n\n\n@dataclass\nclass UpdatePasswordsMergedFileResult(ABCAutomationResult):\n    @staticmethod\n    def automation_call() -> str:\n        return \"update-passwords-merged-file\"\n\n\nresult_type_registry.register(UpdatePasswordsMergedFileResult)\n\n\n@dataclass\nclass GetAgentOutputResult(ABCAutomationResult):\n    success: bool\n    service_details: ServiceDetails\n    raw_agent_data: AgentRawData\n\n    @staticmethod\n    def automation_call() -> str:\n        return \"get-agent-output\"\n\n\nresult_type_registry.register(GetAgentOutputResult)\n\n\n@dataclass\nclass NotificationReplayResult(ABCAutomationResult):\n    @staticmethod\n    def automation_call() -> str:\n        return \"notification-replay\"\n\n\nresult_type_registry.register(NotificationReplayResult)\n\n\n@dataclass\nclass NotificationAnalyseResult(ABCAutomationResult):\n    result: NotifyAnalysisInfo | None\n\n    @staticmethod\n    def automation_call() -> str:\n        return \"notification-analyse\"\n\n\nresult_type_registry.register(NotificationAnalyseResult)\n\n\n@dataclass\nclass NotificationTestResult(ABCAutomationResult):\n    result: NotifyAnalysisInfo | None\n\n    @staticmethod\n    def automation_call() -> str:\n        return \"notification-test\"\n\n\nresult_type_registry.register(NotificationTestResult)\n\n\n@dataclass\nclass NotificationGetBulksResult(ABCAutomationResult):\n    result: NotifyBulks\n\n    @staticmethod\n    def automation_call() -> str:\n        return \"notification-get-bulks\"\n\n\nresult_type_registry.register(NotificationGetBulksResult)\n\n\n@dataclass\nclass CreateDiagnosticsDumpResult(ABCAutomationResult):\n    output: str\n    tarfile_path: str\n    tarfile_created: bool\n\n    @staticmethod\n    def automation_call() -> str:\n        return \"create-diagnostics-dump\"\n\n\nresult_type_registry.register(CreateDiagnosticsDumpResult)\n\n\n@dataclass\nclass BakeAgentsResult(ABCAutomationResult):\n    output: str | None\n\n    @staticmethod\n    def automation_call() -> str:\n        return \"bake-agents\"\n\n\nresult_type_registry.register(BakeAgentsResult)\n"}
{"type": "source_file", "path": "cmk/active_checks/check_sftp.py", "content": "#!/usr/bin/env python3\n# Copyright (C) 2019 Checkmk GmbH - License: GNU General Public License v2\n# This file is part of Checkmk (https://checkmk.com). It is subject to the terms and\n# conditions defined in the file COPYING, which is part of this source code package.\n\"\"\"check_sftp - Monitor SFTP servers\"\"\"\n\nimport argparse\nimport os\nimport sys\nimport time\nfrom collections.abc import Sequence\nfrom pathlib import Path\nfrom typing import NamedTuple\n\nimport paramiko\nfrom pydantic import BaseModel\n\nfrom cmk.utils import password_store\nfrom cmk.utils.ssh_client import get_ssh_client\n\n_LOCAL_DIR = \"var/check_mk/active_checks/check_sftp\"\n\n\nclass SecurityError(ValueError):\n    \"\"\"Raised when a security issue is detected, such as attempted path traversal.\"\"\"\n\n\nclass Args(BaseModel):\n    host: str\n    user: None | str\n    secret: None | str\n    secret_reference: None | str\n    port: int\n    get_remote: None | str\n    get_local: None | str\n    put_local: None | str\n    put_remote: None | str\n    get_timestamp: None | str\n    timeout: float\n    debug: bool\n    look_for_keys: bool\n\n    def resolve_secret(self) -> None | str:\n        if self.secret is not None:\n            return self.secret\n        if self.secret_reference is not None:\n            secret_id, file = self.secret_reference.split(\":\", 1)\n            return password_store.lookup(Path(file), secret_id)\n        return None\n\n\ndef parse_arguments(sys_args: Sequence[str]) -> Args:\n    parser = argparse.ArgumentParser(prog=__doc__)\n\n    parser.add_argument(\"--host\", default=\"localhost\", help=\"SFTP server address\")\n    parser.add_argument(\"--user\", default=None, help=\"Username for sftp login\")\n\n    group = parser.add_mutually_exclusive_group()\n    group.add_argument(\"--secret\", default=None, help=\"Secret for sftp login\")\n    group.add_argument(\n        \"--secret-reference\",\n        default=None,\n        help=\"Password store reference to a secret for sftp login\",\n    )\n\n    parser.add_argument(\"--port\", type=int, default=22, help=\"Alternative port number\")\n    parser.add_argument(\n        \"--get-remote\",\n        metavar=\"FILE\",\n        default=None,\n        help=\"Path on the remote to the file that should be pulled from server (relative to the remote home)\",\n    )\n    parser.add_argument(\n        \"--get-local\",\n        metavar=\"DIRECTORY\",\n        default=None,\n        help=f\"Path where the pulled file should be stored (relative to '{_LOCAL_DIR}' in the site's directory)\",\n    )\n    parser.add_argument(\n        \"--put-local\",\n        metavar=\"FILE\",\n        default=None,\n        help=f\"Path to the file to push to server (relative to '{_LOCAL_DIR}' in the site's directory). If the file does not exist, it will be created with a test message.\",\n    )\n    parser.add_argument(\n        \"--put-remote\",\n        metavar=\"DIRECTORY\",\n        default=None,\n        help=\"Path on the remote where the pushed file should be stored (relative to the remote home)\",\n    )\n    parser.add_argument(\n        \"--get-timestamp\",\n        metavar=\"PATH\",\n        default=None,\n        help=\"Path to the file on the remote server for which the timestamp should be checked\",\n    )\n    parser.add_argument(\"--timeout\", type=float, default=10.0, help=\"Set timeout for connection\")\n    parser.add_argument(\n        \"--look-for-keys\",\n        action=\"store_true\",\n        help=\"Search for discoverable keys in the user's '~/.ssh' directory\",\n    )\n    parser.add_argument(\"--debug\", action=\"store_true\", help=\"Raise python exceptions.\")\n\n    return Args.model_validate(vars(parser.parse_args(sys_args)))\n\n\ndef output_check_result(s: str) -> None:\n    sys.stdout.write(\"%s\\n\" % s)\n\n\nclass CheckSftp:\n    @property\n    def local_directory(self) -> str:\n        return f\"{self.omd_root}/{_LOCAL_DIR}\"\n\n    def is_in_local_directory(self, path: str) -> bool:\n        return os.path.normpath(path).startswith(self.local_directory)\n\n    class TransferOptions(NamedTuple):\n        local: str\n        remote: str\n\n    def __init__(self, client: paramiko.SSHClient, omd_root: str, args: Args):\n        self.omd_root = omd_root\n        self.host: str = args.host\n        self.user: str | None = args.user\n        self.pass_: str | None = args.resolve_secret()\n        self.port: int = args.port\n        self.timeout: float = args.timeout\n        self.look_for_keys: bool = args.look_for_keys\n        self.debug: bool = args.debug\n\n        self.connection = self.connect(client)\n\n        remote_workdir = self.connection.getcwd()\n        assert remote_workdir is not None  # help mypy -- we just set it above, see getcwd() docs\n\n        self.upload_options: None | CheckSftp.TransferOptions = (\n            None\n            if args.put_local is None\n            else self.process_put_options(\n                self.local_directory, args.put_local, remote_workdir, args.put_remote\n            )\n        )\n\n        self.download_options: None | CheckSftp.TransferOptions = (\n            None\n            if args.get_remote is None\n            else self.process_get_options(\n                remote_workdir, args.get_remote, self.local_directory, args.get_local\n            )\n        )\n\n        self.timestamp_path: None | str = (\n            os.path.normpath(f\"{remote_workdir}/{args.get_timestamp}\")\n            if args.get_timestamp\n            else None\n        )\n\n    @staticmethod\n    def resolve_transfer_paths(src_file: str, dst_dir: str) -> tuple[str, str]:\n        \"\"\"Normalize the paths and append the src file name to the destination directory.\"\"\"\n        return (\n            os.path.normpath(src_file),\n            os.path.normpath(f\"{dst_dir}/{src_file.split('/')[-1]}\"),\n        )\n\n    def process_put_options(\n        self, local_base: str, local_file: str, remote_base: str, remote_dir: str | None\n    ) -> None | TransferOptions:\n        local, remote = self.resolve_transfer_paths(\n            f\"{local_base}/{local_file}\", f\"{remote_base}/{remote_dir or '.'}\"\n        )\n\n        if not self.is_in_local_directory(local):\n            raise SecurityError(\"Invalid local path for put operation\")\n\n        return CheckSftp.TransferOptions(local, remote)\n\n    def process_get_options(\n        self, remote_base: str, remote_file: str, local_base: str, local_dir: str | None\n    ) -> None | TransferOptions:\n        remote, local = self.resolve_transfer_paths(\n            f\"{remote_base}/{remote_file}\", f\"{local_base}/{local_dir or '.'}\"\n        )\n\n        if not self.is_in_local_directory(local):\n            raise SecurityError(\"Invalid local path for get operation\")\n\n        return CheckSftp.TransferOptions(local, remote)\n\n    def connect(self, client: paramiko.SSHClient) -> paramiko.sftp_client.SFTPClient:\n        client.set_missing_host_key_policy(paramiko.AutoAddPolicy())  # nosec B507\n        client.connect(\n            hostname=self.host,\n            username=self.user,\n            password=self.pass_,\n            port=self.port,\n            timeout=self.timeout,\n            look_for_keys=self.look_for_keys,\n        )\n        sftp = client.open_sftp()\n        sftp.chdir(\".\")  # make sure we have a working directory, see sftp.getcwd() docs\n        return sftp\n\n    @staticmethod\n    def remote_has_file(sftp: paramiko.sftp_client.SFTPClient, path: str) -> bool:\n        try:\n            sftp.stat(path)\n            return True\n        except FileNotFoundError:\n            return False\n\n    @staticmethod\n    def get_timestamp(sftp: paramiko.sftp_client.SFTPClient, path: str) -> float | None:\n        return sftp.stat(path).st_mtime\n\n    def check_file_upload(self) -> None:\n        assert self.upload_options is not None\n        local_file = self.upload_options.local\n\n        if not os.path.isfile(local_file):\n            os.makedirs(os.path.dirname(local_file), exist_ok=True)\n            with open(local_file, \"w\") as f:\n                f.write(\"This is a test by Check_MK\\n\")\n\n        self.connection.put(local_file, self.upload_options.remote)\n\n    def check_file_download(self) -> None:\n        assert self.download_options is not None\n        os.makedirs(os.path.dirname(self.download_options.local), exist_ok=True)\n        self.connection.get(self.download_options.remote, self.download_options.local)\n\n    def run_optional_checks(self) -> tuple[int, list[str]]:\n        status, messages = 0, []\n\n        # Remove the uploaded file if it didn't exist. But only at the very end, since\n        # we might need it for the download check.\n        new_file_on_remote: None | str = None\n\n        if self.upload_options is not None:\n            try:\n                if not self.remote_has_file(self.connection, self.upload_options.remote):\n                    new_file_on_remote = self.upload_options.remote\n\n                self.check_file_upload()\n                messages.append(\"Successfully put file to SFTP server\")\n            except Exception:\n                if self.debug:\n                    raise\n                status = max(status, 2)\n                messages.append(\"Could not put file to SFTP server! (!!)\")\n\n        if self.download_options is not None:\n            try:\n                self.check_file_download()\n                messages.append(\"Successfully got file from SFTP server\")\n            except Exception:\n                if self.debug:\n                    raise\n                status = max(status, 2)\n                messages.append(\"Could not get file from SFTP server! (!!)\")\n\n        if self.timestamp_path is not None:\n            try:\n                timestamp = self.get_timestamp(self.connection, self.timestamp_path)\n                messages.append(\n                    \"Timestamp of {} is: {}\".format(\n                        self.timestamp_path.split(\"/\")[-1], time.ctime(timestamp)\n                    )\n                )\n            except Exception:\n                if self.debug:\n                    raise\n                status = max(status, 2)\n                messages.append(\"Could not get timestamp of file! (!!)\")\n\n        if new_file_on_remote is not None:\n            self.connection.remove(new_file_on_remote)\n\n        return status, messages\n\n\ndef main() -> int:\n    if (omd_root := os.getenv(\"OMD_ROOT\")) is None:\n        sys.stderr.write(\"This check must be executed from within a site\\n\")\n        sys.exit(1)\n\n    args = parse_arguments(sys.argv[1:])\n\n    try:\n        check = CheckSftp(get_ssh_client(), omd_root, args)\n    except SecurityError as e:\n        if args.debug:\n            raise\n        output_check_result(f\"Security issue detected: {e}\")\n        return 2\n    except paramiko.ssh_exception.BadHostKeyException as e:\n        if args.debug:\n            raise\n        output_check_result(str(e))\n        return 2\n    except Exception:\n        if args.debug:\n            raise\n        output_check_result(\"Connection failed!\")\n        return 2\n\n    login_info = [\"Login successful\"]\n    status, checks_info = check.run_optional_checks()\n    output_check_result(\", \".join(login_info + checks_info))\n    return status\n"}
{"type": "source_file", "path": "buildscripts/scripts/validate_changes.py", "content": "#!/usr/bin/env python3\n# Copyright (C) 2019 Checkmk GmbH - License: GNU General Public License v2\n# This file is part of Checkmk (https://checkmk.com). It is subject to the terms and\n# conditions defined in the file COPYING, which is part of this source code package.\n\n\"\"\"What would Gerrit do?\n\nThis script evaluates a YAML file containing dynamic information about Jenkins pipeline stages\nand environmental conditions (e.g. changed files since last commit) and generates a static list\nof stages (i.e. without any variables left to evaluate).\nThis list can either be executed directly or returned (JSON encoded on stdout or in file) in order\nto be read and handled later by a Jenkins pipelined job.\n\"\"\"\n\nimport argparse\nimport asyncio\nimport json\nimport logging\nimport os\nimport re\nimport subprocess\nimport sys\nimport time\nfrom collections.abc import Mapping, Sequence\nfrom pathlib import Path\nfrom typing import Any, TypedDict\n\nimport yaml\n\nLOG = logging.getLogger(\"validate_changes\")\n\nVars = Mapping[str, str]  # Just a shortcut for generic str -> str mapping\n\n\nclass StageInfo(TypedDict, total=False):\n    \"\"\"May contain a raw or finalized info set for a Jenkins pipeline stage\"\"\"\n\n    NAME: str\n    ONLY_WHEN_NOT_EMPTY: str\n    DIR: str\n    ENV_VARS: Vars\n    ENV_VAR_LIST: Sequence[str]\n    SEC_VAR_LIST: Sequence[str]\n    JENKINS_API_ACCESS: bool\n    BAZEL_LOCKS_AMOUNT: int\n    COMMAND: str\n    TEXT_ON_SKIP: str\n    SKIPPED: str\n    RESULT_CHECK_TYPE: str\n    RESULT_CHECK_FILE_PATTERN: str\n\n\nStages = Sequence[StageInfo]\n\n\ndef parse_args() -> argparse.Namespace:\n    \"\"\"Parse command line arguments and return argument object\"\"\"\n    parser = argparse.ArgumentParser(description=__doc__)\n    parser.add_argument(\n        \"--verbose\",\n        \"-v\",\n        action=\"count\",\n        default=0,\n        help=\"Be verbose (can be applied multiple times)\",\n    )\n    parser.add_argument(\n        \"--env\",\n        \"-e\",\n        type=str,\n        default=[],\n        action=\"append\",\n        help=\"Set a variable to be used to expand commands\",\n    )\n    parser.add_argument(\n        \"--write-file\",\n        \"-w\",\n        type=str,\n        help=\"Don't execute the stages but write a JSON encoded list of stages to\"\n        \" given file ('-' for stdout\",\n    )\n    parser.add_argument(\n        \"--no-skip\",\n        action=\"store_true\",\n        help=\"Ignore conditions for skipping stages (activate all)\",\n    )\n    parser.add_argument(\n        \"--exitfirst\",\n        \"-x\",\n        action=\"store_true\",\n        help=\"Exit on first failing stage command\",\n    )\n    parser.add_argument(\n        \"--filter-substring\",\n        \"-k\",\n        type=str,\n        default=[],\n        action=\"append\",\n        help=\"Filter for substring in stage name\",\n    )\n    parser.add_argument(\n        \"input\",\n        type=Path,\n        help=\"A YAML encoded file containing information about stages to generate\",\n        default=Path(os.path.dirname(__file__)) / \"stages.yml\",\n        nargs=\"?\",\n    )\n    return parser.parse_args()\n\n\ndef to_stage_info(raw_stage: Mapping[Any, Any]) -> StageInfo:\n    \"\"\"Return StageInfo created from validated @raw_stage\"\"\"\n    return StageInfo(\n        NAME=str(raw_stage[\"NAME\"]),\n        ONLY_WHEN_NOT_EMPTY=str(raw_stage.get(\"ONLY_WHEN_NOT_EMPTY\", \"\")),\n        DIR=str(raw_stage.get(\"DIR\", \"\")),\n        ENV_VARS={str(k): str(v) for k, v in raw_stage.get(\"ENV_VARS\", {}).items()},\n        SEC_VAR_LIST=list(raw_stage.get(\"SEC_VAR_LIST\", [])),\n        JENKINS_API_ACCESS=bool(raw_stage.get(\"JENKINS_API_ACCESS\", False)),\n        BAZEL_LOCKS_AMOUNT=int(raw_stage.get(\"BAZEL_LOCKS_AMOUNT\", -1)),\n        COMMAND=str(raw_stage[\"COMMAND\"]),\n        TEXT_ON_SKIP=str(raw_stage.get(\"TEXT_ON_SKIP\", \"\")),\n        RESULT_CHECK_TYPE=str(raw_stage.get(\"RESULT_CHECK_TYPE\", \"\")),\n        RESULT_CHECK_FILE_PATTERN=str(raw_stage.get(\"RESULT_CHECK_FILE_PATTERN\", \"\")),\n    )\n\n\ndef load_file(filename: Path) -> tuple[Sequence[Vars], Stages]:\n    \"\"\"Read and parse a YAML file containing 'VARIABLES' and 'STAGES' and return a tuple with\n    typed content\"\"\"\n    try:\n        raw_data = yaml.safe_load(Path.read_text(filename))\n    except FileNotFoundError:\n        raise RuntimeError(\n            f\"Could not find {filename}. Must be a YAML file containing stage declarations.\"\n        )\n\n    return (\n        [{str(k): str(v) for k, v in e.items()} for e in raw_data[\"VARIABLES\"]],\n        list(map(to_stage_info, raw_data[\"STAGES\"])),\n    )\n\n\ndef replace_variables(string: str, env_vars: Vars) -> str:\n    \"\"\"Replace all occurrences of '${VAR_NAME}' in @string based on @env_vars\n    >>> replace_variables(\"foo: ${foo}\", {\"foo\": \"bar\"})\n    'foo: bar'\n    \"\"\"\n\n    def replace_match(match: re.Match) -> str:\n        var_name = match.group(1)\n        return env_vars.get(var_name, match.group(0))\n\n    return re.sub(r\"\\$\\{(\\w+)\\}\", replace_match, string)\n\n\ndef apply_variables(in_data: StageInfo, env_vars: Vars) -> StageInfo:\n    \"\"\"Apply variables to a stage info set. Make sure the ENV_VARS sub-mapping gets handled.\"\"\"\n    return StageInfo(\n        NAME=replace_variables(in_data[\"NAME\"], env_vars),\n        ONLY_WHEN_NOT_EMPTY=replace_variables(in_data[\"ONLY_WHEN_NOT_EMPTY\"], env_vars),\n        DIR=replace_variables(in_data[\"DIR\"], env_vars),\n        ENV_VARS={k: replace_variables(v, env_vars) for k, v in in_data[\"ENV_VARS\"].items()},\n        SEC_VAR_LIST=list(in_data[\"SEC_VAR_LIST\"]),\n        JENKINS_API_ACCESS=in_data.get(\"JENKINS_API_ACCESS\", False),\n        BAZEL_LOCKS_AMOUNT=int(replace_variables(str(in_data[\"BAZEL_LOCKS_AMOUNT\"]), env_vars)),\n        COMMAND=replace_variables(in_data[\"COMMAND\"], env_vars),\n        TEXT_ON_SKIP=replace_variables(in_data[\"TEXT_ON_SKIP\"], env_vars),\n        RESULT_CHECK_TYPE=replace_variables(in_data[\"RESULT_CHECK_TYPE\"], env_vars),\n        RESULT_CHECK_FILE_PATTERN=replace_variables(in_data[\"RESULT_CHECK_FILE_PATTERN\"], env_vars),\n    )\n\n\ndef finalize_stage(stage: StageInfo, env_vars: Vars, no_skip: bool) -> StageInfo:\n    \"\"\"Return an updated list of stages with conditions applied and values reformatted\"\"\"\n    condition_vars = stage.get(\"ONLY_WHEN_NOT_EMPTY\")\n    skip_stage = condition_vars and not any(env_vars[v.strip()] for v in condition_vars.split(\",\"))\n    result = (\n        StageInfo(\n            NAME=stage[\"NAME\"],\n            DIR=stage.get(\"DIR\", \"\"),\n            ENV_VAR_LIST=[f\"{k}={v}\" for k, v in stage.get(\"ENV_VARS\", {}).items()],\n            SEC_VAR_LIST=list(stage.get(\"SEC_VAR_LIST\", [])),\n            JENKINS_API_ACCESS=stage.get(\"JENKINS_API_ACCESS\", False),\n            BAZEL_LOCKS_AMOUNT=int(stage.get(\"BAZEL_LOCKS_AMOUNT\", -1)),\n            COMMAND=stage[\"COMMAND\"],\n            RESULT_CHECK_TYPE=stage[\"RESULT_CHECK_TYPE\"],\n            RESULT_CHECK_FILE_PATTERN=stage[\"RESULT_CHECK_FILE_PATTERN\"],\n        )\n        if no_skip or not skip_stage\n        else StageInfo(  #\n            NAME=stage[\"NAME\"],\n            SKIPPED=(\n                f\"Reason: {stage.get('TEXT_ON_SKIP') or 'not provided'},\"\n                f\" Condition: {condition_vars}\"\n            ),\n        )\n    )\n\n    for key, value in result.items():\n        if \"${\" in str(value):\n            raise RuntimeError(\n                f\"There unexpanded variables left in stage {stage['NAME']}: {key}={value}.\"\n                \" Did you forget to provide them with --env?\"\n            )\n\n    return result\n\n\ndef run_shell_command(cmd: str, replace_newlines: bool) -> str:\n    \"\"\"Run a command and return preprocessed stdout\"\"\"\n    with subprocess.Popen(\n        [\"sh\"],\n        text=True,\n        stdin=subprocess.PIPE,\n        stdout=subprocess.PIPE,\n    ) as proc_sh:\n        stdout_str, _ = proc_sh.communicate(cmd)\n    stdout_str = stdout_str.strip()\n    return stdout_str.replace(\"\\n\", \" \") if replace_newlines else stdout_str\n\n\ndef evaluate_vars(raw_vars: Sequence[Vars], env_vars: Vars) -> Vars:\n    \"\"\"Evaluate receipts for variables. Make sure already evaluated variables can be used in\n    later steps.\n    >>> evaluate_vars([\n    ...    {\"NAME\": \"FOO\", \"SH\": \"echo foo ${FIRST}\"},\n    ...    {\"NAME\": \"BAR\", \"SH\": \"echo bar ${FOO}\"},\n    ...    {\"NAME\": \"VOLATILE\", \"SH\": \"echo default\"},\n    ...  ],\n    ...  {\n    ...    \"FIRST\": \"first\",\n    ...    \"SECOND\": \"second\",\n    ...    \"VOLATILE\": \"overwritten\",\n    ... })\n    {'FIRST': 'first', 'SECOND': 'second', 'VOLATILE': 'overwritten', 'FOO': 'foo first', 'BAR': 'bar foo first'}\n    \"\"\"\n    result: dict[str, str] = dict(env_vars)\n    for e in raw_vars:\n        if e[\"NAME\"] in result:\n            LOG.info(\"Trying to set existing variable %r\", e[\"NAME\"])\n            continue\n\n        cmd = replace_variables(e[\"SH\"], result)\n        if \"${\" in cmd:\n            raise RuntimeError(\n                f\"There are still unexpanded variables in command: {cmd!r}.\"\n                \" Did you forget to provide them with --env?\"\n            )\n\n        LOG.debug(\"evaluate %r run command %r\", e[\"NAME\"], cmd)\n        replace_newlines = convert_newline_entry_to_bool(e.get(\"REPLACE_NEWLINES\", False))\n        cmd_result = run_shell_command(cmd, replace_newlines)\n        LOG.debug(\"set to %r\", cmd_result)\n        result[e[\"NAME\"]] = cmd_result\n\n    return result\n\n\ndef convert_newline_entry_to_bool(entry: str | bool) -> bool:\n    if isinstance(entry, str):\n        return entry.lower() in (\n            \"y\",\n            \"yes\",\n            \"t\",\n            \"true\",\n            \"on\",\n            \"1\",\n        )\n\n    return bool(entry)\n\n\ndef compile_stage_info(stages_file: Path, env_vars: Vars, no_skip: bool) -> tuple[Vars, Stages]:\n    \"\"\"Return a list of stages loaded from provided YAML file with variables applied\"\"\"\n    raw_vars, raw_stages = load_file(stages_file)\n    finalized_vars = evaluate_vars(raw_vars, env_vars)\n    return (\n        finalized_vars,\n        [\n            finalize_stage(\n                apply_variables(stage, finalized_vars),\n                finalized_vars,\n                no_skip,\n            )\n            for stage in raw_stages\n        ],\n    )\n\n\nasync def run_cmd(\n    cmd: str,\n    env: Vars,\n    cwd: str | None,\n    stdout_prefix: str = \"\",\n    stderr_prefix: str = \"\",\n    output: list[str] | None = None,\n) -> bool:\n    \"\"\"Run a command while continuously capturing its stdout/stdin and printing it out in a\n    predefined way for either stdout or stderr\"\"\"\n\n    async def process_lines(\n        stream: asyncio.StreamReader,\n        prefix: str = \"\",\n        output: list[str] | None = None,\n    ) -> None:\n        async for line in stream:\n            l = line.decode()\n            if l.strip():\n                msg = f\"{prefix}{l}\"\n                if output is not None:\n                    output.append(msg)\n                else:\n                    print(msg)\n\n    process = await asyncio.create_subprocess_exec(\n        # Use `bash` rather than `sh` in order to provide things like `&>`\n        \"bash\",\n        \"-c\",\n        cmd,\n        cwd=cwd,\n        limit=1024 * 512,  # see https://stackoverflow.com/questions/55457370\n        stdout=subprocess.PIPE,\n        stderr=subprocess.PIPE,\n        # This is to make Python scripts behave, i.e. not buffer stdout.\n        # this works only for Python of course but a general solution would be nice of course.\n        # If someone knows a better way to deactivate buffering, drop me a line please.\n        env={**os.environ, **{\"PYTHONUNBUFFERED\": \"1\"}, **env},\n    )\n\n    assert process.stdout and process.stderr\n    await asyncio.gather(\n        process_lines(process.stdout, stdout_prefix, output),\n        process_lines(process.stderr, stderr_prefix, output),\n    )\n    await process.wait()\n\n    return process.returncode == 0\n\n\nasync def run_locally(\n    stages: Stages, exitfirst: bool, filter_substring: list[str], verbosity: int\n) -> None:\n    \"\"\"Not yet implementd: run all stages by executing each command\"\"\"\n    col = {\n        \"red\": \"\\033[1;31m\",\n        \"purple\": \"\\033[1;35m\",\n        \"green\": \"\\033[1;32m\",\n        \"bold\": \"\\033[0;37m\",\n        \"reset\": \"\\033[0;0m\",\n    }\n    results = {}\n    for stage in stages:\n        name = stage[\"NAME\"]\n        filtered = filter_substring and any(_ in name for _ in filter_substring)\n        if \"SKIPPED\" in stage or filtered:\n            results[name] = (\n                f\"SKIPPED Reason: none of {filter_substring!r} in name\"\n                if filtered\n                else f\"SKIPPED {stage['SKIPPED']}\"\n            )\n            print(f\"Stage {name!r}: {results[name]}\")\n            continue\n\n        print(f\"RUN stage ======== {col['bold']}{name}{col['reset']} ============\")\n\n        for key, value in stage.items():\n            LOG.debug(\"%s: %s\", key, value)\n\n        output: list[str] = []\n        t_before = time.time()\n        cmd_successful = await run_cmd(\n            cmd=stage[\"COMMAND\"],\n            env=dict(v.split(\"=\", 1) for v in stage[\"ENV_VAR_LIST\"]),\n            cwd=stage[\"DIR\"] or None,\n            stdout_prefix=f\"{col['bold']}{name}: {col['reset']}\",\n            stderr_prefix=f\"{col['bold']}{name}: {col['purple']}stderr:{col['reset']} \",\n            output=output if verbosity == 0 else None,\n        )\n        duration = time.time() - t_before\n\n        if cmd_successful:\n            results[name] = f\"{col['green']}SUCCESSFUL{col['reset']} ({duration:.2f}s)\"\n        else:\n            print(\"The stage failed and here is, what was captured:\")\n            for line in output:\n                print(line)\n            result_file_hint = (\n                f\" ({stage['RESULT_CHECK_FILE_PATTERN']})\"\n                if \"RESULT_CHECK_FILE_PATTERN\" in stage\n                else \"\"\n            )\n            results[name] = f\"{col['red']}FAILED{col['reset']} ({duration:.2f}s){result_file_hint}\"\n            if \"RESULT_CHECK_FILE_PATTERN\" in stage:\n                print(\n                    f\"Also a result file '{stage['RESULT_CHECK_FILE_PATTERN']}' has been captured:\"\n                )\n                if stage[\"RESULT_CHECK_FILE_PATTERN\"].endswith(\".txt\"):\n                    with open(stage[\"RESULT_CHECK_FILE_PATTERN\"]) as err_file:\n                        for l in err_file.readlines():\n                            print(f\"   {col['purple']}>>>{col['reset']} {l.rstrip()}\")\n                else:\n                    # in case we're dealing with an unknown file format we just print the file name\n                    print(f\"   {col['bold']}{stage['RESULT_CHECK_FILE_PATTERN']}{col['reset']}\")\n            if exitfirst:\n                print(\n                    f\"{col['red']}Stage {name!r} returned non-zero\"\n                    f\" and you told me to stop if that happens.{col['reset']}\"\n                )\n                break\n        print(f\"Stage {name!r}: {results[name]}\")\n\n    print(\"Summary:\")\n    for stage_name, summary in results.items():\n        print(f\" {stage_name:24s} | {summary}\")\n\n\ndef main() -> None:\n    \"\"\"Generate and either process or write a static list of stages\"\"\"\n    args = parse_args()\n    logging.basicConfig(\n        format=\"%(levelname)s %(name)s %(asctime)s: %(message)s\",\n        datefmt=\"%H:%M:%S\",\n        level=getattr(logging, {0: \"WARNING\", 1: \"INFO\", 2: \"DEBUG\"}.get(args.verbose, \"WARNING\")),\n    )\n    LOG.debug(\"Python: %s %s\", \".\".join(map(str, sys.version_info)), sys.executable)\n    LOG.debug(\"Args: %s\", args.__dict__)\n    LOG.debug(\"CWD: %s\", os.getcwd())\n    env_vars = {key: value for var in args.env for key, value in (var.split(\"=\", 1),)}\n    LOG.debug(\"Variables provided via command: %s\", env_vars)\n    for key, value in os.environ.items():\n        LOG.debug(\"ENV: %s: %s\", key, value)\n\n    if not args.write_file:\n        print(f\"Read and process {args.input}\")\n\n    variables, stages = compile_stage_info(args.input, env_vars, args.no_skip)\n\n    if not args.write_file:\n        print(\"Modified files ($CHANGED_FILES_REL):\")\n        for file in variables.get(\"CHANGED_FILES_REL\", \"\").split():\n            print(f\"  {file}\")\n\n    if args.write_file:\n        obj = {\"VARIABLES\": variables, \"STAGES\": stages}\n        if args.write_file == \"-\":\n            json.dump(obj=obj, fp=sys.stdout, indent=2)\n        else:\n            with open(Path(args.write_file), \"w\", encoding=\"UTF-8\") as f:\n                json.dump(obj=obj, fp=f, indent=2)\n    else:\n        print(f\"Found {len(stages)} stage commands to run locally\")\n        loop = asyncio.new_event_loop()\n        asyncio.set_event_loop(loop)\n        loop.run_until_complete(\n            run_locally(stages, args.exitfirst, args.filter_substring, args.verbose)\n        )\n        loop.close()\n\n\nif __name__ == \"__main__\":\n    main()\n"}
{"type": "source_file", "path": "cmk/base/api/__init__.py", "content": "#!/usr/bin/env python3\n# Copyright (C) 2019 Checkmk GmbH - License: GNU General Public License v2\n# This file is part of Checkmk (https://checkmk.com). It is subject to the terms and\n# conditions defined in the file COPYING, which is part of this source code package.\n"}
{"type": "source_file", "path": "buildscripts/scripts/unpublish-container-image.py", "content": "#!/usr/bin/env python3\n# Copyright (C) 2024 Checkmk GmbH - License: GNU General Public License v2\n# This file is part of Checkmk (https://checkmk.com). It is subject to the terms and\n# conditions defined in the file COPYING, which is part of this source code package.\nimport argparse\nfrom collections.abc import Iterator, Sequence\nfrom typing import Literal\n\nfrom cmk.ccc.version import Version\n\nfrom .lib.common import load_editions_file\nfrom .lib.registry import DockerImage, edition_to_registry, get_default_registries, Registry\n\nEdition = Literal[\"raw\", \"cloud\", \"enterprise\", \"managed\"]\n\n\ndef main():\n    arguments = parse_arguments()\n    editions = list(load_editions_file(arguments.editions_file)[\"editions\"])\n    editions = [arguments.edition] if arguments.edition != \"all\" else editions\n    registries = get_default_registries()\n\n    match arguments.action:\n        case \"list\":\n            for edition in editions:\n                registry = edition_to_registry(edition, registries)\n\n                container_name_and_namespace = (\n                    f\"{get_container_namespace(edition)}/check-mk-{edition}\"\n                )\n                for image in registry.list_images(container_name_and_namespace):\n                    print(image)\n\n        case \"delete\":\n            for image, edition, registry in get_docker_image_and_registry(\n                arguments.image_tag, editions, registries\n            ):\n                delete_image_from_registry(image, edition, registry, arguments.dry_run)\n        case _:\n            raise RuntimeError(\"Unexpected action\")\n\n\ndef parse_arguments() -> argparse.Namespace:\n    parser = argparse.ArgumentParser(\n        description=\"Delete a specific container image from the registries\",\n        epilog=(\n            \"Registry credentials will be taken from ~/.cmk-credentials, \"\n            \"they have to be present in form <username>:<password> in there.\"\n        ),\n    )\n    parser.add_argument(\"--editions_file\", required=True)\n    parser.add_argument(\n        \"--edition\",\n        default=\"all\",\n        help=\"Specify for what edition to process. Default: all\",\n        choices=[\"raw\", \"cloud\", \"enterprise\", \"managed\", \"all\"],\n    )\n\n    subparsers = parser.add_subparsers(help=\"action to perform\", required=True)\n\n    # List actions\n    list_parser = subparsers.add_parser(\"list\", help=\"List available images and tags\")\n    list_parser.set_defaults(action=\"list\")\n\n    # Delete actions\n    parser_delete = subparsers.add_parser(\"delete\", help=\"Delete an image with a specific tag\")\n    parser_delete.add_argument(\n        \"--dry-run\", action=\"store_true\", help=\"Perform a run without altering the registries.\"\n    )\n    parser_delete.add_argument(\"--image-tag\", required=True, help=\"The version to delete\")\n    parser_delete.set_defaults(action=\"delete\")\n\n    return parser.parse_args()\n\n\ndef is_latest(image_tag: str) -> bool:\n    assert image_tag, f\"Expected to receive image tag, got {image_tag!r}\"\n\n    # Has to match branch-specific latest (e.g. \"2.3.0-latest\") and literal \"latest\"\n    if image_tag.endswith(\"latest\"):\n        return True\n\n    return False\n\n\ndef get_docker_image_and_registry(\n    version: str, editions: Sequence[Edition], registries: list[Registry]\n) -> Iterator[tuple[DockerImage, str, Registry]]:\n    for edition in editions:\n        registry = edition_to_registry(edition, registries)\n\n        yield (\n            DockerImage(\n                tag=version,\n                image_name=f\"{get_container_namespace(edition)}/check-mk-{edition}\",\n            ),\n            edition,\n            registry,\n        )\n\n\ndef get_container_namespace(edition: Edition) -> str:\n    match edition:\n        case \"raw\" | \"cloud\":\n            return \"checkmk\"\n        case \"enterprise\" | \"managed\":\n            return edition\n        case _:\n            raise RuntimeError(f\"Unknown edition {edition}\")\n\n\ndef delete_image_from_registry(\n    image: DockerImage, edition: Edition, registry: Registry, dry_run: bool\n) -> None:\n    if not registry.image_exists(image, edition):\n        print(f\"Image {image.full_name()} does not exist on {registry.url} - ignoring\")\n        return\n\n    # We do not want to delete any latest (\"latest\", a version-specific latest like\n    # \"2.3.0-latest\" or 2.3.0-daily), because this would impact different workflows\n    # expecting such a version.\n    # In such a case we will move the tag to the previous version found on the registry.\n    if registry.is_latest_image(image):\n        previous_release_tag = registry.get_previous_release_tag(image, edition)\n        previous_release_image = DockerImage(image.image_name, tag=previous_release_tag)\n        branch_latest_tag = get_branch_specific_tag(previous_release_tag, \"latest\")\n\n        current_image_tags = registry.get_all_image_tags(image)\n\n        image_is_branch_latest = branch_latest_tag in current_image_tags\n        if image_is_branch_latest:\n            print(\n                f\"Will be moving {branch_latest_tag!r} to point to {previous_release_image.full_name()!r}\"\n            )\n\n            if not dry_run:\n                registry.tag(\n                    source=previous_release_image.full_name(),\n                    new_tag=branch_latest_tag,\n                )\n        else:\n            print(f\"Tag {branch_latest_tag!r} does not point to this image, not moving this tag.\")\n\n        image_is_latest_latest = \"latest\" in current_image_tags\n        if image_is_latest_latest:\n            print(f\"Will be moving 'latest' to point to {previous_release_image.full_name()!r}\")\n\n            if not dry_run:\n                registry.tag(\n                    source=previous_release_image.full_name(),\n                    new_tag=\"latest\",\n                )\n        else:\n            print(\"Tag 'latest' does not point to this image, not moving this tag.\")\n\n        branch_specific_daily_tag = get_branch_specific_tag(\n            previous_release_image.full_name(), \"daily\"\n        )\n        image_is_branch_daily = branch_specific_daily_tag in current_image_tags\n        if image_is_branch_daily:\n            print(\n                f\"Will be moving {branch_specific_daily_tag!r} to point to {previous_release_image.full_name()!r}\"\n            )\n\n            if not dry_run:\n                registry.tag(\n                    source=previous_release_image.full_name(),\n                    new_tag=branch_specific_daily_tag,\n                )\n        else:\n            print(\n                f\"Tag {branch_specific_daily_tag!r} does not point to this image, not moving this tag.\"\n            )\n\n    if dry_run:\n        print(f\"Would be deleting image {image.full_name()} from {registry.url}\")\n        return\n\n    # At this point the image with the tag we want to remove should be the only reference to\n    # that image. We assume that deleting the tag will also remove the image - removal of\n    # the image hash should not be required, because the registry should handle deleting images\n    # without tags applied.\n    print(f\"Deleting image {image.full_name()} from {registry.url}...\")\n    registry.delete_image(image)\n    print(f\"Deleted image {image.full_name()} from {registry.url}\")\n\n\ndef get_branch_specific_tag(version: str, suffix: str) -> str:\n    \"\"\"\n    Get the branch-specific tag with desired suffix\n\n    >>> get_branch_specific_tag(\"2.3.0p45\", \"latest\")\n    '2.3.0-latest'\n    >>> get_branch_specific_tag(\"2.3.0-2023.12.31\", \"daily\")\n    '2.3.0-daily'\n    \"\"\"\n    version_object = Version.from_str(version)\n    return f\"{version_object.base}-{suffix}\"\n\n\nif __name__ == \"__main__\":\n    main()\n"}
{"type": "source_file", "path": "cmk/base/api/agent_based/plugin_classes.py", "content": "#!/usr/bin/env python3\n# Copyright (C) 2019 Checkmk GmbH - License: GNU General Public License v2\n# This file is part of Checkmk (https://checkmk.com). It is subject to the terms and\n# conditions defined in the file COPYING, which is part of this source code package.\n\nfrom __future__ import annotations\n\nfrom collections.abc import Callable, Mapping, Sequence\nfrom dataclasses import dataclass\nfrom typing import Any, Literal, NamedTuple, Protocol, Self\n\nfrom cmk.utils.rulesets import RuleSetName\nfrom cmk.utils.sectionname import SectionName\n\nfrom cmk.snmplib import SNMPDetectBaseType\n\nfrom cmk.checkengine.checking import CheckPluginName\nfrom cmk.checkengine.inventory import InventoryPlugin, InventoryPluginName\nfrom cmk.checkengine.sectionparser import ParsedSectionName\n\nfrom cmk.agent_based.v2 import (\n    CheckResult,\n    DiscoveryResult,\n    HostLabelGenerator,\n    StringByteTable,\n    StringTable,\n)\nfrom cmk.discover_plugins import PluginLocation\n\n\n@dataclass(frozen=True)\nclass LegacyPluginLocation:\n    file_name: str\n\n\nCheckFunction = Callable[..., CheckResult]\nDiscoveryFunction = Callable[..., DiscoveryResult]\n\nRuleSetTypeName = Literal[\"merged\", \"all\"]\n\nAgentParseFunction = Callable[[StringTable], Any]\n\nHostLabelFunction = Callable[..., HostLabelGenerator]\n\nSNMPParseFunction = Callable[[list[StringTable]], Any] | Callable[[list[StringByteTable]], Any]\n\nSimpleSNMPParseFunction = Callable[[StringTable], Any] | Callable[[StringByteTable], Any]\n\n\nclass AgentSectionPlugin(NamedTuple):\n    name: SectionName\n    parsed_section_name: ParsedSectionName\n    parse_function: AgentParseFunction\n    host_label_function: HostLabelFunction\n    host_label_default_parameters: Mapping[str, object] | None\n    host_label_ruleset_name: RuleSetName | None\n    host_label_ruleset_type: RuleSetTypeName\n    supersedes: set[SectionName]\n    # We need to allow 'None' for the trivial agent section :-|\n    location: PluginLocation | LegacyPluginLocation | None\n\n\nclass _OIDSpecLike(Protocol):\n    @property\n    def column(self) -> int | str: ...\n\n    @property\n    def encoding(self) -> Literal[\"string\", \"binary\"]: ...\n\n    @property\n    def save_to_cache(self) -> bool: ...\n\n\nclass _SNMPTreeLike(Protocol):\n    @property\n    def base(self) -> str: ...\n\n    @property\n    def oids(self) -> Sequence[_OIDSpecLike]: ...\n\n\nclass SNMPSectionPlugin(NamedTuple):\n    name: SectionName\n    parsed_section_name: ParsedSectionName\n    parse_function: SNMPParseFunction\n    host_label_function: HostLabelFunction\n    host_label_default_parameters: Mapping[str, object] | None\n    host_label_ruleset_name: RuleSetName | None\n    host_label_ruleset_type: RuleSetTypeName\n    detect_spec: SNMPDetectBaseType\n    trees: Sequence[_SNMPTreeLike]\n    supersedes: set[SectionName]\n    location: PluginLocation | LegacyPluginLocation\n\n\nSectionPlugin = AgentSectionPlugin | SNMPSectionPlugin\n\n\nclass CheckPlugin(NamedTuple):\n    name: CheckPluginName\n    sections: list[ParsedSectionName]\n    service_name: str\n    discovery_function: DiscoveryFunction\n    discovery_default_parameters: Mapping[str, object] | None\n    discovery_ruleset_name: RuleSetName | None\n    discovery_ruleset_type: RuleSetTypeName\n    check_function: CheckFunction\n    check_default_parameters: Mapping[str, object] | None\n    check_ruleset_name: RuleSetName | None\n    cluster_check_function: CheckFunction | None\n    location: PluginLocation | LegacyPluginLocation\n\n\n@dataclass(frozen=True, kw_only=True)\nclass AgentBasedPlugins:\n    agent_sections: Mapping[SectionName, AgentSectionPlugin]\n    snmp_sections: Mapping[SectionName, SNMPSectionPlugin]\n    check_plugins: Mapping[CheckPluginName, CheckPlugin]\n    inventory_plugins: Mapping[InventoryPluginName, InventoryPlugin]\n    errors: Sequence[str]\n\n    @classmethod\n    def empty(cls) -> Self:\n        return cls(\n            agent_sections={},\n            snmp_sections={},\n            check_plugins={},\n            inventory_plugins={},\n            errors=(),\n        )\n"}
