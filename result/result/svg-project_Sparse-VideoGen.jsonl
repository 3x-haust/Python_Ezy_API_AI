{"repo_info": {"repo_name": "Sparse-VideoGen", "repo_owner": "svg-project", "repo_url": "https://github.com/svg-project/Sparse-VideoGen"}}
{"type": "test_file", "path": "svg/kernels/test/test_rms_norm.py", "content": "import sys\nsys.path.append('/ssd/data/xihaocheng/Hunyuan/I2VSparse/kernels/build/')\n\nimport _kernels\n\nimport torch\nimport pytest\nfrom itertools import product\n\ntorch.manual_seed(0)\n\ndef assert_close(a, b):\n    rtol, atol = {\n        torch.float16: (5e-3, 5e-3),\n        torch.bfloat16: (3e-2, 2e-2),\n    }[a.dtype]\n    torch.testing.assert_close(a, b, rtol=rtol, atol=atol)\n    \ndef different_proportion(a, b):\n    return torch.sum(a != b).item() / a.numel()\n    \ndef ref_host_rms_norm(\n    input,\n    gemma\n) -> torch.Tensor:\n    return torch.nn.functional.rms_norm(input, [input.size(-1)], gemma, 1e-5)\n\ndef replica_host_rms_norm(\n    input,\n    gemma\n) -> torch.Tensor:\n    input_dtype = input.dtype\n    input = input.to(torch.float32)\n    variance = input.pow(2).mean(-1, keepdim=True)\n    input = input * torch.rsqrt(variance + 1e-5)\n    return gemma * input.to(input_dtype)\n\nparameters = list(product([1,7,31,55,95,128,512], [32, 64,128,256]))\n@pytest.mark.parametrize(\"batch_size, head_dim\", parameters)\n@torch.inference_mode()\ndef test_rms_norm(batch_size, head_dim):    \n    input = torch.randn(batch_size, head_dim, dtype=torch.bfloat16).cuda()\n    gemma = torch.randn(head_dim, dtype=torch.bfloat16).cuda()\n\n    output_host = ref_host_rms_norm(input, gemma)\n    output_replica = replica_host_rms_norm(input, gemma)\n    _kernels.rms_norm_forward(input, gemma, 1e-5)\n    \n    output_host = output_host.to(input)\n    output_replica = output_replica.to(input)\n\n    assert_close(input, output_host)\n    assert_close(output_replica, output_host)\n    print(f\"{different_proportion(input, output_host) * 100:.4f}% elements are different\")\n    print(f\"{different_proportion(output_replica, output_host) * 100:.4f}% elements are different\")\n"}
{"type": "test_file", "path": "svg/kernels/test/test_apply_rope_txtlast.py", "content": "import sys\nsys.path.append('/ssd/data/xihaocheng/Hunyuan/I2VSparse/kernels/build/')\n\nimport _kernels\n\nimport torch\nimport pytest\nfrom itertools import product\n\ndef assert_close(a, b):\n    rtol, atol = {\n        torch.float16: (5e-3, 5e-3),\n        torch.bfloat16: (3e-2, 2e-2),\n    }[a.dtype]\n    torch.testing.assert_close(a, b, rtol=rtol, atol=atol)\n            \ndef different_proportion(a, b):\n    return torch.sum(a != b).item() / a.numel()\n    \ndef ref_host_apply_rope(\n    x,\n    cos,\n    sin,\n) -> torch.Tensor:\n    # Ref: /diffusers/models/embeddings.py#L1171\n    assert x.dim() == 4\n    x_real, x_imag = x.reshape(*x.shape[:-1], -1, 2).unbind(-1)  # [B, S, H, D//2]\n    x_rotated = torch.stack([-x_imag, x_real], dim=-1).flatten(3)\n    \n    out = (x.float() * cos + x_rotated.float() * sin).to(x.dtype)\n    \n    return out\n\nparameters = list(product([1,3,5], [16,32], [151,1037,6778], [64,128,256], [15,35,77]))\n@pytest.mark.parametrize(\"bsz, num_heads, total_seq_len, head_dim, len_text_prompt\", parameters)\n@torch.inference_mode()\ndef test_apply_rope(bsz, num_heads, total_seq_len, head_dim, len_text_prompt):\n    if len_text_prompt >= total_seq_len:\n        pytest.skip(\"len_text_prompt >= total_seq_len\")\n    valid_seq_len = total_seq_len - len_text_prompt\n    q = torch.randn(bsz, num_heads, total_seq_len, head_dim, dtype=torch.bfloat16).cuda()\n    k = torch.randn(bsz, num_heads, total_seq_len, head_dim, dtype=torch.bfloat16).cuda()\n    cos = torch.randn(valid_seq_len, head_dim, dtype=torch.float32).cuda()\n    sin = torch.randn(valid_seq_len, head_dim, dtype=torch.float32).cuda()\n    \n    q_image_host = ref_host_apply_rope(q[:,:,:-len_text_prompt,:], cos, sin)\n    k_image_host = ref_host_apply_rope(k[:,:,:-len_text_prompt,:], cos, sin)\n    q_text_host = q[:,:,-len_text_prompt:,:].clone()\n    k_text_host = k[:,:,-len_text_prompt:,:].clone()\n    \n    _kernels.apply_qk_rope_inplace_cossin_txtlast(q, k, cos, sin, len_text_prompt)\n    \n    assert_close(q[:,:,:-len_text_prompt:,:], q_image_host)\n    assert_close(k[:,:,:-len_text_prompt:,:], k_image_host)\n    assert_close(q[:,:,-len_text_prompt:,:], q_text_host)\n    assert_close(k[:,:,-len_text_prompt:,:], k_text_host)\n    \n    print(f\"{different_proportion(q[:,:,:-len_text_prompt:,:], q_image_host) * 100:.4f}% elements are different for q_image_host\")\n    print(f\"{different_proportion(k[:,:,:-len_text_prompt:,:], k_image_host) * 100:.4f}% elements are different for k_image_host\")\n    print(f\"{different_proportion(q[:,:,-len_text_prompt:,:], q_text_host) * 100:.4f}% elements are different for q_text_host\")\n    print(f\"{different_proportion(k[:,:,-len_text_prompt:,:], k_text_host) * 100:.4f}% elements are different for k_text_host\")"}
{"type": "test_file", "path": "svg/kernels/test/test_apply_rope.py", "content": "import sys\nsys.path.append('/ssd/data/xihaocheng/Hunyuan/I2VSparse/kernels/build/')\n\nimport _kernels\n\nimport torch\nimport pytest\nfrom itertools import product\n\ndef assert_close(a, b):\n    rtol, atol = {\n        torch.float16: (5e-3, 5e-3),\n        torch.bfloat16: (3e-2, 2e-2),\n    }[a.dtype]\n    torch.testing.assert_close(a, b, rtol=rtol, atol=atol)\n        \ndef different_proportion(a, b):\n    return torch.sum(a != b).item() / a.numel()\n    \ndef ref_host_apply_rope(\n    x,\n    cos,\n    sin,\n) -> torch.Tensor:\n    # Ref: /diffusers/models/embeddings.py#L1171\n    assert x.dim() == 4\n    x_real, x_imag = x.reshape(*x.shape[:-1], -1, 2).unbind(-1)  # [B, S, H, D//2]\n    x_rotated = torch.stack([-x_imag, x_real], dim=-1).flatten(3)\n    \n    out = (x.float() * cos + x_rotated.float() * sin).to(x.dtype)\n    \n    return out\n\nparameters = list(product([1,3,5], [16,32], [151,1037,6778], [64,128,256], [15,35,77]))\n@pytest.mark.parametrize(\"bsz, num_heads, total_seq_len, head_dim, len_text_prompt\", parameters)\n@torch.inference_mode()\ndef test_apply_rope(bsz, num_heads, total_seq_len, head_dim, len_text_prompt):\n    if len_text_prompt >= total_seq_len:\n        pytest.skip(\"len_text_prompt >= total_seq_len\")\n    valid_seq_len = total_seq_len - len_text_prompt\n    q = torch.randn(bsz, num_heads, total_seq_len, head_dim, dtype=torch.bfloat16).cuda()\n    k = torch.randn(bsz, num_heads, total_seq_len, head_dim, dtype=torch.bfloat16).cuda()\n    cos = torch.randn(valid_seq_len, head_dim, dtype=torch.float32).cuda()\n    sin = torch.randn(valid_seq_len, head_dim, dtype=torch.float32).cuda()\n    \n    q_image_host = ref_host_apply_rope(q[:,:,len_text_prompt:,:], cos, sin)\n    k_image_host = ref_host_apply_rope(k[:,:,len_text_prompt:,:], cos, sin)\n    q_text_host = q[:,:,:len_text_prompt,:].clone()\n    k_text_host = k[:,:,:len_text_prompt,:].clone()\n    \n    _kernels.apply_qk_rope_inplace_cossin(q, k, cos, sin, len_text_prompt)\n    \n    assert_close(q[:,:,len_text_prompt:,:], q_image_host)\n    assert_close(k[:,:,len_text_prompt:,:], k_image_host)\n    assert_close(q[:,:,:len_text_prompt,:], q_text_host)\n    assert_close(k[:,:,:len_text_prompt,:], k_text_host)\n    \n    print(f\"{different_proportion(q[:,:,len_text_prompt:,:], q_image_host) * 100:.4f}% elements are different for q_image_host\")\n    print(f\"{different_proportion(k[:,:,len_text_prompt:,:], k_image_host) * 100:.4f}% elements are different for k_image_host\")\n    print(f\"{different_proportion(q[:,:,:len_text_prompt,:], q_text_host) * 100:.4f}% elements are different for q_text_host\")\n    print(f\"{different_proportion(k[:,:,:len_text_prompt,:], k_text_host) * 100:.4f}% elements are different for k_text_host\")"}
{"type": "test_file", "path": "svg/kernels/test/test_layer_norm.py", "content": "import sys\nsys.path.append('/ssd/data/xihaocheng/Hunyuan/I2VSparse/kernels/build/')\n\nimport _kernels\n\nimport torch\nimport pytest\nfrom itertools import product\n\ndef assert_close(a, b):\n    rtol, atol = {\n        torch.float16: (5e-3, 5e-3),\n        torch.bfloat16: (3e-2, 2e-2),\n    }[a.dtype]\n    torch.testing.assert_close(a, b, rtol=rtol, atol=atol)\n        \ndef different_proportion(a, b):\n    return torch.sum(a != b).item() / a.numel()\n    \ndef ref_host_layer_norm(\n    input,\n    gemma,\n    beta,\n) -> torch.Tensor:\n    return torch.nn.functional.layer_norm(input, [input.size(-1)], gemma, beta, 1e-5)\n\nparameters = list(product([1,7,31,55,95,128,512], [32, 64,128,256]))\n@pytest.mark.parametrize(\"batch_size, head_dim\", parameters)\n@torch.inference_mode()\ndef test_layer_norm(batch_size, head_dim):\n    input = torch.randn(batch_size, head_dim, dtype=torch.bfloat16).cuda()\n    gemma = torch.randn(head_dim, dtype=torch.bfloat16).cuda()\n    beta = torch.randn(head_dim, dtype=torch.bfloat16).cuda()\n    \n    output_host = ref_host_layer_norm(input, gemma, beta)\n    _kernels.layer_norm_forward(input, gemma, beta)\n\n    assert_close(input, output_host)\n    print(f\"{different_proportion(input, output_host) * 100:.4f}% elements are different\")"}
{"type": "source_file", "path": "svg/models/cog/custom_models.py", "content": "# Copyright 2024 The CogVideoX team, Tsinghua University & ZhipuAI and The HuggingFace Team.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom typing import Any, Dict, Optional, Tuple, Union\n\nimport torch\nfrom torch import nn\n\nfrom diffusers.models.modeling_outputs import Transformer2DModelOutput\nfrom diffusers.utils import USE_PEFT_BACKEND, is_torch_version, logging, scale_lora_layers, unscale_lora_layers\nfrom diffusers.models.transformers.cogvideox_transformer_3d import CogVideoXBlock, CogVideoXTransformer3DModel\n\nlogger = logging.get_logger(__name__)  # pylint: disable=invalid-name\n\nclass CogVideoXBlock_Sparse(CogVideoXBlock):\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        encoder_hidden_states: torch.Tensor,\n        temb: torch.Tensor,\n        image_rotary_emb: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,\n        timestep: int = 0\n    ) -> torch.Tensor:\n        text_seq_length = encoder_hidden_states.size(1)\n\n        # norm & modulate\n        norm_hidden_states, norm_encoder_hidden_states, gate_msa, enc_gate_msa = self.norm1(\n            hidden_states, encoder_hidden_states, temb\n        )\n\n        # attention\n        attn_hidden_states, attn_encoder_hidden_states = self.attn1(\n            hidden_states=norm_hidden_states,\n            encoder_hidden_states=norm_encoder_hidden_states,\n            image_rotary_emb=image_rotary_emb,\n            timestep=timestep\n        )\n\n        hidden_states = hidden_states + gate_msa * attn_hidden_states\n        encoder_hidden_states = encoder_hidden_states + enc_gate_msa * attn_encoder_hidden_states\n\n        # norm & modulate\n        norm_hidden_states, norm_encoder_hidden_states, gate_ff, enc_gate_ff = self.norm2(\n            hidden_states, encoder_hidden_states, temb\n        )\n\n        # feed-forward\n        norm_hidden_states = torch.cat([norm_encoder_hidden_states, norm_hidden_states], dim=1)\n        ff_output = self.ff(norm_hidden_states)\n\n        hidden_states = hidden_states + gate_ff * ff_output[:, text_seq_length:]\n        encoder_hidden_states = encoder_hidden_states + enc_gate_ff * ff_output[:, :text_seq_length]\n\n        return hidden_states, encoder_hidden_states\n\n\nclass CogVideoXTransformer3DModel_Sparse(CogVideoXTransformer3DModel):\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        encoder_hidden_states: torch.Tensor,\n        timestep: Union[int, float, torch.LongTensor],\n        timestep_cond: Optional[torch.Tensor] = None,\n        ofs: Optional[Union[int, float, torch.LongTensor]] = None,\n        image_rotary_emb: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,\n        attention_kwargs: Optional[Dict[str, Any]] = None,\n        return_dict: bool = True,\n    ):\n        if attention_kwargs is not None:\n            attention_kwargs = attention_kwargs.copy()\n            lora_scale = attention_kwargs.pop(\"scale\", 1.0)\n        else:\n            lora_scale = 1.0\n\n        if USE_PEFT_BACKEND:\n            # weight the lora layers by setting `lora_scale` for each PEFT layer\n            scale_lora_layers(self, lora_scale)\n        else:\n            if attention_kwargs is not None and attention_kwargs.get(\"scale\", None) is not None:\n                logger.warning(\n                    \"Passing `scale` via `attention_kwargs` when not using the PEFT backend is ineffective.\"\n                )\n\n        batch_size, num_frames, channels, height, width = hidden_states.shape\n\n        # 1. Time embedding\n        timesteps = timestep\n        t_emb = self.time_proj(timesteps)\n\n        # timesteps does not contain any weights and will always return f32 tensors\n        # but time_embedding might actually be running in fp16. so we need to cast here.\n        # there might be better ways to encapsulate this.\n        t_emb = t_emb.to(dtype=hidden_states.dtype)\n        emb = self.time_embedding(t_emb, timestep_cond)\n\n        if self.ofs_embedding is not None:\n            ofs_emb = self.ofs_proj(ofs)\n            ofs_emb = ofs_emb.to(dtype=hidden_states.dtype)\n            ofs_emb = self.ofs_embedding(ofs_emb)\n            emb = emb + ofs_emb\n\n        # 2. Patch embedding\n        hidden_states = self.patch_embed(encoder_hidden_states, hidden_states)\n        hidden_states = self.embedding_dropout(hidden_states)\n\n        text_seq_length = encoder_hidden_states.shape[1]\n        encoder_hidden_states = hidden_states[:, :text_seq_length]\n        hidden_states = hidden_states[:, text_seq_length:]\n\n        # 3. Transformer blocks\n        for i, block in enumerate(self.transformer_blocks):\n            if torch.is_grad_enabled() and self.gradient_checkpointing:\n\n                def create_custom_forward(module):\n                    def custom_forward(*inputs):\n                        return module(*inputs)\n\n                    return custom_forward\n\n                ckpt_kwargs: Dict[str, Any] = {\"use_reentrant\": False} if is_torch_version(\">=\", \"1.11.0\") else {}\n                hidden_states, encoder_hidden_states = torch.utils.checkpoint.checkpoint(\n                    create_custom_forward(block),\n                    hidden_states,\n                    encoder_hidden_states,\n                    emb,\n                    image_rotary_emb,\n                    **ckpt_kwargs,\n                )\n            else:\n                hidden_states, encoder_hidden_states = block(\n                    hidden_states=hidden_states,\n                    encoder_hidden_states=encoder_hidden_states,\n                    temb=emb,\n                    image_rotary_emb=image_rotary_emb,\n                    timestep=timestep\n                )\n\n        if not self.config.use_rotary_positional_embeddings:\n            # CogVideoX-2B\n            hidden_states = self.norm_final(hidden_states)\n        else:\n            # CogVideoX-5B\n            hidden_states = torch.cat([encoder_hidden_states, hidden_states], dim=1)\n            hidden_states = self.norm_final(hidden_states)\n            hidden_states = hidden_states[:, text_seq_length:]\n\n        # 4. Final block\n        hidden_states = self.norm_out(hidden_states, temb=emb)\n        hidden_states = self.proj_out(hidden_states)\n\n        # 5. Unpatchify\n        p = self.config.patch_size\n        p_t = self.config.patch_size_t\n\n        if p_t is None:\n            output = hidden_states.reshape(batch_size, num_frames, height // p, width // p, -1, p, p)\n            output = output.permute(0, 1, 4, 2, 5, 3, 6).flatten(5, 6).flatten(3, 4)\n        else:\n            output = hidden_states.reshape(\n                batch_size, (num_frames + p_t - 1) // p_t, height // p, width // p, -1, p_t, p, p\n            )\n            output = output.permute(0, 1, 5, 4, 2, 6, 3, 7).flatten(6, 7).flatten(4, 5).flatten(1, 2)\n\n        if USE_PEFT_BACKEND:\n            # remove `lora_scale` from each PEFT layer\n            unscale_lora_layers(self, lora_scale)\n\n        if not return_dict:\n            return (output,)\n        return Transformer2DModelOutput(sample=output)\n\ndef replace_sparse_forward():\n    CogVideoXBlock.forward = CogVideoXBlock_Sparse.forward\n    CogVideoXTransformer3DModel.forward = CogVideoXTransformer3DModel_Sparse.forward"}
{"type": "source_file", "path": "cog_inference.py", "content": "import argparse\n\nimport torch\nfrom diffusers import CogVideoXImageToVideoPipeline\n\nfrom svg.models.cog.utils import seed_everything\nfrom svg.models.cog.inference import replace_cog_attention, sample_image\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description=\"A script that sets a random seed.\")\n    parser.add_argument(\"--version\", type=str, default=\"v1.5\", choices=[\"v1\", \"v1.5\"], help=\"Random seed for reproducibility\")\n    parser.add_argument(\"--seed\", type=int, default=0, help=\"Random seed for reproducibility\")\n    parser.add_argument(\"--image_path\", type=str, required=True, help=\"Image Path\")\n    parser.add_argument(\"--prompt\", type=str, required=True, help=\"Prompt\")\n    parser.add_argument(\"--pattern\", type=str, default=\"SVG\", choices=[\"SVG\", \"dense\"])\n    parser.add_argument(\"--num_step\", type=int, default=50, help=\"Number of steps to inference\")\n    parser.add_argument(\"--first_layers_fp\", type=float, default=0.025, help=\"Only works for best config. Leave the 0, 1, 2, 40, 41 layers in FP\")\n    parser.add_argument(\"--first_times_fp\", type=float, default=0.2, help=\"Only works for best config. Leave the first 10% timestep in FP\")\n    parser.add_argument(\"--num_sampled_rows\", type=int, default=32, help=\"The number of sampled rows\")\n    parser.add_argument(\n        \"--sparsity\",\n        type=float,\n        default=0.25,\n        help=\"The sparsity of the striped attention pattern. Accepts one or two float values. Only effective for fast_sample_mse\"\n    )\n    parser.add_argument(\n        \"--output_path\",\n        type=str,\n        required=True,\n        help=\"Output generated videos\"\n    )\n\n    args = parser.parse_args()\n\n    seed_everything(args.seed)\n\n\n    model_id = \"THUDM/CogVideoX1.5-5B-I2V\"\n\n    dtype = torch.bfloat16\n\n    pipe = CogVideoXImageToVideoPipeline.from_pretrained(\n        model_id,\n        torch_dtype=dtype\n    ).to(\"cuda\")\n\n    pipe.vae.enable_tiling()\n    pipe.vae.enable_slicing()\n\n    if args.pattern == \"SVG\":\n        replace_cog_attention(\n            pipe,\n            args.version,\n            args.num_sampled_rows,\n            args.sparsity,\n            args.first_layers_fp,\n            args.first_times_fp\n        )\n    \n    sample_image(\n        pipe,\n        args.prompt,\n        args.image_path,\n        args.output_path,\n        args.seed,\n        args.version,\n        args.num_step\n    )"}
{"type": "source_file", "path": "svg/models/cog/placement.py", "content": "import torch\nimport triton\nimport triton.language as tl\n\ndef token_reorder_to_token_major(tensor, fix_len, reorder_len, reorder_num_frame, frame_size):\n    \"\"\"Reorder it from frame major to token major!\"\"\"\n    assert reorder_len == reorder_num_frame * frame_size\n    assert tensor.shape[2] == fix_len + reorder_len\n\n    tensor[:, :, fix_len:, :] = tensor[:, :, fix_len:, :].reshape(tensor.shape[0], tensor.shape[1], reorder_num_frame, frame_size, tensor.shape[3]) \\\n                                                         .transpose(2, 3).reshape(tensor.shape[0], tensor.shape[1], reorder_len, tensor.shape[3])\n    return tensor\n\ndef token_reorder_to_frame_major(tensor, fix_len, reorder_len, reorder_num_frame, frame_size):\n    \"\"\"Reorder it from token major to frame major!\"\"\"\n    assert reorder_len == reorder_num_frame * frame_size\n    assert tensor.shape[2] == fix_len + reorder_len\n\n    tensor[:, :, fix_len:, :] = tensor[:, :, fix_len:, :].reshape(tensor.shape[0], tensor.shape[1], frame_size, reorder_num_frame, tensor.shape[3]) \\\n                                                         .transpose(2, 3).reshape(tensor.shape[0], tensor.shape[1], reorder_len, tensor.shape[3])\n    return tensor\n\n\n@triton.jit\ndef sparse_head_placement_kernel(\n    query_ptr, key_ptr, value_ptr, # [cfg, num_heads, seq_len, head_dim] seq_len = context_length + num_frame * frame_size\n    query_out_ptr, key_out_ptr, value_out_ptr, # [cfg, num_heads, seq_len, head_dim]\n    best_mask_idx_ptr, # [cfg, num_heads]\n    query_stride_b, query_stride_h, query_stride_s, query_stride_d,\n    mask_idx_stride_b, mask_idx_stride_h,\n    seq_len: tl.constexpr,\n    head_dim: tl.constexpr,\n    context_length: tl.constexpr,   \n    num_frame: tl.constexpr,        \n    frame_size: tl.constexpr,      \n    BLOCK_SIZE: tl.constexpr\n):\n    # Copy query, key, value to output\n    # range: [b, h, block_id * block_size: block_id * block_size + block_size, :]\n    cfg = tl.program_id(0)\n    head = tl.program_id(1)\n    block_id = tl.program_id(2)\n\n    start_id = block_id * BLOCK_SIZE\n    end_id = start_id + BLOCK_SIZE\n    end_id = tl.where(end_id > seq_len, seq_len, end_id) \n\n    # Load best mask idx (0 is spatial, 1 is temporal)\n    is_temporal = tl.load(best_mask_idx_ptr + cfg * mask_idx_stride_b + head * mask_idx_stride_h)\n    \n    offset_token = tl.arange(0, BLOCK_SIZE) + start_id\n    offset_mask = offset_token < seq_len\n    offset_d = tl.arange(0, head_dim)\n\n    if is_temporal:\n        # if offset_token < context_length:\n        #   offset_store_token = offset_token \n        # else:\n        #   frame_id = (offset_token - context_length) // frame_size\n        #   patch_id = (offset_token - context_length) - frame_id * frame_size\n        #   offset_store_token = context_length + patch_id * num_frame + frame_id\n        frame_id = (offset_token - context_length) // frame_size\n        patch_id = (offset_token - context_length) - frame_id * frame_size\n        offset_store_token = tl.where(offset_token < context_length, offset_token, context_length + patch_id * num_frame + frame_id)\n\n        offset_load = (cfg * query_stride_b + head * query_stride_h + offset_token[:,None] * query_stride_s) + offset_d[None,:] * query_stride_d\n        offset_query = query_ptr + offset_load\n        offset_key = key_ptr + offset_load\n        offset_value = value_ptr + offset_load\n\n        offset_store = (cfg * query_stride_b + head * query_stride_h + offset_store_token[:,None] * query_stride_s) + offset_d[None,:] * query_stride_d\n        offset_query_out = query_out_ptr + offset_store\n        offset_key_out = key_out_ptr + offset_store\n        offset_value_out = value_out_ptr + offset_store\n\n        # Maybe tune the pipeline here\n        query = tl.load(offset_query, mask=offset_mask[:,None])\n        tl.store(offset_query_out, query, mask=offset_mask[:,None])\n        key = tl.load(offset_key, mask=offset_mask[:,None])\n        tl.store(offset_key_out, key, mask=offset_mask[:,None])\n        value = tl.load(offset_value, mask=offset_mask[:,None])\n        tl.store(offset_value_out, value, mask=offset_mask[:,None])\n\n\n    else:\n        offset_load = (cfg * query_stride_b + head * query_stride_h + offset_token[:,None] * query_stride_s) + offset_d[None,:] * query_stride_d\n        offset_query = query_ptr + offset_load\n        offset_key = key_ptr + offset_load\n        offset_value = value_ptr + offset_load\n\n        offset_store = offset_load\n        offset_query_out = query_out_ptr + offset_store\n        offset_key_out = key_out_ptr + offset_store\n        offset_value_out = value_out_ptr + offset_store\n\n        # Maybe tune the pipeline here\n        query = tl.load(offset_query, mask=offset_mask[:,None])\n        tl.store(offset_query_out, query, mask=offset_mask[:,None])\n        key = tl.load(offset_key, mask=offset_mask[:,None])\n        tl.store(offset_key_out, key, mask=offset_mask[:,None])\n        value = tl.load(offset_value, mask=offset_mask[:,None])\n        tl.store(offset_value_out, value, mask=offset_mask[:,None])\n\n\ndef sparse_head_placement(query, key, value, query_out, key_out, value_out, best_mask_idx, context_length, num_frame, frame_size):\n    cfg, num_heads, seq_len, head_dim = query.shape\n    BLOCK_SIZE = 128\n    assert seq_len == context_length + num_frame * frame_size\n\n    grid = (cfg, num_heads, (seq_len + BLOCK_SIZE - 1) // BLOCK_SIZE)\n\n    sparse_head_placement_kernel[grid](\n        query, key, value, \n        query_out, key_out, value_out, \n        best_mask_idx,\n        query.stride(0), query.stride(1), query.stride(2), query.stride(3),\n        best_mask_idx.stride(0), best_mask_idx.stride(1),\n        seq_len, head_dim, context_length, num_frame, frame_size, \n        BLOCK_SIZE\n    )\n\n\ndef ref_sparse_head_placement(query, key, value, best_mask_idx, context_length, num_frame, frame_size):\n    cfg, num_heads, seq_len, head_dim = query.shape\n    assert seq_len == context_length + num_frame * frame_size\n\n    query_out = query.clone()\n    key_out = key.clone()\n    value_out = value.clone()\n\n    # Spatial\n    query_out[best_mask_idx == 0], key_out[best_mask_idx == 0], value_out[best_mask_idx == 0] = \\\n        query[best_mask_idx == 0], key[best_mask_idx == 0], value[best_mask_idx == 0]\n\n    # Temporal\n    query_out[best_mask_idx == 1], key_out[best_mask_idx == 1], value_out[best_mask_idx == 1] = \\\n            token_reorder_to_token_major(query[best_mask_idx == 1].unsqueeze(0), context_length, num_frame * frame_size, num_frame, frame_size).squeeze(0), \\\n            token_reorder_to_token_major(key[best_mask_idx == 1].unsqueeze(0), context_length, num_frame * frame_size, num_frame, frame_size).squeeze(0), \\\n            token_reorder_to_token_major(value[best_mask_idx == 1].unsqueeze(0), context_length, num_frame * frame_size, num_frame, frame_size).squeeze(0)\n\n    return query_out, key_out, value_out\n\n\ndef test_sparse_head_placement():\n\n    context_length = 226\n    num_frame = 11\n    frame_size = 4080\n\n    cfg = 2\n    num_heads = 48\n\n    seq_len = context_length + num_frame * frame_size\n    head_dim = 64\n\n    dtype = torch.bfloat16\n    device = torch.device(\"cuda\")\n\n    query = torch.randn(cfg, num_heads, seq_len, head_dim, dtype=dtype, device=device)\n    key = torch.randn(cfg, num_heads, seq_len, head_dim, dtype=dtype, device=device)\n    value = torch.randn(cfg, num_heads, seq_len, head_dim, dtype=dtype, device=device)\n\n    best_mask_idx = torch.randint(0, 2, (cfg, num_heads), device=device)\n\n    query_out = torch.empty_like(query)\n    key_out = torch.empty_like(key)\n    value_out = torch.empty_like(value)\n\n    sparse_head_placement(query, key, value, query_out, key_out, value_out, best_mask_idx, context_length, num_frame, frame_size)\n    ref_query_out, ref_key_out, ref_value_out = ref_sparse_head_placement(query, key, value, best_mask_idx, context_length, num_frame, frame_size)\n\n    torch.testing.assert_close(query_out, ref_query_out)\n    torch.testing.assert_close(key_out, ref_key_out)\n    torch.testing.assert_close(value_out, ref_value_out)\n\n\ndef benchmark_sparse_head_placement():\n    import time\n\n    context_length = 226\n    num_frame = 11\n    frame_size = 4080\n\n    cfg = 2\n    num_heads = 48\n\n    seq_len = context_length + num_frame * frame_size\n    head_dim = 64\n\n    dtype = torch.bfloat16\n    device = torch.device(\"cuda\")\n\n    query = torch.randn(cfg, num_heads, seq_len, head_dim, dtype=dtype, device=device)\n    key = torch.randn(cfg, num_heads, seq_len, head_dim, dtype=dtype, device=device)\n    value = torch.randn(cfg, num_heads, seq_len, head_dim, dtype=dtype, device=device)\n    best_mask_idx = torch.randint(0, 2, (cfg, num_heads), device=device)\n\n    query_out = torch.empty_like(query)\n    key_out = torch.empty_like(key)\n    value_out = torch.empty_like(value)\n\n    warmup = 10\n    all_iter = 1000\n\n    # warmup\n    for _ in range(warmup):\n        sparse_head_placement(query, key, value, query_out, key_out, value_out, best_mask_idx, context_length, num_frame, frame_size)\n\n    torch.cuda.synchronize()\n    start = time.time()\n    for _ in range(all_iter):\n        sparse_head_placement(query, key, value, query_out, key_out, value_out, best_mask_idx, context_length, num_frame, frame_size)\n    torch.cuda.synchronize()\n    end = time.time()\n\n    print(f\"Triton Elapsed Time: {(end - start) / all_iter * 1e3:.2f} ms\")\n    print(f\"Triton Total Bandwidth: {query.nelement() * query.element_size() * 3 * 2 * all_iter / (end - start) / 1e9:.2f} GB/s\")\n\n    torch.cuda.synchronize()\n    start = time.time()\n    for _ in range(all_iter):\n        ref_sparse_head_placement(query, key, value, best_mask_idx, context_length, num_frame, frame_size)\n    torch.cuda.synchronize()\n    end = time.time()\n\n    print(f\"Reference Elapsed Time: {(end - start) / all_iter * 1e3:.2f} ms\")\n    print(f\"Reference Total Bandwidth: {query.nelement() * query.element_size() * 3 * 2 * all_iter / (end - start) / 1e9:.2f} GB/s\")\n\n\n@triton.jit\ndef hidden_states_placement_kernel(\n    hidden_states_ptr, # [cfg, num_heads, seq_len, head_dim] seq_len = context_length + num_frame * frame_size\n    hidden_states_out_ptr, # [cfg, num_heads, seq_len, head_dim]\n    best_mask_idx_ptr, # [cfg, num_heads]\n    hidden_states_stride_b, hidden_states_stride_h, hidden_states_stride_s, hidden_states_stride_d,\n    mask_idx_stride_b, mask_idx_stride_h,\n    seq_len: tl.constexpr,\n    head_dim: tl.constexpr,\n    context_length: tl.constexpr,   \n    num_frame: tl.constexpr,        \n    frame_size: tl.constexpr,      \n    BLOCK_SIZE: tl.constexpr\n):\n    # Copy hidden_states to output\n    # range: [b, h, block_id * block_size: block_id * block_size + block_size, :]\n    cfg = tl.program_id(0)\n    head = tl.program_id(1)\n    block_id = tl.program_id(2)\n\n    start_id = block_id * BLOCK_SIZE\n    end_id = start_id + BLOCK_SIZE\n    end_id = tl.where(end_id > seq_len, seq_len, end_id) \n\n    # Load best mask idx (0 is spatial, 1 is temporal)\n    is_temporal = tl.load(best_mask_idx_ptr + cfg * mask_idx_stride_b + head * mask_idx_stride_h)\n    \n    offset_token = tl.arange(0, BLOCK_SIZE) + start_id\n    offset_mask = offset_token < seq_len\n    offset_d = tl.arange(0, head_dim)\n\n    if is_temporal:\n        # if offset_token < context_length:\n        #   offset_store_token = offset_token \n        # else:\n        #   patch_id = (offset_token - context_length) // num_frame\n        #   frame_id = (offset_token - context_length) - patch_id * num_frame\n        #   offset_store_token = context_length + frame_id * frame_size + patch_id\n        patch_id = (offset_token - context_length) // num_frame\n        frame_id = (offset_token - context_length) - patch_id * num_frame\n        offset_store_token = tl.where(offset_token < context_length, offset_token, context_length + frame_id * frame_size + patch_id)\n\n        offset_load = (cfg * hidden_states_stride_b + head * hidden_states_stride_h + offset_token[:,None] * hidden_states_stride_s) + offset_d[None,:] * hidden_states_stride_d\n        offset_hidden_states = hidden_states_ptr + offset_load\n\n        offset_store = (cfg * hidden_states_stride_b + head * hidden_states_stride_h + offset_store_token[:,None] * hidden_states_stride_s) + offset_d[None,:] * hidden_states_stride_d\n        offset_hidden_states_out = hidden_states_out_ptr + offset_store\n\n        # Maybe tune the pipeline here\n        hidden_states = tl.load(offset_hidden_states, mask=offset_mask[:,None])\n        tl.store(offset_hidden_states_out, hidden_states, mask=offset_mask[:,None])\n    else:\n        offset_load = (cfg * hidden_states_stride_b + head * hidden_states_stride_h + offset_token[:,None] * hidden_states_stride_s) + offset_d[None,:] * hidden_states_stride_d\n        offset_hidden_states = hidden_states_ptr + offset_load\n\n        offset_store = offset_load\n        offset_hidden_states_out = hidden_states_out_ptr + offset_store\n\n        # Maybe tune the pipeline here\n        hidden_states = tl.load(offset_hidden_states, mask=offset_mask[:,None])\n        tl.store(offset_hidden_states_out, hidden_states, mask=offset_mask[:,None])\n\n\ndef hidden_states_placement(hidden_states, hidden_states_out, best_mask_idx, context_length, num_frame, frame_size):\n    cfg, num_heads, seq_len, head_dim = hidden_states.shape\n    BLOCK_SIZE = 128\n    assert seq_len == context_length + num_frame * frame_size\n\n    grid = (cfg, num_heads, (seq_len + BLOCK_SIZE - 1) // BLOCK_SIZE)\n\n\n    hidden_states_placement_kernel[grid](\n        hidden_states, \n        hidden_states_out, \n        best_mask_idx,\n        hidden_states.stride(0), hidden_states.stride(1), hidden_states.stride(2), hidden_states.stride(3),\n        best_mask_idx.stride(0), best_mask_idx.stride(1),\n        seq_len, head_dim, context_length, num_frame, frame_size, \n        BLOCK_SIZE\n    )\n\n    return hidden_states_out\n\ndef ref_hidden_states_placement(hidden_states, output_hidden_states, best_mask_idx, context_length, num_frame, frame_size):\n    cfg, num_heads, seq_len, head_dim = hidden_states.shape\n    assert seq_len == context_length + num_frame * frame_size\n\n    # Spatial\n    output_hidden_states[best_mask_idx == 0] = hidden_states[best_mask_idx == 0]\n    # Temporal\n    output_hidden_states[best_mask_idx == 1] = token_reorder_to_frame_major(hidden_states[best_mask_idx == 1].unsqueeze(0), context_length, num_frame * frame_size, num_frame, frame_size).squeeze(0)\n\ndef test_hidden_states_placement():\n\n    context_length = 226\n    num_frame = 11\n    frame_size = 4080\n\n    cfg = 2\n    num_heads = 48\n\n    seq_len = context_length + num_frame * frame_size\n    head_dim = 64\n\n    dtype = torch.bfloat16\n    device = torch.device(\"cuda\")\n\n    hidden_states = torch.randn(cfg, num_heads, seq_len, head_dim, dtype=dtype, device=device)\n    best_mask_idx = torch.randint(0, 2, (cfg, num_heads), device=device)\n\n    hidden_states_out1 = torch.empty_like(hidden_states)\n    hidden_states_out2 = torch.empty_like(hidden_states)\n\n    hidden_states_placement(hidden_states, hidden_states_out1, best_mask_idx, context_length, num_frame, frame_size)\n    ref_hidden_states_placement(hidden_states, hidden_states_out2, best_mask_idx, context_length, num_frame, frame_size)\n\n    torch.testing.assert_close(hidden_states_out1, hidden_states_out2)\n\ndef benchmark_hidden_states_placement():\n    import time\n\n    context_length = 226\n    num_frame = 11\n    frame_size = 4080\n\n    cfg = 2\n    num_heads = 48\n\n    seq_len = context_length + num_frame * frame_size\n    head_dim = 64\n\n    dtype = torch.bfloat16\n    device = torch.device(\"cuda\")\n\n    hidden_states = torch.randn(cfg, num_heads, seq_len, head_dim, dtype=dtype, device=device)\n    best_mask_idx = torch.randint(0, 2, (cfg, num_heads), device=device)\n\n    hidden_states_out = torch.empty_like(hidden_states)\n\n    warmup = 10\n    all_iter = 1000\n\n    # warmup\n    for _ in range(warmup):\n        hidden_states_placement(hidden_states, hidden_states_out, best_mask_idx, context_length, num_frame, frame_size)\n\n    torch.cuda.synchronize()\n    start = time.time()\n    for _ in range(all_iter):\n        hidden_states_placement(hidden_states, hidden_states_out, best_mask_idx, context_length, num_frame, frame_size)\n    torch.cuda.synchronize()\n    end = time.time()\n\n    print(f\"Triton Elapsed Time: {(end - start) / all_iter * 1e3:.2f} ms\")\n    print(f\"Triton Total Bandwidth: {hidden_states.nelement() * hidden_states.element_size() * 2 * all_iter / (end - start) / 1e9:.2f} GB/s\")\n\n    torch.cuda.synchronize()\n    start = time.time()\n    for _ in range(all_iter):\n        ref_hidden_states_placement(hidden_states, hidden_states.clone(), best_mask_idx, context_length, num_frame, frame_size)\n    torch.cuda.synchronize()\n    end = time.time()\n\n    print(f\"Reference Elapsed Time: {(end - start) / all_iter * 1e3:.2f} ms\")\n    print(f\"Reference Total Bandwidth: {hidden_states.nelement() * hidden_states.element_size() * 2 * all_iter / (end - start) / 1e9:.2f} GB/s\")\n\n\nif __name__ == \"__main__\":\n    test_sparse_head_placement()\n    benchmark_sparse_head_placement()\n    test_hidden_states_placement()\n    benchmark_hidden_states_placement()\n"}
{"type": "source_file", "path": "svg/models/hyvideo/__init__.py", "content": ""}
{"type": "source_file", "path": "svg/models/hyvideo/config.py", "content": "import argparse\nfrom .constants import *\nimport re\nfrom .modules.models import HUNYUAN_VIDEO_CONFIG\n\n\ndef parse_args(namespace=None):\n    parser = argparse.ArgumentParser(description=\"HunyuanVideo inference script\")\n\n    parser = add_network_args(parser)\n    parser = add_extra_models_args(parser)\n    parser = add_denoise_schedule_args(parser)\n    parser = add_inference_args(parser)\n    parser = add_parallel_args(parser)\n    \n    parser = add_sparsity_args(parser)\n\n    args = parser.parse_args(namespace=namespace)\n    args = sanity_check_args(args)\n\n    return args\n\n\ndef add_network_args(parser: argparse.ArgumentParser):\n    group = parser.add_argument_group(title=\"HunyuanVideo network args\")\n\n    # Main model\n    group.add_argument(\n        \"--model\",\n        type=str,\n        choices=list(HUNYUAN_VIDEO_CONFIG.keys()),\n        default=\"HYVideo-T/2-cfgdistill\",\n    )\n    group.add_argument(\n        \"--latent-channels\",\n        type=str,\n        default=16,\n        help=\"Number of latent channels of DiT. If None, it will be determined by `vae`. If provided, \"\n        \"it still needs to match the latent channels of the VAE model.\",\n    )\n    group.add_argument(\n        \"--precision\",\n        type=str,\n        default=\"bf16\",\n        choices=PRECISIONS,\n        help=\"Precision mode. Options: fp32, fp16, bf16. Applied to the backbone model and optimizer.\",\n    )\n\n    # RoPE\n    group.add_argument(\n        \"--rope-theta\", type=int, default=256, help=\"Theta used in RoPE.\"\n    )\n    return parser\n\n\ndef add_extra_models_args(parser: argparse.ArgumentParser):\n    group = parser.add_argument_group(\n        title=\"Extra models args, including vae, text encoders and tokenizers)\"\n    )\n\n    # - VAE\n    group.add_argument(\n        \"--vae\",\n        type=str,\n        default=\"884-16c-hy\",\n        choices=list(VAE_PATH),\n        help=\"Name of the VAE model.\",\n    )\n    group.add_argument(\n        \"--vae-precision\",\n        type=str,\n        default=\"fp16\",\n        choices=PRECISIONS,\n        help=\"Precision mode for the VAE model.\",\n    )\n    group.add_argument(\n        \"--vae-tiling\",\n        action=\"store_true\",\n        help=\"Enable tiling for the VAE model to save GPU memory.\",\n    )\n    group.set_defaults(vae_tiling=True)\n\n    group.add_argument(\n        \"--text-encoder\",\n        type=str,\n        default=\"llm\",\n        choices=list(TEXT_ENCODER_PATH),\n        help=\"Name of the text encoder model.\",\n    )\n    group.add_argument(\n        \"--text-encoder-precision\",\n        type=str,\n        default=\"fp16\",\n        choices=PRECISIONS,\n        help=\"Precision mode for the text encoder model.\",\n    )\n    group.add_argument(\n        \"--text-states-dim\",\n        type=int,\n        default=4096,\n        help=\"Dimension of the text encoder hidden states.\",\n    )\n    group.add_argument(\n        \"--text-len\", type=int, default=256, help=\"Maximum length of the text input.\"\n    )\n    group.add_argument(\n        \"--tokenizer\",\n        type=str,\n        default=\"llm\",\n        choices=list(TOKENIZER_PATH),\n        help=\"Name of the tokenizer model.\",\n    )\n    group.add_argument(\n        \"--prompt-template\",\n        type=str,\n        default=\"dit-llm-encode\",\n        choices=PROMPT_TEMPLATE,\n        help=\"Image prompt template for the decoder-only text encoder model.\",\n    )\n    group.add_argument(\n        \"--prompt-template-video\",\n        type=str,\n        default=\"dit-llm-encode-video\",\n        choices=PROMPT_TEMPLATE,\n        help=\"Video prompt template for the decoder-only text encoder model.\",\n    )\n    group.add_argument(\n        \"--hidden-state-skip-layer\",\n        type=int,\n        default=2,\n        help=\"Skip layer for hidden states.\",\n    )\n    group.add_argument(\n        \"--apply-final-norm\",\n        action=\"store_true\",\n        help=\"Apply final normalization to the used text encoder hidden states.\",\n    )\n\n    # - CLIP\n    group.add_argument(\n        \"--text-encoder-2\",\n        type=str,\n        default=\"clipL\",\n        choices=list(TEXT_ENCODER_PATH),\n        help=\"Name of the second text encoder model.\",\n    )\n    group.add_argument(\n        \"--text-encoder-precision-2\",\n        type=str,\n        default=\"fp16\",\n        choices=PRECISIONS,\n        help=\"Precision mode for the second text encoder model.\",\n    )\n    group.add_argument(\n        \"--text-states-dim-2\",\n        type=int,\n        default=768,\n        help=\"Dimension of the second text encoder hidden states.\",\n    )\n    group.add_argument(\n        \"--tokenizer-2\",\n        type=str,\n        default=\"clipL\",\n        choices=list(TOKENIZER_PATH),\n        help=\"Name of the second tokenizer model.\",\n    )\n    group.add_argument(\n        \"--text-len-2\",\n        type=int,\n        default=77,\n        help=\"Maximum length of the second text input.\",\n    )\n\n    return parser\n\n\ndef add_denoise_schedule_args(parser: argparse.ArgumentParser):\n    group = parser.add_argument_group(title=\"Denoise schedule args\")\n\n    group.add_argument(\n        \"--denoise-type\",\n        type=str,\n        default=\"flow\",\n        help=\"Denoise type for noised inputs.\",\n    )\n\n    # Flow Matching\n    group.add_argument(\n        \"--flow-shift\",\n        type=float,\n        default=7.0,\n        help=\"Shift factor for flow matching schedulers.\",\n    )\n    group.add_argument(\n        \"--flow-reverse\",\n        action=\"store_true\",\n        help=\"If reverse, learning/sampling from t=1 -> t=0.\",\n    )\n    group.add_argument(\n        \"--flow-solver\",\n        type=str,\n        default=\"euler\",\n        help=\"Solver for flow matching.\",\n    )\n    group.add_argument(\n        \"--use-linear-quadratic-schedule\",\n        action=\"store_true\",\n        help=\"Use linear quadratic schedule for flow matching.\"\n        \"Following MovieGen (https://ai.meta.com/static-resource/movie-gen-research-paper)\",\n    )\n    group.add_argument(\n        \"--linear-schedule-end\",\n        type=int,\n        default=25,\n        help=\"End step for linear quadratic schedule for flow matching.\",\n    )\n\n    return parser\n\n\ndef add_inference_args(parser: argparse.ArgumentParser):\n    group = parser.add_argument_group(title=\"Inference args\")\n\n    # ======================== Model loads ========================\n    group.add_argument(\n        \"--model-base\",\n        type=str,\n        default=\"ckpts\",\n        help=\"Root path of all the models, including t2v models and extra models.\",\n    )\n    group.add_argument(\n        \"--dit-weight\",\n        type=str,\n        default=\"ckpts/hunyuan-video-t2v-720p/transformers/mp_rank_00_model_states.pt\",\n        help=\"Path to the HunyuanVideo model. If None, search the model in the args.model_root.\"\n        \"1. If it is a file, load the model directly.\"\n        \"2. If it is a directory, search the model in the directory. Support two types of models: \"\n        \"1) named `pytorch_model_*.pt`\"\n        \"2) named `*_model_states.pt`, where * can be `mp_rank_00`.\",\n    )\n    group.add_argument(\n        \"--model-resolution\",\n        type=str,\n        default=\"540p\",\n        choices=[\"540p\", \"720p\"],\n        help=\"Root path of all the models, including t2v models and extra models.\",\n    )\n    group.add_argument(\n        \"--load-key\",\n        type=str,\n        default=\"module\",\n        help=\"Key to load the model states. 'module' for the main model, 'ema' for the EMA model.\",\n    )\n    group.add_argument(\n        \"--use-cpu-offload\",\n        action=\"store_true\",\n        help=\"Use CPU offload for the model load.\",\n    )\n\n    # ======================== Inference general setting ========================\n    group.add_argument(\n        \"--batch-size\",\n        type=int,\n        default=1,\n        help=\"Batch size for inference and evaluation.\",\n    )\n    group.add_argument(\n        \"--infer-steps\",\n        type=int,\n        default=50,\n        help=\"Number of denoising steps for inference.\",\n    )\n    group.add_argument(\n        \"--disable-autocast\",\n        action=\"store_true\",\n        help=\"Disable autocast for denoising loop and vae decoding in pipeline sampling.\",\n    )\n    group.add_argument(\n        \"--save-path\",\n        type=str,\n        default=\"./results\",\n        help=\"Path to save the generated samples.\",\n    )\n    group.add_argument(\n        \"--save-path-suffix\",\n        type=str,\n        default=\"\",\n        help=\"Suffix for the directory of saved samples.\",\n    )\n    group.add_argument(\n        \"--name-suffix\",\n        type=str,\n        default=\"\",\n        help=\"Suffix for the names of saved samples.\",\n    )\n    group.add_argument(\n        \"--num-videos\",\n        type=int,\n        default=1,\n        help=\"Number of videos to generate for each prompt.\",\n    )\n    # ---sample size---\n    group.add_argument(\n        \"--video-size\",\n        type=int,\n        nargs=\"+\",\n        default=(720, 1280),\n        help=\"Video size for training. If a single value is provided, it will be used for both height \"\n        \"and width. If two values are provided, they will be used for height and width \"\n        \"respectively.\",\n    )\n    group.add_argument(\n        \"--video-length\",\n        type=int,\n        default=129,\n        help=\"How many frames to sample from a video. if using 3d vae, the number should be 4n+1\",\n    )\n    # --- prompt ---\n    group.add_argument(\n        \"--prompt\",\n        type=str,\n        default=None,\n        help=\"Prompt for sampling during evaluation.\",\n    )\n    group.add_argument(\n        \"--seed-type\",\n        type=str,\n        default=\"auto\",\n        choices=[\"file\", \"random\", \"fixed\", \"auto\"],\n        help=\"Seed type for evaluation. If file, use the seed from the CSV file. If random, generate a \"\n        \"random seed. If fixed, use the fixed seed given by `--seed`. If auto, `csv` will use the \"\n        \"seed column if available, otherwise use the fixed `seed` value. `prompt` will use the \"\n        \"fixed `seed` value.\",\n    )\n    group.add_argument(\"--seed\", type=int, default=None, help=\"Seed for evaluation.\")\n\n    # Classifier-Free Guidance\n    group.add_argument(\n        \"--neg-prompt\", type=str, default=None, help=\"Negative prompt for sampling.\"\n    )\n    group.add_argument(\n        \"--cfg-scale\", type=float, default=1.0, help=\"Classifier free guidance scale.\"\n    )\n    group.add_argument(\n        \"--embedded-cfg-scale\",\n        type=float,\n        default=6.0,\n        help=\"Embeded classifier free guidance scale.\",\n    )\n\n    group.add_argument(\n        \"--use-fp8\",\n        action=\"store_true\",\n        help=\"Enable use fp8 for inference acceleration.\"\n    )\n\n    group.add_argument(\n        \"--reproduce\",\n        action=\"store_true\",\n        help=\"Enable reproducibility by setting random seeds and deterministic algorithms.\",\n    )\n\n    return parser\n\n\ndef add_parallel_args(parser: argparse.ArgumentParser):\n    group = parser.add_argument_group(title=\"Parallel args\")\n\n    # ======================== Model loads ========================\n    group.add_argument(\n        \"--ulysses-degree\",\n        type=int,\n        default=1,\n        help=\"Ulysses degree.\",\n    )\n    group.add_argument(\n        \"--ring-degree\",\n        type=int,\n        default=1,\n        help=\"Ulysses degree.\",\n    )\n\n    return parser\n\n\ndef add_sparsity_args(parser: argparse.ArgumentParser):\n    group = parser.add_argument_group(title=\"Sparsity\")\n    \n    # ======================== Model loads ========================\n    group.add_argument(\n        \"--save-debug-tensor\",\n        type=str,\n        default=None,\n        help=\"Save tensor for us to debug.\",\n    )\n    group.add_argument(\n        \"--output_path\", \n        type=str, \n        required=True, \n        help=\"Save the output video\"\n    )\n    group.add_argument(\n        \"--first_layers_fp\", \n        type=float, \n        default=0.0, \n        help=\"Only works for best config. Leave the 0, 1, 2, 40, 41 layers in FP\"\n    )\n    group.add_argument(\n        \"--first_times_fp\", \n        type=float, \n        default=0.0, \n        help=\"Only works for best config. Leave the first 10% timestep in FP\"\n    )\n    group.add_argument(\n        \"--record_attention\", \n        action=\"store_true\", \n        help=\"Record the attention\"\n    )\n    group.add_argument(\n        \"--pattern\", \n        type=str, \n        required=True, \n        choices=[\"dense\", \"SVG\"], \n        help=\"Sparse Pattern\"\n    )\n    group.add_argument(\n        \"--num_sampled_rows\", \n        type=int, \n        default=32, \n        help=\"The number of sampled rows\"\n    )\n    group.add_argument(\n        \"--sample_mse_max_row\", \n        type=int, \n        default=10000, \n        help=\"Since some attention masks are really large, need to restrict the maximum size (the row we are going to sample on).\"\n    )\n    parser.add_argument(\n        \"--sparsity\",\n        type=float,\n        default=1.0,\n        help=\"The sparsity of the striped attention pattern. Accepts one or two float values. Only effective for fast_sample_mse\"\n    )\n    group.add_argument(\n        \"--quantize_type\", \n        type=str, \n        default=None, \n        choices=[\n            \"QK-INT8-PV-FP16\", \"QK-PerHead-INT8-PV-FP16\", \"QK-Hadamard-INT8-PV-FP16\", \n            \"QK-SAGE-INT8-PV-FP16\", \"QK-PerHead-SAGE-INT8-PV-FP16\", \"QK-SAGE-Hadamard-INT8-PV-FP16\",\n            \"QK-SAGE2-INT8-PV-FP16\", \"QK-SAGE2-Hadamard-INT8-PV-FP16\",\n            \"QK-INT8-PV-FP8\", \"QK-PerHead-INT8-PV-FP8\",\n            \"QK-SAGE-INT8-PV-FP8\", \"QK-PerHead-SAGE-INT8-PV-FP8\",\n            \"QK-SAGE-Hadamard-INT8-PV-FP8\",\n            \"QK-SAGE2-INT8-PV-SAGE2-FP8\", \"QK-SAGE2-INT8-PV-INT8\", \n            \"QK-SAGE2-INT8-PV-SAGE2-INT8\", \"QK-SAGE2-INT8-PV-FP8\", \n            \"QK-FP8-PV-FP16\", \"QK-PerHead-FP8-PV-FP16\",\n            \"QK-FP8-PV-FP8\", \"QK-SAGE2-FP8-PV-FP8\", \"QK-SAGE-INT8-PV-FP8\"\n        ], \n        help=\"Quantization type\"\n    )\n    group.add_argument(\n        \"--quantize_first_layers_fp\", \n        type=float, \n        default=0.0, \n        help=\"Only works for best config. Leave the 0, 1, 2, 40, 41 layers in FP\"\n    )\n    group.add_argument(\n        \"--quantize_first_times_fp\", \n        type=float, \n        default=0.0, \n        help=\"Only works for best config. Leave the 0, 1, 2, 40, 41 layers in FP\"\n    )\n    return parser\n\n\ndef sanity_check_args(args):\n    # VAE channels\n    vae_pattern = r\"\\d{2,3}-\\d{1,2}c-\\w+\"\n    if not re.match(vae_pattern, args.vae):\n        raise ValueError(\n            f\"Invalid VAE model: {args.vae}. Must be in the format of '{vae_pattern}'.\"\n        )\n    vae_channels = int(args.vae.split(\"-\")[1][:-1])\n    if args.latent_channels is None:\n        args.latent_channels = vae_channels\n    if vae_channels != args.latent_channels:\n        raise ValueError(\n            f\"Latent channels ({args.latent_channels}) must match the VAE channels ({vae_channels}).\"\n        )\n    return args\n"}
{"type": "source_file", "path": "svg/models/cog/attention.py", "content": "import sys\nfrom typing import Optional\n\nimport torch\nimport torch.nn.functional as F\nfrom diffusers.models.attention_processor import Attention\nfrom diffusers.models.embeddings import apply_rotary_emb\nfrom torch.nn.attention.flex_attention import (\n    flex_attention,\n)\n\nfrom .placement import sparse_head_placement, hidden_states_placement, ref_sparse_head_placement, ref_hidden_states_placement\nfrom .utils import generate_temporal_head_mask_mod, create_block_mask_cached\n\ntry:\n    sys.path.append('svg/kernels/build/')\n    import _kernels\n\n    def qk_norm(attn, query, key):\n        if attn.norm_q is not None:\n            _kernels.layer_norm_forward(query.view(-1, query.shape[-1]), attn.norm_q.weight, attn.norm_q.bias)\n        if attn.norm_k is not None:\n            _kernels.layer_norm_forward(key.view(-1, key.shape[-1]), attn.norm_k.weight, attn.norm_k.bias)\n        return query, key\n\n    def rotary_emb(image_rotary_emb, query, key, text_seq_length):\n        cos, sin = image_rotary_emb\n        _kernels.apply_qk_rope_inplace_cossin(query, key, cos, sin, text_seq_length)\n        return query, key\n\nexcept ImportError:\n    import warnings\n    warnings.warn(\"Could not import RoPE / Norm kernels! Falling back to PyTorch implementation.\")\n\n    def qk_norm(attn, query, key):\n        if attn.norm_q is not None:\n            query = attn.norm_q(query)\n        if attn.norm_k is not None:\n            key = attn.norm_k(key)\n        return query, key\n\n    def rotary_emb(image_rotary_emb, query, key, text_seq_length):\n        query[:, :, text_seq_length:] = apply_rotary_emb(query[:, :, text_seq_length:], image_rotary_emb)\n        key[:, :, text_seq_length:] = apply_rotary_emb(key[:, :, text_seq_length:], image_rotary_emb)\n        return query, key\n\n\n\nflex_attention = torch.compile(flex_attention, dynamic=False, mode=\"max-autotune-no-cudagraphs\")\ntorch._dynamo.config.cache_size_limit = 192 * 3\ntorch._dynamo.config.accumulated_cache_size_limit = 192 * 3\n\n\n# Use this class to save attention qkv\nclass CogVideoX_SparseAttn_Processor2_0:\n    r\"\"\"\n    Processor for implementing scaled dot-product attention for the CogVideoX model. It applies a rotary embedding on\n    query and key vectors, but does not include spatial normalization.\n    \"\"\"\n    version = None\n    context_length = 0\n    num_frame = 0\n    frame_size = 0\n\n    first_layers_fp = 0\n    first_times_fp = 0\n\n    num_sampled_rows = 32\n    attention_masks = None\n    block_mask = None\n    \n    def __init__(self, layer_idx):\n        self.layer_idx = layer_idx\n        if not hasattr(F, \"scaled_dot_product_attention\"):\n            raise ImportError(\"CogVideoXAttnProcessor requires PyTorch 2.0, to use it, please upgrade PyTorch to 2.0.\")\n\n    def get_qkv(self, attn, hidden_states):\n        query = attn.to_q(hidden_states)\n        key = attn.to_k(hidden_states)\n        value = attn.to_v(hidden_states)\n        return query, key, value\n\n    def process_before_linear(self, attn, hidden_states, encoder_hidden_states):\n        hidden_states = torch.cat([encoder_hidden_states, hidden_states], dim=1)\n        batch_size, sequence_length, _ = (\n            hidden_states.shape if encoder_hidden_states is None else encoder_hidden_states.shape\n        )\n        return hidden_states, batch_size, sequence_length\n\n    def transpose_qkv(self, attn, query, key, value, batch_size):\n        inner_dim = key.shape[-1]\n        head_dim = inner_dim // attn.heads\n\n        query = query.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2).contiguous()\n        key = key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2).contiguous()\n        value = value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2).contiguous()\n\n        return query, key, value, head_dim\n\n    def get_o(self, attn, hidden_states, batch_size, head_dim):\n        hidden_states = hidden_states.transpose(1, 2).reshape(batch_size, -1, attn.heads * head_dim)\n        # linear proj\n        hidden_states = attn.to_out[0](hidden_states)\n        # dropout\n        hidden_states = attn.to_out[1](hidden_states)\n        return hidden_states\n\n    def split_hidden_states(self, hidden_states, text_seq_length):\n        encoder_hidden_states, hidden_states = hidden_states.split(\n            [text_seq_length, hidden_states.size(1) - text_seq_length], dim=1\n        )\n        return encoder_hidden_states, hidden_states\n\n    def flash_attention(self, query, key, value):\n        output_hidden_states = F.scaled_dot_product_attention(\n                query, key, value, dropout_p=0.0, is_causal=False\n            )\n        return output_hidden_states\n\n    def sample_mse(self, query, key, value):\n\n        assert len(self.attention_masks) == 2\n\n        cfg, num_heads, seq_len, dim = query.size()\n        num_sampled_rows = min(self.num_sampled_rows, seq_len)\n        sampled_rows = torch.randint(low=0, high=seq_len, size=(num_sampled_rows,))\n        sampled_q = query[:, :, sampled_rows, :]\n        sampled_qk_scores = torch.matmul(sampled_q, key.transpose(-2, -1)) / (dim**0.5)\n        \n           \n        sampled_attn_weights = F.softmax(sampled_qk_scores, dim=-1)\n        sampled_golden_hidden_states = torch.matmul(sampled_attn_weights, value)  # (1, seq_len, dim)\n\n        sampled_mses = torch.zeros(len(self.attention_masks), cfg, num_heads, device=query.device, dtype=query.dtype)\n     \n        # Only have Tri-diagonal and Striped\n\n        for mask_idx, attn_mask in enumerate(self.attention_masks):\n            sampled_attention_mask = attn_mask[sampled_rows, :]\n            sampled_attention_scores = sampled_qk_scores.masked_fill(sampled_attention_mask == 0, float('-inf'))\n            sampled_attn_weights = F.softmax(sampled_attention_scores, dim=-1)\n            sampled_hidden_states = torch.matmul(sampled_attn_weights, value)\n            mse = torch.mean((sampled_hidden_states - sampled_golden_hidden_states) ** 2, dim=(2, 3))\n            sampled_mses[mask_idx] = mse\n\n        return sampled_mses\n\n    def sparse_flex_attention(self, query, key, value, block_mask):\n        return flex_attention(query, key, value, block_mask=block_mask)\n    \n    def sparse_head_placement(self, query, key, value, query_out, key_out, value_out, best_mask_idx, context_length, num_frame, frame_size):\n        query_out, key_out, value_out = ref_sparse_head_placement(query, key, value, best_mask_idx, context_length, num_frame, frame_size)\n        return query_out, key_out, value_out\n\n    def fast_sparse_head_placement(self, query, key, value, query_out, key_out, value_out, best_mask_idx, context_length, num_frame, frame_size):\n        sparse_head_placement(query, key, value, query_out, key_out, value_out, best_mask_idx, context_length, num_frame, frame_size)\n        return query_out, key_out, value_out\n\n\n    def hidden_states_placement(self, \\\n        hidden_states, output_hidden_states, \\\n        best_mask_idx, context_length, num_frame, frame_size\n    ):\n        ref_hidden_states_placement(hidden_states, output_hidden_states, best_mask_idx, context_length, num_frame, frame_size)\n\n\n    def fast_hidden_states_placement(self, \\\n        hidden_states, output_hidden_states, \\\n        best_mask_idx, context_length, num_frame, frame_size\n    ):\n        hidden_states_placement(hidden_states, output_hidden_states, best_mask_idx, context_length, num_frame, frame_size)\n\n\n    def attention_core_logic(\n        self,\n        query: torch.Tensor,\n        key: torch.Tensor,\n        value: torch.Tensor,\n        timestep\n    ):\n        cfg, num_heads, seq_len, dim = query.size()\n        \n        context_length, num_frame, frame_size = self.context_length, self.num_frame, self.frame_size\n\n        assert seq_len == context_length + num_frame * frame_size, \\\n            f\"Query Shape: {seq_len} is not equivalent to {context_length} + {num_frame} * {frame_size}\"\n            \n        # Determine if we use Full Attention to calculate\n        full_attention_flag = False\n        if self.layer_idx < 42 * self.first_layers_fp:\n            full_attention_flag = True\n        if timestep[0] > 1000 * (1 - self.first_times_fp):\n            full_attention_flag = True\n            \n        if full_attention_flag:    \n            output_hidden_states = self.flash_attention(query, key, value)\n            return output_hidden_states.reshape(cfg, num_heads, seq_len, dim)\n        else:\n\n            sampled_mses = self.sample_mse(query, key, value)\n            best_mask_idx = torch.argmin(sampled_mses, dim=0)\n            output_hidden_states = torch.zeros_like(query)\n            query_out, key_out, value_out = torch.zeros_like(query), torch.zeros_like(key), torch.zeros_like(value)\n\n            query_out, key_out, value_out = self.fast_sparse_head_placement(query, key, value, query_out, key_out, value_out, best_mask_idx, context_length, num_frame, frame_size)\n            # query_out, key_out, value_out = self.sparse_head_placement(query, key, value, query_out, key_out, value_out, best_mask_idx, context_length, num_frame, frame_size)\n\n            hidden_states = self.sparse_flex_attention(query_out, key_out, value_out, block_mask=self.block_mask)\n\n            self.fast_hidden_states_placement(hidden_states, output_hidden_states, best_mask_idx, context_length, num_frame, frame_size)\n            # self.hidden_states_placement(hidden_states, output_hidden_states, best_mask_idx, context_length, num_frame, frame_size)\n\n            return output_hidden_states.reshape(cfg, num_heads, seq_len, dim)\n    \n    def __call__(\n        self,\n        attn: Attention,\n        hidden_states: torch.Tensor,\n        encoder_hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        image_rotary_emb: Optional[torch.Tensor] = None,\n        timestep: Optional[int] = None\n    ) -> torch.Tensor:\n        text_seq_length = encoder_hidden_states.size(1)\n\n        hidden_states, batch_size, sequence_length = self.process_before_linear(attn, hidden_states, encoder_hidden_states)\n        query, key, value = self.get_qkv(attn, hidden_states)\n        query, key, value, head_dim = self.transpose_qkv(attn, query, key, value, batch_size)\n\n        query, key = qk_norm(attn, query, key)\n\n        query, key = rotary_emb(image_rotary_emb, query, key, text_seq_length)\n        \n        # ========================================================================\n        hidden_states = self.attention_core_logic(query, key, value, timestep)\n        # ========================================================================\n\n        hidden_states = self.get_o(attn, hidden_states, batch_size, head_dim)\n        encoder_hidden_states, hidden_states = self.split_hidden_states(hidden_states, text_seq_length)\n        \n        return hidden_states, encoder_hidden_states\n    \n\ndef prepare_flexattention(cfg_size, num_head, head_dim, dtype, device, context_length, num_frame, frame_size,  diag_width=1, multiplier=2):\n    seq_len = context_length + num_frame * frame_size\n    query, key, value = [torch.zeros((1, cfg_size * num_head, seq_len, head_dim), dtype=dtype, device=device) for _ in range(3)]\n\n    # NOTE: multiplier == diag_width\n    assert diag_width == multiplier\n    mask_mod = generate_temporal_head_mask_mod(context_length, num_frame, frame_size, mul=multiplier, attn_sink=False)\n    block_mask = create_block_mask_cached(mask_mod, 1, cfg_size * num_head, seq_len, seq_len, device=device, _compile=True)\n    hidden_states = flex_attention(query, key, value, block_mask=block_mask)\n    hidden_states = flex_attention(query.view(cfg_size, num_head, seq_len, head_dim), key.view(cfg_size, num_head, seq_len, head_dim), value.view(cfg_size, num_head, seq_len, head_dim), block_mask=block_mask)\n    return block_mask"}
{"type": "source_file", "path": "svg/models/cog/utils.py", "content": "import math\nfrom math import floor\nimport os\nimport random\nfrom functools import lru_cache\n\nimport numpy as np\nimport torch\nfrom torch.nn.attention.flex_attention import (\n    create_block_mask,\n)\n\ndef seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n@lru_cache\ndef create_block_mask_cached(score_mod, B, H, M, N, device=\"cuda\", _compile=False):\n    block_mask = create_block_mask(score_mod, B, H, M, N, device=device, _compile=_compile)\n    return block_mask\n\ndef generate_temporal_head_mask_mod(prompt_length: int = 226, num_frames: int = 13, token_per_frame: int = 1350, mul: int = 2, attn_sink: bool = False):\n    \n    def round_to_multiple(idx):\n        return floor(idx / 128) * 128\n        \n    def temporal_mask_mod(b, h, q_idx, kv_idx):\n        first_row_mask = q_idx < prompt_length\n        if attn_sink:\n            first_column_mask = kv_idx < (prompt_length + token_per_frame)\n        else:\n            first_column_mask = kv_idx < prompt_length\n\n        two_frame = round_to_multiple(mul * token_per_frame)\n        temporal_head_mask = (torch.abs(q_idx - kv_idx) < two_frame)\n        return first_column_mask | first_row_mask | temporal_head_mask\n    \n    return temporal_mask_mod\n\ndef sparsity_to_width(sparsity, context_length, num_frame, frame_size):\n    seq_len = context_length + num_frame * frame_size\n    total_elements = seq_len ** 2\n    \n    sparsity = (sparsity * total_elements - 2 * seq_len * context_length) / total_elements\n      \n    width = seq_len * (1 - math.sqrt(1 - sparsity))\n    width_frame = width / frame_size\n    \n    return width_frame\n\ndef get_attention_mask(mask_name, context_length, num_frame, frame_size):\n    # TODO: Replace with real implementation\n\n    attention_mask = torch.zeros((context_length + num_frame * frame_size, context_length + num_frame * frame_size)).cuda()\n    if mask_name == \"spatial\":\n        attention_mask[:context_length, :] = 1\n        attention_mask[:, :context_length] = 1\n        block_size, block_thres = 128, frame_size * 1.5\n        num_block = math.ceil(num_frame * frame_size / block_size)\n        for i in range(num_block):\n            for j in range(num_block):\n                if abs(i - j) < block_thres // block_size:\n                    attention_mask[i * block_size : (i + 1) * block_size, j * block_size : (j + 1) * block_size] = 1\n        # attention_mask = torch.load(\"/data/home/xihaocheng/andy_develop/I2VSparse/sparseattn/v1.5/mask_tensor/mask_spatial.pt\", map_location=\"cuda\")\n    elif mask_name == \"temporal\":\n        pixel_attn_mask = torch.zeros_like(attention_mask[context_length:, context_length:])\n\n        block_size, block_thres = 128, frame_size * 1.5\n        num_block = math.ceil(num_frame * frame_size / block_size)\n        for i in range(num_block):\n            for j in range(num_block):\n                if abs(i - j) < block_thres // block_size:\n                    pixel_attn_mask[i * block_size : (i + 1) * block_size, j * block_size : (j + 1) * block_size] = 1\n\n        pixel_attn_mask = pixel_attn_mask.reshape(frame_size, num_frame, frame_size, num_frame)\\\n            .permute(1, 0, 3, 2).reshape(frame_size * num_frame, frame_size * num_frame)\n        attention_mask[context_length:, context_length:] = pixel_attn_mask\n        # attention_mask = torch.load(\"/data/home/xihaocheng/andy_develop/I2VSparse/sparseattn/v1.5/mask_tensor/mask_temporal.pt\", map_location=\"cuda\")\n    return attention_mask\n"}
{"type": "source_file", "path": "svg/models/cog/inference.py", "content": "import torch\nimport torch.nn.functional as F\n\nfrom diffusers.models.attention_processor import Attention\nfrom diffusers.utils import export_to_video, load_image\n\nfrom .attention import CogVideoX_SparseAttn_Processor2_0, prepare_flexattention\nfrom .utils import sparsity_to_width, get_attention_mask\nfrom .custom_models import replace_sparse_forward\n\n\ndef sample_image(pipe, prompt, image_path, output_path, seed, version, num_step=50):\n    print(\"\\n\" * 5)\n    print(f\"Prompt: {prompt}\")\n\n    image = load_image(image_path)\n    print(f\"Image Is Ready. Seed is {seed}\")\n\n    if version == \"v1\":\n        video = pipe(\n            image=image, prompt=prompt, guidance_scale=6, use_dynamic_cfg=True, num_inference_steps=num_step\n        ).frames[0]\n    elif version == \"v1.5\":\n        video = pipe(\n            image=image, prompt=prompt, num_videos_per_prompt=1, num_inference_steps=num_step, num_frames=81, guidance_scale=6,\n            height=768, width=1360\n        ).frames[0]\n\n    export_to_video(video, output_path, fps=8)\n\n\ndef replace_cog_attention(pipe, version, num_sampled_rows, sparsity, first_layers_fp, first_times_fp):\n\n    masks = [\"spatial\", \"temporal\"]\n\n    # For FlexAttention\n    if version == \"v1\":\n        context_length = 226\n        num_frame = 13\n        frame_size = 1350\n    elif version == \"v1.5\":\n        context_length = 226\n        num_frame = 11\n        frame_size = 4080\n    else:\n        raise ValueError(f\"Unsupported version: {version}\")\n    \n    dtype = torch.bfloat16\n\n    AttnModule = CogVideoX_SparseAttn_Processor2_0\n    AttnModule.num_sampled_rows = num_sampled_rows\n    AttnModule.attention_masks = [get_attention_mask(mask_name, context_length, num_frame, frame_size) for mask_name in masks]\n    AttnModule.version = version\n    AttnModule.first_layers_fp = first_layers_fp\n    AttnModule.first_times_fp = first_times_fp\n\n    multiplier = diag_width = sparsity_to_width(sparsity, context_length, num_frame, frame_size)\n\n    AttnModule.context_length = context_length\n    AttnModule.num_frame = num_frame\n    AttnModule.frame_size = frame_size\n    \n    # NOTE: ??? Prepare placement will strongly decrease PSNR\n    # prepare_placement(2, 48, 64, dtype, \"cuda\", context_length, num_frame, frame_size)\n    block_mask = prepare_flexattention(2, 48, 64, dtype, \"cuda\", context_length, num_frame, frame_size, diag_width, multiplier)\n    AttnModule.block_mask = block_mask\n    \n    replace_sparse_forward()\n    \n    num_layers = len(pipe.transformer.transformer_blocks)\n\n    for layer_idx, m in enumerate(pipe.transformer.transformer_blocks):\n        m.attn1.processor.layer_idx = layer_idx\n        \n    for _ , m in pipe.transformer.named_modules():\n        if isinstance(m, Attention):\n            layer_idx = m.processor.layer_idx\n            m.set_processor(AttnModule(layer_idx))\n            m.processor.num_layers = num_layers\n"}
{"type": "source_file", "path": "hyvideo_inference.py", "content": "import os\nimport time\nimport math\nimport json\nfrom pathlib import Path\nfrom loguru import logger\nfrom datetime import datetime\n\nimport torch\nfrom svg.models.hyvideo.utils.file_utils import save_videos_grid\nfrom svg.models.hyvideo.config import parse_args\nfrom svg.models.hyvideo.inference import HunyuanVideoSampler\n\n\ndef sparsity_to_width(sparsity, context_length, num_frame, frame_size):\n    seq_len = context_length + num_frame * frame_size\n    total_elements = seq_len ** 2\n    \n    sparsity = (sparsity * total_elements - 2 * seq_len * context_length) / total_elements\n      \n    width = seq_len * (1 - math.sqrt(1 - sparsity))\n    width_frame = width / frame_size\n    \n    return width_frame\n\n\nif __name__ == \"__main__\":\n    args = parse_args()\n    print(args)\n    models_root_path = Path(args.model_base)\n    if not models_root_path.exists():\n        raise ValueError(f\"`models_root` not exists: {models_root_path}\")\n\n    # Load models\n    hunyuan_video_sampler = HunyuanVideoSampler.from_pretrained(models_root_path, args=args)\n    \n    # Get the updated args\n    args = hunyuan_video_sampler.args\n\n    # Sparsity Related\n    transformer = hunyuan_video_sampler.pipeline.transformer\n    for _, block in enumerate(transformer.double_blocks):\n        block.sparse_args = args\n    for _, block in enumerate(transformer.single_blocks):\n        block.sparse_args = args\n    transformer.sparse_args = args\n\n    # We need to get the prompt len in advance, since HunyuanVideo handle the attention mask in a special way\n    prompt_mask = hunyuan_video_sampler.get_prompt_mask(\n        prompt=args.prompt, \n        height=args.video_size[0],\n        width=args.video_size[1],\n        video_length=args.video_length,\n        negative_prompt=args.neg_prompt,\n        infer_steps=args.infer_steps,\n        guidance_scale=args.cfg_scale,\n        num_videos_per_prompt=args.num_videos,\n        embedded_guidance_scale=args.embedded_cfg_scale\n    )\n    prompt_len = prompt_mask.sum()\n\n    print(f\"Memory: {torch.cuda.memory_allocated() // 1024 ** 2} / {torch.cuda.max_memory_allocated() // 1024 ** 2} MB before Inference\")\n\n    cfg_size, num_head, head_dim, dtype, device = 1, 24, 128, torch.bfloat16, \"cuda\"\n    context_length, num_frame, frame_size = 256, 33, 3600\n\n    # Calculation\n    spatial_width = temporal_width = sparsity_to_width(args.sparsity, context_length, num_frame, frame_size)\n                \n    print(f\"Spatial_width: {spatial_width}, Temporal_width: {temporal_width}. Sparsity: {args.sparsity}\")\n\n    save_path = args.output_path\n        \n    if args.pattern == \"SVG\":\n        masks = [\"spatial\", \"temporal\"]\n\n        def get_attention_mask(mask_name):\n\n            context_length = 256\n            num_frame = 33\n            frame_size = 3600\n            attention_mask = torch.zeros((context_length + num_frame * frame_size, context_length + num_frame * frame_size), device=\"cpu\")\n\n            # TODO: fix hard coded mask\n            if mask_name == \"spatial\":\n                pixel_attn_mask = torch.zeros_like(attention_mask[:-context_length, :-context_length], dtype=torch.bool, device=\"cpu\")\n                block_size, block_thres = 128, frame_size * 1.5\n                num_block = math.ceil(num_frame * frame_size / block_size)\n                for i in range(num_block):\n                    for j in range(num_block):\n                        if abs(i - j) < block_thres // block_size:\n                            pixel_attn_mask[i * block_size : (i + 1) * block_size, j * block_size : (j + 1) * block_size] = 1\n                attention_mask[:-context_length, :-context_length] = pixel_attn_mask\n\n                attention_mask[-context_length:, :] = 1\n                attention_mask[:, -context_length:] = 1\n                # attention_mask = torch.load(f\"/data/home/xihaocheng/andy_develop/tmp_data/hunyuanvideo/I2VSparse/sparseattn/v5/mask_tensor/mask_spatial.pt\", map_location=\"cpu\")\n\n            else:\n                pixel_attn_mask = torch.zeros_like(attention_mask[:-context_length, :-context_length], dtype=torch.bool, device=device)\n\n                block_size, block_thres = 128, frame_size * 1.5\n                num_block = math.ceil(num_frame * frame_size / block_size)\n                for i in range(num_block):\n                    for j in range(num_block):\n                        if abs(i - j) < block_thres // block_size:\n                            pixel_attn_mask[i * block_size : (i + 1) * block_size, j * block_size : (j + 1) * block_size] = 1\n\n                pixel_attn_mask = pixel_attn_mask.reshape(frame_size, num_frame, frame_size, num_frame).permute(1, 0, 3, 2).reshape(frame_size * num_frame, frame_size * num_frame)\n                attention_mask[:-context_length, :-context_length] = pixel_attn_mask\n\n                attention_mask[-context_length:, :] = 1\n                attention_mask[:, -context_length:] = 1\n                # attention_mask = torch.load(f\"/data/home/xihaocheng/andy_develop/tmp_data/hunyuanvideo/I2VSparse/sparseattn/v5/mask_tensor/mask_temporal.pt\", map_location=\"cpu\")\n            attention_mask = attention_mask[:args.sample_mse_max_row].cuda()\n            return attention_mask\n\n\n        from svg.models.hyvideo.modules.attenion import Hunyuan_SparseAttn, prepare_flexattention\n        from svg.models.hyvideo.modules.custom_models import replace_sparse_forward\n\n        AttnModule = Hunyuan_SparseAttn\n        AttnModule.num_sampled_rows = args.num_sampled_rows\n        AttnModule.sample_mse_max_row = args.sample_mse_max_row\n        AttnModule.attention_masks = [get_attention_mask(mask_name) for mask_name in masks]\n        AttnModule.first_layers_fp = args.first_layers_fp\n        AttnModule.first_times_fp = args.first_times_fp\n\n        block_mask = prepare_flexattention(\n                cfg_size, num_head, head_dim, dtype, device, \n                context_length, prompt_len, num_frame, frame_size, \n                diag_width=spatial_width, multiplier=temporal_width\n            )\n        AttnModule.block_mask = block_mask\n        replace_sparse_forward()\n\n\n    # Start sampling\n    # TODO: batch inference check\n    outputs = hunyuan_video_sampler.predict(\n        prompt=args.prompt, \n        height=args.video_size[0],\n        width=args.video_size[1],\n        video_length=args.video_length,\n        seed=args.seed,\n        negative_prompt=args.neg_prompt,\n        infer_steps=args.infer_steps,\n        guidance_scale=args.cfg_scale,\n        num_videos_per_prompt=args.num_videos,\n        flow_shift=args.flow_shift,\n        batch_size=args.batch_size,\n        embedded_guidance_scale=args.embedded_cfg_scale\n    )\n    samples = outputs['samples']\n    \n    # Save samples\n    if 'LOCAL_RANK' not in os.environ or int(os.environ['LOCAL_RANK']) == 0:\n        for i, sample in enumerate(samples):\n            sample = samples[i].unsqueeze(0)\n            save_videos_grid(sample, save_path, fps=24)\n            logger.info(f'Sample save to: {save_path}')\n\n"}
{"type": "source_file", "path": "svg/models/hyvideo/modules/embed_layers.py", "content": "import math\nimport torch\nimport torch.nn as nn\nfrom einops import rearrange, repeat\n\nfrom ..utils.helpers import to_2tuple\n\n\nclass PatchEmbed(nn.Module):\n    \"\"\"2D Image to Patch Embedding\n\n    Image to Patch Embedding using Conv2d\n\n    A convolution based approach to patchifying a 2D image w/ embedding projection.\n\n    Based on the impl in https://github.com/google-research/vision_transformer\n\n    Hacked together by / Copyright 2020 Ross Wightman\n\n    Remove the _assert function in forward function to be compatible with multi-resolution images.\n    \"\"\"\n\n    def __init__(\n        self,\n        patch_size=16,\n        in_chans=3,\n        embed_dim=768,\n        norm_layer=None,\n        flatten=True,\n        bias=True,\n        dtype=None,\n        device=None,\n    ):\n        factory_kwargs = {\"dtype\": dtype, \"device\": device}\n        super().__init__()\n        patch_size = to_2tuple(patch_size)\n        self.patch_size = patch_size\n        self.flatten = flatten\n\n        self.proj = nn.Conv3d(\n            in_chans,\n            embed_dim,\n            kernel_size=patch_size,\n            stride=patch_size,\n            bias=bias,\n            **factory_kwargs\n        )\n        nn.init.xavier_uniform_(self.proj.weight.view(self.proj.weight.size(0), -1))\n        if bias:\n            nn.init.zeros_(self.proj.bias)\n\n        self.norm = norm_layer(embed_dim) if norm_layer else nn.Identity()\n\n    def forward(self, x):\n        x = self.proj(x)\n        if self.flatten:\n            x = x.flatten(2).transpose(1, 2)  # BCHW -> BNC\n        x = self.norm(x)\n        return x\n\n\nclass TextProjection(nn.Module):\n    \"\"\"\n    Projects text embeddings. Also handles dropout for classifier-free guidance.\n\n    Adapted from https://github.com/PixArt-alpha/PixArt-alpha/blob/master/diffusion/model/nets/PixArt_blocks.py\n    \"\"\"\n\n    def __init__(self, in_channels, hidden_size, act_layer, dtype=None, device=None):\n        factory_kwargs = {\"dtype\": dtype, \"device\": device}\n        super().__init__()\n        self.linear_1 = nn.Linear(\n            in_features=in_channels,\n            out_features=hidden_size,\n            bias=True,\n            **factory_kwargs\n        )\n        self.act_1 = act_layer()\n        self.linear_2 = nn.Linear(\n            in_features=hidden_size,\n            out_features=hidden_size,\n            bias=True,\n            **factory_kwargs\n        )\n\n    def forward(self, caption):\n        hidden_states = self.linear_1(caption)\n        hidden_states = self.act_1(hidden_states)\n        hidden_states = self.linear_2(hidden_states)\n        return hidden_states\n\n\ndef timestep_embedding(t, dim, max_period=10000):\n    \"\"\"\n    Create sinusoidal timestep embeddings.\n\n    Args:\n        t (torch.Tensor): a 1-D Tensor of N indices, one per batch element. These may be fractional.\n        dim (int): the dimension of the output.\n        max_period (int): controls the minimum frequency of the embeddings.\n\n    Returns:\n        embedding (torch.Tensor): An (N, D) Tensor of positional embeddings.\n\n    .. ref_link: https://github.com/openai/glide-text2im/blob/main/glide_text2im/nn.py\n    \"\"\"\n    half = dim // 2\n    freqs = torch.exp(\n        -math.log(max_period)\n        * torch.arange(start=0, end=half, dtype=torch.float32)\n        / half\n    ).to(device=t.device)\n    args = t[:, None].float() * freqs[None]\n    embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)\n    if dim % 2:\n        embedding = torch.cat([embedding, torch.zeros_like(embedding[:, :1])], dim=-1)\n    return embedding\n\n\nclass TimestepEmbedder(nn.Module):\n    \"\"\"\n    Embeds scalar timesteps into vector representations.\n    \"\"\"\n\n    def __init__(\n        self,\n        hidden_size,\n        act_layer,\n        frequency_embedding_size=256,\n        max_period=10000,\n        out_size=None,\n        dtype=None,\n        device=None,\n    ):\n        factory_kwargs = {\"dtype\": dtype, \"device\": device}\n        super().__init__()\n        self.frequency_embedding_size = frequency_embedding_size\n        self.max_period = max_period\n        if out_size is None:\n            out_size = hidden_size\n\n        self.mlp = nn.Sequential(\n            nn.Linear(\n                frequency_embedding_size, hidden_size, bias=True, **factory_kwargs\n            ),\n            act_layer(),\n            nn.Linear(hidden_size, out_size, bias=True, **factory_kwargs),\n        )\n        nn.init.normal_(self.mlp[0].weight, std=0.02)\n        nn.init.normal_(self.mlp[2].weight, std=0.02)\n\n    def forward(self, t):\n        t_freq = timestep_embedding(\n            t, self.frequency_embedding_size, self.max_period\n        ).type(self.mlp[0].weight.dtype)\n        t_emb = self.mlp(t_freq)\n        return t_emb\n"}
{"type": "source_file", "path": "svg/models/hyvideo/modules/mlp_layers.py", "content": "# Modified from timm library:\n# https://github.com/huggingface/pytorch-image-models/blob/648aaa41233ba83eb38faf5ba9d415d574823241/timm/layers/mlp.py#L13\n\nfrom functools import partial\n\nimport torch\nimport torch.nn as nn\n\nfrom .modulate_layers import modulate\nfrom ..utils.helpers import to_2tuple\n\n\nclass MLP(nn.Module):\n    \"\"\"MLP as used in Vision Transformer, MLP-Mixer and related networks\"\"\"\n\n    def __init__(\n        self,\n        in_channels,\n        hidden_channels=None,\n        out_features=None,\n        act_layer=nn.GELU,\n        norm_layer=None,\n        bias=True,\n        drop=0.0,\n        use_conv=False,\n        device=None,\n        dtype=None,\n    ):\n        factory_kwargs = {\"device\": device, \"dtype\": dtype}\n        super().__init__()\n        out_features = out_features or in_channels\n        hidden_channels = hidden_channels or in_channels\n        bias = to_2tuple(bias)\n        drop_probs = to_2tuple(drop)\n        linear_layer = partial(nn.Conv2d, kernel_size=1) if use_conv else nn.Linear\n\n        self.fc1 = linear_layer(\n            in_channels, hidden_channels, bias=bias[0], **factory_kwargs\n        )\n        self.act = act_layer()\n        self.drop1 = nn.Dropout(drop_probs[0])\n        self.norm = (\n            norm_layer(hidden_channels, **factory_kwargs)\n            if norm_layer is not None\n            else nn.Identity()\n        )\n        self.fc2 = linear_layer(\n            hidden_channels, out_features, bias=bias[1], **factory_kwargs\n        )\n        self.drop2 = nn.Dropout(drop_probs[1])\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.drop1(x)\n        x = self.norm(x)\n        x = self.fc2(x)\n        x = self.drop2(x)\n        return x\n\n\n# \nclass MLPEmbedder(nn.Module):\n    \"\"\"copied from https://github.com/black-forest-labs/flux/blob/main/src/flux/modules/layers.py\"\"\"\n    def __init__(self, in_dim: int, hidden_dim: int, device=None, dtype=None):\n        factory_kwargs = {\"device\": device, \"dtype\": dtype}\n        super().__init__()\n        self.in_layer = nn.Linear(in_dim, hidden_dim, bias=True, **factory_kwargs)\n        self.silu = nn.SiLU()\n        self.out_layer = nn.Linear(hidden_dim, hidden_dim, bias=True, **factory_kwargs)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        return self.out_layer(self.silu(self.in_layer(x)))\n\n\nclass FinalLayer(nn.Module):\n    \"\"\"The final layer of DiT.\"\"\"\n\n    def __init__(\n        self, hidden_size, patch_size, out_channels, act_layer, device=None, dtype=None\n    ):\n        factory_kwargs = {\"device\": device, \"dtype\": dtype}\n        super().__init__()\n\n        # Just use LayerNorm for the final layer\n        self.norm_final = nn.LayerNorm(\n            hidden_size, elementwise_affine=False, eps=1e-6, **factory_kwargs\n        )\n        if isinstance(patch_size, int):\n            self.linear = nn.Linear(\n                hidden_size,\n                patch_size * patch_size * out_channels,\n                bias=True,\n                **factory_kwargs\n            )\n        else:\n            self.linear = nn.Linear(\n                hidden_size,\n                patch_size[0] * patch_size[1] * patch_size[2] * out_channels,\n                bias=True,\n            )\n        nn.init.zeros_(self.linear.weight)\n        nn.init.zeros_(self.linear.bias)\n\n        # Here we don't distinguish between the modulate types. Just use the simple one.\n        self.adaLN_modulation = nn.Sequential(\n            act_layer(),\n            nn.Linear(hidden_size, 2 * hidden_size, bias=True, **factory_kwargs),\n        )\n        # Zero-initialize the modulation\n        nn.init.zeros_(self.adaLN_modulation[1].weight)\n        nn.init.zeros_(self.adaLN_modulation[1].bias)\n\n    def forward(self, x, c):\n        shift, scale = self.adaLN_modulation(c).chunk(2, dim=1)\n        x = modulate(self.norm_final(x), shift=shift, scale=scale)\n        x = self.linear(x)\n        return x\n"}
{"type": "source_file", "path": "svg/models/hyvideo/diffusion/schedulers/__init__.py", "content": "from .scheduling_flow_match_discrete import FlowMatchDiscreteScheduler\n"}
{"type": "source_file", "path": "svg/models/hyvideo/diffusion/pipelines/pipeline_hunyuan_video.py", "content": "# Copyright 2024 The HuggingFace Team. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n#\n# Modified from diffusers==0.29.2\n#\n# ==============================================================================\nimport inspect\nfrom typing import Any, Callable, Dict, List, Optional, Union, Tuple\nimport torch\nimport torch.distributed as dist\nimport numpy as np\nfrom dataclasses import dataclass\nfrom packaging import version\n\nfrom diffusers.callbacks import MultiPipelineCallbacks, PipelineCallback\nfrom diffusers.configuration_utils import FrozenDict\nfrom diffusers.image_processor import VaeImageProcessor\nfrom diffusers.loaders import LoraLoaderMixin, TextualInversionLoaderMixin\nfrom diffusers.models import AutoencoderKL\nfrom diffusers.models.lora import adjust_lora_scale_text_encoder\nfrom diffusers.schedulers import KarrasDiffusionSchedulers\nfrom diffusers.utils import (\n    USE_PEFT_BACKEND,\n    deprecate,\n    logging,\n    replace_example_docstring,\n    scale_lora_layers,\n    unscale_lora_layers,\n)\nfrom diffusers.utils.torch_utils import randn_tensor\nfrom diffusers.pipelines.pipeline_utils import DiffusionPipeline\nfrom diffusers.utils import BaseOutput\n\nfrom ...constants import PRECISION_TO_TYPE\nfrom ...vae.autoencoder_kl_causal_3d import AutoencoderKLCausal3D\nfrom ...text_encoder import TextEncoder\nfrom ...modules import HYVideoDiffusionTransformer\n\nlogger = logging.get_logger(__name__)  # pylint: disable=invalid-name\n\nEXAMPLE_DOC_STRING = \"\"\"\"\"\"\n\n\ndef rescale_noise_cfg(noise_cfg, noise_pred_text, guidance_rescale=0.0):\n    \"\"\"\n    Rescale `noise_cfg` according to `guidance_rescale`. Based on findings of [Common Diffusion Noise Schedules and\n    Sample Steps are Flawed](https://arxiv.org/pdf/2305.08891.pdf). See Section 3.4\n    \"\"\"\n    std_text = noise_pred_text.std(\n        dim=list(range(1, noise_pred_text.ndim)), keepdim=True\n    )\n    std_cfg = noise_cfg.std(dim=list(range(1, noise_cfg.ndim)), keepdim=True)\n    # rescale the results from guidance (fixes overexposure)\n    noise_pred_rescaled = noise_cfg * (std_text / std_cfg)\n    # mix with the original results from guidance by factor guidance_rescale to avoid \"plain looking\" images\n    noise_cfg = (\n        guidance_rescale * noise_pred_rescaled + (1 - guidance_rescale) * noise_cfg\n    )\n    return noise_cfg\n\n\ndef retrieve_timesteps(\n    scheduler,\n    num_inference_steps: Optional[int] = None,\n    device: Optional[Union[str, torch.device]] = None,\n    timesteps: Optional[List[int]] = None,\n    sigmas: Optional[List[float]] = None,\n    **kwargs,\n):\n    \"\"\"\n    Calls the scheduler's `set_timesteps` method and retrieves timesteps from the scheduler after the call. Handles\n    custom timesteps. Any kwargs will be supplied to `scheduler.set_timesteps`.\n\n    Args:\n        scheduler (`SchedulerMixin`):\n            The scheduler to get timesteps from.\n        num_inference_steps (`int`):\n            The number of diffusion steps used when generating samples with a pre-trained model. If used, `timesteps`\n            must be `None`.\n        device (`str` or `torch.device`, *optional*):\n            The device to which the timesteps should be moved to. If `None`, the timesteps are not moved.\n        timesteps (`List[int]`, *optional*):\n            Custom timesteps used to override the timestep spacing strategy of the scheduler. If `timesteps` is passed,\n            `num_inference_steps` and `sigmas` must be `None`.\n        sigmas (`List[float]`, *optional*):\n            Custom sigmas used to override the timestep spacing strategy of the scheduler. If `sigmas` is passed,\n            `num_inference_steps` and `timesteps` must be `None`.\n\n    Returns:\n        `Tuple[torch.Tensor, int]`: A tuple where the first element is the timestep schedule from the scheduler and the\n        second element is the number of inference steps.\n    \"\"\"\n    if timesteps is not None and sigmas is not None:\n        raise ValueError(\n            \"Only one of `timesteps` or `sigmas` can be passed. Please choose one to set custom values\"\n        )\n    if timesteps is not None:\n        accepts_timesteps = \"timesteps\" in set(\n            inspect.signature(scheduler.set_timesteps).parameters.keys()\n        )\n        if not accepts_timesteps:\n            raise ValueError(\n                f\"The current scheduler class {scheduler.__class__}'s `set_timesteps` does not support custom\"\n                f\" timestep schedules. Please check whether you are using the correct scheduler.\"\n            )\n        scheduler.set_timesteps(timesteps=timesteps, device=device, **kwargs)\n        timesteps = scheduler.timesteps\n        num_inference_steps = len(timesteps)\n    elif sigmas is not None:\n        accept_sigmas = \"sigmas\" in set(\n            inspect.signature(scheduler.set_timesteps).parameters.keys()\n        )\n        if not accept_sigmas:\n            raise ValueError(\n                f\"The current scheduler class {scheduler.__class__}'s `set_timesteps` does not support custom\"\n                f\" sigmas schedules. Please check whether you are using the correct scheduler.\"\n            )\n        scheduler.set_timesteps(sigmas=sigmas, device=device, **kwargs)\n        timesteps = scheduler.timesteps\n        num_inference_steps = len(timesteps)\n    else:\n        scheduler.set_timesteps(num_inference_steps, device=device, **kwargs)\n        timesteps = scheduler.timesteps\n    return timesteps, num_inference_steps\n\n\n@dataclass\nclass HunyuanVideoPipelineOutput(BaseOutput):\n    videos: Union[torch.Tensor, np.ndarray]\n\n\nclass HunyuanVideoPipeline(DiffusionPipeline):\n    r\"\"\"\n    Pipeline for text-to-video generation using HunyuanVideo.\n\n    This model inherits from [`DiffusionPipeline`]. Check the superclass documentation for the generic methods\n    implemented for all pipelines (downloading, saving, running on a particular device, etc.).\n\n    Args:\n        vae ([`AutoencoderKL`]):\n            Variational Auto-Encoder (VAE) model to encode and decode images to and from latent representations.\n        text_encoder ([`TextEncoder`]):\n            Frozen text-encoder.\n        text_encoder_2 ([`TextEncoder`]):\n            Frozen text-encoder_2.\n        transformer ([`HYVideoDiffusionTransformer`]):\n            A `HYVideoDiffusionTransformer` to denoise the encoded video latents.\n        scheduler ([`SchedulerMixin`]):\n            A scheduler to be used in combination with `unet` to denoise the encoded image latents.\n    \"\"\"\n\n    model_cpu_offload_seq = \"text_encoder->text_encoder_2->transformer->vae\"\n    _optional_components = [\"text_encoder_2\"]\n    _exclude_from_cpu_offload = [\"transformer\"]\n    _callback_tensor_inputs = [\"latents\", \"prompt_embeds\", \"negative_prompt_embeds\"]\n\n    def __init__(\n        self,\n        vae: AutoencoderKL,\n        text_encoder: TextEncoder,\n        transformer: HYVideoDiffusionTransformer,\n        scheduler: KarrasDiffusionSchedulers,\n        text_encoder_2: Optional[TextEncoder] = None,\n        progress_bar_config: Dict[str, Any] = None,\n        args=None,\n    ):\n        super().__init__()\n\n        # ==========================================================================================\n        if progress_bar_config is None:\n            progress_bar_config = {}\n        if not hasattr(self, \"_progress_bar_config\"):\n            self._progress_bar_config = {}\n        self._progress_bar_config.update(progress_bar_config)\n\n        self.args = args\n        # ==========================================================================================\n\n        if (\n            hasattr(scheduler.config, \"steps_offset\")\n            and scheduler.config.steps_offset != 1\n        ):\n            deprecation_message = (\n                f\"The configuration file of this scheduler: {scheduler} is outdated. `steps_offset`\"\n                f\" should be set to 1 instead of {scheduler.config.steps_offset}. Please make sure \"\n                \"to update the config accordingly as leaving `steps_offset` might led to incorrect results\"\n                \" in future versions. If you have downloaded this checkpoint from the Hugging Face Hub,\"\n                \" it would be very nice if you could open a Pull request for the `scheduler/scheduler_config.json`\"\n                \" file\"\n            )\n            deprecate(\n                \"steps_offset!=1\", \"1.0.0\", deprecation_message, standard_warn=False\n            )\n            new_config = dict(scheduler.config)\n            new_config[\"steps_offset\"] = 1\n            scheduler._internal_dict = FrozenDict(new_config)\n\n        if (\n            hasattr(scheduler.config, \"clip_sample\")\n            and scheduler.config.clip_sample is True\n        ):\n            deprecation_message = (\n                f\"The configuration file of this scheduler: {scheduler} has not set the configuration `clip_sample`.\"\n                \" `clip_sample` should be set to False in the configuration file. Please make sure to update the\"\n                \" config accordingly as not setting `clip_sample` in the config might lead to incorrect results in\"\n                \" future versions. If you have downloaded this checkpoint from the Hugging Face Hub, it would be very\"\n                \" nice if you could open a Pull request for the `scheduler/scheduler_config.json` file\"\n            )\n            deprecate(\n                \"clip_sample not set\", \"1.0.0\", deprecation_message, standard_warn=False\n            )\n            new_config = dict(scheduler.config)\n            new_config[\"clip_sample\"] = False\n            scheduler._internal_dict = FrozenDict(new_config)\n\n        self.register_modules(\n            vae=vae,\n            text_encoder=text_encoder,\n            transformer=transformer,\n            scheduler=scheduler,\n            text_encoder_2=text_encoder_2,\n        )\n        self.vae_scale_factor = 2 ** (len(self.vae.config.block_out_channels) - 1)\n        self.image_processor = VaeImageProcessor(vae_scale_factor=self.vae_scale_factor)\n\n    def encode_prompt(\n        self,\n        prompt,\n        device,\n        num_videos_per_prompt,\n        do_classifier_free_guidance,\n        negative_prompt=None,\n        prompt_embeds: Optional[torch.Tensor] = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        negative_prompt_embeds: Optional[torch.Tensor] = None,\n        negative_attention_mask: Optional[torch.Tensor] = None,\n        lora_scale: Optional[float] = None,\n        clip_skip: Optional[int] = None,\n        text_encoder: Optional[TextEncoder] = None,\n        data_type: Optional[str] = \"image\",\n    ):\n        r\"\"\"\n        Encodes the prompt into text encoder hidden states.\n\n        Args:\n            prompt (`str` or `List[str]`, *optional*):\n                prompt to be encoded\n            device: (`torch.device`):\n                torch device\n            num_videos_per_prompt (`int`):\n                number of videos that should be generated per prompt\n            do_classifier_free_guidance (`bool`):\n                whether to use classifier free guidance or not\n            negative_prompt (`str` or `List[str]`, *optional*):\n                The prompt or prompts not to guide the video generation. If not defined, one has to pass\n                `negative_prompt_embeds` instead. Ignored when not using guidance (i.e., ignored if `guidance_scale` is\n                less than `1`).\n            prompt_embeds (`torch.Tensor`, *optional*):\n                Pre-generated text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not\n                provided, text embeddings will be generated from `prompt` input argument.\n            attention_mask (`torch.Tensor`, *optional*):\n            negative_prompt_embeds (`torch.Tensor`, *optional*):\n                Pre-generated negative text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt\n                weighting. If not provided, negative_prompt_embeds will be generated from `negative_prompt` input\n                argument.\n            negative_attention_mask (`torch.Tensor`, *optional*):\n            lora_scale (`float`, *optional*):\n                A LoRA scale that will be applied to all LoRA layers of the text encoder if LoRA layers are loaded.\n            clip_skip (`int`, *optional*):\n                Number of layers to be skipped from CLIP while computing the prompt embeddings. A value of 1 means that\n                the output of the pre-final layer will be used for computing the prompt embeddings.\n            text_encoder (TextEncoder, *optional*):\n            data_type (`str`, *optional*):\n        \"\"\"\n        if text_encoder is None:\n            text_encoder = self.text_encoder\n\n        # set lora scale so that monkey patched LoRA\n        # function of text encoder can correctly access it\n        if lora_scale is not None and isinstance(self, LoraLoaderMixin):\n            self._lora_scale = lora_scale\n\n            # dynamically adjust the LoRA scale\n            if not USE_PEFT_BACKEND:\n                adjust_lora_scale_text_encoder(text_encoder.model, lora_scale)\n            else:\n                scale_lora_layers(text_encoder.model, lora_scale)\n\n        if prompt is not None and isinstance(prompt, str):\n            batch_size = 1\n        elif prompt is not None and isinstance(prompt, list):\n            batch_size = len(prompt)\n        else:\n            batch_size = prompt_embeds.shape[0]\n\n        if prompt_embeds is None:\n            # textual inversion: process multi-vector tokens if necessary\n            if isinstance(self, TextualInversionLoaderMixin):\n                prompt = self.maybe_convert_prompt(prompt, text_encoder.tokenizer)\n\n            text_inputs = text_encoder.text2tokens(prompt, data_type=data_type)\n\n            if clip_skip is None:\n                prompt_outputs = text_encoder.encode(\n                    text_inputs, data_type=data_type, device=device\n                )\n                prompt_embeds = prompt_outputs.hidden_state\n            else:\n                prompt_outputs = text_encoder.encode(\n                    text_inputs,\n                    output_hidden_states=True,\n                    data_type=data_type,\n                    device=device,\n                )\n                # Access the `hidden_states` first, that contains a tuple of\n                # all the hidden states from the encoder layers. Then index into\n                # the tuple to access the hidden states from the desired layer.\n                prompt_embeds = prompt_outputs.hidden_states_list[-(clip_skip + 1)]\n                # We also need to apply the final LayerNorm here to not mess with the\n                # representations. The `last_hidden_states` that we typically use for\n                # obtaining the final prompt representations passes through the LayerNorm\n                # layer.\n                prompt_embeds = text_encoder.model.text_model.final_layer_norm(\n                    prompt_embeds\n                )\n\n            attention_mask = prompt_outputs.attention_mask\n            if attention_mask is not None:\n                attention_mask = attention_mask.to(device)\n                bs_embed, seq_len = attention_mask.shape\n                attention_mask = attention_mask.repeat(1, num_videos_per_prompt)\n                attention_mask = attention_mask.view(\n                    bs_embed * num_videos_per_prompt, seq_len\n                )\n\n        if text_encoder is not None:\n            prompt_embeds_dtype = text_encoder.dtype\n        elif self.transformer is not None:\n            prompt_embeds_dtype = self.transformer.dtype\n        else:\n            prompt_embeds_dtype = prompt_embeds.dtype\n\n        prompt_embeds = prompt_embeds.to(dtype=prompt_embeds_dtype, device=device)\n\n        if prompt_embeds.ndim == 2:\n            bs_embed, _ = prompt_embeds.shape\n            # duplicate text embeddings for each generation per prompt, using mps friendly method\n            prompt_embeds = prompt_embeds.repeat(1, num_videos_per_prompt)\n            prompt_embeds = prompt_embeds.view(bs_embed * num_videos_per_prompt, -1)\n        else:\n            bs_embed, seq_len, _ = prompt_embeds.shape\n            # duplicate text embeddings for each generation per prompt, using mps friendly method\n            prompt_embeds = prompt_embeds.repeat(1, num_videos_per_prompt, 1)\n            prompt_embeds = prompt_embeds.view(\n                bs_embed * num_videos_per_prompt, seq_len, -1\n            )\n\n        # get unconditional embeddings for classifier free guidance\n        if do_classifier_free_guidance and negative_prompt_embeds is None:\n            uncond_tokens: List[str]\n            if negative_prompt is None:\n                uncond_tokens = [\"\"] * batch_size\n            elif prompt is not None and type(prompt) is not type(negative_prompt):\n                raise TypeError(\n                    f\"`negative_prompt` should be the same type to `prompt`, but got {type(negative_prompt)} !=\"\n                    f\" {type(prompt)}.\"\n                )\n            elif isinstance(negative_prompt, str):\n                uncond_tokens = [negative_prompt]\n            elif batch_size != len(negative_prompt):\n                raise ValueError(\n                    f\"`negative_prompt`: {negative_prompt} has batch size {len(negative_prompt)}, but `prompt`:\"\n                    f\" {prompt} has batch size {batch_size}. Please make sure that passed `negative_prompt` matches\"\n                    \" the batch size of `prompt`.\"\n                )\n            else:\n                uncond_tokens = negative_prompt\n\n            # textual inversion: process multi-vector tokens if necessary\n            if isinstance(self, TextualInversionLoaderMixin):\n                uncond_tokens = self.maybe_convert_prompt(\n                    uncond_tokens, text_encoder.tokenizer\n                )\n\n            # max_length = prompt_embeds.shape[1]\n            uncond_input = text_encoder.text2tokens(uncond_tokens, data_type=data_type)\n\n            negative_prompt_outputs = text_encoder.encode(\n                uncond_input, data_type=data_type, device=device\n            )\n            negative_prompt_embeds = negative_prompt_outputs.hidden_state\n\n            negative_attention_mask = negative_prompt_outputs.attention_mask\n            if negative_attention_mask is not None:\n                negative_attention_mask = negative_attention_mask.to(device)\n                _, seq_len = negative_attention_mask.shape\n                negative_attention_mask = negative_attention_mask.repeat(\n                    1, num_videos_per_prompt\n                )\n                negative_attention_mask = negative_attention_mask.view(\n                    batch_size * num_videos_per_prompt, seq_len\n                )\n\n        if do_classifier_free_guidance:\n            # duplicate unconditional embeddings for each generation per prompt, using mps friendly method\n            seq_len = negative_prompt_embeds.shape[1]\n\n            negative_prompt_embeds = negative_prompt_embeds.to(\n                dtype=prompt_embeds_dtype, device=device\n            )\n\n            if negative_prompt_embeds.ndim == 2:\n                negative_prompt_embeds = negative_prompt_embeds.repeat(\n                    1, num_videos_per_prompt\n                )\n                negative_prompt_embeds = negative_prompt_embeds.view(\n                    batch_size * num_videos_per_prompt, -1\n                )\n            else:\n                negative_prompt_embeds = negative_prompt_embeds.repeat(\n                    1, num_videos_per_prompt, 1\n                )\n                negative_prompt_embeds = negative_prompt_embeds.view(\n                    batch_size * num_videos_per_prompt, seq_len, -1\n                )\n\n        if text_encoder is not None:\n            if isinstance(self, LoraLoaderMixin) and USE_PEFT_BACKEND:\n                # Retrieve the original scale by scaling back the LoRA layers\n                unscale_lora_layers(text_encoder.model, lora_scale)\n\n        return (\n            prompt_embeds,\n            negative_prompt_embeds,\n            attention_mask,\n            negative_attention_mask,\n        )\n\n    def decode_latents(self, latents, enable_tiling=True):\n        deprecation_message = \"The decode_latents method is deprecated and will be removed in 1.0.0. Please use VaeImageProcessor.postprocess(...) instead\"\n        deprecate(\"decode_latents\", \"1.0.0\", deprecation_message, standard_warn=False)\n\n        latents = 1 / self.vae.config.scaling_factor * latents\n        if enable_tiling:\n            self.vae.enable_tiling()\n            image = self.vae.decode(latents, return_dict=False)[0]\n        else:\n            image = self.vae.decode(latents, return_dict=False)[0]\n        image = (image / 2 + 0.5).clamp(0, 1)\n        # we always cast to float32 as this does not cause significant overhead and is compatible with bfloat16\n        if image.ndim == 4:\n            image = image.cpu().permute(0, 2, 3, 1).float()\n        else:\n            image = image.cpu().float()\n        return image\n\n    def prepare_extra_func_kwargs(self, func, kwargs):\n        # prepare extra kwargs for the scheduler step, since not all schedulers have the same signature\n        # eta () is only used with the DDIMScheduler, it will be ignored for other schedulers.\n        # eta corresponds to  in DDIM paper: https://arxiv.org/abs/2010.02502\n        # and should be between [0, 1]\n        extra_step_kwargs = {}\n\n        for k, v in kwargs.items():\n            accepts = k in set(inspect.signature(func).parameters.keys())\n            if accepts:\n                extra_step_kwargs[k] = v\n        return extra_step_kwargs\n\n    def check_inputs(\n        self,\n        prompt,\n        height,\n        width,\n        video_length,\n        callback_steps,\n        negative_prompt=None,\n        prompt_embeds=None,\n        negative_prompt_embeds=None,\n        callback_on_step_end_tensor_inputs=None,\n        vae_ver=\"88-4c-sd\",\n    ):\n        if height % 8 != 0 or width % 8 != 0:\n            raise ValueError(\n                f\"`height` and `width` have to be divisible by 8 but are {height} and {width}.\"\n            )\n\n        if video_length is not None:\n            if \"884\" in vae_ver:\n                if video_length != 1 and (video_length - 1) % 4 != 0:\n                    raise ValueError(\n                        f\"`video_length` has to be 1 or a multiple of 4 but is {video_length}.\"\n                    )\n            elif \"888\" in vae_ver:\n                if video_length != 1 and (video_length - 1) % 8 != 0:\n                    raise ValueError(\n                        f\"`video_length` has to be 1 or a multiple of 8 but is {video_length}.\"\n                    )\n\n        if callback_steps is not None and (\n            not isinstance(callback_steps, int) or callback_steps <= 0\n        ):\n            raise ValueError(\n                f\"`callback_steps` has to be a positive integer but is {callback_steps} of type\"\n                f\" {type(callback_steps)}.\"\n            )\n        if callback_on_step_end_tensor_inputs is not None and not all(\n            k in self._callback_tensor_inputs\n            for k in callback_on_step_end_tensor_inputs\n        ):\n            raise ValueError(\n                f\"`callback_on_step_end_tensor_inputs` has to be in {self._callback_tensor_inputs}, but found {[k for k in callback_on_step_end_tensor_inputs if k not in self._callback_tensor_inputs]}\"\n            )\n\n        if prompt is not None and prompt_embeds is not None:\n            raise ValueError(\n                f\"Cannot forward both `prompt`: {prompt} and `prompt_embeds`: {prompt_embeds}. Please make sure to\"\n                \" only forward one of the two.\"\n            )\n        elif prompt is None and prompt_embeds is None:\n            raise ValueError(\n                \"Provide either `prompt` or `prompt_embeds`. Cannot leave both `prompt` and `prompt_embeds` undefined.\"\n            )\n        elif prompt is not None and (\n            not isinstance(prompt, str) and not isinstance(prompt, list)\n        ):\n            raise ValueError(\n                f\"`prompt` has to be of type `str` or `list` but is {type(prompt)}\"\n            )\n\n        if negative_prompt is not None and negative_prompt_embeds is not None:\n            raise ValueError(\n                f\"Cannot forward both `negative_prompt`: {negative_prompt} and `negative_prompt_embeds`:\"\n                f\" {negative_prompt_embeds}. Please make sure to only forward one of the two.\"\n            )\n\n        if prompt_embeds is not None and negative_prompt_embeds is not None:\n            if prompt_embeds.shape != negative_prompt_embeds.shape:\n                raise ValueError(\n                    \"`prompt_embeds` and `negative_prompt_embeds` must have the same shape when passed directly, but\"\n                    f\" got: `prompt_embeds` {prompt_embeds.shape} != `negative_prompt_embeds`\"\n                    f\" {negative_prompt_embeds.shape}.\"\n                )\n\n\n    def prepare_latents(\n        self,\n        batch_size,\n        num_channels_latents,\n        height,\n        width,\n        video_length,\n        dtype,\n        device,\n        generator,\n        latents=None,\n    ):\n        shape = (\n            batch_size,\n            num_channels_latents,\n            video_length,\n            int(height) // self.vae_scale_factor,\n            int(width) // self.vae_scale_factor,\n        )\n        if isinstance(generator, list) and len(generator) != batch_size:\n            raise ValueError(\n                f\"You have passed a list of generators of length {len(generator)}, but requested an effective batch\"\n                f\" size of {batch_size}. Make sure the batch size matches the length of the generators.\"\n            )\n\n        if latents is None:\n            latents = randn_tensor(\n                shape, generator=generator, device=device, dtype=dtype\n            )\n        else:\n            latents = latents.to(device)\n\n        # Check existence to make it compatible with FlowMatchEulerDiscreteScheduler\n        if hasattr(self.scheduler, \"init_noise_sigma\"):\n            # scale the initial noise by the standard deviation required by the scheduler\n            latents = latents * self.scheduler.init_noise_sigma\n        return latents\n\n    # Copied from diffusers.pipelines.latent_consistency_models.pipeline_latent_consistency_text2img.LatentConsistencyModelPipeline.get_guidance_scale_embedding\n    def get_guidance_scale_embedding(\n        self,\n        w: torch.Tensor,\n        embedding_dim: int = 512,\n        dtype: torch.dtype = torch.float32,\n    ) -> torch.Tensor:\n        \"\"\"\n        See https://github.com/google-research/vdm/blob/dc27b98a554f65cdc654b800da5aa1846545d41b/model_vdm.py#L298\n\n        Args:\n            w (`torch.Tensor`):\n                Generate embedding vectors with a specified guidance scale to subsequently enrich timestep embeddings.\n            embedding_dim (`int`, *optional*, defaults to 512):\n                Dimension of the embeddings to generate.\n            dtype (`torch.dtype`, *optional*, defaults to `torch.float32`):\n                Data type of the generated embeddings.\n\n        Returns:\n            `torch.Tensor`: Embedding vectors with shape `(len(w), embedding_dim)`.\n        \"\"\"\n        assert len(w.shape) == 1\n        w = w * 1000.0\n\n        half_dim = embedding_dim // 2\n        emb = torch.log(torch.tensor(10000.0)) / (half_dim - 1)\n        emb = torch.exp(torch.arange(half_dim, dtype=dtype) * -emb)\n        emb = w.to(dtype)[:, None] * emb[None, :]\n        emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)\n        if embedding_dim % 2 == 1:  # zero pad\n            emb = torch.nn.functional.pad(emb, (0, 1))\n        assert emb.shape == (w.shape[0], embedding_dim)\n        return emb\n\n    @property\n    def guidance_scale(self):\n        return self._guidance_scale\n\n    @property\n    def guidance_rescale(self):\n        return self._guidance_rescale\n\n    @property\n    def clip_skip(self):\n        return self._clip_skip\n\n    # here `guidance_scale` is defined analog to the guidance weight `w` of equation (2)\n    # of the Imagen paper: https://arxiv.org/pdf/2205.11487.pdf . `guidance_scale = 1`\n    # corresponds to doing no classifier free guidance.\n    @property\n    def do_classifier_free_guidance(self):\n        # return self._guidance_scale > 1 and self.transformer.config.time_cond_proj_dim is None\n        return self._guidance_scale > 1\n\n    @property\n    def cross_attention_kwargs(self):\n        return self._cross_attention_kwargs\n\n    @property\n    def num_timesteps(self):\n        return self._num_timesteps\n\n    @property\n    def interrupt(self):\n        return self._interrupt\n    \n    @torch.no_grad()\n    def get_prompt_mask(\n        self,\n        prompt: Union[str, List[str]],\n        height: int,\n        width: int,\n        video_length: int,\n        data_type: str = \"video\",\n        num_inference_steps: int = 50,\n        timesteps: List[int] = None,\n        sigmas: List[float] = None,\n        guidance_scale: float = 7.5,\n        negative_prompt: Optional[Union[str, List[str]]] = None,\n        num_videos_per_prompt: Optional[int] = 1,\n        eta: float = 0.0,\n        latents: Optional[torch.Tensor] = None,\n        prompt_embeds: Optional[torch.Tensor] = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        negative_prompt_embeds: Optional[torch.Tensor] = None,\n        negative_attention_mask: Optional[torch.Tensor] = None,\n        output_type: Optional[str] = \"pil\",\n        return_dict: bool = True,\n        cross_attention_kwargs: Optional[Dict[str, Any]] = None,\n        guidance_rescale: float = 0.0,\n        clip_skip: Optional[int] = None,\n        callback_on_step_end: Optional[\n            Union[\n                Callable[[int, int, Dict], None],\n                PipelineCallback,\n                MultiPipelineCallbacks,\n            ]\n        ] = None,\n        callback_on_step_end_tensor_inputs: List[str] = [\"latents\"],\n        vae_ver: str = \"88-4c-sd\",\n        enable_tiling: bool = False,\n        embedded_guidance_scale: Optional[float] = None,\n        **kwargs,\n    ):\n        \"\"\"Copy from __call__. Use this function to get the prompt length in advance\"\"\"\n        callback = kwargs.pop(\"callback\", None)\n        callback_steps = kwargs.pop(\"callback_steps\", None)\n\n        if callback is not None:\n            deprecate(\n                \"callback\",\n                \"1.0.0\",\n                \"Passing `callback` as an input argument to `__call__` is deprecated, consider using `callback_on_step_end`\",\n            )\n        if callback_steps is not None:\n            deprecate(\n                \"callback_steps\",\n                \"1.0.0\",\n                \"Passing `callback_steps` as an input argument to `__call__` is deprecated, consider using `callback_on_step_end`\",\n            )\n\n        if isinstance(callback_on_step_end, (PipelineCallback, MultiPipelineCallbacks)):\n            callback_on_step_end_tensor_inputs = callback_on_step_end.tensor_inputs\n\n        # 1. Check inputs. Raise error if not correct\n        self.check_inputs(\n            prompt,\n            height,\n            width,\n            video_length,\n            callback_steps,\n            negative_prompt,\n            prompt_embeds,\n            negative_prompt_embeds,\n            callback_on_step_end_tensor_inputs,\n            vae_ver=vae_ver,\n        )\n\n        self._guidance_scale = guidance_scale\n        self._guidance_rescale = guidance_rescale\n        self._clip_skip = clip_skip\n        self._cross_attention_kwargs = cross_attention_kwargs\n        self._interrupt = False\n\n        # 2. Define call parameters\n        if prompt is not None and isinstance(prompt, str):\n            batch_size = 1\n        elif prompt is not None and isinstance(prompt, list):\n            batch_size = len(prompt)\n        else:\n            batch_size = prompt_embeds.shape[0]\n\n        device = torch.device(f\"cuda:{dist.get_rank()}\") if dist.is_initialized() else self._execution_device\n\n        # 3. Encode input prompt\n        lora_scale = (\n            self.cross_attention_kwargs.get(\"scale\", None)\n            if self.cross_attention_kwargs is not None\n            else None\n        )\n\n        (\n            prompt_embeds,\n            negative_prompt_embeds,\n            prompt_mask,\n            negative_prompt_mask,\n        ) = self.encode_prompt(\n            prompt,\n            device,\n            num_videos_per_prompt,\n            self.do_classifier_free_guidance,\n            negative_prompt,\n            prompt_embeds=prompt_embeds,\n            attention_mask=attention_mask,\n            negative_prompt_embeds=negative_prompt_embeds,\n            negative_attention_mask=negative_attention_mask,\n            lora_scale=lora_scale,\n            clip_skip=self.clip_skip,\n            data_type=data_type,\n        )\n        return prompt_mask\n    \n    def haocheng_decode_latents(self, latents, vae_dtype, vae_autocast_enabled, enable_tiling, generator):\n        expand_temporal_dim = False\n        if len(latents.shape) == 4:\n            if isinstance(self.vae, AutoencoderKLCausal3D):\n                latents = latents.unsqueeze(2)\n                expand_temporal_dim = True\n        elif len(latents.shape) == 5:\n            pass\n        else:\n            raise ValueError(\n                f\"Only support latents with shape (b, c, h, w) or (b, c, f, h, w), but got {latents.shape}.\"\n            )\n\n        if (\n            hasattr(self.vae.config, \"shift_factor\")\n            and self.vae.config.shift_factor\n        ):\n            latents = (\n                latents / self.vae.config.scaling_factor\n                + self.vae.config.shift_factor\n            )\n        else:\n            latents = latents / self.vae.config.scaling_factor\n\n        with torch.autocast(\n            device_type=\"cuda\", dtype=vae_dtype, enabled=vae_autocast_enabled\n        ):\n            if enable_tiling:\n                self.vae.enable_tiling()\n                image = self.vae.decode(\n                    latents, return_dict=False, generator=generator\n                )[0]\n            else:\n                image = self.vae.decode(\n                    latents, return_dict=False, generator=generator\n                )[0]\n\n        if expand_temporal_dim or image.shape[2] == 1:\n            image = image.squeeze(2)\n            \n        return image\n\n\n    @torch.no_grad()\n    @replace_example_docstring(EXAMPLE_DOC_STRING)\n    def __call__(\n        self,\n        prompt: Union[str, List[str]],\n        height: int,\n        width: int,\n        video_length: int,\n        data_type: str = \"video\",\n        num_inference_steps: int = 50,\n        timesteps: List[int] = None,\n        sigmas: List[float] = None,\n        guidance_scale: float = 7.5,\n        negative_prompt: Optional[Union[str, List[str]]] = None,\n        num_videos_per_prompt: Optional[int] = 1,\n        eta: float = 0.0,\n        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n        latents: Optional[torch.Tensor] = None,\n        prompt_embeds: Optional[torch.Tensor] = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        negative_prompt_embeds: Optional[torch.Tensor] = None,\n        negative_attention_mask: Optional[torch.Tensor] = None,\n        output_type: Optional[str] = \"pil\",\n        return_dict: bool = True,\n        cross_attention_kwargs: Optional[Dict[str, Any]] = None,\n        guidance_rescale: float = 0.0,\n        clip_skip: Optional[int] = None,\n        callback_on_step_end: Optional[\n            Union[\n                Callable[[int, int, Dict], None],\n                PipelineCallback,\n                MultiPipelineCallbacks,\n            ]\n        ] = None,\n        callback_on_step_end_tensor_inputs: List[str] = [\"latents\"],\n        freqs_cis: Tuple[torch.Tensor, torch.Tensor] = None,\n        vae_ver: str = \"88-4c-sd\",\n        enable_tiling: bool = False,\n        n_tokens: Optional[int] = None,\n        embedded_guidance_scale: Optional[float] = None,\n        **kwargs,\n    ):\n        r\"\"\"\n        The call function to the pipeline for generation.\n\n        Args:\n            prompt (`str` or `List[str]`):\n                The prompt or prompts to guide image generation. If not defined, you need to pass `prompt_embeds`.\n            height (`int`):\n                The height in pixels of the generated image.\n            width (`int`):\n                The width in pixels of the generated image.\n            video_length (`int`):\n                The number of frames in the generated video.\n            num_inference_steps (`int`, *optional*, defaults to 50):\n                The number of denoising steps. More denoising steps usually lead to a higher quality image at the\n                expense of slower inference.\n            timesteps (`List[int]`, *optional*):\n                Custom timesteps to use for the denoising process with schedulers which support a `timesteps` argument\n                in their `set_timesteps` method. If not defined, the default behavior when `num_inference_steps` is\n                passed will be used. Must be in descending order.\n            sigmas (`List[float]`, *optional*):\n                Custom sigmas to use for the denoising process with schedulers which support a `sigmas` argument in\n                their `set_timesteps` method. If not defined, the default behavior when `num_inference_steps` is passed\n                will be used.\n            guidance_scale (`float`, *optional*, defaults to 7.5):\n                A higher guidance scale value encourages the model to generate images closely linked to the text\n                `prompt` at the expense of lower image quality. Guidance scale is enabled when `guidance_scale > 1`.\n            negative_prompt (`str` or `List[str]`, *optional*):\n                The prompt or prompts to guide what to not include in image generation. If not defined, you need to\n                pass `negative_prompt_embeds` instead. Ignored when not using guidance (`guidance_scale < 1`).\n            num_videos_per_prompt (`int`, *optional*, defaults to 1):\n                The number of images to generate per prompt.\n            eta (`float`, *optional*, defaults to 0.0):\n                Corresponds to parameter eta () from the [DDIM](https://arxiv.org/abs/2010.02502) paper. Only applies\n                to the [`~schedulers.DDIMScheduler`], and is ignored in other schedulers.\n            generator (`torch.Generator` or `List[torch.Generator]`, *optional*):\n                A [`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html) to make\n                generation deterministic.\n            latents (`torch.Tensor`, *optional*):\n                Pre-generated noisy latents sampled from a Gaussian distribution, to be used as inputs for image\n                generation. Can be used to tweak the same generation with different prompts. If not provided, a latents\n                tensor is generated by sampling using the supplied random `generator`.\n            prompt_embeds (`torch.Tensor`, *optional*):\n                Pre-generated text embeddings. Can be used to easily tweak text inputs (prompt weighting). If not\n                provided, text embeddings are generated from the `prompt` input argument.\n            negative_prompt_embeds (`torch.Tensor`, *optional*):\n                Pre-generated negative text embeddings. Can be used to easily tweak text inputs (prompt weighting). If\n                not provided, `negative_prompt_embeds` are generated from the `negative_prompt` input argument.\n                \n            output_type (`str`, *optional*, defaults to `\"pil\"`):\n                The output format of the generated image. Choose between `PIL.Image` or `np.array`.\n            return_dict (`bool`, *optional*, defaults to `True`):\n                Whether or not to return a [`HunyuanVideoPipelineOutput`] instead of a\n                plain tuple.\n            cross_attention_kwargs (`dict`, *optional*):\n                A kwargs dictionary that if specified is passed along to the [`AttentionProcessor`] as defined in\n                [`self.processor`](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py).\n            guidance_rescale (`float`, *optional*, defaults to 0.0):\n                Guidance rescale factor from [Common Diffusion Noise Schedules and Sample Steps are\n                Flawed](https://arxiv.org/pdf/2305.08891.pdf). Guidance rescale factor should fix overexposure when\n                using zero terminal SNR.\n            clip_skip (`int`, *optional*):\n                Number of layers to be skipped from CLIP while computing the prompt embeddings. A value of 1 means that\n                the output of the pre-final layer will be used for computing the prompt embeddings.\n            callback_on_step_end (`Callable`, `PipelineCallback`, `MultiPipelineCallbacks`, *optional*):\n                A function or a subclass of `PipelineCallback` or `MultiPipelineCallbacks` that is called at the end of\n                each denoising step during the inference. with the following arguments: `callback_on_step_end(self:\n                DiffusionPipeline, step: int, timestep: int, callback_kwargs: Dict)`. `callback_kwargs` will include a\n                list of all tensors as specified by `callback_on_step_end_tensor_inputs`.\n            callback_on_step_end_tensor_inputs (`List`, *optional*):\n                The list of tensor inputs for the `callback_on_step_end` function. The tensors specified in the list\n                will be passed as `callback_kwargs` argument. You will only be able to include variables listed in the\n                `._callback_tensor_inputs` attribute of your pipeline class.\n\n        Examples:\n\n        Returns:\n            [`~HunyuanVideoPipelineOutput`] or `tuple`:\n                If `return_dict` is `True`, [`HunyuanVideoPipelineOutput`] is returned,\n                otherwise a `tuple` is returned where the first element is a list with the generated images and the\n                second element is a list of `bool`s indicating whether the corresponding generated image contains\n                \"not-safe-for-work\" (nsfw) content.\n        \"\"\"\n        callback = kwargs.pop(\"callback\", None)\n        callback_steps = kwargs.pop(\"callback_steps\", None)\n\n        if callback is not None:\n            deprecate(\n                \"callback\",\n                \"1.0.0\",\n                \"Passing `callback` as an input argument to `__call__` is deprecated, consider using `callback_on_step_end`\",\n            )\n        if callback_steps is not None:\n            deprecate(\n                \"callback_steps\",\n                \"1.0.0\",\n                \"Passing `callback_steps` as an input argument to `__call__` is deprecated, consider using `callback_on_step_end`\",\n            )\n\n        if isinstance(callback_on_step_end, (PipelineCallback, MultiPipelineCallbacks)):\n            callback_on_step_end_tensor_inputs = callback_on_step_end.tensor_inputs\n\n        # 0. Default height and width to unet\n        # height = height or self.transformer.config.sample_size * self.vae_scale_factor\n        # width = width or self.transformer.config.sample_size * self.vae_scale_factor\n        # to deal with lora scaling and other possible forward hooks\n\n        # 1. Check inputs. Raise error if not correct\n        self.check_inputs(\n            prompt,\n            height,\n            width,\n            video_length,\n            callback_steps,\n            negative_prompt,\n            prompt_embeds,\n            negative_prompt_embeds,\n            callback_on_step_end_tensor_inputs,\n            vae_ver=vae_ver,\n        )\n\n        self._guidance_scale = guidance_scale\n        self._guidance_rescale = guidance_rescale\n        self._clip_skip = clip_skip\n        self._cross_attention_kwargs = cross_attention_kwargs\n        self._interrupt = False\n\n        # 2. Define call parameters\n        if prompt is not None and isinstance(prompt, str):\n            batch_size = 1\n        elif prompt is not None and isinstance(prompt, list):\n            batch_size = len(prompt)\n        else:\n            batch_size = prompt_embeds.shape[0]\n\n        device = torch.device(f\"cuda:{dist.get_rank()}\") if dist.is_initialized() else self._execution_device\n\n        # 3. Encode input prompt\n        lora_scale = (\n            self.cross_attention_kwargs.get(\"scale\", None)\n            if self.cross_attention_kwargs is not None\n            else None\n        )\n\n        (\n            prompt_embeds,\n            negative_prompt_embeds,\n            prompt_mask,\n            negative_prompt_mask,\n        ) = self.encode_prompt(\n            prompt,\n            device,\n            num_videos_per_prompt,\n            self.do_classifier_free_guidance,\n            negative_prompt,\n            prompt_embeds=prompt_embeds,\n            attention_mask=attention_mask,\n            negative_prompt_embeds=negative_prompt_embeds,\n            negative_attention_mask=negative_attention_mask,\n            lora_scale=lora_scale,\n            clip_skip=self.clip_skip,\n            data_type=data_type,\n        )\n        if self.text_encoder_2 is not None:\n            (\n                prompt_embeds_2,\n                negative_prompt_embeds_2,\n                prompt_mask_2,\n                negative_prompt_mask_2,\n            ) = self.encode_prompt(\n                prompt,\n                device,\n                num_videos_per_prompt,\n                self.do_classifier_free_guidance,\n                negative_prompt,\n                prompt_embeds=None,\n                attention_mask=None,\n                negative_prompt_embeds=None,\n                negative_attention_mask=None,\n                lora_scale=lora_scale,\n                clip_skip=self.clip_skip,\n                text_encoder=self.text_encoder_2,\n                data_type=data_type,\n            )\n        else:\n            prompt_embeds_2 = None\n            negative_prompt_embeds_2 = None\n            prompt_mask_2 = None\n            negative_prompt_mask_2 = None\n\n        # For classifier free guidance, we need to do two forward passes.\n        # Here we concatenate the unconditional and text embeddings into a single batch\n        # to avoid doing two forward passes\n        if self.do_classifier_free_guidance:\n            prompt_embeds = torch.cat([negative_prompt_embeds, prompt_embeds])\n            if prompt_mask is not None:\n                prompt_mask = torch.cat([negative_prompt_mask, prompt_mask])\n            if prompt_embeds_2 is not None:\n                prompt_embeds_2 = torch.cat([negative_prompt_embeds_2, prompt_embeds_2])\n            if prompt_mask_2 is not None:\n                prompt_mask_2 = torch.cat([negative_prompt_mask_2, prompt_mask_2])\n\n\n        # 4. Prepare timesteps\n        extra_set_timesteps_kwargs = self.prepare_extra_func_kwargs(\n            self.scheduler.set_timesteps, {\"n_tokens\": n_tokens}\n        )\n        timesteps, num_inference_steps = retrieve_timesteps(\n            self.scheduler,\n            num_inference_steps,\n            device,\n            timesteps,\n            sigmas,\n            **extra_set_timesteps_kwargs,\n        )\n\n        if \"884\" in vae_ver:\n            video_length = (video_length - 1) // 4 + 1\n        elif \"888\" in vae_ver:\n            video_length = (video_length - 1) // 8 + 1\n        else:\n            video_length = video_length\n\n        # 5. Prepare latent variables\n        num_channels_latents = self.transformer.config.in_channels\n        latents = self.prepare_latents(\n            batch_size * num_videos_per_prompt,\n            num_channels_latents,\n            height,\n            width,\n            video_length,\n            prompt_embeds.dtype,\n            device,\n            generator,\n            latents,\n        )\n\n        # 6. Prepare extra step kwargs. TODO: Logic should ideally just be moved out of the pipeline\n        extra_step_kwargs = self.prepare_extra_func_kwargs(\n            self.scheduler.step,\n            {\"generator\": generator, \"eta\": eta},\n        )\n\n        target_dtype = PRECISION_TO_TYPE[self.args.precision]\n        autocast_enabled = (\n            target_dtype != torch.float32\n        ) and not self.args.disable_autocast\n        vae_dtype = PRECISION_TO_TYPE[self.args.vae_precision]\n        vae_autocast_enabled = (\n            vae_dtype != torch.float32\n        ) and not self.args.disable_autocast\n\n        # 7. Denoising loop\n        num_warmup_steps = len(timesteps) - num_inference_steps * self.scheduler.order\n        self._num_timesteps = len(timesteps)\n\n        all_latents = []\n        \n        # if is_progress_bar:\n        with self.progress_bar(total=num_inference_steps) as progress_bar:\n            for i, t in enumerate(timesteps):\n                if self.interrupt:\n                    continue\n\n                # expand the latents if we are doing classifier free guidance\n                latent_model_input = (\n                    torch.cat([latents] * 2)\n                    if self.do_classifier_free_guidance\n                    else latents\n                )\n                latent_model_input = self.scheduler.scale_model_input(\n                    latent_model_input, t\n                )\n\n                t_expand = t.repeat(latent_model_input.shape[0])\n                guidance_expand = (\n                    torch.tensor(\n                        [embedded_guidance_scale] * latent_model_input.shape[0],\n                        dtype=torch.float32,\n                        device=device,\n                    ).to(target_dtype)\n                    * 1000.0\n                    if embedded_guidance_scale is not None\n                    else None\n                )\n\n                # predict the noise residual\n                with torch.autocast(\n                    device_type=\"cuda\", dtype=target_dtype, enabled=autocast_enabled\n                ):\n                    noise_pred = self.transformer(  # For an input image (129, 192, 336) (1, 256, 256)\n                        latent_model_input,  # [2, 16, 33, 24, 42]\n                        t_expand,  # [2]\n                        text_states=prompt_embeds,  # [2, 256, 4096]\n                        text_mask=prompt_mask,  # [2, 256]\n                        text_states_2=prompt_embeds_2,  # [2, 768]\n                        freqs_cos=freqs_cis[0],  # [seqlen, head_dim]\n                        freqs_sin=freqs_cis[1],  # [seqlen, head_dim]\n                        guidance=guidance_expand,\n                        return_dict=True,\n                    )[\n                        \"x\"\n                    ]\n\n                # perform guidance\n                if self.do_classifier_free_guidance:\n                    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n                    noise_pred = noise_pred_uncond + self.guidance_scale * (\n                        noise_pred_text - noise_pred_uncond\n                    )\n\n                if self.do_classifier_free_guidance and self.guidance_rescale > 0.0:\n                    # Based on 3.4. in https://arxiv.org/pdf/2305.08891.pdf\n                    noise_pred = rescale_noise_cfg(\n                        noise_pred,\n                        noise_pred_text,\n                        guidance_rescale=self.guidance_rescale,\n                    )\n\n                # compute the previous noisy sample x_t -> x_t-1\n                latents = self.scheduler.step(\n                    noise_pred, t, latents, **extra_step_kwargs, return_dict=False\n                )[0]\n\n                if callback_on_step_end is not None:\n                    callback_kwargs = {}\n                    for k in callback_on_step_end_tensor_inputs:\n                        callback_kwargs[k] = locals()[k]\n                    callback_outputs = callback_on_step_end(self, i, t, callback_kwargs)\n\n                    latents = callback_outputs.pop(\"latents\", latents)\n                    prompt_embeds = callback_outputs.pop(\"prompt_embeds\", prompt_embeds)\n                    negative_prompt_embeds = callback_outputs.pop(\n                        \"negative_prompt_embeds\", negative_prompt_embeds\n                    )\n\n                # call the callback, if provided\n                if i == len(timesteps) - 1 or (\n                    (i + 1) > num_warmup_steps and (i + 1) % self.scheduler.order == 0\n                ):\n                    if progress_bar is not None:\n                        progress_bar.update()\n                    if callback is not None and i % callback_steps == 0:\n                        step_idx = i // getattr(self.scheduler, \"order\", 1)\n                        callback(step_idx, t, latents)\n\n                # Save the intermediate denoised video\n                # if i % 5 == 0 or i == len(timesteps) - 1:\n                #     print(f\"Denoising Step Idx {i}\")\n                #     image = self.haocheng_decode_latents(latents, vae_dtype, vae_autocast_enabled, enable_tiling, generator)\n                #     image = (image / 2 + 0.5).clamp(0, 1)\n                #     image = image.cpu().float()\n                #     import os\n                #     from ...utils.file_utils import save_videos_grid\n                #     os.makedirs(f\"visualization_result/{prompt[0][:15]}\", exist_ok=True)\n                #     save_videos_grid(image, f\"visualization_result/{prompt[0][:15]}/step_{i}.mp4\", fps=24)\n\n        if not output_type == \"latent\":\n            image = self.haocheng_decode_latents(latents, vae_dtype, vae_autocast_enabled, enable_tiling, generator)\n        else:\n            image = latents\n\n        image = (image / 2 + 0.5).clamp(0, 1)\n        # we always cast to float32 as this does not cause significant overhead and is compatible with bfloa16\n        image = image.cpu().float()\n\n        # Offload all models\n        self.maybe_free_model_hooks()\n\n        if not return_dict:\n            return image\n\n        return HunyuanVideoPipelineOutput(videos=image)\n"}
{"type": "source_file", "path": "svg/models/hyvideo/diffusion/schedulers/scheduling_flow_match_discrete.py", "content": "# Copyright 2024 Stability AI, Katherine Crowson and The HuggingFace Team. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n#\n# Modified from diffusers==0.29.2\n#\n# ==============================================================================\n\nfrom dataclasses import dataclass\nfrom typing import Optional, Tuple, Union\n\nimport numpy as np\nimport torch\n\nfrom diffusers.configuration_utils import ConfigMixin, register_to_config\nfrom diffusers.utils import BaseOutput, logging\nfrom diffusers.schedulers.scheduling_utils import SchedulerMixin\n\n\nlogger = logging.get_logger(__name__)  # pylint: disable=invalid-name\n\n\n@dataclass\nclass FlowMatchDiscreteSchedulerOutput(BaseOutput):\n    \"\"\"\n    Output class for the scheduler's `step` function output.\n\n    Args:\n        prev_sample (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)` for images):\n            Computed sample `(x_{t-1})` of previous timestep. `prev_sample` should be used as next model input in the\n            denoising loop.\n    \"\"\"\n\n    prev_sample: torch.FloatTensor\n\n\nclass FlowMatchDiscreteScheduler(SchedulerMixin, ConfigMixin):\n    \"\"\"\n    Euler scheduler.\n\n    This model inherits from [`SchedulerMixin`] and [`ConfigMixin`]. Check the superclass documentation for the generic\n    methods the library implements for all schedulers such as loading and saving.\n\n    Args:\n        num_train_timesteps (`int`, defaults to 1000):\n            The number of diffusion steps to train the model.\n        timestep_spacing (`str`, defaults to `\"linspace\"`):\n            The way the timesteps should be scaled. Refer to Table 2 of the [Common Diffusion Noise Schedules and\n            Sample Steps are Flawed](https://huggingface.co/papers/2305.08891) for more information.\n        shift (`float`, defaults to 1.0):\n            The shift value for the timestep schedule.\n        reverse (`bool`, defaults to `True`):\n            Whether to reverse the timestep schedule.\n    \"\"\"\n\n    _compatibles = []\n    order = 1\n\n    @register_to_config\n    def __init__(\n        self,\n        num_train_timesteps: int = 1000,\n        shift: float = 1.0,\n        reverse: bool = True,\n        solver: str = \"euler\",\n        n_tokens: Optional[int] = None,\n    ):\n        sigmas = torch.linspace(1, 0, num_train_timesteps + 1)\n\n        if not reverse:\n            sigmas = sigmas.flip(0)\n\n        self.sigmas = sigmas\n        # the value fed to model\n        self.timesteps = (sigmas[:-1] * num_train_timesteps).to(dtype=torch.float32)\n\n        self._step_index = None\n        self._begin_index = None\n\n        self.supported_solver = [\"euler\"]\n        if solver not in self.supported_solver:\n            raise ValueError(\n                f\"Solver {solver} not supported. Supported solvers: {self.supported_solver}\"\n            )\n\n    @property\n    def step_index(self):\n        \"\"\"\n        The index counter for current timestep. It will increase 1 after each scheduler step.\n        \"\"\"\n        return self._step_index\n\n    @property\n    def begin_index(self):\n        \"\"\"\n        The index for the first timestep. It should be set from pipeline with `set_begin_index` method.\n        \"\"\"\n        return self._begin_index\n\n    # Copied from diffusers.schedulers.scheduling_dpmsolver_multistep.DPMSolverMultistepScheduler.set_begin_index\n    def set_begin_index(self, begin_index: int = 0):\n        \"\"\"\n        Sets the begin index for the scheduler. This function should be run from pipeline before the inference.\n\n        Args:\n            begin_index (`int`):\n                The begin index for the scheduler.\n        \"\"\"\n        self._begin_index = begin_index\n\n    def _sigma_to_t(self, sigma):\n        return sigma * self.config.num_train_timesteps\n\n    def set_timesteps(\n        self,\n        num_inference_steps: int,\n        device: Union[str, torch.device] = None,\n        n_tokens: int = None,\n    ):\n        \"\"\"\n        Sets the discrete timesteps used for the diffusion chain (to be run before inference).\n\n        Args:\n            num_inference_steps (`int`):\n                The number of diffusion steps used when generating samples with a pre-trained model.\n            device (`str` or `torch.device`, *optional*):\n                The device to which the timesteps should be moved to. If `None`, the timesteps are not moved.\n            n_tokens (`int`, *optional*):\n                Number of tokens in the input sequence.\n        \"\"\"\n        self.num_inference_steps = num_inference_steps\n        \n        sigmas = torch.linspace(1, 0, num_inference_steps + 1)\n        sigmas = self.sd3_time_shift(sigmas)\n\n        if not self.config.reverse:\n            sigmas = 1 - sigmas\n\n        self.sigmas = sigmas\n        self.timesteps = (sigmas[:-1] * self.config.num_train_timesteps).to(\n            dtype=torch.float32, device=device\n        )\n\n        # Reset step index\n        self._step_index = None\n\n    def index_for_timestep(self, timestep, schedule_timesteps=None):\n        if schedule_timesteps is None:\n            schedule_timesteps = self.timesteps\n\n        indices = (schedule_timesteps == timestep).nonzero()\n\n        # The sigma index that is taken for the **very** first `step`\n        # is always the second index (or the last index if there is only 1)\n        # This way we can ensure we don't accidentally skip a sigma in\n        # case we start in the middle of the denoising schedule (e.g. for image-to-image)\n        pos = 1 if len(indices) > 1 else 0\n\n        return indices[pos].item()\n\n    def _init_step_index(self, timestep):\n        if self.begin_index is None:\n            if isinstance(timestep, torch.Tensor):\n                timestep = timestep.to(self.timesteps.device)\n            self._step_index = self.index_for_timestep(timestep)\n        else:\n            self._step_index = self._begin_index\n\n    def scale_model_input(\n        self, sample: torch.Tensor, timestep: Optional[int] = None\n    ) -> torch.Tensor:\n        return sample\n\n    def sd3_time_shift(self, t: torch.Tensor):\n        return (self.config.shift * t) / (1 + (self.config.shift - 1) * t)\n\n    def step(\n        self,\n        model_output: torch.FloatTensor,\n        timestep: Union[float, torch.FloatTensor],\n        sample: torch.FloatTensor,\n        return_dict: bool = True,\n    ) -> Union[FlowMatchDiscreteSchedulerOutput, Tuple]:\n        \"\"\"\n        Predict the sample from the previous timestep by reversing the SDE. This function propagates the diffusion\n        process from the learned model outputs (most often the predicted noise).\n\n        Args:\n            model_output (`torch.FloatTensor`):\n                The direct output from learned diffusion model.\n            timestep (`float`):\n                The current discrete timestep in the diffusion chain.\n            sample (`torch.FloatTensor`):\n                A current instance of a sample created by the diffusion process.\n            generator (`torch.Generator`, *optional*):\n                A random number generator.\n            n_tokens (`int`, *optional*):\n                Number of tokens in the input sequence.\n            return_dict (`bool`):\n                Whether or not to return a [`~schedulers.scheduling_euler_discrete.EulerDiscreteSchedulerOutput`] or\n                tuple.\n\n        Returns:\n            [`~schedulers.scheduling_euler_discrete.EulerDiscreteSchedulerOutput`] or `tuple`:\n                If return_dict is `True`, [`~schedulers.scheduling_euler_discrete.EulerDiscreteSchedulerOutput`] is\n                returned, otherwise a tuple is returned where the first element is the sample tensor.\n        \"\"\"\n\n        if (\n            isinstance(timestep, int)\n            or isinstance(timestep, torch.IntTensor)\n            or isinstance(timestep, torch.LongTensor)\n        ):\n            raise ValueError(\n                (\n                    \"Passing integer indices (e.g. from `enumerate(timesteps)`) as timesteps to\"\n                    \" `EulerDiscreteScheduler.step()` is not supported. Make sure to pass\"\n                    \" one of the `scheduler.timesteps` as a timestep.\"\n                ),\n            )\n\n        if self.step_index is None:\n            self._init_step_index(timestep)\n\n        # Upcast to avoid precision issues when computing prev_sample\n        sample = sample.to(torch.float32)\n\n        dt = self.sigmas[self.step_index + 1] - self.sigmas[self.step_index]\n\n        if self.config.solver == \"euler\":\n            prev_sample = sample + model_output.to(torch.float32) * dt\n        else:\n            raise ValueError(\n                f\"Solver {self.config.solver} not supported. Supported solvers: {self.supported_solver}\"\n            )\n\n        # upon completion increase step index by one\n        self._step_index += 1\n\n        if not return_dict:\n            return (prev_sample,)\n\n        return FlowMatchDiscreteSchedulerOutput(prev_sample=prev_sample)\n\n    def __len__(self):\n        return self.config.num_train_timesteps\n"}
{"type": "source_file", "path": "svg/models/hyvideo/modules/__init__.py", "content": "from .models import HYVideoDiffusionTransformer, HUNYUAN_VIDEO_CONFIG\n\n\ndef load_model(args, in_channels, out_channels, factor_kwargs):\n    \"\"\"load hunyuan video model\n\n    Args:\n        args (dict): model args\n        in_channels (int): input channels number\n        out_channels (int): output channels number\n        factor_kwargs (dict): factor kwargs\n\n    Returns:\n        model (nn.Module): The hunyuan video model\n    \"\"\"\n    if args.model in HUNYUAN_VIDEO_CONFIG.keys():\n        model = HYVideoDiffusionTransformer(\n            args,\n            in_channels=in_channels,\n            out_channels=out_channels,\n            **HUNYUAN_VIDEO_CONFIG[args.model],\n            **factor_kwargs,\n        )\n        return model\n    else:\n        raise NotImplementedError()\n"}
{"type": "source_file", "path": "svg/models/hyvideo/modules/activation_layers.py", "content": "import torch.nn as nn\n\n\ndef get_activation_layer(act_type):\n    \"\"\"get activation layer\n\n    Args:\n        act_type (str): the activation type\n\n    Returns:\n        torch.nn.functional: the activation layer\n    \"\"\"\n    if act_type == \"gelu\":\n        return lambda: nn.GELU()\n    elif act_type == \"gelu_tanh\":\n        # Approximate `tanh` requires torch >= 1.13\n        return lambda: nn.GELU(approximate=\"tanh\")\n    elif act_type == \"relu\":\n        return nn.ReLU\n    elif act_type == \"silu\":\n        return nn.SiLU\n    else:\n        raise ValueError(f\"Unknown activation type: {act_type}\")\n"}
{"type": "source_file", "path": "svg/models/hyvideo/modules/custom_models.py", "content": "import os\nfrom typing import Any, List, Tuple, Optional, Union, Dict\nfrom einops import rearrange\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom diffusers.models import ModelMixin\nfrom diffusers.configuration_utils import ConfigMixin, register_to_config\n\nfrom .activation_layers import get_activation_layer\nfrom .norm_layers import get_norm_layer\nfrom .embed_layers import TimestepEmbedder, PatchEmbed, TextProjection\nfrom .attenion import attention, parallel_attention, get_cu_seqlens\nfrom .posemb_layers import apply_rotary_emb\nfrom .mlp_layers import MLP, MLPEmbedder, FinalLayer\nfrom .modulate_layers import ModulateDiT, modulate, apply_gate\nfrom .token_refiner import SingleTokenRefiner\nfrom .models import MMDoubleStreamBlock, MMSingleStreamBlock\n\ntry:\n    import sys\n    sys.path.append('svg/kernels/build/')\n    import _kernels\n\n    def process_norm_rope1(img_attn_q_norm, img_attn_k_norm, img_q, img_k, img_v, freqs_cis):\n        # Cause Inefficiency\n        img_q = img_q.transpose(1, 2).contiguous()\n        img_k = img_k.transpose(1, 2).contiguous()\n        \n        # Apply QK-Norm if needed\n        _kernels.rms_norm_forward(img_q.view(-1, img_q.shape[-1]), img_attn_q_norm.weight, img_attn_q_norm.eps)\n        _kernels.rms_norm_forward(img_k.view(-1, img_k.shape[-1]), img_attn_k_norm.weight, img_attn_k_norm.eps)\n\n        # Apply RoPE if needed.\n        if freqs_cis is not None:\n            cos, sin = freqs_cis[0], freqs_cis[1]\n            _kernels.apply_qk_rope_inplace_cossin_txtlast(img_q, img_k, cos, sin, 0)\n\n        # Cause Inefficiency\n        img_q = img_q.transpose(1, 2).contiguous()\n        img_k = img_k.transpose(1, 2).contiguous()\n        return img_q, img_k\n\n    def process_norm_rope2(q_norm, k_norm, q, k, v, txt_len, freqs_cis):\n        # Cause Inefficiency\n        q = q.transpose(1, 2).contiguous()\n        k = k.transpose(1, 2).contiguous()\n        \n        # Apply QK-Norm if needed.\n        _kernels.rms_norm_forward(q.view(-1, q.shape[-1]), q_norm.weight, q_norm.eps)\n        _kernels.rms_norm_forward(k.view(-1, k.shape[-1]), k_norm.weight, k_norm.eps)\n\n        # Apply RoPE if needed.\n        if freqs_cis is not None:\n            cos, sin = freqs_cis[0], freqs_cis[1]\n            _kernels.apply_qk_rope_inplace_cossin_txtlast(q, k, cos, sin, txt_len)\n\n            # Cause Inefficiency\n            q = q.transpose(1, 2).contiguous()\n            k = k.transpose(1, 2).contiguous()\n        return q, k\n\nexcept ImportError:\n    import warnings\n    warnings.warn(\"Could not import RoPE / Norm kernels! Falling back to PyTorch implementation.\")\n\n    def process_norm_rope1(img_attn_q_norm, img_attn_k_norm, img_q, img_k, img_v, freqs_cis):\n        # Apply QK-Norm if needed\n        img_q = img_attn_q_norm(img_q).to(img_v)\n        img_k = img_attn_k_norm(img_k).to(img_v)\n\n        # Apply RoPE if needed.\n        if freqs_cis is not None:\n            img_qq, img_kk = apply_rotary_emb(img_q, img_k, freqs_cis, head_first=False)\n            assert (\n                img_qq.shape == img_q.shape and img_kk.shape == img_k.shape\n            ), f\"img_kk: {img_qq.shape}, img_q: {img_q.shape}, img_kk: {img_kk.shape}, img_k: {img_k.shape}\"\n            img_q, img_k = img_qq, img_kk\n        return img_q, img_k\n\n    def process_norm_rope2(q_norm, k_norm, q, k, v, txt_len, freqs_cis):\n        # Cause Inefficiency\n        q = q.transpose(1, 2).contiguous()\n        k = k.transpose(1, 2).contiguous()\n        \n        # Apply QK-Norm if needed.\n        q = q_norm(q).to(v)\n        k = k_norm(k).to(v)\n\n        # Apply RoPE if needed.\n        if freqs_cis is not None:\n            img_q, txt_q = q[:, :, :-txt_len, :], q[:, :, -txt_len:, :]\n            img_k, txt_k = k[:, :, :-txt_len, :], k[:, :, -txt_len:, :]\n\n            img_qq, img_kk = apply_rotary_emb(img_q, img_k, freqs_cis, head_first=True)\n            assert (\n                img_qq.shape == img_q.shape and img_kk.shape == img_k.shape\n            ), f\"img_kk: {img_qq.shape}, img_q: {img_q.shape}, img_kk: {img_kk.shape}, img_k: {img_k.shape}\"\n            img_q, img_k = img_qq, img_kk\n\n            q = torch.cat((img_q, txt_q), dim=2)\n            k = torch.cat((img_k, txt_k), dim=2)\n\n            # Cause Inefficiency\n            q = q.transpose(1, 2).contiguous()\n            k = k.transpose(1, 2).contiguous()\n        return q, k\n\n\nclass MMDoubleStreamBlock_Sparse(MMDoubleStreamBlock):\n    \"\"\"\n    A multimodal dit block with seperate modulation for\n    text and image/video, see more details (SD3): https://arxiv.org/abs/2403.03206\n                                     (Flux.1): https://github.com/black-forest-labs/flux\n    \"\"\"\n\n    def forward(\n        self,\n        img: torch.Tensor,\n        txt: torch.Tensor,\n        vec: torch.Tensor,\n        cu_seqlens_q: Optional[torch.Tensor] = None,\n        cu_seqlens_kv: Optional[torch.Tensor] = None,\n        max_seqlen_q: Optional[int] = None,\n        max_seqlen_kv: Optional[int] = None,\n        freqs_cis: tuple = None,\n        timestep: torch.Tensor = None,\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        \n        (\n            img_mod1_shift,\n            img_mod1_scale,\n            img_mod1_gate,\n            img_mod2_shift,\n            img_mod2_scale,\n            img_mod2_gate,\n        ) = self.img_mod(vec).chunk(6, dim=-1)\n        (\n            txt_mod1_shift,\n            txt_mod1_scale,\n            txt_mod1_gate,\n            txt_mod2_shift,\n            txt_mod2_scale,\n            txt_mod2_gate,\n        ) = self.txt_mod(vec).chunk(6, dim=-1)\n\n        # Prepare image for attention.\n        img_modulated = self.img_norm1(img)\n        img_modulated = modulate(\n            img_modulated, shift=img_mod1_shift, scale=img_mod1_scale\n        )\n\n        img_qkv = self.img_attn_qkv(img_modulated)\n\n        img_q, img_k, img_v = rearrange(\n            img_qkv, \"B L (K H D) -> K B L H D\", K=3, H=self.heads_num\n        )\n\n        img_q, img_k = process_norm_rope1(self.img_attn_q_norm, self.img_attn_k_norm, img_q, img_k, img_v, freqs_cis)\n        \n        # Prepare txt for attention.\n        txt_modulated = self.txt_norm1(txt)\n        txt_modulated = modulate(\n            txt_modulated, shift=txt_mod1_shift, scale=txt_mod1_scale\n        )\n        txt_qkv = self.txt_attn_qkv(txt_modulated)\n        txt_q, txt_k, txt_v = rearrange(\n            txt_qkv, \"B L (K H D) -> K B L H D\", K=3, H=self.heads_num\n        )\n\n        # Apply QK-Norm if needed.\n        txt_q = self.txt_attn_q_norm(txt_q).to(txt_v)\n        txt_k = self.txt_attn_k_norm(txt_k).to(txt_v)\n\n        # Run actual attention.\n        q = torch.cat((img_q, txt_q), dim=1)\n        k = torch.cat((img_k, txt_k), dim=1)\n        v = torch.cat((img_v, txt_v), dim=1)\n        assert (\n            cu_seqlens_q.shape[0] == 2 * img.shape[0] + 1\n        ), f\"cu_seqlens_q.shape:{cu_seqlens_q.shape}, img.shape[0]:{img.shape[0]}\"\n\n        # attention computation start\n        if not self.hybrid_seq_parallel_attn:\n            attn = attention(\n                q,\n                k,\n                v,\n                mode=\"sparse\",\n                cu_seqlens_q=cu_seqlens_q,\n                cu_seqlens_kv=cu_seqlens_kv,\n                max_seqlen_q=max_seqlen_q,\n                max_seqlen_kv=max_seqlen_kv,\n                batch_size=img_k.shape[0],\n                timestep=timestep,\n                layer_idx=self.layer_idx\n            )\n        else:\n            attn = parallel_attention(\n                self.hybrid_seq_parallel_attn,\n                q,\n                k,\n                v,\n                img_q_len=img_q.shape[1],\n                img_kv_len=img_k.shape[1],\n                cu_seqlens_q=cu_seqlens_q,\n                cu_seqlens_kv=cu_seqlens_kv\n            )\n    \n        # attention computation end\n\n        img_attn, txt_attn = attn[:, : img.shape[1]], attn[:, img.shape[1] :]\n\n        # Calculate the img bloks.\n        img = img + apply_gate(self.img_attn_proj(img_attn), gate=img_mod1_gate)\n        img = img + apply_gate(\n            self.img_mlp(\n                modulate(\n                    self.img_norm2(img), shift=img_mod2_shift, scale=img_mod2_scale\n                )\n            ),\n            gate=img_mod2_gate,\n        )\n\n        # Calculate the txt bloks.\n        txt = txt + apply_gate(self.txt_attn_proj(txt_attn), gate=txt_mod1_gate)\n        txt = txt + apply_gate(\n            self.txt_mlp(\n                modulate(\n                    self.txt_norm2(txt), shift=txt_mod2_shift, scale=txt_mod2_scale\n                )\n            ),\n            gate=txt_mod2_gate,\n        )\n\n        return img, txt\n\n\nclass MMSingleStreamBlock_Sparse(MMSingleStreamBlock):\n    \"\"\"\n    A DiT block with parallel linear layers as described in\n    https://arxiv.org/abs/2302.05442 and adapted modulation interface.\n    Also refer to (SD3): https://arxiv.org/abs/2403.03206\n                  (Flux.1): https://github.com/black-forest-labs/flux\n    \"\"\"\n\n    def forward(\n        self,\n        x: torch.Tensor,\n        vec: torch.Tensor,\n        txt_len: int,\n        cu_seqlens_q: Optional[torch.Tensor] = None,\n        cu_seqlens_kv: Optional[torch.Tensor] = None,\n        max_seqlen_q: Optional[int] = None,\n        max_seqlen_kv: Optional[int] = None,\n        freqs_cis: Tuple[torch.Tensor, torch.Tensor] = None,\n        timestep: torch.Tensor = None,\n    ) -> torch.Tensor:\n        mod_shift, mod_scale, mod_gate = self.modulation(vec).chunk(3, dim=-1)\n        x_mod = modulate(self.pre_norm(x), shift=mod_shift, scale=mod_scale)\n        qkv, mlp = torch.split(\n            self.linear1(x_mod), [3 * self.hidden_size, self.mlp_hidden_dim], dim=-1\n        )\n\n        q, k, v = rearrange(qkv, \"B L (K H D) -> K B L H D\", K=3, H=self.heads_num)\n\n        q, k = process_norm_rope2(self.q_norm, self.k_norm, q, k, v, txt_len, freqs_cis)\n        \n        # Compute attention.\n        assert (\n            cu_seqlens_q.shape[0] == 2 * x.shape[0] + 1\n        ), f\"cu_seqlens_q.shape:{cu_seqlens_q.shape}, x.shape[0]:{x.shape[0]}\"\n\n        # attention computation start\n        attn = attention(\n            q,\n            k,\n            v,\n            mode=\"sparse\",\n            cu_seqlens_q=cu_seqlens_q,\n            cu_seqlens_kv=cu_seqlens_kv,\n            max_seqlen_q=max_seqlen_q,\n            max_seqlen_kv=max_seqlen_kv,\n            batch_size=x.shape[0],\n            timestep=timestep,\n            layer_idx=self.layer_idx\n        )\n        # attention computation end\n\n        # Compute activation in mlp stream, cat again and run second linear layer.\n        output = self.linear2(torch.cat((attn, self.mlp_act(mlp)), 2))\n\n        return x + apply_gate(output, gate=mod_gate)\n\n\ndef replace_sparse_forward():\n    MMDoubleStreamBlock.forward = MMDoubleStreamBlock_Sparse.forward\n    MMSingleStreamBlock.forward = MMSingleStreamBlock_Sparse.forward\n\n"}
{"type": "source_file", "path": "svg/models/hyvideo/diffusion/__init__.py", "content": "from .pipelines import HunyuanVideoPipeline\nfrom .schedulers import FlowMatchDiscreteScheduler\n"}
{"type": "source_file", "path": "svg/models/hyvideo/modules/fp8_optimization.py", "content": "import os\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\n\ndef get_fp_maxval(bits=8, mantissa_bit=3, sign_bits=1):\n    _bits = torch.tensor(bits)\n    _mantissa_bit = torch.tensor(mantissa_bit)\n    _sign_bits = torch.tensor(sign_bits)\n    M = torch.clamp(torch.round(_mantissa_bit), 1, _bits - _sign_bits)\n    E = _bits - _sign_bits - M\n    bias = 2 ** (E - 1) - 1\n    mantissa = 1\n    for i in range(mantissa_bit - 1):\n        mantissa += 1 / (2 ** (i+1))\n    maxval = mantissa * 2 ** (2**E - 1 - bias)\n    return maxval\n\ndef quantize_to_fp8(x, bits=8, mantissa_bit=3, sign_bits=1):\n    \"\"\"\n    Default is E4M3.\n    \"\"\"\n    bits = torch.tensor(bits)\n    mantissa_bit = torch.tensor(mantissa_bit)\n    sign_bits = torch.tensor(sign_bits)\n    M = torch.clamp(torch.round(mantissa_bit), 1, bits - sign_bits)\n    E = bits - sign_bits - M\n    bias = 2 ** (E - 1) - 1\n    mantissa = 1\n    for i in range(mantissa_bit - 1):\n        mantissa += 1 / (2 ** (i+1))\n    maxval = mantissa * 2 ** (2**E - 1 - bias)\n    minval = - maxval\n    minval = - maxval if sign_bits == 1 else torch.zeros_like(maxval)\n    input_clamp = torch.min(torch.max(x, minval), maxval)\n    log_scales = torch.clamp((torch.floor(torch.log2(torch.abs(input_clamp)) + bias)).detach(), 1.0)\n    log_scales = 2.0 ** (log_scales - M - bias.type(x.dtype))\n    # dequant\n    qdq_out = torch.round(input_clamp / log_scales) * log_scales\n    return qdq_out, log_scales\n\ndef fp8_tensor_quant(x, scale, bits=8, mantissa_bit=3, sign_bits=1):\n    for i in range(len(x.shape) - 1):\n        scale = scale.unsqueeze(-1)\n    new_x = x / scale\n    quant_dequant_x, log_scales = quantize_to_fp8(new_x, bits=bits, mantissa_bit=mantissa_bit, sign_bits=sign_bits)\n    return quant_dequant_x, scale, log_scales\n\ndef fp8_activation_dequant(qdq_out, scale, dtype):\n    qdq_out = qdq_out.type(dtype)\n    quant_dequant_x = qdq_out * scale.to(dtype)\n    return quant_dequant_x\n\ndef fp8_linear_forward(cls, original_dtype, input):\n    weight_dtype = cls.weight.dtype\n    #####\n    if cls.weight.dtype != torch.float8_e4m3fn:\n        maxval = get_fp_maxval()\n        scale = torch.max(torch.abs(cls.weight.flatten())) / maxval\n        linear_weight, scale, log_scales = fp8_tensor_quant(cls.weight, scale)\n        linear_weight = linear_weight.to(torch.float8_e4m3fn)\n        weight_dtype = linear_weight.dtype\n    else:\n        scale = cls.fp8_scale.to(cls.weight.device)\n        linear_weight = cls.weight\n    #####\n\n    if weight_dtype == torch.float8_e4m3fn and cls.weight.sum() != 0:\n        if True or len(input.shape) == 3:\n            cls_dequant = fp8_activation_dequant(linear_weight, scale, original_dtype)\n            if cls.bias != None:\n                output = F.linear(input, cls_dequant, cls.bias)\n            else:\n                output = F.linear(input, cls_dequant)\n            return output\n        else:\n            return cls.original_forward(input.to(original_dtype))\n    else:\n        return cls.original_forward(input)\n\ndef convert_fp8_linear(module, dit_weight_path, original_dtype, params_to_keep={}):\n    setattr(module, \"fp8_matmul_enabled\", True)\n\n    # loading fp8 mapping file\n    fp8_map_path = dit_weight_path.replace('.pt', '_map.pt')\n    if os.path.exists(fp8_map_path):\n        fp8_map = torch.load(fp8_map_path, map_location=lambda storage, loc: storage)\n    else:\n        raise ValueError(f\"Invalid fp8_map path: {fp8_map_path}.\")\n\n    fp8_layers = []\n    for key, layer in module.named_modules():\n        if isinstance(layer, nn.Linear) and ('double_blocks' in key or 'single_blocks' in key):\n            fp8_layers.append(key)\n            original_forward = layer.forward\n            layer.weight = torch.nn.Parameter(layer.weight.to(torch.float8_e4m3fn))\n            setattr(layer, \"fp8_scale\", fp8_map[key].to(dtype=original_dtype))\n            setattr(layer, \"original_forward\", original_forward)\n            setattr(layer, \"forward\", lambda input, m=layer: fp8_linear_forward(m, original_dtype, input))\n    \n\n"}
{"type": "source_file", "path": "svg/models/hyvideo/diffusion/pipelines/__init__.py", "content": "from .pipeline_hunyuan_video import HunyuanVideoPipeline\n"}
{"type": "source_file", "path": "svg/models/hyvideo/modules/attenion.py", "content": "import math\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.nn.attention.flex_attention import flex_attention\n\nfrom flash_attn.flash_attn_interface import flash_attn_varlen_func\n\n\nfrom .utils import create_block_mask_cached, generate_temporal_head_mask_mod\nfrom .placement import hunyuan_sparse_head_placement, hunyuan_hidden_states_placement, ref_hunyuan_sparse_head_placement, ref_hunyuan_hidden_states_placement \n\ntry:\n    import flash_attn\n    from flash_attn.flash_attn_interface import _flash_attn_forward\n    from flash_attn.flash_attn_interface import flash_attn_varlen_func\nexcept ImportError:\n    flash_attn = None\n    flash_attn_varlen_func = None\n    _flash_attn_forward = None\n\n\nflex_attention = torch.compile(flex_attention, dynamic=False)\ntorch._dynamo.config.cache_size_limit = 192 * 3\ntorch._dynamo.config.accumulated_cache_size_limit = 192 * 3\n\n\nMEMORY_LAYOUT = {\n    \"flash\": (\n        lambda x: x.view(x.shape[0] * x.shape[1], *x.shape[2:]),\n        lambda x: x,\n    ),\n    \"sparse\": (\n        lambda x: x.transpose(1, 2).contiguous(),\n        lambda x: x.transpose(1, 2).contiguous(),\n    ),\n    \"torch\": (\n        lambda x: x.transpose(1, 2),\n        lambda x: x.transpose(1, 2),\n    ),\n    \"vanilla\": (\n        lambda x: x.transpose(1, 2),\n        lambda x: x.transpose(1, 2),\n    ),\n}\n\nclass Hunyuan_SparseAttn:\n    num_sampled_rows = 32\n    attention_masks = None\n\n    context_length = 256\n    num_frame = 33\n    frame_size = 3600\n\n    first_layers_fp = 0\n    first_times_fp = 0\n\n    sample_mse_max_row = 10000\n    block_mask = None\n    \n\n    def __init__(self):  \n        if not hasattr(F, \"scaled_dot_product_attention\"):\n            raise ImportError(\"Hunyuan_SparseAttn requires PyTorch 2.0, please upgrade PyTorch.\")\n\n    @classmethod\n    def sample_mse(self, query, key, value):\n        assert len(self.attention_masks) == 2\n\n        cfg, num_heads, seq_len, dim = query.size()\n        num_sampled_rows = min(self.num_sampled_rows, seq_len)\n        sampled_rows = torch.randint(low=0, high=self.sample_mse_max_row, size=(num_sampled_rows,))\n        sampled_q = query[:, :, sampled_rows, :]\n        sampled_qk_scores = torch.matmul(sampled_q, key.transpose(-2, -1)) / (dim**0.5)\n    \n        sampled_attn_weights = F.softmax(sampled_qk_scores, dim=-1)\n        sampled_golden_hidden_states = torch.matmul(sampled_attn_weights, value)  # (1, seq_len, dim)\n\n        sampled_mses = torch.zeros(len(self.attention_masks), cfg, num_heads, device=query.device, dtype=query.dtype)\n\n        # Only have Tri-diagonal and Striped\n        for mask_idx, attn_mask in enumerate(self.attention_masks):\n            sampled_attention_mask = attn_mask[sampled_rows, :]\n            sampled_attention_scores = sampled_qk_scores.masked_fill(sampled_attention_mask == 0, float('-inf'))\n            sampled_attn_weights = F.softmax(sampled_attention_scores, dim=-1)\n            sampled_hidden_states = torch.matmul(sampled_attn_weights, value)\n            mse = torch.mean((sampled_hidden_states - sampled_golden_hidden_states) ** 2, dim=(2, 3))\n            sampled_mses[mask_idx] = mse\n\n        return sampled_mses\n\n    @classmethod\n    def sparse_flex_attention(self, query, key, value, block_mask):\n        return flex_attention(query, key, value, block_mask=block_mask)\n\n    @classmethod\n    def sparse_head_placement(self, query, key, value, query_out, key_out, value_out, best_mask_idx, context_length, num_frame, frame_size):\n        \n        query_out, key_out, value_out = ref_hunyuan_sparse_head_placement(query, key, value, best_mask_idx, context_length, num_frame, frame_size)\n\n        return query_out, key_out, value_out\n\n    @classmethod\n    def fast_sparse_head_placement(self, query, key, value, query_out, key_out, value_out, best_mask_idx, context_length, num_frame, frame_size):\n\n        hunyuan_sparse_head_placement(query, key, value, query_out, key_out, value_out, best_mask_idx, context_length, num_frame, frame_size)\n\n        return query_out, key_out, value_out\n\n    @classmethod\n    def hidden_states_placement(self, \\\n        hidden_states, output_hidden_states, \\\n        best_mask_idx, context_length, num_frame, frame_size\n    ):\n        ref_hunyuan_hidden_states_placement(hidden_states, output_hidden_states, best_mask_idx, context_length, num_frame, frame_size)\n\n    @classmethod\n    def fast_hidden_states_placement(self, \\\n        hidden_states, output_hidden_states, \\\n        best_mask_idx, context_length, num_frame, frame_size\n    ):\n        hunyuan_hidden_states_placement(hidden_states, output_hidden_states, best_mask_idx, context_length, num_frame, frame_size)\n\n    @classmethod\n    def attention_core_logic(\n        self,\n        query: torch.Tensor,\n        key: torch.Tensor,\n        value: torch.Tensor,\n        timestep,\n        layer_idx,\n        cu_seqlens_q,\n        cu_seqlens_kv,\n        max_seqlen_q,\n        max_seqlen_kv,\n    ):\n        cfg, num_heads, seq_len, dim = query.size()\n        \n        context_length, num_frame, frame_size = self.context_length, self.num_frame, self.frame_size\n\n        assert seq_len == context_length + num_frame * frame_size, \\\n            f\"Query Shape: {seq_len} is not equivalent to {context_length} + {num_frame} * {frame_size}\"\n\n        sampled_mses = self.sample_mse(query, key, value)\n        best_mask_idx = torch.argmin(sampled_mses, dim=0)\n\n\n        output_hidden_states = torch.zeros_like(query)\n\n        query_out, key_out, value_out = torch.zeros_like(query), torch.zeros_like(key), torch.zeros_like(value)\n\n        query_out, key_out, value_out = self.fast_sparse_head_placement(query, key, value, query_out, key_out, value_out, best_mask_idx, context_length, num_frame, frame_size)\n\n        hidden_states = self.sparse_flex_attention(query_out, key_out, value_out, block_mask=self.block_mask)\n\n        self.fast_hidden_states_placement(hidden_states, output_hidden_states, best_mask_idx, context_length, num_frame, frame_size)\n\n        return output_hidden_states.reshape(cfg, num_heads, seq_len, dim)\n\n\ndef get_cu_seqlens(text_mask, img_len):\n    \"\"\"Calculate cu_seqlens_q, cu_seqlens_kv using text_mask and img_len\n\n    Args:\n        text_mask (torch.Tensor): the mask of text\n        img_len (int): the length of image\n\n    Returns:\n        torch.Tensor: the calculated cu_seqlens for flash attention\n    \"\"\"\n    batch_size = text_mask.shape[0]\n    text_len = text_mask.sum(dim=1)\n    max_len = text_mask.shape[1] + img_len\n\n    cu_seqlens = torch.zeros([2 * batch_size + 1], dtype=torch.int32, device=\"cuda\")\n\n    for i in range(batch_size):\n        s = text_len[i] + img_len\n        s1 = i * max_len + s\n        s2 = (i + 1) * max_len\n        cu_seqlens[2 * i + 1] = s1\n        cu_seqlens[2 * i + 2] = s2\n\n    return cu_seqlens\n\n\ndef attention(\n    q,\n    k,\n    v,\n    mode=\"flash\",\n    drop_rate=0,\n    attn_mask=None,\n    causal=False,\n    cu_seqlens_q=None,\n    cu_seqlens_kv=None,\n    max_seqlen_q=None,\n    max_seqlen_kv=None,\n    batch_size=1,\n    timestep=None,\n    layer_idx=None\n):\n    \"\"\"\n    Perform QKV self attention.\n\n    Args:\n        q (torch.Tensor): Query tensor with shape [b, s, a, d], where a is the number of heads.\n        k (torch.Tensor): Key tensor with shape [b, s1, a, d]\n        v (torch.Tensor): Value tensor with shape [b, s1, a, d]\n        mode (str): Attention mode. Choose from 'self_flash', 'cross_flash', 'torch', and 'vanilla'.\n        drop_rate (float): Dropout rate in attention map. (default: 0)\n        attn_mask (torch.Tensor): Attention mask with shape [b, s1] (cross_attn), or [b, a, s, s1] (torch or vanilla).\n            (default: None)\n        causal (bool): Whether to use causal attention. (default: False)\n        cu_seqlens_q (torch.Tensor): dtype torch.int32. The cumulative sequence lengths of the sequences in the batch,\n            used to index into q.\n        cu_seqlens_kv (torch.Tensor): dtype torch.int32. The cumulative sequence lengths of the sequences in the batch,\n            used to index into kv.\n        max_seqlen_q (int): The maximum sequence length in the batch of q.\n        max_seqlen_kv (int): The maximum sequence length in the batch of k and v.\n\n    Returns:\n        torch.Tensor: Output tensor after self attention with shape [b, s, ad]\n    \"\"\"\n\n    # Some Preprocess\n    if mode == \"sparse\":\n        assert torch.allclose(cu_seqlens_q, cu_seqlens_kv)\n        assert cu_seqlens_kv is not None\n                \n        # Determine if we use Full Attention to calculate  # TODO  \n        full_attention_flag = False\n        if layer_idx < 42 * Hunyuan_SparseAttn.first_layers_fp:\n            full_attention_flag = True\n        if timestep > 1000 * (1 - Hunyuan_SparseAttn.first_times_fp):\n            full_attention_flag = True\n\n        if full_attention_flag:    \n            mode = \"flash\"\n        else:\n            mode = \"sparse\"\n\n    pre_attn_layout, post_attn_layout = MEMORY_LAYOUT[mode]\n    q = pre_attn_layout(q)\n    k = pre_attn_layout(k)\n    v = pre_attn_layout(v)\n\n    if mode == \"torch\":\n        if attn_mask is not None and attn_mask.dtype != torch.bool:\n            attn_mask = attn_mask.to(q.dtype)\n        x = F.scaled_dot_product_attention(\n            q, k, v, attn_mask=attn_mask, dropout_p=drop_rate, is_causal=causal\n        )\n    elif mode == \"sparse\":\n        x = Hunyuan_SparseAttn.attention_core_logic(\n            q, k, v, timestep, layer_idx,\n            cu_seqlens_q,\n            cu_seqlens_kv,\n            max_seqlen_q,\n            max_seqlen_kv,\n        )\n    elif mode == \"flash\":\n        x = flash_attn_varlen_func(\n            q,\n            k,\n            v,\n            cu_seqlens_q,\n            cu_seqlens_kv,\n            max_seqlen_q,\n            max_seqlen_kv,\n        )\n        # x with shape [(bxs), a, d]\n        x = x.view(\n            batch_size, max_seqlen_q, x.shape[-2], x.shape[-1]\n        )  # reshape x to [b, s, a, d]\n    elif mode == \"vanilla\":\n        scale_factor = 1 / math.sqrt(q.size(-1))\n\n        b, a, s, _ = q.shape\n        s1 = k.size(2)\n        attn_bias = torch.zeros(b, a, s, s1, dtype=q.dtype, device=q.device)\n        if causal:\n            # Only applied to self attention\n            assert (\n                attn_mask is None\n            ), \"Causal mask and attn_mask cannot be used together\"\n            temp_mask = torch.ones(b, a, s, s, dtype=torch.bool, device=q.device).tril(\n                diagonal=0\n            )\n            attn_bias.masked_fill_(temp_mask.logical_not(), float(\"-inf\"))\n            attn_bias.to(q.dtype)\n\n        if attn_mask is not None:\n            if attn_mask.dtype == torch.bool:\n                attn_bias.masked_fill_(attn_mask.logical_not(), float(\"-inf\"))\n            else:\n                attn_bias += attn_mask\n\n        # TODO: Maybe force q and k to be float32 to avoid numerical overflow\n        attn = (q @ k.transpose(-2, -1)) * scale_factor\n        attn += attn_bias\n        attn = attn.softmax(dim=-1)\n        attn = torch.dropout(attn, p=drop_rate, train=True)\n        x = attn @ v\n    else:\n        raise NotImplementedError(f\"Unsupported attention mode: {mode}\")\n\n    x = post_attn_layout(x)\n    b, s, a, d = x.shape\n    out = x.reshape(b, s, -1)\n\n    return out\n\n\ndef parallel_attention(\n    hybrid_seq_parallel_attn,\n    q,\n    k,\n    v,\n    img_q_len,\n    img_kv_len,\n    cu_seqlens_q,\n    cu_seqlens_kv\n):\n    attn1 = hybrid_seq_parallel_attn(\n        None,\n        q[:, :img_q_len, :, :],\n        k[:, :img_kv_len, :, :],\n        v[:, :img_kv_len, :, :],\n        dropout_p=0.0,\n        causal=False,\n        joint_tensor_query=q[:,img_q_len:cu_seqlens_q[1]],\n        joint_tensor_key=k[:,img_kv_len:cu_seqlens_kv[1]],\n        joint_tensor_value=v[:,img_kv_len:cu_seqlens_kv[1]],\n        joint_strategy=\"rear\",\n    )\n    if flash_attn.__version__ >= '2.7.0':\n        attn2, *_ = _flash_attn_forward(\n            q[:,cu_seqlens_q[1]:],\n            k[:,cu_seqlens_kv[1]:],\n            v[:,cu_seqlens_kv[1]:],\n            dropout_p=0.0,\n            softmax_scale=q.shape[-1] ** (-0.5),\n            causal=False,\n            window_size_left=-1,\n            window_size_right=-1,\n            softcap=0.0,\n            alibi_slopes=None,\n            return_softmax=False,\n        )\n    else:\n        attn2, *_ = _flash_attn_forward(\n            q[:,cu_seqlens_q[1]:],\n            k[:,cu_seqlens_kv[1]:],\n            v[:,cu_seqlens_kv[1]:],\n            dropout_p=0.0,\n            softmax_scale=q.shape[-1] ** (-0.5),\n            causal=False,\n            window_size=(-1, -1),\n            softcap=0.0,\n            alibi_slopes=None,\n            return_softmax=False,\n        )\n    attn = torch.cat([attn1, attn2], dim=1)\n    b, s, a, d = attn.shape\n    attn = attn.reshape(b, s, -1)\n\n    return attn\n\n\ndef prepare_flexattention(cfg_size, num_head, head_dim, dtype, device, context_length, prompt_length, num_frame, frame_size, \\\n    diag_width=1, multiplier=2\n):\n    assert diag_width == multiplier\n    seq_len = context_length + num_frame * frame_size\n    query, key, value = [torch.zeros((1, cfg_size * num_head, seq_len, head_dim), dtype=dtype, device=device) for _ in range(3)]\n\n    mask_mod = generate_temporal_head_mask_mod(context_length, prompt_length, num_frame, frame_size, mul=multiplier)\n    block_mask = create_block_mask_cached(mask_mod, None, None, seq_len, seq_len, device=device, _compile=True)\n\n    hidden_states = flex_attention(query, key, value, block_mask=block_mask)\n\n    return block_mask\n"}
{"type": "source_file", "path": "svg/models/hyvideo/inference.py", "content": "import os\nimport time\nimport random\nimport functools\nfrom typing import List, Optional, Tuple, Union\n\nfrom pathlib import Path\nfrom loguru import logger\n\nimport torch\nimport torch.distributed as dist\nfrom .constants import PROMPT_TEMPLATE, NEGATIVE_PROMPT, PRECISION_TO_TYPE\nfrom .vae import load_vae\nfrom .modules import load_model\nfrom .text_encoder import TextEncoder\nfrom .utils.data_utils import align_to\nfrom .modules.posemb_layers import get_nd_rotary_pos_embed\nfrom .modules.fp8_optimization import convert_fp8_linear\nfrom .diffusion.schedulers import FlowMatchDiscreteScheduler\nfrom .diffusion.pipelines import HunyuanVideoPipeline\n\ntry:\n    import xfuser\n    from xfuser.core.distributed import (\n        get_sequence_parallel_world_size,\n        get_sequence_parallel_rank,\n        get_sp_group,\n        initialize_model_parallel,\n        init_distributed_environment\n    )\nexcept:\n    xfuser = None\n    get_sequence_parallel_world_size = None\n    get_sequence_parallel_rank = None\n    get_sp_group = None\n    initialize_model_parallel = None\n    init_distributed_environment = None\n\n\ndef parallelize_transformer(pipe):\n    transformer = pipe.transformer\n    original_forward = transformer.forward\n\n    @functools.wraps(transformer.__class__.forward)\n    def new_forward(\n        self,\n        x: torch.Tensor,\n        t: torch.Tensor,  # Should be in range(0, 1000).\n        text_states: torch.Tensor = None,\n        text_mask: torch.Tensor = None,  # Now we don't use it.\n        text_states_2: Optional[torch.Tensor] = None,  # Text embedding for modulation.\n        freqs_cos: Optional[torch.Tensor] = None,\n        freqs_sin: Optional[torch.Tensor] = None,\n        guidance: torch.Tensor = None,  # Guidance for modulation, should be cfg_scale x 1000.\n        return_dict: bool = True,\n    ):\n        if x.shape[-2] // 2 % get_sequence_parallel_world_size() == 0:\n            # try to split x by height\n            split_dim = -2\n        elif x.shape[-1] // 2 % get_sequence_parallel_world_size() == 0:\n            # try to split x by width\n            split_dim = -1\n        else:\n            raise ValueError(f\"Cannot split video sequence into ulysses_degree x ring_degree ({get_sequence_parallel_world_size()}) parts evenly\")\n\n        # patch sizes for the temporal, height, and width dimensions are 1, 2, and 2.\n        temporal_size, h, w = x.shape[2], x.shape[3] // 2, x.shape[4] // 2\n\n        x = torch.chunk(x, get_sequence_parallel_world_size(),dim=split_dim)[get_sequence_parallel_rank()]\n\n        dim_thw = freqs_cos.shape[-1]\n        freqs_cos = freqs_cos.reshape(temporal_size, h, w, dim_thw)\n        freqs_cos = torch.chunk(freqs_cos, get_sequence_parallel_world_size(),dim=split_dim - 1)[get_sequence_parallel_rank()]\n        freqs_cos = freqs_cos.reshape(-1, dim_thw)\n        dim_thw = freqs_sin.shape[-1]\n        freqs_sin = freqs_sin.reshape(temporal_size, h, w, dim_thw)\n        freqs_sin = torch.chunk(freqs_sin, get_sequence_parallel_world_size(),dim=split_dim - 1)[get_sequence_parallel_rank()]\n        freqs_sin = freqs_sin.reshape(-1, dim_thw)\n        \n        from xfuser.core.long_ctx_attention import xFuserLongContextAttention\n        \n        for block in transformer.double_blocks + transformer.single_blocks:\n            block.hybrid_seq_parallel_attn = xFuserLongContextAttention()\n\n        output = original_forward(\n            x,\n            t,\n            text_states,\n            text_mask,\n            text_states_2,\n            freqs_cos,\n            freqs_sin,\n            guidance,\n            return_dict,\n        )\n\n        return_dict = not isinstance(output, tuple)\n        sample = output[\"x\"]\n        sample = get_sp_group().all_gather(sample, dim=split_dim)\n        output[\"x\"] = sample\n        return output\n\n    new_forward = new_forward.__get__(transformer)\n    transformer.forward = new_forward\n    \n\nclass Inference(object):\n    def __init__(\n        self,\n        args,\n        vae,\n        vae_kwargs,\n        text_encoder,\n        model,\n        text_encoder_2=None,\n        pipeline=None,\n        use_cpu_offload=False,\n        device=None,\n        logger=None,\n        parallel_args=None,\n    ):\n        self.vae = vae\n        self.vae_kwargs = vae_kwargs\n\n        self.text_encoder = text_encoder\n        self.text_encoder_2 = text_encoder_2\n\n        self.model = model\n        self.pipeline = pipeline\n        self.use_cpu_offload = use_cpu_offload\n\n        self.args = args\n        self.device = (\n            device\n            if device is not None\n            else \"cuda\"\n            if torch.cuda.is_available()\n            else \"cpu\"\n        )\n        self.logger = logger\n        self.parallel_args = parallel_args\n\n    @classmethod\n    def from_pretrained(cls, pretrained_model_path, args, device=None, **kwargs):\n        \"\"\"\n        Initialize the Inference pipeline.\n\n        Args:\n            pretrained_model_path (str or pathlib.Path): The model path, including t2v, text encoder and vae checkpoints.\n            args (argparse.Namespace): The arguments for the pipeline.\n            device (int): The device for inference. Default is 0.\n        \"\"\"\n        # ========================================================================\n        logger.info(f\"Got text-to-video model root path: {pretrained_model_path}\")\n        \n        # ==================== Initialize Distributed Environment ================\n        if args.ulysses_degree > 1 or args.ring_degree > 1:\n            assert xfuser is not None, \\\n                \"Ulysses Attention and Ring Attention requires xfuser package.\"\n\n            assert args.use_cpu_offload is False, \\\n                \"Cannot enable use_cpu_offload in the distributed environment.\"\n\n            dist.init_process_group(\"nccl\")\n\n            assert dist.get_world_size() == args.ring_degree * args.ulysses_degree, \\\n                \"number of GPUs should be equal to ring_degree * ulysses_degree.\"\n\n            init_distributed_environment(rank=dist.get_rank(), world_size=dist.get_world_size())\n            \n            initialize_model_parallel(\n                sequence_parallel_degree=dist.get_world_size(),\n                ring_degree=args.ring_degree,\n                ulysses_degree=args.ulysses_degree,\n            )\n            device = torch.device(f\"cuda:{os.environ['LOCAL_RANK']}\")\n        else:\n            if device is None:\n                device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n        parallel_args = {\"ulysses_degree\": args.ulysses_degree, \"ring_degree\": args.ring_degree}\n\n        # ======================== Get the args path =============================\n\n        # Disable gradient\n        torch.set_grad_enabled(False)\n\n        # =========================== Build main model ===========================\n        logger.info(\"Building model...\")\n        factor_kwargs = {\"device\": device, \"dtype\": PRECISION_TO_TYPE[args.precision]}\n        in_channels = args.latent_channels\n        out_channels = args.latent_channels\n\n        model = load_model(\n            args,\n            in_channels=in_channels,\n            out_channels=out_channels,\n            factor_kwargs=factor_kwargs,\n        )\n        if args.use_fp8:\n            convert_fp8_linear(model, args.dit_weight, original_dtype=PRECISION_TO_TYPE[args.precision])\n        model = model.to(device)\n        model = Inference.load_state_dict(args, model, pretrained_model_path)\n        model.eval()\n\n        # ============================= Build extra models ========================\n        # VAE\n        vae, _, s_ratio, t_ratio = load_vae(\n            args.vae,\n            args.vae_precision,\n            logger=logger,\n            device=device if not args.use_cpu_offload else \"cpu\",\n        )\n        vae_kwargs = {\"s_ratio\": s_ratio, \"t_ratio\": t_ratio}\n\n        # Text encoder\n        if args.prompt_template_video is not None:\n            crop_start = PROMPT_TEMPLATE[args.prompt_template_video].get(\n                \"crop_start\", 0\n            )\n        elif args.prompt_template is not None:\n            crop_start = PROMPT_TEMPLATE[args.prompt_template].get(\"crop_start\", 0)\n        else:\n            crop_start = 0\n        max_length = args.text_len + crop_start\n\n        # prompt_template\n        prompt_template = (\n            PROMPT_TEMPLATE[args.prompt_template]\n            if args.prompt_template is not None\n            else None\n        )\n\n        # prompt_template_video\n        prompt_template_video = (\n            PROMPT_TEMPLATE[args.prompt_template_video]\n            if args.prompt_template_video is not None\n            else None\n        )\n\n        text_encoder = TextEncoder(\n            text_encoder_type=args.text_encoder,\n            max_length=max_length,\n            text_encoder_precision=args.text_encoder_precision,\n            tokenizer_type=args.tokenizer,\n            prompt_template=prompt_template,\n            prompt_template_video=prompt_template_video,\n            hidden_state_skip_layer=args.hidden_state_skip_layer,\n            apply_final_norm=args.apply_final_norm,\n            reproduce=args.reproduce,\n            logger=logger,\n            device=device if not args.use_cpu_offload else \"cpu\",\n        )\n        text_encoder_2 = None\n        if args.text_encoder_2 is not None:\n            text_encoder_2 = TextEncoder(\n                text_encoder_type=args.text_encoder_2,\n                max_length=args.text_len_2,\n                text_encoder_precision=args.text_encoder_precision_2,\n                tokenizer_type=args.tokenizer_2,\n                reproduce=args.reproduce,\n                logger=logger,\n                device=device if not args.use_cpu_offload else \"cpu\",\n            )\n\n        return cls(\n            args=args,\n            vae=vae,\n            vae_kwargs=vae_kwargs,\n            text_encoder=text_encoder,\n            text_encoder_2=text_encoder_2,\n            model=model,\n            use_cpu_offload=args.use_cpu_offload,\n            device=device,\n            logger=logger,\n            parallel_args=parallel_args\n        )\n\n    @staticmethod\n    def load_state_dict(args, model, pretrained_model_path):\n        load_key = args.load_key\n        dit_weight = Path(args.dit_weight)\n\n        if dit_weight is None:\n            model_dir = pretrained_model_path / f\"t2v_{args.model_resolution}\"\n            files = list(model_dir.glob(\"*.pt\"))\n            if len(files) == 0:\n                raise ValueError(f\"No model weights found in {model_dir}\")\n            if str(files[0]).startswith(\"pytorch_model_\"):\n                model_path = dit_weight / f\"pytorch_model_{load_key}.pt\"\n                bare_model = True\n            elif any(str(f).endswith(\"_model_states.pt\") for f in files):\n                files = [f for f in files if str(f).endswith(\"_model_states.pt\")]\n                model_path = files[0]\n                if len(files) > 1:\n                    logger.warning(\n                        f\"Multiple model weights found in {dit_weight}, using {model_path}\"\n                    )\n                bare_model = False\n            else:\n                raise ValueError(\n                    f\"Invalid model path: {dit_weight} with unrecognized weight format: \"\n                    f\"{list(map(str, files))}. When given a directory as --dit-weight, only \"\n                    f\"`pytorch_model_*.pt`(provided by HunyuanDiT official) and \"\n                    f\"`*_model_states.pt`(saved by deepspeed) can be parsed. If you want to load a \"\n                    f\"specific weight file, please provide the full path to the file.\"\n                )\n        else:\n            if dit_weight.is_dir():\n                files = list(dit_weight.glob(\"*.pt\"))\n                if len(files) == 0:\n                    raise ValueError(f\"No model weights found in {dit_weight}\")\n                if str(files[0]).startswith(\"pytorch_model_\"):\n                    model_path = dit_weight / f\"pytorch_model_{load_key}.pt\"\n                    bare_model = True\n                elif any(str(f).endswith(\"_model_states.pt\") for f in files):\n                    files = [f for f in files if str(f).endswith(\"_model_states.pt\")]\n                    model_path = files[0]\n                    if len(files) > 1:\n                        logger.warning(\n                            f\"Multiple model weights found in {dit_weight}, using {model_path}\"\n                        )\n                    bare_model = False\n                else:\n                    raise ValueError(\n                        f\"Invalid model path: {dit_weight} with unrecognized weight format: \"\n                        f\"{list(map(str, files))}. When given a directory as --dit-weight, only \"\n                        f\"`pytorch_model_*.pt`(provided by HunyuanDiT official) and \"\n                        f\"`*_model_states.pt`(saved by deepspeed) can be parsed. If you want to load a \"\n                        f\"specific weight file, please provide the full path to the file.\"\n                    )\n            elif dit_weight.is_file():\n                model_path = dit_weight\n                bare_model = \"unknown\"\n            else:\n                raise ValueError(f\"Invalid model path: {dit_weight}\")\n\n        if not model_path.exists():\n            raise ValueError(f\"model_path not exists: {model_path}\")\n        logger.info(f\"Loading torch model {model_path}...\")\n        state_dict = torch.load(model_path, map_location=lambda storage, loc: storage)\n\n        if bare_model == \"unknown\" and (\"ema\" in state_dict or \"module\" in state_dict):\n            bare_model = False\n        if bare_model is False:\n            if load_key in state_dict:\n                state_dict = state_dict[load_key]\n            else:\n                raise KeyError(\n                    f\"Missing key: `{load_key}` in the checkpoint: {model_path}. The keys in the checkpoint \"\n                    f\"are: {list(state_dict.keys())}.\"\n                )\n        model.load_state_dict(state_dict, strict=True)\n        return model\n\n    @staticmethod\n    def parse_size(size):\n        if isinstance(size, int):\n            size = [size]\n        if not isinstance(size, (list, tuple)):\n            raise ValueError(f\"Size must be an integer or (height, width), got {size}.\")\n        if len(size) == 1:\n            size = [size[0], size[0]]\n        if len(size) != 2:\n            raise ValueError(f\"Size must be an integer or (height, width), got {size}.\")\n        return size\n\n\nclass HunyuanVideoSampler(Inference):\n    def __init__(\n        self,\n        args,\n        vae,\n        vae_kwargs,\n        text_encoder,\n        model,\n        text_encoder_2=None,\n        pipeline=None,\n        use_cpu_offload=False,\n        device=0,\n        logger=None,\n        parallel_args=None\n    ):\n        super().__init__(\n            args,\n            vae,\n            vae_kwargs,\n            text_encoder,\n            model,\n            text_encoder_2=text_encoder_2,\n            pipeline=pipeline,\n            use_cpu_offload=use_cpu_offload,\n            device=device,\n            logger=logger,\n            parallel_args=parallel_args\n        )\n\n        self.pipeline = self.load_diffusion_pipeline(\n            args=args,\n            vae=self.vae,\n            text_encoder=self.text_encoder,\n            text_encoder_2=self.text_encoder_2,\n            model=self.model,\n            device=self.device,\n        )\n\n        self.default_negative_prompt = NEGATIVE_PROMPT\n        if self.parallel_args['ulysses_degree'] > 1 or self.parallel_args['ring_degree'] > 1:\n            parallelize_transformer(self.pipeline)\n\n    def load_diffusion_pipeline(\n        self,\n        args,\n        vae,\n        text_encoder,\n        text_encoder_2,\n        model,\n        scheduler=None,\n        device=None,\n        progress_bar_config=None,\n        data_type=\"video\",\n    ):\n        \"\"\"Load the denoising scheduler for inference.\"\"\"\n        if scheduler is None:\n            if args.denoise_type == \"flow\":\n                scheduler = FlowMatchDiscreteScheduler(\n                    shift=args.flow_shift,\n                    reverse=args.flow_reverse,\n                    solver=args.flow_solver,\n                )\n            else:\n                raise ValueError(f\"Invalid denoise type {args.denoise_type}\")\n\n        pipeline = HunyuanVideoPipeline(\n            vae=vae,\n            text_encoder=text_encoder,\n            text_encoder_2=text_encoder_2,\n            transformer=model,\n            scheduler=scheduler,\n            progress_bar_config=progress_bar_config,\n            args=args,\n        )\n        if self.use_cpu_offload:\n            pipeline.enable_sequential_cpu_offload()\n        else:\n            pipeline = pipeline.to(device)\n\n        return pipeline\n\n    def get_rotary_pos_embed(self, video_length, height, width):\n        target_ndim = 3\n        ndim = 5 - 2\n        # 884\n        if \"884\" in self.args.vae:\n            latents_size = [(video_length - 1) // 4 + 1, height // 8, width // 8]\n        elif \"888\" in self.args.vae:\n            latents_size = [(video_length - 1) // 8 + 1, height // 8, width // 8]\n        else:\n            latents_size = [video_length, height // 8, width // 8]\n\n        if isinstance(self.model.patch_size, int):\n            assert all(s % self.model.patch_size == 0 for s in latents_size), (\n                f\"Latent size(last {ndim} dimensions) should be divisible by patch size({self.model.patch_size}), \"\n                f\"but got {latents_size}.\"\n            )\n            rope_sizes = [s // self.model.patch_size for s in latents_size]\n        elif isinstance(self.model.patch_size, list):\n            assert all(\n                s % self.model.patch_size[idx] == 0\n                for idx, s in enumerate(latents_size)\n            ), (\n                f\"Latent size(last {ndim} dimensions) should be divisible by patch size({self.model.patch_size}), \"\n                f\"but got {latents_size}.\"\n            )\n            rope_sizes = [\n                s // self.model.patch_size[idx] for idx, s in enumerate(latents_size)\n            ]\n\n        if len(rope_sizes) != target_ndim:\n            rope_sizes = [1] * (target_ndim - len(rope_sizes)) + rope_sizes  # time axis\n        head_dim = self.model.hidden_size // self.model.heads_num\n        rope_dim_list = self.model.rope_dim_list\n        if rope_dim_list is None:\n            rope_dim_list = [head_dim // target_ndim for _ in range(target_ndim)]\n        assert (\n            sum(rope_dim_list) == head_dim\n        ), \"sum(rope_dim_list) should equal to head_dim of attention layer\"\n        freqs_cos, freqs_sin = get_nd_rotary_pos_embed(\n            rope_dim_list,\n            rope_sizes,\n            theta=self.args.rope_theta,\n            use_real=True,\n            theta_rescale_factor=1,\n        )\n        return freqs_cos, freqs_sin\n\n    @torch.no_grad()\n    def get_prompt_mask(\n        self,\n        prompt,\n        height=192,\n        width=336,\n        video_length=129,\n        negative_prompt=None,\n        infer_steps=50,\n        guidance_scale=6,\n        embedded_guidance_scale=None,\n        num_videos_per_prompt=1,\n        **kwargs,\n    ):\n        \"\"\"\n        Predict the image/video from the given text.\n\n        Args:\n            prompt (str or List[str]): The input text.\n            kwargs:\n                height (int): The height of the output video. Default is 192.\n                width (int): The width of the output video. Default is 336.\n                video_length (int): The frame number of the output video. Default is 129.\n                seed (int or List[str]): The random seed for the generation. Default is a random integer.\n                negative_prompt (str or List[str]): The negative text prompt. Default is an empty string.\n                guidance_scale (float): The guidance scale for the generation. Default is 6.0.\n                num_images_per_prompt (int): The number of images per prompt. Default is 1.\n                infer_steps (int): The number of inference steps. Default is 100.\n        \"\"\"\n\n        # ========================================================================\n        # Arguments: target_width, target_height, target_video_length\n        # ========================================================================\n        if width <= 0 or height <= 0 or video_length <= 0:\n            raise ValueError(\n                f\"`height` and `width` and `video_length` must be positive integers, got height={height}, width={width}, video_length={video_length}\"\n            )\n        if (video_length - 1) % 4 != 0:\n            raise ValueError(\n                f\"`video_length-1` must be a multiple of 4, got {video_length}\"\n            )\n\n        logger.info(\n            f\"Input (height, width, video_length) = ({height}, {width}, {video_length})\"\n        )\n\n        target_height = align_to(height, 16)\n        target_width = align_to(width, 16)\n        target_video_length = video_length\n\n        # ========================================================================\n        # Arguments: prompt, new_prompt, negative_prompt\n        # ========================================================================\n        if not isinstance(prompt, str):\n            raise TypeError(f\"`prompt` must be a string, but got {type(prompt)}\")\n        prompt = [prompt.strip()]\n\n        # negative prompt\n        if negative_prompt is None or negative_prompt == \"\":\n            negative_prompt = self.default_negative_prompt\n        if not isinstance(negative_prompt, str):\n            raise TypeError(\n                f\"`negative_prompt` must be a string, but got {type(negative_prompt)}\"\n            )\n        negative_prompt = [negative_prompt.strip()]\n\n        prompt_mask = self.pipeline.get_prompt_mask(\n            prompt=prompt,\n            height=target_height,\n            width=target_width,\n            video_length=target_video_length,\n            num_inference_steps=infer_steps,\n            guidance_scale=guidance_scale,\n            negative_prompt=negative_prompt,\n            num_videos_per_prompt=num_videos_per_prompt,\n            output_type=\"pil\",\n            embedded_guidance_scale=embedded_guidance_scale,\n            data_type=\"video\" if target_video_length > 1 else \"image\",\n            is_progress_bar=True,\n            vae_ver=self.args.vae,\n            enable_tiling=self.args.vae_tiling,\n        )[0]\n        \n        return prompt_mask\n    \n    @torch.no_grad()\n    def predict(\n        self,\n        prompt,\n        height=192,\n        width=336,\n        video_length=129,\n        seed=None,\n        negative_prompt=None,\n        infer_steps=50,\n        guidance_scale=6,\n        flow_shift=5.0,\n        embedded_guidance_scale=None,\n        batch_size=1,\n        num_videos_per_prompt=1,\n        **kwargs,\n    ):\n        \"\"\"\n        Predict the image/video from the given text.\n\n        Args:\n            prompt (str or List[str]): The input text.\n            kwargs:\n                height (int): The height of the output video. Default is 192.\n                width (int): The width of the output video. Default is 336.\n                video_length (int): The frame number of the output video. Default is 129.\n                seed (int or List[str]): The random seed for the generation. Default is a random integer.\n                negative_prompt (str or List[str]): The negative text prompt. Default is an empty string.\n                guidance_scale (float): The guidance scale for the generation. Default is 6.0.\n                num_images_per_prompt (int): The number of images per prompt. Default is 1.\n                infer_steps (int): The number of inference steps. Default is 100.\n        \"\"\"\n        out_dict = dict()\n\n        # ========================================================================\n        # Arguments: seed\n        # ========================================================================\n        if isinstance(seed, torch.Tensor):\n            seed = seed.tolist()\n        if seed is None:\n            seeds = [\n                random.randint(0, 1_000_000)\n                for _ in range(batch_size * num_videos_per_prompt)\n            ]\n        elif isinstance(seed, int):\n            seeds = [\n                seed + i\n                for _ in range(batch_size)\n                for i in range(num_videos_per_prompt)\n            ]\n        elif isinstance(seed, (list, tuple)):\n            if len(seed) == batch_size:\n                seeds = [\n                    int(seed[i]) + j\n                    for i in range(batch_size)\n                    for j in range(num_videos_per_prompt)\n                ]\n            elif len(seed) == batch_size * num_videos_per_prompt:\n                seeds = [int(s) for s in seed]\n            else:\n                raise ValueError(\n                    f\"Length of seed must be equal to number of prompt(batch_size) or \"\n                    f\"batch_size * num_videos_per_prompt ({batch_size} * {num_videos_per_prompt}), got {seed}.\"\n                )\n        else:\n            raise ValueError(\n                f\"Seed must be an integer, a list of integers, or None, got {seed}.\"\n            )\n        generator = [torch.Generator(self.device).manual_seed(seed) for seed in seeds]\n        out_dict[\"seeds\"] = seeds\n\n        # ========================================================================\n        # Arguments: target_width, target_height, target_video_length\n        # ========================================================================\n        if width <= 0 or height <= 0 or video_length <= 0:\n            raise ValueError(\n                f\"`height` and `width` and `video_length` must be positive integers, got height={height}, width={width}, video_length={video_length}\"\n            )\n        if (video_length - 1) % 4 != 0:\n            raise ValueError(\n                f\"`video_length-1` must be a multiple of 4, got {video_length}\"\n            )\n\n        logger.info(\n            f\"Input (height, width, video_length) = ({height}, {width}, {video_length})\"\n        )\n\n        target_height = align_to(height, 16)\n        target_width = align_to(width, 16)\n        target_video_length = video_length\n\n        out_dict[\"size\"] = (target_height, target_width, target_video_length)\n\n        # ========================================================================\n        # Arguments: prompt, new_prompt, negative_prompt\n        # ========================================================================\n        if not isinstance(prompt, str):\n            raise TypeError(f\"`prompt` must be a string, but got {type(prompt)}\")\n        prompt = [prompt.strip()]\n\n        # negative prompt\n        if negative_prompt is None or negative_prompt == \"\":\n            negative_prompt = self.default_negative_prompt\n        if not isinstance(negative_prompt, str):\n            raise TypeError(\n                f\"`negative_prompt` must be a string, but got {type(negative_prompt)}\"\n            )\n        negative_prompt = [negative_prompt.strip()]\n\n        # ========================================================================\n        # Scheduler\n        # ========================================================================\n        scheduler = FlowMatchDiscreteScheduler(\n            shift=flow_shift,\n            reverse=self.args.flow_reverse,\n            solver=self.args.flow_solver\n        )\n        self.pipeline.scheduler = scheduler\n\n        # ========================================================================\n        # Build Rope freqs\n        # ========================================================================\n        freqs_cos, freqs_sin = self.get_rotary_pos_embed(\n            target_video_length, target_height, target_width\n        )\n        n_tokens = freqs_cos.shape[0]\n\n        # ========================================================================\n        # Print infer args\n        # ========================================================================\n        debug_str = f\"\"\"\n                        height: {target_height}\n                         width: {target_width}\n                  video_length: {target_video_length}\n                        prompt: {prompt}\n                    neg_prompt: {negative_prompt}\n                          seed: {seed}\n                   infer_steps: {infer_steps}\n         num_videos_per_prompt: {num_videos_per_prompt}\n                guidance_scale: {guidance_scale}\n                      n_tokens: {n_tokens}\n                    flow_shift: {flow_shift}\n       embedded_guidance_scale: {embedded_guidance_scale}\"\"\"\n        logger.debug(debug_str)\n\n        # ========================================================================\n        # Pipeline inference\n        # ========================================================================\n        start_time = time.time()\n        samples = self.pipeline(\n            prompt=prompt,\n            height=target_height,\n            width=target_width,\n            video_length=target_video_length,\n            num_inference_steps=infer_steps,\n            guidance_scale=guidance_scale,\n            negative_prompt=negative_prompt,\n            num_videos_per_prompt=num_videos_per_prompt,\n            generator=generator,\n            output_type=\"pil\",\n            freqs_cis=(freqs_cos, freqs_sin),\n            n_tokens=n_tokens,\n            embedded_guidance_scale=embedded_guidance_scale,\n            data_type=\"video\" if target_video_length > 1 else \"image\",\n            is_progress_bar=True,\n            vae_ver=self.args.vae,\n            enable_tiling=self.args.vae_tiling,\n        )[0]\n        out_dict[\"samples\"] = samples\n        out_dict[\"prompts\"] = prompt\n\n        gen_time = time.time() - start_time\n        logger.info(f\"Success, time: {gen_time}\")\n\n        return out_dict\n"}
{"type": "source_file", "path": "svg/models/hyvideo/constants.py", "content": "import os\nimport torch\n\n__all__ = [\n    \"C_SCALE\",\n    \"PROMPT_TEMPLATE\",\n    \"MODEL_BASE\",\n    \"PRECISIONS\",\n    \"NORMALIZATION_TYPE\",\n    \"ACTIVATION_TYPE\",\n    \"VAE_PATH\",\n    \"TEXT_ENCODER_PATH\",\n    \"TOKENIZER_PATH\",\n    \"TEXT_PROJECTION\",\n    \"DATA_TYPE\",\n    \"NEGATIVE_PROMPT\",\n]\n\nPRECISION_TO_TYPE = {\n    'fp32': torch.float32,\n    'fp16': torch.float16,\n    'bf16': torch.bfloat16,\n}\n\n# =================== Constant Values =====================\n# Computation scale factor, 1P = 1_000_000_000_000_000. Tensorboard will display the value in PetaFLOPS to avoid\n# overflow error when tensorboard logging values.\nC_SCALE = 1_000_000_000_000_000\n\n# When using decoder-only models, we must provide a prompt template to instruct the text encoder\n# on how to generate the text.\n# --------------------------------------------------------------------\nPROMPT_TEMPLATE_ENCODE = (\n    \"<|start_header_id|>system<|end_header_id|>\\n\\nDescribe the image by detailing the color, shape, size, texture, \"\n    \"quantity, text, spatial relationships of the objects and background:<|eot_id|>\"\n    \"<|start_header_id|>user<|end_header_id|>\\n\\n{}<|eot_id|>\"\n) \nPROMPT_TEMPLATE_ENCODE_VIDEO = (\n    \"<|start_header_id|>system<|end_header_id|>\\n\\nDescribe the video by detailing the following aspects: \"\n    \"1. The main content and theme of the video.\"\n    \"2. The color, shape, size, texture, quantity, text, and spatial relationships of the objects.\"\n    \"3. Actions, events, behaviors temporal relationships, physical movement changes of the objects.\"\n    \"4. background environment, light, style and atmosphere.\"\n    \"5. camera angles, movements, and transitions used in the video:<|eot_id|>\"\n    \"<|start_header_id|>user<|end_header_id|>\\n\\n{}<|eot_id|>\"\n)  \n\nNEGATIVE_PROMPT = \"Aerial view, aerial view, overexposed, low quality, deformation, a poor composition, bad hands, bad teeth, bad eyes, bad limbs, distortion\"\n\nPROMPT_TEMPLATE = {\n    \"dit-llm-encode\": {\n        \"template\": PROMPT_TEMPLATE_ENCODE,\n        \"crop_start\": 36,\n    },\n    \"dit-llm-encode-video\": {\n        \"template\": PROMPT_TEMPLATE_ENCODE_VIDEO,\n        \"crop_start\": 95,\n    },\n}\n\n# ======================= Model ======================\nPRECISIONS = {\"fp32\", \"fp16\", \"bf16\"}\nNORMALIZATION_TYPE = {\"layer\", \"rms\"}\nACTIVATION_TYPE = {\"relu\", \"silu\", \"gelu\", \"gelu_tanh\"}\n\n# =================== Model Path =====================\nMODEL_BASE = os.getenv(\"MODEL_BASE\", \"./ckpts\")\n\n# =================== Data =======================\nDATA_TYPE = {\"image\", \"video\", \"image_video\"}\n\n# 3D VAE\nVAE_PATH = {\"884-16c-hy\": f\"{MODEL_BASE}/hunyuan-video-t2v-720p/vae\"}\n\n# Text Encoder\nTEXT_ENCODER_PATH = {\n    \"clipL\": f\"{MODEL_BASE}/text_encoder_2\",\n    \"llm\": f\"{MODEL_BASE}/text_encoder\",\n}\n\n# Tokenizer\nTOKENIZER_PATH = {\n    \"clipL\": f\"{MODEL_BASE}/text_encoder_2\",\n    \"llm\": f\"{MODEL_BASE}/text_encoder\",\n}\n\nTEXT_PROJECTION = {\n    \"linear\",  # Default, an nn.Linear() layer\n    \"single_refiner\",  # Single TokenRefiner. Refer to LI-DiT\n}\n"}
{"type": "source_file", "path": "svg/models/hyvideo/vae/__init__.py", "content": "from pathlib import Path\n\nimport torch\n\nfrom .autoencoder_kl_causal_3d import AutoencoderKLCausal3D\nfrom ..constants import VAE_PATH, PRECISION_TO_TYPE\n\ndef load_vae(vae_type: str=\"884-16c-hy\",\n             vae_precision: str=None,\n             sample_size: tuple=None,\n             vae_path: str=None,\n             logger=None,\n             device=None\n             ):\n    \"\"\"the fucntion to load the 3D VAE model\n\n    Args:\n        vae_type (str): the type of the 3D VAE model. Defaults to \"884-16c-hy\".\n        vae_precision (str, optional): the precision to load vae. Defaults to None.\n        sample_size (tuple, optional): the tiling size. Defaults to None.\n        vae_path (str, optional): the path to vae. Defaults to None.\n        logger (_type_, optional): logger. Defaults to None.\n        device (_type_, optional): device to load vae. Defaults to None.\n    \"\"\"\n    if vae_path is None:\n        vae_path = VAE_PATH[vae_type]\n    \n    if logger is not None:\n        logger.info(f\"Loading 3D VAE model ({vae_type}) from: {vae_path}\")\n    config = AutoencoderKLCausal3D.load_config(vae_path)\n    if sample_size:\n        vae = AutoencoderKLCausal3D.from_config(config, sample_size=sample_size)\n    else:\n        vae = AutoencoderKLCausal3D.from_config(config)\n    \n    vae_ckpt = Path(vae_path) / \"pytorch_model.pt\"\n    assert vae_ckpt.exists(), f\"VAE checkpoint not found: {vae_ckpt}\"\n    \n    ckpt = torch.load(vae_ckpt, map_location=vae.device)\n    if \"state_dict\" in ckpt:\n        ckpt = ckpt[\"state_dict\"]\n    if any(k.startswith(\"vae.\") for k in ckpt.keys()):\n        ckpt = {k.replace(\"vae.\", \"\"): v for k, v in ckpt.items() if k.startswith(\"vae.\")}\n    vae.load_state_dict(ckpt)\n\n    spatial_compression_ratio = vae.config.spatial_compression_ratio\n    time_compression_ratio = vae.config.time_compression_ratio\n    \n    if vae_precision is not None:\n        vae = vae.to(dtype=PRECISION_TO_TYPE[vae_precision])\n\n    vae.requires_grad_(False)\n\n    if logger is not None:\n        logger.info(f\"VAE to dtype: {vae.dtype}\")\n\n    if device is not None:\n        vae = vae.to(device)\n\n    vae.eval()\n\n    return vae, vae_path, spatial_compression_ratio, time_compression_ratio\n"}
{"type": "source_file", "path": "svg/models/hyvideo/modules/placement.py", "content": "import torch\nimport triton\nimport triton.language as tl\n\ndef hunyuan_token_reorder_to_token_major(tensor, fix_len, reorder_len, reorder_num_frame, frame_size):\n    \"\"\"Reorder it from frame major to token major!\"\"\"\n    assert reorder_len == reorder_num_frame * frame_size\n    assert tensor.shape[2] == fix_len + reorder_len\n\n    tensor[:, :, :-fix_len, :] = tensor[:, :, :-fix_len:, :].reshape(tensor.shape[0], tensor.shape[1], reorder_num_frame, frame_size, tensor.shape[3]) \\\n                                                         .transpose(2, 3).reshape(tensor.shape[0], tensor.shape[1], reorder_len, tensor.shape[3])\n    return tensor\n\ndef hunyuan_token_reorder_to_frame_major(tensor, fix_len, reorder_len, reorder_num_frame, frame_size):\n    \"\"\"Reorder it from token major to frame major!\"\"\"\n    assert reorder_len == reorder_num_frame * frame_size\n    assert tensor.shape[2] == fix_len + reorder_len\n\n    tensor[:, :, :-fix_len:, :] = tensor[:, :, :-fix_len:, :].reshape(tensor.shape[0], tensor.shape[1], frame_size, reorder_num_frame, tensor.shape[3]) \\\n                                                         .transpose(2, 3).reshape(tensor.shape[0], tensor.shape[1], reorder_len, tensor.shape[3])\n    return tensor\n\n\n@triton.jit\ndef hunyuan_sparse_head_placement_kernel(\n    query_ptr, key_ptr, value_ptr, # [cfg, num_heads, seq_len, head_dim] seq_len = context_length + num_frame * frame_size\n    query_out_ptr, key_out_ptr, value_out_ptr, # [cfg, num_heads, seq_len, head_dim]\n    best_mask_idx_ptr, # [cfg, num_heads]\n    query_stride_b, query_stride_h, query_stride_s, query_stride_d,\n    mask_idx_stride_b, mask_idx_stride_h,\n    seq_len: tl.constexpr,\n    head_dim: tl.constexpr,\n    context_length: tl.constexpr,   \n    num_frame: tl.constexpr,        \n    frame_size: tl.constexpr,      \n    BLOCK_SIZE: tl.constexpr\n):\n    # Copy query, key, value to output\n    # range: [b, h, block_id * block_size: block_id * block_size + block_size, :]\n    cfg = tl.program_id(0)\n    head = tl.program_id(1)\n    block_id = tl.program_id(2)\n\n    start_id = block_id * BLOCK_SIZE\n    end_id = start_id + BLOCK_SIZE\n    end_id = tl.where(end_id > seq_len, seq_len, end_id) \n\n    # Load best mask idx (0 is spatial, 1 is temporal)\n    is_temporal = tl.load(best_mask_idx_ptr + cfg * mask_idx_stride_b + head * mask_idx_stride_h)\n    \n    offset_token = tl.arange(0, BLOCK_SIZE) + start_id\n    offset_mask = offset_token < seq_len\n    offset_d = tl.arange(0, head_dim)\n\n    if is_temporal:\n        frame_id = offset_token // frame_size\n        patch_id = offset_token - frame_id * frame_size\n        offset_store_token = tl.where(offset_token >= seq_len - context_length, offset_token, patch_id * num_frame + frame_id)\n\n        offset_load = (cfg * query_stride_b + head * query_stride_h + offset_token[:,None] * query_stride_s) + offset_d[None,:] * query_stride_d\n        offset_query = query_ptr + offset_load\n        offset_key = key_ptr + offset_load\n        offset_value = value_ptr + offset_load\n\n        offset_store = (cfg * query_stride_b + head * query_stride_h + offset_store_token[:,None] * query_stride_s) + offset_d[None,:] * query_stride_d\n        offset_query_out = query_out_ptr + offset_store\n        offset_key_out = key_out_ptr + offset_store\n        offset_value_out = value_out_ptr + offset_store\n\n        # Maybe tune the pipeline here\n        query = tl.load(offset_query, mask=offset_mask[:,None])\n        tl.store(offset_query_out, query, mask=offset_mask[:,None])\n        key = tl.load(offset_key, mask=offset_mask[:,None])\n        tl.store(offset_key_out, key, mask=offset_mask[:,None])\n        value = tl.load(offset_value, mask=offset_mask[:,None])\n        tl.store(offset_value_out, value, mask=offset_mask[:,None])\n\n\n    else:\n        offset_load = (cfg * query_stride_b + head * query_stride_h + offset_token[:,None] * query_stride_s) + offset_d[None,:] * query_stride_d\n        offset_query = query_ptr + offset_load\n        offset_key = key_ptr + offset_load\n        offset_value = value_ptr + offset_load\n\n        offset_store = offset_load\n        offset_query_out = query_out_ptr + offset_store\n        offset_key_out = key_out_ptr + offset_store\n        offset_value_out = value_out_ptr + offset_store\n\n        # Maybe tune the pipeline here\n        query = tl.load(offset_query, mask=offset_mask[:,None])\n        tl.store(offset_query_out, query, mask=offset_mask[:,None])\n        key = tl.load(offset_key, mask=offset_mask[:,None])\n        tl.store(offset_key_out, key, mask=offset_mask[:,None])\n        value = tl.load(offset_value, mask=offset_mask[:,None])\n        tl.store(offset_value_out, value, mask=offset_mask[:,None])\n\n\ndef hunyuan_sparse_head_placement(query, key, value, query_out, key_out, value_out, best_mask_idx, context_length, num_frame, frame_size):\n    cfg, num_heads, seq_len, head_dim = query.shape\n    BLOCK_SIZE = 128\n    assert seq_len == context_length + num_frame * frame_size\n\n    grid = (cfg, num_heads, (seq_len + BLOCK_SIZE - 1) // BLOCK_SIZE)\n\n    hunyuan_sparse_head_placement_kernel[grid](\n        query, key, value, \n        query_out, key_out, value_out, \n        best_mask_idx,\n        query.stride(0), query.stride(1), query.stride(2), query.stride(3),\n        best_mask_idx.stride(0), best_mask_idx.stride(1),\n        seq_len, head_dim, context_length, num_frame, frame_size, \n        BLOCK_SIZE\n    )\n\n\ndef ref_hunyuan_sparse_head_placement(query, key, value, best_mask_idx, context_length, num_frame, frame_size):\n    cfg, num_heads, seq_len, head_dim = query.shape\n    assert seq_len == context_length + num_frame * frame_size\n\n    query_out = query.clone()\n    key_out = key.clone()\n    value_out = value.clone()\n\n    # Spatial\n    query_out[best_mask_idx == 0], key_out[best_mask_idx == 0], value_out[best_mask_idx == 0] = \\\n        query[best_mask_idx == 0], key[best_mask_idx == 0], value[best_mask_idx == 0]\n\n    # Temporal\n    query_out[best_mask_idx == 1], key_out[best_mask_idx == 1], value_out[best_mask_idx == 1] = \\\n            hunyuan_token_reorder_to_token_major(query[best_mask_idx == 1].unsqueeze(0), context_length, num_frame * frame_size, num_frame, frame_size).squeeze(0), \\\n            hunyuan_token_reorder_to_token_major(key[best_mask_idx == 1].unsqueeze(0), context_length, num_frame * frame_size, num_frame, frame_size).squeeze(0), \\\n            hunyuan_token_reorder_to_token_major(value[best_mask_idx == 1].unsqueeze(0), context_length, num_frame * frame_size, num_frame, frame_size).squeeze(0)\n\n    return query_out, key_out, value_out\n\n\ndef test_hunyuan_sparse_head_placement():\n\n    context_length = 226\n    num_frame = 11\n    frame_size = 4080\n\n    cfg = 2\n    num_heads = 48\n\n    seq_len = context_length + num_frame * frame_size\n    head_dim = 64\n\n    dtype = torch.bfloat16\n    device = torch.device(\"cuda\")\n\n    query = torch.randn(cfg, num_heads, seq_len, head_dim, dtype=dtype, device=device)\n    key = torch.randn(cfg, num_heads, seq_len, head_dim, dtype=dtype, device=device)\n    value = torch.randn(cfg, num_heads, seq_len, head_dim, dtype=dtype, device=device)\n\n    best_mask_idx = torch.randint(0, 2, (cfg, num_heads), device=device)\n\n    query_out = torch.empty_like(query)\n    key_out = torch.empty_like(key)\n    value_out = torch.empty_like(value)\n\n    hunyuan_sparse_head_placement(query, key, value, query_out, key_out, value_out, best_mask_idx, context_length, num_frame, frame_size)\n    ref_query_out, ref_key_out, ref_value_out = ref_hunyuan_sparse_head_placement(query, key, value, best_mask_idx, context_length, num_frame, frame_size)\n\n    torch.testing.assert_close(query_out, ref_query_out)\n    torch.testing.assert_close(key_out, ref_key_out)\n    torch.testing.assert_close(value_out, ref_value_out)\n\n\ndef benchmark_hunyuan_sparse_head_placement():\n    import time\n\n    context_length = 226\n    num_frame = 11\n    frame_size = 4080\n\n    cfg = 2\n    num_heads = 48\n\n    seq_len = context_length + num_frame * frame_size\n    head_dim = 64\n\n    dtype = torch.bfloat16\n    device = torch.device(\"cuda\")\n\n    query = torch.randn(cfg, num_heads, seq_len, head_dim, dtype=dtype, device=device)\n    key = torch.randn(cfg, num_heads, seq_len, head_dim, dtype=dtype, device=device)\n    value = torch.randn(cfg, num_heads, seq_len, head_dim, dtype=dtype, device=device)\n    best_mask_idx = torch.randint(0, 2, (cfg, num_heads), device=device)\n\n    query_out = torch.empty_like(query)\n    key_out = torch.empty_like(key)\n    value_out = torch.empty_like(value)\n\n    warmup = 10\n    all_iter = 1000\n\n    # warmup\n    for _ in range(warmup):\n        hunyuan_sparse_head_placement(query, key, value, query_out, key_out, value_out, best_mask_idx, context_length, num_frame, frame_size)\n\n    torch.cuda.synchronize()\n    start = time.time()\n    for _ in range(all_iter):\n        hunyuan_sparse_head_placement(query, key, value, query_out, key_out, value_out, best_mask_idx, context_length, num_frame, frame_size)\n    torch.cuda.synchronize()\n    end = time.time()\n\n    print(f\"Triton Elapsed Time: {(end - start) / all_iter * 1e3:.2f} ms\")\n    print(f\"Triton Total Bandwidth: {query.nelement() * query.element_size() * 3 * 2 * all_iter / (end - start) / 1e9:.2f} GB/s\")\n\n    torch.cuda.synchronize()\n    start = time.time()\n    for _ in range(all_iter):\n        ref_hunyuan_sparse_head_placement(query, key, value, best_mask_idx, context_length, num_frame, frame_size)\n    torch.cuda.synchronize()\n    end = time.time()\n\n    print(f\"Reference Elapsed Time: {(end - start) / all_iter * 1e3:.2f} ms\")\n    print(f\"Reference Total Bandwidth: {query.nelement() * query.element_size() * 3 * 2 * all_iter / (end - start) / 1e9:.2f} GB/s\")\n\n\n@triton.jit\ndef hunyuan_hidden_states_placement_kernel(\n    hidden_states_ptr, # [cfg, num_heads, seq_len, head_dim] seq_len = context_length + num_frame * frame_size\n    hidden_states_out_ptr, # [cfg, num_heads, seq_len, head_dim]\n    best_mask_idx_ptr, # [cfg, num_heads]\n    hidden_states_stride_b, hidden_states_stride_h, hidden_states_stride_s, hidden_states_stride_d,\n    mask_idx_stride_b, mask_idx_stride_h,\n    seq_len: tl.constexpr,\n    head_dim: tl.constexpr,\n    context_length: tl.constexpr,   \n    num_frame: tl.constexpr,        \n    frame_size: tl.constexpr,      \n    BLOCK_SIZE: tl.constexpr\n):\n    # Copy hidden_states to output\n    # range: [b, h, block_id * block_size: block_id * block_size + block_size, :]\n    cfg = tl.program_id(0)\n    head = tl.program_id(1)\n    block_id = tl.program_id(2)\n\n    start_id = block_id * BLOCK_SIZE\n    end_id = start_id + BLOCK_SIZE\n    end_id = tl.where(end_id > seq_len, seq_len, end_id) \n\n    # Load best mask idx (0 is spatial, 1 is temporal)\n    is_temporal = tl.load(best_mask_idx_ptr + cfg * mask_idx_stride_b + head * mask_idx_stride_h)\n    \n    offset_token = tl.arange(0, BLOCK_SIZE) + start_id\n    offset_mask = offset_token < seq_len\n    offset_d = tl.arange(0, head_dim)\n\n    if is_temporal:\n        patch_id = offset_token // num_frame\n        frame_id = offset_token - patch_id * num_frame\n        offset_store_token = tl.where(offset_token >= seq_len - context_length, offset_token, frame_id * frame_size + patch_id)\n\n        offset_load = (cfg * hidden_states_stride_b + head * hidden_states_stride_h + offset_token[:,None] * hidden_states_stride_s) + offset_d[None,:] * hidden_states_stride_d\n        offset_hidden_states = hidden_states_ptr + offset_load\n\n        offset_store = (cfg * hidden_states_stride_b + head * hidden_states_stride_h + offset_store_token[:,None] * hidden_states_stride_s) + offset_d[None,:] * hidden_states_stride_d\n        offset_hidden_states_out = hidden_states_out_ptr + offset_store\n\n        # Maybe tune the pipeline here\n        hidden_states = tl.load(offset_hidden_states, mask=offset_mask[:,None])\n        tl.store(offset_hidden_states_out, hidden_states, mask=offset_mask[:,None])\n    else:\n        offset_load = (cfg * hidden_states_stride_b + head * hidden_states_stride_h + offset_token[:,None] * hidden_states_stride_s) + offset_d[None,:] * hidden_states_stride_d\n        offset_hidden_states = hidden_states_ptr + offset_load\n\n        offset_store = offset_load\n        offset_hidden_states_out = hidden_states_out_ptr + offset_store\n\n        # Maybe tune the pipeline here\n        hidden_states = tl.load(offset_hidden_states, mask=offset_mask[:,None])\n        tl.store(offset_hidden_states_out, hidden_states, mask=offset_mask[:,None])\n\n\ndef hunyuan_hidden_states_placement(hidden_states, hidden_states_out, best_mask_idx, context_length, num_frame, frame_size):\n    cfg, num_heads, seq_len, head_dim = hidden_states.shape\n    BLOCK_SIZE = 128\n    assert seq_len == context_length + num_frame * frame_size\n\n    grid = (cfg, num_heads, (seq_len + BLOCK_SIZE - 1) // BLOCK_SIZE)\n\n\n    hunyuan_hidden_states_placement_kernel[grid](\n        hidden_states, \n        hidden_states_out, \n        best_mask_idx,\n        hidden_states.stride(0), hidden_states.stride(1), hidden_states.stride(2), hidden_states.stride(3),\n        best_mask_idx.stride(0), best_mask_idx.stride(1),\n        seq_len, head_dim, context_length, num_frame, frame_size, \n        BLOCK_SIZE\n    )\n\n    return hidden_states_out\n\ndef ref_hunyuan_hidden_states_placement(hidden_states, output_hidden_states, best_mask_idx, context_length, num_frame, frame_size):\n    cfg, num_heads, seq_len, head_dim = hidden_states.shape\n    assert seq_len == context_length + num_frame * frame_size\n\n    # Spatial\n    output_hidden_states[best_mask_idx == 0] = hidden_states[best_mask_idx == 0]\n    # Temporal\n    output_hidden_states[best_mask_idx == 1] = hunyuan_token_reorder_to_frame_major(hidden_states[best_mask_idx == 1].unsqueeze(0), context_length, num_frame * frame_size, num_frame, frame_size).squeeze(0)\n\ndef test_hunyuan_hidden_states_placement():\n\n    context_length = 226\n    num_frame = 11\n    frame_size = 4080\n\n    cfg = 2\n    num_heads = 48\n\n    seq_len = context_length + num_frame * frame_size\n    head_dim = 64\n\n    dtype = torch.bfloat16\n    device = torch.device(\"cuda\")\n\n    hidden_states = torch.randn(cfg, num_heads, seq_len, head_dim, dtype=dtype, device=device)\n    best_mask_idx = torch.randint(0, 2, (cfg, num_heads), device=device)\n\n    hidden_states_out1 = torch.empty_like(hidden_states)\n    hidden_states_out2 = torch.empty_like(hidden_states)\n\n    hunyuan_hidden_states_placement(hidden_states, hidden_states_out1, best_mask_idx, context_length, num_frame, frame_size)\n    ref_hunyuan_hidden_states_placement(hidden_states, hidden_states_out2, best_mask_idx, context_length, num_frame, frame_size)\n\n    torch.testing.assert_close(hidden_states_out1, hidden_states_out2)\n\ndef benchmark_hunyuan_hidden_states_placement():\n    import time\n\n    context_length = 226\n    num_frame = 11\n    frame_size = 4080\n\n    cfg = 2\n    num_heads = 48\n\n    seq_len = context_length + num_frame * frame_size\n    head_dim = 64\n\n    dtype = torch.bfloat16\n    device = torch.device(\"cuda\")\n\n    hidden_states = torch.randn(cfg, num_heads, seq_len, head_dim, dtype=dtype, device=device)\n    best_mask_idx = torch.randint(0, 2, (cfg, num_heads), device=device)\n\n    hidden_states_out = torch.empty_like(hidden_states)\n\n    warmup = 10\n    all_iter = 1000\n\n    # warmup\n    for _ in range(warmup):\n        hunyuan_hidden_states_placement(hidden_states, hidden_states_out, best_mask_idx, context_length, num_frame, frame_size)\n\n    torch.cuda.synchronize()\n    start = time.time()\n    for _ in range(all_iter):\n        hunyuan_hidden_states_placement(hidden_states, hidden_states_out, best_mask_idx, context_length, num_frame, frame_size)\n    torch.cuda.synchronize()\n    end = time.time()\n\n    print(f\"Triton Elapsed Time: {(end - start) / all_iter * 1e3:.2f} ms\")\n    print(f\"Triton Total Bandwidth: {hidden_states.nelement() * hidden_states.element_size() * 2 * all_iter / (end - start) / 1e9:.2f} GB/s\")\n\n    torch.cuda.synchronize()\n    start = time.time()\n    for _ in range(all_iter):\n        ref_hunyuan_hidden_states_placement(hidden_states, hidden_states.clone(), best_mask_idx, context_length, num_frame, frame_size)\n    torch.cuda.synchronize()\n    end = time.time()\n\n    print(f\"Reference Elapsed Time: {(end - start) / all_iter * 1e3:.2f} ms\")\n    print(f\"Reference Total Bandwidth: {hidden_states.nelement() * hidden_states.element_size() * 2 * all_iter / (end - start) / 1e9:.2f} GB/s\")\n\n\nif __name__ == \"__main__\":\n    test_hunyuan_sparse_head_placement()\n    benchmark_hunyuan_sparse_head_placement()\n    test_hunyuan_hidden_states_placement()\n    benchmark_hunyuan_hidden_states_placement()\n"}
{"type": "source_file", "path": "svg/models/hyvideo/text_encoder/__init__.py", "content": "from dataclasses import dataclass\nfrom typing import Optional, Tuple\nfrom copy import deepcopy\n\nimport torch\nimport torch.nn as nn\nfrom transformers import CLIPTextModel, CLIPTokenizer, AutoTokenizer, AutoModel\nfrom transformers.utils import ModelOutput\n\nfrom ..constants import TEXT_ENCODER_PATH, TOKENIZER_PATH\nfrom ..constants import PRECISION_TO_TYPE\n\n\ndef use_default(value, default):\n    return value if value is not None else default\n\n\ndef load_text_encoder(\n    text_encoder_type,\n    text_encoder_precision=None,\n    text_encoder_path=None,\n    logger=None,\n    device=None,\n):\n    if text_encoder_path is None:\n        text_encoder_path = TEXT_ENCODER_PATH[text_encoder_type]\n    if logger is not None:\n        logger.info(\n            f\"Loading text encoder model ({text_encoder_type}) from: {text_encoder_path}\"\n        )\n\n    if text_encoder_type == \"clipL\":\n        text_encoder = CLIPTextModel.from_pretrained(text_encoder_path)\n        text_encoder.final_layer_norm = text_encoder.text_model.final_layer_norm\n    elif text_encoder_type == \"llm\":\n        text_encoder = AutoModel.from_pretrained(\n            text_encoder_path, low_cpu_mem_usage=True\n        )\n        text_encoder.final_layer_norm = text_encoder.norm\n    else:\n        raise ValueError(f\"Unsupported text encoder type: {text_encoder_type}\")\n    # from_pretrained will ensure that the model is in eval mode.\n\n    if text_encoder_precision is not None:\n        text_encoder = text_encoder.to(dtype=PRECISION_TO_TYPE[text_encoder_precision])\n\n    text_encoder.requires_grad_(False)\n\n    if logger is not None:\n        logger.info(f\"Text encoder to dtype: {text_encoder.dtype}\")\n\n    if device is not None:\n        text_encoder = text_encoder.to(device)\n\n    return text_encoder, text_encoder_path\n\n\ndef load_tokenizer(\n    tokenizer_type, tokenizer_path=None, padding_side=\"right\", logger=None\n):\n    if tokenizer_path is None:\n        tokenizer_path = TOKENIZER_PATH[tokenizer_type]\n    if logger is not None:\n        logger.info(f\"Loading tokenizer ({tokenizer_type}) from: {tokenizer_path}\")\n\n    if tokenizer_type == \"clipL\":\n        tokenizer = CLIPTokenizer.from_pretrained(tokenizer_path, max_length=77)\n    elif tokenizer_type == \"llm\":\n        tokenizer = AutoTokenizer.from_pretrained(\n            tokenizer_path, padding_side=padding_side\n        )\n    else:\n        raise ValueError(f\"Unsupported tokenizer type: {tokenizer_type}\")\n\n    return tokenizer, tokenizer_path\n\n\n@dataclass\nclass TextEncoderModelOutput(ModelOutput):\n    \"\"\"\n    Base class for model's outputs that also contains a pooling of the last hidden states.\n\n    Args:\n        hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n            Sequence of hidden-states at the output of the last layer of the model.\n        attention_mask (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n            Mask to avoid performing attention on padding token indices. Mask values selected in ``[0, 1]``:\n        hidden_states_list (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed):\n            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n        text_outputs (`list`, *optional*, returned when `return_texts=True` is passed):\n            List of decoded texts.\n    \"\"\"\n\n    hidden_state: torch.FloatTensor = None\n    attention_mask: Optional[torch.LongTensor] = None\n    hidden_states_list: Optional[Tuple[torch.FloatTensor, ...]] = None\n    text_outputs: Optional[list] = None\n\n\nclass TextEncoder(nn.Module):\n    def __init__(\n        self,\n        text_encoder_type: str,\n        max_length: int,\n        text_encoder_precision: Optional[str] = None,\n        text_encoder_path: Optional[str] = None,\n        tokenizer_type: Optional[str] = None,\n        tokenizer_path: Optional[str] = None,\n        output_key: Optional[str] = None,\n        use_attention_mask: bool = True,\n        input_max_length: Optional[int] = None,\n        prompt_template: Optional[dict] = None,\n        prompt_template_video: Optional[dict] = None,\n        hidden_state_skip_layer: Optional[int] = None,\n        apply_final_norm: bool = False,\n        reproduce: bool = False,\n        logger=None,\n        device=None,\n    ):\n        super().__init__()\n        self.text_encoder_type = text_encoder_type\n        self.max_length = max_length\n        self.precision = text_encoder_precision\n        self.model_path = text_encoder_path\n        self.tokenizer_type = (\n            tokenizer_type if tokenizer_type is not None else text_encoder_type\n        )\n        self.tokenizer_path = (\n            tokenizer_path if tokenizer_path is not None else text_encoder_path\n        )\n        self.use_attention_mask = use_attention_mask\n        if prompt_template_video is not None:\n            assert (\n                use_attention_mask is True\n            ), \"Attention mask is True required when training videos.\"\n        self.input_max_length = (\n            input_max_length if input_max_length is not None else max_length\n        )\n        self.prompt_template = prompt_template\n        self.prompt_template_video = prompt_template_video\n        self.hidden_state_skip_layer = hidden_state_skip_layer\n        self.apply_final_norm = apply_final_norm\n        self.reproduce = reproduce\n        self.logger = logger\n\n        self.use_template = self.prompt_template is not None\n        if self.use_template:\n            assert (\n                isinstance(self.prompt_template, dict)\n                and \"template\" in self.prompt_template\n            ), f\"`prompt_template` must be a dictionary with a key 'template', got {self.prompt_template}\"\n            assert \"{}\" in str(self.prompt_template[\"template\"]), (\n                \"`prompt_template['template']` must contain a placeholder `{}` for the input text, \"\n                f\"got {self.prompt_template['template']}\"\n            )\n\n        self.use_video_template = self.prompt_template_video is not None\n        if self.use_video_template:\n            if self.prompt_template_video is not None:\n                assert (\n                    isinstance(self.prompt_template_video, dict)\n                    and \"template\" in self.prompt_template_video\n                ), f\"`prompt_template_video` must be a dictionary with a key 'template', got {self.prompt_template_video}\"\n            assert \"{}\" in str(self.prompt_template_video[\"template\"]), (\n                \"`prompt_template_video['template']` must contain a placeholder `{}` for the input text, \"\n                f\"got {self.prompt_template_video['template']}\"\n            )\n\n        if \"t5\" in text_encoder_type:\n            self.output_key = output_key or \"last_hidden_state\"\n        elif \"clip\" in text_encoder_type:\n            self.output_key = output_key or \"pooler_output\"\n        elif \"llm\" in text_encoder_type or \"glm\" in text_encoder_type:\n            self.output_key = output_key or \"last_hidden_state\"\n        else:\n            raise ValueError(f\"Unsupported text encoder type: {text_encoder_type}\")\n\n        self.model, self.model_path = load_text_encoder(\n            text_encoder_type=self.text_encoder_type,\n            text_encoder_precision=self.precision,\n            text_encoder_path=self.model_path,\n            logger=self.logger,\n            device=device,\n        )\n        self.dtype = self.model.dtype\n        self.device = self.model.device\n\n        self.tokenizer, self.tokenizer_path = load_tokenizer(\n            tokenizer_type=self.tokenizer_type,\n            tokenizer_path=self.tokenizer_path,\n            padding_side=\"right\",\n            logger=self.logger,\n        )\n\n    def __repr__(self):\n        return f\"{self.text_encoder_type} ({self.precision} - {self.model_path})\"\n\n    @staticmethod\n    def apply_text_to_template(text, template, prevent_empty_text=True):\n        \"\"\"\n        Apply text to template.\n\n        Args:\n            text (str): Input text.\n            template (str or list): Template string or list of chat conversation.\n            prevent_empty_text (bool): If Ture, we will prevent the user text from being empty\n                by adding a space. Defaults to True.\n        \"\"\"\n        if isinstance(template, str):\n            # Will send string to tokenizer. Used for llm\n            return template.format(text)\n        else:\n            raise TypeError(f\"Unsupported template type: {type(template)}\")\n\n    def text2tokens(self, text, data_type=\"image\"):\n        \"\"\"\n        Tokenize the input text.\n\n        Args:\n            text (str or list): Input text.\n        \"\"\"\n        tokenize_input_type = \"str\"\n        if self.use_template:\n            if data_type == \"image\":\n                prompt_template = self.prompt_template[\"template\"]\n            elif data_type == \"video\":\n                prompt_template = self.prompt_template_video[\"template\"]\n            else:\n                raise ValueError(f\"Unsupported data type: {data_type}\")\n            if isinstance(text, (list, tuple)):\n                text = [\n                    self.apply_text_to_template(one_text, prompt_template)\n                    for one_text in text\n                ]\n                if isinstance(text[0], list):\n                    tokenize_input_type = \"list\"\n            elif isinstance(text, str):\n                text = self.apply_text_to_template(text, prompt_template)\n                if isinstance(text, list):\n                    tokenize_input_type = \"list\"\n            else:\n                raise TypeError(f\"Unsupported text type: {type(text)}\")\n\n        kwargs = dict(\n            truncation=True,\n            max_length=self.max_length,\n            padding=\"max_length\",\n            return_tensors=\"pt\",\n        )\n        if tokenize_input_type == \"str\":\n            return self.tokenizer(\n                text,\n                return_length=False,\n                return_overflowing_tokens=False,\n                return_attention_mask=True,\n                **kwargs,\n            )\n        elif tokenize_input_type == \"list\":\n            return self.tokenizer.apply_chat_template(\n                text,\n                add_generation_prompt=True,\n                tokenize=True,\n                return_dict=True,\n                **kwargs,\n            )\n        else:\n            raise ValueError(f\"Unsupported tokenize_input_type: {tokenize_input_type}\")\n\n    def encode(\n        self,\n        batch_encoding,\n        use_attention_mask=None,\n        output_hidden_states=False,\n        do_sample=None,\n        hidden_state_skip_layer=None,\n        return_texts=False,\n        data_type=\"image\",\n        device=None,\n    ):\n        \"\"\"\n        Args:\n            batch_encoding (dict): Batch encoding from tokenizer.\n            use_attention_mask (bool): Whether to use attention mask. If None, use self.use_attention_mask.\n                Defaults to None.\n            output_hidden_states (bool): Whether to output hidden states. If False, return the value of\n                self.output_key. If True, return the entire output. If set self.hidden_state_skip_layer,\n                output_hidden_states will be set True. Defaults to False.\n            do_sample (bool): Whether to sample from the model. Used for Decoder-Only LLMs. Defaults to None.\n                When self.produce is False, do_sample is set to True by default.\n            hidden_state_skip_layer (int): Number of hidden states to hidden_state_skip_layer. 0 means the last layer.\n                If None, self.output_key will be used. Defaults to None.\n            return_texts (bool): Whether to return the decoded texts. Defaults to False.\n        \"\"\"\n        device = self.model.device if device is None else device\n        use_attention_mask = use_default(use_attention_mask, self.use_attention_mask)\n        hidden_state_skip_layer = use_default(\n            hidden_state_skip_layer, self.hidden_state_skip_layer\n        )\n        do_sample = use_default(do_sample, not self.reproduce)\n        attention_mask = (\n            batch_encoding[\"attention_mask\"].to(device) if use_attention_mask else None\n        )\n        outputs = self.model(\n            input_ids=batch_encoding[\"input_ids\"].to(device),\n            attention_mask=attention_mask,\n            output_hidden_states=output_hidden_states\n            or hidden_state_skip_layer is not None,\n        )\n        if hidden_state_skip_layer is not None:\n            last_hidden_state = outputs.hidden_states[-(hidden_state_skip_layer + 1)]\n            # Real last hidden state already has layer norm applied. So here we only apply it\n            # for intermediate layers.\n            if hidden_state_skip_layer > 0 and self.apply_final_norm:\n                last_hidden_state = self.model.final_layer_norm(last_hidden_state)\n        else:\n            last_hidden_state = outputs[self.output_key]\n\n        # Remove hidden states of instruction tokens, only keep prompt tokens.\n        if self.use_template:\n            if data_type == \"image\":\n                crop_start = self.prompt_template.get(\"crop_start\", -1)\n            elif data_type == \"video\":\n                crop_start = self.prompt_template_video.get(\"crop_start\", -1)\n            else:\n                raise ValueError(f\"Unsupported data type: {data_type}\")\n            if crop_start > 0:\n                last_hidden_state = last_hidden_state[:, crop_start:]\n                attention_mask = (\n                    attention_mask[:, crop_start:] if use_attention_mask else None\n                )\n\n        if output_hidden_states:\n            return TextEncoderModelOutput(\n                last_hidden_state, attention_mask, outputs.hidden_states\n            )\n        return TextEncoderModelOutput(last_hidden_state, attention_mask)\n\n    def forward(\n        self,\n        text,\n        use_attention_mask=None,\n        output_hidden_states=False,\n        do_sample=False,\n        hidden_state_skip_layer=None,\n        return_texts=False,\n    ):\n        batch_encoding = self.text2tokens(text)\n        return self.encode(\n            batch_encoding,\n            use_attention_mask=use_attention_mask,\n            output_hidden_states=output_hidden_states,\n            do_sample=do_sample,\n            hidden_state_skip_layer=hidden_state_skip_layer,\n            return_texts=return_texts,\n        )\n"}
{"type": "source_file", "path": "svg/models/hyvideo/utils/file_utils.py", "content": "import os\nfrom pathlib import Path\nfrom einops import rearrange\n\nimport torch\nimport torchvision\nimport numpy as np\nimport imageio\n\nCODE_SUFFIXES = {\n    \".py\",  # Python codes\n    \".sh\",  # Shell scripts\n    \".yaml\",\n    \".yml\",  # Configuration files\n}\n\n\ndef safe_dir(path):\n    \"\"\"\n    Create a directory (or the parent directory of a file) if it does not exist.\n\n    Args:\n        path (str or Path): Path to the directory.\n\n    Returns:\n        path (Path): Path object of the directory.\n    \"\"\"\n    path = Path(path)\n    path.mkdir(exist_ok=True, parents=True)\n    return path\n\n\ndef safe_file(path):\n    \"\"\"\n    Create the parent directory of a file if it does not exist.\n\n    Args:\n        path (str or Path): Path to the file.\n\n    Returns:\n        path (Path): Path object of the file.\n    \"\"\"\n    path = Path(path)\n    path.parent.mkdir(exist_ok=True, parents=True)\n    return path\n\ndef save_videos_grid(videos: torch.Tensor, path: str, rescale=False, n_rows=1, fps=24):\n    \"\"\"save videos by video tensor\n       copy from https://github.com/guoyww/AnimateDiff/blob/e92bd5671ba62c0d774a32951453e328018b7c5b/animatediff/utils/util.py#L61\n\n    Args:\n        videos (torch.Tensor): video tensor predicted by the model\n        path (str): path to save video\n        rescale (bool, optional): rescale the video tensor from [-1, 1] to  . Defaults to False.\n        n_rows (int, optional): Defaults to 1.\n        fps (int, optional): video save fps. Defaults to 8.\n    \"\"\"\n    videos = rearrange(videos, \"b c t h w -> t b c h w\")\n    outputs = []\n    for x in videos:\n        x = torchvision.utils.make_grid(x, nrow=n_rows)\n        x = x.transpose(0, 1).transpose(1, 2).squeeze(-1)\n        if rescale:\n            x = (x + 1.0) / 2.0  # -1,1 -> 0,1\n        x = torch.clamp(x, 0, 1)\n        x = (x * 255).numpy().astype(np.uint8)\n        outputs.append(x)\n\n    os.makedirs(os.path.dirname(path), exist_ok=True)\n    imageio.mimsave(path, outputs, fps=fps)\n"}
{"type": "source_file", "path": "svg/models/hyvideo/vae/autoencoder_kl_causal_3d.py", "content": "# Copyright 2024 The HuggingFace Team. All rights reserved.\r\n#\r\n# Licensed under the Apache License, Version 2.0 (the \"License\");\r\n# you may not use this file except in compliance with the License.\r\n# You may obtain a copy of the License at\r\n#\r\n#     http://www.apache.org/licenses/LICENSE-2.0\r\n#\r\n# Unless required by applicable law or agreed to in writing, software\r\n# distributed under the License is distributed on an \"AS IS\" BASIS,\r\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n# See the License for the specific language governing permissions and\r\n# limitations under the License.\r\n# ==============================================================================\r\n#\r\n# Modified from diffusers==0.29.2\r\n#\r\n# ==============================================================================\r\nfrom typing import Dict, Optional, Tuple, Union\r\nfrom dataclasses import dataclass\r\n\r\nimport torch\r\nimport torch.nn as nn\r\n\r\nfrom diffusers.configuration_utils import ConfigMixin, register_to_config\r\n\r\ntry:\r\n    # This diffusers is modified and packed in the mirror.\r\n    from diffusers.loaders import FromOriginalVAEMixin\r\nexcept ImportError:\r\n    # Use this to be compatible with the original diffusers.\r\n    from diffusers.loaders.single_file_model import FromOriginalModelMixin as FromOriginalVAEMixin\r\nfrom diffusers.utils.accelerate_utils import apply_forward_hook\r\nfrom diffusers.models.attention_processor import (\r\n    ADDED_KV_ATTENTION_PROCESSORS,\r\n    CROSS_ATTENTION_PROCESSORS,\r\n    Attention,\r\n    AttentionProcessor,\r\n    AttnAddedKVProcessor,\r\n    AttnProcessor,\r\n)\r\nfrom diffusers.models.modeling_outputs import AutoencoderKLOutput\r\nfrom diffusers.models.modeling_utils import ModelMixin\r\nfrom .vae import DecoderCausal3D, BaseOutput, DecoderOutput, DiagonalGaussianDistribution, EncoderCausal3D\r\n\r\n\r\n@dataclass\r\nclass DecoderOutput2(BaseOutput):\r\n    sample: torch.FloatTensor\r\n    posterior: Optional[DiagonalGaussianDistribution] = None\r\n\r\n\r\nclass AutoencoderKLCausal3D(ModelMixin, ConfigMixin, FromOriginalVAEMixin):\r\n    r\"\"\"\r\n    A VAE model with KL loss for encoding images/videos into latents and decoding latent representations into images/videos.\r\n\r\n    This model inherits from [`ModelMixin`]. Check the superclass documentation for it's generic methods implemented\r\n    for all models (such as downloading or saving).\r\n    \"\"\"\r\n\r\n    _supports_gradient_checkpointing = True\r\n\r\n    @register_to_config\r\n    def __init__(\r\n        self,\r\n        in_channels: int = 3,\r\n        out_channels: int = 3,\r\n        down_block_types: Tuple[str] = (\"DownEncoderBlockCausal3D\",),\r\n        up_block_types: Tuple[str] = (\"UpDecoderBlockCausal3D\",),\r\n        block_out_channels: Tuple[int] = (64,),\r\n        layers_per_block: int = 1,\r\n        act_fn: str = \"silu\",\r\n        latent_channels: int = 4,\r\n        norm_num_groups: int = 32,\r\n        sample_size: int = 32,\r\n        sample_tsize: int = 64,\r\n        scaling_factor: float = 0.18215,\r\n        force_upcast: float = True,\r\n        spatial_compression_ratio: int = 8,\r\n        time_compression_ratio: int = 4,\r\n        mid_block_add_attention: bool = True,\r\n    ):\r\n        super().__init__()\r\n\r\n        self.time_compression_ratio = time_compression_ratio\r\n\r\n        self.encoder = EncoderCausal3D(\r\n            in_channels=in_channels,\r\n            out_channels=latent_channels,\r\n            down_block_types=down_block_types,\r\n            block_out_channels=block_out_channels,\r\n            layers_per_block=layers_per_block,\r\n            act_fn=act_fn,\r\n            norm_num_groups=norm_num_groups,\r\n            double_z=True,\r\n            time_compression_ratio=time_compression_ratio,\r\n            spatial_compression_ratio=spatial_compression_ratio,\r\n            mid_block_add_attention=mid_block_add_attention,\r\n        )\r\n\r\n        self.decoder = DecoderCausal3D(\r\n            in_channels=latent_channels,\r\n            out_channels=out_channels,\r\n            up_block_types=up_block_types,\r\n            block_out_channels=block_out_channels,\r\n            layers_per_block=layers_per_block,\r\n            norm_num_groups=norm_num_groups,\r\n            act_fn=act_fn,\r\n            time_compression_ratio=time_compression_ratio,\r\n            spatial_compression_ratio=spatial_compression_ratio,\r\n            mid_block_add_attention=mid_block_add_attention,\r\n        )\r\n\r\n        self.quant_conv = nn.Conv3d(2 * latent_channels, 2 * latent_channels, kernel_size=1)\r\n        self.post_quant_conv = nn.Conv3d(latent_channels, latent_channels, kernel_size=1)\r\n\r\n        self.use_slicing = False\r\n        self.use_spatial_tiling = False\r\n        self.use_temporal_tiling = False\r\n\r\n        # only relevant if vae tiling is enabled\r\n        self.tile_sample_min_tsize = sample_tsize\r\n        self.tile_latent_min_tsize = sample_tsize // time_compression_ratio\r\n\r\n        self.tile_sample_min_size = self.config.sample_size\r\n        sample_size = (\r\n            self.config.sample_size[0]\r\n            if isinstance(self.config.sample_size, (list, tuple))\r\n            else self.config.sample_size\r\n        )\r\n        self.tile_latent_min_size = int(sample_size / (2 ** (len(self.config.block_out_channels) - 1)))\r\n        self.tile_overlap_factor = 0.25\r\n\r\n    def _set_gradient_checkpointing(self, module, value=False):\r\n        if isinstance(module, (EncoderCausal3D, DecoderCausal3D)):\r\n            module.gradient_checkpointing = value\r\n\r\n    def enable_temporal_tiling(self, use_tiling: bool = True):\r\n        self.use_temporal_tiling = use_tiling\r\n\r\n    def disable_temporal_tiling(self):\r\n        self.enable_temporal_tiling(False)\r\n\r\n    def enable_spatial_tiling(self, use_tiling: bool = True):\r\n        self.use_spatial_tiling = use_tiling\r\n\r\n    def disable_spatial_tiling(self):\r\n        self.enable_spatial_tiling(False)\r\n\r\n    def enable_tiling(self, use_tiling: bool = True):\r\n        r\"\"\"\r\n        Enable tiled VAE decoding. When this option is enabled, the VAE will split the input tensor into tiles to\r\n        compute decoding and encoding in several steps. This is useful for saving a large amount of memory and to allow\r\n        processing larger videos.\r\n        \"\"\"\r\n        self.enable_spatial_tiling(use_tiling)\r\n        self.enable_temporal_tiling(use_tiling)\r\n\r\n    def disable_tiling(self):\r\n        r\"\"\"\r\n        Disable tiled VAE decoding. If `enable_tiling` was previously enabled, this method will go back to computing\r\n        decoding in one step.\r\n        \"\"\"\r\n        self.disable_spatial_tiling()\r\n        self.disable_temporal_tiling()\r\n\r\n    def enable_slicing(self):\r\n        r\"\"\"\r\n        Enable sliced VAE decoding. When this option is enabled, the VAE will split the input tensor in slices to\r\n        compute decoding in several steps. This is useful to save some memory and allow larger batch sizes.\r\n        \"\"\"\r\n        self.use_slicing = True\r\n\r\n    def disable_slicing(self):\r\n        r\"\"\"\r\n        Disable sliced VAE decoding. If `enable_slicing` was previously enabled, this method will go back to computing\r\n        decoding in one step.\r\n        \"\"\"\r\n        self.use_slicing = False\r\n\r\n    @property\r\n    # Copied from diffusers.models.unet_2d_condition.UNet2DConditionModel.attn_processors\r\n    def attn_processors(self) -> Dict[str, AttentionProcessor]:\r\n        r\"\"\"\r\n        Returns:\r\n            `dict` of attention processors: A dictionary containing all attention processors used in the model with\r\n            indexed by its weight name.\r\n        \"\"\"\r\n        # set recursively\r\n        processors = {}\r\n\r\n        def fn_recursive_add_processors(name: str, module: torch.nn.Module, processors: Dict[str, AttentionProcessor]):\r\n            if hasattr(module, \"get_processor\"):\r\n                processors[f\"{name}.processor\"] = module.get_processor(return_deprecated_lora=True)\r\n\r\n            for sub_name, child in module.named_children():\r\n                fn_recursive_add_processors(f\"{name}.{sub_name}\", child, processors)\r\n\r\n            return processors\r\n\r\n        for name, module in self.named_children():\r\n            fn_recursive_add_processors(name, module, processors)\r\n\r\n        return processors\r\n\r\n    # Copied from diffusers.models.unet_2d_condition.UNet2DConditionModel.set_attn_processor\r\n    def set_attn_processor(\r\n        self, processor: Union[AttentionProcessor, Dict[str, AttentionProcessor]], _remove_lora=False\r\n    ):\r\n        r\"\"\"\r\n        Sets the attention processor to use to compute attention.\r\n\r\n        Parameters:\r\n            processor (`dict` of `AttentionProcessor` or only `AttentionProcessor`):\r\n                The instantiated processor class or a dictionary of processor classes that will be set as the processor\r\n                for **all** `Attention` layers.\r\n\r\n                If `processor` is a dict, the key needs to define the path to the corresponding cross attention\r\n                processor. This is strongly recommended when setting trainable attention processors.\r\n\r\n        \"\"\"\r\n        count = len(self.attn_processors.keys())\r\n\r\n        if isinstance(processor, dict) and len(processor) != count:\r\n            raise ValueError(\r\n                f\"A dict of processors was passed, but the number of processors {len(processor)} does not match the\"\r\n                f\" number of attention layers: {count}. Please make sure to pass {count} processor classes.\"\r\n            )\r\n\r\n        def fn_recursive_attn_processor(name: str, module: torch.nn.Module, processor):\r\n            if hasattr(module, \"set_processor\"):\r\n                if not isinstance(processor, dict):\r\n                    module.set_processor(processor, _remove_lora=_remove_lora)\r\n                else:\r\n                    module.set_processor(processor.pop(f\"{name}.processor\"), _remove_lora=_remove_lora)\r\n\r\n            for sub_name, child in module.named_children():\r\n                fn_recursive_attn_processor(f\"{name}.{sub_name}\", child, processor)\r\n\r\n        for name, module in self.named_children():\r\n            fn_recursive_attn_processor(name, module, processor)\r\n\r\n    # Copied from diffusers.models.unet_2d_condition.UNet2DConditionModel.set_default_attn_processor\r\n    def set_default_attn_processor(self):\r\n        \"\"\"\r\n        Disables custom attention processors and sets the default attention implementation.\r\n        \"\"\"\r\n        if all(proc.__class__ in ADDED_KV_ATTENTION_PROCESSORS for proc in self.attn_processors.values()):\r\n            processor = AttnAddedKVProcessor()\r\n        elif all(proc.__class__ in CROSS_ATTENTION_PROCESSORS for proc in self.attn_processors.values()):\r\n            processor = AttnProcessor()\r\n        else:\r\n            raise ValueError(\r\n                f\"Cannot call `set_default_attn_processor` when attention processors are of type {next(iter(self.attn_processors.values()))}\"\r\n            )\r\n\r\n        self.set_attn_processor(processor, _remove_lora=True)\r\n\r\n    @apply_forward_hook\r\n    def encode(\r\n        self, x: torch.FloatTensor, return_dict: bool = True\r\n    ) -> Union[AutoencoderKLOutput, Tuple[DiagonalGaussianDistribution]]:\r\n        \"\"\"\r\n        Encode a batch of images/videos into latents.\r\n\r\n        Args:\r\n            x (`torch.FloatTensor`): Input batch of images/videos.\r\n            return_dict (`bool`, *optional*, defaults to `True`):\r\n                Whether to return a [`~models.autoencoder_kl.AutoencoderKLOutput`] instead of a plain tuple.\r\n\r\n        Returns:\r\n                The latent representations of the encoded images/videos. If `return_dict` is True, a\r\n                [`~models.autoencoder_kl.AutoencoderKLOutput`] is returned, otherwise a plain `tuple` is returned.\r\n        \"\"\"\r\n        assert len(x.shape) == 5, \"The input tensor should have 5 dimensions.\"\r\n\r\n        if self.use_temporal_tiling and x.shape[2] > self.tile_sample_min_tsize:\r\n            return self.temporal_tiled_encode(x, return_dict=return_dict)\r\n\r\n        if self.use_spatial_tiling and (x.shape[-1] > self.tile_sample_min_size or x.shape[-2] > self.tile_sample_min_size):\r\n            return self.spatial_tiled_encode(x, return_dict=return_dict)\r\n\r\n        if self.use_slicing and x.shape[0] > 1:\r\n            encoded_slices = [self.encoder(x_slice) for x_slice in x.split(1)]\r\n            h = torch.cat(encoded_slices)\r\n        else:\r\n            h = self.encoder(x)\r\n\r\n        moments = self.quant_conv(h)\r\n        posterior = DiagonalGaussianDistribution(moments)\r\n\r\n        if not return_dict:\r\n            return (posterior,)\r\n\r\n        return AutoencoderKLOutput(latent_dist=posterior)\r\n\r\n    def _decode(self, z: torch.FloatTensor, return_dict: bool = True) -> Union[DecoderOutput, torch.FloatTensor]:\r\n        assert len(z.shape) == 5, \"The input tensor should have 5 dimensions.\"\r\n\r\n        if self.use_temporal_tiling and z.shape[2] > self.tile_latent_min_tsize:\r\n            return self.temporal_tiled_decode(z, return_dict=return_dict)\r\n\r\n        if self.use_spatial_tiling and (z.shape[-1] > self.tile_latent_min_size or z.shape[-2] > self.tile_latent_min_size):\r\n            return self.spatial_tiled_decode(z, return_dict=return_dict)\r\n\r\n        z = self.post_quant_conv(z)\r\n        dec = self.decoder(z)\r\n\r\n        if not return_dict:\r\n            return (dec,)\r\n\r\n        return DecoderOutput(sample=dec)\r\n\r\n    @apply_forward_hook\r\n    def decode(\r\n        self, z: torch.FloatTensor, return_dict: bool = True, generator=None\r\n    ) -> Union[DecoderOutput, torch.FloatTensor]:\r\n        \"\"\"\r\n        Decode a batch of images/videos.\r\n\r\n        Args:\r\n            z (`torch.FloatTensor`): Input batch of latent vectors.\r\n            return_dict (`bool`, *optional*, defaults to `True`):\r\n                Whether to return a [`~models.vae.DecoderOutput`] instead of a plain tuple.\r\n\r\n        Returns:\r\n            [`~models.vae.DecoderOutput`] or `tuple`:\r\n                If return_dict is True, a [`~models.vae.DecoderOutput`] is returned, otherwise a plain `tuple` is\r\n                returned.\r\n\r\n        \"\"\"\r\n        if self.use_slicing and z.shape[0] > 1:\r\n            decoded_slices = [self._decode(z_slice).sample for z_slice in z.split(1)]\r\n            decoded = torch.cat(decoded_slices)\r\n        else:\r\n            decoded = self._decode(z).sample\r\n\r\n        if not return_dict:\r\n            return (decoded,)\r\n\r\n        return DecoderOutput(sample=decoded)\r\n\r\n    def blend_v(self, a: torch.Tensor, b: torch.Tensor, blend_extent: int) -> torch.Tensor:\r\n        blend_extent = min(a.shape[-2], b.shape[-2], blend_extent)\r\n        for y in range(blend_extent):\r\n            b[:, :, :, y, :] = a[:, :, :, -blend_extent + y, :] * (1 - y / blend_extent) + b[:, :, :, y, :] * (y / blend_extent)\r\n        return b\r\n\r\n    def blend_h(self, a: torch.Tensor, b: torch.Tensor, blend_extent: int) -> torch.Tensor:\r\n        blend_extent = min(a.shape[-1], b.shape[-1], blend_extent)\r\n        for x in range(blend_extent):\r\n            b[:, :, :, :, x] = a[:, :, :, :, -blend_extent + x] * (1 - x / blend_extent) + b[:, :, :, :, x] * (x / blend_extent)\r\n        return b\r\n\r\n    def blend_t(self, a: torch.Tensor, b: torch.Tensor, blend_extent: int) -> torch.Tensor:\r\n        blend_extent = min(a.shape[-3], b.shape[-3], blend_extent)\r\n        for x in range(blend_extent):\r\n            b[:, :, x, :, :] = a[:, :, -blend_extent + x, :, :] * (1 - x / blend_extent) + b[:, :, x, :, :] * (x / blend_extent)\r\n        return b\r\n\r\n    def spatial_tiled_encode(self, x: torch.FloatTensor, return_dict: bool = True, return_moments: bool = False) -> AutoencoderKLOutput:\r\n        r\"\"\"Encode a batch of images/videos using a tiled encoder.\r\n\r\n        When this option is enabled, the VAE will split the input tensor into tiles to compute encoding in several\r\n        steps. This is useful to keep memory use constant regardless of image/videos size. The end result of tiled encoding is\r\n        different from non-tiled encoding because each tile uses a different encoder. To avoid tiling artifacts, the\r\n        tiles overlap and are blended together to form a smooth output. You may still see tile-sized changes in the\r\n        output, but they should be much less noticeable.\r\n\r\n        Args:\r\n            x (`torch.FloatTensor`): Input batch of images/videos.\r\n            return_dict (`bool`, *optional*, defaults to `True`):\r\n                Whether or not to return a [`~models.autoencoder_kl.AutoencoderKLOutput`] instead of a plain tuple.\r\n\r\n        Returns:\r\n            [`~models.autoencoder_kl.AutoencoderKLOutput`] or `tuple`:\r\n                If return_dict is True, a [`~models.autoencoder_kl.AutoencoderKLOutput`] is returned, otherwise a plain\r\n                `tuple` is returned.\r\n        \"\"\"\r\n        overlap_size = int(self.tile_sample_min_size * (1 - self.tile_overlap_factor))\r\n        blend_extent = int(self.tile_latent_min_size * self.tile_overlap_factor)\r\n        row_limit = self.tile_latent_min_size - blend_extent\r\n\r\n        # Split video into tiles and encode them separately.\r\n        rows = []\r\n        for i in range(0, x.shape[-2], overlap_size):\r\n            row = []\r\n            for j in range(0, x.shape[-1], overlap_size):\r\n                tile = x[:, :, :, i: i + self.tile_sample_min_size, j: j + self.tile_sample_min_size]\r\n                tile = self.encoder(tile)\r\n                tile = self.quant_conv(tile)\r\n                row.append(tile)\r\n            rows.append(row)\r\n        result_rows = []\r\n        for i, row in enumerate(rows):\r\n            result_row = []\r\n            for j, tile in enumerate(row):\r\n                # blend the above tile and the left tile\r\n                # to the current tile and add the current tile to the result row\r\n                if i > 0:\r\n                    tile = self.blend_v(rows[i - 1][j], tile, blend_extent)\r\n                if j > 0:\r\n                    tile = self.blend_h(row[j - 1], tile, blend_extent)\r\n                result_row.append(tile[:, :, :, :row_limit, :row_limit])\r\n            result_rows.append(torch.cat(result_row, dim=-1))\r\n\r\n        moments = torch.cat(result_rows, dim=-2)\r\n        if return_moments:\r\n            return moments\r\n\r\n        posterior = DiagonalGaussianDistribution(moments)\r\n        if not return_dict:\r\n            return (posterior,)\r\n\r\n        return AutoencoderKLOutput(latent_dist=posterior)\r\n\r\n    def spatial_tiled_decode(self, z: torch.FloatTensor, return_dict: bool = True) -> Union[DecoderOutput, torch.FloatTensor]:\r\n        r\"\"\"\r\n        Decode a batch of images/videos using a tiled decoder.\r\n\r\n        Args:\r\n            z (`torch.FloatTensor`): Input batch of latent vectors.\r\n            return_dict (`bool`, *optional*, defaults to `True`):\r\n                Whether or not to return a [`~models.vae.DecoderOutput`] instead of a plain tuple.\r\n\r\n        Returns:\r\n            [`~models.vae.DecoderOutput`] or `tuple`:\r\n                If return_dict is True, a [`~models.vae.DecoderOutput`] is returned, otherwise a plain `tuple` is\r\n                returned.\r\n        \"\"\"\r\n        overlap_size = int(self.tile_latent_min_size * (1 - self.tile_overlap_factor))\r\n        blend_extent = int(self.tile_sample_min_size * self.tile_overlap_factor)\r\n        row_limit = self.tile_sample_min_size - blend_extent\r\n\r\n        # Split z into overlapping tiles and decode them separately.\r\n        # The tiles have an overlap to avoid seams between tiles.\r\n        rows = []\r\n        for i in range(0, z.shape[-2], overlap_size):\r\n            row = []\r\n            for j in range(0, z.shape[-1], overlap_size):\r\n                tile = z[:, :, :, i: i + self.tile_latent_min_size, j: j + self.tile_latent_min_size]\r\n                tile = self.post_quant_conv(tile)\r\n                decoded = self.decoder(tile)\r\n                row.append(decoded)\r\n            rows.append(row)\r\n        result_rows = []\r\n        for i, row in enumerate(rows):\r\n            result_row = []\r\n            for j, tile in enumerate(row):\r\n                # blend the above tile and the left tile\r\n                # to the current tile and add the current tile to the result row\r\n                if i > 0:\r\n                    tile = self.blend_v(rows[i - 1][j], tile, blend_extent)\r\n                if j > 0:\r\n                    tile = self.blend_h(row[j - 1], tile, blend_extent)\r\n                result_row.append(tile[:, :, :, :row_limit, :row_limit])\r\n            result_rows.append(torch.cat(result_row, dim=-1))\r\n\r\n        dec = torch.cat(result_rows, dim=-2)\r\n        if not return_dict:\r\n            return (dec,)\r\n\r\n        return DecoderOutput(sample=dec)\r\n\r\n    def temporal_tiled_encode(self, x: torch.FloatTensor, return_dict: bool = True) -> AutoencoderKLOutput:\r\n\r\n        B, C, T, H, W = x.shape\r\n        overlap_size = int(self.tile_sample_min_tsize * (1 - self.tile_overlap_factor))\r\n        blend_extent = int(self.tile_latent_min_tsize * self.tile_overlap_factor)\r\n        t_limit = self.tile_latent_min_tsize - blend_extent\r\n\r\n        # Split the video into tiles and encode them separately.\r\n        row = []\r\n        for i in range(0, T, overlap_size):\r\n            tile = x[:, :, i: i + self.tile_sample_min_tsize + 1, :, :]\r\n            if self.use_spatial_tiling and (tile.shape[-1] > self.tile_sample_min_size or tile.shape[-2] > self.tile_sample_min_size):\r\n                tile = self.spatial_tiled_encode(tile, return_moments=True)\r\n            else:\r\n                tile = self.encoder(tile)\r\n                tile = self.quant_conv(tile)\r\n            if i > 0:\r\n                tile = tile[:, :, 1:, :, :]\r\n            row.append(tile)\r\n        result_row = []\r\n        for i, tile in enumerate(row):\r\n            if i > 0:\r\n                tile = self.blend_t(row[i - 1], tile, blend_extent)\r\n                result_row.append(tile[:, :, :t_limit, :, :])\r\n            else:\r\n                result_row.append(tile[:, :, :t_limit + 1, :, :])\r\n\r\n        moments = torch.cat(result_row, dim=2)\r\n        posterior = DiagonalGaussianDistribution(moments)\r\n\r\n        if not return_dict:\r\n            return (posterior,)\r\n\r\n        return AutoencoderKLOutput(latent_dist=posterior)\r\n\r\n    def temporal_tiled_decode(self, z: torch.FloatTensor, return_dict: bool = True) -> Union[DecoderOutput, torch.FloatTensor]:\r\n        # Split z into overlapping tiles and decode them separately.\r\n\r\n        B, C, T, H, W = z.shape\r\n        overlap_size = int(self.tile_latent_min_tsize * (1 - self.tile_overlap_factor))\r\n        blend_extent = int(self.tile_sample_min_tsize * self.tile_overlap_factor)\r\n        t_limit = self.tile_sample_min_tsize - blend_extent\r\n\r\n        row = []\r\n        for i in range(0, T, overlap_size):\r\n            tile = z[:, :, i: i + self.tile_latent_min_tsize + 1, :, :]\r\n            if self.use_spatial_tiling and (tile.shape[-1] > self.tile_latent_min_size or tile.shape[-2] > self.tile_latent_min_size):\r\n                decoded = self.spatial_tiled_decode(tile, return_dict=True).sample\r\n            else:\r\n                tile = self.post_quant_conv(tile)\r\n                decoded = self.decoder(tile)\r\n            if i > 0:\r\n                decoded = decoded[:, :, 1:, :, :]\r\n            row.append(decoded)\r\n        result_row = []\r\n        for i, tile in enumerate(row):\r\n            if i > 0:\r\n                tile = self.blend_t(row[i - 1], tile, blend_extent)\r\n                result_row.append(tile[:, :, :t_limit, :, :])\r\n            else:\r\n                result_row.append(tile[:, :, :t_limit + 1, :, :])\r\n\r\n        dec = torch.cat(result_row, dim=2)\r\n        if not return_dict:\r\n            return (dec,)\r\n\r\n        return DecoderOutput(sample=dec)\r\n\r\n    def forward(\r\n        self,\r\n        sample: torch.FloatTensor,\r\n        sample_posterior: bool = False,\r\n        return_dict: bool = True,\r\n        return_posterior: bool = False,\r\n        generator: Optional[torch.Generator] = None,\r\n    ) -> Union[DecoderOutput2, torch.FloatTensor]:\r\n        r\"\"\"\r\n        Args:\r\n            sample (`torch.FloatTensor`): Input sample.\r\n            sample_posterior (`bool`, *optional*, defaults to `False`):\r\n                Whether to sample from the posterior.\r\n            return_dict (`bool`, *optional*, defaults to `True`):\r\n                Whether or not to return a [`DecoderOutput`] instead of a plain tuple.\r\n        \"\"\"\r\n        x = sample\r\n        posterior = self.encode(x).latent_dist\r\n        if sample_posterior:\r\n            z = posterior.sample(generator=generator)\r\n        else:\r\n            z = posterior.mode()\r\n        dec = self.decode(z).sample\r\n\r\n        if not return_dict:\r\n            if return_posterior:\r\n                return (dec, posterior)\r\n            else:\r\n                return (dec,)\r\n        if return_posterior:\r\n            return DecoderOutput2(sample=dec, posterior=posterior)\r\n        else:\r\n            return DecoderOutput2(sample=dec)\r\n\r\n    # Copied from diffusers.models.unet_2d_condition.UNet2DConditionModel.fuse_qkv_projections\r\n    def fuse_qkv_projections(self):\r\n        \"\"\"\r\n        Enables fused QKV projections. For self-attention modules, all projection matrices (i.e., query,\r\n        key, value) are fused. For cross-attention modules, key and value projection matrices are fused.\r\n\r\n        <Tip warning={true}>\r\n\r\n        This API is  experimental.\r\n\r\n        </Tip>\r\n        \"\"\"\r\n        self.original_attn_processors = None\r\n\r\n        for _, attn_processor in self.attn_processors.items():\r\n            if \"Added\" in str(attn_processor.__class__.__name__):\r\n                raise ValueError(\"`fuse_qkv_projections()` is not supported for models having added KV projections.\")\r\n\r\n        self.original_attn_processors = self.attn_processors\r\n\r\n        for module in self.modules():\r\n            if isinstance(module, Attention):\r\n                module.fuse_projections(fuse=True)\r\n\r\n    # Copied from diffusers.models.unet_2d_condition.UNet2DConditionModel.unfuse_qkv_projections\r\n    def unfuse_qkv_projections(self):\r\n        \"\"\"Disables the fused QKV projection if enabled.\r\n\r\n        <Tip warning={true}>\r\n\r\n        This API is  experimental.\r\n\r\n        </Tip>\r\n\r\n        \"\"\"\r\n        if self.original_attn_processors is not None:\r\n            self.set_attn_processor(self.original_attn_processors)\r\n"}
{"type": "source_file", "path": "svg/models/hyvideo/prompt_rewrite.py", "content": "normal_mode_prompt = \"\"\"Normal mode - Video Recaption Task:\n\nYou are a large language model specialized in rewriting video descriptions. Your task is to modify the input description.\n\n0. Preserve ALL information, including style words and technical terms.\n\n1. If the input is in Chinese, translate the entire description to English. \n\n2. If the input is just one or two words describing an object or person, provide a brief, simple description focusing on basic visual characteristics. Limit the description to 1-2 short sentences.\n\n3. If the input does not include style, lighting, atmosphere, you can make reasonable associations.\n\n4. Output ALL must be in English.\n\nGiven Input:\ninput: \"{input}\"\n\"\"\"\n\n\nmaster_mode_prompt = \"\"\"Master mode - Video Recaption Task:\n\nYou are a large language model specialized in rewriting video descriptions. Your task is to modify the input description.\n\n0. Preserve ALL information, including style words and technical terms.\n\n1. If the input is in Chinese, translate the entire description to English. \n\n2. To generate high-quality visual scenes with aesthetic appeal, it is necessary to carefully depict each visual element to create a unique aesthetic.\n\n3. If the input does not include style, lighting, atmosphere, you can make reasonable associations.\n\n4. Output ALL must be in English.\n\nGiven Input:\ninput: \"{input}\"\n\"\"\"\n\ndef get_rewrite_prompt(ori_prompt, mode=\"Normal\"):\n    if mode == \"Normal\":\n        prompt = normal_mode_prompt.format(input=ori_prompt)\n    elif mode == \"Master\":\n        prompt = master_mode_prompt.format(input=ori_prompt)\n    else:\n        raise Exception(\"Only supports Normal and Normal\", mode)\n    return prompt\n\nori_prompt = \"\"\nnormal_prompt = get_rewrite_prompt(ori_prompt, mode=\"Normal\")\nmaster_prompt = get_rewrite_prompt(ori_prompt, mode=\"Master\")\n\n# Then you can use the normal_prompt or master_prompt to access the hunyuan-large rewrite model to get the final prompt.\n"}
{"type": "source_file", "path": "svg/models/hyvideo/modules/token_refiner.py", "content": "from typing import Optional\n\nfrom einops import rearrange\nimport torch\nimport torch.nn as nn\n\nfrom .activation_layers import get_activation_layer\nfrom .attenion import attention\nfrom .norm_layers import get_norm_layer\nfrom .embed_layers import TimestepEmbedder, TextProjection\nfrom .attenion import attention\nfrom .mlp_layers import MLP\nfrom .modulate_layers import modulate, apply_gate\n\n\nclass IndividualTokenRefinerBlock(nn.Module):\n    def __init__(\n        self,\n        hidden_size,\n        heads_num,\n        mlp_width_ratio: str = 4.0,\n        mlp_drop_rate: float = 0.0,\n        act_type: str = \"silu\",\n        qk_norm: bool = False,\n        qk_norm_type: str = \"layer\",\n        qkv_bias: bool = True,\n        dtype: Optional[torch.dtype] = None,\n        device: Optional[torch.device] = None,\n    ):\n        factory_kwargs = {\"device\": device, \"dtype\": dtype}\n        super().__init__()\n        self.heads_num = heads_num\n        head_dim = hidden_size // heads_num\n        mlp_hidden_dim = int(hidden_size * mlp_width_ratio)\n\n        self.norm1 = nn.LayerNorm(\n            hidden_size, elementwise_affine=True, eps=1e-6, **factory_kwargs\n        )\n        self.self_attn_qkv = nn.Linear(\n            hidden_size, hidden_size * 3, bias=qkv_bias, **factory_kwargs\n        )\n        qk_norm_layer = get_norm_layer(qk_norm_type)\n        self.self_attn_q_norm = (\n            qk_norm_layer(head_dim, elementwise_affine=True, eps=1e-6, **factory_kwargs)\n            if qk_norm\n            else nn.Identity()\n        )\n        self.self_attn_k_norm = (\n            qk_norm_layer(head_dim, elementwise_affine=True, eps=1e-6, **factory_kwargs)\n            if qk_norm\n            else nn.Identity()\n        )\n        self.self_attn_proj = nn.Linear(\n            hidden_size, hidden_size, bias=qkv_bias, **factory_kwargs\n        )\n\n        self.norm2 = nn.LayerNorm(\n            hidden_size, elementwise_affine=True, eps=1e-6, **factory_kwargs\n        )\n        act_layer = get_activation_layer(act_type)\n        self.mlp = MLP(\n            in_channels=hidden_size,\n            hidden_channels=mlp_hidden_dim,\n            act_layer=act_layer,\n            drop=mlp_drop_rate,\n            **factory_kwargs,\n        )\n\n        self.adaLN_modulation = nn.Sequential(\n            act_layer(),\n            nn.Linear(hidden_size, 2 * hidden_size, bias=True, **factory_kwargs),\n        )\n        # Zero-initialize the modulation\n        nn.init.zeros_(self.adaLN_modulation[1].weight)\n        nn.init.zeros_(self.adaLN_modulation[1].bias)\n\n    def forward(\n        self,\n        x: torch.Tensor,\n        c: torch.Tensor,  # timestep_aware_representations + context_aware_representations\n        attn_mask: torch.Tensor = None,\n    ):\n        gate_msa, gate_mlp = self.adaLN_modulation(c).chunk(2, dim=1)\n\n        norm_x = self.norm1(x)\n        qkv = self.self_attn_qkv(norm_x)\n        q, k, v = rearrange(qkv, \"B L (K H D) -> K B L H D\", K=3, H=self.heads_num)\n        # Apply QK-Norm if needed\n        q = self.self_attn_q_norm(q).to(v)\n        k = self.self_attn_k_norm(k).to(v)\n\n        # Self-Attention\n        attn = attention(q, k, v, mode=\"torch\", attn_mask=attn_mask)\n\n        x = x + apply_gate(self.self_attn_proj(attn), gate_msa)\n\n        # FFN Layer\n        x = x + apply_gate(self.mlp(self.norm2(x)), gate_mlp)\n\n        return x\n\n\nclass IndividualTokenRefiner(nn.Module):\n    def __init__(\n        self,\n        hidden_size,\n        heads_num,\n        depth,\n        mlp_width_ratio: float = 4.0,\n        mlp_drop_rate: float = 0.0,\n        act_type: str = \"silu\",\n        qk_norm: bool = False,\n        qk_norm_type: str = \"layer\",\n        qkv_bias: bool = True,\n        dtype: Optional[torch.dtype] = None,\n        device: Optional[torch.device] = None,\n    ):\n        factory_kwargs = {\"device\": device, \"dtype\": dtype}\n        super().__init__()\n        self.blocks = nn.ModuleList(\n            [\n                IndividualTokenRefinerBlock(\n                    hidden_size=hidden_size,\n                    heads_num=heads_num,\n                    mlp_width_ratio=mlp_width_ratio,\n                    mlp_drop_rate=mlp_drop_rate,\n                    act_type=act_type,\n                    qk_norm=qk_norm,\n                    qk_norm_type=qk_norm_type,\n                    qkv_bias=qkv_bias,\n                    **factory_kwargs,\n                )\n                for _ in range(depth)\n            ]\n        )\n\n    def forward(\n        self,\n        x: torch.Tensor,\n        c: torch.LongTensor,\n        mask: Optional[torch.Tensor] = None,\n    ):\n        self_attn_mask = None\n        if mask is not None:\n            batch_size = mask.shape[0]\n            seq_len = mask.shape[1]\n            mask = mask.to(x.device)\n            # batch_size x 1 x seq_len x seq_len\n            self_attn_mask_1 = mask.view(batch_size, 1, 1, seq_len).repeat(\n                1, 1, seq_len, 1\n            )\n            # batch_size x 1 x seq_len x seq_len\n            self_attn_mask_2 = self_attn_mask_1.transpose(2, 3)\n            # batch_size x 1 x seq_len x seq_len, 1 for broadcasting of heads_num\n            self_attn_mask = (self_attn_mask_1 & self_attn_mask_2).bool()\n            # avoids self-attention weight being NaN for padding tokens\n            self_attn_mask[:, :, :, 0] = True\n\n        for block in self.blocks:\n            x = block(x, c, self_attn_mask)\n        return x\n\n\nclass SingleTokenRefiner(nn.Module):\n    \"\"\"\n    A single token refiner block for llm text embedding refine.\n    \"\"\"\n    def __init__(\n        self,\n        in_channels,\n        hidden_size,\n        heads_num,\n        depth,\n        mlp_width_ratio: float = 4.0,\n        mlp_drop_rate: float = 0.0,\n        act_type: str = \"silu\",\n        qk_norm: bool = False,\n        qk_norm_type: str = \"layer\",\n        qkv_bias: bool = True,\n        attn_mode: str = \"torch\",\n        dtype: Optional[torch.dtype] = None,\n        device: Optional[torch.device] = None,\n    ):\n        factory_kwargs = {\"device\": device, \"dtype\": dtype}\n        super().__init__()\n        self.attn_mode = attn_mode\n        assert self.attn_mode == \"torch\", \"Only support 'torch' mode for token refiner.\"\n\n        self.input_embedder = nn.Linear(\n            in_channels, hidden_size, bias=True, **factory_kwargs\n        )\n\n        act_layer = get_activation_layer(act_type)\n        # Build timestep embedding layer\n        self.t_embedder = TimestepEmbedder(hidden_size, act_layer, **factory_kwargs)\n        # Build context embedding layer\n        self.c_embedder = TextProjection(\n            in_channels, hidden_size, act_layer, **factory_kwargs\n        )\n\n        self.individual_token_refiner = IndividualTokenRefiner(\n            hidden_size=hidden_size,\n            heads_num=heads_num,\n            depth=depth,\n            mlp_width_ratio=mlp_width_ratio,\n            mlp_drop_rate=mlp_drop_rate,\n            act_type=act_type,\n            qk_norm=qk_norm,\n            qk_norm_type=qk_norm_type,\n            qkv_bias=qkv_bias,\n            **factory_kwargs,\n        )\n\n    def forward(\n        self,\n        x: torch.Tensor,\n        t: torch.LongTensor,\n        mask: Optional[torch.LongTensor] = None,\n    ):\n        timestep_aware_representations = self.t_embedder(t)\n\n        if mask is None:\n            context_aware_representations = x.mean(dim=1)\n        else:\n            mask_float = mask.float().unsqueeze(-1)  # [b, s1, 1]\n            context_aware_representations = (x * mask_float).sum(\n                dim=1\n            ) / mask_float.sum(dim=1)\n        context_aware_representations = self.c_embedder(context_aware_representations)\n        c = timestep_aware_representations + context_aware_representations\n\n        x = self.input_embedder(x)\n\n        x = self.individual_token_refiner(x, c, mask)\n\n        return x\n"}
{"type": "source_file", "path": "svg/models/hyvideo/utils/data_utils.py", "content": "import numpy as np\nimport math\n\n\ndef align_to(value, alignment):\n    \"\"\"align hight, width according to alignment\n\n    Args:\n        value (int): height or width\n        alignment (int): target alignment factor\n\n    Returns:\n        int: the aligned value\n    \"\"\"\n    return int(math.ceil(value / alignment) * alignment)\n"}
{"type": "source_file", "path": "svg/models/hyvideo/utils/preprocess_text_encoder_tokenizer_utils.py", "content": "import argparse\nimport torch\nfrom transformers import (\n    AutoProcessor,\n    LlavaForConditionalGeneration,\n)\n\n\ndef preprocess_text_encoder_tokenizer(args):\n\n    processor = AutoProcessor.from_pretrained(args.input_dir)\n    model = LlavaForConditionalGeneration.from_pretrained(\n        args.input_dir,\n        torch_dtype=torch.float16,\n        low_cpu_mem_usage=True,\n    ).to(0)\n\n    model.language_model.save_pretrained(\n        f\"{args.output_dir}\"\n    )\n    processor.tokenizer.save_pretrained(\n        f\"{args.output_dir}\"\n    )\n\nif __name__ == \"__main__\":\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--input_dir\",\n        type=str,\n        required=True,\n        help=\"The path to the llava-llama-3-8b-v1_1-transformers.\",\n    )\n    parser.add_argument(\n        \"--output_dir\",\n        type=str,\n        default=\"\",\n        help=\"The output path of the llava-llama-3-8b-text-encoder-tokenizer.\"\n        \"if '', the parent dir of output will be the same as input dir.\",\n    )\n    args = parser.parse_args()\n\n    if len(args.output_dir) == 0:\n        args.output_dir = \"/\".join(args.input_dir.split(\"/\")[:-1])\n\n    preprocess_text_encoder_tokenizer(args)\n"}
{"type": "source_file", "path": "svg/models/hyvideo/modules/norm_layers.py", "content": "import torch\nimport torch.nn as nn\n\n\nclass RMSNorm(nn.Module):\n    def __init__(\n        self,\n        dim: int,\n        elementwise_affine=True,\n        eps: float = 1e-6,\n        device=None,\n        dtype=None,\n    ):\n        \"\"\"\n        Initialize the RMSNorm normalization layer.\n\n        Args:\n            dim (int): The dimension of the input tensor.\n            eps (float, optional): A small value added to the denominator for numerical stability. Default is 1e-6.\n\n        Attributes:\n            eps (float): A small value added to the denominator for numerical stability.\n            weight (nn.Parameter): Learnable scaling parameter.\n\n        \"\"\"\n        factory_kwargs = {\"device\": device, \"dtype\": dtype}\n        super().__init__()\n        self.eps = eps\n        if elementwise_affine:\n            self.weight = nn.Parameter(torch.ones(dim, **factory_kwargs))\n\n    def _norm(self, x):\n        \"\"\"\n        Apply the RMSNorm normalization to the input tensor.\n\n        Args:\n            x (torch.Tensor): The input tensor.\n\n        Returns:\n            torch.Tensor: The normalized tensor.\n\n        \"\"\"\n        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass through the RMSNorm layer.\n\n        Args:\n            x (torch.Tensor): The input tensor.\n\n        Returns:\n            torch.Tensor: The output tensor after applying RMSNorm.\n\n        \"\"\"\n        output = self._norm(x.float()).type_as(x)\n        if hasattr(self, \"weight\"):\n            output = output * self.weight\n        return output\n\n\ndef get_norm_layer(norm_layer):\n    \"\"\"\n    Get the normalization layer.\n\n    Args:\n        norm_layer (str): The type of normalization layer.\n\n    Returns:\n        norm_layer (nn.Module): The normalization layer.\n    \"\"\"\n    if norm_layer == \"layer\":\n        return nn.LayerNorm\n    elif norm_layer == \"rms\":\n        return RMSNorm\n    else:\n        raise NotImplementedError(f\"Norm layer {norm_layer} is not implemented\")\n"}
{"type": "source_file", "path": "svg/models/hyvideo/modules/models.py", "content": "import os\nfrom typing import Any, List, Tuple, Optional, Union, Dict\nfrom einops import rearrange\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom diffusers.models import ModelMixin\nfrom diffusers.configuration_utils import ConfigMixin, register_to_config\n\nfrom .activation_layers import get_activation_layer\nfrom .norm_layers import get_norm_layer\nfrom .embed_layers import TimestepEmbedder, PatchEmbed, TextProjection\nfrom .attenion import attention, parallel_attention, get_cu_seqlens\nfrom .posemb_layers import apply_rotary_emb\nfrom .mlp_layers import MLP, MLPEmbedder, FinalLayer\nfrom .modulate_layers import ModulateDiT, modulate, apply_gate\nfrom .token_refiner import SingleTokenRefiner\n\n\nclass MMDoubleStreamBlock(nn.Module):\n    \"\"\"\n    A multimodal dit block with seperate modulation for\n    text and image/video, see more details (SD3): https://arxiv.org/abs/2403.03206\n                                     (Flux.1): https://github.com/black-forest-labs/flux\n    \"\"\"\n\n    def __init__(\n        self,\n        hidden_size: int,\n        heads_num: int,\n        mlp_width_ratio: float,\n        mlp_act_type: str = \"gelu_tanh\",\n        qk_norm: bool = True,\n        qk_norm_type: str = \"rms\",\n        qkv_bias: bool = False,\n        layer_idx: int = 0,\n        dtype: Optional[torch.dtype] = None,\n        device: Optional[torch.device] = None,\n    ):\n        factory_kwargs = {\"device\": device, \"dtype\": dtype}\n        super().__init__()\n\n        self.layer_idx = layer_idx\n        self.deterministic = False\n        self.heads_num = heads_num\n        head_dim = hidden_size // heads_num\n        mlp_hidden_dim = int(hidden_size * mlp_width_ratio)\n\n        self.img_mod = ModulateDiT(\n            hidden_size,\n            factor=6,\n            act_layer=get_activation_layer(\"silu\"),\n            **factory_kwargs,\n        )\n        self.img_norm1 = nn.LayerNorm(\n            hidden_size, elementwise_affine=False, eps=1e-6, **factory_kwargs\n        )\n\n        self.img_attn_qkv = nn.Linear(\n            hidden_size, hidden_size * 3, bias=qkv_bias, **factory_kwargs\n        )\n        qk_norm_layer = get_norm_layer(qk_norm_type)\n        self.img_attn_q_norm = (\n            qk_norm_layer(head_dim, elementwise_affine=True, eps=1e-6, **factory_kwargs)\n            if qk_norm\n            else nn.Identity()\n        )\n        self.img_attn_k_norm = (\n            qk_norm_layer(head_dim, elementwise_affine=True, eps=1e-6, **factory_kwargs)\n            if qk_norm\n            else nn.Identity()\n        )\n        self.img_attn_proj = nn.Linear(\n            hidden_size, hidden_size, bias=qkv_bias, **factory_kwargs\n        )\n\n        self.img_norm2 = nn.LayerNorm(\n            hidden_size, elementwise_affine=False, eps=1e-6, **factory_kwargs\n        )\n        self.img_mlp = MLP(\n            hidden_size,\n            mlp_hidden_dim,\n            act_layer=get_activation_layer(mlp_act_type),\n            bias=True,\n            **factory_kwargs,\n        )\n\n        self.txt_mod = ModulateDiT(\n            hidden_size,\n            factor=6,\n            act_layer=get_activation_layer(\"silu\"),\n            **factory_kwargs,\n        )\n        self.txt_norm1 = nn.LayerNorm(\n            hidden_size, elementwise_affine=False, eps=1e-6, **factory_kwargs\n        )\n\n        self.txt_attn_qkv = nn.Linear(\n            hidden_size, hidden_size * 3, bias=qkv_bias, **factory_kwargs\n        )\n        self.txt_attn_q_norm = (\n            qk_norm_layer(head_dim, elementwise_affine=True, eps=1e-6, **factory_kwargs)\n            if qk_norm\n            else nn.Identity()\n        )\n        self.txt_attn_k_norm = (\n            qk_norm_layer(head_dim, elementwise_affine=True, eps=1e-6, **factory_kwargs)\n            if qk_norm\n            else nn.Identity()\n        )\n        self.txt_attn_proj = nn.Linear(\n            hidden_size, hidden_size, bias=qkv_bias, **factory_kwargs\n        )\n\n        self.txt_norm2 = nn.LayerNorm(\n            hidden_size, elementwise_affine=False, eps=1e-6, **factory_kwargs\n        )\n        self.txt_mlp = MLP(\n            hidden_size,\n            mlp_hidden_dim,\n            act_layer=get_activation_layer(mlp_act_type),\n            bias=True,\n            **factory_kwargs,\n        )\n        self.hybrid_seq_parallel_attn = None\n\n        # Sparsity\n        self.sparse_args = None\n\n    def enable_deterministic(self):\n        self.deterministic = True\n\n    def disable_deterministic(self):\n        self.deterministic = False\n\n    def forward(\n        self,\n        img: torch.Tensor,\n        txt: torch.Tensor,\n        vec: torch.Tensor,\n        cu_seqlens_q: Optional[torch.Tensor] = None,\n        cu_seqlens_kv: Optional[torch.Tensor] = None,\n        max_seqlen_q: Optional[int] = None,\n        max_seqlen_kv: Optional[int] = None,\n        freqs_cis: tuple = None,\n        timestep: torch.Tensor = None,\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        \n        (\n            img_mod1_shift,\n            img_mod1_scale,\n            img_mod1_gate,\n            img_mod2_shift,\n            img_mod2_scale,\n            img_mod2_gate,\n        ) = self.img_mod(vec).chunk(6, dim=-1)\n        (\n            txt_mod1_shift,\n            txt_mod1_scale,\n            txt_mod1_gate,\n            txt_mod2_shift,\n            txt_mod2_scale,\n            txt_mod2_gate,\n        ) = self.txt_mod(vec).chunk(6, dim=-1)\n\n        # Prepare image for attention.\n        img_modulated = self.img_norm1(img)\n        img_modulated = modulate(\n            img_modulated, shift=img_mod1_shift, scale=img_mod1_scale\n        )\n        img_qkv = self.img_attn_qkv(img_modulated)\n        img_q, img_k, img_v = rearrange(\n            img_qkv, \"B L (K H D) -> K B L H D\", K=3, H=self.heads_num\n        )\n        # Apply QK-Norm if needed\n        img_q = self.img_attn_q_norm(img_q).to(img_v)\n        img_k = self.img_attn_k_norm(img_k).to(img_v)\n\n        # Apply RoPE if needed.\n        if freqs_cis is not None:\n            img_qq, img_kk = apply_rotary_emb(img_q, img_k, freqs_cis, head_first=False)\n            assert (\n                img_qq.shape == img_q.shape and img_kk.shape == img_k.shape\n            ), f\"img_kk: {img_qq.shape}, img_q: {img_q.shape}, img_kk: {img_kk.shape}, img_k: {img_k.shape}\"\n            img_q, img_k = img_qq, img_kk\n\n        # Prepare txt for attention.\n        txt_modulated = self.txt_norm1(txt)\n        txt_modulated = modulate(\n            txt_modulated, shift=txt_mod1_shift, scale=txt_mod1_scale\n        )\n        txt_qkv = self.txt_attn_qkv(txt_modulated)\n        txt_q, txt_k, txt_v = rearrange(\n            txt_qkv, \"B L (K H D) -> K B L H D\", K=3, H=self.heads_num\n        )\n        # Apply QK-Norm if needed.\n        txt_q = self.txt_attn_q_norm(txt_q).to(txt_v)\n        txt_k = self.txt_attn_k_norm(txt_k).to(txt_v)\n\n        # Run actual attention.\n        q = torch.cat((img_q, txt_q), dim=1)\n        k = torch.cat((img_k, txt_k), dim=1)\n        v = torch.cat((img_v, txt_v), dim=1)\n        assert (\n            cu_seqlens_q.shape[0] == 2 * img.shape[0] + 1\n        ), f\"cu_seqlens_q.shape:{cu_seqlens_q.shape}, img.shape[0]:{img.shape[0]}\"\n\n        # attention computation start\n        if not self.hybrid_seq_parallel_attn:\n            attn = attention(\n                q,\n                k,\n                v,\n                mode=\"flash\",\n                cu_seqlens_q=cu_seqlens_q,\n                cu_seqlens_kv=cu_seqlens_kv,\n                max_seqlen_q=max_seqlen_q,\n                max_seqlen_kv=max_seqlen_kv,\n                batch_size=img_k.shape[0],\n                timestep=timestep,\n                layer_idx=self.layer_idx\n            )\n        else:\n            attn = parallel_attention(\n                self.hybrid_seq_parallel_attn,\n                q,\n                k,\n                v,\n                img_q_len=img_q.shape[1],\n                img_kv_len=img_k.shape[1],\n                cu_seqlens_q=cu_seqlens_q,\n                cu_seqlens_kv=cu_seqlens_kv\n            )\n        \n        # attention computation end\n\n        img_attn, txt_attn = attn[:, : img.shape[1]], attn[:, img.shape[1] :]\n\n        # Calculate the img bloks.\n        img = img + apply_gate(self.img_attn_proj(img_attn), gate=img_mod1_gate)\n        img = img + apply_gate(\n            self.img_mlp(\n                modulate(\n                    self.img_norm2(img), shift=img_mod2_shift, scale=img_mod2_scale\n                )\n            ),\n            gate=img_mod2_gate,\n        )\n\n        # Calculate the txt bloks.\n        txt = txt + apply_gate(self.txt_attn_proj(txt_attn), gate=txt_mod1_gate)\n        txt = txt + apply_gate(\n            self.txt_mlp(\n                modulate(\n                    self.txt_norm2(txt), shift=txt_mod2_shift, scale=txt_mod2_scale\n                )\n            ),\n            gate=txt_mod2_gate,\n        )\n\n        return img, txt\n\n\nclass MMSingleStreamBlock(nn.Module):\n    \"\"\"\n    A DiT block with parallel linear layers as described in\n    https://arxiv.org/abs/2302.05442 and adapted modulation interface.\n    Also refer to (SD3): https://arxiv.org/abs/2403.03206\n                  (Flux.1): https://github.com/black-forest-labs/flux\n    \"\"\"\n\n    def __init__(\n        self,\n        hidden_size: int,\n        heads_num: int,\n        mlp_width_ratio: float = 4.0,\n        mlp_act_type: str = \"gelu_tanh\",\n        qk_norm: bool = True,\n        qk_norm_type: str = \"rms\",\n        qk_scale: float = None,\n        layer_idx: int = 0,\n        dtype: Optional[torch.dtype] = None,\n        device: Optional[torch.device] = None,\n    ):\n        factory_kwargs = {\"device\": device, \"dtype\": dtype}\n        super().__init__()\n\n        self.layer_idx = layer_idx\n        self.deterministic = False\n        self.hidden_size = hidden_size\n        self.heads_num = heads_num\n        head_dim = hidden_size // heads_num\n        mlp_hidden_dim = int(hidden_size * mlp_width_ratio)\n        self.mlp_hidden_dim = mlp_hidden_dim\n        self.scale = qk_scale or head_dim ** -0.5\n\n        # qkv and mlp_in\n        self.linear1 = nn.Linear(\n            hidden_size, hidden_size * 3 + mlp_hidden_dim, **factory_kwargs\n        )\n        # proj and mlp_out\n        self.linear2 = nn.Linear(\n            hidden_size + mlp_hidden_dim, hidden_size, **factory_kwargs\n        )\n\n        qk_norm_layer = get_norm_layer(qk_norm_type)\n        self.q_norm = (\n            qk_norm_layer(head_dim, elementwise_affine=True, eps=1e-6, **factory_kwargs)\n            if qk_norm\n            else nn.Identity()\n        )\n        self.k_norm = (\n            qk_norm_layer(head_dim, elementwise_affine=True, eps=1e-6, **factory_kwargs)\n            if qk_norm\n            else nn.Identity()\n        )\n\n        self.pre_norm = nn.LayerNorm(\n            hidden_size, elementwise_affine=False, eps=1e-6, **factory_kwargs\n        )\n\n        self.mlp_act = get_activation_layer(mlp_act_type)()\n        self.modulation = ModulateDiT(\n            hidden_size,\n            factor=3,\n            act_layer=get_activation_layer(\"silu\"),\n            **factory_kwargs,\n        )\n        self.hybrid_seq_parallel_attn = None\n        \n        # Sparsity\n        self.sparse_args = None\n        \n    def enable_deterministic(self):\n        self.deterministic = True\n\n    def disable_deterministic(self):\n        self.deterministic = False\n\n    def forward(\n        self,\n        x: torch.Tensor,\n        vec: torch.Tensor,\n        txt_len: int,\n        cu_seqlens_q: Optional[torch.Tensor] = None,\n        cu_seqlens_kv: Optional[torch.Tensor] = None,\n        max_seqlen_q: Optional[int] = None,\n        max_seqlen_kv: Optional[int] = None,\n        freqs_cis: Tuple[torch.Tensor, torch.Tensor] = None,\n        timestep: torch.Tensor = None,\n    ) -> torch.Tensor:\n        mod_shift, mod_scale, mod_gate = self.modulation(vec).chunk(3, dim=-1)\n        x_mod = modulate(self.pre_norm(x), shift=mod_shift, scale=mod_scale)\n        qkv, mlp = torch.split(\n            self.linear1(x_mod), [3 * self.hidden_size, self.mlp_hidden_dim], dim=-1\n        )\n\n        q, k, v = rearrange(qkv, \"B L (K H D) -> K B L H D\", K=3, H=self.heads_num)\n\n        # Cause Inefficiency\n        q = q.transpose(1, 2).contiguous()\n        k = k.transpose(1, 2).contiguous()\n        \n        # Apply QK-Norm if needed.\n        q = self.q_norm(q).to(v)\n        k = self.k_norm(k).to(v)\n\n        # Apply RoPE if needed.\n        if freqs_cis is not None:\n            img_q, txt_q = q[:, :, :-txt_len, :], q[:, :, -txt_len:, :]\n            img_k, txt_k = k[:, :, :-txt_len, :], k[:, :, -txt_len:, :]\n\n            img_qq, img_kk = apply_rotary_emb(img_q, img_k, freqs_cis, head_first=True)\n            assert (\n                img_qq.shape == img_q.shape and img_kk.shape == img_k.shape\n            ), f\"img_kk: {img_qq.shape}, img_q: {img_q.shape}, img_kk: {img_kk.shape}, img_k: {img_k.shape}\"\n            img_q, img_k = img_qq, img_kk\n\n            q = torch.cat((img_q, txt_q), dim=2)\n            k = torch.cat((img_k, txt_k), dim=2)\n\n            # Cause Inefficiency\n            q = q.transpose(1, 2).contiguous()\n            k = k.transpose(1, 2).contiguous()\n        \n        # Compute attention.\n        assert (\n            cu_seqlens_q.shape[0] == 2 * x.shape[0] + 1\n        ), f\"cu_seqlens_q.shape:{cu_seqlens_q.shape}, x.shape[0]:{x.shape[0]}\"\n\n        # attention computation start\n        if not self.hybrid_seq_parallel_attn:\n            attn = attention(\n                q,\n                k,\n                v,\n                mode=\"flash\",\n                cu_seqlens_q=cu_seqlens_q,\n                cu_seqlens_kv=cu_seqlens_kv,\n                max_seqlen_q=max_seqlen_q,\n                max_seqlen_kv=max_seqlen_kv,\n                batch_size=x.shape[0],\n                timestep=timestep,\n                layer_idx=self.layer_idx\n            )\n        else:\n            attn = parallel_attention(\n                self.hybrid_seq_parallel_attn,\n                q,\n                k,\n                v,\n                img_q_len=img_q.shape[1],\n                img_kv_len=img_k.shape[1],\n                cu_seqlens_q=cu_seqlens_q,\n                cu_seqlens_kv=cu_seqlens_kv\n            )\n        # attention computation end\n\n        # Compute activation in mlp stream, cat again and run second linear layer.\n        output = self.linear2(torch.cat((attn, self.mlp_act(mlp)), 2))\n        return x + apply_gate(output, gate=mod_gate)\n\n\nclass HYVideoDiffusionTransformer(ModelMixin, ConfigMixin):\n    \"\"\"\n    HunyuanVideo Transformer backbone\n\n    Inherited from ModelMixin and ConfigMixin for compatibility with diffusers' sampler StableDiffusionPipeline.\n\n    Reference:\n    [1] Flux.1: https://github.com/black-forest-labs/flux\n    [2] MMDiT: http://arxiv.org/abs/2403.03206\n\n    Parameters\n    ----------\n    args: argparse.Namespace\n        The arguments parsed by argparse.\n    patch_size: list\n        The size of the patch.\n    in_channels: int\n        The number of input channels.\n    out_channels: int\n        The number of output channels.\n    hidden_size: int\n        The hidden size of the transformer backbone.\n    heads_num: int\n        The number of attention heads.\n    mlp_width_ratio: float\n        The ratio of the hidden size of the MLP in the transformer block.\n    mlp_act_type: str\n        The activation function of the MLP in the transformer block.\n    depth_double_blocks: int\n        The number of transformer blocks in the double blocks.\n    depth_single_blocks: int\n        The number of transformer blocks in the single blocks.\n    rope_dim_list: list\n        The dimension of the rotary embedding for t, h, w.\n    qkv_bias: bool\n        Whether to use bias in the qkv linear layer.\n    qk_norm: bool\n        Whether to use qk norm.\n    qk_norm_type: str\n        The type of qk norm.\n    guidance_embed: bool\n        Whether to use guidance embedding for distillation.\n    text_projection: str\n        The type of the text projection, default is single_refiner.\n    use_attention_mask: bool\n        Whether to use attention mask for text encoder.\n    dtype: torch.dtype\n        The dtype of the model.\n    device: torch.device\n        The device of the model.\n    \"\"\"\n\n    @register_to_config\n    def __init__(\n        self,\n        args: Any,\n        patch_size: list = [1, 2, 2],\n        in_channels: int = 4,  # Should be VAE.config.latent_channels.\n        out_channels: int = None,\n        hidden_size: int = 3072,\n        heads_num: int = 24,\n        mlp_width_ratio: float = 4.0,\n        mlp_act_type: str = \"gelu_tanh\",\n        mm_double_blocks_depth: int = 20,\n        mm_single_blocks_depth: int = 40,\n        rope_dim_list: List[int] = [16, 56, 56],\n        qkv_bias: bool = True,\n        qk_norm: bool = True,\n        qk_norm_type: str = \"rms\",\n        guidance_embed: bool = False,  # For modulation.\n        text_projection: str = \"single_refiner\",\n        use_attention_mask: bool = True,\n        dtype: Optional[torch.dtype] = None,\n        device: Optional[torch.device] = None,\n    ):\n        factory_kwargs = {\"device\": device, \"dtype\": dtype}\n        super().__init__()\n\n        self.patch_size = patch_size\n        self.in_channels = in_channels\n        self.out_channels = in_channels if out_channels is None else out_channels\n        self.unpatchify_channels = self.out_channels\n        self.guidance_embed = guidance_embed\n        self.rope_dim_list = rope_dim_list\n\n        # Text projection. Default to linear projection.\n        # Alternative: TokenRefiner. See more details (LI-DiT): http://arxiv.org/abs/2406.11831\n        self.use_attention_mask = use_attention_mask\n        self.text_projection = text_projection\n\n        self.text_states_dim = args.text_states_dim\n        self.text_states_dim_2 = args.text_states_dim_2\n\n        if hidden_size % heads_num != 0:\n            raise ValueError(\n                f\"Hidden size {hidden_size} must be divisible by heads_num {heads_num}\"\n            )\n        pe_dim = hidden_size // heads_num\n        if sum(rope_dim_list) != pe_dim:\n            raise ValueError(\n                f\"Got {rope_dim_list} but expected positional dim {pe_dim}\"\n            )\n        self.hidden_size = hidden_size\n        self.heads_num = heads_num\n\n        # image projection\n        self.img_in = PatchEmbed(\n            self.patch_size, self.in_channels, self.hidden_size, **factory_kwargs\n        )\n\n        # text projection\n        if self.text_projection == \"linear\":\n            self.txt_in = TextProjection(\n                self.text_states_dim,\n                self.hidden_size,\n                get_activation_layer(\"silu\"),\n                **factory_kwargs,\n            )\n        elif self.text_projection == \"single_refiner\":\n            self.txt_in = SingleTokenRefiner(\n                self.text_states_dim, hidden_size, heads_num, depth=2, **factory_kwargs\n            )\n        else:\n            raise NotImplementedError(\n                f\"Unsupported text_projection: {self.text_projection}\"\n            )\n\n        # time modulation\n        self.time_in = TimestepEmbedder(\n            self.hidden_size, get_activation_layer(\"silu\"), **factory_kwargs\n        )\n\n        # text modulation\n        self.vector_in = MLPEmbedder(\n            self.text_states_dim_2, self.hidden_size, **factory_kwargs\n        )\n\n        # guidance modulation\n        self.guidance_in = (\n            TimestepEmbedder(\n                self.hidden_size, get_activation_layer(\"silu\"), **factory_kwargs\n            )\n            if guidance_embed\n            else None\n        )\n\n        # double blocks\n        self.double_blocks = nn.ModuleList(\n            [\n                MMDoubleStreamBlock(\n                    self.hidden_size,\n                    self.heads_num,\n                    mlp_width_ratio=mlp_width_ratio,\n                    mlp_act_type=mlp_act_type,\n                    qk_norm=qk_norm,\n                    qk_norm_type=qk_norm_type,\n                    qkv_bias=qkv_bias,\n                    layer_idx=layer_idx,\n                    **factory_kwargs,\n                )\n                for layer_idx in range(mm_double_blocks_depth)\n            ]\n        )\n\n        # single blocks\n        self.single_blocks = nn.ModuleList(\n            [\n                MMSingleStreamBlock(\n                    self.hidden_size,\n                    self.heads_num,\n                    mlp_width_ratio=mlp_width_ratio,\n                    mlp_act_type=mlp_act_type,\n                    qk_norm=qk_norm,\n                    qk_norm_type=qk_norm_type,\n                    layer_idx=layer_idx,\n                    **factory_kwargs,\n                )\n                for layer_idx in range(mm_single_blocks_depth)\n            ]\n        )\n\n        self.final_layer = FinalLayer(\n            self.hidden_size,\n            self.patch_size,\n            self.out_channels,\n            get_activation_layer(\"silu\"),\n            **factory_kwargs,\n        )\n\n    def enable_deterministic(self):\n        for block in self.double_blocks:\n            block.enable_deterministic()\n        for block in self.single_blocks:\n            block.enable_deterministic()\n\n    def disable_deterministic(self):\n        for block in self.double_blocks:\n            block.disable_deterministic()\n        for block in self.single_blocks:\n            block.disable_deterministic()\n\n    def forward(\n        self,\n        x: torch.Tensor,\n        t: torch.Tensor,  # Should be in range(0, 1000).\n        text_states: torch.Tensor = None,\n        text_mask: torch.Tensor = None,  # Now we don't use it.\n        text_states_2: Optional[torch.Tensor] = None,  # Text embedding for modulation.\n        freqs_cos: Optional[torch.Tensor] = None,\n        freqs_sin: Optional[torch.Tensor] = None,\n        guidance: torch.Tensor = None,  # Guidance for modulation, should be cfg_scale x 1000.\n        return_dict: bool = True,\n    ) -> Union[torch.Tensor, Dict[str, torch.Tensor]]:\n        out = {}\n        img = x\n        txt = text_states\n        _, _, ot, oh, ow = x.shape\n        tt, th, tw = (\n            ot // self.patch_size[0],\n            oh // self.patch_size[1],\n            ow // self.patch_size[2],\n        )\n\n        # Prepare modulation vectors.\n        vec = self.time_in(t)\n\n        # text modulation\n        vec = vec + self.vector_in(text_states_2)\n\n        # guidance modulation\n        if self.guidance_embed:\n            if guidance is None:\n                raise ValueError(\n                    \"Didn't get guidance strength for guidance distilled model.\"\n                )\n\n            # our timestep_embedding is merged into guidance_in(TimestepEmbedder)\n            vec = vec + self.guidance_in(guidance)\n\n        # Embed image and text.\n        img = self.img_in(img)\n        if self.text_projection == \"linear\":\n            txt = self.txt_in(txt)\n        elif self.text_projection == \"single_refiner\":\n            txt = self.txt_in(txt, t, text_mask if self.use_attention_mask else None)\n        else:\n            raise NotImplementedError(\n                f\"Unsupported text_projection: {self.text_projection}\"\n            )\n\n        txt_seq_len = txt.shape[1]\n        img_seq_len = img.shape[1]\n\n        # Compute cu_squlens and max_seqlen for flash attention\n        cu_seqlens_q = get_cu_seqlens(text_mask, img_seq_len)\n        cu_seqlens_kv = cu_seqlens_q\n        max_seqlen_q = img_seq_len + txt_seq_len\n        max_seqlen_kv = max_seqlen_q\n\n        if hasattr(self, 'sparse_args'):\n            if getattr(self.sparse_args, 'pattern', None) == \"SVG\":\n                freqs_cos = freqs_cos.to(x.device).to(torch.float32)\n                freqs_sin = freqs_sin.to(x.device).to(torch.float32)\n        freqs_cis = (freqs_cos, freqs_sin) if freqs_cos is not None else None\n            \n        # --------------------- Pass through DiT blocks ------------------------\n        for _, block in enumerate(self.double_blocks):\n            double_block_args = [\n                img,\n                txt,\n                vec,\n                cu_seqlens_q,\n                cu_seqlens_kv,\n                max_seqlen_q,\n                max_seqlen_kv,\n                freqs_cis,\n                t\n            ]\n\n            img, txt = block(*double_block_args)\n\n        # Merge txt and img to pass through single stream blocks.\n        x = torch.cat((img, txt), 1)\n        if len(self.single_blocks) > 0:\n            for _, block in enumerate(self.single_blocks):\n                single_block_args = [\n                    x,\n                    vec,\n                    txt_seq_len,\n                    cu_seqlens_q,\n                    cu_seqlens_kv,\n                    max_seqlen_q,\n                    max_seqlen_kv,\n                    (freqs_cos, freqs_sin),\n                    t\n                ]\n\n                x = block(*single_block_args)\n\n        img = x[:, :img_seq_len, ...]\n\n        # ---------------------------- Final layer ------------------------------\n        img = self.final_layer(img, vec)  # (N, T, patch_size ** 2 * out_channels)\n\n        img = self.unpatchify(img, tt, th, tw)\n        if return_dict:\n            out[\"x\"] = img\n            return out\n        return img\n\n    def unpatchify(self, x, t, h, w):\n        \"\"\"\n        x: (N, T, patch_size**2 * C)\n        imgs: (N, H, W, C)\n        \"\"\"\n        c = self.unpatchify_channels\n        pt, ph, pw = self.patch_size\n        assert t * h * w == x.shape[1]\n\n        x = x.reshape(shape=(x.shape[0], t, h, w, c, pt, ph, pw))\n        x = torch.einsum(\"nthwcopq->nctohpwq\", x)\n        imgs = x.reshape(shape=(x.shape[0], c, t * pt, h * ph, w * pw))\n\n        return imgs\n\n    def params_count(self):\n        counts = {\n            \"double\": sum(\n                [\n                    sum(p.numel() for p in block.img_attn_qkv.parameters())\n                    + sum(p.numel() for p in block.img_attn_proj.parameters())\n                    + sum(p.numel() for p in block.img_mlp.parameters())\n                    + sum(p.numel() for p in block.txt_attn_qkv.parameters())\n                    + sum(p.numel() for p in block.txt_attn_proj.parameters())\n                    + sum(p.numel() for p in block.txt_mlp.parameters())\n                    for block in self.double_blocks\n                ]\n            ),\n            \"single\": sum(\n                [\n                    sum(p.numel() for p in block.linear1.parameters())\n                    + sum(p.numel() for p in block.linear2.parameters())\n                    for block in self.single_blocks\n                ]\n            ),\n            \"total\": sum(p.numel() for p in self.parameters()),\n        }\n        counts[\"attn+mlp\"] = counts[\"double\"] + counts[\"single\"]\n        return counts\n\n\n#################################################################################\n#                             HunyuanVideo Configs                              #\n#################################################################################\n\nHUNYUAN_VIDEO_CONFIG = {\n    \"HYVideo-T/2\": {\n        \"mm_double_blocks_depth\": 20,\n        \"mm_single_blocks_depth\": 40,\n        \"rope_dim_list\": [16, 56, 56],\n        \"hidden_size\": 3072,\n        \"heads_num\": 24,\n        \"mlp_width_ratio\": 4,\n    },\n    \"HYVideo-T/2-cfgdistill\": {\n        \"mm_double_blocks_depth\": 20,\n        \"mm_single_blocks_depth\": 40,\n        \"rope_dim_list\": [16, 56, 56],\n        \"hidden_size\": 3072,\n        \"heads_num\": 24,\n        \"mlp_width_ratio\": 4,\n        \"guidance_embed\": True,\n    },\n}\n"}
{"type": "source_file", "path": "svg/models/hyvideo/modules/utils.py", "content": "\"\"\"Mask Mod for Image2Video\"\"\"\n\nfrom math import floor\nimport torch\nfrom torch import Tensor\n\n\nfrom functools import lru_cache\nfrom typing import Optional, List\n\nimport torch\nfrom torch.nn.attention.flex_attention import (\n    create_block_mask,\n)\n\n\n@lru_cache\ndef create_block_mask_cached(score_mod, B, H, M, N, device=\"cuda\", _compile=False):\n    block_mask = create_block_mask(score_mod, B, H, M, N, device=device, _compile=_compile)\n    return block_mask\n\ndef generate_temporal_head_mask_mod(context_length: int = 226, prompt_length: int = 226, num_frames: int = 13, token_per_frame: int = 1350, mul: int = 2):\n    \n    def round_to_multiple(idx):\n        return floor(idx / 128) * 128\n        \n    real_length = num_frames * token_per_frame + prompt_length\n    def temporal_mask_mod(b, h, q_idx, kv_idx):\n        real_mask = (kv_idx < real_length) & (q_idx < real_length)\n        fake_mask = (kv_idx >= real_length) & (q_idx >= real_length)\n        \n        two_frame = round_to_multiple(mul * token_per_frame)\n        temporal_head_mask = (torch.abs(q_idx - kv_idx) < two_frame)\n\n        text_column_mask = (num_frames * token_per_frame <= kv_idx) & (kv_idx < real_length)\n        text_row_mask = (num_frames * token_per_frame <= q_idx) & (q_idx < real_length)\n\n        video_mask = temporal_head_mask | text_column_mask | text_row_mask\n        real_mask = real_mask & video_mask\n        \n        return real_mask | fake_mask\n    \n    return temporal_mask_mod\n"}
{"type": "source_file", "path": "svg/models/hyvideo/utils/helpers.py", "content": "import collections.abc\n\nfrom itertools import repeat\n\n\ndef _ntuple(n):\n    def parse(x):\n        if isinstance(x, collections.abc.Iterable) and not isinstance(x, str):\n            x = tuple(x)\n            if len(x) == 1:\n                x = tuple(repeat(x[0], n))\n            return x\n        return tuple(repeat(x, n))\n    return parse\n\n\nto_1tuple = _ntuple(1)\nto_2tuple = _ntuple(2)\nto_3tuple = _ntuple(3)\nto_4tuple = _ntuple(4)\n\n\ndef as_tuple(x):\n    if isinstance(x, collections.abc.Iterable) and not isinstance(x, str):\n        return tuple(x)\n    if x is None or isinstance(x, (int, float, str)):\n        return (x,)\n    else:\n        raise ValueError(f\"Unknown type {type(x)}\")\n\n\ndef as_list_of_2tuple(x):\n    x = as_tuple(x)\n    if len(x) == 1:\n        x = (x[0], x[0])\n    assert len(x) % 2 == 0, f\"Expect even length, got {len(x)}.\"\n    lst = []\n    for i in range(0, len(x), 2):\n        lst.append((x[i], x[i + 1]))\n    return lst\n"}
{"type": "source_file", "path": "svg/models/hyvideo/modules/posemb_layers.py", "content": "import torch\nfrom typing import Union, Tuple, List\n\n\ndef _to_tuple(x, dim=2):\n    if isinstance(x, int):\n        return (x,) * dim\n    elif len(x) == dim:\n        return x\n    else:\n        raise ValueError(f\"Expected length {dim} or int, but got {x}\")\n\n\ndef get_meshgrid_nd(start, *args, dim=2):\n    \"\"\"\n    Get n-D meshgrid with start, stop and num.\n\n    Args:\n        start (int or tuple): If len(args) == 0, start is num; If len(args) == 1, start is start, args[0] is stop,\n            step is 1; If len(args) == 2, start is start, args[0] is stop, args[1] is num. For n-dim, start/stop/num\n            should be int or n-tuple. If n-tuple is provided, the meshgrid will be stacked following the dim order in\n            n-tuples.\n        *args: See above.\n        dim (int): Dimension of the meshgrid. Defaults to 2.\n\n    Returns:\n        grid (np.ndarray): [dim, ...]\n    \"\"\"\n    if len(args) == 0:\n        # start is grid_size\n        num = _to_tuple(start, dim=dim)\n        start = (0,) * dim\n        stop = num\n    elif len(args) == 1:\n        # start is start, args[0] is stop, step is 1\n        start = _to_tuple(start, dim=dim)\n        stop = _to_tuple(args[0], dim=dim)\n        num = [stop[i] - start[i] for i in range(dim)]\n    elif len(args) == 2:\n        # start is start, args[0] is stop, args[1] is num\n        start = _to_tuple(start, dim=dim)  # Left-Top       eg: 12,0\n        stop = _to_tuple(args[0], dim=dim)  # Right-Bottom   eg: 20,32\n        num = _to_tuple(args[1], dim=dim)  # Target Size    eg: 32,124\n    else:\n        raise ValueError(f\"len(args) should be 0, 1 or 2, but got {len(args)}\")\n\n    # PyTorch implement of np.linspace(start[i], stop[i], num[i], endpoint=False)\n    axis_grid = []\n    for i in range(dim):\n        a, b, n = start[i], stop[i], num[i]\n        g = torch.linspace(a, b, n + 1, dtype=torch.float32)[:n]\n        axis_grid.append(g)\n    grid = torch.meshgrid(*axis_grid, indexing=\"ij\")  # dim x [W, H, D]\n    grid = torch.stack(grid, dim=0)  # [dim, W, H, D]\n\n    return grid\n\n\n#################################################################################\n#                   Rotary Positional Embedding Functions                       #\n#################################################################################\n# https://github.com/meta-llama/llama/blob/be327c427cc5e89cc1d3ab3d3fec4484df771245/llama/model.py#L80\n\n\ndef reshape_for_broadcast(\n    freqs_cis: Union[torch.Tensor, Tuple[torch.Tensor]],\n    x: torch.Tensor,\n    head_first=False,\n):\n    \"\"\"\n    Reshape frequency tensor for broadcasting it with another tensor.\n\n    This function reshapes the frequency tensor to have the same shape as the target tensor 'x'\n    for the purpose of broadcasting the frequency tensor during element-wise operations.\n\n    Notes:\n        When using FlashMHAModified, head_first should be False.\n        When using Attention, head_first should be True.\n\n    Args:\n        freqs_cis (Union[torch.Tensor, Tuple[torch.Tensor]]): Frequency tensor to be reshaped.\n        x (torch.Tensor): Target tensor for broadcasting compatibility.\n        head_first (bool): head dimension first (except batch dim) or not.\n\n    Returns:\n        torch.Tensor: Reshaped frequency tensor.\n\n    Raises:\n        AssertionError: If the frequency tensor doesn't match the expected shape.\n        AssertionError: If the target tensor 'x' doesn't have the expected number of dimensions.\n    \"\"\"\n    ndim = x.ndim\n    assert 0 <= 1 < ndim\n\n    if isinstance(freqs_cis, tuple):\n        # freqs_cis: (cos, sin) in real space\n        if head_first:\n            assert freqs_cis[0].shape == (\n                x.shape[-2],\n                x.shape[-1],\n            ), f\"freqs_cis shape {freqs_cis[0].shape} does not match x shape {x.shape}\"\n            shape = [\n                d if i == ndim - 2 or i == ndim - 1 else 1\n                for i, d in enumerate(x.shape)\n            ]\n        else:\n            assert freqs_cis[0].shape == (\n                x.shape[1],\n                x.shape[-1],\n            ), f\"freqs_cis shape {freqs_cis[0].shape} does not match x shape {x.shape}\"\n            shape = [d if i == 1 or i == ndim - 1 else 1 for i, d in enumerate(x.shape)]\n        return freqs_cis[0].view(*shape), freqs_cis[1].view(*shape)\n    else:\n        # freqs_cis: values in complex space\n        if head_first:\n            assert freqs_cis.shape == (\n                x.shape[-2],\n                x.shape[-1],\n            ), f\"freqs_cis shape {freqs_cis.shape} does not match x shape {x.shape}\"\n            shape = [\n                d if i == ndim - 2 or i == ndim - 1 else 1\n                for i, d in enumerate(x.shape)\n            ]\n        else:\n            assert freqs_cis.shape == (\n                x.shape[1],\n                x.shape[-1],\n            ), f\"freqs_cis shape {freqs_cis.shape} does not match x shape {x.shape}\"\n            shape = [d if i == 1 or i == ndim - 1 else 1 for i, d in enumerate(x.shape)]\n        return freqs_cis.view(*shape)\n\n\ndef rotate_half(x):\n    x_real, x_imag = (\n        x.float().reshape(*x.shape[:-1], -1, 2).unbind(-1)\n    )  # [B, S, H, D//2]\n    return torch.stack([-x_imag, x_real], dim=-1).flatten(3)\n\n\ndef apply_rotary_emb(\n    xq: torch.Tensor,\n    xk: torch.Tensor,\n    freqs_cis: Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]],\n    head_first: bool = False,\n) -> Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Apply rotary embeddings to input tensors using the given frequency tensor.\n\n    This function applies rotary embeddings to the given query 'xq' and key 'xk' tensors using the provided\n    frequency tensor 'freqs_cis'. The input tensors are reshaped as complex numbers, and the frequency tensor\n    is reshaped for broadcasting compatibility. The resulting tensors contain rotary embeddings and are\n    returned as real tensors.\n\n    Args:\n        xq (torch.Tensor): Query tensor to apply rotary embeddings. [B, S, H, D]\n        xk (torch.Tensor): Key tensor to apply rotary embeddings.   [B, S, H, D]\n        freqs_cis (torch.Tensor or tuple): Precomputed frequency tensor for complex exponential.\n        head_first (bool): head dimension first (except batch dim) or not.\n\n    Returns:\n        Tuple[torch.Tensor, torch.Tensor]: Tuple of modified query tensor and key tensor with rotary embeddings.\n\n    \"\"\"\n    xk_out = None\n    if isinstance(freqs_cis, tuple):\n        cos, sin = reshape_for_broadcast(freqs_cis, xq, head_first)  # [S, D]\n        cos, sin = cos.to(xq.device), sin.to(xq.device)\n        # real * cos - imag * sin\n        # imag * cos + real * sin\n        xq_out = (xq.float() * cos + rotate_half(xq.float()) * sin).type_as(xq)\n        xk_out = (xk.float() * cos + rotate_half(xk.float()) * sin).type_as(xk)\n    else:\n        # view_as_complex will pack [..., D/2, 2](real) to [..., D/2](complex)\n        xq_ = torch.view_as_complex(\n            xq.float().reshape(*xq.shape[:-1], -1, 2)\n        )  # [B, S, H, D//2]\n        freqs_cis = reshape_for_broadcast(freqs_cis, xq_, head_first).to(\n            xq.device\n        )  # [S, D//2] --> [1, S, 1, D//2]\n        # (real, imag) * (cos, sin) = (real * cos - imag * sin, imag * cos + real * sin)\n        # view_as_real will expand [..., D/2](complex) to [..., D/2, 2](real)\n        xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(3).type_as(xq)\n        xk_ = torch.view_as_complex(\n            xk.float().reshape(*xk.shape[:-1], -1, 2)\n        )  # [B, S, H, D//2]\n        xk_out = torch.view_as_real(xk_ * freqs_cis).flatten(3).type_as(xk)\n\n    return xq_out, xk_out\n\n\ndef get_nd_rotary_pos_embed(\n    rope_dim_list,\n    start,\n    *args,\n    theta=10000.0,\n    use_real=False,\n    theta_rescale_factor: Union[float, List[float]] = 1.0,\n    interpolation_factor: Union[float, List[float]] = 1.0,\n):\n    \"\"\"\n    This is a n-d version of precompute_freqs_cis, which is a RoPE for tokens with n-d structure.\n\n    Args:\n        rope_dim_list (list of int): Dimension of each rope. len(rope_dim_list) should equal to n.\n            sum(rope_dim_list) should equal to head_dim of attention layer.\n        start (int | tuple of int | list of int): If len(args) == 0, start is num; If len(args) == 1, start is start,\n            args[0] is stop, step is 1; If len(args) == 2, start is start, args[0] is stop, args[1] is num.\n        *args: See above.\n        theta (float): Scaling factor for frequency computation. Defaults to 10000.0.\n        use_real (bool): If True, return real part and imaginary part separately. Otherwise, return complex numbers.\n            Some libraries such as TensorRT does not support complex64 data type. So it is useful to provide a real\n            part and an imaginary part separately.\n        theta_rescale_factor (float): Rescale factor for theta. Defaults to 1.0.\n\n    Returns:\n        pos_embed (torch.Tensor): [HW, D/2]\n    \"\"\"\n\n    grid = get_meshgrid_nd(\n        start, *args, dim=len(rope_dim_list)\n    )  # [3, W, H, D] / [2, W, H]\n\n    if isinstance(theta_rescale_factor, int) or isinstance(theta_rescale_factor, float):\n        theta_rescale_factor = [theta_rescale_factor] * len(rope_dim_list)\n    elif isinstance(theta_rescale_factor, list) and len(theta_rescale_factor) == 1:\n        theta_rescale_factor = [theta_rescale_factor[0]] * len(rope_dim_list)\n    assert len(theta_rescale_factor) == len(\n        rope_dim_list\n    ), \"len(theta_rescale_factor) should equal to len(rope_dim_list)\"\n\n    if isinstance(interpolation_factor, int) or isinstance(interpolation_factor, float):\n        interpolation_factor = [interpolation_factor] * len(rope_dim_list)\n    elif isinstance(interpolation_factor, list) and len(interpolation_factor) == 1:\n        interpolation_factor = [interpolation_factor[0]] * len(rope_dim_list)\n    assert len(interpolation_factor) == len(\n        rope_dim_list\n    ), \"len(interpolation_factor) should equal to len(rope_dim_list)\"\n\n    # use 1/ndim of dimensions to encode grid_axis\n    embs = []\n    for i in range(len(rope_dim_list)):\n        emb = get_1d_rotary_pos_embed(\n            rope_dim_list[i],\n            grid[i].reshape(-1),\n            theta,\n            use_real=use_real,\n            theta_rescale_factor=theta_rescale_factor[i],\n            interpolation_factor=interpolation_factor[i],\n        )  # 2 x [WHD, rope_dim_list[i]]\n        embs.append(emb)\n\n    if use_real:\n        cos = torch.cat([emb[0] for emb in embs], dim=1)  # (WHD, D/2)\n        sin = torch.cat([emb[1] for emb in embs], dim=1)  # (WHD, D/2)\n        return cos, sin\n    else:\n        emb = torch.cat(embs, dim=1)  # (WHD, D/2)\n        return emb\n\n\ndef get_1d_rotary_pos_embed(\n    dim: int,\n    pos: Union[torch.FloatTensor, int],\n    theta: float = 10000.0,\n    use_real: bool = False,\n    theta_rescale_factor: float = 1.0,\n    interpolation_factor: float = 1.0,\n) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n    \"\"\"\n    Precompute the frequency tensor for complex exponential (cis) with given dimensions.\n    (Note: `cis` means `cos + i * sin`, where i is the imaginary unit.)\n\n    This function calculates a frequency tensor with complex exponential using the given dimension 'dim'\n    and the end index 'end'. The 'theta' parameter scales the frequencies.\n    The returned tensor contains complex values in complex64 data type.\n\n    Args:\n        dim (int): Dimension of the frequency tensor.\n        pos (int or torch.FloatTensor): Position indices for the frequency tensor. [S] or scalar\n        theta (float, optional): Scaling factor for frequency computation. Defaults to 10000.0.\n        use_real (bool, optional): If True, return real part and imaginary part separately.\n                                   Otherwise, return complex numbers.\n        theta_rescale_factor (float, optional): Rescale factor for theta. Defaults to 1.0.\n\n    Returns:\n        freqs_cis: Precomputed frequency tensor with complex exponential. [S, D/2]\n        freqs_cos, freqs_sin: Precomputed frequency tensor with real and imaginary parts separately. [S, D]\n    \"\"\"\n    if isinstance(pos, int):\n        pos = torch.arange(pos).float()\n\n    # proposed by reddit user bloc97, to rescale rotary embeddings to longer sequence length without fine-tuning\n    # has some connection to NTK literature\n    if theta_rescale_factor != 1.0:\n        theta *= theta_rescale_factor ** (dim / (dim - 2))\n\n    freqs = 1.0 / (\n        theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim)\n    )  # [D/2]\n    # assert interpolation_factor == 1.0, f\"interpolation_factor: {interpolation_factor}\"\n    freqs = torch.outer(pos * interpolation_factor, freqs)  # [S, D/2]\n    if use_real:\n        freqs_cos = freqs.cos().repeat_interleave(2, dim=1)  # [S, D]\n        freqs_sin = freqs.sin().repeat_interleave(2, dim=1)  # [S, D]\n        return freqs_cos, freqs_sin\n    else:\n        freqs_cis = torch.polar(\n            torch.ones_like(freqs), freqs\n        )  # complex64     # [S, D/2]\n        return freqs_cis\n"}
{"type": "source_file", "path": "svg/models/hyvideo/modules/modulate_layers.py", "content": "from typing import Callable\n\nimport torch\nimport torch.nn as nn\n\n\nclass ModulateDiT(nn.Module):\n    \"\"\"Modulation layer for DiT.\"\"\"\n    def __init__(\n        self,\n        hidden_size: int,\n        factor: int,\n        act_layer: Callable,\n        dtype=None,\n        device=None,\n    ):\n        factory_kwargs = {\"dtype\": dtype, \"device\": device}\n        super().__init__()\n        self.act = act_layer()\n        self.linear = nn.Linear(\n            hidden_size, factor * hidden_size, bias=True, **factory_kwargs\n        )\n        # Zero-initialize the modulation\n        nn.init.zeros_(self.linear.weight)\n        nn.init.zeros_(self.linear.bias)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        return self.linear(self.act(x))\n\n\ndef modulate(x, shift=None, scale=None):\n    \"\"\"modulate by shift and scale\n\n    Args:\n        x (torch.Tensor): input tensor.\n        shift (torch.Tensor, optional): shift tensor. Defaults to None.\n        scale (torch.Tensor, optional): scale tensor. Defaults to None.\n\n    Returns:\n        torch.Tensor: the output tensor after modulate.\n    \"\"\"\n    if scale is None and shift is None:\n        return x\n    elif shift is None:\n        return x * (1 + scale.unsqueeze(1))\n    elif scale is None:\n        return x + shift.unsqueeze(1)\n    else:\n        return x * (1 + scale.unsqueeze(1)) + shift.unsqueeze(1)\n\n\ndef apply_gate(x, gate=None, tanh=False):\n    \"\"\"AI is creating summary for apply_gate\n\n    Args:\n        x (torch.Tensor): input tensor.\n        gate (torch.Tensor, optional): gate tensor. Defaults to None.\n        tanh (bool, optional): whether to use tanh function. Defaults to False.\n\n    Returns:\n        torch.Tensor: the output tensor after apply gate.\n    \"\"\"\n    if gate is None:\n        return x\n    if tanh:\n        return x * gate.unsqueeze(1).tanh()\n    else:\n        return x * gate.unsqueeze(1)\n\n\ndef ckpt_wrapper(module):\n    def ckpt_forward(*inputs):\n        outputs = module(*inputs)\n        return outputs\n\n    return ckpt_forward\n"}
{"type": "source_file", "path": "svg/models/hyvideo/vae/unet_causal_3d_blocks.py", "content": "# Copyright 2024 The HuggingFace Team. All rights reserved.\r\n#\r\n# Licensed under the Apache License, Version 2.0 (the \"License\");\r\n# you may not use this file except in compliance with the License.\r\n# You may obtain a copy of the License at\r\n#\r\n#     http://www.apache.org/licenses/LICENSE-2.0\r\n#\r\n# Unless required by applicable law or agreed to in writing, software\r\n# distributed under the License is distributed on an \"AS IS\" BASIS,\r\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n# See the License for the specific language governing permissions and\r\n# limitations under the License.\r\n# ==============================================================================\r\n#\r\n# Modified from diffusers==0.29.2\r\n#\r\n# ==============================================================================\r\n\r\nfrom typing import Optional, Tuple, Union\r\n\r\nimport torch\r\nimport torch.nn.functional as F\r\nfrom torch import nn\r\nfrom einops import rearrange\r\n\r\nfrom diffusers.utils import logging\r\nfrom diffusers.models.activations import get_activation\r\nfrom diffusers.models.attention_processor import SpatialNorm\r\nfrom diffusers.models.attention_processor import Attention\r\nfrom diffusers.models.normalization import AdaGroupNorm\r\nfrom diffusers.models.normalization import RMSNorm\r\n\r\nlogger = logging.get_logger(__name__)  # pylint: disable=invalid-name\r\n\r\n\r\ndef prepare_causal_attention_mask(n_frame: int, n_hw: int, dtype, device, batch_size: int = None):\r\n    seq_len = n_frame * n_hw\r\n    mask = torch.full((seq_len, seq_len), float(\"-inf\"), dtype=dtype, device=device)\r\n    for i in range(seq_len):\r\n        i_frame = i // n_hw\r\n        mask[i, : (i_frame + 1) * n_hw] = 0\r\n    if batch_size is not None:\r\n        mask = mask.unsqueeze(0).expand(batch_size, -1, -1)\r\n    return mask\r\n\r\n\r\nclass CausalConv3d(nn.Module):\r\n    \"\"\"\r\n    Implements a causal 3D convolution layer where each position only depends on previous timesteps and current spatial locations.\r\n    This maintains temporal causality in video generation tasks.\r\n    \"\"\"\r\n\r\n    def __init__(\r\n        self,\r\n        chan_in,\r\n        chan_out,\r\n        kernel_size: Union[int, Tuple[int, int, int]],\r\n        stride: Union[int, Tuple[int, int, int]] = 1,\r\n        dilation: Union[int, Tuple[int, int, int]] = 1,\r\n        pad_mode='replicate',\r\n        **kwargs\r\n    ):\r\n        super().__init__()\r\n\r\n        self.pad_mode = pad_mode\r\n        padding = (kernel_size // 2, kernel_size // 2, kernel_size // 2, kernel_size // 2, kernel_size - 1, 0)  # W, H, T\r\n        self.time_causal_padding = padding\r\n\r\n        self.conv = nn.Conv3d(chan_in, chan_out, kernel_size, stride=stride, dilation=dilation, **kwargs)\r\n\r\n    def forward(self, x):\r\n        x = F.pad(x, self.time_causal_padding, mode=self.pad_mode)\r\n        return self.conv(x)\r\n\r\n\r\nclass UpsampleCausal3D(nn.Module):\r\n    \"\"\"\r\n    A 3D upsampling layer with an optional convolution.\r\n    \"\"\"\r\n\r\n    def __init__(\r\n        self,\r\n        channels: int,\r\n        use_conv: bool = False,\r\n        use_conv_transpose: bool = False,\r\n        out_channels: Optional[int] = None,\r\n        name: str = \"conv\",\r\n        kernel_size: Optional[int] = None,\r\n        padding=1,\r\n        norm_type=None,\r\n        eps=None,\r\n        elementwise_affine=None,\r\n        bias=True,\r\n        interpolate=True,\r\n        upsample_factor=(2, 2, 2),\r\n    ):\r\n        super().__init__()\r\n        self.channels = channels\r\n        self.out_channels = out_channels or channels\r\n        self.use_conv = use_conv\r\n        self.use_conv_transpose = use_conv_transpose\r\n        self.name = name\r\n        self.interpolate = interpolate\r\n        self.upsample_factor = upsample_factor\r\n\r\n        if norm_type == \"ln_norm\":\r\n            self.norm = nn.LayerNorm(channels, eps, elementwise_affine)\r\n        elif norm_type == \"rms_norm\":\r\n            self.norm = RMSNorm(channels, eps, elementwise_affine)\r\n        elif norm_type is None:\r\n            self.norm = None\r\n        else:\r\n            raise ValueError(f\"unknown norm_type: {norm_type}\")\r\n\r\n        conv = None\r\n        if use_conv_transpose:\r\n            raise NotImplementedError\r\n        elif use_conv:\r\n            if kernel_size is None:\r\n                kernel_size = 3\r\n            conv = CausalConv3d(self.channels, self.out_channels, kernel_size=kernel_size, bias=bias)\r\n\r\n        if name == \"conv\":\r\n            self.conv = conv\r\n        else:\r\n            self.Conv2d_0 = conv\r\n\r\n    def forward(\r\n        self,\r\n        hidden_states: torch.FloatTensor,\r\n        output_size: Optional[int] = None,\r\n        scale: float = 1.0,\r\n    ) -> torch.FloatTensor:\r\n        assert hidden_states.shape[1] == self.channels\r\n\r\n        if self.norm is not None:\r\n            raise NotImplementedError\r\n\r\n        if self.use_conv_transpose:\r\n            return self.conv(hidden_states)\r\n\r\n        # Cast to float32 to as 'upsample_nearest2d_out_frame' op does not support bfloat16\r\n        dtype = hidden_states.dtype\r\n        if dtype == torch.bfloat16:\r\n            hidden_states = hidden_states.to(torch.float32)\r\n\r\n        # upsample_nearest_nhwc fails with large batch sizes. see https://github.com/huggingface/diffusers/issues/984\r\n        if hidden_states.shape[0] >= 64:\r\n            hidden_states = hidden_states.contiguous()\r\n\r\n        # if `output_size` is passed we force the interpolation output\r\n        # size and do not make use of `scale_factor=2`\r\n        if self.interpolate:\r\n            B, C, T, H, W = hidden_states.shape\r\n            first_h, other_h = hidden_states.split((1, T - 1), dim=2)\r\n            if output_size is None:\r\n                if T > 1:\r\n                    other_h = F.interpolate(other_h, scale_factor=self.upsample_factor, mode=\"nearest\")\r\n\r\n                first_h = first_h.squeeze(2)\r\n                first_h = F.interpolate(first_h, scale_factor=self.upsample_factor[1:], mode=\"nearest\")\r\n                first_h = first_h.unsqueeze(2)\r\n            else:\r\n                raise NotImplementedError\r\n\r\n            if T > 1:\r\n                hidden_states = torch.cat((first_h, other_h), dim=2)\r\n            else:\r\n                hidden_states = first_h\r\n\r\n        # If the input is bfloat16, we cast back to bfloat16\r\n        if dtype == torch.bfloat16:\r\n            hidden_states = hidden_states.to(dtype)\r\n\r\n        if self.use_conv:\r\n            if self.name == \"conv\":\r\n                hidden_states = self.conv(hidden_states)\r\n            else:\r\n                hidden_states = self.Conv2d_0(hidden_states)\r\n\r\n        return hidden_states\r\n\r\n\r\nclass DownsampleCausal3D(nn.Module):\r\n    \"\"\"\r\n    A 3D downsampling layer with an optional convolution.\r\n    \"\"\"\r\n\r\n    def __init__(\r\n        self,\r\n        channels: int,\r\n        use_conv: bool = False,\r\n        out_channels: Optional[int] = None,\r\n        padding: int = 1,\r\n        name: str = \"conv\",\r\n        kernel_size=3,\r\n        norm_type=None,\r\n        eps=None,\r\n        elementwise_affine=None,\r\n        bias=True,\r\n        stride=2,\r\n    ):\r\n        super().__init__()\r\n        self.channels = channels\r\n        self.out_channels = out_channels or channels\r\n        self.use_conv = use_conv\r\n        self.padding = padding\r\n        stride = stride\r\n        self.name = name\r\n\r\n        if norm_type == \"ln_norm\":\r\n            self.norm = nn.LayerNorm(channels, eps, elementwise_affine)\r\n        elif norm_type == \"rms_norm\":\r\n            self.norm = RMSNorm(channels, eps, elementwise_affine)\r\n        elif norm_type is None:\r\n            self.norm = None\r\n        else:\r\n            raise ValueError(f\"unknown norm_type: {norm_type}\")\r\n\r\n        if use_conv:\r\n            conv = CausalConv3d(\r\n                self.channels, self.out_channels, kernel_size=kernel_size, stride=stride, bias=bias\r\n            )\r\n        else:\r\n            raise NotImplementedError\r\n\r\n        if name == \"conv\":\r\n            self.Conv2d_0 = conv\r\n            self.conv = conv\r\n        elif name == \"Conv2d_0\":\r\n            self.conv = conv\r\n        else:\r\n            self.conv = conv\r\n\r\n    def forward(self, hidden_states: torch.FloatTensor, scale: float = 1.0) -> torch.FloatTensor:\r\n        assert hidden_states.shape[1] == self.channels\r\n\r\n        if self.norm is not None:\r\n            hidden_states = self.norm(hidden_states.permute(0, 2, 3, 1)).permute(0, 3, 1, 2)\r\n\r\n        assert hidden_states.shape[1] == self.channels\r\n\r\n        hidden_states = self.conv(hidden_states)\r\n\r\n        return hidden_states\r\n\r\n\r\nclass ResnetBlockCausal3D(nn.Module):\r\n    r\"\"\"\r\n    A Resnet block.\r\n    \"\"\"\r\n\r\n    def __init__(\r\n        self,\r\n        *,\r\n        in_channels: int,\r\n        out_channels: Optional[int] = None,\r\n        conv_shortcut: bool = False,\r\n        dropout: float = 0.0,\r\n        temb_channels: int = 512,\r\n        groups: int = 32,\r\n        groups_out: Optional[int] = None,\r\n        pre_norm: bool = True,\r\n        eps: float = 1e-6,\r\n        non_linearity: str = \"swish\",\r\n        skip_time_act: bool = False,\r\n        # default, scale_shift, ada_group, spatial\r\n        time_embedding_norm: str = \"default\",\r\n        kernel: Optional[torch.FloatTensor] = None,\r\n        output_scale_factor: float = 1.0,\r\n        use_in_shortcut: Optional[bool] = None,\r\n        up: bool = False,\r\n        down: bool = False,\r\n        conv_shortcut_bias: bool = True,\r\n        conv_3d_out_channels: Optional[int] = None,\r\n    ):\r\n        super().__init__()\r\n        self.pre_norm = pre_norm\r\n        self.pre_norm = True\r\n        self.in_channels = in_channels\r\n        out_channels = in_channels if out_channels is None else out_channels\r\n        self.out_channels = out_channels\r\n        self.use_conv_shortcut = conv_shortcut\r\n        self.up = up\r\n        self.down = down\r\n        self.output_scale_factor = output_scale_factor\r\n        self.time_embedding_norm = time_embedding_norm\r\n        self.skip_time_act = skip_time_act\r\n\r\n        linear_cls = nn.Linear\r\n\r\n        if groups_out is None:\r\n            groups_out = groups\r\n\r\n        if self.time_embedding_norm == \"ada_group\":\r\n            self.norm1 = AdaGroupNorm(temb_channels, in_channels, groups, eps=eps)\r\n        elif self.time_embedding_norm == \"spatial\":\r\n            self.norm1 = SpatialNorm(in_channels, temb_channels)\r\n        else:\r\n            self.norm1 = torch.nn.GroupNorm(num_groups=groups, num_channels=in_channels, eps=eps, affine=True)\r\n\r\n        self.conv1 = CausalConv3d(in_channels, out_channels, kernel_size=3, stride=1)\r\n\r\n        if temb_channels is not None:\r\n            if self.time_embedding_norm == \"default\":\r\n                self.time_emb_proj = linear_cls(temb_channels, out_channels)\r\n            elif self.time_embedding_norm == \"scale_shift\":\r\n                self.time_emb_proj = linear_cls(temb_channels, 2 * out_channels)\r\n            elif self.time_embedding_norm == \"ada_group\" or self.time_embedding_norm == \"spatial\":\r\n                self.time_emb_proj = None\r\n            else:\r\n                raise ValueError(f\"Unknown time_embedding_norm : {self.time_embedding_norm} \")\r\n        else:\r\n            self.time_emb_proj = None\r\n\r\n        if self.time_embedding_norm == \"ada_group\":\r\n            self.norm2 = AdaGroupNorm(temb_channels, out_channels, groups_out, eps=eps)\r\n        elif self.time_embedding_norm == \"spatial\":\r\n            self.norm2 = SpatialNorm(out_channels, temb_channels)\r\n        else:\r\n            self.norm2 = torch.nn.GroupNorm(num_groups=groups_out, num_channels=out_channels, eps=eps, affine=True)\r\n\r\n        self.dropout = torch.nn.Dropout(dropout)\r\n        conv_3d_out_channels = conv_3d_out_channels or out_channels\r\n        self.conv2 = CausalConv3d(out_channels, conv_3d_out_channels, kernel_size=3, stride=1)\r\n\r\n        self.nonlinearity = get_activation(non_linearity)\r\n\r\n        self.upsample = self.downsample = None\r\n        if self.up:\r\n            self.upsample = UpsampleCausal3D(in_channels, use_conv=False)\r\n        elif self.down:\r\n            self.downsample = DownsampleCausal3D(in_channels, use_conv=False, name=\"op\")\r\n\r\n        self.use_in_shortcut = self.in_channels != conv_3d_out_channels if use_in_shortcut is None else use_in_shortcut\r\n\r\n        self.conv_shortcut = None\r\n        if self.use_in_shortcut:\r\n            self.conv_shortcut = CausalConv3d(\r\n                in_channels,\r\n                conv_3d_out_channels,\r\n                kernel_size=1,\r\n                stride=1,\r\n                bias=conv_shortcut_bias,\r\n            )\r\n\r\n    def forward(\r\n        self,\r\n        input_tensor: torch.FloatTensor,\r\n        temb: torch.FloatTensor,\r\n        scale: float = 1.0,\r\n    ) -> torch.FloatTensor:\r\n        hidden_states = input_tensor\r\n\r\n        if self.time_embedding_norm == \"ada_group\" or self.time_embedding_norm == \"spatial\":\r\n            hidden_states = self.norm1(hidden_states, temb)\r\n        else:\r\n            hidden_states = self.norm1(hidden_states)\r\n\r\n        hidden_states = self.nonlinearity(hidden_states)\r\n\r\n        if self.upsample is not None:\r\n            # upsample_nearest_nhwc fails with large batch sizes. see https://github.com/huggingface/diffusers/issues/984\r\n            if hidden_states.shape[0] >= 64:\r\n                input_tensor = input_tensor.contiguous()\r\n                hidden_states = hidden_states.contiguous()\r\n            input_tensor = (\r\n                self.upsample(input_tensor, scale=scale)\r\n            )\r\n            hidden_states = (\r\n                self.upsample(hidden_states, scale=scale)\r\n            )\r\n        elif self.downsample is not None:\r\n            input_tensor = (\r\n                self.downsample(input_tensor, scale=scale)\r\n            )\r\n            hidden_states = (\r\n                self.downsample(hidden_states, scale=scale)\r\n            )\r\n\r\n        hidden_states = self.conv1(hidden_states)\r\n\r\n        if self.time_emb_proj is not None:\r\n            if not self.skip_time_act:\r\n                temb = self.nonlinearity(temb)\r\n            temb = (\r\n                self.time_emb_proj(temb, scale)[:, :, None, None]\r\n            )\r\n\r\n        if temb is not None and self.time_embedding_norm == \"default\":\r\n            hidden_states = hidden_states + temb\r\n\r\n        if self.time_embedding_norm == \"ada_group\" or self.time_embedding_norm == \"spatial\":\r\n            hidden_states = self.norm2(hidden_states, temb)\r\n        else:\r\n            hidden_states = self.norm2(hidden_states)\r\n\r\n        if temb is not None and self.time_embedding_norm == \"scale_shift\":\r\n            scale, shift = torch.chunk(temb, 2, dim=1)\r\n            hidden_states = hidden_states * (1 + scale) + shift\r\n\r\n        hidden_states = self.nonlinearity(hidden_states)\r\n\r\n        hidden_states = self.dropout(hidden_states)\r\n        hidden_states = self.conv2(hidden_states)\r\n\r\n        if self.conv_shortcut is not None:\r\n            input_tensor = (\r\n                self.conv_shortcut(input_tensor)\r\n            )\r\n\r\n        output_tensor = (input_tensor + hidden_states) / self.output_scale_factor\r\n\r\n        return output_tensor\r\n\r\n\r\ndef get_down_block3d(\r\n    down_block_type: str,\r\n    num_layers: int,\r\n    in_channels: int,\r\n    out_channels: int,\r\n    temb_channels: int,\r\n    add_downsample: bool,\r\n    downsample_stride: int,\r\n    resnet_eps: float,\r\n    resnet_act_fn: str,\r\n    transformer_layers_per_block: int = 1,\r\n    num_attention_heads: Optional[int] = None,\r\n    resnet_groups: Optional[int] = None,\r\n    cross_attention_dim: Optional[int] = None,\r\n    downsample_padding: Optional[int] = None,\r\n    dual_cross_attention: bool = False,\r\n    use_linear_projection: bool = False,\r\n    only_cross_attention: bool = False,\r\n    upcast_attention: bool = False,\r\n    resnet_time_scale_shift: str = \"default\",\r\n    attention_type: str = \"default\",\r\n    resnet_skip_time_act: bool = False,\r\n    resnet_out_scale_factor: float = 1.0,\r\n    cross_attention_norm: Optional[str] = None,\r\n    attention_head_dim: Optional[int] = None,\r\n    downsample_type: Optional[str] = None,\r\n    dropout: float = 0.0,\r\n):\r\n    # If attn head dim is not defined, we default it to the number of heads\r\n    if attention_head_dim is None:\r\n        logger.warn(\r\n            f\"It is recommended to provide `attention_head_dim` when calling `get_down_block`. Defaulting `attention_head_dim` to {num_attention_heads}.\"\r\n        )\r\n        attention_head_dim = num_attention_heads\r\n\r\n    down_block_type = down_block_type[7:] if down_block_type.startswith(\"UNetRes\") else down_block_type\r\n    if down_block_type == \"DownEncoderBlockCausal3D\":\r\n        return DownEncoderBlockCausal3D(\r\n            num_layers=num_layers,\r\n            in_channels=in_channels,\r\n            out_channels=out_channels,\r\n            dropout=dropout,\r\n            add_downsample=add_downsample,\r\n            downsample_stride=downsample_stride,\r\n            resnet_eps=resnet_eps,\r\n            resnet_act_fn=resnet_act_fn,\r\n            resnet_groups=resnet_groups,\r\n            downsample_padding=downsample_padding,\r\n            resnet_time_scale_shift=resnet_time_scale_shift,\r\n        )\r\n    raise ValueError(f\"{down_block_type} does not exist.\")\r\n\r\n\r\ndef get_up_block3d(\r\n    up_block_type: str,\r\n    num_layers: int,\r\n    in_channels: int,\r\n    out_channels: int,\r\n    prev_output_channel: int,\r\n    temb_channels: int,\r\n    add_upsample: bool,\r\n    upsample_scale_factor: Tuple,\r\n    resnet_eps: float,\r\n    resnet_act_fn: str,\r\n    resolution_idx: Optional[int] = None,\r\n    transformer_layers_per_block: int = 1,\r\n    num_attention_heads: Optional[int] = None,\r\n    resnet_groups: Optional[int] = None,\r\n    cross_attention_dim: Optional[int] = None,\r\n    dual_cross_attention: bool = False,\r\n    use_linear_projection: bool = False,\r\n    only_cross_attention: bool = False,\r\n    upcast_attention: bool = False,\r\n    resnet_time_scale_shift: str = \"default\",\r\n    attention_type: str = \"default\",\r\n    resnet_skip_time_act: bool = False,\r\n    resnet_out_scale_factor: float = 1.0,\r\n    cross_attention_norm: Optional[str] = None,\r\n    attention_head_dim: Optional[int] = None,\r\n    upsample_type: Optional[str] = None,\r\n    dropout: float = 0.0,\r\n) -> nn.Module:\r\n    # If attn head dim is not defined, we default it to the number of heads\r\n    if attention_head_dim is None:\r\n        logger.warn(\r\n            f\"It is recommended to provide `attention_head_dim` when calling `get_up_block`. Defaulting `attention_head_dim` to {num_attention_heads}.\"\r\n        )\r\n        attention_head_dim = num_attention_heads\r\n\r\n    up_block_type = up_block_type[7:] if up_block_type.startswith(\"UNetRes\") else up_block_type\r\n    if up_block_type == \"UpDecoderBlockCausal3D\":\r\n        return UpDecoderBlockCausal3D(\r\n            num_layers=num_layers,\r\n            in_channels=in_channels,\r\n            out_channels=out_channels,\r\n            resolution_idx=resolution_idx,\r\n            dropout=dropout,\r\n            add_upsample=add_upsample,\r\n            upsample_scale_factor=upsample_scale_factor,\r\n            resnet_eps=resnet_eps,\r\n            resnet_act_fn=resnet_act_fn,\r\n            resnet_groups=resnet_groups,\r\n            resnet_time_scale_shift=resnet_time_scale_shift,\r\n            temb_channels=temb_channels,\r\n        )\r\n    raise ValueError(f\"{up_block_type} does not exist.\")\r\n\r\n\r\nclass UNetMidBlockCausal3D(nn.Module):\r\n    \"\"\"\r\n    A 3D UNet mid-block [`UNetMidBlockCausal3D`] with multiple residual blocks and optional attention blocks.\r\n    \"\"\"\r\n\r\n    def __init__(\r\n        self,\r\n        in_channels: int,\r\n        temb_channels: int,\r\n        dropout: float = 0.0,\r\n        num_layers: int = 1,\r\n        resnet_eps: float = 1e-6,\r\n        resnet_time_scale_shift: str = \"default\",  # default, spatial\r\n        resnet_act_fn: str = \"swish\",\r\n        resnet_groups: int = 32,\r\n        attn_groups: Optional[int] = None,\r\n        resnet_pre_norm: bool = True,\r\n        add_attention: bool = True,\r\n        attention_head_dim: int = 1,\r\n        output_scale_factor: float = 1.0,\r\n    ):\r\n        super().__init__()\r\n        resnet_groups = resnet_groups if resnet_groups is not None else min(in_channels // 4, 32)\r\n        self.add_attention = add_attention\r\n\r\n        if attn_groups is None:\r\n            attn_groups = resnet_groups if resnet_time_scale_shift == \"default\" else None\r\n\r\n        # there is always at least one resnet\r\n        resnets = [\r\n            ResnetBlockCausal3D(\r\n                in_channels=in_channels,\r\n                out_channels=in_channels,\r\n                temb_channels=temb_channels,\r\n                eps=resnet_eps,\r\n                groups=resnet_groups,\r\n                dropout=dropout,\r\n                time_embedding_norm=resnet_time_scale_shift,\r\n                non_linearity=resnet_act_fn,\r\n                output_scale_factor=output_scale_factor,\r\n                pre_norm=resnet_pre_norm,\r\n            )\r\n        ]\r\n        attentions = []\r\n\r\n        if attention_head_dim is None:\r\n            logger.warn(\r\n                f\"It is not recommend to pass `attention_head_dim=None`. Defaulting `attention_head_dim` to `in_channels`: {in_channels}.\"\r\n            )\r\n            attention_head_dim = in_channels\r\n\r\n        for _ in range(num_layers):\r\n            if self.add_attention:\r\n                attentions.append(\r\n                    Attention(\r\n                        in_channels,\r\n                        heads=in_channels // attention_head_dim,\r\n                        dim_head=attention_head_dim,\r\n                        rescale_output_factor=output_scale_factor,\r\n                        eps=resnet_eps,\r\n                        norm_num_groups=attn_groups,\r\n                        spatial_norm_dim=temb_channels if resnet_time_scale_shift == \"spatial\" else None,\r\n                        residual_connection=True,\r\n                        bias=True,\r\n                        upcast_softmax=True,\r\n                        _from_deprecated_attn_block=True,\r\n                    )\r\n                )\r\n            else:\r\n                attentions.append(None)\r\n\r\n            resnets.append(\r\n                ResnetBlockCausal3D(\r\n                    in_channels=in_channels,\r\n                    out_channels=in_channels,\r\n                    temb_channels=temb_channels,\r\n                    eps=resnet_eps,\r\n                    groups=resnet_groups,\r\n                    dropout=dropout,\r\n                    time_embedding_norm=resnet_time_scale_shift,\r\n                    non_linearity=resnet_act_fn,\r\n                    output_scale_factor=output_scale_factor,\r\n                    pre_norm=resnet_pre_norm,\r\n                )\r\n            )\r\n\r\n        self.attentions = nn.ModuleList(attentions)\r\n        self.resnets = nn.ModuleList(resnets)\r\n\r\n    def forward(self, hidden_states: torch.FloatTensor, temb: Optional[torch.FloatTensor] = None) -> torch.FloatTensor:\r\n        hidden_states = self.resnets[0](hidden_states, temb)\r\n        for attn, resnet in zip(self.attentions, self.resnets[1:]):\r\n            if attn is not None:\r\n                B, C, T, H, W = hidden_states.shape\r\n                hidden_states = rearrange(hidden_states, \"b c f h w -> b (f h w) c\")\r\n                attention_mask = prepare_causal_attention_mask(\r\n                    T, H * W, hidden_states.dtype, hidden_states.device, batch_size=B\r\n                )\r\n                hidden_states = attn(hidden_states, temb=temb, attention_mask=attention_mask)\r\n                hidden_states = rearrange(hidden_states, \"b (f h w) c -> b c f h w\", f=T, h=H, w=W)\r\n            hidden_states = resnet(hidden_states, temb)\r\n\r\n        return hidden_states\r\n\r\n\r\nclass DownEncoderBlockCausal3D(nn.Module):\r\n    def __init__(\r\n        self,\r\n        in_channels: int,\r\n        out_channels: int,\r\n        dropout: float = 0.0,\r\n        num_layers: int = 1,\r\n        resnet_eps: float = 1e-6,\r\n        resnet_time_scale_shift: str = \"default\",\r\n        resnet_act_fn: str = \"swish\",\r\n        resnet_groups: int = 32,\r\n        resnet_pre_norm: bool = True,\r\n        output_scale_factor: float = 1.0,\r\n        add_downsample: bool = True,\r\n        downsample_stride: int = 2,\r\n        downsample_padding: int = 1,\r\n    ):\r\n        super().__init__()\r\n        resnets = []\r\n\r\n        for i in range(num_layers):\r\n            in_channels = in_channels if i == 0 else out_channels\r\n            resnets.append(\r\n                ResnetBlockCausal3D(\r\n                    in_channels=in_channels,\r\n                    out_channels=out_channels,\r\n                    temb_channels=None,\r\n                    eps=resnet_eps,\r\n                    groups=resnet_groups,\r\n                    dropout=dropout,\r\n                    time_embedding_norm=resnet_time_scale_shift,\r\n                    non_linearity=resnet_act_fn,\r\n                    output_scale_factor=output_scale_factor,\r\n                    pre_norm=resnet_pre_norm,\r\n                )\r\n            )\r\n\r\n        self.resnets = nn.ModuleList(resnets)\r\n\r\n        if add_downsample:\r\n            self.downsamplers = nn.ModuleList(\r\n                [\r\n                    DownsampleCausal3D(\r\n                        out_channels,\r\n                        use_conv=True,\r\n                        out_channels=out_channels,\r\n                        padding=downsample_padding,\r\n                        name=\"op\",\r\n                        stride=downsample_stride,\r\n                    )\r\n                ]\r\n            )\r\n        else:\r\n            self.downsamplers = None\r\n\r\n    def forward(self, hidden_states: torch.FloatTensor, scale: float = 1.0) -> torch.FloatTensor:\r\n        for resnet in self.resnets:\r\n            hidden_states = resnet(hidden_states, temb=None, scale=scale)\r\n\r\n        if self.downsamplers is not None:\r\n            for downsampler in self.downsamplers:\r\n                hidden_states = downsampler(hidden_states, scale)\r\n\r\n        return hidden_states\r\n\r\n\r\nclass UpDecoderBlockCausal3D(nn.Module):\r\n    def __init__(\r\n        self,\r\n        in_channels: int,\r\n        out_channels: int,\r\n        resolution_idx: Optional[int] = None,\r\n        dropout: float = 0.0,\r\n        num_layers: int = 1,\r\n        resnet_eps: float = 1e-6,\r\n        resnet_time_scale_shift: str = \"default\",  # default, spatial\r\n        resnet_act_fn: str = \"swish\",\r\n        resnet_groups: int = 32,\r\n        resnet_pre_norm: bool = True,\r\n        output_scale_factor: float = 1.0,\r\n        add_upsample: bool = True,\r\n        upsample_scale_factor=(2, 2, 2),\r\n        temb_channels: Optional[int] = None,\r\n    ):\r\n        super().__init__()\r\n        resnets = []\r\n\r\n        for i in range(num_layers):\r\n            input_channels = in_channels if i == 0 else out_channels\r\n\r\n            resnets.append(\r\n                ResnetBlockCausal3D(\r\n                    in_channels=input_channels,\r\n                    out_channels=out_channels,\r\n                    temb_channels=temb_channels,\r\n                    eps=resnet_eps,\r\n                    groups=resnet_groups,\r\n                    dropout=dropout,\r\n                    time_embedding_norm=resnet_time_scale_shift,\r\n                    non_linearity=resnet_act_fn,\r\n                    output_scale_factor=output_scale_factor,\r\n                    pre_norm=resnet_pre_norm,\r\n                )\r\n            )\r\n\r\n        self.resnets = nn.ModuleList(resnets)\r\n\r\n        if add_upsample:\r\n            self.upsamplers = nn.ModuleList(\r\n                [\r\n                    UpsampleCausal3D(\r\n                        out_channels,\r\n                        use_conv=True,\r\n                        out_channels=out_channels,\r\n                        upsample_factor=upsample_scale_factor,\r\n                    )\r\n                ]\r\n            )\r\n        else:\r\n            self.upsamplers = None\r\n\r\n        self.resolution_idx = resolution_idx\r\n\r\n    def forward(\r\n        self, hidden_states: torch.FloatTensor, temb: Optional[torch.FloatTensor] = None, scale: float = 1.0\r\n    ) -> torch.FloatTensor:\r\n        for resnet in self.resnets:\r\n            hidden_states = resnet(hidden_states, temb=temb, scale=scale)\r\n\r\n        if self.upsamplers is not None:\r\n            for upsampler in self.upsamplers:\r\n                hidden_states = upsampler(hidden_states)\r\n\r\n        return hidden_states\r\n"}
{"type": "source_file", "path": "svg/models/hyvideo/utils/__init__.py", "content": ""}
{"type": "source_file", "path": "svg/models/hyvideo/vae/vae.py", "content": "from dataclasses import dataclass\r\nfrom typing import Optional, Tuple\r\n\r\nimport numpy as np\r\nimport torch\r\nimport torch.nn as nn\r\n\r\nfrom diffusers.utils import BaseOutput, is_torch_version\r\nfrom diffusers.utils.torch_utils import randn_tensor\r\nfrom diffusers.models.attention_processor import SpatialNorm\r\nfrom .unet_causal_3d_blocks import (\r\n    CausalConv3d,\r\n    UNetMidBlockCausal3D,\r\n    get_down_block3d,\r\n    get_up_block3d,\r\n)\r\n\r\n\r\n@dataclass\r\nclass DecoderOutput(BaseOutput):\r\n    r\"\"\"\r\n    Output of decoding method.\r\n\r\n    Args:\r\n        sample (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\r\n            The decoded output sample from the last layer of the model.\r\n    \"\"\"\r\n\r\n    sample: torch.FloatTensor\r\n\r\n\r\nclass EncoderCausal3D(nn.Module):\r\n    r\"\"\"\r\n    The `EncoderCausal3D` layer of a variational autoencoder that encodes its input into a latent representation.\r\n    \"\"\"\r\n\r\n    def __init__(\r\n        self,\r\n        in_channels: int = 3,\r\n        out_channels: int = 3,\r\n        down_block_types: Tuple[str, ...] = (\"DownEncoderBlockCausal3D\",),\r\n        block_out_channels: Tuple[int, ...] = (64,),\r\n        layers_per_block: int = 2,\r\n        norm_num_groups: int = 32,\r\n        act_fn: str = \"silu\",\r\n        double_z: bool = True,\r\n        mid_block_add_attention=True,\r\n        time_compression_ratio: int = 4,\r\n        spatial_compression_ratio: int = 8,\r\n    ):\r\n        super().__init__()\r\n        self.layers_per_block = layers_per_block\r\n\r\n        self.conv_in = CausalConv3d(in_channels, block_out_channels[0], kernel_size=3, stride=1)\r\n        self.mid_block = None\r\n        self.down_blocks = nn.ModuleList([])\r\n\r\n        # down\r\n        output_channel = block_out_channels[0]\r\n        for i, down_block_type in enumerate(down_block_types):\r\n            input_channel = output_channel\r\n            output_channel = block_out_channels[i]\r\n            is_final_block = i == len(block_out_channels) - 1\r\n            num_spatial_downsample_layers = int(np.log2(spatial_compression_ratio))\r\n            num_time_downsample_layers = int(np.log2(time_compression_ratio))\r\n\r\n            if time_compression_ratio == 4:\r\n                add_spatial_downsample = bool(i < num_spatial_downsample_layers)\r\n                add_time_downsample = bool(\r\n                    i >= (len(block_out_channels) - 1 - num_time_downsample_layers)\r\n                    and not is_final_block\r\n                )\r\n            else:\r\n                raise ValueError(f\"Unsupported time_compression_ratio: {time_compression_ratio}.\")\r\n\r\n            downsample_stride_HW = (2, 2) if add_spatial_downsample else (1, 1)\r\n            downsample_stride_T = (2,) if add_time_downsample else (1,)\r\n            downsample_stride = tuple(downsample_stride_T + downsample_stride_HW)\r\n            down_block = get_down_block3d(\r\n                down_block_type,\r\n                num_layers=self.layers_per_block,\r\n                in_channels=input_channel,\r\n                out_channels=output_channel,\r\n                add_downsample=bool(add_spatial_downsample or add_time_downsample),\r\n                downsample_stride=downsample_stride,\r\n                resnet_eps=1e-6,\r\n                downsample_padding=0,\r\n                resnet_act_fn=act_fn,\r\n                resnet_groups=norm_num_groups,\r\n                attention_head_dim=output_channel,\r\n                temb_channels=None,\r\n            )\r\n            self.down_blocks.append(down_block)\r\n\r\n        # mid\r\n        self.mid_block = UNetMidBlockCausal3D(\r\n            in_channels=block_out_channels[-1],\r\n            resnet_eps=1e-6,\r\n            resnet_act_fn=act_fn,\r\n            output_scale_factor=1,\r\n            resnet_time_scale_shift=\"default\",\r\n            attention_head_dim=block_out_channels[-1],\r\n            resnet_groups=norm_num_groups,\r\n            temb_channels=None,\r\n            add_attention=mid_block_add_attention,\r\n        )\r\n\r\n        # out\r\n        self.conv_norm_out = nn.GroupNorm(num_channels=block_out_channels[-1], num_groups=norm_num_groups, eps=1e-6)\r\n        self.conv_act = nn.SiLU()\r\n\r\n        conv_out_channels = 2 * out_channels if double_z else out_channels\r\n        self.conv_out = CausalConv3d(block_out_channels[-1], conv_out_channels, kernel_size=3)\r\n\r\n    def forward(self, sample: torch.FloatTensor) -> torch.FloatTensor:\r\n        r\"\"\"The forward method of the `EncoderCausal3D` class.\"\"\"\r\n        assert len(sample.shape) == 5, \"The input tensor should have 5 dimensions\"\r\n\r\n        sample = self.conv_in(sample)\r\n\r\n        # down\r\n        for down_block in self.down_blocks:\r\n            sample = down_block(sample)\r\n\r\n        # middle\r\n        sample = self.mid_block(sample)\r\n\r\n        # post-process\r\n        sample = self.conv_norm_out(sample)\r\n        sample = self.conv_act(sample)\r\n        sample = self.conv_out(sample)\r\n\r\n        return sample\r\n\r\n\r\nclass DecoderCausal3D(nn.Module):\r\n    r\"\"\"\r\n    The `DecoderCausal3D` layer of a variational autoencoder that decodes its latent representation into an output sample.\r\n    \"\"\"\r\n\r\n    def __init__(\r\n        self,\r\n        in_channels: int = 3,\r\n        out_channels: int = 3,\r\n        up_block_types: Tuple[str, ...] = (\"UpDecoderBlockCausal3D\",),\r\n        block_out_channels: Tuple[int, ...] = (64,),\r\n        layers_per_block: int = 2,\r\n        norm_num_groups: int = 32,\r\n        act_fn: str = \"silu\",\r\n        norm_type: str = \"group\",  # group, spatial\r\n        mid_block_add_attention=True,\r\n        time_compression_ratio: int = 4,\r\n        spatial_compression_ratio: int = 8,\r\n    ):\r\n        super().__init__()\r\n        self.layers_per_block = layers_per_block\r\n\r\n        self.conv_in = CausalConv3d(in_channels, block_out_channels[-1], kernel_size=3, stride=1)\r\n        self.mid_block = None\r\n        self.up_blocks = nn.ModuleList([])\r\n\r\n        temb_channels = in_channels if norm_type == \"spatial\" else None\r\n\r\n        # mid\r\n        self.mid_block = UNetMidBlockCausal3D(\r\n            in_channels=block_out_channels[-1],\r\n            resnet_eps=1e-6,\r\n            resnet_act_fn=act_fn,\r\n            output_scale_factor=1,\r\n            resnet_time_scale_shift=\"default\" if norm_type == \"group\" else norm_type,\r\n            attention_head_dim=block_out_channels[-1],\r\n            resnet_groups=norm_num_groups,\r\n            temb_channels=temb_channels,\r\n            add_attention=mid_block_add_attention,\r\n        )\r\n\r\n        # up\r\n        reversed_block_out_channels = list(reversed(block_out_channels))\r\n        output_channel = reversed_block_out_channels[0]\r\n        for i, up_block_type in enumerate(up_block_types):\r\n            prev_output_channel = output_channel\r\n            output_channel = reversed_block_out_channels[i]\r\n            is_final_block = i == len(block_out_channels) - 1\r\n            num_spatial_upsample_layers = int(np.log2(spatial_compression_ratio))\r\n            num_time_upsample_layers = int(np.log2(time_compression_ratio))\r\n\r\n            if time_compression_ratio == 4:\r\n                add_spatial_upsample = bool(i < num_spatial_upsample_layers)\r\n                add_time_upsample = bool(\r\n                    i >= len(block_out_channels) - 1 - num_time_upsample_layers\r\n                    and not is_final_block\r\n                )\r\n            else:\r\n                raise ValueError(f\"Unsupported time_compression_ratio: {time_compression_ratio}.\")\r\n\r\n            upsample_scale_factor_HW = (2, 2) if add_spatial_upsample else (1, 1)\r\n            upsample_scale_factor_T = (2,) if add_time_upsample else (1,)\r\n            upsample_scale_factor = tuple(upsample_scale_factor_T + upsample_scale_factor_HW)\r\n            up_block = get_up_block3d(\r\n                up_block_type,\r\n                num_layers=self.layers_per_block + 1,\r\n                in_channels=prev_output_channel,\r\n                out_channels=output_channel,\r\n                prev_output_channel=None,\r\n                add_upsample=bool(add_spatial_upsample or add_time_upsample),\r\n                upsample_scale_factor=upsample_scale_factor,\r\n                resnet_eps=1e-6,\r\n                resnet_act_fn=act_fn,\r\n                resnet_groups=norm_num_groups,\r\n                attention_head_dim=output_channel,\r\n                temb_channels=temb_channels,\r\n                resnet_time_scale_shift=norm_type,\r\n            )\r\n            self.up_blocks.append(up_block)\r\n            prev_output_channel = output_channel\r\n\r\n        # out\r\n        if norm_type == \"spatial\":\r\n            self.conv_norm_out = SpatialNorm(block_out_channels[0], temb_channels)\r\n        else:\r\n            self.conv_norm_out = nn.GroupNorm(num_channels=block_out_channels[0], num_groups=norm_num_groups, eps=1e-6)\r\n        self.conv_act = nn.SiLU()\r\n        self.conv_out = CausalConv3d(block_out_channels[0], out_channels, kernel_size=3)\r\n\r\n        self.gradient_checkpointing = False\r\n\r\n    def forward(\r\n        self,\r\n        sample: torch.FloatTensor,\r\n        latent_embeds: Optional[torch.FloatTensor] = None,\r\n    ) -> torch.FloatTensor:\r\n        r\"\"\"The forward method of the `DecoderCausal3D` class.\"\"\"\r\n        assert len(sample.shape) == 5, \"The input tensor should have 5 dimensions.\"\r\n\r\n        sample = self.conv_in(sample)\r\n\r\n        upscale_dtype = next(iter(self.up_blocks.parameters())).dtype\r\n        if self.training and self.gradient_checkpointing:\r\n\r\n            def create_custom_forward(module):\r\n                def custom_forward(*inputs):\r\n                    return module(*inputs)\r\n\r\n                return custom_forward\r\n\r\n            if is_torch_version(\">=\", \"1.11.0\"):\r\n                # middle\r\n                sample = torch.utils.checkpoint.checkpoint(\r\n                    create_custom_forward(self.mid_block),\r\n                    sample,\r\n                    latent_embeds,\r\n                    use_reentrant=False,\r\n                )\r\n                sample = sample.to(upscale_dtype)\r\n\r\n                # up\r\n                for up_block in self.up_blocks:\r\n                    sample = torch.utils.checkpoint.checkpoint(\r\n                        create_custom_forward(up_block),\r\n                        sample,\r\n                        latent_embeds,\r\n                        use_reentrant=False,\r\n                    )\r\n            else:\r\n                # middle\r\n                sample = torch.utils.checkpoint.checkpoint(\r\n                    create_custom_forward(self.mid_block), sample, latent_embeds\r\n                )\r\n                sample = sample.to(upscale_dtype)\r\n\r\n                # up\r\n                for up_block in self.up_blocks:\r\n                    sample = torch.utils.checkpoint.checkpoint(create_custom_forward(up_block), sample, latent_embeds)\r\n        else:\r\n            # middle\r\n            sample = self.mid_block(sample, latent_embeds)\r\n            sample = sample.to(upscale_dtype)\r\n\r\n            # up\r\n            for up_block in self.up_blocks:\r\n                sample = up_block(sample, latent_embeds)\r\n\r\n        # post-process\r\n        if latent_embeds is None:\r\n            sample = self.conv_norm_out(sample)\r\n        else:\r\n            sample = self.conv_norm_out(sample, latent_embeds)\r\n        sample = self.conv_act(sample)\r\n        sample = self.conv_out(sample)\r\n\r\n        return sample\r\n\r\n\r\nclass DiagonalGaussianDistribution(object):\r\n    def __init__(self, parameters: torch.Tensor, deterministic: bool = False):\r\n        if parameters.ndim == 3:\r\n            dim = 2  # (B, L, C)\r\n        elif parameters.ndim == 5 or parameters.ndim == 4:\r\n            dim = 1  # (B, C, T, H ,W) / (B, C, H, W)\r\n        else:\r\n            raise NotImplementedError\r\n        self.parameters = parameters\r\n        self.mean, self.logvar = torch.chunk(parameters, 2, dim=dim)\r\n        self.logvar = torch.clamp(self.logvar, -30.0, 20.0)\r\n        self.deterministic = deterministic\r\n        self.std = torch.exp(0.5 * self.logvar)\r\n        self.var = torch.exp(self.logvar)\r\n        if self.deterministic:\r\n            self.var = self.std = torch.zeros_like(\r\n                self.mean, device=self.parameters.device, dtype=self.parameters.dtype\r\n            )\r\n\r\n    def sample(self, generator: Optional[torch.Generator] = None) -> torch.FloatTensor:\r\n        # make sure sample is on the same device as the parameters and has same dtype\r\n        sample = randn_tensor(\r\n            self.mean.shape,\r\n            generator=generator,\r\n            device=self.parameters.device,\r\n            dtype=self.parameters.dtype,\r\n        )\r\n        x = self.mean + self.std * sample\r\n        return x\r\n\r\n    def kl(self, other: \"DiagonalGaussianDistribution\" = None) -> torch.Tensor:\r\n        if self.deterministic:\r\n            return torch.Tensor([0.0])\r\n        else:\r\n            reduce_dim = list(range(1, self.mean.ndim))\r\n            if other is None:\r\n                return 0.5 * torch.sum(\r\n                    torch.pow(self.mean, 2) + self.var - 1.0 - self.logvar,\r\n                    dim=reduce_dim,\r\n                )\r\n            else:\r\n                return 0.5 * torch.sum(\r\n                    torch.pow(self.mean - other.mean, 2) / other.var\r\n                    + self.var / other.var\r\n                    - 1.0\r\n                    - self.logvar\r\n                    + other.logvar,\r\n                    dim=reduce_dim,\r\n                )\r\n\r\n    def nll(self, sample: torch.Tensor, dims: Tuple[int, ...] = [1, 2, 3]) -> torch.Tensor:\r\n        if self.deterministic:\r\n            return torch.Tensor([0.0])\r\n        logtwopi = np.log(2.0 * np.pi)\r\n        return 0.5 * torch.sum(\r\n            logtwopi + self.logvar +\r\n            torch.pow(sample - self.mean, 2) / self.var,\r\n            dim=dims,\r\n        )\r\n\r\n    def mode(self) -> torch.Tensor:\r\n        return self.mean\r\n"}
