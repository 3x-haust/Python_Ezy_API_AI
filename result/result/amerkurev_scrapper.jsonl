{"repo_info": {"repo_name": "scrapper", "repo_owner": "amerkurev", "repo_url": "https://github.com/amerkurev/scrapper"}}
{"type": "test_file", "path": "app/internal/util/test_htmlutil.py", "content": "from app.internal.util.htmlutil import levenshtein_similarity\n\n\ndef test_levenshtein_similarity():\n    assert levenshtein_similarity('hello', 'hello') == 1.0\n    assert levenshtein_similarity('hello', 'world') == 0.19999999999999996\n    assert levenshtein_similarity('hello', 'hell') == 0.8\n    assert levenshtein_similarity('hello', 'helo') == 0.8\n    assert levenshtein_similarity('hello', 'buy') == 0.0\n"}
{"type": "test_file", "path": "app/routers/test_misc.py", "content": "from fastapi.testclient import TestClient\n\nfrom main import app\n\n\ndef test_ping():\n    with TestClient(app) as client:\n        response = client.get('/ping')\n        assert response.status_code == 200\n\n\ndef test_docs():\n    with TestClient(app) as client:\n        response = client.get('/docs')\n        assert response.status_code == 200\n"}
{"type": "test_file", "path": "app/routers/test_query_params.py", "content": "from fastapi.testclient import TestClient\n\nfrom main import app\nfrom settings import USER_SCRIPTS_DIR\n\n\ndef test_various_query_params():\n    api_url = '/api/article'\n\n    with TestClient(app) as client:\n        # test stealth mode and page scroll down\n        url = 'https://en.wikipedia.org/wiki/Web_scraping'\n        params = {\n            'url': url,\n            'cache': False,\n            'full-content': True,\n            'stealth': True,\n            'sleep': 1000,  # 1 second\n            'scroll-down': 500,  # 500 pixels\n        }\n        response = client.get(api_url, params=params)\n        assert response.status_code == 200\n\n        # test persistent context (incognito=no)\n        url = 'https://en.wikipedia.org/wiki/Data_scraping'\n        params = {\n            'url': url,\n            'cache': False,\n            'incognito': False,\n            'resource': 'document,stylesheet,fetch',\n            'extra-http-headers': [\n                'Accept-Language:da, en-gb, en',\n                'Accept-Charset: ISO-8859-1,utf-8;q=0.7,*;q=0.3;',\n                'Accept: text/html;q=0.9,text/plain',\n            ],\n            'user-scripts-timeout': 1000,  # 1 second\n            'http-credentials': 'username:password',\n        }\n        response = client.get(api_url, params=params)\n        assert response.status_code == 200\n\n        # wrong http header\n        params = {\n            'url': url,\n            'extra-http-headers': 'Accept-Language',\n        }\n        response = client.get(api_url, params=params)\n        assert response.status_code == 422\n        assert response.json() == {\n            'detail': [{\n                'input': ['Accept-Language'],\n                'loc': ['query', 'extra_http_headers'],\n                'msg': 'Invalid HTTP header',\n                'type': 'extra_http_headers_parsing',\n            }]\n        }\n\n        # test fake proxy\n        params = {\n            'url': url,\n            'user-scripts-timeout': 1000,  # 1 second\n            'http-credentials': 'username',\n            'proxy-server': 'http://myproxyserver.com',\n            'proxy-username': 'user',\n            'proxy-password': 'pass',\n            'proxy-bypass': 'localhost',\n        }\n        response = client.get(api_url, params=params)\n        assert response.status_code == 500\n        assert 'PlaywrightError: NS_ERROR_' in response.text\n\n        # test user scripts\n        with open(USER_SCRIPTS_DIR / 'my-script.js', mode='w', encoding='utf-8') as f:\n            f.write('console.log(\"Hello world!\");')\n\n        url = 'https://en.wikipedia.org/wiki/World_Wide_Web'\n        params = {\n            'url': url,\n            'cache': False,\n            'user-scripts': 'my-script.js',\n        }\n        response = client.get(api_url, params=params)\n        assert response.status_code == 200\n\n        # test user script that not exists\n        params = {\n            'url': url,\n            'user-scripts': 'not-exists.js',\n        }\n        response = client.get(api_url, params=params)\n        assert response.status_code == 422\n        assert response.json() == {\n            'detail': [{\n                'input': 'not-exists.js',\n                'loc': ['query', 'user_scripts'],\n                'msg': 'User script not found',\n                'type': 'user_scripts_parsing',\n            }]\n        }\n\n        # test huge page with taking screenshot\n        params = {\n            'url': 'https://en.wikipedia.org/wiki/African_humid_period',\n            'cache': False,\n            'screenshot': True,\n            'device': 'Desktop Firefox',  # to raise (Page.screenshot): Cannot take screenshot larger than 32767\n        }\n        response = client.get(api_url, params=params)\n        assert response.status_code == 200\n\n        # test viewport and screen settings\n        url = 'https://en.wikipedia.org/wiki/World_Wide_Web'\n        params = {\n            'url': url,\n            'cache': False,\n            'viewport-width': 390,\n            'viewport-height': 844,\n            'screen-width': 1170,\n            'screen-height': 2532,\n            'user-agent': 'Mozilla/5.0 (iPhone; CPU iPhone OS 14_4 like Mac OS X) '\n        }\n        response = client.get(api_url, params=params)\n        assert response.status_code == 200\n\n        # test wrong device name\n        params = {\n            'url': url,\n            'device': 'not-exists',\n        }\n        response = client.get(api_url, params=params)\n        assert response.status_code == 422\n        assert response.json() == {\n            'detail': [{\n                'input': 'not-exists',\n                'loc': ['query', 'device'],\n                'msg': 'Device not found',\n                'type': 'device_parsing',\n            }]\n        }\n"}
{"type": "test_file", "path": "app/routers/test_results.py", "content": "from fastapi.testclient import TestClient\n\nfrom main import app\n\n\ndef test_get_article():\n    api_url = '/api/article'\n    url = 'https://en.wikipedia.org/wiki/Web_crawler'\n    _get_result(api_url, url)\n\n\ndef test_get_links():\n    api_url = '/api/links'\n    url = 'https://en.wikinews.org/wiki/Main_Page'\n    _get_result(api_url, url)\n\n\ndef test_get_any_page():\n    api_url = '/api/page'\n    url = 'https://en.wikipedia.org/wiki/Robots.txt'\n    _get_result(api_url, url)\n\n\ndef _get_result(api_url: str, url: str):\n    with TestClient(app) as client:\n        # empty url\n        response = client.get(api_url)\n        assert response.status_code == 422\n        assert response.json() == {\n            'detail': [{\n                'input': None,\n                'loc': ['query', 'url'],\n                'msg': 'Field required',\n                'type': 'missing',\n                'url': 'https://errors.pydantic.dev/2.5/v/missing'\n            }]\n        }\n\n        # invalid url\n        response = client.get(f'{api_url}?url=//example.com')\n        assert response.status_code == 422\n        assert response.json() == {\n            'detail': [{\n                'input': '//example.com',\n                'loc': ['query', 'url'],\n                'msg': 'Invalid URL',\n                'type': 'url_parsing',\n            }]\n        }\n\n        # at this time cache is empty, and data is fetched from the web\n        params = {'url': url, 'cache': True, 'full-content': True, 'screenshot': True}\n        response = client.get(api_url, params=params)\n        assert response.status_code == 200\n\n        # but now cache is not empty, and data is fetched from the cache\n        response = client.get(api_url, params=params)\n        assert response.status_code == 200\n        data = response.json()\n\n        # get the screenshot\n        response = client.get(data['screenshotUri'])\n        assert response.status_code == 200\n        assert response.headers['content-type'] == 'image/jpeg'\n\n        # get result from cache\n        response = client.get(data['resultUri'])\n        assert response.status_code == 200\n        assert response.json() == data\n\n        # get html result view\n        response = client.get(f'/view/{data[\"id\"]}')\n        assert response.status_code == 200\n        assert '<!doctype html>' in response.text\n\n        # not found error\n        response = client.get('/screenshot/0000')\n        assert response.status_code == 404\n\n        response = client.get('/result/0000')\n        assert response.status_code == 404\n\n        response = client.get('/view/0000')\n        assert response.status_code == 404\n"}
{"type": "test_file", "path": "app/test_main.py", "content": "from fastapi.testclient import TestClient\n\nfrom main import app\n\n\nclient = TestClient(app)\n\n\ndef test_get_favicon():\n    response = client.get('/favicon.ico')\n    assert response.status_code == 200\n    assert response.headers['content-type'] == 'image/vnd.microsoft.icon'\n\n\ndef test_index_html():\n    response = client.get('/')\n    assert response.status_code == 200\n    assert 'https://example.com/article.html' in response.text\n    assert 'apiEndpoint = \"/api/article\"' in response.text\n\n    response = client.get('/links')\n    assert response.status_code == 200\n    assert 'https://example.com/news.html' in response.text\n    assert 'apiEndpoint = \"/api/links\"' in response.text\n"}
{"type": "test_file", "path": "app/internal/test_errors.py", "content": "from fastapi.exceptions import HTTPException, RequestValidationError\n\nfrom .errors import ArticleParsingError, LinksParsingError, QueryParsingError\n\n\ndef test_errors():\n    page_url = 'https://example.com'\n    msg = \"The page doesn't contain any articles.\"\n    err = ArticleParsingError(page_url, msg)\n    assert isinstance(err, HTTPException)\n    assert err.status_code == 400\n    assert err.detail == [\n        {\n            'type': 'article_parsing',\n            'loc': ('readability.js',),\n            'msg': \"The page doesn't contain any articles.\",\n            'input': page_url,\n        }\n    ]\n\n    msg = \"The page doesn't contain any links.\"\n    err = LinksParsingError(page_url, msg)\n    assert isinstance(err, HTTPException)\n    assert err.status_code == 400\n    assert err.detail == [\n        {\n            'type': 'links_parsing',\n            'loc': ('links.js',),\n            'msg': \"The page doesn't contain any links.\",\n            'input': page_url,\n        }\n    ]\n\n    field = 'url'\n    msg = 'Invalid URL'\n    value = 'example.com'\n    err = QueryParsingError(field, msg, value)\n    assert isinstance(err, RequestValidationError)\n    assert err.errors() == [\n        {\n            'type': f'{field}_parsing',\n            'loc': ('query', field),\n            'msg': msg,\n            'input': value,\n        }\n    ]\n\n    field = 'http_credentials'\n    msg = 'Invalid HTTP credentials'\n    value = {'username': 'user', 'password': 'pass'}\n    err = QueryParsingError(field, msg, value)\n    assert isinstance(err, RequestValidationError)\n    assert err.errors() == [\n        {\n            'type': f'{field}_parsing',\n            'loc': ('query', field),\n            'msg': msg,\n            'input': value,\n        }\n    ]\n"}
{"type": "source_file", "path": "app/internal/util/htmlutil.py", "content": "from collections.abc import MutableMapping\n\nfrom bs4 import BeautifulSoup\n\n\nTITLE_MAX_DISTANCE = 350\nACCEPTABLE_LINK_TEXT_LEN = 40\n\n\ndef improve_content(title: str, content: str) -> str:\n    tree = BeautifulSoup(content, 'html.parser')\n\n    # 1. remove all p and div tags that contain one word or less (or only digits),\n    # and not contain any images (or headers)\n    for el in tree.find_all(['p', 'div']):\n        # skip if the element has any images, headers, code blocks, lists, tables, forms, etc.\n        if el.find([\n            'img', 'picture', 'svg', 'canvas', 'video', 'audio', 'iframe', 'embed', 'object', 'param', 'source',\n            'h1', 'h2', 'h3', 'h4', 'h5', 'h6',\n            'pre', 'code', 'blockquote', 'dl', 'ol', 'ul', 'table', 'form',\n        ]):\n            continue\n        text = el.get_text(strip=True)\n        # remove the element if it contains one word or less (or only digits)\n        words = text.split()\n        if len(words) <= 1 or (''.join(words)).isnumeric():\n            el.decompose()\n\n    # 2. move the first tag h1 (or h2) to the top of the tree\n    title_distance = 0\n\n    for el in tree.find_all(string=True):\n        if el.parent.name in ('h1', 'h2', 'h3'):\n            text = el.parent.get_text(strip=True)\n            # stop if the header is similar to the title\n            min_len = min(len(text), len(title))\n            # remove all non-alphabetic characters and convert to lowercase\n            # noinspection PyTypeChecker\n            str1 = ''.join(filter(str.isalpha, text[:min_len])).lower()\n            # noinspection PyTypeChecker\n            str2 = ''.join(filter(str.isalpha, title[:min_len])).lower()\n            if str1 and str2 and levenshtein_similarity(str1, str2) > 0.9:\n                title = text\n                el.parent.decompose()  # 'real' move will be below, at 3.1 or 3.2\n                break\n\n        # stop if distance is too big\n        title_distance += len(el.text)\n        if title_distance > TITLE_MAX_DISTANCE:\n            # will be used article['title'] as title\n            break\n\n    # 3.1 check if article tag already exists, and then insert the title into it\n    for el in tree.find_all():\n        if el.name == 'article':\n            el.insert(0, BeautifulSoup(f'<h1>{title}</h1>', 'html.parser'))\n            return str(tree)\n\n    # 3.2 if not, create a new article tag and insert the title into it\n    content = str(tree)\n    return f'<article><h1>{title}</h1>{content}</article>'\n\n\ndef improve_link(link: MutableMapping) -> MutableMapping:\n    lines = link['text'].splitlines()\n    text = ''\n    # find the longest line\n    for line in lines:\n        if len(line) > len(text):\n            text = line\n        # stop if the line is long enough\n        if len(text) > ACCEPTABLE_LINK_TEXT_LEN:\n            break\n\n    link['text'] = text\n    return link\n\n\ndef social_meta_tags(full_page_content: str) -> dict:\n    og = {}  # open graph\n    twitter = {}\n    tree = BeautifulSoup(full_page_content, 'html.parser')\n    for el in tree.find_all('meta'):\n        attrs = el.attrs\n        # Open Graph protocol\n        if 'property' in attrs and attrs['property'].startswith('og:'):\n            key = attrs['property'][3:]  # len('og:') == 3\n            if key and 'content' in attrs:\n                og[key] = attrs['content']\n\n        # Twitter protocol\n        if 'name' in attrs and attrs['name'].startswith('twitter:'):\n            key = attrs['name'][8:]  # len('twitter:') == 8\n            if key and 'content' in attrs:\n                twitter[key] = attrs['content']\n\n    res = {key: props for key, props in (('og', og), ('twitter', twitter)) if props}\n    return res\n\n\ndef levenshtein_similarity(str1: str, str2: str) -> float:\n    # Create a matrix to hold the distances\n    d = [[0] * (len(str2) + 1) for _ in range(len(str1) + 1)]\n\n    # Initialize the first row and column of the matrix\n    for i in range(len(str1) + 1):\n        d[i][0] = i\n    for j in range(len(str2) + 1):\n        d[0][j] = j\n\n    # Fill in the rest of the matrix\n    for i in range(1, len(str1) + 1):\n        for j in range(1, len(str2) + 1):\n            if str1[i - 1] == str2[j - 1]:\n                d[i][j] = d[i - 1][j - 1]\n            else:\n                d[i][j] = min(d[i - 1][j], d[i][j - 1], d[i - 1][j - 1]) + 1\n\n    # return normalized distance\n    return 1 - d[-1][-1] / max(len(str1), len(str2))\n\n\ndef improve_text_content(text: str) -> str:\n    s = '\\n'.join(filter(None, map(str.strip, text.splitlines())))\n    return s\n"}
{"type": "source_file", "path": "app/main.py", "content": "from fastapi import FastAPI, status\nfrom fastapi.requests import Request\nfrom fastapi.responses import HTMLResponse, PlainTextResponse, FileResponse\nfrom fastapi.staticfiles import StaticFiles\nfrom fastapi.templating import Jinja2Templates\nfrom playwright.async_api import Error as PlaywrightError\n\nfrom dependencies import lifespan\nfrom settings import TEMPLATES_DIR, STATIC_DIR, ICON_PATH\nfrom routers import article, links, misc, any_page, results\nfrom version import revision\n\n# at startup\nprint('revision:', revision)\n\n\napp = FastAPI(\n    title='Scrapper',\n    summary='Web scraper with a simple REST API living in Docker and using a Headless browser and Readability.js for parsing.',\n    contact={\n        'name': 'GitHub',\n        'url': 'https://github.com/amerkurev/scrapper',\n    },\n    license_info={\n        'name': 'Apache-2.0 license',\n        'url': 'https://github.com/amerkurev/scrapper/blob/master/LICENSE',\n    },\n    description=f'revision: {revision}',\n    lifespan=lifespan,\n)\napp.mount('/static', StaticFiles(directory=STATIC_DIR), name='static')\napp.include_router(article.router)\napp.include_router(links.router)\napp.include_router(any_page.router)\napp.include_router(misc.router)\napp.include_router(results.router)\ntemplates = Jinja2Templates(directory=TEMPLATES_DIR)\n\n\n@app.get('/favicon.ico', response_class=FileResponse, include_in_schema=False)\nasync def favicon():\n    return FileResponse(ICON_PATH, media_type='image/vnd.microsoft.icon')\n\n\n@app.get('/', response_class=HTMLResponse, include_in_schema=False)\n@app.get('/links', response_class=HTMLResponse, include_in_schema=False)\nasync def root(request: Request):\n    for_example = (\n        'cache=yes',\n        'full-content=no',\n        'stealth=no',\n        'screenshot=no',\n        'incognito=yes',\n        'timeout=60000',\n        'wait-until=domcontentloaded',\n        'sleep=0',\n        'device=iPhone 12',\n    )\n    context = {\n        'request': request,\n        'revision': revision,\n        'for_example': '&#10;'.join(for_example),\n    }\n    return templates.TemplateResponse('index.html', context=context)\n\n\n@app.exception_handler(PlaywrightError)\nasync def playwright_exception_handler(_, err):\n    content = f'PlaywrightError: {err}'\n    return PlainTextResponse(content, status_code=status.HTTP_500_INTERNAL_SERVER_ERROR)\n"}
{"type": "source_file", "path": "app/routers/links.py", "content": "import asyncio\nimport datetime\nimport hashlib\n\nfrom collections import defaultdict\nfrom operator import itemgetter\nfrom statistics import median\n\nfrom typing import Annotated, Mapping, Sequence\n\nimport tldextract\n\nfrom fastapi import APIRouter, Query, Depends\nfrom fastapi.requests import Request\nfrom pydantic import BaseModel\nfrom playwright.async_api import Browser\n\nfrom settings import PARSER_SCRIPTS_DIR\nfrom internal import cache\nfrom internal.browser import (\n    new_context,\n    page_processing,\n    get_screenshot,\n)\nfrom internal.util import htmlutil, split_url\nfrom internal.errors import LinksParsingError\nfrom .query_params import (\n    URLParam,\n    CommonQueryParams,\n    BrowserQueryParams,\n    ProxyQueryParams,\n    LinkParserQueryParams,\n)\n\n\nrouter = APIRouter(prefix='/api/links', tags=['links'])\n\n\nclass Links(BaseModel):\n    id: Annotated[str, Query(description='unique result ID')]\n    url: Annotated[str, Query(description='page URL after redirects, may not match the query URL')]\n    domain: Annotated[str, Query(description=\"page's registered domain\")]\n    date: Annotated[str, Query(description='date of extracted article in ISO 8601 format')]\n    query: Annotated[dict, Query(description='request parameters')]\n    meta: Annotated[dict, Query(description='social meta tags (open graph, twitter)')]\n    resultUri: Annotated[str, Query(description='URL of the current result, the data here is always taken from cache')]\n    fullContent: Annotated[str | None, Query(description='full HTML contents of the page')] = None\n    screenshotUri: Annotated[str | None, Query(description='URL of the screenshot of the page')] = None\n    title: Annotated[str | None, Query(description=\"page's title\")] = None\n    links: Annotated[list[dict], Query(description='list of links')]\n\n\n@router.get('', summary='Parse news links from the given URL', response_model=Links)\nasync def parser_links(\n    request: Request,\n    url: Annotated[URLParam, Depends()],\n    common_params: Annotated[CommonQueryParams, Depends()],\n    browser_params: Annotated[BrowserQueryParams, Depends()],\n    proxy_params: Annotated[ProxyQueryParams, Depends()],\n    link_parser_params: Annotated[LinkParserQueryParams, Depends()],\n) -> dict:\n    \"\"\"\n    Parse news links from the given URL.<br><br>\n    The page from the URL should contain hyperlinks to news articles. For example, this could be the main page of a website.\n    \"\"\"\n    # split URL into parts: host with scheme, path with query, query params as a dict\n    host_url, full_path, query_dict = split_url(request.url)\n\n    # get cache data if exists\n    r_id = cache.make_key(full_path)  # unique result ID\n    if common_params.cache:\n        data = cache.load_result(key=r_id)\n        if data:\n            return data\n\n    browser: Browser = request.state.browser\n    semaphore: asyncio.Semaphore = request.state.semaphore\n\n    # create a new browser context\n    async with semaphore:\n        async with new_context(browser, browser_params, proxy_params) as context:\n            page = await context.new_page()\n            await page_processing(\n                page=page,\n                url=url.url,\n                params=common_params,\n                browser_params=browser_params,\n            )\n            page_content = await page.content()\n            screenshot = await get_screenshot(page) if common_params.screenshot else None\n            page_url = page.url\n            title = await page.title()\n\n            # evaluating JavaScript: parse DOM and extract links of articles\n            parser_args = {}\n            with open(PARSER_SCRIPTS_DIR / 'links.js', encoding='utf-8') as f:\n                links = await page.evaluate(f.read() % parser_args)\n\n    # parser error: links are not extracted, result has 'err' field\n    if 'err' in links:\n        raise LinksParsingError(page_url, links['err'])  # pragma: no cover\n\n    # filter links by domain\n    domain = tldextract.extract(url.url).domain\n    links = [x for x in links if allowed_domain(x['href'], domain)]\n\n    links_dict = group_links(links)\n\n    # get stat for groups of links and filter groups with\n    # median length of text and words more than 40 and 3\n    links = []\n    for _, group in links_dict.items():\n        stat = get_stat(\n            group,\n            text_len_threshold=link_parser_params.text_len_threshold,\n            words_threshold=link_parser_params.words_threshold,\n        )\n        if stat['approved']:\n            links.extend(group)\n\n    # sort links by 'pos' field, to show links in the same order as they are on the page\n    # ('pos' is position of link in DOM)\n    links.sort(key=itemgetter('pos'))\n    links = list(map(htmlutil.improve_link, map(link_fields, links)))\n\n    # set common fields\n    r = {\n        'id': r_id,\n        'url': page_url,\n        'domain': tldextract.extract(page_url).registered_domain,\n        'date': datetime.datetime.utcnow().isoformat(),  # ISO 8601 format\n        'resultUri': f'{host_url}/result/{r_id}',\n        'query': query_dict,\n        'links': links,\n        'title': title,\n        'meta': htmlutil.social_meta_tags(page_content),\n    }\n\n    if common_params.full_content:\n        r['fullContent'] = page_content\n    if common_params.screenshot:\n        r['screenshotUri'] = f'{host_url}/screenshot/{r_id}'\n\n    # save result to disk\n    cache.dump_result(r, key=r_id, screenshot=screenshot)\n    return r\n\n\ndef allowed_domain(href: str, domain: str) -> bool:\n    # check if the link is from the same domain\n    if href.startswith('http'):\n        # absolute link\n        return tldextract.extract(href).domain == domain\n    return True  # relative link\n\n\ndef group_links(links: Sequence[Mapping]) -> dict:\n    # group links by 'CSS selector', 'color', 'font', 'parent padding', 'parent margin' and 'parent background color' properties\n    links_dict = defaultdict(list)\n    for link in links:\n        links_dict[make_key(link)].append(link)\n    return links_dict\n\n\ndef make_key(link: Mapping) -> str:\n    # make key from 'CSS selector', 'color', 'font', 'parent padding', 'parent margin' and 'parent background color' properties\n    props = link['cssSel'], link['color'], link['font'], link['parentPadding'], link['parentMargin'], link['parentBgColor']\n    s = '|'.join(props)\n    return hashlib.sha1(s.encode()).hexdigest()[:7]  # because 7 chars is enough for uniqueness\n\n\ndef get_stat(links: Sequence[Mapping], text_len_threshold: int, words_threshold: int) -> dict:\n    # Get stat for group of links\n    median_text_len = median([len(x['text']) for x in links])\n    median_words_count = median([len(x['words']) for x in links])\n    approved = median_text_len > text_len_threshold and median_words_count > words_threshold\n    return {\n        'count': len(links),\n        'median_text_len': median_text_len,\n        'median_words_count': median_words_count,\n        'approved': approved,\n    }\n\n\ndef link_fields(link: Mapping) -> dict:\n    return {\n        'url': link['url'],\n        'text': link['text'],\n    }\n"}
{"type": "source_file", "path": "app/internal/util/__init__.py", "content": "from urllib.parse import parse_qs\nfrom starlette.datastructures import URL\n\n\ndef split_url(url: URL) -> (str, str, dict):\n    \"\"\"\n    Split URL into parts. Return host_url, full_path, query_dict.\n    :param url: Starlette URL object\n    :return:\n        host_url - just the host with scheme\n        full_path - just the path with query\n        query_dict - query params as a dict\n    \"\"\"\n    # just the host with scheme\n    host_url = URL(scheme=url.scheme, netloc=url.netloc)\n    # just the path with query\n    full_path = URL(path=url.path, query=url.query)\n    # query params as a dict\n    query_dict = parse_qs(qs=url.query, keep_blank_values=True)\n    return host_url, full_path, query_dict\n"}
{"type": "source_file", "path": "app/settings.py", "content": "import json\nimport os\nfrom functools import cache\nfrom pathlib import Path\n\n\nBASE_DIR = Path(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\nUSER_DATA_DIR = Path(os.environ.get('USER_DATA_DIR', BASE_DIR / 'user_data'))\nUSER_SCRIPTS_DIR = Path(os.environ.get('USER_SCRIPTS_DIR', BASE_DIR / 'user_scripts'))\nAPP_DIR = Path(os.environ.get('APP_DIR', BASE_DIR / 'app'))\nTEMPLATES_DIR = APP_DIR / 'templates'\nSTATIC_DIR = APP_DIR / 'static'\nSCRIPTS_DIR = APP_DIR / 'scripts'\nREADABILITY_SCRIPT = SCRIPTS_DIR / 'readability' / '0.5.0' / 'Readability.js'  # commit hash\nPARSER_SCRIPTS_DIR = SCRIPTS_DIR / 'parser'\nSTEALTH_SCRIPTS_DIR = SCRIPTS_DIR / 'stealth'\nICON_PATH = STATIC_DIR / 'icons' / 'favicon.ico'\n\nBROWSER_CONTEXT_LIMIT = int(os.environ.get('BROWSER_CONTEXT_LIMIT', 20))\nSCREENSHOT_TYPE = os.environ.get('SCREENSHOT_TYPE', 'jpeg')  # jpeg, png\nSCREENSHOT_QUALITY = int(os.environ.get('SCREENSHOT_QUALITY', 80))  # 0-100\n\nassert BROWSER_CONTEXT_LIMIT > 0, 'BROWSER_CONTEXT_LIMIT must be greater than 0'\nassert SCREENSHOT_TYPE in ('jpeg', 'png'), 'SCREENSHOT_TYPE must be jpeg or png'\nassert 0 <= SCREENSHOT_QUALITY <= 100, 'SCREENSHOT_QUALITY must be between 0 and 100'\n\n\n@cache\ndef load_device_registry():\n    # https://playwright.dev/python/docs/emulation#devices\n    # https://github.com/microsoft/playwright/blob/main/packages/playwright-core/src/server/deviceDescriptorsSource.json\n    src_file = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'internal', 'deviceDescriptorsSource.json')\n    with open(src_file, encoding='utf-8') as f:\n        return json.load(f)\n\n\nDEVICE_REGISTRY = load_device_registry()\n"}
{"type": "source_file", "path": "app/dependencies.py", "content": "import asyncio\nimport contextlib\nimport os\nfrom typing import TypedDict\n\nfrom fastapi import FastAPI\nfrom playwright.async_api import async_playwright, Browser\n\nfrom settings import USER_SCRIPTS_DIR, BROWSER_CONTEXT_LIMIT\n\n\nclass State(TypedDict):\n    # https://playwright.dev/python/docs/api/class-browsertype\n    browser: Browser\n    semaphore: asyncio.Semaphore\n\n\n@contextlib.asynccontextmanager\nasync def lifespan(_: FastAPI):\n    os.makedirs(USER_SCRIPTS_DIR, exist_ok=True)\n    semaphore = asyncio.Semaphore(BROWSER_CONTEXT_LIMIT)\n\n    async with async_playwright() as playwright:\n        firefox = playwright.firefox\n        browser = await firefox.launch(headless=True)\n        yield State(browser=browser, semaphore=semaphore)\n"}
{"type": "source_file", "path": "app/internal/errors.py", "content": "from typing import Any\n\nfrom fastapi import status\nfrom fastapi.exceptions import HTTPException, RequestValidationError\n\n\nclass ArticleParsingError(HTTPException):\n\n    def __init__(self, url: str, msg: str):\n        obj = {\n            'type': 'article_parsing',\n            'loc': ('readability.js',),\n            'msg': msg,\n            'input': url,\n        }\n        super().__init__(\n            status_code=status.HTTP_400_BAD_REQUEST,\n            detail=[obj]\n        )\n\n\nclass LinksParsingError(HTTPException):\n\n    def __init__(self, url: str, msg: str):\n        obj = {\n            'type': 'links_parsing',\n            'loc': ('links.js',),\n            'msg': msg,\n            'input': url,\n        }\n        super().__init__(\n            status_code=status.HTTP_400_BAD_REQUEST,\n            detail=[obj]\n        )\n\n\nclass QueryParsingError(RequestValidationError):\n\n    def __init__(self, field: str, msg: str, value: Any):\n        obj = {\n            'type': f'{field}_parsing',\n            'loc': ('query', field),\n            'msg': msg,\n            'input': value,\n        }\n        super().__init__([obj])\n"}
{"type": "source_file", "path": "app/routers/query_params.py", "content": "# pylint: disable=too-many-arguments,too-many-instance-attributes,too-many-locals\nfrom enum import Enum\nfrom email.errors import MessageParseError\nfrom email.parser import Parser as HeaderParser\nfrom typing import Annotated\nfrom urllib.parse import urlparse\n\nimport validators\n\nfrom fastapi import Query\n\nfrom internal.errors import QueryParsingError\nfrom settings import USER_SCRIPTS_DIR, DEVICE_REGISTRY\n\n\nclass WaitUntilEnum(str, Enum):\n    LOAD = 'load'\n    DOMCONTENTLOADED = 'domcontentloaded'\n    NETWORKIDLE = 'networkidle'\n    COMMIT = 'commit'\n\n\nclass URLParam:\n    def __init__(\n        self,\n        url: Annotated[\n            str,\n            Query(\n                description='Page URL.<br><br>',\n            )\n        ],\n    ):\n        if validators.url(url) is not True:\n            raise QueryParsingError('url', 'Invalid URL', url)\n        self.url = url\n\n\nclass CommonQueryParams:\n    \"\"\"Common scraper settings\"\"\"\n    def __init__(\n        self,\n        cache: Annotated[\n            bool,\n            Query(\n                description=(\n                    'All results of the parsing process will be cached in the `user_data` directory.<br>'\n                    'Cache can be disabled by setting the cache option to false. '\n                    'In this case, the page will be fetched and parsed every time.<br>'\n                    'Cache is enabled by default.<br><br>'\n                ),\n            ),\n        ] = True,\n        full_content: Annotated[\n            bool,\n            Query(\n                alias='full-content',\n                description=(\n                    'If this option is set to true, the result will have the full HTML contents of the page '\n                    '(`fullContent` field in the result).<br><br>'\n                )\n            ),\n        ] = False,\n        stealth: Annotated[\n            bool,\n            Query(\n                description=(\n                    'Stealth mode allows you to bypass anti-scraping techniques. It is disabled by default.<br>Mostly taken from '\n                    '[https://github.com/berstend/puppeteer-extra/tree/master/packages/puppeteer-extra-plugin-stealth/evasions]'\n                    '(https://github.com/berstend/puppeteer-extra/tree/master/packages/puppeteer-extra-plugin-stealth/evasions)<br><br>'\n                )\n            ),\n        ] = False,\n        screenshot: Annotated[\n            bool,\n            Query(\n                description=(\n                    'If this option is set to true, the result will have the link to the screenshot of the page '\n                    '(`screenshot` field in the result).<br>'\n                    'Important implementation details: Initially, Scrapper attempts to take a screenshot of the entire scrollable page.<br>'\n                    'If it fails because the image is too large, it will only capture the currently visible viewport.<br><br>'\n                )\n            ),\n        ] = False,\n        user_scripts: Annotated[\n            str | None,\n            Query(\n                alias='user-scripts',\n                description=(\n                    'To use your JavaScript scripts on a webpage, put your script files into the `user_scripts` directory. '\n                    'Then, list the scripts you need in the `user-scripts` parameter, separating them with commas. '\n                    'These scripts will run after the page loads but before the article parser starts. This means you can use these scripts'\n                    ' to do things like remove ad blocks or automatically click the cookie acceptance button. '\n                    'Keep in mind, script names cannot include commas, as they are used for separation.<br>'\n                    'For example, you might pass `remove-ads.js, click-cookie-accept-button.js`.<br><br>'\n                    'If you plan to run asynchronous long-running scripts, check `user-scripts-timeout` parameter.'\n                ),\n            ),\n        ] = None,\n        user_scripts_timeout: Annotated[\n            int,\n            Query(\n                alias='user-scripts-timeout',\n                description='Waits for the given timeout in milliseconds after injecting users scripts.<br>'\n                            'For example if you want to navigate through page to specific content, set a longer period (higher value).<br>'\n                            'The default value is 0, which means no sleep.<br><br>',\n                ge=0,\n            ),\n        ] = 0,\n    ):\n        self.cache = cache\n        self.full_content = full_content\n        self.stealth = stealth\n        self.screenshot = screenshot\n        self.user_scripts = None\n        self.user_scripts_timeout = user_scripts_timeout\n\n        if user_scripts:\n            user_scripts = list(filter(None, map(str.strip, user_scripts.split(','))))\n            if user_scripts:\n                # check if all files exist\n                for script in user_scripts:\n                    if not (USER_SCRIPTS_DIR / script).exists():\n                        raise QueryParsingError('user_scripts', 'User script not found', script)\n                self.user_scripts = user_scripts\n\n\nclass BrowserQueryParams:\n    \"\"\"Browser settings\"\"\"\n    def __init__(\n        self,\n        incognito: Annotated[\n            bool,\n            Query(\n                description=\"Allows creating `incognito` browser contexts. \"\n                            \"Incognito browser contexts don't write any browsing data to disk.<br><br>\",\n            ),\n        ] = True,\n        timeout: Annotated[\n            int,\n            Query(\n                description='Maximum operation time to navigate to the page in milliseconds; defaults to 60000 (60 seconds).<br>'\n                            'Pass 0 to disable the timeout.<br><br>',\n                ge=0,\n            ),\n        ] = 60000,\n        wait_until: Annotated[\n            WaitUntilEnum,\n            Query(\n                alias='wait-until',\n                description=(\n                    'When to consider navigation succeeded, defaults to `domcontentloaded`. Events can be either:<br>'\n                    '`load` - consider operation to be finished when the `load` event is fired.<br>'\n                    '`domcontentloaded` - consider operation to be finished when the DOMContentLoaded event is fired.<br>'\n                    '`networkidle` -  consider operation to be finished when there are no network connections for at least 500 ms.<br>'\n                    '`commit` - consider operation to be finished when network response is received and the document started loading.<br>'\n                    'See for details: [https://playwright.dev/python/docs/navigations]'\n                    '(https://playwright.dev/python/docs/navigations#navigation-lifecycle)<br><br>'\n                )\n            ),\n        ] = WaitUntilEnum.DOMCONTENTLOADED,\n        sleep: Annotated[\n            int,\n            Query(\n                description='Waits for the given timeout in milliseconds before parsing the article, and after the page has loaded.<br>'\n                            'In many cases, a sleep timeout is not necessary. However, for some websites, it can be quite useful.<br>'\n                            'Other waiting mechanisms, such as waiting for selector visibility, are not currently supported.<br>'\n                            'The default value is 0, which means no sleep.<br><br>',\n                ge=0,\n            ),\n        ] = 0,\n        resource: Annotated[\n            str | None,\n            Query(\n                description='List of resource types allowed to be loaded on the page.<br>'\n                            'All other resources will not be allowed, and their network requests will be aborted.<br>'\n                            'The following resource types are supported:<br>'\n                            '`document`<br>'\n                            '`stylesheet`<br>'\n                            '`image`<br>'\n                            '`media`<br>'\n                            '`font`<br>'\n                            '`script`<br>'\n                            '`texttrack`<br>'\n                            '`xhr`<br>'\n                            '`fetch`<br>'\n                            '`eventsource`<br>'\n                            '`websocket`<br>'\n                            '`manifest`<br>'\n                            '`other`<br><br>'\n                            'By default, all resource types are allowed.',\n            ),\n        ] = None,\n        viewport_width: Annotated[\n            int | None,\n            Query(\n                alias='viewport-width',\n                description='The viewport width in pixels. '\n                            \"It's better to use the `device` parameter instead of specifying it explicitly.<br><br>\",\n                ge=1,\n            ),\n        ] = None,\n        viewport_height: Annotated[\n            int | None,\n            Query(\n                alias='viewport-height',\n                description='The viewport height in pixels. '\n                            \"It's better to use the `device` parameter instead of specifying it explicitly.<br><br>\",\n                ge=1,\n            ),\n        ] = None,\n        screen_width: Annotated[\n            int | None,\n            Query(\n                alias='screen-width',\n                description='Emulates consistent window screen size available inside web page via window.screen. '\n                            'Is only used when the viewport is set. The page width in pixels.<br><br>',\n                ge=1,\n            ),\n        ] = None,\n        screen_height: Annotated[\n            int | None,\n            Query(\n                alias='screen-height',\n                description='Emulates consistent window screen size available inside web page via window.screen. '\n                            'Is only used when the viewport is set. The page height in pixels.<br><br>',\n                ge=1,\n            ),\n        ] = None,\n        device: Annotated[\n            str,\n            Query(\n                description=(\n                    'Simulates browser behavior for a specific device, such as user agent, screen size, viewport, '\n                    'and whether it has touch enabled.<br>Individual parameters like `user-agent`, `viewport-width`, and `viewport-height` '\n                    'can also be used; in such cases, they will override the `device` settings.<br>'\n                    'List of [available devices]'\n                    '(https://github.com/microsoft/playwright/blob/main/packages/playwright-core/src/server/deviceDescriptorsSource.json).'\n                    '<br><br>'\n                ),\n            ),\n        ] = 'iPhone 12',\n        scroll_down: Annotated[\n            int,\n            Query(\n                alias='scroll-down',\n                description=(\n                    'Scroll down the page by a specified number of pixels.<br>'\n                    'This is particularly useful when dealing with lazy-loading pages '\n                    '(pages that are loaded only as you scroll down).<br>'\n                    'This parameter is used in conjunction with the `sleep` parameter.<br>'\n                    \"Make sure to set a positive value for the `sleep` parameter, otherwise, the scroll function won't work.<br><br>\"\n                ),\n                ge=0,\n            ),\n        ] = 0,\n        ignore_https_errors: Annotated[\n            bool,\n            Query(\n                alias='ignore-https-errors',\n                description='Whether to ignore HTTPS errors when sending network requests.<br>'\n                            'The default setting is to ignore HTTPS errors.<br><br>',\n            ),\n        ] = True,\n        user_agent: Annotated[\n            str | None,\n            Query(\n                alias='user-agent',\n                description='Specify user agent to emulate.<br>'\n                            \"It's better to use the `device` parameter instead of specifying it explicitly.<br><br>\",\n            ),\n        ] = None,\n        locale: Annotated[\n            str | None,\n            Query(\n                description='Specify user locale, for example en-GB, de-DE, etc.<br>'\n                            'Locale will affect navigator.language value, '\n                            'Accept-Language request header value as well as number and date formatting rules.',\n            ),\n        ] = None,\n        timezone: Annotated[\n            str | None,\n            Query(\n                description=\"Changes the timezone of the context. See ICU's metaZones.txt for a list of supported timezone IDs.\",\n            ),\n        ] = None,\n        http_credentials: Annotated[\n            str | None,\n            Query(\n                alias='http-credentials',\n                description='Credentials for HTTP authentication '\n                            '(string containing username and password separated by a colon, e.g. `username:password`).',\n            ),\n        ] = None,\n        extra_http_headers: Annotated[\n            list | None,\n            Query(\n                alias='extra-http-headers',\n                description='Contains additional HTTP headers to be sent with every request. '\n                            'Example: `X-API-Key:123456;X-Auth-Token:abcdef`.',\n            ),\n        ] = None,\n    ):\n        self.incognito = incognito\n        self.timeout = timeout\n        self.wait_until = wait_until\n        self.sleep = sleep\n        self.resource = None\n        self.viewport_width = viewport_width\n        self.viewport_height = viewport_height\n        self.screen_width = screen_width\n        self.screen_height = screen_height\n        self.device = device\n        self.scroll_down = scroll_down\n        self.ignore_https_errors = ignore_https_errors\n        self.user_agent = user_agent\n        self.locale = locale\n        self.timezone = timezone\n        self.http_credentials = None\n        self.extra_http_headers = None\n\n        if resource:\n            resource = list(filter(None, map(str.strip, resource.split(','))))\n            if resource:\n                self.resource = resource\n\n        if device not in DEVICE_REGISTRY:\n            raise QueryParsingError('device', 'Device not found', device)\n\n        if http_credentials:\n            fake_url = f'http://{http_credentials}@localhost'\n            try:\n                p = urlparse(fake_url)\n                self.http_credentials = {\n                    'username': p.username or '',  # expected only string, not None\n                    'password': p.password or '',  # same\n                }\n            except ValueError as exc:  # pragma: no cover\n                raise QueryParsingError('http_credentials', 'Invalid HTTP credentials', http_credentials) from exc\n\n        if extra_http_headers:\n            try:\n                headers = HeaderParser().parsestr('\\r\\n'.join(extra_http_headers))\n                self.extra_http_headers = dict(headers)\n                # check if headers were parsed correctly\n                if not self.extra_http_headers:\n                    raise MessageParseError()\n            except MessageParseError as exc:\n                raise QueryParsingError('extra_http_headers', 'Invalid HTTP header', extra_http_headers) from exc\n\n\nclass ProxyQueryParams:\n    \"\"\"Network proxy settings\"\"\"\n    def __init__(\n        self,\n        proxy_server: Annotated[\n            str | None,\n            Query(\n                alias='proxy-server',\n                description='Proxy to be used for all requests.<br>'\n                            'HTTP and SOCKS proxies are supported, for example http://myproxy.com:3128 or socks5://myproxy.com:3128.<br>'\n                            'Short form myproxy.com:3128 is considered an HTTP proxy.',\n            ),\n        ] = None,\n        proxy_bypass: Annotated[\n            str | None,\n            Query(\n                alias='proxy-bypass',\n                description='Optional comma-separated domains to bypass proxy, for example `.com, chromium.org, .domain.com`.',\n            ),\n        ] = None,\n        proxy_username: Annotated[\n            str | None,\n            Query(\n                alias='proxy-username',\n                description='Optional username to use if HTTP proxy requires authentication.',\n            ),\n        ] = None,\n        proxy_password: Annotated[\n            str | None,\n            Query(\n                alias='proxy-password',\n                description='Optional password to use if HTTP proxy requires authentication.',\n            ),\n        ] = None,\n    ):\n        self.proxy_server = proxy_server\n        self.proxy_bypass = proxy_bypass\n        self.proxy_username = proxy_username\n        self.proxy_password = proxy_password\n\n\nclass ReadabilityQueryParams:\n    \"\"\"Readability settings\"\"\"\n    def __init__(\n        self,\n        max_elems_to_parse: Annotated[\n            int,\n            Query(\n                alias='max-elems-to-parse',\n                description='The maximum number of elements to parse. The default value is 0, which means no limit.<br><br>',\n                ge=0,\n            ),\n        ] = 0,\n        nb_top_candidates: Annotated[\n            int,\n            Query(\n                alias='nb-top-candidates',\n                description='The number of top candidates to consider when analysing how tight the competition is among candidates.<br>'\n                            'The default value is 5.<br><br>',\n                ge=1,\n            ),\n        ] = 5,\n        char_threshold: Annotated[\n            int,\n            Query(\n                alias='char-threshold',\n                description='The number of chars an article must have in order to return a result.<br>'\n                            'The default value is 500.<br><br>',\n                ge=1,\n            ),\n        ] = 500,\n    ):\n        self.max_elems_to_parse = max_elems_to_parse\n        self.nb_top_candidates = nb_top_candidates\n        self.char_threshold = char_threshold\n\n\nclass LinkParserQueryParams:\n    \"\"\"Link parser settings\"\"\"\n    def __init__(\n        self,\n        text_len_threshold: Annotated[\n            int,\n            Query(\n                alias='text-len-threshold',\n                description='The median (middle value) of the link text length in characters. The default value is 40 characters.<br>'\n                            'Hyperlinks must adhere to this criterion to be included in the results.<br>'\n                            'However, this criterion is not a strict threshold value, and some links may ignore it.<br><br>',\n                ge=0,\n            ),\n        ] = 40,\n        words_threshold: Annotated[\n            int,\n            Query(\n                alias='words-threshold',\n                description='The median (middle value) of the number of words in the link text. The default value is 3 words.<br>'\n                            'Hyperlinks must adhere to this criterion to be included in the results.<br>'\n                            'However, this criterion is not a strict threshold value, and some links may ignore it.<br><br>',\n                ge=0,\n            ),\n        ] = 3,\n    ):\n        self.text_len_threshold = text_len_threshold\n        self.words_threshold = words_threshold\n"}
{"type": "source_file", "path": "app/routers/results.py", "content": "from typing import Annotated\n\nfrom fastapi import APIRouter, Path, HTTPException, status\nfrom fastapi.requests import Request\nfrom fastapi.responses import HTMLResponse, FileResponse\nfrom fastapi.templating import Jinja2Templates\n\nfrom internal import cache\nfrom settings import TEMPLATES_DIR, SCREENSHOT_TYPE\nfrom version import revision\n\n\nrouter = APIRouter(tags=['results'])\ntemplates = Jinja2Templates(directory=TEMPLATES_DIR)\n\n\n@router.get('/view/{r_id}', response_class=HTMLResponse, include_in_schema=False)\nasync def result_html(\n    request: Request,\n    r_id: Annotated[str, Path(title='Result ID', description='Unique result ID')],\n):\n    data = cache.load_result(key=r_id)\n    if not data:\n        raise HTTPException(\n            status_code=status.HTTP_404_NOT_FOUND,\n            detail=f'Not found result with id: {r_id}'\n        )\n\n    context = {'request': request, 'data': data, 'revision': revision}\n    return templates.TemplateResponse('view.html', context=context)\n\n\n@router.get('/result/{r_id}', include_in_schema=False)\nasync def result_json(\n    r_id: Annotated[str, Path(title='Result ID', description='Unique result ID')],\n):\n    data = cache.load_result(key=r_id)\n    if not data:\n        raise HTTPException(\n            status_code=status.HTTP_404_NOT_FOUND,\n            detail=f'Not found result with id: {r_id}'\n        )\n    return data\n\n\n@router.get('/screenshot/{r_id}', response_class=FileResponse, include_in_schema=False)\nasync def result_screenshot(\n    r_id: Annotated[str, Path(title='Result ID', description='Unique result ID')],\n):\n    path = cache.screenshot_location(r_id)\n    if not path.exists():\n        raise HTTPException(\n            status_code=status.HTTP_404_NOT_FOUND,\n            detail=f'Not found result with id: {r_id}'\n        )\n    return FileResponse(path, media_type=f'image/{SCREENSHOT_TYPE}')\n"}
{"type": "source_file", "path": "app/routers/article.py", "content": "import asyncio\nimport datetime\nfrom typing import Annotated\n\nimport tldextract\n\nfrom fastapi import APIRouter, Query, Depends\nfrom fastapi.requests import Request\nfrom pydantic import BaseModel\nfrom playwright.async_api import Browser\n\nfrom settings import READABILITY_SCRIPT, PARSER_SCRIPTS_DIR\nfrom internal import cache\nfrom internal.browser import (\n    new_context,\n    page_processing,\n    get_screenshot,\n)\nfrom internal.util import htmlutil, split_url\nfrom internal.errors import ArticleParsingError\nfrom .query_params import (\n    URLParam,\n    CommonQueryParams,\n    BrowserQueryParams,\n    ProxyQueryParams,\n    ReadabilityQueryParams,\n)\n\n\nrouter = APIRouter(prefix='/api/article', tags=['article'])\n\n\nclass Article(BaseModel):\n    byline: Annotated[str | None, Query(description='author metadata')]\n    content: Annotated[str | None, Query(description='HTML string of processed article content')]\n    dir: Annotated[str | None, Query(description='content direction')]\n    excerpt: Annotated[str | None, Query(description='article description, or short excerpt from the content')]\n    id: Annotated[str, Query(description='unique result ID')]\n    url: Annotated[str, Query(description='page URL after redirects, may not match the query URL')]\n    domain: Annotated[str, Query(description=\"page's registered domain\")]\n    lang: Annotated[str | None, Query(description='content language')]\n    length: Annotated[int | None, Query(description='length of extracted article, in characters')]\n    date: Annotated[str, Query(description='date of extracted article in ISO 8601 format')]\n    query: Annotated[dict, Query(description='request parameters')]\n    meta: Annotated[dict, Query(description='social meta tags (open graph, twitter)')]\n    resultUri: Annotated[str, Query(description='URL of the current result, the data here is always taken from cache')]\n    fullContent: Annotated[str | None, Query(description='full HTML contents of the page')] = None\n    screenshotUri: Annotated[str | None, Query(description='URL of the screenshot of the page')] = None\n    siteName: Annotated[str | None, Query(description='name of the site')]\n    textContent: Annotated[str | None, Query(description='text content of the article, with all the HTML tags removed')]\n    title: Annotated[str | None, Query(description='article title')]\n    publishedTime: Annotated[str | None, Query(description='article publication time')]\n\n\n@router.get('', summary='Parse article from the given URL', response_model=Article)\nasync def parse_article(\n    request: Request,\n    url: Annotated[URLParam, Depends()],\n    common_params: Annotated[CommonQueryParams, Depends()],\n    browser_params: Annotated[BrowserQueryParams, Depends()],\n    proxy_params: Annotated[ProxyQueryParams, Depends()],\n    readability_params: Annotated[ReadabilityQueryParams, Depends()],\n) -> dict:\n    \"\"\"\n    Parse article from the given URL.<br><br>\n    The page from the URL should contain the text of the article that needs to be extracted.\n    \"\"\"\n    # pylint: disable=duplicate-code\n    # split URL into parts: host with scheme, path with query, query params as a dict\n    host_url, full_path, query_dict = split_url(request.url)\n\n    # get cache data if exists\n    r_id = cache.make_key(full_path)  # unique result ID\n    if common_params.cache:\n        data = cache.load_result(key=r_id)\n        if data:\n            return data\n\n    browser: Browser = request.state.browser\n    semaphore: asyncio.Semaphore = request.state.semaphore\n\n    # create a new browser context\n    async with semaphore:\n        async with new_context(browser, browser_params, proxy_params) as context:\n            page = await context.new_page()\n            await page_processing(\n                page=page,\n                url=url.url,\n                params=common_params,\n                browser_params=browser_params,\n                init_scripts=[READABILITY_SCRIPT],\n            )\n            page_content = await page.content()\n            screenshot = await get_screenshot(page) if common_params.screenshot else None\n            page_url = page.url\n\n            # evaluating JavaScript: parse DOM and extract article content\n            parser_args = {\n                # Readability options:\n                'maxElemsToParse': readability_params.max_elems_to_parse,\n                'nbTopCandidates': readability_params.nb_top_candidates,\n                'charThreshold': readability_params.char_threshold,\n            }\n            with open(PARSER_SCRIPTS_DIR / 'article.js', encoding='utf-8') as f:\n                article = await page.evaluate(f.read() % parser_args)\n\n    if article is None:\n        raise ArticleParsingError(page_url, \"The page doesn't contain any articles.\")  # pragma: no cover\n\n    # parser error: article is not extracted, result has 'err' field\n    if 'err' in article:\n        raise ArticleParsingError(page_url, article['err'])  # pragma: no cover\n\n    # set common fields\n    article['id'] = r_id\n    article['url'] = page_url\n    article['domain'] = tldextract.extract(page_url).registered_domain\n    article['date'] = datetime.datetime.utcnow().isoformat()  # ISO 8601 format\n    article['resultUri'] = f'{host_url}/result/{r_id}'\n    article['query'] = query_dict\n    article['meta'] = htmlutil.social_meta_tags(page_content)\n\n    if common_params.full_content:\n        article['fullContent'] = page_content\n    if common_params.screenshot:\n        article['screenshotUri'] = f'{host_url}/screenshot/{r_id}'\n\n    if 'title' in article and 'content' in article:\n        article['content'] = htmlutil.improve_content(\n            title=article['title'],\n            content=article['content'],\n        )\n\n    if 'textContent' in article:\n        article['textContent'] = htmlutil.improve_text_content(article['textContent'])\n        article['length'] = len(article['textContent']) - article['textContent'].count('\\n')\n\n    # save result to disk\n    cache.dump_result(article, key=r_id, screenshot=screenshot)\n    return article\n"}
{"type": "source_file", "path": "app/__init__.py", "content": ""}
{"type": "source_file", "path": "app/routers/misc.py", "content": "import datetime\nfrom typing import Annotated\n\nfrom fastapi import APIRouter, Query\nfrom fastapi.requests import Request\nfrom pydantic import BaseModel\nfrom playwright.async_api import Browser\n\nfrom version import revision\n\n\nrouter = APIRouter(tags=['misc'])\n\n\nclass PingData(BaseModel):\n    browserType: Annotated[str, Query(description='the browser type (chromium, firefox or webkit)')]\n    browserVersion: Annotated[str, Query(description='the browser version')]\n    contexts: Annotated[int, Query(description='number of active browser contexts')]\n    isConnected: Annotated[bool, Query(description='indicates that the browser is connected')]\n    now: Annotated[datetime.datetime, Query(description='UTC time now')]\n    revision: Annotated[str, Query(description='the scrapper revision')]\n\n\n@router.get('/ping', summary='Ping the Scrapper', response_model=PingData)\nasync def ping(request: Request) -> dict:\n    \"\"\"\n    The ping endpoint checks if the Scrapper is running, both from Docker and externally.\n    \"\"\"\n    browser: Browser = request.state.browser\n    return {\n        'browserType': browser.browser_type.name,\n        'browserVersion': browser.version,\n        'contexts': len(browser.contexts),\n        'isConnected': browser.is_connected(),\n        'now': datetime.datetime.utcnow(),\n        'revision': revision,\n    }\n"}
{"type": "source_file", "path": "app/internal/browser.py", "content": "import contextlib\nimport copy\nfrom collections.abc import Sequence\n\nfrom playwright.async_api import Browser, BrowserContext, Page, Route\nfrom playwright.async_api import Error as PlaywrightError\nfrom routers.query_params import CommonQueryParams, BrowserQueryParams, ProxyQueryParams\n\nfrom settings import (\n    USER_DATA_DIR,\n    USER_SCRIPTS_DIR,\n    STEALTH_SCRIPTS_DIR,\n    SCREENSHOT_TYPE,\n    SCREENSHOT_QUALITY,\n    DEVICE_REGISTRY,\n)\n\n\ndef get_device(device: str) -> dict:\n    return copy.deepcopy(DEVICE_REGISTRY[device])\n\n\n@contextlib.asynccontextmanager\nasync def new_context(\n    browser: Browser,\n    params: BrowserQueryParams,\n    proxy: ProxyQueryParams,\n) -> BrowserContext:\n    # https://playwright.dev/python/docs/emulation\n    options = get_device(params.device)\n\n    # PlaywrightError: options.isMobile is not supported in Firefox\n    del options['is_mobile']\n    del options['default_browser_type']\n\n    options |= {\n        'bypass_csp': True,\n        'ignore_https_errors': params.ignore_https_errors,\n        'locale': params.locale,\n        'timezone_id': params.timezone,\n        'http_credentials': params.http_credentials,\n        'extra_http_headers': params.extra_http_headers,\n    }\n\n    # viewport and screen settings:\n    if params.viewport_width:\n        options['viewport']['width'] = params.viewport_width\n    if params.viewport_height:\n        options['viewport']['height'] = params.viewport_height\n    if params.screen_width:\n        options['screen']['width'] = params.screen_width\n    if params.screen_height:\n        options['screen']['height'] = params.screen_height\n\n    # user agent settings:\n    if params.user_agent:\n        options['user_agent'] = params.user_agent\n\n    # proxy settings:\n    if proxy.proxy_server:\n        options['proxy'] = {\n            'server': proxy.proxy_server,\n        }\n        if proxy.proxy_username:\n            options['proxy']['username'] = proxy.proxy_username\n        if proxy.proxy_password:\n            options['proxy']['password'] = proxy.proxy_password\n        if proxy.proxy_bypass:\n            options['proxy']['bypass'] = proxy.proxy_bypass\n\n    # https://playwright.dev/python/docs/api/class-browser#browser-new-context\n    if params.incognito:\n        # create a new incognito browser context\n        # (more efficient way, because it doesn't create a new browser instance)\n        context = await browser.new_context(**options)\n    else:\n        # create a persistent browser context\n        # (less efficient way, because it creates a new browser instance)\n        context = await browser.browser_type.launch_persistent_context(\n            headless=True,\n            user_data_dir=USER_DATA_DIR,\n            **options,\n        )\n    try:\n        yield context\n    finally:\n        # context should always be closed at the end\n        await context.close()\n\n\nasync def page_processing(\n    page: Page,\n    url: str,\n    params: CommonQueryParams,\n    browser_params: BrowserQueryParams,\n    init_scripts: Sequence[str] = None,\n):\n    # add stealth scripts for bypassing anti-scraping mechanisms\n    if params.stealth:\n        await use_stealth_mode(page)\n\n    # add extra init scripts\n    if init_scripts:\n        for path in init_scripts:\n            await page.add_init_script(path=path)\n\n    # block by resource types\n    if browser_params.resource:\n        handler = resource_blocker(whitelist=browser_params.resource)\n        await page.route('**/*', handler)\n\n    # navigate to the given url\n    # noinspection PyTypeChecker\n    await page.goto(url, timeout=browser_params.timeout, wait_until=browser_params.wait_until)\n\n    # wait for the given timeout in milliseconds and scroll down the page\n    n = 10\n    if browser_params.sleep:\n        for _ in range(n):\n            # scroll down the page by 1/n of the given scroll_down value\n            if browser_params.scroll_down:\n                await page.mouse.wheel(0, browser_params.scroll_down / n)\n            # sleep for 1/n of the given sleep value\n            await page.wait_for_timeout(browser_params.sleep / n)\n\n        # scroll to the top of the page for the screenshot to be in the correct position\n        if browser_params.scroll_down:\n            await page.mouse.wheel(0, 0)\n\n    # add user scripts for DOM manipulation\n    if params.user_scripts:\n        for script in params.user_scripts:\n            await page.add_script_tag(path=USER_SCRIPTS_DIR / script)\n\n    # wait for the given timeout in milliseconds after user scripts were injected.\n    if params.user_scripts_timeout:\n        await page.wait_for_timeout(params.user_scripts_timeout)\n\n\ndef resource_blocker(whitelist: Sequence[str]):  # list of resource types to allow\n    async def block(route: Route):\n        if route.request.resource_type in whitelist:\n            await route.continue_()\n        else:\n            await route.abort()\n    return block\n\n\nasync def use_stealth_mode(page: Page):\n    for script in STEALTH_SCRIPTS_DIR.glob('*.js'):\n        await page.add_init_script(path=script)\n\n\nasync def get_screenshot(page: Page):\n    # First try to take a screenshot of the full scrollable page,\n    # if it fails, take a screenshot of the currently visible viewport.\n    kwargs = {'type': SCREENSHOT_TYPE, 'quality': SCREENSHOT_QUALITY}\n    try:\n        # try to take a full page screenshot\n        return await page.screenshot(full_page=True, **kwargs)\n    except PlaywrightError as exc:\n        # if the page is too large, take a screenshot of the currently visible viewport\n        if 'Cannot take screenshot larger than ' in exc.message:\n            return await page.screenshot(full_page=False, **kwargs)\n        raise exc  # pragma: no cover\n"}
{"type": "source_file", "path": "app/routers/any_page.py", "content": "import asyncio\nimport datetime\nfrom typing import Annotated\n\nimport tldextract\n\nfrom fastapi import APIRouter, Query, Depends\nfrom fastapi.requests import Request\nfrom pydantic import BaseModel\nfrom playwright.async_api import Browser\n\nfrom internal import cache\nfrom internal.browser import (\n    new_context,\n    page_processing,\n    get_screenshot,\n)\nfrom internal.util import htmlutil, split_url\nfrom .query_params import (\n    URLParam,\n    CommonQueryParams,\n    BrowserQueryParams,\n    ProxyQueryParams,\n)\n\n\nrouter = APIRouter(prefix='/api/page', tags=['page'])\n\n\nclass AnyPage(BaseModel):\n    id: Annotated[str, Query(description='unique result ID')]\n    url: Annotated[str, Query(description='page URL after redirects, may not match the query URL')]\n    domain: Annotated[str, Query(description=\"page's registered domain\")]\n    date: Annotated[str, Query(description='date of fetched page in ISO 8601 format')]\n    query: Annotated[dict, Query(description='request parameters')]\n    meta: Annotated[dict, Query(description='social meta tags (open graph, twitter)')]\n    resultUri: Annotated[str, Query(description='URL of the current result, the data here is always taken from cache')]\n    fullContent: Annotated[str | None, Query(description='full HTML contents of the page')] = None\n    screenshotUri: Annotated[str | None, Query(description='URL of the screenshot of the page')] = None\n    title: Annotated[str | None, Query(description=\"page's title\")] = None\n\n\n@router.get('', summary='Get any page from the given URL', response_model=AnyPage)\nasync def get_any_page(\n    request: Request,\n    url: Annotated[URLParam, Depends()],\n    common_params: Annotated[CommonQueryParams, Depends()],\n    browser_params: Annotated[BrowserQueryParams, Depends()],\n    proxy_params: Annotated[ProxyQueryParams, Depends()],\n) -> dict:\n    \"\"\"\n    Get any page from the given URL.<br><br>\n    Page is fetched using Playwright, but no additional processing is done.\n    \"\"\"\n    # pylint: disable=duplicate-code\n    # split URL into parts: host with scheme, path with query, query params as a dict\n    host_url, full_path, query_dict = split_url(request.url)\n\n    # get cache data if exists\n    r_id = cache.make_key(full_path)  # unique result ID\n    if common_params.cache:\n        data = cache.load_result(key=r_id)\n        if data:\n            return data\n\n    browser: Browser = request.state.browser\n    semaphore: asyncio.Semaphore = request.state.semaphore\n\n    # create a new browser context\n    async with semaphore:\n        async with new_context(browser, browser_params, proxy_params) as context:\n            page = await context.new_page()\n            await page_processing(\n                page=page,\n                url=url.url,\n                params=common_params,\n                browser_params=browser_params,\n            )\n            page_content = await page.content()\n            screenshot = await get_screenshot(page) if common_params.screenshot else None\n            page_url = page.url\n            title = await page.title()\n\n    r = {\n        'id': r_id,\n        'url': page_url,\n        'domain': tldextract.extract(page_url).registered_domain,\n        'date': datetime.datetime.utcnow().isoformat(),  # ISO 8601 format\n        'resultUri': f'{host_url}/result/{r_id}',\n        'query': query_dict,\n        'title': title,\n        'meta': htmlutil.social_meta_tags(page_content),\n    }\n\n    if common_params.full_content:\n        r['fullContent'] = page_content\n    if common_params.screenshot:\n        r['screenshotUri'] = f'{host_url}/screenshot/{r_id}'\n\n    # save result to disk\n    cache.dump_result(r, key=r_id, screenshot=screenshot)\n    return r\n"}
{"type": "source_file", "path": "app/internal/__init__.py", "content": ""}
{"type": "source_file", "path": "app/internal/cache.py", "content": "import os\nimport hashlib\nimport json\n\nfrom pathlib import Path\nfrom typing import Any\n\nfrom settings import USER_DATA_DIR, SCREENSHOT_TYPE\n\n\ndef make_key(s: Any) -> str:\n    return hashlib.sha1(str(s).encode()).hexdigest()\n\n\ndef dump_result(data: Any, key: str, screenshot: bytes | None = None) -> None:\n    path = json_location(key)\n\n    # create dir if not exists\n    d = os.path.dirname(path)\n    if not os.path.exists(d):\n        os.makedirs(d, exist_ok=True)\n\n    # save result as json\n    with open(path, mode='w', encoding='utf-8') as f:\n        json.dump(data, f, ensure_ascii=True)\n\n    # save screenshot\n    if screenshot:\n        with open(screenshot_location(key), mode='wb') as f:\n            f.write(screenshot)\n\n\ndef load_result(key: str) -> Any | None:\n    path = json_location(key)\n    if not path.exists():\n        return None\n    with open(path, mode='r', encoding='utf-8') as f:\n        return json.load(f)\n\n\ndef json_location(filename: str) -> Path:\n    return USER_DATA_DIR / '_res' / filename[:2] / filename\n\n\ndef screenshot_location(filename: str) -> Path:\n    return USER_DATA_DIR / '_res' / filename[:2] / (filename + '.' + SCREENSHOT_TYPE)\n"}
{"type": "source_file", "path": "app/routers/__init__.py", "content": ""}
{"type": "source_file", "path": "app/version.py", "content": "revision = ''  # pylint: disable=invalid-name\n"}
