{"repo_info": {"repo_name": "ADAPT", "repo_owner": "jxbbb", "repo_url": "https://github.com/jxbbb/ADAPT"}}
{"type": "test_file", "path": "src/timm/models/layers/test_time_pool.py", "content": "\"\"\" Test Time Pooling (Average-Max Pool)\n\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\n\nimport logging\nfrom torch import nn\nimport torch.nn.functional as F\n\nfrom .adaptive_avgmax_pool import adaptive_avgmax_pool2d\n\n\n_logger = logging.getLogger(__name__)\n\n\nclass TestTimePoolHead(nn.Module):\n    def __init__(self, base, original_pool=7):\n        super(TestTimePoolHead, self).__init__()\n        self.base = base\n        self.original_pool = original_pool\n        base_fc = self.base.get_classifier()\n        if isinstance(base_fc, nn.Conv2d):\n            self.fc = base_fc\n        else:\n            self.fc = nn.Conv2d(\n                self.base.num_features, self.base.num_classes, kernel_size=1, bias=True)\n            self.fc.weight.data.copy_(base_fc.weight.data.view(self.fc.weight.size()))\n            self.fc.bias.data.copy_(base_fc.bias.data.view(self.fc.bias.size()))\n        self.base.reset_classifier(0)  # delete original fc layer\n\n    def forward(self, x):\n        x = self.base.forward_features(x)\n        x = F.avg_pool2d(x, kernel_size=self.original_pool, stride=1)\n        x = self.fc(x)\n        x = adaptive_avgmax_pool2d(x, 1)\n        return x.view(x.size(0), -1)\n\n\ndef apply_test_time_pool(model, config, use_test_size=True):\n    test_time_pool = False\n    if not hasattr(model, 'default_cfg') or not model.default_cfg:\n        return model, False\n    if use_test_size and 'test_input_size' in model.default_cfg:\n        df_input_size = model.default_cfg['test_input_size']\n    else:\n        df_input_size = model.default_cfg['input_size']\n    if config['input_size'][-1] > df_input_size[-1] and config['input_size'][-2] > df_input_size[-2]:\n        _logger.info('Target input size %s > pretrained default %s, using test time pooling' %\n                     (str(config['input_size'][-2:]), str(df_input_size[-2:])))\n        model = TestTimePoolHead(model, original_pool=model.default_cfg['pool_size'])\n        test_time_pool = True\n    return model, test_time_pool\n"}
{"type": "source_file", "path": "src/datasets/data_utils/video_ops.py", "content": "from PIL import Image\nimport io\nimport av\nimport torch\nimport numpy as np\nfrom src.datasets.data_utils import video_decoder as decoder\nimport code\n\ndef get_video_decoding_kwargs(container, num_frames, target_fps,\n                              num_clips=None, clip_idx=None,\n                              sampling_strategy=\"rand\",\n                              safeguard_duration=False, video_max_pts=None,\n                              start=None, end=None):\n    if num_clips is None:\n        three_clip_names = [\"start\", \"middle\", \"end\"]  # uniformly 3 clips\n        assert sampling_strategy in [\"rand\", \"uniform\"] + three_clip_names\n        if sampling_strategy == \"rand\":\n            decoder_kwargs = dict(\n                container=container,\n                sampling_rate=1,\n                num_frames=num_frames,\n                clip_idx=-1,  # random sampling\n                num_clips=None,  # will not be used when clip_idx is `-1`\n                target_fps=target_fps,\n                start=start, end=end\n            )\n        elif sampling_strategy == \"uniform\":\n            decoder_kwargs = dict(\n                container=container,\n                sampling_rate=1,  # will not be used when clip_idx is `-2`\n                num_frames=num_frames,\n                clip_idx=-2,  # uniformly sampling from the whole video\n                num_clips=1,  # will not be used when clip_idx is `-2`\n                target_fps=target_fps,  # will not be used when clip_idx is `-2`\n                start=start, end=end\n            )\n        else:  # in three_clip_names\n            decoder_kwargs = dict(\n                container=container,\n                sampling_rate=1,\n                num_frames=num_frames,\n                clip_idx=three_clip_names.index(sampling_strategy),\n                num_clips=3,\n                target_fps=target_fps,\n                start=start, end=end\n            )\n    else:  # multi_clip_ensemble, num_clips and clip_idx are only used here\n        assert clip_idx is not None\n        # sampling_strategy will not be used, as uniform sampling will be used by default.\n        # uniformly sample `num_clips` from the video,\n        # each clip sample num_frames frames at target_fps.\n        decoder_kwargs = dict(\n            container=container,\n            sampling_rate=1,\n            num_frames=num_frames,\n            clip_idx=clip_idx,\n            num_clips=num_clips,\n            target_fps=target_fps,\n            safeguard_duration=safeguard_duration,\n            video_max_pts=video_max_pts,\n            start=start, end=end\n        )\n    return decoder_kwargs\n\ndef extract_frames_from_video_path(\n        video_path, target_fps=3, num_frames=3,\n        multi_thread_decode=False, sampling_strategy=\"rand\",\n        safeguard_duration=False, start=None, end=None):\n    in_mem_bytes_io = video_path\n    try:\n        frames, video_max_pts = extract_frames_from_video_binary(\n            in_mem_bytes_io, target_fps=target_fps, num_frames=num_frames,\n            multi_thread_decode=multi_thread_decode,\n            sampling_strategy=sampling_strategy,\n            safeguard_duration=safeguard_duration,\n            start=start, end=end)\n    except Exception as e:\n        print(f\"Error processing video {video_path}, {e}\")\n        return None, None\n    return frames, video_max_pts\n\n\ndef extract_frames_from_video_binary(\n        in_mem_bytes_io, target_fps=3, num_frames=3, num_clips=None, clip_idx=None,\n        multi_thread_decode=False, sampling_strategy=\"rand\",\n        safeguard_duration=False, video_max_pts=None,\n        start=None, end=None):\n    \"\"\"\n    Args:\n        in_mem_bytes_io: binary from read file object\n            >>> with open(video_path, \"rb\") as f:\n            >>>     input_bytes = f.read()\n            >>> frames = extract_frames_from_video_binary(input_bytes)\n            OR from saved binary in lmdb database\n            >>> env = lmdb.open(\"lmdb_dir\", readonly=True)\n            >>> txn = env.begin()\n            >>> stream = io.BytesIO(txn.get(str(\"key\").encode(\"utf-8\")))\n            >>> frames = extract_frames_from_video_binary(stream)\n            >>> from torchvision.utils import save_image\n            >>> save_image(frames[0], \"path/to/example.jpg\")  # save the extracted frames.\n        target_fps: int, the input video may have different fps, convert it to\n            the target video fps before frame sampling.\n        num_frames: int, number of frames to sample.\n        multi_thread_decode: bool, if True, perform multi-thread decoding.\n        sampling_strategy: str, how to sample frame from video, one of\n            [\"rand\", \"uniform\", \"start\", \"middle\", \"end\"]\n            `rand`: randomly sample consecutive num_frames from the video at target_fps\n                Note it randomly samples a clip containing num_frames at target_fps,\n                not uniformly sample from the whole video\n            `uniform`: uniformly sample num_frames of equal distance from the video, without\n                considering target_fps/sampling_rate, etc. E.g., when sampling_strategy=uniform\n                and num_frames=3, it samples 3 frames at [0, N/2-1, N-1] given a video w/ N frames.\n                However, note that when num_frames=1, it will sample 1 frame at [0].\n                Also note that `target_fps` will not be used under `uniform` sampling strategy.\n            `start`/`middle`/`end`: first uniformly segment the video into 3 clips, then sample\n                num_frames from the corresponding clip at target_fps. E.g., num_frames=3, a video\n                w/ 30 frames, it samples [0, 1, 2]; [9, 10, 11]; [18, 19, 20] for start/middle/end.\n            If the total #frames at target_fps in the video/clip is less than num_frames,\n            there will be some duplicated frames\n        num_clips: int,\n        clip_idx: int\n        safeguard_duration:\n        video_max_pts: resue it to improve efficiency\n    Returns:\n        torch.uint8, (T, C, H, W)\n    \"\"\"\n    try:\n        # Add `metadata_errors=\"ignore\"` to ignore metadata decoding error.\n        # When verified visually, it does not seem to affect the extracted frames.\n        video_container = av.open(in_mem_bytes_io, metadata_errors=\"ignore\")\n    except Exception as e:\n        print(f\"extract_frames_from_video_binary(), Exception in loading video binary: {e}\")\n        return None, None\n\n    if multi_thread_decode:\n        # Enable multiple threads for decoding.\n        video_container.streams.video[0].thread_type = \"AUTO\"\n    try:\n        # (T, H, W, C), channels are RGB\n        # see docs in decoder.decode for usage of these parameters.\n        decoder_kwargs = get_video_decoding_kwargs(\n            container=video_container, num_frames=num_frames,\n            target_fps=target_fps, num_clips=num_clips, clip_idx=clip_idx,\n            sampling_strategy=sampling_strategy,\n            safeguard_duration=safeguard_duration, video_max_pts=video_max_pts, \n            start=start, end=end)\n        frames, video_max_pts = decoder.decode(**decoder_kwargs)\n    except Exception as e:\n        print(f\"extract_frames_from_video_binary(), Exception in decoding video: {e}\")\n        return None, None\n\n    # For some reason in PyAV, the video container may not auto-close, and it could occupy computational resource\n    # check more details at https://pyav.org/docs/stable/overview/caveats.html#garbage-collection\n    video_container.close()\n\n    # (T, H, W, C) -> (T, C, H, W)\n    if frames is not None:\n        frames = frames.permute(0, 3, 1, 2)\n    return frames, video_max_pts"}
{"type": "source_file", "path": "src/modeling/load_swin.py", "content": "import torch\nfrom src.utils.logger import LOGGER as logger\nfrom src.modeling.video_swin.swin_transformer import SwinTransformer3D\nfrom src.modeling.video_swin.config import Config\n\ndef get_swin_model(args):\n    if int(args.img_res) == 384:\n        assert args.vidswin_size == \"large\"\n        config_path = 'src/modeling/video_swin/swin_%s_384_patch244_window81212_kinetics%s_22k.py'%(args.vidswin_size, args.kinetics)\n        model_path = 'models/video_swin_transformer/swin_%s_384_patch244_window81212_kinetics%s_22k.pth'%(args.vidswin_size, args.kinetics)\n    else:\n        # in the case that args.img_res == '224'\n        config_path = 'src/modeling/video_swin/swin_%s_patch244_window877_kinetics%s_22k.py'%(args.vidswin_size, args.kinetics)\n        model_path = 'models/video_swin_transformer/swin_%s_patch244_window877_kinetics%s_22k.pth'%(args.vidswin_size, args.kinetics)\n    if args.pretrained_2d:\n        config_path = 'src/modeling/video_swin/swin_base_patch244_window877_kinetics400_22k.py'\n        model_path = 'models/swin_transformer/swin_base_patch4_window7_224_22k.pth'\n\n    logger.info(f'video swin (config path): {config_path}')\n    if args.pretrained_checkpoint == '':\n        logger.info(f'video swin (model path): {model_path}')\n    cfg = Config.fromfile(config_path)\n    pretrained_path = model_path if args.pretrained_2d else None\n    backbone = SwinTransformer3D(\n                    pretrained=pretrained_path,\n                    pretrained2d=args.pretrained_2d,\n                    patch_size=cfg.model['backbone']['patch_size'],\n                    in_chans=3,\n                    embed_dim=cfg.model['backbone']['embed_dim'],\n                    depths=cfg.model['backbone']['depths'],\n                    num_heads=cfg.model['backbone']['num_heads'],\n                    window_size=cfg.model['backbone']['window_size'],\n                    mlp_ratio=4.,\n                    qkv_bias=True,\n                    qk_scale=None,\n                    drop_rate=0.,\n                    attn_drop_rate=0.,\n                    drop_path_rate=0.2,\n                    norm_layer=torch.nn.LayerNorm,\n                    patch_norm=cfg.model['backbone']['patch_norm'],\n                    frozen_stages=-1,\n                    use_checkpoint=False)\n\n    video_swin = myVideoSwin(args=args, cfg=cfg, backbone=backbone)\n\n    if not args.pretrained_2d:\n        checkpoint_3d = torch.load(model_path, map_location='cpu')\n        video_swin.load_state_dict(checkpoint_3d['state_dict'], strict=False)\n    else:\n        video_swin.backbone.init_weights()\n    return video_swin\n\ndef reload_pretrained_swin(video_swin, args):\n    if not args.reload_pretrained_swin:\n        return video_swin\n    if int(args.img_res) == 384:\n        model_path = './models/video_swin_transformer/swin_%s_384_patch244_window81212_kinetics%s_22k.pth'%(args.vidswin_size, args.kinetics)\n    else:\n        # in the case that args.img_res == '224'\n        model_path = './models/video_swin_transformer/swin_%s_patch244_window877_kinetics%s_22k.pth'%(args.vidswin_size, args.kinetics)\n\n    checkpoint_3d = torch.load(model_path, map_location='cpu')\n    missing, unexpected = video_swin.load_state_dict(checkpoint_3d['state_dict'], strict=False)\n    logger.info(f\"re-loaded video_swin_transformer from {model_path}\")\n\n    logger.info(f\"Missing keys in loaded video_swin_transformerr: {missing}\")\n    logger.info(f\"Unexpected keys in loaded video_swin_transformer: {unexpected}\")\n    return video_swin\n\nclass myVideoSwin(torch.nn.Module):\n    def __init__(self, args, cfg, backbone):\n        super(myVideoSwin, self).__init__()\n        self.backbone = backbone\n        self.use_grid_feature = args.grid_feat\n\n    def forward(self, x):\n        x = self.backbone(x)\n        return x\n"}
{"type": "source_file", "path": "src/layers/bert/__init__.py", "content": "__version__ = \"1.0.0\"\nfrom .tokenization_bert import BertTokenizer, BasicTokenizer, WordpieceTokenizer\nfrom .tokenization_utils import (PreTrainedTokenizer, clean_up_tokenization)\n\nfrom .modeling_bert import (BertConfig, BertModel, BertForPreTraining, BertEncoder,\n                       BertForMaskedLM, BertForNextSentencePrediction,\n                       BertForSequenceClassification, BertForMultipleChoice,\n                       BertForTokenClassification, BertForQuestionAnswering,\n                       BertForImageCaptioning, BertImgForPreTraining,\n                       BertForVLGrounding, BertImgForGroundedPreTraining,\n                       load_tf_weights_in_bert, BERT_PRETRAINED_MODEL_ARCHIVE_MAP,\n                       BERT_PRETRAINED_CONFIG_ARCHIVE_MAP)\nfrom .modeling_utils import (WEIGHTS_NAME, CONFIG_NAME, TF_WEIGHTS_NAME,\n                          PretrainedConfig, PreTrainedModel, prune_layer, Conv1D)\n\nfrom .file_utils import (PYTORCH_PRETRAINED_BERT_CACHE, cached_path)\n"}
{"type": "source_file", "path": "src/datasets/data_utils/video_decoder.py", "content": "#!/usr/bin/env python3\n# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\n# reference https://github.com/facebookresearch/SlowFast/blob/master/slowfast/datasets/decoder.py\n\nimport math\nimport numpy as np\nimport random\nimport torch\nimport time\n\ndef temporal_sampling(frames, start_idx, end_idx, num_samples):\n    \"\"\"\n    Given the start and end frame index, sample num_samples frames between\n    the start and end with equal interval.\n    Args:\n        frames (list(av.video.frame.VideoFrame)): a list of decoded video frames\n        start_idx (int): the index of the start frame.\n        end_idx (int): the index of the end frame.\n        num_samples (int): number of frames to sample.\n    Returns:\n        frames (tersor): a tensor of temporal sampled video frames, dimension is\n            `num clip frames` x `channel` x `height` x `width`.\n    \"\"\"\n    index = torch.linspace(start_idx, end_idx, num_samples)\n    index = torch.clamp(index, 0, len(frames) - 1).long().tolist()\n    frames = [frames[idx] for idx in index]\n    return frames\n\n    # seq = np.arange(0,len(frames)).tolist()\n    # print('seq_len:', len(frames), 'num_samples:', num_samples)\n    # new_index = random.sample(seq,num_samples)\n    # new_index.sort()\n    # frames = [frames[idx] for idx in new_index]\n\n    # if len(frames)<=num_samples:\n    #     index = torch.linspace(start_idx, end_idx, num_samples)\n    #     index = torch.clamp(index, 0, len(frames) - 1).long().tolist()\n    #     frames = [frames[idx] for idx in index]\n    # else:\n    #     seq = np.arange(0,len(frames)).tolist()\n    #     # print('seq_len:', len(frames), 'num_samples:', num_samples)\n    #     new_index = random.sample(seq,num_samples)\n    #     new_index.sort()\n    #     frames = [frames[idx] for idx in new_index]\n\n    # return frames\n\n\n\ndef get_start_end_idx(video_size, clip_size, clip_idx, num_clips):\n    \"\"\"\n    Sample a clip of size clip_size from a video of size video_size and\n    return the indices of the first and last frame of the clip. If clip_idx is\n    -1, the clip is randomly sampled, otherwise uniformly split the video to\n    num_clips clips, and select the start and end index of clip_idx-th video\n    clip.\n    Args:\n        video_size (int): number of overall frames.\n        clip_size (int): size of the clip to sample from the frames.\n            i.e., #frames to get at the original frame rate.\n        clip_idx (int): if clip_idx is -1, perform random jitter sampling. If\n            clip_idx is larger than -1, uniformly split the video to num_clips\n            clips, and select the start and end index of the clip_idx-th video\n            clip.\n        num_clips (int): overall number of clips to uniformly sample from the\n            given video for testing.\n    Returns:\n        start_idx (int): the start frame index.\n        end_idx (int): the end frame index.\n    \"\"\"\n    delta = max(video_size - clip_size, 0)\n    if clip_idx == -1:\n        # Random temporal sampling.\n        start_idx = random.uniform(0, delta)\n    else:\n        # Uniformly sample the clip with the given index.\n        start_idx = delta * clip_idx / num_clips\n    end_idx = start_idx + clip_size - 1\n    return start_idx, end_idx\n\n    # delta = max(video_size - clip_size, 0)\n    # if clip_idx == -1:\n    #     # Random temporal sampling.\n    #     start_idx = random.uniform(0, delta)\n    # else:\n    #     # Uniformly sample the clip with the given index.\n    #     start_idx = delta * clip_idx / num_clips\n    # if clip_idx == -1 and clip_size>int(video_size*0.7):\n    #     end_idx = start_idx + random.uniform(0, clip_size-start_idx) - 1\n    # else:\n    #     end_idx = start_idx + clip_size - 1\n    # return start_idx, end_idx\n\n\ndef pyav_decode_stream(\n    container, start_pts, end_pts, stream, stream_name, buffer_size=0\n):\n    \"\"\"\n    Decode the video with PyAV decoder.\n    Args:\n        container (container): PyAV container.\n        start_pts (int): the starting Presentation TimeStamp to fetch the\n            video frames.\n        end_pts (int): the ending Presentation TimeStamp of the decoded frames.\n        stream (stream): PyAV stream.\n        stream_name (dict): a dictionary of streams. For example, {\"video\": 0}\n            means video stream at stream index 0.\n        buffer_size (int): number of additional frames to decode beyond end_pts.\n    Returns:\n        result (list): list of frames decoded.\n        max_pts (int): max Presentation TimeStamp of the video sequence.\n    \"\"\"\n    # Seeking in the stream is imprecise. Thus, seek to an ealier PTS by a\n    # margin pts.\n    margin = 1024\n    seek_offset = max(start_pts - margin, 0)\n\n    container.seek(seek_offset, any_frame=False, backward=True, stream=stream)\n    frames = {}\n    buffer_count = 0\n    max_pts = 0\n    for frame in container.decode(**stream_name):\n        max_pts = max(max_pts, frame.pts)\n        if frame.pts < start_pts:\n            continue\n        if frame.pts <= end_pts:\n            frames[frame.pts] = frame\n        else:\n            buffer_count += 1\n            frames[frame.pts] = frame\n            if buffer_count >= buffer_size:\n                break\n    result = [frames[pts] for pts in sorted(frames)]\n    return result, max_pts\n\n\ndef pyav_decode(\n        container, sampling_rate, num_frames, clip_idx,\n        num_clips=10, target_fps=30, safeguard_duration=False, video_max_pts=None, start=None, end=None):\n    \"\"\"\n    Convert the video from its original fps to the target_fps. If the video\n    support selective decoding (contain decoding information in the video head),\n    the perform temporal selective decoding and sample a clip from the video\n    with the PyAV decoder. If the video does not support selective decoding,\n    decode the entire video.\n\n    Args:\n        container (container): pyav container.\n        sampling_rate (int): frame sampling rate (interval between two sampled\n            frames.\n        num_frames (int): number of frames to sample.\n        clip_idx (int): if clip_idx is -1, perform random temporal sampling. If\n            clip_idx is larger than -1, uniformly split the video to num_clips\n            clips, and select the clip_idx-th video clip.\n            If clip_idx is -2, uniformly sample `num_frames` from the whole video\n            specified by `container`, ignore all the other args (e.g.,\n            sampling_rate, target_fps, etc.).\n        num_clips (int): overall number of clips to uniformly sample from the\n            given video.\n        target_fps (int): the input video may has different fps, convert it to\n            the target video fps before frame sampling.\n    Returns:\n        frames (tensor): decoded frames from the video. Return None if the no\n            video stream was found.\n        fps (float): the number of frames per second of the video.\n        decode_all_video (bool): If True, the entire video was decoded.\n    \"\"\"\n    # Try to fetch the decoding information from the video head. Some of the\n    # videos does not support fetching the decoding information, for that case\n    # it will get None duration.\n    fps = float(container.streams.video[0].average_rate)\n    frames_length = container.streams.video[0].frames\n    duration = container.streams.video[0].duration\n    if duration == None:\n        # If failed to fetch the decoding information, decode the entire video.\n        decode_all_video = True\n        video_start_pts, video_end_pts = 0, math.inf\n        video_max_pts = None\n    else:\n        if container.streams.video and safeguard_duration:\n            if video_max_pts:\n                # reuse if possible, to improve efficiency\n                duration = video_max_pts\n            else:\n                # decode the whole video to get the last frame pts\n                _, max_pts = pyav_decode_stream(\n                    container,\n                    0,\n                    math.inf,\n                    container.streams.video[0],\n                    {\"video\": 0},\n                )\n                if max_pts < 0.8*duration:\n                    print(f\"max_frame_pts and duration mismatch:{max_pts} vs. {duration}\")\n                    duration = max_pts\n        video_max_pts = duration\n        # Perform selective decoding.\n        decode_all_video = False\n        clip_size = sampling_rate * num_frames / target_fps * fps\n        sample_clip_idx = clip_idx\n        sample_num_clips = num_clips\n        if clip_idx == -2:\n            # the sampled clip will be the entire video\n            clip_size = frames_length\n            sample_clip_idx = 0\n            sample_num_clips = 1\n        start_idx, end_idx = get_start_end_idx(\n            frames_length,\n            clip_size,\n            sample_clip_idx,\n            sample_num_clips,\n        )\n        timebase = duration / frames_length\n        video_start_pts = int(start_idx * timebase)\n        video_end_pts = int(end_idx * timebase)\n\n    frames = None\n    if start!=None and end!=None and duration!=None:\n        timebase = duration / frames_length\n        video_start_pts = int(start * fps * timebase)\n        video_end_pts = int(end * fps * timebase)\n    elif start!=None and end!=None and duration==None and fps!=None:\n        video_start_pts = int(start * fps)\n        video_end_pts = int(end * fps)\n    elif start!=None and end!=None and duration==None:\n        video_start_pts = int(start * 30)\n        video_end_pts = int(end * 30)\n    # frames = None\n    # if start!=None and end!=None and duration!=None:\n    #     timebase = duration / frames_length\n    #     offset = np.random.uniform(0,end/2-start/2)\n    #     video_start_pts = int((start+offset) * fps * timebase)\n    #     video_end_pts = int((end-offset) * fps * timebase)\n    # elif start!=None and end!=None and duration==None and fps!=None:\n    #     offset = np.random.uniform(0,end/2-start/2)\n    #     video_start_pts = int((start+offset) * fps)\n    #     video_end_pts = int((end-offset) * fps)\n    # elif start!=None and end!=None and duration==None:\n    #     offset = np.random.uniform(0,end/2-start/2)\n    #     video_start_pts = int((start+offset) * 30)\n    #     video_end_pts = int((end-offset) * 30)\n    # print('video_start_pts:', video_start_pts)\n    # print('video_end_pts:', video_end_pts)\n    # If video stream was found, fetch video frames from the video.\n    if container.streams.video:\n        video_frames, max_pts = pyav_decode_stream(\n            container,\n            video_start_pts,\n            video_end_pts,\n            container.streams.video[0],\n            {\"video\": 0},\n        )\n\n        frames = video_frames\n        # move to after frame sampling\n        # frames = [frame.to_rgb().to_ndarray() for frame in video_frames]\n        # frames = torch.as_tensor(np.stack(frames))\n    return frames, fps, decode_all_video, video_max_pts\n\n\ndef decode(\n    container,\n    sampling_rate,\n    num_frames,\n    clip_idx=-1,\n    num_clips=10,\n    video_meta=None,\n    target_fps=30,\n    backend=\"pyav\",\n    max_spatial_scale=0,\n    safeguard_duration=False,\n    video_max_pts=None,\n    start=None, end=None,\n):\n    \"\"\"\n    Decode the video and perform temporal sampling.\n    Args:\n        container (container): pyav container.\n        sampling_rate (int): frame sampling rate (interval between two sampled\n            frames).\n        num_frames (int): number of frames to sample.\n        clip_idx (int): if clip_idx is -1, perform random temporal\n            sampling. If clip_idx is larger than -1, uniformly split the\n            video to num_clips clips, and select the\n            clip_idx-th video clip.\n        num_clips (int): overall number of clips to uniformly\n            sample from the given video.\n        video_meta (dict): a dict contains VideoMetaData. Details can be find\n            at `pytorch/vision/torchvision/io/_video_opt.py`.\n        target_fps (int): the input video may have different fps, convert it to\n            the target video fps before frame sampling.\n        backend (str): decoding backend includes `pyav` and `torchvision`. The\n            default one is `pyav`.\n        max_spatial_scale (int): keep the aspect ratio and resize the frame so\n            that shorter edge size is max_spatial_scale. Only used in\n            `torchvision` backend.\n    Returns:\n        frames (tensor): decoded frames from the video.\n    \"\"\"\n    # Currently support two decoders: 1) PyAV, and 2) TorchVision.\n    assert clip_idx >= -2, \"Not valied clip_idx {}\".format(clip_idx)\n    try:\n        if backend == \"pyav\":\n            frames, fps, decode_all_video, video_max_pts = pyav_decode(\n                container,\n                sampling_rate,\n                num_frames,\n                clip_idx,\n                num_clips,\n                target_fps,\n                safeguard_duration=safeguard_duration,\n                video_max_pts=video_max_pts, start=start, end=end\n            )\n        else:\n            raise NotImplementedError(\n                \"Unknown decoding backend {}\".format(backend)\n            )\n    except Exception as e:\n        print(\"Failed to decode by {} with exception: {}\".format(backend, e))\n        print(\"Failed to decode the video: {}\".format(container.name))\n        container.close()\n        return None, video_max_pts\n\n    # Return None if the frames was not decoded successfully.\n    if frames is None or len(frames) == 0:\n        container.close()\n        return None, video_max_pts\n    clip_size = sampling_rate * num_frames / target_fps * fps\n    sample_clip_idx = clip_idx\n    sample_num_clips = num_clips\n    if clip_idx == -2:\n        clip_size = len(frames)\n        sample_clip_idx = 0\n        sample_num_clips = 1\n\n    start_idx, end_idx = get_start_end_idx(\n        len(frames),\n        clip_size,\n        sample_clip_idx if decode_all_video else 0,\n        sample_num_clips if decode_all_video else 1,\n    )\n    # Perform temporal sampling from the decoded video.\n    frames = temporal_sampling(frames, start_idx, end_idx, num_frames)\n    frames = [frame.to_rgb().to_ndarray() for frame in frames]\n\n    \"\"\"\n    av package sometimes get wrong when the machine is changed, so remove it there\n    later the whole av would be replaced by ffmpeg\n    \"\"\"\n    # if container.streams.video[0].metadata.get('rotate') is not None:\n    #     rotate = 360 - int(container.streams.video[0].metadata.get('rotate'))\n    #     begin_time = time.time()\n    #     for i in range(rotate//90):\n    #         frames = [np.rot90(frame) for frame in frames]\n    #     rotate_time = time.time() - begin_time\n    #     print(f\"rotate time {rotate_time}\")\n    frames = torch.as_tensor(np.stack(frames))\n    return frames, video_max_pts"}
{"type": "source_file", "path": "src/datasets/caption_tensorizer.py", "content": "from logging import raiseExceptions\nimport torch\nimport random\nimport os.path as op\nfrom src.utils.logger import LOGGER\nimport re, html\n\nFLAIR_TAG = {\n    \"noun\": [\"NN\", \"NNP\", \"NNPS\", \"NNS\", \"PRP\", \"PRP$\", \"WP\", \"WP$\"],\n    \"verb\": [\"VB\", \"VBD\", \"VBG\", \"VBP\", \"VBZ\"],\n    \"adjective\": [\"JJ\", \"JJR\", \"JJS\"],\n    \"adverb\": [\"RB\",\"RBR\", \"RBS\", \"WRB\"],\n    \"number\": [\"CD\"]}\n\n\nclass CaptionTensorizer(object):\n    def __init__(self, tokenizer, max_img_seq_length=50, max_seq_length=70, \n            max_seq_a_length=40, mask_prob=0.15, max_masked_tokens=3,\n            attn_mask_type='seq2seq', is_train=True, mask_b=False,\n            text_mask_type='random', tag_to_mask=None,\n            mask_tag_prob=0.8, random_mask_prob=0.5, use_sep_cap = False):\n        \"\"\"Constructor.\n        Args:\n            tokenizer: tokenizer for text processing.\n            max_img_seq_length: max image sequence length.\n            max_seq_length: max text sequence length.\n            max_seq_a_length: max caption sequence length.\n            is_train: train or test mode.\n            mask_prob: probability to mask a input token.\n            max_masked_tokens: maximum number of tokens to be masked in one sentence.\n            attn_mask_type: attention mask type, support seq2seq/bidirectional/cap_s2s/cap_bidir.\n            mask_b: whether to mask text_b or not during training.\n        \"\"\"\n        self.tokenizer = tokenizer\n        self.is_train = is_train\n        self.max_img_seq_len = max_img_seq_length\n        self.max_seq_len = max_seq_length\n        self.max_seq_a_len = max_seq_a_length\n        self.mask_prob = mask_prob\n        self.max_masked_tokens = max_masked_tokens\n        self.attn_mask_type = attn_mask_type\n        self.text_mask_type = text_mask_type\n        self.mask_b = use_sep_cap\n        self.tag_to_mask = None\n        self.mask_tag_prob = 0\n        self.random_mask_prob = 1\n        self.use_sep_cap = use_sep_cap\n        if is_train:\n            assert attn_mask_type in ('seq2seq', 'bidirectional', 'cap_s2s', 'cap_bidir', 'learn_vid_att', 'learn_without_crossattn', 'learn_with_swap_crossattn')\n            assert text_mask_type in ('random', 'bert_attn', 'pos_tag', 'attn_on_the_fly')\n            if self.text_mask_type == 'pos_tag':\n                self.tag_to_mask = tag_to_mask\n                self.included_tags = set()\n                for type in self.tag_to_mask:\n                    self.included_tags.update(set(FLAIR_TAG[type]))\n                self.mask_tag_prob = mask_tag_prob\n            if self.text_mask_type != \"random\":\n                self.random_mask_prob = random_mask_prob\n        else:\n            assert attn_mask_type in ('seq2seq', 'learn_vid_att', 'learn_without_crossattn', 'learn_with_swap_crossattn')\n        \n        self._triangle_mask = torch.tril(torch.ones((self.max_seq_len, \n            self.max_seq_len), dtype=torch.long))\n    \n    def get_pos_tag_mask_idx(self, seq_a_len, text_meta):\n        \n        ''' The rest   \n        ADD\tEmail\n        AFX\tAffix\n        CC\tCoordinating conjunction\n        DT\tDeterminer\n        EX\tExistential there\n        FW\tForeign word\n        HYPH\tHyphen\n        IN\tPreposition or subordinating conjunction\n        LS\tList item marker\n        MD\tModal\n        NFP\tSuperfluous punctuation\n        PDT\tPredeterminer\n        POS\tPossessive ending\n        RP\tParticle\n        SYM\tSymbol\n        TO\tto\n        UH\tInterjection\n        WDT\tWh-determiner\n        XX\n        '''\n        # process loaded pos_tags\n        pos_tags =  text_meta[\"bert_pos_tag\"] \n        if len(pos_tags) > seq_a_len - 2:\n            pos_tags = pos_tags[:seq_a_len-2]\n        pos_tags = [None] + pos_tags + [None]\n        padding_len = seq_a_len - len(pos_tags)\n        pos_tags += [None] * padding_len\n        allow_masked_ids = set()\n        for bert_idx, tag in enumerate(pos_tags):\n            if tag is None:\n                continue\n            if bert_idx >= seq_a_len:\n                break\n            if tag not in self.included_tags:\n                continue\n            allow_masked_ids.add(bert_idx)\n        return pos_tags, allow_masked_ids\n    \n    def get_bert_attn_mask_idx(self, seq_a_len, text_meta, num_masked):\n        # process loaded bert attention weights (assuming max_len = 50)\n        attn_weights =  text_meta[\"bert_attn\"] \n        if len(attn_weights) > seq_a_len:\n            attn_weights = attn_weights[:seq_a_len]\n        elif len(attn_weights) < seq_a_len:\n            # pad with zeros\n            padding_len = seq_a_len - len(attn_weights)\n            attn_weights = [0.0] * padding_len\n        mask_idx = torch.multinomial(torch.tensor(attn_weights), num_masked).tolist()\n        return mask_idx\n    \n    def get_attn_masks(self, seq_a_len, seq_b_len):\n        # image features\n        img_len = self.max_img_seq_len\n\n        max_len = self.max_seq_len + self.max_img_seq_len\n        # C: caption, L: label, R: image region\n        c_start, c_end = 0, seq_a_len\n        l_start, l_end = self.max_seq_a_len, self.max_seq_a_len+seq_b_len\n        r_start, r_end = self.max_seq_len, self.max_seq_len + img_len\n        if self.is_train and self.attn_mask_type == 'bidirectional':\n            raiseExceptions(\"attn_mask_type is not learn_vid_att\")\n            attention_mask = torch.zeros(max_len, dtype=torch.long)\n            attention_mask[c_start : c_end] = 1 # for text_a\n            attention_mask[l_start : l_end] = 1 # for text_b if any\n            attention_mask[r_start : r_end] = 1 # for image\n        elif self.is_train and self.attn_mask_type in ('cap_s2s', 'cap_bidir'):\n            raiseExceptions(\"attn_mask_type is not learn_vid_att\")\n            # caption is a single modality, and without attention on others\n            attention_mask = torch.zeros((max_len, max_len), dtype=torch.long)\n            # no attention between [CLS] and caption\n            attention_mask[0, 0] = 1\n            if self.attn_mask_type == 'cap_s2s':\n                attention_mask[c_start + 1 : c_end, c_start + 1 : c_end].copy_(\n                    self._triangle_mask[0 : seq_a_len - 1, 0 : seq_a_len - 1]\n                )\n            else:\n                attention_mask[c_start + 1 : c_end, c_start + 1: c_end] = 1\n            attention_mask[l_start : l_end, l_start : l_end] = 1\n            attention_mask[r_start : r_end, r_start : r_end] = 1\n            # cross attention for L-R, R-L\n            attention_mask[l_start : l_end, r_start : r_end] = 1\n            attention_mask[r_start : r_end, l_start : l_end] = 1\n            # cross attention between [CLS] and L/R\n            attention_mask[0, l_start : l_end] = 1\n            attention_mask[l_start : l_end, 0] = 1\n            attention_mask[0, r_start : r_end] = 1\n            attention_mask[r_start : r_end, 0] = 1\n        elif self.attn_mask_type in ('learn_vid_att'):\n            # prepare attention mask:\n            # note that there is no attention from caption to image\n            # because otherwise it will violate the triangle attention \n            # for caption as caption will have full attention on image. \n            attention_mask = torch.zeros((max_len, max_len), dtype=torch.long)\n            # triangle mask for caption_a to caption_a\n            attention_mask[c_start : c_end, c_start : c_end].copy_(\n                    self._triangle_mask[0 : seq_a_len, 0 : seq_a_len]\n            )\n\n            # triangle mask for caption_b to caption_b\n            attention_mask[l_start : l_end, l_start : l_end].copy_(\n                    self._triangle_mask[0 : seq_b_len, 0 : seq_b_len]\n            )\n\n            # full attention for C_b-C_a\n            attention_mask[l_start : l_end, c_start : c_end] = 1\n\n            # full attention for C_a-R, C_b-R\n            attention_mask[c_start : c_end, r_start : r_end] = 1\n            attention_mask[l_start : l_end, r_start : r_end] = 1\n\n            # full attention for video tokens:\n            attention_mask[r_start : r_end, r_start : r_end] = 1\n        elif self.attn_mask_type in ('learn_without_crossattn'):\n            # prepare attention mask:\n            # note that there is no attention from caption to image\n            # because otherwise it will violate the triangle attention \n            # for caption as caption will have full attention on image. \n            attention_mask = torch.zeros((max_len, max_len), dtype=torch.long)\n            # triangle mask for caption_a to caption_a\n            attention_mask[c_start : c_end, c_start : c_end].copy_(\n                    self._triangle_mask[0 : seq_a_len, 0 : seq_a_len]\n            )\n\n            # triangle mask for caption_b to caption_b\n            attention_mask[l_start : l_end, l_start : l_end].copy_(\n                    self._triangle_mask[0 : seq_b_len, 0 : seq_b_len]\n            )\n\n            # full attention for C_a-R, C_b-R\n            attention_mask[c_start : c_end, r_start : r_end] = 1\n            attention_mask[l_start : l_end, r_start : r_end] = 1\n\n            # full attention for video tokens:\n            attention_mask[r_start : r_end, r_start : r_end] = 1        \n        elif self.attn_mask_type in ('learn_with_swap_crossattn'):\n            # prepare attention mask:\n            # note that there is no attention from caption to image\n            # because otherwise it will violate the triangle attention \n            # for caption as caption will have full attention on image. \n            attention_mask = torch.zeros((max_len, max_len), dtype=torch.long)\n            # triangle mask for caption_a to caption_a\n            attention_mask[c_start : c_end, c_start : c_end].copy_(\n                    self._triangle_mask[0 : seq_a_len, 0 : seq_a_len]\n            )\n\n            # triangle mask for caption_b to caption_b\n            attention_mask[l_start : l_end, l_start : l_end].copy_(\n                    self._triangle_mask[0 : seq_b_len, 0 : seq_b_len]\n            )\n\n            # full attention for C_b-C_a\n            attention_mask[c_start : c_end, l_start : l_end] = 1\n\n            # full attention for C_a-R, C_b-R\n            attention_mask[c_start : c_end, r_start : r_end] = 1\n            attention_mask[l_start : l_end, r_start : r_end] = 1\n\n            # full attention for video tokens:\n            attention_mask[r_start : r_end, r_start : r_end] = 1\n        else:\n            # prepare attention mask:\n            # note that there is no attention from caption to image\n            # because otherwise it will violate the triangle attention \n            # for caption as caption will have full attention on image. \n            raiseExceptions(\"attn_mask_type is not learn_vid_att\")\n            attention_mask = torch.zeros((max_len, max_len), dtype=torch.long)\n            # triangle mask for caption to caption\n            attention_mask[c_start : c_end, c_start : c_end].copy_(\n                    self._triangle_mask[0 : seq_a_len, 0 : seq_a_len]\n            )\n            # full attention for L-L, R-R\n            attention_mask[l_start : l_end, l_start : l_end] = 1\n            attention_mask[r_start : r_end, r_start : r_end] = 1\n            # full attention for C-L, C-R\n            attention_mask[c_start : c_end, l_start : l_end] = 1\n            attention_mask[c_start : c_end, r_start : r_end] = 1\n            # full attention for L-R:\n            attention_mask[l_start : l_end, r_start : r_end] = 1\n            attention_mask[r_start : r_end, l_start : l_end] = 1\n        return attention_mask\n    \n    def get_text_mask_idx(self, seq_a_len, seq_b_len, text_meta=None):\n        # randomly mask words for prediction, ignore [CLS], [PAD]\n        # it is important to mask [SEP] for image captioning as it means [EOS].\n\n        # 1. get the number of masked tokens\n        if self.mask_b:\n            # can mask both text_a and text_b\n            num_masked = min(max(round(self.mask_prob * (seq_a_len+seq_b_len)), 1), self.max_masked_tokens)\n        else:\n            # only mask text_a\n            num_masked = min(max(round(self.mask_prob * seq_a_len), 1), self.max_masked_tokens)\n        num_masked = int(num_masked)\n\n        # 2. get the masking candidates\n        if self.mask_b:\n            # text b always random masking\n            text_b_candidate = list(range(self.max_seq_a_len+1, self.max_seq_a_len+seq_b_len))\n        else:\n            text_b_candidate = []\n        if self.text_mask_type == 'random':\n            # random\n            candidate_masked_idx = list(range(1, seq_a_len))\n            candidate_masked_idx += text_b_candidate\n            random.shuffle(candidate_masked_idx)\n            masked_idx = candidate_masked_idx[:num_masked]\n        else:\n            raiseExceptions(\"text_mask_type is not random\")\n        masked_idx = sorted(masked_idx)\n        return masked_idx\n    \n    def mask_text_inputs(self, tokens, seq_a_len, seq_b_len, text_meta=None):\n        if self.is_train:\n            if self.text_mask_type == \"attn_on_the_fly\" and random.random() > self.random_mask_prob and len(tokens)> 2:\n                # self.text_mask_type == \"attn_on_the_fly\"\n                masked_pos = torch.zeros(self.max_seq_len, dtype=torch.int)\n                masked_pos[1: seq_a_len] += 1\n                masked_pos[0] = self.tokenizer.convert_tokens_to_ids([self.tokenizer.mask_token])[0]\n                mlm_targets = [-1] * self.max_masked_tokens\n            else:\n                masked_idx = self.get_text_mask_idx(seq_a_len, seq_b_len, text_meta)\n                try:\n                    masked_token = [tokens[i] for i in masked_idx]\n                except Exception as e:\n                    overflow_idx = []\n                    for i in masked_idx:\n                        if i >= len(tokens) or i < 0:\n                            overflow_idx.append(i)\n                    raise ValueError(f\"Error {e}\\nOverflow: {overflow_idx} in tokens {tokens}\")\n                for pos in masked_idx:\n                    if random.random() <= 0.8:\n                        # 80% chance to be a ['MASK'] token\n                        tokens[pos] = self.tokenizer.mask_token\n                    elif random.random() <= 0.5:\n                        # 10% chance to be a random word ((1-0.8)*0.5)\n                        tokens[pos] = self.tokenizer.get_random_token()\n                    else:\n                        # 10% chance to remain the same (1-0.8-0.1)\n                        pass\n\n                masked_pos = torch.zeros(self.max_seq_len, dtype=torch.int)\n                masked_pos[masked_idx] = 1\n                \n                # get the actual number of masked tokens\n                num_masked = len(masked_token)\n                mlm_targets = self.tokenizer.convert_tokens_to_ids(masked_token)\n                if num_masked < self.max_masked_tokens:\n                    mlm_targets = mlm_targets + ([-1] * (self.max_masked_tokens - num_masked))\n                assert len(mlm_targets) == self.max_masked_tokens, f\"mismatch in len(masked_ids) {len(mlm_targets)} vs. max_masked_tokens {self.max_masked_tokens}\"\n        elif not self.is_train:\n            masked_pos = torch.ones(self.max_seq_len, dtype=torch.int)\n            mlm_targets = None\n        \n        return tokens, masked_pos, mlm_targets\n    \n    def prepro_raw_txt(self, text):\n        # in case there are html special characters\n        text = html.unescape(text)\n        # FIXME: quick hack for text with emoji, may adopt twitter tokenizer later\n        emoji_pattern = re.compile(\"[\"\n            u\"\\U0001F600-\\U0001F64F\"  # emoticons\n            u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n            u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n            u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                            \"]+\", flags=re.UNICODE)\n        text = emoji_pattern.sub(r'', text)\n        return text\n    \n    def tokenize_text_inputs(\n            self, text_a, text_b=None, cls_token_segment_id=0,\n            pad_token_segment_id=0, sequence_a_segment_id=0,\n            sequence_b_segment_id=1, text_meta=None):\n        text_a = self.prepro_raw_txt(text_a)\n        if self.is_train:\n            tokens_a = self.tokenizer.tokenize(text_a)\n        else:\n            # fake tokens to generate masks\n            tokens_a = [self.tokenizer.mask_token] * (self.max_seq_a_len - 2)\n        if len(tokens_a) > self.max_seq_a_len - 2:\n            tokens_a = tokens_a[:(self.max_seq_a_len - 2)]\n\n        tokens = [self.tokenizer.cls_token] + tokens_a + [self.tokenizer.sep_token]\n        segment_ids = [cls_token_segment_id] + [sequence_a_segment_id] * (len(tokens) - 1)\n        seq_a_len = len(tokens)\n\n        padding_a_len = self.max_seq_a_len - seq_a_len\n        tokens += [self.tokenizer.pad_token] * padding_a_len\n        segment_ids += ([pad_token_segment_id] * padding_a_len)\n\n\n        seq_b_len = 0\n        if text_b is not None:\n            # pad text_a to keep it in fixed length for better inference.\n\n            text_b = self.prepro_raw_txt(text_b)\n            if self.is_train:\n                tokens_b = self.tokenizer.tokenize(text_b)\n            else:\n                tokens_b = [self.tokenizer.mask_token] * (self.max_seq_a_len - 2)\n            if len(tokens_b) > self.max_seq_len - len(tokens) - 2:\n                tokens_b = tokens_b[: (self.max_seq_len - len(tokens) - 2)]\n            tokens_b = [self.tokenizer.cls_token] + tokens_b + [self.tokenizer.sep_token]\n            segment_ids += [sequence_b_segment_id] * (len(tokens_b))\n            seq_b_len = len(tokens_b)\n            tokens += tokens_b\n\n            seq_len = len(tokens)\n            padding_b_len = self.max_seq_len - seq_len\n            tokens += [self.tokenizer.pad_token] * padding_b_len\n            segment_ids += ([sequence_b_segment_id] * padding_b_len)\n\n        \n        return tokens, segment_ids, seq_a_len, seq_b_len\n\n    def tensorize_example_e2e(self, text_a, img_feat, text_b=None,\n            cls_token_segment_id=0, pad_token_segment_id=0,\n            sequence_a_segment_id=0, sequence_b_segment_id=1, text_meta=None):\n        # tokenize the texts\n        tokens, segment_ids, seq_a_len, seq_b_len = self.tokenize_text_inputs(\n            text_a, text_b, cls_token_segment_id, pad_token_segment_id,\n            sequence_a_segment_id, sequence_b_segment_id, text_meta)\n        \n        # masking the tokens\n        tokens_after_masking, masked_pos, mlm_targets = self.mask_text_inputs(\n            tokens, seq_a_len, seq_b_len, text_meta)\n\n        # pad on the right for image captioning\n        input_ids = self.tokenizer.convert_tokens_to_ids(tokens_after_masking)\n\n        attention_mask = self.get_attn_masks(seq_a_len, seq_b_len)\n\n        input_ids = torch.tensor(input_ids, dtype=torch.long)\n        segment_ids = torch.tensor(segment_ids, dtype=torch.long)\n\n        if self.is_train:\n            mlm_targets = torch.tensor(mlm_targets, dtype=torch.long)\n            return (input_ids, attention_mask, segment_ids, img_feat, masked_pos, mlm_targets)\n        return input_ids, attention_mask, segment_ids, img_feat, masked_pos\n\n\ndef build_tensorizer(args, tokenizer, is_train=True):\n    if hasattr(args, 'mask_od_labels'):\n        mask_b = args.mask_od_labels\n    else:\n        mask_b = False\n    if is_train:\n        if  args.text_mask_type == \"pos_tag\":\n            # if op.exists(args.tagger_model_path):\n            #     tagger = SequenceTagger.load(args.tagger_model_path)\n            # else:\n            #     LOGGER.info(f'{args.tagger_model_path} does not exists, download on the fly...')\n            #     tagger = SequenceTagger.load('pos-fast')\n            tag_to_mask = set(args.tag_to_mask)\n        # elif args.text_mask_type == \"bert_attn\":\n        #     bert = \n        else:\n            tagger = None\n            tag_to_mask = None\n        return CaptionTensorizer(\n            tokenizer,\n            max_img_seq_length=args.max_img_seq_length if not args.use_car_sensor else args.max_img_seq_length+2,\n            max_seq_length=args.max_seq_length,\n            max_seq_a_length=args.max_seq_a_length,\n            mask_prob=args.mask_prob,\n            max_masked_tokens=args.max_masked_tokens,\n            attn_mask_type=args.attn_mask_type,\n            is_train=True,\n            mask_b=mask_b,\n            text_mask_type=args.text_mask_type,\n            mask_tag_prob=args.mask_tag_prob,\n            tag_to_mask=tag_to_mask,\n            random_mask_prob=args.random_mask_prob,\n            # tagger=tagger,\n            use_sep_cap = args.use_sep_cap,\n        )\n    return CaptionTensorizer(\n            tokenizer,\n            max_img_seq_length=args.max_img_seq_length if not args.use_car_sensor else args.max_img_seq_length+2,\n            max_seq_length=args.max_seq_length if args.add_od_labels or args.use_sep_cap else args.max_gen_length,\n            max_seq_a_length=args.max_gen_length,\n            is_train=False,\n            attn_mask_type=args.attn_mask_type,\n            use_sep_cap = args.use_sep_cap,\n    )\n\n"}
{"type": "source_file", "path": "demo/visualize_attn.py", "content": "import numpy as np\nfrom visualize import visualize_grid_attention_v2\nimport os\nimport shutil\n\nattention_probs = np.load(\"attn_visualize/head_135_11/atten.npy\")\nword_num = len(attention_probs)-784\n\n# attention_probs = np.expand_dims(attention_probs, axis=0)\n\nlast_word_attention = attention_probs[17][word_num:].reshape(16, 7, 7)\n\nfor t_i in range(16):\n    img_attn = last_word_attention[t_i]\n\n    save_path = f\"attn_visualize_one_video/{t_i}\"\n    os.makedirs(save_path, exist_ok=True)\n\n    frameid = 2*t_i\n    shutil.copyfile(f\"demo/{frameid}.png\", save_path+f\"/{frameid}.png\")\n    visualize_grid_attention_v2(f\"demo/{frameid}.png\",\n                                    save_path=save_path,\n                                    attention_mask=img_attn,\n                                    save_image=True,\n                                    save_original_image=False,\n                                    quality=100)\n\nprint(f\"word_num: {word_num}\")"}
{"type": "source_file", "path": "src/modeling/video_swin/swin_base_patch244_window1677_sthv2.py", "content": "_base_ = [\n    'swin_base.py', 'default_runtime.py'\n]\n\n# dataset settings\ndataset_type = 'VideoDataset'\ndata_root = 'data/sthv2/videos'\ndata_root_val = 'data/sthv2/videos'\nann_file_train = 'data/sthv2/sthv2_train_list_videos.txt'\nann_file_val = 'data/sthv2/sthv2_val_list_videos.txt'\nann_file_test = 'data/sthv2/sthv2_val_list_videos.txt'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_bgr=False)\ntrain_pipeline = [\n    dict(type='DecordInit'),\n    dict(type='SampleFrames', clip_len=32, frame_interval=2, num_clips=1, frame_uniform=True),\n    dict(type='DecordDecode'),\n    dict(type='Resize', scale=(-1, 256)),\n    dict(type='RandomResizedCrop'),\n    dict(type='Resize', scale=(224, 224), keep_ratio=False),\n    dict(type='Flip', flip_ratio=0),\n    dict(type='Imgaug', transforms=[dict(type='RandAugment', n=4, m=7)]),\n    dict(type='Normalize', **img_norm_cfg),\n    dict(type='RandomErasing', probability=0.25),\n    dict(type='FormatShape', input_format='NCTHW'),\n    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n    dict(type='ToTensor', keys=['imgs', 'label'])\n]\nval_pipeline = [\n    dict(type='DecordInit'),\n    dict(\n        type='SampleFrames',\n        clip_len=32,\n        frame_interval=2,\n        num_clips=1,\n        frame_uniform=True,\n        test_mode=True),\n    dict(type='DecordDecode'),\n    dict(type='Resize', scale=(-1, 256)),\n    dict(type='CenterCrop', crop_size=224),\n    dict(type='Flip', flip_ratio=0),\n    dict(type='Normalize', **img_norm_cfg),\n    dict(type='FormatShape', input_format='NCTHW'),\n    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n    dict(type='ToTensor', keys=['imgs'])\n]\ntest_pipeline = [\n    dict(type='DecordInit'),\n    dict(\n        type='SampleFrames',\n        clip_len=32,\n        frame_interval=2,\n        num_clips=1,\n        frame_uniform=True,\n        test_mode=True),\n    dict(type='DecordDecode'),\n    dict(type='Resize', scale=(-1, 224)),\n    dict(type='ThreeCrop', crop_size=224),\n    dict(type='Flip', flip_ratio=0),\n    dict(type='Normalize', **img_norm_cfg),\n    dict(type='FormatShape', input_format='NCTHW'),\n    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n    dict(type='ToTensor', keys=['imgs'])\n]\ndata = dict(\n    videos_per_gpu=8,\n    workers_per_gpu=1,\n    val_dataloader=dict(\n        videos_per_gpu=1,\n        workers_per_gpu=1\n    ),\n    test_dataloader=dict(\n        videos_per_gpu=1,\n        workers_per_gpu=1\n    ),\n    train=dict(\n        type=dataset_type,\n        ann_file=ann_file_train,\n        data_prefix=data_root,\n        pipeline=train_pipeline),\n    val=dict(\n        type=dataset_type,\n        ann_file=ann_file_val,\n        data_prefix=data_root_val,\n        pipeline=val_pipeline),\n    test=dict(\n        type=dataset_type,\n        ann_file=ann_file_test,\n        data_prefix=data_root_val,\n        pipeline=test_pipeline))\nevaluation = dict(\n    interval=5, metrics=['top_k_accuracy', 'mean_class_accuracy'])\n\n# optimizer\noptimizer = dict(type='AdamW', lr=3e-4, betas=(0.9, 0.999), weight_decay=0.05,\n                 paramwise_cfg=dict(custom_keys={'absolute_pos_embed': dict(decay_mult=0.),\n                                                 'relative_position_bias_table': dict(decay_mult=0.),\n                                                 'norm': dict(decay_mult=0.),\n                                                 'backbone': dict(lr_mult=0.1)}))\n# learning policy\nlr_config = dict(\n    policy='CosineAnnealing',\n    min_lr=0,\n    warmup='linear',\n    warmup_by_epoch=True,\n    warmup_iters=2.5\n)\ntotal_epochs = 60\n\n# runtime settings\ncheckpoint_config = dict(interval=1)\nwork_dir = work_dir = './work_dirs/sthv2_swin_base_patch244_window1677.py'\nfind_unused_parameters = False\n\n\n# do not use mmdet version fp16\nfp16 = None\noptimizer_config = dict(\n    type=\"DistOptimizerHook\",\n    update_interval=8,\n    grad_clip=None,\n    coalesce=True,\n    bucket_size_mb=-1,\n    use_fp16=True,\n)\n\nmodel=dict(backbone=dict(patch_size=(2,4,4), window_size=(16,7,7), drop_path_rate=0.4),\n           cls_head=dict(num_classes=174),\n           test_cfg=dict(max_testing_views=2), \n           train_cfg=dict(blending=dict(type='LabelSmoothing', num_classes=174, smoothing=0.1)))\n"}
{"type": "source_file", "path": "src/configs/config.py", "content": "\"\"\"\nModified from ClipBERT code\n\"\"\"\nimport os\nimport sys\nimport json\nimport argparse\nimport torch\n\nfrom easydict import EasyDict as edict\nfrom src.utils.miscellaneous import str_to_bool, check_yaml_file\nfrom src.utils.logger import LOGGER\nfrom os import path as op\nfrom packaging import version\n\n\ndef parse_with_config(parsed_args):\n    \"\"\"This function will set args based on the input config file.\n    (1) it only overwrites unset parameters,\n        i.e., these parameters not set from user command line input\n    (2) it also sets configs in the config file but declared in the parser\n    \"\"\"\n    # convert to EasyDict object, enabling access from attributes even for nested config\n    # e.g., args.train_datasets[0].name\n    args = edict(vars(parsed_args))\n    if args.config is not None:\n        config_args = json.load(open(args.config))\n        override_keys = {arg[2:].split(\"=\")[0] for arg in sys.argv[1:]\n                         if arg.startswith(\"--\")}\n        for k, v in config_args.items():\n            if k not in override_keys:\n                setattr(args, k, v)\n    del args.config\n    return args\n\n\nclass SharedConfigs(object):\n    \"\"\"Shared options for pre-training and downstream tasks.\n    For each downstream task, implement a get_*_args function,\n    see `get_pretraining_args()`\n\n    Usage:\n    >>> shared_configs = SharedConfigs()\n    >>> pretraining_config = shared_configs.get_pretraining_args()\n    \"\"\"\n\n    def __init__(self, desc=\"shared config\"):\n        parser = argparse.ArgumentParser(description=desc)\n        # path configs\n        parser.add_argument(\"--data_dir\", default='datasets', type=str, required=False,\n                            help=\"Directory with all datasets, each in one subfolder\")\n        parser.add_argument(\"--output_dir\", default='output/', type=str, required=False,\n                            help=\"The output directory to save checkpoint and test results.\")\n        parser.add_argument(\"--train_yaml\", default='coco_caption/train.yaml', type=str, required=False,\n                            help=\"Yaml file with all data for training.\")\n\n        # multimodal transformer modeling config\n        parser.add_argument(\"--model_name_or_path\", default=None, type=str, required=False,\n                            help=\"Path to pre-trained model or model type.\")\n        parser.add_argument(\"--config_name\", default=\"\", type=str,\n                            help=\"Pretrained config name or path if not the same as model_name.\")\n        parser.add_argument(\"--tokenizer_name\", default=\"\", type=str,\n                            help=\"Pretrained tokenizer name or path if not the same as model_name.\")\n        parser.add_argument(\"--num_hidden_layers\", default=-1, type=int, required=False,\n                            help=\"Update model config if given\")\n        parser.add_argument(\"--hidden_size\", default=-1, type=int, required=False,\n                            help=\"Update model config if given\")\n        parser.add_argument(\"--num_attention_heads\", default=-1, type=int, required=False,\n                            help=\"Update model config if given. Note that the division of \"\n                            \"hidden_size / num_attention_heads should be in integer.\")\n        parser.add_argument(\"--intermediate_size\", default=-1, type=int, required=False,\n                            help=\"Update model config if given.\")\n        parser.add_argument(\"--img_feature_dim\", default=512, type=int,\n                            help=\"Update model config if given.The Image Feature Dimension.\")\n        parser.add_argument(\"--load_partial_weights\", type=str_to_bool, nargs='?',\n                            const=True, default=False,\n                            help=\"Only valid when change num_hidden_layers, img_feature_dim, but not other structures. \"\n                            \"If set to true, will load the first few layers weight from pretrained model.\")\n        parser.add_argument(\"--freeze_embedding\", type=str_to_bool, nargs='?', const=True, default=False,\n                            help=\"Whether to freeze word embeddings in Bert\")\n        parser.add_argument(\"--drop_out\", default=0.1, type=float,\n                            help=\"Drop out ratio in BERT.\")\n\n        # inputs to multimodal transformer config\n        parser.add_argument(\"--max_seq_length\", default=70, type=int,\n                            help=\"The maximum total input sequence length after tokenization.\")\n        parser.add_argument(\"--max_seq_a_length\", default=40, type=int,\n                            help=\"The maximum sequence length for caption.\")\n        parser.add_argument(\"--max_img_seq_length\", default=50, type=int,\n                            help=\"The maximum total input image sequence length.\")\n        parser.add_argument(\"--do_lower_case\", type=str_to_bool, nargs='?', const=True, default=False,\n                            help=\"Set this flag if you are using an uncased model.\")\n        parser.add_argument(\"--add_od_labels\", type=str_to_bool, nargs='?', const=True, default=False,\n                            help=\"Whether to add object detection labels or not\")\n        parser.add_argument(\"--od_label_conf\", default=0.0, type=float,\n                            help=\"Confidence threshold to select od labels.\")\n        parser.add_argument(\"--use_asr\", type=str_to_bool, nargs='?', const=True, default=False,\n                            help=\"Whether to add ASR/transcript as additional modality input\")\n        parser.add_argument(\"--use_sep_cap\", type=str_to_bool, nargs='?', const=True, default=False,\n                            help=\"Whether to use action and justification in the meanwhile\")\n        parser.add_argument(\"--use_swap_cap\", type=str_to_bool, nargs='?', const=True, default=False,\n                            help=\"swap action and justification place, should use with --use_sep_cap\")\n        parser.add_argument(\"--use_car_sensor\", type=str_to_bool, nargs='?', const=True, default=False,\n                            help=\"use car tensor to do multi input (Single+) in Table III\")\n        parser.add_argument(\"--multitask\", type=str_to_bool, nargs='?', const=True, default=False,\n                            help=\"use car tensor to do multitask\")\n        parser.add_argument(\"--only_signal\", type=str_to_bool, nargs='?', const=True, default=False,\n                            help=\"only do control signal prediction task\")\n        parser.add_argument(\"--signal_types\", metavar='str', nargs='+', default=['course'],\n                            choices=['course', 'speed', 'accelerator', 'curvature'], \n                            help=\"Control Signal type\")\n        parser.add_argument(\"--unique_labels_on\", type=str_to_bool, nargs='?', const=True, default=False,\n                            help=\"Use unique labels only.\")\n        parser.add_argument(\"--no_sort_by_conf\", type=str_to_bool, nargs='?', const=True, default=False,\n                            help=\"By default, we will sort feature/labels by confidence, \"\n                            \"which is helpful when truncate the feature/labels.\")\n        #======= mask token\n        parser.add_argument(\"--mask_prob\", default=0.15, type=float,\n                            help= \"Probability to mask input sentence during training.\")\n        parser.add_argument(\"--max_masked_tokens\", type=int, default=3,\n                            help=\"The max number of masked tokens per sentence.\")\n        parser.add_argument(\"--attn_mask_type\", type=str, default='seq2seq',\n                           choices=['seq2seq', 'bidirectional', 'learn_vid_mask', 'learn_without_crossattn', 'learn_with_swap_crossattn'], \n                            help=\"Attention mask type, support seq2seq, bidirectional\")\n        parser.add_argument(\"--text_mask_type\", type=str, default='random',\n                           choices=['random', 'pos_tag', 'bert_attn', 'attn_on_the_fly'], \n                            help=\"Attention mask type, support random, pos_tag, bert_attn (precomputed_bert_attn), attn_on_the_fly\")\n        parser.add_argument(\"--tag_to_mask\", default=[\"noun\", \"verb\"], type=str, nargs=\"+\", \n                            choices=[\"noun\", \"verb\", \"adjective\", \"adverb\", \"number\"],\n                            help= \"what tags to mask\")\n        parser.add_argument(\"--mask_tag_prob\", default=0.8, type=float,\n                            help= \"Probability to mask input text tokens with included tags during training.\")\n        parser.add_argument(\"--tagger_model_path\", type=str, default='models/flair/en-pos-ontonotes-fast-v0.5.pt', \n                            help=\"checkpoint path to tagger model\")\n        parser.add_argument(\"--random_mask_prob\", default=0, type=float,\n                            help= \"Probability to mask input text tokens randomly when using other text_mask_type\")\n\n        # data loading\n        parser.add_argument(\"--on_memory\", type=str_to_bool, nargs='?', const=True, default=False,\n                            help=\"Option to load labels/caption to memory before training.\")\n        parser.add_argument(\"--effective_batch_size\", default=-1, type=int,\n                            help=\"Batch size over all GPUs for training.\")\n        parser.add_argument(\"--per_gpu_train_batch_size\", default=64, type=int,\n                            help=\"Batch size per GPU/CPU for training.\")\n        parser.add_argument(\"--num_workers\", default=4, type=int,\n                            help=\"Workers in dataloader.\")\n        parser.add_argument('--limited_samples', type=int, default=-1, \n                            help=\"Set # of samples per node. Data partition for cross-node training.\")\n        \n        # training configs\n        parser.add_argument(\"--learning_rate\", default=3e-5, type=float,\n                            help=\"The initial lr.\")\n        parser.add_argument(\"--weight_decay\", default=0.05, type=float,\n                            help=\"Weight deay.\")\n        parser.add_argument(\"--adam_epsilon\", default=1e-8, type=float,\n                            help=\"Epsilon for Adam.\")\n        parser.add_argument(\"--max_grad_norm\", default=1.0, type=float,\n                            help=\"Max gradient norm.\")\n        parser.add_argument(\"--warmup_ratio\", default=0.1, type=float,\n                            help=\"Linear warmup.\")\n        parser.add_argument(\"--scheduler\", default='warmup_linear', type=str,\n                            help=\"warmup_linear (triangle) or step\",\n                            choices=[\"warmup_linear\", \"step\"])\n        parser.add_argument('--gradient_accumulation_steps', type=int, default=1)\n        parser.add_argument(\"--num_train_epochs\", default=20, type=int,\n                            help=\"Total number of training epochs to perform.\")\n        parser.add_argument('--logging_steps', type=int, default=20,\n                            help=\"Log every X steps.\")\n        parser.add_argument('--save_steps', type=int, default=2000,\n                            help=\"Save checkpoint every X steps. Will also perform evaluatin.\")\n        parser.add_argument('--restore_ratio', type=float, default=0.05,\n                            help=\"save restorer checkpoint for 0.05 ratio\")\n        parser.add_argument(\"--device\", type=str, default='cuda',\n                            help=\"cuda or cpu\")\n        parser.add_argument('--seed', type=int, default=88,\n                            help=\"random seed for initialization.\")\n        parser.add_argument(\"--local_rank\", type=int, default=0,\n                            help=\"For distributed training.\")\n        # ========= mix-precision training (>torch1.6 only)\n        parser.add_argument('--mixed_precision_method', default='apex', type=str,\n                            help=\"set mixed_precision_method, options: apex, deepspeed, fairscale\",\n                            choices=[\"apex\", \"deepspeed\", \"fairscale\"])\n        parser.add_argument('--zero_opt_stage', type=int,\n                            help=\"zero_opt_stage, only allowed in deepspeed\", \n                            default=-1, choices=[0, 1, 2, 3])\n        parser.add_argument('--amp_opt_level', default=0,\n                            help=\"amp optimization level, can set for both deepspeed and apex\",  type=int,\n                            choices=[0, 1, 2, 3])\n        parser.add_argument('--deepspeed_fp16',\n                            help=\"use fp16 for deepspeed\",  type=str_to_bool, nargs='?', const=True, default=False)\n        parser.add_argument('--fairscale_fp16',\n                            help=\"use fp16 for fairscale\",  type=str_to_bool, nargs='?', const=True, default=False)\n        # ========= resume training or load pre_trained weights\n        parser.add_argument('--pretrained_checkpoint', type=str, default='')\n\n        # for debug purpose\n        parser.add_argument('--debug', type=str_to_bool, nargs='?', const=True, default=False)\n        parser.add_argument('--debug_speed', type=str_to_bool, nargs='?', const=True, default=False)\n\n        # can use config files, will only overwrite unset parameters\n        parser.add_argument(\"--config\", help=\"JSON config files\")\n        self.parser = parser\n\n    def parse_args(self):\n        parsed_args = self.parser.parse_args()\n        args = parse_with_config(parsed_args)\n        return args\n\n    def add_downstream_args(self):\n        # downstream finetuning args (not needed for pretraining)\n        self.parser.add_argument(\"--eval_model_dir\", type=str, default='',\n                                 help=\"Model directory for evaluation.\")\n        \n        # training/validation/inference mode (only needed for captioning)\n        self.parser.add_argument(\"--val_yaml\", default='coco_caption/val.yaml',\n                                 type=str, required=False,\n                                 help=\"Yaml file with all data for validation\")\n        self.parser.add_argument(\"--test_yaml\", default='coco_caption/test.yaml', type=str,\n                                 required=False, nargs='+',\n                                 help=\"Yaml file with all data for testing, could be multiple files.\")\n\n        self.parser.add_argument(\"--do_train\", type=str_to_bool, nargs='?',\n                                 const=True, default=False,\n                                 help=\"Whether to run training.\")\n        self.parser.add_argument(\"--do_test\", type=str_to_bool,\n                                 nargs='?', const=True, default=False,\n                                 help=\"Whether to run inference.\")\n        self.parser.add_argument(\"--do_eval\", type=str_to_bool, nargs='?',\n                                 const=True, default=False,\n                                 help=\"Whether to run evaluation.\")\n        self.parser.add_argument(\"--do_signal_eval\", type=str_to_bool, nargs='?',\n                                 const=True, default=False,\n                                 help=\"Whether to run evaluation.\")\n        self.parser.add_argument(\"--evaluate_during_training\", type=str_to_bool,\n                                 nargs='?', const=True, default=False,\n                                 help=\"Run evaluation during training at each save_steps.\")\n        self.parser.add_argument(\"--per_gpu_eval_batch_size\", default=64, type=int,\n                                 help=\"Batch size per GPU/CPU for evaluation.\")\n        return\n\n    def shared_video_captioning_config(self, cbs=False, scst=False):\n        self.add_downstream_args()\n        # image feature masking (only used in captioning?)\n        self.parser.add_argument('--mask_img_feat', type=str_to_bool,\n                                 nargs='?', const=True, default=False,\n                                 help='Enable image fetuare masking')\n        self.parser.add_argument('--max_masked_img_tokens', type=int, default=10,\n                                 help=\"Maximum masked object featrues\")\n\n        # basic decoding configs\n        self.parser.add_argument(\"--tie_weights\", type=str_to_bool, nargs='?',\n                                 const=True, default=False,\n                                 help=\"Whether to tie decoding weights to that of encoding\")\n        self.parser.add_argument(\"--label_smoothing\", default=0, type=float,\n                                 help=\".\")\n        self.parser.add_argument(\"--drop_worst_ratio\", default=0, type=float,\n                                 help=\".\")\n        self.parser.add_argument(\"--drop_worst_after\", default=0, type=int,\n                                 help=\".\")\n        self.parser.add_argument('--max_gen_length', type=int, default=20,\n                                 help=\"max length of generated sentences\")\n        self.parser.add_argument('--output_hidden_states', type=str_to_bool,\n                                 nargs='?', const=True, default=False,\n                                 help=\"Turn on for fast decoding\")\n        self.parser.add_argument('--num_return_sequences', type=int, default=1,\n                                 help=\"repeating times per image\")\n        self.parser.add_argument('--num_beams', type=int, default=1,\n                                 help=\"beam search width\")\n        self.parser.add_argument('--num_keep_best', type=int, default=1,\n                                 help=\"number of hypotheses to keep in beam search\")\n        self.parser.add_argument('--temperature', type=float, default=1,\n                                 help=\"temperature in softmax for sampling\")\n        self.parser.add_argument('--top_k', type=int, default=0,\n                                 help=\"filter distribution for sampling\")\n        self.parser.add_argument('--top_p', type=float, default=1,\n                                 help=\"filter distribution for sampling\")\n        self.parser.add_argument('--repetition_penalty', type=int, default=1,\n                                 help=\"repetition penalty from CTRL paper \"\n                                 \"(https://arxiv.org/abs/1909.05858)\")\n        self.parser.add_argument('--length_penalty', type=int, default=1,\n                                 help=\"beam search length penalty\")\n        \n        if cbs:\n            self.constraint_beam_search_args()\n        if scst:\n            self.self_critic_args()\n\n        return\n    \n    def constraint_beam_search_args(self):\n        \n        # for Constrained Beam Search\n        self.parser.add_argument('--use_cbs', type=str_to_bool, nargs='?', const=True, default=False,\n                                 help='Use constrained beam search for decoding')\n        self.parser.add_argument('--min_constraints_to_satisfy', type=int, default=2,\n                                 help=\"minimum number of constraints to satisfy\")\n        self.parser.add_argument('--use_hypo', type=str_to_bool, nargs='?', const=True, default=False,\n                                 help='Store hypotheses for constrained beam search')\n        self.parser.add_argument('--decoding_constraint', type=str_to_bool, nargs='?',\n                                 const=True, default=False,\n                                 help='When decoding enforce the constraint that the'\n                                 'word cannot be consecutively predicted twice in a row')\n        self.parser.add_argument('--remove_bad_endings', type=str_to_bool, nargs='?',\n                                 const=True, default=False,\n                                 help='When decoding enforce that the tokens in bad endings,'\n                                 'e.g., a, the, etc cannot be predicted at the end of the sentence')\n        return\n\n    def self_critic_args(self):\n        # for self-critical sequence training\n        self.parser.add_argument('--scst', type=str_to_bool, nargs='?', const=True, default=False,\n                                 help='Self-critical sequence training')\n        self.parser.add_argument('--sc_train_sample_n', type=int, default=5,\n                                 help=\"number of sampled captions for sc training\")\n        self.parser.add_argument('--sc_baseline_type', type=str, default='greedy',\n                                 help=\"baseline tyep of REINFORCE algorithm\")\n        self.parser.add_argument('--cider_cached_tokens', type=str,\n                                 default='coco_caption/gt/coco-train-words.p',\n                                 help=\"path to cached cPickle file used to calculate CIDEr scores\")\n        return\n\nshared_configs = SharedConfigs()\n\ndef basic_check_arguments(args):\n    args.output_dir = args.output_dir.replace(\" \", \"_\")\n    if args.debug_speed:\n        args.logging_steps = 1\n        args.num_train_epochs = 1\n\n    if args.debug:\n        args.effective_batch_size = args.num_gpus\n        args.per_gpu_train_batch_size = 1\n        args.num_train_epochs = 1\n        args.logging_steps = 5\n        args.max_img_seq_length = 98\n\n    # can add some basic checks here\n    if args.mixed_precision_method != \"deepspeed\":\n        LOGGER.info(\"Deepspeed is not enabled. We will disable the relevant args --zero_opt_stage and --deepspeed_fp16.\")\n        args.zero_opt_stage = -1\n        args.deepspeed_fp16 = False\n    \n    if args.mixed_precision_method != \"fairscale\":\n        LOGGER.info(\"Fairscale is not enabled. We will disable the relevant args --fairscale_fp16.\")\n        args.zero_opt_stage = -1\n        args.fairscale_fp16 = False\n    \n    if args.mixed_precision_method != \"apex\":\n        LOGGER.info(\"Disable restorer for deepspeed or fairscale\")\n        args.restore_ratio = -1\n    \n    if args.text_mask_type != \"pos_tag\":\n        LOGGER.info(\"Disable --mask_tag_prob\")\n        args.mask_tag_prob = -1\n\n    if hasattr(args, 'do_train') and args.do_train:\n        check_yaml_file(op.join(args.data_dir, args.train_yaml))\n        if args.evaluate_during_training:\n            check_yaml_file(op.join(args.data_dir, args.val_yaml))\n        # check after num_gpus initialized\n        if args.effective_batch_size > 0:\n            assert args.effective_batch_size % args.num_gpus == 0\n            args.per_gpu_train_batch_size = int(args.effective_batch_size / args.num_gpus)\n            args.per_gpu_eval_batch_size = int(args.effective_batch_size / args.num_gpus)\n        else:\n            assert args.per_gpu_train_batch_size > 0\n            args.effective_batch_size = args.per_gpu_train_batch_size * args.num_gpus\n            args.per_gpu_eval_batch_size = max(\n                args.per_gpu_eval_batch_size, args.per_gpu_train_batch_size)\n\n        if args.use_asr:\n            args.add_od_labels = True\n        if args.add_od_labels:\n            assert args.max_seq_length > args.max_seq_a_length\n        elif args.use_sep_cap:\n            assert args.max_seq_length == 2*args.max_seq_a_length\n        else:\n            assert args.max_seq_length == args.max_seq_a_length\n        if args.use_swap_cap:\n            assert args.use_sep_cap\n    if hasattr(args, 'do_test') and args.do_test:\n        for test_yaml in args.test_yaml:\n            check_yaml_file(op.join(args.data_dir, test_yaml))\n\ndef restore_training_settings(args):\n    ''' Restore args for inference and SCST training\n    Only works for downstream finetuning\n    '''\n    if args.do_train:\n        if hasattr(args, 'scst') and not args.scst:\n            return args\n        checkpoint = args.model_name_or_path\n    else:\n        assert args.do_test or args.do_eval\n        checkpoint = args.eval_model_dir\n    # restore training settings, check hasattr for backward compatibility\n    try:\n        # train_args = torch.load(op.join(checkpoint, os.pardir, 'log', 'args.json')) #\n        json_path = op.join(checkpoint, os.pardir, 'log', 'args.json')\n        f = open(json_path,'r')\n        json_data = json.load(f)\n        from easydict import EasyDict\n        train_args = EasyDict(json_data)\n    except Exception as e:\n        train_args = torch.load(op.join(checkpoint, 'training_args.bin'))\n\n    if args.add_od_labels:\n        if hasattr(train_args, 'max_seq_a_length'):\n            if hasattr(train_args, 'scst') and train_args.scst:\n                max_od_labels_len = train_args.max_seq_length - train_args.max_gen_length\n            else:\n                max_od_labels_len = train_args.max_seq_length - train_args.max_seq_a_length\n            max_seq_length = args.max_gen_length + max_od_labels_len\n            args.max_seq_length = max_seq_length\n            LOGGER.warning('Override max_seq_length to {} = max_gen_length:{} + od_labels_len:{}'.format(\n                    max_seq_length, args.max_gen_length, max_od_labels_len))\n\n    override_params = ['do_lower_case', 'add_od_labels',\n            'img_feature_dim', 'no_sort_by_conf','num_hidden_layers']\n    for param in override_params:\n        if hasattr(train_args, param):\n            train_v = getattr(train_args, param)\n            test_v = getattr(args, param)\n            if train_v != test_v:\n                LOGGER.warning('Override {} with train args: {} -> {}'.format(param,\n                    test_v, train_v))\n                setattr(args, param, train_v)\n\n    if hasattr(args, 'scst') and args.scst==True:\n        args.max_seq_length = train_args.max_gen_length\n        args.max_seq_a_length = train_args.max_gen_length\n    return args\n\n"}
{"type": "source_file", "path": "src/modeling/load_bert.py", "content": "from src.layers.bert import BertTokenizer, BertConfig, BertForImageCaptioning\nfrom src.utils.logger import LOGGER as logger\n\ndef get_bert_model(args):\n    # Load pretrained bert and tokenizer based on training configs\n    config_class, model_class, tokenizer_class = BertConfig, BertForImageCaptioning, BertTokenizer\n    config = config_class.from_pretrained(args.config_name if args.config_name else \\\n            args.model_name_or_path, num_labels=2, finetuning_task='image_captioning')\n\n    tokenizer = tokenizer_class.from_pretrained(args.tokenizer_name if args.tokenizer_name \\\n            else args.model_name_or_path, do_lower_case=args.do_lower_case)\n    config.img_feature_type = 'frcnn'\n    config.hidden_dropout_prob = args.drop_out\n    config.loss_type = 'classification'\n    config.tie_weights = args.tie_weights\n    config.freeze_embedding = args.freeze_embedding\n    config.label_smoothing = args.label_smoothing\n    config.drop_worst_ratio = args.drop_worst_ratio\n    config.drop_worst_after = args.drop_worst_after\n    # update model structure if specified in arguments\n    update_params = ['img_feature_dim', 'num_hidden_layers', 'hidden_size', 'num_attention_heads', 'intermediate_size']\n    model_structure_changed = [False] * len(update_params)\n    # model_structure_changed[0] = True  # cclin hack\n    for idx, param in enumerate(update_params):\n        arg_param = getattr(args, param)\n        # bert-base-uncased do not have img_feature_dim\n        config_param = getattr(config, param) if hasattr(config, param) else -1\n        if arg_param > 0 and arg_param != config_param:\n            logger.info(f\"Update config parameter {param}: {config_param} -> {arg_param}\")\n            setattr(config, param, arg_param)\n            model_structure_changed[idx] = True\n    if any(model_structure_changed):\n        assert config.hidden_size % config.num_attention_heads == 0\n        if args.load_partial_weights:\n            # can load partial weights when changing layer only.\n            assert not any(model_structure_changed[2:]), \"Cannot load partial weights \" \\\n                \"when any of ({}) is changed.\".format(', '.join(update_params[2:]))\n            model = model_class.from_pretrained(args.model_name_or_path,\n                from_tf=bool('.ckpt' in args.model_name_or_path), config=config)\n            logger.info(\"Load partial weights for bert layers.\")\n        else:\n            model = model_class(config=config) # init from scratch\n            logger.info(\"Init model from scratch.\")\n    else:\n        model = model_class.from_pretrained(args.model_name_or_path,\n            from_tf=bool('.ckpt' in args.model_name_or_path), config=config)\n        logger.info(f\"Load pretrained model: {args.model_name_or_path}\")\n\n    total_params = sum(p.numel() for p in model.parameters())\n    logger.info(f'Model total parameters: {total_params}')\n    return model, config, tokenizer"}
{"type": "source_file", "path": "src/modeling/video_swin/swin_base_patch244_window877_kinetics400_22k.py", "content": "_base_ = [\n    'swin_base.py', 'default_runtime.py'\n]\nmodel=dict(backbone=dict(patch_size=(2,4,4), drop_path_rate=0.2), test_cfg=dict(max_testing_views=2))\n\n# dataset settings\ndataset_type = 'VideoDataset'\ndata_root = 'data/kinetics400/train'\ndata_root_val = 'data/kinetics400/val'\nann_file_train = 'data/kinetics400/kinetics400_train_list.txt'\nann_file_val = 'data/kinetics400/kinetics400_val_list.txt'\nann_file_test = 'data/kinetics400/kinetics400_val_list.txt'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_bgr=False)\ntrain_pipeline = [\n    dict(type='DecordInit'),\n    dict(type='SampleFrames', clip_len=32, frame_interval=2, num_clips=1),\n    dict(type='DecordDecode'),\n    dict(type='Resize', scale=(-1, 256)),\n    dict(type='RandomResizedCrop'),\n    dict(type='Resize', scale=(224, 224), keep_ratio=False),\n    dict(type='Flip', flip_ratio=0.5),\n    dict(type='Normalize', **img_norm_cfg),\n    dict(type='FormatShape', input_format='NCTHW'),\n    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n    dict(type='ToTensor', keys=['imgs', 'label'])\n]\nval_pipeline = [\n    dict(type='DecordInit'),\n    dict(\n        type='SampleFrames',\n        clip_len=32,\n        frame_interval=2,\n        num_clips=1,\n        test_mode=True),\n    dict(type='DecordDecode'),\n    dict(type='Resize', scale=(-1, 256)),\n    dict(type='CenterCrop', crop_size=224),\n    dict(type='Flip', flip_ratio=0),\n    dict(type='Normalize', **img_norm_cfg),\n    dict(type='FormatShape', input_format='NCTHW'),\n    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n    dict(type='ToTensor', keys=['imgs'])\n]\ntest_pipeline = [\n    dict(type='DecordInit'),\n    dict(\n        type='SampleFrames',\n        clip_len=32,\n        frame_interval=2,\n        num_clips=4,\n        test_mode=True),\n    dict(type='DecordDecode'),\n    dict(type='Resize', scale=(-1, 224)),\n    dict(type='ThreeCrop', crop_size=224),\n    dict(type='Flip', flip_ratio=0),\n    dict(type='Normalize', **img_norm_cfg),\n    dict(type='FormatShape', input_format='NCTHW'),\n    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n    dict(type='ToTensor', keys=['imgs'])\n]\ndata = dict(\n    videos_per_gpu=8,\n    workers_per_gpu=4,\n    val_dataloader=dict(\n        videos_per_gpu=1,\n        workers_per_gpu=1\n    ),\n    test_dataloader=dict(\n        videos_per_gpu=1,\n        workers_per_gpu=1\n    ),\n    train=dict(\n        type=dataset_type,\n        ann_file=ann_file_train,\n        data_prefix=data_root,\n        pipeline=train_pipeline),\n    val=dict(\n        type=dataset_type,\n        ann_file=ann_file_val,\n        data_prefix=data_root_val,\n        pipeline=val_pipeline),\n    test=dict(\n        type=dataset_type,\n        ann_file=ann_file_test,\n        data_prefix=data_root_val,\n        pipeline=test_pipeline))\nevaluation = dict(\n    interval=5, metrics=['top_k_accuracy', 'mean_class_accuracy'])\n\n# optimizer\noptimizer = dict(type='AdamW', lr=3e-4, betas=(0.9, 0.999), weight_decay=0.05,\n                 paramwise_cfg=dict(custom_keys={'absolute_pos_embed': dict(decay_mult=0.),\n                                                 'relative_position_bias_table': dict(decay_mult=0.),\n                                                 'norm': dict(decay_mult=0.),\n                                                 'backbone': dict(lr_mult=0.1)}))\n# learning policy\nlr_config = dict(\n    policy='CosineAnnealing',\n    min_lr=0,\n    warmup='linear',\n    warmup_by_epoch=True,\n    warmup_iters=2.5\n)\ntotal_epochs = 30\n\n# runtime settings\ncheckpoint_config = dict(interval=1)\nwork_dir = work_dir = './work_dirs/k400_swin_base_22k_patch244_window877.py'\nfind_unused_parameters = False\n\n\n# do not use mmdet version fp16\nfp16 = None\noptimizer_config = dict(\n    type=\"DistOptimizerHook\",\n    update_interval=8,\n    grad_clip=None,\n    coalesce=True,\n    bucket_size_mb=-1,\n    use_fp16=True,\n)\n"}
{"type": "source_file", "path": "src/modeling/swin/__init__.py", "content": "from .build import build_model"}
{"type": "source_file", "path": "src/modeling/load_sensor_pred_head.py", "content": "import torch\nfrom src.utils.logger import LOGGER as logger\nfrom torch import nn\nfrom src.layers.bert.modeling_bert import BertEncoder\nfrom src.layers.bert import BertConfig, BertEncoder\n\ndef get_sensor_pred_model(args):\n    return Sensor_Pred_Head(args)\n\n\nclass Sensor_Pred_Head(torch.nn.Module):\n    \"\"\" This is the Control Signal Prediction head that performs sensor regression \"\"\"\n    def __init__(self, args):\n        \"\"\" Initializes the prediction head.\n        A simple transformer that performs sensor regression. \n        We simply use a transformer to regress the whole signals of a video, which is superficial and could be optimized to a large extent.\n        \"\"\"\n        super(Sensor_Pred_Head, self).__init__()\n\n        self.img_feature_dim = int(args.img_feature_dim)\n        self.use_grid_feat = args.grid_feat\n\n        # Motion Transformer implemented by bert\n        self.config = BertConfig.from_pretrained(args.config_name if args.config_name else \\\n            args.model_name_or_path, num_labels=2, finetuning_task='image_captioning')\n        self.encoder = BertEncoder(self.config)\n\n        # type number of control signals to be used\n        # TODO: Set this variable as an argument, corresponging to the control signal in dataloader\n        \n        self.sensor_dim = len(args.signal_types)\n        self.sensor_embedding = torch.nn.Linear(self.sensor_dim, self.config.hidden_size)\n        self.sensor_dropout = nn.Dropout(self.config.hidden_dropout_prob)\n\n        # a mlp to transform the dimension of video feature \n        self.img_dim = self.img_feature_dim\n        self.img_embedding = nn.Linear(self.img_dim, self.config.hidden_size, bias=True)\n        self.img_dropout = nn.Dropout(self.config.hidden_dropout_prob)\n\n        # a sample regression decoder\n        self.decoder = nn.Linear(self.config.hidden_size, self.sensor_dim)\n\n\n    def forward(self, *args, **kwargs):\n        \"\"\"The forward process.\n        Parameters:\n            img_feats: video features extracted by video swin\n            car_info: ground truth of control signals\n        \"\"\"\n        vid_feats = kwargs['img_feats']\n        car_info  = kwargs['car_info']\n\n        car_info = car_info.permute(0, 2, 1)\n\n        B, S, C = car_info.shape\n        assert C == self.sensor_dim, f\"{C}, {self.sensor_dim}\"\n        frame_num = S\n\n        img_embedding_output = self.img_embedding(vid_feats)\n        img_embedding_output = self.img_dropout(img_embedding_output)\n\n\n        extended_attention_mask = self.get_attn_mask(img_embedding_output)\n\n        encoder_outputs = self.encoder(img_embedding_output,\n                                        extended_attention_mask)\n        sequence_output = encoder_outputs[0][:, :frame_num, :]\n\n        pred_tensor = self.decoder(sequence_output)\n\n        loss = self.get_l2_loss(pred_tensor, car_info)\n\n        return loss, pred_tensor\n\n    def get_attn_mask(self, img_embedding_output):\n        \"\"\"Get attention mask that should be passed to motion transformer.\"\"\"\n        device = img_embedding_output.device\n        bsz = img_embedding_output.shape[0]\n        img_len = img_embedding_output.shape[1]\n\n\n        attention_mask = torch.ones((bsz, img_len, img_len), dtype=torch.long)\n\n\n        if attention_mask.dim() == 2:\n            extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n        elif attention_mask.dim() == 3:\n            extended_attention_mask = attention_mask.unsqueeze(1)\n        else:\n            raise NotImplementedError\n\n        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n        # masked positions, this operation will create a tensor which is 0.0 for\n        # positions we want to attend and -10000.0 for masked positions.\n        # Since we are adding it to the raw scores before the softmax, this is\n        # effectively the same as removing these entirely.\n        extended_attention_mask = extended_attention_mask.to(dtype=next(self.parameters()).dtype) # fp16 compatibility\n        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n\n        return extended_attention_mask.to(device)\n\n    def get_l2_loss(self, pred, targ):\n        loss_func = nn.MSELoss()\n        return loss_func(pred, targ)"}
{"type": "source_file", "path": "src/datasets/data_utils/volume_transforms.py", "content": "import numpy as np\nfrom PIL import Image\nimport torch\n\n\ndef my_convert_img(img):\n    \"\"\"Converts (H, W, C) numpy.ndarray to (C, W, H) format\n    \"\"\"\n    if len(img.shape) == 3:\n        img = img.transpose(2, 0, 1)\n    if len(img.shape) == 2:\n        img = np.expand_dims(img, 0)\n    return img\n\n\nclass ClipToTensor(object):\n    \"\"\"Convert a list of m (H x W x C) numpy.ndarrays in the range [0, 255]\n    to a torch.FloatTensor of shape (C x m x H x W) in the range [0, 1.0]\n    \"\"\"\n\n    def __init__(self, channel_nb=3, div_255=True, numpy=False):\n        self.channel_nb = channel_nb\n        self.div_255 = div_255\n        self.numpy = numpy\n\n    def __call__(self, clip):\n        \"\"\"\n        Args: clip (list of numpy.ndarray): clip (list of images)\n        to be converted to tensor.\n        \"\"\"\n        # Retrieve shape\n        if isinstance(clip[0], np.ndarray):\n            h, w, ch = clip[0].shape\n            assert ch == self.channel_nb, 'Got {0} instead of 3 channels'.format(\n                ch)\n        elif isinstance(clip[0], Image.Image):\n            w, h = clip[0].size\n        else:\n            raise TypeError('Expected numpy.ndarray or PIL.Image\\\n            but got list of {0}'.format(type(clip[0])))\n\n        np_clip = np.zeros([self.channel_nb, len(clip), int(h), int(w)])\n\n        # Convert\n        for img_idx, img in enumerate(clip):\n            if isinstance(img, np.ndarray):\n                pass\n            elif isinstance(img, Image.Image):\n                img = np.array(img, copy=False)\n            else:\n                raise TypeError('Expected numpy.ndarray or PIL.Image\\\n                but got list of {0}'.format(type(clip[0])))\n            img = my_convert_img(img)\n            np_clip[:, img_idx, :, :] = img\n        if self.numpy:\n            if self.div_255:\n                np_clip = np_clip / 255\n            return np_clip\n\n        else:\n            tensor_clip = torch.from_numpy(np_clip)\n\n            if not isinstance(tensor_clip, torch.FloatTensor):\n                tensor_clip = tensor_clip.float()\n            if self.div_255:\n                tensor_clip = tensor_clip.div(255)\n            return tensor_clip\n\n\nclass ToTensor(object):\n    \"\"\"Converts numpy array to tensor\n    \"\"\"\n\n    def __call__(self, array):\n        tensor = torch.from_numpy(array)\n        return tensor\n"}
{"type": "source_file", "path": "src/datasets/vision_language_tsv.py", "content": "\"\"\"\nCopyright (c) Microsoft Corporation.\nLicensed under the MIT license.\n\n\"\"\"\nimport torch\nimport torchvision.transforms as transforms\nimport cv2\nimport math\nimport json\nfrom PIL import Image\nimport os.path as op\nimport numpy as np\nfrom numpy.random import randint\nimport random\nfrom src.utils.tsv_file import TSVFile, CompositeTSVFile\nfrom src.utils.tsv_file_ops import tsv_reader\nfrom src.utils.load_files import load_linelist_file, load_from_yaml_file, find_file_path_in_yaml, load_box_linelist_file\nfrom .data_utils.image_ops import img_from_base64\nfrom .data_utils.video_ops import extract_frames_from_video_binary, extract_frames_from_video_path\nfrom src.utils.logger import LOGGER\nimport base64\nimport h5py\n# video_transforms & volume_transforms from https://github.com/hassony2/torch_videovision\nfrom .data_utils.video_transforms import Compose, Resize, RandomCrop, ColorJitter, Normalize, CenterCrop, RandomHorizontalFlip, RandomResizedCrop\nfrom .data_utils.volume_transforms import ClipToTensor\nimport code, time\n\nclass VisionLanguageTSVDataset(object):\n    def __init__(self, args, yaml_file, tokenizer, tensorizer=None, is_train=True, on_memory=False):\n\n        self.args = args\n        self.tokenizer = tokenizer\n        self.tensorizer = tensorizer\n\n        self.yaml_file = yaml_file\n        self.root = op.dirname(yaml_file)\n\n        self.cfg = load_from_yaml_file(yaml_file)\n        self.is_composite = self.cfg.get('composite', False)\n        self.cap_linelist_file = find_file_path_in_yaml(self.cfg.get('caption_linelist', None), self.root)\n\n        self.visual_file = self.cfg.get('img', None)\n        self.visual_tsv = self.get_tsv_file(self.visual_file)\n\n        self.label_file = self.cfg.get('label', None)\n        self.label_tsv = self.get_tsv_file(self.label_file)\n\n        self.cap_file = self.cfg.get('caption', None)\n        self.cap_tsv = self.get_tsv_file(self.cap_file)\n \n        if self.is_composite:\n            assert op.isfile(self.cap_linelist_file)\n            self.cap_line_list = [int(row[2]) for row in tsv_reader(self.cap_linelist_file)]\n            self.img_line_list = [i for i in range(len(self.cap_line_list))]\n        elif self.cap_linelist_file:\n            line_list = load_box_linelist_file(self.cap_linelist_file)\n            self.img_line_list = line_list[0]\n            self.cap_line_list = line_list[1]\n        else:\n            # one caption per image/video\n            self.img_line_list = [i for i in range(self.label_tsv.num_rows())]\n            self.cap_line_list = [0 for i in range(self.label_tsv.num_rows())]\n\n        if is_train:\n            assert self.cap_tsv is not None\n            assert tokenizer is not None\n\n        self.is_train = is_train\n        self.image_keys = self.prepare_image_keys()\n        self.key2index = self.prepare_image_key_to_index()\n        self.on_memory = on_memory\n        if on_memory:\n            if self.cap_tsv is not None:\n                self.load_caption_to_memory()\n\n        self.is_train = is_train\n        self.img_res = getattr(args, 'img_res', 224)\n        self.patch_size = getattr(args, 'patch_size', 16)\n\n        self.img_feature_dim = args.img_feature_dim\n        self.decoder_target_fps = 3\n        self.decoder_num_frames = getattr(args, 'max_num_frames', 2)\n        self.decoder_multi_thread_decode = False\n\n        self.decoder_safeguard_duration = False\n        self.add_od_labels = getattr(args, 'add_od_labels', False)\n        self.use_asr = getattr(args, 'use_asr', False)\n        self.use_sep_cap = getattr(args, 'use_sep_cap', False)\n        self.use_swap_cap = getattr(args, 'use_swap_cap', False)\n\n        self.use_car_sensor = getattr(args, 'use_car_sensor', False)\n        self.multitask = getattr(args, 'multitask', False)\n        self.only_signal = getattr(args, 'only_signal', False)\n\n        self.signal_types = getattr(args, 'signal_types', ['course', 'speed'])\n\n        if self.multitask or self.only_signal:\n            assert len(self.signal_types) != 0\n\n        LOGGER.info(f'Use_asr: {self.use_asr}')\n        # use uniform sampling as default for now\n        self.decoder_sampling_strategy = getattr(args, 'decoder_sampling_strategy', 'uniform')\n        LOGGER.info(f'isTrainData: {self.is_train}\\n[PyAV video parameters] '\n                    f'Num of Frame: {self.decoder_num_frames}, '\n                    f'FPS: {self.decoder_target_fps}, '\n                    f'Sampling: {self.decoder_sampling_strategy}')\n        # Initialize video transforms\n        # adapt from https://github.com/hassony2/torch_videovision\n\n        if is_train==True:\n            self.raw_video_crop_list = [\n                Resize(self.img_res),\n                RandomCrop((self.img_res,self.img_res)),\n                ClipToTensor(channel_nb=3),\n                Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n            ]\n        else:\n            self.raw_video_crop_list = [\n                Resize(self.img_res),\n                CenterCrop((self.img_res,self.img_res)),\n                ClipToTensor(channel_nb=3),\n                Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n            ]            \n        self.raw_video_prcoess = Compose(self.raw_video_crop_list)\n\n    def get_composite_source_idx(self):\n        if self.is_composite:\n            assert op.isfile(self.cap_linelist_file)\n            self.composite_source_idx = [int(row[0]) for row in tsv_reader(self.cap_linelist_file)]\n        else:\n            # only a single tsv file is used as input\n            self.composite_source_idx = [0 for _ in range(len(self.cap_line_list))]\n        return self.composite_source_idx\n\n    def get_tsv_file(self, tsv_file):\n        if tsv_file:\n            if self.is_composite:\n                return CompositeTSVFile(tsv_file, self.cap_linelist_file, root=self.root)\n            tsv_path = find_file_path_in_yaml(tsv_file, self.root)\n            return TSVFile(tsv_path)\n\n    def load_caption_to_memory(self):\n        self.caption_on_memory = {}\n        for img_idx in set(self.img_line_list):\n            row = self.get_row_from_tsv(self.cap_tsv, img_idx)\n            for cap_idx, data in enumerate(json.loads(row[1])):\n                self.caption_on_memory[(img_idx, cap_idx)] = data['caption']\n\n    def get_valid_tsv(self):\n        if self.is_train:\n            return self.cap_tsv\n        # sorted by file size\n        if self.cap_tsv:\n            return self.cap_tsv\n        if self.visual_tsv:\n            return self.visual_tsv\n\n    def prepare_image_keys(self):\n        tsv = self.get_valid_tsv()\n        return [tsv.get_key(i) for i in range(tsv.num_rows())]\n        \n    def prepare_image_key_to_index(self):\n        tsv = self.get_valid_tsv()\n        return {tsv.get_key(i) : i for i in range(tsv.num_rows())}\n\n    def get_image_cap_index(self, idx):\n        return self.img_line_list[idx], self.cap_line_list[idx]\n\n    def get_row_from_tsv(self, tsv, img_idx):\n        row = tsv[img_idx]\n        if self.is_composite:\n            assert self.image_keys[img_idx].endswith(row[0])\n        else:\n            # print(row[0], self.image_keys[img_idx])\n            assert row[0].split('/')[0] == self.image_keys[img_idx].split('_')[-1] or row[0].split('_')[-1] == self.image_keys[img_idx].split('_')[-1]\n            # print(row)\n            # print()\n            # print(row[0].split('/')[0])\n            # print(row[0].split('_')[-1])\n            # print(self.image_keys[img_idx].split('_')[-1])\n            # from IPython import embed\n            # embed()\n        return row\n\n    def get_caption(self, img_idx, cap_idx):\n        if self.is_train:\n            if self.on_memory:\n                return self.caption_on_memory[(img_idx, cap_idx)]\n            row = self.get_row_from_tsv(self.cap_tsv, img_idx)\n            return json.loads(row[1])[cap_idx]['caption']\n        return \"\"\n\n    def get_caption_and_timeinfo(self, data, cap_idx):\n        caption, tag, start, end = '', ' ', None, None\n        data_sample = data[cap_idx]\n        if self.is_train:\n            if self.use_sep_cap:\n                caption = (data_sample['action'], data_sample['justification'])\n                if self.use_swap_cap:\n                    caption = (data_sample['justification'], data_sample['action'])\n            else:\n                caption = data_sample['caption']\n            if 'start' in data_sample.keys():\n                start = data_sample['start']\n            if 'end' in data_sample.keys():\n                end = data_sample['end']\n            if 'asr' in data_sample.keys() and self.use_asr:\n                asr = data_sample['asr'].lower()\n                tag = asr\n        else:\n            if self.use_sep_cap:\n                caption = ('','')\n            if 'start' in data_sample.keys():\n                start = data_sample['start']\n            if 'end' in data_sample.keys():\n                end = data_sample['end']\n            if 'asr' in data_sample.keys() and self.use_asr:\n                asr = data_sample['asr'].lower()\n                tag = asr\n        return caption, tag, start, end\n\n    def get_caption_and_timeinfo_wrapper(self, img_idx, cap_idx):\n        row = self.get_row_from_tsv(self.cap_tsv, img_idx)\n        data_sample = json.loads(row[1])\n        caption, asr_or_tag, start, end = self.get_caption_and_timeinfo(data_sample, cap_idx) \n        return caption, asr_or_tag, start, end\n\n    def get_caption_file_in_coco_format(self):\n        # for evaluation\n        cap_file_coco_format = find_file_path_in_yaml(self.cfg.get('caption_coco_format', \n            None), self.root)\n        if cap_file_coco_format:\n            return cap_file_coco_format\n        test_split = op.basename(self.yaml_file).split('.')[0]\n        return op.join(self.root, test_split + '_caption_coco_format.json')\n\n    def get_captions_by_key(self, key):\n        # get a list of captions for image (by key)\n        img_idx = self.key2index[key]\n        cap_info = json.loads(self.cap_tsv[img_idx][1])\n        return [c['caption'] for c in cap_info]\n\n    def get_video_key(self, idx):\n        # line_no = self.get_line_no(idx)\n        # return self.label_tsv[line_no][0]\n        return self.get_row_from_tsv(self.label_tsv, idx)[0]\n\n    def apply_augmentations(self, frames):\n        # if failed to decode video, generate fake frames (should be corner case)\n        if frames is None:\n            frames = np.zeros((self.decoder_num_frames,self.img_res,self.img_res,3)).astype(np.uint8)\n        # (T, C, H, W) -> (T, H, W, C), channel is RGB\n        elif 'torch' in str(frames.dtype):\n            frames = frames.numpy()\n            frames = np.transpose(frames, (0, 2, 3, 1))\n        else:\n            frames = frames.astype(np.uint8)\n            frames = np.transpose(frames, (0, 2, 3, 1))\n        num_of_frames, height, width, channels = frames.shape\n\n        frame_list = []\n        for i in range(self.decoder_num_frames):\n            if num_of_frames==1: \n                # if it is from image-caption dataset, we duplicate the image\n                # convert numpy to PIL format, compatible to augmentation operations\n                frame_list.append(Image.fromarray(frames[0]))\n            else:\n                # if it is from video-caption dataset, we add each frame to the list\n                # convert numpy to PIL format, compatible to augmentation operations\n                frame_list.append(Image.fromarray(frames[i]))\n        \n        # adapt from torch_videovision: https://github.com/hassony2/torch_videovision\n        # after augmentation, output tensor (C x T x H x W) in the range [0, 1.0]\n        crop_frames = self.raw_video_prcoess(frame_list)\n        # (C x T x H x W) --> (T x C x H x W)\n        crop_frames = crop_frames.permute(1, 0, 2, 3)\n        return crop_frames \n\n    def get_image(self, bytestring): \n        # output numpy array (T, C, H, W), channel is RGB, T = 1\n        cv2_im = img_from_base64(bytestring)\n        cv2_im = cv2_im[:,:,::-1] # COLOR_BGR2RGB\n        # cv2_im = cv2.cvtColor(cv2_im, cv2.COLOR_BGR2RGB)\n        output = np.transpose(cv2_im[np.newaxis, ...], (0, 3, 1, 2))\n        return output\n\n    def get_frames_from_tsv(self, binary_frms):\n        # get pre-extracted video frames from tsv files\n        frames = []\n        _C, _H, _W = 3, 224, 224\n        if self.decoder_num_frames > len(binary_frms):\n            print(f\"Corrupt videos, requested {self.decoder_num_frames} frames, \"\n                  f\"but got only {len(binary_frms)} frames, will return all zeros instead\")\n            return np.zeros((self.decoder_num_frames, _C, _H, _W), dtype=np.int64)\n\n        def sampling(start,end,n):\n            if n == 1:\n                return [int(round((start+end)/2.))]\n            if n < 1:\n                raise Exception(\"behaviour not defined for n<2\")\n            step = (end-start)/float(n-1)\n            return [int(round(start+x*step)) for x in range(n)]\n\n        for i in sampling(0, len(binary_frms)-1, self.decoder_num_frames):\n            try:\n                image = self.get_image(binary_frms[i])\n            except Exception as e:\n                print(f\"Corrupt frame at {i}\")\n                image = np.zeros((1, _C, _H, _W), dtype=np.int64)\n            _, _C, _H, _W = image.shape\n            frames.append(image)\n        return np.vstack(frames)\n\n    def decode_and_get_frames(self, clip_path_name, start=None, end=None):\n        # online decode raw video file, and get video frames\n        # output tensor (T, C, H, W), channel is RGB, T = self.decoder_num_frames\n        if 'TVC' in clip_path_name:\n            # default clip_path_name: datasets/TVC/videos/{tv_show}/{tv_show}_clips/{tv_show}_{seasoninfo}/{video_id}.mp4_{start_time}_{end_time}\n            # To load video file, we will need to remove start&end info here\n            resolved_video_path = '_'.join(clip_path_name.split('_')[0:-2])\n        else: # VATEX, MSVD, MSRVTT, Youcook2\n            resolved_video_path = clip_path_name\n        frames, video_max_pts = extract_frames_from_video_path(resolved_video_path,\n                                                self.decoder_target_fps,\n                                                self.decoder_num_frames,\n                                                self.decoder_multi_thread_decode,\n                                                self.decoder_sampling_strategy,\n                                                self.decoder_safeguard_duration,\n                                                start, end)\n        return frames\n\n    def get_visual_data(self, idx, start=None, end=None):\n        row = self.get_row_from_tsv(self.visual_tsv, idx)\n        # if the input is a video tsv with only video file paths, \n        # extract video frames on-the-fly, and return a video-frame tensor\n        if row[0] == row[-1]: \n            return self.decode_and_get_frames(row[-1], start, end), True\n        # if the input is a video tsv with frames pre-extracted,\n        # return a video-frame tensor\n        elif len(row) >= self.decoder_num_frames + 2:\n            return self.get_frames_from_tsv(row[2:]), True\n        # if the input is a image tsv, return image numpy array\n        else: \n            return self.get_image(row[-1]), False\n\n    def get_car_info(self, img_key):\n        def sampling(start,end,n):\n            if n == 1:\n                return [int(round((start+end)/2.))]\n            if n < 1:\n                raise Exception(\"behaviour not defined for n<2\")\n            step = (end-start)/float(n-1)\n            return [int(round(start+x*step)) for x in range(n)]\n\n        sensor_type_num = len(self.signal_types)\n        # all_choices = ['course', 'curvature', 'accelerator', 'speed']\n        all_choices = self.signal_types\n        infos = []\n        info_path = op.join(self.root, \"processed_video_info\", img_key+\".h5\")\n        if not op.exists(info_path):\n            # only happens when testing\n            # print(f\"Not existed {info_path}\")\n            return -1*torch.ones((sensor_type_num, self.decoder_num_frames))\n        # print(f\"Loading {info_path}\")\n        all_infos = h5py.File(info_path)\n\n        info_meta_data = torch.zeros((sensor_type_num, self.decoder_num_frames))\n        for key in all_infos.keys():\n            samp_id = sampling(0, all_infos[key].shape[0]-1, self.decoder_num_frames)\n            if key == 'speed' and key in all_choices:\n                infos.append(torch.tensor(all_infos[key], dtype=torch.float32)[samp_id])\n            if key == 'course' and key in all_choices:\n                courses = torch.tensor(all_infos[key], dtype=torch.float32)[samp_id]\n                new_courses=[]\n                for i in range(len(samp_id)):\n                    if i ==0:\n                        new_courses.append(0)\n                    else:\n                        if courses[i]-courses[i-1] <= -180:\n                            new_courses.append(courses[i]-courses[i-1]+360)\n                        elif courses[i]-courses[i-1] > 180:\n                            new_courses.append(courses[i]-courses[i-1]-360)\n                        else:\n                            new_courses.append(courses[i]-courses[i-1])\n                infos.append(torch.tensor(new_courses, dtype=torch.float32))\n            # assert all_infos[key].shape[0] == self.decoder_num_frames, \"tensor infos get wrong\"\n            if key == 'curvature' and key in all_choices:\n                infos.append(torch.tensor(all_infos['curvature'], dtype=torch.float32))\n            if key == 'accelerator' and key in all_choices:\n                infos.append(torch.tensor(all_infos['accelerator'], dtype=torch.float32))\n        info_meta_data = torch.stack(infos, dim=0)\n        assert info_meta_data.shape == (sensor_type_num, self.decoder_num_frames)\n        return info_meta_data\n\n    def __len__(self):\n        return len(self.img_line_list)\n\n    def __getitem__(self, idx):\n        if self.args.debug_speed:\n            idx = idx % self.args.effective_batch_size\n        img_idx, cap_idx = self.get_image_cap_index(idx)\n        img_key = self.image_keys[img_idx]\n        caption_sample, tag, start, end = self.get_caption_and_timeinfo_wrapper(img_idx, cap_idx)\n        car_infos = 0\n        if self.use_car_sensor or self.multitask or self.only_signal:\n            car_infos = self.get_car_info(img_key)\n        # get image or video frames\n        # frames: (T, C, H, W),  is_video: binary tag\n        raw_frames, is_video = self.get_visual_data(img_idx, start, end) \n\n        # apply augmentation. frozen-in-time if the input is an image\n        # preproc_frames: (T, C, H, W), C = 3, H = W = self.img_res, channel is RGB   \n        preproc_frames = self.apply_augmentations(raw_frames)\n        \n        # tokenize caption and generate attention maps\n        # it will consider only # of visual tokens for building attention maps. # is args.max_img_seq_length \n        if isinstance(caption_sample, dict):\n            caption = caption_sample[\"caption\"]\n        else:\n            caption = caption_sample\n            caption_sample = None\n\n        if self.args.add_od_labels==True:\n            example = self.tensorizer.tensorize_example_e2e(caption, preproc_frames, text_b=tag, text_meta=caption_sample)\n        elif self.args.use_sep_cap==True:\n            example = self.tensorizer.tensorize_example_e2e(caption[0], preproc_frames, text_b=caption[1], text_meta=caption_sample)\n        else:\n            example = self.tensorizer.tensorize_example_e2e(caption, preproc_frames, text_meta=caption_sample)\n\n        # preparing outputs\n        meta_data = {}\n        meta_data['caption'] = caption # raw text data, not tokenized\n        meta_data['img_key'] = img_key\n        meta_data['is_video'] = is_video # True: video data, False: image data\n        meta_data['tag'] = tag\n        example =  example + (car_infos,)\n\n        return img_key, example, meta_data\n\nclass VisionLanguageTSVYamlDataset(VisionLanguageTSVDataset):\n    \"\"\" TSVDataset taking a Yaml file for easy function call\n    \"\"\"\n    def __init__(self, args, yaml_file, tokenizer, tensorizer=None, is_train=True, on_memory=False):\n        # print('Init video/image captioning dataloader...')\n        super(VisionLanguageTSVYamlDataset, self).__init__(\n            args, yaml_file, tokenizer, tensorizer, is_train, on_memory)\n"}
{"type": "source_file", "path": "src/modeling/swin/config.py", "content": "# --------------------------------------------------------\n# Swin Transformer\n# Copyright (c) 2021 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ze Liu\n# --------------------------------------------------------'\n\nimport os\nimport yaml\nfrom yacs.config import CfgNode as CN\n\n_C = CN()\n\n# Base config files\n_C.BASE = ['']\n\n# -----------------------------------------------------------------------------\n# Data settings\n# -----------------------------------------------------------------------------\n_C.DATA = CN()\n# Batch size for a single GPU, could be overwritten by command line argument\n_C.DATA.BATCH_SIZE = 128\n# Path to dataset, could be overwritten by command line argument\n_C.DATA.DATA_PATH = ''\n# Dataset name\n_C.DATA.DATASET = 'imagenet'\n# Input image size\n_C.DATA.IMG_SIZE = 224\n# Interpolation to resize image (random, bilinear, bicubic)\n_C.DATA.INTERPOLATION = 'bicubic'\n# Use zipped dataset instead of folder dataset\n# could be overwritten by command line argument\n_C.DATA.ZIP_MODE = False\n# Cache Data in Memory, could be overwritten by command line argument\n_C.DATA.CACHE_MODE = 'part'\n# Pin CPU memory in DataLoader for more efficient (sometimes) transfer to GPU.\n_C.DATA.PIN_MEMORY = True\n# Number of data loading threads\n_C.DATA.NUM_WORKERS = 8\n\n# -----------------------------------------------------------------------------\n# Model settings\n# -----------------------------------------------------------------------------\n_C.MODEL = CN()\n# Model type\n_C.MODEL.TYPE = 'swin'\n# Model name\n_C.MODEL.NAME = 'swin_tiny_patch4_window7_224'\n# Checkpoint to resume, could be overwritten by command line argument\n_C.MODEL.RESUME = ''\n# Number of classes, overwritten in data preparation\n_C.MODEL.NUM_CLASSES = 1000\n# Dropout rate\n_C.MODEL.DROP_RATE = 0.0\n# Drop path rate\n_C.MODEL.DROP_PATH_RATE = 0.1\n# Label Smoothing\n_C.MODEL.LABEL_SMOOTHING = 0.1\n\n# Swin Transformer parameters\n_C.MODEL.SWIN = CN()\n_C.MODEL.SWIN.PATCH_SIZE = 4\n_C.MODEL.SWIN.IN_CHANS = 3\n_C.MODEL.SWIN.EMBED_DIM = 96\n_C.MODEL.SWIN.DEPTHS = [2, 2, 6, 2]\n_C.MODEL.SWIN.NUM_HEADS = [3, 6, 12, 24]\n_C.MODEL.SWIN.WINDOW_SIZE = 7\n_C.MODEL.SWIN.MLP_RATIO = 4.\n_C.MODEL.SWIN.QKV_BIAS = True\n_C.MODEL.SWIN.QK_SCALE = None\n_C.MODEL.SWIN.APE = False\n_C.MODEL.SWIN.PATCH_NORM = True\n\n# -----------------------------------------------------------------------------\n# Training settings\n# -----------------------------------------------------------------------------\n_C.TRAIN = CN()\n_C.TRAIN.START_EPOCH = 0\n_C.TRAIN.EPOCHS = 300\n_C.TRAIN.WARMUP_EPOCHS = 20\n_C.TRAIN.WEIGHT_DECAY = 0.05\n_C.TRAIN.BASE_LR = 5e-4\n_C.TRAIN.WARMUP_LR = 5e-7\n_C.TRAIN.MIN_LR = 5e-6\n# Clip gradient norm\n_C.TRAIN.CLIP_GRAD = 5.0\n# Auto resume from latest checkpoint\n_C.TRAIN.AUTO_RESUME = True\n# Gradient accumulation steps\n# could be overwritten by command line argument\n_C.TRAIN.ACCUMULATION_STEPS = 0\n# Whether to use gradient checkpointing to save memory\n# could be overwritten by command line argument\n_C.TRAIN.USE_CHECKPOINT = False\n\n# LR scheduler\n_C.TRAIN.LR_SCHEDULER = CN()\n_C.TRAIN.LR_SCHEDULER.NAME = 'cosine'\n# Epoch interval to decay LR, used in StepLRScheduler\n_C.TRAIN.LR_SCHEDULER.DECAY_EPOCHS = 30\n# LR decay rate, used in StepLRScheduler\n_C.TRAIN.LR_SCHEDULER.DECAY_RATE = 0.1\n\n# Optimizer\n_C.TRAIN.OPTIMIZER = CN()\n_C.TRAIN.OPTIMIZER.NAME = 'adamw'\n# Optimizer Epsilon\n_C.TRAIN.OPTIMIZER.EPS = 1e-8\n# Optimizer Betas\n_C.TRAIN.OPTIMIZER.BETAS = (0.9, 0.999)\n# SGD momentum\n_C.TRAIN.OPTIMIZER.MOMENTUM = 0.9\n\n# -----------------------------------------------------------------------------\n# Augmentation settings\n# -----------------------------------------------------------------------------\n_C.AUG = CN()\n# Color jitter factor\n_C.AUG.COLOR_JITTER = 0.4\n# Use AutoAugment policy. \"v0\" or \"original\"\n_C.AUG.AUTO_AUGMENT = 'rand-m9-mstd0.5-inc1'\n# Random erase prob\n_C.AUG.REPROB = 0.25\n# Random erase mode\n_C.AUG.REMODE = 'pixel'\n# Random erase count\n_C.AUG.RECOUNT = 1\n# Mixup alpha, mixup enabled if > 0\n_C.AUG.MIXUP = 0.8\n# Cutmix alpha, cutmix enabled if > 0\n_C.AUG.CUTMIX = 1.0\n# Cutmix min/max ratio, overrides alpha and enables cutmix if set\n_C.AUG.CUTMIX_MINMAX = None\n# Probability of performing mixup or cutmix when either/both is enabled\n_C.AUG.MIXUP_PROB = 1.0\n# Probability of switching to cutmix when both mixup and cutmix enabled\n_C.AUG.MIXUP_SWITCH_PROB = 0.5\n# How to apply mixup/cutmix params. Per \"batch\", \"pair\", or \"elem\"\n_C.AUG.MIXUP_MODE = 'batch'\n\n# -----------------------------------------------------------------------------\n# Testing settings\n# -----------------------------------------------------------------------------\n_C.TEST = CN()\n# Whether to use center crop when testing\n_C.TEST.CROP = True\n\n# -----------------------------------------------------------------------------\n# Misc\n# -----------------------------------------------------------------------------\n# Mixed precision opt level, if O0, no amp is used ('O0', 'O1', 'O2')\n# overwritten by command line argument\n_C.AMP_OPT_LEVEL = ''\n# Path to output folder, overwritten by command line argument\n_C.OUTPUT = ''\n# Tag of experiment, overwritten by command line argument\n_C.TAG = 'default'\n# Frequency to save checkpoint\n_C.SAVE_FREQ = 1\n# Frequency to logging info\n_C.PRINT_FREQ = 10\n# Fixed random seed\n_C.SEED = 0\n# Perform evaluation only, overwritten by command line argument\n_C.EVAL_MODE = False\n# Test throughput only, overwritten by command line argument\n_C.THROUGHPUT_MODE = False\n# local rank for DistributedDataParallel, given by command line argument\n_C.LOCAL_RANK = 0\n\n\ndef _update_config_from_file(config, cfg_file):\n    config.defrost()\n    with open(cfg_file, 'r') as f:\n        yaml_cfg = yaml.load(f, Loader=yaml.FullLoader)\n\n    for cfg in yaml_cfg.setdefault('BASE', ['']):\n        if cfg:\n            _update_config_from_file(\n                config, os.path.join(os.path.dirname(cfg_file), cfg)\n            )\n    print('=> merge config from {}'.format(cfg_file))\n    config.merge_from_file(cfg_file)\n    config.freeze()\n\n\ndef update_config(config, yaml_file):\n    _update_config_from_file(config, yaml_file)\n\n    config.defrost()\n    # if args.opts:\n    #     config.merge_from_list(args.opts)\n\n    # # merge from specific arguments\n    # if args.batch_size:\n    #     config.DATA.BATCH_SIZE = args.batch_size\n    # if args.data_path:\n    #     config.DATA.DATA_PATH = args.data_path\n    # if args.zip:\n    #     config.DATA.ZIP_MODE = True\n    # if args.cache_mode:\n    #     config.DATA.CACHE_MODE = args.cache_mode\n    # if args.resume:\n    #     config.MODEL.RESUME = args.resume\n    # if args.accumulation_steps:\n    #     config.TRAIN.ACCUMULATION_STEPS = args.accumulation_steps\n    # if args.use_checkpoint:\n    #     config.TRAIN.USE_CHECKPOINT = True\n    # if args.amp_opt_level:\n    #     config.AMP_OPT_LEVEL = args.amp_opt_level\n    # if args.output:\n    #     config.OUTPUT = args.output\n    # if args.tag:\n    #     config.TAG = args.tag\n    # if args.eval:\n    #     config.EVAL_MODE = True\n    # if args.throughput:\n    #     config.THROUGHPUT_MODE = True\n\n    # # set local rank for distributed training\n    # config.LOCAL_RANK = args.local_rank\n\n    # # output folder\n    # config.OUTPUT = os.path.join(config.OUTPUT, config.MODEL.NAME, config.TAG)\n\n    config.freeze()\n\n\ndef get_config(yaml_file):\n    \"\"\"Get a yacs CfgNode object with default values.\"\"\"\n    # Return a clone so that the defaults will not be altered\n    # This is for the \"local variable\" use pattern\n    config = _C.clone()\n    update_config(config, yaml_file)\n\n    return config\n"}
{"type": "source_file", "path": "src/layers/bert/tokenization_bert.py", "content": "# coding=utf-8\n# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Tokenization classes.\"\"\"\n\nfrom __future__ import absolute_import, division, print_function, unicode_literals\n\nimport collections\nimport os\nimport unicodedata\nfrom io import open\nfrom random import randint\n\nfrom .tokenization_utils import PreTrainedTokenizer, clean_up_tokenization\n\nimport logging\nfrom src.utils.comm import is_main_process\nlogger = logging.getLogger(__name__)  # pylint: disable=invalid-name\nif not is_main_process():\n    logger.disabled = True\n\nVOCAB_FILES_NAMES = {'vocab_file': 'vocab.txt'}\n\nPRETRAINED_VOCAB_FILES_MAP = {\n    'vocab_file':\n    {\n        'bert-base-uncased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt\",\n        'bert-large-uncased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-vocab.txt\",\n        'bert-base-cased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt\",\n        'bert-large-cased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased-vocab.txt\",\n        'bert-base-multilingual-uncased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt\",\n        'bert-base-multilingual-cased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-vocab.txt\",\n        'bert-base-chinese': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-vocab.txt\",\n        'bert-base-german-cased': \"https://int-deepset-models-bert.s3.eu-central-1.amazonaws.com/pytorch/bert-base-german-cased-vocab.txt\",\n        'bert-large-uncased-whole-word-masking': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-whole-word-masking-vocab.txt\",\n        'bert-large-cased-whole-word-masking': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased-whole-word-masking-vocab.txt\",\n        'bert-large-uncased-whole-word-masking-finetuned-squad': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-whole-word-masking-finetuned-squad-vocab.txt\",\n        'bert-large-cased-whole-word-masking-finetuned-squad': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased-whole-word-masking-finetuned-squad-vocab.txt\",\n        'bert-base-cased-finetuned-mrpc': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-finetuned-mrpc-vocab.txt\",\n    }\n}\n\nPRETRAINED_POSITIONAL_EMBEDDINGS_SIZES = {\n    'bert-base-uncased': 512,\n    'bert-large-uncased': 512,\n    'bert-base-cased': 512,\n    'bert-large-cased': 512,\n    'bert-base-multilingual-uncased': 512,\n    'bert-base-multilingual-cased': 512,\n    'bert-base-chinese': 512,\n    'bert-base-german-cased': 512,\n    'bert-large-uncased-whole-word-masking': 512,\n    'bert-large-cased-whole-word-masking': 512,\n    'bert-large-uncased-whole-word-masking-finetuned-squad': 512,\n    'bert-large-cased-whole-word-masking-finetuned-squad': 512,\n    'bert-base-cased-finetuned-mrpc': 512,\n}\n\ndef load_vocab(vocab_file):\n    \"\"\"Loads a vocabulary file into a dictionary.\"\"\"\n    vocab = collections.OrderedDict()\n    with open(vocab_file, \"r\", encoding=\"utf-8\") as reader:\n        tokens = reader.readlines()\n    for index, token in enumerate(tokens):\n        token = token.rstrip('\\n')\n        vocab[token] = index\n    return vocab\n\n\ndef whitespace_tokenize(text):\n    \"\"\"Runs basic whitespace cleaning and splitting on a piece of text.\"\"\"\n    text = text.strip()\n    if not text:\n        return []\n    tokens = text.split()\n    return tokens\n\n\nclass BertTokenizer(PreTrainedTokenizer):\n    r\"\"\"\n    Constructs a BertTokenizer.\n    :class:`~pytorch_pretrained_bert.BertTokenizer` runs end-to-end tokenization: punctuation splitting + wordpiece\n\n    Args:\n        vocab_file: Path to a one-wordpiece-per-line vocabulary file\n        do_lower_case: Whether to lower case the input. Only has an effect when do_wordpiece_only=False\n        do_basic_tokenize: Whether to do basic tokenization before wordpiece.\n        max_len: An artificial maximum length to truncate tokenized sequences to; Effective maximum length is always the\n            minimum of this value (if specified) and the underlying BERT model's sequence length.\n        never_split: List of tokens which will never be split during tokenization. Only has an effect when\n            do_wordpiece_only=False\n    \"\"\"\n\n    vocab_files_names = VOCAB_FILES_NAMES\n    pretrained_vocab_files_map = PRETRAINED_VOCAB_FILES_MAP\n    max_model_input_sizes = PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES\n\n    def __init__(self, vocab_file, do_lower_case=True, do_basic_tokenize=True, never_split=None,\n                 unk_token=\"[UNK]\", sep_token=\"[SEP]\", pad_token=\"[PAD]\", cls_token=\"[CLS]\",\n                 mask_token=\"[MASK]\", tokenize_chinese_chars=True, **kwargs):\n        \"\"\"Constructs a BertTokenizer.\n\n        Args:\n            **vocab_file**: Path to a one-wordpiece-per-line vocabulary file\n            **do_lower_case**: (`optional`) boolean (default True)\n                Whether to lower case the input\n                Only has an effect when do_basic_tokenize=True\n            **do_basic_tokenize**: (`optional`) boolean (default True)\n                Whether to do basic tokenization before wordpiece.\n            **never_split**: (`optional`) list of string\n                List of tokens which will never be split during tokenization.\n                Only has an effect when do_basic_tokenize=True\n            **tokenize_chinese_chars**: (`optional`) boolean (default True)\n                Whether to tokenize Chinese characters.\n                This should likely be desactivated for Japanese:\n                see: https://github.com/huggingface/pytorch-pretrained-BERT/issues/328\n        \"\"\"\n        super(BertTokenizer, self).__init__(unk_token=unk_token, sep_token=sep_token,\n                                            pad_token=pad_token, cls_token=cls_token,\n                                            mask_token=mask_token, **kwargs)\n        if not os.path.isfile(vocab_file):\n            raise ValueError(\n                \"Can't find a vocabulary file at path '{}'. To load the vocabulary from a Google pretrained \"\n                \"model use `tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)`\".format(vocab_file))\n        self.vocab = load_vocab(vocab_file)\n        self.ids_to_tokens = collections.OrderedDict(\n            [(ids, tok) for tok, ids in self.vocab.items()])\n        self.do_basic_tokenize = do_basic_tokenize\n        if do_basic_tokenize:\n            self.basic_tokenizer = BasicTokenizer(do_lower_case=do_lower_case,\n                                                  never_split=never_split,\n                                                  tokenize_chinese_chars=tokenize_chinese_chars)\n        self.wordpiece_tokenizer = WordpieceTokenizer(vocab=self.vocab, unk_token=self.unk_token)\n\n    @property\n    def vocab_size(self):\n        return len(self.vocab)\n\n    def _tokenize(self, text):\n        split_tokens = []\n        if self.do_basic_tokenize:\n            for token in self.basic_tokenizer.tokenize(text, never_split=self.all_special_tokens):\n                for sub_token in self.wordpiece_tokenizer.tokenize(token):\n                    split_tokens.append(sub_token)\n        else:\n            split_tokens = self.wordpiece_tokenizer.tokenize(text)\n        return split_tokens\n\n    def _tokenize_for_pos_tag(self, text):\n        split_tokens = []\n        basic_tokens = []\n        sub_to_token_idx_map = []\n        if self.do_basic_tokenize:\n            for idx, token in enumerate(\n                    self.basic_tokenizer.tokenize(text, never_split=self.all_special_tokens)):\n                basic_tokens.append(token)\n                for sub_token in self.wordpiece_tokenizer.tokenize(token):\n                    split_tokens.append(sub_token)\n                    sub_to_token_idx_map.append(idx)\n            return (split_tokens, basic_tokens, sub_to_token_idx_map)\n        else:\n            raise ValueError(f\"_tokenize_for_pos_tag must set self.do_basic_tokenize as True\")\n\n    def _convert_token_to_id(self, token):\n        \"\"\" Converts a token (str/unicode) in an id using the vocab. \"\"\"\n        return self.vocab.get(token, self.vocab.get(self.unk_token))\n\n    def _convert_id_to_token(self, index):\n        \"\"\"Converts an index (integer) in a token (string/unicode) using the vocab.\"\"\"\n        return self.ids_to_tokens.get(index, self.unk_token)\n\n    def convert_tokens_to_string(self, tokens):\n        \"\"\" Converts a sequence of tokens (string) in a single string. \"\"\"\n        out_string = ' '.join(tokens).replace(' ##', '').strip()\n        return out_string\n\n    def save_vocabulary(self, vocab_path):\n        \"\"\"Save the tokenizer vocabulary to a directory or file.\"\"\"\n        index = 0\n        if os.path.isdir(vocab_path):\n            vocab_file = os.path.join(vocab_path, VOCAB_FILES_NAMES['vocab_file'])\n        with open(vocab_file, \"w\", encoding=\"utf-8\") as writer:\n            for token, token_index in sorted(self.vocab.items(), key=lambda kv: kv[1]):\n                if index != token_index:\n                    logger.warning(\"Saving vocabulary to {}: vocabulary indices are not consecutive.\"\n                                   \" Please check that the vocabulary is not corrupted!\".format(vocab_file))\n                    index = token_index\n                writer.write(token + u'\\n')\n                index += 1\n        return (vocab_file,)\n\n    def get_random_token(self):\n        i = randint(0, len(self.vocab))\n        return self._convert_id_to_token(i)\n\n    @classmethod\n    def from_pretrained(cls, pretrained_model_name_or_path, *inputs, **kwargs):\n        \"\"\" Instantiate a BertTokenizer from pre-trained vocabulary files.\n        \"\"\"\n        if pretrained_model_name_or_path in PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES:\n            if '-cased' in pretrained_model_name_or_path and kwargs.get('do_lower_case', True):\n                logger.warning(\"The pre-trained model you are loading is a cased model but you have not set \"\n                               \"`do_lower_case` to False. We are setting `do_lower_case=False` for you but \"\n                               \"you may want to check this behavior.\")\n                kwargs['do_lower_case'] = False\n            elif '-cased' not in pretrained_model_name_or_path and not kwargs.get('do_lower_case', True):\n                logger.warning(\"The pre-trained model you are loading is an uncased model but you have set \"\n                               \"`do_lower_case` to False. We are setting `do_lower_case=True` for you \"\n                               \"but you may want to check this behavior.\")\n                kwargs['do_lower_case'] = True\n\n        return super(BertTokenizer, cls)._from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)\n\n\nclass BasicTokenizer(object):\n    \"\"\"Runs basic tokenization (punctuation splitting, lower casing, etc.).\"\"\"\n\n    def __init__(self, do_lower_case=True, never_split=None, tokenize_chinese_chars=True):\n        \"\"\" Constructs a BasicTokenizer.\n\n        Args:\n            **do_lower_case**: Whether to lower case the input.\n            **never_split**: (`optional`) list of str\n                Kept for backward compatibility purposes.\n                Now implemented directly at the base class level (see :func:`PreTrainedTokenizer.tokenize`)\n                List of token not to split.\n            **tokenize_chinese_chars**: (`optional`) boolean (default True)\n                Whether to tokenize Chinese characters.\n                This should likely be desactivated for Japanese:\n                see: https://github.com/huggingface/pytorch-pretrained-BERT/issues/328\n        \"\"\"\n        if never_split is None:\n            never_split = []\n        self.do_lower_case = do_lower_case\n        self.never_split = never_split\n        self.tokenize_chinese_chars = tokenize_chinese_chars\n\n    def tokenize(self, text, never_split=None):\n        \"\"\" Basic Tokenization of a piece of text.\n            Split on \"white spaces\" only, for sub-word tokenization, see WordPieceTokenizer.\n\n        Args:\n            **never_split**: (`optional`) list of str\n                Kept for backward compatibility purposes.\n                Now implemented directly at the base class level (see :func:`PreTrainedTokenizer.tokenize`)\n                List of token not to split.\n        \"\"\"\n        never_split = self.never_split + (never_split if never_split is not None else [])\n        text = self._clean_text(text)\n        # This was added on November 1st, 2018 for the multilingual and Chinese\n        # models. This is also applied to the English models now, but it doesn't\n        # matter since the English models were not trained on any Chinese data\n        # and generally don't have any Chinese data in them (there are Chinese\n        # characters in the vocabulary because Wikipedia does have some Chinese\n        # words in the English Wikipedia.).\n        if self.tokenize_chinese_chars:\n            text = self._tokenize_chinese_chars(text)\n        orig_tokens = whitespace_tokenize(text)\n        split_tokens = []\n        for token in orig_tokens:\n            if self.do_lower_case and token not in never_split:\n                token = token.lower()\n                token = self._run_strip_accents(token)\n            split_tokens.extend(self._run_split_on_punc(token))\n\n        output_tokens = whitespace_tokenize(\" \".join(split_tokens))\n        return output_tokens\n\n    def _run_strip_accents(self, text):\n        \"\"\"Strips accents from a piece of text.\"\"\"\n        text = unicodedata.normalize(\"NFD\", text)\n        output = []\n        for char in text:\n            cat = unicodedata.category(char)\n            if cat == \"Mn\":\n                continue\n            output.append(char)\n        return \"\".join(output)\n\n    def _run_split_on_punc(self, text, never_split=None):\n        \"\"\"Splits punctuation on a piece of text.\"\"\"\n        if never_split is not None and text in never_split:\n            return [text]\n        chars = list(text)\n        i = 0\n        start_new_word = True\n        output = []\n        while i < len(chars):\n            char = chars[i]\n            if _is_punctuation(char):\n                output.append([char])\n                start_new_word = True\n            else:\n                if start_new_word:\n                    output.append([])\n                start_new_word = False\n                output[-1].append(char)\n            i += 1\n\n        return [\"\".join(x) for x in output]\n\n    def _tokenize_chinese_chars(self, text):\n        \"\"\"Adds whitespace around any CJK character.\"\"\"\n        output = []\n        for char in text:\n            cp = ord(char)\n            if self._is_chinese_char(cp):\n                output.append(\" \")\n                output.append(char)\n                output.append(\" \")\n            else:\n                output.append(char)\n        return \"\".join(output)\n\n    def _is_chinese_char(self, cp):\n        \"\"\"Checks whether CP is the codepoint of a CJK character.\"\"\"\n        # This defines a \"chinese character\" as anything in the CJK Unicode block:\n        #   https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)\n        #\n        # Note that the CJK Unicode block is NOT all Japanese and Korean characters,\n        # despite its name. The modern Korean Hangul alphabet is a different block,\n        # as is Japanese Hiragana and Katakana. Those alphabets are used to write\n        # space-separated words, so they are not treated specially and handled\n        # like the all of the other languages.\n        if ((cp >= 0x4E00 and cp <= 0x9FFF) or  #\n                (cp >= 0x3400 and cp <= 0x4DBF) or  #\n                (cp >= 0x20000 and cp <= 0x2A6DF) or  #\n                (cp >= 0x2A700 and cp <= 0x2B73F) or  #\n                (cp >= 0x2B740 and cp <= 0x2B81F) or  #\n                (cp >= 0x2B820 and cp <= 0x2CEAF) or\n                (cp >= 0xF900 and cp <= 0xFAFF) or  #\n                (cp >= 0x2F800 and cp <= 0x2FA1F)):  #\n            return True\n\n        return False\n\n    def _clean_text(self, text):\n        \"\"\"Performs invalid character removal and whitespace cleanup on text.\"\"\"\n        output = []\n        for char in text:\n            cp = ord(char)\n            if cp == 0 or cp == 0xfffd or _is_control(char):\n                continue\n            if _is_whitespace(char):\n                output.append(\" \")\n            else:\n                output.append(char)\n        return \"\".join(output)\n\n\nclass WordpieceTokenizer(object):\n    \"\"\"Runs WordPiece tokenization.\"\"\"\n\n    def __init__(self, vocab, unk_token, max_input_chars_per_word=100):\n        self.vocab = vocab\n        self.unk_token = unk_token\n        self.max_input_chars_per_word = max_input_chars_per_word\n\n    def tokenize(self, text):\n        \"\"\"Tokenizes a piece of text into its word pieces.\n\n        This uses a greedy longest-match-first algorithm to perform tokenization\n        using the given vocabulary.\n\n        For example:\n          input = \"unaffable\"\n          output = [\"un\", \"##aff\", \"##able\"]\n\n        Args:\n          text: A single token or whitespace separated tokens. This should have\n            already been passed through `BasicTokenizer`.\n\n        Returns:\n          A list of wordpiece tokens.\n        \"\"\"\n\n        output_tokens = []\n        for token in whitespace_tokenize(text):\n            chars = list(token)\n            if len(chars) > self.max_input_chars_per_word:\n                output_tokens.append(self.unk_token)\n                continue\n\n            is_bad = False\n            start = 0\n            sub_tokens = []\n            while start < len(chars):\n                end = len(chars)\n                cur_substr = None\n                while start < end:\n                    substr = \"\".join(chars[start:end])\n                    if start > 0:\n                        substr = \"##\" + substr\n                    if substr in self.vocab:\n                        cur_substr = substr\n                        break\n                    end -= 1\n                if cur_substr is None:\n                    is_bad = True\n                    break\n                sub_tokens.append(cur_substr)\n                start = end\n\n            if is_bad:\n                output_tokens.append(self.unk_token)\n            else:\n                output_tokens.extend(sub_tokens)\n        return output_tokens\n\n\ndef _is_whitespace(char):\n    \"\"\"Checks whether `chars` is a whitespace character.\"\"\"\n    # \\t, \\n, and \\r are technically contorl characters but we treat them\n    # as whitespace since they are generally considered as such.\n    if char == \" \" or char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n        return True\n    cat = unicodedata.category(char)\n    if cat == \"Zs\":\n        return True\n    return False\n\n\ndef _is_control(char):\n    \"\"\"Checks whether `chars` is a control character.\"\"\"\n    # These are technically control characters but we count them as whitespace\n    # characters.\n    if char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n        return False\n    cat = unicodedata.category(char)\n    if cat.startswith(\"C\"):\n        return True\n    return False\n\n\ndef _is_punctuation(char):\n    \"\"\"Checks whether `chars` is a punctuation character.\"\"\"\n    cp = ord(char)\n    # We treat all non-letter/number ASCII as punctuation.\n    # Characters such as \"^\", \"$\", and \"`\" are not in the Unicode\n    # Punctuation class but we treat them as punctuation anyways, for\n    # consistency.\n    if ((cp >= 33 and cp <= 47) or (cp >= 58 and cp <= 64) or\n            (cp >= 91 and cp <= 96) or (cp >= 123 and cp <= 126)):\n        return True\n    cat = unicodedata.category(char)\n    if cat.startswith(\"P\"):\n        return True\n    return False\n"}
{"type": "source_file", "path": "src/modeling/video_swin/swin_large_384_patch244_window81212_kinetics600_22k.py", "content": "_base_ = [\r\n    'swin_large.py', 'default_runtime.py'\r\n]\r\nmodel=dict(backbone=dict(patch_size=(2,4,4), window_size=(8,12,12), drop_path_rate=0.4), test_cfg=dict(max_testing_views=1), cls_head=dict(num_classes=600), train_cfg=dict(blending=dict(type='LabelSmoothing', num_classes=600, smoothing=0.1)))\r\n\r\n# dataset settings\r\ndataset_type = 'VideoDataset'\r\ndata_root = 'data/kinetics600/train'\r\ndata_root_val = 'data/kinetics600/val'\r\nann_file_train = 'data/kinetics600/kinetics600_train_list.txt'\r\nann_file_val = 'data/kinetics600/kinetics600_val_list.txt'\r\nann_file_test = 'data/kinetics600/kinetics600_val_list.txt'\r\nimg_norm_cfg = dict(\r\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_bgr=False)\r\ntrain_pipeline = [\r\n    dict(type='DecordInit'),\r\n    dict(type='SampleFrames', clip_len=32, frame_interval=2, num_clips=1),\r\n    dict(type='DecordDecode'),\r\n    dict(type='Resize', scale=(-1, 416)),\r\n    dict(type='RandomResizedCrop'),\r\n    dict(type='Resize', scale=(384, 384), keep_ratio=False),\r\n    dict(type='Flip', flip_ratio=0.5),\r\n    dict(type='Imgaug', transforms=[dict(type='RandAugment', n=4, m=7)]),\r\n    dict(type='Normalize', **img_norm_cfg),\r\n    dict(type='RandomErasing', probability=0.25),\r\n    dict(type='FormatShape', input_format='NCTHW'),\r\n    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\r\n    dict(type='ToTensor', keys=['imgs', 'label'])\r\n]\r\nval_pipeline = [\r\n    dict(type='DecordInit'),\r\n    dict(\r\n        type='SampleFrames',\r\n        clip_len=32,\r\n        frame_interval=2,\r\n        num_clips=1,\r\n        test_mode=True),\r\n    dict(type='DecordDecode'),\r\n    dict(type='Resize', scale=(-1, 416)),\r\n    dict(type='CenterCrop', crop_size=384),\r\n    dict(type='Flip', flip_ratio=0),\r\n    dict(type='Normalize', **img_norm_cfg),\r\n    dict(type='FormatShape', input_format='NCTHW'),\r\n    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\r\n    dict(type='ToTensor', keys=['imgs'])\r\n]\r\ntest_pipeline = [\r\n    dict(type='DecordInit'),\r\n    dict(\r\n        type='SampleFrames',\r\n        clip_len=32,\r\n        frame_interval=2,\r\n        num_clips=4,\r\n        test_mode=True),\r\n    dict(type='DecordDecode'),\r\n    dict(type='Resize', scale=(-1, 384)),\r\n    dict(type='ThreeCrop', crop_size=384),\r\n    dict(type='Flip', flip_ratio=0),\r\n    dict(type='Normalize', **img_norm_cfg),\r\n    dict(type='FormatShape', input_format='NCTHW'),\r\n    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\r\n    dict(type='ToTensor', keys=['imgs'])\r\n]\r\ndata = dict(\r\n    videos_per_gpu=8,\r\n    workers_per_gpu=1,\r\n    val_dataloader=dict(\r\n        videos_per_gpu=1,\r\n        workers_per_gpu=1\r\n    ),\r\n    test_dataloader=dict(\r\n        videos_per_gpu=1,\r\n        workers_per_gpu=1\r\n    ),\r\n    train=dict(\r\n        type=dataset_type,\r\n        ann_file=ann_file_train,\r\n        data_prefix=data_root,\r\n        pipeline=train_pipeline),\r\n    val=dict(\r\n        type=dataset_type,\r\n        ann_file=ann_file_val,\r\n        data_prefix=data_root_val,\r\n        pipeline=val_pipeline),\r\n    test=dict(\r\n        type=dataset_type,\r\n        ann_file=ann_file_test,\r\n        data_prefix=data_root_val,\r\n        pipeline=test_pipeline))\r\nevaluation = dict(\r\n    interval=5, metrics=['top_k_accuracy', 'mean_class_accuracy'])\r\n\r\n# optimizer\r\noptimizer = dict(type='AdamW', lr=3e-4, betas=(0.9, 0.999), weight_decay=0.05,\r\n                 paramwise_cfg=dict(custom_keys={'absolute_pos_embed': dict(decay_mult=0.),\r\n                                                 'relative_position_bias_table': dict(decay_mult=0.),\r\n                                                 'norm': dict(decay_mult=0.),\r\n                                                 'backbone': dict(lr_mult=0.1)}))\r\n# learning policy\r\nlr_config = dict(\r\n    policy='CosineAnnealing',\r\n    min_lr=0,\r\n    warmup='linear',\r\n    warmup_by_epoch=True,\r\n    warmup_iters=2.5\r\n)\r\ntotal_epochs = 60\r\n\r\n# runtime settings\r\ncheckpoint_config = dict(interval=1)\r\nwork_dir = work_dir = './work_dirs/swin_large_384_patch244_window81212_kinetics600_22k'\r\nfind_unused_parameters = False\r\n\r\n\r\n# do not use mmdet version fp16\r\nfp16 = None\r\noptimizer_config = dict(\r\n    type=\"DistOptimizerHook\",\r\n    update_interval=8,\r\n    grad_clip=None,\r\n    coalesce=True,\r\n    bucket_size_mb=-1,\r\n    use_fp16=True,\r\n)"}
{"type": "source_file", "path": "src/datasets/data_sampler.py", "content": "import math\nimport random\nfrom datetime import datetime\nimport torch\nfrom torch.utils.data import Sampler, Dataset\nimport torch.distributed as dist\nfrom src.utils.comm import get_local_rank, get_local_size, get_rank, get_world_size\nfrom .sampler_utils import PrepareData\nimport logging\nlogger = logging.getLogger(__name__)\n\n\nclass DistributedSamplerLimited(Sampler):\n    def __init__(self, dataset: Dataset, num_replicas: int = None,\n                 rank: int = None, shuffle: bool = True,\n                 seed: int = 0, drop_last: bool = False, limited=-1) -> None:\n        if num_replicas is None:\n            if not dist.is_available():\n                raise RuntimeError(\"Requires distributed package to be available\")\n            num_replicas = dist.get_world_size()\n        if rank is None:\n            if not dist.is_available():\n                raise RuntimeError(\"Requires distributed package to be available\")\n            rank = dist.get_rank()\n        if rank >= num_replicas or rank < 0:\n            raise ValueError(\n                \"Invalid rank {}, rank should be in the interval\"\n                \" [0, {}]\".format(rank, num_replicas - 1))\n        self.dataset = dataset\n        self.num_replicas = num_replicas\n        self.rank = rank\n        self.epoch = 0\n        self.drop_last = drop_last\n        print(f'Dbg: distribeted sampler limited: rank={rank}, num_replicas={num_replicas}')\n        # If the dataset length is evenly divisible by # of replicas, then there\n        # is no need to drop any data, since the dataset will be split equally.\n        if self.drop_last and len(self.dataset) % self.num_replicas != 0:  # type: ignore\n            # Split to nearest available length that is evenly divisible.\n            # This is to ensure each rank receives the same amount of data when\n            # using this Sampler.\n            self.num_samples = math.ceil(\n                # `type:ignore` is required because Dataset cannot provide a default __len__\n                # see NOTE in pytorch/torch/utils/data/sampler.py\n                (len(self.dataset) - self.num_replicas) / self.num_replicas  # type: ignore\n            )\n        else:\n            self.num_samples = math.ceil(len(self.dataset) / self.num_replicas)  # type: ignore\n        self.total_size = self.num_samples * self.num_replicas\n        self.shuffle = shuffle\n        self.seed = seed\n        self.limited = limited\n        if self.limited > -1:\n            self.num_samples = min(self.limited, self.num_samples)\n\n    def __iter__(self):\n        if self.shuffle:\n            # deterministically shuffle based on epoch and seed\n            g = torch.Generator()\n            g.manual_seed(self.seed + self.epoch)\n            indices = torch.randperm(len(self.dataset), generator=g).tolist()  # type: ignore\n        else:\n            indices = list(range(len(self.dataset)))  # type: ignore\n\n        if not self.drop_last:\n            # add extra samples to make it evenly divisible\n            padding_size = self.total_size - len(indices)\n            if padding_size <= len(indices):\n                indices += indices[:padding_size]\n            else:\n                indices += (indices * math.ceil(padding_size / len(indices)))[:padding_size]\n        else:\n            # remove tail of data to make it evenly divisible.\n            indices = indices[:self.total_size]\n        assert len(indices) == self.total_size\n\n        # subsample\n        indices = indices[self.rank:self.total_size:self.num_replicas]\n        if self.limited > -1 and len(indices) > self.limited:\n            print(f'Trim indices: {len(indices)} --> {self.limited}')\n            indices = indices[:self.limited]\n        assert len(indices) == self.num_samples\n        # shuffle subsample\n        if self.shuffle:  # and self.epoch > 0:\n            # random.seed(self.seed + self.epoch)\n            random.seed(datetime.now())\n            random.shuffle(indices)\n\n        return iter(indices)\n\n    def __len__(self) -> int:\n        return self.num_samples\n\n    def set_epoch(self, epoch: int) -> None:\n        self.epoch = 0  # keep data unchanged\n        # self.epoch = epoch\n\n\nclass NodeSplitSampler(Sampler):\n    def __init__(self, dataset, shuffle, random_seed, first_epoch_skip_shuffle=False,\n    prepare_data=True):\n        self.dataset = dataset\n        self.shuffle = shuffle\n        self.random_seed = random_seed\n\n        self.world_size = get_world_size()\n        self.local_size = get_local_size()\n        self.node_size = self.world_size // self.local_size\n        self.rank = get_rank()\n        self.node_idx = self.rank // self.local_size\n        self.local_rank = get_local_rank()\n        self.next_epoch_skip_shuffle = first_epoch_skip_shuffle\n\n        # only be used when shuffle = True and first_epoch_skip_shuffle = True\n        self.prepare_data = prepare_data\n        self.prepare = None\n        self.skip = 0\n\n    def get_index_on_node(self):\n        # there is no need to cache source_list as we only call this function\n        # once in the whole training life-time\n        source_list = self.dataset.get_composite_source_idx()\n        idx_split = list(enumerate(source_list))\n        idx_split = torch.tensor(idx_split)\n        if self.shuffle:\n            random_idx = self.get_shufle_idx(len(idx_split))\n            idx_split = idx_split[random_idx]\n            max_split = idx_split[:, 1].max() + 1\n            priority = self.get_shufle_idx(max_split)\n            sort_idx = torch.argsort(priority[idx_split[:, 1]])\n            idx_split = idx_split[sort_idx]\n        num_idx_on_node = (len(idx_split) + self.node_size - 1) // self.node_size\n        offset = num_idx_on_node * self.node_idx\n        offset_end = offset + num_idx_on_node\n        offset_end = min(offset_end, len(idx_split))\n        unique_split_index = list(set(idx_split[offset:offset_end, 1].tolist()))\n        logger.info(unique_split_index)\n        if self.shuffle and self.next_epoch_skip_shuffle and self.prepare_data:\n            if get_local_rank() == 0:\n                self.prepare = PrepareData(\n                    self.dataset,\n                    prepare_t_versions=[],\n                    fixed_samples_in_node=True)\n                for s in unique_split_index:\n                    self.prepare.prepare(s)\n        return idx_split[offset:offset_end, 0]\n\n    def get_shufle_idx(self, n):\n        g = torch.Generator()\n        g.manual_seed(self.random_seed)\n        random_idx = torch.randperm(n, generator=g)\n        self.random_seed += 99\n        return random_idx\n\n    def get_index_on_rank(self, idx_on_node):\n        if self.shuffle:\n            if not self.next_epoch_skip_shuffle:\n                curr_idx_on_node = idx_on_node[self.get_shufle_idx(len(idx_on_node))]\n            else:\n                curr_idx_on_node = idx_on_node\n                self.next_epoch_skip_shuffle = False\n        else:\n            curr_idx_on_node = idx_on_node\n        idx_rank_size = (len(curr_idx_on_node) + self.local_size - 1) // self.local_size\n        offset = idx_rank_size * self.local_rank\n        offset_end = offset + idx_rank_size\n        offset_end = min(offset_end, len(curr_idx_on_node))\n        curr_idx_on_node = curr_idx_on_node.tolist()\n        for i in range(offset, offset_end):\n            yield curr_idx_on_node[i]\n\n    def skip(self, num):\n        self.skip = num\n\n    def __iter__(self):\n        self.curr_idx = 0\n        idx_on_node = self.get_index_on_node()\n        if self.skip > 0:\n            logging.info('we will skip {}'.format(self.skip))\n        while True:\n            for i in self.get_index_on_rank(idx_on_node):\n                if self.skip <= 0:\n                    yield i\n                else:\n                    self.skip -= 1\n\n    def __len__(self):\n        raise ValueError('should not be called')\n"}
{"type": "source_file", "path": "src/modeling/swin/build.py", "content": "# --------------------------------------------------------\n# Swin Transformer\n# Copyright (c) 2021 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ze Liu\n# --------------------------------------------------------\n\nfrom .swin_transformer import SwinTransformer\n\ndef build_model(config):\n    model_type = config.MODEL.TYPE\n    if model_type == 'swin':\n        model = SwinTransformer(img_size=config.DATA.IMG_SIZE,\n                                patch_size=config.MODEL.SWIN.PATCH_SIZE,\n                                in_chans=config.MODEL.SWIN.IN_CHANS,\n                                num_classes=config.MODEL.NUM_CLASSES,\n                                embed_dim=config.MODEL.SWIN.EMBED_DIM,\n                                depths=config.MODEL.SWIN.DEPTHS,\n                                num_heads=config.MODEL.SWIN.NUM_HEADS,\n                                window_size=config.MODEL.SWIN.WINDOW_SIZE,\n                                mlp_ratio=config.MODEL.SWIN.MLP_RATIO,\n                                qkv_bias=config.MODEL.SWIN.QKV_BIAS,\n                                qk_scale=config.MODEL.SWIN.QK_SCALE,\n                                drop_rate=config.MODEL.DROP_RATE,\n                                drop_path_rate=config.MODEL.DROP_PATH_RATE,\n                                ape=config.MODEL.SWIN.APE,\n                                patch_norm=config.MODEL.SWIN.PATCH_NORM,\n                                use_checkpoint=config.TRAIN.USE_CHECKPOINT)\n    else:\n        raise NotImplementedError(f\"Unkown model: {model_type}\")\n\n    return model"}
{"type": "source_file", "path": "src/modeling/signal_predicting.py", "content": "import torch\nfrom fairscale.nn.misc import checkpoint_wrapper\nimport random\nfrom src.modeling.load_sensor_pred_head import get_sensor_pred_model\n\nclass SignalVideoTransformer(torch.nn.Module):\n    \"\"\" This is the one head module that performs Control Signal Prediction. \"\"\"\n    def __init__(self, args, config, swin, transformer_encoder):\n        \"\"\" Initializes the model.\n        Parameters:\n            args: basic args of ADAPT, mostly defined in `src/configs/VidSwinBert/BDDX_multi_default.json` and input args\n            config: config of transformer_encoder, mostly defined in `models/captioning/bert-base-uncased/config.json`\n            swin: torch module of the backbone to be used. See `src/modeling/load_swin.py`\n            transformer_encoder: torch module of the transformer architecture. See `src/modeling/load_bert.py`\n        \"\"\"\n        super(SignalVideoTransformer, self).__init__()\n        self.config = config\n        self.use_checkpoint = args.use_checkpoint and not args.freeze_backbone\n        if self.use_checkpoint:\n            self.swin = checkpoint_wrapper(swin, offload_to_cpu=True)\n        else:\n            self.swin = swin\n        self.trans_encoder = transformer_encoder\n        self.img_feature_dim = int(args.img_feature_dim)\n        self.use_grid_feat = args.grid_feat\n        self.latent_feat_size = self.swin.backbone.norm.normalized_shape[0]\n        self.fc = torch.nn.Linear(self.latent_feat_size, self.img_feature_dim)\n        self.compute_mask_on_the_fly = False # deprecated\n        self.mask_prob = args.mask_prob\n        self.mask_token_id = -1\n        self.max_img_seq_length = args.max_img_seq_length\n\n        # get Control Signal Prediction Head\n        self.sensor_pred_head = get_sensor_pred_model(args)\n\n        # if only_signal is True, it means we \n        # remove Driving Caption Generation head and only use Control Signal Prediction head \n        self.only_signal = getattr(args, 'only_signal', False)\n        assert self.only_signal\n\n\n    def forward(self, *args, **kwargs):\n        \"\"\" The forward process of Control Signal Prediction Head, \n        Parameters:\n            input_ids: word tokens of input sentences tokenized by tokenizer\n            attention_mask: multimodal attention mask in Vision-Language transformer\n            token_type_ids: typen tokens of input sentences, \n                            0 means it is a narration sentence and 1 means a reasoning sentence, same size with input_ids\n            img_feats: preprocessed frames of the video\n            car_info: control signals of ego car in the video\n        \"\"\"\n\n        # grad cam can only input a tuple (args, kwargs)\n        if isinstance(args, tuple) and len(args) != 0:\n            kwargs = args[0]\n            args= ()\n\n        # video swin to extract video features\n        images = kwargs['img_feats']\n        B, S, C, H, W = images.shape  # batch, segment, chanel, hight, width\n        # (B x S x C x H x W) --> (B x C x S x H x W)\n        images = images.permute(0, 2, 1, 3, 4)\n        vid_feats = self.swin(images)\n\n        # tokenize video features to video tokens\n        if self.use_grid_feat==True:\n            vid_feats = vid_feats.permute(0, 2, 3, 4, 1)\n        vid_feats = vid_feats.view(B, -1, self.latent_feat_size)\n\n        # use an mlp to transform video token dimension\n        vid_feats = self.fc(vid_feats)\n\n        # prepare VL transformer inputs\n        kwargs['img_feats'] = vid_feats\n\n        # only Control Signal Prediction head \n        sensor_outputs = self.sensor_pred_head(*args, **kwargs)        \n        return sensor_outputs\n\n    \n    def get_loss_sparsity(self, video_attention):\n        sparsity_loss = 0\n        sparsity_loss += (torch.mean(torch.abs(video_attention)))\n        return sparsity_loss\n\n    def reload_attn_mask(self, pretrain_attn_mask): \n        import numpy\n        pretrained_num_tokens = int(numpy.sqrt(pretrain_attn_mask.shape[0]))\n\n        pretrained_learn_att = pretrain_attn_mask.reshape(\n                                pretrained_num_tokens,pretrained_num_tokens)\n        scale_factor = 1\n        vid_att_len = self.max_img_seq_length\n        learn_att = self.learn_vid_att.weight.reshape(vid_att_len,vid_att_len)\n        with torch.no_grad():\n            for i in range(int(scale_factor)):\n                learn_att[pretrained_num_tokens*i:pretrained_num_tokens*(i+1), \n                            pretrained_num_tokens*i:pretrained_num_tokens*(i+1)] = pretrained_learn_att \n\n    def freeze_backbone(self, freeze=True):\n        for _, p in self.swin.named_parameters():\n            p.requires_grad =  not freeze\n"}
{"type": "source_file", "path": "src/modeling/video_swin/swin_small_patch244_window877_kinetics400_1k.py", "content": "_base_ = [\n    '../../_base_/models/swin/swin_small.py', '../../_base_/default_runtime.py'\n]\nmodel=dict(backbone=dict(patch_size=(2,4,4), drop_path_rate=0.1), test_cfg=dict(max_testing_views=4))\n\n# dataset settings\ndataset_type = 'VideoDataset'\ndata_root = 'data/kinetics400/train'\ndata_root_val = 'data/kinetics400/val'\nann_file_train = 'data/kinetics400/kinetics400_train_list.txt'\nann_file_val = 'data/kinetics400/kinetics400_val_list.txt'\nann_file_test = 'data/kinetics400/kinetics400_val_list.txt'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_bgr=False)\ntrain_pipeline = [\n    dict(type='DecordInit'),\n    dict(type='SampleFrames', clip_len=32, frame_interval=2, num_clips=1),\n    dict(type='DecordDecode'),\n    dict(type='Resize', scale=(-1, 256)),\n    dict(type='RandomResizedCrop'),\n    dict(type='Resize', scale=(224, 224), keep_ratio=False),\n    dict(type='Flip', flip_ratio=0.5),\n    dict(type='Normalize', **img_norm_cfg),\n    dict(type='FormatShape', input_format='NCTHW'),\n    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n    dict(type='ToTensor', keys=['imgs', 'label'])\n]\nval_pipeline = [\n    dict(type='DecordInit'),\n    dict(\n        type='SampleFrames',\n        clip_len=32,\n        frame_interval=2,\n        num_clips=1,\n        test_mode=True),\n    dict(type='DecordDecode'),\n    dict(type='Resize', scale=(-1, 256)),\n    dict(type='CenterCrop', crop_size=224),\n    dict(type='Flip', flip_ratio=0),\n    dict(type='Normalize', **img_norm_cfg),\n    dict(type='FormatShape', input_format='NCTHW'),\n    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n    dict(type='ToTensor', keys=['imgs'])\n]\ntest_pipeline = [\n    dict(type='DecordInit'),\n    dict(\n        type='SampleFrames',\n        clip_len=32,\n        frame_interval=2,\n        num_clips=4,\n        test_mode=True),\n    dict(type='DecordDecode'),\n    dict(type='Resize', scale=(-1, 224)),\n    dict(type='ThreeCrop', crop_size=224),\n    dict(type='Flip', flip_ratio=0),\n    dict(type='Normalize', **img_norm_cfg),\n    dict(type='FormatShape', input_format='NCTHW'),\n    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n    dict(type='ToTensor', keys=['imgs'])\n]\ndata = dict(\n    videos_per_gpu=8,\n    workers_per_gpu=4,\n    val_dataloader=dict(\n        videos_per_gpu=1,\n        workers_per_gpu=1\n    ),\n    test_dataloader=dict(\n        videos_per_gpu=1,\n        workers_per_gpu=1\n    ),\n    train=dict(\n        type=dataset_type,\n        ann_file=ann_file_train,\n        data_prefix=data_root,\n        pipeline=train_pipeline),\n    val=dict(\n        type=dataset_type,\n        ann_file=ann_file_val,\n        data_prefix=data_root_val,\n        pipeline=val_pipeline),\n    test=dict(\n        type=dataset_type,\n        ann_file=ann_file_test,\n        data_prefix=data_root_val,\n        pipeline=test_pipeline))\nevaluation = dict(\n    interval=5, metrics=['top_k_accuracy', 'mean_class_accuracy'])\n\n# optimizer\noptimizer = dict(type='AdamW', lr=1e-3, betas=(0.9, 0.999), weight_decay=0.02,\n                 paramwise_cfg=dict(custom_keys={'absolute_pos_embed': dict(decay_mult=0.),\n                                                 'relative_position_bias_table': dict(decay_mult=0.),\n                                                 'norm': dict(decay_mult=0.),\n                                                 'backbone': dict(lr_mult=0.1)}))\n# learning policy\nlr_config = dict(\n    policy='CosineAnnealing',\n    min_lr=0,\n    warmup='linear',\n    warmup_by_epoch=True,\n    warmup_iters=2.5\n)\ntotal_epochs = 30\n\n# runtime settings\ncheckpoint_config = dict(interval=1)\nwork_dir = work_dir = './work_dirs/k400_swin_small_patch244_window877.py'\nfind_unused_parameters = False\n\n\n# do not use mmdet version fp16\nfp16 = None\noptimizer_config = dict(\n    type=\"DistOptimizerHook\",\n    update_interval=8,\n    grad_clip=None,\n    coalesce=True,\n    bucket_size_mb=-1,\n    use_fp16=True,\n)\n"}
{"type": "source_file", "path": "src/modeling/video_swin/swin_base_patch244_window877_kinetics400_1k.py", "content": "_base_ = [\n    'swin_base.py', 'default_runtime.py'\n]\nmodel=dict(backbone=dict(patch_size=(2,4,4), drop_path_rate=0.3), test_cfg=dict(max_testing_views=4))\n\n# dataset settings\ndataset_type = 'VideoDataset'\ndata_root = 'data/kinetics400/train'\ndata_root_val = 'data/kinetics400/val'\nann_file_train = 'data/kinetics400/kinetics400_train_list.txt'\nann_file_val = 'data/kinetics400/kinetics400_val_list.txt'\nann_file_test = 'data/kinetics400/kinetics400_val_list.txt'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_bgr=False)\ntrain_pipeline = [\n    dict(type='DecordInit'),\n    dict(type='SampleFrames', clip_len=32, frame_interval=2, num_clips=1),\n    dict(type='DecordDecode'),\n    dict(type='Resize', scale=(-1, 256)),\n    dict(type='RandomResizedCrop'),\n    dict(type='Resize', scale=(224, 224), keep_ratio=False),\n    dict(type='Flip', flip_ratio=0.5),\n    dict(type='Normalize', **img_norm_cfg),\n    dict(type='FormatShape', input_format='NCTHW'),\n    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n    dict(type='ToTensor', keys=['imgs', 'label'])\n]\nval_pipeline = [\n    dict(type='DecordInit'),\n    dict(\n        type='SampleFrames',\n        clip_len=32,\n        frame_interval=2,\n        num_clips=1,\n        test_mode=True),\n    dict(type='DecordDecode'),\n    dict(type='Resize', scale=(-1, 256)),\n    dict(type='CenterCrop', crop_size=224),\n    dict(type='Flip', flip_ratio=0),\n    dict(type='Normalize', **img_norm_cfg),\n    dict(type='FormatShape', input_format='NCTHW'),\n    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n    dict(type='ToTensor', keys=['imgs'])\n]\ntest_pipeline = [\n    dict(type='DecordInit'),\n    dict(\n        type='SampleFrames',\n        clip_len=32,\n        frame_interval=2,\n        num_clips=4,\n        test_mode=True),\n    dict(type='DecordDecode'),\n    dict(type='Resize', scale=(-1, 224)),\n    dict(type='ThreeCrop', crop_size=224),\n    dict(type='Flip', flip_ratio=0),\n    dict(type='Normalize', **img_norm_cfg),\n    dict(type='FormatShape', input_format='NCTHW'),\n    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n    dict(type='ToTensor', keys=['imgs'])\n]\ndata = dict(\n    videos_per_gpu=8,\n    workers_per_gpu=4,\n    val_dataloader=dict(\n        videos_per_gpu=1,\n        workers_per_gpu=1\n    ),\n    test_dataloader=dict(\n        videos_per_gpu=1,\n        workers_per_gpu=1\n    ),\n    train=dict(\n        type=dataset_type,\n        ann_file=ann_file_train,\n        data_prefix=data_root,\n        pipeline=train_pipeline),\n    val=dict(\n        type=dataset_type,\n        ann_file=ann_file_val,\n        data_prefix=data_root_val,\n        pipeline=val_pipeline),\n    test=dict(\n        type=dataset_type,\n        ann_file=ann_file_test,\n        data_prefix=data_root_val,\n        pipeline=test_pipeline))\nevaluation = dict(\n    interval=5, metrics=['top_k_accuracy', 'mean_class_accuracy'])\n\n# optimizer\noptimizer = dict(type='AdamW', lr=1e-3, betas=(0.9, 0.999), weight_decay=0.05,\n                 paramwise_cfg=dict(custom_keys={'absolute_pos_embed': dict(decay_mult=0.),\n                                                 'relative_position_bias_table': dict(decay_mult=0.),\n                                                 'norm': dict(decay_mult=0.),\n                                                 'backbone': dict(lr_mult=0.1)}))\n# learning policy\nlr_config = dict(\n    policy='CosineAnnealing',\n    min_lr=0,\n    warmup='linear',\n    warmup_by_epoch=True,\n    warmup_iters=2.5\n)\ntotal_epochs = 30\n\n# runtime settings\ncheckpoint_config = dict(interval=1)\nwork_dir = work_dir = './work_dirs/k400_swin_base_patch244_window877.py'\nfind_unused_parameters = False\n\n\n# do not use mmdet version fp16\nfp16 = None\noptimizer_config = dict(\n    type=\"DistOptimizerHook\",\n    update_interval=8,\n    grad_clip=None,\n    coalesce=True,\n    bucket_size_mb=-1,\n    use_fp16=True,\n)\n"}
{"type": "source_file", "path": "src/layers/bert/modeling_utils.py", "content": "# coding=utf-8\n# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"PyTorch BERT model.\"\"\"\n\nfrom __future__ import (absolute_import, division, print_function,\n                        unicode_literals)\n\nimport copy\nimport json\nimport os\nfrom io import open\n\nimport six\nimport torch\nfrom torch import nn\nfrom torch.nn import CrossEntropyLoss\nfrom torch.nn import functional as F\n\nfrom .file_utils import cached_path\n\nimport logging\nfrom src.utils.comm import is_main_process\nlogger = logging.getLogger(__name__)  # pylint: disable=invalid-name\nif not is_main_process():\n    logger.disabled = True\n\nCONFIG_NAME = \"config.json\"\nWEIGHTS_NAME = \"pytorch_model.bin\"\nTF_WEIGHTS_NAME = 'model.ckpt'\n\n\ntry:\n    from torch.nn import Identity\nexcept ImportError:\n    # Older PyTorch compatibility\n    class Identity(nn.Module):\n        r\"\"\"A placeholder identity operator that is argument-insensitive.\n        \"\"\"\n        def __init__(self, *args, **kwargs):\n            super(Identity, self).__init__()\n\n        def forward(self, input):\n            return input\n\n\nif not six.PY2:\n    def add_start_docstrings(*docstr):\n        def docstring_decorator(fn):\n            fn.__doc__ = ''.join(docstr) + fn.__doc__\n            return fn\n        return docstring_decorator\nelse:\n    # Not possible to update class docstrings on python2\n    def add_start_docstrings(*docstr):\n        def docstring_decorator(fn):\n            return fn\n        return docstring_decorator\n\n\nclass PretrainedConfig(object):\n    \"\"\" Base class for all configuration classes.\n        Handle a few common parameters and methods for loading/downloading/saving configurations.\n    \"\"\"\n    pretrained_config_archive_map = {}\n\n    def __init__(self, **kwargs):\n        self.finetuning_task = kwargs.pop('finetuning_task', None)\n        self.num_labels = kwargs.pop('num_labels', 2)\n        self.output_attentions = kwargs.pop('output_attentions', False)\n        self.output_hidden_states = kwargs.pop('output_hidden_states', False)\n        self.torchscript = kwargs.pop('torchscript', False)\n\n    def save_pretrained(self, save_directory):\n        \"\"\" Save a configuration object to a directory, so that it\n            can be re-loaded using the `from_pretrained(save_directory)` class method.\n        \"\"\"\n        assert os.path.isdir(save_directory), \"Saving path should be a directory where the model and configuration can be saved\"\n\n        # If we save using the predefined names, we can load using `from_pretrained`\n        output_config_file = os.path.join(save_directory, CONFIG_NAME)\n\n        self.to_json_file(output_config_file)\n\n    @classmethod\n    def from_pretrained(cls, pretrained_model_name_or_path, **kwargs):\n        r\"\"\" Instantiate a PretrainedConfig from a pre-trained model configuration.\n\n        Params:\n            **pretrained_model_name_or_path**: either:\n                - a string with the `shortcut name` of a pre-trained model configuration to load from cache\n                    or download and cache if not already stored in cache (e.g. 'bert-base-uncased').\n                - a path to a `directory` containing a configuration file saved\n                    using the `save_pretrained(save_directory)` method.\n                - a path or url to a saved configuration `file`.\n            **cache_dir**: (`optional`) string:\n                Path to a directory in which a downloaded pre-trained model\n                configuration should be cached if the standard cache should not be used.\n            **return_unused_kwargs**: (`optional`) bool:\n                - If False, then this function returns just the final configuration object.\n                - If True, then this functions returns a tuple `(config, unused_kwargs)` where `unused_kwargs`\n                is a dictionary consisting of the key/value pairs whose keys are not configuration attributes:\n                ie the part of kwargs which has not been used to update `config` and is otherwise ignored.\n            **kwargs**: (`optional`) dict:\n                Dictionary of key/value pairs with which to update the configuration object after loading.\n                - The values in kwargs of any keys which are configuration attributes will be used\n                to override the loaded values.\n                - Behavior concerning key/value pairs whose keys are *not* configuration attributes is controlled\n                by the `return_unused_kwargs` keyword parameter.\n\n        Examples::\n\n            >>> config = BertConfig.from_pretrained('bert-base-uncased')    # Download configuration from S3 and cache.\n            >>> config = BertConfig.from_pretrained('./test/saved_model/')  # E.g. config (or model) was saved using `save_pretrained('./test/saved_model/')`\n            >>> config = BertConfig.from_pretrained('./test/saved_model/my_configuration.json')\n            >>> config = BertConfig.from_pretrained('bert-base-uncased', output_attention=True, foo=False)\n            >>> assert config.output_attention == True\n            >>> config, unused_kwargs = BertConfig.from_pretrained('bert-base-uncased', output_attention=True,\n            >>>                                                    foo=False, return_unused_kwargs=True)\n            >>> assert config.output_attention == True\n            >>> assert unused_kwargs == {'foo': False}\n\n        \"\"\"\n        cache_dir = kwargs.pop('cache_dir', None)\n        return_unused_kwargs = kwargs.pop('return_unused_kwargs', False)\n\n        if pretrained_model_name_or_path in cls.pretrained_config_archive_map:\n            config_file = cls.pretrained_config_archive_map[pretrained_model_name_or_path]\n        elif os.path.isdir(pretrained_model_name_or_path):\n            config_file = os.path.join(pretrained_model_name_or_path, CONFIG_NAME)\n        else:\n            config_file = pretrained_model_name_or_path\n        # redirect to the cache, if necessary\n        try:\n            resolved_config_file = cached_path(config_file, cache_dir=cache_dir)\n        except EnvironmentError:\n            if pretrained_model_name_or_path in cls.pretrained_config_archive_map:\n                logger.error(\n                    \"Couldn't reach server at '{}' to download pretrained model configuration file.\".format(\n                        config_file))\n            else:\n                logger.error(\n                    \"Model name '{}' was not found in model name list ({}). \"\n                    \"We assumed '{}' was a path or url but couldn't find any file \"\n                    \"associated to this path or url.\".format(\n                        pretrained_model_name_or_path,\n                        ', '.join(cls.pretrained_config_archive_map.keys()),\n                        config_file))\n            return None\n        if resolved_config_file == config_file:\n            logger.info(\"loading configuration file {}\".format(config_file))\n        else:\n            logger.info(\"loading configuration file {} from cache at {}\".format(\n                config_file, resolved_config_file))\n\n        # Load config\n        config = cls.from_json_file(resolved_config_file)\n\n        # Update config with kwargs if needed\n        to_remove = []\n        for key, value in kwargs.items():\n            if hasattr(config, key):\n                setattr(config, key, value)\n                to_remove.append(key)\n        # add img_layer_norm_eps, use_img_layernorm\n        if \"img_layer_norm_eps\" in kwargs:\n            setattr(config, \"img_layer_norm_eps\", kwargs[\"img_layer_norm_eps\"])\n            to_remove.append(\"img_layer_norm_eps\")\n        if \"use_img_layernorm\" in kwargs:\n            setattr(config, \"use_img_layernorm\", kwargs[\"use_img_layernorm\"])\n            to_remove.append(\"use_img_layernorm\")\n        for key in to_remove:\n            kwargs.pop(key, None)\n\n        logger.info(\"Model config %s\", config)\n        if return_unused_kwargs:\n            return config, kwargs\n        else:\n            return config\n\n    @classmethod\n    def from_dict(cls, json_object):\n        \"\"\"Constructs a `Config` from a Python dictionary of parameters.\"\"\"\n        config = cls(vocab_size_or_config_json_file=-1)\n        for key, value in json_object.items():\n            config.__dict__[key] = value\n        return config\n\n    @classmethod\n    def from_json_file(cls, json_file):\n        \"\"\"Constructs a `BertConfig` from a json file of parameters.\"\"\"\n        with open(json_file, \"r\", encoding='utf-8') as reader:\n            text = reader.read()\n        return cls.from_dict(json.loads(text))\n\n    def __eq__(self, other):\n        return self.__dict__ == other.__dict__\n\n    def __repr__(self):\n        return str(self.to_json_string())\n\n    def to_dict(self):\n        \"\"\"Serializes this instance to a Python dictionary.\"\"\"\n        output = copy.deepcopy(self.__dict__)\n        return output\n\n    def to_json_string(self):\n        \"\"\"Serializes this instance to a JSON string.\"\"\"\n        return json.dumps(self.to_dict(), indent=2, sort_keys=True) + \"\\n\"\n\n    def to_json_file(self, json_file_path):\n        \"\"\" Save this instance to a json file.\"\"\"\n        with open(json_file_path, \"w\", encoding='utf-8') as writer:\n            writer.write(self.to_json_string())\n\n\nclass PreTrainedModel(nn.Module):\n    \"\"\" Base class for all models. Handle loading/storing model config and\n        a simple interface for dowloading and loading pretrained models.\n    \"\"\"\n    config_class = PretrainedConfig\n    pretrained_model_archive_map = {}\n    load_tf_weights = lambda model, config, path: None\n    base_model_prefix = \"\"\n    input_embeddings = None\n\n    def __init__(self, config, *inputs, **kwargs):\n        super(PreTrainedModel, self).__init__()\n        if not isinstance(config, PretrainedConfig):\n            raise ValueError(\n                \"Parameter config in `{}(config)` should be an instance of class `PretrainedConfig`. \"\n                \"To create a model from a pretrained model use \"\n                \"`model = {}.from_pretrained(PRETRAINED_MODEL_NAME)`\".format(\n                    self.__class__.__name__, self.__class__.__name__\n                ))\n        # Save config in model\n        self.config = config\n\n    def _get_resized_embeddings(self, old_embeddings, new_num_tokens=None):\n        \"\"\" Build a resized Embedding Module from a provided token Embedding Module.\n            Increasing the size will add newly initialized vectors at the end\n            Reducing the size will remove vectors from the end\n\n        Args:\n            new_num_tokens: (`optional`) int\n                New number of tokens in the embedding matrix.\n                Increasing the size will add newly initialized vectors at the end\n                Reducing the size will remove vectors from the end\n                If not provided or None: return the provided token Embedding Module.\n        Return: ``torch.nn.Embeddings``\n            Pointer to the resized Embedding Module or the old Embedding Module if new_num_tokens is None\n        \"\"\"\n        if new_num_tokens is None:\n            return old_embeddings\n\n        old_num_tokens, old_embedding_dim = old_embeddings.weight.size()\n        if old_num_tokens == new_num_tokens:\n            return old_embeddings\n\n        # Build new embeddings\n        new_embeddings = nn.Embedding(new_num_tokens, old_embedding_dim)\n        new_embeddings.to(old_embeddings.weight.device)\n\n        # initialize all new embeddings (in particular added tokens)\n        self.init_weights(new_embeddings)\n\n        # Copy word embeddings from the previous weights\n        num_tokens_to_copy = min(old_num_tokens, new_num_tokens)\n        new_embeddings.weight.data[:num_tokens_to_copy, :] = old_embeddings.weight.data[:num_tokens_to_copy, :]\n\n        return new_embeddings\n\n    def _tie_or_clone_weights(self, first_module, second_module):\n        \"\"\" Tie or clone module weights depending of weither we are using TorchScript or not\n        \"\"\"\n        if self.config.torchscript:\n            first_module.weight = nn.Parameter(second_module.weight.clone())\n        else:\n            first_module.weight = second_module.weight\n\n    def resize_token_embeddings(self, new_num_tokens=None):\n        \"\"\" Resize input token embeddings matrix of the model if new_num_tokens != config.vocab_size.\n            Take care of tying weights embeddings afterwards if the model class has a `tie_weights()` method.\n\n        Args:\n            new_num_tokens: (`optional`) int\n                New number of tokens in the embedding matrix.\n                Increasing the size will add newly initialized vectors at the end\n                Reducing the size will remove vectors from the end\n                If not provided or None: does nothing and just returns a pointer to the input tokens Embedding Module of the model.\n\n        Return: ``torch.nn.Embeddings``\n            Pointer to the input tokens Embedding Module of the model\n        \"\"\"\n        base_model = getattr(self, self.base_model_prefix, self)  # get the base model if needed\n        model_embeds = base_model._resize_token_embeddings(new_num_tokens)\n        if new_num_tokens is None:\n            return model_embeds\n\n        # Update base model and current model config\n        self.config.vocab_size = new_num_tokens\n        base_model.vocab_size = new_num_tokens\n\n        # Tie weights again if needed\n        if hasattr(self, 'tie_weights'):\n            self.tie_weights()\n\n        return model_embeds\n\n    def prune_heads(self, heads_to_prune):\n        \"\"\" Prunes heads of the base model.\n            Args:\n                heads_to_prune: dict of {layer_num (int): list of heads to prune in this layer (list of int)}\n        \"\"\"\n        base_model = getattr(self, self.base_model_prefix, self)  # get the base model if needed\n        base_model._prune_heads(heads_to_prune)\n\n    def save_pretrained(self, save_directory):\n        \"\"\" Save a model with its configuration file to a directory, so that it\n            can be re-loaded using the `from_pretrained(save_directory)` class method.\n        \"\"\"\n        assert os.path.isdir(save_directory), \"Saving path should be a directory where the model and configuration can be saved\"\n\n        # Only save the model it-self if we are using distributed training\n        model_to_save = self.module if hasattr(self, 'module') else self\n\n        # Save configuration file\n        model_to_save.config.save_pretrained(save_directory)\n\n        # If we save using the predefined names, we can load using `from_pretrained`\n        output_model_file = os.path.join(save_directory, WEIGHTS_NAME)\n\n        torch.save(model_to_save.state_dict(), output_model_file)\n\n    @classmethod\n    def from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs):\n        r\"\"\"Instantiate a pretrained pytorch model from a pre-trained model configuration.\n\n            The model is set in evaluation mode by default using `model.eval()` (Dropout modules are desactivated)\n            To train the model, you should first set it back in training mode with `model.train()`\n\n        Params:\n            **pretrained_model_name_or_path**: either:\n                - a string with the `shortcut name` of a pre-trained model to load from cache\n                    or download and cache if not already stored in cache (e.g. 'bert-base-uncased').\n                - a path to a `directory` containing a configuration file saved\n                    using the `save_pretrained(save_directory)` method.\n                - a path or url to a tensorflow index checkpoint `file` (e.g. `./tf_model/model.ckpt.index`).\n                    In this case, ``from_tf`` should be set to True and a configuration object should be\n                    provided as `config` argument. This loading option is slower than converting the TensorFlow\n                    checkpoint in a PyTorch model using the provided conversion scripts and loading\n                    the PyTorch model afterwards.\n            **model_args**: (`optional`) Sequence:\n                All remaning positional arguments will be passed to the underlying model's __init__ function\n            **config**: an optional configuration for the model to use instead of an automatically loaded configuation.\n                Configuration can be automatically loaded when:\n                - the model is a model provided by the library (loaded with a `shortcut name` of a pre-trained model), or\n                - the model was saved using the `save_pretrained(save_directory)` (loaded by suppling the save directory).\n            **state_dict**: an optional state dictionnary for the model to use instead of a state dictionary loaded\n                from saved weights file.\n                This option can be used if you want to create a model from a pretrained configuraton but load your own weights.\n                In this case though, you should check if using `save_pretrained(dir)` and `from_pretrained(save_directory)` is not\n                a simpler option.\n            **cache_dir**: (`optional`) string:\n                Path to a directory in which a downloaded pre-trained model\n                configuration should be cached if the standard cache should not be used.\n            **output_loading_info**: (`optional`) boolean:\n                Set to ``True`` to also return a dictionnary containing missing keys, unexpected keys and error messages.\n            **kwargs**: (`optional`) dict:\n                Dictionary of key, values to update the configuration object after loading.\n                Can be used to override selected configuration parameters. E.g. ``output_attention=True``.\n\n               - If a configuration is provided with `config`, **kwargs will be directly passed\n                 to the underlying model's __init__ method.\n               - If a configuration is not provided, **kwargs will be first passed to the pretrained\n                 model configuration class loading function (`PretrainedConfig.from_pretrained`).\n                 Each key of **kwargs that corresponds to a configuration attribute\n                 will be used to override said attribute with the supplied **kwargs value.\n                 Remaining keys that do not correspond to any configuration attribute will\n                 be passed to the underlying model's __init__ function.\n\n        Examples::\n\n            >>> model = BertModel.from_pretrained('bert-base-uncased')    # Download model and configuration from S3 and cache.\n            >>> model = BertModel.from_pretrained('./test/saved_model/')  # E.g. model was saved using `save_pretrained('./test/saved_model/')`\n            >>> model = BertModel.from_pretrained('bert-base-uncased', output_attention=True)  # Update configuration during loading\n            >>> assert model.config.output_attention == True\n            >>> # Loading from a TF checkpoint file instead of a PyTorch model (slower)\n            >>> config = BertConfig.from_json_file('./tf_model/my_tf_model_config.json')\n            >>> model = BertModel.from_pretrained('./tf_model/my_tf_checkpoint.ckpt.index', from_tf=True, config=config)\n\n        \"\"\"\n        config = kwargs.pop('config', None)\n        state_dict = kwargs.pop('state_dict', None)\n        cache_dir = kwargs.pop('cache_dir', None)\n        from_tf = kwargs.pop('from_tf', False)\n        output_loading_info = kwargs.pop('output_loading_info', False)\n\n        # Load config\n        if config is None:\n            config, model_kwargs = cls.config_class.from_pretrained(\n                pretrained_model_name_or_path, *model_args,\n                cache_dir=cache_dir, return_unused_kwargs=True,\n                **kwargs\n            )\n        else:\n            model_kwargs = kwargs\n\n        # Load model\n        if pretrained_model_name_or_path in cls.pretrained_model_archive_map:\n            archive_file = cls.pretrained_model_archive_map[pretrained_model_name_or_path]\n        elif os.path.isdir(pretrained_model_name_or_path):\n            if from_tf:\n                # Directly load from a TensorFlow checkpoint\n                archive_file = os.path.join(pretrained_model_name_or_path, TF_WEIGHTS_NAME + \".index\")\n            else:\n                archive_file = os.path.join(pretrained_model_name_or_path, WEIGHTS_NAME)\n        else:\n            if from_tf:\n                # Directly load from a TensorFlow checkpoint\n                archive_file = pretrained_model_name_or_path + \".index\"\n            else:\n                archive_file = pretrained_model_name_or_path\n        # redirect to the cache, if necessary\n        try:\n            resolved_archive_file = cached_path(archive_file, cache_dir=cache_dir)\n        except EnvironmentError:\n            if pretrained_model_name_or_path in cls.pretrained_model_archive_map:\n                logger.error(\n                    \"Couldn't reach server at '{}' to download pretrained weights.\".format(\n                        archive_file))\n            else:\n                logger.error(\n                    \"Model name '{}' was not found in model name list ({}). \"\n                    \"We assumed '{}' was a path or url but couldn't find any file \"\n                    \"associated to this path or url.\".format(\n                        pretrained_model_name_or_path,\n                        ', '.join(cls.pretrained_model_archive_map.keys()),\n                        archive_file))\n            return None\n        if resolved_archive_file == archive_file:\n            logger.info(\"loading weights file {}\".format(archive_file))\n        else:\n            logger.info(\"loading weights file {} from cache at {}\".format(\n                archive_file, resolved_archive_file))\n\n        # Instantiate model.\n        model = cls(config, *model_args, **model_kwargs)\n\n        if state_dict is None and not from_tf:\n            state_dict = torch.load(resolved_archive_file, map_location='cpu')\n\n        if from_tf:\n            # Directly load from a TensorFlow checkpoint\n            return cls.load_tf_weights(model, config, resolved_archive_file[:-6])  # Remove the '.index'\n\n        # Convert old format to new format if needed from a PyTorch state_dict\n        old_keys = []\n        new_keys = []\n        for key in state_dict.keys():\n            new_key = None\n            if 'gamma' in key:\n                new_key = key.replace('gamma', 'weight')\n            if 'beta' in key:\n                new_key = key.replace('beta', 'bias')\n            if new_key:\n                old_keys.append(key)\n                new_keys.append(new_key)\n        for old_key, new_key in zip(old_keys, new_keys):\n            state_dict[new_key] = state_dict.pop(old_key)\n\n        # Load from a PyTorch state_dict\n        missing_keys = []\n        unexpected_keys = []\n        error_msgs = []\n        # copy state_dict so _load_from_state_dict can modify it\n        metadata = getattr(state_dict, '_metadata', None)\n        state_dict = state_dict.copy()\n        if metadata is not None:\n            state_dict._metadata = metadata\n\n        def load(module, prefix=''):\n            local_metadata = {} if metadata is None else metadata.get(prefix[:-1], {})\n            module._load_from_state_dict(\n                state_dict, prefix, local_metadata, True, missing_keys, unexpected_keys, error_msgs)\n            for name, child in module._modules.items():\n                if child is not None:\n                    load(child, prefix + name + '.')\n\n        # Make sure we are able to load base models as well as derived models (with heads)\n        start_prefix = ''\n        model_to_load = model\n        if not hasattr(model, cls.base_model_prefix) and any(s.startswith(cls.base_model_prefix) for s in state_dict.keys()):\n            start_prefix = cls.base_model_prefix + '.'\n        if hasattr(model, cls.base_model_prefix) and not any(s.startswith(cls.base_model_prefix) for s in state_dict.keys()):\n            model_to_load = getattr(model, cls.base_model_prefix)\n\n        load(model_to_load, prefix=start_prefix)\n        if len(missing_keys) > 0:\n            logger.info(\"Weights of {} not initialized from pretrained model: {}\".format(\n                model.__class__.__name__, missing_keys))\n        if len(unexpected_keys) > 0:\n            logger.info(\"Weights from pretrained model not used in {}: {}\".format(\n                model.__class__.__name__, unexpected_keys))\n        if len(error_msgs) == 2 and \"size mismatch for cls.seq_relationship.weight\" in error_msgs[0]:\n            logger.info('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n                model.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n        elif len(error_msgs) > 0:\n            raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n                               model.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\n        if hasattr(model, 'tie_weights'):\n            model.tie_weights()  # make sure word embedding weights are still tied\n\n        # Set model in evaluation mode to desactivate DropOut modules by default\n        model.eval()\n\n        if output_loading_info:\n            loading_info = {\"missing_keys\": missing_keys, \"unexpected_keys\": unexpected_keys, \"error_msgs\": error_msgs}\n            return model, loading_info\n\n        return model\n\n    def prepare_inputs_for_generation(self, input_ids, **kwargs):\n        return {\"input_ids\": input_ids}\n\n    def _do_output_past(self, outputs):\n        has_output_past = hasattr(self.config, \"output_past\") and self.config.output_past\n        has_mem_len = hasattr(self.config, \"mem_len\") and self.config.mem_len\n\n        if has_output_past and not has_mem_len and len(outputs) > 1:\n            return True\n        elif has_mem_len and self.config.mem_len > 0 and len(outputs) > 1:\n            return True\n\n        return False\n\n    def generate(\n        self,\n        input_ids=None,\n        max_length=None,\n        do_sample=None,\n        num_beams=None,\n        temperature=None,\n        top_k=None,\n        top_p=None,\n        repetition_penalty=None,\n        bos_token_id=None,\n        pad_token_id=None,\n        eos_token_ids=None,\n        length_penalty=None,\n        num_return_sequences=None,\n    ):\n        r\"\"\" Generates sequences for models with a LM head. The method currently supports greedy or penalized greedy decoding, sampling with top-k or nucleus sampling\n        and beam-search.\n\n        Adapted in part from `Facebook's XLM beam search code`_.\n\n        .. _`Facebook's XLM beam search code`:\n           https://github.com/facebookresearch/XLM/blob/9e6f6814d17be4fe5b15f2e6c43eb2b2d76daeb4/src/model/transformer.py#L529\n\n\n        Parameters:\n\n            input_ids: (`optional`) `torch.LongTensor` of shape `(batch_size, sequence_length)`\n                The sequence used as a prompt for the generation. If `None` the method initializes\n                it as an empty `torch.LongTensor` of shape `(1,)`.\n\n            max_length: (`optional`) int\n                The max length of the sequence to be generated.  Between 1 and infinity. Default to 20.\n\n            do_sample: (`optional`) bool\n                If set to `False` greedy decoding is used. Otherwise sampling is used. Default to greedy sampling.\n\n            num_beams: (`optional`) int\n                Number of beams for beam search. Must be between 1 and infinity. 1 means no beam search. Default to 1.\n\n            temperature: (`optional`) float\n                The value used to module the next token probabilities. Must be strictely positive. Default to 1.0.\n\n            top_k: (`optional`) int\n                The number of highest probability vocabulary tokens to keep for top-k-filtering. Between 1 and infinity. Default to 50.\n\n            top_p: (`optional`) float\n                The cumulative probability of parameter highest probability vocabulary tokens to keep for nucleus sampling. Must be between 0 and 1. Default to 1.\n\n            repetition_penalty: (`optional`) float\n                The parameter for repetition penalty. Between 1.0 and infinity. 1.0 means no penalty. Default to 1.0.\n\n            bos_token_id: (`optional`) int\n                Beginning of sentence token if no prompt is provided. Default to 0.\n\n            eos_token_ids: (`optional`) int or list of int\n                End of sequence token or list of tokens to stop the generation. Default to 0.\n            length_penalty: (`optional`) float\n                Exponential penalty to the length. Default to 1.\n\n            num_return_sequences: (`optional`) int\n                The number of independently computed returned sequences for each element in the batch. Default to 1.\n\n        Examples::\n\n            tokenizer = AutoTokenizer.from_pretrained('distilgpt2')   # Initialize tokenizer\n            model = AutoModelWithLMHead.from_pretrained('distilgpt2')    # Download model and configuration from S3 and cache.\n            outputs = model.generate(max_length=40, bos_token_id=tokenizer.bos_token_id, eos_token_ids=tokenizer.eos_token_id)  # do greedy decoding without beam search\n            print('Generated: {}'.format(tokenizer.decode(outputs[0], skip_special_tokens=True)))\n\n            tokenizer = AutoTokenizer.from_pretrained('openai-gpt')   # Initialize tokenizer\n            model = AutoModelWithLMHead.from_pretrained('openai-gpt')    # Download model and configuration from S3 and cache.\n            input_context = 'The dog'\n            input_ids = torch.tensor(tokenizer.encode(input_context)).unsqueeze(0)  # encode input context\n            outputs = model.generate(input_ids=input_ids, do_sample=True, num_beams=5, num_return_sequences=3, temperature=1.5)  # generate 3 independent sequences using beam search decoding (5 beams) with sampling from initial context 'The dog'\n            for i in range(3): #  3 output sequences were generated\n                print('Generated {}: {}'.format(i, tokenizer.decode(outputs[0][i], skip_special_tokens=True)))\n\n            tokenizer = AutoTokenizer.from_pretrained('distilgpt2')   # Initialize tokenizer\n            model = AutoModelWithLMHead.from_pretrained('distilgpt2')    # Download model and configuration from S3 and cache.\n            input_context = 'The dog'\n            input_ids = torch.tensor(tokenizer.encode(input_context)).unsqueeze(0)  # encode input context\n            outputs = model.generate(input_ids=input_ids, max_length=40, temperature=0.7, bos_token_id=tokenizer.bos_token_id, eos_token_ids=tokenizer.eos_token_id, num_beams=3)  # generate sequences using greedy beam search decoding (3 beams)\n            print('Generated: {}'.format(tokenizer.decode(outputs[0], skip_special_tokens=True)))\n\n            tokenizer = AutoTokenizer.from_pretrained('ctrl')   # Initialize tokenizer\n            model = AutoModelWithLMHead.from_pretrained('ctrl')    # Download model and configuration from S3 and cache.\n            input_context = 'Legal My neighbor is'  # \"Legal\" is one of the control codes for ctrl\n            input_ids = torch.tensor(tokenizer.encode(input_context)).unsqueeze(0)  # encode input context\n            outputs = model.generate(input_ids=input_ids, max_length=50, temperature=0.7, repetition_penalty=1.2)  # generate sequences using using greedy search\n            print('Generated: {}'.format(tokenizer.decode(outputs[0], skip_special_tokens=True)))\n\n        \"\"\"\n\n        # We cannot generate if the model does not have a LM head\n        if self.get_output_embeddings() is None:\n            raise AttributeError(\n                \"You tried to generate sequences with a model that does not have a LM Head.\"\n                \"Please use another model class (e.g. `OpenAIGPTLMHeadModel`, `XLNetLMHeadModel`, `GPT2LMHeadModel`, `CTRLLMHeadModel`, `T5WithLMHeadModel`, `TransfoXLLMHeadModel`)\"\n            )\n\n        max_length = max_length if max_length is not None else self.config.max_length\n        do_sample = do_sample if do_sample is not None else self.config.do_sample\n        num_beams = num_beams if num_beams is not None else self.config.num_beams\n        temperature = temperature if temperature is not None else self.config.temperature\n        top_k = top_k if top_k is not None else self.config.top_k\n        top_p = top_p if top_p is not None else self.config.top_p\n        repetition_penalty = repetition_penalty if repetition_penalty is not None else self.config.repetition_penalty\n        bos_token_id = bos_token_id if bos_token_id is not None else self.config.bos_token_id\n        pad_token_id = pad_token_id if pad_token_id is not None else self.config.pad_token_id\n        eos_token_ids = eos_token_ids if eos_token_ids is not None else self.config.eos_token_ids\n        length_penalty = length_penalty if length_penalty is not None else self.config.length_penalty\n        num_return_sequences = (\n            num_return_sequences if num_return_sequences is not None else self.config.num_return_sequences\n        )\n\n        if input_ids is not None:\n            batch_size = input_ids.shape[0]  # overriden by the input batch_size\n        else:\n            batch_size = 1\n        if isinstance(eos_token_ids, int):\n            eos_token_ids = [eos_token_ids]\n\n        assert isinstance(max_length, int) and max_length > 0, \"`max_length` should be a strictely positive integer.\"\n        assert isinstance(do_sample, bool), \"`do_sample` should be a boolean.\"\n        assert isinstance(num_beams, int) and num_beams > 0, \"`num_beams` should be a strictely positive integer.\"\n        assert temperature > 0, \"`temperature` should be strictely positive.\"\n        assert isinstance(top_k, int) and top_k >= 0, \"`top_k` should be a positive integer.\"\n        assert 0 <= top_p <= 1, \"`top_p` should be between 0 and 1.\"\n        assert repetition_penalty >= 1.0, \"`repetition_penalty` should be >= 1.\"\n        assert isinstance(bos_token_id, int) and bos_token_id >= 0, \"`bos_token_id` should be a positive integer.\"\n        assert isinstance(pad_token_id, int) and pad_token_id >= 0, \"`pad_token_id` should be a positive integer.\"\n        assert isinstance(eos_token_ids, (list, tuple)) and (\n            e >= 0 for e in eos_token_ids\n        ), \"`eos_token_ids` should be a positive integer or a list/tuple of positive integers.\"\n        assert length_penalty > 0, \"`length_penalty` should be strictely positive.\"\n        assert (\n            isinstance(num_return_sequences, int) and num_return_sequences > 0\n        ), \"`num_return_sequences` should be a strictely positive integer.\"\n\n        if input_ids is None:\n            input_ids = torch.full(\n                (batch_size, 1), bos_token_id, dtype=torch.long, device=next(self.parameters()).device\n            )\n        else:\n            assert input_ids.dim() == 2, \"Input prompt should be of shape (batch_size, sequence length).\"\n\n        # current position and vocab size\n        cur_len = input_ids.shape[1]\n        vocab_size = self.config.vocab_size\n\n        if num_return_sequences != 1:\n            # Expand input to num return sequences\n            input_ids = input_ids.unsqueeze(1).expand(batch_size, num_return_sequences, cur_len)\n            input_ids = input_ids.contiguous().view(\n                batch_size * num_return_sequences, cur_len\n            )  # (batch_size * num_return_sequences, cur_len)\n            effective_batch_size = batch_size * num_return_sequences\n        else:\n            effective_batch_size = batch_size\n\n        if num_beams > 1:\n            output = self._generate_beam_search(\n                input_ids,\n                cur_len,\n                max_length,\n                do_sample,\n                temperature,\n                top_k,\n                top_p,\n                repetition_penalty,\n                pad_token_id,\n                eos_token_ids,\n                effective_batch_size,\n                length_penalty,\n                num_beams,\n                vocab_size,\n            )\n        else:\n            output = self._generate_no_beam_search(\n                input_ids,\n                cur_len,\n                max_length,\n                do_sample,\n                temperature,\n                top_k,\n                top_p,\n                repetition_penalty,\n                pad_token_id,\n                eos_token_ids,\n                effective_batch_size,\n            )\n\n        if num_return_sequences != 1:\n            for i in range(len(output)):\n                output[i] = output[i].view(batch_size, num_return_sequences, -1)\n        return output\n\n    def _decode_step(self, input_ids, past):\n        model_inputs = self.prepare_inputs_for_generation(input_ids, past=past)\n        outputs = self(**model_inputs)  # (batch_size * num_beams, cur_len, vocab_size)\n        token_len = outputs[0].shape[1]\n        if self.od_labels_len == 0:\n            next_token_idx = token_len - 1\n        else:\n            if token_len == 2:\n                assert self._do_output_past(outputs)\n                next_token_idx = 1\n            else:\n                next_token_idx = token_len - self.od_labels_len - 1\n\n        next_token_logits = outputs[0][:, next_token_idx, :]  # (batch_size * num_beams, vocab_size)\n        assert outputs[0].shape[1] == model_inputs['input_ids'].shape[1]\n\n        # if model has past, then set the past variable to speed up decoding\n        if self._do_output_past(outputs):\n            past = outputs[1]\n        return next_token_logits, past\n\n    def _generate_no_beam_search(\n        self,\n        input_ids,\n        cur_len,\n        max_length,\n        do_sample,\n        temperature,\n        top_k,\n        top_p,\n        repetition_penalty,\n        bos_token_id,\n        pad_token_id,\n        eos_token_ids,\n        batch_size,\n    ):\n        \"\"\" Generate sequences for each example without beam search (num_beams == 1).\n            All returned sequence are generated independantly.\n        \"\"\"\n        assert self.num_keep_best == 1, 'cannot generate >1 sentences in greedy search'\n        # current position / max lengths / length of generated sentences / unfinished sentences\n        unfinished_sents = []\n        if torch._C._get_tracing_state():\n            cur_unfinished = torch.ones(1, dtype=input_ids)\n        else:\n            cur_unfinished = input_ids.new(batch_size).fill_(1)\n\n        # log of scores for each sentence in the batch\n        logprobs = []\n\n        past = None\n\n        generate_caption_num = 0\n        while cur_len < max_length:\n            model_inputs = self.prepare_inputs_for_generation(input_ids, past=past)\n            outputs = self(**model_inputs)\n            if cur_len == 1:\n                token_len = 2 + self.od_labels_len\n                next_token_idx = 1\n            else:\n                assert cur_len > 1\n                if not self._do_output_past(outputs):\n                    token_len = cur_len + 1 + self.od_labels_len\n                    next_token_idx = cur_len\n                else:\n                    token_len = 2\n                    next_token_idx = 1\n\n            assert outputs[0].shape[1] == token_len, f\"{outputs[0].shape[1], {token_len}}\"\n            next_token_logits = outputs[0][:, next_token_idx, :]\n\n            # if model has past, then set the past variable to speed up decoding\n            if self._do_output_past(outputs):\n                past = outputs[1]\n\n            # repetition penalty from CTRL paper (https://arxiv.org/abs/1909.05858)\n            if repetition_penalty != 1.0:\n                for i in range(batch_size):\n                    for previous_token in set(input_ids[i].tolist()):\n                        # if score < 0 then repetition penalty has to multiplied to reduce the previous token probability\n                        if next_token_logits[i, previous_token] < 0:\n                            next_token_logits[i, previous_token] *= repetition_penalty\n                        else:\n                            next_token_logits[i, previous_token] /= repetition_penalty\n\n            if do_sample:\n                # Temperature (higher temperature => more likely to sample low probability tokens)\n                if temperature != 1.0:\n                    next_token_logits = next_token_logits / temperature\n                # Top-p/top-k filtering\n                next_token_logits = top_k_top_p_filtering(next_token_logits, top_k=top_k, top_p=top_p)\n                # Sample\n                next_token = torch.multinomial(F.softmax(next_token_logits, dim=-1), num_samples=1).squeeze(1)\n            else:\n                # Greedy decoding\n                next_token = torch.argmax(next_token_logits, dim=-1)\n\n            # Compute scores\n            _scores = F.log_softmax(next_token_logits, dim=-1)  # (batch_size, vocab_size)\n            _scores = torch.gather(_scores, -1, next_token.unsqueeze(-1))  # (batch_size, 1)\n            logprobs.append(_scores)  # (batch_size, 1)\n            unfinished_sents.append(cur_unfinished)\n\n            # update generations and finished sentences\n            tokens_to_add = next_token * cur_unfinished + pad_token_id * (1 - cur_unfinished)\n            input_ids = torch.cat([input_ids, tokens_to_add.unsqueeze(-1)], dim=-1)\n\n            #for t in input_ids:\n                #print(self.tokenizer.convert_ids_to_tokens(t.tolist()))\n\n            for eos_token_id in eos_token_ids:\n                cur_unfinished = cur_unfinished.mul(tokens_to_add.ne(eos_token_id).long())\n            cur_len = cur_len + 1\n\n            # stop when there is a </s> in each sentence, or if we exceed the maximul length\n            if cur_unfinished.max() == 0:\n                if self.use_sep_cap and generate_caption_num == 0:\n                    half_pad_len = max_length//2 - input_ids.shape[1]\n                    if half_pad_len > 0:\n                        padding_ids = input_ids.new(batch_size, half_pad_len).fill_(pad_token_id)\n                        input_ids = torch.cat([input_ids, padding_ids], dim=1)\n                        cur_len += half_pad_len\n                    input_ids = torch.cat([input_ids, (bos_token_id * (1 - cur_unfinished)).unsqueeze(-1)], dim=-1)\n                    if torch._C._get_tracing_state():\n                        cur_unfinished = torch.ones(1, dtype=input_ids)\n                    else:\n                        cur_unfinished = input_ids.new(batch_size).fill_(1)\n                    cur_len += 1\n                    generate_caption_num += 1\n                    continue\n                else:\n                    break\n\n        # add eos_token_ids to unfinished sentences\n        if cur_len == max_length:\n            input_ids[:, -1].masked_fill_(cur_unfinished.to(dtype=torch.bool), eos_token_ids[0])\n\n        logprobs = torch.cat(logprobs, dim=1)\n        unfinished_sents = torch.stack(unfinished_sents, dim=1).float()\n        sum_logprobs = (logprobs * unfinished_sents).sum(dim=1)\n        # return logprobs to keep consistent with beam search output\n        logprobs = sum_logprobs / unfinished_sents.sum(dim=1)\n\n        # pad to the same length, otherwise DataParallel will give error\n        pad_len = max_length - input_ids.shape[1]\n        if pad_len > 0:\n            padding_ids = input_ids.new(batch_size, pad_len).fill_(pad_token_id)\n            input_ids = torch.cat([input_ids, padding_ids], dim=1)\n\n        # (batch_size, n_best, max_len), (batch_size, n_best)\n        return input_ids.unsqueeze(1), logprobs.unsqueeze(1)\n\n    def _generate_beam_search(\n        self,\n        input_ids,\n        cur_len,\n        max_length,\n        do_sample,\n        temperature,\n        top_k,\n        top_p,\n        repetition_penalty,\n        pad_token_id,\n        eos_token_ids,\n        batch_size,\n        length_penalty,\n        num_beams,\n        vocab_size,\n    ):\n        \"\"\" Generate sequences for each example with beam search.\n        \"\"\"\n        # Expand input to num beams\n        input_ids = input_ids.unsqueeze(1).expand(batch_size, num_beams, cur_len)\n        input_ids = input_ids.contiguous().view(batch_size * num_beams, cur_len)  # (batch_size * num_beams, cur_len)\n\n        # generated hypotheses\n        num_keep_best = self.num_keep_best\n        generated_hyps = [\n            BeamHypotheses(num_keep_best, max_length, length_penalty, early_stopping=False) for _ in range(batch_size)\n        ]\n        # NOTE: Expand >1 words to leave some spare tokens to keep the\n        # beam size, because some sentences may end here and cannot expand\n        # in the next level\n        TOPN_PER_BEAM = 2\n\n        # scores for each sentence in the beam\n        beam_scores = torch.zeros((batch_size, num_beams), dtype=torch.float, device=input_ids.device)\n        beam_scores[:, 1:] = -1e9\n        beam_scores = beam_scores.view(-1)  # shape (batch_size * num_beams,)\n\n        # cache compute states\n        past = None\n\n        # done sentences\n        done = [False for _ in range(batch_size)]\n\n        while cur_len < max_length:\n            model_inputs = self.prepare_inputs_for_generation(input_ids, past=past)\n            outputs = self(**model_inputs)  # (batch_size * num_beams, cur_len, vocab_size)\n            if cur_len == 1:\n                token_len = 2 + self.od_labels_len\n                next_token_idx = 1\n            else:\n                assert cur_len > 1\n                if not self._do_output_past(outputs):\n                    token_len = cur_len + 1 + self.od_labels_len\n                    next_token_idx = cur_len\n                else:\n                    token_len = 2\n                    next_token_idx = 1\n\n            assert outputs[0].shape[1] == token_len\n            scores = outputs[0][:, next_token_idx, :]  # (batch_size * num_beams, vocab_size)\n            assert outputs[0].shape[1] == model_inputs['input_ids'].shape[1]\n\n            # if model has past, then set the past variable to speed up decoding\n            if self._do_output_past(outputs):\n                past = outputs[1]\n\n            # repetition penalty (from CTRL paper https://arxiv.org/abs/1909.05858)\n            if repetition_penalty != 1.0:\n                for i in range(batch_size * num_beams):\n                    for previous_token in set(input_ids[i].tolist()):\n                        # if score < 0 then repetition penalty has to multiplied to reduce the previous token probability\n                        if scores[i, previous_token] < 0:\n                            scores[i, previous_token] *= repetition_penalty\n                        else:\n                            scores[i, previous_token] /= repetition_penalty\n\n            if do_sample:\n                # Temperature (higher temperature => more likely to sample low probability tokens)\n                if temperature != 1.0:\n                    scores = scores / temperature\n                # Top-p/top-k filtering\n                scores = top_k_top_p_filtering(\n                    scores, top_k=top_k, top_p=top_p, min_tokens_to_keep=2\n                )  # (batch_size * num_beams, vocab_size)\n                # Sample [TOPN_PER_BEAM] next words for each beam (so we have some spare tokens and match output of greedy beam search)\n                next_words = torch.multinomial(F.softmax(scores, dim=-1),\n                        num_samples=TOPN_PER_BEAM)  # (batch_size * num_beams, TOPN_PER_BEAM)\n                # Compute next scores\n                _scores = F.log_softmax(scores, dim=-1)  # (batch_size * num_beams, vocab_size)\n                _scores = torch.gather(_scores, -1, next_words)  # (batch_size * num_beams, TOPN_PER_BEAM)\n                next_scores = _scores + beam_scores[:, None].expand_as(_scores)  # (batch_size * num_beams, TOPN_PER_BEAM)\n                # Match shape of greedy beam search\n                beam_indices = torch.arange(num_beams) * vocab_size\n                beam_indices = beam_indices.repeat(batch_size, TOPN_PER_BEAM).to(next_words.device)\n                next_words = next_words.view(batch_size, TOPN_PER_BEAM * num_beams)  # (batch_size, TOPN_PER_BEAM * num_beams)\n                next_words = next_words + beam_indices\n                next_scores = next_scores.view(batch_size, TOPN_PER_BEAM * num_beams)  # (batch_size, TOPN_PER_BEAM * num_beams)\n            else:\n                # do greedy beam search\n                scores = F.log_softmax(scores, dim=-1)  # (batch_size * num_beams, vocab_size)\n                assert scores.size() == (batch_size * num_beams, vocab_size)\n                # Add the log prob of the new beams to the log prob of the beginning of the sequence (sum of logs == log of the product)\n                _scores = scores + beam_scores[:, None].expand_as(scores)  # (batch_size * num_beams, vocab_size)\n                # re-organize to group the beam together (we are keeping top hypothesis accross beams)\n                _scores = _scores.view(batch_size, num_beams * vocab_size)  # (batch_size, num_beams * vocab_size)\n                next_scores, next_words = torch.topk(_scores, TOPN_PER_BEAM * num_beams, dim=1, largest=True, sorted=True)\n\n            assert next_scores.size() == next_words.size() == (batch_size, TOPN_PER_BEAM * num_beams)\n\n            # next batch beam content\n            # list of (batch_size * num_beams) tuple(next hypothesis score, next word, current position in the batch)\n            next_batch_beam = []\n\n            # for each sentence\n            for batch_ex in range(batch_size):\n\n                # if we are done with this sentence\n                done[batch_ex] = done[batch_ex] or generated_hyps[batch_ex].is_done(next_scores[batch_ex].max().item())\n                if done[batch_ex]:\n                    next_batch_beam.extend([(0, pad_token_id, 0)] * num_beams)  # pad the batch\n                    continue\n\n                # next sentence beam content\n                next_sent_beam = []\n\n                # next words for this sentence\n                for idx, score in zip(next_words[batch_ex], next_scores[batch_ex]):\n\n                    # get beam and word IDs\n                    beam_id = idx // vocab_size\n                    word_id = idx % vocab_size\n\n                    # end of sentence, or next word\n                    if word_id.item() in eos_token_ids or cur_len + 1 == max_length:\n                        generated_hyps[batch_ex].add(\n                            input_ids[batch_ex * num_beams + beam_id, :cur_len].clone(), score.item()\n                        )\n                    else:\n                        next_sent_beam.append((score, word_id, batch_ex * num_beams + beam_id))\n\n                    # the beam for next step is full\n                    if len(next_sent_beam) == num_beams:\n                        break\n\n                # update next beam content\n                if cur_len + 1 == max_length:\n                    assert len(next_sent_beam) == 0\n                else:\n                    assert len(next_sent_beam) == num_beams\n\n                if len(next_sent_beam) == 0:\n                    next_sent_beam = [(0, pad_token_id, 0)] * num_beams  # pad the batch\n                next_batch_beam.extend(next_sent_beam)\n                assert len(next_batch_beam) == num_beams * (batch_ex + 1)\n\n            # sanity check / prepare next batch\n            assert len(next_batch_beam) == batch_size * num_beams\n            beam_scores = beam_scores.new([x[0] for x in next_batch_beam])\n            beam_words = input_ids.new([x[1] for x in next_batch_beam])\n            beam_idx = input_ids.new([x[2] for x in next_batch_beam])\n\n            # re-order batch\n            input_ids = input_ids[beam_idx, :]\n            input_ids = torch.cat([input_ids, beam_words.unsqueeze(1)], dim=-1)\n\n            # re-order internal states\n            if past:\n                reordered_past = []\n                for layer_past in past:\n                    # get the correct batch idx from layer past batch dim\n                    # batch dim of `past` and `mems` is at 1st position\n                    reordered_layer_past = [layer_past[i].unsqueeze(0).clone().detach() for i in beam_idx]\n                    reordered_layer_past = torch.cat(reordered_layer_past, dim=0)\n                    # check that shape matches\n                    assert reordered_layer_past.shape == layer_past.shape\n                    reordered_past.append(reordered_layer_past)\n                past = tuple(reordered_past)\n\n            # update current length\n            cur_len = cur_len + 1\n\n            # stop when we are done with each sentence\n            if all(done):\n                break\n\n        # visualize hypotheses\n        # print([len(x) for x in generated_hyps], cur_len)\n        # globals().update( locals() );\n        # !import code; code.interact(local=vars())\n        # for ii in range(batch_size):\n        #     for ss, ww in sorted(generated_hyps[ii].hyp, key=lambda x: x[0], reverse=True):\n        #         print(\"%.3f \" % ss + \" \".join(self.dico[x] for x in ww.tolist()))\n        #     print(\"\")\n\n        # select the best hypotheses\n        tgt_len = torch.ones(batch_size, num_keep_best, dtype=torch.long)\n        logprobs = torch.zeros(batch_size, num_keep_best,\n                dtype=torch.float).fill_(-1e5).to(input_ids.device)\n        all_best = []\n\n        for i, hypotheses in enumerate(generated_hyps):\n            best = []\n            hyp_scores = torch.tensor([x[0] for x in hypotheses.hyp])\n            _, best_indices = torch.topk(hyp_scores,\n                    min(num_keep_best, len(hyp_scores)), largest=True)\n            for best_idx, hyp_idx in enumerate(best_indices):\n                conf, best_hyp = hypotheses.hyp[hyp_idx]\n                best.append(best_hyp)\n                logprobs[i, best_idx] = conf\n                tgt_len[i, best_idx] = len(best_hyp) + 1  # +1 for the <EOS> symbol\n\n            all_best.append(best)\n\n        # generate target batch, pad to the same length\n        decoded = input_ids.new(batch_size, num_keep_best, max_length).fill_(pad_token_id)\n        for batch_idx, best in enumerate(all_best):\n            for best_idx, hypo in enumerate(best):\n                decoded[batch_idx, best_idx, : tgt_len[batch_idx, best_idx] - 1] = hypo\n                decoded[batch_idx, best_idx, tgt_len[batch_idx, best_idx] - 1] = eos_token_ids[0]\n\n        return decoded, logprobs\n\n\ndef top_k_top_p_filtering(logits, top_k=0, top_p=1.0, filter_value=-float(\"Inf\"), min_tokens_to_keep=1):\n    \"\"\" Filter a distribution of logits using top-k and/or nucleus (top-p) filtering\n        Args:\n            logits: logits distribution shape (batch size, vocabulary size)\n            if top_k > 0: keep only top k tokens with highest probability (top-k filtering).\n            if top_p < 1.0: keep the top tokens with cumulative probability >= top_p (nucleus filtering).\n                Nucleus filtering is described in Holtzman et al. (http://arxiv.org/abs/1904.09751)\n            Make sure we keep at least min_tokens_to_keep per batch example in the output\n        From: https://gist.github.com/thomwolf/1a5a29f6962089e871b94cbd09daf317\n    \"\"\"\n    if top_k > 0:\n        top_k = min(max(top_k, min_tokens_to_keep), logits.size(-1))  # Safety check\n        # Remove all tokens with a probability less than the last token of the top-k\n        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n        logits[indices_to_remove] = filter_value\n\n    if top_p < 1.0:\n        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n\n        # Remove tokens with cumulative probability above the threshold (token with 0 are kept)\n        sorted_indices_to_remove = cumulative_probs > top_p\n        if min_tokens_to_keep > 1:\n            # Keep at least min_tokens_to_keep (set to min_tokens_to_keep-1 because we add the first one below)\n            sorted_indices_to_remove[..., :min_tokens_to_keep] = 0\n        # Shift the indices to the right to keep also the first token above the threshold\n        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n        sorted_indices_to_remove[..., 0] = 0\n\n        # scatter sorted tensors to original indexing\n        indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n        logits[indices_to_remove] = filter_value\n    return logits\n\n\nclass BeamHypotheses(object):\n    def __init__(self, n_hyp, max_length, length_penalty, early_stopping):\n        \"\"\"\n        Initialize n-best list of hypotheses.\n        \"\"\"\n        self.max_length = max_length - 1  # ignoring bos_token\n        self.length_penalty = length_penalty\n        self.early_stopping = early_stopping\n        self.n_hyp = n_hyp\n        self.hyp = []\n        self.worst_score = 1e9\n\n    def __len__(self):\n        \"\"\"\n        Number of hypotheses in the list.\n        \"\"\"\n        return len(self.hyp)\n\n    def add(self, hyp, sum_logprobs):\n        \"\"\"\n        Add a new hypothesis to the list.\n        \"\"\"\n        score = sum_logprobs / len(hyp) ** self.length_penalty\n        if len(self) < self.n_hyp or score > self.worst_score:\n            self.hyp.append((score, hyp))\n            if len(self) > self.n_hyp:\n                sorted_scores = sorted([(s, idx) for idx, (s, _) in enumerate(self.hyp)])\n                del self.hyp[sorted_scores[0][1]]\n                self.worst_score = sorted_scores[1][0]\n            else:\n                self.worst_score = min(score, self.worst_score)\n\n    def is_done(self, best_sum_logprobs):\n        \"\"\"\n        If there are enough hypotheses and that none of the hypotheses being generated\n        can become better than the worst one in the heap, then we are done with this sentence.\n        \"\"\"\n        if len(self) < self.n_hyp:\n            return False\n        elif self.early_stopping:\n            return True\n        else:\n            return self.worst_score >= best_sum_logprobs / self.max_length ** self.length_penalty\n\n\nclass Conv1D(nn.Module):\n    def __init__(self, nf, nx):\n        \"\"\" Conv1D layer as defined by Radford et al. for OpenAI GPT (and also used in GPT-2)\n            Basically works like a Linear layer but the weights are transposed\n        \"\"\"\n        super(Conv1D, self).__init__()\n        self.nf = nf\n        w = torch.empty(nx, nf)\n        nn.init.normal_(w, std=0.02)\n        self.weight = nn.Parameter(w)\n        self.bias = nn.Parameter(torch.zeros(nf))\n\n    def forward(self, x):\n        size_out = x.size()[:-1] + (self.nf,)\n        x = torch.addmm(self.bias, x.view(-1, x.size(-1)), self.weight)\n        x = x.view(*size_out)\n        return x\n\n\nclass PoolerStartLogits(nn.Module):\n    \"\"\" Compute SQuAD start_logits from sequence hidden states. \"\"\"\n    def __init__(self, config):\n        super(PoolerStartLogits, self).__init__()\n        self.dense = nn.Linear(config.hidden_size, 1)\n\n    def forward(self, hidden_states, p_mask=None):\n        \"\"\" Args:\n            **p_mask**: (`optional`) ``torch.FloatTensor`` of shape `(batch_size, seq_len)`\n                invalid position mask such as query and special symbols (PAD, SEP, CLS)\n                1.0 means token should be masked.\n        \"\"\"\n        x = self.dense(hidden_states).squeeze(-1)\n\n        if p_mask is not None:\n            x = x * (1 - p_mask) - 1e30 * p_mask\n\n        return x\n\n\nclass PoolerEndLogits(nn.Module):\n    \"\"\" Compute SQuAD end_logits from sequence hidden states and start token hidden state.\n    \"\"\"\n    def __init__(self, config):\n        super(PoolerEndLogits, self).__init__()\n        self.dense_0 = nn.Linear(config.hidden_size * 2, config.hidden_size)\n        self.activation = nn.Tanh()\n        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n        self.dense_1 = nn.Linear(config.hidden_size, 1)\n\n    def forward(self, hidden_states, start_states=None, start_positions=None, p_mask=None):\n        \"\"\" Args:\n            One of ``start_states``, ``start_positions`` should be not None.\n            If both are set, ``start_positions`` overrides ``start_states``.\n\n            **start_states**: ``torch.LongTensor`` of shape identical to hidden_states\n                hidden states of the first tokens for the labeled span.\n            **start_positions**: ``torch.LongTensor`` of shape ``(batch_size,)``\n                position of the first token for the labeled span:\n            **p_mask**: (`optional`) ``torch.FloatTensor`` of shape ``(batch_size, seq_len)``\n                Mask of invalid position such as query and special symbols (PAD, SEP, CLS)\n                1.0 means token should be masked.\n        \"\"\"\n        assert start_states is not None or start_positions is not None, \"One of start_states, start_positions should be not None\"\n        if start_positions is not None:\n            slen, hsz = hidden_states.shape[-2:]\n            start_positions = start_positions[:, None, None].expand(-1, -1, hsz) # shape (bsz, 1, hsz)\n            start_states = hidden_states.gather(-2, start_positions) # shape (bsz, 1, hsz)\n            start_states = start_states.expand(-1, slen, -1) # shape (bsz, slen, hsz)\n\n        x = self.dense_0(torch.cat([hidden_states, start_states], dim=-1))\n        x = self.activation(x)\n        x = self.LayerNorm(x)\n        x = self.dense_1(x).squeeze(-1)\n\n        if p_mask is not None:\n            x = x * (1 - p_mask) - 1e30 * p_mask\n\n        return x\n\n\nclass PoolerAnswerClass(nn.Module):\n    \"\"\" Compute SQuAD 2.0 answer class from classification and start tokens hidden states. \"\"\"\n    def __init__(self, config):\n        super(PoolerAnswerClass, self).__init__()\n        self.dense_0 = nn.Linear(config.hidden_size * 2, config.hidden_size)\n        self.activation = nn.Tanh()\n        self.dense_1 = nn.Linear(config.hidden_size, 1, bias=False)\n\n    def forward(self, hidden_states, start_states=None, start_positions=None, cls_index=None):\n        \"\"\"\n        Args:\n            One of ``start_states``, ``start_positions`` should be not None.\n            If both are set, ``start_positions`` overrides ``start_states``.\n\n            **start_states**: ``torch.LongTensor`` of shape identical to ``hidden_states``.\n                hidden states of the first tokens for the labeled span.\n            **start_positions**: ``torch.LongTensor`` of shape ``(batch_size,)``\n                position of the first token for the labeled span.\n            **cls_index**: torch.LongTensor of shape ``(batch_size,)``\n                position of the CLS token. If None, take the last token.\n\n            note(Original repo):\n                no dependency on end_feature so that we can obtain one single `cls_logits`\n                for each sample\n        \"\"\"\n        hsz = hidden_states.shape[-1]\n        assert start_states is not None or start_positions is not None, \"One of start_states, start_positions should be not None\"\n        if start_positions is not None:\n            start_positions = start_positions[:, None, None].expand(-1, -1, hsz) # shape (bsz, 1, hsz)\n            start_states = hidden_states.gather(-2, start_positions).squeeze(-2) # shape (bsz, hsz)\n\n        if cls_index is not None:\n            cls_index = cls_index[:, None, None].expand(-1, -1, hsz) # shape (bsz, 1, hsz)\n            cls_token_state = hidden_states.gather(-2, cls_index).squeeze(-2) # shape (bsz, hsz)\n        else:\n            cls_token_state = hidden_states[:, -1, :] # shape (bsz, hsz)\n\n        x = self.dense_0(torch.cat([start_states, cls_token_state], dim=-1))\n        x = self.activation(x)\n        x = self.dense_1(x).squeeze(-1)\n\n        return x\n\n\nclass SQuADHead(nn.Module):\n    r\"\"\" A SQuAD head inspired by XLNet.\n\n    Parameters:\n        config (:class:`~pytorch_transformers.XLNetConfig`): Model configuration class with all the parameters of the model.\n\n    Inputs:\n        **hidden_states**: ``torch.FloatTensor`` of shape ``(batch_size, seq_len, hidden_size)``\n            hidden states of sequence tokens\n        **start_positions**: ``torch.LongTensor`` of shape ``(batch_size,)``\n            position of the first token for the labeled span.\n        **end_positions**: ``torch.LongTensor`` of shape ``(batch_size,)``\n            position of the last token for the labeled span.\n        **cls_index**: torch.LongTensor of shape ``(batch_size,)``\n            position of the CLS token. If None, take the last token.\n        **is_impossible**: ``torch.LongTensor`` of shape ``(batch_size,)``\n            Whether the question has a possible answer in the paragraph or not.\n        **p_mask**: (`optional`) ``torch.FloatTensor`` of shape ``(batch_size, seq_len)``\n            Mask of invalid position such as query and special symbols (PAD, SEP, CLS)\n            1.0 means token should be masked.\n\n    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n        **loss**: (`optional`, returned if both ``start_positions`` and ``end_positions`` are provided) ``torch.FloatTensor`` of shape ``(1,)``:\n            Classification loss as the sum of start token, end token (and is_impossible if provided) classification losses.\n        **start_top_log_probs**: (`optional`, returned if ``start_positions`` or ``end_positions`` is not provided)\n            ``torch.FloatTensor`` of shape ``(batch_size, config.start_n_top)``\n            Log probabilities for the top config.start_n_top start token possibilities (beam-search).\n        **start_top_index**: (`optional`, returned if ``start_positions`` or ``end_positions`` is not provided)\n            ``torch.LongTensor`` of shape ``(batch_size, config.start_n_top)``\n            Indices for the top config.start_n_top start token possibilities (beam-search).\n        **end_top_log_probs**: (`optional`, returned if ``start_positions`` or ``end_positions`` is not provided)\n            ``torch.FloatTensor`` of shape ``(batch_size, config.start_n_top * config.end_n_top)``\n            Log probabilities for the top ``config.start_n_top * config.end_n_top`` end token possibilities (beam-search).\n        **end_top_index**: (`optional`, returned if ``start_positions`` or ``end_positions`` is not provided)\n            ``torch.LongTensor`` of shape ``(batch_size, config.start_n_top * config.end_n_top)``\n            Indices for the top ``config.start_n_top * config.end_n_top`` end token possibilities (beam-search).\n        **cls_logits**: (`optional`, returned if ``start_positions`` or ``end_positions`` is not provided)\n            ``torch.FloatTensor`` of shape ``(batch_size,)``\n            Log probabilities for the ``is_impossible`` label of the answers.\n    \"\"\"\n    def __init__(self, config):\n        super(SQuADHead, self).__init__()\n        self.start_n_top = config.start_n_top\n        self.end_n_top = config.end_n_top\n\n        self.start_logits = PoolerStartLogits(config)\n        self.end_logits = PoolerEndLogits(config)\n        self.answer_class = PoolerAnswerClass(config)\n\n    def forward(self, hidden_states, start_positions=None, end_positions=None,\n                cls_index=None, is_impossible=None, p_mask=None):\n        outputs = ()\n\n        start_logits = self.start_logits(hidden_states, p_mask=p_mask)\n\n        if start_positions is not None and end_positions is not None:\n            # If we are on multi-GPU, let's remove the dimension added by batch splitting\n            for x in (start_positions, end_positions, cls_index, is_impossible):\n                if x is not None and x.dim() > 1:\n                    x.squeeze_(-1)\n\n            # during training, compute the end logits based on the ground truth of the start position\n            end_logits = self.end_logits(hidden_states, start_positions=start_positions, p_mask=p_mask)\n\n            loss_fct = CrossEntropyLoss()\n            start_loss = loss_fct(start_logits, start_positions)\n            end_loss = loss_fct(end_logits, end_positions)\n            total_loss = (start_loss + end_loss) / 2\n\n            if cls_index is not None and is_impossible is not None:\n                # Predict answerability from the representation of CLS and START\n                cls_logits = self.answer_class(hidden_states, start_positions=start_positions, cls_index=cls_index)\n                loss_fct_cls = nn.BCEWithLogitsLoss()\n                cls_loss = loss_fct_cls(cls_logits, is_impossible)\n\n                # note(zhiliny): by default multiply the loss by 0.5 so that the scale is comparable to start_loss and end_loss\n                total_loss += cls_loss * 0.5\n\n            outputs = (total_loss,) + outputs\n\n        else:\n            # during inference, compute the end logits based on beam search\n            bsz, slen, hsz = hidden_states.size()\n            start_log_probs = F.softmax(start_logits, dim=-1) # shape (bsz, slen)\n\n            start_top_log_probs, start_top_index = torch.topk(start_log_probs, self.start_n_top, dim=-1) # shape (bsz, start_n_top)\n            start_top_index_exp = start_top_index.unsqueeze(-1).expand(-1, -1, hsz) # shape (bsz, start_n_top, hsz)\n            start_states = torch.gather(hidden_states, -2, start_top_index_exp) # shape (bsz, start_n_top, hsz)\n            start_states = start_states.unsqueeze(1).expand(-1, slen, -1, -1) # shape (bsz, slen, start_n_top, hsz)\n\n            hidden_states_expanded = hidden_states.unsqueeze(2).expand_as(start_states) # shape (bsz, slen, start_n_top, hsz)\n            p_mask = p_mask.unsqueeze(-1) if p_mask is not None else None\n            end_logits = self.end_logits(hidden_states_expanded, start_states=start_states, p_mask=p_mask)\n            end_log_probs = F.softmax(end_logits, dim=1) # shape (bsz, slen, start_n_top)\n\n            end_top_log_probs, end_top_index = torch.topk(end_log_probs, self.end_n_top, dim=1) # shape (bsz, end_n_top, start_n_top)\n            end_top_log_probs = end_top_log_probs.view(-1, self.start_n_top * self.end_n_top)\n            end_top_index = end_top_index.view(-1, self.start_n_top * self.end_n_top)\n\n            start_states = torch.einsum(\"blh,bl->bh\", hidden_states, start_log_probs)\n            cls_logits = self.answer_class(hidden_states, start_states=start_states, cls_index=cls_index)\n\n            outputs = (start_top_log_probs, start_top_index, end_top_log_probs, end_top_index, cls_logits) + outputs\n\n        # return start_top_log_probs, start_top_index, end_top_log_probs, end_top_index, cls_logits\n        # or (if labels are provided) (total_loss,)\n        return outputs\n\n\nclass SequenceSummary(nn.Module):\n    r\"\"\" Compute a single vector summary of a sequence hidden states according to various possibilities:\n        Args of the config class:\n            summary_type:\n                - 'last' => [default] take the last token hidden state (like XLNet)\n                - 'first' => take the first token hidden state (like Bert)\n                - 'mean' => take the mean of all tokens hidden states\n                - 'token_ids' => supply a Tensor of classification token indices (GPT/GPT-2)\n                - 'attn' => Not implemented now, use multi-head attention\n            summary_use_proj: Add a projection after the vector extraction\n            summary_proj_to_labels: If True, the projection outputs to config.num_labels classes (otherwise to hidden_size). Default: False.\n            summary_activation: 'tanh' => add a tanh activation to the output, Other => no activation. Default\n            summary_first_dropout: Add a dropout before the projection and activation\n            summary_last_dropout: Add a dropout after the projection and activation\n    \"\"\"\n    def __init__(self, config):\n        super(SequenceSummary, self).__init__()\n\n        self.summary_type = config.summary_type if hasattr(config, 'summary_use_proj') else 'last'\n        if config.summary_type == 'attn':\n            # We should use a standard multi-head attention module with absolute positional embedding for that.\n            # Cf. https://github.com/zihangdai/xlnet/blob/master/modeling.py#L253-L276\n            # We can probably just use the multi-head attention module of PyTorch >=1.1.0\n            raise NotImplementedError\n\n        self.summary = Identity()\n        if hasattr(config, 'summary_use_proj') and config.summary_use_proj:\n            if hasattr(config, 'summary_proj_to_labels') and config.summary_proj_to_labels and config.num_labels > 0:\n                num_classes = config.num_labels\n            else:\n                num_classes = config.hidden_size\n            self.summary = nn.Linear(config.hidden_size, num_classes)\n\n        self.activation = Identity()\n        if hasattr(config, 'summary_activation') and config.summary_activation == 'tanh':\n            self.activation = nn.Tanh()\n\n        self.first_dropout = Identity()\n        if hasattr(config, 'summary_first_dropout') and config.summary_first_dropout > 0:\n            self.first_dropout = nn.Dropout(config.summary_first_dropout)\n\n        self.last_dropout = Identity()\n        if hasattr(config, 'summary_last_dropout') and config.summary_last_dropout > 0:\n            self.last_dropout = nn.Dropout(config.summary_last_dropout)\n\n    def forward(self, hidden_states, token_ids=None):\n        \"\"\" hidden_states: float Tensor in shape [bsz, seq_len, hidden_size], the hidden-states of the last layer.\n            token_ids: [optional] index of the classification token if summary_type == 'token_ids',\n                shape (bsz,) or more generally (bsz, ...) where ... are optional leading dimensions of hidden_states.\n                if summary_type == 'token_ids' and token_ids is None:\n                    we take the last token of the sequence as classification token\n        \"\"\"\n        if self.summary_type == 'last':\n            output = hidden_states[:, -1]\n        elif self.summary_type == 'first':\n            output = hidden_states[:, 0]\n        elif self.summary_type == 'mean':\n            output = hidden_states.mean(dim=1)\n        elif self.summary_type == 'token_ids':\n            if token_ids is None:\n                token_ids = torch.full_like(hidden_states[..., :1, :], hidden_states.shape[-2]-1, dtype=torch.long)\n            else:\n                token_ids = token_ids.unsqueeze(-1).unsqueeze(-1)\n                token_ids = token_ids.expand((-1,) * (token_ids.dim()-1) + (hidden_states.size(-1),))\n            # shape of token_ids: (bsz, XX, 1, hidden_size) where XX are optional leading dim of hidden_states\n            output = hidden_states.gather(-2, token_ids).squeeze(-2) # shape (bsz, XX, hidden_size)\n        elif self.summary_type == 'attn':\n            raise NotImplementedError\n\n        output = self.first_dropout(output)\n        output = self.summary(output)\n        output = self.activation(output)\n        output = self.last_dropout(output)\n\n        return output\n\n\ndef prune_linear_layer(layer, index, dim=0):\n    \"\"\" Prune a linear layer (a model parameters) to keep only entries in index.\n        Return the pruned layer as a new layer with requires_grad=True.\n        Used to remove heads.\n    \"\"\"\n    index = index.to(layer.weight.device)\n    W = layer.weight.index_select(dim, index).clone().detach()\n    if layer.bias is not None:\n        if dim == 1:\n            b = layer.bias.clone().detach()\n        else:\n            b = layer.bias[index].clone().detach()\n    new_size = list(layer.weight.size())\n    new_size[dim] = len(index)\n    new_layer = nn.Linear(new_size[1], new_size[0], bias=layer.bias is not None).to(layer.weight.device)\n    new_layer.weight.requires_grad = False\n    new_layer.weight.copy_(W.contiguous())\n    new_layer.weight.requires_grad = True\n    if layer.bias is not None:\n        new_layer.bias.requires_grad = False\n        new_layer.bias.copy_(b.contiguous())\n        new_layer.bias.requires_grad = True\n    return new_layer\n\n\ndef prune_conv1d_layer(layer, index, dim=1):\n    \"\"\" Prune a Conv1D layer (a model parameters) to keep only entries in index.\n        A Conv1D work as a Linear layer (see e.g. BERT) but the weights are transposed.\n        Return the pruned layer as a new layer with requires_grad=True.\n        Used to remove heads.\n    \"\"\"\n    index = index.to(layer.weight.device)\n    W = layer.weight.index_select(dim, index).clone().detach()\n    if dim == 0:\n        b = layer.bias.clone().detach()\n    else:\n        b = layer.bias[index].clone().detach()\n    new_size = list(layer.weight.size())\n    new_size[dim] = len(index)\n    new_layer = Conv1D(new_size[1], new_size[0]).to(layer.weight.device)\n    new_layer.weight.requires_grad = False\n    new_layer.weight.copy_(W.contiguous())\n    new_layer.weight.requires_grad = True\n    new_layer.bias.requires_grad = False\n    new_layer.bias.copy_(b.contiguous())\n    new_layer.bias.requires_grad = True\n    return new_layer\n\n\ndef prune_layer(layer, index, dim=None):\n    \"\"\" Prune a Conv1D or nn.Linear layer (a model parameters) to keep only entries in index.\n        Return the pruned layer as a new layer with requires_grad=True.\n        Used to remove heads.\n    \"\"\"\n    if isinstance(layer, nn.Linear):\n        return prune_linear_layer(layer, index, dim=0 if dim is None else dim)\n    elif isinstance(layer, Conv1D):\n        return prune_conv1d_layer(layer, index, dim=1 if dim is None else dim)\n    else:\n        raise ValueError(\"Can't prune layer of class {}\".format(layer.__class__))\n"}
{"type": "source_file", "path": "src/modeling/multitask_e2e_vid_swin_bert.py", "content": "import torch\nfrom fairscale.nn.misc import checkpoint_wrapper\nimport random\nfrom src.modeling.load_sensor_pred_head import get_sensor_pred_model\n\nclass MultitaskVideoTransformer(torch.nn.Module):\n    \"\"\" This is the multi-task module that performs Driving Caption Generation and Control Signal Prediction. \"\"\"\n    def __init__(self, args, config, swin, transformer_encoder):\n        \"\"\" Initializes the model.\n        Parameters:\n            args: basic args of ADAPT, mostly defined in `src/configs/VidSwinBert/BDDX_multi_default.json` and input args\n            config: config of transformer_encoder, mostly defined in `models/captioning/bert-base-uncased/config.json`\n            swin: torch module of the backbone to be used. See `src/modeling/load_swin.py`\n            transformer_encoder: torch module of the transformer architecture. See `src/modeling/load_bert.py`\n        \"\"\"\n        super(MultitaskVideoTransformer, self).__init__()\n        self.config = config\n        self.use_checkpoint = args.use_checkpoint and not args.freeze_backbone\n        if self.use_checkpoint:\n            self.swin = checkpoint_wrapper(swin, offload_to_cpu=True)\n        else:\n            self.swin = swin\n        self.trans_encoder = transformer_encoder\n        self.img_feature_dim = int(args.img_feature_dim)\n        self.use_grid_feat = args.grid_feat\n        self.latent_feat_size = self.swin.backbone.norm.normalized_shape[0]\n        self.fc = torch.nn.Linear(self.latent_feat_size, self.img_feature_dim)\n        self.compute_mask_on_the_fly = False # deprecated\n        self.mask_prob = args.mask_prob\n        self.mask_token_id = -1\n        self.max_img_seq_length = args.max_img_seq_length\n\n        # get Control Signal Prediction Head\n        self.sensor_pred_head = get_sensor_pred_model(args)\n\n        # if only_signal is True, it means we \n        # remove Driving Caption Generation head and only use Control Signal Prediction head \n        self.only_signal = getattr(args, 'only_signal', False)\n\n        # sparse attention mask defined in SwinBert\n        self.learn_mask_enabled = getattr(args, 'learn_mask_enabled', False)\n        self.sparse_mask_soft2hard = getattr(args, 'sparse_mask_soft2hard', False)\n        if self.learn_mask_enabled==True:\n            self.learn_vid_att = torch.nn.Embedding(args.max_img_seq_length*args.max_img_seq_length,1)\n            self.sigmoid = torch.nn.Sigmoid()\n\n    def forward(self, *args, **kwargs):\n        \"\"\" The forward process of ADAPT, \n        Parameters:\n            input_ids: word tokens of input sentences tokenized by tokenizer\n            attention_mask: multimodal attention mask in Vision-Language transformer\n            token_type_ids: typen tokens of input sentences, \n                            0 means it is a narration sentence and 1 means a reasoning sentence, same size with input_ids\n            img_feats: preprocessed frames of the video\n            masked_pos: [MASK] position when performing MLM, used to locate the masked words\n            masked_ids: groung truth of [MASK] when performing MLM\n            car_info: control signals of ego car in the video\n        \"\"\"\n\n        # grad cam can only input a tuple (args, kwargs)\n        if isinstance(args, tuple) and len(args) != 0:\n            kwargs = args[0]\n            args= ()\n\n        # video swin to extract video features\n        images = kwargs['img_feats']\n        B, S, C, H, W = images.shape  # batch, segment, chanel, hight, width\n        # (B x S x C x H x W) --> (B x C x S x H x W)\n        images = images.permute(0, 2, 1, 3, 4)\n        vid_feats = self.swin(images)\n\n        # tokenize video features to video tokens\n        if self.use_grid_feat==True:\n            vid_feats = vid_feats.permute(0, 2, 3, 4, 1)\n        vid_feats = vid_feats.view(B, -1, self.latent_feat_size)\n\n        # use an mlp to transform video token dimension\n        vid_feats = self.fc(vid_feats)\n\n        # prepare VL transformer inputs\n        kwargs['img_feats'] = vid_feats\n\n        # disable bert attention outputs to avoid some bugs\n        if self.trans_encoder.bert.encoder.output_attentions:\n            self.trans_encoder.bert.encoder.set_output_attentions(False)\n        \n        if self.only_signal:\n            # only Control Signal Prediction head \n            sensor_outputs = self.sensor_pred_head(*args, **kwargs)        \n            return sensor_outputs\n        \n        else:\n            # learn soft attention mask\n            if self.learn_mask_enabled:\n                kwargs['attention_mask'] = kwargs['attention_mask'].float()\n                vid_att_len = self.max_img_seq_length\n                learn_att = self.learn_vid_att.weight.reshape(vid_att_len,vid_att_len)\n                learn_att = self.sigmoid(learn_att)\n                diag_mask = torch.diag(torch.ones(vid_att_len)).cuda()\n                video_attention = (1. - diag_mask)*learn_att\n                learn_att = diag_mask + video_attention\n                if self.sparse_mask_soft2hard:\n                    learn_att = (learn_att>=0.5)*1.0\n                    learn_att = learn_att.cuda()\n                    learn_att.requires_grad = False\n                kwargs['attention_mask'][:, -vid_att_len::, -vid_att_len::] = learn_att\n\n            # Driving Caption Generation head, output is ()\n            outputs = self.trans_encoder(*args, **kwargs)\n\n            # Control Signal Prediction head, output is ()\n            sensor_outputs = self.sensor_pred_head(*args, **kwargs)\n\n            outputs = outputs + sensor_outputs\n\n            # sparse attention mask loss\n            if self.learn_mask_enabled:\n                loss_sparsity = self.get_loss_sparsity(video_attention)  \n                outputs = outputs + (loss_sparsity, )\n\n            return outputs\n    \n    def get_loss_sparsity(self, video_attention):\n        sparsity_loss = 0\n        sparsity_loss += (torch.mean(torch.abs(video_attention)))\n        return sparsity_loss\n\n    def reload_attn_mask(self, pretrain_attn_mask): \n        import numpy\n        pretrained_num_tokens = int(numpy.sqrt(pretrain_attn_mask.shape[0]))\n\n        pretrained_learn_att = pretrain_attn_mask.reshape(\n                                pretrained_num_tokens,pretrained_num_tokens)\n        scale_factor = 1\n        vid_att_len = self.max_img_seq_length\n        learn_att = self.learn_vid_att.weight.reshape(vid_att_len,vid_att_len)\n        with torch.no_grad():\n            for i in range(int(scale_factor)):\n                learn_att[pretrained_num_tokens*i:pretrained_num_tokens*(i+1), \n                            pretrained_num_tokens*i:pretrained_num_tokens*(i+1)] = pretrained_learn_att \n\n    def freeze_backbone(self, freeze=True):\n        for _, p in self.swin.named_parameters():\n            p.requires_grad =  not freeze\n"}
{"type": "source_file", "path": "src/datasets/data_utils/video_functional.py", "content": "import numbers\nimport torch\nimport cv2\nimport numpy as np\nimport PIL\nfrom PIL import Image\n\ndef _is_tensor_clip(clip):\n    return torch.is_tensor(clip) and clip.ndimension() == 4\n\n\ndef crop_clip(clip, min_h, min_w, h, w):\n    if isinstance(clip[0], np.ndarray):\n        cropped = [img[min_h:min_h + h, min_w:min_w + w, :] for img in clip]\n\n    elif isinstance(clip[0], PIL.Image.Image):\n        cropped = [\n            img.crop((min_w, min_h, min_w + w, min_h + h)) for img in clip\n        ]\n    else:\n        raise TypeError('Expected numpy.ndarray or PIL.Image' +\n                        'but got list of {0}'.format(type(clip[0])))\n    return cropped\n\n\ndef to_grayscale(img, num_output_channels=1):\n    \"\"\"Convert image to grayscale version of image.\n\n    Args:\n        img (PIL Image): Image to be converted to grayscale.\n\n    Returns:\n        PIL Image: Grayscale version of the image.\n            if num_output_channels = 1 : returned image is single channel\n\n            if num_output_channels = 3 : returned image is 3 channel with r = g = b\n    \"\"\"\n    if not isinstance(img,PIL.Image.Image):\n        raise TypeError('img should be PIL Image. Got {}'.format(type(img)))\n\n    if num_output_channels == 1:\n        img = img.convert('L')\n    elif num_output_channels == 3:\n        img = img.convert('L')\n        np_img = np.array(img, dtype=np.uint8)\n        np_img = np.dstack([np_img, np_img, np_img])\n        img = Image.fromarray(np_img, 'RGB')\n    else:\n        raise ValueError('num_output_channels should be either 1 or 3')\n\n    return img\n\ndef resize_clip(clip, size, interpolation='bilinear'):\n    if isinstance(clip[0], np.ndarray):\n        if isinstance(size, numbers.Number):\n            im_h, im_w, im_c = clip[0].shape\n            # Min spatial dim already matches minimal size\n            if (im_w <= im_h and im_w == size) or (im_h <= im_w\n                                                   and im_h == size):\n                return clip\n            new_h, new_w = get_resize_sizes(im_h, im_w, size)\n            size = (new_w, new_h)\n        else:\n            size = size[1], size[0]\n        if interpolation == 'bilinear':\n            np_inter = cv2.INTER_LINEAR\n        else:\n            np_inter = cv2.INTER_NEAREST\n        scaled = [\n            cv2.resize(img, size, interpolation=np_inter) for img in clip\n        ]\n    elif isinstance(clip[0], PIL.Image.Image):\n        if isinstance(size, numbers.Number):\n            im_w, im_h = clip[0].size\n            # Min spatial dim already matches minimal size\n            if (im_w <= im_h and im_w == size) or (im_h <= im_w\n                                                   and im_h == size):\n                return clip\n            new_h, new_w = get_resize_sizes(im_h, im_w, size)\n            size = (new_w, new_h)\n        else:\n            size = size[1], size[0]\n        if interpolation == 'bilinear':\n            pil_inter = PIL.Image.NEAREST\n        else:\n            pil_inter = PIL.Image.BILINEAR\n        scaled = [img.resize(size, pil_inter) for img in clip]\n    else:\n        raise TypeError('Expected numpy.ndarray or PIL.Image' +\n                        'but got list of {0}'.format(type(clip[0])))\n    return scaled\n\n\ndef get_resize_sizes(im_h, im_w, size):\n    if im_w < im_h:\n        ow = size\n        oh = int(size * im_h / im_w)\n    else:\n        oh = size\n        ow = int(size * im_w / im_h)\n    return oh, ow\n\n\ndef normalize(clip, mean, std, inplace=False):\n    if not _is_tensor_clip(clip):\n        raise TypeError('tensor is not a torch clip.')\n\n    if not inplace:\n        clip = clip.clone()\n\n    dtype = clip.dtype\n    mean = torch.as_tensor(mean, dtype=dtype, device=clip.device)\n    std = torch.as_tensor(std, dtype=dtype, device=clip.device)\n    clip.sub_(mean[:, None, None, None]).div_(std[:, None, None, None])\n\n    return clip\n"}
{"type": "source_file", "path": "src/datasets/sampler_utils.py", "content": "import time\nfrom src.utils.tsv_io import TSVDataset\nfrom src.utils.tsv_file import load_list_file\nfrom src.utils.tsv_io import get_tsv_lineidx, get_tsv_lineidx_8b\nfrom src.utils.qd_common import exclusive_open_to_read\nimport os.path as op\nimport logging\nimport math\nimport torch\nimport torch.distributed as dist\nfrom torch.utils.data.sampler import Sampler\nfrom src.utils.comm import get_local_rank, get_local_size, get_rank, get_world_size\n\n\nclass RankSplitSampler(Sampler):\n    def __init__(self, dataset, shuffle, random_seed):\n        self.dataset = dataset\n        self.shuffle = shuffle\n        self.random_seed = random_seed\n\n        self.world_size = get_world_size()\n        self.rank = get_rank()\n\n    def get_index(self):\n        source_list = self.dataset.get_composite_source_idx()\n        idx_split = list(enumerate(source_list))\n        idx_split = torch.tensor(idx_split)\n        if self.shuffle:\n            g = torch.Generator()\n            g.manual_seed(self.random_seed)\n            random_idx = torch.randperm(len(idx_split), generator=g)\n            idx_split = idx_split[random_idx]\n        sort_idx = torch.argsort(idx_split[:, 1])\n        idx_split = idx_split[sort_idx]\n        rank_size = (len(idx_split) + self.world_size - 1) // self.world_size\n        offset = rank_size * self.rank\n        offset_end = offset + rank_size\n        offset_end = min(offset_end, len(idx_split))\n        return idx_split[offset:offset_end, 0].tolist()\n\n    def __iter__(self):\n        self.curr_idx = 0\n        all_idx = self.get_index()\n        while True:\n            if self.curr_idx >= len(all_idx):\n                self.curr_idx -= len(all_idx)\n            yield all_idx[self.curr_idx]\n            self.curr_idx += 1\n\n    def __len__(self):\n        raise ValueError('should not be called')\n\ndef create_prepare_tsv_file_process(max_len=8):\n    import threading\n    import queue\n    prepare_queue = queue.Queue()\n    p = threading.Thread(\n        target=prepare_tsv_file_process, args=(prepare_queue, max_len),\n        daemon=True,\n    )\n    p.start()\n    return p, prepare_queue\n\ndef prepare_tsv_file_process(queue, max_len=8):\n    ftype = 'blobfuse'\n    logging.info('ftype = {}'.format(ftype))\n\n    prepared = []\n\n    while True:\n        start = time.time()\n        fnames = queue.get()\n        end = time.time()\n        if (end - start) > 5:\n            logging.info('waiting {} to get a new tsv to prepare'.format(\n                end - start))\n        curr_fs = []\n        for fname in fnames:\n            curr_fs.append(fname)\n            if fname.endswith('.tsv'):\n                lineidx = get_tsv_lineidx(fname)\n                from src.utils.tsv_io import QDFile\n                if QDFile.isfile(lineidx):\n                    curr_fs.append(lineidx)\n                lineidx8b = get_tsv_lineidx_8b(fname)\n                if QDFile.isfile(lineidx8b):\n                    curr_fs.append(lineidx8b)\n\n        def unprepare(info):\n            logging.info('unprepare {}'.format(info['fnames']))\n            if ftype == 'blobfuse':\n                for f in info['fps']:\n                    f.close()\n            logging.info('unprepared {}'.format(info['fnames']))\n\n        sames = [i for i, p in enumerate(prepared)\n                   if all(f in  p['fnames'] for f in curr_fs)]\n        if len(sames) > 0:\n            i = sames[0]\n            p = prepared[i]\n            del prepared[i]\n            prepared.append(p)\n            logging.info('no need to prepare {} as it prepared'.format(\n                curr_fs\n            ))\n            continue\n\n        while max_len > 0 and len(prepared) >= max_len:\n            unprepare(prepared.pop(0))\n\n        logging.info('prepare {}'.format(curr_fs))\n        start = time.time()\n        if ftype == 'blobfuse':\n            info = {\n                'fnames': curr_fs,\n                'fps': [exclusive_open_to_read(x) for x in curr_fs]\n            }\n        prepared.append(info)\n        logging.info('use {}s, prepared {}, all hold={}'.format(\n            time.time() - start,\n            curr_fs,\n            ', '.join([f for p in prepared for f in p['fnames']]),\n        ))\n\ndef ordered_unique(sequence):\n    seen = set()\n    return [x for x in sequence if not (x in seen or seen.add(x))]\n\nclass PrepareData(object):\n    def __init__(self, dataset, prepare_t_versions=[],\n                 fixed_samples_in_node=False,\n                 disable_prepare=None,\n                 ):\n        self.prepare_files = None\n        self.prepare_process = None\n        self.dataset = dataset\n        self.prepare_t_versions = prepare_t_versions\n        self.fixed_samples_in_node = fixed_samples_in_node\n        self.disable_prepare = disable_prepare\n\n    def get_composite_source_files(self):\n        root = self.dataset.root\n        assert self.dataset.is_composite\n        result = []\n        for t in ['visual_tsv', 'label_tsv', 'cap_tsv']:\n            tsv = getattr(self.dataset, t, None)\n            if tsv is not None:\n                result.append(\n                    [op.join(root, f) for f in tsv.file_list])\n        return result\n\n    def prepare(self, split):\n        if self.disable_prepare:\n            return\n        self.ensure_init_prepare()\n        q = self.prepare_queue\n        size = q.qsize()\n        if size > 100:\n            logging.info('prepare queue is too long {}'.format(size))\n        q.put([ps[split] for ps in self.prepare_files])\n\n    def ensure_init_prepare(self):\n        if self.prepare_files is None:\n            self.prepare_files = self.get_composite_source_files()\n        if self.prepare_process is None:\n            max_len = 8 if not self.fixed_samples_in_node else 0\n            p, prepare_queue = create_prepare_tsv_file_process(\n                max_len=max_len)\n            self.prepare_process = p\n            self.prepare_queue = prepare_queue\n\nclass SplitBySplitSampler(Sampler):\n    # only used in training mode.\n    # prefer to use PrepareData(), but this class has already been used for a\n    # while and is working great. New approaches can leverage PrepareData, but\n    # at this moment, it is ok not to re-factor it\n    def __init__(self, dataset, group_size=1, shuffle=True,\n                 fixed_samples_in_node=False,\n                 random_seed=9,\n                 prepare_t_versions=[],\n                 disable_prepare=None,\n                 ):\n        from src.utils.qd_common import print_frame_info\n        print_frame_info()\n        self.dataset = dataset\n        self.group_size = group_size\n        self.random_seed = random_seed\n        self.shuffle = shuffle\n\n        self.rank = get_rank()\n        self.local_rank = get_local_rank()\n        self.world_size = get_world_size()\n        self.local_size = get_local_size()\n\n        self.node_size = self.world_size // self.local_size\n        self.node_idx = self.rank // self.local_size\n\n        self.shuffle_group_process = None\n\n        self.prepare_process = None\n        self.prepare_queue = None\n        self.prepare_files = None\n        # currently, we only support to prepare one kind of files, but it could\n        # be extendeed to multiple files if we need\n        self.prepare_t_versions = prepare_t_versions\n        self.sub_process_create_shuffle = False\n        self._idx_split = None\n        self.iter_shuffle_group = None\n\n        self.curr_group_buffers = None\n        self.next_group_index = 0\n        self.cache_group_index_on_node = None\n\n        self.disable_prepare = disable_prepare\n        self.get_group_process = None\n        self.fixed_samples_in_node = fixed_samples_in_node\n\n    def get_composite_source_idx(self):\n        return self.dataset.get_composite_source_idx()\n\n    def get_composite_source_files(self):\n        data = self.dataset.dataset.data\n        split = self.dataset.dataset.split\n        dataset = TSVDataset(data)\n        result = []\n        for t, version in self.prepare_t_versions:\n            tsv = dataset.get_data(split, t, version)\n            if op.isfile(tsv):\n                result.append([tsv])\n            else:\n                x_tsv = dataset.get_data(split + 'X', t, version)\n                assert op.isfile(x_tsv)\n                result.append(load_list_file(x_tsv))\n        return result\n\n    def load_idx_split(self):\n        logging.info('loading source list')\n        source_list = self.get_composite_source_idx()\n        logging.info('loaded source list')\n        idx_split = list(enumerate(source_list))\n        idx_split = torch.tensor(idx_split)\n        return idx_split\n\n    @property\n    def idx_split(self):\n        if self._idx_split is None:\n            self._idx_split = self.load_idx_split()\n            self._idx_split.share_memory_()\n        return self._idx_split\n\n    def get_shufle_idx(self, n):\n        g = torch.Generator()\n        g.manual_seed(self.random_seed)\n        random_idx = torch.randperm(n, generator=g)\n        self.random_seed += 99\n        return random_idx\n\n    def get_group_index_on_node_random(self):\n        idx_split = self.idx_split\n\n        max_split = idx_split[:, 1].max() + 1\n        priority = self.get_shufle_idx(max_split)\n\n        random_idx = self.get_shufle_idx(len(idx_split))\n        idx_split = idx_split[random_idx]\n\n        idx_split = torch.cat([idx_split[idx_split[:, 1] == p] for p in priority])\n\n        num_idx_on_node = (len(idx_split) + self.node_size - 1) // self.node_size\n        offset = num_idx_on_node * self.node_idx\n        offset_end = offset + num_idx_on_node\n        offset_end = min(offset_end, len(idx_split))\n        idx_split = idx_split[offset:offset_end]\n\n        unique_split_index = ordered_unique(idx_split[:, 1].tolist())\n        logging.info(unique_split_index)\n        result = [\n            {\n                'idx_in_group': idx_split[idx_split[:, 1] == s][:, 0].tolist(),\n                'split_in_group': s,\n            }\n            for s in unique_split_index\n        ]\n        return result\n\n    def get_group_index_on_node(self):\n        if self.shuffle and not self.fixed_samples_in_node:\n            return self.get_group_index_on_node_random()\n        elif self.shuffle and self.fixed_samples_in_node:\n            if self.cache_group_index_on_node is None:\n                self.cache_group_index_on_node = self.get_group_index_on_node_random()\n            idx = self.get_shufle_idx(len(self.cache_group_index_on_node))\n            group_in_node = [self.cache_group_index_on_node[i] for i in idx]\n            for g in group_in_node:\n                idx = self.get_shufle_idx(len(g['idx_in_group']))\n                g['idx_in_group'] = [g['idx_in_group'][i] for i in idx]\n            return group_in_node\n        else:\n            if self.cache_group_index_on_node is None:\n                self.cache_group_index_on_node = self.get_group_index_on_node_random()\n            return self.cache_group_index_on_node\n\n    def get_next_group_index_on_node(self):\n        if self.curr_group_buffers is None:\n            self.curr_group_buffers = self.get_group_index_on_node()\n            self.next_group_index = 0\n        if self.next_group_index >= len(self.curr_group_buffers):\n            self.curr_group_buffers = self.get_group_index_on_node()\n            self.next_group_index = 0\n        g = self.curr_group_buffers[self.next_group_index]\n        self.next_group_index += 1\n        return g\n\n    def get_group_thread(self, q):\n        while True:\n            if q.qsize() < 8:\n                g = self.get_next_group_index_on_node()\n                q.put(g)\n            else:\n                time.sleep(1)\n\n    def __iter__(self):\n        use_thread_to_get_group = True\n        if not use_thread_to_get_group:\n            group_buffers = [self.get_next_group_index_on_node()\n                             for _ in range(4)]\n            if self.local_rank == 0:\n                for g in group_buffers:\n                    self.prepare(g['split_in_group'])\n            assert len(group_buffers) > 0\n            idx = self.local_rank\n            while True:\n                while idx >= len(group_buffers[0]['idx_in_group']):\n                    idx -= len(group_buffers[0]['idx_in_group'])\n                    group_buffers.pop(0)\n                    new_g = self.get_next_group_index_on_node()\n                    if self.local_rank == 0:\n                        self.prepare(new_g['split_in_group'])\n                    group_buffers.append(new_g)\n                r = group_buffers[0]['idx_in_group'][idx]\n                yield r\n                idx += self.local_size\n        else:\n            self.ensure_init_get_group_thread()\n            group_buffers = [self.get_group_queue.get()\n                             for _ in range(4)]\n            if self.local_rank == 0:\n                for g in group_buffers:\n                    self.prepare(g['split_in_group'])\n            assert len(group_buffers) > 0\n            idx = self.local_rank\n            while True:\n                while idx >= len(group_buffers[0]['idx_in_group']):\n                    idx -= len(group_buffers[0]['idx_in_group'])\n                    group_buffers.pop(0)\n                    start = time.time()\n                    new_g = self.get_group_queue.get()\n                    cost = time.time() - start\n                    logging.info('time to get group index on node: {}'.format(cost))\n                    if self.local_rank == 0:\n                        self.prepare(new_g['split_in_group'])\n                    group_buffers.append(new_g)\n                r = group_buffers[0]['idx_in_group'][idx]\n                yield r\n                idx += self.local_size\n\n    def ensure_init_get_group_thread(self):\n        if self.get_group_process is None:\n            import threading\n            import queue\n            q = queue.Queue()\n            t = threading.Thread(\n                target=self.get_group_thread, args=(q,),\n                daemon=True,\n            )\n            t.start()\n            self.get_group_process = t\n            self.get_group_queue = q\n\n    def ensure_init_prepare(self):\n        if self.prepare_files is None:\n            self.prepare_files = self.get_composite_source_files()\n        if self.prepare_process is None:\n            max_len = 8 if not self.fixed_samples_in_node else 0\n            p, prepare_queue = create_prepare_tsv_file_process(\n                max_len=max_len)\n            self.prepare_process = p\n            self.prepare_queue = prepare_queue\n\n    def prepare(self, split):\n        if self.disable_prepare:\n            return\n        self.ensure_init_prepare()\n        q = self.prepare_queue\n        size = q.qsize()\n        if size > 100:\n            logging.info('prepare queue is too long {}'.format(size))\n        q.put([ps[split] for ps in self.prepare_files])\n\n    def __len__(self):\n        raise ValueError('should not be called')\n\nclass AttachIterationNumberBatchSampler(object):\n    def __init__(self, batch_sampler, start_iter, num_iters,\n                 gradient_accumulate=1):\n        self.batch_sampler = batch_sampler\n        self.curr_iter = start_iter\n        self.max_iter = num_iters\n        self.gradient_accumulate = gradient_accumulate\n\n    def __getattr__(self, att):\n        return getattr(self.batch_sampler, att)\n\n    def __iter__(self):\n        #if hasattr(self.batch_sampler, 'skip') and self.curr_iter > 0:\n            #logging.info('we will skip {} batches'.format(self.curr_iter))\n            #self.batch_sampler.skip(self.curr_iter)\n        for idx_batch, batch in enumerate(self.batch_sampler):\n            batch = [{'iteration': self.curr_iter,\n                      'idx': i,\n                      'max_iter': self.max_iter} for i in batch]\n            yield batch\n            if (idx_batch + 1) % self.gradient_accumulate == 0:\n                self.curr_iter += 1\n\n    def __len__(self):\n        return len(self.batch_sampler)\n\nclass OrderedSplitSampler(Sampler):\n    def __init__(self, data_length):\n        curr_rank = get_rank()\n        world_size = get_world_size()\n        rank_size = (data_length + world_size - 1) // world_size\n        start = rank_size * curr_rank\n        end = start + rank_size\n        assert start >= 0 and start <= data_length\n        if curr_rank < world_size - 1:\n            assert end >= 0 and end <= data_length\n        end = min(end, data_length)\n        self.start = start\n        self.end = end\n\n    def __iter__(self):\n        return iter(range(self.start, self.end))\n\n    def __len__(self):\n        return self.end - self.start\n\nclass BatchSampler(Sampler):\n    r\"\"\"Wraps another sampler to yield a mini-batch of indices.\n\n    Args:\n        sampler (Sampler): Base sampler.\n        batch_size (int): Size of mini-batch.\n        drop_last (bool): If ``True``, the sampler will drop the last batch if\n            its size would be less than ``batch_size``\n\n    Example:\n        >>> list(BatchSampler(SequentialSampler(range(10)), batch_size=3, drop_last=False))\n        [[0, 1, 2], [3, 4, 5], [6, 7, 8], [9]]\n        >>> list(BatchSampler(SequentialSampler(range(10)), batch_size=3, drop_last=True))\n        [[0, 1, 2], [3, 4, 5], [6, 7, 8]]\n    \"\"\"\n\n    def __init__(self, sampler, batch_size, drop_last):\n        if not isinstance(sampler, Sampler):\n            raise ValueError(\"sampler should be an instance of \"\n                             \"torch.utils.data.Sampler, but got sampler={}\"\n                             .format(sampler))\n        if not isinstance(drop_last, bool):\n            raise ValueError(\"drop_last should be a boolean value, but got \"\n                             \"drop_last={}\".format(drop_last))\n        self.sampler = sampler\n        self.batch_size = batch_size\n        self.drop_last = drop_last\n\n    def __iter__(self):\n        batch = []\n        for idx in self.sampler:\n            batch.append(idx)\n            if len(batch) == self.batch_size:\n                yield batch\n                batch = []\n        if len(batch) > 0 and not self.drop_last:\n            yield batch\n\n    def __len__(self):\n        if self.drop_last:\n            return len(self.sampler) // self.batch_size\n        else:\n            return (len(self.sampler) + self.batch_size - 1) // self.batch_size\n\nclass IterationBasedBatchSampler(BatchSampler):\n    \"\"\"\n    Wraps a BatchSampler, resampling from it until\n    a specified number of iterations have been sampled\n    \"\"\"\n\n    def __init__(self, batch_sampler, num_iterations, start_iter=0,\n                 ):\n        self.batch_sampler = batch_sampler\n        self.num_iterations = num_iterations\n        self.start_iter = start_iter\n\n        if hasattr(batch_sampler, 'batch_size'):\n            self.batch_size = batch_sampler.batch_size\n\n        if hasattr(batch_sampler, 'drop_last'):\n            self.drop_last = batch_sampler.drop_last\n\n    def __iter__(self):\n        iteration = self.start_iter\n        while iteration <= self.num_iterations:\n            # if the underlying sampler has a set_epoch method, like\n            # DistributedSampler, used for making each process see\n            # a different split of the dataset, then set it\n            if hasattr(self.batch_sampler.sampler, \"set_epoch\"):\n                self.batch_sampler.sampler.set_epoch(iteration)\n            for batch in self.batch_sampler:\n                iteration += 1\n                if iteration > self.num_iterations:\n                    break\n                yield batch\n\n    def __len__(self):\n        return self.num_iterations\n\nclass DynamicBatchSampler(BatchSampler):\n    def __init__(self, sampler, get_batch_size, start_iter=0):\n        self.sampler = sampler\n        self.get_batch_size = get_batch_size\n        self.start_iter = start_iter\n\n    def __iter__(self):\n        batch = []\n        batch_size = None\n        curr_iter = self.start_iter\n        for idx in self.sampler:\n            batch.append(idx)\n            if batch_size is None:\n                batch_size = self.get_batch_size(curr_iter)\n            if len(batch) == batch_size:\n                yield batch\n                batch_size = None\n                curr_iter += 1\n                batch = []\n\nclass DistributedSampler(Sampler):\n    \"\"\"Sampler that restricts data loading to a subset of the dataset.\n    It is especially useful in conjunction with\n    :class:`torch.nn.parallel.DistributedDataParallel`. In such case, each\n    process can pass a DistributedSampler instance as a DataLoader sampler,\n    and load a subset of the original dataset that is exclusive to it.\n    .. note::\n        Dataset is assumed to be of constant size.\n    Arguments:\n        dataset: Dataset used for sampling.\n        num_replicas (optional): Number of processes participating in\n            distributed training.\n        rank (optional): Rank of the current process within num_replicas.\n    \"\"\"\n\n    def __init__(self, dataset, num_replicas=None, rank=None, shuffle=True,\n            length_divisible=1):\n        if num_replicas is None:\n            if not dist.is_available():\n                raise RuntimeError(\"Requires distributed package to be available\")\n            num_replicas = get_world_size()\n        if rank is None:\n            if not dist.is_available():\n                raise RuntimeError(\"Requires distributed package to be available\")\n            rank = get_rank()\n        self.dataset = dataset\n        self.num_replicas = num_replicas\n        self.rank = rank\n        self.epoch = 0\n        self.num_samples = int(math.ceil(len(self.dataset) * 1.0 / self.num_replicas))\n        if length_divisible > 1:\n            import logging\n            logging.info('before making divisible = {}'.format(self.num_samples))\n            self.num_samples = ((self.num_samples + length_divisible - 1) //\n                    length_divisible) * length_divisible\n            logging.info('adjust to = {}'.format(self.num_samples))\n        self.total_size = self.num_samples * self.num_replicas\n        self.shuffle = shuffle\n\n    def __iter__(self):\n        if self.shuffle:\n            # deterministically shuffle based on epoch\n            g = torch.Generator()\n            g.manual_seed(self.epoch)\n            indices = torch.randperm(len(self.dataset), generator=g).tolist()\n        else:\n            indices = torch.arange(len(self.dataset)).tolist()\n\n        # add extra samples to make it evenly divisible\n        assert (self.total_size - len(indices)) <= len(indices), 'not implemented'\n        indices += indices[: (self.total_size - len(indices))]\n        assert len(indices) == self.total_size\n\n        # subsample\n        offset = self.num_samples * self.rank\n        indices = indices[offset : offset + self.num_samples]\n        assert len(indices) == self.num_samples\n\n        return iter(indices)\n\n    def __len__(self):\n        return self.num_samples\n\n    def set_epoch(self, epoch):\n        self.epoch = epoch\n"}
{"type": "source_file", "path": "src/layers/bert/tokenization_utils.py", "content": "# coding=utf-8\n# Copyright 2018 The Open AI Team Authors and The HuggingFace Inc. team.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Tokenization classes for OpenAI GPT.\"\"\"\nfrom __future__ import (absolute_import, division, print_function,\n                        unicode_literals)\n\nimport os\nimport json\nimport six\nfrom io import open\n\nfrom .file_utils import cached_path\n\nimport logging\nfrom src.utils.comm import is_main_process\nlogger = logging.getLogger(__name__)  # pylint: disable=invalid-name\nif not is_main_process():\n    logger.disabled = True\n\nSPECIAL_TOKENS_MAP_FILE = 'special_tokens_map.json'\nADDED_TOKENS_FILE = 'added_tokens.json'\n\nclass PreTrainedTokenizer(object):\n    \"\"\" An abstract class to handle dowloading and loading pretrained tokenizers and adding tokens to the vocabulary.\n\n        Derived class can set up a few special tokens to be used in common scripts and internals:\n            bos_token, eos_token, EOP_TOKEN, EOD_TOKEN, unk_token, sep_token, pad_token, cls_token, mask_token\n            additional_special_tokens = []\n\n        We defined an added_tokens_encoder to add new tokens to the vocabulary without having to handle the\n            specific vocabulary augmentation methods of the various underlying dictionnary structures (BPE, sentencepiece...).\n    \"\"\"\n    vocab_files_names = {}\n    pretrained_vocab_files_map = {}\n    max_model_input_sizes = {}\n\n    SPECIAL_TOKENS_ATTRIBUTES = [\"bos_token\", \"eos_token\", \"unk_token\", \"sep_token\",\n                                 \"pad_token\", \"cls_token\", \"mask_token\",\n                                 \"additional_special_tokens\"]\n\n    @property\n    def bos_token(self):\n        if self._bos_token is None:\n            logger.error(\"Using bos_token, but it is not set yet.\")\n        return self._bos_token\n\n    @property\n    def eos_token(self):\n        if self._eos_token is None:\n            logger.error(\"Using eos_token, but it is not set yet.\")\n        return self._eos_token\n\n    @property\n    def unk_token(self):\n        if self._unk_token is None:\n            logger.error(\"Using unk_token, but it is not set yet.\")\n        return self._unk_token\n\n    @property\n    def sep_token(self):\n        if self._sep_token is None:\n            logger.error(\"Using sep_token, but it is not set yet.\")\n        return self._sep_token\n\n    @property\n    def pad_token(self):\n        if self._pad_token is None:\n            logger.error(\"Using pad_token, but it is not set yet.\")\n        return self._pad_token\n\n    @property\n    def cls_token(self):\n        if self._cls_token is None:\n            logger.error(\"Using cls_token, but it is not set yet.\")\n        return self._cls_token\n\n    @property\n    def mask_token(self):\n        if self._mask_token is None:\n            logger.error(\"Using mask_token, but it is not set yet.\")\n        return self._mask_token\n\n    @property\n    def additional_special_tokens(self):\n        if self._additional_special_tokens is None:\n            logger.error(\"Using additional_special_tokens, but it is not set yet.\")\n        return self._additional_special_tokens\n\n    @bos_token.setter\n    def bos_token(self, value):\n        self._bos_token = value\n\n    @eos_token.setter\n    def eos_token(self, value):\n        self._eos_token = value\n\n    @unk_token.setter\n    def unk_token(self, value):\n        self._unk_token = value\n\n    @sep_token.setter\n    def sep_token(self, value):\n        self._sep_token = value\n\n    @pad_token.setter\n    def pad_token(self, value):\n        self._pad_token = value\n\n    @cls_token.setter\n    def cls_token(self, value):\n        self._cls_token = value\n\n    @mask_token.setter\n    def mask_token(self, value):\n        self._mask_token = value\n\n    @additional_special_tokens.setter\n    def additional_special_tokens(self, value):\n        self._additional_special_tokens = value\n\n    def __init__(self, max_len=None, **kwargs):\n        self._bos_token = None\n        self._eos_token = None\n        self._unk_token = None\n        self._sep_token = None\n        self._pad_token = None\n        self._cls_token = None\n        self._mask_token = None\n        self._additional_special_tokens = []\n\n        self.max_len = max_len if max_len is not None else int(1e12)\n        self.added_tokens_encoder = {}\n        self.added_tokens_decoder = {}\n\n        for key, value in kwargs.items():\n            if key in self.SPECIAL_TOKENS_ATTRIBUTES:\n                setattr(self, key, value)\n\n\n    @classmethod\n    def from_pretrained(cls, *inputs, **kwargs):\n        return cls._from_pretrained(*inputs, **kwargs)\n\n\n    @classmethod\n    def _from_pretrained(cls, pretrained_model_name_or_path, cache_dir=None, *inputs, **kwargs):\n        \"\"\"\n        Instantiate a PreTrainedTokenizer from pre-trained vocabulary files.\n        Download and cache the vocabulary files if needed.\n        \"\"\"\n        s3_models = list(cls.max_model_input_sizes.keys())\n        vocab_files = {}\n        if pretrained_model_name_or_path in s3_models:\n            for file_id, map_list in cls.pretrained_vocab_files_map.items():\n                vocab_files[file_id] = map_list[pretrained_model_name_or_path]\n        else:\n            logger.info(\n                \"Model name '{}' not found in model shortcut name list ({}). \"\n                \"Assuming '{}' is a path or url to a directory containing tokenizer files.\".format(\n                    pretrained_model_name_or_path, ', '.join(s3_models),\n                    pretrained_model_name_or_path))\n            all_vocab_files_names = {'added_tokens_file': ADDED_TOKENS_FILE,\n                                     'special_tokens_map_file': SPECIAL_TOKENS_MAP_FILE}\n            all_vocab_files_names.update(cls.vocab_files_names)\n            for file_id, file_name in all_vocab_files_names.items():\n                if os.path.isdir(pretrained_model_name_or_path):\n                    full_file_name = os.path.join(pretrained_model_name_or_path, file_name)\n                else:\n                    full_file_name = pretrained_model_name_or_path\n                if not os.path.exists(full_file_name):\n                    logger.info(\"Didn't find file {}. We won't load it.\".format(full_file_name))\n                    full_file_name = None\n                vocab_files[file_id] = full_file_name\n            if all(full_file_name is None for full_file_name in vocab_files.values()):\n                logger.error(\n                    \"Model name '{}' was not found in model name list ({}). \"\n                    \"We assumed '{}' was a path or url but couldn't find tokenizer files\"\n                    \"at this path or url.\".format(\n                        pretrained_model_name_or_path, ', '.join(s3_models),\n                        pretrained_model_name_or_path, ))\n                return None\n\n        # Get files from url, cache, or disk depending on the case\n        try:\n            resolved_vocab_files = {}\n            for file_id, file_path in vocab_files.items():\n                if file_path is None:\n                    resolved_vocab_files[file_id] = None\n                else:\n                    resolved_vocab_files[file_id] = cached_path(file_path, cache_dir=cache_dir)\n        except EnvironmentError:\n            if pretrained_model_name_or_path in s3_models:\n                logger.error(\"Couldn't reach server to download vocabulary.\")\n            else:\n                logger.error(\n                    \"Model name '{}' was not found in model name list ({}). \"\n                    \"We assumed '{}' was a path or url but couldn't find files {} \"\n                    \"at this path or url.\".format(\n                        pretrained_model_name_or_path, ', '.join(s3_models),\n                        pretrained_model_name_or_path, str(vocab_files.keys())))\n            return None\n\n        for file_id, file_path in vocab_files.items():\n            if file_path == resolved_vocab_files[file_id]:\n                logger.info(\"loading file {}\".format(file_path))\n            else:\n                logger.info(\"loading file {} from cache at {}\".format(\n                    file_path, resolved_vocab_files[file_id]))\n\n        # Set max length if needed\n        if pretrained_model_name_or_path in cls.max_model_input_sizes:\n            # if we're using a pretrained model, ensure the tokenizer\n            # wont index sequences longer than the number of positional embeddings\n            max_len = cls.max_model_input_sizes[pretrained_model_name_or_path]\n            if max_len is not None and isinstance(max_len, (int, float)):\n                kwargs['max_len'] = min(kwargs.get('max_len', int(1e12)), max_len)\n\n        # Merge resolved_vocab_files arguments in kwargs.\n        added_tokens_file = resolved_vocab_files.pop('added_tokens_file', None)\n        special_tokens_map_file = resolved_vocab_files.pop('special_tokens_map_file', None)\n        for args_name, file_path in resolved_vocab_files.items():\n            if args_name not in kwargs:\n                kwargs[args_name] = file_path\n        if special_tokens_map_file is not None:\n            special_tokens_map = json.load(open(special_tokens_map_file, encoding=\"utf-8\"))\n            for key, value in special_tokens_map.items():\n                if key not in kwargs:\n                    kwargs[key] = value\n\n        # Instantiate tokenizer.\n        tokenizer = cls(*inputs, **kwargs)\n\n        # Add supplementary tokens.\n        if added_tokens_file is not None:\n            added_tok_encoder = json.load(open(added_tokens_file, encoding=\"utf-8\"))\n            added_tok_decoder = {v:k for k, v in added_tok_encoder.items()}\n            tokenizer.added_tokens_encoder.update(added_tok_encoder)\n            tokenizer.added_tokens_decoder.update(added_tok_decoder)\n\n        return tokenizer\n\n\n    def save_pretrained(self, save_directory):\n        \"\"\" Save the tokenizer vocabulary files (with added tokens) and the\n            special-tokens-to-class-attributes-mapping to a directory, so that it\n            can be re-loaded using the `from_pretrained(save_directory)` class method.\n        \"\"\"\n        if not os.path.isdir(save_directory):\n            logger.error(\"Saving directory ({}) should be a directory\".format(save_directory))\n            return\n\n        special_tokens_map_file = os.path.join(save_directory, SPECIAL_TOKENS_MAP_FILE)\n        added_tokens_file = os.path.join(save_directory, ADDED_TOKENS_FILE)\n\n        with open(special_tokens_map_file, 'w', encoding='utf-8') as f:\n            f.write(json.dumps(self.special_tokens_map, ensure_ascii=False))\n\n        with open(added_tokens_file, 'w', encoding='utf-8') as f:\n            if self.added_tokens_encoder:\n                out_str = json.dumps(self.added_tokens_decoder, ensure_ascii=False)\n            else:\n                out_str = u\"{}\"\n            f.write(out_str)\n\n        vocab_files = self.save_vocabulary(save_directory)\n\n        return vocab_files + (special_tokens_map_file, added_tokens_file)\n\n\n    def save_vocabulary(self, save_directory):\n        \"\"\" Save the tokenizer vocabulary to a directory. This method doesn't save added tokens\n            and special token mappings.\n            \n            Please use `save_pretrained()` to save the full Tokenizer state so that it can be\n            reloaded using the `from_pretrained(save_directory)` class method.\n        \"\"\"\n        raise NotImplementedError\n\n\n    def vocab_size(self):\n        raise NotImplementedError\n\n\n    def __len__(self):\n        return self.vocab_size + len(self.added_tokens_encoder)\n\n\n    def add_tokens(self, new_tokens):\n        \"\"\" Add a list of new tokens to the tokenizer class. If the new tokens are not in the\n            vocabulary, they are added to the added_tokens_encoder with indices starting from\n            the last index of the current vocabulary.\n\n            Returns:\n                Number of tokens added to the vocabulary which can be used to correspondingly\n                    increase the size of the associated model embedding matrices.\n        \"\"\"\n        if not new_tokens:\n            return 0\n\n        to_add_tokens = []\n        for token in new_tokens:\n            if self.convert_tokens_to_ids(token) == self.convert_tokens_to_ids(self.unk_token):\n                to_add_tokens.append(token)\n                logger.info(\"Adding %s to the vocabulary\", token)\n\n        added_tok_encoder = dict((tok, len(self) + i) for i, tok in enumerate(to_add_tokens))\n        added_tok_decoder = {v:k for k, v in added_tok_encoder.items()}\n        self.added_tokens_encoder.update(added_tok_encoder)\n        self.added_tokens_decoder.update(added_tok_decoder)\n\n        return len(to_add_tokens)\n\n\n    def add_special_tokens(self, special_tokens_dict):\n        \"\"\" Add a dictionnary of special tokens (eos, pad, cls...) to the encoder and link them\n            to class attributes. If the special tokens are not in the vocabulary, they are added\n            to it and indexed starting from the last index of the current vocabulary.\n\n            Returns:\n                Number of tokens added to the vocabulary which can be used to correspondingly\n                    increase the size of the associated model embedding matrices.\n        \"\"\"\n        if not special_tokens_dict:\n            return 0\n\n        added_special_tokens = self.add_tokens(special_tokens_dict.values())\n        for key, value in special_tokens_dict.items():\n            logger.info(\"Assigning %s to the %s key of the tokenizer\", value, key)\n            setattr(self, key, value)\n\n        return added_special_tokens\n\n\n    def tokenize(self, text, **kwargs):\n        \"\"\" Converts a string in a sequence of tokens (string), using the tokenizer.\n            Split in words for word-based vocabulary or sub-words for sub-word-based\n            vocabularies (BPE/SentencePieces/WordPieces).\n\n            Take care of added tokens.\n        \"\"\"\n        def split_on_tokens(tok_list, text):\n            if not text:\n                return []\n            if not tok_list:\n                return self._tokenize(text, **kwargs)\n            tok = tok_list[0]\n            split_text = text.split(tok)\n            return sum((split_on_tokens(tok_list[1:], sub_text.strip()) + [tok] \\\n                        for sub_text in split_text), [])[:-1]\n\n        added_tokens = list(self.added_tokens_encoder.keys()) + self.all_special_tokens\n        tokenized_text = split_on_tokens(added_tokens, text)\n        return tokenized_text\n\n\n    def tokenize_for_pos_tag(self, text, **kwargs):\n        \"\"\" Converts a string in a sequence of tokens (string), using the tokenizer.\n            Split in words for word-based vocabulary or sub-words for sub-word-based\n            vocabularies (BPE/SentencePieces/WordPieces).\n\n            Take care of added tokens.\n        \"\"\"\n        def split_on_tokens(tok_list, text):\n            if not text:\n                return ([], [], [])\n            if not tok_list:\n                # (split_tokens, basic_tokens, sub_to_token_idx_map)\n                return self._tokenize_for_pos_tag(text, **kwargs)\n            tok = tok_list[0]\n            split_text = text.split(tok)\n            split_tokens, basic_tokens, sub_to_token_idx_map  = [], [], []\n            for sub_text in split_text:\n                s_tok, b_tok, s2b_map = split_on_tokens(tok_list[1:], sub_text.strip()) \n                sub_to_token_idx_map.extend([i+len(basic_tokens) for i in s2b_map] + [len(s2b_map)+len(basic_tokens)])\n                split_tokens.extend(s_tok + [tok])\n                basic_tokens.extend(b_tok + [tok])\n            split_tokens = split_tokens[:-1]\n            basic_tokens = basic_tokens[:-1]\n            sub_to_token_idx_map = sub_to_token_idx_map[:-1]\n            return (split_tokens, basic_tokens, sub_to_token_idx_map)\n\n        added_tokens = list(self.added_tokens_encoder.keys()) + self.all_special_tokens\n        tokenized_text = split_on_tokens(added_tokens, text)\n        return tokenized_text\n\n    def _tokenize(self, text, **kwargs):\n        \"\"\" Converts a string in a sequence of tokens (string), using the tokenizer.\n            Split in words for word-based vocabulary or sub-words for sub-word-based\n            vocabularies (BPE/SentencePieces/WordPieces).\n\n            Don't take care of added tokens.\n        \"\"\"\n        raise NotImplementedError\n\n    def _tokenize_for_pos_tag(self, text, **kwargs):\n        \"\"\" Converts a string in a sequence of tokens (string), using the tokenizer.\n            Split in words for word-based vocabulary or sub-words for sub-word-based\n            vocabularies (BPE/SentencePieces/WordPieces).\n\n            Don't take care of added tokens.\n        \"\"\"\n        raise NotImplementedError\n\n    def convert_tokens_to_ids(self, tokens):\n        \"\"\" Converts a single token or a sequence of tokens (str/unicode) in a integer id\n            (resp.) a sequence of ids, using the vocabulary.\n        \"\"\"\n        if isinstance(tokens, str) or (six.PY2 and isinstance(tokens, unicode)):\n            return self._convert_token_to_id_with_added_voc(tokens)\n\n        ids = []\n        for token in tokens:\n            ids.append(self._convert_token_to_id_with_added_voc(token))\n        if len(ids) > self.max_len:\n            logger.warning(\"Token indices sequence length is longer than the specified maximum sequence length \"\n                           \"for this model ({} > {}). Running this sequence through the model will result in \"\n                           \"indexing errors\".format(len(ids), self.max_len))\n        return ids\n\n    def _convert_token_to_id_with_added_voc(self, token):\n        if token in self.added_tokens_encoder:\n            return self.added_tokens_encoder[token]\n        return self._convert_token_to_id(token)\n\n    def _convert_token_to_id(self, token):\n        raise NotImplementedError\n\n\n    def encode(self, text):\n        \"\"\" Converts a string in a sequence of ids (integer), using the tokenizer and vocabulary.\n            same as self.convert_tokens_to_ids(self.tokenize(text)).\n        \"\"\"\n        return self.convert_tokens_to_ids(self.tokenize(text))\n\n\n    def convert_ids_to_tokens(self, ids, skip_special_tokens=False):\n        \"\"\" Converts a single index or a sequence of indices (integers) in a token \"\n            (resp.) a sequence of tokens (str/unicode), using the vocabulary and added tokens.\n\n            Args:\n                skip_special_tokens: Don't decode special tokens (self.all_special_tokens). Default: False\n        \"\"\"\n        if isinstance(ids, int):\n            if ids in self.added_tokens_decoder:\n                return self.added_tokens_decoder[ids]\n            else:\n                return self._convert_id_to_token(ids)\n        tokens = []\n        for index in ids:\n            if index in self.all_special_ids and skip_special_tokens:\n                continue\n            if index in self.added_tokens_decoder:\n                tokens.append(self.added_tokens_decoder[index])\n            else:\n                tokens.append(self._convert_id_to_token(index))\n        return tokens\n\n    def _convert_id_to_token(self, index):\n        raise NotImplementedError\n\n    def convert_tokens_to_string(self, tokens):\n        \"\"\" Converts a sequence of tokens (string) in a single string.\n            The most simple way to do it is ' '.join(self.convert_ids_to_tokens(token_ids))\n            but we often want to remove sub-word tokenization artifacts at the same time.\n        \"\"\"\n        return ' '.join(self.convert_ids_to_tokens(tokens))\n\n    def decode(self, token_ids, skip_special_tokens=False, clean_up_tokenization_spaces=True):\n        \"\"\" Converts a sequence of ids (integer) in a string, using the tokenizer and vocabulary\n            with options to remove special tokens and clean up tokenization spaces.\n        \"\"\"\n        filtered_tokens = self.convert_ids_to_tokens(token_ids, skip_special_tokens=skip_special_tokens)\n        text = self.convert_tokens_to_string(filtered_tokens)\n        if clean_up_tokenization_spaces:\n            text = clean_up_tokenization(text)\n        return text\n\n    @property\n    def special_tokens_map(self):\n        \"\"\" A dictionary mapping special token class attribute (cls_token, unk_token...) to their\n            values ('<unk>', '<cls>'...)\n        \"\"\"\n        set_attr = {}\n        for attr in self.SPECIAL_TOKENS_ATTRIBUTES:\n            attr_value = getattr(self, \"_\" + attr)\n            if attr_value:\n                set_attr[attr] = attr_value\n        return set_attr\n\n    @property\n    def all_special_tokens(self):\n        \"\"\" List all the special tokens ('<unk>', '<cls>'...) mapped to class attributes\n            (cls_token, unk_token...).\n        \"\"\"\n        all_toks = []\n        set_attr = self.special_tokens_map\n        for attr_value in set_attr.values():\n            all_toks = all_toks + (attr_value if isinstance(attr_value, (list, tuple)) else [attr_value])\n        all_toks = list(set(all_toks))\n        return all_toks\n\n    @property\n    def all_special_ids(self):\n        \"\"\" List the vocabulary indices of the special tokens ('<unk>', '<cls>'...) mapped to\n            class attributes (cls_token, unk_token...).\n        \"\"\"\n        all_toks = self.all_special_tokens\n        all_ids = list(self.convert_tokens_to_ids(t) for t in all_toks)\n        return all_ids\n\n\n\ndef clean_up_tokenization(out_string):\n    out_string = out_string.replace(' .', '.').replace(' ?', '?').replace(' !', '!').replace(' ,', ','\n                    ).replace(\" ' \", \"'\").replace(\" n't\", \"n't\").replace(\" 'm\", \"'m\").replace(\" do not\", \" don't\"\n                    ).replace(\" 's\", \"'s\").replace(\" 've\", \"'ve\").replace(\" 're\", \"'re\")\n    return out_string\n"}
{"type": "source_file", "path": "src/datasets/data_utils/video_transforms.py", "content": "import numbers\nimport random\nimport numpy as np\nimport PIL\nimport skimage.transform\nimport torchvision\nimport math\nimport torch\n\nfrom . import video_functional as F\n\n\nclass Compose(object):\n    \"\"\"Composes several transforms\n\n    Args:\n    transforms (list of ``Transform`` objects): list of transforms\n    to compose\n    \"\"\"\n\n    def __init__(self, transforms):\n        self.transforms = transforms\n\n    def __call__(self, clip):\n        for t in self.transforms:\n            clip = t(clip)\n        return clip\n\n\nclass RandomHorizontalFlip(object):\n    \"\"\"Horizontally flip the list of given images randomly with a given probability.\n\n    Args:\n        p (float): probability of the image being flipped. Default value is 0.5\n    \"\"\"\n\n    def __init__(self, p=0.5):\n        self.p = p\n\n    def __call__(self, clip):\n        \"\"\"\n        Args:\n        img (PIL.Image or numpy.ndarray): List of images to be cropped\n        in format (h, w, c) in numpy.ndarray\n\n        Returns:\n        PIL.Image or numpy.ndarray: Randomly flipped clip\n        \"\"\"\n        if random.random() < self.p:\n            if isinstance(clip[0], np.ndarray):\n                return [np.fliplr(img) for img in clip]\n            elif isinstance(clip[0], PIL.Image.Image):\n                return [\n                    img.transpose(PIL.Image.FLIP_LEFT_RIGHT) for img in clip\n                ]\n            else:\n                raise TypeError('Expected numpy.ndarray or PIL.Image' +\n                                ' but got list of {0}'.format(type(clip[0])))\n        return clip\n\n    def __repr__(self):\n        return self.__class__.__name__ + '(p={})'.format(self.p)\n\n\nclass RandomVerticalFlip(object):\n    \"\"\"Vertically flip the list of given images randomly with a given probability.\n\n    Args:\n        p (float): probability of the image being flipped. Default value is 0.5\n    \"\"\"\n\n    def __init__(self, p=0.5):\n        self.p = p\n\n    def __call__(self, clip):\n        \"\"\"\n\n        Args:\n            img (PIL.Image or numpy.ndarray): List of images to be flipped\n            in format (h, w, c) in numpy.ndarray\n\n        Returns:\n            PIL.Image or numpy.ndarray: Randomly flipped clip\n        \"\"\"\n        if random.random() < self.p:\n            if isinstance(clip[0], np.ndarray):\n                return [np.flipud(img) for img in clip]\n            elif isinstance(clip[0], PIL.Image.Image):\n                return [\n                    img.transpose(PIL.Image.FLIP_TOP_BOTTOM) for img in clip\n                ]\n            else:\n                raise TypeError('Expected numpy.ndarray or PIL.Image' +\n                                ' but got list of {0}'.format(type(clip[0])))\n        return clip\n\n    def __repr__(self):\n        return self.__class__.__name__ + '(p={})'.format(self.p)\n\nclass RandomGrayscale(object):\n    \"\"\"Randomly convert image to grayscale with a probability of p (default 0.1).\n    The image can be a PIL Image or a Tensor, in which case it is expected\n    to have [..., 3, H, W] shape, where ... means an arbitrary number of leading\n    dimensions\n    Args:\n        p (float): probability that image should be converted to grayscale.\n    Returns:\n        PIL Image or Tensor: Grayscale version of the input image with probability p and unchanged\n        with probability (1-p).\n        - If input image is 1 channel: grayscale version is 1 channel\n        - If input image is 3 channel: grayscale version is 3 channel with r == g == b\n    \"\"\"\n    def __init__(self, p=0.1):\n        super().__init__()\n        self.p = p\n    def __call__(self,clip):\n        \"\"\"\n        Args:\n            list of imgs (PIL Image or Tensor): Image to be converted to grayscale.\n        Returns:\n            PIL Image or Tensor: Randomly grayscaled image.\n        \"\"\"\n        num_output_channels = 1 if clip[0].mode == 'L' else 3\n        if torch.rand(1)<self.p:\n            for i in range(len(clip)):\n                clip[i]=F.to_grayscale(clip[i],num_output_channels)\n        return clip\n\nclass RandomResize(object):\n    \"\"\"Resizes a list of (H x W x C) numpy.ndarray to the final size\n\n    The larger the original image is, the more times it takes to\n    interpolate\n\n    Args:\n    interpolation (str): Can be one of 'nearest', 'bilinear'\n    defaults to nearest\n    size (tuple): (widht, height)\n    \"\"\"\n\n    def __init__(self, ratio=(3. / 4., 4. / 3.), interpolation='nearest'):\n        self.ratio = ratio\n        self.interpolation = interpolation\n\n    def __call__(self, clip):\n        scaling_factor = random.uniform(self.ratio[0], self.ratio[1])\n\n        if isinstance(clip[0], np.ndarray):\n            im_h, im_w, im_c = clip[0].shape\n        elif isinstance(clip[0], PIL.Image.Image):\n            im_w, im_h = clip[0].size\n\n        new_w = int(im_w * scaling_factor)\n        new_h = int(im_h * scaling_factor)\n        new_size = (new_w, new_h)\n        resized = F.resize_clip(\n            clip, new_size, interpolation=self.interpolation)\n        return resized\n\n\nclass Resize(object):\n    \"\"\"Resizes a list of (H x W x C) numpy.ndarray to the final size\n\n    The larger the original image is, the more times it takes to\n    interpolate\n\n    Args:\n    interpolation (str): Can be one of 'nearest', 'bilinear'\n    defaults to nearest\n    size (tuple): (widht, height)\n    \"\"\"\n\n    def __init__(self, size, interpolation='nearest'):\n        self.size = size\n        self.interpolation = interpolation\n\n    def __call__(self, clip):\n        resized = F.resize_clip(\n            clip, self.size, interpolation=self.interpolation)\n        return resized\n\n\nclass RandomCrop(object):\n    \"\"\"Extract random crop at the same location for a list of images\n\n    Args:\n    size (sequence or int): Desired output size for the\n    crop in format (h, w)\n    \"\"\"\n\n    def __init__(self, size):\n        if isinstance(size, numbers.Number):\n            size = (size, size)\n\n        self.size = size\n\n    def __call__(self, clip):\n        \"\"\"\n        Args:\n        img (PIL.Image or numpy.ndarray): List of images to be cropped\n        in format (h, w, c) in numpy.ndarray\n\n        Returns:\n        PIL.Image or numpy.ndarray: Cropped list of images\n        \"\"\"\n        h, w = self.size\n        if isinstance(clip[0], np.ndarray):\n            im_h, im_w, im_c = clip[0].shape\n        elif isinstance(clip[0], PIL.Image.Image):\n            im_w, im_h = clip[0].size\n        else:\n            raise TypeError('Expected numpy.ndarray or PIL.Image' +\n                            'but got list of {0}'.format(type(clip[0])))\n        if w > im_w or h > im_h:\n            error_msg = (\n                'Initial image size should be larger then '\n                'cropped size but got cropped sizes : ({w}, {h}) while '\n                'initial image is ({im_w}, {im_h})'.format(\n                    im_w=im_w, im_h=im_h, w=w, h=h))\n            raise ValueError(error_msg)\n\n        x1 = random.randint(0, im_w - w)\n        y1 = random.randint(0, im_h - h)\n        cropped = F.crop_clip(clip, y1, x1, h, w)\n\n        return cropped\n\nclass RandomResizedCrop(object):\n    \"\"\"Crop the given list of PIL Images to random size and aspect ratio.\n\n    A crop of random size (default: of 0.08 to 1.0) of the original size and a random\n    aspect ratio (default: of 3/4 to 4/3) of the original aspect ratio is made. This crop\n    is finally resized to given size.\n    This is popularly used to train the Inception networks.\n\n    Args:\n        size: expected output size of each edge\n        scale: range of size of the origin size cropped\n        ratio: range of aspect ratio of the origin aspect ratio cropped\n        interpolation: Default: PIL.Image.BILINEAR\n    \"\"\"\n\n    def __init__(self, size, scale=(0.08, 1.0), ratio=(3. / 4., 4. / 3.), interpolation='bilinear'):\n        if isinstance(size, (tuple, list)):\n            self.size = size\n        else:\n            self.size = (size, size)\n        if (scale[0] > scale[1]) or (ratio[0] > ratio[1]):\n            warnings.warn(\"range should be of kind (min, max)\")\n\n        self.interpolation = interpolation\n        self.scale = scale\n        self.ratio = ratio\n\n    @staticmethod\n    def get_params(clip, scale, ratio):\n        \"\"\"Get parameters for ``crop`` for a random sized crop.\n\n        Args:\n            img (list of PIL Image): Image to be cropped.\n            scale (tuple): range of size of the origin size cropped\n            ratio (tuple): range of aspect ratio of the origin aspect ratio cropped\n\n        Returns:\n            tuple: params (i, j, h, w) to be passed to ``crop`` for a random\n                sized crop.\n        \"\"\"\n        if isinstance(clip[0], np.ndarray):\n            height, width, im_c = clip[0].shape\n        elif isinstance(clip[0], PIL.Image.Image):\n            width, height = clip[0].size\n        area = height * width\n\n        for _ in range(10):\n            target_area = random.uniform(*scale) * area\n            log_ratio = (math.log(ratio[0]), math.log(ratio[1]))\n            aspect_ratio = math.exp(random.uniform(*log_ratio))\n\n            w = int(round(math.sqrt(target_area * aspect_ratio)))\n            h = int(round(math.sqrt(target_area / aspect_ratio)))\n\n            if 0 < w <= width and 0 < h <= height:\n                i = random.randint(0, height - h)\n                j = random.randint(0, width - w)\n                return i, j, h, w\n\n        # Fallback to central crop\n        in_ratio = float(width) / float(height)\n        if (in_ratio < min(ratio)):\n            w = width\n            h = int(round(w / min(ratio)))\n        elif (in_ratio > max(ratio)):\n            h = height\n            w = int(round(h * max(ratio)))\n        else:  # whole image\n            w = width\n            h = height\n        i = (height - h) // 2\n        j = (width - w) // 2\n        return i, j, h, w\n\n    def __call__(self, clip):\n        \"\"\"\n        Args:\n            clip: list of img (PIL Image): Image to be cropped and resized.\n\n        Returns:\n            list of PIL Image: Randomly cropped and resized image.\n        \"\"\"\n        i, j, h, w = self.get_params(clip, self.scale, self.ratio)\n        imgs=F.crop_clip(clip,i,j,h,w)\n        return F.resize_clip(clip,self.size,self.interpolation)\n        # return F.resized_crop(img, i, j, h, w, self.size, self.interpolation)\n\n    def __repr__(self):\n        interpolate_str = _pil_interpolation_to_str[self.interpolation]\n        format_string = self.__class__.__name__ + '(size={0}'.format(self.size)\n        format_string += ', scale={0}'.format(tuple(round(s, 4) for s in self.scale))\n        format_string += ', ratio={0}'.format(tuple(round(r, 4) for r in self.ratio))\n        format_string += ', interpolation={0})'.format(interpolate_str)\n        return format_string\n    \nclass RandomRotation(object):\n    \"\"\"Rotate entire clip randomly by a random angle within\n    given bounds\n\n    Args:\n    degrees (sequence or int): Range of degrees to select from\n    If degrees is a number instead of sequence like (min, max),\n    the range of degrees, will be (-degrees, +degrees).\n\n    \"\"\"\n\n    def __init__(self, degrees):\n        if isinstance(degrees, numbers.Number):\n            if degrees < 0:\n                raise ValueError('If degrees is a single number,'\n                                 'must be positive')\n            degrees = (-degrees, degrees)\n        else:\n            if len(degrees) != 2:\n                raise ValueError('If degrees is a sequence,'\n                                 'it must be of len 2.')\n\n        self.degrees = degrees\n\n    def __call__(self, clip):\n        \"\"\"\n        Args:\n        img (PIL.Image or numpy.ndarray): List of images to be cropped\n        in format (h, w, c) in numpy.ndarray\n\n        Returns:\n        PIL.Image or numpy.ndarray: Cropped list of images\n        \"\"\"\n        angle = random.uniform(self.degrees[0], self.degrees[1])\n        if isinstance(clip[0], np.ndarray):\n            rotated = [skimage.transform.rotate(img, angle) for img in clip]\n        elif isinstance(clip[0], PIL.Image.Image):\n            rotated = [img.rotate(angle) for img in clip]\n        else:\n            raise TypeError('Expected numpy.ndarray or PIL.Image' +\n                            'but got list of {0}'.format(type(clip[0])))\n\n        return rotated\n\n\nclass CenterCrop(object):\n    \"\"\"Extract center crop at the same location for a list of images\n\n    Args:\n    size (sequence or int): Desired output size for the\n    crop in format (h, w)\n    \"\"\"\n\n    def __init__(self, size):\n        if isinstance(size, numbers.Number):\n            size = (size, size)\n\n        self.size = size\n\n    def __call__(self, clip):\n        \"\"\"\n        Args:\n        img (PIL.Image or numpy.ndarray): List of images to be cropped\n        in format (h, w, c) in numpy.ndarray\n\n        Returns:\n        PIL.Image or numpy.ndarray: Cropped list of images\n        \"\"\"\n        h, w = self.size\n        if isinstance(clip[0], np.ndarray):\n            im_h, im_w, im_c = clip[0].shape\n        elif isinstance(clip[0], PIL.Image.Image):\n            im_w, im_h = clip[0].size\n        else:\n            raise TypeError('Expected numpy.ndarray or PIL.Image' +\n                            'but got list of {0}'.format(type(clip[0])))\n        if w > im_w or h > im_h:\n            error_msg = (\n                'Initial image size should be larger then '\n                'cropped size but got cropped sizes : ({w}, {h}) while '\n                'initial image is ({im_w}, {im_h})'.format(\n                    im_w=im_w, im_h=im_h, w=w, h=h))\n            raise ValueError(error_msg)\n\n        x1 = int(round((im_w - w) / 2.))\n        y1 = int(round((im_h - h) / 2.))\n        cropped = F.crop_clip(clip, y1, x1, h, w)\n\n        return cropped\n\n\nclass ColorJitter(object):\n    \"\"\"Randomly change the brightness, contrast and saturation and hue of the clip\n\n    Args:\n    brightness (float): How much to jitter brightness. brightness_factor\n    is chosen uniformly from [max(0, 1 - brightness), 1 + brightness].\n    contrast (float): How much to jitter contrast. contrast_factor\n    is chosen uniformly from [max(0, 1 - contrast), 1 + contrast].\n    saturation (float): How much to jitter saturation. saturation_factor\n    is chosen uniformly from [max(0, 1 - saturation), 1 + saturation].\n    hue(float): How much to jitter hue. hue_factor is chosen uniformly from\n    [-hue, hue]. Should be >=0 and <= 0.5.\n    \"\"\"\n\n    def __init__(self, brightness=0, contrast=0, saturation=0, hue=0):\n        self.brightness = brightness\n        self.contrast = contrast\n        self.saturation = saturation\n        self.hue = hue\n\n    def get_params(self, brightness, contrast, saturation, hue):\n        if brightness > 0:\n            brightness_factor = random.uniform(\n                max(0, 1 - brightness), 1 + brightness)\n        else:\n            brightness_factor = None\n\n        if contrast > 0:\n            contrast_factor = random.uniform(\n                max(0, 1 - contrast), 1 + contrast)\n        else:\n            contrast_factor = None\n\n        if saturation > 0:\n            saturation_factor = random.uniform(\n                max(0, 1 - saturation), 1 + saturation)\n        else:\n            saturation_factor = None\n\n        if hue > 0:\n            hue_factor = random.uniform(-hue, hue)\n        else:\n            hue_factor = None\n        return brightness_factor, contrast_factor, saturation_factor, hue_factor\n\n    def __call__(self, clip):\n        \"\"\"\n        Args:\n        clip (list): list of PIL.Image\n\n        Returns:\n        list PIL.Image : list of transformed PIL.Image\n        \"\"\"\n        if isinstance(clip[0], np.ndarray):\n            raise TypeError(\n                'Color jitter not yet implemented for numpy arrays')\n        elif isinstance(clip[0], PIL.Image.Image):\n            brightness, contrast, saturation, hue = self.get_params(\n                self.brightness, self.contrast, self.saturation, self.hue)\n\n            # Create img transform function sequence\n            img_transforms = []\n            if brightness is not None:\n                img_transforms.append(lambda img: torchvision.transforms.functional.adjust_brightness(img, brightness))\n            if saturation is not None:\n                img_transforms.append(lambda img: torchvision.transforms.functional.adjust_saturation(img, saturation))\n            if hue is not None:\n                img_transforms.append(lambda img: torchvision.transforms.functional.adjust_hue(img, hue))\n            if contrast is not None:\n                img_transforms.append(lambda img: torchvision.transforms.functional.adjust_contrast(img, contrast))\n            random.shuffle(img_transforms)\n\n            # Apply to all images\n            jittered_clip = []\n            for img in clip:\n                for func in img_transforms:\n                    jittered_img = func(img)\n                jittered_clip.append(jittered_img)\n\n        else:\n            raise TypeError('Expected numpy.ndarray or PIL.Image' +\n                            'but got list of {0}'.format(type(clip[0])))\n        return jittered_clip\n\nclass Normalize(object):\n    \"\"\"Normalize a clip with mean and standard deviation.\n    Given mean: ``(M1,...,Mn)`` and std: ``(S1,..,Sn)`` for ``n`` channels, this transform\n    will normalize each channel of the input ``torch.*Tensor`` i.e.\n    ``input[channel] = (input[channel] - mean[channel]) / std[channel]``\n\n    .. note::\n        This transform acts out of place, i.e., it does not mutates the input tensor.\n\n    Args:\n        mean (sequence): Sequence of means for each channel.\n        std (sequence): Sequence of standard deviations for each channel.\n    \"\"\"\n\n    def __init__(self, mean, std):\n        self.mean = mean\n        self.std = std\n\n    def __call__(self, clip):\n        \"\"\"\n        Args:\n            clip (Tensor): Tensor clip of size (T, C, H, W) to be normalized.\n\n        Returns:\n            Tensor: Normalized Tensor clip.\n        \"\"\"\n        return F.normalize(clip, self.mean, self.std)\n\n\n    def __repr__(self):\n        return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)\n"}
{"type": "source_file", "path": "src/modeling/video_swin/default_runtime.py", "content": "checkpoint_config = dict(interval=1)\nlog_config = dict(\n    interval=20,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook'),\n    ])\n# runtime settings\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]"}
{"type": "source_file", "path": "src/modeling/video_captioning_e2e_vid_swin_bert.py", "content": "import torch\nfrom fairscale.nn.misc import checkpoint_wrapper\nimport random\n\n\nclass VideoTransformer(torch.nn.Module):\n    \"\"\" This is the one head module that performs Dirving Caption Generation. \"\"\"\n    def __init__(self, args, config, swin, transformer_encoder):\n        super(VideoTransformer, self).__init__()\n        \"\"\" Initializes the model.\n        Parameters:\n            args: basic args of ADAPT, mostly defined in `src/configs/VidSwinBert/BDDX_multi_default.json` and input args\n            config: config of transformer_encoder, mostly defined in `models/captioning/bert-base-uncased/config.json`\n            swin: torch module of the backbone to be used. See `src/modeling/load_swin.py`\n            transformer_encoder: torch module of the transformer architecture. See `src/modeling/load_bert.py`\n        \"\"\"\n        self.config = config\n        self.use_checkpoint = args.use_checkpoint and not args.freeze_backbone\n        if self.use_checkpoint:\n            self.swin = checkpoint_wrapper(swin, offload_to_cpu=True)\n        else:\n            self.swin = swin\n        self.trans_encoder = transformer_encoder\n        self.img_feature_dim = int(args.img_feature_dim)\n        self.use_grid_feat = args.grid_feat\n        self.latent_feat_size = self.swin.backbone.norm.normalized_shape[0]\n        self.fc = torch.nn.Linear(self.latent_feat_size, self.img_feature_dim)\n        self.compute_mask_on_the_fly = False # deprecated\n        self.mask_prob = args.mask_prob\n        self.mask_token_id = -1\n        self.max_img_seq_length = args.max_img_seq_length\n\n        self.max_num_frames = getattr(args, 'max_num_frames', 2)\n        self.expand_car_info = torch.nn.Linear(self.max_num_frames, self.img_feature_dim)\n\n        # add sensor information\n        self.use_car_sensor = getattr(args, 'use_car_sensor', False)\n\n        # learn soft attention mask\n        self.learn_mask_enabled = getattr(args, 'learn_mask_enabled', False)\n        self.sparse_mask_soft2hard = getattr(args, 'sparse_mask_soft2hard', False)\n        \n        if self.learn_mask_enabled==True:\n            self.learn_vid_att = torch.nn.Embedding(args.max_img_seq_length*args.max_img_seq_length,1)\n            self.sigmoid = torch.nn.Sigmoid()\n\n    def forward(self, *args, **kwargs):\n        \"\"\" The forward process of ADAPT, \n        Parameters:\n            input_ids: word tokens of input sentences tokenized by tokenizer\n            attention_mask: multimodal attention mask in Vision-Language transformer\n            token_type_ids: typen tokens of input sentences, \n                            0 means it is a narration sentence and 1 means a reasoning sentence, same size with input_ids\n            img_feats: preprocessed frames of the video\n            masked_pos: [MASK] position when performing MLM, used to locate the masked words\n            masked_ids: groung truth of [MASK] when performing MLM\n        \"\"\"\n        # grad cam can only input a tuple (args, kwargs)\n        if isinstance(args, tuple) and len(args) != 0:\n            kwargs = args[0]\n            args= ()\n\n        images = kwargs['img_feats']\n        B, S, C, H, W = images.shape  # batch, segment, chanel, hight, width\n        # (B x S x C x H x W) --> (B x C x S x H x W)\n        images = images.permute(0, 2, 1, 3, 4)\n        vid_feats = self.swin(images)\n\n        # tokenize video features to video tokens\n        if self.use_grid_feat==True:\n            vid_feats = vid_feats.permute(0, 2, 3, 4, 1)\n        vid_feats = vid_feats.view(B, -1, self.latent_feat_size)\n\n        # use an mlp to transform video token dimension\n        vid_feats = self.fc(vid_feats)\n\n        # use video features to predict car tensor\n        if self.use_car_sensor:\n            car_infos = kwargs['car_info']\n            car_infos = self.expand_car_info(car_infos)\n            vid_feats = torch.cat((vid_feats, car_infos), dim=1)\n\n        # prepare VL transformer inputs\n        kwargs['img_feats'] = vid_feats\n\n        # disable bert attention outputs to avoid some bugs\n        if self.trans_encoder.bert.encoder.output_attentions:\n            self.trans_encoder.bert.encoder.set_output_attentions(False)\n\n        # learn soft attention mask\n        if self.learn_mask_enabled:\n            kwargs['attention_mask'] = kwargs['attention_mask'].float()\n            vid_att_len = self.max_img_seq_length\n            learn_att = self.learn_vid_att.weight.reshape(vid_att_len,vid_att_len)\n            learn_att = self.sigmoid(learn_att)\n            diag_mask = torch.diag(torch.ones(vid_att_len)).cuda()\n            video_attention = (1. - diag_mask)*learn_att\n            learn_att = diag_mask + video_attention\n            if self.sparse_mask_soft2hard:\n                learn_att = (learn_att>=0.5)*1.0\n                learn_att = learn_att.cuda()\n                learn_att.requires_grad = False\n            kwargs['attention_mask'][:, -vid_att_len::, -vid_att_len::] = learn_att\n\n        # Driving Caption Generation head\n        outputs = self.trans_encoder(*args, **kwargs)\n\n        # sparse attention mask loss\n        if self.learn_mask_enabled:\n            loss_sparsity = self.get_loss_sparsity(video_attention)  \n            outputs = outputs + (loss_sparsity, )\n\n        return outputs\n    \n    def get_loss_sparsity(self, video_attention):\n        sparsity_loss = 0\n        sparsity_loss += (torch.mean(torch.abs(video_attention)))\n        return sparsity_loss\n\n    def reload_attn_mask(self, pretrain_attn_mask): \n        import numpy\n        pretrained_num_tokens = int(numpy.sqrt(pretrain_attn_mask.shape[0]))\n\n        pretrained_learn_att = pretrain_attn_mask.reshape(\n                                pretrained_num_tokens,pretrained_num_tokens)\n        scale_factor = 1\n        vid_att_len = self.max_img_seq_length\n        learn_att = self.learn_vid_att.weight.reshape(vid_att_len,vid_att_len)\n        with torch.no_grad():\n            for i in range(int(scale_factor)):\n                learn_att[pretrained_num_tokens*i:pretrained_num_tokens*(i+1), \n                            pretrained_num_tokens*i:pretrained_num_tokens*(i+1)] = pretrained_learn_att \n\n    def freeze_backbone(self, freeze=True):\n        for _, p in self.swin.named_parameters():\n            p.requires_grad =  not freeze\n\n "}
{"type": "source_file", "path": "src/modeling/video_swin/config.py", "content": "# Copyright (c) Open-MMLab. All rights reserved.\nimport ast\nimport copy\nimport os\nimport os.path as osp\nimport platform\nimport shutil\nimport sys\nimport tempfile\nimport uuid\nimport warnings\nfrom argparse import Action, ArgumentParser\nfrom collections import abc\nfrom importlib import import_module\n\nfrom addict import Dict\nfrom yapf.yapflib.yapf_api import FormatCode\n\n# from .misc import import_modules_from_strings\n# from .path import check_file_exist\n\nif platform.system() == 'Windows':\n    import regex as re\nelse:\n    import re\n\nBASE_KEY = '_base_'\nDELETE_KEY = '_delete_'\nRESERVED_KEYS = ['filename', 'text', 'pretty_text']\n\n\n\ndef check_file_exist(filename, msg_tmpl='file \"{}\" does not exist'):\n    if not osp.isfile(filename):\n        raise FileNotFoundError(msg_tmpl.format(filename))\n\ndef import_modules_from_strings(imports, allow_failed_imports=False):\n    \"\"\"Import modules from the given list of strings.\n    Args:\n        imports (list | str | None): The given module names to be imported.\n        allow_failed_imports (bool): If True, the failed imports will return\n            None. Otherwise, an ImportError is raise. Default: False.\n    Returns:\n        list[module] | module | None: The imported modules.\n    Examples:\n        >>> osp, sys = import_modules_from_strings(\n        ...     ['os.path', 'sys'])\n        >>> import os.path as osp_\n        >>> import sys as sys_\n        >>> assert osp == osp_\n        >>> assert sys == sys_\n    \"\"\"\n    if not imports:\n        return\n    single_import = False\n    if isinstance(imports, str):\n        single_import = True\n        imports = [imports]\n    if not isinstance(imports, list):\n        raise TypeError(\n            f'custom_imports must be a list but got type {type(imports)}')\n    imported = []\n    for imp in imports:\n        if not isinstance(imp, str):\n            raise TypeError(\n                f'{imp} is of type {type(imp)} and cannot be imported.')\n        try:\n            imported_tmp = import_module(imp)\n        except ImportError:\n            if allow_failed_imports:\n                warnings.warn(f'{imp} failed to import and is ignored.',\n                              UserWarning)\n                imported_tmp = None\n            else:\n                raise ImportError\n        imported.append(imported_tmp)\n    if single_import:\n        imported = imported[0]\n    return imported\n\n\nclass ConfigDict(Dict):\n\n    def __missing__(self, name):\n        raise KeyError(name)\n\n    def __getattr__(self, name):\n        try:\n            value = super(ConfigDict, self).__getattr__(name)\n        except KeyError:\n            ex = AttributeError(f\"'{self.__class__.__name__}' object has no \"\n                                f\"attribute '{name}'\")\n        except Exception as e:\n            ex = e\n        else:\n            return value\n        raise ex\n\n\ndef add_args(parser, cfg, prefix=''):\n    for k, v in cfg.items():\n        if isinstance(v, str):\n            parser.add_argument('--' + prefix + k)\n        elif isinstance(v, int):\n            parser.add_argument('--' + prefix + k, type=int)\n        elif isinstance(v, float):\n            parser.add_argument('--' + prefix + k, type=float)\n        elif isinstance(v, bool):\n            parser.add_argument('--' + prefix + k, action='store_true')\n        elif isinstance(v, dict):\n            add_args(parser, v, prefix + k + '.')\n        elif isinstance(v, abc.Iterable):\n            parser.add_argument('--' + prefix + k, type=type(v[0]), nargs='+')\n        else:\n            print(f'cannot parse key {prefix + k} of type {type(v)}')\n    return parser\n\n\nclass Config:\n    \"\"\"A facility for config and config files.\n\n    It supports common file formats as configs: python/json/yaml. The interface\n    is the same as a dict object and also allows access config values as\n    attributes.\n\n    Example:\n        >>> cfg = Config(dict(a=1, b=dict(b1=[0, 1])))\n        >>> cfg.a\n        1\n        >>> cfg.b\n        {'b1': [0, 1]}\n        >>> cfg.b.b1\n        [0, 1]\n        >>> cfg = Config.fromfile('tests/data/config/a.py')\n        >>> cfg.filename\n        \"/home/kchen/projects/mmcv/tests/data/config/a.py\"\n        >>> cfg.item4\n        'test'\n        >>> cfg\n        \"Config [path: /home/kchen/projects/mmcv/tests/data/config/a.py]: \"\n        \"{'item1': [1, 2], 'item2': {'a': 0}, 'item3': True, 'item4': 'test'}\"\n    \"\"\"\n\n    @staticmethod\n    def _validate_py_syntax(filename):\n        with open(filename, 'r', encoding='utf-8') as f:\n            # Setting encoding explicitly to resolve coding issue on windows\n            content = f.read()\n        try:\n            ast.parse(content)\n        except SyntaxError as e:\n            raise SyntaxError('There are syntax errors in config '\n                              f'file {filename}: {e}')\n\n    @staticmethod\n    def _substitute_predefined_vars(filename, temp_config_name):\n        file_dirname = osp.dirname(filename)\n        file_basename = osp.basename(filename)\n        file_basename_no_extension = osp.splitext(file_basename)[0]\n        file_extname = osp.splitext(filename)[1]\n        support_templates = dict(\n            fileDirname=file_dirname,\n            fileBasename=file_basename,\n            fileBasenameNoExtension=file_basename_no_extension,\n            fileExtname=file_extname)\n        with open(filename, 'r', encoding='utf-8') as f:\n            # Setting encoding explicitly to resolve coding issue on windows\n            config_file = f.read()\n        for key, value in support_templates.items():\n            regexp = r'\\{\\{\\s*' + str(key) + r'\\s*\\}\\}'\n            value = value.replace('\\\\', '/')\n            config_file = re.sub(regexp, value, config_file)\n        with open(temp_config_name, 'w') as tmp_config_file:\n            tmp_config_file.write(config_file)\n\n    @staticmethod\n    def _pre_substitute_base_vars(filename, temp_config_name):\n        \"\"\"Substitute base variable placehoders to string, so that parsing\n        would work.\"\"\"\n        with open(filename, 'r', encoding='utf-8') as f:\n            # Setting encoding explicitly to resolve coding issue on windows\n            config_file = f.read()\n        base_var_dict = {}\n        regexp = r'\\{\\{\\s*' + BASE_KEY + r'\\.([\\w\\.]+)\\s*\\}\\}'\n        base_vars = set(re.findall(regexp, config_file))\n        for base_var in base_vars:\n            randstr = f'_{base_var}_{uuid.uuid4().hex.lower()[:6]}'\n            base_var_dict[randstr] = base_var\n            regexp = r'\\{\\{\\s*' + BASE_KEY + r'\\.' + base_var + r'\\s*\\}\\}'\n            config_file = re.sub(regexp, f'\"{randstr}\"', config_file)\n        with open(temp_config_name, 'w') as tmp_config_file:\n            tmp_config_file.write(config_file)\n        return base_var_dict\n\n    @staticmethod\n    def _substitute_base_vars(cfg, base_var_dict, base_cfg):\n        \"\"\"Substitute variable strings to their actual values.\"\"\"\n        cfg = copy.deepcopy(cfg)\n\n        if isinstance(cfg, dict):\n            for k, v in cfg.items():\n                if isinstance(v, str) and v in base_var_dict:\n                    new_v = base_cfg\n                    for new_k in base_var_dict[v].split('.'):\n                        new_v = new_v[new_k]\n                    cfg[k] = new_v\n                elif isinstance(v, (list, tuple, dict)):\n                    cfg[k] = Config._substitute_base_vars(\n                        v, base_var_dict, base_cfg)\n        elif isinstance(cfg, tuple):\n            cfg = tuple(\n                Config._substitute_base_vars(c, base_var_dict, base_cfg)\n                for c in cfg)\n        elif isinstance(cfg, list):\n            cfg = [\n                Config._substitute_base_vars(c, base_var_dict, base_cfg)\n                for c in cfg\n            ]\n        elif isinstance(cfg, str) and cfg in base_var_dict:\n            new_v = base_cfg\n            for new_k in base_var_dict[cfg].split('.'):\n                new_v = new_v[new_k]\n            cfg = new_v\n\n        return cfg\n\n    @staticmethod\n    def _file2dict(filename, use_predefined_variables=True):\n        filename = osp.abspath(osp.expanduser(filename))\n        check_file_exist(filename)\n        fileExtname = osp.splitext(filename)[1]\n        if fileExtname not in ['.py', '.json', '.yaml', '.yml']:\n            raise IOError('Only py/yml/yaml/json type are supported now!')\n\n        with tempfile.TemporaryDirectory() as temp_config_dir:\n            temp_config_file = tempfile.NamedTemporaryFile(\n                dir=temp_config_dir, suffix=fileExtname)\n            if platform.system() == 'Windows':\n                temp_config_file.close()\n            temp_config_name = osp.basename(temp_config_file.name)\n            # Substitute predefined variables\n            if use_predefined_variables:\n                Config._substitute_predefined_vars(filename,\n                                                   temp_config_file.name)\n            else:\n                shutil.copyfile(filename, temp_config_file.name)\n            # Substitute base variables from placeholders to strings\n            base_var_dict = Config._pre_substitute_base_vars(\n                temp_config_file.name, temp_config_file.name)\n\n            if filename.endswith('.py'):\n                temp_module_name = osp.splitext(temp_config_name)[0]\n                sys.path.insert(0, temp_config_dir)\n                Config._validate_py_syntax(filename)\n                mod = import_module(temp_module_name)\n                sys.path.pop(0)\n                cfg_dict = {\n                    name: value\n                    for name, value in mod.__dict__.items()\n                    if not name.startswith('__')\n                }\n                # delete imported module\n                del sys.modules[temp_module_name]\n            elif filename.endswith(('.yml', '.yaml', '.json')):\n                import mmcv\n                cfg_dict = mmcv.load(temp_config_file.name)\n            # close temp file\n            temp_config_file.close()\n\n        cfg_text = filename + '\\n'\n        with open(filename, 'r', encoding='utf-8') as f:\n            # Setting encoding explicitly to resolve coding issue on windows\n            cfg_text += f.read()\n\n        if BASE_KEY in cfg_dict:\n            cfg_dir = osp.dirname(filename)\n            base_filename = cfg_dict.pop(BASE_KEY)\n            base_filename = base_filename if isinstance(\n                base_filename, list) else [base_filename]\n\n            cfg_dict_list = list()\n            cfg_text_list = list()\n            for f in base_filename:\n                _cfg_dict, _cfg_text = Config._file2dict(osp.join(cfg_dir, f))\n                cfg_dict_list.append(_cfg_dict)\n                cfg_text_list.append(_cfg_text)\n\n            base_cfg_dict = dict()\n            for c in cfg_dict_list:\n                if len(base_cfg_dict.keys() & c.keys()) > 0:\n                    raise KeyError('Duplicate key is not allowed among bases')\n                base_cfg_dict.update(c)\n\n            # Subtitute base variables from strings to their actual values\n            cfg_dict = Config._substitute_base_vars(cfg_dict, base_var_dict,\n                                                    base_cfg_dict)\n\n            base_cfg_dict = Config._merge_a_into_b(cfg_dict, base_cfg_dict)\n            cfg_dict = base_cfg_dict\n\n            # merge cfg_text\n            cfg_text_list.append(cfg_text)\n            cfg_text = '\\n'.join(cfg_text_list)\n\n        return cfg_dict, cfg_text\n\n    @staticmethod\n    def _merge_a_into_b(a, b, allow_list_keys=False):\n        \"\"\"merge dict ``a`` into dict ``b`` (non-inplace).\n\n        Values in ``a`` will overwrite ``b``. ``b`` is copied first to avoid\n        in-place modifications.\n\n        Args:\n            a (dict): The source dict to be merged into ``b``.\n            b (dict): The origin dict to be fetch keys from ``a``.\n            allow_list_keys (bool): If True, int string keys (e.g. '0', '1')\n              are allowed in source ``a`` and will replace the element of the\n              corresponding index in b if b is a list. Default: False.\n\n        Returns:\n            dict: The modified dict of ``b`` using ``a``.\n\n        Examples:\n            # Normally merge a into b.\n            >>> Config._merge_a_into_b(\n            ...     dict(obj=dict(a=2)), dict(obj=dict(a=1)))\n            {'obj': {'a': 2}}\n\n            # Delete b first and merge a into b.\n            >>> Config._merge_a_into_b(\n            ...     dict(obj=dict(_delete_=True, a=2)), dict(obj=dict(a=1)))\n            {'obj': {'a': 2}}\n\n            # b is a list\n            >>> Config._merge_a_into_b(\n            ...     {'0': dict(a=2)}, [dict(a=1), dict(b=2)], True)\n            [{'a': 2}, {'b': 2}]\n        \"\"\"\n        b = b.copy()\n        for k, v in a.items():\n            if allow_list_keys and k.isdigit() and isinstance(b, list):\n                k = int(k)\n                if len(b) <= k:\n                    raise KeyError(f'Index {k} exceeds the length of list {b}')\n                b[k] = Config._merge_a_into_b(v, b[k], allow_list_keys)\n            elif isinstance(v,\n                            dict) and k in b and not v.pop(DELETE_KEY, False):\n                allowed_types = (dict, list) if allow_list_keys else dict\n                if not isinstance(b[k], allowed_types):\n                    raise TypeError(\n                        f'{k}={v} in child config cannot inherit from base '\n                        f'because {k} is a dict in the child config but is of '\n                        f'type {type(b[k])} in base config. You may set '\n                        f'`{DELETE_KEY}=True` to ignore the base config')\n                b[k] = Config._merge_a_into_b(v, b[k], allow_list_keys)\n            else:\n                b[k] = v\n        return b\n\n    @staticmethod\n    def fromfile(filename,\n                 use_predefined_variables=True,\n                 import_custom_modules=True):\n        cfg_dict, cfg_text = Config._file2dict(filename,\n                                               use_predefined_variables)\n        if import_custom_modules and cfg_dict.get('custom_imports', None):\n            import_modules_from_strings(**cfg_dict['custom_imports'])\n        return Config(cfg_dict, cfg_text=cfg_text, filename=filename)\n\n    @staticmethod\n    def fromstring(cfg_str, file_format):\n        \"\"\"Generate config from config str.\n\n        Args:\n            cfg_str (str): Config str.\n            file_format (str): Config file format corresponding to the\n               config str. Only py/yml/yaml/json type are supported now!\n\n        Returns:\n            obj:`Config`: Config obj.\n        \"\"\"\n        if file_format not in ['.py', '.json', '.yaml', '.yml']:\n            raise IOError('Only py/yml/yaml/json type are supported now!')\n        if file_format != '.py' and 'dict(' in cfg_str:\n            # check if users specify a wrong suffix for python\n            warnings.warn(\n                'Please check \"file_format\", the file format may be .py')\n        with tempfile.NamedTemporaryFile(\n                'w', suffix=file_format, delete=False) as temp_file:\n            temp_file.write(cfg_str)\n            # on windows, previous implementation cause error\n            # see PR 1077 for details\n        cfg = Config.fromfile(temp_file.name)\n        os.remove(temp_file.name)\n        return cfg\n\n    @staticmethod\n    def auto_argparser(description=None):\n        \"\"\"Generate argparser from config file automatically (experimental)\"\"\"\n        partial_parser = ArgumentParser(description=description)\n        partial_parser.add_argument('config', help='config file path')\n        cfg_file = partial_parser.parse_known_args()[0].config\n        cfg = Config.fromfile(cfg_file)\n        parser = ArgumentParser(description=description)\n        parser.add_argument('config', help='config file path')\n        add_args(parser, cfg)\n        return parser, cfg\n\n    def __init__(self, cfg_dict=None, cfg_text=None, filename=None):\n        if cfg_dict is None:\n            cfg_dict = dict()\n        elif not isinstance(cfg_dict, dict):\n            raise TypeError('cfg_dict must be a dict, but '\n                            f'got {type(cfg_dict)}')\n        for key in cfg_dict:\n            if key in RESERVED_KEYS:\n                raise KeyError(f'{key} is reserved for config file')\n\n        super(Config, self).__setattr__('_cfg_dict', ConfigDict(cfg_dict))\n        super(Config, self).__setattr__('_filename', filename)\n        if cfg_text:\n            text = cfg_text\n        elif filename:\n            with open(filename, 'r') as f:\n                text = f.read()\n        else:\n            text = ''\n        super(Config, self).__setattr__('_text', text)\n\n    @property\n    def filename(self):\n        return self._filename\n\n    @property\n    def text(self):\n        return self._text\n\n    @property\n    def pretty_text(self):\n\n        indent = 4\n\n        def _indent(s_, num_spaces):\n            s = s_.split('\\n')\n            if len(s) == 1:\n                return s_\n            first = s.pop(0)\n            s = [(num_spaces * ' ') + line for line in s]\n            s = '\\n'.join(s)\n            s = first + '\\n' + s\n            return s\n\n        def _format_basic_types(k, v, use_mapping=False):\n            if isinstance(v, str):\n                v_str = f\"'{v}'\"\n            else:\n                v_str = str(v)\n\n            if use_mapping:\n                k_str = f\"'{k}'\" if isinstance(k, str) else str(k)\n                attr_str = f'{k_str}: {v_str}'\n            else:\n                attr_str = f'{str(k)}={v_str}'\n            attr_str = _indent(attr_str, indent)\n\n            return attr_str\n\n        def _format_list(k, v, use_mapping=False):\n            # check if all items in the list are dict\n            if all(isinstance(_, dict) for _ in v):\n                v_str = '[\\n'\n                v_str += '\\n'.join(\n                    f'dict({_indent(_format_dict(v_), indent)}),'\n                    for v_ in v).rstrip(',')\n                if use_mapping:\n                    k_str = f\"'{k}'\" if isinstance(k, str) else str(k)\n                    attr_str = f'{k_str}: {v_str}'\n                else:\n                    attr_str = f'{str(k)}={v_str}'\n                attr_str = _indent(attr_str, indent) + ']'\n            else:\n                attr_str = _format_basic_types(k, v, use_mapping)\n            return attr_str\n\n        def _contain_invalid_identifier(dict_str):\n            contain_invalid_identifier = False\n            for key_name in dict_str:\n                contain_invalid_identifier |= \\\n                    (not str(key_name).isidentifier())\n            return contain_invalid_identifier\n\n        def _format_dict(input_dict, outest_level=False):\n            r = ''\n            s = []\n\n            use_mapping = _contain_invalid_identifier(input_dict)\n            if use_mapping:\n                r += '{'\n            for idx, (k, v) in enumerate(input_dict.items()):\n                is_last = idx >= len(input_dict) - 1\n                end = '' if outest_level or is_last else ','\n                if isinstance(v, dict):\n                    v_str = '\\n' + _format_dict(v)\n                    if use_mapping:\n                        k_str = f\"'{k}'\" if isinstance(k, str) else str(k)\n                        attr_str = f'{k_str}: dict({v_str}'\n                    else:\n                        attr_str = f'{str(k)}=dict({v_str}'\n                    attr_str = _indent(attr_str, indent) + ')' + end\n                elif isinstance(v, list):\n                    attr_str = _format_list(k, v, use_mapping) + end\n                else:\n                    attr_str = _format_basic_types(k, v, use_mapping) + end\n\n                s.append(attr_str)\n            r += '\\n'.join(s)\n            if use_mapping:\n                r += '}'\n            return r\n\n        cfg_dict = self._cfg_dict.to_dict()\n        text = _format_dict(cfg_dict, outest_level=True)\n        # copied from setup.cfg\n        yapf_style = dict(\n            based_on_style='pep8',\n            blank_line_before_nested_class_or_def=True,\n            split_before_expression_after_opening_paren=True)\n        text, _ = FormatCode(text, style_config=yapf_style, verify=True)\n\n        return text\n\n    def __repr__(self):\n        return f'Config (path: {self.filename}): {self._cfg_dict.__repr__()}'\n\n    def __len__(self):\n        return len(self._cfg_dict)\n\n    def __getattr__(self, name):\n        return getattr(self._cfg_dict, name)\n\n    def __getitem__(self, name):\n        return self._cfg_dict.__getitem__(name)\n\n    def __setattr__(self, name, value):\n        if isinstance(value, dict):\n            value = ConfigDict(value)\n        self._cfg_dict.__setattr__(name, value)\n\n    def __setitem__(self, name, value):\n        if isinstance(value, dict):\n            value = ConfigDict(value)\n        self._cfg_dict.__setitem__(name, value)\n\n    def __iter__(self):\n        return iter(self._cfg_dict)\n\n    def __getstate__(self):\n        return (self._cfg_dict, self._filename, self._text)\n\n    def __setstate__(self, state):\n        _cfg_dict, _filename, _text = state\n        super(Config, self).__setattr__('_cfg_dict', _cfg_dict)\n        super(Config, self).__setattr__('_filename', _filename)\n        super(Config, self).__setattr__('_text', _text)\n\n    def dump(self, file=None):\n        cfg_dict = super(Config, self).__getattribute__('_cfg_dict').to_dict()\n        if self.filename.endswith('.py'):\n            if file is None:\n                return self.pretty_text\n            else:\n                with open(file, 'w') as f:\n                    f.write(self.pretty_text)\n        else:\n            import mmcv\n            if file is None:\n                file_format = self.filename.split('.')[-1]\n                return mmcv.dump(cfg_dict, file_format=file_format)\n            else:\n                mmcv.dump(cfg_dict, file)\n\n    def merge_from_dict(self, options, allow_list_keys=True):\n        \"\"\"Merge list into cfg_dict.\n\n        Merge the dict parsed by MultipleKVAction into this cfg.\n\n        Examples:\n            >>> options = {'model.backbone.depth': 50,\n            ...            'model.backbone.with_cp':True}\n            >>> cfg = Config(dict(model=dict(backbone=dict(type='ResNet'))))\n            >>> cfg.merge_from_dict(options)\n            >>> cfg_dict = super(Config, self).__getattribute__('_cfg_dict')\n            >>> assert cfg_dict == dict(\n            ...     model=dict(backbone=dict(depth=50, with_cp=True)))\n\n            # Merge list element\n            >>> cfg = Config(dict(pipeline=[\n            ...     dict(type='LoadImage'), dict(type='LoadAnnotations')]))\n            >>> options = dict(pipeline={'0': dict(type='SelfLoadImage')})\n            >>> cfg.merge_from_dict(options, allow_list_keys=True)\n            >>> cfg_dict = super(Config, self).__getattribute__('_cfg_dict')\n            >>> assert cfg_dict == dict(pipeline=[\n            ...     dict(type='SelfLoadImage'), dict(type='LoadAnnotations')])\n\n        Args:\n            options (dict): dict of configs to merge from.\n            allow_list_keys (bool): If True, int string keys (e.g. '0', '1')\n              are allowed in ``options`` and will replace the element of the\n              corresponding index in the config if the config is a list.\n              Default: True.\n        \"\"\"\n        option_cfg_dict = {}\n        for full_key, v in options.items():\n            d = option_cfg_dict\n            key_list = full_key.split('.')\n            for subkey in key_list[:-1]:\n                d.setdefault(subkey, ConfigDict())\n                d = d[subkey]\n            subkey = key_list[-1]\n            d[subkey] = v\n\n        cfg_dict = super(Config, self).__getattribute__('_cfg_dict')\n        super(Config, self).__setattr__(\n            '_cfg_dict',\n            Config._merge_a_into_b(\n                option_cfg_dict, cfg_dict, allow_list_keys=allow_list_keys))\n\n\nclass DictAction(Action):\n    \"\"\"\n    argparse action to split an argument into KEY=VALUE form\n    on the first = and append to a dictionary. List options can\n    be passed as comma separated values, i.e 'KEY=V1,V2,V3', or with explicit\n    brackets, i.e. 'KEY=[V1,V2,V3]'. It also support nested brackets to build\n    list/tuple values. e.g. 'KEY=[(V1,V2),(V3,V4)]'\n    \"\"\"\n\n    @staticmethod\n    def _parse_int_float_bool(val):\n        try:\n            return int(val)\n        except ValueError:\n            pass\n        try:\n            return float(val)\n        except ValueError:\n            pass\n        if val.lower() in ['true', 'false']:\n            return True if val.lower() == 'true' else False\n        return val\n\n    @staticmethod\n    def _parse_iterable(val):\n        \"\"\"Parse iterable values in the string.\n\n        All elements inside '()' or '[]' are treated as iterable values.\n\n        Args:\n            val (str): Value string.\n\n        Returns:\n            list | tuple: The expanded list or tuple from the string.\n\n        Examples:\n            >>> DictAction._parse_iterable('1,2,3')\n            [1, 2, 3]\n            >>> DictAction._parse_iterable('[a, b, c]')\n            ['a', 'b', 'c']\n            >>> DictAction._parse_iterable('[(1, 2, 3), [a, b], c]')\n            [(1, 2, 3), ['a', 'b], 'c']\n        \"\"\"\n\n        def find_next_comma(string):\n            \"\"\"Find the position of next comma in the string.\n\n            If no ',' is found in the string, return the string length. All\n            chars inside '()' and '[]' are treated as one element and thus ','\n            inside these brackets are ignored.\n            \"\"\"\n            assert (string.count('(') == string.count(')')) and (\n                    string.count('[') == string.count(']')), \\\n                f'Imbalanced brackets exist in {string}'\n            end = len(string)\n            for idx, char in enumerate(string):\n                pre = string[:idx]\n                # The string before this ',' is balanced\n                if ((char == ',') and (pre.count('(') == pre.count(')'))\n                        and (pre.count('[') == pre.count(']'))):\n                    end = idx\n                    break\n            return end\n\n        # Strip ' and \" characters and replace whitespace.\n        val = val.strip('\\'\\\"').replace(' ', '')\n        is_tuple = False\n        if val.startswith('(') and val.endswith(')'):\n            is_tuple = True\n            val = val[1:-1]\n        elif val.startswith('[') and val.endswith(']'):\n            val = val[1:-1]\n        elif ',' not in val:\n            # val is a single value\n            return DictAction._parse_int_float_bool(val)\n\n        values = []\n        while len(val) > 0:\n            comma_idx = find_next_comma(val)\n            element = DictAction._parse_iterable(val[:comma_idx])\n            values.append(element)\n            val = val[comma_idx + 1:]\n        if is_tuple:\n            values = tuple(values)\n        return values\n\n    def __call__(self, parser, namespace, values, option_string=None):\n        options = {}\n        for kv in values:\n            key, val = kv.split('=', maxsplit=1)\n            options[key] = self._parse_iterable(val)\n        setattr(namespace, self.dest, options)"}
{"type": "source_file", "path": "src/modeling/video_swin/swin_large_patch244_window877_kinetics400_22k.py", "content": "_base_ = [\r\n    'swin_large.py', 'default_runtime.py'\r\n]\r\nmodel=dict(backbone=dict(patch_size=(2,4,4), drop_path_rate=0.2), test_cfg=dict(max_testing_views=1))\r\n\r\n# dataset settings\r\ndataset_type = 'VideoDataset'\r\ndata_root = 'data/kinetics400/train'\r\ndata_root_val = 'data/kinetics400/val'\r\nann_file_train = 'data/kinetics400/kinetics400_train_list.txt'\r\nann_file_val = 'data/kinetics400/kinetics400_val_list.txt'\r\nann_file_test = 'data/kinetics400/kinetics400_val_list.txt'\r\nimg_norm_cfg = dict(\r\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_bgr=False)\r\ntrain_pipeline = [\r\n    dict(type='DecordInit'),\r\n    dict(type='SampleFrames', clip_len=32, frame_interval=2, num_clips=1),\r\n    dict(type='DecordDecode'),\r\n    dict(type='Resize', scale=(-1, 256)),\r\n    dict(type='RandomResizedCrop'),\r\n    dict(type='Resize', scale=(224, 224), keep_ratio=False),\r\n    dict(type='Flip', flip_ratio=0.5),\r\n    dict(type='Normalize', **img_norm_cfg),\r\n    dict(type='FormatShape', input_format='NCTHW'),\r\n    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\r\n    dict(type='ToTensor', keys=['imgs', 'label'])\r\n]\r\nval_pipeline = [\r\n    dict(type='DecordInit'),\r\n    dict(\r\n        type='SampleFrames',\r\n        clip_len=32,\r\n        frame_interval=2,\r\n        num_clips=1,\r\n        test_mode=True),\r\n    dict(type='DecordDecode'),\r\n    dict(type='Resize', scale=(-1, 256)),\r\n    dict(type='CenterCrop', crop_size=224),\r\n    dict(type='Flip', flip_ratio=0),\r\n    dict(type='Normalize', **img_norm_cfg),\r\n    dict(type='FormatShape', input_format='NCTHW'),\r\n    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\r\n    dict(type='ToTensor', keys=['imgs'])\r\n]\r\ntest_pipeline = [\r\n    dict(type='DecordInit'),\r\n    dict(\r\n        type='SampleFrames',\r\n        clip_len=32,\r\n        frame_interval=2,\r\n        num_clips=4,\r\n        test_mode=True),\r\n    dict(type='DecordDecode'),\r\n    dict(type='Resize', scale=(-1, 224)),\r\n    dict(type='ThreeCrop', crop_size=224),\r\n    dict(type='Flip', flip_ratio=0),\r\n    dict(type='Normalize', **img_norm_cfg),\r\n    dict(type='FormatShape', input_format='NCTHW'),\r\n    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\r\n    dict(type='ToTensor', keys=['imgs'])\r\n]\r\ndata = dict(\r\n    videos_per_gpu=8,\r\n    workers_per_gpu=1,\r\n    val_dataloader=dict(\r\n        videos_per_gpu=1,\r\n        workers_per_gpu=1\r\n    ),\r\n    test_dataloader=dict(\r\n        videos_per_gpu=1,\r\n        workers_per_gpu=1\r\n    ),\r\n    train=dict(\r\n        type=dataset_type,\r\n        ann_file=ann_file_train,\r\n        data_prefix=data_root,\r\n        pipeline=train_pipeline),\r\n    val=dict(\r\n        type=dataset_type,\r\n        ann_file=ann_file_val,\r\n        data_prefix=data_root_val,\r\n        pipeline=val_pipeline),\r\n    test=dict(\r\n        type=dataset_type,\r\n        ann_file=ann_file_test,\r\n        data_prefix=data_root_val,\r\n        pipeline=test_pipeline))\r\nevaluation = dict(\r\n    interval=5, metrics=['top_k_accuracy', 'mean_class_accuracy'])\r\n\r\n# optimizer\r\noptimizer = dict(type='AdamW', lr=3e-4, betas=(0.9, 0.999), weight_decay=0.05,\r\n                 paramwise_cfg=dict(custom_keys={'absolute_pos_embed': dict(decay_mult=0.),\r\n                                                 'relative_position_bias_table': dict(decay_mult=0.),\r\n                                                 'norm': dict(decay_mult=0.),\r\n                                                 'backbone': dict(lr_mult=0.1)}))\r\n# learning policy\r\nlr_config = dict(\r\n    policy='CosineAnnealing',\r\n    min_lr=0,\r\n    warmup='linear',\r\n    warmup_by_epoch=True,\r\n    warmup_iters=2.5\r\n)\r\ntotal_epochs = 30\r\n\r\n# runtime settings\r\ncheckpoint_config = dict(interval=1)\r\nwork_dir = work_dir = './work_dirs/swin_large_patch244_window877_kinetics400_22k'\r\nfind_unused_parameters = False\r\n\r\n\r\n# do not use mmdet version fp16\r\nfp16 = None\r\noptimizer_config = dict(\r\n    type=\"DistOptimizerHook\",\r\n    update_interval=8,\r\n    grad_clip=None,\r\n    coalesce=True,\r\n    bucket_size_mb=-1,\r\n    use_fp16=True,\r\n)"}
{"type": "source_file", "path": "src/modeling/swin/swin_transformer.py", "content": "# --------------------------------------------------------\n# Swin Transformer\n# Copyright (c) 2021 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ze Liu\n# --------------------------------------------------------\n\nimport torch\nimport torch.nn as nn\nimport torch.utils.checkpoint as checkpoint\nfrom src.timm.models.layers import DropPath, to_2tuple, trunc_normal_\n\n\nclass Mlp(nn.Module):\n    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.act = act_layer()\n        self.fc2 = nn.Linear(hidden_features, out_features)\n        self.drop = nn.Dropout(drop)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.drop(x)\n        x = self.fc2(x)\n        x = self.drop(x)\n        return x\n\n\ndef window_partition(x, window_size):\n    \"\"\"\n    Args:\n        x: (B, H, W, C)\n        window_size (int): window size\n\n    Returns:\n        windows: (num_windows*B, window_size, window_size, C)\n    \"\"\"\n    B, H, W, C = x.shape\n    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)\n    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)\n    return windows\n\n\ndef window_reverse(windows, window_size, H, W):\n    \"\"\"\n    Args:\n        windows: (num_windows*B, window_size, window_size, C)\n        window_size (int): Window size\n        H (int): Height of image\n        W (int): Width of image\n\n    Returns:\n        x: (B, H, W, C)\n    \"\"\"\n    B = int(windows.shape[0] / (H * W / window_size / window_size))\n    x = windows.view(B, H // window_size, W // window_size, window_size, window_size, -1)\n    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)\n    return x\n\n\nclass WindowAttention(nn.Module):\n    r\"\"\" Window based multi-head self attention (W-MSA) module with relative position bias.\n    It supports both of shifted and non-shifted window.\n\n    Args:\n        dim (int): Number of input channels.\n        window_size (tuple[int]): The height and width of the window.\n        num_heads (int): Number of attention heads.\n        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True\n        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set\n        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0\n        proj_drop (float, optional): Dropout ratio of output. Default: 0.0\n    \"\"\"\n\n    def __init__(self, dim, window_size, num_heads, qkv_bias=True, qk_scale=None, attn_drop=0., proj_drop=0.):\n\n        super().__init__()\n        self.dim = dim\n        self.window_size = window_size  # Wh, Ww\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        self.scale = qk_scale or head_dim ** -0.5\n\n        # define a parameter table of relative position bias\n        self.relative_position_bias_table = nn.Parameter(\n            torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads))  # 2*Wh-1 * 2*Ww-1, nH\n\n        # get pair-wise relative position index for each token inside the window\n        coords_h = torch.arange(self.window_size[0])\n        coords_w = torch.arange(self.window_size[1])\n        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n        coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n        relative_coords[:, :, 0] += self.window_size[0] - 1  # shift to start from 0\n        relative_coords[:, :, 1] += self.window_size[1] - 1\n        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n        relative_position_index = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n        self.register_buffer(\"relative_position_index\", relative_position_index)\n\n        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n\n        trunc_normal_(self.relative_position_bias_table, std=.02)\n        self.softmax = nn.Softmax(dim=-1)\n\n    def forward(self, x, mask=None):\n        \"\"\"\n        Args:\n            x: input features with shape of (num_windows*B, N, C)\n            mask: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None\n        \"\"\"\n        B_, N, C = x.shape\n        qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n        q, k, v = qkv[0], qkv[1], qkv[2]  # make torchscript happy (cannot use tensor as tuple)\n\n        q = q * self.scale\n        attn = (q @ k.transpose(-2, -1))\n\n        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(\n            self.window_size[0] * self.window_size[1], self.window_size[0] * self.window_size[1], -1)  # Wh*Ww,Wh*Ww,nH\n        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n        attn = attn + relative_position_bias.unsqueeze(0)\n\n        if mask is not None:\n            nW = mask.shape[0]\n            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)\n            attn = attn.view(-1, self.num_heads, N, N)\n            attn = self.softmax(attn)\n        else:\n            attn = self.softmax(attn)\n\n        attn = self.attn_drop(attn)\n\n        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x\n\n    def extra_repr(self) -> str:\n        return f'dim={self.dim}, window_size={self.window_size}, num_heads={self.num_heads}'\n\n    def flops(self, N):\n        # calculate flops for 1 window with token length of N\n        flops = 0\n        # qkv = self.qkv(x)\n        flops += N * self.dim * 3 * self.dim\n        # attn = (q @ k.transpose(-2, -1))\n        flops += self.num_heads * N * (self.dim // self.num_heads) * N\n        #  x = (attn @ v)\n        flops += self.num_heads * N * N * (self.dim // self.num_heads)\n        # x = self.proj(x)\n        flops += N * self.dim * self.dim\n        return flops\n\n\nclass SwinTransformerBlock(nn.Module):\n    r\"\"\" Swin Transformer Block.\n\n    Args:\n        dim (int): Number of input channels.\n        input_resolution (tuple[int]): Input resulotion.\n        num_heads (int): Number of attention heads.\n        window_size (int): Window size.\n        shift_size (int): Shift size for SW-MSA.\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n        drop (float, optional): Dropout rate. Default: 0.0\n        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n        drop_path (float, optional): Stochastic depth rate. Default: 0.0\n        act_layer (nn.Module, optional): Activation layer. Default: nn.GELU\n        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n    \"\"\"\n\n    def __init__(self, dim, input_resolution, num_heads, window_size=7, shift_size=0,\n                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0., drop_path=0.,\n                 act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n        super().__init__()\n        self.dim = dim\n        self.input_resolution = input_resolution\n        self.num_heads = num_heads\n        self.window_size = window_size\n        self.shift_size = shift_size\n        self.mlp_ratio = mlp_ratio\n        if min(self.input_resolution) <= self.window_size:\n            # if window size is larger than input resolution, we don't partition windows\n            self.shift_size = 0\n            self.window_size = min(self.input_resolution)\n        assert 0 <= self.shift_size < self.window_size, \"shift_size must in 0-window_size\"\n\n        self.norm1 = norm_layer(dim)\n        self.attn = WindowAttention(\n            dim, window_size=to_2tuple(self.window_size), num_heads=num_heads,\n            qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n\n        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n        self.norm2 = norm_layer(dim)\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n\n        if self.shift_size > 0:\n            # calculate attention mask for SW-MSA\n            H, W = self.input_resolution\n            img_mask = torch.zeros((1, H, W, 1))  # 1 H W 1\n            h_slices = (slice(0, -self.window_size),\n                        slice(-self.window_size, -self.shift_size),\n                        slice(-self.shift_size, None))\n            w_slices = (slice(0, -self.window_size),\n                        slice(-self.window_size, -self.shift_size),\n                        slice(-self.shift_size, None))\n            cnt = 0\n            for h in h_slices:\n                for w in w_slices:\n                    img_mask[:, h, w, :] = cnt\n                    cnt += 1\n\n            mask_windows = window_partition(img_mask, self.window_size)  # nW, window_size, window_size, 1\n            mask_windows = mask_windows.view(-1, self.window_size * self.window_size)\n            attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n            attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))\n        else:\n            attn_mask = None\n\n        self.register_buffer(\"attn_mask\", attn_mask)\n\n    def forward(self, x):\n        H, W = self.input_resolution\n        B, L, C = x.shape\n        assert L == H * W, \"input feature has wrong size\"\n\n        shortcut = x\n        x = self.norm1(x)\n        x = x.view(B, H, W, C)\n\n        # cyclic shift\n        if self.shift_size > 0:\n            shifted_x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))\n        else:\n            shifted_x = x\n\n        # partition windows\n        x_windows = window_partition(shifted_x, self.window_size)  # nW*B, window_size, window_size, C\n        x_windows = x_windows.view(-1, self.window_size * self.window_size, C)  # nW*B, window_size*window_size, C\n\n        # W-MSA/SW-MSA\n        attn_windows = self.attn(x_windows, mask=self.attn_mask)  # nW*B, window_size*window_size, C\n\n        # merge windows\n        attn_windows = attn_windows.view(-1, self.window_size, self.window_size, C)\n        shifted_x = window_reverse(attn_windows, self.window_size, H, W)  # B H' W' C\n\n        # reverse cyclic shift\n        if self.shift_size > 0:\n            x = torch.roll(shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2))\n        else:\n            x = shifted_x\n        x = x.view(B, H * W, C)\n\n        # FFN\n        x = shortcut + self.drop_path(x)\n        x = x + self.drop_path(self.mlp(self.norm2(x)))\n\n        return x\n\n    def extra_repr(self) -> str:\n        return f\"dim={self.dim}, input_resolution={self.input_resolution}, num_heads={self.num_heads}, \" \\\n               f\"window_size={self.window_size}, shift_size={self.shift_size}, mlp_ratio={self.mlp_ratio}\"\n\n    def flops(self):\n        flops = 0\n        H, W = self.input_resolution\n        # norm1\n        flops += self.dim * H * W\n        # W-MSA/SW-MSA\n        nW = H * W / self.window_size / self.window_size\n        flops += nW * self.attn.flops(self.window_size * self.window_size)\n        # mlp\n        flops += 2 * H * W * self.dim * self.dim * self.mlp_ratio\n        # norm2\n        flops += self.dim * H * W\n        return flops\n\n\nclass PatchMerging(nn.Module):\n    r\"\"\" Patch Merging Layer.\n\n    Args:\n        input_resolution (tuple[int]): Resolution of input feature.\n        dim (int): Number of input channels.\n        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n    \"\"\"\n\n    def __init__(self, input_resolution, dim, norm_layer=nn.LayerNorm):\n        super().__init__()\n        self.input_resolution = input_resolution\n        self.dim = dim\n        self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)\n        self.norm = norm_layer(4 * dim)\n\n    def forward(self, x):\n        \"\"\"\n        x: B, H*W, C\n        \"\"\"\n        H, W = self.input_resolution\n        B, L, C = x.shape\n        assert L == H * W, \"input feature has wrong size\"\n        assert H % 2 == 0 and W % 2 == 0, f\"x size ({H}*{W}) are not even.\"\n\n        x = x.view(B, H, W, C)\n\n        x0 = x[:, 0::2, 0::2, :]  # B H/2 W/2 C\n        x1 = x[:, 1::2, 0::2, :]  # B H/2 W/2 C\n        x2 = x[:, 0::2, 1::2, :]  # B H/2 W/2 C\n        x3 = x[:, 1::2, 1::2, :]  # B H/2 W/2 C\n        x = torch.cat([x0, x1, x2, x3], -1)  # B H/2 W/2 4*C\n        x = x.view(B, -1, 4 * C)  # B H/2*W/2 4*C\n\n        x = self.norm(x)\n        x = self.reduction(x)\n\n        return x\n\n    def extra_repr(self) -> str:\n        return f\"input_resolution={self.input_resolution}, dim={self.dim}\"\n\n    def flops(self):\n        H, W = self.input_resolution\n        flops = H * W * self.dim\n        flops += (H // 2) * (W // 2) * 4 * self.dim * 2 * self.dim\n        return flops\n\n\nclass BasicLayer(nn.Module):\n    \"\"\" A basic Swin Transformer layer for one stage.\n\n    Args:\n        dim (int): Number of input channels.\n        input_resolution (tuple[int]): Input resolution.\n        depth (int): Number of blocks.\n        num_heads (int): Number of attention heads.\n        window_size (int): Local window size.\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n        drop (float, optional): Dropout rate. Default: 0.0\n        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0\n        norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm\n        downsample (nn.Module | None, optional): Downsample layer at the end of the layer. Default: None\n        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False.\n    \"\"\"\n\n    def __init__(self, dim, input_resolution, depth, num_heads, window_size,\n                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0.,\n                 drop_path=0., norm_layer=nn.LayerNorm, downsample=None, use_checkpoint=False):\n\n        super().__init__()\n        self.dim = dim\n        self.input_resolution = input_resolution\n        self.depth = depth\n        self.use_checkpoint = use_checkpoint\n\n        # build blocks\n        self.blocks = nn.ModuleList([\n            SwinTransformerBlock(dim=dim, input_resolution=input_resolution,\n                                 num_heads=num_heads, window_size=window_size,\n                                 shift_size=0 if (i % 2 == 0) else window_size // 2,\n                                 mlp_ratio=mlp_ratio,\n                                 qkv_bias=qkv_bias, qk_scale=qk_scale,\n                                 drop=drop, attn_drop=attn_drop,\n                                 drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,\n                                 norm_layer=norm_layer)\n            for i in range(depth)])\n\n        # patch merging layer\n        if downsample is not None:\n            self.downsample = downsample(input_resolution, dim=dim, norm_layer=norm_layer)\n        else:\n            self.downsample = None\n\n    def forward(self, x):\n        for blk in self.blocks:\n            if self.use_checkpoint:\n                x = checkpoint.checkpoint(blk, x)\n            else:\n                x = blk(x)\n        if self.downsample is not None:\n            x = self.downsample(x)\n        return x\n\n    def extra_repr(self) -> str:\n        return f\"dim={self.dim}, input_resolution={self.input_resolution}, depth={self.depth}\"\n\n    def flops(self):\n        flops = 0\n        for blk in self.blocks:\n            flops += blk.flops()\n        if self.downsample is not None:\n            flops += self.downsample.flops()\n        return flops\n\n\nclass PatchEmbed(nn.Module):\n    r\"\"\" Image to Patch Embedding\n\n    Args:\n        img_size (int): Image size.  Default: 224.\n        patch_size (int): Patch token size. Default: 4.\n        in_chans (int): Number of input image channels. Default: 3.\n        embed_dim (int): Number of linear projection output channels. Default: 96.\n        norm_layer (nn.Module, optional): Normalization layer. Default: None\n    \"\"\"\n\n    def __init__(self, img_size=224, patch_size=4, in_chans=3, embed_dim=96, norm_layer=None):\n        super().__init__()\n        img_size = to_2tuple(img_size)\n        patch_size = to_2tuple(patch_size)\n        patches_resolution = [img_size[0] // patch_size[0], img_size[1] // patch_size[1]]\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.patches_resolution = patches_resolution\n        self.num_patches = patches_resolution[0] * patches_resolution[1]\n\n        self.in_chans = in_chans\n        self.embed_dim = embed_dim\n\n        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n        if norm_layer is not None:\n            self.norm = norm_layer(embed_dim)\n        else:\n            self.norm = None\n\n    def forward(self, x):\n        B, C, H, W = x.shape\n        # FIXME look at relaxing size constraints\n        assert H == self.img_size[0] and W == self.img_size[1], \\\n            f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n        x = self.proj(x).flatten(2).transpose(1, 2)  # B Ph*Pw C\n        if self.norm is not None:\n            x = self.norm(x)\n        return x\n\n    def flops(self):\n        Ho, Wo = self.patches_resolution\n        flops = Ho * Wo * self.embed_dim * self.in_chans * (self.patch_size[0] * self.patch_size[1])\n        if self.norm is not None:\n            flops += Ho * Wo * self.embed_dim\n        return flops\n\n\nclass SwinTransformer(nn.Module):\n    r\"\"\" Swin Transformer\n        A PyTorch impl of : `Swin Transformer: Hierarchical Vision Transformer using Shifted Windows`  -\n          https://arxiv.org/pdf/2103.14030\n\n    Args:\n        img_size (int | tuple(int)): Input image size. Default 224\n        patch_size (int | tuple(int)): Patch size. Default: 4\n        in_chans (int): Number of input image channels. Default: 3\n        num_classes (int): Number of classes for classification head. Default: 1000\n        embed_dim (int): Patch embedding dimension. Default: 96\n        depths (tuple(int)): Depth of each Swin Transformer layer.\n        num_heads (tuple(int)): Number of attention heads in different layers.\n        window_size (int): Window size. Default: 7\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4\n        qkv_bias (bool): If True, add a learnable bias to query, key, value. Default: True\n        qk_scale (float): Override default qk scale of head_dim ** -0.5 if set. Default: None\n        drop_rate (float): Dropout rate. Default: 0\n        attn_drop_rate (float): Attention dropout rate. Default: 0\n        drop_path_rate (float): Stochastic depth rate. Default: 0.1\n        norm_layer (nn.Module): Normalization layer. Default: nn.LayerNorm.\n        ape (bool): If True, add absolute position embedding to the patch embedding. Default: False\n        patch_norm (bool): If True, add normalization after patch embedding. Default: True\n        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False\n    \"\"\"\n\n    def __init__(self, img_size=224, patch_size=4, in_chans=3, num_classes=1000,\n                 embed_dim=96, depths=[2, 2, 6, 2], num_heads=[3, 6, 12, 24],\n                 window_size=7, mlp_ratio=4., qkv_bias=True, qk_scale=None,\n                 drop_rate=0., attn_drop_rate=0., drop_path_rate=0.1,\n                 norm_layer=nn.LayerNorm, ape=False, patch_norm=True,\n                 use_checkpoint=False, **kwargs):\n        super().__init__()\n\n        self.num_classes = num_classes\n        self.num_layers = len(depths)\n        self.embed_dim = embed_dim\n        self.ape = ape\n        self.patch_norm = patch_norm\n        self.num_features = int(embed_dim * 2 ** (self.num_layers - 1))\n        self.mlp_ratio = mlp_ratio\n\n        # split image into non-overlapping patches\n        self.patch_embed = PatchEmbed(\n            img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim,\n            norm_layer=norm_layer if self.patch_norm else None)\n        num_patches = self.patch_embed.num_patches\n        patches_resolution = self.patch_embed.patches_resolution\n        self.patches_resolution = patches_resolution\n\n        # absolute position embedding\n        if self.ape:\n            self.absolute_pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim))\n            trunc_normal_(self.absolute_pos_embed, std=.02)\n\n        self.pos_drop = nn.Dropout(p=drop_rate)\n\n        # stochastic depth\n        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]  # stochastic depth decay rule\n\n        # build layers\n        self.layers = nn.ModuleList()\n        for i_layer in range(self.num_layers):\n            layer = BasicLayer(dim=int(embed_dim * 2 ** i_layer),\n                               input_resolution=(patches_resolution[0] // (2 ** i_layer),\n                                                 patches_resolution[1] // (2 ** i_layer)),\n                               depth=depths[i_layer],\n                               num_heads=num_heads[i_layer],\n                               window_size=window_size,\n                               mlp_ratio=self.mlp_ratio,\n                               qkv_bias=qkv_bias, qk_scale=qk_scale,\n                               drop=drop_rate, attn_drop=attn_drop_rate,\n                               drop_path=dpr[sum(depths[:i_layer]):sum(depths[:i_layer + 1])],\n                               norm_layer=norm_layer,\n                               downsample=PatchMerging if (i_layer < self.num_layers - 1) else None,\n                               use_checkpoint=use_checkpoint)\n            self.layers.append(layer)\n\n        self.norm = norm_layer(self.num_features)\n        self.avgpool = nn.AdaptiveAvgPool1d(1)\n        self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()\n\n        self.apply(self._init_weights)\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            trunc_normal_(m.weight, std=.02)\n            if isinstance(m, nn.Linear) and m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.LayerNorm):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)\n\n    @torch.jit.ignore\n    def no_weight_decay(self):\n        return {'absolute_pos_embed'}\n\n    @torch.jit.ignore\n    def no_weight_decay_keywords(self):\n        return {'relative_position_bias_table'}\n\n    def forward_features(self, x):\n        x = self.patch_embed(x)\n        if self.ape:\n            x = x + self.absolute_pos_embed\n        x = self.pos_drop(x)\n\n        for layer in self.layers:\n            x = layer(x)\n\n        x = self.norm(x)  # B L C\n        x = self.avgpool(x.transpose(1, 2))  # B C 1\n        x = torch.flatten(x, 1)\n        return x\n\n    def forward(self, x):\n        x = self.forward_features(x)\n        x = self.head(x)\n        return x\n\n    def forward_global_gridfeat(self, x):\n        x = self.patch_embed(x)\n        if self.ape:\n            x = x + self.absolute_pos_embed\n        x = self.pos_drop(x)\n\n        for layer in self.layers:\n            x = layer(x)\n\n        x = self.norm(x)  # B L C\n        xx = self.avgpool(x.transpose(1, 2))  # B C 1\n        xx = torch.flatten(xx, 1)\n        return xx, x\n\n    def forward_pyramid_feat(self, x):\n        \"\"\"Forward function.\"\"\"\n        x = self.patch_embed(x)\n        if self.ape:\n            x = x + self.absolute_pos_embed\n        x = self.pos_drop(x)\n\n        outs = []\n        for layer in self.layers:\n            x = layer(x)\n            outs.append(x)\n\n        return tuple(outs)\n\n    def flops(self):\n        flops = 0\n        flops += self.patch_embed.flops()\n        for i, layer in enumerate(self.layers):\n            flops += layer.flops()\n        flops += self.num_features * self.patches_resolution[0] * self.patches_resolution[1] // (2 ** self.num_layers)\n        flops += self.num_features * self.num_classes\n        return flops\n"}
{"type": "source_file", "path": "src/modeling/video_swin/swin_base.py", "content": "# model settings\n_base_ = \"swin_tiny.py\"\nmodel = dict(backbone=dict(depths=[2, 2, 18, 2],\n                           embed_dim=128,\n                           num_heads=[4, 8, 16, 32]),\n             cls_head=dict(in_channels=1024))"}
{"type": "source_file", "path": "src/datasets/vl_dataloader.py", "content": "\"\"\"\nCopyright (c) Microsoft Corporation.\nLicensed under the MIT license.\n\n\"\"\"\nimport os.path as op\nimport torch\nfrom src.utils.comm import get_world_size\nfrom .vision_language_tsv import (VisionLanguageTSVYamlDataset)\nfrom .caption_tensorizer import build_tensorizer\nfrom .data_sampler import DistributedSamplerLimited, NodeSplitSampler\nfrom src.utils.logger import LOGGER as logger\n\n\ndef build_dataset(args, yaml_file, tokenizer, is_train=True):\n    logger.info(f'yaml_file:{yaml_file}')\n    if not op.isfile(yaml_file):\n        yaml_file = op.join(args.data_dir, yaml_file)\n        assert op.isfile(yaml_file), f\"{yaml_file} does not exists\"\n    tensorizer = build_tensorizer(args, tokenizer, is_train=is_train)\n    dataset_class = VisionLanguageTSVYamlDataset\n    return dataset_class(args, yaml_file, tokenizer, tensorizer, is_train, args.on_memory)\n\n\nclass IterationBasedBatchSampler(torch.utils.data.sampler.BatchSampler):\n    \"\"\"\n    Wraps a BatchSampler, resampling from it until\n    a specified number of iterations have been sampled\n    \"\"\"\n\n    def __init__(self, batch_sampler, num_iterations, start_iter=0):\n        self.batch_sampler = batch_sampler\n        self.num_iterations = num_iterations\n        self.start_iter = start_iter\n\n    def __iter__(self):\n        iteration = self.start_iter\n        while iteration <= self.num_iterations:\n            # if the underlying sampler has a set_epoch method, like\n            # DistributedSampler, used for making each process see\n            # a different split of the dataset, then set it\n            if hasattr(self.batch_sampler.sampler, \"set_epoch\"):\n                self.batch_sampler.sampler.set_epoch(iteration)\n            for batch in self.batch_sampler:\n                iteration += 1\n                if iteration > self.num_iterations:\n                    break\n                yield batch\n\n    def __len__(self):\n        return self.num_iterations\n\n\ndef make_batch_data_sampler(sampler, images_per_gpu, num_iters=None, start_iter=0):\n    batch_sampler = torch.utils.data.sampler.BatchSampler(\n        sampler, images_per_gpu, drop_last=False\n    )\n    if num_iters is not None and num_iters >= 0:\n        batch_sampler = IterationBasedBatchSampler(\n            batch_sampler, num_iters, start_iter\n        )\n    return batch_sampler\n\n\ndef make_data_sampler(dataset, shuffle, distributed, random_seed, limited_samples=-1):\n    if distributed:\n        if dataset.is_composite:\n            # first_epoch_skip_shuffle not working yet\n            logger.info(\"Enable NodeSplitSampler with first_epoch_skip_shuffle=True\")\n            return NodeSplitSampler(\n                dataset, shuffle=shuffle, random_seed=random_seed,\n                first_epoch_skip_shuffle=True)\n        elif limited_samples < 1:\n            return torch.utils.data.distributed.DistributedSampler(dataset, shuffle=shuffle, seed=random_seed)\n        else:  # use limited distributed sampler\n            return DistributedSamplerLimited(dataset, shuffle=shuffle, limited=limited_samples)\n    if shuffle:\n        sampler = torch.utils.data.sampler.RandomSampler(dataset)\n    else:\n        sampler = torch.utils.data.sampler.SequentialSampler(dataset)\n    return sampler\n\n\ndef make_data_loader(args, yaml_file, tokenizer, is_distributed=True,\n        is_train=True, start_iter=0, num_gpus=8):\n\n    dataset = build_dataset(args, yaml_file, tokenizer, is_train=is_train)\n    a,b,c = dataset.__getitem__(100)\n    # a,b,c = dataset.__getitem__(1100)\n    if is_train==True:\n        shuffle = True\n        images_per_gpu = args.per_gpu_train_batch_size\n        images_per_batch = images_per_gpu * get_world_size()\n        iters_per_batch = len(dataset) // images_per_batch\n        num_iters = iters_per_batch * args.num_train_epochs\n        logger.info(\"Train with {} images per GPU.\".format(images_per_gpu))\n        logger.info(\"Total batch size {}\".format(images_per_batch))\n        logger.info(\"Total training steps {}\".format(num_iters))\n    else:\n        shuffle = False\n        images_per_gpu = args.per_gpu_eval_batch_size\n        num_iters = None\n        start_iter = 0\n\n    if hasattr(args, 'limited_samples'):\n        limited_samples = args.limited_samples // num_gpus\n    else:\n        limited_samples = -1\n    random_seed = args.seed\n    sampler = make_data_sampler(\n        dataset, shuffle, is_distributed, limited_samples=limited_samples,\n        random_seed=random_seed)\n    batch_sampler = make_batch_data_sampler(\n        sampler, images_per_gpu, num_iters, start_iter\n    )\n    data_loader = torch.utils.data.DataLoader(\n        dataset, num_workers=args.num_workers, batch_sampler=batch_sampler,\n        pin_memory=True, worker_init_fn=init_seeds,\n    )\n    return data_loader\n\ndef init_seeds(seed=88):\n    import os, random\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    import numpy as np\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)"}
{"type": "source_file", "path": "src/modeling/video_swin/swin_large.py", "content": "# model settings\n_base_ = \"swin_tiny.py\"\nmodel = dict(backbone=dict(depths=[2, 2, 18, 2],\n                           embed_dim=192,\n                           num_heads=[6, 12, 24, 48]),\n             cls_head=dict(in_channels=1536))"}
{"type": "source_file", "path": "src/datasets/data_utils/image_ops.py", "content": "import numpy as np\r\nimport base64\r\nimport cv2\r\nimport torch\r\nimport scipy.misc\r\n\r\ndef img_from_base64(imagestring):\r\n    try:\r\n        jpgbytestring = base64.b64decode(imagestring)\r\n        nparr = np.frombuffer(jpgbytestring, np.uint8)\r\n        r = cv2.imdecode(nparr, cv2.IMREAD_COLOR)\r\n        return r\r\n    except ValueError:\r\n        return None\r\n\r\ndef myimrotate(img,\r\n             angle,\r\n             center=None,\r\n             scale=1.0,\r\n             border_value=0,\r\n             auto_bound=False):\r\n    \"\"\"Rotate an image.\r\n    Args:\r\n        img (ndarray): Image to be rotated.\r\n        angle (float): Rotation angle in degrees, positive values mean\r\n            clockwise rotation.\r\n        center (tuple): Center of the rotation in the source image, by default\r\n            it is the center of the image.\r\n        scale (float): Isotropic scale factor.\r\n        border_value (int): Border value.\r\n        auto_bound (bool): Whether to adjust the image size to cover the whole\r\n            rotated image.\r\n    Returns:\r\n        ndarray: The rotated image.\r\n    \"\"\"\r\n    if center is not None and auto_bound:\r\n        raise ValueError('`auto_bound` conflicts with `center`')\r\n    h, w = img.shape[:2]\r\n    if center is None:\r\n        center = ((w - 1) * 0.5, (h - 1) * 0.5)\r\n    assert isinstance(center, tuple)\r\n\r\n    matrix = cv2.getRotationMatrix2D(center, angle, scale)\r\n    if auto_bound:\r\n        cos = np.abs(matrix[0, 0])\r\n        sin = np.abs(matrix[0, 1])\r\n        new_w = h * sin + w * cos\r\n        new_h = h * cos + w * sin\r\n        matrix[0, 2] += (new_w - w) * 0.5\r\n        matrix[1, 2] += (new_h - h) * 0.5\r\n        w = int(np.round(new_w))\r\n        h = int(np.round(new_h))\r\n    rotated = cv2.warpAffine(img, matrix, (w, h), borderValue=border_value)\r\n    return rotated\r\n\r\ndef myimresize(img, size, return_scale=False, interpolation='bilinear'):\r\n    \"\"\"Resize image to a given size.\r\n    Args:\r\n        img (ndarray): The input image.\r\n        size (tuple): Target (w, h).\r\n        return_scale (bool): Whether to return `w_scale` and `h_scale`.\r\n        interpolation (str): Interpolation method, accepted values are\r\n            \"nearest\", \"bilinear\", \"bicubic\", \"area\", \"lanczos\".\r\n    Returns:\r\n        tuple or ndarray: (`resized_img`, `w_scale`, `h_scale`) or\r\n            `resized_img`.\r\n    \"\"\"\r\n    h, w = img.shape[:2]\r\n    resized_img = cv2.resize(\r\n        img, (size[0],size[1]), interpolation=cv2.INTER_LINEAR)\r\n    if not return_scale:\r\n        return resized_img\r\n    else:\r\n        w_scale = size[0] / w\r\n        h_scale = size[1] / h\r\n        return resized_img, w_scale, h_scale\r\n\r\n\r\ndef get_transform(center, scale, res, rot=0):\r\n    \"\"\"Generate transformation matrix.\"\"\"\r\n    h = 200 * scale\r\n    t = np.zeros((3, 3))\r\n    t[0, 0] = float(res[1]) / h\r\n    t[1, 1] = float(res[0]) / h\r\n    t[0, 2] = res[1] * (-float(center[0]) / h + .5)\r\n    t[1, 2] = res[0] * (-float(center[1]) / h + .5)\r\n    t[2, 2] = 1\r\n    if not rot == 0:\r\n        rot = -rot # To match direction of rotation from cropping\r\n        rot_mat = np.zeros((3,3))\r\n        rot_rad = rot * np.pi / 180\r\n        sn,cs = np.sin(rot_rad), np.cos(rot_rad)\r\n        rot_mat[0,:2] = [cs, -sn]\r\n        rot_mat[1,:2] = [sn, cs]\r\n        rot_mat[2,2] = 1\r\n        # Need to rotate around center\r\n        t_mat = np.eye(3)\r\n        t_mat[0,2] = -res[1]/2\r\n        t_mat[1,2] = -res[0]/2\r\n        t_inv = t_mat.copy()\r\n        t_inv[:2,2] *= -1\r\n        t = np.dot(t_inv,np.dot(rot_mat,np.dot(t_mat,t)))\r\n    return t\r\n\r\ndef transform(pt, center, scale, res, invert=0, rot=0):\r\n    \"\"\"Transform pixel location to different reference.\"\"\"\r\n    t = get_transform(center, scale, res, rot=rot)\r\n    if invert:\r\n        # t = np.linalg.inv(t)\r\n        t_torch = torch.from_numpy(t)\r\n        t_torch = torch.inverse(t_torch)\r\n        t = t_torch.numpy()\r\n    new_pt = np.array([pt[0]-1, pt[1]-1, 1.]).T\r\n    new_pt = np.dot(t, new_pt)\r\n    return new_pt[:2].astype(int)+1\r\n\r\ndef crop(img, center, scale, res, rot=0):\r\n    \"\"\"Crop image according to the supplied bounding box.\"\"\"\r\n    # Upper left point\r\n    ul = np.array(transform([1, 1], center, scale, res, invert=1))-1\r\n    # Bottom right point\r\n    br = np.array(transform([res[0]+1, \r\n                             res[1]+1], center, scale, res, invert=1))-1\r\n    # Padding so that when rotated proper amount of context is included\r\n    pad = int(np.linalg.norm(br - ul) / 2 - float(br[1] - ul[1]) / 2)\r\n    print('pad:',pad, 'rot:',rot, 'ul:', ul, \"br:\",br)\r\n    if not rot == 0:\r\n        ul -= pad\r\n        br += pad\r\n    new_shape = [br[1] - ul[1], br[0] - ul[0]]\r\n    if len(img.shape) > 2:\r\n        new_shape += [img.shape[2]]\r\n    new_img = np.zeros(new_shape)\r\n    print('new_shape:',new_shape, ' old_shape:', img.shape)\r\n\r\n    # Range to fill new array\r\n    new_x = max(0, -ul[0]), min(br[0], len(img[0])) - ul[0]\r\n    new_y = max(0, -ul[1]), min(br[1], len(img)) - ul[1]\r\n    # Range to sample from original image\r\n    old_x = max(0, ul[0]), min(len(img[0]), br[0])\r\n    old_y = max(0, ul[1]), min(len(img), br[1])\r\n\r\n    new_img[new_y[0]:new_y[1], new_x[0]:new_x[1]] = img[old_y[0]:old_y[1], \r\n                                                        old_x[0]:old_x[1]]\r\n    if not rot == 0:\r\n        # Remove padding\r\n        # new_img = scipy.misc.imrotate(new_img, rot)\r\n        new_img = myimrotate(new_img, rot)\r\n        new_img = new_img[pad:-pad, pad:-pad]\r\n\r\n    # new_img = scipy.misc.imresize(new_img, res)\r\n    new_img = myimresize(new_img, [res[0], res[1]])\r\n    return new_img\r\n\r\ndef uncrop(img, center, scale, orig_shape, rot=0, is_rgb=True):\r\n    \"\"\"'Undo' the image cropping/resizing.\r\n    This function is used when evaluating mask/part segmentation.\r\n    \"\"\"\r\n    res = img.shape[:2]\r\n    # Upper left point\r\n    ul = np.array(transform([1, 1], center, scale, res, invert=1))-1\r\n    # Bottom right point\r\n    br = np.array(transform([res[0]+1,res[1]+1], center, scale, res, invert=1))-1\r\n    # size of cropped image\r\n    crop_shape = [br[1] - ul[1], br[0] - ul[0]]\r\n\r\n    new_shape = [br[1] - ul[1], br[0] - ul[0]]\r\n    if len(img.shape) > 2:\r\n        new_shape += [img.shape[2]]\r\n    new_img = np.zeros(orig_shape, dtype=np.uint8)\r\n    # Range to fill new array\r\n    new_x = max(0, -ul[0]), min(br[0], orig_shape[1]) - ul[0]\r\n    new_y = max(0, -ul[1]), min(br[1], orig_shape[0]) - ul[1]\r\n    # Range to sample from original image\r\n    old_x = max(0, ul[0]), min(orig_shape[1], br[0])\r\n    old_y = max(0, ul[1]), min(orig_shape[0], br[1])\r\n    # img = scipy.misc.imresize(img, crop_shape, interp='nearest')\r\n    img = myimresize(img, [crop_shape[0],crop_shape[1]])\r\n    new_img[old_y[0]:old_y[1], old_x[0]:old_x[1]] = img[new_y[0]:new_y[1], new_x[0]:new_x[1]]\r\n    return new_img\r\n\r\ndef rot_aa(aa, rot):\r\n    \"\"\"Rotate axis angle parameters.\"\"\"\r\n    # pose parameters\r\n    R = np.array([[np.cos(np.deg2rad(-rot)), -np.sin(np.deg2rad(-rot)), 0],\r\n                  [np.sin(np.deg2rad(-rot)), np.cos(np.deg2rad(-rot)), 0],\r\n                  [0, 0, 1]])\r\n    # find the rotation of the body in camera frame\r\n    per_rdg, _ = cv2.Rodrigues(aa)\r\n    # apply the global rotation to the global orientation\r\n    resrot, _ = cv2.Rodrigues(np.dot(R,per_rdg))\r\n    aa = (resrot.T)[0]\r\n    return aa\r\n\r\ndef flip_img(img):\r\n    \"\"\"Flip rgb images or masks.\r\n    channels come last, e.g. (256,256,3).\r\n    \"\"\"\r\n    img = np.fliplr(img)\r\n    return img\r\n\r\ndef flip_kp(kp):\r\n    \"\"\"Flip keypoints.\"\"\"\r\n    flipped_parts = [5, 4, 3, 2, 1, 0, 11, 10, 9, 8, 7, 6, 12, 13, 14, 15, 16, 17, 18, 19, 21, 20, 23, 22]\r\n    kp = kp[flipped_parts]\r\n    kp[:,0] = - kp[:,0]\r\n    return kp\r\n\r\ndef flip_pose(pose):\r\n    \"\"\"Flip pose.\r\n    The flipping is based on SMPL parameters.\r\n    \"\"\"\r\n    flippedParts = [0, 1, 2, 6, 7, 8, 3, 4, 5, 9, 10, 11, 15, 16, 17, 12, 13,\r\n                    14 ,18, 19, 20, 24, 25, 26, 21, 22, 23, 27, 28, 29, 33, \r\n                    34, 35, 30, 31, 32, 36, 37, 38, 42, 43, 44, 39, 40, 41, \r\n                    45, 46, 47, 51, 52, 53, 48, 49, 50, 57, 58, 59, 54, 55, \r\n                    56, 63, 64, 65, 60, 61, 62, 69, 70, 71, 66, 67, 68]\r\n    pose = pose[flippedParts]\r\n    # we also negate the second and the third dimension of the axis-angle\r\n    pose[1::3] = -pose[1::3]\r\n    pose[2::3] = -pose[2::3]\r\n    return pose\r\n\r\ndef flip_aa(aa):\r\n    \"\"\"Flip axis-angle representation.\r\n    We negate the second and the third dimension of the axis-angle.\r\n    \"\"\"\r\n    aa[1] = -aa[1]\r\n    aa[2] = -aa[2]\r\n    return aa"}
{"type": "source_file", "path": "src/modeling/video_swin/swin_large_384_patch244_window81212_kinetics400_22k.py", "content": "_base_ = [\r\n    'swin_large.py', 'default_runtime.py'\r\n]\r\nmodel=dict(backbone=dict(patch_size=(2,4,4), window_size=(8,12,12), drop_path_rate=0.5), test_cfg=dict(max_testing_views=1), train_cfg=dict(blending=dict(type='LabelSmoothing', num_classes=400, smoothing=0.1)))\r\n\r\n# dataset settings\r\ndataset_type = 'VideoDataset'\r\ndata_root = 'data/kinetics400/train'\r\ndata_root_val = 'data/kinetics400/val'\r\nann_file_train = 'data/kinetics400/kinetics400_train_list.txt'\r\nann_file_val = 'data/kinetics400/kinetics400_val_list.txt'\r\nann_file_test = 'data/kinetics400/kinetics400_val_list.txt'\r\nimg_norm_cfg = dict(\r\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_bgr=False)\r\ntrain_pipeline = [\r\n    dict(type='DecordInit'),\r\n    dict(type='SampleFrames', clip_len=32, frame_interval=2, num_clips=1),\r\n    dict(type='DecordDecode'),\r\n    dict(type='Resize', scale=(-1, 416)),\r\n    dict(type='RandomResizedCrop'),\r\n    dict(type='Resize', scale=(384, 384), keep_ratio=False),\r\n    dict(type='Flip', flip_ratio=0.5),\r\n    dict(type='Imgaug', transforms=[dict(type='RandAugment', n=4, m=7)]),\r\n    dict(type='Normalize', **img_norm_cfg),\r\n    dict(type='RandomErasing', probability=0.25),\r\n    dict(type='FormatShape', input_format='NCTHW'),\r\n    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\r\n    dict(type='ToTensor', keys=['imgs', 'label'])\r\n]\r\nval_pipeline = [\r\n    dict(type='DecordInit'),\r\n    dict(\r\n        type='SampleFrames',\r\n        clip_len=32,\r\n        frame_interval=2,\r\n        num_clips=1,\r\n        test_mode=True),\r\n    dict(type='DecordDecode'),\r\n    dict(type='Resize', scale=(-1, 416)),\r\n    dict(type='CenterCrop', crop_size=384),\r\n    dict(type='Flip', flip_ratio=0),\r\n    dict(type='Normalize', **img_norm_cfg),\r\n    dict(type='FormatShape', input_format='NCTHW'),\r\n    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\r\n    dict(type='ToTensor', keys=['imgs'])\r\n]\r\ntest_pipeline = [\r\n    dict(type='DecordInit'),\r\n    dict(\r\n        type='SampleFrames',\r\n        clip_len=32,\r\n        frame_interval=2,\r\n        num_clips=4,\r\n        test_mode=True),\r\n    dict(type='DecordDecode'),\r\n    dict(type='Resize', scale=(-1, 384)),\r\n    dict(type='ThreeCrop', crop_size=384),\r\n    dict(type='Flip', flip_ratio=0),\r\n    dict(type='Normalize', **img_norm_cfg),\r\n    dict(type='FormatShape', input_format='NCTHW'),\r\n    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\r\n    dict(type='ToTensor', keys=['imgs'])\r\n]\r\ndata = dict(\r\n    videos_per_gpu=8,\r\n    workers_per_gpu=1,\r\n    val_dataloader=dict(\r\n        videos_per_gpu=1,\r\n        workers_per_gpu=1\r\n    ),\r\n    test_dataloader=dict(\r\n        videos_per_gpu=1,\r\n        workers_per_gpu=1\r\n    ),\r\n    train=dict(\r\n        type=dataset_type,\r\n        ann_file=ann_file_train,\r\n        data_prefix=data_root,\r\n        pipeline=train_pipeline),\r\n    val=dict(\r\n        type=dataset_type,\r\n        ann_file=ann_file_val,\r\n        data_prefix=data_root_val,\r\n        pipeline=val_pipeline),\r\n    test=dict(\r\n        type=dataset_type,\r\n        ann_file=ann_file_test,\r\n        data_prefix=data_root_val,\r\n        pipeline=test_pipeline))\r\nevaluation = dict(\r\n    interval=5, metrics=['top_k_accuracy', 'mean_class_accuracy'])\r\n\r\n# optimizer\r\noptimizer = dict(type='AdamW', lr=3e-4, betas=(0.9, 0.999), weight_decay=0.05,\r\n                 paramwise_cfg=dict(custom_keys={'absolute_pos_embed': dict(decay_mult=0.),\r\n                                                 'relative_position_bias_table': dict(decay_mult=0.),\r\n                                                 'norm': dict(decay_mult=0.),\r\n                                                 'backbone': dict(lr_mult=0.1)}))\r\n# learning policy\r\nlr_config = dict(\r\n    policy='CosineAnnealing',\r\n    min_lr=0,\r\n    warmup='linear',\r\n    warmup_by_epoch=True,\r\n    warmup_iters=2.5\r\n)\r\ntotal_epochs = 60\r\n\r\n# runtime settings\r\ncheckpoint_config = dict(interval=1)\r\nwork_dir = work_dir = './work_dirs/swin_large_384_patch244_window81212_kinetics400_22k'\r\nfind_unused_parameters = False\r\n\r\n\r\n# do not use mmdet version fp16\r\nfp16 = None\r\noptimizer_config = dict(\r\n    type=\"DistOptimizerHook\",\r\n    update_interval=8,\r\n    grad_clip=None,\r\n    coalesce=True,\r\n    bucket_size_mb=-1,\r\n    use_fp16=True,\r\n)"}
{"type": "source_file", "path": "src/layers/bert/file_utils.py", "content": "\"\"\"\nUtilities for working with the local dataset cache.\nThis file is adapted from the AllenNLP library at https://github.com/allenai/allennlp\nCopyright by the AllenNLP authors.\n\"\"\"\nfrom __future__ import (absolute_import, division, print_function, unicode_literals)\n\nimport sys\nimport json\nimport os\nimport shutil\nimport tempfile\nimport fnmatch\nfrom functools import wraps\nfrom hashlib import sha256\nfrom io import open\n\nimport boto3\nimport requests\nfrom botocore.exceptions import ClientError\nfrom tqdm import tqdm\n\ntry:\n    from torch.hub import _get_torch_home\n    torch_cache_home = _get_torch_home()\nexcept ImportError:\n    torch_cache_home = os.path.expanduser(\n        os.getenv('TORCH_HOME', os.path.join(\n            os.getenv('XDG_CACHE_HOME', '~/.cache'), 'torch')))\ndefault_cache_path = os.path.join(torch_cache_home, 'pytorch_transformers')\n\ntry:\n    from urllib.parse import urlparse\nexcept ImportError:\n    from urlparse import urlparse\n\ntry:\n    from pathlib import Path\n    PYTORCH_PRETRAINED_BERT_CACHE = Path(\n        os.getenv('PYTORCH_PRETRAINED_BERT_CACHE', default_cache_path))\nexcept (AttributeError, ImportError):\n    PYTORCH_PRETRAINED_BERT_CACHE = os.getenv('PYTORCH_PRETRAINED_BERT_CACHE',\n                                              default_cache_path)\n\n\nimport logging\nfrom src.utils.comm import is_main_process\nlogger = logging.getLogger(__name__)  # pylint: disable=invalid-name\nif not is_main_process():\n    logger.disabled = True\n\ndef url_to_filename(url, etag=None):\n    \"\"\"\n    Convert `url` into a hashed filename in a repeatable way.\n    If `etag` is specified, append its hash to the url's, delimited\n    by a period.\n    \"\"\"\n    url_bytes = url.encode('utf-8')\n    url_hash = sha256(url_bytes)\n    filename = url_hash.hexdigest()\n\n    if etag:\n        etag_bytes = etag.encode('utf-8')\n        etag_hash = sha256(etag_bytes)\n        filename += '.' + etag_hash.hexdigest()\n\n    return filename\n\n\ndef filename_to_url(filename, cache_dir=None):\n    \"\"\"\n    Return the url and etag (which may be ``None``) stored for `filename`.\n    Raise ``EnvironmentError`` if `filename` or its stored metadata do not exist.\n    \"\"\"\n    if cache_dir is None:\n        cache_dir = PYTORCH_PRETRAINED_BERT_CACHE\n    if sys.version_info[0] == 3 and isinstance(cache_dir, Path):\n        cache_dir = str(cache_dir)\n\n    cache_path = os.path.join(cache_dir, filename)\n    if not os.path.exists(cache_path):\n        raise EnvironmentError(\"file {} not found\".format(cache_path))\n\n    meta_path = cache_path + '.json'\n    if not os.path.exists(meta_path):\n        raise EnvironmentError(\"file {} not found\".format(meta_path))\n\n    with open(meta_path, encoding=\"utf-8\") as meta_file:\n        metadata = json.load(meta_file)\n    url = metadata['url']\n    etag = metadata['etag']\n\n    return url, etag\n\n\ndef cached_path(url_or_filename, cache_dir=None):\n    \"\"\"\n    Given something that might be a URL (or might be a local path),\n    determine which. If it's a URL, download the file and cache it, and\n    return the path to the cached file. If it's already a local path,\n    make sure the file exists and then return the path.\n    \"\"\"\n    if cache_dir is None:\n        cache_dir = PYTORCH_PRETRAINED_BERT_CACHE\n    if sys.version_info[0] == 3 and isinstance(url_or_filename, Path):\n        url_or_filename = str(url_or_filename)\n    if sys.version_info[0] == 3 and isinstance(cache_dir, Path):\n        cache_dir = str(cache_dir)\n\n    parsed = urlparse(url_or_filename)\n\n    if parsed.scheme in ('http', 'https', 's3'):\n        # URL, so get it from the cache (downloading if necessary)\n        return get_from_cache(url_or_filename, cache_dir)\n    elif os.path.exists(url_or_filename):\n        # File, and it exists.\n        return url_or_filename\n    elif parsed.scheme == '':\n        # File, but it doesn't exist.\n        raise EnvironmentError(\"file {} not found\".format(url_or_filename))\n    else:\n        # Something unknown\n        raise ValueError(\"unable to parse {} as a URL or as a local path\".format(url_or_filename))\n\n\ndef split_s3_path(url):\n    \"\"\"Split a full s3 path into the bucket name and path.\"\"\"\n    parsed = urlparse(url)\n    if not parsed.netloc or not parsed.path:\n        raise ValueError(\"bad s3 path {}\".format(url))\n    bucket_name = parsed.netloc\n    s3_path = parsed.path\n    # Remove '/' at beginning of path.\n    if s3_path.startswith(\"/\"):\n        s3_path = s3_path[1:]\n    return bucket_name, s3_path\n\n\ndef s3_request(func):\n    \"\"\"\n    Wrapper function for s3 requests in order to create more helpful error\n    messages.\n    \"\"\"\n\n    @wraps(func)\n    def wrapper(url, *args, **kwargs):\n        try:\n            return func(url, *args, **kwargs)\n        except ClientError as exc:\n            if int(exc.response[\"Error\"][\"Code\"]) == 404:\n                raise EnvironmentError(\"file {} not found\".format(url))\n            else:\n                raise\n\n    return wrapper\n\n\n@s3_request\ndef s3_etag(url):\n    \"\"\"Check ETag on S3 object.\"\"\"\n    s3_resource = boto3.resource(\"s3\")\n    bucket_name, s3_path = split_s3_path(url)\n    s3_object = s3_resource.Object(bucket_name, s3_path)\n    return s3_object.e_tag\n\n\n@s3_request\ndef s3_get(url, temp_file):\n    \"\"\"Pull a file directly from S3.\"\"\"\n    s3_resource = boto3.resource(\"s3\")\n    bucket_name, s3_path = split_s3_path(url)\n    s3_resource.Bucket(bucket_name).download_fileobj(s3_path, temp_file)\n\n\ndef http_get(url, temp_file):\n    req = requests.get(url, stream=True)\n    content_length = req.headers.get('Content-Length')\n    total = int(content_length) if content_length is not None else None\n    progress = tqdm(unit=\"B\", total=total)\n    for chunk in req.iter_content(chunk_size=1024):\n        if chunk: # filter out keep-alive new chunks\n            progress.update(len(chunk))\n            temp_file.write(chunk)\n    progress.close()\n\n\ndef get_from_cache(url, cache_dir=None):\n    \"\"\"\n    Given a URL, look for the corresponding dataset in the local cache.\n    If it's not there, download it. Then return the path to the cached file.\n    \"\"\"\n    if cache_dir is None:\n        cache_dir = PYTORCH_PRETRAINED_BERT_CACHE\n    if sys.version_info[0] == 3 and isinstance(cache_dir, Path):\n        cache_dir = str(cache_dir)\n    if sys.version_info[0] == 2 and not isinstance(cache_dir, str):\n        cache_dir = str(cache_dir)\n\n    if not os.path.exists(cache_dir):\n        os.makedirs(cache_dir)\n\n    # Get eTag to add to filename, if it exists.\n    if url.startswith(\"s3://\"):\n        etag = s3_etag(url)\n    else:\n        try:\n            response = requests.head(url, allow_redirects=True)\n            if response.status_code != 200:\n                etag = None\n            else:\n                etag = response.headers.get(\"ETag\")\n        except EnvironmentError:\n            etag = None\n\n    if sys.version_info[0] == 2 and etag is not None:\n        etag = etag.decode('utf-8')\n    filename = url_to_filename(url, etag)\n\n    # get cache path to put the file\n    cache_path = os.path.join(cache_dir, filename)\n\n    # If we don't have a connection (etag is None) and can't identify the file\n    # try to get the last downloaded one\n    if not os.path.exists(cache_path) and etag is None:\n        matching_files = fnmatch.filter(os.listdir(cache_dir), filename + '.*')\n        matching_files = list(filter(lambda s: not s.endswith('.json'), matching_files))\n        if matching_files:\n            cache_path = os.path.join(cache_dir, matching_files[-1])\n\n    if not os.path.exists(cache_path):\n        # Download to temporary file, then copy to cache dir once finished.\n        # Otherwise you get corrupt cache entries if the download gets interrupted.\n        with tempfile.NamedTemporaryFile() as temp_file:\n            logger.info(\"%s not found in cache, downloading to %s\", url, temp_file.name)\n\n            # GET file object\n            if url.startswith(\"s3://\"):\n                s3_get(url, temp_file)\n            else:\n                http_get(url, temp_file)\n\n            # we are copying the file before closing it, so flush to avoid truncation\n            temp_file.flush()\n            # shutil.copyfileobj() starts at the current position, so go to the start\n            temp_file.seek(0)\n\n            logger.info(\"copying %s to cache at %s\", temp_file.name, cache_path)\n            with open(cache_path, 'wb') as cache_file:\n                shutil.copyfileobj(temp_file, cache_file)\n\n            logger.info(\"creating metadata file for %s\", cache_path)\n            meta = {'url': url, 'etag': etag}\n            meta_path = cache_path + '.json'\n            with open(meta_path, 'w') as meta_file:\n                output_string = json.dumps(meta)\n                if sys.version_info[0] == 2 and isinstance(output_string, str):\n                    output_string = unicode(output_string, 'utf-8')  # The beauty of python 2\n                meta_file.write(output_string)\n\n            logger.info(\"removing temp file %s\", temp_file.name)\n\n    return cache_path\n"}
{"type": "source_file", "path": "src/prepro/create_image_frame_tsv.py", "content": "import os.path as op\nimport json\nimport os\nimport sys\nfrom pathlib import Path    \nimport argparse\nfrom tqdm import tqdm\nimport numpy as np\nimport multiprocessing as mp\npythonpath = os.path.abspath(\n    os.path.dirname(os.path.dirname(__file__)))\nprint(pythonpath)\nsys.path.insert(0, pythonpath)\nfrom PIL import Image\nimport io\nimport base64\nimport cv2\n\ndef ensure_directory(path):\n    if path == '' or path == '.':\n        return\n    if path != None and len(path) > 0:\n        assert not op.isfile(path), '{} is a file'.format(path)\n        if not os.path.exists(path) and not op.islink(path):\n            try:\n                os.makedirs(path)\n            except:\n                if os.path.isdir(path):\n                    # another process has done makedir\n                    pass\n                else:\n                    raise\n\ndef tsv_writer(values, tsv_file_name, sep='\\t'):\n    ensure_directory(os.path.dirname(tsv_file_name))\n    tsv_lineidx_file = os.path.splitext(tsv_file_name)[0] + '.lineidx'\n    tsv_8b_file = tsv_lineidx_file + '.8b'\n    idx = 0\n    tsv_file_name_tmp = tsv_file_name + '.tmp'\n    tsv_lineidx_file_tmp = tsv_lineidx_file + '.tmp'\n    tsv_8b_file_tmp = tsv_8b_file + '.tmp'\n    import sys\n    is_py2 = sys.version_info.major == 2\n    if not is_py2:\n        sep = sep.encode()\n    with open(tsv_file_name_tmp, 'wb') as fp, open(tsv_lineidx_file_tmp, 'w') as fpidx, open(tsv_8b_file_tmp, 'wb') as fp8b:\n        assert values is not None\n        for value in values:\n            assert value is not None\n            if is_py2:\n                v = sep.join(map(lambda v: v.encode('utf-8') if isinstance(v, unicode) else str(v), value)) + '\\n'\n            else:\n                value = map(lambda v: v if type(v) == bytes else str(v).encode(),\n                        value)\n                v = sep.join(value) + b'\\n'\n            fp.write(v)\n            fpidx.write(str(idx) + '\\n')\n            # although we can use sys.byteorder to retrieve the system-default\n            # byte order, let's use little always to make it consistent and\n            # simple\n            fp8b.write(idx.to_bytes(8, 'little'))\n            idx = idx + len(v)\n    # the following might crash if there are two processes which are writing at\n    # the same time. One process finishes the renaming first and the second one\n    # will crash. In this case, we know there must be some errors when you run\n    # the code, and it should be a bug to fix rather than to use try-catch to\n    # protect it here.\n    os.rename(tsv_file_name_tmp, tsv_file_name)\n    os.rename(tsv_lineidx_file_tmp, tsv_lineidx_file)\n    os.rename(tsv_8b_file_tmp, tsv_8b_file)\n\ndef resize_and_to_binary(img_path, target_image_size):\n    if img_path is None:\n        if target_image_size < 0:\n            target_image_size = 256\n        resized = np.zeros((target_image_size, target_image_size, 3), dtype = \"uint8\")\n        s = (target_image_size, target_image_size)\n    else:\n        # im = Image.open(img_path)\n        im = cv2.imread(img_path)\n        height, width, channels = im.shape\n        s = (width, height)\n        if target_image_size > 0:\n            s = min(width, height)\n                \n            r = target_image_size / s\n            s = (round(r * width), round(r * height))\n            # im = im.resize(s)\n            resized = cv2.resize(im, s)\n        else:\n            resized = im\n\n    # binary = io.BytesIO()\n    # im.save(binary, format='JPEG')\n    # binary = binary.getvalue()\n    binary = cv2.imencode('.jpg', resized)[1].tobytes()\n    encoded_base64 = base64.b64encode(binary)\n    return encoded_base64, s\n\n\ndef load_tsv_to_mem(tsv_file, sep='\\t'):\n    data = []\n    with open(tsv_file, 'r') as fp:\n        for _, line in enumerate(fp):\n            data.append([x.strip() for x in line.split(sep)])\n    return data\n\n\ndef get_image_binaries(list_of_paths, image_size=56):\n    batch = []\n    is_None = [v is None for v in list_of_paths]\n    assert not any(is_None) or all(is_None)\n    for img_path in list_of_paths:\n        if img_path is None or isinstance(img_path, str):\n            x, shape = resize_and_to_binary(img_path, target_image_size=image_size)\n        else:\n            raise ValueError(f'img_path not str, but {type(img_path)}')\n        batch.append(x)\n    return batch, shape\n\n\ndef prepare_single_video_frames(caption_id, vid_path, num_frames=32):\n    previous_image_path = None\n    images = []\n    local_data_path = vid_path.replace(\"datasets\", \"_datasets\")\n    if not op.exists(local_data_path) and not op.exists(vid_path):\n        # print(f'{vid_path} does not exists')\n        images = [None]*num_frames\n        return None\n\n    video_id = Path(vid_path).stem\n    for i in range(num_frames):\n        current_image_path = op.join(data_path, str(caption_id).zfill(5), f'{video_id}_frame{(i+1):04d}.jpg')\n        if not op.exists(current_image_path):\n            print(f'{current_image_path} does not exists')\n            exit()\n            if previous_image_path:\n                current_image_path = previous_image_path \n            else:\n                print(f'The first image for {video_id} does not exists')\n                images = [None]*num_frames\n                return images\n        images.append(current_image_path)\n        previous_image_path = current_image_path\n    return images\n\n\ndef process_video_chunk(item, image_size=56, num_frames=32):\n    # line_items = []\n    # for item in items:\n    caption_id = item['id']\n    vid_path = '/data/hdd01/jinbu/BDDX/BDD-V/Videos/videos/' + item['vidName'] + '.mov'\n    \n    # vid_info_root = '/data/hdd01/jinbu/video_preprocess/code/data/processed/processed_video_info'\n    # vid_info_path = [os.path.join(vid_info_root, \"training_\"+item['vidName']+'_'+str(caption_id).zfill(5)+'.h5'), \n    #                  os.path.join(vid_info_root, \"validation_\"+item['vidName']+'_'+str(caption_id).zfill(5)+'.h5'), \n    #                  os.path.join(vid_info_root, \"testing_\"+item['vidName']+'_'+str(caption_id).zfill(5)+'.h5')]\n    # if not os.path.exists(vid_info_path[0]) and not os.path.exists(vid_info_path[0]) and not os.path.exists(vid_info_path[0]):\n    #     return None\n    \n    images = prepare_single_video_frames(caption_id, vid_path, num_frames)\n    if images == None:\n        return None\n    image_binaries, image_shape = get_image_binaries(images, image_size)\n    \n    resolved_data_vid_id = str(caption_id).zfill(5)+'/'+ item['vidName'] \n    line_item = [str(resolved_data_vid_id), json.dumps({\"class\": -1, \"width\": image_shape[0], \"height\": image_shape[1]})]\n    line_item += image_binaries\n    return line_item\n    #     line_items.append(line_item)\n    # return line_items\n\n\ndef main():\n    # FIXME: the tsv save path\n    output_folder = f\"/data/jinbu/BDD-V/video_preprocess/frame_tsv\"\n    os.makedirs(output_folder, exist_ok=True)\n    # To generate a tsv file:\n    # data_path: path to raw video files\n    global data_path\n\n    image_size = 256\n\n    num_frames = 32\n\n    # FIXME: the frame save dir generated by extract_frames.py\n    data_path = f\"/data/jinbu/BDD-V/data/{num_frames}frames/\"\n\n    # FIXME: if you want to debug, you can set the num_worker to 0\n    num_workers = 32\n\n    # FIXME: the caption annotation file\n    video_info_tsv = '/data/jinbu/BDD-V/data/processed/captions_BDDX.json'\n    \n    for split in ['training', 'validation', 'testing']:\n        if split == 'training':\n            data = json.load(open(video_info_tsv))['annotations'][:21143]\n        elif split == 'validation':\n            data = json.load(open(video_info_tsv))['annotations'][21143:23662]\n        elif split == 'testing':\n            data = json.load(open(video_info_tsv))['annotations'][23662:]\n        else:\n            exit(\"split error\")\n        # data = load_tsv_to_mem(f'datasets/{args.dataset}/{args.split}.img.tsv')\n\n        if num_workers > 0 :\n            resolved_visual_file = f\"{output_folder}/{split}_{num_frames}frames_img_size{image_size}.img.tsv\"\n            print(\"generating visual file for\", resolved_visual_file)\n\n            from functools import partial\n            worker = partial(\n                process_video_chunk, image_size=image_size, num_frames=num_frames)\n\n            def gen_rows():\n                with mp.Pool(num_workers) as pool, tqdm(total=len(data)) as pbar:\n                    for _, line_item in enumerate(\n                            pool.imap(worker, data, chunksize=8)):\n                        pbar.update(1)\n                        if line_item is not None:\n                            yield(line_item)\n\n            tsv_writer(gen_rows(), resolved_visual_file)\n        else:\n            for idx, d in tqdm(enumerate(data),\n                            total=len(data), desc=\"extracting frames from video\"):\n                process_video_chunk(d, image_size=image_size, num_frames=num_frames)\n\n\n\nif __name__ == '__main__':\n    main()\n\n\n"}
{"type": "source_file", "path": "src/modeling/video_swin/swin_transformer.py", "content": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.checkpoint as checkpoint\nimport numpy as np\nfrom src.timm.models.layers import DropPath, trunc_normal_\n\n# from mmcv.runner import load_checkpoint\n\nfrom functools import reduce, lru_cache\nfrom operator import mul\nfrom einops import rearrange\n\n\ndef _get_checkpoint_loader(cls, path):\n    \"\"\"Finds a loader that supports the given path. Falls back to the local\n    loader if no other loader is found.\n    Args:\n        path (str): checkpoint path\n    Returns:\n        loader (function): checkpoint loader\n    \"\"\"\n\n    for p in cls._schemes:\n        if path.startswith(p):\n            return cls._schemes[p]\n\ndef load_checkpoint(cls, filename, map_location=None):\n    \"\"\"load checkpoint through URL scheme path.\n    Args:\n        filename (str): checkpoint file name with given prefix\n        map_location (str, optional): Same as :func:`torch.load`.\n            Default: None\n        logger (:mod:`logging.Logger`, optional): The logger for message.\n            Default: None\n    Returns:\n        dict or OrderedDict: The loaded checkpoint.\n    \"\"\"\n\n    checkpoint_loader = cls._get_checkpoint_loader(filename)\n    class_name = checkpoint_loader.__name__\n    return checkpoint_loader(filename, map_location)\n\n\n\nclass Mlp(nn.Module):\n    \"\"\" Multilayer perceptron.\"\"\"\n\n    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.act = act_layer()\n        self.fc2 = nn.Linear(hidden_features, out_features)\n        self.drop = nn.Dropout(drop)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.drop(x)\n        x = self.fc2(x)\n        x = self.drop(x)\n        return x\n\n\ndef window_partition(x, window_size):\n    \"\"\"\n    Args:\n        x: (B, D, H, W, C)\n        window_size (tuple[int]): window size\n\n    Returns:\n        windows: (B*num_windows, window_size*window_size, C)\n    \"\"\"\n    B, D, H, W, C = x.shape\n    x = x.view(B, D // window_size[0], window_size[0], H // window_size[1], window_size[1], W // window_size[2], window_size[2], C)\n    windows = x.permute(0, 1, 3, 5, 2, 4, 6, 7).contiguous().view(-1, reduce(mul, window_size), C)\n    return windows\n\n\ndef window_reverse(windows, window_size, B, D, H, W):\n    \"\"\"\n    Args:\n        windows: (B*num_windows, window_size, window_size, C)\n        window_size (tuple[int]): Window size\n        H (int): Height of image\n        W (int): Width of image\n\n    Returns:\n        x: (B, D, H, W, C)\n    \"\"\"\n    x = windows.view(B, D // window_size[0], H // window_size[1], W // window_size[2], window_size[0], window_size[1], window_size[2], -1)\n    x = x.permute(0, 1, 4, 2, 5, 3, 6, 7).contiguous().view(B, D, H, W, -1)\n    return x\n\n\n\n\ndef get_window_size(x_size, window_size, shift_size=None):\n    use_window_size = list(window_size)\n    if shift_size is not None:\n        use_shift_size = list(shift_size)\n    for i in range(len(x_size)):\n        if x_size[i] <= window_size[i]:\n            use_window_size[i] = x_size[i]\n            if shift_size is not None:\n                use_shift_size[i] = 0\n\n    if shift_size is None:\n        return tuple(use_window_size)\n    else:\n        return tuple(use_window_size), tuple(use_shift_size)\n\n\nclass WindowAttention3D(nn.Module):\n    \"\"\" Window based multi-head self attention (W-MSA) module with relative position bias.\n    It supports both of shifted and non-shifted window.\n    Args:\n        dim (int): Number of input channels.\n        window_size (tuple[int]): The temporal length, height and width of the window.\n        num_heads (int): Number of attention heads.\n        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True\n        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set\n        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0\n        proj_drop (float, optional): Dropout ratio of output. Default: 0.0\n    \"\"\"\n\n    def __init__(self, dim, window_size, num_heads, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n\n        super().__init__()\n        self.dim = dim\n        self.window_size = window_size  # Wd, Wh, Ww\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        self.scale = qk_scale or head_dim ** -0.5\n\n        # define a parameter table of relative position bias\n        self.relative_position_bias_table = nn.Parameter(\n            torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1) * (2 * window_size[2] - 1), num_heads))  # 2*Wd-1 * 2*Wh-1 * 2*Ww-1, nH\n\n        # get pair-wise relative position index for each token inside the window\n        coords_d = torch.arange(self.window_size[0])\n        coords_h = torch.arange(self.window_size[1])\n        coords_w = torch.arange(self.window_size[2])\n        coords = torch.stack(torch.meshgrid(coords_d, coords_h, coords_w))  # 3, Wd, Wh, Ww\n        coords_flatten = torch.flatten(coords, 1)  # 3, Wd*Wh*Ww\n        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 3, Wd*Wh*Ww, Wd*Wh*Ww\n        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wd*Wh*Ww, Wd*Wh*Ww, 3\n        relative_coords[:, :, 0] += self.window_size[0] - 1  # shift to start from 0\n        relative_coords[:, :, 1] += self.window_size[1] - 1\n        relative_coords[:, :, 2] += self.window_size[2] - 1\n\n        relative_coords[:, :, 0] *= (2 * self.window_size[1] - 1) * (2 * self.window_size[2] - 1)\n        relative_coords[:, :, 1] *= (2 * self.window_size[2] - 1)\n        relative_position_index = relative_coords.sum(-1)  # Wd*Wh*Ww, Wd*Wh*Ww\n        self.register_buffer(\"relative_position_index\", relative_position_index)\n\n        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n\n        trunc_normal_(self.relative_position_bias_table, std=.02)\n        self.softmax = nn.Softmax(dim=-1)\n\n    def forward(self, x, mask=None):\n        \"\"\" Forward function.\n        Args:\n            x: input features with shape of (num_windows*B, N, C)\n            mask: (0/-inf) mask with shape of (num_windows, N, N) or None\n        \"\"\"\n        B_, N, C = x.shape\n        qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n        q, k, v = qkv[0], qkv[1], qkv[2]  # B_, nH, N, C\n\n        q = q * self.scale\n        attn = q @ k.transpose(-2, -1)\n\n        relative_position_bias = self.relative_position_bias_table[self.relative_position_index[:N, :N].reshape(-1)].reshape(\n            N, N, -1)  # Wd*Wh*Ww,Wd*Wh*Ww,nH\n        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wd*Wh*Ww, Wd*Wh*Ww\n        attn = attn + relative_position_bias.unsqueeze(0) # B_, nH, N, N\n\n        if mask is not None:\n            nW = mask.shape[0]\n            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)\n            attn = attn.view(-1, self.num_heads, N, N)\n            attn = self.softmax(attn)\n        else:\n            attn = self.softmax(attn)\n\n        attn = self.attn_drop(attn)\n\n        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x\n\n\nclass SwinTransformerBlock3D(nn.Module):\n    \"\"\" Swin Transformer Block.\n\n    Args:\n        dim (int): Number of input channels.\n        num_heads (int): Number of attention heads.\n        window_size (tuple[int]): Window size.\n        shift_size (tuple[int]): Shift size for SW-MSA.\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n        drop (float, optional): Dropout rate. Default: 0.0\n        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n        drop_path (float, optional): Stochastic depth rate. Default: 0.0\n        act_layer (nn.Module, optional): Activation layer. Default: nn.GELU\n        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n    \"\"\"\n\n    def __init__(self, dim, num_heads, window_size=(2,7,7), shift_size=(0,0,0),\n                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0., drop_path=0.,\n                 act_layer=nn.GELU, norm_layer=nn.LayerNorm, use_checkpoint=False):\n        super().__init__()\n        self.dim = dim\n        self.num_heads = num_heads\n        self.window_size = window_size\n        self.shift_size = shift_size\n        self.mlp_ratio = mlp_ratio\n        self.use_checkpoint=use_checkpoint\n\n        assert 0 <= self.shift_size[0] < self.window_size[0], \"shift_size must in 0-window_size\"\n        assert 0 <= self.shift_size[1] < self.window_size[1], \"shift_size must in 0-window_size\"\n        assert 0 <= self.shift_size[2] < self.window_size[2], \"shift_size must in 0-window_size\"\n\n        self.norm1 = norm_layer(dim)\n        self.attn = WindowAttention3D(\n            dim, window_size=self.window_size, num_heads=num_heads,\n            qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n\n        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n        self.norm2 = norm_layer(dim)\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n\n    def forward_part1(self, x, mask_matrix):\n        B, D, H, W, C = x.shape\n        window_size, shift_size = get_window_size((D, H, W), self.window_size, self.shift_size)\n\n        x = self.norm1(x)\n        # pad feature maps to multiples of window size\n        pad_l = pad_t = pad_d0 = 0\n        pad_d1 = (window_size[0] - D % window_size[0]) % window_size[0]\n        pad_b = (window_size[1] - H % window_size[1]) % window_size[1]\n        pad_r = (window_size[2] - W % window_size[2]) % window_size[2]\n        x = F.pad(x, (0, 0, pad_l, pad_r, pad_t, pad_b, pad_d0, pad_d1))\n        _, Dp, Hp, Wp, _ = x.shape\n        # cyclic shift\n        if any(i > 0 for i in shift_size):\n            shifted_x = torch.roll(x, shifts=(-shift_size[0], -shift_size[1], -shift_size[2]), dims=(1, 2, 3))\n            attn_mask = mask_matrix\n        else:\n            shifted_x = x\n            attn_mask = None\n        # partition windows\n        x_windows = window_partition(shifted_x, window_size)  # B*nW, Wd*Wh*Ww, C\n        # W-MSA/SW-MSA\n        attn_windows = self.attn(x_windows, mask=attn_mask)  # B*nW, Wd*Wh*Ww, C\n        # merge windows\n        attn_windows = attn_windows.view(-1, *(window_size+(C,)))\n        shifted_x = window_reverse(attn_windows, window_size, B, Dp, Hp, Wp)  # B D' H' W' C\n        # reverse cyclic shift\n        if any(i > 0 for i in shift_size):\n            x = torch.roll(shifted_x, shifts=(shift_size[0], shift_size[1], shift_size[2]), dims=(1, 2, 3))\n        else:\n            x = shifted_x\n\n        if pad_d1 >0 or pad_r > 0 or pad_b > 0:\n            x = x[:, :D, :H, :W, :].contiguous()\n        return x\n\n    def forward_part2(self, x):\n        return self.drop_path(self.mlp(self.norm2(x)))\n\n    def forward(self, x, mask_matrix):\n        \"\"\" Forward function.\n\n        Args:\n            x: Input feature, tensor size (B, D, H, W, C).\n            mask_matrix: Attention mask for cyclic shift.\n        \"\"\"\n\n        shortcut = x\n        if self.use_checkpoint:\n            x = checkpoint.checkpoint(self.forward_part1, x, mask_matrix)\n        else:\n            x = self.forward_part1(x, mask_matrix)\n        x = shortcut + self.drop_path(x)\n\n        if self.use_checkpoint:\n            x = x + checkpoint.checkpoint(self.forward_part2, x)\n        else:\n            x = x + self.forward_part2(x)\n\n        return x\n\n\nclass PatchMerging(nn.Module):\n    \"\"\" Patch Merging Layer\n\n    Args:\n        dim (int): Number of input channels.\n        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n    \"\"\"\n    def __init__(self, dim, norm_layer=nn.LayerNorm):\n        super().__init__()\n        self.dim = dim\n        self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)\n        self.norm = norm_layer(4 * dim)\n\n    def forward(self, x):\n        \"\"\" Forward function.\n\n        Args:\n            x: Input feature, tensor size (B, D, H, W, C).\n        \"\"\"\n        B, D, H, W, C = x.shape\n\n        # padding\n        pad_input = (H % 2 == 1) or (W % 2 == 1)\n        if pad_input:\n            x = F.pad(x, (0, 0, 0, W % 2, 0, H % 2))\n\n        x0 = x[:, :, 0::2, 0::2, :]  # B D H/2 W/2 C\n        x1 = x[:, :, 1::2, 0::2, :]  # B D H/2 W/2 C\n        x2 = x[:, :, 0::2, 1::2, :]  # B D H/2 W/2 C\n        x3 = x[:, :, 1::2, 1::2, :]  # B D H/2 W/2 C\n        x = torch.cat([x0, x1, x2, x3], -1)  # B D H/2 W/2 4*C\n\n        x = self.norm(x)\n        x = self.reduction(x)\n\n        return x\n\n\n# cache each stage results\n@lru_cache()\ndef compute_mask(D, H, W, window_size, shift_size, device):\n    img_mask = torch.zeros((1, D, H, W, 1), device=device)  # 1 Dp Hp Wp 1\n    cnt = 0\n    for d in slice(-window_size[0]), slice(-window_size[0], -shift_size[0]), slice(-shift_size[0],None):\n        for h in slice(-window_size[1]), slice(-window_size[1], -shift_size[1]), slice(-shift_size[1],None):\n            for w in slice(-window_size[2]), slice(-window_size[2], -shift_size[2]), slice(-shift_size[2],None):\n                img_mask[:, d, h, w, :] = cnt\n                cnt += 1\n    mask_windows = window_partition(img_mask, window_size)  # nW, ws[0]*ws[1]*ws[2], 1\n    mask_windows = mask_windows.squeeze(-1)  # nW, ws[0]*ws[1]*ws[2]\n    attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n    attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))\n    return attn_mask\n\n\nclass BasicLayer(nn.Module):\n    \"\"\" A basic Swin Transformer layer for one stage.\n\n    Args:\n        dim (int): Number of feature channels\n        depth (int): Depths of this stage.\n        num_heads (int): Number of attention head.\n        window_size (tuple[int]): Local window size. Default: (1,7,7).\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4.\n        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n        drop (float, optional): Dropout rate. Default: 0.0\n        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0\n        norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm\n        downsample (nn.Module | None, optional): Downsample layer at the end of the layer. Default: None\n    \"\"\"\n\n    def __init__(self,\n                 dim,\n                 depth,\n                 num_heads,\n                 window_size=(1,7,7),\n                 mlp_ratio=4.,\n                 qkv_bias=False,\n                 qk_scale=None,\n                 drop=0.,\n                 attn_drop=0.,\n                 drop_path=0.,\n                 norm_layer=nn.LayerNorm,\n                 downsample=None,\n                 use_checkpoint=False):\n        super().__init__()\n        self.window_size = window_size\n        self.shift_size = tuple(i // 2 for i in window_size)\n        self.depth = depth\n        self.use_checkpoint = use_checkpoint\n\n        # build blocks\n        self.blocks = nn.ModuleList([\n            SwinTransformerBlock3D(\n                dim=dim,\n                num_heads=num_heads,\n                window_size=window_size,\n                shift_size=(0,0,0) if (i % 2 == 0) else self.shift_size,\n                mlp_ratio=mlp_ratio,\n                qkv_bias=qkv_bias,\n                qk_scale=qk_scale,\n                drop=drop,\n                attn_drop=attn_drop,\n                drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,\n                norm_layer=norm_layer,\n                use_checkpoint=use_checkpoint,\n            )\n            for i in range(depth)])\n        \n        self.downsample = downsample\n        if self.downsample is not None:\n            self.downsample = downsample(dim=dim, norm_layer=norm_layer)\n\n    def forward(self, x):\n        \"\"\" Forward function.\n\n        Args:\n            x: Input feature, tensor size (B, C, D, H, W).\n        \"\"\"\n        # calculate attention mask for SW-MSA\n        B, C, D, H, W = x.shape\n        window_size, shift_size = get_window_size((D,H,W), self.window_size, self.shift_size)\n        x = rearrange(x, 'b c d h w -> b d h w c')\n        Dp = int(np.ceil(D / window_size[0])) * window_size[0]\n        Hp = int(np.ceil(H / window_size[1])) * window_size[1]\n        Wp = int(np.ceil(W / window_size[2])) * window_size[2]\n        attn_mask = compute_mask(Dp, Hp, Wp, window_size, shift_size, x.device)\n        for blk in self.blocks:\n            # safeguard fp16\n            attn_mask = attn_mask.to(dtype=x.dtype)\n            x = blk(x, attn_mask)\n        x = x.view(B, D, H, W, -1)\n\n        if self.downsample is not None:\n            x = self.downsample(x)\n        x = rearrange(x, 'b d h w c -> b c d h w')\n        return x\n\n\nclass PatchEmbed3D(nn.Module):\n    \"\"\" Video to Patch Embedding.\n\n    Args:\n        patch_size (int): Patch token size. Default: (2,4,4).\n        in_chans (int): Number of input video channels. Default: 3.\n        embed_dim (int): Number of linear projection output channels. Default: 96.\n        norm_layer (nn.Module, optional): Normalization layer. Default: None\n    \"\"\"\n    def __init__(self, patch_size=(2,4,4), in_chans=3, embed_dim=96, norm_layer=None):\n        super().__init__()\n        self.patch_size = patch_size\n\n        self.in_chans = in_chans\n        self.embed_dim = embed_dim\n\n        self.proj = nn.Conv3d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n        if norm_layer is not None:\n            self.norm = norm_layer(embed_dim)\n        else:\n            self.norm = None\n\n    def forward(self, x):\n        \"\"\"Forward function.\"\"\"\n        # padding\n        _, _, D, H, W = x.size()\n        if W % self.patch_size[2] != 0:\n            x = F.pad(x, (0, self.patch_size[2] - W % self.patch_size[2]))\n        if H % self.patch_size[1] != 0:\n            x = F.pad(x, (0, 0, 0, self.patch_size[1] - H % self.patch_size[1]))\n        if D % self.patch_size[0] != 0:\n            x = F.pad(x, (0, 0, 0, 0, 0, self.patch_size[0] - D % self.patch_size[0]))\n\n        x = self.proj(x)  # B C D Wh Ww\n        if self.norm is not None:\n            D, Wh, Ww = x.size(2), x.size(3), x.size(4)\n            x = x.flatten(2).transpose(1, 2)\n            x = self.norm(x)\n            x = x.transpose(1, 2).view(-1, self.embed_dim, D, Wh, Ww)\n\n        return x\n\nclass SwinTransformer3D(nn.Module):\n    \"\"\" Swin Transformer backbone.\n        A PyTorch impl of : `Swin Transformer: Hierarchical Vision Transformer using Shifted Windows`  -\n          https://arxiv.org/pdf/2103.14030\n\n    Args:\n        patch_size (int | tuple(int)): Patch size. Default: (4,4,4).\n        in_chans (int): Number of input image channels. Default: 3.\n        embed_dim (int): Number of linear projection output channels. Default: 96.\n        depths (tuple[int]): Depths of each Swin Transformer stage.\n        num_heads (tuple[int]): Number of attention head of each stage.\n        window_size (int): Window size. Default: 7.\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4.\n        qkv_bias (bool): If True, add a learnable bias to query, key, value. Default: Truee\n        qk_scale (float): Override default qk scale of head_dim ** -0.5 if set.\n        drop_rate (float): Dropout rate.\n        attn_drop_rate (float): Attention dropout rate. Default: 0.\n        drop_path_rate (float): Stochastic depth rate. Default: 0.2.\n        norm_layer: Normalization layer. Default: nn.LayerNorm.\n        patch_norm (bool): If True, add normalization after patch embedding. Default: False.\n        frozen_stages (int): Stages to be frozen (stop grad and set eval mode).\n            -1 means not freezing any parameters.\n    \"\"\"\n\n    def __init__(self,\n                 pretrained=None,\n                 pretrained2d=False,\n                 patch_size=(4,4,4),\n                 in_chans=3,\n                 embed_dim=96,\n                 depths=[2, 2, 6, 2],\n                 num_heads=[3, 6, 12, 24],\n                 window_size=(2,7,7),\n                 mlp_ratio=4.,\n                 qkv_bias=True,\n                 qk_scale=None,\n                 drop_rate=0.,\n                 attn_drop_rate=0.,\n                 drop_path_rate=0.2,\n                 norm_layer=nn.LayerNorm,\n                 patch_norm=False,\n                 frozen_stages=-1,\n                 use_checkpoint=False):\n        super().__init__()\n\n        self.pretrained = pretrained\n        self.pretrained2d = pretrained2d\n        self.num_layers = len(depths)\n        self.embed_dim = embed_dim\n        self.patch_norm = patch_norm\n        self.frozen_stages = frozen_stages\n        self.window_size = window_size\n        self.patch_size = patch_size\n\n        # split image into non-overlapping patches\n        self.patch_embed = PatchEmbed3D(\n            patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim,\n            norm_layer=norm_layer if self.patch_norm else None)\n\n        self.pos_drop = nn.Dropout(p=drop_rate)\n\n        # stochastic depth\n        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]  # stochastic depth decay rule\n\n        # build layers\n        self.layers = nn.ModuleList()\n        for i_layer in range(self.num_layers):\n            layer = BasicLayer(\n                dim=int(embed_dim * 2**i_layer),\n                depth=depths[i_layer],\n                num_heads=num_heads[i_layer],\n                window_size=window_size,\n                mlp_ratio=mlp_ratio,\n                qkv_bias=qkv_bias,\n                qk_scale=qk_scale,\n                drop=drop_rate,\n                attn_drop=attn_drop_rate,\n                drop_path=dpr[sum(depths[:i_layer]):sum(depths[:i_layer + 1])],\n                norm_layer=norm_layer,\n                downsample=PatchMerging if i_layer<self.num_layers-1 else None,\n                use_checkpoint=use_checkpoint)\n            self.layers.append(layer)\n\n        self.num_features = int(embed_dim * 2**(self.num_layers-1))\n\n        # add a norm layer for each output\n        self.norm = norm_layer(self.num_features)\n\n        self._freeze_stages()\n\n    def _freeze_stages(self):\n        if self.frozen_stages >= 0:\n            self.patch_embed.eval()\n            for param in self.patch_embed.parameters():\n                param.requires_grad = False\n\n        if self.frozen_stages >= 1:\n            self.pos_drop.eval()\n            for i in range(0, self.frozen_stages):\n                m = self.layers[i]\n                m.eval()\n                for param in m.parameters():\n                    param.requires_grad = False\n\n    def inflate_weights(self):\n        \"\"\"Inflate the swin2d parameters to swin3d.\n\n        The differences between swin3d and swin2d mainly lie in an extra\n        axis. To utilize the pretrained parameters in 2d model,\n        the weight of swin2d models should be inflated to fit in the shapes of\n        the 3d counterpart.\n\n        Args:\n            logger (logging.Logger): The logger used to print\n                debugging infomation.\n        \"\"\"\n        checkpoint = torch.load(self.pretrained, map_location='cpu')\n        state_dict = checkpoint['model']\n\n        # delete relative_position_index since we always re-init it\n        relative_position_index_keys = [k for k in state_dict.keys() if \"relative_position_index\" in k]\n        for k in relative_position_index_keys:\n            del state_dict[k]\n\n        # delete attn_mask since we always re-init it\n        attn_mask_keys = [k for k in state_dict.keys() if \"attn_mask\" in k]\n        for k in attn_mask_keys:\n            del state_dict[k]\n\n        state_dict['patch_embed.proj.weight'] = state_dict['patch_embed.proj.weight'].unsqueeze(2).repeat(1,1,self.patch_size[0],1,1) / self.patch_size[0]\n\n        # bicubic interpolate relative_position_bias_table if not match\n        relative_position_bias_table_keys = [k for k in state_dict.keys() if \"relative_position_bias_table\" in k]\n        for k in relative_position_bias_table_keys:\n            relative_position_bias_table_pretrained = state_dict[k]\n            relative_position_bias_table_current = self.state_dict()[k]\n            L1, nH1 = relative_position_bias_table_pretrained.size()\n            L2, nH2 = relative_position_bias_table_current.size()\n            L2 = (2*self.window_size[1]-1) * (2*self.window_size[2]-1)\n            wd = self.window_size[0]\n            if nH1 != nH2:\n                print(f\"Error in loading {k}, passing\")\n            else:\n                if L1 != L2:\n                    S1 = int(L1 ** 0.5)\n                    relative_position_bias_table_pretrained_resized = torch.nn.functional.interpolate(\n                        relative_position_bias_table_pretrained.permute(1, 0).view(1, nH1, S1, S1), size=(2*self.window_size[1]-1, 2*self.window_size[2]-1),\n                        mode='bicubic')\n                    relative_position_bias_table_pretrained = relative_position_bias_table_pretrained_resized.view(nH2, L2).permute(1, 0)\n            state_dict[k] = relative_position_bias_table_pretrained.repeat(2*wd-1,1)\n\n        msg = self.load_state_dict(state_dict, strict=False)\n        print(msg)\n        print(f\"=> loaded successfully '{self.pretrained}'\")\n        del checkpoint\n        torch.cuda.empty_cache()\n\n    def init_weights(self, pretrained=None):\n        \"\"\"Initialize the weights in backbone.\n\n        Args:\n            pretrained (str, optional): Path to pre-trained weights.\n                Defaults to None.\n        \"\"\"\n        def _init_weights(m):\n            if isinstance(m, nn.Linear):\n                trunc_normal_(m.weight, std=.02)\n                if isinstance(m, nn.Linear) and m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.LayerNorm):\n                nn.init.constant_(m.bias, 0)\n                nn.init.constant_(m.weight, 1.0)\n\n        if pretrained:\n            self.pretrained = pretrained\n        if isinstance(self.pretrained, str):\n            self.apply(_init_weights)\n            print(f'load model from: {self.pretrained}')\n\n            if self.pretrained2d:\n                print('Inflate 2D model into 3D model.')\n                # Inflate 2D model into 3D model.\n                self.inflate_weights()\n            else:\n                print('Directly load 3D model')\n                # Directly load 3D model.\n                load_checkpoint(self, self.pretrained, map_location=False)\n        elif self.pretrained is None:\n            self.apply(_init_weights)\n        else:\n            raise TypeError('pretrained must be a str or None')\n\n    def forward(self, x):\n        \"\"\"Forward function.\"\"\"\n        x = self.patch_embed(x)\n\n        x = self.pos_drop(x)\n\n        for layer in self.layers:\n            x = layer(x.contiguous())\n\n        x = rearrange(x, 'n c d h w -> n d h w c')\n        x = self.norm(x)\n        x = rearrange(x, 'n d h w c -> n c d h w')\n\n        return x\n\n    def train(self, mode=True):\n        \"\"\"Convert the model into training mode while keep layers freezed.\"\"\"\n        super(SwinTransformer3D, self).train(mode)\n        self._freeze_stages()\n\n"}
{"type": "source_file", "path": "src/pytorch_grad_cam/ablation_cam.py", "content": "import numpy as np\r\nimport torch\r\nimport tqdm\r\nfrom typing import Callable, List\r\nfrom .base_cam import BaseCAM\r\nfrom .utils.find_layers import replace_layer_recursive\r\nfrom .ablation_layer import AblationLayer\r\n\r\n\r\n\"\"\" Implementation of AblationCAM\r\nhttps://openaccess.thecvf.com/content_WACV_2020/papers/Desai_Ablation-CAM_Visual_Explanations_for_Deep_Convolutional_Network_via_Gradient-free_Localization_WACV_2020_paper.pdf\r\n\r\nAblate individual activations, and then measure the drop in the target score.\r\n\r\nIn the current implementation, the target layer activations is cached, so it won't be re-computed.\r\nHowever layers before it, if any, will not be cached.\r\nThis means that if the target layer is a large block, for example model.featuers (in vgg), there will\r\nbe a large save in run time.\r\n\r\nSince we have to go over many channels and ablate them, and every channel ablation requires a forward pass,\r\nit would be nice if we could avoid doing that for channels that won't contribute anwyay, making it much faster.\r\nThe parameter ratio_channels_to_ablate controls how many channels should be ablated, using an experimental method\r\n(to be improved). The default 1.0 value means that all channels will be ablated.\r\n\"\"\"\r\n\r\n\r\nclass AblationCAM(BaseCAM):\r\n    def __init__(self,\r\n                 model: torch.nn.Module,\r\n                 target_layers: List[torch.nn.Module],\r\n                 use_cuda: bool = False,\r\n                 reshape_transform: Callable = None,\r\n                 ablation_layer: torch.nn.Module = AblationLayer(),\r\n                 batch_size: int = 32,\r\n                 ratio_channels_to_ablate: float = 1.0) -> None:\r\n\r\n        super(AblationCAM, self).__init__(model,\r\n                                          target_layers,\r\n                                          use_cuda,\r\n                                          reshape_transform,\r\n                                          uses_gradients=False)\r\n        self.batch_size = batch_size\r\n        self.ablation_layer = ablation_layer\r\n        self.ratio_channels_to_ablate = ratio_channels_to_ablate\r\n\r\n    def save_activation(self, module, input, output) -> None:\r\n        \"\"\" Helper function to save the raw activations from the target layer \"\"\"\r\n        self.activations = output\r\n\r\n    def assemble_ablation_scores(self,\r\n                                 new_scores: list,\r\n                                 original_score: float,\r\n                                 ablated_channels: np.ndarray,\r\n                                 number_of_channels: int) -> np.ndarray:\r\n        \"\"\" Take the value from the channels that were ablated,\r\n            and just set the original score for the channels that were skipped \"\"\"\r\n\r\n        index = 0\r\n        result = []\r\n        sorted_indices = np.argsort(ablated_channels)\r\n        ablated_channels = ablated_channels[sorted_indices]\r\n        new_scores = np.float32(new_scores)[sorted_indices]\r\n\r\n        for i in range(number_of_channels):\r\n            if index < len(ablated_channels) and ablated_channels[index] == i:\r\n                weight = new_scores[index]\r\n                index = index + 1\r\n            else:\r\n                weight = original_score\r\n            result.append(weight)\r\n\r\n        return result\r\n\r\n    def get_cam_weights(self,\r\n                        input_tensor: torch.Tensor,\r\n                        target_layer: torch.nn.Module,\r\n                        targets: List[Callable],\r\n                        activations: torch.Tensor,\r\n                        grads: torch.Tensor) -> np.ndarray:\r\n\r\n        # Do a forward pass, compute the target scores, and cache the\r\n        # activations\r\n        handle = target_layer.register_forward_hook(self.save_activation)\r\n        with torch.no_grad():\r\n            outputs = self.model(input_tensor)\r\n            handle.remove()\r\n            original_scores = np.float32(\r\n                [target(output).cpu().item() for target, output in zip(targets, outputs)])\r\n\r\n        # Replace the layer with the ablation layer.\r\n        # When we finish, we will replace it back, so the original model is\r\n        # unchanged.\r\n        ablation_layer = self.ablation_layer\r\n        replace_layer_recursive(self.model, target_layer, ablation_layer)\r\n\r\n        number_of_channels = activations.shape[1]\r\n        weights = []\r\n        # This is a \"gradient free\" method, so we don't need gradients here.\r\n        with torch.no_grad():\r\n            # Loop over each of the batch images and ablate activations for it.\r\n            for batch_index, (target, tensor) in enumerate(\r\n                    zip(targets, input_tensor)):\r\n                new_scores = []\r\n                batch_tensor = tensor.repeat(self.batch_size, 1, 1, 1)\r\n\r\n                # Check which channels should be ablated. Normally this will be all channels,\r\n                # But we can also try to speed this up by using a low\r\n                # ratio_channels_to_ablate.\r\n                channels_to_ablate = ablation_layer.activations_to_be_ablated(\r\n                    activations[batch_index, :], self.ratio_channels_to_ablate)\r\n                number_channels_to_ablate = len(channels_to_ablate)\r\n\r\n                for i in tqdm.tqdm(\r\n                    range(\r\n                        0,\r\n                        number_channels_to_ablate,\r\n                        self.batch_size)):\r\n                    if i + self.batch_size > number_channels_to_ablate:\r\n                        batch_tensor = batch_tensor[:(\r\n                            number_channels_to_ablate - i)]\r\n\r\n                    # Change the state of the ablation layer so it ablates the next channels.\r\n                    # TBD: Move this into the ablation layer forward pass.\r\n                    ablation_layer.set_next_batch(\r\n                        input_batch_index=batch_index,\r\n                        activations=self.activations,\r\n                        num_channels_to_ablate=batch_tensor.size(0))\r\n                    score = [target(o).cpu().item()\r\n                             for o in self.model(batch_tensor)]\r\n                    new_scores.extend(score)\r\n                    ablation_layer.indices = ablation_layer.indices[batch_tensor.size(\r\n                        0):]\r\n\r\n                new_scores = self.assemble_ablation_scores(\r\n                    new_scores,\r\n                    original_scores[batch_index],\r\n                    channels_to_ablate,\r\n                    number_of_channels)\r\n                weights.extend(new_scores)\r\n\r\n        weights = np.float32(weights)\r\n        weights = weights.reshape(activations.shape[:2])\r\n        original_scores = original_scores[:, None]\r\n        weights = (original_scores - weights) / original_scores\r\n\r\n        # Replace the model back to the original state\r\n        replace_layer_recursive(self.model, ablation_layer, target_layer)\r\n        return weights\r\n"}
{"type": "source_file", "path": "src/modeling/video_swin/swin_tiny.py", "content": "# model settings\nmodel = dict(\n    type='Recognizer3D',\n    backbone=dict(\n        type='SwinTransformer3D',\n        patch_size=(4,4,4),\n        embed_dim=96,\n        depths=[2, 2, 6, 2],\n        num_heads=[3, 6, 12, 24],\n        window_size=(8,7,7),\n        mlp_ratio=4.,\n        qkv_bias=True,\n        qk_scale=None,\n        drop_rate=0.,\n        attn_drop_rate=0.,\n        drop_path_rate=0.2,\n        patch_norm=True),\n    cls_head=dict(\n        type='I3DHead',\n        in_channels=768,\n        num_classes=400,\n        spatial_type='avg',\n        dropout_ratio=0.5),\n    test_cfg = dict(average_clips='prob'))"}
{"type": "source_file", "path": "src/modeling/video_swin/swin_tiny_patch244_window877_kinetics400_1k.py", "content": "_base_ = [\n    '../../_base_/models/swin/swin_tiny.py', '../../_base_/default_runtime.py'\n]\nmodel=dict(backbone=dict(patch_size=(2,4,4), drop_path_rate=0.1), test_cfg=dict(max_testing_views=4))\n\n# dataset settings\ndataset_type = 'VideoDataset'\ndata_root = 'data/kinetics400/train'\ndata_root_val = 'data/kinetics400/val'\nann_file_train = 'data/kinetics400/kinetics400_train_list.txt'\nann_file_val = 'data/kinetics400/kinetics400_val_list.txt'\nann_file_test = 'data/kinetics400/kinetics400_val_list.txt'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_bgr=False)\ntrain_pipeline = [\n    dict(type='DecordInit'),\n    dict(type='SampleFrames', clip_len=32, frame_interval=2, num_clips=1),\n    dict(type='DecordDecode'),\n    dict(type='Resize', scale=(-1, 256)),\n    dict(type='RandomResizedCrop'),\n    dict(type='Resize', scale=(224, 224), keep_ratio=False),\n    dict(type='Flip', flip_ratio=0.5),\n    dict(type='Normalize', **img_norm_cfg),\n    dict(type='FormatShape', input_format='NCTHW'),\n    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n    dict(type='ToTensor', keys=['imgs', 'label'])\n]\nval_pipeline = [\n    dict(type='DecordInit'),\n    dict(\n        type='SampleFrames',\n        clip_len=32,\n        frame_interval=2,\n        num_clips=1,\n        test_mode=True),\n    dict(type='DecordDecode'),\n    dict(type='Resize', scale=(-1, 256)),\n    dict(type='CenterCrop', crop_size=224),\n    dict(type='Flip', flip_ratio=0),\n    dict(type='Normalize', **img_norm_cfg),\n    dict(type='FormatShape', input_format='NCTHW'),\n    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n    dict(type='ToTensor', keys=['imgs'])\n]\ntest_pipeline = [\n    dict(type='DecordInit'),\n    dict(\n        type='SampleFrames',\n        clip_len=32,\n        frame_interval=2,\n        num_clips=4,\n        test_mode=True),\n    dict(type='DecordDecode'),\n    dict(type='Resize', scale=(-1, 224)),\n    dict(type='ThreeCrop', crop_size=224),\n    dict(type='Flip', flip_ratio=0),\n    dict(type='Normalize', **img_norm_cfg),\n    dict(type='FormatShape', input_format='NCTHW'),\n    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n    dict(type='ToTensor', keys=['imgs'])\n]\ndata = dict(\n    videos_per_gpu=8,\n    workers_per_gpu=4,\n    val_dataloader=dict(\n        videos_per_gpu=1,\n        workers_per_gpu=1\n    ),\n    test_dataloader=dict(\n        videos_per_gpu=1,\n        workers_per_gpu=1\n    ),\n    train=dict(\n        type=dataset_type,\n        ann_file=ann_file_train,\n        data_prefix=data_root,\n        pipeline=train_pipeline),\n    val=dict(\n        type=dataset_type,\n        ann_file=ann_file_val,\n        data_prefix=data_root_val,\n        pipeline=val_pipeline),\n    test=dict(\n        type=dataset_type,\n        ann_file=ann_file_test,\n        data_prefix=data_root_val,\n        pipeline=test_pipeline))\nevaluation = dict(\n    interval=5, metrics=['top_k_accuracy', 'mean_class_accuracy'])\n\n# optimizer\noptimizer = dict(type='AdamW', lr=1e-3, betas=(0.9, 0.999), weight_decay=0.02,\n                 paramwise_cfg=dict(custom_keys={'absolute_pos_embed': dict(decay_mult=0.),\n                                                 'relative_position_bias_table': dict(decay_mult=0.),\n                                                 'norm': dict(decay_mult=0.),\n                                                 'backbone': dict(lr_mult=0.1)}))\n# learning policy\nlr_config = dict(\n    policy='CosineAnnealing',\n    min_lr=0,\n    warmup='linear',\n    warmup_by_epoch=True,\n    warmup_iters=2.5\n)\ntotal_epochs = 30\n\n# runtime settings\ncheckpoint_config = dict(interval=1)\nwork_dir = work_dir = './work_dirs/k400_swin_tiny_patch244_window877.py'\nfind_unused_parameters = False\n\n\n# do not use mmdet version fp16\nfp16 = None\noptimizer_config = dict(\n    type=\"DistOptimizerHook\",\n    update_interval=4,\n    grad_clip=None,\n    coalesce=True,\n    bucket_size_mb=-1,\n    use_fp16=True,\n)\n"}
{"type": "source_file", "path": "src/prepro/extract_frames.py", "content": "import os\nfrom os.path import join\nfrom tqdm import tqdm\nimport multiprocessing as mp\nimport subprocess\nimport datetime\nimport  json\n\ndef get_video_duration(video_file):\n    result = subprocess.run([\"ffprobe\", \"-v\", \"error\", \"-show_entries\",\n                             \"format=duration\", \"-of\",\n                             \"default=noprint_wrappers=1:nokey=1\", video_file],\n        stdout=subprocess.PIPE,\n        stderr=subprocess.STDOUT)\n    return float(result.stdout)\n\n\ndef extract_frame_from_video(video_path, save_frame_path, fps=1, num_frames=-1,\n                             start_ts=-1, end_ts=-1,\n                             suppress_msg=False, other_args=\"\", overwrite=True):\n    \"\"\"Uniformly split a video into clips of length {clip_len}.\n    i.e., in the case of clip_len=60, the clips will be 00:00:00-00:01:00, 00:01:00-00:02:00, etc, ...\n\n    Note that we drop the first (usually opening remark, etc) and last (ask for subscription, etc) clip.\n\n    Args:\n        video_path:\n        save_frame_path:\n        fps: frame_per_second, default 1\n        suppress_msg:\n        other_args: str, other ffmpeg args, such as re-scale to 720p with '-vf scale=-1:720'\n\n    Returns:\n\n    \"\"\"\n    extra_args = \" -hide_banner -loglevel panic \" if suppress_msg else \"\"\n    extra_args += \" -y \" if overwrite else \"\"\n    if start_ts != -1 and end_ts != -1:\n        \n        dur_to_use = get_video_duration(video_path)\n        if end_ts > dur_to_use:\n            if start_ts > dur_to_use:\n                print(\"start_ts > dur_to_use\")\n                exit()\n            else:\n                end_ts = int(dur_to_use)\n        if int(end_ts - start_ts) == 0:\n            if start_ts < 2:\n                end_ts += 2\n            elif dur_to_use-end_ts < 2:\n                start_ts -= 2\n            else:\n                start_ts -= 1\n                end_ts   += 1\n        elif int(end_ts - start_ts) == 1:\n            if start_ts < 2:\n                end_ts   += 1\n            else:\n                start_ts -= 1\n        else:\n            pass\n\n        start_ts_str = str(datetime.timedelta(seconds=start_ts))\n        end_ts_str = str(datetime.timedelta(seconds=end_ts))\n        duration = str(datetime.timedelta(seconds=(end_ts - start_ts)))\n        \n\n            \n        \n        # print(start_ts, end_ts, duration)\n        extra_args += f\"-ss {start_ts_str} -t {duration} \"\n    # extra_args2 = \" -vf scale=720:-2 \"\n    # -preset veryfast:  (upgrade to latest ffmpeg if error)\n    # https://superuser.com/questions/490683/cheat-sheets-and-presets-settings-that-actually-work-with-ffmpeg-1-0\n    if num_frames <= 0 :\n        split_cmd_template = \"ffmpeg {extra} -i {video_path} -vf fps={fps} {output_frame_path}%06d.jpg\"\n        \n        cur_split_cmd = split_cmd_template.format(\n            extra=extra_args, video_path=video_path, fps=fps, output_frame_path=save_frame_path)\n    else:\n        # get duration of the video\n        if start_ts != -1 and end_ts != -1:\n            duration = end_ts - start_ts\n        else:\n            duration = get_video_duration(video_path)\n        if duration <= 0:\n            duration = 10\n            print(video_path)\n        frame_rate = num_frames/duration\n\n        # if not suppress_msg:\n        #     print(duration, frame_rate, num_frames)\n        output_exists = True\n        for frame_idx in range(num_frames):\n            if not os.path.exists(f\"{save_frame_path}{(frame_idx+1):04d}.jpg\"):\n                print(f\"{save_frame_path}{(frame_idx+1):04d}.jpg does not exist\")\n                output_exists = False\n                # save_frame_path = save_frame_path.replace(f\"{num_frames}frames\", f\"{num_frames}frames_debug\")\n\n                break\n        if output_exists:\n            return\n        split_cmd_template = \"ffmpeg {extra} -i {video_path} -vf fps={frame_rate} {output_frame_path}%04d.jpg\"\n\n        cur_split_cmd = split_cmd_template.format(\n            extra=extra_args, video_path=video_path, frame_rate=frame_rate, output_frame_path=save_frame_path)\n        if not suppress_msg:\n            print(cur_split_cmd)\n    try:\n        _ = subprocess.run(cur_split_cmd.split(), stdout=subprocess.PIPE)\n    except Exception as e:\n        print(f\"Error returned by ffmpeg cmd {e}\")\n\n\nCOMMON_VIDEO_ETX = set([\n    \".webm\", \".mpg\", \".mpeg\", \".mpv\", \".ogg\",\n    \".mp4\", \".m4p\", \".mpv\", \".avi\", \".wmv\", \".qt\",\n    \".mov\", \".flv\", \".swf\"])\n\n\ndef extract_frame(video_info, save_dir, fps, num_frames, debug=False, corrupt_files=[]):\n    (video_file_path, sTime, eTime, caption_id) = video_info\n    filename = os.path.basename(video_file_path)\n    vid = os.path.splitext(filename)[0]\n    frame_name = f\"{vid}_frame\"\n    frame_save_dir = join(save_dir, caption_id)\n    frame_save_path = join(frame_save_dir, frame_name)\n\n    launch_extract = True\n    if launch_extract:\n        os.makedirs(save_dir, exist_ok=True)\n        os.makedirs(frame_save_dir, exist_ok=True)\n        # scale=width:height\n        extract_frame_from_video(video_file_path, frame_save_path, fps=fps, num_frames=num_frames,\n                                 start_ts=sTime, end_ts=eTime,\n                                 suppress_msg=not debug, other_args=\"\")\n\n\ndef load_tsv_to_mem(tsv_file, sep='\\t'):\n    data = []\n    with open(tsv_file, 'r') as fp:\n        for _, line in enumerate(fp):\n            data.append([x.strip() for x in line.split(sep)])\n    return data\n\n\ndef extract_all_frames(video_root_dir, save_dir, fps, num_frames,\n                       video_info_tsv, corrupt_files, num_workers, debug=False):\n\n    with open(video_info_tsv) as f_obj:\n        raw_video_info = json.load(f_obj)\n\n\n        videoFiles = []\n        for _, line_item in enumerate(raw_video_info['annotations']):\n            vidName = line_item['vidName']\n            sTime = int(line_item['sTime'])\n            eTime = int(line_item['eTime'])\n            caption_id = str(line_item['id']).zfill(5)\n            \n            input_file = video_root_dir+vidName+'.mov'\n            # input_file = input_file.replace('datasets','_datasets')\n            if os.path.isfile(input_file):\n                videoFiles.append((input_file, sTime, eTime, caption_id))\n        if debug:\n            videoFiles = videoFiles[:1]\n\n        if num_workers > 0:\n            from functools import partial\n            extract_frame_partial = partial(\n                extract_frame, fps=fps,\n                save_dir=save_dir, debug=debug, corrupt_files=corrupt_files,\n                num_frames=num_frames)\n\n            with mp.Pool(num_workers) as pool, tqdm(total=len(videoFiles)) as pbar:\n                for idx, _ in enumerate(\n                        pool.imap_unordered(\n                            extract_frame_partial, videoFiles, chunksize=8)):\n                    pbar.update(1)\n        else:\n            for idx, d in tqdm(enumerate(videoFiles),\n                            total=len(videoFiles), desc=\"extracting frames from video\"):\n                extract_frame(d, save_dir, fps=fps, debug=debug,corrupt_files=corrupt_files, num_frames=num_frames)\n\n\ndef main():\n\n    # FIXME: put the raw video path here\n    video_root_dir = \"/data/hdd01/jinbu/BDDX/BDD-V/Videos/videos/\"\n\n    # FIXME: the frame save dir\n    save_dir = 'data/32frames'\n\n    # FIXME: the caption annotation file\n    video_info_tsv = '/data/hdd01/jinbu/video_preprocess/code/data/processed/captions_BDDX.json'\n\n    extract_all_frames(video_root_dir, save_dir, 1,\n                       32, video_info_tsv, corrupt_files=None,\n                       num_workers=16, debug=False)\n\n\nif __name__ == '__main__':\n    main()"}
{"type": "source_file", "path": "src/modeling/video_swin/swin_base_patch244_window877_kinetics600_22k.py", "content": "_base_ = \"swin_base_patch244_window877_kinetics400_22k.py\"\n\ndata_root = 'data/kinetics600/train'\ndata_root_val = 'data/kinetics600/val'\nann_file_train = 'data/kinetics600/kinetics600_train_list.txt'\nann_file_val = 'data/kinetics600/kinetics600_val_list.txt'\nann_file_test = 'data/kinetics600/kinetics600_val_list.txt'\n\ndata = dict(\n    train=dict(\n        ann_file=ann_file_train,\n        data_prefix=data_root),\n    val=dict(\n        ann_file=ann_file_val,\n        data_prefix=data_root_val),\n    test=dict(\n        ann_file=ann_file_test,\n        data_prefix=data_root_val))\n\nmodel=dict(cls_head=dict(num_classes=600))\n"}
{"type": "source_file", "path": "src/prepro/tsv_preproc_BDDX.py", "content": "import os\nimport sys\npythonpath = os.path.abspath(\n    os.path.dirname(os.path.dirname(os.path.dirname(__file__))))\nprint(pythonpath)\nsys.path.insert(0, pythonpath)\nsys.path.append(\"/data/hdd01/jinbu/SwinBERT-main\")\nimport os.path as op\nimport json, yaml, code, io\nimport numpy as np\nimport pandas as pd\nfrom src.utils.tsv_file_ops import tsv_writer\nfrom src.utils.tsv_file_ops import generate_linelist_file\nfrom collections import defaultdict\n\n\n# !!! please follow the readme to organize the output of the previous script \n\n# # FIXME: data path to raw video files\ndata_vid_id = \"/data/hdd01/jinbu/BDDX/BDD-V/Videos/videos//{}\"\ndataset_path = './datasets/BDDX/'\n# annotations downloaded from official downstream dataset\nBDDX_anns = 'datasets/BDDX/captions_BDDX.json'\nBDDX_subtitle = 'datasets/BDDX/captions_BDDX.json'\n\n# To generate tsv files:\n# {}.img.tsv: we use it to store video path info \nvisual_file = \"./datasets/BDDX/{}.img.tsv\"\n# {}.caption.tsv: we use it to store  captions\ncap_file = \"./datasets/BDDX/{}.caption.tsv\"\n# {}.linelist.tsv: since each video may have multiple captions, we need to store the corresponance between vidoe id and caption id\nlinelist_file = \"./datasets/BDDX/{}.linelist.tsv\"\n# {}.label.tsv: we store any useful labels or metadara here, such as object tags. Now we only have captions. maybe can remove it in future.\nlabel_file = \"./datasets/BDDX/{}.label.tsv\"\n\ndef write_to_yaml_file(context, file_name):\n    with open(file_name, 'w') as fp:\n        yaml.dump(context, fp, encoding='utf-8')\n\ndef tsv_reader(tsv_file, sep='\\t'):\n    with open(tsv_file, 'r') as fp:\n        for i, line in enumerate(fp):\n            yield [x.strip() for x in line.split(sep)]\ndef load_jsonl(filename):\n    with open(filename, \"r\") as f:\n        return [json.loads(l.strip(\"\\n\")) for l in f.readlines()]\n\ndef config_save_file(tsv_file, save_file=None, append_str='.new.tsv'):\n    if save_file is not None:\n        return save_file\n    return op.splitext(tsv_file)[0] + append_str\n\ndef generate_caption_linelist_file(caption_tsv_file, save_file=None):\n    num_captions = []\n    for row in tsv_reader(caption_tsv_file):\n        num_captions.append(len(json.loads(row[1])))\n\n    cap_linelist = ['\\t'.join([str(img_idx), str(cap_idx)]) \n            for img_idx in range(len(num_captions)) \n            for cap_idx in range(num_captions[img_idx])\n    ]\n    save_file = config_save_file(caption_tsv_file, save_file, '.linelist.tsv')\n    with open(save_file, 'w') as f:\n        f.write('\\n'.join(cap_linelist))\n    return save_file\n\ndef dump_tsv_gt_to_coco_format(caption_tsv_file, outfile):\n    annotations = []\n    images = []\n    cap_id = 0\n    caption_tsv = tsv_reader(caption_tsv_file)\n\n    for cap_row  in caption_tsv:\n        image_id = cap_row[0]\n        key = image_id\n        caption_data = json.loads(cap_row[1])\n        count = len(caption_data)\n        for i in range(count):\n            action = caption_data[i]['action']\n            justification = caption_data[i]['justification']\n            annotations.append(\n                        {'image_id': image_id, 'action': action, 'justification': justification,\n                        'id': cap_id})\n            cap_id += 1\n\n        images.append({'id': image_id, 'file_name': key})\n\n    with open(outfile, 'w') as fp:\n        json.dump({'annotations': annotations, 'images': images,\n                'type': 'captions', 'info': 'dummy', 'licenses': 'dummy'},\n                fp)\n \n\n\ndef process_new(split):\n    f = open(BDDX_anns, 'r')\n    database = json.load(f)\n    \n\n    asr_dict = {}\n    \n    annos = database['annotations']\n    if split == 'training':\n        video_list = database['videos'][:21143]\n        subtitle = annos[:21143]\n    elif split == 'validation':\n        video_list = database['videos'][21143:23662]\n        subtitle = annos[21143:23662]\n    elif split == 'testing':\n        video_list = database['videos'][23662:]\n        subtitle = annos[23662:]\n    else:\n        exit(\"split error\")\n    asr_len = []\n    # for i in range(len(subtitle)):\n    #     data = subtitle[i]\n    #     key = data['vidName']\n    #     sub = data\n    #     text = ''\n    #     for item in sub:\n    #         text += item['text']\n    #         text += ' '\n    #     asr_dict[key] = text[0:-1]\n    #     asr_len.append(len(text[0:-1].split(' ')))\n\n\n    img_label = []\n    rows_label = []\n    caption_label = []\n\n    for video_ord, video_info in enumerate(video_list):\n        video_name = video_info['video_name']\n        annotations = subtitle[video_ord]\n        assert annotations['vidName'] == video_name\n        if True:\n            if True:\n                sample_id = annotations['id']\n                action = annotations['action']\n                justification = annotations['justification']\n\n                resolved_video_name = '{}'.format(video_name)\n                resolved_data_vid_id = split + '_' + resolved_video_name\n                output_captions = []\n\n                output_captions.append({\"action\": action, \"justification\": justification})\n        \n                caption_label.append([str(resolved_data_vid_id),json.dumps(output_captions)]) \n                rows_label.append([str(resolved_data_vid_id),json.dumps(output_captions)]) \n                img_label.append([str(resolved_data_vid_id), str(resolved_data_vid_id)])\n\n    resolved_visual_file = visual_file.format(split)\n    print(\"generating visual file for\", resolved_visual_file)\n    tsv_writer(img_label, resolved_visual_file)\n\n    resolved_label_file = label_file.format(split)\n    print(\"generating label file for\", resolved_label_file)\n    tsv_writer(rows_label, resolved_label_file)\n\n    resolved_linelist_file = linelist_file.format(split)\n    print(\"generating linelist file for\", rows_label)\n    generate_linelist_file(resolved_label_file, save_file=resolved_linelist_file)\n\n    resolved_cap_file = cap_file.format(split)\n    print(\"generating cap file for\", resolved_cap_file)\n    tsv_writer(caption_label, resolved_cap_file)\n    print(\"generating cap linelist file for\", resolved_cap_file)\n    resolved_cap_linelist_file = generate_caption_linelist_file(resolved_cap_file)\n\n    gt_file_coco = op.splitext(resolved_cap_file)[0] + '_coco_format.json'\n    print(\"convert gt to\", gt_file_coco)\n    dump_tsv_gt_to_coco_format(resolved_cap_file, gt_file_coco)\n\n    out_cfg = {}\n    all_field = ['img', 'label', 'caption', 'caption_linelist', 'caption_coco_format']\n    all_tsvfile = [resolved_visual_file, resolved_label_file, resolved_cap_file, resolved_cap_linelist_file, gt_file_coco]\n    for field, tsvpath in zip(all_field, all_tsvfile):\n        out_cfg[field] = tsvpath.split('/')[-1]\n    out_yaml = '{}.yaml'.format(split)\n    write_to_yaml_file(out_cfg, op.join(dataset_path, out_yaml))\n    print('Create yaml file: {}'.format(op.join(dataset_path, out_yaml)))\n\n\ndef main():\n    process_new('training')\n    process_new('validation')\n    process_new('testing')\n\nif __name__ == '__main__':\n    main()\n\n\n\n"}
{"type": "source_file", "path": "src/pytorch_grad_cam/ablation_layer.py", "content": "import torch\r\nfrom collections import OrderedDict\r\nimport numpy as np\r\nfrom .utils.svd_on_activations import get_2d_projection\r\n\r\n\r\nclass AblationLayer(torch.nn.Module):\r\n    def __init__(self):\r\n        super(AblationLayer, self).__init__()\r\n\r\n    def objectiveness_mask_from_svd(self, activations, threshold=0.01):\r\n        \"\"\" Experimental method to get a binary mask to compare if the activation is worth ablating.\r\n            The idea is to apply the EigenCAM method by doing PCA on the activations.\r\n            Then we create a binary mask by comparing to a low threshold.\r\n            Areas that are masked out, are probably not interesting anyway.\r\n        \"\"\"\r\n\r\n        projection = get_2d_projection(activations[None, :])[0, :]\r\n        projection = np.abs(projection)\r\n        projection = projection - projection.min()\r\n        projection = projection / projection.max()\r\n        projection = projection > threshold\r\n        return projection\r\n\r\n    def activations_to_be_ablated(\r\n            self,\r\n            activations,\r\n            ratio_channels_to_ablate=1.0):\r\n        \"\"\" Experimental method to get a binary mask to compare if the activation is worth ablating.\r\n            Create a binary CAM mask with objectiveness_mask_from_svd.\r\n            Score each Activation channel, by seeing how much of its values are inside the mask.\r\n            Then keep the top channels.\r\n\r\n        \"\"\"\r\n        if ratio_channels_to_ablate == 1.0:\r\n            self.indices = np.int32(range(activations.shape[0]))\r\n            return self.indices\r\n\r\n        projection = self.objectiveness_mask_from_svd(activations)\r\n\r\n        scores = []\r\n        for channel in activations:\r\n            normalized = np.abs(channel)\r\n            normalized = normalized - normalized.min()\r\n            normalized = normalized / np.max(normalized)\r\n            score = (projection * normalized).sum() / normalized.sum()\r\n            scores.append(score)\r\n        scores = np.float32(scores)\r\n\r\n        indices = list(np.argsort(scores))\r\n        high_score_indices = indices[::-\r\n                                     1][: int(len(indices) *\r\n                                              ratio_channels_to_ablate)]\r\n        low_score_indices = indices[: int(\r\n            len(indices) * ratio_channels_to_ablate)]\r\n        self.indices = np.int32(high_score_indices + low_score_indices)\r\n        return self.indices\r\n\r\n    def set_next_batch(\r\n            self,\r\n            input_batch_index,\r\n            activations,\r\n            num_channels_to_ablate):\r\n        \"\"\" This creates the next batch of activations from the layer.\r\n            Just take corresponding batch member from activations, and repeat it num_channels_to_ablate times.\r\n        \"\"\"\r\n        self.activations = activations[input_batch_index, :, :, :].clone(\r\n        ).unsqueeze(0).repeat(num_channels_to_ablate, 1, 1, 1)\r\n\r\n    def __call__(self, x):\r\n        output = self.activations\r\n        for i in range(output.size(0)):\r\n            # Commonly the minimum activation will be 0,\r\n            # And then it makes sense to zero it out.\r\n            # However depending on the architecture,\r\n            # If the values can be negative, we use very negative values\r\n            # to perform the ablation, deviating from the paper.\r\n            if torch.min(output) == 0:\r\n                output[i, self.indices[i], :] = 0\r\n            else:\r\n                ABLATION_VALUE = 1e7\r\n                output[i, self.indices[i], :] = torch.min(\r\n                    output) - ABLATION_VALUE\r\n\r\n        return output\r\n\r\n\r\nclass AblationLayerVit(AblationLayer):\r\n    def __init__(self):\r\n        super(AblationLayerVit, self).__init__()\r\n\r\n    def __call__(self, x):\r\n        output = self.activations\r\n        output = output.transpose(1, len(output.shape) - 1)\r\n        for i in range(output.size(0)):\r\n\r\n            # Commonly the minimum activation will be 0,\r\n            # And then it makes sense to zero it out.\r\n            # However depending on the architecture,\r\n            # If the values can be negative, we use very negative values\r\n            # to perform the ablation, deviating from the paper.\r\n            if torch.min(output) == 0:\r\n                output[i, self.indices[i], :] = 0\r\n            else:\r\n                ABLATION_VALUE = 1e7\r\n                output[i, self.indices[i], :] = torch.min(\r\n                    output) - ABLATION_VALUE\r\n\r\n        output = output.transpose(len(output.shape) - 1, 1)\r\n\r\n        return output\r\n\r\n    def set_next_batch(\r\n            self,\r\n            input_batch_index,\r\n            activations,\r\n            num_channels_to_ablate):\r\n        \"\"\" This creates the next batch of activations from the layer.\r\n            Just take corresponding batch member from activations, and repeat it num_channels_to_ablate times.\r\n        \"\"\"\r\n        repeat_params = [num_channels_to_ablate] + \\\r\n            len(activations.shape[:-1]) * [1]\r\n        self.activations = activations[input_batch_index, :, :].clone(\r\n        ).unsqueeze(0).repeat(*repeat_params)\r\n\r\n\r\nclass AblationLayerFasterRCNN(AblationLayer):\r\n    def __init__(self):\r\n        super(AblationLayerFasterRCNN, self).__init__()\r\n\r\n    def set_next_batch(\r\n            self,\r\n            input_batch_index,\r\n            activations,\r\n            num_channels_to_ablate):\r\n        \"\"\" Extract the next batch member from activations,\r\n            and repeat it num_channels_to_ablate times.\r\n        \"\"\"\r\n        self.activations = OrderedDict()\r\n        for key, value in activations.items():\r\n            fpn_activation = value[input_batch_index,\r\n                                   :, :, :].clone().unsqueeze(0)\r\n            self.activations[key] = fpn_activation.repeat(\r\n                num_channels_to_ablate, 1, 1, 1)\r\n\r\n    def __call__(self, x):\r\n        result = self.activations\r\n        layers = {0: '0', 1: '1', 2: '2', 3: '3', 4: 'pool'}\r\n        num_channels_to_ablate = result['pool'].size(0)\r\n        for i in range(num_channels_to_ablate):\r\n            pyramid_layer = int(self.indices[i] / 256)\r\n            index_in_pyramid_layer = int(self.indices[i] % 256)\r\n            result[layers[pyramid_layer]][i,\r\n                                          index_in_pyramid_layer, :, :] = -1000\r\n        return result\r\n"}
{"type": "source_file", "path": "src/pytorch_grad_cam/ablation_cam_multilayer.py", "content": "import cv2\r\nimport numpy as np\r\nimport torch\r\nimport tqdm\r\nfrom .base_cam import BaseCAM\r\n\r\n\r\nclass AblationLayer(torch.nn.Module):\r\n    def __init__(self, layer, reshape_transform, indices):\r\n        super(AblationLayer, self).__init__()\r\n\r\n        self.layer = layer\r\n        self.reshape_transform = reshape_transform\r\n        # The channels to zero out:\r\n        self.indices = indices\r\n\r\n    def forward(self, x):\r\n        self.__call__(x)\r\n\r\n    def __call__(self, x):\r\n        output = self.layer(x)\r\n\r\n        # Hack to work with ViT,\r\n        # Since the activation channels are last and not first like in CNNs\r\n        # Probably should remove it?\r\n        if self.reshape_transform is not None:\r\n            output = output.transpose(1, 2)\r\n\r\n        for i in range(output.size(0)):\r\n\r\n            # Commonly the minimum activation will be 0,\r\n            # And then it makes sense to zero it out.\r\n            # However depending on the architecture,\r\n            # If the values can be negative, we use very negative values\r\n            # to perform the ablation, deviating from the paper.\r\n            if torch.min(output) == 0:\r\n                output[i, self.indices[i], :] = 0\r\n            else:\r\n                ABLATION_VALUE = 1e5\r\n                output[i, self.indices[i], :] = torch.min(\r\n                    output) - ABLATION_VALUE\r\n\r\n        if self.reshape_transform is not None:\r\n            output = output.transpose(2, 1)\r\n\r\n        return output\r\n\r\n\r\ndef replace_layer_recursive(model, old_layer, new_layer):\r\n    for name, layer in model._modules.items():\r\n        if layer == old_layer:\r\n            model._modules[name] = new_layer\r\n            return True\r\n        elif replace_layer_recursive(layer, old_layer, new_layer):\r\n            return True\r\n    return False\r\n\r\n\r\nclass AblationCAM(BaseCAM):\r\n    def __init__(self, model, target_layers, use_cuda=False,\r\n                 reshape_transform=None):\r\n        super(AblationCAM, self).__init__(model, target_layers, use_cuda,\r\n                                          reshape_transform)\r\n\r\n        if len(target_layers) > 1:\r\n            print(\r\n                \"Warning. You are usign Ablation CAM with more than 1 layers. \"\r\n                \"This is supported only if all layers have the same output shape\")\r\n\r\n    def set_ablation_layers(self):\r\n        self.ablation_layers = []\r\n        for target_layer in self.target_layers:\r\n            ablation_layer = AblationLayer(target_layer,\r\n                                           self.reshape_transform, indices=[])\r\n            self.ablation_layers.append(ablation_layer)\r\n            replace_layer_recursive(self.model, target_layer, ablation_layer)\r\n\r\n    def unset_ablation_layers(self):\r\n        # replace the model back to the original state\r\n        for ablation_layer, target_layer in zip(\r\n                self.ablation_layers, self.target_layers):\r\n            replace_layer_recursive(self.model, ablation_layer, target_layer)\r\n\r\n    def set_ablation_layer_batch_indices(self, indices):\r\n        for ablation_layer in self.ablation_layers:\r\n            ablation_layer.indices = indices\r\n\r\n    def trim_ablation_layer_batch_indices(self, keep):\r\n        for ablation_layer in self.ablation_layers:\r\n            ablation_layer.indices = ablation_layer.indices[:keep]\r\n\r\n    def get_cam_weights(self,\r\n                        input_tensor,\r\n                        target_category,\r\n                        activations,\r\n                        grads):\r\n        with torch.no_grad():\r\n            outputs = self.model(input_tensor).cpu().numpy()\r\n            original_scores = []\r\n            for i in range(input_tensor.size(0)):\r\n                original_scores.append(outputs[i, target_category[i]])\r\n        original_scores = np.float32(original_scores)\r\n\r\n        self.set_ablation_layers()\r\n\r\n        if hasattr(self, \"batch_size\"):\r\n            BATCH_SIZE = self.batch_size\r\n        else:\r\n            BATCH_SIZE = 32\r\n\r\n        number_of_channels = activations.shape[1]\r\n        weights = []\r\n\r\n        with torch.no_grad():\r\n            # Iterate over the input batch\r\n            for tensor, category in zip(input_tensor, target_category):\r\n                batch_tensor = tensor.repeat(BATCH_SIZE, 1, 1, 1)\r\n                for i in tqdm.tqdm(range(0, number_of_channels, BATCH_SIZE)):\r\n                    self.set_ablation_layer_batch_indices(\r\n                        list(range(i, i + BATCH_SIZE)))\r\n\r\n                    if i + BATCH_SIZE > number_of_channels:\r\n                        keep = number_of_channels - i\r\n                        batch_tensor = batch_tensor[:keep]\r\n                        self.trim_ablation_layer_batch_indices(self, keep)\r\n                    score = self.model(batch_tensor)[:, category].cpu().numpy()\r\n                    weights.extend(score)\r\n\r\n        weights = np.float32(weights)\r\n        weights = weights.reshape(activations.shape[:2])\r\n        original_scores = original_scores[:, None]\r\n        weights = (original_scores - weights) / original_scores\r\n\r\n        # replace the model back to the original state\r\n        self.unset_ablation_layers()\r\n        return weights\r\n"}
{"type": "source_file", "path": "src/layers/bert/modeling_bert.py", "content": "# coding=utf-8\n# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"PyTorch BERT model. \"\"\"\n\nfrom __future__ import absolute_import, division, print_function, unicode_literals\n\nimport json\nimport math\nimport os\nimport sys\nfrom io import open\n\nimport torch\nfrom torch import nn\nfrom torch.distributions import kl_divergence, Categorical\nfrom torch.nn import CrossEntropyLoss, MSELoss\nimport torch.nn.functional as F\n\nfrom torch.nn.utils.weight_norm import weight_norm\n\nfrom .modeling_utils import (WEIGHTS_NAME, CONFIG_NAME, PretrainedConfig, PreTrainedModel,\n                             prune_linear_layer, add_start_docstrings)\n\nimport logging\nfrom src.utils.comm import is_main_process\nlogger = logging.getLogger(__name__)  # pylint: disable=invalid-name\nif not is_main_process():\n    logger.disabled = True\n\nimport torch.utils.checkpoint as torch_checkpoint\nBERT_PRETRAINED_MODEL_ARCHIVE_MAP = {\n    'bert-base-uncased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin\",\n    'bert-large-uncased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-pytorch_model.bin\",\n    'bert-base-cased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-pytorch_model.bin\",\n    'bert-large-cased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased-pytorch_model.bin\",\n    'bert-base-multilingual-uncased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-pytorch_model.bin\",\n    'bert-base-multilingual-cased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-pytorch_model.bin\",\n    'bert-base-chinese': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-pytorch_model.bin\",\n    'bert-base-german-cased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-cased-pytorch_model.bin\",\n    'bert-large-uncased-whole-word-masking': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-whole-word-masking-pytorch_model.bin\",\n    'bert-large-cased-whole-word-masking': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased-whole-word-masking-pytorch_model.bin\",\n    'bert-large-uncased-whole-word-masking-finetuned-squad': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-whole-word-masking-finetuned-squad-pytorch_model.bin\",\n    'bert-large-cased-whole-word-masking-finetuned-squad': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased-whole-word-masking-finetuned-squad-pytorch_model.bin\",\n    'bert-base-cased-finetuned-mrpc': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-finetuned-mrpc-pytorch_model.bin\",\n}\n\nBERT_PRETRAINED_CONFIG_ARCHIVE_MAP = {\n    'bert-base-uncased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json\",\n    'bert-large-uncased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-config.json\",\n    'bert-base-cased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json\",\n    'bert-large-cased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased-config.json\",\n    'bert-base-multilingual-uncased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json\",\n    'bert-base-multilingual-cased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-config.json\",\n    'bert-base-chinese': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-config.json\",\n    'bert-base-german-cased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-cased-config.json\",\n    'bert-large-uncased-whole-word-masking': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-whole-word-masking-config.json\",\n    'bert-large-cased-whole-word-masking': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased-whole-word-masking-config.json\",\n    'bert-large-uncased-whole-word-masking-finetuned-squad': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-whole-word-masking-finetuned-squad-config.json\",\n    'bert-large-cased-whole-word-masking-finetuned-squad': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased-whole-word-masking-finetuned-squad-config.json\",\n    'bert-base-cased-finetuned-mrpc': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-finetuned-mrpc-config.json\",\n}\n\n\ndef load_tf_weights_in_bert(model, config, tf_checkpoint_path):\n    \"\"\" Load tf checkpoints in a pytorch model.\n    \"\"\"\n    try:\n        import re\n        import numpy as np\n        import tensorflow as tf\n    except ImportError:\n        logger.error(\"Loading a TensorFlow models in PyTorch, requires TensorFlow to be installed. Please see \"\n            \"https://www.tensorflow.org/install/ for installation instructions.\")\n        raise\n    tf_path = os.path.abspath(tf_checkpoint_path)\n    logger.info(\"Converting TensorFlow checkpoint from {}\".format(tf_path))\n    # Load weights from TF model\n    init_vars = tf.train.list_variables(tf_path)\n    names = []\n    arrays = []\n    for name, shape in init_vars:\n        logger.info(\"Loading TF weight {} with shape {}\".format(name, shape))\n        array = tf.train.load_variable(tf_path, name)\n        names.append(name)\n        arrays.append(array)\n\n    for name, array in zip(names, arrays):\n        name = name.split('/')\n        # adam_v and adam_m are variables used in AdamWeightDecayOptimizer to calculated m and v\n        # which are not required for using pretrained model\n        if any(n in [\"adam_v\", \"adam_m\", \"global_step\"] for n in name):\n            logger.info(\"Skipping {}\".format(\"/\".join(name)))\n            continue\n        pointer = model\n        for m_name in name:\n            if re.fullmatch(r'[A-Za-z]+_\\d+', m_name):\n                l = re.split(r'_(\\d+)', m_name)\n            else:\n                l = [m_name]\n            if l[0] == 'kernel' or l[0] == 'gamma':\n                pointer = getattr(pointer, 'weight')\n            elif l[0] == 'output_bias' or l[0] == 'beta':\n                pointer = getattr(pointer, 'bias')\n            elif l[0] == 'output_weights':\n                pointer = getattr(pointer, 'weight')\n            elif l[0] == 'squad':\n                pointer = getattr(pointer, 'classifier')\n            else:\n                try:\n                    pointer = getattr(pointer, l[0])\n                except AttributeError:\n                    logger.info(\"Skipping {}\".format(\"/\".join(name)))\n                    continue\n            if len(l) >= 2:\n                num = int(l[1])\n                pointer = pointer[num]\n        if m_name[-11:] == '_embeddings':\n            pointer = getattr(pointer, 'weight')\n        elif m_name == 'kernel':\n            array = np.transpose(array)\n        try:\n            assert pointer.shape == array.shape\n        except AssertionError as e:\n            e.args += (pointer.shape, array.shape)\n            raise\n        logger.info(\"Initialize PyTorch weight {}\".format(name))\n        pointer.data = torch.from_numpy(array)\n    return model\n\n\ndef gelu(x):\n    \"\"\"Implementation of the gelu activation function.\n        For information: OpenAI GPT's gelu is slightly different (and gives slightly different results):\n        0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n        Also see https://arxiv.org/abs/1606.08415\n    \"\"\"\n    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n\n\ndef swish(x):\n    return x * torch.sigmoid(x)\n\n\nACT2FN = {\"gelu\": gelu, \"relu\": torch.nn.functional.relu, \"swish\": swish}\n\n\nclass BertConfig(PretrainedConfig):\n    r\"\"\"\n        :class:`~pytorch_transformers.BertConfig` is the configuration class to store the configuration of a\n        `BertModel`.\n\n\n        Arguments:\n            vocab_size_or_config_json_file: Vocabulary size of `inputs_ids` in `BertModel`.\n            hidden_size: Size of the encoder layers and the pooler layer.\n            num_hidden_layers: Number of hidden layers in the Transformer encoder.\n            num_attention_heads: Number of attention heads for each attention layer in\n                the Transformer encoder.\n            intermediate_size: The size of the \"intermediate\" (i.e., feed-forward)\n                layer in the Transformer encoder.\n            hidden_act: The non-linear activation function (function or string) in the\n                encoder and pooler. If string, \"gelu\", \"relu\" and \"swish\" are supported.\n            hidden_dropout_prob: The dropout probabilitiy for all fully connected\n                layers in the embeddings, encoder, and pooler.\n            attention_probs_dropout_prob: The dropout ratio for the attention\n                probabilities.\n            max_position_embeddings: The maximum sequence length that this model might\n                ever be used with. Typically set this to something large just in case\n                (e.g., 512 or 1024 or 2048).\n            type_vocab_size: The vocabulary size of the `token_type_ids` passed into\n                `BertModel`.\n            initializer_range: The sttdev of the truncated_normal_initializer for\n                initializing all weight matrices.\n            layer_norm_eps: The epsilon used by LayerNorm.\n    \"\"\"\n    pretrained_config_archive_map = BERT_PRETRAINED_CONFIG_ARCHIVE_MAP\n\n    def __init__(self,\n                 vocab_size_or_config_json_file=30522,\n                 hidden_size=768,\n                 num_hidden_layers=12,\n                 num_attention_heads=12,\n                 intermediate_size=3072,\n                 hidden_act=\"gelu\",\n                 hidden_dropout_prob=0.1,\n                 attention_probs_dropout_prob=0.1,\n                 max_position_embeddings=512,\n                 type_vocab_size=2,\n                 initializer_range=0.02,\n                 layer_norm_eps=1e-12,\n                 **kwargs):\n        super(BertConfig, self).__init__(**kwargs)\n        if isinstance(vocab_size_or_config_json_file, str) or (sys.version_info[0] == 2\n                        and isinstance(vocab_size_or_config_json_file, unicode)):\n            with open(vocab_size_or_config_json_file, \"r\", encoding='utf-8') as reader:\n                json_config = json.loads(reader.read())\n            for key, value in json_config.items():\n                self.__dict__[key] = value\n        elif isinstance(vocab_size_or_config_json_file, int):\n            self.vocab_size = vocab_size_or_config_json_file\n            self.hidden_size = hidden_size\n            self.num_hidden_layers = num_hidden_layers\n            self.num_attention_heads = num_attention_heads\n            self.hidden_act = hidden_act\n            self.intermediate_size = intermediate_size\n            self.hidden_dropout_prob = hidden_dropout_prob\n            self.attention_probs_dropout_prob = attention_probs_dropout_prob\n            self.max_position_embeddings = max_position_embeddings\n            self.type_vocab_size = type_vocab_size\n            self.initializer_range = initializer_range\n            self.layer_norm_eps = layer_norm_eps\n        else:\n            raise ValueError(\"First argument must be either a vocabulary size (int)\"\n                             \"or the path to a pretrained model config file (str)\")\n\n\n\n# try:\n#     from apex.normalization.fused_layer_norm import FusedLayerNorm\n# except ImportError:\n#     logger.info(\"Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\")\n#     FusedLayerNorm = None\n\n# class BertLayerNorm(nn.Module):\n#     def __init__(self, hidden_size, eps=1e-12):\n#         \"\"\"Construct a layernorm module in the TF style (epsilon inside the square root).\n#         \"\"\"\n#         super(BertLayerNorm, self).__init__()\n#         self.weight = nn.Parameter(torch.ones(hidden_size))\n#         self.bias = nn.Parameter(torch.zeros(hidden_size))\n#         self.variance_epsilon = eps\n\n#     def forward(self, x):\n#         u = x.mean(-1, keepdim=True)\n#         s = (x - u).pow(2).mean(-1, keepdim=True)\n#         x = (x - u) / torch.sqrt(s + self.variance_epsilon)\n#         return self.weight * x + self.bias\n\n# prefer the apex version, but avoid conditional import. While FusedLayerNorm can load BertLayerNorm, the other way is not true\n# LayerNormClass = FusedLayerNorm or BertLayerNorm\nLayerNormClass = torch.nn.LayerNorm\nBertLayerNorm = torch.nn.LayerNorm\n\nclass BertEmbeddings(nn.Module):\n    \"\"\"Construct the embeddings from word, position and token_type embeddings.\n    \"\"\"\n    def __init__(self, config):\n        super(BertEmbeddings, self).__init__()\n        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=0)\n        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)\n\n        # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load\n        # any TensorFlow checkpoint file\n        self.LayerNorm = LayerNormClass(config.hidden_size, eps=config.layer_norm_eps)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n\n    def forward(self, input_ids, token_type_ids=None, position_ids=None):\n        seq_length = input_ids.size(1)\n        if position_ids is None:\n            position_ids = torch.arange(seq_length, dtype=torch.long, device=input_ids.device)\n            position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n        if token_type_ids is None:\n            token_type_ids = torch.zeros_like(input_ids)\n\n        words_embeddings = self.word_embeddings(input_ids)\n        position_embeddings = self.position_embeddings(position_ids)\n        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n\n        embeddings = words_embeddings + position_embeddings + token_type_embeddings\n        embeddings = self.LayerNorm(embeddings)\n        embeddings = self.dropout(embeddings)\n        return embeddings\n\n\nclass BertImgEmbeddings(nn.Module):\n    \"\"\" BERT Language - Image Embedding\n    Construct the embeddings from word & Images, position and token_type embeddings.\n    \"\"\"\n    def __init__(self, config):\n        super(BertImgEmbeddings, self).__init__()\n        self.img_dim = 565\n\n        self.img_embeddings = nn.Linear(self.img_dim, self.config.hidden_size, bias=True)\n        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)\n\n        # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load\n        # any TensorFlow checkpoint file\n        self.LayerNorm = LayerNormClass(config.hidden_size, eps=config.layer_norm_eps)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n\n    def forward(self, input_ids, token_type_ids=None, position_ids=None):\n        seq_length = input_ids.size(1)\n        if position_ids is None:\n            position_ids = torch.arange(seq_length, dtype=torch.long, device=input_ids.device)\n            position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n        if token_type_ids is None:\n            token_type_ids = torch.zeros_like(input_ids)\n\n        img_embeddings = self.img_embeddings(input_ids)\n        position_embeddings = self.position_embeddings(position_ids)\n        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n\n        embeddings = img_embeddings + position_embeddings + token_type_embeddings\n        embeddings = self.LayerNorm(embeddings)\n        embeddings = self.dropout(embeddings)\n        return embeddings\n\n\nclass BertSelfAttention(nn.Module):\n    def __init__(self, config):\n        super(BertSelfAttention, self).__init__()\n        if config.hidden_size % config.num_attention_heads != 0:\n            raise ValueError(\n                \"The hidden size (%d) is not a multiple of the number of attention \"\n                \"heads (%d)\" % (config.hidden_size, config.num_attention_heads))\n        self.output_attentions = config.output_attentions\n\n        self.num_attention_heads = config.num_attention_heads\n        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n        self.all_head_size = self.num_attention_heads * self.attention_head_size\n\n        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n\n        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n\n    def transpose_for_scores(self, x):\n        if torch._C._get_tracing_state():\n            # exporter is not smart enough to detect dynamic size for some paths\n            x = x.view(x.shape[0], -1, self.num_attention_heads, self.attention_head_size)\n        else:\n            new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n            x = x.view(*new_x_shape)\n        return x.permute(0, 2, 1, 3)\n\n    def forward(self, hidden_states, attention_mask, head_mask=None,\n            history_state=None):\n        if history_state is not None:\n            x_states = torch.cat([history_state, hidden_states], dim=1)\n            mixed_query_layer = self.query(hidden_states)\n            mixed_key_layer = self.key(x_states)\n            mixed_value_layer = self.value(x_states)\n        else:\n            mixed_query_layer = self.query(hidden_states)\n            mixed_key_layer = self.key(hidden_states)\n            mixed_value_layer = self.value(hidden_states)\n\n        query_layer = self.transpose_for_scores(mixed_query_layer)\n        key_layer = self.transpose_for_scores(mixed_key_layer)\n        value_layer = self.transpose_for_scores(mixed_value_layer)\n\n        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n        # Apply the attention mask is (precomputed for all layers in BertModel forward() function)\n        attention_scores = attention_scores + attention_mask\n\n        # Normalize the attention scores to probabilities.\n        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n\n        # This is actually dropping out entire tokens to attend to, which might\n        # seem a bit unusual, but is taken from the original Transformer paper.\n        attention_probs = self.dropout(attention_probs)\n\n        # Mask heads if we want to\n        if head_mask is not None:\n            attention_probs = attention_probs * head_mask\n\n        context_layer = torch.matmul(attention_probs, value_layer)\n\n        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n        context_layer = context_layer.view(*new_context_layer_shape)\n\n\n        visualize_attn = False\n        if visualize_attn:\n            import numpy as np\n            from visualize import visualize_grid_attention_v2\n            import os\n            for i in range(len(attention_probs[0])):\n                os.makedirs(\"attn_visualize\", exist_ok=True)\n                j = len(os.listdir(\"attn_visualize\"))//12\n                save_path = f\"attn_visualize/head_{j}_{i}\"\n                os.makedirs(save_path, exist_ok=True)\n                # if j % 5 != 0:\n                #     continue\n                # if i != 11:\n                #     continue\n                visualize_grid_attention_v2(\"demo/black.jpg\",\n                           save_path=save_path,\n                           attention_mask=(np.array(attention_probs[0][-1].cpu())-np.eye(len(attention_probs[0][-1]))),\n                           save_image=True,\n                           save_original_image=False,\n                           quality=100)\n                np.save(os.path.join(save_path, \"atten.npy\"), np.array(attention_probs[0][-1].cpu()))\n\n        outputs = (context_layer, attention_probs) if self.output_attentions else (context_layer,)\n        return outputs\n\n\nclass BertSelfOutput(nn.Module):\n    def __init__(self, config):\n        super(BertSelfOutput, self).__init__()\n        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n        self.LayerNorm = LayerNormClass(config.hidden_size, eps=config.layer_norm_eps)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n\n    def forward(self, hidden_states, input_tensor):\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.dropout(hidden_states)\n        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n        return hidden_states\n\n\nclass BertAttention(nn.Module):\n    def __init__(self, config):\n        super(BertAttention, self).__init__()\n        self.self = BertSelfAttention(config)\n        self.output = BertSelfOutput(config)\n\n    def prune_heads(self, heads):\n        if len(heads) == 0:\n            return\n        mask = torch.ones(self.self.num_attention_heads, self.self.attention_head_size)\n        for head in heads:\n            mask[head] = 0\n        mask = mask.view(-1).contiguous().eq(1)\n        index = torch.arange(len(mask))[mask].long()\n        # Prune linear layers\n        self.self.query = prune_linear_layer(self.self.query, index)\n        self.self.key = prune_linear_layer(self.self.key, index)\n        self.self.value = prune_linear_layer(self.self.value, index)\n        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n        # Update hyper params\n        self.self.num_attention_heads = self.self.num_attention_heads - len(heads)\n        self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n\n    def forward(self, input_tensor, attention_mask, head_mask=None,\n            history_state=None):\n        self_outputs = self.self(input_tensor, attention_mask, head_mask,\n                history_state)\n        attention_output = self.output(self_outputs[0], input_tensor)\n        outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\n        return outputs\n\n\nclass BertIntermediate(nn.Module):\n    def __init__(self, config):\n        super(BertIntermediate, self).__init__()\n        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)\n        if isinstance(config.hidden_act, str) or (sys.version_info[0] == 2 and isinstance(config.hidden_act, unicode)):\n            self.intermediate_act_fn = ACT2FN[config.hidden_act]\n        else:\n            self.intermediate_act_fn = config.hidden_act\n\n    def forward(self, hidden_states):\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.intermediate_act_fn(hidden_states)\n        return hidden_states\n\n\nclass BertOutput(nn.Module):\n    def __init__(self, config):\n        super(BertOutput, self).__init__()\n        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n        self.LayerNorm = LayerNormClass(config.hidden_size, eps=config.layer_norm_eps)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n\n    def forward(self, hidden_states, input_tensor):\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.dropout(hidden_states)\n        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n        return hidden_states\n\n\nclass BertLayer(nn.Module):\n    def __init__(self, config):\n        super(BertLayer, self).__init__()\n        self.attention = BertAttention(config)\n        self.intermediate = BertIntermediate(config)\n        self.output = BertOutput(config)\n\n    def forward(self, hidden_states, attention_mask, head_mask=None,\n                history_state=None):\n        attention_outputs = self.attention(hidden_states, attention_mask,\n                head_mask, history_state)\n        attention_output = attention_outputs[0]\n        intermediate_output = self.intermediate(attention_output)\n        layer_output = self.output(intermediate_output, attention_output)\n        outputs = (layer_output,) + attention_outputs[1:]  # add attentions if we output them\n        return outputs\n\nclass TIMMVitEncoder(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        logger.info(config)\n        from src import timm\n        logger.info('Loading network: {}'.format(config.net))\n        logger.info('pretrained: {}'.format(config.pretrained))\n        extra_param = getattr(config, 'timm_param', {})\n        model = timm.create_model(\n            config.net, pretrained=config.pretrained,\n            **extra_param,\n        )\n        self.blocks = model.blocks\n        self.patch_embed = model.patch_embed\n        self.pos_drop = model.pos_drop\n        self.pos_embed = model.pos_embed\n\n    def forward(self, hidden_states, attention_mask, head_mask=None,\n                encoder_history_states=None):\n        assert all(m is None for m in head_mask), 'not supported'\n        assert encoder_history_states is None, 'not supported'\n\n        for blk in self.blocks:\n            # hidden_states = blk(hidden_states)\n            hidden_states = blk(hidden_states, attention_mask)\n        return (hidden_states,)\n\nclass BertEncoder(nn.Module):\n    def __init__(self, config):\n        super(BertEncoder, self).__init__()\n        self.output_attentions = config.output_attentions\n        self.output_hidden_states = config.output_hidden_states\n        self.layer = nn.ModuleList([BertLayer(config) for _ in range(config.num_hidden_layers)])\n    \n    def set_output_attentions(self, flag):\n        for idx in range(len(self.layer)):\n            self.layer[idx].attention.self.output_attentions = flag\n        self.output_attentions = flag\n\n    def forward(self, hidden_states, attention_mask, head_mask=None,\n                encoder_history_states=None):\n        all_hidden_states = ()\n        all_attentions = ()\n        for i, layer_module in enumerate(self.layer):\n            if self.output_hidden_states:\n                all_hidden_states = all_hidden_states + (hidden_states,)\n\n            history_state = None if encoder_history_states is None else encoder_history_states[i]\n            # layer_outputs = layer_module(\n            #         hidden_states, attention_mask, head_mask[i],\n            #         history_state)\n            layer_outputs = layer_module(\n                hidden_states, attention_mask,\n                (None if head_mask is None else head_mask[i]),\n                history_state,\n            )\n            hidden_states = layer_outputs[0]\n\n            if self.output_attentions:\n                all_attentions = all_attentions + (layer_outputs[1],)\n\n        outputs = (hidden_states,)\n        if self.output_hidden_states:\n            outputs = outputs + (all_hidden_states,)\n        if self.output_attentions:\n            outputs = outputs + (all_attentions,)\n        return outputs  # outputs, (hidden states), (attentions)\n\n\nclass BertPooler(nn.Module):\n    def __init__(self, config):\n        super(BertPooler, self).__init__()\n        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n        self.activation = nn.Tanh()\n\n    def forward(self, hidden_states):\n        # We \"pool\" the model by simply taking the hidden state corresponding\n        # to the first token.\n        first_token_tensor = hidden_states[:, 0]\n        pooled_output = self.dense(first_token_tensor)\n        pooled_output = self.activation(pooled_output)\n        return pooled_output\n\n\nclass BertPredictionHeadTransform(nn.Module):\n    def __init__(self, config):\n        super(BertPredictionHeadTransform, self).__init__()\n        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n        if isinstance(config.hidden_act, str) or (sys.version_info[0] == 2 and isinstance(config.hidden_act, unicode)):\n            self.transform_act_fn = ACT2FN[config.hidden_act]\n        else:\n            self.transform_act_fn = config.hidden_act\n        self.LayerNorm = LayerNormClass(config.hidden_size, eps=config.layer_norm_eps)\n\n    def forward(self, hidden_states):\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.transform_act_fn(hidden_states)\n        hidden_states = self.LayerNorm(hidden_states)\n        return hidden_states\n\n\nclass BertLMPredictionHead(nn.Module):\n    def __init__(self, config):\n        super(BertLMPredictionHead, self).__init__()\n        self.transform = BertPredictionHeadTransform(config)\n\n        # The output weights are the same as the input embeddings, but there is\n        # an output-only bias for each token.\n        self.decoder = nn.Linear(config.hidden_size,\n                                 config.vocab_size,\n                                 bias=False)\n\n        self.bias = nn.Parameter(torch.zeros(config.vocab_size))\n\n    def forward(self, hidden_states):\n        hidden_states = self.transform(hidden_states)\n        hidden_states = self.decoder(hidden_states) + self.bias\n        return hidden_states\n\n\n# cclin\nclass BertIFPredictionHead(nn.Module):\n    # image feature\n    def __init__(self, config):\n        super(BertIFPredictionHead, self).__init__()\n        self.transform = BertPredictionHeadTransform(config)\n\n        # The output weights are the same as the input embeddings, but there is\n        # an output-only bias for each token.\n        self.decoder = nn.Linear(config.hidden_size,\n                                 2048,  # TODO: HACK!! cclin\n                                 bias=False)\n                                 # config.vocab_size,\n\n        self.bias = nn.Parameter(torch.zeros(2048))  # TODO: HACK!! cclin\n\n    def forward(self, hidden_states):\n        hidden_states = self.transform(hidden_states)\n        hidden_states = self.decoder(hidden_states) + self.bias\n        return torch.nn.functional.relu(hidden_states)\n        #return hidden_states\n\n\nclass BertOnlyMLMHead(nn.Module):\n    def __init__(self, config):\n        super(BertOnlyMLMHead, self).__init__()\n        self.predictions = BertLMPredictionHead(config)\n\n    def forward(self, sequence_output):\n        prediction_scores = self.predictions(sequence_output)\n        return prediction_scores\n\n\nclass BertOnlyNSPHead(nn.Module):\n    def __init__(self, config):\n        super(BertOnlyNSPHead, self).__init__()\n        self.seq_relationship = nn.Linear(config.hidden_size, 2)\n\n    def forward(self, pooled_output):\n        seq_relationship_score = self.seq_relationship(pooled_output)\n        return seq_relationship_score\n\n\nclass BertPreTrainingHeads(nn.Module):\n    def __init__(self, config):\n        super(BertPreTrainingHeads, self).__init__()\n        self.predictions = BertLMPredictionHead(config)\n        num_seq_relations = config.num_contrast_classes if hasattr(config, \"num_contrast_classes\") else 2\n        self.seq_relationship = nn.Linear(config.hidden_size, num_seq_relations)\n\n    def forward(self, sequence_output, pooled_output):\n        prediction_scores = self.predictions(sequence_output)\n        seq_relationship_score = self.seq_relationship(pooled_output)\n        return prediction_scores, seq_relationship_score\n\n\nclass BertPreTrainedModel(PreTrainedModel):\n    \"\"\" An abstract class to handle weights initialization and\n        a simple interface for dowloading and loading pretrained models.\n    \"\"\"\n    config_class = BertConfig\n    pretrained_model_archive_map = BERT_PRETRAINED_MODEL_ARCHIVE_MAP\n    load_tf_weights = load_tf_weights_in_bert\n    base_model_prefix = \"bert\"\n\n    def __init__(self, *inputs, **kwargs):\n        super(BertPreTrainedModel, self).__init__(*inputs, **kwargs)\n\n    def init_weights(self, module):\n        \"\"\" Initialize the weights.\n        \"\"\"\n        if isinstance(module, (nn.Linear, nn.Embedding)):\n            # Slightly different from the TF version which uses truncated_normal for initialization\n            # cf https://github.com/pytorch/pytorch/pull/5617\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n        elif isinstance(module, BertLayerNorm) or isinstance(module, LayerNormClass):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        if isinstance(module, nn.Linear) and module.bias is not None:\n            module.bias.data.zero_()\n\n\n\n\n\nBERT_START_DOCSTRING = r\"\"\"    The BERT model was proposed in\n    `BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding`_\n    by Jacob Devlin, Ming-Wei Chang, Kenton Lee and Kristina Toutanova. It's a bidirectional transformer\n    pre-trained using a combination of masked language modeling objective and next sentence prediction\n    on a large corpus comprising the Toronto Book Corpus and Wikipedia.\n\n    This model is a PyTorch `torch.nn.Module`_ sub-class. Use it as a regular PyTorch Module and\n    refer to the PyTorch documentation for all matter related to general usage and behavior.\n\n    .. _`BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding`:\n        https://arxiv.org/abs/1810.04805\n\n    .. _`torch.nn.Module`:\n        https://pytorch.org/docs/stable/nn.html#module\n\n    Parameters:\n        config (:class:`~pytorch_transformers.BertConfig`): Model configuration class with all the parameters of the model.\n\"\"\"\n\nBERT_INPUTS_DOCSTRING = r\"\"\"\n    Inputs:\n        **input_ids**: ``torch.LongTensor`` of shape ``(batch_size, sequence_length)``:\n            Indices of input sequence tokens in the vocabulary.\n            To match pre-training, BERT input sequence should be formatted with [CLS] and [SEP] tokens as follows:\n\n            (a) For sequence pairs:\n\n                ``tokens:         [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]``\n\n                ``token_type_ids:   0   0  0    0    0     0       0   0   1  1  1  1   1   1``\n\n            (b) For single sequences:\n\n                ``tokens:         [CLS] the dog is hairy . [SEP]``\n\n                ``token_type_ids:   0   0   0   0  0     0   0``\n\n            Indices can be obtained using :class:`pytorch_transformers.BertTokenizer`.\n            See :func:`pytorch_transformers.PreTrainedTokenizer.encode` and\n            :func:`pytorch_transformers.PreTrainedTokenizer.convert_tokens_to_ids` for details.\n        **position_ids**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size, sequence_length)``:\n            Indices of positions of each input sequence tokens in the position embeddings.\n            Selected in the range ``[0, config.max_position_embeddings - 1[``.\n        **token_type_ids**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size, sequence_length)``:\n            Segment token indices to indicate first and second portions of the inputs.\n            Indices are selected in ``[0, 1]``: ``0`` corresponds to a `sentence A` token, ``1``\n            corresponds to a `sentence B` token\n            (see `BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding`_ for more details).\n        **attention_mask**: (`optional`) ``torch.FloatTensor`` of shape ``(batch_size, sequence_length)``:\n            Mask to avoid performing attention on padding token indices.\n            Mask values selected in ``[0, 1]``:\n            ``1`` for tokens that are NOT MASKED, ``0`` for MASKED tokens.\n        **head_mask**: (`optional`) ``torch.FloatTensor`` of shape ``(num_heads,)`` or ``(num_layers, num_heads)``:\n            Mask to nullify selected heads of the self-attention modules.\n            Mask values selected in ``[0, 1]``:\n            ``1`` indicates the head is **not masked**, ``0`` indicates the head is **masked**.\n\"\"\"\n\n@add_start_docstrings(\"The bare Bert Model transformer outputing raw hidden-states without any specific head on top.\",\n                      BERT_START_DOCSTRING, BERT_INPUTS_DOCSTRING)\nclass BertModel(BertPreTrainedModel):\n    r\"\"\"\n    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n        **last_hidden_state**: ``torch.FloatTensor`` of shape ``(batch_size, sequence_length, hidden_size)``\n            Sequence of hidden-states at the output of the last layer of the model.\n        **pooler_output**: ``torch.FloatTensor`` of shape ``(batch_size, hidden_size)``\n            Last layer hidden-state of the first token of the sequence (classification token)\n            further processed by a Linear layer and a Tanh activation function. The Linear\n            layer weights are trained from the next sentence prediction (classification)\n            objective during Bert pretraining. This output is usually *not* a good summary\n            of the semantic content of the input, you're often better with averaging or pooling\n            the sequence of hidden-states for the whole input sequence.\n        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\n            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\n            of shape ``(batch_size, sequence_length, hidden_size)``:\n            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\n            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n\n    Examples::\n\n        >>> config = BertConfig.from_pretrained('bert-base-uncased')\n        >>> tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n        >>> model = BertModel(config)\n        >>> input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\")).unsqueeze(0)  # Batch size 1\n        >>> outputs = model(input_ids)\n        >>> last_hidden_states = outputs[0]  # The last hidden-state is the first element of the output tuple\n\n    \"\"\"\n    def __init__(self, config):\n        super(BertModel, self).__init__(config)\n\n        self.embeddings = BertEmbeddings(config)\n        self.encoder = BertEncoder(config)\n        self.pooler = BertPooler(config)\n\n        self.apply(self.init_weights)\n\n    def _resize_token_embeddings(self, new_num_tokens):\n        old_embeddings = self.embeddings.word_embeddings\n        new_embeddings = self._get_resized_embeddings(old_embeddings, new_num_tokens)\n        self.embeddings.word_embeddings = new_embeddings\n        return self.embeddings.word_embeddings\n\n    def _prune_heads(self, heads_to_prune):\n        \"\"\" Prunes heads of the model.\n            heads_to_prune: dict of {layer_num: list of heads to prune in this layer}\n            See base class PreTrainedModel\n        \"\"\"\n        for layer, heads in heads_to_prune.items():\n            self.encoder.layer[layer].attention.prune_heads(heads)\n\n    def forward(self, input_ids, token_type_ids=None, attention_mask=None, position_ids=None, head_mask=None):\n        if attention_mask is None:\n            attention_mask = torch.ones_like(input_ids)\n        if token_type_ids is None:\n            token_type_ids = torch.zeros_like(input_ids)\n\n        # We create a 3D attention mask from a 2D tensor mask.\n        # Sizes are [batch_size, 1, 1, to_seq_length]\n        # So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]\n        # this attention mask is more simple than the triangular masking of causal attention\n        # used in OpenAI GPT, we just need to prepare the broadcast dimension here.\n        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n\n        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n        # masked positions, this operation will create a tensor which is 0.0 for\n        # positions we want to attend and -10000.0 for masked positions.\n        # Since we are adding it to the raw scores before the softmax, this is\n        # effectively the same as removing these entirely.\n        extended_attention_mask = extended_attention_mask.to(dtype=next(self.parameters()).dtype) # fp16 compatibility\n        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n\n        # Prepare head mask if needed\n        # 1.0 in head_mask indicate we keep the head\n        # attention_probs has shape bsz x n_heads x N x N\n        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n        if head_mask is not None:\n            if head_mask.dim() == 1:\n                head_mask = head_mask.unsqueeze(0).unsqueeze(0).unsqueeze(-1).unsqueeze(-1)\n                head_mask = head_mask.expand(self.config.num_hidden_layers, -1, -1, -1, -1)\n            elif head_mask.dim() == 2:\n                head_mask = head_mask.unsqueeze(1).unsqueeze(-1).unsqueeze(-1)  # We can specify head_mask for each layer\n            head_mask = head_mask.to(dtype=next(self.parameters()).dtype) # switch to fload if need + fp16 compatibility\n        else:\n            head_mask = [None] * self.config.num_hidden_layers\n\n        embedding_output = self.embeddings(input_ids, position_ids=position_ids, token_type_ids=token_type_ids)\n        # add img_embedding_output and sum with embedding_output\n        #logger.info('embedding_output: %s' % str(embedding_output.shape))\n\n        encoder_outputs = self.encoder(embedding_output,\n                                       extended_attention_mask,\n                                       head_mask=head_mask)\n        sequence_output = encoder_outputs[0]\n        pooled_output = self.pooler(sequence_output)\n\n        outputs = (sequence_output, pooled_output,) + encoder_outputs[1:]  # add hidden_states and attentions if they are here\n        return outputs  # sequence_output, pooled_output, (hidden_states), (attentions)\n\n\nclass BertImgModel(BertPreTrainedModel):\n    r\"\"\"\n    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n        **last_hidden_state**: ``torch.FloatTensor`` of shape ``(batch_size, sequence_length, hidden_size)``\n            Sequence of hidden-states at the output of the last layer of the model.\n        **pooler_output**: ``torch.FloatTensor`` of shape ``(batch_size, hidden_size)``\n            Last layer hidden-state of the first token of the sequence (classification token)\n            further processed by a Linear layer and a Tanh activation function. The Linear\n            layer weights are trained from the next sentence prediction (classification)\n            objective during Bert pretraining. This output is usually *not* a good summary\n            of the semantic content of the input, you're often better with averaging or pooling\n            the sequence of hidden-states for the whole input sequence.\n        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\n            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\n            of shape ``(batch_size, sequence_length, hidden_size)``:\n            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\n            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n\n    Examples::\n\n        >>> config = BertConfig.from_pretrained('bert-base-uncased')\n        >>> tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n        >>> model = BertModel(config)\n        >>> input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\")).unsqueeze(0)  # Batch size 1\n        >>> outputs = model(input_ids)\n        >>> last_hidden_states = outputs[0]  # The last hidden-state is the first element of the output tuple\n\n    \"\"\"\n    def __init__(self, config):\n        super(BertImgModel, self).__init__(config)\n\n        self.embeddings = BertEmbeddings(config)\n        self.encoder = BertEncoder(config)\n        self.pooler = BertPooler(config)\n\n        self.img_dim = config.img_feature_dim #2054 #565\n        logger.info('BertImgModel Image Dimension: {}'.format(self.img_dim))\n        self.img_feature_type = config.img_feature_type\n        try:\n            self.use_img_layernorm = config.use_img_layernorm\n        except:\n            self.use_img_layernorm = None\n\n        if config.img_feature_type == 'dis_code':\n            self.code_embeddings = nn.Embedding(config.code_voc, config.code_dim, padding_idx=0)\n            self.img_embedding = nn.Linear(config.code_dim, self.config.hidden_size, bias=True)\n        elif config.img_feature_type == 'dis_code_t': # transpose\n            self.code_embeddings = nn.Embedding(config.code_voc, config.code_dim, padding_idx=0)\n            self.img_embedding = nn.Linear(config.code_size, self.config.hidden_size, bias=True)\n        elif config.img_feature_type == 'dis_code_scale': # scaled\n            self.input_embeddings = nn.Linear(config.code_dim, config.code_size, bias=True)\n            self.code_embeddings = nn.Embedding(config.code_voc, config.code_dim, padding_idx=0)\n            self.img_embedding = nn.Linear(config.code_dim, self.config.hidden_size, bias=True)\n        else:\n            self.img_embedding = nn.Linear(self.img_dim, self.config.hidden_size, bias=True)\n            self.dropout = nn.Dropout(config.hidden_dropout_prob)\n            if self.use_img_layernorm:\n                self.LayerNorm = LayerNormClass(config.hidden_size, eps=config.img_layer_norm_eps)\n\n        self.apply(self.init_weights)\n        self.model_type = getattr(config, 'model_type', 'bert')\n        if self.model_type == 'TIMM_vit':\n            self.encoder = TIMMVitEncoder(config)\n\n        # re-initialize img_embedding weight\n        # self.img_embedding.weight.data.normal_(mean=0.0, std=config.img_initializer_range)\n\n    def _resize_token_embeddings(self, new_num_tokens):\n        old_embeddings = self.embeddings.word_embeddings\n        new_embeddings = self._get_resized_embeddings(old_embeddings, new_num_tokens)\n        self.embeddings.word_embeddings = new_embeddings\n        return self.embeddings.word_embeddings\n\n    def _prune_heads(self, heads_to_prune):\n        \"\"\" Prunes heads of the model.\n            heads_to_prune: dict of {layer_num: list of heads to prune in this layer}\n            See base class PreTrainedModel\n        \"\"\"\n        for layer, heads in heads_to_prune.items():\n            self.encoder.layer[layer].attention.prune_heads(heads)\n\n    def forward(self, input_ids, token_type_ids=None, attention_mask=None,\n            position_ids=None, head_mask=None, img_feats=None,\n            encoder_history_states=None):\n\n        if attention_mask is None:\n            if img_feats is not None:\n                attention_mask = torch.ones((input_ids.shape[0], input_ids.shape[1] + img_feats.shape[1]), device=input_ids.device)\n            else:\n                attention_mask = torch.ones_like(input_ids)\n            #if img_feats is not None: attention_mask = torch.ones_like((input_ids.shape[0], input_ids.shape[1]+img_feats.shape[1]))\n            #else: attention_mask = torch.ones_like(input_ids)\n\n        if token_type_ids is None:\n            token_type_ids = torch.zeros_like(input_ids)\n\n        # We create a 3D attention mask from a 2D tensor mask.\n        # Sizes are [batch_size, 1, 1, to_seq_length]\n        # So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]\n        # this attention mask is more simple than the triangular masking of causal attention\n        # used in OpenAI GPT, we just need to prepare the broadcast dimension here.\n        if attention_mask.dim() == 2:\n            extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n        elif attention_mask.dim() == 3:\n            extended_attention_mask = attention_mask.unsqueeze(1)\n        else:\n            raise NotImplementedError\n\n        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n        # masked positions, this operation will create a tensor which is 0.0 for\n        # positions we want to attend and -10000.0 for masked positions.\n        # Since we are adding it to the raw scores before the softmax, this is\n        # effectively the same as removing these entirely.\n        extended_attention_mask = extended_attention_mask.to(dtype=next(self.parameters()).dtype) # fp16 compatibility\n        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n\n        # Prepare head mask if needed\n        # 1.0 in head_mask indicate we keep the head\n        # attention_probs has shape bsz x n_heads x N x N\n        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n        if head_mask is not None:\n            if head_mask.dim() == 1:\n                head_mask = head_mask.unsqueeze(0).unsqueeze(0).unsqueeze(-1).unsqueeze(-1)\n                head_mask = head_mask.expand(self.config.num_hidden_layers, -1, -1, -1, -1)\n            elif head_mask.dim() == 2:\n                head_mask = head_mask.unsqueeze(1).unsqueeze(-1).unsqueeze(-1)  # We can specify head_mask for each layer\n            head_mask = head_mask.to(dtype=next(self.parameters()).dtype) # switch to fload if need + fp16 compatibility\n        else:\n            head_mask = [None] * self.config.num_hidden_layers\n\n        embedding_output = self.embeddings(input_ids, position_ids=position_ids,\n                token_type_ids=token_type_ids)\n        # add img_embedding_output and sum with embedding_output\n        #logger.info('embedding_output: %s' % str(embedding_output.shape))\n        if encoder_history_states is not None:\n            if encoder_history_states[0].shape[1] != 0:\n                assert img_feats is None or img_feats.shape[1]==0, \"Cannot take image features while using encoder history states\"\n\n        if img_feats is not None:\n            if self.img_feature_type == 'dis_code':\n                code_emb = self.code_embeddings(img_feats)\n                img_embedding_output = self.img_embedding(code_emb)\n            elif self.img_feature_type == 'dis_code_t': # transpose\n                code_emb = self.code_embeddings(img_feats)\n                code_emb = code_emb.permute(0, 2, 1)\n                img_embedding_output = self.img_embedding(code_emb)\n            elif self.img_feature_type == 'dis_code_scale': # left scaled\n                code_emb = self.code_embeddings(img_feats)\n                #scale_output =\n                # add scale ouput\n                img_embedding_output = self.img_embedding(code_emb)\n            elif self.img_feature_type == 'e2e' and self.model_type == 'TIMM_vit':\n                img_embedding_output = img_feats\n            else:\n                if torch._C._get_tracing_state():\n                    # Ugly workaround to make this work for ONNX.\n                    #  It is also valid for PyTorch bu I keep this path separate to remove once fixed in ONNX\n                    img_embedding_output = self.img_embedding(img_feats.squeeze(0)).unsqueeze(0)\n                else:\n                    img_embedding_output = self.img_embedding(img_feats)\n                #logger.info('img_embedding_output: %s' % str(img_embedding_output.shape))\n                if self.use_img_layernorm:\n                    img_embedding_output = self.LayerNorm(img_embedding_output)\n\n                # add dropout on image embedding\n                img_embedding_output = self.dropout(img_embedding_output)\n\n                # sum two embeddings\n                #padding_matrix = torch.zeros((embedding_output.shape[0], embedding_output.shape[1]-img_embedding_output.shape[1], embedding_output.shape[2])).cuda()\n                #img_embedding_output = torch.cat((padding_matrix, img_embedding_output), 1)\n                #embedding_output = embedding_output + img_embedding_output\n\n            # concatenate two embeddings\n            embedding_output = torch.cat((embedding_output, img_embedding_output), 1)\n\n        encoder_outputs = self.encoder(embedding_output,\n                extended_attention_mask, head_mask=head_mask,\n                encoder_history_states=encoder_history_states)\n        sequence_output = encoder_outputs[0]\n        pooled_output = self.pooler(sequence_output)\n\n        outputs = (sequence_output, pooled_output,) + encoder_outputs[1:]  # add hidden_states and attentions if they are here\n        return outputs  # sequence_output, pooled_output, (hidden_states), (attentions)\n\n# Place_Holder\nclass LangImgModel(BertPreTrainedModel):\n    r\"\"\"\n    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n        **last_hidden_state**: ``torch.FloatTensor`` of shape ``(batch_size, sequence_length, hidden_size)``\n            Sequence of hidden-states at the output of the last layer of the model.\n        **pooler_output**: ``torch.FloatTensor`` of shape ``(batch_size, hidden_size)``\n            Last layer hidden-state of the first token of the sequence (classification token)\n            further processed by a Linear layer and a Tanh activation function. The Linear\n            layer weights are trained from the next sentence prediction (classification)\n            objective during Bert pretraining. This output is usually *not* a good summary\n            of the semantic content of the input, you're often better with averaging or pooling\n            the sequence of hidden-states for the whole input sequence.\n        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\n            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\n            of shape ``(batch_size, sequence_length, hidden_size)``:\n            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\n            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n\n    Examples::\n\n        >>> config = BertConfig.from_pretrained('bert-base-uncased')\n        >>> tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n        >>> model = BertModel(config)\n        >>> input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\")).unsqueeze(0)  # Batch size 1\n        >>> outputs = model(input_ids)\n        >>> last_hidden_states = outputs[0]  # The last hidden-state is the first element of the output tuple\n\n    \"\"\"\n    def __init__(self, config):\n        super(LangImgModel, self).__init__(config)\n\n        self.embeddings = BertEmbeddings(config)\n        self.img_embedding = BertImgEmbeddings(config)\n\n        self.encoder = BertEncoder(config)\n        self.pooler = BertPooler(config)\n\n        #self.img_embedding = nn.Linear(565, self.config.hidden_size, bias=True)\n\n        self.apply(self.init_weights)\n\n    def _resize_token_embeddings(self, new_num_tokens):\n        old_embeddings = self.embeddings.word_embeddings\n        new_embeddings = self._get_resized_embeddings(old_embeddings, new_num_tokens)\n        self.embeddings.word_embeddings = new_embeddings\n        return self.embeddings.word_embeddings\n\n    def _prune_heads(self, heads_to_prune):\n        \"\"\" Prunes heads of the model.\n            heads_to_prune: dict of {layer_num: list of heads to prune in this layer}\n            See base class PreTrainedModel\n        \"\"\"\n        for layer, heads in heads_to_prune.items():\n            self.encoder.layer[layer].attention.prune_heads(heads)\n\n    def forward(self, input_ids, token_type_ids=None, attention_mask=None, position_ids=None, head_mask=None, img_feats=None):\n        if attention_mask is None:\n            attention_mask = torch.ones_like(input_ids)\n            #if img_feats is not None:\n            #    attention_mask = torch.ones_like((input_ids.shape[0], input_ids.shape[1]+img_feats.shape[1]))\n            #else: attention_mask = torch.ones_like(input_ids)\n\n        if token_type_ids is None:\n            token_type_ids = torch.zeros_like(input_ids)\n\n        # We create a 3D attention mask from a 2D tensor mask.\n        # Sizes are [batch_size, 1, 1, to_seq_length]\n        # So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]\n        # this attention mask is more simple than the triangular masking of causal attention\n        # used in OpenAI GPT, we just need to prepare the broadcast dimension here.\n        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n\n        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n        # masked positions, this operation will create a tensor which is 0.0 for\n        # positions we want to attend and -10000.0 for masked positions.\n        # Since we are adding it to the raw scores before the softmax, this is\n        # effectively the same as removing these entirely.\n        extended_attention_mask = extended_attention_mask.to(dtype=next(self.parameters()).dtype) # fp16 compatibility\n        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n\n        # Prepare head mask if needed\n        # 1.0 in head_mask indicate we keep the head\n        # attention_probs has shape bsz x n_heads x N x N\n        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n        if head_mask is not None:\n            if head_mask.dim() == 1:\n                head_mask = head_mask.unsqueeze(0).unsqueeze(0).unsqueeze(-1).unsqueeze(-1)\n                head_mask = head_mask.expand(self.config.num_hidden_layers, -1, -1, -1, -1)\n            elif head_mask.dim() == 2:\n                head_mask = head_mask.unsqueeze(1).unsqueeze(-1).unsqueeze(-1)  # We can specify head_mask for each layer\n            head_mask = head_mask.to(dtype=next(self.parameters()).dtype) # switch to fload if need + fp16 compatibility\n        else:\n            head_mask = [None] * self.config.num_hidden_layers\n\n        embedding_output = self.embeddings(input_ids, position_ids=position_ids, token_type_ids=token_type_ids)\n        # add img_embedding_output and sum with embedding_output\n        #logger.info('embedding_output: %s' % str(embedding_output.shape))\n        if img_feats is not None:\n            img_embedding_output = self.img_embedding(img_feats)\n            #logger.info('img_embedding_output: %s' % str(img_embedding_output.shape))\n\n            # sum two embeddings\n            #padding_matrix = torch.zeros((embedding_output.shape[0], embedding_output.shape[1]-img_embedding_output.shape[1], embedding_output.shape[2])).cuda()\n            #img_embedding_output = torch.cat((padding_matrix, img_embedding_output), 1)\n            #embedding_output = embedding_output + img_embedding_output\n\n            # concatenate two embeddings\n            embedding_output = torch.cat((embedding_output, img_embedding_output), 1)\n\n        encoder_outputs = self.encoder(embedding_output, extended_attention_mask, head_mask=head_mask)\n        sequence_output = encoder_outputs[0]\n        pooled_output = self.pooler(sequence_output)\n\n        outputs = (sequence_output, pooled_output,) + encoder_outputs[1:]  # add hidden_states and attentions if they are here\n        return outputs  # sequence_output, pooled_output, (hidden_states), (attentions)\n\n\n@add_start_docstrings(\"\"\"Bert Model with two heads on top as done during the pre-training:\n    a `masked language modeling` head and a `next sentence prediction (classification)` head. \"\"\",\n    BERT_START_DOCSTRING, BERT_INPUTS_DOCSTRING)\nclass BertForPreTraining(BertPreTrainedModel):\n    r\"\"\"\n        **masked_lm_labels**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size, sequence_length)``:\n            Labels for computing the masked language modeling loss.\n            Indices should be in ``[-1, 0, ..., config.vocab_size]`` (see ``input_ids`` docstring)\n            Tokens with indices set to ``-1`` are ignored (masked), the loss is only computed for the tokens with labels\n            in ``[0, ..., config.vocab_size]``\n        **next_sentence_label**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size,)``:\n            Labels for computing the next sequence prediction (classification) loss. Input should be a sequence pair (see ``input_ids`` docstring)\n            Indices should be in ``[0, 1]``.\n            ``0`` indicates sequence B is a continuation of sequence A,\n            ``1`` indicates sequence B is a random sequence.\n\n    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n        **loss**: (`optional`, returned when both ``masked_lm_labels`` and ``next_sentence_label`` are provided) ``torch.FloatTensor`` of shape ``(1,)``:\n            Total loss as the sum of the masked language modeling loss and the next sequence prediction (classification) loss.\n        **prediction_scores**: ``torch.FloatTensor`` of shape ``(batch_size, sequence_length, config.vocab_size)``\n            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n        **seq_relationship_scores**: ``torch.FloatTensor`` of shape ``(batch_size, sequence_length, 2)``\n            Prediction scores of the next sequence prediction (classification) head (scores of True/False continuation before SoftMax).\n        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\n            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\n            of shape ``(batch_size, sequence_length, hidden_size)``:\n            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\n            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n\n    Examples::\n\n        >>> config = BertConfig.from_pretrained('bert-base-uncased')\n        >>> tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n        >>>\n        >>> model = BertForPreTraining(config)\n        >>> input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\")).unsqueeze(0)  # Batch size 1\n        >>> outputs = model(input_ids)\n        >>> prediction_scores, seq_relationship_scores = outputs[:2]\n\n    \"\"\"\n    def __init__(self, config):\n        super(BertForPreTraining, self).__init__(config)\n\n        self.bert = BertModel(config)\n        self.cls = BertPreTrainingHeads(config)\n\n        self.apply(self.init_weights)\n        self.tie_weights()\n\n    def tie_weights(self):\n        \"\"\" Make sure we are sharing the input and output embeddings.\n            Export to TorchScript can't handle parameter sharing so we are cloning them instead.\n        \"\"\"\n        self._tie_or_clone_weights(self.cls.predictions.decoder,\n                                   self.bert.embeddings.word_embeddings)\n\n    def forward(self, input_ids, token_type_ids=None, attention_mask=None, masked_lm_labels=None,\n                next_sentence_label=None, position_ids=None, head_mask=None):\n        outputs = self.bert(input_ids, position_ids=position_ids, token_type_ids=token_type_ids,\n                            attention_mask=attention_mask, head_mask=head_mask)\n\n        sequence_output, pooled_output = outputs[:2]\n        prediction_scores, seq_relationship_score = self.cls(sequence_output, pooled_output)\n\n        outputs = (prediction_scores, seq_relationship_score,) + outputs[2:]  # add hidden states and attention if they are here\n\n        if masked_lm_labels is not None and next_sentence_label is not None:\n            loss_fct = CrossEntropyLoss(ignore_index=-1)\n            masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), masked_lm_labels.view(-1))\n            next_sentence_loss = loss_fct(seq_relationship_score.view(-1, 2), next_sentence_label.view(-1))\n            total_loss = masked_lm_loss + next_sentence_loss\n            outputs = (total_loss,) + outputs\n\n        return outputs  # (loss), prediction_scores, seq_relationship_score, (hidden_states), (attentions)\n\n\nclass BertImgForPreTraining(BertPreTrainedModel):\n    r\"\"\"\n        **masked_lm_labels**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size, sequence_length)``:\n            Labels for computing the masked language modeling loss.\n            Indices should be in ``[-1, 0, ..., config.vocab_size]`` (see ``input_ids`` docstring)\n            Tokens with indices set to ``-1`` are ignored (masked), the loss is only computed for the tokens with labels\n            in ``[0, ..., config.vocab_size]``\n        **next_sentence_label**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size,)``:\n            Labels for computing the next sequence prediction (classification) loss. Input should be a sequence pair (see ``input_ids`` docstring)\n            Indices should be in ``[0, 1]``.\n            ``0`` indicates sequence B is a continuation of sequence A,\n            ``1`` indicates sequence B is a random sequence.\n\n    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n        **loss**: (`optional`, returned when both ``masked_lm_labels`` and ``next_sentence_label`` are provided) ``torch.FloatTensor`` of shape ``(1,)``:\n            Total loss as the sum of the masked language modeling loss and the next sequence prediction (classification) loss.\n        **prediction_scores**: ``torch.FloatTensor`` of shape ``(batch_size, sequence_length, config.vocab_size)``\n            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n        **seq_relationship_scores**: ``torch.FloatTensor`` of shape ``(batch_size, sequence_length, 2)``\n            Prediction scores of the next sequence prediction (classification) head (scores of True/False continuation before SoftMax).\n        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\n            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\n            of shape ``(batch_size, sequence_length, hidden_size)``:\n            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\n            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n\n    Examples::\n\n        >>> config = BertConfig.from_pretrained('bert-base-uncased')\n        >>> tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n        >>>\n        >>> model = BertForPreTraining(config)\n        >>> input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\")).unsqueeze(0)  # Batch size 1\n        >>> outputs = model(input_ids)\n        >>> prediction_scores, seq_relationship_scores = outputs[:2]\n\n    \"\"\"\n    def __init__(self, config):\n        super(BertImgForPreTraining, self).__init__(config)\n\n        #self.bert = BertModel(config) # original BERT\n        self.bert = BertImgModel(config)\n        self.cls = BertPreTrainingHeads(config)\n        self.num_seq_relations = config.num_contrast_classes if hasattr(config, \"num_contrast_classes\") else 2\n\n        self.apply(self.init_weights)\n        self.tie_weights()\n\n    def tie_weights(self):\n        \"\"\" Make sure we are sharing the input and output embeddings.\n            Export to TorchScript can't handle parameter sharing so we are cloning them instead.\n        \"\"\"\n        self._tie_or_clone_weights(self.cls.predictions.decoder,\n                                   self.bert.embeddings.word_embeddings)\n\n    def forward(self, input_ids, token_type_ids=None, attention_mask=None, masked_lm_labels=None,\n            next_sentence_label=None, position_ids=None, head_mask=None, img_feats=None):\n        outputs = self.bert(input_ids, position_ids=position_ids, token_type_ids=token_type_ids,\n                            attention_mask=attention_mask, head_mask=head_mask, img_feats=img_feats)\n\n        sequence_output, pooled_output = outputs[:2]\n        prediction_scores, seq_relationship_score = self.cls(sequence_output, pooled_output)\n\n        outputs = (prediction_scores, seq_relationship_score,) + outputs[2:]  # add hidden states and attention if they are here\n\n        if masked_lm_labels is not None and next_sentence_label is not None:\n            loss_fct = CrossEntropyLoss(ignore_index=-1)\n            masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), masked_lm_labels.view(-1))\n            next_sentence_loss = loss_fct(seq_relationship_score.view(-1, self.num_seq_relations), next_sentence_label.view(-1))\n            total_loss = masked_lm_loss + next_sentence_loss\n            outputs = (total_loss,) + outputs + (masked_lm_loss,)\n\n        return outputs  # (loss), prediction_scores, seq_relationship_score, (hidden_states), (attentions)\n\n\n\n@add_start_docstrings(\"\"\"Bert Model with a `language modeling` head on top. \"\"\",\n    BERT_START_DOCSTRING, BERT_INPUTS_DOCSTRING)\nclass BertForMaskedLM(BertPreTrainedModel):\n    r\"\"\"\n        **masked_lm_labels**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size, sequence_length)``:\n            Labels for computing the masked language modeling loss.\n            Indices should be in ``[-1, 0, ..., config.vocab_size]`` (see ``input_ids`` docstring)\n            Tokens with indices set to ``-1`` are ignored (masked), the loss is only computed for the tokens with labels\n            in ``[0, ..., config.vocab_size]``\n\n    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n        **loss**: (`optional`, returned when ``masked_lm_labels`` is provided) ``torch.FloatTensor`` of shape ``(1,)``:\n            Masked language modeling loss.\n        **prediction_scores**: ``torch.FloatTensor`` of shape ``(batch_size, sequence_length, config.vocab_size)``\n            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\n            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\n            of shape ``(batch_size, sequence_length, hidden_size)``:\n            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\n            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n\n    Examples::\n\n        >>> config = BertConfig.from_pretrained('bert-base-uncased')\n        >>> tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n        >>>\n        >>> model = BertForMaskedLM(config)\n        >>> input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\")).unsqueeze(0)  # Batch size 1\n        >>> outputs = model(input_ids, masked_lm_labels=input_ids)\n        >>> loss, prediction_scores = outputs[:2]\n\n    \"\"\"\n    def __init__(self, config):\n        super(BertForMaskedLM, self).__init__(config)\n\n        self.bert = BertModel(config)\n        self.cls = BertOnlyMLMHead(config)\n\n        self.apply(self.init_weights)\n        self.tie_weights()\n\n    def tie_weights(self):\n        \"\"\" Make sure we are sharing the input and output embeddings.\n            Export to TorchScript can't handle parameter sharing so we are cloning them instead.\n        \"\"\"\n        self._tie_or_clone_weights(self.cls.predictions.decoder,\n                                   self.bert.embeddings.word_embeddings)\n\n    def forward(self, input_ids, token_type_ids=None, attention_mask=None, masked_lm_labels=None,\n                position_ids=None, head_mask=None):\n        outputs = self.bert(input_ids, position_ids=position_ids, token_type_ids=token_type_ids,\n                            attention_mask=attention_mask, head_mask=head_mask)\n\n        sequence_output = outputs[0]\n        prediction_scores = self.cls(sequence_output)\n\n        outputs = (prediction_scores,) + outputs[2:]  # Add hidden states and attention is they are here\n        if masked_lm_labels is not None:\n            loss_fct = CrossEntropyLoss(ignore_index=-1)\n            masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), masked_lm_labels.view(-1))\n            outputs = (masked_lm_loss,) + outputs\n\n        return outputs  # (masked_lm_loss), prediction_scores, (hidden_states), (attentions)\n\n\n@add_start_docstrings(\"\"\"Bert Model with a `next sentence prediction (classification)` head on top. \"\"\",\n    BERT_START_DOCSTRING, BERT_INPUTS_DOCSTRING)\nclass BertForNextSentencePrediction(BertPreTrainedModel):\n    r\"\"\"\n        **next_sentence_label**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size,)``:\n            Labels for computing the next sequence prediction (classification) loss. Input should be a sequence pair (see ``input_ids`` docstring)\n            Indices should be in ``[0, 1]``.\n            ``0`` indicates sequence B is a continuation of sequence A,\n            ``1`` indicates sequence B is a random sequence.\n\n    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n        **loss**: (`optional`, returned when ``next_sentence_label`` is provided) ``torch.FloatTensor`` of shape ``(1,)``:\n            Next sequence prediction (classification) loss.\n        **seq_relationship_scores**: ``torch.FloatTensor`` of shape ``(batch_size, sequence_length, 2)``\n            Prediction scores of the next sequence prediction (classification) head (scores of True/False continuation before SoftMax).\n        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\n            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\n            of shape ``(batch_size, sequence_length, hidden_size)``:\n            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\n            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n\n    Examples::\n\n        >>> config = BertConfig.from_pretrained('bert-base-uncased')\n        >>> tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n        >>>\n        >>> model = BertForNextSentencePrediction(config)\n        >>> input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\")).unsqueeze(0)  # Batch size 1\n        >>> outputs = model(input_ids)\n        >>> seq_relationship_scores = outputs[0]\n\n    \"\"\"\n    def __init__(self, config):\n        super(BertForNextSentencePrediction, self).__init__(config)\n\n        self.bert = BertModel(config)\n        self.cls = BertOnlyNSPHead(config)\n\n        self.apply(self.init_weights)\n\n    def forward(self, input_ids, token_type_ids=None, attention_mask=None, next_sentence_label=None,\n                position_ids=None, head_mask=None):\n        outputs = self.bert(input_ids, position_ids=position_ids, token_type_ids=token_type_ids,\n                            attention_mask=attention_mask, head_mask=head_mask)\n        pooled_output = outputs[1]\n\n        seq_relationship_score = self.cls(pooled_output)\n\n        outputs = (seq_relationship_score,) + outputs[2:]  # add hidden states and attention if they are here\n        if next_sentence_label is not None:\n            loss_fct = CrossEntropyLoss(ignore_index=-1)\n            next_sentence_loss = loss_fct(seq_relationship_score.view(-1, 2), next_sentence_label.view(-1))\n            outputs = (next_sentence_loss,) + outputs\n\n        return outputs  # (next_sentence_loss), seq_relationship_score, (hidden_states), (attentions)\n\n\ndef instance_bce_with_logits(logits, labels, reduction='mean'):\n    assert logits.dim() == 2\n\n    loss = nn.functional.binary_cross_entropy_with_logits(logits, labels, reduction=reduction)\n    if reduction == 'mean':\n        loss *= labels.size(1)\n    return loss\n\n\n@add_start_docstrings(\"\"\"Bert Model transformer with a sequence classification/regression head on top (a linear layer on top of\n    the pooled output) e.g. for GLUE tasks. \"\"\",\n    BERT_START_DOCSTRING, BERT_INPUTS_DOCSTRING)\nclass BertForSequenceClassification(BertPreTrainedModel):\n    r\"\"\"\n        **labels**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size,)``:\n            Labels for computing the sequence classification/regression loss.\n            Indices should be in ``[0, ..., config.num_labels]``.\n            If ``config.num_labels == 1`` a regression loss is computed (Mean-Square loss),\n            If ``config.num_labels > 1`` a classification loss is computed (Cross-Entropy).\n\n    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n        **loss**: (`optional`, returned when ``labels`` is provided) ``torch.FloatTensor`` of shape ``(1,)``:\n            Classification (or regression if config.num_labels==1) loss.\n        **logits**: ``torch.FloatTensor`` of shape ``(batch_size, config.num_labels)``\n            Classification (or regression if config.num_labels==1) scores (before SoftMax).\n        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\n            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\n            of shape ``(batch_size, sequence_length, hidden_size)``:\n            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\n            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n\n    Examples::\n\n        >>> config = BertConfig.from_pretrained('bert-base-uncased')\n        >>> tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n        >>>\n        >>> model = BertForSequenceClassification(config)\n        >>> input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\")).unsqueeze(0)  # Batch size 1\n        >>> labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1\n        >>> outputs = model(input_ids, labels=labels)\n        >>> loss, logits = outputs[:2]\n\n    \"\"\"\n    def __init__(self, config):\n        super(BertForSequenceClassification, self).__init__(config)\n        self.num_labels = config.num_labels\n        self.loss_type = config.loss_type\n        self.config = config\n\n        #self.bert = BertModel(config) # original BERT\n        self.bert = BertImgModel(config) # baseline 1\n\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n\n        self.classifier = nn.Linear(config.hidden_size, self.config.num_labels) # original\n        #self.classifier = weight_norm(nn.Linear(config.hidden_size, self.config.num_labels), dim=None)\n\n        self.apply(self.init_weights)\n\n    def init_code_embedding(self, em):\n        self.bert.code_embeddings.weight.data = em.clone()\n\n    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None,\n            position_ids=None, head_mask=None, img_feats=None):\n        outputs = self.bert(input_ids, position_ids=position_ids, token_type_ids=token_type_ids,\n                            attention_mask=attention_mask, head_mask=head_mask, img_feats=img_feats)\n        pooled_output = outputs[1]\n\n        pooled_output = self.dropout(pooled_output)\n        logits = self.classifier(pooled_output)\n\n        outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here\n\n        if labels is not None:\n            if self.num_labels == 1: #  doing regression\n                loss_fct = MSELoss()\n                labels = labels.to(torch.float)\n                loss = loss_fct(logits.view(-1), labels.view(-1))\n                if self.loss_type == \"ranking\":\n                    pos = logits[0 : int(len(labels) / 2)]\n                    neg = logits[int(len(labels) / 2):]\n                    loss += (self.config.margin + neg - pos).clamp(min=0).mean()\n            else:\n                # cross-entropy loss\n                #loss_fct = CrossEntropyLoss()\n                #loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n\n                # Loss from BAN codebase\n                #loss = instance_bce_with_logits(logits, labels)\n\n                if self.loss_type == 'kl':\n                    # KL Loss: https://github.com/uclanlp/visualbert/blob/master/pytorch_pretrained_bert/modeling.py\n                    loss_fct = torch.nn.KLDivLoss(reduction=\"batchmean\")\n                    log_softmax = torch.nn.LogSoftmax(dim=-1)\n                    reshaped_logits = logits.contiguous().view(-1, 3129)\n                    reshaped_logits = log_softmax(reshaped_logits)\n                    loss = loss_fct(reshaped_logits, labels.contiguous())\n                elif self.loss_type == 'bce': # [VQA]\n                    #logger.info('logits: {}, labels: {}'.format(logits.shape, labels.shape))\n                    loss = instance_bce_with_logits(logits, labels)\n                elif self.loss_type == 'ranking': # [Retrieval]\n                    # [0, batch_size/2) are the positive samples,\n                    # [batch_size/2, batch_size) are the negative samples.\n                    # 1) cross_entropy loss\n                    loss_fct = CrossEntropyLoss()\n                    loss_sfmx = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n                    # 2) ranking loss\n                    softmax = torch.nn.Softmax(dim=1)\n                    probs = softmax(logits)[:, 1]\n                    pos_probs = probs[0 : int(len(labels) / 2)]\n                    neg_probs = probs[int(len(labels) / 2):]\n                    loss_ranking = (self.config.margin + neg_probs - pos_probs).clamp(min=0).mean()\n                    loss = loss_sfmx + loss_ranking\n                else: # cross_entropy [GQA]\n                    loss_fct = CrossEntropyLoss()\n                    loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n            outputs = (loss,) + outputs\n\n        return outputs  # (loss), logits, (hidden_states), (attentions)\n\n\nclass BertCaptioningHeads(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.predictions = BertLMPredictionHead(config)\n\n    def forward(self, sequence_output):\n        prediction_scores = self.predictions(sequence_output)\n        return prediction_scores\n\n\nclass BertCaptioningLoss(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.label_smoothing = getattr(config, 'label_smoothing', 0)\n        self.drop_worst_ratio = getattr(config, 'drop_worst_ratio', 0)\n        self.drop_worst_after = getattr(config, 'drop_worst_after', 0)\n        self.log_soft = nn.LogSoftmax(dim=1)\n        self.kl = nn.KLDivLoss(reduction='none')\n        self.iter = 0\n\n    def forward(self, logits, target):\n        self.iter += 1\n        eps = self.label_smoothing\n        n_class = logits.size(1)\n        one_hot = torch.zeros_like(logits).scatter(1, target.view(-1, 1), 1)\n        one_hot = one_hot * (1 - eps) + (1 - one_hot) * eps / (n_class - 1)\n        log_prb = self.log_soft(logits)\n        loss = self.kl(log_prb, one_hot).sum(1)\n\n        if self.drop_worst_ratio > 0 and self.iter > self.drop_worst_after:\n            loss, _ = torch.topk(loss,\n                    k=int(loss.shape[0] * (1-self.drop_worst_ratio)),\n                    largest=False)\n\n        loss = loss.mean()\n\n        return loss\n\n\n# cclin\nclass BertImgFeatureLoss(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.label_smoothing = getattr(config, 'label_smoothing', 0)\n        self.drop_worst_ratio = getattr(config, 'drop_worst_ratio', 0)\n        self.drop_worst_after = getattr(config, 'drop_worst_after', 0)\n        self.log_soft = nn.LogSoftmax(dim=1)\n        # self.kl = nn.KLDivLoss(reduction='none')\n        self.cri = nn.MSELoss()\n        # self.cri = nn.SmoothL1Loss()\n        self.iter = 0\n\n    def forward(self, logits, target):\n        self.iter += 1\n        # log_prb = self.log_soft(logits)\n        target = target.view(-1, target.shape[-1])\n        # loss = self.cri(logits, target).sum(1)\n        loss = self.cri(logits, target)\n\n        # if self.drop_worst_ratio > 0 and self.iter > self.drop_worst_after:\n        #     loss, _ = torch.topk(loss,\n        #             k=int(loss.shape[0] * (1-self.drop_worst_ratio)),\n        #             largest=False)\n\n        # loss = loss.mean()\n\n        return loss\n\n\n@add_start_docstrings(\"\"\"Bert Model transformer for image captioning\"\"\",\n    BERT_START_DOCSTRING)\nclass BertForImageCaptioning(BertPreTrainedModel):\n    r\"\"\"\n    Bert for Image Captioning.\n    \"\"\"\n    def __init__(self, config):\n        super(BertForImageCaptioning, self).__init__(config)\n        self.config = config\n        self.bert = BertImgModel(config)\n        self.cls = BertCaptioningHeads(config)\n        self.loss = BertCaptioningLoss(config)\n        # cclin\n        self.cls_img_feat = BertIFPredictionHead(config)\n        self.loss_img_feat = BertImgFeatureLoss(config)\n\n        self.apply(self.init_weights)\n        self.tie_weights()\n\n        self.model_type = getattr(config, 'model_type', 'bert')\n        if self.model_type == 'TIMM_vit':\n            self.bert = BertImgModel(config)\n\n    def tie_weights(self):\n        if hasattr(self.config, 'tie_weights') and self.config.tie_weights:\n            self._tie_or_clone_weights(self.cls.predictions.decoder,\n                                       self.bert.embeddings.word_embeddings)\n        freeze = False\n        if hasattr(self.config, 'freeze_embedding'):\n            freeze = self.config.freeze_embedding\n        self.bert.embeddings.word_embeddings.weight.requires_grad = not freeze\n\n    def forward(self, *args, **kwargs):\n        is_decode = kwargs.get('is_decode', False)\n        inference_mode = kwargs.get('inference_mode', '')\n        if inference_mode:\n            kwargs.pop('inference_mode')\n            if inference_mode == 'prod':\n                return self.prod_generate(*args, **kwargs)\n            if inference_mode == 'prod_no_hidden':\n                return self.prod_no_hidden_generate(*args, **kwargs)\n            assert False, 'unknown inference_mode: {}'.format(inference_mode)\n        if is_decode:\n            return self.generate(*args, **kwargs)\n        else:\n            return self.encode_forward(*args, **kwargs)\n\n    def encode_forward(self, input_ids, img_feats, attention_mask,\n            masked_pos=None, masked_ids=None,\n            masked_pos_img=None, masked_token_img=None,\n            token_type_ids=None, position_ids=None, head_mask=None,\n            is_training=True, encoder_history_states=None, car_info=None):\n        outputs = self.bert(input_ids, img_feats=img_feats, attention_mask=attention_mask,\n                position_ids=position_ids, token_type_ids=token_type_ids,\n                head_mask=head_mask,\n                encoder_history_states=encoder_history_states)\n        if is_training:\n            sequence_output = outputs[0][:, :masked_pos.shape[-1], :]\n            # num_masks_in_batch * hidden_size\n            # a_start_time = time.time()\n            sequence_output_masked = sequence_output[masked_pos==1, :] # it is slow, but don't have better solution now\n            # print('bert forward (after sequence_output_masked original):', time.time() - a_start_time)\n            # a_start_time = time.time()\n            # sequence_output = sequence_output.cpu()\n            # print('bert forward (after sequence_output_masked 1):', time.time() - a_start_time)\n            # a_start_time = time.time()\n            # sequence_output_masked = sequence_output[index,:]\n            # print('bert forward (after sequence_output_masked 2):', time.time() - a_start_time)\n            # a_start_time = time.time()\n            # sequence_output_masked = sequence_output_masked.cuda()\n            # # sequence_output_masked = sequence_output.cpu()[index,:].cuda()\n            # print('bert forward (after sequence_output_masked 3):', time.time() - a_start_time)\n            # index = masked_pos.bool()\n            # a_start_time = time.time()\n            # new_index = index.unsqueeze(-1).expand(-1, -1, sequence_output.shape[-1])\n            # sequence_output_masked = torch.masked_select(sequence_output, new_index).view(-1,sequence_output.shape[-1])\n            # print('bert forward (after sequence_output_masked v2):', time.time() - a_start_time)\n            class_logits = self.cls(sequence_output_masked)\n            masked_ids = masked_ids[masked_ids != -1]   # remove padded target\n            masked_loss = self.loss(class_logits.float(), masked_ids)\n            if masked_pos_img is not None:\n                sequence_output_img = outputs[0][:, masked_pos.shape[-1]:, :]\n                # num_masks_in_batch * hidden_size\n                sequence_output_masked_img = sequence_output_img[masked_pos_img==1, :]\n                # img cclin\n                img_feat = self.cls_img_feat(sequence_output_masked_img)\n                masked_loss_img = self.loss_img_feat(img_feat.float(), masked_token_img)\n                masked_loss += 0.1 * masked_loss_img  # TODO\n            outputs = (masked_loss, class_logits,) + outputs[2:]\n        else:\n            sequence_output = outputs[0][:, :input_ids.shape[-1], :]\n            class_logits = self.cls(sequence_output)\n            outputs = (class_logits,) + outputs[2:]\n        return outputs\n\n    def prepare_inputs_for_generation(self, curr_ids, past=None):\n        # NOTE: if attention is on, it should be the token used to mask words in training\n        mask_token_id = self.mask_token_id\n        batch_size = curr_ids.shape[0]\n        mask_ids = torch.full(\n            (batch_size, 1), mask_token_id, dtype=torch.long, device=curr_ids.device\n        )\n\n        def _slice(t, start, end):\n            if t is None:\n                return t\n            assert t.shape == (batch_size, self.max_seq_len + self.od_labels_len)\n            return t[:, start: end]\n\n        def _remove_elements(t, start, end):\n            if t is None:\n                return t\n            assert t.shape == (batch_size, self.max_seq_len + self.od_labels_len)\n            return torch.cat([t[:, :start], t[:, end:]], dim=1)\n\n        if past is None:\n            input_ids = torch.cat([curr_ids, mask_ids], dim=1)\n\n            curr_len = input_ids.shape[1]\n            full_len = self.max_seq_len + self.od_labels_len + self.img_seq_len\n            assert self.full_attention_mask.shape == (batch_size,\n                    full_len, full_len)\n\n            def _remove_rows_cols(t, row_start, row_end, col_start, col_end):\n                t00 = t[:, :row_start, :col_start]\n                t01 = t[:, :row_start, col_end:]\n                t10 = t[:, row_end:, :col_start]\n                t11 = t[:, row_end:, col_end:]\n                res = torch.cat([torch.cat([t00, t01], dim=2), torch.cat([t10, t11],\n                            dim=2)], dim=1)\n                assert res.shape == (t.shape[0], t.shape[1]-row_end+row_start,\n                        t.shape[2]-col_end+col_start)\n                return res\n\n            seq_start = curr_len\n            seq_end = self.max_seq_len\n            attention_mask = _remove_rows_cols(self.full_attention_mask, seq_start,\n                    seq_end, seq_start, seq_end)\n\n            masked_pos = _remove_elements(self.full_masked_pos, seq_start, seq_end)\n            token_type_ids = _remove_elements(self.full_token_type_ids, seq_start, seq_end)\n            position_ids = _remove_elements(self.full_position_ids, seq_start, seq_end)\n            img_feats = self.img_feats\n\n            if self.add_od_labels:\n                assert self.od_label_ids.shape[1] == self.od_labels_len\n                input_ids = torch.cat([input_ids, self.od_label_ids], dim=1)\n        else:\n            last_token = curr_ids[:, -1:]\n            # The representation of last token should be re-computed, because\n            # it depends on both self-attention context and input tensor\n            input_ids = torch.cat([last_token, mask_ids], dim=1)\n            start_pos = curr_ids.shape[1] - 1\n            end_pos = start_pos + input_ids.shape[1]\n            masked_pos = _slice(self.full_masked_pos, start_pos, end_pos)\n            token_type_ids = _slice(self.full_token_type_ids, start_pos, end_pos)\n            position_ids = _slice(self.full_position_ids, start_pos, end_pos)\n\n            img_feats = None\n            assert past[0].shape[0] == batch_size\n            if self.prev_encoded_layers is None:\n                assert start_pos == 1  # the first token after BOS\n                assert past[0].shape[1] == 2 + self.od_labels_len + self.img_seq_len\n                # reorder to [od_labels, img_feats, sentence]\n                self.prev_encoded_layers = [\n                        torch.cat([x[:, 2:, :], x[:, :start_pos,:]], dim=1)\n                        for x in past]\n                s2s = self.full_attention_mask[:, :self.max_seq_len,\n                        :self.max_seq_len]\n                s2i = self.full_attention_mask[:, :self.max_seq_len,\n                        self.max_seq_len:]\n                i2s = self.full_attention_mask[:, self.max_seq_len:,\n                        :self.max_seq_len]\n                i2i = self.full_attention_mask[:, self.max_seq_len:,\n                        self.max_seq_len:]\n                self.full_attention_mask = torch.cat(\n                        [torch.cat([i2i, i2s], dim=2),\n                        torch.cat([s2i, s2s], dim=2)],\n                        dim=1)\n            else:\n                assert start_pos > 1\n                assert past[0].shape[1] == 2\n                self.prev_encoded_layers = [torch.cat([x, p[:, :-1, :]], dim=1)\n                        for x, p in zip(self.prev_encoded_layers, past)]\n\n            attention_mask = self.full_attention_mask[:,\n                self.od_labels_len+self.img_seq_len+start_pos: self.od_labels_len+self.img_seq_len+end_pos,\n                :self.od_labels_len+self.img_seq_len+end_pos]\n\n        return {'input_ids': input_ids, 'img_feats': img_feats,\n            'masked_pos': masked_pos, 'attention_mask': attention_mask,\n            'token_type_ids': token_type_ids, 'position_ids': position_ids,\n            'is_training': False,\n            'encoder_history_states': self.prev_encoded_layers}\n\n    def get_output_embeddings(self):\n        return self.decoder\n\n    def generate(self, img_feats, attention_mask, masked_pos, token_type_ids=None,\n            position_ids=None, head_mask=None, input_ids=None, max_length=None,\n            do_sample=None, num_beams=None, temperature=None, top_k=None, top_p=None,\n            repetition_penalty=None, bos_token_id=None, pad_token_id=None,\n            eos_token_ids=None, mask_token_id=None, length_penalty=None,\n            num_return_sequences=None,\n            num_keep_best=1, is_decode=None,\n            add_od_labels=False, od_labels_start_posid=None,\n            use_sep_cap=None,\n            use_cbs=False, fsm=None, num_constraints=None,\n            min_constraints_to_satisfy=None, use_hypo=False,\n            decoding_constraint_flag=None, bad_ending_ids=None,\n            car_info = None\n            ):\n        \"\"\" Generates captions given image features\n        \"\"\"\n        assert is_decode\n        batch_size = img_feats.shape[0]\n        self.img_seq_len = img_feats.shape[1]\n        self.max_seq_len = max_length\n        self.mask_token_id = mask_token_id\n        self.prev_encoded_layers = None\n        # NOTE: num_keep_best is not equavilant to num_return_sequences\n        # num_keep_best is the number of hypotheses to keep in beam search\n        # num_return_sequences is the repeating times of input, coupled with\n        # do_sample=True can generate more than one samples per image\n        self.num_keep_best = num_keep_best\n\n        vocab_size = self.config.vocab_size\n        if not use_cbs:\n            num_fsm_states = 1\n        else:\n            b, num_fsm_states, f1, v = fsm.shape\n            assert b==batch_size and v==vocab_size and f1==num_fsm_states\n\n        self.use_sep_cap=use_sep_cap\n\n        self.add_od_labels = add_od_labels\n        # avoid position_ids collision of caption and od labels\n        self.od_labels_start_posid = max(od_labels_start_posid, self.max_seq_len)\n        if self.add_od_labels:\n            # get od labels part from input_ids\n            assert input_ids.shape[0] == batch_size\n            od_label_ids = input_ids[:, self.max_seq_len:]\n            self.od_labels_len = input_ids.shape[1] - self.max_seq_len\n            input_ids = None\n        else:\n            self.od_labels_len = 0\n            od_label_ids = None\n            assert input_ids.shape == (batch_size, self.max_seq_len)\n            input_ids = None\n\n        if input_ids is None:\n            input_ids = torch.full(\n                (batch_size, 1), bos_token_id, dtype=torch.long, device=next(self.parameters()).device\n            )\n        else:\n            assert input_ids.dim() == 2, \"Input prompt should be of shape (batch_size, sequence length).\"\n            assert input_ids.shape[0] == batch_size, \"Input batch size must match image features\"\n\n        cur_len = input_ids.shape[1]\n        if  num_return_sequences != 1:\n            # Expand input to num return sequences\n            input_ids = self._expand_for_beams(input_ids, num_return_sequences)\n            effective_batch_size = batch_size * num_return_sequences\n        else:\n            effective_batch_size = batch_size\n\n        if position_ids is None:\n            position_ids = torch.arange(self.max_seq_len, dtype=torch.long, device=input_ids.device)\n            posids_len = self.max_seq_len\n            if self.add_od_labels:\n                od_labels_posids = torch.arange(\n                        self.od_labels_start_posid,\n                        self.od_labels_start_posid + self.od_labels_len, dtype=torch.long, device=input_ids.device)\n                position_ids = torch.cat([position_ids, od_labels_posids])\n                posids_len += self.od_labels_len\n            position_ids = position_ids.unsqueeze(0).expand([batch_size, posids_len])\n\n        num_expand = num_beams * num_fsm_states * num_return_sequences\n        self.od_label_ids = self._expand_for_beams(od_label_ids, num_expand)\n        self.img_feats = self._expand_for_beams(img_feats, num_expand)\n        self.full_attention_mask = self._expand_for_beams(attention_mask, num_expand)\n        self.full_masked_pos = self._expand_for_beams(masked_pos, num_expand)\n        self.full_token_type_ids = self._expand_for_beams(token_type_ids, num_expand)\n        self.full_position_ids = self._expand_for_beams(position_ids, num_expand)\n        self.full_head_mask = self._expand_for_beams(head_mask, num_expand)\n\n        if not use_cbs:\n            if num_beams > 1:\n                output = self._generate_beam_search(\n                    input_ids,\n                    cur_len,\n                    max_length,\n                    do_sample,\n                    temperature,\n                    top_k,\n                    top_p,\n                    repetition_penalty,\n                    pad_token_id,\n                    eos_token_ids,\n                    effective_batch_size,\n                    length_penalty,\n                    num_beams,\n                    vocab_size,\n                )\n            else:\n                output = self._generate_no_beam_search(\n                    input_ids,\n                    cur_len,\n                    max_length,\n                    do_sample,\n                    temperature,\n                    top_k,\n                    top_p,\n                    repetition_penalty,\n                    bos_token_id,\n                    pad_token_id,\n                    eos_token_ids,\n                    effective_batch_size,\n                )\n        else:\n            from src.modeling.utils_cbs import (ConstrainedBeamSearch,\n                    select_best_beam_with_constraints)\n            assert self.num_keep_best == 1, 'not supported n_best > 1 for CBS'\n            searcher = ConstrainedBeamSearch(eos_token_ids, max_length,\n                    num_beams, use_hypo=use_hypo,\n                    decoding_constraint_flag=decoding_constraint_flag,\n                    bad_ending_ids=bad_ending_ids)\n    \n            curr_ids, sum_logprobs = searcher.search(\n                    input_ids,\n                    None,\n                    self._decode_step,\n                    fsm,\n            )\n\n            curr_ids, logprobs = select_best_beam_with_constraints(\n                curr_ids,\n                sum_logprobs,\n                num_constraints,\n                min_constraints_to_satisfy,\n                eos_token_ids,\n            )\n            # (batch_size, n_best, max_len), (batch_size, n_best)\n            output = (curr_ids.unsqueeze(1), logprobs.unsqueeze(1))\n\n        return output\n\n    def _expand_for_beams(self, x, num_expand):\n        if x is None or num_expand == 1:\n            return x\n\n        input_shape = list(x.shape)\n        expanded_shape = input_shape[:1] + [num_expand] + input_shape[1:]\n        x = x.unsqueeze(1).expand(expanded_shape)\n        # (batch_size * num_expand, ...)\n        x = x.contiguous().view([input_shape[0] * num_expand] + input_shape[1:])\n        return x\n\n    def _do_output_past(self, outputs):\n        return len(outputs) > 1\n\n    def prod_generate(self, img_feats, od_label_ids, max_length,\n            bos_token_id, eos_token_ids, mask_token_id, od_labels_start_posid,\n            add_od_labels=True, cls_token_segment_id=0,\n            sequence_a_segment_id=0, sequence_b_segment_id=1,\n            ):\n        \"\"\" Generates captions for PROD, batch size=1, num_beams=1.\n            Use faster generation where output_hidden_states = True\n        \"\"\"\n        batch_size = img_feats.shape[0]\n        assert batch_size == 1\n        device = img_feats.device\n        assert od_label_ids.shape[0] == batch_size\n        od_labels_len = od_label_ids.shape[1]\n        img_seq_len = img_feats.shape[1]\n\n        mask_ids = torch.full(\n            (1, 1), mask_token_id, dtype=torch.long, device=device\n        )\n\n        # prepare inputs\n        cur_ids = torch.full((1, 1), bos_token_id,\n                dtype=torch.long, device=device)\n\n        input_ids = torch.cat([cur_ids, mask_ids, od_label_ids], dim=1)\n        token_type_ids = torch.cat([\n                torch.tensor([[cls_token_segment_id, sequence_a_segment_id]],\n                    dtype=torch.long, device=device),\n                torch.full((1, od_labels_len), sequence_b_segment_id,\n                    dtype=torch.long, device=device)\n                ], dim=1)\n\n        position_ids = torch.arange(2, dtype=torch.long, device=device)\n        od_labels_start_posid = max(od_labels_start_posid, max_length)\n        if add_od_labels:\n            od_labels_posids = torch.arange(\n                    od_labels_start_posid, od_labels_start_posid + od_labels_len,\n                    dtype=torch.long, device=device)\n            position_ids = torch.cat([position_ids, od_labels_posids])\n        posids_len = 2 + od_labels_len\n        position_ids = position_ids.unsqueeze(0).expand([1, posids_len])\n\n        attention_mask = torch.ones(\n                (1, 2+od_labels_len+img_seq_len, 2+od_labels_len+img_seq_len),\n                dtype=torch.long, device=device)\n        attention_mask[:, 0, 1] = 0   # words in sentence can not see words after itself\n        attention_mask[:, 2:, :2] = 0 # od_label, img_feat can not see sentence\n\n        # make empty history states for the first step\n        encoder_history_states = tuple(\n            torch.empty([1, 0, self.config.hidden_size], device=device)\n            for _ in range(self.config.num_hidden_layers)\n        )\n\n        # prepare inputs for >1 steps\n        token_type_ids_after_first = torch.full([1, 2], sequence_a_segment_id,\n                dtype=torch.long, device=device)\n        img_feats_after_first = torch.empty([1, 0, self.config.img_feature_dim],\n                device=device)  # place holder to avoid None\n\n        # initial model inputs for the first step\n        model_inputs = {'input_ids': input_ids, 'img_feats': img_feats,\n            'attention_mask': attention_mask,\n            'token_type_ids': token_type_ids, 'position_ids': position_ids,\n            'is_training': False,\n            'encoder_history_states': encoder_history_states}\n        cur_len = cur_ids.shape[1]\n        sum_logprob = 0\n        while True:\n            outputs = self(**model_inputs)\n\n            assert self._do_output_past(outputs)\n            if cur_len == 1:\n                assert outputs[0].shape[1] == 2 + od_labels_len\n            else:\n                assert cur_len > 1\n                assert outputs[0].shape[1] == 2\n\n            # greedy decoding\n            next_token_idx = 1\n            next_token_logits = outputs[0][:, next_token_idx, :]\n            next_token = torch.argmax(next_token_logits, dim=-1)\n            # Compute scores\n            _scores = F.log_softmax(next_token_logits, dim=-1)  # (batch_size, vocab_size)\n            sum_logprob += _scores[:, next_token].item()\n\n            if next_token in eos_token_ids:\n                break\n            cur_ids = torch.cat([cur_ids, next_token.unsqueeze(-1)], dim=-1)\n            cur_len = cur_ids.shape[1]\n            if cur_len == max_length:\n                break\n\n            # prepare model inputs for the next step\n            past = outputs[1]\n            last_token = cur_ids[:, -1:]\n            input_ids = torch.cat([last_token, mask_ids], dim=1)\n            position_ids = torch.arange(cur_len - 1, cur_len + 1,\n                    dtype=torch.long, device=device)\n            attention_mask = torch.ones([1, 2, od_labels_len+img_seq_len+cur_len+1],\n                    dtype=torch.long, device=device)\n            attention_mask[:, 0, -1] = 0\n            assert past[0].shape[0] == batch_size\n            # special handle for the first step\n            if cur_len == 2:\n                assert past[0].shape[1] == 2 + od_labels_len + img_seq_len\n                # remove the first token after BOS\n                # reorder to [od_labels, img_feats, sentence]\n                encoder_history_states = [\n                        torch.cat([x[:, 2:, :], x[:, :1,:]], dim=1)\n                        for x in past]\n            else:\n                assert cur_len > 2\n                assert past[0].shape[1] == 2\n                encoder_history_states = [torch.cat([x, p[:, :-1, :]], dim=1)\n                        for x, p in zip(encoder_history_states, past)]\n\n            model_inputs = {'input_ids': input_ids,\n                'img_feats': img_feats_after_first,\n                'attention_mask': attention_mask,\n                'token_type_ids': token_type_ids_after_first,\n                'position_ids': position_ids,\n                'is_training': False,\n                'encoder_history_states': encoder_history_states}\n\n        logprob = sum_logprob / cur_ids.shape[1]\n\n        # (batch_size, max_len), (batch_size)\n        return cur_ids, torch.full((1,), logprob, device=device)\n\n    def prod_no_hidden_generate(self, img_feats, od_label_ids, max_length,\n            bos_token_id, eos_token_ids, mask_token_id, od_labels_start_posid,\n            add_od_labels=True, cls_token_segment_id=0,\n            sequence_a_segment_id=0, sequence_b_segment_id=1,\n            ):\n        \"\"\" Generates captions for PROD, batch size=1, num_beams=1.\n            Use output_hidden_states = False\n        \"\"\"\n        batch_size = img_feats.shape[0]\n        assert batch_size == 1\n        device = img_feats.device\n        assert od_label_ids.shape[0] == batch_size\n        od_labels_len = od_label_ids.shape[1]\n        img_seq_len = img_feats.shape[1]\n\n        mask_ids = torch.full(\n            (1, 1), mask_token_id, dtype=torch.long, device=device\n        )\n\n        # prepare inputs\n        cur_ids = torch.full((1, 1), bos_token_id,\n                dtype=torch.long, device=device)\n        od_labels_start_posid = max(od_labels_start_posid, max_length)\n        triangle_mask = torch.tril(torch.ones([max_length, max_length],\n                dtype=torch.long, device=device))\n\n        def _prepare_inputs(cur_ids):\n            cur_len = cur_ids.shape[1]\n            input_ids = torch.cat([cur_ids, mask_ids, od_label_ids], dim=1)\n            token_type_ids = torch.cat([\n                    torch.tensor([[cls_token_segment_id]],\n                        dtype=torch.long, device=device),\n                    torch.full((1, cur_len), sequence_a_segment_id,\n                        dtype=torch.long, device=device),\n                    torch.full((1, od_labels_len), sequence_b_segment_id,\n                        dtype=torch.long, device=device)\n                    ], dim=1)\n\n            token_len = cur_len + 1\n            position_ids = torch.arange(token_len, dtype=torch.long, device=device)\n            if add_od_labels:\n                od_labels_posids = torch.arange(\n                        od_labels_start_posid, od_labels_start_posid + od_labels_len,\n                        dtype=torch.long, device=device)\n                position_ids = torch.cat([position_ids, od_labels_posids])\n            posids_len = token_len + od_labels_len\n            position_ids = position_ids.unsqueeze(0).expand([1, posids_len])\n\n            attention_mask = torch.ones(\n                    (1, token_len+od_labels_len+img_seq_len, token_len+od_labels_len+img_seq_len),\n                    dtype=torch.long, device=device)\n            attention_mask[:, :token_len,\n                    :token_len].copy_(triangle_mask[:token_len, :token_len])\n            attention_mask[:, token_len:, :token_len] = 0 # od_label, img_feat can not see sentence\n            return input_ids, token_type_ids, position_ids, attention_mask\n\n        # initial model inputs for the first step\n        input_ids, token_type_ids, position_ids, attention_mask = \\\n                _prepare_inputs(cur_ids)\n        model_inputs = {'input_ids': input_ids, 'img_feats': img_feats,\n            'attention_mask': attention_mask,\n            'token_type_ids': token_type_ids, 'position_ids': position_ids,\n            'is_training': False,\n            }\n        cur_len = cur_ids.shape[1]\n        sum_logprob = 0\n        while True:\n            outputs = self(**model_inputs)\n\n            assert not self._do_output_past(outputs)\n\n            # greedy decoding\n            next_token_idx = cur_len\n            next_token_logits = outputs[0][:, next_token_idx, :]\n            next_token = torch.argmax(next_token_logits, dim=-1)\n            # Compute scores\n            _scores = F.log_softmax(next_token_logits, dim=-1)  # (batch_size, vocab_size)\n            sum_logprob += _scores[:, next_token].item()\n\n            if next_token in eos_token_ids:\n                break\n            cur_ids = torch.cat([cur_ids, next_token.unsqueeze(-1)], dim=-1)\n            cur_len = cur_ids.shape[1]\n            if cur_len == max_length:\n                break\n\n            # prepare model inputs for the next step\n            input_ids, token_type_ids, position_ids, attention_mask = \\\n                    _prepare_inputs(cur_ids)\n            model_inputs = {'input_ids': input_ids,\n                'img_feats': img_feats,\n                'attention_mask': attention_mask,\n                'token_type_ids': token_type_ids,\n                'position_ids': position_ids,\n                'is_training': False,\n                }\n\n        logprob = sum_logprob / cur_ids.shape[1]\n\n        # (batch_size, max_len), (batch_size)\n        return cur_ids, torch.full((1,), logprob, device=device)\n\n@add_start_docstrings(\"\"\"Bert Model with a multiple choice classification head on top (a linear layer on top of\n    the pooled output and a softmax) e.g. for RocStories/SWAG tasks. \"\"\",\n    BERT_START_DOCSTRING)\nclass BertForMultipleChoice(BertPreTrainedModel):\n    r\"\"\"\n    Inputs:\n        **input_ids**: ``torch.LongTensor`` of shape ``(batch_size, num_choices, sequence_length)``:\n            Indices of input sequence tokens in the vocabulary.\n            The second dimension of the input (`num_choices`) indicates the number of choices to score.\n            To match pre-training, BERT input sequence should be formatted with [CLS] and [SEP] tokens as follows:\n\n            (a) For sequence pairs:\n\n                ``tokens:         [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]``\n\n                ``token_type_ids:   0   0  0    0    0     0       0   0   1  1  1  1   1   1``\n\n            (b) For single sequences:\n\n                ``tokens:         [CLS] the dog is hairy . [SEP]``\n\n                ``token_type_ids:   0   0   0   0  0     0   0``\n\n            Indices can be obtained using :class:`pytorch_transformers.BertTokenizer`.\n            See :func:`pytorch_transformers.PreTrainedTokenizer.encode` and\n            :func:`pytorch_transformers.PreTrainedTokenizer.convert_tokens_to_ids` for details.\n        **token_type_ids**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size, num_choices, sequence_length)``:\n            Segment token indices to indicate first and second portions of the inputs.\n            The second dimension of the input (`num_choices`) indicates the number of choices to score.\n            Indices are selected in ``[0, 1]``: ``0`` corresponds to a `sentence A` token, ``1``\n            corresponds to a `sentence B` token\n            (see `BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding`_ for more details).\n        **attention_mask**: (`optional`) ``torch.FloatTensor`` of shape ``(batch_size, num_choices, sequence_length)``:\n            Mask to avoid performing attention on padding token indices.\n            The second dimension of the input (`num_choices`) indicates the number of choices to score.\n            Mask values selected in ``[0, 1]``:\n            ``1`` for tokens that are NOT MASKED, ``0`` for MASKED tokens.\n        **head_mask**: (`optional`) ``torch.FloatTensor`` of shape ``(num_heads,)`` or ``(num_layers, num_heads)``:\n            Mask to nullify selected heads of the self-attention modules.\n            Mask values selected in ``[0, 1]``:\n            ``1`` indicates the head is **not masked**, ``0`` indicates the head is **masked**.\n        **labels**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size,)``:\n            Labels for computing the multiple choice classification loss.\n            Indices should be in ``[0, ..., num_choices]`` where `num_choices` is the size of the second dimension\n            of the input tensors. (see `input_ids` above)\n\n    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n        **loss**: (`optional`, returned when ``labels`` is provided) ``torch.FloatTensor`` of shape ``(1,)``:\n            Classification loss.\n        **classification_scores**: ``torch.FloatTensor`` of shape ``(batch_size, num_choices)`` where `num_choices` is the size of the second dimension\n            of the input tensors. (see `input_ids` above).\n            Classification scores (before SoftMax).\n        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\n            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\n            of shape ``(batch_size, sequence_length, hidden_size)``:\n            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\n            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n\n    Examples::\n\n        >>> config = BertConfig.from_pretrained('bert-base-uncased')\n        >>> tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n        >>>\n        >>> model = BertForMultipleChoice(config)\n        >>> choices = [\"Hello, my dog is cute\", \"Hello, my cat is amazing\"]\n        >>> input_ids = torch.tensor([tokenizer.encode(s) for s in choices]).unsqueeze(0)  # Batch size 1, 2 choices\n        >>> labels = torch.tensor(1).unsqueeze(0)  # Batch size 1\n        >>> outputs = model(input_ids, labels=labels)\n        >>> loss, classification_scores = outputs[:2]\n\n    \"\"\"\n    def __init__(self, config):\n        super(BertForMultipleChoice, self).__init__(config)\n\n        self.bert = BertModel(config)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.classifier = nn.Linear(config.hidden_size, 1)\n\n        self.apply(self.init_weights)\n\n    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None,\n                position_ids=None, head_mask=None):\n        num_choices = input_ids.shape[1]\n\n        flat_input_ids = input_ids.view(-1, input_ids.size(-1))\n        flat_position_ids = position_ids.view(-1, position_ids.size(-1)) if position_ids is not None else None\n        flat_token_type_ids = token_type_ids.view(-1, token_type_ids.size(-1)) if token_type_ids is not None else None\n        flat_attention_mask = attention_mask.view(-1, attention_mask.size(-1)) if attention_mask is not None else None\n        outputs = self.bert(flat_input_ids, position_ids=flat_position_ids, token_type_ids=flat_token_type_ids,\n                            attention_mask=flat_attention_mask, head_mask=head_mask)\n        pooled_output = outputs[1]\n\n        pooled_output = self.dropout(pooled_output)\n        logits = self.classifier(pooled_output)\n        reshaped_logits = logits.view(-1, num_choices)\n\n        outputs = (reshaped_logits,) + outputs[2:]  # add hidden states and attention if they are here\n\n        if labels is not None:\n            loss_fct = CrossEntropyLoss()\n            loss = loss_fct(reshaped_logits, labels)\n            outputs = (loss,) + outputs\n\n        return outputs  # (loss), reshaped_logits, (hidden_states), (attentions)\n\n\n@add_start_docstrings(\"\"\"Bert Model with a token classification head on top (a linear layer on top of\n    the hidden-states output) e.g. for Named-Entity-Recognition (NER) tasks. \"\"\",\n    BERT_START_DOCSTRING, BERT_INPUTS_DOCSTRING)\nclass BertForTokenClassification(BertPreTrainedModel):\n    r\"\"\"\n        **labels**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size, sequence_length)``:\n            Labels for computing the token classification loss.\n            Indices should be in ``[0, ..., config.num_labels]``.\n\n    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n        **loss**: (`optional`, returned when ``labels`` is provided) ``torch.FloatTensor`` of shape ``(1,)``:\n            Classification loss.\n        **scores**: ``torch.FloatTensor`` of shape ``(batch_size, sequence_length, config.num_labels)``\n            Classification scores (before SoftMax).\n        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\n            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\n            of shape ``(batch_size, sequence_length, hidden_size)``:\n            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\n            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n\n    Examples::\n\n        >>> config = BertConfig.from_pretrained('bert-base-uncased')\n        >>> tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n        >>>\n        >>> model = BertForTokenClassification(config)\n        >>> input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\")).unsqueeze(0)  # Batch size 1\n        >>> labels = torch.tensor([1] * input_ids.size(1)).unsqueeze(0)  # Batch size 1\n        >>> outputs = model(input_ids, labels=labels)\n        >>> loss, scores = outputs[:2]\n\n    \"\"\"\n    def __init__(self, config):\n        super(BertForTokenClassification, self).__init__(config)\n        self.num_labels = config.num_labels\n\n        self.bert = BertModel(config)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n\n        self.apply(self.init_weights)\n\n    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None,\n                position_ids=None, head_mask=None):\n        outputs = self.bert(input_ids, position_ids=position_ids, token_type_ids=token_type_ids,\n                            attention_mask=attention_mask, head_mask=head_mask)\n        sequence_output = outputs[0]\n\n        sequence_output = self.dropout(sequence_output)\n        logits = self.classifier(sequence_output)\n\n        outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here\n        if labels is not None:\n            loss_fct = CrossEntropyLoss()\n            # Only keep active parts of the loss\n            if attention_mask is not None:\n                active_loss = attention_mask.view(-1) == 1\n                active_logits = logits.view(-1, self.num_labels)[active_loss]\n                active_labels = labels.view(-1)[active_loss]\n                loss = loss_fct(active_logits, active_labels)\n            else:\n                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n            outputs = (loss,) + outputs\n\n        return outputs  # (loss), scores, (hidden_states), (attentions)\n\n\n@add_start_docstrings(\"\"\"Bert Model with a span classification head on top for extractive question-answering tasks like SQuAD (a linear layers on top of\n    the hidden-states output to compute `span start logits` and `span end logits`). \"\"\",\n    BERT_START_DOCSTRING, BERT_INPUTS_DOCSTRING)\nclass BertForQuestionAnswering(BertPreTrainedModel):\n    r\"\"\"\n        **start_positions**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size,)``:\n            Labels for position (index) of the start of the labelled span for computing the token classification loss.\n            Positions are clamped to the length of the sequence (`sequence_length`).\n            Position outside of the sequence are not taken into account for computing the loss.\n        **end_positions**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size,)``:\n            Labels for position (index) of the end of the labelled span for computing the token classification loss.\n            Positions are clamped to the length of the sequence (`sequence_length`).\n            Position outside of the sequence are not taken into account for computing the loss.\n\n    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n        **loss**: (`optional`, returned when ``labels`` is provided) ``torch.FloatTensor`` of shape ``(1,)``:\n            Total span extraction loss is the sum of a Cross-Entropy for the start and end positions.\n        **start_scores**: ``torch.FloatTensor`` of shape ``(batch_size, sequence_length,)``\n            Span-start scores (before SoftMax).\n        **end_scores**: ``torch.FloatTensor`` of shape ``(batch_size, sequence_length,)``\n            Span-end scores (before SoftMax).\n        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\n            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\n            of shape ``(batch_size, sequence_length, hidden_size)``:\n            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\n            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n\n    Examples::\n\n        >>> config = BertConfig.from_pretrained('bert-base-uncased')\n        >>> tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n        >>>\n        >>> model = BertForQuestionAnswering(config)\n        >>> input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\")).unsqueeze(0)  # Batch size 1\n        >>> start_positions = torch.tensor([1])\n        >>> end_positions = torch.tensor([3])\n        >>> outputs = model(input_ids, start_positions=start_positions, end_positions=end_positions)\n        >>> loss, start_scores, end_scores = outputs[:2]\n\n    \"\"\"\n    def __init__(self, config):\n        super(BertForQuestionAnswering, self).__init__(config)\n        self.num_labels = config.num_labels\n\n        self.bert = BertModel(config)\n        self.qa_outputs = nn.Linear(config.hidden_size, config.num_labels)\n\n        self.apply(self.init_weights)\n\n    def forward(self, input_ids, token_type_ids=None, attention_mask=None, start_positions=None,\n                end_positions=None, position_ids=None, head_mask=None):\n        outputs = self.bert(input_ids, position_ids=position_ids, token_type_ids=token_type_ids,\n                            attention_mask=attention_mask, head_mask=head_mask)\n        sequence_output = outputs[0]\n\n        logits = self.qa_outputs(sequence_output)\n        start_logits, end_logits = logits.split(1, dim=-1)\n        start_logits = start_logits.squeeze(-1)\n        end_logits = end_logits.squeeze(-1)\n\n        outputs = (start_logits, end_logits,) + outputs[2:]\n        if start_positions is not None and end_positions is not None:\n            # If we are on multi-GPU, split add a dimension\n            if len(start_positions.size()) > 1:\n                start_positions = start_positions.squeeze(-1)\n            if len(end_positions.size()) > 1:\n                end_positions = end_positions.squeeze(-1)\n            # sometimes the start/end positions are outside our model inputs, we ignore these terms\n            ignored_index = start_logits.size(1)\n            start_positions.clamp_(0, ignored_index)\n            end_positions.clamp_(0, ignored_index)\n\n            loss_fct = CrossEntropyLoss(ignore_index=ignored_index)\n            start_loss = loss_fct(start_logits, start_positions)\n            end_loss = loss_fct(end_logits, end_positions)\n            total_loss = (start_loss + end_loss) / 2\n            outputs = (total_loss,) + outputs\n\n        return outputs  # (loss), start_logits, end_logits, (hidden_states), (attentions)\n\n\n@add_start_docstrings(\"\"\"Bert Model with an extra single-head attention layer on top of embeddings to ground regions to text.\"\"\",\n    BERT_START_DOCSTRING, BERT_INPUTS_DOCSTRING)\nclass BertForVLGrounding(BertPreTrainedModel):\n    r\"\"\"\n        **labels_attention_mask**: ``torch.FloatTensor`` of shape ``(batch_size, seq_length, attr_length)``:\n            Mask to perform attention between text captions + tags (of length seq_length) and region features (of length attr_length).\n            Like everywhere else in this repo, 1.0 represents KEEP and 0.0 represents MASK OUT.\n    \"\"\"\n    def __init__(self, config):\n        super(BertForVLGrounding, self).__init__(config)\n        self.attention_head_size = config.hidden_size // config.num_attention_heads\n        self.bert = BertImgModel(config)\n        self.query_layer = nn.Linear(config.hidden_size, self.attention_head_size)\n        self.key_layer = nn.Linear(config.hidden_size, self.attention_head_size)\n\n    def transpose_for_scores(self, x):\n        # this function is copied from the BertSelfAttention class\n        new_x_shape = x.size()[:-1] + (1, self.attention_head_size)\n        x = x.view(*new_x_shape)\n        return x.permute(0, 2, 1, 3)\n\n    def forward(self, input_ids, token_type_ids=None, attention_mask=None,\n                img_feats=None, labels_attention_mask=None):\n        # standard conversion of labels_attention_mask to be added to pre-softmax scores\n        labels_attention_mask = labels_attention_mask.unsqueeze(1)\n        labels_attention_mask = (1.0 - labels_attention_mask) * -10000.0\n\n        # standard sequence output to take bert embeddings\n        outputs = self.bert(input_ids, token_type_ids, attention_mask=attention_mask, img_feats=img_feats)\n        sequence_output = outputs[0]\n\n        # extract queries correspond to input_ids, keys correspond to img_feats\n        num_img_regions = img_feats.shape[1]\n        queries = sequence_output[:, :-num_img_regions, :]\n        keys = sequence_output[:, -num_img_regions:, :]\n        queries = self.transpose_for_scores(self.query_layer(queries))\n        keys = self.transpose_for_scores(self.key_layer(keys))\n\n        # take the dot product between \"query\" and \"key\" to get the raw attention scores.\n        attention_scores = torch.matmul(queries, keys.transpose(-1, -2))\n        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n        attention_scores = attention_scores + labels_attention_mask\n\n        # squeeze the dimension corresponding to attention heads (we only have one)\n        attention_scores = attention_scores.squeeze(1)\n        return outputs + (attention_scores,)\n\n\n@add_start_docstrings(\"\"\"Bert Pre-Training Model with an extra single-head attention layer on top of embeddings to ground regions to text.\"\"\",\n    BERT_START_DOCSTRING, BERT_INPUTS_DOCSTRING)\nclass BertImgForGroundedPreTraining(BertImgForPreTraining):\n    r\"\"\"\n        **masked_lm_labels**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size, sequence_length)``:\n            Labels for computing the masked language modeling loss.\n            Indices should be in ``[-1, 0, ..., config.vocab_size]`` (see ``input_ids`` docstring)\n            Tokens with indices set to ``-1`` are ignored (masked), the loss is only computed for the tokens with labels\n            in ``[0, ..., config.vocab_size]``\n        **next_sentence_label**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size,)``:\n            Labels for computing the next sequence prediction (classification) loss. Input should be a sequence pair (see ``input_ids`` docstring)\n            Indices should be in ``[0, 1]``.\n            ``0`` indicates sequence B is a continuation of sequence A,\n            ``1`` indicates sequence B is a random sequence.\n        **labels_attention_mask**: ``torch.FloatTensor`` of shape ``(batch_size, seq_length, attr_length)``:\n            Mask to perform attention between text captions + tags (of length seq_length) and region features (of length attr_length).\n            Like everywhere else in this repo, 1.0 represents KEEP and 0.0 represents MASK OUT.\n\n    Outputs:\n        **loss**: (`optional`, returned when both ``masked_lm_labels`` and ``next_sentence_label`` are provided) ``torch.FloatTensor`` of shape ``(1,)``:\n            Total loss as the sum of the masked language modeling loss and the next sequence prediction (classification) loss and the grounding.\n    \"\"\"\n    def __init__(self, config):\n        super(BertImgForGroundedPreTraining, self).__init__(config)\n        self.attention_head_size = config.hidden_size // config.num_attention_heads\n        self.query_layer = nn.Linear(config.hidden_size, self.attention_head_size)\n        self.key_layer = nn.Linear(config.hidden_size, self.attention_head_size)\n        self.apply(self.init_weights)\n\n    def transpose_for_scores(self, x):\n        # this function is copied from the BertSelfAttention class\n        new_x_shape = x.size()[:-1] + (1, self.attention_head_size)\n        x = x.view(*new_x_shape)\n        return x.permute(0, 2, 1, 3)\n\n    def forward(self, input_ids, token_type_ids=None, attention_mask=None, masked_lm_labels=None,\n                next_sentence_label=None, position_ids=None, head_mask=None, img_feats=None,\n                grounding_labels=None, grounding_mask=None, grounding_weight=1.0):\n\n        # standard sequence output to take bert embeddings\n        outputs = self.bert(input_ids, position_ids=position_ids, token_type_ids=token_type_ids,\n                            attention_mask=attention_mask, head_mask=head_mask, img_feats=img_feats)\n        sequence_output, pooled_output = outputs[:2]\n\n        # compute masked token predictions and contrastive prediction\n        prediction_scores, seq_relationship_score = self.cls(sequence_output, pooled_output)\n        outputs = (prediction_scores, seq_relationship_score,) + outputs[2:]  # add hidden states and attention if they are here\n\n        # compute loss\n        if masked_lm_labels is not None and next_sentence_label is not None and grounding_labels is not None and grounding_mask is not None:\n\n            # losses below are both scalars\n            loss_fct = CrossEntropyLoss(ignore_index=-1)\n            masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), masked_lm_labels.view(-1))\n            next_sentence_loss = loss_fct(seq_relationship_score.view(-1, self.num_seq_relations), next_sentence_label.view(-1))\n\n            # standard conversion of labels_attention_mask to be added to pre-softmax scores\n            grounding_mask = grounding_mask.unsqueeze(1)\n            grounding_mask = (1.0 - grounding_mask) * -10000.0\n\n            # extract queries correspond to input_ids, keys correspond to img_feats\n            num_img_regions = img_feats.shape[1]\n            queries = sequence_output[:, :-num_img_regions, :]\n            keys = sequence_output[:, -num_img_regions:, :]\n            queries = self.transpose_for_scores(self.query_layer(queries))\n            keys = self.transpose_for_scores(self.key_layer(keys))\n\n            # take the dot product between \"query\" and \"key\" to get the raw attention scores.\n            attention_logits = torch.matmul(queries, keys.transpose(-1, -2))\n            attention_logits = attention_logits / math.sqrt(self.attention_head_size)\n            attention_logits = attention_logits + grounding_mask\n\n            # squeeze the dimension corresponding to attention heads (we only have one)\n            attention_logits = attention_logits.squeeze(1)\n\n            # used to only consider losses for tokens corresponding to phrases;\n            # this mask has shape (batch_size, number of tokens)\n            loss_mask = (grounding_labels.sum(dim=-1) > 0).float()\n\n            if loss_mask.sum() == 0:\n                grounding_loss = torch.zeros_like(masked_lm_loss)\n            else:\n                grounding_loss = kl_divergence(Categorical(probs=grounding_labels[loss_mask == 1].float()),\n                                               Categorical(logits=attention_logits[loss_mask == 1])).mean()\n\n            total_loss = masked_lm_loss + next_sentence_loss + grounding_weight * grounding_loss\n            outputs = (total_loss,) + outputs + (masked_lm_loss, grounding_loss)\n\n        return outputs\n\n"}
