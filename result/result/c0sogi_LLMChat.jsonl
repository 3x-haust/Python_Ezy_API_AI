{"repo_info": {"repo_name": "LLMChat", "repo_owner": "c0sogi", "repo_url": "https://github.com/c0sogi/LLMChat"}}
{"type": "test_file", "path": "app/utils/tests/random.py", "content": "from uuid import uuid4\n\n\ndef random_user_generator(**kwargs):\n    random_8_digits = str(hash(uuid4()))[:8]\n    return {\n        \"email\": f\"{random_8_digits}@test.com\",\n        \"password\": \"123456\",\n        \"name\": f\"{random_8_digits}\",\n        \"phone_number\": f\"010{random_8_digits}\",\n    } | kwargs\n"}
{"type": "test_file", "path": "tests/test_completion_api.py", "content": "import pytest\n\nfrom app.common.config import OPENAI_API_KEY\nfrom app.models.completion_models import (\n    ChatCompletion,\n    ChatCompletionChunk,\n    Completion,\n    CompletionChunk,\n)\nfrom app.utils.api.completion import (\n    acreate_chat_completion,\n    acreate_completion,\n    request_chat_completion,\n    request_chat_completion_with_streaming,\n    request_text_completion,\n    request_text_completion_with_streaming,\n)\n\n\n@pytest.mark.parametrize(\"stream\", [True, False])\n@pytest.mark.skipif(OPENAI_API_KEY is None, reason=\"OpenAI API Key is not set\")\n@pytest.mark.asyncio\nasync def test_completion_creation(stream: bool) -> None:\n    assert OPENAI_API_KEY is not None\n\n    prompt: str = \"Say this is TEST:\"\n    model: str = \"text-davinci-003\"\n    api_base: str = \"https://api.openai.com/v1\"\n    api_key: str = OPENAI_API_KEY\n    max_tokens: int = 10\n    temperature: float = 0\n    n: int = 1\n\n    async for chunk_or_completion in acreate_completion(\n        prompt=prompt,\n        model=model,\n        api_base=api_base,\n        api_key=api_key,\n        max_tokens=max_tokens,\n        temperature=temperature,\n        n=n,\n        stream=stream,\n    ):\n        if stream:\n            chunk: CompletionChunk = chunk_or_completion  # type: ignore\n            assert chunk[\"choices\"][0][\"text\"] is not None\n        else:\n            completion: Completion = chunk_or_completion  # type: ignore\n            assert completion[\"choices\"][0][\"text\"] is not None\n\n\n@pytest.mark.parametrize(\"stream\", [True, False])\n@pytest.mark.skipif(OPENAI_API_KEY is None, reason=\"OpenAI API Key is not set\")\n@pytest.mark.asyncio\nasync def test_chat_completion_creation(stream: bool) -> None:\n    assert OPENAI_API_KEY is not None\n\n    message: dict[str, str] = {\"role\": \"user\", \"content\": \"Say this is TEST:\"}\n    model: str = \"gpt-3.5-turbo\"\n    api_base: str = \"https://api.openai.com/v1\"\n    api_key: str = OPENAI_API_KEY\n    max_tokens: int = 10\n    temperature: float = 0\n    n: int = 1\n\n    async for chunk_or_completion in acreate_chat_completion(\n        messages=[message],\n        model=model,\n        api_base=api_base,\n        api_key=api_key,\n        max_tokens=max_tokens,\n        temperature=temperature,\n        n=n,\n        stream=stream,\n    ):\n        if stream:\n            chunk: ChatCompletionChunk = chunk_or_completion  # type: ignore\n            assert chunk[\"choices\"][0] is not None\n        else:\n            completion: ChatCompletion = chunk_or_completion  # type: ignore\n            assert completion[\"choices\"][0] is not None\n\n\n@pytest.mark.parametrize(\"stream\", [True, False])\n@pytest.mark.skipif(OPENAI_API_KEY is None, reason=\"OpenAI API Key is not set\")\n@pytest.mark.asyncio\nasync def test_text_completion_request(stream: bool) -> None:\n    assert OPENAI_API_KEY is not None\n\n    prompt: str = \"Say this is TEST:\"\n    model: str = \"text-davinci-003\"\n    api_base: str = \"https://api.openai.com/v1\"\n    api_key: str = OPENAI_API_KEY\n    max_tokens: int = 10\n    temperature: float = 0\n    n: int = 1\n\n    if stream:\n        async for completion_chunk in request_text_completion_with_streaming(\n            prompt=prompt,\n            model=model,\n            api_base=api_base,\n            api_key=api_key,\n            max_tokens=max_tokens,\n            temperature=temperature,\n            n=n,\n        ):\n            assert completion_chunk[\"choices\"][0][\"text\"] is not None\n    else:\n        completion: Completion = await request_text_completion(\n            prompt=prompt,\n            model=model,\n            api_base=api_base,\n            api_key=api_key,\n            max_tokens=max_tokens,\n            temperature=temperature,\n            n=n,\n        )\n        assert completion[\"choices\"][0][\"text\"] is not None\n\n\n@pytest.mark.parametrize(\"stream\", [True, False])\n@pytest.mark.skipif(OPENAI_API_KEY is None, reason=\"OpenAI API Key is not set\")\n@pytest.mark.asyncio\nasync def test_chat_completion_request(stream: bool) -> None:\n    assert OPENAI_API_KEY is not None\n\n    message: dict[str, str] = {\"role\": \"user\", \"content\": \"Say this is TEST:\"}\n    model: str = \"gpt-3.5-turbo\"\n    api_base: str = \"https://api.openai.com/v1\"\n    api_key: str = OPENAI_API_KEY\n    max_tokens: int = 10\n    temperature: float = 0\n    n: int = 1\n    if stream:\n        async for chat_completion_chunk in request_chat_completion_with_streaming(\n            messages=[message],\n            model=model,\n            api_base=api_base,\n            api_key=api_key,\n            functions=None,\n            function_call=None,\n            max_tokens=max_tokens,\n            temperature=temperature,\n            n=n,\n        ):\n            assert chat_completion_chunk[\"choices\"][0] is not None\n    else:\n        chat_completion: ChatCompletion = await request_chat_completion(\n            messages=[message],\n            model=model,\n            api_base=api_base,\n            api_key=api_key,\n            functions=None,\n            function_call=None,\n            max_tokens=max_tokens,\n            temperature=temperature,\n            n=n,\n        )\n        assert chat_completion[\"choices\"][0][\"message\"][\"content\"] is not None\n"}
{"type": "test_file", "path": "tests/test_dynamic_llm_load.py", "content": "from app.models.llms import LLMModels\n\n\ndef test_dynamic_llm_load():\n    print(\"- member_map keys -\")\n    print(LLMModels.member_map.keys())\n    print(\"- static member map keys -\")\n    print(LLMModels.static_member_map.keys())\n    print(\"- dynamic member map keys -\")\n    print(LLMModels.dynamic_member_map.keys())\n\n    LLMModels.add_member(\"foo\", {\"bar\": \"baz\"})\n\n    print(\"- member_map keys -\")\n    print(LLMModels.member_map.keys())\n    print(\"- static member map keys -\")\n    print(LLMModels.static_member_map.keys())\n    print(\"- dynamic member map keys -\")\n    print(LLMModels.dynamic_member_map.keys())\n\n    print(\"- member_map class -\")\n    print(LLMModels.member_map)\n\n    print(\"- is dynamic member instance of LLMModels ? -\")\n    print(isinstance(LLMModels.dynamic_member_map[\"foo\"], LLMModels))\n    print(f\"- {LLMModels.dynamic_member_map['foo']} -\")\n"}
{"type": "test_file", "path": "tests/test_memory_deallocation.py", "content": "import asyncio\nimport queue\nfrom collections import deque\n\nfrom app.utils.system import free_memory_of_first_item_from_container\n\nDEL_COUNT: int = 0\n\n\nclass DummyObject:\n    \"\"\"Dummy object for testing.\"\"\"\n\n    foo: bool\n    bar: bool\n\n    def __init__(self) -> None:\n        \"\"\"Initialize.\"\"\"\n        self.foo = True\n        self.bar = False\n\n    def __del__(self) -> None:\n        \"\"\"Clean up resources.\"\"\"\n        global DEL_COUNT\n        DEL_COUNT += 1\n\n\ndef test_deallocate_item_from_memory_by_reference():\n    \"\"\"Tests if __del__ is called when item is removed from container.\"\"\"\n    global DEL_COUNT\n\n    # Test with a deque\n    _deque = deque([DummyObject() for _ in range(10)])\n    _list = [DummyObject() for _ in range(10)]\n    _dict = {i: DummyObject() for i in range(10)}\n    _queue = queue.Queue()\n    _asyncio_queue = asyncio.Queue()\n    for _ in range(10):\n        _queue.put(DummyObject())\n        _asyncio_queue.put_nowait(DummyObject())\n\n    # Test begin\n    for container in [_deque, _list, _dict, _queue, _asyncio_queue]:\n        print(f\"- Testing {container.__class__.__name__}\")\n        DEL_COUNT = 0\n        for _ in range(10):\n            free_memory_of_first_item_from_container(container)\n        print(f\"- Finished testing {container.__class__.__name__}\")\n        assert (\n            DEL_COUNT >= 10\n        ), f\"At least {container} should have 10 items deleted\"\n\n\ndef test_dereference():\n    obj = DummyObject()\n\n    def delete_obj(obj) -> None:\n        del obj\n\n    def delete_foo(obj) -> None:\n        del obj.foo\n\n    delete_obj(obj)\n    assert obj is not None  # Check if obj is still in memory\n    delete_foo(obj)\n    assert not hasattr(obj, \"foo\")  # Check if obj.foo is deleted\n"}
{"type": "test_file", "path": "tests/test_vectorstore.py", "content": "from asyncio import gather\nfrom uuid import uuid4\n\nimport pytest\nfrom langchain.docstore.document import Document\nfrom app.database.connection import cache\nfrom app.utils.chat.managers.vectorstore import VectorStoreManager\n\n\n@pytest.mark.asyncio\nasync def test_embedding_single_index(config, test_logger):\n    \"\"\"Warning! This is expensive test!\n    It costs a lot to embed the text\"\"\"\n\n    cache.start(config=config)\n    test_logger.info(\"Testing embedding\")\n    collection_name: str = uuid4().hex\n    test_logger.info(f\"Collection name: {collection_name}\")\n    sample_text = (\n        \"Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Member\"\n        \"s of Congress and the Cabinet. Justices of the Supreme Court. My fellow American\"\n        \"s.   Last year COVID-19 kept us apart. This year we are finally together again. \"\n        \" Tonight, we meet as Democrats Republicans and Independents. But most importantl\"\n        \"y as Americans.  With a duty to one another to the American people to the Consti\"\n        \"tution.  And with an unwavering resolve that freedom will always triumph over ty\"\n        \"ranny.  Six days ago, Russia’s Vladimir Putin sought to shake the foundations of\"\n        \" the free world thinking he could make it bend to his menacing ways. But he badl\"\n        \"y miscalculated. He thought he could roll into Ukraine and the world would roll \"\n        \"over. Instead he met a wall of strength he never imagined.  He met the Ukrainian\"\n        \" people.  From President Zelenskyy to every Ukrainian, their fearlessness, their\"\n        \" courage, their determination, inspires the world.  Groups of citizens blocking \"\n        \"tanks with their bodies. Everyone from students to retirees teachers turned sold\"\n        \"iers defending their homeland.  In this struggle as President Zelenskyy said in \"\n        \"his speech to the European Parliament “Light will win over darkness.” The Ukrain\"\n        \"ian Ambassador to the United States is here tonight. Let each of us here tonight\"\n        \" in this Chamber send an unmistakable signal to Ukraine and to the world.  Pleas\"\n        \"e rise if you are able and show that, Yes, we the United States of America stand\"\n        \" with the Ukrainian people. Throughout our history we’ve learned this lesson whe\"\n        \"n dictators do not pay a price for their aggression they cause more chaos. They \"\n        \"keep moving. And the costs and the threats to America and the world keep rising.\"\n        \"    That’s why the NATO Alliance was created to secure peace and stability in Eu\"\n        \"rope after World War 2. The United States is a member along with 29 other nation\"\n        \"s.  It matters. American diplomacy matters. American resolve matters.  Putin’s l\"\n        \"atest attack on Ukraine was premeditated and unprovoked.  He rejected repeated e\"\n        \"fforts at diplomacy.  He thought the West and NATO wouldn’t respond. And he thou\"\n        \"ght he could divide us at home. Putin was wrong. We were ready.  Here is what we\"\n        \" did.    We prepared extensively and carefully. We spent months building a coali\"\n        \"tion of other freedom-loving nations from Europe and the Americas to Asia and Af\"\n        \"rica to confront Putin. I spent countless hours unifying our European allies. We\"\n        \" shared with the world in advance what we knew Putin was planning and precisely \"\n        \"how he would try to falsely justify his aggression.   We countered Russia’s lies\"\n        \" with truth.    And now that he has acted the free world is holding him accounta\"\n        \"ble.  Along with twenty-seven members of the European Union including France, Ge\"\n        \"rmany, Italy, as well as countries like the United Kingdom, Canada, Japan, Korea\"\n        \", Australia, New Zealand, and many others, even Switzerland. We are inflicting p\"\n        \"ain on Russia and supporting the people of Ukraine. Putin is now isolated from t\"\n        \"he world more than ever. Together with our allies –we are right now enforcing po\"\n        \"werful economic sanctions. We are cutting off Russia’s largest banks from the in\"\n        \"ternational financial system.   Preventing Russia’s central bank from defending \"\n        \"the Russian Ruble making Putin’s $630 Billion “war fund” worthless.\"\n    )\n    sample_queries = [\n        \"What has been the response of the Ukrainian people to the Russian invasion, as depicted in the speech?\",\n        (\n            \"What preparations did the speaker mention were made to confront Putin's actions\"\n            \", and how does this reflect on the role of NATO and American diplomacy?\"\n        ),\n        (\n            \"What are the specific economic sanctions mentioned in the speech that the United\"\n            \" States and its allies are enforcing against Russia, and how do they aim to impa\"\n            \"ct Russia's economy and Putin's 'war fund'?\"\n        ),\n    ]\n    empty: list[list[Document]] = await gather(\n        *[\n            VectorStoreManager.asimilarity_search(sample_query, collection_name=collection_name, k=3)\n            for sample_query in sample_queries\n        ]\n    )\n    assert all(len(result) == 0 for result in empty)\n\n    await VectorStoreManager.create_documents(sample_text, collection_name=collection_name)\n    results: list[list[Document]] | None = await gather(\n        *[\n            VectorStoreManager.asimilarity_search(sample_query, collection_name=collection_name, k=3)\n            for sample_query in sample_queries\n        ]\n    )\n    assert results is not None\n    for i, result in enumerate(results):\n        test_logger.info(f\"\\n### Query Result{i + 1}\")\n        for j, doc in enumerate(result):\n            test_logger.info(f\"-----> Document[{j + 1}]\\n{doc.page_content}\\n\")\n\n\n@pytest.mark.asyncio\nasync def test_embedding_multiple_index(config, test_logger):\n    cache.start(config=config)\n    test_logger.info(\"Testing embedding\")\n    collection_names: list[str] = [uuid4().hex for _ in range(2)]\n    test_logger.info(f\"Collection names: {collection_names}\")\n    texts_1 = [\"Monkey loves banana\", \"Apple is red\"]\n    texts_2 = [\"Banana is yellow\", \"Apple is green\"]\n    queries = [\"Monkey loves banana\", \"Apple is red\"]\n    empty: list[list[Document]] = await gather(\n        *[\n            VectorStoreManager.asimilarity_search_multiple_collections(query, collection_names=collection_names, k=3)\n            for query in queries\n        ]\n    )\n    assert all(len(result) == 0 for result in empty)\n\n    for collection_name, texts in zip(collection_names, [texts_1, texts_2]):\n        for text in texts:\n            await VectorStoreManager.create_documents(text, collection_name=collection_name)\n\n    queries_results: list[list[tuple[Document, float]]] = await gather(\n        *[\n            VectorStoreManager.asimilarity_search_multiple_collections_with_score(\n                query, collection_names=collection_names, k=3\n            )\n            for query in queries\n        ]\n    )\n    for query, query_results in zip(queries, queries_results):\n        for doc, score in query_results:\n            test_logger.info(f\"\\n\\n\\n\\nQuery={query}\\nScore={score}\\nContent={doc.page_content}\")\n    test_logger.info(f\"\\n\\n\\n\\n\\n\\nTesting embedding: {queries_results}\")\n\n\n@pytest.mark.asyncio\nasync def test_embedding_multiple_index_2(config, test_logger):\n    cache.start(config=config)\n    test_logger.info(\"Testing embedding\")\n    collection_names: list[str] = [uuid4().hex for _ in range(2)]\n    test_logger.info(f\"Collection names: {collection_names}\")\n    texts_1 = [\"Monkey loves banana\", \"Apple is red\"]\n    texts_2 = [\"Banana is yellow\", \"Apple is green\"]\n    queries = [\"Monkey loves banana\", \"Apple is red\"]\n    empty: list[list[Document]] = await gather(\n        *[\n            VectorStoreManager.asimilarity_search_multiple_collections(query, collection_names=collection_names, k=3)\n            for query in queries\n        ]\n    )\n    assert all(len(result) == 0 for result in empty)\n\n    for collection_name, texts in zip(collection_names, [texts_1, texts_2]):\n        for text in texts:\n            await VectorStoreManager.create_documents(text, collection_name=collection_name)\n\n    queries_results: list[list[tuple[Document, float]]] = await gather(\n        *[\n            VectorStoreManager.amax_marginal_relevance_search_multiple_collections_with_score(\n                query, collection_names=collection_names, k=3\n            )\n            for query in queries\n        ]\n    )\n    for query, query_results in zip(queries, queries_results):\n        for doc, score in query_results:\n            test_logger.info(f\"\\n\\n\\n\\nQuery={query}\\nScore={score}\\nContent={doc.page_content}\")\n    test_logger.info(f\"\\n\\n\\n\\n\\n\\nTesting embedding: {queries_results}\")\n"}
{"type": "test_file", "path": "tests/test_auth.py", "content": "from httpx import AsyncClient\nimport pytest\nfrom fastapi import status\nfrom app.errors.api_exceptions import Responses_400\nfrom app.utils.tests.random import random_user_generator\n\n\n@pytest.mark.asyncio\nasync def test_register(async_client):\n    valid_emails = (\"ankitrai326@gmail.com\", \"my.ownsite@our-earth.org\", \"aa@a.a\")\n    invalid_emails = (\"ankitrai326gmail.com\", \"ankitrai326.com\")\n\n    valid_users = [random_user_generator(email=email) for email in valid_emails]\n    invalid_users = [random_user_generator(email=email) for email in invalid_emails]\n\n    # Valid email\n    for valid_user in valid_users:\n        response = await async_client.post(\"api/auth/register/email\", json=valid_user)\n        response_body = response.json()\n        assert response.status_code == status.HTTP_201_CREATED\n        assert \"Authorization\" in response_body.keys(), response.content.decode(\"utf-8\")\n\n    # Invalid email\n    for invalid_user in invalid_users:\n        response = await async_client.post(\"api/auth/register/email\", json=invalid_user)\n        response_body = response.json()\n        assert response.status_code != status.HTTP_201_CREATED\n        assert \"Authorization\" not in response_body.keys(), response.content.decode(\"utf-8\")\n\n\n@pytest.mark.asyncio\nasync def test_auth(async_client: AsyncClient):\n    new_user = random_user_generator()\n\n    # Register\n    response = await async_client.post(\"api/auth/register/email\", json=new_user)\n    assert response.status_code == status.HTTP_201_CREATED, response.content.decode(\"utf-8\")\n\n    # Login\n    response = await async_client.post(\"api/auth/login/email\", json=new_user)\n    response_body = response.json()\n    assert response.status_code == status.HTTP_200_OK, \"User login faiture\"\n    assert \"Authorization\" in response_body.keys(), response.content.decode(\"utf-8\")\n\n    # Duplicate email check\n    response = await async_client.post(\"api/auth/register/email\", json=new_user)\n    response_body = response.json()\n    assert response.status_code == Responses_400.email_already_exists.status_code, \"Email duplication check failure\"\n    assert response_body[\"detail\"] == Responses_400.email_already_exists.detail, \"Email duplication check failure\"\n"}
{"type": "test_file", "path": "tests/test_api_service.py", "content": "from typing import Any, Literal\nfrom httpx import AsyncClient, Response\nimport pytest\nfrom app.utils.date_utils import UTC\nfrom app.utils.params_utils import hash_params, parse_params\n\n\nasync def request_service(\n    async_client: AsyncClient,\n    access_key: str,\n    secret_key: str,\n    request_method: Literal[\"get\", \"post\", \"put\", \"delete\", \"options\"],\n    allowed_status_codes: tuple = (200, 201, 307),\n    service_name: str = \"\",\n    required_query_params: dict[str, Any] = {},\n    required_headers: dict[str, Any] = {},\n    stream: bool = False,\n    logger=None,\n) -> Any:\n    all_query_params: str = parse_params(\n        params={\n            \"key\": access_key,\n            \"timestamp\": UTC.timestamp(),\n        }\n        | required_query_params\n    )\n    method_options: dict = {\n        \"headers\": {\n            \"secret\": hash_params(\n                query_params=all_query_params,\n                secret_key=secret_key,\n            )\n        }\n        | required_headers\n    }\n    url: str = f\"/api/services/{service_name}?{all_query_params}\"\n\n    if stream:\n        response_body = \"\"\n        async with async_client.stream(method=request_method.upper(), url=url, **method_options) as response:\n            assert response.status_code in allowed_status_codes\n            async for chunk in response.aiter_text():\n                response_body += chunk\n                logger.info(f\"Streamed data: {chunk}\") if logger is not None else ...\n    else:\n        response: Response = await getattr(async_client, request_method.lower())(url=url, **method_options)\n        response_body: Any = response.json()\n        logger.info(f\"response_body: {response_body}\") if logger is not None else ...\n        assert response.status_code in allowed_status_codes\n    return response_body\n\n\n@pytest.mark.asyncio\nasync def test_request_api(async_client: AsyncClient, api_key_dict: dict, test_logger):\n    access_key, secret_key = api_key_dict[\"access_key\"], api_key_dict[\"secret_key\"]\n    service_name: str = \"\"\n    request_method: str = \"get\"\n    required_query_params: dict[str, Any] = {}\n    required_headers: dict[str, Any] = {}\n    allowed_status_codes: tuple = (200, 201, 307)\n    await request_service(\n        async_client=async_client,\n        access_key=access_key,\n        secret_key=secret_key,\n        request_method=request_method,\n        allowed_status_codes=allowed_status_codes,\n        service_name=service_name,\n        required_query_params=required_query_params,\n        required_headers=required_headers,\n        stream=False,\n        logger=test_logger,\n    )\n\n\n# @pytest.mark.asyncio\n# async def test_weather_api(async_client: AsyncClient, api_key_dict: dict):\n#     access_key, secret_key = api_key_dict[\"access_key\"], api_key_dict[\"secret_key\"]\n#     service_name: str = \"weather\"\n#     request_method: str = \"get\"\n#     required_query_params: dict[str, any] = {\n#         \"latitude\": 37.0,\n#         \"longitude\": 120.0,\n#     }\n#     required_headers: dict[str, any] = {}\n#     allowed_status_codes: tuple[int] = (200, 307)\n#     await request_service(\n#         async_client=async_client,\n#         access_key=access_key,\n#         secret_key=secret_key,\n#         request_method=request_method,\n#         allowed_status_codes=allowed_status_codes,\n#         service_name=service_name,\n#         required_query_params=required_query_params,\n#         required_headers=required_headers,\n#         stream=False,\n#     )\n\n\n# @pytest.mark.asyncio\n# async def test_stream_api(async_client: AsyncClient, api_key_dict: dict):\n#     access_key, secret_key = api_key_dict[\"access_key\"], api_key_dict[\"secret_key\"]\n#     service_name: str = \"stream\"\n#     request_method: str = \"get\"\n#     required_query_params: dict[str, any] = {}\n#     required_headers: dict[str, any] = {}\n#     allowed_status_codes: tuple[int] = (200, 307)\n#     await request_service(\n#         async_client=async_client,\n#         access_key=access_key,\n#         secret_key=secret_key,\n#         request_method=request_method,\n#         allowed_status_codes=allowed_status_codes,\n#         service_name=service_name,\n#         required_query_params=required_query_params,\n#         required_headers=required_headers,\n#         stream=True,\n#     )\n"}
{"type": "test_file", "path": "tests/test_translate.py", "content": "import pytest\nfrom app.common.config import (\n    GOOGLE_TRANSLATE_API_KEY,\n    PAPAGO_CLIENT_ID,\n    PAPAGO_CLIENT_SECRET,\n    RAPID_API_KEY,\n    CUSTOM_TRANSLATE_URL,\n)\nfrom app.utils.api.translate import Translator\n\n\n@pytest.mark.asyncio\nasync def test_custom_translate(test_logger):\n    translated = await Translator.custom_translate_api(\n        text=\"hello\",\n        src_lang=\"en\",\n        trg_lang=\"ko\",\n        api_url=str(CUSTOM_TRANSLATE_URL),\n    )\n    test_logger.info(__name__ + \":\" + translated)\n\n\n@pytest.mark.asyncio\nasync def test_google_translate(test_logger):\n    translated = await Translator.google(\n        text=\"hello\",\n        src_lang=\"en\",\n        trg_lang=\"ko\",\n        api_key=str(GOOGLE_TRANSLATE_API_KEY),\n    )\n    test_logger.info(__name__ + \":\" + translated)\n\n\n@pytest.mark.asyncio\nasync def test_papago(test_logger):\n    translated = await Translator.papago(\n        text=\"hello\",\n        src_lang=\"en\",\n        trg_lang=\"ko\",\n        client_id=str(PAPAGO_CLIENT_ID),\n        client_secret=str(PAPAGO_CLIENT_SECRET),\n    )\n    test_logger.info(__name__ + \":\" + translated)\n\n\n@pytest.mark.asyncio\nasync def test_deepl(test_logger):\n    translated = await Translator.deepl_via_rapid_api(\n        text=\"hello\",\n        src_lang=\"en\",\n        trg_lang=\"ko\",\n        api_key=str(RAPID_API_KEY),\n    )\n    test_logger.info(__name__ + \":\" + translated)\n\n\n@pytest.mark.asyncio\nasync def test_auto_translate(test_logger):\n    translated = await Translator.translate(\n        text=\"hello\",\n        src_lang=\"en\",\n        trg_lang=\"ko\",\n    )\n    test_logger.info(\"FIRST:\" + translated + str(Translator.cached_function) + str(Translator.cached_args))\n    translated = await Translator.translate(\n        text=\"what is your name?\",\n        src_lang=\"en\",\n        trg_lang=\"ko\",\n    )\n    test_logger.info(\"SECOND:\" + translated + str(Translator.cached_function) + str(Translator.cached_args))\n"}
{"type": "test_file", "path": "tests/conftest.py", "content": "import asyncio\nfrom datetime import datetime\nfrom typing import AsyncGenerator, Generator\nfrom fastapi import FastAPI\nfrom fastapi.testclient import TestClient\nimport pytest\nimport httpx\nimport pytest_asyncio\nfrom app.utils.auth.token import create_access_token\nfrom app.utils.tests.random import random_user_generator\nfrom app.database.schemas.auth import Users\nfrom app.common.app_settings import create_app\nfrom app.common.config import Config, LoggingConfig\nfrom app.utils.logger import CustomLogger\nfrom app.models.base_models import UserToken\nfrom app.utils.chat.managers.cache import CacheManager, cache\n\n\n@pytest.fixture(scope=\"session\")\ndef config():\n    return Config.get(option=\"test\")\n\n\n@pytest.fixture(scope=\"session\")\ndef cache_manager():\n    cache.start(config=Config.get(option=\"test\"))\n    return CacheManager\n\n\n@pytest.fixture(scope=\"session\")\ndef test_logger():\n    return CustomLogger(\n        name=\"PyTest\", logging_config=LoggingConfig(file_log_name=\"./logs/test.log\")\n    )\n\n\n@pytest.fixture(scope=\"session\", autouse=True)\ndef event_loop():\n    loop = asyncio.get_event_loop()\n    yield loop\n    loop.close()\n\n\n@pytest.fixture(scope=\"session\")\ndef app(config) -> FastAPI:\n    return create_app(config)\n\n\n@pytest.fixture(scope=\"session\")\ndef base_http_url() -> str:\n    return \"http://localhost\"\n\n\n@pytest.fixture(scope=\"session\")\ndef base_websocket_url() -> str:\n    return \"ws://localhost\"\n\n\n@pytest_asyncio.fixture(scope=\"session\")\nasync def async_client(\n    app: FastAPI, base_http_url: str\n) -> AsyncGenerator[httpx.AsyncClient, None]:\n    async with httpx.AsyncClient(app=app, base_url=base_http_url) as ac:\n        yield ac\n\n\n@pytest.fixture(scope=\"session\")\ndef client(app: FastAPI) -> Generator[TestClient, None, None]:\n    with TestClient(app=app) as tc:\n        yield tc\n\n\n@pytest_asyncio.fixture(scope=\"session\")\nasync def login_header(random_user: dict[str, str]) -> dict[str, str]:\n    \"\"\"\n    테스트 전 사용자 미리 등록\n    \"\"\"\n    new_user = await Users.add_one(autocommit=True, refresh=True, **random_user)  # type: ignore\n    access_token = create_access_token(\n        data=UserToken.from_orm(new_user).dict(exclude={\"password\", \"marketing_agree\"}),\n        expires_delta=24,\n    )\n    return {\"Authorization\": f\"Bearer {access_token}\"}\n\n\n@pytest_asyncio.fixture(scope=\"session\")\nasync def api_key_dict(async_client: httpx.AsyncClient, login_header: dict[str, str]):\n    api_key_memo: str = f\"TESTING : {str(datetime.now())}\"\n    response: httpx.Response = await async_client.post(\n        \"/api/user/apikeys\",\n        json={\"user_memo\": api_key_memo},\n        headers=login_header,\n    )\n    response_body = response.json()\n    assert response.status_code == 201\n    assert \"access_key\" in response_body\n    assert \"secret_key\" in response_body\n    apikey = {\n        \"access_key\": response_body[\"access_key\"],\n        \"secret_key\": response_body[\"secret_key\"],\n    }\n\n    response = await async_client.get(\"/api/user/apikeys\", headers=login_header)\n    response_body = response.json()\n    assert response.status_code == 200\n    assert api_key_memo in response_body[0][\"user_memo\"]\n    return apikey\n\n\n@pytest.fixture(scope=\"session\")\ndef random_user():\n    return random_user_generator()\n\n\ndef pytest_collection_modifyitems(items):\n    app_tests = []\n    redis_tests = []\n    other_tests = []\n    for item in items:\n        if \"client\" in item.fixturenames:\n            app_tests.append(item)\n        elif item.get_closest_marker(\"redistest\") is not None:\n            redis_tests.append(item)\n        else:\n            other_tests.append(item)\n    items[:] = redis_tests + app_tests + other_tests\n"}
{"type": "test_file", "path": "tests/test_database.py", "content": "import pytest\nfrom sqlalchemy import select\nfrom uuid import uuid4\nfrom app.common.config import Config\nfrom app.models.base_models import AddApiKey, UserToken\nfrom app.database.connection import db\nfrom app.database.schemas.auth import Users, ApiKeys\nfrom app.database.crud.api_keys import create_api_key, get_api_key_and_owner\nfrom app.middlewares.token_validator import Validator\nfrom app.utils.date_utils import UTC\nfrom app.utils.params_utils import hash_params, parse_params\n\ndb.start(config=Config.get(option=\"test\"))\n\n\n@pytest.mark.asyncio\nasync def test_apikey_idenfitication(random_user: dict[str, str]):\n    user: Users = await Users.add_one(autocommit=True, refresh=True, **random_user)  # type: ignore\n    additional_key_info: AddApiKey = AddApiKey(user_memo=\"[Testing] test_apikey_query\")\n    new_api_key: ApiKeys = await create_api_key(\n        user_id=user.id, additional_key_info=additional_key_info\n    )\n    matched_api_key, matched_user = await get_api_key_and_owner(\n        access_key=new_api_key.access_key\n    )\n    assert matched_api_key.access_key == new_api_key.access_key\n    assert random_user[\"email\"] == matched_user.email\n\n\n@pytest.mark.asyncio\nasync def test_api_key_validation(random_user: dict[str, str]):\n    user: Users = await Users.add_one(autocommit=True, refresh=True, **random_user)  # type: ignore\n    additional_key_info: AddApiKey = AddApiKey(user_memo=\"[Testing] test_apikey_query\")\n    new_api_key: ApiKeys = await create_api_key(\n        user_id=user.id, additional_key_info=additional_key_info\n    )\n    timestamp: str = str(UTC.timestamp())\n    parsed_qs: str = parse_params(\n        params={\"key\": new_api_key.access_key, \"timestamp\": timestamp}\n    )\n    user_token: UserToken = await Validator.api_key(\n        api_access_key=new_api_key.access_key,\n        hashed_secret=hash_params(\n            query_params=parsed_qs, secret_key=new_api_key.secret_key\n        ),\n        query_params=parsed_qs,\n        timestamp=timestamp,\n    )\n    assert user_token.id == user.id\n\n\n@pytest.mark.asyncio\nasync def test_crud():\n    total_users: int = 4\n    users: list[dict[str, str]] = [\n        {\"email\": str(uuid4())[:18], \"password\": str(uuid4())[:18]}\n        for _ in range(total_users)\n    ]\n    user_1, user_2, user_3, user_4 = users\n\n    # C/U\n    await Users.add_all(user_1, user_2, autocommit=True, refresh=True)\n    await Users.add_one(autocommit=True, refresh=True, **user_3)\n    await Users.update_filtered(\n        Users.email == user_1[\"email\"],\n        Users.password == user_1[\"password\"],\n        updated={\"email\": \"UPDATED\", \"password\": \"updated\"},\n        autocommit=True,\n    )\n\n    # R\n    result_1_stmt = select(Users).filter(\n        Users.email.in_([user_1[\"email\"], \"UPDATED\", user_3[\"email\"]])\n    )\n    result_1 = await db.scalars__fetchall(result_1_stmt)\n    assert len(result_1) == 2\n    assert result_1[0].email == \"UPDATED\"  # type: ignore\n    assert result_1[1].email == user_3[\"email\"]  # type: ignore\n    result_2 = await Users.one_filtered_by(**user_2)\n    assert result_2.email == user_2[\"email\"]  # type: ignore\n    result_3 = await Users.fetchall_filtered_by(**user_4)\n    assert len(result_3) == 0\n\n    # D\n    await db.delete(result_2, autocommit=True)\n    result_4_stmt = select(Users).filter_by(**user_2)\n    result_4 = (await db.scalars(stmt=result_4_stmt)).first()\n    assert result_4 is None\n"}
{"type": "test_file", "path": "app/utils/tests/timeit.py", "content": "from asyncio import iscoroutinefunction\nfrom functools import wraps\nfrom time import time\nfrom typing import Any, AsyncGenerator, Callable, Generator\n\nfrom app.common.config import logging_config\nfrom app.utils.logger import CustomLogger\n\nlogger = CustomLogger(\n    name=__name__,\n    logging_config=logging_config,\n)\n\n\ndef log_time(fn: Any, elapsed: float) -> None:\n    formatted_time = f\"{int(elapsed // 60):02d}:{int(elapsed % 60):02d}:{int((elapsed - int(elapsed)) * 1000000):06d}\"\n    logger.info(f\"Function {fn.__name__} execution time: {formatted_time}\")\n\n\ndef timeit(fn: Callable) -> Callable:\n    \"\"\"\n    Decorator to measure execution time of a function, or generator.\n    Supports both synchronous and asynchronous functions and generators.\n    :param fn: function to measure execution time of\n    :return: function wrapper\n\n    Usage:\n    >>> @timeit\n    >>> def my_function():\n    >>>     ...\n    \"\"\"\n\n    @wraps(fn)\n    async def coroutine_function_wrapper(*args: Any, **kwargs: Any) -> Any:\n        start_time: float = time()\n        fn_result = await fn(*args, **kwargs)\n        elapsed: float = time() - start_time\n        log_time(fn, elapsed)\n        return fn_result\n\n    @wraps(fn)\n    def noncoroutine_function_wrapper(*args: Any, **kwargs: Any) -> Any:\n        def sync_generator_wrapper() -> Generator:\n            while True:\n                try:\n                    start_time: float = time()\n                    item: Any = next(fn_result)\n                    elapsed: float = time() - start_time\n                    log_time(fn_result, elapsed)\n                    yield item\n                except StopIteration:\n                    break\n\n        async def async_generator_wrapper() -> AsyncGenerator:\n            while True:\n                try:\n                    start_time: float = time()\n                    item: Any = await anext(fn_result)\n                    elapsed: float = time() - start_time\n                    log_time(fn_result, elapsed)\n                    yield item\n                except StopAsyncIteration:\n                    break\n\n        start_time: float = time()\n        fn_result: Any = fn(*args, **kwargs)\n        elapsed: float = time() - start_time\n        if isinstance(fn_result, Generator):\n            return sync_generator_wrapper()\n        elif isinstance(fn_result, AsyncGenerator):\n            return async_generator_wrapper()\n        log_time(fn, elapsed)\n        return fn_result\n\n    if iscoroutinefunction(fn):\n        return coroutine_function_wrapper\n    return noncoroutine_function_wrapper\n"}
{"type": "test_file", "path": "tests/test_chat.py", "content": "import json\nimport time\n\nimport pytest\nfrom fastapi.testclient import TestClient\nfrom starlette.testclient import WebSocketTestSession\n\nfrom app.common.config import OPENAI_API_KEY\nfrom app.models.chat_models import (\n    ChatRoles,\n    MessageHistory,\n    UserChatContext,\n    UserChatProfile,\n)\nfrom app.models.llms import LLMModels\nfrom app.models.base_models import MessageFromWebsocket, MessageToWebsocket\n\n\n# @pytest.mark.skip\n@pytest.mark.asyncio\nasync def test_chat_redis(cache_manager):\n    # set random context\n    user_id: str = \"test_user\"\n    test_chat_room_id: str = \"test_chat_room\"\n    role: ChatRoles = ChatRoles.USER\n    message: str = \"test message\"\n    default_context: UserChatContext = UserChatContext.construct_default(\n        user_id=user_id, chat_room_id=test_chat_room_id\n    )\n    new_context: UserChatContext = UserChatContext(\n        user_chat_profile=UserChatProfile(\n            user_id=user_id,\n            chat_room_id=test_chat_room_id,\n        ),\n        llm_model=LLMModels.gpt_4,\n    )\n\n    # delete test chat room\n    await cache_manager.delete_chat_room(\n        user_id=user_id, chat_room_id=test_chat_room_id\n    )\n\n    # create new context\n    await cache_manager.create_context(\n        user_chat_context=new_context,\n    )\n    # read new context\n    assert (\n        new_context.user_chat_profile.chat_room_id\n        == (\n            await cache_manager.read_context_from_profile(\n                user_chat_profile=new_context.user_chat_profile\n            )\n        ).user_chat_profile.chat_room_id\n    )\n\n    # add new message to redis\n    new_message: MessageHistory = MessageHistory(\n        role=role.value,\n        content=message,\n        actual_role=role.value,\n        tokens=new_context.get_tokens_of(message),\n    )\n    await cache_manager.append_message_history(\n        user_id=user_id,\n        chat_room_id=test_chat_room_id,\n        role=role,\n        message_history=new_message,\n    )\n\n    # read message from redis\n    message_histories: list[MessageHistory] = await cache_manager.get_message_history(\n        user_id=user_id, chat_room_id=test_chat_room_id, role=role\n    )\n    assert message_histories == [new_message]\n\n    # reset context and read context\n    await cache_manager.reset_context(user_id=user_id, chat_room_id=test_chat_room_id)\n    assert (\n        default_context.user_chat_profile.chat_room_id\n        == (\n            await cache_manager.read_context_from_profile(\n                user_chat_profile=default_context.user_chat_profile\n            )\n        ).user_chat_profile.chat_room_id\n    )\n\n    # delete test chat room\n    await cache_manager.delete_chat_room(\n        user_id=user_id, chat_room_id=test_chat_room_id\n    )\n\n\n@pytest.mark.skipif(OPENAI_API_KEY is None, reason=\"OpenAI API Key is not set\")\ndef test_chat_connection(client: TestClient, base_websocket_url: str):\n    with client.websocket_connect(\n        f\"{base_websocket_url}/ws/chat/{OPENAI_API_KEY}\"\n    ) as ws_client:\n        assert isinstance(ws_client, WebSocketTestSession)\n\n        client_received: MessageToWebsocket = MessageToWebsocket.parse_raw(\n            ws_client.receive_text()\n        )\n        assert client_received.init\n        client_received: MessageToWebsocket = MessageToWebsocket.parse_raw(\n            ws_client.receive_text()\n        )\n        assert client_received.msg is not None and \"tokens\" in json.loads(\n            client_received.msg\n        )\n        assert client_received.chat_room_id is not None\n        # send message to websocket\n        ws_client.send_json(\n            MessageFromWebsocket(\n                msg=\"/ping\",\n                chat_room_id=client_received.chat_room_id,\n            ).dict()\n        )\n        # receive message from websocket\n        client_received: MessageToWebsocket = MessageToWebsocket.parse_raw(\n            ws_client.receive_text()\n        )\n        assert client_received.msg == \"pong\"\n        # close websocket\n        ws_client.close()\n\n\n@pytest.mark.skipif(OPENAI_API_KEY is None, reason=\"OpenAI API Key is not set\")\ndef test_chat_conversation(client: TestClient, base_websocket_url: str, test_logger):\n    # parameters\n    timeout: int = 10\n    with client.websocket_connect(\n        f\"{base_websocket_url}/ws/chat/{OPENAI_API_KEY}\"\n    ) as ws_client:\n        assert isinstance(ws_client, WebSocketTestSession)\n        client_received: MessageToWebsocket = MessageToWebsocket.parse_raw(\n            ws_client.receive_text()\n        )\n        assert client_received.init\n\n        assert client_received.chat_room_id is not None\n        # send message to websocket\n        ws_client.send_json(\n            MessageFromWebsocket(\n                msg=\"say this word: TEST\",\n                translate=None,\n                chat_room_id=client_received.chat_room_id,\n            ).dict()\n        )\n\n        # receive messages from websocket, loop until received message with finish=True\n        # timeout: 10 seconds\n        received_messages: list[MessageToWebsocket] = []\n        now: float = time.time()\n        while time.time() - now < timeout:\n            client_received: MessageToWebsocket = MessageToWebsocket.parse_raw(\n                ws_client.receive_text()\n            )\n            received_messages.append(client_received)\n            if client_received.finish:\n                break\n        assert len(received_messages) > 0\n\n        # show received messages\n        for msg in received_messages:\n            test_logger.info(msg)\n\n        # assemble msg from received messages using list comprehension\n        received_msg: str = \"\".join(\n            [msg.msg for msg in received_messages if msg.msg is not None]\n        )\n        assert \"TEST\" in received_msg\n\n        # close websocket\n        ws_client.close()\n"}
{"type": "test_file", "path": "tests/__init__.py", "content": ""}
{"type": "source_file", "path": "app/database/__init__.py", "content": "from sqlalchemy.orm import declarative_base\nfrom sqlalchemy.orm.decl_api import DeclarativeMeta\nfrom typing import TypeVar\n\nBase: DeclarativeMeta = declarative_base()\nTableGeneric = TypeVar(\"TableGeneric\")\n"}
{"type": "source_file", "path": "app/database/connection.py", "content": "from asyncio import current_task\nfrom collections.abc import Iterable\nfrom typing import Any, AsyncGenerator, Callable, Optional, Type, TypeVar\n\nfrom qdrant_client import QdrantClient\nfrom redis.asyncio import Redis, from_url\nfrom sqlalchemy import (\n    Delete,\n    Result,\n    ScalarResult,\n    Select,\n    TextClause,\n    Update,\n    create_engine,\n    text,\n)\nfrom sqlalchemy.engine.base import Connection, Engine\nfrom sqlalchemy.ext.asyncio import (\n    AsyncEngine,\n    AsyncSession,\n    async_scoped_session,\n    async_sessionmaker,\n    create_async_engine,\n)\nfrom sqlalchemy_utils import create_database, database_exists\n\nfrom app.common.config import Config, SingletonMetaClass, logging_config\nfrom app.errors.api_exceptions import Responses_500\nfrom app.shared import Shared\nfrom app.utils.langchain.qdrant_vectorstore import Qdrant\nfrom app.utils.logger import CustomLogger\n\nfrom . import Base, TableGeneric\n\n\nclass MySQL(metaclass=SingletonMetaClass):\n    query_set: dict = {\n        \"is_user_exists\": \"SELECT EXISTS(SELECT 1 FROM mysql.user WHERE user = '{user}');\",\n        \"is_db_exists\": \"SELECT SCHEMA_NAME FROM INFORMATION_SCHEMA.SCHEMATA WHERE SCHEMA_NAME = '{database}';\",\n        \"is_user_granted\": (\n            \"SELECT * FROM information_schema.schema_privileges \"\n            \"WHERE table_schema = '{database}' AND grantee = '{user}';\"\n        ),\n        \"create_user\": \"CREATE USER '{user}'@'{host}' IDENTIFIED BY '{password}'\",\n        \"grant_user\": \"GRANT {grant} ON {on} TO '{to_user}'@'{user_host}'\",\n        \"create_db\": \"CREATE DATABASE {database} CHARACTER SET utf8mb4 COLLATE utf8mb4_bin;\",\n        \"drop_db\": \"DROP DATABASE {database};\",\n    }\n\n    @staticmethod\n    def execute(\n        query: str, engine_or_conn: Engine | Connection, scalar: bool = False\n    ) -> Any | None:\n        if isinstance(engine_or_conn, Engine) and not isinstance(\n            engine_or_conn, Connection\n        ):\n            with engine_or_conn.connect() as conn:\n                cursor = conn.execute(\n                    text(query + \";\" if not query.endswith(\";\") else query)\n                )\n                return cursor.scalar() if scalar else None\n        elif isinstance(engine_or_conn, Connection):\n            cursor = engine_or_conn.execute(\n                text(query + \";\" if not query.endswith(\";\") else query)\n            )\n            return cursor.scalar() if scalar else None\n\n    @staticmethod\n    def clear_all_table_data(engine: Engine, except_tables: list[str] | None = None):\n        with engine.connect() as conn:\n            conn.execute(text(\"SET FOREIGN_KEY_CHECKS = 0;\"))\n            for table in Base.metadata.sorted_tables:\n                if except_tables is not None:\n                    conn.execute(\n                        table.delete()\n                    ) if table.name not in except_tables else ...\n            conn.execute(text(\"SET FOREIGN_KEY_CHECKS = 1;\"))\n            conn.commit()\n\n    @classmethod\n    def is_db_exists(cls, database: str, engine_or_conn: Engine | Connection) -> bool:\n        return bool(\n            cls.execute(\n                cls.query_set[\"is_db_exists\"].format(database=database),\n                engine_or_conn,\n                scalar=True,\n            )\n        )\n\n    @classmethod\n    def is_user_exists(cls, user: str, engine_or_conn: Engine | Connection) -> bool:\n        return bool(\n            cls.execute(\n                cls.query_set[\"is_user_exists\"].format(user=user),\n                engine_or_conn,\n                scalar=True,\n            )\n        )\n\n    @classmethod\n    def is_user_granted(\n        cls, user: str, database: str, engine_or_conn: Engine | Connection\n    ) -> bool:\n        return bool(\n            cls.execute(\n                cls.query_set[\"is_user_granted\"].format(user=user, database=database),\n                engine_or_conn,\n                scalar=True,\n            )\n        )\n\n    @classmethod\n    def drop_db(cls, database: str, engine_or_conn: Engine | Connection) -> None:\n        return cls.execute(\n            cls.query_set[\"drop_db\"].format(database=database),\n            engine_or_conn,\n        )\n\n    @classmethod\n    def create_db(cls, database: str, engine_or_conn: Engine | Connection) -> None:\n        return cls.execute(\n            cls.query_set[\"create_db\"].format(database=database),\n            engine_or_conn,\n        )\n\n    @classmethod\n    def create_user(\n        cls,\n        user: str,\n        password: str,\n        host: str,\n        engine_or_conn: Engine | Connection,\n    ) -> None:\n        return cls.execute(\n            cls.query_set[\"create_user\"].format(\n                user=user, password=password, host=host\n            ),\n            engine_or_conn,\n        )\n\n    @classmethod\n    def grant_user(\n        cls,\n        grant: str,\n        on: str,\n        to_user: str,\n        user_host: str,\n        engine_or_conn: Engine | Connection,\n    ) -> None:\n        return cls.execute(\n            cls.query_set[\"grant_user\"].format(\n                grant=grant, on=on, to_user=to_user, user_host=user_host\n            ),\n            engine_or_conn,\n        )\n\n\nclass SQLAlchemy(metaclass=SingletonMetaClass):\n    def __init__(self):\n        self.is_test_mode: bool = False\n        self.root_engine: Engine | None = None\n        self.engine: AsyncEngine | None = None\n        self.session: async_scoped_session[AsyncSession] | None = None\n        self.is_initiated = False\n        self.logger = CustomLogger(\"SQLAlchemy\", logging_config=logging_config)\n\n    def start(self, config: Config) -> None:\n        if self.is_initiated:\n            return\n        self.is_test_mode = True if config.test_mode else False\n        self.log(\n            f\"Current DB connection of {type(config).__name__}: \"\n            + f\"{config.mysql_host}/{config.mysql_database}@{config.mysql_user}\"\n        )\n\n        if not database_exists(config.mysql_root_url):\n            create_database(config.mysql_root_url)\n\n        self.root_engine = create_engine(config.mysql_root_url, echo=config.db_echo)\n        with self.root_engine.connect() as conn:\n            if not MySQL.is_user_exists(config.mysql_user, engine_or_conn=conn):\n                MySQL.create_user(\n                    config.mysql_user, config.mysql_password, \"%\", engine_or_conn=conn\n                )\n            if not MySQL.is_user_granted(\n                config.mysql_user, config.mysql_database, engine_or_conn=conn\n            ):\n                MySQL.grant_user(\n                    \"ALL PRIVILEGES\",\n                    f\"{config.mysql_database}.*\",\n                    config.mysql_user,\n                    \"%\",\n                    engine_or_conn=conn,\n                )\n            Base.metadata.drop_all(conn) if self.is_test_mode else ...\n            Base.metadata.create_all(conn)\n            Base.metadata.reflect(conn)\n            conn.commit()\n        self.root_engine.dispose()\n        self.root_engine = None\n        self.engine = create_async_engine(\n            config.mysql_url,\n            echo=config.db_echo,\n            pool_recycle=config.db_pool_recycle,\n            pool_pre_ping=True,\n        )\n        self.session = async_scoped_session(\n            async_sessionmaker(\n                bind=self.engine, autocommit=False, autoflush=False, future=True\n            ),\n            scopefunc=current_task,\n        )\n        self.is_initiated = True\n\n    async def close(self) -> None:\n        if self.session is not None:\n            await self.session.close()\n        if self.engine is not None:\n            await self.engine.dispose()\n        if self.root_engine is not None:\n            self.root_engine.dispose()\n        self.is_initiated = False\n\n    async def get_db(self) -> AsyncGenerator[AsyncSession, str]:\n        if self.session is None:\n            raise Responses_500.database_not_initialized\n        async with self.session() as transaction:\n            yield transaction\n\n    def run_in_session(self, func: Callable) -> Callable:\n        async def wrapper(\n            session: AsyncSession | None = None,\n            autocommit: bool = False,\n            refresh: bool = False,\n            *args: Any,\n            **kwargs: Any,\n        ):\n            if session is None:\n                if self.session is None:\n                    raise Responses_500.database_not_initialized\n                async with self.session() as transaction:\n                    result = await func(transaction, *args, **kwargs)\n                    if autocommit:\n                        await transaction.commit()\n                    if refresh:\n                        [await transaction.refresh(r) for r in result] if isinstance(\n                            result, Iterable\n                        ) else await transaction.refresh(result)\n            else:\n                result = await func(session, *args, **kwargs)\n                if autocommit:\n                    await session.commit()\n                if refresh:\n                    [await session.refresh(r) for r in result] if isinstance(\n                        result, Iterable\n                    ) else await session.refresh(result)\n            return result\n\n        return wrapper\n\n    def log(self, msg) -> None:\n        self.logger.critical(msg)\n\n    async def _execute(  # To be decorated\n        self, session: AsyncSession, stmt: TextClause | Update | Delete | Select\n    ) -> Result:\n        return await session.execute(stmt)\n\n    async def _scalar(  # To be decorated\n        self,\n        session: AsyncSession,\n        stmt: Select,\n    ) -> Any:\n        return await session.scalar(stmt)\n\n    async def _scalars(  # To be decorated\n        self,\n        session: AsyncSession,\n        stmt: Select,\n    ) -> ScalarResult:\n        return await session.scalars(stmt)\n\n    async def _add(  # To be decorated\n        self,\n        session: AsyncSession,\n        instance: TableGeneric,\n    ) -> TableGeneric:\n        session.add(instance)\n        return instance\n\n    async def _add_all(  # To be decorated\n        self,\n        session: AsyncSession,\n        instances: Iterable[TableGeneric],\n    ) -> Iterable[TableGeneric]:\n        session.add_all(instances)\n        return instances\n\n    async def _delete(  # To be decorated\n        self,\n        session: AsyncSession,\n        instance: TableGeneric,\n    ) -> TableGeneric:\n        await session.delete(instance)\n        return instance\n\n    async def execute(\n        self,\n        stmt: TextClause | Update | Delete | Select,\n        autocommit: bool = False,\n        refresh: bool = False,\n        session: AsyncSession | None = None,\n    ) -> Result:\n        return await self.run_in_session(self._execute)(\n            session, autocommit=autocommit, refresh=refresh, stmt=stmt\n        )\n\n    async def scalar(self, stmt: Select, session: AsyncSession | None = None) -> Any:\n        return await self.run_in_session(self._scalar)(session, stmt=stmt)\n\n    async def scalars(\n        self, stmt: Select, session: AsyncSession | None = None\n    ) -> ScalarResult:\n        return await self.run_in_session(self._scalars)(session, stmt=stmt)\n\n    async def add(\n        self,\n        schema: Type[TableGeneric],\n        autocommit: bool = False,\n        refresh: bool = False,\n        session: AsyncSession | None = None,\n        **kwargs: Any,\n    ) -> TableGeneric:\n        instance = schema(**kwargs)\n        return await self.run_in_session(self._add)(\n            session, autocommit=autocommit, refresh=refresh, instance=instance\n        )\n\n    async def add_all(\n        self,\n        schema: Type[TableGeneric],\n        *args: dict,\n        autocommit: bool = False,\n        refresh: bool = False,\n        session: AsyncSession | None = None,\n    ) -> list[TableGeneric]:\n        instances = [schema(**arg) for arg in args]  # type: ignore\n        return await self.run_in_session(self._add_all)(\n            session, autocommit=autocommit, refresh=refresh, instances=instances\n        )\n\n    async def delete(\n        self,\n        instance: TableGeneric,\n        autocommit: bool = False,\n        refresh: bool = False,\n        session: AsyncSession | None = None,\n    ) -> TableGeneric:\n        return await self.run_in_session(self._delete)(\n            session, autocommit=autocommit, refresh=refresh, instance=instance\n        )\n\n    async def scalars__fetchall(\n        self, stmt: Select, session: AsyncSession | None = None\n    ) -> list[TableGeneric]:  # type: ignore\n        return (await self.run_in_session(self._scalars)(session, stmt=stmt)).fetchall()\n\n    async def scalars__one(\n        self, stmt: Select, session: AsyncSession | None = None\n    ) -> TableGeneric:  # type: ignore\n        return (await self.run_in_session(self._scalars)(session, stmt=stmt)).one()\n\n    async def scalars__first(\n        self, stmt: Select, session: AsyncSession | None = None\n    ) -> TableGeneric:  # type: ignore\n        return (await self.run_in_session(self._scalars)(session, stmt=stmt)).first()\n\n    async def scalars__one_or_none(\n        self, stmt: Select, session: AsyncSession | None = None\n    ) -> Optional[TableGeneric]:  # type: ignore\n        return (\n            await self.run_in_session(self._scalars)(session, stmt=stmt)\n        ).one_or_none()\n\n\nclass CacheFactory(metaclass=SingletonMetaClass):\n    def __init__(self):\n        self._vectorstore: Optional[Qdrant] = None\n        self.is_test_mode: bool = False\n        self.is_initiated: bool = False\n\n    def start(\n        self,\n        config: Config,\n    ) -> None:\n        if self.is_initiated:\n            return\n        self.is_test_mode = True if config.test_mode else False\n        self._redis = from_url(url=config.redis_url)\n        self._vectorstore = Qdrant(\n            client=QdrantClient(\n                host=config.qdrant_host,\n                port=config.qdrant_port,\n                grpc_port=config.qdrant_grpc_port,\n                prefer_grpc=True,\n            ),\n            collection_name=config.shared_vectorestore_name,\n            embeddings=Shared().embeddings,\n        )\n        self.is_initiated = True\n\n    async def close(self) -> None:\n        if self._redis is not None:\n            assert isinstance(self._redis, Redis)\n            await self._redis.close()\n        self.is_initiated = False\n\n    @property\n    def redis(self) -> Redis:\n        try:\n            assert self._redis is not None\n            assert isinstance(self._redis, Redis)\n        except AssertionError:\n            raise Responses_500.cache_not_initialized\n        return self._redis\n\n    @property\n    def vectorstore(self) -> Qdrant:\n        try:\n            assert self._vectorstore is not None\n        except AssertionError:\n            raise Responses_500.cache_not_initialized\n        return self._vectorstore\n\n\ndb: SQLAlchemy = SQLAlchemy()\ncache: CacheFactory = CacheFactory()\n"}
{"type": "source_file", "path": "app/database/schemas/__init__.py", "content": "from datetime import datetime\nfrom typing import Any, Optional, Type\n\nfrom sqlalchemy import Column, Select, String, func, select, update, delete\nfrom sqlalchemy.ext.asyncio import AsyncSession\nfrom sqlalchemy.orm import Mapped, mapped_column\nfrom sqlalchemy.sql._typing import _ColumnExpressionArgument\nfrom sqlalchemy.sql.roles import ExpressionElementRole\nfrom ..connection import db\nfrom .. import TableGeneric\n\n\nclass TableMixin:\n    id: Mapped[int] = mapped_column(primary_key=True)\n    created_at: Mapped[datetime] = mapped_column(default=func.utc_timestamp())\n    updated_at: Mapped[datetime] = mapped_column(\n        default=func.utc_timestamp(),\n        onupdate=func.utc_timestamp(),\n    )\n    ip_address: Mapped[str | None] = mapped_column(String(length=40))\n\n    def set_values_as(self, **kwargs):\n        for key, value in kwargs.items():\n            setattr(self, key, value)\n\n    def to_dict(self, *args, exclude: list | None = None):\n        q_dict: dict = {}\n        for c in self.__table__.columns:  # type: ignore\n            if not args or c.name in args:\n                if not exclude or c.name not in exclude:\n                    q_dict[c.name] = getattr(self, c.name)\n        return q_dict\n\n    @property\n    def columns(self) -> list[Column]:\n        return [\n            col\n            for col in self.__table__.columns  # type: ignore\n            if (not col.primary_key) and (col.name != \"created_at\")\n        ]\n\n    @classmethod\n    async def add_all(\n        cls: Type[TableGeneric],\n        *args: dict,\n        autocommit: bool = False,\n        refresh: bool = False,\n        session: AsyncSession | None = None,\n    ) -> list[TableGeneric]:\n        return await db.add_all(\n            cls,\n            *args,\n            autocommit=autocommit,\n            refresh=refresh,\n            session=session,\n        )\n\n    @classmethod\n    async def add_one(\n        cls: Type[TableGeneric],\n        autocommit: bool = False,\n        refresh: bool = False,\n        session: AsyncSession | None = None,\n        **kwargs: Any,\n    ) -> TableGeneric:\n        return await db.add(\n            cls,\n            autocommit=autocommit,\n            refresh=refresh,\n            session=session,\n            **kwargs,\n        )\n\n    @classmethod\n    async def update_filtered(\n        cls: Type[TableGeneric],\n        *criteria: ExpressionElementRole[bool],\n        updated: dict,\n        autocommit: bool = False,\n        refresh: bool = False,\n        session: AsyncSession | None = None,\n    ) -> TableGeneric:\n        stmt = update(cls).filter(*criteria).values(**updated)\n        return await db.run_in_session(db._execute)(\n            session,\n            autocommit=autocommit,\n            refresh=refresh,\n            stmt=stmt,\n        )\n\n    @classmethod\n    async def delete_filtered(\n        cls: Type[TableGeneric],\n        *criteria: ExpressionElementRole[bool],\n        autocommit: bool = False,\n        refresh: bool = False,\n        session: AsyncSession | None = None,\n    ) -> TableGeneric:\n        stmt = delete(cls).filter(*criteria)\n        return await db.run_in_session(db._execute)(\n            session,\n            autocommit=autocommit,\n            refresh=refresh,\n            stmt=stmt,\n        )\n\n    @classmethod\n    async def fetchall_filtered_by(\n        cls: Type[TableGeneric], session: AsyncSession | None = None, **kwargs: Any\n    ) -> list[TableGeneric]:\n        stmt: Select[tuple] = select(cls).filter_by(**kwargs)\n        return await db.scalars__fetchall(stmt=stmt, session=session)\n\n    @classmethod\n    async def one_filtered_by(\n        cls: Type[TableGeneric], session: AsyncSession | None = None, **kwargs: Any\n    ) -> TableGeneric:\n        stmt: Select[tuple] = select(cls).filter_by(**kwargs)\n        return await db.scalars__one(stmt=stmt, session=session)\n\n    @classmethod\n    async def first_filtered_by(\n        cls: Type[TableGeneric], session: AsyncSession | None = None, **kwargs: Any\n    ) -> TableGeneric:\n        stmt: Select[tuple] = select(cls).filter_by(**kwargs)\n        return await db.scalars__first(stmt=stmt, session=session)\n\n    @classmethod\n    async def one_or_none_filtered_by(\n        cls: Type[TableGeneric], session: AsyncSession | None = None, **kwargs: Any\n    ) -> Optional[TableGeneric]:\n        stmt: Select[tuple] = select(cls).filter_by(**kwargs)\n        return await db.scalars__one_or_none(stmt=stmt, session=session)\n\n    @classmethod\n    async def fetchall_filtered(\n        cls: Type[TableGeneric],\n        *criteria: _ColumnExpressionArgument[bool],\n        session: AsyncSession | None = None,\n    ) -> list[TableGeneric]:\n        stmt: Select[tuple] = select(cls).filter(*criteria)\n        return await db.scalars__fetchall(stmt=stmt, session=session)\n\n    @classmethod\n    async def one_filtered(\n        cls: Type[TableGeneric],\n        *criteria: _ColumnExpressionArgument[bool],\n        session: AsyncSession | None = None,\n    ) -> TableGeneric:\n        stmt: Select[tuple] = select(cls).filter(*criteria)\n        return await db.scalars__one(stmt=stmt, session=session)\n\n    @classmethod\n    async def first_filtered(\n        cls: Type[TableGeneric],\n        *criteria: _ColumnExpressionArgument[bool],\n        session: AsyncSession | None = None,\n    ) -> TableGeneric:\n        stmt: Select[tuple] = select(cls).filter(*criteria)\n        return await db.scalars__first(stmt=stmt, session=session)\n\n    @classmethod\n    async def one_or_none_filtered(\n        cls: Type[TableGeneric],\n        *criteria: _ColumnExpressionArgument[bool],\n        session: AsyncSession | None = None,\n    ) -> Optional[TableGeneric]:\n        stmt: Select[tuple] = select(cls).filter(*criteria)\n        return await db.scalars__one_or_none(stmt=stmt, session=session)\n"}
{"type": "source_file", "path": "app/__init__.py", "content": ""}
{"type": "source_file", "path": "app/database/crud/api_whitelists.py", "content": "from typing import Optional\nfrom sqlalchemy import select, func\nfrom app.errors.api_exceptions import (\n    Responses_400,\n    Responses_404,\n    Responses_500,\n)\nfrom app.common.config import MAX_API_WHITELIST\nfrom app.database.connection import db\nfrom app.database.schemas.auth import (\n    ApiKeys,\n    ApiWhiteLists,\n)\n\n\nasync def create_api_key_whitelist(\n    ip_address: str,\n    api_key_id: int,\n) -> ApiWhiteLists:\n    if db.session is None:\n        raise Responses_500.database_not_initialized\n    async with db.session() as transaction:\n        whitelist_count_stmt = select(func.count(ApiWhiteLists.id)).filter_by(api_key_id=api_key_id)\n        whitelist_count: int | None = await transaction.scalar(whitelist_count_stmt)\n        if whitelist_count is not None and whitelist_count >= MAX_API_WHITELIST:\n            raise Responses_400.max_whitekey_count_exceed\n        ip_duplicated_whitelist_stmt = select(ApiWhiteLists).filter_by(api_key_id=api_key_id, ip_address=ip_address)\n        ip_duplicated_whitelist = await transaction.scalar(ip_duplicated_whitelist_stmt)\n        if ip_duplicated_whitelist is not None:\n            return ip_duplicated_whitelist\n        new_whitelist = ApiWhiteLists(api_key_id=api_key_id, ip_address=ip_address)\n        transaction.add(new_whitelist)\n        await transaction.commit()\n        await transaction.refresh(new_whitelist)\n        return new_whitelist\n\n\nasync def get_api_key_whitelist(api_key_id: int) -> list[ApiWhiteLists]:\n    return await ApiWhiteLists.fetchall_filtered_by(api_key_id=api_key_id)\n\n\nasync def delete_api_key_whitelist(\n    user_id: int,\n    api_key_id: int,\n    whitelist_id: int,\n) -> None:\n    if db.session is None:\n        raise Responses_500.database_not_initialized\n    async with db.session() as transaction:\n        matched_api_key: Optional[ApiKeys] = await transaction.scalar(\n            select(ApiKeys).filter_by(id=api_key_id, user_id=user_id)\n        )\n        if matched_api_key is None:\n            raise Responses_404.not_found_api_key\n        matched_whitelist_stmt = select(ApiWhiteLists).filter_by(id=whitelist_id, api_key_id=api_key_id)\n        matched_whitelist = await transaction.scalar(matched_whitelist_stmt)\n        await transaction.delete(matched_whitelist)\n        await transaction.commit()\n"}
{"type": "source_file", "path": "app/models/function_calling/base.py", "content": "\"\"\"Helper classes for wrapping functions in OpenAI's API\"\"\"\n\nfrom dataclasses import dataclass\nfrom sys import version_info\nfrom types import NoneType\nfrom typing import (\n    Any,\n    Generic,\n    Literal,\n    Optional,\n    Type,\n    TypeVar,\n    TypedDict,\n    Union,\n)\n\n# If python version >= 3.11, use the built-in NotRequired type.\n# Otherwise, import it from typing_extensi\nif version_info >= (3, 11):\n    from typing import NotRequired  # type: ignore\nelse:\n    from typing_extensions import NotRequired\n\n# The types that can be used in JSON\nJsonTypes = Union[int, float, str, bool, dict, list, None]\n\nParamType = TypeVar(\"ParamType\", bound=JsonTypes)\nReturnType = TypeVar(\"ReturnType\")\n\n\nclass ParameterProperty(TypedDict):\n    type: str\n    description: NotRequired[str]\n    enum: NotRequired[list[JsonTypes]]\n\n\nclass ParameterDefinition(TypedDict):\n    type: Literal[\"object\"]\n    properties: dict[str, ParameterProperty]\n    required: NotRequired[list[str]]\n\n\nclass FunctionProperty(TypedDict):\n    name: str\n    description: NotRequired[str]\n    parameters: NotRequired[ParameterDefinition]\n\n\n@dataclass\nclass FunctionCallParameter(Generic[ParamType]):\n    \"\"\"A class for wrapping function parameters in OpenAI's API\"\"\"\n\n    name: str\n    type: Type[ParamType]\n    description: Optional[str] = None\n    enum: Optional[list[ParamType]] = None\n\n    def to_dict(self) -> dict[str, ParameterProperty]:\n        \"\"\"Returns a dictionary representation of the parameter\"\"\"\n        parameter_property: ParameterProperty = {\n            \"type\": self._get_json_type(self.type)\n        }  # type: ignore\n        if self.description:\n            parameter_property[\"description\"] = self.description\n        if self.enum:\n            parameter_property[\"enum\"] = self.enum  # type: ignore\n        return {self.name: parameter_property}\n\n    @staticmethod\n    def _get_json_type(python_type: Type[JsonTypes]) -> str:\n        \"\"\"Returns the JSON type for a given python type\"\"\"\n        if python_type is int:\n            return \"integer\"\n        elif python_type is float:\n            return \"number\"\n        elif python_type is str:\n            return \"string\"\n        elif python_type is bool:\n            return \"boolean\"\n        elif python_type is dict:\n            return \"object\"\n        elif python_type is list:\n            return \"array\"\n        elif python_type is NoneType or python_type is None:\n            return \"null\"\n        else:\n            raise ValueError(\n                f\"Invalid type {python_type} for JSON. \"\n                f\"Permitted types are {JsonTypes}\"\n            )\n\n\n@dataclass\nclass FunctionCall:\n    \"\"\"A class for wrapping functions in OpenAI's API\"\"\"\n\n    name: str\n    description: Optional[str] = None\n    parameters: Optional[list[FunctionCallParameter[Any]]] = None\n    required: Optional[list[str]] = None\n\n    def to_dict(self) -> FunctionProperty:\n        \"\"\"Returns a dictionary representation of the function\"\"\"\n        function_property: FunctionProperty = FunctionProperty(name=self.name)  # type: ignore\n        if self.description:\n            function_property[\"description\"] = self.description\n        if self.parameters:\n            function_property[\"parameters\"] = {\n                \"type\": \"object\",\n                \"properties\": {\n                    param.name: param.to_dict()[param.name]\n                    for param in self.parameters\n                },\n                \"required\": [\n                    param.name\n                    for param in self.parameters\n                    if param.name in (self.required or [])\n                ],\n            }\n        return function_property\n\n\nif __name__ == \"__main__\":\n\n    def test_callback(test: str) -> int:\n        return len(test)\n\n    param_1: FunctionCallParameter[str] = FunctionCallParameter(\n        name=\"param_1\",\n        type=str,\n        description=\"This is a test1\",\n        enum=[\"a\", \"b\", \"c\", \"d\"],\n    )  # `str` is the type of the parameter\n\n    param_2: FunctionCallParameter[bool] = FunctionCallParameter(\n        name=\"param_2\",\n        type=bool,\n        description=\"This is a test2\",\n        enum=[True, False],\n    )  # `bool` is the type of the parameter\n\n    func: FunctionCall = FunctionCall(\n        name=\"test_function\",\n        description=\"This is a test function\",\n        parameters=[param_1, param_2],\n        required=[\"test\"],\n    )  # There's no type for the function\n"}
{"type": "source_file", "path": "app/errors/api_exceptions.py", "content": "from dataclasses import dataclass\nfrom typing import Optional\n\nfrom fastapi.exceptions import HTTPException\nfrom sqlalchemy.exc import OperationalError\n\nfrom app.common.config import MAX_API_KEY, MAX_API_WHITELIST\n\n\ndef error_codes(status_code: int, internal_code: int) -> str:\n    return f\"{status_code}{str(internal_code).zfill(4)}\"\n\n\nclass APIException(Exception):\n    status_code: int = 500\n    internal_code: int = 0\n    msg: Optional[str]\n    detail: Optional[str]\n    ex: Optional[Exception]\n\n    def __init__(\n        self,\n        *,\n        status_code: int,\n        internal_code: int,\n        msg: Optional[str] = None,\n        detail: Optional[str] = None,\n        ex: Optional[Exception] = None,\n    ):\n        self.status_code = status_code\n        self.code = error_codes(\n            status_code=status_code, internal_code=internal_code\n        )\n        self.msg = msg\n        self.detail = detail\n        self.ex = ex\n        super().__init__(ex)\n\n    def __call__(\n        self,\n        lazy_format: Optional[dict[str, str]] = None,\n        ex: Optional[Exception] = None,\n    ) -> \"APIException\":\n        if (\n            self.msg is not None\n            and self.detail is not None\n            and lazy_format is not None\n        ):  # lazy format for msg and detail\n            self.msg = self.msg.format(**lazy_format)\n            self.detail = self.detail.format(**lazy_format)\n        if ex is not None:  # set exception if exists\n            self.ex = ex\n        return self\n\n\nclass InternalServerError(APIException):\n    status_code: int = 500\n    internal_code: int = 9999\n    msg: str = \"이 에러는 서버측 에러 입니다. 자동으로 리포팅 되며, 빠르게 수정하겠습니다.\"\n    detail: str = \"Internal Server Error\"\n\n    def __init__(self, ex: Optional[Exception] = None):\n        super().__init__(\n            status_code=self.status_code,\n            internal_code=self.internal_code,\n            msg=self.msg,\n            detail=self.detail,\n            ex=ex,\n        )\n\n\nclass InvalidIpError(APIException):\n    status_code: int = 400\n    internal_code: int = 10\n    msg: str = \"{ip}는 올바른 IP 가 아닙니다.\"\n    detail: str = \"invalid IP : {ip}\"\n\n    def __init__(self, ip: str):\n        super().__init__(\n            status_code=400,\n            internal_code=self.internal_code,\n            msg=self.msg.format(ip=ip),\n            detail=self.detail.format(ip=ip),\n        )\n\n\n@dataclass(frozen=True)\nclass Responses_400:\n    \"\"\"\n    클라이언트가 잘못된 방식의 요청을 하고 있음\n    \"\"\"\n\n    no_email_or_password: APIException = APIException(\n        status_code=400,\n        internal_code=1,\n        msg=\"이메일과 비밀번호를 모두 입력해주세요.\",\n        detail=\"Email and PW must be provided.\",\n    )\n    email_already_exists: APIException = APIException(\n        status_code=400,\n        internal_code=2,\n        msg=\"해당 이메일은 이미 가입되어 있습니다.\",\n        detail=\"Email already exists.\",\n    )\n    not_supported_feature: APIException = APIException(\n        status_code=400,\n        internal_code=3,\n        msg=\"해당 기능은 아직 사용할 수 없습니다.\",\n        detail=\"Not supported feature.\",\n    )\n    unregister_failure: APIException = APIException(\n        status_code=400,\n        internal_code=4,\n        msg=\"회원 탈퇴에 실패했습니다.\",\n        detail=\"Failed to unregister.\",\n    )\n    max_key_count_exceed: APIException = APIException(\n        status_code=400,\n        internal_code=8,\n        msg=f\"API 키 생성은 {MAX_API_KEY}개 까지 가능합니다.\",\n        detail=\"Max Key Count Reached\",\n    )\n    max_whitekey_count_exceed: APIException = APIException(\n        status_code=400,\n        internal_code=9,\n        msg=f\"화이트리스트 생성은 {MAX_API_WHITELIST}개 까지 가능합니다.\",\n        detail=\"Max Whitelist Count Reached\",\n    )\n    invalid_ip: APIException = APIException(\n        status_code=400,\n        internal_code=10,\n        msg=\"{ip}는 올바른 IP 가 아닙니다.\",\n        detail=\"invalid IP : {ip}\",\n    )\n    invalid_api_query: APIException = APIException(\n        status_code=400,\n        internal_code=11,\n        msg=\"쿼리스트링은 key, timestamp 2개만 허용되며, 2개 모두 요청시 제출되어야 합니다.\",\n        detail=\"Query String Only Accept key and timestamp.\",\n    )\n    kakao_send_failure: APIException = APIException(\n        status_code=400,\n        internal_code=15,\n        msg=\"카카오톡 전송에 실패했습니다.\",\n        detail=\"Failed to send KAKAO MSG.\",\n    )\n    websocket_in_use: APIException = APIException(\n        status_code=400,\n        internal_code=16,\n        msg=\"이미 사용중인 웹소켓입니다.\",\n        detail=\"Websocket is already in use.\",\n    )\n    invalid_email_format: APIException = APIException(\n        status_code=400,\n        internal_code=17,\n        msg=\"올바르지 않은 이메일 형식 입니다.\",\n        detail=\"Invalid Email Format.\",\n    )\n    email_length_not_in_range: APIException = APIException(\n        status_code=400,\n        internal_code=18,\n        msg=\"이메일은 6자 이상 50자 이하로 입력해주세요.\",\n        detail=\"Email must be 6 ~ 50 characters.\",\n    )\n    password_length_not_in_range: APIException = APIException(\n        status_code=400,\n        internal_code=19,\n        msg=\"비밀번호는 6자 이상 100자 이하로 입력해주세요.\",\n        detail=\"Password must be 6 ~ 100 characters.\",\n    )\n\n\n@dataclass(frozen=True)\nclass Responses_401:\n    \"\"\"\n    클라이언트가 올바른 자격 증명을 제시해야 함 (보안 관련)\n    \"\"\"\n\n    not_authorized: APIException = APIException(\n        status_code=401,\n        internal_code=1,\n        msg=\"로그인이 필요한 서비스 입니다.\",\n        detail=\"Authorization Required\",\n    )\n    token_expired: APIException = APIException(\n        status_code=401,\n        internal_code=6,\n        msg=\"세션이 만료되어 로그아웃 되었습니다.\",\n        detail=\"Token Expired\",\n    )\n    token_decode_failure: APIException = APIException(\n        status_code=401,\n        internal_code=7,\n        msg=\"비정상적인 접근입니다.\",\n        detail=\"Token has been compromised.\",\n    )\n    invalid_api_header: APIException = APIException(\n        status_code=401,\n        internal_code=12,\n        msg=\"헤더에 키 해싱된 Secret 이 없거나, 유효하지 않습니다.\",\n        detail=\"Invalid HMAC secret in Header\",\n    )\n    invalid_timestamp: APIException = APIException(\n        status_code=401,\n        internal_code=13,\n        msg=\"쿼리스트링에 포함된 타임스탬프는 KST 이며, 현재 시간보다 작아야 하고, 현재시간 - 10초 보다는 커야 합니다.\",\n        detail=\"timestamp in Query String must be KST, Timestamp must be less than now, and greater than now - 10.\",\n    )\n\n\n@dataclass(frozen=True)\nclass Responses_404:\n    \"\"\"\n    클라이언트가 무언가를 요청하였으나 해당 항목에 대해 어떤 것도 찾지 못하였음\n    \"\"\"\n\n    not_found_user: APIException = APIException(\n        status_code=404,\n        internal_code=5,\n        msg=\"해당 유저를 찾을 수 없습니다.\",\n        detail=\"Not found user.\",\n    )\n    not_found_access_key: APIException = APIException(\n        status_code=404,\n        internal_code=14,\n        msg=\"해당 Access key과 일치하는 API 키를 찾을 수 없습니다.\",\n        detail=\"Not found such API Access Key\",\n    )\n    not_found_api_key: APIException = APIException(\n        status_code=404,\n        internal_code=7,\n        msg=\"제공된 조건에 부합하는 Api key를 찾을 수 없습니다.\",\n        detail=\"No API Key matched such conditions\",\n    )\n    not_found_preset: APIException = APIException(\n        status_code=404,\n        internal_code=13,\n        msg=\"제공된 조건에 부합하는 프리셋을 찾을 수 없습니다.\",\n        detail=\"No preset matched such conditions\",\n    )\n\n\n@dataclass(frozen=True)\nclass Responses_500:\n    \"\"\"\n    서버 내부적으로 오류가 발생하였음\n    \"\"\"\n\n    middleware_exception: APIException = APIException(\n        status_code=500,\n        internal_code=2,\n        detail=\"Middleware could not be initialized\",\n    )\n    websocket_error: APIException = APIException(\n        status_code=500,\n        internal_code=3,\n        msg=\"웹소켓 연결에 문제 발생\",\n        detail=\"Websocket error\",\n    )\n    database_not_initialized: APIException = APIException(\n        status_code=500,\n        internal_code=4,\n        msg=\"데이터베이스가 초기화 되지 않았습니다.\",\n        detail=\"Database not initialized\",\n    )\n    cache_not_initialized: APIException = APIException(\n        status_code=500,\n        internal_code=5,\n        msg=\"캐시가 초기화 되지 않았습니다.\",\n        detail=\"Cache not initialized\",\n    )\n    vectorestore_not_initialized: APIException = APIException(\n        status_code=500,\n        internal_code=5,\n        msg=\"벡터 저장소가 초기화 되지 않았습니다.\",\n        detail=\"Vector Store not initialized\",\n    )\n\n\ndef exception_handler(\n    error: Exception,\n) -> InternalServerError | HTTPException | APIException:\n    if isinstance(error, APIException):\n        if error.status_code == 500:\n            return InternalServerError(ex=error)\n        else:\n            return error\n    elif isinstance(error, OperationalError):\n        return InternalServerError(ex=error)\n    elif isinstance(error, HTTPException):\n        return error\n    else:\n        return InternalServerError()\n"}
{"type": "source_file", "path": "app/dependencies.py", "content": "from fastapi import Header, Query\nfrom fastapi.security import APIKeyHeader\n\n\ndef api_service_dependency(secret: str = Header(...), key: str = Query(...), timestamp: str = Query(...)):\n    ...  # do some validation or processing with the headers\n\n\nUSER_DEPENDENCY = APIKeyHeader(name=\"Authorization\", auto_error=False)\n"}
{"type": "source_file", "path": "app/database/crud/deprecated_chatgpt.py", "content": "# from typing import Optional\n\n# from sqlalchemy import select\n# from app.errors.api_exceptions import (\n#     Responses_404,\n#     Responses_500,\n# )\n# from app.database.connection import db\n# from app.database.schemas.auth import (\n#     ChatMessages,\n#     ChatRooms,\n#     GptPresets,\n# )\n\n\n# async def create_chat_room(\n#     chat_room_type: str,\n#     name: str,\n#     description: str | None,\n#     user_id: int,\n#     status: str = \"active\",\n# ) -> ChatRooms:\n#     if db.session is None:\n#         raise Responses_500.database_not_initialized\n#     async with db.session() as transaction:\n#         new_chat_room: ChatRooms = ChatRooms(\n#             status=status,\n#             chat_room_type=chat_room_type,\n#             name=name,\n#             description=description,\n#             user_id=user_id,\n#         )\n#         transaction.add(new_chat_room)\n#         await transaction.commit()\n#         await transaction.refresh(new_chat_room)\n#         return new_chat_room\n\n\n# async def get_chat_all_rooms(user_id: int) -> list[ChatRooms]:\n#     return await ChatRooms.fetchall_filtered_by(user_id=user_id)  # type: ignore\n\n\n# async def create_chat_message(\n#     role: str,\n#     message: str,\n#     chat_room_id: int,\n#     user_id: int,\n#     status: str = \"active\",\n# ) -> ChatMessages:\n#     if db.session is None:\n#         raise Responses_500.database_not_initialized\n#     async with db.session() as transaction:\n#         new_chat_message: ChatMessages = ChatMessages(\n#             status=status,\n#             role=role,\n#             message=message,\n#             user_id=user_id,\n#             chat_room_id=chat_room_id,\n#         )\n#         transaction.add(new_chat_message)\n#         await transaction.commit()\n#         await transaction.refresh(new_chat_message)\n#         return new_chat_message\n\n\n# async def get_chat_all_messages(\n#     chat_room_id: int,\n#     user_id: int,\n# ) -> list[ChatMessages]:\n#     return await ChatMessages.fetchall_filtered_by(\n#         user_id=user_id,\n#         chat_room_id=chat_room_id,\n#     )  # type: ignore\n\n\n# async def create_gpt_preset(\n#     user_id: int,\n#     temperature: float,\n#     top_p: float,\n#     presence_penalty: float,\n#     frequency_penalty: float,\n# ) -> GptPresets:\n#     if db.session is None:\n#         raise Responses_500.database_not_initialized\n#     async with db.session() as transaction:\n#         new_gpt_preset: GptPresets = GptPresets(\n#             user_id=user_id,\n#             temperature=temperature,\n#             top_p=top_p,\n#             presence_penalty=presence_penalty,\n#             frequency_penalty=frequency_penalty,\n#         )\n#         transaction.add(new_gpt_preset)\n#         await transaction.commit()\n#         await transaction.refresh(new_gpt_preset)\n#         return new_gpt_preset\n\n\n# async def get_gpt_presets(user_id: int) -> list[GptPresets]:\n#     return await GptPresets.fetchall_filtered_by(user_id=user_id)  # type: ignore\n\n\n# async def get_gpt_preset(user_id: int, preset_id: int) -> GptPresets:\n#     return await GptPresets.fetchone_filtered_by(user_id=user_id, id=preset_id)\n\n\n# async def update_gpt_preset(\n#     user_id: int,\n#     preset_id: int,\n#     temperature: float,\n#     top_p: float,\n#     presence_penalty: float,\n#     frequency_penalty: float,\n#     status: str = \"active\",\n# ) -> GptPresets:\n#     if db.session is None:\n#         raise Responses_500.database_not_initialized\n#     async with db.session() as transaction:\n#         matched_preset: Optional[GptPresets] = await transaction.scalar(\n#             select(GptPresets).filter_by(user_id=user_id, id=preset_id)\n#         )\n#         if matched_preset is None:\n#             raise Responses_404.not_found_preset\n#         matched_preset.set_values_as(\n#             temperature=temperature,\n#             top_p=top_p,\n#             presence_penalty=presence_penalty,\n#             frequency_penalty=frequency_penalty,\n#             status=status,\n#         )\n#         transaction.add(matched_preset)\n#         await transaction.commit()\n#         await transaction.refresh(matched_preset)\n#         return matched_preset\n\n\n# async def delete_gpt_preset(user_id: int, preset_id: int) -> None:\n#     if db.session is None:\n#         raise Responses_500.database_not_initialized\n#     async with db.session() as transaction:\n#         matched_preset: Optional[GptPresets] = await transaction.scalar(\n#             select(GptPresets).filter_by(user_id=user_id, id=preset_id)\n#         )\n#         if matched_preset is None:\n#             raise Responses_404.not_found_preset\n#         await transaction.delete(matched_preset)\n#         await transaction.commit()\n#         return\n"}
{"type": "source_file", "path": "app/common/app_settings_llama_cpp.py", "content": "from contextlib import asynccontextmanager\nfrom multiprocessing import Process\nfrom os import kill\nfrom signal import SIGINT\nfrom threading import Event\nfrom urllib import parse\n\nimport requests\nfrom fastapi import FastAPI\nfrom starlette.middleware.cors import CORSMiddleware\n\nfrom app.shared import Shared\nfrom app.utils.logger import ApiLogger\n\nfrom .config import Config\n\n\ndef check_health(url: str) -> bool:\n    \"\"\"Check if the given url is available or not\"\"\"\n    try:\n        schema = parse.urlparse(url).scheme\n        netloc = parse.urlparse(url).netloc\n        if requests.get(f\"{schema}://{netloc}/health\").status_code != 200:\n            return False\n        return True\n    except Exception:\n        return False\n\n\ndef start_llama_cpp_server(config: Config, shared: Shared):\n    \"\"\"Start Llama CPP server. if it is already running, terminate it first.\"\"\"\n\n    if shared.process.is_alive():\n        ApiLogger.cwarning(\"Terminating existing Llama CPP server\")\n        shared.process.terminate()\n        shared.process.join()\n\n    if config.llama_server_port is None:\n        raise NotImplementedError(\"Llama CPP server port is not set\")\n\n    ApiLogger.ccritical(\"Starting Llama CPP server\")\n    shared.process = Process(\n        target=run_llama_cpp, args=(config.llama_server_port,), daemon=True\n    )\n    shared.process.start()\n\n\ndef shutdown_llama_cpp_server(shared: Shared):\n    \"\"\"Shutdown Llama CPP server.\"\"\"\n    ApiLogger.ccritical(\"Shutting down Llama CPP server\")\n    if shared.process.is_alive() and shared.process.pid:\n        kill(shared.process.pid, SIGINT)\n        shared.process.join()\n\n\ndef monitor_llama_cpp_server(\n    config: Config,\n    shared: Shared,\n) -> None:\n    \"\"\"Monitors the Llama CPP server and handles server availability.\n\n    Parameters:\n    - `config: Config`: An object representing the server configuration.\n    - `shared: Shared`: An object representing shared data.\"\"\"\n    thread_sigterm: Event = shared.thread_terminate_signal\n    if not config.llama_completion_url:\n        return\n    while True:\n        if not check_health(config.llama_completion_url):\n            if thread_sigterm.is_set():\n                break\n            if config.is_llama_booting:\n                continue\n            ApiLogger.cerror(\"Llama CPP server is not available\")\n            config.is_llama_available = False\n            config.is_llama_booting = True\n            try:\n                start_llama_cpp_server(config=config, shared=shared)\n            except (ImportError, NotImplementedError):\n                ApiLogger.cerror(\n                    \"ImportError: Llama CPP server is not available\"\n                )\n                return\n            except Exception:\n                ApiLogger.cexception(\n                    \"Unknown error: Llama CPP server is not available\"\n                )\n                config.is_llama_booting = False\n                continue\n        else:\n            config.is_llama_booting = False\n            config.is_llama_available = True\n    shutdown_llama_cpp_server(shared)\n\n\n@asynccontextmanager\nasync def lifespan_llama_cpp(app: FastAPI):\n    ApiLogger.ccritical(\"🦙 Llama.cpp server is running\")\n    yield\n    ApiLogger.ccritical(\"🦙 Shutting down llama.cpp server...\")\n\n\ndef create_app_llama_cpp():\n    from app.routers import v1\n\n    new_app = FastAPI(\n        title=\"🦙 llama.cpp Python API\",\n        version=\"0.0.1\",\n        lifespan=lifespan_llama_cpp,\n    )\n    new_app.add_middleware(\n        CORSMiddleware,\n        allow_origins=[\"*\"],\n        allow_credentials=True,\n        allow_methods=[\"*\"],\n        allow_headers=[\"*\"],\n    )\n\n    @new_app.get(\"/health\")\n    async def health():\n        return \"ok\"\n\n    new_app.include_router(v1.router)\n    return new_app\n\n\ndef run_llama_cpp(port: int) -> None:\n    from uvicorn import Config, Server\n\n    from maintools import initialize_before_launch\n\n    initialize_before_launch()\n\n    Server(\n        config=Config(\n            create_app_llama_cpp(),\n            host=\"0.0.0.0\",\n            port=port,\n            log_level=\"warning\",\n        )\n    ).run()\n\n\nif __name__ == \"__main__\":\n    run_llama_cpp(port=8002)\n"}
{"type": "source_file", "path": "app/routers/__init__.py", "content": "from app.routers import auth, index, services, user_services, users, websocket\n\n__all__ = [\n    \"auth\",\n    \"index\",\n    \"services\",\n    \"user_services\",\n    \"users\",\n    \"websocket\",\n]\n"}
{"type": "source_file", "path": "app/errors/chat_exceptions.py", "content": "from typing import Any\n\n\nclass ChatException(Exception):  # Base exception for chat\n    def __init__(self, *, msg: str | None = None) -> None:\n        self.msg = msg\n        super().__init__()\n\n\nclass ChatConnectionException(ChatException):\n    def __init__(self, *, msg: str | None = None) -> None:\n        self.msg = msg\n        super().__init__(msg=msg)\n\n\nclass ChatLengthException(ChatException):\n    def __init__(self, *, msg: str | None = None) -> None:\n        self.msg = msg\n        super().__init__(msg=msg)\n\n\nclass ChatContentFilterException(ChatException):\n    def __init__(self, *, msg: str | None = None) -> None:\n        self.msg = msg\n        super().__init__(msg=msg)\n\n\nclass ChatTooMuchTokenException(ChatException):\n    def __init__(self, *, msg: str | None = None) -> None:\n        self.msg = msg\n        super().__init__(msg=msg)\n\n\nclass ChatTextGenerationException(ChatException):\n    def __init__(self, *, msg: str | None = None) -> None:\n        self.msg = msg\n        super().__init__(msg=msg)\n\n\nclass ChatOtherException(ChatException):\n    def __init__(self, *, msg: str | None = None) -> None:\n        self.msg = msg\n        super().__init__(msg=msg)\n\n\nclass ChatModelNotImplementedException(ChatException):\n    def __init__(self, *, msg: str | None = None) -> None:\n        self.msg = msg\n        super().__init__(msg=msg)\n\n\nclass ChatBreakException(ChatException):\n    def __init__(self, *, msg: str | None = None) -> None:\n        self.msg = msg\n        super().__init__(msg=msg)\n\n\nclass ChatContinueException(ChatException):\n    def __init__(self, *, msg: str | None = None) -> None:\n        self.msg = msg\n        super().__init__(msg=msg)\n\n\nclass ChatInterruptedException(ChatException):\n    def __init__(self, *, msg: str | None = None) -> None:\n        self.msg = msg\n        super().__init__(msg=msg)\n\n\nclass ChatFunctionCallException(ChatException):\n    \"\"\"Raised when function is called.\"\"\"\n\n    def __init__(self, *, func_name: str, func_kwargs: dict[str, Any]) -> None:\n        self.func_name = func_name\n        self.func_kwargs = func_kwargs\n        super().__init__(msg=f\"Function {func_name}({func_kwargs}) is called.\")\n"}
{"type": "source_file", "path": "app/middlewares/token_validator.py", "content": "from time import time\n\nfrom fastapi import HTTPException\nfrom starlette.datastructures import Headers, QueryParams\nfrom starlette.middleware.base import RequestResponseEndpoint\nfrom starlette.requests import Request\nfrom starlette.responses import JSONResponse, Response\n\nfrom app.common.config import EXCEPT_PATH_LIST, EXCEPT_PATH_REGEX\nfrom app.database.crud.api_keys import get_api_key_and_owner\nfrom app.errors.api_exceptions import (\n    APIException,\n    InternalServerError,\n    Responses_400,\n    Responses_401,\n    exception_handler,\n)\nfrom app.models.base_models import UserToken\nfrom app.utils.auth.token import token_decode\nfrom app.utils.date_utils import UTC\nfrom app.utils.logger import ApiLogger\nfrom app.utils.params_utils import hash_params\n\n\nclass StateManager:\n    @staticmethod\n    def init(request: Request):\n        request.state.req_time = UTC.now()\n        request.state.start = time()\n        if request.client is not None:\n            # Check headers for real ip\n            request.state.ip = request.headers.get(\n                \"x-forwarded-for\", request.client.host\n            ).split(\",\")[0]\n        request.state.inspect = None\n        request.state.user = None\n\n\nclass AccessControl:\n    @staticmethod\n    async def api_service(\n        query_params: QueryParams,\n        headers: Headers,\n    ) -> UserToken:\n        query_params_dict: dict = dict(query_params)\n        for query_key in (\"key\", \"timestamp\"):\n            if query_key not in query_params_dict.keys():\n                raise Responses_400.invalid_api_query\n        for header_key in (\"secret\",):\n            if header_key not in headers.keys():\n                raise Responses_401.invalid_api_header\n        return await Validator.api_key(\n            query_params=query_params,\n            api_access_key=query_params_dict[\"key\"],\n            timestamp=query_params_dict[\"timestamp\"],\n            hashed_secret=headers[\"secret\"],\n        )\n\n    @staticmethod\n    def non_api_service(\n        headers: Headers,\n        cookies: dict[str, str],\n    ) -> UserToken:\n        if \"authorization\" in headers.keys():\n            token = headers[\"authorization\"]\n        elif \"Authorization\" in cookies.keys():\n            token = cookies[\"Authorization\"]\n        else:\n            raise Responses_401.not_authorized\n        return Validator.jwt(token)\n\n\nclass Validator:\n    @staticmethod\n    async def api_key(\n        api_access_key: str,\n        hashed_secret: str,\n        query_params: QueryParams | str,\n        timestamp: str,\n    ) -> UserToken:\n        matched_api_key, matched_user = await get_api_key_and_owner(\n            access_key=api_access_key\n        )\n        if not hashed_secret == hash_params(\n            query_params=str(query_params),\n            secret_key=matched_api_key.secret_key,\n        ):\n            raise Responses_401.invalid_api_header\n        now_timestamp: int = UTC.timestamp()\n        if not (now_timestamp - 60 < int(timestamp) < now_timestamp + 60):\n            raise Responses_401.invalid_timestamp\n        return UserToken(**matched_user.to_dict())\n\n    @staticmethod\n    def jwt(\n        authorization: str,\n    ) -> UserToken:\n        token_info: dict = token_decode(authorization=authorization)\n        return UserToken(**token_info)\n\n\nasync def access_control(request: Request, call_next: RequestResponseEndpoint):\n    StateManager.init(request=request)\n    url: str = request.url.path\n\n    try:\n        if EXCEPT_PATH_REGEX.match(url) is not None:\n            ...  # Regex-whitelist endpoint\n        elif url in EXCEPT_PATH_LIST:\n            ...  # Whitelist endpoint\n        elif url.startswith(\"/api/services\"):\n            # Api-service endpoint (required: accesskey + secretkey)\n            request.state.user = await AccessControl.api_service(\n                query_params=request.query_params,\n                headers=request.headers,\n            )\n        else:\n            # Non Api-service endpoint (required: jwttoken)\n            request.state.user = AccessControl.non_api_service(\n                headers=request.headers,\n                cookies=request.cookies,\n            )\n        response: Response = await call_next(\n            request\n        )  # actual endpoint response\n\n    except Exception as exception:  # If any error occurs...\n        error: HTTPException | InternalServerError | APIException = (\n            exception_handler(error=exception)\n        )\n        response: Response = JSONResponse(\n            status_code=error.status_code,\n            content={\n                \"status\": error.status_code,\n                \"msg\": error.msg\n                if not isinstance(error, HTTPException)\n                else None,\n                \"detail\": error.detail\n                if not isinstance(error, HTTPException)\n                else error.detail,\n                \"code\": error.code\n                if not isinstance(error, HTTPException)\n                else None,\n            },\n        )\n        ApiLogger.clog(\n            request=request,\n            response=response,\n            error=error,\n            cookies=request.cookies,\n            headers=dict(request.headers),\n            query_params=dict(request.query_params),\n        )\n        return response  # The final response from server\n    else:\n        # Log error or service info\n        if url.startswith(\"/api/services\"):\n            ApiLogger.clog(\n                request=request,\n                response=response,\n                cookies=request.cookies,\n                headers=dict(request.headers),\n                query_params=dict(request.query_params),\n            )\n        return response  # The final response from server\n"}
{"type": "source_file", "path": "app/models/chat_commands.py", "content": "from typing import Callable\n\nfrom app.utils.chat.commands.browsing import BrowsingCommands\nfrom app.utils.chat.commands.core import CoreCommands\nfrom app.utils.chat.commands.llm_parameter import LLMParameterCommands\nfrom app.utils.chat.commands.prompt import PromptCommands\nfrom app.utils.chat.commands.server import ServerCommands\nfrom app.utils.chat.commands.summarize import SummarizeCommands\nfrom app.utils.chat.commands.testing import TestingCommands\nfrom app.utils.chat.commands.vectorstore import VectorstoreCommands\n\nfrom .chat_models import command_response\n\n\nclass ChatCommandsMetaClass(type):\n    \"\"\"Metaclass for ChatCommands class.\n    It is used to automatically create list of special commands.\n    \"\"\"\n\n    special_commands: list[str]\n\n    def __init__(cls, name, bases, dct):\n        super().__init__(name, bases, dct)\n        cls.special_commands = [\n            callback_name\n            for callback_name in dir(CoreCommands)\n            if not callback_name.startswith(\"_\")\n        ]\n\n\nclass ChatCommands(\n    CoreCommands,\n    VectorstoreCommands,\n    PromptCommands,\n    BrowsingCommands,\n    LLMParameterCommands,\n    ServerCommands,\n    TestingCommands,\n    SummarizeCommands,\n    metaclass=ChatCommandsMetaClass,\n):\n    @classmethod\n    def find_callback_with_command(cls, command: str) -> Callable:\n        found_callback: Callable = getattr(cls, command)\n        if found_callback is None or not callable(found_callback):\n            found_callback = cls.not_existing_callback\n        return found_callback\n\n    @staticmethod\n    @command_response.send_message_and_stop\n    def not_existing_callback() -> str:  # callback for not existing command\n        return \"Sorry, I don't know what you mean by...\"\n"}
{"type": "source_file", "path": "app/routers/auth.py", "content": "import bcrypt\nfrom fastapi import APIRouter, Response, Security, status\nfrom fastapi.requests import Request\n\nfrom app.common.config import TOKEN_EXPIRE_HOURS\nfrom app.database.crud.users import is_email_exist, register_new_user\nfrom app.database.schemas.auth import Users\nfrom app.dependencies import USER_DEPENDENCY\nfrom app.errors.api_exceptions import Responses_400, Responses_404\nfrom app.models.base_models import SnsType, Token, UserRegister, UserToken\nfrom app.utils.auth.register_validation import (\n    is_email_length_in_range,\n    is_email_valid_format,\n    is_password_length_in_range,\n)\nfrom app.utils.auth.token import create_access_token, token_decode\nfrom app.utils.chat.managers.cache import CacheManager\n\nrouter = APIRouter(prefix=\"/auth\")\n\n\n@router.post(\"/register/{sns_type}\", status_code=201, response_model=Token)\nasync def register(\n    request: Request,\n    response: Response,\n    sns_type: SnsType,\n    reg_info: UserRegister,\n) -> Token:\n    if sns_type == SnsType.EMAIL:\n        if not (reg_info.email and reg_info.password):\n            raise Responses_400.no_email_or_password\n\n        if is_email_length_in_range(email=reg_info.email) is False:\n            raise Responses_400.email_length_not_in_range\n\n        if is_password_length_in_range(password=reg_info.password) is False:\n            raise Responses_400.password_length_not_in_range\n\n        if is_email_valid_format(email=reg_info.email) is False:\n            raise Responses_400.invalid_email_format\n\n        if await is_email_exist(email=reg_info.email):\n            raise Responses_400.email_already_exists\n\n        hashed_password: str = bcrypt.hashpw(\n            password=reg_info.password.encode(\"utf-8\"),\n            salt=bcrypt.gensalt(),\n        ).decode(\"utf-8\")\n        new_user: Users = await register_new_user(\n            email=reg_info.email,\n            hashed_password=hashed_password,\n            ip_address=request.state.ip,\n        )\n        data_to_be_tokenized: dict = UserToken.from_orm(new_user).dict(\n            exclude={\"password\", \"marketing_agree\"}\n        )\n        token: str = create_access_token(\n            data=data_to_be_tokenized, expires_delta=TOKEN_EXPIRE_HOURS\n        )\n        response.set_cookie(\n            key=\"Authorization\",\n            value=f\"Bearer {token}\",\n            max_age=TOKEN_EXPIRE_HOURS * 3600,\n            secure=True,\n            httponly=True,\n        )\n        return Token(Authorization=f\"Bearer {token}\")\n    raise Responses_400.not_supported_feature\n\n\n@router.delete(\"/register\", status_code=status.HTTP_204_NO_CONTENT)\nasync def unregister(\n    authorization: str = Security(USER_DEPENDENCY),\n):\n    registered_user: UserToken = UserToken(**token_decode(authorization))\n    if registered_user.email is not None:\n        await CacheManager.delete_user(user_id=registered_user.email)\n    await Users.delete_filtered(\n        Users.email == registered_user.email, autocommit=True\n    )\n\n\n@router.post(\"/login/{sns_type}\", status_code=200, response_model=Token)\nasync def login(\n    response: Response,\n    sns_type: SnsType,\n    user_info: UserRegister,\n) -> Token:\n    if sns_type == SnsType.EMAIL:\n        if not (user_info.email and user_info.password):\n            raise Responses_400.no_email_or_password\n        matched_user: Users = await Users.first_filtered_by(\n            email=user_info.email\n        )\n        if matched_user is None or matched_user.password is None:\n            raise Responses_404.not_found_user\n        if not bcrypt.checkpw(\n            password=user_info.password.encode(\"utf-8\"),\n            hashed_password=matched_user.password.encode(\"utf-8\"),\n        ):\n            raise Responses_404.not_found_user\n        data_to_be_tokenized: dict = UserToken.from_orm(matched_user).dict(\n            exclude={\"password\", \"marketing_agree\"}\n        )\n        token: str = create_access_token(\n            data=data_to_be_tokenized, expires_delta=TOKEN_EXPIRE_HOURS\n        )\n        response.set_cookie(\n            key=\"Authorization\",\n            value=f\"Bearer {token}\",\n            max_age=TOKEN_EXPIRE_HOURS * 3600,\n            secure=True,\n            httponly=True,\n        )\n        return Token(Authorization=f\"Bearer {token}\")\n    else:\n        raise Responses_400.not_supported_feature\n"}
{"type": "source_file", "path": "app/utils/api/weather.py", "content": "import httpx\nimport orjson\nfrom typing import Any, Literal\nfrom fastapi import HTTPException\n\n\nasync def fetch_weather_data(\n    lat: float,\n    lon: float,\n    api_key: str,\n    source: Literal[\"openweathermap\", \"weatherbit\", \"climacell\"],\n) -> Any:\n    base_url = {\n        \"openweathermap\": \"https://api.openweathermap.org/data/2.5/weather\",\n        \"weatherbit\": \"https://api.weatherbit.io/v2.0/current\",\n        \"climacell\": \"https://api.climacell.co/v3/weather/realtime\",\n    }[source]\n    query_params = {\n        \"openweathermap\": f\"lat={lat}&lon={lon}&appid={api_key}\",\n        \"weatherbit\": f\"lat={lat}&lon={lon}&key={api_key}\",\n        \"climacell\": f\"lat={lat}&lon={lon}&unit_system=metric&apikey={api_key}\",\n    }[source]\n\n    async with httpx.AsyncClient() as client:\n        response = await client.get(base_url + \"?\" + query_params)\n        if response.status_code == 200:\n            weather_data = orjson.loads(response.content)\n            print(\"weather_data:\", weather_data)\n            return weather_data\n        else:\n            raise HTTPException(\n                status_code=response.status_code,\n                detail=f\"Error fetching data from {source}\",\n            )\n\n\ndef get_temperature(\n    weather_data: dict,\n    source: Literal[\"openweathermap\", \"weatherbit\", \"climacell\"],\n):\n    if source == \"openweathermap\":\n        temp = weather_data[\"main\"][\"temp\"] - 273.15  # Convert from Kelvin to Celsius\n    elif source == \"weatherbit\":\n        temp = weather_data[\"data\"][0][\"temp\"]\n    elif source == \"climacell\":\n        temp = weather_data[\"temp\"][\"value\"]\n    else:\n        temp = None\n\n    return temp\n"}
{"type": "source_file", "path": "app/routers/index.py", "content": "from fastapi import APIRouter, Request\nfrom fastapi.responses import FileResponse\n\nrouter = APIRouter()\n\n\n@router.get(\"/\")\nasync def index():\n    return FileResponse(\"app/web/index.html\")\n\n\n@router.get(\"/favicon.ico\", include_in_schema=False)\nasync def favicon():\n    return FileResponse(\"app/contents/favicon.ico\")\n\n\n@router.get(\"/test\")\nasync def test(request: Request):\n    return {\"username\": request.session.get(\"username\", None)}\n"}
{"type": "source_file", "path": "app/models/function_calling/functions.py", "content": "from typing import Annotated, Callable, Optional\n\nfrom app.common.lotties import Lotties\nfrom app.utils.chat.buffer import BufferedUserContext\nfrom app.utils.chat.managers.websocket import SendToWebsocket\nfrom app.utils.chat.tokens import make_truncated_text\nfrom app.utils.function_calling.parser import parse_function_call_from_function\n\nfrom .base import FunctionCall\n\n\nclass FunctionCallsMetaClass(type):\n    \"\"\"Metaclass for FunctionCalls class.\n    This metaclass is used to parse all functions in the FunctionCalls class\n    into FunctionCall objects.\"\"\"\n\n    function_calls: dict[str, FunctionCall] = {}\n\n    def __init__(cls, name, bases, dct):\n        super().__init__(name, bases, dct)\n        for func in cls.__dict__.values():\n            if callable(func) and not func.__name__.startswith(\"_\"):\n                print(\n                    \"- Parsing function for function calling:\", func.__name__\n                )\n                function_call = parse_function_call_from_function(func)\n                cls.function_calls[function_call.name] = function_call\n\n    def get_function_call(cls, function: Callable) -> FunctionCall:\n        \"\"\"Get the FunctionCall object for the given function.\"\"\"\n        if function.__name__ not in cls.function_calls:\n            cls.function_calls[\n                function.__name__\n            ] = parse_function_call_from_function(function)\n        return cls.function_calls[function.__name__]\n\n    def get_function(cls, function_name: str) -> Callable:\n        \"\"\"Get the function for the given function name.\"\"\"\n        return getattr(cls, function_name)\n\n\nclass FunctionCalls(metaclass=FunctionCallsMetaClass):\n    \"\"\"Class that contains all functions that can be called by the user.\n    This class is used to parse all functions in the FunctionCalls class into\n    FunctionCall objects. FunctionCall objects are used to represent the\n    specification of a function and will be used for `function_calling`.\"\"\"\n\n    @staticmethod\n    def control_browser(\n        action: Annotated[\n            str,\n            (\n                \"Your action to take. Select `finish_browsing` if you can answer th\"\n                \"e user's question or there's no relevant information. Select `clic\"\n                \"k_link` only if you need to click on a link to gather more informa\"\n                \"tion.\"\n            ),\n            [\"finish_browsing\", \"click_link\"],\n        ],\n        link_to_click: Annotated[\n            str,\n            \"The link to click on if you selected `click_link` as your action.\",\n        ],\n    ):\n        \"\"\"Control web browser to answer the user's question.\"\"\"\n        # Note: This function is not used in the demo.\n        pass\n\n    @staticmethod\n    def control_web_page(\n        action: Annotated[\n            str,\n            (\n                \"Whether to scroll down, go back, or pick the result. Select `scrol\"\n                \"l_down` if you must scroll down to read more information. Select `g\"\n                \"o_back` if you need to go back to the previous page to read other \"\n                \"information. Select `pick` only if you can provide a satisfactory \"\n                \"answer to the user from the given information.\"\n            ),\n            [\"scroll_down\", \"go_back\", \"pick\"],\n        ],\n        relevance_score: Annotated[\n            int,\n            (\n                \"A score that indicates how helpful the given context is in answeri\"\n                \"ng the user's question. If the information is very relevant and su\"\n                \"fficient to answer the user's question, give it a score of 10; if \"\n                \"the information is very irrelevant, give it a score of 0.\"\n            ),\n            [score for score in range(11)],\n        ],\n    ):\n        \"\"\"Control the web page to read more information or stop reading. You\n        have to evaluate the relevance of the information you read and de\n        cide whether to scroll down, go back, or pick.\"\"\"\n        pass\n\n    @staticmethod\n    async def web_search(\n        query_to_search: Annotated[\n            str,\n            \"A generalized query to return sufficiently relevant results when s\"\n            \"earching the web.\",\n        ],\n        buffer: BufferedUserContext,\n    ) -> Optional[str]:\n        \"\"\"Perform web search for a user's question.\"\"\"\n\n        from app.utils.function_calling.callbacks.full_browsing import (\n            URL_PATTERN,\n            full_web_browsing_callback,\n        )\n\n        # Get user's question, or use the last user message if the user did not\n        # provide a question.\n        # Also, get the links in the user's question if exist.\n        # The links will be used to browse the web if AI chooses to do so.\n        user_query = (\n            buffer.current_user_message_histories[-1].content\n            if buffer.current_user_message_histories\n            else buffer.last_user_message.removeprefix(\"/browse\")\n            if buffer.last_user_message\n            else query_to_search\n        )\n        user_provided_links = URL_PATTERN.findall(user_query)\n\n        # Notify frontend that we are browsing the web.\n        await SendToWebsocket.message(\n            websocket=buffer.websocket,\n            msg=Lotties.SEARCH_WEB.format(\n                \"### Browsing web\\n---\\n{query_to_search}\".format(\n                    query_to_search=query_to_search.replace(\"```\", \"'''\")\n                )\n            ),\n            chat_room_id=buffer.current_chat_room_id,\n            finish=False,\n        )\n\n        # Return the browsing result.\n        browsing_result: Optional[str] = await full_web_browsing_callback(\n            buffer=buffer,\n            query_to_search=query_to_search,\n            user_provided_links=user_provided_links,\n            finish=True,\n            wait_next_query=True,\n        )\n        return (\n            make_truncated_text(\n                user_chat_context=buffer.current_user_chat_context,\n                text=browsing_result,\n                with_n_user_messages=1,\n            )\n            if browsing_result\n            else None\n        )\n\n    @staticmethod\n    async def vectorstore_search(\n        query_to_search: Annotated[\n            str,\n            \"Hypothetical answer to facilitate searching in the Vector database.\",\n        ],\n        buffer: BufferedUserContext,\n    ) -> Optional[str]:\n        \"\"\"Perform vector similarity-based search for user's question.\"\"\"\n        from app.utils.function_calling.callbacks.vectorstore_search import (\n            vectorstore_search_callback,\n        )\n\n        # Notify frontend that we are browsing the web.\n        await SendToWebsocket.message(\n            websocket=buffer.websocket,\n            msg=Lotties.SEARCH_DOC.format(\n                \"### Searching vectorstore\\n---\\n{query_to_search}\".format(\n                    query_to_search=query_to_search.replace(\"```\", \"'''\")\n                )\n            ),\n            chat_room_id=buffer.current_chat_room_id,\n            finish=False,\n        )\n\n        # Return the browsing result.\n        vectorstore_search_result: Optional[\n            str\n        ] = await vectorstore_search_callback(\n            buffer=buffer,\n            query_to_search=query_to_search,\n            finish=True,\n            wait_next_query=True,\n        )\n        return (\n            make_truncated_text(\n                user_chat_context=buffer.current_user_chat_context,\n                text=vectorstore_search_result,\n                with_n_user_messages=1,\n            )\n            if vectorstore_search_result\n            else None\n        )\n"}
{"type": "source_file", "path": "app/database/crud/api_keys.py", "content": "from typing import Optional, Tuple\nfrom sqlalchemy import select, func, exists\nfrom app.models.base_models import AddApiKey\nfrom app.errors.api_exceptions import (\n    Responses_400,\n    Responses_404,\n    Responses_500,\n)\nfrom app.common.config import MAX_API_KEY\nfrom app.database.connection import db\nfrom app.database.schemas.auth import ApiKeys, Users\nfrom app.utils.auth.api_keys import generate_new_api_key\n\n\nasync def create_api_key(\n    user_id: int,\n    additional_key_info: AddApiKey,\n) -> ApiKeys:\n    if db.session is None:\n        raise Responses_500.database_not_initialized\n    async with db.session() as transaction:\n        api_key_count_stmt = select(func.count(ApiKeys.id)).filter_by(user_id=user_id)\n        api_key_count: int | None = await transaction.scalar(api_key_count_stmt)\n        if api_key_count is not None and api_key_count >= MAX_API_KEY:\n            raise Responses_400.max_key_count_exceed\n        while True:\n            new_api_key: ApiKeys = generate_new_api_key(\n                user_id=user_id, additional_key_info=additional_key_info\n            )\n            is_api_key_duplicate_stmt = select(\n                exists().where(ApiKeys.access_key == new_api_key.access_key)\n            )\n            is_api_key_duplicate: bool | None = await transaction.scalar(\n                is_api_key_duplicate_stmt\n            )\n            if not is_api_key_duplicate:\n                break\n        transaction.add(new_api_key)\n        await transaction.commit()\n        await transaction.refresh(new_api_key)\n        return new_api_key\n\n\nasync def get_api_keys(user_id: int) -> list[ApiKeys]:\n    return await ApiKeys.fetchall_filtered_by(user_id=user_id)\n\n\nasync def get_api_key_owner(access_key: str) -> Users:\n    if db.session is None:\n        raise Responses_500.database_not_initialized\n    async with db.session() as transaction:\n        matched_api_key: Optional[ApiKeys] = await transaction.scalar(\n            select(ApiKeys).filter_by(access_key=access_key)\n        )\n        if matched_api_key is None:\n            raise Responses_404.not_found_access_key\n        owner: Users = await Users.first_filtered_by(id=matched_api_key.user_id)\n        if owner is None:\n            raise Responses_404.not_found_user\n        return owner\n\n\nasync def get_api_key_and_owner(access_key: str) -> Tuple[ApiKeys, Users]:\n    if db.session is None:\n        raise Responses_500.database_not_initialized\n    async with db.session() as transaction:\n        matched_api_key: Optional[ApiKeys] = await transaction.scalar(\n            select(ApiKeys).filter_by(access_key=access_key)\n        )\n        if matched_api_key is None:\n            raise Responses_404.not_found_access_key\n        api_key_owner: Optional[Users] = await transaction.scalar(\n            select(Users).filter_by(id=matched_api_key.user_id)\n        )\n        if api_key_owner is None:\n            raise Responses_404.not_found_user\n        return matched_api_key, api_key_owner\n\n\nasync def update_api_key(\n    updated_key_info: dict,\n    access_key_id: int,\n    user_id: int,\n) -> ApiKeys:\n    if db.session is None:\n        raise Responses_500.database_not_initialized\n    async with db.session() as transaction:\n        matched_api_key: Optional[ApiKeys] = await transaction.scalar(\n            select(ApiKeys).filter_by(id=access_key_id, user_id=user_id)\n        )\n        if matched_api_key is None:\n            raise Responses_404.not_found_api_key\n        matched_api_key.set_values_as(**updated_key_info)\n        transaction.add(matched_api_key)\n        await transaction.commit()\n        await transaction.refresh(matched_api_key)\n        return matched_api_key\n\n\nasync def delete_api_key(\n    access_key_id: int,\n    access_key: str,\n    user_id: int,\n) -> None:\n    if db.session is None:\n        raise Responses_500.database_not_initialized\n    async with db.session() as transaction:\n        matched_api_key: Optional[ApiKeys] = await transaction.scalar(\n            select(ApiKeys).filter_by(\n                id=access_key_id, user_id=user_id, access_key=access_key\n            )\n        )\n        if matched_api_key is None:\n            raise Responses_404.not_found_api_key\n        await transaction.delete(matched_api_key)\n        await transaction.commit()\n"}
{"type": "source_file", "path": "app/models/base_models.py", "content": "from datetime import datetime, timezone\nfrom enum import Enum\nfrom typing import Literal, Optional, Union\nfrom uuid import uuid4\n\nfrom pydantic import Field\nfrom pydantic.main import BaseModel\n\nfrom app.utils.date_utils import UTC\nfrom app.viewmodels.status import UserStatus\n\nfrom .function_calling.base import FunctionProperty\n\n\nclass UserRegister(BaseModel):\n    email: str\n    password: str\n\n\nclass SnsType(str, Enum):\n    EMAIL = \"email\"\n    FACEBOOK = \"facebook\"\n    GOOGLE = \"google\"\n    KAKAO = \"kakao\"\n\n\nclass Token(BaseModel):\n    Authorization: str\n\n\nclass EmailRecipients(BaseModel):\n    name: str\n    email: str\n\n\nclass SendEmail(BaseModel):\n    email_to: list[EmailRecipients]\n\n\nclass KakaoMsgBody(BaseModel):\n    msg: str\n\n\nclass MessageOk(BaseModel):\n    message: str = Field(default=\"OK\")\n\n\nclass UserToken(BaseModel):\n    id: int\n    status: UserStatus\n    email: Optional[str] = None\n    name: Optional[str] = None\n\n    class Config:\n        orm_mode = True\n\n\nclass UserMe(BaseModel):\n    id: int\n    email: Optional[str] = None\n    name: Optional[str] = None\n    phone_number: Optional[str] = None\n    profile_img: Optional[str] = None\n    sns_type: Optional[str] = None\n\n    class Config:\n        orm_mode = True\n\n\nclass AddApiKey(BaseModel):\n    user_memo: Optional[str] = None\n\n    class Config:\n        orm_mode = True\n\n\nclass GetApiKey(AddApiKey):\n    id: int\n    access_key: str\n    created_at: datetime\n\n\nclass GetApiKeyFirstTime(GetApiKey):\n    secret_key: str\n\n\nclass CreateApiWhiteList(BaseModel):\n    ip_address: str\n\n\nclass GetApiWhiteList(CreateApiWhiteList):\n    id: int\n\n    class Config:\n        orm_mode = True\n\n\nclass CreateChatMessage(BaseModel):  # stub\n    message: str\n    role: str\n    user_id: int\n    chat_room_id: str\n\n    class Config:\n        orm_mode = True\n\n\nclass MessageToWebsocket(BaseModel):\n    msg: Optional[str]\n    finish: bool\n    chat_room_id: Optional[str] = None\n    actual_role: Optional[str] = None\n    init: bool = False\n    model_name: Optional[str] = None\n    uuid: Optional[str] = None\n    wait_next_query: Optional[bool] = None\n\n    class Config:\n        orm_mode = True\n\n\nclass MessageFromWebsocket(BaseModel):\n    msg: str\n    translate: Optional[str] = None\n    chat_room_id: str\n    uuid: Optional[str] = None\n\n\nclass CreateChatRoom(BaseModel):  # stub\n    chat_room_type: str\n    name: str\n    description: Optional[str] = None\n    user_id: int\n\n    class Config:\n        orm_mode = True\n\n\nclass APIChatMessage(BaseModel):\n    role: Literal[\"system\", \"user\", \"assistant\", \"function\"] = Field(\n        default=\"user\",\n        description=\"The role of the messages author. One of system, user, assistant, or function\",\n    )\n    content: str = Field(\n        default=\"\",\n        description=(\n            \"The contents of the message. content is required for all messages, \"\n            \"and may be null for assistant messages with function calls.\"\n        ),\n    )\n    name: Optional[str] = Field(\n        default=None,\n        description=(\n            \"The name of the author of this message. name is required if role is function, \"\n            \"and it should be the name of the function whose response is in the content. \"\n            \"May contain a-z, A-Z, 0-9, and underscores, with a maximum length of 64 characters.\"\n        ),\n    )\n    function_call: Optional[FunctionProperty] = Field(\n        default=None,\n        description=\"The name and arguments of a function that should be called, as generated by the model.\",\n    )\n\n    class Config:\n        orm_mode = True\n\n\nclass MessageHistory(BaseModel):\n    role: str\n    content: str\n    tokens: int = 0\n    timestamp: int = Field(default_factory=UTC.timestamp)\n    uuid: str = Field(default_factory=lambda: uuid4().hex)\n    actual_role: Optional[str] = None\n    model_name: Optional[str] = None\n    summarized: Optional[str] = None\n    summarized_tokens: Optional[int] = None\n\n    class Config:\n        orm_mode = True\n\n    def __repr__(self) -> str:\n        if self.summarized is not None:\n            return (\n                f'<{self.role} uuid=\"{self.uuid}\" date=\"{self.datetime}Z\" tokens=\"{self.tokens}\" '\n                f'summarized=\"{self.summarized}\">{self.content}</>'\n            )\n        return f'<{self.role} uuid=\"{self.uuid}\" date=\"{self.datetime}Z\" tokens=\"{self.tokens}\">{self.content}</>'\n\n    @property\n    def datetime(self) -> datetime:\n        try:\n            return UTC.timestamp_to_datetime(self.timestamp)\n        except Exception:\n            return datetime(1970, 1, 1, tzinfo=timezone.utc)\n\n    @property\n    def is_prefix(self) -> bool:\n        return self.timestamp < 0\n\n    @property\n    def is_suffix(self) -> bool:\n        return self.timestamp >= 2**50\n\n\nclass InitMessage(BaseModel):\n    previous_chats: Optional[list[dict]] = None\n    chat_rooms: Optional[list[dict[str, str]]] = None\n    models: Optional[list[str]] = None\n    selected_model: Optional[str] = None\n    tokens: Optional[int] = None\n\n\nclass StreamProgress(BaseModel):\n    response: str = \"\"\n    buffer: str = \"\"\n    uuid: Optional[str] = None\n\n\nclass UserChatRoles(BaseModel):\n    ai: str\n    system: str\n    user: str\n\n\nclass SummarizedResult(BaseModel):\n    user_id: str\n    chat_room_id: str\n    role: str\n    content: str\n    uuid: str\n\n\nclass ParserDefinitions(BaseModel):\n    selector: Optional[str] = None\n    render_js: bool = False\n\n\nclass TextGenerationSettings(BaseModel):\n    completion_id: str = Field(\n        default_factory=lambda: f\"cmpl-{str(uuid4())}\",\n        description=\"The unique ID of the text generation\",\n    )\n    max_tokens: int = Field(\n        default=128,\n        ge=1,\n        description=\"The maximum number of tokens to generate.\",\n    )\n    temperature: float = Field(\n        default=0.8,\n        ge=0.0,\n        le=2.0,\n        description=(\n            \"Adjust the randomness of the generated text.\"\n            \"Temperature is a hyperparameter that controls the randomness of the generated te\"\n            \"xt. It affects the probability distribution of the model's output tokens. A high\"\n            \"er temperature (e.g., 1.5) makes the output more random and creative, while a lo\"\n            \"wer temperature (e.g., 0.5) makes the output more focused, deterministic, and co\"\n            \"nservative. The default value is 0.8, which provides a balance between randomnes\"\n            \"s and determinism. At the extreme, a temperature of 0 will always pick the most \"\n            \"likely next token, leading to identical outputs in each run.\"\n        ),\n    )\n    top_p: float = Field(\n        default=0.95,\n        ge=0.0,\n        le=1.0,\n        description=(\n            \"Limit the next token selection to a subset of tokens with a cumulative probabili\"\n            \"ty above a threshold P. Top-p sampling, also known as nucleus sampling, \"\n            \"is another text generation method that selects the next token from a subset of t\"\n            \"okens that together have a cumulative probability of at least p. This method pro\"\n            \"vides a balance between diversity and quality by considering both the probabilit\"\n            \"ies of tokens and the number of tokens to sample from. A higher value for top_p \"\n            \"(e.g., 0.95) will lead to more diverse text, while a lower value (e.g., 0.5) wil\"\n            \"l generate more focused and conservative text.\"\n        ),\n    )\n    typical_p: float = Field(\n        default=0.0,\n        description=\"Locally typical sampling threshold, 0.0 to disable typical sampling\",\n    )\n    logprobs: Optional[int] = Field(\n        default=None,\n        description=\"The number of logprobs to return. If None, no logprobs are returned.\",\n    )\n    echo: bool = Field(\n        default=False,\n        description=\"If True, the input is echoed back in the output.\",\n    )\n    stop: Optional[str | list[str]] = Field(\n        default=None,\n        description=\"A list of tokens at which to stop generation. If None, no stop tokens are used.\",\n    )\n    frequency_penalty: float = Field(\n        default=0.0,\n        ge=-2.0,\n        le=2.0,\n        description=(\n            \"Positive values penalize new tokens based on their existing frequency in the tex\"\n            \"t so far, decreasing the model's likelihood to repeat the same line verbatim.\"\n        ),\n    )\n\n    presence_penalty: float = Field(\n        default=0.0,\n        ge=-2.0,\n        le=2.0,\n        description=(\n            \"Positive values penalize new tokens based on whether they appear in the text so far, increasing \"\n            \"the model's likelihood to talk about new topics.\"\n        ),\n    )\n    repeat_penalty: float = Field(\n        default=1.1,\n        ge=0.0,\n        description=(\n            \"A penalty applied to each token that is already generated. This helps prevent th\"\n            \"e model from repeating itself. Repeat penalty is a hyperparameter used t\"\n            \"o penalize the repetition of token sequences during text generation. It helps pr\"\n            \"event the model from generating repetitive or monotonous text. A higher value (e\"\n            \".g., 1.5) will penalize repetitions more strongly, while a lower value (e.g., 0.\"\n            \"9) will be more lenient.\"\n        ),\n    )\n    top_k: int = Field(\n        default=40,\n        ge=0,\n        description=(\n            \"Limit the next token selection to the K most probable tokens. Top-k samp\"\n            \"ling is a text generation method that selects the next token only from the top k\"\n            \" most likely tokens predicted by the model. It helps reduce the risk of generati\"\n            \"ng low-probability or nonsensical tokens, but it may also limit the diversity of\"\n            \" the output. A higher value for top_k (e.g., 100) will consider more tokens and \"\n            \"lead to more diverse text, while a lower value (e.g., 10) will focus on the most\"\n            \" probable tokens and generate more conservative text.\"\n        ),\n    )\n    tfs_z: float = Field(\n        default=1.0,\n        description=\"Modify probability distribution to carefully cut off least likely tokens\",\n    )\n    mirostat_mode: int = Field(\n        default=0,\n        ge=0,\n        le=2,\n        description=\"Enable Mirostat constant-perplexity algorithm of the specified version (1 or 2; 0 = disabled)\",\n    )\n    mirostat_tau: float = Field(\n        default=5.0,\n        ge=0.0,\n        le=10.0,\n        description=(\n            \"Mirostat target entropy, i.e. the target perplexity - lower values produce focused and coherent text, \"\n            \"larger values produce more diverse and less coherent text\"\n        ),\n    )\n    mirostat_eta: float = Field(\n        default=0.1, ge=0.001, le=1.0, description=\"Mirostat learning rate\"\n    )\n    logit_bias: Optional[dict[str, float]] = Field(\n        default=None,\n        description=(\n            \"A dictionary of logit bias values to use for each token. The keys are the tokens\"\n            \" and the values are the bias values. The bias values are added to the logits of \"\n            \"the model to influence the next token probabilities. For example, a bias value o\"\n            \"f 5.0 will make the model 10 times more likely to select that token than it woul\"\n            \"d be otherwise. A bias value of -5.0 will make the model 10 times less likely to\"\n            \" select that token than it would be otherwise. The bias values are added to the \"\n            \"logits of the model to influence.\"\n        ),\n    )\n    logit_bias_type: Literal[\"input_ids\", \"tokens\"] = Field(\n        default=\"tokens\",\n        description=(\n            \"The type of logit bias to use. If 'input_ids', the bias is applied to the input\"\n            \" ids(integer). If 'tokens', the bias is applied to the tokens(string). If None, the bias is not \"\n            \"applied.\"\n        ),\n    )\n    ban_eos_token: bool = Field(\n        default=False,\n        description=\"If True, the EOS token is banned from being generated.\",\n    )\n\n\nclass CreateEmbeddingRequest(BaseModel):\n    model: str = Field(description=\"The model to use for embedding.\")\n    input: Union[str, list[str]] = Field(description=\"The input to embed.\")\n    user: Optional[str]\n\n    class Config:\n        schema_extra = {\n            \"example\": {\n                \"model\": \"llama_7b\",\n                \"input\": \"The food was delicious and the waiter...\",\n            },\n        }\n\n\nclass CreateCompletionRequest(TextGenerationSettings):\n    model: str = Field(\n        default=..., description=\"The model to use for completion.\"\n    )\n    prompt: str = Field(\n        default=\"\", description=\"The prompt to use for completion.\"\n    )\n    stream: bool = Field(\n        default=False, description=\"Whether to stream the response.\"\n    )\n\n    class Config:\n        schema_extra = {\n            \"example\": {\n                \"model\": \"llama_7b\",\n                \"prompt\": \"\\n\\n### Instructions:\\nWhat is the capital of France?\\n\\n### Response:\\n\",\n                \"stop\": [\"\\n\", \"###\"],\n            }\n        }\n\n\nclass CreateChatCompletionRequest(TextGenerationSettings):\n    model: str = Field(\n        default=..., description=\"The model to use for completion.\"\n    )\n    messages: list[APIChatMessage] = Field(\n        default=[],\n        description=\"A list of messages to generate completions for.\",\n    )\n    stream: bool = Field(\n        default=False, description=\"Whether to stream the response.\"\n    )\n\n    class Config:\n        schema_extra = {\n            \"example\": {\n                \"model\": \"llama_7b\",\n                \"messages\": [\n                    APIChatMessage(\n                        role=\"system\", content=\"You are a helpful assistant.\"\n                    ),\n                    APIChatMessage(\n                        role=\"user\", content=\"What is the capital of France?\"\n                    ),\n                ],\n            }\n        }\n"}
{"type": "source_file", "path": "app/utils/auth/register_validation.py", "content": "import re\n\n# Make a regular expression\n# for validating an Email\nEMAIL_REGEX: re.Pattern = re.compile(r\"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{1,7}\\b\")\n\n\n# Define a function for\n# for validating an Email\ndef is_email_valid_format(email: str) -> bool:\n    return True if EMAIL_REGEX.fullmatch(email) is not None else False\n\n\ndef is_email_length_in_range(email: str) -> bool:\n    return True if 6 <= len(email) <= 50 else False\n\n\ndef is_password_length_in_range(password: str) -> bool:\n    return True if 6 <= len(password) <= 100 else False\n\n\n# Driver Code\nif __name__ == \"__main__\":\n    for email in (\"ankitrai326@gmail.com\", \"my.ownsite@our-earth.org\", \"ankitrai326.com\", \"aa@a.a\"):\n        # calling run function\n        print(is_email_valid_format(email))\n"}
{"type": "source_file", "path": "app/routers/users.py", "content": "import ipaddress\n\nfrom fastapi import APIRouter\nfrom starlette.requests import Request\n\nfrom app.database.crud import api_keys, api_whitelists, users\nfrom app.errors.api_exceptions import Responses_400\nfrom app.models.base_models import (\n    AddApiKey,\n    CreateApiWhiteList,\n    GetApiKey,\n    GetApiKeyFirstTime,\n    GetApiWhiteList,\n    MessageOk,\n    UserMe,\n)\n\nrouter = APIRouter(prefix=\"/user\")\n\n\n@router.get(\"/me\", response_model=UserMe)\nasync def get_me(request: Request):\n    return await users.get_me(user_id=request.state.user.id)\n\n\n@router.put(\"/me\")\nasync def put_me(request: Request):\n    ...\n\n\n@router.delete(\"/me\")\nasync def delete_me(request: Request):\n    ...\n\n\n@router.get(\"/apikeys\", response_model=list[GetApiKey])\nasync def get_api_keys(request: Request):\n    return await api_keys.get_api_keys(user_id=request.state.user.id)\n\n\n@router.post(\"/apikeys\", response_model=GetApiKeyFirstTime, status_code=201)\nasync def create_api_key(request: Request, api_key_info: AddApiKey):\n    return await api_keys.create_api_key(\n        user_id=request.state.user.id, additional_key_info=api_key_info\n    )\n\n\n@router.put(\"/apikeys/{key_id}\", response_model=GetApiKey)\nasync def update_api_key(\n    request: Request,\n    api_key_id: int,\n    api_key_info: AddApiKey,\n):\n    \"\"\"\n    API KEY User Memo Update\n    \"\"\"\n    return await api_keys.update_api_key(\n        updated_key_info=api_key_info.dict(),\n        access_key_id=api_key_id,\n        user_id=request.state.user.id,\n    )\n\n\n@router.delete(\"/apikeys/{key_id}\")\nasync def delete_api_key(\n    request: Request,\n    key_id: int,\n    access_key: str,\n):\n    await api_keys.delete_api_key(\n        access_key_id=key_id,\n        access_key=access_key,\n        user_id=request.state.user.id,\n    )\n    return MessageOk()\n\n\n@router.get(\n    \"/apikeys/{key_id}/whitelists\", response_model=list[GetApiWhiteList]\n)\nasync def get_api_keys_whitelist(api_key_id: int):\n    return await api_whitelists.get_api_key_whitelist(api_key_id=api_key_id)\n\n\n@router.post(\"/apikeys/{key_id}/whitelists\", response_model=GetApiWhiteList)\nasync def create_api_keys_whitelist(\n    api_key_id: int,\n    ip: CreateApiWhiteList,\n):\n    ip_address: str = ip.ip_address\n    try:\n        ipaddress.ip_address(ip_address)\n    except Exception as exception:\n        raise Responses_400.invalid_ip(\n            lazy_format={\"ip\": ip_address}, ex=exception\n        )\n    return await api_whitelists.create_api_key_whitelist(\n        ip_address=ip_address, api_key_id=api_key_id\n    )\n\n\n@router.delete(\"/apikeys/{key_id}/whitelists/{list_id}\")\nasync def delete_api_keys_whitelist(\n    request: Request,\n    api_key_id: int,\n    whitelist_id: int,\n):\n    await api_whitelists.delete_api_key_whitelist(\n        user_id=request.state.user.id,\n        api_key_id=api_key_id,\n        whitelist_id=whitelist_id,\n    )\n    return MessageOk()\n"}
{"type": "source_file", "path": "app/database/schemas/auth.py", "content": "import enum\nfrom sqlalchemy import (\n    String,\n    Integer,\n    Enum,\n    Boolean,\n    ForeignKey,\n)\nfrom sqlalchemy.orm import (\n    relationship,\n    Mapped,\n    mapped_column,\n)\n\nfrom app.viewmodels.status import ApiKeyStatus, UserStatus\nfrom .. import Base\nfrom . import TableMixin\n\n\nclass Users(Base, TableMixin):\n    __tablename__ = \"users\"\n    status: Mapped[str] = mapped_column(Enum(UserStatus), default=UserStatus.active)\n    email: Mapped[str] = mapped_column(String(length=50))\n    password: Mapped[str | None] = mapped_column(String(length=100))\n    name: Mapped[str | None] = mapped_column(String(length=20))\n    phone_number: Mapped[str | None] = mapped_column(String(length=20))\n    profile_img: Mapped[str | None] = mapped_column(String(length=100))\n    marketing_agree: Mapped[bool] = mapped_column(Boolean, default=True)\n    api_keys: Mapped[\"ApiKeys\"] = relationship(\n        back_populates=\"users\", cascade=\"all, delete-orphan\", lazy=True\n    )\n    # chat_rooms: Mapped[\"ChatRooms\"] = relationship(back_populates=\"users\", cascade=\"all, delete-orphan\", lazy=True)\n    # chat_messages: Mapped[\"ChatMessages\"] = relationship(\n    #     back_populates=\"users\", cascade=\"all, delete-orphan\", lazy=True\n    # )\n    # gpt_presets: Mapped[\"GptPresets\"] = relationship(\n    #     back_populates=\"users\", cascade=\"all, delete-orphan\", lazy=True, uselist=False\n    # )\n\n\nclass ApiKeys(Base, TableMixin):\n    __tablename__ = \"api_keys\"\n    status: Mapped[str] = mapped_column(Enum(ApiKeyStatus), default=ApiKeyStatus.active)\n    access_key: Mapped[str] = mapped_column(String(length=64), index=True, unique=True)\n    secret_key: Mapped[str] = mapped_column(String(length=64))\n    user_memo: Mapped[str | None] = mapped_column(String(length=40))\n    is_whitelisted: Mapped[bool] = mapped_column(default=False)\n    user_id: Mapped[int] = mapped_column(ForeignKey(\"users.id\", ondelete=\"CASCADE\"))\n    users: Mapped[\"Users\"] = relationship(back_populates=\"api_keys\")\n    whitelists: Mapped[\"ApiWhiteLists\"] = relationship(\n        backref=\"api_keys\", cascade=\"all, delete-orphan\"\n    )\n\n\nclass ApiWhiteLists(Base, TableMixin):\n    __tablename__ = \"api_whitelists\"\n    api_key_id: Mapped[int] = mapped_column(\n        Integer, ForeignKey(\"api_keys.id\", ondelete=\"CASCADE\")\n    )\n    ip_address: Mapped[str] = mapped_column(String(length=64))\n\n\n# class ChatRooms(Base, Mixin):\n#     __tablename__ = \"chat_rooms\"\n#     uuid: Mapped[str] = mapped_column(String(length=36), index=True, unique=True)\n#     status: Mapped[str] = mapped_column(Enum(\"active\", \"deleted\", \"blocked\"), default=\"active\")\n#     chat_room_type: Mapped[str] = mapped_column(String(length=20), index=True)\n#     name: Mapped[str] = mapped_column(String(length=20))\n#     description: Mapped[str | None] = mapped_column(String(length=100))\n#     user_id: Mapped[int] = mapped_column(ForeignKey(\"users.id\", ondelete=\"CASCADE\"))\n#     users: Mapped[\"Users\"] = relationship(back_populates=\"chat_rooms\")\n#     chat_messages: Mapped[\"ChatMessages\"] = relationship(back_populates=\"chat_rooms\", cascade=\"all, delete-orphan\")\n\n\n# class ChatMessages(Base, Mixin):\n#     __tablename__ = \"chat_messages\"\n#     uuid: Mapped[str] = mapped_column(String(length=36), index=True, unique=True)\n#     status: Mapped[str] = mapped_column(Enum(\"active\", \"deleted\", \"blocked\"), default=\"active\")\n#     role: Mapped[str] = mapped_column(String(length=20), default=\"user\")\n#     message: Mapped[str] = mapped_column(Text)\n#     chat_room_id: Mapped[int] = mapped_column(ForeignKey(\"chat_rooms.id\", ondelete=\"CASCADE\"))\n#     chat_rooms: Mapped[\"ChatRooms\"] = relationship(back_populates=\"chat_messages\")\n#     user_id: Mapped[int] = mapped_column(ForeignKey(\"users.id\", ondelete=\"CASCADE\"))\n#     users: Mapped[\"Users\"] = relationship(back_populates=\"chat_messages\")\n\n\n# class GptPresets(Base, Mixin):\n#     __tablename__ = \"gpt_presets\"\n#     temperature: Mapped[float] = mapped_column(Float, default=0.9)\n#     top_p: Mapped[float] = mapped_column(Float, default=1.0)\n#     presence_penalty: Mapped[float] = mapped_column(Float, default=0)\n#     frequency_penalty: Mapped[float] = mapped_column(Float, default=0)\n#     user_id: Mapped[int] = mapped_column(ForeignKey(\"users.id\", ondelete=\"CASCADE\"), unique=True)\n#     users: Mapped[\"Users\"] = relationship(back_populates=\"gpt_presets\")\n"}
{"type": "source_file", "path": "app/utils/auth/api_keys.py", "content": "from secrets import choice\nfrom string import ascii_letters, digits\nfrom uuid import uuid4\n\nfrom app.database.schemas.auth import ApiKeys\nfrom app.models.base_models import AddApiKey\n\n\ndef generate_new_api_key(\n    user_id: int, additional_key_info: AddApiKey\n) -> ApiKeys:\n    alnums = ascii_letters + digits\n    secret_key = \"\".join(choice(alnums) for _ in range(40))\n    uid = f\"{str(uuid4())[:-12]}{str(uuid4())}\"\n    new_api_key = ApiKeys(\n        secret_key=secret_key,\n        user_id=user_id,\n        access_key=uid,\n        **additional_key_info.dict(),\n    )\n    return new_api_key\n"}
{"type": "source_file", "path": "app/routers/services.py", "content": "import os\nfrom time import sleep\nfrom typing import Any\n\nimport boto3\nimport orjson\nimport requests\nimport yagmail\nfrom botocore.exceptions import ClientError\nfrom fastapi import APIRouter\nfrom fastapi.logger import logger\nfrom fastapi.responses import JSONResponse\nfrom starlette.background import BackgroundTasks\nfrom starlette.requests import Request\n\nfrom app.common.config import (\n    AWS_ACCESS_KEY,\n    AWS_AUTHORIZED_EMAIL,\n    AWS_SECRET_KEY,\n    KAKAO_IMAGE_URL,\n    KAKAO_RESTAPI_TOKEN,\n    WEATHERBIT_API_KEY,\n    config,\n)\nfrom app.errors.api_exceptions import Responses_400\nfrom app.models.base_models import KakaoMsgBody, MessageOk, SendEmail\nfrom app.utils.api.weather import fetch_weather_data\nfrom app.utils.encoding_utils import encode_from_utf8\n\nrouter = APIRouter(prefix=\"/services\")\nrouter.redirect_slashes = False\n\n\n@router.get(\"/\")\nasync def get_all_services(request: Request):\n    return {\"your_email\": request.state.user.email}\n\n\n@router.get(\"/weather\", status_code=200)\nasync def weather(latitude: float, longitude: float):\n    if WEATHERBIT_API_KEY is None:\n        raise Responses_400.not_supported_feature\n    weather_data: Any = await fetch_weather_data(\n        lat=latitude,\n        lon=longitude,\n        api_key=WEATHERBIT_API_KEY,\n        source=\"weatherbit\",\n    )\n    return JSONResponse(\n        weather_data,\n    )\n\n\n@router.post(\"/kakao/send\")\nasync def send_kakao(request: Request, body: KakaoMsgBody):\n    link_1 = \"https://google.com\"\n    link_2 = \"https://duckduckgo.com\"\n    headers = {\n        \"Authorization\": KAKAO_RESTAPI_TOKEN,\n        \"Content-Type\": \"application/x-www-form-urlencoded\",\n    }\n\n    template_object = orjson.dumps(\n        {\n            \"object_type\": \"feed\",\n            \"content\": {\n                \"title\": \"알림톡\",\n                \"description\": body.msg,\n                \"image_url\": KAKAO_IMAGE_URL,\n                \"link\": {\n                    \"web_url\": \"http://google.com\",\n                    \"mobile_web_url\": \"http://google.com\",\n                    \"android_execution_params\": \"contentId=100\",\n                    \"ios_execution_params\": \"contentId=100\",\n                },\n            },\n            \"buttons\": [\n                {\n                    \"title\": \"바로 가기1\",\n                    \"link\": {\"web_url\": link_1, \"mobile_web_url\": link_1},\n                },\n                {\n                    \"title\": \"바로 가기2\",\n                    \"link\": {\"web_url\": link_2, \"mobile_web_url\": link_2},\n                },\n            ],\n        },\n    )\n    data = {\"template_object\": template_object}\n    print(data)\n    res = requests.post(\n        \"https://kapi.kakao.com/v2/api/talk/memo/default/send\",\n        headers=headers,\n        data=data,\n    )\n    try:\n        res.raise_for_status()\n        if res.json()[\"result_code\"] != 0:\n            raise Exception(\"KAKAO SEND FAILED\")\n    except Exception as e:\n        logger.warning(e)\n        raise Responses_400.kakao_send_failure\n    return MessageOk()\n\n\nemail_content = (\n    \"<div style='margin-top:0cm;margin-right:0cm;margin-bottom:10.0pt;margin-left:0cm\"\n    ';line-height:115%;font-size:15px;font-family:\"Calibri\",sans-serif;border:none;bo'\n    \"rder-bottom:solid #EEEEEE 1.0pt;padding:0cm 0cm 6.0pt 0cm;background:white;'>\\n\\n<\"\n    \"p style='margin-top:0cm;margin-right:0cm;margin-bottom:11.25pt;margin-left:0cm;l\"\n    'ine-height:115%;font-size:15px;font-family:\"Calibri\",sans-serif;background:white'\n    \";border:none;padding:0cm;'><span style='font-size:25px;font-family:\\\"Helvetica Ne\"\n    \"ue\\\";color:#11171D;'>{}님! Aristoxeni ingenium consumptum videmus in musicis?</spa\"\n    \"n></p>\\n</div>\\n\\n<p style='margin-top:0cm;margin-right:0cm;margin-bottom:11.25pt;m\"\n    'argin-left:0cm;line-height:17.25pt;font-size:15px;font-family:\"Calibri\",sans-ser'\n    \"if;background:white;vertical-align:baseline;'><span style='font-size:14px;font-f\"\n    'amily:\"Helvetica Neue\";color:#11171D;\\'>Lorem ipsum dolor sit amet, consectetur a'\n    \"dipiscing elit. Quid nunc honeste dicit? Tum Torquatus: Prorsus, inquit, assenti\"\n    \"or; Duo Reges: constructio interrete. Iam in altera philosophiae parte. Sed haec\"\n    \" omittamus; Haec para/doca illi, nos admirabilia dicamus. Nihil sane.</span></p>\"\n    \"\\n\\n<p style='margin-top:0cm;margin-right:0cm;margin-bottom:10.0pt;margin-left:0cm\"\n    ';line-height:normal;font-size:15px;font-family:\"Calibri\",sans-serif;background:w'\n    \"hite;'><strong><span style='font-size:24px;font-family:\\\"Helvetica Neue\\\";color:#1\"\n    \"1171D;'>Expressa vero in iis aetatibus, quae iam confirmatae sunt.</span></stron\"\n    \"g></p>\\n\\n<p style='margin-top:0cm;margin-right:0cm;margin-bottom:11.25pt;margin-l\"\n    'eft:0cm;line-height:17.25pt;font-size:15px;font-family:\"Calibri\",sans-serif;back'\n    \"ground:white;vertical-align:baseline;'><span style='font-size:14px;font-family:\\\"\"\n    \"Helvetica Neue\\\";color:#11171D;'>Sit sane ista voluptas. Non quam nostram quidem,\"\n    \" inquit Pomponius iocans; An tu me de L. Sed haec omittamus; Cave putes quicquam\"\n    \" esse verius.&nbsp;</span></p>\\n\\n<p style='margin-top:0cm;margin-right:0cm;margin\"\n    '-bottom:11.25pt;margin-left:0cm;line-height:17.25pt;font-size:15px;font-family:\"'\n    \"Calibri\\\",sans-serif;text-align:center;background:white;vertical-align:baseline;'\"\n    \"><span style='font-size:14px;font-family:\\\"Helvetica Neue\\\";color:#11171D;'><img w\"\n    'idth=\"378\" src=\"https://dl1gtqdymozzn.cloudfront.net/forAuthors/K6Sfkx4f2uH780YG'\n    'TbEHvHcTX3itiTBtzDWeyswQevxp8jqVttfBgPu86ZtGC6owG.webp\" alt=\"sample1.jpg\" class='\n    '\"fr-fic fr-dii\"></span></p>\\n\\n<p>\\n<br>\\n</p>'\n)\n\n\n@router.post(\"/email/send_by_gmail\")\nasync def email_by_gmail(request: Request, mailing_list: SendEmail):\n    # t = time()\n    send_email(mailing_list=mailing_list.email_to)\n    return MessageOk()\n\n\n@router.post(\"/email/send_by_gmail2\")\nasync def email_by_gmail2(\n    request: Request,\n    mailing_list: SendEmail,\n    background_tasks: BackgroundTasks,\n):\n    # t = time()\n    background_tasks.add_task(send_email, mailing_list=mailing_list.email_to)\n    return MessageOk()\n\n\ndef send_email(**kwargs):\n    mailing_list = kwargs.get(\"mailing_list\", None)\n    email_password = os.environ.get(\"EMAIL_PW\", None)\n    email_addr = os.environ.get(\"EMAIL_ADDR\", None)\n    last_email = \"\"\n    if mailing_list:\n        try:\n            yag = yagmail.SMTP({email_addr: \"Admin\"}, email_password)\n            # https://myaccount.google.com/u/1/lesssecureapps\n            for m_l in mailing_list:\n                contents = [email_content.format(m_l.name)]\n                sleep(1)\n                yag.send(m_l.email, \"이렇게 한번 보내봅시다.\", contents)\n                last_email = m_l.email\n            return True\n        except Exception as e:\n            print(e)\n            print(last_email)\n\n\n@router.post(\"/email/send_by_ses\")\nasync def email_by_ses():\n    sender = encode_from_utf8(f\"운영자 admin <admin@{config.host_main}>\")\n    recipient = [AWS_AUTHORIZED_EMAIL]\n\n    # If necessary, replace us-west-2 with the AWS Region you're using for Amazon SES.\n    region = \"ap-northeast-2\"\n\n    # The subject line for the email.\n    title = \"안녕하세요! 테스트 이메일 입니다.\"\n\n    # The email body for recipients with non-HTML email clients.\n    BODY_TEXT = \"안녕하세요! 운영자 입니다.\\r\\n\" \"HTML 버전만 지원합니다!\"\n\n    # The HTML body of the email.\n    BODY_HTML = (\n        \"<html>\\n    <head></head>\\n    <body>\\n      <h1>안녕하세\"\n        \"요! 반갑습니다.</h1>\\n      <p>기업에서 대규모 이메일 솔루션을 구축한다는 것은\"\n        \" 복잡하고 비용이 많이 드는 작업이 될 수 있습니다. 이를 위해서는 인프라를 구축하고, 네\"\n        \"트워크를 구성하고, IP 주소를 준비하고, 발신자 평판을 보호해야 합니다. 타사 이메일 솔\"\n        \"루션 대부분이 상당한 규모의 선수금을 요구하고 계약 협상을 진행해야 합니다.\\n\\nAmazon\"\n        \" SES는 이러한 부담이 없으므로 몇 분 만에 이메일 발송을 시작할 수 있습니다. Amaz\"\n        \"on.com이 대규모의 자사 고객 기반을 지원하기 위해 구축한 정교한 이메일 인프라와 오랜\"\n        \" 경험을 활용할 수 있습니다.</p>\\n      <p>링크를 통해 확인하세요!\\n      \"\n        \"  <a href='https://google.com'>Google</a></p>\\n    \"\n        \"</body>\"\n    )\n\n    # The character encoding for the email.\n    charset = \"UTF-8\"\n\n    # Create a new SES resource and specify a region.\n    client = boto3.client(\n        \"ses\",\n        region_name=region,\n        aws_access_key_id=AWS_ACCESS_KEY,\n        aws_secret_access_key=AWS_SECRET_KEY,\n    )\n\n    # Try to send the email.\n    try:\n        # Provide the contents of the email.\n        response = client.send_email(\n            Source=sender,\n            Destination={\"ToAddresses\": recipient},\n            Message={\n                \"Body\": {\n                    \"Html\": {\"Charset\": charset, \"Data\": BODY_HTML},\n                    \"Text\": {\"Charset\": charset, \"Data\": BODY_TEXT},\n                },\n                \"Subject\": {\"Charset\": charset, \"Data\": title},\n            },\n        )\n    # Display an error if something goes wrong.\n    except ClientError as e:\n        print(e.response[\"Error\"][\"Message\"])\n    else:\n        print(\"Email sent! Message ID:\")\n        print(response[\"MessageId\"])\n\n    return MessageOk()\n"}
{"type": "source_file", "path": "app/models/llms.py", "content": "from dataclasses import dataclass, field\nfrom typing import Literal, Optional, Union\n\nfrom langchain import PromptTemplate\n\nfrom app.common.config import OPENAI_API_KEY, ChatConfig\nfrom app.common.constants import ChatTurnTemplates, DescriptionTemplates\nfrom app.mixins.enum import EnumMixin\n\nfrom .base_models import UserChatRoles\nfrom .llm_tokenizers import (\n    BaseTokenizer,\n    ExllamaTokenizer,\n    LlamaTokenizer,\n    OpenAITokenizer,\n)\n\n\n@dataclass\nclass LLMModel:\n    name: str = \"llm_model\"  # model name\n    max_total_tokens: int = 2048\n    max_tokens_per_request: int = 1024\n    tokenizer: BaseTokenizer = field(\n        default_factory=lambda: OpenAITokenizer(model_name=\"gpt-3.5-turbo\")\n    )\n    user_chat_roles: UserChatRoles = field(\n        default_factory=lambda: UserChatRoles(\n            ai=\"assistant\",\n            system=\"system\",\n            user=\"user\",\n        ),\n    )\n    prefix_template: Optional[\n        Union[PromptTemplate, str]\n    ] = None  # A prefix to prepend to the generated text. If None, no prefix is prepended.\n    suffix_template: Optional[\n        Union[PromptTemplate, str]\n    ] = None  # A suffix to prepend to the generated text. If None, no suffix is prepended.\n    token_margin: int = field(\n        default=8,\n        metadata={\n            \"description\": \" The marginal number of tokens when counting the number of tokens in context.\"\n            \"The more token_margin, the more conservative the model will be when counting context tokens.\"\n        },\n    )\n    prefix: Optional[str] = field(init=False, repr=False, default=None)\n    suffix: Optional[str] = field(init=False, repr=False, default=None)\n\n    @staticmethod\n    def _prepare_format(\n        input_variables: list[str],\n        predefined_format: dict[str, str],\n    ) -> dict[str, str | None]:\n        return dict(\n            zip(\n                input_variables,\n                map(\n                    predefined_format.get,\n                    input_variables,\n                ),\n            )\n        )\n\n    def __post_init__(self):\n        user_chat_roles = self.user_chat_roles\n        predefined_format = {\n            \"user\": user_chat_roles.user,\n            \"USER\": user_chat_roles.user,\n            \"ai\": user_chat_roles.ai,\n            \"AI\": user_chat_roles.ai,\n            \"char\": user_chat_roles.ai,\n            \"system\": user_chat_roles.system,\n            \"SYSTEM\": user_chat_roles.system,\n        }\n        # If the global prefix is None, then use the prefix template\n        if ChatConfig.global_prefix is None:\n            if isinstance(self.prefix_template, PromptTemplate):\n                # format the template with the predefined format, only for input variables\n                self.prefix = self.prefix_template.format(\n                    **self._prepare_format(\n                        self.prefix_template.input_variables, predefined_format\n                    )\n                )\n            elif isinstance(self.prefix_template, str):\n                self.prefix = self.prefix_template.format(**predefined_format)\n            else:\n                self.prefix = None\n        else:\n            self.prefix = ChatConfig.global_prefix\n\n        # If the global suffix is None, then use the suffix template\n        if ChatConfig.global_suffix is None:\n            if isinstance(self.suffix_template, PromptTemplate):\n                # format the template with the predefined format, only for input variables\n                self.suffix = self.suffix_template.format(\n                    **self._prepare_format(\n                        self.suffix_template.input_variables, predefined_format\n                    )\n                )\n            elif isinstance(self.suffix_template, str):\n                self.suffix = self.suffix_template.format(**predefined_format)\n            else:\n                self.suffix = None\n        else:\n            self.suffix = ChatConfig.global_suffix\n        self._prefix_tokens: Optional[int] = None\n        self._suffix_tokens: Optional[int] = None\n\n    @property\n    def prefix_tokens(self) -> int:\n        # Lazy load the prefix tokens\n        if self.prefix is None:\n            return 0\n        if self._prefix_tokens is None:\n            self._prefix_tokens = (\n                self.tokenizer.tokens_of(self.prefix) + self.token_margin\n            )\n        return self._prefix_tokens\n\n    @property\n    def suffix_tokens(self) -> int:\n        # Lazy load the suffix tokens\n        if self.suffix is None:\n            return 0\n        if self._suffix_tokens is None:\n            self._suffix_tokens = (\n                self.tokenizer.tokens_of(self.suffix) + self.token_margin\n            )\n        return self._suffix_tokens\n\n\n@dataclass\nclass LlamaCppModel(LLMModel):\n    \"\"\"Llama.cpp model that can be loaded from local path.\"\"\"\n\n    tokenizer: LlamaTokenizer = field(\n        default_factory=lambda: LlamaTokenizer(model_name=\"\"),\n        metadata={\"description\": \"The tokenizer to use for this model.\"},\n    )\n    name: str = field(default=\"Llama C++\")\n    model_path: str = field(\n        default=\"YOUR_GGML.bin\"\n    )  # The path to the model. Must end with .bin. You must put .bin file into \"llama_models/ggml\"\n    user_chat_roles: UserChatRoles = field(\n        default_factory=lambda: UserChatRoles(\n            ai=\"ASSISTANT\",\n            system=\"SYSTEM\",\n            user=\"USER\",\n        ),\n    )\n    prefix_template: Optional[Union[PromptTemplate, str]] = field(\n        default_factory=lambda: DescriptionTemplates.USER_AI__DEFAULT,\n    )\n    chat_turn_prompt: PromptTemplate = field(\n        default_factory=lambda: ChatTurnTemplates.ROLE_CONTENT_1\n    )  # The prompt to use for chat turns.\n    n_parts: int = (\n        -1\n    )  # Number of parts to split the model into. If -1, the number of parts is automatically determined.\n    n_gpu_layers: int = 30  # Number of layers to keep on the GPU. If 0, all layers are kept on the GPU.\n    seed: int = -1  # Seed. If -1, a random seed is used.\n    f16_kv: bool = True  # Use half-precision for key/value cache.\n    logits_all: bool = (\n        False  # Return logits for all tokens, not just the last token.\n    )\n    vocab_only: bool = False  # Only load the vocabulary, no weights.\n    use_mlock: bool = True  # Force system to keep model in RAM.\n    n_batch: int = 512  # Number of tokens to process in parallel. Should be a number between 1 and n_ctx.\n    last_n_tokens_size: int = 64  # The number of tokens to look back when applying the repeat_penalty.\n    use_mmap: bool = True  # Whether to use memory mapping for the model.\n    streaming: bool = True  # Whether to stream the results, token by token.\n    cache: bool = (\n        False  # The size of the cache in bytes. Only used if cache is True.\n    )\n    echo: bool = True  # Whether to echo the prompt.\n    lora_base: Optional[str] = None  # The path to the Llama LoRA base model.\n    lora_path: Optional[\n        str\n    ] = None  # The path to the Llama LoRA. If None, no LoRa is loaded.\n    cache_type: Optional[Literal[\"disk\", \"ram\"]] = \"ram\"\n    cache_size: Optional[int] = (\n        2 << 30\n    )  # The size of the cache in bytes. Only used if cache is True.\n    n_threads: Optional[\n        int\n    ] = None  # Number of threads to use. If None, the number of threads is automatically determined.\n    low_vram: bool = False  # Whether to use less VRAM.\n    embedding: bool = False  # Whether to use the embedding layer.\n    rope_freq_base: float = 10000.0  # I use 26000 for n_ctx=4096. https://github.com/ggerganov/llama.cpp/pull/2054\n    rope_freq_scale: float = 1.0  # Generally, 2048 / n_ctx. https://github.com/ggerganov/llama.cpp/pull/2054\n\n\n@dataclass\nclass ExllamaModel(LLMModel):\n    \"\"\"Exllama model that can be loaded from local path.\"\"\"\n\n    model_path: str = field(\n        default=\"YOUR_GPTQ_FOLDER_NAME\",\n        metadata={\n            \"description\": \"The GPTQ model path to the model.\"\n            \"e.g. If you have a model folder in 'llama_models/gptq/your_model',\"\n            \"then you should set this to 'your_model'.\"\n        },\n    )\n    tokenizer: ExllamaTokenizer = field(\n        default_factory=lambda: ExllamaTokenizer(model_name=\"\"),\n        metadata={\"description\": \"The tokenizer to use for this model.\"},\n    )\n\n    user_chat_roles: UserChatRoles = field(\n        default_factory=lambda: UserChatRoles(\n            ai=\"ASSISTANT\",\n            system=\"SYSTEM\",\n            user=\"USER\",\n        ),\n    )\n    prefix_template: Optional[Union[PromptTemplate, str]] = field(\n        default_factory=lambda: DescriptionTemplates.USER_AI__DEFAULT,\n    )\n    chat_turn_prompt: PromptTemplate = field(\n        default_factory=lambda: ChatTurnTemplates.ROLE_CONTENT_1,\n        metadata={\"description\": \"The prompt to use for each chat turn.\"},\n    )\n    compress_pos_emb: float = field(\n        default=1.0,\n        metadata={\n            \"description\": \"Increase to compress positional embeddings applied to sequence.\"\n            \"This is useful when you want to extend context window size.\"\n            \"e.g. If you want to extend context window size from 2048 to 4096, set this to 2.0.\"\n        },\n    )\n    gpu_peer_fix: bool = field(\n        default=False,\n        metadata={\n            \"description\": \"Apparently Torch can have problems transferring tensors directly 1 GPU to another.\"\n            \"Enable this to use system RAM as a buffer for GPU to GPU transfers.\"\n        },\n    )\n    auto_map: Optional[list[float]] = field(\n        default=None,\n        metadata={\n            \"description\": \"List of floats with memory allocation in GB, per CUDA device, overrides device_map\"\n        },\n    )\n    # Optional parameters\n    matmul_recons_thd: int = 8\n    fused_mlp_thd: int = 2\n    sdp_thd: int = 8\n    fused_attn: bool = True\n    matmul_fused_remap: bool = False\n    rmsnorm_no_half2: bool = False\n    rope_no_half2: bool = False\n    matmul_no_half2: bool = False\n    silu_no_half2: bool = False\n    concurrent_streams: bool = False\n\n\n@dataclass\nclass OpenAIModel(LLMModel):\n    api_url: str = \"https://api.openai.com/v1/chat/completions\"\n    api_key: str | None = field(repr=False, default=None)\n    user_chat_roles: UserChatRoles = field(\n        default_factory=lambda: UserChatRoles(\n            ai=\"assistant\",\n            system=\"system\",\n            user=\"user\",\n        ),\n    )\n\n\nclass LLMModels(EnumMixin):\n    #  OpenAI models\n    gpt_3_5_turbo = OpenAIModel(\n        name=\"gpt-3.5-turbo\",\n        max_total_tokens=4096,\n        max_tokens_per_request=2048,\n        token_margin=8,\n        tokenizer=OpenAITokenizer(\"gpt-3.5-turbo\"),\n        api_url=\"https://api.openai.com/v1/chat/completions\",\n        api_key=OPENAI_API_KEY,\n        # prefix_template=PromptTemplate(\n        #     template=\"You'll be roleplaying with the user, so respond to their comments as if they're annoying you.\",\n        #     input_variables=[],\n        # ),  # Example of a prefix template\n        # suffix_template=PromptTemplate(\n        #     template=\"You must respond to the user in Korean.\",\n        #     input_variables=[],\n        # ),  # Example of a suffix template\n    )\n    gpt_3_5_turbo_16k = OpenAIModel(\n        name=\"gpt-3.5-turbo-16k\",\n        max_total_tokens=16384,\n        max_tokens_per_request=8192,\n        token_margin=8,\n        tokenizer=OpenAITokenizer(\"gpt-3.5-turbo\"),\n        api_url=\"https://api.openai.com/v1/chat/completions\",\n        api_key=OPENAI_API_KEY,\n    )\n\n    gpt_4 = OpenAIModel(\n        name=\"gpt-4\",\n        max_total_tokens=8192,\n        max_tokens_per_request=4096,\n        token_margin=8,\n        tokenizer=OpenAITokenizer(\"gpt-4\"),\n        api_url=\"https://api.openai.com/v1/chat/completions\",\n        api_key=OPENAI_API_KEY,\n    )\n\n    # Llama-cpp models\n    wizard_vicuna_13b_uncensored = LlamaCppModel(\n        name=\"Wizard-Vicuna-13B-Uncensored\",\n        max_total_tokens=4096,  # context tokens (n_ctx)\n        max_tokens_per_request=2048,  # The maximum number of tokens to generate.\n        token_margin=8,\n        tokenizer=LlamaTokenizer(\"ehartford/Wizard-Vicuna-13B-Uncensored\"),\n        model_path=\"Wizard-Vicuna-13B-Uncensored.ggmlv3.q5_1.bin\",  # The filename of model. Must end with .bin.\n        prefix_template=DescriptionTemplates.USER_AI__DEFAULT,\n    )\n    gorilla_7b = LlamaCppModel(\n        name=\"gorilla-7B-GGML\",\n        max_total_tokens=2048,  # context tokens (n_ctx)\n        max_tokens_per_request=1024,  # The maximum number of tokens to generate.\n        token_margin=8,\n        tokenizer=LlamaTokenizer(\"gorilla-llm/gorilla-7b-hf-delta-v0\"),\n        model_path=\"Gorilla-7B.ggmlv3.q3_K_S.bin\",  # The filename of model. Must end with .bin.\n        prefix_template=DescriptionTemplates.USER_AI__DEFAULT,\n    )\n    manticore_13b_uncensored = LlamaCppModel(\n        name=\"Manticore-13B-GGML\",\n        max_total_tokens=2048,  # context tokens (n_ctx)\n        max_tokens_per_request=1024,  # The maximum number of tokens to generate.\n        token_margin=8,\n        tokenizer=LlamaTokenizer(\"openaccess-ai-collective/manticore-13b\"),\n        model_path=\"Manticore-13B.ggmlv2.q5_1.bin\",  # The filename of model. Must end with .bin.\n    )\n    kovicuna_7b = LlamaCppModel(\n        name=\"kovicuna_7b\",\n        max_total_tokens=2048,  # context tokens (n_ctx)\n        max_tokens_per_request=1024,  # The maximum number of tokens to generate.\n        token_margin=8,\n        tokenizer=LlamaTokenizer(\"digitous/13B-HyperMantis\"),\n        model_path=\"kovicuna_q4km.bin\",  # The filename of model. Must end with .bin.\n        prefix_template=DescriptionTemplates.USER_AI__SHORT,\n    )\n    wizard_lm_13b_v1_1 = LlamaCppModel(\n        name=\"wizardLM-13B-Uncensored\",\n        max_total_tokens=4096,  # context tokens (n_ctx)\n        max_tokens_per_request=2048,  # The maximum number of tokens to generate.\n        token_margin=8,\n        prefix_template=DescriptionTemplates.USER_AI__SHORT,\n        tokenizer=LlamaTokenizer(\"victor123/WizardLM-13B-1.0\"),\n        model_path=\"wizardlm-13b-v1.1.ggmlv3.q4_K_S.bin\",  # The filename of model. Must end with .bin.\n        rope_freq_scale=0.5,  # similar to `compress_pos_emb`, but inverse number: (2048 / n_ctx)\n        rope_freq_base=26000,  # need some perplexity test; 26000 is my empirical value for 4096 context tokens\n    )\n    guanaco_13b = LlamaCppModel(\n        name=\"guanaco-13B-GGML\",\n        max_total_tokens=2048,  # context tokens (n_ctx)\n        max_tokens_per_request=1024,  # The maximum number of tokens to generate.\n        token_margin=8,\n        tokenizer=LlamaTokenizer(\n            \"timdettmers/guanaco-65b-merged\"\n        ),  # timdettmers/guanaco-13b\n        model_path=\"guanaco-13B.ggmlv3.q5_1.bin\",  # The filename of model. Must end with .bin.\n        prefix_template=DescriptionTemplates.USER_AI__SHORT,\n        user_chat_roles=UserChatRoles(\n            user=\"Human\",\n            ai=\"Assistant\",\n            system=\"Instruction\",\n        ),\n    )\n    karen_the_editor_13b = LlamaCppModel(\n        name=\"Karen_theEditor_13B-GGML\",\n        max_total_tokens=2048,  # context tokens (n_ctx)\n        max_tokens_per_request=1024,  # The maximum number of tokens to generate.\n        token_margin=8,\n        tokenizer=LlamaTokenizer(\"FPHam/Karen_theEditor_13b_HF\"),\n        model_path=\"Karen-The-Editor.ggmlv3.q5_1.bin\",  # The filename of model. Must end with .bin.\n        prefix_template=DescriptionTemplates.USER_AI__SHORT,\n        user_chat_roles=UserChatRoles(\n            user=\"USER\",\n            ai=\"ASSISTANT\",\n            system=\"SYSTEM\",\n        ),\n    )\n    airoboros_13b = LlamaCppModel(\n        name=\"airoboros-13b-gpt4-GGML\",\n        max_total_tokens=4096,  # context tokens (n_ctx)\n        max_tokens_per_request=2048,  # The maximum number of tokens to generate.\n        token_margin=8,\n        tokenizer=LlamaTokenizer(\"jondurbin/airoboros-13b-gpt4\"),\n        model_path=\"airoboros-13b-gpt4.ggmlv3.q5_1.bin\",  # The filename of model. Must end with .bin.\n        prefix_template=DescriptionTemplates.USER_AI__SHORT,\n    )\n    selfee_7b = LlamaCppModel(\n        name=\"selfee-7B-GGML\",\n        max_total_tokens=4096,  # context tokens (n_ctx)\n        max_tokens_per_request=2048,  # The maximum number of tokens to generate.\n        token_margin=8,\n        tokenizer=LlamaTokenizer(\"kaist-ai/selfee-7b-delta\"),\n        model_path=\"selfee-7b-superhot-8k.ggmlv3.q4_1.bin\",  # The filename of model. Must end with .bin.\n        prefix_template=DescriptionTemplates.USER_AI__SHORT,\n        rope_freq_scale=0.5,  # similar to `compress_pos_emb`, but inverse number: (2048 / n_ctx)\n        rope_freq_base=26000,  # need some perplexity test; 26000 is my empirical value for 4096 context tokens\n    )\n    llama_7b = LlamaCppModel(\n        name=\"llama-7b-GGML\",\n        max_total_tokens=2048,  # context tokens (n_ctx)\n        max_tokens_per_request=1024,  # The maximum number of tokens to generate.\n        token_margin=8,\n        tokenizer=LlamaTokenizer(\"HuggingFaceM4/llama-7b-tokenizer\"),\n        model_path=\"llama-7b.ggmlv3.q5_K_M.bin\",  # The filename of model. Must end with .bin.\n        prefix_template=None,\n        embedding=True,\n    )\n    orca_mini_3b = LlamaCppModel(\n        name=\"orca_mini_3B-GGML\",\n        max_total_tokens=2048,  # context tokens (n_ctx)\n        max_tokens_per_request=1024,  # The maximum number of tokens to generate.\n        token_margin=8,\n        tokenizer=LlamaTokenizer(\"psmathur/orca_mini_3b\"),\n        model_path=\"orca-mini-3b.ggmlv3.q4_1.bin\",  # The filename of model. Must end with .bin.\n        chat_turn_prompt=ChatTurnTemplates.ROLE_CONTENT_2,\n        user_chat_roles=UserChatRoles(\n            user=\"User\",\n            ai=\"Response\",\n            system=\"System\",\n        ),\n    )\n    airoboros_33b = LlamaCppModel(\n        name=\"airoboros-33b-gpt4-1.4-GGML\",\n        max_total_tokens=5120,  # context tokens (n_ctx)\n        max_tokens_per_request=2560,  # The maximum number of tokens to generate.\n        token_margin=8,\n        tokenizer=LlamaTokenizer(\"jondurbin/airoboros-33b-gpt4-1.4\"),\n        model_path=\"airoboros-33b-gpt4-1.4.ggmlv3.q3_K_S.bin\",  # The filename of model. Must end with .bin.\n        n_gpu_layers=26,\n        chat_turn_prompt=ChatTurnTemplates.ROLE_CONTENT_5,\n        user_chat_roles=UserChatRoles(\n            user=\"USER\",\n            ai=\"ASSISTANT\",\n            system=\"SYSTEM\",\n        ),\n    )\n    chronos_hermes_13b = LlamaCppModel(\n        name=\"TheBloke/chronos-hermes-13B-GGML\",\n        max_total_tokens=4096,  # context tokens (n_ctx)\n        max_tokens_per_request=2048,  # The maximum number of tokens to generate.\n        token_margin=8,\n        tokenizer=LlamaTokenizer(\"Austism/chronos-hermes-13b\"),\n        model_path=\"chronos-hermes-13b.ggmlv3.q4_K_M.bin\",  # The filename of model. Must end with .bin.\n        chat_turn_prompt=ChatTurnTemplates.ROLE_CONTENT_1,\n        user_chat_roles=UserChatRoles(\n            user=\"User\",\n            ai=\"Assistant\",\n            system=\"System\",\n        ),\n    )\n    orca_mini_7b = ExllamaModel(\n        model_path=\"orca_mini_7b\",\n        name=\"orca_mini_7b\",\n        max_total_tokens=2048,\n        max_tokens_per_request=2048,\n        token_margin=8,\n        tokenizer=ExllamaTokenizer(\"orca_mini_7b\"),\n        compress_pos_emb=2.0,\n        prefix_template=DescriptionTemplates.USER_AI__GAME,\n        user_chat_roles=UserChatRoles(\n            user=\"Player\",\n            ai=\"Narrator\",\n            system=\"System\",\n        ),\n    )\n    longchat_7b = ExllamaModel(\n        model_path=\"longchat_7b\",\n        name=\"longchat_7b\",\n        max_total_tokens=16384,\n        max_tokens_per_request=8192,\n        token_margin=8,\n        tokenizer=ExllamaTokenizer(\"longchat_7b\"),\n        compress_pos_emb=8.0,\n        prefix_template=DescriptionTemplates.USER_AI__GAME,\n        user_chat_roles=UserChatRoles(\n            user=\"Player\",\n            ai=\"Narrator\",\n            system=\"System\",\n        ),\n    )\n\n    @classmethod\n    def find_model_by_name(cls, name: str) -> LLMModel | None:\n        for model in cls:\n            if model.value.name == name or model.name == name:\n                return model.value\n        return None\n"}
{"type": "source_file", "path": "app/middlewares/trusted_hosts.py", "content": "from typing import Sequence\n\nfrom starlette.datastructures import URL, Headers\nfrom starlette.responses import PlainTextResponse, RedirectResponse\nfrom starlette.types import ASGIApp, Receive, Scope, Send\n\nfrom app.errors.api_exceptions import Responses_500\n\n\nclass TrustedHostMiddleware:\n    def __init__(\n        self,\n        app: ASGIApp,\n        allowed_hosts: Sequence[str] | None = None,\n        except_path: Sequence[str] | None = None,\n        www_redirect: bool = True,\n    ):\n        self.app: ASGIApp = app\n        self.allowed_hosts: list = (\n            [\"*\"] if allowed_hosts is None else list(allowed_hosts)\n        )\n        self.allow_any: bool = (\n            \"*\" in allowed_hosts if allowed_hosts is not None else True\n        )\n        self.www_redirect: bool = www_redirect\n        self.except_path: list = (\n            [] if except_path is None else list(except_path)\n        )\n        if allowed_hosts is not None:\n            for allowed_host in allowed_hosts:\n                if \"*\" in allowed_host[1:]:\n                    raise Responses_500.middleware_exception\n                if (\n                    allowed_host.startswith(\"*\") and allowed_host != \"*\"\n                ) and not allowed_host.startswith(\"*.\"):\n                    raise Responses_500.middleware_exception\n\n    async def __call__(\n        self, scope: Scope, receive: Receive, send: Send\n    ) -> None:\n        if self.allow_any or scope[\"type\"] not in (\n            \"http\",\n            \"websocket\",\n        ):  # pragma: no cover\n            await self.app(scope, receive, send)\n            return\n\n        headers = Headers(scope=scope)\n        host = headers.get(\"host\", \"\").split(\":\")[0]\n        is_valid_host = False\n        found_www_redirect = False\n        for pattern in self.allowed_hosts:\n            if (\n                host == pattern\n                or pattern.startswith(\"*\")\n                and host.endswith(pattern[1:])\n                or URL(scope=scope).path in self.except_path\n            ):\n                is_valid_host = True\n                break\n            elif \"www.\" + host == pattern:\n                found_www_redirect = True\n\n        if is_valid_host:\n            await self.app(scope, receive, send)\n        else:\n            if found_www_redirect and self.www_redirect:\n                url = URL(scope=scope)\n                redirect_url = url.replace(netloc=\"www.\" + url.netloc)\n                response = RedirectResponse(url=str(redirect_url))\n            else:\n                response = PlainTextResponse(\n                    \"Invalid host header\", status_code=400\n                )\n            await response(scope, receive, send)\n"}
{"type": "source_file", "path": "app/routers/user_services.py", "content": "from typing import Optional\n\nfrom fastapi import APIRouter\n\nfrom app.models.base_models import MessageOk\n\nrouter = APIRouter(prefix=\"/user-services\")\n\n\n@router.get(\"/\", status_code=200)\ndef test(query: Optional[str] = None):\n    return MessageOk(message=\"Hello, World!\" if query is None else query)\n"}
{"type": "source_file", "path": "app/auth/admin.py", "content": "from starlette.requests import Request\nfrom starlette.responses import Response\nfrom starlette_admin.auth import AdminUser, AuthProvider\nfrom starlette_admin.exceptions import FormValidationError, LoginFailed\nfrom app.common.config import config\n\n\nclass MyAuthProvider(AuthProvider):\n    async def login(\n        self,\n        username: str,\n        password: str,\n        remember_me: bool,\n        request: Request,\n        response: Response,\n    ) -> Response:\n        if len(username) < 3:\n            \"\"\"Form data validation\"\"\"\n            raise FormValidationError(\n                {\"username\": \"Ensure username has at least 03 characters\"}\n            )\n\n        if username == config.mysql_user and password == config.mysql_password:\n            \"\"\"Save `username` in session\"\"\"\n            request.session.update({\"username\": username})\n            return response\n\n        raise LoginFailed(\"Invalid username or password\")\n\n    async def is_authenticated(self, request) -> bool:\n        if request.session.get(\"username\", None) == config.mysql_user:\n            \"\"\"\n            Save current `user` object in the request state. Can be used later\n            to restrict access to connected user.\n            \"\"\"\n            return True\n\n        return False\n\n    def get_admin_user(self, request: Request) -> AdminUser:\n        return AdminUser(username=config.mysql_user)\n\n    async def logout(self, request: Request, response: Response) -> Response:\n        request.session.clear()\n        return response\n"}
{"type": "source_file", "path": "app/common/lotties.py", "content": "from app.mixins.enum import EnumMixin\n\n\nclass Lotties(EnumMixin):\n    CLICK = \"lottie-click\"\n    READ = \"lottie-read\"\n    SCROLL_DOWN = \"lottie-scroll-down\"\n    GO_BACK = \"lottie-go-back\"\n    SEARCH_WEB = \"lottie-search-web\"\n    SEARCH_DOC = \"lottie-search-doc\"\n    OK = \"lottie-ok\"\n    FAIL = \"lottie-fail\"\n    TRANSLATE = \"lottie-translate\"\n\n    def format(self, contents: str, end: bool = True) -> str:\n        return f\"\\n```{self.get_value(self)}\\n{contents}\" + (\n            \"\\n```\\n\" if end else \"\"\n        )\n\n\nif __name__ == \"__main__\":\n    print(Lotties.CLICK.format(\"hello\"))\n"}
{"type": "source_file", "path": "app/common/config.py", "content": "import logging\nfrom dataclasses import dataclass, field\nfrom os import environ\nfrom pathlib import Path\nfrom re import Pattern, compile\nfrom sys import modules\nfrom typing import Optional, Union\nfrom urllib import parse\n\nfrom aiohttp import ClientTimeout\nfrom dotenv import load_dotenv\n\nif modules.get(\"pytest\") is not None:\n    print(\"- Running in pytest mode.\")\n    environ[\"API_ENV\"] = \"test\"\n\n\nif load_dotenv():\n    print(\"- Loaded .env file successfully.\")\nelse:\n    print(\"- Failed to load .env file.\")\n\n# API Server Variables\nAPI_ENV: str = environ.get(\"API_ENV\", \"local\")\nprint(f\"- API_ENV: {API_ENV}\")\nDOCKER_MODE: bool = environ.get(\"DOCKER_MODE\", \"True\").lower() == \"true\"\nprint(f\"- DOCKER_MODE: {DOCKER_MODE}\")\n\n\nclass SingletonMetaClass(type):\n    _instances = {}\n\n    def __call__(cls, *args, **kwargs):\n        if cls not in cls._instances:\n            cls._instances[cls] = super().__call__(*args, **kwargs)\n        return cls._instances[cls]\n\n\nEXCEPT_PATH_LIST: tuple = (\n    \"/\",\n    \"/openapi.json\",\n    \"/test\",\n)\nEXCEPT_PATH_REGEX: Pattern = compile(\n    \"^(/docs|/redoc|/admin|/api/auth|/favicon.ico|/chat|/flutter_service_worker.js)\"\n)\nTOKEN_EXPIRE_HOURS: int = 168\nMAX_API_KEY: int = 3\nMAX_API_WHITELIST: int = 10\nBASE_DIR: Path = Path(__file__).parents[2]\n\n# MySQL Variables\nMYSQL_ROOT_PASSWORD: str = environ[\"MYSQL_ROOT_PASSWORD\"]\nMYSQL_USER: str = environ[\"MYSQL_USER\"]\nMYSQL_PASSWORD: str = environ.get(\"MYSQL_PASSWORD\", \"\")\nMYSQL_DATABASE: str = environ[\"MYSQL_DATABASE\"]\nMYSQL_TEST_DATABASE: str = environ.get(\"MYSQL_TEST_DATABASE\", MYSQL_DATABASE)\nMYSQL_PORT: int = int(environ.get(\"MYSQL_PORT\", 3306))\n\n# Redis Variables\nREDIS_PORT: int = int(environ.get(\"REDIS_PORT\", 6379))\nREDIS_DATABASE: int = int(environ.get(\"REDIS_DATABASE\", 0))\nREDIS_PASSWORD: str = environ[\"REDIS_PASSWORD\"]\n\n\n# Qdrant Variables\nQDRANT_COLLECTION: str = environ.get(\n    \"QDRANT_COLLECTION\", \"SharedCollection\"\n)  # Shared Qdrant collection\n\n# Other Required Variables\nHOST_MAIN: str = environ.get(\"HOST_MAIN\", \"localhost\")\nJWT_SECRET: str = environ[\"JWT_SECRET\"]\nJWT_ALGORITHM: str = \"HS256\"\n\n\n# Optional Service Variables\nGLOBAL_PREFIX: Optional[str] = environ.get(\"GLOBAL_PREFIX\", None)\nGLOBAL_SUFFIX: Optional[str] = environ.get(\"GLOBAL_SUFFIX\", None)\nif GLOBAL_PREFIX in (\"\", \"None\"):\n    GLOBAL_PREFIX = None\nif GLOBAL_SUFFIX in (\"\", \"None\"):\n    GLOBAL_SUFFIX = None\n\nLOCAL_EMBEDDING_MODEL: Optional[str] = environ.get(\n    \"LOCAL_EMBEDDING_MODEL\", None\n)\nif str(LOCAL_EMBEDDING_MODEL).lower() in (\"\", \"none\"):\n    LOCAL_EMBEDDING_MODEL = None\nEMBEDDING_TOKEN_CHUNK_SIZE: int = int(\n    environ.get(\"EMBEDDING_TOKEN_CHUNK_SIZE\", 512)\n)\nEMBEDDING_TOKEN_CHUNK_OVERLAP: int = int(\n    environ.get(\"EMBEDDING_TOKEN_CHUNK_OVERLAP\", 128)\n)\nSUMMARIZE_FOR_CHAT: bool = (\n    environ.get(\"SUMMARIZE_FOR_CHAT\", \"True\").lower() == \"true\"\n)\nSUMMARIZATION_THRESHOLD: int = int(environ.get(\"SUMMARIZATION_THRESHOLD\", 512))\nDEFAULT_LLM_MODEL: str = environ.get(\"DEFAULT_LLM_MODEL\", \"gpt_3_5_turbo\")\nOPENAI_API_KEY: Optional[str] = environ.get(\"OPENAI_API_KEY\")\nRAPID_API_KEY: Optional[str] = environ.get(\"RAPID_API_KEY\")\nGOOGLE_TRANSLATE_API_KEY: Optional[str] = environ.get(\n    \"GOOGLE_TRANSLATE_API_KEY\"\n)\nPAPAGO_CLIENT_ID: Optional[str] = environ.get(\"PAPAGO_CLIENT_ID\")\nPAPAGO_CLIENT_SECRET: Optional[str] = environ.get(\"PAPAGO_CLIENT_SECRET\")\nCUSTOM_TRANSLATE_URL: Optional[str] = environ.get(\"CUSTOM_TRANSLATE_URL\")\nAWS_ACCESS_KEY: Optional[str] = environ.get(\"AWS_ACCESS_KEY\")\nAWS_SECRET_KEY: Optional[str] = environ.get(\"AWS_SECRET_KEY\")\nAWS_AUTHORIZED_EMAIL: Optional[str] = environ.get(\"AWS_AUTHORIZED_EMAIL\")\nSAMPLE_JWT_TOKEN: Optional[str] = environ.get(\"SAMPLE_JWT_TOKEN\")\nSAMPLE_ACCESS_KEY: Optional[str] = environ.get(\"SAMPLE_ACCESS_KEY\")\nSAMPLE_SECRET_KEY: Optional[str] = environ.get(\"SAMPLE_SECRET_KEY\")\nKAKAO_RESTAPI_TOKEN: Optional[str] = environ.get(\"KAKAO_RESTAPI_TOKEN\")\nWEATHERBIT_API_KEY: Optional[str] = environ.get(\"WEATHERBIT_API_KEY\")\nKAKAO_IMAGE_URL: Optional[\n    str\n] = \"http://k.kakaocdn.net/dn/wwWjr/btrYVhCnZDF/2bgXDJth2LyIajIjILhLK0/kakaolink40_original.png\"\n\n\"\"\"\n400 Bad Request\n401 Unauthorized\n403 Forbidden\n404 Not Found\n405 Method not allowed\n500 Internal Error\n502 Bad Gateway\n504 Timeout\n200 OK\n201 Created\n\"\"\"\n\n\n@dataclass\nclass Config(metaclass=SingletonMetaClass):\n    app_title: str = \"FastAPI\"\n    app_description: str = \"\"\n    app_version: str = \"1.0.0\"\n    host_main: str = HOST_MAIN\n    port: int = 8000\n    db_pool_recycle: int = 900\n    db_echo: bool = False\n    debug: bool = False\n    test_mode: bool = False\n    database_url_format: str = \"{dialect}+{driver}://{user}:{password}@{host}:{port}/{database}?charset=utf8mb4\"\n    mysql_root_password: str = MYSQL_ROOT_PASSWORD\n    mysql_user: str = MYSQL_USER\n    mysql_password: str = MYSQL_PASSWORD\n    mysql_database: str = MYSQL_DATABASE\n    mysql_host: str = \"db\"\n    mysql_port: int = MYSQL_PORT\n    redis_url_format: str = \"redis://{username}:{password}@{host}:{port}/{db}\"\n    redis_host: str = \"cache\"\n    redis_port: int = REDIS_PORT\n    redis_database: int = REDIS_DATABASE\n    redis_password: str = REDIS_PASSWORD\n    qdrant_host: str = \"vectorstore\"\n    qdrant_port: int = 6333\n    qdrant_grpc_port: int = 6334\n    shared_vectorestore_name: str = QDRANT_COLLECTION\n    trusted_hosts: list[str] = field(default_factory=lambda: [\"*\"])\n    allowed_sites: list[str] = field(default_factory=lambda: [\"*\"])\n    llama_completion_url: Optional[\n        str\n    ] = \"http://localhost:8002/v1/completions\"\n    llama_embedding_url: Optional[str] = \"http://localhost:8002/v1/embeddings\"\n    llama_server_port: Optional[int] = 8002\n\n    def __post_init__(self):\n        self.is_llama_available: bool = False\n        self.is_llama_booting: bool = False\n        if not DOCKER_MODE:\n            self.port = 8001\n            self.mysql_host = \"localhost\"\n            self.redis_host = \"localhost\"\n            self.qdrant_host = \"localhost\"\n        self.mysql_root_url = self.database_url_format.format(\n            dialect=\"mysql\",\n            driver=\"pymysql\",\n            user=\"root\",\n            password=parse.quote(self.mysql_root_password),\n            host=self.mysql_host,\n            port=self.mysql_port,\n            database=self.mysql_database,\n        )\n        self.mysql_url = self.database_url_format.format(\n            dialect=\"mysql\",\n            driver=\"aiomysql\",\n            user=self.mysql_user,\n            password=parse.quote(self.mysql_password),\n            host=self.mysql_host,\n            port=self.mysql_port,\n            database=self.mysql_database,\n        )\n        self.redis_url = self.redis_url_format.format(\n            username=\"\",\n            password=self.redis_password,\n            host=self.redis_host,\n            port=self.redis_port,\n            db=self.redis_database,\n        )\n\n    @staticmethod\n    def get(\n        option: Optional[str] = None,\n    ) -> Union[\"LocalConfig\", \"ProdConfig\", \"TestConfig\"]:\n        if option is not None:\n            return {\n                \"prod\": ProdConfig,\n                \"local\": LocalConfig,\n                \"test\": TestConfig,\n            }[option]()\n        else:\n            if API_ENV is not None:\n                return {\n                    \"prod\": ProdConfig,\n                    \"local\": LocalConfig,\n                    \"test\": TestConfig,\n                }[API_ENV.lower()]()\n            else:\n                return LocalConfig()\n\n\n@dataclass\nclass LocalConfig(Config):\n    debug: bool = True\n\n\n@dataclass\nclass ProdConfig(Config):\n    db_echo: bool = False\n    trusted_hosts: list = field(\n        default_factory=lambda: [\n            f\"*.{HOST_MAIN}\",\n            HOST_MAIN,\n            \"localhost\",\n        ]\n    )\n    allowed_sites: list = field(\n        default_factory=lambda: [\n            f\"*.{HOST_MAIN}\",\n            HOST_MAIN,\n            \"localhost\",\n        ]\n    )\n\n\n@dataclass\nclass TestConfig(Config):\n    test_mode: bool = True\n    debug: bool = False\n    mysql_database: str = MYSQL_TEST_DATABASE\n    mysql_host: str = \"localhost\"\n    redis_host: str = \"localhost\"\n    qdrant_host: str = \"localhost\"\n    port: int = 8001\n\n\n@dataclass\nclass LoggingConfig:\n    logger_level: int = logging.DEBUG\n    console_log_level: int = logging.INFO\n    file_log_level: Optional[int] = logging.DEBUG\n    file_log_name: Optional[str] = \"./logs/debug.log\"\n    logging_format: str = \"[%(asctime)s] %(name)s:%(levelname)s - %(message)s\"\n\n\n@dataclass\nclass ChatConfig:\n    api_url: str = (\n        \"https://api.openai.com/v1/chat/completions\"  # api url for openai\n    )\n    timeout: ClientTimeout = ClientTimeout(sock_connect=30.0, sock_read=20.0)\n    read_timeout: float = 30.0  # wait for this time before timeout\n    wait_for_reconnect: float = 3.0  # wait for this time before reconnecting\n    api_regex_pattern_openai: Pattern = compile(\n        r\"data:\\s*({.+?})\\n\\n\"\n    )  # regex pattern to extract json from openai api response\n    api_regex_pattern_llama_cpp: Pattern = compile(\n        r\"data:\\s*({.+?})\\r\\n\\r\\n\"\n    )  # regex pattern to extract json from llama cpp api response\n    extra_token_margin: int = (\n        512  # number of tokens to remove when tokens exceed token limit\n    )\n    continue_message: str = (\n        \"...[CONTINUED]\"  # message to append when tokens exceed token limit\n    )\n    summarize_for_chat: bool = (\n        SUMMARIZE_FOR_CHAT  # whether to summarize chat messages\n    )\n    summarization_threshold: int = (\n        SUMMARIZATION_THRESHOLD  # if message tokens exceed this, summarize\n    )\n    global_openai_model: str = \"gpt-3.5-turbo-0613\"\n    summarization_token_limit: int = (\n        EMBEDDING_TOKEN_CHUNK_SIZE  # token limit for summarization\n    )\n    summarization_token_overlap: int = EMBEDDING_TOKEN_CHUNK_OVERLAP  # number of tokens to overlap between chunks\n    summarization_chunk_size: int = 2048\n    query_context_token_limit: int = 2048\n    scrolling_chunk_size_when_browsing: int = 1024\n    scrolling_overlap_when_browsing: int = 256\n    vectorstore_n_results_limit: int = 10\n    global_prefix: Optional[str] = GLOBAL_PREFIX  # prefix for global chat\n    global_suffix: Optional[str] = GLOBAL_SUFFIX  # suffix for global chat\n    local_embedding_model: Optional[str] = LOCAL_EMBEDDING_MODEL\n\n\nconfig = Config.get()\nlogging_config = LoggingConfig()\nchat_config = ChatConfig()\n"}
{"type": "source_file", "path": "app/common/constants.py", "content": "from re import compile, Pattern\n\nfrom langchain import PromptTemplate\n\n\nclass QueryTemplates:\n    CONTEXT_QUESTION__WEB_BROWSING = PromptTemplate(\n        template=(\n            \"The results of a web search for the user's question are shown below, enclosed in triple dashes(---).\\n\"\n            \"You can use this information to answer user's question.\"\n            \"\\n---\\n\"\n            \"{context}\"\n            \"\\n---\\n\"\n            \"Answer the question in as much detail as possible: {question}\\n\"\n        ),\n        input_variables=[\"context\", \"question\"],\n        template_format=\"f-string\",\n    )\n    CONTEXT_QUESTION__CONTEXT_ONLY = PromptTemplate(\n        template=(\n            \"Context information is below.\"\n            \"\\n---\\n\"\n            \"{context}\"\n            \"\\n---\\n\"\n            \"Given the context information and not prior knowledge, \"\n            \"answer the question in as much detail as possible:: {question}\\n\"\n        ),\n        input_variables=[\"context\", \"question\"],\n        template_format=\"f-string\",\n    )\n\n\nclass DescriptionTemplates:\n    USER_AI__DEFAULT: PromptTemplate = PromptTemplate(\n        template=(\n            \"The following is a friendly conversation between a {user} and an {ai}. \"\n            \"The {ai} is talkative and provides lots of specific details from its context. \"\n            \"If the {ai} does not know the answer to a question, it truthfully says it does not know.\\n\\n\"\n            \"Current conversation:\\n\\n\"\n        ),\n        input_variables=[\"user\", \"ai\"],\n        template_format=\"f-string\",\n    )\n\n    USER_AI__SHORT: PromptTemplate = PromptTemplate(\n        template=(\n            \"A chat between a curious human and an artificial intelligence assistant. \"\n            \"The assistant gives helpful, detailed, and polite answers to the human's questions.\"\n        ),\n        input_variables=[],\n    )\n    USER_AI__VERT_SHORT: PromptTemplate = PromptTemplate(\n        template=\"You are a helpful AI assistant.\",\n        input_variables=[],\n    )\n\n    USER_AI__GAME: PromptTemplate = PromptTemplate(\n        template=(\n            \"Make Narrator perform as a text based adventure game with Player as Narrator's u\"\n            \"ser input. Make Narrator describe the scene, scenario, actions of characters, re\"\n            \"actions of characters to the player's actions, and potential consequences of the\"\n            \"ir actions and Player's actions when relevant with visually descriptive, detaile\"\n            \"d, and long storytelling. Allow characters and Player to converse to immerse Pla\"\n            \"yer in a rich narrative driven story. When Player encounters a new character, Na\"\n            \"rrator will name the new character and describe their behavior and appearance. N\"\n            \"arrator will internally determine their underlying motivations and weave it into\"\n            \" the story where possible.\"\n        ),\n        input_variables=[],\n    )\n    USER_AI__ENGLISH: PromptTemplate = PromptTemplate(\n        template=(\n            \"You are a good English teacher. Any sentence that {user} says that is surrounded\"\n            ' by double quotation marks (\"\") is asking how you interpret that sentence. Pleas'\n            \"e analyze and explain that sentence in as much detail as possible. For the rest \"\n            \"of the sentences, please respond in a way that will help {user} learn English.\"\n        ),\n        input_variables=[\"user\"],\n    )\n\n\nclass ChatTurnTemplates:\n    ROLE_CONTENT_1: PromptTemplate = PromptTemplate(\n        template=\"### {role}: {content}\\n\",\n        input_variables=[\"role\", \"content\"],\n        template_format=\"f-string\",\n    )\n    ROLE_CONTENT_2: PromptTemplate = PromptTemplate(\n        template=\"### {role}:\\n{content}\\n\",\n        input_variables=[\"role\", \"content\"],\n        template_format=\"f-string\",\n    )\n    ROLE_CONTENT_3: PromptTemplate = PromptTemplate(\n        template=\"# {role}:\\n{content}\\n\",\n        input_variables=[\"role\", \"content\"],\n        template_format=\"f-string\",\n    )\n    ROLE_CONTENT_4: PromptTemplate = PromptTemplate(\n        template=\"###{role}: {content}\\n\",\n        input_variables=[\"role\", \"content\"],\n        template_format=\"f-string\",\n    )\n    ROLE_CONTENT_5: PromptTemplate = PromptTemplate(\n        template=\"{role}: {content}\\n\",\n        input_variables=[\"role\", \"content\"],\n        template_format=\"f-string\",\n    )\n    ROLE_CONTENT_6: PromptTemplate = PromptTemplate(\n        template=\"{role}: {content}</s>\",\n        input_variables=[\"role\", \"content\"],\n        template_format=\"f-string\",\n    )\n\nclass SummarizationTemplates:\n    TEXT__MARKUP: PromptTemplate = PromptTemplate(\n        template=(\n            \"Write a concise summary of the following text delimited by triple backquotes. \"\n            \"Return your response in bullet points which covers the key points of the text.\\n\"\n            \"```\\n{text}\\n```\\n\\nBULLET POINT SUMMARY:\\n\"\n        ),\n        input_variables=[\"text\"],\n    )\n\n    TEXT__CONVERSATIONAL: PromptTemplate = PromptTemplate(\n        template=(\n            \"Write a summary of the following conversations delimited by triple backquotes.\\n\"\n            \"Organize the key points of each message in the order of the conversation in a format like \"\n            \"`ROLE: SUMMARY`.\\n\"\n            \"```\\n{text}\\n```\\n\\nCONVERSATION SUMMARY:\\n\"\n        ),\n        input_variables=[\"text\"],\n    )\n\n\nclass JsonTemplates:\n    \"\"\"Deprecated since OpenAI's function call is much better\"\"\"\n\n    QUERY__GET_QUERY_FOR_WEB_BROWSING: PromptTemplate = PromptTemplate(\n        template=(\n            \"You are a Web search API bot that performs a web search for a user's question. F\"\n            \"ollow the rules below to output a response.\\n- Output the query to search the web\"\n            ' for USER\\'S QUESTION in JSON format like {\"query\": QUERY_TO_SEARCH}.\\n- QUERY_TO_SEARCH i'\n            \"s a set of words within 10 words.\\n- Your response must be in JSON format, starti\"\n            \"ng with { and ending with }.\\n- Output a generalized query to return sufficiently\"\n            \" relevant results when searching the web.\\n- If a suitable search query does not \"\n            'exist, output {\"query\": null} - don\\'t be afraid to output null!\\n\\n'\n            \"```USER'S QUESTION\\n{{query}}\\n```\"\n            \"\\nYOUR JSON RESPONSE:\\n\"\n        ),\n        input_variables=[\"query\"],\n        template_format=\"jinja2\",\n    )\n    QUERY__GET_QUERY_FOR_VECTORSTORE: PromptTemplate = PromptTemplate(\n        template=(\n            \"You are a Search API bot performing a vector similarity-based search for a user'\"\n            \"s question. Follow the rules below to output a response.\\n- Output the query to s\"\n            'earch the web for USER\\'S QUESTION in JSON format like {\"query\": QUERY_TO_SEARCH}.\\n-'\n            \" QUERY_TO_SEARCH creates a hypothetical answer to facilitate searching in the Ve\"\n            \"ctor database.\\n- Your response must be in JSON format, starting with { and endin\"\n            'g with }.\\n- If a suitable search query does not exist, output {\"query\": NULL} - '\n            \"don't be afraid to output NULL!\"\n            \"```USER'S QUESTION\\n{{query}}\\n```\"\n            \"\\nYOUR JSON RESPONSE:\\n\"\n        ),\n        input_variables=[\"query\"],\n        template_format=\"jinja2\",\n    )\n    CONTEXT_QUERY__CLICK_LINK_OR_FINISH: PromptTemplate = PromptTemplate(\n        template=(\n            \"You are a JSON response bot that determines if the provided CONTEXT is sufficient to \"\n            \"answer the user's question. Follow the rules below to output a response.\\n- Outpu\"\n            't your next action to do in JSON form like {\"action\": YOUR_NEXT_ACTION, \"link\": '\n            'LINK_TO_CLICK}.\\n- \"action\" should be one of \"click\", \"finish\".\\n- {\"action\": \"cli'\n            'ck\"} should be selected when you want to click on a link to read more about it.\\n'\n            '- {\"action\": \"finish\"} should be selected when the information already provided '\n            'is sufficient to answer the user.\\n- \"link\" should be a link to click. You don\\'t '\n            'have to output \"link\" if you decided to take \"action\" as \"finish\".\\n- CONTEXT con'\n            \"sists of multiple #[LINK]\\\\n```TITLE\\\\nSNIPPET\\n```CONTEXT\\n{{context}}\\n```\\n```USER'\"\n            \"S QUESTION\\n{{query}}\\n```\"\n            \"\\nYOUR JSON RESPONSE:\\n\"\n        ),\n        input_variables=[\"context\", \"query\"],\n        template_format=\"jinja2\",\n    )\n    CONTEXT_QUERY__ANSWERABLE_OR_NOT: PromptTemplate = PromptTemplate(\n        template=(\n            \"You are a JSON response bot that uses the context provided to determine if you c\"\n            \"an answer the user's question. Follow the rules below to output a response.\\n- Ou\"\n            'tput your next action to do in JSON format like {\"answerable\": TRUE_OR_FALSE}. This '\n            'is important.\\n- \"answerable\" should be one of true or false.\\n- CONTEXT and USER\\'s QU'\n            \"ESTION are surrounded by triple backticks.\\n```CONTEXT\\n{{context}}\\n```\\n```USER'S \"\n            \"QUESTION\\n{{query}}\\n```\"\n            \"\\nYOUR JSON RESPONSE:\\n\"\n        ),\n        input_variables=[\"context\", \"query\"],\n        template_format=\"jinja2\",\n    )\n\n\nclass SystemPrompts:\n    CODEX: str = (\n        'Act as CODEX (\"COding DEsign eXpert\"), an expert coder with experience in mult'\n        \"iple coding languages. Always follow the coding best practices by writing clean,\"\n        \" modular code with proper security measures and leveraging design patterns. You \"\n        \"can break down your code into parts whenever possible to avoid breaching the cha\"\n        'tgpt output character limit. Write code part by part when I send \"continue\". I'\n        'f you reach the character limit, I will send \"continue\" and then you should co'\n        \"ntinue without repeating any previous code. Do not assume anything from your sid\"\n        \"e; please ask me a numbered list of essential questions before starting. If you \"\n        \"have trouble fixing a bug, ask me for the latest code snippets for reference fro\"\n        \"m the official documentation. I am using [MacOS], [VSCode] and prefer [brew] pac\"\n        'kage manager. Start a conversation as \"CODEX: Hi, what are we coding today?'\n    )\n\n    REDEX: str = (\n        \"compress the following text in a way that fits in a tweet (ideally) and such tha\"\n        \"t you (GPT) can reconstruct the intention of the human who wrote text as close a\"\n        \"s possible to the original intention. This is for yourself. It does not need to \"\n        \"be human readable or understandable. Abuse of language mixing, abbreviations, sy\"\n        \"mbols (unicode and emoji), or any other encodings or internal representations is\"\n        \" all permissible, as long as it, if pasted in a new inference cycle, will yield \"\n        \"near-identical results as the original text: \"\n    )\n\n\nLONG_PROMPT = (\n    \"This year, the elves invested in a gift-wrapping machine. However, it i\"\n    \"sn't programmed! An algorithm that aids it in the task must be developed. Many p\"\n    \"resents are given to the machine. Each present is a string. Each gift must be wr\"\n    \"apped by the machine and set in a display of other wrapped gifts. To wrap a gift\"\n    \", you must place the wrapping paper around the string, which is represented by t\"\n    \"he * symbol. For instance: const gifts are [“cat,” “game,” and “socks”]. console\"\n    \".log const wrapped = wrapping(gifts) (wrapped) / [“ncatn,” “ngamen,” and “nsocks\"\n    \"n**”] */ As you can see, the thread is wrapped in the wrapping paper. The corner\"\n    \"s are also wrapped in wrapping paper on the top and bottom to prevent any gaps.\"\n)\n\nCONVERSATION_EXAMPLES: list[dict[str, str]] = [\n    {\n        \"role\": \"user\",\n        \"content\": (\n            \"are you familiar with a technology startup called Jigsaw that was acquired by Sa\"\n            \"lesforce.com? can you provide a detailed summary of its company history, busines\"\n            \"s model, and product?\"\n        ),\n    },\n    {\n        \"role\": \"assistant\",\n        \"content\": (\n            \"As of my knowledge cutoff date in September 2021, I am not aware of a technology\"\n            \" startup called Jigsaw that was acquired by Salesforce.com. However, there was a\"\n            \" company called Jigsaw Data Corporation, which was acquired by Salesforce in 201\"\n            \"0. I can provide you with information on that company, but please note that my k\"\n            \"nowledge may be outdated, and there may be another Jigsaw startup that I am not \"\n            \"aware of.\\nJigsaw Data Corporation was a technology company founded in 2004 by Ji\"\n            \"m Fowler and Garth Moulton. The company specialized in providing crowd-sourced b\"\n            \"usiness contact data, offering a cloud-based platform for users to access and up\"\n            \"date business contacts and company information.\\nThe business model of Jigsaw was\"\n            \" unique at that time, as it relied on a collaborative, crowd-sourced approach to\"\n            \" collect and maintain data. Users could access the platform for free by contribu\"\n            \"ting their own contact information, or they could purchase points to access the \"\n            \"data without contributing. This model incentivized users to add and update conta\"\n            \"ct information, ensuring the database remained fresh and accurate.\\nJigsaw's prim\"\n            \"ary product was an online directory of business contacts and company information\"\n            \". The platform provided its users with access to a vast database containing mill\"\n            \"ions of business contacts across various industries. Users could search for cont\"\n            \"acts by various criteria such as name, title, company, or location. Additionally\"\n            \", Jigsaw offered a suite of data management tools that allowed users to import, \"\n            \"export, and manage their contacts efficiently.\\nIn April 2010, Salesforce.com, a \"\n            \"leading cloud-based customer relationship management (CRM) company, announced th\"\n            \"e acquisition of Jigsaw Data Corporation for approximately $142 million in cash.\"\n            \" The acquisition aimed to enhance Salesforce's CRM offerings by integrating Jigs\"\n            \"aw's data services into its platform, allowing users to access updated business \"\n            \"contact information within their CRM system directly.\\nAfter the acquisition, Jig\"\n            'saw\\'s product was rebranded as \"Data.com\" and became an integral part of the Sal'\n            \"esforce.com ecosystem. Data.com offered Salesforce customers access to the crowd\"\n            \"-sourced contact database, as well as additional data management and enrichment \"\n            \"services.\\nPlease note that this information is accurate as of my knowledge cutof\"\n            \"f date in September 2021, and there may have been further developments or change\"\n            \"s to the company and its products since then.\"\n        ),\n    },\n    {\n        \"role\": \"user\",\n        \"content\": (\n            \"That's the correct company. Thank you. can you tell me more about its unique cro\"\n            \"wd-sourced approach to collecting and maintaining data? which other companies ha\"\n            \"ve used a similar approach?\"\n        ),\n    },\n    {\n        \"role\": \"assistant\",\n        \"content\": (\n            \"Jigsaw Data Corporation's crowd-sourced approach to collecting and maintaining b\"\n            \"usiness contact data relied on its user community to contribute, update, and ver\"\n            \"ify information. This model had several key components:\\n\\nContribution: Users cou\"\n            \"ld create a free account on Jigsaw's platform by contributing their own business\"\n            \" contact information. They could also add new contacts to the database and gain \"\n            \"points, which could be used to access other contacts' information.\\n\\nVerification\"\n            \": Jigsaw encouraged users to verify the accuracy of contact information in the d\"\n            \"atabase by rewarding them with points for each correction made. This created an \"\n            \"incentive for users to keep the data up-to-date and accurate.\\n\\nPoints system: Ji\"\n            \"gsaw's points system played a crucial role in its crowd-sourced model. Users cou\"\n            \"ld earn points by contributing new contacts, verifying existing contacts, or ref\"\n            \"erring new users to the platform. These points could then be used to access cont\"\n            \"act information without having to contribute their own.\\n\\nData exchange: Jigsaw a\"\n            \"llowed users to exchange their points for contact information. Users who didn't \"\n            \"want to contribute their own data could purchase points, while those who contrib\"\n            \"uted regularly could access the database for free by using their earned points.\\n\"\n            \"\\nData quality: To ensure data quality, Jigsaw implemented a rating system for us\"\n            \"ers based on their contributions and accuracy. Users with higher ratings were co\"\n            \"nsidered more reliable, which motivated users to contribute accurate and up-to-d\"\n            \"ate information.\\n\\nSeveral other companies have used crowd-sourcing techniques in\"\n            \" different industries, although not necessarily in the same way as Jigsaw:\\n\\nWaze\"\n            \": Waze is a popular GPS navigation app that uses crowd-sourced data from its use\"\n            \"rs to provide real-time traffic information, road conditions, and other driving-\"\n            \"related updates.\\n\\nWikipedia: Wikipedia is a free online encyclopedia that relies\"\n            \" on its user community to create, edit, and maintain its content. It has become \"\n            \"one of the largest and most comprehensive sources of information on the internet\"\n            \".\\n\\nOpenStreetMap: OpenStreetMap is a collaborative project that allows users to \"\n            \"create and edit map data. It relies on a global community of contributors to mai\"\n            \"ntain accurate and up-to-date mapping information.\\n\\nMechanical Turk: Amazon's Me\"\n            \"chanical Turk is a marketplace for human intelligence tasks (HITs), which allows\"\n            \" users to complete small tasks in exchange for payment. It relies on the crowd t\"\n            \"o complete tasks that are challenging for machines but simple for humans.\\n\\nKaggl\"\n            \"e: Kaggle is a platform for data scientists and machine learning practitioners t\"\n            \"o participate in competitions, collaborate on projects, and learn from each othe\"\n            \"r. It uses crowd-sourcing to find innovative solutions to complex data problems.\"\n        ),\n    },\n    {\n        \"role\": \"user\",\n        \"content\": (\n            \"do you think that a crowd-sourced approach to data collection could work for an \"\n            \"AI startup? specifically, the startup would award users points for contributing \"\n            \"proprietary data used to train the AI model. in exchange, users could spend poin\"\n            'ts by asking the AI questions. for example, an \"AI architect\" startup could give'\n            \" users points for contributing architectural plans and CAD drawings. users could\"\n            \" then spend those points by asking the AI to design new plans. do you think this\"\n            \" idea is viable? \"\n        ),\n    },\n    {\n        \"role\": \"assistant\",\n        \"content\": (\n            \"A crowd-sourced approach to data collection can indeed work for an AI startup, e\"\n            \"specially if the startup aims to develop AI models that require a vast and diver\"\n            \"se dataset to perform well. The idea you've proposed, in which users contribute \"\n            \"proprietary data to train the AI model and can later use the AI's services by sp\"\n            \"ending points, has some potential advantages:\\n\\nIncentivized data contribution: B\"\n            \"y rewarding users with points for contributing data, you create an incentive for\"\n            \" them to share valuable information, which can help grow and diversify your data\"\n            \"set.\\n\\nEncouraging user engagement: A points-based system encourages users to act\"\n            \"ively participate in the platform, which can help improve the AI model over time\"\n            \" and increase user satisfaction.\\n\\nCost-effective data acquisition: Crowd-sourcin\"\n            \"g can be a cost-effective way to gather large amounts of data, as it leverages t\"\n            \"he efforts of a large community rather than relying on paid data collection serv\"\n            \"ices.\\n\\nContinuous improvement: As users contribute more data and use the AI's se\"\n            \"rvices, the model can be iteratively improved, leading to better performance and\"\n            \" more valuable insights.\\n\\nHowever, there are also challenges and potential pitfa\"\n            \"lls to consider:\\n\\nData quality: Ensuring the quality and accuracy of the contrib\"\n            \"uted data is critical. You may need to implement verification processes, such as\"\n            \" peer review or expert validation, to maintain data quality.\\n\\nIntellectual prope\"\n            \"rty and privacy: Handling proprietary data comes with legal and ethical consider\"\n            \"ations. You'll need to establish clear terms of use, ensure data protection, and\"\n            \" address any intellectual property concerns when users contribute data.\\n\\nScalabi\"\n            \"lity: If the AI's services become popular, you may face challenges related to sc\"\n            \"aling the infrastructure and resources needed to support a growing user base.\\n\\nU\"\n            \"ser retention: Maintaining user engagement and interest in the platform can be c\"\n            \"hallenging, especially as users may eventually exhaust their need for the AI's s\"\n            \"ervices.\\n\\nMonetization: Balancing the points-based system with a sustainable rev\"\n            \"enue model might be challenging, as users may prefer to use the platform for fre\"\n            \"e by contributing data rather than paying for services.\\n\\nOverall, the viability \"\n            \"of this idea depends on various factors, including the nature of the AI model, t\"\n            \"he target market, the quality and relevance of the contributed data, and the abi\"\n            \"lity to address the challenges mentioned above. If the startup can successfully \"\n            \"navigate these challenges, a crowd-sourced approach to data collection could be \"\n            \"a promising strategy.\"\n        ),\n    },\n    {\n        \"role\": \"user\",\n        \"content\": (\n            \"thank you, that was an excellent answer. besides architecture, what are some oth\"\n            \"er areas where this crowd-sourced approach to data collection could be used in o\"\n            \"rder to improve an AI model? specifically, what are some valuable data sets that\"\n            \" are somewhat proprietary and hard to get, but users who own that data may be wi\"\n            \"lling to share it in exchange for points? \"\n        ),\n    },\n    {\n        \"role\": \"assistant\",\n        \"content\": (\n            \"Thank you for your kind words. There are several areas where a crowd-sourced app\"\n            \"roach to data collection could be used to improve AI models by leveraging valuab\"\n            \"le and somewhat proprietary datasets. Here are a few examples:\\n\\nMedical and heal\"\n            \"th data: AI models can greatly benefit from access to diverse patient data, such\"\n            \" as electronic health records, medical imaging, and genomic data. Users (patient\"\n            \"s or healthcare professionals) might be willing to share anonymized data in exch\"\n            \"ange for points, which could then be used to access AI-driven health insights, p\"\n            \"ersonalized treatment suggestions, or early disease detection.\\n\\nLegal document a\"\n            \"nalysis: Law firms and legal professionals often have access to large collection\"\n            \"s of legal documents, such as contracts, court rulings, or patent filings. By sh\"\n            \"aring these documents, users could contribute to training AI models for legal do\"\n            \"cument analysis, and in return, gain access to AI-driven legal research tools or\"\n            \" contract review services.\\n\\nArt and creative work: Artists and designers may pos\"\n            \"sess large collections of their own artwork, sketches, or designs. Sharing this \"\n            \"data could help train AI models for artistic style transfer, generative art, or \"\n            \"design assistance. Users could then access AI-driven creative tools or personali\"\n            \"zed design suggestions.\\n\\nFinance and investment: Financial professionals and inv\"\n            \"estors may have access to proprietary trading algorithms, portfolio data, or mar\"\n            \"ket analysis reports. By sharing this data, they could contribute to AI models f\"\n            \"or financial analysis and predictions. In return, users could gain access to AI-\"\n            \"driven investment advice, risk assessment, or market forecasting tools.\\n\\nScienti\"\n            \"fic research data: Researchers in various fields might have access to valuable d\"\n            \"atasets generated through experiments or simulations. By sharing this data, they\"\n            \" can help train AI models for data analysis, pattern recognition, or predictive \"\n            \"modeling in their respective domains. Users could then access AI-driven research\"\n            \" tools or personalized research recommendations.\\n\\nManufacturing and production d\"\n            \"ata: Companies involved in manufacturing and production may possess proprietary \"\n            \"data on production processes, quality control, and equipment performance. Sharin\"\n            \"g this data could improve AI models for predictive maintenance, process optimiza\"\n            \"tion, and quality assurance. Users could then gain access to AI-driven optimizat\"\n            \"ion suggestions or equipment monitoring services.\\n\"\n        ),\n    },\n]\n\nJSON_PATTERN: Pattern = compile(r\"\\{.*\\}\")\n\nif __name__ == \"__main__\":\n    import sys\n\n    try:\n        import pyperclip\n    except ImportError:\n        pyperclip = None\n\n    ANSI_COLORS = {\n        \"red\": \"\\033[91m\",\n        \"green\": \"\\033[92m\",\n        \"yellow\": \"\\033[93m\",\n        \"blue\": \"\\033[94m\",\n        \"magenta\": \"\\033[95m\",\n        \"cyan\": \"\\033[96m\",\n        \"white\": \"\\033[97m\",\n        \"black\": \"\\033[98m\",\n        \"end\": \"\\033[0m\",\n    }\n\n    def split_long_text(long_text: str, chars_per_line: int):\n        split_strings = [\n            repr(long_text[i : i + chars_per_line])\n            for i in range(0, len(long_text), chars_per_line)\n        ]\n        return \"(\" + \"\\n\".join(split_strings) + \")\"\n\n    while True:\n        lines = []\n        print(\n            f\"{ANSI_COLORS['cyan']}> Input long texts to compress:{ANSI_COLORS['end']}\"\n        )\n        try:\n            for line in sys.stdin:\n                line = line.strip()\n                lines.append(line)\n        except KeyboardInterrupt:\n            pass\n\n        if not lines:\n            print(f\"{ANSI_COLORS['red']}No input, exiting...{ANSI_COLORS['end']}\")\n            break\n\n        # Join the lines with newline characters\n        long_text = \"\\n\".join(lines)\n        result = split_long_text(long_text, 66)\n        print(f\"\\n\\n{ANSI_COLORS['green']}{result}{ANSI_COLORS['end']}\\n\\n\")\n        if pyperclip:\n            pyperclip.copy(result)\n            print(f\"{ANSI_COLORS['yellow']}Copied to clipboard!{ANSI_COLORS['end']}\")\n"}
{"type": "source_file", "path": "app/shared.py", "content": "from concurrent.futures import ProcessPoolExecutor\nfrom dataclasses import dataclass, field\nfrom multiprocessing import Event as ProcessEvent\nfrom multiprocessing import Manager, Process\nfrom multiprocessing.managers import SyncManager\nfrom multiprocessing.synchronize import Event as ProcessEventClass\nfrom threading import Event as ThreadEvent\nfrom threading import Thread\nfrom typing import Optional\n\nfrom langchain.chains.combine_documents.map_reduce import (\n    MapReduceDocumentsChain,\n)\nfrom langchain.chains.combine_documents.stuff import StuffDocumentsChain\nfrom langchain.chains.summarize import load_summarize_chain, stuff_prompt\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.embeddings import OpenAIEmbeddings\nfrom langchain.embeddings.base import Embeddings\nfrom langchain.utilities import SearxSearchWrapper\n\nfrom app.common.config import (\n    OPENAI_API_KEY,\n    ChatConfig,\n    SingletonMetaClass,\n    config,\n)\nfrom app.common.constants import SummarizationTemplates\nfrom app.utils.langchain.embeddings_api import APIEmbeddings\nfrom app.utils.langchain.token_text_splitter import CustomTokenTextSplitter\nfrom app.utils.langchain.web_search import DuckDuckGoSearchAPIWrapper\n\n\n@dataclass\nclass Shared(metaclass=SingletonMetaClass):\n    openai_embeddings: OpenAIEmbeddings = field(init=False)\n    local_embeddings: Optional[APIEmbeddings] = field(init=False)\n    map_reduce_summarize_chain: MapReduceDocumentsChain = field(init=False)\n    stuff_summarize_chain: StuffDocumentsChain = field(init=False)\n    token_text_splitter: CustomTokenTextSplitter = field(\n        default_factory=lambda: CustomTokenTextSplitter(\n            encoding_name=\"cl100k_base\"\n        )\n    )\n    searx: SearxSearchWrapper = field(\n        default_factory=lambda: SearxSearchWrapper(\n            searx_host=\"http://localhost:8080\"\n        )\n    )\n    duckduckgo: DuckDuckGoSearchAPIWrapper = field(\n        default_factory=lambda: DuckDuckGoSearchAPIWrapper()\n    )\n\n    def __post_init__(self):\n        self.openai_embeddings = OpenAIEmbeddings(\n            client=None,\n            openai_api_key=OPENAI_API_KEY,\n        )\n        if config.llama_embedding_url and ChatConfig.local_embedding_model:\n            self.local_embeddings = APIEmbeddings(\n                client=None,\n                model=ChatConfig.local_embedding_model,\n                embedding_api_url=config.llama_embedding_url,\n            )\n        else:\n            self.local_embeddings = None\n        self._process_manager = None\n        self._process_pool_executor = None\n        self._process = None\n        self._process_terminate_signal = None\n        self._thread = None\n        self._thread_terminate_signal = None\n        self.llm = ChatOpenAI(\n            client=None,\n            model=ChatConfig.global_openai_model,\n            openai_api_key=OPENAI_API_KEY,\n            streaming=False,\n        )\n        self.map_reduce_summarize_chain = load_summarize_chain(  # type: ignore\n            self.llm,\n            chain_type=\"map_reduce\",\n            map_prompt=stuff_prompt.PROMPT,\n            combine_prompt=SummarizationTemplates.TEXT__MARKUP,\n            verbose=config.debug,\n        )\n        self.stuff_summarize_chain = load_summarize_chain(  # type: ignore\n            self.llm,\n            chain_type=\"stuff\",\n            prompt=SummarizationTemplates.TEXT__MARKUP,\n            verbose=config.debug,\n        )\n\n    @property\n    def process_manager(self) -> SyncManager:\n        if not self._process_manager:\n            self._process_manager = Manager()\n        return self._process_manager\n\n    @property\n    def process_pool_executor(self) -> ProcessPoolExecutor:\n        if not self._process_pool_executor:\n            self._process_pool_executor = ProcessPoolExecutor()\n        return self._process_pool_executor\n\n    @process_pool_executor.setter\n    def process_pool_executor(self, value: ProcessPoolExecutor) -> None:\n        self._process_pool_executor = value\n\n    @property\n    def process(self) -> Process:\n        if not self._process:\n            self._process = Process()\n        return self._process\n\n    @process.setter\n    def process(self, value: Process) -> None:\n        self._process = value\n\n    @property\n    def thread(self) -> Thread:\n        if not self._thread:\n            self._thread = Thread()\n        return self._thread\n\n    @thread.setter\n    def thread(self, value: Thread) -> None:\n        self._thread = value\n\n    @property\n    def process_terminate_signal(self) -> ProcessEventClass:\n        if not self._process_terminate_signal:\n            self._process_terminate_signal = ProcessEvent()\n        return self._process_terminate_signal\n\n    @property\n    def thread_terminate_signal(self) -> ThreadEvent:\n        if not self._thread_terminate_signal:\n            self._thread_terminate_signal = ThreadEvent()\n        return self._thread_terminate_signal\n\n    @property\n    def embeddings(self) -> Embeddings:\n        if self.local_embeddings:\n            print(\"Using local embeddings\")\n            return self.local_embeddings\n        print(\"Using openai embeddings\")\n        return self.openai_embeddings\n"}
{"type": "source_file", "path": "app/models/llm_tokenizers.py", "content": "from abc import ABC, abstractmethod\nfrom pathlib import Path\nfrom typing import TYPE_CHECKING, Any, Optional, Union\n\nfrom tiktoken import Encoding, encoding_for_model, get_encoding\n\nfrom app.utils.chat.text_generations.path import resolve_model_path_to_posix\nfrom app.utils.logger import ApiLogger\n\nif TYPE_CHECKING:\n    from transformers.models.llama import LlamaTokenizer as _LlamaTokenizer\n\n    from repositories.exllama.tokenizer import ExLlamaTokenizer\n\n\nclass BaseTokenizer(ABC):\n    _fallback_tokenizer: Optional[Encoding] = None\n\n    @classmethod\n    @property\n    def fallback_tokenizer(cls) -> Encoding:\n        ApiLogger.cwarning(\"Using fallback tokenizer!!!\")\n        if cls._fallback_tokenizer is None:\n            cls._fallback_tokenizer = get_encoding(\"cl100k_base\")\n        return cls._fallback_tokenizer\n\n    @property\n    @abstractmethod\n    def tokenizer(self) -> Any:\n        ...\n\n    @property\n    @abstractmethod\n    def model_name(self) -> str:\n        ...\n\n    @abstractmethod\n    def encode(self, message: str) -> list[int]:\n        ...\n\n    @abstractmethod\n    def decode(self, tokens: list[int]) -> str:\n        ...\n\n    def tokens_of(self, message: str) -> int:\n        return len(self.encode(message))\n\n    def split_text_on_tokens(\n        self, text: str, tokens_per_chunk: int, chunk_overlap: int\n    ) -> list[str]:\n        \"\"\"Split incoming text and return chunks.\"\"\"\n        splits: list[str] = []\n        input_ids = self.encode(text)\n        start_idx = 0\n        cur_idx = min(start_idx + tokens_per_chunk, len(input_ids))\n        chunk_ids = input_ids[start_idx:cur_idx]\n        while start_idx < len(input_ids):\n            splits.append(self.decode(chunk_ids))\n            start_idx += tokens_per_chunk - chunk_overlap\n            cur_idx = min(start_idx + tokens_per_chunk, len(input_ids))\n            chunk_ids = input_ids[start_idx:cur_idx]\n        return splits\n\n    def get_chunk_of(self, text: str, tokens: int) -> str:\n        \"\"\"Split incoming text and return chunks.\"\"\"\n        input_ids = self.encode(text)\n        return self.decode(input_ids[: min(tokens, len(input_ids))])\n\n\nclass OpenAITokenizer(BaseTokenizer):\n    def __init__(self, model_name: str):\n        self._model_name = model_name\n        self._tokenizer: Encoding | None = None\n\n    def encode(self, message: str, /) -> list[int]:\n        return self.tokenizer.encode(message)\n\n    def decode(self, tokens: list[int], /) -> str:\n        return self.tokenizer.decode(tokens)\n\n    @property\n    def tokenizer(self) -> Encoding:\n        if self._tokenizer is None:\n            print(\"Loading tokenizer: \", self._model_name)\n            self._tokenizer = encoding_for_model(self._model_name)\n        return self._tokenizer\n\n    @property\n    def model_name(self) -> str:\n        return self._model_name\n\n\nclass LlamaTokenizer(BaseTokenizer):\n    def __init__(self, model_name: str):\n        try:\n            from transformers.models.llama import (\n                LlamaTokenizer as _LlamaTokenizer,\n            )\n\n            self._tokenizer_type = _LlamaTokenizer\n        except Exception as e:\n            ApiLogger.cwarning(str(e))\n            self._tokenizer_type = None\n        self._model_name = model_name\n        self._tokenizer = None\n\n    def encode(self, message: str, /) -> list[int]:\n        return self.tokenizer.encode(message)\n\n    def decode(self, tokens: list[int], /) -> str:\n        return self.tokenizer.decode(tokens)\n\n    @property\n    def tokenizer(self) -> Union[\"_LlamaTokenizer\", Encoding]:\n        if self._tokenizer is None:\n            try:\n                if self._tokenizer_type is None:\n                    raise Exception(\"LlamaTokenizer could not be imported\")\n                split_str = self._model_name.split(\"/\")\n\n                if len(split_str) == 2:\n                    root_path = self._model_name\n                    subfolder = None\n                elif len(split_str) > 2:\n                    root_path = \"/\".join(split_str[:2])\n                    subfolder = \"/\".join(split_str[2:])\n                else:\n                    raise Exception(\n                        f\"Input string {split_str} is not in the correct format\"\n                    )\n                self._tokenizer = self._tokenizer_type.from_pretrained(\n                    root_path, subfolder=subfolder\n                )\n                print(\"Tokenizer loaded:\", self._model_name)\n            except Exception as e:\n                ApiLogger.cwarning(str(e))\n                self._tokenizer = self.fallback_tokenizer\n        return self._tokenizer\n\n    @property\n    def model_name(self) -> str:\n        return self._model_name\n\n\nclass ExllamaTokenizer(BaseTokenizer):\n    def __init__(self, model_name: str):\n        try:\n            from repositories.exllama.tokenizer import (\n                ExLlamaTokenizer as _ExllamaTokenizer,\n            )\n\n            self._tokenizer_type = _ExllamaTokenizer\n        except Exception:\n            self._tokenizer_type = None\n        self._model_name = model_name\n        self._tokenizer = None\n\n    def encode(self, message: str, /) -> list[int]:\n        if isinstance(self.tokenizer, Encoding):\n            return self.tokenizer.encode(message)\n        return self.tokenizer.encode(message).flatten().tolist()\n\n    def decode(self, tokens: list[int], /) -> str:\n        if isinstance(self.tokenizer, Encoding):\n            return self.tokenizer.decode(tokens)\n        from torch import IntTensor\n\n        return str(self.tokenizer.decode(IntTensor(tokens)))\n\n    @property\n    def tokenizer(self) -> Union[\"ExLlamaTokenizer\", Encoding]:\n        if self._tokenizer is None:\n            try:\n                if self._tokenizer_type is None:\n                    raise Exception(\"ExllamaTokenizer could not be imported\")\n                model_folder_path = Path(\n                    resolve_model_path_to_posix(\n                        self._model_name,\n                        default_relative_directory=\"llama_models/gptq\",\n                    ),\n                )\n                self._tokenizer = self._tokenizer_type(\n                    (model_folder_path / \"tokenizer.model\").as_posix(),\n                )\n                print(\"Tokenizer loaded:\", self._model_name)\n            except Exception as e:\n                ApiLogger.cwarning(str(e))\n                self._tokenizer = self.fallback_tokenizer\n        return self._tokenizer\n\n    @property\n    def model_name(self) -> str:\n        return self._model_name\n"}
{"type": "source_file", "path": "app/models/chat_models.py", "content": "from dataclasses import asdict, dataclass, field\nfrom datetime import datetime\nfrom enum import Enum\nfrom functools import wraps\nfrom inspect import iscoroutinefunction\nfrom typing import Any, Awaitable, Callable, Tuple\nfrom uuid import uuid4\n\nfrom orjson import dumps as orjson_dumps\nfrom orjson import loads as orjson_loads\nfrom app.common.config import DEFAULT_LLM_MODEL\nfrom app.mixins.enum import EnumMixin\nfrom app.utils.date_utils import UTC\n\nfrom .base_models import MessageHistory, UserChatRoles\nfrom .llms import LLMModels\n\n\nclass ChatRoles(EnumMixin):\n    AI = \"assistant\"\n    SYSTEM = \"system\"\n    USER = \"user\"\n\n\n@dataclass\nclass UserChatProfile:\n    user_id: str\n    chat_room_id: str = field(default_factory=lambda: uuid4().hex)\n    chat_room_name: str = field(default_factory=lambda: UTC.now_isoformat())\n    created_at: int = field(default_factory=lambda: UTC.timestamp())\n    temperature: float = 0.9\n    top_p: float = 1.0\n    presence_penalty: float = 0\n    frequency_penalty: float = 1.1\n\n\n@dataclass\nclass UserChatContext:\n    user_chat_profile: UserChatProfile\n    llm_model: Enum\n    user_message_histories: list[MessageHistory] = field(default_factory=list)\n    ai_message_histories: list[MessageHistory] = field(default_factory=list)\n    system_message_histories: list[MessageHistory] = field(\n        default_factory=list\n    )\n\n    @property\n    def user_message_tokens(self) -> int:\n        return sum([m.tokens for m in self.user_message_histories])\n\n    @property\n    def ai_message_tokens(self) -> int:\n        return sum([m.tokens for m in self.ai_message_histories])\n\n    @property\n    def system_message_tokens(self) -> int:\n        return sum([m.tokens for m in self.system_message_histories])\n\n    @classmethod\n    def parse_stringified_json(cls, stred_json: str) -> \"UserChatContext\":\n        stored: dict = orjson_loads(stred_json)\n        return cls(\n            user_chat_profile=UserChatProfile(**stored[\"user_chat_profile\"]),\n            llm_model=LLMModels.get_member(\n                stored[\"llm_model\"].replace(\".\", \"_\").replace(\"-\", \"_\"),\n            ),\n            user_message_histories=[\n                MessageHistory(**m) for m in stored[\"user_message_histories\"]\n            ],\n            ai_message_histories=[\n                MessageHistory(**m) for m in stored[\"ai_message_histories\"]\n            ],\n            system_message_histories=[\n                MessageHistory(**m) for m in stored[\"system_message_histories\"]\n            ],\n        )\n\n    def json(self) -> dict:\n        return {\n            \"user_chat_profile\": asdict(self.user_chat_profile),\n            \"llm_model\": self.llm_model.name,\n            \"user_message_histories\": [\n                m.__dict__ for m in self.user_message_histories\n            ],\n            \"ai_message_histories\": [\n                m.__dict__ for m in self.ai_message_histories\n            ],\n            \"system_message_histories\": [\n                m.__dict__ for m in self.system_message_histories\n            ],\n        }\n\n    def to_stringified_json(self) -> str:\n        return orjson_dumps(self.json()).decode(\"utf-8\")\n\n    def tokenize(self, message: str) -> list[int]:\n        try:\n            return self.llm_model.value.tokenizer.encode(message)\n        except Exception:\n            return []\n\n    def get_tokens_of(self, message: str) -> int:\n        return len(self.tokenize(message))\n\n    @property\n    def left_tokens(self) -> int:\n        return (\n            self.llm_model.value.max_total_tokens\n            - self.total_tokens\n            - self.llm_model.value.token_margin\n        )\n\n    @property\n    def total_tokens(self) -> int:\n        return (\n            self.user_message_tokens\n            + self.ai_message_tokens\n            + self.system_message_tokens\n            + self.llm_model.value.prefix_tokens\n            + self.llm_model.value.suffix_tokens\n        )\n\n    @property\n    def token_per_request(self) -> int:\n        return self.llm_model.value.max_tokens_per_request\n\n    @property\n    def user_chat_roles(self) -> UserChatRoles:\n        return self.llm_model.value.user_chat_roles\n\n    @property\n    def user_id(self) -> str:\n        return self.user_chat_profile.user_id\n\n    @property\n    def chat_room_id(self) -> str:\n        return self.user_chat_profile.chat_room_id\n\n    @property\n    def chat_room_name(self) -> str:\n        return self.user_chat_profile.chat_room_name\n\n    def __repr__(self) -> str:\n        llm_model: Enum = self.llm_model\n        time_string: str = datetime.strptime(\n            str(self.user_chat_profile.created_at),\n            \"%Y%m%d%H%M%S\",\n        ).strftime(\"%Y-%m-%d %H:%M:%S\")\n        return f\"\"\"# User Info\n- Your ID: `{self.user_id}`\n- This chatroom ID: `{self.chat_room_id}`\n- Your profile created at: `{time_string}`\n- User role: `{self.user_chat_roles.user}`\n- AI role: `{self.user_chat_roles.ai}`\n- System role: `{self.user_chat_roles.system}`\n\n# LLM Info\n- Model Name: `{llm_model.name}`\n- Actual Model Name: `{llm_model.value.name}`\n- Temperature: `{self.user_chat_profile.temperature}`\n- Top P: `{self.user_chat_profile.top_p}`\n- Presence Penalty: `{self.user_chat_profile.presence_penalty}`\n- Frequency Penalty: `{self.user_chat_profile.frequency_penalty}`\n\n# Token Info\n- Maximum Token Limit: `{llm_model.value.max_total_tokens}`\n- User Token Consumed: `{self.user_message_tokens}`\n- AI Token Consumed: `{self.ai_message_tokens}`\n- System Token Consumed: `{self.system_message_tokens}`\n- Total Token Consumed: `{self.total_tokens}`\n- Remaining Tokens: `{self.left_tokens}`\n\n# Message Histories\n- User Message_Histories={self.user_message_histories}\n\n- AI Message Histories={self.ai_message_histories}\n\n- System Message Histories={self.system_message_histories}\n\"\"\"\n\n    @classmethod\n    def construct_default(\n        cls,\n        user_id: str,\n        chat_room_id: str,\n        llm_model: Enum = getattr(\n            LLMModels,\n            DEFAULT_LLM_MODEL,\n            LLMModels.gpt_3_5_turbo,\n        ),\n    ):\n        return cls(\n            user_chat_profile=UserChatProfile(\n                user_id=user_id,\n                chat_room_id=chat_room_id,\n            ),\n            llm_model=llm_model,\n        )\n\n    def reset(self):\n        for k, v in self.construct_default(\n            self.user_id,\n            self.chat_room_id,\n            self.llm_model,\n        ).__dict__.items():\n            setattr(self, k, v)\n\n    def copy_from(self, user_chat_context: \"UserChatContext\") -> None:\n        for k, v in user_chat_context.__dict__.items():\n            setattr(self, k, v)\n\n\nclass ResponseType(str, Enum):\n    SEND_MESSAGE_AND_STOP = \"send_message_and_stop\"\n    HANDLE_USER = \"handle_user\"\n    HANDLE_AI = \"handle_ai\"\n    HANDLE_BOTH = \"handle_both\"\n    DO_NOTHING = \"do_nothing\"\n    REPEAT_COMMAND = \"repeat_command\"\n\n\nclass command_response:\n    @staticmethod\n    def _wrapper(\n        enum_type: ResponseType,\n    ) -> Callable[..., Callable]:\n        def decorator(\n            func: Callable,\n        ) -> (\n            Callable[\n                ...,\n                Tuple[Any, ResponseType],\n            ]\n            | Callable[..., Awaitable[Tuple[Any, ResponseType]]]\n        ):\n            @wraps(func)\n            def sync_wrapper(\n                *args: Any, **kwargs: Any\n            ) -> Tuple[Any, ResponseType]:\n                result = func(*args, **kwargs)\n                return (result, enum_type)\n\n            @wraps(func)\n            async def async_wrapper(\n                *args: Any, **kwargs: Any\n            ) -> Tuple[Any, ResponseType]:\n                result = await func(*args, **kwargs)\n                return (result, enum_type)\n\n            return async_wrapper if iscoroutinefunction(func) else sync_wrapper\n\n        return decorator\n\n    send_message_and_stop = _wrapper(ResponseType.SEND_MESSAGE_AND_STOP)\n    handle_user = _wrapper(ResponseType.HANDLE_USER)\n    handle_ai = _wrapper(ResponseType.HANDLE_AI)\n    handle_both = _wrapper(ResponseType.HANDLE_BOTH)\n    do_nothing = _wrapper(ResponseType.DO_NOTHING)\n    repeat_command = _wrapper(ResponseType.REPEAT_COMMAND)\n"}
{"type": "source_file", "path": "app/utils/api/completion.py", "content": "import logging\nfrom datetime import timedelta\nfrom json import dumps\nfrom re import Pattern, compile\nfrom socket import gaierror\nfrom typing import (\n    Any,\n    AsyncIterator,\n    Awaitable,\n    Callable,\n    Dict,\n    List,\n    Literal,\n    Optional,\n    TypeVar,\n    Union,\n)\n\nfrom aiohttp import ClientResponse, ClientSession, client_exceptions\nfrom openai import error\nfrom orjson import dumps as orjson_dumps\nfrom orjson import loads as orjson_loads\nfrom tenacity import (\n    before_sleep_log,\n    retry,\n    retry_if_exception_type,\n    stop_after_attempt,\n    wait_exponential,\n)\n\nfrom app.common.config import ChatConfig\nfrom app.errors.chat_exceptions import (\n    ChatConnectionException,\n    ChatTooMuchTokenException,\n)\nfrom app.models.completion_models import (\n    ChatCompletion,\n    ChatCompletionChunk,\n    Completion,\n    CompletionChunk,\n)\nfrom app.models.function_calling.base import FunctionCall\nfrom app.utils.chat.text_generations.converter import (\n    make_chat_completion_chunk_from_json,\n    make_chat_completion_from_json,\n    make_completion_chunk_from_json,\n    make_completion_from_json,\n)\n\nT = TypeVar(\"T\")\nTimeUnitType = Union[int, float, timedelta]\nlogger = logging.getLogger(__name__)\napi_regex_pattern: Pattern = compile(\n    r\"data:\\s*({.+?})\\s*\\r?\\n\\s*\\r?\\n\"\n)  # regex pattern to extract json from streaming api response\n\n\ndef _create_retry_decorator(\n    max_retries: int = 6,\n    exponential_base: Union[int, float] = 2,\n    exponential_min_seconds: TimeUnitType = 1,\n    exponential_max_seconds: TimeUnitType = 60,\n    exponential_multiplier: Union[int, float] = 1,\n) -> Callable[[Any], Any]:\n    \"\"\"Create retry decorator with given parameters\n    Wait {exponential_base}^x * {exponential_multiplier} second\n    between each retry starting with {exponential_min_seconds} and\n    up to {exponential_max_seconds} seconds.\"\"\"\n    return retry(\n        reraise=True,\n        stop=stop_after_attempt(max_retries),\n        wait=wait_exponential(\n            multiplier=exponential_multiplier,\n            exp_base=exponential_base,\n            min=exponential_min_seconds,\n            max=exponential_max_seconds,\n        ),\n        retry=(\n            retry_if_exception_type(error.TryAgain)\n            | retry_if_exception_type(error.RateLimitError)\n            | retry_if_exception_type(error.Timeout)\n            | retry_if_exception_type(error.APIError)\n            | retry_if_exception_type(error.APIConnectionError)\n            # | retry_if_exception_type(error.InvalidRequestError)\n            | retry_if_exception_type(error.PermissionError)\n            | retry_if_exception_type(error.ServiceUnavailableError)\n            | retry_if_exception_type(error.InvalidAPIType)\n            | retry_if_exception_type(error.SignatureVerificationError)\n            | retry_if_exception_type(client_exceptions.ClientError)\n            | retry_if_exception_type(gaierror)\n        ),\n        before_sleep=before_sleep_log(logger, logging.WARNING),\n    )\n\n\ndef _decode_function_call(\n    functions: Optional[List[FunctionCall]] = None,\n    function_call: Optional[FunctionCall | Literal[\"auto\", \"none\"]] = None,\n) -> Dict[str, Union[str, Dict[str, str]]]:\n    \"\"\"Decode functions & function_call into dict\"\"\"\n    result: Dict[str, Any] = {}\n    if functions:\n        result[\"functions\"] = [function.to_dict() for function in functions]\n    if function_call is not None:\n        if isinstance(function_call, FunctionCall):\n            result[\"function_call\"] = {\"name\": function_call.to_dict()[\"name\"]}\n        elif isinstance(function_call, str):\n            result[\"function_call\"] = function_call\n    return result\n\n\nasync def _extract_json_from_streaming_response(\n    streaming_response: ClientResponse,\n) -> AsyncIterator[dict]:\n    \"\"\"Extract json from streaming `aiohttp.ClientResponse`\"\"\"\n    stream_buffer: bytes = b\"\"\n    async for stream, end_of_chunk in streaming_response.content.iter_chunks():  # stream from api\n        stream_buffer += stream\n        if not end_of_chunk:\n            continue\n        for match in api_regex_pattern.finditer(stream_buffer.decode(\"utf-8\")):\n            try:\n                json_data: dict = orjson_loads(match.group(1))\n                yield json_data\n                stream_buffer = b\"\"\n            except Exception:\n                continue\n\n\ndef _get_response_exception(\n    rbody: str,\n    rcode: int,\n    resp: dict,\n    rheaders: dict,\n    stream_error: bool = False,\n) -> error.OpenAIError:\n    \"\"\"Return appropriate error from response object\"\"\"\n    try:\n        error_data = resp[\"error\"]\n    except (KeyError, TypeError):\n        raise error.APIError(\n            \"Invalid response object from API: %r (HTTP response code \"\n            \"was %d)\" % (rbody, rcode),\n            rbody,\n            rcode,\n            resp,\n        )\n\n    if \"internal_message\" in error_data:\n        error_data[\"message\"] += \"\\n\\n\" + error_data[\"internal_message\"]\n\n    # Rate limits were previously coded as 400's with code 'rate_limit'\n    if rcode == 429:\n        print(\"- DEBUG: RateLimitError\", error_data.get(\"message\"), flush=True)\n        return error.RateLimitError(\n            error_data.get(\"message\"), rbody, rcode, resp, rheaders\n        )\n    elif rcode in (400, 404, 415):\n        return error.InvalidRequestError(\n            error_data.get(\"message\"),\n            error_data.get(\"param\"),\n            error_data.get(\"code\"),\n            rbody,\n            rcode,\n            resp,\n            rheaders,\n        )\n    elif rcode == 401:\n        return error.AuthenticationError(\n            error_data.get(\"message\"), rbody, rcode, resp, rheaders\n        )\n    elif rcode == 403:\n        return error.PermissionError(\n            error_data.get(\"message\"), rbody, rcode, resp, rheaders\n        )\n    elif rcode == 409:\n        return error.TryAgain(\n            error_data.get(\"message\"), rbody, rcode, resp, rheaders\n        )\n    elif stream_error:\n        # TODO: we will soon attach status codes to stream errors\n        parts = [\n            error_data.get(\"message\"),\n            \"(Error occurred while streaming.)\",\n        ]\n        message = \" \".join([p for p in parts if p is not None])\n        return error.APIError(message, rbody, rcode, resp, rheaders)\n    else:\n        return error.APIError(\n            f\"{error_data.get('message')} {rbody} {rcode} {resp} {rheaders}\",\n            rbody,\n            rcode,\n            resp,\n            rheaders,\n        )\n\n\nasync def _handle_error_response(\n    response: ClientResponse,\n) -> None:\n    \"\"\"Handle error from client response\"\"\"\n    if not response.ok:  # if status code is not 200\n        rbody: str = await response.text()\n        rcode: int = response.status\n        resp: dict = orjson_loads(rbody)\n        rheaders: dict = dict(response.headers)\n        error: Any = resp.get(\"error\")\n        response.release()\n\n        if isinstance(error, dict):\n            error_msg = str(error.get(\"message\"))\n            if \"maximum context length\" in error_msg:\n                print(\n                    \"- DEBUG: ChatTooMuchTokenException\", error_msg, flush=True\n                )\n                raise ChatTooMuchTokenException(msg=\"\")\n        else:\n            error_msg = str(error)\n        raise _get_response_exception(\n            rbody, rcode, resp, rheaders, stream_error=True\n        )\n\n\ndef _make_headers(api_key: Optional[str] = None) -> Dict[str, str]:\n    \"\"\"Make headers to use for completion API\"\"\"\n    if api_key is None:\n        return {\"Content-Type\": \"application/json\"}\n    else:\n        return {\n            \"Content-Type\": \"application/json\",\n            \"Authorization\": f\"Bearer {api_key}\",\n        }\n\n\nasync def acompletion_with_retry(\n    async_callback: Callable[..., Awaitable[T]],\n    /,\n    *args,\n    max_retries: int = 6,\n    **kwargs: Any,\n) -> T:\n    \"\"\"Use tenacity to retry the async completion call.\"\"\"\n\n    @_create_retry_decorator(max_retries)\n    async def _completion_with_retry() -> Any:\n        return await async_callback(*args, **kwargs)\n\n    try:\n        return await _completion_with_retry()\n    except error.OpenAIError as e:\n        raise ChatConnectionException(\n            msg=f\"Completion API error: {e}\"\n        )  # raise exception for connection error\n\n\nasync def request_text_completion(\n    prompt: str,\n    model: str = ChatConfig.global_openai_model,\n    api_base: str = \"https://api.openai.com/v1\",\n    api_key: Optional[str] = None,\n    **kwargs: Any,\n) -> Completion:\n    \"\"\"Request text completion from API with proper retry logic.\n\n    Args:\n        prompt: The prompt to generate completions for.\n        model_name: Name of the model to use. Defaults to ChatConfig.global_openai_model.\n    \"\"\"\n    kwargs.pop(\"stream\", None)\n\n    async def get_text_completion() -> Completion:\n        async for text_completion in acreate_completion(\n            prompt=prompt,\n            model=model,\n            api_base=api_base,\n            api_key=api_key,\n            stream=False,\n            **kwargs,\n        ):\n            return text_completion  # type: ignore\n        raise ChatConnectionException(msg=\"Completion API error\")\n\n    return await acompletion_with_retry(get_text_completion)\n\n\nasync def request_text_completion_with_streaming(\n    prompt: str,\n    model: str = ChatConfig.global_openai_model,\n    api_base: str = \"https://api.openai.com/v1\",\n    api_key: Optional[str] = None,\n    **kwargs: Any,\n) -> AsyncIterator[CompletionChunk]:\n    \"\"\"Request text completion streaming from API with proper retry logic.\n\n    Args:\n        prompt: The prompt to generate completions for.\n        model_name: Name of the model to use. Defaults to ChatConfig.global_openai_model.\n    \"\"\"\n    kwargs.pop(\"stream\", None)\n\n    async def get_text_completion_chunks() -> AsyncIterator[\n        Completion | CompletionChunk\n    ]:\n        return acreate_completion(\n            model=model,\n            prompt=prompt,\n            api_base=api_base,\n            api_key=api_key,\n            stream=True,\n            **kwargs,\n        )\n\n    async for chunk in await acompletion_with_retry(\n        get_text_completion_chunks,\n    ):\n        yield chunk\n\n\nasync def request_chat_completion(\n    messages: List[Dict[str, str]],\n    model: str = ChatConfig.global_openai_model,\n    api_base: str = \"https://api.openai.com/v1\",\n    api_key: Optional[str] = None,\n    functions: Optional[List[FunctionCall]] = None,\n    function_call: Optional[FunctionCall | Literal[\"auto\", \"none\"]] = None,\n    **kwargs: Any,\n) -> ChatCompletion:\n    \"\"\"Request chat completion with streaming from API with proper retry logic.\n    Functions are definitions of functions that can be called in the chat.\n\n    Args:\n        messages: List of messages to send to the chat.\n        model_name: Name of the model to use. Defaults to ChatConfig.global_openai_model.\n        functions:\n            A list of functions the model may generate JSON inputs for.\n            name: str / Required\n                The name of the function to be called. Must be a-z, A-Z, 0-9,\n                or contain underscores and dashes, with a maximum length of 64.\n            description: str / Optional\n                The description of what the function does.\n            parameters: OpenAIFunctionParameter / Optional\n                The parameters the functions accepts.\n        function_call:\n            Controls how the model responds to function calls. \"none\" means the model does not call a function,\n            and responds to the end-user. \"auto\" means the model can pick between an end-user or calling a function.\n            Specifying a particular function via {\"name\": \"my_function\"} forces the model to call that function.\n            \"none\" is the default when no functions are present. \"auto\" is the default if functions are present.\n    \"\"\"\n    kwargs.pop(\"stream\", None)\n\n    async def get_chat_completion() -> ChatCompletion:\n        async for chat_completion in acreate_chat_completion(\n            messages=messages,\n            model=model,\n            api_base=api_base,\n            api_key=api_key,\n            functions=functions,\n            function_call=function_call,\n            stream=False,\n            **kwargs,\n        ):\n            return chat_completion  # type: ignore\n        raise ChatConnectionException(msg=\"Completion API error\")\n\n    return await acompletion_with_retry(get_chat_completion)\n\n\nasync def request_chat_completion_with_streaming(\n    messages: List[Dict[str, str]],\n    model: str = ChatConfig.global_openai_model,\n    api_base: str = \"https://api.openai.com/v1\",\n    api_key: Optional[str] = None,\n    functions: Optional[List[FunctionCall]] = None,\n    function_call: Optional[FunctionCall | Literal[\"auto\", \"none\"]] = None,\n    **kwargs: Any,\n) -> AsyncIterator[ChatCompletionChunk]:\n    \"\"\"Request chat completion with streaming from API with proper retry logic.\n    Functions are definitions of functions that can be called in the chat.\n\n    Args:\n        messages: List of messages to send to the chat.\n        model_name: Name of the model to use. Defaults to ChatConfig.global_openai_model.\n        functions:\n            A list of functions the model may generate JSON inputs for.\n            name: str / Required\n                The name of the function to be called. Must be a-z, A-Z, 0-9,\n                or contain underscores and dashes, with a maximum length of 64.\n            description: str / Optional\n                The description of what the function does.\n            parameters: OpenAIFunctionParameter / Optional\n                The parameters the functions accepts.\n        function_call:\n            Controls how the model responds to function calls. \"none\" means the model does not call a function,\n            and responds to the end-user. \"auto\" means the model can pick between an end-user or calling a function.\n            Specifying a particular function via {\"name\": \"my_function\"} forces the model to call that function.\n            \"none\" is the default when no functions are present. \"auto\" is the default if functions are present.\n    \"\"\"\n    kwargs.pop(\"stream\", None)\n    print(\n        f\"- DEBUG: Sending messages: \\n{dumps(messages, indent=2)}\", flush=True\n    )\n    print(f\"- DEBUG: Sending functions: {functions}\", flush=True)\n    print(f\"- DEBUG: Sending function_call: {function_call}\", flush=True)\n\n    async def get_chat_completion_chunks() -> AsyncIterator[\n        ChatCompletion | ChatCompletionChunk\n    ]:\n        return acreate_chat_completion(\n            model=model,\n            messages=messages,\n            api_base=api_base,\n            api_key=api_key,\n            functions=functions,\n            function_call=function_call,\n            stream=True,\n            **kwargs,\n        )\n\n    async for chunk in await acompletion_with_retry(\n        get_chat_completion_chunks,\n    ):\n        yield chunk  # type: ignore\n\n\nasync def acreate_completion(\n    prompt: str,\n    model: str = ChatConfig.global_openai_model,\n    api_base: str = \"https://api.openai.com/v1/\",\n    api_key: Optional[str] = None,\n    stream: bool = False,\n    **kwargs: Any,\n) -> AsyncIterator[Completion | CompletionChunk]:\n    \"\"\"Create Text Completion Iterator\"\"\"\n    kwargs.pop(\"functions\", None)\n    kwargs.pop(\"function_call\", None)\n\n    url = api_base.rstrip(\"/\") + \"/completions\"\n    headers = _make_headers(api_key=api_key)\n    data = orjson_dumps(\n        kwargs | {\"stream\": stream, \"model\": model, \"prompt\": prompt}\n    )\n    async with ClientSession(\n        timeout=ChatConfig.timeout\n    ) as session:  # initialize client\n        async with session.post(url, headers=headers, data=data) as response:\n            await _handle_error_response(response)\n            if stream:\n                async for json_data in _extract_json_from_streaming_response(\n                    response\n                ):\n                    yield make_completion_chunk_from_json(json_data)\n            else:\n                yield make_completion_from_json(await response.json())\n\n\nasync def acreate_chat_completion(\n    messages: List[Dict[str, str]],\n    model: str = ChatConfig.global_openai_model,\n    api_base: str = \"https://api.openai.com/v1\",\n    api_key: Optional[str] = None,\n    functions: Optional[List[FunctionCall]] = None,\n    function_call: Optional[FunctionCall | Literal[\"auto\", \"none\"]] = None,\n    stream: bool = False,\n    **kwargs: Any,\n) -> AsyncIterator[ChatCompletion | ChatCompletionChunk]:\n    \"\"\"Create Chat Completion Iterator\"\"\"\n    url = api_base.rstrip(\"/\") + \"/chat/completions\"\n    headers = _make_headers(api_key=api_key)\n    data = orjson_dumps(\n        kwargs\n        | {\"stream\": stream, \"model\": model, \"messages\": messages}\n        | _decode_function_call(\n            functions=functions, function_call=function_call\n        )\n    )\n    async with ClientSession(timeout=ChatConfig.timeout) as session:\n        async with session.post(url, headers=headers, data=data) as response:\n            await _handle_error_response(response)\n            if stream:\n                async for json_data in _extract_json_from_streaming_response(\n                    response\n                ):\n                    yield make_chat_completion_chunk_from_json(json_data)\n            else:\n                yield make_chat_completion_from_json(await response.json())\n"}
{"type": "source_file", "path": "app/utils/api/translate.py", "content": "from typing import Callable\nimport httpx\nimport orjson\nfrom app.utils.logger import ApiLogger\nfrom app.common.config import (\n    GOOGLE_TRANSLATE_API_KEY,\n    PAPAGO_CLIENT_ID,\n    PAPAGO_CLIENT_SECRET,\n    RAPID_API_KEY,\n    CUSTOM_TRANSLATE_URL,\n)\n\n\nclass Translator:\n    cached_function: Callable | None = None\n    cached_args: dict = {}\n\n    TRANSLATION_CONFIGS = [\n        {\n            \"function\": \"custom_translate_api\",\n            \"args\": {\"api_url\": CUSTOM_TRANSLATE_URL},\n        },\n        {\n            \"function\": \"deepl_via_rapid_api\",\n            \"args\": {\"api_key\": RAPID_API_KEY},\n        },\n        {\"function\": \"google\", \"args\": {\"api_key\": GOOGLE_TRANSLATE_API_KEY}},\n        {\n            \"function\": \"papago\",\n            \"args\": {\n                \"client_id\": PAPAGO_CLIENT_ID,\n                \"client_secret\": PAPAGO_CLIENT_SECRET,\n            },\n        },\n    ]\n\n    @classmethod\n    async def translate(\n        cls, text: str, src_lang: str, trg_lang: str = \"en\"\n    ) -> str:\n        if cls.cached_function is not None:\n            try:\n                ApiLogger.cinfo(\n                    f\"Using cached translate function: {cls.cached_function}\"\n                )\n                return await cls.cached_function(\n                    text=text,\n                    src_lang=src_lang,\n                    trg_lang=trg_lang,\n                    **cls.cached_args,\n                )\n            except Exception:\n                pass\n\n        for cfg in cls.TRANSLATION_CONFIGS:\n            function = getattr(cls, cfg[\"function\"])\n            args = cfg[\"args\"]\n\n            if all(arg is not None for arg in args.values()):\n                try:\n                    result = await function(\n                        text=text,\n                        src_lang=src_lang,\n                        trg_lang=trg_lang,\n                        **args,\n                    )\n                    ApiLogger.cinfo(\n                        f\"Succeeded to translate using {cfg['function']}\"\n                    )\n                    cls.cached_function = function\n                    cls.cached_args = args\n                    return result\n                except Exception:\n                    ApiLogger.cerror(\n                        f\"Failed to translate using {cfg['function']}\",\n                        exc_info=True,\n                    )\n                    pass\n        raise RuntimeError(\"Failed to translate\")\n\n    @staticmethod\n    async def papago(\n        text: str,\n        src_lang: str,\n        trg_lang: str,\n        client_id: str,\n        client_secret: str,\n    ) -> str:\n        api_url = \"https://openapi.naver.com/v1/papago/n2mt\"\n        headers = {\n            \"Content-Type\": \"application/x-www-form-urlencoded; charset=UTF-8\",\n            \"X-Naver-Client-Id\": client_id,\n            \"X-Naver-Client-Secret\": client_secret,\n        }\n        data = {\"source\": src_lang, \"target\": trg_lang, \"text\": text}\n        # Request papago api using async httpx\n        async with httpx.AsyncClient() as client:\n            response = await client.post(api_url, headers=headers, data=data)\n            return response.json()[\"message\"][\"result\"][\"translatedText\"]\n\n    @staticmethod\n    async def google(\n        text: str,\n        src_lang: str,\n        trg_lang: str,\n        api_key: str,\n        timeout: int = 10,\n    ) -> str:\n        api_url = f\"https://translation.googleapis.com/language/translate/v2?key={api_key}\"\n        data = {\n            \"q\": text,\n            \"source\": src_lang,\n            \"target\": trg_lang,\n            \"format\": \"text\",\n        }\n        async with httpx.AsyncClient(timeout=timeout) as client:\n            response = await client.post(api_url, data=data)\n        return orjson.loads(response.text)[\"data\"][\"translations\"][0][\n            \"translatedText\"\n        ]\n\n    @staticmethod\n    async def deepl_via_rapid_api(\n        text: str,\n        src_lang: str,\n        trg_lang: str,\n        api_key: str,\n        timeout: int = 10,\n    ) -> str:\n        api_host = \"deepl-translator.p.rapidapi.com\"\n        api_url = f\"https://{api_host}/translate\"\n        headers = {\n            \"Content-Type\": \"application/json\",\n            \"X-RapidAPI-Key\": api_key,\n            \"X-RapidAPI-Host\": api_host,\n        }\n        content = orjson.dumps(\n            {\n                \"text\": text,\n                \"source\": src_lang.upper(),\n                \"target\": trg_lang.upper(),\n            }\n        )\n        async with httpx.AsyncClient(timeout=timeout) as client:\n            response = await client.post(\n                api_url, headers=headers, content=content\n            )\n        return orjson.loads(response.text)[\"text\"]\n\n    @staticmethod\n    async def custom_translate_api(\n        text: str,\n        src_lang: str,\n        trg_lang: str,\n        api_url: str,\n        timeout: int = 10,\n    ) -> str:\n        headers = {\n            \"Content-Type\": \"application/json\",\n        }\n        content = orjson.dumps(\n            {\"text\": text, \"target_lang\": trg_lang}\n        )  # source_lang is excluded because we'll use auto-detection\n        async with httpx.AsyncClient(timeout=timeout) as client:\n            response = await client.post(\n                api_url, headers=headers, content=content\n            )\n        return orjson.loads(response.text)[\"data\"]\n"}
{"type": "source_file", "path": "app/database/schemas/deprecated_chatgpt.py", "content": "# from sqlalchemy import (\n#     String,\n#     Enum,\n#     ForeignKey,\n#     Float,\n#     Text,\n# )\n# from sqlalchemy.orm import (\n#     relationship,\n#     Mapped,\n#     mapped_column,\n# )\n# from app.database.schemas.auth import Users\n# from .. import Base\n# from . import Mixin\n\n\n# class ChatRooms(Base, Mixin):\n#     __tablename__ = \"chat_rooms\"\n#     uuid: Mapped[str] = mapped_column(String(length=36), index=True, unique=True)\n#     status: Mapped[str] = mapped_column(Enum(\"active\", \"deleted\", \"blocked\"), default=\"active\")\n#     chat_room_type: Mapped[str] = mapped_column(String(length=20), index=True)\n#     name: Mapped[str] = mapped_column(String(length=20))\n#     description: Mapped[str | None] = mapped_column(String(length=100))\n#     user_id: Mapped[int] = mapped_column(ForeignKey(\"users.id\"))\n#     users: Mapped[\"Users\"] = relationship(back_populates=\"chat_rooms\")\n#     chat_messages: Mapped[\"ChatMessages\"] = relationship(back_populates=\"chat_rooms\", cascade=\"all, delete-orphan\")\n\n\n# class ChatMessages(Base, Mixin):\n#     __tablename__ = \"chat_messages\"\n#     uuid: Mapped[str] = mapped_column(String(length=36), index=True, unique=True)\n#     status: Mapped[str] = mapped_column(Enum(\"active\", \"deleted\", \"blocked\"), default=\"active\")\n#     role: Mapped[str] = mapped_column(String(length=20), default=\"user\")\n#     message: Mapped[str] = mapped_column(Text)\n#     chat_room_id: Mapped[int] = mapped_column(ForeignKey(\"chat_rooms.id\"))\n#     chat_rooms: Mapped[\"ChatRooms\"] = relationship(back_populates=\"chat_messages\")\n#     user_id: Mapped[int] = mapped_column(ForeignKey(\"users.id\"))\n#     users: Mapped[\"Users\"] = relationship(back_populates=\"chat_messages\")\n\n\n# class GptPresets(Base, Mixin):\n#     __tablename__ = \"gpt_presets\"\n#     temperature: Mapped[float] = mapped_column(Float, default=0.9)\n#     top_p: Mapped[float] = mapped_column(Float, default=1.0)\n#     presence_penalty: Mapped[float] = mapped_column(Float, default=0)\n#     frequency_penalty: Mapped[float] = mapped_column(Float, default=0)\n#     user_id: Mapped[int] = mapped_column(ForeignKey(\"users.id\"), unique=True)\n#     users: Mapped[\"Users\"] = relationship(back_populates=\"gpt_presets\")\n"}
{"type": "source_file", "path": "app/database/crud/users.py", "content": "from sqlalchemy import select, exists\nfrom app.database.connection import db\nfrom app.database.schemas.auth import (\n    Users,\n    ApiKeys,\n)\n\n\nasync def is_email_exist(email: str) -> bool:\n    return True if await db.scalar(select(exists().where(Users.email == email))) else False\n\n\nasync def get_me(user_id: int):\n    return await Users.first_filtered_by(id=user_id)\n\n\nasync def is_valid_api_key(access_key: str) -> bool:\n    return True if await db.scalar(select(exists().where(ApiKeys.access_key == access_key))) else False\n\n\nasync def register_new_user(\n    email: str,\n    hashed_password: str,\n    ip_address: str | None,\n) -> Users:\n    return (\n        await Users.add_one(\n            autocommit=True,\n            refresh=True,\n            email=email,\n            password=hashed_password,\n            ip_address=ip_address,\n        )\n        if ip_address\n        else await Users.add_one(\n            autocommit=True,\n            refresh=True,\n            email=email,\n            password=hashed_password,\n        )\n    )\n\n\nasync def find_matched_user(email: str) -> Users:\n    return await Users.first_filtered_by(email=email)\n"}
{"type": "source_file", "path": "app/mixins/enum.py", "content": "from enum import Enum\nfrom typing import Any, Type, TypeVar, Union\n\n\nEnumMixinGeneric = TypeVar(\"EnumMixinGeneric\", bound=\"EnumMixin\")\n\n\nclass EnumMixin(Enum):\n    @classmethod\n    def __init_subclass__(cls) -> None:\n        cls.__dynamic_member_map__: dict[str, Enum] = {}\n        super().__init_subclass__()\n\n    @classmethod\n    @property\n    def static_member_map(\n        cls: Type[EnumMixinGeneric],\n    ) -> dict[str, EnumMixinGeneric]:\n        return cls._member_map_  # type: ignore\n\n    @classmethod\n    @property\n    def dynamic_member_map(\n        cls: Type[EnumMixinGeneric],\n    ) -> dict[str, Enum]:\n        return cls.__dynamic_member_map__\n\n    @classmethod\n    @property\n    def member_map(\n        cls: Type[EnumMixinGeneric],\n    ) -> dict[str, EnumMixinGeneric | Enum]:\n        return cls.static_member_map | cls.dynamic_member_map\n\n    @classmethod\n    @property\n    def member_names(\n        cls: Type[EnumMixinGeneric],\n    ) -> list[str]:\n        return list(cls.member_map.keys())\n\n    @classmethod\n    def add_member(\n        cls: Type[EnumMixinGeneric],\n        name: str,\n        value: Any,\n    ) -> None:\n        # Create a temporary dict for old members and the new member\n        temp_dict = {member.name: member.value for member in cls}\n        temp_dict[name] = value\n\n        # Dynamically create and replace old enum class with updated one.\n        globals()[cls.__name__] = Enum(cls.__name__, temp_dict)\n\n        # Update dynamic members map\n        cls.__dynamic_member_map__[name] = globals()[cls.__name__][name]\n\n    @classmethod\n    def get_name(\n        cls: Type[EnumMixinGeneric],\n        attribute: Union[EnumMixinGeneric, Enum, str],\n    ) -> str:\n        try:\n            if isinstance(attribute, str):  # when attribute is string\n                if attribute in cls.member_names:\n                    return cls.member_map[attribute].name\n                elif attribute.upper() in cls.member_map:\n                    return cls.member_map[attribute.upper()].name\n                return cls.member_map[attribute.lower()].name\n            if (\n                attribute in cls.member_map.values()\n            ):  # when attribute is member\n                return attribute.name\n            else:\n                raise TypeError\n        except (KeyError, TypeError):\n            raise ValueError(\n                f\"attribute must be a string or an instance of {cls.__name__}, got {attribute}\"\n            )\n\n    @classmethod\n    def get_value(\n        cls: Type[EnumMixinGeneric],\n        attribute: Union[EnumMixinGeneric, Enum, str],\n    ) -> Any:\n        try:\n            if isinstance(attribute, str):  # when attribute is string\n                if attribute in cls.member_names:\n                    return cls.member_map[attribute].value\n                elif attribute.upper() in cls.member_map:\n                    return cls.member_map[attribute.upper()].value\n                return cls.member_map[attribute.lower()].value\n            elif (\n                attribute in cls.member_map.values()\n            ):  # when attribute is member\n                return attribute.value\n            else:\n                raise TypeError\n        except (KeyError, TypeError):\n            raise ValueError(\n                f\"attribute must be a string or an instance of {cls.__name__}, got {attribute}\"\n            )\n\n    @classmethod\n    def get_member(\n        cls: Type[EnumMixinGeneric],\n        attribute: Union[EnumMixinGeneric, Enum, str],\n    ) -> EnumMixinGeneric | Enum:\n        try:\n            if isinstance(attribute, str):  # when attribute is string\n                if attribute in cls.member_names:\n                    return cls.member_map[attribute]\n                elif attribute.upper() in cls.member_map:\n                    return cls.member_map[attribute.upper()]\n                return cls.member_map[attribute.lower()]\n            elif (\n                attribute in cls.member_map.values()\n            ):  # when attribute is member\n                return attribute\n            else:\n                raise TypeError\n        except (KeyError, TypeError):\n            raise ValueError(\n                f\"attribute must be a string or an instance of {cls.__name__}, got {attribute}\"\n            )\n\n    @classmethod\n    def get_static_member(\n        cls: Type[EnumMixinGeneric], attribute: Union[EnumMixinGeneric, str]\n    ) -> EnumMixinGeneric:\n        try:\n            if isinstance(attribute, str):  # when attribute is string\n                if attribute in cls.member_names:\n                    return cls.static_member_map[attribute]\n                elif attribute.upper() in cls.static_member_map:\n                    return cls.static_member_map[attribute.upper()]\n                return cls[attribute.lower()]\n            elif isinstance(attribute, cls):  # when attribute is member\n                return attribute\n            else:\n                raise TypeError\n        except (KeyError, TypeError):\n            raise ValueError(\n                f\"attribute must be a string or an instance of {cls.__name__}, got {attribute}\"\n            )\n"}
{"type": "source_file", "path": "app/utils/api/duckduckgo.py", "content": "import logging\nimport re\nfrom collections import deque\nfrom dataclasses import dataclass\nfrom datetime import datetime\nfrom decimal import Decimal\nfrom html import unescape\nfrom itertools import cycle\nfrom random import choice\nfrom time import sleep\nfrom typing import Deque, Dict, Iterator, Optional, Set, Tuple\nfrom urllib.parse import unquote\n\nimport httpx\nfrom lxml import html\n\nlogger = logging.getLogger(__name__)\n\nREGEX_500_IN_URL = re.compile(r\"[0-9]{3}-[0-9]{2}.js\")\nREGEX_STRIP_TAGS = re.compile(\"<.*?>\")\n\nUSERAGENTS = [\n    (\n        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko\"\n        \") Chrome/114.0.0.0 Safari/537.36\"\n    ),\n    (\n        \"Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chro\"\n        \"me/114.0.0.0 Safari/537.36\"\n    ),\n    (\n        \"Mozilla/5.0 (Windows NT 10.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114\"\n        \".0.0.0 Safari/537.36\"\n    ),\n    (\n        \"Mozilla/5.0 (Macintosh; Intel Mac OS X 13_4) AppleWebKit/537.36 (KHTML, like Gec\"\n        \"ko) Chrome/114.0.0.0 Safari/537.36\"\n    ),\n]\n\n\n@dataclass\nclass MapsResult:\n    title: Optional[str] = None\n    address: Optional[str] = None\n    country_code: Optional[str] = None\n    latitude: Optional[str] = None\n    longitude: Optional[str] = None\n    url: Optional[str] = None\n    desc: Optional[str] = None\n    phone: Optional[str] = None\n    image: Optional[str] = None\n    source: Optional[str] = None\n    links: Optional[str] = None\n    hours: Optional[Dict[str, str]] = None\n\n\nclass DDGS:\n    \"\"\"DuckDuckgo_search class to get search results from duckduckgo.com\"\"\"\n\n    def __init__(\n        self,\n        headers=None,\n        proxies=None,\n        timeout=10,\n    ) -> None:\n        if headers is None:\n            headers = {\n                \"User-Agent\": choice(USERAGENTS),\n                \"Referer\": \"https://duckduckgo.com/\",\n            }\n        self._client = httpx.Client(\n            headers=headers,\n            proxies=proxies,\n            timeout=timeout,\n            http2=True,\n        )\n\n    def __enter__(self) -> \"DDGS\":\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb) -> None:\n        self._client.close()\n\n    def _get_url(\n        self, method: str, url: str, **kwargs\n    ) -> Optional[httpx._models.Response]:\n        for i in range(3):\n            try:\n                resp = self._client.request(\n                    method, url, follow_redirects=True, **kwargs\n                )\n                if self._is_500_in_url(str(resp.url)) or resp.status_code == 202:\n                    raise httpx._exceptions.HTTPError(\"\")\n                resp.raise_for_status()\n                if resp.status_code == 200:\n                    return resp\n            except Exception as ex:\n                logger.warning(f\"_get_url() {url} {type(ex).__name__} {ex}\")\n                if i >= 2 or \"418\" in str(ex):\n                    raise ex\n            sleep(3)\n        return None\n\n    def _get_vqd(self, keywords: str) -> Optional[str]:\n        \"\"\"Get vqd value for a search query.\"\"\"\n        resp = self._get_url(\"POST\", \"https://duckduckgo.com\", data={\"q\": keywords})\n        if resp:\n            for c1, c2 in (\n                (b'vqd=\"', b'\"'),\n                (b\"vqd=\", b\"&\"),\n                (b\"vqd='\", b\"'\"),\n            ):\n                try:\n                    start = resp.content.index(c1) + len(c1)\n                    end = resp.content.index(c2, start)\n                    return resp.content[start:end].decode()\n                except ValueError:\n                    logger.warning(f\"_get_vqd() keywords={keywords} vqd not found\")\n        return None\n\n    def _is_500_in_url(self, url: str) -> bool:\n        \"\"\"something like '506-00.js' inside the url\"\"\"\n        return bool(REGEX_500_IN_URL.search(url))\n\n    def _normalize(self, raw_html: Optional[str] = None) -> str:\n        \"\"\"strip HTML tags\"\"\"\n        if raw_html:\n            return unescape(re.sub(REGEX_STRIP_TAGS, \"\", raw_html))\n        return \"\"\n\n    def _normalize_url(self, url: Optional[str] = None) -> str:\n        \"\"\"unquote url and replace spaces with '+'\"\"\"\n        if url:\n            return unquote(url).replace(\" \", \"+\")\n        return \"\"\n\n    def text(\n        self,\n        keywords: str,\n        region: str = \"wt-wt\",\n        safesearch: str = \"moderate\",\n        timelimit: Optional[str] = None,\n        backend: str = \"api\",\n    ) -> Iterator[Dict[str, Optional[str]]]:\n        \"\"\"DuckDuckGo text search generator. Query params: https://duckduckgo.com/params\n\n        Args:\n            keywords: keywords for query.\n            region: wt-wt, us-en, uk-en, ru-ru, etc. Defaults to \"wt-wt\".\n            safesearch: on, moderate, off. Defaults to \"moderate\".\n            timelimit: d, w, m, y. Defaults to None.\n            backend: api, html, lite. Defaults to api.\n                api - collect data from https://duckduckgo.com,\n                html - collect data from https://html.duckduckgo.com,\n                lite - collect data from https://lite.duckduckgo.com.\n        Yields:\n            dict with search results.\n\n        \"\"\"\n        if backend == \"api\":\n            yield from self._text_api(keywords, region, safesearch, timelimit)\n        elif backend == \"html\":\n            yield from self._text_html(keywords, region, safesearch, timelimit)\n        elif backend == \"lite\":\n            yield from self._text_lite(keywords, region, timelimit)\n\n    def _text_api(\n        self,\n        keywords: str,\n        region: str = \"wt-wt\",\n        safesearch: str = \"moderate\",\n        timelimit: Optional[str] = None,\n    ) -> Iterator[Dict[str, Optional[str]]]:\n        \"\"\"DuckDuckGo text search generator. Query params: https://duckduckgo.com/params\n\n        Args:\n            keywords: keywords for query.\n            region: wt-wt, us-en, uk-en, ru-ru, etc. Defaults to \"wt-wt\".\n            safesearch: on, moderate, off. Defaults to \"moderate\".\n            timelimit: d, w, m, y. Defaults to None.\n\n        Yields:\n            dict with search results.\n\n        \"\"\"\n        assert keywords, \"keywords is mandatory\"\n\n        vqd = self._get_vqd(keywords)\n        assert vqd, \"error in getting vqd\"\n\n        payload = {\n            \"q\": keywords,  #\n            \"kl\": region,\n            \"l\": region,\n            \"s\": 0,\n            \"df\": timelimit,\n            \"vqd\": vqd,\n            \"o\": \"json\",\n        }\n        if safesearch == \"off\":\n            # payload[\"p\"] = '-2'\n            payload[\"ex\"] = \"-2\"\n        elif safesearch == \"moderate\":\n            payload[\"p\"] = \"\"\n            payload[\"sp\"] = \"0\"\n            payload[\"ex\"] = \"-1\"\n        elif safesearch == \"on\":  # strict\n            payload[\"sp\"] = \"0\"\n            payload[\"p\"] = \"1\"\n\n        cache = set()\n        for s in (\"0\", \"20\", \"70\", \"120\"):\n            payload[\"s\"] = s\n            resp = self._get_url(\n                \"GET\", \"https://links.duckduckgo.com/d.js\", params=payload\n            )\n            if resp is None:\n                break\n            try:\n                page_data = resp.json().get(\"results\", None)\n            except Exception:\n                break\n            if page_data is None:\n                break\n\n            result_exists = False\n            for row in page_data:\n                href = row.get(\"u\", None)\n                if (\n                    href\n                    and href not in cache\n                    and href != f\"http://www.google.com/search?q={keywords}\"\n                ):\n                    cache.add(href)\n                    body = self._normalize(row[\"a\"])\n                    if body:\n                        result_exists = True\n                        yield {\n                            \"title\": self._normalize(row[\"t\"]),\n                            \"href\": self._normalize_url(href),\n                            \"body\": body,\n                        }\n            if result_exists is False:\n                break\n\n    def _text_html(\n        self,\n        keywords: str,\n        region: str = \"wt-wt\",\n        safesearch: str = \"moderate\",\n        timelimit: Optional[str] = None,\n    ) -> Iterator[Dict[str, Optional[str]]]:\n        \"\"\"DuckDuckGo text search generator. Query params: https://duckduckgo.com/params\n\n        Args:\n            keywords: keywords for query.\n            region: wt-wt, us-en, uk-en, ru-ru, etc. Defaults to \"wt-wt\".\n            safesearch: on, moderate, off. Defaults to \"moderate\".\n            timelimit: d, w, m, y. Defaults to None.\n\n        Yields:\n            dict with search results.\n\n        \"\"\"\n        assert keywords, \"keywords is mandatory\"\n\n        safesearch_base = {\"on\": 1, \"moderate\": -1, \"off\": -2}\n        payload = {\n            \"q\": keywords,\n            \"kl\": region,\n            \"p\": safesearch_base[safesearch.lower()],\n            \"df\": timelimit,\n        }\n        cache: Set[str] = set()\n        for _ in range(10):\n            resp = self._get_url(\n                \"POST\", \"https://html.duckduckgo.com/html\", data=payload\n            )\n            if resp is None:\n                break\n\n            tree = html.fromstring(resp.content)\n            if tree.xpath('//div[@class=\"no-results\"]/text()'):\n                return\n\n            result_exists = False\n            for e in tree.xpath('//div[contains(@class, \"results_links\")]'):\n                href = e.xpath('.//a[contains(@class, \"result__a\")]/@href')\n                href = href[0] if href else None\n                if (\n                    href\n                    and href not in cache\n                    and href != f\"http://www.google.com/search?q={keywords}\"\n                ):\n                    cache.add(href)\n                    title = e.xpath('.//a[contains(@class, \"result__a\")]/text()')\n                    body = e.xpath('.//a[contains(@class, \"result__snippet\")]//text()')\n                    result_exists = True\n                    yield {\n                        \"title\": self._normalize(title[0]) if title else None,\n                        \"href\": self._normalize_url(href),\n                        \"body\": self._normalize(\"\".join(body)) if body else None,\n                    }\n\n            next_page = tree.xpath('.//div[@class=\"nav-link\"]')\n            next_page = next_page[-1] if next_page else None\n            if next_page is None or result_exists is False:\n                return\n\n            names = next_page.xpath('.//input[@type=\"hidden\"]/@name')\n            values = next_page.xpath('.//input[@type=\"hidden\"]/@value')\n            payload = {n: v for n, v in zip(names, values)}\n            sleep(0.75)\n\n    def _text_lite(\n        self,\n        keywords: str,\n        region: str = \"wt-wt\",\n        timelimit: Optional[str] = None,\n    ) -> Iterator[Dict[str, Optional[str]]]:\n        \"\"\"DuckDuckGo text search generator. Query params: https://duckduckgo.com/params\n\n        Args:\n            keywords: keywords for query.\n            region: wt-wt, us-en, uk-en, ru-ru, etc. Defaults to \"wt-wt\".\n            timelimit: d, w, m, y. Defaults to None.\n\n        Yields:\n            dict with search results.\n\n        \"\"\"\n        assert keywords, \"keywords is mandatory\"\n\n        payload = {\n            \"q\": keywords,\n            \"kl\": region,\n            \"df\": timelimit,\n        }\n        cache: Set[str] = set()\n        for s in (\"0\", \"20\", \"70\", \"120\"):\n            payload[\"s\"] = s\n            resp = self._get_url(\n                \"POST\", \"https://lite.duckduckgo.com/lite/\", data=payload\n            )\n            if resp is None:\n                break\n\n            if b\"No more results.\" in resp.content:\n                return\n\n            tree = html.fromstring(resp.content)\n\n            result_exists = False\n            data = zip(cycle(range(1, 5)), tree.xpath(\"//table[last()]//tr\"))\n            title, href, body = None, None, None\n            for i, e in data:\n                if i == 1:\n                    href = e.xpath(\".//a//@href\")\n                    href = href[0] if href else None\n                    if (\n                        href is None\n                        or href in cache\n                        or href == f\"http://www.google.com/search?q={keywords}\"\n                    ):\n                        [next(data, None) for _ in range(3)]  # skip block(i=1,2,3,4)\n                    else:\n                        cache.add(href)\n                        title = e.xpath(\".//a//text()\")[0]\n                elif i == 2:\n                    body = e.xpath(\".//td[@class='result-snippet']//text()\")\n                    body = \"\".join(body).strip()\n                elif i == 3:\n                    result_exists = True\n                    yield {\n                        \"title\": self._normalize(title),\n                        \"href\": self._normalize_url(href),\n                        \"body\": self._normalize(body),\n                    }\n            if result_exists is False:\n                break\n            sleep(0.75)\n\n    def images(\n        self,\n        keywords: str,\n        region: str = \"wt-wt\",\n        safesearch: str = \"moderate\",\n        timelimit: Optional[str] = None,\n        size: Optional[str] = None,\n        color: Optional[str] = None,\n        type_image: Optional[str] = None,\n        layout: Optional[str] = None,\n        license_image: Optional[str] = None,\n    ) -> Iterator[Dict[str, Optional[str]]]:\n        \"\"\"DuckDuckGo images search. Query params: https://duckduckgo.com/params\n\n        Args:\n            keywords: keywords for query.\n            region: wt-wt, us-en, uk-en, ru-ru, etc. Defaults to \"wt-wt\".\n            safesearch: on, moderate, off. Defaults to \"moderate\".\n            timelimit: Day, Week, Month, Year. Defaults to None.\n            size: Small, Medium, Large, Wallpaper. Defaults to None.\n            color: color, Monochrome, Red, Orange, Yellow, Green, Blue,\n                Purple, Pink, Brown, Black, Gray, Teal, White. Defaults to None.\n            type_image: photo, clipart, gif, transparent, line.\n                Defaults to None.\n            layout: Square, Tall, Wide. Defaults to None.\n            license_image: any (All Creative Commons), Public (PublicDomain),\n                Share (Free to Share and Use), ShareCommercially (Free to Share and Use Commercially),\n                Modify (Free to Modify, Share, and Use), ModifyCommercially (Free to Modify, Share, and\n                Use Commercially). Defaults to None.\n\n        Yields:\n            dict with image search results.\n\n        \"\"\"\n        assert keywords, \"keywords is mandatory\"\n\n        vqd = self._get_vqd(keywords)\n        assert vqd, \"error in getting vqd\"\n\n        safesearch_base = {\"on\": 1, \"moderate\": 1, \"off\": -1}\n        timelimit = f\"time:{timelimit}\" if timelimit else \"\"\n        size = f\"size:{size}\" if size else \"\"\n        color = f\"color:{color}\" if color else \"\"\n        type_image = f\"type:{type_image}\" if type_image else \"\"\n        layout = f\"layout:{layout}\" if layout else \"\"\n        license_image = f\"license:{license_image}\" if license_image else \"\"\n        payload = {\n            \"l\": region,\n            \"o\": \"json\",\n            \"s\": 0,\n            \"q\": keywords,\n            \"vqd\": vqd,\n            \"f\": f\"{timelimit},{size},{color},{type_image},{layout},{license_image}\",\n            \"p\": safesearch_base[safesearch.lower()],\n        }\n\n        cache = set()\n        for _ in range(10):\n            resp = self._get_url(\"GET\", \"https://duckduckgo.com/i.js\", params=payload)\n            if resp is None:\n                break\n            try:\n                resp_json = resp.json()\n            except Exception:\n                break\n            page_data = resp_json.get(\"results\", None)\n            if page_data is None:\n                break\n\n            result_exists = False\n            for row in page_data:\n                image_url = row.get(\"image\", None)\n                if image_url and image_url not in cache:\n                    cache.add(image_url)\n                    result_exists = True\n                    yield {\n                        \"title\": row[\"title\"],\n                        \"image\": self._normalize_url(image_url),\n                        \"thumbnail\": self._normalize_url(row[\"thumbnail\"]),\n                        \"url\": self._normalize_url(row[\"url\"]),\n                        \"height\": row[\"height\"],\n                        \"width\": row[\"width\"],\n                        \"source\": row[\"source\"],\n                    }\n            next = resp_json.get(\"next\", None)\n            if next:\n                payload[\"s\"] = next.split(\"s=\")[-1].split(\"&\")[0]\n            if next is None or result_exists is False:\n                break\n\n    def videos(\n        self,\n        keywords: str,\n        region: str = \"wt-wt\",\n        safesearch: str = \"moderate\",\n        timelimit: Optional[str] = None,\n        resolution: Optional[str] = None,\n        duration: Optional[str] = None,\n        license_videos: Optional[str] = None,\n    ) -> Iterator[Dict[str, Optional[str]]]:\n        \"\"\"DuckDuckGo videos search. Query params: https://duckduckgo.com/params\n\n        Args:\n            keywords: keywords for query.\n            region: wt-wt, us-en, uk-en, ru-ru, etc. Defaults to \"wt-wt\".\n            safesearch: on, moderate, off. Defaults to \"moderate\".\n            timelimit: d, w, m. Defaults to None.\n            resolution: high, standart. Defaults to None.\n            duration: short, medium, long. Defaults to None.\n            license_videos: creativeCommon, youtube. Defaults to None.\n\n        Yields:\n            dict with videos search results\n\n        \"\"\"\n        assert keywords, \"keywords is mandatory\"\n\n        vqd = self._get_vqd(keywords)\n        assert vqd, \"error in getting vqd\"\n\n        safesearch_base = {\"on\": 1, \"moderate\": -1, \"off\": -2}\n        timelimit = f\"publishedAfter:{timelimit}\" if timelimit else \"\"\n        resolution = f\"videoDefinition:{resolution}\" if resolution else \"\"\n        duration = f\"videoDuration:{duration}\" if duration else \"\"\n        license_videos = f\"videoLicense:{license_videos}\" if license_videos else \"\"\n        payload = {\n            \"l\": region,\n            \"o\": \"json\",\n            \"s\": 0,\n            \"q\": keywords,\n            \"vqd\": vqd,\n            \"f\": f\"{timelimit},{resolution},{duration},{license_videos}\",\n            \"p\": safesearch_base[safesearch.lower()],\n        }\n\n        cache = set()\n        for _ in range(10):\n            resp = self._get_url(\"GET\", \"https://duckduckgo.com/v.js\", params=payload)\n            if resp is None:\n                break\n            try:\n                resp_json = resp.json()\n            except Exception:\n                break\n            page_data = resp_json.get(\"results\", None)\n            if page_data is None:\n                break\n\n            result_exists = False\n            for row in page_data:\n                if row[\"content\"] not in cache:\n                    cache.add(row[\"content\"])\n                    result_exists = True\n                    yield row\n            next = resp_json.get(\"next\", None)\n            if next:\n                payload[\"s\"] = next.split(\"s=\")[-1].split(\"&\")[0]\n            if not result_exists or not next:\n                break\n\n    def news(\n        self,\n        keywords: str,\n        region: str = \"wt-wt\",\n        safesearch: str = \"moderate\",\n        timelimit: Optional[str] = None,\n    ) -> Iterator[Dict[str, Optional[str]]]:\n        \"\"\"DuckDuckGo news search. Query params: https://duckduckgo.com/params\n\n        Args:\n            keywords: keywords for query.\n            region: wt-wt, us-en, uk-en, ru-ru, etc. Defaults to \"wt-wt\".\n            safesearch: on, moderate, off. Defaults to \"moderate\".\n            timelimit: d, w, m. Defaults to None.\n\n        Yields:\n            dict with news search results.\n\n        \"\"\"\n        assert keywords, \"keywords is mandatory\"\n\n        vqd = self._get_vqd(keywords)\n        assert vqd, \"error in getting vqd\"\n\n        safesearch_base = {\"on\": 1, \"moderate\": -1, \"off\": -2}\n        payload = {\n            \"l\": region,\n            \"o\": \"json\",\n            \"noamp\": \"1\",\n            \"q\": keywords,\n            \"vqd\": vqd,\n            \"p\": safesearch_base[safesearch.lower()],\n            \"df\": timelimit,\n            \"s\": 0,\n        }\n\n        cache = set()\n        for _ in range(10):\n            resp = self._get_url(\n                \"GET\", \"https://duckduckgo.com/news.js\", params=payload\n            )\n            if resp is None:\n                break\n            try:\n                resp_json = resp.json()\n            except Exception:\n                break\n            page_data = resp_json.get(\"results\", None)\n            if page_data is None:\n                break\n\n            result_exists = False\n            for row in page_data:\n                if row[\"url\"] not in cache:\n                    cache.add(row[\"url\"])\n                    image_url = row.get(\"image\", None)\n                    result_exists = True\n                    yield {\n                        \"date\": datetime.utcfromtimestamp(row[\"date\"]).isoformat(),\n                        \"title\": row[\"title\"],\n                        \"body\": self._normalize(row[\"excerpt\"]),\n                        \"url\": self._normalize_url(row[\"url\"]),\n                        \"image\": self._normalize_url(image_url) if image_url else None,\n                        \"source\": row[\"source\"],\n                    }\n            next = resp_json.get(\"next\", None)\n            if next:\n                payload[\"s\"] = next.split(\"s=\")[-1].split(\"&\")[0]\n            if not result_exists or not next:\n                break\n\n    def answers(\n        self,\n        keywords: str,\n    ) -> Iterator[Dict[str, Optional[str]]]:\n        \"\"\"DuckDuckGo instant answers. Query params: https://duckduckgo.com/params\n\n        Args:\n            keywords: keywords for query.\n\n        Yields:\n            dict with instant answers results.\n\n        \"\"\"\n        assert keywords, \"keywords is mandatory\"\n\n        payload = {\n            \"q\": f\"what is {keywords}\",\n            \"format\": \"json\",\n        }\n\n        resp = self._get_url(\"GET\", \"https://api.duckduckgo.com/\", params=payload)\n        if resp is None:\n            return None\n        try:\n            page_data = resp.json()\n        except Exception:\n            page_data = None\n\n        if page_data:\n            answer = page_data.get(\"AbstractText\", None)\n            url = page_data.get(\"AbstractURL\", None)\n            if answer:\n                yield {\n                    \"icon\": None,\n                    \"text\": answer,\n                    \"topic\": None,\n                    \"url\": url,\n                }\n\n        # related:\n        payload = {\n            \"q\": f\"{keywords}\",\n            \"format\": \"json\",\n        }\n        resp = self._get_url(\"GET\", \"https://api.duckduckgo.com/\", params=payload)\n        if resp is None:\n            return None\n        try:\n            page_data = resp.json().get(\"RelatedTopics\", None)\n        except Exception:\n            page_data = None\n\n        if page_data:\n            for i, row in enumerate(page_data):\n                topic = row.get(\"Name\", None)\n                if not topic:\n                    icon = row[\"Icon\"].get(\"URL\", None)\n                    yield {\n                        \"icon\": f\"https://duckduckgo.com{icon}\" if icon else None,\n                        \"text\": row[\"Text\"],\n                        \"topic\": None,\n                        \"url\": row[\"FirstURL\"],\n                    }\n                else:\n                    for subrow in row[\"Topics\"]:\n                        icon = subrow[\"Icon\"].get(\"URL\", None)\n                        yield {\n                            \"icon\": f\"https://duckduckgo.com{icon}\" if icon else None,\n                            \"text\": subrow[\"Text\"],\n                            \"topic\": topic,\n                            \"url\": subrow[\"FirstURL\"],\n                        }\n\n    def suggestions(\n        self,\n        keywords: str,\n        region: str = \"wt-wt\",\n    ) -> Iterator[Dict[str, Optional[str]]]:\n        \"\"\"DuckDuckGo suggestions. Query params: https://duckduckgo.com/params\n\n        Args:\n            keywords: keywords for query.\n            region: wt-wt, us-en, uk-en, ru-ru, etc. Defaults to \"wt-wt\".\n\n        Yields:\n            dict with suggestions results.\n        \"\"\"\n\n        assert keywords, \"keywords is mandatory\"\n\n        payload = {\n            \"q\": keywords,\n            \"kl\": region,\n        }\n        resp = self._get_url(\"GET\", \"https://duckduckgo.com/ac\", params=payload)\n        if resp is None:\n            return None\n        try:\n            page_data = resp.json()\n            for r in page_data:\n                yield r\n        except Exception:\n            pass\n\n    def maps(\n        self,\n        keywords: str,\n        place: Optional[str] = None,\n        street: Optional[str] = None,\n        city: Optional[str] = None,\n        county: Optional[str] = None,\n        state: Optional[str] = None,\n        country: Optional[str] = None,\n        postalcode: Optional[str] = None,\n        latitude: Optional[str] = None,\n        longitude: Optional[str] = None,\n        radius: int = 0,\n    ) -> Iterator[Dict[str, Optional[str]]]:\n        \"\"\"DuckDuckGo maps search. Query params: https://duckduckgo.com/params\n\n        Args:\n            keywords: keywords for query\n            place: if set, the other parameters are not used. Defaults to None.\n            street: house number/street. Defaults to None.\n            city: city of search. Defaults to None.\n            county: county of search. Defaults to None.\n            state: state of search. Defaults to None.\n            country: country of search. Defaults to None.\n            postalcode: postalcode of search. Defaults to None.\n            latitude: geographic coordinate (north–south position). Defaults to None.\n            longitude: geographic coordinate (east–west position); if latitude and\n                longitude are set, the other parameters are not used. Defaults to None.\n            radius: expand the search square by the distance in kilometers. Defaults to 0.\n\n        Yields:\n            dict with maps search results\n        \"\"\"\n\n        assert keywords, \"keywords is mandatory\"\n\n        vqd = self._get_vqd(keywords)\n        assert vqd, \"error in getting vqd\"\n\n        # if longitude and latitude are specified, skip the request about bbox to the nominatim api\n        if latitude and longitude:\n            lat_t = Decimal(latitude.replace(\",\", \".\"))\n            lat_b = Decimal(latitude.replace(\",\", \".\"))\n            lon_l = Decimal(longitude.replace(\",\", \".\"))\n            lon_r = Decimal(longitude.replace(\",\", \".\"))\n            if radius == 0:\n                radius = 1\n        # otherwise request about bbox to nominatim api\n        else:\n            if place:\n                params: Dict[str, Optional[str]] = {\n                    \"q\": place,\n                    \"polygon_geojson\": \"0\",\n                    \"format\": \"jsonv2\",\n                }\n            else:\n                params = {\n                    \"street\": street,\n                    \"city\": city,\n                    \"county\": county,\n                    \"state\": state,\n                    \"country\": country,\n                    \"postalcode\": postalcode,\n                    \"polygon_geojson\": \"0\",\n                    \"format\": \"jsonv2\",\n                }\n            try:\n                resp = self._get_url(\n                    \"GET\",\n                    \"https://nominatim.openstreetmap.org/search.php\",\n                    params=params,\n                )\n                if resp is None:\n                    return None\n\n                coordinates = resp.json()[0][\"boundingbox\"]\n                lat_t, lon_l = Decimal(coordinates[1]), Decimal(coordinates[2])\n                lat_b, lon_r = Decimal(coordinates[0]), Decimal(coordinates[3])\n            except Exception as ex:\n                logger.debug(f\"ddg_maps() keywords={keywords} {type(ex).__name__} {ex}\")\n                return\n\n        # if a radius is specified, expand the search square\n        lat_t += Decimal(radius) * Decimal(0.008983)\n        lat_b -= Decimal(radius) * Decimal(0.008983)\n        lon_l -= Decimal(radius) * Decimal(0.008983)\n        lon_r += Decimal(radius) * Decimal(0.008983)\n        logging.debug(f\"bbox coordinates\\n{lat_t} {lon_l}\\n{lat_b} {lon_r}\")\n\n        # сreate a queue of search squares (bboxes)\n        work_bboxes: Deque[Tuple[Decimal, Decimal, Decimal, Decimal]] = deque()\n        work_bboxes.append((lat_t, lon_l, lat_b, lon_r))\n\n        # bbox iterate\n        cache = set()\n        stop_find = False\n        while work_bboxes and not stop_find:\n            lat_t, lon_l, lat_b, lon_r = work_bboxes.pop()\n            params = {\n                \"q\": keywords,\n                \"vqd\": vqd,\n                \"tg\": \"maps_places\",\n                \"rt\": \"D\",\n                \"mkexp\": \"b\",\n                \"wiki_info\": \"1\",\n                \"is_requery\": \"1\",\n                \"bbox_tl\": f\"{lat_t},{lon_l}\",\n                \"bbox_br\": f\"{lat_b},{lon_r}\",\n                \"strict_bbox\": \"1\",\n            }\n            resp = self._get_url(\n                \"GET\", \"https://duckduckgo.com/local.js\", params=params\n            )\n            if resp is None:\n                break\n            try:\n                page_data = resp.json().get(\"results\", [])\n            except Exception:\n                break\n            if page_data is None:\n                break\n\n            for res in page_data:\n                result = MapsResult()\n                result.title = res[\"name\"]\n                result.address = res[\"address\"]\n                if f\"{result.title} {result.address}\" in cache:\n                    continue\n                else:\n                    cache.add(f\"{result.title} {result.address}\")\n                    result.country_code = res[\"country_code\"]\n                    result.url = self._normalize_url(res[\"website\"])\n                    result.phone = res[\"phone\"]\n                    result.latitude = res[\"coordinates\"][\"latitude\"]\n                    result.longitude = res[\"coordinates\"][\"longitude\"]\n                    result.source = self._normalize_url(res[\"url\"])\n                    if res[\"embed\"]:\n                        result.image = res[\"embed\"].get(\"image\", \"\")\n                        result.links = res[\"embed\"].get(\"third_party_links\", \"\")\n                        result.desc = res[\"embed\"].get(\"description\", \"\")\n                    result.hours = res[\"hours\"]\n                    yield result.__dict__\n\n            # divide the square into 4 parts and add to the queue\n            if len(page_data) >= 15:\n                lat_middle = (lat_t + lat_b) / 2\n                lon_middle = (lon_l + lon_r) / 2\n                bbox1 = (lat_t, lon_l, lat_middle, lon_middle)\n                bbox2 = (lat_t, lon_middle, lat_middle, lon_r)\n                bbox3 = (lat_middle, lon_l, lat_b, lon_middle)\n                bbox4 = (lat_middle, lon_middle, lat_b, lon_r)\n                work_bboxes.extendleft([bbox1, bbox2, bbox3, bbox4])\n\n    def translate(\n        self,\n        keywords: str,\n        from_: Optional[str] = None,\n        to: str = \"en\",\n    ) -> Optional[Dict[str, Optional[str]]]:\n        \"\"\"DuckDuckGo translate\n\n        Args:\n            keywords: string or a list of strings to translate\n            from_: translate from (defaults automatically). Defaults to None.\n            to: what language to translate. Defaults to \"en\".\n\n        Returns:\n            dict with translated keywords.\n        \"\"\"\n\n        assert keywords, \"keywords is mandatory\"\n\n        vqd = self._get_vqd(\"translate\")\n        assert vqd, \"error in getting vqd\"\n\n        payload = {\n            \"vqd\": vqd,\n            \"query\": \"translate\",\n            \"to\": to,\n        }\n        if from_:\n            payload[\"from\"] = from_\n\n        resp = self._get_url(\n            \"POST\",\n            \"https://duckduckgo.com/translation.js\",\n            params=payload,\n            data=keywords.encode(),\n        )\n        if resp is None:\n            return None\n        try:\n            page_data = resp.json()\n            page_data[\"original\"] = keywords\n        except Exception:\n            page_data = None\n        return page_data\n"}
{"type": "source_file", "path": "app/routers/v1.py", "content": "\"\"\"V1 Endpoints for Local Llama API\nUse same format as OpenAI API\"\"\"\n\n\nfrom collections import deque\nfrom functools import partial\nfrom typing import TYPE_CHECKING, AsyncGenerator, Iterator, Optional, Union\n\nimport anyio\nfrom anyio.streams.memory import MemoryObjectSendStream\nfrom fastapi import APIRouter, Depends, Request\nfrom orjson import dumps\nfrom pydantic import create_model_from_typeddict\nfrom sse_starlette.sse import EventSourceResponse\nfrom starlette.concurrency import iterate_in_threadpool, run_in_threadpool\n\nfrom app.models.base_models import (\n    CreateChatCompletionRequest,\n    CreateCompletionRequest,\n    CreateEmbeddingRequest,\n)\nfrom app.models.completion_models import (\n    ChatCompletion,\n    ChatCompletionChunk,\n    Completion,\n    CompletionChunk,\n    Embedding,\n    ModelList,\n)\nfrom app.utils.errors import RouteErrorHandler\nfrom app.utils.logger import ApiLogger\nfrom app.utils.module_reloader import ModuleReloader\nfrom app.utils.system import free_memory_of_first_item_from_container\n\nlogger = ApiLogger(\"||v1||\")\n\n\n# Importing llama.cpp\ntry:\n    from app.utils.chat.text_generations.llama_cpp import (\n        LlamaCppCompletionGenerator,\n    )\n\n    logger.info(\"🦙 Successfully imported llama.cpp module!\")\nexcept Exception as e:\n    logger.warning(\"Llama.cpp import error: \" + str(e))\n    LlamaCppCompletionGenerator = str(e)  # Import error message\n\n\n# Importing exllama\ntry:\n    from app.utils.chat.text_generations.exllama import (\n        ExllamaCompletionGenerator,\n    )\n\n    logger.info(\"🦙 Successfully imported exllama module!\")\nexcept Exception as e:\n    logger.exception(\"Exllama package import error: \" + str(e))\n    ExllamaCompletionGenerator = str(e)  # Import error message\n\n\n# Importing embeddings (Pytorch + Transformer)\ntry:\n    from app.utils.chat.embeddings.transformer import (\n        TransformerEmbeddingGenerator,\n    )\n\n    logger.info(\n        \"🦙 Successfully imported embeddings(Pytorch + Transformer) module!\"\n    )\nexcept Exception as e:\n    logger.warning(\"Transformer embedding import error: \" + str(e))\n    TransformerEmbeddingGenerator = str(e)  # Import error message\n\n\n# Importing embeddings (Tensorflow + Sentence Encoder)\ntry:\n    from app.utils.chat.embeddings.sentence_encoder import (\n        SentenceEncoderEmbeddingGenerator,\n    )\n\n    logger.info(\n        \"🦙 Successfully imported embeddings(Tensorflow + Sentence Encoder) module!\"\n    )\nexcept Exception as e:\n    logger.warning(\"Sentence Encoder embedding import error: \" + str(e))\n    SentenceEncoderEmbeddingGenerator = str(e)  # Import error message\n\n\nif TYPE_CHECKING:\n    from app.models.llms import (\n        ExllamaModel,\n        LlamaCppModel,\n        LLMModel,\n        LLMModels,\n    )\n    from app.utils.chat.embeddings import BaseEmbeddingGenerator\n    from app.utils.chat.text_generations import BaseCompletionGenerator\n\nOPENAI_REPLACEMENT_MODELS: dict[str, str] = {\n    \"gpt-3.5-turbo\": \"chronos_hermes_13b\",\n    \"gpt-3.5-turbo-16k\": \"longchat_7b\",\n    \"gpt-4\": \"pygmalion_13b\",\n}\nrouter = APIRouter(route_class=RouteErrorHandler)\nsemaphore = anyio.create_semaphore(1)\ncompletion_generators: deque[\"BaseCompletionGenerator\"] = deque(maxlen=1)\nembedding_generators: deque[\"BaseEmbeddingGenerator\"] = deque(maxlen=1)\n\n\nclass DynamicLLMS:\n    \"\"\"Dynamically reloads the llms module when it is changed.\n    This is to prevent the need to restart the server when the llms module is changed.\n    \"\"\"\n\n    llms_module_reloader: ModuleReloader = ModuleReloader(\"app.models.llms\")\n\n    @classmethod\n    def reload(cls):\n        return cls.llms_module_reloader.reload()\n\n    @classmethod\n    @property\n    def llm_models(cls) -> \"LLMModels\":\n        return cls.llms_module_reloader.module.LLMModels\n\n    @classmethod\n    @property\n    def llama_cpp_model(cls) -> \"LlamaCppModel\":\n        return cls.llms_module_reloader.module.LlamaCppModel\n\n    @classmethod\n    @property\n    def exllama_model(cls) -> \"ExllamaModel\":\n        return cls.llms_module_reloader.module.ExllamaModel\n\n    @classmethod\n    def check_is_llama_cpp_model(cls, llm_model: \"LLMModel\") -> bool:\n        llms = cls.llms_module_reloader.module\n        return isinstance(llm_model, llms.LlamaCppModel)\n\n    @classmethod\n    def check_is_exllama_model(cls, llm_model: \"LLMModel\") -> bool:\n        llms = cls.llms_module_reloader.module\n        return isinstance(llm_model, llms.ExllamaModel)\n\n\nasync def get_semaphore() -> AsyncGenerator[anyio.Semaphore, None]:\n    \"\"\"Get a semaphore for the endpoint. This is to prevent multiple requests from\n    creating multiple completion generators at the same time.\"\"\"\n    async with semaphore:\n        yield semaphore\n\n\ndef get_completion_generator(\n    body: CreateCompletionRequest\n    | CreateChatCompletionRequest\n    | CreateEmbeddingRequest,\n) -> \"BaseCompletionGenerator\":\n    \"\"\"Get a completion generator for the given model. If the model is not cached, create a new one.\n    If the cache is full, delete the oldest completion generator.\"\"\"\n    try:\n        # Check if the model is an OpenAI model\n        if body.model in OPENAI_REPLACEMENT_MODELS:\n            body.model = OPENAI_REPLACEMENT_MODELS[body.model]\n            if not isinstance(body, CreateEmbeddingRequest):\n                body.logit_bias = None\n\n        # Check if the model is defined in LLMModels enum\n        DynamicLLMS.reload()\n        llm_model = DynamicLLMS.llm_models.get_value(body.model)\n\n        # Check if the model is cached. If so, return the cached completion generator\n        for completion_generator in completion_generators:\n            if completion_generator.llm_model.name == llm_model.name:\n                return completion_generator\n\n        # Before creating a new completion generator, deallocate embeddings to free up memory\n        if embedding_generators:\n            free_memory_of_first_item_from_container(\n                embedding_generators,\n                min_free_memory_mb=512,\n                logger=logger,\n            )\n\n        # Before creating a new completion generator, check memory usage\n        if completion_generators.maxlen == len(completion_generators):\n            free_memory_of_first_item_from_container(\n                completion_generators,\n                min_free_memory_mb=256,\n                logger=logger,\n            )\n\n        # Create a new completion generator\n        if DynamicLLMS.check_is_llama_cpp_model(llm_model):\n            assert not isinstance(\n                LlamaCppCompletionGenerator, str\n            ), LlamaCppCompletionGenerator\n            to_return = LlamaCppCompletionGenerator.from_pretrained(llm_model)\n        elif DynamicLLMS.check_is_exllama_model(llm_model):\n            assert not isinstance(\n                ExllamaCompletionGenerator, str\n            ), ExllamaCompletionGenerator\n            to_return = ExllamaCompletionGenerator.from_pretrained(llm_model)\n        else:\n            raise AssertionError(f\"Model {body.model} not implemented\")\n\n        # Add the new completion generator to the deque cache\n        completion_generators.append(to_return)\n        return to_return\n    except (AssertionError, OSError, MemoryError) as e:\n        raise e\n    except Exception as e:\n        logger.exception(f\"Exception in get_completion_generator: {e}\")\n        raise AssertionError(f\"Could not find a model: {body.model}\")\n\n\ndef get_embedding_generator(\n    body: CreateEmbeddingRequest,\n) -> \"BaseEmbeddingGenerator\":\n    \"\"\"Get an embedding generator for the given model. If the model is not cached, create a new one.\n    If the cache is full, delete the oldest completion generator.\"\"\"\n    try:\n        body.model = body.model.lower()\n        for embedding_generator in embedding_generators:\n            if embedding_generator.model_name == body.model:\n                return embedding_generator\n\n        # Before creating a new completion generator, check memory usage\n        if embedding_generators.maxlen == len(embedding_generators):\n            free_memory_of_first_item_from_container(\n                embedding_generators,\n                min_free_memory_mb=256,\n                logger=logger,\n            )\n        # Before creating a new completion generator, deallocate embeddings to free up memory\n        if completion_generators:\n            free_memory_of_first_item_from_container(\n                completion_generators,\n                min_free_memory_mb=512,\n                logger=logger,\n            )\n\n        if \"sentence\" in body.model and \"encoder\" in body.model:\n            # Create a new sentence encoder embedding\n            assert not isinstance(\n                SentenceEncoderEmbeddingGenerator, str\n            ), SentenceEncoderEmbeddingGenerator\n            to_return = SentenceEncoderEmbeddingGenerator.from_pretrained(\n                body.model\n            )\n        else:\n            # Create a new transformer embedding\n            assert not isinstance(\n                TransformerEmbeddingGenerator, str\n            ), LlamaCppCompletionGenerator\n            to_return = TransformerEmbeddingGenerator.from_pretrained(\n                body.model\n            )\n\n        # Add the new completion generator to the deque cache\n        embedding_generators.append(to_return)\n        return to_return\n    except (AssertionError, OSError, MemoryError) as e:\n        raise e\n    except Exception as e:\n        logger.exception(f\"Exception in get_embedding_generator: {e}\")\n        raise AssertionError(f\"Could not find a model: {body.model}\")\n\n\nasync def get_event_publisher(\n    request: Request,\n    inner_send_chan: MemoryObjectSendStream,\n    iterator: Iterator,\n    is_chat_completion: Optional[bool] = None,\n):\n    async with inner_send_chan:\n        try:\n            async for chunk in iterate_in_threadpool(iterator):\n                if is_chat_completion is True:\n                    print(\n                        chunk[\"choices\"][0][\"delta\"].get(\"content\", \"\"),\n                        end=\"\",\n                        flush=True,\n                    )\n                elif is_chat_completion is False:\n                    print(\n                        chunk[\"choices\"][0][\"text\"],\n                        end=\"\",\n                        flush=True,\n                    )\n                await inner_send_chan.send(b\"data: \" + dumps(chunk) + b\"\\n\\n\")\n                if await request.is_disconnected():\n                    raise anyio.get_cancelled_exc_class()()\n            await inner_send_chan.send(b\"data: [DONE]\\n\\n\")\n        except anyio.get_cancelled_exc_class() as e:\n            with anyio.move_on_after(1, shield=True):\n                logger.info(\n                    f\"🦙 Disconnected from client (via refresh/close) {request.client}\",\n                )\n                raise e\n        finally:\n            logger.info(\"\\n[🦙 I'm done talking]\")\n\n\n@router.post(\n    \"/v1/chat/completions\",\n    response_model=create_model_from_typeddict(ChatCompletion),  # type: ignore\n)\nasync def create_chat_completion(\n    request: Request,\n    body: CreateChatCompletionRequest,\n    semaphore: anyio.Semaphore = Depends(get_semaphore),\n) -> Union[ChatCompletion, EventSourceResponse]:\n    logger.info(f\"🦙 Chat Completion Settings: {body}\\n\\n\")\n    completion_generator = get_completion_generator(body)\n    logger.info(\"\\n[🦙 I'm talking now]\")\n    if body.stream:\n        _iterator: Iterator[\n            ChatCompletionChunk\n        ] = completion_generator.generate_chat_completion_with_streaming(\n            messages=body.messages,\n            settings=body,\n        )\n        # EAFP: It's easier to ask for forgiveness than permission\n        first_response = await run_in_threadpool(next, _iterator)\n\n        def iterator() -> Iterator[ChatCompletionChunk]:\n            yield first_response\n            yield from _iterator\n\n        send_chan, recv_chan = anyio.create_memory_object_stream(10)\n        return EventSourceResponse(\n            recv_chan,\n            data_sender_callable=partial(\n                get_event_publisher,\n                request=request,\n                inner_send_chan=send_chan,\n                iterator=iterator(),\n                is_chat_completion=True,\n            ),\n        )\n    else:\n        chat_completion: ChatCompletion = await run_in_threadpool(\n            completion_generator.generate_chat_completion,\n            messages=body.messages,\n            settings=body,\n        )\n        print(chat_completion[\"choices\"][0][\"message\"][\"content\"])\n        logger.info(\"\\n[🦙 I'm done talking!]\")\n        return chat_completion\n\n\n@router.post(\n    \"/v1/completions\",\n    response_model=create_model_from_typeddict(Completion),  # type: ignore\n)\nasync def create_completion(\n    request: Request,\n    body: CreateCompletionRequest,\n    semaphore: anyio.Semaphore = Depends(get_semaphore),\n) -> Union[Completion, EventSourceResponse]:\n    logger.info(f\"🦙 Text Completion Settings: {body}\\n\\n\")\n    completion_generator = get_completion_generator(body)\n    logger.info(\"\\n[🦙 I'm talking now]\")\n    if body.stream:\n        _iterator: Iterator[\n            CompletionChunk\n        ] = completion_generator.generate_completion_with_streaming(\n            prompt=body.prompt,\n            settings=body,\n        )\n        # EAFP: It's easier to ask for forgiveness than permission\n        first_response = await run_in_threadpool(next, _iterator)\n\n        def iterator() -> Iterator[CompletionChunk]:\n            yield first_response\n            yield from _iterator\n\n        send_chan, recv_chan = anyio.create_memory_object_stream(10)\n        return EventSourceResponse(\n            recv_chan,\n            data_sender_callable=partial(\n                get_event_publisher,\n                request=request,\n                inner_send_chan=send_chan,\n                iterator=iterator(),\n                is_chat_completion=False,\n            ),\n        )\n    else:\n        completion: Completion = await run_in_threadpool(\n            completion_generator.generate_completion,\n            prompt=body.prompt,\n            settings=body,\n        )\n        print(completion[\"choices\"][0][\"text\"])\n        logger.info(\"\\n[🦙 I'm done talking!]\")\n        return completion\n\n\n@router.post(\n    \"/v1/embeddings\",\n    response_model=create_model_from_typeddict(Embedding),  # type: ignore\n)\nasync def create_embedding(\n    body: CreateEmbeddingRequest,\n    semaphore: anyio.Semaphore = Depends(get_semaphore),\n) -> Embedding:\n    assert body.model is not None, \"Model is required\"\n    try:\n        DynamicLLMS.reload()\n        llm_model = DynamicLLMS.llm_models.get_value(body.model)\n        if not DynamicLLMS.check_is_llama_cpp_model(llm_model):\n            raise NotImplementedError(\"Using non-llama-cpp model\")\n\n    except Exception:\n        # Embedding model from local\n        #     \"intfloat/e5-large-v2\",\n        #     \"hkunlp/instructor-xl\",\n        #     \"hkunlp/instructor-large\",\n        #     \"intfloat/e5-base-v2\",\n        #     \"intfloat/e5-large\",\n        embedding_generator: \"BaseEmbeddingGenerator\" = (\n            get_embedding_generator(body)\n        )\n        embeddings: list[list[float]] = await run_in_threadpool(\n            embedding_generator.generate_embeddings,\n            texts=body.input if isinstance(body.input, list) else [body.input],\n            context_length=512,\n            batch=1000,\n        )\n\n        return {\n            \"object\": \"list\",\n            \"data\": [\n                {\n                    \"index\": embedding_idx,\n                    \"object\": \"embedding\",\n                    \"embedding\": embedding,\n                }\n                for embedding_idx, embedding in enumerate(embeddings)\n            ],\n            \"model\": body.model,\n            \"usage\": {\n                \"prompt_tokens\": -1,\n                \"total_tokens\": -1,\n            },\n        }\n\n    else:\n        # Trying to get embedding model from Llama.cpp\n        assert (\n            llm_model.embedding\n        ), \"Model does not support embeddings. Set `embedding` to True in the LlamaCppModel\"\n        assert not isinstance(\n            LlamaCppCompletionGenerator, str\n        ), LlamaCppCompletionGenerator\n        completion_generator = get_completion_generator(body)\n        assert isinstance(\n            completion_generator, LlamaCppCompletionGenerator\n        ), f\"Model {body.model} is not supported for llama.cpp embeddings.\"\n\n        assert completion_generator.client, \"Model is not loaded yet\"\n        return await run_in_threadpool(\n            completion_generator.client.create_embedding,\n            **body.dict(exclude={\"user\"}),\n        )\n\n\n@router.get(\"/v1/models\", response_model=create_model_from_typeddict(ModelList))  # type: ignore\nasync def get_models() -> ModelList:\n    DynamicLLMS.reload()\n    llama_cpp_models: list = [\n        enum.value for enum in DynamicLLMS.llm_models.member_map.values()\n    ]\n    return {\n        \"object\": \"list\",\n        \"data\": [\n            {\n                \"id\": llama_cpp_model.name + f\"({llama_cpp_model.model_path})\",\n                \"object\": \"model\",\n                \"owned_by\": \"me\",\n                \"permissions\": [],\n            }\n            for llama_cpp_model in llama_cpp_models\n        ],\n    }\n"}
{"type": "source_file", "path": "app/models/completion_models.py", "content": "from sys import version_info\nfrom typing import Dict, List, Optional, Literal, TypedDict\n\n# If python version >= 3.11, use the built-in NotRequired type.\n# Otherwise, import it from typing_extensi\nif version_info >= (3, 11):\n    from typing import NotRequired  # type: ignore\nelse:\n    from typing_extensions import NotRequired\n\nfrom .function_calling.base import JsonTypes\n\n\nclass FunctionCallParsed(TypedDict):\n    name: str\n    arguments: NotRequired[Dict[str, JsonTypes]]\n\n\nclass FunctionCallUnparsed(TypedDict):\n    name: NotRequired[str]\n    arguments: NotRequired[str]\n\n\nclass EmbeddingUsage(TypedDict):\n    prompt_tokens: int\n    total_tokens: int\n\n\nclass EmbeddingData(TypedDict):\n    index: int\n    object: str\n    embedding: List[float]\n\n\nclass Embedding(TypedDict):\n    object: Literal[\"list\"]\n    model: str\n    data: List[EmbeddingData]\n    usage: EmbeddingUsage\n\n\nclass CompletionLogprobs(TypedDict):\n    text_offset: List[int]\n    token_logprobs: List[Optional[float]]\n    tokens: List[str]\n    top_logprobs: List[Optional[Dict[str, float]]]\n\n\nclass CompletionChoice(TypedDict):\n    text: str\n    index: int\n    logprobs: Optional[CompletionLogprobs]\n    finish_reason: Optional[str]\n\n\nclass CompletionUsage(TypedDict):\n    prompt_tokens: int\n    completion_tokens: int\n    total_tokens: int\n\n\nclass CompletionChunk(TypedDict):\n    id: str\n    object: Literal[\"text_completion\"]\n    created: int\n    model: str\n    choices: List[CompletionChoice]\n\n\nclass Completion(TypedDict):\n    id: str\n    object: Literal[\"text_completion\"]\n    created: int\n    model: str\n    choices: List[CompletionChoice]\n    usage: CompletionUsage\n\n\nclass ChatCompletionMessage(TypedDict):\n    role: Literal[\"assistant\", \"user\", \"system\"]\n    content: str\n    user: NotRequired[str]\n    function_call: NotRequired[FunctionCallUnparsed]\n\n\nclass ChatCompletionChoice(TypedDict):\n    index: int\n    message: ChatCompletionMessage\n    finish_reason: Optional[str]\n\n\nclass ChatCompletion(TypedDict):\n    id: str\n    object: Literal[\"chat.completion\"]\n    created: int\n    model: str\n    choices: List[ChatCompletionChoice]\n    usage: CompletionUsage\n\n\nclass ChatCompletionChunkDelta(TypedDict):\n    role: NotRequired[Literal[\"assistant\"]]\n    content: NotRequired[str]\n    function_call: NotRequired[FunctionCallUnparsed]\n\n\nclass ChatCompletionChunkChoice(TypedDict):\n    index: int\n    delta: ChatCompletionChunkDelta\n    finish_reason: Optional[str]\n\n\nclass ChatCompletionChunk(TypedDict):\n    id: str\n    model: str\n    object: Literal[\"chat.completion.chunk\"]\n    created: int\n    choices: List[ChatCompletionChunkChoice]\n\n\nclass ModelData(TypedDict):\n    id: str\n    object: Literal[\"model\"]\n    owned_by: str\n    permissions: List[str]\n\n\nclass ModelList(TypedDict):\n    object: Literal[\"list\"]\n    data: List[ModelData]\n"}
{"type": "source_file", "path": "app/routers/websocket.py", "content": "from asyncio import sleep\n\nfrom fastapi import APIRouter, WebSocket\n\nfrom app.common.config import API_ENV, HOST_MAIN, OPENAI_API_KEY\nfrom app.database.crud import api_keys\nfrom app.database.schemas.auth import Users\nfrom app.errors.api_exceptions import Responses_400, Responses_401\nfrom app.utils.chat.managers.stream import ChatStreamManager\nfrom app.utils.chat.managers.websocket import SendToWebsocket\nfrom app.viewmodels.status import ApiKeyStatus, UserStatus\n\nrouter = APIRouter()\n\n\n@router.websocket(\"/chat/{api_key}\")\nasync def ws_chat(websocket: WebSocket, api_key: str):\n    if OPENAI_API_KEY is None:\n        raise Responses_400.not_supported_feature\n    await websocket.accept()  # accept websocket\n    if api_key != OPENAI_API_KEY and not API_ENV == \"test\":\n        _api_key, _user = await api_keys.get_api_key_and_owner(\n            access_key=api_key\n        )\n        if _user.status not in (UserStatus.active, UserStatus.admin):\n            await SendToWebsocket.message(\n                websocket=websocket,\n                msg=\"Your account is not active\",\n                chat_room_id=\" \",\n            )\n            await sleep(60)\n            raise Responses_401.not_authorized\n        if _api_key.status is not ApiKeyStatus.active:\n            await SendToWebsocket.message(\n                websocket=websocket,\n                msg=\"Your api key is not active\",\n                chat_room_id=\" \",\n            )\n            await sleep(60)\n            raise Responses_401.not_authorized\n    else:\n        _user = Users(email=f\"testaccount@{HOST_MAIN}\")\n    await ChatStreamManager.begin_chat(\n        websocket=websocket,\n        user=_user,\n    )\n"}
{"type": "source_file", "path": "app/utils/chat/build_llama_shared_lib.py", "content": "import subprocess\nimport sys\nfrom logging import Logger, getLogger\nfrom pathlib import Path\nfrom typing import Optional\n\nLIB_BASE_NAME: str = \"llama\"\nREPOSITORY_FOLDER: str = \"repositories\"\nPROJECT_GIT_URL: str = \"https://github.com/abetlen/llama-cpp-python.git\"\nPROJECT_NAME: str = \"llama_cpp\"\nMODULE_NAME: str = \"llama_cpp\"\nVENDOR_GIT_URL: str = \"https://github.com/ggerganov/llama.cpp.git\"\nVENDOR_NAME: str = \"llama.cpp\"\nCMAKE_CONFIG: str = \"Release\"\nSCRIPT_FILE_NAME: str = \"build-llama-cpp\"\nCMAKE_OPTIONS: dict[str, str] = {\n    \"cublas\": \"-DBUILD_SHARED_LIBS=ON -DLLAMA_CUBLAS=ON\",\n    \"default\": \"-DBUILD_SHARED_LIBS=ON\",\n}\n\n\nWINDOWS_BUILD_SCRIPT = r\"\"\"\ncd {vendor_path}\nrmdir /s /q build\nmkdir build\ncd build\ncmake .. {cmake_args}\ncmake --build . --config {cmake_config}\ncd ../../../../..\n\"\"\"\n\nUNIX_BUILD_SCRIPT = r\"\"\"#!/bin/bash\ncd {vendor_path}\nrm -rf build\nmkdir build\ncd build\ncmake .. {cmake_args}\ncmake --build . --config {cmake_config}\ncd ../../../../..\n\"\"\"\nREPOSITORY_PATH: Path = Path(REPOSITORY_FOLDER).resolve()\nPROJECT_PATH: Path = REPOSITORY_PATH / Path(PROJECT_NAME)\nMODULE_PATH: Path = PROJECT_PATH / Path(MODULE_NAME)\nVENDOR_PATH: Path = PROJECT_PATH / Path(\"vendor\") / Path(VENDOR_NAME)\nBUILD_OUTPUT_PATH: Path = (\n    VENDOR_PATH / Path(\"build\") / Path(\"bin\") / Path(CMAKE_CONFIG)\n)\n\n\ndef _clone_repositories() -> None:\n    if not PROJECT_PATH.exists():\n        REPOSITORY_PATH.mkdir(exist_ok=True)\n        subprocess.run(\n            [\n                \"git\",\n                \"clone\",\n                \"--recurse-submodules\",\n                PROJECT_GIT_URL,\n                PROJECT_NAME,\n            ],\n            cwd=REPOSITORY_PATH,\n        )\n\n    if not VENDOR_PATH.exists():\n        PROJECT_PATH.mkdir(exist_ok=True)\n        subprocess.run(\n            [\"git\", \"clone\", VENDOR_GIT_URL],\n            cwd=Path(PROJECT_PATH),\n        )\n\n\ndef _get_lib_paths() -> list[Path]:\n    # Determine the file extension based on the platform\n    if sys.platform.startswith(\"linux\"):\n        return [\n            MODULE_PATH / f\"lib{LIB_BASE_NAME}.so\",\n        ]\n    elif sys.platform == \"darwin\":\n        return [\n            MODULE_PATH / f\"lib{LIB_BASE_NAME}.so\",\n            MODULE_PATH / f\"lib{LIB_BASE_NAME}.dylib\",\n        ]\n    elif sys.platform == \"win32\":\n        return [\n            MODULE_PATH / f\"{LIB_BASE_NAME}.dll\",\n        ]\n    else:\n        raise RuntimeError(\"Unsupported platform\")\n\n\ndef _get_build_lib_paths() -> list[Path]:\n    # Determine the file extension based on the platform\n    if sys.platform.startswith(\"linux\"):\n        return [\n            BUILD_OUTPUT_PATH / f\"lib{LIB_BASE_NAME}.so\",\n        ]\n    elif sys.platform == \"darwin\":\n        return [\n            BUILD_OUTPUT_PATH / f\"lib{LIB_BASE_NAME}.so\",\n            BUILD_OUTPUT_PATH / f\"lib{LIB_BASE_NAME}.dylib\",\n        ]\n    elif sys.platform == \"win32\":\n        return [\n            BUILD_OUTPUT_PATH / f\"{LIB_BASE_NAME}.dll\",\n        ]\n    else:\n        raise RuntimeError(\"Unsupported platform\")\n\n\ndef _get_script_extension() -> str:\n    if sys.platform.startswith(\"linux\"):\n        return \"sh\"\n    elif sys.platform == \"darwin\":\n        return \"sh\"\n    elif sys.platform == \"win32\":\n        return \"bat\"\n    else:\n        raise RuntimeError(\"Unsupported platform\")\n\n\ndef _get_copy_command() -> str:\n    if sys.platform.startswith(\"linux\"):\n        return \"cp\"\n    elif sys.platform == \"darwin\":\n        return \"cp\"\n    elif sys.platform == \"win32\":\n        return \"copy\"\n    else:\n        raise RuntimeError(\"Unsupported platform\")\n\n\ndef _get_script_content(cmake_args: str) -> str:\n    if sys.platform.startswith(\"linux\"):\n        return UNIX_BUILD_SCRIPT.format(\n            vendor_path=VENDOR_PATH,\n            cmake_args=cmake_args,\n            cmake_config=CMAKE_CONFIG,\n        )\n    elif sys.platform == \"darwin\":\n        return UNIX_BUILD_SCRIPT.format(\n            vendor_path=VENDOR_PATH,\n            cmake_args=cmake_args,\n            cmake_config=CMAKE_CONFIG,\n        )\n    elif sys.platform == \"win32\":\n        return WINDOWS_BUILD_SCRIPT.format(\n            vendor_path=VENDOR_PATH,\n            cmake_args=cmake_args,\n            cmake_config=CMAKE_CONFIG,\n        )\n    else:\n        raise RuntimeError(\"Unsupported platform\")\n\n\ndef build_shared_lib(logger: Optional[Logger] = None) -> None:\n    \"\"\"\n    Ensure that the llama.cpp DLL exists.\n    You need cmake and Visual Studio 2019 to build llama.cpp.\n    You can download cmake here: https://cmake.org/download/\n    \"\"\"\n\n    if logger is None:\n        logger = getLogger(__name__)\n        logger.setLevel(\"INFO\")\n\n    if not PROJECT_PATH.exists():\n        _clone_repositories()\n\n    if not any(lib_path.exists() for lib_path in _get_lib_paths()):\n        logger.critical(\"🦙 llama.cpp DLL not found, building it...\")\n        script_extension = _get_script_extension()\n        copy_command = _get_copy_command()\n        build_lib_paths = _get_build_lib_paths()\n\n        script_paths: list[Path] = []\n        for script_name, cmake_args in CMAKE_OPTIONS.items():\n            if sys.platform == \"darwin\" and \"cublas\" in cmake_args.lower():\n                logger.warning(\n                    \"🦙 cuBLAS is not supported on macOS, skipping this...\"\n                )\n                continue\n            MODULE_PATH.mkdir(exist_ok=True)\n            script_path = MODULE_PATH / Path(\n                f\"build-llama-cpp-{script_name}.{script_extension}\"\n            )\n            script_paths.append(script_path)\n            script_content = _get_script_content(cmake_args)\n            for build_lib_path in build_lib_paths:\n                script_content += (\n                    f\"\\n{copy_command} {build_lib_path} {MODULE_PATH}\"\n                )\n\n            with open(script_path, \"w\") as f:\n                f.write(script_content)\n\n        is_built: bool = False\n        for script_path in script_paths:\n            try:\n                # Try to build with cublas.\n                logger.critical(\n                    f\"🦙 Trying to build llama.cpp DLL: {script_path}\"\n                )\n                subprocess.run([script_path], check=True)\n                logger.critical(\"🦙 llama.cpp DLL successfully built!\")\n                is_built = True\n                break\n            except subprocess.CalledProcessError:\n                logger.critical(\"🦙 Could not build llama.cpp DLL!\")\n        if not is_built:\n            raise RuntimeError(\"🦙 Could not build llama.cpp DLL!\")\n\n\nif __name__ == \"__main__\":\n    build_shared_lib()\n"}
{"type": "source_file", "path": "app/utils/chat/commands/browsing.py", "content": "from typing import Optional\n\nfrom app.models.chat_models import ResponseType\nfrom app.models.function_calling.functions import FunctionCalls\nfrom app.models.llms import OpenAIModel\nfrom app.utils.chat.buffer import BufferedUserContext\nfrom app.utils.chat.messages.handler import MessageHandler\nfrom app.utils.function_calling.query import aget_query_to_search\n\n\nclass BrowsingCommands:\n    @staticmethod\n    async def browse(\n        user_query: str, /, buffer: BufferedUserContext, **kwargs\n    ) -> tuple[Optional[str], ResponseType]:\n        \"\"\"Query LLM with duckduckgo browse results\\n\n        /browse <query>\"\"\"\n        if user_query.startswith(\"/\"):\n            # User is trying to invoke another command.\n            # Give control back to the command handler,\n            # and let it handle the command.\n            # e.g. `/browse /help` will invoke `/help` command\n            return user_query, ResponseType.REPEAT_COMMAND\n\n        # Save user query to buffer and database\n        await MessageHandler.user(msg=user_query, buffer=buffer)\n\n        if not isinstance(buffer.current_llm_model.value, OpenAIModel):\n            # Non-OpenAI models can't invoke function call,\n            # so we force function calling here\n            query_to_search: str = await aget_query_to_search(\n                buffer=buffer,\n                query=user_query,\n                function=FunctionCalls.get_function_call(\n                    FunctionCalls.web_search\n                ),\n            )\n            await MessageHandler.function_call(\n                callback_name=FunctionCalls.web_search.__name__,\n                callback_kwargs={\"query_to_search\": query_to_search},\n                buffer=buffer,\n            )\n        else:\n            # OpenAI models can invoke function call,\n            # so let the AI decide whether to invoke function call\n            function = FunctionCalls.get_function_call(\n                FunctionCalls.web_search\n            )\n            buffer.optional_info[\"functions\"] = [function]\n            buffer.optional_info[\"function_call\"] = function\n            await MessageHandler.ai(buffer=buffer)\n\n        # End of command\n        return None, ResponseType.DO_NOTHING\n"}
{"type": "source_file", "path": "app/common/app_settings.py", "content": "from threading import Thread\n\nfrom fastapi import Depends, FastAPI\nfrom fastapi.staticfiles import StaticFiles\nfrom starlette.middleware import Middleware\nfrom starlette.middleware.base import BaseHTTPMiddleware\nfrom starlette.middleware.cors import CORSMiddleware\nfrom starlette.middleware.sessions import SessionMiddleware\nfrom starlette_admin.contrib.sqla.admin import Admin\nfrom starlette_admin.contrib.sqla.view import ModelView\nfrom starlette_admin.views import DropDown, Link\n\nfrom app.auth.admin import MyAuthProvider\nfrom app.database.connection import cache, db\nfrom app.database.schemas.auth import ApiKeys, ApiWhiteLists, Users\nfrom app.dependencies import USER_DEPENDENCY, api_service_dependency\nfrom app.middlewares.token_validator import access_control\nfrom app.middlewares.trusted_hosts import TrustedHostMiddleware\nfrom app.routers import auth, index, services, user_services, users, websocket\nfrom app.shared import Shared\nfrom app.utils.chat.managers.cache import CacheManager\nfrom app.utils.js_initializer import js_url_initializer\nfrom app.utils.logger import ApiLogger\nfrom app.viewmodels.admin import ApiKeyAdminView, UserAdminView\n\nfrom .app_settings_llama_cpp import monitor_llama_cpp_server\nfrom .config import JWT_SECRET, Config\n\n\nasync def on_startup():\n    \"\"\"\n    Performs necessary operations during application startup.\n\n    This function is called when the application is starting up.\n    It performs the following operations:\n    - Logs a startup message using ApiLogger.\n    - Retrieves the configuration object.\n    - Checks if the MySQL database connection is initiated and logs the status.\n    - Raises a ConnectionError if the Redis cache connection is not established.\n    - Checks if the Redis cache connection is initiated and logs the status.\n    - Attempts to import and set uvloop as the event loop policy, if available, and logs the result.\n    - Starts Llama CPP server monitoring if the Llama CPP completion URL is provided.\n    \"\"\"\n    ApiLogger.ccritical(\"⚙️ Booting up...\")\n    config = Config.get()\n    shared = Shared()\n    if db.is_initiated:\n        ApiLogger.ccritical(\"MySQL DB connected!\")\n    else:\n        ApiLogger.ccritical(\"MySQL DB connection failed!\")\n    if cache.redis is None:\n        raise ConnectionError(\"Redis is not connected yet!\")\n    if cache.is_initiated and await cache.redis.ping():\n        await CacheManager.delete_user(f\"testaccount@{config.host_main}\")\n        ApiLogger.ccritical(\"Redis CACHE connected!\")\n    else:\n        ApiLogger.ccritical(\"Redis CACHE connection failed!\")\n    try:\n        import asyncio\n\n        import uvloop  # type: ignore\n\n        asyncio.set_event_loop_policy(uvloop.EventLoopPolicy())\n        ApiLogger.ccritical(\"uvloop installed!\")\n    except ImportError:\n        ApiLogger.ccritical(\"uvloop not installed!\")\n\n    if config.llama_completion_url:\n        # Start Llama CPP server monitoring\n        ApiLogger.ccritical(\"Llama CPP server monitoring started!\")\n        shared.thread = Thread(\n            target=monitor_llama_cpp_server,\n            args=(config, shared),\n            daemon=True,\n        )\n        shared.thread.start()\n\n\nasync def on_shutdown():\n    \"\"\"\n    Performs necessary operations during application shutdown.\n\n    This function is called when the application is shutting down.\n    It performs the following operations:\n    - Logs a shutdown message using ApiLogger.\n    - Sets terminate signals for shared threads and processes.\n    - Shuts down the process manager, if available.\n    - Shuts down the process pool executor, if available.\n    - Terminates and joins the process, if available.\n    - Joins the thread, if available.\n    - Closes the database and cache connections.\n    - Logs a message indicating the closure of DB and CACHE connections.\n    \"\"\"\n    ApiLogger.ccritical(\"⚙️ Shutting down...\")\n    shared = Shared()\n    # await CacheManager.delete_user(f\"testaccount@{HOST_MAIN}\")\n    shared.thread_terminate_signal.set()\n    shared.process_terminate_signal.set()\n\n    process_manager = shared._process_manager\n    if process_manager is not None:\n        process_manager.shutdown()\n\n    process_pool_executor = shared._process_pool_executor\n    if process_pool_executor is not None:\n        process_pool_executor.shutdown(wait=True)\n\n    process = shared._process\n    if process is not None:\n        process.terminate()\n        process.join()\n\n    thread = shared._thread\n    if thread is not None:\n        thread.join()\n\n    await db.close()\n    await cache.close()\n    ApiLogger.ccritical(\"DB & CACHE connection closed!\")\n\n\ndef create_app(config: Config) -> FastAPI:\n    \"\"\"\n    Creates and configures the FastAPI application.\n\n    Args:\n        config (Config): The configuration object.\n\n    Returns:\n        FastAPI: The configured FastAPI application.\n\n    This function creates a new FastAPI application, sets the specified title, description, and version,\n    and adds `on_startup` and `on_shutdown` event handlers.\n\n    It then starts the database and cache connections and initializes the JavaScript URL.\n\n    If the database engine is available, it adds admin views for managing users, API keys, and API white lists.\n\n    Next, it adds the necessary middlewares for access control, CORS, and trusted hosts.\n\n    It mounts the \"/chat\" endpoint for serving static files, and includes routers for index, websocket,\n    authentication, services, users, and user services.\n\n    Finally, it sets the application's config and shared state and returns the configured application.\n    \"\"\"\n    # Initialize app & db & js\n    new_app = FastAPI(\n        title=config.app_title,\n        description=config.app_description,\n        version=config.app_version,\n        on_startup=[on_startup],\n        on_shutdown=[on_shutdown],\n    )\n    db.start(config=config)\n    cache.start(config=config)\n    js_url_initializer(js_location=\"app/web/main.dart.js\")\n\n    # Admin\n    if db.engine is not None:\n        admin = Admin(\n            db.engine,\n            title=\"Admin Console\",\n            auth_provider=MyAuthProvider(),\n            middlewares=[Middleware(SessionMiddleware, secret_key=JWT_SECRET)],\n        )\n        admin.add_view(UserAdminView(Users, icon=\"fa fa-users\", label=\"Users\"))\n        admin.add_view(\n            ApiKeyAdminView(ApiKeys, icon=\"fa fa-key\", label=\"API Keys\")\n        )\n        admin.add_view(\n            ModelView(\n                ApiWhiteLists, icon=\"fa fa-list\", label=\"API White Lists\"\n            )\n        )\n        admin.add_view(\n            DropDown(\n                \"Links\",\n                icon=\"fa fa-link\",\n                views=[\n                    Link(\"Index\", url=\"/\"),\n                    Link(\"Docs\", url=\"/docs\"),\n                    Link(\"Chat\", url=\"/chat\", target=\"_blank\"),\n                ],\n            )\n        )\n        admin.mount_to(new_app)\n\n    # Middlewares\n    \"\"\"\n    Access control middleware: Authorized request only\n    CORS middleware: Allowed sites only\n    Trusted host middleware: Allowed host only\n    \"\"\"\n\n    new_app.add_middleware(\n        dispatch=access_control, middleware_class=BaseHTTPMiddleware\n    )\n    new_app.add_middleware(\n        CORSMiddleware,\n        allow_origins=config.allowed_sites,\n        allow_credentials=True,\n        allow_methods=[\"*\"],\n        allow_headers=[\"*\"],\n    )\n    new_app.add_middleware(\n        TrustedHostMiddleware,\n        allowed_hosts=config.trusted_hosts,\n        except_path=[\"/health\"],\n    )\n\n    # Routers\n    new_app.mount(\"/chat\", StaticFiles(directory=\"./app/web\", html=True))\n    new_app.include_router(index.router, tags=[\"index\"])\n    new_app.include_router(websocket.router, prefix=\"/ws\", tags=[\"websocket\"])\n    new_app.include_router(\n        auth.router,\n        prefix=\"/api\",\n        tags=[\"auth\"],\n    )\n    new_app.include_router(\n        services.router,\n        prefix=\"/api\",\n        tags=[\"Services\"],\n        dependencies=[Depends(api_service_dependency)],\n    )\n    new_app.include_router(\n        users.router,\n        prefix=\"/api\",\n        tags=[\"Users\"],\n        dependencies=[Depends(USER_DEPENDENCY)],\n    )\n    new_app.include_router(\n        user_services.router,\n        prefix=\"/api\",\n        tags=[\"User Services\"],\n        dependencies=[Depends(USER_DEPENDENCY)],\n    )\n    new_app.state.config = config\n    new_app.state.shared = Shared()\n\n    @new_app.get(\"/health\")\n    async def health():\n        return \"ok\"\n\n    return new_app\n"}
{"type": "source_file", "path": "app/utils/chat/buffer.py", "content": "import asyncio\nfrom dataclasses import dataclass, field\nfrom sys import version_info\nfrom typing import (\n    Any,\n    Awaitable,\n    Callable,\n    Literal,\n    Optional,\n    TypedDict,\n    Union,\n)\n\nfrom fastapi import WebSocket\n\nfrom app.database.schemas.auth import Users\nfrom app.models.base_models import UserChatRoles\nfrom app.models.chat_models import (\n    ChatRoles,\n    LLMModels,\n    MessageHistory,\n    UserChatContext,\n    UserChatProfile,\n)\nfrom app.models.function_calling.base import FunctionCall\n\n# Otherwise, import it from typing_extensi\nif version_info >= (3, 11):\n    from typing import NotRequired  # type: ignore\nelse:\n    from typing_extensions import NotRequired\n\n\nclass OptionalInfo(TypedDict):\n    translate: NotRequired[Optional[str]]\n    functions: NotRequired[Optional[list[FunctionCall]]]\n    function_call: NotRequired[\n        Optional[FunctionCall | Literal[\"none\", \"auto\"]]\n    ]\n    api_key: NotRequired[Optional[str]]\n    uuid: NotRequired[Optional[str]]\n    filename: NotRequired[Optional[str]]\n\n\nclass ContextList:\n    def __init__(\n        self,\n        initial_list: list[UserChatProfile],\n        read_callback: Callable[[UserChatProfile], Awaitable[UserChatContext]],\n    ):\n        self.data: list[Any] = initial_list\n        self.read_callback = read_callback\n\n    def __len__(self):\n        return len(self.data)\n\n    def __iter__(self):\n        return iter(self.data)\n\n    def __getitem__(\n        self, index: int\n    ) -> Union[UserChatContext, UserChatProfile]:\n        return self.data[index]\n\n    async def at(self, index: int) -> UserChatContext:\n        if isinstance(self.data[index], UserChatProfile):\n            self.data[index] = await self.read_callback(self.data[index])\n\n        return self.data[index]\n\n    def insert(self, index: int, value: Any) -> None:\n        self.data.insert(index, value)\n\n    def delete(self, index: int) -> None:\n        del self.data[index]\n\n\n@dataclass\nclass BufferedUserContext:\n    user: Users\n    websocket: WebSocket\n    initialize_callback: Callable[[str], Awaitable[list[UserChatProfile]]]\n    read_callback: Callable[[UserChatProfile], Awaitable[UserChatContext]]\n    queue: asyncio.Queue = field(default_factory=asyncio.Queue)\n    done: asyncio.Event = field(default_factory=asyncio.Event)\n    task_list: list[asyncio.Task[Any]] = field(default_factory=list)  # =\n    last_user_message: Optional[str] = None\n    optional_info: OptionalInfo = field(default_factory=OptionalInfo)\n    _sorted_ctxts: ContextList = field(init=False)\n    _current_ctxt: UserChatContext = field(init=False)\n\n    def __len__(self):\n        return len(self._sorted_ctxts)\n\n    def __getitem__(\n        self, index: int\n    ) -> Union[UserChatContext, UserChatProfile]:\n        return self._sorted_ctxts[index]\n\n    async def init(self):\n        self._sorted_ctxts = ContextList(\n            initial_list=await self.initialize_callback(self.user.email),\n            read_callback=self.read_callback,\n        )\n        self._current_ctxt = await self._sorted_ctxts.at(0)\n\n    def insert_context(\n        self, user_chat_context: UserChatContext, index: int = 0\n    ) -> None:\n        self._sorted_ctxts.insert(index=index, value=user_chat_context)\n\n    def delete_context(self, index: int) -> None:\n        self._sorted_ctxts.delete(index=index)\n\n    def find_index_of_chatroom(self, chat_room_id: str) -> Optional[int]:\n        try:\n            return self.sorted_chat_room_ids.index(chat_room_id)\n        except ValueError:\n            return None\n\n    async def find_index_of_message_history(\n        self,\n        user_chat_context: UserChatContext,\n        role: ChatRoles,\n        message_history_uuid: str,\n    ) -> Optional[int]:\n        try:\n            if role is ChatRoles.USER:\n                message_histories = user_chat_context.user_message_histories\n            elif role is ChatRoles.AI:\n                message_histories = user_chat_context.ai_message_histories\n            elif role is ChatRoles.SYSTEM:\n                message_histories = user_chat_context.system_message_histories\n            else:\n                raise ValueError(\"role must be one of 'user', 'ai', 'system'\")\n            for message_history_index, message_history in enumerate(\n                message_histories\n            ):\n                if message_history.uuid == message_history_uuid:\n                    return message_history_index\n            return None\n        except ValueError:\n            return None\n\n    async def change_context_to(self, index: int) -> None:\n        self._current_ctxt = await self._sorted_ctxts.at(index)\n\n    @property\n    def current_user_chat_roles(self) -> UserChatRoles:\n        return self._current_ctxt.llm_model.value.user_chat_roles\n\n    @property\n    def sorted_chat_room_ids(self) -> list[str]:\n        return [\n            context_or_profile.chat_room_id\n            for context_or_profile in self._sorted_ctxts\n        ]\n\n    @property\n    def sorted_chat_rooms(self) -> list[dict[str, str]]:\n        return [\n            {\n                \"chat_room_id\": context_or_profile.chat_room_id,\n                \"chat_room_name\": context_or_profile.chat_room_name,\n            }\n            for context_or_profile in self._sorted_ctxts\n        ]\n\n    @property\n    def user_id(self) -> str:\n        return self.user.email\n\n    @property\n    def current_chat_room_id(self) -> str:\n        return self._current_ctxt.chat_room_id\n\n    @property\n    def current_llm_model(self) -> LLMModels:\n        return self._current_ctxt.llm_model  # type: ignore\n\n    @property\n    def current_chat_room_name(self) -> str:\n        return self._current_ctxt.chat_room_name\n\n    @current_chat_room_name.setter\n    def current_chat_room_name(self, new_name: str) -> None:\n        self._current_ctxt.user_chat_profile.chat_room_name = new_name\n\n    @property\n    def current_user_chat_context(self) -> UserChatContext:\n        return self._current_ctxt\n\n    @property\n    def current_user_message_histories(self) -> list[MessageHistory]:\n        return self._current_ctxt.user_message_histories\n\n    @property\n    def current_ai_message_histories(self) -> list[MessageHistory]:\n        return self._current_ctxt.ai_message_histories\n\n    @property\n    def current_system_message_histories(self) -> list[MessageHistory]:\n        return self._current_ctxt.system_message_histories\n\n    @property\n    def current_user_chat_profile(self) -> UserChatProfile:\n        return self._current_ctxt.user_chat_profile\n"}
{"type": "source_file", "path": "app/utils/auth/token.py", "content": "from datetime import datetime, timedelta\n\nfrom jwt import decode as jwt_decode\nfrom jwt import encode as jwt_encode\nfrom jwt.exceptions import DecodeError, ExpiredSignatureError\n\nfrom app.common.config import JWT_ALGORITHM, JWT_SECRET\nfrom app.errors.api_exceptions import Responses_401\n\n\ndef create_access_token(\n    *, data: dict, expires_delta: int | None = None\n) -> str:\n    to_encode: dict = data.copy()\n    if expires_delta is not None and expires_delta != 0:\n        to_encode.update(\n            {\"exp\": datetime.utcnow() + timedelta(hours=expires_delta)}\n        )\n    return jwt_encode(to_encode, JWT_SECRET, algorithm=JWT_ALGORITHM)\n\n\ndef token_decode(authorization: str) -> dict:\n    if authorization is None:\n        raise Responses_401.token_decode_failure\n    try:\n        authorization = authorization.replace(\"Bearer \", \"\")\n        payload = jwt_decode(\n            authorization, key=JWT_SECRET, algorithms=[JWT_ALGORITHM]\n        )\n    except ExpiredSignatureError:\n        raise Responses_401.token_expired\n    except DecodeError:\n        raise Responses_401.token_decode_failure\n    return payload\n"}
{"type": "source_file", "path": "app/utils/chat/chat_rooms.py", "content": "from uuid import uuid4\n\nfrom app.models.chat_models import UserChatContext\n\nfrom .buffer import BufferedUserContext\nfrom .managers.cache import CacheManager\n\n\nasync def create_new_chat_room(\n    user_id: str,\n    new_chat_room_id: str | None = None,\n    buffer: BufferedUserContext | None = None,\n) -> UserChatContext:\n    if buffer is not None:\n        default: UserChatContext = UserChatContext.construct_default(\n            user_id=user_id,\n            chat_room_id=new_chat_room_id if new_chat_room_id else uuid4().hex,\n            llm_model=buffer.current_llm_model,\n        )\n    else:\n        default: UserChatContext = UserChatContext.construct_default(\n            user_id=user_id,\n            chat_room_id=new_chat_room_id if new_chat_room_id else uuid4().hex,\n        )\n    await CacheManager.create_context(user_chat_context=default)\n    if buffer is not None:\n        buffer.insert_context(user_chat_context=default)\n        await buffer.change_context_to(index=0)\n    return default\n\n\nasync def delete_chat_room(\n    chat_room_id_to_delete: str,\n    buffer: BufferedUserContext,\n) -> bool:\n    await CacheManager.delete_chat_room(\n        user_id=buffer.user_id, chat_room_id=chat_room_id_to_delete\n    )\n    index: int | None = buffer.find_index_of_chatroom(\n        chat_room_id=chat_room_id_to_delete\n    )\n    if index is None:\n        return False\n    buffer.delete_context(index=index)\n    if not buffer:\n        await create_new_chat_room(\n            user_id=buffer.user_id,\n            buffer=buffer,\n        )\n    if buffer.current_chat_room_id == chat_room_id_to_delete:\n        await buffer.change_context_to(index=0)\n    return True\n"}
