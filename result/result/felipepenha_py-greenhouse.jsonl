{"repo_info": {"repo_name": "py-greenhouse", "repo_owner": "felipepenha", "repo_url": "https://github.com/felipepenha/py-greenhouse"}}
{"type": "test_file", "path": "examples/palmer_penguins/tests/test_data_sourcing.py", "content": "import pandera as pa\nfrom src import data_sourcing\n\n\ndef test_data_sourcing_get():\n\n    df = data_sourcing.get()\n\n    print(df)\n\n    cats_sex = [\n        \"male\",\n        \"female\",\n    ]\n    cats_species = [\n        \"Adelie\",\n        \"Gentoo\",\n        \"Chinstrap\",\n    ]\n\n    schema = pa.DataFrameSchema(\n        {\n            \"bill_length_mm\": pa.Column(\n                float,\n                nullable=True,\n            ),\n            \"bill_depth_mm\": pa.Column(\n                float,\n                nullable=True,\n            ),\n            \"flipper_length_mm\": pa.Column(\n                float,\n                nullable=True,\n            ),\n            \"body_mass_g\": pa.Column(\n                float,\n                nullable=True,\n            ),\n            \"sex\": pa.Column(\n                str,\n                checks=pa.Check.isin(cats_sex),\n                nullable=True,\n            ),\n            \"species\": pa.Column(\n                str,\n                checks=pa.Check.isin(cats_species),\n                nullable=True,\n            ),\n        }\n    )\n\n    schema(df)\n"}
{"type": "test_file", "path": "tests/test_data_sourcing.py", "content": "import pandera as pa\nfrom src import data_sourcing\n\n\ndef test_data_sourcing_get():\n\n    df = data_sourcing.get()\n\n    print(df)\n\n    schema = pa.DataFrameSchema(\n        {\n            \"id\": pa.Column(\n                str,\n                nullable=True,\n            ),\n            \"x\": pa.Column(\n                float,\n                nullable=True,\n            ),\n            \"y\": pa.Column(\n                float,\n                nullable=True,\n            ),\n        }\n    )\n\n    schema(df)\n"}
{"type": "test_file", "path": "tests/test_data_splitting.py", "content": "from src import data_splitting\nimport pandas as pd\nimport pytest\n\n\n@pytest.fixture\ndef df_10_rows():\n\n    return pd.DataFrame(\n        {\n            \"col_1\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n            \"col_2\": [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\", \"j\"],\n        }\n    )\n\n\ndef test_data_splitting_train_ratio(df_10_rows):\n\n    train, valid, test = data_splitting.split(\n        df_10_rows, train_ratio=0.7, valid_ratio=0.2, test_ratio=0.1, seed=0\n    )\n\n    assert len(train) == 7\n\n\ndef test_data_splitting_valid_ratio(df_10_rows):\n\n    train, valid, test = data_splitting.split(\n        df_10_rows, train_ratio=0.7, valid_ratio=0.2, test_ratio=0.1, seed=0\n    )\n\n    assert len(valid) == 2\n\n\ndef test_data_splitting_test_ratio(df_10_rows):\n\n    train, valid, test = data_splitting.split(\n        df_10_rows, train_ratio=0.7, valid_ratio=0.2, test_ratio=0.1, seed=0\n    )\n\n    assert len(test) == 1\n\n\ndef test_data_splitting_train_vs_valid(df_10_rows):\n\n    train, valid, test = data_splitting.split(\n        df_10_rows, train_ratio=0.7, valid_ratio=0.2, test_ratio=0.1, seed=0\n    )\n\n    df_check = train.merge(\n        valid,\n        how=\"inner\",\n        right_on=[\"col_1\", \"col_2\"],\n        left_on=[\"col_1\", \"col_2\"],\n        sort=False,\n    )\n\n    assert df_check.empty\n\n\ndef test_data_splitting_train_vs_test(df_10_rows):\n\n    train, valid, test = data_splitting.split(\n        df_10_rows, train_ratio=0.7, valid_ratio=0.2, test_ratio=0.1, seed=0\n    )\n\n    df_check = train.merge(\n        test,\n        how=\"inner\",\n        right_on=[\"col_1\", \"col_2\"],\n        left_on=[\"col_1\", \"col_2\"],\n        sort=False,\n    )\n\n    assert df_check.empty\n\n\ndef test_data_splitting_valid_vs_test(df_10_rows):\n\n    train, valid, test = data_splitting.split(\n        df_10_rows, train_ratio=0.7, valid_ratio=0.2, test_ratio=0.1, seed=0\n    )\n\n    df_check = valid.merge(\n        test,\n        how=\"inner\",\n        right_on=[\"col_1\", \"col_2\"],\n        left_on=[\"col_1\", \"col_2\"],\n        sort=False,\n    )\n\n    assert df_check.empty\n"}
{"type": "test_file", "path": "tests/test_feature_engineering.py", "content": "from src import feature_engineering\nimport pandas as pd\nfrom pandas import _testing\nimport numpy as np\n\n\ndef test_numerical_missing_imputation_twofeatures():\n\n    df = pd.DataFrame(\n        {\n            \"a\": [1.0, 1.5, 2.0, 0.0, 1.25, np.nan],\n            \"b\": [1.0, 1.5, 2.0, 0.0, 0.0, np.nan],\n            \"c\": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n            \"d\": [\"apple\", \"apple\", \"pear\", \"apple\", \"pear\", \"apple\"],\n        }\n    )\n\n    expected = pd.DataFrame(\n        {\n            \"a\": [1.0, 1.5, 2.0, 0.0, 1.25, 1.25],\n            \"b\": [1.0, 1.5, 2.0, 0.0, 0.0, 1.0],\n        }\n    )\n\n    train, valid, test = feature_engineering.numerical_missing_imputation(\n        train=df,\n        valid=df,\n        test=df,\n        cols=[\n            \"a\",\n            \"b\",\n        ],\n    )\n\n    _testing.assert_frame_equal(train, expected)\n\n\ndef test_one_hot_encoding():\n\n    df = pd.DataFrame(\n        {\n            \"class\": [\"a\", \"b\", \"c\", \"a\", np.nan],\n            \"col_1\": [0.0, 0.0, 0.0, 0.0, 0.0],\n            \"col_2\": [\"apple\", \"apple\", \"pear\", \"apple\", \"pear\"],\n        }\n    )\n\n    expected = pd.DataFrame(\n        {\n            \"class_a\": [1, 0, 0, 1, 0],\n            \"class_b\": [0, 1, 0, 0, 0],\n            \"class_c\": [0, 0, 1, 0, 0],\n            \"class_na\": [0, 0, 0, 0, 1],\n        }\n    )\n\n    train, valid, test = feature_engineering.one_hot_encoding(\n        train=df,\n        valid=df,\n        test=df,\n        cols=[\n            \"class\",\n        ],\n    )\n\n    _testing.assert_frame_equal(train, expected)\n"}
{"type": "test_file", "path": "examples/vanilla/tests/test_data_sourcing.py", "content": "import pandera as pa\nfrom src import data_sourcing\n\n\ndef test_data_sourcing_get():\n\n    df = data_sourcing.get()\n\n    print(df)\n\n    schema = pa.DataFrameSchema(\n        {\n            \"id\": pa.Column(\n                str,\n                nullable=True,\n            ),\n            \"x\": pa.Column(\n                float,\n                nullable=True,\n            ),\n            \"y\": pa.Column(\n                float,\n                nullable=True,\n            ),\n        }\n    )\n\n    schema(df)\n"}
{"type": "source_file", "path": "examples/palmer_penguins/src/eda_monitoring.py", "content": "import pandas_profiling\n\n\ndef export_eda_report(df, path, preffix, suffix):\n\n    profile = pandas_profiling.ProfileReport(df, title=\"Pandas Profiling Report\")\n\n    path = \"{}/{}_ead_monitoring_{}.html\".format(path, preffix, suffix)\n\n    profile.to_file(path)\n\n    pass\n"}
{"type": "source_file", "path": "examples/palmer_penguins/src/data_sourcing.py", "content": "import palmerpenguins\n\n\ndef get():\n    \"\"\"Get the data.\n    This template function uses the Palmer Peguins dataset as a place holder.\n    Replace it by your own code to import your project's data.\n\n    Parameters\n    ----------\n    None\n\n    Returns\n    -------\n    pandas dataframe\n        Dataframe containing data.\n\n    Examples\n    --------\n\n    Raises\n    ------\n\n    Notes\n    -----\n\n    \"\"\"\n\n    df = palmerpenguins.load_penguins()\n\n    cols = [\n        \"bill_length_mm\",\n        \"bill_depth_mm\",\n        \"flipper_length_mm\",\n        \"body_mass_g\",\n        \"sex\",\n        \"species\",\n    ]\n\n    return df[cols]\n"}
{"type": "source_file", "path": "examples/palmer_penguins/src/data_preprocessing.py", "content": "def clean(df):\n    \"\"\"Cleansing: a data pre-processing step. Usually, getting rid of garbage\n    such as undesired characters.\n\n    Cleansing must be a set of operations independent of data splitting.\n\n    Parameters\n    ----------\n    df: pandas dataframe\n\n    Returns\n    -------\n    pandas dataframe\n        Cleansed dataframe\n\n    Examples\n    --------\n\n    Raises\n    ------\n\n    Notes\n    -----\n\n    \"\"\"\n\n    return df\n\n\ndef normalize(df):\n    \"\"\"Normalization: a data pre-processing step. Usually, making adjusting\n    loser and upper casing, abbrevations, word order, and so on.\n\n    Normalization must be a set of operations independent of data splitting.\n\n    Parameters\n    ----------\n    df: pandas dataframe\n\n    Returns\n    -------\n    pandas dataframe\n        Normalized dataframe\n\n    Examples\n    --------\n\n    Raises\n    ------\n\n    Notes\n    -----\n\n    \"\"\"\n\n    return df\n"}
{"type": "source_file", "path": "examples/palmer_penguins/src/api.py", "content": "from fastapi import FastAPI\nimport joblib\nimport numpy as np\n\nfrom pydantic import BaseModel\n\napp = FastAPI()\n\n\nclass ModelIn(BaseModel):\n    sex: str\n    bill_length_mm: str\n    bill_depth_mm: str\n    flipper_length_mm: str\n    body_mass_g: str\n\n\nclass ModelOut(BaseModel):\n    prob_0: float\n    prob_1: float\n    prob_2: float\n    species_code: int\n    species_name: str\n\n\napp = FastAPI()\n\n\n@app.post(\n    \"/predict/\",\n    response_model=ModelOut,\n)\nasync def root(input: ModelIn):\n\n    sex_male = {\"male\": 1, \"female\": 0, \"na\": 0}[input.sex]\n\n    sex_female = {\"male\": 0, \"female\": 1, \"na\": 0}[input.sex]\n\n    sex_na = {\"male\": 0, \"female\": 0, \"na\": 1}[input.sex]\n\n    X = [\n        [\n            int(sex_male),\n            int(sex_female),\n            int(sex_na),\n            float(input.bill_length_mm),\n            float(input.bill_depth_mm),\n            float(input.flipper_length_mm),\n            float(input.body_mass_g),\n        ],\n    ]\n\n    model = joblib.load(\"/usr/app/models/clf_random.joblib\")\n\n    out_dict = {}\n\n    out_dict[\"prob_0\"], out_dict[\"prob_1\"], out_dict[\"prob_2\"] = np.transpose(\n        model.predict_proba(X)\n    )\n\n    out_dict[\"prob_0\"] = out_dict[\"prob_0\"][0]\n\n    out_dict[\"prob_1\"] = out_dict[\"prob_1\"][0]\n    out_dict[\"prob_2\"] = out_dict[\"prob_2\"][0]\n\n    encoder = joblib.load(\"/usr/app/models/label_encoder.joblib\")\n\n    # Recover classes\n    classes = encoder.classes_\n\n    # Enumerate classes to recover codes (integers)\n    # Convert enumerate to dictionary\n    map_classes = dict(enumerate(classes))\n\n    code = model.predict(X)[0]\n\n    out_dict[\"species_code\"] = int(code)\n    out_dict[\"species_name\"] = map_classes[code]\n\n    return out_dict\n"}
{"type": "source_file", "path": "examples/vanilla/src/api.py", "content": "from fastapi import FastAPI\nfrom pydantic import BaseModel\n\nfrom src.modeling import VanillaModel\n\napp = FastAPI()\n\n\nclass ModelIn(BaseModel):\n    x: str\n\n\nclass ModelOut(BaseModel):\n    pred: float\n\n\nclass ModelOutHealth(BaseModel):\n    id: str\n\n\napp = FastAPI()\n\n\n@app.post(\"/health\")\nasync def health():\n\n    return {\"id\": \"Healthy\"}\n\n\n@app.post(\n    \"/predict/\",\n    response_model=ModelOut,\n)\nasync def root(input: ModelIn):\n\n    X = [\n        float(input.x),\n    ]\n\n    # Load your model from /models\n\n    # Note: for saving your model, we suggest using the\n    #       `joblib` python package\n\n    # Ex:   path \"/usr/app/models/\"\n    #       joblib.dump(self.m, path)\n    #       model = joblib.load(path)\n\n    # Vanila model always predict 0, so that\n    # inputs in the training phase are arbitrary\n    model = VanillaModel().fit(x=[0], y=[0])\n\n    out_dict = {}\n\n    out_dict[\"pred\"] = model.predict(X)[0]\n\n    return out_dict\n"}
{"type": "source_file", "path": "examples/vanilla/src/data_preprocessing.py", "content": "def clean(df):\n    \"\"\"Cleansing: a data pre-processing step. Usually, getting rid of garbage\n    such as undesired characters.\n\n    Cleansing must be a set of operations independent of data splitting.\n\n    ** Vanilla definition. **\n    Include your own code below to import your project's data.\n\n    Parameters\n    ----------\n    df: pandas dataframe\n\n    Returns\n    -------\n    pandas dataframe\n        Cleansed dataframe\n\n    Examples\n    --------\n\n    Raises\n    ------\n\n    Notes\n    -----\n\n    \"\"\"\n\n    return df\n\n\ndef normalize(df):\n    \"\"\"Normalization: a data pre-processing step. Usually, making adjusting\n    loser and upper casing, abbrevations, word order, and so on.\n\n    Normalization must be a set of operations independent of data splitting.\n\n    ** Vanilla definition. **\n    Include your own code below to import your project's data.\n\n    Parameters\n    ----------\n    df: pandas dataframe\n\n    Returns\n    -------\n    pandas dataframe\n        Normalized dataframe\n\n    Examples\n    --------\n\n    Raises\n    ------\n\n    Notes\n    -----\n\n    \"\"\"\n\n    return df\n"}
{"type": "source_file", "path": "examples/palmer_penguins/src/main.py", "content": "import greenhouse_clock\nimport data_sourcing\nimport data_splitting\nimport data_preprocessing\nimport feature_engineering\nimport eda_monitoring\nimport modeling\nimport performance_monitoring\n\nfrom prefect import Flow, task, context\n\nimport pandas as pd\n\n# Pandas options for better shell display\npd.set_option(\"display.max_rows\", 100)\npd.set_option(\"display.max_columns\", None)\npd.set_option(\"display.width\", None)\n\nstart_time = greenhouse_clock.get_time()\n\n\n@task\ndef sourcing():\n\n    return data_sourcing.get()\n\n\n@task\ndef cleansing(df):\n\n    return data_preprocessing.clean(df)\n\n\n@task\ndef normalizing(df):\n\n    return data_preprocessing.normalize(df)\n\n\n@task(nout=3)\ndef splitting(df):\n\n    return data_splitting.split(df)\n\n\n@task(nout=3)\ndef one_hot(train, valid, test, cols):\n\n    logger = context.get(\"logger\")\n\n    logger.info(train)\n\n    train_hot, valid_hot, test_hot = feature_engineering.one_hot_encoding(\n        train=train,\n        valid=valid,\n        test=test,\n        cols=cols,\n    )\n\n    train = train.join(train_hot)\n    valid = valid.join(valid_hot)\n    test = test.join(test_hot)\n\n    logger.info(train)\n\n    return train, valid, test\n\n\n@task(nout=3)\ndef imputation(train, valid, test, cols, imputation_method):\n\n    logger = context.get(\"logger\")\n\n    # Find rows where the numerical variables are nan\n    mask = train[cols].isna()\n\n    logger.info(train[mask])\n\n    train_imp, valid_imp, test_imp = feature_engineering.numerical_missing_imputation(\n        train=train,\n        valid=valid,\n        test=test,\n        cols=cols,\n        imputation_method=imputation_method,\n    )\n\n    train = train.join(train_imp, rsuffix=\"_imputed\")\n    valid = valid.join(valid_imp, rsuffix=\"_imputed\")\n    test = test.join(test_imp, rsuffix=\"_imputed\")\n\n    logger.info(train[mask])\n\n    return train, valid, test\n\n\n@task\ndef eda(df, path, preffix, suffix):\n\n    eda_monitoring.export_eda_report(df=df, path=path, preffix=preffix, suffix=suffix)\n\n    pass\n\n\n@task(nout=5)\ndef model(train, valid, test, obs, y_col, x_col):\n\n    mo = modeling.model()\n\n    mo.fit(train=train, y_col=y_col, x_col=x_col)\n\n    lst = list(mo.transform_sets(train=train, valid=valid, test=test))\n\n    lst.append(mo.transform_new(obs=obs))\n\n    return lst\n\n\n@task\ndef threshold(y_true, y_score):\n\n    return performance_monitoring.optimal_threshold(y_true=y_true, y_score=y_score)\n\n\n@task\ndef performance(y_true, y_score, best_hyperparams, path, opt_thr, suffix):\n\n    return performance_monitoring.report_performance(\n        y_true=y_true,\n        y_score=y_score,\n        best_hyperparams=best_hyperparams,\n        path=path,\n        opt_thr=opt_thr,\n        suffix=suffix,\n    )\n\n\n@task\ndef binarize(binary_map, series):\n\n    return series.map(binary_map)\n\n\n@task\ndef print_out(s):\n\n    print(s)\n\n    pass\n\n\n@task\ndef df_to_csv(df, filename):\n\n    df.to_csv(filename)\n\n    pass\n\n\n# Define prefect flow\nwith Flow(\"greenhouse\") as flow:\n\n    df = sourcing()\n    df = cleansing(df)\n    df = normalizing(df)\n    train, valid, test = splitting(df)\n\n    # eda(\n    #     df=train,\n    #     path=\"monitor/\",\n    #     preffix=start_time,\n    #     suffix=\"before_feat_eng\"\n    # )\n\n    # Categorical\n    cat_cols = [\n        \"sex\",\n    ]\n\n    train, valid, test = one_hot(\n        train=train,\n        valid=valid,\n        test=test,\n        cols=cat_cols,\n    )\n\n    # Numerical\n    num_cols = [\n        \"bill_length_mm\",\n        \"bill_depth_mm\",\n        \"flipper_length_mm\",\n        \"body_mass_g\",\n    ]\n\n    train, valid, test = imputation(\n        train=train,\n        valid=valid,\n        test=test,\n        cols=num_cols,\n        imputation_method=\"median\",\n    )\n\n    # eda(\n    #     df=train,\n    #     path=\"monitor/\",\n    #     preffix=start_time,\n    #     suffix=\"after_feat_eng\"\n    # )\n\n    y_col = [\"species\"]\n\n    x_col = [\n        \"sex_male\",\n        \"sex_female\",\n        \"sex_na\",\n        \"bill_length_mm_imputed\",\n        \"bill_depth_mm_imputed\",\n        \"flipper_length_mm_imputed\",\n        \"body_mass_g_imputed\",\n    ]\n\n    # `obs=test` just as an example here.\n    # It should be actually new data, unseen by the model.\n    train, valid, test, best_hyperparams, new = model(\n        train=train,\n        valid=valid,\n        test=test,\n        obs=test,\n        y_col=y_col,\n        x_col=x_col,\n    )\n\n    path = \"data/\"\n    filename = path + \"{}_predict_new.csv\".format(start_time)\n\n    df_to_csv(df=new, filename=filename)\n\n    # Obtain the optimal threshold of\n    # class 0 vs 1+2\n    # from the training set\n    opt_thr = threshold(y_true=train[\"actual\"], y_score=train[\"prob_0\"])\n\n    # class 0 --> 1\n    # class 1 or class 2 --> 0\n\n    binary_map = {\n        0: 1,\n        1: 0,\n        2: 0,\n    }\n\n    # Performance report over training set\n    performance(\n        y_true=binarize(binary_map=binary_map, series=train[\"actual\"]),\n        y_score=train[\"prob_0\"],\n        best_hyperparams=best_hyperparams,\n        path=\"monitor/\",\n        opt_thr=opt_thr,\n        suffix=\"_train\",\n    )\n\n    # Performance report over validation set\n    performance(\n        y_true=binarize(binary_map=binary_map, series=valid[\"actual\"]),\n        y_score=valid[\"prob_0\"],\n        best_hyperparams=best_hyperparams,\n        path=\"monitor/\",\n        opt_thr=opt_thr,\n        suffix=\"_valid\",\n    )\n\n    # Performance report over test set\n    performance(\n        y_true=binarize(binary_map=binary_map, series=test[\"actual\"]),\n        y_score=test[\"prob_0\"],\n        best_hyperparams=best_hyperparams,\n        path=\"monitor/\",\n        opt_thr=opt_thr,\n        suffix=\"_test\",\n    )\n\n\nif __name__ == \"__main__\":\n\n    # Run prefect flow\n    flow.run()\n\n    # Export flow as a PDF\n    flow.visualize(filename=\"flow/prefect_flow\")\n"}
{"type": "source_file", "path": "examples/palmer_penguins/src/performance_monitoring.py", "content": "import numpy as np\nfrom sklearn import metrics\nimport json\n\nimport greenhouse_clock\n\nmeta = {}\n\n# Timestamp for files\nmeta[\"timestr\"] = greenhouse_clock.get_time()\n\n\ndef optimal_threshold(y_true, y_score):\n\n    # Performance extracted from the \"ROC curve\"\n    fpr, tpr, thr = metrics.roc_curve(\n        y_true=y_true, y_score=y_score, pos_label=1, drop_intermediate=False\n    )\n\n    diff = np.abs(tpr - fpr)\n\n    # Numpy index of the maximum separation between TPR and FPR\n    diff_idx = np.argmax(diff)\n\n    # Optimum threshold based on max diff criterium\n    return thr[diff_idx]\n\n\ndef report_performance(\n    y_true, y_score, best_hyperparams, path, opt_thr=0.5, suffix=\"_\"\n):\n    \"\"\"\n    References\n    ----------\n    https://scikit-learn.org/stable/modules/generated/\n    sklearn.metrics.classification_report.html\n    \"\"\"\n\n    meta[\"optimal_hyperparameters\"] = best_hyperparams\n\n    meta[\"optimal_threshold\"] = opt_thr\n\n    # Performance extracted from the \"ROC curve\"\n    fpr, tpr, thr = metrics.roc_curve(\n        y_true=y_true, y_score=y_score, pos_label=1, drop_intermediate=False\n    )\n\n    meta[\"AUC\"] = metrics.auc(fpr, tpr)\n\n    diff = np.abs(tpr - fpr)\n\n    # Maximum difference between TPR and FPR\n    meta[\"max_diff_FPR_TPR\"] = np.max(diff)\n\n    # Numpy index of the maximum separation between TPR and FPR\n    diff_idx = np.argmax(diff)\n\n    # Update optimum threshold based on max diff criterium\n    meta[\"threshold_from_max_diff\"] = thr[diff_idx]\n\n    # Predicted classes based on \"optimal_threshold\"\n    y_pred = [int(k >= opt_thr) for k in y_score]\n\n    meta[\"classification_report\"] = metrics.classification_report(\n        y_true=y_true, y_pred=y_pred, output_dict=True\n    )\n\n    filename = \"{0}metadata{1}.json\".format(path, suffix)\n\n    # Export to JSON\n    with open(filename, \"w\") as fp:\n        json.dump(meta, fp, indent=4)\n\n    pass\n"}
{"type": "source_file", "path": "examples/palmer_penguins/src/data_splitting.py", "content": "def split(df, train_ratio=0.8, valid_ratio=0.1, test_ratio=0.1, seed=0):\n    \"\"\"Data splitting into 3 sets: train, valid, test\n\n    train: training set. Used for training the ML model.\n    valid: validation set. Used for frequent validation.\n    test: test set. Used for final test.\n\n    Parameters\n    ----------\n    df: pandas dataframe\n        Input data\n\n    train_ratio: float\n        Amount of data that goes into training, in percentage\n\n    valid_ratio: float\n        Amount of data that goes into validation, in percentage\n\n    test_ratio: float\n        Amount of data that goes into testing, in percentage\n\n    seed: int\n        Seed for the data shuffling.\n        It is important to keep it fixed throughout the tuning of the model.\n\n    Returns\n    -------\n    list\n        (train, valid, test)\n        (pandas dataframe, pandas dataframe, pandas dataframe)\n\n    Examples\n    --------\n\n    >>> len(data)\n    100\n    >>> train, valid, test = split(data)\n    >>> len(train)\n    80\n    >>> len(valid)\n    10\n    >>> len(test)\n    10\n\n    Raises\n    ------\n\n    Notes\n    -----\n\n    \"\"\"\n\n    # Train set extracted from a random sample from `df`\n    train = df.sample(frac=train_ratio, random_state=seed)\n\n    # Everything from `df` except `train`\n    rest = df.copy().drop(train.index)\n\n    # Valid set ratio within `rest`\n    new_ratio = valid_ratio / (valid_ratio + test_ratio)\n\n    # Train set extracted from a random sample from `rest`\n    valid = rest.sample(frac=new_ratio, random_state=seed)\n\n    # Test set is everything in rest `except` for `valid`\n    test = rest.drop(valid.index)\n\n    return train, valid, test\n"}
{"type": "source_file", "path": "examples/palmer_penguins/src/greenhouse_clock.py", "content": "import time\n\n\ndef get_time(format=\"%Y%m%d%H%M%S\"):\n\n    return time.strftime(format)\n"}
{"type": "source_file", "path": "examples/palmer_penguins/src/feature_engineering.py", "content": "from feature_engine import encoding, imputation\n\n\ndef numerical_missing_imputation(train, valid, test, cols, imputation_method=\"median\"):\n    \"\"\"Missing imputation for numerical variables.\n\n    The algorithm learns from the train set and applies transformations\n    to all three input datasets: train, valid, test.\n\n    Parameters\n    ----------\n    train: pandas dataframe\n        Training set\n\n    valid: pandas dataframe\n        Validation set\n\n    test: pandas dataframe\n        Test set\n\n    cols: list\n        List of numerical columns\n\n    imputation_method: string\n        Desired method of imputation. Options are 'mean' and 'median'.\n        Default value: 'median'.\n\n    Returns\n    -------\n    list\n        (train, valid, test)\n        (pandas dataframe, pandas dataframe, pandas dataframe)\n\n    Examples\n    --------\n\n    Raises\n    ------\n\n    Notes\n    -----\n\n    \"\"\"\n\n    fe = imputation.MeanMedianImputer(\n        imputation_method=imputation_method, variables=cols\n    )\n\n    # Fit over training set\n    fe.fit(train[cols])\n\n    # Apply to train, valid, test\n    return (\n        fe.transform(train[cols]),\n        fe.transform(valid[cols]),\n        fe.transform(test[cols]),\n    )\n\n\ndef one_hot_encoding(train, valid, test, cols):\n    \"\"\"One-hot-encoding of all categories found in `cols`.\n\n    The algorithm learns from the train set and applies transformations\n    to all three input datasets: train, valid, test.\n\n    Missing values in col lead to col_na=1\n\n    Parameters\n    ----------\n    train: pandas dataframe\n        Training set\n\n    valid: pandas dataframe\n        Validation set\n\n    test: pandas dataframe\n        Test set\n\n    cols: list\n        List of numerical columns\n\n    Returns\n    -------\n    list\n        (train, valid, test)\n        (pandas dataframe, pandas dataframe, pandas dataframe)\n\n    Examples\n    --------\n\n    Raises\n    ------\n\n    Notes\n    -----\n\n    \"\"\"\n\n    fe = encoding.OneHotEncoder(variables=cols)\n\n    for k in cols:\n        train[k] = train[k].fillna(\"na\")\n        valid[k] = valid[k].fillna(\"na\")\n        test[k] = test[k].fillna(\"na\")\n\n    # Fit over training set\n    fe.fit(train[cols])\n\n    # Apply to train, valid, test\n    return (\n        fe.transform(train[cols]),\n        fe.transform(valid[cols]),\n        fe.transform(test[cols]),\n    )\n"}
{"type": "source_file", "path": "examples/palmer_penguins/src/modeling.py", "content": "from sklearn import preprocessing\nimport numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\nimport joblib\n\n\nclass model:\n    def __init__(self):\n\n        pass\n\n    def fit(self, train, y_col, x_col, n_jobs=1, seed=1):\n\n        self.x_col = x_col\n        self.y_col = y_col\n\n        x_train = train[self.x_col].values\n\n        self.le = preprocessing.LabelEncoder()\n\n        # Trin encoder over training set\n        (self.le).fit(train[self.y_col].values.ravel())\n\n        path = \"/usr/app/models/label_encoder.joblib\"\n\n        joblib.dump(self.le, path)\n\n        y_train = (self.le).transform(train[self.y_col].values.ravel())\n\n        # Store the grid in a dictionary\n        grid = {}\n\n        grid[\"max_features\"] = [4, 5]\n        grid[\"max_depth\"] = [4, 5]\n        grid[\"n_estimators\"] = [50, 75, 200]\n\n        clf = RandomForestClassifier(random_state=seed)\n\n        self.clf_random = RandomizedSearchCV(\n            estimator=clf,\n            param_distributions=grid,\n            n_iter=10,\n            cv=None,\n            verbose=2,\n            random_state=seed,\n            n_jobs=n_jobs,\n        )\n\n        # Train model over training set\n        (self.clf_random).fit(x_train, y_train.ravel())\n\n        path = \"/usr/app/models/clf_random.joblib\"\n\n        joblib.dump(self.clf_random, path)\n\n    def transform_sets(self, train, valid, test):\n\n        x_train = train[self.x_col].values\n        x_valid = valid[self.x_col].values\n        x_test = test[self.x_col].values\n\n        y_train = (self.le).transform(train[self.y_col].values.ravel())\n        y_valid = (self.le).transform(valid[self.y_col].values.ravel())\n        y_test = (self.le).transform(test[self.y_col].values.ravel())\n\n        train_out = train.copy(deep=True)[self.y_col]\n        valid_out = valid.copy(deep=True)[self.y_col]\n        test_out = test.copy(deep=True)[self.y_col]\n\n        train_out[\"actual\"] = y_train\n        valid_out[\"actual\"] = y_valid\n        test_out[\"actual\"] = y_test\n\n        # Predict\n        train_out[\"pred\"] = (self.clf_random).predict(x_train)\n        valid_out[\"pred\"] = (self.clf_random).predict(x_valid)\n        test_out[\"pred\"] = (self.clf_random).predict(x_test)\n\n        train_out[\"prob_0\"], train_out[\"prob_1\"], train_out[\"prob_2\"] = np.transpose(\n            (self.clf_random).predict_proba(x_train)\n        )\n        valid_out[\"prob_0\"], valid_out[\"prob_1\"], valid_out[\"prob_2\"] = np.transpose(\n            (self.clf_random).predict_proba(x_valid)\n        )\n        test_out[\"prob_0\"], test_out[\"prob_1\"], test_out[\"prob_2\"] = np.transpose(\n            (self.clf_random).predict_proba(x_test)\n        )\n\n        return train_out, valid_out, test_out, (self.clf_random).best_params_\n\n    def transform_new(self, obs):\n        \"\"\"\n        obs: pandas dataframe\n        \"\"\"\n\n        x_obs = obs[self.x_col].values\n\n        # Predict\n        obs_out = pd.DataFrame({\"pred\": (self.clf_random).predict(x_obs)})\n\n        obs_out[\"prob_0\"], obs_out[\"prob_1\"], obs_out[\"prob_2\"] = np.transpose(\n            (self.clf_random).predict_proba(x_obs)\n        )\n\n        return obs_out\n"}
{"type": "source_file", "path": "examples/vanilla/src/data_sourcing.py", "content": "import numpy as np\nimport pandas as pd\n\n\ndef get():\n    \"\"\"Get the data.\n\n    ** Vanilla definition. **\n    Include your own code below to import your project's data.\n\n    Parameters\n    ----------\n    None\n\n    Returns\n    -------\n    df: pandas dataframe\n\n    Examples\n    --------\n\n    Raises\n    ------\n\n    Notes\n    -----\n\n    \"\"\"\n\n    df = pd.DataFrame(\n        {\n            \"id\": [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"],\n            \"x\": [0.0, np.nan, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n            \"y\": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n        }\n    )\n\n    return df\n"}
{"type": "source_file", "path": "examples/vanilla/src/feature_engineering.py", "content": "from feature_engine import encoding, imputation\n\n# Note: we suggest using the below helper functions\n#       for missing imputation (for numerical) and\n#       one-hot-encoding (for categorical).\n#       You will find most of other popular Feature\n#       Engineering methods in the `feature_engine`\n#       python package.\n\n\ndef numerical_missing_imputation(train, valid, test, cols, imputation_method=\"median\"):\n    \"\"\"Missing imputation for numerical variables.\n\n    The algorithm learns from the train set and applies transformations\n    to all three input datasets: train, valid, test.\n\n    Parameters\n    ----------\n    train: pandas dataframe\n        Training set\n\n    valid: pandas dataframe\n        Validation set\n\n    test: pandas dataframe\n        Test set\n\n    cols: list\n        List of numerical columns\n\n    imputation_method: string\n        Desired method of imputation. Options are 'mean' and 'median'.\n        Default value: 'median'.\n\n    Returns\n    -------\n    list\n        (train, valid, test)\n        (pandas dataframe, pandas dataframe, pandas dataframe)\n\n    Examples\n    --------\n\n    Raises\n    ------\n\n    Notes\n    -----\n\n    \"\"\"\n\n    fe = imputation.MeanMedianImputer(\n        imputation_method=imputation_method, variables=cols\n    )\n\n    # Fit over training set\n    fe.fit(train[cols])\n\n    # Apply to train, valid, test\n    return (\n        fe.transform(train[cols]),\n        fe.transform(valid[cols]),\n        fe.transform(test[cols]),\n    )\n\n\ndef one_hot_encoding(train, valid, test, cols):\n    \"\"\"One-hot-encoding of all categories found in `cols`.\n\n    The algorithm learns from the train set and applies transformations\n    to all three input datasets: train, valid, test.\n\n    Missing values in col lead to col_na=1\n\n    Parameters\n    ----------\n    train: pandas dataframe\n        Training set\n\n    valid: pandas dataframe\n        Validation set\n\n    test: pandas dataframe\n        Test set\n\n    cols: list\n        List of numerical columns\n\n    Returns\n    -------\n    list\n        (train, valid, test)\n        (pandas dataframe, pandas dataframe, pandas dataframe)\n\n    Examples\n    --------\n\n    Raises\n    ------\n\n    Notes\n    -----\n\n    \"\"\"\n\n    fe = encoding.OneHotEncoder(variables=cols)\n\n    for k in cols:\n        train[k] = train[k].fillna(\"na\")\n        valid[k] = valid[k].fillna(\"na\")\n        test[k] = test[k].fillna(\"na\")\n\n    # Fit over training set\n    fe.fit(train[cols])\n\n    # Apply to train, valid, test\n    return (\n        fe.transform(train[cols]),\n        fe.transform(valid[cols]),\n        fe.transform(test[cols]),\n    )\n"}
{"type": "source_file", "path": "examples/vanilla/src/main.py", "content": "import greenhouse_clock\nimport data_sourcing\nimport data_splitting\nimport data_preprocessing\nimport feature_engineering\nfrom modeling import model\nimport performance_monitoring\n\nstart_time = greenhouse_clock.get_time()\n\nif __name__ == \"__main__\":\n\n    # Run prefect flow\n    df = data_sourcing.get()\n    df = data_preprocessing.clean(df)\n    df = data_preprocessing.normalize(df)\n\n    train, valid, test = data_splitting.split(df)\n\n    (\n        train[\"x\"],\n        valid[\"x\"],\n        test[\"x\"],\n    ) = feature_engineering.numerical_missing_imputation(\n        train=train, valid=valid, test=test, cols=[\"x\"], imputation_method=\"median\"\n    )\n\n    m = model().fit(train=train, y_col=\"y\", x_col=\"x\")\n\n    train[\"pred\"], valid[\"pred\"], test[\"pred\"] = m.transform_sets(train, valid, test)\n\n    performance_monitoring.report_performance(\n        y_true=valid[\"y\"],\n        y_score=valid[\"pred\"],\n        path=\"/usr/app/monitor/\",\n        suffix=\"_valid\",\n    )\n"}
{"type": "source_file", "path": "examples/vanilla/src/data_splitting.py", "content": "def split(df, train_ratio=0.8, valid_ratio=0.1, test_ratio=0.1, seed=0):\n    \"\"\"Data splitting into 3 sets: train, valid, test\n\n    train: training set. Used for training the ML model.\n    valid: validation set. Used for frequent validation.\n    test: test set. Used for final test.\n\n    Parameters\n    ----------\n    df: pandas dataframe\n        Input data\n\n    train_ratio: float\n        Amount of data that goes into training, in percentage\n\n    valid_ratio: float\n        Amount of data that goes into validation, in percentage\n\n    test_ratio: float\n        Amount of data that goes into testing, in percentage\n\n    seed: int\n        Seed for the data shuffling.\n        It is important to keep it fixed throughout the tuning of the model.\n\n    Returns\n    -------\n    list\n        (train, valid, test)\n        (pandas dataframe, pandas dataframe, pandas dataframe)\n\n    Examples\n    --------\n\n    >>> len(data)\n    100\n    >>> train, valid, test = split(data)\n    >>> len(train)\n    80\n    >>> len(valid)\n    10\n    >>> len(test)\n    10\n\n    Raises\n    ------\n\n    Notes\n    -----\n\n    \"\"\"\n\n    # Train set extracted from a random sample from `df`\n    train = df.sample(frac=train_ratio, random_state=seed)\n\n    # Everything from `df` except `train`\n    rest = df.copy().drop(train.index)\n\n    # Valid set ratio within `rest`\n    new_ratio = valid_ratio / (valid_ratio + test_ratio)\n\n    # Train set extracted from a random sample from `rest`\n    valid = rest.sample(frac=new_ratio, random_state=seed)\n\n    # Test set is everything in rest `except` for `valid`\n    test = rest.drop(valid.index)\n\n    return train, valid, test\n"}
{"type": "source_file", "path": "src/main.py", "content": "import greenhouse_clock\nimport data_sourcing\nimport data_splitting\nimport data_preprocessing\nimport feature_engineering\nfrom modeling import model\nimport performance_monitoring\n\nstart_time = greenhouse_clock.get_time()\n\nif __name__ == \"__main__\":\n\n    # Run prefect flow\n    df = data_sourcing.get()\n    df = data_preprocessing.clean(df)\n    df = data_preprocessing.normalize(df)\n\n    train, valid, test = data_splitting.split(df)\n\n    (\n        train[\"x\"],\n        valid[\"x\"],\n        test[\"x\"],\n    ) = feature_engineering.numerical_missing_imputation(\n        train=train, valid=valid, test=test, cols=[\"x\"], imputation_method=\"median\"\n    )\n\n    m = model().fit(train=train, y_col=\"y\", x_col=\"x\")\n\n    train[\"pred\"], valid[\"pred\"], test[\"pred\"] = m.transform_sets(train, valid, test)\n\n    performance_monitoring.report_performance(\n        y_true=valid[\"y\"],\n        y_score=valid[\"pred\"],\n        path=\"/usr/app/monitor/\",\n        suffix=\"_valid\",\n    )\n"}
{"type": "source_file", "path": "examples/vanilla/src/modeling.py", "content": "class VanillaModel:\n    \"\"\"Vanilla model where the predictions are always 0\"\"\"\n\n    def __init__(self):\n\n        pass\n\n    def fit(self, x, y):\n\n        self.fitted = [0]\n\n        return self\n\n    def predict(self, x):\n\n        return self.fitted * len(x)\n\n\nclass model:\n    \"\"\"\n    Replace below `VanillaModel` by an actual ML\n    model such as the ones provided by sklearn.\n\n    We are assuming supervised models (a and y are available),\n    but you may also adapt it for unsupervised models\n    (only x available). In that case, erase any reference to\n    `y` below.\n\n    References\n    ----------\n    https://scikit-learn.org/stable/\n    \"\"\"\n\n    def __init__(self):\n\n        pass\n\n    def fit(self, train, y_col, x_col):\n\n        self.x_col = x_col\n        self.y_col = y_col\n\n        self.m = VanillaModel().fit(x=train[x_col], y=train[y_col])\n\n        # Save your model in /models\n\n        # Note: for saving your model, we suggest using the\n        #       `joblib` python package\n\n        # Ex:   path \"/usr/app/models/\"\n        #       joblib.dump(self.m, path)\n\n        return self\n\n    def transform_sets(self, train, valid, test):\n\n        x_train = train[self.x_col].values\n        x_valid = valid[self.x_col].values\n        x_test = test[self.x_col].values\n\n        y_train = train[self.y_col].values\n        y_valid = valid[self.y_col].values\n        y_test = test[self.y_col].values\n\n        train_out = train.copy(deep=True)[self.y_col]\n        valid_out = valid.copy(deep=True)[self.y_col]\n        test_out = test.copy(deep=True)[self.y_col]\n\n        train_out[\"actual\"] = y_train\n        valid_out[\"actual\"] = y_valid\n        test_out[\"actual\"] = y_test\n\n        # Predict\n        train_out[\"pred\"] = (self.m).predict(x_train)\n        valid_out[\"pred\"] = (self.m).predict(x_valid)\n        test_out[\"pred\"] = (self.m).predict(x_test)\n\n        return train_out, valid_out, test_out\n"}
{"type": "source_file", "path": "examples/vanilla/src/greenhouse_clock.py", "content": "import time\n\n\ndef get_time(format=\"%Y%m%d%H%M%S\"):\n\n    return time.strftime(format)\n"}
{"type": "source_file", "path": "examples/vanilla/src/performance_monitoring.py", "content": "import json\nimport greenhouse_clock\n\nmeta = {}\n\n# Timestamp for files\nmeta[\"timestr\"] = greenhouse_clock.get_time()\n\n\ndef report_performance(y_true, y_score, path, suffix=\"\"):\n    \"\"\"\n\n    We suggest using `sklearn.metrics.classification_report`\n\n    References\n    ----------\n    https://scikit-learn.org/stable/modules/generated/\n    sklearn.metrics.classification_report.html\n    \"\"\"\n\n    # Plug-in here your performance metrics as dictionary entries\n    meta[\"performance_metric_name\"] = 0\n\n    filename = \"{0}metadata{1}.json\".format(path, suffix)\n\n    # Export to JSON\n    with open(filename, \"w\") as fp:\n        json.dump(meta, fp, indent=4)\n\n    pass\n"}
{"type": "source_file", "path": "src/data_sourcing.py", "content": "import numpy as np\nimport pandas as pd\n\n\ndef get():\n    \"\"\"Get the data.\n\n    ** Vanilla definition. **\n    Include your own code below to import your project's data.\n\n    Parameters\n    ----------\n    None\n\n    Returns\n    -------\n    df: pandas dataframe\n\n    Examples\n    --------\n\n    Raises\n    ------\n\n    Notes\n    -----\n\n    \"\"\"\n\n    df = pd.DataFrame(\n        {\n            \"id\": [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"],\n            \"x\": [0.0, np.nan, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n            \"y\": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n        }\n    )\n\n    return df\n"}
{"type": "source_file", "path": "src/feature_engineering.py", "content": "from feature_engine import encoding, imputation\n\n# Note: we suggest using the below helper functions\n#       for missing imputation (for numerical) and\n#       one-hot-encoding (for categorical).\n#       You will find most of other popular Feature\n#       Engineering methods in the `feature_engine`\n#       python package.\n\n\ndef numerical_missing_imputation(train, valid, test, cols, imputation_method=\"median\"):\n    \"\"\"Missing imputation for numerical variables.\n\n    The algorithm learns from the train set and applies transformations\n    to all three input datasets: train, valid, test.\n\n    Parameters\n    ----------\n    train: pandas dataframe\n        Training set\n\n    valid: pandas dataframe\n        Validation set\n\n    test: pandas dataframe\n        Test set\n\n    cols: list\n        List of numerical columns\n\n    imputation_method: string\n        Desired method of imputation. Options are 'mean' and 'median'.\n        Default value: 'median'.\n\n    Returns\n    -------\n    list\n        (train, valid, test)\n        (pandas dataframe, pandas dataframe, pandas dataframe)\n\n    Examples\n    --------\n\n    Raises\n    ------\n\n    Notes\n    -----\n\n    \"\"\"\n\n    fe = imputation.MeanMedianImputer(\n        imputation_method=imputation_method, variables=cols\n    )\n\n    # Fit over training set\n    fe.fit(train[cols])\n\n    # Apply to train, valid, test\n    return (\n        fe.transform(train[cols]),\n        fe.transform(valid[cols]),\n        fe.transform(test[cols]),\n    )\n\n\ndef one_hot_encoding(train, valid, test, cols):\n    \"\"\"One-hot-encoding of all categories found in `cols`.\n\n    The algorithm learns from the train set and applies transformations\n    to all three input datasets: train, valid, test.\n\n    Missing values in col lead to col_na=1\n\n    Parameters\n    ----------\n    train: pandas dataframe\n        Training set\n\n    valid: pandas dataframe\n        Validation set\n\n    test: pandas dataframe\n        Test set\n\n    cols: list\n        List of numerical columns\n\n    Returns\n    -------\n    list\n        (train, valid, test)\n        (pandas dataframe, pandas dataframe, pandas dataframe)\n\n    Examples\n    --------\n\n    Raises\n    ------\n\n    Notes\n    -----\n\n    \"\"\"\n\n    fe = encoding.OneHotEncoder(variables=cols)\n\n    for k in cols:\n        train[k] = train[k].fillna(\"na\")\n        valid[k] = valid[k].fillna(\"na\")\n        test[k] = test[k].fillna(\"na\")\n\n    # Fit over training set\n    fe.fit(train[cols])\n\n    # Apply to train, valid, test\n    return (\n        fe.transform(train[cols]),\n        fe.transform(valid[cols]),\n        fe.transform(test[cols]),\n    )\n"}
{"type": "source_file", "path": "src/__main__.py", "content": ""}
{"type": "source_file", "path": "src/__init__.py", "content": ""}
{"type": "source_file", "path": "src/eda_monitoring.py", "content": "import pandas_profiling\n\n\ndef export_eda_report(df, path, preffix, suffix):\n\n    profile = pandas_profiling.ProfileReport(df, title=\"Pandas Profiling Report\")\n\n    path = \"{}/{}_ead_monitoring_{}.html\".format(path, preffix, suffix)\n\n    profile.to_file(path)\n\n    pass\n"}
{"type": "source_file", "path": "src/data_splitting.py", "content": "def split(df, train_ratio=0.8, valid_ratio=0.1, test_ratio=0.1, seed=0):\n    \"\"\"Data splitting into 3 sets: train, valid, test\n\n    train: training set. Used for training the ML model.\n    valid: validation set. Used for frequent validation.\n    test: test set. Used for final test.\n\n    Parameters\n    ----------\n    df: pandas dataframe\n        Input data\n\n    train_ratio: float\n        Amount of data that goes into training, in percentage\n\n    valid_ratio: float\n        Amount of data that goes into validation, in percentage\n\n    test_ratio: float\n        Amount of data that goes into testing, in percentage\n\n    seed: int\n        Seed for the data shuffling.\n        It is important to keep it fixed throughout the tuning of the model.\n\n    Returns\n    -------\n    list\n        (train, valid, test)\n        (pandas dataframe, pandas dataframe, pandas dataframe)\n\n    Examples\n    --------\n\n    >>> len(data)\n    100\n    >>> train, valid, test = split(data)\n    >>> len(train)\n    80\n    >>> len(valid)\n    10\n    >>> len(test)\n    10\n\n    Raises\n    ------\n\n    Notes\n    -----\n\n    \"\"\"\n\n    # Train set extracted from a random sample from `df`\n    train = df.sample(frac=train_ratio, random_state=seed)\n\n    # Everything from `df` except `train`\n    rest = df.copy().drop(train.index)\n\n    # Valid set ratio within `rest`\n    new_ratio = valid_ratio / (valid_ratio + test_ratio)\n\n    # Train set extracted from a random sample from `rest`\n    valid = rest.sample(frac=new_ratio, random_state=seed)\n\n    # Test set is everything in rest `except` for `valid`\n    test = rest.drop(valid.index)\n\n    return train, valid, test\n"}
{"type": "source_file", "path": "src/api.py", "content": "from fastapi import FastAPI\nfrom pydantic import BaseModel\n\nfrom src.modeling import VanillaModel\n\napp = FastAPI()\n\n\nclass ModelIn(BaseModel):\n    x: str\n\n\nclass ModelOut(BaseModel):\n    pred: float\n\n\nclass ModelOutHealth(BaseModel):\n    id: str\n\n\napp = FastAPI()\n\n\n@app.post(\"/health\")\nasync def health():\n\n    return {\"id\": \"Healthy\"}\n\n\n@app.post(\n    \"/predict/\",\n    response_model=ModelOut,\n)\nasync def root(input: ModelIn):\n\n    X = [\n        float(input.x),\n    ]\n\n    # Load your model from /models\n\n    # Note: for saving your model, we suggest using the\n    #       `joblib` python package\n\n    # Ex:   path \"/usr/app/models/\"\n    #       joblib.dump(self.m, path)\n    #       model = joblib.load(path)\n\n    # Vanila model always predict 0, so that\n    # inputs in the training phase are arbitrary\n    model = VanillaModel().fit(x=[0], y=[0])\n\n    out_dict = {}\n\n    out_dict[\"pred\"] = model.predict(X)[0]\n\n    return out_dict\n"}
{"type": "source_file", "path": "src/data_preprocessing.py", "content": "def clean(df):\n    \"\"\"Cleansing: a data pre-processing step. Usually, getting rid of garbage\n    such as undesired characters.\n\n    Cleansing must be a set of operations independent of data splitting.\n\n    ** Vanilla definition. **\n    Include your own code below to import your project's data.\n\n    Parameters\n    ----------\n    df: pandas dataframe\n\n    Returns\n    -------\n    pandas dataframe\n        Cleansed dataframe\n\n    Examples\n    --------\n\n    Raises\n    ------\n\n    Notes\n    -----\n\n    \"\"\"\n\n    return df\n\n\ndef normalize(df):\n    \"\"\"Normalization: a data pre-processing step. Usually, making adjusting\n    loser and upper casing, abbrevations, word order, and so on.\n\n    Normalization must be a set of operations independent of data splitting.\n\n    ** Vanilla definition. **\n    Include your own code below to import your project's data.\n\n    Parameters\n    ----------\n    df: pandas dataframe\n\n    Returns\n    -------\n    pandas dataframe\n        Normalized dataframe\n\n    Examples\n    --------\n\n    Raises\n    ------\n\n    Notes\n    -----\n\n    \"\"\"\n\n    return df\n"}
{"type": "source_file", "path": "src/greenhouse_clock.py", "content": "import time\n\n\ndef get_time(format=\"%Y%m%d%H%M%S\"):\n\n    return time.strftime(format)\n"}
{"type": "source_file", "path": "src/modeling.py", "content": "class VanillaModel:\n    \"\"\"Vanilla model where the predictions are always 0\"\"\"\n\n    def __init__(self):\n\n        pass\n\n    def fit(self, x, y):\n\n        self.fitted = [0]\n\n        return self\n\n    def predict(self, x):\n\n        return self.fitted * len(x)\n\n\nclass model:\n    \"\"\"\n    Replace below `VanillaModel` by an actual ML\n    model such as the ones provided by sklearn.\n\n    We are assuming supervised models (a and y are available),\n    but you may also adapt it for unsupervised models\n    (only x available). In that case, erase any reference to\n    `y` below.\n\n    References\n    ----------\n    https://scikit-learn.org/stable/\n    \"\"\"\n\n    def __init__(self):\n\n        pass\n\n    def fit(self, train, y_col, x_col):\n\n        self.x_col = x_col\n        self.y_col = y_col\n\n        self.m = VanillaModel().fit(x=train[x_col], y=train[y_col])\n\n        # Save your model in /models\n\n        # Note: for saving your model, we suggest using the\n        #       `joblib` python package\n\n        # Ex:   path \"/usr/app/models/\"\n        #       joblib.dump(self.m, path)\n\n        return self\n\n    def transform_sets(self, train, valid, test):\n\n        x_train = train[self.x_col].values\n        x_valid = valid[self.x_col].values\n        x_test = test[self.x_col].values\n\n        y_train = train[self.y_col].values\n        y_valid = valid[self.y_col].values\n        y_test = test[self.y_col].values\n\n        train_out = train.copy(deep=True)[self.y_col]\n        valid_out = valid.copy(deep=True)[self.y_col]\n        test_out = test.copy(deep=True)[self.y_col]\n\n        train_out[\"actual\"] = y_train\n        valid_out[\"actual\"] = y_valid\n        test_out[\"actual\"] = y_test\n\n        # Predict\n        train_out[\"pred\"] = (self.m).predict(x_train)\n        valid_out[\"pred\"] = (self.m).predict(x_valid)\n        test_out[\"pred\"] = (self.m).predict(x_test)\n\n        return train_out, valid_out, test_out\n"}
{"type": "source_file", "path": "src/performance_monitoring.py", "content": "import json\nimport greenhouse_clock\n\nmeta = {}\n\n# Timestamp for files\nmeta[\"timestr\"] = greenhouse_clock.get_time()\n\n\ndef report_performance(y_true, y_score, path, suffix=\"\"):\n    \"\"\"\n\n    We suggest using `sklearn.metrics.classification_report`\n\n    References\n    ----------\n    https://scikit-learn.org/stable/modules/generated/\n    sklearn.metrics.classification_report.html\n    \"\"\"\n\n    # Plug-in here your performance metrics as dictionary entries\n    meta[\"performance_metric_name\"] = 0\n\n    filename = \"{0}metadata{1}.json\".format(path, suffix)\n\n    # Export to JSON\n    with open(filename, \"w\") as fp:\n        json.dump(meta, fp, indent=4)\n\n    pass\n"}
