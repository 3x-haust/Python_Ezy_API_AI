{"repo_info": {"repo_name": "LightRAG-for-OpenAI-Standard-Frontend", "repo_owner": "HerSophia", "repo_url": "https://github.com/HerSophia/LightRAG-for-OpenAI-Standard-Frontend"}}
{"type": "source_file", "path": "Gradio/formatted_json.py", "content": "import json\nimport os\n\n\ndef format_json_file(input_file_path, output_file_path):\n    \"\"\"\n    è¯»å– JSON æ–‡ä»¶ï¼Œæ ¼å¼åŒ–å…¶å†…å®¹å¹¶å†™å…¥åˆ°ä¸€ä¸ªæ–°æ–‡ä»¶ã€‚\n\n    :param input_file_path: è¾“å…¥ JSON æ–‡ä»¶çš„ç›¸å¯¹è·¯å¾„\n    :param output_file_path: è¾“å‡ºæ ¼å¼åŒ– JSON æ–‡ä»¶çš„ç›¸å¯¹è·¯å¾„\n    \"\"\"\n    try:\n        # æ£€æŸ¥è¾“å…¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨\n        if not os.path.exists(input_file_path):\n            print(f\"æ–‡ä»¶ {input_file_path} ä¸å­˜åœ¨ï¼\")\n            return\n\n        # è¯»å– JSON æ–‡ä»¶å†…å®¹\n        with open(input_file_path, 'r', encoding='utf-8') as input_file:\n            data = json.load(input_file)\n\n        # æ ¼å¼åŒ– JSON å†…å®¹\n        formatted_json = json.dumps(data, indent=4, ensure_ascii=False)\n\n        # è¾“å‡ºæ ¼å¼åŒ–å†…å®¹åˆ°æ–°æ–‡ä»¶\n        with open(output_file_path, 'w', encoding='utf-8') as output_file:\n            output_file.write(formatted_json)\n\n        print(\"æ ¼å¼åŒ–åçš„ JSON å†…å®¹ï¼š\")\n        print(formatted_json)\n        print(f\"\\næ ¼å¼åŒ–åçš„å†…å®¹å·²ä¿å­˜åˆ° {output_file_path}\")\n\n    except json.JSONDecodeError as e:\n        print(f\"JSON è§£ç é”™è¯¯ï¼š{e}\")\n    except Exception as e:\n        print(f\"å‘ç”Ÿé”™è¯¯ï¼š{e}\")\n\n\n# ç¤ºä¾‹è°ƒç”¨\ninput_file = \"./test1.json\"  # è¾“å…¥æ–‡ä»¶è·¯å¾„\noutput_file = \"./formatted_output.json\"  # è¾“å‡ºæ–‡ä»¶è·¯å¾„\nformat_json_file(input_file, output_file)\n"}
{"type": "source_file", "path": "Gradio/Lorebook.py", "content": "import json\n\nclass Entry:\n    \"\"\"\n    å®šä¹‰å•ä¸ª entry çš„æ•°æ®ç»“æ„\n    \"\"\"\n    \"\"\"\n    uidï¼šuid\n    keyï¼šå…³é”®è¯\n    commentï¼šæ ‡é¢˜\n    contentï¼šæ¡ç›®å†…å®¹\n    orderï¼šé¡ºåºï¼Œä»¥å¯¹llmå½±å“çš„æ•ˆæœåˆ†çº§å¦‚ä¸‹ï¼š0-è§’è‰²å®šä¹‰ä¹‹å‰ï¼ˆæœ€å¼±ï¼‰ï¼Œ1-è§’è‰²å®šä¹‰ä¹‹åï¼ˆæ¬¡å¼±ï¼‰ï¼Œ2-ä½œè€…æ³¨é‡Šä¹‹å‰ï¼ˆè¾ƒå¼ºï¼‰ï¼Œ3-ä½œè€…æ³¨é‡Šä¹‹åï¼ˆæå¼ºï¼‰ï¼Œ4-@ Dï¼ˆå¯å˜ï¼‰ï¼Œ5-ç¤ºä¾‹æ¶ˆæ¯ä¹‹å‰ï¼Œ6-ç¤ºä¾‹æ¶ˆæ¯ä¹‹å‰ï¼Œ\n    positionï¼šä½ç½®ï¼Œé»˜è®¤ä¸º100ï¼Œå½±å“æ•ˆæœæ€»ä½“å—æ·±åº¦é™åˆ¶\n    depthï¼šæ·±åº¦ï¼ŒæŒ‡æ’å…¥ç¬¬Xæ¡æ¶ˆæ¯çš„ä¸Šæ–¹ã€‚é»˜è®¤ä¸º4,0ä¸ºæœ€å¼º\n    probabilityï¼šæ¦‚ç‡ï¼Œ0ä¸ºä¸è§¦å‘\n    \"\"\"\n    def __init__(self, uid, key, comment, content, order, position, depth, probability):\n        self.uid = uid\n        self.key = key\n        self.comment = comment\n        self.content = content\n        self.order = order\n        self.position = position\n        self.depth = depth\n        self.probability = probability\n\n    def __str__(self):\n        \"\"\"\n        å®šä¹‰æ‰“å°æ ¼å¼\n        \"\"\"\n        return (\n            f\"UID: {self.uid}\\n\"\n            f\"Key: {self.key}\\n\"\n            f\"Comment: {self.comment}\\n\"\n            f\"Content:\\n{self.content}\\n\"\n            f\"Order: {self.order}\\n\"\n            f\"Position: {self.position}\\n\"\n            f\"Depth: {self.depth}\\n\"\n            f\"Probability: {self.probability}\\n\"\n        )\n\nclass JSONManager:\n    \"\"\"\n    ç®¡ç†æ•´ä¸ª JSON æ•°æ®ï¼Œæä¾›æå–å’Œæ“ä½œåŠŸèƒ½\n    \"\"\"\n    def __init__(self, json_file_path):\n        self.file_path = json_file_path\n        self.entries = self._load_entries()\n\n    def _load_entries(self):\n        \"\"\"\n        åŠ è½½ JSON æ–‡ä»¶å¹¶è½¬æ¢ä¸º Entry å®ä¾‹åˆ—è¡¨\n        \"\"\"\n        try:\n            with open(self.file_path, 'r', encoding='utf-8') as file:\n                data = json.load(file)\n            entries = data.get(\"entries\", {})\n            return [\n                Entry(\n                    uid=entry_data[\"uid\"],\n                    key=entry_data[\"key\"],\n                    comment=entry_data[\"comment\"],\n                    content=entry_data[\"content\"],\n                    order=entry_data[\"order\"],\n                    position=entry_data[\"position\"],\n                    depth=entry_data[\"depth\"],\n                    probability=entry_data[\"probability\"]\n                )\n                for entry_data in entries.values()\n            ]\n        except Exception as e:\n            print(f\"Error loading JSON file: {e}\")\n            return []\n\n    def get_entry_by_uid(self, uid):\n        \"\"\"\n        æ ¹æ® UID è·å–å•ä¸ª Entry\n        \"\"\"\n        for entry in self.entries:\n            if entry.uid == uid:\n                return entry\n        return None\n\n    def get_entries_by_key(self, key):\n        \"\"\"\n        æ ¹æ® key æœç´¢å¹¶è¿”å›åŒ¹é…çš„ Entry åˆ—è¡¨\n        \"\"\"\n        return [entry for entry in self.entries if key in entry.key]\n\n    def get_all_entries(self):\n        \"\"\"\n        è¿”å›æ‰€æœ‰ Entry çš„åˆ—è¡¨\n        \"\"\"\n        return self.entries\n\nclass EntrySorter:\n    \"\"\"\n    ç”¨äºæ’åºå’Œè¾“å‡ºæ¡ç›®\n    \"\"\"\n\n    def __init__(self, entries):\n        \"\"\"\n        åˆå§‹åŒ–ï¼ŒåŠ è½½ Entry åˆ—è¡¨\n        :param entries: List of Entry objects\n        \"\"\"\n        self.entries = entries\n\n    def filter_and_sort(self):\n        \"\"\"\n        è¿‡æ»¤å’Œæ’åºæ¡ç›®\n        :return: æ’åºåçš„ Entry åˆ—è¡¨\n        \"\"\"\n        # è¿‡æ»¤ probability ä¸º 0 çš„æ¡ç›®\n        valid_entries = [entry for entry in self.entries if entry.probability > 0]\n\n        # æŒ‰ä¼˜å…ˆçº§æ’åºï¼šorder (é™åº) -> depth (å‡åº) -> position (å‡åº)\n        sorted_entries = sorted(\n            valid_entries,\n            key=lambda x: (-x.order, x.depth, x.position)\n        )\n        return sorted_entries\n\n    def export_to_txt(self, output_file):\n        \"\"\"\n        å¯¼å‡ºæ’åºç»“æœåˆ° txt æ–‡ä»¶\n        :param output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„\n        \"\"\"\n        sorted_entries = self.filter_and_sort()\n\n        with open(output_file, 'w', encoding='utf-8') as file:\n            for entry in sorted_entries:\n                file.write(f\"{entry.comment} - {','.join(entry.key)} - {entry.content}\\n\")\n        print(f\"æ’åºåçš„æ¡ç›®å·²ä¿å­˜åˆ° {output_file}\")\n\n\n# ç¤ºä¾‹è°ƒç”¨\nif __name__ == \"__main__\":\n    # æ›¿æ¢ä¸ºæ‚¨çš„ JSON æ–‡ä»¶è·¯å¾„\n    file_path = \"./æ–—ç½—.json\"\n\n    # åˆå§‹åŒ– JSONManager\n    manager = JSONManager(file_path)\n\n    # è·å–æ‰€æœ‰æ¡ç›®\n    all_entries = manager.get_all_entries()\n\n\n    # åˆ›å»º EntrySorter å¹¶å¯¼å‡ºæ’åºç»“æœ\n    sorter = EntrySorter(all_entries)\n    output_path = \"sorted_entries.txt\"\n    sorter.export_to_txt(output_path)\n\n    \"\"\"\n    print(\"æ‰€æœ‰æ¡ç›®:\")\n    for entry in all_entries:\n        print(entry)\n        print(\"-\" * 30)\n\n    # æ ¹æ® UID è·å–æ¡ç›®\n    uid = 1\n    specific_entry = manager.get_entry_by_uid(uid)\n    print(f\"UID ä¸º {uid} çš„æ¡ç›®:\")\n    print(specific_entry)\n\n    # æ ¹æ® Key æœç´¢æ¡ç›®\n    search_key = \"44\"\n    matching_entries = manager.get_entries_by_key(search_key)\n    print(f\"Key åŒ…å« '{search_key}' çš„æ¡ç›®:\")\n    for entry in matching_entries:\n        print(entry)\n    \"\"\"\n"}
{"type": "source_file", "path": "examples/lightrag_azure_openai_demo.py", "content": "import os\nimport asyncio\nfrom lightrag import LightRAG, QueryParam\nfrom lightrag.utils import EmbeddingFunc\nimport numpy as np\nfrom dotenv import load_dotenv\nimport logging\nfrom openai import AzureOpenAI\n\nlogging.basicConfig(level=logging.INFO)\n\nload_dotenv()\n\nAZURE_OPENAI_API_VERSION = os.getenv(\"AZURE_OPENAI_API_VERSION\")\nAZURE_OPENAI_DEPLOYMENT = os.getenv(\"AZURE_OPENAI_DEPLOYMENT\")\nAZURE_OPENAI_API_KEY = os.getenv(\"AZURE_OPENAI_API_KEY\")\nAZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n\nAZURE_EMBEDDING_DEPLOYMENT = os.getenv(\"AZURE_EMBEDDING_DEPLOYMENT\")\nAZURE_EMBEDDING_API_VERSION = os.getenv(\"AZURE_EMBEDDING_API_VERSION\")\n\nWORKING_DIR = \"./dickens\"\n\nif os.path.exists(WORKING_DIR):\n    import shutil\n\n    shutil.rmtree(WORKING_DIR)\n\nos.mkdir(WORKING_DIR)\n\n\nasync def llm_model_func(\n    prompt, system_prompt=None, history_messages=[], keyword_extraction=False, **kwargs\n) -> str:\n    client = AzureOpenAI(\n        api_key=AZURE_OPENAI_API_KEY,\n        api_version=AZURE_OPENAI_API_VERSION,\n        azure_endpoint=AZURE_OPENAI_ENDPOINT,\n    )\n\n    messages = []\n    if system_prompt:\n        messages.append({\"role\": \"system\", \"content\": system_prompt})\n    if history_messages:\n        messages.extend(history_messages)\n    messages.append({\"role\": \"user\", \"content\": prompt})\n\n    chat_completion = client.chat.completions.create(\n        model=AZURE_OPENAI_DEPLOYMENT,  # model = \"deployment_name\".\n        messages=messages,\n        temperature=kwargs.get(\"temperature\", 0),\n        top_p=kwargs.get(\"top_p\", 1),\n        n=kwargs.get(\"n\", 1),\n    )\n    return chat_completion.choices[0].message.content\n\n\nasync def embedding_func(texts: list[str]) -> np.ndarray:\n    client = AzureOpenAI(\n        api_key=AZURE_OPENAI_API_KEY,\n        api_version=AZURE_EMBEDDING_API_VERSION,\n        azure_endpoint=AZURE_OPENAI_ENDPOINT,\n    )\n    embedding = client.embeddings.create(model=AZURE_EMBEDDING_DEPLOYMENT, input=texts)\n\n    embeddings = [item.embedding for item in embedding.data]\n    return np.array(embeddings)\n\n\nasync def test_funcs():\n    result = await llm_model_func(\"How are you?\")\n    print(\"Resposta do llm_model_func: \", result)\n\n    result = await embedding_func([\"How are you?\"])\n    print(\"Resultado do embedding_func: \", result.shape)\n    print(\"DimensÃ£o da embedding: \", result.shape[1])\n\n\nasyncio.run(test_funcs())\n\nembedding_dimension = 3072\n\nrag = LightRAG(\n    working_dir=WORKING_DIR,\n    llm_model_func=llm_model_func,\n    embedding_func=EmbeddingFunc(\n        embedding_dim=embedding_dimension,\n        max_token_size=8192,\n        func=embedding_func,\n    ),\n)\n\nbook1 = open(\"./book_1.txt\", encoding=\"utf-8\")\nbook2 = open(\"./book_2.txt\", encoding=\"utf-8\")\n\nrag.insert([book1.read(), book2.read()])\n\nquery_text = \"What are the main themes?\"\n\nprint(\"Result (Naive):\")\nprint(rag.query(query_text, param=QueryParam(mode=\"naive\")))\n\nprint(\"\\nResult (Local):\")\nprint(rag.query(query_text, param=QueryParam(mode=\"local\")))\n\nprint(\"\\nResult (Global):\")\nprint(rag.query(query_text, param=QueryParam(mode=\"global\")))\n\nprint(\"\\nResult (Hybrid):\")\nprint(rag.query(query_text, param=QueryParam(mode=\"hybrid\")))\n"}
{"type": "source_file", "path": "examples/insert_custom_kg.py", "content": "import os\nfrom lightrag import LightRAG\nfrom lightrag.llm import gpt_4o_mini_complete\n#########\n# Uncomment the below two lines if running in a jupyter notebook to handle the async nature of rag.insert()\n# import nest_asyncio\n# nest_asyncio.apply()\n#########\n\nWORKING_DIR = \"./custom_kg\"\n\nif not os.path.exists(WORKING_DIR):\n    os.mkdir(WORKING_DIR)\n\nrag = LightRAG(\n    working_dir=WORKING_DIR,\n    llm_model_func=gpt_4o_mini_complete,  # Use gpt_4o_mini_complete LLM model\n    # llm_model_func=gpt_4o_complete  # Optionally, use a stronger model\n)\n\ncustom_kg = {\n    \"entities\": [\n        {\n            \"entity_name\": \"CompanyA\",\n            \"entity_type\": \"Organization\",\n            \"description\": \"A major technology company\",\n            \"source_id\": \"Source1\",\n        },\n        {\n            \"entity_name\": \"ProductX\",\n            \"entity_type\": \"Product\",\n            \"description\": \"A popular product developed by CompanyA\",\n            \"source_id\": \"Source1\",\n        },\n        {\n            \"entity_name\": \"PersonA\",\n            \"entity_type\": \"Person\",\n            \"description\": \"A renowned researcher in AI\",\n            \"source_id\": \"Source2\",\n        },\n        {\n            \"entity_name\": \"UniversityB\",\n            \"entity_type\": \"Organization\",\n            \"description\": \"A leading university specializing in technology and sciences\",\n            \"source_id\": \"Source2\",\n        },\n        {\n            \"entity_name\": \"CityC\",\n            \"entity_type\": \"Location\",\n            \"description\": \"A large metropolitan city known for its culture and economy\",\n            \"source_id\": \"Source3\",\n        },\n        {\n            \"entity_name\": \"EventY\",\n            \"entity_type\": \"Event\",\n            \"description\": \"An annual technology conference held in CityC\",\n            \"source_id\": \"Source3\",\n        },\n    ],\n    \"relationships\": [\n        {\n            \"src_id\": \"CompanyA\",\n            \"tgt_id\": \"ProductX\",\n            \"description\": \"CompanyA develops ProductX\",\n            \"keywords\": \"develop, produce\",\n            \"weight\": 1.0,\n            \"source_id\": \"Source1\",\n        },\n        {\n            \"src_id\": \"PersonA\",\n            \"tgt_id\": \"UniversityB\",\n            \"description\": \"PersonA works at UniversityB\",\n            \"keywords\": \"employment, affiliation\",\n            \"weight\": 0.9,\n            \"source_id\": \"Source2\",\n        },\n        {\n            \"src_id\": \"CityC\",\n            \"tgt_id\": \"EventY\",\n            \"description\": \"EventY is hosted in CityC\",\n            \"keywords\": \"host, location\",\n            \"weight\": 0.8,\n            \"source_id\": \"Source3\",\n        },\n    ],\n    \"chunks\": [\n        {\n            \"content\": \"ProductX, developed by CompanyA, has revolutionized the market with its cutting-edge features.\",\n            \"source_id\": \"Source1\",\n        },\n        {\n            \"content\": \"PersonA is a prominent researcher at UniversityB, focusing on artificial intelligence and machine learning.\",\n            \"source_id\": \"Source2\",\n        },\n        {\n            \"content\": \"EventY, held in CityC, attracts technology enthusiasts and companies from around the globe.\",\n            \"source_id\": \"Source3\",\n        },\n        {\n            \"content\": \"None\",\n            \"source_id\": \"UNKNOWN\",\n        },\n    ],\n}\n\nrag.insert_custom_kg(custom_kg)\n"}
{"type": "source_file", "path": "examples/lightrag_api_openai_compatible_demo.py", "content": "from fastapi import FastAPI, HTTPException, File, UploadFile\nfrom pydantic import BaseModel\nimport os\nfrom lightrag import LightRAG, QueryParam\nfrom lightrag.llm import openai_complete_if_cache, openai_embedding\nfrom lightrag.utils import EmbeddingFunc\nimport numpy as np\nfrom typing import Optional\nimport asyncio\nimport nest_asyncio\n\n# Apply nest_asyncio to solve event loop issues\nnest_asyncio.apply()\n\nDEFAULT_RAG_DIR = \"index_default\"\napp = FastAPI(title=\"LightRAG API\", description=\"API for RAG operations\")\n\n# Configure working directory\nWORKING_DIR = os.environ.get(\"RAG_DIR\", f\"{DEFAULT_RAG_DIR}\")\nprint(f\"WORKING_DIR: {WORKING_DIR}\")\nLLM_MODEL = os.environ.get(\"LLM_MODEL\", \"gpt-4o-mini\")\nprint(f\"LLM_MODEL: {LLM_MODEL}\")\nEMBEDDING_MODEL = os.environ.get(\"EMBEDDING_MODEL\", \"text-embedding-3-large\")\nprint(f\"EMBEDDING_MODEL: {EMBEDDING_MODEL}\")\nEMBEDDING_MAX_TOKEN_SIZE = int(os.environ.get(\"EMBEDDING_MAX_TOKEN_SIZE\", 8192))\nprint(f\"EMBEDDING_MAX_TOKEN_SIZE: {EMBEDDING_MAX_TOKEN_SIZE}\")\n\nif not os.path.exists(WORKING_DIR):\n    os.mkdir(WORKING_DIR)\n\n\n# LLM model function\n\n\nasync def llm_model_func(\n    prompt, system_prompt=None, history_messages=[], keyword_extraction=False, **kwargs\n) -> str:\n    return await openai_complete_if_cache(\n        LLM_MODEL,\n        prompt,\n        system_prompt=system_prompt,\n        history_messages=history_messages,\n        **kwargs,\n    )\n\n\n# Embedding function\n\n\nasync def embedding_func(texts: list[str]) -> np.ndarray:\n    return await openai_embedding(\n        texts,\n        model=EMBEDDING_MODEL,\n    )\n\n\nasync def get_embedding_dim():\n    test_text = [\"This is a test sentence.\"]\n    embedding = await embedding_func(test_text)\n    embedding_dim = embedding.shape[1]\n    print(f\"{embedding_dim=}\")\n    return embedding_dim\n\n\n# Initialize RAG instance\nrag = LightRAG(\n    working_dir=WORKING_DIR,\n    llm_model_func=llm_model_func,\n    embedding_func=EmbeddingFunc(\n        embedding_dim=asyncio.run(get_embedding_dim()),\n        max_token_size=EMBEDDING_MAX_TOKEN_SIZE,\n        func=embedding_func,\n    ),\n)\n\n\n# Data models\n\n\nclass QueryRequest(BaseModel):\n    query: str\n    mode: str = \"hybrid\"\n    only_need_context: bool = False\n\n\nclass InsertRequest(BaseModel):\n    text: str\n\n\nclass Response(BaseModel):\n    status: str\n    data: Optional[str] = None\n    message: Optional[str] = None\n\n\n# API routes\n\n\n@app.post(\"/query\", response_model=Response)\nasync def query_endpoint(request: QueryRequest):\n    try:\n        loop = asyncio.get_event_loop()\n        result = await loop.run_in_executor(\n            None,\n            lambda: rag.query(\n                request.query,\n                param=QueryParam(\n                    mode=request.mode, only_need_context=request.only_need_context\n                ),\n            ),\n        )\n        return Response(status=\"success\", data=result)\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n\n\n@app.post(\"/insert\", response_model=Response)\nasync def insert_endpoint(request: InsertRequest):\n    try:\n        loop = asyncio.get_event_loop()\n        await loop.run_in_executor(None, lambda: rag.insert(request.text))\n        return Response(status=\"success\", message=\"Text inserted successfully\")\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n\n\n@app.post(\"/insert_file\", response_model=Response)\nasync def insert_file(file: UploadFile = File(...)):\n    try:\n        file_content = await file.read()\n        # Read file content\n        try:\n            content = file_content.decode(\"utf-8\")\n        except UnicodeDecodeError:\n            # If UTF-8 decoding fails, try other encodings\n            content = file_content.decode(\"gbk\")\n        # Insert file content\n        loop = asyncio.get_event_loop()\n        await loop.run_in_executor(None, lambda: rag.insert(content))\n\n        return Response(\n            status=\"success\",\n            message=f\"File content from {file.filename} inserted successfully\",\n        )\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n\n\n@app.get(\"/health\")\nasync def health_check():\n    return {\"status\": \"healthy\"}\n\n\nif __name__ == \"__main__\":\n    import uvicorn\n\n    uvicorn.run(app, host=\"0.0.0.0\", port=8020)\n\n# Usage example\n# To run the server, use the following command in your terminal:\n# python lightrag_api_openai_compatible_demo.py\n\n# Example requests:\n# 1. Query:\n# curl -X POST \"http://127.0.0.1:8020/query\" -H \"Content-Type: application/json\" -d '{\"query\": \"your query here\", \"mode\": \"hybrid\"}'\n\n# 2. Insert text:\n# curl -X POST \"http://127.0.0.1:8020/insert\" -H \"Content-Type: application/json\" -d '{\"text\": \"your text here\"}'\n\n# 3. Insert file:\n# curl -X POST \"http://127.0.0.1:8020/insert_file\" -H \"Content-Type: application/json\" -d '{\"file_path\": \"path/to/your/file.txt\"}'\n\n# 4. Health check:\n# curl -X GET \"http://127.0.0.1:8020/health\"\n"}
{"type": "source_file", "path": "examples/lightrag_nvidia_demo.py", "content": "import os\nimport asyncio\nfrom lightrag import LightRAG, QueryParam\nfrom lightrag.llm import (\n    openai_complete_if_cache,\n    nvidia_openai_embedding,\n)\nfrom lightrag.utils import EmbeddingFunc\nimport numpy as np\n\n# for custom llm_model_func\nfrom lightrag.utils import locate_json_string_body_from_string\n\nWORKING_DIR = \"./dickens\"\n\nif not os.path.exists(WORKING_DIR):\n    os.mkdir(WORKING_DIR)\n\n# some method to use your API key (choose one)\n# NVIDIA_OPENAI_API_KEY = os.getenv(\"NVIDIA_OPENAI_API_KEY\")\nNVIDIA_OPENAI_API_KEY = \"nvapi-xxxx\"  # your api key\n\n# using pre-defined function for nvidia LLM API. OpenAI compatible\n# llm_model_func = nvidia_openai_complete\n\n\n# If you trying to make custom llm_model_func to use llm model on NVIDIA API like other example:\nasync def llm_model_func(\n    prompt, system_prompt=None, history_messages=[], keyword_extraction=False, **kwargs\n) -> str:\n    result = await openai_complete_if_cache(\n        \"nvidia/llama-3.1-nemotron-70b-instruct\",\n        prompt,\n        system_prompt=system_prompt,\n        history_messages=history_messages,\n        api_key=NVIDIA_OPENAI_API_KEY,\n        base_url=\"https://integrate.api.nvidia.com/v1\",\n        **kwargs,\n    )\n    if keyword_extraction:\n        return locate_json_string_body_from_string(result)\n    return result\n\n\n# custom embedding\nnvidia_embed_model = \"nvidia/nv-embedqa-e5-v5\"\n\n\nasync def indexing_embedding_func(texts: list[str]) -> np.ndarray:\n    return await nvidia_openai_embedding(\n        texts,\n        model=nvidia_embed_model,  # maximum 512 token\n        # model=\"nvidia/llama-3.2-nv-embedqa-1b-v1\",\n        api_key=NVIDIA_OPENAI_API_KEY,\n        base_url=\"https://integrate.api.nvidia.com/v1\",\n        input_type=\"passage\",\n        trunc=\"END\",  # handling on server side if input token is longer than maximum token\n        encode=\"float\",\n    )\n\n\nasync def query_embedding_func(texts: list[str]) -> np.ndarray:\n    return await nvidia_openai_embedding(\n        texts,\n        model=nvidia_embed_model,  # maximum 512 token\n        # model=\"nvidia/llama-3.2-nv-embedqa-1b-v1\",\n        api_key=NVIDIA_OPENAI_API_KEY,\n        base_url=\"https://integrate.api.nvidia.com/v1\",\n        input_type=\"query\",\n        trunc=\"END\",  # handling on server side if input token is longer than maximum token\n        encode=\"float\",\n    )\n\n\n# dimension are same\nasync def get_embedding_dim():\n    test_text = [\"This is a test sentence.\"]\n    embedding = await indexing_embedding_func(test_text)\n    embedding_dim = embedding.shape[1]\n    return embedding_dim\n\n\n# function test\nasync def test_funcs():\n    result = await llm_model_func(\"How are you?\")\n    print(\"llm_model_func: \", result)\n\n    result = await indexing_embedding_func([\"How are you?\"])\n    print(\"embedding_func: \", result)\n\n\n# asyncio.run(test_funcs())\n\n\nasync def main():\n    try:\n        embedding_dimension = await get_embedding_dim()\n        print(f\"Detected embedding dimension: {embedding_dimension}\")\n\n        # lightRAG class during indexing\n        rag = LightRAG(\n            working_dir=WORKING_DIR,\n            llm_model_func=llm_model_func,\n            # llm_model_name=\"meta/llama3-70b-instruct\", #un comment if\n            embedding_func=EmbeddingFunc(\n                embedding_dim=embedding_dimension,\n                max_token_size=512,  # maximum token size, somehow it's still exceed maximum number of token\n                # so truncate (trunc) parameter on embedding_func will handle it and try to examine the tokenizer used in LightRAG\n                # so you can adjust to be able to fit the NVIDIA model (future work)\n                func=indexing_embedding_func,\n            ),\n        )\n\n        # reading file\n        with open(\"./book.txt\", \"r\", encoding=\"utf-8\") as f:\n            await rag.ainsert(f.read())\n\n        # redefine rag to change embedding into query type\n        rag = LightRAG(\n            working_dir=WORKING_DIR,\n            llm_model_func=llm_model_func,\n            # llm_model_name=\"meta/llama3-70b-instruct\", #un comment if\n            embedding_func=EmbeddingFunc(\n                embedding_dim=embedding_dimension,\n                max_token_size=512,\n                func=query_embedding_func,\n            ),\n        )\n\n        # Perform naive search\n        print(\"==============Naive===============\")\n        print(\n            await rag.aquery(\n                \"What are the top themes in this story?\", param=QueryParam(mode=\"naive\")\n            )\n        )\n\n        # Perform local search\n        print(\"==============local===============\")\n        print(\n            await rag.aquery(\n                \"What are the top themes in this story?\", param=QueryParam(mode=\"local\")\n            )\n        )\n\n        # Perform global search\n        print(\"==============global===============\")\n        print(\n            await rag.aquery(\n                \"What are the top themes in this story?\",\n                param=QueryParam(mode=\"global\"),\n            )\n        )\n\n        # Perform hybrid search\n        print(\"==============hybrid===============\")\n        print(\n            await rag.aquery(\n                \"What are the top themes in this story?\",\n                param=QueryParam(mode=\"hybrid\"),\n            )\n        )\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n"}
{"type": "source_file", "path": "built your Graphï¼ˆä¸å†ä½¿ç”¨ï¼Œä»…åšå‚è€ƒï¼‰.py", "content": "import asyncio\nimport os\n\nimport numpy as np\nfrom dotenv import load_dotenv\n\nfrom lightrag import LightRAG, QueryParam\nfrom lightrag.llm import openai_complete_if_cache, openai_embedding, openai_compatible_complete_if_cache, \\\n    openai_compatible_embedding\nfrom lightrag.utils import EmbeddingFunc\n\nload_dotenv()\n\n#ä½ çš„çŸ¥è¯†å›¾è°±å­˜æ”¾çš„æ–‡ä»¶å¤¹\nWORKING_DIR = os.getenv(\"RAG_DIR\")\nprint(f\"WORKING_DIR: {WORKING_DIR}\")\n\n#ä½ çš„æ–‡æ¡£ï¼Œä¾‹å¦‚./book.txtï¼Œå»ºè®®å­˜æ”¾åœ¨textè¿™ä¸ªæ–‡ä»¶å¤¹ä»¥ä¾¿ç®¡ç†\nfile_DIR = os.getenv(\"file_DIR\")\nprint(f\"file_DIR: {file_DIR}\")\n\nLLM_MODEL = os.getenv(\"LLM_MODEL\")\nprint(f\"LLM_MODEL: {LLM_MODEL}\")\nEMBEDDING_MODEL = os.getenv(\"EMBEDDING_MODEL\")\nprint(f\"EMBEDDING_MODEL: {EMBEDDING_MODEL}\")\nEMBEDDING_MAX_TOKEN_SIZE = int(os.getenv(\"EMBEDDING_MAX_TOKEN_SIZE\"))\nprint(f\"EMBEDDING_MAX_TOKEN_SIZE: {EMBEDDING_MAX_TOKEN_SIZE}\")\n\nBASE_URL=os.getenv(\"OPENAI_BASE_URL\")\nAPI_KEY = os.getenv(\"OPENAI_API_KEY\")\n\nif not os.path.exists(WORKING_DIR):\n    os.mkdir(WORKING_DIR)\n\n\nasync def llm_model_func(\n    prompt, system_prompt=None, history_messages=[], **kwargs\n) -> str:\n    return await openai_compatible_complete_if_cache(\n        LLM_MODEL,\n        prompt,\n        system_prompt=system_prompt,\n        history_messages=history_messages,\n        api_key=API_KEY,\n        base_url=BASE_URL,\n        **kwargs,\n    )\n\n\nasync def embedding_func(texts: list[str]) -> np.ndarray:\n    return await openai_compatible_embedding(\n        texts,\n        model=EMBEDDING_MODEL,\n        api_key=API_KEY,\n        base_url=BASE_URL,\n    )\n\n\nasync def get_embedding_dim():\n    test_text = [\"This is a test sentence.\"]\n    embedding = await embedding_func(test_text)\n    embedding_dim = embedding.shape[1]\n    return embedding_dim\n\n\n# function test\nasync def test_funcs():\n    result = await llm_model_func(\"How are you?\")\n    print(\"llm_model_func: \", result)\n\n    result = await embedding_func([\"How are you?\"])\n    print(\"embedding_func: \", result)\n\n\n# asyncio.run(test_funcs())\n\n\nasync def main():\n    try:\n        embedding_dimension = await get_embedding_dim()\n        print(f\"Detected embedding dimension: {embedding_dimension}\")\n\n        rag = LightRAG(\n            working_dir=WORKING_DIR,\n            llm_model_func=llm_model_func,\n            embedding_func=EmbeddingFunc(\n                embedding_dim=embedding_dimension,\n                max_token_size=EMBEDDING_MAX_TOKEN_SIZE,\n                func=embedding_func,\n            ),\n        )\n\n\n        with open(\"text/book.txt\", \"r\", encoding=\"utf-8\", errors='ignore') as f:\n            await rag.ainsert(f.read())\n\n        '''\n        #ä»¥ä¸‹æ˜¯æœç´¢æ–¹æ³•ï¼Œå…±å››ç§ï¼Œè¯·æŸ¥çœ‹å®˜æ–¹æ–‡æ¡£ä»¥ä¾¿é€‰æ‹©\n        # Perform naive search\n        print(\n            await rag.aquery(\n                \"What kind of story is told in this first chapter? Please answer in Chinese.\", param=QueryParam(mode=\"naive\")\n            )\n        )\n\n        # Perform local search\n        print(\n            await rag.aquery(\n                \"What kind of story is told in this first chapter? Please answer in Chinese.\", param=QueryParam(mode=\"local\")\n            )\n        )\n\n        # Perform global search\n        print(\n            await rag.aquery(\n                \"What kind of story is told in this first chapter? Please answer in Chinese.\", param=QueryParam(mode=\"global\"),\n            )\n        )\n\n        '''\n        # Perform hybrid search\n        print(\n            await rag.aquery(\n                \"What kind of story is told in this first chapter? Please answer in Chinese.\", param=QueryParam(mode=\"hybrid\"),\n            )\n        )\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n"}
{"type": "source_file", "path": "examples/graph_visual_with_neo4j.py", "content": "import os\nimport json\nfrom lightrag.utils import xml_to_json\nfrom neo4j import GraphDatabase\n\n# Constants\nWORKING_DIR = \"./dickens\"\nBATCH_SIZE_NODES = 500\nBATCH_SIZE_EDGES = 100\n\n# Neo4j connection credentials\nNEO4J_URI = \"bolt://localhost:7687\"\nNEO4J_USERNAME = \"neo4j\"\nNEO4J_PASSWORD = \"your_password\"\n\n\ndef convert_xml_to_json(xml_path, output_path):\n    \"\"\"Converts XML file to JSON and saves the output.\"\"\"\n    if not os.path.exists(xml_path):\n        print(f\"Error: File not found - {xml_path}\")\n        return None\n\n    json_data = xml_to_json(xml_path)\n    if json_data:\n        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n            json.dump(json_data, f, ensure_ascii=False, indent=2)\n        print(f\"JSON file created: {output_path}\")\n        return json_data\n    else:\n        print(\"Failed to create JSON data\")\n        return None\n\n\ndef process_in_batches(tx, query, data, batch_size):\n    \"\"\"Process data in batches and execute the given query.\"\"\"\n    for i in range(0, len(data), batch_size):\n        batch = data[i : i + batch_size]\n        tx.run(query, {\"nodes\": batch} if \"nodes\" in query else {\"edges\": batch})\n\n\ndef main():\n    # Paths\n    xml_file = os.path.join(WORKING_DIR, \"graph_chunk_entity_relation.graphml\")\n    json_file = os.path.join(WORKING_DIR, \"graph_data.json\")\n\n    # Convert XML to JSON\n    json_data = convert_xml_to_json(xml_file, json_file)\n    if json_data is None:\n        return\n\n    # Load nodes and edges\n    nodes = json_data.get(\"nodes\", [])\n    edges = json_data.get(\"edges\", [])\n\n    # Neo4j queries\n    create_nodes_query = \"\"\"\n    UNWIND $nodes AS node\n    MERGE (e:Entity {id: node.id})\n    SET e.entity_type = node.entity_type,\n        e.description = node.description,\n        e.source_id = node.source_id,\n        e.displayName = node.id\n    REMOVE e:Entity\n    WITH e, node\n    CALL apoc.create.addLabels(e, [node.id]) YIELD node AS labeledNode\n    RETURN count(*)\n    \"\"\"\n\n    create_edges_query = \"\"\"\n    UNWIND $edges AS edge\n    MATCH (source {id: edge.source})\n    MATCH (target {id: edge.target})\n    WITH source, target, edge,\n         CASE\n            WHEN edge.keywords CONTAINS 'lead' THEN 'lead'\n            WHEN edge.keywords CONTAINS 'participate' THEN 'participate'\n            WHEN edge.keywords CONTAINS 'uses' THEN 'uses'\n            WHEN edge.keywords CONTAINS 'located' THEN 'located'\n            WHEN edge.keywords CONTAINS 'occurs' THEN 'occurs'\n           ELSE REPLACE(SPLIT(edge.keywords, ',')[0], '\\\"', '')\n         END AS relType\n    CALL apoc.create.relationship(source, relType, {\n      weight: edge.weight,\n      description: edge.description,\n      keywords: edge.keywords,\n      source_id: edge.source_id\n    }, target) YIELD rel\n    RETURN count(*)\n    \"\"\"\n\n    set_displayname_and_labels_query = \"\"\"\n    MATCH (n)\n    SET n.displayName = n.id\n    WITH n\n    CALL apoc.create.setLabels(n, [n.entity_type]) YIELD node\n    RETURN count(*)\n    \"\"\"\n\n    # Create a Neo4j driver\n    driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USERNAME, NEO4J_PASSWORD))\n\n    try:\n        # Execute queries in batches\n        with driver.session() as session:\n            # Insert nodes in batches\n            session.execute_write(\n                process_in_batches, create_nodes_query, nodes, BATCH_SIZE_NODES\n            )\n\n            # Insert edges in batches\n            session.execute_write(\n                process_in_batches, create_edges_query, edges, BATCH_SIZE_EDGES\n            )\n\n            # Set displayName and labels\n            session.run(set_displayname_and_labels_query)\n\n    except Exception as e:\n        print(f\"Error occurred: {e}\")\n\n    finally:\n        driver.close()\n\n\nif __name__ == \"__main__\":\n    main()\n"}
{"type": "source_file", "path": "examples/lightrag_bedrock_demo.py", "content": "\"\"\"\nLightRAG meets Amazon Bedrock â›°ï¸\n\"\"\"\n\nimport os\nimport logging\n\nfrom lightrag import LightRAG, QueryParam\nfrom lightrag.llm import bedrock_complete, bedrock_embedding\nfrom lightrag.utils import EmbeddingFunc\n\nlogging.getLogger(\"aiobotocore\").setLevel(logging.WARNING)\n\nWORKING_DIR = \"./dickens\"\nif not os.path.exists(WORKING_DIR):\n    os.mkdir(WORKING_DIR)\n\nrag = LightRAG(\n    working_dir=WORKING_DIR,\n    llm_model_func=bedrock_complete,\n    llm_model_name=\"Anthropic Claude 3 Haiku // Amazon Bedrock\",\n    embedding_func=EmbeddingFunc(\n        embedding_dim=1024, max_token_size=8192, func=bedrock_embedding\n    ),\n)\n\nwith open(\"./book.txt\", \"r\", encoding=\"utf-8\") as f:\n    rag.insert(f.read())\n\nfor mode in [\"naive\", \"local\", \"global\", \"hybrid\"]:\n    print(\"\\n+-\" + \"-\" * len(mode) + \"-+\")\n    print(f\"| {mode.capitalize()} |\")\n    print(\"+-\" + \"-\" * len(mode) + \"-+\\n\")\n    print(\n        rag.query(\"What are the top themes in this story?\", param=QueryParam(mode=mode))\n    )\n"}
{"type": "source_file", "path": "Gradio/welcome_page.py", "content": "import mimetypes\nimport os\nimport shutil\nimport sys\nimport zipfile\nimport subprocess\n\nfrom importlib.metadata import distributions\nfrom typing import List, Tuple\nfrom datetime import datetime, time, timedelta\n\nimport gradio as gr\nfrom dotenv import load_dotenv, set_key\nfrom pathlib import Path\n\nfrom fastapi import requests\n\nfrom playwright.sync_api import sync_playwright\nfrom sympy import false\n\nload_dotenv()\n\n# æ¬¢è¿ç•Œé¢<å¼€å§‹>\nclass welcome_pages:\n\n    def __init__(self):\n        \"\"\"åˆå§‹åŒ–å¹¶æ„å»ºæ¬¢è¿é¡µé¢\"\"\"\n        self.ui = self.build_welcome_page()\n\n    def load_readme(self):\n        \"\"\"åŠ è½½ README.md å†…å®¹\"\"\"\n        try:\n            with open(\"README.md\", \"r\", encoding=\"utf-8\") as file:\n                return file.read()\n        except FileNotFoundError:\n            return \"README.md æ–‡ä»¶æœªæ‰¾åˆ°ï¼Œè¯·æ£€æŸ¥é¡¹ç›®ç›®å½•ã€‚\"\n\n    def load_license(self):\n        \"\"\"åŠ è½½å¼€æºåè®®å†…å®¹\"\"\"\n        try:\n            with open(\"LICENSE\", \"r\", encoding=\"utf-8\") as file:\n                return file.read()\n        except FileNotFoundError:\n            return \"LICENSE æ–‡ä»¶æœªæ‰¾åˆ°ï¼Œè¯·æ£€æŸ¥é¡¹ç›®ç›®å½•ã€‚\"\n\n    def load_requirements(self):\n        \"\"\"è¯»å– requirements.txt ä¸­çš„ä¾èµ–åŒ…ä¿¡æ¯\"\"\"\n        with open(\"requirements.txt\", \"r\") as f:\n            requirements = f.read().splitlines()\n        return [pkg.split(\"==\")[0] for pkg in requirements], requirements\n\n    def check_installed_packages(self):\n        \"\"\"è·å–å½“å‰ç¯å¢ƒä¸­å·²å®‰è£…çš„ä¾èµ–åŒ…åŠç‰ˆæœ¬\"\"\"\n        installed_packages = {dist.metadata[\"Name\"].lower(): dist.version for dist in distributions()}\n        return installed_packages\n\n    def check_dependency_status(self):\n        \"\"\"æ£€æŸ¥ä¾èµ–åŒ…çŠ¶æ€\"\"\"\n        required_packages, full_requirements = self.load_requirements()\n        installed_packages = self.check_installed_packages()\n\n        missing_packages = []\n        mismatched_versions = []\n\n        for req in full_requirements:\n            pkg, _, version = req.partition(\"==\")\n            pkg_lower = pkg.lower()\n            if pkg_lower not in installed_packages:\n                missing_packages.append(f\"ğŸš« {req}\")\n            elif installed_packages[pkg_lower] != version:\n                mismatched_versions.append(\n                    f\"âš ï¸ {pkg} (expected {version}, found {installed_packages[pkg_lower]})\"\n                )\n\n        if not missing_packages and not mismatched_versions:\n            return \"âœ… æ‰€æœ‰ä¾èµ–åŒ…å·²å®‰è£…\", [], []\n        else:\n            return (\n                \"éƒ¨åˆ†ä¾èµ–åŒ…å­˜åœ¨é—®é¢˜ï¼Œè¯·æŸ¥çœ‹ä¸‹æ–¹åˆ—è¡¨ã€‚\",\n                missing_packages,\n                mismatched_versions,\n            )\n\n    def install_missing_packages(missing_packages):\n        \"\"\"å®‰è£…ç¼ºå¤±çš„ä¾èµ–åŒ…\"\"\"\n        try:\n            for package in missing_packages:\n                pkg = package.split(\" \")[1]  # æå–åŒ…åï¼ˆå¿½ç•¥ç¬¦å· ğŸš«ï¼‰\n                subprocess.check_call([\"pip\", \"install\", pkg])\n            return \"âœ… ç¼ºå¤±çš„ä¾èµ–åŒ…å·²æˆåŠŸå®‰è£…\"\n        except subprocess.CalledProcessError as e:\n            return f\"âŒ å®‰è£…å¤±è´¥: {e}\"\n\n\n    # å®‰è£…æŒ‰é’®é€»è¾‘\n    def install_and_update(missing_packages, self=None):\n        if not missing_packages:\n            return \"æ²¡æœ‰éœ€è¦å®‰è£…çš„ä¾èµ–åŒ…\"\n        install_result = self.install_missing_packages(missing_packages)\n        status, _, _ = self.check_dependency_status()  # æ£€æŸ¥å®‰è£…åçš„çŠ¶æ€\n        return status, install_result\n\n\n    def check_lightrag_status(self):\n        \"\"\"æ£€æŸ¥ LightRAG åç«¯çŠ¶æ€\"\"\"\n        # ç¤ºä¾‹å®ç°ï¼Œå¯ä»¥æ‰©å±•ä¸ºå®é™…åç«¯æœåŠ¡çš„æ£€æŸ¥é€»è¾‘\n        return \"âœ…LightRAG åç«¯è¿è¡Œæ­£å¸¸\"\n\n    def check_model_connection_status(self):\n        \"\"\"æ£€æŸ¥å¤§æ¨¡å‹è¿æ¥çŠ¶æ€\"\"\"\n        # ç¤ºä¾‹å®ç°ï¼Œå¯ä»¥æ‰©å±•ä¸ºå®é™…æ¨¡å‹è¿æ¥çš„æ£€æŸ¥é€»è¾‘\n        return \"âœ…å¤§æ¨¡å‹è¿æ¥æˆåŠŸ\"\n\n\n    # åˆ·æ–°æŒ‰é’®é€»è¾‘\n    def refresh_status(self):\n        status, missing, mismatched = self.check_dependency_status()\n        return (\n            status,\n            missing + mismatched,  # å±•ç¤ºæ‰€æœ‰ç¼ºå¤±å’Œç‰ˆæœ¬é—®é¢˜\n            bool(missing or mismatched),\n        )\n\n    # æ¬¢è¿ç•Œé¢<ç»“æŸ>\n\n\n    # UI\n\n    def build_welcome_page(self):\n        \"\"\"åˆ›å»ºæ¬¢è¿ä½¿ç”¨é¡µé¢\"\"\"\n        with gr.Blocks(visible=False, elem_id=\"welcome-page\") as welcome_page:\n            # æ ‡é¢˜\n            gr.Markdown(\"# æ¬¢è¿ä½¿ç”¨\", elem_id=\"welcome-title\", elem_classes=\"center-text\")\n\n            # ä¸»ä½“å†…å®¹\n            with gr.Row():\n                # å·¦ä¾§ README å†…å®¹å—\n                with gr.Column(scale=3):\n                    gr.Markdown(self.load_readme(), label=\"é¡¹ç›®ç®€ä»‹\")\n\n                # å³ä¾§çŠ¶æ€æ \n                with gr.Column(scale=1):\n                    gr.Markdown(\"## ç³»ç»ŸçŠ¶æ€\")\n                    dependency_status = gr.Textbox(\n                        label=\"ä¾èµ–åŒ…çŠ¶æ€\",\n                        value=self.check_dependency_status()[0],\n                        interactive=False,\n                        placeholder=\"ä¾èµ–åŒ…å®‰è£…çŠ¶æ€æ˜¾ç¤ºåœ¨æ­¤å¤„\"\n                    )\n                    missing_packages_dropdown = gr.Dropdown(\n                        label=\"ç¼ºå¤±ä¾èµ–åŒ…åˆ—è¡¨\",\n                        choices=[],\n                        visible=True,\n                        interactive=False,\n                        multiselect=True,\n                        allow_custom_value=True\n                    )\n                    install_button = gr.Button(\n                        \"å®‰è£…ç¼ºå¤±çš„ä¾èµ–åŒ…\",\n                        visible=False,\n                        variant=\"primary\",\n                    )\n                    lightrag_status = gr.Textbox(\n                        label=\"LightRAG åç«¯çŠ¶æ€\",\n                        value=self.check_lightrag_status(),\n                        interactive=False,\n                        placeholder=\"åç«¯çŠ¶æ€æ˜¾ç¤ºåœ¨æ­¤å¤„\"\n                    )\n                    model_connection_status = gr.Textbox(\n                        label=\"å¤§æ¨¡å‹è¿æ¥çŠ¶æ€\",\n                        value=self.check_model_connection_status(),\n                        interactive=False,\n                        placeholder=\"æ¨¡å‹è¿æ¥çŠ¶æ€æ˜¾ç¤ºåœ¨æ­¤å¤„\"\n                    )\n                    refresh_button = gr.Button(\"ğŸ”„åˆ·æ–°çŠ¶æ€\", variant=\"primary\")\n\n            # åº•éƒ¨é“¾æ¥ä¸å¼€æºåè®®\n            with gr.Row():\n                with gr.Column(scale=3):\n                    gr.Markdown(\"### ğŸ“‚ é¡¹ç›®é“¾æ¥\")\n                    gr.Markdown(\"\"\"\n                    - [GitHub ä»“åº“](https://github.com/your_repo)\n                    - [é¡¹ç›®ä½¿ç”¨è¯´æ˜ä¹¦](https://your_docs_link)\n                    - [è§†é¢‘æ•™ç¨‹](https://your_video_link)\n                    \"\"\")\n\n                with gr.Column(scale=1):\n                    license_textbox = gr.Textbox(\n                        label=\"å¼€æºåè®®\",\n                        value=self.load_license(),\n                        lines=10,\n                        interactive=False\n                    )\n            # é¡µé¢åˆå§‹åŒ–æ—¶çš„æ£€æŸ¥é€»è¾‘\n            def initialize_status():\n                status, missing, mismatched = self.check_dependency_status()\n                all_issues = missing + mismatched\n                show_install_button = bool(missing)  # ä»…ç¼ºå¤±åŒ…æ—¶æ˜¾ç¤ºå®‰è£…æŒ‰é’®\n                return (\n                    status,\n                    all_issues,\n                    missing,  # æ§åˆ¶å®‰è£…æŒ‰é’®æ˜¯å¦æ˜¾ç¤º\n                    show_install_button,\n                )\n\n            welcome_page.load(\n                fn=initialize_status,\n                inputs=[],\n                outputs=[\n                    dependency_status,\n                    missing_packages_dropdown,\n                    missing_packages_dropdown,\n                    install_button,\n                ],\n            )\n\n            # åˆ·æ–°æŒ‰é’®é€»è¾‘\n            def refresh_status():\n                status, missing, mismatched = self.check_dependency_status()\n                all_issues = missing + mismatched\n                return (\n                    status,\n                    all_issues,\n                    missing,  # æ§åˆ¶å®‰è£…æŒ‰é’®æ˜¯å¦æ˜¾ç¤º\n                    bool(missing),\n                )\n\n            refresh_button.click(\n                fn=refresh_status,\n                inputs=[],\n                outputs=[\n                    dependency_status,\n                    missing_packages_dropdown,\n                    missing_packages_dropdown,\n                    install_button,\n                ],\n            )\n\n            # å®‰è£…æŒ‰é’®é€»è¾‘\n            def install_and_update(missing_packages):\n                if not missing_packages:\n                    return \"æ²¡æœ‰éœ€è¦å®‰è£…çš„ä¾èµ–åŒ…\"\n                install_result = self.install_missing_packages(missing_packages)\n                status, _, _ = self.check_dependency_status()  # æ£€æŸ¥å®‰è£…åçš„çŠ¶æ€\n                return status, install_result\n\n            install_button.click(\n                fn=install_and_update,\n                inputs=[missing_packages_dropdown],\n                outputs=[\n                    dependency_status,\n                    gr.Textbox(placeholder=\"å®‰è£…çŠ¶æ€\", interactive=False),\n                ],\n            )\n        return welcome_page"}
{"type": "source_file", "path": "examples/lightrag_api_oracle_demo..py", "content": "from fastapi import FastAPI, HTTPException, File, UploadFile\nfrom contextlib import asynccontextmanager\nfrom pydantic import BaseModel\nfrom typing import Optional\n\nimport sys\nimport os\nfrom pathlib import Path\n\nimport asyncio\nimport nest_asyncio\nfrom lightrag import LightRAG, QueryParam\nfrom lightrag.llm import openai_complete_if_cache, openai_embedding\nfrom lightrag.utils import EmbeddingFunc\nimport numpy as np\n\nfrom lightrag.kg.oracle_impl import OracleDB\n\n\nprint(os.getcwd())\n\nscript_directory = Path(__file__).resolve().parent.parent\nsys.path.append(os.path.abspath(script_directory))\n\n\n# Apply nest_asyncio to solve event loop issues\nnest_asyncio.apply()\n\nDEFAULT_RAG_DIR = \"index_default\"\n\n\n# We use OpenAI compatible API to call LLM on Oracle Cloud\n# More docs here https://github.com/jin38324/OCI_GenAI_access_gateway\nBASE_URL = \"http://xxx.xxx.xxx.xxx:8088/v1/\"\nAPIKEY = \"ocigenerativeai\"\n\n# Configure working directory\nWORKING_DIR = os.environ.get(\"RAG_DIR\", f\"{DEFAULT_RAG_DIR}\")\nprint(f\"WORKING_DIR: {WORKING_DIR}\")\nLLM_MODEL = os.environ.get(\"LLM_MODEL\", \"cohere.command-r-plus\")\nprint(f\"LLM_MODEL: {LLM_MODEL}\")\nEMBEDDING_MODEL = os.environ.get(\"EMBEDDING_MODEL\", \"cohere.embed-multilingual-v3.0\")\nprint(f\"EMBEDDING_MODEL: {EMBEDDING_MODEL}\")\nEMBEDDING_MAX_TOKEN_SIZE = int(os.environ.get(\"EMBEDDING_MAX_TOKEN_SIZE\", 512))\nprint(f\"EMBEDDING_MAX_TOKEN_SIZE: {EMBEDDING_MAX_TOKEN_SIZE}\")\n\n\nif not os.path.exists(WORKING_DIR):\n    os.mkdir(WORKING_DIR)\n\n\nasync def llm_model_func(\n    prompt, system_prompt=None, history_messages=[], **kwargs\n) -> str:\n    return await openai_complete_if_cache(\n        LLM_MODEL,\n        prompt,\n        system_prompt=system_prompt,\n        history_messages=history_messages,\n        api_key=APIKEY,\n        base_url=BASE_URL,\n        **kwargs,\n    )\n\n\nasync def embedding_func(texts: list[str]) -> np.ndarray:\n    return await openai_embedding(\n        texts,\n        model=EMBEDDING_MODEL,\n        api_key=APIKEY,\n        base_url=BASE_URL,\n    )\n\n\nasync def get_embedding_dim():\n    test_text = [\"This is a test sentence.\"]\n    embedding = await embedding_func(test_text)\n    embedding_dim = embedding.shape[1]\n    return embedding_dim\n\n\nasync def init():\n    # Detect embedding dimension\n    embedding_dimension = await get_embedding_dim()\n    print(f\"Detected embedding dimension: {embedding_dimension}\")\n    # Create Oracle DB connection\n    # The `config` parameter is the connection configuration of Oracle DB\n    # More docs here https://python-oracledb.readthedocs.io/en/latest/user_guide/connection_handling.html\n    # We storage data in unified tables, so we need to set a `workspace` parameter to specify which docs we want to store and query\n    # Below is an example of how to connect to Oracle Autonomous Database on Oracle Cloud\n\n    oracle_db = OracleDB(\n        config={\n            \"user\": \"\",\n            \"password\": \"\",\n            \"dsn\": \"\",\n            \"config_dir\": \"\",\n            \"wallet_location\": \"\",\n            \"wallet_password\": \"\",\n            \"workspace\": \"\",\n        }  # specify which docs you want to store and query\n    )\n\n    # Check if Oracle DB tables exist, if not, tables will be created\n    await oracle_db.check_tables()\n    # Initialize LightRAG\n    # We use Oracle DB as the KV/vector/graph storage\n    rag = LightRAG(\n        enable_llm_cache=False,\n        working_dir=WORKING_DIR,\n        chunk_token_size=512,\n        llm_model_func=llm_model_func,\n        embedding_func=EmbeddingFunc(\n            embedding_dim=embedding_dimension,\n            max_token_size=512,\n            func=embedding_func,\n        ),\n        graph_storage=\"OracleGraphStorage\",\n        kv_storage=\"OracleKVStorage\",\n        vector_storage=\"OracleVectorDBStorage\",\n    )\n\n    # Setthe KV/vector/graph storage's `db` property, so all operation will use same connection pool\n    rag.graph_storage_cls.db = oracle_db\n    rag.key_string_value_json_storage_cls.db = oracle_db\n    rag.vector_db_storage_cls.db = oracle_db\n\n    return rag\n\n\n# Data models\n\n\nclass QueryRequest(BaseModel):\n    query: str\n    mode: str = \"hybrid\"\n    only_need_context: bool = False\n\n\nclass InsertRequest(BaseModel):\n    text: str\n\n\nclass Response(BaseModel):\n    status: str\n    data: Optional[str] = None\n    message: Optional[str] = None\n\n\n# API routes\n\nrag = None\n\n\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n    global rag\n    rag = await init()\n    print(\"done!\")\n    yield\n\n\napp = FastAPI(\n    title=\"LightRAG API\", description=\"API for RAG operations\", lifespan=lifespan\n)\n\n\n@app.post(\"/query\", response_model=Response)\nasync def query_endpoint(request: QueryRequest):\n    try:\n        # loop = asyncio.get_event_loop()\n        result = await rag.aquery(\n            request.query,\n            param=QueryParam(\n                mode=request.mode, only_need_context=request.only_need_context\n            ),\n        )\n        return Response(status=\"success\", data=result)\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n\n\n@app.post(\"/insert\", response_model=Response)\nasync def insert_endpoint(request: InsertRequest):\n    try:\n        loop = asyncio.get_event_loop()\n        await loop.run_in_executor(None, lambda: rag.insert(request.text))\n        return Response(status=\"success\", message=\"Text inserted successfully\")\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n\n\n@app.post(\"/insert_file\", response_model=Response)\nasync def insert_file(file: UploadFile = File(...)):\n    try:\n        file_content = await file.read()\n        # Read file content\n        try:\n            content = file_content.decode(\"utf-8\")\n        except UnicodeDecodeError:\n            # If UTF-8 decoding fails, try other encodings\n            content = file_content.decode(\"gbk\")\n        # Insert file content\n        loop = asyncio.get_event_loop()\n        await loop.run_in_executor(None, lambda: rag.insert(content))\n\n        return Response(\n            status=\"success\",\n            message=f\"File content from {file.filename} inserted successfully\",\n        )\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n\n\n@app.get(\"/health\")\nasync def health_check():\n    return {\"status\": \"healthy\"}\n\n\nif __name__ == \"__main__\":\n    import uvicorn\n\n    uvicorn.run(app, host=\"0.0.0.0\", port=8020)\n\n# Usage example\n# To run the server, use the following command in your terminal:\n# python lightrag_api_openai_compatible_demo.py\n\n# Example requests:\n# 1. Query:\n# curl -X POST \"http://127.0.0.1:8020/query\" -H \"Content-Type: application/json\" -d '{\"query\": \"your query here\", \"mode\": \"hybrid\"}'\n\n# 2. Insert text:\n# curl -X POST \"http://127.0.0.1:8020/insert\" -H \"Content-Type: application/json\" -d '{\"text\": \"your text here\"}'\n\n# 3. Insert file:\n# curl -X POST \"http://127.0.0.1:8020/insert_file\" -H \"Content-Type: application/json\" -d '{\"file_path\": \"path/to/your/file.txt\"}'\n\n# 4. Health check:\n# curl -X GET \"http://127.0.0.1:8020/health\"\n"}
{"type": "source_file", "path": "examples/lightrag_hf_demo.py", "content": "import os\n\nfrom lightrag import LightRAG, QueryParam\nfrom lightrag.llm import hf_model_complete, hf_embedding\nfrom lightrag.utils import EmbeddingFunc\nfrom transformers import AutoModel, AutoTokenizer\n\nWORKING_DIR = \"./dickens\"\n\nif not os.path.exists(WORKING_DIR):\n    os.mkdir(WORKING_DIR)\n\nrag = LightRAG(\n    working_dir=WORKING_DIR,\n    llm_model_func=hf_model_complete,\n    llm_model_name=\"meta-llama/Llama-3.1-8B-Instruct\",\n    embedding_func=EmbeddingFunc(\n        embedding_dim=384,\n        max_token_size=5000,\n        func=lambda texts: hf_embedding(\n            texts,\n            tokenizer=AutoTokenizer.from_pretrained(\n                \"sentence-transformers/all-MiniLM-L6-v2\"\n            ),\n            embed_model=AutoModel.from_pretrained(\n                \"sentence-transformers/all-MiniLM-L6-v2\"\n            ),\n        ),\n    ),\n)\n\n\nwith open(\"./book.txt\", \"r\", encoding=\"utf-8\") as f:\n    rag.insert(f.read())\n\n# Perform naive search\nprint(\n    rag.query(\"What are the top themes in this story?\", param=QueryParam(mode=\"naive\"))\n)\n\n# Perform local search\nprint(\n    rag.query(\"What are the top themes in this story?\", param=QueryParam(mode=\"local\"))\n)\n\n# Perform global search\nprint(\n    rag.query(\"What are the top themes in this story?\", param=QueryParam(mode=\"global\"))\n)\n\n# Perform hybrid search\nprint(\n    rag.query(\"What are the top themes in this story?\", param=QueryParam(mode=\"hybrid\"))\n)\n"}
{"type": "source_file", "path": "examples/graph_visual_with_html.py", "content": "import networkx as nx\nfrom pyvis.network import Network\nimport random\n\n# Load the GraphML file\nG = nx.read_graphml(\"./dickens/graph_chunk_entity_relation.graphml\")\n\n# Create a Pyvis network\nnet = Network(height=\"100vh\", notebook=True)\n\n# Convert NetworkX graph to Pyvis network\nnet.from_nx(G)\n\n\n# Add colors and title to nodes\nfor node in net.nodes:\n    node[\"color\"] = \"#{:06x}\".format(random.randint(0, 0xFFFFFF))\n    if \"description\" in node:\n        node[\"title\"] = node[\"description\"]\n\n# Add title to edges\nfor edge in net.edges:\n    if \"description\" in edge:\n        edge[\"title\"] = edge[\"description\"]\n\n# Save and display the network\nnet.show(\"knowledge_graph.html\")\n"}
{"type": "source_file", "path": "examples/lightrag_lmdeploy_demo.py", "content": "import os\n\nfrom lightrag import LightRAG, QueryParam\nfrom lightrag.llm import lmdeploy_model_if_cache, hf_embedding\nfrom lightrag.utils import EmbeddingFunc\nfrom transformers import AutoModel, AutoTokenizer\n\nWORKING_DIR = \"./dickens\"\n\nif not os.path.exists(WORKING_DIR):\n    os.mkdir(WORKING_DIR)\n\n\nasync def lmdeploy_model_complete(\n    prompt=None,\n    system_prompt=None,\n    history_messages=[],\n    keyword_extraction=False,\n    **kwargs,\n) -> str:\n    model_name = kwargs[\"hashing_kv\"].global_config[\"llm_model_name\"]\n    return await lmdeploy_model_if_cache(\n        model_name,\n        prompt,\n        system_prompt=system_prompt,\n        history_messages=history_messages,\n        ## please specify chat_template if your local path does not follow original HF file name,\n        ## or model_name is a pytorch model on huggingface.co,\n        ## you can refer to https://github.com/InternLM/lmdeploy/blob/main/lmdeploy/model.py\n        ## for a list of chat_template available in lmdeploy.\n        chat_template=\"llama3\",\n        # model_format ='awq', # if you are using awq quantization model.\n        # quant_policy=8, # if you want to use online kv cache, 4=kv int4, 8=kv int8.\n        **kwargs,\n    )\n\n\nrag = LightRAG(\n    working_dir=WORKING_DIR,\n    llm_model_func=lmdeploy_model_complete,\n    llm_model_name=\"meta-llama/Llama-3.1-8B-Instruct\",  # please use definite path for local model\n    embedding_func=EmbeddingFunc(\n        embedding_dim=384,\n        max_token_size=5000,\n        func=lambda texts: hf_embedding(\n            texts,\n            tokenizer=AutoTokenizer.from_pretrained(\n                \"sentence-transformers/all-MiniLM-L6-v2\"\n            ),\n            embed_model=AutoModel.from_pretrained(\n                \"sentence-transformers/all-MiniLM-L6-v2\"\n            ),\n        ),\n    ),\n)\n\n\nwith open(\"./book.txt\", \"r\", encoding=\"utf-8\") as f:\n    rag.insert(f.read())\n\n# Perform naive search\nprint(\n    rag.query(\"What are the top themes in this story?\", param=QueryParam(mode=\"naive\"))\n)\n\n# Perform local search\nprint(\n    rag.query(\"What are the top themes in this story?\", param=QueryParam(mode=\"local\"))\n)\n\n# Perform global search\nprint(\n    rag.query(\"What are the top themes in this story?\", param=QueryParam(mode=\"global\"))\n)\n\n# Perform hybrid search\nprint(\n    rag.query(\"What are the top themes in this story?\", param=QueryParam(mode=\"hybrid\"))\n)\n"}
{"type": "source_file", "path": "Gradio/graph_visual_with_html.py", "content": "import networkx as nx\nfrom pyvis.network import Network\nimport random\nimport os\n\n\nclass KnowledgeGraphVisualizer:\n    def __init__(self, graphml_path: str):\n        \"\"\"\n        åˆå§‹åŒ– KnowledgeGraphVisualizer ç±»ã€‚\n\n        :param graphml_path: è¾“å…¥çš„ GraphML æ–‡ä»¶è·¯å¾„ã€‚\n        \"\"\"\n        self.graphml_path = graphml_path\n        self.graph = None\n        self.net = None\n        self.html_output_path = None\n\n    def load_graph(self):\n        \"\"\"\n        ä» GraphML æ–‡ä»¶åŠ è½½å›¾å½¢ã€‚\n        \"\"\"\n        if not os.path.exists(self.graphml_path):\n            raise FileNotFoundError(f\"GraphML file not found at: {self.graphml_path}\")\n\n        # åŠ è½½å›¾å½¢\n        self.graph = nx.read_graphml(self.graphml_path)\n\n    def create_network(self):\n        \"\"\"\n        åˆ›å»º Pyvis ç½‘ç»œï¼Œå¹¶è®¾ç½®èŠ‚ç‚¹é¢œè‰²ã€‚\n        \"\"\"\n        # åˆ›å»ºä¸€ä¸ª Pyvis ç½‘ç»œå®ä¾‹\n        self.net = Network(height=\"100vh\", notebook=True)\n\n        # å°† NetworkX å›¾è½¬ä¸º Pyvis ç½‘ç»œ\n        self.net.from_nx(self.graph)\n\n        # ç»™æ¯ä¸ªèŠ‚ç‚¹æ·»åŠ éšæœºé¢œè‰²\n        for node in self.net.nodes:\n            node[\"color\"] = \"#{:06x}\".format(random.randint(0, 0xFFFFFF))\n\n    def save_html(self):\n        \"\"\"\n        ä¿å­˜ç”Ÿæˆçš„ç½‘ç»œå›¾ä¸º HTML æ–‡ä»¶ã€‚\n        \"\"\"\n        # è·å– HTML è¾“å‡ºè·¯å¾„ï¼ˆä¿å­˜åœ¨ä¸ GraphML æ–‡ä»¶ç›¸åŒçš„ç›®å½•ï¼‰\n        output_dir = os.path.dirname(self.graphml_path)  # è·å– GraphML æ–‡ä»¶æ‰€åœ¨çš„ç›®å½•\n        os.makedirs(output_dir, exist_ok=True)  # ç¡®ä¿ç›®å½•å­˜åœ¨\n\n        # ä½¿ç”¨ GraphML æ–‡ä»¶åç”Ÿæˆ HTML æ–‡ä»¶åï¼Œæ›¿æ¢æ‰©å±•åä¸º .html\n        file_name_without_ext = os.path.splitext(os.path.basename(self.graphml_path))[0]\n        self.html_output_path = os.path.join(output_dir, f\"knowledge_graph.html\")\n\n        # ä¿å­˜å¹¶æ˜¾ç¤ºç½‘ç»œå›¾\n        self.net.show(self.html_output_path)\n\n    def generate_graph(self):\n        \"\"\"\n        ç”ŸæˆçŸ¥è¯†å›¾è°±å¹¶ä¿å­˜ä¸º HTML æ–‡ä»¶ã€‚\n        \"\"\"\n        # åŠ è½½å›¾å½¢\n        self.load_graph()\n\n        # åˆ›å»ºç½‘ç»œå¹¶æ·»åŠ é¢œè‰²\n        self.create_network()\n\n        # ä¿å­˜ HTML æ–‡ä»¶\n        self.save_html()\n\n        return self.html_output_path\n\n\n# ç”¨æ³•ç¤ºä¾‹ï¼š\nif __name__ == \"__main__\":\n    # GraphML æ–‡ä»¶è·¯å¾„\n    graphml_path = \"H:/LightRAG for Sillytavern/LightRAG-for-OpenAI-Standard-Frontend/graph/ç¬¬äºŒç« .txt_20241218193114/graph_chunk_entity_relation.graphml\"\n\n    # åˆå§‹åŒ–çŸ¥è¯†å›¾è°±å¯è§†åŒ–å¯¹è±¡\n    visualizer = KnowledgeGraphVisualizer(graphml_path)\n\n    # ç”Ÿæˆå›¾å¹¶ä¿å­˜ä¸º HTML æ–‡ä»¶\n    html_file_path = visualizer.generate_graph()\n    print(f\"HTML file has been saved to: {html_file_path}\")\n"}
{"type": "source_file", "path": "examples/lightrag_api_ollama_demo.py", "content": "from fastapi import FastAPI, HTTPException, File, UploadFile\nfrom pydantic import BaseModel\nimport os\nfrom lightrag import LightRAG, QueryParam\nfrom lightrag.llm import ollama_embedding, ollama_model_complete\nfrom lightrag.utils import EmbeddingFunc\nfrom typing import Optional\nimport asyncio\nimport nest_asyncio\nimport aiofiles\n\n# Apply nest_asyncio to solve event loop issues\nnest_asyncio.apply()\n\nDEFAULT_RAG_DIR = \"index_default\"\napp = FastAPI(title=\"LightRAG API\", description=\"API for RAG operations\")\n\nDEFAULT_INPUT_FILE = \"book.txt\"\nINPUT_FILE = os.environ.get(\"INPUT_FILE\", f\"{DEFAULT_INPUT_FILE}\")\nprint(f\"INPUT_FILE: {INPUT_FILE}\")\n\n# Configure working directory\nWORKING_DIR = os.environ.get(\"RAG_DIR\", f\"{DEFAULT_RAG_DIR}\")\nprint(f\"WORKING_DIR: {WORKING_DIR}\")\n\n\nif not os.path.exists(WORKING_DIR):\n    os.mkdir(WORKING_DIR)\n\n\nrag = LightRAG(\n    working_dir=WORKING_DIR,\n    llm_model_func=ollama_model_complete,\n    llm_model_name=\"gemma2:9b\",\n    llm_model_max_async=4,\n    llm_model_max_token_size=8192,\n    llm_model_kwargs={\"host\": \"http://localhost:11434\", \"options\": {\"num_ctx\": 8192}},\n    embedding_func=EmbeddingFunc(\n        embedding_dim=768,\n        max_token_size=8192,\n        func=lambda texts: ollama_embedding(\n            texts, embed_model=\"nomic-embed-text\", host=\"http://localhost:11434\"\n        ),\n    ),\n)\n\n\n# Data models\nclass QueryRequest(BaseModel):\n    query: str\n    mode: str = \"hybrid\"\n    only_need_context: bool = False\n\n\nclass InsertRequest(BaseModel):\n    text: str\n\n\nclass Response(BaseModel):\n    status: str\n    data: Optional[str] = None\n    message: Optional[str] = None\n\n\n# API routes\n@app.post(\"/query\", response_model=Response)\nasync def query_endpoint(request: QueryRequest):\n    try:\n        loop = asyncio.get_event_loop()\n        result = await loop.run_in_executor(\n            None,\n            lambda: rag.query(\n                request.query,\n                param=QueryParam(\n                    mode=request.mode, only_need_context=request.only_need_context\n                ),\n            ),\n        )\n        return Response(status=\"success\", data=result)\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n\n\n# insert by text\n@app.post(\"/insert\", response_model=Response)\nasync def insert_endpoint(request: InsertRequest):\n    try:\n        loop = asyncio.get_event_loop()\n        await loop.run_in_executor(None, lambda: rag.insert(request.text))\n        return Response(status=\"success\", message=\"Text inserted successfully\")\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n\n\n# insert by file in payload\n@app.post(\"/insert_file\", response_model=Response)\nasync def insert_file(file: UploadFile = File(...)):\n    try:\n        file_content = await file.read()\n        # Read file content\n        try:\n            content = file_content.decode(\"utf-8\")\n        except UnicodeDecodeError:\n            # If UTF-8 decoding fails, try other encodings\n            content = file_content.decode(\"gbk\")\n        # Insert file content\n        loop = asyncio.get_event_loop()\n        await loop.run_in_executor(None, lambda: rag.insert(content))\n\n        return Response(\n            status=\"success\",\n            message=f\"File content from {file.filename} inserted successfully\",\n        )\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n\n\n# insert by local default file\n@app.post(\"/insert_default_file\", response_model=Response)\n@app.get(\"/insert_default_file\", response_model=Response)\nasync def insert_default_file():\n    try:\n        # Read file content from book.txt\n        async with aiofiles.open(INPUT_FILE, \"r\", encoding=\"utf-8\") as file:\n            content = await file.read()\n        print(f\"read input file {INPUT_FILE} successfully\")\n        # Insert file content\n        loop = asyncio.get_event_loop()\n        await loop.run_in_executor(None, lambda: rag.insert(content))\n\n        return Response(\n            status=\"success\",\n            message=f\"File content from {INPUT_FILE} inserted successfully\",\n        )\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n\n\n@app.get(\"/health\")\nasync def health_check():\n    return {\"status\": \"healthy\"}\n\n\nif __name__ == \"__main__\":\n    import uvicorn\n\n    uvicorn.run(app, host=\"0.0.0.0\", port=8020)\n\n# Usage example\n# To run the server, use the following command in your terminal:\n# python lightrag_api_openai_compatible_demo.py\n\n# Example requests:\n# 1. Query:\n# curl -X POST \"http://127.0.0.1:8020/query\" -H \"Content-Type: application/json\" -d '{\"query\": \"your query here\", \"mode\": \"hybrid\"}'\n\n# 2. Insert text:\n# curl -X POST \"http://127.0.0.1:8020/insert\" -H \"Content-Type: application/json\" -d '{\"text\": \"your text here\"}'\n\n# 3. Insert file:\n# curl -X POST \"http://127.0.0.1:8020/insert_file\" -H \"Content-Type: application/json\" -d '{\"file_path\": \"path/to/your/file.txt\"}'\n\n# 4. Health check:\n# curl -X GET \"http://127.0.0.1:8020/health\"\n"}
{"type": "source_file", "path": "examples/lightrag_ollama_age_demo.py", "content": "import asyncio\nimport inspect\nimport logging\nimport os\n\nfrom lightrag import LightRAG, QueryParam\nfrom lightrag.llm import ollama_embedding, ollama_model_complete\nfrom lightrag.utils import EmbeddingFunc\n\nWORKING_DIR = \"./dickens_age\"\n\nlogging.basicConfig(format=\"%(levelname)s:%(message)s\", level=logging.INFO)\n\nif not os.path.exists(WORKING_DIR):\n    os.mkdir(WORKING_DIR)\n\n# AGE\nos.environ[\"AGE_POSTGRES_DB\"] = \"postgresDB\"\nos.environ[\"AGE_POSTGRES_USER\"] = \"postgresUser\"\nos.environ[\"AGE_POSTGRES_PASSWORD\"] = \"postgresPW\"\nos.environ[\"AGE_POSTGRES_HOST\"] = \"localhost\"\nos.environ[\"AGE_POSTGRES_PORT\"] = \"5455\"\nos.environ[\"AGE_GRAPH_NAME\"] = \"dickens\"\n\nrag = LightRAG(\n    working_dir=WORKING_DIR,\n    llm_model_func=ollama_model_complete,\n    llm_model_name=\"llama3.1:8b\",\n    llm_model_max_async=4,\n    llm_model_max_token_size=32768,\n    llm_model_kwargs={\"host\": \"http://localhost:11434\", \"options\": {\"num_ctx\": 32768}},\n    embedding_func=EmbeddingFunc(\n        embedding_dim=768,\n        max_token_size=8192,\n        func=lambda texts: ollama_embedding(\n            texts, embed_model=\"nomic-embed-text\", host=\"http://localhost:11434\"\n        ),\n    ),\n    graph_storage=\"AGEStorage\",\n)\n\nwith open(\"./book.txt\", \"r\", encoding=\"utf-8\") as f:\n    rag.insert(f.read())\n\n# Perform naive search\nprint(\n    rag.query(\"What are the top themes in this story?\", param=QueryParam(mode=\"naive\"))\n)\n\n# Perform local search\nprint(\n    rag.query(\"What are the top themes in this story?\", param=QueryParam(mode=\"local\"))\n)\n\n# Perform global search\nprint(\n    rag.query(\"What are the top themes in this story?\", param=QueryParam(mode=\"global\"))\n)\n\n# Perform hybrid search\nprint(\n    rag.query(\"What are the top themes in this story?\", param=QueryParam(mode=\"hybrid\"))\n)\n\n# stream response\nresp = rag.query(\n    \"What are the top themes in this story?\",\n    param=QueryParam(mode=\"hybrid\", stream=True),\n)\n\n\nasync def print_stream(stream):\n    async for chunk in stream:\n        print(chunk, end=\"\", flush=True)\n\n\nif inspect.isasyncgen(resp):\n    asyncio.run(print_stream(resp))\nelse:\n    print(resp)\n"}
{"type": "source_file", "path": "examples/generate_query.py", "content": "from openai import OpenAI\n\n# os.environ[\"OPENAI_API_KEY\"] = \"\"\n\n\ndef openai_complete_if_cache(\n    model=\"gpt-4o-mini\", prompt=None, system_prompt=None, history_messages=[], **kwargs\n) -> str:\n    openai_client = OpenAI()\n\n    messages = []\n    if system_prompt:\n        messages.append({\"role\": \"system\", \"content\": system_prompt})\n    messages.extend(history_messages)\n    messages.append({\"role\": \"user\", \"content\": prompt})\n\n    response = openai_client.chat.completions.create(\n        model=model, messages=messages, **kwargs\n    )\n    return response.choices[0].message.content\n\n\nif __name__ == \"__main__\":\n    description = \"\"\n    prompt = f\"\"\"\n    Given the following description of a dataset:\n\n    {description}\n\n    Please identify 5 potential users who would engage with this dataset. For each user, list 5 tasks they would perform with this dataset. Then, for each (user, task) combination, generate 5 questions that require a high-level understanding of the entire dataset.\n\n    Output the results in the following structure:\n    - User 1: [user description]\n        - Task 1: [task description]\n            - Question 1:\n            - Question 2:\n            - Question 3:\n            - Question 4:\n            - Question 5:\n        - Task 2: [task description]\n            ...\n        - Task 5: [task description]\n    - User 2: [user description]\n        ...\n    - User 5: [user description]\n        ...\n    \"\"\"\n\n    result = openai_complete_if_cache(model=\"gpt-4o-mini\", prompt=prompt)\n\n    file_path = \"./queries.txt\"\n    with open(file_path, \"w\") as file:\n        file.write(result)\n\n    print(f\"Queries written to {file_path}\")\n"}
{"type": "source_file", "path": "examples/batch_eval.py", "content": "import re\nimport json\nimport jsonlines\n\nfrom openai import OpenAI\n\n\ndef batch_eval(query_file, result1_file, result2_file, output_file_path):\n    client = OpenAI()\n\n    with open(query_file, \"r\") as f:\n        data = f.read()\n\n    queries = re.findall(r\"- Question \\d+: (.+)\", data)\n\n    with open(result1_file, \"r\") as f:\n        answers1 = json.load(f)\n    answers1 = [i[\"result\"] for i in answers1]\n\n    with open(result2_file, \"r\") as f:\n        answers2 = json.load(f)\n    answers2 = [i[\"result\"] for i in answers2]\n\n    requests = []\n    for i, (query, answer1, answer2) in enumerate(zip(queries, answers1, answers2)):\n        sys_prompt = \"\"\"\n        ---Role---\n        You are an expert tasked with evaluating two answers to the same question based on three criteria: **Comprehensiveness**, **Diversity**, and **Empowerment**.\n        \"\"\"\n\n        prompt = f\"\"\"\n        You will evaluate two answers to the same question based on three criteria: **Comprehensiveness**, **Diversity**, and **Empowerment**.\n\n        - **Comprehensiveness**: How much detail does the answer provide to cover all aspects and details of the question?\n        - **Diversity**: How varied and rich is the answer in providing different perspectives and insights on the question?\n        - **Empowerment**: How well does the answer help the reader understand and make informed judgments about the topic?\n\n        For each criterion, choose the better answer (either Answer 1 or Answer 2) and explain why. Then, select an overall winner based on these three categories.\n\n        Here is the question:\n        {query}\n\n        Here are the two answers:\n\n        **Answer 1:**\n        {answer1}\n\n        **Answer 2:**\n        {answer2}\n\n        Evaluate both answers using the three criteria listed above and provide detailed explanations for each criterion.\n\n        Output your evaluation in the following JSON format:\n\n        {{\n            \"Comprehensiveness\": {{\n                \"Winner\": \"[Answer 1 or Answer 2]\",\n                \"Explanation\": \"[Provide explanation here]\"\n            }},\n            \"Empowerment\": {{\n                \"Winner\": \"[Answer 1 or Answer 2]\",\n                \"Explanation\": \"[Provide explanation here]\"\n            }},\n            \"Overall Winner\": {{\n                \"Winner\": \"[Answer 1 or Answer 2]\",\n                \"Explanation\": \"[Summarize why this answer is the overall winner based on the three criteria]\"\n            }}\n        }}\n        \"\"\"\n\n        request_data = {\n            \"custom_id\": f\"request-{i+1}\",\n            \"method\": \"POST\",\n            \"url\": \"/v1/chat/completions\",\n            \"body\": {\n                \"model\": \"gpt-4o-mini\",\n                \"messages\": [\n                    {\"role\": \"system\", \"content\": sys_prompt},\n                    {\"role\": \"user\", \"content\": prompt},\n                ],\n            },\n        }\n\n        requests.append(request_data)\n\n    with jsonlines.open(output_file_path, mode=\"w\") as writer:\n        for request in requests:\n            writer.write(request)\n\n    print(f\"Batch API requests written to {output_file_path}\")\n\n    batch_input_file = client.files.create(\n        file=open(output_file_path, \"rb\"), purpose=\"batch\"\n    )\n    batch_input_file_id = batch_input_file.id\n\n    batch = client.batches.create(\n        input_file_id=batch_input_file_id,\n        endpoint=\"/v1/chat/completions\",\n        completion_window=\"24h\",\n        metadata={\"description\": \"nightly eval job\"},\n    )\n\n    print(f\"Batch {batch.id} has been created.\")\n\n\nif __name__ == \"__main__\":\n    batch_eval()\n"}
{"type": "source_file", "path": "examples/lightrag_api_open_webui_demo.py", "content": "from datetime import datetime, timezone\nfrom fastapi import FastAPI\nfrom fastapi.responses import StreamingResponse\nimport inspect\nimport json\nfrom pydantic import BaseModel\nfrom typing import Optional\n\nimport os\nimport logging\nfrom lightrag import LightRAG, QueryParam\nfrom lightrag.llm import ollama_model_complete, ollama_embed\nfrom lightrag.utils import EmbeddingFunc\n\nimport nest_asyncio\n\nWORKING_DIR = \"./dickens\"\n\nlogging.basicConfig(format=\"%(levelname)s:%(message)s\", level=logging.INFO)\n\nif not os.path.exists(WORKING_DIR):\n    os.mkdir(WORKING_DIR)\n\nrag = LightRAG(\n    working_dir=WORKING_DIR,\n    llm_model_func=ollama_model_complete,\n    llm_model_name=\"qwen2.5:latest\",\n    llm_model_max_async=4,\n    llm_model_max_token_size=32768,\n    llm_model_kwargs={\"host\": \"http://localhost:11434\", \"options\": {\"num_ctx\": 32768}},\n    embedding_func=EmbeddingFunc(\n        embedding_dim=1024,\n        max_token_size=8192,\n        func=lambda texts: ollama_embed(\n            texts=texts, embed_model=\"bge-m3:latest\", host=\"http://127.0.0.1:11434\"\n        ),\n    ),\n)\n\nwith open(\"./book.txt\", \"r\", encoding=\"utf-8\") as f:\n    rag.insert(f.read())\n\n# Apply nest_asyncio to solve event loop issues\nnest_asyncio.apply()\n\napp = FastAPI(title=\"LightRAG\", description=\"LightRAG API open-webui\")\n\n\n# Data models\nMODEL_NAME = \"LightRAG:latest\"\n\n\nclass Message(BaseModel):\n    role: Optional[str] = None\n    content: str\n\n\nclass OpenWebUIRequest(BaseModel):\n    stream: Optional[bool] = None\n    model: Optional[str] = None\n    messages: list[Message]\n\n\n# API routes\n\n\n@app.get(\"/\")\nasync def index():\n    return \"Set Ollama link to http://ip:port/ollama in Open-WebUI Settings\"\n\n\n@app.get(\"/ollama/api/version\")\nasync def ollama_version():\n    return {\"version\": \"0.4.7\"}\n\n\n@app.get(\"/ollama/api/tags\")\nasync def ollama_tags():\n    return {\n        \"models\": [\n            {\n                \"name\": MODEL_NAME,\n                \"model\": MODEL_NAME,\n                \"modified_at\": \"2024-11-12T20:22:37.561463923+08:00\",\n                \"size\": 4683087332,\n                \"digest\": \"845dbda0ea48ed749caafd9e6037047aa19acfcfd82e704d7ca97d631a0b697e\",\n                \"details\": {\n                    \"parent_model\": \"\",\n                    \"format\": \"gguf\",\n                    \"family\": \"qwen2\",\n                    \"families\": [\"qwen2\"],\n                    \"parameter_size\": \"7.6B\",\n                    \"quantization_level\": \"Q4_K_M\",\n                },\n            }\n        ]\n    }\n\n\n@app.post(\"/ollama/api/chat\")\nasync def ollama_chat(request: OpenWebUIRequest):\n    resp = rag.query(\n        request.messages[-1].content, param=QueryParam(mode=\"hybrid\", stream=True)\n    )\n    if inspect.isasyncgen(resp):\n\n        async def ollama_resp(chunks):\n            async for chunk in chunks:\n                yield (\n                    json.dumps(\n                        {\n                            \"model\": MODEL_NAME,\n                            \"created_at\": datetime.now(timezone.utc).strftime(\n                                \"%Y-%m-%dT%H:%M:%S.%fZ\"\n                            ),\n                            \"message\": {\n                                \"role\": \"assistant\",\n                                \"content\": chunk,\n                            },\n                            \"done\": False,\n                        },\n                        ensure_ascii=False,\n                    ).encode(\"utf-8\")\n                    + b\"\\n\"\n                )  # the b\"\\n\" is important\n\n        return StreamingResponse(ollama_resp(resp), media_type=\"application/json\")\n    else:\n        return resp\n\n\n@app.get(\"/health\")\nasync def health_check():\n    return {\"status\": \"healthy\"}\n\n\nif __name__ == \"__main__\":\n    import uvicorn\n\n    uvicorn.run(app, host=\"0.0.0.0\", port=8020)\n"}
{"type": "source_file", "path": "Gradio_web.py", "content": "import ast\nimport json\nimport mimetypes\nimport os\nimport shutil\nimport sys\nimport webbrowser\nimport zipfile\nimport subprocess\nimport asyncio\nimport importlib.metadata\n\nimport pkg_resources\nfrom IPython.terminal.ipapp import frontend_flags\nfrom packaging.requirements import Requirement\nfrom packaging.version import Version, InvalidVersion\nfrom importlib.metadata import distributions\nfrom time import sleep\nfrom typing import List, Tuple\nfrom datetime import datetime, time, timedelta\n\nimport gradio as gr\nimport httpx\nfrom click import style\nfrom dotenv import load_dotenv, set_key\nfrom pathlib import Path\n\nfrom fastapi import requests\nfrom numba.typed.listobject import new_list\n\nfrom playwright.sync_api import sync_playwright\nfrom scipy.ndimage import label\nfrom sympy import false\n\nload_dotenv()\n\n# åŠ è½½ .env æ–‡ä»¶\nENV_FILE = \".env\"\nenv_path = Path(ENV_FILE)\nPort = os.getenv(\"API_port\",\"\")\n# é…ç½® RAG åç«¯çš„åŸºç¡€ URL\nRAG_API_URL = f\"http://localhost:{Port}/v1\"\n# Constants\nSUPPORTED_FILE_TYPES = ['txt','pdf','doc','ppt','csv']\nFILE_BACKUP_DIR = \"./backup/files\"\nGRAPH_BACKUP_DIR = \"./backup/graph\"\nENV_VARS = os.getenv(\"RAG_DIR\",\"\")\nBUILT_YOUR_GRAPH_SCRIPT = \"./build_your_graph.py\"\n\nStart_page_IsNotShow = os.getenv(\"start_page_show\",\"\") == 'True'\n\n# åˆ›å»ºå¿…è¦çš„å¤‡ä»½ç›®å½•\nos.makedirs(FILE_BACKUP_DIR, exist_ok=True)\nos.makedirs(GRAPH_BACKUP_DIR, exist_ok=True)\n\n# ç¯å¢ƒå˜é‡è·å–ä¸æ›´æ–°<å¼€å§‹>\n\ndef get_env_variables():\n    \"\"\"\n    è¯»å–æ‰€æœ‰ç¯å¢ƒå˜é‡å¹¶ä»¥å­—å…¸å½¢å¼è¿”å›\n    \"\"\"\n    keys = [\n        \"RAG_DIR\",\n        \"file_DIR\",\n        \"API_port\",\n        \"OPENAI_API_KEY\",\n        \"LLM_MODEL\",\n        \"LLM_MODEL_TOKEN_SIZE\",\n        \"EMBEDDING_MODEL\",\n        \"EMBEDDING_MAX_TOKEN_SIZE\",\n        \"OPENAI_BASE_URL\",\n        \"start_page_IsNotShow\",\n        \"FRONTEND_PORT\",\n    ]\n    return {key: os.getenv(key, \"\") for key in keys}\n\ndef update_env_variable(key, value):\n    \"\"\"\n    æ›´æ–° .env æ–‡ä»¶ä¸­çš„æŸä¸ªç¯å¢ƒå˜é‡\n    \"\"\"\n    if key not in get_env_variables():\n        return f\"Error: {key} is not a valid environment variable.\"\n\n    set_key(env_path, key, value)\n    os.environ[key] = value  # åŒæ—¶æ›´æ–°å½“å‰ç¯å¢ƒå˜é‡\n    return f\"Successfully updated {key} to {value}.\"\n\ndef reset_env_variable(key):\n    \"\"\"\n    é‡ç½®æŸä¸ªç¯å¢ƒå˜é‡ä¸ºç©º\n    \"\"\"\n    return update_env_variable(key, \"\")\n\n# ç¯å¢ƒå˜é‡è·å–ä¸æ›´æ–°<ç»“æŸ>\n\n\n# æ–‡æ¡£æ–‡ä»¶ç®¡ç†<å¼€å§‹>\n\ndef list_files_in_folder(folder=\"./files\"):\n    \"\"\"\n    åˆ—å‡ºæŒ‡å®šç›®å½•åŠå­ç›®å½•ä¸‹çš„æ”¯æŒçš„æ–‡ä»¶ç±»å‹ï¼Œå¹¶è¿”å›ç›¸å¯¹è·¯å¾„åˆ—è¡¨ï¼ŒæŒ‰æ–‡ä»¶åæ’åºã€‚\n\n    å‚æ•°:\n        folder (str): æŒ‡å®šçš„æ ¹ç›®å½•ï¼Œé»˜è®¤ä¸º \"./text\"ã€‚\n\n    è¿”å›:\n        list: åŒ…å«ç›¸å¯¹è·¯å¾„çš„æ–‡ä»¶åˆ—è¡¨ã€‚\n    \"\"\"\n    all_files = []\n    folder = os.path.abspath(folder)  # è·å–æ ¹ç›®å½•çš„ç»å¯¹è·¯å¾„\n\n    for root, _, files in os.walk(folder):\n        for file in files:\n            if file.split(\".\")[-1].lower() in SUPPORTED_FILE_TYPES:\n                # æ‹¼æ¥æ–‡ä»¶è·¯å¾„å¹¶ç¡®ä¿æ˜¯ç›¸å¯¹è·¯å¾„\n                full_path = os.path.join(root, file)\n                rel_path = \".\" + os.path.relpath(full_path, start=os.getcwd())  # è½¬ä¸ºç›¸å¯¹è·¯å¾„\n                rel_path = rel_path.replace(\"\\\\\", \"/\")\n                all_files.append(full_path)\n    return sorted(all_files)\n\ndef refresh_file_list_display():\n    \"\"\"\n    åˆ·æ–°æ–‡ä»¶åˆ—è¡¨ï¼Œè¿”å›æ–‡ä»¶å Markdown åˆ—è¡¨å’Œæ–‡ä»¶è·¯å¾„å­—å…¸\n    \"\"\"\n    files = list_files_in_folder()\n    file_dict = {}\n\n    for file_path in files:\n        file_name = os.path.basename(file_path)\n\n        # å¦‚æœæ–‡ä»¶åå·²å­˜åœ¨ï¼Œæ·»åŠ æ–‡ä»¶åˆ›å»ºæ—¶é—´ä½œä¸ºåç¼€\n        if file_name in file_dict:\n            creation_time = datetime.fromtimestamp(os.path.getctime(file_path)).strftime('%Y-%m-%d_%H-%M-%S')\n            unique_file_name = f\"{file_name}--{creation_time}\"\n            file_dict[unique_file_name] = file_path # ç‰¹æ®Šæ–‡ä»¶å -> æ–‡ä»¶è·¯å¾„\n        else:\n            file_dict[file_name] = file_path  # æ–‡ä»¶å -> æ–‡ä»¶è·¯å¾„\n    markdown_list = \"\\n\".join(file_dict.keys())  # ç”Ÿæˆæ–‡ä»¶ååˆ—è¡¨\n    return markdown_list, file_dict\n\ndef refresh_dropdown_choices(file_dict):\n    \"\"\"\n    æ ¹æ®æ–‡ä»¶å­—å…¸ç”Ÿæˆ Dropdown çš„å¯é€‰é¡¹\n    \"\"\"\n    if file_dict is None:\n        return []  # é˜²æ­¢é”™è¯¯å‘ç”Ÿï¼Œè¿”å›ç©ºåˆ—è¡¨\n    #print(list(file_dict.keys()))\n    return list(file_dict.keys())  # è¿”å›æ‰€æœ‰æ–‡ä»¶å\n\ndef derefresh_dropdown_choices_temp():\n    \"\"\"\n    æ ¹æ®æ–‡ä»¶å­—å…¸ç”Ÿæˆ Dropdown çš„å¯é€‰é¡¹\n    \"\"\"\n    markdown_list, file_dict = refresh_file_list_display()\n    if file_dict is None:\n        return []  # é˜²æ­¢é”™è¯¯å‘ç”Ÿï¼Œè¿”å›ç©ºåˆ—è¡¨\n    #print(list(file_dict.keys()))\n    return list(file_dict.keys())  # è¿”å›æ‰€æœ‰æ–‡ä»¶å\n\ndef handle_file_selection(file_name, file_dict):\n    \"\"\"æ ¹æ®é€‰æ‹©çš„æ–‡ä»¶åè¿”å›å®Œæ•´è·¯å¾„\"\"\"\n    return file_dict.get(file_name, None)\n\ndef open_text_folder(folder_paths):\n    \"\"\"åœ¨æ–‡ä»¶èµ„æºç®¡ç†å™¨ä¸­æ‰“å¼€æŒ‡å®šæ–‡ä»¶å¤¹ï¼Œåˆ é™¤è·¯å¾„ä¸­çš„æ–‡ä»¶å\"\"\"\n    folder_paths = eval(folder_paths)\n    if isinstance(folder_paths, list):\n\n        for folder_path in folder_paths:\n            folder_path = os.path.dirname(folder_path)  # è·å–æ–‡ä»¶å¤¹è·¯å¾„\n            try:\n                if os.name == \"nt\":  # Windows\n                    os.startfile(folder_path)\n                elif os.name == \"posix\":  # macOS/Linux\n                    os.system(f\"open {folder_path}\" if sys.platform == \"darwin\" else f\"xdg-open {folder_path}\")\n            except Exception as e:\n                return f\"æ‰“å¼€æ–‡ä»¶å¤¹å¤±è´¥ï¼š{str(e)}\"\n    else:\n        return \"é”™è¯¯ï¼šæœªä¼ å…¥æ–‡ä»¶å¤¹è·¯å¾„åˆ—è¡¨ã€‚\"\n\n    return f\"å·²æˆåŠŸæ‰“å¼€ {len(folder_paths)} ä¸ªæ–‡ä»¶å¤¹ã€‚\"\n\ndef open_text_file(file_paths):\n    \"\"\"ä½¿ç”¨ç³»ç»Ÿé»˜è®¤ç¨‹åºæ‰“å¼€å¤šä¸ªæ–‡ä»¶\"\"\"\n    file_paths = eval(file_paths)\n    if not isinstance(file_paths, list) or len(file_paths) == 0:\n        return \"é”™è¯¯ï¼šæœªä¼ å…¥æ–‡ä»¶è·¯å¾„åˆ—è¡¨ã€‚\"\n\n    results = []\n    for file_path in file_paths:\n        if not os.path.isfile(file_path):\n            results.append(f\"âŒ æ–‡ä»¶ä¸å­˜åœ¨ï¼š{file_path}\")\n            continue\n\n        try:\n            if os.name == \"nt\":  # Windows\n                os.startfile(file_path)\n            elif os.name == \"posix\":  # macOS/Linux\n                os.system(f\"open {file_path}\" if sys.platform == \"darwin\" else f\"xdg-open {file_path}\")\n            results.append(f\"âœ… æ–‡ä»¶ {file_path} å·²æ‰“å¼€ã€‚\")\n        except Exception as e:\n            results.append(f\"âŒ æ‰“å¼€æ–‡ä»¶å¤±è´¥ï¼š{file_path}ï¼Œé”™è¯¯ï¼š{str(e)}\")\n\n    return \"\\n\".join(results)\n\ndef set_rag_env_variable(file_paths):\n    \"\"\"\n    è®¾ç½® file_DIR ç¯å¢ƒå˜é‡çš„å€¼ä¸ºæŒ‡å®šæ–‡ä»¶çš„è·¯å¾„ã€‚\n\n    å‚æ•°:\n    - file_path (str): æ–‡ä»¶è·¯å¾„ï¼Œå¿…é¡»æ˜¯ç›¸å¯¹è·¯å¾„ä¸”åœ¨ ./text/ ç›®å½•ä¸‹ã€‚\n\n    è¿”å›:\n    - str: è®¾ç½®ç»“æœä¿¡æ¯ã€‚\n    \"\"\"\n    file_paths = eval(file_paths)\n    file_path = (\"./\" + os.path.relpath(file_paths[0], start=os.getcwd())).replace(\"\\\\\", \"/\")  # è½¬ä¸ºç›¸å¯¹è·¯å¾„\n    print(file_path)\n    # éªŒè¯æ–‡ä»¶è·¯å¾„æ˜¯å¦ç¬¦åˆè¦æ±‚\n    if not file_path.startswith(\"./files/\"):\n        return \"Error: æ–‡ä»¶è·¯å¾„å¿…é¡»ä½äº ./files/ ç›®å½•ä¸‹ã€‚\"\n\n    if not os.path.isfile(file_path):\n        return f\"Error: æ–‡ä»¶ {file_path} ä¸å­˜åœ¨ã€‚\"\n\n    # è·å–å½“å‰ file_DIR çš„å€¼\n    current_value = os.getenv(\"file_DIR\", \"\")\n    reset_result = reset_env_variable(\"file_DIR\")  # é‡ç½® RAG_DIR ç¯å¢ƒå˜é‡\n\n    if \"Error\" in reset_result:\n        return f\"é‡ç½® file_DIR å¤±è´¥: {reset_result}\"\n\n    # å°†è·¯å¾„è½¬æ¢ä¸º Windows é£æ ¼ï¼ˆç”¨åæ–œæ ï¼‰\n    windows_style_path = file_path.replace(\"\\\\\", \"/\")\n\n    # æ›´æ–° .env æ–‡ä»¶å’Œç¯å¢ƒå˜é‡\n    update_result = update_env_variable(\"file_DIR\", windows_style_path)\n\n    if \"Successfully updated\" in update_result:\n        return (\n            f\"file_DIR æ›´æ–°æˆåŠŸï¼\\n\"\n            f\"æ—§å€¼: {current_value}\\n\"\n            f\"æ–°å€¼: {windows_style_path}\"\n        )\n    else:\n        return f\"æ›´æ–°å¤±è´¥: {update_result}\"\n\ndef delete_file_with_backup(file_paths):\n    \"\"\"åˆ é™¤å¤šä¸ªæ–‡ä»¶ï¼Œå…ˆå¤‡ä»½ååˆ é™¤\"\"\"\n    file_paths = eval(file_paths)\n    if not isinstance(file_paths, list) or len(file_paths) == 0:\n        return \"é”™è¯¯ï¼šæœªä¼ å…¥æ–‡ä»¶è·¯å¾„åˆ—è¡¨ã€‚\"\n\n    results = []\n    for file_path in file_paths:\n        try:\n            if not os.path.isfile(file_path):\n                results.append(f\"âŒ æ–‡ä»¶ä¸å­˜åœ¨ï¼š{file_path}\")\n                continue\n\n            backup_name = os.path.join(\n                FILE_BACKUP_DIR,\n                f\"{os.path.basename(file_path)}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n            )\n            shutil.copy2(file_path, backup_name)  # å¤‡ä»½æ–‡ä»¶\n            os.remove(file_path)  # åˆ é™¤æ–‡ä»¶\n            results.append(f\"âœ… æ–‡ä»¶ {file_path} å·²åˆ é™¤ï¼Œå¤‡ä»½å­˜å‚¨åœ¨ {backup_name}\")\n        except Exception as e:\n            results.append(f\"âŒ åˆ é™¤å¤±è´¥ï¼š{file_path}ï¼Œé”™è¯¯ï¼š{str(e)}\")\n\n    return \"\\n\".join(results)\n\ndef create_unique_folder(file_name):\n    \"\"\"\n    æ ¹æ®æ–‡ä»¶ååœ¨ ./files ä¸­åˆ›å»ºå”¯ä¸€çš„æ–‡ä»¶å¤¹ï¼Œä¸åŒ…å«æ–‡ä»¶æ ¼å¼åç¼€ã€‚\n    \"\"\"\n    base_folder = \"./files\"\n    # å»é™¤æ–‡ä»¶åçš„åç¼€\n    folder_name = os.path.splitext(file_name)[0]\n    folder_path = os.path.join(base_folder, folder_name)\n\n    # å¦‚æœæ–‡ä»¶å¤¹å·²å­˜åœ¨ï¼Œåˆ™ç”Ÿæˆå¸¦æœ‰æ—¶é—´æˆ³çš„å”¯ä¸€æ–‡ä»¶å¤¹åç§°\n    if os.path.exists(folder_path):\n        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        folder_name = f\"{folder_name}_{timestamp}\"\n        folder_path = os.path.join(base_folder, folder_name)\n\n    # åˆ›å»ºæ–‡ä»¶å¤¹\n    os.makedirs(folder_path, exist_ok=True)\n    return folder_path\n\ndef upload_files_and_save(files):\n    \"\"\"\n    ä¸Šä¼ å¤šä¸ªæ–‡ä»¶å¹¶ä¿å­˜åˆ°æ–°åˆ›å»ºçš„æ–‡ä»¶å¤¹ä¸­ã€‚\n    :param files: ä¸Šä¼ çš„æ–‡ä»¶åˆ—è¡¨ï¼ˆGradio è¿”å›çš„æ–‡ä»¶åˆ—è¡¨ï¼‰\n    :return: æ¯ä¸ªæ–‡ä»¶çš„å¤„ç†ç»“æœåˆ—è¡¨\n    \"\"\"\n    if not files or len(files) == 0:\n        return \"æœªä¸Šä¼ ä»»ä½•æ–‡ä»¶ã€‚\"\n\n    results = []  # ç”¨äºå­˜å‚¨æ¯ä¸ªæ–‡ä»¶çš„å¤„ç†ç»“æœ\n    uploaded_files = {}  # ç”¨äºå­˜å‚¨æˆåŠŸä¸Šä¼ æ–‡ä»¶çš„å­—å…¸\n\n    for file in files:\n        file_name = os.path.basename(file)\n        try:\n            # ç¡®å®šæ–‡ä»¶åå’Œæ‰©å±•å\n            file_ext = file_name.split('.')[-1].lower()\n\n            # éªŒè¯æ–‡ä»¶ç±»å‹\n            if file_ext not in SUPPORTED_FILE_TYPES:\n                results.append(f\"âŒ ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_name} ({file_ext})ã€‚æ”¯æŒçš„ç±»å‹åŒ…æ‹¬: {', '.join(SUPPORTED_FILE_TYPES)}\")\n                continue\n\n            # åˆ›å»ºå”¯ä¸€æ–‡ä»¶å¤¹\n            folder_path = create_unique_folder(file_name)\n            os.makedirs(folder_path, exist_ok=True)\n\n            # ç›®æ ‡æ–‡ä»¶è·¯å¾„\n            file_path = os.path.join(folder_path, file_name)\n\n            # è·å– Gradio è¿”å›çš„æ–‡ä»¶è·¯å¾„å¹¶å¤åˆ¶åˆ°ç›®æ ‡è·¯å¾„\n            shutil.copy(file, file_path)\n\n            # è®°å½•æˆåŠŸä¸Šä¼ çš„æ–‡ä»¶\n            uploaded_files[file_name] = file_path\n\n            results.append(f\"âœ… æ–‡ä»¶ {file_name} ä¸Šä¼ æˆåŠŸï¼Œå·²ä¿å­˜è‡³æ–‡ä»¶å¤¹: {folder_path}\")\n        except Exception as e:\n            results.append(f\"âŒ æ–‡ä»¶ {file_name} ä¸Šä¼ è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}\")\n\n    return \"\\n\".join(results),uploaded_files,uploaded_files\n\ndef debug_file(file):\n    if not file:\n        return \"æœªä¸Šä¼ ä»»ä½•æ–‡ä»¶ã€‚\"\n\n    try:\n        return {\n            \"æ–‡ä»¶å\": file.name,\n            \"æ–‡ä»¶ç±»å‹\": str(type(file)),\n            \"æ”¯æŒçš„æ“ä½œ\": dir(file),\n        }\n    except Exception as e:\n        return f\"è°ƒè¯•æ–‡ä»¶ä¿¡æ¯æ—¶å‡ºé”™: {str(e)}\"\n\ndef build_graph_for_files(prebuild_dict:dict):\n    \"\"\"\n    æ„å»ºçŸ¥è¯†å›¾è°±ï¼šè°ƒç”¨æœåŠ¡ç«¯æ¥å£å¤„ç†å¤šä¸ªæ–‡ä»¶\n    :param prebuild_dict: å­—å…¸ï¼Œkey æ˜¯æ–‡ä»¶åï¼Œvalue æ˜¯æ–‡ä»¶è·¯å¾„\n    :return: å¤šæ–‡ä»¶çš„æ„å»ºç»“æœ\n    \"\"\"\n    if isinstance(prebuild_dict, str):\n        prebuild_dict = ast.literal_eval(prebuild_dict)\n    base_path = \"./graph\"\n    file_name = list(prebuild_dict.keys())\n    timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n    graph_path = os.path.join(base_path, file_name[0])\n\n    if os.path.exists(graph_path):\n        graph_path = f\"{graph_path}_{timestamp}\"\n    os.makedirs(graph_path, exist_ok=True)\n    graph_path = graph_path.replace(\"\\\\\", \"/\")\n    update_env_variable(\"RAG_DIR\", graph_path)\n    try:\n        # è°ƒç”¨æ–‡ä»¶ä¸Šä¼ ä¸å¤„ç†æ¥å£\n        sleep(1.0)\n        response = asyncio.run(upload_files_to_rag(prebuild_dict))\n\n        # å¦‚æœå“åº”ä¸­åŒ…å«æ–‡ä»¶å¤„ç†ç»“æœï¼Œåˆ™è¿”å›\n        if isinstance(response, list):\n            return response  # è¿”å›æœåŠ¡ç«¯çš„æ–‡ä»¶å¤„ç†ç»“æœåˆ—è¡¨\n        else:\n            # å¦åˆ™è¿”å›é”™è¯¯ä¿¡æ¯\n            return [{\"status\": \"failed\", \"message\": response.get(\"message\", \"Unknown error\")}]\n    except Exception as e:\n        return [{\"status\": \"failed\", \"message\": f\"Failed to build graph: {str(e)}\"}]\n\ndef insert_graph_for_files(preinsert_dict):\n    \"\"\"\n    æ„å»ºçŸ¥è¯†å›¾è°±ï¼šè°ƒç”¨æœåŠ¡ç«¯æ¥å£å¤„ç†å¤šä¸ªæ–‡ä»¶\n    :param preinsert_dict: å­—å…¸ï¼Œkey æ˜¯æ–‡ä»¶åï¼Œvalue æ˜¯æ–‡ä»¶è·¯å¾„\n    :return: å¤šæ–‡ä»¶çš„æ’å…¥ç»“æœ\n    \"\"\"\n    if isinstance(preinsert_dict, str):\n        preinsert_dict = ast.literal_eval(preinsert_dict)\n    try:\n        # è°ƒç”¨æ–‡ä»¶ä¸Šä¼ ä¸å¤„ç†æ¥å£\n        response = asyncio.run(upload_files_to_rag(preinsert_dict))\n\n        # å¦‚æœå“åº”ä¸­åŒ…å«æ–‡ä»¶å¤„ç†ç»“æœï¼Œåˆ™è¿”å›\n        if isinstance(response, list):\n            return response  # è¿”å›æœåŠ¡ç«¯çš„æ–‡ä»¶å¤„ç†ç»“æœåˆ—è¡¨\n        else:\n            # å¦åˆ™è¿”å›é”™è¯¯ä¿¡æ¯\n            return [{\"status\": \"failed\", \"message\": response.get(\"message\", \"Unknown error\")}]\n    except Exception as e:\n        return [{\"status\": \"failed\", \"message\": f\"Failed to insert graph: {str(e)}\"}]\n\n\nasync def upload_files_to_rag(prebuild_dict_result, purpose=\"knowledge_graph_frontend\"):\n    \"\"\"\n    ä¸Šä¼ æ–‡ä»¶åå’Œè·¯å¾„å­—å…¸åˆ° RAG ç³»ç»Ÿ\n    :param prebuild_dict_result: å­—å…¸ï¼Œkey æ˜¯æ–‡ä»¶åï¼Œvalue æ˜¯æ–‡ä»¶è·¯å¾„\n    :param purpose: ä¸Šä¼ çš„ç›®çš„\n    :return: æœåŠ¡ç«¯è¿”å›çš„å¤šæ–‡ä»¶å¤„ç†ç»“æœ\n    \"\"\"\n    load_dotenv(override=True)\n    retries = 5  # æœ€å¤§é‡è¯•æ¬¡æ•°\n    async with httpx.AsyncClient(timeout=300.0) as client:\n        for attempt in range(1, retries + 1):\n            try:\n                # æ„é€ è¯·æ±‚ä½“\n                payload = {\n                    \"files\": prebuild_dict_result,\n                    \"purpose\": purpose,\n                }\n                response = await client.post(RAG_API_URL + f\"/files\", json=payload)\n\n                # æ£€æŸ¥å“åº”çŠ¶æ€\n                if response.status_code == 200:\n                    find_html_file(os.getenv(\"RAG_DIR\"))\n                    return response.json()  # æˆåŠŸè¿”å› JSON æ•°æ®\n                else:\n                    find_html_file(os.getenv(\"RAG_DIR\"))\n                    return {\n                        \"status\": \"failed\",\n                        \"message\": f\"Server returned error: {response.status_code}, {response.text}\",\n                    }\n            except Exception as e:\n                if attempt == retries:\n                    return {\n                        \"status\": \"failed\",\n                        \"message\": f\"Failed to communicate with server: {str(e)}\",\n                    }\n\ndef debug_and_return(name):\n    \"\"\"\n    è¿”å›æ–‡ä»¶åï¼ŒåŒæ—¶è¾“å‡ºè°ƒè¯•ä¿¡æ¯\n    \"\"\"\n    #debug_message = f\"è°ƒè¯•ï¼šå½“å‰é€‰æ‹©çš„æ–‡ä»¶åæ˜¯ {name}\"\n    #print(debug_message)  # æ§åˆ¶å°è°ƒè¯•\n    return name\n\n\n# æ–‡æ¡£æ–‡ä»¶ç®¡ç†<ç»“æŸ>\n\n\n# å›¾è°±ç®¡ç†<å¼€å§‹>\n'''\ndef setup_file_upload_interaction(file_uploader, purpose_input, upload_button, upload_result):\n    \"\"\"è®¾ç½®æ–‡ä»¶ä¸Šä¼ äº¤äº’é€»è¾‘\"\"\"\n    upload_button.click(\n        fn=upload_file_to_rag,\n        inputs=[file_uploader, purpose_input],\n        outputs=upload_result,\n    )\n'''\ndef list_subdirectories(base_path=\"./graph\"):\n    \"\"\"\n    åˆ—å‡ºæŒ‡å®šæ–‡ä»¶å¤¹ä¸‹çš„æ‰€æœ‰æ¬¡çº§æ–‡ä»¶å¤¹ï¼Œå¹¶è¿”å›æ–‡ä»¶å¤¹åç§°ä¸å…¶ç»å¯¹è·¯å¾„çš„æ˜ å°„å­—å…¸ã€‚\n    å¯¹äºé‡åæ–‡ä»¶å¤¹ï¼Œæ·»åŠ åˆ›å»ºæ—¶é—´åç¼€ä»¥åŒºåˆ†ã€‚\n    \"\"\"\n    if not os.path.exists(base_path):\n        return {}, \"The specified base path does not exist.\"\n\n    subdirectories = [d for d in os.listdir(base_path) if os.path.isdir(os.path.join(base_path, d))]\n    folder_dict = {}\n\n    for folder in subdirectories:\n        folder_path = os.path.join(base_path, folder)\n        if folder in folder_dict:\n            # å¦‚æœå‡ºç°åŒåæ–‡ä»¶å¤¹ï¼Œæ·»åŠ åˆ›å»ºæ—¶é—´åç¼€\n            creation_time = datetime.fromtimestamp(os.path.getctime(folder_path)).strftime('%Y-%m-%d_%H-%M-%S')\n            unique_folder_name = f\"{folder}--{creation_time}\"\n            folder_dict[unique_folder_name] = os.path.abspath(folder_path)\n        else:\n            folder_dict[folder] = os.path.abspath(folder_path)\n    # ç”Ÿæˆ Markdown æ ¼å¼æ–‡ä»¶ååˆ—è¡¨\n    markdown_list = \"\\n\".join(list(folder_dict.keys()))\n    selective_list = list(folder_dict.keys())\n    return markdown_list,folder_dict,selective_list\n\ndef open_rag_folder(folder_path):\n    \"\"\"åœ¨æ–‡ä»¶èµ„æºç®¡ç†å™¨ä¸­æ‰“å¼€æŒ‡å®šæ–‡ä»¶å¤¹\"\"\"\n    if os.name == \"nt\":  # Windows\n        os.startfile(folder_path)\n    elif os.name == \"posix\":  # macOS/Linux\n        os.system(f\"open {folder_path}\" if sys.platform == \"darwin\" else f\"xdg-open {folder_path}\")\n\ndef backup_and_delete_graph_folder(selected_graph_abs_path):\n    \"\"\"\n    å¤‡ä»½çŸ¥è¯†å›¾è°±æ–‡ä»¶å¤¹å¹¶åˆ é™¤åŸè·¯å¾„\n    :param selected_graph_abs_path: å³å°†è¦åˆ é™¤çš„è·¯å¾„ï¼ˆç»å¯¹è·¯å¾„ï¼‰\n    :return: æ“ä½œç»“æœå­—ç¬¦ä¸²\n    \"\"\"\n    try:\n        # æ£€æŸ¥è·¯å¾„æœ‰æ•ˆæ€§\n        if not selected_graph_abs_path or not os.path.exists(selected_graph_abs_path):\n            return \"æ— æ³•å¤‡ä»½ï¼Œè·¯å¾„ä¸å­˜åœ¨æˆ–æœªæä¾›ã€‚\"\n\n        # æå–æ–‡ä»¶å¤¹åä½œä¸ºå˜é‡\n        folder_name = os.path.basename(os.path.normpath(selected_graph_abs_path))\n        PreBackup_folder = f\"{folder_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n        backup_path = os.path.join(GRAPH_BACKUP_DIR, PreBackup_folder)\n\n        # åˆ›å»ºå¤‡ä»½æ–‡ä»¶å¤¹\n        os.makedirs(GRAPH_BACKUP_DIR, exist_ok=True)\n\n        # å¤‡ä»½è·¯å¾„\n        shutil.copytree(selected_graph_abs_path, backup_path)\n\n        # åˆ é™¤åŸè·¯å¾„\n        shutil.rmtree(selected_graph_abs_path)\n\n        return f\"å¤‡ä»½æˆåŠŸï¼å›¾è°±å·²å¤‡ä»½è‡³ {backup_path}ï¼Œå¹¶æˆåŠŸåˆ é™¤å›¾è°±ã€‚\"\n    except Exception as e:\n        return f\"å¤‡ä»½æˆ–åˆ é™¤è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}\"\n\ndef find_html_file(folder_path, filename=\"knowledge_graph.html\"):\n    \"\"\"åœ¨æŒ‡å®šæ–‡ä»¶å¤¹ä¸‹é€’å½’æŸ¥æ‰¾ HTML æ–‡ä»¶\"\"\"\n    for root, _, files in os.walk(folder_path):\n        if filename in files:\n            file_path = os.path.join(root, filename)\n            webbrowser.open(file_path)\n            return os.path.join(root, filename)\n    return None\n\n# å•ä¸ª ZIP æ–‡ä»¶è§£å‹é€»è¾‘\nasync def upload_and_extract_zip(file, base_path=\"./files\"):\n    \"\"\"ä¸Šä¼ å¹¶è§£å‹ zip æ–‡ä»¶\"\"\"\n    folder_name = os.path.splitext(os.path.basename(file.name))[0]\n    timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n    dest_folder = os.path.join(base_path, folder_name)\n\n    if os.path.exists(dest_folder):\n        dest_folder = f\"{dest_folder}_{timestamp}\"\n    os.makedirs(dest_folder, exist_ok=True)\n\n    try:\n        with zipfile.ZipFile(file, 'r') as zip_ref:\n            # å°è¯•ä»¥ GBK è§£ç \n            for zip_info in zip_ref.infolist():\n                zip_info.filename = zip_info.filename.encode('cp437').decode('utf-8')  # è½¬æ¢ç¼–ç \n                zip_ref.extract(zip_info, dest_folder)\n            print(\"utf8\")\n    except UnicodeDecodeError:\n        # ä½¿ç”¨ GBK é‡è¯» ZIP æ–‡ä»¶\n        with zipfile.ZipFile(file, 'r') as zip_ref:\n            for zip_info in zip_ref.infolist():\n                zip_info.filename = zip_info.filename.encode('cp437').decode('gbk')  # è½¬æ¢ç¼–ç \n                zip_ref.extract(zip_info, dest_folder)\n            print(\"gbk\")\n\n\n    return f\"âœ… æ–‡ä»¶ {file.name} å·²è§£å‹è‡³: {dest_folder}\"\n\n# å°†å¤šä¸ªæ–‡ä»¶å¤„ç†çš„é€»è¾‘æ‹†åˆ†å‡ºæ¥\nasync def process_uploaded_zips_with_progress(files,progress=gr.Progress(track_tqdm=True)):\n    \"\"\"å¤„ç†å¤šä¸ª ZIP æ–‡ä»¶çš„è§£å‹é€»è¾‘ï¼Œä½¿ç”¨ Gradio è¿›åº¦æ¡\"\"\"\n    if not files or len(files) == 0:\n        return \"æœªä¸Šä¼ ä»»ä½•æ–‡ä»¶ã€‚\"\n    idx = 0\n    results = []\n    total_files = len(files)\n    progress(0,desc=\"æ­£åœ¨å¤„ç†ä¸­ï¼Œè¯·ç¨å...\",total=total_files)# ä½¿ç”¨ Gradio çš„è¿›åº¦æ¡\n    for file in progress.tqdm(files):\n        try:\n            result = await upload_and_extract_zip(file)\n            results.append(result)\n        except Exception as e:\n            results.append(f\"âŒ æ–‡ä»¶ {file.name} è§£å‹å¤±è´¥: {str(e)}\")\n        progress.update(idx + 1)  # æ›´æ–°è¿›åº¦\n\n    return \"\\n\".join(results)\n\n\ndef set_env_variable_from_folder(folder_path):\n    \"\"\"å°†æ–‡ä»¶å¤¹è·¯å¾„è®¾ç½®ä¸ºç¯å¢ƒå˜é‡\"\"\"\n    update_env_variable(\"RAG_DIR\", folder_path)\n    return f\"å·²å°†è·¯å¾„ {folder_path} è®¾ç½®ä¸ºç¯å¢ƒå˜é‡ RAG_DIR\"\n\n# å›¾è°±ç®¡ç†<ç»“æŸ>\n\n# æ¬¢è¿ç•Œé¢<å¼€å§‹>\n\ndef load_readme():\n    \"\"\"åŠ è½½ README.md å†…å®¹\"\"\"\n    try:\n        with open(\"README.md\", \"r\", encoding=\"utf-8\") as file:\n            return file.read()\n    except FileNotFoundError:\n        return \"README.md æ–‡ä»¶æœªæ‰¾åˆ°ï¼Œè¯·æ£€æŸ¥é¡¹ç›®ç›®å½•ã€‚\"\n\ndef load_license():\n    \"\"\"åŠ è½½å¼€æºåè®®å†…å®¹\"\"\"\n    try:\n        with open(\"LICENSE\", \"r\", encoding=\"utf-8\") as file:\n            return file.read()\n    except FileNotFoundError:\n        return \"LICENSE æ–‡ä»¶æœªæ‰¾åˆ°ï¼Œè¯·æ£€æŸ¥é¡¹ç›®ç›®å½•ã€‚\"\n\n\ndef load_requirements(file_path=\"requirements.txt\"):\n    \"\"\"\n    è¯»å– requirements.txt ä¸­çš„ä¾èµ–åŒ…ä¿¡æ¯ï¼Œå¹¶æ”¯æŒå¤æ‚ç‰ˆæœ¬çº¦æŸã€‚\n    :param file_path: requirements.txt æ–‡ä»¶è·¯å¾„\n    :return: (åŒ…ååˆ—è¡¨, å®Œæ•´ä¾èµ–è¡Œåˆ—è¡¨)\n    \"\"\"\n    try:\n        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n            lines = f.read().splitlines()\n\n        package_names = []\n        valid_requirements = []\n\n        for line in lines:\n            line = line.strip()\n            if not line or line.startswith(\"#\"):\n                # å¿½ç•¥ç©ºè¡Œå’Œæ³¨é‡Šè¡Œ\n                continue\n\n            try:\n                # è§£æä¾èµ–é¡¹\n                req = Requirement(line)\n                package_names.append(req.name.lower())\n                valid_requirements.append(line)\n            except Exception as e:\n                # å¦‚æœæŸè¡Œä¸æ˜¯æœ‰æ•ˆçš„ä¾èµ–æ ¼å¼ï¼Œè¾“å‡ºè­¦å‘Šæˆ–è·³è¿‡\n                print(f\"âš ï¸ æ— æ³•è§£æçš„ä¾èµ–é¡¹: {line}. é”™è¯¯: {e}\")\n\n        return package_names, valid_requirements\n\n    except FileNotFoundError:\n        raise FileNotFoundError(f\"æ‰¾ä¸åˆ°æŒ‡å®šçš„æ–‡ä»¶: {file_path}\")\n    except Exception as e:\n        raise ValueError(f\"åŠ è½½ä¾èµ–é¡¹æ—¶å‡ºé”™: {str(e)}\")\n\ndef check_installed_packages():\n    \"\"\"\n    è·å–å½“å‰ç¯å¢ƒä¸­å·²å®‰è£…çš„ä¾èµ–åŒ…åŠç‰ˆæœ¬ã€‚\n    \"\"\"\n    installed_packages = {\n        dist.key: dist.version for dist in pkg_resources.working_set\n    }\n    return installed_packages\n\ndef parse_requirement(requirement):\n    \"\"\"\n    è§£æä¾èµ–é¡¹ï¼ˆæ”¯æŒå¤æ‚ç‰ˆæœ¬çº¦æŸï¼‰ã€‚\n    \"\"\"\n    try:\n        req = Requirement(requirement)\n        return req.name.lower(), req.specifier\n    except Exception as e:\n        raise ValueError(f\"æ— æ³•è§£æä¾èµ–é¡¹: {requirement}. é”™è¯¯: {str(e)}\")\n\ndef check_dependency_status():\n    \"\"\"æ£€æŸ¥ä¾èµ–åŒ…çŠ¶æ€\"\"\"\n    required_packages, full_requirements = load_requirements()\n    installed_packages = check_installed_packages()\n\n    missing_packages = []\n    mismatched_versions = []\n\n    for req in full_requirements:\n        try:\n            pkg, specifier = parse_requirement(req)\n            if pkg not in installed_packages:\n                missing_packages.append(f\"ğŸš« {req}\")\n            else:\n                installed_version = Version(installed_packages[pkg])\n                if not specifier.contains(installed_version):\n                    mismatched_versions.append(\n                        f\"âš ï¸ {pkg} (expected {specifier}, found {installed_version})\"\n                    )\n        except InvalidVersion as e:\n            mismatched_versions.append(f\"âš ï¸ æ— æ³•è§£æç‰ˆæœ¬: {req}. é”™è¯¯: {str(e)}\")\n\n    if not missing_packages and not mismatched_versions:\n        return \"âœ… æ‰€æœ‰ä¾èµ–åŒ…å·²å®‰è£…\", [], []\n    else:\n        return (\n            \"éƒ¨åˆ†ä¾èµ–åŒ…å­˜åœ¨é—®é¢˜ï¼Œè¯·æŸ¥çœ‹ä¸‹æ–¹åˆ—è¡¨ã€‚\",\n            missing_packages,\n            mismatched_versions,\n        )\n\n\ndef install_missing_packages(missing_packages):\n    \"\"\"å®‰è£…ç¼ºå¤±çš„ä¾èµ–åŒ…\"\"\"\n    try:\n        for package in missing_packages:\n            pkg = package.split(\" \")[1]  # æå–åŒ…åï¼ˆå¿½ç•¥ç¬¦å· ğŸš«ï¼‰\n            subprocess.check_call([\"pip\", \"install\", pkg])\n        return \"âœ… ç¼ºå¤±çš„ä¾èµ–åŒ…å·²æˆåŠŸå®‰è£…\"\n    except subprocess.CalledProcessError as e:\n        return f\"âŒ å®‰è£…å¤±è´¥: {e}\"\n\n\n# å®‰è£…æŒ‰é’®é€»è¾‘\ndef install_and_update(missing_packages):\n            if not missing_packages:\n                return \"æ²¡æœ‰éœ€è¦å®‰è£…çš„ä¾èµ–åŒ…\"\n            install_result = install_missing_packages(missing_packages)\n            status, _, _ = check_dependency_status()  # æ£€æŸ¥å®‰è£…åçš„çŠ¶æ€\n            return status, install_result\n\nasync def check_lightrag_status():\n    \"\"\"æ£€æŸ¥ LightRAG åç«¯çŠ¶æ€\"\"\"\n    retries = 5  # æœ€å¤§é‡è¯•æ¬¡æ•°\n    async with httpx.AsyncClient(timeout=5.0) as client:  # è®¾ç½®è¶…æ—¶æ—¶é—´\n        for attempt in range(1, retries + 1):\n            try:\n                response = await client.post(RAG_API_URL + \"/connect\")\n                if response.status_code == 200:\n                    data = response.json()\n                    if isinstance(data, dict) and data.get(\"connective\") is True:\n                        return \"âœ…LightRAG åç«¯è¿è¡Œæ­£å¸¸\"\n            except (httpx.ConnectError, httpx.TimeoutException):\n                # æ•è·è¿æ¥é”™è¯¯æˆ–è¶…æ—¶\n                if attempt == retries:\n                    return \"âŒLightRAG åç«¯æœªæ­£å¸¸è¿è¡Œ\"\n                continue  # ç»§ç»­é‡è¯•\n    return \"âŒLightRAG åç«¯æœªæ­£å¸¸è¿è¡Œï¼Œå¯ç‚¹å‡»ğŸ’»ä»¥å°è¯•å¯åŠ¨\"\n\ndef check_model_connection_status():\n    \"\"\"æ£€æŸ¥å¤§æ¨¡å‹è¿æ¥çŠ¶æ€\"\"\"\n    # ç¤ºä¾‹å®ç°ï¼Œå¯ä»¥æ‰©å±•ä¸ºå®é™…æ¨¡å‹è¿æ¥çš„æ£€æŸ¥é€»è¾‘\n    return \"âœ…å¤§æ¨¡å‹è¿æ¥æˆåŠŸ\"\n\ndef check_port():\n    port = os.getenv(\"API_port\",\"\")\n    web = f\"http://localhost:{port}/v1\"\n    return web\n\n# åˆ·æ–°æŒ‰é’®é€»è¾‘\ndef refresh_status():\n    status, missing, mismatched = check_dependency_status()\n    return (\n        status,\n        missing + mismatched,  # å±•ç¤ºæ‰€æœ‰ç¼ºå¤±å’Œç‰ˆæœ¬é—®é¢˜\n        bool(missing or mismatched),\n    )\n\n# æ¬¢è¿ç•Œé¢<ç»“æŸ>\n\n\n# HTML to Graph<å¼€å§‹>\n\n# å…¨å±€çŠ¶æ€\nSTATE = {\n    \"notification_hidden_until\": \"2024-12-5 00:00:00\",  # é€šçŸ¥æ éšè—æˆªæ­¢æ—¶é—´\n    \"dependencies_installed\": False    # æ˜¯å¦å·²å®‰è£…ä¾èµ–\n}\n\n# æ£€æŸ¥ä¾èµ–æ˜¯å¦å·²å®‰è£…\ndef check_dependencies():\n    # å‡è®¾ä¾èµ–ä¸ºæŸä¸ª pip åŒ…ï¼Œä¾‹å¦‚ 'some_package'\n    try:\n        import some_package\n        return True\n    except ImportError:\n        return False\n\n# å®‰è£…ä¾èµ–é€»è¾‘\ndef install_dependencies():\n    try:\n        os.system(\"pip install some_package\")  # æ›¿æ¢ä¸ºå®é™…ä¾èµ–\n        return True, \"ä¾èµ–å®‰è£…æˆåŠŸï¼\"\n    except Exception as e:\n        return False, f\"å®‰è£…ä¾èµ–æ—¶å‡ºé”™: {str(e)}\"\n\ndef handle_notification_action(action, remember=False):\n    today = datetime.today()  # å½“å‰æ—¥æœŸæ—¶é—´\n    if action == \"install\":\n        success, message = install_dependencies()\n        STATE[\"dependencies_installed\"] = success\n        return message, success\n    elif action == \"dismiss\":\n        if remember:\n            STATE[\"notification_hidden_until\"] = str(datetime.strptime(str(today + timedelta(days=7)),\"%Y-%m-%d\"))\n        else:\n            STATE[\"notification_hidden_until\"] = \"2024-12-1 00:00:00\"\n        return True, True\n\n# æ„å»ºé€šçŸ¥æ é€»è¾‘\ndef should_show_notification():\n        \"\"\"åˆ¤æ–­æ˜¯å¦åº”è¯¥æ˜¾ç¤ºé€šçŸ¥æ \"\"\"\n        today = datetime.today()\n        hidden_until = datetime.strptime(STATE.get(\"notification_hidden_until\"), \"%Y-%m-%d %H:%M:%S\")\n        diff = today - hidden_until\n        #print(diff.days >= 7)\n        return diff.days >= 7 # å¦‚æœè¶…è¿‡7å¤©ï¼Œåˆ™æ˜¾ç¤ºé€šçŸ¥æ \n\ndef handle_install_dependencies():\n        \"\"\"å¤„ç†å®‰è£…ä¾èµ–çš„é€»è¾‘\"\"\"\n        STATE[\"dependencies_installed\"], message = install_dependencies()\n        return message, should_show_notification()\n\ndef close_notification(remember):\n        \"\"\"å…³é—­é€šçŸ¥æ é€»è¾‘\"\"\"\n        if remember:\n            STATE[\"notification_hidden_until\"] = datetime.today().strftime(\"%Y-%m-%d %H:%M:%S\")\n        return False, False,True\n\n# æ„å»ºé€šçŸ¥æ \ndef notification_ui():\n    with gr.Blocks() as notification_ui:\n        notification_bar = gr.Group()  # é€šçŸ¥æ \n\n        with notification_bar:\n            gr.Markdown(\"### é€šçŸ¥ï¼šæ­¤é¡µé¢ä¸ºå¯é€‰åŠŸèƒ½ï¼Œä¾èµ–å°šæœªå®‰è£…ã€‚\")\n            gr.Markdown(\"è¯·æ ¹æ®éœ€æ±‚å®‰è£…ä¾èµ–ï¼Œæˆ–ç›´æ¥å¼€å§‹ä½¿ç”¨ã€‚\")\n            install_btn = gr.Button(\"æˆ‘å·²çŸ¥æ™“å¹¶å¼€å§‹å®‰è£…ç›¸å…³ä¾èµ–\")\n            close_btn = gr.Button(\"å¼€å§‹ä½¿ç”¨\")\n            remember_checkbox = gr.Checkbox(label=\"ä¸ƒå¤©å†…ä¸å†æ˜¾ç¤º\")\n\n            # æŒ‰é’®äº¤äº’\n            install_btn.click(\n                fn=handle_install_dependencies,\n                inputs=[],\n                outputs=[notification_bar, notification_ui]\n            )\n            close_btn.click(\n                fn=close_notification,\n                inputs=[remember_checkbox],\n                outputs=[notification_bar, notification_ui]\n            )\n\n    return notification_ui,\"è°ƒè¯•ï¼šé€šçŸ¥æ \"\n\n# è½¬æ¢HTMLåˆ°PDFå‡½æ•°\ndef html_to_pdf(urls, output_dir=\"./PDF_generate\"):\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    generated_files = []\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    with sync_playwright() as p:\n        browser = p.chromium.launch()\n        for url in urls:\n            page = browser.new_page()\n            page.goto(url)\n            domain = url.split(\"//\")[-1].split(\"/\")[0]  # æå–åŸŸå\n            file_name = f\"{domain}-{timestamp}.pdf\"\n            file_dir = os.path.join(output_dir, domain)\n            if not os.path.exists(file_dir):\n                os.makedirs(file_dir)\n            output_path = os.path.join(file_dir, file_name)\n            page.pdf(path=output_path)\n            generated_files.append(output_path)\n        browser.close()\n    return generated_files\n\n# æ‰“å¼€ PDF åŠŸèƒ½\ndef open_pdf(filepath):\n    if os.path.exists(filepath):\n        os.system(f\"start {filepath}\")  # Windows ä¸Šæ‰“å¼€æ–‡ä»¶\n        return f\"æ‰“å¼€ PDF æ–‡ä»¶: {filepath}\"\n    else:\n        return \"æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¯·æ£€æŸ¥è·¯å¾„ï¼\"\n\n# åˆ é™¤PDFå¹¶å¤‡ä»½\ndef delete_pdf_with_backup(pdf_paths, backup_dir=\"./backup/PDF_generate\"):\n    if not os.path.exists(backup_dir):\n        os.makedirs(backup_dir)\n    deleted_files = []\n    for pdf in pdf_paths:\n        if os.path.exists(pdf):\n            backup_path = os.path.join(backup_dir, os.path.basename(pdf))\n            shutil.move(pdf, backup_path)\n            deleted_files.append((pdf, backup_path))\n    return deleted_files\n\n# HTML to Graph<ç»“æŸ>\n\n# ä¾§è¾¹æ <å¼€å§‹>\n\n# é€šè¿‡åç«¯æœåŠ¡è·å–æ¨¡å‹ä¿¡æ¯\nasync def fetch_model_info(base_url, api_key):\n    headers = {\n        \"Authorization\": f\"Bearer {api_key}\"\n    }\n    async with httpx.AsyncClient() as client:\n        try:\n            response = await client.get(RAG_API_URL+ f\"/models\")\n            response.raise_for_status()\n            response_data = response.json()\n            all_models = [model[\"id\"] for model in response_data.get(\"data\", [])]\n            # ç­›é€‰é€»è¾‘\n            large_models = [\n                m for m in all_models\n                if isinstance(m, str) and \"embedding\" not in m.lower() and \"embed\" not in m.lower()\n            ]\n            embed_models = [\n                m for m in all_models\n                if isinstance(m, str) and (\"embedding\" in m.lower() or \"embed\" in m.lower())\n            ]\n            return large_models, embed_models\n        except Exception as e:\n            return str(e), []\n\n# è·å–ç¯å¢ƒå˜é‡çš„åˆå§‹å€¼\ndef get_initial_values():\n    base_url = os.getenv(\"OPENAI_BASE_URL\", \"\")\n    api_key = os.getenv(\"OPENAI_API_KEY\", \"\")\n    API_port = os.getenv(\"API_port\", \"\")\n    LLM = os.getenv(\"EMBEDDING_MODEL\",\"\")\n    EMBED = os.getenv(\"EMBEDDING_MODEL\",\"\")\n    api_key_display = \"API_KEYå·²ä¿å­˜\" if api_key else \"\"\n    return base_url, api_key_display,API_port\n\n# æ£€æŸ¥å¹¶è¡¥å…¨ BASE_URL\ndef normalize_base_url(base_url):\n    base_url = base_url.strip()  # å»é™¤é¦–å°¾ç©ºæ ¼\n    if not base_url.endswith(\"/v1\"):  # æ£€æŸ¥æ˜¯å¦ä»¥ /v1 ç»“å°¾\n        if not base_url.endswith(\"/\"):  # å¦‚æœæ²¡æœ‰æœ«å°¾çš„æ–œæ ï¼Œå…ˆæ·»åŠ \n            base_url += \"/\"\n        base_url += \"v1\"\n    return base_url\n\ndef load_model_configs(json_file):\n    \"\"\"\n    ä» JSON æ–‡ä»¶ä¸­åŠ¨æ€åŠ è½½æ¨¡å‹é…ç½®ï¼Œå¹¶æŒ‰ç…§æ¨¡å‹ç±»åˆ«å­˜å…¥å­—å…¸ã€‚\n    :param json_file: JSON æ–‡ä»¶è·¯å¾„\n    :return: (LLM æ¨¡å‹å­—å…¸, Embedding æ¨¡å‹å­—å…¸)\n    \"\"\"\n    try:\n        with open(json_file, 'r', encoding='utf-8') as file:\n            data = json.load(file)\n\n        llm_models = {}\n        embedding_models = {}\n\n        # éå† JSON æ•°æ®å¹¶åˆ†ç±»\n        for model_category, models in data.items():\n            if model_category == \"LLM\":\n                llm_models.update(models)\n            elif model_category == \"Embedding\":\n                embedding_models.update(models)\n\n        return llm_models, embedding_models\n    except Exception as e:\n        raise ValueError(f\"åŠ è½½ JSON æ–‡ä»¶å¤±è´¥: {str(e)}\")\n\ndef get_max_tokens(llm_name, embedding_name):\n    \"\"\"\n    æ ¹æ®æ¨¡å‹åç§°è·å–å…¶å¯¹åº”çš„ Max_tokens\n    :param llm_name: å¤§æ¨¡å‹åç§°\n    :param embedding_name: åµŒå…¥æ¨¡å‹åç§°\n    :return: (LLM æ¨¡å‹ Max_tokens, Embedding æ¨¡å‹ Max_tokens)\n    \"\"\"\n    llm_dict,embedding_dict = load_model_configs(\"./models.json\")\n    llm_tokens = llm_dict.get(llm_name, None)\n    embedding_tokens = embedding_dict.get(embedding_name, None)\n\n    if llm_tokens is None:\n        raise ValueError(f\"å¤§æ¨¡å‹ '{llm_name}' çš„ Max_tokens æœªæ‰¾åˆ°ã€‚\")\n    if embedding_tokens is None:\n        raise ValueError(f\"åµŒå…¥æ¨¡å‹ '{embedding_name}' çš„ Max_tokens æœªæ‰¾åˆ°ã€‚\")\n\n    return llm_tokens, embedding_tokens\n\n# ä¿å­˜è®¾ç½®çš„é€»è¾‘\ndef save_settings(base_url, api_key,port,llm_max_tokens,embed_max_tokens):\n    base_url = normalize_base_url(base_url)  # æ£€æŸ¥å¹¶è¡¥å…¨ BASE_URL\n    api_key = api_key.strip()\n    port = str(port).strip()\n    llm_max_token = str(llm_max_tokens).strip()\n    embed_max_token = str(embed_max_tokens).strip()\n    Port = os.getenv(\"API_port\",\"\") #å…¨å±€å˜é‡æ›´æ–°\n    update_env_variable(\"OPENAI_BASE_URL\", base_url)\n    update_env_variable(\"API_port\",port)\n    update_env_variable(\"LLM_MODEL_TOKEN_SIZE\",llm_max_token)\n    update_env_variable(\"EMBEDDING_MAX_TOKEN_SIZE\",embed_max_token)\n    if api_key and api_key != \"API_KEYå·²ä¿å­˜\":\n        os.environ[\"OPENAI_API_KEY\"] = api_key\n\n# ä¾§è¾¹æ <ç»“æŸ>\n\n# UI æ„å»ºæ¨¡å—åŒ–å‡½æ•°\n\ndef notification_bar():\n    \"\"\"\n    çº¯ CSS å®ç°å³ä¸Šè§’é€šçŸ¥æ ï¼Œæ”¯æŒå¤šæ¡æ¶ˆæ¯å †å å’Œè‡ªåŠ¨æ¶ˆå¤±ã€‚è¿˜æš‚æ—¶ä¸å¯ç”¨ã€‚\n    \"\"\"\n    html_content = \"\"\"\n    <style>\n    #notifications-container {\n        position: fixed;\n        top: 10px;\n        right: 10px;\n        z-index: 1000; /* ç¡®ä¿é€šçŸ¥æ æ˜¾ç¤ºåœ¨æœ€ä¸Šå±‚ */\n        display: flex;\n        flex-direction: column;\n        gap: 10px; /* é€šçŸ¥æ ä¹‹é—´çš„é—´è· */\n        pointer-events: none; /* ç¡®ä¿é¼ æ ‡ç‚¹å‡»ç©¿é€åˆ°ä¸»é¡µé¢ */\n    }\n\n    .notification {\n        background-color: #4caf50; /* é»˜è®¤ç»¿è‰²é€šçŸ¥ */\n        color: white;\n        padding: 10px 20px;\n        border-radius: 5px;\n        font-size: 14px;\n        box-shadow: 0 2px 5px rgba(0,0,0,0.2);\n        opacity: 0; /* åˆå§‹é€æ˜ */\n        transform: translateY(-20px); /* åˆå§‹ä¸Šç§» */\n        animation: slideInOut 5s ease-in-out forwards; /* åŠ¨ç”»æ§åˆ¶æ˜¾ç¤ºä¸éšè— */\n    }\n\n    .notification.error {\n        background-color: #f44336; /* çº¢è‰²é€šçŸ¥ */\n    }\n\n    .notification.warning {\n        background-color: #ff9800; /* æ©™è‰²é€šçŸ¥ */\n    }\n\n    .notification.success {\n        background-color: #4caf50; /* ç»¿è‰²é€šçŸ¥ */\n    }\n\n    @keyframes slideInOut {\n        0% { opacity: 0; transform: translateY(-20px); }\n        10% { opacity: 1; transform: translateY(0); }\n        90% { opacity: 1; transform: translateY(0); }\n        100% { opacity: 0; transform: translateY(-20px); }\n    }\n    </style>\n    <div id=\"notifications-container\"></div>\n    <script>\n    function addNotification(message, type = 'success') {\n        const container = document.getElementById('notifications-container');\n        if (!container) return;\n\n        // åˆ›å»ºé€šçŸ¥å…ƒç´ \n        const notification = document.createElement('div');\n        notification.className = `notification ${type}`;\n        notification.textContent = message;\n\n        // æ·»åŠ åˆ°å®¹å™¨ä¸­\n        container.appendChild(notification);\n\n        // è‡ªåŠ¨ç§»é™¤é€šçŸ¥\n        setTimeout(() => {\n            notification.style.opacity = '0';\n            notification.addEventListener('transitionend', () => notification.remove());\n        }, 5000); // 5 ç§’åè‡ªåŠ¨åˆ é™¤\n    }\n\n    // æµ‹è¯•ç”¨é€šçŸ¥ï¼ˆå¯ç§»é™¤ï¼‰\n    setTimeout(() => addNotification('ä¿å­˜æˆåŠŸï¼', 'success'), 1000);\n    setTimeout(() => addNotification('ä¿å­˜å¤±è´¥ï¼šè¯·æ£€æŸ¥è¾“å…¥ï¼', 'error'), 2000);\n    setTimeout(() => addNotification('è­¦å‘Šï¼šAPI Key å°†è¿‡æœŸï¼', 'warning'), 3000);\n    </script>\n    \"\"\"\n    return html_content\n\ndef sidebar_ui():\n    custom_css = \"\"\"\n        .SideBar {\n            width: auto !important;\n            height: 100% !important;\n            max-width: 25% !important;\n            background-color: #f5f5f5;\n            padding: 10px;\n            box-shadow: 2px 0 5px rgba(0, 0, 0, 0.1);\n        }\n\n        .Closed-SideBar {\n            width: auto !important;\n            height: 100% !important;\n            max-width: 5% !important;\n            background-color: #f5f5f5;\n            box-shadow: 2px 0 5px rgba(0, 0, 0, 0.1);\n            display: flex;\n            justify-content: flex-end; /* å°†å†…å®¹é å³å¯¹é½ */\n        }\n        \n        #Closed-SideBar-button {\n            width: auto !important;\n            height: 100% !important;\n            max-width: 5% !important;\n            background: linear-gradient(90deg, #4caf50, #8bc34a);\n            color: white;\n            border: none;\n            border-radius: 5px;\n            padding: 10px;\n            font-size: 1rem;\n            cursor: pointer;\n            transition: background 0.3s ease-in-out;\n            display: flex;\n            justify-content: flex-end; /* å°†å†…å®¹é å³å¯¹é½ */\n        }\n\n        .gradient-button {\n            background: linear-gradient(90deg, #4caf50, #8bc34a);\n            color: white;\n            border: none;\n            border-radius: 5px;\n            padding: 10px;\n            font-size: 1rem;\n            cursor: pointer;\n            transition: background 0.3s ease-in-out;\n        }\n\n        .gradient-button:hover {\n            background: linear-gradient(90deg, #8bc34a, #4caf50);\n        }\n        \n        #ASideBar {\n            text-align: center; /* å±…ä¸­å¯¹é½ */\n            font-size: 28px; /* å­—ä½“å¤§å° */\n            font-weight: bold; /* åŠ ç²— */\n            background-color: #f5f5f5; /* èƒŒæ™¯è‰²ä¸ä¾§è¾¹æ ä¸€è‡´ */\n            padding: 15px; /* å†…è¾¹è· */\n            border-radius: 5px; /* åœ†è§’ */\n            box-shadow: 2px 2px 10px rgba(0, 0, 0, 0.1); /* é˜´å½±æ•ˆæœ */\n            margin-bottom: 20px; /* ä¸‹è¾¹è· */\n            color: #333; /* å­—ä½“é¢œè‰² */\n        }\n    \"\"\"\n    #gr.HTML(notification_bar())\n    with gr.Blocks() as Thesidebar:\n        with gr.Column(elem_classes=\"SideBar\") as SideBar:\n            with gr.Row():\n                #gr.Markdown(\"ä¾§è¾¹æ \",elem_id=\"ASideBar\")\n                close_button = gr.Button(\"âŒ å…³é—­ä¾§è¾¹æ \", elem_id=\"close-sidebar\", elem_classes=\"gradient-button\")\n\n            # è¾“å…¥ BASE_URL å’Œ API_KEY\n            base_url_input = gr.Textbox(label=\"BASE_URL\", placeholder=\"è¯·è¾“å…¥ BASE_URL(æ³¨æ„è¦æ·»åŠ /v1)\")\n            api_key_input = gr.Textbox(label=\"API_KEY\", placeholder=\"è¯·è¾“å…¥ API_KEY\")\n            API_port_input = gr.Textbox(label=\"API_PORT\",placeholder=\"è¯·å¡«å…¥ä½ æƒ³è®¾ç½®çš„RAGç³»ç»Ÿç«¯å£\")\n\n\n            # å¤§æ¨¡å‹å’ŒåµŒå…¥æ¨¡å‹ä¸‹æ‹‰æ¡†\n            large_model_dropdown = gr.Dropdown(label=\"é€‰æ‹©å¤§æ¨¡å‹\", elem_id=\"llms\",choices=[],interactive=True)\n            llm_MAX_tokens = gr.Textbox(label=\"å¤§æ¨¡å‹çš„Max_tokens\",elem_id=\"llm_max_tokens\",placeholder=\"è¯·æŸ¥è¯¢æ‰€ä½¿ç”¨çš„å¤§æ¨¡å‹çš„Max_tokenså¹¶å¡«å…¥\")\n            embed_model_dropdown = gr.Dropdown(label=\"é€‰æ‹©åµŒå…¥æ¨¡å‹\", elem_id=\"embedding\",choices=[],interactive=True)\n            embed_MAX_tokens = gr.Textbox(label=\"åµŒå…¥çš„Max_tokens\",elem_id=\"embed_max_tokens\",placeholder=\"è¯·æŸ¥è¯¢æ‰€ä½¿ç”¨çš„åµŒå…¥æ¨¡å‹çš„Max_tokenså¹¶å¡«å…¥\")\n\n            gr.Markdown(\"\"\"\n                ### â„¹ï¸ Tipsï¼š\n                    - ä»¥ä¸Šä¸ºä¸»è¦é€‰é¡¹ã€‚\n                    - è¯·è®°å¾—å¡«å…¥åŸºæœ¬ä¿¡æ¯ç„¶åä¿å­˜ã€‚\n                    - è¯·åˆ·æ–°æ¨¡å‹ä¿¡æ¯ï¼Œé€‰æ‹©æ¨¡å‹æˆ–è€…å¡«å…¥Tokensåä¼šè‡ªåŠ¨ä¿å­˜å¯¹åº”ä¿¡æ¯ã€‚\n                    - å¦‚æœMax_tokensé‡åˆ°äº†é”™è¯¯ï¼Œè¯·åœ¨æ ¹ç›®å½•çš„models.jsonä¸­æ·»åŠ ä½ ä½¿ç”¨çš„æ¨¡å‹ä»¥åŠå¯¹åº”çš„tokensã€‚\n                    \"\"\",\n                    show_copy_button=False,\n                    container=True\n            )\n            # ä¿å­˜è®¾ç½®æŒ‰é’®\n            save_button = gr.Button(\"ä¿å­˜\", elem_id=\"save-settings\", elem_classes=\"gradient-button\")\n\n            # è·å–æ¨¡å‹ä¿¡æ¯æŒ‰é’®\n            fetch_models_button = gr.Button(\"åˆ·æ–°æ¨¡å‹ä¿¡æ¯\", elem_id=\"fetch-models\", elem_classes=\"gradient-button\")\n\n            # é€‰æ‹©ä¸Šä¸‹æ–‡ç­–ç•¥\n            with gr.Row():\n                gr.Dropdown(\n                    label=\"é€‰æ‹©ä¸Šä¸‹æ–‡ç­–ç•¥ï¼ˆæš‚æ—¶ä¸å¯ç”¨ï¼‰\",\n                    choices=[\"ç­–ç•¥1\", \"ç­–ç•¥2\", \"ç­–ç•¥3\"],\n                    value=\"ç­–ç•¥1\",\n                    interactive=True\n                )\n                gr.Dropdown(\n                    label=\"é€‰æ‹© Promptï¼ˆæš‚æ—¶ä¸å¯ç”¨ï¼‰\",\n                    choices=[\"Prompt1\", \"Prompt2\", \"Prompt3\"],\n                    value=\"Prompt1\",\n                    interactive=True\n                )\n\n\n            with gr.Accordion(label=\"æ¬¡è¦è®¾ç½®ï¼ˆæš‚æ—¶ä¸å¯ç”¨ï¼‰\",elem_id=\"Addition\") as addition:\n                with gr.Column():\n                    gr.Textbox(label=\"æ¬¡è¦BASE_URL\",elem_id=\"sub_BASE_URL\",placeholder=\"è¯·è¾“å…¥æ¬¡è¦BASE_URL(æ³¨æ„è¦æ·»åŠ /v1)\")\n                    gr.Textbox(label=\"æ¬¡è¦API_KEY\", elem_id=\"sub_API_KEY\",placeholder=\"è¯·è¾“å…¥æ¬¡è¦API_KEY\")\n\n                    # å¤§æ¨¡å‹å’ŒåµŒå…¥æ¨¡å‹ä¸‹æ‹‰æ¡†\n                    sub_large_model_dropdown = gr.Dropdown(label=\"é€‰æ‹©æ¬¡è¦å¤§æ¨¡å‹\", elem_id=\"sub_llms\", choices=[], interactive=True)\n                    gr.Markdown(\"\"\"\n                        ### â„¹ï¸ Tipsï¼š\n                        - ä»¥ä¸Šä¸ºæ¬¡è¦é€‰é¡¹ã€‚\n                        - åœ¨æœ‰Rate Limitçš„æƒ…å†µä¸‹å¯ä»¥é€‰æ‹©ä½¿ç”¨ï¼ŒRAGç³»ç»Ÿä¼šä½¿ç”¨ä»¥ä¸Šè®¾å®šä»¥æ‰§è¡Œä¸é‚£ä¹ˆé‡è¦çš„ä»»åŠ¡ã€‚\n                        - ä¾‹å¦‚é—®ç­”æˆ–è€…èŠå¤©æ—¶æå–å…³é”®è¯ä¼šä½¿ç”¨æ¬¡è¦å¤§æ¨¡å‹ï¼Œè€Œä¸æ˜¯ä¸»è¦çš„å¤§æ¨¡å‹ã€‚\n                        \"\"\")\n\n            # äº¤äº’é€»è¾‘\n            save_button.click(\n                fn=save_settings,\n                inputs=[base_url_input, api_key_input,API_port_input],\n                outputs=None\n            )\n            save_button.click(\n                lambda url, key,port,llm_max_tokens,embed_max_tokens: f\"<script>addNotification('ä¿å­˜æˆåŠŸï¼', 'success');</script>\"\n                if url and key and port and llm_max_tokens and embed_max_tokens else\n                f\"<script>addNotification('ä¿å­˜å¤±è´¥ï¼šè¯·è¾“å…¥å®Œæ•´ä¿¡æ¯ï¼', 'error');</script>\",\n                inputs=[base_url_input, api_key_input,API_port_input,llm_MAX_tokens,embed_MAX_tokens],\n                outputs=None\n            )\n\n            def return_model_info(base_url_input, api_key_input):\n                large_models, embed_models = asyncio.run(fetch_model_info(base_url_input, api_key_input))\n                update_env_variable(\"LLM_MODEL\", large_models[0])\n                update_env_variable(\"EMBEDDING_MODEL\", embed_models[0])\n                return gr.update(elem_id=\"llms\",choices=large_models,value=large_models[0]),gr.update(elem_id=\"embedding\",choices=embed_models,value=embed_models[0])\n\n            fetch_models_button.click(\n                fn=return_model_info,\n                inputs=[base_url_input, api_key_input],\n                outputs=[large_model_dropdown, embed_model_dropdown],\n            )\n            def large_model_dropdown_input(large_model_dropdown,embed_model_dropdown):\n                update_env_variable(\"LLM_MODEL\",large_model_dropdown)\n                large_model_max_tokens, embed_model_max_tokens = get_max_tokens(large_model_dropdown,embed_model_dropdown)\n                return gr.update(elem_id=\"llm_max_tokens\",value=large_model_max_tokens)\n            large_model_dropdown.change(\n                fn=large_model_dropdown_input,\n                inputs=[large_model_dropdown,embed_model_dropdown],\n                outputs=[llm_MAX_tokens],\n            )\n            def embed_model_dropdown_input(large_model_dropdown,embed_model_dropdown):\n                update_env_variable(\"EMBEDDING_MODEL\", embed_model_dropdown)\n                large_model_max_tokens,embed_model_max_tokens = get_max_tokens(large_model_dropdown,embed_model_dropdown)\n                return gr.update(elem_id=\"embed_max_tokens\",value=embed_model_max_tokens)\n            embed_model_dropdown.change(\n                fn=embed_model_dropdown_input,\n                inputs=[large_model_dropdown,embed_model_dropdown],\n                outputs=[embed_MAX_tokens],\n            )\n            Thesidebar.load(\n                fn=get_initial_values,\n                inputs=[],\n                outputs=[base_url_input, api_key_input,API_port_input]\n            )\n\n            # `closed_sidebar` å®šä¹‰\n        with gr.Row(elem_classes=\"Closed-SideBar\", visible=False) as closed_sidebar:\n            #gr.Markdown(\"ä¾§è¾¹æ \",elem_id=\"ASideBar\")\n            open_button = gr.Button(\"ğŸ”“ æ‰“å¼€ä¾§è¾¹æ \", elem_id=\"Closed-SideBar-button\")\n\n            # çŠ¶æ€æ›´æ–°å‡½æ•°\n\n            def toggle_sidebar():\n                # JS è„šæœ¬ï¼šåˆ‡æ¢ sidebar å’Œ closed_sidebar çš„æ˜¾ç¤ºçŠ¶æ€\n                return gr.update(elem_classes=\"Closed-SideBar\",visible=True), gr.update(elem_classes=\"SideBar\",visible=False)\n\n            def toggle_back_sidebar():\n                # JS è„šæœ¬ï¼šåˆ‡æ¢ sidebar å’Œ closed_sidebar çš„æ˜¾ç¤ºçŠ¶æ€\n                return gr.update(elem_classes=\"SideBar\",visible=True), gr.update(elem_classes=\"Closed-SideBar\",visible=False)\n\n            # æŒ‰é’®ç‚¹å‡»äº‹ä»¶\n\n        close_button.click(fn=toggle_sidebar, outputs=[closed_sidebar, SideBar])\n        open_button.click(fn=toggle_back_sidebar, outputs=[SideBar, closed_sidebar])\n    return Thesidebar\n\ndef welcome_page():\n    \"\"\"åˆ›å»ºæ¬¢è¿ä½¿ç”¨é¡µé¢\"\"\"\n    with gr.Blocks(visible=False, elem_id=\"welcome-page\") as welcome_page:\n        # æ ‡é¢˜\n        gr.Markdown(\"# æ¬¢è¿ä½¿ç”¨\", elem_id=\"welcome-title\", elem_classes=\"center-text\")\n\n        # ä¸»ä½“å†…å®¹\n        with gr.Row():\n            # å·¦ä¾§ README å†…å®¹å—\n            with gr.Column(scale=3):\n                gr.Markdown(load_readme(), label=\"é¡¹ç›®ç®€ä»‹\")\n\n            # å³ä¾§çŠ¶æ€æ \n            with gr.Column(scale=1):\n                gr.Markdown(\"## ç³»ç»ŸçŠ¶æ€\")\n                dependency_status = gr.Textbox(\n                    label=\"ä¾èµ–åŒ…çŠ¶æ€\",\n                    value=check_dependency_status()[0],\n                    interactive=False,\n                    placeholder=\"ä¾èµ–åŒ…å®‰è£…çŠ¶æ€æ˜¾ç¤ºåœ¨æ­¤å¤„\"\n                )\n                missing_packages_dropdown = gr.Dropdown(\n                    label=\"ç¼ºå¤±ä¾èµ–åŒ…åˆ—è¡¨\",\n                    choices=[],\n                    visible=True,\n                    interactive=False,\n                    multiselect=True,\n                    allow_custom_value=True,\n                    elem_id=\"Missing_packages_dropdown\"\n                )\n                install_button = gr.Button(\n                    \"å®‰è£…ç¼ºå¤±çš„ä¾èµ–åŒ…\",\n                    visible=False,\n                    variant=\"primary\",\n                    elem_id=\"Install_button\"\n                )\n                with gr.Column():  # åˆ›å»ºæ°´å¹³å¸ƒå±€\n                    lightrag_status = gr.Textbox(\n                        label=\"LightRAG åç«¯çŠ¶æ€\",\n                        value=\"æŒ‰ä¸‹æ–¹çš„ğŸ”„æŒ‰é’®ä»¥è¿›è¡Œæµ‹è¯•\",\n                        interactive=False,\n                        placeholder=\"åç«¯çŠ¶æ€æ˜¾ç¤ºåœ¨æ­¤å¤„\"\n                    )\n                    with gr.Row():\n                        '''\n                        lightrag_fireup_button = gr.Button(\n                            \"ğŸ’»\",\n                            size=\"sm\",  # å°æŒ‰é’®\n                            elem_id=\"fireup-btn\",  # ä¸ºæŒ‰é’®è®¾ç½® IDï¼Œæ–¹ä¾¿æ ·å¼å®šåˆ¶\n                            min_width = 100,\n                        )'''\n                        lightrag_status_refresh_button = gr.Button(\n                            \"ğŸ”„\",\n                            size=\"sm\",  # å°æŒ‰é’®\n                            elem_id=\"status-refresh-btn\",  # ä¸ºæŒ‰é’®è®¾ç½® IDï¼Œæ–¹ä¾¿æ ·å¼å®šåˆ¶\n                            min_width = 100,\n                        )\n                \"\"\"\n                ####ä¸å†ä½¿ç”¨\n                model_connection_status = gr.Textbox(\n                    label=\"å¤§æ¨¡å‹è¿æ¥çŠ¶æ€\",\n                    value=check_model_connection_status(),\n                    interactive=False,\n                    placeholder=\"æ¨¡å‹è¿æ¥çŠ¶æ€æ˜¾ç¤ºåœ¨æ­¤å¤„\"\n                )\n                \"\"\"\n                api_port = gr.Textbox(\n                    label=\"LightRAGåç«¯åœ°å€\",\n                    value=check_port(),\n                    interactive=False,\n                    elem_id=\"API_port\",\n                    placeholder=\"å½“å‰çš„åç«¯åœ°å€ä¸º\",\n                    show_copy_button=True\n                )\n\n                refresh_button = gr.Button(\"ğŸ”„åˆ·æ–°çŠ¶æ€\", variant=\"primary\")\n\n        # åº•éƒ¨é“¾æ¥ä¸å¼€æºåè®®\n        with gr.Row():\n            with gr.Column(scale=3):\n                gr.Markdown(\"### ğŸ“‚ é¡¹ç›®é“¾æ¥\")\n                gr.Markdown(\"\"\"\n                - [GitHub ä»“åº“](https://github.com/HerSophia/LightRAGforSillyTavern)\n                - [é¡¹ç›®ä½¿ç”¨è¯´æ˜ä¹¦](https://your_docs_link)\n                - [è§†é¢‘æ•™ç¨‹](https://your_video_link)\n                \"\"\")\n\n            with gr.Column(scale=1):\n                license_textbox = gr.Textbox(\n                    label=\"å¼€æºåè®®\",\n                    value=load_license(),\n                    lines=10,\n                    interactive=False\n                )\n        # é¡µé¢åˆå§‹åŒ–æ—¶çš„æ£€æŸ¥é€»è¾‘\n        def initialize_status():\n            status, missing, mismatched = check_dependency_status()\n            if (len(mismatched)):\n                show_missing_packages_dropdown = False\n            else:\n                show_missing_packages_dropdown = True\n            all_issues = missing + mismatched\n            show_install_button = bool(missing)  # ä»…ç¼ºå¤±åŒ…æ—¶æ˜¾ç¤ºå®‰è£…æŒ‰é’®\n            return (\n                status,\n                gr.update(elem_id=\"Missing_packages_dropdown\",visible=show_missing_packages_dropdown),\n                all_issues,\n                missing,  # æ§åˆ¶å®‰è£…æŒ‰é’®æ˜¯å¦æ˜¾ç¤º\n                gr.update(elem_id=\"Install_button\",visible=show_install_button)\n            )\n\n        welcome_page.load(\n            fn=initialize_status,\n            inputs=[],\n            outputs=[\n                dependency_status,\n                missing_packages_dropdown,\n                missing_packages_dropdown,\n                missing_packages_dropdown,\n                install_button,\n            ],\n        )\n\n        # åˆ·æ–°æŒ‰é’®é€»è¾‘\n        def refresh_status():\n            status, missing, mismatched = check_dependency_status()\n            all_issues = missing + mismatched\n            show_install_button = bool(missing)\n            new_web = check_port()\n            return (\n                status,\n                all_issues,\n                missing,  # æ§åˆ¶å®‰è£…æŒ‰é’®æ˜¯å¦æ˜¾ç¤º\n                gr.update(elem_id=\"Install_button\",visible=show_install_button),\n                gr.update(elem_id=\"API_port\"),\n                new_web\n            )\n\n        def lightrag_status_refresh():\n            lightrag_status = asyncio.run(check_lightrag_status())\n            return lightrag_status\n\n        lightrag_status_refresh_button.click(\n            fn=lightrag_status_refresh,\n            inputs=[],\n            outputs=[lightrag_status]\n        )\n        refresh_button.click(\n            fn=refresh_status,\n            inputs=[],\n            outputs=[\n                dependency_status,\n                missing_packages_dropdown,\n                missing_packages_dropdown,\n                install_button,\n                api_port,\n                api_port\n            ],\n        )\n\n        # å®‰è£…æŒ‰é’®é€»è¾‘\n        def install_and_update(missing_packages):\n            if not missing_packages:\n                return \"æ²¡æœ‰éœ€è¦å®‰è£…çš„ä¾èµ–åŒ…\"\n            install_result = install_missing_packages(missing_packages)\n            status, _, _ = check_dependency_status()  # æ£€æŸ¥å®‰è£…åçš„çŠ¶æ€\n            return status, install_result\n\n        install_button.click(\n            fn=install_and_update,\n            inputs=[missing_packages_dropdown],\n            outputs=[\n                dependency_status\n            ],\n        )\n    return welcome_page\n\ndef file_management_ui():\n    \"\"\"\n    åˆ›å»ºæ–‡æœ¬æ–‡ä»¶ç®¡ç†é¡µé¢ UI\n    \"\"\"\n    with gr.Blocks() as file_ui:\n        gr.Markdown(\"# ğŸ“‚ æ–‡æœ¬æ–‡ä»¶ç®¡ç†\")\n\n        # å·¦ä¾§æ–‡ä»¶åˆ—è¡¨\n        with gr.Row():\n            with gr.Column():\n                # æ˜¾ç¤ºæ–‡ä»¶åˆ—è¡¨\n                file_list_output = gr.Textbox(label=\"æ–‡ä»¶åˆ—è¡¨\", lines=15, interactive=False)\n                refresh_files_button = gr.Button(\"ğŸ”„ åˆ·æ–°æ–‡ä»¶åˆ—è¡¨\", variant=\"primary\")\n\n            # å³ä¾§æ–‡ä»¶æ“ä½œ\n            with gr.Column():\n                selected_file = gr.Dropdown(label=\"é€‰æ‹©æ–‡ä»¶\", choices=[], interactive=True,multiselect=True)\n\n                selected_file_path = gr.Textbox(label=\"é€‰ä¸­çš„æ–‡ä»¶è·¯å¾„\",visible=True)  # ç”¨äºè®°å½•å®Œæ•´è·¯å¾„\n                with gr.Row():\n                    open_text_folder_button = gr.Button(\"ğŸ“ æ‰“å¼€æ–‡ä»¶å¤¹\", variant=\"secondary\")\n                    open_text_file_button = gr.Button(\"ğŸ“„ æ‰“å¼€æ–‡ä»¶\", variant=\"secondary\")\n                with gr.Row():\n                    set_env_button = gr.Button(\"ğŸ› ï¸ è®¾ç½®ä¸ºç¯å¢ƒå˜é‡\", variant=\"primary\")\n                    delete_text_file_button = gr.Button(\"ğŸ—‘ï¸ åˆ é™¤æ–‡ä»¶\", variant=\"stop\")\n                delete_confirmation_row = gr.Row(elem_id=\"Delete_confirmation_row\",visible=False)\n                with delete_confirmation_row:\n                    confirm_delete_button = gr.Button(\"ç¡®è®¤åˆ é™¤\", variant=\"stop\",elem_id=\"Confirm_delete_button\")\n                    cancel_delete_button = gr.Button(\"å–æ¶ˆ\", variant=\"secondary\")\n                with gr.Row():\n                    selected_file_build_graph_button = gr.Button(\"æ„å»ºå›¾è°±\")\n                    selected_file_insert_graph_button = gr.Button(\"æ’å…¥è‡³ç°æœ‰å›¾è°±\")\n                graph_build_confirmation_row = gr.Row(elem_id=\"Graph_build_confirmation_row\",visible=False)\n                with graph_build_confirmation_row:\n                    selected_confirm_build_button = gr.Button(\"ç¡®è®¤æ„å»º\",elem_id=\"Selected_Confirm_build_button\")\n                    selected_cancel_build_button = gr.Button(\"å–æ¶ˆ\",elem_id=\"Selected_Cancel_build_button\")\n                graph_insert_confirmation_row = gr.Row(elem_id=\"Graph_insert_confirmation_row\", visible=False)\n                with graph_insert_confirmation_row:\n                    selected_confirm_insert_button = gr.Button(\"ç¡®è®¤æ„å»º\", elem_id=\"Selected_Confirm_insert_button\")\n                    selected_cancel_insert_button = gr.Button(\"å–æ¶ˆ\", elem_id=\"Selected_Cancel_insert_button\")\n\n                dict_selected_files = gr.Textbox(visible=False)\n                operate_result = gr.Textbox(label=\"æ“ä½œç»“æœ\", interactive=False, lines=2)\n\n        # ä¸Šä¼ æ–‡ä»¶å’Œæ“ä½œæŒ‰é’®\n        with gr.Row():\n            with gr.Column():\n                gr.Markdown(\"### ä¸Šä¼ æ–‡ä»¶\")\n                file_uploader = gr.File(label=\"ä¸Šä¼ æ–‡ä»¶\", file_types=['text','.pdf','.doc','.ppt','.csv'],file_count=\"multiple\")\n                upload_button = gr.Button(\"ä¸Šä¼ \")\n                upload_result = gr.Textbox(label=\"ä¸Šä¼ ç»“æœ\", interactive=False, lines=5)\n\n                with gr.Row():\n                    with gr.Column():\n                        # ä¸ºæ–‡ä»¶æ„å»ºå›¾è°±ã€\n\n                        upload_file_build_graph_button = gr.Button(\"å°†ä¸Šä¼ çš„æ–‡ä»¶æ„å»ºå›¾è°±\")\n\n                        up_prebuild_dict_result = gr.Textbox(visible=False)\n                        upload_file_build_confirmation_row = gr.Row(elem_id=\"Upload_file_build_confirmation_row\",visible=False)\n                        with upload_file_build_confirmation_row:\n                            upload_file_confirm_build_button = gr.Button(\"ç¡®è®¤æ„å»º\", variant=\"stop\")\n                            upload_file_cancel_build_button = gr.Button(\"å–æ¶ˆ\", variant=\"secondary\")\n                        build_result = gr.Textbox(label=\"æ„å»ºç»“æœ\", interactive=False, lines=5)\n\n                    # æ’å…¥è‡³ç°æœ‰å›¾è°±\n                    with gr.Column():\n                        upload_file_insert_graph_button = gr.Button(\"å°†ä¸Šä¼ çš„æ–‡ä»¶æ’å…¥è‡³ç°æœ‰å›¾è°±\")\n                        up_preinsert_dict_result = gr.Textbox(visible=False)\n                        upload_file_insert_confirmation_row = gr.Row(elem_id=\"Upload_file_insert_confirmation_row\",visible=False)\n                        with upload_file_insert_confirmation_row:\n                            upload_file_confirm_insert_button = gr.Button(\"ç¡®è®¤æ’å…¥\", variant=\"stop\")\n                            upload_file_cancel_insert_button = gr.Button(\"å–æ¶ˆ\", variant=\"secondary\")\n\n                        insert_result = gr.Textbox(label=\"æ’å…¥ç»“æœ\", interactive=False, lines=5)\n\n\n        # Tips åŒºåŸŸ\n        gr.Markdown(\"### â„¹ï¸ Tips\")\n        gr.Markdown(\"\"\"\n        - **åˆ·æ–°æ–‡ä»¶åˆ—è¡¨**: æ›´æ–°å·¦ä¾§æ–‡ä»¶åˆ—è¡¨ã€‚\n        - **é€‰æ‹©æ–‡ä»¶**: åœ¨åˆ—è¡¨ä¸­é€‰æ‹©æ–‡ä»¶è¿›è¡Œæ“ä½œã€‚\n        - **æ‰“å¼€æ–‡ä»¶å¤¹**: åœ¨èµ„æºç®¡ç†å™¨ä¸­æ‰“å¼€æ–‡ä»¶æ‰€åœ¨çš„æ–‡ä»¶å¤¹ã€‚\n        - **æ‰“å¼€æ–‡ä»¶**: ä½¿ç”¨ç³»ç»Ÿé»˜è®¤ç¨‹åºæ‰“å¼€æ–‡ä»¶ã€‚\n        - **è®¾ç½®ä¸ºç¯å¢ƒå˜é‡**: å°†æ–‡ä»¶è·¯å¾„è®¾ç½®ä¸ºç¯å¢ƒå˜é‡./filesã€‚\n        - **åˆ é™¤æ–‡ä»¶**: åˆ é™¤æ–‡ä»¶å¹¶å¤‡ä»½ã€‚åˆ é™¤å‰ä¼šæç¤ºç¡®è®¤ã€‚\n        - **ä¸Šä¼ æ–‡ä»¶**: ä¸Šä¼ æ”¯æŒçš„æ–‡ä»¶ç±»å‹è‡³ç³»ç»Ÿã€‚           \n        - **ä¸ºè¯¥æ–‡ä»¶æ„å»ºå›¾è°±**: è®¾ç½®ç¯å¢ƒå˜é‡å¹¶æ„å»ºçŸ¥è¯†å›¾è°±ã€‚\n        - **æ’å…¥è‡³ç°æœ‰å›¾è°±**: å°†æ–‡ä»¶å†…å®¹æ’å…¥å½“å‰é€‰æ‹©çš„çŸ¥è¯†å›¾è°±ã€‚\n        \"\"\")\n\n\n        # äº¤äº’é€»è¾‘\n        file_mapping = gr.State()  # ç”¨äºå­˜å‚¨æ–‡ä»¶åå’Œè·¯å¾„çš„æ˜ å°„å­—å…¸\n        selected_file_name = gr.Textbox(visible=False)  # éšè—çš„ Textboxï¼Œç”¨äºè®°å½•é€‰æ‹©çš„æ–‡ä»¶å\n        selected_file_path_invisible = gr.Textbox(visible=False)\n        isselected_file = []\n        #debug_output = gr.Textbox(label=\"è°ƒè¯•ä¿¡æ¯\", lines=2, interactive=False)\n\n        def get_file_list(file_mapping):\n            file_list = refresh_dropdown_choices(file_mapping)\n            return gr.update(choices=file_list,value=file_list[0] if file_list else None)\n\n        file_ui.load(\n            fn=refresh_file_list_display,\n            inputs=[],\n            outputs=[file_list_output, file_mapping],  # æ›´æ–°æ–‡ä»¶åˆ—è¡¨å¹¶æ˜¾ç¤ºè°ƒè¯•ä¿¡æ¯\n        )\n\n        file_list_output.change(\n            fn=get_file_list,\n            inputs=[file_mapping],\n            outputs=[selected_file]\n        )\n\n        # åˆ·æ–°æ–‡ä»¶åˆ—è¡¨æ—¶æ›´æ–°æ–‡ä»¶ååˆ—è¡¨å’Œè·¯å¾„æ˜ å°„\n        refresh_files_button.click(\n            fn=refresh_file_list_display,\n            inputs=[],\n            outputs=[file_list_output, file_mapping],  # æ›´æ–°æ–‡ä»¶åˆ—è¡¨å¹¶æ˜¾ç¤ºè°ƒè¯•ä¿¡æ¯\n        )\n\n        # æ›´æ–° Dropdown çš„é€‰é¡¹\n        refresh_files_button.click(\n            fn=get_file_list,\n            inputs=[file_mapping],\n            outputs=[selected_file] # æ›´æ–° Dropdown çš„é€‰é¡¹å¹¶æ˜¾ç¤ºè°ƒè¯•ä¿¡æ¯\n        )\n\n        selected_file.change(\n            fn=debug_and_return,\n            inputs=[selected_file],\n            outputs=[selected_file_name],\n        )\n        def get_selected_file_path(names, mapping):\n            names = eval(names)\n            path = []\n            dict_selected_files = {}\n            for name in names:\n                path.append(mapping.get(name))\n                dict_selected_files[name] = mapping.get(name)\n            path_textbox = \"\\n\".join(path)\n            #print(dict_selected_files)\n            return path_textbox,path,dict_selected_files\n\n        # æ ¹æ®æ–‡ä»¶åæŸ¥æ‰¾æ–‡ä»¶è·¯å¾„\n        selected_file_name.change(\n            fn=get_selected_file_path,\n            inputs=[selected_file_name, file_mapping],\n            outputs=[selected_file_path,selected_file_path_invisible,dict_selected_files],  # æ›´æ–°æ–‡ä»¶è·¯å¾„\n        )\n        # æŒ‰é’®åŠŸèƒ½ç»‘å®š\n        open_text_folder_button.click(\n            fn=open_text_folder,\n            inputs=[selected_file_path_invisible],\n            outputs=[operate_result],\n        )\n        open_text_file_button.click(\n            fn=open_text_file,\n            inputs=[selected_file_path_invisible],\n            outputs=[operate_result],\n        )\n        set_env_button.click(\n            fn=set_rag_env_variable,\n            inputs=[selected_file_path_invisible],\n            outputs=[operate_result],\n        )\n        # æŒ‰ä¸‹â€œåˆ é™¤æ–‡ä»¶â€æŒ‰é’®ï¼Œæ˜¾ç¤ºç¡®è®¤åˆ é™¤å’Œå–æ¶ˆæŒ‰é’®\n        delete_text_file_button.click(\n            fn=lambda: gr.update(visible=True),\n            inputs=[],\n            outputs=[delete_confirmation_row],\n        )\n\n        # ç¡®è®¤åˆ é™¤\n        confirm_delete_button.click(\n            fn=delete_file_with_backup,\n            inputs=[selected_file_path_invisible],\n            outputs=[operate_result,delete_confirmation_row],\n        )\n\n        # å–æ¶ˆåˆ é™¤\n        cancel_delete_button.click(\n            fn=lambda: gr.update(visible=False),\n            inputs=[],\n            outputs=[delete_confirmation_row],\n        )\n\n        def selected_files_to_build():\n            return f\"ä½ ç¡®å®šè¦ä¸ºè¿™äº›æ–‡ä»¶æ„é€ å›¾è°±å—ï¼Ÿå¯¹åº”æ–‡ä»¶å¤¹å°†ä¼šæ˜¯./graph/ï¼ˆç¬¬ä¸€ä¸ªæ–‡ä»¶çš„åå­—ï¼‰\",gr.update(elem_id=\"Graph_build_confirmation_row\",visible=True)\n\n        selected_file_build_graph_button.click(\n            fn=selected_files_to_build,\n            inputs=[],\n            outputs=[operate_result,graph_build_confirmation_row],\n        )\n        selected_confirm_build_button.click(\n            fn=build_graph_for_files,\n            inputs=[dict_selected_files],\n            outputs=[operate_result],\n        )\n        selected_cancel_build_button.click(\n            fn=lambda : (f\"å·²å–æ¶ˆ\",gr.update(elem_id=\"Graph_build_confirmation_row\",visible=False)),\n            inputs=[],\n            outputs=[operate_result,graph_build_confirmation_row]\n        )\n        # æŒ‰ä¸‹â€œæ’å…¥åˆ°ç°æœ‰å›¾è°±â€æŒ‰é’®ï¼Œæ˜¾ç¤ºç¡®è®¤æ’å…¥å’Œå–æ¶ˆæŒ‰é’®ï¼Œå¹¶æç¤ºè·¯å¾„ä¿¡æ¯\n        selected_file_insert_graph_button.click(\n            fn=lambda path: (\n                f\"å½“å‰é€‰æ‹©çŸ¥è¯†å›¾è°±ä¸º {os.getenv('RAG_DIR', 'æœªè®¾ç½®')}, ä½ ç¡®å®šè¦æ’å…¥æ–‡ä»¶ {os.path.basename(path)}?\",\n                gr.update(visible=True)),\n            inputs=[selected_file_path_invisible],\n            outputs=[operate_result, graph_insert_confirmation_row],  # åŒæ—¶æ›´æ–°æç¤ºä¿¡æ¯å’ŒæŒ‰é’®çš„å¯è§æ€§\n        )\n        # ç¡®è®¤æ’å…¥\n        selected_confirm_insert_button.click(\n            fn=insert_graph_for_files,\n            inputs=[dict_selected_files],\n            outputs=[operate_result],\n        )\n        # å–æ¶ˆæ’å…¥\n        selected_cancel_insert_button.click(\n            fn=lambda: (gr.update(value=\"å–æ¶ˆæ’å…¥æ“ä½œã€‚\"), gr.update(visible=False)),\n            inputs=[],\n            outputs=[operate_result, graph_insert_confirmation_row],  # éšè—æŒ‰é’®å¹¶æ›´æ–°æç¤ºä¿¡æ¯\n        )\n\n        upload_button.click(\n            fn=upload_files_and_save,\n            inputs=[file_uploader],\n            outputs=[upload_result,up_prebuild_dict_result,up_preinsert_dict_result],\n        )\n\n        upload_file_build_graph_button.click(\n            fn=selected_files_to_build,\n            inputs=[],\n            outputs=[build_result,upload_file_build_confirmation_row]\n        )\n        upload_file_confirm_build_button.click(\n            fn=build_graph_for_files,\n            inputs=[up_prebuild_dict_result],\n            outputs=[build_result]\n        )\n        upload_file_cancel_build_button.click(\n            fn=lambda: (f\"å·²å–æ¶ˆ\", gr.update(elem_id=\"Upload_file_build_confirmation_row\", visible=False)),\n            inputs=[],\n            outputs=[build_result, upload_file_build_confirmation_row]\n        )\n\n        upload_file_insert_graph_button.click(\n            fn=lambda path: (\n                f\"å½“å‰é€‰æ‹©çŸ¥è¯†å›¾è°±ä¸º {os.getenv('RAG_DIR', 'æœªè®¾ç½®')}, ä½ ç¡®å®šè¦æ’å…¥è¿™äº›æ–‡ä»¶?\",\n                gr.update(visible=True)),\n            inputs=[],\n            outputs=[insert_result,upload_file_insert_confirmation_row]\n        )\n        upload_file_confirm_insert_button.click(\n            fn=build_graph_for_files,\n            inputs=[up_preinsert_dict_result],\n            outputs=[insert_result]\n        )\n        upload_file_cancel_insert_button.click(\n            fn=lambda: (f\"å·²æ„é€ \", gr.update(elem_id=\"Upload_file_insert_confirmation_row\", visible=False)),\n            inputs=[],\n            outputs=[insert_result, upload_file_insert_confirmation_row]\n        )\n\n\n    return file_ui\n\ndef graph_ui():\n    \"\"\"åˆ›å»ºå›¾è°±ç®¡ç†é¡µé¢\"\"\"\n    with gr.Blocks(visible=False, elem_id=\"graph-page\") as graph_page:  # ä½¿ç”¨ Blocks æ›¿ä»£ Column\n        gr.Markdown(\"# ğŸ“š å›¾è°±ç®¡ç†é¡µé¢\")\n\n        # ä¸Šéƒ¨å¸ƒå±€\n        with gr.Row():\n            # å·¦ä¸Šè§’ï¼šæ–‡ä»¶å¤¹åˆ—è¡¨\n            with gr.Column():\n                folder_list = gr.Textbox(\n                    label=\"æ–‡ä»¶å¤¹åˆ—è¡¨\",\n                    lines=22,\n                    interactive=False,\n                    placeholder=\"åŠ è½½ä¸­...\",\n                    elem_id=\"folder_list\"\n                )\n                update_folder_list_button = gr.Button(\n                    \"ğŸ”„åˆ·æ–°\",min_width = 100\n                )\n\n            # å³ä¸Šè§’ï¼šæ–‡ä»¶å¤¹æ“ä½œæŒ‰é’®\n            with gr.Column():\n                rag_folder_selector = gr.Dropdown(choices=[], label=\"é€‰æ‹©æ–‡ä»¶å¤¹\")\n                selected_graph_abs_path = gr.Textbox(label=\"é€‰ä¸­çš„æ–‡ä»¶è·¯å¾„\", elem_id=\"Rag_folder_selector\",visible=True)  # ç”¨äºè®°å½•å®Œæ•´è·¯å¾„\n                selected_graph_rel_path = gr.Textbox(label=\"é€‰ä¸­çš„æ–‡ä»¶è·¯å¾„\", elem_id=\"Rag_folder_selector\",visible=False)\n                with gr.Row():\n                    open_button = gr.Button(\"ğŸ“‚ æ‰“å¼€æ–‡ä»¶å¤¹\",min_width = 100)\n                    open_html_button = gr.Button(\"ğŸ“„ æ‰“å¼€ HTML\",min_width = 100)\n                with gr.Row():\n                    set_env_button = gr.Button(\"ğŸ› ï¸ è®¾ä¸ºç¯å¢ƒå˜é‡\", variant=\"primary\",min_width = 100)\n                    delete_button = gr.Button(\"ï¸ğŸ—‘ï¸ åˆ é™¤æ–‡ä»¶å¤¹\", variant=\"stop\",min_width = 100)\n                delete_status = gr.Textbox(label=\"åˆ é™¤çŠ¶æ€\", interactive=False)\n                env_status = gr.Textbox(label=\"ç¯å¢ƒå˜é‡çŠ¶æ€\", interactive=False)\n                HTML_path = gr.Textbox(label=\"HTML æ–‡ä»¶è·¯å¾„\", interactive=False)\n\n        # åº•éƒ¨ï¼šä¸Šä¼ æ–‡ä»¶\n        with gr.Row():\n            with gr.Column():\n                upload_zip = gr.File(label=\"ä¸Šä¼  ZIP æ–‡ä»¶\", file_types=[\".zip\"],file_count=\"multiple\")\n                upload_button = gr.Button(\"ä¸Šä¼ \", min_width=100)\n            upload_status = gr.Textbox(label=\"ä¸Šä¼ çŠ¶æ€\", interactive=False,lines=9)\n\n        # Tips æ \n        with gr.Row():\n            gr.Markdown(\"\"\"\n            ### â„¹ï¸ Tips:\n            - **æ‰“å¼€æ–‡ä»¶å¤¹**: åœ¨æ–‡ä»¶èµ„æºç®¡ç†å™¨ä¸­æ‰“å¼€æ‰€é€‰æ–‡ä»¶å¤¹ã€‚\n            - **æ‰“å¼€ HTML**: æœç´¢æ‰€é€‰æ–‡ä»¶å¤¹ä¸­åä¸º `knowledge_graph.html` çš„æ–‡ä»¶å¹¶æ‰“å¼€ã€‚æ‰“å¼€åä¼šå±•ç°ç›¸åº”çš„çŸ¥è¯†å›¾è°±ã€‚\n            - **è®¾ä¸ºç¯å¢ƒå˜é‡**: å°†æ‰€é€‰æ–‡ä»¶å¤¹è·¯å¾„è®¾ç½®ä¸ºç¯å¢ƒå˜é‡ã€‚\n            - **åˆ é™¤æ–‡ä»¶å¤¹**: åˆ é™¤æ‰€é€‰æ–‡ä»¶å¤¹åŠå…¶æ‰€æœ‰å†…å®¹ï¼Œæ“ä½œä¸å¯æ¢å¤ï¼Œè¯·è°¨æ…ã€‚\n            - **ä¸Šä¼  ZIP æ–‡ä»¶**: å°† ZIP æ–‡ä»¶è§£å‹è‡³ `./graph` çš„å­æ–‡ä»¶å¤¹ï¼Œæ–‡ä»¶å¤¹åä¸ ZIP æ–‡ä»¶åä¸€è‡´ã€‚ZIPä¸­çš„å†…å®¹å°±æ˜¯ä½ çš„æˆ–è€…ä»–äººåˆ†äº«çš„å›¾è°±ã€‚\n            \"\"\",\n            elem_id=\"tips-bar\",\n            )\n\n        folder_path_map = gr.State()\n\n        # ç»‘å®šäº‹ä»¶\n\n        def page_load():\n            folder_list, folder_path_dic, selective_list = list_subdirectories()\n            return folder_list,folder_path_dic,gr.update(elem_id=\"Rag_folder_selector\",choices=selective_list,value=selective_list[0] if selective_list else None)\n\n        graph_page.load(\n            fn=page_load,\n            inputs=None,\n            outputs=[folder_list,folder_path_map,rag_folder_selector]\n        )\n\n        def update_folder_list():\n            folder_path_dic = {}\n            folder_list,folder_path_dic,selective_list = list_subdirectories()\n            #print(selective_list)\n            return gr.update(elem_id=\"folder_list\",value=folder_list),gr.update(elem_id=\"Rag_folder_selector\",choices=selective_list,value=selective_list[0] if selective_list else None),folder_path_dic\n\n        update_folder_list_button.click(\n            fn=update_folder_list,\n            inputs=[],\n            outputs=[folder_list,rag_folder_selector,folder_path_map]\n        )\n\n        def mapping_path(folder_name, folder_dict):\n            \"\"\"\n                æ ¹æ®æ–‡ä»¶å¤¹åç§°è¿”å›å¯¹åº”çš„ç»å¯¹è·¯å¾„å’Œç›¸å¯¹è·¯å¾„ã€‚\n                :param folder_name: è¦æŸ¥æ‰¾çš„æ–‡ä»¶å¤¹åç§°\n                :param folder_dict: åŒ…å«æ–‡ä»¶å¤¹åç§°ä¸ç»å¯¹è·¯å¾„çš„å­—å…¸\n            \"\"\"\n            base_path = \"./graph\"\n            if folder_name not in folder_dict:\n                return {\"error\": \"Folder name not found in the dictionary.\"}\n            absolute_path = folder_dict[folder_name]\n            relative_path = f\"./graph/\" + os.path.relpath(absolute_path, start=base_path)\n            return absolute_path,relative_path\n\n        rag_folder_selector.change(\n            fn=mapping_path,\n            inputs=[rag_folder_selector,folder_path_map],\n            outputs=[selected_graph_abs_path,selected_graph_rel_path]\n        )\n\n        open_button.click(\n            fn=open_rag_folder,\n            inputs=selected_graph_abs_path,\n            outputs=None,\n        )\n\n        open_html_button.click(\n            fn=find_html_file,\n            inputs=selected_graph_abs_path,\n            outputs=HTML_path,\n        )\n\n        set_env_button.click(\n            fn=set_env_variable_from_folder,\n            inputs=selected_graph_rel_path,\n            outputs=env_status,\n        )\n\n        delete_button.click(\n            fn=backup_and_delete_graph_folder,\n            inputs=[selected_graph_abs_path],\n            outputs=[delete_status],\n        )\n        # æŒ‰é’®ç‚¹å‡»é€»è¾‘ï¼šå…ˆæš‚å­˜ä¸Šä¼ æ–‡ä»¶ï¼Œå†è§¦å‘è§£å‹\n        uploaded_files = gr.State([])  # ç”¨äºæš‚å­˜ä¸Šä¼ çš„æ–‡ä»¶\n\n        upload_zip.upload(\n            fn=lambda files: files,  # æš‚å­˜ä¸Šä¼ çš„æ–‡ä»¶\n            inputs=upload_zip,\n            outputs=[uploaded_files]\n        )\n\n        upload_button.click(\n            fn=process_uploaded_zips_with_progress,  # ç»Ÿä¸€å¤„ç†ä¸Šä¼ çš„ ZIP æ–‡ä»¶\n            inputs=[uploaded_files],\n            outputs=[upload_status]\n        )\n\n\n        upload_status.change(\n            fn=update_folder_list,\n            inputs=[],\n            outputs=[folder_list,rag_folder_selector,folder_path_map]\n        )\n\n\n\n    return graph_page\n\ndef pdf_management_ui():\n    \"\"\"åˆ›å»º PDF ç®¡ç†é¡µé¢\"\"\"\n    with gr.Blocks() as ui:  # ä¸»æ¡†æ¶\n        # å®šä¹‰çŠ¶æ€å˜é‡\n        pdf_page_visible = gr.State(value=not should_show_notification() and STATE.get(\"dependencies_installed\", False))\n        notification_page_visible = gr.State(value=not pdf_page_visible.value)\n\n        # PDF ç®¡ç†é¡µé¢\n        with gr.Accordion(visible=pdf_page_visible.value, elem_id=\"pdf-management-page\") as pui:\n            gr.Markdown(\"# ğŸŒ HTML to PDF è½¬æ¢å·¥å…·\")\n\n            # é¡¶éƒ¨å¸ƒå±€\n            with gr.Row():\n                # å·¦ä¾§ï¼šURLè¾“å…¥å’Œåˆ—è¡¨æ˜¾ç¤º\n                with gr.Column():\n                    gr.Markdown(\"### ğŸŒ ç½‘é¡µåœ°å€\")\n                    url_input = gr.Textbox(\n                        label=\"è¾“å…¥ç½‘é¡µåœ°å€ï¼ˆæ¯è¡Œä¸€ä¸ªï¼‰\",\n                        lines=5,\n                        placeholder=\"è¯·è¾“å…¥ä¸€ä¸ªæˆ–å¤šä¸ªç½‘å€ï¼Œæ¯è¡Œä¸€ä¸ª\",\n                        elem_id=\"url-input\"\n                    )\n                    add_button = gr.Button(\n                        \"+ æ·»åŠ åˆ°åˆ—è¡¨\",\n                        variant=\"primary\",\n                        elem_id=\"add-button\",\n                    )\n                    urls_display = gr.Textbox(\n                        label=\"å·²æ·»åŠ çš„ç½‘é¡µ\",\n                        lines=10,\n                        interactive=False,\n                        placeholder=\"å½“å‰æœªæ·»åŠ ä»»ä½•ç½‘é¡µ\",\n                        elem_id=\"url-display\"\n                    )\n                    url_list = gr.State([])\n\n                # å³ä¾§ï¼šåŠŸèƒ½æŒ‰é’®åŒº\n                with gr.Column():\n                    gr.Markdown(\"### ğŸ“„ PDF æ“ä½œ\")\n                    generate_single_pdf = gr.Button(\n                        \"ğŸ“˜ ç”Ÿæˆå•ä¸ª PDF\",\n                        variant=\"primary\",\n                    )\n                    generate_multiple_pdfs = gr.Button(\n                        \"ğŸ“š ç”Ÿæˆå¤šä¸ª PDF\",\n                        variant=\"primary\",\n                    )\n                    selected_pdf = gr.Textbox(\n                        label=\"é€‰æ‹©çš„ PDF æ–‡ä»¶è·¯å¾„\",\n                        placeholder=\"è¯·è¾“å…¥æˆ–é€‰æ‹© PDF æ–‡ä»¶è·¯å¾„\",\n                        elem_id=\"pdf-path\"\n                    )\n                    open_pdf_button = gr.Button(\n                        \"ğŸ“‚ æ‰“å¼€ PDF\",\n                        variant=\"secondary\",\n                    )\n                    delete_pdf_button = gr.Button(\n                        \"ğŸ—‘ï¸ åˆ é™¤ PDF\",\n                        variant=\"stop\",\n                    )\n\n            # åº•éƒ¨ï¼šæ“ä½œç»“æœæ˜¾ç¤º\n            with gr.Row():\n                operation_output = gr.Textbox(\n                    label=\"æ“ä½œç»“æœ\",\n                    lines=5,\n                    interactive=False,\n                    elem_id=\"operation-output\"\n                )\n\n            # æç¤ºåŒºåŸŸ\n            gr.Markdown(\"\"\"\n            ### â„¹ï¸ Tips:\n            - **æ·»åŠ åˆ°åˆ—è¡¨**: å°†è¾“å…¥çš„ç½‘é¡µåœ°å€åŠ å…¥å¾…è½¬æ¢åˆ—è¡¨ï¼Œæ”¯æŒå¤šä¸ª URLï¼Œä¸€ä¸ªURLä¸€è¡Œã€‚\n            - **ç”Ÿæˆå•ä¸ª PDF**: å°†ç¬¬ä¸€ä¸ªç½‘å€è½¬æ¢ä¸º PDFã€‚\n            - **ç”Ÿæˆå¤šä¸ª PDF**: æ‰¹é‡å°†æ‰€æœ‰ç½‘å€è½¬æ¢ä¸ºå¤šä¸ª PDF æ–‡ä»¶ã€‚\n            - **æ‰“å¼€ PDF**: ä½¿ç”¨ç³»ç»Ÿé»˜è®¤åº”ç”¨æ‰“å¼€é€‰æ‹©çš„ PDF æ–‡ä»¶ã€‚\n            - **åˆ é™¤ PDF**: åˆ é™¤é€‰æ‹©çš„ PDF æ–‡ä»¶ï¼Œè¯·è°¨æ…æ“ä½œã€‚\n            \"\"\", elem_id=\"tips-bar\")\n\n            def add_unique_url(input_urls, urls):\n                \"\"\"\n                æ·»åŠ ç”¨æˆ·è¾“å…¥çš„å¤šä¸ªç½‘å€åˆ°å·²æœ‰ç½‘å€åˆ—è¡¨ä¸­ï¼Œå¹¶å»é™¤é‡å¤é¡¹\n                \"\"\"\n                # æ‹†åˆ†ç”¨æˆ·è¾“å…¥çš„ç½‘å€åˆ—è¡¨ï¼ŒæŒ‰æ¢è¡Œç¬¦å’Œé€—å·åˆ†å‰²\n                new_urls = [url.strip() for url in input_urls.splitlines() if url.strip()]\n\n                # åˆå¹¶æ–°ç½‘å€ä¸å·²æœ‰ç½‘å€\n                combined_urls = urls + new_urls\n\n                # å»é‡å¹¶ä¿æŒé¡ºåº\n                deduplicated_urls = list(dict.fromkeys(combined_urls))  # ä½¿ç”¨ dict ä¿æŒé¡ºåºçš„å»é‡æ–¹å¼\n\n                # è¿”å›æ›´æ–°åçš„åˆ—è¡¨å’Œæ˜¾ç¤ºå†…å®¹\n                return deduplicated_urls, \"\\n\".join(deduplicated_urls)\n\n            # æŒ‰é’®äº¤äº’é€»è¾‘\n            add_button.click(\n                fn=add_unique_url,\n                inputs=[url_input, url_list],\n                outputs=[url_list, urls_display]\n            )\n\n            generate_single_pdf.click(\n                fn=lambda urls: html_to_pdf([urls[0]]) if urls else \"è¯·å…ˆæ·»åŠ è‡³å°‘ä¸€ä¸ª URL\",\n                inputs=[url_list],\n                outputs=[operation_output]\n            )\n\n            generate_multiple_pdfs.click(\n                fn=lambda urls: html_to_pdf(urls) if urls else \"è¯·å…ˆæ·»åŠ è‡³å°‘ä¸€ä¸ª URL\",\n                inputs=[url_list],\n                outputs=[operation_output]\n            )\n\n            open_pdf_button.click(\n                fn=open_pdf,\n                inputs=[selected_pdf],\n                outputs=[operation_output]\n            )\n\n            delete_pdf_button.click(\n                fn=lambda pdf: delete_pdf_with_backup([pdf]) if pdf else \"è¯·å…ˆé€‰æ‹©ä¸€ä¸ª PDF è·¯å¾„\",\n                inputs=[selected_pdf],\n                outputs=[operation_output]\n            )\n\n        # é€šçŸ¥é¡µé¢\n        with gr.Accordion(visible=notification_page_visible.value, elem_id=\"notification-page\") as notification_ui:\n            gr.Markdown(\"### âš ï¸ é€šçŸ¥ï¼šæ­¤é¡µé¢åŠŸèƒ½å°šæœªå®Œæˆï¼Œç›®å‰å¤„äºä¸å¯ç”¨çŠ¶æ€\", visible=notification_page_visible.value)\n            gr.Markdown(\"è¯·å®‰è£…ç›¸å…³ä¾èµ–ï¼Œæˆ–ç›´æ¥è·³è¿‡æ­¤é€šçŸ¥å¼€å§‹ä½¿ç”¨å·¥å…·ã€‚\", visible=notification_page_visible.value)\n            install_btn = gr.Button(\n                \"å®‰è£…ä¾èµ–\",\n                variant=\"primary\",\n                visible=notification_page_visible.value,\n            )\n            close_btn = gr.Button(\n                \"è·³è¿‡å¹¶å¼€å§‹ä½¿ç”¨\",\n                variant=\"secondary\",\n                visible=notification_page_visible.value,\n            )\n            remember_checkbox = gr.Checkbox(\n                label=\"7 å¤©å†…ä¸å†æ˜¾ç¤º\",\n                elem_id=\"remember-checkbox\",\n                visible=notification_page_visible.value\n            )\n\n            # å®‰è£…ä¾èµ–é€»è¾‘\n            install_btn.click(\n                fn=handle_install_dependencies,\n                inputs=[],\n                outputs=[notification_ui]\n            )\n\n            # è·³è¿‡é€»è¾‘ï¼šåˆ‡æ¢é¡µé¢å¯è§æ€§\n            def skip_notification(remember, pdf_visible):\n                # æ ¹æ®ç”¨æˆ·æ“ä½œè°ƒæ•´é¡µé¢çŠ¶æ€\n                pdf_visible = True\n                return pdf_visible, not pdf_visible, gr.update(visible=pdf_visible), gr.update(visible=False)\n\n            close_btn.click(\n                fn=skip_notification,\n                inputs=[remember_checkbox, pdf_page_visible],\n                outputs=[pdf_page_visible, notification_page_visible, pui, notification_ui]\n            )\n\n    return ui\n\ndef intro_animation():\n    \"\"\"\n    çº¯ CSS å®ç°æ¸å˜æ–‡å­—å’ŒèƒŒæ™¯å¼•å¯¼åŠ¨ç”»ï¼Œå¹¶åœ¨åŠ¨ç”»ç»“æŸåæ¢å¤æ»šåŠ¨æ¡ã€‚\n    \"\"\"\n    html_content = \"\"\"\n    <style>\n    body {\n        margin: 0;\n        overflow: hidden; /* é˜²æ­¢æ»šåŠ¨æ¡åœ¨åŠ¨ç”»æœŸé—´æ˜¾ç¤º */\n        animation: restoreOverflow 2s ease-in-out 4s forwards; /* åœ¨åŠ¨ç”»ç»“æŸåæ¢å¤æ»šåŠ¨æ¡ */\n    }\n\n    #intro-page {\n        position: fixed;\n        top: 0;\n        left: 0;\n        width: 100%;\n        height: 100%;\n        background-color: #ffffff; /* èƒŒæ™¯ä¸ºçº¯ç™½ */\n        z-index: 9999; /* ç¡®ä¿å¼•å¯¼åŠ¨ç”»å±‚åœ¨æœ€å‰é¢ */\n        display: flex;\n        align-items: center;\n        justify-content: center;\n        animation: fadeOut 2s ease-in-out 4s forwards; /* å»¶è¿Ÿ4ç§’åå¼€å§‹æ¸éš */\n    }\n\n    #intro-text {\n        font-size: 2rem;\n        color: #333333;\n        text-align: center; /* å±…ä¸­å¯¹é½æ–‡å­— */\n        opacity: 0;\n        animation: fadeInText 2s ease-in-out forwards; /* æ–‡æœ¬æ¸æ˜¾ */\n    }\n    \n    #intro-text div {\n        margin-top: 10px; /* è®¾ç½®æ¯è¡Œä¹‹é—´çš„é—´è· */\n    }\n    \n    @keyframes fadeInText {\n        0% { opacity: 0; }\n        100% { opacity: 1; }\n    }\n\n    @keyframes fadeOut {\n        0% { opacity: 1; }\n        100% {\n            opacity: 0;\n            z-index: -1; /* æœ€åé˜¶æ®µéšè—åŠ¨ç”»å±‚ */\n            display: none; /* ç¡®ä¿ä¸å†å ç”¨ç©ºé—´ */\n        }\n    }\n\n    @keyframes restoreOverflow {\n        0% { overflow: hidden; }\n        100% { overflow: auto; } /* æ¢å¤æ»šåŠ¨æ¡ */\n    }\n    </style>\n    <div id=\"intro-page\">\n        <div id=\"intro-text\">\n            <div>æ¬¢è¿ä½¿ç”¨</div>\n            <div>LightRAG for OpenAI Standard Frontend</div>\n        </div>\n    </div>\n    \"\"\"\n    return html_content\n\ndef settings_ui():\n    with gr.Blocks() as settings:\n        gr.Markdown(\"# å…³äºå‰ç«¯\")\n        with gr.Row():\n            with gr.Column():\n                gr.Markdown(\"## å‰ç«¯è®¾ç½®é¢æ¿\")\n                start_page = gr.Checkbox(label=\"è·³è¿‡å¯åŠ¨é¡µé¢\",elem_id=\"Start_page\")\n                frontend_port = gr.Textbox(label=\"å‰ç«¯ç«¯å£\",elem_id=\"Frontend_port\")\n            with gr.Column():\n                gr.Markdown(\"## å…³äºæˆ‘ä»¬\")\n                who_we_are_textbox = gr.Textbox(\n                    label=\"æˆ‘ä»¬æ¥è‡ªï¼Ÿ\",\n                    value=\"æˆ‘ä»¬ç®—ä¸ä¸Šä»€ä¹ˆæ­£å„¿å…«ç»çš„å›¢é˜Ÿï¼Œæ›´åƒæ˜¯ä¸€ç¾¤é—²çš„æ²¡äº‹å¹²çš„çˆ±å¥½è€…ã€‚å…¶ä¸­ä¸å°‘æ˜¯å¤§å­¦ç”Ÿï¼Œå› æ­¤æœ¬é¡¹ç›®çš„æ›´æ–°ä¼šæ¯”è¾ƒæ…¢ï¼Œè¯·è°…è§£ã€‚\",\n                    lines=3,\n                    interactive=False\n                )\n                community_textbox = gr.Textbox(\n                    label=\"æƒ³è¦äº¤æµï¼Ÿ\",\n                    value=\n                          \"QQç¾¤ï¼šxxx\\n\"\n                          \"Discordï¼šxxx\",\n                    lines=2,\n                    interactive=False\n                )\n\n        def start_page_show(bool_start_page):\n            skip = True if bool_start_page else False\n            update_env_variable(\"start_page_IsNotShow\", str(skip))\n            return gr.update(elem_id=\"Start_page\",value = skip)\n\n        def check_settings():\n            IsNotShow = os.getenv(\"start_page_IsNotShow\",\"\") == 'True'\n            the_frontend_port = os.getenv(\"FRONTEND_PORT\",\"\")\n            return gr.update(elem_id=\"Start_page\",value = IsNotShow),gr.update(elem_id=\"Frontend_port\",value = the_frontend_port)\n\n        settings.load(\n            fn=check_settings,\n            outputs=[start_page,frontend_port]\n        )\n\n        start_page.change(\n            fn=start_page_show,\n            inputs=[start_page]\n        )\n        def Frontend_port(port):\n            update_env_variable(\"FRONTEND_PORT\",port)\n            return None\n\n        frontend_port.change(\n            fn=Frontend_port,\n            inputs=[frontend_port]\n        )\n        return  settings\n\n'''\n\ndef switch_page(page):\n    \"\"\"æ ¹æ®é¡µé¢çŠ¶æ€è¿”å›æ›´æ–°\"\"\"\n    if page == \"env_management\":\n        return (\n            gr.update(visible=True),  # ç¯å¢ƒå˜é‡é¡µé¢å¯è§\n            gr.update(visible=False),  # æ–‡ä»¶ç®¡ç†é¡µé¢éšè—\n            gr.update(visible=False),  # å›¾è°±ç®¡ç†é¡µé¢éšè—\n        )\n    elif page == \"file_management\":\n        return (\n            gr.update(visible=False),  # ç¯å¢ƒå˜é‡é¡µé¢éšè—\n            gr.update(visible=True),  # æ–‡ä»¶ç®¡ç†é¡µé¢å¯è§\n            gr.update(visible=False),  # å›¾è°±ç®¡ç†é¡µé¢éšè—\n        )\n    elif page == \"graph_management\":\n        return (\n            gr.update(visible=False),  # ç¯å¢ƒå˜é‡é¡µé¢éšè—\n            gr.update(visible=False),  # æ–‡ä»¶ç®¡ç†é¡µé¢éšè—\n            gr.update(visible=True),  # å›¾è°±ç®¡ç†é¡µé¢å¯è§\n        )\n    # é»˜è®¤éšè—æ‰€æœ‰é¡µé¢\n    return (\n        gr.update(visible=False),\n        gr.update(visible=False),\n        gr.update(visible=False),\n    )\n# æ›´æ–°å¯¼èˆªæ å’Œä¸»ç•Œé¢\ndef create_navbar():\n    \"\"\"åˆ›å»ºå¯¼èˆªæ \"\"\"\n    with gr.Row():\n        env_button = gr.Button(\"ç¯å¢ƒå˜é‡ç®¡ç†\", variant=\"secondary\", elem_id=\"env-btn\")\n        file_button = gr.Button(\"æ–‡æœ¬æ–‡ä»¶ç®¡ç†\", variant=\"secondary\", elem_id=\"file-btn\")\n        graph_button = gr.Button(\"çŸ¥è¯†å›¾è°±ç®¡ç†\", variant=\"secondary\", elem_id=\"graph-btn\")\n    return env_button, file_button, graph_button\n\n\ndef build_ui():\n    \"\"\"ä¸»ç•Œé¢æ„å»º\"\"\"\n    with gr.Blocks() as ui:\n        current_page = gr.State(\"file_management\")  # åˆå§‹é¡µé¢çŠ¶æ€\n\n        # å¯¼èˆªæ \n        env_button, file_button, graph_button = create_navbar()\n\n        # é¡µé¢å®¹å™¨\n        with gr.Row():\n            with gr.Column(visible=False, elem_id=\"env-page\") as env_page:\n                env_variables_ui()\n            with gr.Column(visible=True, elem_id=\"file-page\") as file_page:\n                create_file_upload_ui()\n            with gr.Column(visible=False, elem_id=\"graph-page\") as graph_page:\n                create_graph_ui()\n\n        # ç‚¹å‡»æŒ‰é’®æ›´æ–°é¡µé¢çŠ¶æ€\n        env_button.click(\n            fn=lambda: \"env_management\",\n            inputs=None,\n            outputs=current_page,\n        )\n        file_button.click(\n            fn=lambda: \"file_management\",\n            inputs=None,\n            outputs=current_page,\n        )\n        graph_button.click(\n            fn=lambda: \"graph_management\",\n            inputs=None,\n            outputs=current_page,\n        )\n\n        # é¡µé¢çŠ¶æ€å˜æ›´æ—¶æ›´æ–°å¯è§æ€§\n        current_page.change(\n            fn=switch_page,\n            inputs=current_page,\n            outputs=[env_page, file_page, graph_page],\n        )\n\n    return ui\n'''\n\ndef build_ui_with_tabs():\n    # è‡ªå®šä¹‰CSS\n    custom_css = \"\"\"\n            .SideBar {\n                width: auto !important;\n                height: 100% !important;\n                max-width: 25% !important;\n                background-color: #f5f5f5;\n                padding: 10px;\n                box-shadow: 2px 0 5px rgba(0, 0, 0, 0.1);\n            }\n\n            .Closed-SideBar {\n                width: 50% !important;\n                height: 100% !important;\n                max-width: 5% !important;\n                background-color: #f5f5f5;\n                box-shadow: 2px 0 5px rgba(0, 0, 0, 0.1);\n                text-align: right; /* å°†å†…å®¹é å³å¯¹é½ */\n            }\n            \n            #Closed-SideBar-button {\n                width: 30% !important;\n                height: 50% !important;\n                max-width: 5% !important;\n                background: linear-gradient(90deg, #4caf50, #8bc34a);\n                color: white;\n                border: none;\n                border-radius: 5px;\n                padding: 10px;\n                font-size: 1rem;\n                cursor: pointer;\n                transition: background 0.3s ease-in-out;\n                text-align: right;\n            }\n            \n            .gradient-button {\n                background: linear-gradient(90deg, #4caf50, #8bc34a);\n                color: white;\n                border: none;\n                border-radius: 5px;\n                padding: 10px;\n                font-size: 1rem;\n                cursor: pointer;\n                transition: background 0.3s ease-in-out;\n            }\n\n            .gradient-button:hover {\n                background: linear-gradient(90deg, #8bc34a, #4caf50);\n            }\n            #admin-page-title {\n                text-align: center; /* å±…ä¸­å¯¹é½æ–‡æœ¬ */\n                font-size: 24px; /* è°ƒæ•´å­—ä½“å¤§å° */\n                font-weight: bold; /* å¯é€‰ï¼šä½¿æ–‡æœ¬åŠ ç²— */\n            }\n            #ASideBar {\n                width: auto !important;\n                height: 20% !important;\n                max-width: 40% !important;\n                text-align: center; /* å±…ä¸­å¯¹é½ */\n                font-size: 40px; /* å­—ä½“å¤§å° */\n                font-weight: bold; /* åŠ ç²— */\n                background-color: #f5f5f5; /* èƒŒæ™¯è‰²ä¸ä¾§è¾¹æ ä¸€è‡´ */\n                padding: 15px; /* å†…è¾¹è· */\n                border-radius: 5px; /* åœ†è§’ */\n                box-shadow: 2px 2px 10px rgba(0, 0, 0, 0.1); /* é˜´å½±æ•ˆæœ */\n                margin-bottom: 20px; /* ä¸‹è¾¹è· */\n                color: #333; /* å­—ä½“é¢œè‰² */\n            }\n    \"\"\"\n    \"\"\"æ„å»ºå¸¦æœ‰ Tabs çš„ä¸»ç•Œé¢\"\"\"\n\n    with gr.Blocks(css=custom_css) as ui:\n        def get_intro_animation():\n            '''\n            å›è°ƒå‡½æ•°åˆ¤æ–­æ˜¯å¦æ˜¾ç¤ºå¯åŠ¨é¡µé¢\n            '''\n            load_dotenv()\n            Start_page_IsNotShow = os.getenv('start_page_IsNotShow', 'False').lower() == 'true'\n            if not Start_page_IsNotShow:\n                return intro_animation()\n            return \"\"\n\n        gr.HTML(get_intro_animation)\n\n        with gr.Column():\n            gr.Markdown(\"# ç®¡ç†ç•Œé¢\",elem_id=\"admin-page-title\")\n            with gr.Row():\n                sidebar_ui()\n\n            # ä½¿ç”¨ Tabs åˆ›å»ºå¯¼èˆªæ \n                with gr.TabItem(\"æ¬¢è¿ä½¿ç”¨\"):\n                    welcome_page()  # æ¬¢è¿é¡µé¢\n\n                with gr.TabItem(\"æ–‡ä»¶ç®¡ç†\"):\n                    file_management_ui()  # æ–‡ä»¶ç®¡ç†é¡µé¢\n\n                with gr.TabItem(\"å›¾è°±ç®¡ç†\"):\n                    graph_ui()  # å›¾è°±ç®¡ç†é¡µé¢\n\n                with gr.TabItem(\"HTML to Graph\"):\n                    pdf_management_ui()\n\n                with gr.TabItem(\"å…³äºå‰ç«¯\"):\n                    settings_ui()\n\n\n\n\n    return ui\n\n\n# å¯åŠ¨ Gradio åº”ç”¨\nif __name__ == \"__main__\":\n    load_dotenv(override=True)\n    F_port = int(os.getenv(\"FRONTEND_PORT\",\"\"))\n    build_ui_with_tabs().launch(server_port=F_port, share=False)\n    sleep(5)\n    webbrowser.open(f\"http://127.0.0.1:{F_port}\")\n    #asyncio.run(fetch_model_info(os.getenv(\"OPENAI_BASE_URL\", \"\"),os.getenv(\"OPENAI_API_KEY\", \"\")))\n"}
{"type": "source_file", "path": "Gradio/pkg.py", "content": "import pkg_resources\n\ndef export_dependencies(output_file):\n    \"\"\"\n    å°†å½“å‰è™šæ‹Ÿç¯å¢ƒçš„æ‰€æœ‰ä¾èµ–åŒ…åŠå…¶ç‰ˆæœ¬å·å¯¼å‡ºåˆ°æŒ‡å®šçš„ txt æ–‡ä»¶ä¸­ã€‚\n    :param output_file: è¾“å‡ºæ–‡ä»¶çš„è·¯å¾„\n    \"\"\"\n    try:\n        # è·å–æ‰€æœ‰å·²å®‰è£…çš„ä¾èµ–åŒ…\n        installed_packages = pkg_resources.working_set\n\n        # æŒ‰åŒ…åæ’åºï¼Œç”Ÿæˆæ ¼å¼åŒ–å­—ç¬¦ä¸²\n        dependencies = sorted([f\"{pkg.key}=={pkg.version}\" for pkg in installed_packages])\n\n        # å°†ç»“æœå†™å…¥åˆ°æŒ‡å®šæ–‡ä»¶\n        with open(output_file, \"w\", encoding=\"utf-8\") as f:\n            f.write(\"\\n\".join(dependencies))\n        print(f\"ä¾èµ–åŒ…åˆ—è¡¨å·²æˆåŠŸå¯¼å‡ºè‡³ {output_file}\")\n    except Exception as e:\n        print(f\"å¯¼å‡ºä¾èµ–åŒ…åˆ—è¡¨æ—¶å‡ºé”™: {e}\")\n\n# ç¤ºä¾‹ç”¨æ³•\nif __name__ == \"__main__\":\n    output_file = \"./dependencies.txt\"  # è¾“å‡ºæ–‡ä»¶è·¯å¾„\n    export_dependencies(output_file)"}
{"type": "source_file", "path": "examples/lightrag_ollama_demo.py", "content": "import asyncio\nimport os\nimport inspect\nimport logging\nfrom lightrag import LightRAG, QueryParam\nfrom lightrag.llm import ollama_model_complete, ollama_embedding\nfrom lightrag.utils import EmbeddingFunc\n\nWORKING_DIR = \"./dickens\"\n\nlogging.basicConfig(format=\"%(levelname)s:%(message)s\", level=logging.INFO)\n\nif not os.path.exists(WORKING_DIR):\n    os.mkdir(WORKING_DIR)\n\nrag = LightRAG(\n    working_dir=WORKING_DIR,\n    llm_model_func=ollama_model_complete,\n    llm_model_name=\"gemma2:2b\",\n    llm_model_max_async=4,\n    llm_model_max_token_size=32768,\n    llm_model_kwargs={\"host\": \"http://localhost:11434\", \"options\": {\"num_ctx\": 32768}},\n    embedding_func=EmbeddingFunc(\n        embedding_dim=768,\n        max_token_size=8192,\n        func=lambda texts: ollama_embedding(\n            texts, embed_model=\"nomic-embed-text\", host=\"http://localhost:11434\"\n        ),\n    ),\n)\n\nwith open(\"./book.txt\", \"r\", encoding=\"utf-8\") as f:\n    rag.insert(f.read())\n\n# Perform naive search\nprint(\n    rag.query(\"What are the top themes in this story?\", param=QueryParam(mode=\"naive\"))\n)\n\n# Perform local search\nprint(\n    rag.query(\"What are the top themes in this story?\", param=QueryParam(mode=\"local\"))\n)\n\n# Perform global search\nprint(\n    rag.query(\"What are the top themes in this story?\", param=QueryParam(mode=\"global\"))\n)\n\n# Perform hybrid search\nprint(\n    rag.query(\"What are the top themes in this story?\", param=QueryParam(mode=\"hybrid\"))\n)\n\n# stream response\nresp = rag.query(\n    \"What are the top themes in this story?\",\n    param=QueryParam(mode=\"hybrid\", stream=True),\n)\n\n\nasync def print_stream(stream):\n    async for chunk in stream:\n        print(chunk, end=\"\", flush=True)\n\n\nif inspect.isasyncgen(resp):\n    asyncio.run(print_stream(resp))\nelse:\n    print(resp)\n"}
{"type": "source_file", "path": "examples/lightrag_api_oracle_demo.py", "content": "from fastapi import FastAPI, HTTPException, File, UploadFile\nfrom fastapi import Query\nfrom contextlib import asynccontextmanager\nfrom pydantic import BaseModel\nfrom typing import Optional, Any\n\nimport sys\nimport os\n\n\nfrom pathlib import Path\n\nimport asyncio\nimport nest_asyncio\nfrom lightrag import LightRAG, QueryParam\nfrom lightrag.llm import openai_complete_if_cache, openai_embedding\nfrom lightrag.utils import EmbeddingFunc\nimport numpy as np\n\nfrom lightrag.kg.oracle_impl import OracleDB\n\nprint(os.getcwd())\nscript_directory = Path(__file__).resolve().parent.parent\nsys.path.append(os.path.abspath(script_directory))\n\n\n# Apply nest_asyncio to solve event loop issues\nnest_asyncio.apply()\n\nDEFAULT_RAG_DIR = \"index_default\"\n\n\n# We use OpenAI compatible API to call LLM on Oracle Cloud\n# More docs here https://github.com/jin38324/OCI_GenAI_access_gateway\nBASE_URL = \"http://xxx.xxx.xxx.xxx:8088/v1/\"\nAPIKEY = \"ocigenerativeai\"\n\n# Configure working directory\nWORKING_DIR = os.environ.get(\"RAG_DIR\", f\"{DEFAULT_RAG_DIR}\")\nprint(f\"WORKING_DIR: {WORKING_DIR}\")\nLLM_MODEL = os.environ.get(\"LLM_MODEL\", \"cohere.command-r-plus-08-2024\")\nprint(f\"LLM_MODEL: {LLM_MODEL}\")\nEMBEDDING_MODEL = os.environ.get(\"EMBEDDING_MODEL\", \"cohere.embed-multilingual-v3.0\")\nprint(f\"EMBEDDING_MODEL: {EMBEDDING_MODEL}\")\nEMBEDDING_MAX_TOKEN_SIZE = int(os.environ.get(\"EMBEDDING_MAX_TOKEN_SIZE\", 512))\nprint(f\"EMBEDDING_MAX_TOKEN_SIZE: {EMBEDDING_MAX_TOKEN_SIZE}\")\n\nif not os.path.exists(WORKING_DIR):\n    os.mkdir(WORKING_DIR)\n\n\nasync def llm_model_func(\n    prompt, system_prompt=None, history_messages=[], keyword_extraction=False, **kwargs\n) -> str:\n    return await openai_complete_if_cache(\n        LLM_MODEL,\n        prompt,\n        system_prompt=system_prompt,\n        history_messages=history_messages,\n        api_key=APIKEY,\n        base_url=BASE_URL,\n        **kwargs,\n    )\n\n\nasync def embedding_func(texts: list[str]) -> np.ndarray:\n    return await openai_embedding(\n        texts,\n        model=EMBEDDING_MODEL,\n        api_key=APIKEY,\n        base_url=BASE_URL,\n    )\n\n\nasync def get_embedding_dim():\n    test_text = [\"This is a test sentence.\"]\n    embedding = await embedding_func(test_text)\n    embedding_dim = embedding.shape[1]\n    return embedding_dim\n\n\nasync def init():\n    # Detect embedding dimension\n    embedding_dimension = await get_embedding_dim()\n    print(f\"Detected embedding dimension: {embedding_dimension}\")\n    # Create Oracle DB connection\n    # The `config` parameter is the connection configuration of Oracle DB\n    # More docs here https://python-oracledb.readthedocs.io/en/latest/user_guide/connection_handling.html\n    # We storage data in unified tables, so we need to set a `workspace` parameter to specify which docs we want to store and query\n    # Below is an example of how to connect to Oracle Autonomous Database on Oracle Cloud\n\n    oracle_db = OracleDB(\n        config={\n            \"user\": \"\",\n            \"password\": \"\",\n            \"dsn\": \"\",\n            \"config_dir\": \"path_to_config_dir\",\n            \"wallet_location\": \"path_to_wallet_location\",\n            \"wallet_password\": \"wallet_password\",\n            \"workspace\": \"company\",\n        }  # specify which docs you want to store and query\n    )\n\n    # Check if Oracle DB tables exist, if not, tables will be created\n    await oracle_db.check_tables()\n    # Initialize LightRAG\n    # We use Oracle DB as the KV/vector/graph storage\n    # You can add `addon_params={\"example_number\": 1, \"language\": \"Simplfied Chinese\"}` to control the prompt\n    rag = LightRAG(\n        enable_llm_cache=False,\n        working_dir=WORKING_DIR,\n        chunk_token_size=512,\n        llm_model_func=llm_model_func,\n        embedding_func=EmbeddingFunc(\n            embedding_dim=embedding_dimension,\n            max_token_size=512,\n            func=embedding_func,\n        ),\n        graph_storage=\"OracleGraphStorage\",\n        kv_storage=\"OracleKVStorage\",\n        vector_storage=\"OracleVectorDBStorage\",\n    )\n\n    # Setthe KV/vector/graph storage's `db` property, so all operation will use same connection pool\n    rag.graph_storage_cls.db = oracle_db\n    rag.key_string_value_json_storage_cls.db = oracle_db\n    rag.vector_db_storage_cls.db = oracle_db\n\n    return rag\n\n\n# Extract and Insert into LightRAG storage\n# with open(\"./dickens/book.txt\", \"r\", encoding=\"utf-8\") as f:\n#   await rag.ainsert(f.read())\n\n# # Perform search in different modes\n# modes = [\"naive\", \"local\", \"global\", \"hybrid\"]\n# for mode in modes:\n#     print(\"=\"*20, mode, \"=\"*20)\n#     print(await rag.aquery(\"è¿™ç¯‡æ–‡æ¡£æ˜¯å…³äºä»€ä¹ˆå†…å®¹çš„?\", param=QueryParam(mode=mode)))\n#     print(\"-\"*100, \"\\n\")\n\n# Data models\n\n\nclass QueryRequest(BaseModel):\n    query: str\n    mode: str = \"hybrid\"\n    only_need_context: bool = False\n    only_need_prompt: bool = False\n\n\nclass DataRequest(BaseModel):\n    limit: int = 100\n\n\nclass InsertRequest(BaseModel):\n    text: str\n\n\nclass Response(BaseModel):\n    status: str\n    data: Optional[Any] = None\n    message: Optional[str] = None\n\n\n# API routes\n\nrag = None\n\n\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n    global rag\n    rag = await init()\n    print(\"done!\")\n    yield\n\n\napp = FastAPI(\n    title=\"LightRAG API\", description=\"API for RAG operations\", lifespan=lifespan\n)\n\n\n@app.post(\"/query\", response_model=Response)\nasync def query_endpoint(request: QueryRequest):\n    # try:\n    # loop = asyncio.get_event_loop()\n    if request.mode == \"naive\":\n        top_k = 3\n    else:\n        top_k = 60\n    result = await rag.aquery(\n        request.query,\n        param=QueryParam(\n            mode=request.mode,\n            only_need_context=request.only_need_context,\n            only_need_prompt=request.only_need_prompt,\n            top_k=top_k,\n        ),\n    )\n    return Response(status=\"success\", data=result)\n    # except Exception as e:\n    #     raise HTTPException(status_code=500, detail=str(e))\n\n\n@app.get(\"/data\", response_model=Response)\nasync def query_all_nodes(type: str = Query(\"nodes\"), limit: int = Query(100)):\n    if type == \"nodes\":\n        result = await rag.chunk_entity_relation_graph.get_all_nodes(limit=limit)\n    elif type == \"edges\":\n        result = await rag.chunk_entity_relation_graph.get_all_edges(limit=limit)\n    elif type == \"statistics\":\n        result = await rag.chunk_entity_relation_graph.get_statistics()\n    return Response(status=\"success\", data=result)\n\n\n@app.post(\"/insert\", response_model=Response)\nasync def insert_endpoint(request: InsertRequest):\n    try:\n        loop = asyncio.get_event_loop()\n        await loop.run_in_executor(None, lambda: rag.insert(request.text))\n        return Response(status=\"success\", message=\"Text inserted successfully\")\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n\n\n@app.post(\"/insert_file\", response_model=Response)\nasync def insert_file(file: UploadFile = File(...)):\n    try:\n        file_content = await file.read()\n        # Read file content\n        try:\n            content = file_content.decode(\"utf-8\")\n        except UnicodeDecodeError:\n            # If UTF-8 decoding fails, try other encodings\n            content = file_content.decode(\"gbk\")\n        # Insert file content\n        loop = asyncio.get_event_loop()\n        await loop.run_in_executor(None, lambda: rag.insert(content))\n\n        return Response(\n            status=\"success\",\n            message=f\"File content from {file.filename} inserted successfully\",\n        )\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n\n\n@app.get(\"/health\")\nasync def health_check():\n    return {\"status\": \"healthy\"}\n\n\nif __name__ == \"__main__\":\n    import uvicorn\n\n    uvicorn.run(app, host=\"127.0.0.1\", port=8020)\n\n# Usage example\n# To run the server, use the following command in your terminal:\n# python lightrag_api_openai_compatible_demo.py\n\n# Example requests:\n# 1. Query:\n# curl -X POST \"http://127.0.0.1:8020/query\" -H \"Content-Type: application/json\" -d '{\"query\": \"your query here\", \"mode\": \"hybrid\"}'\n\n# 2. Insert text:\n# curl -X POST \"http://127.0.0.1:8020/insert\" -H \"Content-Type: application/json\" -d '{\"text\": \"your text here\"}'\n\n# 3. Insert file:\n# curl -X POST \"http://127.0.0.1:8020/insert_file\" -H \"Content-Type: application/json\" -d '{\"file_path\": \"path/to/your/file.txt\"}'\n\n# 4. Health check:\n# curl -X GET \"http://127.0.0.1:8020/health\"\n"}
{"type": "source_file", "path": "examples/lightrag_openai_compatible_demo_embedding_cache.py", "content": "import os\nimport asyncio\nfrom lightrag import LightRAG, QueryParam\nfrom lightrag.llm import openai_complete_if_cache, openai_embedding\nfrom lightrag.utils import EmbeddingFunc\nimport numpy as np\n\nWORKING_DIR = \"./dickens\"\n\nif not os.path.exists(WORKING_DIR):\n    os.mkdir(WORKING_DIR)\n\n\nasync def llm_model_func(\n    prompt, system_prompt=None, history_messages=[], keyword_extraction=False, **kwargs\n) -> str:\n    return await openai_complete_if_cache(\n        \"solar-mini\",\n        prompt,\n        system_prompt=system_prompt,\n        history_messages=history_messages,\n        api_key=os.getenv(\"UPSTAGE_API_KEY\"),\n        base_url=\"https://api.upstage.ai/v1/solar\",\n        **kwargs,\n    )\n\n\nasync def embedding_func(texts: list[str]) -> np.ndarray:\n    return await openai_embedding(\n        texts,\n        model=\"solar-embedding-1-large-query\",\n        api_key=os.getenv(\"UPSTAGE_API_KEY\"),\n        base_url=\"https://api.upstage.ai/v1/solar\",\n    )\n\n\nasync def get_embedding_dim():\n    test_text = [\"This is a test sentence.\"]\n    embedding = await embedding_func(test_text)\n    embedding_dim = embedding.shape[1]\n    return embedding_dim\n\n\n# function test\nasync def test_funcs():\n    result = await llm_model_func(\"How are you?\")\n    print(\"llm_model_func: \", result)\n\n    result = await embedding_func([\"How are you?\"])\n    print(\"embedding_func: \", result)\n\n\n# asyncio.run(test_funcs())\n\n\nasync def main():\n    try:\n        embedding_dimension = await get_embedding_dim()\n        print(f\"Detected embedding dimension: {embedding_dimension}\")\n\n        rag = LightRAG(\n            working_dir=WORKING_DIR,\n            embedding_cache_config={\n                \"enabled\": True,\n                \"similarity_threshold\": 0.90,\n            },\n            llm_model_func=llm_model_func,\n            embedding_func=EmbeddingFunc(\n                embedding_dim=embedding_dimension,\n                max_token_size=8192,\n                func=embedding_func,\n            ),\n        )\n\n        with open(\"./book.txt\", \"r\", encoding=\"utf-8\") as f:\n            await rag.ainsert(f.read())\n\n        # Perform naive search\n        print(\n            await rag.aquery(\n                \"What are the top themes in this story?\", param=QueryParam(mode=\"naive\")\n            )\n        )\n\n        # Perform local search\n        print(\n            await rag.aquery(\n                \"What are the top themes in this story?\", param=QueryParam(mode=\"local\")\n            )\n        )\n\n        # Perform global search\n        print(\n            await rag.aquery(\n                \"What are the top themes in this story?\",\n                param=QueryParam(mode=\"global\"),\n            )\n        )\n\n        # Perform hybrid search\n        print(\n            await rag.aquery(\n                \"What are the top themes in this story?\",\n                param=QueryParam(mode=\"hybrid\"),\n            )\n        )\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n"}
{"type": "source_file", "path": "examples/lightrag_ollama_neo4j_milvus_mongo_demo.py", "content": "import os\nfrom lightrag import LightRAG, QueryParam\nfrom lightrag.llm import ollama_model_complete, ollama_embed\nfrom lightrag.utils import EmbeddingFunc\n\n# WorkingDir\nROOT_DIR = os.path.dirname(os.path.abspath(__file__))\nWORKING_DIR = os.path.join(ROOT_DIR, \"myKG\")\nif not os.path.exists(WORKING_DIR):\n    os.mkdir(WORKING_DIR)\nprint(f\"WorkingDir: {WORKING_DIR}\")\n\n# mongo\nos.environ[\"MONGO_URI\"] = \"mongodb://root:root@localhost:27017/\"\nos.environ[\"MONGO_DATABASE\"] = \"LightRAG\"\n\n# neo4j\nBATCH_SIZE_NODES = 500\nBATCH_SIZE_EDGES = 100\nos.environ[\"NEO4J_URI\"] = \"bolt://localhost:7687\"\nos.environ[\"NEO4J_USERNAME\"] = \"neo4j\"\nos.environ[\"NEO4J_PASSWORD\"] = \"neo4j\"\n\n# milvus\nos.environ[\"MILVUS_URI\"] = \"http://localhost:19530\"\nos.environ[\"MILVUS_USER\"] = \"root\"\nos.environ[\"MILVUS_PASSWORD\"] = \"root\"\nos.environ[\"MILVUS_DB_NAME\"] = \"lightrag\"\n\n\nrag = LightRAG(\n    working_dir=WORKING_DIR,\n    llm_model_func=ollama_model_complete,\n    llm_model_name=\"qwen2.5:14b\",\n    llm_model_max_async=4,\n    llm_model_max_token_size=32768,\n    llm_model_kwargs={\"host\": \"http://127.0.0.1:11434\", \"options\": {\"num_ctx\": 32768}},\n    embedding_func=EmbeddingFunc(\n        embedding_dim=1024,\n        max_token_size=8192,\n        func=lambda texts: ollama_embed(\n            texts=texts, embed_model=\"bge-m3:latest\", host=\"http://127.0.0.1:11434\"\n        ),\n    ),\n    kv_storage=\"MongoKVStorage\",\n    graph_storage=\"Neo4JStorage\",\n    vector_storage=\"MilvusVectorDBStorge\",\n)\n\nfile = \"./book.txt\"\nwith open(file, \"r\") as f:\n    rag.insert(f.read())\n\nprint(\n    rag.query(\"What are the top themes in this story?\", param=QueryParam(mode=\"hybrid\"))\n)\n"}
{"type": "source_file", "path": "examples/lightrag_jinaai_demo.py", "content": "import numpy as np\nfrom lightrag import LightRAG, QueryParam\nfrom lightrag.utils import EmbeddingFunc\nfrom lightrag.llm import jina_embedding, openai_complete_if_cache\nimport os\nimport asyncio\n\n\nasync def embedding_func(texts: list[str]) -> np.ndarray:\n    return await jina_embedding(texts, api_key=\"YourJinaAPIKey\")\n\n\nWORKING_DIR = \"./dickens\"\n\nif not os.path.exists(WORKING_DIR):\n    os.mkdir(WORKING_DIR)\n\n\nasync def llm_model_func(\n    prompt, system_prompt=None, history_messages=[], **kwargs\n) -> str:\n    return await openai_complete_if_cache(\n        \"solar-mini\",\n        prompt,\n        system_prompt=system_prompt,\n        history_messages=history_messages,\n        api_key=os.getenv(\"UPSTAGE_API_KEY\"),\n        base_url=\"https://api.upstage.ai/v1/solar\",\n        **kwargs,\n    )\n\n\nrag = LightRAG(\n    working_dir=WORKING_DIR,\n    llm_model_func=llm_model_func,\n    embedding_func=EmbeddingFunc(\n        embedding_dim=1024, max_token_size=8192, func=embedding_func\n    ),\n)\n\n\nasync def lightraginsert(file_path, semaphore):\n    async with semaphore:\n        try:\n            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n                content = f.read()\n        except UnicodeDecodeError:\n            # If UTF-8 decoding fails, try other encodings\n            with open(file_path, \"r\", encoding=\"gbk\") as f:\n                content = f.read()\n        await rag.ainsert(content)\n\n\nasync def process_files(directory, concurrency_limit):\n    semaphore = asyncio.Semaphore(concurrency_limit)\n    tasks = []\n    for root, dirs, files in os.walk(directory):\n        for f in files:\n            file_path = os.path.join(root, f)\n            if f.startswith(\".\"):\n                continue\n            tasks.append(lightraginsert(file_path, semaphore))\n    await asyncio.gather(*tasks)\n\n\nasync def main():\n    try:\n        rag = LightRAG(\n            working_dir=WORKING_DIR,\n            llm_model_func=llm_model_func,\n            embedding_func=EmbeddingFunc(\n                embedding_dim=1024,\n                max_token_size=8192,\n                func=embedding_func,\n            ),\n        )\n\n        asyncio.run(process_files(WORKING_DIR, concurrency_limit=4))\n\n        # Perform naive search\n        print(\n            await rag.aquery(\n                \"What are the top themes in this story?\", param=QueryParam(mode=\"naive\")\n            )\n        )\n\n        # Perform local search\n        print(\n            await rag.aquery(\n                \"What are the top themes in this story?\", param=QueryParam(mode=\"local\")\n            )\n        )\n\n        # Perform global search\n        print(\n            await rag.aquery(\n                \"What are the top themes in this story?\",\n                param=QueryParam(mode=\"global\"),\n            )\n        )\n\n        # Perform hybrid search\n        print(\n            await rag.aquery(\n                \"What are the top themes in this story?\",\n                param=QueryParam(mode=\"hybrid\"),\n            )\n        )\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n"}
{"type": "source_file", "path": "examples/lightrag_openai_compatible_demo.py", "content": "import os\nimport asyncio\nfrom lightrag import LightRAG, QueryParam\nfrom lightrag.llm import openai_complete_if_cache, openai_embedding\nfrom lightrag.utils import EmbeddingFunc\nimport numpy as np\n\nWORKING_DIR = \"./dickens\"\n\nif not os.path.exists(WORKING_DIR):\n    os.mkdir(WORKING_DIR)\n\n\nasync def llm_model_func(\n    prompt, system_prompt=None, history_messages=[], keyword_extraction=False, **kwargs\n) -> str:\n    return await openai_complete_if_cache(\n        \"solar-mini\",\n        prompt,\n        system_prompt=system_prompt,\n        history_messages=history_messages,\n        api_key=os.getenv(\"UPSTAGE_API_KEY\"),\n        base_url=\"https://api.upstage.ai/v1/solar\",\n        **kwargs,\n    )\n\n\nasync def embedding_func(texts: list[str]) -> np.ndarray:\n    return await openai_embedding(\n        texts,\n        model=\"solar-embedding-1-large-query\",\n        api_key=os.getenv(\"UPSTAGE_API_KEY\"),\n        base_url=\"https://api.upstage.ai/v1/solar\",\n    )\n\n\nasync def get_embedding_dim():\n    test_text = [\"This is a test sentence.\"]\n    embedding = await embedding_func(test_text)\n    embedding_dim = embedding.shape[1]\n    return embedding_dim\n\n\n# function test\nasync def test_funcs():\n    result = await llm_model_func(\"How are you?\")\n    print(\"llm_model_func: \", result)\n\n    result = await embedding_func([\"How are you?\"])\n    print(\"embedding_func: \", result)\n\n\n# asyncio.run(test_funcs())\n\n\nasync def main():\n    try:\n        embedding_dimension = await get_embedding_dim()\n        print(f\"Detected embedding dimension: {embedding_dimension}\")\n\n        rag = LightRAG(\n            working_dir=WORKING_DIR,\n            llm_model_func=llm_model_func,\n            embedding_func=EmbeddingFunc(\n                embedding_dim=embedding_dimension,\n                max_token_size=8192,\n                func=embedding_func,\n            ),\n        )\n\n        with open(\"./book.txt\", \"r\", encoding=\"utf-8\") as f:\n            await rag.ainsert(f.read())\n\n        # Perform naive search\n        print(\n            await rag.aquery(\n                \"What are the top themes in this story?\", param=QueryParam(mode=\"naive\")\n            )\n        )\n\n        # Perform local search\n        print(\n            await rag.aquery(\n                \"What are the top themes in this story?\", param=QueryParam(mode=\"local\")\n            )\n        )\n\n        # Perform global search\n        print(\n            await rag.aquery(\n                \"What are the top themes in this story?\",\n                param=QueryParam(mode=\"global\"),\n            )\n        )\n\n        # Perform hybrid search\n        print(\n            await rag.aquery(\n                \"What are the top themes in this story?\",\n                param=QueryParam(mode=\"hybrid\"),\n            )\n        )\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n"}
{"type": "source_file", "path": "examples/lightrag_siliconcloud_demo.py", "content": "import os\nimport asyncio\nfrom lightrag import LightRAG, QueryParam\nfrom lightrag.llm import openai_complete_if_cache, siliconcloud_embedding\nfrom lightrag.utils import EmbeddingFunc\nimport numpy as np\n\nWORKING_DIR = \"./dickens\"\n\nif not os.path.exists(WORKING_DIR):\n    os.mkdir(WORKING_DIR)\n\n\nasync def llm_model_func(\n    prompt, system_prompt=None, history_messages=[], keyword_extraction=False, **kwargs\n) -> str:\n    return await openai_complete_if_cache(\n        \"Qwen/Qwen2.5-7B-Instruct\",\n        prompt,\n        system_prompt=system_prompt,\n        history_messages=history_messages,\n        api_key=os.getenv(\"SILICONFLOW_API_KEY\"),\n        base_url=\"https://api.siliconflow.cn/v1/\",\n        **kwargs,\n    )\n\n\nasync def embedding_func(texts: list[str]) -> np.ndarray:\n    return await siliconcloud_embedding(\n        texts,\n        model=\"netease-youdao/bce-embedding-base_v1\",\n        api_key=os.getenv(\"SILICONFLOW_API_KEY\"),\n        max_token_size=512,\n    )\n\n\n# function test\nasync def test_funcs():\n    result = await llm_model_func(\"How are you?\")\n    print(\"llm_model_func: \", result)\n\n    result = await embedding_func([\"How are you?\"])\n    print(\"embedding_func: \", result)\n\n\nasyncio.run(test_funcs())\n\n\nrag = LightRAG(\n    working_dir=WORKING_DIR,\n    llm_model_func=llm_model_func,\n    embedding_func=EmbeddingFunc(\n        embedding_dim=768, max_token_size=512, func=embedding_func\n    ),\n)\n\n\nwith open(\"./book.txt\") as f:\n    rag.insert(f.read())\n\n# Perform naive search\nprint(\n    rag.query(\"What are the top themes in this story?\", param=QueryParam(mode=\"naive\"))\n)\n\n# Perform local search\nprint(\n    rag.query(\"What are the top themes in this story?\", param=QueryParam(mode=\"local\"))\n)\n\n# Perform global search\nprint(\n    rag.query(\"What are the top themes in this story?\", param=QueryParam(mode=\"global\"))\n)\n\n# Perform hybrid search\nprint(\n    rag.query(\"What are the top themes in this story?\", param=QueryParam(mode=\"hybrid\"))\n)\n"}
{"type": "source_file", "path": "examples/vram_management_demo.py", "content": "import os\nimport time\nfrom lightrag import LightRAG, QueryParam\nfrom lightrag.llm import ollama_model_complete, ollama_embedding\nfrom lightrag.utils import EmbeddingFunc\n\n# Working directory and the directory path for text files\nWORKING_DIR = \"./dickens\"\nTEXT_FILES_DIR = \"/llm/mt\"\n\n# Create the working directory if it doesn't exist\nif not os.path.exists(WORKING_DIR):\n    os.mkdir(WORKING_DIR)\n\n# Initialize LightRAG\nrag = LightRAG(\n    working_dir=WORKING_DIR,\n    llm_model_func=ollama_model_complete,\n    llm_model_name=\"qwen2.5:3b-instruct-max-context\",\n    embedding_func=EmbeddingFunc(\n        embedding_dim=768,\n        max_token_size=8192,\n        func=lambda texts: ollama_embedding(texts, embed_model=\"nomic-embed-text\"),\n    ),\n)\n\n# Read all .txt files from the TEXT_FILES_DIR directory\ntexts = []\nfor filename in os.listdir(TEXT_FILES_DIR):\n    if filename.endswith(\".txt\"):\n        file_path = os.path.join(TEXT_FILES_DIR, filename)\n        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n            texts.append(file.read())\n\n\n# Batch insert texts into LightRAG with a retry mechanism\ndef insert_texts_with_retry(rag, texts, retries=3, delay=5):\n    for _ in range(retries):\n        try:\n            rag.insert(texts)\n            return\n        except Exception as e:\n            print(\n                f\"Error occurred during insertion: {e}. Retrying in {delay} seconds...\"\n            )\n            time.sleep(delay)\n    raise RuntimeError(\"Failed to insert texts after multiple retries.\")\n\n\ninsert_texts_with_retry(rag, texts)\n\n# Perform different types of queries and handle potential errors\ntry:\n    print(\n        rag.query(\n            \"What are the top themes in this story?\", param=QueryParam(mode=\"naive\")\n        )\n    )\nexcept Exception as e:\n    print(f\"Error performing naive search: {e}\")\n\ntry:\n    print(\n        rag.query(\n            \"What are the top themes in this story?\", param=QueryParam(mode=\"local\")\n        )\n    )\nexcept Exception as e:\n    print(f\"Error performing local search: {e}\")\n\ntry:\n    print(\n        rag.query(\n            \"What are the top themes in this story?\", param=QueryParam(mode=\"global\")\n        )\n    )\nexcept Exception as e:\n    print(f\"Error performing global search: {e}\")\n\ntry:\n    print(\n        rag.query(\n            \"What are the top themes in this story?\", param=QueryParam(mode=\"hybrid\")\n        )\n    )\nexcept Exception as e:\n    print(f\"Error performing hybrid search: {e}\")\n\n\n# Function to clear VRAM resources\ndef clear_vram():\n    os.system(\"sudo nvidia-smi --gpu-reset\")\n\n\n# Regularly clear VRAM to prevent overflow\nclear_vram_interval = 3600  # Clear once every hour\nstart_time = time.time()\n\nwhile True:\n    current_time = time.time()\n    if current_time - start_time > clear_vram_interval:\n        clear_vram()\n        start_time = current_time\n    time.sleep(60)  # Check the time every minute\n"}
{"type": "source_file", "path": "examples/lightrag_tidb_demo.py", "content": "import asyncio\nimport os\n\nimport numpy as np\n\nfrom lightrag import LightRAG, QueryParam\nfrom lightrag.kg.tidb_impl import TiDB\nfrom lightrag.llm import siliconcloud_embedding, openai_complete_if_cache\nfrom lightrag.utils import EmbeddingFunc\n\nWORKING_DIR = \"./dickens\"\n\n# We use SiliconCloud API to call LLM on Oracle Cloud\n# More docs here https://docs.siliconflow.cn/introduction\nBASE_URL = \"https://api.siliconflow.cn/v1/\"\nAPIKEY = \"\"\nCHATMODEL = \"\"\nEMBEDMODEL = \"\"\n\nTIDB_HOST = \"\"\nTIDB_PORT = \"\"\nTIDB_USER = \"\"\nTIDB_PASSWORD = \"\"\nTIDB_DATABASE = \"lightrag\"\n\nif not os.path.exists(WORKING_DIR):\n    os.mkdir(WORKING_DIR)\n\n\nasync def llm_model_func(\n    prompt, system_prompt=None, history_messages=[], keyword_extraction=False, **kwargs\n) -> str:\n    return await openai_complete_if_cache(\n        CHATMODEL,\n        prompt,\n        system_prompt=system_prompt,\n        history_messages=history_messages,\n        api_key=APIKEY,\n        base_url=BASE_URL,\n        **kwargs,\n    )\n\n\nasync def embedding_func(texts: list[str]) -> np.ndarray:\n    return await siliconcloud_embedding(\n        texts,\n        # model=EMBEDMODEL,\n        api_key=APIKEY,\n    )\n\n\nasync def get_embedding_dim():\n    test_text = [\"This is a test sentence.\"]\n    embedding = await embedding_func(test_text)\n    embedding_dim = embedding.shape[1]\n    return embedding_dim\n\n\nasync def main():\n    try:\n        # Detect embedding dimension\n        embedding_dimension = await get_embedding_dim()\n        print(f\"Detected embedding dimension: {embedding_dimension}\")\n\n        # Create TiDB DB connection\n        tidb = TiDB(\n            config={\n                \"host\": TIDB_HOST,\n                \"port\": TIDB_PORT,\n                \"user\": TIDB_USER,\n                \"password\": TIDB_PASSWORD,\n                \"database\": TIDB_DATABASE,\n                \"workspace\": \"company\",  # specify which docs you want to store and query\n            }\n        )\n\n        # Check if TiDB DB tables exist, if not, tables will be created\n        await tidb.check_tables()\n\n        # Initialize LightRAG\n        # We use TiDB DB as the KV/vector\n        # You can add `addon_params={\"example_number\": 1, \"language\": \"Simplfied Chinese\"}` to control the prompt\n        rag = LightRAG(\n            enable_llm_cache=False,\n            working_dir=WORKING_DIR,\n            chunk_token_size=512,\n            llm_model_func=llm_model_func,\n            embedding_func=EmbeddingFunc(\n                embedding_dim=embedding_dimension,\n                max_token_size=512,\n                func=embedding_func,\n            ),\n            kv_storage=\"TiDBKVStorage\",\n            vector_storage=\"TiDBVectorDBStorage\",\n            graph_storage=\"TiDBGraphStorage\",\n        )\n\n        if rag.llm_response_cache:\n            rag.llm_response_cache.db = tidb\n        rag.full_docs.db = tidb\n        rag.text_chunks.db = tidb\n        rag.entities_vdb.db = tidb\n        rag.relationships_vdb.db = tidb\n        rag.chunks_vdb.db = tidb\n        rag.chunk_entity_relation_graph.db = tidb\n\n        # Extract and Insert into LightRAG storage\n        with open(\"./dickens/demo.txt\", \"r\", encoding=\"utf-8\") as f:\n            await rag.ainsert(f.read())\n\n        # Perform search in different modes\n        modes = [\"naive\", \"local\", \"global\", \"hybrid\"]\n        for mode in modes:\n            print(\"=\" * 20, mode, \"=\" * 20)\n            print(\n                await rag.aquery(\n                    \"What are the top themes in this story?\",\n                    param=QueryParam(mode=mode),\n                )\n            )\n            print(\"-\" * 100, \"\\n\")\n\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n"}
{"type": "source_file", "path": "lightrag/api/lollms_lightrag_server.py", "content": "from fastapi import FastAPI, HTTPException, File, UploadFile, Form\nfrom pydantic import BaseModel\nimport logging\nimport argparse\nfrom lightrag import LightRAG, QueryParam\nfrom lightrag.llm import lollms_model_complete, lollms_embed\nfrom lightrag.utils import EmbeddingFunc\nfrom typing import Optional, List\nfrom enum import Enum\nfrom pathlib import Path\nimport shutil\nimport aiofiles\nfrom ascii_colors import trace_exception\nimport os\n\nfrom fastapi import Depends, Security\nfrom fastapi.security import APIKeyHeader\nfrom fastapi.middleware.cors import CORSMiddleware\n\nfrom starlette.status import HTTP_403_FORBIDDEN\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(\n        description=\"LightRAG FastAPI Server with separate working and input directories\"\n    )\n\n    # Server configuration\n    parser.add_argument(\n        \"--host\", default=\"0.0.0.0\", help=\"Server host (default: 0.0.0.0)\"\n    )\n    parser.add_argument(\n        \"--port\", type=int, default=9621, help=\"Server port (default: 9621)\"\n    )\n\n    # Directory configuration\n    parser.add_argument(\n        \"--working-dir\",\n        default=\"./rag_storage\",\n        help=\"Working directory for RAG storage (default: ./rag_storage)\",\n    )\n    parser.add_argument(\n        \"--input-dir\",\n        default=\"./inputs\",\n        help=\"Directory containing input documents (default: ./inputs)\",\n    )\n\n    # Model configuration\n    parser.add_argument(\n        \"--model\",\n        default=\"mistral-nemo:latest\",\n        help=\"LLM model name (default: mistral-nemo:latest)\",\n    )\n    parser.add_argument(\n        \"--embedding-model\",\n        default=\"bge-m3:latest\",\n        help=\"Embedding model name (default: bge-m3:latest)\",\n    )\n    parser.add_argument(\n        \"--lollms-host\",\n        default=\"http://localhost:9600\",\n        help=\"lollms host URL (default: http://localhost:9600)\",\n    )\n\n    # RAG configuration\n    parser.add_argument(\n        \"--max-async\", type=int, default=4, help=\"Maximum async operations (default: 4)\"\n    )\n    parser.add_argument(\n        \"--max-tokens\",\n        type=int,\n        default=32768,\n        help=\"Maximum token size (default: 32768)\",\n    )\n    parser.add_argument(\n        \"--embedding-dim\",\n        type=int,\n        default=1024,\n        help=\"Embedding dimensions (default: 1024)\",\n    )\n    parser.add_argument(\n        \"--max-embed-tokens\",\n        type=int,\n        default=8192,\n        help=\"Maximum embedding token size (default: 8192)\",\n    )\n\n    # Logging configuration\n    parser.add_argument(\n        \"--log-level\",\n        default=\"INFO\",\n        choices=[\"DEBUG\", \"INFO\", \"WARNING\", \"ERROR\", \"CRITICAL\"],\n        help=\"Logging level (default: INFO)\",\n    )\n\n    parser.add_argument(\n        \"--key\",\n        type=str,\n        help=\"API key for authentication. This protects lightrag server against unauthorized access\",\n        default=None,\n    )\n\n    return parser.parse_args()\n\n\nclass DocumentManager:\n    \"\"\"Handles document operations and tracking\"\"\"\n\n    def __init__(self, input_dir: str, supported_extensions: tuple = (\".txt\", \".md\")):\n        self.input_dir = Path(input_dir)\n        self.supported_extensions = supported_extensions\n        self.indexed_files = set()\n\n        # Create input directory if it doesn't exist\n        self.input_dir.mkdir(parents=True, exist_ok=True)\n\n    def scan_directory(self) -> List[Path]:\n        \"\"\"Scan input directory for new files\"\"\"\n        new_files = []\n        for ext in self.supported_extensions:\n            for file_path in self.input_dir.rglob(f\"*{ext}\"):\n                if file_path not in self.indexed_files:\n                    new_files.append(file_path)\n        return new_files\n\n    def mark_as_indexed(self, file_path: Path):\n        \"\"\"Mark a file as indexed\"\"\"\n        self.indexed_files.add(file_path)\n\n    def is_supported_file(self, filename: str) -> bool:\n        \"\"\"Check if file type is supported\"\"\"\n        return any(filename.lower().endswith(ext) for ext in self.supported_extensions)\n\n\n# Pydantic models\nclass SearchMode(str, Enum):\n    naive = \"naive\"\n    local = \"local\"\n    global_ = \"global\"\n    hybrid = \"hybrid\"\n\n\nclass QueryRequest(BaseModel):\n    query: str\n    mode: SearchMode = SearchMode.hybrid\n    stream: bool = False\n    only_need_context: bool = False\n\n\nclass QueryResponse(BaseModel):\n    response: str\n\n\nclass InsertTextRequest(BaseModel):\n    text: str\n    description: Optional[str] = None\n\n\nclass InsertResponse(BaseModel):\n    status: str\n    message: str\n    document_count: int\n\n\ndef get_api_key_dependency(api_key: Optional[str]):\n    if not api_key:\n        # If no API key is configured, return a dummy dependency that always succeeds\n        async def no_auth():\n            return None\n\n        return no_auth\n\n    # If API key is configured, use proper authentication\n    api_key_header = APIKeyHeader(name=\"X-API-Key\", auto_error=False)\n\n    async def api_key_auth(api_key_header_value: str | None = Security(api_key_header)):\n        if not api_key_header_value:\n            raise HTTPException(\n                status_code=HTTP_403_FORBIDDEN, detail=\"API Key required\"\n            )\n        if api_key_header_value != api_key:\n            raise HTTPException(\n                status_code=HTTP_403_FORBIDDEN, detail=\"Invalid API Key\"\n            )\n        return api_key_header_value\n\n    return api_key_auth\n\n\ndef create_app(args):\n    # Setup logging\n    logging.basicConfig(\n        format=\"%(levelname)s:%(message)s\", level=getattr(logging, args.log_level)\n    )\n\n    # Check if API key is provided either through env var or args\n    api_key = os.getenv(\"LIGHTRAG_API_KEY\") or args.key\n\n    # Initialize FastAPI\n    app = FastAPI(\n        title=\"LightRAG API\",\n        description=\"API for querying text using LightRAG with separate storage and input directories\"\n        + \"(With authentication)\"\n        if api_key\n        else \"\",\n        version=\"1.0.0\",\n        openapi_tags=[{\"name\": \"api\"}],\n    )\n\n    # Add CORS middleware\n    app.add_middleware(\n        CORSMiddleware,\n        allow_origins=[\"*\"],\n        allow_credentials=True,\n        allow_methods=[\"*\"],\n        allow_headers=[\"*\"],\n    )\n\n    # Create the optional API key dependency\n    optional_api_key = get_api_key_dependency(api_key)\n\n    # Create working directory if it doesn't exist\n    Path(args.working_dir).mkdir(parents=True, exist_ok=True)\n\n    # Initialize document manager\n    doc_manager = DocumentManager(args.input_dir)\n\n    # Initialize RAG\n    rag = LightRAG(\n        working_dir=args.working_dir,\n        llm_model_func=lollms_model_complete,\n        llm_model_name=args.model,\n        llm_model_max_async=args.max_async,\n        llm_model_max_token_size=args.max_tokens,\n        llm_model_kwargs={\n            \"host\": args.lollms_host,\n            \"options\": {\"num_ctx\": args.max_tokens},\n        },\n        embedding_func=EmbeddingFunc(\n            embedding_dim=args.embedding_dim,\n            max_token_size=args.max_embed_tokens,\n            func=lambda texts: lollms_embed(\n                texts, embed_model=args.embedding_model, host=args.lollms_host\n            ),\n        ),\n    )\n\n    @app.on_event(\"startup\")\n    async def startup_event():\n        \"\"\"Index all files in input directory during startup\"\"\"\n        try:\n            new_files = doc_manager.scan_directory()\n            for file_path in new_files:\n                try:\n                    # Use async file reading\n                    async with aiofiles.open(file_path, \"r\", encoding=\"utf-8\") as f:\n                        content = await f.read()\n                        # Use the async version of insert directly\n                        await rag.ainsert(content)\n                        doc_manager.mark_as_indexed(file_path)\n                        logging.info(f\"Indexed file: {file_path}\")\n                except Exception as e:\n                    trace_exception(e)\n                    logging.error(f\"Error indexing file {file_path}: {str(e)}\")\n\n            logging.info(f\"Indexed {len(new_files)} documents from {args.input_dir}\")\n\n        except Exception as e:\n            logging.error(f\"Error during startup indexing: {str(e)}\")\n\n    @app.post(\"/documents/scan\", dependencies=[Depends(optional_api_key)])\n    async def scan_for_new_documents():\n        \"\"\"Manually trigger scanning for new documents\"\"\"\n        try:\n            new_files = doc_manager.scan_directory()\n            indexed_count = 0\n\n            for file_path in new_files:\n                try:\n                    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n                        content = f.read()\n                        await rag.ainsert(content)\n                        doc_manager.mark_as_indexed(file_path)\n                        indexed_count += 1\n                except Exception as e:\n                    logging.error(f\"Error indexing file {file_path}: {str(e)}\")\n\n            return {\n                \"status\": \"success\",\n                \"indexed_count\": indexed_count,\n                \"total_documents\": len(doc_manager.indexed_files),\n            }\n        except Exception as e:\n            raise HTTPException(status_code=500, detail=str(e))\n\n    @app.post(\"/documents/upload\", dependencies=[Depends(optional_api_key)])\n    async def upload_to_input_dir(file: UploadFile = File(...)):\n        \"\"\"Upload a file to the input directory\"\"\"\n        try:\n            if not doc_manager.is_supported_file(file.filename):\n                raise HTTPException(\n                    status_code=400,\n                    detail=f\"Unsupported file type. Supported types: {doc_manager.supported_extensions}\",\n                )\n\n            file_path = doc_manager.input_dir / file.filename\n            with open(file_path, \"wb\") as buffer:\n                shutil.copyfileobj(file.file, buffer)\n\n            # Immediately index the uploaded file\n            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n                content = f.read()\n                await rag.ainsert(content)\n                doc_manager.mark_as_indexed(file_path)\n\n            return {\n                \"status\": \"success\",\n                \"message\": f\"File uploaded and indexed: {file.filename}\",\n                \"total_documents\": len(doc_manager.indexed_files),\n            }\n        except Exception as e:\n            raise HTTPException(status_code=500, detail=str(e))\n\n    @app.post(\n        \"/query\", response_model=QueryResponse, dependencies=[Depends(optional_api_key)]\n    )\n    async def query_text(request: QueryRequest):\n        try:\n            response = await rag.aquery(\n                request.query,\n                param=QueryParam(\n                    mode=request.mode,\n                    stream=request.stream,\n                    only_need_context=request.only_need_context,\n                ),\n            )\n\n            if request.stream:\n                result = \"\"\n                async for chunk in response:\n                    result += chunk\n                return QueryResponse(response=result)\n            else:\n                return QueryResponse(response=response)\n        except Exception as e:\n            raise HTTPException(status_code=500, detail=str(e))\n\n    @app.post(\"/query/stream\", dependencies=[Depends(optional_api_key)])\n    async def query_text_stream(request: QueryRequest):\n        try:\n            response = rag.query(\n                request.query,\n                param=QueryParam(\n                    mode=request.mode,\n                    stream=True,\n                    only_need_context=request.only_need_context,\n                ),\n            )\n\n            async def stream_generator():\n                async for chunk in response:\n                    yield chunk\n\n            return stream_generator()\n        except Exception as e:\n            raise HTTPException(status_code=500, detail=str(e))\n\n    @app.post(\n        \"/documents/text\",\n        response_model=InsertResponse,\n        dependencies=[Depends(optional_api_key)],\n    )\n    async def insert_text(request: InsertTextRequest):\n        try:\n            rag.insert(request.text)\n            return InsertResponse(\n                status=\"success\",\n                message=\"Text successfully inserted\",\n                document_count=len(rag),\n            )\n        except Exception as e:\n            raise HTTPException(status_code=500, detail=str(e))\n\n    @app.post(\n        \"/documents/file\",\n        response_model=InsertResponse,\n        dependencies=[Depends(optional_api_key)],\n    )\n    async def insert_file(file: UploadFile = File(...), description: str = Form(None)):\n        try:\n            content = await file.read()\n\n            if file.filename.endswith((\".txt\", \".md\")):\n                text = content.decode(\"utf-8\")\n                await rag.ainsert(text)\n            else:\n                raise HTTPException(\n                    status_code=400,\n                    detail=\"Unsupported file type. Only .txt and .md files are supported\",\n                )\n\n            return InsertResponse(\n                status=\"success\",\n                message=f\"File '{file.filename}' successfully inserted\",\n                document_count=1,\n            )\n        except UnicodeDecodeError:\n            raise HTTPException(status_code=400, detail=\"File encoding not supported\")\n        except Exception as e:\n            raise HTTPException(status_code=500, detail=str(e))\n\n    @app.post(\n        \"/documents/batch\",\n        response_model=InsertResponse,\n        dependencies=[Depends(optional_api_key)],\n    )\n    async def insert_batch(files: List[UploadFile] = File(...)):\n        try:\n            inserted_count = 0\n            failed_files = []\n\n            for file in files:\n                try:\n                    content = await file.read()\n                    if file.filename.endswith((\".txt\", \".md\")):\n                        text = content.decode(\"utf-8\")\n                        await rag.ainsert(text)\n                        inserted_count += 1\n                    else:\n                        failed_files.append(f\"{file.filename} (unsupported type)\")\n                except Exception as e:\n                    failed_files.append(f\"{file.filename} ({str(e)})\")\n\n            status_message = f\"Successfully inserted {inserted_count} documents\"\n            if failed_files:\n                status_message += f\". Failed files: {', '.join(failed_files)}\"\n\n            return InsertResponse(\n                status=\"success\" if inserted_count > 0 else \"partial_success\",\n                message=status_message,\n                document_count=len(files),\n            )\n        except Exception as e:\n            raise HTTPException(status_code=500, detail=str(e))\n\n    @app.delete(\n        \"/documents\",\n        response_model=InsertResponse,\n        dependencies=[Depends(optional_api_key)],\n    )\n    async def clear_documents():\n        try:\n            rag.text_chunks = []\n            rag.entities_vdb = None\n            rag.relationships_vdb = None\n            return InsertResponse(\n                status=\"success\",\n                message=\"All documents cleared successfully\",\n                document_count=0,\n            )\n        except Exception as e:\n            raise HTTPException(status_code=500, detail=str(e))\n\n    @app.get(\"/health\", dependencies=[Depends(optional_api_key)])\n    async def get_status():\n        \"\"\"Get current system status\"\"\"\n        return {\n            \"status\": \"healthy\",\n            \"working_directory\": str(args.working_dir),\n            \"input_directory\": str(args.input_dir),\n            \"indexed_files\": len(doc_manager.indexed_files),\n            \"configuration\": {\n                \"model\": args.model,\n                \"embedding_model\": args.embedding_model,\n                \"max_tokens\": args.max_tokens,\n                \"lollms_host\": args.lollms_host,\n            },\n        }\n\n    return app\n\n\ndef main():\n    args = parse_args()\n    import uvicorn\n\n    app = create_app(args)\n    uvicorn.run(app, host=args.host, port=args.port)\n\n\nif __name__ == \"__main__\":\n    main()\n"}
{"type": "source_file", "path": "examples/lightrag_ollama_gremlin_demo.py", "content": "import asyncio\nimport inspect\nimport os\n\n# Uncomment these lines below to filter out somewhat verbose INFO level\n# logging prints (the default loglevel is INFO).\n# This has to go before the lightrag imports to work,\n# which triggers linting errors, so we keep it commented out:\n# import logging\n# logging.basicConfig(format=\"%(levelname)s:%(message)s\", level=logging.WARN)\n\nfrom lightrag import LightRAG, QueryParam\nfrom lightrag.llm import ollama_embedding, ollama_model_complete\nfrom lightrag.utils import EmbeddingFunc\n\nWORKING_DIR = \"./dickens_gremlin\"\n\nif not os.path.exists(WORKING_DIR):\n    os.mkdir(WORKING_DIR)\n\n# Gremlin\nos.environ[\"GREMLIN_HOST\"] = \"localhost\"\nos.environ[\"GREMLIN_PORT\"] = \"8182\"\nos.environ[\"GREMLIN_GRAPH\"] = \"dickens\"\n\n# Creating a non-default source requires manual\n# configuration and a restart on the server: use the dafault \"g\"\nos.environ[\"GREMLIN_TRAVERSE_SOURCE\"] = \"g\"\n\n# No authorization by default on docker tinkerpop/gremlin-server\nos.environ[\"GREMLIN_USER\"] = \"\"\nos.environ[\"GREMLIN_PASSWORD\"] = \"\"\n\nrag = LightRAG(\n    working_dir=WORKING_DIR,\n    llm_model_func=ollama_model_complete,\n    llm_model_name=\"llama3.1:8b\",\n    llm_model_max_async=4,\n    llm_model_max_token_size=32768,\n    llm_model_kwargs={\"host\": \"http://localhost:11434\", \"options\": {\"num_ctx\": 32768}},\n    embedding_func=EmbeddingFunc(\n        embedding_dim=768,\n        max_token_size=8192,\n        func=lambda texts: ollama_embedding(\n            texts, embed_model=\"nomic-embed-text\", host=\"http://localhost:11434\"\n        ),\n    ),\n    graph_storage=\"GremlinStorage\",\n)\n\nwith open(\"./book.txt\", \"r\", encoding=\"utf-8\") as f:\n    rag.insert(f.read())\n\n# Perform naive search\nprint(\n    rag.query(\"What are the top themes in this story?\", param=QueryParam(mode=\"naive\"))\n)\n\n# Perform local search\nprint(\n    rag.query(\"What are the top themes in this story?\", param=QueryParam(mode=\"local\"))\n)\n\n# Perform global search\nprint(\n    rag.query(\"What are the top themes in this story?\", param=QueryParam(mode=\"global\"))\n)\n\n# Perform hybrid search\nprint(\n    rag.query(\"What are the top themes in this story?\", param=QueryParam(mode=\"hybrid\"))\n)\n\n# stream response\nresp = rag.query(\n    \"What are the top themes in this story?\",\n    param=QueryParam(mode=\"hybrid\", stream=True),\n)\n\n\nasync def print_stream(stream):\n    async for chunk in stream:\n        print(chunk, end=\"\", flush=True)\n\n\nif inspect.isasyncgen(resp):\n    asyncio.run(print_stream(resp))\nelse:\n    print(resp)\n"}
{"type": "source_file", "path": "lightrag/__init__.py", "content": "from .lightrag import LightRAG as LightRAG, QueryParam as QueryParam\n\n__version__ = \"1.1.0\"\n__author__ = \"Zirui Guo\"\n__url__ = \"https://github.com/HKUDS/LightRAG\"\n"}
{"type": "source_file", "path": "examples/lightrag_oracle_demo.py", "content": "import sys\nimport os\nfrom pathlib import Path\nimport asyncio\nfrom lightrag import LightRAG, QueryParam\nfrom lightrag.llm import openai_complete_if_cache, openai_embedding\nfrom lightrag.utils import EmbeddingFunc\nimport numpy as np\nfrom lightrag.kg.oracle_impl import OracleDB\n\nprint(os.getcwd())\nscript_directory = Path(__file__).resolve().parent.parent\nsys.path.append(os.path.abspath(script_directory))\n\nWORKING_DIR = \"./dickens\"\n\n# We use OpenAI compatible API to call LLM on Oracle Cloud\n# More docs here https://github.com/jin38324/OCI_GenAI_access_gateway\nBASE_URL = \"http://xxx.xxx.xxx.xxx:8088/v1/\"\nAPIKEY = \"ocigenerativeai\"\nCHATMODEL = \"cohere.command-r-plus\"\nEMBEDMODEL = \"cohere.embed-multilingual-v3.0\"\n\n\nif not os.path.exists(WORKING_DIR):\n    os.mkdir(WORKING_DIR)\n\n\nasync def llm_model_func(\n    prompt, system_prompt=None, history_messages=[], keyword_extraction=False, **kwargs\n) -> str:\n    return await openai_complete_if_cache(\n        CHATMODEL,\n        prompt,\n        system_prompt=system_prompt,\n        history_messages=history_messages,\n        api_key=APIKEY,\n        base_url=BASE_URL,\n        **kwargs,\n    )\n\n\nasync def embedding_func(texts: list[str]) -> np.ndarray:\n    return await openai_embedding(\n        texts,\n        model=EMBEDMODEL,\n        api_key=APIKEY,\n        base_url=BASE_URL,\n    )\n\n\nasync def get_embedding_dim():\n    test_text = [\"This is a test sentence.\"]\n    embedding = await embedding_func(test_text)\n    embedding_dim = embedding.shape[1]\n    return embedding_dim\n\n\nasync def main():\n    try:\n        # Detect embedding dimension\n        embedding_dimension = await get_embedding_dim()\n        print(f\"Detected embedding dimension: {embedding_dimension}\")\n\n        # Create Oracle DB connection\n        # The `config` parameter is the connection configuration of Oracle DB\n        # More docs here https://python-oracledb.readthedocs.io/en/latest/user_guide/connection_handling.html\n        # We storage data in unified tables, so we need to set a `workspace` parameter to specify which docs we want to store and query\n        # Below is an example of how to connect to Oracle Autonomous Database on Oracle Cloud\n        oracle_db = OracleDB(\n            config={\n                \"user\": \"username\",\n                \"password\": \"xxxxxxxxx\",\n                \"dsn\": \"xxxxxxx_medium\",\n                \"config_dir\": \"dir/path/to/oracle/config\",\n                \"wallet_location\": \"dir/path/to/oracle/wallet\",\n                \"wallet_password\": \"xxxxxxxxx\",\n                \"workspace\": \"company\",  # specify which docs you want to store and query\n            }\n        )\n\n        # Check if Oracle DB tables exist, if not, tables will be created\n        await oracle_db.check_tables()\n\n        # Initialize LightRAG\n        # We use Oracle DB as the KV/vector/graph storage\n        # You can add `addon_params={\"example_number\": 1, \"language\": \"Simplfied Chinese\"}` to control the prompt\n        rag = LightRAG(\n            enable_llm_cache=False,\n            working_dir=WORKING_DIR,\n            chunk_token_size=512,\n            llm_model_func=llm_model_func,\n            embedding_func=EmbeddingFunc(\n                embedding_dim=embedding_dimension,\n                max_token_size=512,\n                func=embedding_func,\n            ),\n            graph_storage=\"OracleGraphStorage\",\n            kv_storage=\"OracleKVStorage\",\n            vector_storage=\"OracleVectorDBStorage\",\n        )\n\n        # Setthe KV/vector/graph storage's `db` property, so all operation will use same connection pool\n        rag.graph_storage_cls.db = oracle_db\n        rag.key_string_value_json_storage_cls.db = oracle_db\n        rag.vector_db_storage_cls.db = oracle_db\n        # add embedding_func for graph database, it's deleted in commit 5661d76860436f7bf5aef2e50d9ee4a59660146c\n        rag.chunk_entity_relation_graph.embedding_func = rag.embedding_func\n\n        # Extract and Insert into LightRAG storage\n        with open(\"./dickens/demo.txt\", \"r\", encoding=\"utf-8\") as f:\n            await rag.ainsert(f.read())\n\n        # Perform search in different modes\n        modes = [\"naive\", \"local\", \"global\", \"hybrid\"]\n        for mode in modes:\n            print(\"=\" * 20, mode, \"=\" * 20)\n            print(\n                await rag.aquery(\n                    \"What are the top themes in this story?\",\n                    param=QueryParam(mode=mode),\n                )\n            )\n            print(\"-\" * 100, \"\\n\")\n\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n"}
{"type": "source_file", "path": "get_all_edges_nx.py", "content": "import networkx as nx\n\nG = nx.read_graphml(\"./dickensTestEmbedcall/graph_chunk_entity_relation.graphml\")\n\n\ndef get_all_edges_and_nodes(G):\n    # Get all edges and their properties\n    edges_with_properties = []\n    for u, v, data in G.edges(data=True):\n        edges_with_properties.append(\n            {\n                \"start\": u,\n                \"end\": v,\n                \"label\": data.get(\n                    \"label\", \"\"\n                ),  # Assuming 'label' is used for edge type\n                \"properties\": data,\n                \"start_node_properties\": G.nodes[u],\n                \"end_node_properties\": G.nodes[v],\n            }\n        )\n\n    return edges_with_properties\n\n\n# Example usage\nif __name__ == \"__main__\":\n    # Assume G is your NetworkX graph loaded from Neo4j\n\n    all_edges = get_all_edges_and_nodes(G)\n\n    # Print all edges and node properties\n    for edge in all_edges:\n        print(f\"Edge Label: {edge['label']}\")\n        print(f\"Edge Properties: {edge['properties']}\")\n        print(f\"Start Node: {edge['start']}\")\n        print(f\"Start Node Properties: {edge['start_node_properties']}\")\n        print(f\"End Node: {edge['end']}\")\n        print(f\"End Node Properties: {edge['end_node_properties']}\")\n        print(\"---\")\n"}
{"type": "source_file", "path": "examples/lightrag_zhipu_demo.py", "content": "import os\nimport logging\n\n\nfrom lightrag import LightRAG, QueryParam\nfrom lightrag.llm import zhipu_complete, zhipu_embedding\nfrom lightrag.utils import EmbeddingFunc\n\nWORKING_DIR = \"./dickens\"\n\nlogging.basicConfig(format=\"%(levelname)s:%(message)s\", level=logging.INFO)\n\nif not os.path.exists(WORKING_DIR):\n    os.mkdir(WORKING_DIR)\n\napi_key = os.environ.get(\"ZHIPUAI_API_KEY\")\nif api_key is None:\n    raise Exception(\"Please set ZHIPU_API_KEY in your environment\")\n\n\nrag = LightRAG(\n    working_dir=WORKING_DIR,\n    llm_model_func=zhipu_complete,\n    llm_model_name=\"glm-4-flashx\",  # Using the most cost/performance balance model, but you can change it here.\n    llm_model_max_async=4,\n    llm_model_max_token_size=32768,\n    embedding_func=EmbeddingFunc(\n        embedding_dim=2048,  # Zhipu embedding-3 dimension\n        max_token_size=8192,\n        func=lambda texts: zhipu_embedding(texts),\n    ),\n)\n\nwith open(\"./book.txt\", \"r\", encoding=\"utf-8\") as f:\n    rag.insert(f.read())\n\n# Perform naive search\nprint(\n    rag.query(\"What are the top themes in this story?\", param=QueryParam(mode=\"naive\"))\n)\n\n# Perform local search\nprint(\n    rag.query(\"What are the top themes in this story?\", param=QueryParam(mode=\"local\"))\n)\n\n# Perform global search\nprint(\n    rag.query(\"What are the top themes in this story?\", param=QueryParam(mode=\"global\"))\n)\n\n# Perform hybrid search\nprint(\n    rag.query(\"What are the top themes in this story?\", param=QueryParam(mode=\"hybrid\"))\n)\n"}
{"type": "source_file", "path": "examples/lightrag_zhipu_postgres_demo.py", "content": "import asyncio\nimport logging\nimport os\nimport time\nfrom dotenv import load_dotenv\n\nfrom lightrag import LightRAG, QueryParam\nfrom lightrag.kg.postgres_impl import PostgreSQLDB\nfrom lightrag.llm import ollama_embedding, zhipu_complete\nfrom lightrag.utils import EmbeddingFunc\n\nload_dotenv()\nROOT_DIR = os.environ.get(\"ROOT_DIR\")\nWORKING_DIR = f\"{ROOT_DIR}/dickens-pg\"\n\nlogging.basicConfig(format=\"%(levelname)s:%(message)s\", level=logging.INFO)\n\nif not os.path.exists(WORKING_DIR):\n    os.mkdir(WORKING_DIR)\n\n# AGE\nos.environ[\"AGE_GRAPH_NAME\"] = \"dickens\"\n\npostgres_db = PostgreSQLDB(\n    config={\n        \"host\": \"localhost\",\n        \"port\": 15432,\n        \"user\": \"rag\",\n        \"password\": \"rag\",\n        \"database\": \"rag\",\n    }\n)\n\n\nasync def main():\n    await postgres_db.initdb()\n    # Check if PostgreSQL DB tables exist, if not, tables will be created\n    await postgres_db.check_tables()\n\n    rag = LightRAG(\n        working_dir=WORKING_DIR,\n        llm_model_func=zhipu_complete,\n        llm_model_name=\"glm-4-flashx\",\n        llm_model_max_async=4,\n        llm_model_max_token_size=32768,\n        enable_llm_cache_for_entity_extract=True,\n        embedding_func=EmbeddingFunc(\n            embedding_dim=768,\n            max_token_size=8192,\n            func=lambda texts: ollama_embedding(\n                texts, embed_model=\"nomic-embed-text\", host=\"http://localhost:11434\"\n            ),\n        ),\n        kv_storage=\"PGKVStorage\",\n        doc_status_storage=\"PGDocStatusStorage\",\n        graph_storage=\"PGGraphStorage\",\n        vector_storage=\"PGVectorStorage\",\n    )\n    # Set the KV/vector/graph storage's `db` property, so all operation will use same connection pool\n    rag.doc_status.db = postgres_db\n    rag.full_docs.db = postgres_db\n    rag.text_chunks.db = postgres_db\n    rag.llm_response_cache.db = postgres_db\n    rag.key_string_value_json_storage_cls.db = postgres_db\n    rag.chunks_vdb.db = postgres_db\n    rag.relationships_vdb.db = postgres_db\n    rag.entities_vdb.db = postgres_db\n    rag.graph_storage_cls.db = postgres_db\n    rag.chunk_entity_relation_graph.db = postgres_db\n    # add embedding_func for graph database, it's deleted in commit 5661d76860436f7bf5aef2e50d9ee4a59660146c\n    rag.chunk_entity_relation_graph.embedding_func = rag.embedding_func\n\n    with open(f\"{ROOT_DIR}/book.txt\", \"r\", encoding=\"utf-8\") as f:\n        await rag.ainsert(f.read())\n\n    print(\"==== Trying to test the rag queries ====\")\n    print(\"**** Start Naive Query ****\")\n    start_time = time.time()\n    # Perform naive search\n    print(\n        await rag.aquery(\n            \"What are the top themes in this story?\", param=QueryParam(mode=\"naive\")\n        )\n    )\n    print(f\"Naive Query Time: {time.time() - start_time} seconds\")\n    # Perform local search\n    print(\"**** Start Local Query ****\")\n    start_time = time.time()\n    print(\n        await rag.aquery(\n            \"What are the top themes in this story?\", param=QueryParam(mode=\"local\")\n        )\n    )\n    print(f\"Local Query Time: {time.time() - start_time} seconds\")\n    # Perform global search\n    print(\"**** Start Global Query ****\")\n    start_time = time.time()\n    print(\n        await rag.aquery(\n            \"What are the top themes in this story?\", param=QueryParam(mode=\"global\")\n        )\n    )\n    print(f\"Global Query Time: {time.time() - start_time}\")\n    # Perform hybrid search\n    print(\"**** Start Hybrid Query ****\")\n    print(\n        await rag.aquery(\n            \"What are the top themes in this story?\", param=QueryParam(mode=\"hybrid\")\n        )\n    )\n    print(f\"Hybrid Query Time: {time.time() - start_time} seconds\")\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n"}
{"type": "source_file", "path": "examples/lightrag_openai_demo.py", "content": "import os\n\nfrom lightrag import LightRAG, QueryParam\nfrom lightrag.llm import gpt_4o_mini_complete\n\nWORKING_DIR = \"./dickens\"\n\nif not os.path.exists(WORKING_DIR):\n    os.mkdir(WORKING_DIR)\n\nrag = LightRAG(\n    working_dir=WORKING_DIR,\n    llm_model_func=gpt_4o_mini_complete,\n    # llm_model_func=gpt_4o_complete\n)\n\n\nwith open(\"./book.txt\", \"r\", encoding=\"utf-8\") as f:\n    rag.insert(f.read())\n\n# Perform naive search\nprint(\n    rag.query(\"What are the top themes in this story?\", param=QueryParam(mode=\"naive\"))\n)\n\n# Perform local search\nprint(\n    rag.query(\"What are the top themes in this story?\", param=QueryParam(mode=\"local\"))\n)\n\n# Perform global search\nprint(\n    rag.query(\"What are the top themes in this story?\", param=QueryParam(mode=\"global\"))\n)\n\n# Perform hybrid search\nprint(\n    rag.query(\"What are the top themes in this story?\", param=QueryParam(mode=\"hybrid\"))\n)\n"}
{"type": "source_file", "path": "examples/lightrag_openai_compatible_stream_demo.py", "content": "import os\nimport inspect\nfrom lightrag import LightRAG\nfrom lightrag.llm import openai_complete, openai_embedding\nfrom lightrag.utils import EmbeddingFunc\nfrom lightrag.lightrag import always_get_an_event_loop\nfrom lightrag import QueryParam\n\n# WorkingDir\nROOT_DIR = os.path.dirname(os.path.abspath(__file__))\nWORKING_DIR = os.path.join(ROOT_DIR, \"dickens\")\nif not os.path.exists(WORKING_DIR):\n    os.mkdir(WORKING_DIR)\nprint(f\"WorkingDir: {WORKING_DIR}\")\n\napi_key = \"empty\"\nrag = LightRAG(\n    working_dir=WORKING_DIR,\n    llm_model_func=openai_complete,\n    llm_model_name=\"qwen2.5-14b-instruct@4bit\",\n    llm_model_max_async=4,\n    llm_model_max_token_size=32768,\n    llm_model_kwargs={\"base_url\": \"http://127.0.0.1:1234/v1\", \"api_key\": api_key},\n    embedding_func=EmbeddingFunc(\n        embedding_dim=1024,\n        max_token_size=8192,\n        func=lambda texts: openai_embedding(\n            texts=texts,\n            model=\"text-embedding-bge-m3\",\n            base_url=\"http://127.0.0.1:1234/v1\",\n            api_key=api_key,\n        ),\n    ),\n)\n\nwith open(\"./book.txt\", \"r\", encoding=\"utf-8\") as f:\n    rag.insert(f.read())\n\nresp = rag.query(\n    \"What are the top themes in this story?\",\n    param=QueryParam(mode=\"hybrid\", stream=True),\n)\n\n\nasync def print_stream(stream):\n    async for chunk in stream:\n        if chunk:\n            print(chunk, end=\"\", flush=True)\n\n\nloop = always_get_an_event_loop()\nif inspect.isasyncgen(resp):\n    loop.run_until_complete(print_stream(resp))\nelse:\n    print(resp)\n"}
{"type": "source_file", "path": "lightrag/api/azure_openai_lightrag_server.py", "content": "from fastapi import FastAPI, HTTPException, File, UploadFile, Form\nfrom pydantic import BaseModel\nimport asyncio\nimport logging\nimport argparse\nfrom lightrag import LightRAG, QueryParam\nfrom lightrag.llm import (\n    azure_openai_complete_if_cache,\n    azure_openai_embedding,\n)\nfrom lightrag.utils import EmbeddingFunc\nfrom typing import Optional, List\nfrom enum import Enum\nfrom pathlib import Path\nimport shutil\nimport aiofiles\nfrom ascii_colors import trace_exception\nimport os\nfrom dotenv import load_dotenv\nimport inspect\nimport json\nfrom fastapi.responses import StreamingResponse\n\nfrom fastapi import Depends, Security\nfrom fastapi.security import APIKeyHeader\nfrom fastapi.middleware.cors import CORSMiddleware\n\nfrom starlette.status import HTTP_403_FORBIDDEN\n\nload_dotenv()\n\nAZURE_OPENAI_API_VERSION = os.getenv(\"AZURE_OPENAI_API_VERSION\")\nAZURE_OPENAI_DEPLOYMENT = os.getenv(\"AZURE_OPENAI_DEPLOYMENT\")\nAZURE_OPENAI_API_KEY = os.getenv(\"AZURE_OPENAI_API_KEY\")\nAZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n\nAZURE_EMBEDDING_DEPLOYMENT = os.getenv(\"AZURE_EMBEDDING_DEPLOYMENT\")\nAZURE_EMBEDDING_API_VERSION = os.getenv(\"AZURE_EMBEDDING_API_VERSION\")\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(\n        description=\"LightRAG FastAPI Server with OpenAI integration\"\n    )\n\n    # Server configuration\n    parser.add_argument(\n        \"--host\", default=\"0.0.0.0\", help=\"Server host (default: 0.0.0.0)\"\n    )\n    parser.add_argument(\n        \"--port\", type=int, default=9621, help=\"Server port (default: 9621)\"\n    )\n\n    # Directory configuration\n    parser.add_argument(\n        \"--working-dir\",\n        default=\"./rag_storage\",\n        help=\"Working directory for RAG storage (default: ./rag_storage)\",\n    )\n    parser.add_argument(\n        \"--input-dir\",\n        default=\"./inputs\",\n        help=\"Directory containing input documents (default: ./inputs)\",\n    )\n\n    # Model configuration\n    parser.add_argument(\n        \"--model\", default=\"gpt-4o\", help=\"OpenAI model name (default: gpt-4o)\"\n    )\n    parser.add_argument(\n        \"--embedding-model\",\n        default=\"text-embedding-3-large\",\n        help=\"OpenAI embedding model (default: text-embedding-3-large)\",\n    )\n\n    # RAG configuration\n    parser.add_argument(\n        \"--max-tokens\",\n        type=int,\n        default=32768,\n        help=\"Maximum token size (default: 32768)\",\n    )\n    parser.add_argument(\n        \"--max-embed-tokens\",\n        type=int,\n        default=8192,\n        help=\"Maximum embedding token size (default: 8192)\",\n    )\n    parser.add_argument(\n        \"--enable-cache\",\n        default=True,\n        help=\"Enable response cache (default: True)\",\n    )\n    # Logging configuration\n    parser.add_argument(\n        \"--log-level\",\n        default=\"INFO\",\n        choices=[\"DEBUG\", \"INFO\", \"WARNING\", \"ERROR\", \"CRITICAL\"],\n        help=\"Logging level (default: INFO)\",\n    )\n\n    parser.add_argument(\n        \"--key\",\n        type=str,\n        help=\"API key for authentication. This protects lightrag server against unauthorized access\",\n        default=None,\n    )\n\n    return parser.parse_args()\n\n\nclass DocumentManager:\n    \"\"\"Handles document operations and tracking\"\"\"\n\n    def __init__(self, input_dir: str, supported_extensions: tuple = (\".txt\", \".md\")):\n        self.input_dir = Path(input_dir)\n        self.supported_extensions = supported_extensions\n        self.indexed_files = set()\n\n        # Create input directory if it doesn't exist\n        self.input_dir.mkdir(parents=True, exist_ok=True)\n\n    def scan_directory(self) -> List[Path]:\n        \"\"\"Scan input directory for new files\"\"\"\n        new_files = []\n        for ext in self.supported_extensions:\n            for file_path in self.input_dir.rglob(f\"*{ext}\"):\n                if file_path not in self.indexed_files:\n                    new_files.append(file_path)\n        return new_files\n\n    def mark_as_indexed(self, file_path: Path):\n        \"\"\"Mark a file as indexed\"\"\"\n        self.indexed_files.add(file_path)\n\n    def is_supported_file(self, filename: str) -> bool:\n        \"\"\"Check if file type is supported\"\"\"\n        return any(filename.lower().endswith(ext) for ext in self.supported_extensions)\n\n\n# Pydantic models\nclass SearchMode(str, Enum):\n    naive = \"naive\"\n    local = \"local\"\n    global_ = \"global\"\n    hybrid = \"hybrid\"\n\n\nclass QueryRequest(BaseModel):\n    query: str\n    mode: SearchMode = SearchMode.hybrid\n    only_need_context: bool = False\n    # stream: bool = False\n\n\nclass QueryResponse(BaseModel):\n    response: str\n\n\nclass InsertTextRequest(BaseModel):\n    text: str\n    description: Optional[str] = None\n\n\nclass InsertResponse(BaseModel):\n    status: str\n    message: str\n    document_count: int\n\n\ndef get_api_key_dependency(api_key: Optional[str]):\n    if not api_key:\n        # If no API key is configured, return a dummy dependency that always succeeds\n        async def no_auth():\n            return None\n\n        return no_auth\n\n    # If API key is configured, use proper authentication\n    api_key_header = APIKeyHeader(name=\"X-API-Key\", auto_error=False)\n\n    async def api_key_auth(api_key_header_value: str | None = Security(api_key_header)):\n        if not api_key_header_value:\n            raise HTTPException(\n                status_code=HTTP_403_FORBIDDEN, detail=\"API Key required\"\n            )\n        if api_key_header_value != api_key:\n            raise HTTPException(\n                status_code=HTTP_403_FORBIDDEN, detail=\"Invalid API Key\"\n            )\n        return api_key_header_value\n\n    return api_key_auth\n\n\nasync def get_embedding_dim(embedding_model: str) -> int:\n    \"\"\"Get embedding dimensions for the specified model\"\"\"\n    test_text = [\"This is a test sentence.\"]\n    embedding = await azure_openai_embedding(test_text, model=embedding_model)\n    return embedding.shape[1]\n\n\ndef create_app(args):\n    # Setup logging\n    logging.basicConfig(\n        format=\"%(levelname)s:%(message)s\", level=getattr(logging, args.log_level)\n    )\n\n    # Check if API key is provided either through env var or args\n    api_key = os.getenv(\"LIGHTRAG_API_KEY\") or args.key\n\n    # Initialize FastAPI\n    app = FastAPI(\n        title=\"LightRAG API\",\n        description=\"API for querying text using LightRAG with separate storage and input directories\"\n        + \"(With authentication)\"\n        if api_key\n        else \"\",\n        version=\"1.0.0\",\n        openapi_tags=[{\"name\": \"api\"}],\n    )\n\n    # Add CORS middleware\n    app.add_middleware(\n        CORSMiddleware,\n        allow_origins=[\"*\"],\n        allow_credentials=True,\n        allow_methods=[\"*\"],\n        allow_headers=[\"*\"],\n    )\n\n    # Create the optional API key dependency\n    optional_api_key = get_api_key_dependency(api_key)\n\n    # Create working directory if it doesn't exist\n    Path(args.working_dir).mkdir(parents=True, exist_ok=True)\n\n    # Initialize document manager\n    doc_manager = DocumentManager(args.input_dir)\n\n    # Get embedding dimensions\n    embedding_dim = asyncio.run(get_embedding_dim(args.embedding_model))\n\n    async def async_openai_complete(\n        prompt, system_prompt=None, history_messages=[], **kwargs\n    ):\n        \"\"\"Async wrapper for OpenAI completion\"\"\"\n        kwargs.pop(\"keyword_extraction\", None)\n\n        return await azure_openai_complete_if_cache(\n            args.model,\n            prompt,\n            system_prompt=system_prompt,\n            history_messages=history_messages,\n            base_url=AZURE_OPENAI_ENDPOINT,\n            api_key=AZURE_OPENAI_API_KEY,\n            api_version=AZURE_OPENAI_API_VERSION,\n            **kwargs,\n        )\n\n    # Initialize RAG with OpenAI configuration\n    rag = LightRAG(\n        enable_llm_cache=args.enable_cache,\n        working_dir=args.working_dir,\n        llm_model_func=async_openai_complete,\n        llm_model_name=args.model,\n        llm_model_max_token_size=args.max_tokens,\n        embedding_func=EmbeddingFunc(\n            embedding_dim=embedding_dim,\n            max_token_size=args.max_embed_tokens,\n            func=lambda texts: azure_openai_embedding(\n                texts, model=args.embedding_model\n            ),\n        ),\n    )\n\n    @app.on_event(\"startup\")\n    async def startup_event():\n        \"\"\"Index all files in input directory during startup\"\"\"\n        try:\n            new_files = doc_manager.scan_directory()\n            for file_path in new_files:\n                try:\n                    # Use async file reading\n                    async with aiofiles.open(file_path, \"r\", encoding=\"utf-8\") as f:\n                        content = await f.read()\n                        # Use the async version of insert directly\n                        await rag.ainsert(content)\n                        doc_manager.mark_as_indexed(file_path)\n                        logging.info(f\"Indexed file: {file_path}\")\n                except Exception as e:\n                    trace_exception(e)\n                    logging.error(f\"Error indexing file {file_path}: {str(e)}\")\n\n            logging.info(f\"Indexed {len(new_files)} documents from {args.input_dir}\")\n\n        except Exception as e:\n            logging.error(f\"Error during startup indexing: {str(e)}\")\n\n    @app.post(\"/documents/scan\", dependencies=[Depends(optional_api_key)])\n    async def scan_for_new_documents():\n        \"\"\"Manually trigger scanning for new documents\"\"\"\n        try:\n            new_files = doc_manager.scan_directory()\n            indexed_count = 0\n\n            for file_path in new_files:\n                try:\n                    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n                        content = f.read()\n                        await rag.ainsert(content)\n                        doc_manager.mark_as_indexed(file_path)\n                        indexed_count += 1\n                except Exception as e:\n                    logging.error(f\"Error indexing file {file_path}: {str(e)}\")\n\n            return {\n                \"status\": \"success\",\n                \"indexed_count\": indexed_count,\n                \"total_documents\": len(doc_manager.indexed_files),\n            }\n        except Exception as e:\n            raise HTTPException(status_code=500, detail=str(e))\n\n    @app.post(\"/resetcache\", dependencies=[Depends(optional_api_key)])\n    async def reset_cache():\n        \"\"\"Manually reset cache\"\"\"\n        try:\n            cachefile = args.working_dir + \"/kv_store_llm_response_cache.json\"\n            if os.path.exists(cachefile):\n                with open(cachefile, \"w\") as f:\n                    f.write(\"{}\")\n            return {\"status\": \"success\"}\n        except Exception as e:\n            raise HTTPException(status_code=500, detail=str(e))\n\n    @app.post(\"/documents/upload\", dependencies=[Depends(optional_api_key)])\n    async def upload_to_input_dir(file: UploadFile = File(...)):\n        \"\"\"Upload a file to the input directory\"\"\"\n        try:\n            if not doc_manager.is_supported_file(file.filename):\n                raise HTTPException(\n                    status_code=400,\n                    detail=f\"Unsupported file type. Supported types: {doc_manager.supported_extensions}\",\n                )\n\n            file_path = doc_manager.input_dir / file.filename\n            with open(file_path, \"wb\") as buffer:\n                shutil.copyfileobj(file.file, buffer)\n\n            # Immediately index the uploaded file\n            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n                content = f.read()\n                await rag.ainsert(content)\n                doc_manager.mark_as_indexed(file_path)\n\n            return {\n                \"status\": \"success\",\n                \"message\": f\"File uploaded and indexed: {file.filename}\",\n                \"total_documents\": len(doc_manager.indexed_files),\n            }\n        except Exception as e:\n            raise HTTPException(status_code=500, detail=str(e))\n\n    @app.post(\n        \"/query\", response_model=QueryResponse, dependencies=[Depends(optional_api_key)]\n    )\n    async def query_text(request: QueryRequest):\n        try:\n            response = await rag.aquery(\n                request.query,\n                param=QueryParam(\n                    mode=request.mode,\n                    stream=False,\n                    only_need_context=request.only_need_context,\n                ),\n            )\n            return QueryResponse(response=response)\n        except Exception as e:\n            raise HTTPException(status_code=500, detail=str(e))\n\n    @app.post(\"/query/stream\", dependencies=[Depends(optional_api_key)])\n    async def query_text_stream(request: QueryRequest):\n        try:\n            response = await rag.aquery(\n                request.query,\n                param=QueryParam(\n                    mode=request.mode,\n                    stream=True,\n                    only_need_context=request.only_need_context,\n                ),\n            )\n            if inspect.isasyncgen(response):\n\n                async def stream_generator():\n                    async for chunk in response:\n                        yield json.dumps({\"data\": chunk}) + \"\\n\"\n\n                return StreamingResponse(\n                    stream_generator(), media_type=\"application/json\"\n                )\n            else:\n                return QueryResponse(response=response)\n\n        except Exception as e:\n            raise HTTPException(status_code=500, detail=str(e))\n\n    @app.post(\n        \"/documents/text\",\n        response_model=InsertResponse,\n        dependencies=[Depends(optional_api_key)],\n    )\n    async def insert_text(request: InsertTextRequest):\n        try:\n            await rag.ainsert(request.text)\n            return InsertResponse(\n                status=\"success\",\n                message=\"Text successfully inserted\",\n                document_count=1,\n            )\n        except Exception as e:\n            raise HTTPException(status_code=500, detail=str(e))\n\n    @app.post(\n        \"/documents/file\",\n        response_model=InsertResponse,\n        dependencies=[Depends(optional_api_key)],\n    )\n    async def insert_file(file: UploadFile = File(...), description: str = Form(None)):\n        try:\n            content = await file.read()\n\n            if file.filename.endswith((\".txt\", \".md\")):\n                text = content.decode(\"utf-8\")\n                rag.insert(text)\n            else:\n                raise HTTPException(\n                    status_code=400,\n                    detail=\"Unsupported file type. Only .txt and .md files are supported\",\n                )\n\n            return InsertResponse(\n                status=\"success\",\n                message=f\"File '{file.filename}' successfully inserted\",\n                document_count=1,\n            )\n        except UnicodeDecodeError:\n            raise HTTPException(status_code=400, detail=\"File encoding not supported\")\n        except Exception as e:\n            raise HTTPException(status_code=500, detail=str(e))\n\n    @app.post(\n        \"/documents/batch\",\n        response_model=InsertResponse,\n        dependencies=[Depends(optional_api_key)],\n    )\n    async def insert_batch(files: List[UploadFile] = File(...)):\n        try:\n            inserted_count = 0\n            failed_files = []\n\n            for file in files:\n                try:\n                    content = await file.read()\n                    if file.filename.endswith((\".txt\", \".md\")):\n                        text = content.decode(\"utf-8\")\n                        rag.insert(text)\n                        inserted_count += 1\n                    else:\n                        failed_files.append(f\"{file.filename} (unsupported type)\")\n                except Exception as e:\n                    failed_files.append(f\"{file.filename} ({str(e)})\")\n\n            status_message = f\"Successfully inserted {inserted_count} documents\"\n            if failed_files:\n                status_message += f\". Failed files: {', '.join(failed_files)}\"\n\n            return InsertResponse(\n                status=\"success\" if inserted_count > 0 else \"partial_success\",\n                message=status_message,\n                document_count=len(files),\n            )\n        except Exception as e:\n            raise HTTPException(status_code=500, detail=str(e))\n\n    @app.delete(\n        \"/documents\",\n        response_model=InsertResponse,\n        dependencies=[Depends(optional_api_key)],\n    )\n    async def clear_documents():\n        try:\n            rag.text_chunks = []\n            rag.entities_vdb = None\n            rag.relationships_vdb = None\n            return InsertResponse(\n                status=\"success\",\n                message=\"All documents cleared successfully\",\n                document_count=0,\n            )\n        except Exception as e:\n            raise HTTPException(status_code=500, detail=str(e))\n\n    @app.get(\"/health\", dependencies=[Depends(optional_api_key)])\n    async def get_status():\n        \"\"\"Get current system status\"\"\"\n        return {\n            \"status\": \"healthy\",\n            \"working_directory\": str(args.working_dir),\n            \"input_directory\": str(args.input_dir),\n            \"indexed_files\": len(doc_manager.indexed_files),\n            \"configuration\": {\n                \"model\": args.model,\n                \"embedding_model\": args.embedding_model,\n                \"max_tokens\": args.max_tokens,\n                \"embedding_dim\": embedding_dim,\n            },\n        }\n\n    return app\n\n\ndef main():\n    args = parse_args()\n    import uvicorn\n\n    app = create_app(args)\n    uvicorn.run(app, host=args.host, port=args.port)\n\n\nif __name__ == \"__main__\":\n    main()\n"}
{"type": "source_file", "path": "lightrag/api/ollama_lightrag_server.py", "content": "from fastapi import FastAPI, HTTPException, File, UploadFile, Form\nfrom pydantic import BaseModel\nimport logging\nimport argparse\nfrom lightrag import LightRAG, QueryParam\nfrom lightrag.llm import ollama_model_complete, ollama_embed\nfrom lightrag.utils import EmbeddingFunc\nfrom typing import Optional, List\nfrom enum import Enum\nfrom pathlib import Path\nimport shutil\nimport aiofiles\nfrom ascii_colors import trace_exception\nimport os\n\nfrom fastapi import Depends, Security\nfrom fastapi.security import APIKeyHeader\nfrom fastapi.middleware.cors import CORSMiddleware\n\nfrom starlette.status import HTTP_403_FORBIDDEN\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(\n        description=\"LightRAG FastAPI Server with separate working and input directories\"\n    )\n\n    # Server configuration\n    parser.add_argument(\n        \"--host\", default=\"0.0.0.0\", help=\"Server host (default: 0.0.0.0)\"\n    )\n    parser.add_argument(\n        \"--port\", type=int, default=9621, help=\"Server port (default: 9621)\"\n    )\n\n    # Directory configuration\n    parser.add_argument(\n        \"--working-dir\",\n        default=\"./rag_storage\",\n        help=\"Working directory for RAG storage (default: ./rag_storage)\",\n    )\n    parser.add_argument(\n        \"--input-dir\",\n        default=\"./inputs\",\n        help=\"Directory containing input documents (default: ./inputs)\",\n    )\n\n    # Model configuration\n    parser.add_argument(\n        \"--model\",\n        default=\"mistral-nemo:latest\",\n        help=\"LLM model name (default: mistral-nemo:latest)\",\n    )\n    parser.add_argument(\n        \"--embedding-model\",\n        default=\"bge-m3:latest\",\n        help=\"Embedding model name (default: bge-m3:latest)\",\n    )\n    parser.add_argument(\n        \"--ollama-host\",\n        default=\"http://localhost:11434\",\n        help=\"Ollama host URL (default: http://localhost:11434)\",\n    )\n\n    # RAG configuration\n    parser.add_argument(\n        \"--max-async\", type=int, default=4, help=\"Maximum async operations (default: 4)\"\n    )\n    parser.add_argument(\n        \"--max-tokens\",\n        type=int,\n        default=32768,\n        help=\"Maximum token size (default: 32768)\",\n    )\n    parser.add_argument(\n        \"--embedding-dim\",\n        type=int,\n        default=1024,\n        help=\"Embedding dimensions (default: 1024)\",\n    )\n    parser.add_argument(\n        \"--max-embed-tokens\",\n        type=int,\n        default=8192,\n        help=\"Maximum embedding token size (default: 8192)\",\n    )\n\n    # Logging configuration\n    parser.add_argument(\n        \"--log-level\",\n        default=\"INFO\",\n        choices=[\"DEBUG\", \"INFO\", \"WARNING\", \"ERROR\", \"CRITICAL\"],\n        help=\"Logging level (default: INFO)\",\n    )\n    parser.add_argument(\n        \"--key\",\n        type=str,\n        help=\"API key for authentication. This protects lightrag server against unauthorized access\",\n        default=None,\n    )\n\n    return parser.parse_args()\n\n\nclass DocumentManager:\n    \"\"\"Handles document operations and tracking\"\"\"\n\n    def __init__(self, input_dir: str, supported_extensions: tuple = (\".txt\", \".md\")):\n        self.input_dir = Path(input_dir)\n        self.supported_extensions = supported_extensions\n        self.indexed_files = set()\n\n        # Create input directory if it doesn't exist\n        self.input_dir.mkdir(parents=True, exist_ok=True)\n\n    def scan_directory(self) -> List[Path]:\n        \"\"\"Scan input directory for new files\"\"\"\n        new_files = []\n        for ext in self.supported_extensions:\n            for file_path in self.input_dir.rglob(f\"*{ext}\"):\n                if file_path not in self.indexed_files:\n                    new_files.append(file_path)\n        return new_files\n\n    def mark_as_indexed(self, file_path: Path):\n        \"\"\"Mark a file as indexed\"\"\"\n        self.indexed_files.add(file_path)\n\n    def is_supported_file(self, filename: str) -> bool:\n        \"\"\"Check if file type is supported\"\"\"\n        return any(filename.lower().endswith(ext) for ext in self.supported_extensions)\n\n\n# Pydantic models\nclass SearchMode(str, Enum):\n    naive = \"naive\"\n    local = \"local\"\n    global_ = \"global\"\n    hybrid = \"hybrid\"\n\n\nclass QueryRequest(BaseModel):\n    query: str\n    mode: SearchMode = SearchMode.hybrid\n    stream: bool = False\n    only_need_context: bool = False\n\n\nclass QueryResponse(BaseModel):\n    response: str\n\n\nclass InsertTextRequest(BaseModel):\n    text: str\n    description: Optional[str] = None\n\n\nclass InsertResponse(BaseModel):\n    status: str\n    message: str\n    document_count: int\n\n\ndef get_api_key_dependency(api_key: Optional[str]):\n    if not api_key:\n        # If no API key is configured, return a dummy dependency that always succeeds\n        async def no_auth():\n            return None\n\n        return no_auth\n\n    # If API key is configured, use proper authentication\n    api_key_header = APIKeyHeader(name=\"X-API-Key\", auto_error=False)\n\n    async def api_key_auth(api_key_header_value: str | None = Security(api_key_header)):\n        if not api_key_header_value:\n            raise HTTPException(\n                status_code=HTTP_403_FORBIDDEN, detail=\"API Key required\"\n            )\n        if api_key_header_value != api_key:\n            raise HTTPException(\n                status_code=HTTP_403_FORBIDDEN, detail=\"Invalid API Key\"\n            )\n        return api_key_header_value\n\n    return api_key_auth\n\n\ndef create_app(args):\n    # Setup logging\n    logging.basicConfig(\n        format=\"%(levelname)s:%(message)s\", level=getattr(logging, args.log_level)\n    )\n\n    # Check if API key is provided either through env var or args\n    api_key = os.getenv(\"LIGHTRAG_API_KEY\") or args.key\n\n    # Initialize FastAPI\n    app = FastAPI(\n        title=\"LightRAG API\",\n        description=\"API for querying text using LightRAG with separate storage and input directories\"\n        + \"(With authentication)\"\n        if api_key\n        else \"\",\n        version=\"1.0.0\",\n        openapi_tags=[{\"name\": \"api\"}],\n    )\n\n    # Add CORS middleware\n    app.add_middleware(\n        CORSMiddleware,\n        allow_origins=[\"*\"],\n        allow_credentials=True,\n        allow_methods=[\"*\"],\n        allow_headers=[\"*\"],\n    )\n\n    # Create the optional API key dependency\n    optional_api_key = get_api_key_dependency(api_key)\n\n    # Create working directory if it doesn't exist\n    Path(args.working_dir).mkdir(parents=True, exist_ok=True)\n\n    # Initialize document manager\n    doc_manager = DocumentManager(args.input_dir)\n\n    # Initialize RAG\n    rag = LightRAG(\n        working_dir=args.working_dir,\n        llm_model_func=ollama_model_complete,\n        llm_model_name=args.model,\n        llm_model_max_async=args.max_async,\n        llm_model_max_token_size=args.max_tokens,\n        llm_model_kwargs={\n            \"host\": args.ollama_host,\n            \"options\": {\"num_ctx\": args.max_tokens},\n        },\n        embedding_func=EmbeddingFunc(\n            embedding_dim=args.embedding_dim,\n            max_token_size=args.max_embed_tokens,\n            func=lambda texts: ollama_embed(\n                texts, embed_model=args.embedding_model, host=args.ollama_host\n            ),\n        ),\n    )\n\n    @app.on_event(\"startup\")\n    async def startup_event():\n        \"\"\"Index all files in input directory during startup\"\"\"\n        try:\n            new_files = doc_manager.scan_directory()\n            for file_path in new_files:\n                try:\n                    # Use async file reading\n                    async with aiofiles.open(file_path, \"r\", encoding=\"utf-8\") as f:\n                        content = await f.read()\n                        # Use the async version of insert directly\n                        await rag.ainsert(content)\n                        doc_manager.mark_as_indexed(file_path)\n                        logging.info(f\"Indexed file: {file_path}\")\n                except Exception as e:\n                    trace_exception(e)\n                    logging.error(f\"Error indexing file {file_path}: {str(e)}\")\n\n            logging.info(f\"Indexed {len(new_files)} documents from {args.input_dir}\")\n\n        except Exception as e:\n            logging.error(f\"Error during startup indexing: {str(e)}\")\n\n    @app.post(\"/documents/scan\", dependencies=[Depends(optional_api_key)])\n    async def scan_for_new_documents():\n        \"\"\"Manually trigger scanning for new documents\"\"\"\n        try:\n            new_files = doc_manager.scan_directory()\n            indexed_count = 0\n\n            for file_path in new_files:\n                try:\n                    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n                        content = f.read()\n                        await rag.ainsert(content)\n                        doc_manager.mark_as_indexed(file_path)\n                        indexed_count += 1\n                except Exception as e:\n                    logging.error(f\"Error indexing file {file_path}: {str(e)}\")\n\n            return {\n                \"status\": \"success\",\n                \"indexed_count\": indexed_count,\n                \"total_documents\": len(doc_manager.indexed_files),\n            }\n        except Exception as e:\n            raise HTTPException(status_code=500, detail=str(e))\n\n    @app.post(\"/documents/upload\", dependencies=[Depends(optional_api_key)])\n    async def upload_to_input_dir(file: UploadFile = File(...)):\n        \"\"\"Upload a file to the input directory\"\"\"\n        try:\n            if not doc_manager.is_supported_file(file.filename):\n                raise HTTPException(\n                    status_code=400,\n                    detail=f\"Unsupported file type. Supported types: {doc_manager.supported_extensions}\",\n                )\n\n            file_path = doc_manager.input_dir / file.filename\n            with open(file_path, \"wb\") as buffer:\n                shutil.copyfileobj(file.file, buffer)\n\n            # Immediately index the uploaded file\n            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n                content = f.read()\n                await rag.ainsert(content)\n                doc_manager.mark_as_indexed(file_path)\n\n            return {\n                \"status\": \"success\",\n                \"message\": f\"File uploaded and indexed: {file.filename}\",\n                \"total_documents\": len(doc_manager.indexed_files),\n            }\n        except Exception as e:\n            raise HTTPException(status_code=500, detail=str(e))\n\n    @app.post(\n        \"/query\", response_model=QueryResponse, dependencies=[Depends(optional_api_key)]\n    )\n    async def query_text(request: QueryRequest):\n        try:\n            response = await rag.aquery(\n                request.query,\n                param=QueryParam(\n                    mode=request.mode,\n                    stream=request.stream,\n                    only_need_context=request.only_need_context,\n                ),\n            )\n\n            if request.stream:\n                result = \"\"\n                async for chunk in response:\n                    result += chunk\n                return QueryResponse(response=result)\n            else:\n                return QueryResponse(response=response)\n        except Exception as e:\n            raise HTTPException(status_code=500, detail=str(e))\n\n    @app.post(\"/query/stream\", dependencies=[Depends(optional_api_key)])\n    async def query_text_stream(request: QueryRequest):\n        try:\n            response = rag.query(\n                request.query,\n                param=QueryParam(\n                    mode=request.mode,\n                    stream=True,\n                    only_need_context=request.only_need_context,\n                ),\n            )\n\n            async def stream_generator():\n                async for chunk in response:\n                    yield chunk\n\n            return stream_generator()\n        except Exception as e:\n            raise HTTPException(status_code=500, detail=str(e))\n\n    @app.post(\n        \"/documents/text\",\n        response_model=InsertResponse,\n        dependencies=[Depends(optional_api_key)],\n    )\n    async def insert_text(request: InsertTextRequest):\n        try:\n            await rag.ainsert(request.text)\n            return InsertResponse(\n                status=\"success\",\n                message=\"Text successfully inserted\",\n                document_count=len(rag),\n            )\n        except Exception as e:\n            raise HTTPException(status_code=500, detail=str(e))\n\n    @app.post(\n        \"/documents/file\",\n        response_model=InsertResponse,\n        dependencies=[Depends(optional_api_key)],\n    )\n    async def insert_file(file: UploadFile = File(...), description: str = Form(None)):\n        try:\n            content = await file.read()\n\n            if file.filename.endswith((\".txt\", \".md\")):\n                text = content.decode(\"utf-8\")\n                await rag.ainsert(text)\n            else:\n                raise HTTPException(\n                    status_code=400,\n                    detail=\"Unsupported file type. Only .txt and .md files are supported\",\n                )\n\n            return InsertResponse(\n                status=\"success\",\n                message=f\"File '{file.filename}' successfully inserted\",\n                document_count=1,\n            )\n        except UnicodeDecodeError:\n            raise HTTPException(status_code=400, detail=\"File encoding not supported\")\n        except Exception as e:\n            raise HTTPException(status_code=500, detail=str(e))\n\n    @app.post(\n        \"/documents/batch\",\n        response_model=InsertResponse,\n        dependencies=[Depends(optional_api_key)],\n    )\n    async def insert_batch(files: List[UploadFile] = File(...)):\n        try:\n            inserted_count = 0\n            failed_files = []\n\n            for file in files:\n                try:\n                    content = await file.read()\n                    if file.filename.endswith((\".txt\", \".md\")):\n                        text = content.decode(\"utf-8\")\n                        await rag.ainsert(text)\n                        inserted_count += 1\n                    else:\n                        failed_files.append(f\"{file.filename} (unsupported type)\")\n                except Exception as e:\n                    failed_files.append(f\"{file.filename} ({str(e)})\")\n\n            status_message = f\"Successfully inserted {inserted_count} documents\"\n            if failed_files:\n                status_message += f\". Failed files: {', '.join(failed_files)}\"\n\n            return InsertResponse(\n                status=\"success\" if inserted_count > 0 else \"partial_success\",\n                message=status_message,\n                document_count=len(files),\n            )\n        except Exception as e:\n            raise HTTPException(status_code=500, detail=str(e))\n\n    @app.delete(\n        \"/documents\",\n        response_model=InsertResponse,\n        dependencies=[Depends(optional_api_key)],\n    )\n    async def clear_documents():\n        try:\n            rag.text_chunks = []\n            rag.entities_vdb = None\n            rag.relationships_vdb = None\n            return InsertResponse(\n                status=\"success\",\n                message=\"All documents cleared successfully\",\n                document_count=0,\n            )\n        except Exception as e:\n            raise HTTPException(status_code=500, detail=str(e))\n\n    @app.get(\"/health\", dependencies=[Depends(optional_api_key)])\n    async def get_status():\n        \"\"\"Get current system status\"\"\"\n        return {\n            \"status\": \"healthy\",\n            \"working_directory\": str(args.working_dir),\n            \"input_directory\": str(args.input_dir),\n            \"indexed_files\": len(doc_manager.indexed_files),\n            \"configuration\": {\n                \"model\": args.model,\n                \"embedding_model\": args.embedding_model,\n                \"max_tokens\": args.max_tokens,\n                \"ollama_host\": args.ollama_host,\n            },\n        }\n\n    return app\n\n\ndef main():\n    args = parse_args()\n    import uvicorn\n\n    app = create_app(args)\n    uvicorn.run(app, host=args.host, port=args.port)\n\n\nif __name__ == \"__main__\":\n    main()\n"}
{"type": "source_file", "path": "lightrag/kg/milvus_impl.py", "content": "import asyncio\nimport os\nfrom tqdm.asyncio import tqdm as tqdm_async\nfrom dataclasses import dataclass\nimport numpy as np\nfrom lightrag.utils import logger\nfrom ..base import BaseVectorStorage\n\nfrom pymilvus import MilvusClient\n\n\n@dataclass\nclass MilvusVectorDBStorge(BaseVectorStorage):\n    @staticmethod\n    def create_collection_if_not_exist(\n        client: MilvusClient, collection_name: str, **kwargs\n    ):\n        if client.has_collection(collection_name):\n            return\n        client.create_collection(\n            collection_name, max_length=64, id_type=\"string\", **kwargs\n        )\n\n    def __post_init__(self):\n        self._client = MilvusClient(\n            uri=os.environ.get(\n                \"MILVUS_URI\",\n                os.path.join(self.global_config[\"working_dir\"], \"milvus_lite.db\"),\n            ),\n            user=os.environ.get(\"MILVUS_USER\", \"\"),\n            password=os.environ.get(\"MILVUS_PASSWORD\", \"\"),\n            token=os.environ.get(\"MILVUS_TOKEN\", \"\"),\n            db_name=os.environ.get(\"MILVUS_DB_NAME\", \"\"),\n        )\n        self._max_batch_size = self.global_config[\"embedding_batch_num\"]\n        MilvusVectorDBStorge.create_collection_if_not_exist(\n            self._client,\n            self.namespace,\n            dimension=self.embedding_func.embedding_dim,\n        )\n\n    async def upsert(self, data: dict[str, dict]):\n        logger.info(f\"Inserting {len(data)} vectors to {self.namespace}\")\n        if not len(data):\n            logger.warning(\"You insert an empty data to vector DB\")\n            return []\n        list_data = [\n            {\n                \"id\": k,\n                **{k1: v1 for k1, v1 in v.items() if k1 in self.meta_fields},\n            }\n            for k, v in data.items()\n        ]\n        contents = [v[\"content\"] for v in data.values()]\n        batches = [\n            contents[i : i + self._max_batch_size]\n            for i in range(0, len(contents), self._max_batch_size)\n        ]\n\n        async def wrapped_task(batch):\n            result = await self.embedding_func(batch)\n            pbar.update(1)\n            return result\n\n        embedding_tasks = [wrapped_task(batch) for batch in batches]\n        pbar = tqdm_async(\n            total=len(embedding_tasks), desc=\"Generating embeddings\", unit=\"batch\"\n        )\n        embeddings_list = await asyncio.gather(*embedding_tasks)\n\n        embeddings = np.concatenate(embeddings_list)\n        for i, d in enumerate(list_data):\n            d[\"vector\"] = embeddings[i]\n        results = self._client.upsert(collection_name=self.namespace, data=list_data)\n        return results\n\n    async def query(self, query, top_k=5):\n        embedding = await self.embedding_func([query])\n        results = self._client.search(\n            collection_name=self.namespace,\n            data=embedding,\n            limit=top_k,\n            output_fields=list(self.meta_fields),\n            search_params={\"metric_type\": \"COSINE\", \"params\": {\"radius\": 0.2}},\n        )\n        print(results)\n        return [\n            {**dp[\"entity\"], \"id\": dp[\"id\"], \"distance\": dp[\"distance\"]}\n            for dp in results[0]\n        ]\n"}
{"type": "source_file", "path": "lightrag/kg/gremlin_impl.py", "content": "import asyncio\nimport inspect\nimport json\nimport os\nfrom dataclasses import dataclass\nfrom typing import Any, Dict, List, Tuple, Union\n\nfrom gremlin_python.driver import client, serializer\nfrom gremlin_python.driver.aiohttp.transport import AiohttpTransport\nfrom gremlin_python.driver.protocol import GremlinServerError\nfrom tenacity import (\n    retry,\n    retry_if_exception_type,\n    stop_after_attempt,\n    wait_exponential,\n)\n\nfrom lightrag.utils import logger\n\nfrom ..base import BaseGraphStorage\n\n\n@dataclass\nclass GremlinStorage(BaseGraphStorage):\n    @staticmethod\n    def load_nx_graph(file_name):\n        print(\"no preloading of graph with Gremlin in production\")\n\n    def __init__(self, namespace, global_config, embedding_func):\n        super().__init__(\n            namespace=namespace,\n            global_config=global_config,\n            embedding_func=embedding_func,\n        )\n\n        self._driver = None\n        self._driver_lock = asyncio.Lock()\n\n        USER = os.environ.get(\"GREMLIN_USER\", \"\")\n        PASSWORD = os.environ.get(\"GREMLIN_PASSWORD\", \"\")\n        HOST = os.environ[\"GREMLIN_HOST\"]\n        PORT = int(os.environ[\"GREMLIN_PORT\"])\n\n        # TraversalSource, a custom one has to be created manually,\n        # default it \"g\"\n        SOURCE = os.environ.get(\"GREMLIN_TRAVERSE_SOURCE\", \"g\")\n\n        # All vertices will have graph={GRAPH} property, so that we can\n        # have several logical graphs for one source\n        GRAPH = GremlinStorage._to_value_map(os.environ[\"GREMLIN_GRAPH\"])\n\n        self.graph_name = GRAPH\n\n        self._driver = client.Client(\n            f\"ws://{HOST}:{PORT}/gremlin\",\n            SOURCE,\n            username=USER,\n            password=PASSWORD,\n            message_serializer=serializer.GraphSONSerializersV3d0(),\n            transport_factory=lambda: AiohttpTransport(call_from_event_loop=True),\n        )\n\n    def __post_init__(self):\n        self._node_embed_algorithms = {\n            \"node2vec\": self._node2vec_embed,\n        }\n\n    async def close(self):\n        if self._driver:\n            self._driver.close()\n            self._driver = None\n\n    async def __aexit__(self, exc_type, exc, tb):\n        if self._driver:\n            self._driver.close()\n\n    async def index_done_callback(self):\n        print(\"KG successfully indexed.\")\n\n    @staticmethod\n    def _to_value_map(value: Any) -> str:\n        \"\"\"Dump supported Python object as Gremlin valueMap\"\"\"\n        json_str = json.dumps(value, ensure_ascii=False, sort_keys=False)\n        parsed_str = json_str.replace(\"'\", r\"\\'\")\n\n        # walk over the string and replace curly brackets with square brackets\n        # outside of strings, as well as replace double quotes with single quotes\n        # and \"deescape\" double quotes inside of strings\n        outside_str = True\n        escaped = False\n        remove_indices = []\n        for i, c in enumerate(parsed_str):\n            if escaped:\n                # previous character was an \"odd\" backslash\n                escaped = False\n                if c == '\"':\n                    # we want to \"deescape\" double quotes: store indices to delete\n                    remove_indices.insert(0, i - 1)\n            elif c == \"\\\\\":\n                escaped = True\n            elif c == '\"':\n                outside_str = not outside_str\n                parsed_str = parsed_str[:i] + \"'\" + parsed_str[i + 1 :]\n            elif c == \"{\" and outside_str:\n                parsed_str = parsed_str[:i] + \"[\" + parsed_str[i + 1 :]\n            elif c == \"}\" and outside_str:\n                parsed_str = parsed_str[:i] + \"]\" + parsed_str[i + 1 :]\n        for idx in remove_indices:\n            parsed_str = parsed_str[:idx] + parsed_str[idx + 1 :]\n        return parsed_str\n\n    @staticmethod\n    def _convert_properties(properties: Dict[str, Any]) -> str:\n        \"\"\"Create chained .property() commands from properties dict\"\"\"\n        props = []\n        for k, v in properties.items():\n            prop_name = GremlinStorage._to_value_map(k)\n            props.append(f\".property({prop_name}, {GremlinStorage._to_value_map(v)})\")\n        return \"\".join(props)\n\n    @staticmethod\n    def _fix_name(name: str) -> str:\n        \"\"\"Strip double quotes and format as a proper field name\"\"\"\n        name = GremlinStorage._to_value_map(name.strip('\"').replace(r\"\\'\", \"'\"))\n\n        return name\n\n    async def _query(self, query: str) -> List[Dict[str, Any]]:\n        \"\"\"\n        Query the Gremlin graph\n\n        Args:\n            query (str): a query to be executed\n\n        Returns:\n            List[Dict[str, Any]]: a list of dictionaries containing the result set\n        \"\"\"\n\n        result = list(await asyncio.wrap_future(self._driver.submit_async(query)))\n        if result:\n            result = result[0]\n\n        return result\n\n    async def has_node(self, node_id: str) -> bool:\n        entity_name = GremlinStorage._fix_name(node_id)\n\n        query = f\"\"\"g\n                 .V().has('graph', {self.graph_name})\n                 .has('entity_name', {entity_name})\n                 .limit(1)\n                 .count()\n                 .project('has_node')\n                    .by(__.choose(__.is(gt(0)), constant(true), constant(false)))\n                 \"\"\"\n        result = await self._query(query)\n        logger.debug(\n            \"{%s}:query:{%s}:result:{%s}\",\n            inspect.currentframe().f_code.co_name,\n            query,\n            result[0][\"has_node\"],\n        )\n\n        return result[0][\"has_node\"]\n\n    async def has_edge(self, source_node_id: str, target_node_id: str) -> bool:\n        entity_name_source = GremlinStorage._fix_name(source_node_id)\n        entity_name_target = GremlinStorage._fix_name(target_node_id)\n\n        query = f\"\"\"g\n                 .V().has('graph', {self.graph_name})\n                 .has('entity_name', {entity_name_source})\n                 .outE()\n                 .inV().has('graph', {self.graph_name})\n                 .has('entity_name', {entity_name_target})\n                 .limit(1)\n                 .count()\n                 .project('has_edge')\n                    .by(__.choose(__.is(gt(0)), constant(true), constant(false)))\n                 \"\"\"\n        result = await self._query(query)\n        logger.debug(\n            \"{%s}:query:{%s}:result:{%s}\",\n            inspect.currentframe().f_code.co_name,\n            query,\n            result[0][\"has_edge\"],\n        )\n\n        return result[0][\"has_edge\"]\n\n    async def get_node(self, node_id: str) -> Union[dict, None]:\n        entity_name = GremlinStorage._fix_name(node_id)\n        query = f\"\"\"g\n                 .V().has('graph', {self.graph_name})\n                 .has('entity_name', {entity_name})\n                 .limit(1)\n                 .project('properties')\n                    .by(elementMap())\n                 \"\"\"\n        result = await self._query(query)\n        if result:\n            node = result[0]\n            node_dict = node[\"properties\"]\n            logger.debug(\n                \"{%s}: query: {%s}, result: {%s}\",\n                inspect.currentframe().f_code.co_name,\n                query.format,\n                node_dict,\n            )\n            return node_dict\n\n    async def node_degree(self, node_id: str) -> int:\n        entity_name = GremlinStorage._fix_name(node_id)\n        query = f\"\"\"g\n                 .V().has('graph', {self.graph_name})\n                 .has('entity_name', {entity_name})\n                 .outE()\n                 .inV().has('graph', {self.graph_name})\n                 .count()\n                 .project('total_edge_count')\n                    .by()\n                 \"\"\"\n        result = await self._query(query)\n        edge_count = result[0][\"total_edge_count\"]\n\n        logger.debug(\n            \"{%s}:query:{%s}:result:{%s}\",\n            inspect.currentframe().f_code.co_name,\n            query,\n            edge_count,\n        )\n\n        return edge_count\n\n    async def edge_degree(self, src_id: str, tgt_id: str) -> int:\n        src_degree = await self.node_degree(src_id)\n        trg_degree = await self.node_degree(tgt_id)\n\n        # Convert None to 0 for addition\n        src_degree = 0 if src_degree is None else src_degree\n        trg_degree = 0 if trg_degree is None else trg_degree\n\n        degrees = int(src_degree) + int(trg_degree)\n        logger.debug(\n            \"{%s}:query:src_Degree+trg_degree:result:{%s}\",\n            inspect.currentframe().f_code.co_name,\n            degrees,\n        )\n        return degrees\n\n    async def get_edge(\n        self, source_node_id: str, target_node_id: str\n    ) -> Union[dict, None]:\n        \"\"\"\n        Find all edges between nodes of two given names\n\n        Args:\n            source_node_id (str): Name of the source nodes\n            target_node_id (str): Name of the target nodes\n\n        Returns:\n            dict|None: Dict of found edge properties, or None if not found\n        \"\"\"\n        entity_name_source = GremlinStorage._fix_name(source_node_id)\n        entity_name_target = GremlinStorage._fix_name(target_node_id)\n        query = f\"\"\"g\n                 .V().has('graph', {self.graph_name})\n                 .has('entity_name', {entity_name_source})\n                 .outE()\n                 .inV().has('graph', {self.graph_name})\n                 .has('entity_name', {entity_name_target})\n                 .limit(1)\n                 .project('edge_properties')\n                 .by(__.bothE().elementMap())\n                 \"\"\"\n        result = await self._query(query)\n        if result:\n            edge_properties = result[0][\"edge_properties\"]\n            logger.debug(\n                \"{%s}:query:{%s}:result:{%s}\",\n                inspect.currentframe().f_code.co_name,\n                query,\n                edge_properties,\n            )\n            return edge_properties\n\n    async def get_node_edges(self, source_node_id: str) -> List[Tuple[str, str]]:\n        \"\"\"\n        Retrieves all edges (relationships) for a particular node identified by its name.\n        :return: List of tuples containing edge sources and targets\n        \"\"\"\n        node_name = GremlinStorage._fix_name(source_node_id)\n        query = f\"\"\"g\n                 .E()\n                 .filter(\n                     __.or(\n                         __.outV().has('graph', {self.graph_name})\n                           .has('entity_name', {node_name}),\n                         __.inV().has('graph', {self.graph_name})\n                           .has('entity_name', {node_name})\n                     )\n                 )\n                 .project('source_name', 'target_name')\n                 .by(__.outV().values('entity_name'))\n                 .by(__.inV().values('entity_name'))\n                 \"\"\"\n        result = await self._query(query)\n        edges = [(res[\"source_name\"], res[\"target_name\"]) for res in result]\n\n        return edges\n\n    @retry(\n        stop=stop_after_attempt(10),\n        wait=wait_exponential(multiplier=1, min=4, max=10),\n        retry=retry_if_exception_type((GremlinServerError,)),\n    )\n    async def upsert_node(self, node_id: str, node_data: Dict[str, Any]):\n        \"\"\"\n        Upsert a node in the Gremlin graph.\n\n        Args:\n            node_id: The unique identifier for the node (used as name)\n            node_data: Dictionary of node properties\n        \"\"\"\n        name = GremlinStorage._fix_name(node_id)\n        properties = GremlinStorage._convert_properties(node_data)\n\n        query = f\"\"\"g\n                 .V().has('graph', {self.graph_name})\n                 .has('entity_name', {name})\n                 .fold()\n                 .coalesce(\n                     __.unfold(),\n                     __.addV('ENTITY')\n                         .property('graph', {self.graph_name})\n                         .property('entity_name', {name})\n                 )\n                 {properties}\n                 \"\"\"\n\n        try:\n            await self._query(query)\n            logger.debug(\n                \"Upserted node with name {%s} and properties: {%s}\",\n                name,\n                properties,\n            )\n        except Exception as e:\n            logger.error(\"Error during upsert: {%s}\", e)\n            raise\n\n    @retry(\n        stop=stop_after_attempt(10),\n        wait=wait_exponential(multiplier=1, min=4, max=10),\n        retry=retry_if_exception_type((GremlinServerError,)),\n    )\n    async def upsert_edge(\n        self, source_node_id: str, target_node_id: str, edge_data: Dict[str, Any]\n    ):\n        \"\"\"\n        Upsert an edge and its properties between two nodes identified by their names.\n\n        Args:\n            source_node_id (str): Name of the source node (used as identifier)\n            target_node_id (str): Name of the target node (used as identifier)\n            edge_data (dict): Dictionary of properties to set on the edge\n        \"\"\"\n        source_node_name = GremlinStorage._fix_name(source_node_id)\n        target_node_name = GremlinStorage._fix_name(target_node_id)\n        edge_properties = GremlinStorage._convert_properties(edge_data)\n\n        query = f\"\"\"g\n                 .V().has('graph', {self.graph_name})\n                 .has('entity_name', {source_node_name}).as('source')\n                 .V().has('graph', {self.graph_name})\n                 .has('entity_name', {target_node_name}).as('target')\n                 .coalesce(\n                      __.select('source').outE('DIRECTED').where(__.inV().as('target')),\n                      __.select('source').addE('DIRECTED').to(__.select('target'))\n                  )\n                  .property('graph', {self.graph_name})\n                 {edge_properties}\n                 \"\"\"\n        try:\n            await self._query(query)\n            logger.debug(\n                \"Upserted edge from {%s} to {%s} with properties: {%s}\",\n                source_node_name,\n                target_node_name,\n                edge_properties,\n            )\n        except Exception as e:\n            logger.error(\"Error during edge upsert: {%s}\", e)\n            raise\n\n    async def _node2vec_embed(self):\n        print(\"Implemented but never called.\")\n"}
{"type": "source_file", "path": "lightrag/api/openai_lightrag_server.py", "content": "from fastapi import FastAPI, HTTPException, File, UploadFile, Form\nfrom pydantic import BaseModel\nimport asyncio\nimport logging\nimport argparse\nfrom lightrag import LightRAG, QueryParam\nfrom lightrag.llm import openai_complete_if_cache, openai_embedding\nfrom lightrag.utils import EmbeddingFunc\nfrom typing import Optional, List\nfrom enum import Enum\nfrom pathlib import Path\nimport shutil\nimport aiofiles\nfrom ascii_colors import trace_exception\nimport nest_asyncio\n\nimport os\n\nfrom fastapi import Depends, Security\nfrom fastapi.security import APIKeyHeader\nfrom fastapi.middleware.cors import CORSMiddleware\n\nfrom starlette.status import HTTP_403_FORBIDDEN\n\n# Apply nest_asyncio to solve event loop issues\nnest_asyncio.apply()\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(\n        description=\"LightRAG FastAPI Server with OpenAI integration\"\n    )\n\n    # Server configuration\n    parser.add_argument(\n        \"--host\", default=\"0.0.0.0\", help=\"Server host (default: 0.0.0.0)\"\n    )\n    parser.add_argument(\n        \"--port\", type=int, default=9621, help=\"Server port (default: 9621)\"\n    )\n\n    # Directory configuration\n    parser.add_argument(\n        \"--working-dir\",\n        default=\"./rag_storage\",\n        help=\"Working directory for RAG storage (default: ./rag_storage)\",\n    )\n    parser.add_argument(\n        \"--input-dir\",\n        default=\"./inputs\",\n        help=\"Directory containing input documents (default: ./inputs)\",\n    )\n\n    # Model configuration\n    parser.add_argument(\n        \"--model\", default=\"gpt-4\", help=\"OpenAI model name (default: gpt-4)\"\n    )\n    parser.add_argument(\n        \"--embedding-model\",\n        default=\"text-embedding-3-large\",\n        help=\"OpenAI embedding model (default: text-embedding-3-large)\",\n    )\n\n    # RAG configuration\n    parser.add_argument(\n        \"--max-tokens\",\n        type=int,\n        default=32768,\n        help=\"Maximum token size (default: 32768)\",\n    )\n    parser.add_argument(\n        \"--max-embed-tokens\",\n        type=int,\n        default=8192,\n        help=\"Maximum embedding token size (default: 8192)\",\n    )\n\n    # Logging configuration\n    parser.add_argument(\n        \"--log-level\",\n        default=\"INFO\",\n        choices=[\"DEBUG\", \"INFO\", \"WARNING\", \"ERROR\", \"CRITICAL\"],\n        help=\"Logging level (default: INFO)\",\n    )\n\n    parser.add_argument(\n        \"--key\",\n        type=str,\n        help=\"API key for authentication. This protects lightrag server against unauthorized access\",\n        default=None,\n    )\n\n    return parser.parse_args()\n\n\nclass DocumentManager:\n    \"\"\"Handles document operations and tracking\"\"\"\n\n    def __init__(self, input_dir: str, supported_extensions: tuple = (\".txt\", \".md\")):\n        self.input_dir = Path(input_dir)\n        self.supported_extensions = supported_extensions\n        self.indexed_files = set()\n\n        # Create input directory if it doesn't exist\n        self.input_dir.mkdir(parents=True, exist_ok=True)\n\n    def scan_directory(self) -> List[Path]:\n        \"\"\"Scan input directory for new files\"\"\"\n        new_files = []\n        for ext in self.supported_extensions:\n            for file_path in self.input_dir.rglob(f\"*{ext}\"):\n                if file_path not in self.indexed_files:\n                    new_files.append(file_path)\n        return new_files\n\n    def mark_as_indexed(self, file_path: Path):\n        \"\"\"Mark a file as indexed\"\"\"\n        self.indexed_files.add(file_path)\n\n    def is_supported_file(self, filename: str) -> bool:\n        \"\"\"Check if file type is supported\"\"\"\n        return any(filename.lower().endswith(ext) for ext in self.supported_extensions)\n\n\n# Pydantic models\nclass SearchMode(str, Enum):\n    naive = \"naive\"\n    local = \"local\"\n    global_ = \"global\"\n    hybrid = \"hybrid\"\n\n\nclass QueryRequest(BaseModel):\n    query: str\n    mode: SearchMode = SearchMode.hybrid\n    stream: bool = False\n    only_need_context: bool = False\n\n\nclass QueryResponse(BaseModel):\n    response: str\n\n\nclass InsertTextRequest(BaseModel):\n    text: str\n    description: Optional[str] = None\n\n\nclass InsertResponse(BaseModel):\n    status: str\n    message: str\n    document_count: int\n\n\ndef get_api_key_dependency(api_key: Optional[str]):\n    if not api_key:\n        # If no API key is configured, return a dummy dependency that always succeeds\n        async def no_auth():\n            return None\n\n        return no_auth\n\n    # If API key is configured, use proper authentication\n    api_key_header = APIKeyHeader(name=\"X-API-Key\", auto_error=False)\n\n    async def api_key_auth(api_key_header_value: str | None = Security(api_key_header)):\n        if not api_key_header_value:\n            raise HTTPException(\n                status_code=HTTP_403_FORBIDDEN, detail=\"API Key required\"\n            )\n        if api_key_header_value != api_key:\n            raise HTTPException(\n                status_code=HTTP_403_FORBIDDEN, detail=\"Invalid API Key\"\n            )\n        return api_key_header_value\n\n    return api_key_auth\n\n\nasync def get_embedding_dim(embedding_model: str) -> int:\n    \"\"\"Get embedding dimensions for the specified model\"\"\"\n    test_text = [\"This is a test sentence.\"]\n    embedding = await openai_embedding(test_text, model=embedding_model)\n    return embedding.shape[1]\n\n\ndef create_app(args):\n    # Setup logging\n    logging.basicConfig(\n        format=\"%(levelname)s:%(message)s\", level=getattr(logging, args.log_level)\n    )\n\n    # Check if API key is provided either through env var or args\n    api_key = os.getenv(\"LIGHTRAG_API_KEY\") or args.key\n\n    # Initialize FastAPI\n    app = FastAPI(\n        title=\"LightRAG API\",\n        description=\"API for querying text using LightRAG with separate storage and input directories\"\n        + \"(With authentication)\"\n        if api_key\n        else \"\",\n        version=\"1.0.0\",\n        openapi_tags=[{\"name\": \"api\"}],\n    )\n\n    # Add CORS middleware\n    app.add_middleware(\n        CORSMiddleware,\n        allow_origins=[\"*\"],\n        allow_credentials=True,\n        allow_methods=[\"*\"],\n        allow_headers=[\"*\"],\n    )\n\n    # Create the optional API key dependency\n    optional_api_key = get_api_key_dependency(api_key)\n\n    # Add CORS middleware\n    app.add_middleware(\n        CORSMiddleware,\n        allow_origins=[\"*\"],\n        allow_credentials=True,\n        allow_methods=[\"*\"],\n        allow_headers=[\"*\"],\n    )\n\n    # Create working directory if it doesn't exist\n    Path(args.working_dir).mkdir(parents=True, exist_ok=True)\n\n    # Initialize document manager\n    doc_manager = DocumentManager(args.input_dir)\n\n    # Get embedding dimensions\n    embedding_dim = asyncio.run(get_embedding_dim(args.embedding_model))\n\n    async def async_openai_complete(\n        prompt, system_prompt=None, history_messages=[], **kwargs\n    ):\n        \"\"\"Async wrapper for OpenAI completion\"\"\"\n        return await openai_complete_if_cache(\n            args.model,\n            prompt,\n            system_prompt=system_prompt,\n            history_messages=history_messages,\n            **kwargs,\n        )\n\n    # Initialize RAG with OpenAI configuration\n    rag = LightRAG(\n        working_dir=args.working_dir,\n        llm_model_func=async_openai_complete,\n        llm_model_name=args.model,\n        llm_model_max_token_size=args.max_tokens,\n        embedding_func=EmbeddingFunc(\n            embedding_dim=embedding_dim,\n            max_token_size=args.max_embed_tokens,\n            func=lambda texts: openai_embedding(texts, model=args.embedding_model),\n        ),\n    )\n\n    @app.on_event(\"startup\")\n    async def startup_event():\n        \"\"\"Index all files in input directory during startup\"\"\"\n        try:\n            new_files = doc_manager.scan_directory()\n            for file_path in new_files:\n                try:\n                    # Use async file reading\n                    async with aiofiles.open(file_path, \"r\", encoding=\"utf-8\") as f:\n                        content = await f.read()\n                        # Use the async version of insert directly\n                        await rag.ainsert(content)\n                        doc_manager.mark_as_indexed(file_path)\n                        logging.info(f\"Indexed file: {file_path}\")\n                except Exception as e:\n                    trace_exception(e)\n                    logging.error(f\"Error indexing file {file_path}: {str(e)}\")\n\n            logging.info(f\"Indexed {len(new_files)} documents from {args.input_dir}\")\n\n        except Exception as e:\n            logging.error(f\"Error during startup indexing: {str(e)}\")\n\n    @app.post(\"/documents/scan\", dependencies=[Depends(optional_api_key)])\n    async def scan_for_new_documents():\n        \"\"\"Manually trigger scanning for new documents\"\"\"\n        try:\n            new_files = doc_manager.scan_directory()\n            indexed_count = 0\n\n            for file_path in new_files:\n                try:\n                    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n                        content = f.read()\n                        rag.insert(content)\n                        doc_manager.mark_as_indexed(file_path)\n                        indexed_count += 1\n                except Exception as e:\n                    logging.error(f\"Error indexing file {file_path}: {str(e)}\")\n\n            return {\n                \"status\": \"success\",\n                \"indexed_count\": indexed_count,\n                \"total_documents\": len(doc_manager.indexed_files),\n            }\n        except Exception as e:\n            raise HTTPException(status_code=500, detail=str(e))\n\n    @app.post(\"/documents/upload\", dependencies=[Depends(optional_api_key)])\n    async def upload_to_input_dir(file: UploadFile = File(...)):\n        \"\"\"Upload a file to the input directory\"\"\"\n        try:\n            if not doc_manager.is_supported_file(file.filename):\n                raise HTTPException(\n                    status_code=400,\n                    detail=f\"Unsupported file type. Supported types: {doc_manager.supported_extensions}\",\n                )\n\n            file_path = doc_manager.input_dir / file.filename\n            with open(file_path, \"wb\") as buffer:\n                shutil.copyfileobj(file.file, buffer)\n\n            # Immediately index the uploaded file\n            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n                content = f.read()\n                rag.insert(content)\n                doc_manager.mark_as_indexed(file_path)\n\n            return {\n                \"status\": \"success\",\n                \"message\": f\"File uploaded and indexed: {file.filename}\",\n                \"total_documents\": len(doc_manager.indexed_files),\n            }\n        except Exception as e:\n            raise HTTPException(status_code=500, detail=str(e))\n\n    @app.post(\n        \"/query\", response_model=QueryResponse, dependencies=[Depends(optional_api_key)]\n    )\n    async def query_text(request: QueryRequest):\n        try:\n            response = await rag.aquery(\n                request.query,\n                param=QueryParam(\n                    mode=request.mode,\n                    stream=request.stream,\n                    only_need_context=request.only_need_context,\n                ),\n            )\n\n            if request.stream:\n                result = \"\"\n                async for chunk in response:\n                    result += chunk\n                return QueryResponse(response=result)\n            else:\n                return QueryResponse(response=response)\n        except Exception as e:\n            raise HTTPException(status_code=500, detail=str(e))\n\n    @app.post(\"/query/stream\", dependencies=[Depends(optional_api_key)])\n    async def query_text_stream(request: QueryRequest):\n        try:\n            response = rag.query(\n                request.query,\n                param=QueryParam(\n                    mode=request.mode,\n                    stream=True,\n                    only_need_context=request.only_need_context,\n                ),\n            )\n\n            async def stream_generator():\n                async for chunk in response:\n                    yield chunk\n\n            return stream_generator()\n        except Exception as e:\n            raise HTTPException(status_code=500, detail=str(e))\n\n    @app.post(\n        \"/documents/text\",\n        response_model=InsertResponse,\n        dependencies=[Depends(optional_api_key)],\n    )\n    async def insert_text(request: InsertTextRequest):\n        try:\n            rag.insert(request.text)\n            return InsertResponse(\n                status=\"success\",\n                message=\"Text successfully inserted\",\n                document_count=len(rag),\n            )\n        except Exception as e:\n            raise HTTPException(status_code=500, detail=str(e))\n\n    @app.post(\n        \"/documents/file\",\n        response_model=InsertResponse,\n        dependencies=[Depends(optional_api_key)],\n    )\n    async def insert_file(file: UploadFile = File(...), description: str = Form(None)):\n        try:\n            content = await file.read()\n\n            if file.filename.endswith((\".txt\", \".md\")):\n                text = content.decode(\"utf-8\")\n                rag.insert(text)\n            else:\n                raise HTTPException(\n                    status_code=400,\n                    detail=\"Unsupported file type. Only .txt and .md files are supported\",\n                )\n\n            return InsertResponse(\n                status=\"success\",\n                message=f\"File '{file.filename}' successfully inserted\",\n                document_count=1,\n            )\n        except UnicodeDecodeError:\n            raise HTTPException(status_code=400, detail=\"File encoding not supported\")\n        except Exception as e:\n            raise HTTPException(status_code=500, detail=str(e))\n\n    @app.post(\n        \"/documents/batch\",\n        response_model=InsertResponse,\n        dependencies=[Depends(optional_api_key)],\n    )\n    async def insert_batch(files: List[UploadFile] = File(...)):\n        try:\n            inserted_count = 0\n            failed_files = []\n\n            for file in files:\n                try:\n                    content = await file.read()\n                    if file.filename.endswith((\".txt\", \".md\")):\n                        text = content.decode(\"utf-8\")\n                        rag.insert(text)\n                        inserted_count += 1\n                    else:\n                        failed_files.append(f\"{file.filename} (unsupported type)\")\n                except Exception as e:\n                    failed_files.append(f\"{file.filename} ({str(e)})\")\n\n            status_message = f\"Successfully inserted {inserted_count} documents\"\n            if failed_files:\n                status_message += f\". Failed files: {', '.join(failed_files)}\"\n\n            return InsertResponse(\n                status=\"success\" if inserted_count > 0 else \"partial_success\",\n                message=status_message,\n                document_count=len(files),\n            )\n        except Exception as e:\n            raise HTTPException(status_code=500, detail=str(e))\n\n    @app.delete(\n        \"/documents\",\n        response_model=InsertResponse,\n        dependencies=[Depends(optional_api_key)],\n    )\n    async def clear_documents():\n        try:\n            rag.text_chunks = []\n            rag.entities_vdb = None\n            rag.relationships_vdb = None\n            return InsertResponse(\n                status=\"success\",\n                message=\"All documents cleared successfully\",\n                document_count=0,\n            )\n        except Exception as e:\n            raise HTTPException(status_code=500, detail=str(e))\n\n    @app.get(\"/health\", dependencies=[Depends(optional_api_key)])\n    async def get_status():\n        \"\"\"Get current system status\"\"\"\n        return {\n            \"status\": \"healthy\",\n            \"working_directory\": str(args.working_dir),\n            \"input_directory\": str(args.input_dir),\n            \"indexed_files\": len(doc_manager.indexed_files),\n            \"configuration\": {\n                \"model\": args.model,\n                \"embedding_model\": args.embedding_model,\n                \"max_tokens\": args.max_tokens,\n                \"embedding_dim\": embedding_dim,\n            },\n        }\n\n    return app\n\n\ndef main():\n    args = parse_args()\n    import uvicorn\n\n    app = create_app(args)\n    uvicorn.run(app, host=args.host, port=args.port)\n\n\nif __name__ == \"__main__\":\n    main()\n"}
{"type": "source_file", "path": "lightrag/kg/age_impl.py", "content": "import asyncio\nimport inspect\nimport json\nimport os\nimport sys\nfrom contextlib import asynccontextmanager\nfrom dataclasses import dataclass\nfrom typing import Any, Dict, List, NamedTuple, Optional, Tuple, Union\n\nimport psycopg\nfrom psycopg.rows import namedtuple_row\nfrom psycopg_pool import AsyncConnectionPool, PoolTimeout\nfrom tenacity import (\n    retry,\n    retry_if_exception_type,\n    stop_after_attempt,\n    wait_exponential,\n)\n\nfrom lightrag.utils import logger\n\nfrom ..base import BaseGraphStorage\n\nif sys.platform.startswith(\"win\"):\n    import asyncio.windows_events\n\n    asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())\n\n\nclass AGEQueryException(Exception):\n    \"\"\"Exception for the AGE queries.\"\"\"\n\n    def __init__(self, exception: Union[str, Dict]) -> None:\n        if isinstance(exception, dict):\n            self.message = exception[\"message\"] if \"message\" in exception else \"unknown\"\n            self.details = exception[\"details\"] if \"details\" in exception else \"unknown\"\n        else:\n            self.message = exception\n            self.details = \"unknown\"\n\n    def get_message(self) -> str:\n        return self.message\n\n    def get_details(self) -> Any:\n        return self.details\n\n\n@dataclass\nclass AGEStorage(BaseGraphStorage):\n    @staticmethod\n    def load_nx_graph(file_name):\n        print(\"no preloading of graph with AGE in production\")\n\n    def __init__(self, namespace, global_config, embedding_func):\n        super().__init__(\n            namespace=namespace,\n            global_config=global_config,\n            embedding_func=embedding_func,\n        )\n        self._driver = None\n        self._driver_lock = asyncio.Lock()\n        DB = os.environ[\"AGE_POSTGRES_DB\"].replace(\"\\\\\", \"\\\\\\\\\").replace(\"'\", \"\\\\'\")\n        USER = os.environ[\"AGE_POSTGRES_USER\"].replace(\"\\\\\", \"\\\\\\\\\").replace(\"'\", \"\\\\'\")\n        PASSWORD = (\n            os.environ[\"AGE_POSTGRES_PASSWORD\"]\n            .replace(\"\\\\\", \"\\\\\\\\\")\n            .replace(\"'\", \"\\\\'\")\n        )\n        HOST = os.environ[\"AGE_POSTGRES_HOST\"].replace(\"\\\\\", \"\\\\\\\\\").replace(\"'\", \"\\\\'\")\n        PORT = int(os.environ[\"AGE_POSTGRES_PORT\"])\n        self.graph_name = os.environ[\"AGE_GRAPH_NAME\"]\n\n        connection_string = f\"dbname='{DB}' user='{USER}' password='{PASSWORD}' host='{HOST}' port={PORT}\"\n\n        self._driver = AsyncConnectionPool(connection_string, open=False)\n\n        return None\n\n    def __post_init__(self):\n        self._node_embed_algorithms = {\n            \"node2vec\": self._node2vec_embed,\n        }\n\n    async def close(self):\n        if self._driver:\n            await self._driver.close()\n            self._driver = None\n\n    async def __aexit__(self, exc_type, exc, tb):\n        if self._driver:\n            await self._driver.close()\n\n    async def index_done_callback(self):\n        print(\"KG successfully indexed.\")\n\n    @staticmethod\n    def _record_to_dict(record: NamedTuple) -> Dict[str, Any]:\n        \"\"\"\n        Convert a record returned from an age query to a dictionary\n\n        Args:\n            record (): a record from an age query result\n\n        Returns:\n            Dict[str, Any]: a dictionary representation of the record where\n                the dictionary key is the field name and the value is the\n                value converted to a python type\n        \"\"\"\n        # result holder\n        d = {}\n\n        # prebuild a mapping of vertex_id to vertex mappings to be used\n        # later to build edges\n        vertices = {}\n        for k in record._fields:\n            v = getattr(record, k)\n            # agtype comes back '{key: value}::type' which must be parsed\n            if isinstance(v, str) and \"::\" in v:\n                dtype = v.split(\"::\")[-1]\n                v = v.split(\"::\")[0]\n                if dtype == \"vertex\":\n                    vertex = json.loads(v)\n                    vertices[vertex[\"id\"]] = vertex.get(\"properties\")\n\n        # iterate returned fields and parse appropriately\n        for k in record._fields:\n            v = getattr(record, k)\n            if isinstance(v, str) and \"::\" in v:\n                dtype = v.split(\"::\")[-1]\n                v = v.split(\"::\")[0]\n            else:\n                dtype = \"\"\n\n            if dtype == \"vertex\":\n                vertex = json.loads(v)\n                field = json.loads(v).get(\"properties\")\n                if not field:\n                    field = {}\n                field[\"label\"] = AGEStorage._decode_graph_label(vertex[\"label\"])\n                d[k] = field\n            # convert edge from id-label->id by replacing id with node information\n            # we only do this if the vertex was also returned in the query\n            # this is an attempt to be consistent with neo4j implementation\n            elif dtype == \"edge\":\n                edge = json.loads(v)\n                d[k] = (\n                    vertices.get(edge[\"start_id\"], {}),\n                    edge[\n                        \"label\"\n                    ],  # we don't use decode_graph_label(), since edge label is always \"DIRECTED\"\n                    vertices.get(edge[\"end_id\"], {}),\n                )\n            else:\n                d[k] = json.loads(v) if isinstance(v, str) else v\n\n        return d\n\n    @staticmethod\n    def _format_properties(\n        properties: Dict[str, Any], _id: Union[str, None] = None\n    ) -> str:\n        \"\"\"\n        Convert a dictionary of properties to a string representation that\n        can be used in a cypher query insert/merge statement.\n\n        Args:\n            properties (Dict[str,str]): a dictionary containing node/edge properties\n            id (Union[str, None]): the id of the node or None if none exists\n\n        Returns:\n            str: the properties dictionary as a properly formatted string\n        \"\"\"\n        props = []\n        # wrap property key in backticks to escape\n        for k, v in properties.items():\n            prop = f\"`{k}`: {json.dumps(v)}\"\n            props.append(prop)\n        if _id is not None and \"id\" not in properties:\n            props.append(\n                f\"id: {json.dumps(_id)}\" if isinstance(_id, str) else f\"id: {_id}\"\n            )\n        return \"{\" + \", \".join(props) + \"}\"\n\n    @staticmethod\n    def _encode_graph_label(label: str) -> str:\n        \"\"\"\n        Since AGE suports only alphanumerical labels, we will encode generic label as HEX string\n\n        Args:\n            label (str): the original label\n\n        Returns:\n            str: the encoded label\n        \"\"\"\n        return \"x\" + label.encode().hex()\n\n    @staticmethod\n    def _decode_graph_label(encoded_label: str) -> str:\n        \"\"\"\n        Since AGE suports only alphanumerical labels, we will encode generic label as HEX string\n\n        Args:\n            encoded_label (str): the encoded label\n\n        Returns:\n            str: the decoded label\n        \"\"\"\n        return bytes.fromhex(encoded_label.removeprefix(\"x\")).decode()\n\n    @staticmethod\n    def _get_col_name(field: str, idx: int) -> str:\n        \"\"\"\n        Convert a cypher return field to a pgsql select field\n        If possible keep the cypher column name, but create a generic name if necessary\n\n        Args:\n            field (str): a return field from a cypher query to be formatted for pgsql\n            idx (int): the position of the field in the return statement\n\n        Returns:\n            str: the field to be used in the pgsql select statement\n        \"\"\"\n        # remove white space\n        field = field.strip()\n        # if an alias is provided for the field, use it\n        if \" as \" in field:\n            return field.split(\" as \")[-1].strip()\n        # if the return value is an unnamed primitive, give it a generic name\n        if field.isnumeric() or field in (\"true\", \"false\", \"null\"):\n            return f\"column_{idx}\"\n        # otherwise return the value stripping out some common special chars\n        return field.replace(\"(\", \"_\").replace(\")\", \"\")\n\n    @staticmethod\n    def _wrap_query(query: str, graph_name: str, **params: str) -> str:\n        \"\"\"\n        Convert a cypher query to an Apache Age compatible\n        sql query by wrapping the cypher query in ag_catalog.cypher,\n        casting results to agtype and building a select statement\n\n        Args:\n            query (str): a valid cypher query\n            graph_name (str): the name of the graph to query\n            params (dict): parameters for the query\n\n        Returns:\n            str: an equivalent pgsql query\n        \"\"\"\n\n        # pgsql template\n        template = \"\"\"SELECT {projection} FROM ag_catalog.cypher('{graph_name}', $$\n            {query}\n        $$) AS ({fields});\"\"\"\n\n        # if there are any returned fields they must be added to the pgsql query\n        if \"return\" in query.lower():\n            # parse return statement to identify returned fields\n            fields = (\n                query.lower()\n                .split(\"return\")[-1]\n                .split(\"distinct\")[-1]\n                .split(\"order by\")[0]\n                .split(\"skip\")[0]\n                .split(\"limit\")[0]\n                .split(\",\")\n            )\n\n            # raise exception if RETURN * is found as we can't resolve the fields\n            if \"*\" in [x.strip() for x in fields]:\n                raise ValueError(\n                    \"AGE graph does not support 'RETURN *'\"\n                    + \" statements in Cypher queries\"\n                )\n\n            # get pgsql formatted field names\n            fields = [\n                AGEStorage._get_col_name(field, idx) for idx, field in enumerate(fields)\n            ]\n\n            # build resulting pgsql relation\n            fields_str = \", \".join(\n                [field.split(\".\")[-1] + \" agtype\" for field in fields]\n            )\n\n        # if no return statement we still need to return a single field of type agtype\n        else:\n            fields_str = \"a agtype\"\n\n        select_str = \"*\"\n\n        return template.format(\n            graph_name=graph_name,\n            query=query.format(**params),\n            fields=fields_str,\n            projection=select_str,\n        )\n\n    async def _query(self, query: str, **params: str) -> List[Dict[str, Any]]:\n        \"\"\"\n        Query the graph by taking a cypher query, converting it to an\n        age compatible query, executing it and converting the result\n\n        Args:\n            query (str): a cypher query to be executed\n            params (dict): parameters for the query\n\n        Returns:\n            List[Dict[str, Any]]: a list of dictionaries containing the result set\n        \"\"\"\n        # convert cypher query to pgsql/age query\n        wrapped_query = self._wrap_query(query, self.graph_name, **params)\n\n        await self._driver.open()\n\n        # create graph if it doesn't exist\n        async with self._get_pool_connection() as conn:\n            async with conn.cursor() as curs:\n                try:\n                    await curs.execute('SET search_path = ag_catalog, \"$user\", public')\n                    await curs.execute(f\"SELECT create_graph('{self.graph_name}')\")\n                    await conn.commit()\n                except (\n                    psycopg.errors.InvalidSchemaName,\n                    psycopg.errors.UniqueViolation,\n                ):\n                    await conn.rollback()\n\n        # execute the query, rolling back on an error\n        async with self._get_pool_connection() as conn:\n            async with conn.cursor(row_factory=namedtuple_row) as curs:\n                try:\n                    await curs.execute('SET search_path = ag_catalog, \"$user\", public')\n                    await curs.execute(wrapped_query)\n                    await conn.commit()\n                except psycopg.Error as e:\n                    await conn.rollback()\n                    raise AGEQueryException(\n                        {\n                            \"message\": f\"Error executing graph query: {query.format(**params)}\",\n                            \"detail\": str(e),\n                        }\n                    ) from e\n\n                data = await curs.fetchall()\n                if data is None:\n                    result = []\n                # decode records\n                else:\n                    result = [AGEStorage._record_to_dict(d) for d in data]\n\n                return result\n\n    async def has_node(self, node_id: str) -> bool:\n        entity_name_label = node_id.strip('\"')\n\n        query = \"\"\"\n                MATCH (n:`{label}`) RETURN count(n) > 0 AS node_exists\n                \"\"\"\n        params = {\"label\": AGEStorage._encode_graph_label(entity_name_label)}\n        single_result = (await self._query(query, **params))[0]\n        logger.debug(\n            \"{%s}:query:{%s}:result:{%s}\",\n            inspect.currentframe().f_code.co_name,\n            query.format(**params),\n            single_result[\"node_exists\"],\n        )\n\n        return single_result[\"node_exists\"]\n\n    async def has_edge(self, source_node_id: str, target_node_id: str) -> bool:\n        entity_name_label_source = source_node_id.strip('\"')\n        entity_name_label_target = target_node_id.strip('\"')\n\n        query = \"\"\"\n                MATCH (a:`{src_label}`)-[r]-(b:`{tgt_label}`)\n                RETURN COUNT(r) > 0 AS edge_exists\n                \"\"\"\n        params = {\n            \"src_label\": AGEStorage._encode_graph_label(entity_name_label_source),\n            \"tgt_label\": AGEStorage._encode_graph_label(entity_name_label_target),\n        }\n        single_result = (await self._query(query, **params))[0]\n        logger.debug(\n            \"{%s}:query:{%s}:result:{%s}\",\n            inspect.currentframe().f_code.co_name,\n            query.format(**params),\n            single_result[\"edge_exists\"],\n        )\n        return single_result[\"edge_exists\"]\n\n    async def get_node(self, node_id: str) -> Union[dict, None]:\n        entity_name_label = node_id.strip('\"')\n        query = \"\"\"\n                MATCH (n:`{label}`) RETURN n\n                \"\"\"\n        params = {\"label\": AGEStorage._encode_graph_label(entity_name_label)}\n        record = await self._query(query, **params)\n        if record:\n            node = record[0]\n            node_dict = node[\"n\"]\n            logger.debug(\n                \"{%s}: query: {%s}, result: {%s}\",\n                inspect.currentframe().f_code.co_name,\n                query.format(**params),\n                node_dict,\n            )\n            return node_dict\n        return None\n\n    async def node_degree(self, node_id: str) -> int:\n        entity_name_label = node_id.strip('\"')\n\n        query = \"\"\"\n                MATCH (n:`{label}`)-[]->(x)\n                RETURN count(x) AS total_edge_count\n                \"\"\"\n        params = {\"label\": AGEStorage._encode_graph_label(entity_name_label)}\n        record = (await self._query(query, **params))[0]\n        if record:\n            edge_count = int(record[\"total_edge_count\"])\n            logger.debug(\n                \"{%s}:query:{%s}:result:{%s}\",\n                inspect.currentframe().f_code.co_name,\n                query.format(**params),\n                edge_count,\n            )\n            return edge_count\n\n    async def edge_degree(self, src_id: str, tgt_id: str) -> int:\n        entity_name_label_source = src_id.strip('\"')\n        entity_name_label_target = tgt_id.strip('\"')\n        src_degree = await self.node_degree(entity_name_label_source)\n        trg_degree = await self.node_degree(entity_name_label_target)\n\n        # Convert None to 0 for addition\n        src_degree = 0 if src_degree is None else src_degree\n        trg_degree = 0 if trg_degree is None else trg_degree\n\n        degrees = int(src_degree) + int(trg_degree)\n        logger.debug(\n            \"{%s}:query:src_Degree+trg_degree:result:{%s}\",\n            inspect.currentframe().f_code.co_name,\n            degrees,\n        )\n        return degrees\n\n    async def get_edge(\n        self, source_node_id: str, target_node_id: str\n    ) -> Union[dict, None]:\n        \"\"\"\n        Find all edges between nodes of two given labels\n\n        Args:\n            source_node_label (str): Label of the source nodes\n            target_node_label (str): Label of the target nodes\n\n        Returns:\n            list: List of all relationships/edges found\n        \"\"\"\n        entity_name_label_source = source_node_id.strip('\"')\n        entity_name_label_target = target_node_id.strip('\"')\n\n        query = \"\"\"\n                MATCH (a:`{src_label}`)-[r]->(b:`{tgt_label}`)\n                RETURN properties(r) as edge_properties\n                LIMIT 1\n                \"\"\"\n        params = {\n            \"src_label\": AGEStorage._encode_graph_label(entity_name_label_source),\n            \"tgt_label\": AGEStorage._encode_graph_label(entity_name_label_target),\n        }\n        record = await self._query(query, **params)\n        if record and record[0] and record[0][\"edge_properties\"]:\n            result = record[0][\"edge_properties\"]\n            logger.debug(\n                \"{%s}:query:{%s}:result:{%s}\",\n                inspect.currentframe().f_code.co_name,\n                query.format(**params),\n                result,\n            )\n            return result\n\n    async def get_node_edges(self, source_node_id: str) -> List[Tuple[str, str]]:\n        \"\"\"\n        Retrieves all edges (relationships) for a particular node identified by its label.\n        :return: List of dictionaries containing edge information\n        \"\"\"\n        node_label = source_node_id.strip('\"')\n\n        query = \"\"\"\n                MATCH (n:`{label}`)\n                OPTIONAL MATCH (n)-[r]-(connected)\n                RETURN n, r, connected\n                \"\"\"\n        params = {\"label\": AGEStorage._encode_graph_label(node_label)}\n        results = await self._query(query, **params)\n        edges = []\n        for record in results:\n            source_node = record[\"n\"] if record[\"n\"] else None\n            connected_node = record[\"connected\"] if record[\"connected\"] else None\n\n            source_label = (\n                source_node[\"label\"] if source_node and source_node[\"label\"] else None\n            )\n            target_label = (\n                connected_node[\"label\"]\n                if connected_node and connected_node[\"label\"]\n                else None\n            )\n\n            if source_label and target_label:\n                edges.append((source_label, target_label))\n\n        return edges\n\n    @retry(\n        stop=stop_after_attempt(3),\n        wait=wait_exponential(multiplier=1, min=4, max=10),\n        retry=retry_if_exception_type((AGEQueryException,)),\n    )\n    async def upsert_node(self, node_id: str, node_data: Dict[str, Any]):\n        \"\"\"\n        Upsert a node in the AGE database.\n\n        Args:\n            node_id: The unique identifier for the node (used as label)\n            node_data: Dictionary of node properties\n        \"\"\"\n        label = node_id.strip('\"')\n        properties = node_data\n\n        query = \"\"\"\n                MERGE (n:`{label}`)\n                SET n += {properties}\n                \"\"\"\n        params = {\n            \"label\": AGEStorage._encode_graph_label(label),\n            \"properties\": AGEStorage._format_properties(properties),\n        }\n        try:\n            await self._query(query, **params)\n            logger.debug(\n                \"Upserted node with label '{%s}' and properties: {%s}\",\n                label,\n                properties,\n            )\n        except Exception as e:\n            logger.error(\"Error during upsert: {%s}\", e)\n            raise\n\n    @retry(\n        stop=stop_after_attempt(3),\n        wait=wait_exponential(multiplier=1, min=4, max=10),\n        retry=retry_if_exception_type((AGEQueryException,)),\n    )\n    async def upsert_edge(\n        self, source_node_id: str, target_node_id: str, edge_data: Dict[str, Any]\n    ):\n        \"\"\"\n        Upsert an edge and its properties between two nodes identified by their labels.\n\n        Args:\n            source_node_id (str): Label of the source node (used as identifier)\n            target_node_id (str): Label of the target node (used as identifier)\n            edge_data (dict): Dictionary of properties to set on the edge\n        \"\"\"\n        source_node_label = source_node_id.strip('\"')\n        target_node_label = target_node_id.strip('\"')\n        edge_properties = edge_data\n\n        query = \"\"\"\n                MATCH (source:`{src_label}`)\n                WITH source\n                MATCH (target:`{tgt_label}`)\n                MERGE (source)-[r:DIRECTED]->(target)\n                SET r += {properties}\n                RETURN r\n                \"\"\"\n        params = {\n            \"src_label\": AGEStorage._encode_graph_label(source_node_label),\n            \"tgt_label\": AGEStorage._encode_graph_label(target_node_label),\n            \"properties\": AGEStorage._format_properties(edge_properties),\n        }\n        try:\n            await self._query(query, **params)\n            logger.debug(\n                \"Upserted edge from '{%s}' to '{%s}' with properties: {%s}\",\n                source_node_label,\n                target_node_label,\n                edge_properties,\n            )\n        except Exception as e:\n            logger.error(\"Error during edge upsert: {%s}\", e)\n            raise\n\n    async def _node2vec_embed(self):\n        print(\"Implemented but never called.\")\n\n    @asynccontextmanager\n    async def _get_pool_connection(self, timeout: Optional[float] = None):\n        \"\"\"Workaround for a psycopg_pool bug\"\"\"\n\n        try:\n            connection = await self._driver.getconn(timeout=timeout)\n        except PoolTimeout:\n            await self._driver._add_connection(None)  # workaround...\n            connection = await self._driver.getconn(timeout=timeout)\n\n        try:\n            async with connection:\n                yield connection\n        finally:\n            await self._driver.putconn(connection)\n"}
{"type": "source_file", "path": "lightrag/kg/__init__.py", "content": "# print (\"init package vars here. ......\")\n"}
{"type": "source_file", "path": "lightrag/base.py", "content": "from dataclasses import dataclass, field\nfrom typing import TypedDict, Union, Literal, Generic, TypeVar, Optional, Dict, Any\nfrom enum import Enum\n\nimport numpy as np\n\nfrom .utils import EmbeddingFunc\n\nTextChunkSchema = TypedDict(\n    \"TextChunkSchema\",\n    {\"tokens\": int, \"content\": str, \"full_doc_id\": str, \"chunk_order_index\": int},\n)\n\nT = TypeVar(\"T\")\n\n\n@dataclass\nclass QueryParam:\n    mode: Literal[\"local\", \"global\", \"hybrid\", \"naive\", \"mix\"] = \"global\"\n    only_need_context: bool = False\n    only_need_prompt: bool = False\n    response_type: str = \"Multiple Paragraphs\"\n    stream: bool = False\n    # Number of top-k items to retrieve; corresponds to entities in \"local\" mode and relationships in \"global\" mode.\n    top_k: int = 60\n    # Number of document chunks to retrieve.\n    # top_n: int = 10\n    # Number of tokens for the original chunks.\n    max_token_for_text_unit: int = 4000\n    # Number of tokens for the relationship descriptions\n    max_token_for_global_context: int = 4000\n    # Number of tokens for the entity descriptions\n    max_token_for_local_context: int = 4000\n\n\n@dataclass\nclass StorageNameSpace:\n    namespace: str\n    global_config: dict\n\n    async def index_done_callback(self):\n        \"\"\"commit the storage operations after indexing\"\"\"\n        pass\n\n    async def query_done_callback(self):\n        \"\"\"commit the storage operations after querying\"\"\"\n        pass\n\n\n@dataclass\nclass BaseVectorStorage(StorageNameSpace):\n    embedding_func: EmbeddingFunc\n    meta_fields: set = field(default_factory=set)\n\n    async def query(self, query: str, top_k: int) -> list[dict]:\n        raise NotImplementedError\n\n    async def upsert(self, data: dict[str, dict]):\n        \"\"\"Use 'content' field from value for embedding, use key as id.\n        If embedding_func is None, use 'embedding' field from value\n        \"\"\"\n        raise NotImplementedError\n\n\n@dataclass\nclass BaseKVStorage(Generic[T], StorageNameSpace):\n    embedding_func: EmbeddingFunc\n\n    async def all_keys(self) -> list[str]:\n        raise NotImplementedError\n\n    async def get_by_id(self, id: str) -> Union[T, None]:\n        raise NotImplementedError\n\n    async def get_by_ids(\n        self, ids: list[str], fields: Union[set[str], None] = None\n    ) -> list[Union[T, None]]:\n        raise NotImplementedError\n\n    async def filter_keys(self, data: list[str]) -> set[str]:\n        \"\"\"return un-exist keys\"\"\"\n        raise NotImplementedError\n\n    async def upsert(self, data: dict[str, T]):\n        raise NotImplementedError\n\n    async def drop(self):\n        raise NotImplementedError\n\n\n@dataclass\nclass BaseGraphStorage(StorageNameSpace):\n    embedding_func: EmbeddingFunc = None\n\n    async def has_node(self, node_id: str) -> bool:\n        raise NotImplementedError\n\n    async def has_edge(self, source_node_id: str, target_node_id: str) -> bool:\n        raise NotImplementedError\n\n    async def node_degree(self, node_id: str) -> int:\n        raise NotImplementedError\n\n    async def edge_degree(self, src_id: str, tgt_id: str) -> int:\n        raise NotImplementedError\n\n    async def get_node(self, node_id: str) -> Union[dict, None]:\n        raise NotImplementedError\n\n    async def get_edge(\n        self, source_node_id: str, target_node_id: str\n    ) -> Union[dict, None]:\n        raise NotImplementedError\n\n    async def get_node_edges(\n        self, source_node_id: str\n    ) -> Union[list[tuple[str, str]], None]:\n        raise NotImplementedError\n\n    async def upsert_node(self, node_id: str, node_data: dict[str, str]):\n        raise NotImplementedError\n\n    async def upsert_edge(\n        self, source_node_id: str, target_node_id: str, edge_data: dict[str, str]\n    ):\n        raise NotImplementedError\n\n    async def delete_node(self, node_id: str):\n        raise NotImplementedError\n\n    async def embed_nodes(self, algorithm: str) -> tuple[np.ndarray, list[str]]:\n        raise NotImplementedError(\"Node embedding is not used in lightrag.\")\n\n\nclass DocStatus(str, Enum):\n    \"\"\"Document processing status enum\"\"\"\n\n    PENDING = \"pending\"\n    PROCESSING = \"processing\"\n    PROCESSED = \"processed\"\n    FAILED = \"failed\"\n\n\n@dataclass\nclass DocProcessingStatus:\n    \"\"\"Document processing status data structure\"\"\"\n\n    content_summary: str  # First 100 chars of document content\n    content_length: int  # Total length of document\n    status: DocStatus  # Current processing status\n    created_at: str  # ISO format timestamp\n    updated_at: str  # ISO format timestamp\n    chunks_count: Optional[int] = None  # Number of chunks after splitting\n    error: Optional[str] = None  # Error message if failed\n    metadata: Dict[str, Any] = field(default_factory=dict)  # Additional metadata\n\n\nclass DocStatusStorage(BaseKVStorage):\n    \"\"\"Base class for document status storage\"\"\"\n\n    async def get_status_counts(self) -> Dict[str, int]:\n        \"\"\"Get counts of documents in each status\"\"\"\n        raise NotImplementedError\n\n    async def get_failed_docs(self) -> Dict[str, DocProcessingStatus]:\n        \"\"\"Get all failed documents\"\"\"\n        raise NotImplementedError\n\n    async def get_pending_docs(self) -> Dict[str, DocProcessingStatus]:\n        \"\"\"Get all pending documents\"\"\"\n        raise NotImplementedError\n"}
{"type": "source_file", "path": "lightrag/kg/mongo_impl.py", "content": "import os\nfrom tqdm.asyncio import tqdm as tqdm_async\nfrom dataclasses import dataclass\nfrom pymongo import MongoClient\n\nfrom lightrag.utils import logger\n\nfrom lightrag.base import BaseKVStorage\n\n\n@dataclass\nclass MongoKVStorage(BaseKVStorage):\n    def __post_init__(self):\n        client = MongoClient(\n            os.environ.get(\"MONGO_URI\", \"mongodb://root:root@localhost:27017/\")\n        )\n        database = client.get_database(os.environ.get(\"MONGO_DATABASE\", \"LightRAG\"))\n        self._data = database.get_collection(self.namespace)\n        logger.info(f\"Use MongoDB as KV {self.namespace}\")\n\n    async def all_keys(self) -> list[str]:\n        return [x[\"_id\"] for x in self._data.find({}, {\"_id\": 1})]\n\n    async def get_by_id(self, id):\n        return self._data.find_one({\"_id\": id})\n\n    async def get_by_ids(self, ids, fields=None):\n        if fields is None:\n            return list(self._data.find({\"_id\": {\"$in\": ids}}))\n        return list(\n            self._data.find(\n                {\"_id\": {\"$in\": ids}},\n                {field: 1 for field in fields},\n            )\n        )\n\n    async def filter_keys(self, data: list[str]) -> set[str]:\n        existing_ids = [\n            str(x[\"_id\"]) for x in self._data.find({\"_id\": {\"$in\": data}}, {\"_id\": 1})\n        ]\n        return set([s for s in data if s not in existing_ids])\n\n    async def upsert(self, data: dict[str, dict]):\n        for k, v in tqdm_async(data.items(), desc=\"Upserting\"):\n            self._data.update_one({\"_id\": k}, {\"$set\": v}, upsert=True)\n            data[k][\"_id\"] = k\n        return data\n\n    async def drop(self):\n        \"\"\" \"\"\"\n        pass\n"}
{"type": "source_file", "path": "lightrag/kg/chroma_impl.py", "content": "import asyncio\nfrom dataclasses import dataclass\nfrom typing import Union\nimport numpy as np\nfrom chromadb import HttpClient\nfrom chromadb.config import Settings\nfrom lightrag.base import BaseVectorStorage\nfrom lightrag.utils import logger\n\n\n@dataclass\nclass ChromaVectorDBStorage(BaseVectorStorage):\n    \"\"\"ChromaDB vector storage implementation.\"\"\"\n\n    cosine_better_than_threshold: float = 0.2\n\n    def __post_init__(self):\n        try:\n            # Use global config value if specified, otherwise use default\n            self.cosine_better_than_threshold = self.global_config.get(\n                \"cosine_better_than_threshold\", self.cosine_better_than_threshold\n            )\n\n            config = self.global_config.get(\"vector_db_storage_cls_kwargs\", {})\n            user_collection_settings = config.get(\"collection_settings\", {})\n            # Default HNSW index settings for ChromaDB\n            default_collection_settings = {\n                # Distance metric used for similarity search (cosine similarity)\n                \"hnsw:space\": \"cosine\",\n                # Number of nearest neighbors to explore during index construction\n                # Higher values = better recall but slower indexing\n                \"hnsw:construction_ef\": 128,\n                # Number of nearest neighbors to explore during search\n                # Higher values = better recall but slower search\n                \"hnsw:search_ef\": 128,\n                # Number of connections per node in the HNSW graph\n                # Higher values = better recall but more memory usage\n                \"hnsw:M\": 16,\n                # Number of vectors to process in one batch during indexing\n                \"hnsw:batch_size\": 100,\n                # Number of updates before forcing index synchronization\n                # Lower values = more frequent syncs but slower indexing\n                \"hnsw:sync_threshold\": 1000,\n            }\n            collection_settings = {\n                **default_collection_settings,\n                **user_collection_settings,\n            }\n\n            auth_provider = config.get(\n                \"auth_provider\", \"chromadb.auth.token_authn.TokenAuthClientProvider\"\n            )\n            auth_credentials = config.get(\"auth_token\", \"secret-token\")\n            headers = {}\n\n            if \"token_authn\" in auth_provider:\n                headers = {\n                    config.get(\"auth_header_name\", \"X-Chroma-Token\"): auth_credentials\n                }\n            elif \"basic_authn\" in auth_provider:\n                auth_credentials = config.get(\"auth_credentials\", \"admin:admin\")\n\n            self._client = HttpClient(\n                host=config.get(\"host\", \"localhost\"),\n                port=config.get(\"port\", 8000),\n                headers=headers,\n                settings=Settings(\n                    chroma_api_impl=\"rest\",\n                    chroma_client_auth_provider=auth_provider,\n                    chroma_client_auth_credentials=auth_credentials,\n                    allow_reset=True,\n                    anonymized_telemetry=False,\n                ),\n            )\n\n            self._collection = self._client.get_or_create_collection(\n                name=self.namespace,\n                metadata={\n                    **collection_settings,\n                    \"dimension\": self.embedding_func.embedding_dim,\n                },\n            )\n            # Use batch size from collection settings if specified\n            self._max_batch_size = self.global_config.get(\n                \"embedding_batch_num\", collection_settings.get(\"hnsw:batch_size\", 32)\n            )\n        except Exception as e:\n            logger.error(f\"ChromaDB initialization failed: {str(e)}\")\n            raise\n\n    async def upsert(self, data: dict[str, dict]):\n        if not data:\n            logger.warning(\"Empty data provided to vector DB\")\n            return []\n\n        try:\n            ids = list(data.keys())\n            documents = [v[\"content\"] for v in data.values()]\n            metadatas = [\n                {k: v for k, v in item.items() if k in self.meta_fields}\n                or {\"_default\": \"true\"}\n                for item in data.values()\n            ]\n\n            # Process in batches\n            batches = [\n                documents[i : i + self._max_batch_size]\n                for i in range(0, len(documents), self._max_batch_size)\n            ]\n\n            embedding_tasks = [self.embedding_func(batch) for batch in batches]\n            embeddings_list = []\n\n            # Pre-allocate embeddings_list with known size\n            embeddings_list = [None] * len(embedding_tasks)\n\n            # Use asyncio.gather instead of as_completed if order doesn't matter\n            embeddings_results = await asyncio.gather(*embedding_tasks)\n            embeddings_list = list(embeddings_results)\n\n            embeddings = np.concatenate(embeddings_list)\n\n            # Upsert in batches\n            for i in range(0, len(ids), self._max_batch_size):\n                batch_slice = slice(i, i + self._max_batch_size)\n\n                self._collection.upsert(\n                    ids=ids[batch_slice],\n                    embeddings=embeddings[batch_slice].tolist(),\n                    documents=documents[batch_slice],\n                    metadatas=metadatas[batch_slice],\n                )\n\n            return ids\n\n        except Exception as e:\n            logger.error(f\"Error during ChromaDB upsert: {str(e)}\")\n            raise\n\n    async def query(self, query: str, top_k=5) -> Union[dict, list[dict]]:\n        try:\n            embedding = await self.embedding_func([query])\n\n            results = self._collection.query(\n                query_embeddings=embedding.tolist(),\n                n_results=top_k * 2,  # Request more results to allow for filtering\n                include=[\"metadatas\", \"distances\", \"documents\"],\n            )\n\n            # Filter results by cosine similarity threshold and take top k\n            # We request 2x results initially to have enough after filtering\n            # ChromaDB returns cosine similarity (1 = identical, 0 = orthogonal)\n            # We convert to distance (0 = identical, 1 = orthogonal) via (1 - similarity)\n            # Only keep results with distance below threshold, then take top k\n            return [\n                {\n                    \"id\": results[\"ids\"][0][i],\n                    \"distance\": 1 - results[\"distances\"][0][i],\n                    \"content\": results[\"documents\"][0][i],\n                    **results[\"metadatas\"][0][i],\n                }\n                for i in range(len(results[\"ids\"][0]))\n                if (1 - results[\"distances\"][0][i]) >= self.cosine_better_than_threshold\n            ][:top_k]\n\n        except Exception as e:\n            logger.error(f\"Error during ChromaDB query: {str(e)}\")\n            raise\n\n    async def index_done_callback(self):\n        # ChromaDB handles persistence automatically\n        pass\n"}
