{"repo_info": {"repo_name": "LightRAG-for-OpenAI-Standard-Frontend", "repo_owner": "HerSophia", "repo_url": "https://github.com/HerSophia/LightRAG-for-OpenAI-Standard-Frontend"}}
{"type": "source_file", "path": "Gradio/formatted_json.py", "content": "import json\nimport os\n\n\ndef format_json_file(input_file_path, output_file_path):\n    \"\"\"\n    读取 JSON 文件，格式化其内容并写入到一个新文件。\n\n    :param input_file_path: 输入 JSON 文件的相对路径\n    :param output_file_path: 输出格式化 JSON 文件的相对路径\n    \"\"\"\n    try:\n        # 检查输入文件是否存在\n        if not os.path.exists(input_file_path):\n            print(f\"文件 {input_file_path} 不存在！\")\n            return\n\n        # 读取 JSON 文件内容\n        with open(input_file_path, 'r', encoding='utf-8') as input_file:\n            data = json.load(input_file)\n\n        # 格式化 JSON 内容\n        formatted_json = json.dumps(data, indent=4, ensure_ascii=False)\n\n        # 输出格式化内容到新文件\n        with open(output_file_path, 'w', encoding='utf-8') as output_file:\n            output_file.write(formatted_json)\n\n        print(\"格式化后的 JSON 内容：\")\n        print(formatted_json)\n        print(f\"\\n格式化后的内容已保存到 {output_file_path}\")\n\n    except json.JSONDecodeError as e:\n        print(f\"JSON 解码错误：{e}\")\n    except Exception as e:\n        print(f\"发生错误：{e}\")\n\n\n# 示例调用\ninput_file = \"./test1.json\"  # 输入文件路径\noutput_file = \"./formatted_output.json\"  # 输出文件路径\nformat_json_file(input_file, output_file)\n"}
{"type": "source_file", "path": "Gradio/Lorebook.py", "content": "import json\n\nclass Entry:\n    \"\"\"\n    定义单个 entry 的数据结构\n    \"\"\"\n    \"\"\"\n    uid：uid\n    key：关键词\n    comment：标题\n    content：条目内容\n    order：顺序，以对llm影响的效果分级如下：0-角色定义之前（最弱），1-角色定义之后（次弱），2-作者注释之前（较强），3-作者注释之后（极强），4-@ D（可变），5-示例消息之前，6-示例消息之前，\n    position：位置，默认为100，影响效果总体受深度限制\n    depth：深度，指插入第X条消息的上方。默认为4,0为最强\n    probability：概率，0为不触发\n    \"\"\"\n    def __init__(self, uid, key, comment, content, order, position, depth, probability):\n        self.uid = uid\n        self.key = key\n        self.comment = comment\n        self.content = content\n        self.order = order\n        self.position = position\n        self.depth = depth\n        self.probability = probability\n\n    def __str__(self):\n        \"\"\"\n        定义打印格式\n        \"\"\"\n        return (\n            f\"UID: {self.uid}\\n\"\n            f\"Key: {self.key}\\n\"\n            f\"Comment: {self.comment}\\n\"\n            f\"Content:\\n{self.content}\\n\"\n            f\"Order: {self.order}\\n\"\n            f\"Position: {self.position}\\n\"\n            f\"Depth: {self.depth}\\n\"\n            f\"Probability: {self.probability}\\n\"\n        )\n\nclass JSONManager:\n    \"\"\"\n    管理整个 JSON 数据，提供提取和操作功能\n    \"\"\"\n    def __init__(self, json_file_path):\n        self.file_path = json_file_path\n        self.entries = self._load_entries()\n\n    def _load_entries(self):\n        \"\"\"\n        加载 JSON 文件并转换为 Entry 实例列表\n        \"\"\"\n        try:\n            with open(self.file_path, 'r', encoding='utf-8') as file:\n                data = json.load(file)\n            entries = data.get(\"entries\", {})\n            return [\n                Entry(\n                    uid=entry_data[\"uid\"],\n                    key=entry_data[\"key\"],\n                    comment=entry_data[\"comment\"],\n                    content=entry_data[\"content\"],\n                    order=entry_data[\"order\"],\n                    position=entry_data[\"position\"],\n                    depth=entry_data[\"depth\"],\n                    probability=entry_data[\"probability\"]\n                )\n                for entry_data in entries.values()\n            ]\n        except Exception as e:\n            print(f\"Error loading JSON file: {e}\")\n            return []\n\n    def get_entry_by_uid(self, uid):\n        \"\"\"\n        根据 UID 获取单个 Entry\n        \"\"\"\n        for entry in self.entries:\n            if entry.uid == uid:\n                return entry\n        return None\n\n    def get_entries_by_key(self, key):\n        \"\"\"\n        根据 key 搜索并返回匹配的 Entry 列表\n        \"\"\"\n        return [entry for entry in self.entries if key in entry.key]\n\n    def get_all_entries(self):\n        \"\"\"\n        返回所有 Entry 的列表\n        \"\"\"\n        return self.entries\n\nclass EntrySorter:\n    \"\"\"\n    用于排序和输出条目\n    \"\"\"\n\n    def __init__(self, entries):\n        \"\"\"\n        初始化，加载 Entry 列表\n        :param entries: List of Entry objects\n        \"\"\"\n        self.entries = entries\n\n    def filter_and_sort(self):\n        \"\"\"\n        过滤和排序条目\n        :return: 排序后的 Entry 列表\n        \"\"\"\n        # 过滤 probability 为 0 的条目\n        valid_entries = [entry for entry in self.entries if entry.probability > 0]\n\n        # 按优先级排序：order (降序) -> depth (升序) -> position (升序)\n        sorted_entries = sorted(\n            valid_entries,\n            key=lambda x: (-x.order, x.depth, x.position)\n        )\n        return sorted_entries\n\n    def export_to_txt(self, output_file):\n        \"\"\"\n        导出排序结果到 txt 文件\n        :param output_file: 输出文件路径\n        \"\"\"\n        sorted_entries = self.filter_and_sort()\n\n        with open(output_file, 'w', encoding='utf-8') as file:\n            for entry in sorted_entries:\n                file.write(f\"{entry.comment} - {','.join(entry.key)} - {entry.content}\\n\")\n        print(f\"排序后的条目已保存到 {output_file}\")\n\n\n# 示例调用\nif __name__ == \"__main__\":\n    # 替换为您的 JSON 文件路径\n    file_path = \"./斗罗.json\"\n\n    # 初始化 JSONManager\n    manager = JSONManager(file_path)\n\n    # 获取所有条目\n    all_entries = manager.get_all_entries()\n\n\n    # 创建 EntrySorter 并导出排序结果\n    sorter = EntrySorter(all_entries)\n    output_path = \"sorted_entries.txt\"\n    sorter.export_to_txt(output_path)\n\n    \"\"\"\n    print(\"所有条目:\")\n    for entry in all_entries:\n        print(entry)\n        print(\"-\" * 30)\n\n    # 根据 UID 获取条目\n    uid = 1\n    specific_entry = manager.get_entry_by_uid(uid)\n    print(f\"UID 为 {uid} 的条目:\")\n    print(specific_entry)\n\n    # 根据 Key 搜索条目\n    search_key = \"44\"\n    matching_entries = manager.get_entries_by_key(search_key)\n    print(f\"Key 包含 '{search_key}' 的条目:\")\n    for entry in matching_entries:\n        print(entry)\n    \"\"\"\n"}
{"type": "source_file", "path": "examples/lightrag_azure_openai_demo.py", "content": "import os\nimport asyncio\nfrom lightrag import LightRAG, QueryParam\nfrom lightrag.utils import EmbeddingFunc\nimport numpy as np\nfrom dotenv import load_dotenv\nimport logging\nfrom openai import AzureOpenAI\n\nlogging.basicConfig(level=logging.INFO)\n\nload_dotenv()\n\nAZURE_OPENAI_API_VERSION = os.getenv(\"AZURE_OPENAI_API_VERSION\")\nAZURE_OPENAI_DEPLOYMENT = os.getenv(\"AZURE_OPENAI_DEPLOYMENT\")\nAZURE_OPENAI_API_KEY = os.getenv(\"AZURE_OPENAI_API_KEY\")\nAZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n\nAZURE_EMBEDDING_DEPLOYMENT = os.getenv(\"AZURE_EMBEDDING_DEPLOYMENT\")\nAZURE_EMBEDDING_API_VERSION = os.getenv(\"AZURE_EMBEDDING_API_VERSION\")\n\nWORKING_DIR = \"./dickens\"\n\nif os.path.exists(WORKING_DIR):\n    import shutil\n\n    shutil.rmtree(WORKING_DIR)\n\nos.mkdir(WORKING_DIR)\n\n\nasync def llm_model_func(\n    prompt, system_prompt=None, history_messages=[], keyword_extraction=False, **kwargs\n) -> str:\n    client = AzureOpenAI(\n        api_key=AZURE_OPENAI_API_KEY,\n        api_version=AZURE_OPENAI_API_VERSION,\n        azure_endpoint=AZURE_OPENAI_ENDPOINT,\n    )\n\n    messages = []\n    if system_prompt:\n        messages.append({\"role\": \"system\", \"content\": system_prompt})\n    if history_messages:\n        messages.extend(history_messages)\n    messages.append({\"role\": \"user\", \"content\": prompt})\n\n    chat_completion = client.chat.completions.create(\n        model=AZURE_OPENAI_DEPLOYMENT,  # model = \"deployment_name\".\n        messages=messages,\n        temperature=kwargs.get(\"temperature\", 0),\n        top_p=kwargs.get(\"top_p\", 1),\n        n=kwargs.get(\"n\", 1),\n    )\n    return chat_completion.choices[0].message.content\n\n\nasync def embedding_func(texts: list[str]) -> np.ndarray:\n    client = AzureOpenAI(\n        api_key=AZURE_OPENAI_API_KEY,\n        api_version=AZURE_EMBEDDING_API_VERSION,\n        azure_endpoint=AZURE_OPENAI_ENDPOINT,\n    )\n    embedding = client.embeddings.create(model=AZURE_EMBEDDING_DEPLOYMENT, input=texts)\n\n    embeddings = [item.embedding for item in embedding.data]\n    return np.array(embeddings)\n\n\nasync def test_funcs():\n    result = await llm_model_func(\"How are you?\")\n    print(\"Resposta do llm_model_func: \", result)\n\n    result = await embedding_func([\"How are you?\"])\n    print(\"Resultado do embedding_func: \", result.shape)\n    print(\"Dimensão da embedding: \", result.shape[1])\n\n\nasyncio.run(test_funcs())\n\nembedding_dimension = 3072\n\nrag = LightRAG(\n    working_dir=WORKING_DIR,\n    llm_model_func=llm_model_func,\n    embedding_func=EmbeddingFunc(\n        embedding_dim=embedding_dimension,\n        max_token_size=8192,\n        func=embedding_func,\n    ),\n)\n\nbook1 = open(\"./book_1.txt\", encoding=\"utf-8\")\nbook2 = open(\"./book_2.txt\", encoding=\"utf-8\")\n\nrag.insert([book1.read(), book2.read()])\n\nquery_text = \"What are the main themes?\"\n\nprint(\"Result (Naive):\")\nprint(rag.query(query_text, param=QueryParam(mode=\"naive\")))\n\nprint(\"\\nResult (Local):\")\nprint(rag.query(query_text, param=QueryParam(mode=\"local\")))\n\nprint(\"\\nResult (Global):\")\nprint(rag.query(query_text, param=QueryParam(mode=\"global\")))\n\nprint(\"\\nResult (Hybrid):\")\nprint(rag.query(query_text, param=QueryParam(mode=\"hybrid\")))\n"}
{"type": "source_file", "path": "examples/insert_custom_kg.py", "content": "import os\nfrom lightrag import LightRAG\nfrom lightrag.llm import gpt_4o_mini_complete\n#########\n# Uncomment the below two lines if running in a jupyter notebook to handle the async nature of rag.insert()\n# import nest_asyncio\n# nest_asyncio.apply()\n#########\n\nWORKING_DIR = \"./custom_kg\"\n\nif not os.path.exists(WORKING_DIR):\n    os.mkdir(WORKING_DIR)\n\nrag = LightRAG(\n    working_dir=WORKING_DIR,\n    llm_model_func=gpt_4o_mini_complete,  # Use gpt_4o_mini_complete LLM model\n    # llm_model_func=gpt_4o_complete  # Optionally, use a stronger model\n)\n\ncustom_kg = {\n    \"entities\": [\n        {\n            \"entity_name\": \"CompanyA\",\n            \"entity_type\": \"Organization\",\n            \"description\": \"A major technology company\",\n            \"source_id\": \"Source1\",\n        },\n        {\n            \"entity_name\": \"ProductX\",\n            \"entity_type\": \"Product\",\n            \"description\": \"A popular product developed by CompanyA\",\n            \"source_id\": \"Source1\",\n        },\n        {\n            \"entity_name\": \"PersonA\",\n            \"entity_type\": \"Person\",\n            \"description\": \"A renowned researcher in AI\",\n            \"source_id\": \"Source2\",\n        },\n        {\n            \"entity_name\": \"UniversityB\",\n            \"entity_type\": \"Organization\",\n            \"description\": \"A leading university specializing in technology and sciences\",\n            \"source_id\": \"Source2\",\n        },\n        {\n            \"entity_name\": \"CityC\",\n            \"entity_type\": \"Location\",\n            \"description\": \"A large metropolitan city known for its culture and economy\",\n            \"source_id\": \"Source3\",\n        },\n        {\n            \"entity_name\": \"EventY\",\n            \"entity_type\": \"Event\",\n            \"description\": \"An annual technology conference held in CityC\",\n            \"source_id\": \"Source3\",\n        },\n    ],\n    \"relationships\": [\n        {\n            \"src_id\": \"CompanyA\",\n            \"tgt_id\": \"ProductX\",\n            \"description\": \"CompanyA develops ProductX\",\n            \"keywords\": \"develop, produce\",\n            \"weight\": 1.0,\n            \"source_id\": \"Source1\",\n        },\n        {\n            \"src_id\": \"PersonA\",\n            \"tgt_id\": \"UniversityB\",\n            \"description\": \"PersonA works at UniversityB\",\n            \"keywords\": \"employment, affiliation\",\n            \"weight\": 0.9,\n            \"source_id\": \"Source2\",\n        },\n        {\n            \"src_id\": \"CityC\",\n            \"tgt_id\": \"EventY\",\n            \"description\": \"EventY is hosted in CityC\",\n            \"keywords\": \"host, location\",\n            \"weight\": 0.8,\n            \"source_id\": \"Source3\",\n        },\n    ],\n    \"chunks\": [\n        {\n            \"content\": \"ProductX, developed by CompanyA, has revolutionized the market with its cutting-edge features.\",\n            \"source_id\": \"Source1\",\n        },\n        {\n            \"content\": \"PersonA is a prominent researcher at UniversityB, focusing on artificial intelligence and machine learning.\",\n            \"source_id\": \"Source2\",\n        },\n        {\n            \"content\": \"EventY, held in CityC, attracts technology enthusiasts and companies from around the globe.\",\n            \"source_id\": \"Source3\",\n        },\n        {\n            \"content\": \"None\",\n            \"source_id\": \"UNKNOWN\",\n        },\n    ],\n}\n\nrag.insert_custom_kg(custom_kg)\n"}
{"type": "source_file", "path": "examples/lightrag_api_openai_compatible_demo.py", "content": "from fastapi import FastAPI, HTTPException, File, UploadFile\nfrom pydantic import BaseModel\nimport os\nfrom lightrag import LightRAG, QueryParam\nfrom lightrag.llm import openai_complete_if_cache, openai_embedding\nfrom lightrag.utils import EmbeddingFunc\nimport numpy as np\nfrom typing import Optional\nimport asyncio\nimport nest_asyncio\n\n# Apply nest_asyncio to solve event loop issues\nnest_asyncio.apply()\n\nDEFAULT_RAG_DIR = \"index_default\"\napp = FastAPI(title=\"LightRAG API\", description=\"API for RAG operations\")\n\n# Configure working directory\nWORKING_DIR = os.environ.get(\"RAG_DIR\", f\"{DEFAULT_RAG_DIR}\")\nprint(f\"WORKING_DIR: {WORKING_DIR}\")\nLLM_MODEL = os.environ.get(\"LLM_MODEL\", \"gpt-4o-mini\")\nprint(f\"LLM_MODEL: {LLM_MODEL}\")\nEMBEDDING_MODEL = os.environ.get(\"EMBEDDING_MODEL\", \"text-embedding-3-large\")\nprint(f\"EMBEDDING_MODEL: {EMBEDDING_MODEL}\")\nEMBEDDING_MAX_TOKEN_SIZE = int(os.environ.get(\"EMBEDDING_MAX_TOKEN_SIZE\", 8192))\nprint(f\"EMBEDDING_MAX_TOKEN_SIZE: {EMBEDDING_MAX_TOKEN_SIZE}\")\n\nif not os.path.exists(WORKING_DIR):\n    os.mkdir(WORKING_DIR)\n\n\n# LLM model function\n\n\nasync def llm_model_func(\n    prompt, system_prompt=None, history_messages=[], keyword_extraction=False, **kwargs\n) -> str:\n    return await openai_complete_if_cache(\n        LLM_MODEL,\n        prompt,\n        system_prompt=system_prompt,\n        history_messages=history_messages,\n        **kwargs,\n    )\n\n\n# Embedding function\n\n\nasync def embedding_func(texts: list[str]) -> np.ndarray:\n    return await openai_embedding(\n        texts,\n        model=EMBEDDING_MODEL,\n    )\n\n\nasync def get_embedding_dim():\n    test_text = [\"This is a test sentence.\"]\n    embedding = await embedding_func(test_text)\n    embedding_dim = embedding.shape[1]\n    print(f\"{embedding_dim=}\")\n    return embedding_dim\n\n\n# Initialize RAG instance\nrag = LightRAG(\n    working_dir=WORKING_DIR,\n    llm_model_func=llm_model_func,\n    embedding_func=EmbeddingFunc(\n        embedding_dim=asyncio.run(get_embedding_dim()),\n        max_token_size=EMBEDDING_MAX_TOKEN_SIZE,\n        func=embedding_func,\n    ),\n)\n\n\n# Data models\n\n\nclass QueryRequest(BaseModel):\n    query: str\n    mode: str = \"hybrid\"\n    only_need_context: bool = False\n\n\nclass InsertRequest(BaseModel):\n    text: str\n\n\nclass Response(BaseModel):\n    status: str\n    data: Optional[str] = None\n    message: Optional[str] = None\n\n\n# API routes\n\n\n@app.post(\"/query\", response_model=Response)\nasync def query_endpoint(request: QueryRequest):\n    try:\n        loop = asyncio.get_event_loop()\n        result = await loop.run_in_executor(\n            None,\n            lambda: rag.query(\n                request.query,\n                param=QueryParam(\n                    mode=request.mode, only_need_context=request.only_need_context\n                ),\n            ),\n        )\n        return Response(status=\"success\", data=result)\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n\n\n@app.post(\"/insert\", response_model=Response)\nasync def insert_endpoint(request: InsertRequest):\n    try:\n        loop = asyncio.get_event_loop()\n        await loop.run_in_executor(None, lambda: rag.insert(request.text))\n        return Response(status=\"success\", message=\"Text inserted successfully\")\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n\n\n@app.post(\"/insert_file\", response_model=Response)\nasync def insert_file(file: UploadFile = File(...)):\n    try:\n        file_content = await file.read()\n        # Read file content\n        try:\n            content = file_content.decode(\"utf-8\")\n        except UnicodeDecodeError:\n            # If UTF-8 decoding fails, try other encodings\n            content = file_content.decode(\"gbk\")\n        # Insert file content\n        loop = asyncio.get_event_loop()\n        await loop.run_in_executor(None, lambda: rag.insert(content))\n\n        return Response(\n            status=\"success\",\n            message=f\"File content from {file.filename} inserted successfully\",\n        )\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n\n\n@app.get(\"/health\")\nasync def health_check():\n    return {\"status\": \"healthy\"}\n\n\nif __name__ == \"__main__\":\n    import uvicorn\n\n    uvicorn.run(app, host=\"0.0.0.0\", port=8020)\n\n# Usage example\n# To run the server, use the following command in your terminal:\n# python lightrag_api_openai_compatible_demo.py\n\n# Example requests:\n# 1. Query:\n# curl -X POST \"http://127.0.0.1:8020/query\" -H \"Content-Type: application/json\" -d '{\"query\": \"your query here\", \"mode\": \"hybrid\"}'\n\n# 2. Insert text:\n# curl -X POST \"http://127.0.0.1:8020/insert\" -H \"Content-Type: application/json\" -d '{\"text\": \"your text here\"}'\n\n# 3. Insert file:\n# curl -X POST \"http://127.0.0.1:8020/insert_file\" -H \"Content-Type: application/json\" -d '{\"file_path\": \"path/to/your/file.txt\"}'\n\n# 4. Health check:\n# curl -X GET \"http://127.0.0.1:8020/health\"\n"}
{"type": "source_file", "path": "examples/lightrag_nvidia_demo.py", "content": "import os\nimport asyncio\nfrom lightrag import LightRAG, QueryParam\nfrom lightrag.llm import (\n    openai_complete_if_cache,\n    nvidia_openai_embedding,\n)\nfrom lightrag.utils import EmbeddingFunc\nimport numpy as np\n\n# for custom llm_model_func\nfrom lightrag.utils import locate_json_string_body_from_string\n\nWORKING_DIR = \"./dickens\"\n\nif not os.path.exists(WORKING_DIR):\n    os.mkdir(WORKING_DIR)\n\n# some method to use your API key (choose one)\n# NVIDIA_OPENAI_API_KEY = os.getenv(\"NVIDIA_OPENAI_API_KEY\")\nNVIDIA_OPENAI_API_KEY = \"nvapi-xxxx\"  # your api key\n\n# using pre-defined function for nvidia LLM API. OpenAI compatible\n# llm_model_func = nvidia_openai_complete\n\n\n# If you trying to make custom llm_model_func to use llm model on NVIDIA API like other example:\nasync def llm_model_func(\n    prompt, system_prompt=None, history_messages=[], keyword_extraction=False, **kwargs\n) -> str:\n    result = await openai_complete_if_cache(\n        \"nvidia/llama-3.1-nemotron-70b-instruct\",\n        prompt,\n        system_prompt=system_prompt,\n        history_messages=history_messages,\n        api_key=NVIDIA_OPENAI_API_KEY,\n        base_url=\"https://integrate.api.nvidia.com/v1\",\n        **kwargs,\n    )\n    if keyword_extraction:\n        return locate_json_string_body_from_string(result)\n    return result\n\n\n# custom embedding\nnvidia_embed_model = \"nvidia/nv-embedqa-e5-v5\"\n\n\nasync def indexing_embedding_func(texts: list[str]) -> np.ndarray:\n    return await nvidia_openai_embedding(\n        texts,\n        model=nvidia_embed_model,  # maximum 512 token\n        # model=\"nvidia/llama-3.2-nv-embedqa-1b-v1\",\n        api_key=NVIDIA_OPENAI_API_KEY,\n        base_url=\"https://integrate.api.nvidia.com/v1\",\n        input_type=\"passage\",\n        trunc=\"END\",  # handling on server side if input token is longer than maximum token\n        encode=\"float\",\n    )\n\n\nasync def query_embedding_func(texts: list[str]) -> np.ndarray:\n    return await nvidia_openai_embedding(\n        texts,\n        model=nvidia_embed_model,  # maximum 512 token\n        # model=\"nvidia/llama-3.2-nv-embedqa-1b-v1\",\n        api_key=NVIDIA_OPENAI_API_KEY,\n        base_url=\"https://integrate.api.nvidia.com/v1\",\n        input_type=\"query\",\n        trunc=\"END\",  # handling on server side if input token is longer than maximum token\n        encode=\"float\",\n    )\n\n\n# dimension are same\nasync def get_embedding_dim():\n    test_text = [\"This is a test sentence.\"]\n    embedding = await indexing_embedding_func(test_text)\n    embedding_dim = embedding.shape[1]\n    return embedding_dim\n\n\n# function test\nasync def test_funcs():\n    result = await llm_model_func(\"How are you?\")\n    print(\"llm_model_func: \", result)\n\n    result = await indexing_embedding_func([\"How are you?\"])\n    print(\"embedding_func: \", result)\n\n\n# asyncio.run(test_funcs())\n\n\nasync def main():\n    try:\n        embedding_dimension = await get_embedding_dim()\n        print(f\"Detected embedding dimension: {embedding_dimension}\")\n\n        # lightRAG class during indexing\n        rag = LightRAG(\n            working_dir=WORKING_DIR,\n            llm_model_func=llm_model_func,\n            # llm_model_name=\"meta/llama3-70b-instruct\", #un comment if\n            embedding_func=EmbeddingFunc(\n                embedding_dim=embedding_dimension,\n                max_token_size=512,  # maximum token size, somehow it's still exceed maximum number of token\n                # so truncate (trunc) parameter on embedding_func will handle it and try to examine the tokenizer used in LightRAG\n                # so you can adjust to be able to fit the NVIDIA model (future work)\n                func=indexing_embedding_func,\n            ),\n        )\n\n        # reading file\n        with open(\"./book.txt\", \"r\", encoding=\"utf-8\") as f:\n            await rag.ainsert(f.read())\n\n        # redefine rag to change embedding into query type\n        rag = LightRAG(\n            working_dir=WORKING_DIR,\n            llm_model_func=llm_model_func,\n            # llm_model_name=\"meta/llama3-70b-instruct\", #un comment if\n            embedding_func=EmbeddingFunc(\n                embedding_dim=embedding_dimension,\n                max_token_size=512,\n                func=query_embedding_func,\n            ),\n        )\n\n        # Perform naive search\n        print(\"==============Naive===============\")\n        print(\n            await rag.aquery(\n                \"What are the top themes in this story?\", param=QueryParam(mode=\"naive\")\n            )\n        )\n\n        # Perform local search\n        print(\"==============local===============\")\n        print(\n            await rag.aquery(\n                \"What are the top themes in this story?\", param=QueryParam(mode=\"local\")\n            )\n        )\n\n        # Perform global search\n        print(\"==============global===============\")\n        print(\n            await rag.aquery(\n                \"What are the top themes in this story?\",\n                param=QueryParam(mode=\"global\"),\n            )\n        )\n\n        # Perform hybrid search\n        print(\"==============hybrid===============\")\n        print(\n            await rag.aquery(\n                \"What are the top themes in this story?\",\n                param=QueryParam(mode=\"hybrid\"),\n            )\n        )\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n"}
{"type": "source_file", "path": "built your Graph（不再使用，仅做参考）.py", "content": "import asyncio\nimport os\n\nimport numpy as np\nfrom dotenv import load_dotenv\n\nfrom lightrag import LightRAG, QueryParam\nfrom lightrag.llm import openai_complete_if_cache, openai_embedding, openai_compatible_complete_if_cache, \\\n    openai_compatible_embedding\nfrom lightrag.utils import EmbeddingFunc\n\nload_dotenv()\n\n#你的知识图谱存放的文件夹\nWORKING_DIR = os.getenv(\"RAG_DIR\")\nprint(f\"WORKING_DIR: {WORKING_DIR}\")\n\n#你的文档，例如./book.txt，建议存放在text这个文件夹以便管理\nfile_DIR = os.getenv(\"file_DIR\")\nprint(f\"file_DIR: {file_DIR}\")\n\nLLM_MODEL = os.getenv(\"LLM_MODEL\")\nprint(f\"LLM_MODEL: {LLM_MODEL}\")\nEMBEDDING_MODEL = os.getenv(\"EMBEDDING_MODEL\")\nprint(f\"EMBEDDING_MODEL: {EMBEDDING_MODEL}\")\nEMBEDDING_MAX_TOKEN_SIZE = int(os.getenv(\"EMBEDDING_MAX_TOKEN_SIZE\"))\nprint(f\"EMBEDDING_MAX_TOKEN_SIZE: {EMBEDDING_MAX_TOKEN_SIZE}\")\n\nBASE_URL=os.getenv(\"OPENAI_BASE_URL\")\nAPI_KEY = os.getenv(\"OPENAI_API_KEY\")\n\nif not os.path.exists(WORKING_DIR):\n    os.mkdir(WORKING_DIR)\n\n\nasync def llm_model_func(\n    prompt, system_prompt=None, history_messages=[], **kwargs\n) -> str:\n    return await openai_compatible_complete_if_cache(\n        LLM_MODEL,\n        prompt,\n        system_prompt=system_prompt,\n        history_messages=history_messages,\n        api_key=API_KEY,\n        base_url=BASE_URL,\n        **kwargs,\n    )\n\n\nasync def embedding_func(texts: list[str]) -> np.ndarray:\n    return await openai_compatible_embedding(\n        texts,\n        model=EMBEDDING_MODEL,\n        api_key=API_KEY,\n        base_url=BASE_URL,\n    )\n\n\nasync def get_embedding_dim():\n    test_text = [\"This is a test sentence.\"]\n    embedding = await embedding_func(test_text)\n    embedding_dim = embedding.shape[1]\n    return embedding_dim\n\n\n# function test\nasync def test_funcs():\n    result = await llm_model_func(\"How are you?\")\n    print(\"llm_model_func: \", result)\n\n    result = await embedding_func([\"How are you?\"])\n    print(\"embedding_func: \", result)\n\n\n# asyncio.run(test_funcs())\n\n\nasync def main():\n    try:\n        embedding_dimension = await get_embedding_dim()\n        print(f\"Detected embedding dimension: {embedding_dimension}\")\n\n        rag = LightRAG(\n            working_dir=WORKING_DIR,\n            llm_model_func=llm_model_func,\n            embedding_func=EmbeddingFunc(\n                embedding_dim=embedding_dimension,\n                max_token_size=EMBEDDING_MAX_TOKEN_SIZE,\n                func=embedding_func,\n            ),\n        )\n\n\n        with open(\"text/book.txt\", \"r\", encoding=\"utf-8\", errors='ignore') as f:\n            await rag.ainsert(f.read())\n\n        '''\n        #以下是搜索方法，共四种，请查看官方文档以便选择\n        # Perform naive search\n        print(\n            await rag.aquery(\n                \"What kind of story is told in this first chapter? Please answer in Chinese.\", param=QueryParam(mode=\"naive\")\n            )\n        )\n\n        # Perform local search\n        print(\n            await rag.aquery(\n                \"What kind of story is told in this first chapter? Please answer in Chinese.\", param=QueryParam(mode=\"local\")\n            )\n        )\n\n        # Perform global search\n        print(\n            await rag.aquery(\n                \"What kind of story is told in this first chapter? Please answer in Chinese.\", param=QueryParam(mode=\"global\"),\n            )\n        )\n\n        '''\n        # Perform hybrid search\n        print(\n            await rag.aquery(\n                \"What kind of story is told in this first chapter? Please answer in Chinese.\", param=QueryParam(mode=\"hybrid\"),\n            )\n        )\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n"}
{"type": "source_file", "path": "examples/graph_visual_with_neo4j.py", "content": "import os\nimport json\nfrom lightrag.utils import xml_to_json\nfrom neo4j import GraphDatabase\n\n# Constants\nWORKING_DIR = \"./dickens\"\nBATCH_SIZE_NODES = 500\nBATCH_SIZE_EDGES = 100\n\n# Neo4j connection credentials\nNEO4J_URI = \"bolt://localhost:7687\"\nNEO4J_USERNAME = \"neo4j\"\nNEO4J_PASSWORD = \"your_password\"\n\n\ndef convert_xml_to_json(xml_path, output_path):\n    \"\"\"Converts XML file to JSON and saves the output.\"\"\"\n    if not os.path.exists(xml_path):\n        print(f\"Error: File not found - {xml_path}\")\n        return None\n\n    json_data = xml_to_json(xml_path)\n    if json_data:\n        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n            json.dump(json_data, f, ensure_ascii=False, indent=2)\n        print(f\"JSON file created: {output_path}\")\n        return json_data\n    else:\n        print(\"Failed to create JSON data\")\n        return None\n\n\ndef process_in_batches(tx, query, data, batch_size):\n    \"\"\"Process data in batches and execute the given query.\"\"\"\n    for i in range(0, len(data), batch_size):\n        batch = data[i : i + batch_size]\n        tx.run(query, {\"nodes\": batch} if \"nodes\" in query else {\"edges\": batch})\n\n\ndef main():\n    # Paths\n    xml_file = os.path.join(WORKING_DIR, \"graph_chunk_entity_relation.graphml\")\n    json_file = os.path.join(WORKING_DIR, \"graph_data.json\")\n\n    # Convert XML to JSON\n    json_data = convert_xml_to_json(xml_file, json_file)\n    if json_data is None:\n        return\n\n    # Load nodes and edges\n    nodes = json_data.get(\"nodes\", [])\n    edges = json_data.get(\"edges\", [])\n\n    # Neo4j queries\n    create_nodes_query = \"\"\"\n    UNWIND $nodes AS node\n    MERGE (e:Entity {id: node.id})\n    SET e.entity_type = node.entity_type,\n        e.description = node.description,\n        e.source_id = node.source_id,\n        e.displayName = node.id\n    REMOVE e:Entity\n    WITH e, node\n    CALL apoc.create.addLabels(e, [node.id]) YIELD node AS labeledNode\n    RETURN count(*)\n    \"\"\"\n\n    create_edges_query = \"\"\"\n    UNWIND $edges AS edge\n    MATCH (source {id: edge.source})\n    MATCH (target {id: edge.target})\n    WITH source, target, edge,\n         CASE\n            WHEN edge.keywords CONTAINS 'lead' THEN 'lead'\n            WHEN edge.keywords CONTAINS 'participate' THEN 'participate'\n            WHEN edge.keywords CONTAINS 'uses' THEN 'uses'\n            WHEN edge.keywords CONTAINS 'located' THEN 'located'\n            WHEN edge.keywords CONTAINS 'occurs' THEN 'occurs'\n           ELSE REPLACE(SPLIT(edge.keywords, ',')[0], '\\\"', '')\n         END AS relType\n    CALL apoc.create.relationship(source, relType, {\n      weight: edge.weight,\n      description: edge.description,\n      keywords: edge.keywords,\n      source_id: edge.source_id\n    }, target) YIELD rel\n    RETURN count(*)\n    \"\"\"\n\n    set_displayname_and_labels_query = \"\"\"\n    MATCH (n)\n    SET n.displayName = n.id\n    WITH n\n    CALL apoc.create.setLabels(n, [n.entity_type]) YIELD node\n    RETURN count(*)\n    \"\"\"\n\n    # Create a Neo4j driver\n    driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USERNAME, NEO4J_PASSWORD))\n\n    try:\n        # Execute queries in batches\n        with driver.session() as session:\n            # Insert nodes in batches\n            session.execute_write(\n                process_in_batches, create_nodes_query, nodes, BATCH_SIZE_NODES\n            )\n\n            # Insert edges in batches\n            session.execute_write(\n                process_in_batches, create_edges_query, edges, BATCH_SIZE_EDGES\n            )\n\n            # Set displayName and labels\n            session.run(set_displayname_and_labels_query)\n\n    except Exception as e:\n        print(f\"Error occurred: {e}\")\n\n    finally:\n        driver.close()\n\n\nif __name__ == \"__main__\":\n    main()\n"}
{"type": "source_file", "path": "examples/lightrag_bedrock_demo.py", "content": "\"\"\"\nLightRAG meets Amazon Bedrock ⛰️\n\"\"\"\n\nimport os\nimport logging\n\nfrom lightrag import LightRAG, QueryParam\nfrom lightrag.llm import bedrock_complete, bedrock_embedding\nfrom lightrag.utils import EmbeddingFunc\n\nlogging.getLogger(\"aiobotocore\").setLevel(logging.WARNING)\n\nWORKING_DIR = \"./dickens\"\nif not os.path.exists(WORKING_DIR):\n    os.mkdir(WORKING_DIR)\n\nrag = LightRAG(\n    working_dir=WORKING_DIR,\n    llm_model_func=bedrock_complete,\n    llm_model_name=\"Anthropic Claude 3 Haiku // Amazon Bedrock\",\n    embedding_func=EmbeddingFunc(\n        embedding_dim=1024, max_token_size=8192, func=bedrock_embedding\n    ),\n)\n\nwith open(\"./book.txt\", \"r\", encoding=\"utf-8\") as f:\n    rag.insert(f.read())\n\nfor mode in [\"naive\", \"local\", \"global\", \"hybrid\"]:\n    print(\"\\n+-\" + \"-\" * len(mode) + \"-+\")\n    print(f\"| {mode.capitalize()} |\")\n    print(\"+-\" + \"-\" * len(mode) + \"-+\\n\")\n    print(\n        rag.query(\"What are the top themes in this story?\", param=QueryParam(mode=mode))\n    )\n"}
{"type": "source_file", "path": "Gradio/welcome_page.py", "content": "import mimetypes\nimport os\nimport shutil\nimport sys\nimport zipfile\nimport subprocess\n\nfrom importlib.metadata import distributions\nfrom typing import List, Tuple\nfrom datetime import datetime, time, timedelta\n\nimport gradio as gr\nfrom dotenv import load_dotenv, set_key\nfrom pathlib import Path\n\nfrom fastapi import requests\n\nfrom playwright.sync_api import sync_playwright\nfrom sympy import false\n\nload_dotenv()\n\n# 欢迎界面<开始>\nclass welcome_pages:\n\n    def __init__(self):\n        \"\"\"初始化并构建欢迎页面\"\"\"\n        self.ui = self.build_welcome_page()\n\n    def load_readme(self):\n        \"\"\"加载 README.md 内容\"\"\"\n        try:\n            with open(\"README.md\", \"r\", encoding=\"utf-8\") as file:\n                return file.read()\n        except FileNotFoundError:\n            return \"README.md 文件未找到，请检查项目目录。\"\n\n    def load_license(self):\n        \"\"\"加载开源协议内容\"\"\"\n        try:\n            with open(\"LICENSE\", \"r\", encoding=\"utf-8\") as file:\n                return file.read()\n        except FileNotFoundError:\n            return \"LICENSE 文件未找到，请检查项目目录。\"\n\n    def load_requirements(self):\n        \"\"\"读取 requirements.txt 中的依赖包信息\"\"\"\n        with open(\"requirements.txt\", \"r\") as f:\n            requirements = f.read().splitlines()\n        return [pkg.split(\"==\")[0] for pkg in requirements], requirements\n\n    def check_installed_packages(self):\n        \"\"\"获取当前环境中已安装的依赖包及版本\"\"\"\n        installed_packages = {dist.metadata[\"Name\"].lower(): dist.version for dist in distributions()}\n        return installed_packages\n\n    def check_dependency_status(self):\n        \"\"\"检查依赖包状态\"\"\"\n        required_packages, full_requirements = self.load_requirements()\n        installed_packages = self.check_installed_packages()\n\n        missing_packages = []\n        mismatched_versions = []\n\n        for req in full_requirements:\n            pkg, _, version = req.partition(\"==\")\n            pkg_lower = pkg.lower()\n            if pkg_lower not in installed_packages:\n                missing_packages.append(f\"🚫 {req}\")\n            elif installed_packages[pkg_lower] != version:\n                mismatched_versions.append(\n                    f\"⚠️ {pkg} (expected {version}, found {installed_packages[pkg_lower]})\"\n                )\n\n        if not missing_packages and not mismatched_versions:\n            return \"✅ 所有依赖包已安装\", [], []\n        else:\n            return (\n                \"部分依赖包存在问题，请查看下方列表。\",\n                missing_packages,\n                mismatched_versions,\n            )\n\n    def install_missing_packages(missing_packages):\n        \"\"\"安装缺失的依赖包\"\"\"\n        try:\n            for package in missing_packages:\n                pkg = package.split(\" \")[1]  # 提取包名（忽略符号 🚫）\n                subprocess.check_call([\"pip\", \"install\", pkg])\n            return \"✅ 缺失的依赖包已成功安装\"\n        except subprocess.CalledProcessError as e:\n            return f\"❌ 安装失败: {e}\"\n\n\n    # 安装按钮逻辑\n    def install_and_update(missing_packages, self=None):\n        if not missing_packages:\n            return \"没有需要安装的依赖包\"\n        install_result = self.install_missing_packages(missing_packages)\n        status, _, _ = self.check_dependency_status()  # 检查安装后的状态\n        return status, install_result\n\n\n    def check_lightrag_status(self):\n        \"\"\"检查 LightRAG 后端状态\"\"\"\n        # 示例实现，可以扩展为实际后端服务的检查逻辑\n        return \"✅LightRAG 后端运行正常\"\n\n    def check_model_connection_status(self):\n        \"\"\"检查大模型连接状态\"\"\"\n        # 示例实现，可以扩展为实际模型连接的检查逻辑\n        return \"✅大模型连接成功\"\n\n\n    # 刷新按钮逻辑\n    def refresh_status(self):\n        status, missing, mismatched = self.check_dependency_status()\n        return (\n            status,\n            missing + mismatched,  # 展示所有缺失和版本问题\n            bool(missing or mismatched),\n        )\n\n    # 欢迎界面<结束>\n\n\n    # UI\n\n    def build_welcome_page(self):\n        \"\"\"创建欢迎使用页面\"\"\"\n        with gr.Blocks(visible=False, elem_id=\"welcome-page\") as welcome_page:\n            # 标题\n            gr.Markdown(\"# 欢迎使用\", elem_id=\"welcome-title\", elem_classes=\"center-text\")\n\n            # 主体内容\n            with gr.Row():\n                # 左侧 README 内容块\n                with gr.Column(scale=3):\n                    gr.Markdown(self.load_readme(), label=\"项目简介\")\n\n                # 右侧状态栏\n                with gr.Column(scale=1):\n                    gr.Markdown(\"## 系统状态\")\n                    dependency_status = gr.Textbox(\n                        label=\"依赖包状态\",\n                        value=self.check_dependency_status()[0],\n                        interactive=False,\n                        placeholder=\"依赖包安装状态显示在此处\"\n                    )\n                    missing_packages_dropdown = gr.Dropdown(\n                        label=\"缺失依赖包列表\",\n                        choices=[],\n                        visible=True,\n                        interactive=False,\n                        multiselect=True,\n                        allow_custom_value=True\n                    )\n                    install_button = gr.Button(\n                        \"安装缺失的依赖包\",\n                        visible=False,\n                        variant=\"primary\",\n                    )\n                    lightrag_status = gr.Textbox(\n                        label=\"LightRAG 后端状态\",\n                        value=self.check_lightrag_status(),\n                        interactive=False,\n                        placeholder=\"后端状态显示在此处\"\n                    )\n                    model_connection_status = gr.Textbox(\n                        label=\"大模型连接状态\",\n                        value=self.check_model_connection_status(),\n                        interactive=False,\n                        placeholder=\"模型连接状态显示在此处\"\n                    )\n                    refresh_button = gr.Button(\"🔄刷新状态\", variant=\"primary\")\n\n            # 底部链接与开源协议\n            with gr.Row():\n                with gr.Column(scale=3):\n                    gr.Markdown(\"### 📂 项目链接\")\n                    gr.Markdown(\"\"\"\n                    - [GitHub 仓库](https://github.com/your_repo)\n                    - [项目使用说明书](https://your_docs_link)\n                    - [视频教程](https://your_video_link)\n                    \"\"\")\n\n                with gr.Column(scale=1):\n                    license_textbox = gr.Textbox(\n                        label=\"开源协议\",\n                        value=self.load_license(),\n                        lines=10,\n                        interactive=False\n                    )\n            # 页面初始化时的检查逻辑\n            def initialize_status():\n                status, missing, mismatched = self.check_dependency_status()\n                all_issues = missing + mismatched\n                show_install_button = bool(missing)  # 仅缺失包时显示安装按钮\n                return (\n                    status,\n                    all_issues,\n                    missing,  # 控制安装按钮是否显示\n                    show_install_button,\n                )\n\n            welcome_page.load(\n                fn=initialize_status,\n                inputs=[],\n                outputs=[\n                    dependency_status,\n                    missing_packages_dropdown,\n                    missing_packages_dropdown,\n                    install_button,\n                ],\n            )\n\n            # 刷新按钮逻辑\n            def refresh_status():\n                status, missing, mismatched = self.check_dependency_status()\n                all_issues = missing + mismatched\n                return (\n                    status,\n                    all_issues,\n                    missing,  # 控制安装按钮是否显示\n                    bool(missing),\n                )\n\n            refresh_button.click(\n                fn=refresh_status,\n                inputs=[],\n                outputs=[\n                    dependency_status,\n                    missing_packages_dropdown,\n                    missing_packages_dropdown,\n                    install_button,\n                ],\n            )\n\n            # 安装按钮逻辑\n            def install_and_update(missing_packages):\n                if not missing_packages:\n                    return \"没有需要安装的依赖包\"\n                install_result = self.install_missing_packages(missing_packages)\n                status, _, _ = self.check_dependency_status()  # 检查安装后的状态\n                return status, install_result\n\n            install_button.click(\n                fn=install_and_update,\n                inputs=[missing_packages_dropdown],\n                outputs=[\n                    dependency_status,\n                    gr.Textbox(placeholder=\"安装状态\", interactive=False),\n                ],\n            )\n        return welcome_page"}
{"type": "source_file", "path": "examples/lightrag_api_oracle_demo..py", "content": "from fastapi import FastAPI, HTTPException, File, UploadFile\nfrom contextlib import asynccontextmanager\nfrom pydantic import BaseModel\nfrom typing import Optional\n\nimport sys\nimport os\nfrom pathlib import Path\n\nimport asyncio\nimport nest_asyncio\nfrom lightrag import LightRAG, QueryParam\nfrom lightrag.llm import openai_complete_if_cache, openai_embedding\nfrom lightrag.utils import EmbeddingFunc\nimport numpy as np\n\nfrom lightrag.kg.oracle_impl import OracleDB\n\n\nprint(os.getcwd())\n\nscript_directory = Path(__file__).resolve().parent.parent\nsys.path.append(os.path.abspath(script_directory))\n\n\n# Apply nest_asyncio to solve event loop issues\nnest_asyncio.apply()\n\nDEFAULT_RAG_DIR = \"index_default\"\n\n\n# We use OpenAI compatible API to call LLM on Oracle Cloud\n# More docs here https://github.com/jin38324/OCI_GenAI_access_gateway\nBASE_URL = \"http://xxx.xxx.xxx.xxx:8088/v1/\"\nAPIKEY = \"ocigenerativeai\"\n\n# Configure working directory\nWORKING_DIR = os.environ.get(\"RAG_DIR\", f\"{DEFAULT_RAG_DIR}\")\nprint(f\"WORKING_DIR: {WORKING_DIR}\")\nLLM_MODEL = os.environ.get(\"LLM_MODEL\", \"cohere.command-r-plus\")\nprint(f\"LLM_MODEL: {LLM_MODEL}\")\nEMBEDDING_MODEL = os.environ.get(\"EMBEDDING_MODEL\", \"cohere.embed-multilingual-v3.0\")\nprint(f\"EMBEDDING_MODEL: {EMBEDDING_MODEL}\")\nEMBEDDING_MAX_TOKEN_SIZE = int(os.environ.get(\"EMBEDDING_MAX_TOKEN_SIZE\", 512))\nprint(f\"EMBEDDING_MAX_TOKEN_SIZE: {EMBEDDING_MAX_TOKEN_SIZE}\")\n\n\nif not os.path.exists(WORKING_DIR):\n    os.mkdir(WORKING_DIR)\n\n\nasync def llm_model_func(\n    prompt, system_prompt=None, history_messages=[], **kwargs\n) -> str:\n    return await openai_complete_if_cache(\n        LLM_MODEL,\n        prompt,\n        system_prompt=system_prompt,\n        history_messages=history_messages,\n        api_key=APIKEY,\n        base_url=BASE_URL,\n        **kwargs,\n    )\n\n\nasync def embedding_func(texts: list[str]) -> np.ndarray:\n    return await openai_embedding(\n        texts,\n        model=EMBEDDING_MODEL,\n        api_key=APIKEY,\n        base_url=BASE_URL,\n    )\n\n\nasync def get_embedding_dim():\n    test_text = [\"This is a test sentence.\"]\n    embedding = await embedding_func(test_text)\n    embedding_dim = embedding.shape[1]\n    return embedding_dim\n\n\nasync def init():\n    # Detect embedding dimension\n    embedding_dimension = await get_embedding_dim()\n    print(f\"Detected embedding dimension: {embedding_dimension}\")\n    # Create Oracle DB connection\n    # The `config` parameter is the connection configuration of Oracle DB\n    # More docs here https://python-oracledb.readthedocs.io/en/latest/user_guide/connection_handling.html\n    # We storage data in unified tables, so we need to set a `workspace` parameter to specify which docs we want to store and query\n    # Below is an example of how to connect to Oracle Autonomous Database on Oracle Cloud\n\n    oracle_db = OracleDB(\n        config={\n            \"user\": \"\",\n            \"password\": \"\",\n            \"dsn\": \"\",\n            \"config_dir\": \"\",\n            \"wallet_location\": \"\",\n            \"wallet_password\": \"\",\n            \"workspace\": \"\",\n        }  # specify which docs you want to store and query\n    )\n\n    # Check if Oracle DB tables exist, if not, tables will be created\n    await oracle_db.check_tables()\n    # Initialize LightRAG\n    # We use Oracle DB as the KV/vector/graph storage\n    rag = LightRAG(\n        enable_llm_cache=False,\n        working_dir=WORKING_DIR,\n        chunk_token_size=512,\n        llm_model_func=llm_model_func,\n        embedding_func=EmbeddingFunc(\n            embedding_dim=embedding_dimension,\n            max_token_size=512,\n            func=embedding_func,\n        ),\n        graph_storage=\"OracleGraphStorage\",\n        kv_storage=\"OracleKVStorage\",\n        vector_storage=\"OracleVectorDBStorage\",\n    )\n\n    # Setthe KV/vector/graph storage's `db` property, so all operation will use same connection pool\n    rag.graph_storage_cls.db = oracle_db\n    rag.key_string_value_json_storage_cls.db = oracle_db\n    rag.vector_db_storage_cls.db = oracle_db\n\n    return rag\n\n\n# Data models\n\n\nclass QueryRequest(BaseModel):\n    query: str\n    mode: str = \"hybrid\"\n    only_need_context: bool = False\n\n\nclass InsertRequest(BaseModel):\n    text: str\n\n\nclass Response(BaseModel):\n    status: str\n    data: Optional[str] = None\n    message: Optional[str] = None\n\n\n# API routes\n\nrag = None\n\n\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n    global rag\n    rag = await init()\n    print(\"done!\")\n    yield\n\n\napp = FastAPI(\n    title=\"LightRAG API\", description=\"API for RAG operations\", lifespan=lifespan\n)\n\n\n@app.post(\"/query\", response_model=Response)\nasync def query_endpoint(request: QueryRequest):\n    try:\n        # loop = asyncio.get_event_loop()\n        result = await rag.aquery(\n            request.query,\n            param=QueryParam(\n                mode=request.mode, only_need_context=request.only_need_context\n            ),\n        )\n        return Response(status=\"success\", data=result)\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n\n\n@app.post(\"/insert\", response_model=Response)\nasync def insert_endpoint(request: InsertRequest):\n    try:\n        loop = asyncio.get_event_loop()\n        await loop.run_in_executor(None, lambda: rag.insert(request.text))\n        return Response(status=\"success\", message=\"Text inserted successfully\")\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n\n\n@app.post(\"/insert_file\", response_model=Response)\nasync def insert_file(file: UploadFile = File(...)):\n    try:\n        file_content = await file.read()\n        # Read file content\n        try:\n            content = file_content.decode(\"utf-8\")\n        except UnicodeDecodeError:\n            # If UTF-8 decoding fails, try other encodings\n            content = file_content.decode(\"gbk\")\n        # Insert file content\n        loop = asyncio.get_event_loop()\n        await loop.run_in_executor(None, lambda: rag.insert(content))\n\n        return Response(\n            status=\"success\",\n            message=f\"File content from {file.filename} inserted successfully\",\n        )\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n\n\n@app.get(\"/health\")\nasync def health_check():\n    return {\"status\": \"healthy\"}\n\n\nif __name__ == \"__main__\":\n    import uvicorn\n\n    uvicorn.run(app, host=\"0.0.0.0\", port=8020)\n\n# Usage example\n# To run the server, use the following command in your terminal:\n# python lightrag_api_openai_compatible_demo.py\n\n# Example requests:\n# 1. Query:\n# curl -X POST \"http://127.0.0.1:8020/query\" -H \"Content-Type: application/json\" -d '{\"query\": \"your query here\", \"mode\": \"hybrid\"}'\n\n# 2. Insert text:\n# curl -X POST \"http://127.0.0.1:8020/insert\" -H \"Content-Type: application/json\" -d '{\"text\": \"your text here\"}'\n\n# 3. Insert file:\n# curl -X POST \"http://127.0.0.1:8020/insert_file\" -H \"Content-Type: application/json\" -d '{\"file_path\": \"path/to/your/file.txt\"}'\n\n# 4. Health check:\n# curl -X GET \"http://127.0.0.1:8020/health\"\n"}
{"type": "source_file", "path": "examples/lightrag_hf_demo.py", "content": "import os\n\nfrom lightrag import LightRAG, QueryParam\nfrom lightrag.llm import hf_model_complete, hf_embedding\nfrom lightrag.utils import EmbeddingFunc\nfrom transformers import AutoModel, AutoTokenizer\n\nWORKING_DIR = \"./dickens\"\n\nif not os.path.exists(WORKING_DIR):\n    os.mkdir(WORKING_DIR)\n\nrag = LightRAG(\n    working_dir=WORKING_DIR,\n    llm_model_func=hf_model_complete,\n    llm_model_name=\"meta-llama/Llama-3.1-8B-Instruct\",\n    embedding_func=EmbeddingFunc(\n        embedding_dim=384,\n        max_token_size=5000,\n        func=lambda texts: hf_embedding(\n            texts,\n            tokenizer=AutoTokenizer.from_pretrained(\n                \"sentence-transformers/all-MiniLM-L6-v2\"\n            ),\n            embed_model=AutoModel.from_pretrained(\n                \"sentence-transformers/all-MiniLM-L6-v2\"\n            ),\n        ),\n    ),\n)\n\n\nwith open(\"./book.txt\", \"r\", encoding=\"utf-8\") as f:\n    rag.insert(f.read())\n\n# Perform naive search\nprint(\n    rag.query(\"What are the top themes in this story?\", param=QueryParam(mode=\"naive\"))\n)\n\n# Perform local search\nprint(\n    rag.query(\"What are the top themes in this story?\", param=QueryParam(mode=\"local\"))\n)\n\n# Perform global search\nprint(\n    rag.query(\"What are the top themes in this story?\", param=QueryParam(mode=\"global\"))\n)\n\n# Perform hybrid search\nprint(\n    rag.query(\"What are the top themes in this story?\", param=QueryParam(mode=\"hybrid\"))\n)\n"}
{"type": "source_file", "path": "examples/graph_visual_with_html.py", "content": "import networkx as nx\nfrom pyvis.network import Network\nimport random\n\n# Load the GraphML file\nG = nx.read_graphml(\"./dickens/graph_chunk_entity_relation.graphml\")\n\n# Create a Pyvis network\nnet = Network(height=\"100vh\", notebook=True)\n\n# Convert NetworkX graph to Pyvis network\nnet.from_nx(G)\n\n\n# Add colors and title to nodes\nfor node in net.nodes:\n    node[\"color\"] = \"#{:06x}\".format(random.randint(0, 0xFFFFFF))\n    if \"description\" in node:\n        node[\"title\"] = node[\"description\"]\n\n# Add title to edges\nfor edge in net.edges:\n    if \"description\" in edge:\n        edge[\"title\"] = edge[\"description\"]\n\n# Save and display the network\nnet.show(\"knowledge_graph.html\")\n"}
{"type": "source_file", "path": "examples/lightrag_lmdeploy_demo.py", "content": "import os\n\nfrom lightrag import LightRAG, QueryParam\nfrom lightrag.llm import lmdeploy_model_if_cache, hf_embedding\nfrom lightrag.utils import EmbeddingFunc\nfrom transformers import AutoModel, AutoTokenizer\n\nWORKING_DIR = \"./dickens\"\n\nif not os.path.exists(WORKING_DIR):\n    os.mkdir(WORKING_DIR)\n\n\nasync def lmdeploy_model_complete(\n    prompt=None,\n    system_prompt=None,\n    history_messages=[],\n    keyword_extraction=False,\n    **kwargs,\n) -> str:\n    model_name = kwargs[\"hashing_kv\"].global_config[\"llm_model_name\"]\n    return await lmdeploy_model_if_cache(\n        model_name,\n        prompt,\n        system_prompt=system_prompt,\n        history_messages=history_messages,\n        ## please specify chat_template if your local path does not follow original HF file name,\n        ## or model_name is a pytorch model on huggingface.co,\n        ## you can refer to https://github.com/InternLM/lmdeploy/blob/main/lmdeploy/model.py\n        ## for a list of chat_template available in lmdeploy.\n        chat_template=\"llama3\",\n        # model_format ='awq', # if you are using awq quantization model.\n        # quant_policy=8, # if you want to use online kv cache, 4=kv int4, 8=kv int8.\n        **kwargs,\n    )\n\n\nrag = LightRAG(\n    working_dir=WORKING_DIR,\n    llm_model_func=lmdeploy_model_complete,\n    llm_model_name=\"meta-llama/Llama-3.1-8B-Instruct\",  # please use definite path for local model\n    embedding_func=EmbeddingFunc(\n        embedding_dim=384,\n        max_token_size=5000,\n        func=lambda texts: hf_embedding(\n            texts,\n            tokenizer=AutoTokenizer.from_pretrained(\n                \"sentence-transformers/all-MiniLM-L6-v2\"\n            ),\n            embed_model=AutoModel.from_pretrained(\n                \"sentence-transformers/all-MiniLM-L6-v2\"\n            ),\n        ),\n    ),\n)\n\n\nwith open(\"./book.txt\", \"r\", encoding=\"utf-8\") as f:\n    rag.insert(f.read())\n\n# Perform naive search\nprint(\n    rag.query(\"What are the top themes in this story?\", param=QueryParam(mode=\"naive\"))\n)\n\n# Perform local search\nprint(\n    rag.query(\"What are the top themes in this story?\", param=QueryParam(mode=\"local\"))\n)\n\n# Perform global search\nprint(\n    rag.query(\"What are the top themes in this story?\", param=QueryParam(mode=\"global\"))\n)\n\n# Perform hybrid search\nprint(\n    rag.query(\"What are the top themes in this story?\", param=QueryParam(mode=\"hybrid\"))\n)\n"}
{"type": "source_file", "path": "Gradio/graph_visual_with_html.py", "content": "import networkx as nx\nfrom pyvis.network import Network\nimport random\nimport os\n\n\nclass KnowledgeGraphVisualizer:\n    def __init__(self, graphml_path: str):\n        \"\"\"\n        初始化 KnowledgeGraphVisualizer 类。\n\n        :param graphml_path: 输入的 GraphML 文件路径。\n        \"\"\"\n        self.graphml_path = graphml_path\n        self.graph = None\n        self.net = None\n        self.html_output_path = None\n\n    def load_graph(self):\n        \"\"\"\n        从 GraphML 文件加载图形。\n        \"\"\"\n        if not os.path.exists(self.graphml_path):\n            raise FileNotFoundError(f\"GraphML file not found at: {self.graphml_path}\")\n\n        # 加载图形\n        self.graph = nx.read_graphml(self.graphml_path)\n\n    def create_network(self):\n        \"\"\"\n        创建 Pyvis 网络，并设置节点颜色。\n        \"\"\"\n        # 创建一个 Pyvis 网络实例\n        self.net = Network(height=\"100vh\", notebook=True)\n\n        # 将 NetworkX 图转为 Pyvis 网络\n        self.net.from_nx(self.graph)\n\n        # 给每个节点添加随机颜色\n        for node in self.net.nodes:\n            node[\"color\"] = \"#{:06x}\".format(random.randint(0, 0xFFFFFF))\n\n    def save_html(self):\n        \"\"\"\n        保存生成的网络图为 HTML 文件。\n        \"\"\"\n        # 获取 HTML 输出路径（保存在与 GraphML 文件相同的目录）\n        output_dir = os.path.dirname(self.graphml_path)  # 获取 GraphML 文件所在的目录\n        os.makedirs(output_dir, exist_ok=True)  # 确保目录存在\n\n        # 使用 GraphML 文件名生成 HTML 文件名，替换扩展名为 .html\n        file_name_without_ext = os.path.splitext(os.path.basename(self.graphml_path))[0]\n        self.html_output_path = os.path.join(output_dir, f\"knowledge_graph.html\")\n\n        # 保存并显示网络图\n        self.net.show(self.html_output_path)\n\n    def generate_graph(self):\n        \"\"\"\n        生成知识图谱并保存为 HTML 文件。\n        \"\"\"\n        # 加载图形\n        self.load_graph()\n\n        # 创建网络并添加颜色\n        self.create_network()\n\n        # 保存 HTML 文件\n        self.save_html()\n\n        return self.html_output_path\n\n\n# 用法示例：\nif __name__ == \"__main__\":\n    # GraphML 文件路径\n    graphml_path = \"H:/LightRAG for Sillytavern/LightRAG-for-OpenAI-Standard-Frontend/graph/第二章.txt_20241218193114/graph_chunk_entity_relation.graphml\"\n\n    # 初始化知识图谱可视化对象\n    visualizer = KnowledgeGraphVisualizer(graphml_path)\n\n    # 生成图并保存为 HTML 文件\n    html_file_path = visualizer.generate_graph()\n    print(f\"HTML file has been saved to: {html_file_path}\")\n"}
{"type": "source_file", "path": "examples/lightrag_api_ollama_demo.py", "content": "from fastapi import FastAPI, HTTPException, File, UploadFile\nfrom pydantic import BaseModel\nimport os\nfrom lightrag import LightRAG, QueryParam\nfrom lightrag.llm import ollama_embedding, ollama_model_complete\nfrom lightrag.utils import EmbeddingFunc\nfrom typing import Optional\nimport asyncio\nimport nest_asyncio\nimport aiofiles\n\n# Apply nest_asyncio to solve event loop issues\nnest_asyncio.apply()\n\nDEFAULT_RAG_DIR = \"index_default\"\napp = FastAPI(title=\"LightRAG API\", description=\"API for RAG operations\")\n\nDEFAULT_INPUT_FILE = \"book.txt\"\nINPUT_FILE = os.environ.get(\"INPUT_FILE\", f\"{DEFAULT_INPUT_FILE}\")\nprint(f\"INPUT_FILE: {INPUT_FILE}\")\n\n# Configure working directory\nWORKING_DIR = os.environ.get(\"RAG_DIR\", f\"{DEFAULT_RAG_DIR}\")\nprint(f\"WORKING_DIR: {WORKING_DIR}\")\n\n\nif not os.path.exists(WORKING_DIR):\n    os.mkdir(WORKING_DIR)\n\n\nrag = LightRAG(\n    working_dir=WORKING_DIR,\n    llm_model_func=ollama_model_complete,\n    llm_model_name=\"gemma2:9b\",\n    llm_model_max_async=4,\n    llm_model_max_token_size=8192,\n    llm_model_kwargs={\"host\": \"http://localhost:11434\", \"options\": {\"num_ctx\": 8192}},\n    embedding_func=EmbeddingFunc(\n        embedding_dim=768,\n        max_token_size=8192,\n        func=lambda texts: ollama_embedding(\n            texts, embed_model=\"nomic-embed-text\", host=\"http://localhost:11434\"\n        ),\n    ),\n)\n\n\n# Data models\nclass QueryRequest(BaseModel):\n    query: str\n    mode: str = \"hybrid\"\n    only_need_context: bool = False\n\n\nclass InsertRequest(BaseModel):\n    text: str\n\n\nclass Response(BaseModel):\n    status: str\n    data: Optional[str] = None\n    message: Optional[str] = None\n\n\n# API routes\n@app.post(\"/query\", response_model=Response)\nasync def query_endpoint(request: QueryRequest):\n    try:\n        loop = asyncio.get_event_loop()\n        result = await loop.run_in_executor(\n            None,\n            lambda: rag.query(\n                request.query,\n                param=QueryParam(\n                    mode=request.mode, only_need_context=request.only_need_context\n                ),\n            ),\n        )\n        return Response(status=\"success\", data=result)\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n\n\n# insert by text\n@app.post(\"/insert\", response_model=Response)\nasync def insert_endpoint(request: InsertRequest):\n    try:\n        loop = asyncio.get_event_loop()\n        await loop.run_in_executor(None, lambda: rag.insert(request.text))\n        return Response(status=\"success\", message=\"Text inserted successfully\")\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n\n\n# insert by file in payload\n@app.post(\"/insert_file\", response_model=Response)\nasync def insert_file(file: UploadFile = File(...)):\n    try:\n        file_content = await file.read()\n        # Read file content\n        try:\n            content = file_content.decode(\"utf-8\")\n        except UnicodeDecodeError:\n            # If UTF-8 decoding fails, try other encodings\n            content = file_content.decode(\"gbk\")\n        # Insert file content\n        loop = asyncio.get_event_loop()\n        await loop.run_in_executor(None, lambda: rag.insert(content))\n\n        return Response(\n            status=\"success\",\n            message=f\"File content from {file.filename} inserted successfully\",\n        )\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n\n\n# insert by local default file\n@app.post(\"/insert_default_file\", response_model=Response)\n@app.get(\"/insert_default_file\", response_model=Response)\nasync def insert_default_file():\n    try:\n        # Read file content from book.txt\n        async with aiofiles.open(INPUT_FILE, \"r\", encoding=\"utf-8\") as file:\n            content = await file.read()\n        print(f\"read input file {INPUT_FILE} successfully\")\n        # Insert file content\n        loop = asyncio.get_event_loop()\n        await loop.run_in_executor(None, lambda: rag.insert(content))\n\n        return Response(\n            status=\"success\",\n            message=f\"File content from {INPUT_FILE} inserted successfully\",\n        )\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n\n\n@app.get(\"/health\")\nasync def health_check():\n    return {\"status\": \"healthy\"}\n\n\nif __name__ == \"__main__\":\n    import uvicorn\n\n    uvicorn.run(app, host=\"0.0.0.0\", port=8020)\n\n# Usage example\n# To run the server, use the following command in your terminal:\n# python lightrag_api_openai_compatible_demo.py\n\n# Example requests:\n# 1. Query:\n# curl -X POST \"http://127.0.0.1:8020/query\" -H \"Content-Type: application/json\" -d '{\"query\": \"your query here\", \"mode\": \"hybrid\"}'\n\n# 2. Insert text:\n# curl -X POST \"http://127.0.0.1:8020/insert\" -H \"Content-Type: application/json\" -d '{\"text\": \"your text here\"}'\n\n# 3. Insert file:\n# curl -X POST \"http://127.0.0.1:8020/insert_file\" -H \"Content-Type: application/json\" -d '{\"file_path\": \"path/to/your/file.txt\"}'\n\n# 4. Health check:\n# curl -X GET \"http://127.0.0.1:8020/health\"\n"}
{"type": "source_file", "path": "examples/lightrag_ollama_age_demo.py", "content": "import asyncio\nimport inspect\nimport logging\nimport os\n\nfrom lightrag import LightRAG, QueryParam\nfrom lightrag.llm import ollama_embedding, ollama_model_complete\nfrom lightrag.utils import EmbeddingFunc\n\nWORKING_DIR = \"./dickens_age\"\n\nlogging.basicConfig(format=\"%(levelname)s:%(message)s\", level=logging.INFO)\n\nif not os.path.exists(WORKING_DIR):\n    os.mkdir(WORKING_DIR)\n\n# AGE\nos.environ[\"AGE_POSTGRES_DB\"] = \"postgresDB\"\nos.environ[\"AGE_POSTGRES_USER\"] = \"postgresUser\"\nos.environ[\"AGE_POSTGRES_PASSWORD\"] = \"postgresPW\"\nos.environ[\"AGE_POSTGRES_HOST\"] = \"localhost\"\nos.environ[\"AGE_POSTGRES_PORT\"] = \"5455\"\nos.environ[\"AGE_GRAPH_NAME\"] = \"dickens\"\n\nrag = LightRAG(\n    working_dir=WORKING_DIR,\n    llm_model_func=ollama_model_complete,\n    llm_model_name=\"llama3.1:8b\",\n    llm_model_max_async=4,\n    llm_model_max_token_size=32768,\n    llm_model_kwargs={\"host\": \"http://localhost:11434\", \"options\": {\"num_ctx\": 32768}},\n    embedding_func=EmbeddingFunc(\n        embedding_dim=768,\n        max_token_size=8192,\n        func=lambda texts: ollama_embedding(\n            texts, embed_model=\"nomic-embed-text\", host=\"http://localhost:11434\"\n        ),\n    ),\n    graph_storage=\"AGEStorage\",\n)\n\nwith open(\"./book.txt\", \"r\", encoding=\"utf-8\") as f:\n    rag.insert(f.read())\n\n# Perform naive search\nprint(\n    rag.query(\"What are the top themes in this story?\", param=QueryParam(mode=\"naive\"))\n)\n\n# Perform local search\nprint(\n    rag.query(\"What are the top themes in this story?\", param=QueryParam(mode=\"local\"))\n)\n\n# Perform global search\nprint(\n    rag.query(\"What are the top themes in this story?\", param=QueryParam(mode=\"global\"))\n)\n\n# Perform hybrid search\nprint(\n    rag.query(\"What are the top themes in this story?\", param=QueryParam(mode=\"hybrid\"))\n)\n\n# stream response\nresp = rag.query(\n    \"What are the top themes in this story?\",\n    param=QueryParam(mode=\"hybrid\", stream=True),\n)\n\n\nasync def print_stream(stream):\n    async for chunk in stream:\n        print(chunk, end=\"\", flush=True)\n\n\nif inspect.isasyncgen(resp):\n    asyncio.run(print_stream(resp))\nelse:\n    print(resp)\n"}
{"type": "source_file", "path": "examples/generate_query.py", "content": "from openai import OpenAI\n\n# os.environ[\"OPENAI_API_KEY\"] = \"\"\n\n\ndef openai_complete_if_cache(\n    model=\"gpt-4o-mini\", prompt=None, system_prompt=None, history_messages=[], **kwargs\n) -> str:\n    openai_client = OpenAI()\n\n    messages = []\n    if system_prompt:\n        messages.append({\"role\": \"system\", \"content\": system_prompt})\n    messages.extend(history_messages)\n    messages.append({\"role\": \"user\", \"content\": prompt})\n\n    response = openai_client.chat.completions.create(\n        model=model, messages=messages, **kwargs\n    )\n    return response.choices[0].message.content\n\n\nif __name__ == \"__main__\":\n    description = \"\"\n    prompt = f\"\"\"\n    Given the following description of a dataset:\n\n    {description}\n\n    Please identify 5 potential users who would engage with this dataset. For each user, list 5 tasks they would perform with this dataset. Then, for each (user, task) combination, generate 5 questions that require a high-level understanding of the entire dataset.\n\n    Output the results in the following structure:\n    - User 1: [user description]\n        - Task 1: [task description]\n            - Question 1:\n            - Question 2:\n            - Question 3:\n            - Question 4:\n            - Question 5:\n        - Task 2: [task description]\n            ...\n        - Task 5: [task description]\n    - User 2: [user description]\n        ...\n    - User 5: [user description]\n        ...\n    \"\"\"\n\n    result = openai_complete_if_cache(model=\"gpt-4o-mini\", prompt=prompt)\n\n    file_path = \"./queries.txt\"\n    with open(file_path, \"w\") as file:\n        file.write(result)\n\n    print(f\"Queries written to {file_path}\")\n"}
{"type": "source_file", "path": "examples/batch_eval.py", "content": "import re\nimport json\nimport jsonlines\n\nfrom openai import OpenAI\n\n\ndef batch_eval(query_file, result1_file, result2_file, output_file_path):\n    client = OpenAI()\n\n    with open(query_file, \"r\") as f:\n        data = f.read()\n\n    queries = re.findall(r\"- Question \\d+: (.+)\", data)\n\n    with open(result1_file, \"r\") as f:\n        answers1 = json.load(f)\n    answers1 = [i[\"result\"] for i in answers1]\n\n    with open(result2_file, \"r\") as f:\n        answers2 = json.load(f)\n    answers2 = [i[\"result\"] for i in answers2]\n\n    requests = []\n    for i, (query, answer1, answer2) in enumerate(zip(queries, answers1, answers2)):\n        sys_prompt = \"\"\"\n        ---Role---\n        You are an expert tasked with evaluating two answers to the same question based on three criteria: **Comprehensiveness**, **Diversity**, and **Empowerment**.\n        \"\"\"\n\n        prompt = f\"\"\"\n        You will evaluate two answers to the same question based on three criteria: **Comprehensiveness**, **Diversity**, and **Empowerment**.\n\n        - **Comprehensiveness**: How much detail does the answer provide to cover all aspects and details of the question?\n        - **Diversity**: How varied and rich is the answer in providing different perspectives and insights on the question?\n        - **Empowerment**: How well does the answer help the reader understand and make informed judgments about the topic?\n\n        For each criterion, choose the better answer (either Answer 1 or Answer 2) and explain why. Then, select an overall winner based on these three categories.\n\n        Here is the question:\n        {query}\n\n        Here are the two answers:\n\n        **Answer 1:**\n        {answer1}\n\n        **Answer 2:**\n        {answer2}\n\n        Evaluate both answers using the three criteria listed above and provide detailed explanations for each criterion.\n\n        Output your evaluation in the following JSON format:\n\n        {{\n            \"Comprehensiveness\": {{\n                \"Winner\": \"[Answer 1 or Answer 2]\",\n                \"Explanation\": \"[Provide explanation here]\"\n            }},\n            \"Empowerment\": {{\n                \"Winner\": \"[Answer 1 or Answer 2]\",\n                \"Explanation\": \"[Provide explanation here]\"\n            }},\n            \"Overall Winner\": {{\n                \"Winner\": \"[Answer 1 or Answer 2]\",\n                \"Explanation\": \"[Summarize why this answer is the overall winner based on the three criteria]\"\n            }}\n        }}\n        \"\"\"\n\n        request_data = {\n            \"custom_id\": f\"request-{i+1}\",\n            \"method\": \"POST\",\n            \"url\": \"/v1/chat/completions\",\n            \"body\": {\n                \"model\": \"gpt-4o-mini\",\n                \"messages\": [\n                    {\"role\": \"system\", \"content\": sys_prompt},\n                    {\"role\": \"user\", \"content\": prompt},\n                ],\n            },\n        }\n\n        requests.append(request_data)\n\n    with jsonlines.open(output_file_path, mode=\"w\") as writer:\n        for request in requests:\n            writer.write(request)\n\n    print(f\"Batch API requests written to {output_file_path}\")\n\n    batch_input_file = client.files.create(\n        file=open(output_file_path, \"rb\"), purpose=\"batch\"\n    )\n    batch_input_file_id = batch_input_file.id\n\n    batch = client.batches.create(\n        input_file_id=batch_input_file_id,\n        endpoint=\"/v1/chat/completions\",\n        completion_window=\"24h\",\n        metadata={\"description\": \"nightly eval job\"},\n    )\n\n    print(f\"Batch {batch.id} has been created.\")\n\n\nif __name__ == \"__main__\":\n    batch_eval()\n"}
{"type": "source_file", "path": "examples/lightrag_api_open_webui_demo.py", "content": "from datetime import datetime, timezone\nfrom fastapi import FastAPI\nfrom fastapi.responses import StreamingResponse\nimport inspect\nimport json\nfrom pydantic import BaseModel\nfrom typing import Optional\n\nimport os\nimport logging\nfrom lightrag import LightRAG, QueryParam\nfrom lightrag.llm import ollama_model_complete, ollama_embed\nfrom lightrag.utils import EmbeddingFunc\n\nimport nest_asyncio\n\nWORKING_DIR = \"./dickens\"\n\nlogging.basicConfig(format=\"%(levelname)s:%(message)s\", level=logging.INFO)\n\nif not os.path.exists(WORKING_DIR):\n    os.mkdir(WORKING_DIR)\n\nrag = LightRAG(\n    working_dir=WORKING_DIR,\n    llm_model_func=ollama_model_complete,\n    llm_model_name=\"qwen2.5:latest\",\n    llm_model_max_async=4,\n    llm_model_max_token_size=32768,\n    llm_model_kwargs={\"host\": \"http://localhost:11434\", \"options\": {\"num_ctx\": 32768}},\n    embedding_func=EmbeddingFunc(\n        embedding_dim=1024,\n        max_token_size=8192,\n        func=lambda texts: ollama_embed(\n            texts=texts, embed_model=\"bge-m3:latest\", host=\"http://127.0.0.1:11434\"\n        ),\n    ),\n)\n\nwith open(\"./book.txt\", \"r\", encoding=\"utf-8\") as f:\n    rag.insert(f.read())\n\n# Apply nest_asyncio to solve event loop issues\nnest_asyncio.apply()\n\napp = FastAPI(title=\"LightRAG\", description=\"LightRAG API open-webui\")\n\n\n# Data models\nMODEL_NAME = \"LightRAG:latest\"\n\n\nclass Message(BaseModel):\n    role: Optional[str] = None\n    content: str\n\n\nclass OpenWebUIRequest(BaseModel):\n    stream: Optional[bool] = None\n    model: Optional[str] = None\n    messages: list[Message]\n\n\n# API routes\n\n\n@app.get(\"/\")\nasync def index():\n    return \"Set Ollama link to http://ip:port/ollama in Open-WebUI Settings\"\n\n\n@app.get(\"/ollama/api/version\")\nasync def ollama_version():\n    return {\"version\": \"0.4.7\"}\n\n\n@app.get(\"/ollama/api/tags\")\nasync def ollama_tags():\n    return {\n        \"models\": [\n            {\n                \"name\": MODEL_NAME,\n                \"model\": MODEL_NAME,\n                \"modified_at\": \"2024-11-12T20:22:37.561463923+08:00\",\n                \"size\": 4683087332,\n                \"digest\": \"845dbda0ea48ed749caafd9e6037047aa19acfcfd82e704d7ca97d631a0b697e\",\n                \"details\": {\n                    \"parent_model\": \"\",\n                    \"format\": \"gguf\",\n                    \"family\": \"qwen2\",\n                    \"families\": [\"qwen2\"],\n                    \"parameter_size\": \"7.6B\",\n                    \"quantization_level\": \"Q4_K_M\",\n                },\n            }\n        ]\n    }\n\n\n@app.post(\"/ollama/api/chat\")\nasync def ollama_chat(request: OpenWebUIRequest):\n    resp = rag.query(\n        request.messages[-1].content, param=QueryParam(mode=\"hybrid\", stream=True)\n    )\n    if inspect.isasyncgen(resp):\n\n        async def ollama_resp(chunks):\n            async for chunk in chunks:\n                yield (\n                    json.dumps(\n                        {\n                            \"model\": MODEL_NAME,\n                            \"created_at\": datetime.now(timezone.utc).strftime(\n                                \"%Y-%m-%dT%H:%M:%S.%fZ\"\n                            ),\n                            \"message\": {\n                                \"role\": \"assistant\",\n                                \"content\": chunk,\n                            },\n                            \"done\": False,\n                        },\n                        ensure_ascii=False,\n                    ).encode(\"utf-8\")\n                    + b\"\\n\"\n                )  # the b\"\\n\" is important\n\n        return StreamingResponse(ollama_resp(resp), media_type=\"application/json\")\n    else:\n        return resp\n\n\n@app.get(\"/health\")\nasync def health_check():\n    return {\"status\": \"healthy\"}\n\n\nif __name__ == \"__main__\":\n    import uvicorn\n\n    uvicorn.run(app, host=\"0.0.0.0\", port=8020)\n"}
{"type": "source_file", "path": "Gradio_web.py", "content": "import ast\nimport json\nimport mimetypes\nimport os\nimport shutil\nimport sys\nimport webbrowser\nimport zipfile\nimport subprocess\nimport asyncio\nimport importlib.metadata\n\nimport pkg_resources\nfrom IPython.terminal.ipapp import frontend_flags\nfrom packaging.requirements import Requirement\nfrom packaging.version import Version, InvalidVersion\nfrom importlib.metadata import distributions\nfrom time import sleep\nfrom typing import List, Tuple\nfrom datetime import datetime, time, timedelta\n\nimport gradio as gr\nimport httpx\nfrom click import style\nfrom dotenv import load_dotenv, set_key\nfrom pathlib import Path\n\nfrom fastapi import requests\nfrom numba.typed.listobject import new_list\n\nfrom playwright.sync_api import sync_playwright\nfrom scipy.ndimage import label\nfrom sympy import false\n\nload_dotenv()\n\n# 加载 .env 文件\nENV_FILE = \".env\"\nenv_path = Path(ENV_FILE)\nPort = os.getenv(\"API_port\",\"\")\n# 配置 RAG 后端的基础 URL\nRAG_API_URL = f\"http://localhost:{Port}/v1\"\n# Constants\nSUPPORTED_FILE_TYPES = ['txt','pdf','doc','ppt','csv']\nFILE_BACKUP_DIR = \"./backup/files\"\nGRAPH_BACKUP_DIR = \"./backup/graph\"\nENV_VARS = os.getenv(\"RAG_DIR\",\"\")\nBUILT_YOUR_GRAPH_SCRIPT = \"./build_your_graph.py\"\n\nStart_page_IsNotShow = os.getenv(\"start_page_show\",\"\") == 'True'\n\n# 创建必要的备份目录\nos.makedirs(FILE_BACKUP_DIR, exist_ok=True)\nos.makedirs(GRAPH_BACKUP_DIR, exist_ok=True)\n\n# 环境变量获取与更新<开始>\n\ndef get_env_variables():\n    \"\"\"\n    读取所有环境变量并以字典形式返回\n    \"\"\"\n    keys = [\n        \"RAG_DIR\",\n        \"file_DIR\",\n        \"API_port\",\n        \"OPENAI_API_KEY\",\n        \"LLM_MODEL\",\n        \"LLM_MODEL_TOKEN_SIZE\",\n        \"EMBEDDING_MODEL\",\n        \"EMBEDDING_MAX_TOKEN_SIZE\",\n        \"OPENAI_BASE_URL\",\n        \"start_page_IsNotShow\",\n        \"FRONTEND_PORT\",\n    ]\n    return {key: os.getenv(key, \"\") for key in keys}\n\ndef update_env_variable(key, value):\n    \"\"\"\n    更新 .env 文件中的某个环境变量\n    \"\"\"\n    if key not in get_env_variables():\n        return f\"Error: {key} is not a valid environment variable.\"\n\n    set_key(env_path, key, value)\n    os.environ[key] = value  # 同时更新当前环境变量\n    return f\"Successfully updated {key} to {value}.\"\n\ndef reset_env_variable(key):\n    \"\"\"\n    重置某个环境变量为空\n    \"\"\"\n    return update_env_variable(key, \"\")\n\n# 环境变量获取与更新<结束>\n\n\n# 文档文件管理<开始>\n\ndef list_files_in_folder(folder=\"./files\"):\n    \"\"\"\n    列出指定目录及子目录下的支持的文件类型，并返回相对路径列表，按文件名排序。\n\n    参数:\n        folder (str): 指定的根目录，默认为 \"./text\"。\n\n    返回:\n        list: 包含相对路径的文件列表。\n    \"\"\"\n    all_files = []\n    folder = os.path.abspath(folder)  # 获取根目录的绝对路径\n\n    for root, _, files in os.walk(folder):\n        for file in files:\n            if file.split(\".\")[-1].lower() in SUPPORTED_FILE_TYPES:\n                # 拼接文件路径并确保是相对路径\n                full_path = os.path.join(root, file)\n                rel_path = \".\" + os.path.relpath(full_path, start=os.getcwd())  # 转为相对路径\n                rel_path = rel_path.replace(\"\\\\\", \"/\")\n                all_files.append(full_path)\n    return sorted(all_files)\n\ndef refresh_file_list_display():\n    \"\"\"\n    刷新文件列表，返回文件名 Markdown 列表和文件路径字典\n    \"\"\"\n    files = list_files_in_folder()\n    file_dict = {}\n\n    for file_path in files:\n        file_name = os.path.basename(file_path)\n\n        # 如果文件名已存在，添加文件创建时间作为后缀\n        if file_name in file_dict:\n            creation_time = datetime.fromtimestamp(os.path.getctime(file_path)).strftime('%Y-%m-%d_%H-%M-%S')\n            unique_file_name = f\"{file_name}--{creation_time}\"\n            file_dict[unique_file_name] = file_path # 特殊文件名 -> 文件路径\n        else:\n            file_dict[file_name] = file_path  # 文件名 -> 文件路径\n    markdown_list = \"\\n\".join(file_dict.keys())  # 生成文件名列表\n    return markdown_list, file_dict\n\ndef refresh_dropdown_choices(file_dict):\n    \"\"\"\n    根据文件字典生成 Dropdown 的可选项\n    \"\"\"\n    if file_dict is None:\n        return []  # 防止错误发生，返回空列表\n    #print(list(file_dict.keys()))\n    return list(file_dict.keys())  # 返回所有文件名\n\ndef derefresh_dropdown_choices_temp():\n    \"\"\"\n    根据文件字典生成 Dropdown 的可选项\n    \"\"\"\n    markdown_list, file_dict = refresh_file_list_display()\n    if file_dict is None:\n        return []  # 防止错误发生，返回空列表\n    #print(list(file_dict.keys()))\n    return list(file_dict.keys())  # 返回所有文件名\n\ndef handle_file_selection(file_name, file_dict):\n    \"\"\"根据选择的文件名返回完整路径\"\"\"\n    return file_dict.get(file_name, None)\n\ndef open_text_folder(folder_paths):\n    \"\"\"在文件资源管理器中打开指定文件夹，删除路径中的文件名\"\"\"\n    folder_paths = eval(folder_paths)\n    if isinstance(folder_paths, list):\n\n        for folder_path in folder_paths:\n            folder_path = os.path.dirname(folder_path)  # 获取文件夹路径\n            try:\n                if os.name == \"nt\":  # Windows\n                    os.startfile(folder_path)\n                elif os.name == \"posix\":  # macOS/Linux\n                    os.system(f\"open {folder_path}\" if sys.platform == \"darwin\" else f\"xdg-open {folder_path}\")\n            except Exception as e:\n                return f\"打开文件夹失败：{str(e)}\"\n    else:\n        return \"错误：未传入文件夹路径列表。\"\n\n    return f\"已成功打开 {len(folder_paths)} 个文件夹。\"\n\ndef open_text_file(file_paths):\n    \"\"\"使用系统默认程序打开多个文件\"\"\"\n    file_paths = eval(file_paths)\n    if not isinstance(file_paths, list) or len(file_paths) == 0:\n        return \"错误：未传入文件路径列表。\"\n\n    results = []\n    for file_path in file_paths:\n        if not os.path.isfile(file_path):\n            results.append(f\"❌ 文件不存在：{file_path}\")\n            continue\n\n        try:\n            if os.name == \"nt\":  # Windows\n                os.startfile(file_path)\n            elif os.name == \"posix\":  # macOS/Linux\n                os.system(f\"open {file_path}\" if sys.platform == \"darwin\" else f\"xdg-open {file_path}\")\n            results.append(f\"✅ 文件 {file_path} 已打开。\")\n        except Exception as e:\n            results.append(f\"❌ 打开文件失败：{file_path}，错误：{str(e)}\")\n\n    return \"\\n\".join(results)\n\ndef set_rag_env_variable(file_paths):\n    \"\"\"\n    设置 file_DIR 环境变量的值为指定文件的路径。\n\n    参数:\n    - file_path (str): 文件路径，必须是相对路径且在 ./text/ 目录下。\n\n    返回:\n    - str: 设置结果信息。\n    \"\"\"\n    file_paths = eval(file_paths)\n    file_path = (\"./\" + os.path.relpath(file_paths[0], start=os.getcwd())).replace(\"\\\\\", \"/\")  # 转为相对路径\n    print(file_path)\n    # 验证文件路径是否符合要求\n    if not file_path.startswith(\"./files/\"):\n        return \"Error: 文件路径必须位于 ./files/ 目录下。\"\n\n    if not os.path.isfile(file_path):\n        return f\"Error: 文件 {file_path} 不存在。\"\n\n    # 获取当前 file_DIR 的值\n    current_value = os.getenv(\"file_DIR\", \"\")\n    reset_result = reset_env_variable(\"file_DIR\")  # 重置 RAG_DIR 环境变量\n\n    if \"Error\" in reset_result:\n        return f\"重置 file_DIR 失败: {reset_result}\"\n\n    # 将路径转换为 Windows 风格（用反斜杠）\n    windows_style_path = file_path.replace(\"\\\\\", \"/\")\n\n    # 更新 .env 文件和环境变量\n    update_result = update_env_variable(\"file_DIR\", windows_style_path)\n\n    if \"Successfully updated\" in update_result:\n        return (\n            f\"file_DIR 更新成功！\\n\"\n            f\"旧值: {current_value}\\n\"\n            f\"新值: {windows_style_path}\"\n        )\n    else:\n        return f\"更新失败: {update_result}\"\n\ndef delete_file_with_backup(file_paths):\n    \"\"\"删除多个文件，先备份后删除\"\"\"\n    file_paths = eval(file_paths)\n    if not isinstance(file_paths, list) or len(file_paths) == 0:\n        return \"错误：未传入文件路径列表。\"\n\n    results = []\n    for file_path in file_paths:\n        try:\n            if not os.path.isfile(file_path):\n                results.append(f\"❌ 文件不存在：{file_path}\")\n                continue\n\n            backup_name = os.path.join(\n                FILE_BACKUP_DIR,\n                f\"{os.path.basename(file_path)}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n            )\n            shutil.copy2(file_path, backup_name)  # 备份文件\n            os.remove(file_path)  # 删除文件\n            results.append(f\"✅ 文件 {file_path} 已删除，备份存储在 {backup_name}\")\n        except Exception as e:\n            results.append(f\"❌ 删除失败：{file_path}，错误：{str(e)}\")\n\n    return \"\\n\".join(results)\n\ndef create_unique_folder(file_name):\n    \"\"\"\n    根据文件名在 ./files 中创建唯一的文件夹，不包含文件格式后缀。\n    \"\"\"\n    base_folder = \"./files\"\n    # 去除文件名的后缀\n    folder_name = os.path.splitext(file_name)[0]\n    folder_path = os.path.join(base_folder, folder_name)\n\n    # 如果文件夹已存在，则生成带有时间戳的唯一文件夹名称\n    if os.path.exists(folder_path):\n        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        folder_name = f\"{folder_name}_{timestamp}\"\n        folder_path = os.path.join(base_folder, folder_name)\n\n    # 创建文件夹\n    os.makedirs(folder_path, exist_ok=True)\n    return folder_path\n\ndef upload_files_and_save(files):\n    \"\"\"\n    上传多个文件并保存到新创建的文件夹中。\n    :param files: 上传的文件列表（Gradio 返回的文件列表）\n    :return: 每个文件的处理结果列表\n    \"\"\"\n    if not files or len(files) == 0:\n        return \"未上传任何文件。\"\n\n    results = []  # 用于存储每个文件的处理结果\n    uploaded_files = {}  # 用于存储成功上传文件的字典\n\n    for file in files:\n        file_name = os.path.basename(file)\n        try:\n            # 确定文件名和扩展名\n            file_ext = file_name.split('.')[-1].lower()\n\n            # 验证文件类型\n            if file_ext not in SUPPORTED_FILE_TYPES:\n                results.append(f\"❌ 不支持的文件类型: {file_name} ({file_ext})。支持的类型包括: {', '.join(SUPPORTED_FILE_TYPES)}\")\n                continue\n\n            # 创建唯一文件夹\n            folder_path = create_unique_folder(file_name)\n            os.makedirs(folder_path, exist_ok=True)\n\n            # 目标文件路径\n            file_path = os.path.join(folder_path, file_name)\n\n            # 获取 Gradio 返回的文件路径并复制到目标路径\n            shutil.copy(file, file_path)\n\n            # 记录成功上传的文件\n            uploaded_files[file_name] = file_path\n\n            results.append(f\"✅ 文件 {file_name} 上传成功，已保存至文件夹: {folder_path}\")\n        except Exception as e:\n            results.append(f\"❌ 文件 {file_name} 上传过程中出现错误: {str(e)}\")\n\n    return \"\\n\".join(results),uploaded_files,uploaded_files\n\ndef debug_file(file):\n    if not file:\n        return \"未上传任何文件。\"\n\n    try:\n        return {\n            \"文件名\": file.name,\n            \"文件类型\": str(type(file)),\n            \"支持的操作\": dir(file),\n        }\n    except Exception as e:\n        return f\"调试文件信息时出错: {str(e)}\"\n\ndef build_graph_for_files(prebuild_dict:dict):\n    \"\"\"\n    构建知识图谱：调用服务端接口处理多个文件\n    :param prebuild_dict: 字典，key 是文件名，value 是文件路径\n    :return: 多文件的构建结果\n    \"\"\"\n    if isinstance(prebuild_dict, str):\n        prebuild_dict = ast.literal_eval(prebuild_dict)\n    base_path = \"./graph\"\n    file_name = list(prebuild_dict.keys())\n    timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n    graph_path = os.path.join(base_path, file_name[0])\n\n    if os.path.exists(graph_path):\n        graph_path = f\"{graph_path}_{timestamp}\"\n    os.makedirs(graph_path, exist_ok=True)\n    graph_path = graph_path.replace(\"\\\\\", \"/\")\n    update_env_variable(\"RAG_DIR\", graph_path)\n    try:\n        # 调用文件上传与处理接口\n        sleep(1.0)\n        response = asyncio.run(upload_files_to_rag(prebuild_dict))\n\n        # 如果响应中包含文件处理结果，则返回\n        if isinstance(response, list):\n            return response  # 返回服务端的文件处理结果列表\n        else:\n            # 否则返回错误信息\n            return [{\"status\": \"failed\", \"message\": response.get(\"message\", \"Unknown error\")}]\n    except Exception as e:\n        return [{\"status\": \"failed\", \"message\": f\"Failed to build graph: {str(e)}\"}]\n\ndef insert_graph_for_files(preinsert_dict):\n    \"\"\"\n    构建知识图谱：调用服务端接口处理多个文件\n    :param preinsert_dict: 字典，key 是文件名，value 是文件路径\n    :return: 多文件的插入结果\n    \"\"\"\n    if isinstance(preinsert_dict, str):\n        preinsert_dict = ast.literal_eval(preinsert_dict)\n    try:\n        # 调用文件上传与处理接口\n        response = asyncio.run(upload_files_to_rag(preinsert_dict))\n\n        # 如果响应中包含文件处理结果，则返回\n        if isinstance(response, list):\n            return response  # 返回服务端的文件处理结果列表\n        else:\n            # 否则返回错误信息\n            return [{\"status\": \"failed\", \"message\": response.get(\"message\", \"Unknown error\")}]\n    except Exception as e:\n        return [{\"status\": \"failed\", \"message\": f\"Failed to insert graph: {str(e)}\"}]\n\n\nasync def upload_files_to_rag(prebuild_dict_result, purpose=\"knowledge_graph_frontend\"):\n    \"\"\"\n    上传文件名和路径字典到 RAG 系统\n    :param prebuild_dict_result: 字典，key 是文件名，value 是文件路径\n    :param purpose: 上传的目的\n    :return: 服务端返回的多文件处理结果\n    \"\"\"\n    load_dotenv(override=True)\n    retries = 5  # 最大重试次数\n    async with httpx.AsyncClient(timeout=300.0) as client:\n        for attempt in range(1, retries + 1):\n            try:\n                # 构造请求体\n                payload = {\n                    \"files\": prebuild_dict_result,\n                    \"purpose\": purpose,\n                }\n                response = await client.post(RAG_API_URL + f\"/files\", json=payload)\n\n                # 检查响应状态\n                if response.status_code == 200:\n                    find_html_file(os.getenv(\"RAG_DIR\"))\n                    return response.json()  # 成功返回 JSON 数据\n                else:\n                    find_html_file(os.getenv(\"RAG_DIR\"))\n                    return {\n                        \"status\": \"failed\",\n                        \"message\": f\"Server returned error: {response.status_code}, {response.text}\",\n                    }\n            except Exception as e:\n                if attempt == retries:\n                    return {\n                        \"status\": \"failed\",\n                        \"message\": f\"Failed to communicate with server: {str(e)}\",\n                    }\n\ndef debug_and_return(name):\n    \"\"\"\n    返回文件名，同时输出调试信息\n    \"\"\"\n    #debug_message = f\"调试：当前选择的文件名是 {name}\"\n    #print(debug_message)  # 控制台调试\n    return name\n\n\n# 文档文件管理<结束>\n\n\n# 图谱管理<开始>\n'''\ndef setup_file_upload_interaction(file_uploader, purpose_input, upload_button, upload_result):\n    \"\"\"设置文件上传交互逻辑\"\"\"\n    upload_button.click(\n        fn=upload_file_to_rag,\n        inputs=[file_uploader, purpose_input],\n        outputs=upload_result,\n    )\n'''\ndef list_subdirectories(base_path=\"./graph\"):\n    \"\"\"\n    列出指定文件夹下的所有次级文件夹，并返回文件夹名称与其绝对路径的映射字典。\n    对于重名文件夹，添加创建时间后缀以区分。\n    \"\"\"\n    if not os.path.exists(base_path):\n        return {}, \"The specified base path does not exist.\"\n\n    subdirectories = [d for d in os.listdir(base_path) if os.path.isdir(os.path.join(base_path, d))]\n    folder_dict = {}\n\n    for folder in subdirectories:\n        folder_path = os.path.join(base_path, folder)\n        if folder in folder_dict:\n            # 如果出现同名文件夹，添加创建时间后缀\n            creation_time = datetime.fromtimestamp(os.path.getctime(folder_path)).strftime('%Y-%m-%d_%H-%M-%S')\n            unique_folder_name = f\"{folder}--{creation_time}\"\n            folder_dict[unique_folder_name] = os.path.abspath(folder_path)\n        else:\n            folder_dict[folder] = os.path.abspath(folder_path)\n    # 生成 Markdown 格式文件名列表\n    markdown_list = \"\\n\".join(list(folder_dict.keys()))\n    selective_list = list(folder_dict.keys())\n    return markdown_list,folder_dict,selective_list\n\ndef open_rag_folder(folder_path):\n    \"\"\"在文件资源管理器中打开指定文件夹\"\"\"\n    if os.name == \"nt\":  # Windows\n        os.startfile(folder_path)\n    elif os.name == \"posix\":  # macOS/Linux\n        os.system(f\"open {folder_path}\" if sys.platform == \"darwin\" else f\"xdg-open {folder_path}\")\n\ndef backup_and_delete_graph_folder(selected_graph_abs_path):\n    \"\"\"\n    备份知识图谱文件夹并删除原路径\n    :param selected_graph_abs_path: 即将要删除的路径（绝对路径）\n    :return: 操作结果字符串\n    \"\"\"\n    try:\n        # 检查路径有效性\n        if not selected_graph_abs_path or not os.path.exists(selected_graph_abs_path):\n            return \"无法备份，路径不存在或未提供。\"\n\n        # 提取文件夹名作为变量\n        folder_name = os.path.basename(os.path.normpath(selected_graph_abs_path))\n        PreBackup_folder = f\"{folder_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n        backup_path = os.path.join(GRAPH_BACKUP_DIR, PreBackup_folder)\n\n        # 创建备份文件夹\n        os.makedirs(GRAPH_BACKUP_DIR, exist_ok=True)\n\n        # 备份路径\n        shutil.copytree(selected_graph_abs_path, backup_path)\n\n        # 删除原路径\n        shutil.rmtree(selected_graph_abs_path)\n\n        return f\"备份成功！图谱已备份至 {backup_path}，并成功删除图谱。\"\n    except Exception as e:\n        return f\"备份或删除过程中出现错误: {str(e)}\"\n\ndef find_html_file(folder_path, filename=\"knowledge_graph.html\"):\n    \"\"\"在指定文件夹下递归查找 HTML 文件\"\"\"\n    for root, _, files in os.walk(folder_path):\n        if filename in files:\n            file_path = os.path.join(root, filename)\n            webbrowser.open(file_path)\n            return os.path.join(root, filename)\n    return None\n\n# 单个 ZIP 文件解压逻辑\nasync def upload_and_extract_zip(file, base_path=\"./files\"):\n    \"\"\"上传并解压 zip 文件\"\"\"\n    folder_name = os.path.splitext(os.path.basename(file.name))[0]\n    timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n    dest_folder = os.path.join(base_path, folder_name)\n\n    if os.path.exists(dest_folder):\n        dest_folder = f\"{dest_folder}_{timestamp}\"\n    os.makedirs(dest_folder, exist_ok=True)\n\n    try:\n        with zipfile.ZipFile(file, 'r') as zip_ref:\n            # 尝试以 GBK 解码\n            for zip_info in zip_ref.infolist():\n                zip_info.filename = zip_info.filename.encode('cp437').decode('utf-8')  # 转换编码\n                zip_ref.extract(zip_info, dest_folder)\n            print(\"utf8\")\n    except UnicodeDecodeError:\n        # 使用 GBK 重读 ZIP 文件\n        with zipfile.ZipFile(file, 'r') as zip_ref:\n            for zip_info in zip_ref.infolist():\n                zip_info.filename = zip_info.filename.encode('cp437').decode('gbk')  # 转换编码\n                zip_ref.extract(zip_info, dest_folder)\n            print(\"gbk\")\n\n\n    return f\"✅ 文件 {file.name} 已解压至: {dest_folder}\"\n\n# 将多个文件处理的逻辑拆分出来\nasync def process_uploaded_zips_with_progress(files,progress=gr.Progress(track_tqdm=True)):\n    \"\"\"处理多个 ZIP 文件的解压逻辑，使用 Gradio 进度条\"\"\"\n    if not files or len(files) == 0:\n        return \"未上传任何文件。\"\n    idx = 0\n    results = []\n    total_files = len(files)\n    progress(0,desc=\"正在处理中，请稍后...\",total=total_files)# 使用 Gradio 的进度条\n    for file in progress.tqdm(files):\n        try:\n            result = await upload_and_extract_zip(file)\n            results.append(result)\n        except Exception as e:\n            results.append(f\"❌ 文件 {file.name} 解压失败: {str(e)}\")\n        progress.update(idx + 1)  # 更新进度\n\n    return \"\\n\".join(results)\n\n\ndef set_env_variable_from_folder(folder_path):\n    \"\"\"将文件夹路径设置为环境变量\"\"\"\n    update_env_variable(\"RAG_DIR\", folder_path)\n    return f\"已将路径 {folder_path} 设置为环境变量 RAG_DIR\"\n\n# 图谱管理<结束>\n\n# 欢迎界面<开始>\n\ndef load_readme():\n    \"\"\"加载 README.md 内容\"\"\"\n    try:\n        with open(\"README.md\", \"r\", encoding=\"utf-8\") as file:\n            return file.read()\n    except FileNotFoundError:\n        return \"README.md 文件未找到，请检查项目目录。\"\n\ndef load_license():\n    \"\"\"加载开源协议内容\"\"\"\n    try:\n        with open(\"LICENSE\", \"r\", encoding=\"utf-8\") as file:\n            return file.read()\n    except FileNotFoundError:\n        return \"LICENSE 文件未找到，请检查项目目录。\"\n\n\ndef load_requirements(file_path=\"requirements.txt\"):\n    \"\"\"\n    读取 requirements.txt 中的依赖包信息，并支持复杂版本约束。\n    :param file_path: requirements.txt 文件路径\n    :return: (包名列表, 完整依赖行列表)\n    \"\"\"\n    try:\n        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n            lines = f.read().splitlines()\n\n        package_names = []\n        valid_requirements = []\n\n        for line in lines:\n            line = line.strip()\n            if not line or line.startswith(\"#\"):\n                # 忽略空行和注释行\n                continue\n\n            try:\n                # 解析依赖项\n                req = Requirement(line)\n                package_names.append(req.name.lower())\n                valid_requirements.append(line)\n            except Exception as e:\n                # 如果某行不是有效的依赖格式，输出警告或跳过\n                print(f\"⚠️ 无法解析的依赖项: {line}. 错误: {e}\")\n\n        return package_names, valid_requirements\n\n    except FileNotFoundError:\n        raise FileNotFoundError(f\"找不到指定的文件: {file_path}\")\n    except Exception as e:\n        raise ValueError(f\"加载依赖项时出错: {str(e)}\")\n\ndef check_installed_packages():\n    \"\"\"\n    获取当前环境中已安装的依赖包及版本。\n    \"\"\"\n    installed_packages = {\n        dist.key: dist.version for dist in pkg_resources.working_set\n    }\n    return installed_packages\n\ndef parse_requirement(requirement):\n    \"\"\"\n    解析依赖项（支持复杂版本约束）。\n    \"\"\"\n    try:\n        req = Requirement(requirement)\n        return req.name.lower(), req.specifier\n    except Exception as e:\n        raise ValueError(f\"无法解析依赖项: {requirement}. 错误: {str(e)}\")\n\ndef check_dependency_status():\n    \"\"\"检查依赖包状态\"\"\"\n    required_packages, full_requirements = load_requirements()\n    installed_packages = check_installed_packages()\n\n    missing_packages = []\n    mismatched_versions = []\n\n    for req in full_requirements:\n        try:\n            pkg, specifier = parse_requirement(req)\n            if pkg not in installed_packages:\n                missing_packages.append(f\"🚫 {req}\")\n            else:\n                installed_version = Version(installed_packages[pkg])\n                if not specifier.contains(installed_version):\n                    mismatched_versions.append(\n                        f\"⚠️ {pkg} (expected {specifier}, found {installed_version})\"\n                    )\n        except InvalidVersion as e:\n            mismatched_versions.append(f\"⚠️ 无法解析版本: {req}. 错误: {str(e)}\")\n\n    if not missing_packages and not mismatched_versions:\n        return \"✅ 所有依赖包已安装\", [], []\n    else:\n        return (\n            \"部分依赖包存在问题，请查看下方列表。\",\n            missing_packages,\n            mismatched_versions,\n        )\n\n\ndef install_missing_packages(missing_packages):\n    \"\"\"安装缺失的依赖包\"\"\"\n    try:\n        for package in missing_packages:\n            pkg = package.split(\" \")[1]  # 提取包名（忽略符号 🚫）\n            subprocess.check_call([\"pip\", \"install\", pkg])\n        return \"✅ 缺失的依赖包已成功安装\"\n    except subprocess.CalledProcessError as e:\n        return f\"❌ 安装失败: {e}\"\n\n\n# 安装按钮逻辑\ndef install_and_update(missing_packages):\n            if not missing_packages:\n                return \"没有需要安装的依赖包\"\n            install_result = install_missing_packages(missing_packages)\n            status, _, _ = check_dependency_status()  # 检查安装后的状态\n            return status, install_result\n\nasync def check_lightrag_status():\n    \"\"\"检查 LightRAG 后端状态\"\"\"\n    retries = 5  # 最大重试次数\n    async with httpx.AsyncClient(timeout=5.0) as client:  # 设置超时时间\n        for attempt in range(1, retries + 1):\n            try:\n                response = await client.post(RAG_API_URL + \"/connect\")\n                if response.status_code == 200:\n                    data = response.json()\n                    if isinstance(data, dict) and data.get(\"connective\") is True:\n                        return \"✅LightRAG 后端运行正常\"\n            except (httpx.ConnectError, httpx.TimeoutException):\n                # 捕获连接错误或超时\n                if attempt == retries:\n                    return \"❌LightRAG 后端未正常运行\"\n                continue  # 继续重试\n    return \"❌LightRAG 后端未正常运行，可点击💻以尝试启动\"\n\ndef check_model_connection_status():\n    \"\"\"检查大模型连接状态\"\"\"\n    # 示例实现，可以扩展为实际模型连接的检查逻辑\n    return \"✅大模型连接成功\"\n\ndef check_port():\n    port = os.getenv(\"API_port\",\"\")\n    web = f\"http://localhost:{port}/v1\"\n    return web\n\n# 刷新按钮逻辑\ndef refresh_status():\n    status, missing, mismatched = check_dependency_status()\n    return (\n        status,\n        missing + mismatched,  # 展示所有缺失和版本问题\n        bool(missing or mismatched),\n    )\n\n# 欢迎界面<结束>\n\n\n# HTML to Graph<开始>\n\n# 全局状态\nSTATE = {\n    \"notification_hidden_until\": \"2024-12-5 00:00:00\",  # 通知栏隐藏截止时间\n    \"dependencies_installed\": False    # 是否已安装依赖\n}\n\n# 检查依赖是否已安装\ndef check_dependencies():\n    # 假设依赖为某个 pip 包，例如 'some_package'\n    try:\n        import some_package\n        return True\n    except ImportError:\n        return False\n\n# 安装依赖逻辑\ndef install_dependencies():\n    try:\n        os.system(\"pip install some_package\")  # 替换为实际依赖\n        return True, \"依赖安装成功！\"\n    except Exception as e:\n        return False, f\"安装依赖时出错: {str(e)}\"\n\ndef handle_notification_action(action, remember=False):\n    today = datetime.today()  # 当前日期时间\n    if action == \"install\":\n        success, message = install_dependencies()\n        STATE[\"dependencies_installed\"] = success\n        return message, success\n    elif action == \"dismiss\":\n        if remember:\n            STATE[\"notification_hidden_until\"] = str(datetime.strptime(str(today + timedelta(days=7)),\"%Y-%m-%d\"))\n        else:\n            STATE[\"notification_hidden_until\"] = \"2024-12-1 00:00:00\"\n        return True, True\n\n# 构建通知栏逻辑\ndef should_show_notification():\n        \"\"\"判断是否应该显示通知栏\"\"\"\n        today = datetime.today()\n        hidden_until = datetime.strptime(STATE.get(\"notification_hidden_until\"), \"%Y-%m-%d %H:%M:%S\")\n        diff = today - hidden_until\n        #print(diff.days >= 7)\n        return diff.days >= 7 # 如果超过7天，则显示通知栏\n\ndef handle_install_dependencies():\n        \"\"\"处理安装依赖的逻辑\"\"\"\n        STATE[\"dependencies_installed\"], message = install_dependencies()\n        return message, should_show_notification()\n\ndef close_notification(remember):\n        \"\"\"关闭通知栏逻辑\"\"\"\n        if remember:\n            STATE[\"notification_hidden_until\"] = datetime.today().strftime(\"%Y-%m-%d %H:%M:%S\")\n        return False, False,True\n\n# 构建通知栏\ndef notification_ui():\n    with gr.Blocks() as notification_ui:\n        notification_bar = gr.Group()  # 通知栏\n\n        with notification_bar:\n            gr.Markdown(\"### 通知：此页面为可选功能，依赖尚未安装。\")\n            gr.Markdown(\"请根据需求安装依赖，或直接开始使用。\")\n            install_btn = gr.Button(\"我已知晓并开始安装相关依赖\")\n            close_btn = gr.Button(\"开始使用\")\n            remember_checkbox = gr.Checkbox(label=\"七天内不再显示\")\n\n            # 按钮交互\n            install_btn.click(\n                fn=handle_install_dependencies,\n                inputs=[],\n                outputs=[notification_bar, notification_ui]\n            )\n            close_btn.click(\n                fn=close_notification,\n                inputs=[remember_checkbox],\n                outputs=[notification_bar, notification_ui]\n            )\n\n    return notification_ui,\"调试：通知栏\"\n\n# 转换HTML到PDF函数\ndef html_to_pdf(urls, output_dir=\"./PDF_generate\"):\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    generated_files = []\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    with sync_playwright() as p:\n        browser = p.chromium.launch()\n        for url in urls:\n            page = browser.new_page()\n            page.goto(url)\n            domain = url.split(\"//\")[-1].split(\"/\")[0]  # 提取域名\n            file_name = f\"{domain}-{timestamp}.pdf\"\n            file_dir = os.path.join(output_dir, domain)\n            if not os.path.exists(file_dir):\n                os.makedirs(file_dir)\n            output_path = os.path.join(file_dir, file_name)\n            page.pdf(path=output_path)\n            generated_files.append(output_path)\n        browser.close()\n    return generated_files\n\n# 打开 PDF 功能\ndef open_pdf(filepath):\n    if os.path.exists(filepath):\n        os.system(f\"start {filepath}\")  # Windows 上打开文件\n        return f\"打开 PDF 文件: {filepath}\"\n    else:\n        return \"文件不存在，请检查路径！\"\n\n# 删除PDF并备份\ndef delete_pdf_with_backup(pdf_paths, backup_dir=\"./backup/PDF_generate\"):\n    if not os.path.exists(backup_dir):\n        os.makedirs(backup_dir)\n    deleted_files = []\n    for pdf in pdf_paths:\n        if os.path.exists(pdf):\n            backup_path = os.path.join(backup_dir, os.path.basename(pdf))\n            shutil.move(pdf, backup_path)\n            deleted_files.append((pdf, backup_path))\n    return deleted_files\n\n# HTML to Graph<结束>\n\n# 侧边栏<开始>\n\n# 通过后端服务获取模型信息\nasync def fetch_model_info(base_url, api_key):\n    headers = {\n        \"Authorization\": f\"Bearer {api_key}\"\n    }\n    async with httpx.AsyncClient() as client:\n        try:\n            response = await client.get(RAG_API_URL+ f\"/models\")\n            response.raise_for_status()\n            response_data = response.json()\n            all_models = [model[\"id\"] for model in response_data.get(\"data\", [])]\n            # 筛选逻辑\n            large_models = [\n                m for m in all_models\n                if isinstance(m, str) and \"embedding\" not in m.lower() and \"embed\" not in m.lower()\n            ]\n            embed_models = [\n                m for m in all_models\n                if isinstance(m, str) and (\"embedding\" in m.lower() or \"embed\" in m.lower())\n            ]\n            return large_models, embed_models\n        except Exception as e:\n            return str(e), []\n\n# 获取环境变量的初始值\ndef get_initial_values():\n    base_url = os.getenv(\"OPENAI_BASE_URL\", \"\")\n    api_key = os.getenv(\"OPENAI_API_KEY\", \"\")\n    API_port = os.getenv(\"API_port\", \"\")\n    LLM = os.getenv(\"EMBEDDING_MODEL\",\"\")\n    EMBED = os.getenv(\"EMBEDDING_MODEL\",\"\")\n    api_key_display = \"API_KEY已保存\" if api_key else \"\"\n    return base_url, api_key_display,API_port\n\n# 检查并补全 BASE_URL\ndef normalize_base_url(base_url):\n    base_url = base_url.strip()  # 去除首尾空格\n    if not base_url.endswith(\"/v1\"):  # 检查是否以 /v1 结尾\n        if not base_url.endswith(\"/\"):  # 如果没有末尾的斜杠，先添加\n            base_url += \"/\"\n        base_url += \"v1\"\n    return base_url\n\ndef load_model_configs(json_file):\n    \"\"\"\n    从 JSON 文件中动态加载模型配置，并按照模型类别存入字典。\n    :param json_file: JSON 文件路径\n    :return: (LLM 模型字典, Embedding 模型字典)\n    \"\"\"\n    try:\n        with open(json_file, 'r', encoding='utf-8') as file:\n            data = json.load(file)\n\n        llm_models = {}\n        embedding_models = {}\n\n        # 遍历 JSON 数据并分类\n        for model_category, models in data.items():\n            if model_category == \"LLM\":\n                llm_models.update(models)\n            elif model_category == \"Embedding\":\n                embedding_models.update(models)\n\n        return llm_models, embedding_models\n    except Exception as e:\n        raise ValueError(f\"加载 JSON 文件失败: {str(e)}\")\n\ndef get_max_tokens(llm_name, embedding_name):\n    \"\"\"\n    根据模型名称获取其对应的 Max_tokens\n    :param llm_name: 大模型名称\n    :param embedding_name: 嵌入模型名称\n    :return: (LLM 模型 Max_tokens, Embedding 模型 Max_tokens)\n    \"\"\"\n    llm_dict,embedding_dict = load_model_configs(\"./models.json\")\n    llm_tokens = llm_dict.get(llm_name, None)\n    embedding_tokens = embedding_dict.get(embedding_name, None)\n\n    if llm_tokens is None:\n        raise ValueError(f\"大模型 '{llm_name}' 的 Max_tokens 未找到。\")\n    if embedding_tokens is None:\n        raise ValueError(f\"嵌入模型 '{embedding_name}' 的 Max_tokens 未找到。\")\n\n    return llm_tokens, embedding_tokens\n\n# 保存设置的逻辑\ndef save_settings(base_url, api_key,port,llm_max_tokens,embed_max_tokens):\n    base_url = normalize_base_url(base_url)  # 检查并补全 BASE_URL\n    api_key = api_key.strip()\n    port = str(port).strip()\n    llm_max_token = str(llm_max_tokens).strip()\n    embed_max_token = str(embed_max_tokens).strip()\n    Port = os.getenv(\"API_port\",\"\") #全局变量更新\n    update_env_variable(\"OPENAI_BASE_URL\", base_url)\n    update_env_variable(\"API_port\",port)\n    update_env_variable(\"LLM_MODEL_TOKEN_SIZE\",llm_max_token)\n    update_env_variable(\"EMBEDDING_MAX_TOKEN_SIZE\",embed_max_token)\n    if api_key and api_key != \"API_KEY已保存\":\n        os.environ[\"OPENAI_API_KEY\"] = api_key\n\n# 侧边栏<结束>\n\n# UI 构建模块化函数\n\ndef notification_bar():\n    \"\"\"\n    纯 CSS 实现右上角通知栏，支持多条消息堆叠和自动消失。还暂时不可用。\n    \"\"\"\n    html_content = \"\"\"\n    <style>\n    #notifications-container {\n        position: fixed;\n        top: 10px;\n        right: 10px;\n        z-index: 1000; /* 确保通知栏显示在最上层 */\n        display: flex;\n        flex-direction: column;\n        gap: 10px; /* 通知栏之间的间距 */\n        pointer-events: none; /* 确保鼠标点击穿透到主页面 */\n    }\n\n    .notification {\n        background-color: #4caf50; /* 默认绿色通知 */\n        color: white;\n        padding: 10px 20px;\n        border-radius: 5px;\n        font-size: 14px;\n        box-shadow: 0 2px 5px rgba(0,0,0,0.2);\n        opacity: 0; /* 初始透明 */\n        transform: translateY(-20px); /* 初始上移 */\n        animation: slideInOut 5s ease-in-out forwards; /* 动画控制显示与隐藏 */\n    }\n\n    .notification.error {\n        background-color: #f44336; /* 红色通知 */\n    }\n\n    .notification.warning {\n        background-color: #ff9800; /* 橙色通知 */\n    }\n\n    .notification.success {\n        background-color: #4caf50; /* 绿色通知 */\n    }\n\n    @keyframes slideInOut {\n        0% { opacity: 0; transform: translateY(-20px); }\n        10% { opacity: 1; transform: translateY(0); }\n        90% { opacity: 1; transform: translateY(0); }\n        100% { opacity: 0; transform: translateY(-20px); }\n    }\n    </style>\n    <div id=\"notifications-container\"></div>\n    <script>\n    function addNotification(message, type = 'success') {\n        const container = document.getElementById('notifications-container');\n        if (!container) return;\n\n        // 创建通知元素\n        const notification = document.createElement('div');\n        notification.className = `notification ${type}`;\n        notification.textContent = message;\n\n        // 添加到容器中\n        container.appendChild(notification);\n\n        // 自动移除通知\n        setTimeout(() => {\n            notification.style.opacity = '0';\n            notification.addEventListener('transitionend', () => notification.remove());\n        }, 5000); // 5 秒后自动删除\n    }\n\n    // 测试用通知（可移除）\n    setTimeout(() => addNotification('保存成功！', 'success'), 1000);\n    setTimeout(() => addNotification('保存失败：请检查输入！', 'error'), 2000);\n    setTimeout(() => addNotification('警告：API Key 将过期！', 'warning'), 3000);\n    </script>\n    \"\"\"\n    return html_content\n\ndef sidebar_ui():\n    custom_css = \"\"\"\n        .SideBar {\n            width: auto !important;\n            height: 100% !important;\n            max-width: 25% !important;\n            background-color: #f5f5f5;\n            padding: 10px;\n            box-shadow: 2px 0 5px rgba(0, 0, 0, 0.1);\n        }\n\n        .Closed-SideBar {\n            width: auto !important;\n            height: 100% !important;\n            max-width: 5% !important;\n            background-color: #f5f5f5;\n            box-shadow: 2px 0 5px rgba(0, 0, 0, 0.1);\n            display: flex;\n            justify-content: flex-end; /* 将内容靠右对齐 */\n        }\n        \n        #Closed-SideBar-button {\n            width: auto !important;\n            height: 100% !important;\n            max-width: 5% !important;\n            background: linear-gradient(90deg, #4caf50, #8bc34a);\n            color: white;\n            border: none;\n            border-radius: 5px;\n            padding: 10px;\n            font-size: 1rem;\n            cursor: pointer;\n            transition: background 0.3s ease-in-out;\n            display: flex;\n            justify-content: flex-end; /* 将内容靠右对齐 */\n        }\n\n        .gradient-button {\n            background: linear-gradient(90deg, #4caf50, #8bc34a);\n            color: white;\n            border: none;\n            border-radius: 5px;\n            padding: 10px;\n            font-size: 1rem;\n            cursor: pointer;\n            transition: background 0.3s ease-in-out;\n        }\n\n        .gradient-button:hover {\n            background: linear-gradient(90deg, #8bc34a, #4caf50);\n        }\n        \n        #ASideBar {\n            text-align: center; /* 居中对齐 */\n            font-size: 28px; /* 字体大小 */\n            font-weight: bold; /* 加粗 */\n            background-color: #f5f5f5; /* 背景色与侧边栏一致 */\n            padding: 15px; /* 内边距 */\n            border-radius: 5px; /* 圆角 */\n            box-shadow: 2px 2px 10px rgba(0, 0, 0, 0.1); /* 阴影效果 */\n            margin-bottom: 20px; /* 下边距 */\n            color: #333; /* 字体颜色 */\n        }\n    \"\"\"\n    #gr.HTML(notification_bar())\n    with gr.Blocks() as Thesidebar:\n        with gr.Column(elem_classes=\"SideBar\") as SideBar:\n            with gr.Row():\n                #gr.Markdown(\"侧边栏\",elem_id=\"ASideBar\")\n                close_button = gr.Button(\"❌ 关闭侧边栏\", elem_id=\"close-sidebar\", elem_classes=\"gradient-button\")\n\n            # 输入 BASE_URL 和 API_KEY\n            base_url_input = gr.Textbox(label=\"BASE_URL\", placeholder=\"请输入 BASE_URL(注意要添加/v1)\")\n            api_key_input = gr.Textbox(label=\"API_KEY\", placeholder=\"请输入 API_KEY\")\n            API_port_input = gr.Textbox(label=\"API_PORT\",placeholder=\"请填入你想设置的RAG系统端口\")\n\n\n            # 大模型和嵌入模型下拉框\n            large_model_dropdown = gr.Dropdown(label=\"选择大模型\", elem_id=\"llms\",choices=[],interactive=True)\n            llm_MAX_tokens = gr.Textbox(label=\"大模型的Max_tokens\",elem_id=\"llm_max_tokens\",placeholder=\"请查询所使用的大模型的Max_tokens并填入\")\n            embed_model_dropdown = gr.Dropdown(label=\"选择嵌入模型\", elem_id=\"embedding\",choices=[],interactive=True)\n            embed_MAX_tokens = gr.Textbox(label=\"嵌入的Max_tokens\",elem_id=\"embed_max_tokens\",placeholder=\"请查询所使用的嵌入模型的Max_tokens并填入\")\n\n            gr.Markdown(\"\"\"\n                ### ℹ️ Tips：\n                    - 以上为主要选项。\n                    - 请记得填入基本信息然后保存。\n                    - 请刷新模型信息，选择模型或者填入Tokens后会自动保存对应信息。\n                    - 如果Max_tokens遇到了错误，请在根目录的models.json中添加你使用的模型以及对应的tokens。\n                    \"\"\",\n                    show_copy_button=False,\n                    container=True\n            )\n            # 保存设置按钮\n            save_button = gr.Button(\"保存\", elem_id=\"save-settings\", elem_classes=\"gradient-button\")\n\n            # 获取模型信息按钮\n            fetch_models_button = gr.Button(\"刷新模型信息\", elem_id=\"fetch-models\", elem_classes=\"gradient-button\")\n\n            # 选择上下文策略\n            with gr.Row():\n                gr.Dropdown(\n                    label=\"选择上下文策略（暂时不可用）\",\n                    choices=[\"策略1\", \"策略2\", \"策略3\"],\n                    value=\"策略1\",\n                    interactive=True\n                )\n                gr.Dropdown(\n                    label=\"选择 Prompt（暂时不可用）\",\n                    choices=[\"Prompt1\", \"Prompt2\", \"Prompt3\"],\n                    value=\"Prompt1\",\n                    interactive=True\n                )\n\n\n            with gr.Accordion(label=\"次要设置（暂时不可用）\",elem_id=\"Addition\") as addition:\n                with gr.Column():\n                    gr.Textbox(label=\"次要BASE_URL\",elem_id=\"sub_BASE_URL\",placeholder=\"请输入次要BASE_URL(注意要添加/v1)\")\n                    gr.Textbox(label=\"次要API_KEY\", elem_id=\"sub_API_KEY\",placeholder=\"请输入次要API_KEY\")\n\n                    # 大模型和嵌入模型下拉框\n                    sub_large_model_dropdown = gr.Dropdown(label=\"选择次要大模型\", elem_id=\"sub_llms\", choices=[], interactive=True)\n                    gr.Markdown(\"\"\"\n                        ### ℹ️ Tips：\n                        - 以上为次要选项。\n                        - 在有Rate Limit的情况下可以选择使用，RAG系统会使用以上设定以执行不那么重要的任务。\n                        - 例如问答或者聊天时提取关键词会使用次要大模型，而不是主要的大模型。\n                        \"\"\")\n\n            # 交互逻辑\n            save_button.click(\n                fn=save_settings,\n                inputs=[base_url_input, api_key_input,API_port_input],\n                outputs=None\n            )\n            save_button.click(\n                lambda url, key,port,llm_max_tokens,embed_max_tokens: f\"<script>addNotification('保存成功！', 'success');</script>\"\n                if url and key and port and llm_max_tokens and embed_max_tokens else\n                f\"<script>addNotification('保存失败：请输入完整信息！', 'error');</script>\",\n                inputs=[base_url_input, api_key_input,API_port_input,llm_MAX_tokens,embed_MAX_tokens],\n                outputs=None\n            )\n\n            def return_model_info(base_url_input, api_key_input):\n                large_models, embed_models = asyncio.run(fetch_model_info(base_url_input, api_key_input))\n                update_env_variable(\"LLM_MODEL\", large_models[0])\n                update_env_variable(\"EMBEDDING_MODEL\", embed_models[0])\n                return gr.update(elem_id=\"llms\",choices=large_models,value=large_models[0]),gr.update(elem_id=\"embedding\",choices=embed_models,value=embed_models[0])\n\n            fetch_models_button.click(\n                fn=return_model_info,\n                inputs=[base_url_input, api_key_input],\n                outputs=[large_model_dropdown, embed_model_dropdown],\n            )\n            def large_model_dropdown_input(large_model_dropdown,embed_model_dropdown):\n                update_env_variable(\"LLM_MODEL\",large_model_dropdown)\n                large_model_max_tokens, embed_model_max_tokens = get_max_tokens(large_model_dropdown,embed_model_dropdown)\n                return gr.update(elem_id=\"llm_max_tokens\",value=large_model_max_tokens)\n            large_model_dropdown.change(\n                fn=large_model_dropdown_input,\n                inputs=[large_model_dropdown,embed_model_dropdown],\n                outputs=[llm_MAX_tokens],\n            )\n            def embed_model_dropdown_input(large_model_dropdown,embed_model_dropdown):\n                update_env_variable(\"EMBEDDING_MODEL\", embed_model_dropdown)\n                large_model_max_tokens,embed_model_max_tokens = get_max_tokens(large_model_dropdown,embed_model_dropdown)\n                return gr.update(elem_id=\"embed_max_tokens\",value=embed_model_max_tokens)\n            embed_model_dropdown.change(\n                fn=embed_model_dropdown_input,\n                inputs=[large_model_dropdown,embed_model_dropdown],\n                outputs=[embed_MAX_tokens],\n            )\n            Thesidebar.load(\n                fn=get_initial_values,\n                inputs=[],\n                outputs=[base_url_input, api_key_input,API_port_input]\n            )\n\n            # `closed_sidebar` 定义\n        with gr.Row(elem_classes=\"Closed-SideBar\", visible=False) as closed_sidebar:\n            #gr.Markdown(\"侧边栏\",elem_id=\"ASideBar\")\n            open_button = gr.Button(\"🔓 打开侧边栏\", elem_id=\"Closed-SideBar-button\")\n\n            # 状态更新函数\n\n            def toggle_sidebar():\n                # JS 脚本：切换 sidebar 和 closed_sidebar 的显示状态\n                return gr.update(elem_classes=\"Closed-SideBar\",visible=True), gr.update(elem_classes=\"SideBar\",visible=False)\n\n            def toggle_back_sidebar():\n                # JS 脚本：切换 sidebar 和 closed_sidebar 的显示状态\n                return gr.update(elem_classes=\"SideBar\",visible=True), gr.update(elem_classes=\"Closed-SideBar\",visible=False)\n\n            # 按钮点击事件\n\n        close_button.click(fn=toggle_sidebar, outputs=[closed_sidebar, SideBar])\n        open_button.click(fn=toggle_back_sidebar, outputs=[SideBar, closed_sidebar])\n    return Thesidebar\n\ndef welcome_page():\n    \"\"\"创建欢迎使用页面\"\"\"\n    with gr.Blocks(visible=False, elem_id=\"welcome-page\") as welcome_page:\n        # 标题\n        gr.Markdown(\"# 欢迎使用\", elem_id=\"welcome-title\", elem_classes=\"center-text\")\n\n        # 主体内容\n        with gr.Row():\n            # 左侧 README 内容块\n            with gr.Column(scale=3):\n                gr.Markdown(load_readme(), label=\"项目简介\")\n\n            # 右侧状态栏\n            with gr.Column(scale=1):\n                gr.Markdown(\"## 系统状态\")\n                dependency_status = gr.Textbox(\n                    label=\"依赖包状态\",\n                    value=check_dependency_status()[0],\n                    interactive=False,\n                    placeholder=\"依赖包安装状态显示在此处\"\n                )\n                missing_packages_dropdown = gr.Dropdown(\n                    label=\"缺失依赖包列表\",\n                    choices=[],\n                    visible=True,\n                    interactive=False,\n                    multiselect=True,\n                    allow_custom_value=True,\n                    elem_id=\"Missing_packages_dropdown\"\n                )\n                install_button = gr.Button(\n                    \"安装缺失的依赖包\",\n                    visible=False,\n                    variant=\"primary\",\n                    elem_id=\"Install_button\"\n                )\n                with gr.Column():  # 创建水平布局\n                    lightrag_status = gr.Textbox(\n                        label=\"LightRAG 后端状态\",\n                        value=\"按下方的🔄按钮以进行测试\",\n                        interactive=False,\n                        placeholder=\"后端状态显示在此处\"\n                    )\n                    with gr.Row():\n                        '''\n                        lightrag_fireup_button = gr.Button(\n                            \"💻\",\n                            size=\"sm\",  # 小按钮\n                            elem_id=\"fireup-btn\",  # 为按钮设置 ID，方便样式定制\n                            min_width = 100,\n                        )'''\n                        lightrag_status_refresh_button = gr.Button(\n                            \"🔄\",\n                            size=\"sm\",  # 小按钮\n                            elem_id=\"status-refresh-btn\",  # 为按钮设置 ID，方便样式定制\n                            min_width = 100,\n                        )\n                \"\"\"\n                ####不再使用\n                model_connection_status = gr.Textbox(\n                    label=\"大模型连接状态\",\n                    value=check_model_connection_status(),\n                    interactive=False,\n                    placeholder=\"模型连接状态显示在此处\"\n                )\n                \"\"\"\n                api_port = gr.Textbox(\n                    label=\"LightRAG后端地址\",\n                    value=check_port(),\n                    interactive=False,\n                    elem_id=\"API_port\",\n                    placeholder=\"当前的后端地址为\",\n                    show_copy_button=True\n                )\n\n                refresh_button = gr.Button(\"🔄刷新状态\", variant=\"primary\")\n\n        # 底部链接与开源协议\n        with gr.Row():\n            with gr.Column(scale=3):\n                gr.Markdown(\"### 📂 项目链接\")\n                gr.Markdown(\"\"\"\n                - [GitHub 仓库](https://github.com/HerSophia/LightRAGforSillyTavern)\n                - [项目使用说明书](https://your_docs_link)\n                - [视频教程](https://your_video_link)\n                \"\"\")\n\n            with gr.Column(scale=1):\n                license_textbox = gr.Textbox(\n                    label=\"开源协议\",\n                    value=load_license(),\n                    lines=10,\n                    interactive=False\n                )\n        # 页面初始化时的检查逻辑\n        def initialize_status():\n            status, missing, mismatched = check_dependency_status()\n            if (len(mismatched)):\n                show_missing_packages_dropdown = False\n            else:\n                show_missing_packages_dropdown = True\n            all_issues = missing + mismatched\n            show_install_button = bool(missing)  # 仅缺失包时显示安装按钮\n            return (\n                status,\n                gr.update(elem_id=\"Missing_packages_dropdown\",visible=show_missing_packages_dropdown),\n                all_issues,\n                missing,  # 控制安装按钮是否显示\n                gr.update(elem_id=\"Install_button\",visible=show_install_button)\n            )\n\n        welcome_page.load(\n            fn=initialize_status,\n            inputs=[],\n            outputs=[\n                dependency_status,\n                missing_packages_dropdown,\n                missing_packages_dropdown,\n                missing_packages_dropdown,\n                install_button,\n            ],\n        )\n\n        # 刷新按钮逻辑\n        def refresh_status():\n            status, missing, mismatched = check_dependency_status()\n            all_issues = missing + mismatched\n            show_install_button = bool(missing)\n            new_web = check_port()\n            return (\n                status,\n                all_issues,\n                missing,  # 控制安装按钮是否显示\n                gr.update(elem_id=\"Install_button\",visible=show_install_button),\n                gr.update(elem_id=\"API_port\"),\n                new_web\n            )\n\n        def lightrag_status_refresh():\n            lightrag_status = asyncio.run(check_lightrag_status())\n            return lightrag_status\n\n        lightrag_status_refresh_button.click(\n            fn=lightrag_status_refresh,\n            inputs=[],\n            outputs=[lightrag_status]\n        )\n        refresh_button.click(\n            fn=refresh_status,\n            inputs=[],\n            outputs=[\n                dependency_status,\n                missing_packages_dropdown,\n                missing_packages_dropdown,\n                install_button,\n                api_port,\n                api_port\n            ],\n        )\n\n        # 安装按钮逻辑\n        def install_and_update(missing_packages):\n            if not missing_packages:\n                return \"没有需要安装的依赖包\"\n            install_result = install_missing_packages(missing_packages)\n            status, _, _ = check_dependency_status()  # 检查安装后的状态\n            return status, install_result\n\n        install_button.click(\n            fn=install_and_update,\n            inputs=[missing_packages_dropdown],\n            outputs=[\n                dependency_status\n            ],\n        )\n    return welcome_page\n\ndef file_management_ui():\n    \"\"\"\n    创建文本文件管理页面 UI\n    \"\"\"\n    with gr.Blocks() as file_ui:\n        gr.Markdown(\"# 📂 文本文件管理\")\n\n        # 左侧文件列表\n        with gr.Row():\n            with gr.Column():\n                # 显示文件列表\n                file_list_output = gr.Textbox(label=\"文件列表\", lines=15, interactive=False)\n                refresh_files_button = gr.Button(\"🔄 刷新文件列表\", variant=\"primary\")\n\n            # 右侧文件操作\n            with gr.Column():\n                selected_file = gr.Dropdown(label=\"选择文件\", choices=[], interactive=True,multiselect=True)\n\n                selected_file_path = gr.Textbox(label=\"选中的文件路径\",visible=True)  # 用于记录完整路径\n                with gr.Row():\n                    open_text_folder_button = gr.Button(\"📁 打开文件夹\", variant=\"secondary\")\n                    open_text_file_button = gr.Button(\"📄 打开文件\", variant=\"secondary\")\n                with gr.Row():\n                    set_env_button = gr.Button(\"🛠️ 设置为环境变量\", variant=\"primary\")\n                    delete_text_file_button = gr.Button(\"🗑️ 删除文件\", variant=\"stop\")\n                delete_confirmation_row = gr.Row(elem_id=\"Delete_confirmation_row\",visible=False)\n                with delete_confirmation_row:\n                    confirm_delete_button = gr.Button(\"确认删除\", variant=\"stop\",elem_id=\"Confirm_delete_button\")\n                    cancel_delete_button = gr.Button(\"取消\", variant=\"secondary\")\n                with gr.Row():\n                    selected_file_build_graph_button = gr.Button(\"构建图谱\")\n                    selected_file_insert_graph_button = gr.Button(\"插入至现有图谱\")\n                graph_build_confirmation_row = gr.Row(elem_id=\"Graph_build_confirmation_row\",visible=False)\n                with graph_build_confirmation_row:\n                    selected_confirm_build_button = gr.Button(\"确认构建\",elem_id=\"Selected_Confirm_build_button\")\n                    selected_cancel_build_button = gr.Button(\"取消\",elem_id=\"Selected_Cancel_build_button\")\n                graph_insert_confirmation_row = gr.Row(elem_id=\"Graph_insert_confirmation_row\", visible=False)\n                with graph_insert_confirmation_row:\n                    selected_confirm_insert_button = gr.Button(\"确认构建\", elem_id=\"Selected_Confirm_insert_button\")\n                    selected_cancel_insert_button = gr.Button(\"取消\", elem_id=\"Selected_Cancel_insert_button\")\n\n                dict_selected_files = gr.Textbox(visible=False)\n                operate_result = gr.Textbox(label=\"操作结果\", interactive=False, lines=2)\n\n        # 上传文件和操作按钮\n        with gr.Row():\n            with gr.Column():\n                gr.Markdown(\"### 上传文件\")\n                file_uploader = gr.File(label=\"上传文件\", file_types=['text','.pdf','.doc','.ppt','.csv'],file_count=\"multiple\")\n                upload_button = gr.Button(\"上传\")\n                upload_result = gr.Textbox(label=\"上传结果\", interactive=False, lines=5)\n\n                with gr.Row():\n                    with gr.Column():\n                        # 为文件构建图谱、\n\n                        upload_file_build_graph_button = gr.Button(\"将上传的文件构建图谱\")\n\n                        up_prebuild_dict_result = gr.Textbox(visible=False)\n                        upload_file_build_confirmation_row = gr.Row(elem_id=\"Upload_file_build_confirmation_row\",visible=False)\n                        with upload_file_build_confirmation_row:\n                            upload_file_confirm_build_button = gr.Button(\"确认构建\", variant=\"stop\")\n                            upload_file_cancel_build_button = gr.Button(\"取消\", variant=\"secondary\")\n                        build_result = gr.Textbox(label=\"构建结果\", interactive=False, lines=5)\n\n                    # 插入至现有图谱\n                    with gr.Column():\n                        upload_file_insert_graph_button = gr.Button(\"将上传的文件插入至现有图谱\")\n                        up_preinsert_dict_result = gr.Textbox(visible=False)\n                        upload_file_insert_confirmation_row = gr.Row(elem_id=\"Upload_file_insert_confirmation_row\",visible=False)\n                        with upload_file_insert_confirmation_row:\n                            upload_file_confirm_insert_button = gr.Button(\"确认插入\", variant=\"stop\")\n                            upload_file_cancel_insert_button = gr.Button(\"取消\", variant=\"secondary\")\n\n                        insert_result = gr.Textbox(label=\"插入结果\", interactive=False, lines=5)\n\n\n        # Tips 区域\n        gr.Markdown(\"### ℹ️ Tips\")\n        gr.Markdown(\"\"\"\n        - **刷新文件列表**: 更新左侧文件列表。\n        - **选择文件**: 在列表中选择文件进行操作。\n        - **打开文件夹**: 在资源管理器中打开文件所在的文件夹。\n        - **打开文件**: 使用系统默认程序打开文件。\n        - **设置为环境变量**: 将文件路径设置为环境变量./files。\n        - **删除文件**: 删除文件并备份。删除前会提示确认。\n        - **上传文件**: 上传支持的文件类型至系统。           \n        - **为该文件构建图谱**: 设置环境变量并构建知识图谱。\n        - **插入至现有图谱**: 将文件内容插入当前选择的知识图谱。\n        \"\"\")\n\n\n        # 交互逻辑\n        file_mapping = gr.State()  # 用于存储文件名和路径的映射字典\n        selected_file_name = gr.Textbox(visible=False)  # 隐藏的 Textbox，用于记录选择的文件名\n        selected_file_path_invisible = gr.Textbox(visible=False)\n        isselected_file = []\n        #debug_output = gr.Textbox(label=\"调试信息\", lines=2, interactive=False)\n\n        def get_file_list(file_mapping):\n            file_list = refresh_dropdown_choices(file_mapping)\n            return gr.update(choices=file_list,value=file_list[0] if file_list else None)\n\n        file_ui.load(\n            fn=refresh_file_list_display,\n            inputs=[],\n            outputs=[file_list_output, file_mapping],  # 更新文件列表并显示调试信息\n        )\n\n        file_list_output.change(\n            fn=get_file_list,\n            inputs=[file_mapping],\n            outputs=[selected_file]\n        )\n\n        # 刷新文件列表时更新文件名列表和路径映射\n        refresh_files_button.click(\n            fn=refresh_file_list_display,\n            inputs=[],\n            outputs=[file_list_output, file_mapping],  # 更新文件列表并显示调试信息\n        )\n\n        # 更新 Dropdown 的选项\n        refresh_files_button.click(\n            fn=get_file_list,\n            inputs=[file_mapping],\n            outputs=[selected_file] # 更新 Dropdown 的选项并显示调试信息\n        )\n\n        selected_file.change(\n            fn=debug_and_return,\n            inputs=[selected_file],\n            outputs=[selected_file_name],\n        )\n        def get_selected_file_path(names, mapping):\n            names = eval(names)\n            path = []\n            dict_selected_files = {}\n            for name in names:\n                path.append(mapping.get(name))\n                dict_selected_files[name] = mapping.get(name)\n            path_textbox = \"\\n\".join(path)\n            #print(dict_selected_files)\n            return path_textbox,path,dict_selected_files\n\n        # 根据文件名查找文件路径\n        selected_file_name.change(\n            fn=get_selected_file_path,\n            inputs=[selected_file_name, file_mapping],\n            outputs=[selected_file_path,selected_file_path_invisible,dict_selected_files],  # 更新文件路径\n        )\n        # 按钮功能绑定\n        open_text_folder_button.click(\n            fn=open_text_folder,\n            inputs=[selected_file_path_invisible],\n            outputs=[operate_result],\n        )\n        open_text_file_button.click(\n            fn=open_text_file,\n            inputs=[selected_file_path_invisible],\n            outputs=[operate_result],\n        )\n        set_env_button.click(\n            fn=set_rag_env_variable,\n            inputs=[selected_file_path_invisible],\n            outputs=[operate_result],\n        )\n        # 按下“删除文件”按钮，显示确认删除和取消按钮\n        delete_text_file_button.click(\n            fn=lambda: gr.update(visible=True),\n            inputs=[],\n            outputs=[delete_confirmation_row],\n        )\n\n        # 确认删除\n        confirm_delete_button.click(\n            fn=delete_file_with_backup,\n            inputs=[selected_file_path_invisible],\n            outputs=[operate_result,delete_confirmation_row],\n        )\n\n        # 取消删除\n        cancel_delete_button.click(\n            fn=lambda: gr.update(visible=False),\n            inputs=[],\n            outputs=[delete_confirmation_row],\n        )\n\n        def selected_files_to_build():\n            return f\"你确定要为这些文件构造图谱吗？对应文件夹将会是./graph/（第一个文件的名字）\",gr.update(elem_id=\"Graph_build_confirmation_row\",visible=True)\n\n        selected_file_build_graph_button.click(\n            fn=selected_files_to_build,\n            inputs=[],\n            outputs=[operate_result,graph_build_confirmation_row],\n        )\n        selected_confirm_build_button.click(\n            fn=build_graph_for_files,\n            inputs=[dict_selected_files],\n            outputs=[operate_result],\n        )\n        selected_cancel_build_button.click(\n            fn=lambda : (f\"已取消\",gr.update(elem_id=\"Graph_build_confirmation_row\",visible=False)),\n            inputs=[],\n            outputs=[operate_result,graph_build_confirmation_row]\n        )\n        # 按下“插入到现有图谱”按钮，显示确认插入和取消按钮，并提示路径信息\n        selected_file_insert_graph_button.click(\n            fn=lambda path: (\n                f\"当前选择知识图谱为 {os.getenv('RAG_DIR', '未设置')}, 你确定要插入文件 {os.path.basename(path)}?\",\n                gr.update(visible=True)),\n            inputs=[selected_file_path_invisible],\n            outputs=[operate_result, graph_insert_confirmation_row],  # 同时更新提示信息和按钮的可见性\n        )\n        # 确认插入\n        selected_confirm_insert_button.click(\n            fn=insert_graph_for_files,\n            inputs=[dict_selected_files],\n            outputs=[operate_result],\n        )\n        # 取消插入\n        selected_cancel_insert_button.click(\n            fn=lambda: (gr.update(value=\"取消插入操作。\"), gr.update(visible=False)),\n            inputs=[],\n            outputs=[operate_result, graph_insert_confirmation_row],  # 隐藏按钮并更新提示信息\n        )\n\n        upload_button.click(\n            fn=upload_files_and_save,\n            inputs=[file_uploader],\n            outputs=[upload_result,up_prebuild_dict_result,up_preinsert_dict_result],\n        )\n\n        upload_file_build_graph_button.click(\n            fn=selected_files_to_build,\n            inputs=[],\n            outputs=[build_result,upload_file_build_confirmation_row]\n        )\n        upload_file_confirm_build_button.click(\n            fn=build_graph_for_files,\n            inputs=[up_prebuild_dict_result],\n            outputs=[build_result]\n        )\n        upload_file_cancel_build_button.click(\n            fn=lambda: (f\"已取消\", gr.update(elem_id=\"Upload_file_build_confirmation_row\", visible=False)),\n            inputs=[],\n            outputs=[build_result, upload_file_build_confirmation_row]\n        )\n\n        upload_file_insert_graph_button.click(\n            fn=lambda path: (\n                f\"当前选择知识图谱为 {os.getenv('RAG_DIR', '未设置')}, 你确定要插入这些文件?\",\n                gr.update(visible=True)),\n            inputs=[],\n            outputs=[insert_result,upload_file_insert_confirmation_row]\n        )\n        upload_file_confirm_insert_button.click(\n            fn=build_graph_for_files,\n            inputs=[up_preinsert_dict_result],\n            outputs=[insert_result]\n        )\n        upload_file_cancel_insert_button.click(\n            fn=lambda: (f\"已构造\", gr.update(elem_id=\"Upload_file_insert_confirmation_row\", visible=False)),\n            inputs=[],\n            outputs=[insert_result, upload_file_insert_confirmation_row]\n        )\n\n\n    return file_ui\n\ndef graph_ui():\n    \"\"\"创建图谱管理页面\"\"\"\n    with gr.Blocks(visible=False, elem_id=\"graph-page\") as graph_page:  # 使用 Blocks 替代 Column\n        gr.Markdown(\"# 📚 图谱管理页面\")\n\n        # 上部布局\n        with gr.Row():\n            # 左上角：文件夹列表\n            with gr.Column():\n                folder_list = gr.Textbox(\n                    label=\"文件夹列表\",\n                    lines=22,\n                    interactive=False,\n                    placeholder=\"加载中...\",\n                    elem_id=\"folder_list\"\n                )\n                update_folder_list_button = gr.Button(\n                    \"🔄刷新\",min_width = 100\n                )\n\n            # 右上角：文件夹操作按钮\n            with gr.Column():\n                rag_folder_selector = gr.Dropdown(choices=[], label=\"选择文件夹\")\n                selected_graph_abs_path = gr.Textbox(label=\"选中的文件路径\", elem_id=\"Rag_folder_selector\",visible=True)  # 用于记录完整路径\n                selected_graph_rel_path = gr.Textbox(label=\"选中的文件路径\", elem_id=\"Rag_folder_selector\",visible=False)\n                with gr.Row():\n                    open_button = gr.Button(\"📂 打开文件夹\",min_width = 100)\n                    open_html_button = gr.Button(\"📄 打开 HTML\",min_width = 100)\n                with gr.Row():\n                    set_env_button = gr.Button(\"🛠️ 设为环境变量\", variant=\"primary\",min_width = 100)\n                    delete_button = gr.Button(\"️🗑️ 删除文件夹\", variant=\"stop\",min_width = 100)\n                delete_status = gr.Textbox(label=\"删除状态\", interactive=False)\n                env_status = gr.Textbox(label=\"环境变量状态\", interactive=False)\n                HTML_path = gr.Textbox(label=\"HTML 文件路径\", interactive=False)\n\n        # 底部：上传文件\n        with gr.Row():\n            with gr.Column():\n                upload_zip = gr.File(label=\"上传 ZIP 文件\", file_types=[\".zip\"],file_count=\"multiple\")\n                upload_button = gr.Button(\"上传\", min_width=100)\n            upload_status = gr.Textbox(label=\"上传状态\", interactive=False,lines=9)\n\n        # Tips 栏\n        with gr.Row():\n            gr.Markdown(\"\"\"\n            ### ℹ️ Tips:\n            - **打开文件夹**: 在文件资源管理器中打开所选文件夹。\n            - **打开 HTML**: 搜索所选文件夹中名为 `knowledge_graph.html` 的文件并打开。打开后会展现相应的知识图谱。\n            - **设为环境变量**: 将所选文件夹路径设置为环境变量。\n            - **删除文件夹**: 删除所选文件夹及其所有内容，操作不可恢复，请谨慎。\n            - **上传 ZIP 文件**: 将 ZIP 文件解压至 `./graph` 的子文件夹，文件夹名与 ZIP 文件名一致。ZIP中的内容就是你的或者他人分享的图谱。\n            \"\"\",\n            elem_id=\"tips-bar\",\n            )\n\n        folder_path_map = gr.State()\n\n        # 绑定事件\n\n        def page_load():\n            folder_list, folder_path_dic, selective_list = list_subdirectories()\n            return folder_list,folder_path_dic,gr.update(elem_id=\"Rag_folder_selector\",choices=selective_list,value=selective_list[0] if selective_list else None)\n\n        graph_page.load(\n            fn=page_load,\n            inputs=None,\n            outputs=[folder_list,folder_path_map,rag_folder_selector]\n        )\n\n        def update_folder_list():\n            folder_path_dic = {}\n            folder_list,folder_path_dic,selective_list = list_subdirectories()\n            #print(selective_list)\n            return gr.update(elem_id=\"folder_list\",value=folder_list),gr.update(elem_id=\"Rag_folder_selector\",choices=selective_list,value=selective_list[0] if selective_list else None),folder_path_dic\n\n        update_folder_list_button.click(\n            fn=update_folder_list,\n            inputs=[],\n            outputs=[folder_list,rag_folder_selector,folder_path_map]\n        )\n\n        def mapping_path(folder_name, folder_dict):\n            \"\"\"\n                根据文件夹名称返回对应的绝对路径和相对路径。\n                :param folder_name: 要查找的文件夹名称\n                :param folder_dict: 包含文件夹名称与绝对路径的字典\n            \"\"\"\n            base_path = \"./graph\"\n            if folder_name not in folder_dict:\n                return {\"error\": \"Folder name not found in the dictionary.\"}\n            absolute_path = folder_dict[folder_name]\n            relative_path = f\"./graph/\" + os.path.relpath(absolute_path, start=base_path)\n            return absolute_path,relative_path\n\n        rag_folder_selector.change(\n            fn=mapping_path,\n            inputs=[rag_folder_selector,folder_path_map],\n            outputs=[selected_graph_abs_path,selected_graph_rel_path]\n        )\n\n        open_button.click(\n            fn=open_rag_folder,\n            inputs=selected_graph_abs_path,\n            outputs=None,\n        )\n\n        open_html_button.click(\n            fn=find_html_file,\n            inputs=selected_graph_abs_path,\n            outputs=HTML_path,\n        )\n\n        set_env_button.click(\n            fn=set_env_variable_from_folder,\n            inputs=selected_graph_rel_path,\n            outputs=env_status,\n        )\n\n        delete_button.click(\n            fn=backup_and_delete_graph_folder,\n            inputs=[selected_graph_abs_path],\n            outputs=[delete_status],\n        )\n        # 按钮点击逻辑：先暂存上传文件，再触发解压\n        uploaded_files = gr.State([])  # 用于暂存上传的文件\n\n        upload_zip.upload(\n            fn=lambda files: files,  # 暂存上传的文件\n            inputs=upload_zip,\n            outputs=[uploaded_files]\n        )\n\n        upload_button.click(\n            fn=process_uploaded_zips_with_progress,  # 统一处理上传的 ZIP 文件\n            inputs=[uploaded_files],\n            outputs=[upload_status]\n        )\n\n\n        upload_status.change(\n            fn=update_folder_list,\n            inputs=[],\n            outputs=[folder_list,rag_folder_selector,folder_path_map]\n        )\n\n\n\n    return graph_page\n\ndef pdf_management_ui():\n    \"\"\"创建 PDF 管理页面\"\"\"\n    with gr.Blocks() as ui:  # 主框架\n        # 定义状态变量\n        pdf_page_visible = gr.State(value=not should_show_notification() and STATE.get(\"dependencies_installed\", False))\n        notification_page_visible = gr.State(value=not pdf_page_visible.value)\n\n        # PDF 管理页面\n        with gr.Accordion(visible=pdf_page_visible.value, elem_id=\"pdf-management-page\") as pui:\n            gr.Markdown(\"# 🌐 HTML to PDF 转换工具\")\n\n            # 顶部布局\n            with gr.Row():\n                # 左侧：URL输入和列表显示\n                with gr.Column():\n                    gr.Markdown(\"### 🌍 网页地址\")\n                    url_input = gr.Textbox(\n                        label=\"输入网页地址（每行一个）\",\n                        lines=5,\n                        placeholder=\"请输入一个或多个网址，每行一个\",\n                        elem_id=\"url-input\"\n                    )\n                    add_button = gr.Button(\n                        \"+ 添加到列表\",\n                        variant=\"primary\",\n                        elem_id=\"add-button\",\n                    )\n                    urls_display = gr.Textbox(\n                        label=\"已添加的网页\",\n                        lines=10,\n                        interactive=False,\n                        placeholder=\"当前未添加任何网页\",\n                        elem_id=\"url-display\"\n                    )\n                    url_list = gr.State([])\n\n                # 右侧：功能按钮区\n                with gr.Column():\n                    gr.Markdown(\"### 📄 PDF 操作\")\n                    generate_single_pdf = gr.Button(\n                        \"📘 生成单个 PDF\",\n                        variant=\"primary\",\n                    )\n                    generate_multiple_pdfs = gr.Button(\n                        \"📚 生成多个 PDF\",\n                        variant=\"primary\",\n                    )\n                    selected_pdf = gr.Textbox(\n                        label=\"选择的 PDF 文件路径\",\n                        placeholder=\"请输入或选择 PDF 文件路径\",\n                        elem_id=\"pdf-path\"\n                    )\n                    open_pdf_button = gr.Button(\n                        \"📂 打开 PDF\",\n                        variant=\"secondary\",\n                    )\n                    delete_pdf_button = gr.Button(\n                        \"🗑️ 删除 PDF\",\n                        variant=\"stop\",\n                    )\n\n            # 底部：操作结果显示\n            with gr.Row():\n                operation_output = gr.Textbox(\n                    label=\"操作结果\",\n                    lines=5,\n                    interactive=False,\n                    elem_id=\"operation-output\"\n                )\n\n            # 提示区域\n            gr.Markdown(\"\"\"\n            ### ℹ️ Tips:\n            - **添加到列表**: 将输入的网页地址加入待转换列表，支持多个 URL，一个URL一行。\n            - **生成单个 PDF**: 将第一个网址转换为 PDF。\n            - **生成多个 PDF**: 批量将所有网址转换为多个 PDF 文件。\n            - **打开 PDF**: 使用系统默认应用打开选择的 PDF 文件。\n            - **删除 PDF**: 删除选择的 PDF 文件，请谨慎操作。\n            \"\"\", elem_id=\"tips-bar\")\n\n            def add_unique_url(input_urls, urls):\n                \"\"\"\n                添加用户输入的多个网址到已有网址列表中，并去除重复项\n                \"\"\"\n                # 拆分用户输入的网址列表，按换行符和逗号分割\n                new_urls = [url.strip() for url in input_urls.splitlines() if url.strip()]\n\n                # 合并新网址与已有网址\n                combined_urls = urls + new_urls\n\n                # 去重并保持顺序\n                deduplicated_urls = list(dict.fromkeys(combined_urls))  # 使用 dict 保持顺序的去重方式\n\n                # 返回更新后的列表和显示内容\n                return deduplicated_urls, \"\\n\".join(deduplicated_urls)\n\n            # 按钮交互逻辑\n            add_button.click(\n                fn=add_unique_url,\n                inputs=[url_input, url_list],\n                outputs=[url_list, urls_display]\n            )\n\n            generate_single_pdf.click(\n                fn=lambda urls: html_to_pdf([urls[0]]) if urls else \"请先添加至少一个 URL\",\n                inputs=[url_list],\n                outputs=[operation_output]\n            )\n\n            generate_multiple_pdfs.click(\n                fn=lambda urls: html_to_pdf(urls) if urls else \"请先添加至少一个 URL\",\n                inputs=[url_list],\n                outputs=[operation_output]\n            )\n\n            open_pdf_button.click(\n                fn=open_pdf,\n                inputs=[selected_pdf],\n                outputs=[operation_output]\n            )\n\n            delete_pdf_button.click(\n                fn=lambda pdf: delete_pdf_with_backup([pdf]) if pdf else \"请先选择一个 PDF 路径\",\n                inputs=[selected_pdf],\n                outputs=[operation_output]\n            )\n\n        # 通知页面\n        with gr.Accordion(visible=notification_page_visible.value, elem_id=\"notification-page\") as notification_ui:\n            gr.Markdown(\"### ⚠️ 通知：此页面功能尚未完成，目前处于不可用状态\", visible=notification_page_visible.value)\n            gr.Markdown(\"请安装相关依赖，或直接跳过此通知开始使用工具。\", visible=notification_page_visible.value)\n            install_btn = gr.Button(\n                \"安装依赖\",\n                variant=\"primary\",\n                visible=notification_page_visible.value,\n            )\n            close_btn = gr.Button(\n                \"跳过并开始使用\",\n                variant=\"secondary\",\n                visible=notification_page_visible.value,\n            )\n            remember_checkbox = gr.Checkbox(\n                label=\"7 天内不再显示\",\n                elem_id=\"remember-checkbox\",\n                visible=notification_page_visible.value\n            )\n\n            # 安装依赖逻辑\n            install_btn.click(\n                fn=handle_install_dependencies,\n                inputs=[],\n                outputs=[notification_ui]\n            )\n\n            # 跳过逻辑：切换页面可见性\n            def skip_notification(remember, pdf_visible):\n                # 根据用户操作调整页面状态\n                pdf_visible = True\n                return pdf_visible, not pdf_visible, gr.update(visible=pdf_visible), gr.update(visible=False)\n\n            close_btn.click(\n                fn=skip_notification,\n                inputs=[remember_checkbox, pdf_page_visible],\n                outputs=[pdf_page_visible, notification_page_visible, pui, notification_ui]\n            )\n\n    return ui\n\ndef intro_animation():\n    \"\"\"\n    纯 CSS 实现渐变文字和背景引导动画，并在动画结束后恢复滚动条。\n    \"\"\"\n    html_content = \"\"\"\n    <style>\n    body {\n        margin: 0;\n        overflow: hidden; /* 防止滚动条在动画期间显示 */\n        animation: restoreOverflow 2s ease-in-out 4s forwards; /* 在动画结束后恢复滚动条 */\n    }\n\n    #intro-page {\n        position: fixed;\n        top: 0;\n        left: 0;\n        width: 100%;\n        height: 100%;\n        background-color: #ffffff; /* 背景为纯白 */\n        z-index: 9999; /* 确保引导动画层在最前面 */\n        display: flex;\n        align-items: center;\n        justify-content: center;\n        animation: fadeOut 2s ease-in-out 4s forwards; /* 延迟4秒后开始渐隐 */\n    }\n\n    #intro-text {\n        font-size: 2rem;\n        color: #333333;\n        text-align: center; /* 居中对齐文字 */\n        opacity: 0;\n        animation: fadeInText 2s ease-in-out forwards; /* 文本渐显 */\n    }\n    \n    #intro-text div {\n        margin-top: 10px; /* 设置每行之间的间距 */\n    }\n    \n    @keyframes fadeInText {\n        0% { opacity: 0; }\n        100% { opacity: 1; }\n    }\n\n    @keyframes fadeOut {\n        0% { opacity: 1; }\n        100% {\n            opacity: 0;\n            z-index: -1; /* 最后阶段隐藏动画层 */\n            display: none; /* 确保不再占用空间 */\n        }\n    }\n\n    @keyframes restoreOverflow {\n        0% { overflow: hidden; }\n        100% { overflow: auto; } /* 恢复滚动条 */\n    }\n    </style>\n    <div id=\"intro-page\">\n        <div id=\"intro-text\">\n            <div>欢迎使用</div>\n            <div>LightRAG for OpenAI Standard Frontend</div>\n        </div>\n    </div>\n    \"\"\"\n    return html_content\n\ndef settings_ui():\n    with gr.Blocks() as settings:\n        gr.Markdown(\"# 关于前端\")\n        with gr.Row():\n            with gr.Column():\n                gr.Markdown(\"## 前端设置面板\")\n                start_page = gr.Checkbox(label=\"跳过启动页面\",elem_id=\"Start_page\")\n                frontend_port = gr.Textbox(label=\"前端端口\",elem_id=\"Frontend_port\")\n            with gr.Column():\n                gr.Markdown(\"## 关于我们\")\n                who_we_are_textbox = gr.Textbox(\n                    label=\"我们来自？\",\n                    value=\"我们算不上什么正儿八经的团队，更像是一群闲的没事干的爱好者。其中不少是大学生，因此本项目的更新会比较慢，请谅解。\",\n                    lines=3,\n                    interactive=False\n                )\n                community_textbox = gr.Textbox(\n                    label=\"想要交流？\",\n                    value=\n                          \"QQ群：xxx\\n\"\n                          \"Discord：xxx\",\n                    lines=2,\n                    interactive=False\n                )\n\n        def start_page_show(bool_start_page):\n            skip = True if bool_start_page else False\n            update_env_variable(\"start_page_IsNotShow\", str(skip))\n            return gr.update(elem_id=\"Start_page\",value = skip)\n\n        def check_settings():\n            IsNotShow = os.getenv(\"start_page_IsNotShow\",\"\") == 'True'\n            the_frontend_port = os.getenv(\"FRONTEND_PORT\",\"\")\n            return gr.update(elem_id=\"Start_page\",value = IsNotShow),gr.update(elem_id=\"Frontend_port\",value = the_frontend_port)\n\n        settings.load(\n            fn=check_settings,\n            outputs=[start_page,frontend_port]\n        )\n\n        start_page.change(\n            fn=start_page_show,\n            inputs=[start_page]\n        )\n        def Frontend_port(port):\n            update_env_variable(\"FRONTEND_PORT\",port)\n            return None\n\n        frontend_port.change(\n            fn=Frontend_port,\n            inputs=[frontend_port]\n        )\n        return  settings\n\n'''\n\ndef switch_page(page):\n    \"\"\"根据页面状态返回更新\"\"\"\n    if page == \"env_management\":\n        return (\n            gr.update(visible=True),  # 环境变量页面可见\n            gr.update(visible=False),  # 文件管理页面隐藏\n            gr.update(visible=False),  # 图谱管理页面隐藏\n        )\n    elif page == \"file_management\":\n        return (\n            gr.update(visible=False),  # 环境变量页面隐藏\n            gr.update(visible=True),  # 文件管理页面可见\n            gr.update(visible=False),  # 图谱管理页面隐藏\n        )\n    elif page == \"graph_management\":\n        return (\n            gr.update(visible=False),  # 环境变量页面隐藏\n            gr.update(visible=False),  # 文件管理页面隐藏\n            gr.update(visible=True),  # 图谱管理页面可见\n        )\n    # 默认隐藏所有页面\n    return (\n        gr.update(visible=False),\n        gr.update(visible=False),\n        gr.update(visible=False),\n    )\n# 更新导航栏和主界面\ndef create_navbar():\n    \"\"\"创建导航栏\"\"\"\n    with gr.Row():\n        env_button = gr.Button(\"环境变量管理\", variant=\"secondary\", elem_id=\"env-btn\")\n        file_button = gr.Button(\"文本文件管理\", variant=\"secondary\", elem_id=\"file-btn\")\n        graph_button = gr.Button(\"知识图谱管理\", variant=\"secondary\", elem_id=\"graph-btn\")\n    return env_button, file_button, graph_button\n\n\ndef build_ui():\n    \"\"\"主界面构建\"\"\"\n    with gr.Blocks() as ui:\n        current_page = gr.State(\"file_management\")  # 初始页面状态\n\n        # 导航栏\n        env_button, file_button, graph_button = create_navbar()\n\n        # 页面容器\n        with gr.Row():\n            with gr.Column(visible=False, elem_id=\"env-page\") as env_page:\n                env_variables_ui()\n            with gr.Column(visible=True, elem_id=\"file-page\") as file_page:\n                create_file_upload_ui()\n            with gr.Column(visible=False, elem_id=\"graph-page\") as graph_page:\n                create_graph_ui()\n\n        # 点击按钮更新页面状态\n        env_button.click(\n            fn=lambda: \"env_management\",\n            inputs=None,\n            outputs=current_page,\n        )\n        file_button.click(\n            fn=lambda: \"file_management\",\n            inputs=None,\n            outputs=current_page,\n        )\n        graph_button.click(\n            fn=lambda: \"graph_management\",\n            inputs=None,\n            outputs=current_page,\n        )\n\n        # 页面状态变更时更新可见性\n        current_page.change(\n            fn=switch_page,\n            inputs=current_page,\n            outputs=[env_page, file_page, graph_page],\n        )\n\n    return ui\n'''\n\ndef build_ui_with_tabs():\n    # 自定义CSS\n    custom_css = \"\"\"\n            .SideBar {\n                width: auto !important;\n                height: 100% !important;\n                max-width: 25% !important;\n                background-color: #f5f5f5;\n                padding: 10px;\n                box-shadow: 2px 0 5px rgba(0, 0, 0, 0.1);\n            }\n\n            .Closed-SideBar {\n                width: 50% !important;\n                height: 100% !important;\n                max-width: 5% !important;\n                background-color: #f5f5f5;\n                box-shadow: 2px 0 5px rgba(0, 0, 0, 0.1);\n                text-align: right; /* 将内容靠右对齐 */\n            }\n            \n            #Closed-SideBar-button {\n                width: 30% !important;\n                height: 50% !important;\n                max-width: 5% !important;\n                background: linear-gradient(90deg, #4caf50, #8bc34a);\n                color: white;\n                border: none;\n                border-radius: 5px;\n                padding: 10px;\n                font-size: 1rem;\n                cursor: pointer;\n                transition: background 0.3s ease-in-out;\n                text-align: right;\n            }\n            \n            .gradient-button {\n                background: linear-gradient(90deg, #4caf50, #8bc34a);\n                color: white;\n                border: none;\n                border-radius: 5px;\n                padding: 10px;\n                font-size: 1rem;\n                cursor: pointer;\n                transition: background 0.3s ease-in-out;\n            }\n\n            .gradient-button:hover {\n                background: linear-gradient(90deg, #8bc34a, #4caf50);\n            }\n            #admin-page-title {\n                text-align: center; /* 居中对齐文本 */\n                font-size: 24px; /* 调整字体大小 */\n                font-weight: bold; /* 可选：使文本加粗 */\n            }\n            #ASideBar {\n                width: auto !important;\n                height: 20% !important;\n                max-width: 40% !important;\n                text-align: center; /* 居中对齐 */\n                font-size: 40px; /* 字体大小 */\n                font-weight: bold; /* 加粗 */\n                background-color: #f5f5f5; /* 背景色与侧边栏一致 */\n                padding: 15px; /* 内边距 */\n                border-radius: 5px; /* 圆角 */\n                box-shadow: 2px 2px 10px rgba(0, 0, 0, 0.1); /* 阴影效果 */\n                margin-bottom: 20px; /* 下边距 */\n                color: #333; /* 字体颜色 */\n            }\n    \"\"\"\n    \"\"\"构建带有 Tabs 的主界面\"\"\"\n\n    with gr.Blocks(css=custom_css) as ui:\n        def get_intro_animation():\n            '''\n            回调函数判断是否显示启动页面\n            '''\n            load_dotenv()\n            Start_page_IsNotShow = os.getenv('start_page_IsNotShow', 'False').lower() == 'true'\n            if not Start_page_IsNotShow:\n                return intro_animation()\n            return \"\"\n\n        gr.HTML(get_intro_animation)\n\n        with gr.Column():\n            gr.Markdown(\"# 管理界面\",elem_id=\"admin-page-title\")\n            with gr.Row():\n                sidebar_ui()\n\n            # 使用 Tabs 创建导航栏\n                with gr.TabItem(\"欢迎使用\"):\n                    welcome_page()  # 欢迎页面\n\n                with gr.TabItem(\"文件管理\"):\n                    file_management_ui()  # 文件管理页面\n\n                with gr.TabItem(\"图谱管理\"):\n                    graph_ui()  # 图谱管理页面\n\n                with gr.TabItem(\"HTML to Graph\"):\n                    pdf_management_ui()\n\n                with gr.TabItem(\"关于前端\"):\n                    settings_ui()\n\n\n\n\n    return ui\n\n\n# 启动 Gradio 应用\nif __name__ == \"__main__\":\n    load_dotenv(override=True)\n    F_port = int(os.getenv(\"FRONTEND_PORT\",\"\"))\n    build_ui_with_tabs().launch(server_port=F_port, share=False)\n    sleep(5)\n    webbrowser.open(f\"http://127.0.0.1:{F_port}\")\n    #asyncio.run(fetch_model_info(os.getenv(\"OPENAI_BASE_URL\", \"\"),os.getenv(\"OPENAI_API_KEY\", \"\")))\n"}
{"type": "source_file", "path": "Gradio/pkg.py", "content": "import pkg_resources\n\ndef export_dependencies(output_file):\n    \"\"\"\n    将当前虚拟环境的所有依赖包及其版本号导出到指定的 txt 文件中。\n    :param output_file: 输出文件的路径\n    \"\"\"\n    try:\n        # 获取所有已安装的依赖包\n        installed_packages = pkg_resources.working_set\n\n        # 按包名排序，生成格式化字符串\n        dependencies = sorted([f\"{pkg.key}=={pkg.version}\" for pkg in installed_packages])\n\n        # 将结果写入到指定文件\n        with open(output_file, \"w\", encoding=\"utf-8\") as f:\n            f.write(\"\\n\".join(dependencies))\n        print(f\"依赖包列表已成功导出至 {output_file}\")\n    except Exception as e:\n        print(f\"导出依赖包列表时出错: {e}\")\n\n# 示例用法\nif __name__ == \"__main__\":\n    output_file = \"./dependencies.txt\"  # 输出文件路径\n    export_dependencies(output_file)"}
{"type": "source_file", "path": "examples/lightrag_ollama_demo.py", "content": "import asyncio\nimport os\nimport inspect\nimport logging\nfrom lightrag import LightRAG, QueryParam\nfrom lightrag.llm import ollama_model_complete, ollama_embedding\nfrom lightrag.utils import EmbeddingFunc\n\nWORKING_DIR = \"./dickens\"\n\nlogging.basicConfig(format=\"%(levelname)s:%(message)s\", level=logging.INFO)\n\nif not os.path.exists(WORKING_DIR):\n    os.mkdir(WORKING_DIR)\n\nrag = LightRAG(\n    working_dir=WORKING_DIR,\n    llm_model_func=ollama_model_complete,\n    llm_model_name=\"gemma2:2b\",\n    llm_model_max_async=4,\n    llm_model_max_token_size=32768,\n    llm_model_kwargs={\"host\": \"http://localhost:11434\", \"options\": {\"num_ctx\": 32768}},\n    embedding_func=EmbeddingFunc(\n        embedding_dim=768,\n        max_token_size=8192,\n        func=lambda texts: ollama_embedding(\n            texts, embed_model=\"nomic-embed-text\", host=\"http://localhost:11434\"\n        ),\n    ),\n)\n\nwith open(\"./book.txt\", \"r\", encoding=\"utf-8\") as f:\n    rag.insert(f.read())\n\n# Perform naive search\nprint(\n    rag.query(\"What are the top themes in this story?\", param=QueryParam(mode=\"naive\"))\n)\n\n# Perform local search\nprint(\n    rag.query(\"What are the top themes in this story?\", param=QueryParam(mode=\"local\"))\n)\n\n# Perform global search\nprint(\n    rag.query(\"What are the top themes in this story?\", param=QueryParam(mode=\"global\"))\n)\n\n# Perform hybrid search\nprint(\n    rag.query(\"What are the top themes in this story?\", param=QueryParam(mode=\"hybrid\"))\n)\n\n# stream response\nresp = rag.query(\n    \"What are the top themes in this story?\",\n    param=QueryParam(mode=\"hybrid\", stream=True),\n)\n\n\nasync def print_stream(stream):\n    async for chunk in stream:\n        print(chunk, end=\"\", flush=True)\n\n\nif inspect.isasyncgen(resp):\n    asyncio.run(print_stream(resp))\nelse:\n    print(resp)\n"}
{"type": "source_file", "path": "examples/lightrag_api_oracle_demo.py", "content": "from fastapi import FastAPI, HTTPException, File, UploadFile\nfrom fastapi import Query\nfrom contextlib import asynccontextmanager\nfrom pydantic import BaseModel\nfrom typing import Optional, Any\n\nimport sys\nimport os\n\n\nfrom pathlib import Path\n\nimport asyncio\nimport nest_asyncio\nfrom lightrag import LightRAG, QueryParam\nfrom lightrag.llm import openai_complete_if_cache, openai_embedding\nfrom lightrag.utils import EmbeddingFunc\nimport numpy as np\n\nfrom lightrag.kg.oracle_impl import OracleDB\n\nprint(os.getcwd())\nscript_directory = Path(__file__).resolve().parent.parent\nsys.path.append(os.path.abspath(script_directory))\n\n\n# Apply nest_asyncio to solve event loop issues\nnest_asyncio.apply()\n\nDEFAULT_RAG_DIR = \"index_default\"\n\n\n# We use OpenAI compatible API to call LLM on Oracle Cloud\n# More docs here https://github.com/jin38324/OCI_GenAI_access_gateway\nBASE_URL = \"http://xxx.xxx.xxx.xxx:8088/v1/\"\nAPIKEY = \"ocigenerativeai\"\n\n# Configure working directory\nWORKING_DIR = os.environ.get(\"RAG_DIR\", f\"{DEFAULT_RAG_DIR}\")\nprint(f\"WORKING_DIR: {WORKING_DIR}\")\nLLM_MODEL = os.environ.get(\"LLM_MODEL\", \"cohere.command-r-plus-08-2024\")\nprint(f\"LLM_MODEL: {LLM_MODEL}\")\nEMBEDDING_MODEL = os.environ.get(\"EMBEDDING_MODEL\", \"cohere.embed-multilingual-v3.0\")\nprint(f\"EMBEDDING_MODEL: {EMBEDDING_MODEL}\")\nEMBEDDING_MAX_TOKEN_SIZE = int(os.environ.get(\"EMBEDDING_MAX_TOKEN_SIZE\", 512))\nprint(f\"EMBEDDING_MAX_TOKEN_SIZE: {EMBEDDING_MAX_TOKEN_SIZE}\")\n\nif not os.path.exists(WORKING_DIR):\n    os.mkdir(WORKING_DIR)\n\n\nasync def llm_model_func(\n    prompt, system_prompt=None, history_messages=[], keyword_extraction=False, **kwargs\n) -> str:\n    return await openai_complete_if_cache(\n        LLM_MODEL,\n        prompt,\n        system_prompt=system_prompt,\n        history_messages=history_messages,\n        api_key=APIKEY,\n        base_url=BASE_URL,\n        **kwargs,\n    )\n\n\nasync def embedding_func(texts: list[str]) -> np.ndarray:\n    return await openai_embedding(\n        texts,\n        model=EMBEDDING_MODEL,\n        api_key=APIKEY,\n        base_url=BASE_URL,\n    )\n\n\nasync def get_embedding_dim():\n    test_text = [\"This is a test sentence.\"]\n    embedding = await embedding_func(test_text)\n    embedding_dim = embedding.shape[1]\n    return embedding_dim\n\n\nasync def init():\n    # Detect embedding dimension\n    embedding_dimension = await get_embedding_dim()\n    print(f\"Detected embedding dimension: {embedding_dimension}\")\n    # Create Oracle DB connection\n    # The `config` parameter is the connection configuration of Oracle DB\n    # More docs here https://python-oracledb.readthedocs.io/en/latest/user_guide/connection_handling.html\n    # We storage data in unified tables, so we need to set a `workspace` parameter to specify which docs we want to store and query\n    # Below is an example of how to connect to Oracle Autonomous Database on Oracle Cloud\n\n    oracle_db = OracleDB(\n        config={\n            \"user\": \"\",\n            \"password\": \"\",\n            \"dsn\": \"\",\n            \"config_dir\": \"path_to_config_dir\",\n            \"wallet_location\": \"path_to_wallet_location\",\n            \"wallet_password\": \"wallet_password\",\n            \"workspace\": \"company\",\n        }  # specify which docs you want to store and query\n    )\n\n    # Check if Oracle DB tables exist, if not, tables will be created\n    await oracle_db.check_tables()\n    # Initialize LightRAG\n    # We use Oracle DB as the KV/vector/graph storage\n    # You can add `addon_params={\"example_number\": 1, \"language\": \"Simplfied Chinese\"}` to control the prompt\n    rag = LightRAG(\n        enable_llm_cache=False,\n        working_dir=WORKING_DIR,\n        chunk_token_size=512,\n        llm_model_func=llm_model_func,\n        embedding_func=EmbeddingFunc(\n            embedding_dim=embedding_dimension,\n            max_token_size=512,\n            func=embedding_func,\n        ),\n        graph_storage=\"OracleGraphStorage\",\n        kv_storage=\"OracleKVStorage\",\n        vector_storage=\"OracleVectorDBStorage\",\n    )\n\n    # Setthe KV/vector/graph storage's `db` property, so all operation will use same connection pool\n    rag.graph_storage_cls.db = oracle_db\n    rag.key_string_value_json_storage_cls.db = oracle_db\n    rag.vector_db_storage_cls.db = oracle_db\n\n    return rag\n\n\n# Extract and Insert into LightRAG storage\n# with open(\"./dickens/book.txt\", \"r\", encoding=\"utf-8\") as f:\n#   await rag.ainsert(f.read())\n\n# # Perform search in different modes\n# modes = [\"naive\", \"local\", \"global\", \"hybrid\"]\n# for mode in modes:\n#     print(\"=\"*20, mode, \"=\"*20)\n#     print(await rag.aquery(\"这篇文档是关于什么内容的?\", param=QueryParam(mode=mode)))\n#     print(\"-\"*100, \"\\n\")\n\n# Data models\n\n\nclass QueryRequest(BaseModel):\n    query: str\n    mode: str = \"hybrid\"\n    only_need_context: bool = False\n    only_need_prompt: bool = False\n\n\nclass DataRequest(BaseModel):\n    limit: int = 100\n\n\nclass InsertRequest(BaseModel):\n    text: str\n\n\nclass Response(BaseModel):\n    status: str\n    data: Optional[Any] = None\n    message: Optional[str] = None\n\n\n# API routes\n\nrag = None\n\n\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n    global rag\n    rag = await init()\n    print(\"done!\")\n    yield\n\n\napp = FastAPI(\n    title=\"LightRAG API\", description=\"API for RAG operations\", lifespan=lifespan\n)\n\n\n@app.post(\"/query\", response_model=Response)\nasync def query_endpoint(request: QueryRequest):\n    # try:\n    # loop = asyncio.get_event_loop()\n    if request.mode == \"naive\":\n        top_k = 3\n    else:\n        top_k = 60\n    result = await rag.aquery(\n        request.query,\n        param=QueryParam(\n            mode=request.mode,\n            only_need_context=request.only_need_context,\n            only_need_prompt=request.only_need_prompt,\n            top_k=top_k,\n        ),\n    )\n    return Response(status=\"success\", data=result)\n    # except Exception as e:\n    #     raise HTTPException(status_code=500, detail=str(e))\n\n\n@app.get(\"/data\", response_model=Response)\nasync def query_all_nodes(type: str = Query(\"nodes\"), limit: int = Query(100)):\n    if type == \"nodes\":\n        result = await rag.chunk_entity_relation_graph.get_all_nodes(limit=limit)\n    elif type == \"edges\":\n        result = await rag.chunk_entity_relation_graph.get_all_edges(limit=limit)\n    elif type == \"statistics\":\n        result = await rag.chunk_entity_relation_graph.get_statistics()\n    return Response(status=\"success\", data=result)\n\n\n@app.post(\"/insert\", response_model=Response)\nasync def insert_endpoint(request: InsertRequest):\n    try:\n        loop = asyncio.get_event_loop()\n        await loop.run_in_executor(None, lambda: rag.insert(request.text))\n        return Response(status=\"success\", message=\"Text inserted successfully\")\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n\n\n@app.post(\"/insert_file\", response_model=Response)\nasync def insert_file(file: UploadFile = File(...)):\n    try:\n        file_content = await file.read()\n        # Read file content\n        try:\n            content = file_content.decode(\"utf-8\")\n        except UnicodeDecodeError:\n            # If UTF-8 decoding fails, try other encodings\n            content = file_content.decode(\"gbk\")\n        # Insert file content\n        loop = asyncio.get_event_loop()\n        await loop.run_in_executor(None, lambda: rag.insert(content))\n\n        return Response(\n            status=\"success\",\n            message=f\"File content from {file.filename} inserted successfully\",\n        )\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n\n\n@app.get(\"/health\")\nasync def health_check():\n    return {\"status\": \"healthy\"}\n\n\nif __name__ == \"__main__\":\n    import uvicorn\n\n    uvicorn.run(app, host=\"127.0.0.1\", port=8020)\n\n# Usage example\n# To run the server, use the following command in your terminal:\n# python lightrag_api_openai_compatible_demo.py\n\n# Example requests:\n# 1. Query:\n# curl -X POST \"http://127.0.0.1:8020/query\" -H \"Content-Type: application/json\" -d '{\"query\": \"your query here\", \"mode\": \"hybrid\"}'\n\n# 2. Insert text:\n# curl -X POST \"http://127.0.0.1:8020/insert\" -H \"Content-Type: application/json\" -d '{\"text\": \"your text here\"}'\n\n# 3. Insert file:\n# curl -X POST \"http://127.0.0.1:8020/insert_file\" -H \"Content-Type: application/json\" -d '{\"file_path\": \"path/to/your/file.txt\"}'\n\n# 4. Health check:\n# curl -X GET \"http://127.0.0.1:8020/health\"\n"}
{"type": "source_file", "path": "examples/lightrag_openai_compatible_demo_embedding_cache.py", "content": "import os\nimport asyncio\nfrom lightrag import LightRAG, QueryParam\nfrom lightrag.llm import openai_complete_if_cache, openai_embedding\nfrom lightrag.utils import EmbeddingFunc\nimport numpy as np\n\nWORKING_DIR = \"./dickens\"\n\nif not os.path.exists(WORKING_DIR):\n    os.mkdir(WORKING_DIR)\n\n\nasync def llm_model_func(\n    prompt, system_prompt=None, history_messages=[], keyword_extraction=False, **kwargs\n) -> str:\n    return await openai_complete_if_cache(\n        \"solar-mini\",\n        prompt,\n        system_prompt=system_prompt,\n        history_messages=history_messages,\n        api_key=os.getenv(\"UPSTAGE_API_KEY\"),\n        base_url=\"https://api.upstage.ai/v1/solar\",\n        **kwargs,\n    )\n\n\nasync def embedding_func(texts: list[str]) -> np.ndarray:\n    return await openai_embedding(\n        texts,\n        model=\"solar-embedding-1-large-query\",\n        api_key=os.getenv(\"UPSTAGE_API_KEY\"),\n        base_url=\"https://api.upstage.ai/v1/solar\",\n    )\n\n\nasync def get_embedding_dim():\n    test_text = [\"This is a test sentence.\"]\n    embedding = await embedding_func(test_text)\n    embedding_dim = embedding.shape[1]\n    return embedding_dim\n\n\n# function test\nasync def test_funcs():\n    result = await llm_model_func(\"How are you?\")\n    print(\"llm_model_func: \", result)\n\n    result = await embedding_func([\"How are you?\"])\n    print(\"embedding_func: \", result)\n\n\n# asyncio.run(test_funcs())\n\n\nasync def main():\n    try:\n        embedding_dimension = await get_embedding_dim()\n        print(f\"Detected embedding dimension: {embedding_dimension}\")\n\n        rag = LightRAG(\n            working_dir=WORKING_DIR,\n            embedding_cache_config={\n                \"enabled\": True,\n                \"similarity_threshold\": 0.90,\n            },\n            llm_model_func=llm_model_func,\n            embedding_func=EmbeddingFunc(\n                embedding_dim=embedding_dimension,\n                max_token_size=8192,\n                func=embedding_func,\n            ),\n        )\n\n        with open(\"./book.txt\", \"r\", encoding=\"utf-8\") as f:\n            await rag.ainsert(f.read())\n\n        # Perform naive search\n        print(\n            await rag.aquery(\n                \"What are the top themes in this story?\", param=QueryParam(mode=\"naive\")\n            )\n        )\n\n        # Perform local search\n        print(\n            await rag.aquery(\n                \"What are the top themes in this story?\", param=QueryParam(mode=\"local\")\n            )\n        )\n\n        # Perform global search\n        print(\n            await rag.aquery(\n                \"What are the top themes in this story?\",\n                param=QueryParam(mode=\"global\"),\n            )\n        )\n\n        # Perform hybrid search\n        print(\n            await rag.aquery(\n                \"What are the top themes in this story?\",\n                param=QueryParam(mode=\"hybrid\"),\n            )\n        )\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n"}
{"type": "source_file", "path": "examples/lightrag_ollama_neo4j_milvus_mongo_demo.py", "content": "import os\nfrom lightrag import LightRAG, QueryParam\nfrom lightrag.llm import ollama_model_complete, ollama_embed\nfrom lightrag.utils import EmbeddingFunc\n\n# WorkingDir\nROOT_DIR = os.path.dirname(os.path.abspath(__file__))\nWORKING_DIR = os.path.join(ROOT_DIR, \"myKG\")\nif not os.path.exists(WORKING_DIR):\n    os.mkdir(WORKING_DIR)\nprint(f\"WorkingDir: {WORKING_DIR}\")\n\n# mongo\nos.environ[\"MONGO_URI\"] = \"mongodb://root:root@localhost:27017/\"\nos.environ[\"MONGO_DATABASE\"] = \"LightRAG\"\n\n# neo4j\nBATCH_SIZE_NODES = 500\nBATCH_SIZE_EDGES = 100\nos.environ[\"NEO4J_URI\"] = \"bolt://localhost:7687\"\nos.environ[\"NEO4J_USERNAME\"] = \"neo4j\"\nos.environ[\"NEO4J_PASSWORD\"] = \"neo4j\"\n\n# milvus\nos.environ[\"MILVUS_URI\"] = \"http://localhost:19530\"\nos.environ[\"MILVUS_USER\"] = \"root\"\nos.environ[\"MILVUS_PASSWORD\"] = \"root\"\nos.environ[\"MILVUS_DB_NAME\"] = \"lightrag\"\n\n\nrag = LightRAG(\n    working_dir=WORKING_DIR,\n    llm_model_func=ollama_model_complete,\n    llm_model_name=\"qwen2.5:14b\",\n    llm_model_max_async=4,\n    llm_model_max_token_size=32768,\n    llm_model_kwargs={\"host\": \"http://127.0.0.1:11434\", \"options\": {\"num_ctx\": 32768}},\n    embedding_func=EmbeddingFunc(\n        embedding_dim=1024,\n        max_token_size=8192,\n        func=lambda texts: ollama_embed(\n            texts=texts, embed_model=\"bge-m3:latest\", host=\"http://127.0.0.1:11434\"\n        ),\n    ),\n    kv_storage=\"MongoKVStorage\",\n    graph_storage=\"Neo4JStorage\",\n    vector_storage=\"MilvusVectorDBStorge\",\n)\n\nfile = \"./book.txt\"\nwith open(file, \"r\") as f:\n    rag.insert(f.read())\n\nprint(\n    rag.query(\"What are the top themes in this story?\", param=QueryParam(mode=\"hybrid\"))\n)\n"}
{"type": "source_file", "path": "examples/lightrag_jinaai_demo.py", "content": "import numpy as np\nfrom lightrag import LightRAG, QueryParam\nfrom lightrag.utils import EmbeddingFunc\nfrom lightrag.llm import jina_embedding, openai_complete_if_cache\nimport os\nimport asyncio\n\n\nasync def embedding_func(texts: list[str]) -> np.ndarray:\n    return await jina_embedding(texts, api_key=\"YourJinaAPIKey\")\n\n\nWORKING_DIR = \"./dickens\"\n\nif not os.path.exists(WORKING_DIR):\n    os.mkdir(WORKING_DIR)\n\n\nasync def llm_model_func(\n    prompt, system_prompt=None, history_messages=[], **kwargs\n) -> str:\n    return await openai_complete_if_cache(\n        \"solar-mini\",\n        prompt,\n        system_prompt=system_prompt,\n        history_messages=history_messages,\n        api_key=os.getenv(\"UPSTAGE_API_KEY\"),\n        base_url=\"https://api.upstage.ai/v1/solar\",\n        **kwargs,\n    )\n\n\nrag = LightRAG(\n    working_dir=WORKING_DIR,\n    llm_model_func=llm_model_func,\n    embedding_func=EmbeddingFunc(\n        embedding_dim=1024, max_token_size=8192, func=embedding_func\n    ),\n)\n\n\nasync def lightraginsert(file_path, semaphore):\n    async with semaphore:\n        try:\n            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n                content = f.read()\n        except UnicodeDecodeError:\n            # If UTF-8 decoding fails, try other encodings\n            with open(file_path, \"r\", encoding=\"gbk\") as f:\n                content = f.read()\n        await rag.ainsert(content)\n\n\nasync def process_files(directory, concurrency_limit):\n    semaphore = asyncio.Semaphore(concurrency_limit)\n    tasks = []\n    for root, dirs, files in os.walk(directory):\n        for f in files:\n            file_path = os.path.join(root, f)\n            if f.startswith(\".\"):\n                continue\n            tasks.append(lightraginsert(file_path, semaphore))\n    await asyncio.gather(*tasks)\n\n\nasync def main():\n    try:\n        rag = LightRAG(\n            working_dir=WORKING_DIR,\n            llm_model_func=llm_model_func,\n            embedding_func=EmbeddingFunc(\n                embedding_dim=1024,\n                max_token_size=8192,\n                func=embedding_func,\n            ),\n        )\n\n        asyncio.run(process_files(WORKING_DIR, concurrency_limit=4))\n\n        # Perform naive search\n        print(\n            await rag.aquery(\n                \"What are the top themes in this story?\", param=QueryParam(mode=\"naive\")\n            )\n        )\n\n        # Perform local search\n        print(\n            await rag.aquery(\n                \"What are the top themes in this story?\", param=QueryParam(mode=\"local\")\n            )\n        )\n\n        # Perform global search\n        print(\n            await rag.aquery(\n                \"What are the top themes in this story?\",\n                param=QueryParam(mode=\"global\"),\n            )\n        )\n\n        # Perform hybrid search\n        print(\n            await rag.aquery(\n                \"What are the top themes in this story?\",\n                param=QueryParam(mode=\"hybrid\"),\n            )\n        )\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n"}
{"type": "source_file", "path": "examples/lightrag_openai_compatible_demo.py", "content": "import os\nimport asyncio\nfrom lightrag import LightRAG, QueryParam\nfrom lightrag.llm import openai_complete_if_cache, openai_embedding\nfrom lightrag.utils import EmbeddingFunc\nimport numpy as np\n\nWORKING_DIR = \"./dickens\"\n\nif not os.path.exists(WORKING_DIR):\n    os.mkdir(WORKING_DIR)\n\n\nasync def llm_model_func(\n    prompt, system_prompt=None, history_messages=[], keyword_extraction=False, **kwargs\n) -> str:\n    return await openai_complete_if_cache(\n        \"solar-mini\",\n        prompt,\n        system_prompt=system_prompt,\n        history_messages=history_messages,\n        api_key=os.getenv(\"UPSTAGE_API_KEY\"),\n        base_url=\"https://api.upstage.ai/v1/solar\",\n        **kwargs,\n    )\n\n\nasync def embedding_func(texts: list[str]) -> np.ndarray:\n    return await openai_embedding(\n        texts,\n        model=\"solar-embedding-1-large-query\",\n        api_key=os.getenv(\"UPSTAGE_API_KEY\"),\n        base_url=\"https://api.upstage.ai/v1/solar\",\n    )\n\n\nasync def get_embedding_dim():\n    test_text = [\"This is a test sentence.\"]\n    embedding = await embedding_func(test_text)\n    embedding_dim = embedding.shape[1]\n    return embedding_dim\n\n\n# function test\nasync def test_funcs():\n    result = await llm_model_func(\"How are you?\")\n    print(\"llm_model_func: \", result)\n\n    result = await embedding_func([\"How are you?\"])\n    print(\"embedding_func: \", result)\n\n\n# asyncio.run(test_funcs())\n\n\nasync def main():\n    try:\n        embedding_dimension = await get_embedding_dim()\n        print(f\"Detected embedding dimension: {embedding_dimension}\")\n\n        rag = LightRAG(\n            working_dir=WORKING_DIR,\n            llm_model_func=llm_model_func,\n            embedding_func=EmbeddingFunc(\n                embedding_dim=embedding_dimension,\n                max_token_size=8192,\n                func=embedding_func,\n            ),\n        )\n\n        with open(\"./book.txt\", \"r\", encoding=\"utf-8\") as f:\n            await rag.ainsert(f.read())\n\n        # Perform naive search\n        print(\n            await rag.aquery(\n                \"What are the top themes in this story?\", param=QueryParam(mode=\"naive\")\n            )\n        )\n\n        # Perform local search\n        print(\n            await rag.aquery(\n                \"What are the top themes in this story?\", param=QueryParam(mode=\"local\")\n            )\n        )\n\n        # Perform global search\n        print(\n            await rag.aquery(\n                \"What are the top themes in this story?\",\n                param=QueryParam(mode=\"global\"),\n            )\n        )\n\n        # Perform hybrid search\n        print(\n            await rag.aquery(\n                \"What are the top themes in this story?\",\n                param=QueryParam(mode=\"hybrid\"),\n            )\n        )\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n"}
{"type": "source_file", "path": "examples/lightrag_siliconcloud_demo.py", "content": "import os\nimport asyncio\nfrom lightrag import LightRAG, QueryParam\nfrom lightrag.llm import openai_complete_if_cache, siliconcloud_embedding\nfrom lightrag.utils import EmbeddingFunc\nimport numpy as np\n\nWORKING_DIR = \"./dickens\"\n\nif not os.path.exists(WORKING_DIR):\n    os.mkdir(WORKING_DIR)\n\n\nasync def llm_model_func(\n    prompt, system_prompt=None, history_messages=[], keyword_extraction=False, **kwargs\n) -> str:\n    return await openai_complete_if_cache(\n        \"Qwen/Qwen2.5-7B-Instruct\",\n        prompt,\n        system_prompt=system_prompt,\n        history_messages=history_messages,\n        api_key=os.getenv(\"SILICONFLOW_API_KEY\"),\n        base_url=\"https://api.siliconflow.cn/v1/\",\n        **kwargs,\n    )\n\n\nasync def embedding_func(texts: list[str]) -> np.ndarray:\n    return await siliconcloud_embedding(\n        texts,\n        model=\"netease-youdao/bce-embedding-base_v1\",\n        api_key=os.getenv(\"SILICONFLOW_API_KEY\"),\n        max_token_size=512,\n    )\n\n\n# function test\nasync def test_funcs():\n    result = await llm_model_func(\"How are you?\")\n    print(\"llm_model_func: \", result)\n\n    result = await embedding_func([\"How are you?\"])\n    print(\"embedding_func: \", result)\n\n\nasyncio.run(test_funcs())\n\n\nrag = LightRAG(\n    working_dir=WORKING_DIR,\n    llm_model_func=llm_model_func,\n    embedding_func=EmbeddingFunc(\n        embedding_dim=768, max_token_size=512, func=embedding_func\n    ),\n)\n\n\nwith open(\"./book.txt\") as f:\n    rag.insert(f.read())\n\n# Perform naive search\nprint(\n    rag.query(\"What are the top themes in this story?\", param=QueryParam(mode=\"naive\"))\n)\n\n# Perform local search\nprint(\n    rag.query(\"What are the top themes in this story?\", param=QueryParam(mode=\"local\"))\n)\n\n# Perform global search\nprint(\n    rag.query(\"What are the top themes in this story?\", param=QueryParam(mode=\"global\"))\n)\n\n# Perform hybrid search\nprint(\n    rag.query(\"What are the top themes in this story?\", param=QueryParam(mode=\"hybrid\"))\n)\n"}
{"type": "source_file", "path": "examples/vram_management_demo.py", "content": "import os\nimport time\nfrom lightrag import LightRAG, QueryParam\nfrom lightrag.llm import ollama_model_complete, ollama_embedding\nfrom lightrag.utils import EmbeddingFunc\n\n# Working directory and the directory path for text files\nWORKING_DIR = \"./dickens\"\nTEXT_FILES_DIR = \"/llm/mt\"\n\n# Create the working directory if it doesn't exist\nif not os.path.exists(WORKING_DIR):\n    os.mkdir(WORKING_DIR)\n\n# Initialize LightRAG\nrag = LightRAG(\n    working_dir=WORKING_DIR,\n    llm_model_func=ollama_model_complete,\n    llm_model_name=\"qwen2.5:3b-instruct-max-context\",\n    embedding_func=EmbeddingFunc(\n        embedding_dim=768,\n        max_token_size=8192,\n        func=lambda texts: ollama_embedding(texts, embed_model=\"nomic-embed-text\"),\n    ),\n)\n\n# Read all .txt files from the TEXT_FILES_DIR directory\ntexts = []\nfor filename in os.listdir(TEXT_FILES_DIR):\n    if filename.endswith(\".txt\"):\n        file_path = os.path.join(TEXT_FILES_DIR, filename)\n        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n            texts.append(file.read())\n\n\n# Batch insert texts into LightRAG with a retry mechanism\ndef insert_texts_with_retry(rag, texts, retries=3, delay=5):\n    for _ in range(retries):\n        try:\n            rag.insert(texts)\n            return\n        except Exception as e:\n            print(\n                f\"Error occurred during insertion: {e}. Retrying in {delay} seconds...\"\n            )\n            time.sleep(delay)\n    raise RuntimeError(\"Failed to insert texts after multiple retries.\")\n\n\ninsert_texts_with_retry(rag, texts)\n\n# Perform different types of queries and handle potential errors\ntry:\n    print(\n        rag.query(\n            \"What are the top themes in this story?\", param=QueryParam(mode=\"naive\")\n        )\n    )\nexcept Exception as e:\n    print(f\"Error performing naive search: {e}\")\n\ntry:\n    print(\n        rag.query(\n            \"What are the top themes in this story?\", param=QueryParam(mode=\"local\")\n        )\n    )\nexcept Exception as e:\n    print(f\"Error performing local search: {e}\")\n\ntry:\n    print(\n        rag.query(\n            \"What are the top themes in this story?\", param=QueryParam(mode=\"global\")\n        )\n    )\nexcept Exception as e:\n    print(f\"Error performing global search: {e}\")\n\ntry:\n    print(\n        rag.query(\n            \"What are the top themes in this story?\", param=QueryParam(mode=\"hybrid\")\n        )\n    )\nexcept Exception as e:\n    print(f\"Error performing hybrid search: {e}\")\n\n\n# Function to clear VRAM resources\ndef clear_vram():\n    os.system(\"sudo nvidia-smi --gpu-reset\")\n\n\n# Regularly clear VRAM to prevent overflow\nclear_vram_interval = 3600  # Clear once every hour\nstart_time = time.time()\n\nwhile True:\n    current_time = time.time()\n    if current_time - start_time > clear_vram_interval:\n        clear_vram()\n        start_time = current_time\n    time.sleep(60)  # Check the time every minute\n"}
{"type": "source_file", "path": "examples/lightrag_tidb_demo.py", "content": "import asyncio\nimport os\n\nimport numpy as np\n\nfrom lightrag import LightRAG, QueryParam\nfrom lightrag.kg.tidb_impl import TiDB\nfrom lightrag.llm import siliconcloud_embedding, openai_complete_if_cache\nfrom lightrag.utils import EmbeddingFunc\n\nWORKING_DIR = \"./dickens\"\n\n# We use SiliconCloud API to call LLM on Oracle Cloud\n# More docs here https://docs.siliconflow.cn/introduction\nBASE_URL = \"https://api.siliconflow.cn/v1/\"\nAPIKEY = \"\"\nCHATMODEL = \"\"\nEMBEDMODEL = \"\"\n\nTIDB_HOST = \"\"\nTIDB_PORT = \"\"\nTIDB_USER = \"\"\nTIDB_PASSWORD = \"\"\nTIDB_DATABASE = \"lightrag\"\n\nif not os.path.exists(WORKING_DIR):\n    os.mkdir(WORKING_DIR)\n\n\nasync def llm_model_func(\n    prompt, system_prompt=None, history_messages=[], keyword_extraction=False, **kwargs\n) -> str:\n    return await openai_complete_if_cache(\n        CHATMODEL,\n        prompt,\n        system_prompt=system_prompt,\n        history_messages=history_messages,\n        api_key=APIKEY,\n        base_url=BASE_URL,\n        **kwargs,\n    )\n\n\nasync def embedding_func(texts: list[str]) -> np.ndarray:\n    return await siliconcloud_embedding(\n        texts,\n        # model=EMBEDMODEL,\n        api_key=APIKEY,\n    )\n\n\nasync def get_embedding_dim():\n    test_text = [\"This is a test sentence.\"]\n    embedding = await embedding_func(test_text)\n    embedding_dim = embedding.shape[1]\n    return embedding_dim\n\n\nasync def main():\n    try:\n        # Detect embedding dimension\n        embedding_dimension = await get_embedding_dim()\n        print(f\"Detected embedding dimension: {embedding_dimension}\")\n\n        # Create TiDB DB connection\n        tidb = TiDB(\n            config={\n                \"host\": TIDB_HOST,\n                \"port\": TIDB_PORT,\n                \"user\": TIDB_USER,\n                \"password\": TIDB_PASSWORD,\n                \"database\": TIDB_DATABASE,\n                \"workspace\": \"company\",  # specify which docs you want to store and query\n            }\n        )\n\n        # Check if TiDB DB tables exist, if not, tables will be created\n        await tidb.check_tables()\n\n        # Initialize LightRAG\n        # We use TiDB DB as the KV/vector\n        # You can add `addon_params={\"example_number\": 1, \"language\": \"Simplfied Chinese\"}` to control the prompt\n        rag = LightRAG(\n            enable_llm_cache=False,\n            working_dir=WORKING_DIR,\n            chunk_token_size=512,\n            llm_model_func=llm_model_func,\n            embedding_func=EmbeddingFunc(\n                embedding_dim=embedding_dimension,\n                max_token_size=512,\n                func=embedding_func,\n            ),\n            kv_storage=\"TiDBKVStorage\",\n            vector_storage=\"TiDBVectorDBStorage\",\n            graph_storage=\"TiDBGraphStorage\",\n        )\n\n        if rag.llm_response_cache:\n            rag.llm_response_cache.db = tidb\n        rag.full_docs.db = tidb\n        rag.text_chunks.db = tidb\n        rag.entities_vdb.db = tidb\n        rag.relationships_vdb.db = tidb\n        rag.chunks_vdb.db = tidb\n        rag.chunk_entity_relation_graph.db = tidb\n\n        # Extract and Insert into LightRAG storage\n        with open(\"./dickens/demo.txt\", \"r\", encoding=\"utf-8\") as f:\n            await rag.ainsert(f.read())\n\n        # Perform search in different modes\n        modes = [\"naive\", \"local\", \"global\", \"hybrid\"]\n        for mode in modes:\n            print(\"=\" * 20, mode, \"=\" * 20)\n            print(\n                await rag.aquery(\n                    \"What are the top themes in this story?\",\n                    param=QueryParam(mode=mode),\n                )\n            )\n            print(\"-\" * 100, \"\\n\")\n\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n"}
{"type": "source_file", "path": "lightrag/api/lollms_lightrag_server.py", "content": "from fastapi import FastAPI, HTTPException, File, UploadFile, Form\nfrom pydantic import BaseModel\nimport logging\nimport argparse\nfrom lightrag import LightRAG, QueryParam\nfrom lightrag.llm import lollms_model_complete, lollms_embed\nfrom lightrag.utils import EmbeddingFunc\nfrom typing import Optional, List\nfrom enum import Enum\nfrom pathlib import Path\nimport shutil\nimport aiofiles\nfrom ascii_colors import trace_exception\nimport os\n\nfrom fastapi import Depends, Security\nfrom fastapi.security import APIKeyHeader\nfrom fastapi.middleware.cors import CORSMiddleware\n\nfrom starlette.status import HTTP_403_FORBIDDEN\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(\n        description=\"LightRAG FastAPI Server with separate working and input directories\"\n    )\n\n    # Server configuration\n    parser.add_argument(\n        \"--host\", default=\"0.0.0.0\", help=\"Server host (default: 0.0.0.0)\"\n    )\n    parser.add_argument(\n        \"--port\", type=int, default=9621, help=\"Server port (default: 9621)\"\n    )\n\n    # Directory configuration\n    parser.add_argument(\n        \"--working-dir\",\n        default=\"./rag_storage\",\n        help=\"Working directory for RAG storage (default: ./rag_storage)\",\n    )\n    parser.add_argument(\n        \"--input-dir\",\n        default=\"./inputs\",\n        help=\"Directory containing input documents (default: ./inputs)\",\n    )\n\n    # Model configuration\n    parser.add_argument(\n        \"--model\",\n        default=\"mistral-nemo:latest\",\n        help=\"LLM model name (default: mistral-nemo:latest)\",\n    )\n    parser.add_argument(\n        \"--embedding-model\",\n        default=\"bge-m3:latest\",\n        help=\"Embedding model name (default: bge-m3:latest)\",\n    )\n    parser.add_argument(\n        \"--lollms-host\",\n        default=\"http://localhost:9600\",\n        help=\"lollms host URL (default: http://localhost:9600)\",\n    )\n\n    # RAG configuration\n    parser.add_argument(\n        \"--max-async\", type=int, default=4, help=\"Maximum async operations (default: 4)\"\n    )\n    parser.add_argument(\n        \"--max-tokens\",\n        type=int,\n        default=32768,\n        help=\"Maximum token size (default: 32768)\",\n    )\n    parser.add_argument(\n        \"--embedding-dim\",\n        type=int,\n        default=1024,\n        help=\"Embedding dimensions (default: 1024)\",\n    )\n    parser.add_argument(\n        \"--max-embed-tokens\",\n        type=int,\n        default=8192,\n        help=\"Maximum embedding token size (default: 8192)\",\n    )\n\n    # Logging configuration\n    parser.add_argument(\n        \"--log-level\",\n        default=\"INFO\",\n        choices=[\"DEBUG\", \"INFO\", \"WARNING\", \"ERROR\", \"CRITICAL\"],\n        help=\"Logging level (default: INFO)\",\n    )\n\n    parser.add_argument(\n        \"--key\",\n        type=str,\n        help=\"API key for authentication. This protects lightrag server against unauthorized access\",\n        default=None,\n    )\n\n    return parser.parse_args()\n\n\nclass DocumentManager:\n    \"\"\"Handles document operations and tracking\"\"\"\n\n    def __init__(self, input_dir: str, supported_extensions: tuple = (\".txt\", \".md\")):\n        self.input_dir = Path(input_dir)\n        self.supported_extensions = supported_extensions\n        self.indexed_files = set()\n\n        # Create input directory if it doesn't exist\n        self.input_dir.mkdir(parents=True, exist_ok=True)\n\n    def scan_directory(self) -> List[Path]:\n        \"\"\"Scan input directory for new files\"\"\"\n        new_files = []\n        for ext in self.supported_extensions:\n            for file_path in self.input_dir.rglob(f\"*{ext}\"):\n                if file_path not in self.indexed_files:\n                    new_files.append(file_path)\n        return new_files\n\n    def mark_as_indexed(self, file_path: Path):\n        \"\"\"Mark a file as indexed\"\"\"\n        self.indexed_files.add(file_path)\n\n    def is_supported_file(self, filename: str) -> bool:\n        \"\"\"Check if file type is supported\"\"\"\n        return any(filename.lower().endswith(ext) for ext in self.supported_extensions)\n\n\n# Pydantic models\nclass SearchMode(str, Enum):\n    naive = \"naive\"\n    local = \"local\"\n    global_ = \"global\"\n    hybrid = \"hybrid\"\n\n\nclass QueryRequest(BaseModel):\n    query: str\n    mode: SearchMode = SearchMode.hybrid\n    stream: bool = False\n    only_need_context: bool = False\n\n\nclass QueryResponse(BaseModel):\n    response: str\n\n\nclass InsertTextRequest(BaseModel):\n    text: str\n    description: Optional[str] = None\n\n\nclass InsertResponse(BaseModel):\n    status: str\n    message: str\n    document_count: int\n\n\ndef get_api_key_dependency(api_key: Optional[str]):\n    if not api_key:\n        # If no API key is configured, return a dummy dependency that always succeeds\n        async def no_auth():\n            return None\n\n        return no_auth\n\n    # If API key is configured, use proper authentication\n    api_key_header = APIKeyHeader(name=\"X-API-Key\", auto_error=False)\n\n    async def api_key_auth(api_key_header_value: str | None = Security(api_key_header)):\n        if not api_key_header_value:\n            raise HTTPException(\n                status_code=HTTP_403_FORBIDDEN, detail=\"API Key required\"\n            )\n        if api_key_header_value != api_key:\n            raise HTTPException(\n                status_code=HTTP_403_FORBIDDEN, detail=\"Invalid API Key\"\n            )\n        return api_key_header_value\n\n    return api_key_auth\n\n\ndef create_app(args):\n    # Setup logging\n    logging.basicConfig(\n        format=\"%(levelname)s:%(message)s\", level=getattr(logging, args.log_level)\n    )\n\n    # Check if API key is provided either through env var or args\n    api_key = os.getenv(\"LIGHTRAG_API_KEY\") or args.key\n\n    # Initialize FastAPI\n    app = FastAPI(\n        title=\"LightRAG API\",\n        description=\"API for querying text using LightRAG with separate storage and input directories\"\n        + \"(With authentication)\"\n        if api_key\n        else \"\",\n        version=\"1.0.0\",\n        openapi_tags=[{\"name\": \"api\"}],\n    )\n\n    # Add CORS middleware\n    app.add_middleware(\n        CORSMiddleware,\n        allow_origins=[\"*\"],\n        allow_credentials=True,\n        allow_methods=[\"*\"],\n        allow_headers=[\"*\"],\n    )\n\n    # Create the optional API key dependency\n    optional_api_key = get_api_key_dependency(api_key)\n\n    # Create working directory if it doesn't exist\n    Path(args.working_dir).mkdir(parents=True, exist_ok=True)\n\n    # Initialize document manager\n    doc_manager = DocumentManager(args.input_dir)\n\n    # Initialize RAG\n    rag = LightRAG(\n        working_dir=args.working_dir,\n        llm_model_func=lollms_model_complete,\n        llm_model_name=args.model,\n        llm_model_max_async=args.max_async,\n        llm_model_max_token_size=args.max_tokens,\n        llm_model_kwargs={\n            \"host\": args.lollms_host,\n            \"options\": {\"num_ctx\": args.max_tokens},\n        },\n        embedding_func=EmbeddingFunc(\n            embedding_dim=args.embedding_dim,\n            max_token_size=args.max_embed_tokens,\n            func=lambda texts: lollms_embed(\n                texts, embed_model=args.embedding_model, host=args.lollms_host\n            ),\n        ),\n    )\n\n    @app.on_event(\"startup\")\n    async def startup_event():\n        \"\"\"Index all files in input directory during startup\"\"\"\n        try:\n            new_files = doc_manager.scan_directory()\n            for file_path in new_files:\n                try:\n                    # Use async file reading\n                    async with aiofiles.open(file_path, \"r\", encoding=\"utf-8\") as f:\n                        content = await f.read()\n                        # Use the async version of insert directly\n                        await rag.ainsert(content)\n                        doc_manager.mark_as_indexed(file_path)\n                        logging.info(f\"Indexed file: {file_path}\")\n                except Exception as e:\n                    trace_exception(e)\n                    logging.error(f\"Error indexing file {file_path}: {str(e)}\")\n\n            logging.info(f\"Indexed {len(new_files)} documents from {args.input_dir}\")\n\n        except Exception as e:\n            logging.error(f\"Error during startup indexing: {str(e)}\")\n\n    @app.post(\"/documents/scan\", dependencies=[Depends(optional_api_key)])\n    async def scan_for_new_documents():\n        \"\"\"Manually trigger scanning for new documents\"\"\"\n        try:\n            new_files = doc_manager.scan_directory()\n            indexed_count = 0\n\n            for file_path in new_files:\n                try:\n                    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n                        content = f.read()\n                        await rag.ainsert(content)\n                        doc_manager.mark_as_indexed(file_path)\n                        indexed_count += 1\n                except Exception as e:\n                    logging.error(f\"Error indexing file {file_path}: {str(e)}\")\n\n            return {\n                \"status\": \"success\",\n                \"indexed_count\": indexed_count,\n                \"total_documents\": len(doc_manager.indexed_files),\n            }\n        except Exception as e:\n            raise HTTPException(status_code=500, detail=str(e))\n\n    @app.post(\"/documents/upload\", dependencies=[Depends(optional_api_key)])\n    async def upload_to_input_dir(file: UploadFile = File(...)):\n        \"\"\"Upload a file to the input directory\"\"\"\n        try:\n            if not doc_manager.is_supported_file(file.filename):\n                raise HTTPException(\n                    status_code=400,\n                    detail=f\"Unsupported file type. Supported types: {doc_manager.supported_extensions}\",\n                )\n\n            file_path = doc_manager.input_dir / file.filename\n            with open(file_path, \"wb\") as buffer:\n                shutil.copyfileobj(file.file, buffer)\n\n            # Immediately index the uploaded file\n            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n                content = f.read()\n                await rag.ainsert(content)\n                doc_manager.mark_as_indexed(file_path)\n\n            return {\n                \"status\": \"success\",\n                \"message\": f\"File uploaded and indexed: {file.filename}\",\n                \"total_documents\": len(doc_manager.indexed_files),\n            }\n        except Exception as e:\n            raise HTTPException(status_code=500, detail=str(e))\n\n    @app.post(\n        \"/query\", response_model=QueryResponse, dependencies=[Depends(optional_api_key)]\n    )\n    async def query_text(request: QueryRequest):\n        try:\n            response = await rag.aquery(\n                request.query,\n                param=QueryParam(\n                    mode=request.mode,\n                    stream=request.stream,\n                    only_need_context=request.only_need_context,\n                ),\n            )\n\n            if request.stream:\n                result = \"\"\n                async for chunk in response:\n                    result += chunk\n                return QueryResponse(response=result)\n            else:\n                return QueryResponse(response=response)\n        except Exception as e:\n            raise HTTPException(status_code=500, detail=str(e))\n\n    @app.post(\"/query/stream\", dependencies=[Depends(optional_api_key)])\n    async def query_text_stream(request: QueryRequest):\n        try:\n            response = rag.query(\n                request.query,\n                param=QueryParam(\n                    mode=request.mode,\n                    stream=True,\n                    only_need_context=request.only_need_context,\n                ),\n            )\n\n            async def stream_generator():\n                async for chunk in response:\n                    yield chunk\n\n            return stream_generator()\n        except Exception as e:\n            raise HTTPException(status_code=500, detail=str(e))\n\n    @app.post(\n        \"/documents/text\",\n        response_model=InsertResponse,\n        dependencies=[Depends(optional_api_key)],\n    )\n    async def insert_text(request: InsertTextRequest):\n        try:\n            rag.insert(request.text)\n            return InsertResponse(\n                status=\"success\",\n                message=\"Text successfully inserted\",\n                document_count=len(rag),\n            )\n        except Exception as e:\n            raise HTTPException(status_code=500, detail=str(e))\n\n    @app.post(\n        \"/documents/file\",\n        response_model=InsertResponse,\n        dependencies=[Depends(optional_api_key)],\n    )\n    async def insert_file(file: UploadFile = File(...), description: str = Form(None)):\n        try:\n            content = await file.read()\n\n            if file.filename.endswith((\".txt\", \".md\")):\n                text = content.decode(\"utf-8\")\n                await rag.ainsert(text)\n            else:\n                raise HTTPException(\n                    status_code=400,\n                    detail=\"Unsupported file type. Only .txt and .md files are supported\",\n                )\n\n            return InsertResponse(\n                status=\"success\",\n                message=f\"File '{file.filename}' successfully inserted\",\n                document_count=1,\n            )\n        except UnicodeDecodeError:\n            raise HTTPException(status_code=400, detail=\"File encoding not supported\")\n        except Exception as e:\n            raise HTTPException(status_code=500, detail=str(e))\n\n    @app.post(\n        \"/documents/batch\",\n        response_model=InsertResponse,\n        dependencies=[Depends(optional_api_key)],\n    )\n    async def insert_batch(files: List[UploadFile] = File(...)):\n        try:\n            inserted_count = 0\n            failed_files = []\n\n            for file in files:\n                try:\n                    content = await file.read()\n                    if file.filename.endswith((\".txt\", \".md\")):\n                        text = content.decode(\"utf-8\")\n                        await rag.ainsert(text)\n                        inserted_count += 1\n                    else:\n                        failed_files.append(f\"{file.filename} (unsupported type)\")\n                except Exception as e:\n                    failed_files.append(f\"{file.filename} ({str(e)})\")\n\n            status_message = f\"Successfully inserted {inserted_count} documents\"\n            if failed_files:\n                status_message += f\". Failed files: {', '.join(failed_files)}\"\n\n            return InsertResponse(\n                status=\"success\" if inserted_count > 0 else \"partial_success\",\n                message=status_message,\n                document_count=len(files),\n            )\n        except Exception as e:\n            raise HTTPException(status_code=500, detail=str(e))\n\n    @app.delete(\n        \"/documents\",\n        response_model=InsertResponse,\n        dependencies=[Depends(optional_api_key)],\n    )\n    async def clear_documents():\n        try:\n            rag.text_chunks = []\n            rag.entities_vdb = None\n            rag.relationships_vdb = None\n            return InsertResponse(\n                status=\"success\",\n                message=\"All documents cleared successfully\",\n                document_count=0,\n            )\n        except Exception as e:\n            raise HTTPException(status_code=500, detail=str(e))\n\n    @app.get(\"/health\", dependencies=[Depends(optional_api_key)])\n    async def get_status():\n        \"\"\"Get current system status\"\"\"\n        return {\n            \"status\": \"healthy\",\n            \"working_directory\": str(args.working_dir),\n            \"input_directory\": str(args.input_dir),\n            \"indexed_files\": len(doc_manager.indexed_files),\n            \"configuration\": {\n                \"model\": args.model,\n                \"embedding_model\": args.embedding_model,\n                \"max_tokens\": args.max_tokens,\n                \"lollms_host\": args.lollms_host,\n            },\n        }\n\n    return app\n\n\ndef main():\n    args = parse_args()\n    import uvicorn\n\n    app = create_app(args)\n    uvicorn.run(app, host=args.host, port=args.port)\n\n\nif __name__ == \"__main__\":\n    main()\n"}
{"type": "source_file", "path": "examples/lightrag_ollama_gremlin_demo.py", "content": "import asyncio\nimport inspect\nimport os\n\n# Uncomment these lines below to filter out somewhat verbose INFO level\n# logging prints (the default loglevel is INFO).\n# This has to go before the lightrag imports to work,\n# which triggers linting errors, so we keep it commented out:\n# import logging\n# logging.basicConfig(format=\"%(levelname)s:%(message)s\", level=logging.WARN)\n\nfrom lightrag import LightRAG, QueryParam\nfrom lightrag.llm import ollama_embedding, ollama_model_complete\nfrom lightrag.utils import EmbeddingFunc\n\nWORKING_DIR = \"./dickens_gremlin\"\n\nif not os.path.exists(WORKING_DIR):\n    os.mkdir(WORKING_DIR)\n\n# Gremlin\nos.environ[\"GREMLIN_HOST\"] = \"localhost\"\nos.environ[\"GREMLIN_PORT\"] = \"8182\"\nos.environ[\"GREMLIN_GRAPH\"] = \"dickens\"\n\n# Creating a non-default source requires manual\n# configuration and a restart on the server: use the dafault \"g\"\nos.environ[\"GREMLIN_TRAVERSE_SOURCE\"] = \"g\"\n\n# No authorization by default on docker tinkerpop/gremlin-server\nos.environ[\"GREMLIN_USER\"] = \"\"\nos.environ[\"GREMLIN_PASSWORD\"] = \"\"\n\nrag = LightRAG(\n    working_dir=WORKING_DIR,\n    llm_model_func=ollama_model_complete,\n    llm_model_name=\"llama3.1:8b\",\n    llm_model_max_async=4,\n    llm_model_max_token_size=32768,\n    llm_model_kwargs={\"host\": \"http://localhost:11434\", \"options\": {\"num_ctx\": 32768}},\n    embedding_func=EmbeddingFunc(\n        embedding_dim=768,\n        max_token_size=8192,\n        func=lambda texts: ollama_embedding(\n            texts, embed_model=\"nomic-embed-text\", host=\"http://localhost:11434\"\n        ),\n    ),\n    graph_storage=\"GremlinStorage\",\n)\n\nwith open(\"./book.txt\", \"r\", encoding=\"utf-8\") as f:\n    rag.insert(f.read())\n\n# Perform naive search\nprint(\n    rag.query(\"What are the top themes in this story?\", param=QueryParam(mode=\"naive\"))\n)\n\n# Perform local search\nprint(\n    rag.query(\"What are the top themes in this story?\", param=QueryParam(mode=\"local\"))\n)\n\n# Perform global search\nprint(\n    rag.query(\"What are the top themes in this story?\", param=QueryParam(mode=\"global\"))\n)\n\n# Perform hybrid search\nprint(\n    rag.query(\"What are the top themes in this story?\", param=QueryParam(mode=\"hybrid\"))\n)\n\n# stream response\nresp = rag.query(\n    \"What are the top themes in this story?\",\n    param=QueryParam(mode=\"hybrid\", stream=True),\n)\n\n\nasync def print_stream(stream):\n    async for chunk in stream:\n        print(chunk, end=\"\", flush=True)\n\n\nif inspect.isasyncgen(resp):\n    asyncio.run(print_stream(resp))\nelse:\n    print(resp)\n"}
{"type": "source_file", "path": "lightrag/__init__.py", "content": "from .lightrag import LightRAG as LightRAG, QueryParam as QueryParam\n\n__version__ = \"1.1.0\"\n__author__ = \"Zirui Guo\"\n__url__ = \"https://github.com/HKUDS/LightRAG\"\n"}
{"type": "source_file", "path": "examples/lightrag_oracle_demo.py", "content": "import sys\nimport os\nfrom pathlib import Path\nimport asyncio\nfrom lightrag import LightRAG, QueryParam\nfrom lightrag.llm import openai_complete_if_cache, openai_embedding\nfrom lightrag.utils import EmbeddingFunc\nimport numpy as np\nfrom lightrag.kg.oracle_impl import OracleDB\n\nprint(os.getcwd())\nscript_directory = Path(__file__).resolve().parent.parent\nsys.path.append(os.path.abspath(script_directory))\n\nWORKING_DIR = \"./dickens\"\n\n# We use OpenAI compatible API to call LLM on Oracle Cloud\n# More docs here https://github.com/jin38324/OCI_GenAI_access_gateway\nBASE_URL = \"http://xxx.xxx.xxx.xxx:8088/v1/\"\nAPIKEY = \"ocigenerativeai\"\nCHATMODEL = \"cohere.command-r-plus\"\nEMBEDMODEL = \"cohere.embed-multilingual-v3.0\"\n\n\nif not os.path.exists(WORKING_DIR):\n    os.mkdir(WORKING_DIR)\n\n\nasync def llm_model_func(\n    prompt, system_prompt=None, history_messages=[], keyword_extraction=False, **kwargs\n) -> str:\n    return await openai_complete_if_cache(\n        CHATMODEL,\n        prompt,\n        system_prompt=system_prompt,\n        history_messages=history_messages,\n        api_key=APIKEY,\n        base_url=BASE_URL,\n        **kwargs,\n    )\n\n\nasync def embedding_func(texts: list[str]) -> np.ndarray:\n    return await openai_embedding(\n        texts,\n        model=EMBEDMODEL,\n        api_key=APIKEY,\n        base_url=BASE_URL,\n    )\n\n\nasync def get_embedding_dim():\n    test_text = [\"This is a test sentence.\"]\n    embedding = await embedding_func(test_text)\n    embedding_dim = embedding.shape[1]\n    return embedding_dim\n\n\nasync def main():\n    try:\n        # Detect embedding dimension\n        embedding_dimension = await get_embedding_dim()\n        print(f\"Detected embedding dimension: {embedding_dimension}\")\n\n        # Create Oracle DB connection\n        # The `config` parameter is the connection configuration of Oracle DB\n        # More docs here https://python-oracledb.readthedocs.io/en/latest/user_guide/connection_handling.html\n        # We storage data in unified tables, so we need to set a `workspace` parameter to specify which docs we want to store and query\n        # Below is an example of how to connect to Oracle Autonomous Database on Oracle Cloud\n        oracle_db = OracleDB(\n            config={\n                \"user\": \"username\",\n                \"password\": \"xxxxxxxxx\",\n                \"dsn\": \"xxxxxxx_medium\",\n                \"config_dir\": \"dir/path/to/oracle/config\",\n                \"wallet_location\": \"dir/path/to/oracle/wallet\",\n                \"wallet_password\": \"xxxxxxxxx\",\n                \"workspace\": \"company\",  # specify which docs you want to store and query\n            }\n        )\n\n        # Check if Oracle DB tables exist, if not, tables will be created\n        await oracle_db.check_tables()\n\n        # Initialize LightRAG\n        # We use Oracle DB as the KV/vector/graph storage\n        # You can add `addon_params={\"example_number\": 1, \"language\": \"Simplfied Chinese\"}` to control the prompt\n        rag = LightRAG(\n            enable_llm_cache=False,\n            working_dir=WORKING_DIR,\n            chunk_token_size=512,\n            llm_model_func=llm_model_func,\n            embedding_func=EmbeddingFunc(\n                embedding_dim=embedding_dimension,\n                max_token_size=512,\n                func=embedding_func,\n            ),\n            graph_storage=\"OracleGraphStorage\",\n            kv_storage=\"OracleKVStorage\",\n            vector_storage=\"OracleVectorDBStorage\",\n        )\n\n        # Setthe KV/vector/graph storage's `db` property, so all operation will use same connection pool\n        rag.graph_storage_cls.db = oracle_db\n        rag.key_string_value_json_storage_cls.db = oracle_db\n        rag.vector_db_storage_cls.db = oracle_db\n        # add embedding_func for graph database, it's deleted in commit 5661d76860436f7bf5aef2e50d9ee4a59660146c\n        rag.chunk_entity_relation_graph.embedding_func = rag.embedding_func\n\n        # Extract and Insert into LightRAG storage\n        with open(\"./dickens/demo.txt\", \"r\", encoding=\"utf-8\") as f:\n            await rag.ainsert(f.read())\n\n        # Perform search in different modes\n        modes = [\"naive\", \"local\", \"global\", \"hybrid\"]\n        for mode in modes:\n            print(\"=\" * 20, mode, \"=\" * 20)\n            print(\n                await rag.aquery(\n                    \"What are the top themes in this story?\",\n                    param=QueryParam(mode=mode),\n                )\n            )\n            print(\"-\" * 100, \"\\n\")\n\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n"}
{"type": "source_file", "path": "get_all_edges_nx.py", "content": "import networkx as nx\n\nG = nx.read_graphml(\"./dickensTestEmbedcall/graph_chunk_entity_relation.graphml\")\n\n\ndef get_all_edges_and_nodes(G):\n    # Get all edges and their properties\n    edges_with_properties = []\n    for u, v, data in G.edges(data=True):\n        edges_with_properties.append(\n            {\n                \"start\": u,\n                \"end\": v,\n                \"label\": data.get(\n                    \"label\", \"\"\n                ),  # Assuming 'label' is used for edge type\n                \"properties\": data,\n                \"start_node_properties\": G.nodes[u],\n                \"end_node_properties\": G.nodes[v],\n            }\n        )\n\n    return edges_with_properties\n\n\n# Example usage\nif __name__ == \"__main__\":\n    # Assume G is your NetworkX graph loaded from Neo4j\n\n    all_edges = get_all_edges_and_nodes(G)\n\n    # Print all edges and node properties\n    for edge in all_edges:\n        print(f\"Edge Label: {edge['label']}\")\n        print(f\"Edge Properties: {edge['properties']}\")\n        print(f\"Start Node: {edge['start']}\")\n        print(f\"Start Node Properties: {edge['start_node_properties']}\")\n        print(f\"End Node: {edge['end']}\")\n        print(f\"End Node Properties: {edge['end_node_properties']}\")\n        print(\"---\")\n"}
{"type": "source_file", "path": "examples/lightrag_zhipu_demo.py", "content": "import os\nimport logging\n\n\nfrom lightrag import LightRAG, QueryParam\nfrom lightrag.llm import zhipu_complete, zhipu_embedding\nfrom lightrag.utils import EmbeddingFunc\n\nWORKING_DIR = \"./dickens\"\n\nlogging.basicConfig(format=\"%(levelname)s:%(message)s\", level=logging.INFO)\n\nif not os.path.exists(WORKING_DIR):\n    os.mkdir(WORKING_DIR)\n\napi_key = os.environ.get(\"ZHIPUAI_API_KEY\")\nif api_key is None:\n    raise Exception(\"Please set ZHIPU_API_KEY in your environment\")\n\n\nrag = LightRAG(\n    working_dir=WORKING_DIR,\n    llm_model_func=zhipu_complete,\n    llm_model_name=\"glm-4-flashx\",  # Using the most cost/performance balance model, but you can change it here.\n    llm_model_max_async=4,\n    llm_model_max_token_size=32768,\n    embedding_func=EmbeddingFunc(\n        embedding_dim=2048,  # Zhipu embedding-3 dimension\n        max_token_size=8192,\n        func=lambda texts: zhipu_embedding(texts),\n    ),\n)\n\nwith open(\"./book.txt\", \"r\", encoding=\"utf-8\") as f:\n    rag.insert(f.read())\n\n# Perform naive search\nprint(\n    rag.query(\"What are the top themes in this story?\", param=QueryParam(mode=\"naive\"))\n)\n\n# Perform local search\nprint(\n    rag.query(\"What are the top themes in this story?\", param=QueryParam(mode=\"local\"))\n)\n\n# Perform global search\nprint(\n    rag.query(\"What are the top themes in this story?\", param=QueryParam(mode=\"global\"))\n)\n\n# Perform hybrid search\nprint(\n    rag.query(\"What are the top themes in this story?\", param=QueryParam(mode=\"hybrid\"))\n)\n"}
{"type": "source_file", "path": "examples/lightrag_zhipu_postgres_demo.py", "content": "import asyncio\nimport logging\nimport os\nimport time\nfrom dotenv import load_dotenv\n\nfrom lightrag import LightRAG, QueryParam\nfrom lightrag.kg.postgres_impl import PostgreSQLDB\nfrom lightrag.llm import ollama_embedding, zhipu_complete\nfrom lightrag.utils import EmbeddingFunc\n\nload_dotenv()\nROOT_DIR = os.environ.get(\"ROOT_DIR\")\nWORKING_DIR = f\"{ROOT_DIR}/dickens-pg\"\n\nlogging.basicConfig(format=\"%(levelname)s:%(message)s\", level=logging.INFO)\n\nif not os.path.exists(WORKING_DIR):\n    os.mkdir(WORKING_DIR)\n\n# AGE\nos.environ[\"AGE_GRAPH_NAME\"] = \"dickens\"\n\npostgres_db = PostgreSQLDB(\n    config={\n        \"host\": \"localhost\",\n        \"port\": 15432,\n        \"user\": \"rag\",\n        \"password\": \"rag\",\n        \"database\": \"rag\",\n    }\n)\n\n\nasync def main():\n    await postgres_db.initdb()\n    # Check if PostgreSQL DB tables exist, if not, tables will be created\n    await postgres_db.check_tables()\n\n    rag = LightRAG(\n        working_dir=WORKING_DIR,\n        llm_model_func=zhipu_complete,\n        llm_model_name=\"glm-4-flashx\",\n        llm_model_max_async=4,\n        llm_model_max_token_size=32768,\n        enable_llm_cache_for_entity_extract=True,\n        embedding_func=EmbeddingFunc(\n            embedding_dim=768,\n            max_token_size=8192,\n            func=lambda texts: ollama_embedding(\n                texts, embed_model=\"nomic-embed-text\", host=\"http://localhost:11434\"\n            ),\n        ),\n        kv_storage=\"PGKVStorage\",\n        doc_status_storage=\"PGDocStatusStorage\",\n        graph_storage=\"PGGraphStorage\",\n        vector_storage=\"PGVectorStorage\",\n    )\n    # Set the KV/vector/graph storage's `db` property, so all operation will use same connection pool\n    rag.doc_status.db = postgres_db\n    rag.full_docs.db = postgres_db\n    rag.text_chunks.db = postgres_db\n    rag.llm_response_cache.db = postgres_db\n    rag.key_string_value_json_storage_cls.db = postgres_db\n    rag.chunks_vdb.db = postgres_db\n    rag.relationships_vdb.db = postgres_db\n    rag.entities_vdb.db = postgres_db\n    rag.graph_storage_cls.db = postgres_db\n    rag.chunk_entity_relation_graph.db = postgres_db\n    # add embedding_func for graph database, it's deleted in commit 5661d76860436f7bf5aef2e50d9ee4a59660146c\n    rag.chunk_entity_relation_graph.embedding_func = rag.embedding_func\n\n    with open(f\"{ROOT_DIR}/book.txt\", \"r\", encoding=\"utf-8\") as f:\n        await rag.ainsert(f.read())\n\n    print(\"==== Trying to test the rag queries ====\")\n    print(\"**** Start Naive Query ****\")\n    start_time = time.time()\n    # Perform naive search\n    print(\n        await rag.aquery(\n            \"What are the top themes in this story?\", param=QueryParam(mode=\"naive\")\n        )\n    )\n    print(f\"Naive Query Time: {time.time() - start_time} seconds\")\n    # Perform local search\n    print(\"**** Start Local Query ****\")\n    start_time = time.time()\n    print(\n        await rag.aquery(\n            \"What are the top themes in this story?\", param=QueryParam(mode=\"local\")\n        )\n    )\n    print(f\"Local Query Time: {time.time() - start_time} seconds\")\n    # Perform global search\n    print(\"**** Start Global Query ****\")\n    start_time = time.time()\n    print(\n        await rag.aquery(\n            \"What are the top themes in this story?\", param=QueryParam(mode=\"global\")\n        )\n    )\n    print(f\"Global Query Time: {time.time() - start_time}\")\n    # Perform hybrid search\n    print(\"**** Start Hybrid Query ****\")\n    print(\n        await rag.aquery(\n            \"What are the top themes in this story?\", param=QueryParam(mode=\"hybrid\")\n        )\n    )\n    print(f\"Hybrid Query Time: {time.time() - start_time} seconds\")\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n"}
{"type": "source_file", "path": "examples/lightrag_openai_demo.py", "content": "import os\n\nfrom lightrag import LightRAG, QueryParam\nfrom lightrag.llm import gpt_4o_mini_complete\n\nWORKING_DIR = \"./dickens\"\n\nif not os.path.exists(WORKING_DIR):\n    os.mkdir(WORKING_DIR)\n\nrag = LightRAG(\n    working_dir=WORKING_DIR,\n    llm_model_func=gpt_4o_mini_complete,\n    # llm_model_func=gpt_4o_complete\n)\n\n\nwith open(\"./book.txt\", \"r\", encoding=\"utf-8\") as f:\n    rag.insert(f.read())\n\n# Perform naive search\nprint(\n    rag.query(\"What are the top themes in this story?\", param=QueryParam(mode=\"naive\"))\n)\n\n# Perform local search\nprint(\n    rag.query(\"What are the top themes in this story?\", param=QueryParam(mode=\"local\"))\n)\n\n# Perform global search\nprint(\n    rag.query(\"What are the top themes in this story?\", param=QueryParam(mode=\"global\"))\n)\n\n# Perform hybrid search\nprint(\n    rag.query(\"What are the top themes in this story?\", param=QueryParam(mode=\"hybrid\"))\n)\n"}
{"type": "source_file", "path": "examples/lightrag_openai_compatible_stream_demo.py", "content": "import os\nimport inspect\nfrom lightrag import LightRAG\nfrom lightrag.llm import openai_complete, openai_embedding\nfrom lightrag.utils import EmbeddingFunc\nfrom lightrag.lightrag import always_get_an_event_loop\nfrom lightrag import QueryParam\n\n# WorkingDir\nROOT_DIR = os.path.dirname(os.path.abspath(__file__))\nWORKING_DIR = os.path.join(ROOT_DIR, \"dickens\")\nif not os.path.exists(WORKING_DIR):\n    os.mkdir(WORKING_DIR)\nprint(f\"WorkingDir: {WORKING_DIR}\")\n\napi_key = \"empty\"\nrag = LightRAG(\n    working_dir=WORKING_DIR,\n    llm_model_func=openai_complete,\n    llm_model_name=\"qwen2.5-14b-instruct@4bit\",\n    llm_model_max_async=4,\n    llm_model_max_token_size=32768,\n    llm_model_kwargs={\"base_url\": \"http://127.0.0.1:1234/v1\", \"api_key\": api_key},\n    embedding_func=EmbeddingFunc(\n        embedding_dim=1024,\n        max_token_size=8192,\n        func=lambda texts: openai_embedding(\n            texts=texts,\n            model=\"text-embedding-bge-m3\",\n            base_url=\"http://127.0.0.1:1234/v1\",\n            api_key=api_key,\n        ),\n    ),\n)\n\nwith open(\"./book.txt\", \"r\", encoding=\"utf-8\") as f:\n    rag.insert(f.read())\n\nresp = rag.query(\n    \"What are the top themes in this story?\",\n    param=QueryParam(mode=\"hybrid\", stream=True),\n)\n\n\nasync def print_stream(stream):\n    async for chunk in stream:\n        if chunk:\n            print(chunk, end=\"\", flush=True)\n\n\nloop = always_get_an_event_loop()\nif inspect.isasyncgen(resp):\n    loop.run_until_complete(print_stream(resp))\nelse:\n    print(resp)\n"}
{"type": "source_file", "path": "lightrag/api/azure_openai_lightrag_server.py", "content": "from fastapi import FastAPI, HTTPException, File, UploadFile, Form\nfrom pydantic import BaseModel\nimport asyncio\nimport logging\nimport argparse\nfrom lightrag import LightRAG, QueryParam\nfrom lightrag.llm import (\n    azure_openai_complete_if_cache,\n    azure_openai_embedding,\n)\nfrom lightrag.utils import EmbeddingFunc\nfrom typing import Optional, List\nfrom enum import Enum\nfrom pathlib import Path\nimport shutil\nimport aiofiles\nfrom ascii_colors import trace_exception\nimport os\nfrom dotenv import load_dotenv\nimport inspect\nimport json\nfrom fastapi.responses import StreamingResponse\n\nfrom fastapi import Depends, Security\nfrom fastapi.security import APIKeyHeader\nfrom fastapi.middleware.cors import CORSMiddleware\n\nfrom starlette.status import HTTP_403_FORBIDDEN\n\nload_dotenv()\n\nAZURE_OPENAI_API_VERSION = os.getenv(\"AZURE_OPENAI_API_VERSION\")\nAZURE_OPENAI_DEPLOYMENT = os.getenv(\"AZURE_OPENAI_DEPLOYMENT\")\nAZURE_OPENAI_API_KEY = os.getenv(\"AZURE_OPENAI_API_KEY\")\nAZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n\nAZURE_EMBEDDING_DEPLOYMENT = os.getenv(\"AZURE_EMBEDDING_DEPLOYMENT\")\nAZURE_EMBEDDING_API_VERSION = os.getenv(\"AZURE_EMBEDDING_API_VERSION\")\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(\n        description=\"LightRAG FastAPI Server with OpenAI integration\"\n    )\n\n    # Server configuration\n    parser.add_argument(\n        \"--host\", default=\"0.0.0.0\", help=\"Server host (default: 0.0.0.0)\"\n    )\n    parser.add_argument(\n        \"--port\", type=int, default=9621, help=\"Server port (default: 9621)\"\n    )\n\n    # Directory configuration\n    parser.add_argument(\n        \"--working-dir\",\n        default=\"./rag_storage\",\n        help=\"Working directory for RAG storage (default: ./rag_storage)\",\n    )\n    parser.add_argument(\n        \"--input-dir\",\n        default=\"./inputs\",\n        help=\"Directory containing input documents (default: ./inputs)\",\n    )\n\n    # Model configuration\n    parser.add_argument(\n        \"--model\", default=\"gpt-4o\", help=\"OpenAI model name (default: gpt-4o)\"\n    )\n    parser.add_argument(\n        \"--embedding-model\",\n        default=\"text-embedding-3-large\",\n        help=\"OpenAI embedding model (default: text-embedding-3-large)\",\n    )\n\n    # RAG configuration\n    parser.add_argument(\n        \"--max-tokens\",\n        type=int,\n        default=32768,\n        help=\"Maximum token size (default: 32768)\",\n    )\n    parser.add_argument(\n        \"--max-embed-tokens\",\n        type=int,\n        default=8192,\n        help=\"Maximum embedding token size (default: 8192)\",\n    )\n    parser.add_argument(\n        \"--enable-cache\",\n        default=True,\n        help=\"Enable response cache (default: True)\",\n    )\n    # Logging configuration\n    parser.add_argument(\n        \"--log-level\",\n        default=\"INFO\",\n        choices=[\"DEBUG\", \"INFO\", \"WARNING\", \"ERROR\", \"CRITICAL\"],\n        help=\"Logging level (default: INFO)\",\n    )\n\n    parser.add_argument(\n        \"--key\",\n        type=str,\n        help=\"API key for authentication. This protects lightrag server against unauthorized access\",\n        default=None,\n    )\n\n    return parser.parse_args()\n\n\nclass DocumentManager:\n    \"\"\"Handles document operations and tracking\"\"\"\n\n    def __init__(self, input_dir: str, supported_extensions: tuple = (\".txt\", \".md\")):\n        self.input_dir = Path(input_dir)\n        self.supported_extensions = supported_extensions\n        self.indexed_files = set()\n\n        # Create input directory if it doesn't exist\n        self.input_dir.mkdir(parents=True, exist_ok=True)\n\n    def scan_directory(self) -> List[Path]:\n        \"\"\"Scan input directory for new files\"\"\"\n        new_files = []\n        for ext in self.supported_extensions:\n            for file_path in self.input_dir.rglob(f\"*{ext}\"):\n                if file_path not in self.indexed_files:\n                    new_files.append(file_path)\n        return new_files\n\n    def mark_as_indexed(self, file_path: Path):\n        \"\"\"Mark a file as indexed\"\"\"\n        self.indexed_files.add(file_path)\n\n    def is_supported_file(self, filename: str) -> bool:\n        \"\"\"Check if file type is supported\"\"\"\n        return any(filename.lower().endswith(ext) for ext in self.supported_extensions)\n\n\n# Pydantic models\nclass SearchMode(str, Enum):\n    naive = \"naive\"\n    local = \"local\"\n    global_ = \"global\"\n    hybrid = \"hybrid\"\n\n\nclass QueryRequest(BaseModel):\n    query: str\n    mode: SearchMode = SearchMode.hybrid\n    only_need_context: bool = False\n    # stream: bool = False\n\n\nclass QueryResponse(BaseModel):\n    response: str\n\n\nclass InsertTextRequest(BaseModel):\n    text: str\n    description: Optional[str] = None\n\n\nclass InsertResponse(BaseModel):\n    status: str\n    message: str\n    document_count: int\n\n\ndef get_api_key_dependency(api_key: Optional[str]):\n    if not api_key:\n        # If no API key is configured, return a dummy dependency that always succeeds\n        async def no_auth():\n            return None\n\n        return no_auth\n\n    # If API key is configured, use proper authentication\n    api_key_header = APIKeyHeader(name=\"X-API-Key\", auto_error=False)\n\n    async def api_key_auth(api_key_header_value: str | None = Security(api_key_header)):\n        if not api_key_header_value:\n            raise HTTPException(\n                status_code=HTTP_403_FORBIDDEN, detail=\"API Key required\"\n            )\n        if api_key_header_value != api_key:\n            raise HTTPException(\n                status_code=HTTP_403_FORBIDDEN, detail=\"Invalid API Key\"\n            )\n        return api_key_header_value\n\n    return api_key_auth\n\n\nasync def get_embedding_dim(embedding_model: str) -> int:\n    \"\"\"Get embedding dimensions for the specified model\"\"\"\n    test_text = [\"This is a test sentence.\"]\n    embedding = await azure_openai_embedding(test_text, model=embedding_model)\n    return embedding.shape[1]\n\n\ndef create_app(args):\n    # Setup logging\n    logging.basicConfig(\n        format=\"%(levelname)s:%(message)s\", level=getattr(logging, args.log_level)\n    )\n\n    # Check if API key is provided either through env var or args\n    api_key = os.getenv(\"LIGHTRAG_API_KEY\") or args.key\n\n    # Initialize FastAPI\n    app = FastAPI(\n        title=\"LightRAG API\",\n        description=\"API for querying text using LightRAG with separate storage and input directories\"\n        + \"(With authentication)\"\n        if api_key\n        else \"\",\n        version=\"1.0.0\",\n        openapi_tags=[{\"name\": \"api\"}],\n    )\n\n    # Add CORS middleware\n    app.add_middleware(\n        CORSMiddleware,\n        allow_origins=[\"*\"],\n        allow_credentials=True,\n        allow_methods=[\"*\"],\n        allow_headers=[\"*\"],\n    )\n\n    # Create the optional API key dependency\n    optional_api_key = get_api_key_dependency(api_key)\n\n    # Create working directory if it doesn't exist\n    Path(args.working_dir).mkdir(parents=True, exist_ok=True)\n\n    # Initialize document manager\n    doc_manager = DocumentManager(args.input_dir)\n\n    # Get embedding dimensions\n    embedding_dim = asyncio.run(get_embedding_dim(args.embedding_model))\n\n    async def async_openai_complete(\n        prompt, system_prompt=None, history_messages=[], **kwargs\n    ):\n        \"\"\"Async wrapper for OpenAI completion\"\"\"\n        kwargs.pop(\"keyword_extraction\", None)\n\n        return await azure_openai_complete_if_cache(\n            args.model,\n            prompt,\n            system_prompt=system_prompt,\n            history_messages=history_messages,\n            base_url=AZURE_OPENAI_ENDPOINT,\n            api_key=AZURE_OPENAI_API_KEY,\n            api_version=AZURE_OPENAI_API_VERSION,\n            **kwargs,\n        )\n\n    # Initialize RAG with OpenAI configuration\n    rag = LightRAG(\n        enable_llm_cache=args.enable_cache,\n        working_dir=args.working_dir,\n        llm_model_func=async_openai_complete,\n        llm_model_name=args.model,\n        llm_model_max_token_size=args.max_tokens,\n        embedding_func=EmbeddingFunc(\n            embedding_dim=embedding_dim,\n            max_token_size=args.max_embed_tokens,\n            func=lambda texts: azure_openai_embedding(\n                texts, model=args.embedding_model\n            ),\n        ),\n    )\n\n    @app.on_event(\"startup\")\n    async def startup_event():\n        \"\"\"Index all files in input directory during startup\"\"\"\n        try:\n            new_files = doc_manager.scan_directory()\n            for file_path in new_files:\n                try:\n                    # Use async file reading\n                    async with aiofiles.open(file_path, \"r\", encoding=\"utf-8\") as f:\n                        content = await f.read()\n                        # Use the async version of insert directly\n                        await rag.ainsert(content)\n                        doc_manager.mark_as_indexed(file_path)\n                        logging.info(f\"Indexed file: {file_path}\")\n                except Exception as e:\n                    trace_exception(e)\n                    logging.error(f\"Error indexing file {file_path}: {str(e)}\")\n\n            logging.info(f\"Indexed {len(new_files)} documents from {args.input_dir}\")\n\n        except Exception as e:\n            logging.error(f\"Error during startup indexing: {str(e)}\")\n\n    @app.post(\"/documents/scan\", dependencies=[Depends(optional_api_key)])\n    async def scan_for_new_documents():\n        \"\"\"Manually trigger scanning for new documents\"\"\"\n        try:\n            new_files = doc_manager.scan_directory()\n            indexed_count = 0\n\n            for file_path in new_files:\n                try:\n                    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n                        content = f.read()\n                        await rag.ainsert(content)\n                        doc_manager.mark_as_indexed(file_path)\n                        indexed_count += 1\n                except Exception as e:\n                    logging.error(f\"Error indexing file {file_path}: {str(e)}\")\n\n            return {\n                \"status\": \"success\",\n                \"indexed_count\": indexed_count,\n                \"total_documents\": len(doc_manager.indexed_files),\n            }\n        except Exception as e:\n            raise HTTPException(status_code=500, detail=str(e))\n\n    @app.post(\"/resetcache\", dependencies=[Depends(optional_api_key)])\n    async def reset_cache():\n        \"\"\"Manually reset cache\"\"\"\n        try:\n            cachefile = args.working_dir + \"/kv_store_llm_response_cache.json\"\n            if os.path.exists(cachefile):\n                with open(cachefile, \"w\") as f:\n                    f.write(\"{}\")\n            return {\"status\": \"success\"}\n        except Exception as e:\n            raise HTTPException(status_code=500, detail=str(e))\n\n    @app.post(\"/documents/upload\", dependencies=[Depends(optional_api_key)])\n    async def upload_to_input_dir(file: UploadFile = File(...)):\n        \"\"\"Upload a file to the input directory\"\"\"\n        try:\n            if not doc_manager.is_supported_file(file.filename):\n                raise HTTPException(\n                    status_code=400,\n                    detail=f\"Unsupported file type. Supported types: {doc_manager.supported_extensions}\",\n                )\n\n            file_path = doc_manager.input_dir / file.filename\n            with open(file_path, \"wb\") as buffer:\n                shutil.copyfileobj(file.file, buffer)\n\n            # Immediately index the uploaded file\n            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n                content = f.read()\n                await rag.ainsert(content)\n                doc_manager.mark_as_indexed(file_path)\n\n            return {\n                \"status\": \"success\",\n                \"message\": f\"File uploaded and indexed: {file.filename}\",\n                \"total_documents\": len(doc_manager.indexed_files),\n            }\n        except Exception as e:\n            raise HTTPException(status_code=500, detail=str(e))\n\n    @app.post(\n        \"/query\", response_model=QueryResponse, dependencies=[Depends(optional_api_key)]\n    )\n    async def query_text(request: QueryRequest):\n        try:\n            response = await rag.aquery(\n                request.query,\n                param=QueryParam(\n                    mode=request.mode,\n                    stream=False,\n                    only_need_context=request.only_need_context,\n                ),\n            )\n            return QueryResponse(response=response)\n        except Exception as e:\n            raise HTTPException(status_code=500, detail=str(e))\n\n    @app.post(\"/query/stream\", dependencies=[Depends(optional_api_key)])\n    async def query_text_stream(request: QueryRequest):\n        try:\n            response = await rag.aquery(\n                request.query,\n                param=QueryParam(\n                    mode=request.mode,\n                    stream=True,\n                    only_need_context=request.only_need_context,\n                ),\n            )\n            if inspect.isasyncgen(response):\n\n                async def stream_generator():\n                    async for chunk in response:\n                        yield json.dumps({\"data\": chunk}) + \"\\n\"\n\n                return StreamingResponse(\n                    stream_generator(), media_type=\"application/json\"\n                )\n            else:\n                return QueryResponse(response=response)\n\n        except Exception as e:\n            raise HTTPException(status_code=500, detail=str(e))\n\n    @app.post(\n        \"/documents/text\",\n        response_model=InsertResponse,\n        dependencies=[Depends(optional_api_key)],\n    )\n    async def insert_text(request: InsertTextRequest):\n        try:\n            await rag.ainsert(request.text)\n            return InsertResponse(\n                status=\"success\",\n                message=\"Text successfully inserted\",\n                document_count=1,\n            )\n        except Exception as e:\n            raise HTTPException(status_code=500, detail=str(e))\n\n    @app.post(\n        \"/documents/file\",\n        response_model=InsertResponse,\n        dependencies=[Depends(optional_api_key)],\n    )\n    async def insert_file(file: UploadFile = File(...), description: str = Form(None)):\n        try:\n            content = await file.read()\n\n            if file.filename.endswith((\".txt\", \".md\")):\n                text = content.decode(\"utf-8\")\n                rag.insert(text)\n            else:\n                raise HTTPException(\n                    status_code=400,\n                    detail=\"Unsupported file type. Only .txt and .md files are supported\",\n                )\n\n            return InsertResponse(\n                status=\"success\",\n                message=f\"File '{file.filename}' successfully inserted\",\n                document_count=1,\n            )\n        except UnicodeDecodeError:\n            raise HTTPException(status_code=400, detail=\"File encoding not supported\")\n        except Exception as e:\n            raise HTTPException(status_code=500, detail=str(e))\n\n    @app.post(\n        \"/documents/batch\",\n        response_model=InsertResponse,\n        dependencies=[Depends(optional_api_key)],\n    )\n    async def insert_batch(files: List[UploadFile] = File(...)):\n        try:\n            inserted_count = 0\n            failed_files = []\n\n            for file in files:\n                try:\n                    content = await file.read()\n                    if file.filename.endswith((\".txt\", \".md\")):\n                        text = content.decode(\"utf-8\")\n                        rag.insert(text)\n                        inserted_count += 1\n                    else:\n                        failed_files.append(f\"{file.filename} (unsupported type)\")\n                except Exception as e:\n                    failed_files.append(f\"{file.filename} ({str(e)})\")\n\n            status_message = f\"Successfully inserted {inserted_count} documents\"\n            if failed_files:\n                status_message += f\". Failed files: {', '.join(failed_files)}\"\n\n            return InsertResponse(\n                status=\"success\" if inserted_count > 0 else \"partial_success\",\n                message=status_message,\n                document_count=len(files),\n            )\n        except Exception as e:\n            raise HTTPException(status_code=500, detail=str(e))\n\n    @app.delete(\n        \"/documents\",\n        response_model=InsertResponse,\n        dependencies=[Depends(optional_api_key)],\n    )\n    async def clear_documents():\n        try:\n            rag.text_chunks = []\n            rag.entities_vdb = None\n            rag.relationships_vdb = None\n            return InsertResponse(\n                status=\"success\",\n                message=\"All documents cleared successfully\",\n                document_count=0,\n            )\n        except Exception as e:\n            raise HTTPException(status_code=500, detail=str(e))\n\n    @app.get(\"/health\", dependencies=[Depends(optional_api_key)])\n    async def get_status():\n        \"\"\"Get current system status\"\"\"\n        return {\n            \"status\": \"healthy\",\n            \"working_directory\": str(args.working_dir),\n            \"input_directory\": str(args.input_dir),\n            \"indexed_files\": len(doc_manager.indexed_files),\n            \"configuration\": {\n                \"model\": args.model,\n                \"embedding_model\": args.embedding_model,\n                \"max_tokens\": args.max_tokens,\n                \"embedding_dim\": embedding_dim,\n            },\n        }\n\n    return app\n\n\ndef main():\n    args = parse_args()\n    import uvicorn\n\n    app = create_app(args)\n    uvicorn.run(app, host=args.host, port=args.port)\n\n\nif __name__ == \"__main__\":\n    main()\n"}
{"type": "source_file", "path": "lightrag/api/ollama_lightrag_server.py", "content": "from fastapi import FastAPI, HTTPException, File, UploadFile, Form\nfrom pydantic import BaseModel\nimport logging\nimport argparse\nfrom lightrag import LightRAG, QueryParam\nfrom lightrag.llm import ollama_model_complete, ollama_embed\nfrom lightrag.utils import EmbeddingFunc\nfrom typing import Optional, List\nfrom enum import Enum\nfrom pathlib import Path\nimport shutil\nimport aiofiles\nfrom ascii_colors import trace_exception\nimport os\n\nfrom fastapi import Depends, Security\nfrom fastapi.security import APIKeyHeader\nfrom fastapi.middleware.cors import CORSMiddleware\n\nfrom starlette.status import HTTP_403_FORBIDDEN\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(\n        description=\"LightRAG FastAPI Server with separate working and input directories\"\n    )\n\n    # Server configuration\n    parser.add_argument(\n        \"--host\", default=\"0.0.0.0\", help=\"Server host (default: 0.0.0.0)\"\n    )\n    parser.add_argument(\n        \"--port\", type=int, default=9621, help=\"Server port (default: 9621)\"\n    )\n\n    # Directory configuration\n    parser.add_argument(\n        \"--working-dir\",\n        default=\"./rag_storage\",\n        help=\"Working directory for RAG storage (default: ./rag_storage)\",\n    )\n    parser.add_argument(\n        \"--input-dir\",\n        default=\"./inputs\",\n        help=\"Directory containing input documents (default: ./inputs)\",\n    )\n\n    # Model configuration\n    parser.add_argument(\n        \"--model\",\n        default=\"mistral-nemo:latest\",\n        help=\"LLM model name (default: mistral-nemo:latest)\",\n    )\n    parser.add_argument(\n        \"--embedding-model\",\n        default=\"bge-m3:latest\",\n        help=\"Embedding model name (default: bge-m3:latest)\",\n    )\n    parser.add_argument(\n        \"--ollama-host\",\n        default=\"http://localhost:11434\",\n        help=\"Ollama host URL (default: http://localhost:11434)\",\n    )\n\n    # RAG configuration\n    parser.add_argument(\n        \"--max-async\", type=int, default=4, help=\"Maximum async operations (default: 4)\"\n    )\n    parser.add_argument(\n        \"--max-tokens\",\n        type=int,\n        default=32768,\n        help=\"Maximum token size (default: 32768)\",\n    )\n    parser.add_argument(\n        \"--embedding-dim\",\n        type=int,\n        default=1024,\n        help=\"Embedding dimensions (default: 1024)\",\n    )\n    parser.add_argument(\n        \"--max-embed-tokens\",\n        type=int,\n        default=8192,\n        help=\"Maximum embedding token size (default: 8192)\",\n    )\n\n    # Logging configuration\n    parser.add_argument(\n        \"--log-level\",\n        default=\"INFO\",\n        choices=[\"DEBUG\", \"INFO\", \"WARNING\", \"ERROR\", \"CRITICAL\"],\n        help=\"Logging level (default: INFO)\",\n    )\n    parser.add_argument(\n        \"--key\",\n        type=str,\n        help=\"API key for authentication. This protects lightrag server against unauthorized access\",\n        default=None,\n    )\n\n    return parser.parse_args()\n\n\nclass DocumentManager:\n    \"\"\"Handles document operations and tracking\"\"\"\n\n    def __init__(self, input_dir: str, supported_extensions: tuple = (\".txt\", \".md\")):\n        self.input_dir = Path(input_dir)\n        self.supported_extensions = supported_extensions\n        self.indexed_files = set()\n\n        # Create input directory if it doesn't exist\n        self.input_dir.mkdir(parents=True, exist_ok=True)\n\n    def scan_directory(self) -> List[Path]:\n        \"\"\"Scan input directory for new files\"\"\"\n        new_files = []\n        for ext in self.supported_extensions:\n            for file_path in self.input_dir.rglob(f\"*{ext}\"):\n                if file_path not in self.indexed_files:\n                    new_files.append(file_path)\n        return new_files\n\n    def mark_as_indexed(self, file_path: Path):\n        \"\"\"Mark a file as indexed\"\"\"\n        self.indexed_files.add(file_path)\n\n    def is_supported_file(self, filename: str) -> bool:\n        \"\"\"Check if file type is supported\"\"\"\n        return any(filename.lower().endswith(ext) for ext in self.supported_extensions)\n\n\n# Pydantic models\nclass SearchMode(str, Enum):\n    naive = \"naive\"\n    local = \"local\"\n    global_ = \"global\"\n    hybrid = \"hybrid\"\n\n\nclass QueryRequest(BaseModel):\n    query: str\n    mode: SearchMode = SearchMode.hybrid\n    stream: bool = False\n    only_need_context: bool = False\n\n\nclass QueryResponse(BaseModel):\n    response: str\n\n\nclass InsertTextRequest(BaseModel):\n    text: str\n    description: Optional[str] = None\n\n\nclass InsertResponse(BaseModel):\n    status: str\n    message: str\n    document_count: int\n\n\ndef get_api_key_dependency(api_key: Optional[str]):\n    if not api_key:\n        # If no API key is configured, return a dummy dependency that always succeeds\n        async def no_auth():\n            return None\n\n        return no_auth\n\n    # If API key is configured, use proper authentication\n    api_key_header = APIKeyHeader(name=\"X-API-Key\", auto_error=False)\n\n    async def api_key_auth(api_key_header_value: str | None = Security(api_key_header)):\n        if not api_key_header_value:\n            raise HTTPException(\n                status_code=HTTP_403_FORBIDDEN, detail=\"API Key required\"\n            )\n        if api_key_header_value != api_key:\n            raise HTTPException(\n                status_code=HTTP_403_FORBIDDEN, detail=\"Invalid API Key\"\n            )\n        return api_key_header_value\n\n    return api_key_auth\n\n\ndef create_app(args):\n    # Setup logging\n    logging.basicConfig(\n        format=\"%(levelname)s:%(message)s\", level=getattr(logging, args.log_level)\n    )\n\n    # Check if API key is provided either through env var or args\n    api_key = os.getenv(\"LIGHTRAG_API_KEY\") or args.key\n\n    # Initialize FastAPI\n    app = FastAPI(\n        title=\"LightRAG API\",\n        description=\"API for querying text using LightRAG with separate storage and input directories\"\n        + \"(With authentication)\"\n        if api_key\n        else \"\",\n        version=\"1.0.0\",\n        openapi_tags=[{\"name\": \"api\"}],\n    )\n\n    # Add CORS middleware\n    app.add_middleware(\n        CORSMiddleware,\n        allow_origins=[\"*\"],\n        allow_credentials=True,\n        allow_methods=[\"*\"],\n        allow_headers=[\"*\"],\n    )\n\n    # Create the optional API key dependency\n    optional_api_key = get_api_key_dependency(api_key)\n\n    # Create working directory if it doesn't exist\n    Path(args.working_dir).mkdir(parents=True, exist_ok=True)\n\n    # Initialize document manager\n    doc_manager = DocumentManager(args.input_dir)\n\n    # Initialize RAG\n    rag = LightRAG(\n        working_dir=args.working_dir,\n        llm_model_func=ollama_model_complete,\n        llm_model_name=args.model,\n        llm_model_max_async=args.max_async,\n        llm_model_max_token_size=args.max_tokens,\n        llm_model_kwargs={\n            \"host\": args.ollama_host,\n            \"options\": {\"num_ctx\": args.max_tokens},\n        },\n        embedding_func=EmbeddingFunc(\n            embedding_dim=args.embedding_dim,\n            max_token_size=args.max_embed_tokens,\n            func=lambda texts: ollama_embed(\n                texts, embed_model=args.embedding_model, host=args.ollama_host\n            ),\n        ),\n    )\n\n    @app.on_event(\"startup\")\n    async def startup_event():\n        \"\"\"Index all files in input directory during startup\"\"\"\n        try:\n            new_files = doc_manager.scan_directory()\n            for file_path in new_files:\n                try:\n                    # Use async file reading\n                    async with aiofiles.open(file_path, \"r\", encoding=\"utf-8\") as f:\n                        content = await f.read()\n                        # Use the async version of insert directly\n                        await rag.ainsert(content)\n                        doc_manager.mark_as_indexed(file_path)\n                        logging.info(f\"Indexed file: {file_path}\")\n                except Exception as e:\n                    trace_exception(e)\n                    logging.error(f\"Error indexing file {file_path}: {str(e)}\")\n\n            logging.info(f\"Indexed {len(new_files)} documents from {args.input_dir}\")\n\n        except Exception as e:\n            logging.error(f\"Error during startup indexing: {str(e)}\")\n\n    @app.post(\"/documents/scan\", dependencies=[Depends(optional_api_key)])\n    async def scan_for_new_documents():\n        \"\"\"Manually trigger scanning for new documents\"\"\"\n        try:\n            new_files = doc_manager.scan_directory()\n            indexed_count = 0\n\n            for file_path in new_files:\n                try:\n                    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n                        content = f.read()\n                        await rag.ainsert(content)\n                        doc_manager.mark_as_indexed(file_path)\n                        indexed_count += 1\n                except Exception as e:\n                    logging.error(f\"Error indexing file {file_path}: {str(e)}\")\n\n            return {\n                \"status\": \"success\",\n                \"indexed_count\": indexed_count,\n                \"total_documents\": len(doc_manager.indexed_files),\n            }\n        except Exception as e:\n            raise HTTPException(status_code=500, detail=str(e))\n\n    @app.post(\"/documents/upload\", dependencies=[Depends(optional_api_key)])\n    async def upload_to_input_dir(file: UploadFile = File(...)):\n        \"\"\"Upload a file to the input directory\"\"\"\n        try:\n            if not doc_manager.is_supported_file(file.filename):\n                raise HTTPException(\n                    status_code=400,\n                    detail=f\"Unsupported file type. Supported types: {doc_manager.supported_extensions}\",\n                )\n\n            file_path = doc_manager.input_dir / file.filename\n            with open(file_path, \"wb\") as buffer:\n                shutil.copyfileobj(file.file, buffer)\n\n            # Immediately index the uploaded file\n            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n                content = f.read()\n                await rag.ainsert(content)\n                doc_manager.mark_as_indexed(file_path)\n\n            return {\n                \"status\": \"success\",\n                \"message\": f\"File uploaded and indexed: {file.filename}\",\n                \"total_documents\": len(doc_manager.indexed_files),\n            }\n        except Exception as e:\n            raise HTTPException(status_code=500, detail=str(e))\n\n    @app.post(\n        \"/query\", response_model=QueryResponse, dependencies=[Depends(optional_api_key)]\n    )\n    async def query_text(request: QueryRequest):\n        try:\n            response = await rag.aquery(\n                request.query,\n                param=QueryParam(\n                    mode=request.mode,\n                    stream=request.stream,\n                    only_need_context=request.only_need_context,\n                ),\n            )\n\n            if request.stream:\n                result = \"\"\n                async for chunk in response:\n                    result += chunk\n                return QueryResponse(response=result)\n            else:\n                return QueryResponse(response=response)\n        except Exception as e:\n            raise HTTPException(status_code=500, detail=str(e))\n\n    @app.post(\"/query/stream\", dependencies=[Depends(optional_api_key)])\n    async def query_text_stream(request: QueryRequest):\n        try:\n            response = rag.query(\n                request.query,\n                param=QueryParam(\n                    mode=request.mode,\n                    stream=True,\n                    only_need_context=request.only_need_context,\n                ),\n            )\n\n            async def stream_generator():\n                async for chunk in response:\n                    yield chunk\n\n            return stream_generator()\n        except Exception as e:\n            raise HTTPException(status_code=500, detail=str(e))\n\n    @app.post(\n        \"/documents/text\",\n        response_model=InsertResponse,\n        dependencies=[Depends(optional_api_key)],\n    )\n    async def insert_text(request: InsertTextRequest):\n        try:\n            await rag.ainsert(request.text)\n            return InsertResponse(\n                status=\"success\",\n                message=\"Text successfully inserted\",\n                document_count=len(rag),\n            )\n        except Exception as e:\n            raise HTTPException(status_code=500, detail=str(e))\n\n    @app.post(\n        \"/documents/file\",\n        response_model=InsertResponse,\n        dependencies=[Depends(optional_api_key)],\n    )\n    async def insert_file(file: UploadFile = File(...), description: str = Form(None)):\n        try:\n            content = await file.read()\n\n            if file.filename.endswith((\".txt\", \".md\")):\n                text = content.decode(\"utf-8\")\n                await rag.ainsert(text)\n            else:\n                raise HTTPException(\n                    status_code=400,\n                    detail=\"Unsupported file type. Only .txt and .md files are supported\",\n                )\n\n            return InsertResponse(\n                status=\"success\",\n                message=f\"File '{file.filename}' successfully inserted\",\n                document_count=1,\n            )\n        except UnicodeDecodeError:\n            raise HTTPException(status_code=400, detail=\"File encoding not supported\")\n        except Exception as e:\n            raise HTTPException(status_code=500, detail=str(e))\n\n    @app.post(\n        \"/documents/batch\",\n        response_model=InsertResponse,\n        dependencies=[Depends(optional_api_key)],\n    )\n    async def insert_batch(files: List[UploadFile] = File(...)):\n        try:\n            inserted_count = 0\n            failed_files = []\n\n            for file in files:\n                try:\n                    content = await file.read()\n                    if file.filename.endswith((\".txt\", \".md\")):\n                        text = content.decode(\"utf-8\")\n                        await rag.ainsert(text)\n                        inserted_count += 1\n                    else:\n                        failed_files.append(f\"{file.filename} (unsupported type)\")\n                except Exception as e:\n                    failed_files.append(f\"{file.filename} ({str(e)})\")\n\n            status_message = f\"Successfully inserted {inserted_count} documents\"\n            if failed_files:\n                status_message += f\". Failed files: {', '.join(failed_files)}\"\n\n            return InsertResponse(\n                status=\"success\" if inserted_count > 0 else \"partial_success\",\n                message=status_message,\n                document_count=len(files),\n            )\n        except Exception as e:\n            raise HTTPException(status_code=500, detail=str(e))\n\n    @app.delete(\n        \"/documents\",\n        response_model=InsertResponse,\n        dependencies=[Depends(optional_api_key)],\n    )\n    async def clear_documents():\n        try:\n            rag.text_chunks = []\n            rag.entities_vdb = None\n            rag.relationships_vdb = None\n            return InsertResponse(\n                status=\"success\",\n                message=\"All documents cleared successfully\",\n                document_count=0,\n            )\n        except Exception as e:\n            raise HTTPException(status_code=500, detail=str(e))\n\n    @app.get(\"/health\", dependencies=[Depends(optional_api_key)])\n    async def get_status():\n        \"\"\"Get current system status\"\"\"\n        return {\n            \"status\": \"healthy\",\n            \"working_directory\": str(args.working_dir),\n            \"input_directory\": str(args.input_dir),\n            \"indexed_files\": len(doc_manager.indexed_files),\n            \"configuration\": {\n                \"model\": args.model,\n                \"embedding_model\": args.embedding_model,\n                \"max_tokens\": args.max_tokens,\n                \"ollama_host\": args.ollama_host,\n            },\n        }\n\n    return app\n\n\ndef main():\n    args = parse_args()\n    import uvicorn\n\n    app = create_app(args)\n    uvicorn.run(app, host=args.host, port=args.port)\n\n\nif __name__ == \"__main__\":\n    main()\n"}
{"type": "source_file", "path": "lightrag/kg/milvus_impl.py", "content": "import asyncio\nimport os\nfrom tqdm.asyncio import tqdm as tqdm_async\nfrom dataclasses import dataclass\nimport numpy as np\nfrom lightrag.utils import logger\nfrom ..base import BaseVectorStorage\n\nfrom pymilvus import MilvusClient\n\n\n@dataclass\nclass MilvusVectorDBStorge(BaseVectorStorage):\n    @staticmethod\n    def create_collection_if_not_exist(\n        client: MilvusClient, collection_name: str, **kwargs\n    ):\n        if client.has_collection(collection_name):\n            return\n        client.create_collection(\n            collection_name, max_length=64, id_type=\"string\", **kwargs\n        )\n\n    def __post_init__(self):\n        self._client = MilvusClient(\n            uri=os.environ.get(\n                \"MILVUS_URI\",\n                os.path.join(self.global_config[\"working_dir\"], \"milvus_lite.db\"),\n            ),\n            user=os.environ.get(\"MILVUS_USER\", \"\"),\n            password=os.environ.get(\"MILVUS_PASSWORD\", \"\"),\n            token=os.environ.get(\"MILVUS_TOKEN\", \"\"),\n            db_name=os.environ.get(\"MILVUS_DB_NAME\", \"\"),\n        )\n        self._max_batch_size = self.global_config[\"embedding_batch_num\"]\n        MilvusVectorDBStorge.create_collection_if_not_exist(\n            self._client,\n            self.namespace,\n            dimension=self.embedding_func.embedding_dim,\n        )\n\n    async def upsert(self, data: dict[str, dict]):\n        logger.info(f\"Inserting {len(data)} vectors to {self.namespace}\")\n        if not len(data):\n            logger.warning(\"You insert an empty data to vector DB\")\n            return []\n        list_data = [\n            {\n                \"id\": k,\n                **{k1: v1 for k1, v1 in v.items() if k1 in self.meta_fields},\n            }\n            for k, v in data.items()\n        ]\n        contents = [v[\"content\"] for v in data.values()]\n        batches = [\n            contents[i : i + self._max_batch_size]\n            for i in range(0, len(contents), self._max_batch_size)\n        ]\n\n        async def wrapped_task(batch):\n            result = await self.embedding_func(batch)\n            pbar.update(1)\n            return result\n\n        embedding_tasks = [wrapped_task(batch) for batch in batches]\n        pbar = tqdm_async(\n            total=len(embedding_tasks), desc=\"Generating embeddings\", unit=\"batch\"\n        )\n        embeddings_list = await asyncio.gather(*embedding_tasks)\n\n        embeddings = np.concatenate(embeddings_list)\n        for i, d in enumerate(list_data):\n            d[\"vector\"] = embeddings[i]\n        results = self._client.upsert(collection_name=self.namespace, data=list_data)\n        return results\n\n    async def query(self, query, top_k=5):\n        embedding = await self.embedding_func([query])\n        results = self._client.search(\n            collection_name=self.namespace,\n            data=embedding,\n            limit=top_k,\n            output_fields=list(self.meta_fields),\n            search_params={\"metric_type\": \"COSINE\", \"params\": {\"radius\": 0.2}},\n        )\n        print(results)\n        return [\n            {**dp[\"entity\"], \"id\": dp[\"id\"], \"distance\": dp[\"distance\"]}\n            for dp in results[0]\n        ]\n"}
{"type": "source_file", "path": "lightrag/kg/gremlin_impl.py", "content": "import asyncio\nimport inspect\nimport json\nimport os\nfrom dataclasses import dataclass\nfrom typing import Any, Dict, List, Tuple, Union\n\nfrom gremlin_python.driver import client, serializer\nfrom gremlin_python.driver.aiohttp.transport import AiohttpTransport\nfrom gremlin_python.driver.protocol import GremlinServerError\nfrom tenacity import (\n    retry,\n    retry_if_exception_type,\n    stop_after_attempt,\n    wait_exponential,\n)\n\nfrom lightrag.utils import logger\n\nfrom ..base import BaseGraphStorage\n\n\n@dataclass\nclass GremlinStorage(BaseGraphStorage):\n    @staticmethod\n    def load_nx_graph(file_name):\n        print(\"no preloading of graph with Gremlin in production\")\n\n    def __init__(self, namespace, global_config, embedding_func):\n        super().__init__(\n            namespace=namespace,\n            global_config=global_config,\n            embedding_func=embedding_func,\n        )\n\n        self._driver = None\n        self._driver_lock = asyncio.Lock()\n\n        USER = os.environ.get(\"GREMLIN_USER\", \"\")\n        PASSWORD = os.environ.get(\"GREMLIN_PASSWORD\", \"\")\n        HOST = os.environ[\"GREMLIN_HOST\"]\n        PORT = int(os.environ[\"GREMLIN_PORT\"])\n\n        # TraversalSource, a custom one has to be created manually,\n        # default it \"g\"\n        SOURCE = os.environ.get(\"GREMLIN_TRAVERSE_SOURCE\", \"g\")\n\n        # All vertices will have graph={GRAPH} property, so that we can\n        # have several logical graphs for one source\n        GRAPH = GremlinStorage._to_value_map(os.environ[\"GREMLIN_GRAPH\"])\n\n        self.graph_name = GRAPH\n\n        self._driver = client.Client(\n            f\"ws://{HOST}:{PORT}/gremlin\",\n            SOURCE,\n            username=USER,\n            password=PASSWORD,\n            message_serializer=serializer.GraphSONSerializersV3d0(),\n            transport_factory=lambda: AiohttpTransport(call_from_event_loop=True),\n        )\n\n    def __post_init__(self):\n        self._node_embed_algorithms = {\n            \"node2vec\": self._node2vec_embed,\n        }\n\n    async def close(self):\n        if self._driver:\n            self._driver.close()\n            self._driver = None\n\n    async def __aexit__(self, exc_type, exc, tb):\n        if self._driver:\n            self._driver.close()\n\n    async def index_done_callback(self):\n        print(\"KG successfully indexed.\")\n\n    @staticmethod\n    def _to_value_map(value: Any) -> str:\n        \"\"\"Dump supported Python object as Gremlin valueMap\"\"\"\n        json_str = json.dumps(value, ensure_ascii=False, sort_keys=False)\n        parsed_str = json_str.replace(\"'\", r\"\\'\")\n\n        # walk over the string and replace curly brackets with square brackets\n        # outside of strings, as well as replace double quotes with single quotes\n        # and \"deescape\" double quotes inside of strings\n        outside_str = True\n        escaped = False\n        remove_indices = []\n        for i, c in enumerate(parsed_str):\n            if escaped:\n                # previous character was an \"odd\" backslash\n                escaped = False\n                if c == '\"':\n                    # we want to \"deescape\" double quotes: store indices to delete\n                    remove_indices.insert(0, i - 1)\n            elif c == \"\\\\\":\n                escaped = True\n            elif c == '\"':\n                outside_str = not outside_str\n                parsed_str = parsed_str[:i] + \"'\" + parsed_str[i + 1 :]\n            elif c == \"{\" and outside_str:\n                parsed_str = parsed_str[:i] + \"[\" + parsed_str[i + 1 :]\n            elif c == \"}\" and outside_str:\n                parsed_str = parsed_str[:i] + \"]\" + parsed_str[i + 1 :]\n        for idx in remove_indices:\n            parsed_str = parsed_str[:idx] + parsed_str[idx + 1 :]\n        return parsed_str\n\n    @staticmethod\n    def _convert_properties(properties: Dict[str, Any]) -> str:\n        \"\"\"Create chained .property() commands from properties dict\"\"\"\n        props = []\n        for k, v in properties.items():\n            prop_name = GremlinStorage._to_value_map(k)\n            props.append(f\".property({prop_name}, {GremlinStorage._to_value_map(v)})\")\n        return \"\".join(props)\n\n    @staticmethod\n    def _fix_name(name: str) -> str:\n        \"\"\"Strip double quotes and format as a proper field name\"\"\"\n        name = GremlinStorage._to_value_map(name.strip('\"').replace(r\"\\'\", \"'\"))\n\n        return name\n\n    async def _query(self, query: str) -> List[Dict[str, Any]]:\n        \"\"\"\n        Query the Gremlin graph\n\n        Args:\n            query (str): a query to be executed\n\n        Returns:\n            List[Dict[str, Any]]: a list of dictionaries containing the result set\n        \"\"\"\n\n        result = list(await asyncio.wrap_future(self._driver.submit_async(query)))\n        if result:\n            result = result[0]\n\n        return result\n\n    async def has_node(self, node_id: str) -> bool:\n        entity_name = GremlinStorage._fix_name(node_id)\n\n        query = f\"\"\"g\n                 .V().has('graph', {self.graph_name})\n                 .has('entity_name', {entity_name})\n                 .limit(1)\n                 .count()\n                 .project('has_node')\n                    .by(__.choose(__.is(gt(0)), constant(true), constant(false)))\n                 \"\"\"\n        result = await self._query(query)\n        logger.debug(\n            \"{%s}:query:{%s}:result:{%s}\",\n            inspect.currentframe().f_code.co_name,\n            query,\n            result[0][\"has_node\"],\n        )\n\n        return result[0][\"has_node\"]\n\n    async def has_edge(self, source_node_id: str, target_node_id: str) -> bool:\n        entity_name_source = GremlinStorage._fix_name(source_node_id)\n        entity_name_target = GremlinStorage._fix_name(target_node_id)\n\n        query = f\"\"\"g\n                 .V().has('graph', {self.graph_name})\n                 .has('entity_name', {entity_name_source})\n                 .outE()\n                 .inV().has('graph', {self.graph_name})\n                 .has('entity_name', {entity_name_target})\n                 .limit(1)\n                 .count()\n                 .project('has_edge')\n                    .by(__.choose(__.is(gt(0)), constant(true), constant(false)))\n                 \"\"\"\n        result = await self._query(query)\n        logger.debug(\n            \"{%s}:query:{%s}:result:{%s}\",\n            inspect.currentframe().f_code.co_name,\n            query,\n            result[0][\"has_edge\"],\n        )\n\n        return result[0][\"has_edge\"]\n\n    async def get_node(self, node_id: str) -> Union[dict, None]:\n        entity_name = GremlinStorage._fix_name(node_id)\n        query = f\"\"\"g\n                 .V().has('graph', {self.graph_name})\n                 .has('entity_name', {entity_name})\n                 .limit(1)\n                 .project('properties')\n                    .by(elementMap())\n                 \"\"\"\n        result = await self._query(query)\n        if result:\n            node = result[0]\n            node_dict = node[\"properties\"]\n            logger.debug(\n                \"{%s}: query: {%s}, result: {%s}\",\n                inspect.currentframe().f_code.co_name,\n                query.format,\n                node_dict,\n            )\n            return node_dict\n\n    async def node_degree(self, node_id: str) -> int:\n        entity_name = GremlinStorage._fix_name(node_id)\n        query = f\"\"\"g\n                 .V().has('graph', {self.graph_name})\n                 .has('entity_name', {entity_name})\n                 .outE()\n                 .inV().has('graph', {self.graph_name})\n                 .count()\n                 .project('total_edge_count')\n                    .by()\n                 \"\"\"\n        result = await self._query(query)\n        edge_count = result[0][\"total_edge_count\"]\n\n        logger.debug(\n            \"{%s}:query:{%s}:result:{%s}\",\n            inspect.currentframe().f_code.co_name,\n            query,\n            edge_count,\n        )\n\n        return edge_count\n\n    async def edge_degree(self, src_id: str, tgt_id: str) -> int:\n        src_degree = await self.node_degree(src_id)\n        trg_degree = await self.node_degree(tgt_id)\n\n        # Convert None to 0 for addition\n        src_degree = 0 if src_degree is None else src_degree\n        trg_degree = 0 if trg_degree is None else trg_degree\n\n        degrees = int(src_degree) + int(trg_degree)\n        logger.debug(\n            \"{%s}:query:src_Degree+trg_degree:result:{%s}\",\n            inspect.currentframe().f_code.co_name,\n            degrees,\n        )\n        return degrees\n\n    async def get_edge(\n        self, source_node_id: str, target_node_id: str\n    ) -> Union[dict, None]:\n        \"\"\"\n        Find all edges between nodes of two given names\n\n        Args:\n            source_node_id (str): Name of the source nodes\n            target_node_id (str): Name of the target nodes\n\n        Returns:\n            dict|None: Dict of found edge properties, or None if not found\n        \"\"\"\n        entity_name_source = GremlinStorage._fix_name(source_node_id)\n        entity_name_target = GremlinStorage._fix_name(target_node_id)\n        query = f\"\"\"g\n                 .V().has('graph', {self.graph_name})\n                 .has('entity_name', {entity_name_source})\n                 .outE()\n                 .inV().has('graph', {self.graph_name})\n                 .has('entity_name', {entity_name_target})\n                 .limit(1)\n                 .project('edge_properties')\n                 .by(__.bothE().elementMap())\n                 \"\"\"\n        result = await self._query(query)\n        if result:\n            edge_properties = result[0][\"edge_properties\"]\n            logger.debug(\n                \"{%s}:query:{%s}:result:{%s}\",\n                inspect.currentframe().f_code.co_name,\n                query,\n                edge_properties,\n            )\n            return edge_properties\n\n    async def get_node_edges(self, source_node_id: str) -> List[Tuple[str, str]]:\n        \"\"\"\n        Retrieves all edges (relationships) for a particular node identified by its name.\n        :return: List of tuples containing edge sources and targets\n        \"\"\"\n        node_name = GremlinStorage._fix_name(source_node_id)\n        query = f\"\"\"g\n                 .E()\n                 .filter(\n                     __.or(\n                         __.outV().has('graph', {self.graph_name})\n                           .has('entity_name', {node_name}),\n                         __.inV().has('graph', {self.graph_name})\n                           .has('entity_name', {node_name})\n                     )\n                 )\n                 .project('source_name', 'target_name')\n                 .by(__.outV().values('entity_name'))\n                 .by(__.inV().values('entity_name'))\n                 \"\"\"\n        result = await self._query(query)\n        edges = [(res[\"source_name\"], res[\"target_name\"]) for res in result]\n\n        return edges\n\n    @retry(\n        stop=stop_after_attempt(10),\n        wait=wait_exponential(multiplier=1, min=4, max=10),\n        retry=retry_if_exception_type((GremlinServerError,)),\n    )\n    async def upsert_node(self, node_id: str, node_data: Dict[str, Any]):\n        \"\"\"\n        Upsert a node in the Gremlin graph.\n\n        Args:\n            node_id: The unique identifier for the node (used as name)\n            node_data: Dictionary of node properties\n        \"\"\"\n        name = GremlinStorage._fix_name(node_id)\n        properties = GremlinStorage._convert_properties(node_data)\n\n        query = f\"\"\"g\n                 .V().has('graph', {self.graph_name})\n                 .has('entity_name', {name})\n                 .fold()\n                 .coalesce(\n                     __.unfold(),\n                     __.addV('ENTITY')\n                         .property('graph', {self.graph_name})\n                         .property('entity_name', {name})\n                 )\n                 {properties}\n                 \"\"\"\n\n        try:\n            await self._query(query)\n            logger.debug(\n                \"Upserted node with name {%s} and properties: {%s}\",\n                name,\n                properties,\n            )\n        except Exception as e:\n            logger.error(\"Error during upsert: {%s}\", e)\n            raise\n\n    @retry(\n        stop=stop_after_attempt(10),\n        wait=wait_exponential(multiplier=1, min=4, max=10),\n        retry=retry_if_exception_type((GremlinServerError,)),\n    )\n    async def upsert_edge(\n        self, source_node_id: str, target_node_id: str, edge_data: Dict[str, Any]\n    ):\n        \"\"\"\n        Upsert an edge and its properties between two nodes identified by their names.\n\n        Args:\n            source_node_id (str): Name of the source node (used as identifier)\n            target_node_id (str): Name of the target node (used as identifier)\n            edge_data (dict): Dictionary of properties to set on the edge\n        \"\"\"\n        source_node_name = GremlinStorage._fix_name(source_node_id)\n        target_node_name = GremlinStorage._fix_name(target_node_id)\n        edge_properties = GremlinStorage._convert_properties(edge_data)\n\n        query = f\"\"\"g\n                 .V().has('graph', {self.graph_name})\n                 .has('entity_name', {source_node_name}).as('source')\n                 .V().has('graph', {self.graph_name})\n                 .has('entity_name', {target_node_name}).as('target')\n                 .coalesce(\n                      __.select('source').outE('DIRECTED').where(__.inV().as('target')),\n                      __.select('source').addE('DIRECTED').to(__.select('target'))\n                  )\n                  .property('graph', {self.graph_name})\n                 {edge_properties}\n                 \"\"\"\n        try:\n            await self._query(query)\n            logger.debug(\n                \"Upserted edge from {%s} to {%s} with properties: {%s}\",\n                source_node_name,\n                target_node_name,\n                edge_properties,\n            )\n        except Exception as e:\n            logger.error(\"Error during edge upsert: {%s}\", e)\n            raise\n\n    async def _node2vec_embed(self):\n        print(\"Implemented but never called.\")\n"}
{"type": "source_file", "path": "lightrag/api/openai_lightrag_server.py", "content": "from fastapi import FastAPI, HTTPException, File, UploadFile, Form\nfrom pydantic import BaseModel\nimport asyncio\nimport logging\nimport argparse\nfrom lightrag import LightRAG, QueryParam\nfrom lightrag.llm import openai_complete_if_cache, openai_embedding\nfrom lightrag.utils import EmbeddingFunc\nfrom typing import Optional, List\nfrom enum import Enum\nfrom pathlib import Path\nimport shutil\nimport aiofiles\nfrom ascii_colors import trace_exception\nimport nest_asyncio\n\nimport os\n\nfrom fastapi import Depends, Security\nfrom fastapi.security import APIKeyHeader\nfrom fastapi.middleware.cors import CORSMiddleware\n\nfrom starlette.status import HTTP_403_FORBIDDEN\n\n# Apply nest_asyncio to solve event loop issues\nnest_asyncio.apply()\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(\n        description=\"LightRAG FastAPI Server with OpenAI integration\"\n    )\n\n    # Server configuration\n    parser.add_argument(\n        \"--host\", default=\"0.0.0.0\", help=\"Server host (default: 0.0.0.0)\"\n    )\n    parser.add_argument(\n        \"--port\", type=int, default=9621, help=\"Server port (default: 9621)\"\n    )\n\n    # Directory configuration\n    parser.add_argument(\n        \"--working-dir\",\n        default=\"./rag_storage\",\n        help=\"Working directory for RAG storage (default: ./rag_storage)\",\n    )\n    parser.add_argument(\n        \"--input-dir\",\n        default=\"./inputs\",\n        help=\"Directory containing input documents (default: ./inputs)\",\n    )\n\n    # Model configuration\n    parser.add_argument(\n        \"--model\", default=\"gpt-4\", help=\"OpenAI model name (default: gpt-4)\"\n    )\n    parser.add_argument(\n        \"--embedding-model\",\n        default=\"text-embedding-3-large\",\n        help=\"OpenAI embedding model (default: text-embedding-3-large)\",\n    )\n\n    # RAG configuration\n    parser.add_argument(\n        \"--max-tokens\",\n        type=int,\n        default=32768,\n        help=\"Maximum token size (default: 32768)\",\n    )\n    parser.add_argument(\n        \"--max-embed-tokens\",\n        type=int,\n        default=8192,\n        help=\"Maximum embedding token size (default: 8192)\",\n    )\n\n    # Logging configuration\n    parser.add_argument(\n        \"--log-level\",\n        default=\"INFO\",\n        choices=[\"DEBUG\", \"INFO\", \"WARNING\", \"ERROR\", \"CRITICAL\"],\n        help=\"Logging level (default: INFO)\",\n    )\n\n    parser.add_argument(\n        \"--key\",\n        type=str,\n        help=\"API key for authentication. This protects lightrag server against unauthorized access\",\n        default=None,\n    )\n\n    return parser.parse_args()\n\n\nclass DocumentManager:\n    \"\"\"Handles document operations and tracking\"\"\"\n\n    def __init__(self, input_dir: str, supported_extensions: tuple = (\".txt\", \".md\")):\n        self.input_dir = Path(input_dir)\n        self.supported_extensions = supported_extensions\n        self.indexed_files = set()\n\n        # Create input directory if it doesn't exist\n        self.input_dir.mkdir(parents=True, exist_ok=True)\n\n    def scan_directory(self) -> List[Path]:\n        \"\"\"Scan input directory for new files\"\"\"\n        new_files = []\n        for ext in self.supported_extensions:\n            for file_path in self.input_dir.rglob(f\"*{ext}\"):\n                if file_path not in self.indexed_files:\n                    new_files.append(file_path)\n        return new_files\n\n    def mark_as_indexed(self, file_path: Path):\n        \"\"\"Mark a file as indexed\"\"\"\n        self.indexed_files.add(file_path)\n\n    def is_supported_file(self, filename: str) -> bool:\n        \"\"\"Check if file type is supported\"\"\"\n        return any(filename.lower().endswith(ext) for ext in self.supported_extensions)\n\n\n# Pydantic models\nclass SearchMode(str, Enum):\n    naive = \"naive\"\n    local = \"local\"\n    global_ = \"global\"\n    hybrid = \"hybrid\"\n\n\nclass QueryRequest(BaseModel):\n    query: str\n    mode: SearchMode = SearchMode.hybrid\n    stream: bool = False\n    only_need_context: bool = False\n\n\nclass QueryResponse(BaseModel):\n    response: str\n\n\nclass InsertTextRequest(BaseModel):\n    text: str\n    description: Optional[str] = None\n\n\nclass InsertResponse(BaseModel):\n    status: str\n    message: str\n    document_count: int\n\n\ndef get_api_key_dependency(api_key: Optional[str]):\n    if not api_key:\n        # If no API key is configured, return a dummy dependency that always succeeds\n        async def no_auth():\n            return None\n\n        return no_auth\n\n    # If API key is configured, use proper authentication\n    api_key_header = APIKeyHeader(name=\"X-API-Key\", auto_error=False)\n\n    async def api_key_auth(api_key_header_value: str | None = Security(api_key_header)):\n        if not api_key_header_value:\n            raise HTTPException(\n                status_code=HTTP_403_FORBIDDEN, detail=\"API Key required\"\n            )\n        if api_key_header_value != api_key:\n            raise HTTPException(\n                status_code=HTTP_403_FORBIDDEN, detail=\"Invalid API Key\"\n            )\n        return api_key_header_value\n\n    return api_key_auth\n\n\nasync def get_embedding_dim(embedding_model: str) -> int:\n    \"\"\"Get embedding dimensions for the specified model\"\"\"\n    test_text = [\"This is a test sentence.\"]\n    embedding = await openai_embedding(test_text, model=embedding_model)\n    return embedding.shape[1]\n\n\ndef create_app(args):\n    # Setup logging\n    logging.basicConfig(\n        format=\"%(levelname)s:%(message)s\", level=getattr(logging, args.log_level)\n    )\n\n    # Check if API key is provided either through env var or args\n    api_key = os.getenv(\"LIGHTRAG_API_KEY\") or args.key\n\n    # Initialize FastAPI\n    app = FastAPI(\n        title=\"LightRAG API\",\n        description=\"API for querying text using LightRAG with separate storage and input directories\"\n        + \"(With authentication)\"\n        if api_key\n        else \"\",\n        version=\"1.0.0\",\n        openapi_tags=[{\"name\": \"api\"}],\n    )\n\n    # Add CORS middleware\n    app.add_middleware(\n        CORSMiddleware,\n        allow_origins=[\"*\"],\n        allow_credentials=True,\n        allow_methods=[\"*\"],\n        allow_headers=[\"*\"],\n    )\n\n    # Create the optional API key dependency\n    optional_api_key = get_api_key_dependency(api_key)\n\n    # Add CORS middleware\n    app.add_middleware(\n        CORSMiddleware,\n        allow_origins=[\"*\"],\n        allow_credentials=True,\n        allow_methods=[\"*\"],\n        allow_headers=[\"*\"],\n    )\n\n    # Create working directory if it doesn't exist\n    Path(args.working_dir).mkdir(parents=True, exist_ok=True)\n\n    # Initialize document manager\n    doc_manager = DocumentManager(args.input_dir)\n\n    # Get embedding dimensions\n    embedding_dim = asyncio.run(get_embedding_dim(args.embedding_model))\n\n    async def async_openai_complete(\n        prompt, system_prompt=None, history_messages=[], **kwargs\n    ):\n        \"\"\"Async wrapper for OpenAI completion\"\"\"\n        return await openai_complete_if_cache(\n            args.model,\n            prompt,\n            system_prompt=system_prompt,\n            history_messages=history_messages,\n            **kwargs,\n        )\n\n    # Initialize RAG with OpenAI configuration\n    rag = LightRAG(\n        working_dir=args.working_dir,\n        llm_model_func=async_openai_complete,\n        llm_model_name=args.model,\n        llm_model_max_token_size=args.max_tokens,\n        embedding_func=EmbeddingFunc(\n            embedding_dim=embedding_dim,\n            max_token_size=args.max_embed_tokens,\n            func=lambda texts: openai_embedding(texts, model=args.embedding_model),\n        ),\n    )\n\n    @app.on_event(\"startup\")\n    async def startup_event():\n        \"\"\"Index all files in input directory during startup\"\"\"\n        try:\n            new_files = doc_manager.scan_directory()\n            for file_path in new_files:\n                try:\n                    # Use async file reading\n                    async with aiofiles.open(file_path, \"r\", encoding=\"utf-8\") as f:\n                        content = await f.read()\n                        # Use the async version of insert directly\n                        await rag.ainsert(content)\n                        doc_manager.mark_as_indexed(file_path)\n                        logging.info(f\"Indexed file: {file_path}\")\n                except Exception as e:\n                    trace_exception(e)\n                    logging.error(f\"Error indexing file {file_path}: {str(e)}\")\n\n            logging.info(f\"Indexed {len(new_files)} documents from {args.input_dir}\")\n\n        except Exception as e:\n            logging.error(f\"Error during startup indexing: {str(e)}\")\n\n    @app.post(\"/documents/scan\", dependencies=[Depends(optional_api_key)])\n    async def scan_for_new_documents():\n        \"\"\"Manually trigger scanning for new documents\"\"\"\n        try:\n            new_files = doc_manager.scan_directory()\n            indexed_count = 0\n\n            for file_path in new_files:\n                try:\n                    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n                        content = f.read()\n                        rag.insert(content)\n                        doc_manager.mark_as_indexed(file_path)\n                        indexed_count += 1\n                except Exception as e:\n                    logging.error(f\"Error indexing file {file_path}: {str(e)}\")\n\n            return {\n                \"status\": \"success\",\n                \"indexed_count\": indexed_count,\n                \"total_documents\": len(doc_manager.indexed_files),\n            }\n        except Exception as e:\n            raise HTTPException(status_code=500, detail=str(e))\n\n    @app.post(\"/documents/upload\", dependencies=[Depends(optional_api_key)])\n    async def upload_to_input_dir(file: UploadFile = File(...)):\n        \"\"\"Upload a file to the input directory\"\"\"\n        try:\n            if not doc_manager.is_supported_file(file.filename):\n                raise HTTPException(\n                    status_code=400,\n                    detail=f\"Unsupported file type. Supported types: {doc_manager.supported_extensions}\",\n                )\n\n            file_path = doc_manager.input_dir / file.filename\n            with open(file_path, \"wb\") as buffer:\n                shutil.copyfileobj(file.file, buffer)\n\n            # Immediately index the uploaded file\n            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n                content = f.read()\n                rag.insert(content)\n                doc_manager.mark_as_indexed(file_path)\n\n            return {\n                \"status\": \"success\",\n                \"message\": f\"File uploaded and indexed: {file.filename}\",\n                \"total_documents\": len(doc_manager.indexed_files),\n            }\n        except Exception as e:\n            raise HTTPException(status_code=500, detail=str(e))\n\n    @app.post(\n        \"/query\", response_model=QueryResponse, dependencies=[Depends(optional_api_key)]\n    )\n    async def query_text(request: QueryRequest):\n        try:\n            response = await rag.aquery(\n                request.query,\n                param=QueryParam(\n                    mode=request.mode,\n                    stream=request.stream,\n                    only_need_context=request.only_need_context,\n                ),\n            )\n\n            if request.stream:\n                result = \"\"\n                async for chunk in response:\n                    result += chunk\n                return QueryResponse(response=result)\n            else:\n                return QueryResponse(response=response)\n        except Exception as e:\n            raise HTTPException(status_code=500, detail=str(e))\n\n    @app.post(\"/query/stream\", dependencies=[Depends(optional_api_key)])\n    async def query_text_stream(request: QueryRequest):\n        try:\n            response = rag.query(\n                request.query,\n                param=QueryParam(\n                    mode=request.mode,\n                    stream=True,\n                    only_need_context=request.only_need_context,\n                ),\n            )\n\n            async def stream_generator():\n                async for chunk in response:\n                    yield chunk\n\n            return stream_generator()\n        except Exception as e:\n            raise HTTPException(status_code=500, detail=str(e))\n\n    @app.post(\n        \"/documents/text\",\n        response_model=InsertResponse,\n        dependencies=[Depends(optional_api_key)],\n    )\n    async def insert_text(request: InsertTextRequest):\n        try:\n            rag.insert(request.text)\n            return InsertResponse(\n                status=\"success\",\n                message=\"Text successfully inserted\",\n                document_count=len(rag),\n            )\n        except Exception as e:\n            raise HTTPException(status_code=500, detail=str(e))\n\n    @app.post(\n        \"/documents/file\",\n        response_model=InsertResponse,\n        dependencies=[Depends(optional_api_key)],\n    )\n    async def insert_file(file: UploadFile = File(...), description: str = Form(None)):\n        try:\n            content = await file.read()\n\n            if file.filename.endswith((\".txt\", \".md\")):\n                text = content.decode(\"utf-8\")\n                rag.insert(text)\n            else:\n                raise HTTPException(\n                    status_code=400,\n                    detail=\"Unsupported file type. Only .txt and .md files are supported\",\n                )\n\n            return InsertResponse(\n                status=\"success\",\n                message=f\"File '{file.filename}' successfully inserted\",\n                document_count=1,\n            )\n        except UnicodeDecodeError:\n            raise HTTPException(status_code=400, detail=\"File encoding not supported\")\n        except Exception as e:\n            raise HTTPException(status_code=500, detail=str(e))\n\n    @app.post(\n        \"/documents/batch\",\n        response_model=InsertResponse,\n        dependencies=[Depends(optional_api_key)],\n    )\n    async def insert_batch(files: List[UploadFile] = File(...)):\n        try:\n            inserted_count = 0\n            failed_files = []\n\n            for file in files:\n                try:\n                    content = await file.read()\n                    if file.filename.endswith((\".txt\", \".md\")):\n                        text = content.decode(\"utf-8\")\n                        rag.insert(text)\n                        inserted_count += 1\n                    else:\n                        failed_files.append(f\"{file.filename} (unsupported type)\")\n                except Exception as e:\n                    failed_files.append(f\"{file.filename} ({str(e)})\")\n\n            status_message = f\"Successfully inserted {inserted_count} documents\"\n            if failed_files:\n                status_message += f\". Failed files: {', '.join(failed_files)}\"\n\n            return InsertResponse(\n                status=\"success\" if inserted_count > 0 else \"partial_success\",\n                message=status_message,\n                document_count=len(files),\n            )\n        except Exception as e:\n            raise HTTPException(status_code=500, detail=str(e))\n\n    @app.delete(\n        \"/documents\",\n        response_model=InsertResponse,\n        dependencies=[Depends(optional_api_key)],\n    )\n    async def clear_documents():\n        try:\n            rag.text_chunks = []\n            rag.entities_vdb = None\n            rag.relationships_vdb = None\n            return InsertResponse(\n                status=\"success\",\n                message=\"All documents cleared successfully\",\n                document_count=0,\n            )\n        except Exception as e:\n            raise HTTPException(status_code=500, detail=str(e))\n\n    @app.get(\"/health\", dependencies=[Depends(optional_api_key)])\n    async def get_status():\n        \"\"\"Get current system status\"\"\"\n        return {\n            \"status\": \"healthy\",\n            \"working_directory\": str(args.working_dir),\n            \"input_directory\": str(args.input_dir),\n            \"indexed_files\": len(doc_manager.indexed_files),\n            \"configuration\": {\n                \"model\": args.model,\n                \"embedding_model\": args.embedding_model,\n                \"max_tokens\": args.max_tokens,\n                \"embedding_dim\": embedding_dim,\n            },\n        }\n\n    return app\n\n\ndef main():\n    args = parse_args()\n    import uvicorn\n\n    app = create_app(args)\n    uvicorn.run(app, host=args.host, port=args.port)\n\n\nif __name__ == \"__main__\":\n    main()\n"}
{"type": "source_file", "path": "lightrag/kg/age_impl.py", "content": "import asyncio\nimport inspect\nimport json\nimport os\nimport sys\nfrom contextlib import asynccontextmanager\nfrom dataclasses import dataclass\nfrom typing import Any, Dict, List, NamedTuple, Optional, Tuple, Union\n\nimport psycopg\nfrom psycopg.rows import namedtuple_row\nfrom psycopg_pool import AsyncConnectionPool, PoolTimeout\nfrom tenacity import (\n    retry,\n    retry_if_exception_type,\n    stop_after_attempt,\n    wait_exponential,\n)\n\nfrom lightrag.utils import logger\n\nfrom ..base import BaseGraphStorage\n\nif sys.platform.startswith(\"win\"):\n    import asyncio.windows_events\n\n    asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())\n\n\nclass AGEQueryException(Exception):\n    \"\"\"Exception for the AGE queries.\"\"\"\n\n    def __init__(self, exception: Union[str, Dict]) -> None:\n        if isinstance(exception, dict):\n            self.message = exception[\"message\"] if \"message\" in exception else \"unknown\"\n            self.details = exception[\"details\"] if \"details\" in exception else \"unknown\"\n        else:\n            self.message = exception\n            self.details = \"unknown\"\n\n    def get_message(self) -> str:\n        return self.message\n\n    def get_details(self) -> Any:\n        return self.details\n\n\n@dataclass\nclass AGEStorage(BaseGraphStorage):\n    @staticmethod\n    def load_nx_graph(file_name):\n        print(\"no preloading of graph with AGE in production\")\n\n    def __init__(self, namespace, global_config, embedding_func):\n        super().__init__(\n            namespace=namespace,\n            global_config=global_config,\n            embedding_func=embedding_func,\n        )\n        self._driver = None\n        self._driver_lock = asyncio.Lock()\n        DB = os.environ[\"AGE_POSTGRES_DB\"].replace(\"\\\\\", \"\\\\\\\\\").replace(\"'\", \"\\\\'\")\n        USER = os.environ[\"AGE_POSTGRES_USER\"].replace(\"\\\\\", \"\\\\\\\\\").replace(\"'\", \"\\\\'\")\n        PASSWORD = (\n            os.environ[\"AGE_POSTGRES_PASSWORD\"]\n            .replace(\"\\\\\", \"\\\\\\\\\")\n            .replace(\"'\", \"\\\\'\")\n        )\n        HOST = os.environ[\"AGE_POSTGRES_HOST\"].replace(\"\\\\\", \"\\\\\\\\\").replace(\"'\", \"\\\\'\")\n        PORT = int(os.environ[\"AGE_POSTGRES_PORT\"])\n        self.graph_name = os.environ[\"AGE_GRAPH_NAME\"]\n\n        connection_string = f\"dbname='{DB}' user='{USER}' password='{PASSWORD}' host='{HOST}' port={PORT}\"\n\n        self._driver = AsyncConnectionPool(connection_string, open=False)\n\n        return None\n\n    def __post_init__(self):\n        self._node_embed_algorithms = {\n            \"node2vec\": self._node2vec_embed,\n        }\n\n    async def close(self):\n        if self._driver:\n            await self._driver.close()\n            self._driver = None\n\n    async def __aexit__(self, exc_type, exc, tb):\n        if self._driver:\n            await self._driver.close()\n\n    async def index_done_callback(self):\n        print(\"KG successfully indexed.\")\n\n    @staticmethod\n    def _record_to_dict(record: NamedTuple) -> Dict[str, Any]:\n        \"\"\"\n        Convert a record returned from an age query to a dictionary\n\n        Args:\n            record (): a record from an age query result\n\n        Returns:\n            Dict[str, Any]: a dictionary representation of the record where\n                the dictionary key is the field name and the value is the\n                value converted to a python type\n        \"\"\"\n        # result holder\n        d = {}\n\n        # prebuild a mapping of vertex_id to vertex mappings to be used\n        # later to build edges\n        vertices = {}\n        for k in record._fields:\n            v = getattr(record, k)\n            # agtype comes back '{key: value}::type' which must be parsed\n            if isinstance(v, str) and \"::\" in v:\n                dtype = v.split(\"::\")[-1]\n                v = v.split(\"::\")[0]\n                if dtype == \"vertex\":\n                    vertex = json.loads(v)\n                    vertices[vertex[\"id\"]] = vertex.get(\"properties\")\n\n        # iterate returned fields and parse appropriately\n        for k in record._fields:\n            v = getattr(record, k)\n            if isinstance(v, str) and \"::\" in v:\n                dtype = v.split(\"::\")[-1]\n                v = v.split(\"::\")[0]\n            else:\n                dtype = \"\"\n\n            if dtype == \"vertex\":\n                vertex = json.loads(v)\n                field = json.loads(v).get(\"properties\")\n                if not field:\n                    field = {}\n                field[\"label\"] = AGEStorage._decode_graph_label(vertex[\"label\"])\n                d[k] = field\n            # convert edge from id-label->id by replacing id with node information\n            # we only do this if the vertex was also returned in the query\n            # this is an attempt to be consistent with neo4j implementation\n            elif dtype == \"edge\":\n                edge = json.loads(v)\n                d[k] = (\n                    vertices.get(edge[\"start_id\"], {}),\n                    edge[\n                        \"label\"\n                    ],  # we don't use decode_graph_label(), since edge label is always \"DIRECTED\"\n                    vertices.get(edge[\"end_id\"], {}),\n                )\n            else:\n                d[k] = json.loads(v) if isinstance(v, str) else v\n\n        return d\n\n    @staticmethod\n    def _format_properties(\n        properties: Dict[str, Any], _id: Union[str, None] = None\n    ) -> str:\n        \"\"\"\n        Convert a dictionary of properties to a string representation that\n        can be used in a cypher query insert/merge statement.\n\n        Args:\n            properties (Dict[str,str]): a dictionary containing node/edge properties\n            id (Union[str, None]): the id of the node or None if none exists\n\n        Returns:\n            str: the properties dictionary as a properly formatted string\n        \"\"\"\n        props = []\n        # wrap property key in backticks to escape\n        for k, v in properties.items():\n            prop = f\"`{k}`: {json.dumps(v)}\"\n            props.append(prop)\n        if _id is not None and \"id\" not in properties:\n            props.append(\n                f\"id: {json.dumps(_id)}\" if isinstance(_id, str) else f\"id: {_id}\"\n            )\n        return \"{\" + \", \".join(props) + \"}\"\n\n    @staticmethod\n    def _encode_graph_label(label: str) -> str:\n        \"\"\"\n        Since AGE suports only alphanumerical labels, we will encode generic label as HEX string\n\n        Args:\n            label (str): the original label\n\n        Returns:\n            str: the encoded label\n        \"\"\"\n        return \"x\" + label.encode().hex()\n\n    @staticmethod\n    def _decode_graph_label(encoded_label: str) -> str:\n        \"\"\"\n        Since AGE suports only alphanumerical labels, we will encode generic label as HEX string\n\n        Args:\n            encoded_label (str): the encoded label\n\n        Returns:\n            str: the decoded label\n        \"\"\"\n        return bytes.fromhex(encoded_label.removeprefix(\"x\")).decode()\n\n    @staticmethod\n    def _get_col_name(field: str, idx: int) -> str:\n        \"\"\"\n        Convert a cypher return field to a pgsql select field\n        If possible keep the cypher column name, but create a generic name if necessary\n\n        Args:\n            field (str): a return field from a cypher query to be formatted for pgsql\n            idx (int): the position of the field in the return statement\n\n        Returns:\n            str: the field to be used in the pgsql select statement\n        \"\"\"\n        # remove white space\n        field = field.strip()\n        # if an alias is provided for the field, use it\n        if \" as \" in field:\n            return field.split(\" as \")[-1].strip()\n        # if the return value is an unnamed primitive, give it a generic name\n        if field.isnumeric() or field in (\"true\", \"false\", \"null\"):\n            return f\"column_{idx}\"\n        # otherwise return the value stripping out some common special chars\n        return field.replace(\"(\", \"_\").replace(\")\", \"\")\n\n    @staticmethod\n    def _wrap_query(query: str, graph_name: str, **params: str) -> str:\n        \"\"\"\n        Convert a cypher query to an Apache Age compatible\n        sql query by wrapping the cypher query in ag_catalog.cypher,\n        casting results to agtype and building a select statement\n\n        Args:\n            query (str): a valid cypher query\n            graph_name (str): the name of the graph to query\n            params (dict): parameters for the query\n\n        Returns:\n            str: an equivalent pgsql query\n        \"\"\"\n\n        # pgsql template\n        template = \"\"\"SELECT {projection} FROM ag_catalog.cypher('{graph_name}', $$\n            {query}\n        $$) AS ({fields});\"\"\"\n\n        # if there are any returned fields they must be added to the pgsql query\n        if \"return\" in query.lower():\n            # parse return statement to identify returned fields\n            fields = (\n                query.lower()\n                .split(\"return\")[-1]\n                .split(\"distinct\")[-1]\n                .split(\"order by\")[0]\n                .split(\"skip\")[0]\n                .split(\"limit\")[0]\n                .split(\",\")\n            )\n\n            # raise exception if RETURN * is found as we can't resolve the fields\n            if \"*\" in [x.strip() for x in fields]:\n                raise ValueError(\n                    \"AGE graph does not support 'RETURN *'\"\n                    + \" statements in Cypher queries\"\n                )\n\n            # get pgsql formatted field names\n            fields = [\n                AGEStorage._get_col_name(field, idx) for idx, field in enumerate(fields)\n            ]\n\n            # build resulting pgsql relation\n            fields_str = \", \".join(\n                [field.split(\".\")[-1] + \" agtype\" for field in fields]\n            )\n\n        # if no return statement we still need to return a single field of type agtype\n        else:\n            fields_str = \"a agtype\"\n\n        select_str = \"*\"\n\n        return template.format(\n            graph_name=graph_name,\n            query=query.format(**params),\n            fields=fields_str,\n            projection=select_str,\n        )\n\n    async def _query(self, query: str, **params: str) -> List[Dict[str, Any]]:\n        \"\"\"\n        Query the graph by taking a cypher query, converting it to an\n        age compatible query, executing it and converting the result\n\n        Args:\n            query (str): a cypher query to be executed\n            params (dict): parameters for the query\n\n        Returns:\n            List[Dict[str, Any]]: a list of dictionaries containing the result set\n        \"\"\"\n        # convert cypher query to pgsql/age query\n        wrapped_query = self._wrap_query(query, self.graph_name, **params)\n\n        await self._driver.open()\n\n        # create graph if it doesn't exist\n        async with self._get_pool_connection() as conn:\n            async with conn.cursor() as curs:\n                try:\n                    await curs.execute('SET search_path = ag_catalog, \"$user\", public')\n                    await curs.execute(f\"SELECT create_graph('{self.graph_name}')\")\n                    await conn.commit()\n                except (\n                    psycopg.errors.InvalidSchemaName,\n                    psycopg.errors.UniqueViolation,\n                ):\n                    await conn.rollback()\n\n        # execute the query, rolling back on an error\n        async with self._get_pool_connection() as conn:\n            async with conn.cursor(row_factory=namedtuple_row) as curs:\n                try:\n                    await curs.execute('SET search_path = ag_catalog, \"$user\", public')\n                    await curs.execute(wrapped_query)\n                    await conn.commit()\n                except psycopg.Error as e:\n                    await conn.rollback()\n                    raise AGEQueryException(\n                        {\n                            \"message\": f\"Error executing graph query: {query.format(**params)}\",\n                            \"detail\": str(e),\n                        }\n                    ) from e\n\n                data = await curs.fetchall()\n                if data is None:\n                    result = []\n                # decode records\n                else:\n                    result = [AGEStorage._record_to_dict(d) for d in data]\n\n                return result\n\n    async def has_node(self, node_id: str) -> bool:\n        entity_name_label = node_id.strip('\"')\n\n        query = \"\"\"\n                MATCH (n:`{label}`) RETURN count(n) > 0 AS node_exists\n                \"\"\"\n        params = {\"label\": AGEStorage._encode_graph_label(entity_name_label)}\n        single_result = (await self._query(query, **params))[0]\n        logger.debug(\n            \"{%s}:query:{%s}:result:{%s}\",\n            inspect.currentframe().f_code.co_name,\n            query.format(**params),\n            single_result[\"node_exists\"],\n        )\n\n        return single_result[\"node_exists\"]\n\n    async def has_edge(self, source_node_id: str, target_node_id: str) -> bool:\n        entity_name_label_source = source_node_id.strip('\"')\n        entity_name_label_target = target_node_id.strip('\"')\n\n        query = \"\"\"\n                MATCH (a:`{src_label}`)-[r]-(b:`{tgt_label}`)\n                RETURN COUNT(r) > 0 AS edge_exists\n                \"\"\"\n        params = {\n            \"src_label\": AGEStorage._encode_graph_label(entity_name_label_source),\n            \"tgt_label\": AGEStorage._encode_graph_label(entity_name_label_target),\n        }\n        single_result = (await self._query(query, **params))[0]\n        logger.debug(\n            \"{%s}:query:{%s}:result:{%s}\",\n            inspect.currentframe().f_code.co_name,\n            query.format(**params),\n            single_result[\"edge_exists\"],\n        )\n        return single_result[\"edge_exists\"]\n\n    async def get_node(self, node_id: str) -> Union[dict, None]:\n        entity_name_label = node_id.strip('\"')\n        query = \"\"\"\n                MATCH (n:`{label}`) RETURN n\n                \"\"\"\n        params = {\"label\": AGEStorage._encode_graph_label(entity_name_label)}\n        record = await self._query(query, **params)\n        if record:\n            node = record[0]\n            node_dict = node[\"n\"]\n            logger.debug(\n                \"{%s}: query: {%s}, result: {%s}\",\n                inspect.currentframe().f_code.co_name,\n                query.format(**params),\n                node_dict,\n            )\n            return node_dict\n        return None\n\n    async def node_degree(self, node_id: str) -> int:\n        entity_name_label = node_id.strip('\"')\n\n        query = \"\"\"\n                MATCH (n:`{label}`)-[]->(x)\n                RETURN count(x) AS total_edge_count\n                \"\"\"\n        params = {\"label\": AGEStorage._encode_graph_label(entity_name_label)}\n        record = (await self._query(query, **params))[0]\n        if record:\n            edge_count = int(record[\"total_edge_count\"])\n            logger.debug(\n                \"{%s}:query:{%s}:result:{%s}\",\n                inspect.currentframe().f_code.co_name,\n                query.format(**params),\n                edge_count,\n            )\n            return edge_count\n\n    async def edge_degree(self, src_id: str, tgt_id: str) -> int:\n        entity_name_label_source = src_id.strip('\"')\n        entity_name_label_target = tgt_id.strip('\"')\n        src_degree = await self.node_degree(entity_name_label_source)\n        trg_degree = await self.node_degree(entity_name_label_target)\n\n        # Convert None to 0 for addition\n        src_degree = 0 if src_degree is None else src_degree\n        trg_degree = 0 if trg_degree is None else trg_degree\n\n        degrees = int(src_degree) + int(trg_degree)\n        logger.debug(\n            \"{%s}:query:src_Degree+trg_degree:result:{%s}\",\n            inspect.currentframe().f_code.co_name,\n            degrees,\n        )\n        return degrees\n\n    async def get_edge(\n        self, source_node_id: str, target_node_id: str\n    ) -> Union[dict, None]:\n        \"\"\"\n        Find all edges between nodes of two given labels\n\n        Args:\n            source_node_label (str): Label of the source nodes\n            target_node_label (str): Label of the target nodes\n\n        Returns:\n            list: List of all relationships/edges found\n        \"\"\"\n        entity_name_label_source = source_node_id.strip('\"')\n        entity_name_label_target = target_node_id.strip('\"')\n\n        query = \"\"\"\n                MATCH (a:`{src_label}`)-[r]->(b:`{tgt_label}`)\n                RETURN properties(r) as edge_properties\n                LIMIT 1\n                \"\"\"\n        params = {\n            \"src_label\": AGEStorage._encode_graph_label(entity_name_label_source),\n            \"tgt_label\": AGEStorage._encode_graph_label(entity_name_label_target),\n        }\n        record = await self._query(query, **params)\n        if record and record[0] and record[0][\"edge_properties\"]:\n            result = record[0][\"edge_properties\"]\n            logger.debug(\n                \"{%s}:query:{%s}:result:{%s}\",\n                inspect.currentframe().f_code.co_name,\n                query.format(**params),\n                result,\n            )\n            return result\n\n    async def get_node_edges(self, source_node_id: str) -> List[Tuple[str, str]]:\n        \"\"\"\n        Retrieves all edges (relationships) for a particular node identified by its label.\n        :return: List of dictionaries containing edge information\n        \"\"\"\n        node_label = source_node_id.strip('\"')\n\n        query = \"\"\"\n                MATCH (n:`{label}`)\n                OPTIONAL MATCH (n)-[r]-(connected)\n                RETURN n, r, connected\n                \"\"\"\n        params = {\"label\": AGEStorage._encode_graph_label(node_label)}\n        results = await self._query(query, **params)\n        edges = []\n        for record in results:\n            source_node = record[\"n\"] if record[\"n\"] else None\n            connected_node = record[\"connected\"] if record[\"connected\"] else None\n\n            source_label = (\n                source_node[\"label\"] if source_node and source_node[\"label\"] else None\n            )\n            target_label = (\n                connected_node[\"label\"]\n                if connected_node and connected_node[\"label\"]\n                else None\n            )\n\n            if source_label and target_label:\n                edges.append((source_label, target_label))\n\n        return edges\n\n    @retry(\n        stop=stop_after_attempt(3),\n        wait=wait_exponential(multiplier=1, min=4, max=10),\n        retry=retry_if_exception_type((AGEQueryException,)),\n    )\n    async def upsert_node(self, node_id: str, node_data: Dict[str, Any]):\n        \"\"\"\n        Upsert a node in the AGE database.\n\n        Args:\n            node_id: The unique identifier for the node (used as label)\n            node_data: Dictionary of node properties\n        \"\"\"\n        label = node_id.strip('\"')\n        properties = node_data\n\n        query = \"\"\"\n                MERGE (n:`{label}`)\n                SET n += {properties}\n                \"\"\"\n        params = {\n            \"label\": AGEStorage._encode_graph_label(label),\n            \"properties\": AGEStorage._format_properties(properties),\n        }\n        try:\n            await self._query(query, **params)\n            logger.debug(\n                \"Upserted node with label '{%s}' and properties: {%s}\",\n                label,\n                properties,\n            )\n        except Exception as e:\n            logger.error(\"Error during upsert: {%s}\", e)\n            raise\n\n    @retry(\n        stop=stop_after_attempt(3),\n        wait=wait_exponential(multiplier=1, min=4, max=10),\n        retry=retry_if_exception_type((AGEQueryException,)),\n    )\n    async def upsert_edge(\n        self, source_node_id: str, target_node_id: str, edge_data: Dict[str, Any]\n    ):\n        \"\"\"\n        Upsert an edge and its properties between two nodes identified by their labels.\n\n        Args:\n            source_node_id (str): Label of the source node (used as identifier)\n            target_node_id (str): Label of the target node (used as identifier)\n            edge_data (dict): Dictionary of properties to set on the edge\n        \"\"\"\n        source_node_label = source_node_id.strip('\"')\n        target_node_label = target_node_id.strip('\"')\n        edge_properties = edge_data\n\n        query = \"\"\"\n                MATCH (source:`{src_label}`)\n                WITH source\n                MATCH (target:`{tgt_label}`)\n                MERGE (source)-[r:DIRECTED]->(target)\n                SET r += {properties}\n                RETURN r\n                \"\"\"\n        params = {\n            \"src_label\": AGEStorage._encode_graph_label(source_node_label),\n            \"tgt_label\": AGEStorage._encode_graph_label(target_node_label),\n            \"properties\": AGEStorage._format_properties(edge_properties),\n        }\n        try:\n            await self._query(query, **params)\n            logger.debug(\n                \"Upserted edge from '{%s}' to '{%s}' with properties: {%s}\",\n                source_node_label,\n                target_node_label,\n                edge_properties,\n            )\n        except Exception as e:\n            logger.error(\"Error during edge upsert: {%s}\", e)\n            raise\n\n    async def _node2vec_embed(self):\n        print(\"Implemented but never called.\")\n\n    @asynccontextmanager\n    async def _get_pool_connection(self, timeout: Optional[float] = None):\n        \"\"\"Workaround for a psycopg_pool bug\"\"\"\n\n        try:\n            connection = await self._driver.getconn(timeout=timeout)\n        except PoolTimeout:\n            await self._driver._add_connection(None)  # workaround...\n            connection = await self._driver.getconn(timeout=timeout)\n\n        try:\n            async with connection:\n                yield connection\n        finally:\n            await self._driver.putconn(connection)\n"}
{"type": "source_file", "path": "lightrag/kg/__init__.py", "content": "# print (\"init package vars here. ......\")\n"}
{"type": "source_file", "path": "lightrag/base.py", "content": "from dataclasses import dataclass, field\nfrom typing import TypedDict, Union, Literal, Generic, TypeVar, Optional, Dict, Any\nfrom enum import Enum\n\nimport numpy as np\n\nfrom .utils import EmbeddingFunc\n\nTextChunkSchema = TypedDict(\n    \"TextChunkSchema\",\n    {\"tokens\": int, \"content\": str, \"full_doc_id\": str, \"chunk_order_index\": int},\n)\n\nT = TypeVar(\"T\")\n\n\n@dataclass\nclass QueryParam:\n    mode: Literal[\"local\", \"global\", \"hybrid\", \"naive\", \"mix\"] = \"global\"\n    only_need_context: bool = False\n    only_need_prompt: bool = False\n    response_type: str = \"Multiple Paragraphs\"\n    stream: bool = False\n    # Number of top-k items to retrieve; corresponds to entities in \"local\" mode and relationships in \"global\" mode.\n    top_k: int = 60\n    # Number of document chunks to retrieve.\n    # top_n: int = 10\n    # Number of tokens for the original chunks.\n    max_token_for_text_unit: int = 4000\n    # Number of tokens for the relationship descriptions\n    max_token_for_global_context: int = 4000\n    # Number of tokens for the entity descriptions\n    max_token_for_local_context: int = 4000\n\n\n@dataclass\nclass StorageNameSpace:\n    namespace: str\n    global_config: dict\n\n    async def index_done_callback(self):\n        \"\"\"commit the storage operations after indexing\"\"\"\n        pass\n\n    async def query_done_callback(self):\n        \"\"\"commit the storage operations after querying\"\"\"\n        pass\n\n\n@dataclass\nclass BaseVectorStorage(StorageNameSpace):\n    embedding_func: EmbeddingFunc\n    meta_fields: set = field(default_factory=set)\n\n    async def query(self, query: str, top_k: int) -> list[dict]:\n        raise NotImplementedError\n\n    async def upsert(self, data: dict[str, dict]):\n        \"\"\"Use 'content' field from value for embedding, use key as id.\n        If embedding_func is None, use 'embedding' field from value\n        \"\"\"\n        raise NotImplementedError\n\n\n@dataclass\nclass BaseKVStorage(Generic[T], StorageNameSpace):\n    embedding_func: EmbeddingFunc\n\n    async def all_keys(self) -> list[str]:\n        raise NotImplementedError\n\n    async def get_by_id(self, id: str) -> Union[T, None]:\n        raise NotImplementedError\n\n    async def get_by_ids(\n        self, ids: list[str], fields: Union[set[str], None] = None\n    ) -> list[Union[T, None]]:\n        raise NotImplementedError\n\n    async def filter_keys(self, data: list[str]) -> set[str]:\n        \"\"\"return un-exist keys\"\"\"\n        raise NotImplementedError\n\n    async def upsert(self, data: dict[str, T]):\n        raise NotImplementedError\n\n    async def drop(self):\n        raise NotImplementedError\n\n\n@dataclass\nclass BaseGraphStorage(StorageNameSpace):\n    embedding_func: EmbeddingFunc = None\n\n    async def has_node(self, node_id: str) -> bool:\n        raise NotImplementedError\n\n    async def has_edge(self, source_node_id: str, target_node_id: str) -> bool:\n        raise NotImplementedError\n\n    async def node_degree(self, node_id: str) -> int:\n        raise NotImplementedError\n\n    async def edge_degree(self, src_id: str, tgt_id: str) -> int:\n        raise NotImplementedError\n\n    async def get_node(self, node_id: str) -> Union[dict, None]:\n        raise NotImplementedError\n\n    async def get_edge(\n        self, source_node_id: str, target_node_id: str\n    ) -> Union[dict, None]:\n        raise NotImplementedError\n\n    async def get_node_edges(\n        self, source_node_id: str\n    ) -> Union[list[tuple[str, str]], None]:\n        raise NotImplementedError\n\n    async def upsert_node(self, node_id: str, node_data: dict[str, str]):\n        raise NotImplementedError\n\n    async def upsert_edge(\n        self, source_node_id: str, target_node_id: str, edge_data: dict[str, str]\n    ):\n        raise NotImplementedError\n\n    async def delete_node(self, node_id: str):\n        raise NotImplementedError\n\n    async def embed_nodes(self, algorithm: str) -> tuple[np.ndarray, list[str]]:\n        raise NotImplementedError(\"Node embedding is not used in lightrag.\")\n\n\nclass DocStatus(str, Enum):\n    \"\"\"Document processing status enum\"\"\"\n\n    PENDING = \"pending\"\n    PROCESSING = \"processing\"\n    PROCESSED = \"processed\"\n    FAILED = \"failed\"\n\n\n@dataclass\nclass DocProcessingStatus:\n    \"\"\"Document processing status data structure\"\"\"\n\n    content_summary: str  # First 100 chars of document content\n    content_length: int  # Total length of document\n    status: DocStatus  # Current processing status\n    created_at: str  # ISO format timestamp\n    updated_at: str  # ISO format timestamp\n    chunks_count: Optional[int] = None  # Number of chunks after splitting\n    error: Optional[str] = None  # Error message if failed\n    metadata: Dict[str, Any] = field(default_factory=dict)  # Additional metadata\n\n\nclass DocStatusStorage(BaseKVStorage):\n    \"\"\"Base class for document status storage\"\"\"\n\n    async def get_status_counts(self) -> Dict[str, int]:\n        \"\"\"Get counts of documents in each status\"\"\"\n        raise NotImplementedError\n\n    async def get_failed_docs(self) -> Dict[str, DocProcessingStatus]:\n        \"\"\"Get all failed documents\"\"\"\n        raise NotImplementedError\n\n    async def get_pending_docs(self) -> Dict[str, DocProcessingStatus]:\n        \"\"\"Get all pending documents\"\"\"\n        raise NotImplementedError\n"}
{"type": "source_file", "path": "lightrag/kg/mongo_impl.py", "content": "import os\nfrom tqdm.asyncio import tqdm as tqdm_async\nfrom dataclasses import dataclass\nfrom pymongo import MongoClient\n\nfrom lightrag.utils import logger\n\nfrom lightrag.base import BaseKVStorage\n\n\n@dataclass\nclass MongoKVStorage(BaseKVStorage):\n    def __post_init__(self):\n        client = MongoClient(\n            os.environ.get(\"MONGO_URI\", \"mongodb://root:root@localhost:27017/\")\n        )\n        database = client.get_database(os.environ.get(\"MONGO_DATABASE\", \"LightRAG\"))\n        self._data = database.get_collection(self.namespace)\n        logger.info(f\"Use MongoDB as KV {self.namespace}\")\n\n    async def all_keys(self) -> list[str]:\n        return [x[\"_id\"] for x in self._data.find({}, {\"_id\": 1})]\n\n    async def get_by_id(self, id):\n        return self._data.find_one({\"_id\": id})\n\n    async def get_by_ids(self, ids, fields=None):\n        if fields is None:\n            return list(self._data.find({\"_id\": {\"$in\": ids}}))\n        return list(\n            self._data.find(\n                {\"_id\": {\"$in\": ids}},\n                {field: 1 for field in fields},\n            )\n        )\n\n    async def filter_keys(self, data: list[str]) -> set[str]:\n        existing_ids = [\n            str(x[\"_id\"]) for x in self._data.find({\"_id\": {\"$in\": data}}, {\"_id\": 1})\n        ]\n        return set([s for s in data if s not in existing_ids])\n\n    async def upsert(self, data: dict[str, dict]):\n        for k, v in tqdm_async(data.items(), desc=\"Upserting\"):\n            self._data.update_one({\"_id\": k}, {\"$set\": v}, upsert=True)\n            data[k][\"_id\"] = k\n        return data\n\n    async def drop(self):\n        \"\"\" \"\"\"\n        pass\n"}
{"type": "source_file", "path": "lightrag/kg/chroma_impl.py", "content": "import asyncio\nfrom dataclasses import dataclass\nfrom typing import Union\nimport numpy as np\nfrom chromadb import HttpClient\nfrom chromadb.config import Settings\nfrom lightrag.base import BaseVectorStorage\nfrom lightrag.utils import logger\n\n\n@dataclass\nclass ChromaVectorDBStorage(BaseVectorStorage):\n    \"\"\"ChromaDB vector storage implementation.\"\"\"\n\n    cosine_better_than_threshold: float = 0.2\n\n    def __post_init__(self):\n        try:\n            # Use global config value if specified, otherwise use default\n            self.cosine_better_than_threshold = self.global_config.get(\n                \"cosine_better_than_threshold\", self.cosine_better_than_threshold\n            )\n\n            config = self.global_config.get(\"vector_db_storage_cls_kwargs\", {})\n            user_collection_settings = config.get(\"collection_settings\", {})\n            # Default HNSW index settings for ChromaDB\n            default_collection_settings = {\n                # Distance metric used for similarity search (cosine similarity)\n                \"hnsw:space\": \"cosine\",\n                # Number of nearest neighbors to explore during index construction\n                # Higher values = better recall but slower indexing\n                \"hnsw:construction_ef\": 128,\n                # Number of nearest neighbors to explore during search\n                # Higher values = better recall but slower search\n                \"hnsw:search_ef\": 128,\n                # Number of connections per node in the HNSW graph\n                # Higher values = better recall but more memory usage\n                \"hnsw:M\": 16,\n                # Number of vectors to process in one batch during indexing\n                \"hnsw:batch_size\": 100,\n                # Number of updates before forcing index synchronization\n                # Lower values = more frequent syncs but slower indexing\n                \"hnsw:sync_threshold\": 1000,\n            }\n            collection_settings = {\n                **default_collection_settings,\n                **user_collection_settings,\n            }\n\n            auth_provider = config.get(\n                \"auth_provider\", \"chromadb.auth.token_authn.TokenAuthClientProvider\"\n            )\n            auth_credentials = config.get(\"auth_token\", \"secret-token\")\n            headers = {}\n\n            if \"token_authn\" in auth_provider:\n                headers = {\n                    config.get(\"auth_header_name\", \"X-Chroma-Token\"): auth_credentials\n                }\n            elif \"basic_authn\" in auth_provider:\n                auth_credentials = config.get(\"auth_credentials\", \"admin:admin\")\n\n            self._client = HttpClient(\n                host=config.get(\"host\", \"localhost\"),\n                port=config.get(\"port\", 8000),\n                headers=headers,\n                settings=Settings(\n                    chroma_api_impl=\"rest\",\n                    chroma_client_auth_provider=auth_provider,\n                    chroma_client_auth_credentials=auth_credentials,\n                    allow_reset=True,\n                    anonymized_telemetry=False,\n                ),\n            )\n\n            self._collection = self._client.get_or_create_collection(\n                name=self.namespace,\n                metadata={\n                    **collection_settings,\n                    \"dimension\": self.embedding_func.embedding_dim,\n                },\n            )\n            # Use batch size from collection settings if specified\n            self._max_batch_size = self.global_config.get(\n                \"embedding_batch_num\", collection_settings.get(\"hnsw:batch_size\", 32)\n            )\n        except Exception as e:\n            logger.error(f\"ChromaDB initialization failed: {str(e)}\")\n            raise\n\n    async def upsert(self, data: dict[str, dict]):\n        if not data:\n            logger.warning(\"Empty data provided to vector DB\")\n            return []\n\n        try:\n            ids = list(data.keys())\n            documents = [v[\"content\"] for v in data.values()]\n            metadatas = [\n                {k: v for k, v in item.items() if k in self.meta_fields}\n                or {\"_default\": \"true\"}\n                for item in data.values()\n            ]\n\n            # Process in batches\n            batches = [\n                documents[i : i + self._max_batch_size]\n                for i in range(0, len(documents), self._max_batch_size)\n            ]\n\n            embedding_tasks = [self.embedding_func(batch) for batch in batches]\n            embeddings_list = []\n\n            # Pre-allocate embeddings_list with known size\n            embeddings_list = [None] * len(embedding_tasks)\n\n            # Use asyncio.gather instead of as_completed if order doesn't matter\n            embeddings_results = await asyncio.gather(*embedding_tasks)\n            embeddings_list = list(embeddings_results)\n\n            embeddings = np.concatenate(embeddings_list)\n\n            # Upsert in batches\n            for i in range(0, len(ids), self._max_batch_size):\n                batch_slice = slice(i, i + self._max_batch_size)\n\n                self._collection.upsert(\n                    ids=ids[batch_slice],\n                    embeddings=embeddings[batch_slice].tolist(),\n                    documents=documents[batch_slice],\n                    metadatas=metadatas[batch_slice],\n                )\n\n            return ids\n\n        except Exception as e:\n            logger.error(f\"Error during ChromaDB upsert: {str(e)}\")\n            raise\n\n    async def query(self, query: str, top_k=5) -> Union[dict, list[dict]]:\n        try:\n            embedding = await self.embedding_func([query])\n\n            results = self._collection.query(\n                query_embeddings=embedding.tolist(),\n                n_results=top_k * 2,  # Request more results to allow for filtering\n                include=[\"metadatas\", \"distances\", \"documents\"],\n            )\n\n            # Filter results by cosine similarity threshold and take top k\n            # We request 2x results initially to have enough after filtering\n            # ChromaDB returns cosine similarity (1 = identical, 0 = orthogonal)\n            # We convert to distance (0 = identical, 1 = orthogonal) via (1 - similarity)\n            # Only keep results with distance below threshold, then take top k\n            return [\n                {\n                    \"id\": results[\"ids\"][0][i],\n                    \"distance\": 1 - results[\"distances\"][0][i],\n                    \"content\": results[\"documents\"][0][i],\n                    **results[\"metadatas\"][0][i],\n                }\n                for i in range(len(results[\"ids\"][0]))\n                if (1 - results[\"distances\"][0][i]) >= self.cosine_better_than_threshold\n            ][:top_k]\n\n        except Exception as e:\n            logger.error(f\"Error during ChromaDB query: {str(e)}\")\n            raise\n\n    async def index_done_callback(self):\n        # ChromaDB handles persistence automatically\n        pass\n"}
