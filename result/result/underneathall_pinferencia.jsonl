{"repo_info": {"repo_name": "pinferencia", "repo_owner": "underneathall", "repo_url": "https://github.com/underneathall/pinferencia"}}
{"type": "test_file", "path": "examples/pytorch/mnist/test_sum.py", "content": "import requests\n\nresponse = requests.post(\n    url=\"http://localhost:8000/v1/models/mnist/predict\",\n    json={\n        \"data\": [\n            \"/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAcABwBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APn+uhfwXqy2Ph25VYnPiB3SzhUkPlXCfNkAAEsCCCeOeKx9RsLjStUu9Ou1C3NpM8Eqg5AdSVIz35FVqK9xl0HXhb/C20sdMubjTLMQXs11AhkRXmmDsCwzgAYPpz+XI/GrSLrTfiVqNzPapbw3xE8AWQNvUAKXOOmWVjg+teeUV2fgXxd4hsPE2hWEGuX8Vh9uhja3Fw3lbGcBhtzjGCad8XI7iL4p68twHDGcMm45+QqCuPbBFcVRRU97fXepXb3d9dT3VzJjfNPIXdsAAZY8nAAH4VBX/9k=\",  # noqa\n            \"/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAcABwBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APn+tix8KeINT0+O/sNGvbq1kl8lJIYS4Z/QY+lJrvhfWvDLWq61p8lm91H5sSSEbiuccgHKn2ODWRRXe/Cu/wDENz400fQ9N1m+tbN7oTTQxzOIyi/O+VBxyFI/GsHxr4im8U+MNS1aVmKzTERKSTsjHCgfhisCivW/g/r9zpGgeLpokgAstPe4hkEAaYTNhVw2M7cjoeOa8nmmkuJ5J5naSWRi7uxyWYnJJPrTKK1ND8R6t4buZ7jR717WWeFoJGVVO5G6jkH0HPUVl0V//9k=\",  # noqa\n        ]\n    },\n)\nprint(\"Prediction:\", response.json()[\"data\"])\n"}
{"type": "test_file", "path": "tests/__init__.py", "content": ""}
{"type": "test_file", "path": "tests/api_tests/__init__.py", "content": ""}
{"type": "test_file", "path": "tests/api_tests/app.py", "content": "from pinferencia import Server\n\n\ndef predict(data: str) -> str:\n    return data\n\n\nservice = Server()\nservice.register(model_name=\"test\", model=predict)\nservice.register(model_name=\"test\", model=predict, version_name=\"v1\")\n"}
{"type": "test_file", "path": "tests/api_tests/conftest.py", "content": "import shlex\nimport time\nfrom subprocess import Popen\n\nimport pytest\nimport requests\n\n\n@pytest.fixture(scope=\"session\")\ndef backend_port():\n    return 9999\n\n\n@pytest.fixture(autouse=True, scope=\"session\")\ndef backend(backend_port):\n    args = shlex.split(f\"uvicorn --port {backend_port} tests.api_tests.app:service\")\n    p = Popen(args)\n    for _ in range(60):\n        try:\n            requests.get(f\"http://127.0.0.1:{backend_port}\")\n        except Exception:\n            time.sleep(1)\n    yield\n    p.kill()\n"}
{"type": "test_file", "path": "tests/api_tests/test_list.py", "content": "import requests\n\n\ndef test(backend_port):\n    response = requests.get(f\"http://127.0.0.1:{backend_port}/v1/models\")\n    models = response.json()\n    assert len(models) == 1\n    assert len(models[0][\"versions\"]) == 2\n"}
{"type": "test_file", "path": "tests/api_tests/test_predict.py", "content": "import requests\n\n\ndef test(backend_port):\n    response = requests.post(\n        f\"http://127.0.0.1:{backend_port}/v1/models/test/predict\",\n        json={\"data\": \"abc\"},\n    )\n    prediction = response.json()\n    assert prediction[\"data\"] == \"abc\"\n"}
{"type": "test_file", "path": "tests/conftest.py", "content": "import pytest\nimport numpy as np\n\n\n@pytest.fixture(scope=\"session\")\ndef json_model_data():\n    return [\n        {\"request_data\": \"a\", \"response_data\": 1},\n        {\"request_data\": \"b\", \"response_data\": 2},\n        {\"request_data\": \"c\", \"response_data\": 3},\n        {\"request_data\": \"d\", \"response_data\": 0},\n    ]\n\n\n@pytest.fixture(scope=\"session\")\ndef json_model():\n    from .models.json_model.model import model\n\n    return model\n\n\n@pytest.fixture(scope=\"session\")\ndef json_model_path():\n    from .models.json_model.model import model_path\n\n    return model_path\n\n\n@pytest.fixture(scope=\"session\")\ndef json_model_dir():\n    from .models.json_model.model import model_dir\n\n    return model_dir\n\n\n@pytest.fixture(scope=\"session\")\ndef json_model_handler():\n    from .models.json_model.handler import JsonHandler\n\n    return JsonHandler\n\n\n@pytest.fixture(scope=\"session\")\ndef sum_product_model():\n    from .models.sum_product_model import model\n\n    return model\n\n\n@pytest.fixture\ndef sum_product_model_metadata():\n    from .models.sum_product_model import metadata\n\n    return metadata\n\n\n@pytest.fixture\ndef image_base64_string():\n    return \"iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAIAAAD9b0jDAAADEUlEQVR4nK1WzUrrQBQ+M9NJ0qRJCg1WiBu7cKeCSxGRbrp051bQrQu34gv4BvoEPkhfoNCFKELBhQhaa2z6l99zF+cyNyDW9t77LcJM5uSb78z5mQDMBWMMAKSUmqYBgK7rmqZxzgFAPaWU80n+wDRNGpTLZRqsrKyoVSGEMhBClEqlRXkVHMehgeu6QgjDMIp+GIZBqn+GEAIApJRKhfqSuGgVACqVyt8IBADLsopTwzCU1+p8FgJjLAzDIAiU2N3d3dPTUyJVZrquL8pomqYQYjgc9no9APA8DwAeHx9fXl5s21ZmpVKJMaYO5GdcXFwg4uvrK023t7eHwyEi0pRzrhwvCp8HIUStVkPE2Wy2s7Pjuq7rur1eL03Tk5MTstF13bIsKITu937fkSLi+/v729ubpmm+739+foZhmOc557zVakkpLcuKomg8HsPiCSCEOD4+TpLk/v6eQsEY63Q6cRx/fHwodY7jfK2ob5VmWVav10ul0vX1NdWoaZqu6zLGOOeGYdBOURR9jdK88trf359MJp1OJwzDarUaBAGdwM3NzWw2I2+iKCLVKoDzlFYqlUajYZpmu90GgCAIbNu2bVvTtDRNm82m53m6rlNKFRnnodVqIWK/37+8vDw4ONjY2PA87+7uDhGjKMrz/PDwkCyllEs0qsFgkCRJFEWIiIjdbhcR0zTNsgwR9/b2AKBcLi9RUQBwdHTU7XaJNAxDok7TtN1ub21tkQ3FbYmKAoB6ve77/tra2vr6+vn5+dPTUxzHV1dX1LGURmppP0Pls1Lh+/7DwwMinp2d0Rsqp6/4Nvqj0QgAOOeqxT0/P0+nUwCQUlarVcaYSqxFSQHANE06xDRNydNarZYkCWNsPB4jYpZlS5NOJhNKQCklJfl0OhVCrK6uJkkCAORBHMeLNhTHcVRvVtSj0Yhz3mw26dbL87xosCioAZODhmHc3t7GcYyIm5ubtKUQgnO+6MX3HQaDQb/fV1P1P/BPpADQaDQAwHEc9WPxf1CMOKVdcXXpfXzfh8KlpGmaEIJyS+EXXIc7GO0j1jYAAAAASUVORK5CYII=\"  # noqa\n\n\n@pytest.fixture\ndef image_np_ndarray():\n    return np.array([[1, 2], [3, 4]])\n"}
{"type": "test_file", "path": "tests/e2e_tests/conftest.py", "content": "import base64\nimport time\nfrom io import BytesIO\nfrom multiprocessing import Process\n\nimport pytest\nimport requests\n\nfrom pinferencia.main import file_content, start_backend, start_frontend\n\n\n@pytest.fixture(scope=\"session\")\ndef frontend_kwargs():\n    return {\n        \"server.port\": 9917,\n        \"server.address\": \"127.0.0.1\",\n    }\n\n\n@pytest.fixture(scope=\"session\")\ndef backend_kwargs():\n    return {\n        \"app_dir\": \".\",\n        \"host\": \"127.0.0.1\",\n        \"port\": 9910,\n    }\n\n\n@pytest.fixture(scope=\"session\")\ndef frontend_addr(frontend_kwargs):\n    addr = frontend_kwargs[\"server.address\"]\n    port = frontend_kwargs[\"server.port\"]\n    return f\"http://{addr}:{port}\"\n\n\n@pytest.fixture\ndef page(frontend_addr):\n    from playwright.sync_api import sync_playwright\n\n    launch_args = [\n        \"--use-fake-ui-for-media-stream\",\n        \"--use-fake-device-for-media-stream\",\n    ]\n\n    with sync_playwright() as p:\n        # visit frontend\n        browser = p.chromium.launch(args=launch_args)\n        page = browser.new_page(permissions=[\"camera\"])\n        page.goto(frontend_addr)\n\n        yield page\n\n        browser.close()\n\n\n@pytest.fixture(autouse=True, scope=\"session\")\ndef frontend(frontend_addr, frontend_kwargs, backend_kwargs):\n    backend_address = f'http://{backend_kwargs[\"host\"]}:{backend_kwargs[\"port\"]}'\n    # start frontend\n    frontend_proc = Process(\n        target=start_frontend,\n        args=[file_content.format(backend_address=backend_address)],\n        kwargs=frontend_kwargs,\n    )\n    frontend_proc.start()\n    for _ in range(60):\n        try:\n            requests.get(frontend_addr)\n        except Exception:\n            time.sleep(1)\n    yield\n\n    frontend_proc.terminate()\n\n\n@pytest.fixture(autouse=True, scope=\"session\")\ndef backend(backend_kwargs):\n    app = \"tests.e2e_tests.demo_app:service\"\n    backend_proc = Process(target=start_backend, args=[app], kwargs=backend_kwargs)\n    backend_proc.start()\n    yield\n\n    backend_proc.terminate()\n\n\n@pytest.fixture\ndef image_base64_string():\n    return \"iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAIAAAD9b0jDAAADEUlEQVR4nK1WzUrrQBQ+M9NJ0qRJCg1WiBu7cKeCSxGRbrp051bQrQu34gv4BvoEPkhfoNCFKELBhQhaa2z6l99zF+cyNyDW9t77LcJM5uSb78z5mQDMBWMMAKSUmqYBgK7rmqZxzgFAPaWU80n+wDRNGpTLZRqsrKyoVSGEMhBClEqlRXkVHMehgeu6QgjDMIp+GIZBqn+GEAIApJRKhfqSuGgVACqVyt8IBADLsopTwzCU1+p8FgJjLAzDIAiU2N3d3dPTUyJVZrquL8pomqYQYjgc9no9APA8DwAeHx9fXl5s21ZmpVKJMaYO5GdcXFwg4uvrK023t7eHwyEi0pRzrhwvCp8HIUStVkPE2Wy2s7Pjuq7rur1eL03Tk5MTstF13bIsKITu937fkSLi+/v729ubpmm+739+foZhmOc557zVakkpLcuKomg8HsPiCSCEOD4+TpLk/v6eQsEY63Q6cRx/fHwodY7jfK2ob5VmWVav10ul0vX1NdWoaZqu6zLGOOeGYdBOURR9jdK88trf359MJp1OJwzDarUaBAGdwM3NzWw2I2+iKCLVKoDzlFYqlUajYZpmu90GgCAIbNu2bVvTtDRNm82m53m6rlNKFRnnodVqIWK/37+8vDw4ONjY2PA87+7uDhGjKMrz/PDwkCyllEs0qsFgkCRJFEWIiIjdbhcR0zTNsgwR9/b2AKBcLi9RUQBwdHTU7XaJNAxDok7TtN1ub21tkQ3FbYmKAoB6ve77/tra2vr6+vn5+dPTUxzHV1dX1LGURmppP0Pls1Lh+/7DwwMinp2d0Rsqp6/4Nvqj0QgAOOeqxT0/P0+nUwCQUlarVcaYSqxFSQHANE06xDRNydNarZYkCWNsPB4jYpZlS5NOJhNKQCklJfl0OhVCrK6uJkkCAORBHMeLNhTHcVRvVtSj0Yhz3mw26dbL87xosCioAZODhmHc3t7GcYyIm5ubtKUQgnO+6MX3HQaDQb/fV1P1P/BPpADQaDQAwHEc9WPxf1CMOKVdcXXpfXzfh8KlpGmaEIJyS+EXXIc7GO0j1jYAAAAASUVORK5CYII=\"  # noqa\n\n\n@pytest.fixture\ndef image_byte(image_base64_string):\n    return BytesIO(base64.b64decode(image_base64_string))\n"}
{"type": "test_file", "path": "tests/e2e_tests/demo_app.py", "content": "from typing import List\n\nfrom pinferencia import Server, task\n\n\ndef return_text(data: List[str]) -> List[str]:\n    return [\"abcdefg\"]\n\n\ndef return_json(data: list) -> List[dict]:\n    return [{\"a\": 1, \"b\": 2}]\n\n\ndef return_table(data: List[str]) -> List[List[dict]]:\n    return [[{\"a\": 1, \"b\": 2}, {\"a\": 3, \"b\": 4}]]\n\n\ndef return_invalid_table(data):\n    return [[{\"a\": 1, \"b\": 2}, {\"a\": 3, \"b\": 4}, 1, 2]]\n\n\ndef return_image(data: List[str]):\n    return [\n        \"/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAcABwBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APn+uhfwXqy2Ph25VYnPiB3SzhUkPlXCfNkAAEsCCCeOeKx9RsLjStUu9Ou1C3NpM8Eqg5AdSVIz35FVqK9xl0HXhb/C20sdMubjTLMQXs11AhkRXmmDsCwzgAYPpz+XI/GrSLrTfiVqNzPapbw3xE8AWQNvUAKXOOmWVjg+teeUV2fgXxd4hsPE2hWEGuX8Vh9uhja3Fw3lbGcBhtzjGCad8XI7iL4p68twHDGcMm45+QqCuPbBFcVRRU97fXepXb3d9dT3VzJjfNPIXdsAAZY8nAAH4VBX/9k=\"  # noqa\n    ]\n\n\ndef raise_error(data: str):\n    raise Exception(\"Error\")\n\n\nservice = Server()\nservice.register(\n    model_name=\"invalid-task-model\",\n    model=return_text,\n    metadata={\"task\": \"invalid\"},\n)\nservice.register(\n    model_name=\"return-text-model\",\n    model=return_text,\n    version_name=\"v1\",\n    metadata={\"task\": task.TEXT_TO_TEXT},\n)\nservice.register(\n    model_name=\"return-image-model\",\n    model=return_image,\n    version_name=\"v1\",\n    metadata={\"task\": task.TEXT_TO_IMAGE},\n)\nservice.register(\n    model_name=\"return-json-model\",\n    model=return_json,\n    version_name=\"v1\",\n    metadata={\"task\": task.TEXT_TO_TEXT},\n)\nservice.register(\n    model_name=\"return-table-model\",\n    model=return_table,\n    version_name=\"v1\",\n    metadata={\"task\": task.TEXT_TO_TEXT},\n)\nservice.register(\n    model_name=\"return-invalid-table-model\",\n    model=return_invalid_table,\n    version_name=\"v1\",\n    metadata={\"task\": task.TEXT_TO_TEXT},\n)\nservice.register(\n    model_name=\"return-large-result\",\n    model=lambda _: [\"message \" * 100],\n    version_name=\"v1\",\n    metadata={\"task\": task.TEXT_TO_TEXT},\n)\nservice.register(\n    model_name=\"return-error-message\",\n    model=lambda _: \"Error message\",\n    version_name=\"v1\",\n    metadata={\"task\": task.TEXT_TO_TEXT},\n)\nservice.register(\n    model_name=\"return-500\",\n    model=raise_error,\n    version_name=\"v1\",\n    metadata={\"task\": task.TEXT_TO_TEXT},\n)\nservice.register(\n    model_name=\"custom-title-description\",\n    model=return_text,\n    version_name=\"v1\",\n    metadata={\n        \"task\": task.TEXT_TO_TEXT,\n        \"display_name\": \"My Model\",\n        \"description\": \"This is my model.\",\n    },\n)\n"}
{"type": "test_file", "path": "tests/e2e_tests/templates/__init__.py", "content": ""}
{"type": "test_file", "path": "tests/e2e_tests/__init__.py", "content": ""}
{"type": "test_file", "path": "tests/e2e_tests/templates/test_backend_error.py", "content": "\"\"\"End to End Test For Backend Error\"\"\"\n\nimport pytest\n\n\n@pytest.mark.parametrize(\"task\", [\"Text To Text\", \"Translation\"])\n@pytest.mark.parametrize(\"debug\", [True, False])\ndef test_success(task, debug, page):\n    # choose the return text model\n    model = page.locator(\"text=invalid-task-model\")\n    model.click()\n    return_text_model = page.locator(\"text=return-500\")\n    return_text_model.click()\n\n    # locate the sidebar\n    sidebar = page.locator('section[data-testid=\"stSidebar\"]')\n\n    # open the selector\n    # here, instead of using:\n    # task_selector = sidebar.locator(\n    #     'div[data-baseweb=\"select\"]:below(:text(\"Select the Task\"))'\n    # )\n    # we choose to select the 'Text To Text' spefically, just in case\n    # the task selection is clicked too fast and streamlit re-select the\n    # default task of the model again.\n    task_selector = sidebar.locator(\"text='Text To Text'\")\n    task_selector.wait_for(timeout=10000)\n    task_selector.click()\n\n    # choose the task\n    task = page.locator(\"li[role='option']\").locator(f\"text='{task}'\")\n    task.click()\n\n    # enable debug\n    if debug:\n        sidebar.locator(\"text='Debug'\").click()\n\n    # fill the text area\n    page.fill(\"textarea\", \"Hello.\")\n    main_div = page.locator(\"section.main\")\n\n    # click run button\n    run_btn = main_div.locator(\"text=Run\")\n    run_btn.click()\n\n    # wait for the result\n    result = main_div.locator('div.stAlert:has-text(\"Non 200 response from backend\")')\n    result.wait_for(timeout=10000)\n\n    assert result.count() == 1\n\n    # wait for debug expander\n    if debug:\n        debug_expander = main_div.locator(\n            'div[data-testid=\"stExpander\"]:has-text(\"Debug\")'\n        )\n        debug_expander.wait_for(timeout=10000)\n        assert debug_expander.count() == 1\n\n        # expand the debug panel\n        debug_expander.click()\n        assert (\n            page.locator(\n                'div.stAlert:has-text(\"Non 200 response from backend\")'\n            ).count()\n            == 2\n        )\n"}
{"type": "test_file", "path": "tests/e2e_tests/templates/test_camera_image_to_image.py", "content": "\"\"\"End to End Test For Camera Image to Text Template\"\"\"\n\nimport time\n\nimport pytest\nfrom playwright._impl._api_types import TimeoutError as PlaywrightTimeoutError\n\n\n@pytest.mark.parametrize(\"task\", [\"Camera Image To Image\"])\ndef test_success(task, page):\n    # choose the return text model\n    model = page.locator(\"text=invalid-task-model\")\n    model.click()\n    return_text_model = page.locator(\"text=return-image-model\")\n    return_text_model.click()\n\n    # locate the sidebar\n    sidebar = page.locator('section[data-testid=\"stSidebar\"]')\n\n    # open the selector\n    # here, instead of using:\n    # task_selector = sidebar.locator(\n    #     'div[data-baseweb=\"select\"]:below(:text(\"Select the Task\"))'\n    # )\n    # we choose to select the 'Text To Text' spefically, just in case\n    # the task selection is clicked too fast and streamlit re-select the\n    # default task of the model again.\n    task_selector = sidebar.locator(\"text='Text To Image'\")\n    task_selector.wait_for(timeout=10000)\n    task_selector.click()\n\n    # choose the task\n    task = page.locator(\"li[role='option']\").locator(f\"text='{task}'\")\n    task.click()\n\n    main_div = page.locator(\"section.main\")\n\n    # if the button is clicked too early, it will not work.\n    # Even it is not disabled. Further experiments needed.\n    # Currently a sleep and retry is used until a better solution\n    # with element wait is found.\n    time.sleep(1)\n\n    for _ in range(10):\n        page.click(\"text='Take Photo'\")\n\n        # wait for the result\n        result = main_div.locator('div[data-testid=\"stImage\"]:below(:text(\"Result\"))')\n        try:\n            result.wait_for(timeout=5000)\n            assert result.count() == 1\n            break\n        except PlaywrightTimeoutError:\n            pass\n        except Exception as exc:\n            raise exc\n    else:\n        assert False\n"}
{"type": "test_file", "path": "tests/e2e_tests/templates/test_camera_image_to_text.py", "content": "\"\"\"End to End Test For Camera Image to Text Template\"\"\"\n\nimport time\n\nimport pytest\nfrom playwright._impl._api_types import TimeoutError as PlaywrightTimeoutError\n\n\n@pytest.mark.parametrize(\"task\", [\"Camera Image To Text\"])\ndef test_success(task, page):\n    # choose the return text model\n    model = page.locator(\"text=invalid-task-model\")\n    model.click()\n    return_text_model = page.locator(\"text=return-text-model\")\n    return_text_model.click()\n\n    # locate the sidebar\n    sidebar = page.locator('section[data-testid=\"stSidebar\"]')\n\n    # open the selector\n    # here, instead of using:\n    # task_selector = sidebar.locator(\n    #     'div[data-baseweb=\"select\"]:below(:text(\"Select the Task\"))'\n    # )\n    # we choose to select the 'Text To Text' spefically, just in case\n    # the task selection is clicked too fast and streamlit re-select the\n    # default task of the model again.\n    task_selector = sidebar.locator(\"text='Text To Text'\")\n    task_selector.wait_for(timeout=10000)\n    task_selector.click()\n\n    # choose the task\n    task = page.locator(\"li[role='option']\").locator(f\"text='{task}'\")\n    task.click()\n\n    main_div = page.locator(\"section.main\")\n\n    # if the button is clicked too early, it will not work.\n    # Even it is not disabled. Further experiments needed.\n    # Currently a sleep and retry is used until a better solution\n    # with element wait is found.\n    time.sleep(1)\n\n    for _ in range(10):\n        page.click(\"text='Take Photo'\")\n\n        # wait for the result\n        result = main_div.locator('div.stAlert:has-text(\"abcdefg\")')\n        try:\n            result.wait_for(timeout=5000)\n            assert result.count() == 1\n            break\n        except PlaywrightTimeoutError:\n            pass\n        except Exception as exc:\n            raise exc\n    else:\n        assert False\n"}
{"type": "test_file", "path": "tests/e2e_tests/templates/test_custom_title_description.py", "content": "\"\"\"End to End Test For Custom Title and Description\"\"\"\n\n\ndef test_success(page):\n    # choose the return text model\n    model = page.locator(\"text=invalid-task-model\")\n    model.click()\n    return_text_model = page.locator(\"text=custom-title-description\")\n    return_text_model.click()\n\n    # fill the text area\n    main_div = page.locator(\"section.main\")\n\n    title = main_div.locator(\"text='My Model'\")\n    title.wait_for(timeout=10000)\n\n    assert title.count() == 1\n\n    description = main_div.locator(\"text='This is my model.'\")\n    description.wait_for(timeout=10000)\n\n    assert description.count() == 1\n"}
{"type": "test_file", "path": "tests/e2e_tests/templates/test_image_to_image.py", "content": "\"\"\"End to End Test For Image to Image Template\"\"\"\n\nimport base64\nimport tempfile\n\nimport pytest\n\n\n@pytest.mark.parametrize(\"task\", [\"Image To Image\", \"Image Style Transfer\"])\ndef test_success(task, image_base64_string, page):\n    # choose the return text model\n    model = page.locator(\"text=invalid-task-model\")\n    model.click()\n    return_image_model = page.locator(\"text=return-image-model\")\n    return_image_model.click()\n\n    # locate the sidebar\n    sidebar = page.locator('section[data-testid=\"stSidebar\"]')\n\n    # open the selector\n    # here, instead of using:\n    # task_selector = sidebar.locator(\n    #     'div[data-baseweb=\"select\"]:below(:text(\"Select the Task\"))'\n    # )\n    # we choose to select the 'Text To Text' spefically, just in case\n    # the task selection is clicked too fast and streamlit re-select the\n    # default task of the model again.\n    task_selector = sidebar.locator(\"text='Text To Image'\")\n    task_selector.wait_for(timeout=10000)\n    task_selector.click()\n\n    # choose the task\n    task = page.locator(\"li[role='option']\").locator(f\"text='{task}'\")\n    task.click()\n\n    main_div = page.locator(\"section.main\")\n\n    # upload\n    with page.expect_file_chooser() as fc_info:\n        page.click(\"text='Browse files'\")\n\n    with tempfile.NamedTemporaryFile(mode=\"wb\", suffix=\".jpg\") as f:\n        # create a temporary image file and write the image bytes\n        f.write(base64.b64decode(image_base64_string))\n\n        # flush the content to disk\n        f.flush()\n\n        # choose the created file\n        file_chooser = fc_info.value\n        file_chooser.set_files(f.name)\n\n        page.click(\"text='Upload and Run'\")\n\n        # wait for the result\n        result_column = main_div.locator('div[data-testid=\"column\"]:has-text(\"Result\")')\n        result = result_column.locator('div[data-testid=\"stImage\"]')\n        result.wait_for(timeout=10000)\n\n        assert result.count() == 1\n"}
{"type": "test_file", "path": "tests/e2e_tests/templates/test_image_to_text.py", "content": "\"\"\"End to End Test For Image to Text Template\"\"\"\n\nimport base64\nimport tempfile\n\nimport pytest\n\n\n@pytest.mark.parametrize(\"task\", [\"Image To Text\", \"Image Classification\"])\ndef test_success(task, image_base64_string, page):\n    # choose the return text model\n    model = page.locator(\"text=invalid-task-model\")\n    model.click()\n    return_text_model = page.locator(\"text=return-text-model\")\n    return_text_model.click()\n\n    # locate the sidebar\n    sidebar = page.locator('section[data-testid=\"stSidebar\"]')\n\n    # open the selector\n    # here, instead of using:\n    # task_selector = sidebar.locator(\n    #     'div[data-baseweb=\"select\"]:below(:text(\"Select the Task\"))'\n    # )\n    # we choose to select the 'Text To Text' spefically, just in case\n    # the task selection is clicked too fast and streamlit re-select the\n    # default task of the model again.\n    task_selector = sidebar.locator(\"text='Text To Text'\")\n    task_selector.click()\n\n    # choose the task\n    task = page.locator(\"li[role='option']\").locator(f\"text='{task}'\")\n    task_selector.wait_for(timeout=10000)\n    task.click()\n\n    main_div = page.locator(\"section.main\")\n\n    # upload\n    with page.expect_file_chooser() as fc_info:\n        page.click(\"text='Browse files'\")\n\n    with tempfile.NamedTemporaryFile(mode=\"wb\", suffix=\".jpg\") as f:\n        # create a temporary image file and write the image bytes\n        f.write(base64.b64decode(image_base64_string))\n\n        # flush the content to disk\n        f.flush()\n\n        # choose the created file\n        file_chooser = fc_info.value\n        file_chooser.set_files(f.name)\n\n        page.click(\"text='Upload and Run'\")\n\n        # wait for the result\n        result = main_div.locator('div.stAlert:has-text(\"abcdefg\")')\n        result.wait_for(timeout=10000)\n\n        assert result.count() == 1\n"}
{"type": "test_file", "path": "tests/e2e_tests/templates/test_large_result.py", "content": "\"\"\"End to End Test For Large Result Template\"\"\"\n\n\ndef test_success(page):\n    # choose the return text model\n    model = page.locator(\"text=invalid-task-model\")\n    model.click()\n    return_text_model = page.locator(\"text=return-large-result\")\n    return_text_model.click()\n\n    # locate the sidebar\n    sidebar = page.locator('section[data-testid=\"stSidebar\"]')\n\n    # enable debug\n    sidebar.locator(\"text='Debug'\").click()\n\n    # fill the text area\n    page.fill(\"textarea\", \"Hello.\")\n    main_div = page.locator(\"section.main\")\n\n    # click run button\n    run_btn = main_div.locator(\"text=Run\")\n    run_btn.click()\n\n    debug_expander = main_div.locator('div[data-testid=\"stExpander\"]:has-text(\"Debug\")')\n    debug_expander.click()\n\n    # wait for the result\n    warning_message = main_div.locator('text=\"The JSON body is too large to display.\"')\n    warning_message.wait_for(timeout=10000)\n    assert warning_message.count() == 1\n\n    display_result = debug_expander.locator(\"div.stMarkdown\").locator(\"code\")\n    display_result.wait_for(timeout=10000)\n    assert display_result.count() == 1\n"}
{"type": "test_file", "path": "tests/e2e_tests/templates/test_return_error_message.py", "content": "\"\"\"End to End Test For Error Message\"\"\"\n\nimport pytest\n\n\n@pytest.mark.parametrize(\"task\", [\"Text To Text\", \"Translation\"])\n@pytest.mark.parametrize(\"debug\", [True, False])\ndef test_text_success(task, debug, page):\n    # choose the return text model\n    model = page.locator(\"text=invalid-task-model\")\n    model.click()\n    return_text_model = page.locator(\"text=return-error-message\")\n    return_text_model.click()\n\n    # locate the sidebar\n    sidebar = page.locator('section[data-testid=\"stSidebar\"]')\n\n    # open the selector\n    # here, instead of using:\n    # task_selector = sidebar.locator(\n    #     'div[data-baseweb=\"select\"]:below(:text(\"Select the Task\"))'\n    # )\n    # we choose to select the 'Text To Text' spefically, just in case\n    # the task selection is clicked too fast and streamlit re-select the\n    # default task of the model again.\n    task_selector = sidebar.locator(\"text='Text To Text'\")\n    task_selector.click()\n\n    # choose the task\n    task = page.locator(\"li[role='option']\").locator(f\"text='{task}'\")\n    task_selector.wait_for(timeout=10000)\n    task.click()\n\n    # enable debug\n    if debug:\n        sidebar.locator(\"text='Debug'\").click()\n\n    # fill the text area\n    page.fill(\"textarea\", \"Hello.\")\n    main_div = page.locator(\"section.main\")\n\n    # click run button\n    run_btn = main_div.locator(\"text=Run\")\n    run_btn.click()\n\n    # wait for the result\n    result = main_div.locator('div.stAlert:has-text(\"Error Message\")')\n    result.wait_for(timeout=10000)\n\n    assert result.count() == 1\n\n    # wait for debug expander\n    if debug:\n        debug_expander = main_div.locator(\n            'div[data-testid=\"stExpander\"]:has-text(\"Debug\")'\n        )\n        debug_expander.wait_for(timeout=10000)\n        assert debug_expander.count() == 1\n"}
{"type": "test_file", "path": "tests/e2e_tests/templates/test_raw_request.py", "content": "\"\"\"End to End Test For Text to Text Template\"\"\"\n\nimport pytest\n\n\n@pytest.mark.parametrize(\"task\", [\"Raw Request\"])\ndef test_success(task, page):\n    # choose the return text model\n    model = page.locator(\"text=invalid-task-model\")\n    model.click()\n    return_text_model = page.locator(\"text=return-text-model\")\n    return_text_model.click()\n\n    # locate the sidebar\n    sidebar = page.locator('section[data-testid=\"stSidebar\"]')\n\n    # open the selector\n    # here, instead of using:\n    # task_selector = sidebar.locator(\n    #     'div[data-baseweb=\"select\"]:below(:text(\"Select the Task\"))'\n    # )\n    # we choose to select the 'Text To Text' spefically, just in case\n    # the task selection is clicked too fast and streamlit re-select the\n    # default task of the model again.\n    task_selector = sidebar.locator(\"text='Text To Text'\")\n    task_selector.click()\n\n    # choose the task\n    task = page.locator(\"li[role='option']\").locator(f\"text='{task}'\")\n    task_selector.wait_for(timeout=10000)\n    task.click()\n\n    # fill the text area\n    page.fill(\"textarea\", '{\"data\": [\"a\"]}')\n    main_div = page.locator(\"section.main\")\n\n    # click run button\n    run_btn = main_div.locator(\"text=Run\")\n    run_btn.click()\n\n    # wait for the result\n    result = main_div.locator('div[data-testid=\"stJson\"]:below(:text(\"Run\"))')\n    result.wait_for(timeout=10000)\n\n    assert result.count() == 1\n"}
{"type": "test_file", "path": "tests/e2e_tests/templates/test_text_to_image.py", "content": "\"\"\"End to End Test For Text to Image Template\"\"\"\n\nimport pytest\n\n\n@pytest.mark.parametrize(\"task\", [\"Text To Image\"])\ndef test_success(task, page):\n    # choose the return text model\n    model = page.locator(\"text=invalid-task-model\")\n    model.click()\n    return_image_model = page.locator(\"text=return-image-model\")\n    return_image_model.click()\n\n    # fill the text area\n    main_div = page.locator(\"section.main\")\n    text_input = main_div.locator(\"input\")\n    text_input.fill(\"cup\")\n\n    # click run button\n    run_btn = main_div.locator(\"text=Run\")\n    run_btn.click()\n\n    # wait for the result\n    result = main_div.locator('div[data-testid=\"stImage\"]:below(:text(\"Run\"))')\n    result.wait_for(timeout=10000)\n\n    assert result.count() == 1\n"}
{"type": "test_file", "path": "tests/e2e_tests/templates/test_text_to_text.py", "content": "\"\"\"End to End Test For Text to Text Template\"\"\"\n\nimport pytest\n\n\n@pytest.mark.parametrize(\"task\", [\"Text To Text\", \"Translation\"])\n@pytest.mark.parametrize(\"debug\", [True, False])\ndef test_text_success(task, debug, page):\n    # choose the return text model\n    model = page.locator(\"text=invalid-task-model\")\n    model.click()\n    return_text_model = page.locator(\"text=return-text-model\")\n    return_text_model.click()\n\n    # locate the sidebar\n    sidebar = page.locator('section[data-testid=\"stSidebar\"]')\n\n    # open the selector\n    # here, instead of using:\n    # task_selector = sidebar.locator(\n    #     'div[data-baseweb=\"select\"]:below(:text(\"Select the Task\"))'\n    # )\n    # we choose to select the 'Text To Text' spefically, just in case\n    # the task selection is clicked too fast and streamlit re-select the\n    # default task of the model again.\n    task_selector = sidebar.locator(\"text='Text To Text'\")\n    task_selector.click()\n\n    # choose the task\n    task = page.locator(\"li[role='option']\").locator(f\"text='{task}'\")\n    task_selector.wait_for(timeout=10000)\n    task.click()\n\n    # enable debug\n    if debug:\n        sidebar.locator(\"text='Debug'\").click()\n\n    # fill the text area\n    page.fill(\"textarea\", \"Hello.\")\n    main_div = page.locator(\"section.main\")\n\n    # click run button\n    run_btn = main_div.locator(\"text=Run\")\n    run_btn.click()\n\n    # wait for the result\n    result = main_div.locator('div.stAlert:has-text(\"abcdefg\")')\n    result.wait_for(timeout=10000)\n\n    assert result.count() == 1\n\n    # wait for debug expander\n    if debug:\n        debug_expander = main_div.locator(\n            'div[data-testid=\"stExpander\"]:has-text(\"Debug\")'\n        )\n        debug_expander.wait_for(timeout=10000)\n\n        assert debug_expander.count() == 1\n\n\n@pytest.mark.parametrize(\n    \"model_and_test_id\",\n    [\n        (\"return-json-model\", \"stJson\"),\n        (\"return-table-model\", \"stTable\"),\n        (\"return-invalid-table\", \"stJson\"),\n    ],\n)\n@pytest.mark.parametrize(\"task\", [\"Text To Text\", \"Translation\"])\ndef test_json_table_success(model_and_test_id, task, page):\n    model_name, test_id = model_and_test_id\n    # choose the return text model\n    model = page.locator(\"text=invalid-task-model\")\n    model.click()\n    return_text_model = page.locator(f\"text={model_name}\")\n    return_text_model.click()\n\n    # locate the sidebar\n    sidebar = page.locator('section[data-testid=\"stSidebar\"]')\n\n    # open the selector\n    # here, instead of using:\n    # task_selector = sidebar.locator(\n    #     'div[data-baseweb=\"select\"]:below(:text(\"Select the Task\"))'\n    # )\n    # we choose to select the 'Text To Text' spefically, just in case\n    # the task selection is clicked too fast and streamlit re-select the\n    # default task of the model again.\n    task_selector = sidebar.locator(\"text='Text To Text'\")\n    task_selector.click()\n\n    # choose the task\n    task = page.locator(\"li[role='option']\").locator(f\"text='{task}'\")\n    task.click()\n\n    # fill the text area\n    page.fill(\"textarea\", \"Hello.\")\n    main_div = page.locator(\"section.main\")\n\n    # click run button\n    run_btn = main_div.locator(\"text=Run\")\n    run_btn.click()\n\n    # wait for the result\n    result = main_div.locator(f'div[data-testid=\"{test_id}\"]:below(:text(\"Run\"))')\n    result.wait_for(timeout=10000)\n\n    assert result.count() == 1\n"}
{"type": "test_file", "path": "tests/extra/pytorch/mnist/main.py", "content": "from __future__ import print_function\n\nimport argparse\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import StepLR\nfrom torchvision import datasets, transforms\n\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n        self.dropout1 = nn.Dropout(0.25)\n        self.dropout2 = nn.Dropout(0.5)\n        self.fc1 = nn.Linear(9216, 128)\n        self.fc2 = nn.Linear(128, 10)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = F.relu(x)\n        x = self.conv2(x)\n        x = F.relu(x)\n        x = F.max_pool2d(x, 2)\n        x = self.dropout1(x)\n        x = torch.flatten(x, 1)\n        x = self.fc1(x)\n        x = F.relu(x)\n        x = self.dropout2(x)\n        x = self.fc2(x)\n        output = F.log_softmax(x, dim=1)\n        return output\n\n\ndef train(args, model, device, train_loader, optimizer, epoch):\n    model.train()\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data, target = data.to(device), target.to(device)\n        optimizer.zero_grad()\n        output = model(data)\n        loss = F.nll_loss(output, target)\n        loss.backward()\n        optimizer.step()\n        if batch_idx % args.log_interval == 0:\n            print(\n                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n                    epoch,\n                    batch_idx * len(data),\n                    len(train_loader.dataset),\n                    100.0 * batch_idx / len(train_loader),\n                    loss.item(),\n                )\n            )\n            if args.dry_run:\n                break\n\n\ndef test(model, device, test_loader):\n    model.eval()\n    test_loss = 0\n    correct = 0\n    with torch.no_grad():\n        for data, target in test_loader:\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            test_loss += F.nll_loss(\n                output, target, reduction=\"sum\"\n            ).item()  # sum up batch loss\n            pred = output.argmax(\n                dim=1, keepdim=True\n            )  # get the index of the max log-probability\n            correct += pred.eq(target.view_as(pred)).sum().item()\n\n    test_loss /= len(test_loader.dataset)\n\n    print(\n        \"\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n\".format(\n            test_loss,\n            correct,\n            len(test_loader.dataset),\n            100.0 * correct / len(test_loader.dataset),\n        )\n    )\n\n\ndef main():\n    # Training settings\n    parser = argparse.ArgumentParser(description=\"PyTorch MNIST Example\")\n    parser.add_argument(\n        \"--batch-size\",\n        type=int,\n        default=64,\n        metavar=\"N\",\n        help=\"input batch size for training (default: 64)\",\n    )\n    parser.add_argument(\n        \"--test-batch-size\",\n        type=int,\n        default=1000,\n        metavar=\"N\",\n        help=\"input batch size for testing (default: 1000)\",\n    )\n    parser.add_argument(\n        \"--epochs\",\n        type=int,\n        default=14,\n        metavar=\"N\",\n        help=\"number of epochs to train (default: 14)\",\n    )\n    parser.add_argument(\n        \"--lr\",\n        type=float,\n        default=1.0,\n        metavar=\"LR\",\n        help=\"learning rate (default: 1.0)\",\n    )\n    parser.add_argument(\n        \"--gamma\",\n        type=float,\n        default=0.7,\n        metavar=\"M\",\n        help=\"Learning rate step gamma (default: 0.7)\",\n    )\n    parser.add_argument(\n        \"--no-cuda\",\n        action=\"store_true\",\n        default=False,\n        help=\"disables CUDA training\",\n    )\n    parser.add_argument(\n        \"--dry-run\",\n        action=\"store_true\",\n        default=False,\n        help=\"quickly check a single pass\",\n    )\n    parser.add_argument(\n        \"--seed\",\n        type=int,\n        default=1,\n        metavar=\"S\",\n        help=\"random seed (default: 1)\",\n    )\n    parser.add_argument(\n        \"--log-interval\",\n        type=int,\n        default=10,\n        metavar=\"N\",\n        help=\"how many batches to wait before logging training status\",\n    )\n    parser.add_argument(\n        \"--save-model\",\n        action=\"store_true\",\n        default=False,\n        help=\"For Saving the current Model\",\n    )\n    args = parser.parse_args()\n    use_cuda = not args.no_cuda and torch.cuda.is_available()\n\n    torch.manual_seed(args.seed)\n\n    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n\n    train_kwargs = {\"batch_size\": args.batch_size}\n    test_kwargs = {\"batch_size\": args.test_batch_size}\n    if use_cuda:\n        cuda_kwargs = {\"num_workers\": 1, \"pin_memory\": True, \"shuffle\": True}\n        train_kwargs.update(cuda_kwargs)\n        test_kwargs.update(cuda_kwargs)\n\n    transform = transforms.Compose(\n        [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n    )\n    dataset1 = datasets.MNIST(\"../data\", train=True, download=True, transform=transform)\n    dataset2 = datasets.MNIST(\"../data\", train=False, transform=transform)\n    train_loader = torch.utils.data.DataLoader(dataset1, **train_kwargs)\n    test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)\n\n    model = Net().to(device)\n    optimizer = optim.Adadelta(model.parameters(), lr=args.lr)\n\n    scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)\n    for epoch in range(1, args.epochs + 1):\n        train(args, model, device, train_loader, optimizer, epoch)\n        test(model, device, test_loader)\n        scheduler.step()\n\n    if args.save_model:\n        torch.save(model.state_dict(), \"mnist_cnn.pt\")\n\n\nif __name__ == \"__main__\":\n    main()\n"}
{"type": "test_file", "path": "tests/extra/pytorch/mnist/prepare.py", "content": "import pathlib\n\nimport torch\n\nwork_dir = pathlib.Path(__file__).parent.resolve()\n\nfrom main import Net  # noqa\n\nmodel = Net().to(\"cpu\")\nstate_dict = torch.load(f\"{work_dir}/mnist_cnn.pt\")\nmodel.load_state_dict(state_dict)\n\n# save state dict\ntorch.save(model.state_dict(), f\"{work_dir}/state_dict.pt\")\n\n# save entire model\ntorch.save(model, f\"{work_dir}/entire_model.pt\")\n\n# save torch script model\nmodel_scripted = torch.jit.script(model)\nmodel_scripted.save(\"model_scripted.pt\")\n"}
{"type": "test_file", "path": "tests/extra/pytorch/mnist/test.py", "content": "import pathlib\nimport random\n\nimport torch\nfrom torchvision import datasets, transforms\n\nfrom pinferencia.handlers import TorchEntireModelHandler  # noqa\nfrom pinferencia.handlers import TorchScriptHandler  # noqa\n\ntransform = transforms.Compose(\n    [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n)\nwork_dir = pathlib.Path(__file__).parent.resolve()\ndataset = datasets.MNIST(\n    f\"{work_dir}/data\",\n    train=True,\n    download=True,\n    transform=transform,\n)\nindex = random.randint(0, len(dataset.data))\nimg = dataset.data[index]\ntarget = dataset.targets[index]\ntensor = torch.Tensor([img.numpy()])\ndata = torch.stack([tensor]).to(\"cpu\")\n\n\ndef test_entire_model():\n    # define model path\n    model_path = f\"{work_dir}/entire_model.pt\"\n\n    # load using torch\n    model = torch.load(model_path)\n    model.eval()\n    with torch.no_grad():\n        print(\"Prediction:\", model(data).argmax(1).tolist()[0])\n\n    # load using handler\n    handler = TorchEntireModelHandler(model_path=model_path)\n    model = handler.load_model()\n\n    # predict using handler\n    handler.predict(data)\n    print(\"Handler Prediction:\", model(data).argmax(1).tolist()[0])\n\n\ndef test_torch_script():\n    # define model path\n    model_path = f\"{work_dir}/model_scripted.pt\"\n    model = torch.jit.load(model_path)\n    model.eval()\n    with torch.no_grad():\n        print(\"Prediction:\", model(data).argmax(1).tolist()[0])\n\n    # load using handler\n    handler = TorchScriptHandler(model_path=model_path)\n    model = handler.load_model()\n\n    # predict using handler\n    handler.predict(data)\n    print(\"Handler Prediction:\", model(data).argmax(1).tolist()[0])\n\n\ndef test_state_dict():\n    from main import Net\n\n    device = \"cpu\"\n    model = Net().to(device)\n    state_dict = torch.load(f\"{work_dir}/state_dict.pt\")\n    model.load_state_dict(state_dict)\n    model.eval()\n    with torch.no_grad():\n        print(\"Prediction:\", model(data).argmax(1).tolist()[0])\n\n\nif __name__ == \"__main__\":\n    print(\"=\" * 10)\n    print(\"Target:\", target.numpy())\n    print(\"=\" * 10)\n    print(\"Test Entire Model\")\n    test_entire_model()\n    print(\"=\" * 10)\n    print(\"Test Torch Script\")\n    test_torch_script()\n    print(\"=\" * 10)\n    print(\"Test State Dict\")\n    test_state_dict()\n"}
{"type": "test_file", "path": "tests/models/__init__.py", "content": ""}
{"type": "test_file", "path": "tests/models/json_model/__init__.py", "content": ""}
{"type": "test_file", "path": "tests/models/json_model/app.py", "content": "import pathlib\n\nfrom pinferencia import Server\n\nfrom .mymodel.json_model import JSONModel\n\ncurrent_dir = pathlib.Path(__file__).parent.resolve()\nservice = Server()\n\nservice.register(\n    model_name=\"json\",\n    model=JSONModel(f\"{current_dir}/model.json\"),\n    entrypoint=\"predict\",\n    metadata={},\n)\n\nservice.register(\n    model_name=\"json\",\n    model=JSONModel(f\"{current_dir}/model.json\"),\n    version_name=\"v1\",\n    entrypoint=\"predict\",\n    metadata={},\n)\n"}
{"type": "test_file", "path": "tests/models/json_model/handler.py", "content": "import json\n\nfrom pinferencia.handlers import BaseHandler\n\n\nclass JsonHandler(BaseHandler):\n    def load_model(self):\n        with open(self.model_path) as f:\n            return json.load(f)\n\n    def predict(self, data):\n        return self.model.get(data, 0)\n"}
{"type": "test_file", "path": "tests/models/json_model/model.py", "content": "import pathlib\n\nfrom .mymodel.json_model import JSONModel\n\nmodel_dir = pathlib.Path(__file__).parent.resolve()\nmodel_path = \"model.json\"\nmodel = JSONModel(f\"{model_dir}/model.json\")\ninference_data = [\n    {\"request_data\": \"a\", \"response_data\": 1},\n    {\"request_data\": \"b\", \"response_data\": 2},\n    {\"request_data\": \"c\", \"response_data\": 3},\n    {\"request_data\": \"d\", \"response_data\": 0},\n]\nmetadata = {\n    \"platform\": \"mac os\",\n    \"inputs\": [\n        {\n            \"name\": \"integers\",\n            \"datatype\": \"str\",\n            \"shape\": -1,\n            \"data\": \"a\",\n        }\n    ],\n    \"outputs\": [{\"name\": \"sum\", \"datatype\": \"int64\", \"shape\": -1, \"data\": 1}],\n}\n"}
{"type": "test_file", "path": "tests/models/json_model/mymodel/__init__.py", "content": ""}
{"type": "test_file", "path": "tests/models/json_model/mymodel/json_model.py", "content": "import json\n\n\nclass JSONModel:\n    def __init__(self, json_path):\n        with open(json_path, \"r\") as f:\n            self.json = json.load(f)\n\n    def predict(self, data):\n        return self.json.get(data, 0)\n"}
{"type": "test_file", "path": "tests/models/sum_product_model/__init__.py", "content": "import math\n\n\nclass SumProductModel:\n    def predict(self, data):\n        if hasattr(math, \"prod\"):\n            return {\"sum\": sum(data), \"product\": math.prod(data)}\n        else:\n            # if python version < 3.8\n            import operator\n            from functools import reduce\n\n            return {\"sum\": sum(data), \"product\": reduce(operator.mul, data, 1)}\n\n\nmodel = SumProductModel()\ninference_data = [\n    {\"request_data\": [1, 2], \"response_data\": {\"sum\": 3, \"product\": 2}},\n    {\"request_data\": [1, 2, 3], \"response_data\": {\"sum\": 6, \"product\": 6}},\n    {\"request_data\": [1, 2, -3], \"response_data\": {\"sum\": 0, \"product\": -6}},\n    {\n        \"request_data\": [1, 2, 3, 4],\n        \"response_data\": {\"sum\": 10, \"product\": 24},\n    },\n]\nmetadata = {\n    \"platform\": \"mac os\",\n    \"inputs\": [\n        {\n            \"name\": \"integers\",\n            \"datatype\": \"int64\",\n            \"shape\": [1],\n            \"data\": [1, 2, 3],\n        }\n    ],\n    \"outputs\": [\n        {\"name\": \"sum\", \"datatype\": \"int64\", \"shape\": -1, \"data\": 6},\n        {\"name\": \"product\", \"datatype\": \"int64\", \"shape\": -1, \"data\": 6},\n    ],\n}\n"}
{"type": "test_file", "path": "tests/unittest/test_api_manager.py", "content": "from unittest.mock import patch\n\nimport pytest\n\nfrom pinferencia import Server\nfrom pinferencia.api_manager import BaseAPIManager\n\n\ndef test_extend_base_class():\n    service = Server()\n\n    class TestAPIManager(BaseAPIManager):\n        def register_route(self):\n            return None\n\n    assert (\n        TestAPIManager(service).validate_model_metadata(\n            model_name=\"any\",\n            metadata={\"a\": 1},\n        )\n        == []\n    )\n    assert TestAPIManager(service).register_route() is None\n\n\ndef test_incorrect_extend():\n    class TestAPIManager(BaseAPIManager):\n        pass\n\n    with pytest.raises(TypeError) as exc:\n        service = Server()\n        TestAPIManager(service)\n    assert (\n        \"Can't instantiate abstract class TestAPIManager\" \" with abstract method\"\n    ) in str(exc.value)\n    assert \"register_route\" in str(exc.value)\n\n\ndef test_instantiate():\n    with pytest.raises(TypeError) as exc:\n        service = Server()\n        BaseAPIManager(service)\n    assert (\n        \"Can't instantiate abstract class BaseAPIManager\" \" with abstract method\"\n    ) in str(exc.value)\n    assert \"register_route\" in str(exc.value)\n\n\n@patch.multiple(BaseAPIManager, __abstractmethods__=set())\ndef test_abstract_register_route():\n    service = Server()\n    assert BaseAPIManager(service).register_route() == NotImplemented\n"}
{"type": "test_file", "path": "tests/unittest/test_apis/__init__.py", "content": ""}
{"type": "test_file", "path": "tests/unittest/test_apis/default/__init__.py", "content": ""}
{"type": "test_file", "path": "tests/unittest/test_apis/default/data.py", "content": "from tests.models.json_model.model import inference_data as data\n\ninference_data = [\n    {\n        \"request\": {\"data\": d[\"request_data\"]},\n        \"response\": {\"data\": d[\"response_data\"]},\n    }\n    for d in data\n]\n"}
{"type": "test_file", "path": "tests/unittest/test_apis/default/conftest.py", "content": "import pytest\n\nfrom pinferencia import Server, task\n\n\n@pytest.fixture(scope=\"function\")\ndef json_model_default_service(json_model):\n    service = Server(api=\"default\")\n    service.register(\n        model_name=\"json\",\n        model=json_model,\n        entrypoint=\"predict\",\n        metadata={},\n    )\n    service.register(\n        model_name=\"json\",\n        model=json_model,\n        version_name=\"v1\",\n        entrypoint=\"predict\",\n        metadata={},\n    )\n    return service\n\n\n@pytest.fixture(scope=\"function\")\ndef json_model_with_path_default_service(\n    json_model_dir,\n    json_model_path,\n    json_model_handler,\n):\n    service = Server(api=\"default\", model_dir=json_model_dir)\n    service.register(\n        model_name=\"json\",\n        model=json_model_path,\n        handler=json_model_handler,\n        entrypoint=\"predict\",\n        metadata={},\n        load_now=False,\n    )\n    service.register(\n        model_name=\"json\",\n        model=json_model_path,\n        handler=json_model_handler,\n        version_name=\"v1\",\n        entrypoint=\"predict\",\n        metadata={},\n        load_now=False,\n    )\n    service.register(\n        model_name=\"json\",\n        model=json_model_path,\n        handler=json_model_handler,\n        version_name=\"loaded\",\n        entrypoint=\"predict\",\n        metadata={},\n    )\n    return service\n\n\n@pytest.fixture(scope=\"function\")\ndef add_substract_model_default_service():\n    class MyModel:\n        def add(self, data):\n            return data[0] + data[1]\n\n        def substract(self, data):\n            return data[0] - data[1]\n\n    model = MyModel()\n\n    service = Server()\n    service.register(\n        model_name=\"mymodel\",\n        model=model,\n        version_name=\"add\",\n        entrypoint=\"add\",\n    )\n    service.register(\n        model_name=\"mymodel\",\n        model=model,\n        version_name=\"substract\",\n        entrypoint=\"substract\",\n    )\n    return service\n\n\n@pytest.fixture(scope=\"function\")\ndef dummy_model_service():\n    def dummy(data: str) -> str:\n        return data\n\n    def dummy_v1(data: list) -> list:\n        return data\n\n    service = Server()\n    service.register(\n        model_name=\"dummy\",\n        model=dummy,\n        metadata={\n            \"task\": task.TEXT_TO_TEXT,\n            \"display_name\": \"Dummy Model\",\n            \"description\": \"This is a dummy model.\",\n            \"device\": \"CPU\",\n            \"platform\": \"linux\",\n        },\n    )\n    service.register(\n        model_name=\"dummy\",\n        model=dummy_v1,\n        version_name=\"v1\",\n        metadata={\n            \"task\": task.TEXT_TO_TEXT,\n            \"display_name\": \"Dummy Model V1\",\n            \"description\": \"This is a dummy model v1.\",\n        },\n    )\n    return service\n\n\n@pytest.fixture(scope=\"function\")\ndef dummy_model_service_with_decorator():\n    service = Server()\n\n    @service.decorators.register(\n        model_name=\"dummy\",\n        metadata={\n            \"task\": task.TEXT_TO_TEXT,\n            \"display_name\": \"Dummy Model\",\n            \"description\": \"This is a dummy model.\",\n            \"device\": \"CPU\",\n            \"platform\": \"linux\",\n        },\n    )\n    def dummy(data: str) -> str:\n        return data\n\n    @service.decorators.register(\n        model_name=\"dummy\",\n        version_name=\"v1\",\n        metadata={\n            \"task\": task.TEXT_TO_TEXT,\n            \"display_name\": \"Dummy Model V1\",\n            \"description\": \"This is a dummy model v1.\",\n        },\n    )\n    def dummy_v1(data: list) -> list:\n        return data\n\n    return service\n"}
{"type": "test_file", "path": "tests/unittest/test_apis/default/test_get_model_status.py", "content": "from fastapi.testclient import TestClient\n\nTEST_URL = \"/v1/models/{model_name}/ready\"\n\n\ndef test_200(json_model_default_service):\n    client = TestClient(json_model_default_service)\n    response = client.get(TEST_URL.format(model_name=\"json\"))\n    assert response.status_code == 200\n    assert response.json()\n\n\ndef test_404(json_model_default_service):\n    client = TestClient(json_model_default_service)\n    response = client.get(TEST_URL.format(model_name=\"invalid\"))\n    assert response.status_code == 404\n    assert response.json()[\"detail\"] == \"Model not found.\"\n\n\ndef test_200_false(json_model_with_path_default_service):\n    client = TestClient(json_model_with_path_default_service)\n    response = client.get(TEST_URL.format(model_name=\"json\"))\n    assert response.status_code == 200\n    assert not response.json()\n"}
{"type": "test_file", "path": "tests/unittest/test_apis/default/test_get_version_status.py", "content": "import pytest\nfrom fastapi.testclient import TestClient\n\nTEST_URL = \"/v1/models/{model_name}/versions/{version_name}/ready\"\n\n\n@pytest.mark.parametrize(\"version_name\", [\"default\", \"v1\"])\ndef test_200(\n    json_model_default_service,\n    version_name,\n):\n    client = TestClient(json_model_default_service)\n    response = client.get(\n        TEST_URL.format(\n            model_name=\"json\",\n            version_name=version_name,\n        )\n    )\n    assert response.status_code == 200\n    assert response.json()\n\n\ndef test_404(json_model_default_service):\n    client = TestClient(json_model_default_service)\n    response = client.get(\n        TEST_URL.format(\n            model_name=\"json\",\n            version_name=\"invalid\",\n        )\n    )\n    assert response.status_code == 404\n    assert response.json()[\"detail\"] == \"Model not found.\"\n\n\n@pytest.mark.parametrize(\"version_name\", [\"default\", \"v1\"])\ndef test_200_false(\n    json_model_with_path_default_service,\n    version_name,\n):\n    client = TestClient(json_model_with_path_default_service)\n    response = client.get(\n        TEST_URL.format(\n            model_name=\"json\",\n            version_name=version_name,\n        )\n    )\n\n    assert response.status_code == 200\n    assert not response.json()\n"}
{"type": "test_file", "path": "tests/unittest/test_apis/default/test_health.py", "content": "from fastapi.testclient import TestClient\n\nTEST_URL = \"/v1/healthz\"\n\n\ndef test_200(json_model_default_service):\n    client = TestClient(json_model_default_service)\n    response = client.get(TEST_URL)\n    assert response.status_code == 200\n    assert response.json()\n"}
{"type": "test_file", "path": "tests/unittest/test_apis/default/test_infer_default.py", "content": "import pytest\nfrom fastapi.testclient import TestClient\n\nfrom .data import inference_data\n\nTEST_URL = \"/v1/models/{model_name}/predict\"\n\n\n@pytest.mark.parametrize(\"inference_data\", inference_data)\ndef test_200_json_model(\n    json_model_default_service,\n    inference_data,\n):\n    client = TestClient(json_model_default_service)\n    response = client.post(\n        TEST_URL.format(model_name=\"json\"), json=inference_data[\"request\"]\n    )\n    assert response.status_code == 200\n    assert response.json()[\"data\"] == inference_data[\"response\"][\"data\"]\n\n\n@pytest.mark.parametrize(\n    \"inference_data\",\n    [\n        {\n            \"request\": {\"data\": \"a\"},\n            \"response\": {\"data\": \"a\"},\n        },\n        {\n            \"request\": {\"data\": 1},\n            \"response\": {\"data\": \"1\"},\n        },\n        {\n            \"request\": {\"data\": True},\n            \"response\": {\"data\": \"True\"},\n        },\n    ],\n)\n@pytest.mark.parametrize(\"register_with_decorator\", [True, False])\ndef test_200_dummy_model(\n    inference_data,\n    register_with_decorator,\n    dummy_model_service,\n    dummy_model_service_with_decorator,\n):\n    if register_with_decorator:\n        client = TestClient(dummy_model_service_with_decorator)\n    else:\n        client = TestClient(dummy_model_service)\n    response = client.post(\n        TEST_URL.format(model_name=\"dummy\"),\n        json=inference_data[\"request\"],\n    )\n    assert response.status_code == 200\n    assert response.json()[\"data\"] == inference_data[\"response\"][\"data\"]\n\n\n@pytest.mark.parametrize(\n    \"inference_data\",\n    [\n        {\"request\": {\"data\": {\"a\": 1}}},\n        {\"request\": {\"data\": [1, 2]}},\n    ],\n)\n@pytest.mark.parametrize(\"register_with_decorator\", [True, False])\ndef test_422_dummy_model(\n    inference_data,\n    register_with_decorator,\n    dummy_model_service,\n    dummy_model_service_with_decorator,\n):\n    if register_with_decorator:\n        client = TestClient(dummy_model_service_with_decorator)\n    else:\n        client = TestClient(dummy_model_service)\n    response = client.post(\n        TEST_URL.format(model_name=\"dummy\"),\n        json=inference_data[\"request\"],\n    )\n    assert response.status_code == 422\n"}
{"type": "test_file", "path": "tests/unittest/test_apis/default/test_infer_versions.py", "content": "import pytest\nfrom fastapi.testclient import TestClient\n\nfrom .data import inference_data\n\nTEST_URL = \"/v1/models/{model_name}/versions/{version_name}/predict\"\n\n\n@pytest.mark.parametrize(\"inference_data\", inference_data)\n@pytest.mark.parametrize(\"version_name\", [\"default\", \"v1\"])\ndef test_predict_json_model_versions(\n    json_model_default_service,\n    inference_data,\n    version_name,\n):\n    client = TestClient(json_model_default_service)\n    response = client.post(\n        TEST_URL.format(model_name=\"json\", version_name=version_name),\n        json=inference_data[\"request\"],\n    )\n    assert response.status_code == 200\n    assert response.json()[\"data\"] == inference_data[\"response\"][\"data\"]\n\n\n@pytest.mark.parametrize(\n    \"inference_data\",\n    [\n        {\n            \"version_name\": \"add\",\n            \"request\": {\"data\": [2, 1]},\n            \"response\": {\"data\": 3},\n        },\n        {\n            \"version_name\": \"substract\",\n            \"request\": {\"data\": [2, 1]},\n            \"response\": {\"data\": 1},\n        },\n    ],\n)\ndef test_different_entrypoint(\n    add_substract_model_default_service,\n    inference_data,\n):\n    client = TestClient(add_substract_model_default_service)\n    response = client.post(\n        TEST_URL.format(\n            model_name=\"mymodel\",\n            version_name=inference_data[\"version_name\"],\n        ),\n        json=inference_data[\"request\"],\n    )\n    assert response.status_code == 200\n    assert response.json()[\"data\"] == inference_data[\"response\"][\"data\"]\n\n\n@pytest.mark.parametrize(\n    \"inference_data\",\n    [\n        {\n            \"version_name\": \"v1\",\n            \"request\": {\"data\": [2, 1]},\n            \"response\": {\"data\": [2, 1]},\n        },\n        {\n            \"version_name\": \"default\",\n            \"request\": {\"data\": \"a\"},\n            \"response\": {\"data\": \"a\"},\n        },\n        {\n            \"version_name\": \"default\",\n            \"request\": {\"data\": 1},\n            \"response\": {\"data\": \"1\"},\n        },\n        {\n            \"version_name\": \"default\",\n            \"request\": {\"data\": True},\n            \"response\": {\"data\": \"True\"},\n        },\n    ],\n)\n@pytest.mark.parametrize(\"register_with_decorator\", [True, False])\ndef test_200_dummy_model(\n    inference_data,\n    register_with_decorator,\n    dummy_model_service,\n    dummy_model_service_with_decorator,\n):\n    if register_with_decorator:\n        client = TestClient(dummy_model_service_with_decorator)\n    else:\n        client = TestClient(dummy_model_service)\n    response = client.post(\n        TEST_URL.format(\n            model_name=\"dummy\",\n            version_name=inference_data[\"version_name\"],\n        ),\n        json=inference_data[\"request\"],\n    )\n    assert response.status_code == 200\n    assert response.json()[\"data\"] == inference_data[\"response\"][\"data\"]\n\n\n@pytest.mark.parametrize(\n    \"inference_data\",\n    [\n        {\n            \"version_name\": \"v1\",\n            \"request\": {\"data\": 2},\n        },\n        {\n            \"version_name\": \"v1\",\n            \"request\": {\"data\": {\"a\": 1}},\n        },\n        {\n            \"version_name\": \"default\",\n            \"request\": {\"data\": {\"a\": 1}},\n        },\n        {\n            \"version_name\": \"default\",\n            \"request\": {\"data\": [1, 2]},\n        },\n    ],\n)\n@pytest.mark.parametrize(\"register_with_decorator\", [True, False])\ndef test_422_dummy_model(\n    inference_data,\n    register_with_decorator,\n    dummy_model_service,\n    dummy_model_service_with_decorator,\n):\n    if register_with_decorator:\n        client = TestClient(dummy_model_service_with_decorator)\n    else:\n        client = TestClient(dummy_model_service)\n    response = client.post(\n        TEST_URL.format(\n            model_name=\"dummy\",\n            version_name=inference_data[\"version_name\"],\n        ),\n        json=inference_data[\"request\"],\n    )\n    assert response.status_code == 422\n"}
{"type": "test_file", "path": "tests/unittest/test_apis/default/test_list_models.py", "content": "import pytest\nfrom fastapi.testclient import TestClient\n\nfrom pinferencia import task\n\nTEST_URL = \"/v1/models\"\n\n\ndef test_list_json_models(json_model_default_service):\n    client = TestClient(json_model_default_service)\n    response = client.get(TEST_URL)\n    assert response.status_code == 200\n    assert response.json() == [\n        {\n            \"name\": \"json\",\n            \"versions\": [\n                {\n                    \"name\": \"default\",\n                    \"platform\": \"\",\n                    \"device\": \"\",\n                    \"task\": \"\",\n                    \"display_name\": \"\",\n                    \"description\": \"\",\n                    \"input_type\": \"\",\n                    \"output_type\": \"\",\n                },\n                {\n                    \"name\": \"v1\",\n                    \"platform\": \"\",\n                    \"device\": \"\",\n                    \"task\": \"\",\n                    \"display_name\": \"\",\n                    \"description\": \"\",\n                    \"input_type\": \"\",\n                    \"output_type\": \"\",\n                },\n            ],\n        },\n    ]\n\n\ndef test_list_json_model_with_path(json_model_with_path_default_service):\n    client = TestClient(json_model_with_path_default_service)\n    response = client.get(TEST_URL)\n    assert response.status_code == 200\n    assert response.json() == [\n        {\n            \"name\": \"json\",\n            \"versions\": [\n                {\n                    \"name\": \"default\",\n                    \"platform\": \"\",\n                    \"device\": \"\",\n                    \"task\": \"\",\n                    \"display_name\": \"\",\n                    \"description\": \"\",\n                    \"input_type\": \"\",\n                    \"output_type\": \"\",\n                },\n                {\n                    \"name\": \"v1\",\n                    \"platform\": \"\",\n                    \"device\": \"\",\n                    \"task\": \"\",\n                    \"display_name\": \"\",\n                    \"description\": \"\",\n                    \"input_type\": \"\",\n                    \"output_type\": \"\",\n                },\n                {\n                    \"name\": \"loaded\",\n                    \"platform\": \"\",\n                    \"device\": \"\",\n                    \"task\": \"\",\n                    \"display_name\": \"\",\n                    \"description\": \"\",\n                    \"input_type\": \"\",\n                    \"output_type\": \"\",\n                },\n            ],\n        },\n    ]\n\n\n@pytest.mark.parametrize(\"use_decorator\", [True, False])\ndef test_list_dummy_model(\n    use_decorator, dummy_model_service, dummy_model_service_with_decorator\n):\n    if use_decorator:\n        client = TestClient(dummy_model_service_with_decorator)\n    else:\n        client = TestClient(dummy_model_service)\n    response = client.get(TEST_URL)\n    assert response.status_code == 200\n    assert response.json() == [\n        {\n            \"name\": \"dummy\",\n            \"versions\": [\n                {\n                    \"name\": \"default\",\n                    \"task\": task.TEXT_TO_TEXT,\n                    \"display_name\": \"Dummy Model\",\n                    \"description\": \"This is a dummy model.\",\n                    \"device\": \"CPU\",\n                    \"platform\": \"linux\",\n                    \"input_type\": \"str\",\n                    \"output_type\": \"str\",\n                },\n                {\n                    \"name\": \"v1\",\n                    \"platform\": \"\",\n                    \"device\": \"\",\n                    \"task\": task.TEXT_TO_TEXT,\n                    \"display_name\": \"Dummy Model V1\",\n                    \"description\": \"This is a dummy model v1.\",\n                    \"input_type\": \"list\",\n                    \"output_type\": \"list\",\n                },\n            ],\n        },\n    ]\n"}
{"type": "test_file", "path": "tests/unittest/test_apis/default/test_load_version.py", "content": "import pytest\nfrom fastapi.testclient import TestClient\n\nTEST_URL = \"/v1/models/{model_name}/versions/{version_name}\"\nTEST_READY_URL = TEST_URL + \"/ready\"\nTEST_LOAD_URL = TEST_URL + \"/load\"\n\n\n@pytest.mark.parametrize(\"version_name\", [\"default\", \"v1\"])\ndef test_200(\n    json_model_with_path_default_service,\n    version_name,\n):\n    client = TestClient(json_model_with_path_default_service)\n    response = client.get(\n        TEST_READY_URL.format(\n            model_name=\"json\",\n            version_name=version_name,\n        )\n    )\n    assert response.status_code == 200\n    assert not response.json()\n    response = client.post(\n        TEST_LOAD_URL.format(\n            model_name=\"json\",\n            version_name=version_name,\n        )\n    )\n    assert response.status_code == 200\n    assert response.json()\n    response = client.get(\n        TEST_READY_URL.format(\n            model_name=\"json\",\n            version_name=version_name,\n        )\n    )\n    assert response.status_code == 200\n    assert response.json()\n\n\n@pytest.mark.parametrize(\"version_name\", [\"loaded\"])\ndef test_400_registered_with_path_already_loaded(\n    json_model_with_path_default_service,\n    version_name,\n):\n    client = TestClient(json_model_with_path_default_service)\n    response = client.get(\n        TEST_READY_URL.format(\n            model_name=\"json\",\n            version_name=version_name,\n        )\n    )\n    assert response.status_code == 200\n    assert response.json()\n    response = client.post(\n        TEST_LOAD_URL.format(\n            model_name=\"json\",\n            version_name=version_name,\n        )\n    )\n    assert response.status_code == 400\n    assert response.json()[\"detail\"] == \"Model already loaded.\"\n\n\n@pytest.mark.parametrize(\"version_name\", [\"default\", \"v1\"])\ndef test_400_registered_with_object_already_loaded(\n    json_model_default_service,\n    version_name,\n):\n    client = TestClient(json_model_default_service)\n    response = client.get(\n        TEST_READY_URL.format(\n            model_name=\"json\",\n            version_name=version_name,\n        )\n    )\n    assert response.status_code == 200\n    assert response.json()\n    response = client.post(\n        TEST_LOAD_URL.format(\n            model_name=\"json\",\n            version_name=version_name,\n        )\n    )\n    assert response.status_code == 400\n    assert response.json()[\"detail\"] == \"Model already loaded.\"\n\n\n@pytest.mark.parametrize(\"version_name\", [\"invalid\"])\ndef test_404(\n    json_model_with_path_default_service,\n    version_name,\n):\n    client = TestClient(json_model_with_path_default_service)\n    response = client.post(\n        TEST_LOAD_URL.format(\n            model_name=\"json\",\n            version_name=version_name,\n        )\n    )\n    assert response.status_code == 404\n    assert response.json()[\"detail\"] == \"Model not found.\"\n"}
{"type": "test_file", "path": "tests/unittest/test_apis/default/test_list_versions.py", "content": "from fastapi.testclient import TestClient\n\nTEST_URL = \"/v1/models/{model_name}\"\n\n\ndef test_get_json_model_versions(json_model_default_service):\n    client = TestClient(json_model_default_service)\n    response = client.get(TEST_URL.format(model_name=\"json\"))\n    assert response.status_code == 200\n    assert response.json() == [\n        {\n            \"name\": \"default\",\n            \"platform\": \"\",\n            \"device\": \"\",\n            \"task\": \"\",\n            \"display_name\": \"\",\n            \"description\": \"\",\n            \"input_type\": \"\",\n            \"output_type\": \"\",\n        },\n        {\n            \"name\": \"v1\",\n            \"platform\": \"\",\n            \"device\": \"\",\n            \"task\": \"\",\n            \"display_name\": \"\",\n            \"description\": \"\",\n            \"input_type\": \"\",\n            \"output_type\": \"\",\n        },\n    ]\n"}
{"type": "test_file", "path": "tests/unittest/test_apis/default/test_load_model.py", "content": "from fastapi.testclient import TestClient\n\nTEST_URL = \"/v1/models/{model_name}\"\nTEST_READY_URL = TEST_URL + \"/ready\"\nTEST_LOAD_URL = TEST_URL + \"/load\"\n\n\ndef test_200(json_model_with_path_default_service):\n    client = TestClient(json_model_with_path_default_service)\n    # model is not ready\n    response = client.get(TEST_READY_URL.format(model_name=\"json\"))\n    assert response.status_code == 200\n    assert not response.json()\n    # load the model\n    response = client.post(TEST_LOAD_URL.format(model_name=\"json\"))\n    assert response.status_code == 200\n    assert response.json()\n    # model is ready\n    response = client.get(TEST_READY_URL.format(model_name=\"json\"))\n    assert response.status_code == 200\n    assert response.json()\n\n\ndef test_400_registered_with_path_already_loaded(\n    json_model_with_path_default_service,\n):\n    client = TestClient(json_model_with_path_default_service)\n    # model is not ready\n    response = client.get(TEST_READY_URL.format(model_name=\"json\"))\n    assert response.status_code == 200\n    assert not response.json()\n    # load the model\n    response = client.post(TEST_LOAD_URL.format(model_name=\"json\"))\n    assert response.status_code == 200\n    assert response.json()\n    # model is ready\n    response = client.get(TEST_READY_URL.format(model_name=\"json\"))\n    assert response.status_code == 200\n    assert response.json()\n    # load again return 400\n    response = client.post(TEST_LOAD_URL.format(model_name=\"json\"))\n    assert response.status_code == 400\n    assert response.json()[\"detail\"] == \"Model already loaded.\"\n\n\ndef test_400_registered_with_object_already_loaded(\n    json_model_default_service,\n):\n    client = TestClient(json_model_default_service)\n    # model is ready\n    response = client.get(TEST_READY_URL.format(model_name=\"json\"))\n    assert response.status_code == 200\n    assert response.json()\n    # load the model return 400\n    response = client.post(TEST_LOAD_URL.format(model_name=\"json\"))\n    assert response.status_code == 400\n    assert response.json()[\"detail\"] == \"Model already loaded.\"\n\n\ndef test_404(json_model_with_path_default_service):\n    client = TestClient(json_model_with_path_default_service)\n    response = client.post(TEST_LOAD_URL.format(model_name=\"invalid\"))\n    assert response.status_code == 404\n    assert response.json()[\"detail\"] == \"Model not found.\"\n"}
{"type": "test_file", "path": "tests/unittest/test_apis/default/test_unload_version.py", "content": "import pytest\nfrom fastapi.testclient import TestClient\n\nTEST_URL = \"/v1/models/{model_name}/versions/{version_name}\"\nTEST_READY_URL = TEST_URL + \"/ready\"\nTEST_LOAD_URL = TEST_URL + \"/load\"\nTEST_UNLOAD_URL = TEST_URL + \"/unload\"\n\n\n@pytest.mark.parametrize(\"version_name\", [\"default\", \"v1\"])\ndef test_200(json_model_with_path_default_service, version_name):\n    client = TestClient(json_model_with_path_default_service)\n    # model is not ready\n    response = client.get(\n        TEST_READY_URL.format(\n            model_name=\"json\",\n            version_name=version_name,\n        )\n    )\n    assert response.status_code == 200\n    assert not response.json()\n    # load the model\n    response = client.post(\n        TEST_LOAD_URL.format(\n            model_name=\"json\",\n            version_name=version_name,\n        )\n    )\n    assert response.status_code == 200\n    assert response.json()\n    # model is ready\n    response = client.get(\n        TEST_READY_URL.format(\n            model_name=\"json\",\n            version_name=version_name,\n        )\n    )\n    assert response.status_code == 200\n    assert response.json()\n    # unload the model\n    response = client.post(\n        TEST_UNLOAD_URL.format(\n            model_name=\"json\",\n            version_name=version_name,\n        )\n    )\n    assert response.status_code == 200\n    assert response.json()\n    # model is not ready\n    response = client.get(\n        TEST_READY_URL.format(\n            model_name=\"json\",\n            version_name=version_name,\n        )\n    )\n    assert response.status_code == 200\n    assert not response.json()\n\n\n@pytest.mark.parametrize(\"version_name\", [\"default\", \"v1\"])\ndef test_400_registered_with_path_not_loaded(\n    json_model_with_path_default_service, version_name\n):\n    client = TestClient(json_model_with_path_default_service)\n    # model is not ready\n    response = client.get(\n        TEST_READY_URL.format(\n            model_name=\"json\",\n            version_name=version_name,\n        )\n    )\n    assert response.status_code == 200\n    assert not response.json()\n    # load again return 400\n    response = client.post(\n        TEST_UNLOAD_URL.format(\n            model_name=\"json\",\n            version_name=version_name,\n        )\n    )\n    assert response.status_code == 400\n    assert response.json()[\"detail\"] == \"Model is not loaded.\"\n\n\n@pytest.mark.parametrize(\"version_name\", [\"default\", \"v1\"])\ndef test_400_registered_with_object_cannot_unload(\n    json_model_default_service, version_name\n):\n    client = TestClient(json_model_default_service)\n    # model is ready\n    response = client.get(\n        TEST_READY_URL.format(\n            model_name=\"json\",\n            version_name=version_name,\n        )\n    )\n    assert response.status_code == 200\n    assert response.json()\n    # load the model return 400\n    response = client.post(\n        TEST_UNLOAD_URL.format(\n            model_name=\"json\",\n            version_name=version_name,\n        )\n    )\n    assert response.status_code == 400\n    assert response.json()[\"detail\"] == \"Cannot unload model registered without path.\"\n\n\n@pytest.mark.parametrize(\"version_name\", [\"default\", \"v1\"])\ndef test_404(json_model_with_path_default_service, version_name):\n    client = TestClient(json_model_with_path_default_service)\n    response = client.post(\n        TEST_UNLOAD_URL.format(\n            model_name=\"json\",\n            version_name=\"invalid\",\n        )\n    )\n    assert response.status_code == 404\n    assert response.json()[\"detail\"] == \"Model not found.\"\n"}
{"type": "test_file", "path": "tests/unittest/test_apis/default/test_unload_model.py", "content": "from fastapi.testclient import TestClient\n\nTEST_URL = \"/v1/models/{model_name}\"\nTEST_READY_URL = TEST_URL + \"/ready\"\nTEST_LOAD_URL = TEST_URL + \"/load\"\nTEST_UNLOAD_URL = TEST_URL + \"/unload\"\n\n\ndef test_200(json_model_with_path_default_service):\n    client = TestClient(json_model_with_path_default_service)\n    # model is not ready\n    response = client.get(TEST_READY_URL.format(model_name=\"json\"))\n    assert response.status_code == 200\n    assert not response.json()\n    # load the model\n    response = client.post(TEST_LOAD_URL.format(model_name=\"json\"))\n    assert response.status_code == 200\n    assert response.json()\n    # model is ready\n    response = client.get(TEST_READY_URL.format(model_name=\"json\"))\n    assert response.status_code == 200\n    assert response.json()\n    # unload the model\n    response = client.post(TEST_UNLOAD_URL.format(model_name=\"json\"))\n    assert response.status_code == 200\n    assert response.json()\n    # model is not ready\n    response = client.get(TEST_READY_URL.format(model_name=\"json\"))\n    assert response.status_code == 200\n    assert not response.json()\n\n\ndef test_400_registered_with_path_not_loaded(\n    json_model_with_path_default_service,\n):\n    client = TestClient(json_model_with_path_default_service)\n    # model is not ready\n    response = client.get(TEST_READY_URL.format(model_name=\"json\"))\n    assert response.status_code == 200\n    assert not response.json()\n    # load again return 400\n    response = client.post(TEST_UNLOAD_URL.format(model_name=\"json\"))\n    assert response.status_code == 400\n    assert response.json()[\"detail\"] == \"Model is not loaded.\"\n\n\ndef test_400_registered_with_object_cannot_unload(\n    json_model_default_service,\n):\n    client = TestClient(json_model_default_service)\n    # model is ready\n    response = client.get(TEST_READY_URL.format(model_name=\"json\"))\n    assert response.status_code == 200\n    assert response.json()\n    # load the model return 400\n    response = client.post(TEST_UNLOAD_URL.format(model_name=\"json\"))\n    assert response.status_code == 400\n    assert response.json()[\"detail\"] == \"Cannot unload model registered without path.\"\n\n\ndef test_404(json_model_with_path_default_service):\n    client = TestClient(json_model_with_path_default_service)\n    response = client.post(TEST_UNLOAD_URL.format(model_name=\"invalid\"))\n    assert response.status_code == 404\n    assert response.json()[\"detail\"] == \"Model not found.\"\n"}
{"type": "source_file", "path": "examples/custom_frontend/app.py", "content": "from typing import List\n\nfrom pinferencia import Server\n\n\ndef stat(data: List[float]) -> float:\n    return sum(data)\n\n\nservice = Server()\nservice.register(\n    model_name=\"stat\",\n    model=stat,\n    metadata={\"display_name\": \"Awesome Model\"},\n)\n"}
{"type": "source_file", "path": "examples/custom_frontend/frontend.py", "content": "from pinferencia.frontend.app import Server\n\ndetail_description = \"\"\"\n# My Awesome Model\n\nThis is the service of my awesome model.\n\nIt is **fast**, **simple**, and **beautiful**.\n\nVisit [My Awesome Model Home](/abc) to learn more about it.\n\"\"\"\n\nservice = Server(\n    title=\"My Awesome Model\",\n    short_description=\"This is the short description\",\n    detail_description=detail_description,\n    backend_server=\"http://127.0.0.1:8000\",\n)\n"}
{"type": "source_file", "path": "examples/custom_template/frontend.py", "content": "import json\n\nimport streamlit as st\n\nfrom pinferencia.frontend.app import Server\nfrom pinferencia.frontend.templates.base import BaseTemplate\n\n\nclass StatTemplate(BaseTemplate):\n    title = (\n        '<span style=\"color:salmon;\">Numbers</span> '\n        '<span style=\"color:slategray;\">Statistics</span>'\n    )\n\n    def render(self):\n        super().render()\n        json_template = \"[]\"\n        col1, col2 = st.columns(2)\n        col2.write(\"Request Preview\")\n        raw_text = col1.text_area(\"Raw Data\", value=json_template, height=150)\n        col2.json(raw_text)\n\n        pred_btn = st.button(\"Run\")\n        if pred_btn:\n            with st.spinner(\"Wait for result\"):\n                prediction = self.predict(json.loads(raw_text))\n            st.write(\"Statistics\")\n\n            result_col1, result_col2, result_col3 = st.columns(3)\n            result_col1.metric(label=\"Max\", value=prediction.get(\"max\"))\n            result_col2.metric(label=\"Min\", value=prediction.get(\"min\"))\n            result_col3.metric(label=\"Mean\", value=prediction.get(\"mean\"))\n\n\nbackend_address = \"http://127.0.0.1:8000\"\n\nservice = Server(\n    backend_server=f\"{backend_address}\",\n    custom_templates={\"Stat\": StatTemplate},\n)\n"}
{"type": "source_file", "path": "examples/demo/demo_app.py", "content": "import base64\nfrom io import BytesIO\n\nimport torch\nfrom PIL import Image\nfrom pytorch_pretrained_biggan import (\n    BigGAN,\n    convert_to_images,\n    one_hot_from_names,\n    truncated_noise_sample,\n)\nfrom transformers import pipeline\n\nfrom pinferencia import Server, task\n\n\ndef load_model():\n    image_classification_save_path = \"/tmp/google-vit\"\n    tranlator_save_path = \"/tmp/google-t5\"\n    return (\n        pipeline(task=\"image-classification\", model=image_classification_save_path),\n        pipeline(\n            task=\"translation\",\n            model=tranlator_save_path,\n            tokenizer=tranlator_save_path,\n        ),\n        # pipeline(model=\"google/vit-base-patch16-224\"),\n        # pipeline(model=\"t5-base\", tokenizer=\"t5-base\"),\n        BigGAN.from_pretrained(\"biggan-deep-256\"),\n    )\n\n\nclassifier, translator, generator = load_model()\n\n\ndef classify(images: list):\n    input_images = []\n    for image_data in images:\n        input_images.append(Image.open(BytesIO(base64.b64decode(image_data))))\n    return classifier(input_images)\n\n\ndef translate(text: list):\n    return translator(text)[0][\"translation_text\"]\n\n\ndef generate(text: list):\n    # Prepare a input\n    truncation = 0.4\n    class_vector = one_hot_from_names(text, batch_size=1)\n    noise_vector = truncated_noise_sample(truncation=truncation, batch_size=1)\n\n    # All in tensors\n    noise_vector = torch.from_numpy(noise_vector)\n    class_vector = torch.from_numpy(class_vector)\n\n    # If you have a GPU, put everything on cuda\n    noise_vector = noise_vector.to(\"cpu\")\n    class_vector = class_vector.to(\"cpu\")\n    generator.to(\"cpu\")\n\n    # Generate an image\n    with torch.no_grad():\n        output = generator(noise_vector, class_vector, truncation)\n    results = []\n    for img in convert_to_images(output):\n        buffered = BytesIO()\n        img.save(buffered, format=\"JPEG\")\n        base64_img_str = base64.b64encode(buffered.getvalue()).decode()\n        results.append(base64_img_str)\n    return results\n\n\n# https://huggingface.co/spaces/akhaliq/AnimeGANv2/blob/main/app.py\ndef load_anime_gan():\n    return torch.hub.load(\n        \"AK391/animegan2-pytorch:main\",\n        \"generator\",\n        pretrained=True,\n        device=\"cpu\",\n        progress=False,\n    ), torch.hub.load(\n        \"AK391/animegan2-pytorch:main\",\n        \"face2paint\",\n        size=512,\n        device=\"cpu\",\n        side_by_side=False,\n    )\n\n\nanime_gan_model, anime_gan_entrypoint = load_anime_gan()\n\n\ndef transfer(images: list):\n    results = []\n    for image_data in images:\n        input_image = Image.open(BytesIO(base64.b64decode(image_data))).convert(\"RGB\")\n        result_img = anime_gan_entrypoint(anime_gan_model, input_image)\n        buffered = BytesIO()\n        result_img.save(buffered, format=\"JPEG\")\n        base64_img_str = base64.b64encode(buffered.getvalue()).decode()\n        results.append(base64_img_str)\n    return results\n\n\nservice = Server()\nservice.register(\n    model_name=\"image-classifications\",\n    model=classify,\n    metadata={\"task\": task.IMAGE_CLASSIFICATION},\n    # metadata={\"task\": \"invalid\"},\n)\nservice.register(\n    model_name=\"image-classifications\",\n    model=classify,\n    version_name=\"v1\",\n    metadata={\"task\": task.IMAGE_CLASSIFICATION},\n)\nservice.register(\n    model_name=\"t5\",\n    model=translate,\n    version_name=\"v1\",\n    metadata={\"task\": task.TRANSLATION},\n)\nservice.register(\n    model_name=\"biggan\",\n    model=generate,\n    version_name=\"v1\",\n    metadata={\"task\": task.TEXT_TO_IMAGE},\n)\nservice.register(\n    model_name=\"anime-gan\",\n    model=transfer,\n    version_name=\"v1\",\n    metadata={\n        \"task\": task.IMAGE_STYLE_TRANSFER,\n        \"display_name\": \"Anime GAN\",\n        \"description\": \"This is anime GAN\",\n    },\n)\n"}
{"type": "source_file", "path": "examples/demo/prepare.py", "content": "import nltk\nimport torch\nfrom pytorch_pretrained_biggan import BigGAN\nfrom transformers import pipeline\n\nnltk.download(\"wordnet\")\nnltk.download(\"omw-1.4\")\n\nimage_classification_save_path = \"/tmp/google-vit\"\ntranlator_save_path = \"/tmp/google-t5\"\n\nclassifier = pipeline(model=\"google/vit-base-patch16-224\")\ntranslator = pipeline(model=\"t5-base\", tokenizer=\"t5-base\")\n\nclassifier.save_pretrained(image_classification_save_path)\ntranslator.save_pretrained(tranlator_save_path)\n\nBigGAN.from_pretrained(\"biggan-deep-256\")\n\ntorch.hub.load(\n    \"AK391/animegan2-pytorch:main\",\n    \"generator\",\n    pretrained=True,\n    device=\"cpu\",\n    progress=False,\n)\ntorch.hub.load(\n    \"AK391/animegan2-pytorch:main\",\n    \"face2paint\",\n    size=512,\n    device=\"cpu\",\n    side_by_side=False,\n)\n"}
{"type": "source_file", "path": "examples/doc/app.py", "content": "from pinferencia import Server\n\n\nclass MyModel:\n    def predict(self, data):\n        return sum(data)\n\n\nmodel = MyModel()\n\nservice = Server()\nservice.register(\n    model_name=\"mymodel\",\n    model=model,\n    entrypoint=\"predict\",\n)\n"}
{"type": "source_file", "path": "examples/huggingface/pipeline/nlp/bert/app.py", "content": "from transformers import pipeline\n\nfrom pinferencia import Server, task\n\nbert = pipeline(\"fill-mask\", model=\"bert-base-uncased\")\n\n\ndef predict(text: str) -> list:\n    return bert(text)\n\n\nservice = Server()\nservice.register(\n    model_name=\"bert\",\n    model=predict,\n    metadata={\"task\": task.TEXT_TO_TEXT},\n)\n"}
{"type": "source_file", "path": "examples/huggingface/pipeline/nlp/translation/app.py", "content": "from transformers import pipeline\n\nfrom pinferencia import Server, task\n\nt5 = pipeline(model=\"t5-base\", tokenizer=\"t5-base\")\n\n\ndef translate(text: list) -> list:\n    return [res[\"translation_text\"] for res in t5(text)]\n\n\nservice = Server()\nservice.register(model_name=\"t5\", model=translate, metadata={\"task\": task.TRANSLATION})\n"}
{"type": "source_file", "path": "examples/huggingface/pipeline/nlp/text_generation/app.py", "content": "from transformers import pipeline, set_seed\n\nfrom pinferencia import Server, task\n\ngenerator = pipeline(\"text-generation\", model=\"gpt2\")\nset_seed(42)\n\n\ndef predict(text: str) -> list:\n    return generator(text, max_length=50, num_return_sequences=3)\n\n\nservice = Server()\nservice.register(\n    model_name=\"gpt2\",\n    model=predict,\n    metadata={\"task\": task.TEXT_TO_TEXT},\n)\n"}
{"type": "source_file", "path": "examples/custom_template/app.py", "content": "from typing import List\n\nfrom pinferencia import Server\n\n\ndef stat(data: List[float]) -> dict:\n    return {\n        \"mean\": sum(data) / len(data),\n        \"max\": max(data),\n        \"min\": min(data),\n    }\n\n\nservice = Server()\nservice.register(model_name=\"stat\", model=stat, metadata={\"task\": \"Stat\"})\n"}
{"type": "source_file", "path": "examples/huggingface/pipeline/vision/app.py", "content": "from transformers import pipeline\n\nfrom pinferencia import Server, task\n\nvision_classifier = pipeline(task=\"image-classification\")\n\n\ndef classify(data: str) -> list:\n    return vision_classifier(images=data)\n\n\nservice = Server()\nservice.register(\n    model_name=\"vision\", model=classify, metadata={\"task\": task.TEXT_TO_TEXT}\n)\n"}
{"type": "source_file", "path": "examples/huggingface/pipeline/vision/base64_app.py", "content": "import base64\nfrom io import BytesIO\n\nfrom PIL import Image\nfrom transformers import pipeline\n\nfrom pinferencia import Server, task\n\nvision_classifier = pipeline(task=\"image-classification\")\n\n\ndef classify(images: list):\n    \"\"\"Image Classification\n\n    Args:\n        images (list): list of base64 encoded image strings\n\n    Returns:\n        list: list of classification results\n    \"\"\"\n    input_images = [Image.open(BytesIO(base64.b64decode(img))) for img in images]\n    return vision_classifier(images=input_images)\n\n\nservice = Server()\nservice.register(\n    model_name=\"vision\",\n    model=classify,\n    metadata={\"task\": task.IMAGE_CLASSIFICATION},\n)\n"}
{"type": "source_file", "path": "examples/json_model/app.py", "content": "\"\"\"This is the JSON Model Example in Documentation\"\"\"\nfrom pinferencia import Server, task\n\n\nclass JSONModel:\n    def predict(self, data: str) -> int:\n        knowledge = {\"a\": 1, \"b\": 2}\n        return knowledge.get(data, 0)\n\n\nmodel = JSONModel()\nservice = Server()\nservice.register(\n    model_name=\"json\",\n    model=model,\n    entrypoint=\"predict\",\n    metadata={\"task\": task.TEXT_TO_TEXT},\n)\n"}
{"type": "source_file", "path": "examples/mountain/app.py", "content": "from typing import List\n\nfrom pinferencia import Server\n\n\ndef calc(data: List[int]) -> int:\n    highest = max(data)\n    lowest = min(data)\n    return highest - lowest\n\n\nservice = Server()\nservice.register(model_name=\"mountain\", model=calc)\n"}
{"type": "source_file", "path": "examples/paddlepaddle/image/image_classification/app.py", "content": "import base64\n\nimport cv2\nimport numpy as np\nimport paddlehub as hub\n\nfrom pinferencia import Server, task\n\nclassifier = hub.Module(name=\"mobilenet_v2_animals\")\n\n\ndef base64_str_to_cv2(base64_str: str) -> np.ndarray:\n    return cv2.imdecode(\n        np.fromstring(base64.b64decode(base64_str), np.uint8), cv2.IMREAD_COLOR\n    )\n\n\ndef predict(data: list) -> list:\n    images = [base64_str_to_cv2(base64_img_str) for base64_img_str in data]\n    return classifier.classification(images=images)\n\n\nservice = Server()\nservice.register(\n    model_name=\"classifier\",\n    model=predict,\n    metadata={\"task\": task.IMAGE_TO_TEXT},\n)\n"}
{"type": "source_file", "path": "examples/paddlepaddle/image/face_detection/app.py", "content": "import base64\n\nimport cv2\nimport numpy as np\nimport paddlehub as hub\n\nfrom pinferencia import Server\n\nface_detector = hub.Module(name=\"pyramidbox_lite_server\")\n\n\ndef base64_str_to_cv2(base64_str: str) -> np.ndarray:\n    return cv2.imdecode(\n        np.fromstring(base64.b64decode(base64_str), np.uint8), cv2.IMREAD_COLOR\n    )\n\n\ndef predict(base64_img_str: str):\n    return face_detector.face_detection(\n        images=[base64_str_to_cv2(base64_img_str)], visualization=True, output_dir=\"./\"\n    )\n\n\nservice = Server()\nservice.register(model_name=\"face_detector\", model=predict)\n"}
{"type": "source_file", "path": "examples/paddlepaddle/text/text_generation/app.py", "content": "import paddlehub as hub\n\nfrom pinferencia import Server, task\n\ntext_generation = hub.Module(name=\"ernie_gen_poetry\")\n\n\ndef predict(texts: list) -> list:\n    return text_generation.generate(texts=texts, beam_width=5)\n\n\nservice = Server()\nservice.register(\n    model_name=\"text_generation\",\n    model=predict,\n    metadata={\"task\": task.TEXT_TO_TEXT},\n)\n"}
{"type": "source_file", "path": "pinferencia/apis/__init__.py", "content": ""}
{"type": "source_file", "path": "examples/pytorch/mnist/path_app.py", "content": "import base64\nimport pathlib\nfrom io import BytesIO\n\nimport torch\nfrom main import Net\nfrom PIL import Image\nfrom torchvision import transforms\n\nfrom pinferencia import Server, task\nfrom pinferencia.handlers import BaseHandler\n\n\nclass MNISTHandler(BaseHandler):\n    transform = transforms.Compose(\n        [\n            transforms.ToTensor(),\n            transforms.Normalize((0.1307,), (0.3081,)),\n        ]\n    )\n    use_cuda = torch.cuda.is_available()\n    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n\n    def load_model(self):\n        model = Net().to(self.device)\n        model.load_state_dict(torch.load(self.model_path))\n        model.eval()\n        return model\n\n    def predict(self, data):\n        image = Image.open(BytesIO(base64.b64decode(data)))\n        tensor = self.transform(image)\n        input_data = torch.stack([tensor]).to(self.device)\n        return self.model(input_data).argmax(1).tolist()[0]\n\n\nservice = Server(model_dir=pathlib.Path(__file__).parent.resolve())\nservice.register(\n    model_name=\"mnist\",\n    model=\"mnist_cnn.pt\",\n    handler=MNISTHandler,\n    load_now=True,\n    metadata={\"task\": task.IMAGE_TO_TEXT},\n)\n"}
{"type": "source_file", "path": "examples/paddlepaddle/image/image_generation/app.py", "content": "import base64\nfrom io import BytesIO\n\nimport paddlehub as hub\nfrom PIL import Image\n\nfrom pinferencia import Server, task\nfrom pinferencia.tools import base64_str_to_cv2\n\nimage_generation = hub.Module(name=\"Photo2Cartoon\")\n\n\ndef predict(base64_img_str: str) -> str:\n    result = image_generation.Cartoon_GEN(\n        images=[base64_str_to_cv2(base64_img_str)], visualization=True, output_dir=\"./\"\n    )\n    pil_img = Image.fromarray(result[0])\n    buff = BytesIO()\n    pil_img.save(buff, format=\"JPEG\")\n    return base64.b64encode(buff.getvalue()).decode(\"utf-8\")\n\n\nservice = Server()\nservice.register(\n    model_name=\"image_generation\",\n    model=predict,\n    metadata={\"task\": task.IMAGE_TO_IMAGE},\n)\n"}
{"type": "source_file", "path": "pinferencia/apis/default/v1/routers/__init__.py", "content": "from fastapi import APIRouter\n\nfrom .basic import router as basic_router\nfrom .management import router as management_router\nfrom .metadata import router as metadata_router\n\nrouter = APIRouter()\n\nrouter.include_router(basic_router)\nrouter.include_router(metadata_router)\nrouter.include_router(management_router)\n"}
{"type": "source_file", "path": "pinferencia/apis/default/v1/config.py", "content": "SCHEME = \"default\"\n"}
{"type": "source_file", "path": "pinferencia/apis/default/__init__.py", "content": "import hashlib\nimport logging\nimport typing\n\nfrom fastapi import APIRouter, Request\nfrom pydantic import create_model\n\nfrom pinferencia.api_manager import BaseAPIManager\nfrom pinferencia.context import PredictContext\nfrom pinferencia.repository import DefaultVersionName, ModelRepository\n\nfrom .index import router as index_router\nfrom .v1 import router as v1router\nfrom .v1.config import SCHEME\nfrom .v1.models import RequestBase as V1RequestBase\nfrom .v1.models import ResponseBase as V1ResponseBase\n\nlogger = logging.getLogger(\"uvicorn\")\n\n\nclass APIManager(BaseAPIManager):\n    def register_route(self):\n        self.app.include_router(index_router)\n        self.app.include_router(\n            v1router,\n            prefix=\"/v1\",\n            tags=[\"V1\"],\n        )\n\n        self.request_models = {}\n        self.response_models = {}\n\n    def register_model_endpoint(\n        self,\n        model_name: str,\n        model_repository: ModelRepository,\n        version_name: str = None,\n    ):\n        logger.info(\"Registering Model Endpoint for [%s-%s]\", model_name, version_name)\n\n        # url path to register the model\n        paths = []\n        path = f\"/models/{model_name}\"\n        if version_name:\n            paths.append(path + f\"/versions/{version_name}/predict\")\n        else:\n            paths.append(path + \"/predict\")\n            paths.append(path + f\"/versions/{DefaultVersionName}/predict\")\n\n        # get the model type hint schema of the model\n        model_schema = model_repository.get_model_schema(\n            model_name=model_name,\n            version_name=version_name,\n        )\n\n        # get the input type and output type of the model entrypoint\n        request_type = model_schema.get(\"input_type\") or typing.Any\n        response_type = model_schema.get(\"output_type\") or typing.Any\n\n        # set version name to default if it is null\n        version_name = version_name or DefaultVersionName\n\n        # unique name to create the pydantic model\n        unique_name = (\n            hashlib.md5(model_name.encode()).hexdigest()\n            + hashlib.md5(version_name.encode()).hexdigest()\n        )\n\n        # create request model\n        req_model = create_model(\n            \"V1RequestModel\" + unique_name,\n            data=(request_type, ...),\n            __base__=V1RequestBase,\n        )\n\n        # create response model\n        resp_model = create_model(\n            \"V1V1ResponseModel\" + unique_name,\n            data=(response_type, ...),\n            __base__=V1ResponseBase,\n        )\n\n        # save the model into api manager\n        self.request_models[f\"{model_name}-{version_name}\"] = req_model\n        self.response_models[f\"{model_name}-{version_name}\"] = resp_model\n\n        # template predict endpoint function to dynamically serve different models\n        def predict(\n            request: Request,\n            inference_request: req_model,\n        ):\n            return {\n                \"id\": inference_request.id,\n                \"model_name\": model_name,\n                \"model_version\": version_name,\n                \"data\": request.app.model.predict(\n                    model_name,\n                    data=inference_request.data,\n                    parameters=inference_request.parameters,\n                    version_name=version_name,\n                    context=PredictContext(\n                        scheme=SCHEME, request_data=inference_request.dict()\n                    ),\n                ),\n            }\n\n        # register the route and add to the app\n        router = APIRouter()\n        for path in paths:\n            router.add_api_route(\n                path,\n                predict,\n                methods=[\"post\"],\n                summary=f\"{model_name.title()} {version_name.title()}\",\n                response_model=resp_model,\n                response_model_exclude_unset=True,\n                response_model_exclude_none=True,\n            )\n        self.app.include_router(\n            router,\n            prefix=\"/v1\",\n            tags=[\"V1 - Predict\"],\n        )\n"}
{"type": "source_file", "path": "pinferencia/api_manager.py", "content": "import abc\nimport logging\n\nfrom fastapi import FastAPI\n\nfrom pinferencia.repository import ModelRepository\n\nlogger = logging.getLogger(\"uvicorn\")\n\n\nclass BaseAPIManager(abc.ABC):\n    app = None\n\n    def __init__(self, app: FastAPI):\n        super().__init__()\n        self.app = app\n\n    @abc.abstractmethod\n    def register_route(self):\n        return NotImplemented\n\n    def validate_model_metadata(\n        self,\n        model_name: str,\n        metadata: dict,\n        version_name: str = None,\n    ) -> list:\n        errors = []\n        if metadata is not None and not isinstance(metadata, dict):\n            error_msg = \"metadata is not a dict.\"\n            logger.error(error_msg)\n            errors.append(error_msg)\n        return errors\n\n    def register_model_endpoint(\n        self,\n        model_name: str,\n        model_repository: ModelRepository,\n        version_name: str = None,\n    ):\n        pass\n"}
{"type": "source_file", "path": "examples/pytorch/mnist/func_app.py", "content": "import base64\nfrom io import BytesIO\n\nimport torch\nfrom main import Net\nfrom PIL import Image\nfrom torchvision import transforms\n\nfrom pinferencia import Server, task\n\nuse_cuda = torch.cuda.is_available()\ndevice = torch.device(\"cuda\" if use_cuda else \"cpu\")\n\ntransform = transforms.Compose(\n    [\n        transforms.ToTensor(),\n        transforms.Normalize((0.1307,), (0.3081,)),\n    ]\n)\n\n\nmodel = Net().to(device)\nmodel.load_state_dict(torch.load(\"mnist_cnn.pt\"))\nmodel.eval()\n\n\ndef preprocessing(img_str):\n    image = Image.open(BytesIO(base64.b64decode(img_str)))\n    tensor = transform(image)\n    return torch.stack([tensor]).to(device)\n\n\ndef predict(data):\n    return model(preprocessing(data)).argmax(1).tolist()[0]\n\n\nservice = Server()\nservice.register(\n    model_name=\"mnist\",\n    model=predict,\n    metadata={\"task\": task.IMAGE_TO_TEXT},\n)\n"}
{"type": "source_file", "path": "pinferencia/apis/default/v1/__init__.py", "content": "from .routers import router  # noqa\n"}
{"type": "source_file", "path": "pinferencia/apis/kserve/v1/config.py", "content": "SCHEME = \"kservev1\"\n"}
{"type": "source_file", "path": "examples/paddlepaddle/image/object_detection/app.py", "content": "from pinferencia import Server\n\nimport paddlehub as hub\nimport cv2\n\nvehicle_detection = hub.Module(name=\"yolov3_darknet53_vehicles\")\n\n\ndef predict(path: str):\n    return vehicle_detection.object_detection(\n        images=[cv2.imread(path)], visualization=True, output_dir=\"./\"\n    )\n\n\nservice = Server()\nservice.register(model_name=\"vehicle_detection\", model=predict)\n"}
{"type": "source_file", "path": "pinferencia/__init__.py", "content": "from .app import Server  # noqa\n"}
{"type": "source_file", "path": "pinferencia/apis/default/v1/models.py", "content": "from typing import Any, List, Optional\n\nfrom pydantic import BaseModel, Extra\n\n\nclass ModelVersion(BaseModel):\n    name: str\n    display_name: Optional[str] = \"\"\n    description: Optional[str] = \"\"\n    platform: Optional[str] = \"\"\n    device: Optional[str] = \"\"\n    task: Optional[str] = \"\"\n    input_type: Optional[str] = \"\"\n    output_type: Optional[str] = \"\"\n\n\nclass Model(BaseModel):\n    name: str\n    versions: Optional[List[ModelVersion]] = []\n\n\nclass RequestBase(BaseModel, extra=Extra.forbid):\n    id: Optional[str] = None\n    parameters: Optional[dict] = {}\n\n\nclass Request(RequestBase):\n    data: Any\n\n\nclass ResponseBase(BaseModel):\n    id: Optional[str] = None\n    model_name: str\n    model_version: Optional[str] = None\n    parameters: Optional[dict] = {}\n\n\nclass Response(ResponseBase):\n    data: Any\n"}
{"type": "source_file", "path": "examples/pytorch/mnist/sum_mnist.py", "content": "import base64\nimport pathlib\nfrom io import BytesIO\n\nimport torch\nfrom main import Net\nfrom PIL import Image\nfrom torchvision import transforms\n\nfrom pinferencia import Server\nfrom pinferencia.handlers import BaseHandler\n\n\nclass MNISTHandler(BaseHandler):\n    transform = transforms.Compose(\n        [\n            transforms.ToTensor(),\n            transforms.Normalize((0.1307,), (0.3081,)),\n        ]\n    )\n    use_cuda = torch.cuda.is_available()\n    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n\n    def load_model(self):\n        model = Net().to(self.device)\n        model.load_state_dict(torch.load(self.model_path))\n        model.eval()\n        return model\n\n    def predict(self, data: list) -> int:\n        tensors = []\n        for img in data:\n            image = Image.open(BytesIO(base64.b64decode(img)))\n            tensors.append(self.transform(image))\n        input_data = torch.stack(tensors).to(self.device)\n        return sum(self.model(input_data).argmax(1).tolist())\n\n\nservice = Server(model_dir=pathlib.Path(__file__).parent.resolve())\nservice.register(\n    model_name=\"mnist\",\n    model=\"mnist_cnn.pt\",\n    handler=MNISTHandler,\n    load_now=True,\n    metadata={\"task\": \"Sum Mnist\"},\n)\n"}
{"type": "source_file", "path": "pinferencia/apis/default/v1/routers/basic.py", "content": "from fastapi import APIRouter\n\nrouter = APIRouter()\n\n\n@router.get(\"/healthz\")\nasync def healthz():\n    return True\n"}
{"type": "source_file", "path": "pinferencia/apis/default/index.py", "content": "from fastapi import APIRouter, Request\nfrom fastapi.openapi.docs import get_swagger_ui_html\nfrom starlette.responses import RedirectResponse\n\nrouter = APIRouter()\n\n\n@router.get(\n    \"/\",\n    response_class=RedirectResponse,\n    include_in_schema=False,\n)\nasync def home():\n    return RedirectResponse(url=\"/docs\")\n\n\n@router.get(\"/docs\", include_in_schema=False)\nasync def custom_swagger_ui_html(request: Request):\n    return get_swagger_ui_html(\n        openapi_url=request.app.openapi_url,\n        title=request.app.title + \" - Swagger UI\",\n        swagger_js_url=\"/static/swagger-ui-bundle.js\",\n        swagger_css_url=f\"/static/theme-{request.app.swagger_theme}.css\",\n        swagger_ui_parameters=request.app.swagger_ui_parameters,\n    )\n"}
{"type": "source_file", "path": "examples/pytorch/mnist/get-base64-img.py", "content": "import base64\nimport random\nfrom io import BytesIO\n\nfrom PIL import Image\nfrom torchvision import datasets\n\ndataset = datasets.MNIST(\n    \"./data\",\n    train=True,\n    download=True,\n    transform=None,\n)\nindex = random.randint(0, len(dataset.data))\nimg = dataset.data[index]\nimg = Image.fromarray(img.numpy(), mode=\"L\")\n\nbuffered = BytesIO()\nimg.save(buffered, format=\"JPEG\")\nbase64_img_str = base64.b64encode(buffered.getvalue()).decode()\nprint(\"Base64 String:\", base64_img_str)\nprint(\"target:\", dataset.targets[index].tolist())\n"}
{"type": "source_file", "path": "examples/pytorch/mnist/sum_mnist_frontend.py", "content": "import base64\n\nimport streamlit as st\nfrom PIL import Image\n\nfrom pinferencia.frontend.app import Server\nfrom pinferencia.frontend.templates.base import BaseTemplate\nfrom pinferencia.frontend.templates.utils import display_text_prediction\n\n\nclass SumMnistTemplate(BaseTemplate):\n    title = (\n        '<span style=\"color:salmon;\">Sum</span> '\n        '<span style=\"color:slategray;\">MNIST</span> '\n    )\n\n    def render(self):\n        super().render()\n\n        col1, col2 = st.columns(2)\n        with col1.form(\"First Image\", clear_on_submit=True):\n            first_number = col1.file_uploader(\n                \"Choose an image...\", type=[\"jpg\", \"png\", \"jpeg\"], key=\"1\"\n            )\n\n        with col2.form(\"Second Image\", clear_on_submit=True):\n            second_number = col2.file_uploader(\n                \"Choose an image...\", type=[\"jpg\", \"png\", \"jpeg\"], key=\"2\"\n            )\n\n        st.markdown(\"##### Sum of the two digits:\")\n        images = []\n        if first_number is not None:\n            image1 = Image.open(first_number)\n            col1.image(image1, use_column_width=True)\n            images.append(base64.b64encode(first_number.getvalue()).decode())\n\n        if second_number is not None:\n            image1 = Image.open(second_number)\n            col2.image(image1, use_column_width=True)\n            images.append(base64.b64encode(second_number.getvalue()).decode())\n\n        if first_number and second_number:\n            with st.spinner(\"Waiting for result\"):\n                prediction = self.predict(images)\n                display_text_prediction(prediction, component=st)\n\n\nbackend_address = \"http://127.0.0.1:8000\"\n\nservice = Server(\n    backend_server=f\"{backend_address}\",\n    custom_templates={\"Sum Mnist\": SumMnistTemplate},\n)\n"}
{"type": "source_file", "path": "pinferencia/apis/kserve/v1/routers/predict.py", "content": "from fastapi import APIRouter, Request\n\nfrom pinferencia.apis.kserve.v1.config import SCHEME\nfrom pinferencia.apis.kserve.v1.models import Request as InferenceRequest\nfrom pinferencia.apis.kserve.v1.models import Response as InferenceResponse\nfrom pinferencia.apis.kserve.v1.parsers import InputParser, OutputParser\nfrom pinferencia.context import PredictContext\n\nrouter = APIRouter()\n\n\n@router.post(\n    \"/models/{model_name}/infer\",\n    response_model=InferenceResponse,\n    response_model_exclude_unset=True,\n    response_model_exclude_none=True,\n)\ndef model_predict(\n    request: Request,\n    model_name: str,\n    inference_request: InferenceRequest,\n):\n    return {\n        \"id\": inference_request.id,\n        \"model_name\": model_name,\n        \"predictions\": OutputParser(\n            request.app.model.predict(\n                model_name,\n                data=InputParser(inference_request).data,\n                parameters=inference_request.parameters,\n                context=PredictContext(\n                    scheme=SCHEME, request_data=inference_request.dict()\n                ),\n            ),\n        ).data,\n    }\n\n\n@router.post(\n    \"/models/{model_name}/versions/{version_name}/infer\",\n    response_model=InferenceResponse,\n    response_model_exclude_unset=True,\n    response_model_exclude_none=True,\n)\ndef model_version_predict(\n    request: Request,\n    model_name: str,\n    version_name: str,\n    inference_request: InferenceRequest,\n):\n    return {\n        \"id\": inference_request.id,\n        \"model_name\": model_name,\n        \"model_version\": version_name,\n        \"predictions\": OutputParser(\n            request.app.model.predict(\n                model_name,\n                data=InputParser(inference_request).data,\n                parameters=inference_request.parameters,\n                version_name=version_name,\n                context=PredictContext(\n                    scheme=SCHEME, request_data=inference_request.dict()\n                ),\n            ),\n        ).data,\n    }\n"}
{"type": "source_file", "path": "pinferencia/apis/kserve/v1/__init__.py", "content": "from .routers import router  # noqa\n"}
{"type": "source_file", "path": "pinferencia/apis/default/v1/routers/metadata.py", "content": "from typing import List\n\nfrom fastapi import APIRouter, Request\n\nfrom pinferencia.apis.default.v1.models import Model, ModelVersion\n\nrouter = APIRouter()\n\n\n@router.get(\"/models\", response_model=List[Model])\nasync def list_models(request: Request):\n    return request.app.model.repository.list_models()\n\n\n@router.get(\"/models/{model_name}\", response_model=List[ModelVersion])\nasync def list_model_versions(request: Request, model_name: str):\n    return request.app.model.repository.list_models(model_name)\n"}
{"type": "source_file", "path": "pinferencia/apis/kserve/__init__.py", "content": "import logging\n\nfrom pydantic import ValidationError\n\nfrom pinferencia.api_manager import BaseAPIManager\nfrom pinferencia.apis.default.index import router as index_router\n\nfrom .v1 import router as v1router\nfrom .v2 import router as v2router\nfrom .v2.models import ModelVersion\n\nlogger = logging.getLogger(\"uvicorn\")\n\n\nclass APIManager(BaseAPIManager):\n    def register_route(self):\n        self.app.include_router(index_router)\n        self.app.include_router(\n            v1router,\n            prefix=\"/v1\",\n            tags=[\"V1\"],\n        )\n        self.app.include_router(\n            v2router,\n            prefix=\"/v2\",\n            tags=[\"V2\"],\n        )\n\n    def validate_model_metadata(\n        self, model_name: str, metadata: object, version_name: str = None\n    ) -> list:\n        errors = super().validate_model_metadata(\n            model_name=model_name,\n            metadata=metadata,\n            version_name=version_name,\n        )\n        if errors:\n            return errors\n        metadata = {} if metadata is None else metadata\n        errors.append(\n            self.validate_v2_metadata(model_name=model_name, metadata=metadata)\n        )\n        return [e for e in errors if e]\n\n    def validate_v2_metadata(self, model_name: str, metadata: dict):\n        try:\n            ModelVersion(\n                name=model_name,\n                platform=metadata.get(\"platform\", \"\"),\n                inputs=metadata.get(\"inputs\", []),\n                outputs=metadata.get(\"outputs\", []),\n            )\n        except ValidationError as error:\n            logger.exception(\"Failed to pass kserve v2 metadata validation\")\n            return error\n"}
{"type": "source_file", "path": "pinferencia/apis/kserve/v1/routers/__init__.py", "content": "from fastapi import APIRouter\n\nfrom pinferencia.apis.default.v1.routers.basic import router as basic_router\nfrom pinferencia.apis.default.v1.routers.management import (\n    router as mangement_router,\n)\n\nfrom .metadata import router as metadata_router\nfrom .predict import router as predict_router\n\nrouter = APIRouter()\nrouter.include_router(basic_router)\nrouter.include_router(metadata_router)\nrouter.include_router(mangement_router)\nrouter.include_router(predict_router)\n"}
{"type": "source_file", "path": "pinferencia/apis/default/v1/routers/management.py", "content": "from fastapi import APIRouter, Request\n\nrouter = APIRouter()\n\n\n@router.get(\"/models/{model_name}/ready\", response_model=bool)\nasync def model_is_ready(request: Request, model_name: str):\n    \"\"\"Indicate Whether the Model is Ready for Prediction\n\n    You need to pass the model name registered for the model in\n    the URL.\n\n    - If the API returns **ture**, it means the model is ready.\n    - If the API returns **false**, it means the model is not ready.\n      You need to call the **Load Model** API to load the model.\n    \"\"\"\n    return request.app.model.repository.is_ready(model_name)\n\n\n@router.get(\"/models/{model_name}/versions/{version_name}/ready\", response_model=bool)\nasync def model_version_is_ready(\n    request: Request,\n    model_name: str,\n    version_name: str,\n):\n    return request.app.model.repository.is_ready(model_name, version_name)\n\n\n@router.post(\"/models/{model_name}/load\", response_model=bool)\nasync def load_model(request: Request, model_name: str):\n    return request.app.model.repository.load_model(model_name)\n\n\n@router.post(\"/models/{model_name}/versions/{version_name}/load\", response_model=bool)\nasync def load_version(request: Request, model_name: str, version_name: str):\n    return request.app.model.repository.load_model(\n        model_name, version_name=version_name\n    )\n\n\n@router.post(\"/models/{model_name}/unload\", response_model=bool)\nasync def unload_model(request: Request, model_name: str):\n    return request.app.model.repository.unload_model(model_name)\n\n\n@router.post(\"/models/{model_name}/versions/{version_name}/unload\", response_model=bool)\nasync def unload_version(request: Request, model_name: str, version_name: str):\n    return request.app.model.repository.unload_model(\n        model_name, version_name=version_name\n    )\n"}
{"type": "source_file", "path": "pinferencia/apis/kserve/v1/models.py", "content": "from typing import Any, List, Optional\n\nfrom pydantic import BaseModel, Extra\n\n\nclass ModelVersion(BaseModel):\n    name: str\n    display_name: Optional[str] = \"\"\n    description: Optional[str] = \"\"\n    platform: Optional[str] = \"\"\n    task: Optional[str] = \"\"\n    input_type: Optional[str] = \"\"\n    output_type: Optional[str] = \"\"\n\n\nclass Model(BaseModel):\n    name: str\n    versions: Optional[List[ModelVersion]] = []\n    platform: Optional[str] = \"\"\n\n\nclass Request(BaseModel, extra=Extra.forbid):\n    id: Optional[str] = None\n    parameters: Optional[dict] = {}\n    instances: Any\n\n\nclass Response(BaseModel):\n    id: Optional[str] = None\n    model_name: str\n    model_version: Optional[str] = None\n    predictions: Any\n"}
{"type": "source_file", "path": "pinferencia/apis/kserve/v1/routers/metadata.py", "content": "from typing import List\n\nfrom fastapi import APIRouter, Request\n\nfrom pinferencia.apis.kserve.v1.models import Model, ModelVersion\n\nrouter = APIRouter()\n\n\n@router.get(\"/models\", response_model=List[Model])\nasync def list_models(request: Request):\n    return request.app.model.repository.list_models()\n\n\n@router.get(\"/models/{model_name}\", response_model=List[ModelVersion])\nasync def list_model_versions(request: Request, model_name: str):\n    return request.app.model.repository.list_models(model_name)\n"}
{"type": "source_file", "path": "pinferencia/apis/kserve/v1/parsers.py", "content": "import logging\n\nimport numpy as np\n\nfrom .models import Request\n\nlogger = logging.getLogger(\"uvicorn\")\n\n\nclass InputParser:\n    def __init__(self, request: Request):\n        super().__init__()\n        self.inputs = request.instances\n\n    @property\n    def data(self):\n        return self.inputs\n\n\nclass OutputParser:\n    def __init__(self, raw_data):\n        \"\"\"Instantiate an Output Parser\n\n        Args:\n            raw_data (object): Prediction Result\n        \"\"\"\n        self.raw_data = raw_data\n\n    @property\n    def data(self):\n        npdata = np.array(self.raw_data)\n        return npdata.tolist()\n"}
{"type": "source_file", "path": "pinferencia/apis/default/v1/parsers.py", "content": ""}
{"type": "source_file", "path": "examples/paddlepaddle/image/semantic_segmentation/app.py", "content": "import base64\nfrom io import BytesIO\n\nimport cv2\nimport numpy as np\nimport paddlehub as hub\nfrom PIL import Image\n\nfrom pinferencia import Server, task\n\nsemantic_segmentation = hub.Module(name=\"ExtremeC3_Portrait_Segmentation\")\n\n\ndef base64_str_to_cv2(base64_str: str) -> np.ndarray:\n    return cv2.imdecode(\n        np.fromstring(base64.b64decode(base64_str), np.uint8), cv2.IMREAD_COLOR\n    )\n\n\ndef predict(base64_img_str: str) -> str:\n    images = [base64_str_to_cv2(base64_img_str)]\n    result = semantic_segmentation.Segmentation(\n        images=images,\n        output_dir=\"./\",\n        visualization=True,\n    )\n    pil_img = Image.fromarray(result[0][\"result\"])\n    buff = BytesIO()\n    pil_img.save(buff, format=\"JPEG\")\n    return base64.b64encode(buff.getvalue()).decode(\"utf-8\")\n\n\nservice = Server()\nservice.register(\n    model_name=\"semantic_segmentation\",\n    model=predict,\n    metadata={\"task\": task.IMAGE_TO_IMAGE},\n)\n"}
{"type": "source_file", "path": "examples/paddlepaddle/text/simultaneous_translation/app.py", "content": "import paddlehub as hub\n\nfrom pinferencia import Server\n\nsimultaneous_translation = hub.Module(name=\"transformer_nist_wait_1\")\n\n\ndef predict(text: list):\n    for t in text:\n        print(f\"input: {t}\")\n        result = simultaneous_translation.translate(t)\n        print(f\"model output: {result}\")\n\n\nservice = Server()\nservice.register(model_name=\"simultaneous_translation\", model=predict)\n"}
{"type": "source_file", "path": "examples/paddlepaddle/text/emotion_detection_textcnn/app.py", "content": "import paddlehub as hub\n\nfrom pinferencia import Server, task\n\nemotion_detection_textcnn = hub.Module(name=\"emotion_detection_textcnn\")\n\n\ndef predict(text: list) -> list:\n    return emotion_detection_textcnn.emotion_classify(texts=text)\n\n\nservice = Server()\nservice.register(\n    model_name=\"emotion_detection_textcnn\",\n    model=predict,\n    metadata={\"task\": task.TEXT_TO_TEXT},\n)\n"}
{"type": "source_file", "path": "examples/paddlepaddle/image/text_recognition/app.py", "content": "from pinferencia import Server\n\nimport paddlehub as hub\nimport cv2\n\nocr = hub.Module(\n    name=\"chinese_ocr_db_crnn_mobile\", enable_mkldnn=True\n)  # mkldnn acceleration only works under CPU\n\n\ndef predict(path: str):\n    return ocr.recognize_text(\n        images=[cv2.imread(path)], visualization=True, output_dir=\"./\"\n    )\n\n\nservice = Server()\nservice.register(model_name=\"ocr\", model=predict)\n"}
{"type": "source_file", "path": "examples/paddlepaddle/text/lexical_analysis/app.py", "content": "import paddlehub as hub\n\nfrom pinferencia import Server, task\n\nlexical_analysis = hub.Module(name=\"jieba_paddle\")\n\n\ndef predict(text: str) -> list:\n    return lexical_analysis.cut(text, cut_all=False, HMM=True)\n\n\nservice = Server()\nservice.register(\n    model_name=\"lexical_analysis\", model=predict, metadata={\"task\": task.TEXT_TO_TEXT}\n)\n"}
