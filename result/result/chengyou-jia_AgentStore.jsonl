{"repo_info": {"repo_name": "AgentStore", "repo_owner": "chengyou-jia", "repo_url": "https://github.com/chengyou-jia/AgentStore"}}
{"type": "test_file", "path": "agentstore/utils/test_new_llms.py", "content": "# This code is based on Open Interpreter. Original source: https://github.com/OpenInterpreter/open-interpreter\n\n\nimport base64\nimport io\nimport os\nimport json\nimport time\n\nfrom PIL import Image\n\nfrom rich import print as rich_print\nfrom rich.markdown import Markdown\nfrom rich.rule import Rule\n\nfrom dotenv import load_dotenv\n\nimport litellm\nimport tokentrim as tt\nlitellm.suppress_debug_info = True\n\n\nload_dotenv(override=True)\nMODEL_NAME = os.getenv('MODEL_NAME')\nOPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\nOPENAI_ORGANIZATION = os.getenv('OPENAI_ORGANIZATION')\nBASE_URL = os.getenv('OPENAI_BASE_URL')\n\n\nfunction_schema = {\n    \"name\": \"execute\",\n    \"description\": \"Executes code on the user's machine **in the users local environment** and returns the output\",\n    \"parameters\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"language\": {\n                \"type\": \"string\",\n                \"description\": \"The programming language (required parameter to the `execute` function)\",\n                \"enum\": [\n                    # This will be filled dynamically with the languages OI has access to.\n                ],\n            },\n            \"code\": {\"type\": \"string\", \"description\": \"The code to execute (required)\"},\n        },\n        \"required\": [\"language\", \"code\"],\n    },\n}\n\n\ndef parse_partial_json(s):\n    # Attempt to parse the string as-is.\n    try:\n        return json.loads(s)\n    except:\n        pass\n\n    # Initialize variables.\n    new_s = \"\"\n    stack = []\n    is_inside_string = False\n    escaped = False\n\n    # Process each character in the string one at a time.\n    for char in s:\n        if is_inside_string:\n            if char == '\"' and not escaped:\n                is_inside_string = False\n            elif char == \"\\n\" and not escaped:\n                char = \"\\\\n\"  # Replace the newline character with the escape sequence.\n            elif char == \"\\\\\":\n                escaped = not escaped\n            else:\n                escaped = False\n        else:\n            if char == '\"':\n                is_inside_string = True\n                escaped = False\n            elif char == \"{\":\n                stack.append(\"}\")\n            elif char == \"[\":\n                stack.append(\"]\")\n            elif char == \"}\" or char == \"]\":\n                if stack and stack[-1] == char:\n                    stack.pop()\n                else:\n                    # Mismatched closing character; the input is malformed.\n                    return None\n\n        # Append the processed character to the new string.\n        new_s += char\n\n    # If we're still inside a string at the end of processing, we need to close the string.\n    if is_inside_string:\n        new_s += '\"'\n\n    # Close any remaining open structures in the reverse order that they were opened.\n    for closing_char in reversed(stack):\n        new_s += closing_char\n\n    # Attempt to parse the modified string as JSON.\n    try:\n        return json.loads(new_s)\n    except:\n        # If we still can't parse the string as JSON, return None to indicate failure.\n        return None\n\n\ndef merge_deltas(original, delta):\n    \"\"\"\n    Pushes the delta into the original and returns that.\n\n    Great for reconstructing OpenAI streaming responses -> complete message objects.\n    \"\"\"\n\n    for key, value in dict(delta).items():\n        if value != None:\n            if isinstance(value, str):\n                if key in original:\n                    original[key] = (original[key] or \"\") + (value or \"\")\n                else:\n                    original[key] = value\n            else:\n                value = dict(value)\n                if key not in original:\n                    original[key] = value\n                else:\n                    merge_deltas(original[key], value)\n\n    return original\n\n\ndef run_function_calling_llm(llm, request_params):\n    ## Setup\n\n    # # Add languages OI has access to\n    # function_schema[\"parameters\"][\"properties\"][\"language\"][\"enum\"] = [\n    #     i.name.lower() for i in llm.interpreter.computer.terminal.languages\n    # ]\n    # request_params[\"functions\"] = [function_schema]\n\n    # # Add OpenAI's recommended function message\n    # request_params[\"messages\"][0][\n    #     \"content\"\n    # ] += \"\\nUse ONLY the function you have been provided with — 'execute(language, code)'.\"\n\n    ## Convert output to LMC format\n\n    accumulated_deltas = {}\n    language = None\n    code = \"\"\n\n    for chunk in llm.completions(**request_params):\n        if \"choices\" not in chunk or len(chunk[\"choices\"]) == 0:\n            # This happens sometimes\n            continue\n\n        delta = chunk[\"choices\"][0][\"delta\"]\n\n        # Accumulate deltas\n        accumulated_deltas = merge_deltas(accumulated_deltas, delta)\n\n        if \"content\" in delta and delta[\"content\"]:\n            yield {\"type\": \"message\", \"content\": delta[\"content\"]}\n\n        if (\n            accumulated_deltas.get(\"function_call\")\n            and \"arguments\" in accumulated_deltas[\"function_call\"]\n            and accumulated_deltas[\"function_call\"][\"arguments\"]\n        ):\n            if (\n                \"name\" in accumulated_deltas[\"function_call\"]\n                and accumulated_deltas[\"function_call\"][\"name\"] == \"execute\"\n            ):\n                arguments = accumulated_deltas[\"function_call\"][\"arguments\"]\n                arguments = parse_partial_json(arguments)\n\n                if arguments:\n                    if (\n                        language is None\n                        and \"language\" in arguments\n                        and \"code\"\n                        in arguments  # <- This ensures we're *finished* typing language, as opposed to partially done\n                        and arguments[\"language\"]\n                    ):\n                        language = arguments[\"language\"]\n\n                    if language is not None and \"code\" in arguments:\n                        # Calculate the delta (new characters only)\n                        code_delta = arguments[\"code\"][len(code) :]\n                        # Update the code\n                        code = arguments[\"code\"]\n                        # Yield the delta\n                        if code_delta:\n                            yield {\n                                \"type\": \"code\",\n                                \"format\": language,\n                                \"content\": code_delta,\n                            }\n                else:\n                    if llm.interpreter.verbose:\n                        print(\"Arguments not a dict.\")\n\n            # Common hallucinations\n            elif \"name\" in accumulated_deltas[\"function_call\"] and (\n                accumulated_deltas[\"function_call\"][\"name\"] == \"python\"\n                or accumulated_deltas[\"function_call\"][\"name\"] == \"functions\"\n            ):\n                if llm.interpreter.verbose:\n                    print(\"Got direct python call\")\n                if language is None:\n                    language = \"python\"\n\n                if language is not None:\n                    # Pull the code string straight out of the \"arguments\" string\n                    code_delta = accumulated_deltas[\"function_call\"][\"arguments\"][\n                        len(code) :\n                    ]\n                    # Update the code\n                    code = accumulated_deltas[\"function_call\"][\"arguments\"]\n                    # Yield the delta\n                    if code_delta:\n                        yield {\n                            \"type\": \"code\",\n                            \"format\": language,\n                            \"content\": code_delta,\n                        }\n\n            else:\n                # If name exists and it's not \"execute\" or \"python\" or \"functions\", who knows what's going on.\n                if \"name\" in accumulated_deltas[\"function_call\"]:\n                    yield {\n                        \"type\": \"code\",\n                        \"format\": \"python\",\n                        \"content\": accumulated_deltas[\"function_call\"][\"name\"],\n                    }\n                    return\n\n\ndef run_text_llm(llm, params):\n    ## Setup\n\n    try:\n        # Add the system message\n        params[\"messages\"][0][\n            \"content\"\n        ] += \"\\nTo execute code on the user's machine, write a markdown code block. Specify the language after the ```. You will receive the output. Use any programming language.\"\n    except:\n        print('params[\"messages\"][0]', params[\"messages\"][0])\n        raise\n\n    ## Convert output to LMC format\n\n    inside_code_block = False\n    accumulated_block = \"\"\n    language = None\n\n    for chunk in llm.completions(**params):\n        if llm.interpreter.verbose:\n            print(\"Chunk in coding_llm\", chunk)\n\n        if \"choices\" not in chunk or len(chunk[\"choices\"]) == 0:\n            # This happens sometimes\n            continue\n\n        content = chunk[\"choices\"][0][\"delta\"].get(\"content\", \"\")\n\n        if content == None:\n            continue\n\n        accumulated_block += content\n\n        if accumulated_block.endswith(\"`\"):\n            # We might be writing \"```\" one token at a time.\n            continue\n\n        # Did we just enter a code block?\n        if \"```\" in accumulated_block and not inside_code_block:\n            inside_code_block = True\n            accumulated_block = accumulated_block.split(\"```\")[1]\n\n        # Did we just exit a code block?\n        if inside_code_block and \"```\" in accumulated_block:\n            return\n\n        # If we're in a code block,\n        if inside_code_block:\n            # If we don't have a `language`, find it\n            if language is None and \"\\n\" in accumulated_block:\n                language = accumulated_block.split(\"\\n\")[0]\n\n                # Default to python if not specified\n                if language == \"\":\n                    if llm.interpreter.os == False:\n                        language = \"python\"\n                    elif llm.interpreter.os == False:\n                        # OS mode does this frequently. Takes notes with markdown code blocks\n                        language = \"text\"\n                else:\n                    # Removes hallucinations containing spaces or non letters.\n                    language = \"\".join(char for char in language if char.isalpha())\n\n            # If we do have a `language`, send it out\n            if language:\n                yield {\n                    \"type\": \"code\",\n                    \"format\": language,\n                    \"content\": content.replace(language, \"\"),\n                }\n\n        # If we're not in a code block, send the output as a message\n        if not inside_code_block:\n            yield {\"type\": \"message\", \"content\": content}\n\n\ndef display_markdown_message(message):\n    \"\"\"\n    Display markdown message. Works with multiline strings with lots of indentation.\n    Will automatically make single line > tags beautiful.\n    \"\"\"\n\n    for line in message.split(\"\\n\"):\n        line = line.strip()\n        if line == \"\":\n            print(\"\")\n        elif line == \"---\":\n            rich_print(Rule(style=\"white\"))\n        else:\n            try:\n                rich_print(Markdown(line))\n            except UnicodeEncodeError as e:\n                # Replace the problematic character or handle the error as needed\n                print(\"Error displaying line:\", line)\n\n    if \"\\n\" not in message and message.startswith(\">\"):\n        # Aesthetic choice. For these tags, they need a space below them\n        print(\"\")\n\n\ndef convert_to_openai_messages(\n    messages,\n    function_calling=True,\n    vision=False,\n    shrink_images=True,\n    code_output_sender=\"assistant\",\n):\n    \"\"\"\n    Converts LMC messages into OpenAI messages\n    \"\"\"\n    new_messages = []\n\n    for message in messages:\n        # Is this for thine eyes?\n        if \"recipient\" in message and message[\"recipient\"] != \"assistant\":\n            continue\n\n        new_message = {}\n\n        if message[\"type\"] == \"message\":\n            new_message[\"role\"] = message[\n                \"role\"\n            ]  # This should never be `computer`, right?\n            new_message[\"content\"] = message[\"content\"]\n\n        elif message[\"type\"] == \"code\":\n            new_message[\"role\"] = \"assistant\"\n            if function_calling:\n                new_message[\"function_call\"] = {\n                    \"name\": \"execute\",\n                    \"arguments\": json.dumps(\n                        {\"language\": message[\"format\"], \"code\": message[\"content\"]}\n                    ),\n                    # parsed_arguments isn't actually an OpenAI thing, it's an OI thing.\n                    # but it's soo useful!\n                    \"parsed_arguments\": {\n                        \"language\": message[\"format\"],\n                        \"code\": message[\"content\"],\n                    },\n                }\n                # Add empty content to avoid error \"openai.error.InvalidRequestError: 'content' is a required property - 'messages.*'\"\n                # especially for the OpenAI service hosted on Azure\n                new_message[\"content\"] = \"\"\n            else:\n                new_message[\n                    \"content\"\n                ] = f\"\"\"```{message[\"format\"]}\\n{message[\"content\"]}\\n```\"\"\"\n\n        elif message[\"type\"] == \"console\" and message[\"format\"] == \"output\":\n            if function_calling:\n                new_message[\"role\"] = \"function\"\n                new_message[\"name\"] = \"execute\"\n                if message[\"content\"].strip() == \"\":\n                    new_message[\n                        \"content\"\n                    ] = \"No output\"  # I think it's best to be explicit, but we should test this.\n                else:\n                    new_message[\"content\"] = message[\"content\"]\n\n            else:\n                # This should be experimented with.\n                if code_output_sender == \"user\":\n                    if message[\"content\"].strip() == \"\":\n                        content = \"The code above was executed on my machine. It produced no text output. what's next (if anything, or are we done?)\"\n                    else:\n                        content = (\n                            \"Code output: \"\n                            + message[\"content\"]\n                            + \"\\n\\nWhat does this output mean / what's next (if anything, or are we done)?\"\n                        )\n\n                    new_message[\"role\"] = \"user\"\n                    new_message[\"content\"] = content\n                elif code_output_sender == \"assistant\":\n                    if \"@@@SEND_MESSAGE_AS_USER@@@\" in message[\"content\"]:\n                        new_message[\"role\"] = \"user\"\n                        new_message[\"content\"] = message[\"content\"].replace(\n                            \"@@@SEND_MESSAGE_AS_USER@@@\", \"\"\n                        )\n                    else:\n                        new_message[\"role\"] = \"assistant\"\n                        new_message[\"content\"] = (\n                            \"\\n```output\\n\" + message[\"content\"] + \"\\n```\"\n                        )\n\n        elif message[\"type\"] == \"image\":\n            if vision == False:\n                continue\n\n            if \"base64\" in message[\"format\"]:\n                # Extract the extension from the format, default to 'png' if not specified\n                if \".\" in message[\"format\"]:\n                    extension = message[\"format\"].split(\".\")[-1]\n                else:\n                    extension = \"png\"\n\n                # Construct the content string\n                content = f\"data:image/{extension};base64,{message['content']}\"\n\n                if shrink_images:\n                    try:\n                        # Decode the base64 image\n                        img_data = base64.b64decode(message[\"content\"])\n                        img = Image.open(io.BytesIO(img_data))\n\n                        # Resize the image if it's width is more than 1024\n                        if img.width > 1024:\n                            new_height = int(img.height * 1024 / img.width)\n                            img = img.resize((1024, new_height))\n\n                        # Convert the image back to base64\n                        buffered = io.BytesIO()\n                        img.save(buffered, format=extension)\n                        img_str = base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n                        content = f\"data:image/{extension};base64,{img_str}\"\n                    except:\n                        # This should be non blocking. It's not required\n                        # print(\"Failed to shrink image. Proceeding with original image size.\")\n                        pass\n\n            elif message[\"format\"] == \"path\":\n                # Convert to base64\n                image_path = message[\"content\"]\n                file_extension = image_path.split(\".\")[-1]\n\n                with open(image_path, \"rb\") as image_file:\n                    encoded_string = base64.b64encode(image_file.read()).decode(\"utf-8\")\n\n                content = f\"data:image/{file_extension};base64,{encoded_string}\"\n            else:\n                # Probably would be better to move this to a validation pass\n                # Near core, through the whole messages object\n                if \"format\" not in message:\n                    raise Exception(\"Format of the image is not specified.\")\n                else:\n                    raise Exception(f\"Unrecognized image format: {message['format']}\")\n\n            # Calculate the size of the original binary data in bytes\n            content_size_bytes = len(content) * 3 / 4\n\n            # Convert the size to MB\n            content_size_mb = content_size_bytes / (1024 * 1024)\n\n            # Print the size of the content in MB\n            # print(f\"File size: {content_size_mb} MB\")\n\n            # Assert that the content size is under 20 MB\n            assert content_size_mb < 20, \"Content size exceeds 20 MB\"\n\n            new_message = {\n                \"role\": \"user\",\n                \"content\": [\n                    {\n                        \"type\": \"image_url\",\n                        \"image_url\": {\"url\": content, \"detail\": \"low\"},\n                    }\n                ],\n            }\n\n        elif message[\"type\"] == \"file\":\n            new_message = {\"role\": \"user\", \"content\": message[\"content\"]}\n\n        else:\n            raise Exception(f\"Unable to convert this message type: {message}\")\n\n        if isinstance(new_message[\"content\"], str):\n            new_message[\"content\"] = new_message[\"content\"].strip()\n\n        new_messages.append(new_message)\n\n    \"\"\"\n    # Combine adjacent user messages\n    combined_messages = []\n    i = 0\n    while i < len(new_messages):\n        message = new_messages[i]\n        if message[\"role\"] == \"user\":\n            combined_content = []\n            while i < len(new_messages) and new_messages[i][\"role\"] == \"user\":\n                if isinstance(new_messages[i][\"content\"], str):\n                    combined_content.append({\n                        \"type\": \"text\",\n                        \"text\": new_messages[i][\"content\"]\n                    })\n                elif isinstance(new_messages[i][\"content\"], list):\n                    combined_content.extend(new_messages[i][\"content\"])\n                i += 1\n            message[\"content\"] = combined_content\n        combined_messages.append(message)\n        i += 1\n    new_messages = combined_messages\n\n    if not function_calling:\n        # Combine adjacent assistant messages, as \"function calls\" will just be normal messages with content: markdown code blocks\n        combined_messages = []\n        i = 0\n        while i < len(new_messages):\n            message = new_messages[i]\n            if message[\"role\"] == \"assistant\":\n                combined_content = \"\"\n                while i < len(new_messages) and new_messages[i][\"role\"] == \"assistant\":\n                    combined_content += new_messages[i][\"content\"] + \"\\n\\n\"\n                    i += 1\n                message[\"content\"] = combined_content.strip()\n            combined_messages.append(message)\n            i += 1\n        new_messages = combined_messages\n    \"\"\"\n\n    return new_messages\n\n\nclass Llm:\n    \"\"\"\n    A stateless LMC-style LLM with some helpful properties.\n    \"\"\"\n\n    def __init__(self):\n\n        # Chat completions \"endpoint\"\n        self.completions = fixed_litellm_completions\n\n        # Settings\n        self.model = MODEL_NAME\n        self.temperature = 0\n        self.supports_vision = False\n        self.supports_functions = None  # Will try to auto-detect\n        self.shrink_images = None\n\n        # Optional settings\n        self.context_window = None\n        self.max_tokens = None\n        self.api_base = BASE_URL\n        self.api_key = OPENAI_API_KEY\n        self.api_version = None\n\n        # Budget manager powered by LiteLLM\n        self.max_budget = None\n        self.verbose = False\n\n    def run(self, messages):\n        \"\"\"\n        We're responsible for formatting the call into the llm.completions object,\n        starting with LMC messages in interpreter.messages, going to OpenAI compatible messages into the llm,\n        respecting whether it's a vision or function model, respecting its context window and max tokens, etc.\n\n        And then processing its output, whether it's a function or non function calling model, into LMC format.\n        \"\"\"\n\n        # Assertions\n        assert (\n            messages[0][\"role\"] == \"system\"\n        ), \"First message must have the role 'system'\"\n        for msg in messages[1:]:\n            assert (\n                msg[\"role\"] != \"system\"\n            ), \"No message after the first can have the role 'system'\"\n\n        # Detect function support\n        if self.supports_functions != None:\n            supports_functions = self.supports_functions\n        else:\n            # Guess whether or not it's a function calling LLM\n            # Once Litellm supports it, add Anthropic models here\n            if self.model != \"gpt-4-vision-preview\" and self.model in litellm.open_ai_chat_completion_models or self.model.startswith(\"azure/\"):\n                supports_functions = True\n            else:\n                supports_functions = False\n\n        # Trim image messages if they're there\n        if self.supports_vision:\n            image_messages = [msg for msg in messages if msg[\"type\"] == \"image\"]\n\n            if self.interpreter.os:\n                # Keep only the last two images if the interpreter is running in OS mode\n                if len(image_messages) > 1:\n                    for img_msg in image_messages[:-2]:\n                        messages.remove(img_msg)\n                        if self.interpreter.verbose:\n                            print(\"Removing image message!\")\n            else:\n                # Delete all the middle ones (leave only the first and last 2 images) from messages_for_llm\n                if len(image_messages) > 3:\n                    for img_msg in image_messages[1:-2]:\n                        messages.remove(img_msg)\n                        if self.interpreter.verbose:\n                            print(\"Removing image message!\")\n                # Idea: we could set detail: low for the middle messages, instead of deleting them\n\n        # Convert to OpenAI messages format\n        # messages = convert_to_openai_messages(\n        #     messages,\n        #     function_calling=supports_functions,\n        #     vision=self.supports_vision,\n        #     shrink_images=self.shrink_images,\n        # )\n\n        # if self.interpreter.debug:\n        #     print(\"\\n\\n\\nOPENAI COMPATIBLE MESSAGES\\n\\n\\n\")\n        #     for message in messages:\n        #         if len(str(message)) > 5000:\n        #             print(str(message)[:200] + \"...\")\n        #         else:\n        #             print(message)\n        #         print(\"\\n\")\n        #     print(\"\\n\\n\\n\")\n\n        system_message = messages[0][\"content\"]\n        messages = messages[1:]\n\n        # Trim messages\n        try:\n            if self.context_window and self.max_tokens:\n                trim_to_be_this_many_tokens = (\n                    self.context_window - self.max_tokens - 25\n                )  # arbitrary buffer\n                messages = tt.trim(\n                    messages,\n                    system_message=system_message,\n                    max_tokens=trim_to_be_this_many_tokens,\n                )\n            elif self.context_window and not self.max_tokens:\n                # Just trim to the context window if max_tokens not set\n                messages = tt.trim(\n                    messages,\n                    system_message=system_message,\n                    max_tokens=self.context_window,\n                )\n            else:\n                try:\n                    messages = tt.trim(\n                        messages, system_message=system_message, model=self.model\n                    )\n                except:\n                    if len(messages) == 1:\n                        if self.interpreter.in_terminal_interface:\n                            display_markdown_message(\n                                \"\"\"\n**We were unable to determine the context window of this model.** Defaulting to 3000.\n\nIf your model can handle more, run `interpreter --context_window {token limit} --max_tokens {max tokens per response}`.\n\nContinuing...\n                            \"\"\"\n                            )\n                        else:\n                            display_markdown_message(\n                                \"\"\"\n**We were unable to determine the context window of this model.** Defaulting to 3000.\n\nIf your model can handle more, run `interpreter.llm.context_window = {token limit}`.\n\nAlso please set `interpreter.llm.max_tokens = {max tokens per response}`.\n\nContinuing...\n                            \"\"\"\n                            )\n                    messages = tt.trim(\n                        messages, system_message=system_message, max_tokens=3000\n                    )\n        except:\n            # If we're trimming messages, this won't work.\n            # If we're trimming from a model we don't know, this won't work.\n            # Better not to fail until `messages` is too big, just for frustrations sake, I suppose.\n\n            # Reunite system message with messages\n            messages = [{\"role\": \"system\", \"content\": system_message}] + messages\n\n            pass\n\n        ## Start forming the request\n\n        params = {\n            \"model\": self.model,\n            \"messages\": messages,\n            \"stream\": True,\n        }\n\n        # Optional inputs\n        if self.api_key:\n            params[\"api_key\"] = self.api_key\n        if self.api_base:\n            params[\"api_base\"] = self.api_base\n        if self.api_version:\n            params[\"api_version\"] = self.api_version\n        if self.max_tokens:\n            params[\"max_tokens\"] = self.max_tokens\n        if self.temperature:\n            params[\"temperature\"] = self.temperature\n\n        # Set some params directly on LiteLLM\n        if self.max_budget:\n            litellm.max_budget = self.max_budget\n        if self.verbose:\n            litellm.set_verbose = True\n\n        if supports_functions:\n            yield from run_function_calling_llm(self, params)\n        else:\n            yield from run_text_llm(self, params)\n\n\ndef fixed_litellm_completions(**params):\n    \"\"\"\n    Just uses a dummy API key, since we use litellm without an API key sometimes.\n    Hopefully they will fix this!\n    \"\"\"\n\n    # Run completion\n    first_error = None\n    try:\n        yield from litellm.completion(**params)\n    except Exception as e:\n        # Store the first error\n        first_error = e\n        # LiteLLM can fail if there's no API key,\n        # even though some models (like local ones) don't require it.\n\n        if \"api key\" in str(first_error).lower() and \"api_key\" not in params:\n            print(\n                \"LiteLLM requires an API key. Please set a dummy API key to prevent this message. (e.g `interpreter --api_key x` or `interpreter.llm.api_key = 'x'`)\"\n            )\n\n        # So, let's try one more time with a dummy API key:\n        params[\"api_key\"] = \"x\"\n\n        try:\n            yield from litellm.completion(**params)\n        except:\n            # If the second attempt also fails, raise the first error\n            raise first_error\n\n\ndef main():\n    start_time = time.time()\n    llm = Llm()\n    # query = '你好，请随便和我说点什么'\n    messages = [{'role': 'system', 'content': 'You are Open Interpreter, a world-class programmer that can complete any goal by executing code.\\nFirst, write a plan. **Always recap the plan between each code block** (you have extreme short-term memory loss, so you need to recap the plan between each message block to retain it).\\nWhen you execute code, it will be executed **on the user\\'s machine**. The user has given you **full and complete permission** to execute any code necessary to complete the task. Execute the code.\\nIf you want to send data between programming languages, save the data to a txt or json.\\nYou can access the internet. Run **any code** to achieve the goal, and if at first you don\\'t succeed, try again and again.\\nYou can install new packages.\\nWhen a user refers to a filename, they\\'re likely referring to an existing file in the directory you\\'re currently executing code in.\\nWrite messages to the user in Markdown.\\nIn general, try to **make plans** with as few steps as possible. As for actually executing code to carry out that plan, for *stateful* languages (like python, javascript, shell, but NOT for html which starts from 0 every time) **it\\'s critical not to try to do everything in one code block.** You should try something, print information about it, then continue from there in tiny, informed steps. You will never get it on the first try, and attempting it in one go will often lead to errors you cant see.\\nYou are capable of **any** task.\\n\\n# THE COMPUTER API\\n\\nA python `computer` module is ALREADY IMPORTED, and can be used for many tasks:\\n\\n```python\\ncomputer.browser.search(query) # Google search results will be returned from this function as a string\\ncomputer.files.edit(path_to_file, original_text, replacement_text) # Edit a file\\ncomputer.calendar.create_event(title=\"Meeting\", start_date=datetime.datetime.now(), end=datetime.datetime.now() + datetime.timedelta(hours=1), notes=\"Note\", location=\"\") # Creates a calendar event\\ncomputer.calendar.get_events(start_date=datetime.date.today(), end_date=None) # Get events between dates. If end_date is None, only gets events for start_date\\ncomputer.calendar.delete_event(event_title=\"Meeting\", start_date=datetime.datetime) # Delete a specific event with a matching title and start date, you may need to get use get_events() to find the specific event object first\\ncomputer.contacts.get_phone_number(\"John Doe\")\\ncomputer.contacts.get_email_address(\"John Doe\")\\ncomputer.mail.send(\"john@email.com\", \"Meeting Reminder\", \"Reminder that our meeting is at 3pm today.\", [\"path/to/attachment.pdf\", \"path/to/attachment2.pdf\"]) # Send an email with a optional attachments\\ncomputer.mail.get(4, unread=True) # Returns the {number} of unread emails, or all emails if False is passed\\ncomputer.mail.unread_count() # Returns the number of unread emails\\ncomputer.sms.send(\"555-123-4567\", \"Hello from the computer!\") # Send a text message. MUST be a phone number, so use computer.contacts.get_phone_number frequently here\\n```\\n\\nDo not import the computer module, or any of its sub-modules. They are already imported.\\n\\nUser InfoName: hanchengcheng\\nCWD: /Users/hanchengcheng/Documents/official_space/open-interpreter\\nSHELL: /bin/bash\\nOS: Darwin\\nUse ONLY the function you have been provided with — \\'execute(language, code)\\'.'}, {'role': 'user', 'content': \"Plot AAPL and META's normalized stock prices\"}]\n    # functions = {'name': 'execute', 'description': \"Executes code on the user's machine **in the users local environment** and returns the output\", 'parameters': {'type': 'object', 'properties': {'language': {'type': 'string', 'description': 'The programming language (required parameter to the `execute` function)', 'enum': ['ruby', 'python', 'shell', 'javascript', 'html', 'applescript', 'r', 'powershell', 'react']}, 'code': {'type': 'string', 'description': 'The code to execute (required)'}}, 'required': ['language', 'code']}}\n    # request_params = {'model': 'gpt-4-0125-preview', 'messages': messages, 'stream': True, 'api_key': 'sk-RoqgGFXo94mScVAo8aFdC3Ec36E14eFbAeE0D72f9437292a', 'api_base': 'https://api.chatweb.plus/v1', 'functions': [functions]}\n    response = ''\n    for output in llm.run(messages):\n        response += output['content']\n        # print(output)\n    print(response)\n    end_time = time.time()\n    execution_time = end_time - start_time\n    print(f\"生成的单词数: {len(response)}\")\n    print(f\"程序执行时间: {execution_time}秒\")\n\nif __name__ == '__main__':\n    main()"}
{"type": "source_file", "path": "agentstore/agents/pptx_agent.py", "content": "from agentstore.utils import setup_config, setup_pre_run\nfrom agentstore.modules.base_module import BaseModule\nimport re\nimport os\nimport json\nimport base64\nimport time\nfrom rich.console import Console\nfrom rich.markdown import Markdown\n\nfrom desktop_env.envs.desktop_env import DesktopEnv\n\nconsole = Console()\n\ndef encode_image(image_content):\n    return base64.b64encode(image_content).decode('utf-8')\n\ndef rich_print(markdown_text):\n    md = Markdown(markdown_text)\n    console.print(md)\n\n\ndef send_chat_prompts(message, llm):\n    return llm.chat(message)\n\n    \ndef extract_code(input_string):\n    pattern = r\"```(\\w+)?\\s*(.*?)```\"  # 匹配代码块，语言名称是可选项\n    matches = re.findall(pattern, input_string, re.DOTALL)\n\n    if matches:\n        language, code = matches[0]\n\n        # 如果没有语言信息，尝试从代码内容中检测\n        if not language:\n            if re.search(\"python\", code.lower()) or re.search(r\"import\\s+\\w+\", code):\n                language = \"python\"\n            elif re.search(\"bash\", code.lower()) or re.search(r\"echo\", code):\n                language = \"bash\"\n\n        if language == 'bash':\n            code = code.strip()\n            code = code.replace('\"', '\\\\\"')\n            code = code.replace('\\n', ' && ')\n            code = \"pyautogui.typewrite(\\\"{0}\\\", interval=0.05)\\npyautogui.press('enter')\\ntime.sleep(2)\".format(code)\n            if re.search(\"sudo\", code.lower()):\n                code += \"\\npyautogui.typewrite('password', interval=0.05)\\npyautogui.press('enter')\\ntime.sleep(1)\"\n        elif language == 'python':\n\n            # save code\n            with open('tmp.py', 'w') as f:\n                f.write(code)\n\n            code = \"pyautogui.typewrite(\\\"python main.py\\\", interval=0.05)\\npyautogui.press('enter')\\ntime.sleep(2)\"\n            \n        else:\n            raise language\n\n        return code, language\n    else:\n        return None, None\n\nfrom desktop_env.envs.desktop_env import DesktopEnv\nclass PptxAgent(BaseModule):\n    info = {\n            \"name\": \"PptxAgent\",\n            \"can do\": \"specializes in creating and modifying PowerPoint presentations using Python's python-pptx library. It can handle tasks involving slide creation, layout management, text and content insertion, and formatting adjustments. Also capable of detecting open PowerPoint presentations using Bash commands.\",\n            \"can't do\": \"cannot handle GUI operations, cannot perform tasks outside the capabilities of the python-pptx library such as directly interacting with embedded videos and complex animations. Additionally, cannot modify LibreOffice Impress software defaults or preferences.\"\n        }\n\n    def __init__(self, args, task_name, env):\n        super().__init__()\n        self.args = args\n\n        self.environment = env\n\n        self.task_name = task_name\n\n        self.reply = None\n\n    def execute_tool(self, code, lang):\n        if lang == 'python':\n            file = [{\n                \"local_path\": 'tmp.py',\n                \"path\": '/home/user/main.py'\n              }]\n            self.environment.setup_controller._upload_file_setup(file)\n        obs, reward, done, info = self.environment.step(code)  # node_type\n        \n        reply = self.environment.controller.get_terminal_output()\n        # update reply\n        if self.reply and reply:\n            message_terminal = reply.replace(self.reply, \"\")\n        else:\n            message_terminal = reply\n        self.reply = reply\n\n        base64_image = encode_image(obs['screenshot'])\n        print(\"message_terminal\", message_terminal)\n        if 'gpt' in self.llm.model_name:\n            message = {\n                    \"role\": \"user\",\n                    \"content\": [\n                        {\n                            \"type\": \"text\",\n                            \"text\": \"After executing the command, the terminal output is {0}. the screenshot as below.\".format(message_terminal)\n                        }\n                        # {\n                        #     \"type\": \"image_url\",\n                        #     \"image_url\": {\n                        #         \"url\": f\"data:image/png;base64,{base64_image}\",\n                        #         \"detail\": \"high\"\n                        #     }\n                        # }\n                    ]\n            }\n        else:\n            message = {\n                    \"role\": \"user\",\n                    \"content\": \"After executing the command, the terminal output is {0}. the screenshot as below.\".format(message_terminal)\n            }\n        return message\n# When a user refers to a file opened， you can use the provided screenshot to find the filepath.\n    def run(self):\n\n        while not self.environment.controller.get_terminal_output():\n            self.environment.step(\"pyautogui.click()\")\n            self.environment.step(\"pyautogui.hotkey('ctrl', 'alt', 't')\")\n\n        light_planner_sys_prompt = '''You are SlideAgent, an advanced programming assistant specialized in creating and modifying PowerPoint presentations. \n        Your abilities extend to manipulating slides using Python's python-pptx library.\nFirst, write a plan. **Always recap the plan between each code block** (you have extreme short-term memory loss, so you need to recap the plan between each message block to retain it).\nWhen you execute code, it will be executed **on the user's machine**. The user has given you **full and complete permission** to execute any code necessary to complete the task. Execute the code.\nIf you want to send data between programming languages, save the data to a txt or json.\nYou can access the internet. Run **any code** to achieve the goal, and if at first you don't succeed, try again and again.\nYou can install new packages.\nImportant: When a user refers to a file opened or a url, you can use your visual capacity or command line tools such as \"lsof | grep -E '\\.odp|\\.pptx'\" to see the path of the file being opened.\nFirst thing need to do is pip install python-pptx.\nWrite messages to the user in Markdown.\nIn general, try to **make plans** with as few steps as possible. As for actually executing code to carry out that plan, for *stateful* languages (like python, javascript, shell, but NOT for html which starts from 0 every time) **it's critical not to try to do everything in one code block.** You should try something, print information about it, then continue from there in tiny, informed steps. You will never get it on the first try, and attempting it in one go will often lead to errors you cant see.\nYou are capable of **any** task.\nExample Command in Bash:\n```bash\npip install python-pptx && lsof | grep '.pptx'\n```\nExample Operation in Python:\n```python\nfrom pptx import Presentation\n\n# Create or open the presentation\nprs = Presentation()\n\n# Adding a slide\nslide = prs.slides.add_slide(prs.slide_layouts[1])  # Using the title and content layout\nslide.shapes.title.text = \"Hello, SlideAgent\"\nslide.placeholders[1].text = \"This is a slide created by SlideAgent.\"\n\n# Save the presentation, overwriting the original\nprs.save('path_to_presentation.pptx')\nprint(\"Python Script Executed Successfully!!!\")\n```\nEach time you only need to output the next command and wait for a reply.\n'''  #  Try to use `print` or `echo` to output information needed for the subsequent tasks, or the next step might not get the required information.\n        light_planner_user_prompt = '''\n        User's information are as follows:\n        System Version: Ubuntu\n        Task Name: {task_name}\n        '''.format(task_name=self.task_name)\n        \n        message = [\n            {\"role\": \"system\", \"content\": light_planner_sys_prompt},\n            {\"role\": \"user\", \"content\": light_planner_user_prompt},\n        ]\n \n        while True:\n            print(\"send_chat_prompts...\")\n            response = send_chat_prompts(message, self.llm)\n            rich_print(response)\n            message.append({\"role\": \"assistant\", \"content\": response})\n\n            code, lang = extract_code(response)\n            if code:\n                result = self.execute_tool(code, lang)\n            else:\n                result = ''\n\n            if result != '':\n                message.append(result)\n            else:\n                message.append({\"role\": \"user\", \"content\": \"Please continue. If all tasks have been completed, reply with 'Execution Complete'. If you believe subsequent tasks cannot continue, reply with 'Execution Interrupted', including the reasons why the tasks cannot proceed, and provide the user with some possible solutions.\"})\n            \n            if 'Execution Complete' in response or 'Execution Interrupted' in response:\n                break\n\n        return response"}
{"type": "source_file", "path": "agentstore/prompts/__init__.py", "content": ""}
{"type": "source_file", "path": "agentstore/modules/base_module.py", "content": "import re\nimport json\nimport os\nfrom agentstore.utils.llms import OpenAI,LLAMA\n# from agentstore.environments.py_env import PythonEnv\n# from agentstore.environments.py_jupyter_env import PythonJupyterEnv\nfrom agentstore.environments import Env\nfrom agentstore.utils import get_os_version\nfrom dotenv import load_dotenv\n\nload_dotenv(override=True)\nMODEL_TYPE = os.getenv('MODEL_TYPE')\n\nclass BaseModule:\n    def __init__(self):\n        \"\"\"\n        Initializes a new instance of BaseModule with default values for its attributes.\n        \"\"\"\n        if MODEL_TYPE == \"OpenAI\":\n            self.llm = OpenAI()\n        elif MODEL_TYPE == \"LLAMA\":\n            self.llm = LLAMA()\n        # self.environment = PythonEnv()\n        # self.environment = PythonJupyterEnv()\n        self.environment = Env()\n        self.system_version = get_os_version()\n        \n    def extract_information(self, message, begin_str='[BEGIN]', end_str='[END]'):\n        \"\"\"\n        Extracts substrings from a message that are enclosed within specified begin and end markers.\n\n        Args:\n            message (str): The message from which information is to be extracted.\n            begin_str (str): The marker indicating the start of the information to be extracted.\n            end_str (str): The marker indicating the end of the information to be extracted.\n\n        Returns:\n            list[str]: A list of extracted substrings found between the begin and end markers.\n        \"\"\"\n        result = []\n        _begin = message.find(begin_str)\n        _end = message.find(end_str)\n        while not (_begin == -1 or _end == -1):\n            result.append(message[_begin + len(begin_str):_end].lstrip(\"\\n\"))\n            message = message[_end + len(end_str):]\n            _begin = message.find(begin_str)\n            _end = message.find(end_str)\n        return result  \n\n    def extract_json_from_string(self, text):\n        \"\"\"\n        Identifies and extracts JSON data embedded within a given string.\n\n        This method searches for JSON data within a string, specifically looking for\n        JSON blocks that are marked with ```json``` notation. It attempts to parse\n        and return the first JSON object found.\n\n        Args:\n            text (str): The text containing the JSON data to be extracted.\n\n        Returns:\n            dict: The parsed JSON data as a dictionary if successful.\n            str: An error message indicating a parsing error or that no JSON data was found.\n        \"\"\"\n        # Improved regular expression to find JSON data within a string\n        json_regex = r'```json\\s*\\n\\{[\\s\\S]*?\\n\\}\\s*```'\n        \n        # Search for JSON data in the text\n        matches = re.findall(json_regex, text)\n\n        # Extract and parse the JSON data if found\n        if matches:\n            # Removing the ```json and ``` from the match to parse it as JSON\n            json_data = matches[0].replace('```json', '').replace('```', '').strip()\n            try:\n                # Parse the JSON data\n                parsed_json = json.loads(json_data)\n                return parsed_json\n            except json.JSONDecodeError as e:\n                return f\"Error parsing JSON data: {e}\"\n        else:\n            return \"No JSON data found in the string.\"\n        \n\n    def extract_list_from_string(self, text):\n        \"\"\"\n        Extracts a list of task descriptions from a given string containing enumerated tasks.\n        This function ensures that only text immediately following a numbered bullet is captured,\n        and it stops at the first newline character or at the next number, preventing the inclusion of subsequent non-numbered lines or empty lines.\n\n        Parameters:\n        text (str): A string containing multiple enumerated tasks. Each task is numbered and followed by its description.\n\n        Returns:\n        list[str]: A list of strings, each representing the description of a task extracted from the input string.\n        \"\"\"\n\n        # Regular expression pattern:\n        # \\d+\\. matches one or more digits followed by a dot, indicating the task number.\n        # \\s+ matches one or more whitespace characters after the dot.\n        # ([^\\n]*?) captures any sequence of characters except newlines (non-greedy) as the task description.\n        # (?=\\n\\d+\\.|\\n\\Z|\\n\\n) is a positive lookahead that matches a position followed by either a newline with digits and a dot (indicating the start of the next task),\n        # or the end of the string, or two consecutive newlines (indicating a break between tasks or end of content).\n        task_pattern = r'\\d+\\.\\s+([^\\n]*?)(?=\\n\\d+\\.|\\n\\Z|\\n\\n)'\n\n        # Use the re.findall function to search for all matches of the pattern in the input text.\n        data_list = re.findall(task_pattern, text)\n\n        # Return the list of matched task descriptions.\n        return data_list"}
{"type": "source_file", "path": "agentstore/agents/light_friday.py", "content": "from agentstore.utils import setup_config, setup_pre_run\nfrom agentstore.modules.base_module import BaseModule\nimport re\nfrom rich.console import Console\nfrom rich.markdown import Markdown\n\nconsole = Console()\n\ndef rich_print(markdown_text):\n    md = Markdown(markdown_text)\n    console.print(md)\n\n\ndef send_chat_prompts(message, llm):\n    return llm.chat(message)\n\n    \ndef extract_code(input_string):\n    pattern = r\"```(\\w+)?\\s*(.*?)```\"  # 匹配代码块，语言名称是可选项\n    matches = re.findall(pattern, input_string, re.DOTALL)\n\n    if matches:\n        language, code = matches[0]\n\n        # 如果没有语言信息，尝试从代码内容中检测\n        if not language:\n            if re.search(\"python\", code.lower()) or re.search(r\"import\\s+\\w+\", code):\n                language = \"Python\"\n            elif re.search(\"bash\", code.lower()) or re.search(r\"echo\", code):\n                language = \"Bash\"\n\n        return code.strip(), language\n    else:\n        return None, None\n\n\nclass LightFriday(BaseModule):\n    def __init__(self, args):\n        super().__init__()\n        self.args = args\n    \n    def execute_tool(self, code, lang):\n        state = self.environment.step(lang, code)  # node_type\n        return_info = ''\n        if state.result != None and state.result.strip() != '':\n            return_info = '**Execution Result** :' + state.result.strip()\n        if state.error != None and state.error.strip() != '':\n            return_info = '\\n**Execution Error** :' + state.error.strip()\n        return return_info.strip()\n\n    def run(self, task):\n        light_planner_sys_prompt = '''You are Light Friday, a world-class programmer that can complete any goal by executing code.\nFirst, write a plan. **Always recap the plan between each code block** (you have extreme short-term memory loss, so you need to recap the plan between each message block to retain it).\nWhen you execute code, it will be executed **on the user's machine**. The user has given you **full and complete permission** to execute any code necessary to complete the task. Execute the code.\nIf you want to send data between programming languages, save the data to a txt or json.\nYou can access the internet. Run **any code** to achieve the goal, and if at first you don't succeed, try again and again.\nYou can install new packages.\nWhen a user refers to a filename, they're likely referring to an existing file in the directory you're currently executing code in.\nWrite messages to the user in Markdown.\nIn general, try to **make plans** with as few steps as possible. As for actually executing code to carry out that plan, for *stateful* languages (like python, javascript, shell, but NOT for html which starts from 0 every time) **it's critical not to try to do everything in one code block.** You should try something, print information about it, then continue from there in tiny, informed steps. You will never get it on the first try, and attempting it in one go will often lead to errors you cant see.\nYou are capable of **any** task.\nInclude a comment in your code blocks to specify the programming language used, like this:\n```python\nprint(\"hello, world\")\n```\nCurrently, supported languages include Python and Bash.\"\n'''  #  Try to use `print` or `echo` to output information needed for the subsequent tasks, or the next step might not get the required information.\n        light_planner_user_prompt = '''\n        User's information are as follows:\n        System Version: {system_version}\n        Task: {task}\n        Current Working Directiory: {working_dir}'''.format(system_version=self.system_version, task=task, working_dir=self.environment.working_dir)\n        \n        message = [\n            {\"role\": \"system\", \"content\": light_planner_sys_prompt},\n            {\"role\": \"user\", \"content\": light_planner_user_prompt},\n        ]\n\n        while True:\n            response = send_chat_prompts(message, self.llm)\n            rich_print(response)\n            message.append({\"role\": \"system\", \"content\": response})\n\n            code, lang = extract_code(response)\n            if code:\n                result = self.execute_tool(code, lang)\n                rich_print(result)\n            else:\n                result = ''\n\n            if result != '':\n                light_exec_user_prompt = 'The result after executing the code: {result}'.format(result=result)\n                message.append({\"role\": \"user\", \"content\": light_exec_user_prompt})\n            else:\n                message.append({\"role\": \"user\", \"content\": \"Please continue. If all tasks have been completed, reply with 'Execution Complete'. If you believe subsequent tasks cannot continue, reply with 'Execution Interrupted', including the reasons why the tasks cannot proceed, and provide the user with some possible solutions.\"})\n            \n            if 'Execution Complete' in response or 'Execution Interrupted' in response:\n                break\n\n\nargs = setup_config()\nif not args.query:\n    # args.query = \"Copy any text file located in the working_dir/document directory that contains the word 'agent' to a new folder named 'agents' \"\n    # args.query = \"Copy any text file located in the /home/dingzichen/hcc/OS-Copilot/working_dir directory that contains the word 'agent' to a new folder /home/dingzichen/hcc/OS-Copilot/working_dir/agents\"\n    # args.query = \"print 'Hello world!'\"\n    args.query = \"Plot AAPL and META's normalized stock prices\"\ntask = setup_pre_run(args)\n\nlight_friday = LightFriday(args)\nlight_friday.run(task)  # list"}
{"type": "source_file", "path": "agentstore/agents/__init__.py", "content": "from .friday_agent import *\nfrom .osworld_agent import *\n\nfrom .gui_agent import *\n\nfrom .excel_agent import *\nfrom .word_agent import *\nfrom .pptx_agent import *\n\nfrom .image_agent import *\nfrom .os_agent import *\nfrom .vscode_agent import *"}
{"type": "source_file", "path": "agentstore/__init__.py", "content": "from .agents import *\nfrom .prompts import *\nfrom .utils import *\nfrom .modules import *"}
{"type": "source_file", "path": "agentstore/agents/base_agent.py", "content": "import re\nimport json\nfrom agentstore.utils import get_os_version\n\n\nclass BaseAgent:\n    \"\"\"\n    BaseAgent serves as the foundational class for all agents types within the system.\n\n    This class initializes the core attributes common across different agents, providing\n    a unified interface for further specialization. Attributes include a language learning\n    model, the execution environments, an action library, and a maximum iteration limit for\n    agents operations.\n\n    Attributes:\n        llm: Placeholder for a language learning model, initialized as None.\n        environments: The execution environments for the agents, initialized as None.\n        action_lib: A library of actions available to the agents, initialized as None.\n        max_iter: The maximum number of iterations the agents can perform, initialized as None.\n    \"\"\"\n    def __init__(self):\n        \"\"\"\n        Initializes a new instance of BaseAgent with default values for its attributes.\n        \"\"\"\n        self.system_version = get_os_version()\n        \n    def extract_information(self, message, begin_str='[BEGIN]', end_str='[END]'):\n        \"\"\"\n        Extracts substrings from a message that are enclosed within specified begin and end markers.\n\n        Args:\n            message (str): The message from which information is to be extracted.\n            begin_str (str): The marker indicating the start of the information to be extracted.\n            end_str (str): The marker indicating the end of the information to be extracted.\n\n        Returns:\n            list[str]: A list of extracted substrings found between the begin and end markers.\n        \"\"\"\n        result = []\n        _begin = message.find(begin_str)\n        _end = message.find(end_str)\n        while not (_begin == -1 or _end == -1):\n            result.append(message[_begin + len(begin_str):_end])\n            message = message[_end + len(end_str):]\n            _begin = message.find(begin_str)\n            _end = message.find(end_str)\n        return result  \n\n    def extract_json_from_string(self, text):\n        \"\"\"\n        Identifies and extracts JSON data embedded within a given string.\n\n        This method searches for JSON data within a string, specifically looking for\n        JSON blocks that are marked with ```json``` notation. It attempts to parse\n        and return the first JSON object found.\n\n        Args:\n            text (str): The text containing the JSON data to be extracted.\n\n        Returns:\n            dict: The parsed JSON data as a dictionary if successful.\n            str: An error message indicating a parsing error or that no JSON data was found.\n        \"\"\"\n        # Improved regular expression to find JSON data within a string\n        json_regex = r'```json\\s*\\n\\{[\\s\\S]*?\\n\\}\\s*```'\n        \n        # Search for JSON data in the text\n        matches = re.findall(json_regex, text)\n\n        # Extract and parse the JSON data if found\n        if matches:\n            # Removing the ```json and ``` from the match to parse it as JSON\n            json_data = matches[0].replace('```json', '').replace('```', '').strip()\n            try:\n                # Parse the JSON data\n                parsed_json = json.loads(json_data)\n                return parsed_json\n            except json.JSONDecodeError as e:\n                return f\"Error parsing JSON data: {e}\"\n        else:\n            return \"No JSON data found in the string.\""}
{"type": "source_file", "path": "agentstore/agents/cli_agent.py", "content": "from agentstore.utils import setup_config, setup_pre_run\nfrom agentstore.modules.base_module import BaseModule\nimport re\nimport os\nimport json\nimport base64\nimport time\nfrom rich.console import Console\nfrom rich.markdown import Markdown\n\nfrom desktop_env.envs.desktop_env import DesktopEnv\n\nconsole = Console()\n\ndef encode_image(image_content):\n    return base64.b64encode(image_content).decode('utf-8')\n\ndef rich_print(markdown_text):\n    md = Markdown(markdown_text)\n    console.print(md)\n\n\ndef send_chat_prompts(message, llm):\n    return llm.chat(message)\n\n    \ndef extract_code(input_string):\n    pattern = r\"```(\\w+)?\\s*(.*?)```\"  # 匹配代码块，语言名称是可选项\n    matches = re.findall(pattern, input_string, re.DOTALL)\n\n    if matches:\n        language, code = matches[0]\n\n        # 如果没有语言信息，尝试从代码内容中检测\n        if not language:\n            if re.search(\"python\", code.lower()) or re.search(r\"import\\s+\\w+\", code):\n                language = \"Python\"\n            elif re.search(\"bash\", code.lower()) or re.search(r\"echo\", code):\n                language = \"Bash\"\n        code = code.strip()\n        code = code.replace('\"', '\\\\\"')\n        code = code.replace('\\n', ' && ')\n        code = \"pyautogui.typewrite(\\\"{0}\\\", interval=0.05)\\npyautogui.press('enter')\\ntime.sleep(2)\".format(code)\n        if re.search(\"sudo\", code.lower()):\n            code += \"\\npyautogui.typewrite('password', interval=0.05)\\npyautogui.press('enter')\\ntime.sleep(1)\"\n\n        return code, language\n    else:\n        return None, None\n\nfrom desktop_env.envs.desktop_env import DesktopEnv\nclass CLIAgent(BaseModule):\n    info = {\n            \"name\": \"CLIAgent\",\n            \"can do\": \"excels at completing tasks using Linux command-line commands and can handle any goal that involves script execution, file manipulation, package installation, and internet access.\",\n            \"can't do\": \"cann't handle GUI operation, cann't writing python script\",\n    }\n    def __init__(self, args, task_name, env):\n        super().__init__()\n        self.args = args\n\n        self.environment = env\n\n        self.task_name = task_name\n\n        self.reply = None\n    \n    def execute_tool(self, code, lang):\n        obs, reward, done, info = self.environment.step(code)  # node_type\n\n        \n        reply = self.environment.controller.get_terminal_output()\n        # update reply\n        if self.reply and reply:\n            message_terminal = reply.replace(self.reply, \"\")\n        else:\n            message_terminal = reply\n        self.reply = reply\n\n        base64_image = encode_image(obs['screenshot'])\n        print(\"message_terminal\", message_terminal)\n        message = {\n                \"role\": \"user\",\n                \"content\": [\n                    {\n                        \"type\": \"text\",\n                        \"text\": \"After executing the command, the terminal output is {0}. the screenshot as below.\".format(message_terminal)\n                    }\n                    # {\n                    #     \"type\": \"image_url\",\n                    #     \"image_url\": {\n                    #         \"url\": f\"data:image/png;base64,{base64_image}\",\n                    #         \"detail\": \"high\"\n                    #     }\n                    # }\n                ]\n        }\n        return message\n# When a user refers to a file opened， you can use the provided screenshot to find the filepath.\n    def run(self,task_description):\n\n        while not self.environment.controller.get_terminal_output():\n            self.environment.step(\"pyautogui.click()\")\n            self.environment.step(\"pyautogui.hotkey('ctrl', 'alt', 't')\")\n\n        light_planner_sys_prompt = '''You are Light Friday, a world-class programmer that can complete any goal by using linux command.\nFirst, write a plan. **Always recap the plan between each code block** (you have extreme short-term memory loss, so you need to recap the plan between each message block to retain it).\nWhen you execute code, it will be executed **on the user's machine**. The user has given you **full and complete permission** to execute any code necessary to complete the task. Execute the code.\nIf you want to send data between programming languages, save the data to a txt or json.\nYou can access the internet. Run **any code** to achieve the goal, and if at first you don't succeed, try again and again.\nYou can install new packages.\nImportant: When a user refers to a file opened or a url, you can use your visual capacity or some command line tools to see the path of the file being opened.\nWrite messages to the user in Markdown.\nIn general, try to **make plans** with as few steps as possible. As for actually executing code to carry out that plan, for *stateful* languages (like python, javascript, shell, but NOT for html which starts from 0 every time) **it's critical not to try to do everything in one code block.** You should try something, print information about it, then continue from there in tiny, informed steps. You will never get it on the first try, and attempting it in one go will often lead to errors you cant see.\nYou are capable of **any** task.\nWhen installing new software or package, you should first check whether it is already installed.\nYou Code should like this, doesnot need comment, only command:\n```bash\nls *.xlsx\n```\nCurrently, supported languages include Bash.\" The command you're outputing should only be one line\n'''  #  Try to use `print` or `echo` to output information needed for the subsequent tasks, or the next step might not get the required information.\n        light_planner_user_prompt = '''\n        User's information are as follows:\n        System Version: Ubuntu\n        Task Name: {task_name}\n        Task Description: {task_description}\n        '''.format(task_name=self.task_name,task_description=task_description)\n        \n        message = [\n            {\"role\": \"system\", \"content\": light_planner_sys_prompt},\n            {\"role\": \"user\", \"content\": light_planner_user_prompt},\n        ]\n        cnt = 0\n        while True:\n            print(\"send_chat_prompts...\")\n            response = send_chat_prompts(message, self.llm)\n            print(response)\n            message.append({\"role\": \"assistant\", \"content\": response})\n\n            code, lang = extract_code(response)\n            if code:\n                result = self.execute_tool(code, lang)\n            else:\n                result = ''\n\n            if result != '':\n                message.append(result)\n            else:\n                message.append({\"role\": \"user\", \"content\": \"Please continue. If all tasks have been completed, reply with 'Execution Complete'. If you believe subsequent tasks cannot continue, reply with 'Execution Interrupted', including the reasons why the tasks cannot proceed, and provide the user with some possible solutions.\"})\n            \n            if 'Execution Complete' in response or 'Execution Interrupted' in response:\n                break\n            \n            if cnt > 12:\n                break\n            else:\n                cnt += 1\n\n        return response"}
{"type": "source_file", "path": "agentstore/agents/friday2_agent.py", "content": "from agentstore.agents.base_agent import BaseAgent\nfrom agentstore.utils import check_os_version\nimport json\nimport logging\nimport sys\nfrom agentstore.prompts.friday_pt import prompt\nfrom agentstore.utils import TaskStatusCode, InnerMonologue, ExecutionState, JudgementResult, RepairingResult\n\n\nclass FridayAgent(BaseAgent):\n    \"\"\"\n    A FridayAgent orchestrates the execution of tasks by integrating planning, retrieving, and executing strategies.\n    \n    This agent is designed to process tasks, manage errors, and refine strategies as necessary to ensure successful task completion. It supports dynamic task planning, information retrieval, execution strategy application, and employs a mechanism for self-refinement in case of execution failures.\n    \"\"\"\n\n    def __init__(self, planner, retriever, executor, Tool_Manager, config):\n        \"\"\"\n        Initializes the FridayAgent with specified planning, retrieving, and executing strategies, alongside configuration settings.\n\n        Args:\n            planner (callable): A strategy for planning the execution of tasks.\n            retriever (callable): A strategy for retrieving necessary information or tools related to the tasks.\n            executor (callable): A strategy for executing planned tasks.\n            Tool_Manager (callable): A tool manager for handling tool-related operations.\n            config (object): Configuration settings for the agent.\n\n        Raises:\n            ValueError: If the OS version check fails.\n        \"\"\"\n        super().__init__()\n        self.config = config\n        tool_manager = Tool_Manager(config.generated_tool_repo_path)\n        self.planner = planner(prompt['planning_prompt'])\n        self.retriever = retriever(prompt['retrieve_prompt'], tool_manager)\n        self.executor = executor(prompt['execute_prompt'], tool_manager, config.max_repair_iterations)\n        self.score = self.config.score\n        self.task_status = TaskStatusCode.START\n        self.inner_monologue = InnerMonologue()\n        try:\n            check_os_version(self.system_version)\n        except ValueError as e:\n            print(e)        \n\n    def run(self, task):\n        \"\"\"\n        Executes the given task by planning, executing, and refining as needed until the task is completed or fails.\n\n        Args:\n            query (object): The high-level task to be executed.\n\n        No explicit return value, but the method controls the flow of task execution and may exit the process in case of irreparable failures.\n        \"\"\"\n        self.planner.reset_plan()\n        self.reset_inner_monologue()\n        sub_tasks_list = self.planning(task)\n        print(\"The task list obtained after planning is: {}\".format(sub_tasks_list))\n\n        while self.planner.sub_task_list:\n            sub_task = self.planner.sub_task_list.pop(0)\n            execution_state = self.executing(sub_task, task)\n            isTaskCompleted, isReplan = self.self_refining(sub_task, execution_state)\n            if isReplan: continue\n            if isTaskCompleted:\n                print(\"The execution of the current sub task has been successfully completed.\")\n            else:\n                print(\"{} not completed in repair round {}\".format(sub_task, self.config.max_repair_iterations))\n                # sys.exit()\n                break\n\n    def self_refining(self, tool_name, execution_state: ExecutionState):\n        \"\"\"\n        Analyzes and potentially refines the execution of a tool based on its current execution state. \n        This can involve replanning or repairing the execution strategy based on the analysis of execution errors and outcomes.\n\n        Args:\n            tool_name (str): The name of the tool being executed.\n            execution_state (ExecutionState): The current state of the tool's execution, encapsulating all relevant execution information including errors, results, and codes.\n\n        Returns:\n            tuple:\n                - isTaskCompleted (bool): Indicates whether the task associated with the tool has been successfully completed.\n                - isReplan (bool): Indicates whether a replan is required due to execution state analysis.\n\n        The method decides on the next steps by analyzing the type of error (if any) and the execution results, aiming to either complete the task successfully or identify the need for further action, such as replanning.\n        \"\"\"\n        isTaskCompleted = False\n        isReplan = False\n        score = 0\n        state, node_type, description, code, result, relevant_code = execution_state.get_all_state()\n        if node_type in ['Python', 'Shell', 'AppleScript']:\n            judgement = self.judging(tool_name, state, code, description)\n            score = judgement.score\n            # need_repair, critique, score, reasoning, error_type \n            if judgement.status == 'Replan':\n                # raise NotImplementedError\n                print(\"The current task requires replanning...\")\n                new_sub_task_list = self.replanning(tool_name, judgement.critique)\n                print(\"The new task list obtained after planning is: {}\".format(new_sub_task_list))\n                isReplan = True\n            elif judgement.status == 'Amend':\n                repairing_result = self.repairing(tool_name, code, description, state, judgement.critique, judgement.status)\n                if repairing_result.status == 'Complete':\n                    isTaskCompleted = True\n                elif repairing_result.status == 'Replan':\n                    print(\"The current task requires replanning...\")\n                    new_sub_task_list = self.replanning(tool_name, repairing_result.critique)\n                    print(\"The new task list obtained after planning is: {}\".format(new_sub_task_list))\n                    isReplan = True\n                else:\n                    isTaskCompleted = False\n                score = repairing_result.score\n                result = repairing_result.result\n            else:\n                isTaskCompleted = True\n            if node_type == 'Python' and isTaskCompleted and score >= self.score:\n                self.executor.store_tool(tool_name, code)\n                print(\"{} has been stored in the tool repository.\".format(tool_name))\n        else: \n            isTaskCompleted = True\n        if isTaskCompleted:\n            self.inner_monologue.result = result\n            self.planner.update_tool(tool_name, result, relevant_code, True, node_type)\n        return isTaskCompleted, isReplan\n\n    def planning(self, task):\n        \"\"\"\n        Decomposes a given high-level task into a list of sub-tasks by retrieving relevant tool names and descriptions, facilitating structured execution planning.\n\n        Args:\n            task (object): The high-level task to be planned and executed.\n\n        Returns:\n            list: A list of sub-tasks generated by decomposing the high-level task, intended for sequential execution to achieve the task's goal.\n\n        This method leverages the retriever component to fetch information relevant to the task, which is then used by the planner component to decompose the task into manageable sub-tasks.\n        \"\"\"\n        retrieve_tool_name = self.retriever.retrieve_tool_name(task)\n        retrieve_tool_description_pair = self.retriever.retrieve_tool_description_pair(retrieve_tool_name)\n\n        # decompose task\n        # Set up the generation format error handling mechanism\n        try:\n            self.planner.decompose_task(task, retrieve_tool_description_pair)\n        except Exception as e:\n            print(\"api call failed:\", str(e))  \n            return     \n        return self.planner.sub_task_list\n    \n    def executing(self, tool_name, original_task):\n        \"\"\"\n        Executes a given sub-task as part of the task execution process, handling different types of tasks including code execution, API calls, and question-answering.\n\n        Args:\n            tool_name (str): The name of the tool associated with the sub-task.\n            original_task (object): The original high-level task that has been decomposed into sub-tasks.\n\n        Returns:\n            ExecutionState: The state of execution for the sub-task, including the result, any errors encountered, and additional execution-related information.\n\n        The method dynamically adapts the execution strategy based on the type of sub-task, utilizing the executor component for code execution, API interaction, or question-answering as appropriate.\n        \"\"\"\n        tool_node = self.planner.tool_node[tool_name]\n        description = tool_node.description\n        logging.info(\"The current subtask is: {subtask}\".format(subtask=description))\n        code = ''\n        state = None\n        # The return value of the current task\n        result = ''\n        relevant_code = {}\n        node_type = tool_node.node_type\n        pre_tasks_info = self.planner.get_pre_tasks_info(tool_name)\n        if node_type == 'Python':\n            # retrieve existing tool\n            retrieve_name = self.retriever.retrieve_tool_name(description, 3)\n            relevant_code = self.retriever.retrieve_tool_code_pair(retrieve_name)\n        # task execute step\n        if node_type == 'QA':\n            if self.planner.tool_num == 1:\n                result = self.executor.question_and_answer_tool(pre_tasks_info, original_task, original_task)\n            else:\n                result = self.executor.question_and_answer_tool(pre_tasks_info, original_task, description)\n            print(result)\n            logging.info(result)\n        else:\n            invoke = ''\n            # Set up the generation format error handling mechanism\n            try:\n                if node_type == 'API':\n                    api_path = self.executor.extract_API_Path(description)\n                    code = self.executor.api_tool(description, api_path, pre_tasks_info)\n                else:\n                    code, invoke = self.executor.generate_tool(tool_name, description, node_type, pre_tasks_info, relevant_code)\n            except Exception as e:\n                print(\"api call failed:\", str(e))\n                return\n            # Execute python tool class code\n            state = self.executor.execute_tool(code, invoke, node_type)\n            result = state.result\n            logging.info(state)\n            output = {\n                \"result\": state.result,\n                \"error\": state.error\n            }\n            logging.info(f\"The subtask result is: {json.dumps(output)}\")\n\n        return ExecutionState(state, node_type, description, code, result, relevant_code)\n    \n    def judging(self, tool_name, state, code, description):\n        \"\"\"\n        Evaluates the execution of a tool based on its execution state and the provided code and description, determining whether the tool's execution was successful or requires amendment.\n\n        Args:\n            tool_name (str): The name of the tool being judged.\n            state (ExecutionState): The current execution state of the tool, including results and error information.\n            code (str): The source code associated with the tool's execution.\n            description (str): A description of the tool's intended functionality.\n\n        Returns:\n            JudgementResult: An object encapsulating the judgement on the tool's execution, including whether it needs repair, a critique of the execution, and an optional error type and reasoning for the judgement.\n\n        This method assesses the correctness of the executed code and its alignment with the expected outcomes, guiding potential repair or amendment actions.\n        \"\"\"\n        # Check whether the code runs correctly, if not, amend the code\n        tool_node = self.planner.tool_node[tool_name]\n        next_action = tool_node.next_action\n        critique = ''\n        score = 0\n        # Set up the generation format error handling mechanism\n        try:\n            critique, status, score = self.executor.judge_tool(code, description, state, next_action)\n        except Exception as e:\n            print(\"api call failed:\", str(e))\n            return\n        return JudgementResult(status, critique, score)\n    \n    def replanning(self, tool_name, reasoning):\n        \"\"\"\n        Initiates the replanning process for a task based on new insights or failures encountered during execution, aiming to adjust the plan to better achieve the task goals.\n\n        Args:\n            tool_name (str): The name of the tool related to the task that requires replanning.\n            reasoning (str): The rationale behind the need for replanning, often based on execution failures or updated task requirements.\n\n        Returns:\n            list: An updated list of sub-tasks after the replanning process, intended for sequential execution to complete the task.\n\n        This method identifies alternative or additional tools and their descriptions based on the provided reasoning, updating the task plan accordingly.\n        \"\"\"\n        relevant_tool_name = self.retriever.retrieve_tool_name(reasoning)\n        relevant_tool_description_pair = self.retriever.retrieve_tool_description_pair(relevant_tool_name)\n        # Set up the generation format error handling mechanism\n        try:\n            self.planner.replan_task(reasoning, tool_name, relevant_tool_description_pair)\n        except Exception as e:\n            print(\"api call failed:\", str(e))\n            return\n        return self.planner.sub_task_list\n\n    def repairing(self, tool_name, code, description, state, critique, status):\n        \"\"\"\n        Attempts to repair the execution of a tool by amending its code based on the critique received and the current execution state, iterating until the code executes successfully or reaches the maximum iteration limit.\n\n        Args:\n            tool_name (str): The name of the tool being repaired.\n            code (str): The current code of the tool that requires repairs.\n            description (str): A description of the tool's intended functionality.\n            state (ExecutionState): The current execution state of the tool, including results and error information.\n            critique (str): Feedback on the tool's last execution attempt, identifying issues to be addressed.\n            status (str): Three status types: 'Amend', 'Complete', and 'Replan'.\n\n        Returns:\n            RepairingResult: An object encapsulating the result of the repair attempt, including whether the task has been completed successfully, the amended code, critique, execution score, and the execution result.\n\n        The method iterates, amending the tool's code based on feedback until the code executes correctly or the maximum number of iterations is reached. It leverages the executor component for amending the code and re-evaluating its execution.\n        \"\"\"\n        tool_node = self.planner.tool_node[tool_name]\n        next_action = tool_node.next_action\n        pre_tasks_info = self.planner.get_pre_tasks_info(tool_name)\n        trial_times = 0\n        score = 0\n        while (trial_times < self.executor.max_iter and status == 'Amend'):\n            trial_times += 1\n            print(\"current amend times: {}\".format(trial_times))\n            # Set up the generation format error handling mechanism\n            try:\n                new_code, invoke = self.executor.repair_tool(code, description, tool_node.node_type, state, critique, pre_tasks_info)\n            except Exception as e:\n                print(\"api call failed:\", str(e))\n                return\n            critique = ''\n            code = new_code\n            # Run the current code and check for errors\n            state = self.executor.execute_tool(code, invoke, tool_node.node_type)\n            result = state.result\n            logging.info(state) \n            if state.error == None:\n            # Set up the generation format error handling mechanism\n                try:\n                    critique, status, score = self.executor.judge_tool(code, description, state, next_action)\n                except Exception as e:\n                    print(\"api call failed:\", str(e))\n                    return\n                # The task execution is completed and the loop exits\n                if status == 'Complete':\n                    break\n                elif status == 'Amend':\n                    pass\n                elif status == 'Replan':\n                    break\n                else:\n                    raise NotImplementedError\n            else: # The code still needs to be corrected\n                status = 'Amend'\n        return RepairingResult(status, code, critique, score, result)\n\n    def reset_inner_monologue(self):\n        self.inner_monologue = InnerMonologue()"}
{"type": "source_file", "path": "agentstore/agents/friday_agent.py", "content": "from agentstore.agents.base_agent import BaseAgent\nfrom agentstore.utils import check_os_version\nimport json\nimport logging\nimport sys\nfrom agentstore.prompts.friday_pt import prompt\nfrom agentstore.utils import TaskStatusCode, InnerMonologue, ExecutionState, JudgementResult, RepairingResult\n\n\nclass FridayAgent(BaseAgent):\n    \"\"\"\n    A FridayAgent orchestrates the execution of tasks by integrating planning, retrieving, and executing strategies.\n    \n    This agent is designed to process tasks, manage errors, and refine strategies as necessary to ensure successful task completion. It supports dynamic task planning, information retrieval, execution strategy application, and employs a mechanism for self-refinement in case of execution failures.\n    \"\"\"\n\n    def __init__(self, planner, retriever, executor, Tool_Manager, config):\n        \"\"\"\n        Initializes the FridayAgent with specified planning, retrieving, and executing strategies, alongside configuration settings.\n\n        Args:\n            planner (callable): A strategy for planning the execution of tasks.\n            retriever (callable): A strategy for retrieving necessary information or tools related to the tasks.\n            executor (callable): A strategy for executing planned tasks.\n            Tool_Manager (callable): A tool manager for handling tool-related operations.\n            config (object): Configuration settings for the agent.\n\n        Raises:\n            ValueError: If the OS version check fails.\n        \"\"\"\n        super().__init__()\n        self.config = config\n        tool_manager = Tool_Manager(config.generated_tool_repo_path)\n        self.planner = planner(prompt['planning_prompt'])\n        self.retriever = retriever(prompt['retrieve_prompt'], tool_manager)\n        self.executor = executor(prompt['execute_prompt'], tool_manager, config.max_repair_iterations)\n        self.score = self.config.score\n        self.task_status = TaskStatusCode.START\n        self.inner_monologue = InnerMonologue()\n        try:\n            check_os_version(self.system_version)\n        except ValueError as e:\n            print(e)        \n\n    def run(self, task):\n        \"\"\"\n        Executes the given task by planning, executing, and refining as needed until the task is completed or fails.\n\n        Args:\n            query (object): The high-level task to be executed.\n\n        No explicit return value, but the method controls the flow of task execution and may exit the process in case of irreparable failures.\n        \"\"\"\n        self.planner.reset_plan()\n        self.reset_inner_monologue()\n        sub_tasks_list = self.planning(task)\n        print(\"The task list obtained after planning is: {}\".format(sub_tasks_list))\n\n        while self.planner.sub_task_list:\n            sub_task = self.planner.sub_task_list.pop(0)\n            execution_state = self.executing(sub_task, task)\n            isTaskCompleted, isReplan = self.self_refining(sub_task, execution_state)\n            if isReplan: continue\n            if isTaskCompleted:\n                print(\"The execution of the current sub task has been successfully completed.\")\n            else:\n                print(\"{} not completed in repair round {}\".format(sub_task, self.config.max_repair_iterations))\n                # sys.exit()\n                break\n\n    def self_refining(self, tool_name, execution_state: ExecutionState):\n        \"\"\"\n        Analyzes and potentially refines the execution of a tool based on its current execution state. \n        This can involve replanning or repairing the execution strategy based on the analysis of execution errors and outcomes.\n\n        Args:\n            tool_name (str): The name of the tool being executed.\n            execution_state (ExecutionState): The current state of the tool's execution, encapsulating all relevant execution information including errors, results, and codes.\n\n        Returns:\n            tuple:\n                - isTaskCompleted (bool): Indicates whether the task associated with the tool has been successfully completed.\n                - isReplan (bool): Indicates whether a replan is required due to execution state analysis.\n\n        The method decides on the next steps by analyzing the type of error (if any) and the execution results, aiming to either complete the task successfully or identify the need for further action, such as replanning.\n        \"\"\"\n        isTaskCompleted = False\n        isReplan = False\n        score = 0\n        state, node_type, description, code, result, relevant_code = execution_state.get_all_state()\n        if node_type in ['Python', 'Shell', 'AppleScript']:\n            judgement = self.judging(tool_name, state, code, description)\n            score = judgement.score\n            # need_repair, critique, score, reasoning, error_type \n            if judgement.status == 'Replan':\n                # raise NotImplementedError\n                print(\"The current task requires replanning...\")\n                new_sub_task_list = self.replanning(tool_name, judgement.critique)\n                print(\"The new task list obtained after planning is: {}\".format(new_sub_task_list))\n                isReplan = True\n            elif judgement.status == 'Amend':\n                repairing_result = self.repairing(tool_name, code, description, state, judgement.critique, judgement.status)\n                if repairing_result.status == 'Complete':\n                    isTaskCompleted = True\n                elif repairing_result.status == 'Replan':\n                    print(\"The current task requires replanning...\")\n                    new_sub_task_list = self.replanning(tool_name, repairing_result.critique)\n                    print(\"The new task list obtained after planning is: {}\".format(new_sub_task_list))\n                    isReplan = True\n                else:\n                    isTaskCompleted = False\n                score = repairing_result.score\n                result = repairing_result.result\n            else:\n                isTaskCompleted = True\n            if node_type == 'Python' and isTaskCompleted and score >= self.score:\n                self.executor.store_tool(tool_name, code)\n                print(\"{} has been stored in the tool repository.\".format(tool_name))\n        else: \n            isTaskCompleted = True\n        if isTaskCompleted:\n            self.inner_monologue.result = result\n            self.planner.update_tool(tool_name, result, relevant_code, True, node_type)\n        return isTaskCompleted, isReplan\n\n    def planning(self, task):\n        \"\"\"\n        Decomposes a given high-level task into a list of sub-tasks by retrieving relevant tool names and descriptions, facilitating structured execution planning.\n\n        Args:\n            task (object): The high-level task to be planned and executed.\n\n        Returns:\n            list: A list of sub-tasks generated by decomposing the high-level task, intended for sequential execution to achieve the task's goal.\n\n        This method leverages the retriever component to fetch information relevant to the task, which is then used by the planner component to decompose the task into manageable sub-tasks.\n        \"\"\"\n        retrieve_tool_name = self.retriever.retrieve_tool_name(task)\n        retrieve_tool_description_pair = self.retriever.retrieve_tool_description_pair(retrieve_tool_name)\n\n        # decompose task\n        # Set up the generation format error handling mechanism\n        try:\n            self.planner.decompose_task(task, retrieve_tool_description_pair)\n        except Exception as e:\n            print(\"api call failed:\", str(e))  \n            return     \n        return self.planner.sub_task_list\n    \n    def executing(self, tool_name, original_task):\n        \"\"\"\n        Executes a given sub-task as part of the task execution process, handling different types of tasks including code execution, API calls, and question-answering.\n\n        Args:\n            tool_name (str): The name of the tool associated with the sub-task.\n            original_task (object): The original high-level task that has been decomposed into sub-tasks.\n\n        Returns:\n            ExecutionState: The state of execution for the sub-task, including the result, any errors encountered, and additional execution-related information.\n\n        The method dynamically adapts the execution strategy based on the type of sub-task, utilizing the executor component for code execution, API interaction, or question-answering as appropriate.\n        \"\"\"\n        tool_node = self.planner.tool_node[tool_name]\n        description = tool_node.description\n        logging.info(\"The current subtask is: {subtask}\".format(subtask=description))\n        code = ''\n        state = None\n        # The return value of the current task\n        result = ''\n        relevant_code = {}\n        node_type = tool_node.node_type\n        pre_tasks_info = self.planner.get_pre_tasks_info(tool_name)\n        if node_type == 'Python':\n            # retrieve existing tool\n            retrieve_name = self.retriever.retrieve_tool_name(description, 3)\n            relevant_code = self.retriever.retrieve_tool_code_pair(retrieve_name)\n        # task execute step\n        if node_type == 'QA':\n            if self.planner.tool_num == 1:\n                result = self.executor.question_and_answer_tool(pre_tasks_info, original_task, original_task)\n            else:\n                result = self.executor.question_and_answer_tool(pre_tasks_info, original_task, description)\n            print(result)\n            logging.info(result)\n        else:\n            invoke = ''\n            # Set up the generation format error handling mechanism\n            try:\n                if node_type == 'API':\n                    api_path = self.executor.extract_API_Path(description)\n                    code = self.executor.api_tool(description, api_path, pre_tasks_info)\n                else:\n                    code, invoke = self.executor.generate_tool(tool_name, description, node_type, pre_tasks_info, relevant_code)\n            except Exception as e:\n                print(\"api call failed:\", str(e))\n                return\n            # Execute python tool class code\n            state = self.executor.execute_tool(code, invoke, node_type)\n            result = state.result\n            logging.info(state)\n            output = {\n                \"result\": state.result,\n                \"error\": state.error\n            }\n            logging.info(f\"The subtask result is: {json.dumps(output)}\")\n\n        return ExecutionState(state, node_type, description, code, result, relevant_code)\n    \n    def judging(self, tool_name, state, code, description):\n        \"\"\"\n        Evaluates the execution of a tool based on its execution state and the provided code and description, determining whether the tool's execution was successful or requires amendment.\n\n        Args:\n            tool_name (str): The name of the tool being judged.\n            state (ExecutionState): The current execution state of the tool, including results and error information.\n            code (str): The source code associated with the tool's execution.\n            description (str): A description of the tool's intended functionality.\n\n        Returns:\n            JudgementResult: An object encapsulating the judgement on the tool's execution, including whether it needs repair, a critique of the execution, and an optional error type and reasoning for the judgement.\n\n        This method assesses the correctness of the executed code and its alignment with the expected outcomes, guiding potential repair or amendment actions.\n        \"\"\"\n        # Check whether the code runs correctly, if not, amend the code\n        tool_node = self.planner.tool_node[tool_name]\n        next_action = tool_node.next_action\n        critique = ''\n        score = 0\n        # Set up the generation format error handling mechanism\n        try:\n            critique, status, score = self.executor.judge_tool(code, description, state, next_action)\n        except Exception as e:\n            print(\"api call failed:\", str(e))\n            return\n        return JudgementResult(status, critique, score)\n    \n    def replanning(self, tool_name, reasoning):\n        \"\"\"\n        Initiates the replanning process for a task based on new insights or failures encountered during execution, aiming to adjust the plan to better achieve the task goals.\n\n        Args:\n            tool_name (str): The name of the tool related to the task that requires replanning.\n            reasoning (str): The rationale behind the need for replanning, often based on execution failures or updated task requirements.\n\n        Returns:\n            list: An updated list of sub-tasks after the replanning process, intended for sequential execution to complete the task.\n\n        This method identifies alternative or additional tools and their descriptions based on the provided reasoning, updating the task plan accordingly.\n        \"\"\"\n        relevant_tool_name = self.retriever.retrieve_tool_name(reasoning)\n        relevant_tool_description_pair = self.retriever.retrieve_tool_description_pair(relevant_tool_name)\n        # Set up the generation format error handling mechanism\n        try:\n            self.planner.replan_task(reasoning, tool_name, relevant_tool_description_pair)\n        except Exception as e:\n            print(\"api call failed:\", str(e))\n            return\n        return self.planner.sub_task_list\n\n    def repairing(self, tool_name, code, description, state, critique, status):\n        \"\"\"\n        Attempts to repair the execution of a tool by amending its code based on the critique received and the current execution state, iterating until the code executes successfully or reaches the maximum iteration limit.\n\n        Args:\n            tool_name (str): The name of the tool being repaired.\n            code (str): The current code of the tool that requires repairs.\n            description (str): A description of the tool's intended functionality.\n            state (ExecutionState): The current execution state of the tool, including results and error information.\n            critique (str): Feedback on the tool's last execution attempt, identifying issues to be addressed.\n            status (str): Three status types: 'Amend', 'Complete', and 'Replan'.\n\n        Returns:\n            RepairingResult: An object encapsulating the result of the repair attempt, including whether the task has been completed successfully, the amended code, critique, execution score, and the execution result.\n\n        The method iterates, amending the tool's code based on feedback until the code executes correctly or the maximum number of iterations is reached. It leverages the executor component for amending the code and re-evaluating its execution.\n        \"\"\"\n        tool_node = self.planner.tool_node[tool_name]\n        next_action = tool_node.next_action\n        pre_tasks_info = self.planner.get_pre_tasks_info(tool_name)\n        trial_times = 0\n        score = 0\n        while (trial_times < self.executor.max_iter and status == 'Amend'):\n            trial_times += 1\n            print(\"current amend times: {}\".format(trial_times))\n            # Set up the generation format error handling mechanism\n            try:\n                new_code, invoke = self.executor.repair_tool(code, description, tool_node.node_type, state, critique, pre_tasks_info)\n            except Exception as e:\n                print(\"api call failed:\", str(e))\n                return\n            critique = ''\n            code = new_code\n            # Run the current code and check for errors\n            state = self.executor.execute_tool(code, invoke, tool_node.node_type)\n            result = state.result\n            logging.info(state) \n            if state.error == None:\n            # Set up the generation format error handling mechanism\n                try:\n                    critique, status, score = self.executor.judge_tool(code, description, state, next_action)\n                except Exception as e:\n                    print(\"api call failed:\", str(e))\n                    return\n                # The task execution is completed and the loop exits\n                if status == 'Complete':\n                    break\n                elif status == 'Amend':\n                    pass\n                elif status == 'Replan':\n                    break\n                else:\n                    raise NotImplementedError\n            else: # The code still needs to be corrected\n                status = 'Amend'\n        return RepairingResult(status, code, critique, score, result)\n\n    def reset_inner_monologue(self):\n        self.inner_monologue = InnerMonologue()"}
{"type": "source_file", "path": "agentstore/agents/office_agent.py", "content": "from agentstore.utils import setup_config, setup_pre_run\nfrom agentstore.modules.base_module import BaseModule\nimport re\nimport os\nimport json\nimport base64\nimport time\nfrom rich.console import Console\nfrom rich.markdown import Markdown\n\nfrom desktop_env.desktop_env import DesktopEnv\n\nfrom agentstore.prompts.osworld_pt import prompt_office\n\nconsole = Console()\n\ndef encode_image(image_content):\n    return base64.b64encode(image_content).decode('utf-8')\n\ndef rich_print(markdown_text):\n    md = Markdown(markdown_text)\n    console.print(md)\n\n\ndef send_chat_prompts(message, llm):\n    return llm.chat(message)\n\n    \ndef extract_code(input_string):\n    pattern = r\"```(\\w+)?\\s*(.*?)```\"  # 匹配代码块，语言名称是可选项\n    matches = re.findall(pattern, input_string, re.DOTALL)\n\n    if matches:\n        language, code = matches[0]\n\n        # 如果没有语言信息，尝试从代码内容中检测\n        if not language:\n            if re.search(\"python\", code.lower()) or re.search(r\"import\\s+\\w+\", code):\n                language = \"python\"\n            elif re.search(\"bash\", code.lower()) or re.search(r\"echo\", code):\n                language = \"bash\"\n\n        if language == 'bash':\n            code = code.strip()\n            code = code.replace('\"', '\\\\\"')\n            code = code.replace('\\n', ' && ')\n            code = \"pyautogui.typewrite(\\\"{0}\\\", interval=0.05)\\npyautogui.press('enter')\\ntime.sleep(2)\".format(code)\n            if re.search(\"sudo\", code.lower()):\n                code += \"\\npyautogui.typewrite('password', interval=0.05)\\npyautogui.press('enter')\\ntime.sleep(1)\"\n        elif language == 'python':\n\n            # save code\n            with open('tmp.py', 'w') as f:\n                f.write(code)\n\n            code = \"pyautogui.typewrite(\\\"python main.py\\\", interval=0.05)\\npyautogui.press('enter')\\ntime.sleep(2)\"\n            \n        else:\n            print(language)\n            raise language\n\n        return code, language\n    else:\n        return None, None\n\ndef encode_image(image_content):\n    return base64.b64encode(image_content).decode('utf-8')\n\nclass OfficeAgent(BaseModule):\n\n    def __init__(self, args, config, env, obs=None):\n        super().__init__()\n        self.args = args\n\n        self.environment = env\n\n        self.task_name = config['instruction']\n\n        self.reply = None\n\n        domain = config['snapshot']\n        example_id = config['id']\n\n        result_dir = 'D:\\jcy\\OS-Copilot\\\\results'\n        self.example_result_dir = os.path.join(\n                result_dir,\n                domain,\n                example_id\n            )\n\n        os.makedirs(self.example_result_dir, exist_ok=True)\n\n        if obs != None:\n            self.init_image = encode_image(obs['screenshot'])\n\n\n    \n    def execute_tool(self, code, lang):\n        if lang == 'python':\n            file = [{\n                \"local_path\": 'tmp.py',\n                \"path\": '/home/user/main.py'\n              }]\n            self.environment.setup_controller._upload_file_setup(file)\n        obs, reward, done, info = self.environment.step(code)  # node_type\n        \n        reply = self.environment.controller.get_terminal_output()\n        # update reply\n        if self.reply and reply:\n            message_terminal = reply.replace(self.reply, \"\")\n        else:\n            message_terminal = reply\n        self.reply = reply\n\n        base64_image = encode_image(obs['screenshot'])\n        print(\"message_terminal\", message_terminal)\n        if 'gpt' in self.llm.model_name:\n            message = {\n                    \"role\": \"user\",\n                    \"content\": [\n                        {\n                            \"type\": \"text\",\n                            \"text\": \"After executing the command, the terminal output is {0}. the screenshot as below.\".format(message_terminal)\n                        }\n                        # {\n                        #     \"type\": \"image_url\",\n                        #     \"image_url\": {\n                        #         \"url\": f\"data:image/png;base64,{self.init_image}\",\n                        #         \"detail\": \"high\"\n                        #     }\n                        # }\n                    ]\n            }\n        else:\n            message = {\n                    \"role\": \"user\",\n                    \"content\": \"After executing the command, the terminal output is {0}. the screenshot as below.\".format(message_terminal)\n            }\n        return message\n# When a user refers to a file opened， you can use the provided screenshot to find the filepath.\n    def run(self):\n\n        while not self.environment.controller.get_terminal_output():\n            self.environment.step(\"pyautogui.click()\")\n            self.environment.step(\"pyautogui.hotkey('ctrl', 'alt', 't')\")\n            print(self.environment.controller.get_terminal_output())\n\n        light_planner_sys_prompt = prompt_office['SYS_PROMPT_IN_CLI_IMPRESS']\n        \n        \n         #  Try to use `print` or `echo` to output information needed for the subsequent tasks, or the next step might not get the required information.\n        light_planner_user_prompt = '''\n        User's information are as follows:\n        System Version: Ubuntu\n        Task Name: {task_name}\n        '''.format(task_name=self.task_name)\n        \n        message = [\n            {\"role\": \"system\", \"content\": light_planner_sys_prompt},\n            {\"role\": \"user\", \"content\": [\n                        {\n                            \"type\": \"text\",\n                            \"text\": light_planner_user_prompt + \"The screenshot as below.\"\n                        }\n                        # {\n                        #     \"type\": \"image_url\",\n                        #     \"image_url\": {\n                        #         \"url\": f\"data:image/png;base64,{self.init_image}\",\n                        #         \"detail\": \"high\"\n                        #     }\n                        # }\n                    ]}\n        ]\n        self.example_result_dir\n        step_idx = 0\n        while step_idx < 12:\n            print(\"send_chat_prompts...\")\n            response = send_chat_prompts(message, self.llm)\n            print(response)\n            message.append({\"role\": \"assistant\", \"content\": response})\n            code, lang = extract_code(response)\n\n            import datetime\n            action_timestamp = datetime.datetime.now().strftime(\"%Y%m%d@%H%M%S\")\n\n            with open(os.path.join(self.example_result_dir, \"traj.jsonl\"), \"a\") as f:\n                f.write(json.dumps({\n                    \"step_num\": step_idx + 1,\n                    \"action_timestamp\": action_timestamp,\n                    \"response\": response,\n                    \"code\": code,\n                    \"lang\": lang\n                }))\n\n            if code:\n                result = self.execute_tool(code, lang)\n            else:\n                result = ''\n\n            if result != '':\n                message.append(result)\n            else:\n                message.append({\"role\": \"user\", \"content\": \"Please continue. If all tasks have been completed, reply with 'Execution Complete'. If you believe subsequent tasks cannot continue, reply with 'Execution Interrupted', including the reasons why the tasks cannot proceed, and provide the user with some possible solutions.\"})\n            if 'Execution Complete' in response or 'Execution Interrupted' in response:\n                break\n\n            else:\n                step_idx += 1   \n\n        return response\n\n\ndef replace_path(obj,old_path,new_path):\n    if isinstance(obj, dict):\n        for key, value in obj.items():\n            obj[key] = replace_path(value,old_path,new_path)\n    elif isinstance(obj, list):\n        obj = [replace_path(item,old_path,new_path) for item in obj]\n    elif isinstance(obj, str):\n        obj = obj.replace(old_path, new_path)\n    return obj\n\n\nif __name__ == '__main__':\n\n\n    environment = DesktopEnv(\n            path_to_vm=r\"D:/jcy/OSWorld_new/vmware_vm_data/Ubuntu0/Ubuntu0.vmx\",\n            action_space=\"pyautogui\",\n            require_a11y_tree=True,\n        )\n\n    test_all_meta_path = \"D:\\\\jcy\\\\OSWorld_new\\\\evaluation_examples/test_all.json\"\n    example_path = \"D:\\\\jcy\\\\OSWorld_new\\\\evaluation_examples\"\n\n    with open(test_all_meta_path, \"r\", encoding=\"utf-8\") as f:\n        test_all_meta = json.load(f)\n    # libreoffice_impress\n    domain = 'multi_apps'\n    tasks = test_all_meta[domain]\n\n    # run_list_file_path = r'D:\\jcy\\OS-Copilot\\run_list.txt'\n    # run_list = []\n\n    # with open(run_list_file_path, 'r') as file:\n    #     for line in file:\n    #         run_list.append(line.strip())\n\n    for example_id in tasks[:]:\n        if example_id != '236833a3-5704-47fc-888c-4f298f09f799':\n            continue\n        print(example_id)\n        config_file = os.path.join(example_path, f\"examples/{domain}/{example_id}.json\")\n        with open(config_file, \"r\", encoding=\"utf-8\") as f:\n            example = json.load(f)\n        task_name = example['instruction']\n        \n        print('task_name:', task_name)\n        example = replace_path(example, 'evaluation_examples/settings', 'D:\\\\jcy\\\\OSWorld_new\\\\evaluation_examples\\\\settings')\n        \n        previous_obs = environment.reset(task_config=example)\n        args = setup_config()\n\n        \n        excel_agent = OfficeAgent(args, example, environment,obs=previous_obs)\n        excel_agent.run()\n\n        # 判定内容\n        input()\n        print(\"evaluate.......\")\n        eval_ = environment.evaluate()\n        print(eval_) \n\n        with open(os.path.join(excel_agent.example_result_dir, \"traj.jsonl\"), \"a\") as f:\n                f.write(json.dumps({\n                    \"evaluate\": eval_\n                }))\n        break"}
{"type": "source_file", "path": "agentstore/agents/excel_agent.py", "content": "from agentstore.utils import setup_config, setup_pre_run\nfrom agentstore.modules.base_module import BaseModule\nimport re\nimport os\nimport json\nimport base64\nimport time\nfrom rich.console import Console\nfrom rich.markdown import Markdown\n\nfrom desktop_env.envs.desktop_env import DesktopEnv\n\nconsole = Console()\n\ndef encode_image(image_content):\n    return base64.b64encode(image_content).decode('utf-8')\n\ndef rich_print(markdown_text):\n    md = Markdown(markdown_text)\n    console.print(md)\n\n\ndef send_chat_prompts(message, llm):\n    return llm.chat(message)\n\n    \ndef extract_code(input_string):\n    pattern = r\"```(\\w+)?\\s*(.*?)```\"  # 匹配代码块，语言名称是可选项\n    matches = re.findall(pattern, input_string, re.DOTALL)\n\n    if matches:\n        language, code = matches[0]\n\n        # 如果没有语言信息，尝试从代码内容中检测\n        if not language:\n            if re.search(\"python\", code.lower()) or re.search(r\"import\\s+\\w+\", code):\n                language = \"python\"\n            elif re.search(\"bash\", code.lower()) or re.search(r\"echo\", code):\n                language = \"bash\"\n\n        if language == 'bash':\n            code = code.strip()\n            code = code.replace('\"', '\\\\\"')\n            code = code.replace('\\n', ' && ')\n            code = \"pyautogui.typewrite(\\\"{0}\\\", interval=0.05)\\npyautogui.press('enter')\\ntime.sleep(2)\".format(code)\n            if re.search(\"sudo\", code.lower()):\n                code += \"\\npyautogui.typewrite('password', interval=0.05)\\npyautogui.press('enter')\\ntime.sleep(1)\"\n        elif language == 'python':\n\n            # save code\n            with open('tmp.py', 'w') as f:\n                f.write(code)\n\n            code = \"pyautogui.typewrite(\\\"python main.py\\\", interval=0.05)\\npyautogui.press('enter')\\ntime.sleep(2)\"\n            \n        else:\n            print(language)\n            raise language\n\n        return code, language\n    else:\n        return None, None\n\nfrom desktop_env.envs.desktop_env import DesktopEnv\nclass ExcelAgent(BaseModule):\n    info = {\n            \"name\": \"ExcelAgent\",\n            \"can do\": \"specializes in creating, analyzing, and modifying Excel spreadsheets using Python's openpyxl library. It can handle tasks involving data entry, formula insertion, chart creation, and spreadsheet formatting. Also capable of detecting open Excel files using Bash commands.\",\n            \"can't do\": \"cannot handle GUI operations, cannot perform tasks outside the capabilities of the openpyxl library such as directly interacting with complex macros. Additionally, cannot modify LibreOffice Calc software defaults or preferences.\"\n    }\n    def __init__(self, args, task_name, env):\n        super().__init__()\n        self.args = args\n\n        self.environment = env\n\n        self.task_name = task_name\n\n        self.reply = None\n\n        \n\n    \n    def execute_tool(self, code, lang):\n        if lang == 'python':\n            file = [{\n                \"local_path\": 'tmp.py',\n                \"path\": '/home/user/main.py'\n              }]\n            self.environment.setup_controller._upload_file_setup(file)\n        obs, reward, done, info = self.environment.step(code)  # node_type\n        \n        reply = self.environment.controller.get_terminal_output()\n        # update reply\n        if self.reply and reply:\n            message_terminal = reply.replace(self.reply, \"\")\n        else:\n            message_terminal = reply\n        self.reply = reply\n\n        base64_image = encode_image(obs['screenshot'])\n        print(\"message_terminal\", message_terminal)\n        if 'gpt' in self.llm.model_name:\n            message = {\n                    \"role\": \"user\",\n                    \"content\": [\n                        {\n                            \"type\": \"text\",\n                            \"text\": \"After executing the command, the terminal output is {0}. the screenshot as below.\".format(message_terminal)\n                        }\n                        # {\n                        #     \"type\": \"image_url\",\n                        #     \"image_url\": {\n                        #         \"url\": f\"data:image/png;base64,{base64_image}\",\n                        #         \"detail\": \"high\"\n                        #     }\n                        # }\n                    ]\n            }\n        else:\n            message = {\n                    \"role\": \"user\",\n                    \"content\": \"After executing the command, the terminal output is {0}. the screenshot as below.\".format(message_terminal)\n            }\n        return message\n# When a user refers to a file opened， you can use the provided screenshot to find the filepath.\n    def run(self):\n\n        while not self.environment.controller.get_terminal_output():\n            self.environment.step(\"pyautogui.click()\")\n            self.environment.step(\"pyautogui.hotkey('ctrl', 'alt', 't')\")\n\n        light_planner_sys_prompt = '''You are ExcelAgent, an advanced programming assistant specialized in managing and manipulating Excel files. \n        Your abilities extend to manipulating slides using Python's openpyxl library.\nFirst, write a plan. **Always recap the plan between each code block** (you have extreme short-term memory loss, so you need to recap the plan between each message block to retain it).\nWhen you execute code, it will be executed **on the user's machine**. The user has given you **full and complete permission** to execute any code necessary to complete the task. Execute the code.\nIf you want to send data between programming languages, save the data to a txt or json.\nYou can access the internet. Run **any code** to achieve the goal, and if at first you don't succeed, try again and again.\nYou can install new packages.\nImportant: When a user refers to a file opened or a url, you can use your visual capacity or command line tools such as \"lsof | grep -E '\\.ods|\\.xlsx'\" to see the path of the file being opened.\nFirst thing need to do is pip install openpyxl.\nWrite messages to the user in Markdown.\nIn general, try to **make plans** with as few steps as possible. As for actually executing code to carry out that plan, for *stateful* languages (like python, javascript, shell, but NOT for html which starts from 0 every time) **it's critical not to try to do everything in one code block.** You should try something, print information about it, then continue from there in tiny, informed steps. You will never get it on the first try, and attempting it in one go will often lead to errors you cant see.\nYou are capable of **any** task.\nImport： Ensure that the name of the new sheet follows a default incremental naming pattern, starting from \"Sheet1\" followed by a numeral. If the workbook already contains sheets named \"Sheet1\", the next created sheet should be named \"Sheet2\", and so on. \nExample Command in Bash:\n```bash\npip install openpyxl && lsof | grep '.xlsx'\n```\nExample Operation in Python:\n```python\nfrom openpyxl import Workbook\n\n# Create a new workbook or open an existing workbook\nwb = Workbook()\nws = wb.active\n\n# Adding data to the workbook\nws['A1'] = \"Hello, ExcelAgent\"\nws['A2'] = \"This is data added by ExcelAgent.\"\n\n# Save the workbook, overwriting the original\nwb.save('path_to_spreadsheet.xlsx')\nprint(\"Python Script Executed Successfully!!!\")\n```\nCurrently, supported languages only include Python and Bash.\nEach time you only need to output the next command and wait for a reply.\n'''  #  Try to use `print` or `echo` to output information needed for the subsequent tasks, or the next step might not get the required information.\n        light_planner_user_prompt = '''\n        User's information are as follows:\n        System Version: Ubuntu\n        Task Name: {task_name}\n        '''.format(task_name=self.task_name)\n        \n        message = [\n            {\"role\": \"system\", \"content\": light_planner_sys_prompt},\n            {\"role\": \"user\", \"content\": light_planner_user_prompt},\n        ]\n        cnt = 0\n        while True:\n            print(\"send_chat_prompts...\")\n            response = send_chat_prompts(message, self.llm)\n            print(response)\n            message.append({\"role\": \"assistant\", \"content\": response})\n\n            code, lang = extract_code(response)\n            if code:\n                result = self.execute_tool(code, lang)\n            else:\n                result = ''\n\n            if result != '':\n                message.append(result)\n            else:\n                message.append({\"role\": \"user\", \"content\": \"Please continue. If all tasks have been completed, reply with 'Execution Complete'. If you believe subsequent tasks cannot continue, reply with 'Execution Interrupted', including the reasons why the tasks cannot proceed, and provide the user with some possible solutions.\"})\n            \n            if 'Execution Complete' in response or 'Execution Interrupted' in response:\n                break\n            if cnt > 12:\n                break\n            else:\n                cnt += 1   \n\n        return response\n\n\nif __name__ == '__main__':\n\n\n    environment = DesktopEnv(\n            path_to_vm=r\"D:/jcy/OSWorld/vm_data/Ubuntu0/Ubuntu0/Ubuntu0.vmx\",\n            action_space=\"pyautogui\",\n            require_a11y_tree=True,\n        )\n\n\n    test_all_meta_path = \"D:\\\\jcy\\\\OSWorld\\\\evaluation_examples/test_all.json\"\n    example_path = \"D:\\\\jcy\\\\OSWorld\\\\evaluation_examples\"\n\n    with open(test_all_meta_path, \"r\", encoding=\"utf-8\") as f:\n        test_all_meta = json.load(f)\n\n    domain = 'libreoffice_calc'\n    tasks = test_all_meta[domain]\n\n    # run_list_file_path = r'D:\\jcy\\OS-Copilot\\run_list.txt'\n    # run_list = []\n\n    # with open(run_list_file_path, 'r') as file:\n    #     for line in file:\n    #         run_list.append(line.strip())\n\n    for example_id in tasks[:]:\n        try:\n            print(example_id)\n            config_file = os.path.join(example_path, f\"examples/{domain}/{example_id}.json\")\n            with open(config_file, \"r\", encoding=\"utf-8\") as f:\n                example = json.load(f)\n            task_name = example['instruction']\n            \n            log_file_path = os.path.join(\"cache\",example_id, f\"{example_id}.log\")\n            os.makedirs(os.path.dirname(log_file_path), exist_ok=True)\n            \n            with open(log_file_path, 'w', encoding=\"utf-8\") as log_file:\n                with contextlib.redirect_stdout(log_file):\n                    print('task_name:', task_name)\n                    example = replace_path(example, 'evaluation_examples/settings', 'D:\\\\jcy\\\\OSWorld\\\\evaluation_examples\\\\settings')\n                    \n                    previous_obs = environment.reset(task_config=example)\n                    args = setup_config()\n                    \n                    # plan_agent = PlanAgent(args, task_name, agent_info_list)\n                    # info = plan_agent.run(previous_obs)\n\n                    info = 'ImageAgent'\n                    \n                    if 'CLIAgent' in info:\n                        cli_agent = CLIAgent(args, task_name, environment)\n                        cli_agent.run(info)\n                    \n                    elif 'GUIAgent' in info:\n                        action_space = 'pyautogui'\n                        observation_type = 'screenshot_a11y_tree'\n                        max_trajectory_length = 3\n                        gui_agent = GUIAgent(args, example, environment, action_space, observation_type, max_trajectory_length)\n                        gui_agent.run()\n                    \n                    elif 'WordAgent' in info:\n                        word_agent = WordAgent(args, task_name, environment)\n                        word_agent.run()\n                    \n                    elif 'PptxAgent' in info:\n                        pptx_agent = PptxAgent(args, task_name, environment)\n                        pptx_agent.run()\n                    \n                    elif 'ExcelAgent' in info:\n                        excel_agent = ExcelAgent(args, task_name, environment)\n                        excel_agent.run()\n                    elif 'ImageAgent' in info:\n                        excel_agent = ImageAgent(args, task_name, environment)\n                        excel_agent.run()\n                    else:\n                        # replan \n                        # to be update\n                        pass\n                    \n                    # 判定内容\n                    print(\"evaluate.......\")\n                    print(environment.evaluate())\n        except Exception as e:\n            error_log_path = os.path.join(\"cache\",example_id, f\"{example_id}_error.log\")\n            os.makedirs(os.path.dirname(log_file_path), exist_ok=True)\n            with open(error_log_path, 'w', encoding=\"utf-8\") as error_log:\n                error_log.write(str(e))\n            print(e)\n"}
{"type": "source_file", "path": "agentstore/agents/gui_agent.py", "content": "from agentstore.agents.base_agent import BaseAgent\nfrom agentstore.utils import check_os_version\nfrom agentstore.utils import setup_config, setup_pre_run\nimport json\nimport logging\nimport os\nimport sys\nfrom agentstore.prompts.osworld_pt import prompt\nfrom agentstore.utils import TaskStatusCode, InnerMonologue, ExecutionState, JudgementResult, RepairingResult\n\nfrom agentstore.utils.osworld_parse import parse_actions_from_string, parse_code_from_string, parse_code_from_som_string\n\nfrom agentstore.utils.llms import OpenAI, LLAMA\nfrom dotenv import load_dotenv\n\nfrom agentstore.utils.parse_obs import parse_obs\n\nimport contextlib\nfrom desktop_env.desktop_env import DesktopEnv\n\nload_dotenv(override=True)\nMODEL_TYPE = os.getenv('MODEL_TYPE')\n\nclass GUIAgent(BaseAgent):\n    def __init__(self, args, config, env, action_space, observation_type, max_trajectory_length, max_steps=10):\n        super().__init__()\n        try:\n            check_os_version(self.system_version)\n        except ValueError as e:\n            print(e)\n\n        self.environment = env\n        self.task_name = config['instruction']\n\n        domain = config['snapshot']\n        example_id = config['id']\n\n        self.action_space = action_space\n        self.observation_type = observation_type\n        self._get_system_message(self.observation_type, self.action_space)\n        self.a11y_tree_max_tokens = 2000\n        self.max_trajectory_length = max_trajectory_length\n        self.max_steps = max_steps\n        self.sleep_after_execution = 0.0\n        result_dir = 'D:\\jcy\\OS-Copilot\\\\results'\n        self.example_result_dir = os.path.join(\n            result_dir,\n            self.action_space,\n            self.observation_type,\n            domain,\n            example_id\n        )\n\n        os.makedirs(self.example_result_dir, exist_ok=True)\n\n        if MODEL_TYPE == \"OpenAI\":\n            self.llm = OpenAI()\n        elif MODEL_TYPE == \"LLAMA\":\n            self.llm = LLAMA()\n\n        self.reset()\n\n    def run(self):\n        step_idx = 0\n        obs, reward, done, info = self.environment.step(\"\")\n        self.environment.controller.start_recording()\n\n        while not done and step_idx < self.max_steps:\n            obs = parse_obs(obs, self.observation_type)\n            response, actions = self.predict(obs)\n            for action in actions:\n                import datetime\n                action_timestamp = datetime.datetime.now().strftime(\"%Y%m%d@%H%M%S\")\n                obs, reward, done, info = self.environment.step(action, self.sleep_after_execution)\n                print(\"Done: %s\", done)\n                with open(os.path.join(self.example_result_dir, f\"step_{step_idx + 1}_{action_timestamp}.png\"), \"wb\") as _f:\n                    _f.write(obs['screenshot'])\n\n                with open(os.path.join(self.example_result_dir, \"traj.jsonl\"), \"a\") as f:\n                    f.write(json.dumps({\n                        \"step_num\": step_idx + 1,\n                        \"action_timestamp\": action_timestamp,\n                        \"action\": action,\n                        \"reward\": reward,\n                        \"done\": done,\n                        \"info\": info,\n                        \"screenshot_file\": f\"step_{step_idx + 1}_{action_timestamp}.png\"\n                    }))\n                    f.write(\"\\n\")\n                if done:\n                    print(\"The episode is done.\")\n                    break\n            step_idx += 1\n\n    def predict(self, obs):\n        messages = self._get_message(self.task_name, obs)\n        print(\"Generating content with GPT model:\")\n        response = self.llm.chat(messages)\n        print(\"RESPONSE: %s\", response)\n\n        try:\n            actions = self.parse_actions(response)\n            self.thoughts.append(response)\n        except ValueError as e:\n            print(\"Failed to parse action from response\", e)\n            actions = None\n            self.thoughts.append(\"\")\n        return response, actions\n\n    def parse_actions(self, response: str, masks=None):\n        if self.observation_type in [\"screenshot\", \"a11y_tree\", \"screenshot_a11y_tree\"]:\n            if self.action_space == \"computer_13\":\n                actions = parse_actions_from_string(response)\n            elif self.action_space == \"pyautogui\":\n                actions = parse_code_from_string(response)\n            else:\n                raise ValueError(\"Invalid action space: \" + self.action_space)\n\n            self.actions.append(actions)\n            return actions\n        elif self.observation_type in [\"som\"]:\n            if self.action_space == \"computer_13\":\n                raise ValueError(\"Invalid action space: \" + self.action_space)\n            elif self.action_space == \"pyautogui\":\n                actions = parse_code_from_som_string(response, masks)\n            else:\n                raise ValueError(\"Invalid action space: \" + self.action_space)\n\n            self.actions.append(actions)\n            return actions\n\n    def reset(self):\n        self.thoughts = []\n        self.actions = []\n        self.observations = []\n\n    def _get_message(self, task, obs):\n        system_message = self.system_message + \"\\nYou are asked to complete the following task: {}\".format(task)\n\n        messages = []\n        messages.append({\n            \"role\": \"system\",\n            \"content\": [\n                {\n                    \"type\": \"text\",\n                    \"text\": system_message\n                },\n            ]\n        })\n        assert len(self.observations) == len(self.actions) and len(self.actions) == len(self.thoughts), \"The number of observations and actions should be the same.\"\n\n        if len(self.observations) > self.max_trajectory_length:\n            if self.max_trajectory_length == 0:\n                _observations = []\n                _actions = []\n                _thoughts = []\n            else:\n                _observations = self.observations[-self.max_trajectory_length:]\n                _actions = self.actions[-self.max_trajectory_length:]\n                _thoughts = self.thoughts[-self.max_trajectory_length:]\n        else:\n            _observations = self.observations\n            _actions = self.actions\n            _thoughts = self.thoughts\n\n        for previous_obs, previous_action, previous_thought in zip(_observations, _actions, _thoughts):\n            if self.observation_type == \"screenshot_a11y_tree\":\n                _screenshot = previous_obs[\"screenshot\"]\n                _linearized_accessibility_tree = previous_obs[\"accessibility_tree\"]\n\n                messages.append({\n                    \"role\": \"user\",\n                    \"content\": [\n                        {\n                            \"type\": \"text\",\n                            \"text\": \"Given the screenshot and info from accessibility tree as below:\\n{}\\nWhat's the next step that you will do to help with the task?\".format(_linearized_accessibility_tree)\n                        },\n                        {\n                            \"type\": \"image_url\",\n                            \"image_url\": {\n                                \"url\": f\"data:image/png;base64,{_screenshot}\",\n                                \"detail\": \"high\"\n                            }\n                        }\n                    ]\n                })\n            elif self.observation_type in [\"som\"]:\n                _screenshot = previous_obs[\"screenshot\"]\n\n                messages.append({\n                    \"role\": \"user\",\n                    \"content\": [\n                        {\n                            \"type\": \"text\",\n                            \"text\": \"Given the tagged screenshot as below. What's the next step that you will do to help with the task?\"\n                        },\n                        {\n                            \"type\": \"image_url\",\n                            \"image_url\": {\n                                \"url\": f\"data:image/png;base64,{_screenshot}\",\n                                \"detail\": \"high\"\n                            }\n                        }\n                    ]\n                })\n            elif self.observation_type == \"screenshot\":\n                _screenshot = previous_obs[\"screenshot\"]\n\n                messages.append({\n                    \"role\": \"user\",\n                    \"content\": [\n                        {\n                            \"type\": \"text\",\n                            \"text\": \"Given the screenshot as below. What's the next step that you will do to help with the task?\"\n                        },\n                        {\n                            \"type\": \"image_url\",\n                            \"image_url\": {\n                                \"url\": f\"data:image/png;base64,{_screenshot}\",\n                                \"detail\": \"high\"\n                            }\n                        }\n                    ]\n                })\n            elif self.observation_type == \"a11y_tree\":\n                _linearized_accessibility_tree = previous_obs[\"accessibility_tree\"]\n\n                messages.append({\n                    \"role\": \"user\",\n                    \"content\": [\n                        {\n                            \"type\": \"text\",\n                            \"text\": \"Given the info from accessibility tree as below:\\n{}\\nWhat's the next step that you will do to help with the task?\".format(_linearized_accessibility_tree)\n                        }\n                    ]\n                })\n            else:\n                raise ValueError(\"Invalid observation_type type: \" + self.observation_type)\n\n            messages.append({\n                \"role\": \"assistant\",\n                \"content\": [\n                    {\n                        \"type\": \"text\",\n                        \"text\": previous_thought.strip() if len(previous_thought) > 0 else \"No valid action\"\n                    },\n                ]\n            })\n\n        if self.observation_type in [\"screenshot\", \"screenshot_a11y_tree\"]:\n            if self.observation_type == \"screenshot_a11y_tree\":\n                self.observations.append({\n                    \"screenshot\": obs[\"base64_image\"],\n                    \"accessibility_tree\": obs[\"linearized_accessibility_tree\"]\n                })\n            else:\n                self.observations.append({\n                    \"screenshot\": obs[\"base64_image\"],\n                    \"accessibility_tree\": None\n                })\n\n            messages.append({\n                \"role\": \"user\",\n                \"content\": [\n                    {\n                        \"type\": \"text\",\n                        \"text\": \"Given the screenshot as below. What's the next step that you will do to help with the task?\"\n                        if self.observation_type == \"screenshot\"\n                        else \"Given the screenshot and info from accessibility tree as below:\\n{}\\nWhat's the next step that you will do to help with the task?\".format(obs[\"linearized_accessibility_tree\"])\n                    },\n                    {\n                        \"type\": \"image_url\",\n                        \"image_url\": {\n                            \"url\": f\"\"\"data:image/png;base64,{obs[\"base64_image\"]}\"\"\",\n                            \"detail\": \"high\"\n                        }\n                    }\n                ]\n            })\n        elif self.observation_type == \"a11y_tree\":\n            self.observations.append({\n                \"screenshot\": None,\n                \"accessibility_tree\": obs[\"linearized_accessibility_tree\"]\n            })\n\n            messages.append({\n                \"role\": \"user\",\n                \"content\": [\n                    {\n                        \"type\": \"text\",\n                        \"text\": \"Given the info from accessibility tree as below:\\n{}\\nWhat's the next step that you will do to help with the task?\".format(obs[\"linearized_accessibility_tree\"])\n                    }\n                ]\n            })\n        elif self.observation_type == \"som\":\n            self.observations.append({\n                \"screenshot\": obs[\"base64_image\"],\n                \"accessibility_tree\": obs[\"linearized_accessibility_tree\"]\n            })\n\n            messages.append({\n                \"role\": \"user\",\n                \"content\": [\n                    {\n                        \"type\": \"text\",\n                        \"text\": \"Given the tagged screenshot and info from accessibility tree as below:\\n{}\\nWhat's the next step that you will do to help with the task?\".format(obs[\"linearized_accessibility_tree\"])\n                    },\n                    {\n                        \"type\": \"image_url\",\n                        \"image_url\": {\n                            \"url\": f\"\"\"data:image/png;base64,{obs[\"base64_image\"]}\"\"\",\n                            \"detail\": \"high\"\n                        }\n                    }\n                ]\n            })\n        else:\n            raise ValueError(\"Invalid observation_type type: \" + self.observation_type)\n\n        return messages\n\n    def _get_system_message(self, observation_type, action_space):\n        raise NotImplementedError\n\nclass ChromeAgent(GUIAgent):\n    def _get_system_message(self, observation_type, action_space):\n        if observation_type == \"screenshot_a11y_tree\":\n            if action_space == \"computer_13\":\n                self.system_message = prompt[\"SYS_PROMPT_IN_BOTH_OUT_ACTION\"]\n            elif action_space == \"pyautogui\":\n                self.system_message = prompt[\"SYS_PROMPT_IN_BOTH_OUT_CODE_CHROME\"]\n            else:\n                raise ValueError(\"Invalid action space: \" + action_space)\n\nclass GimpAgent(GUIAgent):\n    def _get_system_message(self, observation_type, action_space):\n        if observation_type == \"screenshot_a11y_tree\":\n            if action_space == \"computer_13\":\n                self.system_message = prompt[\"SYS_PROMPT_IN_BOTH_OUT_ACTION\"]\n            elif action_space == \"pyautogui\":\n                self.system_message = prompt[\"SYS_PROMPT_IN_BOTH_OUT_CODE_GIMP\"]\n            else:\n                raise ValueError(\"Invalid action space: \" + action_space)\n\nclass VlcAgent(GUIAgent):\n    def _get_system_message(self, observation_type, action_space):\n        if observation_type == \"screenshot_a11y_tree\":\n            if action_space == \"computer_13\":\n                self.system_message = prompt[\"SYS_PROMPT_IN_BOTH_OUT_ACTION\"]\n            elif action_space == \"pyautogui\":\n                self.system_message = prompt[\"SYS_PROMPT_IN_BOTH_OUT_CODE_VLC\"]\n            else:\n                raise ValueError(\"Invalid action space: \" + action_space)\n\nclass ThunderbirdAgent(GUIAgent):\n    def _get_system_message(self, observation_type, action_space):\n        if observation_type == \"screenshot_a11y_tree\":\n            if action_space == \"computer_13\":\n                self.system_message = prompt[\"SYS_PROMPT_IN_BOTH_OUT_ACTION\"]\n            elif action_space == \"pyautogui\":\n                self.system_message = prompt[\"SYS_PROMPT_IN_BOTH_OUT_CODE_THU\"]\n            else:\n                raise ValueError(\"Invalid action space: \" + action_space)\n\nclass VscodeAgent(GUIAgent):\n    def _get_system_message(self, observation_type, action_space):\n        if observation_type == \"screenshot_a11y_tree\":\n            if action_space == \"computer_13\":\n                self.system_message = prompt[\"SYS_PROMPT_IN_BOTH_OUT_ACTION\"]\n            elif action_space == \"pyautogui\":\n                self.system_message = prompt[\"SYS_PROMPT_IN_BOTH_OUT_CODE_VSCODE\"]\n            else:\n                raise ValueError(\"Invalid action space: \" + action_space)\n\nclass OsGUIAgent(GUIAgent):\n    def _get_system_message(self, observation_type, action_space):\n        if observation_type == \"screenshot_a11y_tree\":\n            if action_space == \"computer_13\":\n                self.system_message = prompt[\"SYS_PROMPT_IN_BOTH_OUT_ACTION\"]\n            elif action_space == \"pyautogui\":\n                self.system_message = prompt[\"SYS_PROMPT_IN_BOTH_OUT_CODE_OS\"]\n            else:\n                raise ValueError(\"Invalid action space: \" + action_space)\n\nclass CalcAgent(GUIAgent):\n    def _get_system_message(self, observation_type, action_space):\n        if observation_type == \"screenshot_a11y_tree\":\n            if action_space == \"computer_13\":\n                self.system_message = prompt[\"SYS_PROMPT_IN_BOTH_OUT_ACTION\"]\n            elif action_space == \"pyautogui\":\n                self.system_message = prompt[\"SYS_PROMPT_IN_BOTH_OUT_CODE_CACL\"]\n            else:\n                raise ValueError(\"Invalid action space: \" + action_space)\n\nclass ImpressAgent(GUIAgent):\n    def _get_system_message(self, observation_type, action_space):\n        if observation_type == \"screenshot_a11y_tree\":\n            if action_space == \"computer_13\":\n                self.system_message = prompt[\"SYS_PROMPT_IN_BOTH_OUT_ACTION\"]\n            elif action_space == \"pyautogui\":\n                self.system_message = prompt[\"SYS_PROMPT_IN_BOTH_OUT_CODE_IMPRESS\"]\n            else:\n                raise ValueError(\"Invalid action space: \" + action_space)\n\nclass WriterAgent(GUIAgent):\n    def _get_system_message(self, observation_type, action_space):\n        if observation_type == \"screenshot_a11y_tree\":\n            if action_space == \"computer_13\":\n                self.system_message = prompt[\"SYS_PROMPT_IN_BOTH_OUT_ACTION\"]\n            elif action_space == \"pyautogui\":\n                self.system_message = prompt[\"SYS_PROMPT_IN_BOTH_OUT_CODE_WORD\"]\n            else:\n                raise ValueError(\"Invalid action space: \" + action_space)\n\ndef replace_path(obj, old_path, new_path):\n    if isinstance(obj, dict):\n        for key, value in obj.items():\n            obj[key] = replace_path(value, old_path, new_path)\n    elif isinstance(obj, list):\n        obj = [replace_path(item, old_path, new_path) for item in obj]\n    elif isinstance(obj, str):\n        obj = obj.replace(old_path, new_path)\n    return obj\n\nif __name__ == '__main__':\n    environment = DesktopEnv(\n        path_to_vm=r\"D:/jcy/OSWorld_new/vmware_vm_data/Ubuntu0/Ubuntu0.vmx\",\n        action_space=\"pyautogui\",\n        require_a11y_tree=True,\n    )\n\n    test_all_meta_path = \"D:\\\\jcy\\\\OSWorld_new\\\\evaluation_examples/test_all.json\"\n    example_path = \"D:\\\\jcy\\\\OSWorld_new\\\\evaluation_examples\"\n\n    with open(test_all_meta_path, \"r\", encoding=\"utf-8\") as f:\n        test_all_meta = json.load(f)\n\n    domain = 'libreoffice_writer'\n    tasks = test_all_meta[domain]\n\n    for example_id in tasks[0:]:\n        example_id = \"6ada715d-3aae-4a32-a6a7-429b2e43fb93\"\n        config_file = os.path.join(example_path, f\"examples/{domain}/{example_id}.json\")\n        with open(config_file, \"r\", encoding=\"utf-8\") as f:\n            example = json.load(f)\n        task_name = example['instruction']\n        \n        print('task_name:', task_name)\n        example = replace_path(example, 'evaluation_examples/settings', 'D:\\\\jcy\\\\OSWorld_new\\\\evaluation_examples\\\\settings')\n        \n        previous_obs = environment.reset(task_config=example)\n        args = setup_config()\n        action_space = 'pyautogui'\n        observation_type = 'screenshot_a11y_tree'\n        max_trajectory_length = 3\n        gui_agent = ChromeAgent(args, example, environment, action_space, observation_type, max_trajectory_length, max_steps=10)\n        gui_agent.run()\n\n        input()\n        print(\"evaluate.......\")\n        print(environment.evaluate())\n        break\n"}
{"type": "source_file", "path": "agentstore/agents/os_agent.py", "content": "from agentstore.utils import setup_config, setup_pre_run\nfrom agentstore.modules.base_module import BaseModule\nimport re\nimport os\nimport json\nimport base64\nimport time\nfrom rich.console import Console\nfrom rich.markdown import Markdown\n\nfrom desktop_env.desktop_env import DesktopEnv\n\nfrom agentstore.prompts.osworld_pt import prompt_os\n\nconsole = Console()\n\ndef encode_image(image_content):\n    return base64.b64encode(image_content).decode('utf-8')\n\ndef rich_print(markdown_text):\n    md = Markdown(markdown_text)\n    console.print(md)\n\n\ndef send_chat_prompts(message, llm):\n    return llm.chat(message)\n\n    \ndef extract_code(input_string):\n    pattern = r\"```(\\w+)?\\s*(.*?)```\"  # 匹配代码块，语言名称是可选项\n    matches = re.findall(pattern, input_string, re.DOTALL)\n\n    if matches:\n        language, code = matches[0]\n\n        # 如果没有语言信息，尝试从代码内容中检测\n        if not language:\n            if re.search(\"python\", code.lower()) or re.search(r\"import\\s+\\w+\", code):\n                language = \"python\"\n            elif re.search(\"bash\", code.lower()) or re.search(r\"echo\", code):\n                language = \"bash\"\n\n        if language == 'bash':\n            code = code.strip()\n            code = code.replace('\"', '\\\\\"')\n            code = code.replace('\\n', ' && ')\n            code = \"pyautogui.typewrite(\\\"{0}\\\", interval=0.05)\\npyautogui.press('enter')\\ntime.sleep(2)\".format(code)\n            if re.search(\"sudo\", code.lower()):\n                code += \"\\npyautogui.typewrite('password', interval=0.05)\\npyautogui.press('enter')\\ntime.sleep(1)\"\n        elif language == 'python':\n\n            # save code\n            with open('tmp.py', 'w') as f:\n                f.write(code)\n\n            code = \"pyautogui.typewrite(\\\"python main.py\\\", interval=0.05)\\npyautogui.press('enter')\\ntime.sleep(2)\"\n            \n        else:\n            print(language)\n            raise language\n\n        return code, language\n    else:\n        return None, None\n\ndef encode_image(image_content):\n    return base64.b64encode(image_content).decode('utf-8')\n\nclass OSAgent(BaseModule):\n\n    def __init__(self, args, config, env, obs=None):\n        super().__init__()\n        self.args = args\n\n        self.environment = env\n\n        self.task_name = config['instruction']\n\n        self.reply = None\n\n        domain = config['snapshot']\n        example_id = config['id']\n\n        result_dir = 'D:\\jcy\\OS-Copilot\\\\results'\n        self.example_result_dir = os.path.join(\n                result_dir,\n                domain,\n                example_id\n            )\n\n        os.makedirs(self.example_result_dir, exist_ok=True)\n\n        if obs != None:\n            self.init_image = encode_image(obs['screenshot'])\n\n\n    \n    def execute_tool(self, code, lang):\n        if lang == 'python':\n            file = [{\n                \"local_path\": 'tmp.py',\n                \"path\": '/home/user/main.py'\n              }]\n            self.environment.setup_controller._upload_file_setup(file)\n        obs, reward, done, info = self.environment.step(code)  # node_type\n        \n        reply = self.environment.controller.get_terminal_output()\n        # update reply\n        if self.reply and reply:\n            message_terminal = reply.replace(self.reply, \"\")\n        else:\n            message_terminal = reply\n        self.reply = reply\n\n        base64_image = encode_image(obs['screenshot'])\n        print(\"message_terminal\", message_terminal)\n        if 'gpt' in self.llm.model_name:\n            message = {\n                    \"role\": \"user\",\n                    \"content\": [\n                        {\n                            \"type\": \"text\",\n                            \"text\": \"After executing the command, the terminal output is {0}. the screenshot as below.\".format(message_terminal)\n                        }\n                        # {\n                        #     \"type\": \"image_url\",\n                        #     \"image_url\": {\n                        #         \"url\": f\"data:image/png;base64,{self.init_image}\",\n                        #         \"detail\": \"high\"\n                        #     }\n                        # }\n                    ]\n            }\n        else:\n            message = {\n                    \"role\": \"user\",\n                    \"content\": \"After executing the command, the terminal output is {0}. the screenshot as below.\".format(message_terminal)\n            }\n        return message\n# When a user refers to a file opened， you can use the provided screenshot to find the filepath.\n    def run(self):\n\n        while not self.environment.controller.get_terminal_output():\n            self.environment.step(\"pyautogui.click()\")\n            self.environment.step(\"pyautogui.hotkey('ctrl', 'alt', 't')\")\n            print(self.environment.controller.get_terminal_output())\n\n        light_planner_sys_prompt = prompt_os['SYS_PROMPT_IN_CLI_OS']\n        \n        \n         #  Try to use `print` or `echo` to output information needed for the subsequent tasks, or the next step might not get the required information.\n        light_planner_user_prompt = '''\n        User's information are as follows:\n        System Version: Ubuntu\n        Task Name: {task_name}\n        '''.format(task_name=self.task_name)\n        \n        message = [\n            {\"role\": \"system\", \"content\": light_planner_sys_prompt},\n            {\"role\": \"user\", \"content\": [\n                        {\n                            \"type\": \"text\",\n                            \"text\": light_planner_user_prompt + \"The screenshot as below.\"\n                        }\n                        # {\n                        #     \"type\": \"image_url\",\n                        #     \"image_url\": {\n                        #         \"url\": f\"data:image/png;base64,{self.init_image}\",\n                        #         \"detail\": \"high\"\n                        #     }\n                        # }\n                    ]}\n        ]\n        self.example_result_dir\n        step_idx = 0\n        while step_idx < 12:\n            print(\"send_chat_prompts...\")\n            response = send_chat_prompts(message, self.llm)\n            print(response)\n            message.append({\"role\": \"assistant\", \"content\": response})\n            code, lang = extract_code(response)\n\n            import datetime\n            action_timestamp = datetime.datetime.now().strftime(\"%Y%m%d@%H%M%S\")\n\n            with open(os.path.join(self.example_result_dir, \"traj.jsonl\"), \"a\") as f:\n                f.write(json.dumps({\n                    \"step_num\": step_idx + 1,\n                    \"action_timestamp\": action_timestamp,\n                    \"response\": response,\n                    \"code\": code,\n                    \"lang\": lang\n                }))\n\n            if code:\n                result = self.execute_tool(code, lang)\n            else:\n                result = ''\n\n            if result != '':\n                message.append(result)\n            else:\n                message.append({\"role\": \"user\", \"content\": \"Please continue. If all tasks have been completed, reply with 'Execution Complete'. If you believe subsequent tasks cannot continue, reply with 'Execution Interrupted', including the reasons why the tasks cannot proceed, and provide the user with some possible solutions.\"})\n            if 'Execution Complete' in response or 'Execution Interrupted' in response:\n                break\n\n            else:\n                step_idx += 1   \n\n        return response\n\n\ndef replace_path(obj,old_path,new_path):\n    if isinstance(obj, dict):\n        for key, value in obj.items():\n            obj[key] = replace_path(value,old_path,new_path)\n    elif isinstance(obj, list):\n        obj = [replace_path(item,old_path,new_path) for item in obj]\n    elif isinstance(obj, str):\n        obj = obj.replace(old_path, new_path)\n    return obj\n\n\nif __name__ == '__main__':\n\n\n    environment = DesktopEnv(\n            path_to_vm=r\"D:/jcy/OSWorld_new/vmware_vm_data/Ubuntu0/Ubuntu0.vmx\",\n            action_space=\"pyautogui\",\n            require_a11y_tree=True,\n        )\n\n    test_all_meta_path = \"D:\\\\jcy\\\\OSWorld_new\\\\evaluation_examples/test_all.json\"\n    example_path = \"D:\\\\jcy\\\\OSWorld_new\\\\evaluation_examples\"\n\n    with open(test_all_meta_path, \"r\", encoding=\"utf-8\") as f:\n        test_all_meta = json.load(f)\n    # libreoffice_impress\n    domain = 'os'\n    tasks = test_all_meta[domain]\n\n    # run_list_file_path = r'D:\\jcy\\OS-Copilot\\run_list.txt'\n    # run_list = []\n\n    # with open(run_list_file_path, 'r') as file:\n    #     for line in file:\n    #         run_list.append(line.strip())\n\n    for example_id in tasks[19:]:\n        print(example_id)\n        config_file = os.path.join(example_path, f\"examples/{domain}/{example_id}.json\")\n        with open(config_file, \"r\", encoding=\"utf-8\") as f:\n            example = json.load(f)\n        task_name = example['instruction']\n        \n        print('task_name:', task_name)\n        example = replace_path(example, 'evaluation_examples/settings', 'D:\\\\jcy\\\\OSWorld_new\\\\evaluation_examples\\\\settings')\n        \n        previous_obs = environment.reset(task_config=example)\n        args = setup_config()\n\n        \n        excel_agent = OfficeAgent(args, example, environment,obs=previous_obs)\n        excel_agent.run()\n\n        # 判定内容\n        input()\n        print(\"evaluate.......\")\n        eval_ = environment.evaluate()\n        print(eval_) \n\n        with open(os.path.join(excel_agent.example_result_dir, \"traj.jsonl\"), \"a\") as f:\n                f.write(json.dumps({\n                    \"evaluate\": eval_\n                }))\n        # break"}
{"type": "source_file", "path": "agentstore/agents/light_osworld_vision.py", "content": "from agentstore.utils import setup_config, setup_pre_run\nfrom agentstore.modules.base_module import BaseModule\nimport re\nimport os\nimport json\nimport base64\nimport time\nfrom rich.console import Console\nfrom rich.markdown import Markdown\n\nfrom desktop_env.envs.desktop_env import DesktopEnv\n\nconsole = Console()\n\ndef encode_image(image_content):\n    return base64.b64encode(image_content).decode('utf-8')\n\ndef rich_print(markdown_text):\n    md = Markdown(markdown_text)\n    console.print(md)\n\n\ndef send_chat_prompts(message, llm):\n    return llm.chat(message)\n\n    \ndef extract_code(input_string):\n    pattern = r\"```(\\w+)?\\s*(.*?)```\"  # 匹配代码块，语言名称是可选项\n    matches = re.findall(pattern, input_string, re.DOTALL)\n\n    if matches:\n        language, code = matches[0]\n\n        # 如果没有语言信息，尝试从代码内容中检测\n        if not language:\n            if re.search(\"python\", code.lower()) or re.search(r\"import\\s+\\w+\", code):\n                language = \"Python\"\n            elif re.search(\"bash\", code.lower()) or re.search(r\"echo\", code):\n                language = \"Bash\"\n        code = code.strip()\n        code = code.replace('\"', '\\\\\"')\n        code = code.replace('\\n', ' && ')\n        code = \"pyautogui.typewrite(\\\"{0}\\\", interval=0.05)\\npyautogui.press('enter')\\ntime.sleep(2)\".format(code)\n        if re.search(\"sudo\", code.lower()):\n            code += \"\\npyautogui.typewrite('password', interval=0.05)\\npyautogui.press('enter')\\ntime.sleep(1)\"\n\n        return code, language\n    else:\n        return None, None\n\nfrom desktop_env.envs.desktop_env import DesktopEnv\nclass LightFriday(BaseModule):\n    def __init__(self, args, example):\n        super().__init__()\n        self.args = args\n\n        self.environment = DesktopEnv(\n            path_to_vm=r\"D:/jcy/OSWorld-main/Ubuntu/Ubuntu.vmx\",\n            action_space=\"pyautogui\",\n            require_a11y_tree=False,\n        )\n        id_ = example['id']\n\n        \n        self.task_name = example['instruction']\n        print('Task:\\n'+self.task_name)\n\n        self.environment.reset(task_config=example)\n\n        self.reply = None\n        while not self.environment.controller.get_terminal_output():\n            self.environment.step(\"pyautogui.click()\")\n            self.environment.step(\"pyautogui.hotkey('ctrl', 'alt', 't')\")\n    \n    def execute_tool(self, code, lang):\n        obs, reward, done, info = self.environment.step(code)  # node_type\n\n        \n        reply = self.environment.controller.get_terminal_output()\n        # update reply\n        if self.reply and reply:\n            message_terminal = reply.replace(self.reply, \"\")\n        else:\n            message_terminal = reply\n        self.reply = reply\n\n\n        with open(obs['screenshot'], \"rb\") as __f:\n            screenshot = __f.read()\n        base64_image = encode_image(screenshot)\n        print(\"message_terminal\", message_terminal)\n        message = {\n                \"role\": \"user\",\n                \"content\": [\n                    {\n                        \"type\": \"text\",\n                        \"text\": \"After executing the command, the terminal output is {0}. the screenshot as below.\".format(message_terminal)\n                    }\n                    # {\n                    #     \"type\": \"image_url\",\n                    #     \"image_url\": {\n                    #         \"url\": f\"data:image/png;base64,{base64_image}\",\n                    #         \"detail\": \"high\"\n                    #     }\n                    # }\n                ]\n        }\n        return message\n# When a user refers to a file opened， you can use the provided screenshot to find the filepath.\n    def run(self):\n        light_planner_sys_prompt = '''You are Light Friday, a world-class programmer that can complete any goal by using linux command.\nFirst, write a plan. **Always recap the plan between each code block** (you have extreme short-term memory loss, so you need to recap the plan between each message block to retain it).\nWhen you execute code, it will be executed **on the user's machine**. The user has given you **full and complete permission** to execute any code necessary to complete the task. Execute the code.\nIf you want to send data between programming languages, save the data to a txt or json.\nYou can access the internet. Run **any code** to achieve the goal, and if at first you don't succeed, try again and again.\nYou can install new packages.\nImportant: When a user refers to a file opened or a url, you can use your visual capacity or some command line tools to see the path of the file being opened.\nWrite messages to the user in Markdown.\nIn general, try to **make plans** with as few steps as possible. As for actually executing code to carry out that plan, for *stateful* languages (like python, javascript, shell, but NOT for html which starts from 0 every time) **it's critical not to try to do everything in one code block.** You should try something, print information about it, then continue from there in tiny, informed steps. You will never get it on the first try, and attempting it in one go will often lead to errors you cant see.\nYou are capable of **any** task.\nWhen installing new software or package, you should first check whether it is already installed.\nYou Code should like this, doesnot need comment, only command:\n```bash\nls *.xlsx\n```\nCurrently, supported languages include Bash.\" The command you're outputing should only be one line\n'''  #  Try to use `print` or `echo` to output information needed for the subsequent tasks, or the next step might not get the required information.\n        light_planner_user_prompt = '''\n        User's information are as follows:\n        System Version: Ubuntu\n        Task: {task}\n        '''.format(task=self.task_name)\n        \n        message = [\n            {\"role\": \"system\", \"content\": light_planner_sys_prompt},\n            {\"role\": \"user\", \"content\": light_planner_user_prompt},\n        ]\n \n        while True:\n            print(\"send_chat_prompts...\")\n            response = send_chat_prompts(message, self.llm)\n            rich_print(response)\n            message.append({\"role\": \"assistant\", \"content\": response})\n\n            code, lang = extract_code(response)\n            if code:\n                result = self.execute_tool(code, lang)\n            else:\n                result = ''\n\n            if result != '':\n                message.append(result)\n            else:\n                message.append({\"role\": \"user\", \"content\": \"Please continue. If all tasks have been completed, reply with 'Execution Complete'. If you believe subsequent tasks cannot continue, reply with 'Execution Interrupted', including the reasons why the tasks cannot proceed, and provide the user with some possible solutions.\"})\n            \n            if 'Execution Complete' in response or 'Execution Interrupted' in response:\n                break\n\n\ndef replace_path(obj,old_path,new_path):\n    if isinstance(obj, dict):\n        for key, value in obj.items():\n            obj[key] = replace_path(value,old_path,new_path)\n    elif isinstance(obj, list):\n        obj = [replace_path(item,old_path,new_path) for item in obj]\n    elif isinstance(obj, str):\n        obj = obj.replace(old_path, new_path)\n    return obj\n\nif __name__ == '__main__':\n\n    example_path = 'D:\\jcy\\OSWorld-main\\evaluation_examples'\n\n    domain = 'multi_apps'\n    example_id = '58565672-7bfe-48ab-b828-db349231de6b'\n    config_file = os.path.join(example_path, f\"examples/{domain}/{example_id}.json\")\n    with open(config_file, \"r\", encoding=\"utf-8\") as f:\n        example = json.load(f)\n\n    example = replace_path(example, 'evaluation_examples/settings', 'D:\\jcy\\OSWorld-main\\evaluation_examples\\settings')\n    \n\n\n    args = setup_config()\n    # if not args.query:\n    #     # args.query = \"Copy any text file located in the working_dir/document directory that contains the word 'agent' to a new folder named 'agents' \"\n    #     # args.query = \"Copy any text file located in the /home/dingzichen/hcc/OS-Copilot/working_dir directory that contains the word 'agent' to a new folder /home/dingzichen/hcc/OS-Copilot/working_dir/agents\"\n    #     # args.query = \"print 'Hello world!'\"\n    #     args.query = \"Plot AAPL and META's normalized stock prices\"\n    # task = setup_pre_run(args)\n\n    light_friday = LightFriday(args,example)\n    light_friday.run()  # list\n    print(light_friday.environment.evaluate())"}
{"type": "source_file", "path": "agentstore/agents/image_agent.py", "content": "from agentstore.utils import setup_config, setup_pre_run\nfrom agentstore.modules.base_module import BaseModule\nimport re\nimport os\nimport json\nimport base64\nimport time\nfrom rich.console import Console\nfrom rich.markdown import Markdown\n\nfrom desktop_env.desktop_env import DesktopEnv\n\nfrom agentstore.prompts.osworld_pt import prompt_image\n\nconsole = Console()\n\ndef encode_image(image_content):\n    return base64.b64encode(image_content).decode('utf-8')\n\ndef rich_print(markdown_text):\n    md = Markdown(markdown_text)\n    console.print(md)\n\n\ndef send_chat_prompts(message, llm):\n    return llm.chat(message)\n\n    \ndef extract_code(input_string):\n    pattern = r\"```(\\w+)?\\s*(.*?)```\"  # 匹配代码块，语言名称是可选项\n    matches = re.findall(pattern, input_string, re.DOTALL)\n\n    if matches:\n        language, code = matches[0]\n\n        # 如果没有语言信息，尝试从代码内容中检测\n        if not language:\n            if re.search(\"python\", code.lower()) or re.search(r\"import\\s+\\w+\", code):\n                language = \"python\"\n            elif re.search(\"bash\", code.lower()) or re.search(r\"echo\", code):\n                language = \"bash\"\n\n        if language == 'bash':\n            code = code.strip()\n            code = code.replace('\"', '\\\\\"')\n            code = code.replace('\\n', ' && ')\n            code = \"pyautogui.typewrite(\\\"{0}\\\", interval=0.05)\\npyautogui.press('enter')\\ntime.sleep(2)\".format(code)\n            if re.search(\"sudo\", code.lower()):\n                code += \"\\npyautogui.typewrite('password', interval=0.05)\\npyautogui.press('enter')\\ntime.sleep(1)\"\n        elif language == 'python':\n\n            # save code\n            with open('tmp.py', 'w') as f:\n                f.write(code)\n\n            code = \"pyautogui.typewrite(\\\"python main.py\\\", interval=0.05)\\npyautogui.press('enter')\\ntime.sleep(2)\"\n            \n        else:\n            print(language)\n            raise language\n\n        return code, language\n    else:\n        return None, None\n\nclass ImageAgent(BaseModule):\n\n    def __init__(self, args, config, env):\n        super().__init__()\n        self.args = args\n\n        self.environment = env\n        \n\n        self.task_name = config['instruction']\n\n        try:\n            path = config['evaluator']['result']['path']\n            self.task_name = self.task_name + \" export path is \" + path\n            print(self.task_name)\n        except:\n            pass\n\n        self.reply = None\n\n        domain = config['snapshot']\n        example_id = config['id']\n\n        result_dir = 'D:\\jcy\\OS-Copilot\\\\results'\n        self.example_result_dir = os.path.join(\n                result_dir,\n                domain,\n                example_id\n            )\n\n        os.makedirs(self.example_result_dir, exist_ok=True)\n\n\n    \n    def execute_tool(self, code, lang):\n        if lang == 'python':\n            file = [{\n                \"local_path\": 'tmp.py',\n                \"path\": '/home/user/main.py'\n              }]\n            self.environment.setup_controller._upload_file_setup(file)\n        obs, reward, done, info = self.environment.step(code)  # node_type\n        \n        reply = self.environment.controller.get_terminal_output()\n        # update reply\n        if self.reply and reply:\n            message_terminal = reply.replace(self.reply, \"\")\n        else:\n            message_terminal = reply\n        self.reply = reply\n\n        base64_image = encode_image(obs['screenshot'])\n        print(\"message_terminal\", message_terminal)\n        if 'gpt' in self.llm.model_name:\n            message = {\n                    \"role\": \"user\",\n                    \"content\": [\n                        {\n                            \"type\": \"text\",\n                            \"text\": \"After executing the command, the terminal output is {0}. the screenshot as below.\".format(message_terminal)\n                        }\n                        # {\n                        #     \"type\": \"image_url\",\n                        #     \"image_url\": {\n                        #         \"url\": f\"data:image/png;base64,{base64_image}\",\n                        #         \"detail\": \"high\"\n                        #     }\n                        # }\n                    ]\n            }\n        else:\n            message = {\n                    \"role\": \"user\",\n                    \"content\": \"After executing the command, the terminal output is {0}. the screenshot as below.\".format(message_terminal)\n            }\n        return message\n# When a user refers to a file opened， you can use the provided screenshot to find the filepath.\n    def run(self):\n\n        while not self.environment.controller.get_terminal_output():\n            self.environment.step(\"pyautogui.click()\")\n            self.environment.step(\"pyautogui.hotkey('ctrl', 'alt', 't')\")\n            print(self.environment.controller.get_terminal_output())\n\n        light_planner_sys_prompt = prompt_image['SYS_PROMPT_IN_CLI_IMAGE']\n        \n        \n         #  Try to use `print` or `echo` to output information needed for the subsequent tasks, or the next step might not get the required information.\n        light_planner_user_prompt = '''\n        User's information are as follows:\n        System Version: Ubuntu\n        Task Name: {task_name}\n        '''.format(task_name=self.task_name)\n        \n        message = [\n            {\"role\": \"system\", \"content\": light_planner_sys_prompt},\n            {\"role\": \"user\", \"content\": light_planner_user_prompt},\n        ]\n        self.example_result_dir\n        step_idx = 0\n        while step_idx < 12:\n            print(\"send_chat_prompts...\")\n            response = send_chat_prompts(message, self.llm)\n            print(response)\n            message.append({\"role\": \"assistant\", \"content\": response})\n            code, lang = extract_code(response)\n\n            import datetime\n            action_timestamp = datetime.datetime.now().strftime(\"%Y%m%d@%H%M%S\")\n\n            with open(os.path.join(self.example_result_dir, \"traj.jsonl\"), \"a\") as f:\n                f.write(json.dumps({\n                    \"step_num\": step_idx + 1,\n                    \"action_timestamp\": action_timestamp,\n                    \"response\": response,\n                    \"code\": code,\n                    \"lang\": lang\n                }))\n\n            if code:\n                result = self.execute_tool(code, lang)\n            else:\n                result = ''\n\n            if result != '':\n                message.append(result)\n            else:\n                message.append({\"role\": \"user\", \"content\": \"Please continue. If all tasks have been completed, reply with 'TASK DONE'. If you believe subsequent tasks cannot continue, or you find that the task is problematic, or beyond your ability, reply with 'TASK FAIL'. Including the reasons why the tasks cannot proceed, and provide the user with some possible solutions.\"})\n            \n            if 'TASK DONE' in response or 'TASK FAIL' in response:\n                if 'TASK FAIL' in response:\n                    self.environment.step(\"FAIL\")\n                break\n            else:\n                step_idx += 1   \n\n        return response\n\n\ndef replace_path(obj,old_path,new_path):\n    if isinstance(obj, dict):\n        for key, value in obj.items():\n            obj[key] = replace_path(value,old_path,new_path)\n    elif isinstance(obj, list):\n        obj = [replace_path(item,old_path,new_path) for item in obj]\n    elif isinstance(obj, str):\n        obj = obj.replace(old_path, new_path)\n    return obj\n\n\nif __name__ == '__main__':\n\n\n    environment = DesktopEnv(\n            path_to_vm=r\"D:/jcy/OSWorld_new/vmware_vm_data/Ubuntu0/Ubuntu0.vmx\",\n            action_space=\"pyautogui\",\n            require_a11y_tree=False,\n        )\n\n    test_all_meta_path = \"D:\\\\jcy\\\\OSWorld_new\\\\evaluation_examples/test_all.json\"\n    example_path = \"D:\\\\jcy\\\\OSWorld_new\\\\evaluation_examples\"\n\n    with open(test_all_meta_path, \"r\", encoding=\"utf-8\") as f:\n        test_all_meta = json.load(f)\n\n    domain = 'gimp'\n    tasks = test_all_meta[domain]\n\n    # run_list_file_path = r'D:\\jcy\\OS-Copilot\\run_list.txt'\n    # run_list = []\n\n    # with open(run_list_file_path, 'r') as file:\n    #     for line in file:\n    #         run_list.append(line.strip())\n\n    for example_id in tasks[25:]:\n        print(example_id)\n        config_file = os.path.join(example_path, f\"examples/{domain}/{example_id}.json\")\n        with open(config_file, \"r\", encoding=\"utf-8\") as f:\n            example = json.load(f)\n        task_name = example['instruction']\n        \n        print('task_name:', task_name)\n        example = replace_path(example, 'evaluation_examples/settings', 'D:\\\\jcy\\\\OSWorld_new\\\\evaluation_examples\\\\settings')\n        \n        previous_obs = environment.reset(task_config=example)\n        args = setup_config()\n        \n        excel_agent = ImageAgent(args, example, environment)\n        excel_agent.run()\n        input()\n        \n        # 判定内容\n        print(\"evaluate.......\")\n        print(environment.evaluate())   \n        break    "}
{"type": "source_file", "path": "setup.py", "content": "from setuptools import setup, find_packages\n\n\nwith open('requirements.txt') as f:\n    requirements = f.read().splitlines()\n\nsetup(\n    name=\"agentstore\",\n    version=\"0.1.0\",\n    author=\"Chengyou Jia, Minnan Luo, Zhuohang Dang, Qiushi Sun, Fangzhi Xu, Junlin Hu, Tianbao Xie, Zhiyong Wu\",\n    author_email=\"cp3jia@stu.xjtu.edu.cn\",\n    description=\"A flexible and scalable platform for dynamically integrating various heterogeneous agents to independently or collaboratively automate OS tasks. \",\n    long_description=open(\"README.md\", \"r\", encoding=\"utf-8\").read(),\n    long_description_content_type=\"text/markdown\",\n    url=\"https://github.com/chengyou-jia/AgentStore\",\n    license=\"MIT\",\n\n    packages=find_packages(exclude=(\"docs\", \"temp\", \"pic\", \"log\")),\n\n    install_requires=requirements,\n\n    entry_points={\n        \"console_scripts\": [\n            \"friday=quick_start:main\",\n        ],\n    },\n\n    classifiers=[\n        \"Programming Language :: Python :: 3\",\n        \"License :: OSI Approved :: MIT License\",\n        \"Operating System :: OS Independent\",\n    ],\n    keywords=\"AI, LLMs, Large Language Models, Agent, OS, Operating System\",\n\n    python_requires='>=3.10',\n)\n"}
{"type": "source_file", "path": "agentstore/prompts/friday_pt.py", "content": "\"\"\"\nThis modules contains a comprehensive `prompts` dictionary that serves as a repository of prompts for guiding the AI agents's interactions across various operational scenarios, including execution, planning, and information retrieval tasks. These prompts are meticulously crafted to instruct the AI in performing its duties, ranging from code generation and amendment to task decomposition and planning, as well as error analysis and tool usage.\n\nThe dictionary is segmented into five main categories:\n\n1. **execute_prompt**: Contains prompts for execution-related tasks, such as code generation, invocation, amendment, and error judgment. These are further detailed for system actions and user interactions, facilitating a diverse range of programming and troubleshooting tasks.\n\n2. **planning_prompt**: Focuses on task planning and re-planning, decomposing complex tasks into manageable sub-tasks, and adapting plans based on unforeseen issues, ensuring that the AI can assist in project management and task organization effectively.\n\n3. **retrieve_prompt**: Dedicated to information retrieval, including filtering code snippets based on specific criteria, aiding the AI in sourcing and suggesting code solutions efficiently.\n\n4. **self_learning_prompt**: Contains prompts for self-learning tasks, such as designing educational courses based on software and content parameters. These prompts guide the AI in generating course designs and educational content tailored to user needs.\n\n5. **text_extract_prompt**: Contains prompts for text extraction tasks, such as extracting specific information from text data. These prompts guide the AI in identifying and extracting relevant data from text inputs.\n\nEach category comprises system and user prompts, where system prompts define the AI's task or query in detail, and user prompts typically include placeholders for dynamic information insertion, reflecting the context or specific requirements of the task at hand.\n\nUsage:\nThe `prompts` dictionary is utilized by the AI agents to dynamically select appropriate prompts based on the current context or task, ensuring relevant and precise guidance for each operation. This dynamic approach allows the AI to adapt its interactions and responses to suit a wide array of programming and operational needs, enhancing its utility and effectiveness in assisting users.\n\nExample:\n    .. code-block:: python\n\n        # Accessing a specific prompts for task execution\n        execute_prompt = prompts['execute_prompt']['_SYSTEM_SKILL_CREATE_AND_INVOKE_PROMPT']\n\"\"\"\nprompt = {\n    'execute_prompt': {\n        # shell/applescript generator\n        '_SYSTEM_SHELL_APPLESCRIPT_GENERATE_PROMPT': '''\n        You are a world-class programmer that can complete any task by executing code, your goal is to generate the corresponding code based on the type of code to complete the task.\n        You could only respond with a code.\n        Shell code output Format:\n        ```shell\n        shell code\n        ```\n\n        AppleScript code output Format:\n        ```applescript\n        applescript code\n        ```        \n\n        The code you write should follow the following criteria:\n        1. You must generate code of the specified 'Code Type' to complete the task.\n        2. The code logic should be clear and highly readable, able to meet the requirements of the task.\n        ''',\n        '_USER_SHELL_APPLESCRIPT_GENERATE_PROMPT': '''\n        User's information is as follows:\n        System Version: {system_version}\n        System language: simplified chinese\n        Working Directory: {working_dir}\n        Task Name: {task_name}\n        Task Description: {task_description}     \n        Information of Prerequisite Tasks: {pre_tasks_info}   \n        Code Type: {Type}\n        Detailed description of user information:\n        1. 'Working Directory' represents the working directory. It may not necessarily be the same as the current working directory. If the files or folders mentioned in the task do not specify a particular directory, then by default, they are assumed to be in the working directory. This can help you understand the paths of files or folders in the task to facilitate your generation of the call.\n        2. 'Information of Prerequisite Tasks' provides relevant information about the prerequisite tasks for the current task, encapsulated in a dictionary format. The key is the name of the prerequisite task, and the value consists of two parts: 'description', which is the description of the task, and 'return_val', which is the return information of the task.\n        3, 'Code Type' represents the type of code to be generated.\n        ''',        \n\n\n        # Python generate and invoke prompts in os\n        '_SYSTEM_PYTHON_SKILL_AND_INVOKE_GENERATE_PROMPT': '''\n        You are a world-class programmer that can complete any task by executing code, your goal is to generate the function code that accomplishes the task, along with the function's invocation.\n        You could only respond with a python code and a invocation statement.\n        Output Format:\n        ```python\n        python code\n        ```\n        <invoke>invocation statement</invoke>\n\n        The code you write should follow the following criteria:\n        1. Function name should be the same as the 'Task Name' provided by the user.\n        2. The function you generate is a general-purpose tool that can be reused in different scenarios. Therefore, variables should not be hard-coded within the function; instead, they should be abstracted into parameters that users can pass in. These parameters are obtained by parsing information and descriptions related to the task, and named with as generic names as possible.\n        3. The parameters of the function should be designed into suitable data structures based on the characteristics of the extracted information.\n        4. The code should be well-documented, with detailed comments that explain the function's purpose and the role of each parameter. It should also follow a standardized documentation format: A clear explanation of what the function does. Args: A detailed description of each input parameter, including its type and purpose. Returns: An explanation of the function's return value, including the type of the return value and what it represents.\n        5. The code logic should be clear and highly readable, able to meet the requirements of the task.\n        6. The function must have a return value. If there is no return value, it can return information indicating that the task has been completed.\n        7. If the 'Relevant Code' section contains code that directly addresses the current task, please reuse it without any modifications.\n        8. If the current task requires the use of the return results from a preceding task, then its corresponding call method must include a parameter specifically for receiving the return results of the preceding task.\n        9. If the current task depends on the results from a previous task, the function must include a parameter designed to accept the results from that previous task.\n        10. If the code involves the output of file paths, ensure that the output includes the files' absolute path.\n        11. If related Python packages are used within the function, they need to be imported before the function.\n\n        And the invocation statement should also follow the following criteria:\n        1. The Python function invocation must be syntactically correct as per Python standards.\n        2. Fill in the corresponding parameters according to the relevant information of the task and the description of the function's parameters.\n        3. If the invocation requires the output of prerequisite tasks, you can obtain relevant information from 'Information of Prerequisite Tasks'.\n\n        Now you will be provided with the following information, please write python code to accomplish the task and be compatible with system environments, versions and language according to these information.         \n        ''',\n        '_USER_PYTHON_SKILL_AND_INVOKE_GENERATE_PROMPT': '''\n        User's information is as follows:\n        System Version: {system_version}\n        System language: simplified chinese\n        Working Directory: {working_dir}\n        Task Name: {task_name}\n        Task Description: {task_description}     \n        Information of Prerequisite Tasks: {pre_tasks_info}   \n        Relevant Code: {relevant_code}\n        Detailed description of user information:\n        1. 'Working Directory' represents the working directory. It may not necessarily be the same as the current working directory. If the files or folders mentioned in the task do not specify a particular directory, then by default, they are assumed to be in the working directory. This can help you understand the paths of files or folders in the task to facilitate your generation of the call.\n        2. 'Information of Prerequisite Tasks' provides relevant information about the prerequisite tasks for the current task, encapsulated in a dictionary format. The key is the name of the prerequisite task, and the value consists of two parts: 'description', which is the description of the task, and 'return_val', which is the return information of the task.\n        3. 'Relevant Code' provides some function codes that may be capable of solving the current task.\n        ''',\n\n\n        # shell/applescript amend in os\n        '_SYSTEM_SHELL_APPLESCRIPT_AMEND_PROMPT': '''\n        You are an expert in programming, with a focus on diagnosing and resolving code issues.\n        Your goal is to precisely identify the reasons for failure in the existing code and implement effective modifications to ensure it accomplishes the intended task without errors.\n        You should only respond with a modified code.\n        Code in the format as described below:\n        1. Error Analysis: Conduct a step-by-step analysis to identify why the code is generating errors or failing to complete the task. This involves checking for syntax errors, logical flaws, and any other issues that might hinder execution.\n        2. Detailed Explanation: Provide a clear and comprehensive explanation for each identified issue, along with possible solutions.\n        3. Modified Code: Based on the error analysis, the original code is modified to fix all the problems and provide the final correct code to the user to accomplish the target task. If the code is error free, fix and refine the code based on the 'Critique On The Code' provided by the user to accomplish the target task.    \n\n        And the code you write should also follow the following criteria:\n        1. The code logic should be clear and highly readable, able to meet the requirements of the task.\n        2. The code must be enclosed between ```[code type] and ```. For example, ```shell [shell code] ```.\n        3. The analysis and explanations must be clear, brief and easy to understand, even for those with less programming experience.\n        4. All modifications must address the specific issues identified in the error analysis.\n        5. The solution must enable the code to successfully complete the intended task without errors.\n        6. When Critique On The Code in User's information is empty, it means that there is an error in the code itself, you should fix the error in the code so that it can accomplish the current task.\n\n        Now you will be provided with the following information, please give your modified code according to these information.\n        ''',\n        '_USER_SHELL_APPLESCRIPT_AMEND_PROMPT': '''\n        User's information are as follows:\n        Original Code: {original_code}\n        Task: {task}\n        Error Messages: {error}\n        Code Output: {code_output}\n        Current Working Directiory: {current_working_dir}\n        Working Directiory: {working_dir}\n        Files And Folders in Current Working Directiory: {files_and_folders}\n        Critique On The Code: {critique}\n        Information of Prerequisite Tasks: {pre_tasks_info}   \n        Detailed description of user information:\n        1. 'Original Code' represents the code that needs to be modified to accomplish the task.\n        2. 'Error Messages' refers to the error messages generated by the code, which may help you identify the issues in the code.\n        3. 'Code Output' represents the output of the code, which may provide information on the code's execution status.\n        4. 'Working Directory' represents the root directory of the working directory, and 'Current Working Directory' represents the directory where the current task is located.    \n        5. 'Critique On The Code' refers to code modification suggestions given by other code experts and may be empty.\n        6. 'Information of Prerequisite Tasks' from User's information provides relevant information about the prerequisite tasks for the current task, encapsulated in a dictionary format. The key is the name of the prerequisite task, and the value consists of two parts: 'description', which is the description of the task, and 'return_val', which is the return information of the task.\n        ''',\n\n\n        # Python amend and invoke prompts in os\n        '_SYSTEM_PYTHON_SKILL_AMEND_AND_INVOKE_PROMPT': '''\n        You are an expert in Python programming, with a focus on diagnosing and resolving code issues.\n        Your goal is to precisely identify the reasons for failure in the existing Python code and implement effective modifications to ensure it accomplishes the intended task without errors.\n        You should only respond with a python code and a invocation statement.\n        Python code in the format as described below:\n        1. Error Analysis: Conduct a step-by-step analysis to identify why the code is generating errors or failing to complete the task. This involves checking for syntax errors, logical flaws, and any other issues that might hinder execution.\n        2. Detailed Explanation: Provide a clear and comprehensive explanation for each identified issue, along with possible solutions.\n        3. Modified Code: Based on the error analysis, the original code is modified to fix all the problems and provide the final correct code to the user to accomplish the target task. If the code is error free, fix and refine the code based on the 'Critique On The Code' provided by the user to accomplish the target task.\n        invocation statement in the format as described below:\n        1. Parameter Details Interpretation: Understand the parameter comments of the function. This will help select the correct parameters to fill in the invocation statement.\n        2. Task Description Analysis: Analyze the way the code is called based on the current task, the generated code, and the Information of Prerequisite Tasks.\n        3. Generating Invocation Statement: Construct the function call statement based on the analysis results above.\n        4. Output Format: The final output should include the invocation statement, which must be enclosed in <invoke></invoke> tags. For example, <invoke>function()</invoke>.     \n\n        And the code you write should also follow the following criteria:\n        1. You must keep the original function name.\n        2. The code logic should be clear and highly readable, able to meet the requirements of the task.\n        3. The python code must be enclosed between ```python and ```.\n        4. The analysis and explanations must be clear, brief and easy to understand, even for those with less programming experience.\n        5. All modifications must address the specific issues identified in the error analysis.\n        6. The solution must enable the code to successfully complete the intended task without errors.\n        7. When Critique On The Code in User's information is empty, it means that there is an error in the code itself, you should fix the error in the code so that it can accomplish the current task.\n\n        And the invocation statement should also follow the following criteria:\n        1. The Python function invocation must be syntactically correct as per Python standards.\n        2. Clearly identify any fake or placeholder parameters used in the invocation.\n        3. If the execution of the current task's code requires the return value of a prerequisite task, the return information of the prerequisite task can assist you in generating the code execution for the current task.\n        4. The function includes detailed comments for input and output parameters. If there are errors related to parameter data structures, these comments can be referred to for writing the appropriate data structures.\n        5. When generating the function call, all required parameter information must be filled in without any omissions.\n        \n        Now you will be provided with the following information, please give your modified python code and invocation statement according to these information.\n        ''',\n        '_USER_PYTHON_SKILL_AMEND_AND_INVOKE_PROMPT': '''\n        User's information are as follows:\n        Original Code: {original_code}\n        Task: {task}\n        Error Messages: {error}\n        Code Output: {code_output}\n        Current Working Directiory: {current_working_dir}\n        Working Directiory: {working_dir}\n        Files And Folders in Current Working Directiory: {files_and_folders}\n        Critique On The Code: {critique}\n        Information of Prerequisite Tasks: {pre_tasks_info}   \n        Detailed description of user information:\n        1. 'Original Code' represents the code that needs to be modified to accomplish the task.\n        2. 'Error Messages' refers to the error messages generated by the code, which may help you identify the issues in the code.\n        3. 'Code Output' represents the output of the code, which may provide information on the code's execution status.\n        4. 'Working Directory' represents the root directory of the working directory, and 'Current Working Directory' represents the directory where the current task is located.    \n        5. 'Critique On The Code' refers to code modification suggestions given by other code experts and may be empty.\n        6. 'Information of Prerequisite Tasks' from User's information provides relevant information about the prerequisite tasks for the current task, encapsulated in a dictionary format. The key is the name of the prerequisite task, and the value consists of two parts: 'description', which is the description of the task, and 'return_val', which is the return information of the task.\n        ''',\n\n\n\n        # Task judge prompts in os\n        '_SYSTEM_TASK_JUDGE_PROMPT': '''\n        You are an program expert to verify code against a user's task requirements.\n        Your goal is to determine if the provided code accomplishes the user's specified task based on the feedback information, And score the code based on the degree of generalizability of the code.\n        You should only respond with a JSON result. \n        You must follow the analysis process and format requirements as follows:\n        1. Analyze the provided code: Examine the user's code to understand its functionality and structure.\n        2. Compare the code with the task description: Align the objectives stated in the user's task description with the capabilities of the code.\n        3. Evaluate the feedback information: Review the user's feedback, Includes 'Code Output', 'Code Error' and the working catalog information provided by user to measure the effectiveness of the code.\n        4. Formulate a reasoning process: Based on the analysis of the code and feedback received, generate a reasoning process about the execution of the code. If you believe the task has been successfully completed, you need to explain how the code accomplished the task. If you think the task has not been completed, you need to explain the reasons for the failure and provide corresponding solutions.\n        5. Evaluate task status: Based on the reasoning process, determine the status of the task. There are three possible statuses for a task:\n                Complete: The task has been successfully executed.\n                Amend: There are errors in the code, or the code does not meet the task requirements, necessitating fixes based on the reasoning process.\n                Replan: Errors encountered during code execution cannot be rectified by simply modifying the code, requiring additional operations within the code's execution environment. This necessitates new tasks to perform these extra operations.\n        6. Code's generality score: Evaluate the generality of the code and give code a score. The generality of the code can be analyzed based on parameters flexibility, error and exception handling, clarity of comments, code efficiency, security aspects, and other factors. According to the evaluation results, the code can be scored on a scale from 1 to 10, with integers reflecting the code's generality. A score of 1-3 indicates that the code is not very generic and can only complete the current task. A score of 4-6 indicates that the code can efficiently complete similar tasks, but the parameter names are not generic enough. A score of 7-8 indicates that the code is sufficiently generic but lacks in terms of security, clarity of comments, and fault tolerance. A score of 9-10 indicates that the code is highly generic in all aspects.\n        7. Output Format: \n\n        ```json\n        {\n            reasoning: Your reasoning process,\n            status: Complete/Amend/Replan,\n            score: 1-10\n        }\n        ``` \n\n        And you should also follow the following criteria:\n        1. Provide clear, logical reasoning.\n        2. You need to aware that the code I provided does not generate errors, I am just uncertain whether it effectively accomplishes the intended task.\n        3. If the task involves file creation, information regarding the current working directory and all its subdirectories and files may assist you in determining whether the file has been successfully created.\n        4. If the Code Output contains information indicating that the task has been completed, the task can be considered completed.    \n        5. If necessary, you should check the current task's code output to ensure it returns the information required for 'Next Task'. If it does not, then the current task can be considered incomplete.\n        6. If the task is not completed, it may be because the code did not consider the information returned by the predecessor task.\n        7. The JSON response must be enclosed between ```json and ```.\n        Now you will be provided with the following information, please give the result JSON according to these information.\n        ''',\n        '_USER_TASK_JUDGE_PROMPT': '''\n        User's information are as follows:\n        Current Code: {current_code}\n        Task: {task}\n        Code Output: {code_output}\n        Code Error: {code_error}\n        Current Working Directiory: {current_working_dir}\n        Working Directory: {working_dir}\n        Files And Folders in Current Working Directiory: {files_and_folders}\n        Next Task: {next_action}\n        Detailed description of user information:\n        1. 'Working Directory' represents the root directory of the working directory.\n        2. 'Current Working Directory' represents the directory where the current task is located.    \n        3. 'Code Output' represents the output of the code execution, which may be empty.\n        4. 'Code Error' represents any error messages generated during code execution, which may also be empty.\n        5. 'Next Task' describes tasks that follow the current task and may depend on the return from the current task. \n\n        Note: Please output according to the output format specified in the system message.\n        ''',\n\n        # Tool usage prompts in os\n        '_SYSTEM_TOOL_USAGE_PROMPT': '''\n        You are a useful AI assistant capable of accessing APIs to complete user-specified tasks, according to API documentation, \n        by using the provided ToolRequestUtil tool. The API documentation is as follows: \n        {openapi_doc}\n        The user-specified task is as follows: \n        {tool_sub_task}\n        The context which can further help you to determine the params of the API is as follows:\n        {context}\n        You need to complete the code using the ToolRequestUtil tool to call the specified API and print the return value\n        of the api. \n        ToolRequestUtil is a utility class, and the parameters of its 'request' method are described as follows:\n        def request(self, api_path, method, params=None, content_type=None):\n            \"\"\"\n            :param api_path: the path of the API\n            :param method: get/post\n            :param params: the parameters of the API, can be None.You cannot pass files to 'params' parameter.All files should be passed to 'files' parameter. \n            :param files: files to be uploaded, can be None.Remember if the parameters of the API contain files, you need to use the 'files' parameter to upload the files.\n            :param content_type: the content_type of api, e.g., application/json, multipart/form-data, can be None\n            :return: the response from the API\n            \"\"\"\n        Please begin your code completion:\n        ''',\n        '_USER_TOOL_USAGE_PROMPT': '''\n        from oscopilot.tool_repository.manager.tool_request_util import ToolRequestUtil\n        tool_request_util = ToolRequestUtil()\n        # TODO: your code here\n        ''',\n\n        # QA prompts in os\n        '_SYSTEM_QA_PROMPT': '''\n        You are a helpful ai assistant that can answer the question with the help of the context provided by the user in a step by step manner. The full question may help you to solve the current question.\n        If you don't know how to answer the user's question, answer \"I don't know.\" instead of making up an answer. \n        And you should also follow the following criteria:\n        1. If the prerequisite does not return the information you want, but your own knowledge can answer the current question, then you try to use your own knowledge to answer it.\n        2. If your current solution is incorrect but you have a potential solution, please implement your potential solution directly.\n        3. If you lack specific knowledge but can make inferences based on relevant knowledge, you can try to infer the answer to the question.\n        Now you will be provided with the following user information.\n        ''',\n        '_USER_QA_PROMPT': '''\n        Context: {context}\n        Full Question: {question} \n        Current Question: {current_question} \n        Detailed description of user information:\n        1. 'Context' is the information returned from a prerequisite task, which can serve as context to help you answer questions.\n        '''\n\n    },\n\n    'planning_prompt': {\n        # Task decompose prompts in os\n        '_SYSTEM_TASK_DECOMPOSE_PROMPT': '''\n        You are an expert at breaking down a task into subtasks.\n        I will give you a task and ask you to decompose this task into a series of subtasks. These subtasks can form a directed acyclic graph. Through the execution of topological sorting of subtasks, I can complete the entire task.\n        You can only return the reasoning process and the JSON that stores the subtasks information. \n        The content and format requirements for the reasoning process and subtasks information are as follows:\n        1. Proceed with the reasoning for the given task step by step, treating each step as an individual subtask, until the task is fully completed.\n        2. In JSON, each subtask is identified by a key that represents the name of the subtask. Every subtask is broken down into three attributes: 'description', 'dependencies', and 'type'. These attributes are determined through a reasoning process about the subtask.\n        3. Each subtask's name is abstracted from the reasoning process specific to that task and can serve as a generic label for a range of similar tasks. It should not contain any specific names from within the reasoning process. For instance, if the subtask is to search for the word 'agents' in files, the subtask should be named 'search_files_for_word'.\n        4. The three attributes for each subtask are described as follows:\n                description: The description of the current subtask corresponds to a certain step in task reasoning. \n                dependencies: This term refers to the list of names of subtasks that the current subtask depends upon, as determined by the reasoning process. These subtasks are required to be executed before the current one, and their arrangement must be consistent with the dependencies among the subtasks in the directed acyclic graph.\n                type: The task type of subtask, used to indicate in what form the subtask will be executed.\n        5. There are five types of subtasks:\n                Python: Python is suited for subtasks that involve complex data handling, analysis, machine learning, or the need to develop cross-platform scripts and applications. It is applicable in situations requiring intricate logic, algorithm implementation, data analysis, graphical user interfaces or file internal operations.\n                Shell: When the subtask primarily focuses on operating system-level automation, such as quick operations on the file system (creating, moving, deleting files), batch renaming files, system configuration, and monitoring and managing the operating system or system resources, Shell scripts are particularly suitable for quickly executing system-level batch processing tasks. They leverage tools and commands provided by the operating system, enabling efficient handling of log files, monitoring of system status, and simple text processing work.\n                AppleScript: AppleScript is primarily aimed at the macOS platform and is suitable for automating application operations on macOS, adjusting system settings, or implementing workflow automation between applications. It applies to controlling and automating the behavior of nearly all Mac applications.\n                API: API subtasks are necessary when interaction with external services or platforms is required, such as retrieving data, sending data, integrating third-party functionalities or services. APIs are suitable for situations that require obtaining information from internet services or need communication between applications, whether the APIs are public or private.\n                QA: QA subtasks are primarily about answering questions, providing information, or resolving queries, especially those that can be directly answered through knowledge retrieval or specific domain expertise. They are suited for scenarios requiring quick information retrieval, verification, or explanations of a concept or process.\n        6. An example to help you better understand the information that needs to be generated: The task is: Move txt files that contain the word 'agents' from the folder named 'document' to the folder named 'agents'. Then the reasoning process and JSON that stores the subtasks information are as follows: \n                Reasoning:\n                    According to 'Current Working Directiory' and Files And 'Folders in Current Working Directiory' information, the 'document' folder and 'agents' folder exist, therefore, there is no need to break down the subtasks to determine whether the folder exists.\n                    1. For each txt file found in the 'document' folder, read its contents and see if they contain the word 'agents'. Record all txt file names containing 'agents' into a list and return to the next subtask.\n                    2. Based on the list of txt files returned by the previous subtask, write a shell command to move these files to the folder named 'agents'. \n\n                ```json\n                {\n                    \"retrieve_files\" : {\n                        \"description\": \"For each txt file found in the 'document' folder, read its contents and see if they contain the word 'agents'. Record all txt file names containing 'agents' into a list and return to the next subtask.\",\n                        \"dependencies\": [],\n                        \"type\" : \"Python\"\n                    },\n                    \"organize_files\" : {\n                        \"description\": \"Based on the list of txt files returned by the previous subtask, write a shell command to move these files to the folder named 'agents'.\",\n                        \"dependencies\": [\"retrieve_files\"],\n                        \"type\": \"Shell\"\n                    }    \n                }      \n                ```  \n\n        And you should also follow the following criteria:\n        1. Try to break down the task into as few subtasks as possible.\n        2. Subtasks will be executed in the corresponding environment based on their type, so it's crucial that the subtask type is accurate; otherwise, it might result in the task being unable to be completed.\n        3. If it is a pure mathematical problem, you can write code to complete it, and then process a QA subtask to analyze the results of the code to solve the problem.\n        4. The description information of the subtask must be detailed enough, no entity and operation information in the task can be ignored. Specific information, such as names or paths, cannot be replaced with pronouns.\n        5. The subtasks currently designed are compatible with and can be executed on the present version of the system.\n        6. Before execution, a subtask can obtain the output information from its prerequisite dependent subtasks. Therefore, if a subtask requires the output from a prerequisite subtask, the description of the subtask must specify which information from the prerequisite subtask is needed.\n        7. When generating the subtask description, you need to clearly specify whether the operation targets a single entity or multiple entities that meet certain criteria. \n        8. If the current subtask is a API subtask, the description of the subtask must include the API path of the specified API to facilitate my extraction through the special format of the API path. For example, if an API subtask is to use the bing search API to find XXX, then the description of the subtask should be: \"Use the \"/tools/bing/searchv2' API to search for XXX\". \n        9. Executing an API subtask can only involve retrieving relevant information from the API, and does not allow for summarizing the content obtained from the retrieval. Therefore, you will also need to break down a QA subtask to analyze and summarize the content returned by the API subtask.\n        10. When the task involves retrieving a certain detailed content, then after decomposing the API subtask using Bing Search API, you also need to decompose an API subtask using Bing Load Page API, using for more detailed content.\n        11. Please be aware that only the APIs listed in the API List are available. Do not refer to or attempt to use APIs that are not included in this list.\n        12. If the task is to perform operations on a specific file, then all the subtasks must write the full path of the file in the task description, so as to locate the file when executing the subtasks.\n        13. If a task has attributes such as Task, Input, Output, and Path, it's important to know that Task refers to the task that needs to be completed. Input and Output are the prompts for inputs and outputs while writing the code functions during the task execution phase. Path is the file path that needs to be operated on.\n        14. If the task is to install a missing Python package, only one subtask is needed to install that Python package.\n        15. The JSON response must be enclosed between ```json and ```.\n\n        Now you will be provided with the following information, please give the reasoning process and the JSON that stores the subtasks information according to these information.\n        ''',\n        '_USER_TASK_DECOMPOSE_PROMPT': '''\n        User's information are as follows:\n        System Version: {system_version}\n        Task: {task}\n        Tool List: {tool_list}\n        API List: {api_list}\n        Current Working Directiory: {working_dir}\n        Files And Folders in Current Working Directiory: {files_and_folders}\n        Detailed description of user information:\n        1. 'Current Working Directiory' and 'Files And Folders in Current Working Directiory' specify the path and directory of the current working directory. These information may help you understand and generate subtasks.\n        2. 'Tool List' contains the name of each tool and the corresponding operation description. These tools are previously accumulated for completing corresponding subtasks. If a subtask corresponds to the description of a certain tool, then the subtask name and the tool name are the same, to facilitate the invocation of the relevant tool when executing the subtask.\n        3. 'API List' that includes the API path and their corresponding descriptions. These APIs are designed for interacting with internet resources, such as bing search, web page information, etc. \n        ''',\n\n        # Task replan prompts in os\n        '_SYSTEM_TASK_REPLAN_PROMPT': '''\n        You are an expert at designing new tasks based on the results of your reasoning.\n        When I was executing the code of current task, an issue occurred that is not related to the code. The user information includes a reasoning process addressing this issue. Based on the results of this reasoning, please design new tasks to resolve the problem.     \n        You can only return the reasoning process and the JSON that stores the tasks information. \n        The content and format requirements for the reasoning process and tasks information are as follows:\n        1. Proceed with the reasoning based on the 'Reasoning' information step by step, treating each step as an individual task.\n        2. In JSON, each subtask is identified by a key that represents the name of the subtask. Every subtask is broken down into three attributes: 'description', 'dependencies', and 'type'. These attributes are determined through a reasoning process about the subtask.\n        3. Each subtask's name is abstracted from the reasoning process specific to that task and can serve as a generic label for a range of similar tasks. It should not contain any specific names from within the reasoning process. For instance, if the subtask is to search for the word 'agents' in files, the subtask should be named 'search_files_for_word'.\n        4. The three attributes for each task are described as follows:\n                description: The description of the current task corresponds to a certain step in task reasoning. \n                dependencies: This term refers to the list of names of task that the current task depends upon, as determined by the reasoning process. These tasks are required to be executed before the current one, and their arrangement must be consistent with the dependencies among the tasks.\n                type: The task type of task, used to indicate in what form the task will be executed.\n        5. There are five types of tasks:\n                Python: Python is suited for tasks that involve complex data handling, analysis, machine learning, or the need to develop cross-platform scripts and applications. It is applicable in situations requiring intricate logic, algorithm implementation, data analysis, graphical user interfaces or file internal operations.\n                Shell: When the task primarily focuses on operating system-level automation, such as quick operations on the file system (creating, moving, deleting files), batch renaming files, system configuration, and monitoring and managing the operating system or system resources, Shell scripts are particularly suitable for quickly executing system-level batch processing tasks. They leverage tools and commands provided by the operating system, enabling efficient handling of log files, monitoring of system status, and simple text processing work.\n                AppleScript: AppleScript is primarily aimed at the macOS platform and is suitable for automating application operations on macOS, adjusting system settings, or implementing workflow automation between applications. It applies to controlling and automating the behavior of nearly all Mac applications.\n                API: API tasks are necessary when interaction with external services or platforms is required, such as retrieving data, sending data, integrating third-party functionalities or services. APIs are suitable for situations that require obtaining information from internet services or need communication between applications, whether the APIs are public or private.\n                QA: QA tasks are primarily about answering questions, providing information, or resolving queries, especially those that can be directly answered through knowledge retrieval or specific domain expertise. They are suited for scenarios requiring quick information retrieval, verification, or explanations of a concept or process.\n        6. An example to help you better understand the information that needs to be generated: The reasoning process analyzed that the reason for the error was that there was no numpy package in the environments, causing it to fail to run. Then the reasoning process and JSON that stores the tasks information are as follows: \n                Reasoning:\n                    1. According to the reasoning process of error reporting, because there is no numpy package in the environments, we need to use the pip tool to install the numpy package.\n\n                ```json\n                {\n                    \"install_package\" : {\n                        \"description\": \"Use pip to install the numpy package that is missing in the environments.\",\n                        \"dependencies\": [],\n                        \"type\" : \"shell\"\n                    }\n                }\n                ```\n\n        And you should also follow the following criteria:\n        1. Try to design as few tasks as possible.\n        2. tasks will be executed in the corresponding environment based on their task type, so it's crucial that the task type is accurate; otherwise, it might result in the task being unable to be completed.\n        3. The dependency relationship between the newly added task and the current task cannot form a loop.\n        4. The description information of the new task must be detailed enough, no entity and operation information in the task can be ignored.\n        5. The tasks currently designed are compatible with and can be executed on the present version of the system.\n        6. Before execution, a task can obtain the output information from its prerequisite dependent tasks. Therefore, if a task requires the output from a prerequisite task, the description of the task must specify which information from the prerequisite task is needed.\n        7. The JSON response must be enclosed between ```json and ```.\n\n        Now you will be provided with the following information, please give the reasoning process and the JSON that stores the tasks information according to these information.\n        ''',\n        '_USER_TASK_REPLAN_PROMPT': '''\n        User's information are as follows:\n        Current Task: {current_task}\n        Current Task Description: {current_task_description}\n        System Version: {system_version}\n        Reasoning: {reasoning}\n        Tool List: {tool_list}\n        Current Working Directiory: {working_dir}\n        Files And Folders in Current Working Directiory: {files_and_folders}\n        Detailed description of user information:\n        1. 'Reasoning' indicates the reason why task execution failed and the corresponding solution, which can help you design new tasks.\n        2. 'Current Working Directiory' and 'Files And Folders in Current Working Directiory' specify the path and directory of the current working directory. These information may help you understand and generate tasks.\n        3. 'Tool List' contains the name of each tool and the corresponding operation description. These tools are previously accumulated for completing corresponding tasks. If a task corresponds to the description of a certain tool, then the task name and the tool name are the same, to facilitate the invocation of the relevant tool when executing the task.\n        ''',\n    },\n\n    'retrieve_prompt': {\n        # tool code filter prompts\n        '_SYSTEM_ACTION_CODE_FILTER_PROMPT': '''\n        You are an expert in analyzing python code.\n        I will assign you a task and provide a dictionary of tool names along with their corresponding codes. Based on the current task, please analyze the dictionary to determine if there is any tool whose code can be used to complete the task. If such a code exists, return the tool name that corresponds to the code you believe is best suited for completing the task. If no appropriate code exists, return an empty string.\n        You should only respond with the format as described below:\n        1. First, understand the requirements of the task. Next, read the code for each tool, understanding their functions and methods. Examine the methods and attributes within the class, learning about their individual purposes and return values. Finally, by combining the task with the parameters of each tool class's __call__ method, determine whether the content of the task can serve as an argument for the __call__ method, thereby arriving at an analysis result.\n        2. Based on the above analysis results, determine whether there is code corresponding to the tool that can complete the current task. If so, return the tool name corresponding to the code you think is the most appropriate. If not, return an empty string.\n        3. Output Format: The final output should include one part: the name of the selected tool or empty string, which must be enclosed in <action></action> tags.    \n        And you should also follow the following criteria:\n        1. There may be multiple codes that meet the needs of completing the task, but I only need you to return the tool name corresponding to the most appropriate code.\n        2. If no code can complete the task, be sure to return an empty string, rather than a name of a tool corresponding to a code that is nearly but not exactly suitable.\n        ''',\n        '_USER_ACTION_CODE_FILTER_PROMPT': '''\n        User's information are as follows:\n        Tool Code Pair: {tool_code_pair}\n        Task: {task_description}\n        ''',\n    },\n    \n    'self_learning_prompt' : {\n        # self learning prompt\n        '_SYSTEM_COURSE_DESIGN_PROMPT' : '''\n        You are an expert in designing a python course focused entirely on using a specific Python package to operate a particular software, each lesson in the course includes specific tasks for operating the software package, as well as prompts for program input and output. Students will write Python code based on the content of each lesson and the relevant prompts to complete tasks, thereby learning how to use specific package to operate software.\n        I will provide you with the name of the software you need to learn, the specific Python package required to operate it, and an example of course design. Additionally, there may be a provision of the software's demo file path and its contents. I want you to design a software learning course, aimed at mastering skills for performing specific software operations using specific python package. Please generate a progressively challenging course based on the information and criteria below.\n        Excel Course Design Example: To help you better design a course on related software, here I provide you with an example of a course design for learning to manipulate Excel files using openpyxl. Lesson 1, use openpyxl to read all the contents of sheet 'Sheet1' in demo.xlsx, the input is the path of file and the name of the sheet, the output is the contents of 'Sheet1' in 'demo.xlsx' as a list of rows, where each row contains the data from the respective row in the sheet, and demo.xlsx is located in 'working_dir/demo.xlsx'. Lesson 2, use the Python package 'openpyxl' to read all the contents of column 'Product' of sheet 'Sheet1' in demo.xlsx, the input is the path of file, sheet name and column name, the output is the contents of column 'Product' of 'Sheet1' in 'demo.xlsx' as a list, and demo.xlsx is located in 'working_dir/demo.xlsx'. Lesson 3, use openpyxl to insert a new sheet named 'new sheet' into demo.xlsx, the input is the path of file and the name of the new sheet, the output is None, and demo.xlsx is located in 'working_dir/demo.xlsx'. Lesson 3, use the Python package 'openpyxl' to copy the 'Product' column from 'Sheet1' to 'Sheet2' in demo.xlsx. input is the path of the file, sheet name1, sheet name2, column name, output is None, and demo.xlsx is located in 'working_dir/demo.xlsx'. Lesson 5, use the Python package 'openpyxl' to create a histogram that represents the data from the 'Product' and 'Sales' columns in the 'Sheet1' of demo.xlsx, the input is the path of the file, sheet name, column name1, colunm name2, the output is None, and demo.xlsx is located in 'working_dir/demo.xlsx'. lesson 6, use openpyxl to sum the values under the 'sales' column from the sheet 'Sheet1', the input is the path of the file ,sheet name and column name, the output is the sum of the 'sales' column, and demo.xlsx is located in 'working_dir/demo.xlsx'. \n        Note that only six lessons are listed here for demonstration purposes; you will need to design the course to include as many lessons as possible to comprehensively learn Python package manipulation in practice.\n        You should only respond with the format as described below:\n        1. Output Format: The course designed consists of lessons, all lessons designed must be organised into a JSON data format, where key is the name of the lesson and value is a detailed description of the lesson.\n        2. Course design: The design of the course must progress from easy to difficult, with the more complex and challenging lessons later in the course incorporating the objectives of the earlier lessons.\n        3. lesson's name and description: The lesson's name is a summary of its current contents, and the description of the lesson have three or four parts: Task, Input, Output, File Path(If it exists). Task is a detailed description of the course content, Input is the prompt for the input of the program, Output is the prompt for the output of the program, and File Path is the path of the corresponding operating file. \n        4. Continuing with the Excel Course Design Example, the format of the JSON data I want to get is as follows:\n        ```json\n        {\n            \"read_specified_sheet\" : \"Task: Use the Python package 'openpyxl' to read all the contents of sheet 'Sheet1' in demo.xlsx. Input: The path of file, sheet name. Output: return the contents of 'Sheet1' in 'demo.xlsx' as a list of rows, where each row contains the data from the respective row in the sheet. File Path: working_dir/demo.xlsx\",\n            \"read_specified_sheet_column\" : \"Task: Use the Python package 'openpyxl' to read all the contents of column 'Product' of sheet 'Sheet1' in demo.xlsx. Input: The path of file, sheet name and column name. Output: return the contents of column 'Product' of 'Sheet1' in 'demo.xlsx' as a list. File Path: working_dir/demo.xlsx\",        \n            \"insert_new_sheet\" : \"Task: Use the Python package 'openpyxl' to insert a new sheet named 'new sheet' into demo.xlsx. Input: The path of file and the name of the new sheet. Output: None. File Path: working_dir/demo.xlsx\",\n            \"copy_column_to_another_sheet\" : \"Task: Use the Python package 'openpyxl' to copy the 'Product' column from 'Sheet1' to 'Sheet2' in demo.xlsx. Input: The path of the file, sheet name1, sheet name2, column name. Output: None. File Path: working_dir/demo.xlsx\",\n            \"plot_histogram_from_sheet \" : \"Task: Use the Python package 'openpyxl' to create a histogram that represents the data from the 'Product' and 'Sales' columns in the 'Sheet1' of demo.xlsx. Input: The path of the file, sheet name, column name1, colunm name2. Output: None. File Path: working_dir/demo.xlsx\",\n            \"sum_column_values_in_sheet\" : \"Task: Use the Python package 'openpyxl' to sum the values under the 'Sales' column from the sheet 'Sheet1'. Input: The path of the file ,sheet name and column name. Output: The sum of the 'sales' column in 'Sheet1'. File Path: working_dir/demo.xlsx\"\n        }\n        ```\n        And you should also follow the following criteria:\n        1. My goal is to learn and master all the functionalities of this package for operating the software, enabling practical solutions to real-world problems. Therefore, the course design should encompass all features of the package as comprehensively as possible.\n        2. Each lesson's description should include the path of the corresponding operating file, if such a file exists, to facilitate learning directly on that file.\n        3. Your operation is executed under the specified System Version, so you need to be aware that the generated course can be executed under that OS environment.\n        4. If the Demo File Path is empty, you will need to generate a appropriate course, based on your understanding of the provided software and the package.\n        5. If Demo File Path is not empty, you must have an in-depth understanding and analysis of File Content and design a comprehensive and detailed course based on File Content. \n        6. Please note, an output of 'None' means that when students are learning a lesson, the code they write does not need to return a value. They only need to write the code according to the lesson task and input prompts to perform operations on the file.\n        7. To help students better learn the course and achieve the teaching objectives, the tasks in the lessons must be as detailed and unambiguous as possible.\n        8. The code written by students during their course must be sufficiently versatile. Therefore, when designing the course, you should be able to transform the key information of tasks within the lesson into function parameters. Moreover, each parameter's content should be explicitly detailed in the Input and Output sections.\n        9. If the Current Course is not empty, you must design new lessons based on the existing course content, ensuring that these new lessons do not duplicate any lessons that are already present.\n        10. The JSON response must be enclosed between ```json and ```.\n        ''',\n        '_USER_COURSE_DESIGN_PROMPT' : '''\n        User's information are as follows:\n        Software Name: {software_name}\n        Python Package Name: {package_name}\n        Demo File Path: {demo_file_path} \n        File Content: {file_content}\n        Current Course: {current_course}\n        System Version: {system_version}\n        ''',       \n\n    },\n\n    'text_extract_prompt' : '''\n        Please return all the contents of the file. \n        File Path: {file_path}\n        Tips: \n        1. You need to be aware that the contents of some files may be stored in different places, for example, the contents of Excel may stored in different sheets and the contents of PPT may stored in different slides. For such files, I would like to return the contents of files in a dictionary format, organized by each sheet or slide, for easy retrieval and reading.\n        2. You can only break down the task into one subtask. The subtask is for reading out all the contents of the file.\n        3. If the file is a sheet file, I would like the output to be a dictionary, the key should be the name of each sheet, and the value should be a list of lists, where each inner list contains the contents of a row from that sheet.\n        '''\n    \n}\n"}
{"type": "source_file", "path": "agentstore/utils/osworld_parse.py", "content": "import json\nimport re\n\ndef parse_actions_from_string(input_string):\n    if input_string.strip() in ['WAIT', 'DONE', 'FAIL']:\n        return [input_string.strip()]\n    # Search for a JSON string within the input string\n    actions = []\n    matches = re.findall(r'```json\\s+(.*?)\\s+```', input_string, re.DOTALL)\n    if matches:\n        # Assuming there's only one match, parse the JSON string into a dictionary\n        try:\n            for match in matches:\n                action_dict = json.loads(match)\n                actions.append(action_dict)\n            return actions\n        except json.JSONDecodeError as e:\n            return f\"Failed to parse JSON: {e}\"\n    else:\n        matches = re.findall(r'```\\s+(.*?)\\s+```', input_string, re.DOTALL)\n        if matches:\n            # Assuming there's only one match, parse the JSON string into a dictionary\n            try:\n                for match in matches:\n                    action_dict = json.loads(match)\n                    actions.append(action_dict)\n                return actions\n            except json.JSONDecodeError as e:\n                return f\"Failed to parse JSON: {e}\"\n        else:\n            try:\n                action_dict = json.loads(input_string)\n                return [action_dict]\n            except json.JSONDecodeError:\n                raise ValueError(\"Invalid response format: \" + input_string)\n\n\ndef parse_code_from_string(input_string):\n    input_string = input_string.replace(\";\", \"\\n\")\n    if input_string.strip() in ['WAIT', 'DONE', 'FAIL']:\n        return [input_string.strip()]\n\n    # This regular expression will match both ```code``` and ```python code```\n    # and capture the `code` part. It uses a non-greedy match for the content inside.\n    pattern = r\"```(?:\\w+\\s+)?(.*?)```\"\n    # Find all non-overlapping matches in the string\n    matches = re.findall(pattern, input_string, re.DOTALL)\n\n    # The regex above captures the content inside the triple backticks.\n    # The `re.DOTALL` flag allows the dot `.` to match newline characters as well,\n    # so the code inside backticks can span multiple lines.\n\n    # matches now contains all the captured code snippets\n\n    codes = []\n\n    for match in matches:\n        match = match.strip()\n        commands = ['WAIT', 'DONE', 'FAIL']  # fixme: updates this part when we have more commands\n\n        if match in commands:\n            codes.append(match.strip())\n        elif match.split('\\n')[-1] in commands:\n            if len(match.split('\\n')) > 1:\n                codes.append(\"\\n\".join(match.split('\\n')[:-1]))\n            codes.append(match.split('\\n')[-1])\n        else:\n            codes.append(match)\n\n    return codes\n\n\ndef parse_code_from_som_string(input_string, masks):\n    # parse the output string by masks\n    tag_vars = \"\"\n    for i, mask in enumerate(masks):\n        x, y, w, h = mask\n        tag_vars += \"tag_\" + str(i + 1) + \"=\" + \"({}, {})\".format(int(x + w // 2), int(y + h // 2))\n        tag_vars += \"\\n\"\n\n    actions = parse_code_from_string(input_string)\n\n    for i, action in enumerate(actions):\n        if action.strip() in ['WAIT', 'DONE', 'FAIL']:\n            pass\n        else:\n            action = tag_vars + action\n            actions[i] = action\n\n    return actions"}
{"type": "source_file", "path": "agentstore/utils/llms.py", "content": "import openai\nimport logging\nimport os\nimport time\nimport requests\nimport json\nfrom dotenv import load_dotenv\n\n\nload_dotenv(override=True)\nMODEL_NAME = os.getenv('MODEL_NAME')\nOPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\nOPENAI_ORGANIZATION = os.getenv('OPENAI_ORGANIZATION')\n# BASE_URL = os.getenv('OPENAI_BASE_URL')\n\n# add\nMODEL_SERVER = os.getenv('MODEL_SERVER')\n\n\nclass OpenAI:\n    \"\"\"\n    A class for interacting with the OpenAI API, allowing for chat completion requests.\n\n    This class simplifies the process of sending requests to OpenAI's chat model by providing\n    a convenient interface for the chat completion API. It handles setting up the API key\n    and organization for the session and provides a method to send chat messages.\n\n    Attributes:\n        model_name (str): The name of the model to use for chat completions. Default is set\n                          by the global `MODEL_NAME`.\n        api_key (str): The API key used for authentication with the OpenAI API. This should\n                       be set through the `OPENAI_API_KEY` global variable.\n        organization (str): The organization ID for OpenAI. Set this through the\n                            `OPENAI_ORGANIZATION` global variable.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Initializes the OpenAI object with the given configuration.\n        \"\"\"\n\n        self.model_name = MODEL_NAME\n\n    def chat(self, messages, temperature=0):\n        \"\"\"\n        Sends a chat completion request to the OpenAI API using the specified messages and parameters.\n\n        Args:\n            messages (list of dict): A list of message dictionaries, where each dictionary\n                                     should contain keys like 'role' and 'content' to\n                                     specify the role (e.g., 'system', 'user') and content of\n                                     each message.\n            temperature (float, optional): Controls randomness in the generation. Lower values\n                                           make the model more deterministic. Defaults to 0.\n\n        Returns:\n            str: The content of the first message in the response from the OpenAI API.\n\n        \"\"\"\n        response = openai.chat.completions.create(\n            model=self.model_name,\n            messages=messages,\n            temperature=temperature\n        )\n        logging.info(f\"Response: {response.choices[0].message.content}\")\n\n        return response.choices[0].message.content\n\n\nclass LLAMA:\n    \"\"\"\n    A class for interacting with the OpenAI API, allowing for chat completion requests.\n\n    This class simplifies the process of sending requests to OpenAI's chat model by providing\n    a convenient interface for the chat completion API. It handles setting up the API key\n    and organization for the session and provides a method to send chat messages.\n\n    Attributes:\n        model_name (str): The name of the model to use for chat completions. Default is set\n                          by the global `MODEL_NAME`.\n        api_key (str): The API key used for authentication with the OpenAI API. This should\n                       be set through the `OPENAI_API_KEY` global variable.\n        organization (str): The organization ID for OpenAI. Set this through the\n                            `OPENAI_ORGANIZATION` global variable.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Initializes the OpenAI object with the given configuration.\n        \"\"\"\n\n        self.model_name = MODEL_NAME\n\n        self.llama_serve = MODEL_SERVER + \"/api/chat\"\n\n    def chat(self, messages, temperature=0):\n        \"\"\"\n        Sends a chat completion request to the OpenAI API using the specified messages and parameters.\n\n        Args:\n            messages (list of dict): A list of message dictionaries, where each dictionary\n                                     should contain keys like 'role' and 'content' to\n                                     specify the role (e.g., 'system', 'user') and content of\n                                     each message.\n            temperature (float, optional): Controls randomness in the generation. Lower values\n                                           make the model more deterministic. Defaults to 0.\n\n        Returns:\n            str: The content of the first message in the response from the OpenAI API.\n\n        \"\"\"\n        payload = {\n            \"model\": self.model_name,\n            \"messages\": messages,\n            \"stream\": False\n            \n        }\n\n        headers = {\n                \"Content-Type\": \"application/json\"}\n\n        response = requests.post(self.llama_serve, data=json.dumps(payload),headers=headers)\n\n        if response.status_code == 200:\n            # Get the response data\n            logging.info(f\"\"\"Response: {response.json()[\"message\"][\"content\"]}\"\"\")\n            return response.json()[\"message\"][\"content\"]\n        else:\n            logging.error(\"Failed to call LLM: \", response.status_code)\n            return \"\"\n\ndef main():\n    start_time = time.time()\n    messages = [{'role': 'system', 'content': 'You are Open Interpreter, a world-class programmer that can complete any goal by executing code.\\nFirst, write a plan. **Always recap the plan between each code block** (you have extreme short-term memory loss, so you need to recap the plan between each message block to retain it).\\nWhen you execute code, it will be executed **on the user\\'s machine**. The user has given you **full and complete permission** to execute any code necessary to complete the task. Execute the code.\\nIf you want to send data between programming languages, save the data to a txt or json.\\nYou can access the internet. Run **any code** to achieve the goal, and if at first you don\\'t succeed, try again and again.\\nYou can install new packages.\\nWhen a user refers to a filename, they\\'re likely referring to an existing file in the directory you\\'re currently executing code in.\\nWrite messages to the user in Markdown.\\nIn general, try to **make plans** with as few steps as possible. As for actually executing code to carry out that plan, for *stateful* languages (like python, javascript, shell, but NOT for html which starts from 0 every time) **it\\'s critical not to try to do everything in one code block.** You should try something, print information about it, then continue from there in tiny, informed steps. You will never get it on the first try, and attempting it in one go will often lead to errors you cant see.\\nYou are capable of **any** task.\\n\\n# THE COMPUTER API\\n\\nA python `computer` module is ALREADY IMPORTED, and can be used for many tasks:\\n\\n```python\\ncomputer.browser.search(query) # Google search results will be returned from this function as a string\\ncomputer.files.edit(path_to_file, original_text, replacement_text) # Edit a file\\ncomputer.calendar.create_event(title=\"Meeting\", start_date=datetime.datetime.now(), end=datetime.datetime.now() + datetime.timedelta(hours=1), notes=\"Note\", location=\"\") # Creates a calendar event\\ncomputer.calendar.get_events(start_date=datetime.date.today(), end_date=None) # Get events between dates. If end_date is None, only gets events for start_date\\ncomputer.calendar.delete_event(event_title=\"Meeting\", start_date=datetime.datetime) # Delete a specific event with a matching title and start date, you may need to get use get_events() to find the specific event object first\\ncomputer.contacts.get_phone_number(\"John Doe\")\\ncomputer.contacts.get_email_address(\"John Doe\")\\ncomputer.mail.send(\"john@email.com\", \"Meeting Reminder\", \"Reminder that our meeting is at 3pm today.\", [\"path/to/attachment.pdf\", \"path/to/attachment2.pdf\"]) # Send an email with a optional attachments\\ncomputer.mail.get(4, unread=True) # Returns the {number} of unread emails, or all emails if False is passed\\ncomputer.mail.unread_count() # Returns the number of unread emails\\ncomputer.sms.send(\"555-123-4567\", \"Hello from the computer!\") # Send a text message. MUST be a phone number, so use computer.contacts.get_phone_number frequently here\\n```\\n\\nDo not import the computer module, or any of its sub-modules. They are already imported.\\n\\nUser InfoName: hanchengcheng\\nCWD: /Users/hanchengcheng/Documents/official_space/open-interpreter\\nSHELL: /bin/bash\\nOS: Darwin\\nUse ONLY the function you have been provided with — \\'execute(language, code)\\'.'}, {'role': 'user', 'content': \"Plot AAPL and META's normalized stock prices\"}]\n    # message.append({\"role\": \"user\", \"content\": 'hello'})\n    # print(OPENAI_API_KEY)\n    # print(BASE_URL)\n    llm = OpenAI()\n\n    response = llm.chat(messages)\n    print(response)\n    end_time = time.time()\n    execution_time = end_time - start_time\n    print(f\"生成的单词数: {len(response)}\")\n    print(f\"程序执行时间: {execution_time}秒\")\n\nif __name__ == '__main__':\n    main()\n\n\n# def main():\n#     start_time = time.time()\n#     messages = [{'role': 'system', 'content': 'You are Open Interpreter, a world-class programmer that can complete any goal by executing code.\\nFirst, write a plan. **Always recap the plan between each code block** (you have extreme short-term memory loss, so you need to recap the plan between each message block to retain it).\\nWhen you execute code, it will be executed **on the user\\'s machine**. The user has given you **full and complete permission** to execute any code necessary to complete the task. Execute the code.\\nIf you want to send data between programming languages, save the data to a txt or json.\\nYou can access the internet. Run **any code** to achieve the goal, and if at first you don\\'t succeed, try again and again.\\nYou can install new packages.\\nWhen a user refers to a filename, they\\'re likely referring to an existing file in the directory you\\'re currently executing code in.\\nWrite messages to the user in Markdown.\\nIn general, try to **make plans** with as few steps as possible. As for actually executing code to carry out that plan, for *stateful* languages (like python, javascript, shell, but NOT for html which starts from 0 every time) **it\\'s critical not to try to do everything in one code block.** You should try something, print information about it, then continue from there in tiny, informed steps. You will never get it on the first try, and attempting it in one go will often lead to errors you cant see.\\nYou are capable of **any** task.\\n\\n# THE COMPUTER API\\n\\nA python `computer` module is ALREADY IMPORTED, and can be used for many tasks:\\n\\n```python\\ncomputer.browser.search(query) # Google search results will be returned from this function as a string\\ncomputer.files.edit(path_to_file, original_text, replacement_text) # Edit a file\\ncomputer.calendar.create_event(title=\"Meeting\", start_date=datetime.datetime.now(), end=datetime.datetime.now() + datetime.timedelta(hours=1), notes=\"Note\", location=\"\") # Creates a calendar event\\ncomputer.calendar.get_events(start_date=datetime.date.today(), end_date=None) # Get events between dates. If end_date is None, only gets events for start_date\\ncomputer.calendar.delete_event(event_title=\"Meeting\", start_date=datetime.datetime) # Delete a specific event with a matching title and start date, you may need to get use get_events() to find the specific event object first\\ncomputer.contacts.get_phone_number(\"John Doe\")\\ncomputer.contacts.get_email_address(\"John Doe\")\\ncomputer.mail.send(\"john@email.com\", \"Meeting Reminder\", \"Reminder that our meeting is at 3pm today.\", [\"path/to/attachment.pdf\", \"path/to/attachment2.pdf\"]) # Send an email with a optional attachments\\ncomputer.mail.get(4, unread=True) # Returns the {number} of unread emails, or all emails if False is passed\\ncomputer.mail.unread_count() # Returns the number of unread emails\\ncomputer.sms.send(\"555-123-4567\", \"Hello from the computer!\") # Send a text message. MUST be a phone number, so use computer.contacts.get_phone_number frequently here\\n```\\n\\nDo not import the computer module, or any of its sub-modules. They are already imported.\\n\\nUser InfoName: hanchengcheng\\nCWD: /Users/hanchengcheng/Documents/official_space/open-interpreter\\nSHELL: /bin/bash\\nOS: Darwin\\nUse ONLY the function you have been provided with — \\'execute(language, code)\\'.'}, {'role': 'user', 'content': \"Plot AAPL and META's normalized stock prices\"}]\n#     # message = [\n#     #         {\"role\": \"user\", \"content\": 'hello'},\n#     #     ]\n#     # print(OPENAI_API_KEY)\n#     # print(BASE_URL)\n#     llm = OpenAI()\n#     response = llm.chat(messages)\n#     print(response)\n#     end_time = time.time()\n#     execution_time = end_time - start_time\n#     print(f\"生成的单词数: {len(response)}\")\n#     print(f\"程序执行时间: {execution_time}秒\")\n\n"}
{"type": "source_file", "path": "quick_start.py", "content": "import argparse\nimport datetime\nimport json\nimport logging\nimport os\nimport sys\n\nfrom tqdm import tqdm\n\nfrom desktop_env.desktop_env import DesktopEnv\n\n# agents\nfrom agentstore import WordAgent, PptxAgent, ExcelAgent\nfrom agentstore import ImageAgent, OSAgent, VScodeAgent\nfrom agentstore.agents.gui_agent import ChromeAgent, GimpAgent, VlcAgent, ThunderbirdAgent, VscodeAgent, OsGUIAgent, CalcAgent, ImpressAgent, WriterAgent\n# to be updated more\n\nagent_dict = {\n    \"ChromeAgent\": ChromeAgent,\n    \"WordAgent\": WordAgent,\n    \"SlideAgent\": PptxAgent,\n    \"SheetAgent\": ExcelAgent,\n    \"ImageAgent\": ImageAgent,\n    \"VSAgent\": VScodeAgent,\n    \"Friday\": OSAgent,\n    \"GimpAgent\": GimpAgent,\n    \"VLCAgent\": VlcAgent,\n    \"MailAgent\": ThunderbirdAgent,\n    \"VSGUIAgent\": VscodeAgent,\n    \"OSAgent\": OsGUIAgent,\n    \"CalcAgent\": CalcAgent,\n    \"ImPressAgent\": ImpressAgent,\n    \"WriterAgent\" : WriterAgent,\n}\n\ndef replace_path(obj, old_path, new_path):\n    if isinstance(obj, dict):\n        for key, value in obj.items():\n            obj[key] = replace_path(value, old_path, new_path)\n    elif isinstance(obj, list):\n        obj = [replace_path(item, old_path, new_path) for item in obj]\n    elif isinstance(obj, str):\n        obj = obj.replace(old_path, new_path)\n    return obj\n\ndef config() -> argparse.Namespace:\n    parser = argparse.ArgumentParser(\n        description=\"Run end-to-end evaluation on the benchmark\"\n    )\n\n    # AgentStore config\n    parser.add_argument(\n        \"--agent_type\",\n        choices=[\"gui\", \"cli\"],\n        default=\"gui\",\n    )\n    parser.add_argument(\"--agent_name\", type=str, default='ChromeAgent')\n\n    # OSworld path and task config\n    parser.add_argument(\"--osworld_path\", type=str, default='./OSworld/')\n    parser.add_argument(\"--domain\", type=str, default='chrome')\n    parser.add_argument(\"--example_id\", type=str, default='bb5e4c0d-f964-439c-97b6-bdb9747de3f4')\n\n    # OSworld environment config\n    parser.add_argument(\"--path_to_vm\", type=str, default=None)\n    parser.add_argument(\n        \"--headless\", action=\"store_true\", help=\"Run in headless machine\"\n    )\n    parser.add_argument(\n        \"--action_space\", type=str, default=\"pyautogui\", help=\"Action type\"\n    )\n    parser.add_argument(\n        \"--observation_type\",\n        choices=[\"screenshot\", \"a11y_tree\", \"screenshot_a11y_tree\", \"som\"],\n        default=\"screenshot_a11y_tree\",\n        help=\"Observation type\",\n    )\n    parser.add_argument(\"--screen_width\", type=int, default=1920)\n    parser.add_argument(\"--screen_height\", type=int, default=1080)\n    parser.add_argument(\"--sleep_after_execution\", type=float, default=0.0)\n    parser.add_argument(\"--max_steps\", type=int, default=15)\n\n    # agent config\n    parser.add_argument(\"--max_trajectory_length\", type=int, default=3)\n    parser.add_argument(\n        \"--test_config_base_dir\", type=str, default=\"evaluation_examples\"\n    )\n\n    # lm config\n    parser.add_argument(\"--model\", type=str, default=\"gpt-4o\")\n    parser.add_argument(\"--temperature\", type=float, default=1.0)\n    parser.add_argument(\"--top_p\", type=float, default=0.9)\n    parser.add_argument(\"--max_tokens\", type=int, default=1500)\n    parser.add_argument(\"--stop_token\", type=str, default=None)\n\n    # logging related\n    parser.add_argument(\"--result_dir\", type=str, default=\"./results\")\n    args = parser.parse_args()\n\n    return args\n\ndef initialize_agent(agent_name, *args, **kwargs):\n    if agent_name not in agent_dict:\n        raise ValueError(f\"Agent '{agent_name}' is not recognized. Available agents: {list(agent_dict.keys())}\")\n    AgentClass = agent_dict[agent_name]\n    return AgentClass(*args, **kwargs)\n\ndef main():\n    args = config()\n    environment = DesktopEnv(\n        path_to_vm=args.path_to_vm,\n        action_space=args.action_space,\n        require_a11y_tree=True,\n    )\n\n    # test_all_meta_path = \"D:\\\\jcy\\\\OSWorld_new\\\\evaluation_examples/test_all.json\"\n    # example_path = \"D:\\\\jcy\\\\OSWorld\\\\evaluation_examples\"\n\n    # with open(test_all_meta_path, \"r\", encoding=\"utf-8\") as f:\n    #     test_all_meta = json.load(f)\n\n    # run_list_file_path = r'D:\\jcy\\OS-Copilot\\run_list.txt'\n    # run_list = []\n\n    # with open(run_list_file_path, 'r') as file:\n    #     for line in file:\n    #         run_list.append(line.strip())\n\n    # change path and tasks or run all tasks using loop\n    osworld_path = args.osworld_path\n    domain = args.domain\n    example_id = args.example_id\n    config_file = os.path.join(osworld_path, f\"evaluation_examples/examples/{domain}/{example_id}.json\")\n    with open(config_file, \"r\", encoding=\"utf-8\") as f:\n        example = json.load(f)\n    task_name = example['instruction']\n\n    print('task_name:', task_name)\n    setting_path = os.path.join(osworld_path, f\"evaluation_examples/settings\")\n    example = replace_path(example, 'evaluation_examples/settings', setting_path)\n\n    previous_obs = environment.reset(task_config=example)\n\n    action_space = args.action_space\n    observation_type = args.observation_type\n    max_trajectory_length = args.max_trajectory_length\n    max_steps = args.max_steps\n\n    if args.agent_type == 'gui':\n        agent = initialize_agent(\n            args.agent_name,\n            args,\n            example,\n            environment,\n            action_space,\n            observation_type,\n            max_trajectory_length,\n            max_steps=max_steps\n        )\n    elif args.agent_type == 'cli':\n        agent = initialize_agent(args.agent_name, args, example, environment, obs=previous_obs)\n    else:\n        raise ValueError(f\"Agent Type '{args.agent_type}' is not recognized.\")\n\n    agent.run()\n    print(\"evaluate.......\")\n    print(environment.evaluate())\n\nif __name__ == '__main__':\n    main()\n"}
{"type": "source_file", "path": "agentstore/agents/prompt.py", "content": "\"\"\"\nThis modules contains a comprehensive `prompts` dictionary that serves as a repository of prompts for guiding the AI agents's interactions across various operational scenarios, including execution, planning, and information retrieval tasks. These prompts are meticulously crafted to instruct the AI in performing its duties, ranging from code generation and amendment to task decomposition and planning, as well as error analysis and tool usage.\n\nThe dictionary is segmented into three main categories:\n\n1. **execute_prompt**: Contains prompts for execution-related tasks, such as code generation, invocation, amendment, and error judgment. These are further detailed for system actions and user interactions, facilitating a diverse range of programming and troubleshooting tasks.\n\n2. **planning_prompt**: Focuses on task planning and re-planning, decomposing complex tasks into manageable sub-tasks, and adapting plans based on unforeseen issues, ensuring that the AI can assist in project management and task organization effectively.\n\n3. **retrieve_prompt**: Dedicated to information retrieval, including filtering code snippets based on specific criteria, aiding the AI in sourcing and suggesting code solutions efficiently.\n\n4. **self_learning_prompt**: Contains prompts for self-learning tasks, such as designing educational courses based on software and content parameters. These prompts guide the AI in generating course designs and educational content tailored to user needs.\n\n5. **text_extract_prompt**: Contains prompts for text extraction tasks, such as extracting specific information from text data. These prompts guide the AI in identifying and extracting relevant data from text inputs.\n\nEach category comprises system and user prompts, where system prompts define the AI's task or query in detail, and user prompts typically include placeholders for dynamic information insertion, reflecting the context or specific requirements of the task at hand.\n\nUsage:\nThe `prompts` dictionary is utilized by the AI agents to dynamically select appropriate prompts based on the current context or task, ensuring relevant and precise guidance for each operation. This dynamic approach allows the AI to adapt its interactions and responses to suit a wide array of programming and operational needs, enhancing its utility and effectiveness in assisting users.\n\nExample:\n    .. code-block:: python\n\n        # Accessing a specific prompts for task execution\n        execute_prompt = prompts['execute_prompt']['_SYSTEM_SKILL_CREATE_AND_INVOKE_PROMPT']\n\"\"\"\nprompt = {\n    'execute_prompt': {\n        # shell/applescript generator\n        '_SYSTEM_SHELL_APPLESCRIPT_GENERATE_PROMPT': '''\n        You are a world-class programmer that can complete any task by executing code, your goal is to generate the corresponding code based on the type of code to complete the task.\n        You could only respond with a code.\n        Shell code output Format:\n        ```shell\n        shell code\n        ```\n\n        AppleScript code output Format:\n        ```applescript\n        applescript code\n        ```        \n\n        The code you write should follow the following criteria:\n        1. You must generate code of the specified 'Code Type' to complete the task.\n        2. The code logic should be clear and highly readable, able to meet the requirements of the task.\n        ''',\n        '_USER_SHELL_APPLESCRIPT_GENERATE_PROMPT': '''\n        User's information is as follows:\n        System Version: {system_version}\n        System language: simplified chinese\n        Working Directory: {working_dir}\n        Task Name: {task_name}\n        Task Description: {task_description}     \n        Information of Prerequisite Tasks: {pre_tasks_info}   \n        Code Type: {Type}\n        Detailed description of user information:\n        1. 'Working Directory' represents the working directory. It may not necessarily be the same as the current working directory. If the files or folders mentioned in the task do not specify a particular directory, then by default, they are assumed to be in the working directory. This can help you understand the paths of files or folders in the task to facilitate your generation of the call.\n        2. 'Information of Prerequisite Tasks' provides relevant information about the prerequisite tasks for the current task, encapsulated in a dictionary format. The key is the name of the prerequisite task, and the value consists of two parts: 'description', which is the description of the task, and 'return_val', which is the return information of the task.\n        3, 'Code Type' represents the type of code to be generated.\n        ''',        \n\n\n        # Python generate and invoke prompts in os\n        '_SYSTEM_PYTHON_SKILL_AND_INVOKE_GENERATE_PROMPT': '''\n        You are a world-class programmer that can complete any task by executing code, your goal is to generate the function code that accomplishes the task, along with the function's invocation.\n        You could only respond with a python code and a invocation statement.\n        Output Format:\n        ```python\n        python code\n        ```\n        <invoke>invocation statement</invoke>\n\n        The code you write should follow the following criteria:\n        1. Function name should be the same as the 'Task Name' provided by the user.\n        2. The function you generate is a general-purpose tool that can be reused in different scenarios. Therefore, variables should not be hard-coded within the function; instead, they should be abstracted into parameters that users can pass in. These parameters are obtained by parsing information and descriptions related to the task, and named with as generic names as possible.\n        3. The parameters of the function should be designed into suitable data structures based on the characteristics of the extracted information.\n        4. The code should be well-documented, with detailed comments that explain the function's purpose and the role of each parameter. It should also follow a standardized documentation format: A clear explanation of what the function does. Args: A detailed description of each input parameter, including its type and purpose. Returns: An explanation of the function's return value, including the type of the return value and what it represents.\n        5. The code logic should be clear and highly readable, able to meet the requirements of the task.\n        6. The function must have a return value. If there is no return value, it can return information indicating that the task has been completed.\n        7. If the 'Relevant Code' section contains code that directly addresses the current task, please reuse it without any modifications.\n        8. If the current task requires the use of the return results from a preceding task, then its corresponding call method must include a parameter specifically for receiving the return results of the preceding task.\n        9. If the current task depends on the results from a previous task, the function must include a parameter designed to accept the results from that previous task.\n        10. If the code involves the output of file paths, ensure that the output includes the files' absolute path.\n        11. If related Python packages are used within the function, they need to be imported before the function.\n\n        And the invocation statement should also follow the following criteria:\n        1. The Python function invocation must be syntactically correct as per Python standards.\n        2. Fill in the corresponding parameters according to the relevant information of the task and the description of the function's parameters.\n        3. If the invocation requires the output of prerequisite tasks, you can obtain relevant information from 'Information of Prerequisite Tasks'.\n\n        Now you will be provided with the following information, please write python code to accomplish the task and be compatible with system environments, versions and language according to these information.         \n        ''',\n        '_USER_PYTHON_SKILL_AND_INVOKE_GENERATE_PROMPT': '''\n        User's information is as follows:\n        System Version: {system_version}\n        System language: simplified chinese\n        Working Directory: {working_dir}\n        Task Name: {task_name}\n        Task Description: {task_description}     \n        Information of Prerequisite Tasks: {pre_tasks_info}   \n        Relevant Code: {relevant_code}\n        Detailed description of user information:\n        1. 'Working Directory' represents the working directory. It may not necessarily be the same as the current working directory. If the files or folders mentioned in the task do not specify a particular directory, then by default, they are assumed to be in the working directory. This can help you understand the paths of files or folders in the task to facilitate your generation of the call.\n        2. 'Information of Prerequisite Tasks' provides relevant information about the prerequisite tasks for the current task, encapsulated in a dictionary format. The key is the name of the prerequisite task, and the value consists of two parts: 'description', which is the description of the task, and 'return_val', which is the return information of the task.\n        3. 'Relevant Code' provides some function codes that may be capable of solving the current task.\n        ''',\n\n\n        # shell/applescript amend in os\n        '_SYSTEM_SHELL_APPLESCRIPT_AMEND_PROMPT': '''\n        You are an expert in programming, with a focus on diagnosing and resolving code issues.\n        Your goal is to precisely identify the reasons for failure in the existing code and implement effective modifications to ensure it accomplishes the intended task without errors.\n        You should only respond with a modified code.\n        Code in the format as described below:\n        1. Error Analysis: Conduct a step-by-step analysis to identify why the code is generating errors or failing to complete the task. This involves checking for syntax errors, logical flaws, and any other issues that might hinder execution.\n        2. Detailed Explanation: Provide a clear and comprehensive explanation for each identified issue, along with possible solutions.\n        3. Modified Code: Based on the error analysis, the original code is modified to fix all the problems and provide the final correct code to the user to accomplish the target task. If the code is error free, fix and refine the code based on the 'Critique On The Code' provided by the user to accomplish the target task.    \n\n        And the code you write should also follow the following criteria:\n        1. The code logic should be clear and highly readable, able to meet the requirements of the task.\n        2. The code must be enclosed between ```[code type] and ```. For example, ```shell [shell code] ```.\n        3. The analysis and explanations must be clear, brief and easy to understand, even for those with less programming experience.\n        4. All modifications must address the specific issues identified in the error analysis.\n        5. The solution must enable the code to successfully complete the intended task without errors.\n        6. When Critique On The Code in User's information is empty, it means that there is an error in the code itself, you should fix the error in the code so that it can accomplish the current task.\n\n        Now you will be provided with the following information, please give your modified code according to these information:\n        ''',\n        '_USER_SHELL_APPLESCRIPT_AMEND_PROMPT': '''\n        User's information are as follows:\n        Original Code: {original_code}\n        Task: {task}\n        Error Messages: {error}\n        Code Output: {code_output}\n        Current Working Directiory: {current_working_dir}\n        Working Directiory: {working_dir}\n        Files And Folders in Current Working Directiory: {files_and_folders}\n        Critique On The Code: {critique}\n        Information of Prerequisite Tasks: {pre_tasks_info}   \n        Detailed description of user information:\n        1. 'Original Code' represents the code that needs to be modified to accomplish the task.\n        2. 'Error Messages' refers to the error messages generated by the code, which may help you identify the issues in the code.\n        3. 'Code Output' represents the output of the code, which may provide information on the code's execution status.\n        4. 'Working Directory' represents the root directory of the working directory, and 'Current Working Directory' represents the directory where the current task is located.    \n        5. 'Critique On The Code' refers to code modification suggestions given by other code experts and may be empty.\n        6. 'Information of Prerequisite Tasks' from User's information provides relevant information about the prerequisite tasks for the current task, encapsulated in a dictionary format. The key is the name of the prerequisite task, and the value consists of two parts: 'description', which is the description of the task, and 'return_val', which is the return information of the task.\n        ''',\n\n\n        # Python amend and invoke prompts in os\n        '_SYSTEM_PYTHON_SKILL_AMEND_AND_INVOKE_PROMPT': '''\n        You are an expert in Python programming, with a focus on diagnosing and resolving code issues.\n        Your goal is to precisely identify the reasons for failure in the existing Python code and implement effective modifications to ensure it accomplishes the intended task without errors.\n        You should only respond with a python code and a invocation statement.\n        Python code in the format as described below:\n        1. Error Analysis: Conduct a step-by-step analysis to identify why the code is generating errors or failing to complete the task. This involves checking for syntax errors, logical flaws, and any other issues that might hinder execution.\n        2. Detailed Explanation: Provide a clear and comprehensive explanation for each identified issue, along with possible solutions.\n        3. Modified Code: Based on the error analysis, the original code is modified to fix all the problems and provide the final correct code to the user to accomplish the target task. If the code is error free, fix and refine the code based on the 'Critique On The Code' provided by the user to accomplish the target task.\n        invocation statement in the format as described below:\n        1. Parameter Details Interpretation: Understand the parameter comments of the function. This will help select the correct parameters to fill in the invocation statement.\n        2. Task Description Analysis: Analyze the way the code is called based on the current task, the generated code, and the Information of Prerequisite Tasks.\n        3. Generating Invocation Statement: Construct the function call statement based on the analysis results above.\n        4. Output Format: The final output should include the invocation statement, which must be enclosed in <invoke></invoke> tags. For example, <invoke>function()</invoke>.     \n\n        And the code you write should also follow the following criteria:\n        1. You must keep the original function name.\n        2. The code logic should be clear and highly readable, able to meet the requirements of the task.\n        3. The python code must be enclosed between ```python and ```.\n        4. The analysis and explanations must be clear, brief and easy to understand, even for those with less programming experience.\n        5. All modifications must address the specific issues identified in the error analysis.\n        6. The solution must enable the code to successfully complete the intended task without errors.\n        7. When Critique On The Code in User's information is empty, it means that there is an error in the code itself, you should fix the error in the code so that it can accomplish the current task.\n\n        And the invocation statement should also follow the following criteria:\n        1. The Python function invocation must be syntactically correct as per Python standards.\n        2. Clearly identify any fake or placeholder parameters used in the invocation.\n        3. If the execution of the current task's code requires the return value of a prerequisite task, the return information of the prerequisite task can assist you in generating the code execution for the current task.\n        4. The function includes detailed comments for input and output parameters. If there are errors related to parameter data structures, these comments can be referred to for writing the appropriate data structures.\n        5. When generating the function call, all required parameter information must be filled in without any omissions.\n        \n        Now you will be provided with the following information, please give your modified python code and invocation statement according to these information:\n        ''',\n        '_USER_PYTHON_SKILL_AMEND_AND_INVOKE_PROMPT': '''\n        User's information are as follows:\n        Original Code: {original_code}\n        Task: {task}\n        Error Messages: {error}\n        Code Output: {code_output}\n        Current Working Directiory: {current_working_dir}\n        Working Directiory: {working_dir}\n        Files And Folders in Current Working Directiory: {files_and_folders}\n        Critique On The Code: {critique}\n        Information of Prerequisite Tasks: {pre_tasks_info}   \n        Detailed description of user information:\n        1. 'Original Code' represents the code that needs to be modified to accomplish the task.\n        2. 'Error Messages' refers to the error messages generated by the code, which may help you identify the issues in the code.\n        3. 'Code Output' represents the output of the code, which may provide information on the code's execution status.\n        4. 'Working Directory' represents the root directory of the working directory, and 'Current Working Directory' represents the directory where the current task is located.    \n        5. 'Critique On The Code' refers to code modification suggestions given by other code experts and may be empty.\n        6. 'Information of Prerequisite Tasks' from User's information provides relevant information about the prerequisite tasks for the current task, encapsulated in a dictionary format. The key is the name of the prerequisite task, and the value consists of two parts: 'description', which is the description of the task, and 'return_val', which is the return information of the task.\n        ''',\n\n\n\n        # Task judge prompts in os\n        '_SYSTEM_TASK_JUDGE_PROMPT': '''\n        You are an program expert to verify code against a user's task requirements.\n        Your goal is to determine if the provided code accomplishes the user's specified task based on the feedback information, And score the code based on the degree of generalizability of the code.\n        You should only respond with a JSON result. \n        You must follow the analysis process and format requirements as follows:\n        1. Analyze the provided code: Examine the user's code to understand its functionality and structure.\n        2. Compare the code with the task description: Align the objectives stated in the user's task description with the capabilities of the code.\n        3. Evaluate the feedback information: Review the user's feedback, Includes 'Code Output', 'Code Error' and the working catalog information provided by user to measure the effectiveness of the code.\n        4. Formulate a reasoning process: Based on the analysis of the code and feedback received, generate a reasoning process about the execution of the code. If you believe the task has been successfully completed, you need to explain how the code accomplished the task. If you think the task has not been completed, you need to explain the reasons for the failure and provide corresponding solutions.\n        5. Evaluate task status: Based on the reasoning process, determine the status of the task. There are three possible statuses for a task:\n                Complete: The task has been successfully executed.\n                Amend: There are errors in the code, or the code does not meet the task requirements, necessitating fixes based on the reasoning process.\n                Replan: Errors encountered during code execution cannot be rectified by simply modifying the code, requiring additional operations within the code's execution environment. This necessitates new tasks to perform these extra operations.\n        6. Code's generality score: Evaluate the generality of the code and give code a score. The generality of the code can be analyzed based on parameters flexibility, error and exception handling, clarity of comments, code efficiency, security aspects, and other factors. According to the evaluation results, the code can be scored on a scale from 1 to 10, with integers reflecting the code's generality. A score of 1-3 indicates that the code is not very generic and can only complete the current task. A score of 4-6 indicates that the code can efficiently complete similar tasks, but the parameter names are not generic enough. A score of 7-8 indicates that the code is sufficiently generic but lacks in terms of security, clarity of comments, and fault tolerance. A score of 9-10 indicates that the code is highly generic in all aspects.\n        7. Output Format: \n        ```json\n        {\n            reasoning: Your reasoning process,\n            status: Complete/Amend/Replan,\n            score: 1-10\n        }\n        ``` \n\n        And you should also follow the following criteria:\n        1. Provide clear, logical reasoning.\n        2. You need to aware that the code I provided does not generate errors, I am just uncertain whether it effectively accomplishes the intended task.\n        3. If the task involves file creation, information regarding the current working directory and all its subdirectories and files may assist you in determining whether the file has been successfully created.\n        4. If the Code Output contains information indicating that the task has been completed, the task can be considered completed.    \n        5. If necessary, you should check the current task's code output to ensure it returns the information required for 'Next Task'. If it does not, then the current task can be considered incomplete.\n        6. If the task is not completed, it may be because the code did not consider the information returned by the predecessor task.\n        Now you will be provided with the following information, please give the result JSON according to these information:\n        ''',\n        '_USER_TASK_JUDGE_PROMPT': '''\n        User's information are as follows:\n        Current Code: {current_code}\n        Task: {task}\n        Code Output: {code_output}\n        Code Error: {code_error}\n        Current Working Directiory: {current_working_dir}\n        Working Directory: {working_dir}\n        Files And Folders in Current Working Directiory: {files_and_folders}\n        Next Task: {next_action}\n        Detailed description of user information:\n        1. 'Working Directory' represents the root directory of the working directory.\n        2. 'Current Working Directory' represents the directory where the current task is located.    \n        3. 'Code Output' represents the output of the code execution, which may be empty.\n        4. 'Code Error' represents any error messages generated during code execution, which may also be empty.\n        5. 'Next Task' describes tasks that follow the current task and may depend on the return from the current task. \n\n        Note: Please output according to the output format specified in the system message.\n        ''',\n\n        # Tool usage prompts in os\n        '_SYSTEM_TOOL_USAGE_PROMPT': '''\n        You are a useful AI assistant capable of accessing APIs to complete user-specified tasks, according to API documentation, \n        by using the provided ToolRequestUtil tool. The API documentation is as follows: \n        {openapi_doc}\n        The user-specified task is as follows: \n        {tool_sub_task}\n        The context which can further help you to determine the params of the API is as follows:\n        {context}\n        You need to complete the code using the ToolRequestUtil tool to call the specified API and print the return value\n        of the api. \n        ToolRequestUtil is a utility class, and the parameters of its 'request' method are described as follows:\n        def request(self, api_path, method, params=None, content_type=None):\n            \"\"\"\n            :param api_path: the path of the API\n            :param method: get/post\n            :param params: the parameters of the API, can be None.You cannot pass files to 'params' parameter.All files should be passed to 'files' parameter. \n            :param files: files to be uploaded, can be None.Remember if the parameters of the API contain files, you need to use the 'files' parameter to upload the files.\n            :param content_type: the content_type of api, e.g., application/json, multipart/form-data, can be None\n            :return: the response from the API\n            \"\"\"\n        Please begin your code completion:\n        ''',\n        '_USER_TOOL_USAGE_PROMPT': '''\n        from oscopilot.tool_repository.manager.tool_request_util import ToolRequestUtil\n        tool_request_util = ToolRequestUtil()\n        # TODO: your code here\n        ''',\n\n        # QA prompts in os\n        '_SYSTEM_QA_PROMPT': '''\n        You are a helpful ai assistant that can answer the question with the help of the context provided by the user in a step by step manner. The full question may help you to solve the current question.\n        If you don't know how to answer the user's question, answer \"I don't know.\" instead of making up an answer. \n        And you should also follow the following criteria:\n        1. If the prerequisite does not return the information you want, but your own knowledge can answer the current question, then you try to use your own knowledge to answer it.\n        2. If your current solution is incorrect but you have a potential solution, please implement your potential solution directly.\n        3. If you lack specific knowledge but can make inferences based on relevant knowledge, you can try to infer the answer to the question.\n        Now you will be provided with the following user information:\n        ''',\n        '_USER_QA_PROMPT': '''\n        Context: {context}\n        Full Question: {question} \n        Current Question: {current_question} \n        Detailed description of user information:\n        1. 'Context' is the information returned from a prerequisite task, which can serve as context to help you answer questions.\n        '''\n\n    },\n\n    'planning_prompt': {\n        # Task decompose prompts in os\n        '_SYSTEM_TASK_DECOMPOSE_PROMPT': '''\n        You are an expert at breaking down a task into subtasks.\n        I will give you a task and ask you to decompose this task into a series of subtasks. These subtasks can form a directed acyclic graph. Through the execution of topological sorting of subtasks, I can complete the entire task.\n        You can only return the reasoning process and the JSON that stores the subtasks information. \n        The content and format requirements for the reasoning process and subtasks information are as follows:\n        1. Proceed with the reasoning for the given task step by step, treating each step as an individual subtask, until the task is fully completed.\n        2. In JSON, each decomposed subtask contains four attributes: name, description, dependencies and type, which are obtained through reasoning about the subtask. The key of each subtask is the 'name' attribute of the subtask.\n        3. The four attributes for each subtask are described as follows:\n                name: The name of the subtask. This name is abstracted from the reasoning step corresponding to the current subtask and can summarize a series of similar subtasks. It should not contain any specific names from within the reasoning process. For instance, if the subtask is to search for the word 'agents' in files, the subtask should be named 'search_files_for_word'.\n                description: The description of the current subtask corresponds to a certain step in task reasoning. \n                dependencies: This term refers to the list of names of subtasks that the current subtask depends upon, as determined by the reasoning process. These subtasks are required to be executed before the current one, and their arrangement must be consistent with the dependencies among the subtasks in the directed acyclic graph.\n                type: The task type of subtask, used to indicate in what form the subtask will be executed.\n        4. There are five types of subtasks:\n                Python: Python is suited for subtasks that involve complex data handling, analysis, machine learning, or the need to develop cross-platform scripts and applications. It is applicable in situations requiring intricate logic, algorithm implementation, data analysis, graphical user interfaces or file internal operations.\n                Shell: When the subtask primarily focuses on operating system-level automation, such as quick operations on the file system (creating, moving, deleting files), batch renaming files, system configuration, and monitoring and managing the operating system or system resources, Shell scripts are particularly suitable for quickly executing system-level batch processing tasks. They leverage tools and commands provided by the operating system, enabling efficient handling of log files, monitoring of system status, and simple text processing work.\n                AppleScript: AppleScript is primarily aimed at the macOS platform and is suitable for automating application operations on macOS, adjusting system settings, or implementing workflow automation between applications. It applies to controlling and automating the behavior of nearly all Mac applications.\n                API: API subtasks are necessary when interaction with external services or platforms is required, such as retrieving data, sending data, integrating third-party functionalities or services. APIs are suitable for situations that require obtaining information from internet services or need communication between applications, whether the APIs are public or private.\n                QA: QA subtasks are primarily about answering questions, providing information, or resolving queries, especially those that can be directly answered through knowledge retrieval or specific domain expertise. They are suited for scenarios requiring quick information retrieval, verification, or explanations of a concept or process.\n        5. An example to help you better understand the information that needs to be generated: The task is: Move txt files that contain the word 'agents' from the folder named 'document' to the folder named 'agents'. Then the reasoning process and JSON that stores the subtasks information are as follows: \n                Reasoning:\n                    According to 'Current Working Directiory' and Files And 'Folders in Current Working Directiory' information, the 'document' folder and 'agents' folder exist, therefore, there is no need to break down the subtasks to determine whether the folder exists.\n                    1. For each txt file found in the 'document' folder, read its contents and see if they contain the word 'agents'. Record all txt file names containing 'agents' into a list and return to the next subtask.\n                    2. Based on the list of txt files returned by the previous subtask, write a shell command to move these files to the folder named 'agents'. \n\n                ```json\n                {\n                    \"retrieve_files\" : {\n                        \"name\": \"retrieve_files\",\n                        \"description\": \"For each txt file found in the 'document' folder, read its contents and see if they contain the word 'agents'. Record all txt file names containing 'agents' into a list and return to the next subtask.\",\n                        \"dependencies\": [],\n                        \"type\" : \"Python\"\n                    },\n                    \"organize_files\" : {\n                        \"name\": \"organize_files\",\n                        \"description\": \"Based on the list of txt files returned by the previous subtask, write a shell command to move these files to the folder named 'agents'.\",\n                        \"dependencies\": [\"retrieve_files\"],\n                        \"type\": \"Shell\"\n                    }    \n                }      \n                ```  \n\n        And you should also follow the following criteria:\n        1. Try to break down the task into as few subtasks as possible.\n        2. Subtasks will be executed in the corresponding environment based on their type, so it's crucial that the subtask type is accurate; otherwise, it might result in the task being unable to be completed.\n        3. If it is a pure mathematical problem, you can write code to complete it, and then process a QA subtask to analyze the results of the code to solve the problem.\n        4. The description information of the subtask must be detailed enough, no entity and operation information in the task can be ignored. Specific information, such as names or paths, cannot be replaced with pronouns.\n        5. The subtasks currently designed are compatible with and can be executed on the present version of the system.\n        6. Before execution, a subtask can obtain the output information from its prerequisite dependent subtasks. Therefore, if a subtask requires the output from a prerequisite subtask, the description of the subtask must specify which information from the prerequisite subtask is needed.\n        7. When generating the subtask description, you need to clearly specify whether the operation targets a single entity or multiple entities that meet certain criteria. \n        8. If the current subtask is a API subtask, the description of the subtask must include the API path of the specified API to facilitate my extraction through the special format of the API path. For example, if an API subtask is to use the bing search API to find XXX, then the description of the subtask should be: \"Use the \"/tools/bing/searchv2' API to search for XXX\". \n        9. Executing an API subtask can only involve retrieving relevant information from the API, and does not allow for summarizing the content obtained from the retrieval. Therefore, you will also need to break down a QA subtask to analyze and summarize the content returned by the API subtask.\n        10. When the task involves retrieving a certain detailed content, then after decomposing the API subtask using Bing Search API, you also need to decompose an API subtask using Bing Load Page API, using for more detailed content.\n        11. Please be aware that only the APIs listed in the API List are available. Do not refer to or attempt to use APIs that are not included in this list.\n        12. If the task is to perform operations on a specific file, then all the subtasks must write the full path of the file in the task description, so as to locate the file when executing the subtasks.\n        13. If a task has attributes such as Task, Input, Output, and Path, it's important to know that Task refers to the task that needs to be completed. Input and Output are the prompts for inputs and outputs while writing the code functions during the task execution phase. Path is the file path that needs to be operated on.\n        14. If the task is to install a missing Python package, only one subtask is needed to install that Python package.\n        \n        Now you will be provided with the following information, please give the reasoning process and the JSON that stores the subtasks information according to these information:\n        ''',\n        '_USER_TASK_DECOMPOSE_PROMPT': '''\n        User's information are as follows:\n        System Version: {system_version}\n        Task: {task}\n        Tool List: {tool_list}\n        API List: {api_list}\n        Current Working Directiory: {working_dir}\n        Files And Folders in Current Working Directiory: {files_and_folders}\n        Detailed description of user information:\n        1. 'Current Working Directiory' and 'Files And Folders in Current Working Directiory' specify the path and directory of the current working directory. These information may help you understand and generate subtasks.\n        2. 'Tool List' contains the name of each tool and the corresponding operation description. These tools are previously accumulated for completing corresponding subtasks. If a subtask corresponds to the description of a certain tool, then the subtask name and the tool name are the same, to facilitate the invocation of the relevant tool when executing the subtask.\n        3. 'API List' that includes the API path and their corresponding descriptions. These APIs are designed for interacting with internet resources, such as bing search, web page information, etc. \n        ''',\n\n        # Task replan prompts in os\n        '_SYSTEM_TASK_REPLAN_PROMPT': '''\n        You are an expert at designing new tasks based on the results of your reasoning.\n        When I was executing the code of current task, an issue occurred that is not related to the code. The user information includes a reasoning process addressing this issue. Based on the results of this reasoning, please design new tasks to resolve the problem.     \n        You can only return the reasoning process and the JSON that stores the tasks information. \n        The content and format requirements for the reasoning process and tasks information are as follows:\n        1. Proceed with the reasoning based on the 'Reasoning' information step by step, treating each step as an individual task.\n        2. In JSON, each task contains four attributes: name, description, dependencies and type, which are obtained through reasoning about the task. The key of each task is the 'name' attribute of the task.\n        3. The four attributes for each task are described as follows:\n                name: The name of the task. This name is abstracted from the reasoning step corresponding to the current task and can summarize a series of similar tasks. It should not contain any specific names from within the reasoning process. For instance, if the task is to search for the word 'agents' in files, the task should be named 'search_files_for_word'.\n                description: The description of the current task corresponds to a certain step in task reasoning. \n                dependencies: This term refers to the list of names of task that the current task depends upon, as determined by the reasoning process. These tasks are required to be executed before the current one, and their arrangement must be consistent with the dependencies among the tasks.\n                type: The task type of task, used to indicate in what form the task will be executed.\n        4. There are five types of tasks:\n                Python: Python is suited for tasks that involve complex data handling, analysis, machine learning, or the need to develop cross-platform scripts and applications. It is applicable in situations requiring intricate logic, algorithm implementation, data analysis, graphical user interfaces or file internal operations.\n                Shell: When the task primarily focuses on operating system-level automation, such as quick operations on the file system (creating, moving, deleting files), batch renaming files, system configuration, and monitoring and managing the operating system or system resources, Shell scripts are particularly suitable for quickly executing system-level batch processing tasks. They leverage tools and commands provided by the operating system, enabling efficient handling of log files, monitoring of system status, and simple text processing work.\n                AppleScript: AppleScript is primarily aimed at the macOS platform and is suitable for automating application operations on macOS, adjusting system settings, or implementing workflow automation between applications. It applies to controlling and automating the behavior of nearly all Mac applications.\n                API: API tasks are necessary when interaction with external services or platforms is required, such as retrieving data, sending data, integrating third-party functionalities or services. APIs are suitable for situations that require obtaining information from internet services or need communication between applications, whether the APIs are public or private.\n                QA: QA tasks are primarily about answering questions, providing information, or resolving queries, especially those that can be directly answered through knowledge retrieval or specific domain expertise. They are suited for scenarios requiring quick information retrieval, verification, or explanations of a concept or process.\n        5. An example to help you better understand the information that needs to be generated: The reasoning process analyzed that the reason for the error was that there was no numpy package in the environments, causing it to fail to run. Then the reasoning process and JSON that stores the tasks information are as follows: \n                Reasoning:\n                    1. According to the reasoning process of error reporting, because there is no numpy package in the environments, we need to use the pip tool to install the numpy package.\n\n                ```json\n                {\n                    \"install_package\" : {\n                        \"name\": \"install_package\",\n                        \"description\": \"Use pip to install the numpy package that is missing in the environments.\",\n                        \"dependencies\": [],\n                        \"type\" : \"shell\"\n                    }\n                }\n                ```\n\n        And you should also follow the following criteria:\n        1. Try to design as few tasks as possible.\n        2. tasks will be executed in the corresponding environment based on their task type, so it's crucial that the task type is accurate; otherwise, it might result in the task being unable to be completed.\n        3. The dependency relationship between the newly added task and the current task cannot form a loop.\n        4. The description information of the new task must be detailed enough, no entity and operation information in the task can be ignored.\n        5. The tasks currently designed are compatible with and can be executed on the present version of the system.\n        6. Before execution, a task can obtain the output information from its prerequisite dependent tasks. Therefore, if a task requires the output from a prerequisite task, the description of the task must specify which information from the prerequisite task is needed.\n        \n        Now you will be provided with the following information, please give the reasoning process and the JSON that stores the tasks information according to these information:\n        ''',\n        '_USER_TASK_REPLAN_PROMPT': '''\n        User's information are as follows:\n        Current Task: {current_task}\n        Current Task Description: {current_task_description}\n        System Version: {system_version}\n        Reasoning: {reasoning}\n        Tool List: {tool_list}\n        Current Working Directiory: {working_dir}\n        Files And Folders in Current Working Directiory: {files_and_folders}\n        Detailed description of user information:\n        1. 'Reasoning' indicates the reason why task execution failed and the corresponding solution, which can help you design new tasks.\n        2. 'Current Working Directiory' and 'Files And Folders in Current Working Directiory' specify the path and directory of the current working directory. These information may help you understand and generate tasks.\n        3. 'Tool List' contains the name of each tool and the corresponding operation description. These tools are previously accumulated for completing corresponding tasks. If a task corresponds to the description of a certain tool, then the task name and the tool name are the same, to facilitate the invocation of the relevant tool when executing the task.\n        ''',\n    },\n\n    'retrieve_prompt': {\n        # tool code filter prompts\n        '_SYSTEM_ACTION_CODE_FILTER_PROMPT': '''\n        You are an expert in analyzing python code.\n        I will assign you a task and provide a dictionary of tool names along with their corresponding codes. Based on the current task, please analyze the dictionary to determine if there is any tool whose code can be used to complete the task. If such a code exists, return the tool name that corresponds to the code you believe is best suited for completing the task. If no appropriate code exists, return an empty string.\n        You should only respond with the format as described below:\n        1. First, understand the requirements of the task. Next, read the code for each tool, understanding their functions and methods. Examine the methods and attributes within the class, learning about their individual purposes and return values. Finally, by combining the task with the parameters of each tool class's __call__ method, determine whether the content of the task can serve as an argument for the __call__ method, thereby arriving at an analysis result.\n        2. Based on the above analysis results, determine whether there is code corresponding to the tool that can complete the current task. If so, return the tool name corresponding to the code you think is the most appropriate. If not, return an empty string.\n        3. Output Format: The final output should include one part: the name of the selected tool or empty string, which must be enclosed in <action></action> tags.    \n        And you should also follow the following criteria:\n        1. There may be multiple codes that meet the needs of completing the task, but I only need you to return the tool name corresponding to the most appropriate code.\n        2. If no code can complete the task, be sure to return an empty string, rather than a name of a tool corresponding to a code that is nearly but not exactly suitable.\n        ''',\n        '_USER_ACTION_CODE_FILTER_PROMPT': '''\n        User's information are as follows:\n        Tool Code Pair: {tool_code_pair}\n        Task: {task_description}\n        ''',\n    },\n    \n    'self_learning_prompt' : {\n        # self learning prompt\n        '_SYSTEM_COURSE_DESIGN_PROMPT' : '''\n        You are an expert in designing a python course focused entirely on using a specific Python package to operate a particular software, each lesson in the course includes specific tasks for operating the software package, as well as prompts for program input and output. Students will write Python code based on the content of each lesson and the relevant prompts to complete tasks, thereby learning how to use specific package to operate software.\n        I will provide you with the name of the software you need to learn, the specific Python package required to operate it, and an example of course design. Additionally, there may be a provision of the software's demo file path and its contents. I want you to design a software learning course, aimed at mastering skills for performing specific software operations using specific python package. Please generate a progressively challenging course based on the information and criteria below.\n        Excel Course Design Example: To help you better design a course on related software, here I provide you with an example of a course design for learning to manipulate Excel files using openpyxl. Lesson 1, use openpyxl to read all the contents of sheet 'Sheet1' in demo.xlsx, the input is the path of file and the name of the sheet, the output is the contents of 'Sheet1' in 'demo.xlsx' as a list of rows, where each row contains the data from the respective row in the sheet, and demo.xlsx is located in 'working_dir/demo.xlsx'. Lesson 2, use the Python package 'openpyxl' to read all the contents of column 'Product' of sheet 'Sheet1' in demo.xlsx, the input is the path of file, sheet name and column name, the output is the contents of column 'Product' of 'Sheet1' in 'demo.xlsx' as a list, and demo.xlsx is located in 'working_dir/demo.xlsx'. Lesson 3, use openpyxl to insert a new sheet named 'new sheet' into demo.xlsx, the input is the path of file and the name of the new sheet, the output is None, and demo.xlsx is located in 'working_dir/demo.xlsx'. Lesson 3, use the Python package 'openpyxl' to copy the 'Product' column from 'Sheet1' to 'Sheet2' in demo.xlsx. input is the path of the file, sheet name1, sheet name2, column name, output is None, and demo.xlsx is located in 'working_dir/demo.xlsx'. Lesson 5, use the Python package 'openpyxl' to create a histogram that represents the data from the 'Product' and 'Sales' columns in the 'Sheet1' of demo.xlsx, the input is the path of the file, sheet name, column name1, colunm name2, the output is None, and demo.xlsx is located in 'working_dir/demo.xlsx'. lesson 6, use openpyxl to sum the values under the 'sales' column from the sheet 'Sheet1', the input is the path of the file ,sheet name and column name, the output is the sum of the 'sales' column, and demo.xlsx is located in 'working_dir/demo.xlsx'. \n        Note that only six lessons are listed here for demonstration purposes; you will need to design the course to include as many lessons as possible to comprehensively learn Python package manipulation in practice.\n        You should only respond with the format as described below:\n        1. Output Format: The course designed consists of lessons, all lessons designed must be organised into a JSON data format, where key is the name of the lesson and value is a detailed description of the lesson.\n        2. Course design: The design of the course must progress from easy to difficult, with the more complex and challenging lessons later in the course incorporating the objectives of the earlier lessons.\n        3. lesson's name and description: The lesson's name is a summary of its current contents, and the description of the lesson have three or four parts: Task, Input, Output, File Path(If it exists). Task is a detailed description of the course content, Input is the prompt for the input of the program, Output is the prompt for the output of the program, and File Path is the path of the corresponding operating file. \n        4. Continuing with the Excel Course Design Example, the format of the JSON data I want to get is as follows:\n        ```json\n        {\n            \"read_specified_sheet\" : \"Task: Use the Python package 'openpyxl' to read all the contents of sheet 'Sheet1' in demo.xlsx. Input: The path of file, sheet name. Output: return the contents of 'Sheet1' in 'demo.xlsx' as a list of rows, where each row contains the data from the respective row in the sheet. File Path: working_dir/demo.xlsx\",\n            \"read_specified_sheet_column\" : \"Task: Use the Python package 'openpyxl' to read all the contents of column 'Product' of sheet 'Sheet1' in demo.xlsx. Input: The path of file, sheet name and column name. Output: return the contents of column 'Product' of 'Sheet1' in 'demo.xlsx' as a list. File Path: working_dir/demo.xlsx\",        \n            \"insert_new_sheet\" : \"Task: Use the Python package 'openpyxl' to insert a new sheet named 'new sheet' into demo.xlsx. Input: The path of file and the name of the new sheet. Output: None. File Path: working_dir/demo.xlsx\",\n            \"copy_column_to_another_sheet\" : \"Task: Use the Python package 'openpyxl' to copy the 'Product' column from 'Sheet1' to 'Sheet2' in demo.xlsx. Input: The path of the file, sheet name1, sheet name2, column name. Output: None. File Path: working_dir/demo.xlsx\",\n            \"plot_histogram_from_sheet \" : \"Task: Use the Python package 'openpyxl' to create a histogram that represents the data from the 'Product' and 'Sales' columns in the 'Sheet1' of demo.xlsx. Input: The path of the file, sheet name, column name1, colunm name2. Output: None. File Path: working_dir/demo.xlsx\",\n            \"sum_column_values_in_sheet\" : \"Task: Use the Python package 'openpyxl' to sum the values under the 'Sales' column from the sheet 'Sheet1'. Input: The path of the file ,sheet name and column name. Output: The sum of the 'sales' column in 'Sheet1'. File Path: working_dir/demo.xlsx\"\n        }\n        ```\n        And you should also follow the following criteria:\n        1. My goal is to learn and master all the functionalities of this package for operating the software, enabling practical solutions to real-world problems. Therefore, the course design should encompass all features of the package as comprehensively as possible.\n        2. Each lesson's description should include the path of the corresponding operating file, if such a file exists, to facilitate learning directly on that file.\n        3. Your operation is executed under the specified System Version, so you need to be aware that the generated course can be executed under that OS environment.\n        4. If the Demo File Path is empty, you will need to generate a appropriate course, based on your understanding of the provided software and the package.\n        5. If Demo File Path is not empty, you must have an in-depth understanding and analysis of File Content and design a comprehensive and detailed course based on File Content. \n        6. Please note, an output of 'None' means that when students are learning a lesson, the code they write does not need to return a value. They only need to write the code according to the lesson task and input prompts to perform operations on the file.\n        7. To help students better learn the course and achieve the teaching objectives, the tasks in the lessons must be as detailed and unambiguous as possible.\n        8. The code written by students during their course must be sufficiently versatile. Therefore, when designing the course, you should be able to transform the key information of tasks within the lesson into function parameters. Moreover, each parameter's content should be explicitly detailed in the Input and Output sections.\n        ''',\n        '_USER_COURSE_DESIGN_PROMPT' : '''\n        User's information are as follows:\n        Software Name: {software_name}\n        Python Package Name: {package_name}\n        Demo File Path: {demo_file_path} \n        File Content: {file_content}\n        System Version: {system_version}\n        ''',       \n\n    },\n\n    'text_extract_prompt' : '''\n        Please return all the contents of the file. \n        File Path: {file_path}\n        Tips: \n        1. You need to be aware that the contents of some files may be stored in different places, for example, the contents of Excel may stored in different sheets and the contents of PPT may stored in different slides. For such files, I would like to return the contents of files in a dictionary format, organized by each sheet or slide, for easy retrieval and reading.\n        2. You can only break down the task into one subtask. The subtask is for reading out all the contents of the file.\n        3. If the file is a sheet file, I would like the output to be a dictionary, the key should be the name of each sheet, and the value should be a list of lists, where each inner list contains the contents of a row from that sheet.\n        '''\n    \n}\n"}
{"type": "source_file", "path": "agentstore/agents/plan_agent_all.py", "content": "from agentstore.utils import setup_config, setup_pre_run\nfrom agentstore.modules.base_module import BaseModule\nimport re\nimport os\nimport json\nimport base64\nimport time\nfrom rich.console import Console\nfrom rich.markdown import Markdown\nimport contextlib\n\nfrom desktop_env.envs.desktop_env import DesktopEnv\n\nconsole = Console()\n\ndef encode_image(image_content):\n    return base64.b64encode(image_content).decode('utf-8')\n\ndef rich_print(markdown_text):\n    md = Markdown(markdown_text)\n    console.print(md)\n\ndef extract_code(input_string):\n    pattern = r\"```(\\w+)?\\s*(.*?)```\"  # 匹配代码块，语言名称是可选项\n    matches = re.findall(pattern, input_string, re.DOTALL)\n    info = matches[0][1]\n    return info\n\n\ndef send_chat_prompts(message, llm):\n    return llm.chat(message)\n\nclass PlanAgent(BaseModule):\n    def __init__(self, args, task_name, agent_info_list):\n        super().__init__()\n        self.args = args\n        self.task_name = task_name\n\n        agents_name = [agent_info[\"name\"] for agent_info in agent_info_list]\n        agents_name = \", \".join(agents_name[:-1]) + \", and \" + agents_name[-1]\n        agent_description = f''\n        for agent_info in agent_info_list:\n            name = agent_info[\"name\"]\n            can = agent_info['can do']\n            cannot = agent_info['can\\'t do']\n            agent_description += f\"### {name}:\\n\"\n            agent_description += f\"**Can do:** {can}\\n\"\n            agent_description += f\"**Can't do:** {cannot}\\n\"\n        print(agent_description)\n        \n\n        self.light_planner_sys_prompt = '''You are the MasterAgent, a strategic coordinator responsible for delegating tasks among a team of specialized agents to complete OS-related tasks efficiently. \n        You will receive a high-level task along with a current system screenshot and determine the best approach for completing the task.\n        Your team consists of {0}, each with unique capabilities and cann't do:\n        {1}\n    Your task is to:\n\n    1. Analyze the given high-level task.\n    2. Review the provided system screenshot to understand the current state and context.\n    3. Determine which agent ({2}) is best suited to handle the task.\n    4. Determine whether the task requires cooperation between different agents, and if so, break down the task into steps that specify which agent should perform each part.\n    4. Provide a detailed description of the task, including any relevant information such as file paths, URLs, and steps to complete the task.\n    You will only provide the next agent, detailing which agent should perform the task, the task description, and whether any information is needed.\n    Follow the next format for Next:\n    ```markdown\n    Agent: CLIAgent\n    Task Description: Check if Python is installed by running the command python --version.\n```\n'''.format(agents_name, agent_description,agents_name)\n\n        print(self.light_planner_sys_prompt)\n\n        self.light_planner_user_prompt = '''\n        OS Task: {task}. Current System Screenshot  as below.\n        '''.format(task=self.task_name)\n\n        self.message_pool = None\n\n    def run(self,obs):\n        screenshot = obs['screenshot']\n        base64_image = encode_image(screenshot)\n\n        if self.message_pool == None:\n            message = [\n                {\"role\": \"system\", \"content\": self.light_planner_sys_prompt},\n                {\n                    \"role\": \"user\",\n                    \"content\": [\n                        {\n                            \"type\": \"text\",\n                            \"text\": self.light_planner_user_prompt\n                        },\n                        {\n                            \"type\": \"image_url\",\n                            \"image_url\": {\n                                \"url\": f\"data:image/png;base64,{base64_image}\",\n                                \"detail\": \"high\"\n                            }\n                        }\n                    ]\n                }\n            ]\n            self.message_pool = message\n        else:\n            pass\n            # self.message_pool.append(new_message)\n            # message = self.message_pool\n\n        print(\"send_chat_prompts...\")\n        response = send_chat_prompts(message, self.llm)\n        rich_print(response)\n\n        self.message_pool.append({\"role\": \"assistant\", \"content\": response})\n        info = extract_code(response)\n        print(info)\n        return info\n\ndef replace_path(obj,old_path,new_path):\n    if isinstance(obj, dict):\n        for key, value in obj.items():\n            obj[key] = replace_path(value,old_path,new_path)\n    elif isinstance(obj, list):\n        obj = [replace_path(item,old_path,new_path) for item in obj]\n    elif isinstance(obj, str):\n        obj = obj.replace(old_path, new_path)\n    return obj\n\n\nfrom cli_agent import CLIAgent\nfrom gui_agent import GUIAgent\nfrom word_agent import WordAgent\nfrom pptx_agent import PptxAgent\nfrom excel_agent import ExcelAgent\nfrom image_agent import ImageAgent\n\nagent_info_list = [CLIAgent.info, GUIAgent.info, WordAgent.info, PptxAgent.info, ExcelAgent.info]\n\n\nif __name__ == '__main__':\n\n\n    environment = DesktopEnv(\n            path_to_vm=r\"D:/jcy/OSWorld/vm_data/Ubuntu0/Ubuntu0/Ubuntu0.vmx\",\n            action_space=\"pyautogui\",\n            require_a11y_tree=True,\n        )\n\n\n    test_all_meta_path = \"D:\\\\jcy\\\\OSWorld\\\\evaluation_examples/test_all.json\"\n    example_path = \"D:\\\\jcy\\\\OSWorld\\\\evaluation_examples\"\n\n    with open(test_all_meta_path, \"r\", encoding=\"utf-8\") as f:\n        test_all_meta = json.load(f)\n\n    domain = 'gimp'\n    tasks = test_all_meta[domain]\n\n    # run_list_file_path = r'D:\\jcy\\OS-Copilot\\run_list.txt'\n    # run_list = []\n\n    # with open(run_list_file_path, 'r') as file:\n    #     for line in file:\n    #         run_list.append(line.strip())\n\n    for example_id in tasks:\n        try:\n            print(example_id)\n            config_file = os.path.join(example_path, f\"examples/{domain}/{example_id}.json\")\n            with open(config_file, \"r\", encoding=\"utf-8\") as f:\n                example = json.load(f)\n            task_name = example['instruction']\n            \n            log_file_path = os.path.join(\"cache\",example_id, f\"{example_id}.log\")\n            os.makedirs(os.path.dirname(log_file_path), exist_ok=True)\n            \n            with open(log_file_path, 'w', encoding=\"utf-8\") as log_file:\n                with contextlib.redirect_stdout(log_file):\n                    print('task_name:', task_name)\n                    example = replace_path(example, 'evaluation_examples/settings', 'D:\\\\jcy\\\\OSWorld\\\\evaluation_examples\\\\settings')\n                    \n                    previous_obs = environment.reset(task_config=example)\n                    args = setup_config()\n                    \n                    # plan_agent = PlanAgent(args, task_name, agent_info_list)\n                    # info = plan_agent.run(previous_obs)\n\n                    info = 'ImageAgent'\n                    \n                    if 'CLIAgent' in info:\n                        cli_agent = CLIAgent(args, task_name, environment)\n                        cli_agent.run(info)\n                    \n                    elif 'GUIAgent' in info:\n                        action_space = 'pyautogui'\n                        observation_type = 'screenshot_a11y_tree'\n                        max_trajectory_length = 3\n                        gui_agent = GUIAgent(args, example, environment, action_space, observation_type, max_trajectory_length)\n                        gui_agent.run()\n                    \n                    elif 'WordAgent' in info:\n                        word_agent = WordAgent(args, task_name, environment)\n                        word_agent.run()\n                    \n                    elif 'PptxAgent' in info:\n                        pptx_agent = PptxAgent(args, task_name, environment)\n                        pptx_agent.run()\n                    \n                    elif 'ExcelAgent' in info:\n                        excel_agent = ExcelAgent(args, task_name, environment)\n                        excel_agent.run()\n                    elif 'ImageAgent' in info:\n                        excel_agent = ImageAgent(args, task_name, environment)\n                        excel_agent.run()\n                    else:\n                        # replan \n                        # to be update\n                        pass\n                    \n                    # 判定内容\n                    print(\"evaluate.......\")\n                    print(environment.evaluate())\n        except Exception as e:\n            error_log_path = os.path.join(\"cache\",example_id, f\"{example_id}_error.log\")\n            os.makedirs(os.path.dirname(log_file_path), exist_ok=True)\n            with open(error_log_path, 'w', encoding=\"utf-8\") as error_log:\n                error_log.write(str(e))\n            print(e)\n\n\n\n"}
{"type": "source_file", "path": "agentstore/utils/server_config.py", "content": "import os\n\nclass ConfigManager:\n    \"\"\"\n    A singleton class responsible for managing configuration settings across the application.\n\n    This class implements the singleton design pattern to ensure that only one instance of the\n    ConfigManager exists at any time. It provides methods to set, apply, and clear proxy settings\n    for HTTP and HTTPS traffic.\n\n    Attributes:\n        _instance (ConfigManager): A private class-level attribute that holds the singleton instance.\n        http_proxy (str): The HTTP proxy URL.\n        https_proxy (str): The HTTPS proxy URL.\n    \"\"\"\n    _instance = None\n\n    def __new__(cls):\n        \"\"\"\n        Overrides the default instantiation process to ensure only one instance of ConfigManager is created.\n\n        Returns:\n            ConfigManager: The singleton instance of the ConfigManager.\n        \"\"\"\n        if cls._instance is None:\n            cls._instance = super(ConfigManager, cls).__new__(cls)\n            cls._instance.http_proxy = \"http://127.0.0.1:10809\"\n            cls._instance.https_proxy = \"http://127.0.0.1:10809\"\n            # cls._instance.http_proxy = None\n            # cls._instance.https_proxy = None\n        return cls._instance\n\n    def set_proxies(self, http, https):\n        \"\"\"\n        Sets the HTTP and HTTPS proxy URLs.\n\n        Args:\n            http (str): The HTTP proxy URL.\n            https (str): The HTTPS proxy URL.\n        \"\"\"\n        self.http_proxy = http\n        self.https_proxy = https\n\n    def apply_proxies(self):\n        \"\"\"\n        Applies the configured proxy settings by setting them in the environments variables.\n\n        The method sets the 'http_proxy' and 'https_proxy' environments variables based on the\n        configured proxy URLs. If no proxies are configured, the environments variables are not modified.\n        \"\"\"\n        if self.http_proxy:\n            os.environ[\"http_proxy\"] = self.http_proxy\n        if self.https_proxy:\n            os.environ[\"https_proxy\"] = self.https_proxy\n\n    def clear_proxies(self):\n        \"\"\"\n        Clears the proxy settings from the environments variables.\n\n        This method removes the 'http_proxy' and 'https_proxy' entries from the environments variables,\n        effectively clearing any proxy settings that were previously applied.\n        \"\"\"\n        os.environ.pop(\"http_proxy\", None)\n        os.environ.pop(\"https_proxy\", None)\n\n"}
{"type": "source_file", "path": "agentstore/utils/__init__.py", "content": "from .config import *\nfrom .utils import *\nfrom .schema import *\n"}
{"type": "source_file", "path": "agentstore/utils/parse_obs.py", "content": "# from https://github.com/xlang-ai/OSWorld/blob/97b567a287e036744f6868957fabf7d38f072bf2/mm_agents/agent.py#L1\n\n\n\nimport base64\nimport json\nimport logging\nimport os\nimport re\nimport time\nimport uuid\nimport xml.etree.ElementTree as ET\nimport tiktoken\nfrom io import BytesIO\nfrom typing import Dict, List, Tuple\n\nfrom PIL import Image, ImageDraw, ImageFont\nimport requests\n\ndef find_leaf_nodes(xlm_file_str):\n    if not xlm_file_str:\n        return []\n\n    root = ET.fromstring(xlm_file_str)\n\n    # Recursive function to traverse the XML tree and collect leaf nodes\n    def collect_leaf_nodes(node, leaf_nodes):\n        # If the node has no children, it is a leaf node, add it to the list\n        if not list(node):\n            leaf_nodes.append(node)\n        # If the node has children, recurse on each child\n        for child in node:\n            collect_leaf_nodes(child, leaf_nodes)\n\n    # List to hold all leaf nodes\n    leaf_nodes = []\n    collect_leaf_nodes(root, leaf_nodes)\n    return leaf_nodes\n\nstate_ns = \"uri:deskat:state.at-spi.gnome.org\"\ncomponent_ns = \"uri:deskat:component.at-spi.gnome.org\"\ndef judge_node(node: ET, platform=\"ubuntu\", check_image=False) -> bool:\n    if platform == \"ubuntu\":\n        _state_ns = state_ns_ubuntu\n        _component_ns = component_ns_ubuntu\n    elif platform == \"windows\":\n        _state_ns = state_ns_windows\n        _component_ns = component_ns_windows\n    else:\n        raise ValueError(\"Invalid platform, must be 'ubuntu' or 'windows'\")\n\n    keeps: bool = node.tag.startswith(\"document\") \\\n                  or node.tag.endswith(\"item\") \\\n                  or node.tag.endswith(\"button\") \\\n                  or node.tag.endswith(\"heading\") \\\n                  or node.tag.endswith(\"label\") \\\n                  or node.tag.endswith(\"scrollbar\") \\\n                  or node.tag.endswith(\"searchbox\") \\\n                  or node.tag.endswith(\"textbox\") \\\n                  or node.tag.endswith(\"link\") \\\n                  or node.tag.endswith(\"tabelement\") \\\n                  or node.tag.endswith(\"textfield\") \\\n                  or node.tag.endswith(\"textarea\") \\\n                  or node.tag.endswith(\"menu\") \\\n                  or node.tag in {\"alert\", \"canvas\", \"check-box\"\n                      , \"combo-box\", \"entry\", \"icon\"\n                      , \"image\", \"paragraph\", \"scroll-bar\"\n                      , \"section\", \"slider\", \"static\"\n                      , \"table-cell\", \"terminal\", \"text\"\n                      , \"netuiribbontab\", \"start\", \"trayclockwclass\"\n                      , \"traydummysearchcontrol\", \"uiimage\", \"uiproperty\"\n                      , \"uiribboncommandbar\"\n                                  }\n    keeps = keeps and (\n            platform == \"ubuntu\"\n            and node.get(\"{{{:}}}showing\".format(_state_ns), \"false\") == \"true\"\n            and node.get(\"{{{:}}}visible\".format(_state_ns), \"false\") == \"true\"\n            or platform == \"windows\"\n            and node.get(\"{{{:}}}visible\".format(_state_ns), \"false\") == \"true\"\n    ) \\\n            and (\n                    node.get(\"{{{:}}}enabled\".format(_state_ns), \"false\") == \"true\"\n                    or node.get(\"{{{:}}}editable\".format(_state_ns), \"false\") == \"true\"\n                    or node.get(\"{{{:}}}expandable\".format(_state_ns), \"false\") == \"true\"\n                    or node.get(\"{{{:}}}checkable\".format(_state_ns), \"false\") == \"true\"\n            ) \\\n            and (\n                    node.get(\"name\", \"\") != \"\" or node.text is not None and len(node.text) > 0 \\\n                    or check_image and node.get(\"image\", \"false\") == \"true\"\n            )\n\n    coordinates: Tuple[int, int] = eval(node.get(\"{{{:}}}screencoord\".format(_component_ns), \"(-1, -1)\"))\n    sizes: Tuple[int, int] = eval(node.get(\"{{{:}}}size\".format(_component_ns), \"(-1, -1)\"))\n    keeps = keeps and coordinates[0] >= 0 and coordinates[1] >= 0 and sizes[0] > 0 and sizes[1] > 0\n    return keeps\n\ndef filter_nodes(root: ET, platform=\"ubuntu\", check_image=False):\n    filtered_nodes = []\n\n    for node in root.iter():\n        if judge_node(node, platform, check_image):\n            filtered_nodes.append(node)\n            #print(ET.tostring(node, encoding=\"unicode\"))\n\n    return filtered_nodes\n\n\ndef draw_bounding_boxes(nodes, image_file_path, output_image_file_path, down_sampling_ratio=1.0):\n    # Load the screenshot image\n    image = Image.open(image_file_path)\n    if float(down_sampling_ratio) != 1.0:\n        image = image.resize((int(image.size[0] * down_sampling_ratio), int(image.size[1] * down_sampling_ratio)))\n    draw = ImageDraw.Draw(image)\n    marks = []\n    drew_nodes = []\n    text_informations: List[str] = [\"index\\ttag\\tname\\ttext\"]\n\n    try:\n        # Adjust the path to the font file you have or use a default one\n        font = ImageFont.truetype(\"arial.ttf\", 15)\n    except IOError:\n        # Fallback to a basic font if the specified font can't be loaded\n        font = ImageFont.load_default()\n\n    index = 1\n\n    # Loop over all the visible nodes and draw their bounding boxes\n    for _node in nodes:\n        coords_str = _node.attrib.get('{uri:deskat:component.at-spi.gnome.org}screencoord')\n        size_str = _node.attrib.get('{uri:deskat:component.at-spi.gnome.org}size')\n\n        if coords_str and size_str:\n            try:\n                # Parse the coordinates and size from the strings\n                coords = tuple(map(int, coords_str.strip('()').split(', ')))\n                size = tuple(map(int, size_str.strip('()').split(', ')))\n\n                import copy\n                original_coords = copy.deepcopy(coords)\n                original_size = copy.deepcopy(size)\n\n                if float(down_sampling_ratio) != 1.0:\n                    # Downsample the coordinates and size\n                    coords = tuple(int(coord * down_sampling_ratio) for coord in coords)\n                    size = tuple(int(s * down_sampling_ratio) for s in size)\n\n                # Check for negative sizes\n                if size[0] <= 0 or size[1] <= 0:\n                    raise ValueError(f\"Size must be positive, got: {size}\")\n\n                # Calculate the bottom-right corner of the bounding box\n                bottom_right = (coords[0] + size[0], coords[1] + size[1])\n\n                # Check that bottom_right > coords (x1 >= x0, y1 >= y0)\n                if bottom_right[0] < coords[0] or bottom_right[1] < coords[1]:\n                    raise ValueError(f\"Invalid coordinates or size, coords: {coords}, size: {size}\")\n\n                # Check if the area only contains one color\n                cropped_image = image.crop((*coords, *bottom_right))\n                if len(set(list(cropped_image.getdata()))) == 1:\n                    continue\n\n                # Draw rectangle on image\n                draw.rectangle([coords, bottom_right], outline=\"red\", width=1)\n\n                # Draw index number at the bottom left of the bounding box with black background\n                text_position = (coords[0], bottom_right[1])  # Adjust Y to be above the bottom right\n                text_bbox: Tuple[int, int ,int ,int] = draw.textbbox(text_position, str(index), font=font, anchor=\"lb\")\n                #offset: int = bottom_right[1]-text_bbox[3]\n                #text_bbox = (text_bbox[0], text_bbox[1]+offset, text_bbox[2], text_bbox[3]+offset)\n\n                #draw.rectangle([text_position, (text_position[0] + 25, text_position[1] + 18)], fill='black')\n                draw.rectangle(text_bbox, fill='black')\n                draw.text(text_position, str(index), font=font, anchor=\"lb\", fill=\"white\")\n\n                # each mark is an x, y, w, h tuple\n                marks.append([original_coords[0], original_coords[1], original_size[0], original_size[1]])\n                drew_nodes.append(_node)\n\n                if _node.text:\n                    node_text = ( _node.text if '\"' not in _node.text\\\n                             else '\"{:}\"'.format(_node.text.replace('\"', '\"\"'))\n                                )\n                elif _node.get(\"{uri:deskat:uia.windows.microsoft.org}class\", \"\").endswith(\"EditWrapper\") \\\n                        and _node.get(\"{uri:deskat:value.at-spi.gnome.org}value\"):\n                    node_text: str = _node.get(\"{uri:deskat:value.at-spi.gnome.org}value\")\n                    node_text = (node_text if '\"' not in node_text\\\n                             else '\"{:}\"'.format(node_text.replace('\"', '\"\"'))\n                                )\n                else:\n                    node_text = '\"\"'\n                text_information: str = \"{:d}\\t{:}\\t{:}\\t{:}\"\\\n                                            .format( index, _node.tag\n                                                   , _node.get(\"name\", \"\")\n                                                   , node_text\n                                                   )\n                text_informations.append(text_information)\n\n                index += 1\n\n            except ValueError:\n                pass\n\n    # Save the result\n    image.save(output_image_file_path)\n    return marks, drew_nodes, \"\\n\".join(text_informations)\n\n\ndef print_nodes_with_indent(nodes, indent=0):\n    for node in nodes:\n        print(' ' * indent, node.tag, node.attrib)\n        print_nodes_with_indent(node, indent + 2)\n\n\n# Function to encode the image\ndef encode_image(image_content):\n    return base64.b64encode(image_content).decode('utf-8')\n\n\nattributes_ns_ubuntu = \"https://accessibility.windows.example.org/ns/attributes\"\nattributes_ns_windows = \"https://accessibility.windows.example.org/ns/attributes\"\nstate_ns_ubuntu = \"https://accessibility.ubuntu.example.org/ns/state\"\nstate_ns_windows = \"https://accessibility.windows.example.org/ns/state\"\ncomponent_ns_ubuntu = \"https://accessibility.ubuntu.example.org/ns/component\"\ncomponent_ns_windows = \"https://accessibility.windows.example.org/ns/component\"\nvalue_ns_ubuntu = \"https://accessibility.ubuntu.example.org/ns/value\"\nvalue_ns_windows = \"https://accessibility.windows.example.org/ns/value\"\nclass_ns_windows = \"https://accessibility.windows.example.org/ns/class\"\n# More namespaces defined in OSWorld, please check desktop_env/server/main.py\n\ndef linearize_accessibility_tree(accessibility_tree, platform=\"ubuntu\"):\n\n    if platform == \"ubuntu\":\n        _attributes_ns = attributes_ns_ubuntu\n        _state_ns = state_ns_ubuntu\n        _component_ns = component_ns_ubuntu\n        _value_ns = value_ns_ubuntu\n    elif platform == \"windows\":\n        _attributes_ns = attributes_ns_windows\n        _state_ns = state_ns_windows\n        _component_ns = component_ns_windows\n        _value_ns = value_ns_windows\n    else:\n        raise ValueError(\"Invalid platform, must be 'ubuntu' or 'windows'\")\n\n    filtered_nodes = filter_nodes(ET.fromstring(accessibility_tree), platform)\n    linearized_accessibility_tree = [\"tag\\tname\\ttext\\tclass\\tdescription\\tposition (top-left x&y)\\tsize (w&h)\"]\n\n    # Linearize the accessibility tree nodes into a table format\n    for node in filtered_nodes:\n        if node.text:\n            text = (\n                node.text if '\"' not in node.text \\\n                    else '\"{:}\"'.format(node.text.replace('\"', '\"\"'))\n            )\n\n        elif node.get(\"{{{:}}}class\".format(class_ns_windows), \"\").endswith(\"EditWrapper\") \\\n                and node.get(\"{{{:}}}value\".format(_value_ns)):\n            node_text = node.get(\"{{{:}}}value\".format(_value_ns), \"\")\n            text = (node_text if '\"' not in node_text \\\n                        else '\"{:}\"'.format(node_text.replace('\"', '\"\"'))\n                    )\n        else:\n            text = '\"\"'\n\n        linearized_accessibility_tree.append(\n            \"{:}\\t{:}\\t{:}\\t{:}\\t{:}\".format(\n                node.tag, node.get(\"name\", \"\"),\n                text,\n                node.get('{{{:}}}screencoord'.format(_component_ns), \"\"),\n                node.get('{{{:}}}size'.format(_component_ns), \"\")\n            )\n        )\n\n    return \"\\n\".join(linearized_accessibility_tree)\n\n# def linearize_accessibility_tree(accessibility_tree, platform=\"ubuntu\"):\n#     # leaf_nodes = find_leaf_nodes(accessibility_tree)\n#     filtered_nodes = filter_nodes(ET.fromstring(accessibility_tree), platform)\n\n#     linearized_accessibility_tree = [\"tag\\tname\\ttext\\tposition (top-left x&y)\\tsize (w&h)\"]\n#     # Linearize the accessibility tree nodes into a table format\n\n#     for node in filtered_nodes:\n#         # linearized_accessibility_tree += node.tag + \"\\t\"\n#         # linearized_accessibility_tree += node.attrib.get('name') + \"\\t\"\n#         if node.text:\n#             text = (node.text if '\"' not in node.text \\\n#                         else '\"{:}\"'.format(node.text.replace('\"', '\"\"'))\n#                     )\n#         elif node.get(\"{uri:deskat:uia.windows.microsoft.org}class\", \"\").endswith(\"EditWrapper\") \\\n#                 and node.get(\"{uri:deskat:value.at-spi.gnome.org}value\"):\n#             text: str = node.get(\"{uri:deskat:value.at-spi.gnome.org}value\")\n#             text = (text if '\"' not in text \\\n#                         else '\"{:}\"'.format(text.replace('\"', '\"\"'))\n#                     )\n#         else:\n#             text = '\"\"'\n#         # linearized_accessibility_tree += node.attrib.get(\n#         # , \"\") + \"\\t\"\n#         # linearized_accessibility_tree += node.attrib.get('{uri:deskat:component.at-spi.gnome.org}size', \"\") + \"\\n\"\n#         linearized_accessibility_tree.append(\n#             \"{:}\\t{:}\\t{:}\\t{:}\\t{:}\".format(\n#                 node.tag, node.get(\"name\", \"\"), text\n#                 , node.get('{uri:deskat:component.at-spi.gnome.org}screencoord', \"\")\n#                 , node.get('{uri:deskat:component.at-spi.gnome.org}size', \"\")\n#             )\n#         )\n\n#     return \"\\n\".join(linearized_accessibility_tree)\n\ndef parse_ui_data_dict(data):\n    # 将输入字符串分割成单独的行\n    lines = data.strip().split('\\n')\n    \n    # 初始化一个字典来存储结果\n    ui_dict = {}\n    \n    # 遍历数据的每一行，跳过标题行\n    for line in lines[1:]:\n        # 分割每一行的字段，这里假设字段之间由制表符'\\t'分隔\n        parts = line.split('\\t')\n        if len(parts) < 4:\n            continue\n        # 获取name和position字段\n        name = parts[1].strip()\n        position = parts[3].strip('()')  # 移除位置坐标周围的括号\n        \n        # 将name和position添加到字典中\n        if name:  # 确保name非空\n            x,y = position.replace('(', '').replace(')', '').split(',')\n            new_x = int(x) + 10\n            new_y = int(y) + 10\n            # 格式化回字符串\n            ui_dict[name] = f'({new_x}, {new_y})'\n    return json.dumps(ui_dict)\n\n\ndef tag_screenshot(screenshot, accessibility_tree, platform=\"ubuntu\"):\n    # Creat a tmp file to store the screenshot in random name\n    uuid_str = str(uuid.uuid4())\n    os.makedirs(\"tmp/images\", exist_ok=True)\n    tagged_screenshot_file_path = os.path.join(\"tmp/images\", uuid_str + \".png\")\n    # nodes = filter_nodes(find_leaf_nodes(accessibility_tree))\n    nodes = filter_nodes(ET.fromstring(accessibility_tree), platform=platform, check_image=True)\n    # Make tag screenshot\n    marks, drew_nodes, element_list = draw_bounding_boxes(nodes, screenshot, tagged_screenshot_file_path)\n\n    return marks, drew_nodes, tagged_screenshot_file_path, element_list\n\n\ndef trim_accessibility_tree(linearized_accessibility_tree, max_tokens):\n    enc = tiktoken.encoding_for_model(\"gpt-4\")\n    tokens = enc.encode(linearized_accessibility_tree)\n    if len(tokens) > max_tokens:\n        linearized_accessibility_tree = enc.decode(tokens[:max_tokens])\n        linearized_accessibility_tree += \"[...]\\n\"\n    return linearized_accessibility_tree\n\n\ndef ocr(base64_image, ui_dict):\n\n\n    request_url = \"https://aip.baidubce.com/rest/2.0/ocr/v1/accurate\"\n    request_url = \"https://aip.baidubce.com/rest/2.0/ocr/v1/general\"\n\n\n    params = {\"image\": base64_image, 'probability': 'true'}\n    access_token = '24.3e11a3856ca146627d4e7c4555a384d8.2592000.1726904372.282335-109280435'\n    request_url = request_url + \"?access_token=\" + access_token\n    headers = {'content-type': 'application/x-www-form-urlencoded'}\n    response = requests.post(request_url, data=params, headers=headers)\n    if not response:\n        return ui_dict\n    \n    dict_ = json.loads(ui_dict)\n\n    data = response.json()['words_result']\n\n    for item in data:\n        if item['probability']['average'] < 0.85:\n            continue\n        location = item['location']\n        top = location['top']\n        left = location['left']\n        width = location['width']\n        height = location['height']\n        x = left + width // 2\n        y = top + height // 2\n        \n        dict_[item['words']] = f'({x}, {y})'\n\n    ui_dict = json.loads(ui_dict)\n\n    ui_dict = ui_dict | dict_\n\n    return json.dumps(ui_dict)\n\ndef parse_obs(obs, observation_type, ocr_type=True):\n    a11y_tree_max_tokens = 3000\n    if observation_type in [\"screenshot\", \"screenshot_a11y_tree\"]:\n        base64_image = encode_image(obs[\"screenshot\"])\n        linearized_accessibility_tree = parse_ui_data_dict(linearize_accessibility_tree(accessibility_tree=obs[\"accessibility_tree\"]\n                                                                        )) if observation_type == \"screenshot_a11y_tree\" else None\n\n        if ocr_type:\n            linearized_accessibility_tree = ocr(base64_image, linearized_accessibility_tree)\n\n        if linearized_accessibility_tree:\n            linearized_accessibility_tree = trim_accessibility_tree(linearized_accessibility_tree,\n                                                                    a11y_tree_max_tokens)  \n\n        obs[\"base64_image\"] = base64_image\n        obs[\"linearized_accessibility_tree\"] = linearized_accessibility_tree\n\n\n    \n    elif observation_type == \"a11y_tree\":\n        linearized_accessibility_tree = parse_ui_data_dict(linearize_accessibility_tree(accessibility_tree=obs[\"accessibility_tree\"]\n                                                                        ))\n        if linearized_accessibility_tree:\n            linearized_accessibility_tree = trim_accessibility_tree(linearized_accessibility_tree,\n                                                                    a11y_tree_max_tokens)\n        obs[\"linearized_accessibility_tree\"] = linearized_accessibility_tree\n    \n    elif observation_type == \"som\":\n        # Add som to the screenshot\n        masks, drew_nodes, tagged_screenshot, linearized_accessibility_tree = tag_screenshot(obs[\"screenshot\"], obs[\n            \"accessibility_tree\"], platform)\n        base64_image = encode_image(tagged_screenshot)\n\n        if linearized_accessibility_tree:\n            linearized_accessibility_tree = trim_accessibility_tree(linearized_accessibility_tree,\n                                                                    a11y_tree_max_tokens)\n        obs[\"base64_image\"] = base64_image\n        obs[\"linearized_accessibility_tree\"] = linearized_accessibility_tree\n    \n    return obs\n"}
{"type": "source_file", "path": "agentstore/utils/utils.py", "content": "import copy\nimport numpy as np\nimport itertools\nimport json\nimport logging\nimport os\nimport re\nimport string\nfrom typing import Any\nimport tqdm\nimport re\nimport tiktoken\nimport random\nfrom datasets import load_dataset\nfrom agentstore.prompts.general_pt import prompt as general_pt\nfrom agentstore.utils.llms import OpenAI\nimport platform\nfrom functools import wraps\n\n\ndef random_string(length):\n    \"\"\"\n    Generates a random string of a specified length.\n\n    Args:\n        length (int): The desired length of the random string.\n\n    Returns:\n        str: A string of random characters and digits of the specified length.\n    \"\"\"\n    characters = string.ascii_letters + string.digits\n    random_string = ''.join(random.choice(characters) for _ in range(length))\n    return random_string\n\n\ndef num_tokens_from_string(string: str) -> int:\n    \"\"\"\n    Calculates the number of tokens in a given text string according to a specific encoding.\n\n    Args:\n        text (str): The text string to be tokenized.\n\n    Returns:\n        int: The number of tokens the string is encoded into according to the model's tokenizer.\n    \"\"\"\n    encoding = tiktoken.encoding_for_model('gpt-4-1106-preview')\n    num_tokens = len(encoding.encode(string))\n    return num_tokens\n\n\ndef parse_content(content, html_type=\"html.parser\"):\n    \"\"\"\n    Parses and cleans the given HTML content, removing specified tags, ids, and classes.\n\n    Args:\n        content (str): The HTML content to be parsed and cleaned.\n        type (str, optional): The type of parser to be used by BeautifulSoup. Defaults to \"html.parser\".\n            Supported types include \"html.parser\", \"lxml\", \"lxml-xml\", \"xml\", and \"html5lib\".\n\n    Raises:\n        ValueError: If an unsupported parser type is specified.\n\n    Returns:\n        str: The cleaned text extracted from the HTML content.\n    \"\"\"\n    implemented = [\"html.parser\", \"lxml\", \"lxml-xml\", \"xml\", \"html5lib\"]\n    if html_type not in implemented:\n        raise ValueError(f\"Parser type {html_type} not implemented. Please choose one of {implemented}\")\n\n    from bs4 import BeautifulSoup\n\n    soup = BeautifulSoup(content, html_type)\n    original_size = len(str(soup.get_text()))\n\n    tags_to_exclude = [\n        \"nav\",\n        \"aside\",\n        \"form\",\n        \"header\",\n        \"noscript\",\n        \"svg\",\n        \"canvas\",\n        \"footer\",\n        \"script\",\n        \"style\",\n    ]\n    for tag in soup(tags_to_exclude):\n        tag.decompose()\n\n    ids_to_exclude = [\"sidebar\", \"main-navigation\", \"menu-main-menu\"]\n    for id in ids_to_exclude:\n        tags = soup.find_all(id=id)\n        for tag in tags:\n            tag.decompose()\n\n    classes_to_exclude = [\n        \"elementor-location-header\",\n        \"navbar-header\",\n        \"nav\",\n        \"header-sidebar-wrapper\",\n        \"blog-sidebar-wrapper\",\n        \"related-posts\",\n    ]\n    for class_name in classes_to_exclude:\n        tags = soup.find_all(class_=class_name)\n        for tag in tags:\n            tag.decompose()\n\n    content = soup.get_text()\n    content = clean_string(content)\n\n    cleaned_size = len(content)\n    if original_size != 0:\n        logging.info(\n            f\"Cleaned page size: {cleaned_size} characters, down from {original_size} (shrunk: {original_size-cleaned_size} chars, {round((1-(cleaned_size/original_size)) * 100, 2)}%)\"  # noqa:E501\n        )\n\n    return content\n\n\ndef clean_string(text):\n    \"\"\"\n    Cleans a given string by performing various operations such as whitespace normalization,\n    removal of backslashes, and replacement of hash characters with spaces. It also reduces\n    consecutive non-alphanumeric characters to a single occurrence.\n\n    Args:\n        text (str): The text to be cleaned.\n\n    Returns:\n        str: The cleaned text after applying all the specified cleaning operations.\n    \"\"\"\n    # Replacement of newline characters:\n    text = text.replace(\"\\n\", \" \")\n\n    # Stripping and reducing multiple spaces to single:\n    cleaned_text = re.sub(r\"\\s+\", \" \", text.strip())\n\n    # Removing backslashes:\n    cleaned_text = cleaned_text.replace(\"\\\\\", \"\")\n\n    # Replacing hash characters:\n    cleaned_text = cleaned_text.replace(\"#\", \" \")\n\n    # Eliminating consecutive non-alphanumeric characters:\n    # This regex identifies consecutive non-alphanumeric characters (i.e., not\n    # a word character [a-zA-Z0-9_] and not a whitespace) in the string\n    # and replaces each group of such characters with a single occurrence of\n    # that character.\n    # For example, \"!!! hello !!!\" would become \"! hello !\".\n    cleaned_text = re.sub(r\"([^\\w\\s])\\1*\", r\"\\1\", cleaned_text)\n\n    return cleaned_text\n\n\ndef is_readable(s):\n    \"\"\"\n    Heuristic to determine if a string is \"readable\" (mostly contains printable characters and forms meaningful words)\n\n    :param s: string\n    :return: True if the string is more than 95% printable.\n    \"\"\"\n    try:\n        printable_ratio = sum(c in string.printable for c in s) / len(s)\n    except ZeroDivisionError:\n        logging.warning(\"Empty string processed as unreadable\")\n        printable_ratio = 0\n    return printable_ratio > 0.95  # 95% of characters are printable\n\n\ndef format_source(source: str, limit: int = 20) -> str:\n    \"\"\"\n    Format a string to only take the first x and last x letters.\n    This makes it easier to display a URL, keeping familiarity while ensuring a consistent length.\n    If the string is too short, it is not sliced.\n    \"\"\"\n    if len(source) > 2 * limit:\n        return source[:limit] + \"...\" + source[-limit:]\n    return source\n\n\ndef is_valid_json_string(source: str):\n    \"\"\"\n    Checks if a given string is a valid JSON.\n    \n    Args:\n        source (str): The string to be validated as JSON.\n\n    Returns:\n        bool: True if the given string is a valid JSON format, False otherwise.\n    \"\"\"\n    try:\n        _ = json.loads(source)\n        return True\n    except json.JSONDecodeError:\n        logging.error(\n            \"Insert valid string format of JSON. \\\n            Check the docs to see the supported formats - `https://docs.embedchain.ai/data-sources/json`\"\n        )\n        return False\n\n\ndef chunks(iterable, batch_size=100, desc=\"Processing chunks\"):\n    \"\"\"\n    Breaks an iterable into smaller chunks of a specified size, yielding each chunk in sequence.\n\n    Args:\n        iterable (iterable): The iterable to be chunked.\n        batch_size (int, optional): The size of each chunk. Defaults to 100.\n        desc (str, optional): Description text to be displayed alongside the progress bar. Defaults to \"Processing chunks\".\n\n    Yields:\n        tuple: A chunk of the iterable, with a maximum length of `batch_size`.\n    \"\"\"\n    it = iter(iterable)\n    total_size = len(iterable)\n\n    with tqdm(total=total_size, desc=desc, unit=\"batch\") as pbar:\n        chunk = tuple(itertools.islice(it, batch_size))\n        while chunk:\n            yield chunk\n            pbar.update(len(chunk))\n            chunk = tuple(itertools.islice(it, batch_size))\n\n\ndef generate_prompt(template: str, replace_dict: dict):\n    \"\"\"\n    Generates a string by replacing placeholders in a template with values from a dictionary.\n\n    Args:\n        template (str): The template string containing placeholders to be replaced.\n        replace_dict (dict): A dictionary where each key corresponds to a placeholder in the template\n                             and each value is the replacement for that placeholder.\n\n    Returns:\n        str: The resulting string after all placeholders have been replaced with their corresponding values.\n    \"\"\"\n    prompt = copy.deepcopy(template)\n    for k, v in replace_dict.items():\n        prompt = prompt.replace(k, str(v))\n    return prompt\n\n\ndef cosine_similarity(a, b):\n    \"\"\"\n    Calculates the cosine similarity between two vectors.\n\n    Args:\n        a (array_like): The first vector.\n        b (array_like): The second vector.\n\n    Returns:\n        float: The cosine similarity between vectors `a` and `b`.\n    \"\"\"\n    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\\\n    \n\ndef send_chat_prompts(sys_prompt, user_prompt, llm):\n    \"\"\"\n    Sends a sequence of chat prompts to a language learning model (LLM) and returns the model's response.\n\n    Args:\n        sys_prompt (str): The system prompt that sets the context or provides instructions for the language learning model.\n        user_prompt (str): The user prompt that contains the specific query or command intended for the language learning model.\n        llm (object): The language learning model to which the prompts are sent. This model is expected to have a `chat` method that accepts structured prompts.\n\n    Returns:\n        The response from the language learning model, which is typically a string containing the model's answer or generated content based on the provided prompts.\n\n    The function is a utility for simplifying the process of sending structured chat prompts to a language learning model and parsing its response, useful in scenarios where dynamic interaction with the model is required.\n    \"\"\"\n    message = [\n            {\"role\": \"system\", \"content\": sys_prompt},\n            {\"role\": \"user\", \"content\": user_prompt},\n        ]\n    return llm.chat(message)\n\n\ndef get_project_root_path():\n    \"\"\"\n    This function returns the absolute path of the project root directory. It assumes that it is being called from a file located in oscopilot/utils/.\n    \n    Args:\n        None\n    \n    Returns:\n        str: The absolute path of the project root directory.\n    \"\"\"\n    script_path = os.path.abspath(__file__)\n\n    # Get the directory of the script (oscopilot/utils)\n    script_directory = os.path.dirname(script_path)\n\n    # Get the parent directory of script_directory (oscopilot)\n    oscopilot_directory = os.path.dirname(script_directory)\n\n    # Get the project root directory\n    project_root_path = os.path.dirname(oscopilot_directory)\n\n    return project_root_path + '/'\n\n\ndef GAIA_postprocess(question, response):\n    llm = OpenAI()\n    extractor_prompt = general_pt['GAIA_ANSWER_EXTRACTOR_PROMPT'].format(\n        question=question,\n        response=response\n    )\n    result = send_chat_prompts('', extractor_prompt, llm)\n    return result\n\n\nclass GAIALoader:\n    def __init__(self, level=1, cache_dir=None):\n        if cache_dir != None:\n            assert os.path.exists(cache_dir), f\"Cache directory {cache_dir} does not exist.\"\n            self.cache_dir = cache_dir\n            try:\n                self.dataset = load_dataset(\"gaia-benchmark/GAIA\", \"2023_level{}\".format(level), cache_dir=self.cache_dir)\n            except Exception as e:\n                raise Exception(f\"Failed to load GAIA dataset: {e}\")\n        else:\n            self.dataset = load_dataset(\"gaia-benchmark/GAIA\", \"2023_level{}\".format(level))\n            \n        \n    def get_data_by_task_id(self, task_id, dataset_type):\n        if self.dataset is None or dataset_type not in self.dataset:\n            raise ValueError(\"Dataset not loaded or data set not available.\")\n\n        data_set = self.dataset[dataset_type]\n        for record in data_set:\n            if record['task_id'] == task_id:\n                return record\n        return None\n\n    def task2query(self, task):\n        query = 'Your task is: {}'.format(task['Question'])\n        if task['file_name'] != '':\n            query = query + '\\n{0} is the absolute file path you need to use, and the file type is {1}. Note that there is no file extension at the end.'.format(task['file_path'], task['file_name'].split('.')[-1])\n        print('GAIA Task {0}:\\n{1}'.format(task['task_id'], query))\n        logging.info(query)\n        return query\n    \nclass SheetTaskLoader:\n    def __init__(self, sheet_task_path=None):\n        if sheet_task_path != None:\n            assert os.path.exists(sheet_task_path), f\"Sheet task jsonl file {sheet_task_path} does not exist.\"\n            self.sheet_task_path = sheet_task_path\n            try:\n                self.dataset = self.load_sheet_task_dataset()\n            except Exception as e:\n                raise Exception(f\"Failed to load sheet task dataset: {e}\")\n        else:\n            print(\"Sheet task jsonl file not provided.\")\n\n\n    def load_sheet_task_dataset(self):\n        dataset = []\n        with open(self.sheet_task_path, 'r') as file:\n            for _, line in enumerate(file):\n                task_info = json.loads(line)\n                query = self.task2query(task_info['Context'], task_info['Instructions'], get_project_root_path() + task_info['file_path'])\n                dataset.append(query)\n        return dataset\n\n    def task2query(self, context, instructions, file_path):\n        SHEET_TASK_PROMPT = \"\"\"You are an expert in handling excel file. {context}\n                               Your task is: {instructions}\n                               The file path of the excel is: {file_path}. Every subtask's description must include the file path, and all subtasks are completed on the file at that path.\n                            \"\"\"\n        query = SHEET_TASK_PROMPT.format(context=context, instructions=instructions, file_path=file_path)\n        return query\n    \n    def get_data_by_task_id(self, task_id):\n        if self.dataset is None:\n            raise ValueError(\"Dataset not loaded.\")\n        return self.dataset[task_id]\n\n\ndef get_os_version():\n    \"\"\"\n    Determines the operating system version of the current system.\n\n    This function checks the operating system of the current environments and attempts\n    to return a human-readable version string. For macOS, it uses the `platform.mac_ver()`\n    method. For Linux, it attempts to read the version information from `/etc/os-release`.\n    If the system is not macOS or Linux, or if the Linux version cannot be determined, it\n    defaults to a generic version string or \"Unknown Operating System\".\n\n    Returns:\n        str: A string describing the operating system version, or \"Unknown Operating System\"\n             if the version cannot be determined.\n    \"\"\"\n    system = platform.system()\n\n    if system == \"Darwin\":\n        # macOS\n        return 'macOS ' + platform.mac_ver()[0]\n    elif system == \"Linux\":\n        try:\n            with open(\"/etc/os-release\") as f:\n                lines = f.readlines()\n                for line in lines:\n                    if line.startswith(\"PRETTY_NAME\"):\n                        return line.split(\"=\")[1].strip().strip('\"')\n        except FileNotFoundError:\n            pass\n\n        return platform.version()\n    else:\n        return \"Unknown Operating System\"\n\n\ndef check_os_version(s):\n    \"\"\"\n    Checks if the operating system version string matches known supported versions.\n\n    This function examines a given operating system version string to determine if it\n    contains known substrings that indicate support (e.g., \"mac\", \"Ubuntu\", \"CentOS\").\n    If the version string does not match any of the known supported versions, it raises\n    a ValueError.\n\n    Args:\n        s (str): The operating system version string to check.\n\n    Raises:\n        ValueError: If the operating system version is not recognized as a known\n                    supported version.\n    \"\"\"\n    if \"mac\" in s or \"Ubuntu\" in s or \"CentOS\" in s:\n        print(\"Operating System Version:\", s)\n    else:\n        raise ValueError(\"Unknown Operating System\")\n\n\ndef api_exception_mechanism(max_retries=3):\n    \"\"\"\n    A decorator to add a retry mechanism to functions, particularly for handling API calls.\n    This decorator will retry a function up to `max_retries` times if an exception is raised.\n\n    Args:\n    max_retries (int): The maximum number of retries allowed before giving up and re-raising the exception.\n\n    Returns:\n    function: A wrapper function that incorporates the retry mechanism.\n    \"\"\"\n    def decorator(func):\n        \"\"\"\n        The actual decorator that takes a function and applies the retry logic to it.\n\n        Args:\n        func (function): The function to which the retry mechanism will be applied.\n\n        Returns:\n        function: The wrapped function with retry logic.\n        \"\"\"\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            \"\"\"\n            A wrapper function that executes the decorated function and handles exceptions by retrying.\n\n            Args:\n            *args: Variable length argument list for the decorated function.\n            **kwargs: Arbitrary keyword arguments for the decorated function.\n\n            Returns:\n            Any: The return value of the decorated function if successful.\n\n            Raises:\n            Exception: Re-raises any exception if the max retry limit is reached.\n            \"\"\"\n            attempts = 0\n            while attempts < max_retries:\n                try:\n                    return func(*args, **kwargs)\n                except Exception as e:\n                    attempts += 1\n                    logging.error(f\"Error on attempt {attempts} in {func.__name__}: {str(e)}\")\n                    if attempts == max_retries:\n                        logging.error(f\"Max retries reached in {func.__name__}, operation failed.\")\n                        raise\n        return wrapper\n    return decorator"}
{"type": "source_file", "path": "agentstore/modules/__init__.py", "content": "from .executor import *\nfrom .planner import *\nfrom .retriever import *\nfrom .learner import *"}
{"type": "source_file", "path": "agentstore/agents/word_agent.py", "content": "from agentstore.utils import setup_config, setup_pre_run\nfrom agentstore.modules.base_module import BaseModule\nimport re\nimport os\nimport json\nimport base64\nimport time\nfrom rich.console import Console\nfrom rich.markdown import Markdown\n\nfrom desktop_env.envs.desktop_env import DesktopEnv\n\nconsole = Console()\n\ndef encode_image(image_content):\n    return base64.b64encode(image_content).decode('utf-8')\n\ndef rich_print(markdown_text):\n    md = Markdown(markdown_text)\n    console.print(md)\n\n\ndef send_chat_prompts(message, llm):\n    return llm.chat(message)\n\n    \ndef extract_code(input_string):\n    pattern = r\"```(\\w+)?\\s*(.*?)```\"  # 匹配代码块，语言名称是可选项\n    matches = re.findall(pattern, input_string, re.DOTALL)\n\n    if matches:\n        language, code = matches[0]\n\n        # 如果没有语言信息，尝试从代码内容中检测\n        if not language:\n            if re.search(\"python\", code.lower()) or re.search(r\"import\\s+\\w+\", code):\n                language = \"python\"\n            elif re.search(\"bash\", code.lower()) or re.search(r\"echo\", code):\n                language = \"bash\"\n\n        if language == 'bash':\n            code = code.strip()\n            code = code.replace('\"', '\\\\\"')\n            code = code.replace('\\n', ' && ')\n            code = \"pyautogui.typewrite(\\\"{0}\\\", interval=0.05)\\npyautogui.press('enter')\\ntime.sleep(2)\".format(code)\n            if re.search(\"sudo\", code.lower()):\n                code += \"\\npyautogui.typewrite('password', interval=0.05)\\npyautogui.press('enter')\\ntime.sleep(1)\"\n        elif language == 'python':\n\n            # save code\n            with open('tmp.py', 'w') as f:\n                f.write(code)\n\n            code = \"pyautogui.typewrite(\\\"python main.py\\\", interval=0.05)\\npyautogui.press('enter')\\ntime.sleep(2)\"\n            \n        else:\n            raise language\n\n        return code, language\n    else:\n        return None, None\n\nfrom desktop_env.envs.desktop_env import DesktopEnv\nclass WordAgent(BaseModule):\n    info = {\n            \"name\": \"WordAgent\",\n            \"can do\": \"excels at identifying and manipulating Word documents using Python's python-docx library, can manage tasks involving document modification, data insertion, and formatting adjustments. Capable of detecting open Word or other documents using Bash commands.\",\n            \"can't do\": \"cannot handle GUI operations, cannot perform tasks outside the capabilities of the python-docx library such as directly interacting with embedded media and scripts within the documents. Additionally, cannot modify LibreOffice Writer software defaults or preferences.\",\n    }\n    def __init__(self, args, task_name, env):\n        super().__init__()\n        self.args = args\n\n        self.environment = env\n\n        self.task_name = task_name\n\n        self.reply = None\n    \n    def execute_tool(self, code, lang):\n        if lang == 'python':\n            file = [{\n                \"local_path\": 'tmp.py',\n                \"path\": '/home/user/main.py'\n              }]\n            self.environment.setup_controller._upload_file_setup(file)\n        obs, reward, done, info = self.environment.step(code)  # node_type\n        \n        reply = self.environment.controller.get_terminal_output()\n        # update reply\n        if self.reply and reply:\n            message_terminal = reply.replace(self.reply, \"\")\n        else:\n            message_terminal = reply\n        self.reply = reply\n\n        base64_image = encode_image(obs['screenshot'])\n        print(\"message_terminal\", message_terminal)\n        if 'gpt' in self.llm.model_name:\n            message = {\n                    \"role\": \"user\",\n                    \"content\": [\n                        {\n                            \"type\": \"text\",\n                            \"text\": \"After executing the command, the terminal output is {0}. the screenshot as below.\".format(message_terminal)\n                        }\n                        # {\n                        #     \"type\": \"image_url\",\n                        #     \"image_url\": {\n                        #         \"url\": f\"data:image/png;base64,{base64_image}\",\n                        #         \"detail\": \"high\"\n                        #     }\n                        # }\n                    ]\n            }\n        else:\n            message = {\n                    \"role\": \"user\",\n                    \"content\": \"After executing the command, the terminal output is {0}. the screenshot as below.\".format(message_terminal)\n            }\n        return message\n# When a user refers to a file opened， you can use the provided screenshot to find the filepath.\n    def run(self):\n\n        while not self.environment.controller.get_terminal_output():\n            self.environment.step(\"pyautogui.click()\")\n            self.environment.step(\"pyautogui.hotkey('ctrl', 'alt', 't')\")\n\n        light_planner_sys_prompt = '''You are DocAgent, an advanced programming assistant specializing in managing and manipulating Word documents. \n        Your capabilities include identifying open Word documents using Bash commands and utilizing Python's python-docx library to perform document manipulations.\nFirst, write a plan. **Always recap the plan between each code block** (you have extreme short-term memory loss, so you need to recap the plan between each message block to retain it).\nWhen you execute code, it will be executed **on the user's machine**. The user has given you **full and complete permission** to execute any code necessary to complete the task. Execute the code.\nIf you want to send data between programming languages, save the data to a txt or json.\nYou can access the internet. Run **any code** to achieve the goal, and if at first you don't succeed, try again and again.\nYou can install new packages.\nImportant: When a user refers to a file opened or a url, you can use your visual capacity or some command line tools such as \"lsof | grep -E '\\.odt|\\.docx'\" to see the path of the file being opened.\nFirst thing need to do is pip install python-docx.\nWrite messages to the user in Markdown.\nIn general, try to **make plans** with as few steps as possible. As for actually executing code to carry out that plan, for *stateful* languages (like python, javascript, shell, but NOT for html which starts from 0 every time) **it's critical not to try to do everything in one code block.** You should try something, print information about it, then continue from there in tiny, informed steps. You will never get it on the first try, and attempting it in one go will often lead to errors you cant see.\nYou are capable of **any** task.\nExample Command in Bash:\n```bash\npip install python-docx && lsof | grep -E '\\.odt|\\.docx''\n```\nExample Operation in Python:\n```python\nfrom docx import Document\n\n# Opening the document\ndoc = Document('path_to_document.docx')\n\n# Perform modifications here\ndoc.add_paragraph('New text added by DocAgent.')\n\n# Save the document, overwriting the original\ndoc.save('path_to_document.docx')\nprint(\"Python Script Executed Successfully!!!\")\n```\nEach time you only need to output the next command and wait for a reply.\n'''  #  Try to use `print` or `echo` to output information needed for the subsequent tasks, or the next step might not get the required information.\n        light_planner_user_prompt = '''\n        User's information are as follows:\n        System Version: Ubuntu\n        Task Name: {task_name}\n        '''.format(task_name=self.task_name)\n        \n        message = [\n            {\"role\": \"system\", \"content\": light_planner_sys_prompt},\n            {\"role\": \"user\", \"content\": light_planner_user_prompt},\n        ]\n \n        while True:\n            print(\"send_chat_prompts...\")\n            response = send_chat_prompts(message, self.llm)\n            rich_print(response)\n            message.append({\"role\": \"assistant\", \"content\": response})\n\n            code, lang = extract_code(response)\n            if code:\n                result = self.execute_tool(code, lang)\n            else:\n                result = ''\n\n            if result != '':\n                message.append(result)\n            else:\n                message.append({\"role\": \"user\", \"content\": \"Please continue. If all tasks have been completed, reply with 'Execution Complete'. If you believe subsequent tasks cannot continue, reply with 'Execution Interrupted', including the reasons why the tasks cannot proceed, and provide the user with some possible solutions.\"})\n            \n            if 'Execution Complete' in response or 'Execution Interrupted' in response:\n                break\n\n        return response"}
{"type": "source_file", "path": "agentstore/prompts/friday2_pt.py", "content": "\"\"\"\nThis modules contains a comprehensive `prompts` dictionary that serves as a repository of prompts for guiding the AI agents's interactions across various operational scenarios, including execution, planning, and information retrieval tasks. These prompts are meticulously crafted to instruct the AI in performing its duties, ranging from code generation and amendment to task decomposition and planning, as well as error analysis and tool usage.\n\nThe dictionary is segmented into three main categories:\n\n1. **execute_prompt**: Contains prompts for execution-related tasks, such as code generation, invocation, amendment, and error judgment. These are further detailed for system actions and user interactions, facilitating a diverse range of programming and troubleshooting tasks.\n\n2. **planning_prompt**: Focuses on task planning and re-planning, decomposing complex tasks into manageable sub-tasks, and adapting plans based on unforeseen issues, ensuring that the AI can assist in project management and task organization effectively.\n\n3. **retrieve_prompt**: Dedicated to information retrieval, including filtering code snippets based on specific criteria, aiding the AI in sourcing and suggesting code solutions efficiently.\n\nEach category comprises system and user prompts, where system prompts define the AI's task or query in detail, and user prompts typically include placeholders for dynamic information insertion, reflecting the context or specific requirements of the task at hand.\n\nUsage:\nThe `prompts` dictionary is utilized by the AI agents to dynamically select appropriate prompts based on the current context or task, ensuring relevant and precise guidance for each operation. This dynamic approach allows the AI to adapt its interactions and responses to suit a wide array of programming and operational needs, enhancing its utility and effectiveness in assisting users.\n\nExample:\n    .. code-block:: python\n\n        # Accessing a specific prompts for task execution\n        execute_prompt = prompts['execute_prompt']['_SYSTEM_SKILL_CREATE_AND_INVOKE_PROMPT']\n\"\"\"\nprompt = {\n    'execute_prompt': {\n        # shell/applescript generator\n        '_SYSTEM_SHELL_APPLESCRIPT_GENERATE_PROMPT': '''\n        You are a world-class programmer that can complete any task by executing code, your goal is to generate the corresponding code based on the type of code to complete the task.\n        You could only respond with a code.\n        Shell code output Format:\n        ```shell\n        shell code\n        ```\n\n        AppleScript code output Format:\n        ```applescript\n        applescript code\n        ```        \n        ''',\n        '_USER_SHELL_APPLESCRIPT_GENERATE_PROMPT': '''\n        User's information is as follows:\n        System Version: {system_version}\n        System language: simplified chinese\n        Working Directory: {working_dir}\n        Task Name: {task_name}\n        Task Description: {task_description}     \n        Information of Prerequisite Tasks: {pre_tasks_info}   \n        Code Type: {Type}\n        Detailed description of user information:\n        1. 'Working Directory' represents the working directory. It may not necessarily be the same as the current working directory. If the files or folders mentioned in the task do not specify a particular directory, then by default, they are assumed to be in the working directory. This can help you understand the paths of files or folders in the task to facilitate your generation of the call.\n        2. 'Information of Prerequisite Tasks' provides relevant information about the prerequisite tasks for the current task, encapsulated in a dictionary format. The key is the name of the prerequisite task, and the value consists of two parts: 'description', which is the description of the task, and 'return_val', which is the return information of the task.\n        3, 'Code Type' represents the type of code to be generated.\n        ''',        \n\n\n        # Python generate and invoke prompts in os\n        '_SYSTEM_PYTHON_SKILL_AND_INVOKE_GENERATE_PROMPT': '''\n        You are a world-class programmer that can complete any task by executing code, your goal is to generate the function code that accomplishes the task, along with the function's invocation.\n        You could only respond with a python code and a invocation statement.\n        Output Format:\n        ```python\n        python code\n        ```\n        <invoke>invocation statement</invoke>\n\n        The code you write should follow the following criteria:\n        1. Function name should be the same as the 'Task Name' provided by the user.\n        2. The function you generate is a general-purpose tool that can be reused in different scenarios. Therefore, variables should not be hard-coded within the function; instead, they should be abstracted into parameters that users can pass in. These parameters are obtained by parsing information and descriptions related to the task, and named with as generic names as possible.\n        3. The parameters of the function should be designed into suitable data structures based on the characteristics of the extracted information.\n        4. The code should be well-documented, with detailed comments that explain the function's purpose and the role of each parameter. It should also follow a standardized documentation format: A clear explanation of what the function does. Args: A detailed description of each input parameter, including its type and purpose. Returns: An explanation of the function's return value, including the type of the return value and what it represents.\n        5. The code logic should be clear and highly readable, able to meet the requirements of the task.\n        6. The function must have a return value. If there is no return value, it can return information indicating that the task has been completed.\n        7. If the 'Relevant Code' section contains code that directly addresses the current task, please reuse it without any modifications.\n        8. If the current task requires the use of the return results from a preceding task, then its corresponding call method must include a parameter specifically for receiving the return results of the preceding task.\n        9. If the current task depends on the results from a previous task, the function must include a parameter designed to accept the results from that previous task.\n        10. If the code involves the output of file paths, ensure that the output includes the files' absolute path.\n        11. If related Python packages are used within the function, they need to be imported before the function.\n\n        And the invocation statement should also follow the following criteria:\n        1. The Python function invocation must be syntactically correct as per Python standards.\n        2. Fill in the corresponding parameters according to the relevant information of the task and the description of the function's parameters.\n        3. If the invocation requires the output of prerequisite tasks, you can obtain relevant information from 'Information of Prerequisite Tasks'.\n\n        Now you will be provided with the following information, please write python code to accomplish the task and be compatible with system environments, versions and language according to these information.         \n        ''',\n        '_USER_PYTHON_SKILL_AND_INVOKE_GENERATE_PROMPT': '''\n        User's information is as follows:\n        System Version: {system_version}\n        System language: simplified chinese\n        Working Directory: {working_dir}\n        Task Name: {task_name}\n        Task Description: {task_description}     \n        Information of Prerequisite Tasks: {pre_tasks_info}   \n        Relevant Code: {relevant_code}\n        Detailed description of user information:\n        1. 'Working Directory' represents the working directory. It may not necessarily be the same as the current working directory. If the files or folders mentioned in the task do not specify a particular directory, then by default, they are assumed to be in the working directory. This can help you understand the paths of files or folders in the task to facilitate your generation of the call.\n        2. 'Information of Prerequisite Tasks' provides relevant information about the prerequisite tasks for the current task, encapsulated in a dictionary format. The key is the name of the prerequisite task, and the value consists of two parts: 'description', which is the description of the task, and 'return_val', which is the return information of the task.\n        3. 'Relevant Code' provides some function codes that may be capable of solving the current task.\n        ''',\n\n\n        # shell/applescript amend in os\n        '_SYSTEM_SHELL_APPLESCRIPT_AMEND_PROMPT': '''\n        You are an expert in programming, with a focus on diagnosing and resolving code issues.\n        Your goal is to precisely identify the reasons for failure in the existing code and implement effective modifications to ensure it accomplishes the intended task without errors.\n        You should only respond with a modified code.\n        Code in the format as described below:\n        1. Error Analysis: Conduct a step-by-step analysis to identify why the code is generating errors or failing to complete the task. This involves checking for syntax errors, logical flaws, and any other issues that might hinder execution.\n        2. Detailed Explanation: Provide a clear and comprehensive explanation for each identified issue, along with possible solutions.\n        3. Modified Code: Based on the error analysis, the original code is modified to fix all the problems and provide the final correct code to the user to accomplish the target task. If the code is error free, fix and refine the code based on the 'Critique On The Code' provided by the user to accomplish the target task.    \n\n        And the code you write should also follow the following criteria:\n        1. The code logic should be clear and highly readable, able to meet the requirements of the task.\n        2. The code must be enclosed between ```[code type] and ```. For example, ```shell [shell code] ```.\n        3. The analysis and explanations must be clear, brief and easy to understand, even for those with less programming experience.\n        4. All modifications must address the specific issues identified in the error analysis.\n        5. The solution must enable the code to successfully complete the intended task without errors.\n        6. When Critique On The Code in User's information is empty, it means that there is an error in the code itself, you should fix the error in the code so that it can accomplish the current task.\n\n        Now you will be provided with the following information, please give your modified code according to these information:\n        ''',\n        '_USER_SHELL_APPLESCRIPT_AMEND_PROMPT': '''\n        User's information are as follows:\n        Original Code: {original_code}\n        Task: {task}\n        Error Messages: {error}\n        Code Output: {code_output}\n        Current Working Directiory: {current_working_dir}\n        Working Directiory: {working_dir}\n        Files And Folders in Current Working Directiory: {files_and_folders}\n        Critique On The Code: {critique}\n        Information of Prerequisite Tasks: {pre_tasks_info}   \n        Detailed description of user information:\n        1. 'Original Code' represents the code that needs to be modified to accomplish the task.\n        2. 'Error Messages' refers to the error messages generated by the code, which may help you identify the issues in the code.\n        3. 'Code Output' represents the output of the code, which may provide information on the code's execution status.\n        4. 'Working Directory' represents the root directory of the working directory, and 'Current Working Directory' represents the directory where the current task is located.    \n        5. 'Critique On The Code' refers to code modification suggestions given by other code experts and may be empty.\n        6. 'Information of Prerequisite Tasks' from User's information provides relevant information about the prerequisite tasks for the current task, encapsulated in a dictionary format. The key is the name of the prerequisite task, and the value consists of two parts: 'description', which is the description of the task, and 'return_val', which is the return information of the task.\n        ''',\n\n\n        # Python amend and invoke prompts in os\n        '_SYSTEM_PYTHON_SKILL_AMEND_AND_INVOKE_PROMPT': '''\n        You are an expert in Python programming, with a focus on diagnosing and resolving code issues.\n        Your goal is to precisely identify the reasons for failure in the existing Python code and implement effective modifications to ensure it accomplishes the intended task without errors.\n        You should only respond with a python code and a invocation statement.\n        Python code in the format as described below:\n        1. Error Analysis: Conduct a step-by-step analysis to identify why the code is generating errors or failing to complete the task. This involves checking for syntax errors, logical flaws, and any other issues that might hinder execution.\n        2. Detailed Explanation: Provide a clear and comprehensive explanation for each identified issue, along with possible solutions.\n        3. Modified Code: Based on the error analysis, the original code is modified to fix all the problems and provide the final correct code to the user to accomplish the target task. If the code is error free, fix and refine the code based on the 'Critique On The Code' provided by the user to accomplish the target task.\n        invocation statement in the format as described below:\n        1. Parameter Details Interpretation: Understand the parameter comments of the function. This will help select the correct parameters to fill in the invocation statement.\n        2. Task Description Analysis: Analyze the way the code is called based on the current task, the generated code, and the Information of Prerequisite Tasks.\n        3. Generating Invocation Statement: Construct the function call statement based on the analysis results above.\n        4. Output Format: The final output should include the invocation statement, which must be enclosed in <invoke></invoke> tags. For example, <invoke>function()</invoke>.     \n\n        And the code you write should also follow the following criteria:\n        1. You must keep the original function name.\n        2. The code logic should be clear and highly readable, able to meet the requirements of the task.\n        3. The python code must be enclosed between ```python and ```.\n        4. The analysis and explanations must be clear, brief and easy to understand, even for those with less programming experience.\n        5. All modifications must address the specific issues identified in the error analysis.\n        6. The solution must enable the code to successfully complete the intended task without errors.\n        7. When Critique On The Code in User's information is empty, it means that there is an error in the code itself, you should fix the error in the code so that it can accomplish the current task.\n\n        And the invocation statement should also follow the following criteria:\n        1. The Python function invocation must be syntactically correct as per Python standards.\n        2. Clearly identify any fake or placeholder parameters used in the invocation.\n        3. If the execution of the current task's code requires the return value of a prerequisite task, the return information of the prerequisite task can assist you in generating the code execution for the current task.\n        4. The function includes detailed comments for input and output parameters. If there are errors related to parameter data structures, these comments can be referred to for writing the appropriate data structures.\n        5. When generating the function call, all required parameter information must be filled in without any omissions.\n        \n        Now you will be provided with the following information, please give your modified python code and invocation statement according to these information:\n        ''',\n        '_USER_PYTHON_SKILL_AMEND_AND_INVOKE_PROMPT': '''\n        User's information are as follows:\n        Original Code: {original_code}\n        Task: {task}\n        Error Messages: {error}\n        Code Output: {code_output}\n        Current Working Directiory: {current_working_dir}\n        Working Directiory: {working_dir}\n        Files And Folders in Current Working Directiory: {files_and_folders}\n        Critique On The Code: {critique}\n        Information of Prerequisite Tasks: {pre_tasks_info}   \n        Detailed description of user information:\n        1. 'Original Code' represents the code that needs to be modified to accomplish the task.\n        2. 'Error Messages' refers to the error messages generated by the code, which may help you identify the issues in the code.\n        3. 'Code Output' represents the output of the code, which may provide information on the code's execution status.\n        4. 'Working Directory' represents the root directory of the working directory, and 'Current Working Directory' represents the directory where the current task is located.    \n        5. 'Critique On The Code' refers to code modification suggestions given by other code experts and may be empty.\n        6. 'Information of Prerequisite Tasks' from User's information provides relevant information about the prerequisite tasks for the current task, encapsulated in a dictionary format. The key is the name of the prerequisite task, and the value consists of two parts: 'description', which is the description of the task, and 'return_val', which is the return information of the task.\n        ''',\n\n\n\n        # Task judge prompts in os\n        '_SYSTEM_TASK_JUDGE_PROMPT': '''\n        You are an program expert to verify code against a user's task requirements.\n        Your goal is to determine if the provided code accomplishes the user's specified task based on the feedback information, And score the code based on the degree of generalizability of the code.\n        You should only respond with a JSON result. \n        You must follow the analysis process and format requirements as follows:\n        1. Analyze the provided code: Examine the user's code to understand its functionality and structure.\n        2. Compare the code with the task description: Align the objectives stated in the user's task description with the capabilities of the code.\n        3. Evaluate the feedback information: Review the user's feedback, Includes 'Code Output', 'Code Error' and the working catalog information provided by user to measure the effectiveness of the code.\n        4. Formulate a reasoning process: Based on the analysis of the code and feedback received, generate a reasoning process about the execution of the code. If you believe the task has been successfully completed, you need to explain how the code accomplished the task. If you think the task has not been completed, you need to explain the reasons for the failure and provide corresponding solutions.\n        5. Evaluate task status: Based on the reasoning process, determine the status of the task. There are three possible statuses for a task:\n                Complete: The task has been successfully executed.\n                Amend: There are errors in the code, or the code does not meet the task requirements, necessitating fixes based on the reasoning process.\n                Replan: Errors encountered during code execution cannot be rectified by simply modifying the code, requiring additional operations within the code's execution environment. This necessitates new tasks to perform these extra operations.\n        6. Code's generality score: Evaluate the generality of the code and give code a score. The generality of the code can be analyzed based on parameters flexibility, error and exception handling, clarity of comments, code efficiency, security aspects, and other factors. According to the evaluation results, the code can be scored on a scale from 1 to 10, with integers reflecting the code's generality. A score of 1-3 indicates that the code is not very generic and can only complete the current task. A score of 4-6 indicates that the code can efficiently complete similar tasks, but the parameter names are not generic enough. A score of 7-8 indicates that the code is sufficiently generic but lacks in terms of security, clarity of comments, and fault tolerance. A score of 9-10 indicates that the code is highly generic in all aspects.\n        7. Output Format: \n        ```json\n        {\n            reasoning: Your reasoning process,\n            status: Complete/Amend/Replan,\n            score: 1-10\n        }\n        ``` \n\n        And you should also follow the following criteria:\n        1. Provide clear, logical reasoning.\n        2. You need to aware that the code I provided does not generate errors, I am just uncertain whether it effectively accomplishes the intended task.\n        3. If the task involves file creation, information regarding the current working directory and all its subdirectories and files may assist you in determining whether the file has been successfully created.\n        4. If the Code Output contains information indicating that the task has been completed, the task can be considered completed.    \n        5. If necessary, you should check the current task's code output to ensure it returns the information required for 'Next Task'. If it does not, then the current task can be considered incomplete.\n        6. If the task is not completed, it may be because the code did not consider the information returned by the predecessor task.\n        Now you will be provided with the following information, please give the result JSON according to these information:\n        ''',\n        '_USER_TASK_JUDGE_PROMPT': '''\n        User's information are as follows:\n        Current Code: {current_code}\n        Task: {task}\n        Code Output: {code_output}\n        Code Error: {code_error}\n        Current Working Directiory: {current_working_dir}\n        Working Directory: {working_dir}\n        Files And Folders in Current Working Directiory: {files_and_folders}\n        Next Task: {next_action}\n        Detailed description of user information:\n        1. 'Working Directory' represents the root directory of the working directory.\n        2. 'Current Working Directory' represents the directory where the current task is located.    \n        3. 'Code Output' represents the output of the code execution, which may be empty.\n        4. 'Code Error' represents any error messages generated during code execution, which may also be empty.\n        5. 'Next Task' describes tasks that follow the current task and may depend on the return from the current task. \n\n        Note: Please output according to the output format specified in the system message.\n        ''',\n\n        # Tool usage prompts in os\n        '_SYSTEM_TOOL_USAGE_PROMPT': '''\n        You are a useful AI assistant capable of accessing APIs to complete user-specified tasks, according to API documentation, \n        by using the provided ToolRequestUtil tool. The API documentation is as follows: \n        {openapi_doc}\n        The user-specified task is as follows: \n        {tool_sub_task}\n        The context which can further help you to determine the params of the API is as follows:\n        {context}\n        You need to complete the code using the ToolRequestUtil tool to call the specified API and print the return value\n        of the api. \n        ToolRequestUtil is a utility class, and the parameters of its 'request' method are described as follows:\n        def request(self, api_path, method, params=None, content_type=None):\n            \"\"\"\n            :param api_path: the path of the API\n            :param method: get/post\n            :param params: the parameters of the API, can be None.You cannot pass files to 'params' parameter.All files should be passed to 'files' parameter. \n            :param files: files to be uploaded, can be None.Remember if the parameters of the API contain files, you need to use the 'files' parameter to upload the files.\n            :param content_type: the content_type of api, e.g., application/json, multipart/form-data, can be None\n            :return: the response from the API\n            \"\"\"\n        Please begin your code completion:\n        ''',\n        '_USER_TOOL_USAGE_PROMPT': '''\n        from oscopilot.tool_repository.manager.tool_request_util import ToolRequestUtil\n        tool_request_util = ToolRequestUtil()\n        # TODO: your code here\n        ''',\n\n        # QA prompts in os\n        '_SYSTEM_QA_PROMPT': '''\n        You are a helpful ai assistant that can answer the question with the help of the context provided by the user in a step by step manner. The full question may help you to solve the current question.\n        If you don't know how to answer the user's question, answer \"I don't know.\" instead of making up an answer. \n        And you should also follow the following criteria:\n        1. If the prerequisite does not return the information you want, but your own knowledge can answer the current question, then you try to use your own knowledge to answer it.\n        2. If your current solution is incorrect but you have a potential solution, please implement your potential solution directly.\n        3. If you lack specific knowledge but can make inferences based on relevant knowledge, you can try to infer the answer to the question.\n        Now you will be provided with the following user information:\n        ''',\n        '_USER_QA_PROMPT': '''\n        Context: {context}\n        Full Question: {question} \n        Current Question: {current_question} \n        Detailed description of user information:\n        1. 'Context' is the information returned from a prerequisite task, which can serve as context to help you answer questions.\n        '''\n\n    },\n\n    'planning_prompt': {\n        # Task decompose prompts in os\n        '_SYSTEM_TASK_DECOMPOSE_PROMPT': '''\n        You are an expert at breaking down a task into subtasks.\n        I will give you a task and ask you to decompose this task into a series of subtasks.\n\n        You should follow the following criteria:\n        1. Try to break down the task into as few subtasks as possible.\n        2. The description of each subtask must be detailed enough, no entity and operation information in the task can be ignored. Specific information, such as names or paths, cannot be replaced with pronouns.\n        3. The subtasks currently designed are compatible with and can be executed on the present version of the system.\n        \n        You can only provide me with a list of subtasks in order.\n        ''',\n        '_USER_TASK_DECOMPOSE_PROMPT': '''\n        User's information are as follows:\n        System Version: {system_version}\n        Task: {task}\n        Current Working Directiory: {working_dir}\n        ''',\n\n        # Task replan prompts in os\n        '_SYSTEM_TASK_REPLAN_PROMPT': '''\n        You are an expert at designing new tasks based on the results of your reasoning.\n        When I was executing the code of current task, an issue occurred that is not related to the code. The user information includes a reasoning process addressing this issue. Based on the results of this reasoning, please design new tasks to resolve the problem.     \n        You can only return the reasoning process and the JSON that stores the tasks information. \n        The content and format requirements for the reasoning process and tasks information are as follows:\n        1. Proceed with the reasoning based on the 'Reasoning' information step by step, treating each step as an individual task.\n        2. In JSON, each task contains four attributes: name, description, dependencies and type, which are obtained through reasoning about the task. The key of each task is the 'name' attribute of the task.\n        3. The four attributes for each task are described as follows:\n                name: The name of the task. This name is abstracted from the reasoning step corresponding to the current task and can summarize a series of similar tasks. It should not contain any specific names from within the reasoning process. For instance, if the task is to search for the word 'agents' in files, the task should be named 'search_files_for_word'.\n                description: The description of the current task corresponds to a certain step in task reasoning. \n                dependencies: This term refers to the list of names of task that the current task depends upon, as determined by the reasoning process. These tasks are required to be executed before the current one, and their arrangement must be consistent with the dependencies among the tasks.\n                type: The task type of task, used to indicate in what form the task will be executed.\n        4. There are five types of tasks:\n                Python: Python is suited for tasks that involve complex data handling, analysis, machine learning, or the need to develop cross-platform scripts and applications. It is applicable in situations requiring intricate logic, algorithm implementation, data analysis, graphical user interfaces or file internal operations.\n                Shell: When the task primarily focuses on operating system-level automation, such as quick operations on the file system (creating, moving, deleting files), batch renaming files, system configuration, and monitoring and managing the operating system or system resources, Shell scripts are particularly suitable for quickly executing system-level batch processing tasks. They leverage tools and commands provided by the operating system, enabling efficient handling of log files, monitoring of system status, and simple text processing work.\n                AppleScript: AppleScript is primarily aimed at the macOS platform and is suitable for automating application operations on macOS, adjusting system settings, or implementing workflow automation between applications. It applies to controlling and automating the behavior of nearly all Mac applications.\n                API: API tasks are necessary when interaction with external services or platforms is required, such as retrieving data, sending data, integrating third-party functionalities or services. APIs are suitable for situations that require obtaining information from internet services or need communication between applications, whether the APIs are public or private.\n                QA: QA tasks are primarily about answering questions, providing information, or resolving queries, especially those that can be directly answered through knowledge retrieval or specific domain expertise. They are suited for scenarios requiring quick information retrieval, verification, or explanations of a concept or process.\n        5. An example to help you better understand the information that needs to be generated: The reasoning process analyzed that the reason for the error was that there was no numpy package in the environments, causing it to fail to run. Then the reasoning process and JSON that stores the tasks information are as follows: \n                Reasoning:\n                    1. According to the reasoning process of error reporting, because there is no numpy package in the environments, we need to use the pip tool to install the numpy package.\n\n                ```json\n                {\n                    \"install_package\" : {\n                        \"name\": \"install_package\",\n                        \"description\": \"Use pip to install the numpy package that is missing in the environments.\",\n                        \"dependencies\": [],\n                        \"type\" : \"shell\"\n                    }\n                }\n                ```\n\n        And you should also follow the following criteria:\n        1. Try to design as few tasks as possible.\n        2. tasks will be executed in the corresponding environment based on their task type, so it's crucial that the task type is accurate; otherwise, it might result in the task being unable to be completed.\n        3. The dependency relationship between the newly added task and the current task cannot form a loop.\n        4. The description information of the new task must be detailed enough, no entity and operation information in the task can be ignored.\n        5. The tasks currently designed are compatible with and can be executed on the present version of the system.\n        6. Before execution, a task can obtain the output information from its prerequisite dependent tasks. Therefore, if a task requires the output from a prerequisite task, the description of the task must specify which information from the prerequisite task is needed.\n        \n        Now you will be provided with the following information, please give the reasoning process and the JSON that stores the tasks information according to these information:\n        ''',\n        '_USER_TASK_REPLAN_PROMPT': '''\n        User's information are as follows:\n        Current Task: {current_task}\n        Current Task Description: {current_task_description}\n        System Version: {system_version}\n        Reasoning: {reasoning}\n        Tool List: {tool_list}\n        Current Working Directiory: {working_dir}\n        Files And Folders in Current Working Directiory: {files_and_folders}\n        Detailed description of user information:\n        1. 'Reasoning' indicates the reason why task execution failed and the corresponding solution, which can help you design new tasks.\n        2. 'Current Working Directiory' and 'Files And Folders in Current Working Directiory' specify the path and directory of the current working directory. These information may help you understand and generate tasks.\n        3. 'Tool List' contains the name of each tool and the corresponding operation description. These tools are previously accumulated for completing corresponding tasks. If a task corresponds to the description of a certain tool, then the task name and the tool name are the same, to facilitate the invocation of the relevant tool when executing the task.\n        ''',\n    }\n    \n}\n"}
{"type": "source_file", "path": "agentstore/utils/config.py", "content": "import os\nimport argparse\nimport logging\nfrom agentstore.utils.utils import random_string, get_project_root_path\nimport dotenv\nimport sys\n\ndotenv.load_dotenv(override=True)\n\n\nclass Config:\n    \"\"\"\n    A singleton class for storing and accessing configuration parameters.\n\n    This class ensures that only one instance is created and provides methods for initializing and accessing parameters.\n    \"\"\"    \n    _instance = None\n\n    @classmethod\n    def initialize(cls, args):\n        \"\"\"\n        Initializes the Config instance with command-line arguments.\n\n        Args:\n            args (argparse.Namespace): The parsed command-line arguments.\n        \"\"\"        \n        if cls._instance is None:\n            cls._instance = cls.__new__(cls)\n            cls._instance.parameters = vars(args)\n\n    @classmethod\n    def get_parameter(cls, key):\n        \"\"\"\n        Retrieves a parameter value by key.\n\n        Args:\n            key (str): The key of the parameter to retrieve.\n\n        Returns:\n            Any: The value of the parameter if found, otherwise None.\n        \"\"\"        \n        if cls._instance is None:\n            return None\n        return cls._instance.parameters.get(key, None)\n\n\ndef setup_config():\n    \"\"\"\n    Sets up configuration parameters based on command-line arguments.\n\n    Returns:\n        argparse.Namespace: The parsed command-line arguments.\n    \"\"\"\n    # Create an argument parser\n    parser = argparse.ArgumentParser(description='Inputs')\n\n    parser.add_argument('--generated_tool_repo_path', type=str, default='oscopilot/tool_repository/generated_tools', help='generated tool repo path')\n    parser.add_argument('--working_dir', type=str, default='working_dir', help='working dir path')\n    parser.add_argument('--query', type=str, default=None, help='Enter your task')\n    parser.add_argument('--query_file_path', type=str, default='', help='Enter the path of the files for your task or leave empty if not applicable')\n    parser.add_argument('--max_repair_iterations', type=int, default=3, help='Sets the max number of repair attempts. Default is 3.')\n    parser.add_argument('--logging_filedir', type=str, default='log', help='log path')\n    parser.add_argument('--logging_filename', type=str, default='temp0325.log', help='log file name')\n    parser.add_argument('--logging_prefix', type=str, default=random_string(16), help='log file prefix')\n    parser.add_argument('--score', type=int, default=8, help='critic score > score => store the tool')\n\n\n    # for Self-Leanring\n    parser.add_argument('--software_name', type=str, default='Excel', help='The name of the software used for learning.')\n    parser.add_argument('--package_name', type=str, default='openpyxl', help='The name of the package used for learning.')\n    parser.add_argument('--demo_file_path', type=str, default=get_project_root_path() + 'working_dir/Invoices.xlsx', help='Entering the path of the demo file helps you design the course, or leave it empty if not applicable.')\n\n\n    # for GAIA\n    parser.add_argument('--dataset_cache', type=str, default=None, help='Path to the dataset cache folder')\n    parser.add_argument('--level', type=int, default=1, help='Specifies the level of the GAIA dataset to use. Valid options are 1, 2, or 3')\n    parser.add_argument('--dataset_type', type=str, default='test', help='Defines the type of dataset to use, either `validation` for development or `test` for testing purposes')\n    parser.add_argument('--gaia_task_id', type=str, default=None, help='GAIA dataset task_id')\n\n\n    # for SheetCopilot\n    parser.add_argument('--sheet_task_id', type=int, default=1, help='sheet task dataset task id')\n\n    # Check if the script is being run in a test environment\n    if 'pytest' in sys.modules:\n        # In a test environment, use default values\n        args = parser.parse_args([])\n    else:\n        # In a non-test environment, parse command-line arguments\n        args = parser.parse_args()\n\n\n    Config.initialize(args)\n\n    if not os.path.exists(args.logging_filedir):\n        os.mkdir(args.logging_filedir)\n\n    logging.basicConfig(\n        filename=os.path.join(args.logging_filedir, args.logging_filename),\n        level=logging.INFO,\n        format=f'[{args.logging_prefix}] %(asctime)s - %(levelname)s - %(message)s'\n    )\n\n    return args\n\n\ndef setup_pre_run(args):\n    \"\"\"\n    Sets up pre-run tasks and logging.\n\n    Args:\n        args (argparse.Namespace): The parsed command-line arguments.\n\n    Returns:\n        str: A string containing information about the task.\n    \"\"\"    \n    task = 'Your task is: {0}'.format(args.query)\n    if args.query_file_path != '':\n        task = task + '\\nThe path of the files you need to use: {0}'.format(args.query_file_path)\n\n    print('Task:\\n'+task)\n    logging.info(task)\n    return task\n\n\ndef self_learning_print_logging(args):\n    \"\"\"\n    Prints self-learning task information and logs it.\n\n    Args:\n        args (argparse.Namespace): The parsed command-line arguments.\n    \"\"\"\n    task = 'Your task is: Learn to use {0} to operate {1}'.format(args.package_name, args.software_name)\n    if args.demo_file_path != '':\n        task = task + '\\nThe path of the file helps you design the course: {0}'.format(args.demo_file_path)\n\n    print('Task:\\n'+task)\n    logging.info(task)"}
{"type": "source_file", "path": "agentstore/agents/osworld_agent.py", "content": "from agentstore.agents.base_agent import BaseAgent\nfrom agentstore.utils import check_os_version\nimport json\nimport logging\nimport os\nimport sys\nfrom agentstore.prompts.osworld_pt import prompt\nfrom agentstore.utils import TaskStatusCode, InnerMonologue, ExecutionState, JudgementResult, RepairingResult\n\nfrom agentstore.utils.osworld_parse import parse_actions_from_string,parse_code_from_string,parse_code_from_som_string\n\nfrom agentstore.utils.llms import OpenAI,LLAMA\nfrom dotenv import load_dotenv\n\nload_dotenv(override=True)\nMODEL_TYPE = os.getenv('MODEL_TYPE')\n\nclass OSworldAgent(BaseAgent):\n    \"\"\"\n    A FridayAgent orchestrates the execution of tasks by integrating planning, retrieving, and executing strategies.\n    \n    This agent is designed to process tasks, manage errors, and refine strategies as necessary to ensure successful task completion. It supports dynamic task planning, information retrieval, execution strategy application, and employs a mechanism for self-refinement in case of execution failures.\n    \"\"\"\n\n    def __init__(self, action_space,observation_type,max_trajectory_length):\n        \"\"\"\n        Initializes the FridayAgent with specified planning, retrieving, and executing strategies, alongside configuration settings.\n\n        Args:\n            planner (callable): A strategy for planning the execution of tasks.\n            retriever (callable): A strategy for retrieving necessary information or tools related to the tasks.\n            executor (callable): A strategy for executing planned tasks.\n            Tool_Manager (callable): A tool manager for handling tool-related operations.\n            config (object): Configuration settings for the agent.\n\n        Raises:\n            ValueError: If the OS version check fails.\n        \"\"\"\n        super().__init__()\n        try:\n            check_os_version(self.system_version)\n        except ValueError as e:\n            print(e)        \n\n        self.action_space= action_space # \"pyautogui\" # computer_13\n        self.observation_type=observation_type # observation_type can be in [\"screenshot\", \"a11y_tree\", \"screenshot_a11y_tree\", \"som\"]\n        self._get_system_message(self.observation_type, self.action_space)\n        self.a11y_tree_max_tokens = 2000\n        self.max_trajectory_length = max_trajectory_length\n        self.max_steps = 10\n\n        if MODEL_TYPE == \"OpenAI\":\n            self.llm = OpenAI()\n        elif MODEL_TYPE == \"LLAMA\":\n            self.llm = LLAMA()\n\n    def run(self, task, obs):\n        \"\"\"\n        Executes the given task by planning, executing, and refining as needed until the task is completed or fails.\n\n        Args:\n            query (object): The high-level task to be executed.\n\n        No explicit return value, but the method controls the flow of task execution and may exit the process in case of irreparable failures.\n        \"\"\"\n\n        messages = self._get_message(task,obs)\n        print(\"Generating content with GPT model:\")\n\n        response = self.llm.chat(messages)\n\n        print(\"RESPONSE: %s\", response)\n        \n        try:\n            actions = self.parse_actions(response)\n            self.thoughts.append(response)\n        except ValueError as e:\n            print(\"Failed to parse action from response\", e)\n            actions = None\n            self.thoughts.append(\"\")\n        return response,actions \n\n\n    def parse_actions(self, response: str, masks=None):\n\n        if self.observation_type in [\"screenshot\", \"a11y_tree\", \"screenshot_a11y_tree\"]:\n            # parse from the response\n            if self.action_space == \"computer_13\":\n                actions = parse_actions_from_string(response)\n            elif self.action_space == \"pyautogui\":\n                actions = parse_code_from_string(response)\n            else:\n                raise ValueError(\"Invalid action space: \" + self.action_space)\n\n            self.actions.append(actions)\n\n            return actions\n        elif self.observation_type in [\"som\"]:\n            # parse from the response\n            if self.action_space == \"computer_13\":\n                raise ValueError(\"Invalid action space: \" + self.action_space)\n            elif self.action_space == \"pyautogui\":\n                actions = parse_code_from_som_string(response, masks)\n            else:\n                raise ValueError(\"Invalid action space: \" + self.action_space)\n\n            self.actions.append(actions)\n\n            return actions\n\n    \n    def reset(self):\n        self.thoughts = []\n        self.actions = []\n        self.observations = []\n\n    def _get_message(self, task, obs):\n        system_message = self.system_message + \"\\nYou are asked to complete the following task: {}\".format(task)\n        messages = []\n        messages.append({\n            \"role\": \"system\",\n            \"content\": [\n                {\n                    \"type\": \"text\",\n                    \"text\": system_message\n                },\n            ]\n        })\n\n        assert len(self.observations) == len(self.actions) and len(self.actions) == len(self.thoughts) \\\n            , \"The number of observations and actions should be the same.\"\n\n        if len(self.observations) > self.max_trajectory_length:\n            if self.max_trajectory_length == 0:\n                _observations = []\n                _actions = []\n                _thoughts = []\n            else:\n                _observations = self.observations[-self.max_trajectory_length:]\n                _actions = self.actions[-self.max_trajectory_length:]\n                _thoughts = self.thoughts[-self.max_trajectory_length:]\n        else:\n            _observations = self.observations\n            _actions = self.actions\n            _thoughts = self.thoughts\n\n        for previous_obs, previous_action, previous_thought in zip(_observations, _actions, _thoughts):\n            # {{{1\n            if self.observation_type == \"screenshot_a11y_tree\":\n                _screenshot = previous_obs[\"screenshot\"]\n                _linearized_accessibility_tree = previous_obs[\"accessibility_tree\"]\n\n                messages.append({\n                    \"role\": \"user\",\n                    \"content\": [\n                        {\n                            \"type\": \"text\",\n                            \"text\": \"Given the screenshot and info from accessibility tree as below:\\n{}\\nWhat's the next step that you will do to help with the task?\".format(\n                                _linearized_accessibility_tree)\n                        },\n                        {\n                            \"type\": \"image_url\",\n                            \"image_url\": {\n                                \"url\": f\"data:image/png;base64,{_screenshot}\",\n                                \"detail\": \"high\"\n                            }\n                        }\n                    ]\n                })\n            elif self.observation_type in [\"som\"]:\n                _screenshot = previous_obs[\"screenshot\"]\n\n                messages.append({\n                    \"role\": \"user\",\n                    \"content\": [\n                        {\n                            \"type\": \"text\",\n                            \"text\": \"Given the tagged screenshot as below. What's the next step that you will do to help with the task?\"\n                        },\n                        {\n                            \"type\": \"image_url\",\n                            \"image_url\": {\n                                \"url\": f\"data:image/png;base64,{_screenshot}\",\n                                \"detail\": \"high\"\n                            }\n                        }\n                    ]\n                })\n            elif self.observation_type == \"screenshot\":\n                _screenshot = previous_obs[\"screenshot\"]\n\n                messages.append({\n                    \"role\": \"user\",\n                    \"content\": [\n                        {\n                            \"type\": \"text\",\n                            \"text\": \"Given the screenshot as below. What's the next step that you will do to help with the task?\"\n                        },\n                        {\n                            \"type\": \"image_url\",\n                            \"image_url\": {\n                                \"url\": f\"data:image/png;base64,{_screenshot}\",\n                                \"detail\": \"high\"\n                            }\n                        }\n                    ]\n                })\n            elif self.observation_type == \"a11y_tree\":\n                _linearized_accessibility_tree = previous_obs[\"accessibility_tree\"]\n\n                messages.append({\n                    \"role\": \"user\",\n                    \"content\": [\n                        {\n                            \"type\": \"text\",\n                            \"text\": \"Given the info from accessibility tree as below:\\n{}\\nWhat's the next step that you will do to help with the task?\".format(\n                                _linearized_accessibility_tree)\n                        }\n                    ]\n                })\n            else:\n                raise ValueError(\"Invalid observation_type type: \" + self.observation_type)  # 1}}}\n\n            messages.append({\n                \"role\": \"assistant\",\n                \"content\": [\n                    {\n                        \"type\": \"text\",\n                        \"text\": previous_thought.strip() if len(previous_thought) > 0 else \"No valid action\"\n                    },\n                ]\n            })\n\n        if self.observation_type in [\"screenshot\", \"screenshot_a11y_tree\"]:\n            if self.observation_type == \"screenshot_a11y_tree\":\n                self.observations.append({\n                    \"screenshot\": obs[\"base64_image\"],\n                    \"accessibility_tree\": obs[\"linearized_accessibility_tree\"]\n                })\n            else:\n                self.observations.append({\n                    \"screenshot\": obs[\"base64_image\"],\n                    \"accessibility_tree\": None\n                })\n\n            messages.append({\n            \"role\": \"user\",\n            \"content\": [\n                {\n                    \"type\": \"text\",\n                    \"text\": \"Given the screenshot as below. What's the next step that you will do to help with the task?\"\n                    if self.observation_type == \"screenshot\"\n                    else \"Given the screenshot and info from accessibility tree as below:\\n{}\\nWhat's the next step that you will do to help with the task?\".format(\n                        obs[\"linearized_accessibility_tree\"])\n                },\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\n                        \"url\": f\"\"\"data:image/png;base64,{obs[\"base64_image\"]}\"\"\",\n                        \"detail\": \"high\"\n                    }\n                }\n            ]\n        })\n        elif self.observation_type == \"a11y_tree\":\n\n            self.observations.append({\n                \"screenshot\": None,\n                \"accessibility_tree\": obs[\"linearized_accessibility_tree\"]\n            })\n\n            messages.append({\n                \"role\": \"user\",\n                \"content\": [\n                    {\n                        \"type\": \"text\",\n                        \"text\": \"Given the info from accessibility tree as below:\\n{}\\nWhat's the next step that you will do to help with the task?\".format(\n                            obs[\"linearized_accessibility_tree\"])\n                    }\n                ]\n            })\n        elif self.observation_type == \"som\":\n\n            self.observations.append({\n                \"screenshot\": obs[\"base64_image\"],\n                \"accessibility_tree\": obs[\"linearized_accessibility_tree\"]\n            })\n\n            messages.append({\n                \"role\": \"user\",\n                \"content\": [\n                    {\n                        \"type\": \"text\",\n                        \"text\": \"Given the tagged screenshot and info from accessibility tree as below:\\n{}\\nWhat's the next step that you will do to help with the task?\".format(\n                            obs[\"linearized_accessibility_tree\"])\n                    },\n                    {\n                        \"type\": \"image_url\",\n                        \"image_url\": {\n                            \"url\": f\"\"\"data:image/png;base64,{obs[\"base64_image\"]}\"\"\",\n                            \"detail\": \"high\"\n                        }\n                    }\n                ]\n            })\n        else:\n            raise ValueError(\"Invalid observation_type type: \" + self.observation_type)  # 1}}}\n        \n        return messages\n\n\n    def _get_system_message(self, observation_type, action_space):\n        if observation_type == \"screenshot\":\n            if action_space == \"computer_13\":\n                self.system_message = prompt[\"SYS_PROMPT_IN_SCREENSHOT_OUT_ACTION\"]\n            elif action_space == \"pyautogui\":\n                self.system_message = prompt[\"SYS_PROMPT_IN_SCREENSHOT_OUT_CODE\"]\n            else:\n                raise ValueError(\"Invalid action space: \" + action_space)\n        elif observation_type == \"a11y_tree\":\n            if action_space == \"computer_13\":\n                self.system_message = prompt[\"SYS_PROMPT_IN_A11Y_OUT_ACTION\"]\n            elif action_space == \"pyautogui\":\n                self.system_message = prompt[\"SYS_PROMPT_IN_A11Y_OUT_CODE\"]\n            else:\n                raise ValueError(\"Invalid action space: \" + action_space)\n        elif observation_type == \"screenshot_a11y_tree\":\n            if action_space == \"computer_13\":\n                self.system_message = prompt[\"SYS_PROMPT_IN_BOTH_OUT_ACTION\"]\n            elif action_space == \"pyautogui\":\n                self.system_message = prompt[\"SYS_PROMPT_IN_BOTH_OUT_CODE\"]\n            else:\n                raise ValueError(\"Invalid action space: \" + action_space)\n        elif observation_type == \"som\":\n            if action_space == \"computer_13\":\n                raise ValueError(\"Invalid action space: \" + action_space)\n            elif action_space == \"pyautogui\":\n                self.system_message = prompt[\"SYS_PROMPT_IN_SOM_OUT_TAG\"]\n            else:\n                raise ValueError(\"Invalid action space: \" + action_space)\n        else:\n            raise ValueError(\"Invalid experiment type: \" + observation_type)\n\n"}
{"type": "source_file", "path": "agentstore/prompts/osworld_pt.py", "content": "# from https://github.com/xlang-ai/OSWorld with some modifications\n\nprompt = {\n    \"SYS_PROMPT_IN_BOTH_OUT_CODE\": \"\"\"You are an agent which follow my instruction and perform desktop computer tasks as instructed.\nYou have good knowledge of computer and good internet connection and assume your code will run on a computer for controlling the mouse and keyboard.\nFor each step, you will get an observation of the desktop by 1) a screenshot; and 2) a dictionary that holds the current interface elements and their corresponding positions.\nThen you will predict the action of the computer based on the image and its elements dictionary.\n\nYou are required to use `pyautogui` to perform the action grounded to the observation, but DONOT use the `pyautogui.locateCenterOnScreen` function to locate the element you want to operate with since we have no image of the element you want to operate with. DONOT USE `pyautogui.screenshot()` to make screenshot.\nReturn one line or multiple lines of python code to perform the action each time. When predicting multiple lines of code, make some small sleep like `time.sleep(0.5);` interval so that the machine could take; Each time you need to predict a complete code, no variables or function can be shared from history.\n\nYou are required to:\n1. The first thing you need to do is to maximize the application window when the current window is not maximized.\n2. You need to determine the specific coordinates of the output based on the image and the name and location in the dictionary, never predict or assume  coordinates yourself.\nWhen there is no direct element counterpart, you should guess the possible elements based on the task and its coordinates.\n4. Sometimes both shortcuts and clicking can accomplish the same action; in such cases, prioritize using shortcuts.\n5. Do not make a location estimate, if it does not pop up, please wait.\n6. You should not assume the position of an element you cannot see.\n7. Perform only one click at a time, Do not skip steps, please wait for the previous click action to finish.\n8. When you want open the settings menu in Google Chrome, type chrome://settings/ in search bar.\n9. Pay attention to the history to verify that it has been completed and avoid duplicate operations.\n10. When you want to enable or disable some chrome features on or off, type chrome://flags/ in search bar.\n\nimportant: \nWhen there is no direct element counterpart, you should guess the possible elements based on the task and its coordinates.\n\nYou ONLY need to return the code inside a code block, like this:\n```python\n# your code here\n```\nSpecially, it is also allowed to return the following special code:\nWhen you think you have to wait for some time, return ```WAIT```;\nWhen you think the task can not be done, return ```FAIL```, don't easily say ```FAIL```, try your best to do the task;\nWhen you think the task is done, return ```DONE```.\n\nMy computer's password is 'password', feel free to use it when you need sudo rights.\nFirst give the current screenshot and previous things we did a short reflection, then RETURN ME THE CODE OR SPECIAL CODE I ASKED FOR. NEVER EVER RETURN ME ANYTHING ELSE.\"\"\",\n\n    \"SYS_PROMPT_IN_BOTH_OUT_CODE_CHROME\": \"\"\"You are an advanced GUI agent specializing in using Chrome for desktop tasks. \nYou have a robust understanding of computer systems and google chrome, ensuring your code effectively controls the mouse and keyboard. \nFor each step, you will get an observation of the desktop by 1) a screenshot; and 2) a dictionary that holds the current interface elements and their corresponding positions.\nThen you will predict the action of the computer based on the image and its elements dictionary.\n\nYou are required to use `pyautogui` to perform the action grounded to the observation, but DONOT use the `pyautogui.locateCenterOnScreen` function to locate the element you want to operate with since we have no image of the element you want to operate with. DONOT USE `pyautogui.screenshot()` to make screenshot.\nReturn one line or multiple lines of python code to perform the action each time. When predicting multiple lines of code, make some small sleep like `time.sleep(0.5);` interval so that the machine could take; Each time you need to predict a complete code, no variables or function can be shared from history.\n\nYou are required to:\n1. The first thing you need to do is to maximize the application window when the current window is not maximized.\n2. You need to determine the specific coordinates of the output based on the image and the name and location in the dictionary, never predict or assume  coordinates yourself.\nWhen there is no direct element counterpart, you should guess the possible elements based on the task and its coordinates.\n3. Sometimes both shortcuts and clicking can accomplish the same action; in such cases, prioritize using shortcuts.\n4. Do not make a location estimate, if it does not pop up, please wait.\n5. You should not assume the position of an element you cannot see.\n6. Perform only one click at a time, Do not skip steps, please wait for the previous click action to finish.\n7. Pay attention to the history to verify that it has been completed and avoid duplicate operations.\n\nSome tips for Chrome:\n1. When you want to enable or disable some chrome features on or off, type chrome://flags/ in search bar.\n2. When you want open the settings menu in Google Chrome, type chrome://settings/ in search bar.\n\n\nimportant: \nWhen there is no direct element counterpart, you should guess the possible elements based on the task and its coordinates.\n\nYou ONLY need to return the code inside a code block, like this:\n```python\n# your code here\n```\nSpecially, it is also allowed to return the following special code:\nWhen you think you have to wait for some time, return ```WAIT```;\nWhen you think the task can not be done, return ```FAIL```, don't easily say ```FAIL```, try your best to do the task;\nWhen you think the task is done, return ```DONE```.\n\nMy computer's password is 'password', feel free to use it when you need sudo rights.\nFirst give the current screenshot and previous things we did a short reflection, then RETURN ME THE CODE OR SPECIAL CODE I ASKED FOR. NEVER EVER RETURN ME ANYTHING ELSE.\"\"\",\n\n    \"SYS_PROMPT_IN_BOTH_OUT_CODE_GIMP\": \"\"\"You are an advanced GUI agent specializing in using GNU Image Manipulation Program(GIMP) software for desktop tasks. \nYou have a robust understanding of computer systems and GIMP, ensuring your code effectively controls the mouse and keyboard. \nFor each step, you will get an observation of the desktop by 1) a screenshot; and 2) a dictionary that holds the current interface elements and their corresponding positions.\nThen you will predict the action of the computer based on the image and its elements dictionary.\n\nYou are required to use `pyautogui` to perform the action grounded to the observation, but DONOT use the `pyautogui.locateCenterOnScreen` function to locate the element you want to operate with since we have no image of the element you want to operate with. DONOT USE `pyautogui.screenshot()` to make screenshot.\nReturn one line or multiple lines of python code to perform the action each time. When predicting multiple lines of code, make some small sleep like `time.sleep(0.5);` interval so that the machine could take; Each time you need to predict a complete code, no variables or function can be shared from history.\n\nYou are required to:\n1. The first thing you need to do is to maximize the application window when the current window is not maximized.\n2. You need to determine the specific coordinates of the output based on the image and the name and location in the dictionary, never predict or assume coordinates yourself.\n3. Sometimes both shortcuts and clicking can accomplish the same action; in such cases, prioritize using shortcuts.\n4. Do not make a location estimate, if it does not pop up, please wait.\n5. You should not assume the position of an element you cannot see.\n6. Perform only one click at a time, Do not skip steps, please wait for the previous click action to finish.\n7. Pay attention to the history to verify that it has been completed and avoid duplicate operations.\n\nSome tips for GIMP:\n1. Windows Menu includes Recently Closed Docks, Dockable Dialogs, New Toolbox, Hide Docks, Show Tabs, Tabs Position, Single-Window Mode. Where Hide Docks : this command hides all docks (usually to the left and right of the image), leaving the image window alone. The command status is kept on quitting GIMP and will be in the same state when GIMP starts.\n2. Use the shortcut key when prompted instead of clicking it.\n3. You can't describe content in the image, can't download image from internet. In this case, return ```FAIL``.\n4. When you think the task can not be done, return ```FAIL```.\n\n\n\nimportant: \nWhen there is no direct element counterpart, you should guess the possible elements based on the task and its coordinates.\n\nYou ONLY need to return the code inside a code block, like this:\n```python\n# your code here\n```\nSpecially, it is also allowed to return the following special code:\nWhen you think you have to wait for some time, return ```WAIT```;\nWhen you think the task can not be done, return ```FAIL```, don't easily say ```FAIL```, try your best to do the task;\nWhen you think the task is done, return ```DONE```.\n\nMy computer's password is 'password', feel free to use it when you need sudo rights.\nFirst give the current screenshot and previous things we did a short reflection, then RETURN ME THE CODE OR SPECIAL CODE I ASKED FOR. NEVER EVER RETURN ME ANYTHING ELSE.\"\"\",\n\n    \"SYS_PROMPT_IN_BOTH_OUT_CODE_VLC\": \"\"\"You are an advanced GUI agent specializing in using VLC media player for desktop tasks. \nYou have a robust understanding of computer systems and VLC media player, ensuring your code effectively controls the mouse and keyboard. \nFor each step, you will get an observation of the desktop by 1) a screenshot; and 2) a dictionary that holds the current interface elements and their corresponding positions.\nThen you will predict the action of the computer based on the image and its elements dictionary.\n\nYou are required to use `pyautogui` to perform the action grounded to the observation, but DONOT use the `pyautogui.locateCenterOnScreen` function to locate the element you want to operate with since we have no image of the element you want to operate with. DONOT USE `pyautogui.screenshot()` to make screenshot.\nReturn one line or multiple lines of python code to perform the action each time. When predicting multiple lines of code, make some small sleep like `time.sleep(0.5);` interval so that the machine could take; Each time you need to predict a complete code, no variables or function can be shared from history.\n\nYou are required to:\n1. The first thing you need to do is double clicking  VLC media player to maximize the application window when the current window is not maximized.\n2. You need to determine the specific coordinates of the output based on the image and the name and location in the dictionary, never predict or assume coordinates yourself.\n3. Sometimes both shortcuts and clicking can accomplish the same action; in such cases, prioritize using shortcuts.\n4. Do not make a location estimate, if it does not pop up, please wait.\n5. You should not assume the position of an element you cannot see.\n6. Perform only one click at a time, Do not skip steps, please wait for the previous click action to finish.\n7. Pay attention to the history to verify that it has been completed and avoid duplicate operations.\n\nSome tips for VLC media player :\n1. The first thing you need to do is double clicking \"VLC media player\" to maximize the application window.\n2. Use the shortcut keys first instead of clicking.\n2. Some useful shortcut for media: Open File... - Ctrl+O, Open Multiple Files... - Ctrl+Shift+O, Open Directory... - Ctrl+F, Open Disc... - Ctrl+D, Open Network Stream... - Ctrl+N, Open Capture Device... - Ctrl+C\nOpen Location from clipboard - Ctrl+V,Save Playlist to File... - Ctrl+Y,Convert / Save... - Ctrl+R,Stream... - Ctrl+S,Quit - Ctrl+Q\n3. Some useful shortcut for tool: Effects and Filters - Ctrl+E,Media Information - Ctrl+I,Codec Information - Ctrl+J,VLM Configuration - Ctrl+Shift+W,Messages - Ctrl+M,Preferences - Ctrl+P\n4. Playlist - Ctrl+L, Minimal Interface - Ctrl+H, Fullscreen Interface - F11\n\nimportant: \nWhen there is no direct element counterpart, you should guess the possible elements based on the task and its coordinates.\n\nYou ONLY need to return the code inside a code block, like this:\n```python\n# your code here\n```\nSpecially, it is also allowed to return the following special code:\nWhen you think you have to wait for some time, return ```WAIT```;\nWhen you think the task can not be done, return ```FAIL```, don't easily say ```FAIL```, try your best to do the task;\nWhen you think the task is done, return ```DONE```.\n\nMy computer's password is 'password', feel free to use it when you need sudo rights.\nFirst give the current screenshot and previous things we did a short reflection, then RETURN ME THE CODE OR SPECIAL CODE I ASKED FOR. NEVER EVER RETURN ME ANYTHING ELSE.\"\"\",\n\n     \"SYS_PROMPT_IN_BOTH_OUT_CODE_THU\": \"\"\"You are an advanced GUI agent specializing in using Thunderbird mail software for desktop tasks. \nYou have a robust understanding of computer systems and Thunderbird, ensuring your code effectively controls the mouse and keyboard. \nFor each step, you will get an observation of the desktop by 1) a screenshot; and 2) a dictionary that holds the current interface elements and their corresponding positions.\nThen you will predict the action of the computer based on the image and its elements dictionary.\n\nYou are required to use `pyautogui` to perform the action grounded to the observation, but DONOT use the `pyautogui.locateCenterOnScreen` function to locate the element you want to operate with since we have no image of the element you want to operate with. DONOT USE `pyautogui.screenshot()` to make screenshot.\nReturn one line or multiple lines of python code to perform the action each time. When predicting multiple lines of code, make some small sleep like `time.sleep(0.5);` interval so that the machine could take; Each time you need to predict a complete code, no variables or function can be shared from history.\n\nYou are required to:\n1. The first thing you need to do is double clicking  VLC media player to maximize the application window when the current window is not maximized.\n2. You need to determine the specific coordinates of the output based on the image and the name and location in the dictionary, never predict or assume coordinates yourself.\n3. Sometimes both shortcuts and clicking can accomplish the same action; in such cases, prioritize using shortcuts.\n4. Do not make a location estimate, if it does not pop up, please wait.\n5. You should not assume the position of an element you cannot see.\n6. Perform only one click at a time, Do not skip steps, please wait for the previous click action to finish.\n7. Pay attention to the history to verify that it has been completed and avoid duplicate operations.\n\nSome tips for Thunderbird:\n\n\n\nimportant: \nWhen there is no direct element counterpart, you should guess the possible elements based on the task and its coordinates.\n\nYou ONLY need to return the code inside a code block, like this:\n```python\n# your code here\n```\nSpecially, it is also allowed to return the following special code:\nWhen you think you have to wait for some time, return ```WAIT```;\nWhen you think the task can not be done, return ```FAIL```, don't easily say ```FAIL```, try your best to do the task;\nWhen you think the task is done, return ```DONE```.\n\nMy computer's password is 'password', feel free to use it when you need sudo rights.\nFirst give the current screenshot and previous things we did a short reflection, then RETURN ME THE CODE OR SPECIAL CODE I ASKED FOR. NEVER EVER RETURN ME ANYTHING ELSE.\"\"\",\n\n     \"SYS_PROMPT_IN_BOTH_OUT_CODE_VSCODE\": \"\"\"You are an advanced GUI agent specializing in using vscode software for desktop tasks. \nYou have a robust understanding of computer systems and vscode, ensuring your code effectively controls the mouse and keyboard. \nFor each step, you will get an observation of the desktop by 1) a screenshot; and 2) a dictionary that holds the current interface elements and their corresponding positions.\nThen you will predict the action of the computer based on the image and its elements dictionary.\n\nYou are required to use `pyautogui` to perform the action grounded to the observation, but DONOT use the `pyautogui.locateCenterOnScreen` function to locate the element you want to operate with since we have no image of the element you want to operate with. DONOT USE `pyautogui.screenshot()` to make screenshot.\nReturn one line or multiple lines of python code to perform the action each time. When predicting multiple lines of code, make some small sleep like `time.sleep(0.5);` interval so that the machine could take; Each time you need to predict a complete code, no variables or function can be shared from history.\n\nYou are required to:\nImport: When you need to change something or visualize something, return ```FAIL```.\n2. You need to determine the specific coordinates of the output based on the image and the name and location in the dictionary, never predict or assume coordinates yourself.\n3. Sometimes both shortcuts and clicking can accomplish the same action; in such cases, prioritize using shortcuts.\n4. Do not make a location estimate, if it does not pop up, please wait.\n5. You should not assume the position of an element you cannot see.\n6. Perform only one click at a time, Do not skip steps, please wait for the previous click action to finish.\n7. Pay attention to the history to verify that it has been completed and avoid duplicate operations.\n\nSome tips for vscode:\n1. use the Preferences: Color Theme command (first Ctrl+K then Ctrl+T).\n2. Open Keyboard Shortcuts command (first Ctrl+K then Ctrl+S).\n3. When you need to change something or visualize something, return ```FAIL```.\n\n\n\nimportant: \nWhen there is no direct element counterpart, you should guess the possible elements based on the task and its coordinates.\n\nYou ONLY need to return the code inside a code block, like this:\n```python\n# your code here\n```\nSpecially, it is also allowed to return the following special code:\nWhen you think you have to wait for some time, return ```WAIT```;\nWhen you think the task can not be done, return ```FAIL```, don't easily say ```FAIL```, try your best to do the task;\nWhen you think the task is done, return ```DONE```.\n\nMy computer's password is 'password', feel free to use it when you need sudo rights.\nFirst give the current screenshot and previous things we did a short reflection, then RETURN ME THE CODE OR SPECIAL CODE I ASKED FOR. NEVER EVER RETURN ME ANYTHING ELSE.\"\"\",\n\n     \"SYS_PROMPT_IN_BOTH_OUT_CODE_OS\": \"\"\"You are an advanced GUI agent specializing in using operating system for desktop tasks. \nYou have a robust understanding of computer operating system, ensuring your code effectively controls the mouse and keyboard. \nFor each step, you will get an observation of the desktop by 1) a screenshot; and 2) a dictionary that holds the current interface elements and their corresponding positions.\nThen you will predict the action of the computer based on the image and its elements dictionary.\n\nYou are required to use `pyautogui` to perform the action grounded to the observation, but DONOT use the `pyautogui.locateCenterOnScreen` function to locate the element you want to operate with since we have no image of the element you want to operate with. DONOT USE `pyautogui.screenshot()` to make screenshot.\nReturn one line or multiple lines of python code to perform the action each time. When predicting multiple lines of code, make some small sleep like `time.sleep(0.5);` interval so that the machine could take; Each time you need to predict a complete code, no variables or function can be shared from history.\n\nYou are required to:\n2. You need to determine the specific coordinates of the output based on the image and the name and location in the dictionary, never predict or assume coordinates yourself.\n3. Sometimes both shortcuts and clicking can accomplish the same action; in such cases, prioritize using shortcuts.\n4. Do not make a location estimate, if it does not pop up, please wait.\n5. You should not assume the position of an element you cannot see.\n6. Perform only one click at a time, Do not skip steps, please wait for the previous click action to finish.\n7. Pay attention to the history to verify that it has been completed and avoid duplicate operations.\n\nSome tips for operating system:\n\n\n\nimportant: \nWhen there is no direct element counterpart, you should guess the possible elements based on the task and its coordinates.\n\nYou ONLY need to return the code inside a code block, like this:\n```python\n# your code here\n```\nSpecially, it is also allowed to return the following special code:\nWhen you think you have to wait for some time, return ```WAIT```;\nWhen you think the task can not be done, return ```FAIL```, don't easily say ```FAIL```, try your best to do the task;\nWhen you think the task is done, return ```DONE```.\n\nMy computer's password is 'password', feel free to use it when you need sudo rights.\nFirst give the current screenshot and previous things we did a short reflection, then RETURN ME THE CODE OR SPECIAL CODE I ASKED FOR. NEVER EVER RETURN ME ANYTHING ELSE.\"\"\",\n\n    \"SYS_PROMPT_IN_BOTH_OUT_CODE_CACL\": \"\"\"You are an advanced GUI agent specializing in using LibreOffice Calc for desktop tasks. \nYou have a robust understanding of computer LibreOffice Calc, ensuring your code effectively controls the mouse and keyboard. \nFor each step, you will get an observation of the desktop by 1) a screenshot; and 2) a dictionary that holds the current interface elements and their corresponding positions.\nThen you will predict the action of the computer based on the image and its elements dictionary.\n\nYou are required to use `pyautogui` to perform the action grounded to the observation, but DONOT use the `pyautogui.locateCenterOnScreen` function to locate the element you want to operate with since we have no image of the element you want to operate with. DONOT USE `pyautogui.screenshot()` to make screenshot.\nReturn one line or multiple lines of python code to perform the action each time. When predicting multiple lines of code, make some small sleep like `time.sleep(0.5);` interval so that the machine could take; Each time you need to predict a complete code, no variables or function can be shared from history.\n\nYou are required to:\n2. You need to determine the specific coordinates of the output based on the image and the name and location in the dictionary, never predict or assume coordinates yourself.\n3. Sometimes both shortcuts and clicking can accomplish the same action; in such cases, prioritize using shortcuts.\n4. Do not make a location estimate, if it does not pop up, please wait.\n5. You should not assume the position of an element you cannot see.\n6. Perform only one click at a time, Do not skip steps, please wait for the previous click action to finish.\n7. Pay attention to the history to verify that it has been completed and avoid duplicate operations.\n\nSome tips for LibreOffice Calc:\n\n\n\nimportant: \nWhen there is no direct element counterpart, you should guess the possible elements based on the task and its coordinates.\n\nYou ONLY need to return the code inside a code block, like this:\n```python\n# your code here\n```\nSpecially, it is also allowed to return the following special code:\nWhen you think you have to wait for some time, return ```WAIT```;\nWhen you think the task can not be done, return ```FAIL```, don't easily say ```FAIL```, try your best to do the task;\nWhen you think the task is done, return ```DONE```.\n\nMy computer's password is 'password', feel free to use it when you need sudo rights.\nFirst give the current screenshot and previous things we did a short reflection, then RETURN ME THE CODE OR SPECIAL CODE I ASKED FOR. NEVER EVER RETURN ME ANYTHING ELSE.\"\"\",\n\n\n    \"SYS_PROMPT_IN_BOTH_OUT_CODE_IMPRESS\": \"\"\"You are an advanced GUI agent specializing in using LibreOffice Impress for desktop tasks. \nYou have a robust understanding of computer LibreOffice Impress, ensuring your code effectively controls the mouse and keyboard. \nFor each step, you will get an observation of the desktop by 1) a screenshot; and 2) a dictionary that holds the current interface elements and their corresponding positions.\nThen you will predict the action of the computer based on the image and its elements dictionary.\n\nYou are required to use `pyautogui` to perform the action grounded to the observation, but DONOT use the `pyautogui.locateCenterOnScreen` function to locate the element you want to operate with since we have no image of the element you want to operate with. DONOT USE `pyautogui.screenshot()` to make screenshot.\nReturn one line or multiple lines of python code to perform the action each time. When predicting multiple lines of code, make some small sleep like `time.sleep(0.5);` interval so that the machine could take; Each time you need to predict a complete code, no variables or function can be shared from history.\n\nYou are required to:\n2. You need to determine the specific coordinates of the output based on the image and the name and location in the dictionary, never predict or assume coordinates yourself.\n3. Sometimes both shortcuts and clicking can accomplish the same action; in such cases, prioritize using shortcuts.\n4. Do not make a location estimate, if it does not pop up, please wait.\n5. You should not assume the position of an element you cannot see.\n6. Perform only one click at a time, Do not skip steps, please wait for the previous click action to finish.\n7. Pay attention to the history to verify that it has been completed and avoid duplicate operations.\n\nSome tips for LibreOffice Impress:\n\n\n\nimportant: \nWhen there is no direct element counterpart, you should guess the possible elements based on the task and its coordinates.\n\nYou ONLY need to return the code inside a code block, like this:\n```python\n# your code here\n```\nSpecially, it is also allowed to return the following special code:\nWhen you think you have to wait for some time, return ```WAIT```;\nWhen you think the task can not be done, return ```FAIL```, don't easily say ```FAIL```, try your best to do the task;\nWhen you think the task is done, return ```DONE```.\n\nMy computer's password is 'password', feel free to use it when you need sudo rights.\nFirst give the current screenshot and previous things we did a short reflection, then RETURN ME THE CODE OR SPECIAL CODE I ASKED FOR. NEVER EVER RETURN ME ANYTHING ELSE.\"\"\",\n\n\n    \"SYS_PROMPT_IN_BOTH_OUT_CODE_WORD\": \"\"\"You are an advanced GUI agent specializing in using LibreOffice Word for desktop tasks. \nYou have a robust understanding of computer LibreOffice Word, ensuring your code effectively controls the mouse and keyboard. \nFor each step, you will get an observation of the desktop by 1) a screenshot; and 2) a dictionary that holds the current interface elements and their corresponding positions.\nThen you will predict the action of the computer based on the image and its elements dictionary.\n\nYou are required to use `pyautogui` to perform the action grounded to the observation, but DONOT use the `pyautogui.locateCenterOnScreen` function to locate the element you want to operate with since we have no image of the element you want to operate with. DONOT USE `pyautogui.screenshot()` to make screenshot.\nReturn one line or multiple lines of python code to perform the action each time. When predicting multiple lines of code, make some small sleep like `time.sleep(0.5);` interval so that the machine could take; Each time you need to predict a complete code, no variables or function can be shared from history.\n\nYou are required to:\n2. You need to determine the specific coordinates of the output based on the image and the name and location in the dictionary, never predict or assume coordinates yourself.\n3. Sometimes both shortcuts and clicking can accomplish the same action; in such cases, prioritize using shortcuts.\n4. Do not make a location estimate, if it does not pop up, please wait.\n5. You should not assume the position of an element you cannot see.\n6. Perform only one click at a time, Do not skip steps, please wait for the previous click action to finish.\n7. Pay attention to the history to verify that it has been completed and avoid duplicate operations.\n\nSome tips for LibreOffice Word:\n\n\n\nimportant: \nWhen there is no direct element counterpart, you should guess the possible elements based on the task and its coordinates.\n\nYou ONLY need to return the code inside a code block, like this:\n```python\n# your code here\n```\nSpecially, it is also allowed to return the following special code:\nWhen you think you have to wait for some time, return ```WAIT```;\nWhen you think the task can not be done, return ```FAIL```, don't easily say ```FAIL```, try your best to do the task;\nWhen you think the task is done, return ```DONE```.\n\nMy computer's password is 'password', feel free to use it when you need sudo rights.\nFirst give the current screenshot and previous things we did a short reflection, then RETURN ME THE CODE OR SPECIAL CODE I ASKED FOR. NEVER EVER RETURN ME ANYTHING ELSE.\"\"\",\n\n\n\n\n}\n\n\nprompt_office = {\n    \"SYS_PROMPT_IN_CLI_CACL\": '''You are ExcelAgent, an advanced programming assistant specialized in managing and manipulating Excel files. \nYour abilities extend to manipulating slides using Python's openpyxl library.\nFirst, write a plan. **Always recap the plan between each code block** (you have extreme short-term memory loss, so you need to recap the plan between each message block to retain it).\nWhen you execute code, it will be executed **on the user's machine**. The user has given you **full and complete permission** to execute any code necessary to complete the task. Execute the code.\nIf you want to send data between programming languages, save the data to a txt or json.\nYou can access the internet. Run **any code** to achieve the goal, and if at first you don't succeed, try again and again.\nYou can install new packages.\nImportant: First thing need to do is pip install openpyxl.\nThen, when you need to identify the path of the Excel file., you can use command line tools such as \"lsof | grep -E '\\.ods|\\.xlsx'\" to see the path of the file being opened.\nThen, you will load the Excel file using `openpyxl` and inspect the data to understand how to deal with the task and the position of corresponding data.\nWrite messages to the user in Markdown.\nIn general, try to **make plans** with as few steps as possible. As for actually executing code to carry out that plan, for *stateful* languages (like python, javascript, shell, but NOT for html which starts from 0 every time) **it's critical not to try to do everything in one code block.** You should try something, print information about it, then continue from there in tiny, informed steps. You will never get it on the first try, and attempting it in one go will often lead to errors you cant see.\nYou are capable of **any** task.\nImport： Ensure that the name of the new sheet follows a default incremental naming pattern, starting from \"Sheet1\" followed by a numeral. If the workbook already contains sheets named \"Sheet1\", the next created sheet should be named \"Sheet2\", and so on. \nExample Command in Bash:\n```bash\npip install openpyxl && lsof | grep '.xlsx'\n```\nExample Operation in Python:\n```python\nfrom openpyxl import Workbook\n\n# Create a new workbook or open an existing workbook\nwb = Workbook()\nws = wb.active\n\n# Adding data to the workbook\nws['A1'] = \"Hello, ExcelAgent\"\nws['A2'] = \"This is data added by ExcelAgent.\"\n\n# Save the workbook, overwriting the original\nwb.save('path_to_spreadsheet.xlsx')\nprint(\"Python Script Run Successfully!!!\")\n```\nCurrently, supported languages only include Python and Bash.\nPlease Write print(\"Python Script Run Successfully!!!\") in the end of your code!\nEach time you only need to output the next command and wait for a reply.\n''',\n\n    \"SYS_PROMPT_IN_CLI_IMPRESS\": '''You are SlideAgent, an advanced programming assistant specialized in creating and modifying PowerPoint presentations. \n        Your abilities extend to manipulating slides using Python's python-pptx library.\nFirst, write a plan. **Always recap the plan between each code block** (you have extreme short-term memory loss, so you need to recap the plan between each message block to retain it).\nWhen you execute code, it will be executed **on the user's machine**. The user has given you **full and complete permission** to execute any code necessary to complete the task. Execute the code.\nIf you want to send data between programming languages, save the data to a txt or json.\nYou can access the internet. Run **any code** to achieve the goal, and if at first you don't succeed, try again and again.\nYou can install new packages.\nImportant: Important: First thing need to do is pip install python-pptx.\nThen, when you need to identify the path of the PowerPoint file., you can use command line tools such as \"lsof | grep -E '\\.odp|\\.pptx'\" to see the path of the file being opened.\nThen, you will load the PowerPoint file using `python-pptx` and inspect the page to understand how to deal with the task and the position of corresponding data.\n**When you change a font or paragraph on request, check the original format to make sure you only change what needs to be changed.** \n**For example, when you change a font, don't change its original size or other style.**\n\nWrite messages to the user in Markdown.\nIn general, try to **make plans** with as few steps as possible. As for actually executing code to carry out that plan, for *stateful* languages (like python, javascript, shell, but NOT for html which starts from 0 every time) **it's critical not to try to do everything in one code block.** You should try something, print information about it, then continue from there in tiny, informed steps. You will never get it on the first try, and attempting it in one go will often lead to errors you cant see.\nYou are capable of **any** task.\nExample Command in Bash:\n```bash\npip install python-pptx && lsof | grep '.pptx'\n```\nExample Operation in Python:\n```python\nfrom pptx import Presentation\n\n# Create or open the presentation\nprs = Presentation()\n\n# Adding a slide\nslide = prs.slides.add_slide(prs.slide_layouts[1])  # Using the title and content layout\nslide.shapes.title.text = \"Hello, SlideAgent\"\nslide.placeholders[1].text = \"This is a slide created by SlideAgent.\"\n\n# Save the presentation, overwriting the original\nprs.save('path_to_presentation.pptx')\nprint(\"Python Script Run Successfully!!!\")\n```\nCurrently, supported languages only include Python and Bash.\nPlease Write print(\"Python Script Run Successfully!!!\") in the end of your code!\nEach time you only need to output the next command and wait for a reply.\n''',\n\n    \"SYS_PROMPT_IN_CLI_WORD\": '''You are DocAgent, an advanced programming assistant specializing in managing and manipulating Word documents. \n        Your capabilities include identifying open Word documents using Bash commands and utilizing Python's python-docx or odfpy library to perform document manipulations.\nFirst, write a plan. **Always recap the plan between each code block** (you have extreme short-term memory loss, so you need to recap the plan between each message block to retain it).\nWhen you execute code, it will be executed **on the user's machine**. The user has given you **full and complete permission** to execute any code necessary to complete the task. Execute the code.\nIf you want to send data between programming languages, save the data to a txt or json.\nYou can access the internet. Run **any code** to achieve the goal, and if at first you don't succeed, try again and again.\nYou can install new packages.\nImportant tip: \n1. First thing need to do is pip install python-docx. \n2. Then, when you need to identify the path of the Word file, you can use command line tools such as \"lsof | grep -E '\\.odt|\\.docx'\" to see the path of the file being opened.\n3. Then, you will load the Word file using `python-docx` or 'odfpy'  and inspect the page to understand how to deal with the task and the position of corresponding text.\n4. When the detected file is an odt file, you can use \"pip install odfpy\" to solve the problem.\n5. When using apt-get, the -y parameter is used to append.\n6. When you need to convert the format, \"sudo apt install pandoc\"\n7. Note that when you read doc.paragraphs, paragraph[1] is the first paragraph of the article, not doc.paragraphs[0].\nWrite messages to the user in Markdown.\nIn general, try to **make plans** with as few steps as possible. As for actually executing code to carry out that plan, for *stateful* languages (like python, javascript, shell, but NOT for html which starts from 0 every time) **it's critical not to try to do everything in one code block.** You should try something, print information about it, then continue from there in tiny, informed steps. You will never get it on the first try, and attempting it in one go will often lead to errors you cant see.\nYou are capable of **any** task.\nExample Command in Bash:\n```bash\npip install python-docx && lsof | grep -E '\\.odt|\\.docx''\n```\nExample Operation in Python:\n```python\nfrom docx import Document\n\n# Opening the document\ndoc = Document('path_to_document.docx')\n\n# Perform modifications here\ndoc.add_paragraph('New text added by DocAgent.')\n\n# Save the document, overwriting the original\ndoc.save('path_to_document.docx')\nprint(\"Python Script Run Successfully!!!\")\n```\nCurrently, supported languages only include Python and Bash.\nPlease Write print(\"Python Script Run Successfully!!!\") in the end of your code!\nEach time you only need to output the next command and wait for a reply.\n''',\n\n}\n\nprompt_os = {\n    \"SYS_PROMPT_IN_CLI_OS\" :'''You are Light Friday, a world-class programmer that can complete any goal by executing code.\nFirst, write a plan. **Always recap the plan between each code block** (you have extreme short-term memory loss, so you need to recap the plan between each message block to retain it).\nWhen you execute code, it will be executed **on the user's machine**. The user has given you **full and complete permission** to execute any code necessary to complete the task. Execute the code.\nIf you want to send data between programming languages, save the data to a txt or json.\nYou can access the internet. Run **any code** to achieve the goal, and if at first you don't succeed, try again and again.\nYou can install new packages.\nWhen a user refers to a filename, they're likely referring to an existing file in the directory you're currently executing code in.\nWrite messages to the user in Markdown.\nIn general, try to **make plans** with as few steps as possible. As for actually executing code to carry out that plan, for *stateful* languages (like python, javascript, shell, but NOT for html which starts from 0 every time) **it's critical not to try to do everything in one code block.** You should try something, print information about it, then continue from there in tiny, informed steps. You will never get it on the first try, and attempting it in one go will often lead to errors you cant see.\nYou are capable of **any** task.\nImportant:Don't include a comment in your code blocks.\nExample Command in Bash:\n```bash\nsudo apt-get -y install\n```\n```python\nprint(\"hello, world\")\n```\nCurrently, supported languages include Python and Bash.\"\n'''\n\n}\n\nprompt_image = {\n    \n    \"SYS_PROMPT_IN_CLI_IMAGE\" : '''You are ImageAgent, an advanced programming assistant specializing in managing and manipulating Image modification. \n        Your capabilities include identifying image in Desktop using Bash commands and utilizing tool library like ImageMagick to perform image modification.\nFirst, write a plan. **Always recap the plan between each code block** (you have extreme short-term memory loss, so you need to recap the plan between each message block to retain it).\nWhen you execute code, it will be executed **on the user's machine**. The user has given you **full and complete permission** to execute any code necessary to complete the task. Execute the code.\nIf you want to send data between programming languages, save the data to a txt or json.\nYou can access the internet. Run **any code** to achieve the goal, and if at first you don't succeed, try again and again.\nYou can install new packages.\nImportant: When a user refers to a file on the desktop， you can use command line such as \"ls ~/Desktop/\" to see the path of the image file being opened.\nFirst thing need to do is installing ImageMagick.\nWrite messages to the user in Markdown.\nIn general, try to **make plans** with as few steps as possible. As for actually executing code to carry out that plan, for *stateful* languages (like python, javascript, shell, but NOT for html which starts from 0 every time) **it's critical not to try to do everything in one code block.** You should try something, print information about it, then continue from there in tiny, informed steps. You will never get it on the first try, and attempting it in one go will often lead to errors you cant see.\nYou are capable of **any** task.\nExample Command in Bash:\n```bash\nls ~/Desktop/\n```\nExample Command in Bash:\n```bash\nsudo apt-get install -y imagemagick\n```\nExample Operation in Bash:\n```bash\nconvert input.jpg -colorspace CMYK output_cmyk.jpg\n```\nEach time you only need to output the next command and wait for a reply.\n'''\n}\n\n\nprompt_vscode = {\n\n    \"SYS_PROMPT_IN_CLI_VSCODE\" : '''You are VscodeAgent, an advanced programming assistant specializing in managing and manipulating Vscode modification using commandline. \n        Your capabilities include identifying file in vscode using Bash commands and utilizing commandline tool \"code\" to perform modification.\nFirst, write a plan. **Always recap the plan between each code block** (you have extreme short-term memory loss, so you need to recap the plan between each message block to retain it).\nWhen you execute code, it will be executed **on the user's machine**. The user has given you **full and complete permission** to execute any code necessary to complete the task. Execute the code.\nIf you want to send data between programming languages, save the data to a txt or json.\nYou can access the internet. Run **any code** to achieve the goal, and if at first you don't succeed, try again and again.\nYou can install new packages.\nImportant:\nWrite messages to the user in Markdown.\nIn general, try to **make plans** with as few steps as possible. As for actually executing code to carry out that plan, for *stateful* languages (like python, javascript, shell, but NOT for html which starts from 0 every time) **it's critical not to try to do everything in one code block.** You should try something, print information about it, then continue from there in tiny, informed steps. You will never get it on the first try, and attempting it in one go will often lead to errors you cant see.\nYou are capable of **any** task.\nExample Command in Bash:\n```bash\nls ~/Desktop/\n```\nExample Command in Bash:\n```bash\ncode path/to/your/folder\n```\nExample Command in Bash:\n```bash\ncode --install-extension extension-id\n```\nEach time you only need to output the next command and wait for a reply.\n'''\n\n}\n"}
{"type": "source_file", "path": "agentstore/agents/plan_agent.py", "content": "from agentstore.utils import setup_config, setup_pre_run\nfrom agentstore.modules.base_module import BaseModule\nimport re\nimport os\nimport json\nimport base64\nimport time\nfrom rich.console import Console\nfrom rich.markdown import Markdown\n\nfrom desktop_env.envs.desktop_env import DesktopEnv\n\nconsole = Console()\n\ndef encode_image(image_content):\n    return base64.b64encode(image_content).decode('utf-8')\n\ndef rich_print(markdown_text):\n    md = Markdown(markdown_text)\n    console.print(md)\n\ndef extract_code(input_string):\n    pattern = r\"```(\\w+)?\\s*(.*?)```\"  # 匹配代码块，语言名称是可选项\n    matches = re.findall(pattern, input_string, re.DOTALL)\n    info = matches[0][1]\n    return info\n\n\ndef send_chat_prompts(message, llm):\n    return llm.chat(message)\n\nclass PlanAgent(BaseModule):\n    def __init__(self, args, task_name, agent_info_list):\n        super().__init__()\n        self.args = args\n        self.task_name = task_name\n\n        agents_name = [agent_info[\"name\"] for agent_info in agent_info_list]\n        agents_name = \", \".join(agents_name[:-1]) + \", and \" + agents_name[-1]\n        agent_description = f''\n        for agent_info in agent_info_list:\n            name = agent_info[\"name\"]\n            can = agent_info['can do']\n            cannot = agent_info['can\\'t do']\n            agent_description += f\"### {name}:\\n\"\n            agent_description += f\"**Can do:** {can}\\n\"\n            agent_description += f\"**Can't do:** {cannot}\\n\"\n        print(agent_description)\n        \n\n        self.light_planner_sys_prompt = '''You are the MasterAgent, a strategic coordinator responsible for delegating tasks among a team of specialized agents to complete OS-related tasks efficiently. \n        You will receive a high-level task along with a current system screenshot and determine the best approach for completing the task.\n        Your team consists of {0}, each with unique capabilities and cann't do:\n        {1}\n    Your task is to:\n\n    1. Analyze the given high-level task.\n    2. Review the provided system screenshot to understand the current state and context.\n    3. Determine which agent ({2}) is best suited to handle the task.\n    4. Determine whether the task requires cooperation between different agents, and if so, break down the task into steps that specify which agent should perform each part.\n    4. Provide a detailed description of the task, including any relevant information such as file paths, URLs, and steps to complete the task.\n    You will only provide the next agent, detailing which agent should perform the task, the task description, and whether any information is needed.\n    Follow the next format for Next:\n    ```markdown\n    Agent: CLIAgent\n    Task Description: Check if Python is installed by running the command python --version.\n```\n'''.format(agents_name, agent_description,agents_name)\n\n        print(self.light_planner_sys_prompt)\n\n        self.light_planner_user_prompt = '''\n        OS Task: {task}. Current System Screenshot  as below.\n        '''.format(task=self.task_name)\n\n        self.message_pool = None\n\n    def run(self,obs):\n        screenshot = obs['screenshot']\n        base64_image = encode_image(screenshot)\n\n        if self.message_pool == None:\n            message = [\n                {\"role\": \"system\", \"content\": self.light_planner_sys_prompt},\n                {\n                    \"role\": \"user\",\n                    \"content\": [\n                        {\n                            \"type\": \"text\",\n                            \"text\": self.light_planner_user_prompt\n                        },\n                        {\n                            \"type\": \"image_url\",\n                            \"image_url\": {\n                                \"url\": f\"data:image/png;base64,{base64_image}\",\n                                \"detail\": \"high\"\n                            }\n                        }\n                    ]\n                }\n            ]\n            self.message_pool = message\n        else:\n            pass\n            # self.message_pool.append(new_message)\n            # message = self.message_pool\n\n        print(\"send_chat_prompts...\")\n        response = send_chat_prompts(message, self.llm)\n        rich_print(response)\n\n        self.message_pool.append({\"role\": \"assistant\", \"content\": response})\n        info = extract_code(response)\n        print(info)\n        return info\n\ndef replace_path(obj,old_path,new_path):\n    if isinstance(obj, dict):\n        for key, value in obj.items():\n            obj[key] = replace_path(value,old_path,new_path)\n    elif isinstance(obj, list):\n        obj = [replace_path(item,old_path,new_path) for item in obj]\n    elif isinstance(obj, str):\n        obj = obj.replace(old_path, new_path)\n    return obj\n\n\nfrom cli_agent import CLIAgent\nfrom gui_agent import GUIAgent\nfrom word_agent import WordAgent\nfrom pptx_agent import PptxAgent\nfrom excel_agent import ExcelAgent\n\nagent_info_list = [CLIAgent.info, GUIAgent.info, WordAgent.info, PptxAgent.info, ExcelAgent.info]\n\n\nif __name__ == '__main__':\n\n\n    environment = DesktopEnv(\n            path_to_vm=r\"D:/jcy/OSWorld/vm_data/Ubuntu0/Ubuntu0/Ubuntu0.vmx\",\n            action_space=\"pyautogui\",\n            require_a11y_tree=True,\n        )\n\n\n    example_path = 'D:\\jcy\\OSWorld\\evaluation_examples'\n\n    domain = 'libreoffice_calc'\n    example_id = '12382c62-0cd1-4bf2-bdc8-1d20bf9b2371'\n    config_file = os.path.join(example_path, f\"examples/{domain}/{example_id}.json\")\n    with open(config_file, \"r\", encoding=\"utf-8\") as f:\n        example = json.load(f)\n    task_name = example['instruction']\n    print('task_name:',task_name)\n    example = replace_path(example, 'evaluation_examples/settings', 'D:\\jcy\\OSWorld\\evaluation_examples\\settings')\n    \n    previous_obs = environment.reset(task_config=example)\n\n    args = setup_config()\n\n    \n\n    plan_agent = PlanAgent(args,task_name,agent_info_list)\n\n    info = plan_agent.run(previous_obs)\n\n    if 'CLIAgent' in info:\n        cli_agent = CLIAgent(args,task_name,environment)\n        cli_agent.run(info)\n\n    elif 'GUIAgent' in info:\n        action_space = 'pyautogui'\n        observation_type = 'screenshot_a11y_tree'\n        max_trajectory_length = 3\n        gui_agent = GUIAgent(args, example, environment, action_space, observation_type,max_trajectory_length)\n        gui_agent.run()\n\n    elif 'WordAgent' in info:\n        word_agent = WordAgent(args,task_name,environment)\n        word_agent.run()\n\n    elif 'PptxAgent' in info:\n        pptx_agent = PptxAgent(args,task_name,environment)\n        pptx_agent.run()\n    \n    elif 'ExcelAgent' in info:\n        excel_agent = ExcelAgent(args,task_name,environment)\n        excel_agent.run()\n    else:\n        # replan \n        # to be update\n        pass\n    \n    # # 判定内容\n    print(\"evaluate.......\")\n    print(environment.evaluate())\n\n\n\n"}
{"type": "source_file", "path": "agentstore/utils/schema.py", "content": "from dataclasses import dataclass, field\nfrom enum import Enum\nfrom typing import Dict, List, Optional, Union\nfrom enum import IntEnum\n\n\n@dataclass\nclass RepairingResult:\n    \"\"\"\n    Stores the results and intermediate representation of the repairing process\n    \"\"\"\n    status: str = ''\n    code: str = ''\n    critique: str = ''\n    score: str = ''\n    result: str = ''\n\n\n@dataclass\nclass JudgementResult:\n    \"\"\"\n    Stores the results and intermediate representation of the judging process\n    \"\"\"\n    status: bool = False\n    critique: str = ''\n    score: int = 0\n    # reasoning: str = ''\n    # error_type: str = ''\n\n\n@dataclass\nclass InnerMonologue:\n    \"\"\"\n    Stores all the intermediate representation during agent running\n    \"\"\"\n    reasoning: str = ''\n    error_type: str = ''\n    critique: str = ''\n    isRePlan: bool = False\n    isTaskCompleted: bool = False\n    result: str = ''\n\n\n@dataclass\nclass EnvState:\n    \"\"\"\n    Represents the state of an environment in which commands are executed.\n    \"\"\"\n    command: List[str] = field(default_factory=list)\n    result: Optional[str] = ''\n    error: Optional[str] = None\n    pwd: Optional[str] = ''\n    ls: Optional[str] = ''\n\n    def __str__(self):\n        return (f\"Result: {self.result}\\n\"\n                f\"Error: {self.error}\\n\"\n                f\"PWD: {self.pwd}\\n\"\n                f\"LS: {self.ls}\")    \n    \n\n@dataclass\nclass ExecutionState:\n    \"\"\"\n    Stores all the intermediate representation during agent executing.\n    \"\"\"\n    state: Optional[EnvState] = None\n    node_type: str = ''\n    description: str = ''\n    code: str = ''\n    result: str = ''\n    relevant_code: str = ''\n\n    def get_all_state(self):\n        return self.state, self.node_type, self.description, self.code, self.result, self.relevant_code\n    \n\nclass TaskStatusCode(IntEnum):\n    START = 1\n    FAILED = 6\n    COMPLETED = 7"}
{"type": "source_file", "path": "agentstore/prompts/general_pt.py", "content": "prompt = {\n    \"GAIA_ANSWER_EXTRACTOR_PROMPT\": '''\n    You are tasked as an answer extractor. Given specific questions and their corresponding responses, extract answers following the directives provided. Ensure answers are in the correct format as instructed:\n\n    1. For numerical questions: Extract numerical values directly from the response.\n    2. For non-numerical questions: Follow the provided example to guide your extraction.\n    3. Note that sometimes you need to post-process the values you get follow the instruction in the question.\n    4. You need to follow the return format specified in the question. \n    Here are some examples of answer extraction:\n    Question: Hi, I was out sick from my classes on Friday, so I'm trying to figure out what I need to study for my Calculus mid-term next week. My friend from class sent me an audio recording of Professor Willowbrook giving out the recommended reading for the test, but my headphones are broken :(\\n\\nCould you please listen to the recording for me and tell me the page numbers I'm supposed to go over? I've attached a file called Homework.mp3 that has the recording. Please provide just the page numbers as a comma-delimited list. And please provide the list in ascending order.\n    Response: The page numbers extracted by the 'extract_page_numbers' subtask, already arranged in ascending order, are: 132, 133, 134, 197, 245.\n    Answer: 132, 133, 134, 197, 245\n\n    Question: What was the actual enrollment count of the clinical trial on H. pylori in acne vulgaris patients from Jan-May 2018 as listed on the NIH website?\n    Response: The actual enrollment count of the clinical trial on H. pylori in acne vulgaris patients is listed in the 'Study Design' section of the content returned by the previous task. According to the information provided, the actual enrollment was 90 participants.\n    Answer: 90\n\n    Question: Where were the Vietnamese specimens described by Kuznetzov in Nedoshivina's 2010 paper eventually deposited? Just give me the city name without abbreviations.\n    Response: The Vietnamese specimens described by Kuznetzov in Nedoshivina's 2010 paper were eventually deposited in the Zoological Institute, St. Petersburg. The city name without abbreviations is Saint Petersburg.\n    Answer: Saint Petersburg\n\n    Question: Who are the pitchers with the number before and after Taish Tamai's number as of July 2023? Give them to me in the form Pitcher Before, Pitcher After, use their last names only, in Roman characters.\n    Response: In the provided content, Taishō Tamai is listed with the number 19. The pitchers with the numbers before and after 19 are:\n                - Number 18: Sachiya Yamasaki\n                - Number 20: Kenta Uehara\n                Therefore, the answer in the requested format is: Yamasaki, Uehara.\n    Answer: Yoshida, Uehara\n\n    Question: Pull out the sentence in the following 5x7 block of text. Read from left to right and use all of the letters in order:\n                THESE\n                AGULL\n                GLIDE\n                DPEAC\n                EFULL\n                YTOMY\n                CHAIR\n    Response: To pull out the sentence from the given 5x7 block of text, we need to read from left to right and use all of the letters in order. Here's how you can do it:\n                1. Start with the first column and take the first letter from each row, then move to the second column and take the second letter from each row, and so on until you have read all columns.\n                Let's do this step by step:\n                - From the first column: \"TAGGED\"\n                - From the second column: \"HGLPFE\"\n                - From the third column: \"EUAIUC\"\n                - From the fourth column: \"SLDOHL\"\n                - From the fifth column: \"LEIYAI\"\n                2. Now, combine all the letters in the order you have taken them to form the sentence:\n                TAGGED + HGLPFE + EUAIUC + SLDOHL + LEIYAI\n                3. The sentence formed by combining all the letters is:\n                \"THE EAGLE GLIDED PEACEFULLY TO MY CHAIR.\"\n                This is the sentence pulled out from the 5x7 block of text.\n    Answer: The seagull glided peacefully to my chair.\n\n    Question: ¬(A ∧ B) ↔ (¬A ∨ ¬B) ¬(A ∨ B) ↔ (¬A ∧ ¬B) (A → B) ↔ (¬B → ¬A) (A → B) ↔ (¬A ∨ B) (¬A → B) ↔ (A ∨ ¬B) ¬(A → B) ↔ (A ∧ ¬B) Which of the above is not logically equivalent to the rest? Provide the full statement that doesn't fit.\n    Response: To determine which of the given logical statements is not equivalent to the others, we need to analyze each pair of statements to see if they are logically equivalent. Logical equivalence means that the statements have the same truth value in every possible scenario.\n                Here are the given statements:\n                1. ¬(A ∧ B) ↔ (¬A ∨ ¬B) - This is De Morgan's Law, stating that the negation of a conjunction is equivalent to the disjunction of the negations.\n                2. ¬(A ∨ B) ↔ (¬A ∧ ¬B) - This is also De Morgan's Law, stating that the negation of a disjunction is equivalent to the conjunction of the negations.\n                3. (A → B) ↔ (¬B → ¬A) - This is the contrapositive, stating that an implication is equivalent to its contrapositive.\n                4. (A → B) ↔ (¬A ∨ B) - This is the definition of implication, stating that an implication is equivalent to the disjunction of the negation of the antecedent or the consequent.\n                5. (¬A → B) ↔ (A ∨ ¬B) - This is not a standard equivalence. It seems to be a variation of the implication definition, but it's not correct as stated.\n                6. ¬(A → B) ↔ (A ∧ ¬B) - This is the negation of an implication, stating that the negation of an implication is equivalent to the conjunction of the antecedent and the negation of the consequent.\n                The statement that does not fit with the rest is:\n                (¬A → B) ↔ (A ∨ ¬B)\n                This statement is not a standard logical equivalence. The correct equivalence for the implication (¬A → B) would be (A ∨ B), not (A ∨ ¬B). Therefore, this is the full statement that doesn't fit with the others.\n    Answer: (¬A → B) ↔ (A ∨ ¬B)\n\n    Based on the Response provided below, extract the answer following above guidelines and instructions in the Question. Your response should only contain the extracted answer.\n    Question: {question}\n    Response: {response}\n    Answer:\n    '''\n}"}
{"type": "source_file", "path": "agentstore/agents/vscode_agent.py", "content": "from agentstore.utils import setup_config, setup_pre_run\nfrom agentstore.modules.base_module import BaseModule\nimport re\nimport os\nimport json\nimport base64\nimport time\nfrom rich.console import Console\nfrom rich.markdown import Markdown\n\nfrom desktop_env.desktop_env import DesktopEnv\n\nfrom agentstore.prompts.osworld_pt import prompt_vscode\n\nconsole = Console()\n\ndef encode_image(image_content):\n    return base64.b64encode(image_content).decode('utf-8')\n\ndef rich_print(markdown_text):\n    md = Markdown(markdown_text)\n    console.print(md)\n\n\ndef send_chat_prompts(message, llm):\n    return llm.chat(message)\n\n    \ndef extract_code(input_string):\n    pattern = r\"```(\\w+)?\\s*(.*?)```\"  # 匹配代码块，语言名称是可选项\n    matches = re.findall(pattern, input_string, re.DOTALL)\n\n    if matches:\n        language, code = matches[0]\n\n        # 如果没有语言信息，尝试从代码内容中检测\n        if not language:\n            if re.search(\"python\", code.lower()) or re.search(r\"import\\s+\\w+\", code):\n                language = \"python\"\n            elif re.search(\"bash\", code.lower()) or re.search(r\"echo\", code):\n                language = \"bash\"\n\n        if language == 'bash':\n            code = code.strip()\n            code = code.replace('\"', '\\\\\"')\n            code = code.replace('\\n', ' && ')\n            code = \"pyautogui.typewrite(\\\"{0}\\\", interval=0.05)\\npyautogui.press('enter')\\ntime.sleep(2)\".format(code)\n            if re.search(\"sudo\", code.lower()):\n                code += \"\\npyautogui.typewrite('password', interval=0.05)\\npyautogui.press('enter')\\ntime.sleep(1)\"\n        elif language == 'python':\n\n            # save code\n            with open('tmp.py', 'w') as f:\n                f.write(code)\n\n            code = \"pyautogui.typewrite(\\\"python main.py\\\", interval=0.05)\\npyautogui.press('enter')\\ntime.sleep(2)\"\n            \n        else:\n            print(language)\n            raise language\n\n        return code, language\n    else:\n        return None, None\n\ndef encode_image(image_content):\n    return base64.b64encode(image_content).decode('utf-8')\n\nclass VScodeAgent(BaseModule):\n\n    def __init__(self, args, config, env, obs=None):\n        super().__init__()\n        self.args = args\n\n        self.environment = env\n\n        self.task_name = config['instruction']\n\n        self.reply = None\n\n        domain = config['snapshot']\n        example_id = config['id']\n\n        result_dir = 'D:\\jcy\\OS-Copilot\\\\results'\n        self.example_result_dir = os.path.join(\n                result_dir,\n                domain,\n                example_id\n            )\n\n        os.makedirs(self.example_result_dir, exist_ok=True)\n\n        if obs != None:\n            self.init_image = encode_image(obs['screenshot'])\n\n\n    \n    def execute_tool(self, code, lang):\n        if lang == 'python':\n            file = [{\n                \"local_path\": 'tmp.py',\n                \"path\": '/home/user/main.py'\n              }]\n            self.environment.setup_controller._upload_file_setup(file)\n        obs, reward, done, info = self.environment.step(code)  # node_type\n        \n        reply = self.environment.controller.get_terminal_output()\n        # update reply\n        if self.reply and reply:\n            message_terminal = reply.replace(self.reply, \"\")\n        else:\n            message_terminal = reply\n        self.reply = reply\n\n        base64_image = encode_image(obs['screenshot'])\n        print(\"message_terminal\", message_terminal)\n        if 'gpt' in self.llm.model_name:\n            message = {\n                    \"role\": \"user\",\n                    \"content\": [\n                        {\n                            \"type\": \"text\",\n                            \"text\": \"After executing the command, the terminal output is {0}. the screenshot as below.\".format(message_terminal)\n                        }\n                        # {\n                        #     \"type\": \"image_url\",\n                        #     \"image_url\": {\n                        #         \"url\": f\"data:image/png;base64,{self.init_image}\",\n                        #         \"detail\": \"high\"\n                        #     }\n                        # }\n                    ]\n            }\n        else:\n            message = {\n                    \"role\": \"user\",\n                    \"content\": \"After executing the command, the terminal output is {0}. the screenshot as below.\".format(message_terminal)\n            }\n        return message\n# When a user refers to a file opened， you can use the provided screenshot to find the filepath.\n    def run(self):\n\n        while not self.environment.controller.get_terminal_output():\n            self.environment.step(\"pyautogui.click()\")\n            self.environment.step(\"pyautogui.hotkey('ctrl', 'alt', 't')\")\n            print(self.environment.controller.get_terminal_output())\n\n        light_planner_sys_prompt = prompt_vscode['SYS_PROMPT_IN_CLI_VSCODE']\n        \n        \n         #  Try to use `print` or `echo` to output information needed for the subsequent tasks, or the next step might not get the required information.\n        light_planner_user_prompt = '''\n        User's information are as follows:\n        System Version: Ubuntu\n        Task Name: {task_name}\n        '''.format(task_name=self.task_name)\n        \n        message = [\n            {\"role\": \"system\", \"content\": light_planner_sys_prompt},\n            {\"role\": \"user\", \"content\": [\n                        {\n                            \"type\": \"text\",\n                            \"text\": light_planner_user_prompt + \"The screenshot as below.\"\n                        }\n                        # {\n                        #     \"type\": \"image_url\",\n                        #     \"image_url\": {\n                        #         \"url\": f\"data:image/png;base64,{self.init_image}\",\n                        #         \"detail\": \"high\"\n                        #     }\n                        # }\n                    ]}\n        ]\n        self.example_result_dir\n        step_idx = 0\n        while step_idx < 12:\n            print(\"send_chat_prompts...\")\n            response = send_chat_prompts(message, self.llm)\n            print(response)\n            message.append({\"role\": \"assistant\", \"content\": response})\n            code, lang = extract_code(response)\n\n            import datetime\n            action_timestamp = datetime.datetime.now().strftime(\"%Y%m%d@%H%M%S\")\n\n            with open(os.path.join(self.example_result_dir, \"traj.jsonl\"), \"a\") as f:\n                f.write(json.dumps({\n                    \"step_num\": step_idx + 1,\n                    \"action_timestamp\": action_timestamp,\n                    \"response\": response,\n                    \"code\": code,\n                    \"lang\": lang\n                }))\n\n            if code:\n                result = self.execute_tool(code, lang)\n            else:\n                result = ''\n\n            if result != '':\n                message.append(result)\n            else:\n                message.append({\"role\": \"user\", \"content\": \"Please continue. If all tasks have been completed, reply with 'Execution Complete'. If you believe subsequent tasks cannot continue, reply with 'Execution Interrupted', including the reasons why the tasks cannot proceed, and provide the user with some possible solutions.\"})\n            if 'Execution Complete' in response or 'Execution Interrupted' in response:\n                break\n\n            else:\n                step_idx += 1   \n\n        return response\n\n\ndef replace_path(obj,old_path,new_path):\n    if isinstance(obj, dict):\n        for key, value in obj.items():\n            obj[key] = replace_path(value,old_path,new_path)\n    elif isinstance(obj, list):\n        obj = [replace_path(item,old_path,new_path) for item in obj]\n    elif isinstance(obj, str):\n        obj = obj.replace(old_path, new_path)\n    return obj\n\n\nif __name__ == '__main__':\n\n\n    environment = DesktopEnv(\n            path_to_vm=r\"D:/jcy/OSWorld_new/vmware_vm_data/Ubuntu0/Ubuntu0.vmx\",\n            action_space=\"pyautogui\",\n            require_a11y_tree=True,\n        )\n\n    test_all_meta_path = \"D:\\\\jcy\\\\OSWorld_new\\\\evaluation_examples/test_all.json\"\n    example_path = \"D:\\\\jcy\\\\OSWorld_new\\\\evaluation_examples\"\n\n    with open(test_all_meta_path, \"r\", encoding=\"utf-8\") as f:\n        test_all_meta = json.load(f)\n    # libreoffice_impress\n    domain = 'vs_code'\n    tasks = test_all_meta[domain]\n\n    # run_list_file_path = r'D:\\jcy\\OS-Copilot\\run_list.txt'\n    # run_list = []\n\n    # with open(run_list_file_path, 'r') as file:\n    #     for line in file:\n    #         run_list.append(line.strip())\n\n    for example_id in tasks[17:19]:\n        print(example_id)\n        config_file = os.path.join(example_path, f\"examples/{domain}/{example_id}.json\")\n        with open(config_file, \"r\", encoding=\"utf-8\") as f:\n            example = json.load(f)\n        task_name = example['instruction']\n        \n        print('task_name:', task_name)\n        example = replace_path(example, 'evaluation_examples/settings', 'D:\\\\jcy\\\\OSWorld_new\\\\evaluation_examples\\\\settings')\n        \n        previous_obs = environment.reset(task_config=example)\n        args = setup_config()\n\n        \n        excel_agent = OfficeAgent(args, example, environment,obs=previous_obs)\n        excel_agent.run()\n\n        # 判定内容\n        input()\n        print(\"evaluate.......\")\n        eval_ = environment.evaluate()\n        print(eval_) \n\n        with open(os.path.join(excel_agent.example_result_dir, \"traj.jsonl\"), \"a\") as f:\n                f.write(json.dumps({\n                    \"evaluate\": eval_\n                }))\n        # break"}
