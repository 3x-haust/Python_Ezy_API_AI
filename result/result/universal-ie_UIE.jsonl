{"repo_info": {"repo_name": "UIE", "repo_owner": "universal-ie", "repo_url": "https://github.com/universal-ie/UIE"}}
{"type": "source_file", "path": "dataset_processing/data/casie/scripts/check_stanford_corenlp.py", "content": "#!/usr/bin/env python\n# -*- coding:utf-8 -*-\nimport json\nimport os\n\n\ndef walk_folder(folder_name, end_str='.json'):\n    for root, dirs, files in os.walk(folder_name):\n        for file in files:\n            file_name = os.path.join(root, file)\n            if file_name.endswith(end_str):\n                yield file_name\n\n\ndef main(source_folder, corenlp_folder):\n    safe = True\n    mismatch_set = set()\n    for text_file in walk_folder(source_folder, '.txt'):\n        json_file = text_file.replace(source_folder, corenlp_folder) + '.json'\n\n        if not os.path.exists(json_file):\n            print(\"Not found:\", json_file)\n            safe = False\n            continue\n\n        text = open(text_file).read()\n        document = json.load(open(json_file))\n\n        for sentence in document['sentences']:\n            for token in sentence['tokens']:\n                token_start = token['characterOffsetBegin']\n                token_end = token['characterOffsetEnd']\n                if token['originalText'] != text[token_start:token_end].replace('\\n', ''):\n                    safe = False\n                    mismatch_set.add(text_file)\n\n    for filename in mismatch_set:\n        print('mismatch doc:', filename)\n    \n    if safe:\n        print('All Clear')\n\n\nif __name__ == \"__main__\":\n    main('raw_data/content', 'raw_data/corenlp')\n"}
{"type": "source_file", "path": "dataset_processing/universal_ie/task_format/task_format.py", "content": "#!/usr/bin/env python\n# -*- coding:utf-8 -*-\nimport abc\n\n\nclass TaskFormat:\n    __metaclass__ = abc.ABCMeta\n\n    @abc.abstractmethod\n    def __init__(self, language='en'):\n        self.language = language\n\n    @abc.abstractmethod\n    def generate_instance(self):\n        pass\n\n    @staticmethod\n    @abc.abstractmethod\n    def load_from_file(filename, language='en'):\n        pass\n"}
{"type": "source_file", "path": "dataset_processing/scripts/scierc_processing.py", "content": "#!/usr/bin/env python\n# -*- coding:utf-8 -*-\nimport json\nimport os\n\n\ndef make_new_folder(folder_name):\n    if not os.path.exists(folder_name):\n        os.makedirs(folder_name)\n\n\ndef processing_sentence(sentence):\n    new_sentence = {'tokens': sentence['tokens'], 'span_pair_list': sentence['relations']}\n    span_list = list()\n    for entity in sentence['entities']:\n        span = entity\n        span['end'] -= 1\n        span_list += [span]\n    new_sentence['span_list'] = span_list\n    return new_sentence\n\n\ndef main():\n    raw_folder = \"data/dygiepp/scierc\"\n    new_folder = \"data/relation/scierc\"\n    for split in ['train', 'dev', 'test']:\n        raw_file = os.path.join(raw_folder, f\"{split}.json\")\n        new_file = os.path.join(new_folder, f\"{split}.jsonlines\")\n        print(f\"convert {raw_file} to {new_file}\")\n        with open(new_file, 'w') as output:\n            for line in open(raw_file):\n                instance = json.loads(line)\n                sentence_start = 0\n                for sentence, ner, relation in zip(instance['sentences'],\n                                                   instance['ner'],\n                                                   instance['relations']):\n                    ner = [[x[0] - sentence_start, x[1] - sentence_start, x[2]]\n                           for x in ner]\n                    relation = [[x[0] - sentence_start, x[1] - sentence_start,\n                                x[2] - sentence_start, x[3] - sentence_start, x[4]]\n                                for x in relation]\n\n                    span_list = [{'start': x[0], 'end': x[1], 'type': x[2]}\n                                 for x in ner]\n\n                    ner_dict = {(x['start'], x['end']): i\n                                for i, x in enumerate(span_list)}\n\n                    span_pair_list = list()\n                    for rel in relation:\n                        head_index = ner_dict[(rel[0], rel[1])]\n                        tail_index = ner_dict[(rel[2], rel[3])]\n                        span_pair_list += [{\n                            'head': head_index,\n                            'tail': tail_index,\n                            'type': rel[4],\n                        }]\n\n                    sentence_start += len(sentence)\n                    spannet_instance = {\n                        'tokens': sentence,\n                        'span_list': span_list,\n                        'span_pair_list': span_pair_list\n                    }\n                    output.write(\n                        json.dumps(spannet_instance, ensure_ascii=False) + '\\n'\n                    )\n\n\nif __name__ == \"__main__\":\n    main()\n"}
{"type": "source_file", "path": "dataset_processing/scripts/sample_data_shot.py", "content": "#!/usr/bin/env python\n# -*- coding:utf-8 -*-\nimport os\nimport shutil\nimport random\nimport argparse\nfrom collections import defaultdict\nimport json\nimport sys\nfrom universal_ie.record_schema import RecordSchema\n\n\ndef n_shot_smaple(source_filename, target_filename, record_schema,\n                  spot_asoc_key='spot', num_shot=5, min_len=None, seed=None):\n\n    train_data = [json.loads(line.strip()) for line in open(source_filename)]\n\n    if seed:\n        random.seed(seed)\n        random.shuffle(train_data)\n\n    # 记录每一句的类别信息\n    type_to_sentence_dict = defaultdict(list)\n    for index, instance in enumerate(train_data):\n        for spot in instance[spot_asoc_key]:\n            if spot not in record_schema.type_list:\n                continue\n            if min_len is not None and len(instance['tokens']) < min_len:\n                continue\n            type_to_sentence_dict[spot] += [index]\n\n    sampled_data = list()\n    for entity in type_to_sentence_dict:\n\n        if len(type_to_sentence_dict[entity]) < num_shot:\n            sys.stderr.write(\n                f'[WARN] {entity} in {source_filename} is less than shot num {num_shot}\\n'\n            )\n            sampled = type_to_sentence_dict[entity]\n        else:\n            sampled = random.sample(type_to_sentence_dict[entity], num_shot)\n\n        sampled_data += [train_data[index] for index in sampled]\n\n    with open(target_filename, 'w') as output:\n        for instance in sampled_data:\n            output.write(json.dumps(instance) + '\\n')\n\n    return sampled_data\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('-src', help='Source Folder Name', required=True)\n    parser.add_argument('-tgt', help='Target Folder Name, n shot sampled',\n                        required=True)\n    parser.add_argument('-task', help='N-Shot Task name', required=True,\n                        choices=['entity', 'relation', 'event'])\n    parser.add_argument('-seed', help='Default is None, no random')\n    parser.add_argument('-min_len', dest='min_len', help='Default is None', type=int)\n    options = parser.parse_args()\n\n    source_folder = options.src\n    target_folder = options.tgt\n\n    task_name = options.task\n\n    if task_name in ['relation']:\n        spot_asoc_key = 'asoc'\n    else:\n        spot_asoc_key = 'spot'\n\n    os.makedirs(target_folder, exist_ok=True)\n\n    for shot in [1, 5, 10]:\n        shot_folder = os.path.join(target_folder, \"%sshot\" % shot)\n\n        os.makedirs(shot_folder, exist_ok=True)\n\n        n_shot_smaple(\n            source_filename=os.path.join(source_folder, 'train.json'),\n            target_filename=os.path.join(shot_folder, 'train.json'),\n            record_schema=RecordSchema.read_from_file(\n                os.path.join(source_folder, f'{task_name}.schema'),\n            ),\n            spot_asoc_key=spot_asoc_key,\n            num_shot=shot,\n            seed=options.seed,\n            min_len=options.min_len\n        )\n\n        for filename in os.listdir(source_folder):\n            if filename != 'train.json':\n                shutil.copy(\n                    os.path.join(source_folder, filename),\n                    os.path.join(shot_folder, filename),\n                )\n\n\nif __name__ == \"__main__\":\n    main()\n"}
{"type": "source_file", "path": "dataset_processing/universal_ie/task_format/absa.py", "content": "#!/usr/bin/env python\n# -*- coding:utf-8 -*-\n\n\nimport json\nfrom typing import List\nfrom universal_ie.utils import tokens_to_str, change_ptb_token_back\nfrom universal_ie.ie_format import Entity, Label, Relation, Sentence, Span\nfrom universal_ie.task_format.task_format import TaskFormat\n\n\nclass ABSA(TaskFormat):\n    \"\"\" Aspect-Based Sentiment Analysis Data format at https://github.com/yhcc/BARTABSA.\"\"\"\n\n    def __init__(self, sentence_json, language='en'):\n        super().__init__(\n            language=language\n        )\n        self.tokens = sentence_json['words']\n        for index in range(len(self.tokens)):\n            self.tokens[index] = change_ptb_token_back(self.tokens[index])\n        if self.tokens is None:\n            print('[sentence without tokens]:', sentence_json)\n            exit(1)\n        self.aspects = sentence_json['aspects']\n        self.opinions = sentence_json['opinions']\n\n    def generate_instance(self):\n        aspect_entities = dict()\n        opinion_entities = dict()\n        relations = list()\n\n        for aspect, opinion in zip(self.aspects, self.opinions):\n            aspect_span = (aspect['from'], aspect['to'])\n            opinion_span = (opinion['from'], opinion['to'])\n\n            if aspect_span not in aspect_entities:\n                tokens = self.tokens[aspect_span[0]:aspect_span[1]]\n                aspect_entities[aspect_span] = Entity(\n                    span=Span(\n                        tokens=tokens,\n                        indexes=list(range(aspect_span[0], aspect_span[1])),\n                        text=tokens_to_str(tokens, language=self.language),\n                    ),\n                    label=Label('aspect')\n                )\n\n            if opinion_span not in opinion_entities:\n                tokens = self.tokens[opinion_span[0]:opinion_span[1]]\n                opinion_entities[opinion_span] = Entity(\n                    span=Span(\n                        tokens=tokens,\n                        indexes=list(range(opinion_span[0], opinion_span[1])),\n                        text=tokens_to_str(tokens, language=self.language),\n                    ),\n                    label=Label('opinion')\n                )\n\n            if aspect_entities.keys() & opinion_entities.keys():\n                print(aspect_entities.keys())\n                print(opinion_entities.keys())\n\n            relations += [Relation(\n                arg1=aspect_entities[aspect_span],\n                arg2=opinion_entities[opinion_span],\n                label=Label(aspect['polarity']),\n            )]\n\n        return Sentence(\n            tokens=self.tokens,\n            entities=list(aspect_entities.values()) + list(opinion_entities.values()),\n            relations=relations,\n        )\n\n    @staticmethod\n    def load_from_file(filename, language='en') -> List[Sentence]:\n        sentence_list = list()\n        raw_instance_list = json.load(open(filename))\n        print(f\"{filename}: {len(raw_instance_list)}\")\n        for instance in raw_instance_list:\n            instance = ABSA(\n                    sentence_json=instance,\n                    language=language\n                ).generate_instance()\n            sentence_list += [instance]\n        return sentence_list\n"}
{"type": "source_file", "path": "dataset_processing/data/casie/scripts/generate_content.py", "content": "#!/usr/bin/env python\n# -*- coding:utf-8 -*-\nimport json\nimport os\nimport sys\n\n\ndef walk_folder(folder_name, end_str='.json'):\n    for root, dirs, files in os.walk(folder_name):\n        for file in files:\n            file_name = os.path.join(root, file)\n            if file_name.endswith(end_str):\n                yield file_name\n\n\ndef main(annotation_folder, content_folder):\n    if not os.path.exists(content_folder):\n        os.makedirs(content_folder, exist_ok=True)\n\n    for annotation_filename in walk_folder(annotation_folder):\n        content_filename = annotation_filename.replace(annotation_folder, content_folder).replace('.json', '.text')\n        text = open(annotation_filename).read()\n\n        if text == \"\":\n            print(\"Empty file %s\" % annotation_filename)\n            continue\n\n        try:\n            content = json.load(open(annotation_filename))['content']\n        except:\n            print(annotation_filename)\n            continue\n\n        with open(content_filename, 'w') as output:\n            output.write(content)\n\n\nif __name__ == \"__main__\":\n    main(sys.argv[1], sys.argv[2])\n"}
{"type": "source_file", "path": "dataset_processing/universal_ie/dataset.py", "content": "#!/usr/bin/env python\n# -*- coding:utf-8 -*-\nfrom universal_ie.utils import label_format\nimport yaml\nimport os\nfrom typing import Dict\nimport universal_ie.task_format as task_format\n\n\nclass Dataset:\n    def __init__(self, name: str, path: str, data_class: task_format.TaskFormat, split_dict: Dict, language: str, mapper: Dict, other: Dict = None) -> None:\n        self.name = name\n        self.path = path\n        self.data_class = data_class\n        self.split_dict = split_dict\n        self.language = language\n        self.mapper = mapper\n        self.other = other\n\n    def load_dataset(self):\n        datasets = {}\n        for split_name, filename in self.split_dict.items():\n            datasets[split_name] = self.data_class.load_from_file(\n                filename=os.path.join(self.path, filename),\n                language=self.language,\n                **self.other,\n            )\n        return datasets\n\n    @staticmethod\n    def load_yaml_file(yaml_file):\n        dataset_config = yaml.load(open(yaml_file), Loader=yaml.FullLoader)\n        if 'mapper' in dataset_config:\n            mapper = dataset_config['mapper']\n            for key in mapper:\n                mapper[key] = label_format(mapper[key])\n        else:\n            print(f\"{dataset_config['name']} without label mapper.\")\n            mapper = None\n\n        return Dataset(\n            name=dataset_config['name'],  # 数据集名字 Name of Dataset\n            path=dataset_config['path'],  # 数据集路径 Path of Dataset\n            data_class=getattr(task_format, dataset_config['data_class']),  # 数据集对应的 Task Format 名字 Raw data loader\n            split_dict=dataset_config['split'],   # 数据集不同划分文件地址 Data Split Path\n            language=dataset_config['language'],  # 数据集语言 Dataset Language\n            mapper=mapper,\n            other=dataset_config.get('other', {}),\n        )\n"}
{"type": "source_file", "path": "dataset_processing/universal_ie/task_format/__init__.py", "content": "#!/usr/bin/env python\n# -*- coding:utf-8 -*-\nfrom universal_ie.task_format.task_format import TaskFormat\nfrom universal_ie.task_format.oneie import OneIEEvent\nfrom universal_ie.task_format.jointer import JointER\nfrom universal_ie.task_format.mrc_ner import MRCNER\nfrom universal_ie.task_format.absa import ABSA\nfrom universal_ie.task_format.spannet import Spannet\nfrom universal_ie.task_format.casie import CASIE\nfrom universal_ie.task_format.cols import (\n    TokenTagCols,\n    I2b2Conll,\n    TagTokenCols,\n    TokenTagJson,\n    CoNLL03,\n)\n"}
{"type": "source_file", "path": "dataset_processing/scripts/sincere_processing.py", "content": "#!/usr/bin/env python\n# -*- coding:utf-8 -*-\nimport json\nimport os\n\n\ndef make_new_folder(folder_name):\n    if not os.path.exists(folder_name):\n        os.makedirs(folder_name)\n\n\ndef processing_sentence(sentence):\n    new_sentence = {'tokens': sentence['tokens'], 'span_pair_list': sentence['relations']}\n    span_list = list()\n    for entity in sentence['entities']:\n        span = entity\n        span['end'] -= 1\n        span_list += [span]\n    new_sentence['span_list'] = span_list\n    return new_sentence\n\n\ndef main():\n    raw_folder = \"data/sincere\"\n    new_folder = \"data/relation\"\n    for dataset_name in ['ace05', 'conll04']:\n        dataset_file_name = os.path.join(raw_folder, dataset_name) + '.json'\n\n        output_folder = os.path.join(new_folder, dataset_name)\n        make_new_folder(output_folder)\n\n        dataset = json.load(open(dataset_file_name))\n        for split_name in dataset:\n            split_filename = os.path.join(output_folder, split_name + '.jsonlines')\n\n            with open(split_filename, 'w') as output:\n                for sentence in dataset[split_name]:\n                    output.write(json.dumps(processing_sentence(sentence)) + '\\n')\n\n\nif __name__ == \"__main__\":\n    main()\n"}
{"type": "source_file", "path": "dataset_processing/universal_ie/task_format/jointer.py", "content": "#!/usr/bin/env python\n# -*- coding:utf-8 -*-\n\n\nimport json\nfrom typing import List\nfrom universal_ie.utils import tokens_to_str, change_ptb_token_back\nfrom universal_ie.ie_format import Entity, Label, Relation, Sentence, Span\nfrom universal_ie.task_format.task_format import TaskFormat\n\n\nclass JointER(TaskFormat):\n    \"\"\" Joint Entity Relation Data format at https://github.com/yubowen-ph/JointER\"\"\"\n\n    def __init__(self, sentence_json, language='en'):\n        super().__init__(\n            language=language\n        )\n        self.tokens = sentence_json['tokens']\n        for index in range(len(self.tokens)):\n            self.tokens[index] = change_ptb_token_back(self.tokens[index])\n        if self.tokens is None:\n            print('[sentence without tokens]:', sentence_json)\n            exit(1)\n        self.spo_list = sentence_json['spo_list']\n        self.spo_details = sentence_json['spo_details']\n        self.pos_tags = sentence_json['pos_tags']\n\n    def generate_instance(self):\n        entities = dict()\n        relations = dict()\n        entity_map = dict()\n\n        for spo_index, spo in enumerate(self.spo_details):\n            s_s, s_e, s_t = spo[0], spo[1], spo[2]\n            tokens = self.tokens[s_s: s_e]\n            indexes = list(range(s_s, s_e))\n            if (s_s, s_e, s_t) not in entity_map:\n                entities[(s_s, s_e, s_t)] = Entity(\n                    span=Span(\n                        tokens=tokens,\n                        indexes=indexes,\n                        text=tokens_to_str(tokens, language=self.language),\n                    ),\n                    label=Label(s_t)\n                )\n\n            o_s, o_e, o_t = spo[4], spo[5], spo[6]\n            tokens = self.tokens[o_s: o_e]\n            indexes = list(range(o_s, o_e))\n            if (o_s, o_e, o_t) not in entity_map:\n                entities[(o_s, o_e, o_t)] = Entity(\n                    span=Span(\n                        tokens=tokens,\n                        indexes=indexes,\n                        text=tokens_to_str(tokens, language=self.language),\n                    ),\n                    label=Label(o_t)\n                )\n\n            relations[spo_index] = Relation(\n                arg1=entities[(s_s, s_e, s_t)],\n                arg2=entities[(o_s, o_e, o_t)],\n                label=Label(spo[3]),\n            )\n\n        return Sentence(\n            tokens=self.tokens,\n            entities=entities.values(),\n            relations=relations.values(),\n        )\n\n    @staticmethod\n    def load_from_file(filename, language='en') -> List[Sentence]:\n        sentence_list = list()\n        raw_instance_list = json.load(open(filename))\n        print(f\"{filename}: {len(raw_instance_list)}\")\n        for instance in raw_instance_list:\n            instance = JointER(\n                    sentence_json=instance,\n                    language=language\n                ).generate_instance()\n            sentence_list += [instance]\n        return sentence_list\n"}
{"type": "source_file", "path": "dataset_processing/universal_ie/__init__.py", "content": ""}
{"type": "source_file", "path": "dataset_processing/scripts/sample_data_ratio.py", "content": "#!/usr/bin/env python\n# -*- coding:utf-8 -*-\nimport os\nimport math\nimport shutil\nimport random\nimport argparse\n\n\ndef split_ratio_file(in_filename, out_filename, ratio=0.1, seed=None):\n    lines = open(in_filename).readlines()\n    if seed:\n        random.seed(seed)\n        random.shuffle(lines)\n    lines = lines[:math.ceil(len(lines) * ratio)]\n    with open(out_filename, 'w') as output:\n        for line in lines:\n            output.write(line.strip() + '\\n')\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('-src')\n    parser.add_argument('-tgt')\n    parser.add_argument('-seed')\n    options = parser.parse_args()\n\n    source_folder = options.src\n    target_folder = options.tgt\n\n    os.makedirs(target_folder, exist_ok=True)\n\n    for ratio in [0.01, 0.05, 0.1]:\n        ratio_folder = os.path.join(target_folder, \"%s\" % ratio)\n\n        os.makedirs(ratio_folder, exist_ok=True)\n        split_ratio_file(\n            in_filename=os.path.join(source_folder, 'train.json'),\n            out_filename=os.path.join(ratio_folder, 'train.json'),\n            ratio=ratio,\n            seed=options.seed,\n        )\n        for filename in os.listdir(source_folder):\n            if filename != 'train.json':\n                shutil.copy(\n                    os.path.join(source_folder, filename),\n                    os.path.join(ratio_folder, filename),\n                )\n\n\nif __name__ == \"__main__\":\n    main()\n"}
{"type": "source_file", "path": "dataset_processing/universal_ie/task_format/oneie.py", "content": "#!/usr/bin/env python\n# -*- coding:utf-8 -*-\nimport json\nfrom typing import List\nfrom universal_ie.task_format.task_format import TaskFormat\nfrom universal_ie.utils import tokens_to_str\nfrom universal_ie.ie_format import Entity, Event, Label, Sentence, Span\n\n\n\"\"\"\n{\n  \"doc_id\": \"AFP_ENG_20030427.0118\",\n  \"sent_id\": \"AFP_ENG_20030427.0118-1\",\n  \"tokens\": [\"A\", \"Pakistani\", \"court\", \"in\", \"central\", \"Punjab\", \"province\", \"has\", \"sentenced\", \"a\", \"Christian\", \"man\", \"to\", \"life\", \"imprisonment\", \"for\", \"a\", \"blasphemy\", \"conviction\", \",\", \"police\", \"said\", \"Sunday\", \".\"], \"pieces\": [\"A\", \"Pakistani\", \"court\", \"in\", \"central\", \"Punjab\", \"province\", \"has\", \"sentenced\", \"a\", \"Christian\", \"man\", \"to\", \"life\", \"imprisonment\", \"for\", \"a\", \"b\", \"##lasp\", \"##hem\", \"##y\", \"conviction\", \",\", \"police\", \"said\", \"Sunday\", \".\"],\n  \"token_lens\": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1],\n  \"sentence\": \"A Pakistani court in central Punjab province has sentenced a Christian man to life imprisonment for a blasphemy conviction, police said Sunday.\",\n  \"entity_mentions\": [\n    {\"id\": \"AFP_ENG_20030427.0118-E15-53\", \"text\": \"Pakistani\", \"entity_type\": \"GPE\", \"mention_type\": \"NAM\", \"entity_subtype\": \"Nation\", \"start\": 1, \"end\": 2},\n    {\"id\": \"AFP_ENG_20030427.0118-E35-52\", \"text\": \"court\", \"entity_type\": \"ORG\", \"mention_type\": \"NOM\", \"entity_subtype\": \"Government\", \"start\": 2, \"end\": 3},\n    {\"id\": \"AFP_ENG_20030427.0118-E37-54\", \"text\": \"province\", \"entity_type\": \"LOC\", \"mention_type\": \"NOM\", \"entity_subtype\": \"Region-General\", \"start\": 6, \"end\": 7},\n    {\"id\": \"AFP_ENG_20030427.0118-E27-48\", \"text\": \"Christian\", \"entity_type\": \"PER\", \"mention_type\": \"NOM\", \"entity_subtype\": \"Group\", \"start\": 10, \"end\": 11},\n    {\"id\": \"AFP_ENG_20030427.0118-E38-55\", \"text\": \"man\", \"entity_type\": \"PER\", \"mention_type\": \"NOM\", \"entity_subtype\": \"Individual\", \"start\": 11, \"end\": 12},\n    {\"id\": \"AFP_ENG_20030427.0118-E39-56\", \"text\": \"police\", \"entity_type\": \"PER\", \"mention_type\": \"NOM\", \"entity_subtype\": \"Group\", \"start\": 20, \"end\": 21}],\n  \"relation_mentions\": [\n    {\"id\": \"AFP_ENG_20030427.0118-R1-1\", \"relation_type\": \"GEN-AFF\", \"relation_subtype\": \"GEN-AFF:Citizen-Resident-Religion-Ethnicity\",\n      \"arguments\": [\n        {\"entity_id\": \"AFP_ENG_20030427.0118-E38-55\", \"text\": \"man\", \"role\": \"Arg-1\"}, \n        {\"entity_id\": \"AFP_ENG_20030427.0118-E27-48\", \"text\": \"Christian\", \"role\": \"Arg-2\"}\n      ]\n    },\n    {\"id\": \"AFP_ENG_20030427.0118-R3-1\", \"relation_type\": \"PART-WHOLE\", \"relation_subtype\": \"PART-WHOLE:Subsidiary\",\n      \"arguments\": [\n        {\"entity_id\": \"AFP_ENG_20030427.0118-E35-52\", \"text\": \"court\", \"role\": \"Arg-1\"},\n        {\"entity_id\": \"AFP_ENG_20030427.0118-E15-53\", \"text\": \"Pakistani\", \"role\": \"Arg-2\"}\n      ]\n    },\n    {\"id\": \"AFP_ENG_20030427.0118-R4-1\", \"relation_type\": \"GEN-AFF\", \"relation_subtype\": \"GEN-AFF:Org-Location\",\n      \"arguments\": [\n        {\"entity_id\": \"AFP_ENG_20030427.0118-E35-52\", \"text\": \"court\", \"role\": \"Arg-1\"},\n        {\"entity_id\": \"AFP_ENG_20030427.0118-E37-54\", \"text\": \"province\", \"role\": \"Arg-2\"}\n      ]\n    }\n  ],\n  \"event_mentions\": [\n    {\"id\": \"AFP_ENG_20030427.0118-EV1-1\", \"event_type\": \"Justice:Sentence\",\n      \"trigger\": {\"text\": \"sentenced\", \"start\": 8, \"end\": 9},\n      \"arguments\": [\n        {\"entity_id\": \"AFP_ENG_20030427.0118-E35-52\", \"text\": \"court\", \"role\": \"Adjudicator\"},\n        {\"entity_id\": \"AFP_ENG_20030427.0118-E38-55\", \"text\": \"man\", \"role\": \"Defendant\"},\n        {\"entity_id\": \"AFP_ENG_20030427.0118-E37-54\", \"text\": \"province\", \"role\": \"Place\"}\n    ]},\n    {\"id\": \"AFP_ENG_20030427.0118-EV2-1\", \"event_type\": \"Justice:Convict\",\n      \"trigger\": {\"text\": \"conviction\", \"start\": 18, \"end\": 19},\n      \"arguments\": [{\"entity_id\": \"AFP_ENG_20030427.0118-E38-55\", \"text\": \"man\", \"role\": \"Defendant\"}\n    ]}\n]}\n\"\"\"\n\n\nclass OneIEEvent(TaskFormat):\n    def __init__(self, doc_json, language='en'):\n        super().__init__(\n            language=language\n        )\n        self.doc_id = doc_json['doc_id']\n        self.sent_id = doc_json['sent_id']\n        self.tokens = doc_json['tokens']\n        self.entities = doc_json['entity_mentions']\n        self.relations = doc_json['relation_mentions']\n        self.events = doc_json['event_mentions']\n\n    def generate_instance(self):\n        events = dict()\n        entities = dict()\n\n        for span_index, span in enumerate(self.entities):\n            tokens = self.tokens[span['start']: span['end']]\n            indexes = list(range(span['start'], span['end']))\n            entities[span['id']] = Entity(\n                span=Span(\n                    tokens=tokens,\n                    indexes=indexes,\n                    text=tokens_to_str(tokens, language=self.language),\n                    text_id=self.sent_id\n                ),\n                label=Label(span['entity_type']),\n                text_id=self.sent_id,\n                record_id=span['id']\n            )\n\n        for event_index, event in enumerate(self.events):\n            start = event['trigger']['start']\n            end = event['trigger']['end']\n            tokens = self.tokens[start:end]\n            indexes = list(range(start, end))\n            events[event['id']] = Event(\n                span=Span(\n                    tokens=tokens,\n                    indexes=indexes,\n                    text=tokens_to_str(tokens, language=self.language),\n                    text_id=self.sent_id\n                ),\n                label=Label(event['event_type']),\n                args=[(Label(x['role']), entities[x['entity_id']])\n                      for x in event['arguments']],\n                text_id=self.sent_id,\n                record_id=event['id']\n            )\n\n        return Sentence(\n            tokens=self.tokens,\n            entities=list(),\n            relations=list(),\n            events=events.values(),\n            text_id=self.sent_id\n        )\n\n    @staticmethod\n    def load_from_file(filename, language='en') -> List[Sentence]:\n        sentence_list = list()\n        with open(filename) as fin:\n            for line in fin:\n                instance = OneIEEvent(\n                    json.loads(line.strip()),\n                    language=language\n                ).generate_instance()\n                sentence_list += [instance]\n        return sentence_list\n"}
{"type": "source_file", "path": "dataset_processing/data/casie/scripts/split_data.py", "content": "#!/usr/bin/env python\n# -*- coding:utf-8 -*-\nimport os\nimport json\n\n\ndef main():\n    data_file = \"raw_data/casie.jsonlines\"\n    output_folder = \"data\"\n\n    if not os.path.exists(output_folder):\n        os.makedirs(output_folder)\n\n    instance_list = [json.loads(line) for line in open(data_file)]\n\n    instance_list = [(instance['info']['date'], instance['id'], instance) for instance in instance_list]\n    instance_list.sort(reverse=True)\n\n    data_split = {\n        'test': (0, 200),\n        'dev': (200, 300),\n        'train': (300, len(instance_list)),\n    }\n\n    for split_name, (start, end) in data_split.items():\n        with open(os.path.join(output_folder, '%s.jsonlines' % split_name), 'w') as output:\n            for instance in instance_list[start: end]:\n                output.write(json.dumps(instance[2]) + '\\n')\n\n\nif __name__ == \"__main__\":\n    main()\n"}
{"type": "source_file", "path": "dataset_processing/scripts/data_statistics.py", "content": "import json\nimport os\nimport sys\nfrom collections import Counter\nimport tabulate\n\n\ndef count_line_in_file(filename):\n    return sum([1 for _ in open(filename)])\n\n\ndef count_record_in_file(filename, key):\n    counter = Counter()\n    for line in open(filename):\n        instance = json.loads(line)\n        counter.update([key + ' entity'] * len(instance['entity']))\n        counter.update([key + ' relation'] * len(instance['relation']))\n        counter.update([key + ' event'] * len(instance['event']))\n        for event in instance['event']:\n            counter.update([key + ' role'] * len(event['args']))\n    return counter\n\n\ndef count_folder(folder_name):\n    data_map = {\n        'train': 'train.json',\n        'val': 'val.json',\n        'test': 'test.json',\n    }\n    intance_counter = {'name': folder_name}\n    for key, name in data_map.items():\n        filename = f\"{folder_name}/{name}\"\n        if not os.path.exists(filename):\n            sys.stderr.write(f'[warn] {filename} not exists.\\n')\n            continue\n        intance_counter[key] = count_line_in_file(filename)\n        intance_counter.update(count_record_in_file(filename, key))\n\n    for key in ['entity', 'relation', 'event']:\n        filename = f\"{folder_name}/{key}.schema\"\n        if not os.path.exists(filename):\n            sys.stderr.write(f'[warn] {filename} not exists.\\n')\n            intance_counter[key] = 0\n            continue\n        intance_counter[key] = len(json.loads(open(filename).readline()))\n\n    return intance_counter\n\n\ndef walk_dir(folder_name):\n\n    for root, dirs, files in os.walk(folder_name):\n        for file in dirs:\n            folder_name = os.path.join(root, file)\n            if os.path.exists(f\"{os.path.join(root, file)}/record.schema\"):\n                yield os.path.join(root, file)\n\n\ndef main():\n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument('-data')\n    parser.add_argument('-f', dest='format', default='simple')\n    options = parser.parse_args()\n\n    folder_list = list()\n\n    for folder_name in walk_dir(options.data):\n        if 'shot' in folder_name or 'ratio' in folder_name:\n            continue\n        folder_list += [count_folder(folder_name)]\n\n    col_name = ['name',\n                'entity', 'relation', 'event',\n                'train', 'val', 'test',\n                'train entity', 'train relation', 'train event', 'train role',\n                'val entity', 'val relation', 'val event', 'val role',\n                'test entity', 'test relation', 'test event', 'test role',\n                ]\n    table = []\n    for data_info in folder_list:\n        row = [data_info.get(col, 0) for col in col_name]\n        table += [row]\n    table.sort()\n    print(\n        tabulate.tabulate(\n            tabular_data=table,\n            headers=col_name,\n            tablefmt=options.format,\n        )\n    )\n\n\nif __name__ == \"__main__\":\n    main()\n"}
{"type": "source_file", "path": "dataset_processing/universal_ie/generation_format/__init__.py", "content": "#!/usr/bin/env python\n# -*- coding:utf-8 -*-\nfrom universal_ie.generation_format.text2spotasoc import Text2SpotAsoc\n\n\ngeneration_format_dict = {\n    'spotasoc': Text2SpotAsoc\n}\n"}
{"type": "source_file", "path": "dataset_processing/universal_ie/generation_format/text2spotasoc.py", "content": "#!/usr/bin/env python\n# -*- coding:utf-8 -*-\nfrom collections import defaultdict\nfrom typing import List, Dict\nfrom universal_ie.utils import tokens_to_str\nfrom universal_ie.generation_format.generation_format import GenerationFormat, StructureMarker\nfrom universal_ie.ie_format import Entity, Event, Label, Relation, Span\n\n\ndef convert_spot_asoc(spot_asoc_instance, structure_maker):\n    spot_instance_str_rep_list = list()\n    for spot in spot_asoc_instance:\n        spot_str_rep = [\n            spot['label'],\n            structure_maker.target_span_start,\n            spot['span'],\n        ]\n        for asoc_label, asoc_span in spot.get('asoc', list()):\n            asoc_str_rep = [\n                structure_maker.span_start,\n                asoc_label,\n                structure_maker.target_span_start,\n                asoc_span,\n                structure_maker.span_end,\n            ]\n            spot_str_rep += [' '.join(asoc_str_rep)]\n        spot_instance_str_rep_list += [' '.join([\n            structure_maker.record_start,\n            ' '.join(spot_str_rep),\n            structure_maker.record_end,\n        ])]\n    target_text = ' '.join([\n        structure_maker.sent_start,\n        ' '.join(spot_instance_str_rep_list),\n        structure_maker.sent_end,\n    ])\n    return target_text\n\n\nclass Text2SpotAsoc(GenerationFormat):\n    def __init__(self, structure_maker: StructureMarker, label_mapper: Dict = None, language: str = 'en') -> None:\n        super().__init__(\n            structure_maker=structure_maker,\n            label_mapper=label_mapper,\n            language=language\n        )\n\n    def annotate_entities(self, tokens: List[str], entities: List[Entity]):\n        \"\"\" Convert Entities\n\n        Args:\n            tokens (List[str]): ['Trump', 'visits', 'China', '.']\n            entities (List[Entity]): [description]\n\n        Returns:\n            source (str): Trump visits China.\n            target (str): { [ Person : Trump ] [ Geo-political : China ] }\n        \"\"\"\n        return self.annonote_graph(tokens=tokens, entities=entities)[:2]\n\n    def augment_source_span(self, tokens: List[str], span: Span):\n        \"\"\"[summary]\n\n        Args:\n            tokens (List[str]):\n                ['Trump', 'visits', 'China', '.']\n            span (Span):\n                Trump\n\n        Returns:\n            [type]:\n                ['(', 'Trump', ')', 'visits', 'China', '.']\n        \"\"\"\n        return tokens[:span.indexes[0]] \\\n            + [self.structure_maker.source_span_start] \\\n            + tokens[span.indexes[0]:span.indexes[-1] + 1] \\\n            + [self.structure_maker.source_span_end] \\\n            + tokens[span.indexes[-1] + 1:]\n\n    def annotate_given_entities(self, tokens: List[str], entities):\n        \"\"\"\n        entityies is List\n        :param tokens:\n            ['Trump', 'visits', 'China', '.']\n        :param entities:\n            ['Trump', 'China']\n        :return:\n            source (str): ( Trump ) ( China ) : Trump visits China .\n            target (str): { [ Person : Trump ] [ Geo-political : China ] }\n\n        entityies is Entity\n        :param tokens:\n            ['Trump', 'visits', 'China', '.']\n        :param entities:\n            'Trump'\n        :return:\n            source (str): < Trump > visits China .\n            target (str): { [ Person : Trump ] }\n        \"\"\"\n        if isinstance(entities, list):\n            entitytokens = []\n            for entity in entities:\n                entitytokens += [self.structure_maker.span_start]\n                entitytokens += entity.span.tokens\n                entitytokens += [self.structure_maker.span_end]\n            source_text = tokens_to_str(\n                entitytokens + [self.structure_maker.sep_marker] + tokens,\n                language=self.language,\n            )\n            _, target_text = self.annonote_graph(tokens=tokens, entities=entities)[:2]\n\n        elif isinstance(entities, Entity):\n            marked_tokens = self.augment_source_span(tokens=tokens, span=entities.span)\n            source_text = tokens_to_str(marked_tokens, language=self.language)\n            _, target_text = self.annonote_graph(tokens=tokens, entities=[entities])[:2]\n\n        return source_text, target_text\n\n    def annotate_events(self, tokens: List[str], events: List[Event]):\n        \"\"\"\n        :param tokens:\n            ['Trump', 'visits', 'China', '.']\n        :param events:\n\n        :return:\n            source (str): Trump visits China.\n            target (str): { [ Visit : visits ( Person : Trump ) ( Location : China ) ] }\n        \"\"\"\n        return self.annonote_graph(tokens=tokens, events=events)[:2]\n\n    def annotate_event_given_predicate(self, tokens: List[str], event: Event):\n        \"\"\"Annotate Event Given Predicate\n\n        Args:\n            tokens (List[str]):\n                ['Trump', 'visits', 'China', '.']\n            event (Event): Given Predicate\n\n        Returns:\n            [type]: [description]\n        \"\"\"\n        marked_tokens = self.augment_source_span(tokens=tokens, span=event.span)\n        source_text = tokens_to_str(marked_tokens, language=self.language)\n        _, target_text = self.annonote_graph(tokens=tokens, events=[event])[:2]\n        return source_text, target_text\n\n    def annotate_relation_extraction(self,\n                                     tokens: List[str],\n                                     relations: List[Relation]):\n        \"\"\"\n        :param tokens:\n            ['Trump', 'visits', 'China', '.']\n        :param relations:\n\n        :return:\n            source (str): Trump visits China.\n            target (str): { [ Person : Trump ( Visit : China ) ] }\n        \"\"\"\n        return self.annonote_graph(tokens=tokens, relations=relations)[:2]\n\n    def annotate_entities_and_relation_extraction(self,\n                                                  tokens: List[str],\n                                                  entities: List[Entity],\n                                                  relations: List[Relation]):\n        \"\"\"\n        :param tokens:\n            ['Trump', 'visits', 'China', '.']\n        :param relations:\n\n        :return:\n            source (str): Trump visits China.\n            target (str): { [ Person : Trump ( Visit : China ) ] [ Geo-political : China ] }\n        \"\"\"\n        return self.annonote_graph(tokens=tokens, entities=entities, relations=relations)[:2]\n\n    def annonote_graph(self,\n                       tokens: List[str],\n                       entities: List[Entity] = [],\n                       relations: List[Relation] = [],\n                       events: List[Event] = []):\n        \"\"\"Convert Entity Relation Event to Spot-Assocation Graph\n\n        Args:\n            tokens (List[str]): Token List\n            entities (List[Entity], optional): Entity List. Defaults to [].\n            relations (List[Relation], optional): Relation List. Defaults to [].\n            events (List[Event], optional): Event List. Defaults to [].\n\n        Returns:\n            str: [description]\n                {\n                    [ Person : Trump ( Visit : China ) ]\n                    [ Visit : visits ( Person : Trump ) ( Location : China ) ]\n                    [ Geo-political : China ]\n                }\n            set: Set of Spot\n            set: Set of Asoc\n        \"\"\"\n        spot_dict = dict()\n        asoc_dict = defaultdict(list)\n        spot_str_rep_list = list()\n\n        def add_spot(spot):\n            spot_key = (tuple(spot.span.indexes), self.get_label_str(spot.label))\n            spot_dict[spot_key] = spot\n\n            if self.get_label_str(spot.label) not in self.record_role_map:\n                self.record_role_map[self.get_label_str(spot.label)] = set()\n\n        def add_asoc(spot, asoc: Label, tail):\n            spot_key = (tuple(spot.span.indexes), self.get_label_str(spot.label))\n            asoc_dict[spot_key] += [(tail.span.indexes, tail, self.get_label_str(asoc))]\n\n            self.record_role_map[self.get_label_str(spot.label)].add(self.get_label_str(asoc))\n\n        for entity in entities:\n            add_spot(spot=entity)\n\n        for relation in relations:\n            add_spot(spot=relation.arg1)\n            add_asoc(spot=relation.arg1, asoc=relation.label, tail=relation.arg2)\n\n        for event in events:\n            add_spot(spot=event)\n            for arg_role, argument in event.args:\n                add_asoc(spot=event, asoc=arg_role, tail=argument)\n\n        spot_asoc_instance = list()\n        for spot_key in sorted(spot_dict.keys()):\n            offset, label = spot_key\n\n            if spot_dict[spot_key].span.is_empty_span():\n                continue\n\n            spot_instance = {'span': spot_dict[spot_key].span.text,\n                             'label': label,\n                             'asoc': list(),\n                             }\n            for _, tail, asoc in sorted(asoc_dict.get(spot_key, [])):\n\n                if tail.span.is_empty_span():\n                    continue\n\n                spot_instance['asoc'] += [(asoc, tail.span.text)]\n            spot_asoc_instance += [spot_instance]\n\n        target_text = convert_spot_asoc(\n            spot_asoc_instance,\n            structure_maker=self.structure_maker,\n        )\n\n        source_text = tokens_to_str(tokens, language=self.language)\n        spot_labels = set([label for _, label in spot_dict.keys()])\n        asoc_labels = set()\n        for _, asoc_list in asoc_dict.items():\n            for _, _, asoc in asoc_list:\n                asoc_labels.add(asoc)\n        return source_text, target_text, spot_labels, asoc_labels, spot_asoc_instance\n"}
{"type": "source_file", "path": "dataset_processing/universal_ie/generation_format/structure_marker.py", "content": "#!/usr/bin/env python\n# -*- coding:utf-8 -*-\n# 结构标记符\n\n\nclass StructureMarker:\n    def __init__(self) -> None:\n        pass\n\n\nclass BaseStructureMarker(StructureMarker):\n    def __init__(self) -> None:\n        super().__init__()\n        self.sent_start = '<extra_id_0>'\n        self.sent_end = '<extra_id_1>'\n        self.record_start = '<extra_id_0>'\n        self.record_end = '<extra_id_1>'\n        self.span_start = '<extra_id_0>'\n        self.span_end = '<extra_id_1>'\n        self.sep_marker = '<extra_id_2>'\n        self.source_span_start = '<extra_id_3>'\n        self.source_span_end = '<extra_id_4>'\n        self.target_span_start = '<extra_id_5>'\n\n\nclass VisualStructureMarker(StructureMarker):\n    def __init__(self) -> None:\n        super().__init__()\n        self.sent_start = '{'\n        self.sent_end = '}'\n        self.record_start = '['\n        self.record_end = ']'\n        self.span_start = '('\n        self.span_end = ')'\n        self.source_span_start = '<'\n        self.source_span_end = '>'\n        self.target_span_start = ':'\n        self.sep_marker = ':'\n"}
{"type": "source_file", "path": "dataset_processing/universal_ie/record_schema.py", "content": "#!/usr/bin/env python\n# -*- coding:utf-8 -*-\nimport json\nfrom collections import defaultdict\nfrom typing import List\n\n\nclass RecordSchema:\n    def __init__(self, type_list, role_list, type_role_dict):\n        self.type_list = type_list\n        self.role_list = role_list\n        self.type_role_dict = type_role_dict\n\n    @staticmethod\n    def read_from_file(filename):\n        lines = open(filename).readlines()\n        type_list = json.loads(lines[0])\n        role_list = json.loads(lines[1])\n        type_role_dict = json.loads(lines[2])\n        return RecordSchema(type_list, role_list, type_role_dict)\n\n    def write_to_file(self, filename):\n        with open(filename, 'w') as output:\n            output.write(json.dumps(self.type_list, ensure_ascii=False) + '\\n')\n            output.write(json.dumps(self.role_list, ensure_ascii=False) + '\\n')\n            output.write(json.dumps(self.type_role_dict, ensure_ascii=False) + '\\n')\n\n\ndef merge_schema(schema_list: List[RecordSchema]):\n    type_set = set()\n    role_set = set()\n    type_role_dict = defaultdict(list)\n\n    for schema in schema_list:\n\n        for type_name in schema.type_list:\n            type_set.add(type_name)\n\n        for role_name in schema.role_list:\n            role_set.add(role_name)\n\n        for type_name in schema.type_role_dict:\n            type_role_dict[type_name] += schema.type_role_dict[type_name]\n\n    for type_name in type_role_dict:\n        type_role_dict[type_name] = list(set(type_role_dict[type_name]))\n\n    return RecordSchema(type_list=list(type_set),\n                        role_list=list(role_set),\n                        type_role_dict=type_role_dict,\n                        )\n"}
{"type": "source_file", "path": "dataset_processing/universal_ie/task_format/spannet.py", "content": "#!/usr/bin/env python\n# -*- coding:utf-8 -*-\nfrom collections import Counter\nimport json\nfrom typing import List, Dict\nfrom universal_ie.task_format.task_format import TaskFormat\nfrom universal_ie.utils import change_ptb_token_back, tokens_to_str\nfrom universal_ie.ie_format import Entity, Label, Relation, Sentence, Span\nfrom tqdm import tqdm\n\n\nclass Spannet(TaskFormat):\n    \"\"\"\n    {\n        \"tokens\": [\"An\", \"art\", \"exhibit\", \"at\", \"the\", \"Hakawati\", \"Theatre\",\n                   \"in\", \"Arab\", \"east\", \"Jerusalem\", \"was\", \"a\", \"series\",\n                   \"of\", \"portraits\", \"of\", \"Palestinians\", \"killed\", \"in\",\n                   \"the\", \"rebellion\", \".\"],\n        \"span_pair_list\": [\n            {\"type\": \"OrgBased_In\", \"head\": 0, \"tail\": 2}\n        ],\n        \"span_list\": [\n            {\"type\": \"Org\", \"start\": 5, \"end\": 6},\n            {\"type\": \"Other\", \"start\": 8, \"end\": 8},\n            {\"type\": \"Loc\", \"start\": 10, \"end\": 10},\n            {\"type\": \"Other\", \"start\": 17, \"end\": 17}\n        ]\n    }\n    \"\"\"\n    def __init__(self, instance_json: Dict, language='en') -> None:\n        super().__init__(\n            language=language\n        )\n        self.tokens = change_ptb_token_back(instance_json['tokens'])\n        self.span_list = instance_json.get('span_list', [])\n        self.span_pair_list = instance_json.get('span_pair_list', [])\n        self.instance_id = instance_json.get('id', None)\n\n    def generate_instance(self):\n        entities = list()\n        relations = list()\n        for span_index, span in enumerate(self.span_list):\n            tokens = self.tokens[span['start']: span['end'] + 1]\n            indexes = list(range(span['start'], span['end'] + 1))\n            entities += [\n                Entity(\n                    span=Span(\n                        tokens=tokens,\n                        indexes=indexes,\n                        text=tokens_to_str(tokens, language=self.language),\n                        text_id=self.instance_id\n                    ),\n                    label=Label(span['type']),\n                    text_id=self.instance_id,\n                    record_id=self.instance_id + \"#%s\" % span_index if self.instance_id else None)\n            ]\n        for spanpair_index, span_pair in enumerate(self.span_pair_list):\n            relations += [\n                Relation(\n                    arg1=entities[span_pair['head']],\n                    arg2=entities[span_pair['tail']],\n                    label=Label(span_pair['type']),\n                    text_id=self.instance_id,\n                    record_id=self.instance_id + \"##%s\" % spanpair_index if self.instance_id else None\n                )\n            ]\n        return Sentence(tokens=self.tokens,\n                        entities=entities,\n                        relations=relations,\n                        text_id=self.instance_id)\n\n    @staticmethod\n    def load_from_file(filename, language='en') -> List[Sentence]:\n        sentence_list = list()\n        counter = Counter()\n        with open(filename) as fin:\n            for line in tqdm(fin):\n                spannet = Spannet(\n                    json.loads(line.strip()),\n                    language=language\n                )\n                instance = spannet.generate_instance()\n                sentence_list += [instance]\n                counter.update(['sentence'])\n                counter.update(['span'] * len(spannet.span_list))\n        print(filename, counter)\n        return sentence_list\n"}
{"type": "source_file", "path": "dataset_processing/universal_ie/task_format/cols.py", "content": "#!/usr/bin/env python\n# -*- coding:utf-8 -*-\nfrom collections import Counter\nimport json\nfrom typing import List, Optional, Tuple, Set\nfrom tqdm import tqdm\nfrom universal_ie.task_format.task_format import TaskFormat\nfrom universal_ie.utils import tokens_to_str\nfrom universal_ie.ie_format import Entity, Label, Sentence, Span\n\n\n# https://github.com/allenai/allennlp/blob/main/allennlp/data/dataset_readers/dataset_utils/span_utils.py\n# ### Start Code\ndef bio_tags_to_spans(\n    tag_sequence: List[str], classes_to_ignore: List[str] = None\n) -> List[Tuple[str, Tuple[int, int]]]:\n    \"\"\"\n    Given a sequence corresponding to BIO tags, extracts spans.\n    Spans are inclusive and can be of zero length, representing a single word span.\n    Ill-formed spans are also included (i.e those which do not start with a \"B-LABEL\"),\n    as otherwise it is possible to get a perfect precision score whilst still predicting\n    ill-formed spans in addition to the correct spans. This function works properly when\n    the spans are unlabeled (i.e., your labels are simply \"B\", \"I\", and \"O\").\n    # Parameters\n    tag_sequence : `List[str]`, required.\n        The integer class labels for a sequence.\n    classes_to_ignore : `List[str]`, optional (default = `None`).\n        A list of string class labels `excluding` the bio tag\n        which should be ignored when extracting spans.\n    # Returns\n    spans : `List[TypedStringSpan]`\n        The typed, extracted spans from the sequence, in the format (label, (span_start, span_end)).\n        Note that the label `does not` contain any BIO tag prefixes.\n    \"\"\"\n    classes_to_ignore = classes_to_ignore or []\n    spans: Set[Tuple[str, Tuple[int, int]]] = set()\n    span_start = 0\n    span_end = 0\n    active_conll_tag = None\n    for index, string_tag in enumerate(tag_sequence):\n        # Actual BIO tag.\n        bio_tag = string_tag[0]\n        if bio_tag not in [\"B\", \"I\", \"O\"]:\n            raise RuntimeError('Invalid tag sequence %s' % tag_sequence)\n        conll_tag = string_tag[2:]\n        if bio_tag == \"O\" or conll_tag in classes_to_ignore:\n            # The span has ended.\n            if active_conll_tag is not None:\n                spans.add((active_conll_tag, (span_start, span_end)))\n            active_conll_tag = None\n            # We don't care about tags we are\n            # told to ignore, so we do nothing.\n            continue\n        elif bio_tag == \"B\":\n            # We are entering a new span; reset indices\n            # and active tag to new span.\n            if active_conll_tag is not None:\n                spans.add((active_conll_tag, (span_start, span_end)))\n            active_conll_tag = conll_tag\n            span_start = index\n            span_end = index\n        elif bio_tag == \"I\" and conll_tag == active_conll_tag:\n            # We're inside a span.\n            span_end += 1\n        else:\n            # This is the case the bio label is an \"I\", but either:\n            # 1) the span hasn't started - i.e. an ill formed span.\n            # 2) The span is an I tag for a different conll annotation.\n            # We'll process the previous span if it exists, but also\n            # include this span. This is important, because otherwise,\n            # a model may get a perfect F1 score whilst still including\n            # false positive ill-formed spans.\n            if active_conll_tag is not None:\n                spans.add((active_conll_tag, (span_start, span_end)))\n            active_conll_tag = conll_tag\n            span_start = index\n            span_end = index\n    # Last token might have been a part of a valid span.\n    if active_conll_tag is not None:\n        spans.add((active_conll_tag, (span_start, span_end)))\n    return list(spans)\n\n\ndef _iob1_start_of_chunk(\n    prev_bio_tag: Optional[str],\n    prev_conll_tag: Optional[str],\n    curr_bio_tag: str,\n    curr_conll_tag: str,\n) -> bool:\n    if curr_bio_tag == \"B\":\n        return True\n    if curr_bio_tag == \"I\" and prev_bio_tag == \"O\":\n        return True\n    if curr_bio_tag != \"O\" and prev_conll_tag != curr_conll_tag:\n        return True\n    return False\n\n\ndef iob1_tags_to_spans(\n    tag_sequence: List[str], classes_to_ignore: List[str] = None\n) -> List[Tuple[str, Tuple[int, int]]]:\n    \"\"\"\n    Given a sequence corresponding to IOB1 tags, extracts spans.\n    Spans are inclusive and can be of zero length, representing a single word span.\n    Ill-formed spans are also included (i.e., those where \"B-LABEL\" is not preceded\n    by \"I-LABEL\" or \"B-LABEL\").\n    # Parameters\n    tag_sequence : `List[str]`, required.\n        The integer class labels for a sequence.\n    classes_to_ignore : `List[str]`, optional (default = `None`).\n        A list of string class labels `excluding` the bio tag\n        which should be ignored when extracting spans.\n    # Returns\n    spans : `List[TypedStringSpan]`\n        The typed, extracted spans from the sequence, in the format (label, (span_start, span_end)).\n        Note that the label `does not` contain any BIO tag prefixes.\n    \"\"\"\n    classes_to_ignore = classes_to_ignore or []\n    spans: Set[Tuple[str, Tuple[int, int]]] = set()\n    span_start = 0\n    span_end = 0\n    active_conll_tag = None\n    prev_bio_tag = None\n    prev_conll_tag = None\n    for index, string_tag in enumerate(tag_sequence):\n        curr_bio_tag = string_tag[0]\n        curr_conll_tag = string_tag[2:]\n\n        if curr_bio_tag not in [\"B\", \"I\", \"O\"]:\n            raise RuntimeError('Invalid tag sequence %s' % tag_sequence)\n        if curr_bio_tag == \"O\" or curr_conll_tag in classes_to_ignore:\n            # The span has ended.\n            if active_conll_tag is not None:\n                spans.add((active_conll_tag, (span_start, span_end)))\n            active_conll_tag = None\n        elif _iob1_start_of_chunk(prev_bio_tag, prev_conll_tag, curr_bio_tag, curr_conll_tag):\n            # We are entering a new span; reset indices\n            # and active tag to new span.\n            if active_conll_tag is not None:\n                spans.add((active_conll_tag, (span_start, span_end)))\n            active_conll_tag = curr_conll_tag\n            span_start = index\n            span_end = index\n        else:\n            # bio_tag == \"I\" and curr_conll_tag == active_conll_tag\n            # We're continuing a span.\n            span_end += 1\n\n        prev_bio_tag = string_tag[0]\n        prev_conll_tag = string_tag[2:]\n    # Last token might have been a part of a valid span.\n    if active_conll_tag is not None:\n        spans.add((active_conll_tag, (span_start, span_end)))\n    return list(spans)\n\n\ndef bmes_tags_to_spans(\n    tag_sequence: List[str], classes_to_ignore: List[str] = None\n) -> List[Tuple[str, Tuple[int, int]]]:\n    \"\"\"\n    Given a sequence corresponding to BMES tags, extracts spans.\n    Spans are inclusive and can be of zero length, representing a single word span.\n    Ill-formed spans are also included (i.e those which do not start with a \"B-LABEL\"),\n    as otherwise it is possible to get a perfect precision score whilst still predicting\n    ill-formed spans in addition to the correct spans.\n    This function works properly when the spans are unlabeled (i.e., your labels are\n    simply \"B\", \"M\", \"E\" and \"S\").\n    # Parameters\n    tag_sequence : `List[str]`, required.\n        The integer class labels for a sequence.\n    classes_to_ignore : `List[str]`, optional (default = `None`).\n        A list of string class labels `excluding` the bio tag\n        which should be ignored when extracting spans.\n    # Returns\n    spans : `List[TypedStringSpan]`\n        The typed, extracted spans from the sequence, in the format (label, (span_start, span_end)).\n        Note that the label `does not` contain any BIO tag prefixes.\n    \"\"\"\n\n    def extract_bmes_tag_label(text):\n        bmes_tag = text[0]\n        label = text[2:]\n        return bmes_tag, label\n\n    spans: List[Tuple[str, List[int]]] = []\n    prev_bmes_tag: Optional[str] = None\n    for index, tag in enumerate(tag_sequence):\n        bmes_tag, label = extract_bmes_tag_label(tag)\n        if bmes_tag in (\"B\", \"S\"):\n            # Regardless of tag, we start a new span when reaching B & S.\n            spans.append((label, [index, index]))\n        elif bmes_tag in (\"M\", \"E\") and prev_bmes_tag in (\"B\", \"M\") and spans[-1][0] == label:\n            # Only expand the span if\n            # 1. Valid transition: B/M -> M/E.\n            # 2. Matched label.\n            spans[-1][1][1] = index\n        else:\n            # Best effort split for invalid span.\n            spans.append((label, [index, index]))\n        # update previous BMES tag.\n        prev_bmes_tag = bmes_tag\n\n    classes_to_ignore = classes_to_ignore or []\n    return [\n        # to tuple.\n        (span[0], (span[1][0], span[1][1]))\n        for span in spans\n        if span[0] not in classes_to_ignore\n    ]\n\n\ndef bioul_tags_to_spans(\n    tag_sequence: List[str], classes_to_ignore: List[str] = None\n) -> List[Tuple[str, Tuple[int, int]]]:\n    \"\"\"\n    Given a sequence corresponding to BIOUL tags, extracts spans.\n    Spans are inclusive and can be of zero length, representing a single word span.\n    Ill-formed spans are not allowed and will raise `InvalidTagSequence`.\n    This function works properly when the spans are unlabeled (i.e., your labels are\n    simply \"B\", \"I\", \"O\", \"U\", and \"L\").\n    # Parameters\n    tag_sequence : `List[str]`, required.\n        The tag sequence encoded in BIOUL, e.g. [\"B-PER\", \"L-PER\", \"O\"].\n    classes_to_ignore : `List[str]`, optional (default = `None`).\n        A list of string class labels `excluding` the bio tag\n        which should be ignored when extracting spans.\n    # Returns\n    spans : `List[TypedStringSpan]`\n        The typed, extracted spans from the sequence, in the format (label, (span_start, span_end)).\n    \"\"\"\n    spans = []\n    classes_to_ignore = classes_to_ignore or []\n    index = 0\n    while index < len(tag_sequence):\n        label = tag_sequence[index]\n        if label[0] == \"U\":\n            spans.append((label.partition(\"-\")[2], (index, index)))\n        elif label[0] == \"B\":\n            start = index\n            while label[0] != \"L\":\n                index += 1\n                if index >= len(tag_sequence):\n                    raise RuntimeError('Invalid tag sequence %s' % tag_sequence)\n                    # raise InvalidTagSequence(tag_sequence)\n                label = tag_sequence[index]\n                if not (label[0] == \"I\" or label[0] == \"L\"):\n                    raise RuntimeError('Invalid tag sequence %s' % tag_sequence)\n                    # raise InvalidTagSequence(tag_sequence)\n            spans.append((label.partition(\"-\")[2], (start, index)))\n        else:\n            if label != \"O\":\n                raise RuntimeError('Invalid tag sequence %s' % tag_sequence)\n                # raise InvalidTagSequence(tag_sequence)\n        index += 1\n    return [span for span in spans if span[0] not in classes_to_ignore]\n\n\ndef bmeso_tags_to_spans(\n    tag_sequence: List[str], classes_to_ignore: List[str] = None\n) -> List[Tuple[str, Tuple[int, int]]]:\n    \"\"\"\n    bmeso -> bioul\n    B = Beginning\n    I/M = Inside / Middle\n    L/E = Last / End\n    O = Outside\n    U/W/S = Unit-length / Whole / Singleton\n    \"\"\"\n    new_tag = list()\n    for label in tag_sequence:\n        if label[0] == 'M':\n            new_tag += ['I-' + label.partition(\"-\")[2]]\n        elif label[0] == 'E':\n            new_tag += ['L-' + label.partition(\"-\")[2]]\n        elif label[0] == 'S':\n            new_tag += ['U-' + label.partition(\"-\")[2]]\n        else:\n            new_tag += [label]\n\n    return bioul_tags_to_spans(tag_sequence=new_tag, classes_to_ignore=classes_to_ignore)\n\n\ndef bieso_tags_to_spans(\n    tag_sequence: List[str], classes_to_ignore: List[str] = None\n) -> List[Tuple[str, Tuple[int, int]]]:\n    \"\"\"\n    bmeso -> bioul\n    B = Beginning\n    I/M = Inside / Middle\n    L/E = Last / End\n    O = Outside\n    U/W/S = Unit-length / Whole / Singleton\n    \"\"\"\n    new_tag = list()\n    for label in tag_sequence:\n        if label[0] == 'E':\n            new_tag += ['L-' + label.partition(\"-\")[2]]\n        elif label[0] == 'S':\n            new_tag += ['U-' + label.partition(\"-\")[2]]\n        else:\n            new_tag += [label]\n\n    return bioul_tags_to_spans(tag_sequence=new_tag, classes_to_ignore=classes_to_ignore)\n# ### End Code\n\n\n_tagging_span_function = {\n    'bioul': bioul_tags_to_spans,\n    'bmes': bmes_tags_to_spans,\n    'bio': bio_tags_to_spans,\n    'iob1': iob1_tags_to_spans,\n    'bmeso': bmeso_tags_to_spans,\n    'bieso': bieso_tags_to_spans,\n}\n\n\nclass Cols(TaskFormat):\n\n    def __init__(self, tokens: List[str], spans:  List[Tuple[Tuple[int, int], str]], language='en', instance_id=None) -> None:\n        super().__init__(\n            language=language\n        )\n        self.instance_id = instance_id\n        self.tokens = tokens\n        self.spans = spans\n\n    def generate_instance(self):\n        entities = list()\n        for span_index, span in enumerate(self.spans):\n            tokens = self.tokens[span['start']: span['end'] + 1]\n            indexes = list(range(span['start'], span['end'] + 1))\n            entities += [\n                Entity(\n                    span=Span(\n                        tokens=tokens,\n                        indexes=indexes,\n                        text=tokens_to_str(tokens, language=self.language),\n                        text_id=self.instance_id\n                    ),\n                    label=Label(span['type']),\n                    text_id=self.instance_id,\n                    record_id=self.instance_id + \"#%s\" % span_index if self.instance_id else None)\n            ]\n        return Sentence(tokens=self.tokens,\n                        entities=entities,\n                        text_id=self.instance_id)\n\n    @staticmethod\n    def generate_sentence(filename):\n        sentence = list()\n        with open(filename) as fin:\n            for line in fin:\n                if line.strip() == '':\n                    if len(sentence) != 0:\n                        yield sentence\n                        sentence = list()\n\n                else:\n                    sentence += [line.strip().split()]\n\n            if len(sentence) != 0:\n                yield sentence\n\n\nclass TokenTagCols(Cols):\n\n    @staticmethod\n    def load_from_file(filename, language='en', tagging='bio') -> List[Sentence]:\n        sentence_list = list()\n        counter = Counter()\n        for rows in tqdm(Cols.generate_sentence(filename)):\n            tokens = [token[0] for token in rows]\n            ner = [token[1] for token in rows]\n            spans = _tagging_span_function[tagging](ner)\n            spans = list(filter(lambda x: x[0] != \"\", spans))\n            spans = [\n                {'start': span[1][0], 'end': span[1][1], 'type': span[0]}\n                for span in spans\n            ]\n            sentence = Cols(\n                tokens=tokens,\n                spans=spans,\n                language=language,\n            )\n            counter.update(['token'] * len(tokens))\n            counter.update(['sentence'])\n            counter.update(['span'] * len(spans))\n            sentence_list += [sentence.generate_instance()]\n        print(filename, counter)\n        return sentence_list\n\n\nclass TagTokenCols(Cols):\n\n    @staticmethod\n    def load_from_file(filename, language='en', tagging='bio') -> List[Sentence]:\n        sentence_list = list()\n        counter = Counter()\n        for rows in tqdm(Cols.generate_sentence(filename)):\n            tokens = [token[1] for token in rows]\n            ner = [token[0] for token in rows]\n            spans = _tagging_span_function[tagging](ner)\n            spans = [\n                {'start': span[1][0], 'end': span[1][1], 'type': span[0]}\n                for span in spans\n            ]\n            sentence = Cols(\n                tokens=tokens,\n                spans=spans,\n                language=language,\n            )\n            counter.update(['token'] * len(tokens))\n            counter.update(['sentence'])\n            counter.update(['span'] * len(spans))\n            sentence_list += [sentence.generate_instance()]\n        print(filename, counter)\n        return sentence_list\n\n\nclass TokenTagJson(Cols):\n    @staticmethod\n    def load_from_file(filename, language='en', tagging='bio') -> List[Sentence]:\n        sentence_list = list()\n        counter = Counter()\n        for line in open(filename):\n            instance = json.loads(line.strip())\n            tokens = instance['tokens']\n            ner = instance['ner_tags']\n            spans = _tagging_span_function[tagging](ner)\n            spans = list(filter(lambda x: x[0] != \"\", spans))\n            spans = [\n                {'start': span[1][0], 'end': span[1][1], 'type': span[0]}\n                for span in spans\n            ]\n            sentence = Cols(\n                tokens=tokens,\n                spans=spans,\n                language=language,\n            )\n            counter.update(['token'] * len(tokens))\n            counter.update(['sentence'])\n            counter.update(['span'] * len(spans))\n            sentence_list += [sentence.generate_instance()]\n        print(filename, counter)\n        return sentence_list\n\n\nclass I2b2Conll(Cols):\n\n    @staticmethod\n    def load_from_file(filename, language='en') -> List[Sentence]:\n        sentence_list = list()\n        counter = Counter()\n        for rows in tqdm(Cols.generate_sentence(filename)):\n            tokens = [token[0] for token in rows]\n            ner = [token[4] for token in rows]\n            spans = bio_tags_to_spans(ner)\n            spans = [\n                {'start': span[1][0], 'end': span[1][1], 'type': span[0]}\n                for span in spans\n            ]\n            sentence = Cols(\n                tokens=tokens,\n                spans=spans,\n                language=language,\n            )\n            counter.update(['token'] * len(tokens))\n            counter.update(['sentence'])\n            counter.update(['span'] * len(spans))\n            sentence_list += [sentence.generate_instance()]\n        print(filename, counter)\n        return sentence_list\n\n\nclass CoNLL03(Cols):\n\n    @staticmethod\n    def load_from_file(filename, language='en') -> List[Sentence]:\n        sentence_list = list()\n        counter = Counter()\n        for rows in tqdm(Cols.generate_sentence(filename)):\n            if rows[0][0] == '-DOCSTART-':\n                continue\n            tokens = [token[0] for token in rows]\n            ner = [token[3] for token in rows]\n            spans = iob1_tags_to_spans(ner)\n            spans = [\n                {'start': span[1][0], 'end': span[1][1], 'type': span[0]}\n                for span in spans\n            ]\n            sentence = Cols(\n                tokens=tokens,\n                spans=spans,\n                language=language,\n            )\n            counter.update(['token'] * len(tokens))\n            counter.update(['sentence'])\n            counter.update(['span'] * len(spans))\n            sentence_list += [sentence.generate_instance()]\n        print(filename, counter)\n        return sentence_list\n\n\nif __name__ == \"__main__\":\n    pass\n"}
{"type": "source_file", "path": "dataset_processing/universal_ie/generation_format/generation_format.py", "content": "#!/usr/bin/env python\n# -*- coding:utf-8 -*-\nfrom typing import List, Dict, Union\nfrom collections import defaultdict\nfrom universal_ie.record_schema import RecordSchema\nfrom universal_ie.generation_format.structure_marker import StructureMarker\nfrom universal_ie.ie_format import Entity, Relation, Event, Label\nimport abc\n\n\nclass GenerationFormat:\n    __metaclass__ = abc.ABCMeta\n\n    def __init__(self,\n                 structure_maker: StructureMarker,\n                 label_mapper: Dict = None,\n                 language: str = 'en') -> None:\n        self.structure_maker = structure_maker\n        self.language = language\n        self.label_mapper = {} if label_mapper is None else label_mapper\n\n        # 用于从数据中统计 Schema\n        self.record_role_map = defaultdict(set)\n\n    def get_label_str(self, label: Label):\n        return self.label_mapper.get(label.__repr__(), label.__repr__())\n\n    @abc.abstractmethod\n    def annotate_entities(\n        self, tokens: List[str], entities: List[Entity]): pass\n\n    @abc.abstractmethod\n    def annotate_given_entities(self, tokens: List[str], entities: Union[List[Entity], Entity]): pass\n\n    @abc.abstractmethod\n    def annotate_events(self, tokens: List[str], events: List[Event]): pass\n\n    @abc.abstractmethod\n    def annotate_event_given_predicate(self, tokens: List[str], event: Event): pass\n\n    @abc.abstractmethod\n    def annotate_relation_extraction(self, tokens: List[str],\n                                     relations: List[Relation]): pass\n\n    def output_schema(self, filename: str):\n        \"\"\"自动导出 Schema 文件\n        每个 Schema 文件包含三行\n            - 第一行为 Record 的类别名称列表\n            - 第二行为 Role 的类别名称列表\n            - 第三行为 Record-Role 映射关系字典\n        Args:\n            filename (str): [description]\n        \"\"\"\n        record_list = list(self.record_role_map.keys())\n        role_set = set()\n        for record in self.record_role_map:\n            role_set.update(self.record_role_map[record])\n            self.record_role_map[record] = list(self.record_role_map[record])\n        role_list = list(role_set)\n\n        record_schema = RecordSchema(type_list=record_list,\n                                     role_list=role_list,\n                                     type_role_dict=self.record_role_map\n                                     )\n        record_schema.write_to_file(filename)\n\n    def get_entity_schema(self, entities: List[Entity]):\n        schema_role_map = set()\n        for entity in entities:\n            schema_role_map.add(self.get_label_str(entity.label))\n        return RecordSchema(\n            type_list=list(schema_role_map),\n            role_list=list(),\n            type_role_dict=dict()\n        )\n\n    def get_relation_schema(self, relations: List[Relation]):\n        record_role_map = defaultdict(set)\n        role_set = set()\n\n        for relation in relations:\n            record_role_map[self.get_label_str(relation.label)].add(self.get_label_str(relation.arg1.label))\n            record_role_map[self.get_label_str(relation.label)].add(self.get_label_str(relation.arg2.label))\n\n        for record in record_role_map:\n            role_set.update(record_role_map[record])\n            record_role_map[record] = list(self.record_role_map[record])\n\n        return RecordSchema(\n            type_list=list(record_role_map.keys()),\n            role_list=list(role_set),\n            type_role_dict=record_role_map\n        )\n\n    def get_event_schema(self, events: List[Event]):\n        record_role_map = defaultdict(set)\n        role_set = set()\n\n        for event in events:\n            for role, _ in event.args:\n                record_role_map[self.get_label_str(event.label)].add(self.get_label_str(role))\n\n        for record in record_role_map:\n            role_set.update(record_role_map[record])\n            record_role_map[record] = list(self.record_role_map[record])\n\n        return RecordSchema(\n            type_list=list(record_role_map.keys()),\n            role_list=list(role_set),\n            type_role_dict=record_role_map\n        )\n"}
{"type": "source_file", "path": "dataset_processing/scripts/preprocess_golden_horse.py", "content": "#!/usr/bin/env python\n# -*- coding:utf-8 -*-\n\n\ndef convert_file(filename, output_filename, ignore_class=None):\n    with open(output_filename, 'w') as output:\n        for line in open(filename):\n            att = line.strip().split('\\t')\n            if len(att) < 2:\n                output.write('\\n')\n            else:\n                token, tag = att\n                token = token[0]\n                if ignore_class is not None:\n                    if tag.endswith(ignore_class):\n                        tag = 'O'\n                output.write(f'{token}\\t{tag}\\n')\n\n\ndef main():\n    convert_file(\"data/golden-horse/data/weiboNER_2nd_conll.train\", \"data/golden-horse/data/weiboNER_2nd_conll.train.word_tag\",)\n    convert_file(\"data/golden-horse/data/weiboNER_2nd_conll.dev\", \"data/golden-horse/data/weiboNER_2nd_conll.dev.word_tag\",)\n    convert_file(\"data/golden-horse/data/weiboNER_2nd_conll.test\", \"data/golden-horse/data/weiboNER_2nd_conll.test.word_tag\",)\n\n    convert_file(\"data/golden-horse/data/weiboNER_2nd_conll.train\", \"data/golden-horse/data/weiboNER_2nd_conll.train.word_tag.nom\", ignore_class='NAM')\n    convert_file(\"data/golden-horse/data/weiboNER_2nd_conll.dev\", \"data/golden-horse/data/weiboNER_2nd_conll.dev.word_tag.nom\", ignore_class='NAM')\n    convert_file(\"data/golden-horse/data/weiboNER_2nd_conll.test\", \"data/golden-horse/data/weiboNER_2nd_conll.test.word_tag.nom\", ignore_class='NAM')\n\n    convert_file(\"data/golden-horse/data/weiboNER_2nd_conll.train\", \"data/golden-horse/data/weiboNER_2nd_conll.train.word_tag.nam\", ignore_class='NOM')\n    convert_file(\"data/golden-horse/data/weiboNER_2nd_conll.dev\", \"data/golden-horse/data/weiboNER_2nd_conll.dev.word_tag.nam\", ignore_class='NOM')\n    convert_file(\"data/golden-horse/data/weiboNER_2nd_conll.test\", \"data/golden-horse/data/weiboNER_2nd_conll.test.word_tag.nam\", ignore_class='NOM')\n\n\nif __name__ == \"__main__\":\n    main()\n"}
{"type": "source_file", "path": "dataset_processing/universal_ie/task_format/mrc_ner.py", "content": "#!/usr/bin/env python\n# -*- coding:utf-8 -*-\nimport json\nfrom collections import Counter, defaultdict\nfrom typing import Dict, List\nfrom universal_ie.task_format.spannet import Spannet\nfrom universal_ie.ie_format import Sentence\n\n\nclass MRCNER(Spannet):\n    \"\"\" MRC NER format at https://github.com/ShannonAI/mrc-for-flat-nested-ner\"\"\"\n    id_template = \"%s#%s\"\n\n    def __init__(self, instance_json: Dict, language='en'):\n        super().__init__(\n            instance_json=instance_json,\n            language=language\n        )\n\n    @ staticmethod\n    def load_from_file(filename, language='en') -> List[Sentence]:\n        counter = Counter()\n        dataset = defaultdict(dict)\n        with open(filename) as fin:\n            for instance in json.load(fin):\n                counter.update(['label sentence'])\n                key, _ = instance['qas_id'].split('.')\n                dataset[key]['tokens'] = instance['context'].split()\n                if 'spans' not in dataset[key]:\n                    dataset[key]['spans'] = list()\n                for start, end in zip(instance['start_position'],\n                                      instance['end_position']):\n                    dataset[key]['spans'] += [{\n                        'start': start,\n                        'end': end,\n                        'type': instance['entity_label']\n                    }]\n                    counter.update(['span'])\n\n        sentence_list = list()\n        for sentence_id, sentence in dataset.items():\n            counter.update(['sentence'])\n            mrc_instance = MRCNER(\n                instance_json={\n                    'tokens': sentence['tokens'],\n                    'span_list': sentence['spans'],\n                    'id': sentence_id\n                },\n                language=language\n            )\n            sentence_list += [mrc_instance.generate_instance()]\n\n        print(filename, counter)\n\n        return sentence_list\n"}
{"type": "source_file", "path": "dataset_processing/uie_convert.py", "content": "#!/usr/bin/env python\n# -*- coding:utf-8 -*-\nfrom collections import Counter\nimport os\nimport json\nfrom typing import Dict, List\nfrom tqdm import tqdm\nfrom universal_ie.generation_format.generation_format import GenerationFormat\nfrom universal_ie.generation_format import generation_format_dict\nfrom universal_ie.generation_format.structure_marker import BaseStructureMarker\nfrom universal_ie.dataset import Dataset\nfrom universal_ie.ie_format import Sentence\n\n\ndef convert_graph(\n    generation_class: GenerationFormat,\n    output_folder: str,\n    datasets: Dict[str, List[Sentence]],\n    language: str = \"en\",\n    label_mapper: Dict = None,\n):\n    convertor = generation_class(\n        structure_maker=BaseStructureMarker(),\n        language=language,\n        label_mapper=label_mapper,\n    )\n\n    counter = Counter()\n\n    os.makedirs(output_folder, exist_ok=True)\n\n    schema_counter = {\n        \"entity\": list(),\n        \"relation\": list(),\n        \"event\": list(),\n    }\n    for data_type, instance_list in datasets.items():\n        with open(os.path.join(output_folder, f\"{data_type}.json\"), \"w\") as output:\n            for instance in tqdm(instance_list):\n                counter.update([f\"{data_type} sent\"])\n                converted_graph = convertor.annonote_graph(\n                    tokens=instance.tokens,\n                    entities=instance.entities,\n                    relations=instance.relations,\n                    events=instance.events,\n                )\n                src, tgt, spot_labels, asoc_labels = converted_graph[:4]\n                spot_asoc = converted_graph[4]\n\n                schema_counter[\"entity\"] += instance.entities\n                schema_counter[\"relation\"] += instance.relations\n                schema_counter[\"event\"] += instance.events\n\n                output.write(\n                    \"%s\\n\"\n                    % json.dumps(\n                        {\n                            \"text\": src,\n                            \"tokens\": instance.tokens,\n                            \"record\": tgt,\n                            \"entity\": [\n                                entity.to_offset(label_mapper)\n                                for entity in instance.entities\n                            ],\n                            \"relation\": [\n                                relation.to_offset(\n                                    ent_label_mapper=label_mapper,\n                                    rel_label_mapper=label_mapper,\n                                )\n                                for relation in instance.relations\n                            ],\n                            \"event\": [\n                                event.to_offset(evt_label_mapper=label_mapper)\n                                for event in instance.events\n                            ],\n                            \"spot\": list(spot_labels),\n                            \"asoc\": list(asoc_labels),\n                            \"spot_asoc\": spot_asoc,\n                        },\n                        ensure_ascii=False,\n                    )\n                )\n    convertor.output_schema(os.path.join(output_folder, \"record.schema\"))\n    convertor.get_entity_schema(schema_counter[\"entity\"]).write_to_file(\n        os.path.join(output_folder, f\"entity.schema\")\n    )\n    convertor.get_relation_schema(schema_counter[\"relation\"]).write_to_file(\n        os.path.join(output_folder, f\"relation.schema\")\n    )\n    convertor.get_event_schema(schema_counter[\"event\"]).write_to_file(\n        os.path.join(output_folder, f\"event.schema\")\n    )\n    print(counter)\n    print(output_folder)\n    print(\"==========================\")\n\n\ndef convert_to_oneie(output_folder: str, datasets: Dict[str, List[Sentence]]):\n    os.makedirs(output_folder, exist_ok=True)\n    counter = Counter()\n\n    for data_type, instance_list in datasets.items():\n        with open(\n            os.path.join(output_folder, f\"{data_type}.oneie.json\"), \"w\"\n        ) as output:\n            for instance in tqdm(instance_list):\n                counter.update([f\"{data_type} sent\"])\n                entity_mentions = [\n                    {\n                        \"id\": entity.record_id,\n                        \"entity_type\": str(entity.label),\n                        \"text\": entity.span.text,\n                        \"start\": entity.span.indexes[0],\n                        \"end\": entity.span.indexes[-1] + 1,\n                    }\n                    for entity in instance.entities\n                ]\n                relation_mentions = [\n                    {\n                        \"id\": relation.record_id,\n                        \"relation_type\": str(relation.label),\n                        \"argument\": [\n                            {\n                                \"entity_id\": relation.arg1.record_id,\n                                \"text\": relation.arg1.span.text,\n                                \"role\": \"Arg-1\",\n                            },\n                            {\n                                \"entity_id\": relation.arg2.record_id,\n                                \"text\": relation.arg2.span.text,\n                                \"role\": \"Arg-2\",\n                            },\n                        ],\n                    }\n                    for relation in instance.relations\n                ]\n                event_mentions = [\n                    {\n                        \"id\": event.record_id,\n                        \"event_type\": str(event.label),\n                        \"trigger\": {\n                            \"text\": event.span.text,\n                            \"start\": event.span.indexes[0],\n                            \"end\": event.span.indexes[-1] + 1,\n                        },\n                        \"argument\": [\n                            {\n                                \"id\": arg[1].record_id,\n                                \"text\": arg[1].span.text,\n                                \"role\": str(arg[0]),\n                            }\n                            for arg in event.args\n                        ],\n                    }\n                    for event in instance.events\n                ]\n\n                instance_dict = {\n                    \"tokens\": instance.tokens,\n                    \"sent_id\": instance.text_id,\n                    \"entity_mentions\": entity_mentions,\n                    \"relation_mentions\": relation_mentions,\n                    \"event_mentions\": event_mentions,\n                }\n                instance_str = json.dumps(instance_dict, ensure_ascii=False)\n                output.write(f\"{instance_str}\\n\")\n\n    print(counter)\n    print(output_folder)\n    print(\"==========================\")\n\n\ndef main():\n    import argparse\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"-format\", dest=\"generation_format\", default=\"spotasoc\")\n    parser.add_argument(\"-config\", dest=\"config\", default=\"data_config/relation\")\n    parser.add_argument(\"-output\", dest=\"output\", default=\"relation\")\n    options = parser.parse_args()\n\n    generation_class = generation_format_dict.get(options.generation_format)\n\n    if os.path.isfile(options.config):\n        config_list = [options.config]\n    else:\n        config_list = [\n            os.path.join(options.config, x) for x in os.listdir(options.config)\n        ]\n\n    for filename in config_list:\n        dataset = Dataset.load_yaml_file(filename)\n\n        datasets = dataset.load_dataset()\n        label_mapper = dataset.mapper\n        print(label_mapper)\n\n        output_name = (\n            f\"converted_data/text2{options.generation_format}/{options.output}/\"\n            + dataset.name\n        )\n\n        if generation_class:\n            convert_graph(\n                generation_class,\n                output_name,\n                datasets=datasets,\n                language=dataset.language,\n                label_mapper=label_mapper,\n            )\n        elif options.generation_format == \"oneie\":\n            convert_to_oneie(output_name, datasets=datasets)\n\n\nif __name__ == \"__main__\":\n    main()\n"}
{"type": "source_file", "path": "dataset_processing/data/casie/scripts/extract_doc_json.py", "content": "#!/usr/bin/env python\n# -*- coding:utf-8 -*-\nimport json\nimport os\nimport sys\nfrom collections import defaultdict, Counter\nfrom pprint import pprint\n\nfrom tqdm import tqdm\n\n\ndef fix_offset(annotation, span):\n    if annotation['content'][span['startOffset'] - 1:span['endOffset']] == span['text']:\n        span['startOffset'] -= 1\n\n    if annotation['content'][span['startOffset']:span['endOffset'] + 1] == span['text']:\n        span['endOffset'] += 1\n\n    if annotation['content'][span['startOffset'] - 1:span['endOffset'] + 1] == span['text']:\n        span['endOffset'] += 1\n        span['startOffset'] -= 1\n\n    if annotation['content'][span['startOffset'] + 1:span['endOffset'] - 1] == span['text']:\n        span['endOffset'] -= 1\n        span['startOffset'] += 1\n\n    if annotation['content'][span['startOffset'] - 1:span['endOffset'] - 1] == span['text']:\n        span['endOffset'] -= 1\n        span['startOffset'] -= 1\n\n    if annotation['content'][span['startOffset'] - 2:span['endOffset'] - 2] == span['text']:\n        span['endOffset'] -= 2\n        span['startOffset'] -= 2\n\n    if annotation['content'][span['startOffset']:span['endOffset']] != span['text']:\n        print(annotation['sourcefile'], span['startOffset'], span['endOffset'])\n        print(annotation['content'][span['startOffset']:span['endOffset']] + '||' + span['text'])\n\n    return span\n\n\nclass CharSeq:\n    def __init__(self, start, end, text, docid=None, init_use_end=False):\n        assert type(start) == int\n        assert type(end) == int\n        self.start = start\n        # ACE End is End Char Offset, use_end is use in python\n        # text = doc[start:use_end]\n        if init_use_end:\n            self.end = end - 1\n            self.use_end = end\n        else:\n            self.end = end\n            self.use_end = end + 1\n\n        self.length = self.end - self.start + 1\n        self.docid = docid\n        self.text = text\n\n    def __str__(self):\n        return \"(%s)[%s-%s-%s]\" % (self.text, self.start, self.end, self.docid)\n\n    def to_json(self):\n        return {'offset': self.start,\n                'length': self.length,\n                'text': self.text}\n\n    @staticmethod\n    def from_stanford_token(token, docid=None):\n        return CharSeq(start=token['characterOffsetBegin'],\n                       end=token['characterOffsetEnd'],\n                       text=token['originalText'],\n                       docid=docid,\n                       init_use_end=True)\n\n    def exact_match(self, other, check_doc=True):\n        \"\"\"\n        :param other: CharSeq\n        :param check_doc: Boolean\n        :return:\n        \"\"\"\n        if check_doc:\n            assert self.docid == other.docid\n        if self.start == other.start and self.end == other.end:\n            assert self.text == other.text\n            return True\n        return False\n\n    def partial_match(self, other, check_doc=True):\n        \"\"\"\n        :param other: CharSeq\n        :param check_doc: Boolean\n        :return:\n        \"\"\"\n        if check_doc:\n            assert self.docid == other.docid\n        if self.start < other.start and self.end < other.start:\n            return False\n        elif other.start < self.start and other.end < self.start:\n            return False\n        else:\n            return True\n\n\ndef get_token_charseq_dict(sentences, doc_id):\n    token_charseq_dict = dict()\n\n    for senid, sentence in enumerate(sentences):\n        for tokenid, token in enumerate(sentence['tokens']):\n            charseq = CharSeq.from_stanford_token(token, docid=doc_id)\n            token_charseq_dict[(senid, tokenid)] = charseq\n\n    return token_charseq_dict\n\n\ndef find_span_tokens(token_charseq_dict, span, exact_match=False, verbose=True):\n    tokens = list()\n    for token_iden, charseq in token_charseq_dict.items():\n        if exact_match:\n            if charseq.exact_match(span):\n                tokens.append(token_iden)\n        else:\n            if charseq.partial_match(span):\n                tokens.append(token_iden)\n    tokens.sort()\n\n    if len(tokens) == 0 and verbose:\n        sys.stderr.write(\"Miss span: [%s][%s-%s-%s]\\n\" % (span.text, span.docid, span.start, span.end))\n    return tokens\n\n\ndef token_to_json(token):\n    if len(token['originalText']) != (token['characterOffsetEnd'] - token['characterOffsetBegin']):\n        print('Length is not Match', token)\n        # exit(1)\n    return {'characterOffsetBegin': token['characterOffsetBegin'],\n            'characterOffsetEnd': token['characterOffsetEnd'],\n            'word': '_'.join(token['word'].split()),\n            'originalText': token['originalText'],\n            # 'lemma': token['lemma'],\n            # 'pos': token['pos'],\n            # 'ner': token['ner'],\n            }\n\n\ndef process_sentence(sentence):\n    tokens = [token_to_json(token) for token in sentence['tokens']]\n    # parsed_tree = sentence.get('parse', None)\n    # depparsed_tree = sentence.get('basicDependencies', None)\n    return {'tokens': tokens,\n            # 'parse': parsed_tree,\n            # 'depparse': depparsed_tree,\n            'span': (tokens[0]['characterOffsetBegin'],\n                     tokens[-1]['characterOffsetEnd'])\n            }\n\n\ndef span_to_dict(annotation, span, token_charseq_dict):\n    span_start, span_end = span['startOffset'], span['endOffset']\n    return {\n        'text': annotation['content'][span_start:span_end],\n        'span': (span_start, span_end - 1),\n        'tokens': find_span_tokens(token_charseq_dict,\n                                   span=CharSeq(start=span_start,\n                                                end=span_end - 1,\n                                                text=annotation['content'][span_start:span_end],\n                                                docid=annotation['sourcefile']))\n\n    }\n\n\ndef main():\n    annotation_folder = \"raw_data/annotation\"\n    content_folder = \"raw_data/content\"\n    corenlp_folder = \"raw_data/corenlp\"\n    output_filename = \"raw_data/casie.jsonlines\"\n\n    type_dict = defaultdict(Counter)\n\n    doc_list = list()\n\n    for doc_id in tqdm(range(20000)):\n        annotation_filename = f\"{annotation_folder}/{doc_id}.json\"\n        content_filename = f\"{content_folder}/{doc_id}.text\"\n        corenlp_filename = f\"{corenlp_folder}/{doc_id}.text.json\"\n\n        if not os.path.exists(annotation_filename):\n            continue\n\n        content = open(content_filename).read()\n        annotation = json.load(open(annotation_filename))\n        corenlp = json.load(open(corenlp_filename))\n\n        sentences = [process_sentence(sentence) for sentence in corenlp['sentences']]\n        token_charseq_dict = get_token_charseq_dict(sentences=sentences, doc_id=annotation['sourcefile'])\n\n        assert len(annotation['cyberevent']) == 1\n\n        hopper_list = list()\n\n        for hopper_id, hopper in enumerate(annotation['cyberevent']['hopper']):\n\n            hopper_dict = {\n                'id': \"%s-%s\" % (doc_id, hopper_id),\n                'mentions': list()\n            }\n\n            type_dict['relation'].update([hopper.get('relation', None)])\n            for event_id, event in enumerate(hopper['events']):\n\n                type_dict['type'].update([event['type']])\n                type_dict['subtype'].update([event['subtype']])\n                type_dict['realis'].update([event['realis']])\n\n                nugget = event['nugget']\n                if content[nugget['startOffset']:nugget['endOffset']] != nugget['text']:\n                    nugget = fix_offset(annotation, nugget)\n\n                event_dict = {\n                    'id': \"%s-%s-%s\" % (doc_id, hopper_id, event_id),\n                    'type': event['type'],\n                    'subtype': event['subtype'],\n                    'realis': event['realis'],\n                    'nugget': span_to_dict(annotation, span=nugget, token_charseq_dict=token_charseq_dict),\n                    'arguments': list(),\n                }\n\n                for argument_id, argument in enumerate(event.get('argument', [])):\n\n                    type_dict['role'].update([argument['role']['type']])\n                    type_dict['argument_type'].update([argument['type']])\n\n                    if content[argument['startOffset']:argument['endOffset']] != argument['text']:\n                        argument = fix_offset(annotation, argument)\n\n                    argument_dict = {\n                        'id': \"%s-%s-%s-%s\" % (doc_id, hopper_id, event_id, argument_id),\n                        'role': argument['role']['type'],\n                        'filler_type': argument['type']\n                    }\n\n                    argument_dict.update(span_to_dict(annotation, span=argument, token_charseq_dict=token_charseq_dict))\n                    event_dict['arguments'] += [argument_dict]\n\n                hopper_dict['mentions'] += [event_dict]\n            hopper_list += [hopper_dict]\n        doc_list += [{'id': annotation['sourcefile'],\n                      'sentences': sentences,\n                      'text': content,\n                      'stanford_coref': corenlp.get('corefs', {}),\n                      'event': hopper_list,\n                      'info': annotation['info']\n                      }]\n\n    for name, value in type_dict.items():\n        print(name, sum(value.values()))\n        pprint(value)\n\n    with open(output_filename, 'w') as output:\n        for doc in doc_list:\n            output.write(json.dumps(doc, ensure_ascii=False) + '\\n')\n\n\nif __name__ == \"__main__\":\n    main()\n"}
{"type": "source_file", "path": "dataset_processing/universal_ie/task_format/casie.py", "content": "#!/usr/bin/env python\n# -*- coding:utf-8 -*-\n\nfrom collections import defaultdict, Counter\nimport json\nfrom typing import List\nfrom universal_ie.task_format.task_format import TaskFormat\nfrom universal_ie.utils import tokens_to_str\nfrom universal_ie.ie_format import Entity, Event, Label, Sentence, Span\n\n\nclass CASIE(TaskFormat):\n    def __init__(self, sentence_dict, language=\"en\"):\n        super().__init__(language=language)\n        self.sent_id = sentence_dict[\"sent_id\"]\n        self.tokens = sentence_dict[\"tokens\"]\n        self.entities = sentence_dict[\"entity_mentions\"]\n        self.events = sentence_dict[\"event_mentions\"]\n\n    def generate_instance(self):\n        entities = {}\n        events = {}\n\n        for entity in self.entities:\n            indexes = entity[\"indexes\"]\n            tokens = [self.tokens[id] for id in indexes]\n            entities[entity[\"id\"]] = Entity(\n                span=Span(\n                    tokens=tokens,\n                    indexes=indexes,\n                    text=tokens_to_str(tokens, language=self.language),\n                    text_id=self.sent_id,\n                ),\n                label=Label(entity[\"type\"]),\n                text_id=self.sent_id,\n                record_id=entity[\"id\"],\n            )\n\n        for event in self.events:\n            indexes = event[\"trigger\"][\"indexes\"]\n            tokens = [self.tokens[id] for id in indexes]\n            events[event[\"id\"]] = Event(\n                span=Span(\n                    tokens=tokens,\n                    indexes=indexes,\n                    text=tokens_to_str(tokens, language=self.language),\n                    text_id=self.sent_id,\n                ),\n                label=Label(event[\"type\"]),\n                args=[\n                    (Label(x[\"role\"]), entities[x[\"id\"]])\n                    for x in event[\"arguments\"]\n                ],\n                text_id=self.sent_id,\n                record_id=event[\"id\"],\n            )\n\n        return Sentence(\n            tokens=self.tokens,\n            entities=entities.values(),\n            events=events.values(),\n            text_id=self.sent_id,\n        )\n\n    @staticmethod\n    def load_from_file(filename, language=\"en\") -> List[Sentence]:\n        sentence_list = []\n        counter = Counter()\n\n        with open(filename) as fin:\n            for line in fin:\n                doc = json.loads(line.strip())\n\n                entity_mentions = defaultdict(list)\n                event_mentions = defaultdict(list)\n\n                for event in doc[\"event\"]:\n                    for mention in event[\"mentions\"]:\n                        nugget = mention[\"nugget\"]\n                        sent_id = nugget[\"tokens\"][0][0]\n\n                        if nugget[\"tokens\"][0][0] != nugget[\"tokens\"][-1][0]:\n                            counter.update(['cross_sentence_trigger'])\n                            continue\n\n                        event_mention = {\n                            \"id\": mention[\"id\"],\n                            \"type\": mention[\"subtype\"],\n                            \"trigger\": {\"indexes\": [x[1] for x in nugget[\"tokens\"]],},\n                            \"arguments\": [],\n                        }\n                        counter.update(['event mention'])\n\n                        for argument in mention[\"arguments\"]:\n                            if argument[\"tokens\"][0][0] != argument[\"tokens\"][-1][0]:\n                                counter.update(['cross_sentence_arg'])\n                                continue\n                            arg_sent_id = argument[\"tokens\"][0][0]\n                            entity_mention = {\n                                \"id\": argument[\"id\"],\n                                \"indexes\": [x[1] for x in argument[\"tokens\"]],\n                                \"type\": argument[\"filler_type\"],\n                            }\n                            entity_mentions[arg_sent_id].append(entity_mention)\n                            counter.update(['entity'])\n                            if arg_sent_id == sent_id:\n                                event_mention[\"arguments\"].append(\n                                    {\n                                        \"id\": argument[\"id\"],\n                                        \"trigger\": {\n                                            \"indexes\": [x[1] for x in nugget[\"tokens\"]],\n                                        },\n                                        \"role\": argument[\"role\"],\n                                    }\n                                )\n                                counter.update(['argument'])\n                            else:\n                                counter.update(['cross_sentence_tri_arg'])\n\n                        event_mentions[sent_id].append(event_mention)\n\n                for sent_id, sentence in enumerate(doc[\"sentences\"]):\n                    tokens = [token[\"word\"] for token in sentence[\"tokens\"]]\n\n                    sentence_dict = {\n                        \"sent_id\": sent_id,\n                        \"tokens\": tokens,\n                        \"entity_mentions\": entity_mentions[sent_id],\n                        \"event_mentions\": event_mentions[sent_id],\n                    }\n                    instance = CASIE(\n                        sentence_dict, language=language\n                    ).generate_instance()\n\n                    sentence_list.append(instance)\n                    counter.update(['sentence'])\n\n        print(filename, counter)\n        return sentence_list\n"}
{"type": "source_file", "path": "dataset_processing/universal_ie/ie_format.py", "content": "#!/usr/bin/env python\n# -*- coding:utf-8 -*-\nfrom abc import abstractmethod\nfrom collections import defaultdict\nfrom typing import List, Union, Tuple\nfrom universal_ie.utils import change_name_using_label_mapper\n\n\n# All Entity Relation Events are structured records.\n# They both have attributes text_id and record_id\n# 所有的 Entity Relation Event 都是结构化的记录表示 （Record）\n# 他们都有属性 text_id 和 record_id\nclass Record:\n    def __init__(self,\n                 text_id: Union[str, None] = None,\n                 record_id: Union[str, None] = None,\n                 ) -> None:\n        self.text_id = text_id\n        self.record_id = record_id\n\n    @abstractmethod\n    def to_offset(self):\n        pass\n\n\n# Text span\n# 连续或者非连续的文本块\nclass Span:\n    def __init__(self,\n                 tokens: List[str],\n                 indexes: List[int],\n                 text: str,\n                 text_id: Union[str, None] = None,\n                 ) -> None:\n        self.tokens = tokens\n        self.indexes = indexes\n        self.text = text\n        self.text_id = text_id\n\n    def __repr__(self) -> str:\n        return \"[%s](%s)\" % (self.text, self.indexes)\n\n    @staticmethod\n    def get_empty_span(text_id: Union[str, None] = None,):\n        return Span(\n            tokens=list(),\n            indexes=list(),\n            text=\"\",\n            text_id=text_id\n        )\n\n    def is_empty_span(self):\n        \"\"\"Check is empty span.\n\n        Returns:\n            bool: True, Empty Span; False Non-Empty Span\n        \"\"\"\n        return len(self.tokens) == 0 and len(self.indexes) == 0\n\n\n# Label Name\nclass Label:\n    def __init__(self, label_name: Union[str, List[str]]) -> None:\n        self.label_name = label_name\n\n    def __repr__(self) -> str:\n        return self.label_name\n\n    def __lt__(self, other):\n        if not isinstance(other, Label):\n            return NotImplemented\n        return self.label_name < other.label_name\n\n\n# Entity, Span\n# 实体，以文本块为核心的一元结构\nclass Entity(Record):\n    def __init__(self,\n                 span: Span,\n                 label: Label,\n                 text_id: Union[str, None] = None,\n                 record_id: Union[str, None] = None,\n                 ) -> None:\n        super().__init__(text_id=text_id, record_id=record_id)\n        self.span = span\n        self.label = label\n\n    def __lt__(self, other):\n        if not isinstance(other, Entity):\n            return NotImplemented\n        return self.span.indexes < other.span.indexes\n\n    def __repr__(self) -> str:\n        return self.span.__repr__() + self.label.__repr__()\n\n    def to_offset(self, ent_label_mapper=None):\n        if self.span.is_empty_span():\n            # If span is empty, skip entity\n            return {}\n        return {'type': change_name_using_label_mapper(self.label.label_name,\n                                                       ent_label_mapper),\n                'offset': self.span.indexes,\n                'text': self.span.text}\n\n\n# Relation Span Pair\n# 关系，以文本块对为核心的二元结构\nclass Relation(Record):\n    def __init__(self,\n                 arg1: Entity,\n                 arg2: Entity,\n                 label: Label,\n                 text_id: Union[str, None] = None,\n                 record_id: Union[str, None] = None,\n                 ) -> None:\n        super().__init__(text_id=text_id, record_id=record_id)\n        self.arg1 = arg1\n        self.arg2 = arg2\n        self.label = label\n\n    def __repr__(self) -> str:\n        return self.arg1.__repr__() + self.label.__repr__() + self.arg2.__repr__()\n\n    def to_offset(self, rel_label_mapper=None, ent_label_mapper=None):\n        if self.arg1.span.is_empty_span() or self.arg2.span.is_empty_span():\n            # If span is empty, skip relation\n            return {}\n        return {'type': change_name_using_label_mapper(self.label.label_name,\n                                                       rel_label_mapper),\n                'args': [self.arg1.to_offset(ent_label_mapper=ent_label_mapper),\n                         self.arg2.to_offset(ent_label_mapper=ent_label_mapper),\n                         ],\n                }\n\n\n# Event, Trigger-Mult-Argument\n# 事件，以触发词为中心的多元(谓词论元)结构\nclass Event(Record):\n    def __init__(self,\n                 span: Span,\n                 label: Label,\n                 args: List[Tuple[Label, Entity]],\n                 text_id: Union[str, None] = None,\n                 record_id: Union[str, None] = None,\n                 ) -> None:\n        super().__init__(text_id=text_id, record_id=record_id)\n        self.span = span\n        self.label = label\n        self.args = args\n\n    def __repr__(self) -> str:\n        return self.span.__repr__() + self.label.__repr__()\n\n    def to_offset(self, evt_label_mapper=None):\n\n        if self.span.is_empty_span():\n            # If span is empty, skip relation\n            return {}\n\n        args = list()\n        for role, arg in self.args:\n            if arg.span.is_empty_span():\n                continue\n            args += [{\n                    'type': change_name_using_label_mapper(\n                        role.label_name,\n                        evt_label_mapper,\n                    ),\n                    'offset': arg.span.indexes,\n                    'text': arg.span.text\n                }]\n\n        return {'type': change_name_using_label_mapper(self.label.label_name,\n                                                       evt_label_mapper),\n                'offset': self.span.indexes,\n                'text': self.span.text,\n                'args': args}\n\n\nclass Sentence:\n    def __init__(self,\n                 tokens: List[str],\n                 entities: List[Entity] = None,\n                 relations: List[Relation] = None,\n                 events: List[Event] = None,\n                 text_id: Union[str, None] = None,\n                 ) -> None:\n        self.tokens = tokens\n        self.entities = entities or list()\n        self.relations = relations or list()\n        self.events = events or list()\n        self.text_id = text_id\n\n    def count_entity_without_relation(self):\n        entity_set = set()\n        entity_counter = defaultdict(int)\n        for entity in self.entities:\n            entity_set.add((tuple(entity.span.indexes), entity.label.label_name))\n\n        for relation in self.relations:\n            entity1 = (tuple(relation.arg1.span.indexes), relation.arg1.label.label_name)\n            entity2 = (tuple(relation.arg2.span.indexes), relation.arg2.label.label_name)\n            entity_counter[entity1] += 1\n            entity_counter[entity2] += 1\n            entity_set.remove(entity1) if entity1 in entity_set else None\n            entity_set.remove(entity2) if entity2 in entity_set else None\n        overlap_entity = sum([1 if v > 1 else 0 for k, v in entity_counter.items()])\n        return {'entity': len(self.entities),\n                'entity_without_relation': len(entity_set),\n                'overlap_entity': overlap_entity,\n                }\n"}
{"type": "source_file", "path": "scripts/show_length_count.py", "content": "import argparse\nimport json\nimport os\nfrom collections import Counter, defaultdict\nfrom transformers import AutoTokenizer\nfrom tabulate import tabulate\nfrom tqdm import tqdm\nfrom uie.seq2seq.t5_bert_tokenizer import T5BertTokenizer\nfrom uie.extraction.dataset_processer import PrefixGenerator\nfrom uie.extraction.record_schema import RecordSchema\n\n\ndef find_key(count):\n    if count > 512:\n        return '7.>512'\n    elif 384 < count <= 512:\n        return \"6.384-512\"\n    elif 320 < count <= 384:\n        return \"5.320-384\"\n    elif 256 < count <= 320:\n        return \"4.256-320\"\n    elif 192 < count <= 256:\n        return \"3.192-256\"\n    elif 128 < count <= 192:\n        return \"2.128-192\"\n    elif 64 < count <= 128:\n        return \"1. 64-128\"\n    elif count == 0:\n        return \"8. =0\"\n    else:\n        return \"0.    <64\"\n\n\ndef get_acc_list(counter):\n    sum_instance = float(sum(counter.values()))\n    acc_list = list()\n    acc_counter = defaultdict(int)\n    for k in sorted(counter.keys()):\n        v = counter[k]\n        acc_counter[find_key(k)] += v\n    acc = 0\n    for k in sorted(acc_counter.keys()):\n        acc += acc_counter[k]\n        acc_list += [(k, acc, \"%.2f\" % (acc / sum_instance * 100))]\n    return acc_list\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('-data', required=True, nargs='+')\n    parser.add_argument('-tokenize', default='hf_models/t5-small')\n    parser.add_argument('-fast', action='store_true')\n    parser.add_argument('-key', default='record')\n    options = parser.parse_args()\n\n    if \"t5-char\" in options.tokenize:\n        tokenizer = T5BertTokenizer.from_pretrained(options.tokenize)\n    else:\n        tokenizer = AutoTokenizer.from_pretrained(options.tokenize, use_fast=options.fast)\n    print(\"Load tokenize: \", options.tokenize)\n\n    to_add_special_token = list()\n    for special_token in ['<extra_id_0>', '<extra_id_1>', '<extra_id_2>', '<extra_id_3>', '<extra_id_4>', '<extra_id_5>', '<spot>', '<asoc>']:\n        if special_token not in tokenizer.get_vocab():\n            to_add_special_token += [special_token]\n    tokenizer.add_special_tokens({\"additional_special_tokens\": to_add_special_token})\n\n    for data_folder in options.data:\n        print(data_folder)\n\n        record_schema = RecordSchema.read_from_file(data_folder + '/record.schema')\n        schema_prefix = PrefixGenerator.get_schema_prefix(record_schema)\n        len_schema_prefix = len(tokenizer.tokenize(schema_prefix))\n        print(\"Schema Propmt: %s\" % schema_prefix)\n        print(\"Schema Propmt After Toknized: %s\" % tokenizer.tokenize(schema_prefix))\n        print(\"Schema Prompt Length: %s\" % len_schema_prefix)\n        for file_type in {\"train\", \"val\", \"test\", \"align\"}:\n            counter = defaultdict(Counter)\n            filename = os.path.join(data_folder, file_type + '.json')\n            if not os.path.exists(filename):\n                print('Skip %s' % filename)\n                continue\n\n            for line in tqdm(open(filename).readlines(), unit='line'):\n                instance = json.loads(line)\n                text = instance['text']\n                record = instance[options.key]\n                counter['Text'].update([len(tokenizer.tokenize(text))])\n                counter['Record'].update([len(tokenizer.tokenize(record))])\n                counter['Text + Schema'].update([len(tokenizer.tokenize(text)) + len_schema_prefix])\n                counter['Record + Schema Prompt'].update([len(tokenizer.tokenize(record)) + len_schema_prefix])\n                if len(tokenizer.tokenize(record)) > 512:\n                    print(\"[Length > 512 Text  ]:\", text)\n                    print(\"[Length > 512 Record]:\", record)\n\n            for k, v in counter.items():\n                print(file_type, k)\n                table = get_acc_list(v)\n                print(tabulate(table))\n                print(f\"Min: {min(v.keys())}\")\n                print(f\"Max: {max(v.keys())}\")\n\n\nif __name__ == \"__main__\":\n    main()\n"}
{"type": "source_file", "path": "uie/extraction/predict_parser/__init__.py", "content": "#!/usr/bin/env python\n# -*- coding:utf-8 -*-\nfrom uie.extraction.predict_parser.predict_parser import PredictParser\nfrom uie.extraction.predict_parser.spotasoc_predict_parser import SpotAsocPredictParser\n\n\ndecoding_format_dict = {\n    'spotasoc': SpotAsocPredictParser,\n}\n\n\ndef get_predict_parser(decoding_schema, label_constraint):\n    return decoding_format_dict[decoding_schema](label_constraint=label_constraint)\n"}
{"type": "source_file", "path": "inference.py", "content": "#!/usr/bin/env python\n# -*- coding:utf-8 -*-\nimport json\nimport re\nfrom tqdm import tqdm\nimport transformers as huggingface_transformers\nfrom uie.extraction.record_schema import RecordSchema\nfrom uie.sel2record.record import MapConfig\nfrom uie.extraction.scorer import *\nfrom uie.sel2record.sel2record import SEL2Record\nimport math\nimport os\n\n\nsplit_bracket = re.compile(r\"\\s*<extra_id_\\d>\\s*\")\nspecial_to_remove = {'<pad>', '</s>'}\n\n\ndef read_json_file(file_name):\n    return [json.loads(line) for line in open(file_name)]\n\n\ndef schema_to_ssi(schema: RecordSchema):\n    ssi = \"<spot> \" + \"<spot> \".join(sorted(schema.type_list))\n    ssi += \"<asoc> \" + \"<asoc> \".join(sorted(schema.role_list))\n    ssi += \"<extra_id_2> \"\n    return ssi\n\n\ndef post_processing(x):\n    for special in special_to_remove:\n        x = x.replace(special, '')\n    return x.strip()\n\n\nclass HuggingfacePredictor:\n    def __init__(self, model_path, schema_file, max_source_length=256, max_target_length=192) -> None:\n        self._tokenizer = huggingface_transformers.T5TokenizerFast.from_pretrained(\n            model_path)\n        self._model = huggingface_transformers.T5ForConditionalGeneration.from_pretrained(\n            model_path)\n        self._model.cuda()\n        self._schema = RecordSchema.read_from_file(schema_file)\n        self._ssi = schema_to_ssi(self._schema)\n        self._max_source_length = max_source_length\n        self._max_target_length = max_target_length\n\n    def predict(self, text):\n        text = [self._ssi + x for x in text]\n        inputs = self._tokenizer(\n            text, padding=True, return_tensors='pt').to(self._model.device)\n\n        inputs['input_ids'] = inputs['input_ids'][:, :self._max_source_length]\n        inputs['attention_mask'] = inputs['attention_mask'][:,\n                                                            :self._max_source_length]\n\n        result = self._model.generate(\n            input_ids=inputs['input_ids'],\n            attention_mask=inputs['attention_mask'],\n            max_length=self._max_target_length,\n        )\n        return self._tokenizer.batch_decode(result, skip_special_tokens=False, clean_up_tokenization_spaces=False)\n\n\ntask_dict = {\n    'entity': EntityScorer,\n    'relation': RelationScorer,\n    'event': EventScorer,\n}\n\n\ndef main():\n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        '--data', '-d', default='data/text2spotasoc/absa/14lap')\n    parser.add_argument(\n        '--model', '-m', default='./models/uie_n10_21_50w_absa_14lap')\n    parser.add_argument('--max_source_length', default=256, type=int)\n    parser.add_argument('--max_target_length', default=192, type=int)\n    parser.add_argument('--batch_size', default=16, type=int)\n    parser.add_argument('-c', '--config', dest='map_config',\n                        help='Offset Re-mapping Config',\n                        default='config/offset_map/closest_offset_en.yaml')\n    parser.add_argument('--decoding', default='spotasoc')\n    parser.add_argument('--verbose', action='store_true')\n    parser.add_argument('--match_mode', default='normal',\n                        choices=['set', 'normal', 'multimatch'])\n    options = parser.parse_args()\n\n    data_folder = options.data\n    model_path = options.model\n\n    predictor = HuggingfacePredictor(\n        model_path=model_path,\n        schema_file=f\"{data_folder}/record.schema\",\n        max_source_length=options.max_source_length,\n        max_target_length=options.max_target_length,\n    )\n\n    map_config = MapConfig.load_from_yaml(options.map_config)\n    schema_dict = SEL2Record.load_schema_dict(data_folder)\n    sel2record = SEL2Record(\n        schema_dict=schema_dict,\n        decoding_schema=options.decoding,\n        map_config=map_config,\n    )\n\n    for split, split_name in [('val', 'eval'), ('test', 'test')]:\n        gold_filename = f\"{data_folder}/{split}.json\"\n\n        text_list = [x['text'] for x in read_json_file(gold_filename)]\n        token_list = [x['tokens'] for x in read_json_file(gold_filename)]\n\n        batch_num = math.ceil(len(text_list) / options.batch_size)\n\n        predict = list()\n        for index in tqdm(range(batch_num)):\n            start = index * options.batch_size\n            end = index * options.batch_size + options.batch_size\n\n            pred_seq2seq = predictor.predict(text_list[start: end])\n            pred_seq2seq = [post_processing(x) for x in pred_seq2seq]\n\n            predict += pred_seq2seq\n\n        records = list()\n        for p, text, tokens in zip(predict, text_list, token_list):\n            r = sel2record.sel2record(pred=p, text=text, tokens=tokens)\n            records += [r]\n\n        results = dict()\n        for task, scorer in task_dict.items():\n\n            gold_list = [x[task] for x in read_json_file(gold_filename)]\n            pred_list = [x[task] for x in records]\n\n            gold_instance_list = scorer.load_gold_list(gold_list)\n            pred_instance_list = scorer.load_pred_list(pred_list)\n\n            sub_results = scorer.eval_instance_list(\n                gold_instance_list=gold_instance_list,\n                pred_instance_list=pred_instance_list,\n                verbose=options.verbose,\n                match_mode=options.match_mode,\n            )\n            results.update(sub_results)\n\n        with open(os.path.join(options.model, f'{split_name}_preds_record.txt'), 'w') as output:\n            for record in records:\n                output.write(f'{json.dumps(record)}\\n')\n\n        with open(os.path.join(options.model, f'{split_name}_preds_seq2seq.txt'), 'w') as output:\n            for pred in predict:\n                output.write(f'{pred}\\n')\n\n        with open(os.path.join(options.model, f'{split_name}_results.txt'), 'w') as output:\n            for key, value in results.items():\n                output.write(f'{split_name}_{key}={value}\\n')\n\n\nif __name__ == \"__main__\":\n    main()\n"}
{"type": "source_file", "path": "run_uie_finetune.py", "content": "#!/usr/bin/env python\n# coding=utf-8\n# Copyright The HuggingFace Team and The HuggingFace Inc. team. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nFine-tuning the library models for sequence to sequence.\n\"\"\"\n# You can also adapt this script on your own sequence to sequence task. Pointers for this are left as comments.\n\nimport logging\nimport os\nimport sys\nfrom dataclasses import dataclass, field\nfrom typing import Optional\n\nimport numpy as np\nfrom datasets import load_dataset\n\nimport transformers\nfrom transformers import (\n    AutoConfig,\n    AutoModelForSeq2SeqLM,\n    AutoTokenizer,\n    DataCollatorForSeq2Seq,\n    HfArgumentParser,\n    default_data_collator,\n    set_seed\n)\nfrom transformers.trainer_utils import get_last_checkpoint, is_main_process\n\nfrom uie.extraction import constants\nfrom uie.extraction.record_schema import RecordSchema\nfrom uie.extraction.extraction_metrics import get_extract_metrics\nfrom uie.extraction.noiser.spot_asoc_noiser import SpotAsocNoiser\nfrom uie.extraction.dataset_processer import PrefixGenerator\nfrom uie.seq2seq.constrained_seq2seq import ConstraintSeq2SeqTrainingArguments, ConstraintSeq2SeqTrainer\nfrom uie.seq2seq.data_collator import (\n    DataCollatorForMetaSeq2Seq,\n    DynamicSSIGenerator,\n)\nfrom uie.seq2seq.features import RecordFeature\nfrom uie.seq2seq.t5_bert_tokenizer import T5BertTokenizer\nfrom uie.seq2seq.trainer_arguments import ModelArguments, DataTrainingArguments\n\n\nlogger = logging.getLogger(__name__)\n\n\ndef main():\n    # See all possible arguments in src/transformers/training_args.py\n    # or by passing the --help flag to this script.\n    # We now keep distinct sets of args, for a cleaner separation of concerns.\n\n    parser = HfArgumentParser((ModelArguments, DataTrainingArguments, ConstraintSeq2SeqTrainingArguments))\n    if len(sys.argv) == 2 and sys.argv[1].endswith(\".json\"):\n        # If we pass only one argument to the script and it's the path to a json file,\n        # let's parse it to get our arguments.\n        model_args, data_args, training_args = parser.parse_json_file(json_file=os.path.abspath(sys.argv[1]))\n    else:\n        model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n\n    # Detecting last checkpoint.\n    last_checkpoint = None\n    if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:\n        last_checkpoint = get_last_checkpoint(training_args.output_dir)\n        if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:\n            raise ValueError(\n                f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"\n                \"Use --overwrite_output_dir to overcome.\"\n            )\n        elif last_checkpoint is not None:\n            logger.info(\n                f\"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change \"\n                \"the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\"\n            )\n\n    # Setup logging\n    logging.basicConfig(\n        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n        datefmt=\"%m/%d/%Y %H:%M:%S\",\n        handlers=[logging.StreamHandler(sys.stdout)],\n    )\n    logger.setLevel(logging.INFO if is_main_process(training_args.local_rank) else logging.WARN)\n\n    logger.info(\"Options:\")\n    logger.info(model_args)\n    logger.info(data_args)\n    logger.info(training_args)\n\n    # Log on each process the small summary:\n    logger.warning(\n        f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}\"\n        + f\"distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}\"\n    )\n    # Set the verbosity to info of the Transformers logger (on main process only):\n    if is_main_process(training_args.local_rank):\n        transformers.utils.logging.set_verbosity_info()\n    logger.info(\"Training/evaluation parameters %s\", training_args)\n\n    # Set seed before initializing model.\n    set_seed(training_args.seed)\n\n    # Get the datasets: you can either provide your own CSV/JSON training and evaluation files (see below)\n    # or just provide the name of one of the public datasets available on the hub at https://huggingface.co/datasets/\n    # (the dataset will be downloaded automatically from the datasets Hub).\n    #\n    # For CSV/JSON files in the summarization task, this script will use the first column for the full texts and the\n    # second column for the summaries (unless you specify column names for this with the `text_column` and\n    # `record_column` arguments).\n    # For translation, only JSON files are supported, with one field named \"translation\" containing two keys for the\n    # source and target languages (unless you adapt what follows).\n    #\n    # In distributed training, the load_dataset function guarantee that only one local process can concurrently\n    # download the dataset.\n    if data_args.dataset_name is not None:\n        # Downloading and loading a dataset from the hub.\n        datasets = load_dataset(data_args.dataset_name, data_args.dataset_config_name)\n    else:\n        data_files = {}\n        if data_args.train_file is not None:\n            data_files[\"train\"] = data_args.train_file\n            extension = data_args.train_file.split(\".\")[-1]\n        if data_args.validation_file is not None:\n            data_files[\"validation\"] = data_args.validation_file\n            extension = data_args.validation_file.split(\".\")[-1]\n        if data_args.test_file is not None:\n            data_files[\"test\"] = data_args.test_file\n            extension = data_args.test_file.split(\".\")[-1]\n    logger.info(data_files)\n    datasets = load_dataset(\"uie_json.py\", data_files=data_files)\n    # See more about loading any type of standard or custom dataset (from files, python dict, pandas DataFrame, etc) at\n    # https://huggingface.co/docs/datasets/loading_datasets.html.\n    logger.info(datasets)\n    # Load pretrained model and tokenizer\n    #\n    # Distributed training:\n    # The .from_pretrained methods guarantee that only one local process can concurrently\n    # download model & vocab.\n    logger.info(\"Load Config: %s\" % model_args.config_name if model_args.config_name else model_args.model_name_or_path)\n\n    config = AutoConfig.from_pretrained(\n        model_args.config_name if model_args.config_name else model_args.model_name_or_path,\n        cache_dir=model_args.cache_dir,\n        revision=model_args.model_revision,\n        use_auth_token=True if model_args.use_auth_token else None,\n    )\n\n    config.max_length = data_args.max_target_length\n\n    tokenizer_name = model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path\n    if 'char' in tokenizer_name:\n        tokenizer = T5BertTokenizer.from_pretrained(tokenizer_name)\n    else:\n        tokenizer = AutoTokenizer.from_pretrained(\n            tokenizer_name,\n            cache_dir=model_args.cache_dir,\n            use_fast=model_args.use_fast_tokenizer,\n            revision=model_args.model_revision,\n            use_auth_token=True if model_args.use_auth_token else None,\n        )\n\n    to_remove_token_list = list()\n    if tokenizer.bos_token:\n        to_remove_token_list += [tokenizer.bos_token]\n    if tokenizer.eos_token:\n        to_remove_token_list += [tokenizer.eos_token]\n    if tokenizer.pad_token:\n        to_remove_token_list += [tokenizer.pad_token]\n\n    model = AutoModelForSeq2SeqLM.from_pretrained(\n        model_args.model_name_or_path,\n        from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n        config=config,\n        cache_dir=model_args.cache_dir,\n        revision=model_args.model_revision,\n        use_auth_token=True if model_args.use_auth_token else None,\n        mirror='tuna',\n    )\n\n    if training_args.do_train:\n        to_add_special_token = list()\n        for special_token in [constants.type_start, constants.type_end, constants.text_start, constants.span_start, constants.spot_prompt, constants.asoc_prompt]:\n            if special_token not in tokenizer.get_vocab():\n                to_add_special_token += [special_token]\n\n        tokenizer.add_special_tokens(\n            {\"additional_special_tokens\": tokenizer.special_tokens_map_extended['additional_special_tokens'] + to_add_special_token}\n        )\n\n        model.resize_token_embeddings(len(tokenizer))\n\n    logger.info(tokenizer)\n\n    # Set decoder_start_token_id\n    if model.config.decoder_start_token_id is None:\n        raise ValueError(\"Make sure that `config.decoder_start_token_id` is correctly defined\")\n\n    if data_args.record_schema and os.path.exists(data_args.record_schema):\n        record_schema = RecordSchema.read_from_file(data_args.record_schema)\n    else:\n        record_schema = None\n\n    if data_args.source_prefix is not None:\n        if data_args.source_prefix == 'schema':\n            prefix = PrefixGenerator.get_schema_prefix(schema=record_schema)\n        elif data_args.source_prefix.startswith('meta'):\n            prefix = \"\"\n        else:\n            prefix = data_args.source_prefix\n    else:\n        prefix = \"\"\n    logger.info(f\"Prefix: {prefix}\")\n    logger.info(f\"Prefix Length: {len(tokenizer.tokenize(prefix))}\")\n\n    # Preprocessing the datasets.\n    # We need to tokenize inputs and targets.\n    if training_args.do_train:\n        column_names = datasets[\"train\"].column_names\n    elif training_args.do_eval:\n        column_names = datasets[\"validation\"].column_names\n    elif training_args.do_predict:\n        column_names = datasets[\"test\"].column_names\n    else:\n        logger.info(\"There is nothing to do. Please pass `do_train`, `do_eval` and/or `do_predict`.\")\n        return\n\n    # To serialize preprocess_function below, each of those four variables needs to be defined (even if we won't use\n    # them all).\n\n    text_column = data_args.text_column\n    record_column = data_args.record_column\n    logger.info('Using src: %s and tgt: %s' % (text_column, record_column))\n\n    # Temporarily set max_target_length for training.\n    max_target_length = data_args.max_target_length\n    padding = \"max_length\" if data_args.pad_to_max_length else False\n\n    if training_args.label_smoothing_factor > 0 and not hasattr(model, \"prepare_decoder_input_ids_from_labels\"):\n        logger.error(\n            \"label_smoothing is enabled but the `prepare_decoder_input_ids_from_labels` method is not defined for\"\n            f\"`{model.__class__.__name__}`. This will lead to loss being calculated twice and will take up more memory\"\n        )\n\n    def preprocess_function(examples):\n        inputs = examples[text_column]\n        targets = examples[record_column]\n        inputs = [prefix + inp for inp in inputs]\n        model_inputs = tokenizer(inputs, max_length=data_args.max_source_length, padding=padding, truncation=True)\n\n        # Setup the tokenizer for targets\n        with tokenizer.as_target_tokenizer():\n            labels = tokenizer(targets, max_length=max_target_length, padding=padding, truncation=True)\n\n        # If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 when we want to ignore\n        # padding in the loss.\n        if padding == \"max_length\" and data_args.ignore_pad_token_for_loss:\n            labels[\"input_ids\"] = [\n                [(_label if _label != tokenizer.pad_token_id else -100) for _label in label] for label in labels[\"input_ids\"]\n            ]\n\n        model_inputs[\"labels\"] = labels[\"input_ids\"]\n\n        model_inputs['sample_prompt'] = [False] * len(model_inputs['input_ids'])\n        if data_args.source_prefix is not None and data_args.source_prefix.startswith('meta'):\n            model_inputs['spots'] = examples['spot']\n            model_inputs['asocs'] = examples['asoc']\n            model_inputs['spot_asoc'] = examples['spot_asoc']\n            # sample_prompt=True for Finetune and Pretrain\n            model_inputs['sample_prompt'] = [True] * len(model_inputs['input_ids'])\n        return model_inputs\n\n    def preprocess_function_eval(examples):\n        model_inputs = preprocess_function(examples)\n        # sample_prompt=False for evaluation\n        model_inputs['sample_prompt'] = [False] * len(model_inputs['input_ids'])\n        return model_inputs\n\n    def postprocess_text(x_str):\n        # Clean `bos` `eos` `pad` for cleaned text\n        for to_remove_token in to_remove_token_list:\n            x_str = x_str.replace(to_remove_token, '')\n\n        return x_str.strip()\n\n    logger.info(\"Start Data Preprocessing ...\")\n\n    if training_args.do_train:\n        train_dataset = datasets[\"train\"]\n        if data_args.max_train_samples is not None:\n            train_dataset = train_dataset.select(range(data_args.max_train_samples))\n        train_dataset = train_dataset.map(\n            preprocess_function,\n            batched=True,\n            num_proc=data_args.preprocessing_num_workers,\n            remove_columns=column_names,\n            load_from_cache_file=not data_args.overwrite_cache,\n            features=RecordFeature,\n        )\n\n    if training_args.do_eval:\n        max_target_length = data_args.val_max_target_length\n        eval_dataset = datasets[\"validation\"]\n        if data_args.max_val_samples is not None:\n            eval_dataset = eval_dataset.select(range(data_args.max_val_samples))\n        eval_dataset = eval_dataset.map(\n            preprocess_function_eval,\n            batched=True,\n            num_proc=data_args.preprocessing_num_workers,\n            remove_columns=column_names,\n            load_from_cache_file=not data_args.overwrite_cache,\n            features=RecordFeature,\n        )\n\n    if training_args.do_predict:\n        max_target_length = data_args.val_max_target_length\n        test_dataset = datasets[\"test\"]\n        if data_args.max_test_samples is not None:\n            test_dataset = test_dataset.select(range(data_args.max_test_samples))\n        test_dataset = test_dataset.map(\n            preprocess_function_eval,\n            batched=True,\n            num_proc=data_args.preprocessing_num_workers,\n            remove_columns=column_names,\n            load_from_cache_file=not data_args.overwrite_cache,\n            features=RecordFeature,\n        )\n\n    logger.info(\"End Data Preprocessing ...\")\n\n    # Data collator\n    label_pad_token_id = -100 if data_args.ignore_pad_token_for_loss else tokenizer.pad_token_id\n    if data_args.pad_to_max_length:\n        data_collator = default_data_collator\n    elif data_args.source_prefix.startswith('meta'):\n\n        if data_args.spot_noise > 0 or data_args.asoc_noise > 0:\n            if data_args.decoding_format == 'spotasoc':\n                spot_asoc_nosier = SpotAsocNoiser(\n                    spot_noise_ratio=data_args.spot_noise,\n                    asoc_noise_ratio=data_args.asoc_noise,\n                    null_span=constants.null_span,\n                )\n            else:\n                raise NotImplementedError(\n                    f\"decoding_format {data_args.decoding_format} is not implemented.\"\n                )\n        else:\n            spot_asoc_nosier = None\n\n        data_collator = DataCollatorForMetaSeq2Seq(\n            tokenizer,\n            model=model,\n            label_pad_token_id=label_pad_token_id,\n            pad_to_multiple_of=8 if training_args.fp16 else None,\n            max_length=data_args.max_source_length,\n            max_prefix_length=data_args.max_prefix_length,\n            max_target_length=data_args.max_target_length,\n            negative_sampler=DynamicSSIGenerator(\n                tokenizer=tokenizer,\n                schema=record_schema,\n                positive_rate=data_args.meta_positive_rate,\n                negative=data_args.meta_negative,\n                ordered_prompt=data_args.ordered_prompt,\n            ),\n            spot_asoc_nosier=spot_asoc_nosier,\n            decoding_format=data_args.decoding_format,\n        )\n    else:\n        data_collator = DataCollatorForSeq2Seq(\n            tokenizer,\n            model=model,\n            label_pad_token_id=label_pad_token_id,\n            pad_to_multiple_of=8 if training_args.fp16 else None,\n        )\n\n    def compute_metrics(eval_preds):\n        preds, labels = eval_preds\n        if isinstance(preds, tuple):\n            preds = preds[0]\n        decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=False, clean_up_tokenization_spaces=False)\n        if data_args.ignore_pad_token_for_loss:\n            # Replace -100 in the labels as we can't decode them.\n            labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=False, clean_up_tokenization_spaces=False)\n\n        decoded_preds = [postprocess_text(x) for x in decoded_preds]\n        decoded_labels = [postprocess_text(x) for x in decoded_labels]\n\n        result = get_extract_metrics(\n            pred_lns=decoded_preds,\n            tgt_lns=decoded_labels,\n            label_constraint=record_schema,\n            decoding_format=data_args.decoding_format,\n        )\n\n        prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n        result[\"gen_len\"] = np.mean(prediction_lens)\n        result = {k: round(v, 4) for k, v in result.items()}\n        return result\n\n    # Initialize our Trainer\n    trainer = ConstraintSeq2SeqTrainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset if training_args.do_train else None,\n        eval_dataset=eval_dataset if training_args.do_eval else None,\n        tokenizer=tokenizer,\n        data_collator=data_collator,\n        compute_metrics=compute_metrics if training_args.predict_with_generate else None,\n        decoding_type_schema=record_schema,\n        decoding_format=data_args.decoding_format,\n        source_prefix=prefix,\n        task=data_args.task,\n    )\n\n    # Training\n    if training_args.do_train:\n        if model_args.from_checkpoint:\n            if last_checkpoint is not None:\n                checkpoint = last_checkpoint\n            elif os.path.isdir(model_args.model_name_or_path):\n                checkpoint = model_args.model_name_or_path\n        checkpoint = None\n\n        train_result = trainer.train(resume_from_checkpoint=checkpoint)\n        trainer.save_model()  # Saves the tokenizer too for easy upload\n\n        output_train_file = os.path.join(training_args.output_dir, \"train_results.txt\")\n        if trainer.is_world_process_zero():\n            with open(output_train_file, \"w\") as writer:\n                logger.info(\"***** Train results *****\")\n                for key, value in sorted(train_result.metrics.items()):\n                    logger.info(f\"  {key} = {value}\")\n                    writer.write(f\"{key} = {value}\\n\")\n\n            # Need to save the state, since Trainer.save_model saves only the tokenizer with the model\n            trainer.state.save_to_json(os.path.join(training_args.output_dir, \"trainer_state.json\"))\n\n    # Evaluation\n    results = {}\n    if training_args.do_eval:\n        logger.info(\"*** Evaluate ***\")\n\n        results = trainer.evaluate(max_length=data_args.val_max_target_length, num_beams=data_args.num_beams)\n        results = {k: round(v, 4) for k, v in results.items()}\n\n        eval_results = trainer.predict(\n            eval_dataset,\n            metric_key_prefix=\"eval\",\n            max_length=data_args.val_max_target_length,\n            num_beams=data_args.num_beams,\n        )\n\n        output_eval_file = os.path.join(training_args.output_dir, \"eval_results_seq2seq.txt\")\n        if trainer.is_world_process_zero():\n            with open(output_eval_file, \"w\") as writer:\n                logger.info(\"***** Eval results *****\")\n                for key, value in sorted(results.items()):\n                    logger.info(f\"  {key} = {value}\")\n                    writer.write(f\"{key} = {value}\\n\")\n\n            if training_args.predict_with_generate:\n                eval_preds = tokenizer.batch_decode(\n                    eval_results.predictions, skip_special_tokens=False, clean_up_tokenization_spaces=False\n                )\n                eval_preds = [postprocess_text(pred) for pred in eval_preds]\n                output_test_preds_file = os.path.join(training_args.output_dir, \"eval_preds_seq2seq.txt\")\n                with open(output_test_preds_file, \"w\") as writer:\n                    writer.write(\"\\n\".join(eval_preds))\n\n    if training_args.do_predict:\n        logger.info(\"*** Test ***\")\n\n        test_results = trainer.predict(\n            test_dataset,\n            metric_key_prefix=\"test\",\n            max_length=data_args.val_max_target_length,\n            num_beams=data_args.num_beams,\n        )\n        test_metrics = test_results.metrics\n        test_metrics[\"test_loss\"] = round(test_metrics[\"test_loss\"], 4)\n\n        output_test_result_file = os.path.join(training_args.output_dir, \"test_results_seq2seq.txt\")\n        if trainer.is_world_process_zero():\n            with open(output_test_result_file, \"w\") as writer:\n                logger.info(\"***** Test results *****\")\n                for key, value in sorted(test_metrics.items()):\n                    logger.info(f\"  {key} = {value}\")\n                    writer.write(f\"{key} = {value}\\n\")\n\n            if training_args.predict_with_generate:\n                test_preds = tokenizer.batch_decode(\n                    test_results.predictions, skip_special_tokens=False, clean_up_tokenization_spaces=False\n                )\n                test_preds = [postprocess_text(pred) for pred in test_preds]\n                output_test_preds_file = os.path.join(training_args.output_dir, \"test_preds_seq2seq.txt\")\n                with open(output_test_preds_file, \"w\") as writer:\n                    writer.write(\"\\n\".join(test_preds))\n\n    return results\n\n\ndef _mp_fn(index):\n    # For xla_spawn (TPUs)\n    main()\n\n\nif __name__ == \"__main__\":\n    main()\n"}
{"type": "source_file", "path": "uie/__init__.py", "content": "#!/usr/bin/env python\n# -*- coding:utf-8 -*-\n"}
{"type": "source_file", "path": "uie/extraction/constants.py", "content": "#!/usr/bin/env python\n# -*- coding:utf-8 -*-\n\nspot_prompt = '<spot>'\nasoc_prompt = '<asoc>'\n\ntype_start = '<extra_id_0>'\ntype_end = '<extra_id_1>'\ntext_start = '<extra_id_2>'\nspan_start = '<extra_id_5>'\nnull_span = '<extra_id_6>'\nnull_label = '<extra_id_7>'\n\n\nclass StructureMarker:\n    def __init__(self) -> None:\n        pass\n\n\nclass BaseStructureMarker(StructureMarker):\n    def __init__(self) -> None:\n        super().__init__()\n        self.sent_start = '<extra_id_0>'\n        self.sent_end = '<extra_id_1>'\n        self.record_start = '<extra_id_0>'\n        self.record_end = '<extra_id_1>'\n        self.span_start = '<extra_id_0>'\n        self.span_end = '<extra_id_1>'\n        self.text_start = '<extra_id_2>'\n        self.source_span_start = '<extra_id_3>'\n        self.source_span_end = '<extra_id_4>'\n        self.target_span_start = '<extra_id_5>'\n        self.null_span = '<extra_id_6>'\n        self.null_label = '<extra_id_7>'\n"}
{"type": "source_file", "path": "uie/extraction/predict_parser/predict_parser.py", "content": "#!/usr/bin/env python\n# -*- coding:utf-8 -*-\nfrom typing import List, Counter, Tuple\n\n\nclass PredictParser:\n    def __init__(self, label_constraint=None):\n        self.spot_set = label_constraint.type_list if label_constraint else list()\n        self.role_set = label_constraint.role_list if label_constraint else list()\n\n    def decode(self, gold_list, pred_list, text_list=None, raw_list=None) -> Tuple[List, Counter]:\n        pass\n"}
{"type": "source_file", "path": "scripts/get_eval_batch_num.py", "content": "#!/usr/bin/env python\n# -*- coding:utf-8 -*-\nimport sys\nimport math\n\nfile_name = sys.argv[1]\nbatch_size = int(sys.argv[2])\neval_epoch = int(sys.argv[3])\n\nline_num = sum([1 for _ in open(sys.argv[1])])\nprint(int(math.ceil(line_num / float(batch_size)) * eval_epoch))\n\n# python scripts/get_eval_batch_num.py ${run_data_folder}/train.json ${batch_size} 20\n"}
{"type": "source_file", "path": "uie/extraction/dataset_processer.py", "content": "#!/usr/bin/env python\n# -*- coding:utf-8 -*-\nfrom uie.extraction.record_schema import RecordSchema\nfrom uie.extraction.constants import spot_prompt, asoc_prompt, text_start\n\n\nclass TaskConfig:\n    def __init__(self, task_dict) -> None:\n        self.dataset_name = task_dict.get('name', '')\n        self.task_name = task_dict.get('task', '')\n        self.data_path = task_dict.get('path', '')\n        self.decoding_format = task_dict.get('decoding_format', '')\n        self.weight = int(task_dict.get('weight', 0))\n        self.sel2record = task_dict.get('sel2record', '')\n        self.metrics = task_dict.get('metrics', [])\n        self.eval_match_mode = task_dict.get('eval_match_mode', 'normal')\n        self.schema = RecordSchema.read_from_file(f\"{self.data_path}/{self.task_name}.schema\")\n\n    def __repr__(self) -> str:\n        return f\"dataset: {self.dataset_name}\\n\" \\\n               f\"task   : {self.task_name}\\n\" \\\n               f\"format : {self.decoding_format}\\n\" \\\n               f\"path   : {self.data_path}\\n\" \\\n               f\"schema : {self.schema}\\n\" \\\n               f\"metrics: {self.metrics}\\n\" \\\n               f\"eval_match_mode : {self.eval_match_mode}\"\n\n    @staticmethod\n    def load_list_from_yaml(task_config):\n        import yaml\n        configs = yaml.load(open(task_config), Loader=yaml.FullLoader)\n        task_configs = filter(lambda x: x.startswith('T'), configs)\n        for task_config in task_configs:\n            yield TaskConfig(configs[task_config])\n\n\nclass PrefixGenerator:\n    def __init__(self, prefix_dict) -> None:\n        self.type_list = prefix_dict.get('type', 'task dataset').split()\n        self.position = prefix_dict.get('position', 'encoder')\n\n    def __repr__(self) -> str:\n        return f\"Type.   : {self.type_list}\\n\" \\\n               f\"Position: {self.position}\\n\"\n\n    @staticmethod\n    def load_from_yaml(dataset_config):\n        import yaml\n        configs = yaml.load(open(dataset_config), Loader=yaml.FullLoader)\n        return PrefixGenerator(configs['Prefix'])\n\n    @staticmethod\n    def get_schema_prefix(schema: RecordSchema, add_split=True):\n        prefix_list = list()\n        for spot_label in sorted(schema.type_list):\n            prefix_list += [spot_prompt, spot_label]\n        for asoc_label in sorted(schema.role_list):\n            prefix_list += [asoc_prompt, asoc_label]\n        prefix = ' '.join(prefix_list)\n        if add_split:\n            return prefix + f' {text_start} '\n        else:\n            return prefix\n\n    @staticmethod\n    def get_dataset_name_prefix(dataset: TaskConfig, add_split=True):\n        if add_split:\n            return dataset.dataset_name + f' {text_start}'\n        else:\n            return dataset.dataset_name\n\n    @staticmethod\n    def get_task_name_prefix(dataset: TaskConfig, add_split=True):\n        if add_split:\n            return dataset.task_name + f' {text_start}'\n        else:\n            return dataset.task_name\n\n    def get_prefix_by_dataset(self, dataset: TaskConfig):\n        prefix_list = list()\n        for prefix_type in self.type_list:\n            if prefix_type == 'task':\n                prefix = self.get_task_name_prefix(dataset, add_split=False)\n            elif prefix_type == 'dataset':\n                prefix = self.get_dataset_name_prefix(dataset, add_split=False)\n            elif prefix_type == 'schema':\n                prefix = self.get_schema_prefix(dataset.schema, add_split=False)\n            elif prefix_type == 'meta':\n                # Meta 使用 Schema 的 Prefix\n                prefix = self.get_schema_prefix(dataset.schema, add_split=False)\n            else:\n                raise NotImplementedError(\n                    \"Prefix Type %s is not supported\" % prefix_type\n                )\n            prefix_list += [prefix]\n        return ' '.join(prefix_list) + f' {text_start}'\n"}
{"type": "source_file", "path": "uie/extraction/__init__.py", "content": "#!/usr/bin/env python\n# -*- coding:utf-8 -*-\n"}
{"type": "source_file", "path": "uie/extraction/label_tree.py", "content": "from typing import Dict\n\n\ndef list_dictionary(d, n_tab=-1):\n    if isinstance(d, list):\n        for i in d:\n            list_dictionary(i, n_tab)\n    elif isinstance(d, dict):\n        n_tab += 1\n        for key, value in d.items():\n            if key == '<end>':\n                print(\"{}{}\".format(\" \" * n_tab, key))\n            else:\n                print(\"{}{}\".format(\" \" * n_tab, key))\n                list_dictionary(value, n_tab)\n    else:\n        print(\"{}{}\".format(\"\\t\" * n_tab, d))\n\n\ndef print_tree(tree):\n    list_dictionary(tree)\n\n\ndef get_label_name_tree(label_name_list, tokenizer, end_symbol='<end>'):\n    sub_token_tree = dict()\n\n    label_tree = dict()\n    for typename in label_name_list:\n        after_tokenized = tokenizer.encode(typename, add_special_tokens=False)\n        # label_tree[typename] = tokenizer.convert_ids_to_tokens(after_tokenized)\n        label_tree[typename] = after_tokenized\n\n    for _, sub_label_seq in label_tree.items():\n        parent = sub_token_tree\n        for value in sub_label_seq:\n            if value not in parent:\n                parent[value] = dict()\n            parent = parent[value]\n\n        parent[end_symbol] = None\n\n    return sub_token_tree\n\n\nclass PrefixTree:\n    def __init__(self, label_name_list, tokenizer, end_symbol='<end>'):\n        self.label_name_list = label_name_list\n        self._tokenizer = tokenizer\n        self.label_name_tree = get_label_name_tree(label_name_list, tokenizer, end_symbol)\n        self._end_symbol = end_symbol\n\n    def is_end_of_tree(self, tree: Dict):\n        return len(tree) == 1 and self._end_symbol in tree\n"}
{"type": "source_file", "path": "run_uie_pretrain.py", "content": "#!/usr/bin/env python\n# coding=utf-8\n# Copyright The HuggingFace Team and The HuggingFace Inc. team. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nFine-tuning the library models for sequence to sequence.\n\"\"\"\n# You can also adapt this script on your own sequence to sequence task. Pointers for this are left as comments.\n\nimport logging\nimport os\nimport sys\nfrom datasets.arrow_dataset import Dataset\n\nfrom datasets import load_dataset\n\nimport transformers\nfrom transformers import (\n    AutoConfig,\n    AutoModelForSeq2SeqLM,\n    AutoTokenizer,\n    DataCollatorForSeq2Seq,\n    HfArgumentParser,\n    set_seed\n)\nfrom transformers.trainer_utils import get_last_checkpoint, is_main_process\n\nfrom uie.extraction import constants\nfrom uie.extraction.record_schema import RecordSchema\nfrom uie.extraction.noiser.spot_asoc_noiser import SpotAsocNoiser\nfrom uie.extraction.dataset_processer import PrefixGenerator\nfrom uie.seq2seq.constrained_seq2seq import (\n    ConstraintSeq2SeqTrainingArguments,\n    ConstraintSeq2SeqTrainer,\n)\nfrom uie.seq2seq.data_collator import (\n    DataCollatorForMetaSeq2Seq,\n    DynamicSSIGenerator,\n    HybirdDataCollator,\n    DataCollatorForT5MLM,\n)\nfrom uie.seq2seq.features import ProcessedFeature\nfrom uie.seq2seq.t5_bert_tokenizer import T5BertTokenizer\nfrom uie.seq2seq.trainer_arguments import ModelArguments, DataTrainingArguments\n\nlogger = logging.getLogger(__name__)\n\n\ndef main():\n    # See all possible arguments in src/transformers/training_args.py\n    # or by passing the --help flag to this script.\n    # We now keep distinct sets of args, for a cleaner separation of concerns.\n\n    parser = HfArgumentParser((\n        ModelArguments,\n        DataTrainingArguments,\n        ConstraintSeq2SeqTrainingArguments\n    ))\n    if len(sys.argv) == 2 and sys.argv[1].endswith(\".json\"):\n        # If we pass only one argument to the script and it's the path to a json file,\n        # let's parse it to get our arguments.\n        model_args, data_args, training_args = parser.parse_json_file(\n            json_file=os.path.abspath(sys.argv[1]))\n    else:\n        model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n\n    # Detecting last checkpoint.\n    last_checkpoint = None\n    if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:\n        last_checkpoint = get_last_checkpoint(training_args.output_dir)\n        if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:\n            raise ValueError(\n                f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"\n                \"Use --overwrite_output_dir to overcome.\"\n            )\n        elif last_checkpoint is not None:\n            logger.info(\n                f\"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change \"\n                \"the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\"\n            )\n\n    # Setup logging\n    logging.basicConfig(\n        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n        datefmt=\"%m/%d/%Y %H:%M:%S\",\n        handlers=[logging.StreamHandler(sys.stdout)],\n    )\n    logger.setLevel(logging.INFO if is_main_process(\n        training_args.local_rank) else logging.WARN)\n\n    logger.info(\"Options:\")\n    logger.info(model_args)\n    logger.info(data_args)\n    logger.info(training_args)\n\n    # Log on each process the small summary:\n    logger.warning(\n        f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}\"\n        + f\"distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}\"\n    )\n    # Set the verbosity to info of the Transformers logger (on main process only):\n    if is_main_process(training_args.local_rank):\n        transformers.utils.logging.set_verbosity_info()\n    logger.info(\"Training/evaluation parameters %s\", training_args)\n\n    # Set seed before initializing model.\n    set_seed(training_args.seed)\n\n    # Get the datasets: you can either provide your own CSV/JSON training and evaluation files (see below)\n    # or just provide the name of one of the public datasets available on the hub at https://huggingface.co/datasets/\n    # (the dataset will be downloaded automatically from the datasets Hub).\n    #\n    # For CSV/JSON files in the summarization task, this script will use the first column for the full texts and the\n    # second column for the summaries (unless you specify column names for this with the `text_column` and\n    # `record_column` arguments).\n    # For translation, only JSON files are supported, with one field named \"translation\" containing two keys for the\n    # source and target languages (unless you adapt what follows).\n    #\n    # In distributed training, the load_dataset function guarantee that only one local process can concurrently\n    # download the dataset.\n    if data_args.dataset_name is not None:\n        # Downloading and loading a dataset from the hub.\n        datasets = load_dataset(data_args.dataset_name,\n                                data_args.dataset_config_name)\n    else:\n        data_files = {}\n        if data_args.train_file is not None:\n            data_files[\"train\"] = data_args.train_file\n            extension = data_args.train_file.split(\".\")[-1]\n        if data_args.validation_file is not None:\n            data_files[\"validation\"] = data_args.validation_file\n            extension = data_args.validation_file.split(\".\")[-1]\n        if data_args.test_file is not None:\n            data_files[\"test\"] = data_args.test_file\n            extension = data_args.test_file.split(\".\")[-1]\n    logger.info(data_files)\n    datasets = load_dataset(\"uie_json.py\", data_files=data_files)\n    # See more about loading any type of standard or custom dataset (from files, python dict, pandas DataFrame, etc) at\n    # https://huggingface.co/docs/datasets/loading_datasets.html.\n    logger.info(datasets)\n    # Load pretrained model and tokenizer\n    #\n    # Distributed training:\n    # The .from_pretrained methods guarantee that only one local process can concurrently\n    # download model & vocab.\n    logger.info(\"Load Config: %s\" %\n                model_args.config_name if model_args.config_name else model_args.model_name_or_path)\n\n    config = AutoConfig.from_pretrained(\n        model_args.config_name if model_args.config_name else model_args.model_name_or_path,\n        cache_dir=model_args.cache_dir,\n        revision=model_args.model_revision,\n        use_auth_token=True if model_args.use_auth_token else None,\n        mirror='tuna',\n    )\n\n    # !!!\n    config.max_length = data_args.max_target_length\n\n    if \"char\" in model_args.model_name_or_path:\n        tokenizer = T5BertTokenizer.from_pretrained(\n            model_args.model_name_or_path)\n    else:\n        tokenizer = AutoTokenizer.from_pretrained(\n            model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,\n            cache_dir=model_args.cache_dir,\n            use_fast=model_args.use_fast_tokenizer,\n            revision=model_args.model_revision,\n            use_auth_token=True if model_args.use_auth_token else None,\n        )\n\n    to_remove_token_list = list()\n    if tokenizer.bos_token:\n        to_remove_token_list += [tokenizer.bos_token]\n    if tokenizer.eos_token:\n        to_remove_token_list += [tokenizer.eos_token]\n    if tokenizer.pad_token:\n        to_remove_token_list += [tokenizer.pad_token]\n\n    model = AutoModelForSeq2SeqLM.from_pretrained(\n        model_args.model_name_or_path,\n        from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n        config=config,\n        cache_dir=model_args.cache_dir,\n        revision=model_args.model_revision,\n        use_auth_token=True if model_args.use_auth_token else None,\n        mirror='tuna',\n    )\n\n    if training_args.do_train:\n        to_add_special_token = list()\n        for special_token in [constants.type_start, constants.type_end, constants.span_start, constants.spot_prompt, constants.asoc_prompt]:\n            if special_token not in tokenizer.get_vocab():\n                to_add_special_token += [special_token]\n        tokenizer.add_special_tokens(\n            {\"additional_special_tokens\": to_add_special_token})\n        model.resize_token_embeddings(len(tokenizer))\n\n    # Set decoder_start_token_id\n    if model.config.decoder_start_token_id is None:\n        raise ValueError(\n            \"Make sure that `config.decoder_start_token_id` is correctly defined\")\n\n    if data_args.record_schema and os.path.exists(data_args.record_schema):\n        record_schema = RecordSchema.read_from_file(data_args.record_schema)\n    else:\n        record_schema = None\n\n    if data_args.source_prefix is not None:\n        if data_args.source_prefix == 'schema':\n            prefix = PrefixGenerator.get_schema_prefix(schema=record_schema)\n        elif data_args.source_prefix.startswith('meta'):\n            prefix = \"\"\n        else:\n            prefix = data_args.source_prefix\n    else:\n        prefix = \"\"\n    logger.info(f\"Prefix: {prefix}\")\n    logger.info(f\"Prefix Length: {len(tokenizer.tokenize(prefix))}\")\n\n    # Preprocessing the datasets.\n    # We need to tokenize inputs and targets.\n    if training_args.do_train:\n        column_names = datasets[\"train\"].column_names\n    elif training_args.do_eval:\n        column_names = datasets[\"validation\"].column_names\n    elif training_args.do_predict:\n        column_names = datasets[\"test\"].column_names\n    else:\n        logger.info(\n            \"There is nothing to do. Please pass `do_train`, `do_eval` and/or `do_predict`.\")\n        return\n\n    # To serialize preprocess_function below, each of those four variables needs to be defined (even if we won't use\n    # them all).\n\n    text_column = data_args.text_column\n    record_column = data_args.record_column\n    logger.info('Using src: %s and tgt: %s' % (text_column, record_column))\n\n    # Temporarily set max_target_length for training.\n    max_target_length = data_args.max_target_length\n    padding = \"max_length\" if data_args.pad_to_max_length else False\n\n    if training_args.label_smoothing_factor > 0 and not hasattr(model, \"prepare_decoder_input_ids_from_labels\"):\n        logger.error(\n            \"label_smoothing is enabled but the `prepare_decoder_input_ids_from_labels` method is not defined for\"\n            f\"`{model.__class__.__name__}`. This will lead to loss being calculated twice and will take up more memory\"\n        )\n\n    def preprocess_function(examples):\n        inputs = examples[text_column]\n        targets = examples[record_column]\n        inputs = [prefix + inp for inp in inputs]\n        model_inputs = tokenizer(\n            inputs, max_length=data_args.max_source_length, padding=padding, truncation=True)\n\n        # Setup the tokenizer for targets\n        with tokenizer.as_target_tokenizer():\n            labels = tokenizer(\n                targets, max_length=max_target_length, padding=padding, truncation=True)\n\n        # If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 when we want to ignore\n        # padding in the loss.\n        if padding == \"max_length\" and data_args.ignore_pad_token_for_loss:\n            labels[\"input_ids\"] = [\n                [(_label if _label != tokenizer.pad_token_id else -100) for _label in label] for label in labels[\"input_ids\"]\n            ]\n\n        model_inputs[\"labels\"] = labels[\"input_ids\"]\n        model_inputs['spots'] = examples['spot']\n        model_inputs['asocs'] = examples['asoc']\n        model_inputs['spot_asoc'] = examples['spot_asoc']\n        model_inputs['task'] = examples['task']\n        # pretrain use sample_prompt=True\n        model_inputs['sample_prompt'] = [True] * len(model_inputs['labels'])\n        return model_inputs\n\n    def preprocess_function_eval(examples):\n        model_inputs = preprocess_function(examples)\n        # for dev sample several prompt not all prompt in multi-task setting\n        if data_args.source_prefix.startswith('meta'):\n            model_inputs['sample_prompt'] = [True] * len(model_inputs['labels'])\n\n        return model_inputs\n\n    logger.info(\"Start Data Preprocessing ...\")\n\n    if not data_args.preprocess and not os.path.exists(data_args.preprocessed_folder):\n        raise RuntimeError(\n            f\"cannot found {data_args.preprocessed_folder}, please add `--preprocess for data preprocessing`\")\n\n    if training_args.do_train:\n        if data_args.preprocess:\n            train_dataset = datasets[\"train\"]\n            if data_args.max_train_samples is not None:\n                train_dataset = train_dataset.select(\n                    range(data_args.max_train_samples))\n            train_dataset = train_dataset.map(\n                preprocess_function,\n                batched=True,\n                num_proc=data_args.preprocessing_num_workers,\n                remove_columns=column_names,\n                load_from_cache_file=not data_args.overwrite_cache,\n                features=ProcessedFeature,\n            )\n            if data_args.preprocessed_folder is not None:\n                logger.info(\n                    f\"Save to {data_args.preprocessed_folder}/train.data\")\n                train_dataset.save_to_disk(\n                    f\"{data_args.preprocessed_folder}/train.data\"\n                )\n        else:\n            train_dataset = Dataset.load_from_disk(\n                f\"{data_args.preprocessed_folder}/train.data\"\n            )\n\n    if training_args.do_eval:\n        if data_args.preprocess:\n            max_target_length = data_args.val_max_target_length\n            eval_dataset = datasets[\"validation\"]\n            if data_args.max_val_samples is not None:\n                eval_dataset = eval_dataset.select(\n                    range(data_args.max_val_samples))\n            eval_dataset = eval_dataset.map(\n                preprocess_function_eval,\n                batched=True,\n                num_proc=data_args.preprocessing_num_workers,\n                remove_columns=column_names,\n                load_from_cache_file=not data_args.overwrite_cache,\n                features=ProcessedFeature,\n            )\n            if data_args.preprocessed_folder is not None:\n                logger.info(\n                    f\"Save to {data_args.preprocessed_folder}/eval.data\")\n                eval_dataset.save_to_disk(\n                    f\"{data_args.preprocessed_folder}/eval.data\"\n                )\n        else:\n            eval_dataset = Dataset.load_from_disk(\n                f\"{data_args.preprocessed_folder}/eval.data\"\n            )\n\n    if training_args.do_predict:\n        if data_args.preprocess:\n            max_target_length = data_args.val_max_target_length\n            test_dataset = datasets[\"test\"]\n            if data_args.max_test_samples is not None:\n                test_dataset = test_dataset.select(range(data_args.max_test_samples))\n            test_dataset = test_dataset.map(\n                preprocess_function_eval,\n                batched=True,\n                num_proc=data_args.preprocessing_num_workers,\n                remove_columns=column_names,\n                load_from_cache_file=not data_args.overwrite_cache,\n                features=ProcessedFeature,\n            )\n            if data_args.preprocessed_folder is not None:\n                logger.info(\n                    f\"Save to {data_args.preprocessed_folder}/test.data\")\n                test_dataset.save_to_disk(\n                    f\"{data_args.preprocessed_folder}/test.data\"\n                )\n        else:\n            test_dataset = Dataset.load_from_disk(\n                f\"{data_args.preprocessed_folder}/test.data\"\n            )\n\n    logger.info(\"End Data Preprocessing ...\")\n\n    # Data collator\n    label_pad_token_id = -100 if data_args.ignore_pad_token_for_loss else tokenizer.pad_token_id\n    if data_args.spot_noise > 0 or data_args.asoc_noise > 0:\n        if data_args.decoding_format == 'spotasoc':\n            spot_asoc_nosier = SpotAsocNoiser(\n                spot_noise_ratio=data_args.spot_noise,\n                asoc_noise_ratio=data_args.asoc_noise,\n                null_span=constants.null_span,\n            )\n        else:\n            raise NotImplementedError(\n                f\"decoding_format {data_args.decoding_format} is not implemented.\"\n                )\n    else:\n        spot_asoc_nosier = None\n\n    print(spot_asoc_nosier.spot_noise_ratio) if spot_asoc_nosier else print(\"spot_asoc_nosier is None\")\n\n    data_collator = HybirdDataCollator(\n        # meta bucket need to keep more keys, such as ‘spots', 'asocs', 'spot_asoc', 'sample_prompt'\n        # meta bucket 需要保留更多的 key，例如 ‘spots', 'asocs', 'spot_asoc', 'sample_prompt'\n        meta_bucket_name=['pair'],\n        data_collator_dict={\n            'pair': DataCollatorForMetaSeq2Seq(\n                tokenizer,\n                model=model,\n                label_pad_token_id=label_pad_token_id,\n                pad_to_multiple_of=8 if training_args.fp16 else None,\n                max_prefix_length=data_args.max_prefix_length,\n                max_length=data_args.max_source_length,\n                max_target_length=data_args.max_target_length,\n                negative_sampler=DynamicSSIGenerator(\n                    tokenizer=tokenizer,\n                    schema=record_schema,\n                    positive_rate=data_args.meta_positive_rate,\n                    negative=data_args.meta_negative,\n                    ordered_prompt=data_args.ordered_prompt,\n                ),\n                spot_asoc_nosier=spot_asoc_nosier,\n                decoding_format=data_args.decoding_format,\n            ),\n            'record': DataCollatorForSeq2Seq(\n                tokenizer,\n                model=model,\n                label_pad_token_id=label_pad_token_id,\n                pad_to_multiple_of=8 if training_args.fp16 else None,\n            ),\n            'text': DataCollatorForT5MLM(\n                tokenizer,\n                model=model,\n                noise_density=0.15,\n                mean_noise_span_length=3,\n                pad_token_id=label_pad_token_id,\n                decoder_start_token_id=tokenizer.pad_token_id,\n            )\n        }\n    )\n\n    # Initialize our Trainer\n    trainer = ConstraintSeq2SeqTrainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset if training_args.do_train else None,\n        eval_dataset=eval_dataset if training_args.do_eval else None,\n        tokenizer=tokenizer,\n        data_collator=data_collator,\n        decoding_type_schema=record_schema,\n        decoding_format=data_args.decoding_format,\n        source_prefix=prefix,\n        task=data_args.task,\n    )\n\n    # Training\n    if training_args.do_train:\n        checkpoint = None\n        if model_args.from_checkpoint:\n            if last_checkpoint is not None:\n                checkpoint = last_checkpoint\n            elif os.path.isdir(model_args.model_name_or_path):\n                checkpoint = model_args.model_name_or_path\n\n        train_result = trainer.train(resume_from_checkpoint=checkpoint)\n        trainer.save_model()  # Saves the tokenizer too for easy upload\n\n        output_train_file = os.path.join(\n            training_args.output_dir, \"train_results.txt\")\n        if trainer.is_world_process_zero():\n            with open(output_train_file, \"w\") as writer:\n                logger.info(\"***** Train results *****\")\n                for key, value in sorted(train_result.metrics.items()):\n                    logger.info(f\"  {key} = {value}\")\n                    writer.write(f\"{key} = {value}\\n\")\n\n            # Need to save the state, since Trainer.save_model saves only the tokenizer with the model\n            trainer.state.save_to_json(os.path.join(\n                training_args.output_dir, \"trainer_state.json\"))\n\n    # Evaluation\n    results = {}\n    if training_args.do_eval:\n        logger.info(\"*** Evaluate ***\")\n\n        results = trainer.evaluate(\n            max_length=data_args.val_max_target_length, num_beams=data_args.num_beams)\n        results = {k: round(v, 4) for k, v in results.items()}\n\n        output_eval_file = os.path.join(\n            training_args.output_dir, \"eval_results_seq2seq.txt\")\n        if trainer.is_world_process_zero():\n            with open(output_eval_file, \"w\") as writer:\n                logger.info(\"***** Eval results *****\")\n                for key, value in sorted(results.items()):\n                    logger.info(f\"  {key} = {value}\")\n                    writer.write(f\"{key} = {value}\\n\")\n\n    return results\n\n\ndef _mp_fn(index):\n    # For xla_spawn (TPUs)\n    main()\n\n\nif __name__ == \"__main__\":\n    main()\n"}
{"type": "source_file", "path": "dataset_processing/universal_ie/utils.py", "content": "#!/usr/bin/env python\n# -*- coding:utf-8 -*-\nfrom typing import List\nimport os\nimport sys\n\n\nglobal_mislabel_log = set()\n\n\ndef tokens_to_str(tokens: List[str], language: str = 'en') -> str:\n    if language == 'en':\n        return ' '.join(tokens)\n    elif language == 'zh':\n        return ''.join(tokens)\n    else:\n        raise NotImplementedError('Language %s not supported' % language)\n\n\ndef label_format(s):\n    import re\n\n    def uncamelize(s):\n        re_outer = re.compile(r'([^A-Z ])([A-Z])')\n        re_inner = re.compile(r'\\b[A-Z]+(?=[A-Z][a-z])')\n        sub = re_inner.sub(r'\\g<0> ', re_outer.sub(r'\\1 \\2', s)).lower()\n        return sub\n\n    def remove(s):\n        return s.replace(\"_\", \" \").replace(\"-\", \" \").replace(\".\", \" \")\n\n    s = remove(uncamelize(s)).split()\n    if len(s) > 1 and s[0] == s[1]:\n        s = s[1:]\n    return \" \".join(s)\n\n\ndef load_dict_ini_file(filename):\n    print(\"Warning: `load_dict_ini_file` is deprecated.\")\n    if not os.path.exists(filename):\n        sys.stderr.write(f'[warning] cannot load label mapper from {filename}\\n')\n        return {}\n    mapper = dict()\n    for line in open(filename):\n        key, value = line.strip().split('=')\n        mapper[key] = label_format(value)\n    return mapper\n\n\ndef change_ptb_token_back(token):\n    \"\"\"将 PTBTokenized 的 Token 转换会原始字符串\n\n    Args:\n        token (str): PTBTokenize 后的 Token 字符串\n\n    Returns:\n        str: 原始 Token 字符串\n    \"\"\"\n    ptb_token_map = {\n        '``': '\"',\n        \"''\": '\"',\n        '-LRB-': '(',\n        '-RRB-': ')',\n        '-LSB-': '[',\n        '-RSB-': ']',\n        '-LCB-': '{',\n        '-RCB-': '}',\n    }\n    for ptb_token, raw_token in ptb_token_map.items():\n        if token == ptb_token:\n            return raw_token\n    return token\n\n\ndef change_name_using_label_mapper(label_name, label_mapper):\n    if label_mapper is None or len(label_mapper) == 0:\n        return label_name\n    if label_name not in label_mapper:\n        print(f\"{label_name} not found in mapper\")\n        global global_mislabel_log\n        if label_name not in global_mislabel_log:\n            global_mislabel_log.add(label_name)\n    return label_mapper.get(label_name, label_name)\n"}
{"type": "source_file", "path": "scripts/summary_result.py", "content": "#!/usr/bin/env python\n# -*- coding:utf-8 -*-\nimport json\nimport os\nfrom collections import OrderedDict\nimport numpy as np\n\nfrom tabulate import tabulate\n\nevent_record_result_valid_keys = [\n    'eval_offset-evt-trigger-P', 'eval_offset-evt-trigger-R', 'eval_offset-evt-trigger-F1', 'eval_offset-evt-role-P', 'eval_offset-evt-role-R', 'eval_offset-evt-role-F1',\n    'test_offset-evt-trigger-P', 'test_offset-evt-trigger-R', 'test_offset-evt-trigger-F1', 'test_offset-evt-role-P', 'test_offset-evt-role-R', 'test_offset-evt-role-F1',\n]\n\nspan_record_result_valid_keys = [\n    'eval_offset-ent-P', 'eval_offset-ent-R', 'eval_offset-ent-F1',\n    'test_offset-ent-P', 'test_offset-ent-R', 'test_offset-ent-F1',\n]\n\nrelation_strict_record_result_valid_keys = [\n    'eval_offset-rel-strict-P', 'eval_offset-rel-strict-R', 'eval_offset-rel-strict-F1',\n    'test_offset-rel-strict-P', 'test_offset-rel-strict-R', 'test_offset-rel-strict-F1',\n]\n\nrelation_boundary_record_result_valid_keys = [\n    'eval_offset-rel-boundary-P', 'eval_offset-rel-boundary-R', 'eval_offset-rel-boundary-F1',\n    'test_offset-rel-boundary-P', 'test_offset-rel-boundary-R', 'test_offset-rel-boundary-F1',\n]\n\nrecord_result_valid_keys = [\n    'eval_offset-ent-F1', 'eval_offset-rel-boundary-F1', 'eval_offset-rel-strict-F1', 'eval_offset-evt-trigger-F1', 'eval_offset-evt-role-F1',\n    'test_offset-ent-F1', 'test_offset-rel-boundary-F1', 'test_offset-rel-strict-F1', 'test_offset-evt-trigger-F1', 'test_offset-evt-role-F1',\n]\n\n\ndef align_float(x):\n    return '%.2f' % x if isinstance(x, float) else x\n\n\ndef parse_trainer_state(filename):\n    trainer_state = json.load(open(filename))\n    if trainer_state['best_model_checkpoint'] is not None:\n        return trainer_state['best_model_checkpoint'].split('/')[-1].replace('checkpoint-', '')\n    else:\n        return 'last'\n\n\ndef parse_global_step(filename):\n    return str(json.load(open(filename))['global_step'])\n\n\ndef check_out_of_memory(filename):\n    if os.path.exists(filename):\n        try:\n            with open(filename) as fin:\n                for line in fin:\n                    if 'CUDA out of memory' in line:\n                        return True\n        except UnicodeDecodeError:\n            return False\n    return False\n\n\ndef get_run_name(folder_name, prefix):\n    split_list = folder_name.replace('/', '_').split('_') \\\n        if prefix == 'run' \\\n        else folder_name.split('_')[1:]\n    new_att_list = list()\n    for att in split_list:\n        if att.startswith(prefix):\n            continue\n        new_att_list += [att]\n    return '_'.join(new_att_list)\n\n\nclass ResultSummary:\n    def __init__(self, result_valid_keys):\n        self.result_valid_keys = result_valid_keys\n        self.header_result_valid_keys = [\n            value.replace('trigger', 't').replace('role', 'r').replace('eval', 'e').replace('test', 't').replace(\n                'F1', 'F').replace('offset', 'o').replace('string', 's').replace('strict', 's').replace('boundary', 'b')\n            for value in result_valid_keys]\n        for x, y in zip(self.result_valid_keys, self.header_result_valid_keys):\n            print(\"%s -> %s\" % (x, y))\n\n    def parse_best_log(self, folder_name, file_map, default_key='running'):\n        result = dict()\n\n        eval_result_filename = os.path.join(folder_name, file_map['eval'])\n        test_result_filename = os.path.join(folder_name, file_map['test'])\n\n        lines = list()\n        if os.path.exists(eval_result_filename):\n            lines += open(eval_result_filename).readlines()\n        if os.path.exists(test_result_filename):\n            lines += open(test_result_filename).readlines()\n\n        for line in lines:\n            key, value = line.strip().split('=')\n            if key.strip() not in self.result_valid_keys:\n                continue\n            result[key.strip()] = float(value.strip())\n\n        for key in self.result_valid_keys:\n            if key not in result:\n                result[key] = default_key\n\n        return result\n\n    def get_valid_folder(self, model_folders, file_map):\n        all_result = list()\n\n        for model_folder in model_folders:\n            print(model_folder)\n            sub_folder_list = sorted(os.listdir(model_folder))\n            for sub_folder_name in sub_folder_list:\n                if sub_folder_name.endswith('log') or sub_folder_name.endswith('err'):\n                    continue\n\n                sub_folder = os.path.join(model_folder, sub_folder_name)\n                log_filename = sub_folder + '.log'\n\n                default_key = 'running'\n                state_filename = os.path.join(sub_folder, 'trainer_state.json')\n                trained_folder = sub_folder\n\n                if os.path.exists(log_filename):\n                    out_of_memory = check_out_of_memory(log_filename)\n                else:\n                    out_of_memory = False\n\n                if out_of_memory:\n                    result = {key: 'OOM' for key in self.result_valid_keys}\n                    checkpoint = 'OOM'\n                else:\n                    result = self.parse_best_log(\n                        trained_folder, file_map, default_key)\n                    checkpoint = parse_trainer_state(state_filename) if os.path.exists(\n                        state_filename) else default_key\n                    global_step = parse_global_step(state_filename) if os.path.exists(\n                        state_filename) else default_key\n                    checkpoint = checkpoint + '/' + global_step\n\n                all_result += [[sub_folder, checkpoint, result]]\n        return all_result\n\n    def result_to_table(self, all_result, sort_key=0):\n        table = list()\n        for sub_folder_name, checkpoint, result in all_result:\n            table += [[sub_folder_name, checkpoint] +\n                      [result.get(key, 'running') for key in self.result_valid_keys]]\n\n        table = [[align_float(x) for x in y] for y in table]\n\n        table.sort()\n        table.sort(key=lambda x: x[sort_key])\n\n        print(tabulate(table, headers=[\n              'exp', 'checkpoint'] + self.header_result_valid_keys))\n\n    def result_to_table_reduce(self, all_result, sort_key=0, reduce_function=np.mean, reduce_key='run'):\n        table = list()\n        sub_run = OrderedDict()\n        for sub_folder_name, checkpoint, result in all_result:\n\n            sub_run_name = get_run_name(sub_folder_name, reduce_key)\n            if sub_run_name not in sub_run:\n                sub_run[sub_run_name] = list()\n\n            sub_run_result = [result.get(key, 'running')\n                              for key in self.result_valid_keys]\n            if 'running' in sub_run_result or 'OOM' in sub_run_result:\n                continue\n\n            sub_run[sub_run_name] += [sub_run_result]\n\n        for sub_run_name, sub_run_results in sub_run.items():\n            if len(sub_run_results) == 0:\n                table += [[sub_run_name, 0] + ['-']]\n            else:\n                table += [[sub_run_name, len(sub_run_results)] +\n                          list(reduce_function(sub_run_results, 0))]\n\n        table = [[align_float(x) for x in y] for y in table]\n\n        table.sort()\n        table.sort(key=lambda x: x[sort_key])\n\n        print(tabulate(table, headers=['exp', 'num'] +\n                       self.header_result_valid_keys))\n\n\ndef main():\n    record_valid_keys_map = {\n        'entity': span_record_result_valid_keys,\n        'relation': relation_strict_record_result_valid_keys,\n        'relation-boundary': relation_boundary_record_result_valid_keys,\n        'event': event_record_result_valid_keys,\n        'record': record_result_valid_keys,\n    }\n\n    import argparse\n    parser = argparse.ArgumentParser(\n        description='Summary Multi-run Result'\n    )\n    parser.add_argument('-model', dest='model', default=['output'], nargs='+',\n                        help='Output Model Folder Path')\n    parser.add_argument('-sort', dest='sort', default=0,\n                        type=int, help='Sort Column Index')\n    parser.add_argument('-mean', dest='mean', action='store_true',\n                        help='Reduce by mean Function')\n    parser.add_argument('-std', dest='std', action='store_true',\n                        help='Reduce by std Function')\n    parser.add_argument('-record', dest='record', default='record',\n                        choices=record_valid_keys_map.keys(),\n                        help='Record Type')\n    parser.add_argument('-string', dest='offset', action='store_false',\n                        help='Report String Match Result')\n    parser.add_argument('-offset', dest='offset', action='store_true',\n                        help='Report Offset Match Result (default)')\n    parser.set_defaults(offset=True)\n    parser.add_argument('-reduce', dest='reduce', default='run',\n                        help='Reduce Key, default is `run`')\n    options = parser.parse_args()\n\n    if options.record in record_valid_keys_map:\n        file_map = {\n            'eval': 'eval_results.txt',\n            'test': 'test_results.txt',\n        }\n    else:\n        raise NotImplementedError('Invalid Record Type: %s' % options.record)\n\n    result_valid_keys = record_valid_keys_map[options.record]\n\n    if not options.offset:\n        result_valid_keys = [key.replace('offset', 'string')\n                             for key in result_valid_keys]\n\n    result_summary = ResultSummary(\n        result_valid_keys=result_valid_keys\n    )\n    print(options.model)\n\n    def check_valid_model(x):\n        return not (os.path.isfile(x) or x.endswith('_log'))\n\n    valid_model_paths = filter(check_valid_model, options.model)\n\n    all_result = result_summary.get_valid_folder(\n        model_folders=valid_model_paths,\n        file_map=file_map,\n    )\n\n    if options.mean:\n        result_summary.result_to_table_reduce(\n            all_result,\n            sort_key=options.sort,\n            reduce_function=np.mean,\n            reduce_key=options.reduce,\n        )\n    elif options.std:\n        result_summary.result_to_table_reduce(\n            all_result,\n            sort_key=options.sort,\n            reduce_function=np.std,\n            reduce_key=options.reduce\n        )\n    else:\n        result_summary.result_to_table(all_result, sort_key=options.sort)\n\n\nif __name__ == \"__main__\":\n    main()\n"}
{"type": "source_file", "path": "uie/extraction/extraction_metrics.py", "content": "#!/usr/bin/env python\n# -*- coding:utf-8 -*-\nfrom typing import List\nfrom uie.extraction.record_schema import RecordSchema\nfrom uie.extraction.predict_parser import get_predict_parser, PredictParser\nfrom uie.extraction.scorer import Metric, RecordMetric, OrderedRecordMetric\n\n\ndef eval_pred(predict_parser: PredictParser, gold_list, pred_list, text_list=None, raw_list=None):\n    well_formed_list, counter = predict_parser.decode(\n        gold_list, pred_list, text_list, raw_list\n    )\n\n    spot_metric = Metric()\n    asoc_metric = Metric()\n    record_metric = RecordMetric()\n    ordered_record_metric = OrderedRecordMetric()\n\n    for instance in well_formed_list:\n        spot_metric.count_instance(instance['gold_spot'], instance['pred_spot'])\n        asoc_metric.count_instance(instance['gold_asoc'], instance['pred_asoc'])\n        record_metric.count_instance(instance['gold_record'], instance['pred_record'])\n        ordered_record_metric.count_instance(instance['gold_record'], instance['pred_record'])\n\n    spot_result = spot_metric.compute_f1(prefix='spot-')\n    asoc_result = asoc_metric.compute_f1(prefix='asoc-')\n    record_result = record_metric.compute_f1(prefix='record-')\n    ordered_record_result = ordered_record_metric.compute_f1(prefix='ordered-record-')\n\n    overall_f1 = spot_result.get('spot-F1', 0.) + asoc_result.get('asoc-F1', 0.)\n    # print(counter)\n    result = {'overall-F1': overall_f1}\n    result.update(spot_result)\n    result.update(asoc_result)\n    result.update(record_result)\n    result.update(ordered_record_result)\n    result.update(counter)\n    return result\n\n\ndef get_extract_metrics(pred_lns: List[str], tgt_lns: List[str], label_constraint: RecordSchema, decoding_format='tree'):\n    predict_parser = get_predict_parser(decoding_schema=decoding_format, label_constraint=label_constraint)\n    return eval_pred(\n        predict_parser=predict_parser,\n        gold_list=tgt_lns,\n        pred_list=pred_lns\n    )\n"}
{"type": "source_file", "path": "uie/extraction/noiser/spot_asoc_noiser.py", "content": "#!/usr/bin/env python\n# -*- coding:utf-8 -*-\nfrom uie.extraction import constants\nfrom dataclasses import dataclass\nimport numpy as np\nfrom uie.extraction.utils import *\n\n\n@dataclass\nclass SpotAsocNoiser:\n    spot_noise_ratio: float = 0.1\n    asoc_noise_ratio: float = 0.1\n    null_span: str = constants.null_span\n\n    def random_insert_spot(self, spot_asoc, spot_label_list=None):\n        \"\"\"随机插入 Spot，类别从 spot_label_list 中自动选择\n\n        Args:\n            spot_asoc ([type]): [description]\n            spot_label_list ([type], optional): [description]. Defaults to None.\n\n        Returns:\n            [type]: [description]\n        \"\"\"\n        if spot_label_list is None or len(spot_label_list) == 0:\n            return spot_asoc\n        random_num = sum(np.random.binomial(1, self.spot_noise_ratio, len(spot_asoc)))\n        for _ in range(random_num):\n            random_position = np.random.randint(low=0, high=len(spot_asoc))\n            random_label = np.random.choice(spot_label_list)\n            spot_asoc.insert(\n                random_position,\n                {\"span\": self.null_span, \"label\": random_label, 'asoc': list()}\n            )\n        return spot_asoc\n\n    def random_insert_asoc(self, spot_asoc, asoc_label_list=None):\n        \"\"\"随机插入 Asoc，类别从 asoc_label_list 中自动选择\n\n        Args:\n            spot_asoc ([type]): [description]\n            asoc_label_list ([type], optional): [description]. Defaults to None.\n\n        Returns:\n            [type]: [description]\n        \"\"\"\n        if asoc_label_list is None or len(asoc_label_list) == 0:\n            return spot_asoc\n        # asoc_sum = sum([len(x['asoc']) for x in spot_asoc])\n        spot_sum = len(spot_asoc)\n        random_num = sum(np.random.binomial(1, self.asoc_noise_ratio, spot_sum))\n        for _ in range(random_num):\n            random_label = np.random.choice(asoc_label_list)\n            spot_position = np.random.randint(low=0, high=len(spot_asoc))\n            asoc_position = np.random.randint(low=0, high=len(spot_asoc[spot_position]['asoc']) + 1)\n            spot_asoc[spot_position]['asoc'].insert(\n                asoc_position,\n                (random_label, self.null_span)\n            )\n        return spot_asoc\n\n    def add_noise(self, spot_asoc, spot_label_list, asoc_label_list):\n        spot_asoc = self.random_insert_asoc(\n            spot_asoc=spot_asoc,\n            asoc_label_list=asoc_label_list,\n        )\n        spot_asoc = self.random_insert_spot(\n            spot_asoc=spot_asoc,\n            spot_label_list=spot_label_list,\n        )\n        return spot_asoc\n\n\ndef main():\n    from uie.extraction.constants import BaseStructureMarker\n    structure_marker = BaseStructureMarker()\n    spot_asoc = [{\"span\": \"analyzer\", \"label\": \"generic\", \"asoc\": []}, {\"span\": \"`` Amorph ''\", \"label\": \"method\", \"asoc\": []}]\n\n    spot_asoc_noiser = SpotAsocNoiser(\n        spot_noise_ratio=0.5,\n        asoc_noise_ratio=0.5,\n    )\n    spot_asoc_noiser.add_noise(\n        spot_asoc=spot_asoc,\n        spot_label_list=['A', 'B', 'C'],\n        asoc_label_list=['D', 'E', 'F'],\n    )\n    target = convert_spot_asoc(\n        spot_asoc_instance=spot_asoc,\n        structure_maker=structure_marker\n    )\n\n    target = convert_spot_asoc(\n        spot_asoc_instance=spot_asoc,\n        structure_maker=structure_marker\n    )\n\n    replace_map = {\n        '<extra_id_0>': ' ( ',\n        '<extra_id_1>': ' ) ',\n        '<extra_id_5>': ':',\n    }\n    from nltk.tree import Tree\n    for old, new in replace_map.items():\n        target = target.replace(old, new)\n    print(target)\n    Tree.fromstring(target).pretty_print()\n\n\nif __name__ == \"__main__\":\n    main()\n"}
{"type": "source_file", "path": "uie/extraction/noiser/__init__.py", "content": "#!/usr/bin/env python\n# -*- coding:utf-8 -*-\n"}
{"type": "source_file", "path": "scripts/sel2record.py", "content": "#!/usr/bin/env python\n# -*- coding:utf-8 -*-\nimport argparse\nimport os\nimport logging\nimport json\n\nfrom uie.sel2record.record import MapConfig\nfrom uie.sel2record.sel2record import SEL2Record\n\nlogger = logging.getLogger(__name__)\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument('-g', dest='gold_folder', help='Gold Folder')\n    parser.add_argument('-p', dest='pred_folder', nargs='+', help='Pred Folder')\n\n    parser.add_argument('-c', '--config', dest='map_config', help='Offset Mapping Config')\n    parser.add_argument('-d', dest='decoding', default='spotasoc')\n    parser.add_argument('-v', '--verbose', dest='verbose',\n                        action='store_true', help='More details information.')\n    options = parser.parse_args()\n\n    map_config = MapConfig.load_from_yaml(options.map_config)\n    schema_dict = SEL2Record.load_schema_dict(options.gold_folder)\n    sel2record = SEL2Record(\n        schema_dict=schema_dict,\n        decoding_schema=options.decoding,\n        map_config=map_config,\n    )\n\n    data_dict = {\n        'eval': ['eval_preds_seq2seq.txt', 'val.json', 'eval_preds_record.txt'],\n        'test': ['test_preds_seq2seq.txt', 'test.json', 'test_preds_record.txt'],\n    }\n\n    for pred_folder in options.pred_folder:\n        gold_folder = options.gold_folder\n\n        for data_key, (generation, gold_file, record_file) in data_dict.items():\n\n            pred_filename = os.path.join(pred_folder, generation)\n\n            if not os.path.exists(pred_filename):\n                logger.warning(\"%s not found.\\n\" % pred_filename)\n                continue\n\n            gold_filename = os.path.join(gold_folder, gold_file)\n\n            print(\"pred:\", pred_filename) if options.verbose else None\n            print(\"gold:\", gold_filename) if options.verbose else None\n\n            # Only using text and tokens in Gold file\n            gold_list = [json.loads(line) for line in open(gold_filename)]\n            gold_text_list = [gold['text'] for gold in gold_list]\n            gold_token_list = [gold['tokens'] for gold in gold_list]\n\n            pred_list = [line.strip() for line in open(pred_filename).readlines()]\n\n            assert len(gold_text_list) == len(pred_list)\n\n            pred_records = list()\n            for pred, text, tokens in zip(pred_list, gold_text_list, gold_token_list):\n                pred_record = sel2record.sel2record(pred, text, tokens)\n                pred_records += [pred_record]\n\n            with open(os.path.join(pred_folder, record_file), 'w') as output:\n                for record in pred_records:\n                    output.write(json.dumps(record, ensure_ascii=False) + '\\n')\n\n\nif __name__ == \"__main__\":\n    main()\n"}
{"type": "source_file", "path": "scripts/eval_extraction.py", "content": "#!/usr/bin/env python\n# -*- coding:utf-8 -*-\nimport argparse\nimport json\nimport os\nimport sys\nimport numpy as np\nfrom pprint import pprint\nfrom uie.extraction.scorer import EntityScorer, RelationScorer, EventScorer\n\n\ndef read_file(file_name):\n    return [line for line in open(file_name).readlines()]\n\n\ndef write_to_file(result, output_filename, prefix=None):\n    with open(output_filename, 'w') as output:\n        for key, value in result.items():\n            if prefix:\n                key = '%s_%s' % (prefix, key)\n            output.write(\"%s=%s\\n\" % (key, value))\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('-g', dest='gold_folder', help=\"Golden Dataset folder\")\n    parser.add_argument('-p', dest='pred_folder', nargs='+', help=\"Predicted model folder\")\n    parser.add_argument('-v', dest='verbose', action='store_true', help='Show more information during running')\n    parser.add_argument('-w', dest='write_to_file', action='store_true', help=\"Write evaluation results to predicted folder\")\n    parser.add_argument('-m', dest='match_mode', default='normal', choices=['set', 'normal', 'multimatch'])\n    parser.add_argument('-case', dest='case', action='store_true', help='Show case study')\n    options = parser.parse_args()\n\n    data_dict = {\n        'eval': ['eval_preds_record.txt', 'val.json'],\n        'test': ['test_preds_record.txt', 'test.json'],\n    }\n\n    task_dict = {\n        'entity': EntityScorer,\n        'relation': RelationScorer,\n        'event': EventScorer,\n    }\n\n    result_list = {'eval': list(), 'test': list()}\n    for pred_folder in options.pred_folder:\n        gold_folder = options.gold_folder\n\n        for data_key, (generation, gold_file) in data_dict.items():\n\n            gold_filename = os.path.join(gold_folder, gold_file)\n            pred_filename = os.path.join(pred_folder, generation)\n\n            if not os.path.exists(pred_filename):\n                sys.stderr.write(\"%s not found.\\n\" % pred_filename)\n                continue\n\n            print(\"pred:\", pred_filename)\n            print(\"gold:\", gold_filename)\n\n            if options.case:\n                for pred_line, gold_line in zip(read_file(pred_filename), read_file(gold_filename)):\n                    gold_instance = json.loads(gold_line)\n                    pred_instance = json.loads(pred_line)\n                    print('=========================')\n                    print(gold_instance['text'])\n                    for task in task_dict:\n                        scorer = task_dict[task]\n                        gold = scorer.load_gold_list([gold_instance[task]])[0]\n                        pred = scorer.load_pred_list([pred_instance[task]])[0]\n                        min_length = max(\n                            len(gold['string']),\n                            len(pred['string']),\n                            len(gold.get('string_trigger', [])),\n                            len(pred.get('string_trigger', [])),\n                            len(gold.get('string_role', [])),\n                            len(pred.get('string_role', [])),\n                        )\n                        if min_length == 0:\n                            continue\n                        if task == 'entity':\n                            print(\"Entity Gold:\", sorted(gold['string']))\n                            print(\"Entity Pred:\", sorted(pred['string']))\n                        if task == 'relation':\n                            print(\"Relation Gold:\", sorted(gold['string']))\n                            print(\"Relation Pred:\", sorted(pred['string']))\n                        if task == 'event':\n                            print(\"Event Gold Trigger:\", sorted(gold['string_trigger']))\n                            print(\"Event Pred Trigger:\", sorted(pred['string_trigger']))\n                            print(\"Event Gold Role   :\", sorted(gold['string_role']))\n                            print(\"Event Pred Role   :\", sorted(pred['string_role']))\n\n            results = dict()\n            for task in task_dict:\n                if task not in json.loads(read_file(pred_filename)[0]):\n                    continue\n                scorer = task_dict[task]\n                gold_list = [json.loads(line)[task] for line in read_file(gold_filename)]\n                pred_list = [json.loads(line)[task] for line in read_file(pred_filename)]\n\n                assert len(pred_list) == len(gold_list)\n                gold_instance_list = scorer.load_gold_list(gold_list)\n                pred_instance_list = scorer.load_pred_list(pred_list)\n                assert len(pred_instance_list) == len(gold_instance_list)\n                sub_results = scorer.eval_instance_list(\n                    gold_instance_list=gold_instance_list,\n                    pred_instance_list=pred_instance_list,\n                    verbose=options.verbose,\n                    match_mode=options.match_mode,\n                )\n                results.update(sub_results)\n\n            pprint(results)\n            result_list[data_key] += [results]\n\n            if options.write_to_file:\n                output_filename = \"%s/%s_results.txt\" % (pred_folder, data_key)\n                write_to_file(\n                    result=results,\n                    output_filename=output_filename,\n                    prefix=data_key,\n                )\n\n    print(\"===========> AVG <===========\")\n\n    for data_key in data_dict:\n        if len(result_list[data_key]) < 1:\n            continue\n        for key in result_list[data_key][0]:\n            ave = np.mean([result[key] for result in result_list[data_key]])\n            print(data_key, key, ave)\n\n\nif __name__ == \"__main__\":\n    main()\n"}
{"type": "source_file", "path": "uie/extraction/predict_parser/spotasoc_predict_parser.py", "content": "#!/usr/bin/env python\n# -*- coding:utf-8 -*-\nfrom collections import Counter\nimport logging\nfrom nltk.tree import ParentedTree\nimport re\nfrom typing import Tuple, List, Dict\n\n\nfrom uie.extraction.constants import (\n    null_span,\n    type_start,\n    type_end,\n    span_start,\n)\nfrom uie.extraction.predict_parser.predict_parser import PredictParser\nfrom uie.extraction.predict_parser.utils import fix_unk_from_text\n\nlogger = logging.getLogger(__name__)\n\n\nleft_bracket = '【'\nright_bracket = '】'\nbrackets = left_bracket + right_bracket\n\nsplit_bracket = re.compile(r\"<extra_id_\\d>\")\n\n\ndef add_space(text):\n    \"\"\"\n    add space between special token\n    \"\"\"\n    new_text_list = list()\n    for item in zip(split_bracket.findall(text), split_bracket.split(text)[1:]):\n        new_text_list += item\n    return ' '.join(new_text_list)\n\n\ndef convert_bracket(text):\n    text = add_space(text)\n    for start in [type_start]:\n        text = text.replace(start, left_bracket)\n    for end in [type_end]:\n        text = text.replace(end, right_bracket)\n    return text\n\n\ndef find_bracket_num(tree_str):\n    \"\"\"\n    Count Bracket Number (num_left - num_right), 0 indicates num_left = num_right\n    \"\"\"\n    count = 0\n    for char in tree_str:\n        if char == left_bracket:\n            count += 1\n        elif char == right_bracket:\n            count -= 1\n        else:\n            pass\n    return count\n\n\ndef check_well_form(tree_str):\n    return find_bracket_num(tree_str) == 0\n\n\ndef clean_text(tree_str):\n    count = 0\n    sum_count = 0\n\n    tree_str_list = tree_str.split()\n\n    for index, char in enumerate(tree_str_list):\n        if char == left_bracket:\n            count += 1\n            sum_count += 1\n        elif char == right_bracket:\n            count -= 1\n            sum_count += 1\n        else:\n            pass\n        if count == 0 and sum_count > 0:\n            return ' '.join(tree_str_list[:index + 1])\n    return ' '.join(tree_str_list)\n\n\ndef resplit_label_span(label, span, split_symbol=span_start):\n    label_span = label + ' ' + span\n\n    if split_symbol in label_span:\n        splited_label_span = label_span.split(split_symbol)\n        if len(splited_label_span) == 2:\n            return splited_label_span[0].strip(), splited_label_span[1].strip()\n\n    return label, span\n\n\ndef add_bracket(tree_str):\n    \"\"\"add right bracket to fix ill-formed expression\n    \"\"\"\n    tree_str_list = tree_str.split()\n    bracket_num = find_bracket_num(tree_str_list)\n    tree_str_list += [right_bracket] * bracket_num\n    return ' '.join(tree_str_list)\n\n\ndef get_tree_str(tree):\n    \"\"\"get str from sel tree\n    \"\"\"\n    str_list = list()\n    for element in tree:\n        if isinstance(element, str):\n            str_list += [element]\n    return ' '.join(str_list)\n\n\ndef rewrite_label_span(label, span, label_set=None, text=None):\n\n    # Invalid Type\n    if label_set and label not in label_set:\n        logger.debug('Invalid Label: %s' % label)\n        return None, None\n\n    # Fix unk using Text\n    if text is not None and '<unk>' in span:\n        span = fix_unk_from_text(span, text, '<unk>')\n\n    # Invalid Text Span\n    if text is not None and span not in text:\n        logger.debug('Invalid Text Span: %s\\n%s\\n' % (span, text))\n        return None, None\n\n    return label, span\n\n\nclass SpotAsocPredictParser(PredictParser):\n\n    def decode(self, gold_list, pred_list, text_list=None, raw_list=None\n               ) -> Tuple[List[Dict], Counter]:\n        \"\"\"\n\n        :param gold_list:\n        :param pred_list:\n        :param text_list:\n        :param raw_list:\n        :return:\n            dict:\n                pred_spot -> [(type1, text1), (type2, text2), ...]\n                gold_spot -> [(type1, text1), (type2, text2), ...]\n                pred_asoc -> [(spot type1, asoc type1, text1), (spot type2, asoc type2, text2), ...]\n                gold_asoc -> [(spot type1, asoc type1, text1), (spot type2, asoc type2, text2), ...]\n                pred_record -> [{'type': type1, 'text': text1, 'roles': [(spot type1, asoc type1, text1), ...]},\n                                {'type': type2, 'text': text2, 'roles': [(spot type2, asoc type2, text2), ...]},\n                                ]\n                gold_record -> [{'type': type1, 'text': text1, 'roles': [(spot type1, asoc type1, text1), ...]},\n                                {'type': type2, 'text': text2, 'roles': [(spot type2, asoc type2, text2), ...]},\n                                ]\n            Counter:\n        \"\"\"\n        counter = Counter()\n        well_formed_list = []\n\n        if gold_list is None or len(gold_list) == 0:\n            gold_list = [\"%s%s\" % (type_start, type_end)] * len(pred_list)\n\n        if text_list is None:\n            text_list = [None] * len(pred_list)\n\n        if raw_list is None:\n            raw_list = [None] * len(pred_list)\n\n        for gold, pred, text, raw_data in zip(gold_list, pred_list, text_list,\n                                              raw_list):\n            gold = convert_bracket(gold)\n            pred = convert_bracket(pred)\n\n            pred = clean_text(pred)\n\n            try:\n                gold_tree = ParentedTree.fromstring(gold, brackets=brackets)\n            except ValueError:\n                logger.warning(f\"Ill gold: {gold}\")\n                logger.warning(f\"Fix gold: {add_bracket(gold)}\")\n                gold_tree = ParentedTree.fromstring(\n                    add_bracket(gold), brackets=brackets)\n                counter.update(['gold_tree add_bracket'])\n\n            instance = {\n                'gold': gold,\n                'pred': pred,\n                'gold_tree': gold_tree,\n                'text': text,\n                'raw_data': raw_data\n            }\n\n            counter.update(['gold_tree' for _ in gold_tree])\n\n            instance['gold_spot'], instance['gold_asoc'], instance['gold_record'] = self.get_record_list(\n                sel_tree=instance[\"gold_tree\"],\n                text=instance['text']\n            )\n\n            try:\n                if not check_well_form(pred):\n                    pred = add_bracket(pred)\n                    counter.update(['fixed'])\n\n                pred_tree = ParentedTree.fromstring(pred, brackets=brackets)\n                counter.update(['pred_tree' for _ in pred_tree])\n\n                instance['pred_tree'] = pred_tree\n                counter.update(['well-formed'])\n\n            except ValueError:\n                counter.update(['ill-formed'])\n                logger.debug('ill-formed', pred)\n                instance['pred_tree'] = ParentedTree.fromstring(\n                    left_bracket + right_bracket,\n                    brackets=brackets\n                )\n\n            instance['pred_spot'], instance['pred_asoc'], instance['pred_record'] = self.get_record_list(\n                sel_tree=instance[\"pred_tree\"],\n                text=instance['text']\n            )\n\n            well_formed_list += [instance]\n\n        return well_formed_list, counter\n\n    def get_record_list(self, sel_tree, text=None):\n        \"\"\" Convert single sel expression to extraction records\n        Args:\n            sel_tree (Tree): sel tree\n            text (str, optional): _description_. Defaults to None.\n        Returns:\n            spot_list: list of (spot_type: str, spot_span: str)\n            asoc_list: list of (spot_type: str, asoc_label: str, asoc_text: str)\n            record_list: list of {'asocs': list(), 'type': spot_type, 'spot': spot_text}\n        \"\"\"\n\n        spot_list = list()\n        asoc_list = list()\n        record_list = list()\n\n        for spot_tree in sel_tree:\n\n            # Drop incomplete tree\n            if isinstance(spot_tree, str) or len(spot_tree) == 0:\n                continue\n\n            spot_type = spot_tree.label()\n            spot_text = get_tree_str(spot_tree)\n            spot_type, spot_text = resplit_label_span(\n                spot_type, spot_text)\n            spot_type, spot_text = rewrite_label_span(\n                label=spot_type,\n                span=spot_text,\n                label_set=self.spot_set,\n                text=text\n            )\n\n            # Drop empty generated span\n            if spot_text is None or spot_text == null_span:\n                continue\n            # Drop empty generated type\n            if spot_type is None:\n                continue\n            # Drop invalid spot type\n            if self.spot_set is not None and spot_type not in self.spot_set:\n                continue\n\n            record = {'asocs': list(),\n                      'type': spot_type,\n                      'spot': spot_text}\n\n            for asoc_tree in spot_tree:\n                if isinstance(asoc_tree, str) or len(asoc_tree) < 1:\n                    continue\n\n                asoc_label = asoc_tree.label()\n                asoc_text = get_tree_str(asoc_tree)\n                asoc_label, asoc_text = resplit_label_span(\n                    asoc_label, asoc_text)\n                asoc_label, asoc_text = rewrite_label_span(\n                    label=asoc_label,\n                    span=asoc_text,\n                    label_set=self.role_set,\n                    text=text\n                )\n\n                # Drop empty generated span\n                if asoc_text is None or asoc_text == null_span:\n                    continue\n                # Drop empty generated type\n                if asoc_label is None:\n                    continue\n                # Drop invalid asoc type\n                if self.role_set is not None and asoc_label not in self.role_set:\n                    continue\n\n                asoc_list += [(spot_type, asoc_label, asoc_text)]\n                record['asocs'] += [(asoc_label, asoc_text)]\n\n            spot_list += [(spot_type, spot_text)]\n            record_list += [record]\n\n        return spot_list, asoc_list, record_list\n"}
{"type": "source_file", "path": "uie/extraction/predict_parser/utils.py", "content": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\nimport re\n\n\ndef fix_unk_from_text(span, text, unk='<unk>'):\n    \"\"\"\n    Find span from the text to fix unk in the generated span\n    从 text 中找到 span，修复span\n\n    Example:\n    span = \"<unk> colo e Bengo\"\n    text = \"At 159 meters above sea level , Angola International Airport is located at Ícolo e Bengo , part of Luanda Province , in Angola .\"\n\n    span = \"<unk> colo e Bengo\"\n    text = \"Ícolo e Bengo , part of Luanda Province , in Angola .\"\n\n    span = \"Arr<unk> s negre\"\n    text = \"The main ingredients of Arròs negre , which is from Spain , are white rice , cuttlefish or squid , cephalopod ink , cubanelle and cubanelle peppers . Arròs negre is from the Catalonia region .\"\n\n    span = \"colo <unk>\"\n    text = \"At 159 meters above sea level , Angola International Airport is located at e Bengo , part of Luanda Province , in Angola . coloÍ\"\n\n    span = \"Tarō As<unk>\"\n    text = \"The leader of Japan is Tarō Asō .\"\n\n    span = \"Tar<unk> As<unk>\"\n    text = \"The leader of Japan is Tarō Asō .\"\n\n    span = \"<unk>Tar As<unk>\"\n    text = \"The leader of Japan is ōTar Asō .\"\n    \"\"\"\n    if unk not in span:\n        return span\n\n    def clean_wildcard(x):\n        sp = \".*?()[]+\"\n        return re.sub(\"(\"+\"|\".join([f\"\\\\{s}\" for s in sp])+\")\", \"\\\\\\\\\\g<1>\", x)\n\n    match = r'\\s*\\S+\\s*'.join([clean_wildcard(item.strip()) for item in span.split(unk)])\n\n    result = re.search(match, text)\n\n    if not result:\n        return span\n    return result.group().strip()\n\n\ndef test_fix_unk_from_text():\n\n    span_text_list = [\n        (\"<unk> colo e Bengo\",\n         \"At 159 meters above sea level , Angola International Airport is located at Ícolo e Bengo , part of Luanda Province , in Angola .\",\n         \"Ícolo e Bengo\"),\n        (\"<unk> colo e Bengo\",\n         \"Ícolo e Bengo , part of Luanda Province , in Angola .\",\n         \"Ícolo e Bengo\"),\n        (\"Arr<unk> s negre\",\n         \"The main ingredients of Arròs negre , which is from Spain , are white rice , cuttlefish or squid , cephalopod ink , cubanelle and cubanelle peppers . Arròs negre is from the Catalonia region .\",\n         \"Arròs negre\"),\n        (\"colo <unk>\",\n         \"At 159 meters above sea level , Angola International Airport is located at e Bengo , part of Luanda Province , in Angola . coloÍ\",\n         \"coloÍ\"),\n        (\"Tarō As<unk>\", \"The leader of Japan is Tarō Asō .\", \"Tarō Asō\"),\n        (\"Tar<unk> As<unk>\", \"The leader of Japan is Tarō Asō .\", \"Tarō Asō\"),\n        (\"<unk>Tar As<unk>\", \"The leader of Japan is ōTar Asō .\", \"ōTar Asō\"),\n        (\"Atatürk Monument ( <unk> zmir )\",\n         \"The Atatürk Monument ( İzmir ) can be found in Turkey .\",\n         \"Atatürk Monument ( İzmir )\"),\n        (\"The Atatürk Monument [ <unk> zmir ]\",\n         \"The Atatürk Monument [ İzmir ] can be found in Turkey .\",\n         \"The Atatürk Monument [ İzmir ]\")\n    ]\n\n    for span, text, gold in span_text_list:\n        print(span, '|', fix_unk_from_text(span, text))\n        assert fix_unk_from_text(span, text) == gold\n\n\nif __name__ == \"__main__\":\n    test_fix_unk_from_text()\n"}
{"type": "source_file", "path": "uie/extraction/record_schema.py", "content": "#!/usr/bin/env python\n# -*- coding:utf-8 -*-\nimport json\nfrom collections import defaultdict\nfrom typing import List\n\n\nclass RecordSchema:\n    def __init__(self, type_list, role_list, type_role_dict):\n        self.type_list = type_list\n        self.role_list = role_list\n        self.type_role_dict = type_role_dict\n\n    def __repr__(self) -> str:\n        return f\"Type: {self.type_list}\\n\" \\\n            f\"Role: {self.role_list}\\n\" \\\n            f\"Map: {self.type_role_dict}\"\n\n    @staticmethod\n    def get_empty_schema():\n        return RecordSchema(type_list=list(), role_list=list(), type_role_dict=dict())\n\n    @staticmethod\n    def read_from_file(filename):\n        lines = open(filename).readlines()\n        type_list = json.loads(lines[0])\n        role_list = json.loads(lines[1])\n        type_role_dict = json.loads(lines[2])\n        return RecordSchema(type_list, role_list, type_role_dict)\n\n    def write_to_file(self, filename):\n        with open(filename, 'w') as output:\n            output.write(json.dumps(self.type_list) + '\\n')\n            output.write(json.dumps(self.role_list) + '\\n')\n            output.write(json.dumps(self.type_role_dict) + '\\n')\n\n\ndef merge_schema(schema_list: List[RecordSchema]):\n    type_set = set()\n    role_set = set()\n    type_role_dict = defaultdict(list)\n\n    for schema in schema_list:\n\n        for type_name in schema.type_list:\n            type_set.add(type_name)\n\n        for role_name in schema.role_list:\n            role_set.add(role_name)\n\n        for type_name in schema.type_role_dict:\n            type_role_dict[type_name] += schema.type_role_dict[type_name]\n\n    for type_name in type_role_dict:\n        type_role_dict[type_name] = list(set(type_role_dict[type_name]))\n\n    return RecordSchema(type_list=list(type_set),\n                        role_list=list(role_set),\n                        type_role_dict=type_role_dict,\n                        )\n"}
