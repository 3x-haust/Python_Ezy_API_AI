{"repo_info": {"repo_name": "docprompting", "repo_owner": "shuyanzhou", "repo_url": "https://github.com/shuyanzhou/docprompting"}}
{"type": "test_file", "path": "generator/fid/test_reader_simple.py", "content": "# Copyright (c) Facebook, Inc. and its affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\nimport json\nimport os\n\nimport torch\nimport transformers\nimport numpy as np\nfrom pathlib import Path\nimport torch.distributed as dist\nfrom torch.utils.data import DataLoader, SequentialSampler\nfrom dataset_helper.conala.gen_metric import _bleu as conala_bleu\nfrom dataset_helper.tldr.gen_metric import tldr_metrics\nfrom tqdm import tqdm\n\nimport src.slurm\nimport src.util\nfrom src.options import Options\nimport src.data\nimport src.model\n\nTQDM_DISABLED = os.environ['TQDM_DISABLED'] if 'TQDM_DISABLED' in os.environ else False\n\ndef evaluate(model, dataset, dataloader, tokenizer, opt):\n    loss, curr_loss = 0.0, 0.0\n    model.eval()\n    if hasattr(model, \"module\"):\n        model = model.module\n    if opt.write_crossattention_scores:\n        model.overwrite_forward_crossattention()\n        model.reset_score_storage()\n    total = 0\n\n    with torch.no_grad():\n        result_d = []\n\n        with open(f\"{opt.checkpoint_path}/gold.gold\", \"w+\") as fg, \\\n                open(f'{opt.checkpoint_path}/pred.pred', 'w+') as fp, \\\n                open(opt.result_file, 'w+') as fr:\n            for i, batch in enumerate(tqdm(dataloader, disable=TQDM_DISABLED)):\n                (idx, _, _, context_ids, context_mask) = batch\n\n                if opt.write_crossattention_scores:\n                    model.reset_score_storage()\n\n                outputs = model.generate(\n                    input_ids=context_ids.cuda(),\n                    attention_mask=context_mask.cuda(),\n                    max_length=150,\n                    lenpen=opt.lenpen,\n                    num_beams=opt.num_beams,\n                    temperature=opt.temperature,\n                    top_p=opt.top_p,\n                    num_return_sequences=opt.num_return_sequences,\n                )\n                if opt.num_return_sequences == 1:\n                    for k, o in enumerate(outputs):\n                        ans = tokenizer.decode(o, skip_special_tokens=False)\n                        gold = dataset.get_example(idx[k])['target']\n                        ans = ans.replace(\"{{\", \" {{\").replace(\"\\n\", ' ').replace(\"\\r\", \"\").replace(\"<pad>\", \"\").replace(\"<s>\", \"\").replace(\"</s>\", \"\").strip()\n                        ans = \" \".join(ans.split())\n                        gold = gold.replace(\"\\n\", ' ')\n                        fg.write(f\"{gold}\\n\")\n                        fp.write(f\"{ans}\\n\")\n                        cur_result = {'question_id': dataset.get_example(idx[k])['id'], 'gold': gold, 'clean_code': ans}\n                        result_d.append(cur_result)\n                        total += 1\n                        fr.write(json.dumps(cur_result) + \"\\n\")\n                else:\n                    outputs = outputs.view(-1, opt.num_return_sequences, outputs.size(-1))\n                    for k, o in enumerate(outputs):\n                        ans_list = []\n                        gold = dataset.get_example(idx[k])['target']\n                        gold = gold.replace(\"\\n\", ' ')\n                        for j, oj in enumerate(o):\n                            ans = tokenizer.decode(oj, skip_special_tokens=False)\n                            ans = ans.replace(\"{{\", \" {{\").replace(\"\\n\", ' ').replace(\"\\r\", \"\").replace(\"<pad>\", \"\").replace(\"<s>\", \"\").replace(\"</s>\", \"\").strip()\n                            ans = \" \".join(ans.split())\n                            ans_list.append(ans)\n                        cur_result = {'question_id': dataset.get_example(idx[k])['id'], 'gold': gold, 'clean_code': ans_list}\n                        result_d.append(cur_result)\n                        total += 1\n                        fr.write(json.dumps(cur_result) + \"\\n\")\n\n\n    if opt.num_return_sequences == 1:\n        if opt.eval_metric == 'bleu':\n            score = conala_bleu(\n                f\"{opt.checkpoint_path}/gold.gold\",\n                f\"{opt.checkpoint_path}/pred.pred\",\n                smooth=False, code_tokenize=True)\n            score = {'bleu': score}\n\n        elif opt.eval_metric == 'token_f1':\n            score = tldr_metrics(\n                f\"{opt.checkpoint_path}/gold.gold\",\n                f\"{opt.checkpoint_path}/pred.pred\",\n            )\n        else:\n            raise NotImplementedError\n    else:\n        score = 0\n\n    return score, total\n\nif __name__ == \"__main__\":\n    options = Options()\n    options.add_reader_options()\n    options.add_eval_options()\n    opt = options.parse()\n    src.slurm.init_distributed_mode(opt)\n    src.slurm.init_signal_handler()\n    opt.train_batch_size = opt.per_gpu_batch_size * max(1, opt.world_size)\n    opt.checkpoint_path = Path(opt.checkpoint_dir) / opt.name\n    opt.result_file = Path(opt.checkpoint_dir) / opt.name / f'test_results_{opt.result_tag}.json'\n\n    dir_path = Path(opt.checkpoint_dir) / opt.name\n    directory_exists = dir_path.exists()\n\n    if opt.is_distributed:\n        torch.distributed.barrier()\n    dir_path.mkdir(parents=True, exist_ok=True)\n\n    logger = src.util.init_logger(opt.is_main, opt.is_distributed, Path(opt.checkpoint_dir) / opt.name / 'run.log')\n\n    if not directory_exists and opt.is_main:\n        options.print_options(opt)\n\n    if 'codet5' in opt.tokenizer_name:\n        logger.info(f'load the tokenizer from codet5')\n        tokenizer = transformers.RobertaTokenizer.from_pretrained(opt.tokenizer_name)\n    else:\n        logger.info(f'load the tokenizer from t5')\n        tokenizer = transformers.T5Tokenizer.from_pretrained(opt.tokenizer_name)\n\n    if opt.dataset == 'tldr':\n        special_tokens_dict = {'additional_special_tokens': ['{{', '}}']}\n        num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n\n    collator_function = src.data.Collator(opt.text_maxlength, tokenizer)\n    eval_examples = src.data.load_data(\n        opt.eval_data,\n        global_rank=opt.global_rank,\n        # use the global rank and world size attibutes to split the eval set on multiple gpus\n        world_size=opt.world_size\n    )\n    eval_dataset = src.data.Dataset(\n        eval_examples,\n        opt.n_context,\n    )\n\n    eval_sampler = SequentialSampler(eval_dataset)\n    eval_dataloader = DataLoader(\n        eval_dataset,\n        sampler=eval_sampler,\n        batch_size=opt.per_gpu_batch_size,\n        num_workers=20,\n        collate_fn=collator_function\n    )\n\n    model_class = src.model.FiDT5\n    model = model_class.from_pretrained(opt.model_path)\n    model = model.to(opt.device)\n\n    logger.info(\"Start eval\")\n    score, total = evaluate(model, eval_dataset, eval_dataloader, tokenizer, opt)\n\n    logger.info(f'Total number of example {total}')\n    logger.info(json.dumps(score, indent=2))\n\n"}
{"type": "source_file", "path": "generator/fid/src/evaluation.py", "content": "#!/usr/bin/env python3\n# Copyright (c) Facebook, Inc. and its affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport collections\nimport logging\nimport regex\nimport string\nimport unicodedata\nfrom functools import partial\nfrom multiprocessing import Pool as ProcessPool\nfrom typing import Tuple, List, Dict\nimport numpy as np\n\n\"\"\"\nEvaluation code from DPR: https://github.com/facebookresearch/DPR\n\"\"\"\n\nclass SimpleTokenizer(object):\n    ALPHA_NUM = r'[\\p{L}\\p{N}\\p{M}]+'\n    NON_WS = r'[^\\p{Z}\\p{C}]'\n\n    def __init__(self):\n        \"\"\"\n        Args:\n            annotators: None or empty set (only tokenizes).\n        \"\"\"\n        self._regexp = regex.compile(\n            '(%s)|(%s)' % (self.ALPHA_NUM, self.NON_WS),\n            flags=regex.IGNORECASE + regex.UNICODE + regex.MULTILINE\n        )\n\n    def tokenize(self, text, uncased=False):\n        matches = [m for m in self._regexp.finditer(text)]\n        if uncased:\n            tokens = [m.group().lower() for m in matches]\n        else:\n            tokens = [m.group() for m in matches]\n        return tokens\n\nlogger = logging.getLogger(__name__)\n\nQAMatchStats = collections.namedtuple('QAMatchStats', ['top_k_hits', 'questions_doc_hits'])\n\ndef calculate_matches(data: List, workers_num: int):\n    \"\"\"\n    Evaluates answers presence in the set of documents. This function is supposed to be used with a large collection of\n    documents and results. It internally forks multiple sub-processes for evaluation and then merges results\n    :param all_docs: dictionary of the entire documents database. doc_id -> (doc_text, title)\n    :param answers: list of answers's list. One list per question\n    :param closest_docs: document ids of the top results along with their scores\n    :param workers_num: amount of parallel threads to process data\n    :param match_type: type of answer matching. Refer to has_answer code for available options\n    :return: matching information tuple.\n    top_k_hits - a list where the index is the amount of top documents retrieved and the value is the total amount of\n    valid matches across an entire dataset.\n    questions_doc_hits - more detailed info with answer matches for every question and every retrieved document\n    \"\"\"\n\n    logger.info('Matching answers in top docs...')\n\n    tokenizer = SimpleTokenizer()\n    get_score_partial = partial(check_answer, tokenizer=tokenizer)\n\n    processes = ProcessPool(processes=workers_num)\n    scores = processes.map(get_score_partial, data)\n\n    logger.info('Per question validation results len=%d', len(scores))\n\n    n_docs = len(data[0]['ctxs'])\n    top_k_hits = [0] * n_docs\n    for question_hits in scores:\n        best_hit = next((i for i, x in enumerate(question_hits) if x), None)\n        if best_hit is not None:\n            top_k_hits[best_hit:] = [v + 1 for v in top_k_hits[best_hit:]]\n\n    return QAMatchStats(top_k_hits, scores)\n\ndef check_answer(example, tokenizer) -> List[bool]:\n    \"\"\"Search through all the top docs to see if they have any of the answers.\"\"\"\n    answers = example['answers']\n    ctxs = example['ctxs']\n\n    hits = []\n\n    for i, doc in enumerate(ctxs):\n        text = doc['text']\n\n        if text is None:  # cannot find the document for some reason\n            logger.warning(\"no doc in db\")\n            hits.append(False)\n            continue\n\n        hits.append(has_answer(answers, text, tokenizer))\n\n    return hits\n\ndef has_answer(answers, text, tokenizer) -> bool:\n    \"\"\"Check if a document contains an answer string.\"\"\"\n    text = _normalize(text)\n    text = tokenizer.tokenize(text, uncased=True)\n\n    for answer in answers:\n        answer = _normalize(answer)\n        answer = tokenizer.tokenize(answer, uncased=True)\n        for i in range(0, len(text) - len(answer) + 1):\n            if answer == text[i: i + len(answer)]:\n                return True\n    return False\n\n#################################################\n########        READER EVALUATION        ########\n#################################################\n\ndef _normalize(text):\n    return unicodedata.normalize('NFD', text)\n\n#Normalization from SQuAD evaluation script https://worksheets.codalab.org/rest/bundles/0x6b567e1cf2e041ec80d7098f031c5c9e/contents/blob/\ndef normalize_answer(s):\n    def remove_articles(text):\n        return regex.sub(r'\\b(a|an|the)\\b', ' ', text)\n\n    def white_space_fix(text):\n        return ' '.join(text.split())\n\n    def remove_punc(text):\n        exclude = set(string.punctuation)\n        return ''.join(ch for ch in text if ch not in exclude)\n\n    def lower(text):\n        return text.lower()\n\n    return white_space_fix(remove_articles(remove_punc(lower(s))))\n\ndef exact_match_score(prediction, ground_truth):\n    return normalize_answer(prediction) == normalize_answer(ground_truth)\n\ndef ems(prediction, ground_truths):\n    return max([exact_match_score(prediction, gt) for gt in ground_truths])\n\n####################################################\n########        RETRIEVER EVALUATION        ########\n####################################################\n\ndef eval_batch(scores, inversions, avg_topk, idx_topk):\n    for k, s in enumerate(scores):\n        s = s.cpu().numpy()\n        sorted_idx = np.argsort(-s)\n        score(sorted_idx, inversions, avg_topk, idx_topk)\n\ndef count_inversions(arr):\n    inv_count = 0\n    lenarr = len(arr)\n    for i in range(lenarr):\n        for j in range(i + 1, lenarr):\n            if (arr[i] > arr[j]):\n                inv_count += 1\n    return inv_count\n\ndef score(x, inversions, avg_topk, idx_topk):\n    x = np.array(x)\n    inversions.append(count_inversions(x))\n    for k in avg_topk:\n        # ratio of passages in the predicted top-k that are\n        # also in the topk given by gold score\n        avg_pred_topk = (x[:k]<k).mean()\n        avg_topk[k].append(avg_pred_topk)\n    for k in idx_topk:\n        below_k = (x<k)\n        # number of passages required to obtain all passages from gold top-k\n        idx_gold_topk = len(x) - np.argmax(below_k[::-1])\n        idx_topk[k].append(idx_gold_topk)\n"}
{"type": "source_file", "path": "generator/fid/setup.py", "content": "# Copyright (c) Facebook, Inc. and its affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\n\nimport setuptools\n\nwith open(\"README.md\", \"r\") as fh:\n    long_description = fh.read()\n    \nwith open(\"requirements.txt\", \"r\") as f:\n    install_requires = list(f.read().splitlines())\n \n\nsetuptools.setup(\n    name=\"FiD\",\n    version=\"0.1.0\",\n    description=\"Fusion-in-Decoder\",\n    long_description=long_description,\n    long_description_content_type=\"text/markdown\",\n    packages=setuptools.find_packages(),\n    classifiers=[\n        \"Programming Language :: Python :: 3\",\n        \"License :: OSI Approved :: MIT License\",\n        \"Operating System :: OS Independent\",\n    ],\n    python_requires=\">=3.7\",\n    install_requires=install_requires\n)\n"}
{"type": "source_file", "path": "dataset_helper/conala/gen_metric.py", "content": "# Copyright 2017 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n\"\"\"Python implementation of BLEU and smooth-BLEU.\n\nThis module provides a Python implementation of BLEU and smooth-BLEU.\nSmooth BLEU is computed following the method outlined in the paper:\nChin-Yew Lin, Franz Josef Och. ORANGE: a method for evaluating automatic\nevaluation metrics for machine translation. COLING 2004.\n\"\"\"\n\nimport collections\nimport math\nimport json\nimport os.path\nimport re\n\n\ndef _get_ngrams(segment, max_order):\n    \"\"\"Extracts all n-grams upto a given maximum order from an input segment.\n\n  Args:\n    segment: text segment from which n-grams will be extracted.\n    max_order: maximum length in tokens of the n-grams returned by this\n        methods.\n\n  Returns:\n    The Counter containing all n-grams upto max_order in segment\n    with a count of how many times each n-gram occurred.\n  \"\"\"\n    ngram_counts = collections.Counter()\n    for order in range(1, max_order + 1):\n        for i in range(0, len(segment) - order + 1):\n            ngram = tuple(segment[i:i + order])\n            ngram_counts[ngram] += 1\n    return ngram_counts\n\n\ndef compute_bleu(reference_corpus, translation_corpus, max_order=4,\n                 smooth=False):\n    \"\"\"Computes BLEU score of translated segments against one or more references.\n\n  Args:\n    reference_corpus: list of lists of references for each translation. Each\n        reference should be tokenized into a list of tokens.\n    translation_corpus: list of translations to score. Each translation\n        should be tokenized into a list of tokens.\n    max_order: Maximum n-gram order to use when computing BLEU score.\n    smooth: Whether or not to apply Lin et al. 2004 smoothing.\n\n  Returns:\n    3-Tuple with the BLEU score, n-gram precisions, geometric mean of n-gram\n    precisions and brevity penalty.\n  \"\"\"\n    matches_by_order = [0] * max_order\n    possible_matches_by_order = [0] * max_order\n    reference_length = 0\n    translation_length = 0\n    for (references, translation) in zip(reference_corpus,\n                                         translation_corpus):\n        reference_length += min(len(r) for r in references)\n        translation_length += len(translation)\n\n        merged_ref_ngram_counts = collections.Counter()\n        for reference in references:\n            merged_ref_ngram_counts |= _get_ngrams(reference, max_order)\n        translation_ngram_counts = _get_ngrams(translation, max_order)\n        overlap = translation_ngram_counts & merged_ref_ngram_counts\n        for ngram in overlap:\n            matches_by_order[len(ngram) - 1] += overlap[ngram]\n        for order in range(1, max_order + 1):\n            possible_matches = len(translation) - order + 1\n            if possible_matches > 0:\n                possible_matches_by_order[order - 1] += possible_matches\n\n    precisions = [0] * max_order\n    for i in range(0, max_order):\n        if smooth:\n            precisions[i] = ((matches_by_order[i] + 1.) /\n                             (possible_matches_by_order[i] + 1.))\n        else:\n            if possible_matches_by_order[i] > 0:\n                precisions[i] = (float(matches_by_order[i]) /\n                                 possible_matches_by_order[i])\n                # print(i, f\"{precisions[i]:.03f}={float(matches_by_order[i]):.03f}/{possible_matches_by_order[i]}\")\n            else:\n                precisions[i] = 0.0\n    # print(\"========\")\n    if min(precisions) > 0:\n        p_log_sum = sum((1. / max_order) * math.log(p) for p in precisions)\n        geo_mean = math.exp(p_log_sum)\n    else:\n        geo_mean = 0\n\n    ratio = float(translation_length) / reference_length\n\n    if ratio > 1.0:\n        bp = 1.\n    else:\n        bp = math.exp(1 - 1. / ratio)\n\n    bleu = geo_mean * bp\n\n    # print(bleu, precisions, bp, ratio, translation_length, reference_length)\n    return (bleu, precisions, bp, ratio, translation_length, reference_length)\n\n\n\"\"\" The tokenizer that we use for code submissions, from Wang Ling et al., Latent Predictor Networks for Code Generation (2016)\n    @param code: string containing a code snippet\n    @return: list of code tokens\n\"\"\"\n\n\ndef tokenize_for_bleu_eval(code):\n    code = re.sub(r'([^A-Za-z0-9_])', r' \\1 ', code)\n    code = re.sub(r'([a-z])([A-Z])', r'\\1 \\2', code)\n    code = re.sub(r'\\s+', ' ', code)\n    code = code.replace('\"', '`')\n    code = code.replace('\\'', '`')\n    tokens = [t for t in code.split(' ') if t]\n    return tokens\n\n\ndef _bleu(ref_file, trans_file, subword_option=None, smooth=True, code_tokenize=False):\n    assert code_tokenize\n    assert not smooth\n    max_order = 4\n    ref_files = [ref_file]\n    reference_text = []\n    for reference_filename in ref_files:\n        with open(reference_filename) as fh:\n            reference_text.append(fh.readlines())\n    per_segment_references = []\n    for references in zip(*reference_text):\n        reference_list = []\n        for reference in references:\n            if code_tokenize:\n                reference_list.append(tokenize_for_bleu_eval(reference.strip()))\n            else:\n                reference_list.append(reference.strip().split())\n        per_segment_references.append(reference_list)\n    translations = []\n    with open(trans_file) as fh:\n        for line in fh:\n            if code_tokenize:\n                translations.append(tokenize_for_bleu_eval(line.strip()))\n            else:\n                translations.append(line.strip().split())\n    print(f'src length: {len(per_segment_references)}, tgt length: {len(translations)}')\n    bleu_score, _, _, _, _, _ = compute_bleu(per_segment_references, translations, max_order, smooth)\n    return round(100 * bleu_score, 2)\n\n\ndef enum_pred_file(split, re_tag, mode):\n    if mode == 0:\n        for model_name in ['model_13b_TY_MS0N1_DT',  'model_13b_TY_MS0N3_DT', 'model_13b_TY_MS0N5_DT',\n                           'model_13b_TY_MS1N1_DT',  'model_13b_TY_MS1N3_DT', 'model_13b_TY_MS1N5_DT',\n                           'model_13b_TY_MO_DT', 'model_13b_TY_MN_DT', 'model_13b_TY_MN_DCT',\n                           'model_13b_TY_MO_DCT',\n                           'model_13b_TY_MS1N3_DCT',\n                           'model_13b_TY_MS1N5_DCT',\n                           'model_13b_TY_MS1N3_DCT/CUT160',\n                           'model_13b_TY_MS1T10_S160_DCT',\n                           'model_13b_TY_MS1T10_S0_DCT',\n                            'model_13b_TY_MS1T10_S0_RR_DCT',\n                            'model_13b_TY_MN_DCT',\n                            'model_13b_TY_MS1T10_DCT',\n                            'model_13b_TY_MN_DCT_v2'\n                           ][:]:\n\n            pred_file = f\"./data/conala/models/{model_name}/decode.epochbest.t5.b5.l150.cn0.trie0.{split}{re_tag}.json\"\n            if os.path.exists(pred_file):\n                yield pred_file\n\n            # for i in list(range(0, 20)):\n            #     pred_file = f\"./data/conala/models/{model_name}/decode.epoch{i:02d}.t5.b5.l150.cn0.trie0.{split}{re_tag}.json\"\n            #     if os.path.exists(pred_file):\n            #         yield pred_file\n\n    elif mode == 1:\n        pred_d = f\"./data/conala/models/model_13b_TY_MS1_DT/decode.epoch06.t5.b5.l150.cn0.trie0.random_test.oracle_man.json\"\n        yield pred_d\n\n    elif mode == 2:\n        for i in range(10):\n            pred_file = f\"./data/conala/models/model_13b_TY_MN_DM/decode.epoch{i:02d}.t5.b5.l150.cn0.trie0.{split}.json\"\n            if os.path.exists(pred_file):\n                yield pred_file\n        for i in range(10):\n            pred_file = f\"./data/conala/models/model_13b_TY_MN_DM/decode.epoch{i:02d}.t5.b5.l150.cn0.trie0.{split}.json.ft\"\n            if os.path.exists(pred_file):\n                yield pred_file\n\ndef clean_code(code):\n    return code.replace(\"<|endoftext|>\", \"\").strip()\n\n"}
{"type": "source_file", "path": "dataset_helper/conala/execution_eval.py", "content": "\"\"\"\nexecution-based evaluation\n\"\"\"\nimport argparse\nimport json\nimport sys\nimport evaluate\nfrom datasets import load_metric\nimport numpy as np\nfrom collections import defaultdict\nimport os\n\nfrom py import test \nos.environ[\"HF_ALLOW_CODE_EVAL\"] = \"1\"\n\ndef pass_at_k(result_file, unittest_file):\n    with open(unittest_file, 'r') as f:\n        unittests = json.load(f)\n    \n    # select the examples which have unit test\n    selected_predictions = []\n    with open(result_file, 'r') as f:\n        for line in f:\n            pred = json.loads(line)\n            if pred[\"question_id\"] in unittests:\n                selected_predictions.append(pred)\n    print(f\"selected {len(selected_predictions)} examples with unit test\")\n                \n    # run the test   \n    # load the metric from huggingface \n    code_eval_metric = evaluate.load(\"code_eval\")\n    preds = []\n    tests = []\n    for prediction in selected_predictions:\n        suffix = \"\"\n        question_id = prediction[\"question_id\"]\n        unittest = unittests[question_id]\n        entry_point = unittest[\"entry_point\"]\n        test_func = f\"\\n{unittest['test']}\\ncheck({entry_point})\"\n        \n        # wrap the generated code to a runnable function\n        if isinstance(prediction['clean_code'], list):\n            runnable_func = [f\"{unittest['prompt']}{x}{suffix}\" for x in prediction['clean_code']]\n        else:\n            runnable_func = [f\"{unittest['prompt']}{prediction['clean_code']}{suffix}\"]\n\n        preds.append(runnable_func)\n        tests.append(test_func)\n        \n    r = code_eval_metric.compute(\n        predictions=preds,\n        references=tests,\n        k=[1, 5, 10, 50, 100, 150, 200],\n        num_workers=8,\n    )\n    print(r[0])\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--result_file\", type=str, default=\"\")\n    args = parser.parse_args()\n    result_file = args.result_file\n    unittest_file = \"data/conala/unittest_docprompting_conala.json\"\n    assert result_file\n    pass_at_k(result_file, unittest_file)\n"}
{"type": "source_file", "path": "dataset_helper/tldr/gen_metric.py", "content": "import requests\nimport re\nimport json\nfrom collections import defaultdict, Counter\nfrom utils.util import dedup_results, get_bag_of_keywords, clean_anonymize_command, get_bag_of_words\nfrom pathlib import Path\n# from external.nl2bash.eval import eval_tools\n# from external.nl2bash.encoder_decoder import data_utils\n# import bashlex\nfrom glob import glob\nimport os\nfrom sacrebleu.metrics import BLEU, CHRF, TER\nimport sacrebleu\nimport numpy as np\nimport argparse\nimport editdistance\nDEBUG=True\nMETRIC_LIST = ['p', 'r', 'f1', 'cmd_acc', 'no_cmd_p', 'no_cmd_r', 'no_cmd_f1', 'template_matching', 'sentence_bleu']\nDEBUG_INFO = []\n\ndef token_prf(tok_gold, tok_pred, match_blank=False):\n    if match_blank and len(tok_gold) == 0: # do not generate anything\n        if len(tok_pred) == 0:\n            m = {'r': 1, 'p': 1, 'f1': 1}\n        else:\n            m = {'r': 0, 'p': 0, 'f1': 0}\n    else:\n        tok_gold_dict = Counter(tok_gold)\n        tok_pred_dict = Counter(tok_pred)\n        tokens = set([*tok_gold_dict] + [*tok_pred_dict])\n        hit = 0\n        for token in tokens:\n            hit += min(tok_gold_dict.get(token, 0), tok_pred_dict.get(token, 0))\n        p = hit / (sum(tok_pred_dict.values()) + 1e-10)\n        r = hit / (sum(tok_gold_dict.values()) + 1e-10)\n        f1 = 2 * p * r / (p + r + 1e-10)\n        m = {'r': r, 'p': p, 'f1': f1}\n    return m\n\ndef measure_bag_of_word(gold, pred):\n    # tok_gold = get_bag_of_keywords(gold)\n    # tok_pred = get_bag_of_keywords(pred)\n    tok_gold = get_bag_of_words(gold)\n    tok_pred = get_bag_of_words(pred)\n    m = token_prf(tok_gold, tok_pred) # whole sentence\n    gold_cmd = tok_gold[0] if len(tok_gold) else \"NONE_GOLD\"\n    pred_cmd = tok_pred[0] if len(tok_pred) else \"NONE_PRED\"\n    m = {**m, 'cmd_acc': int(gold_cmd == pred_cmd)}\n\n    # without cmd\n    no_cmd_m = token_prf(tok_gold[1:], tok_pred[1:], match_blank=True)\n    no_cmd_m = {f\"no_cmd_{k}\": v for k, v in no_cmd_m.items()}\n    m = {**m, **no_cmd_m}\n    return m\n\n\ndef calc_bleu(gold, pred):\n    ag = clean_anonymize_command(gold)\n    ap = clean_anonymize_command(pred)\n    score = sacrebleu.sentence_bleu(ap, [ag], tokenize='none').score\n    # print(ap, \"|\", ag)\n    # assert score == _score, (score, _score)\n    return  {'sentence_bleu': score}\n\ndef calc_template_matching(gold, pred):\n    ag = clean_anonymize_command(gold)\n    ap = clean_anonymize_command(pred)\n    m = {'template_matching': int(ag == ap)}\n    ag = ' '.join(ag.split()[1:])\n    ap = ' '.join(ap.split()[1:])\n    m['no_cmd_template_matching'] = int(ag == ap)\n    return m\n\ndef calc_edit_distance(gold, pred):\n    ag = clean_anonymize_command(gold)\n    ap = clean_anonymize_command(pred)\n    ag_toks = ag.split()\n    ap_toks = ap.split()\n    m = {'edit_distance': editdistance.eval(ag_toks, ap_toks)}\n    return m\n\n\ndef evaluate_from_json_simple(json_file, reference_file, top_k=10, score_key='per_word_ll', code_entry='clean_code'):\n    json_file = Path(json_file) if not isinstance(json_file, Path) else json_file\n\n    with open(json_file, \"r\") as f:\n        all_pred = json.load(f)\n        all_pred = dedup_results(all_pred)\n\n    with open(reference_file, \"r\") as f:\n        gold = json.load(f)\n\n\n    DEBUG_INFO.append(f\"number of predictions/gold: {len(all_pred)}/{len(gold)}\")\n\n    interval = [1, 3, 5, 10, 20, 30]\n    interval = [x for x in interval if x <= top_k]\n    top_k_metric = {x: defaultdict(float) for x in interval}\n    tot = len(gold)\n    missing = []\n    _bleu = []\n    for sample in gold:\n        nl = sample['nl']\n        gold_cmd = sample['cmd']\n        if nl not in all_pred:\n            missing.append(nl)\n            continue\n        pred = all_pred[nl]\n        # sort\n        pred_score = defaultdict(lambda: -1e10)\n        for pc, score in zip(pred[code_entry], pred[score_key]):\n            pred_score[pc] = max(pred_score[pc], score)\n        pred_score = sorted(pred_score.items(), key=lambda x: x[1], reverse=True)\n        pred_top_k = pred_score[:top_k]\n\n        bow = [measure_bag_of_word(gold_cmd, x[0]) for x in pred_top_k]\n        tm = [calc_template_matching(gold_cmd, x[0]) for x in pred_top_k]\n        bleu = [calc_bleu(gold_cmd, x[0]) for x in pred_top_k]\n        metric = [{**bow[i], **tm[i], **bleu[i]} for i in range(len(bow))]\n\n        for tk in interval:\n            _metric = metric[:tk]\n            for k in METRIC_LIST:\n                v = max([x[k] for x in _metric])\n                top_k_metric[tk][k] += v / tot\n\n    # print(\"\\t\".join([f\"{x: >10}\" for x in [''] + metric_list]))\n    for k, v in top_k_metric.items():\n        print(\"\\t\".join(['  ' for _ in range(4)]), end='\\t')\n        # print('\\t'.join([f\"{k: >10}\"] + [f\"{v[m]:10.3f}\" for m in metric_list]))\n        print('\\t'.join([f\"{v[m]:10.3f}\" for m in METRIC_LIST]))\n\n    m_file = json_file.with_suffix(\".metric\")\n    with open(m_file, \"w+\") as f:\n        json.dump(top_k_metric, f, indent=2)\n\n    DEBUG_INFO.append(f\"missing {len(missing)} samples, evaluate {len(gold) - len(missing)} samples\")\n    return top_k_metric\n\n# class DummyFlag(object):\n#     def __init__(self):\n#         self.explain = False\n#         self.min_vocab_frequency = 1\n#         self.channel = 'token'\n#         self.normalized = False\n#         self.fill_argument_slots = False\n#         self.use_copy = False\n#\n# def evaluate_from_json(json_file, data_dir, sc_path, tg_path, overwrite=True, top_k=10):\n#     score_key = 'per_word_ll' # use which score for ranking results\n#\n#     json_file = Path(json_file) if not isinstance(json_file, Path) else json_file\n#     with open(json_file, \"r\") as f:\n#         d = json.load(f)\n#     prediction_path = Path(json_file.with_suffix(\".prediction.txt\"))\n#     FLAGS = DummyFlag()\n#     FLAGS.data_dir = data_dir\n#\n#     # source, target = ('nl', 'cm') if not FLAGS.explain else ('cm', 'nl')\n#     dataset = data_utils.read_data(FLAGS, sc_path, tg_path, load_features=False,\n#                          use_buckets=False, buckets=None,\n#                          add_start_token=True, add_end_token=True)\n#\n#     # group annotation by nl, multiple references\n#     attribute = 'source'\n#     grouped_dataset = data_utils.group_parallel_data(dataset, attribute=attribute, group=False)\n#     temp_list = [x[0] for x in grouped_dataset]\n#     predictions = [[] for _ in grouped_dataset]\n#     if overwrite or not prediction_path.exists():\n#         for x in d:\n#             nl = x['nl']\n#             gold = x['gold']\n#             cmd_name = x['cmd_name']\n#             # if group:\n#             #     raise NotImplementedError\n#             #     # words, _ = tokenizer.basic_tokenizer(nl)\n#             #     # temp = ' '.join(words)\n#             # else:\n#             temp = f\"{cmd_name} ||| {nl}\"\n#             temp_idx = temp_list.index(temp)\n#\n#             if len(predictions[temp_idx]) != 0 and attribute != 'source':\n#                 raise NotImplementedError('only allow merging nl')\n#\n#             if isinstance(x['code'], list):\n#                 if len(x['code'][0]) >= 10 * len(gold):\n#                     x['code'] = ['PLACEHOLDER']\n#                     x[score_key] = [0]\n#                     print(f\"Skip {x['nl']} due to length issue\")\n#                 assert len(x['code']) == len(x[score_key])\n#                 predictions[temp_idx] += list(zip(x['code'], x[score_key]))\n#             else:\n#                 raise ValueError\n#                 # predictions[temp_idx].append(x['prediction'])\n#\n#         # write the prediction in the same order as grouped_dataset\n#         with open(prediction_path, \"w+\") as f:\n#             for p in predictions:\n#                 p = sorted(p, key=lambda x: x[1], reverse=True)\n#                 p = [x[0] for x in p[:top_k]]\n#                 # assert p\n#                 f.write(f\"{'|||'.join(p)}\\n\")\n#\n#\n#     metrics = eval_tools.automatic_eval(prediction_path, grouped_dataset, top_k=10, FLAGS=FLAGS, verbose=False)\n#     m_file = json_file.with_suffix(\".metric\")\n#     with open(m_file, \"w+\") as f:\n#         json.dump(metrics, f, indent=2)\n\ndef tldr_metrics(src_file, pred_file):\n    src_list = []\n    pred_list = []\n    with open(src_file, 'r') as f:\n        for line in f:\n            src_list.append(line.strip())\n    with open(pred_file, 'r') as f:\n        for line in f:\n            pred_list.append(line.strip())\n    assert len(src_list) == len(pred_list)\n\n    metric_list = defaultdict(list)\n    for src, pred in zip(src_list, pred_list):\n        for k, v in calc_template_matching(src, pred).items():\n            metric_list[k].append(v)\n        for k, v in measure_bag_of_word(src, pred).items():\n            metric_list[k].append(v)\n        for k, v in calc_edit_distance(src, pred).items():\n            metric_list[k].append(v)\n\n    for k, v in metric_list.items():\n        metric_list[k] = np.mean(v)\n\n\n    def clean_for_bleu(s):\n        s = s.replace(\"sudo\", \"\").strip()\n        s = s.replace(\"`\", \"\").replace('\"', \"\").replace(\"'\", \"\")\n        #  '>', '|', '+'\n        s = s.replace(\"|\", \" \").replace(\">\", \" \").replace(\"<\", \" \")\n        s = \" \".join(s.split())\n        s = s.replace(\"={\", \" {\")\n        var_to_pc_holder = defaultdict(lambda: len(var_to_pc_holder))\n        for var in re.findall(\"{{(.*?)}}\", s):\n            _ = var_to_pc_holder[var]\n        for var, id in var_to_pc_holder.items():\n            var_str = \"{{%s}}\" % var\n            s = s.replace(var_str, f\"${id}\")\n        # s = re.sub(\"{{.*?}}\", VAR_STR, s)\n        # print(s)\n        return s\n\n    # calc bleu\n    bleu = BLEU(tokenize='none', smooth_method='add-k', smooth_value=1)\n    pred_list = [clean_for_bleu(x) for x in pred_list]\n    src_list = [clean_for_bleu(x) for x in src_list]\n    bleu_score = bleu.corpus_score(pred_list, [src_list]).score\n    metric_list['bleu'] = bleu_score\n\n\n    def to_characters(s):\n        # s = s.replace(\" \", \"\")\n        # s = ' '.join(list(s))\n        return s\n    # character level\n    bleu = BLEU(tokenize='char')\n    pred_list = [to_characters(x) for x in pred_list]\n    src_list = [to_characters(x) for x in src_list]\n    bleu_score = bleu.corpus_score(pred_list, [src_list]).score\n    metric_list['bleu_char'] = bleu_score\n    return metric_list\n\ndef add_cmd_name(result_file):\n    save_file = result_file.replace(\".cn1.\", '.cn1.processed.')\n    if not os.path.exists(save_file):\n        with open(result_file, \"r\") as f:\n            d = json.load(f)\n            for item in d:\n                cmd_name = item['cmd_name']\n                item['clean_code'] = [f\"{cmd_name} {x}\" for x in item['clean_code']]\n        with open(save_file, \"w+\") as f:\n            json.dump(d, f, indent=2)\n    return save_file\n\ndef get_config(result_file):\n    if \"1.3\" in result_file or \"13\" in result_file:\n        model = \"neo-1.3B\"\n    elif \"125\" in result_file:\n        model = \"neo-125M\"\n    elif \"gpt-j\" in result_file:\n        model = \"gpt-j-6B\"\n    elif \"codex\" in result_file:\n        model = \"codeX\"\n    else:\n        raise ValueError(f\"model not found in {result_file}\")\n\n    if any([x in result_file for x in ['raw_models', 'codex']]):\n        training = \"P\"\n    else:\n        training = \"Y\"\n\n    if any([x in result_file for x in ['_MN', 'codex_no_examples_man', '.nl_cmd.']]):\n        manual = \"N\"\n    elif any([x in result_file for x in ['_MO', 'codex_examples_man', '.man_nl_cmd.']]):\n        manual = \"O\" #at most 2\n    elif \"_MS\" in result_file:\n        manual = \"O+S\"\n    else:\n        raise ValueError(f\"manual config not found in {result_file}\")\n\n    if any([x in result_file for x in ['cn0', '_no_cmdname.']]):\n        cmd_name = 'N'\n    elif any([x in result_file for x in ['cn1', '_with_cmdname.']]):\n        cmd_name = 'Y'\n    else:\n        raise ValueError(f\"command name config not found in {result_file}\")\n\n    return model, training, manual, cmd_name\n\ndef evaluate_lists_of_prediction():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--soft_match', default=None)\n    parser.add_argument('--score_key', default=None)\n    args = parser.parse_args()\n\n    base_path = Path(\"./data/tldr\")\n    data_dir = base_path / \"nl.cm\"\n\n    # f_list = sorted(glob(args.soft_match))\n    # score_key = args.score_key\n    # score_key = 'per_word_ll'\n    score_key = 'fake_per_word_ll'\n    f_list = sorted(\n        glob(\"./data/tldr/models/model_13b_TY_MS_v3/*17*.cmd_dev*trimmed_manual_para_cat_descpt*merge*json\"))\n    # f_list = sorted(glob(\"./data/tldr/models/model_13b_TY_MN/*cmd_test*.json\"))\n    # f_list = sorted(glob(\"./data/tldr/models/rerank/*cmd_dev*.final.json\"))\n    # f_list += sorted(glob(\"./data/tldr/models/raw_models/*.json\"))\n    # f_list = sorted(glob(\"./data/tldr/models/codex/decode.codex.*.json\"))\n    print(f_list)\n    f_list = [x for x in f_list if '.processed.' not in x]\n    print(\"\\t\".join([' ' for _ in range(4)]))\n    print(\"\\t\".join([f\"{x: >10}\" for x in\n                     ['Model', 'Training (N,Y,P)', 'Man (N, Y, OnlyOracle (O), Oracle+Random(O+S))',\n                      'Cmd'] + METRIC_LIST]))\n    summary_results = [[], []]\n    for result_file in f_list:\n        DEBUG_INFO.append(result_file)\n        config = get_config(result_file)\n        config = '\\t'.join(config)\n        print(config, \"\\t\", result_file)\n        l = \"random\" if \"random\" in result_file else \"cmd\"\n        s = \"test\" if \"test\" in result_file else 'dev'\n        reference_file = data_dir / f\"{l}_{s}.seed.json\"\n        if '.cn1.' in result_file and '.processed.' not in result_file:\n            # add command name before eval\n            result_file = add_cmd_name(result_file)\n        m = evaluate_from_json_simple(result_file, reference_file, top_k=30, score_key=score_key,\n                                      code_entry='clean_code')\n\n        summary_results[0].append(\n            [config, m[1]['template_matching'], m[1]['f1'], m[1]['no_cmd_f1'], m[1]['sentence_bleu']])\n        DEBUG_INFO.append(\"============================\")\n\n    print(\"*****************Summary******************\")\n    for r in summary_results:\n        for item in r:\n            print(item[0], '\\t', '\\t'.join([str(x) for x in item[1:]]))\n\n    for l in DEBUG_INFO:\n        print(l)\n\n\ndef debug_tm_bleu():\n    bleu = BLEU()\n    with open('data/fid/tldr.nothing/test_results_test_same.json', 'r') as f:\n        db = []\n        for line in f:\n            db.append(json.loads(line))\n    with open('data/fid/tldr.oracle.r2-bash_man_para.t10/test_results_test_same.json', 'r') as f:\n        do = []\n        for line in f:\n            do.append(json.loads(line))\n    tot = 0\n    for ib, io in zip(db, do):\n        assert ib['question_id'] == io['question_id']\n        gold = clean_anonymize_command(ib['gold'])\n        pb = clean_anonymize_command(ib['clean_code'])\n        po = clean_anonymize_command(io['clean_code'])\n        # if po == gold:\n        #     print(po, '|', gold, '|', bleu.corpus_score([po], [[gold]]).score)\n        print('\\t', gold)\n        print(bleu.corpus_score([pb],[[gold]]).score, '\\t', pb)\n        print(bleu.corpus_score([po], [[gold]]).score, '\\t', po)\n        print('\\n')\n    print(tot)\nif __name__ == \"__main__\":\n    # debug_tm_bleu()\n    # exit(0)\n    m_list = ['cmd_acc', 'template_matching', 'f1', 'bleu_char', 'edit_distance']\n\n    # codex\n    # with open('data/tldr/nl.cm/code-davinci-002.codex.cmd_dev.json', 'r') as f:\n    #     d = json.load(f)\n    #     with open('gold.gold', 'w+') as fg, open('pred.pred', 'w+') as fp:\n    #         for item in d:\n    #             fg.write(item['cmd'].strip().replace(\"\\n\", \"\") + '\\n')\n    #             fp.write(item['codex_response'].replace(\"\\n\", \"\") + '\\n')\n    #     m = tldr_metrics('gold.gold', 'pred.pred')\n    #     for k, v in m.items():\n    #         m[k] = f'{v:.4f}'\n    #     print('\\t'.join([str(m[x]) for x in m_list]))\n\n    # evaluate_lists_of_prediction()\n\n    # gpt\n    # tldr.neo13.train_nlcode\n    for file_name in sorted(list(glob('data/gpt/tldr.*/decode*cmd_test*.train_nlcode*json'))):\n        with open(file_name, 'r') as f:\n            d = json.load(f)\n        with open('gold.gold', 'w+') as fg, open('pred.pred', 'w+') as fp:\n            for item in d:\n                fg.write(item['gold'].strip().replace(\"\\n\", \"\") + '\\n')\n                fp.write(item['clean_code'][0].strip().replace(\"\\n\", \"\") + '\\n')\n\n        m = tldr_metrics('gold.gold', 'pred.pred')\n        print(f\"{file_name}\")\n        for k, v in m.items():\n            m[k] = f'{v:.4f}'\n        print('\\t'.join([str(m[x]) for x in m_list]))\n    # fid\n    for file_name in sorted(list(glob('data/fid/*tldr.*/test_results_test*.json'))):\n        with open(file_name, 'r') as f:\n            data = []\n            for line in f:\n                data.append(json.loads(line))\n        # ids = set()\n        # for item in data:\n        #     if item['question_id'] in ids:\n        #         print(item['question_id'])\n        #     ids.add(item['question_id'])\n        # split to top-n\n        split_data = [[] for _ in range(10)]\n        qid_counter = Counter()\n        for item in data:\n            if item['question_id'] in ['9931', '7895', '3740', '8077', '4737', '7057', '9530']:\n                continue\n            split_idx = qid_counter[item['question_id']]\n            split_data[split_idx].append(item)\n            qid_counter[item['question_id']] += 1\n        assert all([len(x) in [918, 1845, 0] for x in split_data])\n\n        split_data = split_data[:1]\n        for s_idx, cur_split in enumerate(split_data):\n            with open('gold.gold', 'w+') as fg, open('pred.pred', 'w+') as fp:\n                for item in cur_split:\n                    fg.write(item['gold'].strip().replace(\"\\n\", \"\") + '\\n')\n                    fp.write(item['clean_code'].replace(\"\\n\", \"\") + '\\n')\n            m = tldr_metrics('gold.gold', 'pred.pred')\n            print(f\"{file_name}: {s_idx}\")\n            # print(json.dumps(m, indent=2))\n            for k, v in m.items():\n                m[k] = f'{v:.4f}'\n            print('\\t'.join([str(m[x]) for x in m_list]))\n            # ff.write('\\t'.join([str(m[x]) for x in m_list]) + '\\n')\n\n"}
{"type": "source_file", "path": "generator/fid/src/model.py", "content": "# Copyright (c) Facebook, Inc. and its affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport types\nimport torch\nimport transformers\nimport torch.nn.functional as F\nfrom torch import nn\nfrom torch.nn import CrossEntropyLoss\nimport numpy as np\nimport inspect\n\nclass FiDT5(transformers.T5ForConditionalGeneration):\n    def __init__(self, config):\n        super().__init__(config)\n        self.wrap_encoder()\n\n    def forward_(self, **kwargs):\n        if 'input_ids' in kwargs:\n            kwargs['input_ids'] = kwargs['input_ids'].view(kwargs['input_ids'].size(0), -1)\n        if 'attention_mask' in kwargs:\n            kwargs['attention_mask'] = kwargs['attention_mask'].view(kwargs['attention_mask'].size(0), -1)\n\n        return super(FiDT5, self).forward(\n            **kwargs\n        )\n\n    # We need to resize as B x (N * L) instead of (B * N) x L here\n    # because the T5 forward method uses the input tensors to infer\n    # dimensions used in the decoder.\n    # EncoderWrapper resizes the inputs as (B * N) x L.\n    def forward(self, input_ids=None, attention_mask=None, **kwargs):\n        if input_ids != None:\n            # inputs might have already be resized in the generate method\n            if input_ids.dim() == 3:\n                self.encoder.n_passages = input_ids.size(1)\n            input_ids = input_ids.view(input_ids.size(0), -1)\n        if attention_mask != None:\n            attention_mask = attention_mask.view(attention_mask.size(0), -1)\n        return super().forward(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            **kwargs\n        )\n\n    # We need to resize the inputs here, as the generate method expect 2D tensors\n    def generate(self, input_ids, attention_mask, max_length,\n                 num_beams=10, num_return_sequences=1,\n                 temperature=None, top_p=None,\n                 lenpen=None):\n        self.encoder.n_passages = input_ids.size(1)\n        if num_beams != 1: # do beam search\n            return super().generate(\n                input_ids=input_ids.view(input_ids.size(0), -1),\n                attention_mask=attention_mask.view(attention_mask.size(0), -1),\n                max_length=max_length,\n                num_beams=num_beams,\n                length_penalty=lenpen,\n                num_return_sequences=num_return_sequences\n            )\n        else:\n            # sampling based decoding\n            return super().generate(\n                input_ids=input_ids.view(input_ids.size(0), -1),\n                attention_mask=attention_mask.view(attention_mask.size(0), -1),\n                max_length=max_length,\n                do_sample=True,\n                num_beams=1,\n                temperature=temperature,\n                top_p=top_p,\n                num_return_sequences=num_return_sequences\n            )\n\n    def wrap_encoder(self, use_checkpoint=False):\n        \"\"\"\n        Wrap T5 encoder to obtain a Fusion-in-Decoder model.\n        \"\"\"\n        self.encoder = EncoderWrapper(self.encoder, use_checkpoint=use_checkpoint)\n\n    def unwrap_encoder(self):\n        \"\"\"\n        Unwrap Fusion-in-Decoder encoder, useful to load T5 weights.\n        \"\"\"\n        self.encoder = self.encoder.encoder\n        block = []\n        for mod in self.encoder.block:\n            block.append(mod.module)\n        block = nn.ModuleList(block)\n        self.encoder.block = block\n\n    def load_t5(self, state_dict):\n        self.unwrap_encoder()\n        self.load_state_dict(state_dict)\n        self.wrap_encoder()\n\n    def set_checkpoint(self, use_checkpoint):\n        \"\"\"\n        Enable or disable checkpointing in the encoder.\n        See https://pytorch.org/docs/stable/checkpoint.html\n        \"\"\"\n        for mod in self.encoder.encoder.block:\n            mod.use_checkpoint = use_checkpoint\n\n    def reset_score_storage(self):\n        \"\"\"\n        Reset score storage, only used when cross-attention scores are saved\n        to train a retriever.\n        \"\"\"\n        for mod in self.decoder.block:\n            mod.layer[1].EncDecAttention.score_storage = None\n\n    def get_crossattention_scores(self, context_mask, use_softmax=False):\n        \"\"\"\n        Cross-attention scores are aggregated to obtain a single scalar per\n        passage. This scalar can be seen as a similarity score between the\n        question and the input passage. It is obtained by averaging the\n        cross-attention scores obtained on the first decoded token over heads,\n        layers, and tokens of the input passage.\n\n        More details in Distilling Knowledge from Reader to Retriever:\n        https://arxiv.org/abs/2012.04584.\n        \"\"\"\n        scores = []\n        n_passages = context_mask.size(1)\n        for mod in self.decoder.block:\n            scores.append(mod.layer[1].EncDecAttention.score_storage)\n        scores = torch.cat(scores, dim=2)\n        bsz, n_heads, n_layers, _ = scores.size()\n        # batch_size, n_head, n_layers, n_passages, text_maxlength\n        if use_softmax:\n            scores = scores.view(bsz, n_heads, n_layers, n_passages, -1)\n            scores = scores.masked_fill(~context_mask[:, None, None], -1e10)\n            scores = scores.view(bsz, n_heads, n_layers, -1)\n            scores = torch.softmax(scores, dim=-1)\n        scores = scores.view(bsz, n_heads, n_layers, n_passages, -1)\n        scores = scores.masked_fill(~context_mask[:, None, None], 0.)\n        scores = scores.sum(dim=[1, 2, 4])\n        ntokens = context_mask.sum(dim=[2]) * n_layers * n_heads\n        scores = scores/ntokens\n        return scores\n\n    def overwrite_forward_crossattention(self):\n        \"\"\"\n        Replace cross-attention forward function, only used to save\n        cross-attention scores.\n        \"\"\"\n        for mod in self.decoder.block:\n            attn = mod.layer[1].EncDecAttention\n            attn.forward = types.MethodType(cross_attention_forward, attn)\n\nclass EncoderWrapper(torch.nn.Module):\n    \"\"\"\n    Encoder Wrapper for T5 Wrapper to obtain a Fusion-in-Decoder model.\n    \"\"\"\n    def __init__(self, encoder, use_checkpoint=False):\n        super().__init__()\n\n        self.encoder = encoder\n        apply_checkpoint_wrapper(self.encoder, use_checkpoint)\n\n    def forward(self, input_ids=None, attention_mask=None, **kwargs,):\n        # total_length = n_passages * passage_length\n        bsz, total_length = input_ids.shape\n        passage_length = total_length // self.n_passages\n        input_ids = input_ids.view(bsz*self.n_passages, passage_length)\n        attention_mask = attention_mask.view(bsz*self.n_passages, passage_length)\n        outputs = self.encoder(input_ids, attention_mask, **kwargs)\n        outputs = (outputs[0].view(bsz, self.n_passages*passage_length, -1), ) + outputs[1:]\n        return outputs\n\nclass CheckpointWrapper(torch.nn.Module):\n    \"\"\"\n    Wrapper replacing None outputs by empty tensors, which allows the use of\n    checkpointing.\n    \"\"\"\n    def __init__(self, module, use_checkpoint=False):\n        super().__init__()\n        self.module = module\n        self.use_checkpoint = use_checkpoint\n\n    def forward(self, hidden_states, attention_mask, position_bias, **kwargs):\n        if self.use_checkpoint and self.training:\n            kwargs = {k: v for k, v in kwargs.items() if v is not None}\n            def custom_forward(*inputs):\n                output = self.module(*inputs, **kwargs)\n                empty = torch.tensor(\n                    [],\n                    dtype=torch.float,\n                    device=output[0].device,\n                    requires_grad=True)\n                output = tuple(x if x is not None else empty for x in output)\n                return output\n\n            output = torch.utils.checkpoint.checkpoint(\n                custom_forward,\n                hidden_states,\n                attention_mask,\n                position_bias\n            )\n            output = tuple(x if x.size() != 0 else None for x in output)\n        else:\n            output = self.module(hidden_states, attention_mask, position_bias, **kwargs)\n        return output\n\ndef apply_checkpoint_wrapper(t5stack, use_checkpoint):\n    \"\"\"\n    Wrap each block of the encoder to enable checkpointing.\n    \"\"\"\n    block = []\n    for mod in t5stack.block:\n        wrapped_mod = CheckpointWrapper(mod, use_checkpoint)\n        block.append(wrapped_mod)\n    block = nn.ModuleList(block)\n    t5stack.block = block\n\ndef cross_attention_forward(\n        self,\n        input,\n        mask=None,\n        kv=None,\n        position_bias=None,\n        past_key_value_state=None,\n        head_mask=None,\n        query_length=None,\n        use_cache=False,\n        output_attentions=False,\n    ):\n    \"\"\"\n    This only works for computing cross attention over the input\n    \"\"\"\n    assert(kv != None)\n    assert(head_mask == None)\n    assert(position_bias != None or self.has_relative_attention_bias)\n\n    bsz, qlen, dim = input.size()\n    n_heads, d_heads = self.n_heads, self.d_kv\n    klen = kv.size(1)\n\n    q = self.q(input).view(bsz, -1, n_heads, d_heads).transpose(1, 2)\n    if past_key_value_state == None:\n        k = self.k(kv).view(bsz, -1, n_heads, d_heads).transpose(1, 2)\n        v = self.v(kv).view(bsz, -1, n_heads, d_heads).transpose(1, 2)\n    else:\n        k, v = past_key_value_state\n\n    scores = torch.einsum(\"bnqd,bnkd->bnqk\", q, k)\n\n    if mask is not None:\n       scores += mask\n\n    if position_bias is None:\n        position_bias = self.compute_bias(qlen, klen)\n    scores += position_bias\n\n    if self.score_storage is None:\n        self.score_storage = scores\n\n    attn = F.softmax(scores.float(), dim=-1).type_as(scores)\n    attn = F.dropout(attn, p=self.dropout, training=self.training)\n\n    output = torch.matmul(attn, v)\n    output = output.transpose(1, 2).contiguous().view(bsz, -1, self.inner_dim)\n    output = self.o(output)\n\n    if use_cache:\n        output = (output,) + ((k, v),)\n    else:\n        output = (output,) + (None,)\n\n    if output_attentions:\n        output = output + (attn,)\n\n    if self.has_relative_attention_bias:\n        output = output + (position_bias,)\n\n    return output\n\nclass RetrieverConfig(transformers.BertConfig):\n\n    def __init__(self,\n                 indexing_dimension=768,\n                 apply_question_mask=False,\n                 apply_passage_mask=False,\n                 extract_cls=False,\n                 passage_maxlength=200,\n                 question_maxlength=40,\n                 projection=True,\n                 **kwargs):\n        super().__init__(**kwargs)\n        self.indexing_dimension = indexing_dimension\n        self.apply_question_mask = apply_question_mask\n        self.apply_passage_mask = apply_passage_mask\n        self.extract_cls=extract_cls\n        self.passage_maxlength = passage_maxlength\n        self.question_maxlength = question_maxlength\n        self.projection = projection\n\nclass Retriever(transformers.PreTrainedModel):\n\n    config_class = RetrieverConfig\n    base_model_prefix = \"retriever\"\n\n    def __init__(self, config, initialize_wBERT=False):\n        super().__init__(config)\n        assert config.projection or config.indexing_dimension == 768, \\\n            'If no projection then indexing dimension must be equal to 768'\n        self.config = config\n        if initialize_wBERT:\n            self.model = transformers.BertModel.from_pretrained('bert-base-uncased')\n        else:\n            self.model = transformers.BertModel(config)\n        if self.config.projection:\n            self.proj = nn.Linear(\n                self.model.config.hidden_size,\n                self.config.indexing_dimension\n            )\n            self.norm = nn.LayerNorm(self.config.indexing_dimension)\n        self.loss_fct = torch.nn.KLDivLoss()\n\n    def forward(self,\n                question_ids,\n                question_mask,\n                passage_ids,\n                passage_mask,\n                gold_score=None):\n        question_output = self.embed_text(\n            text_ids=question_ids,\n            text_mask=question_mask,\n            apply_mask=self.config.apply_question_mask,\n            extract_cls=self.config.extract_cls,\n        )\n        bsz, n_passages, plen = passage_ids.size()\n        passage_ids = passage_ids.view(bsz * n_passages, plen)\n        passage_mask = passage_mask.view(bsz * n_passages, plen)\n        passage_output = self.embed_text(\n            text_ids=passage_ids,\n            text_mask=passage_mask,\n            apply_mask=self.config.apply_passage_mask,\n            extract_cls=self.config.extract_cls,\n        )\n\n        score = torch.einsum(\n            'bd,bid->bi',\n            question_output,\n            passage_output.view(bsz, n_passages, -1)\n        )\n        score = score / np.sqrt(question_output.size(-1))\n        if gold_score is not None:\n            loss = self.kldivloss(score, gold_score)\n        else:\n            loss = None\n\n        return question_output, passage_output, score, loss\n\n    def embed_text(self, text_ids, text_mask, apply_mask=False, extract_cls=False):\n        text_output = self.model(\n            input_ids=text_ids,\n            attention_mask=text_mask if apply_mask else None\n        )\n        if type(text_output) is not tuple:\n            text_output.to_tuple()\n        text_output = text_output[0]\n        if self.config.projection:\n            text_output = self.proj(text_output)\n            text_output = self.norm(text_output)\n\n        if extract_cls:\n            text_output = text_output[:, 0]\n        else:\n            if apply_mask:\n                text_output = text_output.masked_fill(~text_mask[:, :, None], 0.)\n                text_output = torch.sum(text_output, dim=1) / torch.sum(text_mask, dim=1)[:, None]\n            else:\n                text_output = torch.mean(text_output, dim=1)\n        return text_output\n\n    def kldivloss(self, score, gold_score):\n        gold_score = torch.softmax(gold_score, dim=-1)\n        score = torch.nn.functional.log_softmax(score, dim=-1)\n        return self.loss_fct(score, gold_score)\n"}
{"type": "source_file", "path": "generator/fid/fid_to_reload.py", "content": "\"\"\"\nConvert SimCSE's checkpoints to Huggingface style.\n\"\"\"\n\nimport argparse\nimport shutil\n\nimport torch\nimport os\nimport json\n\ndef change_name(path, new_path=None):\n    if new_path is None:\n        new_path = path\n    state_dict = torch.load(os.path.join(path, \"pytorch_model.bin\"), map_location=torch.device(\"cpu\"))\n    new_state_dict = {}\n    keep = []\n    change = []\n    for key, param in state_dict.items():\n        if key.startswith(\"encoder.encoder\"):\n            key = key.replace(\"encoder.encoder\", \"encoder\")\n            key = key.replace(\"module.layer\", \"layer\")\n            change.append(key)\n        else:\n            keep.append(key)\n        new_state_dict[key] = param\n\n    if not os.path.exists(new_path):\n        os.makedirs(new_path)\n\n    torch.save(new_state_dict, os.path.join(new_path, \"pytorch_model.bin\"))\n    print(f\"kept keys: {keep}\")\n    print(f\"changed keys: {change}\")\n    for file in os.listdir(path):\n        if file != 'pytorch_model.bin':\n            shutil.copyfile(os.path.join(path, file), os.path.join(new_path, file))\n\n    for name in ['config.json', 'special_tokens_map.json', 'vocab.json', 'merges.txt', 'tokenizer_config.json']:\n        shutil.copyfile(os.path.join('/home/shuyanzh/workshop/op_agent/data/fid/codet5-base', name), os.path.join(new_path, name))\n    print(\"Copy tokenization files from codet5-base folder to the target folder\")\n\n    # Change architectures in config. json\n    # config = json.load(open(os.path.join(path, \"config.json\")))\n    # for i in range(len(config[\"architectures\"])):\n    #     config[\"architectures\"][i] = config[\"architectures\"][i].replace(\"ForCL\", \"Model\")\n    # json.dump(config, open(os.path.join(new_path, \"config.json\"), \"w\"), indent=2)\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--path\", type=str, help=\"Path of SimCSE checkpoint folder\")\n    parser.add_argument('--new_path', type=str, help='New place to save checkpoints', default=None)\n    args = parser.parse_args()\n    args.path = '/home/shuyanzh/workshop/op_agent/data/fid/code_t5_nothing/checkpoint/best_dev'\n    args.new_path = '/home/shuyanzh/workshop/op_agent/data/fid/code_t5_nothing/checkpoint/best_dev.reload'\n    print(\"FiD checkpoint -> Fid Reload checkpoint for {}\".format(args.path))\n    change_name(args.path, args.new_path)\n\n\nif __name__ == \"__main__\":\n    main()\n"}
{"type": "source_file", "path": "generator/fid/check_reload_model.py", "content": "import sys\nfrom fid_to_reload import change_name\nimport os\n# if it is a reload checkpoint, check its existence, if not exist, convert from its original saved results\ndef main():\n    args = sys.argv[1]\n    print(args)\n    model_name = None\n    for idx, tok in enumerate(args.split()):\n        if tok == '--model_name':\n            model_name = args.split()[idx+1]\n            break\n    assert model_name\n\n    if 'checkpoint' in model_name:\n        assert '.reload' in model_name\n        path = model_name.replace(\".reload\", \"\")\n        if not os.path.exists(f\"{model_name}/pytorch_model.bin\"):\n            change_name(path, model_name)\n    else:\n        print(\"good to go\")\n\nif __name__ == \"__main__\":\n    main()"}
{"type": "source_file", "path": "generator/fid/src/data.py", "content": "# Copyright (c) Facebook, Inc. and its affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport torch\nimport random\nimport json\nimport numpy as np\n\nclass Dataset(torch.utils.data.Dataset):\n    def __init__(self,\n                 data,\n                 n_context=None,\n                 question_prefix='question:',\n                 title_prefix='title:',\n                 passage_prefix='context:'):\n        self.data = data\n        self.n_context = n_context\n        self.question_prefix = question_prefix\n        self.title_prefix = title_prefix\n        self.passage_prefix = passage_prefix\n        self.sort_data()\n\n    def __len__(self):\n        return len(self.data)\n\n    def get_target(self, example):\n        if 'target' in example:\n            target = example['target']\n            return target + ' </s>'\n        elif 'answers' in example:\n            return random.choice(example['answers']) + ' </s>'\n        else:\n            return None\n\n    def __getitem__(self, index):\n        example = self.data[index]\n        question = self.question_prefix + \" \" + example['question']\n        target = self.get_target(example)\n\n        if 'ctxs' in example and self.n_context is not None:\n            f = self.title_prefix + \" {} \" + self.passage_prefix + \" {}\"\n            contexts = example['ctxs'][:self.n_context]\n            passages = [f.format(c['title'], c['text']) for c in contexts]\n            scores = [float(c['score']) for c in contexts]\n            scores = torch.tensor(scores)\n            # TODO(egrave): do we want to keep this?\n            if len(contexts) == 0:\n                contexts = [question]\n        else:\n            passages, scores = None, None\n\n\n        return {\n            'index' : index,\n            'question' : question,\n            'target' : target,\n            'passages' : passages,\n            'scores' : scores\n        }\n\n    def sort_data(self):\n        if self.n_context is None or not 'score' in self.data[0]['ctxs'][0]:\n            return\n        for ex in self.data:\n            ex['ctxs'].sort(key=lambda x: float(x['score']), reverse=True)\n\n    def get_example(self, index):\n        return self.data[index]\n\ndef encode_passages(batch_text_passages, tokenizer, max_length):\n    passage_ids, passage_masks = [], []\n    for k, text_passages in enumerate(batch_text_passages):\n        p = tokenizer.batch_encode_plus(\n            text_passages,\n            max_length=max_length,\n            pad_to_max_length=True,\n            return_tensors='pt',\n            truncation=True\n        )\n        passage_ids.append(p['input_ids'][None])\n        passage_masks.append(p['attention_mask'][None])\n\n    passage_ids = torch.cat(passage_ids, dim=0)\n    passage_masks = torch.cat(passage_masks, dim=0)\n    return passage_ids, passage_masks.bool()\n\nclass Collator(object):\n    def __init__(self, text_maxlength, tokenizer, answer_maxlength=20):\n        self.tokenizer = tokenizer\n        self.text_maxlength = text_maxlength\n        self.answer_maxlength = answer_maxlength\n\n    def __call__(self, batch):\n        assert(batch[0]['target'] != None)\n        index = torch.tensor([ex['index'] for ex in batch])\n        target = [ex['target'] for ex in batch]\n        target = self.tokenizer.batch_encode_plus(\n            target,\n            max_length=self.answer_maxlength if self.answer_maxlength > 0 else None,\n            pad_to_max_length=True,\n            return_tensors='pt',\n            truncation=True if self.answer_maxlength > 0 else False,\n        )\n        target_ids = target[\"input_ids\"]\n        target_mask = target[\"attention_mask\"].bool()\n        target_ids = target_ids.masked_fill(~target_mask, -100)\n\n        def append_question(example):\n            if example['passages'] is None:\n                return [example['question']]\n            return [example['question'] + \" \" + t for t in example['passages']]\n        text_passages = [append_question(example) for example in batch]\n        passage_ids, passage_masks = encode_passages(text_passages,\n                                                     self.tokenizer,\n                                                     self.text_maxlength)\n\n        return (index, target_ids, target_mask, passage_ids, passage_masks)\n\ndef load_data(data_path=None, global_rank=-1, world_size=-1):\n    assert data_path\n    if data_path.endswith('.jsonl'):\n        data = open(data_path, 'r')\n    elif data_path.endswith('.json'):\n        with open(data_path, 'r') as fin:\n            data = json.load(fin)\n    examples = []\n    for k, example in enumerate(data):\n        if global_rank > -1 and not k%world_size==global_rank:\n            continue\n        if data_path is not None and data_path.endswith('.jsonl'):\n            example = json.loads(example)\n        if not 'id' in example:\n            example['id'] = k\n        for c in example['ctxs']:\n            if not 'score' in c:\n                c['score'] = 1.0 / (k + 1)\n        examples.append(example)\n    ## egrave: is this needed?\n    if data_path is not None and data_path.endswith('.jsonl'):\n        data.close()\n\n    return examples\n\nclass RetrieverCollator(object):\n    def __init__(self, tokenizer, passage_maxlength=200, question_maxlength=40):\n        self.tokenizer = tokenizer\n        self.passage_maxlength = passage_maxlength\n        self.question_maxlength = question_maxlength\n\n    def __call__(self, batch):\n        index = torch.tensor([ex['index'] for ex in batch])\n\n        question = [ex['question'] for ex in batch]\n        question = self.tokenizer.batch_encode_plus(\n            question,\n            pad_to_max_length=True,\n            return_tensors=\"pt\",\n            max_length=self.question_maxlength,\n            truncation=True\n        )\n        question_ids = question['input_ids']\n        question_mask = question['attention_mask'].bool()\n\n        if batch[0]['scores'] is None or batch[0]['passages'] is None:\n            return index, question_ids, question_mask, None, None, None\n\n        scores = [ex['scores'] for ex in batch]\n        scores = torch.stack(scores, dim=0)\n\n        passages = [ex['passages'] for ex in batch]\n        passage_ids, passage_masks = encode_passages(\n            passages,\n            self.tokenizer,\n            self.passage_maxlength\n        )\n\n        return (index, question_ids, question_mask, passage_ids, passage_masks, scores)\n\nclass TextDataset(torch.utils.data.Dataset):\n    def __init__(self,\n                 data,\n                 title_prefix='title:',\n                 passage_prefix='context:'):\n        self.data = data\n        self.title_prefix = title_prefix\n        self.passage_prefix = passage_prefix\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, index):\n        example = self.data[index]\n        text = self.title_prefix + \" \" + example[2] + \" \" + \\\n            self.passage_prefix + \" \" + example[1]\n        return example[0], text\n\nclass TextCollator(object):\n    def __init__(self, tokenizer, maxlength=200):\n        self.tokenizer = tokenizer\n        self.maxlength = maxlength\n\n    def __call__(self, batch):\n        index = [x[0] for x in batch]\n        encoded_batch = self.tokenizer.batch_encode_plus(\n            [x[1] for x in batch],\n            pad_to_max_length=True,\n            return_tensors=\"pt\",\n            max_length=self.maxlength,\n            truncation=True\n        )\n        text_ids = encoded_batch['input_ids']\n        text_mask = encoded_batch['attention_mask'].bool()\n\n        return index, text_ids, text_mask\n"}
{"type": "source_file", "path": "generator/fid/src/index.py", "content": "# Copyright (c) Facebook, Inc. and its affiliates.\n# All rights reserved.\n# \n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport os\nimport logging\nimport pickle\nfrom typing import List, Tuple\n\nimport faiss\nimport numpy as np\nfrom tqdm import tqdm\n\nlogger = logging.getLogger()\n\nclass Indexer(object):\n\n    def __init__(self, vector_sz, n_subquantizers=0, n_bits=8):\n        if n_subquantizers > 0:\n            self.index = faiss.IndexPQ(vector_sz, n_subquantizers, n_bits, faiss.METRIC_INNER_PRODUCT)\n        else:\n            self.index = faiss.IndexFlatIP(vector_sz)\n        self.index_id_to_db_id = np.empty((0), dtype=np.int64)\n\n    def index_data(self, ids, embeddings):\n        self._update_id_mapping(ids)\n        embeddings = embeddings.astype('float32')\n        if not self.index.is_trained:\n            self.index.train(embeddings)\n        self.index.add(embeddings)\n\n        logger.info(f'Total data indexed {len(self.index_id_to_db_id)}')\n\n    def search_knn(self, query_vectors: np.array, top_docs: int, index_batch_size=1024) -> List[Tuple[List[object], List[float]]]:\n        query_vectors = query_vectors.astype('float32')\n        result = []\n        nbatch = (len(query_vectors)-1) // index_batch_size + 1\n        for k in tqdm(range(nbatch)):\n            start_idx = k*index_batch_size\n            end_idx = min((k+1)*index_batch_size, len(query_vectors))\n            q = query_vectors[start_idx: end_idx]\n            scores, indexes = self.index.search(q, top_docs)\n            # convert to external ids\n            db_ids = [[str(self.index_id_to_db_id[i]) for i in query_top_idxs] for query_top_idxs in indexes]\n            result.extend([(db_ids[i], scores[i]) for i in range(len(db_ids))])\n        return result\n\n    def serialize(self, dir_path):\n        index_file = dir_path / 'index.faiss'\n        meta_file = dir_path / 'index_meta.dpr'\n        logger.info(f'Serializing index to {index_file}, meta data to {meta_file}')\n\n        faiss.write_index(self.index, index_file)\n        with open(meta_file, mode='wb') as f:\n            pickle.dump(self.index_id_to_db_id, f)\n\n    def deserialize_from(self, dir_path):\n        index_file = dir_path / 'index.faiss'\n        meta_file = dir_path / 'index_meta.dpr'\n        logger.info(f'Loading index from {index_file}, meta data from {meta_file}')\n\n        self.index = faiss.read_index(index_file)\n        logger.info('Loaded index of type %s and size %d', type(self.index), self.index.ntotal)\n\n        with open(meta_file, \"rb\") as reader:\n            self.index_id_to_db_id = pickle.load(reader)\n        assert len(\n            self.index_id_to_db_id) == self.index.ntotal, 'Deserialized index_id_to_db_id should match faiss index size'\n\n    def _update_id_mapping(self, db_ids: List):\n        new_ids = np.array(db_ids, dtype=np.int64)\n        self.index_id_to_db_id = np.concatenate((self.index_id_to_db_id, new_ids), axis=0)"}
{"type": "source_file", "path": "generator/fid/utils/convert_data.py", "content": "import argparse\nimport glob\nimport os\nimport json\nimport sys\nfrom pathlib import Path\n\n\ndef convert_data(src_file, manual_file, info_file, fid_file, retrieved_manual_list, topk=100, sort_ctx=False):\n    with open(manual_file, 'r') as f:\n        manual_d = json.load(f)\n        for k in list(manual_d.keys()):\n            manual_d[f'|{\"_\".join(k.split(\"_\")[:-1])}|'] = \"_\".join(k.split(\"_\")[:-1])\n        manual_d['|placeholder|'] = 'manual'\n\n    if info_file:\n        with open(info_file, 'r') as f:\n            manual_info = json.load(f)\n            manual_info['|placeholder|'] = {'lib_signature': 'manual'}\n    else:\n        manual_info = None\n\n\n    with open(src_file, 'r') as f:\n        src_d = json.load(f)\n\n    tgt_d = []\n    tot = 0\n    for src_item in src_d:\n        tgt_item = {}\n        tgt_item['id'] = src_item['question_id']\n        tgt_item['question'] = src_item['nl']\n        tgt_item['target'] = src_item['cmd']\n        tgt_item['answers'] = [src_item['cmd']]\n        ctxs = []\n        _ctxs = set()\n        for cur_retrieved in retrieved_manual_list:\n            for man in cur_retrieved[src_item['question_id']]['retrieved']:\n                if manual_info:\n                    title = manual_info[man]['lib_signature'].replace(\".\", \" \")\n                else:\n                    title = man if man != '|placeholder|' else 'manual'\n                if man not in manual_d:\n                    text = \"\"\n                    print(f\"[WARNING] {man} cannot be found\")\n                else:\n                    text = manual_d[man]\n                cur_ctx = {'title': title, 'text': text, 'man_id': man}\n                if man not in _ctxs:\n                    ctxs.append(cur_ctx)\n                    _ctxs.add(man)\n\n        tgt_item['ctxs'] = ctxs[:topk]\n        if len(tgt_item['ctxs']) < topk:\n            for _ in range(topk - len(tgt_item['ctxs'])):\n                tgt_item['ctxs'].append({'title': '', 'text': '', 'man_id': 'fake'})\n        if sort_ctx:\n            tgt_item['ctxs'] = sorted(tgt_item['ctxs'], key=lambda x: int(x['man_id'].split(\"_\")[-1]) if x['man_id'][-1].isdigit() else 10000)\n        tot += len(tgt_item['ctxs'])\n        tgt_d.append(tgt_item)\n\n\n    with open(fid_file, 'w+') as f:\n        json.dump(tgt_d, f, indent=2)\n\n    # with open(str(fid_file).replace(\".json\", \".10.json\"), 'w+') as f:\n    #     json.dump(tgt_d[:10], f, indent=2)\n    print(f\"save {len(tgt_d)} data to {os.path.basename(fid_file)}\")\n\n\ndef process_manual_list(manual_list):\n    _manual_list = []\n    for l in manual_list:\n        keys = list(l[0].keys())\n        r_key = [x for x in keys if 'retrieved' in x][0]\n        # s_key = [x for x in keys if 'score' in x][0]\n        for item in l:\n            item['retrieved'] = item.pop(r_key)\n        l = {x['question_id']: x for x in l}\n        _manual_list.append(l)\n    return _manual_list\n\ndef run_conala(args):\n    root = Path('data/conala/nl.cm')\n    all_splits = []\n    if args.have_train:\n        all_splits.append('cmd_train')\n    if args.have_dev:\n        all_splits.append('cmd_dev')\n    if args.have_test:\n        all_splits.append('cmd_test')\n\n    # # fake\n    if args.gen_fake:\n        for s in all_splits:\n            src_file = root / f'{s}.seed.json'\n            manual_file = root / 'manual_all_raw.json'\n            info_file = root / 'manual.info.json'\n            retrieved_manual_list = []\n            with open(root / f'{s}.oracle_manual.es0.code.library.full.json', 'r') as f:\n                d = json.load(f)\n                d = {x['question_id']: {'retrieved': ['|placeholder|']} for x in d}\n                retrieved_manual_list.append(d)\n            fid_file = root / f'fid.{s}.nothing.json'\n            convert_data(src_file, manual_file, info_file, fid_file, retrieved_manual_list, topk=1)\n\n    if args.gen_mine_fake:\n        s = 'cmd_mined'\n        src_file = root / f'{s}.seed.json'\n        manual_file = root / 'manual_all_raw.json'\n        info_file = root / 'manual.info.json'\n        retrieved_manual_list = []\n        with open(root / f'{s}.oracle_manual.es0.code.library.full.json', 'r') as f:\n            d = json.load(f)\n            d = {x['question_id']: {'retrieved': ['|placeholder|']} for x in d}\n            retrieved_manual_list.append(d)\n        fid_file = root / f'fid.{s}.nothing.json'\n        convert_data(src_file, manual_file, info_file, fid_file, retrieved_manual_list, topk=1)\n\n        with open(fid_file, 'r') as f:\n            d = json.load(f)\n        _d = []\n        for item in d:\n            if '_' in item['id']:\n                if len(item['question'].split()) >= 100 or \\\n                        len(item['target'].split()) >= 100 or \\\n                        len(item['question']) >= 500 or \\\n                        len(item['target']) >= 500:\n                    continue\n\n                _d.append(item)\n\n        print(len(d), len(_d))\n        with open(fid_file, 'w+') as f:\n            json.dump(_d, f, indent=2)\n\n    if args.gen_oracle:\n        for s in all_splits:\n            src_file = root / f'{s}.seed.json'\n            manual_file = root / 'manual_all_raw.json'\n            info_file = root / 'manual.info.json'\n            retrieved_manual_list = []\n            with open(root / f'{s}.oracle_manual.es0.code.library.full.json', 'r') as f:\n                retrieved_manual_list.append(json.load(f))\n            fid_file = root / f'fid.{s}.oracle.json'\n            retrieved_manual_list = process_manual_list(retrieved_manual_list)\n            convert_data(src_file, manual_file, info_file, fid_file, retrieved_manual_list, topk=3)\n\n    if args.gen_retrieval:\n        for s in all_splits:\n            for topk in [15, 20, 25, 30][:]:\n                src_file = root / f'{s}.seed.json'\n                manual_file = root / 'manual_all_raw.json'\n                info_file = root / 'manual.info.json'\n                retrieved_manual_list = []\n                with open(root / f'{s}.{args.retrieval_file_tag}.json', 'r') as f:\n                    retrieved_manual_list.append(json.load(f))\n                fid_file = root / f'fid.{s}.{args.retrieval_file_tag}.t{topk}.json'\n                retrieved_manual_list = process_manual_list(retrieved_manual_list)\n                convert_data(src_file, manual_file, info_file, fid_file, retrieved_manual_list, topk=topk)\n\n    if args.gen_oracle_retrieval:\n        for s in all_splits:\n            for topk in [1, 3, 5, 10][:]:\n                src_file = root / f'{s}.seed.json'\n                manual_file = root / 'manual_all_raw.json'\n                info_file = root / 'manual.info.json'\n                retrieved_manual_list = []\n                with open(root / f'{s}.oracle_manual.es0.code.library.full.json', 'r') as f:\n                    retrieved_manual_list.append(json.load(f))\n                with open(root / f'{s}.{args.retrieval_file_tag}.json', 'r') as f:\n                    retrieved_manual_list.append(json.load(f))\n                fid_file = root / f'fid.{s}.oracle.{args.retrieval_file_tag}.t{topk}.json'\n                retrieved_manual_list = process_manual_list(retrieved_manual_list)\n                convert_data(src_file, manual_file, info_file, fid_file, retrieved_manual_list, topk=topk)\n\ndef run_tldr(args):\n    root = Path('data/tldr/nl.cm')\n    all_splits = []\n    if args.have_train:\n        all_splits.append('cmd_train')\n    if args.have_dev:\n        all_splits.append('cmd_dev')\n    if args.have_test:\n        all_splits.append('cmd_test')\n\n    manual_file = root / 'manual_section.json'\n    # # fake\n    if args.gen_fake:\n        for s in all_splits:\n            src_file = root / f'{s}.seed.json'\n            retrieved_manual_list = []\n            with open(root / f'{s}.oracle_manual.es1.full.oracle.json', 'r') as f:\n                d = json.load(f)\n                d = {x['question_id']: {'retrieved': ['|placeholder|']} for x in d}\n                retrieved_manual_list.append(d)\n            fid_file = root / f'fid.{s}.nothing.json'\n            convert_data(src_file, manual_file, None, fid_file, retrieved_manual_list, topk=1, sort_ctx=True)\n\n    if args.gen_oracle:\n        for s in all_splits:\n            src_file = root / f'{s}.seed.json'\n            retrieved_manual_list = []\n            with open(root / f'{s}.oracle_manual.es1.full.oracle.json', 'r') as f:\n                retrieved_manual_list.append(json.load(f))\n            fid_file = root / f'fid.{s}.oracle.json'\n            retrieved_manual_list = process_manual_list(retrieved_manual_list)\n            convert_data(src_file, manual_file, None, fid_file, retrieved_manual_list, topk=10, sort_ctx=True)\n\n    if args.gen_oracle_cmd:\n        assert 'tldr' in str(root)\n        for s in all_splits:\n            src_file = root / f'{s}.seed.json'\n            retrieved_manual_list = []\n            with open(root / f'{s}.oracle_manual.es1.full.oracle.json', 'r') as f:\n                d = json.load(f)\n                d = {x['question_id']: {'retrieved': [f'|{x[\"cmd_name\"]}|']} for x in d}\n                retrieved_manual_list.append(d)\n            fid_file = root / f'fid.{s}.oracle_cmd.json'\n            convert_data(src_file, manual_file, None, fid_file, retrieved_manual_list, topk=1, sort_ctx=True)\n\n    if args.gen_retrieval:\n        for s in all_splits:\n            for topk in [5][:]:\n                for x in range(30):\n                    if not (root / f'{s}.{args.retrieval_file_tag}.{x}.json').exists():\n                        break\n                    src_file = root / f'{s}.seed.json'\n                    retrieved_manual_list = []\n                    with open(root / f'{s}.{args.retrieval_file_tag}.{x}.json', 'r') as f:\n                        retrieved_manual_list.append(json.load(f))\n                    fid_file = root / f'fid.{s}.{args.retrieval_file_tag}.t{topk}.{x}.json'\n                    retrieved_manual_list = process_manual_list(retrieved_manual_list)\n                    convert_data(src_file, manual_file, None, fid_file, retrieved_manual_list, topk=topk, sort_ctx=True)\n\n                # merge data\n                all_data = []\n                for x in range(30):\n                    fid_file = root / f'fid.{s}.{args.retrieval_file_tag}.t{topk}.{x}.json'\n                    if not fid_file.exists():\n                        break\n                    with open(fid_file, 'r') as f:\n                        curr_data = json.load(f)\n                        all_data.extend(curr_data)\n                    # os.remove(fid_file)\n\n                print(f\"merged: {len(all_data)}\")\n                with open(root / f'fid.{s}.{args.retrieval_file_tag}.t{topk}.json', 'w') as f:\n                    json.dump(all_data, f, indent=2)\n\n\n    if args.gen_oracle_retrieval:\n        for s in all_splits:\n            for topk in [10, 15][:]:\n                src_file = root / f'{s}.seed.json'\n                retrieved_manual_list = []\n                with open(root / f'{s}.oracle_manual.es1.full.oracle.json', 'r') as f:\n                    retrieved_manual_list.append(json.load(f))\n                with open(root / f'{s}.{args.retrieval_file_tag}.0.json', 'r') as f:\n                    retrieved_manual_list.append(json.load(f))\n                fid_file = root / f'fid.{s}.oracle.{args.retrieval_file_tag}.t{topk}.json'\n                retrieved_manual_list = process_manual_list(retrieved_manual_list)\n                convert_data(src_file, manual_file, None, fid_file, retrieved_manual_list, topk=topk, sort_ctx=True)\n\n\ndef config():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--gen_retrieval', action='store_true')\n    parser.add_argument('--gen_oracle', action='store_true')\n    parser.add_argument('--gen_oracle_cmd', action='store_true')\n    parser.add_argument('--gen_fake', action='store_true')\n    parser.add_argument('--gen_mine_fake', action='store_true')\n    parser.add_argument('--gen_oracle_retrieval', action='store_true')\n    parser.add_argument('--retrieval_file_tag')\n    parser.add_argument('--have_train', action='store_true')\n    parser.add_argument('--have_dev', action='store_true')\n    parser.add_argument('--have_test', action='store_true')\n    parser.add_argument('--conala', action='store_true')\n    parser.add_argument('--tldr', action='store_true')\n\n    args = parser.parse_args()\n    return args\n\nif __name__ == \"__main__\":\n    args = config()\n    if args.conala:\n        run_conala(args)\n        data = 'conala'\n    elif args.tldr:\n        run_tldr(args)\n        data = 'tldr'\n    else:\n        raise NotImplementedError\n"}
{"type": "source_file", "path": "generator/fid/train_reader.py", "content": "# Copyright (c) Facebook, Inc. and its affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\nimport json\nimport os\nimport time\nimport sys\nimport torch\nimport transformers\nimport numpy as np\nfrom pathlib import Path\nfrom torch.utils.data import DataLoader, RandomSampler, DistributedSampler, SequentialSampler\nfrom tqdm import tqdm\n\nfrom dataset_helper.conala.gen_metric import _bleu as conala_bleu\nfrom dataset_helper.tldr.gen_metric import tldr_metrics\nfrom src.options import Options\n\nimport src.slurm\nimport src.util\nimport src.evaluation\nimport src.data\nimport src.model\nimport wandb\n\nWANDB_DISABLED= os.environ['WANDB_DISABLED'] if 'WANDB_DISABLED' in os.environ else False\nTQDM_DISABLED = os.environ['TQDM_DISABLED'] if 'TQDM_DISABLED' in os.environ else False\n\ndef train(model, optimizer, scheduler, step, train_dataset, eval_dataset, opt, collator, best_dev_em, checkpoint_path):\n\n    # if opt.is_main:\n    #     try:\n    #         tb_logger = torch.utils.tensorboard.SummaryWriter(Path(opt.checkpoint_dir)/opt.name)\n    #     except:\n    #         tb_logger = None\n    #         logger.warning('Tensorboard is not available.')\n\n    torch.manual_seed(opt.global_rank + opt.seed) #different seed for different sampling depending on global_rank\n    train_sampler = RandomSampler(train_dataset)\n    train_dataloader = DataLoader(\n        train_dataset,\n        sampler=train_sampler,\n        batch_size=opt.per_gpu_batch_size,\n        drop_last=True,\n        num_workers=10,\n        collate_fn=collator\n    )\n\n    loss, curr_loss = 0.0, 0.0\n    epoch = 1\n    model.train()\n    while step < opt.total_steps:\n        epoch += 1\n        opt._epoch = epoch\n        for i, batch in enumerate(tqdm(train_dataloader, disable=TQDM_DISABLED)):\n            step += 1\n            opt._train_step = step\n            (idx, labels, _, context_ids, context_mask) = batch\n\n            train_loss = model(\n                input_ids=context_ids.cuda(),\n                attention_mask=context_mask.cuda(),\n                labels=labels.cuda()\n            )[0]\n\n            train_loss.backward()\n\n            if step % opt.accumulation_steps == 0:\n                torch.nn.utils.clip_grad_norm_(model.parameters(), opt.clip)\n                optimizer.step()\n                scheduler.step()\n                model.zero_grad()\n\n            train_loss = src.util.average_main(train_loss, opt)\n            curr_loss += train_loss.item()\n            wandb.log({'train_loss': train_loss.item(), 'lr': scheduler.get_last_lr()[0]})\n\n            if step % opt.eval_freq == 0:\n                dev_em = evaluate(model, eval_dataset, tokenizer, collator, opt)\n                wandb.log({f'eval_{x}': y for x, y in dev_em.items()})\n                dev_em = dev_em[opt.eval_metric]\n                model.train()\n                if opt.is_main:\n                    if dev_em > best_dev_em:\n                        best_dev_em = dev_em\n                        src.util.save(model, optimizer, scheduler, step, best_dev_em,\n                                      opt, checkpoint_path, 'best_dev')\n                    log = f\"{step} / {opt.total_steps} |\"\n                    log += f\"train: {curr_loss/opt.eval_freq:.3f} |\"\n                    log += f\"evaluation {opt.eval_metric}: {dev_em:.02f}|\"\n                    log += f\"lr: {scheduler.get_last_lr()[0]:.5f}\"\n                    logger.info(log)\n                    # if tb_logger is not None:\n                    #     tb_logger.add_scalar(\"Evaluation\", dev_em, step)\n                    #     tb_logger.add_scalar(\"Training\", curr_loss / (opt.eval_freq), step)\n                    curr_loss = 0.\n\n\n            if opt.is_main and step % opt.save_freq == 0:\n                src.util.save(model, optimizer, scheduler, step, best_dev_em,\n                              opt, checkpoint_path, f\"step-{step}\")\n\n            if step > opt.total_steps:\n                break\n\ndef evaluate_em(model, dataset, tokenizer, collator, opt):\n    sampler = SequentialSampler(dataset)\n    dataloader = DataLoader(dataset,\n        sampler=sampler,\n        batch_size=opt.per_gpu_batch_size,\n        drop_last=False,\n        num_workers=10,\n        collate_fn=collator\n    )\n    model.eval()\n    total = 0\n    exactmatch = []\n    model = model.module if hasattr(model, \"module\") else model\n    with torch.no_grad():\n        for i, batch in enumerate(dataloader):\n            (idx, _, _, context_ids, context_mask) = batch\n\n            outputs = model.generate(\n                input_ids=context_ids.cuda(),\n                attention_mask=context_mask.cuda(),\n                max_length=50\n            )\n\n            for k, o in enumerate(outputs):\n                ans = tokenizer.decode(o, skip_special_tokens=True)\n                gold = dataset.get_example(idx[k])['answers']\n                score = src.evaluation.ems(ans, gold)\n                total += 1\n                exactmatch.append(score)\n\n    exactmatch, total = src.util.weighted_average(np.mean(exactmatch), total, opt)\n    return exactmatch\n\ndef evaluate_customized(model, dataset, tokenizer, collator, opt, result_file=None, is_bleu=False, is_token_f1=False):\n    assert is_bleu != is_token_f1\n    sampler = SequentialSampler(dataset)\n    dataloader = DataLoader(dataset,\n        sampler=sampler,\n        batch_size=opt.per_gpu_batch_size,\n        drop_last=False,\n        num_workers=10,\n        collate_fn=collator\n    )\n    model.eval()\n    model = model.module if hasattr(model, \"module\") else model\n\n    with torch.no_grad():\n\n        result_file = f\"{opt.checkpoint_path}/dev_result_{opt._train_step}.json\" if result_file is None else result_file\n        result_d = []\n\n        with open(f\"{opt.checkpoint_path}/gold.gold\", \"w+\") as fg, open(f'{opt.checkpoint_path}/pred.pred', 'w+') as fp, open(result_file, 'w+') as fr:\n\n            for i, batch in enumerate(tqdm(dataloader, disable=TQDM_DISABLED)):\n                (idx, _, _, context_ids, context_mask) = batch\n\n                outputs = model.generate(\n                    input_ids=context_ids.cuda(),\n                    attention_mask=context_mask.cuda(),\n                    max_length=150,\n                )\n\n                for k, o in enumerate(outputs):\n                    ans = tokenizer.decode(o, skip_special_tokens=False, clean_up_tokenization_spaces=False)\n                    gold = dataset.get_example(idx[k])['target']\n                    ans = ans.replace(\"{{\", \" {{\").replace(\"\\n\", ' ').replace(\"\\r\", \"\").replace(\"<pad>\", \"\").replace(\"<s>\", \"\").replace(\"</s>\", \"\").strip()\n                    ans = \" \".join(ans.split())\n                    gold = gold.replace(\"\\n\", ' ')\n                    fg.write(f\"{gold}\\n\")\n                    fp.write(f\"{ans}\\n\")\n                    cur_result = {'question_id': dataset.get_example(idx[k])['id'], 'gold': gold, 'clean_code': ans}\n                    result_d.append(cur_result)\n\n            json.dump(result_d, fr, indent=2)\n\n\n\n        if is_bleu:\n            score = conala_bleu(\n                f\"{opt.checkpoint_path}/gold.gold\",\n                f\"{opt.checkpoint_path}/pred.pred\",\n                smooth=False, code_tokenize=True)\n            score = {'bleu': score}\n        elif is_token_f1:\n            score = tldr_metrics(\n                f\"{opt.checkpoint_path}/gold.gold\",\n                f\"{opt.checkpoint_path}/pred.pred\")\n\n        else:\n            raise NotImplementedError\n\n    return score\n\ndef evaluate(model, dataset, tokenizer, collator, opt):\n    if opt.eval_metric == 'exact_match':\n        x = evaluate_em(model, dataset, tokenizer, collator, opt)\n        metric = {'exact_match': x}\n    elif opt.eval_metric == 'bleu':\n        metric = evaluate_customized(model, dataset, tokenizer, collator, opt, is_bleu=True)\n    elif opt.eval_metric == 'token_f1':\n        metric = evaluate_customized(model, dataset, tokenizer, collator, opt, is_token_f1=True)\n        metric['token_f1'] = metric.pop('f1')\n    else:\n        raise NotImplementedError(f'{opt.eval_metric} has not been implemented yet')\n    print(json.dumps(metric, indent=2))\n    return metric\n\nif __name__ == \"__main__\":\n    options = Options()\n    options.add_reader_options()\n    options.add_optim_options()\n    opt = options.parse()\n\n    #opt = options.get_options(use_reader=True, use_optim=True)\n\n    torch.manual_seed(opt.seed)\n    src.slurm.init_distributed_mode(opt)\n    src.slurm.init_signal_handler()\n\n    checkpoint_path = Path(opt.checkpoint_dir)/opt.name\n    checkpoint_exists = checkpoint_path.exists()\n    if opt.is_distributed:\n        torch.distributed.barrier()\n    checkpoint_path.mkdir(parents=True, exist_ok=True)\n    opt.checkpoint_path = checkpoint_path\n    #if not checkpoint_exists and opt.is_main:\n    #    options.print_options(opt)\n    #checkpoint_path, checkpoint_exists = util.get_checkpoint_path(opt)\n\n    logger = src.util.init_logger(\n        opt.is_main,\n        opt.is_distributed,\n        checkpoint_path / 'run.log'\n    )\n\n    logger.info(f\"device type: {torch.cuda.get_device_name(0)}, memory: {torch.cuda.get_device_properties(0).total_memory / 1024 / 1024 / 1024}G\")\n\n    # logger.info(json.dumps(vars(opt), indent=2))\n\n    if WANDB_DISABLED:\n        wandb.init(mode='disabled')\n    else:\n        if opt.is_main:\n            # is the master\n            wandb.init(project='fid')\n            wandb.config.update(opt)\n        else:\n            wandb.init(mode='disabled')\n\n    # model_name = 't5-' + opt.model_size\n    model_name = opt.model_name\n    model_class = src.model.FiDT5\n\n    #load data\n    if 'codet5' in model_name or 'code_t5' in model_name:\n        logger.info(f'load the tokenizer from codet5')\n        tokenizer = transformers.RobertaTokenizer.from_pretrained(model_name)\n    else:\n        logger.info(f'load the tokenizer from t5')\n        tokenizer = transformers.T5Tokenizer.from_pretrained(model_name)\n\n    if opt.dataset == 'tldr':\n        special_tokens_dict = {'additional_special_tokens': ['{{', '}}']}\n        num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n\n    collator = src.data.Collator(opt.text_maxlength, tokenizer,\n                                 answer_maxlength=opt.answer_maxlength)\n\n    # use golbal rank and world size to split the eval set on multiple gpus\n    train_examples = src.data.load_data(\n        opt.train_data, \n        global_rank=opt.global_rank, \n        world_size=opt.world_size,\n    )\n    train_dataset = src.data.Dataset(train_examples, opt.n_context)\n    # use golbal rank and world size to split the eval set on multiple gpus\n    eval_examples = src.data.load_data(\n        opt.eval_data,\n        global_rank=opt.global_rank,\n        world_size=opt.world_size,\n    )\n    eval_dataset = src.data.Dataset(eval_examples, opt.n_context)\n\n    if not opt.continue_from_checkpoint:\n        logger.info(\"init a model from T5\")\n        t5 = transformers.T5ForConditionalGeneration.from_pretrained(model_name)\n        t5.resize_token_embeddings(len(tokenizer))\n\n        if opt.encoder_weights is not None:\n            state_dict = torch.load(f'{opt.encoder_weights}/pytorch_model.bin')\n            # rename\n            new_state_dict = {}\n            for k, v in state_dict.items():\n                if k.startswith('_model.encoder.'):\n                    k = k.replace('_model.encoder.', '')\n                new_state_dict[k] = v\n            load_model_keys = list(new_state_dict.keys())\n            model_keys = list(t5.encoder.state_dict().keys())\n            ignored = []\n            missed = []\n            for k in load_model_keys:\n                if k not in model_keys:\n                    ignored.append(k)\n            for k in model_keys:\n                if k not in load_model_keys:\n                    missed.append(k)\n\n            logger.info(f'Some weights in the checkpoint are not used when initializing the encoder : {ignored}')\n            logger.info(f'Some weights in the encoder were not initialized from the checkpoint : {missed}')\n            t5.encoder.load_state_dict(new_state_dict, strict=False)\n            logger.info(f'Loaded encoder weights from {opt.encoder_weights}')\n\n        model = src.model.FiDT5(t5.config)\n        model.load_t5(t5.state_dict())\n        model = model.to(opt.local_rank)\n        optimizer, scheduler = src.util.set_optim(opt, model)\n        step, best_dev_em = 0, 0.0\n    elif opt.model_path == \"none\" and opt.cont_from_checkpoint:\n        load_path = checkpoint_path / 'checkpoint' / 'latest'\n        model, optimizer, scheduler, opt_checkpoint, step, best_dev_em = \\\n            src.util.load(model_class, load_path, opt, reset_params=False)\n        logger.info(f\"Model loaded from checkpoint {load_path}\")\n    else: # load from model path\n        model, optimizer, scheduler, opt_checkpoint, step, best_dev_em = \\\n            src.util.load(model_class, opt.model_path, opt, reset_params=True)\n        logger.info(f\"Model loaded from a model {opt.model_path}\")\n\n    model.set_checkpoint(opt.use_checkpoint)\n\n    if opt.is_distributed:\n        model = torch.nn.parallel.DistributedDataParallel(\n            model,\n            device_ids=[opt.local_rank],\n            output_device=opt.local_rank,\n            find_unused_parameters=False,\n        )\n\n    logger.info(\"Start training\")\n    train(\n        model,\n        optimizer,\n        scheduler,\n        step,\n        train_dataset,\n        eval_dataset,\n        opt,\n        collator,\n        best_dev_em,\n        checkpoint_path\n    )\n"}
{"type": "source_file", "path": "generator/fid/src/preprocess.py", "content": "# Copyright (c) Facebook, Inc. and its affiliates.\n# All rights reserved.\n# \n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport sys\nimport json\nimport parser\nfrom pathlib import Path\nimport numpy as np\nimport util\n\ndef select_examples_TQA(data, index, passages, passages_index):\n    selected_data = []\n    for i, k in enumerate(index):\n        ex = data[k]\n        q = ex['Question']\n        answers = ex['Answer']['Aliases']\n        target = ex['Answer']['Value']\n\n        ctxs = [\n                {                          \n                    'id': idx,\n                    'title': passages[idx][1],\n                    'text': passages[idx][0],\n                }\n                for idx in passages_index[ex['QuestionId']]\n            ]\n\n        if target.isupper():\n            target = target.title()\n        selected_data.append(\n            {\n                'question': q,\n                'answers': answers,\n                'target': target,\n                'ctxs': ctxs,\n            }\n        )\n    return selected_data\n\ndef select_examples_NQ(data, index, passages, passages_index):\n    selected_data = []\n    for i, k in enumerate(index):\n        ctxs = [\n                {\n                    'id': idx,\n                    'title': passages[idx][1],\n                    'text': passages[idx][0],\n                }\n                for idx in passages_index[str(i)]\n            ]\n        dico = {\n            'question': data[k]['question'],\n            'answers': data[k]['answer'],\n            'ctxs': ctxs,\n        }\n        selected_data.append(dico)\n\n    return selected_data\n\nif __name__ == \"__main__\":\n    dir_path = Path(sys.argv[1])\n    save_dir = Path(sys.argv[2])\n\n    passages = util.load_passages(save_dir/'psgs_w100.tsv')\n    passages = {p[0]: (p[1], p[2]) for p in passages}\n\n    #load NQ question idx\n    NQ_idx = {}\n    NQ_passages = {}\n    for split in ['train', 'dev', 'test']:\n        with open(dir_path/('NQ.' + split + '.idx.json'), 'r') as fin:\n            NQ_idx[split] = json.load(fin)\n        with open(dir_path/'nq_passages' /  (split + '.json'), 'r') as fin:\n            NQ_passages[split] = json.load(fin)\n\n\n    originaltrain, originaldev = [], []\n    with open(dir_path/'NQ-open.dev.jsonl') as fin:\n        for k, example in enumerate(fin):\n            example = json.loads(example)\n            originaldev.append(example)\n    \n    with open(dir_path/'NQ-open.train.jsonl') as fin:\n        for k, example in enumerate(fin):\n            example = json.loads(example)\n            originaltrain.append(example)\n\n    NQ_train = select_examples_NQ(originaltrain, NQ_idx['train'], passages, NQ_passages['train'])\n    NQ_dev = select_examples_NQ(originaltrain, NQ_idx['dev'], passages, NQ_passages['dev'])\n    NQ_test = select_examples_NQ(originaldev, NQ_idx['test'], passages, NQ_passages['test'])\n\n    NQ_save_path = save_dir / 'NQ'\n    NQ_save_path.mkdir(parents=True, exist_ok=True)\n\n    with open(NQ_save_path/'train.json', 'w') as fout:\n        json.dump(NQ_train, fout, indent=4)\n    with open(NQ_save_path/'dev.json', 'w') as fout:\n        json.dump(NQ_dev, fout, indent=4)\n    with open(NQ_save_path/'test.json', 'w') as fout:\n        json.dump(NQ_test, fout, indent=4)\n\n    #load Trivia question idx\n    TQA_idx, TQA_passages = {}, {}\n    for split in ['train', 'dev', 'test']:\n        with open(dir_path/('TQA.' + split + '.idx.json'), 'r') as fin:\n            TQA_idx[split] = json.load(fin)\n        with open(dir_path/'tqa_passages' /  (split + '.json'), 'r') as fin:\n            TQA_passages[split] = json.load(fin)\n\n\n    originaltrain, originaldev = [], []\n    with open(dir_path/'triviaqa-unfiltered'/'unfiltered-web-train.json') as fin:\n        originaltrain = json.load(fin)['Data']\n    \n    with open(dir_path/'triviaqa-unfiltered'/'unfiltered-web-dev.json') as fin:\n        originaldev = json.load(fin)['Data']\n\n    TQA_train = select_examples_TQA(originaltrain, TQA_idx['train'], passages, TQA_passages['train'])\n    TQA_dev = select_examples_TQA(originaltrain, TQA_idx['dev'], passages, TQA_passages['dev'])\n    TQA_test = select_examples_TQA(originaldev, TQA_idx['test'], passages, TQA_passages['test'])\n   \n    TQA_save_path = save_dir / 'TQA'\n    TQA_save_path.mkdir(parents=True, exist_ok=True)\n\n    with open(TQA_save_path/'train.json', 'w') as fout:\n        json.dump(TQA_train, fout, indent=4)\n    with open(TQA_save_path/'dev.json', 'w') as fout:\n        json.dump(TQA_dev, fout, indent=4)\n    with open(TQA_save_path/'test.json', 'w') as fout:\n        json.dump(TQA_test, fout, indent=4)\n"}
{"type": "source_file", "path": "generator/fid/src/options.py", "content": "# Copyright (c) Facebook, Inc. and its affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport argparse\nimport os\nfrom pathlib import Path\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nclass Options():\n    def __init__(self):\n        self.parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n        self.initialize_parser()\n\n    def add_optim_options(self):\n        self.parser.add_argument('--warmup_steps', type=int, default=1000)\n        self.parser.add_argument('--total_steps', type=int, default=10000)\n        self.parser.add_argument('--scheduler_steps', type=int, default=None, \n                        help='total number of step for the scheduler, if None then scheduler_total_step = total_step')\n        self.parser.add_argument('--accumulation_steps', type=int, default=1)\n        self.parser.add_argument('--dropout', type=float, default=0.1, help='dropout rate')\n        self.parser.add_argument('--lr', type=float, default= 0.00005, help='learning rate')\n        self.parser.add_argument('--clip', type=float, default=1., help='gradient clipping')\n        self.parser.add_argument('--optim', type=str, default='adamw')\n        self.parser.add_argument('--scheduler', type=str, default='linear')\n        self.parser.add_argument('--weight_decay', type=float, default=0.01)\n        self.parser.add_argument('--fixed_lr', action='store_true')\n\n    def add_eval_options(self):\n        self.parser.add_argument('--write_results', action='store_true', help='save results')\n        self.parser.add_argument('--write_crossattention_scores', action='store_true', \n                        help='save dataset with cross-attention scores')\n        self.parser.add_argument('--use_softmax', help='use softmax instead of logits as attention score', action='store_true')\n        self.parser.add_argument('--tokenizer_name', choices=('models/generator/codet5-base', 't5-base', 't5-large'), default='data/fid/codet5-base')\n        self.parser.add_argument('--result_tag', type=str, help='the evaluation setting')\n        self.parser.add_argument('--lenpen', type=float, default=1.0, help='length penalty')\n        self.parser.add_argument('--num_beams', type=int, default=10)\n        self.parser.add_argument('--num_return_sequences', type=int, default=1, help='number of return sequences')\n        self.parser.add_argument('--temperature', type=float, default=0.8, help='temperature for sampling')\n        self.parser.add_argument('--top_p', type=float, default=0.9, help='top_p for nucleus sampling')\n\n    def add_reader_options(self):\n        self.parser.add_argument('--train_data', type=str, default='none', help='path of train data')\n        self.parser.add_argument('--eval_data', type=str, default='none', help='path of eval data')\n        self.parser.add_argument('--model_size', type=str, default='base')\n        self.parser.add_argument('--model_name', type=str, default='t5-base')\n        self.parser.add_argument('--use_checkpoint', action='store_true', help='use checkpoint in the encoder')\n        self.parser.add_argument('--text_maxlength', type=int, default=200,\n                        help='maximum number of tokens in text segments (question+passage)')\n        self.parser.add_argument('--answer_maxlength', type=int, default=-1, \n                        help='maximum number of tokens used to train the model, no truncation if -1')\n        self.parser.add_argument('--no_title', action='store_true', \n                        help='article titles not included in passages')\n        self.parser.add_argument('--n_context', type=int, default=1)\n        self.parser.add_argument('--encoder_weights', type=str, default=None, help='path of encoder weight')\n        self.parser.add_argument('--dataset', choices=('conala', 'tldr'), default='conala')\n\n    def add_retriever_options(self):\n        self.parser.add_argument('--train_data', type=str, default='none', help='path of train data')\n        self.parser.add_argument('--eval_data', type=str, default='none', help='path of eval data')\n        self.parser.add_argument('--indexing_dimension', type=int, default=768)\n        self.parser.add_argument('--no_projection', action='store_true', \n                        help='No addition Linear layer and layernorm, only works if indexing size equals 768')\n        self.parser.add_argument('--question_maxlength', type=int, default=40, \n                        help='maximum number of tokens in questions')\n        self.parser.add_argument('--passage_maxlength', type=int, default=200, \n                        help='maximum number of tokens in passages')\n        self.parser.add_argument('--no_question_mask', action='store_true')\n        self.parser.add_argument('--no_passage_mask', action='store_true')\n        self.parser.add_argument('--extract_cls', action='store_true')\n        self.parser.add_argument('--no_title', action='store_true', \n                        help='article titles not included in passages')\n        self.parser.add_argument('--n_context', type=int, default=1)\n\n\n    def initialize_parser(self):\n        # basic parameters\n        self.parser.add_argument('--name', type=str, default='experiment_name', help='name of the experiment')\n        self.parser.add_argument('--checkpoint_dir', type=str, default='./checkpoint/', help='models are saved here')\n        self.parser.add_argument('--model_path', type=str, default='none', help='path for retraining')\n        self.parser.add_argument('--continue_from_checkpoint', action='store_true')\n\n        # dataset parameters\n        self.parser.add_argument(\"--per_gpu_batch_size\", default=1, type=int, \n                        help=\"Batch size per GPU/CPU for training.\")\n        self.parser.add_argument('--maxload', type=int, default=-1)\n\n        self.parser.add_argument(\"--local_rank\", type=int, default=-1,\n                        help=\"For distributed training: local_rank\")\n        self.parser.add_argument(\"--main_port\", type=int, default=-1,\n                        help=\"Main port (for multi-node SLURM jobs)\")\n        self.parser.add_argument('--seed', type=int, default=0, help=\"random seed for initialization\")\n        # training parameters\n        self.parser.add_argument('--eval_freq', type=int, default=500,\n                        help='evaluate model every <eval_freq> steps during training')\n        self.parser.add_argument('--save_freq', type=int, default=5000,\n                        help='save model every <save_freq> steps during training')\n        self.parser.add_argument('--eval_print_freq', type=int, default=1000,\n                        help='print intermdiate results of evaluation every <eval_print_freq> steps')\n\n        self.parser.add_argument('--eval_metric', choices=('exact_match', 'bleu', 'token_f1'), default='bleu')\n\n\n    def print_options(self, opt):\n        message = '\\n'\n        for k, v in sorted(vars(opt).items()):\n            comment = ''\n            default_value = self.parser.get_default(k)\n            if v != default_value:\n                comment = f'\\t(default: {default_value})'\n            message += f'{str(k):>30}: {str(v):<40}{comment}\\n'\n\n        expr_dir = Path(opt.checkpoint_dir)/ opt.name\n        model_dir = expr_dir / 'models'\n        model_dir.mkdir(parents=True, exist_ok=True)\n        with open(expr_dir/'opt.log', 'wt') as opt_file:\n            opt_file.write(message)\n            opt_file.write('\\n')\n\n        logger.info(message)\n\n    def parse(self):\n        opt = self.parser.parse_args()\n        return opt\n\n\ndef get_options(use_reader=False,\n                use_retriever=False,\n                use_optim=False,\n                use_eval=False):\n    options = Options()\n    if use_reader:\n        options.add_reader_options()\n    if use_retriever:\n        options.add_retriever_options()\n    if use_optim:\n        options.add_optim_options()\n    if use_eval:\n        options.add_eval_options()\n    return options.parse()\n"}
{"type": "source_file", "path": "retriever/bm25/indexer.py", "content": "import json\nfrom elasticsearch import Elasticsearch\nfrom elasticsearch.helpers import bulk\nfrom tqdm import tqdm\nimport logging\n\nlogging.getLogger('elasticsearch').setLevel(logging.ERROR)\n\nclass ESSearch:\n    def __init__(self, index: str, source: str,\n                 host_address: str='localhost',\n                 re_index=False,\n                 manual_path=None,\n                 func_descpt_path=None):\n        self.es = Elasticsearch(timeout=60, host=host_address)\n        self.source = source\n        self.index = f\"{index}.{source}\"\n        self.manual_path = manual_path\n        self.func_descpt_path = func_descpt_path\n\n        if re_index:\n            self.es.indices.delete(index=self.index, ignore=[400, 404])\n            print(f\"delete {self.index}\")\n            self.es.indices.create(index=self.index)\n            # print(self.es.indices.get_alias().keys())\n            self.create_index()\n\n        print(f\"done init the index {self.index}\")\n        self.es.indices.refresh(self.index)\n        print(self.es.cat.count(self.index, params={\"format\": \"json\"}))\n\n    def gendata(self):\n        descpt_d = None\n        if self.func_descpt_path:\n            with open(self.func_descpt_path, \"r\") as f:\n                descpt_d = json.load(f)\n\n        with open(self.manual_path, \"r\") as f:\n            man_d = json.load(f)\n\n        for lib_key, lib_man in tqdm(man_d.items()):\n            cmd_name = '_'.join(lib_key.split(\"_\")[:-1]) if lib_key[-1].isdigit() else lib_key\n            descpt = descpt_d[lib_key] if descpt_d is not None else \"\"\n            result = {\n                '_index': self.index,\n                '_type': \"_doc\",\n                'manual': lib_man,\n                'func_description': descpt,\n                'library_key': lib_key,\n                'cmd_name': cmd_name\n            }\n            yield result\n\n    def create_index(self):\n        all_docs = list(self.gendata())\n        print(bulk(self.es, all_docs, index=self.index))\n\n\n    def get_topk(self, search_field, query, topk):\n        real_query = {'query': {'match': {search_field: query}},\n                      'size': topk + 10}\n        r_mans = self.es.search(index=self.index, body=real_query)['hits']['hits'][:topk]\n        _r_mans = []\n        for r in r_mans:\n            i = {'library_key': r['_source']['library_key'], 'score': r['_score']}\n            _r_mans.append(i)\n        r_mans = _r_mans\n        return r_mans\n\n\n"}
{"type": "source_file", "path": "generator/fid/utils/save_codet5.py", "content": "import json\nimport shutil\nimport transformers\nassert transformers.__version__ == '4.11.3', (transformers.__version__)\n\ntokenizer = transformers.RobertaTokenizer.from_pretrained('Salesforce/codet5-base')\ntokenizer.save_pretrained('data/fid/codet5-base')\n\nwith open('data/fid/codet5-base/tokenizer_config.json', \"w+\") as f:\n    json.dump({\"model_max_length\": 512}, f)\n\nwith open('data/fid/codet5-base/special_tokens_map.json', 'r') as f:\n    d = json.load(f)\n    d['additional_special_tokens'] = [x['content'] for x in d['additional_special_tokens']]\n    # add_tokens = d.pop('additional_special_tokens')\n    # for item in add_tokens:\n    #     d[item['content']] = item\n\nshutil.move('data/fid/codet5-base/special_tokens_map.json', 'data/fid/codet5-base/special_tokens_map.json.bck')\nwith open('data/fid/codet5-base/special_tokens_map.json', 'w+') as f:\n    json.dump(d, f, indent=2)\n\n\nprint('save tokenizer')\n\nt5 = transformers.T5ForConditionalGeneration.from_pretrained('Salesforce/codet5-base')\nt5.save_pretrained('data/fid/codet5-base')\n\nprint('save model')"}
{"type": "source_file", "path": "generator/fid/src/slurm.py", "content": "# Copyright (c) Facebook, Inc. and its affiliates.\n# All rights reserved.\n# \n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom logging import getLogger\nimport os\nimport sys\nimport torch\nimport socket\nimport signal\nimport subprocess\n\n\nlogger = getLogger()\n\ndef sig_handler(signum, frame):\n    logger.warning(\"Signal handler called with signal \" + str(signum))\n    prod_id = int(os.environ['SLURM_PROCID'])\n    logger.warning(\"Host: %s - Global rank: %i\" % (socket.gethostname(), prod_id))\n    if prod_id == 0:\n        logger.warning(\"Requeuing job \" + os.environ['SLURM_JOB_ID'])\n        os.system('scontrol requeue ' + os.environ['SLURM_JOB_ID'])\n    else:\n        logger.warning(\"Not the main process, no need to requeue.\")\n    sys.exit(-1)\n\n\ndef term_handler(signum, frame):\n    logger.warning(\"Signal handler called with signal \" + str(signum))\n    logger.warning(\"Bypassing SIGTERM.\")\n\n\ndef init_signal_handler():\n    \"\"\"\n    Handle signals sent by SLURM for time limit / pre-emption.\n    \"\"\"\n    signal.signal(signal.SIGUSR1, sig_handler)\n    signal.signal(signal.SIGTERM, term_handler)\n    #logger.warning(\"Signal handler installed.\")\n\n\ndef init_distributed_mode(params):\n    \"\"\"\n    Handle single and multi-GPU / multi-node / SLURM jobs.\n    Initialize the following variables:\n        - n_nodes\n        - node_id\n        - local_rank\n        - global_rank\n        - world_size\n    \"\"\"\n    params.is_slurm_job = 'SLURM_JOB_ID' in os.environ \n    has_local_rank = hasattr(params, 'local_rank')\n\n    # SLURM job\n    if params.is_slurm_job and has_local_rank:\n\n        assert params.local_rank == -1   # on the cluster, this is handled by SLURM\n\n        SLURM_VARIABLES = [\n            'SLURM_JOB_ID',\n            'SLURM_JOB_NODELIST', 'SLURM_JOB_NUM_NODES', 'SLURM_NTASKS', 'SLURM_TASKS_PER_NODE',\n            'SLURM_MEM_PER_NODE', 'SLURM_MEM_PER_CPU',\n            'SLURM_NODEID', 'SLURM_PROCID', 'SLURM_LOCALID', 'SLURM_TASK_PID'\n        ]\n\n        PREFIX = \"%i - \" % int(os.environ['SLURM_PROCID'])\n        for name in SLURM_VARIABLES:\n            value = os.environ.get(name, None)\n            #print(PREFIX + \"%s: %s\" % (name, str(value)))\n\n        # # job ID\n        # params.job_id = os.environ['SLURM_JOB_ID']\n\n        # number of nodes / node ID\n        params.n_nodes = int(os.environ['SLURM_JOB_NUM_NODES'])\n        params.node_id = int(os.environ['SLURM_NODEID'])\n\n        # local rank on the current node / global rank\n        params.local_rank = int(os.environ['SLURM_LOCALID'])\n        params.global_rank = int(os.environ['SLURM_PROCID'])\n\n        # number of processes / GPUs per node\n        params.world_size = int(os.environ['SLURM_NTASKS'])\n        params.n_gpu_per_node = params.world_size // params.n_nodes\n\n        # define master address and master port\n        hostnames = subprocess.check_output(['scontrol', 'show', 'hostnames', os.environ['SLURM_JOB_NODELIST']])\n        params.main_addr = hostnames.split()[0].decode('utf-8')\n        assert 10001 <= params.main_port <= 20000 or params.world_size == 1\n        #print(PREFIX + \"Master address: %s\" % params.master_addr)\n        #print(PREFIX + \"Master port   : %i\" % params.master_port)\n\n        # set environment variables for 'env://'\n        os.environ['MASTER_ADDR'] = params.main_addr\n        os.environ['MASTER_PORT'] = str(params.main_port)\n        os.environ['WORLD_SIZE'] = str(params.world_size)\n        os.environ['RANK'] = str(params.global_rank)\n        params.is_distributed = True\n\n\n    # multi-GPU job (local or multi-node) - jobs started with torch.distributed.launch\n    elif has_local_rank and params.local_rank != -1:\n\n        assert params.main_port == -1\n\n        # read environment variables\n        params.global_rank = int(os.environ['RANK'])\n        params.world_size = int(os.environ['WORLD_SIZE'])\n        params.n_gpu_per_node = int(os.environ['NGPU'])\n\n        # number of nodes / node ID\n        params.n_nodes = params.world_size // params.n_gpu_per_node\n        params.node_id = params.global_rank // params.n_gpu_per_node\n        params.is_distributed = True\n\n    else:\n        n_gpu = torch.cuda.device_count()\n        params.n_nodes = 1\n        params.node_id = 0\n        params.local_rank = 0\n        params.global_rank = 0\n        params.world_size = n_gpu\n        params.n_gpu_per_node = n_gpu\n        params.is_distributed = False\n\n    # define whether this is the master process / if we are in distributed mode\n    params.is_main = params.node_id == 0 and params.local_rank == 0\n    params.multi_node = params.n_nodes > 1\n    params.multi_gpu = params.world_size > 1\n\n    # summary\n    PREFIX = \"%i - \" % params.global_rank\n\n    # set GPU device\n    if params.is_distributed:\n        torch.cuda.set_device(params.local_rank)\n        device = torch.device(\"cuda\", params.local_rank)\n    else:\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    params.device = device\n\n    # summary\n    PREFIX = \"%i - \" % params.global_rank\n    print(PREFIX + \"Number of nodes: %i\" % params.n_nodes)\n    print(PREFIX + \"Node ID        : %i\" % params.node_id)\n    print(PREFIX + \"Local rank     : %i\" % params.local_rank)\n    print(PREFIX + \"Global rank    : %i\" % params.global_rank)\n    print(PREFIX + \"World size     : %i\" % params.world_size)\n    print(PREFIX + \"GPUs per node  : %i\" % params.n_gpu_per_node)\n    print(PREFIX + \"Multi-node     : %s\" % str(params.multi_node))\n    print(PREFIX + \"Multi-GPU      : %s\" % str(params.multi_gpu))\n    print(PREFIX + \"Hostname       : %s\" % socket.gethostname())\n\n    # initialize multi-GPU\n    if params.is_distributed:\n\n        # http://pytorch.apachecn.org/en/0.3.0/distributed.html#environment-variable-initialization\n        # 'env://' will read these environment variables:\n        # MASTER_PORT - required; has to be a free port on machine with rank 0\n        # MASTER_ADDR - required (except for rank 0); address of rank 0 node\n        # WORLD_SIZE - required; can be set either here, or in a call to init function\n        # RANK - required; can be set either here, or in a call to init function\n\n        #print(\"Initializing PyTorch distributed ...\")\n        torch.distributed.init_process_group(\n            init_method='env://',\n            backend='nccl',\n        )"}
{"type": "source_file", "path": "generator/fid/src/util.py", "content": "# Copyright (c) Facebook, Inc. and its affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport os\nimport errno\nimport torch\nimport sys\nimport logging\nimport json\nfrom pathlib import Path\nimport torch.distributed as dist\nimport csv\n\nlogger = logging.getLogger(__name__)\n\ndef init_logger(is_main=True, is_distributed=False, filename=None):\n    if is_distributed:\n        torch.distributed.barrier()\n    handlers = [logging.StreamHandler(sys.stdout)]\n    if filename is not None:\n        handlers.append(logging.FileHandler(filename=filename))\n    logging.basicConfig(\n        datefmt=\"%m/%d/%Y %H:%M:%S\",\n        level=logging.INFO if is_main else logging.WARN,\n        format=\"[%(asctime)s] {%(filename)s:%(lineno)d} %(levelname)s - %(message)s\",\n        handlers=handlers,\n    )\n    logging.getLogger('transformers.tokenization_utils').setLevel(logging.ERROR)\n    logging.getLogger('transformers.tokenization_utils_base').setLevel(logging.ERROR)\n    return logger\n\ndef get_checkpoint_path(opt):\n    checkpoint_path = Path(opt.checkpoint_dir) / opt.name\n    checkpoint_exists = checkpoint_path.exists()\n    if opt.is_distributed:\n        torch.distributed.barrier()\n    checkpoint_path.mkdir(parents=True, exist_ok=True)\n    return checkpoint_path, checkpoint_exists\n\ndef symlink_force(target, link_name):\n    try:\n        os.symlink(target, link_name)\n    except OSError as e:\n        if e.errno == errno.EEXIST:\n            os.remove(link_name)\n            os.symlink(target, link_name)\n        else:\n            raise e\n\ndef save(model, optimizer, scheduler, step, best_eval_metric, opt, dir_path, name):\n    model_to_save = model.module if hasattr(model, \"module\") else model\n    path = os.path.join(dir_path, \"checkpoint\")\n    epoch_path = os.path.join(path, name) #\"step-%s\" % step)\n    os.makedirs(epoch_path, exist_ok=True)\n    model_to_save.save_pretrained(epoch_path)\n    cp = os.path.join(path, \"latest\")\n    fp = os.path.join(epoch_path, \"optimizer.pth.tar\")\n    checkpoint = {\n        \"step\": step,\n        \"optimizer\": optimizer.state_dict(),\n        \"scheduler\": scheduler.state_dict(),\n        \"opt\": opt,\n        \"best_eval_metric\": best_eval_metric,\n    }\n    torch.save(checkpoint, fp)\n    symlink_force(epoch_path, cp)\n\n\ndef load(model_class, dir_path, opt, reset_params=False):\n    epoch_path = os.path.realpath(dir_path)\n    optimizer_path = os.path.join(epoch_path, \"optimizer.pth.tar\")\n    logger.info(\"Loading %s\" % epoch_path)\n    model = model_class.from_pretrained(epoch_path)\n    model = model.to(opt.device)\n    logger.info(\"loading checkpoint %s\" %optimizer_path)\n    checkpoint = torch.load(optimizer_path, map_location=opt.device)\n    opt_checkpoint = checkpoint[\"opt\"]\n    step = checkpoint[\"step\"]\n    if \"best_eval_metric\" in checkpoint:\n        best_eval_metric = checkpoint[\"best_eval_metric\"]\n    else:\n        best_eval_metric = checkpoint[\"best_dev_em\"]\n    if not reset_params:\n        optimizer, scheduler = set_optim(opt_checkpoint, model)\n        scheduler.load_state_dict(checkpoint[\"scheduler\"])\n        optimizer.load_state_dict(checkpoint[\"optimizer\"])\n    else:\n        optimizer, scheduler = set_optim(opt, model)\n\n    return model, optimizer, scheduler, opt_checkpoint, step, best_eval_metric\n\nclass WarmupLinearScheduler(torch.optim.lr_scheduler.LambdaLR):\n    def __init__(self, optimizer, warmup_steps, scheduler_steps, min_ratio, fixed_lr, last_epoch=-1):\n        self.warmup_steps = warmup_steps\n        self.scheduler_steps = scheduler_steps\n        self.min_ratio = min_ratio\n        self.fixed_lr = fixed_lr\n        super(WarmupLinearScheduler, self).__init__(\n            optimizer, self.lr_lambda, last_epoch=last_epoch\n        )\n\n    def lr_lambda(self, step):\n        if step < self.warmup_steps:\n            return (1 - self.min_ratio)*step/float(max(1, self.warmup_steps)) + self.min_ratio\n\n        if self.fixed_lr:\n            return 1.0\n\n        return max(0.0,\n            1.0 + (self.min_ratio - 1) * (step - self.warmup_steps)/float(max(1.0, self.scheduler_steps - self.warmup_steps)),\n        )\n\n\nclass FixedScheduler(torch.optim.lr_scheduler.LambdaLR):\n    def __init__(self, optimizer, last_epoch=-1):\n        super(FixedScheduler, self).__init__(optimizer, self.lr_lambda, last_epoch=last_epoch)\n    def lr_lambda(self, step):\n        return 1.0\n\n\ndef set_dropout(model, dropout_rate):\n    for mod in model.modules():\n        if isinstance(mod, torch.nn.Dropout):\n            mod.p = dropout_rate\n\n\ndef set_optim(opt, model):\n    if opt.optim == 'adam':\n        optimizer = torch.optim.Adam(model.parameters(), lr=opt.lr)\n    elif opt.optim == 'adamw':\n        optimizer = torch.optim.AdamW(model.parameters(), lr=opt.lr, weight_decay=opt.weight_decay)\n    if opt.scheduler == 'fixed':\n        scheduler = FixedScheduler(optimizer)\n    elif opt.scheduler == 'linear':\n        if opt.scheduler_steps is None:\n            scheduler_steps = opt.total_steps\n        else:\n            scheduler_steps = opt.scheduler_steps\n        scheduler = WarmupLinearScheduler(optimizer, warmup_steps=opt.warmup_steps, scheduler_steps=scheduler_steps, min_ratio=0., fixed_lr=opt.fixed_lr)\n    return optimizer, scheduler\n\n\ndef average_main(x, opt):\n    if not opt.is_distributed:\n        return x\n    if opt.world_size > 1:\n        dist.reduce(x, 0, op=dist.ReduceOp.SUM)\n        if opt.is_main:\n            x = x / opt.world_size\n    return x\n\n\ndef sum_main(x, opt):\n    if not opt.is_distributed:\n        return x\n    if opt.world_size > 1:\n        dist.reduce(x, 0, op=dist.ReduceOp.SUM)\n    return x\n\n\ndef weighted_average(x, count, opt):\n    if not opt.is_distributed:\n        return x, count\n    t_loss = torch.tensor([x * count], device=opt.device)\n    t_total = torch.tensor([count], device=opt.device)\n    t_loss = sum_main(t_loss, opt)\n    t_total = sum_main(t_total, opt)\n    return (t_loss / t_total).item(), t_total.item()\n\n\ndef write_output(glob_path, output_path):\n    files = list(glob_path.glob('*.txt'))\n    files.sort()\n    with open(output_path, 'w') as outfile:\n        for path in files:\n            with open(path, 'r') as f:\n                lines = f.readlines()\n                for line in lines:\n                    outfile.write(line)\n            path.unlink()\n    glob_path.rmdir()\n\n\ndef save_distributed_dataset(data, opt):\n    dir_path = Path(opt.checkpoint_dir) / opt.name\n    write_path = dir_path / 'tmp_dir'\n    write_path.mkdir(exist_ok=True)\n    tmp_path = write_path / f'{opt.global_rank}.json'\n    with open(tmp_path, 'w') as fw:\n        json.dump(data, fw)\n    if opt.is_distributed:\n        torch.distributed.barrier()\n    if opt.is_main:\n        final_path = dir_path / f'attention_score.{opt.result_tag}.json'\n        logger.info(f'Writing dataset with scores at {final_path}')\n        glob_path = write_path / '*'\n        results_path = write_path.glob('*.json')\n        alldata = []\n        for path in results_path:\n            with open(path, 'r') as f:\n                data = json.load(f)\n            alldata.extend(data)\n            path.unlink()\n        with open(final_path, 'w') as fout:\n            json.dump(alldata, fout, indent=4)\n        write_path.rmdir()\n\ndef load_passages(path):\n    if not os.path.exists(path):\n        logger.info(f'{path} does not exist')\n        return\n    logger.info(f'Loading passages from: {path}')\n    passages = []\n    with open(path) as fin:\n        reader = csv.reader(fin, delimiter='\\t')\n        for k, row in enumerate(reader):\n            if not row[0] == 'id':\n                try:\n                    passages.append((row[0], row[1], row[2]))\n                except:\n                    logger.warning(f'The following input line has not been correctly loaded: {row}')\n    return passages"}
{"type": "source_file", "path": "retriever/bm25/main.py", "content": "import itertools\nimport json\nimport os.path\n\nfrom tqdm import tqdm\nfrom collections import defaultdict, OrderedDict\nfrom indexer import ESSearch\nfrom retriever.eval import calc_recall, calc_hit, eval_retrieval_from_file\nimport argparse\n\ndef retrieve_manual(test_file, source, index, host, search_conf, saved_file, top_k=10):\n    indexer = ESSearch(index,\n                       source,\n                       host_address=host,\n                       re_index=False)\n\n    query_type, search_field = search_conf['query'], search_conf['field']\n    tag = f\"{index}.{query_type}.{search_field}\"\n    with open(test_file, \"r\") as f:\n        d = json.load(f)\n    results = []\n    for item in tqdm(d):\n        query = item[query_type]\n        try:\n            r_mans = indexer.get_topk(search_field, query, topk=top_k)\n\n            results.append({**item,\n                            f'{tag}.retrieved': [x['library_key'] for x in r_mans],\n                            f'{tag}.score': [x.score if not isinstance(x, dict) else x['score'] for x in r_mans]})\n        except Exception as e:\n            print(repr(e))\n            results.append({**item,\n                            f'{tag}.retrieved': [],\n                            f'{tag}.score': []})\n\n    # metrics = calc_recall(results)\n    saved_file = saved_file.replace(\".json\", f\".{tag}.json\")\n    print(f\"save to {saved_file}\")\n    with open(saved_file, \"w+\") as f:\n        json.dump(results, f, indent=2)\n\n    return saved_file\n\n\ndef doc_base_retrieval(index, source, host, r1_result_file,\n                       retrieval_entry, saved_file, top_k_doc,\n                       top_k_result, oracle_only=False):\n    indexer = ESSearch(index, source, host, re_index=False)\n\n    print(f\"index: {index}\")\n    print(f\"r1 file: {r1_result_file}\")\n    print(f\"r2 file: {saved_file}\")\n\n    with open(r1_result_file, \"r\") as f:\n        r1_result = json.load(f)\n\n    split_flag = False\n    r0 = r1_result[0][retrieval_entry][0]\n    if r0.split(\"_\")[-1].isdigit():\n        split_flag = True\n\n    r2_result = []\n\n    for item in tqdm(r1_result):\n        if oracle_only:\n            if split_flag:\n                pred_cmd = [\"_\".join(item['cmd_name'].split(\"_\")[:-1])]\n            else:\n                pred_cmd = [item['cmd_name']]\n        else:\n            pred_cmd = []\n            for pred in item[retrieval_entry]:\n                cmd_name = pred\n                if split_flag:\n                    cmd_name = \"_\".join(pred.split(\"_\")[:-1])\n                pred_cmd.append(cmd_name)\n            pred_cmd = list(OrderedDict.fromkeys(pred_cmd))\n\n        for cmd_idx, cmd in enumerate(pred_cmd[:top_k_doc]):\n            item_r2 = item.copy()\n            item_r2['parent_cmd'] = cmd\n            item_r2['question_id'] = f\"{item['question_id']}-{cmd_idx}\"\n            item_r2.pop(retrieval_entry)\n            item_r2.pop(retrieval_entry.replace(\".retrieved\", \".score\"))\n            try:\n                real_query = {\n                    \"query\": {\n                        \"bool\": {\n                            \"must\": [\n                                {\"term\": {'cmd_name.keyword': cmd}},\n                                {\"match\": {'manual': item['nl']}}\n                            ]\n                        }\n                    },\n                    \"size\": top_k_result\n                }\n                r_mans = indexer.es.search(index=indexer.index, body=real_query)['hits']['hits'][:top_k_result]\n                _r_mans = []\n                for r in r_mans:\n                    i = {'library_key': r['_source']['library_key'], 'score': r['_score']}\n                    _r_mans.append(i)\n                r_mans = _r_mans\n\n\n                r2_result.append({**item_r2,\n                                retrieval_entry: [x['library_key'] for x in r_mans],\n                                retrieval_entry.replace(\".retrieved\", \".score\"): [x.score if not isinstance(x, dict) else x['score'] for x in r_mans]})\n\n            except Exception as e:\n                print(repr(e))\n                r2_result.append({**item_r2,\n                                retrieval_entry: [],\n                                retrieval_entry.replace(\".retrieved\", \".score\"): []})\n\n\n    print(f\"size of the results: {len(r2_result)}\")\n    with open(saved_file, \"w+\") as f:\n        json.dump(r2_result, f, indent=2)\n\n\ndef config():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--retrieval_stage', type=int, choices=(0, 1, 2),\n                        help='which retrieval stage to run for tldr'\n                        'stage 0: build retrieval index'\n                        'stage 1: stage 1 retrieval that retrieves the bash command'\n                        'stage 2: stage 2 retrieval that retrieves the paragraphs')\n    parser.add_argument('--split', type=str,\n                        choices=('cmd_train', 'cmd_dev', 'cmd_test'),\n                        default='cmd_dev',\n                        help='which data split to run')\n\n    parser.add_argument('--host', type=str, default='localhost')\n\n    args = parser.parse_args()\n    return args\n\nif __name__ == \"__main__\":\n    args = config()\n    stage = args.retrieval_stage\n    split = args.split\n    host = args.host\n    if stage == 0: # build the index\n        index = \"bash_man_whole\"\n        source = \"chunk\"\n        _ = ESSearch(index, source, host_address=host,\n                     re_index=True, manual_path='data/tldr/manual_all_raw.json',\n                     func_descpt_path=None)\n\n        index = \"bash_man_para\"\n        source = \"chunk\"\n        indexer = ESSearch(index, source, host_address=host,\n                           re_index=True, manual_path='data/tldr/manual_section.json',\n                           func_descpt_path=None)\n\n    if stage == 1:\n        index = 'bash_man_whole' # in the first stage, use the whole bash manual to retrieve the bash commands\n        source = \"chunk\"\n        search_config_1 = {'query': 'nl', 'field': 'manual', 'filter_result': False}\n\n        print(split, index)\n        data_file = f\"./data/tldr/{split}.seed.json\"\n        save_file = data_file.replace(\".seed.json\", f\".full.json\")\n        query_type, search_field = search_config_1['query'], search_config_1['field']\n        tag = f\"{index}.{query_type}.{search_field}\"\n        real_save_file = save_file.replace(\".json\", f\".{tag}.json\")\n\n        if not os.path.exists(real_save_file):\n            _ = retrieve_manual(data_file, source, index, host, search_config_1, save_file, top_k=35)\n\n        with open(real_save_file, 'r') as f:\n            d = json.load(f)\n\n        src = []\n        pred = []\n        for item in d:\n            src.append(item['cmd_name'])\n            pred.append(item[f'{tag}.retrieved'])\n\n        calc_hit(src, pred, top_k=[1, 3, 5, 10, 15, 20, 30])\n\n    if stage == 2:\n        source = \"chunk\"\n        r1_index = 'bash_man_whole'\n        r2_index = 'bash_man_para' # in the second stage, use paragraphs to retrieve descriptions of relevant arguments etc\n        search_config_1 = {'query': 'nl', 'field': 'manual', 'filter_result': False}\n\n        data_file = f\"./data/tldr/{split}.seed.json\"\n        query_type, search_field = search_config_1['query'], search_config_1['field']\n        r1_tag = f\"{r1_index}.{query_type}.{search_field}\"\n        r1_save_file = data_file.replace(\".seed.json\", f\".full.{r1_tag}.json\")\n\n        r2_save_file = r1_save_file.replace(\".json\", f\".r2-{r2_index}.json\")\n\n        if not os.path.exists(r2_save_file):\n            _ = doc_base_retrieval(r2_index,\n                                   source,\n                                   host,\n                                   r1_save_file,\n                                   f'{r1_tag}.retrieved',\n                                   r2_save_file,\n                                   top_k_doc=5,\n                                   top_k_result=30,\n                                   oracle_only=False)\n\n"}
{"type": "source_file", "path": "retriever/simcse/arguments.py", "content": "import logging\nfrom dataclasses import dataclass, field\nfrom typing import Optional, Union, List, Dict, Tuple\nimport torch\nfrom transformers import (\n    MODEL_FOR_MASKED_LM_MAPPING,\n    TrainingArguments,\n)\nfrom transformers.file_utils import cached_property, torch_required, is_torch_available, is_torch_tpu_available\n\nlogger = logging.getLogger(__name__)\nMODEL_CONFIG_CLASSES = list(MODEL_FOR_MASKED_LM_MAPPING.keys())\nMODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)\n\n@dataclass\nclass ModelArguments:\n    \"\"\"\n    Arguments pertaining to which model/config/tokenizer we are going to fine-tune, or train from scratch.\n    \"\"\"\n\n    # Huggingface's original arguments\n    model_name_or_path: Optional[str] = field(\n        default=None,\n        metadata={\n            \"help\": \"The model checkpoint for weights initialization.\"\n                    \"Don't set if you want to train a model from scratch.\"\n        },\n    )\n\n    mlp_weight_path: Optional[str] = field(\n        default=None,\n        metadata={\n            \"help\": \"mlp weight path\"\n        },\n    )\n\n    model_type: Optional[str] = field(\n        default=None,\n        metadata={\"help\": \"If training from scratch, pass a model type from the list: \" + \", \".join(MODEL_TYPES)},\n    )\n    config_name: Optional[str] = field(\n        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n    )\n    tokenizer_name: Optional[str] = field(\n        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n    )\n    cache_dir: Optional[str] = field(\n        default=None,\n        metadata={\"help\": \"Where do you want to store the pretrained models downloaded from huggingface.co\"},\n    )\n    use_fast_tokenizer: bool = field(\n        default=True,\n        metadata={\"help\": \"Whether to use one of the fast tokenizer (backed by the tokenizers library) or not.\"},\n    )\n    model_revision: str = field(\n        default=\"main\",\n        metadata={\"help\": \"The specific model version to use (can be a branch name, tag name or commit id).\"},\n    )\n    use_auth_token: bool = field(\n        default=False,\n        metadata={\n            \"help\": \"Will use the token generated when running `transformers-cli login` (necessary to use this script \"\n                    \"with private models).\"\n        },\n    )\n\n    pooler_type: str = field(\n        default=\"cls\",\n        metadata={\n            \"help\": \"What kind of pooler to use (cls, cls_before_pooler, avg, avg_top2, avg_first_last).\"\n        }\n    )\n\n    mlp_only_train: bool = field(\n        default=False,\n        metadata={\n            \"help\": \"Use MLP only during training\"\n        }\n    )\n\n    sim_func: str = field(\n        default='cls_distance',\n        metadata={\"help\": \"the similarity function\",\n                  \"choices\": ['cls_distance.cosine', 'cls_distance.l2', 'bertscore']}\n    )\n\n\n    bert_score_loss: str = field(\n        default='softmax',\n        metadata={'help': 'loss function for bertscore sim function',\n                  'choices': ['softmax', 'hinge']}\n    )\n\n    hinge_margin: float = field(\n        default=1.0\n    )\n\n    hard_negative_weight: float = field(\n        default=0,\n        metadata={\n            \"help\": \"The **logit** of weight for hard negatives (only effective if hard negatives are used).\"\n        }\n    )\n\n    # SimCSE's arguments\n    temp: float = field(\n        default=0.05,\n        metadata={\n            \"help\": \"Temperature for softmax.\"\n        }\n    )\n\n    do_mlm: bool = field(\n        default=False,\n        metadata={\n            \"help\": \"Whether to use MLM auxiliary objective.\"\n        }\n    )\n\n    mlm_weight: float = field(\n        default=0.1,\n        metadata={\n            \"help\": \"Weight for MLM auxiliary objective (only effective if --do_mlm).\"\n        }\n    )\n\n    def __post_init__(self):\n        if self.sim_func == 'cls_distance.l2':\n            self.temp = 1\n\n\n\n@dataclass\nclass DataTrainingArguments:\n    \"\"\"\n    Arguments pertaining to what data we are going to input our model for training and eval.\n    \"\"\"\n\n    # Huggingface's original arguments.\n    dataset_name: Optional[str] = field(\n        default=None, metadata={\"help\": \"The name of the dataset to use (via the datasets library).\"}\n    )\n    dataset_config_name: Optional[str] = field(\n        default=None, metadata={\"help\": \"The configuration name of the dataset to use (via the datasets library).\"}\n    )\n    overwrite_cache: bool = field(\n        default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n    )\n    validation_split_percentage: Optional[int] = field(\n        default=5,\n        metadata={\n            \"help\": \"The percentage of the train set used as validation set in case there's no validation split\"\n        },\n    )\n    preprocessing_num_workers: Optional[int] = field(\n        default=None,\n        metadata={\"help\": \"The number of processes to use for the preprocessing.\"},\n    )\n\n    # SimCSE's arguments\n    train_file: Optional[str] = field(\n        default=None,\n        metadata={\"help\": \"The training data file (.txt or .csv).\"}\n    )\n\n    eval_file: Optional[str] = field(\n        default=None,\n        metadata={\"help\": \"The eval data file (.txt or .csv).\"}\n    )\n\n\n    max_seq_length: Optional[int] = field(\n        default=32,\n        metadata={\n            \"help\": \"The maximum total input sequence length after tokenization. Sequences longer \"\n                    \"than this will be truncated.\"\n        },\n    )\n    pad_to_max_length: bool = field(\n        default=False,\n        metadata={\n            \"help\": \"Whether to pad all samples to `max_seq_length`. \"\n                    \"If False, will pad the samples dynamically when batching to the maximum length in the batch.\"\n        },\n    )\n    mlm_probability: float = field(\n        default=0.15,\n        metadata={\"help\": \"Ratio of tokens to mask for MLM (only effective if --do_mlm)\"}\n    )\n\n    def __post_init__(self):\n        if self.dataset_name is None and self.train_file is None and self.validation_file is None:\n            raise ValueError(\"Need either a dataset name or a training/validation file.\")\n        else:\n            if self.train_file is not None:\n                extension = self.train_file.split(\".\")[-1]\n                assert extension in [\"csv\", \"json\", \"txt\"], \"`train_file` should be a csv, a json or a txt file.\"\n\nx = [1, 2, 3]\nfrom matplotlib.pyplot import plot\nplot(x, \"go\", label=\"temperature\")\n\n@dataclass\nclass OurTrainingArguments(TrainingArguments):\n    # Evaluation\n    ## By default, we evaluate STS (dev) during training (for selecting best checkpoints) and evaluate\n    ## both STS and transfer tasks (dev) at the end of training. Using --eval_transfer will allow evaluating\n    ## both STS and transfer tasks (dev) during training.\n    eval_transfer: bool = field(\n        default=False,\n        metadata={\"help\": \"Evaluate transfer task dev sets (in validation).\"}\n    )\n\n    customized_eval: bool = field(\n        default=True,\n        metadata={\"help\": \"Evaluate on the original set, if True, evaluate on user's own data\"}\n    )\n\n    customized_eval_used_split: Optional[str] = field(\n        default='dev'\n    )\n\n    tmp_tag: Optional[str] = field(\n        default='tmp',\n        metadata={'help': 'tag to save tmp models in case of overwriting'}\n    )\n\n    report_to: Optional[str] = field(\n        default='wandb'\n    )\n\n    logging_steps: int = field(\n        default=1\n    )\n\n    logging_dir: Optional[str] = field(\n        default='logs'\n    )\n\n    disable_tqdm: bool = field(\n        default=True\n    )\n\n    eval_form: str = field(\n        default='reranking',\n        metadata={'choices': ['reranking', 'retrieval']}\n    )\n\n    eval_retriever: str = field(\n        default='t5',\n        metadata={'choices': ['mlm', 't5']},\n    )\n\n    eval_src_file: str = field(\n        default='conala_nl.txt'\n    )\n\n    eval_tgt_file: str = field(\n        default='python_manual_firstpara.tok.txt'\n    )\n\n    eval_root_folder: str = field(\n        default='data/conala',\n        metadata={'help': 'root folder of validation dataset'}\n    )\n\n\n    eval_oracle_file: str = field(\n        default='cmd_dev.oracle_man.full.json'\n    )\n\n    # eval_max_length: int = field(\n    #     default=None,\n    #     metadata={'help': 'the length for dev set, None will call the max length of the tokenizer'}\n    # )\n\n\n    @cached_property\n    @torch_required\n    def _setup_devices(self) -> \"torch.device\":\n        logger.info(\"PyTorch: setting up devices\")\n        if self.no_cuda:\n            device = torch.device(\"cpu\")\n            self._n_gpu = 0\n        elif is_torch_tpu_available():\n            device = xm.xla_device()\n            self._n_gpu = 0\n        elif self.local_rank == -1:\n            # if n_gpu is > 1 we'll use nn.DataParallel.\n            # If you only want to use a specific subset of GPUs use `CUDA_VISIBLE_DEVICES=0`\n            # Explicitly set CUDA to the first (index 0) CUDA device, otherwise `set_device` will\n            # trigger an error that a device index is missing. Index 0 takes into account the\n            # GPUs available in the environment, so `CUDA_VISIBLE_DEVICES=1,2` with `cuda:0`\n            # will use the first GPU in that env, i.e. GPU#1\n            device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n            # Sometimes the line in the postinit has not been run before we end up here, so just checking we're not at\n            # the default value.\n            self._n_gpu = torch.cuda.device_count()\n        else:\n            # Here, we'll use torch.distributed.\n            # Initializes the distributed backend which will take care of synchronizing nodes/GPUs\n            #\n            # deepspeed performs its own DDP internally, and requires the program to be started with:\n            # deepspeed  ./program.py\n            # rather than:\n            # python -m torch.distributed.launch --nproc_per_node=2 ./program.py\n            if self.deepspeed:\n                from .integrations import is_deepspeed_available\n\n                if not is_deepspeed_available():\n                    raise ImportError(\"--deepspeed requires deepspeed: `pip install deepspeed`.\")\n                import deepspeed\n\n                deepspeed.init_distributed()\n            else:\n                torch.distributed.init_process_group(backend=\"nccl\")\n            device = torch.device(\"cuda\", self.local_rank)\n            self._n_gpu = 1\n\n        if device.type == \"cuda\":\n            torch.cuda.set_device(device)\n\n        return device\n\n@dataclass\nclass RetrieverArguments:\n    \"\"\"\n    model_type=model_args.model_name_or_path,\n    num_layers=bertscore_args.bertscore_layer_num,\n    all_layers=bertscore_args.all_layers,\n    idf = bertscore_args.idf,\n    idf_sents= bertscore_args.idf_sents,\n    rescale_with_baseline=bertscore_args.rescale_with_baseline,\n    baseline_path=bertscore_args.baseline_path\n    \"\"\"\n    num_layers: int = field(\n        default=11\n    )\n    all_layers: bool = field(\n        default=False\n    )\n    idf: bool = field(\n        default=False\n    )\n    rescale_with_baseline: bool = field(\n        default=False\n    )\n    baseline_path: str = field(\n        default=None\n    )"}
{"type": "source_file", "path": "retriever/simcse/utils.py", "content": "import sys\nimport os\nimport torch\nfrom math import log\nfrom itertools import chain\nfrom collections import defaultdict, Counter\nfrom multiprocessing import Pool\nfrom functools import partial\n\nimport transformers\nfrom torch import nn\nfrom tqdm.auto import tqdm\nfrom torch.nn.utils.rnn import pad_sequence\nfrom distutils.version import LooseVersion\n\nfrom transformers import BertConfig, XLNetConfig, XLMConfig, RobertaConfig\nfrom transformers import AutoModel, GPT2Tokenizer, AutoTokenizer\n\n# from . import __version__\nfrom transformers import __version__ as trans_version\n\n__all__ = []\n\nSCIBERT_URL_DICT = {\n    \"scibert-scivocab-uncased\": \"https://s3-us-west-2.amazonaws.com/ai2-s2-research/scibert/pytorch_models/scibert_scivocab_uncased.tar\",  # recommend by the SciBERT authors\n    \"scibert-scivocab-cased\": \"https://s3-us-west-2.amazonaws.com/ai2-s2-research/scibert/pytorch_models/scibert_scivocab_cased.tar\",\n    \"scibert-basevocab-uncased\": \"https://s3-us-west-2.amazonaws.com/ai2-s2-research/scibert/pytorch_models/scibert_basevocab_uncased.tar\",\n    \"scibert-basevocab-cased\": \"https://s3-us-west-2.amazonaws.com/ai2-s2-research/scibert/pytorch_models/scibert_basevocab_cased.tar\",\n}\n\n\nlang2model = defaultdict(lambda: \"bert-base-multilingual-cased\")\nlang2model.update(\n    {\n        \"en\": \"roberta-large\",\n        \"zh\": \"bert-base-chinese\",\n        \"tr\": \"dbmdz/bert-base-turkish-cased\",\n        \"en-sci\": \"allenai/scibert_scivocab_uncased\",\n    }\n)\n\n\nmodel2layers = {\n    \"bert-base-uncased\": 9,  # 0.6925188074454226\n    \"bert-large-uncased\": 18,  # 0.7210358126642836\n    \"bert-base-cased-finetuned-mrpc\": 9,  # 0.6721947475618048\n    \"bert-base-multilingual-cased\": 9,  # 0.6680687802637132\n    \"bert-base-chinese\": 8,\n    \"roberta-base\": 10,  # 0.706288719158983\n    \"roberta-large\": 17,  # 0.7385974720781534\n    \"roberta-large-mnli\": 19,  # 0.7535618640417984\n    \"roberta-base-openai-detector\": 7,  # 0.7048158349432633\n    \"roberta-large-openai-detector\": 15,  # 0.7462770207355116\n    \"xlnet-base-cased\": 5,  # 0.6630103662114238\n    \"xlnet-large-cased\": 7,  # 0.6598800720297179\n    \"xlm-mlm-en-2048\": 6,  # 0.651262570131464\n    \"xlm-mlm-100-1280\": 10,  # 0.6475166424401905\n    # \"scibert-scivocab-uncased\": 8,  # 0.6590354319927313\n    # \"scibert-scivocab-cased\": 9,  # 0.6536375053937445\n    # \"scibert-basevocab-uncased\": 9,  # 0.6748944832703548\n    # \"scibert-basevocab-cased\": 9,  # 0.6524624150542374\n    'allenai/scibert_scivocab_uncased': 8, # 0.6590354393124127\n    'allenai/scibert_scivocab_cased': 9, # 0.6536374902465466\n    'nfliu/scibert_basevocab_uncased': 9, # 0.6748945076082333\n    \"distilroberta-base\": 5,  # 0.6797558139322964\n    \"distilbert-base-uncased\": 5,  # 0.6756659152782033\n    \"distilbert-base-uncased-distilled-squad\": 4,  # 0.6718318036382493\n    \"distilbert-base-multilingual-cased\": 5,  # 0.6178131050889238\n    \"albert-base-v1\": 10,  # 0.654237567249745\n    \"albert-large-v1\": 17,  # 0.6755890754323239\n    \"albert-xlarge-v1\": 16,  # 0.7031844211905911\n    \"albert-xxlarge-v1\": 8,  # 0.7508642218461096\n    \"albert-base-v2\": 9,  # 0.6682455591837927\n    \"albert-large-v2\": 14,  # 0.7008537594374035\n    \"albert-xlarge-v2\": 13,  # 0.7317228357869254\n    \"albert-xxlarge-v2\": 8,  # 0.7505160257184014\n    \"xlm-roberta-base\": 9,  # 0.6506799445871697\n    \"xlm-roberta-large\": 17,  # 0.6941551437476826\n    \"google/electra-small-generator\": 9,  # 0.6659421842117754\n    \"google/electra-small-discriminator\": 11,  # 0.6534639151385759\n    \"google/electra-base-generator\": 10,  # 0.6730033453857188\n    \"google/electra-base-discriminator\": 9,  # 0.7032089590812965\n    \"google/electra-large-generator\": 18,  # 0.6813370013104459\n    \"google/electra-large-discriminator\": 14,  # 0.6896675824733477\n    \"google/bert_uncased_L-2_H-128_A-2\": 1,  # 0.5887998733228855\n    \"google/bert_uncased_L-2_H-256_A-4\": 1,  # 0.6114863547661203\n    \"google/bert_uncased_L-2_H-512_A-8\": 1,  # 0.6177345529192847\n    \"google/bert_uncased_L-2_H-768_A-12\": 2,  # 0.6191261237956839\n    \"google/bert_uncased_L-4_H-128_A-2\": 3,  # 0.6076202863798991\n    \"google/bert_uncased_L-4_H-256_A-4\": 3,  # 0.6205239036810148\n    \"google/bert_uncased_L-4_H-512_A-8\": 3,  # 0.6375351621856903\n    \"google/bert_uncased_L-4_H-768_A-12\": 3,  # 0.6561849979644787\n    \"google/bert_uncased_L-6_H-128_A-2\": 5,  # 0.6200458425360283\n    \"google/bert_uncased_L-6_H-256_A-4\": 5,  # 0.6277501629539081\n    \"google/bert_uncased_L-6_H-512_A-8\": 5,  # 0.641952305130849\n    \"google/bert_uncased_L-6_H-768_A-12\": 5,  # 0.6762186226247106\n    \"google/bert_uncased_L-8_H-128_A-2\": 7,  # 0.6186876506711779\n    \"google/bert_uncased_L-8_H-256_A-4\": 7,  # 0.6447993208267708\n    \"google/bert_uncased_L-8_H-512_A-8\": 6,  # 0.6489729408169956\n    \"google/bert_uncased_L-8_H-768_A-12\": 7,  # 0.6705203359541737\n    \"google/bert_uncased_L-10_H-128_A-2\": 8,  # 0.6126762064125278\n    \"google/bert_uncased_L-10_H-256_A-4\": 8,  # 0.6376350032576573\n    \"google/bert_uncased_L-10_H-512_A-8\": 9,  # 0.6579006292799915\n    \"google/bert_uncased_L-10_H-768_A-12\": 8,  # 0.6861146692220176\n    \"google/bert_uncased_L-12_H-128_A-2\": 10,  # 0.6184105693383591\n    \"google/bert_uncased_L-12_H-256_A-4\": 11,  # 0.6374004994430261\n    \"google/bert_uncased_L-12_H-512_A-8\": 10,  # 0.65880012149526\n    \"google/bert_uncased_L-12_H-768_A-12\": 9,  # 0.675911357700092\n    \"amazon/bort\": 0,  # 0.41927911053036643\n    \"facebook/bart-base\": 6,  # 0.7122259132414092\n    \"facebook/bart-large\": 10,  # 0.7448671872459683\n    \"facebook/bart-large-cnn\": 10,  # 0.7393148105835096\n    \"facebook/bart-large-mnli\": 11,  # 0.7531665445691358\n    \"facebook/bart-large-xsum\": 9,  # 0.7496408866539556\n    \"t5-small\": 6,  # 0.6813843919496912\n    \"t5-base\": 11,  # 0.7096044814981418\n    \"t5-large\": 23,  # 0.7244153820191929\n    \"vinai/bertweet-base\": 9,  # 0.6529471006118857\n    \"microsoft/deberta-base\": 9,  # 0.7088459455930344\n    \"microsoft/deberta-base-mnli\": 9,  # 0.7395257063907247\n    \"microsoft/deberta-large\": 16,  # 0.7511806792052013\n    \"microsoft/deberta-large-mnli\": 18,  # 0.7736263649679905\n    \"microsoft/deberta-xlarge\": 18,  # 0.7568670944373346\n    \"microsoft/deberta-xlarge-mnli\": 40,  # 0.7780600929333213\n    \"YituTech/conv-bert-base\": 10,  # 0.7058253551080789\n    \"YituTech/conv-bert-small\": 10,  # 0.6544473011107349\n    \"YituTech/conv-bert-medium-small\": 9,  # 0.6590097075123257\n    \"microsoft/mpnet-base\": 8,  # 0.724976539498804\n    \"squeezebert/squeezebert-uncased\": 9,  # 0.6543868703018726\n    \"squeezebert/squeezebert-mnli\": 9,  # 0.6654799051284791\n    \"squeezebert/squeezebert-mnli-headless\": 9,  # 0.6654799051284791\n    \"tuner007/pegasus_paraphrase\": 15,  # 0.7188349436772694\n    \"google/pegasus-large\": 8,  # 0.63960462272448\n    \"google/pegasus-xsum\": 11,  # 0.6836878575233349\n    \"sshleifer/tiny-mbart\": 2,  # 0.028246072231946733\n    \"facebook/mbart-large-cc25\": 12,  # 0.6582922975802958\n    \"facebook/mbart-large-50\": 12,  # 0.6464972230103133\n    \"facebook/mbart-large-en-ro\": 12,  # 0.6791285137459857\n    \"facebook/mbart-large-50-many-to-many-mmt\": 12,  # 0.6904136529270892\n    \"facebook/mbart-large-50-one-to-many-mmt\": 12,  # 0.6847906439540236\n    \"allenai/led-base-16384\": 6,  # 0.7122259170564179\n    \"facebook/blenderbot_small-90M\": 7,  # 0.6489176335400088\n    \"facebook/blenderbot-400M-distill\": 2,  # 0.5874774070540008\n    \"microsoft/prophetnet-large-uncased\": 4,  # 0.586496184234925\n    \"microsoft/prophetnet-large-uncased-cnndm\": 7,  # 0.6478379437729287\n    \"SpanBERT/spanbert-base-cased\": 8,  # 0.6824006863686848\n    \"SpanBERT/spanbert-large-cased\": 17,  # 0.705352690855603\n    \"microsoft/xprophetnet-large-wiki100-cased\": 7,  # 0.5852499775879524\n    \"ProsusAI/finbert\": 10,  # 0.6923213940752796\n    \"Vamsi/T5_Paraphrase_Paws\": 12,  # 0.6941611753807352\n    \"ramsrigouthamg/t5_paraphraser\": 11,  # 0.7200917597031539\n    \"microsoft/deberta-v2-xlarge\": 10,  # 0.7393675784473045\n    \"microsoft/deberta-v2-xlarge-mnli\": 17,  # 0.7620620803716714\n    \"microsoft/deberta-v2-xxlarge\": 21,  # 0.7520547670281869\n    \"microsoft/deberta-v2-xxlarge-mnli\": 22,  # 0.7742603457742682\n    \"allenai/longformer-base-4096\": 7,  # 0.7089559593129316\n    \"allenai/longformer-large-4096\": 14,  # 0.732408493548181\n    \"allenai/longformer-large-4096-finetuned-triviaqa\": 14,  # 0.7365882744744722\n    \"zhiheng-huang/bert-base-uncased-embedding-relative-key\": 4,  # 0.5995636595368777\n    \"zhiheng-huang/bert-base-uncased-embedding-relative-key-query\": 7,  # 0.6303599452145718\n    \"zhiheng-huang/bert-large-uncased-whole-word-masking-embedding-relative-key-query\": 19,  # 0.6896878492850327\n    'google/mt5-small': 8, # 0.6401166527273479\n    'google/mt5-base': 11, # 0.5663956536597241\n    'google/mt5-large': 19, # 0.6430931371732798\n    'google/mt5-xl': 24, # 0.6707200963021145\n    'google/bigbird-roberta-base': 10, # 0.6695606423502717\n    'google/bigbird-roberta-large': 14, # 0.6755874042374509\n    'google/bigbird-base-trivia-itc': 8, # 0.6930725491629892\n    'princeton-nlp/unsup-simcse-bert-base-uncased': 10, # 0.6703066531921142\n    'princeton-nlp/unsup-simcse-bert-large-uncased': 18, # 0.6958302800755326\n    'princeton-nlp/unsup-simcse-roberta-base': 8, # 0.6436615893535319\n    'princeton-nlp/unsup-simcse-roberta-large': 13, # 0.6812864385585965\n    'princeton-nlp/sup-simcse-bert-base-uncased': 10, # 0.7068074935240984\n    'princeton-nlp/sup-simcse-bert-large-uncased': 18, # 0.7111049471332378\n    'princeton-nlp/sup-simcse-roberta-base': 10, # 0.7253123806661946\n    'princeton-nlp/sup-simcse-roberta-large': 16, # 0.7497820277237173\n    'dbmdz/bert-base-turkish-cased': 10, # WMT18 seg en-tr 0.5522827687776142\n    'dbmdz/distilbert-base-turkish-cased': 4, # WMT18 seg en-tr 0.4742268041237113\n    'google/byt5-small': 1, # 0.5100025975052146\n    'google/byt5-base': 17, # 0.5810347173565313\n    'google/byt5-large': 30, # 0.6151895697554877\n    'microsoft/deberta-v3-xsmall': 10, # 0.6941803815412021\n    'microsoft/deberta-v3-small': 4, # 0.6651551203179679\n    'microsoft/deberta-v3-base': 9, # 0.7261586651018335\n    'microsoft/mdeberta-v3-base': 10, # 0.6778713684091584\n    'microsoft/deberta-v3-large': 12, # 0.6927693082293821\n    'khalidalt/DeBERTa-v3-large-mnli': 18, # 0.7428756686018376\n}\n\n\ndef sent_encode(tokenizer, sent):\n    \"Encoding as sentence based on the tokenizer\"\n    sent = sent.strip()\n    if sent == \"\":\n        return tokenizer.build_inputs_with_special_tokens([])\n    elif isinstance(tokenizer, GPT2Tokenizer):\n        # for RoBERTa and GPT-2\n        if LooseVersion(trans_version) >= LooseVersion(\"4.0.0\"):\n            return tokenizer.encode(\n                sent,\n                add_special_tokens=True,\n                add_prefix_space=True,\n                max_length=tokenizer.model_max_length,\n                truncation=True,\n            )\n        elif LooseVersion(trans_version) >= LooseVersion(\"3.0.0\"):\n            return tokenizer.encode(\n                sent, add_special_tokens=True, add_prefix_space=True, max_length=tokenizer.max_len, truncation=True,\n            )\n        elif LooseVersion(trans_version) >= LooseVersion(\"2.0.0\"):\n            return tokenizer.encode(sent, add_special_tokens=True, add_prefix_space=True, max_length=tokenizer.max_len)\n        else:\n            raise NotImplementedError(f\"transformers version {trans_version} is not supported\")\n    else:\n        if LooseVersion(trans_version) >= LooseVersion(\"4.0.0\"):\n            return tokenizer.encode(\n                sent, add_special_tokens=True, max_length=tokenizer.model_max_length, truncation=True,\n            )\n        elif LooseVersion(trans_version) >= LooseVersion(\"3.0.0\"):\n            return tokenizer.encode(sent, add_special_tokens=True, max_length=tokenizer.max_len, truncation=True)\n        elif LooseVersion(trans_version) >= LooseVersion(\"2.0.0\"):\n            return tokenizer.encode(sent, add_special_tokens=True, max_length=tokenizer.max_len)\n        else:\n            raise NotImplementedError(f\"transformers version {trans_version} is not supported\")\n\n\ndef get_model(model_type, num_layers, all_layers=None):\n    if model_type.startswith(\"scibert\"):\n        model = AutoModel.from_pretrained(cache_scibert(model_type))\n    elif \"t5\" in model_type:\n        from transformers import T5EncoderModel\n        model = T5EncoderModel.from_pretrained(model_type)\n    else:\n        model = AutoModel.from_pretrained(model_type)\n    model.eval()\n\n    if hasattr(model, \"decoder\") and hasattr(model, \"encoder\"):\n        model = model.encoder\n\n    # drop unused layers\n    if not all_layers:\n        if hasattr(model, \"n_layers\"):  # xlm\n            assert (\n                0 <= num_layers <= model.n_layers\n            ), f\"Invalid num_layers: num_layers should be between 0 and {model.n_layers} for {model_type}\"\n            model.n_layers = num_layers\n        elif hasattr(model, \"layer\"):  # xlnet\n            assert (\n                0 <= num_layers <= len(model.layer)\n            ), f\"Invalid num_layers: num_layers should be between 0 and {len(model.layer)} for {model_type}\"\n            model.layer = torch.nn.ModuleList([layer for layer in model.layer[:num_layers]])\n        elif hasattr(model, \"encoder\"):  # albert\n            if hasattr(model.encoder, \"albert_layer_groups\"):\n                assert (\n                    0 <= num_layers <= model.encoder.config.num_hidden_layers\n                ), f\"Invalid num_layers: num_layers should be between 0 and {model.encoder.config.num_hidden_layers} for {model_type}\"\n                model.encoder.config.num_hidden_layers = num_layers\n            elif hasattr(model.encoder, \"block\"):  # t5\n                assert (\n                    0 <= num_layers <= len(model.encoder.block)\n                ), f\"Invalid num_layers: num_layers should be between 0 and {len(model.encoder.block)} for {model_type}\"\n                model.encoder.block = torch.nn.ModuleList([layer for layer in model.encoder.block[:num_layers]])\n            else:  # bert, roberta\n                assert (\n                    0 <= num_layers <= len(model.encoder.layer)\n                ), f\"Invalid num_layers: num_layers should be between 0 and {len(model.encoder.layer)} for {model_type}\"\n                model.encoder.layer = torch.nn.ModuleList([layer for layer in model.encoder.layer[:num_layers]])\n        elif hasattr(model, \"transformer\"):  # bert, roberta\n            assert (\n                0 <= num_layers <= len(model.transformer.layer)\n            ), f\"Invalid num_layers: num_layers should be between 0 and {len(model.transformer.layer)} for {model_type}\"\n            model.transformer.layer = torch.nn.ModuleList([layer for layer in model.transformer.layer[:num_layers]])\n        elif hasattr(model, \"layers\"):  # bart\n            assert (\n                0 <= num_layers <= len(model.layers)\n            ), f\"Invalid num_layers: num_layers should be between 0 and {len(model.layers)} for {model_type}\"\n            model.layers = torch.nn.ModuleList([layer for layer in model.layers[:num_layers]])\n        else:\n            raise ValueError(\"Not supported\")\n    else:\n        if hasattr(model, \"output_hidden_states\"):\n            model.output_hidden_states = True\n        elif hasattr(model, \"encoder\"):\n            model.encoder.output_hidden_states = True\n        elif hasattr(model, \"transformer\"):\n            model.transformer.output_hidden_states = True\n        # else:\n        #     raise ValueError(f\"Not supported model architecture: {model_type}\")\n\n    return model\n\n\ndef get_tokenizer(model_type, use_fast=False):\n    if model_type.startswith(\"scibert\"):\n        model_type = cache_scibert(model_type)\n\n    if LooseVersion(trans_version) >= LooseVersion(\"4.0.0\"):\n        if 'codet5' in model_type:\n            tokenizer = transformers.RobertaTokenizer.from_pretrained(model_type)\n        else:\n            tokenizer = AutoTokenizer.from_pretrained(model_type, use_fast=use_fast)\n    else:\n        assert not use_fast, \"Fast tokenizer is not available for version < 4.0.0\"\n        tokenizer = AutoTokenizer.from_pretrained(model_type)\n\n    return tokenizer\n\n\ndef padding(arr, pad_token, dtype=torch.long):\n    lens = torch.LongTensor([len(a) for a in arr])\n    max_len = lens.max().item()\n    padded = torch.ones(len(arr), max_len, dtype=dtype) * pad_token\n    mask = torch.zeros(len(arr), max_len, dtype=torch.long)\n    for i, a in enumerate(arr):\n        padded[i, : lens[i]] = torch.tensor(a, dtype=dtype)\n        mask[i, : lens[i]] = 1\n    return padded, lens, mask\n\n\ndef bert_encode(model, x, attention_mask, all_layers=False):\n    model.eval()\n    # print(x, attention_mask)\n    with torch.no_grad():\n        out = model(x, attention_mask=attention_mask, output_hidden_states=all_layers)\n    if all_layers:\n        emb = torch.stack(out[-1], dim=2)\n    else:\n        emb = out[0]\n    return emb\n\n\ndef process(a, tokenizer=None):\n    if tokenizer is not None:\n        a = sent_encode(tokenizer, a)\n    return set(a)\n\n\ndef get_idf_dict(arr, tokenizer, nthreads=4):\n    \"\"\"\n    Returns mapping from word piece index to its inverse document frequency.\n\n\n    Args:\n        - :param: `arr` (list of str) : sentences to process.\n        - :param: `tokenizer` : a BERT tokenizer corresponds to `model`.\n        - :param: `nthreads` (int) : number of CPU threads to use\n    \"\"\"\n    idf_count = Counter()\n    num_docs = len(arr)\n\n    process_partial = partial(process, tokenizer=tokenizer)\n\n    with Pool(nthreads) as p:\n        idf_count.update(chain.from_iterable(p.map(process_partial, arr)))\n\n    idf_dict = defaultdict(lambda: log((num_docs + 1) / (1)))\n    idf_dict.update({idx: log((num_docs + 1) / (c + 1)) for (idx, c) in idf_count.items()})\n    return idf_dict\n\n\ndef collate_idf(arr, tokenizer, idf_dict, device=\"cuda:0\"):\n    \"\"\"\n    Helper function that pads a list of sentences to have the same length and\n    loads idf score for words in the sentences.\n\n    Args:\n        - :param: `arr` (list of str): sentences to process.\n        - :param: `tokenize` : a function that takes a string and return list\n                  of tokens.\n        - :param: `numericalize` : a function that takes a list of tokens and\n                  return list of token indexes.\n        - :param: `idf_dict` (dict): mapping a word piece index to its\n                               inverse document frequency\n        - :param: `pad` (str): the padding token.\n        - :param: `device` (str): device to use, e.g. 'cpu' or 'cuda'\n    \"\"\"\n    arr = [sent_encode(tokenizer, a) for a in arr]\n\n    idf_weights = [[idf_dict[i] for i in a] for a in arr]\n\n    pad_token = tokenizer.pad_token_id\n\n    padded, lens, mask = padding(arr, pad_token, dtype=torch.long)\n    padded_idf, _, _ = padding(idf_weights, 0, dtype=torch.float)\n\n    padded = padded.to(device=device)\n    mask = mask.to(device=device)\n    lens = lens.to(device=device)\n    return padded, padded_idf, lens, mask\n\n\ndef get_bert_embedding(all_sens, model, tokenizer, idf_dict, batch_size=-1, device=\"cuda:0\", all_layers=False):\n    \"\"\"\n    Compute BERT embedding in batches.\n\n    Args:\n        - :param: `all_sens` (list of str) : sentences to encode.\n        - :param: `model` : a BERT model from `pytorch_pretrained_bert`.\n        - :param: `tokenizer` : a BERT tokenizer corresponds to `model`.\n        - :param: `idf_dict` (dict) : mapping a word piece index to its\n                               inverse document frequency\n        - :param: `device` (str): device to use, e.g. 'cpu' or 'cuda'\n    \"\"\"\n\n    padded_sens, padded_idf, lens, mask = collate_idf(all_sens, tokenizer, idf_dict, device=device)\n\n    if batch_size == -1:\n        batch_size = len(all_sens)\n\n    embeddings = []\n    with torch.no_grad():\n        for i in range(0, len(all_sens), batch_size):\n            batch_embedding = bert_encode(\n                model, padded_sens[i : i + batch_size], attention_mask=mask[i : i + batch_size], all_layers=all_layers,\n            )\n            embeddings.append(batch_embedding)\n            del batch_embedding\n\n    total_embedding = torch.cat(embeddings, dim=0)\n\n    return total_embedding, mask, padded_idf\n\ndef greedy_cos_idf_for_train(ref_embedding, ref_masks, ref_idf, hyp_embedding, hyp_masks, hyp_idf, all_layers=False):\n    \"\"\"\n    Compute greedy matching based on cosine similarity.\n\n    Args:\n        - :param: `ref_embedding` (torch.Tensor):\n                   embeddings of reference sentences, BxKxd,\n                   B: batch size, K: longest length, d: bert dimenison\n        - :param: `ref_lens` (list of int): list of reference sentence length.\n        - :param: `ref_masks` (torch.LongTensor): BxKxK, BERT attention mask for\n                   reference sentences.\n        - :param: `ref_idf` (torch.Tensor): BxK, idf score of each word\n                   piece in the reference setence\n        - :param: `hyp_embedding` (torch.Tensor):\n                   embeddings of candidate sentences, BxKxd,\n                   B: batch size, K: longest length, d: bert dimenison\n        - :param: `hyp_lens` (list of int): list of candidate sentence length.\n        - :param: `hyp_masks` (torch.LongTensor): BxKxK, BERT attention mask for\n                   candidate sentences.\n        - :param: `hyp_idf` (torch.Tensor): BxK, idf score of each word\n                   piece in the candidate setence\n    \"\"\"\n    assert not all_layers\n    ref_embedding = ref_embedding.div(torch.norm(ref_embedding, dim=-1).unsqueeze(-1))\n    hyp_embedding = hyp_embedding.div(torch.norm(hyp_embedding, dim=-1).unsqueeze(-1))\n\n    if all_layers:\n        B, _, L, D = hyp_embedding.size()\n        hyp_embedding = hyp_embedding.transpose(1, 2).transpose(0, 1).contiguous().view(L * B, hyp_embedding.size(1), D)\n        ref_embedding = ref_embedding.transpose(1, 2).transpose(0, 1).contiguous().view(L * B, ref_embedding.size(1), D)\n\n    batch_size, ref_max_len = ref_embedding.size(0), ref_embedding.size(1)\n    hyp_max_len = hyp_embedding.size(1)\n    flat_hype_embed = hyp_embedding.view(-1, hyp_embedding.size(-1)) # bs * max_len, hidden_size\n    flat_ref_embed = ref_embedding.view(-1, ref_embedding.size(-1))\n    sim = torch.matmul(flat_hype_embed, flat_ref_embed.transpose(0, 1)) # bs * max_len, bs * max_len\n    flat_hype_masks = hyp_masks.view(-1)\n    flat_ref_masks = ref_masks.view(-1)\n    masks = torch.matmul(flat_hype_masks.unsqueeze(-1).float(), flat_ref_masks.unsqueeze(0).float())\n\n    # if all_layers:\n    #     masks = masks.unsqueeze(0).expand(L, -1, -1, -1).contiguous().view_as(sim)\n    # else:\n    #     masks = masks.expand(batch_size, -1, -1).contiguous().view_as(sim)\n\n    sim = sim * masks\n    sim = sim.view(batch_size, hyp_max_len, batch_size, ref_max_len)\n    word_precision = sim.max(dim=-1)[0] # B_h, L_h, B_r\n    word_recall = sim.max(dim=1)[0] # B_h, B_r, L_r\n\n\n    hyp_idf = hyp_idf.div(hyp_idf.sum(dim=1, keepdim=True)) # B_h, L_h\n    ref_idf = ref_idf.div(ref_idf.sum(dim=1, keepdim=True))\n    precision_scale = hyp_idf\n    recall_scale = ref_idf\n    if all_layers:\n        precision_scale = precision_scale.unsqueeze(0).expand(L, B, -1).contiguous().view_as(word_precision)\n        recall_scale = recall_scale.unsqueeze(0).expand(L, B, -1).contiguous().view_as(word_recall)\n    precision_scale = precision_scale.unsqueeze(-1) # B_h, L_h, 1\n    recall_scale = recall_scale.unsqueeze(0) # 1, B_r, L_r apply to B_h, B_r, L_r\n    P = (word_precision * precision_scale).sum(dim=1)\n    R = (word_recall * recall_scale).sum(dim=2)\n    F = 2 * P * R / (P + R + 1e-10) # B, B\n    assert torch.all(F <= 1) and torch.all(F >= -1)\n\n    hyp_zero_mask = hyp_masks.sum(dim=1).eq(2)\n    ref_zero_mask = ref_masks.sum(dim=1).eq(2)\n    # assert not torch.any(hyp_zero_mask)\n    # assert not torch.any(ref_zero_mask)\n    # assert not torch.any(torch.isnan(F))\n\n    if all_layers:\n        P = P.view(L, B)\n        R = R.view(L, B)\n        F = F.view(L, B)\n\n    if torch.any(hyp_zero_mask):\n        print(\n            \"Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\", file=sys.stderr,\n        )\n        P = P.masked_fill(hyp_zero_mask, 0.0)\n        R = R.masked_fill(hyp_zero_mask, 0.0)\n\n    if torch.any(ref_zero_mask):\n        print(\"Warning: Empty reference sentence detected; setting raw BERTScores to 0.\", file=sys.stderr)\n        P = P.masked_fill(ref_zero_mask, 0.0)\n        R = R.masked_fill(ref_zero_mask, 0.0)\n\n    F = F.masked_fill(torch.isnan(F), 0.0)\n\n    return P.transpose(0, 1), R.transpose(0, 1), F.transpose(0, 1)\n\n\ndef greedy_cos_idf(ref_embedding, ref_masks, ref_idf, hyp_embedding, hyp_masks, hyp_idf, all_layers=False):\n    \"\"\"\n    Compute greedy matching based on cosine similarity.\n\n    Args:\n        - :param: `ref_embedding` (torch.Tensor):\n                   embeddings of reference sentences, BxKxd,\n                   B: batch size, K: longest length, d: bert dimenison\n        - :param: `ref_lens` (list of int): list of reference sentence length.\n        - :param: `ref_masks` (torch.LongTensor): BxKxK, BERT attention mask for\n                   reference sentences.\n        - :param: `ref_idf` (torch.Tensor): BxK, idf score of each word\n                   piece in the reference setence\n        - :param: `hyp_embedding` (torch.Tensor):\n                   embeddings of candidate sentences, BxKxd,\n                   B: batch size, K: longest length, d: bert dimenison\n        - :param: `hyp_lens` (list of int): list of candidate sentence length.\n        - :param: `hyp_masks` (torch.LongTensor): BxKxK, BERT attention mask for\n                   candidate sentences.\n        - :param: `hyp_idf` (torch.Tensor): BxK, idf score of each word\n                   piece in the candidate setence\n    \"\"\"\n    ref_embedding.div_(torch.norm(ref_embedding, dim=-1).unsqueeze(-1))\n    hyp_embedding.div_(torch.norm(hyp_embedding, dim=-1).unsqueeze(-1))\n\n    if all_layers:\n        B, _, L, D = hyp_embedding.size()\n        hyp_embedding = hyp_embedding.transpose(1, 2).transpose(0, 1).contiguous().view(L * B, hyp_embedding.size(1), D)\n        ref_embedding = ref_embedding.transpose(1, 2).transpose(0, 1).contiguous().view(L * B, ref_embedding.size(1), D)\n    batch_size = ref_embedding.size(0)\n    sim = torch.bmm(hyp_embedding, ref_embedding.transpose(1, 2)) # B, L_h, L_r\n    masks = torch.bmm(hyp_masks.unsqueeze(2).float(), ref_masks.unsqueeze(1).float())\n    if all_layers:\n        masks = masks.unsqueeze(0).expand(L, -1, -1, -1).contiguous().view_as(sim)\n    else:\n        masks = masks.expand(batch_size, -1, -1).contiguous().view_as(sim)\n\n    masks = masks.float().to(sim.device)\n    sim = sim * masks\n\n    word_precision = sim.max(dim=2)[0]\n    word_recall = sim.max(dim=1)[0]\n\n    hyp_idf.div_(hyp_idf.sum(dim=1, keepdim=True))\n    ref_idf.div_(ref_idf.sum(dim=1, keepdim=True))\n    precision_scale = hyp_idf.to(word_precision.device)\n    recall_scale = ref_idf.to(word_recall.device)\n    if all_layers:\n        precision_scale = precision_scale.unsqueeze(0).expand(L, B, -1).contiguous().view_as(word_precision)\n        recall_scale = recall_scale.unsqueeze(0).expand(L, B, -1).contiguous().view_as(word_recall)\n    P = (word_precision * precision_scale).sum(dim=1)\n    R = (word_recall * recall_scale).sum(dim=1)\n    F = 2 * P * R / (P + R)\n\n    hyp_zero_mask = hyp_masks.sum(dim=1).eq(2)\n    ref_zero_mask = ref_masks.sum(dim=1).eq(2)\n\n    if all_layers:\n        P = P.view(L, B)\n        R = R.view(L, B)\n        F = F.view(L, B)\n\n    if torch.any(hyp_zero_mask):\n        print(\n            \"Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\", file=sys.stderr,\n        )\n        P = P.masked_fill(hyp_zero_mask, 0.0)\n        R = R.masked_fill(hyp_zero_mask, 0.0)\n\n    if torch.any(ref_zero_mask):\n        print(\"Warning: Empty reference sentence detected; setting raw BERTScores to 0.\", file=sys.stderr)\n        P = P.masked_fill(ref_zero_mask, 0.0)\n        R = R.masked_fill(ref_zero_mask, 0.0)\n\n    F = F.masked_fill(torch.isnan(F), 0.0)\n\n    return P, R, F\n\n\ndef bert_cos_score_idf(\n    model, refs, hyps, tokenizer, idf_dict, verbose=False, batch_size=64, device=\"cuda:0\", all_layers=False,\n):\n    \"\"\"\n    Compute BERTScore.\n\n    Args:\n        - :param: `model` : a BERT model in `pytorch_pretrained_bert`\n        - :param: `refs` (list of str): reference sentences\n        - :param: `hyps` (list of str): candidate sentences\n        - :param: `tokenzier` : a BERT tokenizer corresponds to `model`\n        - :param: `idf_dict` : a dictionary mapping a word piece index to its\n                               inverse document frequency\n        - :param: `verbose` (bool): turn on intermediate status update\n        - :param: `batch_size` (int): bert score processing batch size\n        - :param: `device` (str): device to use, e.g. 'cpu' or 'cuda'\n    \"\"\"\n    preds = []\n\n    def dedup_and_sort(l):\n        return sorted(list(set(l)), key=lambda x: len(x.split(\" \")), reverse=True)\n\n    sentences = dedup_and_sort(refs + hyps)\n    embs = []\n    iter_range = range(0, len(sentences), batch_size)\n    if verbose:\n        print(\"computing bert embedding.\")\n        iter_range = tqdm(iter_range)\n    stats_dict = dict()\n    for batch_start in iter_range:\n        sen_batch = sentences[batch_start : batch_start + batch_size]\n        embs, masks, padded_idf = get_bert_embedding(\n            sen_batch, model, tokenizer, idf_dict, device=device, all_layers=all_layers\n        )\n        embs = embs.cpu()\n        masks = masks.cpu()\n        padded_idf = padded_idf.cpu()\n        for i, sen in enumerate(sen_batch):\n            sequence_len = masks[i].sum().item()\n            emb = embs[i, :sequence_len]\n            idf = padded_idf[i, :sequence_len]\n            stats_dict[sen] = (emb, idf)\n\n    def pad_batch_stats(sen_batch, stats_dict, device):\n        stats = [stats_dict[s] for s in sen_batch]\n        emb, idf = zip(*stats)\n        emb = [e.to(device) for e in emb]\n        idf = [i.to(device) for i in idf]\n        lens = [e.size(0) for e in emb]\n        emb_pad = pad_sequence(emb, batch_first=True, padding_value=2.0)\n        idf_pad = pad_sequence(idf, batch_first=True)\n\n        def length_to_mask(lens):\n            lens = torch.tensor(lens, dtype=torch.long)\n            max_len = max(lens)\n            base = torch.arange(max_len, dtype=torch.long).expand(len(lens), max_len)\n            return base < lens.unsqueeze(1)\n\n        pad_mask = length_to_mask(lens).to(device)\n        return emb_pad, pad_mask, idf_pad\n\n    device = next(model.parameters()).device\n    iter_range = range(0, len(refs), batch_size)\n    if verbose:\n        print(\"computing greedy matching.\")\n        iter_range = tqdm(iter_range)\n\n    with torch.no_grad():\n        for batch_start in iter_range:\n            batch_refs = refs[batch_start : batch_start + batch_size]\n            batch_hyps = hyps[batch_start : batch_start + batch_size]\n            ref_stats = pad_batch_stats(batch_refs, stats_dict, device)\n            hyp_stats = pad_batch_stats(batch_hyps, stats_dict, device)\n\n            P, R, F1 = greedy_cos_idf(*ref_stats, *hyp_stats, all_layers)\n            preds.append(torch.stack((P, R, F1), dim=-1).cpu())\n    preds = torch.cat(preds, dim=1 if all_layers else 0)\n    return preds\n\n\ndef get_hash(model, num_layers, idf, rescale_with_baseline, use_custom_baseline, use_fast_tokenizer):\n    msg = \"{}_L{}{}_version={}(hug_trans={})\".format(\n        model, num_layers, \"_idf\" if idf else \"_no-idf\", __version__, trans_version\n    )\n    if rescale_with_baseline:\n        if use_custom_baseline:\n            msg += \"-custom-rescaled\"\n        else:\n            msg += \"-rescaled\"\n    if use_fast_tokenizer:\n        msg += \"_fast-tokenizer\"\n    return msg\n\n\ndef cache_scibert(model_type, cache_folder=\"~/.cache/torch/transformers\"):\n    if not model_type.startswith(\"scibert\"):\n        return model_type\n\n    underscore_model_type = model_type.replace(\"-\", \"_\")\n    cache_folder = os.path.abspath(os.path.expanduser(cache_folder))\n    filename = os.path.join(cache_folder, underscore_model_type)\n\n    # download SciBERT models\n    if not os.path.exists(filename):\n        cmd = f\"mkdir -p {cache_folder}; cd {cache_folder};\"\n        cmd += f\"wget {SCIBERT_URL_DICT[model_type]}; tar -xvf {underscore_model_type}.tar;\"\n        cmd += (\n            f\"rm -f {underscore_model_type}.tar ; cd {underscore_model_type}; tar -zxvf weights.tar.gz; mv weights/* .;\"\n        )\n        cmd += f\"rm -f weights.tar.gz; rmdir weights; mv bert_config.json config.json;\"\n        print(cmd)\n        print(f\"downloading {model_type} model\")\n        os.system(cmd)\n\n    # fix the missing files in scibert\n    json_file = os.path.join(filename, \"special_tokens_map.json\")\n    if not os.path.exists(json_file):\n        with open(json_file, \"w\") as f:\n            print(\n                '{\"unk_token\": \"[UNK]\", \"sep_token\": \"[SEP]\", \"pad_token\": \"[PAD]\", \"cls_token\": \"[CLS]\", \"mask_token\": \"[MASK]\"}',\n                file=f,\n            )\n\n    json_file = os.path.join(filename, \"added_tokens.json\")\n    if not os.path.exists(json_file):\n        with open(json_file, \"w\") as f:\n            print(\"{}\", file=f)\n\n    if \"uncased\" in model_type:\n        json_file = os.path.join(filename, \"tokenizer_config.json\")\n        if not os.path.exists(json_file):\n            with open(json_file, \"w\") as f:\n                print('{\"do_lower_case\": true, \"max_len\": 512, \"init_inputs\": []}', file=f)\n\n    return filename\n"}
{"type": "source_file", "path": "retriever/simcse/data_utils.py", "content": "import torch\nfrom typing import Dict\nfrom dataclasses import dataclass, field\n\n@dataclass\nclass OurDataCollatorWithPadding:\n    def __init__(self, pad_token_id, idf_dict):\n        self.pad_token_id = pad_token_id\n        self.idf_dict = idf_dict\n\n    def padding_func(self, arr, pad_token, dtype=torch.long):\n        lens = torch.LongTensor([len(a) for a in arr])\n        max_len = lens.max().item()\n        padded = torch.ones(len(arr), max_len, dtype=dtype) * pad_token\n        mask = torch.zeros(len(arr), max_len, dtype=torch.long)\n        for i, a in enumerate(arr):\n            padded[i, : lens[i]] = torch.tensor(a, dtype=dtype)\n            mask[i, : lens[i]] = 1\n        return padded, lens, mask\n\n    def negative_sample_mask(self, target_sent, dtype=torch.bool):\n        mask = torch.ones((len(target_sent), len(target_sent)), dtype=dtype)\n        for i in range(len(target_sent)):\n            s1 = target_sent[i]\n            for j in range(len(target_sent)):\n                s2 = target_sent[j]\n                if i != j and s1 == s2:\n                    mask[i, j] = 0\n        return mask\n\n    def __call__(self, batch) -> Dict[str, torch.Tensor]:\n        bs = len(batch)\n        assert bs\n        num_sent = len(batch[0]['input_ids'])\n        pad_token = self.pad_token_id\n\n        flat_input_ids = []\n        for sample in batch:\n            for i in range(num_sent):\n                flat_input_ids.append(sample['input_ids'][i])\n\n        flat_idf_weights = []\n        for input_ids in flat_input_ids:\n            cur_idf_weights = [self.idf_dict[id] for id in input_ids]\n            flat_idf_weights.append(cur_idf_weights)\n\n        padded, lens, mask = self.padding_func(flat_input_ids, pad_token, dtype=torch.long)\n        padded_idf, _, _ = self.padding_func(flat_idf_weights, 0, dtype=torch.float)\n        assert padded.shape == padded_idf.shape\n\n        target_sent = []\n        for sample in batch:\n            target_sent.append(sample['plain_text'][1])\n        negative_sample_mask = self.negative_sample_mask(target_sent)\n\n        # padded = padded.to(device=device)\n        # mask = mask.to(device=device)\n        # lens = lens.to(device=device)\n        # return padded, padded_idf, lens, mask\n        return {'input_ids': padded, 'attention_mask': mask, 'negative_sample_mask': negative_sample_mask,\n                'lengths': lens, 'input_idf': padded_idf, 'num_sent': num_sent}\n\ndef tok_sentences(tokenizer, sentences, has_hard_neg, total, max_length=None):\n    sent_features = tokenizer(\n        sentences,\n        add_special_tokens=True,\n        # add_prefix_space=True,\n        max_length=tokenizer.model_max_length if max_length is None else max_length,\n        truncation=True\n    )\n\n    features = {}\n    if has_hard_neg:\n        for key in sent_features:\n            features[key] = [[sent_features[key][i], sent_features[key][i + total], sent_features[key][i + total * 2]]\n                             for i in range(total)]\n\n    else:\n        for key in sent_features:\n            features[key] = [[sent_features[key][i], sent_features[key][i + total]] for i in range(total)]\n        # get the plain text\n        features['plain_text'] = []\n        for i in range(total):\n            features['plain_text'].append([sentences[i], sentences[i + total]])\n\n    return features\n\n"}
{"type": "source_file", "path": "retriever/simcse/model.py", "content": "import os\nimport time\nimport torch\nimport pandas as pd\nimport warnings\n\nfrom torch import nn\nfrom transformers import PreTrainedModel\nfrom collections import defaultdict\nfrom transformers.modeling_outputs import SequenceClassifierOutput\ntorch.set_printoptions(profile=\"full\")\n\nfrom utils import (\n    get_model,\n    get_tokenizer,\n    get_idf_dict,\n    bert_cos_score_idf,\n    model2layers,\n    get_hash,\n    greedy_cos_idf_for_train,\n    greedy_cos_idf\n)\n\n\nclass RetrievalModel(PreTrainedModel):\n    \"\"\"\n    Adapt the implementation of BertScore to calculate the similarity between a query and a doc\n    with either CLS mean pooling distance or BERTScore F1.\n    \"\"\"\n\n    def __init__(\n        self,\n            config: object,\n            model_type: object = None,\n            num_layers: object = None,\n            batch_size: object = 64,\n            nthreads: object = 4,\n            all_layers: object = False,\n            idf: object = False,\n            idf_sents: object = None,\n            device: object = None,\n            lang: object = None,\n            rescale_with_baseline: object = False,\n            baseline_path: object = None,\n            use_fast_tokenizer: object = False,\n            tokenizer: object = None,\n            training_args: object = None,\n            model_args: object = None\n    ) -> object:\n        super().__init__(config)\n        \"\"\"\n        Args:\n            - :param: `model_type` (str): contexual embedding model specification, default using the suggested\n                      model for the target langauge; has to specify at least one of\n                      `model_type` or `lang`\n            - :param: `num_layers` (int): the layer of representation to use.\n                      default using the number of layer tuned on WMT16 correlation data\n            - :param: `verbose` (bool): turn on intermediate status update\n            - :param: `idf` (bool): a booling to specify whether to use idf or not (this should be True even if `idf_sents` is given)\n            - :param: `idf_sents` (List of str): list of sentences used to compute the idf weights\n            - :param: `device` (str): on which the contextual embedding model will be allocated on.\n                      If this argument is None, the model lives on cuda:0 if cuda is available.\n            - :param: `batch_size` (int): bert score processing batch size\n            - :param: `nthreads` (int): number of threads\n            - :param: `lang` (str): language of the sentences; has to specify\n                      at least one of `model_type` or `lang`. `lang` needs to be\n                      specified when `rescale_with_baseline` is True.\n            - :param: `return_hash` (bool): return hash code of the setting\n            - :param: `rescale_with_baseline` (bool): rescale bertscore with pre-computed baseline\n            - :param: `baseline_path` (str): customized baseline file\n            - :param: `use_fast_tokenizer` (bool): `use_fast` parameter passed to HF tokenizer\n        \"\"\"\n\n        assert lang is not None or model_type is not None, \"Either lang or model_type should be specified\"\n        if rescale_with_baseline:\n            assert lang is not None, \"Need to specify Language when rescaling with baseline\"\n        self.training_args = training_args\n        self.model_args = model_args\n        self._lang = lang\n        self._rescale_with_baseline = rescale_with_baseline\n        self._idf = idf\n        self.batch_size = batch_size\n        self.nthreads = nthreads\n        self.all_layers = all_layers\n\n        assert model_type is not None\n        self._model_type = model_type\n\n        if num_layers is None:\n            self._num_layers = model2layers[self.model_type]\n        else:\n            self._num_layers = num_layers\n\n\n        self._use_fast_tokenizer = use_fast_tokenizer\n        self._tokenizer = get_tokenizer(self.model_type, self._use_fast_tokenizer) if tokenizer is None else tokenizer\n        self._model = get_model(self.model_type, self.num_layers, self.all_layers)\n\n        self._idf_dict = None\n        if idf_sents is not None:\n            self.compute_idf(idf_sents)\n\n        self._baseline_vals = None\n        self.baseline_path = baseline_path\n        self.use_custom_baseline = self.baseline_path is not None\n        if self.baseline_path is None:\n            self.baseline_path = os.path.join(\n                os.path.dirname(__file__), f\"rescale_baseline/{self.lang}/{self.model_type}.tsv\"\n            )\n\n    @property\n    def lang(self):\n        return self._lang\n\n    @property\n    def idf(self):\n        return self._idf\n\n    @property\n    def model_type(self):\n        return self._model_type\n\n    @property\n    def num_layers(self):\n        return self._num_layers\n\n    @property\n    def rescale_with_baseline(self):\n        return self._rescale_with_baseline\n\n    @property\n    def baseline_vals(self):\n        if self._baseline_vals is None:\n            if os.path.isfile(self.baseline_path):\n                if not self.all_layers:\n                    self._baseline_vals = torch.from_numpy(\n                        pd.read_csv(self.baseline_path).iloc[self.num_layers].to_numpy()\n                    )[1:].float()\n                else:\n                    self._baseline_vals = (\n                        torch.from_numpy(pd.read_csv(self.baseline_path).to_numpy())[:, 1:].unsqueeze(1).float()\n                    )\n            else:\n                raise ValueError(f\"Baseline not Found for {self.model_type} on {self.lang} at {self.baseline_path}\")\n\n        return self._baseline_vals\n\n    @property\n    def use_fast_tokenizer(self):\n        return self._use_fast_tokenizer\n\n    @property\n    def hash(self):\n        return get_hash(\n            self.model_type, self.num_layers, self.idf, self.rescale_with_baseline, self.use_custom_baseline, self.use_fast_tokenizer\n        )\n\n    def compute_idf(self, sents):\n        \"\"\"\n        Args:\n\n        \"\"\"\n        if self._idf_dict is not None:\n            warnings.warn(\"Overwriting the previous importance weights.\")\n\n        self._idf_dict = get_idf_dict(sents, self._tokenizer, nthreads=self.nthreads)\n\n    def get_pooling_embedding(self, input_ids, attention_mask, lengths, pooling=\"mean\", normalize=False):\n        out = self._model(input_ids, attention_mask=attention_mask, output_hidden_states=self.all_layers)\n        if self.all_layers:\n            emb = torch.stack(out[-1], dim=2)\n        else:\n            emb = out[0]\n        if pooling == \"mean\":\n            emb.masked_fill_(~attention_mask.bool().unsqueeze(-1), 2)\n            max_len = max(lengths)\n            base = torch.arange(max_len, dtype=torch.long).expand(len(lengths), max_len).to(lengths.device)\n            pad_mask = base < lengths.unsqueeze(1)\n            emb = (emb * pad_mask.unsqueeze(-1)).sum(dim=1) / pad_mask.sum(-1).unsqueeze(-1)\n            if normalize:\n                emb = emb / emb.norm(dim=1, keepdim=True)\n        else:\n            raise NotImplementedError\n        return emb\n\n\n    def calc_pair_tok_embedding(self, input_ids, attention_mask, lengths=None, input_idf=None, num_sent=None):\n\n        batch_size = int(input_ids.shape[0] / num_sent)\n        # calc embeddings\n        out = self._model(input_ids, attention_mask=attention_mask, output_hidden_states=self.all_layers)\n        if self.all_layers:\n            emb = torch.stack(out[-1], dim=2)\n        else:\n            emb = out[0]\n\n        def split_to_pair(m):\n            dim_size = len(m.shape)\n            if dim_size == 1:\n                m = m.view(batch_size, num_sent)\n            elif dim_size == 2:\n                m = m.view(batch_size, num_sent, -1)\n            elif dim_size == 3:\n                max_len = m.size(-2)\n                m = m.view(batch_size, num_sent, max_len, -1)\n            else:\n                raise ValueError('dimension should be only 2 or 3')\n            return m[:, 0], m[:, 1]\n\n        def length_to_mask(lens, max_len):\n            base = torch.arange(max_len, dtype=torch.long).expand(len(lens), max_len).to(lens.device)\n            return base < lens.unsqueeze(1)\n\n        ref_emb, hyp_emb = split_to_pair(emb)\n        ref_att_mask, hyp_attn_mask = split_to_pair(attention_mask)\n        ref_len, hyp_len = split_to_pair(lengths)\n        ref_idf, hyp_idf = split_to_pair(input_idf)\n\n        # pad to calculate greedy cos idf\n        # emb_pad: padding with value 2\n        ref_emb.masked_fill_(~ref_att_mask.bool().unsqueeze(-1), 2)\n        hyp_emb.masked_fill_(~hyp_attn_mask.bool().unsqueeze(-1), 2)\n        # idf_pad: padding with value 0 (already satisfied)\n        # pad_mask: length mask\n        max_len = max(max(ref_len), max(hyp_len))\n        ref_pad_mask = length_to_mask(ref_len, max_len)\n        hyp_pad_mask = length_to_mask(hyp_len, max_len)\n\n        return ref_emb, ref_pad_mask, ref_idf, hyp_emb, hyp_pad_mask, hyp_idf\n\n    def forward(self, input_ids=None, attention_mask=None, negative_sample_mask=None,\n                lengths=None, input_idf=None,\n                num_sent=None, pairwise_similarity=False):\n        ref_emb, ref_pad_mask, ref_idf, \\\n        hyp_emb, hyp_pad_mask, hyp_idf = self.calc_pair_tok_embedding(input_ids, attention_mask,\n                                                             lengths, input_idf, num_sent)\n\n        if 'cls_distance' in self.model_args.sim_func:\n            # ref_emb [B, max_len, embed_size]\n            # ref_pad_mask [B, max_len]\n            # m_ref_emb [B, embed_size]\n            m_ref_emb = (ref_emb * ref_pad_mask.unsqueeze(-1)).sum(dim=1) / ref_pad_mask.sum(-1).unsqueeze(-1)\n            m_hyp_emb = (hyp_emb * hyp_pad_mask.unsqueeze(-1)).sum(dim=1) / hyp_pad_mask.sum(-1).unsqueeze(-1)\n\n            if 'cosine' in self.model_args.sim_func:\n                cos_sim = nn.CosineSimilarity(dim=-1)\n                sim_score = cos_sim(m_ref_emb.unsqueeze(1), m_hyp_emb.unsqueeze(0))\n            elif 'l2' in self.model_args.sim_func:\n                sim_score = torch.matmul(m_ref_emb, m_hyp_emb.transpose(0, 1))\n            else:\n                raise NotImplementedError\n\n            if pairwise_similarity: # pairwise score only\n                return torch.diagonal(sim_score, 0)\n            else:\n                loss_fct = nn.CrossEntropyLoss(reduction='mean')\n                labels = torch.arange(sim_score.size(0)).long().to(sim_score.device)\n                # mask the conflict negative examples\n                sim_score.masked_fill_(~negative_sample_mask, -1e10)\n                sim_score = sim_score / self.model_args.temp\n                loss = loss_fct(sim_score, labels)\n                return SequenceClassifierOutput(loss=loss)\n\n        elif self.model_args.sim_func == 'bertscore':\n            if pairwise_similarity:\n                _, _, sim_score = greedy_cos_idf(ref_emb, ref_pad_mask, ref_idf,\n                                      hyp_emb, hyp_pad_mask, hyp_idf,\n                                      self.all_layers)\n                return sim_score\n            else:\n                _, _, sim_score = greedy_cos_idf_for_train(ref_emb, ref_pad_mask, ref_idf,\n                                          hyp_emb, hyp_pad_mask, hyp_idf,\n                                          self.all_layers)\n                labels = torch.arange(sim_score.size(0)).long().to(sim_score.device)\n                # print(sim_score)\n                if self.model_args.bert_score_loss == 'hinge':\n                    loss_fct = nn.MultiMarginLoss(margin=self.model_args.hinge_margin, reduction='none')\n                    sim_score.masked_fill_(~negative_sample_mask, -1e10)\n                    loss = loss_fct(sim_score, labels)\n                    loss = loss * sim_score.shape[1] / negative_sample_mask.long().sum(-1) # recover x.size(0)\n                    loss = torch.mean(loss)\n                else:\n                    loss_fct = nn.CrossEntropyLoss(reduction='mean')\n                    sim_score.masked_fill_(~negative_sample_mask, -1e10)\n                    sim_score = sim_score / self.model_args.temp\n                    loss = loss_fct(sim_score, labels)\n                # loss_fct = nn.CrossEntropyLoss()\n                return SequenceClassifierOutput(loss = loss)\n\n        else:\n            raise NotImplementedError\n\n\n\n    def score(self, cands, refs, verbose=False, batch_size=64, return_hash=False):\n        \"\"\"\n        Args:\n            - :param: `cands` (list of str): candidate sentences\n            - :param: `refs` (list of str or list of list of str): reference sentences\n\n        Return:\n            - :param: `(P, R, F)`: each is of shape (N); N = number of input\n                      candidate reference pairs. if returning hashcode, the\n                      output will be ((P, R, F), hashcode). If a candidate have\n                      multiple references, the returned score of this candidate is\n                      the *best* score among all references.\n        \"\"\"\n\n        ref_group_boundaries = None\n        if not isinstance(refs[0], str):\n            ref_group_boundaries = []\n            ori_cands, ori_refs = cands, refs\n            cands, refs = [], []\n            count = 0\n            for cand, ref_group in zip(ori_cands, ori_refs):\n                cands += [cand] * len(ref_group)\n                refs += ref_group\n                ref_group_boundaries.append((count, count + len(ref_group)))\n                count += len(ref_group)\n\n        if verbose:\n            print(\"calculating scores...\")\n            start = time.perf_counter()\n\n        if self.idf:\n            assert self._idf_dict, \"IDF weights are not computed\"\n            idf_dict = self._idf_dict\n        else:\n            idf_dict = defaultdict(lambda: 1.0)\n            idf_dict[self._tokenizer.sep_token_id] = 0\n            idf_dict[self._tokenizer.cls_token_id] = 0\n\n        all_preds = bert_cos_score_idf(\n            self._model,\n            refs,\n            cands,\n            self._tokenizer,\n            idf_dict,\n            verbose=verbose,\n            device=self.device,\n            batch_size=batch_size,\n            all_layers=self.all_layers,\n        ).cpu()\n\n        if ref_group_boundaries is not None:\n            max_preds = []\n            for start, end in ref_group_boundaries:\n                max_preds.append(all_preds[start:end].max(dim=0)[0])\n            all_preds = torch.stack(max_preds, dim=0)\n\n        if self.rescale_with_baseline:\n            all_preds = (all_preds - self.baseline_vals) / (1 - self.baseline_vals)\n\n        out = all_preds[..., 0], all_preds[..., 1], all_preds[..., 2]  # P, R, F\n\n        if verbose:\n            time_diff = time.perf_counter() - start\n            print(f\"done in {time_diff:.2f} seconds, {len(refs) / time_diff:.2f} sentences/sec\")\n\n        if return_hash:\n            out = tuple([out, self.hash])\n\n        return out\n\n\n    def __repr__(self):\n        return f\"{self.__class__.__name__}(hash={self.hash}, batch_size={self.batch_size}, nthreads={self.nthreads})\"\n\n    def __str__(self):\n        return self.__repr__()\n"}
{"type": "source_file", "path": "scripts/tldr_gpt_neo.py", "content": "from transformers import AutoModelForCausalLM, AutoTokenizer\nimport json\n\ndevice = 'cuda:0'\nbase_model_name = 'neulab/docprompting-tldr-gpt-neo-1.3B'\ntokenizer = AutoTokenizer.from_pretrained(base_model_name)\nmodel = AutoModelForCausalLM.from_pretrained(base_model_name)\nmodel = model.to(device)\n\nwith open('data/tldr/fid.cmd_dev.codet5.t10.json', 'r') as f:\n    examples = json.load(f)\n\nfor example in examples:\n    manual_list = [doc['text'] for doc in example['ctxs']]\n    manual_list = \"\\n\".join(manual_list).strip()\n    nl = example['question']\n    prompt = f'{tokenizer.bos_token} {manual_list} '\n    prompt += f'{tokenizer.sep_token} {nl} {tokenizer.sep_token}'\n    print(prompt)\n\n    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n    input_ids = input_ids.to(device)\n    gen_tokens = model.generate(\n        input_ids,\n        num_beams=5,\n        max_new_tokens=150,\n        num_return_sequences=2,\n        pad_token_id=tokenizer.eos_token_id\n    )\n    gen_tokens = gen_tokens.reshape(1, -1, gen_tokens.shape[-1])[0][0]\n    gen_text = tokenizer.decode(gen_tokens)\n    # parse\n    gen_text = gen_text.split(tokenizer.sep_token)[2].strip().split(tokenizer.eos_token)[0].strip()\n    print(gen_text)\n"}
{"type": "source_file", "path": "retriever/simcse/trainers.py", "content": "import collections\nimport glob\nimport inspect\nimport math\nimport sys\nimport os\nimport re\nimport json\nimport shutil\nimport time\nimport warnings\nfrom pathlib import Path\nimport importlib.util\nfrom packaging import version\nfrom tqdm import tqdm\nfrom transformers import Trainer\nfrom transformers.modeling_utils import PreTrainedModel\nfrom transformers.training_args import ParallelMode, TrainingArguments\nfrom transformers.utils import logging\nfrom transformers.trainer_utils import (\n    PREFIX_CHECKPOINT_DIR,\n    BestRun,\n    EvalPrediction,\n    HPSearchBackend,\n    PredictionOutput,\n    TrainOutput,\n    default_compute_objective,\n    default_hp_space,\n    set_seed,\n    speed_metrics,\n)\nfrom transformers.file_utils import (\n    WEIGHTS_NAME,\n    is_apex_available,\n    is_datasets_available,\n    is_in_notebook,\n    is_torch_tpu_available,\n)\nfrom transformers.trainer_callback import (\n    CallbackHandler,\n    DefaultFlowCallback,\n    PrinterCallback,\n    ProgressCallback,\n    TrainerCallback,\n    TrainerControl,\n    TrainerState,\n)\nfrom transformers.trainer_pt_utils import (\n    reissue_pt_warnings,\n)\n\nfrom transformers.utils import logging\nfrom transformers.data.data_collator import DataCollator, DataCollatorWithPadding, default_data_collator\nimport torch\nimport torch.nn as nn\nfrom typing import TYPE_CHECKING, Any, Callable, Dict, List, Optional, Tuple, Union\nfrom torch.utils.data.dataloader import DataLoader\nfrom torch.utils.data.dataset import Dataset\nfrom torch.utils.data.distributed import DistributedSampler\nfrom torch.utils.data.sampler import RandomSampler, SequentialSampler\n\nif is_torch_tpu_available():\n    import torch_xla.core.xla_model as xm\n    import torch_xla.debug.metrics as met\n    import torch_xla.distributed.parallel_loader as pl\n\nif is_apex_available():\n    from apex import amp\n\nif version.parse(torch.__version__) >= version.parse(\"1.6\"):\n    _is_native_amp_available = True\n    from torch.cuda.amp import autocast\n\nif is_datasets_available():\n    import datasets\n\nfrom transformers.trainer import _model_unwrap\nfrom transformers.optimization import Adafactor, AdamW, get_scheduler\nimport copy\n\n# Set path to SentEval\nPATH_TO_SENTEVAL = './SentEval'\nPATH_TO_DATA = './SentEval/data'\n\n# Import SentEval\nsys.path.insert(0, PATH_TO_SENTEVAL)\nimport numpy as np\nfrom run_inference import config as retrieval_config\nfrom retriever.eval import eval_retrieval_from_file, eval_hit_from_file\n\nlogger = logging.get_logger(__name__)\n\n\nclass CLTrainer(Trainer):\n\n    def post_processing(self):\n\n        if self.is_world_process_zero():\n            # rename the best model\n            best_ep = \\\n            sorted(self.epoch_metric.items(), key=lambda x: x[1][f\"eval_{self.metric_for_best_model}\"], reverse=True)[\n                0][0]\n            src_fd = f\"{self.args.output_dir}/{PREFIX_CHECKPOINT_DIR}-{best_ep}\"\n            tgt_fd = f\"{self.args.output_dir}/{PREFIX_CHECKPOINT_DIR}-best\"\n            shutil.move(src_fd, tgt_fd)\n            logger.info(f'rename {src_fd} to {tgt_fd}')\n            for fd in os.listdir(self.args.output_dir):\n                if 'checkpoint' in fd and '-best' not in fd:\n                    shutil.rmtree(f\"{self.args.output_dir}/{fd}\")\n                    logger.info(f'remove {fd}')\n            src_r_file = f\"{self.tmp_result_folder}/result_file_{best_ep}.json\"\n            tgt_r_file = f'{tgt_fd}/dev_result.json'\n            shutil.copyfile(src_r_file, tgt_r_file)\n            logger.info(f\"copy the best file from {src_r_file} to {tgt_r_file}\")\n\n    def evaluate(self) -> Dict[str, float]:\n        \"\"\"use the same full retrieval pool for validation.\n        The function calls run_inference.py to have the exact same validation procedure as the inference.\"\"\"\n\n        if self.is_world_process_zero():\n\n            # SentEval prepare and batcher\n            def prepare(params, samples):\n                return\n\n            from run_inference import CodeT5Retriever\n            if self.args.eval_form == 'retrieval':\n                tmp_folder = f\"{self.args.output_dir}/tmp_eval\"\n                self.tmp_result_folder = tmp_folder\n                os.makedirs(tmp_folder, exist_ok=True)\n\n                self.model.eval()\n                eval_config_str = [\n                    f\"--source_embed_save_file {tmp_folder}/src_embedding\",\n                    f\"--target_embed_save_file {tmp_folder}/tgt_embedding\",\n                    f\"--save_file {tmp_folder}/result_file_{self.state.global_step}.json\",\n                    f\"--source_file {self.args.eval_src_file}\",\n                    f\"--target_file {self.args.eval_tgt_file}\",\n                    '--log_level non',\n                    f'--model_name checkpoint-{self.state.global_step}'\n                ]\n                eval_config_str = \" \".join(eval_config_str)\n                eval_config = retrieval_config(eval_config_str)\n                eval_config.root_folder = self.args.eval_root_folder\n\n                assert self.args.eval_retriever == 't5'\n                se = CodeT5Retriever(eval_config)\n\n                if 'recall' in self.args.metric_for_best_model:\n                    metric_func = eval_retrieval_from_file\n                    flag = 'recall'\n                elif 'hit' in self.args.metric_for_best_model:\n                    metric_func = eval_hit_from_file\n                    flag = 'hit'\n                else:\n                    raise NotImplementedError(self.args.metric_for_best_model)\n\n                se.prepare_model(self.model, self.tokenizer)\n                se.encode_file(eval_config.source_file, eval_config.source_embed_save_file, normalize_embed=False,\n                               from_training=True)\n                se.encode_file(eval_config.target_file, eval_config.target_embed_save_file, normalize_embed=False,\n                               from_training=True)\n                se.retrieve(eval_config.source_embed_save_file,\n                            eval_config.target_embed_save_file, eval_config.source_idx_file,\n                            eval_config.target_idx_file, eval_config.top_k, eval_config.save_file)\n\n                metrics_1 = metric_func(\n                    f\"{self.args.eval_oracle_file}\",\n                    eval_config.save_file)\n\n                if flag == 'recall':\n                    se.encode_file(eval_config.source_file, eval_config.source_embed_save_file,\n                                   normalize_embed=True, from_training=True)\n                    se.encode_file(eval_config.target_file, eval_config.target_embed_save_file,\n                                   normalize_embed=True, from_training=True)\n                    se.retrieve(eval_config.source_embed_save_file,\n                                eval_config.target_embed_save_file, eval_config.source_idx_file,\n                                eval_config.target_idx_file, eval_config.top_k, eval_config.save_file)\n\n                    metrics_2 = metric_func(\n                        f\"{eval_config.root_folder}/{self.args.eval_oracle_file}\",\n                        eval_config.save_file)\n\n                    if metrics_1['recall'][10] > metrics_2['recall'][10]:\n                        metrics = metrics_1\n                    else:\n                        metrics = metrics_2\n\n                    r = metrics['recall'][10]\n                    p = metrics['precision'][10]\n                    metrics = {\"eval_recall@10\": r,\n                               \"eval_precision@10\": p,\n                               \"eval_f1@10\": (2 * r * p) / (r + p + 1e-10)}\n                else:\n                    metrics = {'eval_hit@1': metrics_1['hit'][1],\n                               'eval_hit@10': metrics_1['hit'][10]}\n\n            elif self.args.eval_form == 'reranking':\n                from data_utils import tok_sentences\n                self.model.eval()\n\n                with open(self.args.eval_file, 'r') as f:\n                    dataset = []\n                    for line in f:\n                        dataset.append(json.loads(line.strip()))\n                    logger.info(f'size of the eval data: {len(dataset)}')\n\n                def prepare_features(examples):\n                    total = len(examples)\n                    sentences = [x['text1'] for x in examples] + [x['text2'] for x in examples]\n                    features = tok_sentences(self.tokenizer, sentences, has_hard_neg=False, total=total)\n                    # convert them to batch\n                    batch = [{} for _ in range(total)]\n                    for k, v in features.items():\n                        for b_idx, vv in enumerate(v):\n                            batch[b_idx][k] = vv\n                    return batch\n\n                bs = 48\n                f1_list = []\n                with torch.no_grad():\n                    for i in tqdm(range(0, len(dataset), bs)):\n                        batch = dataset[i: i + bs]\n                        batch = prepare_features(batch)\n                        padded_batch = self.data_collator(batch)\n                        for k in padded_batch:\n                            if isinstance(padded_batch[k], torch.Tensor):\n                                padded_batch[k] = padded_batch[k].to(self.args.device)\n                        F1 = self.model(**padded_batch, pairwise_similarity=True).detach().cpu().tolist()\n                        f1_list += F1\n\n                results = collections.defaultdict(lambda: {'retrieved': [], 'score': []})\n                assert len(f1_list) == len(dataset)\n                for sample, prob in zip(dataset, f1_list):\n                    results[sample['question_id']]['retrieved'].append(sample['function_key'])\n                    results[sample['question_id']]['score'].append(prob)\n\n                for k, v in results.items():\n                    sorted_idx = np.argsort(v['score'])[::-1]\n                    results[k]['score'] = [results[k]['score'][x] for x in sorted_idx]\n                    results[k]['retrieved'] = [results[k]['retrieved'][x] for x in sorted_idx]\n\n                root_folder = \"/home/shuyanzh/workshop/op_agent\"\n                tmp_folder = f\"{root_folder}/data/para_data/simcse/eval_tmp_store.{self.args.tmp_tag}\"\n                save_file = f\"{tmp_folder}/result_file_{self.state.global_step}.json\"\n                with open(save_file, 'w+') as f:\n                    json.dump(results, f, indent=2)\n\n                metrics = eval_retrieval_from_file(f\"{root_folder}/data/conala/nl.cm/cmd_dev.oracle_man.full.json\",\n                                                   save_file)\n                r = metrics['recall'][10]\n                p = metrics['precision'][10]\n                metrics = {\"eval_recall@10\": r,\n                           \"eval_precision@10\": p,\n                           \"eval_f1@10\": (2 * r * p) / (r + p + 1e-10)}\n\n            else:\n                raise NotImplementedError\n\n            # if eval_senteval_transfer or self.args.eval_transfer:\n            #     avg_transfer = 0\n            #     for task in ['MR', 'CR', 'SUBJ', 'MPQA', 'SST2', 'TREC', 'MRPC']:\n            #         avg_transfer += results[task]['devacc']\n            #         metrics['eval_{}'.format(task)] = results[task]['devacc']\n            #     avg_transfer /= 7\n            #     metrics['eval_avg_transfer'] = avg_transfer\n\n            self.log(metrics)\n            self.epoch_metric[self.state.global_step] = metrics\n            return metrics\n\n    def train(self, model_path: Optional[str] = None, trial: Union[\"optuna.Trial\", Dict[str, Any]] = None):\n        \"\"\"\n        Main training entry point.\n\n        Args:\n            model_path (:obj:`str`, `optional`):\n                Local path to the model if the model to train has been instantiated from a local path. If present,\n                training will resume from the optimizer/scheduler states loaded here.\n            trial (:obj:`optuna.Trial` or :obj:`Dict[str, Any]`, `optional`):\n                The trial run or the hyperparameter dictionary for hyperparameter search.\n\n        The main difference between ours and Huggingface's original implementation is that we\n        also load model_args when reloading best checkpoints for evaluation.\n        \"\"\"\n        # This might change the seed so needs to run first.\n        self._hp_search_setup(trial)\n\n        # Model re-init\n        if self.model_init is not None:\n            # Seed must be set before instantiating the model when using model_init.\n            set_seed(self.args.seed)\n\n            model = self.call_model_init(trial)\n            if not self.is_model_parallel:\n                model = model.to(self.args.device)\n\n            self.model = model\n            self.model_wrapped = model\n\n            # Reinitializes optimizer and scheduler\n            self.optimizer, self.lr_scheduler = None, None\n\n        # Keeping track whether we can can len() on the dataset or not\n        train_dataset_is_sized = isinstance(self.train_dataset, collections.abc.Sized)\n\n        # Data loader and number of training steps\n        train_dataloader = self.get_train_dataloader()\n\n        # Setting up training control variables:\n        # number of training epochs: num_train_epochs\n        # number of training steps per epoch: num_update_steps_per_epoch\n        # total number of training steps to execute: max_steps\n        if train_dataset_is_sized:\n            num_update_steps_per_epoch = len(train_dataloader) // self.args.gradient_accumulation_steps\n            num_update_steps_per_epoch = max(num_update_steps_per_epoch, 1)\n            if self.args.max_steps > 0:\n                max_steps = self.args.max_steps\n                num_train_epochs = self.args.max_steps // num_update_steps_per_epoch + int(\n                    self.args.max_steps % num_update_steps_per_epoch > 0\n                )\n            else:\n                max_steps = math.ceil(self.args.num_train_epochs * num_update_steps_per_epoch)\n                num_train_epochs = math.ceil(self.args.num_train_epochs)\n        else:\n            # see __init__. max_steps is set when the dataset has no __len__\n            max_steps = self.args.max_steps\n            num_train_epochs = 1\n            num_update_steps_per_epoch = max_steps\n\n        if self.args.deepspeed:\n            model, optimizer, lr_scheduler = init_deepspeed(self, num_training_steps=max_steps)\n            self.model = model.module\n            self.model_wrapped = model  # will get further wrapped in DDP\n            self.deepspeed = model  # DeepSpeedEngine object\n            self.optimizer = optimizer\n            self.lr_scheduler = lr_scheduler\n        else:\n            self.create_optimizer_and_scheduler(num_training_steps=max_steps)\n\n        self.state = TrainerState()\n        self.state.is_hyper_param_search = trial is not None\n\n        # Check if saved optimizer or scheduler states exist\n        self._load_optimizer_and_scheduler(model_path)\n\n        model = self.model_wrapped\n\n        # Mixed precision training with apex (torch < 1.6)\n        if self.use_apex:\n            model, self.optimizer = amp.initialize(model, self.optimizer, opt_level=self.args.fp16_opt_level)\n\n        # Multi-gpu training (should be after apex fp16 initialization)\n        if self.args.n_gpu > 1:\n            model = torch.nn.DataParallel(model)\n\n        # Distributed training (should be after apex fp16 initialization)\n        if self.sharded_dpp:\n            model = ShardedDDP(model, self.optimizer)\n        elif self.args.local_rank != -1:\n            model = torch.nn.parallel.DistributedDataParallel(\n                model,\n                device_ids=[self.args.local_rank],\n                output_device=self.args.local_rank,\n                find_unused_parameters=(\n                    not getattr(model.config, \"gradient_checkpointing\", False)\n                    if isinstance(model, PreTrainedModel)\n                    else True\n                ),\n            )\n            # find_unused_parameters breaks checkpointing as per\n            # https://github.com/huggingface/transformers/pull/4659#issuecomment-643356021\n\n        # for the rest of this function `model` is the outside model, whether it was wrapped or not\n        if model is not self.model:\n            self.model_wrapped = model\n\n        # important: at this point:\n        # self.model         is the Transformers Model\n        # self.model_wrapped is DDP(Transformers Model), DDP(Deepspeed(Transformers Model)), etc.\n\n        # Train!\n        if is_torch_tpu_available():\n            total_train_batch_size = self.args.train_batch_size * xm.xrt_world_size()\n        else:\n            total_train_batch_size = (\n                    self.args.train_batch_size\n                    * self.args.gradient_accumulation_steps\n                    * (torch.distributed.get_world_size() if self.args.local_rank != -1 else 1)\n            )\n\n        num_examples = (\n            self.num_examples(train_dataloader)\n            if train_dataset_is_sized\n            else total_train_batch_size * self.args.max_steps\n        )\n\n        logger.info(\"***** Running training *****\")\n        logger.info(f\"  Num examples = {num_examples}\")\n        logger.info(f\"  Num Epochs = {num_train_epochs}\")\n        logger.info(f\"  Instantaneous batch size per device = {self.args.per_device_train_batch_size}\")\n        logger.info(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_train_batch_size}\")\n        logger.info(f\"  Gradient Accumulation steps = {self.args.gradient_accumulation_steps}\")\n        logger.info(f\"  Total optimization steps = {max_steps}\")\n\n        self.state.epoch = 0\n        start_time = time.time()\n        epochs_trained = 0\n        steps_trained_in_current_epoch = 0\n\n        # Check if continuing training from a checkpoint\n        if model_path and os.path.isfile(os.path.join(model_path, \"trainer_state.json\")):\n            self.state = TrainerState.load_from_json(os.path.join(model_path, \"trainer_state.json\"))\n            epochs_trained = self.state.global_step // num_update_steps_per_epoch\n            if not self.args.ignore_data_skip:\n                steps_trained_in_current_epoch = self.state.global_step % (num_update_steps_per_epoch)\n                steps_trained_in_current_epoch *= self.args.gradient_accumulation_steps\n            else:\n                steps_trained_in_current_epoch = 0\n\n            logger.info(\"  Continuing training from checkpoint, will skip to saved global_step\")\n            logger.info(f\"  Continuing training from epoch {epochs_trained}\")\n            logger.info(f\"  Continuing training from global step {self.state.global_step}\")\n            if not self.args.ignore_data_skip:\n                logger.info(\n                    f\"  Will skip the first {epochs_trained} epochs then the first {steps_trained_in_current_epoch} \"\n                    \"batches in the first epoch.\"\n                )\n\n        # Update the references\n        self.callback_handler.model = self.model\n        self.callback_handler.optimizer = self.optimizer\n        self.callback_handler.lr_scheduler = self.lr_scheduler\n        self.callback_handler.train_dataloader = train_dataloader\n        self.state.trial_name = self.hp_name(trial) if self.hp_name is not None else None\n        self.state.trial_params = hp_params(trial) if trial is not None else None\n        # This should be the same if the state has been saved but in case the training arguments changed, it's safer\n        # to set this after the load.\n        self.state.max_steps = max_steps\n        self.state.num_train_epochs = num_train_epochs\n        self.state.is_local_process_zero = self.is_local_process_zero()\n        self.state.is_world_process_zero = self.is_world_process_zero()\n\n        # tr_loss is a tensor to avoid synchronization of TPUs through .item()\n        tr_loss = torch.tensor(0.0).to(self.args.device)\n        # _total_loss_scalar is updated everytime .item() has to be called on tr_loss and stores the sum of all losses\n        self._total_loss_scalar = 0.0\n        self._globalstep_last_logged = 0\n        self._total_flos = self.state.total_flos\n        model.zero_grad()\n\n        self.control = self.callback_handler.on_train_begin(self.args, self.state, self.control)\n\n        # Skip the first epochs_trained epochs to get the random state of the dataloader at the right point.\n        if not self.args.ignore_data_skip:\n            for epoch in range(epochs_trained):\n                # We just need to begin an iteration to create the randomization of the sampler.\n                for _ in train_dataloader:\n                    break\n\n        for epoch in range(epochs_trained, num_train_epochs):\n            if isinstance(train_dataloader, DataLoader) and isinstance(train_dataloader.sampler, DistributedSampler):\n                train_dataloader.sampler.set_epoch(epoch)\n            epoch_iterator = train_dataloader\n\n            # Reset the past mems state at the beginning of each epoch if necessary.\n            if self.args.past_index >= 0:\n                self._past = None\n\n            steps_in_epoch = len(train_dataloader) if train_dataset_is_sized else self.args.max_steps\n            self.control = self.callback_handler.on_epoch_begin(self.args, self.state, self.control)\n\n            assert train_dataset_is_sized, \"currently we only support sized dataloader!\"\n\n            inputs = None\n            last_inputs = None\n            for step, inputs in enumerate(epoch_iterator):\n                # Skip past any already trained steps if resuming training\n                if steps_trained_in_current_epoch > 0:\n                    steps_trained_in_current_epoch -= 1\n                    continue\n\n                if (step + 1) % self.args.gradient_accumulation_steps == 0:\n                    self.control = self.callback_handler.on_step_begin(self.args, self.state, self.control)\n\n                if ((step + 1) % self.args.gradient_accumulation_steps != 0) and self.args.local_rank != -1:\n                    # Avoid unnecessary DDP synchronization since there will be no backward pass on this example.\n                    with model.no_sync():\n                        tr_loss += self.training_step(model, inputs)\n                else:\n                    tr_loss += self.training_step(model, inputs)\n                self._total_flos += self.floating_point_ops(inputs)\n\n                if (step + 1) % self.args.gradient_accumulation_steps == 0 or (\n                        # last step in epoch but step is always smaller than gradient_accumulation_steps\n                        steps_in_epoch <= self.args.gradient_accumulation_steps\n                        and (step + 1) == steps_in_epoch\n                ):\n                    # Gradient clipping\n                    if self.args.max_grad_norm is not None and self.args.max_grad_norm > 0 and not self.deepspeed:\n                        # deepspeed does its own clipping\n\n                        if self.use_amp:\n                            # AMP: gradients need unscaling\n                            self.scaler.unscale_(self.optimizer)\n\n                        if hasattr(self.optimizer, \"clip_grad_norm\"):\n                            # Some optimizers (like the sharded optimizer) have a specific way to do gradient clipping\n                            self.optimizer.clip_grad_norm(self.args.max_grad_norm)\n                        else:\n                            # Revert to normal clipping otherwise, handling Apex or full precision\n                            torch.nn.utils.clip_grad_norm_(\n                                amp.master_params(self.optimizer) if self.use_apex else model.parameters(),\n                                self.args.max_grad_norm,\n                            )\n\n                    # Optimizer step\n                    if is_torch_tpu_available():\n                        xm.optimizer_step(self.optimizer)\n                    elif self.use_amp:\n                        self.scaler.step(self.optimizer)\n                        self.scaler.update()\n                    else:\n                        self.optimizer.step()\n\n                    self.lr_scheduler.step()\n\n                    model.zero_grad()\n\n                    self.state.global_step += 1\n                    self.state.epoch = epoch + (step + 1) / steps_in_epoch\n                    self.control = self.callback_handler.on_step_end(self.args, self.state, self.control)\n                    self._maybe_log_save_evaluate(tr_loss, model, trial, epoch)\n\n                if self.control.should_epoch_stop or self.control.should_training_stop:\n                    break\n\n            self.control = self.callback_handler.on_epoch_end(self.args, self.state, self.control)\n            self._maybe_log_save_evaluate(tr_loss, model, trial, epoch)\n\n            if self.args.tpu_metrics_debug or self.args.debug:\n                if is_torch_tpu_available():\n                    # tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\n                    xm.master_print(met.metrics_report())\n                else:\n                    logger.warning(\n                        \"You enabled PyTorch/XLA debug metrics but you don't have a TPU \"\n                        \"configured. Check your training configuration if this is unexpected.\"\n                    )\n            if self.control.should_training_stop:\n                break\n\n        if self.args.past_index and hasattr(self, \"_past\"):\n            # Clean the state at the end of training\n            delattr(self, \"_past\")\n\n        logger.info(\"\\n\\nTraining completed. Do not forget to share your model on huggingface.co/models =)\\n\\n\")\n        self.post_processing()\n\n        # if self.args.load_best_model_at_end and self.state.best_model_checkpoint is not None:\n        #     logger.info(\n        #         f\"Loading best model from {self.state.best_model_checkpoint} (score: {self.state.best_metric}).\"\n        #     )\n        #     if isinstance(self.model, PreTrainedModel):\n        #         self.model = self.model.from_pretrained(self.state.best_model_checkpoint, model_args=self.model_args)\n        #         if not self.is_model_parallel:\n        #             self.model = self.model.to(self.args.device)\n        #     else:\n        #         state_dict = torch.load(os.path.join(self.state.best_model_checkpoint, WEIGHTS_NAME))\n        #         self.model.load_state_dict(state_dict)\n        #\n        #     if self.deepspeed:\n        #         self.deepspeed.load_checkpoint(\n        #             self.state.best_model_checkpoint, load_optimizer_states=False, load_lr_scheduler_states=False\n        #         )\n\n        metrics = speed_metrics(\"train\", start_time, self.state.max_steps)\n        if self._total_flos is not None:\n            self.store_flos()\n            metrics[\"total_flos\"] = self.state.total_flos\n        self.log(metrics)\n        self.control = self.callback_handler.on_train_end(self.args, self.state, self.control)\n        # add remaining tr_loss\n        self._total_loss_scalar += tr_loss.item()\n\n        return TrainOutput(self.state.global_step, self._total_loss_scalar / self.state.global_step, metrics)"}
{"type": "source_file", "path": "retriever/simcse/run_inference.py", "content": "import json\nimport os.path\nimport pickle\n\nimport argparse\nimport shlex\n\nimport faiss\nimport numpy as np\nimport torch\nfrom tqdm import tqdm\nimport transformers\nfrom transformers import AutoModel, AutoTokenizer, AutoConfig\nfrom retriever.eval import eval_retrieval_from_file\nfrom model import RetrievalModel\nTQDM_DISABLED = os.environ['TQDM_DISABLED'] if 'TQDM_DISABLED' in os.environ else False\n\nclass Dummy:\n    pass\n\nclass CodeT5Retriever:\n    def __init__(self, args):\n        self.args = args\n\n    def prepare_model(self, model=None, tokenizer=None, config=None):\n        if self.args.log_level == 'verbose':\n            transformers.logging.set_verbosity_info()\n        self.model_name = self.args.model_name\n\n        if model is None:\n            self.tokenizer = transformers.RobertaTokenizer.from_pretrained(self.model_name)\n            model_arg = Dummy()\n            setattr(model_arg, 'sim_func', args.sim_func)\n            config = AutoConfig.from_pretrained(self.model_name)\n            self.model = RetrievalModel(\n                config=config,\n                model_type=self.model_name,\n                num_layers=args.num_layers,\n                tokenizer=tokenizer,\n                training_args=None,\n                model_args=model_arg)\n            self.device = torch.device('cuda') if not self.args.cpu else torch.device('cpu')\n            self.model.eval()\n            self.model = self.model.to(self.device)\n        else: # this is only for evaluation durning training time\n            self.model = model\n            self.tokenizer = tokenizer\n            self.device = self.model.device\n\n    def encode_file(self, text_file, save_file, **kwargs):\n        normalize_embed = kwargs.get('normalize_embed', False)\n        with open(text_file, \"r\") as f:\n            dataset = []\n            for line in f:\n                dataset.append(line.strip())\n                # print(line)\n        print(f\"number of sentences in {text_file}: {len(dataset)}\")\n\n        def pad_batch(examples):\n            sentences = examples\n            sent_features = self.tokenizer(\n                sentences,\n                add_special_tokens=True,\n                max_length=self.tokenizer.model_max_length,\n                truncation=True\n            )\n            arr = sent_features['input_ids']\n            lens = torch.LongTensor([len(a) for a in arr])\n            max_len = lens.max().item()\n            padded = torch.ones(len(arr), max_len, dtype=torch.long) * self.tokenizer.pad_token_id\n            mask = torch.zeros(len(arr), max_len, dtype=torch.long)\n            for i, a in enumerate(arr):\n                padded[i, : lens[i]] = torch.tensor(a, dtype=torch.long)\n                mask[i, : lens[i]] = 1\n            return {'input_ids': padded, 'attention_mask': mask, 'lengths': lens}\n\n        bs = 128\n        with torch.no_grad():\n            all_embeddings = []\n            for i in tqdm(range(0, len(dataset), bs), disable=TQDM_DISABLED):\n                batch = dataset[i: i + bs]\n                padded_batch = pad_batch(batch)\n                for k in padded_batch:\n                    if isinstance(padded_batch[k], torch.Tensor):\n                        padded_batch[k] = padded_batch[k].to(self.device)\n                output = self.model.get_pooling_embedding(**padded_batch, normalize=normalize_embed).detach().cpu().numpy()\n                all_embeddings.append(output)\n\n            all_embeddings = np.concatenate(all_embeddings, axis=0)\n            print(f\"done embedding: {all_embeddings.shape}\")\n\n        if not os.path.exists(os.path.dirname(save_file)):\n            os.makedirs(os.path.dirname(save_file))\n\n        np.save(save_file, all_embeddings)\n\n    @staticmethod\n    def retrieve(source_embed_file, target_embed_file, source_id_file, target_id_file, top_k, save_file):\n        print(f'source: {source_embed_file}, target: {target_embed_file}')\n        with open(source_id_file, \"r\") as f:\n            source_id_map = {}\n            for idx, line in enumerate(f):\n                source_id_map[idx] = line.strip()\n        with open(target_id_file, \"r\") as f:\n            target_id_map = {}\n            for idx, line in enumerate(f):\n                target_id_map[idx] = line.strip()\n\n        source_embed = np.load(source_embed_file + \".npy\")\n        target_embed = np.load(target_embed_file + \".npy\")\n        assert len(source_id_map) == source_embed.shape[0]\n        assert len(target_id_map) == target_embed.shape[0]\n        indexer = faiss.IndexFlatIP(target_embed.shape[1])\n        indexer.add(target_embed)\n        print(source_embed.shape, target_embed.shape)\n        D, I = indexer.search(source_embed, top_k)\n\n        results = {}\n        for source_idx, (dist, retrieved_index) in enumerate(zip(D, I)):\n            source_id = source_id_map[source_idx]\n            results[source_id] = {}\n            retrieved_target_id = [target_id_map[x] for x in retrieved_index]\n            results[source_id]['retrieved'] = retrieved_target_id\n            results[source_id]['score'] = dist.tolist()\n\n        with open(save_file, \"w+\") as f:\n            json.dump(results, f, indent=2)\n\n        return results\n\ndef config(in_program_call=None):\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--model_name', type=str)\n    parser.add_argument('--batch_size', type=int, default=48)\n    parser.add_argument('--source_file', default='data/conala/conala_nl.txt')\n    parser.add_argument('--target_file', default='data/conala/python_manual_firstpara.txt')\n    parser.add_argument('--source_embed_save_file', default='data/conala/.tmp/src_embedding')\n    parser.add_argument('--target_embed_save_file', default='data/conala/.tmp/tgt_embedding')\n    parser.add_argument('--save_file', default='[REPLACE]data/conala/simcse.[MODEL].[SOURCE].[TARGET].[POOLER].t[TOPK].json')\n    parser.add_argument('--top_k', type=int, default=200)\n    parser.add_argument('--cpu', action='store_true')\n    parser.add_argument('--pooler', choices=('cls', 'cls_before_pooler'), default='cls')\n    parser.add_argument('--log_level', default='verbose')\n    parser.add_argument('--nl_cm_folder', default='data/conala/nl.cm')\n    parser.add_argument('--sim_func', default='cls_distance.cosine', choices=('cls_distance.cosine', 'cls_distance.l2', 'bertscore'))\n    parser.add_argument('--num_layers', type=int, default=12)\n    parser.add_argument('--origin_mode', action='store_true')\n    parser.add_argument('--oracle_eval_file', default='data/conala/cmd_dev.oracle_man.full.json')\n    parser.add_argument('--eval_hit', action='store_true')\n    parser.add_argument('--normalize_embed', action='store_true')\n\n\n\n    args = parser.parse_args() if in_program_call is None else parser.parse_args(shlex.split(in_program_call))\n\n    args.source_idx_file = args.source_file.replace(\".txt\", \".id\")\n    args.target_idx_file = args.target_file.replace(\".txt\", \".id\")\n\n    if in_program_call is None and args.save_file.startswith(\"[REPLACE]\"):\n        args.save_file = args.save_file.replace(\"[REPLACE]\", \"\")\n        args.save_file = args.save_file.replace(\"[MODEL]\", os.path.basename(args.model_name))\n        args.save_file = args.save_file.replace(\"[SOURCE]\", os.path.basename(args.source_file).split(\".\")[0])\n        args.save_file = args.save_file.replace(\"[TARGET]\", os.path.basename(args.target_file).split(\".\")[0])\n        args.save_file = args.save_file.replace(\"[POOLER]\", args.pooler)\n        args.save_file = args.save_file.replace(\"[TOPK]\", str(args.top_k))\n    print(json.dumps(vars(args), indent=2))\n    return args\n\nif __name__ == \"__main__\":\n    args = config()\n\n    searcher = CodeT5Retriever(args)\n    searcher.prepare_model()\n    searcher.encode_file(args.source_file, args.source_embed_save_file, normalize_embed=args.normalize_embed)\n    searcher.encode_file(args.target_file, args.target_embed_save_file, normalize_embed=args.normalize_embed)\n    searcher.retrieve(args.source_embed_save_file,\n                      args.target_embed_save_file, args.source_idx_file,\n                      args.target_idx_file, args.top_k, args.save_file)\n\n    flag = 'recall'\n    top_n = 10\n    m1 = eval_retrieval_from_file(args.oracle_eval_file, args.save_file)\n\n\n"}
{"type": "source_file", "path": "retriever/eval.py", "content": "import json\nimport copy\nfrom collections import OrderedDict\n\nimport numpy as np\nimport argparse\n\nTOP_K = [1, 3, 5, 8, 10, 12, 15, 20, 30, 50, 100, 200]\n\ndef align_src_pred(src_file, pred_file):\n    with open(src_file, \"r\") as fsrc, open(pred_file, \"r\") as fpred:\n        src = json.load(fsrc)\n        pred = json.load(fpred)['results']\n        # assert len(src) == len(pred), (len(src), len(pred))\n\n    # re-order src\n    src_nl = [x['nl'] for x in src]\n    _src = []\n    _pred = []\n    for p in pred:\n        if p['nl'] in src_nl:\n            _src.append(src[src_nl.index(p['nl'])])\n            _pred.append(p)\n\n    src = _src\n    pred = _pred\n\n    for s, p in zip(src, pred):\n        assert s['nl'] == p['nl'], (s['nl'], p['nl'])\n\n    print(f\"unique nl: {len(set(src_nl))}\")\n    print(f\"number of samples (src/pred): {len(src)}/{len(pred)}\")\n    print(\"pass nl matching check\")\n\n    return src, pred\n\ndef calc_metrics(src_file, pred_file):\n    src, pred = align_src_pred(src_file, pred_file)\n\n    _src = []\n    _pred = []\n    for s, p in zip(src, pred):\n        cmd_name = s['cmd_name']\n        oracle_man = get_oracle(s, cmd_name)\n        pred_man = p['pred']\n        _src.append(oracle_man)\n        _pred.append(pred_man)\n    calc_recall(_src, _pred)\n\n    # description only\n    _src = []\n    for s in src:\n        _src.append(s['matching_info']['|main|'])\n    calc_recall(_src, _pred)\n\n    _src = []\n    _pred = []\n    for s, p in zip(src, pred):\n        cmd_name = s['cmd_name']\n        pred_man = p['pred']\n        _src.append(cmd_name)\n        _pred.append(pred_man)\n    calc_hit(_src, _pred)\n    # calc_mean_rank(src, pred)\n\n\ndef calc_mean_rank(src, pred):\n    rank = []\n    for s, p in zip(src, pred):\n        cur_rank = []\n        cmd_name = s['cmd_name']\n        pred_man = p['pred']\n        oracle_man = get_oracle(s, cmd_name)\n        for o in oracle_man:\n            if o in pred_man:\n                cur_rank.append(oracle_man.index(o))\n            else:\n                cur_rank.append(101)\n        if cur_rank:\n            rank.append(np.mean(cur_rank))\n\n    print(np.mean(rank))\n\n\ndef calc_hit(src, pred, top_k=None):\n    top_k = TOP_K if top_k is None else top_k\n    hit_n = {x: 0 for x in top_k}\n    assert len(src) == len(pred), (len(src), len(pred))\n\n    for s, p in zip(src, pred):\n        cmd_name = s\n        pred_man = p\n\n        for tk in hit_n.keys():\n            cur_result_vids = pred_man[:tk]\n            cur_hit = any([cmd_name in x for x in cur_result_vids])\n            hit_n[tk] += cur_hit\n\n    hit_n = {k: v / len(pred) for k, v in hit_n.items()}\n    for k in sorted(hit_n.keys()):\n        print(f\"{hit_n[k] :.3f}\", end=\"\\t\")\n    print()\n    return hit_n\n\ndef get_oracle(item, cmd_name):\n    # oracle = [f\"{cmd_name}_{x}\" for x in itertools.chain(*item['matching_info'].values())]\n    oracle = [f\"{cmd_name}_{x}\" for x in item['oracle_man']]\n    return oracle\n\ndef calc_recall(src, pred, print_result=True, top_k=None):\n    top_k = TOP_K if top_k is None else top_k\n    recall_n = {x: 0 for x in top_k}\n    precision_n = {x: 0 for x in top_k}\n\n    for s, p in zip(src, pred):\n        # cmd_name = s['cmd_name']\n        oracle_man = s\n        pred_man = p\n\n        for tk in recall_n.keys():\n            cur_result_vids = pred_man[:tk]\n            cur_hit = sum([x in cur_result_vids for x in oracle_man])\n            # recall_n[tk] += cur_hit / (len(oracle_man) + 1e-10)\n            recall_n[tk] += cur_hit / (len(oracle_man)) if len(oracle_man) else 1\n            precision_n[tk] += cur_hit / tk\n    recall_n = {k: v / len(pred) for k, v in recall_n.items()}\n    precision_n = {k: v / len(pred) for k, v in precision_n.items()}\n\n    if print_result:\n        for k in sorted(recall_n.keys()):\n            print(f\"{recall_n[k] :.3f}\", end=\"\\t\")\n        print()\n        for k in sorted(precision_n.keys()):\n            print(f\"{precision_n[k] :.3f}\", end=\"\\t\")\n        print()\n        for k in sorted(recall_n.keys()):\n            print(f\"{2 * precision_n[k] * recall_n[k] / (precision_n[k] + recall_n[k] + 1e-10) :.3f}\", end=\"\\t\")\n        print()\n\n    return {'recall': recall_n, 'precision': precision_n}\n\ndef clean_dpr_results(result_file):\n    results = {'results': [], 'metrics': {}}\n    with open(result_file, \"r\") as f:\n        d = json.load(f)\n    for _item in d:\n        item = {}\n        item['nl'] = _item['question']\n        item['pred'] = [x['id'] for x in _item['ctxs']]\n        results['results'].append(item)\n\n    with open(result_file + \".clean\", \"w+\") as f:\n        json.dump(results, f, indent=2)\n\ndef recall_per_manual(src_file, result_file, chunk_length_file, topk):\n\n    def find_sum_in_list(len_list, max_num):\n        idx = len(len_list)\n        for i in range(len(len_list) + 1):\n            if sum(len_list[:i]) >= max_num:\n                idx = i - 1\n                break\n        assert sum(len_list[:idx]) <= max_num\n        return idx\n\n    with open(chunk_length_file, \"r\") as f:\n        d = json.load(f)\n        man_chunk_length = {k: len(v) for k, v in d.items()}\n\n    src, pred = align_src_pred(src_file, result_file)\n    hit_man = 0\n    recall = 0\n    tot = len(src)\n    for s, p in zip(src, pred):\n        cmd_name = s['cmd_name']\n        oracle_man = get_oracle(s, cmd_name)\n        pred_man = p['pred']\n        top_k_cmd = p['top_pred_cmd'][:topk]\n        if cmd_name in top_k_cmd:\n            hit_man += 1\n            pred_chunks = pred_man[cmd_name]\n            len_list = [man_chunk_length[x] for x in pred_chunks]\n            idx = find_sum_in_list(len_list, 1536)\n            pred_chunks = pred_chunks[:idx]\n            cur_hit = sum([x in pred_chunks for x in oracle_man])\n            recall += cur_hit / len(oracle_man)\n\n    print(f\"hit rate: {hit_man}/{tot}={hit_man/tot}\")\n    print(f\"recall: {recall}/{hit_man}={recall/hit_man}\")\n\n\ndef eval_hit_from_file(data_file, retrieval_file,\n                             oracle_entry='oracle_man', retrieval_entry='retrieved'):\n    assert 'tldr' in data_file\n    with open(data_file, \"r\") as f:\n        d = json.load(f)\n    gold = ['_'.join(item[oracle_entry][0].split(\"_\")[:-1]) for item in d]\n\n    with open(retrieval_file, \"r\") as f:\n        r_d = json.load(f)\n        # check whether we need to process the retrieved ids\n        split_flag = False\n        k0 = list(r_d.keys())[0]\n        r0 = r_d[k0][retrieval_entry][0]\n        if r0.split(\"_\")[-1].isdigit():\n            split_flag = True\n\n        for k, item in r_d.items():\n            if split_flag:\n                r = ['_'.join(x.split(\"_\")[:-1]) for x in item[retrieval_entry]]\n            else:\n                r = item[retrieval_entry]\n            r = list(OrderedDict.fromkeys(r))\n            item[retrieval_entry] = r\n\n    pred = [r_d[x['question_id']][retrieval_entry] for x in d]\n    print(gold[:3])\n    print(pred[0][:3])\n    metrics = calc_hit(gold, pred)\n    return {'hit': metrics}\n\ndef eval_retrieval_from_file(data_file, retrieval_file,\n                             oracle_entry='oracle_man', retrieval_entry='retrieved', top_k=None):\n\n    assert 'oracle_man.full' in data_file or 'conala' not in data_file, (data_file)\n    # for conala\n    with open(data_file, \"r\") as f:\n        d = json.load(f)\n    gold = [item[oracle_entry] for item in d]\n\n    with open(retrieval_file, \"r\") as f:\n        r_d = json.load(f)\n    pred = [r_d[x['question_id']][retrieval_entry] for x in d]\n    metrics = calc_recall(gold, pred, top_k=top_k)\n    return metrics\n\ndef eval_retrieval_from_loaded(data_file, r_d):\n    # for conala\n    with open(data_file, \"r\") as f:\n        d = json.load(f)\n    gold = [item['oracle_man'] for item in d]\n    pred = [r_d[x['question_id']]['retrieved'] for x in d]\n    metrics = calc_recall(gold, pred, print_result=False)\n    return metrics\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--result-file', default=None)\n    parser.add_argument('--src-file', default=None)\n    parser.add_argument('--chunk-length-file', default=\"data/tldr/nl.cm/manual_section.tok.json\")\n    parser.add_argument('--function', nargs='+', type=int, default=[1])\n    args = parser.parse_args()\n\n    for cur_func in args.function:\n        if cur_func == 0:\n            calc_metrics(args.src_file, args.result_file)\n        elif cur_func == 1:\n            # convert data\n            clean_dpr_results(args.result_file)\n        elif cur_func == 2:\n            clean_dpr_results(args.result_file)\n            args.result_file += \".clean\"\n            calc_metrics(args.src_file, args.result_file)\n        elif cur_func == 3:\n            # measure recall for per-doc retrieval\n            for k in [1, 10, 30, 50]:\n                print(f\"top {k}\")\n                recall_per_manual(args.src_file, args.result_file, args.chunk_length_file, topk=k)\n"}
{"type": "source_file", "path": "utils/constants.py", "content": "import os\n\nTQDM_DISABLE = True if 'TQDM_DISABLE' in os.environ and str(os.environ['TQDM_DISABLE']) == '1' else False\nWANDB_DISABLE = True if 'WANDB_DISABLE' in os.environ and str(os.environ['WANDB_DISABLE']) == '1' else False\n\nVAR_STR = \"[[VAR]]\"\nNONE_STR = \"[[NONE]]\"\n"}
{"type": "source_file", "path": "retriever/simcse/run_train.py", "content": "import logging\nimport os\nimport sys\nfrom dataclasses import dataclass, field\nfrom collections import defaultdict\nfrom datasets import load_dataset\nimport wandb\nimport transformers\nfrom transformers import (\n    CONFIG_MAPPING,\n    MODEL_FOR_MASKED_LM_MAPPING,\n    AutoConfig,\n    AutoTokenizer,\n    HfArgumentParser,\n    set_seed,\n)\nfrom transformers.trainer_utils import is_main_process\nfrom model import RetrievalModel\nfrom trainers import CLTrainer\nfrom data_utils import OurDataCollatorWithPadding, tok_sentences\nfrom arguments import ModelArguments, DataTrainingArguments, OurTrainingArguments, RetrieverArguments\nlogger = logging.getLogger(__name__)\nMODEL_CONFIG_CLASSES = list(MODEL_FOR_MASKED_LM_MAPPING.keys())\nMODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)\n\n\ndef main():\n    # See all possible arguments in src/transformers/training_args.py\n    # or by passing the --help flag to this script.\n    # We now keep distinct sets of args, for a cleaner separation of concerns.\n\n    parser = HfArgumentParser((ModelArguments, DataTrainingArguments, OurTrainingArguments, RetrieverArguments))\n    if len(sys.argv) == 2 and sys.argv[1].endswith(\".json\"):\n        # If we pass only one argument to the script and it's the path to a json file,\n        # let's parse it to get our arguments.\n        model_args, data_args, training_args, bertscore_args = parser.parse_json_file(\n            json_file=os.path.abspath(sys.argv[1]))\n    else:\n        model_args, data_args, training_args, bertscore_args = parser.parse_args_into_dataclasses()\n\n    if (\n            os.path.exists(training_args.output_dir)\n            and os.listdir(training_args.output_dir)\n            and training_args.do_train\n            and not training_args.overwrite_output_dir\n    ):\n        raise ValueError(\n            f\"Output directory ({training_args.output_dir}) already exists and is not empty.\"\n            \"Use --overwrite_output_dir to overcome.\"\n        )\n\n    # Setup logging\n    logging.basicConfig(\n        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n        datefmt=\"%m/%d/%Y %H:%M:%S\",\n        level=logging.INFO if is_main_process(training_args.local_rank) else logging.WARN,\n    )\n\n    # Log on each process the small summary:\n    logger.warning(\n        f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}\"\n        + f\" distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}\"\n    )\n    # Set the verbosity to info of the Transformers logger (on main process only):\n    if is_main_process(training_args.local_rank):\n        transformers.utils.logging.set_verbosity_info()\n        transformers.utils.logging.enable_default_handler()\n        transformers.utils.logging.enable_explicit_format()\n    logger.info(\"Training/evaluation parameters %s\", training_args)\n\n    # Set seed before initializing model.\n    set_seed(training_args.seed)\n    training_args.eval_file = data_args.eval_file\n\n    # Get the datasets: you can either provide your own CSV/JSON/TXT training and evaluation files (see below)\n    # or just provide the name of one of the public datasets available on the hub at https://huggingface.co/datasets/\n    # (the dataset will be downloaded automatically from the datasets Hub\n    #\n    # For CSV/JSON files, this script will use the column called 'text' or the first column. You can easily tweak this\n    # behavior (see below)\n    #\n    # In distributed training, the load_dataset function guarantee that only one local process can concurrently\n    # download the dataset.\n\n    assert 'json' in data_args.train_file\n    data_files = {'train': data_args.train_file}\n    datasets = load_dataset('json', data_files=data_files)\n    # See more about loading any type of standard or custom dataset (from files, python dict, pandas DataFrame, etc) at\n    # https://huggingface.co/docs/datasets/loading_datasets.html.\n\n    # Load pretrained model and tokenizer\n    #\n    # Distributed training:\n    # The .from_pretrained methods guarantee that only one local process can concurrently\n    # download model & vocab.\n    config_kwargs = {\n        \"cache_dir\": model_args.cache_dir,\n        \"revision\": model_args.model_revision,\n        \"use_auth_token\": True if model_args.use_auth_token else None,\n    }\n    if model_args.config_name:\n        config = AutoConfig.from_pretrained(model_args.config_name, **config_kwargs)\n    elif model_args.model_name_or_path:\n        config = AutoConfig.from_pretrained(model_args.model_name_or_path, **config_kwargs)\n    else:\n        config = CONFIG_MAPPING[model_args.model_type]()\n        logger.warning(\"You are instantiating a new config instance from scratch.\")\n\n    # tokenizer_kwargs = {\n    #     \"cache_dir\": model_args.cache_dir,\n    #     \"use_fast\": model_args.use_fast_tokenizer,\n    #     \"revision\": model_args.model_revision,\n    #     \"use_auth_token\": True if model_args.use_auth_token else None,\n    # }\n    assert model_args.model_name_or_path\n    if 'codet5' in model_args.model_name_or_path:\n        tokenizer = transformers.RobertaTokenizerFast.from_pretrained(model_args.model_name_or_path, add_prefix_space=True)\n    else:\n        tokenizer = AutoTokenizer.from_pretrained(model_args.model_name_or_path)\n\n    assert model_args.model_name_or_path\n    # assert training_args.sim_func == 'bertscore'\n    model = RetrievalModel(\n                       config=config,\n                       model_type=model_args.model_name_or_path,\n                       num_layers=bertscore_args.num_layers,\n                       all_layers=bertscore_args.all_layers,\n                       idf=bertscore_args.idf,\n                       rescale_with_baseline=bertscore_args.rescale_with_baseline,\n                       baseline_path=bertscore_args.baseline_path,\n                       tokenizer=tokenizer,\n                       training_args = training_args,\n                       model_args=model_args)\n\n\n    # load idf dict\n    if bertscore_args.idf:\n        raise NotImplementedError\n        # assert _idf_dict, \"IDF weights are not computed\"\n        # idf_dict = _idf_dict\n    else:\n        idf_dict = defaultdict(lambda: 1.0)\n        idf_dict[tokenizer.sep_token_id] = 0\n        idf_dict[tokenizer.cls_token_id] = 0\n\n    def prepare_features(examples):\n        total = len(examples['text1'])\n        for idx in range(total):\n            if examples['text1'][idx] == '':\n                examples['text1'][idx] = \" \"\n            if examples['text2'][idx] == '':\n                examples['text2'][idx] = \" \"\n\n        sentences = examples['text1'] + examples['text2']\n        features = tok_sentences(tokenizer, sentences, has_hard_neg=False, total=total, max_length=data_args.max_seq_length)\n        return features\n\n\n    if training_args.do_train:\n        train_dataset = datasets['train'].map(\n            prepare_features,\n            batched=True,\n            num_proc=data_args.preprocessing_num_workers,\n            load_from_cache_file=not data_args.overwrite_cache,\n        )\n\n    # Data collator\n    data_collator = OurDataCollatorWithPadding(tokenizer.pad_token_id, idf_dict)\n\n    training_args.remove_unused_columns = False\n    trainer = CLTrainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset if training_args.do_train else None,\n        tokenizer=tokenizer,\n        data_collator=data_collator,\n    )\n    trainer.model_args = model_args\n    trainer.epoch_metric = {}\n    trainer.metric_for_best_model = training_args.metric_for_best_model\n    training_args.do_eval = False\n\n    # Training\n    if training_args.do_train:\n        model_path = (\n            model_args.model_name_or_path\n            if (model_args.model_name_or_path is not None and os.path.isdir(model_args.model_name_or_path))\n            else None\n        )\n\n        trainer.train(model_path=model_path)\n\n    # Evaluation\n    results = {}\n    if training_args.do_eval:\n        logger.info(\"*** Evaluate ***\")\n        results = trainer.evaluate(eval_senteval_transfer=True)\n\n        output_eval_file = os.path.join(training_args.output_dir, \"eval_results.txt\")\n        if trainer.is_world_process_zero():\n            with open(output_eval_file, \"w\") as writer:\n                logger.info(\"***** Eval results *****\")\n                for key, value in sorted(results.items()):\n                    logger.info(f\"  {key} = {value}\")\n                    writer.write(f\"{key} = {value}\\n\")\n\n    return results\n\n\ndef _mp_fn(index):\n    # For xla_spawn (TPUs)\n    main()\n\n\nif __name__ == \"__main__\":\n    main()\n"}
{"type": "source_file", "path": "utils/util.py", "content": "#!/usr/bin/env python3\n# Copyright (c) Facebook, Inc. and its affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport logging\nimport os.path\nimport socket\nfrom collections import defaultdict\nfrom typing import Dict, List\nimport re\nimport pickle\nfrom utils.constants import VAR_STR\n\ndef init_logger(args):\n    # setup logger\n    logger = logging.getLogger()\n    handler = logging.StreamHandler()\n    formatter = logging.Formatter(f\"%(asctime)s %(module)s - %(funcName)s: [ {socket.gethostname()} | Node {args.node_id} | Rank {args.global_rank} ] %(message)s\",\n                                  datefmt='%Y-%m-%d %H:%M:%S')\n    handler.setFormatter(formatter)\n    logger.handlers.clear()\n    logger.addHandler(handler)\n    logger.setLevel(logging.INFO)\n\n    return logger\n\ndef dedup_results(saved):\n    exc_key = ['nl', 'gold', 'cmd_name']\n    for item in saved:\n        if 'prediction' in item:\n            item['code'] = item['prediction']\n            item.pop('prediction')\n        if 'score' in item:\n            item['sequence_ll'] = item['score']\n            item.pop('score')\n\n    _saved = [saved[0]]\n    for item in saved[1:]:\n        if item['nl'] == _saved[-1]['nl']:\n            for k in item.keys():\n                if k not in exc_key:\n                    _saved[-1][k] += item[k]\n        else:\n            _saved.append(item)\n    _saved = {x['nl']: x for x in _saved}\n    return _saved\n\ndef clean_command(s):\n    s = s.replace(\"sudo\", \"\").strip()\n    s =  s.replace(\"`\", \"\").replace('\"', \"\").replace(\"'\", \"\")\n    #  '>', '|', '+'\n    s = s.replace(\"|\", \" \").replace(\">\", \" \").replace(\"<\", \" \")\n    s = \" \".join(s.split())\n    return s\n\ndef anonymize_command(s):\n    s = s.replace(\"={\", \" {\")\n    var_to_pc_holder = defaultdict(lambda: len(var_to_pc_holder))\n    for var in re.findall(\"{{(.*?)}}\", s):\n        _ = var_to_pc_holder[var]\n    for var, id in var_to_pc_holder.items():\n        var_str = \"{{%s}}\" % var\n        s = s.replace(var_str, f\"{VAR_STR}_{id}\")\n    # s = re.sub(\"{{.*?}}\", VAR_STR, s)\n    return s\n\ndef get_bag_of_keywords(cmd):\n    cmd = clean_anonymize_command(cmd)\n    # try:\n    #     tokens = list(bashlex.split(cmd))\n    # except NotImplementedError:\n    #     tokens = cmd.strip().split()\n    tokens = cmd.strip().split()\n    tokens = [x for x in tokens if VAR_STR not in x]\n    return tokens\n\ndef get_bag_of_words(cmd):\n    cmd = clean_anonymize_command(cmd)\n    # try:\n    #     tokens = list(bashlex.split(cmd))\n    # except NotImplementedError:\n    #     tokens = cmd.strip().split()\n    tokens = cmd.strip().split()\n    return tokens\n\ndef clean_manual(man_string):\n    cur_man_line = [x.strip() for x in man_string.strip().split(\"\\n\") if len(x.strip().split()) >= 1]\n    cur_man_line = [\" \".join(x.split()) for x in cur_man_line]\n    cur_man_line = \" \".join(cur_man_line)\n    return cur_man_line\n\ndef clean_anonymize_command(s):\n    return anonymize_command(clean_command(s))\n\n# used for constraint command_name decoding\nclass Trie(object):\n    def __init__(self, sequences: List[List[int]] = []):\n        self.trie_dict = {}\n        self.len = 0\n        if sequences:\n            for sequence in sequences:\n                Trie._add_to_trie(sequence, self.trie_dict)\n                self.len += 1\n\n        self.append_trie = None\n        self.bos_token_id = None\n\n    def append(self, trie, bos_token_id):\n        self.append_trie = trie\n        self.bos_token_id = bos_token_id\n\n    def add(self, sequence: List[int]):\n        Trie._add_to_trie(sequence, self.trie_dict)\n        self.len += 1\n\n    def get(self, prefix_sequence: List[int]):\n        return Trie._get_from_trie(\n            prefix_sequence, self.trie_dict, self.append_trie, self.bos_token_id\n        )\n\n    @staticmethod\n    def load_from_dict(trie_dict):\n        trie = Trie()\n        trie.trie_dict = trie_dict\n        trie.len = sum(1 for _ in trie)\n        return trie\n\n    @staticmethod\n    def _add_to_trie(sequence: List[int], trie_dict: Dict):\n        if sequence:\n            if sequence[0] not in trie_dict:\n                trie_dict[sequence[0]] = {}\n            Trie._add_to_trie(sequence[1:], trie_dict[sequence[0]])\n\n    @staticmethod\n    def _get_from_trie(\n        prefix_sequence: List[int],\n        trie_dict: Dict,\n        append_trie=None,\n        bos_token_id: int = None,\n    ):\n        if len(prefix_sequence) == 0:\n            output = list(trie_dict.keys())\n            if append_trie and bos_token_id in output:\n                output.remove(bos_token_id)\n                output += list(append_trie.trie_dict.keys())\n            return output\n        elif prefix_sequence[0] in trie_dict:\n            return Trie._get_from_trie(\n                prefix_sequence[1:],\n                trie_dict[prefix_sequence[0]],\n                append_trie,\n                bos_token_id,\n            )\n        else:\n            if append_trie:\n                return append_trie.get(prefix_sequence)\n            else:\n                return []\n\n    def __iter__(self):\n        def _traverse(prefix_sequence, trie_dict):\n            if trie_dict:\n                for next_token in trie_dict:\n                    yield from _traverse(\n                        prefix_sequence + [next_token], trie_dict[next_token]\n                    )\n            else:\n                yield prefix_sequence\n\n        return _traverse([], self.trie_dict)\n\n    def __len__(self):\n        return self.len\n\n    def __getitem__(self, value):\n        return self.get(value)\n\ndef build_trie():\n    from glob import glob\n    from transformers import AutoTokenizer\n    tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-1.3B\")\n    ss = []\n    for cmd in glob(\"./data/tldr/manual_trimmed/*.txt\"):\n        cmd = os.path.basename(cmd).replace(\".txt\", \"\")\n        tok_cmd = tokenizer(f\" {cmd}\")['input_ids']\n        ss.append([-1] + tok_cmd + [-2])\n    print(f\"number of commands: {len(ss)}\")\n    trie = Trie(ss)\n\n    with open(\"./data/tldr/nl.cm/cmd_trie.pkl\", \"wb\") as f:\n        pickle.dump(trie.trie_dict, f)\n\n\ndef constrain_cmd_name_fn(cmd_trie, tokenizer, batch_idx, prefix_beam):\n    sep_token_idx = tokenizer.sep_token_id\n    if prefix_beam[-1] == sep_token_idx: # the first token\n        next_tok = cmd_trie.get([-1])\n    else:\n        # get the prefix\n        prefix_idx = prefix_beam.index(sep_token_idx)\n        prefix = [-1] + prefix_beam[prefix_idx+1:]\n        next_tok = cmd_trie.get(prefix)\n        # EOS or not a command anymore\n        if [-2] in next_tok or next_tok == []:\n            next_tok = [x for x in range(len(tokenizer))]\n\n    return next_tok\n\nif __name__ == \"__main__\":\n    cmd = \"firejail --net={{eth0}} --ip={{192.168.1.244}} {{/etc/init.d/apache2}} {{start}}\"\n    print(clean_command(cmd))\n    print(anonymize_command(cmd))\n    print(anonymize_command(clean_command(cmd)))\n\n    cmd = \"toilet {{input_text}} -f {{font_filename}} {{font_filename}}\"\n    print(clean_command(cmd))\n    print(anonymize_command(cmd))\n    print(anonymize_command(clean_command(cmd)))\n    # print(get_bag_of_keywords(cmd))\n    # build_trie()\n    # with open(\"./data/tldr/nl.cm/cmd_trie.pkl\", \"rb\") as f:\n    #     d = pickle.load(f)\n    #     trie = Trie.load_from_dict(d)\n    #     print(len(trie.get([-1])))\n    #     print(trie.get([-1, 300]))\n\n    # from transformers import AutoTokenizer\n    # tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-1.3B\")\n    # tokenizer.add_special_tokens({\"sep_token\": \"|nl2code|\"})\n    # nl = \"disable ldap authentication |nl2code|\"\n    # tok_nl = tokenizer(nl)['input_ids']\n    # tt = [335, 499, 2364]\n    # while tt:\n    #     next_tok = constrain_cmd_name_fn(trie, tokenizer, None, tok_nl)\n    #     assert tt[0] in next_tok, (tokenizer.convert_ids_to_tokens(tok_nl), tt[0], tokenizer.convert_ids_to_tokens(next_tok))\n    #     tok_nl.append(tt.pop(0))\n"}
