{"repo_info": {"repo_name": "EnergonAI", "repo_owner": "hpcaitech", "repo_url": "https://github.com/hpcaitech/EnergonAI"}}
{"type": "test_file", "path": "tests/test_checkpoint/test_checkpoint_basic1d.py", "content": "#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n\nimport pprint\nfrom functools import partial\nfrom colossalai.logging import get_dist_logger\n\nimport colossalai.nn as col_nn\nimport pytest\nimport torch\nimport torch.multiprocessing as mp\nimport torch.nn as nn\nfrom energonai.context.parallel_mode import ParallelMode\nfrom energonai.core import global_context as gpc\nfrom energonai.initialize import launch\nfrom colossalai.logging import disable_existing_loggers\nfrom colossalai.utils import free_port, is_using_pp\nfrom energonai.utils.checkpointing import gather_pipeline_parallel_state_dict, load_checkpoint, save_checkpoint\n\n\ndef partition_uniform(num_items, pipeline_parallel_size, num_chunks):\n    assert num_items % num_chunks == 0, \\\n        \"Layer length should be divided by the number of chunks, otherwise parameter method is recomended\"\n\n    logger = get_dist_logger('energonai')\n    parts = [[] for _ in range(pipeline_parallel_size)]\n    partition_items = num_items // num_chunks\n    for idx in range(num_chunks):\n        base_idx = idx * partition_items\n        chunk_size = partition_items // pipeline_parallel_size\n        left = pipeline_parallel_size - partition_items % pipeline_parallel_size\n        if chunk_size == 0:\n            logger.warning(\"Some nodes in Pipeline have no requests\")\n\n        for p in range(pipeline_parallel_size):\n            st = base_idx\n            base_idx += chunk_size + (p >= left)\n            parts[p].append((st, base_idx))\n\n    return parts\n\n\ndef build_pipeline(model):\n\n    pipeline_size = gpc.get_world_size(ParallelMode.PIPELINE)\n    pipeline_rank = gpc.get_local_rank(ParallelMode.PIPELINE)\n    depth = len(model)\n    start, end = partition_uniform(depth, pipeline_size, 1)[pipeline_rank][0]\n    layers = []\n    for i in range(depth):\n        if start <= i < end:\n            layers.append(model[i])\n        else:\n            layers.append(nn.Identity())\n    return nn.Sequential(*tuple(layers))\n\n\ndef check_equal(A, B):\n    assert torch.allclose(A, B, rtol=1e-3, atol=1e-2)\n\n\ndef check_basic_1d(rank, world_size, port):\n    # config = dict(\n    #     parallel=dict(pipeline=dict(size=2), tensor=dict(size=4, mode=\"1d\")),\n    # )\n    disable_existing_loggers()\n    launch(pp_size=2, tp_size=2, rank=rank, world_size=world_size, host=\"localhost\", port=port, backend=\"nccl\")\n    # m1 = nn.Sequential(col_nn.Embedding1D(20, 12), col_nn.Linear1D(12, 20), col_nn.Classifier1D(20, 3),\n    #                    col_nn.Embedding1D(20, 12), col_nn.Linear1D(12, 20), col_nn.Classifier1D(20, 3))\n    # m1 = nn.Sequential(col_nn.Embedding1D(20, 12), col_nn.Embedding1D(20, 12))\n    # m1 = nn.Sequential(col_nn.Linear1D(4, 2), col_nn.Linear1D(2, 4))\n    if gpc.get_local_rank(ParallelMode.PIPELINE) == 0:\n        m1 = nn.Sequential(col_nn.Embedding1D(16, 4), col_nn.Classifier1D(8, 4), col_nn.Linear1D(4, 2),\n                           col_nn.Dropout1D(),\n                           nn.Identity(), nn.Identity(), nn.Identity(), nn.Identity())\n    else:\n        m1 = nn.Sequential(nn.Identity(), nn.Identity(), nn.Identity(), nn.Identity(),\n                           col_nn.Embedding1D(16, 4), col_nn.Classifier1D(8, 4), col_nn.Linear1D(4, 2),\n                           col_nn.Dropout1D())\n    for name, param in m1.named_parameters():\n        print(\"RANK {}: {}, {}\".format(gpc.get_global_rank(), name, param.size()))\n    sd1 = m1.state_dict()\n    # print(f\"Rank {gpc.get_global_rank()}:\\n{pprint.pformat(sd1)}\\n\")\n    save_checkpoint(\"test.pt\", 0, m1)\n    # m2 = nn.Sequential(col_nn.Embedding1D(20, 12), col_nn.Linear1D(12, 20), col_nn.Classifier1D(20, 3),\n    #                    col_nn.Embedding1D(20, 12), col_nn.Linear1D(12, 20), col_nn.Classifier1D(20, 3))\n    m2 = nn.Sequential(col_nn.Embedding1D(16, 4), col_nn.Classifier1D(8, 4), col_nn.Linear1D(4, 2), col_nn.Dropout1D(),\n                       col_nn.Embedding1D(16, 4), col_nn.Classifier1D(8, 4), col_nn.Linear1D(4, 2), col_nn.Dropout1D())\n\n    if is_using_pp():\n        m2 = build_pipeline(m2)\n    load_checkpoint(\"test.pt\", m2)\n    sd2 = m2.state_dict()\n    if is_using_pp() and gpc.get_local_rank(ParallelMode.TENSOR) == 0:\n        sd2 = gather_pipeline_parallel_state_dict(sd2)\n    print(f\"Rank {gpc.get_global_rank()}:\\n{pprint.pformat(sd2)}\\n\")\n\n    if gpc.get_global_rank() == 0:\n        for k, v in sd1.items():\n            assert k in sd2\n            check_equal(v.to(torch.device(\"cpu\")), sd2[k].to(torch.device(\"cpu\")))\n\n\n@pytest.mark.dist\ndef test_checkpoint_1d():\n    world_size = 4\n    run_func = partial(check_basic_1d, world_size=world_size, port=free_port())\n    mp.spawn(run_func, nprocs=world_size)\n\n\nif __name__ == \"__main__\":\n    test_checkpoint_1d()\n"}
{"type": "test_file", "path": "tests/test_checkpoint/test_checkpoint_bert1d.py", "content": "#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n\nimport pprint\nfrom functools import partial\n\nimport colossalai.nn as col_nn\nimport torch\nimport torch.multiprocessing as mp\nimport torch.nn as nn\n\nfrom example.gpt.gpt import gpt2_small\nfrom energonai.context.parallel_mode import ParallelMode\nfrom energonai.engine import InferenceEngine\nfrom example.bert.bert import bert_small\nfrom energonai.core import global_context as gpc\nfrom energonai.initialize import launch\nfrom colossalai.logging import disable_existing_loggers\nfrom colossalai.utils import free_port, is_using_pp\nfrom energonai.utils.checkpointing import gather_pipeline_parallel_state_dict, load_checkpoint, save_checkpoint\n\n\ndef check_equal(A, B):\n    assert torch.allclose(A, B, rtol=1e-3, atol=1e-2)\n\n\ndef check_bert_1d(rank, world_size, port):\n    disable_existing_loggers()\n    launch(pp_size=2, tp_size=2, rank=rank, world_size=world_size, host=\"localhost\", port=port, backend=\"nccl\")\n    state_prefix = \"\"\n    parameter_prefix = \"\"\n    m1 = bert_small()\n    sd1 = m1.state_dict(prefix=state_prefix)\n    for name, param in m1.named_parameters(prefix=parameter_prefix):\n        print(\"RANK {}: {}, {}\".format(gpc.get_global_rank(), name, param.size()))\n    save_checkpoint(\"bert_test.pt\", 0, m1, prefix=state_prefix)\n    print(\"Rank {} building second GPT\".format(gpc.get_global_rank()))\n    m2 = bert_small(checkpoint=True, checkpoint_path=\"bert_test.pt\", prefix=parameter_prefix)\n    sd2 = m2.state_dict(prefix=state_prefix)\n    if is_using_pp() and gpc.get_local_rank(ParallelMode.TENSOR) == 0:\n        sd2 = gather_pipeline_parallel_state_dict(sd2)\n    # print(\"Rank {} : {}\".format(gpc.get_global_rank(), sd2))\n    print(\"Rank {} gather done\".format(gpc.get_global_rank()))\n    # print(f'Rank {gpc.get_global_rank()}:{pprint.pformat(sd2)}')\n    if gpc.get_global_rank() == 0:\n        for k, v in sd1.items():\n            assert k in sd2\n            check_equal(v.to(torch.device(\"cpu\")), sd2[k].to(torch.device(\"cpu\")))\n\n\ndef test_bert():\n    world_size = 4\n    run_func = partial(check_bert_1d, world_size=world_size, port=free_port())\n    mp.spawn(run_func, nprocs=world_size)\n\n\nif __name__ == \"__main__\":\n    test_bert()\n"}
{"type": "test_file", "path": "tests/test_checkpoint/test_checkpoint_gpt1d.py", "content": "#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n\nimport pprint\nfrom functools import partial\n\nimport colossalai.nn as col_nn\nimport torch\nimport torch.multiprocessing as mp\nimport torch.nn as nn\n\nfrom example.gpt.gpt import gpt2_small\nfrom energonai.context.parallel_mode import ParallelMode\nfrom energonai.engine import InferenceEngine\nfrom example.gpt import *\nfrom energonai.core import global_context as gpc\nfrom energonai.initialize import launch\nfrom colossalai.logging import disable_existing_loggers\nfrom colossalai.utils import free_port, is_using_pp\nfrom energonai.utils.checkpointing import gather_pipeline_parallel_state_dict, load_checkpoint, save_checkpoint\n\n\ndef check_equal(A, B):\n    assert torch.allclose(A, B, rtol=1e-3, atol=1e-2)\n\n\ndef check_gpt_1d(rank, world_size, port):\n    disable_existing_loggers()\n    launch(pp_size=2, tp_size=2, rank=rank, world_size=world_size, host=\"localhost\", port=port, backend=\"nccl\")\n    # state_prefix = \"rk_{}.\".format(gpc.get_global_rank())\n    # parameter_prefix = \"rk_{}\".format(gpc.get_global_rank())\n    state_prefix = ''\n    parameter_prefix = ''\n    m1 = gpt2_small(vocab_size=50257)\n    sd1 = m1.state_dict(prefix=state_prefix)\n    for name, param in m1.named_parameters(prefix=parameter_prefix):\n        print(\"RANK {}: {}, {}\".format(gpc.get_global_rank(), name, param.size()))\n    save_checkpoint(\"gpt_test.pt\", 0, m1, prefix=state_prefix)\n    print(\"Rank {} building second GPT\".format(gpc.get_global_rank()))\n    m2 = gpt2_small(checkpoint=True, checkpoint_path=\"gpt_test.pt\", prefix=parameter_prefix, vocab_size=50257)\n    sd2 = m2.state_dict(prefix=state_prefix)\n    if is_using_pp() and gpc.get_local_rank(ParallelMode.TENSOR) == 0:\n        sd2 = gather_pipeline_parallel_state_dict(sd2)\n    # print(\"Rank {} : {}\".format(gpc.get_global_rank(), sd2))\n    print(\"Rank {} gather done\".format(gpc.get_global_rank()))\n    # print(f'Rank {gpc.get_global_rank()}:{pprint.pformat(sd2)}')\n    if gpc.get_global_rank() == 0:\n        for k, v in sd1.items():\n            assert k in sd2\n            check_equal(v.to(torch.device(\"cpu\")), sd2[k].to(torch.device(\"cpu\")))\n\n\ndef test_gpt():\n    world_size = 4\n    run_func = partial(check_gpt_1d, world_size=world_size, port=free_port())\n    mp.spawn(run_func, nprocs=world_size)\n\n\nif __name__ == \"__main__\":\n    test_gpt()\n"}
{"type": "test_file", "path": "tests/test_checkpoint/test_moduledict.py", "content": "import torch\nimport torch.nn as nn\n"}
{"type": "test_file", "path": "tests/test_engine/boring_model_utils.py", "content": "from energonai.testing import BoringModel, get_correct_output\nfrom energonai import launch_engine\nfrom colossalai.utils import free_port\nimport torch\nimport asyncio\n\n\ndef run_boring_model(tp_world_size: int, pp_world_size: int):\n    engine = launch_engine(tp_world_size, pp_world_size, 'localhost', free_port(), free_port(), BoringModel)\n    x = torch.ones(4)\n    correct_output = get_correct_output(x, pp_world_size)\n    engine.submit(0, x)\n    output = asyncio.run(engine.wait(0))\n    try:\n        assert torch.equal(output, correct_output), f'output: {output} vs target: {correct_output}'\n    finally:\n        engine.shutdown()\n"}
{"type": "test_file", "path": "tests/test_engine/test_single_device.py", "content": "from colossalai.testing import rerun_if_address_is_in_use\nfrom test_engine.boring_model_utils import run_boring_model\nimport pytest\n\n\n@pytest.mark.dist\n@pytest.mark.standalone\n@rerun_if_address_is_in_use()\ndef test_single_device():\n    run_boring_model(1, 1)\n\n\nif __name__ == '__main__':\n    test_single_device()\n"}
{"type": "test_file", "path": "tests/test_engine/test_hybrid.py", "content": "from colossalai.testing import rerun_if_address_is_in_use\nfrom test_engine.boring_model_utils import run_boring_model\nimport pytest\n\n\n@pytest.mark.dist\n@pytest.mark.standalone\n@rerun_if_address_is_in_use()\ndef test_hybrid():\n    run_boring_model(2, 2)\n\n\nif __name__ == '__main__':\n    test_hybrid()\n"}
{"type": "test_file", "path": "tests/test_engine/test_tp.py", "content": "from colossalai.testing import rerun_if_address_is_in_use\nfrom test_engine.boring_model_utils import run_boring_model\nimport pytest\n\n\n@pytest.mark.dist\n@pytest.mark.standalone\n@rerun_if_address_is_in_use()\ndef test_tp():\n    run_boring_model(2, 1)\n\n\nif __name__ == '__main__':\n    test_tp()\n"}
{"type": "test_file", "path": "tests/test_kernel/test_ft_transpose_pad.py", "content": "from energonai.kernel import ft_build_padding_offsets, ft_remove_padding, ft_rebuild_padding, ft_transpose_remove_padding, ft_transpose_rebuild_padding\nimport torch\nimport pytest\n\n\nseq_lens =  torch.tensor([24,127,31,65,24,127,31,65], dtype=torch.int).cuda()\nbatch_size = 8\nmax_padding_size = 128\nhead_size = 64\nhead_num = 12\nhidden_size = head_num * head_size\n\n\ndef test_kernel():\n    hidden_states_q = torch.rand(batch_size, max_padding_size, hidden_size).cuda()\n    hidden_states_k = torch.rand(batch_size, max_padding_size, hidden_size).cuda()\n    hidden_states_v = torch.rand(batch_size, max_padding_size, hidden_size).cuda()\n    \n    \n    tmp_mask_offset = torch.zeros(batch_size, max_padding_size, dtype=torch.int).cuda()\n    mask_offset = torch.zeros(batch_size, max_padding_size, dtype=torch.int).cuda()\n    valid_word_num = torch.zeros(1, dtype=torch.int).cuda()\n\n    ft_build_padding_offsets(seq_lens, batch_size, max_padding_size, valid_word_num, tmp_mask_offset)\n    q = ft_remove_padding(hidden_states_q, tmp_mask_offset, mask_offset, valid_word_num[0].item(), hidden_size)\n    k = ft_remove_padding(hidden_states_k, tmp_mask_offset, mask_offset, valid_word_num[0].item(), hidden_size)\n    v = ft_remove_padding(hidden_states_v, tmp_mask_offset, mask_offset, valid_word_num[0].item(), hidden_size)\n        \n    new_qkv_shape = q.shape[:-1] + (head_num, head_size)\n    \n    q = q.view(new_qkv_shape)\n    k = k.view(new_qkv_shape)\n    v = v.view(new_qkv_shape)\n    print(q.size())\n\n    q_buf = torch.zeros(batch_size, head_num, max_padding_size, head_size).cuda()\n    k_buf = torch.zeros(batch_size, head_num, max_padding_size, head_size).cuda()\n    v_buf = torch.zeros(batch_size, head_num, max_padding_size, head_size).cuda()\n\n    ft_transpose_rebuild_padding(q, k, v, q_buf, k_buf, v_buf, batch_size, max_padding_size, head_num, head_size, valid_word_num[0].item(), mask_offset)\n\n    print(q_buf.size())\n\n    q_buf = ft_transpose_remove_padding(v_buf, valid_word_num[0].item(), batch_size, max_padding_size, head_num, head_size, mask_offset)\n\n    print(q_buf.size())\n\n    q_buf = ft_rebuild_padding(q_buf, mask_offset, valid_word_num[0].item(), hidden_size, batch_size, max_padding_size)\n\n    print(q_buf.size())\n\n\n\n\n\n    # ft_transpose_remove_padding()\n    \n    \n\n\n\n    \n\n    # void ft_transpose_remove_padding_wrapper(torch::Tensor Q, torch::Tensor K, torch::Tensor V, torch::Tensor q_buf, torch::Tensor k_buf, torch::Tensor v_buf,\n    #                                       int batch_size, int seq_len, int head_num, int size_per_head, int valid_word_num, torch::Tensor mask_offset){\n\n\n    # print(new_hidden_states.size())\n\n    # def ft_remove_padding(src, tmp_mask_offset, mask_offset, valid_word_num, hidden_dim):\n    # def ft_rebuild_padding(src, mask_offset, valid_word_num, hidden_dim):\n    # def ft_transpose_remove_padding(Q, K, V, q_buf, k_buf, v_buf, batch_size, seq_len, head_num, size_per_head, valid_word_num, mask_offset):\n    # def ft_transpose_rebuild_padding(src, valid_word_num, batch_size, seq_len, head_num, size_per_head, mask_offset):\n\n\nif __name__ == '__main__':\n    test_kernel()"}
{"type": "test_file", "path": "tests/test_kernel/test_linear_func.py", "content": "from energonai.kernel import linear, find_algo\nimport torch\nimport time\n\n\n@torch.no_grad()\ndef test_linear_func():\n    batch_size = 16\n    seq_len = 64\n    din = 12288\n    dout = 49152\n\n    inputs = torch.randn(batch_size, seq_len, din).half().cuda()\n    params = torch.randn(dout, din).half().cuda()\n    tensor_target = torch.nn.functional.linear(inputs, params)\n    tensor_output = linear(inputs, params)\n    diff = torch.abs(tensor_output - tensor_target)\n    max_diff = torch.max(diff)\n    mean_diff = torch.mean(diff)\n    max_array = torch.max(tensor_target)\n\n    if mean_diff > 0.5 or max_diff > 15 or max_diff / max_array > 0.05:\n        print(\"mean_diff:%.2f, max_diff:%.2f, max_diff/max_array:%.4f\" %\n              (mean_diff, max_diff, max_diff / max_array))\n        print('target:', tensor_target, '\\n')\n        print('output:', tensor_output, '\\n')\n        raise AssertionError(\"Wrong value!\")\n\n    print('tests pass')\n\n\n@torch.no_grad()\ndef benchmark_linear_func():\n    algo = find_algo()\n    batch_size = 16\n    seq_len = 64\n    din = 12288\n    dout = 49152\n\n    inner_loop = 8\n    outer_loop = 20\n\n    input_list_1 = []\n    param_list_1 = []\n    input_list_2 = []\n    param_list_2 = []\n    for i in range(inner_loop):\n        input_list_1.append(torch.randn(batch_size, seq_len, din).half().cuda())\n        param_list_1.append(torch.randn(dout, din).half().cuda())\n        input_list_2.append(input_list_1[-1].clone().detach())\n        param_list_2.append(param_list_1[-1].clone().detach())\n\n    torch_count = 0        \n    cublas_count = 0\n\n    for _ in range(outer_loop):\n        for i in range(inner_loop):\n            _ = torch.nn.functional.linear(input_list_2[i], param_list_2[i])\n            torch.cuda.synchronize()\n            _ = linear(input_list_1[i], param_list_1[i], algo)\n            torch.cuda.synchronize()\n            _ = torch.nn.functional.linear(input_list_2[i], param_list_2[i])\n            torch.cuda.synchronize()\n            _ = linear(input_list_1[i], param_list_1[i], algo)\n            torch.cuda.synchronize()\n\n            torch.cuda.synchronize()\n            start_time = time.time()\n            _ = torch.nn.functional.linear(input_list_2[i], param_list_2[i])\n            torch.cuda.synchronize()\n            torch_count += time.time() - start_time\n            \n            torch.cuda.synchronize()\n            start_time = time.time()\n            _ = linear(input_list_1[i], param_list_1[i], algo)\n            torch.cuda.synchronize()\n            cublas_count += time.time() - start_time\n    \n    torch_time = torch_count / inner_loop / outer_loop\n    cublas_time = (cublas_count / inner_loop / outer_loop)\n    print(\"==>  torch time: %.6f\" % torch_time)\n    print(\"==> cublas time: %.6f, speedup: %.4f%%\" % (cublas_time, (torch_time - cublas_time) / torch_time * 100))\n\n\nif __name__ == '__main__':\n    test_linear_func()\n    benchmark_linear_func()\n"}
{"type": "test_file", "path": "tests/test_kernel/test_transpose_pad_fusion_kernel.py", "content": "import torch\nimport random\nimport pytest\nfrom energonai.kernel import transpose_pad, transpose_depad\n\nseq_lens =  torch.tensor([24,127,31,65,24,127,31,65], dtype=torch.int64).cuda()\nbatch_size = 8\nmax_padding_size = 128\nhead_size = 64\nhead_num = 12\nhidden_size = head_num * head_size\n\n\ndef seq_init(x):\n    # for i in range(batch_size):\n    #     len = seq_lens[i]\n    #     for j in range (max_padding_size):\n    #         for k in range(hidden_size):\n    #             if(j<len):\n    #                 x[i,j,k] =  float(random.randint(0,9))\n\n    for i in range(batch_size):\n        len = seq_lens[i]\n        for j in range (max_padding_size):\n            for k in range(hidden_size):\n                if j < len:\n                    tmp = x[i:i+1, j:j+1, :]\n                    tmp.copy_(torch.randn(1,1,hidden_size).float())\n    return x      \n\n\ndef manual_depad(hidden_states):\n        # torch.Size([4, 512, 2048])\n        new_hidden_states = torch.zeros((1, batch_size*max_padding_size, hidden_size), dtype=torch.float).cuda()\n\n        valid_num = 0\n        for i in range(batch_size):\n            tmp = new_hidden_states[:, valid_num : valid_num + seq_lens[i], :]\n            tmp.copy_(hidden_states[i:i+1, 0:seq_lens[i], :])\n            valid_num += seq_lens[i]\n        new_hidden_states = new_hidden_states[:, 0:valid_num, :]\n        return new_hidden_states\n\ndef reshape(x):\n    new_x_shape = x.size()[:-1] + (head_num, head_size)\n    x = x.view(*new_x_shape)\n    return x\n\n# cpp_extension \ndef compare(ta, tb):\n    tta = torch.flatten(ta)\n    ttb = torch.flatten(tb)\n\n    len = torch.numel(tta)\n    for i in range(len):\n        if((tta[i]-ttb[i]) > 0.001):\n            print(i)\n            print(tta[i])\n            print(ttb[i])\n            return False\n    return True\n\ndef test_kernel():\n    # original \n    hidden_states = torch.zeros(batch_size, max_padding_size, head_num*head_size).cuda().float()\n    hidden_states = seq_init(hidden_states)\n    input_pad = reshape(hidden_states)\n    res_original_pad = input_pad.permute(0,2,1,3)\n\n    # transpose_pad\n    hidden_states_depad = manual_depad(hidden_states)\n    input_depad = reshape(hidden_states_depad)\n    res_transpose_pad = transpose_pad(input_depad, batch_size, max_padding_size, seq_lens, head_num, head_size)\n    assert compare(res_transpose_pad, res_original_pad) == True, \"transpose_pad fault.\"\n\n    # transpose_depad\n    sum_seq = torch.sum(seq_lens)\n    res_transpose_depad = transpose_depad(res_original_pad, batch_size, sum_seq, max_padding_size, seq_lens, head_num, head_size)\n    assert compare(input_depad, res_transpose_depad) == True, \"transpose_depad fault.\"\n\n\n\n\n\nif __name__ == '__main__':\n    test_kernel()"}
{"type": "test_file", "path": "tests/test_engine/test_pp.py", "content": "from colossalai.testing import rerun_if_address_is_in_use\nfrom test_engine.boring_model_utils import run_boring_model\nimport pytest\n\n\n@pytest.mark.dist\n@pytest.mark.standalone\n@rerun_if_address_is_in_use()\ndef test_pp():\n    run_boring_model(1, 2)\n\n\nif __name__ == '__main__':\n    test_pp()\n"}
{"type": "source_file", "path": "energonai/__init__.py", "content": "from .batch_mgr import BatchManager\nfrom .engine import launch_engine, SubmitEntry, QueueFullError\nfrom .task import TaskEntry\n\n\n__all__ = ['BatchManager', 'launch_engine', 'SubmitEntry', 'TaskEntry', 'QueueFullError']\n"}
{"type": "source_file", "path": "energonai/batch_mgr.py", "content": "from typing import Any, Hashable, Tuple, Deque, Iterable\nfrom dataclasses import dataclass\nfrom .task import TaskEntry\n\n\n@dataclass\nclass SubmitEntry:\n    uid: Hashable\n    data: Any\n\n\nclass BatchManager:\n    def make_batch(self, q: Deque[SubmitEntry]) -> Tuple[TaskEntry, dict]:\n        entry = q.popleft()\n        return TaskEntry((entry.uid, ), entry.data), {}\n\n    def split_batch(self, task_entry: TaskEntry, **kwargs: Any) -> Iterable[Tuple[Hashable, Any]]:\n        return [(task_entry.uids[0], task_entry.batch)]\n"}
{"type": "source_file", "path": "energonai/communication/__init__.py", "content": "from .collective import all_gather, reduce_scatter, all_reduce, broadcast, reduce\nfrom .p2p import (send_forward, send_forward_recv_forward, send_backward_recv_forward, send_backward,\n                  send_backward_recv_backward, send_forward_recv_backward, send_forward_backward_recv_forward_backward,\n                  recv_forward, recv_backward)\nfrom .ring import ring_forward\nfrom .utils import send_tensor_meta, recv_tensor_meta\n\n__all__ = [\n    'all_gather',\n    'reduce_scatter',\n    'all_reduce',\n    'broadcast',\n    'reduce',\n    'send_forward',\n    'send_forward_recv_forward',\n    'send_forward_backward_recv_forward_backward',\n    'send_backward',\n    'send_backward_recv_backward',\n    'send_backward_recv_forward',\n    'send_forward_recv_backward',\n    'recv_backward',\n    'recv_forward',\n    'ring_forward',\n    'send_tensor_meta',\n    'recv_tensor_meta',\n]\n"}
{"type": "source_file", "path": "energonai/communication/collective.py", "content": "#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n\nimport torch\nimport torch.distributed as dist\nfrom torch.distributed import ReduceOp\nfrom torch import Tensor\n\nfrom colossalai.core import global_context as gpc\nfrom colossalai.context import ParallelMode\nfrom colossalai.utils import get_current_device\n\n\ndef all_gather(tensor: Tensor, dim: int, parallel_mode: ParallelMode, async_op: bool = False) -> Tensor:\n    \"\"\"Gathers all tensors from the parallel group and concatenates them in a \n    specific dimension.\n\n    :param tensor: Tensor to be gathered\n    :param dim: The dimension concatenating in\n    :param parallel_mode: Parallel group mode used in this communication\n    :param async_op: Whether operations are asynchronous\n\n    :type tensor: :class:`torch.Tensor`\n    :type dim: int\n    :type parallel_mode: :class:`colossalai.context.ParallelMode`\n    :type async_op: bool, optional\n\n    :return: The tensor generated by all-gather\n    :rtype: :class:`torch.Tensor`\n    \"\"\"\n    depth = gpc.get_world_size(parallel_mode)\n    if depth == 1:\n        out = [tensor]\n        work = None\n    else:\n        shape = list(tensor.shape)\n        shape[0], shape[dim] = shape[dim], shape[0]\n        shape[0] *= depth\n        out = torch.empty(shape, dtype=tensor.dtype, device=get_current_device())\n        temp = list(torch.chunk(out, depth, dim=0))\n        work = dist.all_gather(tensor_list=temp,\n                               tensor=tensor.transpose(0, dim).contiguous(),\n                               group=gpc.get_group(parallel_mode),\n                               async_op=async_op)\n        out = torch.transpose(out, 0, dim)\n    if async_op:\n        return out, work\n    else:\n        return out\n\n\ndef reduce_scatter(tensor: Tensor,\n                   dim: int,\n                   parallel_mode: ParallelMode,\n                   op: ReduceOp = ReduceOp.SUM,\n                   async_op: bool = False) -> Tensor:\n    \"\"\"Reduces all tensors then scatters it in a specific dimension to all \n    members in the parallel group.\n\n    :param tensor: Tensor to be reduced and scattered\n    :param dim: The dimension scattering in\n    :param parallel_mode: Parallel group mode used in this communication\n    :param op: The type of reduce operation\n    :param async_op: Whether operations are asynchronous\n\n    :type tensor: :class:`torch.Tensor`\n    :type dim: int\n    :type parallel_mode: :class:`colossalai.context.ParallelMode`\n    :type op: ReduceOp, optional\n    :type async_op: bool, optional\n\n    :return: The tensor generated by reduce-scatter\n    :rtype: :class:`Tensor`\n    \"\"\"\n    depth = gpc.get_world_size(parallel_mode)\n    if depth == 1:\n        out = tensor\n        work = None\n    else:\n        temp = list(map(lambda x: x.contiguous(), torch.chunk(tensor, depth, dim=dim)))\n        out = torch.empty(temp[0].shape, dtype=tensor.dtype, device=get_current_device())\n        work = dist.reduce_scatter(output=out,\n                                   input_list=temp,\n                                   op=op,\n                                   group=gpc.get_group(parallel_mode),\n                                   async_op=async_op)\n    if async_op:\n        return out, work\n    else:\n        return out\n\n\ndef all_reduce(tensor: Tensor,\n               parallel_mode: ParallelMode,\n               op: ReduceOp = ReduceOp.SUM,\n               async_op: bool = False) -> Tensor:\n    depth = gpc.get_world_size(parallel_mode)\n    if depth == 1:\n        work = None\n    else:\n        work = dist.all_reduce(tensor.contiguous(), op=op, group=gpc.get_group(parallel_mode), async_op=async_op)\n    if async_op:\n        return tensor, work\n    else:\n        return tensor\n\n\ndef broadcast(tensor: Tensor, src: int, parallel_mode: ParallelMode, async_op: bool = False):\n    depth = gpc.get_world_size(parallel_mode)\n    if depth == 1:\n        work = None\n    else:\n        work = dist.broadcast(tensor.contiguous(), src=src, group=gpc.get_group(parallel_mode), async_op=async_op)\n    if async_op:\n        return tensor, work\n    else:\n        return tensor\n\n\ndef reduce(tensor: Tensor, dst: int, parallel_mode: ParallelMode, op: ReduceOp = ReduceOp.SUM, async_op: bool = False):\n    depth = gpc.get_world_size(parallel_mode)\n    if depth == 1:\n        work = None\n    else:\n        work = dist.reduce(tensor.contiguous(), dst=dst, op=op, group=gpc.get_group(parallel_mode), async_op=async_op)\n    if async_op:\n        return tensor, work\n    else:\n        return tensor\n\n\ndef scatter_object_list(scatter_object_output_list, scatter_object_input_list, src=0, group=None):\n    r\"\"\"Modified from `torch.distributed.scatter_object_list <https://pytorch.org/docs/stable/_modules/torch/distributed/distributed_c10d.html#scatter_object_list>` to fix issues\n    \"\"\"\n    if dist._rank_not_in_group(group):\n        return\n\n    if not isinstance(scatter_object_output_list, list) or len(scatter_object_output_list) < 1:\n        raise RuntimeError(\"Expected argument scatter_object_output_list to be a list of size at least 1.\")\n\n    # set tensor device to cuda if backend is nccl\n    device = torch.cuda.current_device() if dist.get_backend(group) == 'nccl' else torch.device(\"cpu\")\n\n    my_rank = dist.get_rank()    # use global rank\n    if my_rank == src:\n        tensor_list, tensor_sizes = zip(\n            *[dist.distributed_c10d._object_to_tensor(obj) for obj in scatter_object_input_list])\n        tensor_list = list(map(lambda x: x.to(device), tensor_list))\n        tensor_sizes = list(map(lambda x: x.to(device), tensor_sizes))\n\n    # Src rank broadcasts the maximum tensor size. This is because all ranks are\n    # expected to call into scatter() with equal-sized tensors.\n    if my_rank == src:\n        max_tensor_size = max(tensor_sizes)\n        for tensor in tensor_list:\n            tensor.resize_(max_tensor_size)\n    else:\n        max_tensor_size = torch.tensor([0], dtype=torch.long).to(device)\n\n    dist.broadcast(max_tensor_size, src=src, group=group)\n\n    # Scatter actual serialized objects\n    output_tensor = torch.empty(max_tensor_size.item(), dtype=torch.uint8).to(device)\n    dist.scatter(\n        output_tensor,\n        scatter_list=None if my_rank != src else tensor_list,\n        src=src,\n        group=group,\n    )\n\n    # Scatter per-object sizes to trim tensors when deserializing back to object\n    obj_tensor_size = torch.tensor([0], dtype=torch.long).to(device)\n    dist.scatter(\n        obj_tensor_size,\n        scatter_list=None if my_rank != src else tensor_sizes,\n        src=src,\n        group=group,\n    )\n\n    output_tensor, obj_tensor_size = output_tensor.cpu(), obj_tensor_size.cpu()\n    # Deserialize back to object\n    scatter_object_output_list[0] = dist.distributed_c10d._tensor_to_object(output_tensor, obj_tensor_size)\n"}
{"type": "source_file", "path": "energonai/communication/p2p.py", "content": "#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n\nfrom typing import List, Tuple, Union\nimport torch\nimport torch.distributed as dist\n\nfrom colossalai.core import global_context as gpc\nfrom colossalai.context import ParallelMode\nfrom colossalai.utils import get_current_device\nfrom functools import reduce\nimport operator\nfrom .utils import split_tensor_into_1d_equal_chunks, gather_split_1d_tensor\n\nTensorShape = Union[torch.Size, List[int], Tuple[int]]\n\n\ndef _get_tensor_shape(tensor_shape: TensorShape, chunk_tensor: bool = False) -> Tuple[TensorShape, bool]:\n    \"\"\"get the exact tensor shape when communicating and return whether the tensor is a chunk\n\n    :param tensor_shape: shape of tensor\n    :type tensor_shape: TensorShape\n    :param chunk_tensor: whether to chunk tensor, defaults to False\n    :type chunk_tensor: bool, optional\n    :return: exact tensor shape, whether to chunk tensor\n    :rtype: Tuple[Union[torch.Size, List[int], Tuple[int]], bool]\n    \"\"\"\n    if chunk_tensor:\n        tensor_chunk_shape = reduce(operator.mul, tensor_shape, 1)\n        tensor_parallel_world_size = gpc.get_world_size(ParallelMode.TENSOR)\n        if tensor_chunk_shape % tensor_parallel_world_size == 0:\n            tensor_chunk_shape = tensor_chunk_shape // tensor_parallel_world_size\n        else:\n            tensor_chunk_shape = tensor_shape\n            chunk_tensor = False\n    else:\n        tensor_chunk_shape = tensor_shape\n    return tensor_chunk_shape, chunk_tensor\n\n\ndef _communicate(tensor_send_next=None,\n                 tensor_send_prev=None,\n                 recv_prev=False,\n                 recv_next=False,\n                 recv_prev_shape=None,\n                 recv_next_shape=None,\n                 prev_rank=None,\n                 next_rank=None,\n                 dtype=None,\n                 scatter_gather_tensors=False):\n    \"\"\"\n    Adapted from megatron.p2p_communication.\n    Communicate tensors between stages. Used as helper method in other\n    communication methods that are used in pipeline schedule.\n    Takes the following arguments:\n        tensor_send_next: tensor to send to next rank (no tensor sent if\n                          set to None).\n        tensor_send_prev: tensor to send to prev rank (no tensor sent if\n                          set to None).\n        recv_prev: boolean for whether tensor should be received from\n                   previous rank.\n        recv_next: boolean for whether tensor should be received from\n                   next rank.\n    Returns:\n        (tensor_recv_prev, tensor_recv_next)\n    \"\"\"\n\n    # Create placeholder tensors for receive in forward and backward directions\n    # if needed.\n    tensor_recv_prev = None\n    tensor_recv_next = None\n\n    if recv_prev:\n        assert recv_prev_shape is not None\n        recv_prev_chunk_shape, recv_prev_split = _get_tensor_shape(recv_prev_shape, scatter_gather_tensors)\n        tensor_recv_prev = torch.empty(recv_prev_chunk_shape,\n                                       requires_grad=False,\n                                       device=get_current_device(),\n                                       dtype=dtype)\n    if recv_next:\n        assert recv_next_shape is not None\n        recv_next_chunk_shape, recv_next_split = _get_tensor_shape(recv_next_shape, scatter_gather_tensors)\n        tensor_recv_next = torch.empty(recv_next_chunk_shape,\n                                       requires_grad=False,\n                                       device=get_current_device(),\n                                       dtype=dtype)\n\n    if tensor_send_prev is not None or recv_prev:\n        if prev_rank is None:\n            prev_rank = gpc.get_prev_global_rank(ParallelMode.PIPELINE)\n\n    if tensor_send_next is not None or recv_next:\n        if next_rank is None:\n            next_rank = gpc.get_next_global_rank(ParallelMode.PIPELINE)\n\n    if tensor_send_prev is not None:\n        send_prev_split = _get_tensor_shape(tensor_send_prev.shape, scatter_gather_tensors)[1]\n        if send_prev_split:\n            tensor_send_prev = split_tensor_into_1d_equal_chunks(tensor_send_prev)\n\n    if tensor_send_next is not None:\n        send_next_split = _get_tensor_shape(tensor_send_next.shape, scatter_gather_tensors)[1]\n        if send_next_split:\n            tensor_send_next = split_tensor_into_1d_equal_chunks(tensor_send_next)\n\n    ops = []\n    if tensor_send_prev is not None:\n        send_prev_op = dist.P2POp(dist.isend, tensor_send_prev, prev_rank)\n        ops.append(send_prev_op)\n    if tensor_recv_prev is not None:\n        recv_prev_op = dist.P2POp(dist.irecv, tensor_recv_prev, prev_rank)\n        ops.append(recv_prev_op)\n    if tensor_recv_next is not None:\n        recv_next_op = dist.P2POp(dist.irecv, tensor_recv_next, next_rank)\n        ops.append(recv_next_op)\n    if tensor_send_next is not None:\n        send_next_op = dist.P2POp(dist.isend, tensor_send_next, next_rank)\n        ops.append(send_next_op)\n    if len(ops) > 0:\n        reqs = dist.batch_isend_irecv(ops)\n        for req in reqs:\n            req.wait()\n    # To protect against race condition when using batch_isend_irecv().\n    torch.cuda.synchronize()\n\n    if recv_prev and recv_prev_split:\n        tensor_recv_prev = gather_split_1d_tensor(tensor_recv_prev).view(recv_prev_shape).requires_grad_()\n    if recv_next and recv_next_split:\n        tensor_recv_next = gather_split_1d_tensor(tensor_recv_next).view(recv_next_shape).requires_grad_()\n    return tensor_recv_prev, tensor_recv_next\n\n\ndef recv_forward(input_tensor_shape, prev_rank=None, dtype=torch.float, scatter_gather_tensors=False):\n    \"\"\"Receives the input tensor from the previous member in pipeline.\n\n    :param input_tensor_shape: The shape of the tensor to be received\n    :param prev_rank: The rank of the source of the tensor\n    :type input_tensor_shape: torch.Size\n    :type prev_rank: int, optional\n    :return: The input tensor in forward step\n    :rtype: :class:`torch.Tensor`\n    \"\"\"\n    if gpc.is_pipeline_first_stage():\n        input_tensor = None\n    else:\n        input_tensor, _ = _communicate(recv_prev=True,\n                                       recv_prev_shape=input_tensor_shape,\n                                       prev_rank=prev_rank,\n                                       dtype=dtype,\n                                       scatter_gather_tensors=scatter_gather_tensors)\n    return input_tensor\n\n\ndef recv_backward(output_grad_shape, next_rank=None, dtype=torch.float, scatter_gather_tensors=False):\n    \"\"\"Receives the grad tensor from the next member in pipeline.\n\n    :param output_grad_shape: The shape of the tensor to be received\n    :param next_rank: The rank of the source of the tensor\n    :type output_grad_shape: torch.Size\n    :type next_rank: int, optional\n    :return: The grad of output tensor in forward step\n    :rtype: :class:`torch.Tensor`\n    \"\"\"\n    if gpc.is_pipeline_last_stage():\n        output_tensor_grad = None\n    else:\n        _, output_tensor_grad = _communicate(recv_next=True,\n                                             recv_next_shape=output_grad_shape,\n                                             next_rank=next_rank,\n                                             dtype=dtype,\n                                             scatter_gather_tensors=scatter_gather_tensors)\n    return output_tensor_grad\n\n\ndef send_forward(output_tensor, next_rank=None, scatter_gather_tensors=False):\n    \"\"\"Sends the input tensor to the next member in pipeline.\n\n    :param output_tensor: Tensor to be sent\n    :param next_rank: The rank of the recipient of the tensor\n    :type output_tensor: :class:`torch.Tensor`\n    :type next_rank: int, optional\n    \"\"\"\n    if not gpc.is_pipeline_last_stage():\n        _communicate(tensor_send_next=output_tensor, next_rank=next_rank, scatter_gather_tensors=scatter_gather_tensors)\n\n\ndef send_backward(input_tensor_grad, prev_rank=None, scatter_gather_tensors=False):\n    \"\"\"Sends the grad tensor to the previous member in pipeline.\n\n    :param input_tensor_grad: Tensor to be sent\n    :param prev_rank: The rank of the recipient of the tensor\n    :type input_tensor_grad: :class:`torch.Tensor`\n    :type prev_rank: int, optional\n    \"\"\"\n    if not gpc.is_pipeline_first_stage():\n        _communicate(tensor_send_prev=input_tensor_grad,\n                     prev_rank=prev_rank,\n                     scatter_gather_tensors=scatter_gather_tensors)\n\n\ndef send_forward_recv_backward(output_tensor,\n                               output_grad_shape,\n                               recv_next=True,\n                               next_rank=None,\n                               dtype=torch.float,\n                               scatter_gather_tensors=False):\n    \"\"\"Batched communication operation. Sends the input tensor to the \n    next member in pipeline, while receives the grad tensor from the\n    next member in pipeline.\n\n    :param output_tensor: Tensor to be sent\n    :param output_grad_shape: The shape of the tensor to be received\n    :type output_tensor: :class:`torch.Tensor`\n    :type output_grad_shape: :class:`torch.Size`\n    :return: The grad of output tensor in forward step\n    :rtype: :class:`torch.Tensor`\n    \"\"\"\n    if gpc.is_pipeline_last_stage():\n        output_tensor_grad = None\n    else:\n        _, output_tensor_grad = _communicate(tensor_send_next=output_tensor,\n                                             recv_next=recv_next,\n                                             recv_next_shape=output_grad_shape,\n                                             next_rank=next_rank,\n                                             dtype=dtype,\n                                             scatter_gather_tensors=scatter_gather_tensors)\n    return output_tensor_grad\n\n\ndef send_backward_recv_forward(input_tensor_grad,\n                               input_tensor_shape,\n                               recv_prev=True,\n                               prev_rank=None,\n                               dtype=torch.float,\n                               scatter_gather_tensors=False):\n    \"\"\"Batched communication operation. Sends the grad tensor to the \n    previous member in pipeline, while receives the input tensor from the\n    previous member in pipeline.\n\n    :param input_tensor_grad: Tensor to be sent\n    :param input_tensor_shape: The shape of the tensor to be received\n    :type input_tensor_grad: :class:`torch.Tensor`\n    :type input_tensor_shape: :class:`torch.Size`\n    :return: The input tensor in forward step\n    :rtype: :class:`torch.Tensor`\n    \"\"\"\n    if gpc.is_pipeline_first_stage():\n        input_tensor = None\n    else:\n        input_tensor, _ = _communicate(tensor_send_prev=input_tensor_grad,\n                                       recv_prev=recv_prev,\n                                       recv_prev_shape=input_tensor_shape,\n                                       prev_rank=prev_rank,\n                                       dtype=dtype,\n                                       scatter_gather_tensors=scatter_gather_tensors)\n    return input_tensor\n\n\ndef send_forward_recv_forward(output_tensor,\n                              input_tensor_shape,\n                              recv_prev=True,\n                              prev_rank=None,\n                              next_rank=None,\n                              dtype=torch.float,\n                              scatter_gather_tensors=False):\n    \"\"\"Batched communication operation. Sends the input tensor to the \n    next member in pipeline, while receives the input tensor from the\n    previous member in pipeline.\n\n    :param output_tensor: Tensor to be sent\n    :param input_tensor_shape: The shape of the tensor to be received\n    :type output_tensor: :class:`torch.Tensor`\n    :type input_tensor_shape: :class:`torch.Size`\n    :return: The input tensor in forward step\n    :rtype: :class:`torch.Tensor`\n    \"\"\"\n    input_tensor, _ = _communicate(tensor_send_next=output_tensor,\n                                   recv_prev=recv_prev,\n                                   recv_prev_shape=input_tensor_shape,\n                                   prev_rank=prev_rank,\n                                   next_rank=next_rank,\n                                   dtype=dtype,\n                                   scatter_gather_tensors=scatter_gather_tensors)\n    return input_tensor\n\n\ndef send_backward_recv_backward(input_tensor_grad,\n                                output_grad_shape,\n                                recv_next=True,\n                                prev_rank=None,\n                                next_rank=None,\n                                dtype=torch.float,\n                                scatter_gather_tensors=False):\n    \"\"\"Batched communication operation. Sends the grad tensor to the \n    previous member in pipeline, while receives the grad tensor from the\n    next member in pipeline.\n\n    :param input_tensor_grad: Tensor to be sent\n    :param output_grad_shape: The shape of the tensor to be received\n    :type input_tensor_grad: :class:`torch.Tensor`\n    :type output_grad_shape: :class:`torch.Size`\n    :return: The grad of output tensor in forward step\n    :rtype: :class:`torch.Tensor`\n    \"\"\"\n    _, output_tensor_grad = _communicate(tensor_send_prev=input_tensor_grad,\n                                         recv_next=recv_next,\n                                         recv_next_shape=output_grad_shape,\n                                         prev_rank=prev_rank,\n                                         next_rank=next_rank,\n                                         dtype=dtype,\n                                         scatter_gather_tensors=scatter_gather_tensors)\n    return output_tensor_grad\n\n\ndef send_forward_backward_recv_forward_backward(output_tensor,\n                                                input_tensor_grad,\n                                                input_tensor_shape,\n                                                output_grad_shape,\n                                                recv_prev=True,\n                                                recv_next=True,\n                                                prev_rank=None,\n                                                next_rank=None,\n                                                dtype=torch.float,\n                                                scatter_gather_tensors=False):\n    \"\"\"Batched communication operation. Sends the input tensor to the next and \n    the grad tensor to the previous, while receives the grad tensor from the\n    next and the input tensor from the previous.\n\n    :param output_tensor: Tensor sent to the next\n    :param input_tensor_grad: Tensor sent to the previous\n    :param input_tensor_shape: The shape of the tensor received from the previous\n    :param output_grad_shape: The shape of the tensor received from the next\n    :type output_tensor: :class:`torch.Tensor`\n    :type input_tensor_grad: :class:`torch.Tensor`\n    :type input_tensor_shape: :class:`torch.Size`\n    :type output_grad_shape: :class:`torch.Size`\n    :return: (the input tensor in forward step, the grad of output tensor in forward step)\n    :rtype: (Tensor, Tensor)\n    \"\"\"\n    input_tensor, output_tensor_grad = _communicate(tensor_send_next=output_tensor,\n                                                    tensor_send_prev=input_tensor_grad,\n                                                    recv_prev=recv_prev,\n                                                    recv_next=recv_next,\n                                                    recv_prev_shape=input_tensor_shape,\n                                                    recv_next_shape=output_grad_shape,\n                                                    prev_rank=prev_rank,\n                                                    next_rank=next_rank,\n                                                    dtype=dtype,\n                                                    scatter_gather_tensors=scatter_gather_tensors)\n    return input_tensor, output_tensor_grad\n"}
{"type": "source_file", "path": "energonai/communication/utils.py", "content": "import torch\nimport torch.distributed as dist\n\nfrom colossalai.core import global_context as gpc\nfrom colossalai.context import ParallelMode\nfrom colossalai.utils import get_current_device\n\n\ndef send_tensor_meta(tensor, need_meta=True, next_rank=None):\n    \"\"\"Sends tensor meta information before sending a specific tensor. \n    Since the recipient must know the shape of the tensor in p2p communications,\n    meta information of the tensor should be sent before communications. This function\n    synchronizes with :func:`recv_tensor_meta`.\n\n    :param tensor: Tensor to be sent\n    :param need_meta: If False, meta information won't be sent\n    :param next_rank: The rank of the next member in pipeline parallel group\n    :type tensor: Tensor\n    :type need_meta: bool, optional\n    :type next_rank: int\n    :return: False\n    :rtype: bool\n    \"\"\"\n    if need_meta:\n        if next_rank is None:\n            next_rank = gpc.get_next_global_rank(ParallelMode.PIPELINE)\n\n        tensor_kwargs = {'dtype': torch.long, 'device': get_current_device()}\n\n        send_shape = torch.tensor(tensor.size(), **tensor_kwargs)\n        send_ndims = torch.tensor(len(tensor.size()), **tensor_kwargs)\n        dist.send(send_ndims, next_rank)\n        dist.send(send_shape, next_rank)\n\n    return False\n\n\ndef recv_tensor_meta(tensor_shape, prev_rank=None):\n    \"\"\"Recieves tensor meta information before recieving a specific tensor. \n    Since the recipient must know the shape of the tensor in p2p communications,\n    meta information of the tensor should be recieved before communications. This function\n    synchronizes with :func:`send_tensor_meta`.\n\n    :param tensor_shape: The shape of the tensor to be recieved\n    :param prev_rank: The rank of the source of the tensor\n    :type tensor_shape: torch.Size\n    :type prev_rank: int, optional\n    :return: The shape of the tensor to be recieved\n    :rtype: torch.Size\n    \"\"\"\n    if tensor_shape is None:\n        if prev_rank is None:\n            prev_rank = gpc.get_prev_global_rank(ParallelMode.PIPELINE)\n\n        tensor_kwargs = {'dtype': torch.long, 'device': get_current_device()}\n\n        recv_ndims = torch.empty((), **tensor_kwargs)\n        dist.recv(recv_ndims, prev_rank)\n        recv_shape = torch.empty(recv_ndims, **tensor_kwargs)\n        dist.recv(recv_shape, prev_rank)\n\n        tensor_shape = torch.Size(recv_shape)\n\n    return tensor_shape\n\n\ndef split_tensor_into_1d_equal_chunks(tensor, new_buffer=False):\n    \"\"\"Break a tensor into equal 1D chunks.\n\n    :param tensor: Tensor to be splitted before communication\n    :param new_buffer: Whether uses a new buffer to store sliced tensor\n\n    :type tensor: torch.Tensor\n    :type new_buffer: bool, optional\n\n    :return splitted_tensor: The splitted tensor\n    :rtype splitted_tensor: torch.Tensor\n    \"\"\"\n    partition_size = torch.numel(tensor) // gpc.get_world_size(ParallelMode.PARALLEL_1D)\n    start_index = partition_size * gpc.get_local_rank(ParallelMode.PARALLEL_1D)\n    end_index = start_index + partition_size\n    if new_buffer:\n        data = torch.empty(partition_size, dtype=tensor.dtype, device=torch.cuda.current_device(), requires_grad=False)\n        data.copy_(tensor.view(-1)[start_index:end_index])\n    else:\n        data = tensor.view(-1)[start_index:end_index]\n    return data\n\n\ndef gather_split_1d_tensor(tensor):\n    \"\"\"Opposite of above function, gather values from model parallel ranks.\n\n    :param tensor: Tensor to be gathered after communication\n    :type tensor: torch.Tensor\n\n    :return gathered: The gathered tensor\n    :rtype gathered: torch.Tensor\n    \"\"\"\n    world_size = gpc.get_world_size(ParallelMode.PARALLEL_1D)\n    numel = torch.numel(tensor)\n    numel_gathered = world_size * numel\n    gathered = torch.empty(numel_gathered, dtype=tensor.dtype, device=torch.cuda.current_device(), requires_grad=False)\n    chunks = [gathered[i * numel:(i + 1) * numel] for i in range(world_size)]\n    dist.all_gather(chunks, tensor, group=gpc.get_group(ParallelMode.PARALLEL_1D))\n    return gathered\n"}
{"type": "source_file", "path": "energonai/communication/ring.py", "content": "#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n\nimport torch\n\nfrom colossalai.core import global_context as gpc\nfrom colossalai.context import ParallelMode\nfrom colossalai.utils import get_current_device, synchronize\n\n\ndef ring_forward(tensor_send_next: torch.Tensor, parallel_mode: ParallelMode):\n    \"\"\"Sends a tensor to the next member and recieves a tensor from the previous member.\n    This function returns the recieved tensor from the previous member.\n\n    :param tensor_send_next: Tensor sent to next member\n    :param parallel_mode: Parallel group mode used in this communication\n    :type tensor_send_next: :class:`torch.Tensor`\n    :type parallel_mode: :class:`colossalai.context.ParallelMode`\n    :return: The tensor recieved from the previous\n    :rtype: :class:`torch.Tensor`\n    \"\"\"\n    buffer_shape = tensor_send_next.size()\n\n    ops = []\n    current_rank = gpc.get_global_rank()\n\n    tensor_recv_prev = torch.empty(buffer_shape,\n                                   requires_grad=True,\n                                   device=get_current_device(),\n                                   dtype=tensor_send_next.dtype)\n\n    # send to next rank\n    send_next_op = torch.distributed.P2POp(torch.distributed.isend, tensor_send_next,\n                                           gpc.get_next_global_rank(parallel_mode))\n    ops.append(send_next_op)\n\n    # receive from prev rank\n    recv_prev_op = torch.distributed.P2POp(torch.distributed.irecv, tensor_recv_prev,\n                                           gpc.get_prev_global_rank(parallel_mode))\n    ops.append(recv_prev_op)\n\n    if current_rank % 2 == 0:\n        ops = ops[::-1]\n\n    reqs = torch.distributed.batch_isend_irecv(ops)\n    for req in reqs:\n        req.wait()\n\n    # To protect against race condition when using batch_isend_irecv().\n    synchronize()\n\n    return tensor_recv_prev\n"}
{"type": "source_file", "path": "energonai/engine.py", "content": "import asyncio\nimport signal\nimport time\nfrom collections import deque\nfrom threading import Lock, Thread\nfrom typing import Any, Callable, Deque, Dict, Hashable, List, Optional, Tuple\n\nimport torch.distributed.rpc as trpc\nimport torch.nn as nn\nfrom colossalai.logging import get_dist_logger\n\nfrom .batch_mgr import BatchManager, SubmitEntry\nfrom .pipe import Pipe\nfrom .task import TaskEntry\nfrom .utils import Terminator, build_device_maps, use_lock\nfrom .worker import launch_workers\n\n\nclass QueueFullError(Exception):\n    pass\n\n\nclass AsyncEngine:\n    def __init__(self, tp_world_size: int, pp_world_size: int, master_host: str, rpc_port: int, n_proc_per_node: int,\n                 batch_manager: Optional[BatchManager] = None, pipe_size: int = 1, queue_size: int = 0, rpc_disable_shm: bool = True) -> None:\n        self.lock = Lock()\n        self.logger = get_dist_logger('energonai')\n        if batch_manager is None:\n            self.batch_manager = BatchManager()\n        else:\n            assert isinstance(batch_manager, BatchManager)\n            self.batch_manager = batch_manager\n        self.world_size = tp_world_size * pp_world_size\n\n        rpc_options = {}\n        if rpc_disable_shm:\n            # SHM may lead to timeout error. Disabling SHM and only enabling uv transport can solve this problem.\n            # See https://discuss.pytorch.org/t/rpc-behavior-difference-between-pytorch-1-7-0-vs-1-9-0/124772/5\n            # This is a workaround and may be solved in the future.\n            rpc_options['_transports'] = ['uv']\n        trpc.init_rpc('master', rank=0, world_size=self.world_size + 1,\n                      rpc_backend_options=trpc.TensorPipeRpcBackendOptions(\n                          init_method=f'tcp://{master_host}:{rpc_port}',\n                          device_maps=build_device_maps(self.world_size, n_proc_per_node),\n                          **rpc_options\n                      ))\n        self.from_worker_pipes: List[Pipe] = []\n        for i in range(self.world_size):\n            pipe = Pipe(f'{i}_to_m', f'worker{i}', 'master')\n            self.from_worker_pipes.append(pipe)\n        self.submit_pipes: List[Pipe] = []\n        self.completion_pipes: List[Pipe] = []\n        for i, pipe in enumerate(self.from_worker_pipes):\n            worker_pp_rank = pipe.recv()\n            if worker_pp_rank == 0:\n                self.submit_pipes.append(Pipe(f'm_to_{i}', 'master', f'worker{i}', max_size=pipe_size))\n            if worker_pp_rank == pp_world_size - 1:\n                self.completion_pipes.append(pipe)\n\n        self.running: bool = False\n        self.submit_thread = None\n        self.completion_thread = None\n        self.queue_size = queue_size\n        self.submit_queue: Deque[SubmitEntry] = deque()\n        self.batch_info: Dict[Hashable, Any] = {}\n        self.timer_info: Dict[Hashable, Tuple[int, float]] = {}\n        self.completion_map: Dict[Hashable, Any] = {}\n\n        self.logger.info('Engine start')\n        self._start()\n        self.register_sigint()\n\n    def _submit_loop(self) -> None:\n        while self.running:\n            if len(self.submit_queue) > 0:\n                task_entry, batch_info = self.batch_manager.make_batch(self.submit_queue)\n                self.batch_info[task_entry.uids] = batch_info\n                self.timer_info[task_entry.uids] = (len(task_entry.uids), time.time())\n                for pipe in self.submit_pipes:\n                    pipe.send(task_entry)\n            else:\n                time.sleep(0.01)\n\n    def _completion_loop(self) -> None:\n        received_data: Dict[int, Any] = {}\n        while self.running:\n            for i, pipe in enumerate(self.completion_pipes):\n                if i not in received_data:\n                    try:\n                        received_data[i] = pipe.recv_nowait()\n                    except RuntimeError:\n                        pass\n            if len(received_data) == len(self.completion_pipes):\n                # TODO: validate they are all the same\n                task_entries: List[TaskEntry] = list(map(lambda k: received_data[k], sorted(received_data.keys())))\n                received_data.clear()\n                batch_info = self.batch_info.pop(task_entries[0].uids)\n                for uid, output in self.batch_manager.split_batch(task_entries[0], **batch_info):\n                    self.completion_map[uid] = output\n                batch_size, start_time = self.timer_info.pop(task_entries[0].uids)\n                self.logger.info(f'batch size: {batch_size}, time: {time.time() -start_time:.3f}')\n            else:\n                time.sleep(0.01)\n\n    def _start(self) -> None:\n        self.running = True\n        self.submit_thread = Thread(target=self._submit_loop)\n        self.submit_thread.start()\n        self.completion_thread = Thread(target=self._completion_loop)\n        self.completion_thread.start()\n\n    def shutdown(self) -> None:\n        with use_lock(self.lock):\n            if not self.running:\n                return\n            self.running = False\n        Terminator.shield()\n        for i in range(self.world_size):\n            trpc.rpc_sync(f'worker{i}', Terminator.terminate)\n        trpc.shutdown()\n        self.submit_thread.join()\n        self.completion_thread.join()\n\n    def submit(self, uid: Hashable, data: Any) -> None:\n        assert self.submit_thread.is_alive()\n        assert uid not in self.completion_map\n        if self.queue_size > 0 and len(self.submit_queue) >= self.queue_size:\n            raise QueueFullError(f'Submit queue full, size: {self.queue_size}')\n        self.submit_queue.append(SubmitEntry(uid, data))\n\n    async def wait(self, uid: Hashable) -> Any:\n        assert self.completion_thread.is_alive()\n        while True:\n            if uid in self.completion_map:\n                output = self.completion_map[uid]\n                del self.completion_map[uid]\n                return output\n            await asyncio.sleep(0.1)\n\n    def get(self, uid: Hashable, interval: float = 0.05) -> Any:\n        assert self.completion_thread.is_alive()\n        while True:\n            if uid in self.completion_map:\n                output = self.completion_map[uid]\n                del self.completion_map[uid]\n                return output\n            time.sleep(interval)\n\n    def _sigint_handler(self, *_):\n        self.shutdown()\n        raise KeyboardInterrupt\n\n    def register_sigint(self):\n        signal.signal(signal.SIGINT, self._sigint_handler)\n\n\ndef launch_engine(tp_world_size: int, pp_world_size: int, master_host: str, master_port: int, rpc_port: int,\n                  model_fn: Callable[[Any], nn.Module], n_nodes: int = 1, node_rank: int = 0, batch_manager: Optional[BatchManager] = None,\n                  pipe_size: int = 1, queue_size: int = 0, rpc_disable_shm: bool = True, **model_kwargs: Any) -> Optional[AsyncEngine]:\n    world_size = tp_world_size * pp_world_size\n    assert world_size % n_nodes == 0\n    n_proc_per_node = world_size // n_nodes\n    launch_workers(tp_world_size, pp_world_size, master_host, master_port, rpc_port,\n                   model_fn, n_proc_per_node=n_proc_per_node, node_rank=node_rank, pipe_size=pipe_size, **model_kwargs)\n    if node_rank == 0:\n        engine = AsyncEngine(tp_world_size, pp_world_size, master_host, rpc_port,\n                             n_proc_per_node, batch_manager=batch_manager, pipe_size=pipe_size, queue_size=queue_size, rpc_disable_shm=rpc_disable_shm)\n        return engine\n"}
{"type": "source_file", "path": "energonai/kernel/__init__.py", "content": "from .cuda_native import transpose_pad, transpose_depad, depad, scale_mask_softmax\nfrom .cuda_native import ft_build_padding_offsets, ft_remove_padding, ft_rebuild_padding, ft_transpose_remove_padding, ft_transpose_rebuild_padding\nfrom .cuda_native import linear, find_algo\n# from .cuda_native import OneLayerNorm\n\n__all__ = [\n    \"transpose_pad\", \"transpose_depad\", \"depad\", \"scale_mask_softmax\", \"ft_build_padding_offsets\", \"ft_remove_padding\",\n    \"ft_rebuild_padding\", \"ft_transpose_remove_padding\", \"ft_transpose_rebuild_padding\", \"linear\", \"find_algo\", \n    # \"OneLayerNorm\"\n]\n"}
{"type": "source_file", "path": "energonai/kernel/cuda_native/__init__.py", "content": "from .transpose_pad import transpose_pad, transpose_depad, depad\nfrom .transpose_pad import ft_build_padding_offsets, ft_remove_padding, ft_rebuild_padding, ft_transpose_remove_padding, ft_transpose_rebuild_padding\nfrom .scale_mask_softmax import scale_mask_softmax\nfrom .layer_norm import MixedFusedLayerNorm as LayerNorm\nfrom .linear_func import linear, find_algo"}
{"type": "source_file", "path": "energonai/kernel/cuda_native/layer_norm.py", "content": "\"\"\"This code is from NVIDIA apex:\n      https://github.com/NVIDIA/apex\n   with some changes. \"\"\"\n\nimport numbers\nimport torch\nfrom torch.nn.parameter import Parameter\nfrom torch.nn import init\nfrom torch.cuda.amp import custom_fwd, custom_bwd\nimport importlib\n\nglobal colossal_layer_norm_cuda\ncolossal_layer_norm_cuda = None\n\n\nclass FusedLayerNormAffineFunction(torch.autograd.Function):\n\n    @staticmethod\n    @custom_fwd(cast_inputs=torch.float32)\n    def forward(ctx, input, weight, bias, normalized_shape, eps):\n\n        ctx.normalized_shape = normalized_shape\n        ctx.eps = eps\n        input_ = input.contiguous()\n        weight_ = weight.contiguous()\n        bias_ = bias.contiguous()\n        output, mean, invvar = colossal_layer_norm_cuda.forward_affine(input_, ctx.normalized_shape, weight_, bias_,\n                                                                       ctx.eps)\n        ctx.save_for_backward(input_, weight_, bias_, mean, invvar)\n\n        return output\n\n    @staticmethod\n    @custom_bwd\n    def backward(ctx, grad_output):\n\n        input_, weight_, bias_, mean, invvar = ctx.saved_tensors\n        grad_input = grad_weight = grad_bias = None\n        grad_input, grad_weight, grad_bias \\\n            = colossal_layer_norm_cuda.backward_affine(\n                grad_output.contiguous(), mean, invvar,\n                input_, ctx.normalized_shape,\n                weight_, bias_, ctx.eps)\n\n        return grad_input, grad_weight, grad_bias, None, None\n\n\nclass MixedFusedLayerNorm(torch.nn.Module):\n\n    def __init__(self, normalized_shape, eps=1e-5, device=None, dtype=None):\n        super(MixedFusedLayerNorm, self).__init__()\n\n        global colossal_layer_norm_cuda\n        if colossal_layer_norm_cuda is None:\n            try:\n                colossal_layer_norm_cuda = importlib.import_module(\"energonai_layer_norm\")\n            except ImportError:\n                raise RuntimeError('MixedFusedLayerNorm requires cuda extensions')\n\n        if isinstance(normalized_shape, numbers.Integral):\n            normalized_shape = (normalized_shape,)\n        self.normalized_shape = torch.Size(normalized_shape)\n        self.eps = eps\n        self.weight = Parameter(torch.empty(*normalized_shape, device=device, dtype=dtype))\n        self.bias = Parameter(torch.empty(*normalized_shape, device=device, dtype=dtype))\n        self.reset_parameters()\n\n    def reset_parameters(self):\n\n        init.ones_(self.weight)\n        init.zeros_(self.bias)\n\n    def forward(self, input):\n\n        return FusedLayerNormAffineFunction.apply(input, self.weight, self.bias, self.normalized_shape, self.eps)\n\n    def __repr__(self):\n        return f'MixedFusedLayerNorm(normalized_shape={self.normalized_shape}, eps={self.eps})'\n"}
{"type": "source_file", "path": "energonai/kernel/cuda_native/linear_func.py", "content": "import torch\nimport time\nimport importlib\n\ntry:\n    energonai_linear = importlib.import_module(\"energonai_linear_func\")\nexcept ImportError:\n    raise RuntimeError('energonai_linear_func requires cuda extensions')\n\n\ndef linear(inputs, param, algo=-1):\n    \"\"\"\n    Linear function using Cublas\n\n    Args:\n        inputs (tensor): (batch, seq_len, din)\n        param (tensor): (dout, din)\n        algo (int): Cublas GEMM algorithms, defaults to -1. No effect for Ampere architecture gpu or above.\n            -1: Apply Heuristics to select the GEMM algorithm\n            0~23: Explicitly choose a GEMM algorithm\n            99: Apply Heuristics to select the GEMM algorithm while allowing the use of Tensor Core operations if possible\n            100~115: Explicitly choose a GEMM algorithm allowing it to use Tensor Core operations if possible\n    Returns:\n        tensor: (batch, seq_len, dout)\n    \"\"\"\n    assert inputs.is_contiguous()\n    assert param.is_contiguous()\n    assert len(inputs.shape) == 3\n    assert len(param.shape) == 2\n    assert inputs.shape[2] == param.shape[1]\n    assert isinstance(algo, int) and (-1 <= algo <= 23 or 99 <= algo <= 115)\n    return energonai_linear.mlp_gemm(inputs, param, algo)\n\n\n@torch.no_grad()\ndef find_algo():\n    \"\"\"\n    Auto find best algo, may take tens of seconds\n    \n    Returns:\n        int: best algo\n    \"\"\"\n    batch_size = 16\n    seq_len = 64\n    din = 12288\n    dout = 49152\n\n    inner_loop = 3\n    \n    input_list = []\n    param_list = []\n    for i in range(inner_loop):\n        input_list.append(torch.randn(batch_size, seq_len, din).half().cuda())\n        param_list.append(torch.randn(dout, din).half().cuda())\n\n    start_algo = -1\n    end_algo = 23\n    start_algo_t_op = 99\n    end_algo_t_op = 115\n    \n    algo_map = {}\n    for algo in range(start_algo, end_algo + 1):\n        algo_map[algo] = 0\n    for algo in range(start_algo_t_op, end_algo_t_op + 1):\n        algo_map[algo] = 0\n\n    for i in range(inner_loop):\n        _ = linear(input_list[i], param_list[i], start_algo)\n        _ = linear(input_list[i], param_list[i], start_algo)\n\n        for algo in range(start_algo, end_algo + 1):\n            torch.cuda.synchronize()\n            start_time = time.time()\n            _ = linear(input_list[i], param_list[i], algo)\n            torch.cuda.synchronize()\n            algo_map[algo] += time.time() - start_time\n\n        for algo in range(start_algo_t_op, end_algo_t_op + 1):\n            torch.cuda.synchronize()\n            start_time = time.time()\n            _ = linear(input_list[i], param_list[i], algo)\n            torch.cuda.synchronize()\n            algo_map[algo] += time.time() - start_time\n\n    best_idx = None\n    best_value = 999\n    for key, value in algo_map.items():\n        if value < best_value:\n            best_value = value\n            best_idx = key\n        \n    return best_idx\n"}
{"type": "source_file", "path": "energonai/kernel/cuda_native/scale_mask_softmax.py", "content": "import torch\nimport importlib\n\ntry:\n    energonai_scale_mask = importlib.import_module(\"energonai_scale_mask\")\nexcept ImportError:\n    raise RuntimeError('energonai_scale_mask requires cuda extensions')\n\n\ndef scale_mask_softmax(batch_size, batch_seq_len, head_num, src, seq_len_list):\n    src = src.contiguous()\n    dst = energonai_scale_mask.scale_mask_softmax_wrapper(batch_size, batch_seq_len, head_num, src, seq_len_list)\n    return dst"}
{"type": "source_file", "path": "energonai/kernel/cuda_native/transpose_pad.py", "content": "import torch\nimport importlib\n\ntry:\n    energonai_transpose_pad = importlib.import_module(\"energonai_transpose_pad\")\nexcept ImportError:\n    raise RuntimeError('transpose_pad requires cuda extensions')\n\n# from transpose import transpose_pad_wrapper, transpose_depad_wrapper\n\n\ndef transpose_pad(src, batch_size, max_seq_len, seq_len_list, head_num, size_per_head):\n    src = src.contiguous()\n\n    dst = energonai_transpose_pad.transpose_pad_wrapper(src, batch_size, max_seq_len, seq_len_list, head_num,\n                                                      size_per_head)\n\n    return dst\n\n\ndef transpose_depad(src, batch_size, sum_seq, max_seq_len, seq_len_list, head_num, size_per_head):\n    src = src.contiguous()\n\n    dst = energonai_transpose_pad.transpose_depad_wrapper(src, batch_size, sum_seq, max_seq_len, seq_len_list, head_num,\n                                                        size_per_head)\n\n    return dst\n\n\ndef depad(src, batch_size, seq_lens):\n    dst = src[0:1, 0:seq_lens[0], :]\n\n    for i in range(1, batch_size):\n        tlen = seq_lens[i]\n        dst = torch.cat([dst, src[i:i + 1, 0:tlen, :]], dim=1)\n\n    return dst\n\n\n# From FasterTransformer\n\n\ndef ft_build_padding_offsets(seq_lens, batch_size, max_seq_len, valid_word_num, tmp_mask_offset):\n    seq_lens = seq_lens.contiguous()\n    # tmp_mask_offset = tmp_mask_offset.contiguous()\n\n    energonai_transpose_pad.ft_build_padding_offsets_wrapper(seq_lens, batch_size, max_seq_len, valid_word_num,\n                                                           tmp_mask_offset)\n\n\ndef ft_remove_padding(src, tmp_mask_offset, mask_offset, valid_word_num, hidden_dim):\n    src = src.contiguous()\n    # tmp_mask_offset = tmp_mask_offset.contiguous()\n    # mask_offset = mask_offset.contiguous()\n\n    dst = energonai_transpose_pad.ft_remove_padding_wrapper(src, tmp_mask_offset, mask_offset, valid_word_num, hidden_dim)\n    return dst\n\n\ndef ft_rebuild_padding(src, mask_offset, valid_word_num, hidden_dim, batch_size, max_seq_len):\n    src = src.contiguous()\n    # mask_offset = mask_offset.contiguous()\n\n    dst = energonai_transpose_pad.ft_rebuild_padding_wrapper(src, mask_offset, valid_word_num, hidden_dim, batch_size,\n                                                           max_seq_len)\n    return dst\n\n\ndef ft_transpose_rebuild_padding(Q, K, V, q_buf, k_buf, v_buf, batch_size, seq_len, head_num, size_per_head,\n                                 valid_word_num, mask_offset):\n    Q = Q.contiguous()\n    K = K.contiguous()\n    V = V.contiguous()\n    q_buf = q_buf.contiguous()\n    k_buf = k_buf.contiguous()\n    v_buf = v_buf.contiguous()\n\n    energonai_transpose_pad.ft_transpose_rebuild_padding_wrapper(Q, K, V, q_buf, k_buf, v_buf, batch_size, seq_len,\n                                                               head_num, size_per_head, valid_word_num, mask_offset)\n\n\ndef ft_transpose_remove_padding(src, valid_word_num, batch_size, seq_len, head_num, size_per_head, mask_offset):\n    src = src.contiguous()\n\n    dst = energonai_transpose_pad.ft_transpose_remove_padding_wrapper(src, valid_word_num, batch_size, seq_len, head_num,\n                                                                    size_per_head, mask_offset)\n    return dst\n"}
{"type": "source_file", "path": "energonai/legacy_batch_mgr/__init__.py", "content": "from .worker_server import launch_worker\n"}
{"type": "source_file", "path": "energonai/legacy_batch_mgr/naive_batch_manager.py", "content": "\"\"\"\n------------------------------------------\nClass Batch Manager.\na naive version that is used for cases in which padding is not needed.\n------------------------------------------\n\"\"\"\nimport time\nimport redis\nfrom energonai.context import MEATCONFIG\nimport threading\nfrom readerwriterlock import rwlock\nimport logging\nfrom concurrent.futures import ThreadPoolExecutor\n\n\nclass single_request:\n\n    def __init__(self, input_, time_stamp: float, input_str: str):\n        \"\"\"\n        class to store related information for a single request.\n        :param input_: The output of GPT2Tokenizer.tokenizer, a dict including input_ids and attention_mask\n        :param time_stamp: The time stamp when we receive the request. We use the time stamp as a index to\n                            identify the request.\n        :param input_str: The input string of the request.\n        \"\"\"\n        self.input_ = input_\n        self.text = input_str\n        self.time_ = time_stamp\n        self.seq_len = input_['input_ids'].shape[1]\n\n\nclass Manager:\n    \"\"\"\n    Base class of batch manager.\n    \"\"\"\n\n    def __init__(self):\n        pass\n\n    def insert_req(self, time_stamp: float, input_ids, input_str: str):\n        pass\n\n\nclass Naive_Batch_Manager(Manager):\n    \"\"\"\n    This batch manager is mainly used for maintaining a queue of request to be processed. The requests in the\n    queue is wrapped into batches and then sent into the inference engine.\n    \"\"\"\n\n    def __init__(self, forward_func,\n                 result_process):\n        \"\"\"\n        :param forward_func a function of calling a forward propagation, returning a RPC ref.\n        :param result_process a function to process the output of the model before returning the result.\n        \"\"\"\n        super().__init__()\n        self.req_list = []\n        self.max_batch_size = MEATCONFIG['max_batch_size']\n        self.max_sequence_length = MEATCONFIG['max_sequence_length']\n        self.req_list_lock = rwlock.RWLockFair()\n        self.write_lock = self.req_list_lock.gen_wlock()\n        self.running_flag = True\n        self.publisher = redis.StrictRedis('localhost', 6379, charset=\"utf-8\", decode_responses=True)\n        self.max_workers = MEATCONFIG['pp_init_size'] + 2\n        self.pool = ThreadPoolExecutor(max_workers=self.max_workers)\n        self.working_workers = 0\n        self.forward_func = forward_func\n        self.result_process = result_process\n        self.main_thread = threading.Thread(target=self.processing_batch)\n        self.main_thread.start()\n\n    def insert_req(self, time_stamp: float, input_ids, input_str: str):\n        \"\"\"\n        Build a single_request class with the input string and then insert it into the queue.\n        \"\"\"\n        tmp_req = single_request(input_ids, time_stamp, input_str)\n        self.write_lock.acquire()\n        self.req_list.append(tmp_req)\n        self.write_lock.release()\n\n    def subscribe_result(self, time_stamp):\n        \"\"\"\n        waiting for the result and send back.\n        \"\"\"\n        sub = self.publisher.pubsub()\n        sub.subscribe(str(time_stamp))\n        predictions = ''\n        for message in sub.listen():\n            if message is not None and isinstance(message, dict):\n                predictions = message.get('data')\n                if not isinstance(predictions, int):\n                    break\n        return predictions\n\n    def wrap_batch(self):\n        \"\"\"\n        Simply wrap batches by the order of insertion.\n        \"\"\"\n        self.write_lock.acquire()\n        result_batch = self.req_list[0:min(self.max_batch_size, len(self.req_list))]\n        del self.req_list[0:min(self.max_batch_size, len(self.req_list))]\n        self.write_lock.release()\n        return result_batch\n\n    def processing_batch(self):\n        \"\"\"\n        The background process that continuously calls wrap_batch, puts the batch into the inference engine,\n        and starts new processes that wait for and publish the inference result.\n        \"\"\"\n        while self.running_flag:\n            if (self.working_workers < self.max_workers) and (len(self.req_list) > 0):\n                target_batch = self.wrap_batch()\n                pad_len = max([p.seq_len for p in target_batch])\n                logging.info(\"A batch with {} requests and length of {} packed, in-batch length: {}\".format(\n                    len(target_batch), pad_len, [p.seq_len for p in target_batch]))\n                input_text = [i.text for i in target_batch]\n                self.working_workers = self.working_workers + 1\n                output_ = self.forward_func(input_list=input_text)\n                self.pool.submit(self.publish_result, output_, target_batch)\n            time.sleep(0.001)\n\n    def publish_result(self, output, target_batch):\n        \"\"\"\n        Background process that waits for the inference result and uses the publisher of Redis to publish it to\n        the waiting requests.\n        :param output: the rpc reference of the inference result.\n        :param target_batch: the input batch\n        \"\"\"\n        predictions = output.to_here()\n        for i in range(len(target_batch)):\n            temp_st = target_batch[i].time_\n            chosen_pred = predictions[i]\n            result = self.result_process(chosen_pred)\n            self.publisher.publish(str(temp_st), result)\n\n        self.working_workers = self.working_workers - 1\n\n"}
{"type": "source_file", "path": "energonai/legacy_batch_mgr/dynamic_batch_manager.py", "content": "\"\"\"\n------------------------------------------\nClass Batch Manager and the function for generating cached cost.\nPart of the algorithm comes from Turbo Transformer.\n------------------------------------------\n\"\"\"\nimport time\nfrom collections import deque\nimport numpy as np\nimport scipy.stats as stats\nimport redis\nimport math\nimport os\nfrom tqdm import trange\nimport threading\nfrom readerwriterlock import rwlock\nimport logging\nfrom concurrent.futures import ThreadPoolExecutor\nfrom energonai.context import MEATCONFIG\n\nclass gamma_dist:\n    \"\"\"\n    Data structure for recording the distribution of the requests\n    \"\"\"\n    def __init__(self, alpha_, loc_, beta_):\n        self.alpha = alpha_\n        self.loc = loc_\n        self.beta = beta_\n        self.max_list_len = 5 * MEATCONFIG['max_batch_size']\n        self.max_seq_len = MEATCONFIG['max_sequence_length']\n\n    def complete_req_list(self, req_list):\n        \"\"\"\n        Use fake requests to fill the req list to a certain number so that the scheduler\n        can perform better. The length of the fake requests is generated with the current\n        recorded distribution.\n        \"\"\"\n        new_size = self.max_list_len - len(req_list)\n        if new_size <= 0:\n            req_list.sort(key=lambda x: x.seq_len)\n            return req_list\n        res = stats.gamma.rvs(self.alpha, loc=self.loc, scale=self.beta, size=new_size)\n        res = [math.floor(i) + 1 for i in res]\n        res = [i if i < self.max_seq_len else self.max_seq_len for i in res]\n        new_req = [single_request(input_=None, time_stamp=None, seq_len=i, input_str=None) for i in res]\n        new_req.extend(req_list)\n        new_req.sort(key=lambda x: x.seq_len)\n        return new_req\n\n\nclass single_request:\n\n    def __init__(self, input_, time_stamp, input_str, seq_len=None):\n        \"\"\"\n        class to store related information for a single request.\n        :param input_: The output of GPT2Tokenizer.tokenizer, a dict including input_ids and attention_mask\n        :param time_stamp: The time stamp when we receive the request. We use the time stamp as a index to\n                            identify the request.\n        \"\"\"\n        self.input_ = input_\n        self.time_ = time_stamp\n        if seq_len is None:\n            if MEATCONFIG['model_type'] in ['gpt', 'bert']:\n                self.seq_len = input_['input_ids'].shape[1]\n            elif MEATCONFIG['model_type'] == 'vit':\n                self.seq_len = input_.shape[-1]\n        else:\n            self.seq_len = seq_len\n        self.text = input_str\n\n\nclass Manager:\n    \"\"\"\n    Base class of batch manager.\n    \"\"\"\n\n    def __init__(self):\n        pass\n\n    def insert_req(self, time_stamp: float, input_ids, input_str):\n        pass\n\n\nclass Dynamic_Batch_Manager(Manager):\n    \"\"\"\n    This batch manager is mainly used for maintaining a queue of request to be processed. The requests in the\n    queue is wrapped into batches and then sent into the inference engine.\n    \"\"\"\n\n    def __init__(self,\n                 forward_func,\n                 result_process,\n                 load_history=False,\n                 his_len: int = 300, **kwargs):\n        super().__init__()\n        print(MEATCONFIG.config)\n        self.max_batch_size = MEATCONFIG['max_batch_size']\n        self.max_sequence_length = MEATCONFIG['max_sequence_length']\n        self.forward_func = forward_func\n        self.publisher = redis.StrictRedis('localhost', 6379, charset=\"utf-8\", decode_responses=True)\n        self.result_process = result_process\n        if load_history:\n            self.load_history(his_len)\n        else:\n            self.req_history = deque(maxlen=his_len)\n        self.req_list = []\n        self.req_list_lock = rwlock.RWLockFair()\n        self.write_lock = self.req_list_lock.gen_wlock()\n        self.max_his_length = his_len\n        self.gamma_dist_ = self.init_gamma_dist(self.max_sequence_length)\n        self.cached_cost = self.generate_cached_cost()\n        self.running_flag = True\n        self.max_workers = MEATCONFIG['pp_init_size'] + 2\n        self.pool = ThreadPoolExecutor(max_workers=self.max_workers)\n        self.working_workers = 0\n        self.main_thread = threading.Thread(target=self.processing_batch)\n        self.main_thread.start()\n\n    def init_gamma_dist(self, max_seq_len):\n        if len(self.req_history) == 0:\n            return gamma_dist(alpha_=0.022, loc_=11, beta_=3.34,)\n        else:\n            fit_alpha, fit_loc, fit_beta = stats.gamma.fit(self.req_history)\n            return gamma_dist(alpha_=fit_alpha, loc_=fit_loc, beta_=fit_beta)\n\n    def generate_cached_cost(self):\n        \"\"\"\n        Test the running time for different sequence length and batch size on the current machine.\n        \"\"\"\n        logging.log(0, \"fetching cached cost\")\n        cached_name = \"cached_cost_{}_pp{}_tp{}_{}_{}_{}_{}.npy\"\\\n            .format(MEATCONFIG['model_class'].__name__, MEATCONFIG['pp_init_size'], MEATCONFIG['tp_init_size'],\n                    MEATCONFIG['max_sequence_length'], MEATCONFIG['max_batch_size'],\n                    MEATCONFIG['step'], MEATCONFIG['repeat_round'])\n        if os.path.exists(cached_name):\n            logging.log(0, \"loading cached cost from file\")\n            cached_cost = np.load(cached_name).tolist()\n        else:\n            logging.log(0, \"generating new cached cost\")\n            cached_cost = [[0 for i in range(MEATCONFIG['max_batch_size'] + 1)] for j in range(MEATCONFIG['max_sequence_length'] + 1)]\n            for tt in range(5):\n                output_ = self.forward_func(seq_len=MEATCONFIG['max_sequence_length'] - 1, batch_size=MEATCONFIG['max_batch_size'] - 1)\n                pred = output_.to_here()\n                for bs in range(MEATCONFIG['max_batch_size'] - 1):\n                    res = self.result_process(int(pred[bs]))\n            input_text = \"\"\n            for tmp_len in trange(1, MEATCONFIG['max_sequence_length'] + 1, MEATCONFIG['step']):\n                input_text += \"test \"\n                for tmp_batch in range(1, MEATCONFIG['max_batch_size'] + 1):\n                    self.forward_func(seq_len=tmp_len, batch_size=tmp_batch)\n                    start_time = time.time()\n                    for k in range(MEATCONFIG['repeat_round']):\n                        output_ = self.forward_func(seq_len=tmp_len, batch_size=tmp_batch)\n                        pred = output_.to_here()\n                        for bs in range(tmp_batch):\n                            res = self.result_process(int(pred[bs]))\n                    time_cost = (time.time() - start_time) / MEATCONFIG['repeat_round']\n                    cached_cost[tmp_len][tmp_batch] = time_cost\n                    for k in range(1, MEATCONFIG['step']):\n                        cached_cost[tmp_len + k][tmp_batch] = time_cost\n            np.save(cached_name, np.array(cached_cost))\n        logging.log(0, \"cached cost loaded\")\n        return cached_cost\n\n    def load_history(self, his_len):\n        \"\"\"\n        load the distribution history\n        \"\"\"\n        try:\n            f = open(\"req_history.txt\", 'r')\n        except Exception as e:\n            print(\"history file does not exist\", e)\n            return\n        his = f.readlines()\n        his = [int(i.replace('\\n', '')) for i in his]\n        self.req_history = his[:his_len]\n        return\n\n    def subscribe_result(self, time_stamp):\n        \"\"\"\n        waiting for the result and send back.\n        \"\"\"\n        sub = self.publisher.pubsub()\n        sub.subscribe(str(time_stamp))\n        predictions = ''\n        for message in sub.listen():\n            if message is not None and isinstance(message, dict):\n                predictions = message.get('data')\n                if not isinstance(predictions, int):\n                    break\n        return predictions\n\n    def insert_req(self, time_stamp: float, input_ids, input_str: str):\n        \"\"\"\n        Build a single_request class with the input string and then insert it into the queue.\n        \"\"\"\n        tmp_req = single_request(input_ids, time_stamp, input_str)\n        self.write_lock.acquire()\n        self.req_list.append(tmp_req)\n        self.req_history.append(tmp_req.seq_len)\n        self.write_lock.release()\n\n    def wrap_batch(self):\n        \"\"\"\n        Given a sorted sequence list, calculate the best way to wrap the batch with DP according to the\n        cached cost.\n        Part of this function comes from the paper of Turbo Transformer.\n        \"\"\"\n        self.write_lock.acquire()\n        new_req_list = self.gamma_dist_.complete_req_list(self.req_list)\n        self.req_list = []\n        self.write_lock.release()\n        states = [0]\n        start_idx_list = [0]\n        for i in range(1, len(new_req_list) + 1):\n            j = i - 1\n            start_idx = i - 1\n            cur_length = new_req_list[i - 1].seq_len\n            min_cost = self.cached_cost[cur_length][1] + states[j]\n            while j > max(0, i - self.max_batch_size):\n                tmp_cost = states[j - 1] + \\\n                           self.cached_cost[cur_length][i - j + 1]\n                if tmp_cost < min_cost:\n                    min_cost = tmp_cost\n                    start_idx = j - 1\n                j -= 1\n            states.append(min_cost)\n            start_idx_list.append(start_idx)\n        i = len(new_req_list)\n        res_start = -1\n        res_end = -1\n        max_priority = 0\n        cur_timestamp = time.time()\n        while i > 0:\n            end_idx = i\n            start_idx = start_idx_list[i]\n            current_batch = new_req_list[start_idx:end_idx]\n            current_priority = self.cal_priority(current_batch, cur_timestamp)\n            if current_priority > max_priority:\n                max_priority = current_priority\n                res_start = start_idx\n                res_end = end_idx\n            i = start_idx\n        temp_batch = new_req_list[res_start:res_end]\n        del new_req_list[res_start:res_end]\n\n        result_batch = []\n        for req in temp_batch:\n            if req.input_:\n                result_batch.append(req)\n        self.write_lock.acquire()\n        for req_ in new_req_list:\n            if req_.input_:\n                self.req_list.append(req_)\n        self.write_lock.release()\n        return result_batch\n\n    def cal_priority(self, batch, cur_time):\n        completeness = np.sum([1 if i.input_ else 0 for i in batch]) / len(batch)\n        earliest_time_stamp = min([j.time_ if j.time_ else cur_time for j in batch])\n        if cur_time - earliest_time_stamp > MEATCONFIG['max_wait_time']:\n            completeness = 1.1\n        return completeness\n\n    def update_distribution(self):\n        fit_alpha, fit_loc, fit_beta = stats.gamma.fit(self.req_history)\n        self.gamma_dist_.alpha = fit_alpha\n        self.gamma_dist_.loc = fit_loc\n        self.gamma_dist_.beta = fit_beta\n        f = open(\"req_history.txt\", 'w')\n        for i in self.req_history:\n            f.write(i)\n        return\n\n    def processing_batch(self):\n        \"\"\"\n        The background process that continuously calls wrap_batch, puts the batch into the inference engine,\n        and starts new processes that wait for and publish the inference result.\n        \"\"\"\n        round_cnt = 0\n        while self.running_flag:\n            if (self.working_workers < self.max_workers) and (len(self.req_list) > 0):\n                round_cnt += 1\n                target_batch = self.wrap_batch()\n                pad_len = target_batch[-1].seq_len\n                logging.info(\"A batch with {} requests and length of {} packed, in-batch length: {}\".format(\n                    len(target_batch), pad_len, [p.seq_len for p in target_batch]))\n                input_text = [i.text for i in target_batch]\n                self.working_workers = self.working_workers + 1\n                output_ = self.forward_func(input_list=input_text)\n                self.pool.submit(self.publish_result, output_, target_batch)\n                if round_cnt == 10 and len(self.req_history) >= self.max_his_length - 1:\n                    round_cnt = 0\n                    self.update_distribution()\n            time.sleep(0.001)\n\n    def publish_result(self, output, target_batch):\n        \"\"\"\n        Background process that waits for the inference result and uses the publisher of Redis to publish it to\n        the waiting requests.\n        :param output: the rpc reference of the inference result.\n        :param target_batch: the input batch\n        \"\"\"\n        predictions = output.to_here()\n        for i in range(len(target_batch)):\n            temp_st = target_batch[i].time_\n            chosen_pred = predictions[i]\n            result = self.result_process(chosen_pred)\n            self.publisher.publish(str(temp_st), result)\n        \n        self.working_workers = self.working_workers - 1\n"}
{"type": "source_file", "path": "energonai/model/attention.py", "content": "import torch\nfrom torch import nn, dtype\n\nfrom colossalai.nn.layer.utils import divide\nfrom colossalai.nn import Linear1D_Col, Linear1D_Row\nfrom colossalai.utils import get_current_device\n\n\nclass MultiHeadAttention1D(nn.Module):\n    def __init__(self,\n                 hidden_size: int,\n                 num_heads: int,\n                 bias: bool = True,\n                 dtype: dtype = torch.float16,\n                 max_seq_len: int = 512,\n                 fused_qkv: bool = True,\n                 is_decoder: bool = True,\n                 disable_past_cache=False\n                 ) -> None:\n        super().__init__()\n\n        self.hidden_size = hidden_size\n        self.attention_head_size = divide(hidden_size, num_heads)\n        self.fused_qkv = fused_qkv\n        self.is_decoder = is_decoder\n        self.disable_past_cache = disable_past_cache\n        self.scaling = self.attention_head_size**-0.5\n        if fused_qkv:\n            self.query_key_value = Linear1D_Col(hidden_size, 3 * hidden_size, bias=bias, dtype=dtype)\n        else:\n            self.query_ = Linear1D_Col(hidden_size, hidden_size, bias=bias, dtype=dtype)\n            self.key_ = Linear1D_Col(hidden_size, hidden_size, bias=bias, dtype=dtype)\n            self.value_ = Linear1D_Col(hidden_size, hidden_size, bias=bias, dtype=dtype)\n\n        self.softmax = nn.Softmax(dim=-1)\n\n        self.dense = Linear1D_Row(hidden_size, hidden_size, bias=True, dtype=dtype, parallel_input=True)\n\n        if is_decoder:\n            self.causal_mask = torch.tril(torch.ones((max_seq_len, max_seq_len), dtype=torch.uint8,\n                                                     device=get_current_device())).view(1, 1, max_seq_len, max_seq_len).bool()\n            self.causal_mask_bias = torch.tensor(-1e4, dtype=dtype, device=get_current_device())\n\n        self.past_cache = {}\n\n    def _split_heads(self, tensor, num_heads, attn_head_size):\n        new_shape = tensor.size()[:-1] + (num_heads, attn_head_size)\n        tensor = tensor.view(new_shape)\n        return tensor.permute(0, 2, 1, 3)\n\n    def last_word(self, hidden_states):\n        batch_size = hidden_states.shape[0]\n        hidden_size = hidden_states.shape[2]\n        return hidden_states[:, -1, :].view(batch_size, 1, hidden_size)\n\n    def forward(self,\n                hidden_states,\n                attention_mask=None,\n                first_cache=False,\n                seq_lens=None):\n        if self.fused_qkv:\n            if self.disable_past_cache:\n                kvq = self.query_key_value(hidden_states)\n            else:\n                if first_cache:\n                    kvq = self.query_key_value(hidden_states)\n                    self.past_cache['query_key_value'] = kvq\n                else:\n                    kvq = self.query_key_value(self.last_word(hidden_states))\n                    self.past_cache['query_key_value'] = torch.cat((self.past_cache['query_key_value'], kvq), 1)\n                    kvq = self.past_cache['query_key_value']\n            all_head_size = kvq.shape[-1] // 3\n            num_attention_heads = divide(all_head_size, self.attention_head_size)\n            # kvq = self._split_heads(kvq, num_attention_heads, 3 * self.attention_head_size)\n            k, v, q = [t.contiguous() for t in torch.chunk(kvq, 3, dim=-1)]\n        else:\n            if self.disable_past_cache:\n                q = self.query_(hidden_states)\n                k = self.key_(hidden_states)\n                v = self.value_(hidden_states)\n            else:\n                if first_cache:\n                    q = self.query_(hidden_states)\n                    k = self.key_(hidden_states)\n                    v = self.value_(hidden_states)\n                    self.past_cache['q'] = q\n                    self.past_cache['k'] = k\n                    self.past_cache['v'] = v\n                else:\n                    q = self.query_(self.last_word(hidden_states))\n                    k = self.key_(self.last_word(hidden_states))\n                    v = self.value_(self.last_word(hidden_states))\n                    self.past_cache['q'] = torch.cat((self.past_cache['q'], q), 1)\n                    self.past_cache['k'] = torch.cat((self.past_cache['k'], k), 1)\n                    self.past_cache['v'] = torch.cat((self.past_cache['v'], v), 1)\n                    q = self.past_cache['q']\n                    k = self.past_cache['k']\n                    v = self.past_cache['v']\n            all_head_size = q.shape[-1]\n            num_attention_heads = divide(all_head_size, self.attention_head_size)\n        q = self._split_heads(q, num_attention_heads, self.attention_head_size)\n        k = self._split_heads(k, num_attention_heads, self.attention_head_size)\n        v = self._split_heads(v, num_attention_heads, self.attention_head_size)\n\n        q *= self.scaling\n        hidden_states = torch.matmul(q, k.transpose(-1, -2))\n\n        q_len, k_len = q.size(-2), k.size(-2)\n\n        if self.is_decoder:\n            hidden_states = torch.where(self.causal_mask[:, :, 0:q_len, 0:k_len], hidden_states, self.causal_mask_bias)\n\n        if attention_mask is not None:\n            hidden_states = hidden_states + attention_mask\n        dtype = hidden_states.dtype\n        hidden_states = torch.softmax(hidden_states, -1, dtype=torch.float).to(dtype)\n\n        hidden_states = torch.matmul(hidden_states, v)\n\n        hidden_states = hidden_states.transpose(1, 2)\n\n        new_context_layer_shape = hidden_states.size()[:-2] + (all_head_size,)\n\n        hidden_states = hidden_states.reshape(new_context_layer_shape)\n\n        if self.disable_past_cache:\n            hidden_states = self.dense(hidden_states)\n        else:\n            if first_cache:\n                hidden_states = self.dense(hidden_states)\n                self.past_cache['dense'] = hidden_states\n            else:\n                hidden_states = self.dense(self.last_word(hidden_states))\n                self.past_cache['dense'] = torch.cat((self.past_cache['dense'], hidden_states), 1)\n                hidden_states = self.past_cache['dense']\n        return hidden_states\n"}
{"type": "source_file", "path": "energonai/model/embedding.py", "content": "import torch\nfrom torch import nn as nn\nfrom torch import dtype\nfrom colossalai.nn import VocabParallelEmbedding1D\nfrom torch.nn import Embedding\nfrom colossalai.utils import get_current_device\n\n\nclass Embedding1D(nn.Module):\n    def __init__(self,\n                 hidden_size: int,\n                 vocab_size: int,\n                 max_seq_len: int,\n                 num_tokentypes: int = 0,\n                 padding_idx: int = 0,\n                 dtype: dtype = None,\n                 vocab_parallel: bool = False,\n                 ) -> None:\n        super().__init__()\n        if vocab_parallel:\n            self.word_embeddings = VocabParallelEmbedding1D(\n                vocab_size, hidden_size, padding_idx=padding_idx, dtype=dtype)\n        else:\n            self.word_embeddings = Embedding(vocab_size, hidden_size, padding_idx=padding_idx).to(\n                dtype=dtype, device=get_current_device())\n\n        self.position_embeddings = Embedding(max_seq_len, hidden_size).to(dtype=dtype, device=get_current_device())\n\n        if num_tokentypes > 0:\n            self.tokentype_embeddings = Embedding(num_tokentypes, hidden_size).to(\n                dtype=dtype, device=get_current_device())\n        else:\n            self.tokentype_embeddings = None\n\n        # self.position_ids = torch.arange(max_seq_len, dtype=torch.long, device=get_current_device()).expand((1, -1))\n\n    @property\n    def word_embedding_weight(self):\n        return self.word_embeddings.weight\n\n    def forward(self,\n                input_ids,\n                position_ids=None,\n                tokentype_ids=None,\n                past_key_values_length: int = 0):\n\n        seq_length = input_ids.size(1)\n        if position_ids is None:\n            position_ids = torch.arange(seq_length, dtype=torch.long, device=get_current_device()).unsqueeze(0)\n            # position_ids = self.position_ids[:, past_key_values_length : seq_length + past_key_values_length]\n\n        x = self.word_embeddings(input_ids) + self.position_embeddings(position_ids)\n\n        if self.tokentype_embeddings is not None and tokentype_ids is not None:\n            x = x + self.tokentype_embeddings(tokentype_ids)\n\n        return x\n"}
{"type": "source_file", "path": "energonai/pipelinable/__init__.py", "content": "from .split_method import split_transformer_into_partitions\n"}
{"type": "source_file", "path": "examples/bert/bert_config.py", "content": "from bert import bert_small, bert_large, bert_xl, bert_8B, bert_175B\nfrom bert_server import launch_engine\n\nmodel_class = bert_8B\nmodel_type = \"bert\"\nengine_server = launch_engine\ntp_init_size = 2\npp_init_size = 2\nhost = \"127.0.0.1\"\nport = 29400\nhalf = False\nserver_host = \"127.0.0.1\"\nserver_port = 8010\nlog_level = \"info\"\nbackend = \"nccl\""}
{"type": "source_file", "path": "energonai/worker.py", "content": "import time\nfrom contextlib import contextmanager\nfrom typing import Any, Callable\n\nimport colossalai\nimport torch\nimport torch.distributed.rpc as trpc\nimport torch.multiprocessing as mp\nimport torch.nn as nn\nfrom colossalai.context import ParallelMode\nfrom colossalai.core import global_context as gpc\nfrom colossalai.logging import disable_existing_loggers, get_dist_logger\n\nfrom .pipe import Pipe\nfrom .task import TaskEntry\nfrom .utils import Terminator, build_device_maps\n\n\nclass Worker:\n    def __init__(self, rank: int, tp_world_size: int, pp_world_size: int, master_host: str, master_port: int, rpc_port: int, n_proc_per_node: int,\n                 model_fn: Callable[[Any], nn.Module], pipe_size: int = 1, rpc_disable_shm: bool = True, **model_kwargs: Any) -> None:\n        self.global_rank = rank\n        self.world_size = tp_world_size * pp_world_size\n        self.tp_world_size = tp_world_size\n        self.pp_world_size = pp_world_size\n        disable_existing_loggers(exclude=['energonai', 'colossalai'])\n        colossalai.launch({'parallel': {'tensor': {'mode': '1d', 'size': tp_world_size},\n                          'pipeline': pp_world_size}}, rank, self.world_size, master_host, master_port)\n        self.tp_rank = gpc.get_local_rank(ParallelMode.PARALLEL_1D)\n        self.pp_rank = gpc.get_local_rank(ParallelMode.PIPELINE) if gpc.is_initialized(ParallelMode.PIPELINE) else 0\n\n        self.model: nn.Module = model_fn(**model_kwargs).cuda()\n\n        self.rpc_name = f'worker{self.global_rank}'\n        rpc_options = {}\n        if rpc_disable_shm:\n            # SHM may lead to timeout error. Disabling SHM and only enabling uv transport can solve this problem.\n            # See https://discuss.pytorch.org/t/rpc-behavior-difference-between-pytorch-1-7-0-vs-1-9-0/124772/5\n            # This is a workaround and may be solved in the future.\n            rpc_options['_transports'] = ['uv']\n        trpc.init_rpc(self.rpc_name, rank=self.global_rank + 1, world_size=self.world_size + 1,\n                      rpc_backend_options=trpc.TensorPipeRpcBackendOptions(\n                          init_method=f'tcp://{master_host}:{rpc_port}',\n                          device_maps=build_device_maps(self.world_size, n_proc_per_node, rank=self.global_rank),\n                          **rpc_options\n                      ))\n        self.to_master_pipe = Pipe(f'{self.global_rank}_to_m', self.rpc_name, 'master')\n        self.to_master_pipe.send(self.pp_rank)\n\n        if self.pp_rank == 0:\n            self.input_pipe = Pipe(f'm_to_{self.global_rank}', 'master', self.rpc_name, max_size=pipe_size)\n        else:\n            pp_prev_rank = gpc.get_prev_global_rank(ParallelMode.PIPELINE)\n            self.input_pipe = Pipe(f'{pp_prev_rank}_to_{self.global_rank}',\n                                   f'worker{pp_prev_rank}', self.rpc_name, max_size=pipe_size)\n        if self.pp_rank == self.pp_world_size - 1:\n            self.output_pipe = self.to_master_pipe\n        else:\n            pp_next_rank = gpc.get_next_global_rank(ParallelMode.PIPELINE)\n            self.output_pipe = Pipe(f'{self.global_rank}_to_{pp_next_rank}', self.rpc_name,\n                                    f'worker{pp_next_rank}', max_size=pipe_size)\n\n        self.logger = get_dist_logger('energonai')\n        self.logger.info(f'{self.rpc_name} start')\n        self._start()\n\n    @contextmanager\n    def _lifespan(self):\n        try:\n            yield\n        finally:\n            self._shutdown()\n\n    def _start(self) -> None:\n        with self._lifespan():\n            while True:\n                try:\n                    task_entry: TaskEntry = self.input_pipe.recv_nowait()\n                    with torch.inference_mode():\n                        outputs = self._forward(task_entry.batch)\n                    self.output_pipe.send(TaskEntry(task_entry.uids, outputs))\n                except RuntimeError:\n                    time.sleep(0.01)\n\n    def _shutdown(self) -> None:\n        Terminator.shield()\n        trpc.rpc_sync('master', Terminator.terminate)\n        trpc.shutdown()\n\n    def _forward(self, inputs: Any) -> Any:\n        if isinstance(inputs, (tuple, list)):\n            outputs = self.model(*inputs)\n        elif isinstance(inputs, dict):\n            outputs = self.model(**inputs)\n        else:\n            outputs = self.model(inputs)\n        return outputs\n\n\ndef launch_workers(tp_world_size: int, pp_world_size: int, master_host: str, master_port: int, rpc_port: int,\n                   model_fn: Callable[[Any], nn.Module], n_proc_per_node: int = 1, node_rank: int = 0, pipe_size: int = 1, rpc_disable_shm: bool = True,\n                   **model_kwargs: Any) -> None:\n    ctx = mp.get_context('spawn')\n    procs = []\n    for i in range(n_proc_per_node):\n        rank = n_proc_per_node * node_rank + i\n        p = ctx.Process(target=Worker, args=(rank, tp_world_size, pp_world_size,\n                                             master_host, master_port, rpc_port, n_proc_per_node, model_fn, pipe_size, rpc_disable_shm), kwargs=model_kwargs)\n        procs.append(p)\n        p.start()\n"}
{"type": "source_file", "path": "energonai/pipelinable/split_method.py", "content": "from torch.fx.passes.split_module import split_module\nfrom .split_policy import module_equal_partition, naive_equal_partition, transformer_partition\nfrom .energon_tracer import EnergonTracer\nimport torch.fx\n\ndef filter_graph(traced: torch.fx.GraphModule, filter_type: str):\n    len = 0\n    for node in traced.graph.nodes:\n        if node.op == filter_type:\n            len = len + 1\n    return len\n    \n\ndef split_transformer_into_partitions(model_class):\n    model = model_class()\n    graph = EnergonTracer().trace(model)\n    traced = torch.fx.GraphModule(model, graph)\n    depth = filter_graph(traced, \"call_module\") - 1\n    submodules = split_module(traced, model, transformer_partition(depth))\n    del model\n\n    return submodules"}
{"type": "source_file", "path": "energonai/utils/common.py", "content": "import torch\nimport functools\nimport signal\nfrom typing import Optional, Dict, Union, Callable, Any\nfrom threading import Lock\nfrom contextlib import contextmanager\n\nDeviceType = Union[int, str, torch.device]\n\n\ndef build_device_maps(world_size: int, n_proc_per_node: int, rank: Optional[int] = None) -> Dict[str, Dict[DeviceType, DeviceType]]:\n    is_master = rank is None\n    device_maps: Dict[str, Dict[DeviceType, DeviceType]] = {}\n    if is_master:\n        for i in range(world_size):\n            worker_local_rank = i % n_proc_per_node\n            device_maps[f'worker{i}'] = {'cpu': worker_local_rank}\n    else:\n        local_rank = rank % n_proc_per_node\n        for i in range(world_size):\n            if i != rank:\n                worker_local_rank = i % n_proc_per_node\n                device_maps[f'worker{i}'] = {local_rank: worker_local_rank}\n        device_maps['master'] = {local_rank: 'cpu'}\n    return device_maps\n\n\n@contextmanager\ndef use_lock(lock: Lock):\n    try:\n        lock.acquire()\n        yield\n    finally:\n        lock.release()\n\n\ndef run_once(func: Callable[[Any], Any]) -> Callable[[Any], Any]:\n    called: bool = False\n\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        nonlocal called\n        if not called:\n            func(*args, **kwargs)\n            called = True\n    return wrapper\n\n\nclass Terminator:\n    lock = Lock()\n    called: bool = False\n\n    @classmethod\n    def shield(cls):\n        with use_lock(cls.lock):\n            cls.called = True\n\n    @classmethod\n    def terminate(cls):\n        with use_lock(cls.lock):\n            if not cls.called:\n                cls.called = True\n                signal.raise_signal(signal.SIGINT)\n"}
{"type": "source_file", "path": "examples/auto_pipeline/bert_server.py", "content": "import os\nimport torch\nimport uvicorn\nfrom fastapi import FastAPI\nfrom energonai.engine import InferenceEngine\nfrom energonai.context import MEATCONFIG\n\napp = FastAPI() #  api \n\n@app.get(\"/\") # \ndef root():\n    return {\"200\"}\n\n@app.get(\"/model_with_padding\")\ndef run():\n    # for the performance only\n    seq_len = 512\n    batch_size = 32\n\n    input_ids = torch.randint(1, 10, (batch_size, seq_len), dtype=torch.int64)\n    attention_mask = torch.randint(0, 1, (batch_size, 1, seq_len), dtype=torch.int64)\n    # seq_lens = torch.randint(1, 128, (batch_size, ), dtype=torch.int64) # generate seq_lens randomly\n    hidden_states = None\n    sample = dict(hidden_states=hidden_states, input_ids=input_ids, attention_mask=attention_mask)\n\n    output = engine.run(sample)\n    output = output.to_here()\n    print(output)\n    return {\"To return the string result.\"}\n\n# @app.get(\"/model_rm_padding\")\n# def run():\n#     # for the performance only\n#     seq_len = 512\n#     batch_size = 32\n\n#     input_ids = torch.randint(1, 10, (batch_size, seq_len), dtype=torch.int64)\n#     attention_mask = torch.randint(0, 1, (batch_size, 1, seq_len), dtype=torch.int64)\n#     seq_lens = torch.randint(1, 128, (batch_size, ), dtype=torch.int) # generate seq_lens randomly\n#     hidden_states = None\n#     sample = dict(hidden_states=hidden_states, input_ids=input_ids, attention_mask=attention_mask, seq_lens=seq_lens)\n\n#     output = engine.run(sample)\n#     output = output.to_here()\n#     print(output)\n#     return {\"To return the string result.\"}\n    \n\n@app.get(\"/shutdown\")\nasync def shutdown():\n    engine.clear()\n    server.should_exit = True\n    server.force_exit = True\n    await server.shutdown()\n\n\ndef launch_engine(model_class,\n                model_type,\n                max_batch_size: int = 1,\n                tp_init_size: int = -1,\n                pp_init_size: int = -1,\n                host: str = \"localhost\",\n                port: int = 29500,\n                dtype = torch.float,\n                checkpoint: str = None,\n                tokenizer_path: str = None,\n                server_host = \"localhost\",\n                server_port = 8005,\n                log_level = \"info\"\n                ):\n    \n    if checkpoint:\n        model_config = {'dtype': dtype, 'checkpoint': True, 'checkpoint_path': checkpoint}\n    else:\n        model_config = {'dtype': dtype}\n\n    global engine\n    engine = InferenceEngine(model_class, \n                            model_config,\n                            model_type,\n                            max_batch_size = max_batch_size, \n                            tp_init_size = tp_init_size, \n                            pp_init_size = pp_init_size, \n                            auto_pp = MEATCONFIG['auto_pp'],\n                            host = host,\n                            port = port,\n                            dtype = dtype)\n\n    global server\n    config = uvicorn.Config(app, host=server_host, port=server_port, log_level=log_level)\n    server = uvicorn.Server(config=config)\n    server.run()"}
{"type": "source_file", "path": "energonai/utils/timer.py", "content": "import os\nimport sys\nimport time\n\nimport torch\n\n_GLOBAL_TIMERS = None\n\n\nclass _Timer:\n    \"\"\"Timer.\"\"\"\n\n    def __init__(self, name, ignore_first):\n        self.name_ = name\n        self.elapsed_ = 0.0\n        self.started_ = False\n        self.start_time = time.time()\n        if ignore_first:\n            self.times = 2\n        else:\n            self.times = 0\n\n    def start(self):\n        \"\"\"Start the timer.\"\"\"\n        if(self.times != 0):\n            self.times = self.times - 1\n        else: \n            assert not self.started_, 'timer has already been started'\n            torch.cuda.synchronize()\n            self.start_time = time.time()\n            self.started_ = True\n\n    def stop(self):\n        \"\"\"Stop the timer.\"\"\"\n        if(self.times != 0):\n            self.times = self.times - 1\n        else: \n            assert self.started_, 'timer is not started'\n            torch.cuda.synchronize()\n            self.elapsed_ += (time.time() - self.start_time)\n            self.started_ = False\n\n    def reset(self):\n        \"\"\"Reset timer.\"\"\"\n        self.elapsed_ = 0.0\n        self.started_ = False\n\n    def elapsed(self, reset=True):\n        \"\"\"Calculate the elapsed time.\"\"\"\n        started_ = self.started_\n        # If the timing in progress, end it first.\n        if self.started_:\n            self.stop()\n        # Get the elapsed time.\n        elapsed_ = self.elapsed_\n        # Reset the elapsed time\n        if reset:\n            self.reset()\n        # If timing was in progress, set it back.\n        if started_:\n            self.start()\n        return elapsed_\n\n\nclass Timers:\n    \"\"\"Group of timers.\"\"\"\n\n    def __init__(self, ignore_first):\n        self.timers = {}\n        self.ignore_first = ignore_first\n\n    def __call__(self, name):        \n        if name not in self.timers:\n            self.timers[name] = _Timer(name, self.ignore_first)\n        return self.timers[name]\n\n    def write(self, names, writer, iteration, normalizer=1.0, reset=False):\n        \"\"\"Write timers to a tensorboard writer\"\"\"\n        # currently when using add_scalars,\n        # torch.utils.add_scalars makes each timer its own run, which\n        # polutes the runs list, so we just add each as a scalar\n        assert normalizer > 0.0\n        for name in names:\n            value = self.timers[name].elapsed(reset=reset) / normalizer\n            writer.add_scalar(name + '-time', value, iteration)\n\n    def log(self, names, normalizer=1.0, reset=True):\n        \"\"\"Log a group of timers.\"\"\"\n        assert normalizer > 0.0\n        string0 = ''\n        string1 = ''\n        for name in names:\n            elapsed_time = self.timers[name].elapsed(reset=reset) * 1000.0 / normalizer\n            string0 += ' : {}'.format(name)\n            string1 += ' : {:.2f}'.format(elapsed_time)\n\n        if torch.distributed.is_initialized():\n            if torch.distributed.get_rank() == (torch.distributed.get_world_size() - 1):\n                print(f'{string0} \\n {string1}', flush=True)\n        else:\n            print(f'{string0} \\n {string1}', flush=True)\n\n\ndef _ensure_var_is_not_initialized(var, name):\n    \"\"\"Make sure the input variable is not None.\"\"\"\n    assert var is None, '{} is already initialized.'.format(name)\n\n\ndef _set_timers(ignore_first):\n    \"\"\"Initialize timers.\"\"\"\n    global _GLOBAL_TIMERS\n    _ensure_var_is_not_initialized(_GLOBAL_TIMERS, 'timers')\n    _GLOBAL_TIMERS = Timers(ignore_first)\n\n\ndef _ensure_var_is_initialized(var, name):\n    \"\"\"Make sure the input variable is not None.\"\"\"\n    assert var is not None, '{} is not initialized.'.format(name)\n\n\ndef get_timers(ignore_first = False):\n    \"\"\"Return timers.\"\"\"\n    if _GLOBAL_TIMERS is None:\n        _set_timers(ignore_first)\n    _ensure_var_is_initialized(_GLOBAL_TIMERS, 'timers')\n    return _GLOBAL_TIMERS\n"}
{"type": "source_file", "path": "examples/bert/bert.py", "content": "import math\nfrom typing import Callable\n\nimport os\nimport torch\nfrom torch import nn as nn, Tensor, dtype\n\nfrom colossalai.context import ParallelMode\nfrom colossalai.core import global_context as gpc\nfrom colossalai.logging import get_dist_logger\nfrom colossalai.nn.layer.utils import divide, ACT2FN\nfrom colossalai.nn import Linear1D_Col, Linear1D_Row, LayerNorm1D, VocabParallelEmbedding1D\nfrom energonai.kernel import transpose_pad, transpose_depad, depad\nfrom colossalai.utils import get_current_device, is_using_pp\n\n__all__ = [\n    'BertEmbedding1D'\n    'BertMLP1D',\n    'BertSelfAttention1D',\n    'BertTransformerLayer1D'\n]\n\nfrom energonai.utils.checkpointing import load_checkpoint\n\n\nclass BertEmbedding1D(nn.Module):\n    def __init__(self,\n                 embedding_dim: int,  # hidden_size\n                 vocab_size: int,\n                 max_position_embeddings: int,\n                 num_tokentypes: int = 0,\n                 padding_idx: int = 0,\n                 layernorm_epsilon: float = 1e-5,\n                 dtype: dtype = None) -> None:\n        super().__init__()\n        self.word_embeddings = VocabParallelEmbedding1D(vocab_size, embedding_dim, padding_idx=padding_idx, dtype=dtype)\n        self.position_embeddings = VocabParallelEmbedding1D(max_position_embeddings, embedding_dim, dtype=dtype)\n        if num_tokentypes > 0:\n            self.tokentype_embeddings = VocabParallelEmbedding1D(num_tokentypes, embedding_dim, dtype=dtype)\n        else:\n            self.tokentype_embeddings = None\n\n        self.LayerNorm = LayerNorm1D(embedding_dim, eps=layernorm_epsilon, dtype=dtype)\n\n    def forward(self, input_ids, position_ids=None, tokentype_ids=None, seq_lens=None, batch_size=None,\n                max_padding_size=None):\n        max_padding_size = input_ids.shape[1]\n\n        # TODO: register_buffer in advance for position_ids to speedup\n\n        if position_ids is None:\n            position_ids = torch.arange(max_padding_size, dtype=torch.long, device=get_current_device()).unsqueeze(0)\n\n        x = self.word_embeddings(input_ids) + self.position_embeddings(position_ids)\n\n        if self.tokentype_embeddings is not None and tokentype_ids is not None:\n            x = x + self.tokentype_embeddings(tokentype_ids)\n\n        x = self.LayerNorm(x)\n\n        if seq_lens is not None:\n            x = depad(x, batch_size, seq_lens)\n\n        return x\n\n\nclass BertSelfAttention1D(nn.Module):\n    def __init__(self,\n                 hidden_size: int,\n                 num_heads: int,\n                 bias: bool = True,\n                 fuse_scale_mask_softmax: bool = False,\n                 layernorm_epsilon: float = 1e-5,\n                 dtype: dtype = None) -> None:\n        super().__init__()\n        if hidden_size % num_heads != 0:\n            raise ValueError(\n                f\"The hidden size ({hidden_size}) is not a multiple of the number of attention \")\n        self.hidden_size = hidden_size\n        self.attention_head_size = divide(hidden_size, num_heads)\n        self.fuse_scale_mask_softmax = fuse_scale_mask_softmax\n\n        self.query_key_value = Linear1D_Col(hidden_size, 3 * hidden_size, bias=bias, dtype=dtype)\n\n        if fuse_scale_mask_softmax:\n            raise NotImplementedError\n\n        self.dense = Linear1D_Row(hidden_size, hidden_size, bias=True, dtype=dtype, parallel_input=True)\n        self.LayerNorm = LayerNorm1D(hidden_size, eps=layernorm_epsilon, dtype=dtype)\n\n    def forward(self, hidden_states, attention_mask=None, seq_lens=None, batch_size=None, max_padding_size=None):\n\n        attention_output = self.query_key_value(hidden_states)\n        all_head_size = attention_output.shape[-1] // 3\n        num_attention_heads = divide(all_head_size, self.attention_head_size)  # num_heads\n\n        new_qkv_shape = attention_output.shape[:-1] + (num_attention_heads, 3 * self.attention_head_size)\n        attention_output = attention_output.view(new_qkv_shape)\n\n        if seq_lens is not None:\n            # TODO: use FasterTransformer's implementation.\n            attention_output = transpose_pad(attention_output, batch_size, max_padding_size, seq_lens,\n                                             num_attention_heads, self.attention_head_size * 3)\n        else:\n            attention_output = attention_output.permute(0, 2, 1, 3)\n        # TODO: make sure self.attention_head_size*3 is correct\n\n        q, k, v = torch.chunk(attention_output, 3, dim=-1)\n\n        attention_output = torch.matmul(q, k.transpose(-1, -2))\n        if self.fuse_scale_mask_softmax:\n            raise NotImplementedError\n        else:\n            attention_output = attention_output / math.sqrt(self.attention_head_size)\n            # if attention_mask is not None:\n            #     attention_output = attention_output + attention_mask\n            attention_output = nn.functional.softmax(attention_output, dim=-1)\n\n        attention_output = torch.matmul(attention_output, v)\n\n        if seq_lens is not None:\n            sum_seq = torch.sum(seq_lens)\n            attention_output = transpose_depad(attention_output, batch_size, sum_seq, max_padding_size, seq_lens,\n                                               num_attention_heads, self.attention_head_size)\n        else:\n            attention_output = attention_output.permute(0, 2, 1, 3).contiguous()\n\n        new_context_layer_shape = attention_output.size()[:-2] + (all_head_size,)\n        attention_output = attention_output.reshape(new_context_layer_shape)\n        attention_output = self.dense(attention_output)\n\n        hidden_states = self.LayerNorm(attention_output + hidden_states)\n\n        return hidden_states\n\n\ndef gelu_impl(x):\n    \"\"\"OpenAI's gelu implementation.\"\"\"\n    return 0.5 * x * (1.0 + torch.tanh(0.7978845608028654 * x *\n                                       (1.0 + 0.044715 * x * x)))\n\n\nclass BertMLP1D(nn.Module):\n    def __init__(self,\n                 hidden_size: int,\n                 mlp_ratio: float,\n                 activation: Callable = gelu_impl,\n                 layernorm_epsilon: float = 1e-5,\n                 dtype: dtype = None,\n                 bias: bool = True):\n        super().__init__()\n        intermediate_dim = int(hidden_size * mlp_ratio)\n        self.layer_0 = Linear1D_Col(hidden_size, intermediate_dim, bias=bias, dtype=dtype, gather_output=False)\n        self.activation = activation\n        self.layer_1 = Linear1D_Row(intermediate_dim, hidden_size, bias=bias, dtype=dtype, parallel_input=True)\n        self.LayerNorm = LayerNorm1D(hidden_size, eps=layernorm_epsilon, dtype=dtype)\n\n    def forward(self, input_tensor):\n        hidden_states = self.layer_0(input_tensor)\n        hidden_states = self.activation(hidden_states)\n        hidden_states = self.layer_1(hidden_states)\n\n        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n        return hidden_states\n\n\nclass BertTransformerLayer1D(nn.Module):\n    def __init__(self,\n                 hidden_size: int,\n                 num_heads: int,\n                 mlp_ratio: float,\n                 activation: Callable = gelu_impl,\n                 layernorm_epsilon: float = 1e-5,\n                 dtype: dtype = None,\n                 bias: bool = True,\n                 fuse_scale_mask_softmax: bool = False):\n        super().__init__()\n\n        self.attention = BertSelfAttention1D(hidden_size,\n                                             num_heads,\n                                             bias,\n                                             fuse_scale_mask_softmax,\n                                             layernorm_epsilon,\n                                             dtype)\n        self.mlp = BertMLP1D(hidden_size,\n                             mlp_ratio,\n                             activation,\n                             layernorm_epsilon,\n                             dtype,\n                             bias)\n\n    def forward(self, hidden_states, attention_mask, seq_lens=None, batch_size=None, max_padding_size=None):\n        hidden_states = self.attention(hidden_states, attention_mask, seq_lens, batch_size, max_padding_size)\n        hidden_states = self.mlp(hidden_states)\n\n        return hidden_states\n\n\nclass PipelineBert1D(nn.Module):\n\n    def __init__(self,\n                 vocab_size: int = 50304,\n                 max_position_embeddings: int = 1024,\n                 hidden_size: int = 768,\n                 num_heads: int = 12,\n                 depth: int = 12,\n                 mlp_ratio: float = 4.0,\n                 layernorm_epsilon: float = 1e-5,\n                 activation: Callable = nn.functional.gelu,\n                 padding_idx: int = 0,\n                 dtype: dtype = None,\n                 bias: bool = True,\n                 fuse_scale_mask_softmax: bool = False,\n                 first: bool = False,\n                 last: bool = False):\n        super().__init__()\n        self.first = first\n        self.last = last\n\n        if first:\n            self.embed = BertEmbedding1D(embedding_dim=hidden_size,\n                                         vocab_size=vocab_size,\n                                         max_position_embeddings=max_position_embeddings,\n                                         padding_idx=padding_idx,\n                                         layernorm_epsilon=layernorm_epsilon,\n                                         dtype=dtype)\n        self.blocks = nn.ModuleList()\n        self.pp_rank = gpc.get_local_rank(ParallelMode.PIPELINE) if is_using_pp() else 0\n        for id_ in range(depth):\n            self.blocks.register_module(\"blk_{}\".format(id_ + self.pp_rank * depth),\n                                        BertTransformerLayer1D(\n                                            hidden_size=hidden_size,\n                                            num_heads=num_heads,\n                                            mlp_ratio=mlp_ratio,\n                                            activation=activation,\n                                            layernorm_epsilon=layernorm_epsilon,\n                                            dtype=dtype,\n                                            bias=bias,\n                                            fuse_scale_mask_softmax=fuse_scale_mask_softmax,\n            )\n            )\n\n    def forward(self, hidden_states=None, input_ids=None, attention_mask=None, seq_lens=None):\n\n        batch_size = input_ids.shape[0]\n        max_padding_size = input_ids.shape[1]\n\n        if self.first:\n            hidden_states = self.embed(input_ids=input_ids, position_ids=None, tokentype_ids=None, seq_lens=seq_lens,\n                                       batch_size=batch_size, max_padding_size=max_padding_size)  # , seq_lens\n\n        if attention_mask is not None:\n            attention_mask = attention_mask.view(batch_size, -1)\n            attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n            attention_mask = attention_mask.to(dtype=hidden_states.dtype)  # fp16 compatibility\n            attention_mask = (1.0 - attention_mask) * -10000.0\n\n        for block in self.blocks:\n            hidden_states = block(hidden_states=hidden_states, attention_mask=attention_mask, seq_lens=seq_lens,\n                                  batch_size=batch_size, max_padding_size=max_padding_size)\n\n        if self.last:\n            hidden_states = hidden_states[:, 1, :]\n\n        return hidden_states\n\n\ndef partition_uniform(num_items, pipeline_parallel_size, num_chunks):\n    assert num_items % num_chunks == 0, \\\n        \"Layer length should be divided by the number of chunks, otherwise parameter method is recomended\"\n\n    logger = get_dist_logger('energonai')\n    parts = [[] for _ in range(pipeline_parallel_size)]  # 4\n    partition_items = num_items // num_chunks  # 96 // 2\n    for idx in range(num_chunks):\n        base_idx = idx * partition_items\n        chunk_size = partition_items // pipeline_parallel_size\n        left = pipeline_parallel_size - partition_items % pipeline_parallel_size\n        if chunk_size == 0:\n            logger.warning(\"Some nodes in Pipeline have no requests\")\n\n        for p in range(pipeline_parallel_size):\n            st = base_idx\n            base_idx += chunk_size + (p >= left)\n            parts[p].append((st, base_idx))\n\n    return parts\n\n\ndef _create_bert_pipeline_model(depth=48, num_chunks=1, layer_partitions=None, **model_kwargs):\n    logger = get_dist_logger('energonai')\n    pipeline_size = 0\n    pipeline_rank = 0\n    if gpc.is_initialized(ParallelMode.PIPELINE):\n        pipeline_size = gpc.get_world_size(ParallelMode.PIPELINE)\n        pipeline_rank = gpc.get_local_rank(ParallelMode.PIPELINE)\n    else:\n        pipeline_size = 1\n        pipeline_rank = 0\n\n    rank = gpc.get_global_rank()\n\n    parts = partition_uniform(depth, pipeline_size,\n                              num_chunks)[pipeline_rank] if layer_partitions is None else layer_partitions\n    models = []\n    for start, end in parts:\n        model_kwargs['first'] = start == 0\n        model_kwargs['last'] = end == depth\n        model_kwargs['depth'] = end - start\n        chunk = PipelineBert1D(**model_kwargs).to(get_current_device())\n        models.append(chunk)\n        logger.info(f'==> Rank {rank} built layer {start}-{end} / total {depth}')\n\n    if len(models) == 1:\n        model = models[0]\n    else:\n        model = nn.ModuleList(models)\n\n    numel = 0\n    for _, param in model.named_parameters(recurse=True):\n        numel += param.numel()\n\n    if \"checkpoint\" in model_kwargs.keys():\n        if model_kwargs[\"checkpoint\"] is True:\n            if gpc.get_global_rank() == 0:\n                assert \"checkpoint_path\" in model_kwargs.keys(), \"You have to specify a file path to use checkpoint loading\"\n                assert os.path.exists(model_kwargs[\"checkpoint_path\"]), \"Checkpoint file not found\"\n            load_checkpoint(model_kwargs[\"checkpoint_path\"], model, **model_kwargs)\n\n    logger.info(f'Rank{rank}/{pipeline_rank} model size in FP16 = {numel * 2 / 1e9} GB')\n    return model\n\n\ndef bert_small(**kwargs):\n    model_kwargs = dict(hidden_size=768, depth=12, num_heads=12, **kwargs)\n    return _create_bert_pipeline_model(**model_kwargs)\n\n\ndef bert_large(**kwargs):\n    model_kwargs = dict(hidden_size=1024, depth=24, num_heads=16, **kwargs)\n    return _create_bert_pipeline_model(**model_kwargs)\n\n\ndef bert_xl(**kwargs):\n    model_kwargs = dict(hidden_size=1600, depth=48, num_heads=16, **kwargs)\n    return _create_bert_pipeline_model(**model_kwargs)\n\n\ndef bert_8B(**kwargs):\n    model_kwargs = dict(hidden_size=3072, depth=72, num_heads=24, **kwargs)\n    return _create_bert_pipeline_model(**model_kwargs)\n\n\ndef bert_175B(**kwargs):\n    model_kwargs = dict(hidden_size=12288, depth=96, num_heads=96, **kwargs)\n    return _create_bert_pipeline_model(**model_kwargs)\n"}
{"type": "source_file", "path": "energonai/utils/checkpointing_hf_gpt2.py", "content": "import re\nfrom collections import OrderedDict\nimport torch\n\n\n__all__ = [\n    'processing_HF_GPT'\n]\n\nname_map = {\n    'ln_2': 'norm2',\n    'c_attn': 'query_key_value',\n    'attn.c_proj': 'attn.dense',\n    'ln_1': 'norm1',\n    'c_fc': 'dense_1',\n    'mlp.c_proj': 'mlp.dense_2'\n}\n\n\ndef judge_t(key_):\n    key_words = ['attn.query_key_value.weight', 'mlp.dense_1.weight', 'mlp.dense_2.weight', 'attn.dense.weight']\n    for word_ in key_words:\n        if word_ in key_:\n            return True\n    return False\n\n\ndef processing_HF_GPT(state_dict: OrderedDict):\n    if 'model' in state_dict:\n        state_dict = state_dict.pop('model')\n    new_dict = OrderedDict()\n    for k_ in state_dict.keys():\n        new_k = module_name_mapping(k_)\n        if new_k == \"\":\n            continue\n\n        new_v = state_dict[k_]\n        if judge_t(new_k):\n            new_v = torch.transpose(new_v, 0, 1)\n        if \"attn.query_key_value.weight\" in new_k:\n            num_ = re.search(r\"blocks\\.\\d+?\\.\", new_k)\n            if num_:\n                prefix = num_.group()\n            else:\n                prefix = ''\n            # print(\"prefix: {}\".format(prefix))\n            q_, k_, v_ = torch.chunk(new_v, 3, 0)\n            # new_dict[prefix + \"attn.query_.weight\"] = torch.transpose(q_, 0, 1)\n            # new_dict[prefix + \"attn.key_.weight\"] = torch.transpose(k_, 0, 1)\n            # new_dict[prefix + \"attn.value_.weight\"] = torch.transpose(v_, 0, 1)\n            new_dict[prefix + \"attn.query_.weight\"] = q_\n            new_dict[prefix + \"attn.key_.weight\"] = k_\n            new_dict[prefix + \"attn.value_.weight\"] = v_\n        elif \"attn.query_key_value.bias\" in new_k:\n            num_ = re.search(r\"blocks\\.\\d+?\\.\", new_k)\n            if num_:\n                prefix = num_.group()\n            else:\n                prefix = ''\n            # print(\"prefix: {}\".format(prefix))\n            q_, k_, v_ = torch.chunk(new_v, 3, 0)\n            new_dict[prefix + \"attn.query_.bias\"] = q_\n            new_dict[prefix + \"attn.key_.bias\"] = k_\n            new_dict[prefix + \"attn.value_.bias\"] = v_\n        else:\n            new_dict[new_k] = new_v\n    new_dict['head.dense.weight'] = new_dict['embed.word_embeddings.weight'].clone()\n    # print(\"=\"*100)\n    # print(new_dict.keys())\n    return {\"model\": new_dict, \"epoch\": 0}\n\n\ndef id_map(matched):\n    value = matched.group('value')\n    return \"blocks.{}.\".format(value)\n\n\ndef module_name_mapping(ori_name: str):\n    if ori_name == 'wte.weight':\n        return \"embed.word_embeddings.weight\"\n    elif ori_name == 'wpe.weight':\n        return \"embed.position_embeddings.weight\"\n    elif \"ln_f\" in ori_name:\n        return ori_name.replace('ln_f', 'norm')\n    elif \".attn.bias\" in ori_name:\n        return \"\"\n    else:\n        res = re.sub(r\"h\\.(?P<value>\\d+)?\\.\", id_map, ori_name)\n        for k_ in name_map.keys():\n            res = res.replace(k_, name_map[k_])\n        return res\n"}
{"type": "source_file", "path": "energonai/utils/checkpointing.py", "content": "from collections import OrderedDict\n\nimport torch\nimport torch.distributed as dist\n\nfrom colossalai.utils import is_using_pp\nfrom colossalai.context import ParallelMode\nfrom colossalai.core import global_context as gpc\nfrom typing import Optional, Callable\nfrom colossalai.utils.checkpointing import partition_pipeline_parallel_state_dict, broadcast_model\n\n\n__all__ = [\n    \"load_checkpoint\", \"load_state_dict\"\n]\n\nimport os\nfrom multiprocessing import Pool\nfrom time import time\n\n\ndef load_state_dict(path: str):\n    if os.path.isfile(path):\n        return torch.load(path)\n    assert os.path.isdir(path)\n    state_dict = {}\n    files = []\n    for filename in os.listdir(path):\n        filepath = os.path.join(path, filename)\n        if os.path.isfile(filepath):\n            files.append(filepath)\n    procs = int(os.environ.get('LOAD_N_PROC', '1'))\n    procs = min(procs, len(files))\n    print(f'load {len(files)} files using {procs} procs')\n    if procs > 1:\n        with Pool(procs) as pool:\n            state_dicts = pool.map(torch.load, files)\n        for sd in state_dicts:\n            state_dict.update(sd)\n    else:\n        for filepath in files:\n            sd = torch.load(filepath)\n            state_dict.update(sd)\n    return state_dict\n\n\ndef remove_prefix(state_dict, prefix):\n    if prefix[-1] != '.':\n        prefix += '.'\n    res_dict = OrderedDict()\n    for k_ in state_dict.keys():\n        res_dict[k_.replace(prefix, '')] = state_dict[k_]\n    return res_dict\n\n\ndef load_checkpoint(file,\n                    model: torch.nn.Module,\n                    strict: bool = True,\n                    preprocess_fn: Optional[Callable[[dict], dict]] = None,\n                    **kwargs):\n    \"\"\"Loads training states from a checkpoint file.\n\n    Args:\n        file: a file-like object (has to implement read(), readline(), tell(), and seek()), or a string or os.PathLike\n            object containing a file name.\n        model (:class:`torch.nn.Module`): Model to load saved weights and buffers.\n        optimizer (Union[:class:`torch.optim.Optimizer`, :class:`colossalai.nn.optimizer`]): Optimizer to recuperate.\n        lr_scheduler (:class:`torch.optim.lr_scheduler._LRScheduler`, optional):\n            lr_scheduler to recuperate, defaults to None.\n        strict (bool, optional): Whether to strictly enforce that the keys in :attr:`state_dict`\n            of the checkpoint match the names of parameters and buffers in model, defaults to True.\n\n    Returns:\n        int: The saved epoch number.\n\n    Raises:\n        RuntimeError: Raise error if the model/optimizer cannot successfully be recuperated\n    \"\"\"\n    start = time()\n    if gpc.get_local_rank(ParallelMode.MODEL) == 0:\n        model_state = load_state_dict(file)\n        if preprocess_fn:\n            model_state = preprocess_fn(model_state)\n    else:\n        model_state = dict()\n    dist.barrier()\n    print(f'Load file time: {time()-start:.3f} s')\n    # pipeline\n    if is_using_pp():\n        model_state = partition_pipeline_parallel_state_dict(model, model_state, **kwargs)\n    if \"prefix\" in kwargs.keys():\n        if kwargs['prefix'] != '':\n            model_state = remove_prefix(model_state, kwargs[\"prefix\"])\n\n    model.load_state_dict(model_state, strict=strict)\n    broadcast_model(model)\n\n    return -1\n"}
{"type": "source_file", "path": "energonai/model/endecoder.py", "content": "from typing import Callable\nimport torch\nfrom torch import dtype\nfrom torch import nn\nfrom colossalai.nn import LayerNorm1D\n\nfrom .mlp import MLP1D\nfrom .attention import MultiHeadAttention1D\n\n\nclass Block1D(nn.Module):\n    def __init__(self,\n                 hidden_size: int,\n                 num_heads: int,\n                 mlp_ratio: float,\n                 activation: Callable = nn.functional.gelu,\n                 layernorm_epsilon: float = 1e-5,\n                 dtype: dtype = torch.float16,\n                 bias: bool = True,\n                 apply_post_layernorm: bool = False,\n                 max_seq_len: int = 512,\n                 fused_qkv: bool = True,\n                 is_decoder: bool = True,\n                 disable_past_cache=False) -> None:\n        super().__init__()\n\n        self.apply_post_layernorm = apply_post_layernorm\n        self.norm1 = LayerNorm1D(hidden_size, eps=layernorm_epsilon, dtype=dtype)\n\n        self.attn = MultiHeadAttention1D(hidden_size=hidden_size,\n                                         num_heads=num_heads,\n                                         bias=bias,\n                                         dtype=dtype,\n                                         max_seq_len=max_seq_len,\n                                         fused_qkv=fused_qkv,\n                                         is_decoder=is_decoder,\n                                         disable_past_cache=disable_past_cache)\n\n        self.norm2 = LayerNorm1D(hidden_size, eps=layernorm_epsilon, dtype=dtype)\n\n        self.mlp = MLP1D(hidden_size=hidden_size,\n                         mlp_ratio=mlp_ratio,\n                         activation=activation,\n                         dtype=dtype,\n                         bias=bias,\n                         disable_past_cache=disable_past_cache)\n\n    def forward(self, hidden_states, attention_mask=None, first_cache=False, seq_lens=None):\n\n        if not self.apply_post_layernorm:\n            residual = hidden_states\n        hidden_states = self.norm1(hidden_states)\n\n        if self.apply_post_layernorm:\n            residual = hidden_states\n        hidden_states = residual + self.attn(hidden_states=hidden_states,\n                                             attention_mask=attention_mask,\n                                             first_cache=first_cache)\n\n        if not self.apply_post_layernorm:\n            residual = hidden_states\n\n        hidden_states = self.norm2(hidden_states)\n\n        if self.apply_post_layernorm:\n            residual = hidden_states\n        hidden_states = residual + self.mlp(hidden_states=hidden_states,\n                                            first_cache=first_cache)\n\n        return hidden_states\n"}
{"type": "source_file", "path": "energonai/utils/__init__.py", "content": "from .files import ensure_directory_exists\nfrom .timer import get_timers\nfrom .common import build_device_maps, use_lock, run_once, Terminator\n"}
{"type": "source_file", "path": "energonai/model/downstream.py", "content": "from torch import dtype, nn\nfrom colossalai.nn import Classifier1D, VocabParallelClassifier1D\n\n\nclass LMHead1D(nn.Module):\n    def __init__(self,\n                 hidden_size: int,\n                 vocab_size: int,\n                 word_embedding_weight: nn.Parameter = None,\n                 bias: bool = False,\n                 dtype: dtype = None,\n                 vocab_parallel: bool = False) -> None:\n        super().__init__()\n        self.vocab_parallel = vocab_parallel\n        if vocab_parallel:\n            self.dense = VocabParallelClassifier1D(hidden_size, vocab_size, bias=bias, dtype=dtype, gather_output=True)\n        else:\n            self.dense = Classifier1D(hidden_size, vocab_size, word_embedding_weight, bias=bias, dtype=dtype)\n\n    @property\n    def weight(self):\n        return self.dense.weight\n\n    def forward(self, x):\n        x = self.dense(x)\n        return x\n"}
{"type": "source_file", "path": "examples/auto_pipeline/bert_config.py", "content": "from bert import bert_small, bert_large, bert_xl, bert_8B, bert_175B\nfrom bert_server import launch_engine\nfrom bert import BertEmbedding1D, BertTransformerLayer1D\n\nmodel_class = bert_8B\nmodel_type = \"bert\"\nengine_server = launch_engine\n\n\n# parallel\ntp_init_size = 2\npp_init_size = 2\nauto_pp = True\nLeafSet = set([BertTransformerLayer1D, BertEmbedding1D])\n\n\n\nhost = \"127.0.0.1\"\nport = 29400\nhalf = False\nserver_host = \"127.0.0.1\"\nserver_port = 8010\nlog_level = \"info\"\nbackend = \"nccl\""}
{"type": "source_file", "path": "energonai/model/__init__.py", "content": "from .model_factory import gpt2_small, gpt2_large, gpt2_8B, gpt3\nfrom .model_factory import hf_gpt2\nfrom .model_factory import bert_small, bert_large, bert_8B, bert_175B\nfrom .model_factory import opt_125M, opt_6B, opt_30B, opt_66B, opt_175B\n"}
{"type": "source_file", "path": "energonai/model/mlp.py", "content": "\nfrom typing import Callable, Optional\nimport torch\nfrom torch import dtype, nn\nfrom colossalai.nn import Linear1D_Col, Linear1D_Row\n\n\nclass MLP1D(nn.Module):\n\n    def __init__(self,\n                 hidden_size: int,\n                 mlp_ratio: float,\n                 activation: Callable,\n                 dtype: dtype = torch.float16,\n                 bias: bool = True,\n                 disable_past_cache=False):\n        super().__init__()\n        self.disable_past_cache = disable_past_cache\n        intermediate_dim = int(hidden_size * mlp_ratio)\n        self.dense_1 = Linear1D_Col(hidden_size, intermediate_dim, bias=bias, dtype=dtype, gather_output=False)\n        self.activation = activation\n        self.dense_2 = Linear1D_Row(intermediate_dim, hidden_size, bias=bias, dtype=dtype, parallel_input=True)\n        self.past_cache = {}\n\n    def last_word(self, hidden_states):\n        batch_size = hidden_states.shape[0]\n        hidden_size = hidden_states.shape[2]\n        return hidden_states[:, -1, :].view(batch_size, 1, hidden_size)\n\n    def forward(self, hidden_states, first_cache: Optional[bool] = True):\n\n        if self.disable_past_cache:\n            hidden_states = self.dense_1(hidden_states)\n            hidden_states = self.activation(hidden_states)\n            hidden_states = self.dense_2(hidden_states)\n        else:\n            if first_cache:\n                hidden_states = self.dense_1(hidden_states)\n                self.past_cache['dense_1'] = hidden_states\n                hidden_states = self.activation(hidden_states)\n                hidden_states = self.dense_2(hidden_states)\n                self.past_cache['dense_2'] = hidden_states\n            else:\n                hidden_states = self.dense_1(self.last_word(hidden_states))\n                self.past_cache['dense_1'] = torch.cat((self.past_cache['dense_1'], hidden_states), 1)\n                hidden_states = self.activation(self.past_cache['dense_1'])\n                hidden_states = self.dense_2(self.last_word(hidden_states))\n                self.past_cache['dense_2'] = torch.cat((self.past_cache['dense_2'], hidden_states), 1)\n                hidden_states = self.past_cache['dense_2']\n\n        return hidden_states\n"}
{"type": "source_file", "path": "energonai/pipe.py", "content": "import torch.distributed.rpc as trpc\nimport time\nfrom queue import Queue, Empty\nfrom typing import Dict\nfrom threading import Lock\nfrom typing import Any\nfrom .utils import use_lock\n\n\ndef rpc_queue_can_put(q: trpc.RRef) -> bool:\n    q = q.local_value()\n    return not q.full()\n\n\ndef rpc_queue_put(q: trpc.RRef, data: Any) -> None:\n    q = q.local_value()\n    q.put(data)\n\n\nclass Pipe:\n    _queues: Dict[str, Queue] = {}\n    _lock = Lock()\n\n    def __init__(self, name: str, src: str, dest: str, max_size: int = 0) -> None:\n        self.rpc_info = trpc.get_worker_info()\n        self.name = name\n        self.src = src\n        self.dest = dest\n        self.remote_queue: trpc.RRef = None\n        self.local_queue: Queue[Any] = None\n        with use_lock(self._lock):\n            if src == self.rpc_info.name:\n                assert name not in self._queues, f'pipe {name} already exists on {self.rpc_info.name}'\n                self.remote_queue = self.get_remote_queue(max_size)\n                self._queues[name] = self.remote_queue\n\n    @classmethod\n    def rpc_create_local_queue(cls, name: str, max_size: int) -> Queue:\n        with use_lock(cls._lock):\n            assert name not in cls._queues, f'pipe {name} already exists'\n            cls._queues[name] = Queue(max_size)\n            return cls._queues[name]\n\n    def get_remote_queue(self, max_size: int) -> trpc.RRef:\n        return trpc.remote(self.dest, self.rpc_create_local_queue, args=(self.name, max_size))\n\n    def prepare_local_queue(self) -> None:\n        if self.local_queue is None:\n            with use_lock(self._lock):\n                if self.name in self._queues:\n                    self.local_queue = self._queues[self.name]\n\n    def recv(self) -> Any:\n        assert self.dest == self.rpc_info.name\n        while True:\n            self.prepare_local_queue()\n            if self.local_queue is not None:\n                return self.local_queue.get()\n            time.sleep(0.01)\n\n    def recv_nowait(self) -> Any:\n        assert self.dest == self.rpc_info.name\n        self.prepare_local_queue()\n        if self.local_queue is not None:\n            try:\n                return self.local_queue.get_nowait()\n            except Empty:\n                raise RuntimeError('pipe is empty')\n        raise RuntimeError('local queue is not created')\n\n    def send(self, data: Any) -> None:\n        assert self.src == self.rpc_info.name\n        while not trpc.rpc_sync(self.dest, rpc_queue_can_put, args=(self.remote_queue, )):\n            time.sleep(0.1)\n        trpc.rpc_sync(self.dest, rpc_queue_put, args=(self.remote_queue, data))\n"}
{"type": "source_file", "path": "energonai/nemesis/nemesis_manager.py", "content": "\"\"\"\n------------------------------------------\nClass gpu_info and Nemesis_manager\nMainly used for peer memory offloading\n------------------------------------------\n\"\"\"\n\nimport sys\nimport torch\nimport pynvml\n\nNUM_EXPAND = 1024 ** 3\n\n\nclass gpu_info:\n    \"\"\"\n    class used to monitor the status of each gpu device\n    \"\"\"\n\n    def __init__(self, device_id: int):\n        self._handle = pynvml.nvmlDeviceGetHandleByIndex(device_id)\n        info = pynvml.nvmlDeviceGetMemoryInfo(self._handle)\n        self.mem_used = info.used / NUM_EXPAND\n        self.mem_free = info.free / NUM_EXPAND\n        self.mem_managed = 0\n        self._hold_module_list = list()\n\n    def update_mem_state(self):\n        info = pynvml.nvmlDeviceGetMemoryInfo(self._handle)\n        self.mem_used = info.used / NUM_EXPAND\n        self.mem_free = info.free / NUM_EXPAND\n\n    def gpu_register_module(self, module_: torch.nn.Module):\n        self._hold_module_list.append(module_)\n        self.update_mem_state()\n\n    def release_module(self, module_: torch.nn.Module):\n        self._hold_module_list = [t for t in self._hold_module_list if id(t) != id(module_)]\n\n    def check_avail_mem(self, required_space: float):\n        self.update_mem_state()\n        if self.mem_free > required_space:\n            return True\n        else:\n            return False\n\n    def print_status(self):\n        print(\"mem used: {} mem free: {} mem managed: {},\"\n              \" \\n\".format(self.mem_used,\n                           self.mem_free,\n                           self.mem_managed,\n\n                           ))\n\n\nclass Nemesis_Manager:\n\n    def __init__(self):\n        pynvml.nvmlInit()\n        self._gpu_num = pynvml.nvmlDeviceGetCount()\n        self._gpu_info = {\"cuda:{}\".format(i): gpu_info(i) for i in range(self._gpu_num)}\n        self._module_list = list()\n        self.offload_dict = dict()\n        self.event_dict = dict()\n        self.offload_flags = None\n        self.prefetch_dict = dict()\n        self.compute_device_dict = dict()\n        self.module_size = dict()\n        self.layer_num = -1\n        self.offload_interval = -1\n        self.prefetch_layer = 3  # how many layers ahead do we prefetch a offloaded layer\n        self.free_device = None\n        self._model = None\n        # The two cuda streams separately needed for computing and offloading\n        self.offload_stream = torch.cuda.Stream()\n        self.compute_stream = torch.cuda.Stream()\n\n    def register_model(self, model_):\n        self._model = model_\n\n    def set_free_device(self, free_device):\n        \"\"\"\n        Call this function to assign the free device where we offload layers to.\n        \"\"\"\n        self.free_device = free_device\n\n    def set_model_info(self, layer_num, offload_interval):\n        \"\"\"\n        :param layer_num: the number of layers in the model\n        :param offload_interval: One in how many layers we offload the model\n        This function should be called in the initialize function of your model.\n        \"\"\"\n        assert layer_num % offload_interval == 0\n        self.layer_num = layer_num\n        self.offload_interval = offload_interval\n        self.generate_offload_dict()\n\n    def calculate_module_size(self, module_: torch.nn.Module):\n        res_size = 0\n        for ts in module_.parameters():\n            res_size += ts.data.numel() * ts.data.element_size()\n        res_size /= NUM_EXPAND\n        return res_size\n\n    def move_module(self, module_: torch.nn.Module, target_device):\n        with torch.cuda.stream(self.offload_stream):\n            module_ = module_.to(target_device, non_blocking=True)\n\n    def generate_offload_dict(self):\n        assert self.layer_num != -1 and self.offload_interval != -1, 'please set layer num and offload interval first'\n        res_dict = {i: False for i in range(self.layer_num)}\n        for j in range(self.offload_interval - 1, self.layer_num, self.offload_interval):\n            res_dict[j] = True\n        self.offload_flags = res_dict\n\n    def offload_module(self, module_):\n        free_device = self.free_device\n        if free_device is None:\n            raise AssertionError(\"please call set_free_device function first\")\n        Ne_manager.move_module(module_, free_device)\n\n    def apply_hook(self):\n        \"\"\"\n        This function is used for implmenting pre_hooks of pytorch so as to achieve offloading and prefetch.\n        PLEASE CALL THIS FUNCTION before inference if you want to enable offloading.\n        \"\"\"\n        for i in range(len(self._module_list)):\n            if i % self.offload_interval == 0:\n                self.offload_dict[id(self._module_list[i])].append(self._module_list[i - 1])\n            if self.offload_interval == 2:\n                if i % self.offload_interval == 0:\n                    self.prefetch_dict[id(self._module_list[i])].append(self._module_list[i + 1])\n            else:\n                if i % self.offload_interval == self.offload_interval - self.prefetch_layer:\n                    self.prefetch_dict[id(self._module_list[i])].append(self._module_list[i + self.prefetch_layer - 1])\n            if len(self.prefetch_dict[id(self._module_list[i])]) + len(self.offload_dict[id(self._module_list[i])]) > 0:\n                self._module_list[i].register_forward_pre_hook(basic_hook)\n\n    def register_module(self, module_: torch.nn.Module, device: str):\n        self._gpu_info[device].gpu_register_module(module_)\n        self.offload_dict[id(module_)] = list()\n        self.prefetch_dict[id(module_)] = list()\n        self.event_dict[id(module_)] = None\n        self._module_list.append(module_)\n        self.compute_device_dict[id(module_)] = device\n        self.module_size[id(module_)] = self.calculate_module_size(module_)\n\n    def find_free_gpu(self, size: float, ori_gpu):\n        if isinstance(ori_gpu, torch.device):\n            ori_gpu = \"{}:{}\".format(ori_gpu.type, ori_gpu.index)\n        for gpu_name, gpu_ in self._gpu_info.items():\n            if gpu_name == ori_gpu:\n                continue\n            if gpu_.check_avail_mem(size):\n                return gpu_name\n        print(\"Error: No available gpu memory\")\n        sys.exit()\n\n    def print_status(self):\n        print(\"=\" * 60)\n        for gpu_info_ in self._gpu_info.values():\n            gpu_info_.print_status()\n        print(\"=\" * 60)\n\n\nNe_manager = Nemesis_Manager()\n\n\ndef basic_hook(module: torch.nn.Module, input_):\n    \"\"\"\n    The hook function required by pytorch register_forward_pre_hook function.\n    We use this function to launch the offloading and prefetching process on the offload stream\n    so as to achieve overlap.\n    \"\"\"\n    for tg in Ne_manager.offload_dict[id(module)]:\n        cur_device = next(tg.parameters()).device\n        if Ne_manager.compute_device_dict[id(tg)] == \"{}:{}\".format(cur_device.type, cur_device.index):\n            free_device = Ne_manager.free_device\n            if free_device is None:\n                raise AssertionError(\"please call set_free_device function first\")\n            Ne_manager.move_module(tg, free_device)\n    for tg in Ne_manager.prefetch_dict[id(module)]:\n        Ne_manager.move_module(tg, Ne_manager.compute_device_dict[id(tg)])\n        with torch.cuda.stream(Ne_manager.offload_stream):\n            evt_2 = Ne_manager.offload_stream.record_event()\n            Ne_manager.event_dict[id(tg)] = evt_2\n    return\n"}
{"type": "source_file", "path": "energonai/utils/checkpointing_opt.py", "content": "import os\nimport re\nfrom collections import OrderedDict\nfrom typing import Dict\n\nimport torch\nfrom colossalai.context import ParallelMode\nfrom colossalai.core import global_context as gpc\n\n__all__ = [\n    'processing_OPT'\n]\n\nname_map = {\n    'embed_tokens': 'embed.word_embeddings',\n    'embed_positions': 'embed.position_embeddings',\n    # 'layers': 'blocks',\n    'self_attn.q_proj': 'attn.query_',\n    'self_attn.k_proj': 'attn.key_',\n    'self_attn.v_proj': 'attn.value_',\n    'self_attn.out_proj': 'attn.dense',\n    'self_attn_layer_norm': 'norm1.module',\n    'final_layer_norm': 'norm2.module',\n    'fc1': 'mlp.dense_1',\n    'fc2': 'mlp.dense_2'\n}\n\n\ndef judge_t(key_):\n    key_words = ['attn.query_key_value.weight', 'mlp.dense_1.weight', 'mlp.dense_2.weight', 'attn.dense.weight']\n    for word_ in key_words:\n        if word_ in key_:\n            return True\n    return False\n\n\ndef module_name_mapping(ori_name: str):\n    # print(ori_name)\n    if ori_name == 'decoder.embed_tokens.weight':\n        return \"embed.word_embeddings.weight\"\n    elif ori_name == 'decoder.embed_positions.weight':\n        return \"embed.position_embeddings.weight\"\n    elif \"decoder.layer_norm\" in ori_name:\n        return ori_name.replace('decoder.layer_norm', 'norm.module')\n    elif \"decoder.final_layer_norm\" in ori_name:  # hugging face style\n        return ori_name.replace('decoder.final_layer_norm', 'norm.module')\n    # elif \".attn.bias\" in ori_name:\n    #     return \"\"\n    else:\n        res = re.sub(r\"decoder.layers\\.(?P<value>\\d+)?\\.\", id_map, ori_name)\n        for k_ in name_map.keys():\n            res = res.replace(k_, name_map[k_])\n        return res\n\n\ndef processing_OPT(state_dict: OrderedDict):\n    if 'model' in state_dict:\n        state_dict = state_dict.pop('model')\n    new_dict = OrderedDict()\n    for k_ in state_dict.keys():\n        new_k = module_name_mapping(k_)\n        if new_k == \"\":\n            continue\n        new_v = state_dict[k_]\n        new_dict[new_k] = new_v\n        # if judge_t(new_k):\n        #     new_v = torch.transpose(new_v, 0, 1)\n        # if \"attn.query_key_value.weight\" in new_k:\n        #     num_ = re.search(r\"blocks\\.\\d+?\\.\", new_k)\n        #     if num_:\n        #         prefix = num_.group()\n        #     else:\n        #         prefix = ''\n        #     # print(\"prefix: {}\".format(prefix))\n        #     q_, k_, v_ = torch.chunk(new_v, 3, 0)\n        #     # new_dict[prefix + \"attn.query_.weight\"] = torch.transpose(q_, 0, 1)\n        #     # new_dict[prefix + \"attn.key_.weight\"] = torch.transpose(k_, 0, 1)\n        #     # new_dict[prefix + \"attn.value_.weight\"] = torch.transpose(v_, 0, 1)\n        #     new_dict[prefix + \"attn.query_.weight\"] = q_\n        #     new_dict[prefix + \"attn.key_.weight\"] = k_\n        #     new_dict[prefix + \"attn.value_.weight\"] = v_\n        # elif \"attn.query_key_value.bias\" in new_k:\n        #     num_ = re.search(r\"blocks\\.\\d+?\\.\", new_k)\n        #     if num_:\n        #         prefix = num_.group()\n        #     else:\n        #         prefix = ''\n        #     # print(\"prefix: {}\".format(prefix))\n        #     q_, k_, v_ = torch.chunk(new_v, 3, 0)\n        #     new_dict[prefix + \"attn.query_.bias\"] = q_\n        #     new_dict[prefix + \"attn.key_.bias\"] = k_\n        #     new_dict[prefix + \"attn.value_.bias\"] = v_\n        # else:\n        #     new_dict[new_k] = new_v\n    # print(new_dict.keys())\n    if 'head.dense.weight' not in new_dict:\n        new_dict['head.dense.weight'] = new_dict['embed.word_embeddings.weight'].clone()\n\n    if 'decoder.version' in new_dict:\n        del new_dict['decoder.version']\n    # print(\"=\"*100)\n    # print(new_dict.keys())\n    # print(\"---------------------------\")\n    return new_dict  # {\"model\": new_dict, \"epoch\": 0}\n\n\ndef id_map(matched):\n    value = matched.group('value')\n    return \"blocks.{}.\".format(value)\n\n\ndef preprocess_175b(state_dict: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n    key_map = {\n        'decoder.embed_tokens.weight': 'embed.word_embeddings.weight',\n        'decoder.embed_positions.weight': 'embed.position_embeddings.weight',\n        'decoder.layer_norm': 'norm',\n        'decoder.layers': 'blocks',\n        'self_attn.qkv_proj': 'attn.query_key_value',\n        'self_attn.out_proj': 'attn.dense',\n        'self_attn_layer_norm': 'norm1',\n        'final_layer_norm': 'norm2',\n        'fc1': 'mlp.dense_1',\n        'fc2': 'mlp.dense_2'\n    }\n    output_sd = {}\n    for k, v in state_dict.items():\n        new_key = k\n        for old, new in key_map.items():\n            new_key = new_key.replace(old, new)\n        output_sd[new_key] = v\n    output_sd['head.dense.weight'] = output_sd['embed.word_embeddings.weight'].clone()\n    return output_sd\n\n\ndef load_175b(checkpoint_dir: str, model: torch.nn.Module) -> None:\n    tp_rank = gpc.get_local_rank(ParallelMode.PARALLEL_1D)\n    checkpoint_path = os.path.join(checkpoint_dir, f'reshard-model_part-{tp_rank}.pt')\n    print(f'Rank{gpc.get_global_rank()} load {checkpoint_path}')\n    state_dict = torch.load(checkpoint_path)\n    state_dict = preprocess_175b(state_dict)\n    for n, p in model.named_parameters():\n        with torch.no_grad():\n            p.copy_(state_dict[n])\n"}
{"type": "source_file", "path": "energonai/utils/files.py", "content": "import os\n\n\ndef ensure_directory_exists(path: str):\n    # ensure the directory exists\n    # dir = os.path.dirname(path)\n    if not os.path.exists(path):\n        os.makedirs(path)"}
{"type": "source_file", "path": "examples/auto_pipeline/bert.py", "content": "import math\nfrom typing import Callable\n\nimport os\nimport torch\nfrom torch import nn as nn, Tensor, dtype\n\nfrom colossalai.context import ParallelMode\nfrom colossalai.core import global_context as gpc\nfrom colossalai.logging import get_dist_logger\nfrom colossalai.nn.layer.utils import divide, ACT2FN\nfrom colossalai.nn import Linear1D_Col, Linear1D_Row, LayerNorm1D, VocabParallelEmbedding1D\nfrom energonai.kernel import transpose_pad, transpose_depad, depad\nfrom colossalai.utils import get_current_device, is_using_pp\n\n__all__ = [\n    'BertEmbedding1D'\n    'BertMLP1D',\n    'BertSelfAttention1D',\n    'BertTransformerLayer1D'\n]\n\nfrom energonai.utils.checkpointing import load_checkpoint\n\n\nclass BertEmbedding1D(nn.Module):\n    def __init__(self,\n                 embedding_dim: int,  # hidden_size\n                 vocab_size: int,\n                 max_position_embeddings: int,\n                 num_tokentypes: int = 0,\n                 padding_idx: int = 0,\n                 layernorm_epsilon: float = 1e-5,\n                 dtype: dtype = None) -> None:\n        super().__init__()\n        self.word_embeddings = VocabParallelEmbedding1D(vocab_size, embedding_dim, padding_idx=padding_idx, dtype=dtype)\n        self.position_embeddings = VocabParallelEmbedding1D(max_position_embeddings, embedding_dim, dtype=dtype)\n        if num_tokentypes > 0:\n            self.tokentype_embeddings = VocabParallelEmbedding1D(num_tokentypes, embedding_dim, dtype=dtype)\n        else:\n            self.tokentype_embeddings = None\n\n        self.LayerNorm = LayerNorm1D(embedding_dim, eps=layernorm_epsilon, dtype=dtype)\n\n    def forward(self, input_ids, position_ids=None, tokentype_ids=None):\n        # max_padding_size = input_ids.shape[1]\n\n        # TODO: register_buffer in advance for position_ids to speedup\n\n        # if position_ids is None:\n        #     position_ids = torch.arange(max_padding_size, dtype=torch.long, device=get_current_device()).unsqueeze(0)\n\n        x = self.word_embeddings(input_ids)  # + self.position_embeddings(position_ids)\n\n        if self.tokentype_embeddings is not None and tokentype_ids is not None:\n            x = x + self.tokentype_embeddings(tokentype_ids)\n\n        x = self.LayerNorm(x)\n\n        # if seq_lens is not None:\n        #     x = depad(x, batch_size, seq_lens)\n\n        return x\n\n\nclass BertSelfAttention1D(nn.Module):\n    def __init__(self,\n                 hidden_size: int,\n                 num_heads: int,\n                 bias: bool = True,\n                 fuse_scale_mask_softmax: bool = False,\n                 layernorm_epsilon: float = 1e-5,\n                 dtype: dtype = None) -> None:\n        super().__init__()\n        if hidden_size % num_heads != 0:\n            raise ValueError(\n                f\"The hidden size ({hidden_size}) is not a multiple of the number of attention \")\n        self.hidden_size = hidden_size\n        self.attention_head_size = divide(hidden_size, num_heads)\n        self.fuse_scale_mask_softmax = fuse_scale_mask_softmax\n\n        self.query_key_value = Linear1D_Col(hidden_size, 3 * hidden_size, bias=bias, dtype=dtype)\n\n        if fuse_scale_mask_softmax:\n            raise NotImplementedError\n\n        self.dense = Linear1D_Row(hidden_size, hidden_size, bias=True, dtype=dtype, parallel_input=True)\n        self.LayerNorm = LayerNorm1D(hidden_size, eps=layernorm_epsilon, dtype=dtype)\n\n    def forward(self, hidden_states, attention_mask=None):\n\n        attention_output = self.query_key_value(hidden_states)\n        all_head_size = attention_output.shape[-1] // 3\n        num_attention_heads = divide(all_head_size, self.attention_head_size)  # num_heads\n\n        new_qkv_shape = attention_output.shape[:-1] + (num_attention_heads, 3 * self.attention_head_size)\n        attention_output = attention_output.view(new_qkv_shape)\n\n        # if seq_lens is not None:\n        #     # TODO: use FasterTransformer's implementation.\n        #     attention_output = transpose_pad(attention_output, batch_size, max_padding_size, seq_lens,\n        #                                      num_attention_heads, self.attention_head_size * 3)\n        # else:\n        attention_output = attention_output.permute(0, 2, 1, 3)\n        # TODO: make sure self.attention_head_size*3 is correct\n\n        q, k, v = torch.chunk(attention_output, 3, dim=-1)\n\n        attention_output = torch.matmul(q, k.transpose(-1, -2))\n        if self.fuse_scale_mask_softmax:\n            raise NotImplementedError\n        else:\n            attention_output = attention_output / math.sqrt(self.attention_head_size)\n            # if attention_mask is not None:\n            #     attention_output = attention_output + attention_mask\n            attention_output = nn.functional.softmax(attention_output, dim=-1)\n\n        attention_output = torch.matmul(attention_output, v)\n\n        # if seq_lens is not None:\n        #     sum_seq = torch.sum(seq_lens)\n        #     attention_output = transpose_depad(attention_output, batch_size, sum_seq, max_padding_size, seq_lens,\n        #    num_attention_heads, self.attention_head_size)\n        # else:\n        attention_output = attention_output.permute(0, 2, 1, 3).contiguous()\n\n        new_context_layer_shape = attention_output.size()[:-2] + (all_head_size,)\n        attention_output = attention_output.reshape(new_context_layer_shape)\n        attention_output = self.dense(attention_output)\n\n        hidden_states = self.LayerNorm(attention_output + hidden_states)\n\n        return hidden_states\n\n\ndef gelu_impl(x):\n    \"\"\"OpenAI's gelu implementation.\"\"\"\n    return 0.5 * x * (1.0 + torch.tanh(0.7978845608028654 * x *\n                                       (1.0 + 0.044715 * x * x)))\n\n\nclass BertMLP1D(nn.Module):\n    def __init__(self,\n                 hidden_size: int,\n                 mlp_ratio: float,\n                 activation: Callable = gelu_impl,\n                 layernorm_epsilon: float = 1e-5,\n                 dtype: dtype = None,\n                 bias: bool = True):\n        super().__init__()\n        intermediate_dim = int(hidden_size * mlp_ratio)\n        self.layer_0 = Linear1D_Col(hidden_size, intermediate_dim, bias=bias, dtype=dtype, gather_output=False)\n        self.activation = activation\n        self.layer_1 = Linear1D_Row(intermediate_dim, hidden_size, bias=bias, dtype=dtype, parallel_input=True)\n        self.LayerNorm = LayerNorm1D(hidden_size, eps=layernorm_epsilon, dtype=dtype)\n\n    def forward(self, input_tensor):\n        hidden_states = self.layer_0(input_tensor)\n        hidden_states = self.activation(hidden_states)\n        hidden_states = self.layer_1(hidden_states)\n\n        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n        return hidden_states\n\n\nclass BertTransformerLayer1D(nn.Module):\n    def __init__(self,\n                 hidden_size: int,\n                 num_heads: int,\n                 mlp_ratio: float,\n                 activation: Callable = gelu_impl,\n                 layernorm_epsilon: float = 1e-5,\n                 dtype: dtype = None,\n                 bias: bool = True,\n                 fuse_scale_mask_softmax: bool = False):\n        super().__init__()\n\n        self.attention = BertSelfAttention1D(hidden_size,\n                                             num_heads,\n                                             bias,\n                                             fuse_scale_mask_softmax,\n                                             layernorm_epsilon,\n                                             dtype)\n        self.mlp = BertMLP1D(hidden_size,\n                             mlp_ratio,\n                             activation,\n                             layernorm_epsilon,\n                             dtype,\n                             bias)\n\n    def forward(self, hidden_states, attention_mask):\n\n        batch_size = hidden_states.shape[0]\n\n        if attention_mask is not None:\n            attention_mask = attention_mask.view(batch_size, -1)\n            attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n            attention_mask = attention_mask.to(dtype=hidden_states.dtype)  # fp16 compatibility\n            attention_mask = (1.0 - attention_mask) * - 10000.0\n\n        hidden_states = self.attention(hidden_states, attention_mask)\n        hidden_states = self.mlp(hidden_states)\n\n        return hidden_states\n\n\nclass Bert1D(nn.Module):\n\n    def __init__(self,\n                 vocab_size: int = 50304,\n                 max_position_embeddings: int = 1024,\n                 hidden_size: int = 768,\n                 num_heads: int = 12,\n                 depth: int = 12,\n                 mlp_ratio: float = 4.0,\n                 layernorm_epsilon: float = 1e-5,\n                 activation: Callable = nn.functional.gelu,\n                 padding_idx: int = 0,\n                 dtype: dtype = None,\n                 bias: bool = True,\n                 fuse_scale_mask_softmax: bool = False,\n                 ):\n        super().__init__()\n        self.embed = BertEmbedding1D(embedding_dim=hidden_size,\n                                     vocab_size=vocab_size,\n                                     max_position_embeddings=max_position_embeddings,\n                                     padding_idx=padding_idx,\n                                     layernorm_epsilon=layernorm_epsilon,\n                                     dtype=dtype)\n        self.blocks = nn.ModuleList()\n\n        for i in range(depth):\n            self.blocks.append(BertTransformerLayer1D(\n                hidden_size=hidden_size,\n                num_heads=num_heads,\n                mlp_ratio=mlp_ratio,\n                activation=activation,\n                layernorm_epsilon=layernorm_epsilon,\n                dtype=dtype,\n                bias=bias,\n                fuse_scale_mask_softmax=fuse_scale_mask_softmax,)\n            )\n\n    def forward(self, hidden_states=None, input_ids=None, attention_mask=None, seq_lens=None):\n\n        # batch_size = input_ids.shape[0]\n        # max_padding_size = input_ids.shape[1]\n\n        hidden_states = self.embed(input_ids=input_ids, position_ids=None, tokentype_ids=None)  # , seq_lens\n\n        for block in self.blocks:\n            hidden_states = block(hidden_states=hidden_states, attention_mask=attention_mask)\n\n        hidden_states = hidden_states[:, 1, :]\n\n        return hidden_states\n\n\ndef _create_bert_model(model_kwargs):\n    model = Bert1D(**model_kwargs)\n    return model\n\n\ndef bert_small(**kwargs):\n    model_kwargs = dict(hidden_size=768, depth=12, num_heads=12, **kwargs)\n    return _create_bert_model(model_kwargs)\n\n\ndef bert_large(**kwargs):\n    model_kwargs = dict(hidden_size=1024, depth=24, num_heads=16, **kwargs)\n    return _create_bert_model(model_kwargs)\n\n\ndef bert_xl(**kwargs):\n    model_kwargs = dict(hidden_size=1600, depth=48, num_heads=16, **kwargs)\n    return _create_bert_model(model_kwargs)\n\n\ndef bert_8B(**kwargs):\n    model_kwargs = dict(hidden_size=3072, depth=72, num_heads=24, **kwargs)\n    return _create_bert_model(model_kwargs)\n\n\ndef bert_175B(**kwargs):\n    model_kwargs = dict(hidden_size=12288, depth=96, num_heads=96, **kwargs)\n    return _create_bert_model(model_kwargs)\n"}
{"type": "source_file", "path": "energonai/task.py", "content": "from dataclasses import dataclass\nfrom typing import Hashable, Tuple, Any\n\n\n@dataclass\nclass TaskEntry:\n    uids: Tuple[Hashable, ...]\n    batch: Any\n"}
{"type": "source_file", "path": "energonai/pipelinable/energon_tracer.py", "content": "import torch.fx\nfrom energonai.context import MEATCONFIG\n\nclass EnergonTracer(torch.fx.Tracer):\n    def is_leaf_module(self, m: torch.nn.Module, module_qualified_name: str) -> bool:\n        leaves = MEATCONFIG[\"LeafSet\"] # set([BertTransformerLayer])\n        return type(m) in leaves"}
{"type": "source_file", "path": "energonai/model/model_factory.py", "content": "import os\nimport random\nimport time\nfrom typing import Callable, Optional\n\nimport torch\nfrom colossalai.context import ParallelMode\nfrom colossalai.core import global_context as gpc\nfrom colossalai.logging import get_dist_logger\nfrom colossalai.nn import LayerNorm1D\nfrom colossalai.utils import get_current_device, is_using_pp\nfrom torch import dtype, nn\n\nfrom energonai.utils.checkpointing import load_checkpoint\nfrom energonai.utils.checkpointing_hf_gpt2 import processing_HF_GPT\nfrom energonai.utils.checkpointing_opt import load_175b, processing_OPT\n\nfrom .downstream import LMHead1D\nfrom .embedding import Embedding1D\nfrom .endecoder import Block1D\n\ntry:\n    from transformers.generation_logits_process import (\n        LogitsProcessorList, TemperatureLogitsWarper, TopKLogitsWarper,\n        TopPLogitsWarper)\nexcept ImportError:\n    from transformers.generation import (LogitsProcessorList,\n                                         TemperatureLogitsWarper,\n                                         TopKLogitsWarper, TopPLogitsWarper)\n\n\ndef gelu_impl(x):\n    \"\"\"OpenAI's gelu implementation.\"\"\"\n    return 0.5 * x * (1.0 + torch.tanh(0.7978845608028654 * x *\n                                       (1.0 + 0.044715 * x * x)))\n\n\ndef select_top_k(predictions, k=5):\n    predicted_index = random.choice(predictions[0, -1, :].sort(descending=True)[1][:10])  # .item()\n    return predicted_index\n\n\nclass PipelineModel(nn.Module):\n    def __init__(self,\n                 vocab_size: int = 50257,\n                 num_tokentypes: int = 0,\n                 max_seq_len: int = 512,\n                 hidden_size: int = 768,\n                 num_heads: int = 12,\n                 depth: int = 12,\n                 mlp_ratio: float = 4.0,\n                 layernorm_epsilon: float = 1e-5,\n                 activation: Callable = gelu_impl,\n                 padding_idx: int = 0,\n                 dtype: dtype = torch.float16,\n                 bias: bool = True,\n                 apply_post_layernorm: bool = False,\n                 first: bool = False,\n                 last: bool = False,\n                 fused_qkv: bool = True,\n                 checkpoint: str = None,\n                 model_name: str = None,\n                 is_decoder: bool = True,\n                 disable_past_cache=False,\n                 vocab_parallel: bool = False) -> None:\n        super().__init__()\n        self.hidden_size = hidden_size\n        self.first = first\n        self.last = last\n        self.max_seq_len = max_seq_len\n        self.model_name = model_name\n\n        if first:\n            self.embed = Embedding1D(hidden_size=hidden_size,\n                                     vocab_size=vocab_size,\n                                     max_seq_len=max_seq_len,\n                                     num_tokentypes=num_tokentypes,\n                                     padding_idx=padding_idx,\n                                     dtype=dtype,\n                                     vocab_parallel=vocab_parallel)\n\n        self.blocks = nn.ModuleList()\n        self.pp_rank = gpc.get_local_rank(ParallelMode.PIPELINE) if is_using_pp() else 0\n        for id_ in range(depth):\n            self.blocks.add_module(f'{id_ + self.pp_rank * depth}',\n                                   Block1D(hidden_size=hidden_size,\n                                           num_heads=num_heads,\n                                           mlp_ratio=mlp_ratio,\n                                           activation=activation,\n                                           layernorm_epsilon=layernorm_epsilon,\n                                           dtype=dtype,\n                                           bias=bias,\n                                           apply_post_layernorm=apply_post_layernorm,\n                                           max_seq_len=max_seq_len,\n                                           fused_qkv=fused_qkv,\n                                           is_decoder=is_decoder,\n                                           disable_past_cache=disable_past_cache))\n        if last:\n            self.norm = LayerNorm1D(normalized_shape=hidden_size, eps=layernorm_epsilon, dtype=dtype)\n            self.head = LMHead1D(hidden_size=hidden_size, vocab_size=vocab_size,\n                                 bias=False, dtype=dtype, vocab_parallel=vocab_parallel)\n\n    def forward(self, hidden_states=None, input_ids=None, attention_mask=None, seq_lens=None, max_tokens: Optional[int] = None, top_k: Optional[int] = None, top_p: Optional[float] = None, temperature: Optional[float] = None):\n        batch_size = input_ids.shape[0]\n        cur_len = input_ids.shape[1]\n        tgt_len = cur_len + 1 if not max_tokens else max_tokens\n\n        if(cur_len >= tgt_len):\n            return input_ids\n\n        first_cache = True\n        for _ in range(cur_len, tgt_len):\n\n            if self.first:\n                hidden_states = self.embed(input_ids)\n\n            if attention_mask is not None:\n                attention_unfold_mask = attention_mask.view(batch_size, -1)\n                attention_unfold_mask = attention_unfold_mask.unsqueeze(1).unsqueeze(2)\n                attention_unfold_mask = attention_unfold_mask.to(dtype=hidden_states.dtype)  # fp16 compatibility\n                attention_unfold_mask = (1.0 - attention_unfold_mask) * -10000.0\n\n            for block in self.blocks:\n                hidden_states = block(hidden_states=hidden_states,\n                                      attention_mask=attention_unfold_mask,\n                                      first_cache=first_cache)\n\n            if self.last:\n                hidden_states = self.norm(hidden_states)\n                hidden_states = self.head(hidden_states)\n                hidden_states = self.generate(input_ids, hidden_states, top_k=top_k,\n                                              top_p=top_p, temperature=temperature)\n            if torch.all(hidden_states == 50256):\n                break  # hard code here for opt\n            else:\n                input_ids = torch.cat((input_ids, hidden_states.view(-1, 1)), 1)\n                attention_mask = torch.cat((attention_mask, torch.ones(\n                    batch_size, 1, device=torch.cuda.current_device())), 1)\n\n            first_cache = False\n        return input_ids if max_tokens else hidden_states\n\n    def get_logits_processor(self, top_k: Optional[int] = None, top_p: Optional[float] = None, temperature: Optional[float] = None):\n        processor_list = LogitsProcessorList()\n        if temperature is not None and temperature != 1.0:\n            processor_list.append(TemperatureLogitsWarper(temperature))\n        if top_k is not None and top_k != 0:\n            processor_list.append(TopKLogitsWarper(top_k))\n        if top_p is not None and top_p < 1.0:\n            processor_list.append(TopPLogitsWarper(top_p))\n        return processor_list\n\n    def generate(self, input_ids, logits, top_k: Optional[int] = None, top_p: Optional[float] = None, temperature: Optional[float] = None):\n        logits = logits[:, -1, :]\n        logits_processor = self.get_logits_processor(top_k, top_p, temperature)\n        logits = logits_processor(input_ids, logits)\n        logits = torch.softmax(logits, -1, dtype=torch.float)\n        logits = torch.multinomial(logits, num_samples=1).squeeze(1)\n        return logits\n\n\ndef partition_uniform(num_items, pipeline_parallel_size):\n    logger = get_dist_logger('energonai')\n    assert num_items % pipeline_parallel_size == 0, \\\n        \"Layer length should be divided by the number of pipeline size, otherwise parameter method is recomended\"\n\n    parts = [[] for _ in range(pipeline_parallel_size)]\n\n    base_idx = 0\n    chunk_size = num_items // pipeline_parallel_size\n    left = pipeline_parallel_size - num_items % pipeline_parallel_size\n    if chunk_size == 0:\n        logger.warning(\"Some nodes in Pipeline have no requests\")\n\n    for p in range(pipeline_parallel_size):\n        st = base_idx\n        base_idx += chunk_size + (p >= left)\n        parts[p].append((st, base_idx))\n    return parts\n\n\ndef create_pipeline_model(depth: int = 48,\n                          layer_partitions=None,\n                          **model_kwargs):\n    logger = get_dist_logger('energonai')\n    pipeline_size = 0\n    pipeline_rank = 0\n\n    if gpc.is_initialized(ParallelMode.PIPELINE):\n        pipeline_size = gpc.get_world_size(ParallelMode.PIPELINE)\n        pipeline_rank = gpc.get_local_rank(ParallelMode.PIPELINE)\n    else:\n        pipeline_size = 1\n        pipeline_rank = 0\n\n    rank = gpc.get_global_rank()\n\n    parts = partition_uniform(depth, pipeline_size)[pipeline_rank] if layer_partitions is None else layer_partitions\n\n    for start, end in parts:\n        model_kwargs['first'] = start == 0\n        model_kwargs['last'] = end == depth\n        model_kwargs['depth'] = end - start\n        model = PipelineModel(**model_kwargs).to(get_current_device())\n        logger.info(f'==> Rank {rank} built layer {start}-{end} / total {depth}')\n\n    numel = 0\n    for _, param in model.named_parameters(recurse=True):\n        numel += param.numel()\n    logger.info(f'Rank{rank}/{pipeline_rank} model size = {numel * 2 / 1e9} GB')\n\n    if \"checkpoint\" in model_kwargs.keys() and \"model_name\" in model_kwargs.keys():\n        start = time.time()\n        assert os.path.exists(model_kwargs[\"checkpoint\"]), \"Checkpoint file not found\"\n        if model_kwargs['model_name'] == 'opt-175b':\n            load_175b(model_kwargs[\"checkpoint\"], model)\n        else:\n            preprocess_fn = None\n            if model_kwargs[\"model_name\"] == \"hf_gpt2\":\n                preprocess_fn = processing_HF_GPT\n            elif model_kwargs[\"model_name\"] == \"opt\":\n                preprocess_fn = processing_OPT\n            load_checkpoint(model_kwargs[\"checkpoint\"], model, preprocess_fn=preprocess_fn, **model_kwargs)\n        logger.info(f'Load time: {time.time() - start:.3f} s')\n\n    return model\n\n\ndef hf_gpt2(**kwargs):\n    model_kwargs = dict(hidden_size=768,\n                        depth=12,\n                        max_seq_len=1024,\n                        num_heads=12,\n                        fused_qkv=False,\n                        model_name=\"hf_gpt2\",\n                        is_decoder=True,\n                        **kwargs)\n    return create_pipeline_model(**model_kwargs)\n\n\ndef gpt2_small(**kwargs):\n    model_kwargs = dict(hidden_size=768, depth=12, num_heads=12, is_decoder=True, **kwargs)\n    return create_pipeline_model(**model_kwargs)\n\n\ndef gpt2_large(**kwargs):\n    model_kwargs = dict(hidden_size=1536, depth=36, num_heads=12, is_decoder=True, **kwargs)\n    return create_pipeline_model(**model_kwargs)\n\n\ndef gpt2_8B(**kwargs):\n    model_kwargs = dict(hidden_size=3072, depth=72, num_heads=24, is_decoder=True, **kwargs)\n    return create_pipeline_model(**model_kwargs)\n\n\ndef gpt3(**kwargs):\n    model_kwargs = dict(hidden_size=12288, depth=12, num_heads=96, is_decoder=True, **kwargs)\n    return create_pipeline_model(**model_kwargs)\n\n\ndef bert_small(**kwargs):\n    model_kwargs = dict(hidden_size=768, depth=12, num_heads=12, is_decoder=False, **kwargs)\n    return create_pipeline_model(**model_kwargs)\n\n\ndef bert_large(**kwargs):\n    model_kwargs = dict(hidden_size=1024, depth=24, num_heads=16, is_decoder=False, **kwargs)\n    return create_pipeline_model(**model_kwargs)\n\n\ndef bert_8B(**kwargs):\n    model_kwargs = dict(hidden_size=3072, depth=72, num_heads=24, is_decoder=False, **kwargs)\n    return create_pipeline_model(**model_kwargs)\n\n\ndef bert_175B(**kwargs):\n    model_kwargs = dict(hidden_size=12288, depth=96, num_heads=96, is_decoder=False, **kwargs)\n    return create_pipeline_model(**model_kwargs)\n\n\ndef opt_125M(**kwargs):\n    model_kwargs = dict(vocab_size=50272,\n                        hidden_size=768,\n                        depth=12,\n                        max_seq_len=2050,\n                        num_heads=12,\n                        activation=nn.functional.relu,\n                        is_decoder=True,\n                        fused_qkv=False,\n                        model_name=\"opt\",\n                        disable_past_cache=False,\n                        **kwargs)\n    return create_pipeline_model(**model_kwargs)\n\n\ndef opt_6B(**kwargs):\n    model_kwargs = dict(vocab_size=50272,\n                        hidden_size=4096,\n                        depth=32,\n                        max_seq_len=2050,\n                        num_heads=32,\n                        activation=nn.functional.relu,\n                        is_decoder=True,\n                        fused_qkv=False,\n                        model_name=\"opt\",\n                        disable_past_cache=False,\n                        **kwargs)\n    return create_pipeline_model(**model_kwargs)\n\n\ndef opt_30B(**kwargs):\n    model_kwargs = dict(vocab_size=50272,\n                        hidden_size=7168,\n                        depth=48,\n                        max_seq_len=2050,\n                        num_heads=56,\n                        activation=nn.functional.relu,\n                        is_decoder=True,\n                        fused_qkv=False,\n                        model_name=\"opt\",\n                        disable_past_cache=False,\n                        **kwargs)\n    return create_pipeline_model(**model_kwargs)\n\n\ndef opt_66B(**kwargs):\n    model_kwargs = dict(vocab_size=50272,\n                        hidden_size=9216,\n                        depth=64,\n                        max_seq_len=2050,\n                        num_heads=72,\n                        activation=nn.functional.relu,\n                        is_decoder=True,\n                        fused_qkv=False,\n                        model_name=\"opt\",\n                        disable_past_cache=False,\n                        **kwargs)\n    return create_pipeline_model(**model_kwargs)\n\n\ndef opt_175B(**kwargs):\n    model_kwargs = dict(vocab_size=50272,\n                        hidden_size=12288,\n                        depth=96,\n                        max_seq_len=2050,\n                        num_heads=96,\n                        activation=nn.functional.relu,\n                        is_decoder=True,\n                        fused_qkv=True,\n                        model_name=\"opt-175b\",\n                        disable_past_cache=False,\n                        vocab_parallel=True,\n                        **kwargs)\n    return create_pipeline_model(**model_kwargs)\n"}
{"type": "source_file", "path": "examples/bloom/batch.py", "content": "import torch\nfrom typing import List, Deque, Tuple, Hashable, Any\nfrom energonai import BatchManager, SubmitEntry, TaskEntry\n\n\nclass BatchManagerForGeneration(BatchManager):\n    def __init__(self, max_batch_size: int = 1, pad_token_id: int = 0) -> None:\n        super().__init__()\n        self.max_batch_size = max_batch_size\n        self.pad_token_id = pad_token_id\n\n    def _left_padding(self, batch_inputs):\n        max_len = max(len(inputs['input_ids']) for inputs in batch_inputs)\n        outputs = {'input_ids': [], 'attention_mask': []}\n        for inputs in batch_inputs:\n            input_ids, attention_mask = inputs['input_ids'], inputs['attention_mask']\n            padding_len = max_len - len(input_ids)\n            padding = torch.tensor([self.pad_token_id] * padding_len, device=input_ids.device, dtype=torch.int)\n            input_ids = torch.cat((padding, input_ids), 0)\n\n            padding = torch.tensor([0] * padding_len, device=attention_mask.device, dtype=torch.int)\n            attention_mask = torch.cat((padding, attention_mask), 0)\n            outputs['input_ids'].append(input_ids)\n            outputs['attention_mask'].append(attention_mask)\n        return outputs, max_len\n\n    @staticmethod\n    def _make_batch_key(entry: SubmitEntry) -> tuple:\n        data = entry.data\n        return ()\n\n    def make_batch(self, q: Deque[SubmitEntry]) -> Tuple[TaskEntry, dict]:\n        entry = q.popleft()\n        uids = [entry.uid]\n        batch = [entry.data]\n        while len(batch) < self.max_batch_size:\n            if len(q) == 0:\n                break\n            if self._make_batch_key(entry) != self._make_batch_key(q[0]):\n                break\n            if q[0].data['max_new_tokens'] > entry.data['max_new_tokens']:\n                break\n            e = q.popleft()\n            batch.append(e.data)\n            uids.append(e.uid)\n        inputs, max_len = self._left_padding(batch)\n        trunc_lens = []\n        for data in batch:\n            trunc_lens.append(max_len + data['max_new_tokens'])\n        inputs['max_new_tokens'] = max_len + entry.data['max_new_tokens']\n        return TaskEntry(tuple(uids), inputs), {'trunc_lens': trunc_lens}\n\n    def split_batch(self, task_entry: TaskEntry, trunc_lens: List[int] = []) -> List[Tuple[Hashable, Any]]:\n        retval = []\n        for uid, output, trunc_len in zip(task_entry.uids, task_entry.batch, trunc_lens):\n            retval.append((uid, (output[:trunc_len]).reshape(1, -1)))\n        return retval\n"}
{"type": "source_file", "path": "examples/bloom/benchmark/locustfile.py", "content": "from locust import HttpUser, task\nfrom json import JSONDecodeError\n\n\nclass GenerationUser(HttpUser):\n    @task\n    def generate(self):\n        prompt = 'Question: What is the longest river on the earth? Answer:'\n        for i in range(4, 9):\n            data = {'max_new_tokens': 2**i, 'prompt': prompt}\n            with self.client.post('/generation', json=data, catch_response=True) as response:\n                if response.status_code in (200, 406):\n                    response.success()\n                else:\n                    response.failure('Response wrong')\n"}
{"type": "source_file", "path": "examples/bloom/server.py", "content": "import argparse\nimport logging\nimport json\nimport random\nfrom typing import Optional\nimport torch\nimport uvicorn\nimport colossalai\nfrom colossalai.utils.model.colo_init_context import ColoInitContext\nfrom colossalai.tensor import ShardSpec, ComputeSpec, ComputePattern, ColoParameter, ProcessGroup, ReplicaSpec\n\nfrom energonai import QueueFullError, launch_engine\nfrom fastapi import FastAPI, HTTPException, Request\nfrom pydantic import BaseModel, Field\n\nfrom batch import BatchManagerForGeneration\nfrom cache import ListCache, MissCacheError\nfrom transformers import AutoTokenizer, BloomForCausalLM\nfrom transformers import BloomConfig\n\nTP_TARGET = ['mlp', 'self_attention.dense', 'self_attention.query_key_value', 'word_embeddings.weight']  # 'self_attention.attention_dropout',\n\nclass GenerationTaskReq(BaseModel):\n    max_new_tokens: int = Field(gt=0, le=256, example=64)\n    prompt: str = Field(\n        min_length=1, example='Question: Where were the 2004 Olympics held?\\nAnswer: Athens, Greece\\n\\nQuestion: What is the longest river on the earth?\\nAnswer:')\n    # top_k: Optional[int] = Field(default=None, gt=0, example=50)\n    # top_p: Optional[float] = Field(default=None, gt=0.0, lt=1.0, example=0.5)\n    greedy: Optional[bool] = False\n\n\napp = FastAPI()\n\n\n@app.post('/generation')\nasync def generate(data: GenerationTaskReq, request: Request):\n    logger.info(\n        f'{request.client.host}:{request.client.port} - \"{request.method} {request.url.path}\" - {data}')\n    key = (data.prompt, data.max_new_tokens)\n    try:\n        if cache is None:\n            raise MissCacheError()\n        outputs = cache.get(key)\n        output_str = random.choice(outputs)\n        logger.info('Cache hit')\n    except MissCacheError:\n        input_tokens = tokenizer.encode_plus(data.prompt, return_tensors=\"pt\", padding=True)\n        input_tokens['max_new_tokens'] = data.max_new_tokens\n        try:\n            uid = id(data)\n            engine.submit(uid, input_tokens)\n            outputs = await engine.wait(uid)\n            outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n            if cache is not None:\n                cache.add(key, outputs)\n            output_str = outputs\n        except QueueFullError as e:\n            raise HTTPException(status_code=406, detail=e.args[0])\n    return {'text': output_str}\n\n\n@app.on_event(\"shutdown\")\nasync def shutdown(*_):\n    engine.shutdown()\n    server.should_exit = True\n    server.force_exit = True\n    await server.shutdown()\n\n\ndef print_args(args: argparse.Namespace):\n    print('\\n==> Args:')\n    for k, v in args.__dict__.items():\n        print(f'{k} = {v}')\n\nclass WrapCallModule(torch.nn.Module):\n    def __init__(self, model: torch.nn.Module):\n        super(WrapCallModule, self).__init__()\n        self.model = model\n\n    def forward(self, **generate_kwargs):\n        input_ids_batch = generate_kwargs[\"input_ids\"]\n        attention_mask_batch = generate_kwargs[\"attention_mask\"]\n        generate_kwargs[\"input_ids\"] = torch.cat(input_ids_batch, 0)\n        generate_kwargs[\"attention_mask\"] = torch.cat(attention_mask_batch, 0)\n        return self.model.generate(**generate_kwargs)\n\ndef model_fn(**model_kwargs):\n    from utils import run\n    if model_kwargs['tp']!=1:\n        tp = True\n    else:\n        tp = False\n    if model_kwargs['dtype']==\"int8\":\n        use_int8 = True\n    else:\n        use_int8 = False\n    if model_kwargs['random_init']==False:\n        from_pretrain = True\n    else:\n        from_pretrain = False\n    data_path = model_kwargs['name']\n    size = model_kwargs['size']\n    model = run(tp=tp, from_pretrain=from_pretrain, data_path=data_path, use_int8=use_int8, size=size)\n    return WrapCallModule(model)\n\n\nFIXED_CACHE_KEYS = [\n    ('Question: What is the name of the largest continent on earth?\\nAnswer: Asia\\n\\nQuestion: What is at the center of the solar system?\\nAnswer:', 64),\n    ('A chat between a salesman and a student.\\n\\nSalesman: Hi boy, are you looking for a new phone?\\nStudent: Yes, my phone is not functioning well.\\nSalesman: What is your budget? \\nStudent: I have received my scholarship so I am fine with any phone.\\nSalesman: Great, then perhaps this latest flagship phone is just right for you.', 64),\n    (\"English: I am happy today.\\nChinese: \\n\\nEnglish: I am going to play basketball.\\nChinese: \\n\\nEnglish: Let's celebrate our anniversary.\\nChinese:\", 64)\n]\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--name', type=str, help=\"Name path\", required=True)\n    parser.add_argument('--tp', type=int, default=1)\n    parser.add_argument('--master_host', default='localhost')\n    parser.add_argument('--master_port', type=int, default=19991)\n    parser.add_argument('--rpc_port', type=int, default=19981)\n    parser.add_argument('--max_batch_size', type=int, default=1)\n    parser.add_argument('--pipe_size', type=int, default=1)\n    parser.add_argument('--queue_size', type=int, default=0)\n    parser.add_argument('--http_host', default='0.0.0.0')\n    parser.add_argument('--http_port', type=int, default=7070)\n    parser.add_argument('--cache_size', type=int, default=0)\n    parser.add_argument('--cache_list_size', type=int, default=1)\n    parser.add_argument('--dtype', type=str, help=\"module dtype\", default=\"fp16\", choices=[\"fp16\", \"int8\"])\n    parser.add_argument('--random_init', type=bool, help=\"If have no model params\", default=False)\n    parser.add_argument('--random_model_size', type=str, help=\"size of random init model\", default=\"560m\", choices=[\"560m\", \"7b1\", \"175b\"])\n    args = parser.parse_args()\n    print_args(args)\n\n    num_tokens = 100\n    model_kwargs = dict(max_new_tokens=num_tokens, do_sample=False)\n    model_name = args.name\n    model_kwargs['name'] = model_name\n    model_kwargs['dtype'] = args.dtype\n    model_kwargs['random_init'] = args.random_init\n    model_kwargs['tp'] = args.tp\n    model_kwargs['size'] = args.random_model_size\n    \n    logger = logging.getLogger(__name__)\n\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n\n    if args.cache_size > 0:\n        cache = ListCache(args.cache_size, args.cache_list_size,\n                          fixed_keys=FIXED_CACHE_KEYS)\n    else:\n        cache = None\n    engine = launch_engine(args.tp, 1, args.master_host, args.master_port, args.rpc_port, model_fn,\n                           batch_manager=BatchManagerForGeneration(max_batch_size=args.max_batch_size,\n                                                                   pad_token_id=tokenizer.pad_token_id),\n                           pipe_size=args.pipe_size,\n                           queue_size=args.queue_size,\n                           **model_kwargs)\n    print(\"engine start\")\n    config = uvicorn.Config(app, host=args.http_host, port=args.http_port)\n    server = uvicorn.Server(config=config)\n    server.run()\n"}
{"type": "source_file", "path": "examples/bloom/utils.py", "content": "import torch\nfrom torch import nn, Tensor\nimport torch.distributed as dist\nimport bitsandbytes as bnb\nimport torch.nn.functional as F\nfrom typing import Optional, List\nimport time\nimport datetime\nfrom torch.distributed.distributed_c10d import ReduceOp\nimport copy\nfrom transformers import BloomTokenizerFast, BloomForCausalLM, BloomConfig, AutoModelForCausalLM\n\ndef getModelSize(model):\n    param_size = 0\n    param_sum = 0\n    for param in model.parameters():\n        param_size += param.nelement() * param.element_size()\n        param_sum += param.nelement()\n    buffer_size = 0\n    buffer_sum = 0\n    for buffer in model.buffers():\n        buffer_size += buffer.nelement() * buffer.element_size()\n        buffer_sum += buffer.nelement()\n    all_size = (param_size + buffer_size) / 1024 / 1024\n    print('Model Size: {:.3f}MB'.format(all_size))\n    return (param_size, param_sum, buffer_size, buffer_sum, all_size)\n\nclass Linear8bitTP(nn.Linear):\n    def __init__(\n        self,\n        input_features,\n        output_features,\n        bias=True,\n        has_fp16_weights=False,\n        memory_efficient_backward=False,\n        threshold=6.0,\n        weight_data=None,\n        index=None,\n        bias_data=None\n    ):\n        super(Linear8bitTP, self).__init__(\n            input_features, output_features, bias\n        )\n        self.state = bnb.MatmulLtState()\n        self.index = index\n        self.bias = bias_data\n        self.state.threshold = threshold\n        self.state.has_fp16_weights = has_fp16_weights\n        self.state.memory_efficient_backward = memory_efficient_backward\n        if threshold > 0.0 and not has_fp16_weights:\n            self.state.use_pool = True\n\n        self.rank = dist.get_rank()\n        self.world_size = dist.get_world_size()\n        self.register_parameter(\"SCB\", nn.Parameter(torch.empty(0), requires_grad=False))\n        self.weight = weight_data\n        \n\n    def quant(self):  \n        weight = self.weight.data.contiguous().half().to(self.rank)\n        CB, _, SCB, _, _ = bnb.functional.double_quant(weight)\n        delattr(self, \"weight\")\n        setattr(self, \"weight\", nn.Parameter(CB, requires_grad=False))\n        delattr(self, \"SCB\")\n        setattr(self, \"SCB\", nn.Parameter(SCB, requires_grad=False))\n        del weight\n        self.weight.data = self.weight.data.to(\"cpu\")\n        self.SCB.data = self.SCB.data.to(\"cpu\")\n\n    def forward(self, x):\n        self.state.is_training = self.training\n        \n        if self.bias is not None and self.bias.dtype != torch.float16:\n            self.bias.data = self.bias.data.half()\n        \n        self.state.CB = self.weight.data\n        self.state.SCB = self.SCB.data\n        \n        out = bnb.matmul(x, self.weight, bias=self.bias, state=self.state)\n        tensor_list = [torch.zeros_like(out) for _ in range(self.world_size)]\n        dist.all_gather(tensor_list, out)\n        out = torch.cat(tensor_list, dim=2)\n        del tensor_list\n        if self.state.CxB is not None:\n            del self.state.CxB\n        \n        return out\n\nclass LinearTP(nn.Module):\n    __constants__ = ['in_features', 'out_features']\n    in_features: int\n    out_features: int\n    weight: Tensor\n    \n    def __init__(self, in_features:int, out_features:int, bias:bool=False,\n                 weight_data=None, bias_data=None, device=\"meta\", dtype=None,\n                 use_int8:bool=True\n                 ) -> None:\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super(LinearTP, self).__init__()\n        self.rank = dist.get_rank()\n        self.world_size = dist.get_world_size()\n        self.use_int8 = use_int8\n        if use_int8:\n            self.weight = nn.Parameter(torch.empty((out_features, in_features), **factory_kwargs))\n            if bias:\n                self.bias = nn.Parameter(torch.empty(out_features, **factory_kwargs))\n            else:\n                self.register_parameter('bias', None)\n        else:\n            self.weight = weight_data\n            self.bias = bias_data\n        \n    def forward(self, x):\n        if self.use_int8 == True:\n            x = x.chunk(self.world_size, dim=2)[self.rank]\n            out = F.linear(x, self.weight, self.bias)\n            dist.all_reduce(out, op=ReduceOp.SUM)\n        else:\n            out = F.linear(x, self.weight, self.bias)\n            tensor_list = [torch.zeros_like(out) for _ in range(self.world_size)]\n            dist.all_gather(tensor_list, out)\n            out = torch.cat(tensor_list, dim=2)\n            del tensor_list\n        return out\n\nclass EmbeddingTP(nn.Embedding):\n    def __init__(\n        self,\n        num_embeddings: int,\n        embedding_dim: int,\n        padding_idx: Optional[int] = None,\n        max_norm: Optional[float] = None,\n        norm_type: float = 2.0,\n        scale_grad_by_freq: bool = False,\n        sparse: bool = False,\n        weight: Optional[Tensor] = None,\n    ) -> None:\n        super(EmbeddingTP, self).__init__(\n            num_embeddings,\n            embedding_dim,\n            padding_idx,\n            max_norm,\n            norm_type,\n            scale_grad_by_freq,\n            sparse,\n            weight,\n        )\n        self.rank = dist.get_rank()\n        self.world_size = dist.get_world_size()\n        self.weight = weight\n\n    def forward(self, input: Tensor) -> Tensor:\n        emb = F.embedding(\n            input,\n            self.weight,\n            self.padding_idx,\n            self.max_norm,\n            self.norm_type,\n            self.scale_grad_by_freq,\n            self.sparse,\n        )\n        \n        tensor_list = [torch.zeros_like(emb) for _ in range(self.world_size)]\n        dist.all_gather(tensor_list, emb)\n        emb = torch.cat(tensor_list, dim=2)\n        del tensor_list\n        return emb\n\n@torch.no_grad()\ndef replace_tp_module(model : torch.nn.Module, \n                      threshold : float = 6.0, \n                      modules_to_not_convert : str = \"lm_head\",\n                      use_int8: bool = True\n                      ) -> torch.nn.Module:\n    \"\"\"replace_tp_module \n    Args:\n        model (torch.nn.Module): a meta model\n        threshold (float, optional): _description_. Defaults to 6.0.\n        modules_to_not_convert (str, optional): the model names which shall not be shard+quant. Defaults to \"lm_head\" (for Bloom)\n        use_int8(boll, optional): use int8 quantization. Defaults to True.\n    Returns:\n        torch.nn.Module: the meta model after quantization and tensor parallel sharding\n    \"\"\"\n    for name, module in model.named_children():\n        if len(list(module.children())) > 0:\n            replace_tp_module(module, threshold, modules_to_not_convert, use_int8)\n\n        if isinstance(module, nn.Linear) and name not in modules_to_not_convert:\n            if use_int8 == True:    \n                model._modules[name] = Linear8bitTP(\n                        input_features=module.in_features,\n                        output_features=module.out_features,\n                        threshold=6.0,\n                        weight_data=module.weight,\n                        bias_data=module.bias,\n                )\n\n            else:\n                model._modules[name] = LinearTP(\n                        in_features=module.in_features,\n                        out_features=module.out_features,\n                        bias_data=module.bias,\n                        weight_data=module.weight,\n                        bias = module.bias is not None,\n                        use_int8 = use_int8\n                )\n        \n        elif isinstance(module, nn.Embedding):\n            model._modules[name] = EmbeddingTP(\n                num_embeddings=module.num_embeddings,\n                embedding_dim=module.embedding_dim,\n                padding_idx=module.padding_idx,\n                max_norm=module.max_norm,\n                norm_type=module.norm_type,\n                scale_grad_by_freq=module.scale_grad_by_freq,\n                sparse=module.sparse,\n                weight=module.weight,\n            )\n        \n        elif isinstance(module, nn.Linear) and name == 'lm_head':\n            model._modules[name] = LinearTP(\n                in_features=module.in_features,\n                out_features=module.out_features,\n                bias=module.bias is not None\n                )\n            model._modules[name].weight = model._modules['transformer']._modules['word_embeddings'].weight\n    return model\n\n@torch.no_grad()\ndef get_tp_model(model : nn.Module,\n                 rank : int,\n                 world_size : int, \n                 use_int8 : bool = True)-> torch.nn.Module:\n    \"\"\"get_tp_model\n    Shard a meta model for rank process.\n    Args:\n        model (torch.nn.Module): a meta model to be shard for TP.\n        rank (int): the rank number of this process.\n        world_size (int): the world size\n        use_int8 (bool, optional): use int8 quantization. Defaults to True.\n    Returns:\n        torch.nn.Module: a sharded meta model ready for recieving data from rank0.\n    \"\"\"\n    model = replace_tp_module(model, use_int8=use_int8)\n    for name, module in model.named_modules():\n        if isinstance(module, Linear8bitTP):\n            bias_list = list(module.bias.data.chunk(world_size, dim=0))\n            bias = bias_list[rank]\n            \n            weight_list = list(module.weight.data.chunk(world_size, dim=0))\n            weight = weight_list[rank]\n            SCB = torch.zeros_like(bias).to(\"meta\")\n            \n            delattr(module, \"weight\")\n            setattr(module, \"weight\", nn.Parameter(weight.to(torch.int8), requires_grad=False))\n            \n            delattr(module, \"SCB\")\n            setattr(module, \"SCB\", nn.Parameter(SCB.to(torch.float32), requires_grad=False))\n            \n            delattr(module, \"bias\")\n            setattr(module, \"bias\", nn.Parameter(bias))\n            \n            \n        if isinstance(module, EmbeddingTP):   \n            weight_list = list(module.weight.chunk(world_size, dim=1))\n            delattr(module, 'weight')\n            weight = nn.Parameter(weight_list[rank])\n            setattr(module, 'weight', weight)\n            \n        if isinstance(module, LinearTP):\n            if name == 'lm_head':\n                delattr(module, 'weight')\n                setattr(module, 'weight', model._modules['transformer']._modules['word_embeddings'].weight)\n            else:\n                bias_list = list(module.bias.data.chunk(world_size, dim=0))\n                bias = bias_list[rank]\n            \n                weight_list = list(module.weight.data.chunk(world_size, dim=0))\n                weight = weight_list[rank]\n                \n                delattr(module, \"weight\")\n                setattr(module, \"weight\", nn.Parameter(weight, requires_grad=False))\n            \n                delattr(module, \"bias\")\n                setattr(module, \"bias\", nn.Parameter(bias))\n                \n    return model\n\n@torch.no_grad()\ndef get_tp_model_list(model : torch.nn.Module,\n                      meta_model : torch.nn.Module,\n                      world_size : int,\n                      use_int8 : bool=True\n                     ) -> List[torch.nn.Module]:\n    \"\"\"get_tp_model_list\n    Materizate a `world_size` models for each process.\n    Args:\n        model (torch.nn.Module): a materialized model. It is sacrificed after the function finishes executing.\n        meta_model (torch.nn.Module): a meta model with the same structure as the `model`.\n        world_size (int): the world size\n        use_int8 (bool, optional): use int8 quantization. Defaults to True.\n    Returns:\n        List[torch.nn.Module]: a list of materialized models after sharding and quantization.\n    \"\"\"\n    model = replace_tp_module(model, use_int8=use_int8)\n    \n    model_list = []\n    dist_meta_model = replace_tp_module(meta_model, use_int8=use_int8)\n    for i in range(world_size):\n        model_list.append(copy.deepcopy(dist_meta_model))\n    \n    # quantize and shard parameters\n    for name, module in model.named_modules():\n        if isinstance(module, Linear8bitTP):\n            module.quant()\n            weight_list = list(module.weight.data.chunk(world_size, dim=0))\n            SCB_list = list(module.SCB.data.chunk(world_size, dim=0))\n            bias_list = list(module.bias.data.chunk(world_size, dim=0))\n            \n            name_list = name.split('.')\n            for rank in range(world_size):\n                module_tmp = model_list[rank]._modules[name_list[0]]\n                for i in range(1, len(name_list)):\n                    module_tmp = module_tmp._modules[name_list[i]]\n                delattr(module_tmp, \"weight\")\n                setattr(module_tmp, \"weight\", nn.Parameter(weight_list[rank].clone().detach(), requires_grad=False))\n                delattr(module_tmp, \"SCB\")\n                setattr(module_tmp, \"SCB\", nn.Parameter(SCB_list[rank].clone().detach(), requires_grad=False))\n                delattr(module_tmp, \"bias\")\n                setattr(module_tmp, \"bias\", nn.Parameter(bias_list[rank].clone().detach()))\n            del weight_list, SCB_list, bias_list, name_list\n            module.to(\"meta\")\n\n        elif isinstance(module, EmbeddingTP):\n            weight_list = list(module.weight.chunk(world_size, dim=1))\n            name_list = name.split('.')\n            for rank in range(world_size):\n                module_tmp = model_list[rank]._modules[name_list[0]]\n                for i in range(1, len(name_list)):\n                    module_tmp = module_tmp._modules[name_list[i]]\n                module_tmp.weight = nn.Parameter(weight_list[rank].clone().detach(), requires_grad=False)\n            del name_list, weight_list\n            module.to(\"meta\")\n            \n        elif isinstance(module, LinearTP):\n            if name == 'lm_head':\n                name_list = name.split('.')\n                for rank in range(world_size):\n                    module_tmp = model_list[rank]._modules[name_list[0]]\n                    for i in range(1, len(name_list)):\n                        module_tmp = module_tmp._modules[name_list[i]]\n                    module_tmp.weight = model_list[rank]._modules['transformer']._modules['word_embeddings'].weight\n            else:\n                name_list = name.split('.')\n                weight_list = list(module.weight.data.chunk(world_size, dim=0))\n                bias_list = list(module.bias.data.chunk(world_size, dim=0))\n                for rank in range(world_size):\n                    module_tmp = model_list[rank]._modules[name_list[0]]\n                    for i in range(1, len(name_list)):\n                        module_tmp = module_tmp._modules[name_list[i]]\n                    delattr(module_tmp, \"weight\")\n                    setattr(module_tmp, \"weight\", nn.Parameter(weight_list[rank].clone().detach(), requires_grad=False))\n                    delattr(module_tmp, \"bias\")\n                    setattr(module_tmp, \"bias\", nn.Parameter(bias_list[rank].clone().detach()))\n                del name_list, weight_list, bias_list\n            module.to(\"meta\")\n                \n            \n        elif len(list(module.children())) == 0:\n            name_list = name.split('.')\n            for rank in range(world_size):\n                module_tmp = model_list[rank]._modules[name_list[0]]\n                for i in range(1, len(name_list)):\n                    module_tmp = module_tmp._modules[name_list[i]]\n                try:\n                    module_tmp.weight = module.weight\n                except:\n                    pass\n                try:\n                    module_tmp.bias = module.bias\n                except:\n                    pass\n            module.to(\"meta\")\n    \n    return model_list\n            \n\nfrom contextlib import contextmanager\n@contextmanager\ndef init_empty_weights():\n    old_register_parameter = nn.Module.register_parameter\n    \n    def register_empty_param(module, name, param):\n        old_register_parameter(module, name, param)\n        if param is not None:\n            param_cls = type(module._parameters[name])\n            kwargs = module._parameters[name].__dict__\n            module._parameters[name] = param_cls(module._parameters[name].to(torch.device(\"meta\")), **kwargs)\n            \n    try:\n        nn.Module.register_parameter = register_empty_param\n        yield\n    finally:\n        nn.Module.register_parameter = old_register_parameter\n\n@contextmanager\ndef skip_init_context():\n    old_init = nn.Linear.reset_parameters\n    old_emb_init = nn.Embedding.reset_parameters\n    \n    def new_init(self):\n        pass\n    def new_emb_init(self):\n        self._fill_padding_idx_with_zero()\n        \n    try:\n        nn.Linear.reset_parameters = new_init\n        nn.Embedding.reset_parameters = new_emb_init\n        nn.LayerNorm.reset_parameters = new_init\n        yield\n    finally:\n        nn.Linear.reset_parameters = old_init\n        nn.Embedding.reset_parameters = old_emb_init\n        \n@contextmanager\ndef convert_param_attr_context(dtype=torch.float32, use_skip_init : bool = False):\n    old_register_parameter = nn.Module.register_parameter\n    \n    def register_empty_param(module, name, param):\n        if param is not None:\n            param = nn.Parameter(param.data.to(dtype))\n        old_register_parameter(module, name, param)\n            \n    try:\n        nn.Module.register_parameter = register_empty_param\n        if use_skip_init:\n            with skip_init_context():\n                yield\n        else:\n            yield\n    finally:\n        nn.Module.register_parameter = old_register_parameter\n\n\nclass ModelScatter(object):\n    def __init__(self) -> None:\n        self.cpu_group = dist.new_group(backend='gloo', timeout=datetime.timedelta(seconds=18000))\n        self.rank = dist.get_rank()\n        self.world_size = dist.get_world_size()\n\n    def _add_param(self, model, param_tensor, name):\n        param = torch.nn.Parameter(param_tensor, requires_grad=False)\n        name_list = name.split('.')\n        module = model._modules[name_list[0]]\n        for i in range(1, len(name_list) - 1):\n            module = module._modules[name_list[i]]\n        module._parameters[name_list[-1]] = param.to(self.rank)\n        del param_tensor\n\n    def scatter_model(self, src_model : torch.nn.Module, target_model : torch.nn.Module, use_int8 : bool = True) -> torch.nn.Module:\n        \"\"\"scatter_model\n\n        Args:\n            src_model (torch.nn.Module): a global materailized model\n            target_model (torch.nn.Module): a meta model with the same structure as `src_model`\n            use_int8(bool): use int8 quantization. Defaults to True\n        \n        Returns:\n            torch.nn.Module: a local materailized model\n        \"\"\"\n        if self.rank == 0:\n            assert src_model.dtype == target_model.dtype, f\"the src model and the target model should have the same dtype\"\n            assert src_model.device.type == 'cpu'\n            assert target_model.device.type == 'meta'\n\n            # get quant & sharded model_list\n            time0 = time.time()\n            model_list = get_tp_model_list(src_model, target_model, self.world_size, use_int8=use_int8)\n            print(\"Model init complete\", time.time() - time0)\n\n            dist.barrier(self.cpu_group)\n            # send out\n            for name, param in model_list[0].named_parameters():\n                param_list = [param.data]\n                for i in range(1, self.world_size):\n                    param_list.append(model_list[i].state_dict()[name])\n                param_tensor = torch.zeros_like(\n                    param_list[0], dtype=param_list[0].dtype)\n                dist.scatter(param_tensor, scatter_list=param_list,\n                                src=0, group=self.cpu_group)\n                del param_list, param_tensor\n            model = model_list[0]\n            del model_list\n            return model\n        else:\n            model = get_tp_model(target_model, self.rank, self.world_size, use_int8=use_int8)\n            dist.barrier(self.cpu_group)\n            for name, param in model.named_parameters():\n                param_tensor = torch.zeros(\n                    param.data.size(), dtype=param.dtype)\n                dist.scatter(param_tensor, src=0, group=self.cpu_group)\n                self._add_param(model, param_tensor, name)\n            return model\n\ndef run_int8_bloom_inference(use_int8=True, from_pretrain=False, data_path=None, size=\"560m\"):\n    rank = dist.get_rank()\n    world_size = dist.get_world_size()\n\n    model_scatter = ModelScatter()\n\n    if from_pretrain:\n        configuration = BloomConfig.from_json_file(data_path + '/config.json')\n    else:\n        if size == \"175b\":\n            configuration = BloomConfig(\n                hidden_size=14336,\n                n_layer=70,\n                n_head=112,)\n        elif size == \"7b1\":\n            configuration = BloomConfig(\n                hidden_size=4096,\n                n_layer=30,\n                n_head=32,)\n        elif size == \"560m\":\n            configuration = BloomConfig(\n                hidden_size=1024,\n                n_layer=24,\n                n_head=16,)\n        \n    # meta init\n    # get meta_model\n    with init_empty_weights():\n        meta_model = AutoModelForCausalLM.from_config(configuration).half()\n    if rank == 0:           \n        # get pre_trained model\n        if from_pretrain:\n            src_model = AutoModelForCausalLM.from_pretrained(\n                data_path, low_cpu_mem_usage=True, torch_dtype=torch.float16)\n        else:\n            with convert_param_attr_context(dtype=torch.float16, use_skip_init=True):\n                src_model = AutoModelForCausalLM.from_config(configuration)\n        print(\"src_model_load_complete\")\n            \n        model = model_scatter.scatter_model(src_model, meta_model, use_int8)\n\n    else:\n        model = model_scatter.scatter_model(None, meta_model, use_int8)\n        model._modules['lm_head']._parameters['weight'] = model._modules['transformer']._modules['word_embeddings'].weight\n    \n    getModelSize(model)\n    return model\n\ndef run_fp16(from_pretrain=False, data_path=None):\n    if from_pretrain:\n        model = BloomForCausalLM.from_pretrained(\n            data_path, low_cpu_mem_usage=True).half().to(0)\n    else:\n        cfg = BloomConfig(\n            hidden_size=14336,\n            n_layer=70,\n            n_head=112,)\n        with convert_param_attr_context(dtype=torch.float16, use_skip_init=True):\n            model = BloomForCausalLM(cfg)\n    \n    return model\n\ndef run(tp=True, from_pretrain=False, data_path=None, use_int8=True, size=\"560m\"):\n    if tp:\n        model = run_int8_bloom_inference(from_pretrain=from_pretrain, data_path=data_path, use_int8=use_int8, size=size)\n    else:\n        model = run_fp16(from_pretrain, data_path)\n    return model\n"}
{"type": "source_file", "path": "examples/bert/bert_server.py", "content": "import os\nimport torch\nimport uvicorn\nfrom fastapi import FastAPI\nfrom fastapi import Response\nimport torch.distributed.rpc as rpc\nfrom energonai.engine import InferenceEngine\n\napp = FastAPI() #  api \n\n@app.get(\"/\") # \ndef root():\n    return {\"200\"}\n\n@app.get(\"/model_with_padding\")\ndef run():\n    # for the performance only\n    seq_len = 512\n    batch_size = 32\n\n    input_ids = torch.randint(1, 10, (batch_size, seq_len), dtype=torch.int64)\n    attention_mask = torch.randint(0, 1, (batch_size, 1, seq_len), dtype=torch.int64)\n    # seq_lens = torch.randint(1, 128, (batch_size, ), dtype=torch.int64) # generate seq_lens randomly\n    hidden_states = None\n    sample = dict(hidden_states=hidden_states, input_ids=input_ids, attention_mask=attention_mask)\n\n    output = engine.run(sample)\n    output = output.to_here()\n    print(output)\n    return {\"To return the string result.\"}\n\n@app.get(\"/model_rm_padding\")\ndef run():\n    # for the performance only\n    seq_len = 512\n    batch_size = 32\n\n    input_ids = torch.randint(1, 10, (batch_size, seq_len), dtype=torch.int64)\n    attention_mask = torch.randint(0, 1, (batch_size, 1, seq_len), dtype=torch.int64)\n    seq_lens = torch.randint(1, 128, (batch_size, ), dtype=torch.int) # generate seq_lens randomly\n    hidden_states = None\n    sample = dict(hidden_states=hidden_states, input_ids=input_ids, attention_mask=attention_mask, seq_lens=seq_lens)\n\n    output = engine.run(sample)\n    output = output.to_here()\n    print(output)\n    return {\"To return the string result.\"}\n    \n\n@app.get(\"/shutdown\")\nasync def shutdown():\n    engine.clear()\n    server.should_exit = True\n    server.force_exit = True\n    await server.shutdown()\n\n\ndef launch_engine(model_class,\n                model_type,\n                max_batch_size: int = 1,\n                tp_init_size: int = -1,\n                pp_init_size: int = -1,\n                host: str = \"localhost\",\n                port: int = 29500,\n                dtype = torch.float,\n                checkpoint: str = None,\n                tokenizer_path: str = None,\n                server_host = \"localhost\",\n                server_port = 8005,\n                log_level = \"info\"\n                ):\n    \n    if checkpoint:\n        model_config = {'dtype': dtype, 'checkpoint': True, 'checkpoint_path': checkpoint}\n    else:\n        model_config = {'dtype': dtype}\n\n    global engine\n    engine = InferenceEngine(model_class, \n                            model_config,\n                            model_type,\n                            max_batch_size = max_batch_size, \n                            tp_init_size = tp_init_size, \n                            pp_init_size = pp_init_size, \n                            host = host,\n                            port = port,\n                            dtype = dtype)\n\n    global server\n    config = uvicorn.Config(app, host=server_host, port=server_port, log_level=log_level)\n    server = uvicorn.Server(config=config)\n    server.run()"}
{"type": "source_file", "path": "examples/bloom/cache.py", "content": "from collections import OrderedDict\nfrom threading import Lock\nfrom contextlib import contextmanager\nfrom typing import List, Any, Hashable, Dict\n\n\nclass MissCacheError(Exception):\n    pass\n\n\nclass ListCache:\n    def __init__(self, cache_size: int, list_size: int, fixed_keys: List[Hashable] = []) -> None:\n        \"\"\"Cache a list of values. The fixed keys won't be removed. For other keys, LRU is applied.\n        When the value list is not full, a cache miss occurs. Otherwise, a cache hit occurs. Redundant values will be removed.\n\n        Args:\n            cache_size (int): Max size for LRU cache.\n            list_size (int): Value list size.\n            fixed_keys (List[Hashable], optional): The keys which won't be removed. Defaults to [].\n        \"\"\"\n        self.cache_size = cache_size\n        self.list_size = list_size\n        self.cache: OrderedDict[Hashable, List[Any]] = OrderedDict()\n        self.fixed_cache: Dict[Hashable, List[Any]] = {}\n        for key in fixed_keys:\n            self.fixed_cache[key] = []\n        self._lock = Lock()\n\n    def get(self, key: Hashable) -> List[Any]:\n        with self.lock():\n            if key in self.fixed_cache:\n                l = self.fixed_cache[key]\n                if len(l) >= self.list_size:\n                    return l\n            elif key in self.cache:\n                self.cache.move_to_end(key)\n                l = self.cache[key]\n                if len(l) >= self.list_size:\n                    return l\n        raise MissCacheError()\n\n    def add(self, key: Hashable, value: Any) -> None:\n        with self.lock():\n            if key in self.fixed_cache:\n                l = self.fixed_cache[key]\n                if len(l) < self.list_size and value not in l:\n                    l.append(value)\n            elif key in self.cache:\n                self.cache.move_to_end(key)\n                l = self.cache[key]\n                if len(l) < self.list_size and value not in l:\n                    l.append(value)\n            else:\n                if len(self.cache) >= self.cache_size:\n                    self.cache.popitem(last=False)\n                self.cache[key] = [value]\n\n    @contextmanager\n    def lock(self):\n        try:\n            self._lock.acquire()\n            yield\n        finally:\n            self._lock.release()\n"}
{"type": "source_file", "path": "energonai/pipelinable/split_policy.py", "content": "import functools\nfrom torch.fx.node import Node\nfrom energonai.context import MEATCONFIG\n\n\npartition_counter_0 = 0\n\n# partition_nums: nums of each submodule\ndef _naive_equal_partition(node: Node, partition_nums):\n    global partition_counter_0\n    partition = partition_counter_0 // partition_nums\n    partition_counter_0 = partition_counter_0 + 1\n    return partition\n\ndef naive_equal_partition(partition_nums):\n    mod_partition = functools.partial(_naive_equal_partition, partition_nums = partition_nums)\n    return mod_partition\n\npartition_counter_1 = 0\n\n# partition_nums: nums of each submodule\ndef _module_equal_partition(node: Node, partition_nums):\n    global partition_counter_1\n    partition = partition_counter_1 // partition_nums\n    if node.op == 'call_module':\n        partition_counter_1 = partition_counter_1 + 1\n    return partition\n\ndef module_equal_partition(partition_nums):\n    mod_partition = functools.partial(_module_equal_partition, partition_nums = partition_nums)\n    return mod_partition\n\n\n\nfrom colossalai.core import global_context as gpc\nfrom colossalai.context import ParallelMode\npartition_counter_2 = -1 # for embedding layer\n# partition_nums: nums of each submodule\ndef _transformer_partition(node: Node, depth):\n    global partition_counter_2\n    assert gpc.is_initialized(ParallelMode.PIPELINE), \"Pipeline communication group should be initialized!\"\n    \n    pipeline_size = gpc.get_world_size(ParallelMode.PIPELINE)\n    partition_nums = depth // pipeline_size    \n    partition = abs(partition_counter_2) // partition_nums\n    if node.op == 'call_module':\n        partition_counter_2 = partition_counter_2 + 1\n    return partition\n\ndef transformer_partition(depth):\n    mod_partition = functools.partial(_transformer_partition, depth = depth)\n    return mod_partition"}
