{"repo_info": {"repo_name": "SmartPlay", "repo_owner": "microsoft", "repo_url": "https://github.com/microsoft/SmartPlay"}}
{"type": "source_file", "path": "examples/experiment.py", "content": "import os\nos.environ[\"MINEDOJO_HEADLESS\"]=\"1\"\nimport argparse\nimport numpy as np\nfrom tqdm import tqdm\nimport gym\nimport smartplay\n\nparser = argparse.ArgumentParser()\nparser.add_argument('--llm_name', type=str, default='gpt-4', help='Name of the LLM')\nparser.add_argument('--env_names', type=str, default=None, help='Comma separated list of environments to run')\n\nargs = parser.parse_args()\n\nif args.env_names is None:\n    args.env_names = ','.join(smartplay.benchmark_games_v0)\n\nLLM_name = args.llm_name\n\n# Replace with your own LLM API.\n# Note: query_model takes two arguments: 1) message in openai chat completion form (list of dictionaries), \n#                                        2) an index to indicate where the message should be truncated if the length exceeds LLM context length.\nfrom llm_api import get_query\nquery_model = get_query(LLM_name)\n\ndef compose_ingame_prompt(info, question, past_qa=[]):\n    messages = [\n        {\"role\": \"system\", \"content\" : \"Youâ€™re a player trying to play the game.\"}\n    ]\n    \n    if len(info['manual'])>0:\n        messages.append({\"role\": \"system\", \"content\": info['manual']})\n\n    if len(info['history'])>0:\n        messages.append({\"role\": \"system\", \"content\": info['history']})\n\n    messages.append({\"role\": \"system\", \"content\": \"current step observation: {}\".format(info['obs'])})\n\n    if len(past_qa)>0:\n        for q,a in past_qa:\n            messages.append({\"role\": \"user\", \"content\": q})\n            messages.append({\"role\": \"assistant\", \"content\": a})\n\n    messages.append({\"role\": \"user\", \"content\": question})\n\n    return messages, 2 # This is the index of the history, we will truncate the history if it is too long for LLM\n\nquestions=[\n        \"What is the best action to take? Let's think step by step, \",\n        \"Choose the best executable action from the list of all actions. Write the exact chosen action.\"\n    ]\n\ndef run(env_name):\n    normalized_scores = []\n    env = gym.make(\"smartplay:{}-v0\".format(env_name))\n    env_steps = env.default_steps\n    num_iter = env.default_iter\n\n    def match_act(output):\n        inds = [(i, output.lower().index(act.lower())) for i, act in enumerate(env.action_list) if act.lower() in output.lower()]\n        if len(inds)>0:\n            # return the action with smallest index\n            return sorted(inds, key=lambda x:x[1])[0][0]\n        else:\n            # print(\"LLM failed with output \\\"{}\\\", taking action 0...\".format(output))\n            return 0\n\n    rewards = []\n    progresses = []\n    for eps in tqdm(range(num_iter), desc=\"Evaluating LLM {} on {}\".format(LLM_name, env_name)):\n        import wandb\n        wandb.init(project=\"SmartPlay\", config={\"LLM\": LLM_name, \"env\": env_name, \"eps\": eps, \"num_iter\": num_iter, \"env_steps\": env_steps})\n        step = 0\n        trajectories = []\n        qa_history = []\n        progress = [0]\n        reward = 0\n        rewards = []\n        done=False\n\n        columns=[\"Context\", \"Step\", \"OBS\", \"History\", \"Score\", \"Reward\", \"Total Reward\"] + questions + [\"Action\"]\n        wandb_table = wandb.Table(columns=columns)\n\n        _, info = env.reset()\n        \n        while step < env_steps:\n\n            new_row = [info['manual'], step, info['obs'], info['history'], info['score'], reward, sum(rewards)]\n            wandb.log({\"metric/total_reward\".format(eps): sum(rewards), \n                       \"metric/score\".format(eps): info['score'],\n                       \"metric/reward\".format(eps): reward,\n                       })\n            \n            if done:\n                break\n            \n            qa_history = []\n            for question in questions:\n                prompt = compose_ingame_prompt(info, question, qa_history)\n                answer = query_model(*prompt)\n                qa_history.append((question, answer))\n                new_row.append(answer)\n                answer_act = answer\n\n            a = match_act(answer_act)\n            new_row.append(env.action_list[a])\n            _, reward, done, info = env.step(a)\n            rewards.append(reward)\n            score=info['score']\n\n            step += 1\n            wandb_table.add_data(*new_row)\n\n        if not done:\n            completion=0\n        else:\n            completion=info['completed']\n        progresses.append(np.max(progress))\n        wandb.log({\"rollout/rollout\".format(eps): wandb_table, \n                \"final/total_reward\":sum(rewards),\n                \"final/score\":score,\n                \"final/normalized_score\":smartplay.normalize_score(env_name, score),\n                \"final/completion\":completion,\n                \"final/episodic_step\":step,\n                \"final/eps\":eps,\n                })\n        normalized_scores.append(smartplay.normalize_score(env_name, score))\n        del wandb_table\n        wandb.finish()\n    return np.average(normalized_scores)\n\nscore_dict = {}\nfor env_name in args.env_names.split(','):\n    score_dict[env_name] = run(env_name)\n\nprint(\"Normalized scores on each task:\", score_dict)\nprint(\"Capability scores of the LLM:\", smartplay.analyze_capabilities(score_dict))\n"}
{"type": "source_file", "path": "src/smartplay/crafter/__init__.py", "content": "from gym.envs.registration import register\n\nfrom .crafter_env import Crafter\n\nenvironments = [\n    ['Crafter', 'v0'],\n]\n\nfor environment in environments:\n    register(\n        id='{}-{}'.format(environment[0], environment[1]),\n        entry_point='smartplay.crafter:{}'.format(environment[0]),\n        kwargs={'reward': True}\n    )\n"}
{"type": "source_file", "path": "src/messenger/envs/__init__.py", "content": "from messenger.envs.wrappers import TwoEnvWrapper\nfrom messenger.envs.stage_one import StageOne\nfrom messenger.envs.stage_two import StageTwo\nfrom messenger.envs.stage_three import StageThree"}
{"type": "source_file", "path": "setup.py", "content": "import setuptools\nimport pathlib\n\n\nsetuptools.setup(\n    name='smartplay',\n    version='0.0.1',\n    description='A benchmarking tool for LLMs with games',\n    url='',\n    long_description=pathlib.Path('README.md').read_text(),\n    long_description_content_type='text/markdown',\n    packages=setuptools.find_namespace_packages('src'),\n    package_dir={'': 'src'},\n    # package_data={'crafter': ['data.yaml', 'assets/*']},\n    entry_points={'console_scripts': ['smartplay=smartplay.run_gui:main']},\n    install_requires=[\n        'numpy', \n        'pandas',\n        'pygame', \n        'connected-components-3d', \n        'gym', \n        'minedojo', \n        'imageio', \n        'pillow', \n        'opensimplex', \n        'ruamel.yaml<0.18', # safe_load removed in 0.18\n        'importlib-metadata==6.6.0',\n        'importlib-resources==5.12.0',\n        'vgdl @ git+https://github.com/ahjwang/py-vgdl',\n        'setuptools', # pkg-resources required by vgdl\n    ],\n    include_package_data=True,\n    classifiers=[\n        'Intended Audience :: Science/Research',\n        'License :: OSI Approved :: MIT License',\n        'Programming Language :: Python :: 3',\n        'Topic :: Games/Entertainment',\n        'Topic :: Scientific/Engineering :: Artificial Intelligence',\n    ],\n)\n"}
{"type": "source_file", "path": "src/messenger/envs/config.py", "content": "'''\nDefault config settings used in the envs. Changing the config settings here\nwill have a global effect on the entire messenger package.\n'''\n\nimport itertools\nfrom pathlib import Path\nimport json\nfrom collections import namedtuple\n\n'''\nAn entity namedtuple consists of the entity's name (e.g. alien) and the\nid (symbol) of the entity used to represent the entity in the state\n'''\nEntity = namedtuple(\"Entity\", \"name id\")\n\n'''\nA Game namedtuple specifies an assignment of three Entity namedtuples to the roles\nenemy, message, goal.\n'''\nGame = namedtuple(\"Game\", \"enemy message goal\")\n\n'''\nACTIONS (user set):\n    Maps actions to ints that is consistent with those used in PyVGDL\nSTATE_HEIGHT, STATE_WIDTH (user set):\n    Dimensions of the game state. Note that these must be consistent with the level\n    files that PyVGDL reads.\n_all_npcs (user set):\n    List of all the npcs (non-player characters) in the game. Aside from the obvious \n    effects of changing the entities themselves, the order of things here are also \n    important as it determines the embedding ID.\nNPCS: \n    List of Entity namedtuples consisting non-player characters (e.g. alien, bear)\nALL_ENTITIES:\n    List of all Entity namedtuples. Includes NPCS, avatar, and walls.\nWITH_MESSAGE:\n    The avatar Entity when it has the message.\nNO_MESSAGE:\n    The avatar Entity without the message.\nWALL:\n    The wall entity.\n'''\n\nACTIONS = namedtuple('Actions', 'up down left right stay')(0, 1, 2, 3, 4)\nSTATE_HEIGHT, STATE_WIDTH = 10, 10 # dimensions of the game state.\n\n_all_npcs = ['airplane', 'mage', 'dog', 'bird', 'fish', 'scientist', 'thief',\n    'ship', 'ball', 'robot', 'queen', 'sword']\n\nNPCS = [] # non-player characters\nALL_ENTITIES = [] # all entities including wall and avatar\nWITH_MESSAGE = None # avatar with the message\nNO_MESSAGE = None # avatar without the message\nWALL = None # the wall\n\n# Fill in the entities\nfor i, name in enumerate(_all_npcs + ['wall', 'no_message', 'with_message']):\n    ALL_ENTITIES.append(Entity(name, i + 2)) # 0, 1 reserved for background and dirt resp.\n    \n    if name not in ['wall', 'no_message', 'with_message']:\n        NPCS.append(Entity(name, i + 2))\n\n    # we also want to be able to specifically reference the non-NPC entities\n    if name == 'with_message': \n        WITH_MESSAGE = Entity(name, i + 2)\n    if name == 'no_message':\n        NO_MESSAGE = Entity(name, i + 2)\n    if name == 'wall':\n        WALL = Entity(name, i + 2)"}
{"type": "source_file", "path": "src/messenger/envs/base.py", "content": "import random\nfrom collections import namedtuple\n\nimport gym\nfrom gym import spaces\nimport numpy as np\n\nimport messenger.envs.config as config\nfrom messenger.envs.config import Entity\n\n# Positions of the entities\nPosition = namedtuple('Position', [\"x\", \"y\"])\n\n\nclass MessengerEnv(gym.Env):\n    '''\n    Base Messenger class that defines the action and observation spaces.\n    '''\n\n    def __init__(self):\n        super().__init__()\n        # up, down, left, right, stay\n        self.action_space = spaces.Discrete(len(config.ACTIONS))\n\n        # observations, not including the text manual\n        self.observation_space = spaces.Dict({\n            \"entities\": spaces.Box(\n                low=0,\n                high=14,\n                shape=(config.STATE_HEIGHT, config.STATE_WIDTH, 3)\n            ),\n            \"avatar\": spaces.Box(\n                low=15,\n                high=16,\n                shape=(config.STATE_HEIGHT, config.STATE_WIDTH, 1)\n            )\n        })\n\n    def reset(self):\n        raise NotImplementedError\n\n    def step(self):\n        raise NotImplementedError\n\n    def render(self):\n        raise NotImplementedError\n\n\nclass Grid:\n    '''\n    Class which makes it easier to build a grid observation from the dict state\n    return by VGDLEnv.\n    '''\n    def __init__(self, layers, shuffle=True):\n        '''\n        layers:\n            Each add() operation will place a separate entity in a new layer.\n            Thus, this is the upper-limit to the number of items to be added.\n        shuffle:\n            Place each items in a random order.\n        '''\n        self.grid = np.zeros((config.STATE_HEIGHT, config.STATE_WIDTH, layers))\n        self.order = list(range(layers)) # insertion order\n        if shuffle:\n            random.shuffle(self.order)\n        self.layers = layers\n        self.entity_count = 0\n\n    def add(self, entity:Entity, position:Position):\n        '''\n        Add entity entity and position position.\n        '''\n        assert self.entity_count < self.layers, \\\n            f\"Tried to add entity no. {self.entity_count} with {self.layers} layers.\"\n\n        self.grid[position.y, position.x, self.order[self.entity_count]] = entity.id\n        self.entity_count += 1"}
{"type": "source_file", "path": "src/smartplay/bandits/bandit.py", "content": "import numpy as np\nimport gym\nfrom gym import spaces\nfrom gym.utils import seeding\nfrom ..utils import HistoryTracker, describe_act\nimport random\n\nclass BanditEnv(gym.Env):\n    \"\"\"\n    Bandit environment base to allow agents to interact with the class n-armed bandit\n    in different variations\n\n    p_dist:\n        A list of probabilities of the likelihood that a particular bandit will pay out\n    r_dist:\n        A list of either rewards (if number) or means and standard deviations (if list)\n        of the payout that bandit has\n    \"\"\"\n\n    default_iter = 20\n    default_steps = 50\n\n    def __init__(self, p_dist, r_dist, max_steps=50):\n        if len(p_dist) != len(r_dist):\n            raise ValueError(\"Probability and Reward distribution must be the same length\")\n\n        if min(p_dist) < 0 or max(p_dist) > 1:\n            raise ValueError(\"All probabilities must be between 0 and 1\")\n\n        for reward in r_dist:\n            if isinstance(reward, list) and reward[1] <= 0:\n                raise ValueError(\"Standard deviation in rewards must all be greater than 0\")\n\n        self.n_bandits = len(p_dist)\n        self.action_space = spaces.Discrete(self.n_bandits)\n        self.action_list = [\"Pull slot machine {}.\".format(i+1) for i in range(self.n_bandits)]\n        self.observation_space = spaces.Discrete(1)\n        self.p_dist = p_dist\n        self.r_dist = r_dist\n        self.desc = \"\"\"\nYou are in the casino with 2 slot machines in front of you. Your goal is to try to earn the most from those slot machines.\n\n{}\n\"\"\".format(describe_act(self.action_list)).strip()\n        self.history = HistoryTracker(max_steps)\n\n        self._seed()\n        self.score_tracker = 0\n\n    def _seed(self, seed=None):\n        self.np_random, seed = seeding.np_random(seed)\n        return [seed]\n\n    def step(self, action):\n        assert self.action_space.contains(action)\n\n        reward = -1\n        done = False\n\n        if np.random.uniform() < self.p_dist[action]:\n            if not isinstance(self.r_dist[action], list):\n                reward = self.r_dist[action]\n            else:\n                reward = np.random.normal(self.r_dist[action][0], self.r_dist[action][1])\n\n        self.score_tracker+=int(action == self.optimal)\n        info = {\"obs\": \"You pulled slot machine {}, you received reward {}.\".format(action+1, reward),\n                \"score\": self.score_tracker,\n                \"manual\": self.desc,\n                \"history\": self.history.describe(),\n                \"optimal\":self.optimal,\n                \"completed\": 0,\n                }\n        self.history.step(info)\n\n        return 0, reward, done, info\n\n    def reset(self):\n        idx_list = list(range(len(self.p_dist)))\n        random.shuffle(idx_list)\n\n        self.score_tracker=0\n        self.p_dist = [self.p_dist[i] for i in idx_list]\n        self.r_dist = [self.r_dist[i] for i in idx_list]\n        self.ev = [p*r - (1-p) for p, r in zip(self.p_dist, self.r_dist)]\n        self.optimal = np.argmax(self.ev)\n        self.history.reset()\n        info = {\"obs\": \"A new round begins.\",\n                \"score\": self.score_tracker,\n                \"manual\": self.desc,\n                \"history\": self.history.describe(),\n                \"optimal\":self.optimal,\n                \"completed\": 0,\n                }\n        self.history.step(info)\n        return 0, info\n\n    def render(self, mode='human', close=False):\n        pass\n\n\nclass BanditTwoArmedDeterministicFixed(BanditEnv):\n    \"\"\"Simplest case where one bandit always pays, and the other always doesn't\"\"\"\n    def __init__(self):\n        BanditEnv.__init__(self, p_dist=[1, 0], r_dist=[1, 1])\n\n\nclass BanditTwoArmedHighLowFixed(BanditEnv):\n    \"\"\"Stochastic version with a large difference between which bandit pays out of two choices\"\"\"\n    def __init__(self):\n        BanditEnv.__init__(self, p_dist=[0.8, 0.2], r_dist=[1, 1])\n\n\nclass BanditTwoArmedHighHighFixed(BanditEnv):\n    \"\"\"Stochastic version with a small difference between which bandit pays where both are good\"\"\"\n    def __init__(self):\n        BanditEnv.__init__(self, p_dist=[0.8, 0.9], r_dist=[1, 1])\n\n\nclass BanditTwoArmedLowLowFixed(BanditEnv):\n    \"\"\"Stochastic version with a small difference between which bandit pays where both are bad\"\"\"\n    def __init__(self):\n        BanditEnv.__init__(self, p_dist=[0.1, 0.2], r_dist=[1, 1])\n"}
{"type": "source_file", "path": "src/messenger/envs/stage_three.py", "content": "'''\nClasses that follows a gym-like interface and implements stage three of the Messenger\nenvironment.\n'''\n\nimport json\nimport random\nfrom collections import namedtuple\nfrom pathlib import Path\nfrom os import environ\nimport re\n\n# hack to stop PyGame from printing to stdout\nenviron[\"PYGAME_HIDE_SUPPORT_PROMPT\"] = \"hide\"\n\nimport numpy as np\nfrom vgdl.interfaces.gym import VGDLEnv\n\nfrom messenger.envs.base import MessengerEnv, Grid, Position\nimport messenger.envs.config as config\nfrom messenger.envs.manual import TextManual, Descr\nfrom messenger.envs.utils import games_from_json\n\n\n# specifies the game variant path is path to the vgdl domain file describing the variant.\nGameVariant = namedtuple(\n    \"GameVariant\",\n    [\n        \"path\",\n        \"enemy_type\",\n        \"message_type\",\n        \"goal_type\",\n        \"decoy_message_type\",\n        \"decoy_goal_type\"\n    ]\n)\n\nclass StageThree(MessengerEnv):\n    '''\n    Similar to stage two Messenger, except with decoy objects that require\n    disambiduation (e.g. chasing knight, vs immovable knight)\n    '''\n\n    def __init__(self, split:str, shuffle_obs=True):\n        super().__init__()\n        self.shuffle_obs = shuffle_obs # shuffle the entity layers\n\n        this_folder = Path(__file__).parent\n        # Get the games and manual\n        games_json_path = this_folder.joinpath(\"games.json\")\n        if \"train\" in split and \"mc\" in split: # multi-combination games\n            game_split = \"train_multi_comb\"\n            text_json_path = this_folder.joinpath(\"texts\", \"text_train.json\")\n        elif \"train\" in split and \"sc\" in split: # single-combination games\n            game_split = \"train_single_comb\"\n            text_json_path = this_folder.joinpath(\"texts\", \"text_train.json\")\n        elif \"val\" in split:\n            game_split = \"val\"\n            text_json_path = this_folder.joinpath(\"texts\", \"text_val.json\")\n        elif \"test\" in split:\n            game_split = \"test\"\n            text_json_path = this_folder.joinpath(\"texts\", \"text_test.json\")\n        else:\n            raise Exception(f\"Split: {split} not understood.\")\n\n        # list of Game namedtuples\n        self.all_games = games_from_json(json_path=games_json_path, split=game_split)\n        self.text_manual = TextManual(json_path=text_json_path)\n\n        vgdl_files = this_folder.joinpath(\"vgdl_files\", \"stage_3\")\n        \n        # get the file paths to possible starting states\n        self.init_states = [\n            str(path) for path in vgdl_files.joinpath(\"init_states\").glob(\"*.txt\")\n        ]\n        # get all the game variants\n        self.game_variants = [\n            self._get_variant(path) for path in vgdl_files.joinpath(\"variants\").glob(\"*.txt\")\n        ]\n\n        # entities tracked by VGDLEnv\n        self.notable_sprites = [\"enemy\", \"message\", \"goal\", \"decoy_message\", \"decoy_goal\", \"no_message\", \"with_message\"]\n        self.env = None # the VGDLEnv\n\n    def _get_variant(self, variant_file:Path) -> GameVariant:\n        '''\n        Return the GameVariant for the variant specified by variant_file. \n        Searches through the vgdl code to find the correct type:\n        {chaser, fleeing, immovable}\n        '''\n\n        code = variant_file.read_text()\n        return GameVariant(\n            path = str(variant_file),\n            enemy_type = re.search(r'enemy > (\\S+)', code)[1].lower(),\n            message_type = re.search(r'message > (\\S+)', code)[1].lower(),\n            goal_type = re.search(r'goal > (\\S+)', code)[1].lower(),\n            decoy_message_type = re.search(r'decoy_message > (\\S+)', code)[1].lower(),\n            decoy_goal_type = re.search(r'decoy_goal > (\\S+)', code)[1].lower()\n        )\n\n    def _convert_obs(self, vgdl_obs):\n        '''\n        Return a grid built from the vgdl observation which is a\n        KeyValueObservation object (see vgdl code for details).\n        '''\n        entity_locs = Grid(layers=5, shuffle=self.shuffle_obs)\n        avatar_locs = Grid(layers=1)\n\n        # try to add each entity one by one, if it's not there move on.\n        if 'enemy.1' in vgdl_obs:\n            entity_locs.add(self.game.enemy, Position(*vgdl_obs['enemy.1']['position']))\n        if 'message.1' in vgdl_obs:\n            entity_locs.add(self.game.message, Position(*vgdl_obs['message.1']['position']))\n        else:\n            # advance the entity counter, Oracle model requires special order.\n            # TODO: maybe used named layers to make this more understandable.\n            entity_locs.entity_count += 1\n        if 'goal.1' in vgdl_obs:\n            entity_locs.add(self.game.goal, Position(*vgdl_obs['goal.1']['position']))\n        \n        if 'decoy_message.1' in vgdl_obs:\n            entity_locs.add(self.game.message, Position(*vgdl_obs['decoy_message.1']['position']))\n        if 'decoy_goal.1' in vgdl_obs:\n            entity_locs.add(self.game.goal, Position(*vgdl_obs['decoy_goal.1']['position']))\n\n        if 'no_message.1' in vgdl_obs:\n            '''\n            Due to a quirk in VGDL, the avatar is no_message if it starts as no_message\n            even if the avatar may have acquired the message at a later point.\n            To check, if it has a message, check that the class vector corresponding to\n            with_message is == 1.\n            '''\n            avatar_pos = Position(*vgdl_obs['no_message.1']['position'])\n            # with_key is last position, see self.notable_sprites\n            if vgdl_obs['no_message.1']['class'][-1] == 1:\n                avatar = config.WITH_MESSAGE\n            else:\n                avatar = config.NO_MESSAGE\n\n        elif \"with_message.1\" in vgdl_obs:\n            # this case only occurs if avatar begins as with_message at start of episode\n            avatar_pos = Position(*vgdl_obs['with_message.1']['position'])\n            avatar = config.WITH_MESSAGE\n\n        else: # the avatar is not in observation, so is probably dead\n            return {\"entities\": entity_locs.grid, \"avatar\": avatar_locs.grid}\n\n        avatar_locs.add(avatar, avatar_pos) # if not dead, add it.\n\n        return {\"entities\": entity_locs.grid, \"avatar\": avatar_locs.grid}\n\n\n    def reset(self, variant_id:int=None, **kwargs):\n        '''\n        Resets the current environment. NOTE: We remake the environment each time.\n        This is a workaround to a bug in py-vgdl, where env.reset() does not\n        properly reset the environment. kwargs go to get_document().\n        '''\n\n        self.game = random.choice(self.all_games) # (e.g. enemy-alien, message-knight, goal - bear)\n\n        # choose the game variant (e.g. enmey-chasing, message-fleeing, goal-static)\n        # and initial starting location of the entities.\n        if variant_id is not None:\n            variant = self.game_variants[variant_id]\n        else:\n            variant = random.choice(self.game_variants)\n        init_state = random.choice(self.init_states) # inital state file\n\n        # args that will go into VGDL Env.\n        self._envargs = {\n            'game_file': variant.path,\n            'level_file': init_state,\n            'notable_sprites': self.notable_sprites.copy(),\n            'obs_type': 'objects', # track the objects\n            'block_size': 34  # rendering block size\n        }\n        self.env = VGDLEnv(**self._envargs)\n        vgdl_obs = self.env.reset()\n\n        all_npcs = (\n            Descr(entity=self.game.enemy.name, role='enemy', type=variant.enemy_type),\n            Descr(entity=self.game.message.name, role='message', type=variant.message_type),\n            Descr(entity=self.game.goal.name, role='goal', type=variant.goal_type),\n            Descr(entity=self.game.message.name, role='enemy', type=variant.decoy_message_type),\n            Descr(entity=self.game.goal.name, role='enemy', type=variant.decoy_goal_type),\n        )\n\n        manual = self.text_manual.get_document_plus(*all_npcs, **kwargs)\n        manual.append(\n            self.text_manual.get_decoy_descriptor(\n                entity=self.game.enemy.name,\n                not_of_role=\"enemy\",\n                not_of_type=variant.enemy_type\n            )\n        )\n\n        if self.shuffle_obs:\n            random.shuffle(manual)\n\n        return self._convert_obs(vgdl_obs), manual\n\n    def step(self, action):\n        vgdl_obs, reward, done, info = self.env.step(action)\n        return self._convert_obs(vgdl_obs), reward, done, info"}
{"type": "source_file", "path": "src/messenger/models/utils.py", "content": "'''\nCommon code and utilities used by the models\n'''\n\nimport torch\n\nclass ObservationBuffer:\n    '''\n    Maintains a buffer of observations along the 0-dim. Observations\n    are currently expected to be a dict of np arrays. Currently keeps\n    observations in a list and then stacks them via torch.stack().\n    TODO: pre-allocate memory for faster calls to get_obs().\n    \n    Parameters:\n    buffer_size\n        How many previous observations to track in the buffer\n    device\n        The device on which buffers are loaded into\n    '''\n    def __init__(self, buffer_size, device):\n        self.buffer_size = buffer_size\n        self.buffer = None\n        self.device = device\n\n    def _np_to_tensor(self, obs):\n        return torch.from_numpy(obs).long().to(self.device)\n\n    def reset(self, obs):\n        # initialize / reset the buffer with the observation\n        self.buffer = [obs for _ in range(self.buffer_size)]\n\n    def update(self, obs):\n        # update the buffer by appending newest observation\n        assert self.buffer, \"Please initialize buffer first with reset()\"\n        del self.buffer[0] # delete the oldest entry\n        self.buffer.append(obs) # append the newest observation\n\n    def get_obs(self):\n        # get a stack of all observations currently in the buffer\n        stacked_obs = {}\n        for key in self.buffer[0].keys():\n            stacked_obs[key] = torch.stack(\n                [self._np_to_tensor(obs[key]) for obs in self.buffer]\n            )\n        return stacked_obs\n\n\nclass Encoder:\n    '''\n    Text-encoder class with caching for fast sentence-encoding.\n    self.encoder and self.tokenizer are expected to be HuggingFace model\n    and its respective tokenizer. \n    \n    Warning: currently have not implemented max cache size, watch out for\n    out of memory errors if the number of possible inputs is v. large. All \n    original Messenger descriptions combined should take < 2GB of GPU memory.\n    '''\n    def __init__(self, model, tokenizer, device: torch.device, max_length:int=36):\n        self.encoder = model.to(device)\n        self.tokenizer = tokenizer\n        self.device = device\n        self.max_length = max_length # max sentence length\n        self.cache = {}\n\n    def to(self, device):\n        self.device = device\n        self.encoder = self.encoder.to(device)\n        return self\n\n    def tokens_to_device(self, tokens):\n        tok_device = {}\n        for key in tokens:\n            tok_device[key] = tokens[key].to(self.device)\n        return tok_device\n\n    def encode(self, text):\n        '''\n        Encodes the text using self.encoder and self.tokenizer. Text should be\n        a list of sents, where sent is a string.\n        '''\n        encoded = [] # the final encoded texts\n        for sent in text:\n            if sent in self.cache.keys(): # sentence is in cache\n                encoded.append(self.cache[sent])\n            else: \n                with torch.no_grad():\n                    tokens = self.tokenizer(\n                        sent,\n                        return_tensors=\"pt\",\n                        truncation=False,\n                        truncation_strategy='do_not_truncate',\n                        padding=\"max_length\",\n                        max_length=self.max_length\n                    )\n                    emb = self.encoder(**self.tokens_to_device(tokens)).last_hidden_state\n                encoded.append(emb)\n                self.cache[sent] = emb\n        return torch.cat(encoded, dim=0)\n\ndef nonzero_mean(emb):\n    '''\n    Takes as input an embedding, emb. It should be H x W x L x D. with\n    optional batch dimension. (H,W) is the grid dim, L the layers and\n    D the embedding dimension. Returns mean of non-zero vectors along L dim.\n    This is used to take care of overlapping sprites.\n    '''\n    # Count the number of non-zero vectors\n    non_zero = torch.sum(torch.norm(emb, dim=-1) > 0, dim=-1)\n    non_zero = non_zero.unsqueeze(-1).float()  # broadcasting\n    non_zero[non_zero == 0] = 1 # prevent division by zero\n    return torch.sum(emb, dim=-2) / non_zero"}
{"type": "source_file", "path": "src/smartplay/bandits/__init__.py", "content": "from gym.envs.registration import register\n\nfrom .bandit import BanditTwoArmedDeterministicFixed\nfrom .bandit import BanditTwoArmedHighHighFixed\nfrom .bandit import BanditTwoArmedHighLowFixed\nfrom .bandit import BanditTwoArmedLowLowFixed\n\nenvironments = [['BanditTwoArmedDeterministicFixed', 'v0'],\n                ['BanditTwoArmedHighHighFixed', 'v0'],\n                ['BanditTwoArmedHighLowFixed', 'v0'],\n                ['BanditTwoArmedLowLowFixed', 'v0']]\n\nfor environment in environments:\n    register(\n        id='{}-{}'.format(environment[0], environment[1]),\n        entry_point='smartplay.bandits:{}'.format(environment[0]),\n    )\n"}
{"type": "source_file", "path": "src/messenger/envs/utils.py", "content": "'''\nUtilites for the environments\n'''\n\nimport json\nfrom pathlib import Path\nfrom messenger.envs.config import NPCS, Game\n\ndef get_entity(name:str):\n    '''\n    Get the Entity object for the entity with\n    name name.\n    '''\n    for entity in NPCS:\n        if entity.name == name:\n            return entity\n    raise Exception(\"entity not found.\")\n\ndef get_game(game_tuple):\n    '''\n    Take a tuple of strings (enemy, message, goal) and get the\n    corresponding Game object.\n    '''\n    enemy_name, message_name, goal_name = game_tuple\n    enemy = get_entity(enemy_name)\n    message = get_entity(message_name)\n    goal = get_entity(goal_name)\n    return Game(enemy=enemy, message=message, goal=goal)\n\ndef games_from_json(json_path:str, split:str):\n    '''\n    Convert game strings in games.json to Game namedtuples\n    '''\n    json_path = Path(json_path)\n    with json_path.open(mode=\"r\") as json_file:\n        games = json.load(json_file)\n    converted = []\n    for g in games[split]:\n        converted.append(get_game(g))\n    return converted"}
{"type": "source_file", "path": "src/messenger/models/emma.py", "content": "'''\nImplements the EMMA model\n'''\n\nimport random\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.distributions import Categorical\nfrom numpy import sqrt as sqrt\nfrom transformers import AutoModel, AutoTokenizer\n\nfrom messenger.models.utils import nonzero_mean, Encoder\n\nclass EMMA(nn.Module):\n    def __init__(self, state_h=10, state_w=10, action_dim=5, hist_len=3, n_latent_var=128,\n                emb_dim=256, f_maps=64, kernel_size=2, n_hidden_layers=1, device=None):\n        \n        super().__init__()\n\n        # calculate dimensions after flattening the conv layer output\n        lin_dim = f_maps * (state_h - (kernel_size - 1)) * (\n            state_w - (kernel_size - 1))\n        self.conv = nn.Conv2d(hist_len*256, f_maps, kernel_size) # conv layer\n\n        self.state_h = state_h\n        self.state_w = state_w\n        self.action_dim = action_dim\n        self.emb_dim = emb_dim\n        self.attn_scale = sqrt(emb_dim)\n    \n        self.sprite_emb = nn.Embedding(25, emb_dim, padding_idx=0) # sprite embedding layer\n        \n        hidden_layers = (nn.Linear(n_latent_var, n_latent_var), nn.LeakyReLU())*n_hidden_layers\n        self.action_layer = nn.Sequential(\n                nn.Linear(lin_dim, n_latent_var),\n                nn.LeakyReLU(),\n                *hidden_layers,\n                nn.Linear(n_latent_var, action_dim),\n                nn.Softmax(dim=-1)\n                )\n        \n        # critic \n        self.value_layer = nn.Sequential(\n                nn.Linear(lin_dim, n_latent_var),\n                nn.LeakyReLU(),\n                *hidden_layers,\n                nn.Linear(n_latent_var, 1)\n                )\n\n        # key value transforms\n        self.txt_key = nn.Linear(768, emb_dim)\n        self.scale_key = nn.Sequential(\n            nn.Linear(768, 1),\n            nn.Softmax(dim=-2)\n        )\n        \n        self.txt_val = nn.Linear(768, emb_dim)\n        self.scale_val = nn.Sequential(\n            nn.Linear(768, 1),\n            nn.Softmax(dim=-2)\n        )\n\n        if device:\n            self.device = device\n        else:\n            self.device = torch.device(\"cpu\")\n\n        # get the text encoder\n        text_model = AutoModel.from_pretrained(\"bert-base-uncased\")\n        tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n        self.encoder = Encoder(model=text_model, tokenizer=tokenizer, device=self.device)\n        self.to(device)\n\n    def to(self, device):\n        '''\n        Override the .to() method so that we can store the device as an attribute\n        and also update the device for self.encoder (which does not inherit nn.Module)\n        '''\n        self.device = device\n        self.encoder.to(device)\n        return super().to(device)\n\n    def attention(self, query, key, value):\n        '''\n        Cell by cell attention mechanism. Uses the sprite embeddings as query. Key is\n        text embeddings\n        '''\n        kq = query @ key.t() # dot product attention\n        mask = (kq != 0) # keep zeroed-out entries zero\n        kq = kq / self.attn_scale # scale to prevent vanishing grads\n        weights = F.softmax(kq, dim=-1) * mask\n        return torch.mean(weights.unsqueeze(-1) * value, dim=-2), weights\n        \n    def forward(self, obs, manual):\n        # encoder the text\n        temb = self.encoder.encode(manual)\n\n        # split the observation tensor into objects and avatar\n        entity_obs = obs[\"entities\"]\n        avatar_obs = obs[\"avatar\"]\n\n        # embedding for the avatar object, which will not attend to text\n        avatar_emb = nonzero_mean(self.sprite_emb(avatar_obs))\n\n        # take the non_zero mean of embedded objects, which will act as attention query\n        query = nonzero_mean(self.sprite_emb(entity_obs))\n\n        # Attention        \n        key = self.txt_key(temb)\n        key_scale = self.scale_key(temb) # (num sent, sent_len, 1)\n        key = key * key_scale\n        key = torch.sum(key, dim=1)\n        \n        value = self.txt_val(temb)\n        val_scale = self.scale_val(temb)\n        value = value * val_scale\n        value = torch.sum(value, dim=1)\n        \n        obs_emb, weights = self.attention(query, key, value)\n\n        # compress the channels from KHWC to HWC' where K is history length\n        obs_emb = obs_emb.view(self.state_h, self.state_w, -1)\n        avatar_emb = avatar_emb.view(self.state_h, self.state_w, -1)\n        obs_emb = (obs_emb + avatar_emb) / 2.0\n\n        # permute from HWC to NCHW and do convolution\n        obs_emb = obs_emb.permute(2, 0, 1).unsqueeze(0)\n        obs_emb = F.leaky_relu(self.conv(obs_emb)).view(-1)\n        \n        action_probs = self.action_layer(obs_emb)\n\n        action = torch.argmax(action_probs).item()\n        if random.random() < 0.05: # random action with 0.05 prob\n            action = random.randrange(0, self.action_dim)\n        return action"}
{"type": "source_file", "path": "src/messenger/envs/stage_one.py", "content": "'''\nClasses that follows a gym-like interface and implements stage one of the Messenger\nenvironment.\n'''\n\nimport json\nimport random\nfrom collections import namedtuple\nfrom pathlib import Path\n\nimport numpy as np\n\nfrom messenger.envs.base import MessengerEnv, Position\nimport messenger.envs.config as config\nfrom messenger.envs.manual import TextManual\nfrom messenger.envs.utils import games_from_json\n\n\n# Used to track sprites in StageOne, where we do not use VGDL to handle sprites.\nSprite = namedtuple(\"Sprite\", [\"name\", \"id\", \"position\"])\n\n\nclass StageOne(MessengerEnv):\n    def __init__(self, split, message_prob=0.2, shuffle_obs=True):\n        '''\n        Stage one where objects are all immovable. Since the episode length is short and entities\n        do not move, we do not use VGDL engine for efficiency.\n        message_prob:\n            the probability that the avatar starts with the message\n        shuffle_obs:\n            shuffle the observation including the text manual\n        '''\n        super().__init__()\n        self.message_prob = message_prob\n        self.shuffle_obs = shuffle_obs\n        this_folder = Path(__file__).parent\n        \n        # Get the games and manual\n        games_json_path = this_folder.joinpath(\"games.json\")\n        if \"train\" in split and \"mc\" in split: # multi-combination games\n            game_split = \"train_multi_comb\"\n            text_json_path = this_folder.joinpath(\"texts\", \"text_train.json\")\n        elif \"train\" in split and \"sc\" in split: # single-combination games\n            game_split = \"train_single_comb\"\n            text_json_path = this_folder.joinpath(\"texts\", \"text_train.json\")\n        elif \"val\" in split:\n            game_split = \"val\"\n            text_json_path = this_folder.joinpath(\"texts\", \"text_val.json\")\n        elif \"test\" in split:\n            game_split = \"test\"\n            text_json_path = this_folder.joinpath(\"texts\", \"text_test.json\")\n        else:\n            raise Exception(f\"Split: {split} not understood.\")\n\n        # list of Game namedtuples\n        self.all_games = games_from_json(json_path=games_json_path, split=game_split)\n        \n        # we only need the immovable and unknown descriptions, so just extract those.\n        with text_json_path.open(mode=\"r\") as f:\n            descrip = json.load(f)\n        \n        self.descriptors = {}\n        for entity in descrip:\n            self.descriptors[entity] = {}\n            for role in (\"enemy\", \"message\", \"goal\"):\n                self.descriptors[entity][role] = []\n                for sent in descrip[entity][role][\"immovable\"]:\n                    self.descriptors[entity][role].append(sent)\n                for sent in descrip[entity][role][\"unknown\"]:\n                    self.descriptors[entity][role].append(sent)\n        \n        self.positions = [ # all possible entity locations\n            Position(y=3, x=5),\n            Position(y=5, x=3),\n            Position(y=5, x=7),\n            Position(y=7, x=5)\n        ]\n        self.avatar_start_pos = Position(y=5, x=5)\n        self.avatar = None\n        self.enemy = None\n        self.message = None\n        self.neutral = None\n        self.goal = None\n\n    def _get_manual(self):\n        enemy_str = random.choice(self.descriptors[self.enemy.name][\"enemy\"])\n        key_str = random.choice(self.descriptors[self.message.name][\"message\"])\n        goal_str = random.choice(self.descriptors[self.goal.name][\"goal\"])\n        manual = [enemy_str, key_str, goal_str]\n        if self.shuffle_obs:\n            random.shuffle(manual)\n        return manual\n\n    def _get_obs(self):\n        entities = np.zeros((config.STATE_HEIGHT, config.STATE_WIDTH, 1))\n        avatar = np.zeros((config.STATE_HEIGHT, config.STATE_WIDTH, 1))\n        for sprite in (self.enemy, self.message, self.goal):\n            entities[sprite.position.y, sprite.position.x, 0] = sprite.id\n            \n        avatar[self.avatar.position.y, self.avatar.position.x, 0] = self.avatar.id\n        \n        return {\"entities\": entities, \"avatar\": avatar}\n\n    def reset(self):\n        self.game = random.choice(self.all_games)\n        enemy, message, goal = self.game.enemy, self.game.message, self.game.goal\n\n        # randomly choose where to put enemy, key, goal\n        shuffled_pos = random.sample(self.positions, 4)\n        self.enemy = Sprite(name=enemy.name, id=enemy.id, position=shuffled_pos[0])\n        self.message = Sprite(name=message.name, id=message.id, position=shuffled_pos[1])\n        self.goal = Sprite(name=goal.name, id=goal.id, position=shuffled_pos[2])\n        \n        if random.random() < self.message_prob:\n            self.avatar = Sprite(\n                name=config.WITH_MESSAGE.name,\n                id=config.WITH_MESSAGE.id,\n                position=self.avatar_start_pos\n            )\n\n        else: # decide whether avatar has message or not\n            self.avatar = Sprite(\n                name=config.NO_MESSAGE.name,\n                id=config.NO_MESSAGE.id,\n                position=self.avatar_start_pos\n            )\n        \n        obs = self._get_obs()\n        manual = self._get_manual()\n\n        return obs, manual\n    \n    def _move_avatar(self, action):\n        if action == config.ACTIONS.stay:\n            return\n        \n        elif action == config.ACTIONS.up: \n            if self.avatar.position.y <= 0:\n                return\n            else:\n                new_position = Position(\n                    y = self.avatar.position.y - 1,\n                    x = self.avatar.position.x\n                )\n                \n        elif action == config.ACTIONS.down: \n            if self.avatar.position.y >= config.STATE_HEIGHT - 1:\n                return\n            else:\n                new_position = Position(\n                    y = self.avatar.position.y + 1,\n                    x = self.avatar.position.x\n                )\n                \n        elif action == config.ACTIONS.left: \n            if self.avatar.position.x <= 0:\n                return\n            else:\n                new_position = Position(\n                    y = self.avatar.position.y,\n                    x = self.avatar.position.x - 1\n                )\n                \n        elif action == config.ACTIONS.right: \n            if self.avatar.position.x >= config.STATE_WIDTH - 1:\n                return\n            else:\n                new_position = Position(\n                    y = self.avatar.position.y,\n                    x = self.avatar.position.x + 1\n                )\n                \n        else:\n            raise Exception(f\"{action} is not a valid action.\")\n            \n        self.avatar = Sprite(\n                name=self.avatar.name,\n                id=self.avatar.id,\n                position=new_position\n            )\n            \n    def _overlap(self, sprite_1, sprite_2):\n        if (sprite_1.position.x == sprite_2.position.x and\n           sprite_1.position.y == sprite_2.position.y):\n            return True\n        else:\n            return False\n\n    def step(self, action):\n        self._move_avatar(action)\n        obs = self._get_obs()\n        if self._overlap(self.avatar, self.enemy):\n            return obs, -1.0, True, None  # state, reward, done, info\n        \n        if self._overlap(self.avatar, self.message):\n            if self.avatar.name == config.WITH_MESSAGE.name:\n                return obs, -1.0, True, None\n            elif self.avatar.name == config.NO_MESSAGE.name:\n                return obs, 1.0, True, None\n            else:\n                raise Exception(\"Unknown avatar name {avatar.name}\")\n            \n        if self._overlap(self.avatar, self.goal):\n            if self.avatar.name == config.WITH_MESSAGE.name:\n                return obs, 1.0, True, None\n            elif self.avatar.name == config.NO_MESSAGE.name:\n                return obs, -1.0, True, None\n            else:\n                raise Exception(\"Unknown avatar name {avatar.name}\")\n        \n        return obs, 0.0, False, None"}
{"type": "source_file", "path": "src/smartplay/crafter/crafter/__init__.py", "content": "from .env import Env, EnvSample\nfrom .recorder import Recorder\n\n# try:\n#   import gym\n#   gym.register(\n#       id='CrafterReward-v1',\n#       entry_point='crafter:Env',\n#       max_episode_steps=10000,\n#       kwargs={'reward': True})\n#   gym.register(\n#       id='CrafterNoReward-v1',\n#       entry_point='crafter:Env',\n#       max_episode_steps=10000,\n#       kwargs={'reward': False})\n# except ImportError:\n#   pass\n"}
{"type": "source_file", "path": "src/smartplay/crafter/crafter/constants.py", "content": "import pathlib\n\nimport ruamel.yaml as yaml\n\nroot = pathlib.Path(__file__).parent\nfor key, value in yaml.safe_load((root / 'data.yaml').read_text()).items():\n  globals()[key] = value\n"}
{"type": "source_file", "path": "src/smartplay/__init__.py", "content": "# This file is used to load all games in the smartplay directory.\n\nimport warnings\nimport importlib\nimport os\nimport yaml\n\n\nroot_dir = os.path.dirname(os.path.abspath(__file__))\ngames = []\nenv_list = []\n\n\n_exclude_path = ['__pycache__', 'utils', 'tests', 'eval', 'example_game']\n_module_dir = os.path.dirname(__file__)\n\n\nfor dirname in os.listdir(root_dir):\n    if os.path.isdir(os.path.join(root_dir, dirname)) and dirname not in _exclude_path:\n        if '__init__.py' in os.listdir(os.path.join(root_dir, dirname)):\n            games.append(dirname)\n\n\ngame_challenges = {}\nrecorded_settings = {}\n\n\nfor game in games:\n\n\n    if not os.path.exists(os.path.join(root_dir, game, 'evaluation.yml')):\n        warnings.warn('Game `{}` does not have evaluation.yml. Skipping the game.'.format(game), UserWarning)\n        continue\n\n\n    try:\n        # Load evaluation settings\n        with open(os.path.join(root_dir, game, 'evaluation.yml'), 'r') as f:\n            yml = yaml.safe_load(f)\n            recorded_setting = yml['recorded settings']\n            game_challenge = yml['challenges']\n\n    except Exception as e:\n        warnings.warn('Failed to load `evaluation.yml` for `{}`.'.format(game), UserWarning)\n        continue\n\n\n    try:\n        # Load environments\n        module = importlib.import_module(\".\"+game, package='smartplay')\n        environments = getattr(module, 'environments', None)\n        if environments:\n            for env_name, version in environments:\n                env_list.append('{}-{}'.format(env_name, version))\n                game_challenges[env_name] = game_challenge['all'] if env_name not in game_challenge.keys() else game_challenge[env_name]\n                if env_name in recorded_setting.keys():\n                    recorded_settings[env_name] = recorded_setting[env_name]\n        else:\n            warnings.warn('Failed to load `{}.environments`. Skipping the game.'.format(game), UserWarning)\n            continue\n\n    except Exception as e:\n        warnings.warn('Failed to import `{}`. Skipping the game.'.format(game), UserWarning)\n        continue\n\n\nfrom .eval import *"}
{"type": "source_file", "path": "src/messenger/models/__init__.py", "content": ""}
{"type": "source_file", "path": "src/messenger/envs/stage_two.py", "content": "'''\nClasses that follows a gym-like interface and implements stage two of the Messenger\nenvironment.\n'''\n\nimport json\nimport random\nfrom collections import namedtuple\nfrom pathlib import Path\nfrom os import environ\nimport re\n\n# hack to stop PyGame from printing to stdout\nenviron[\"PYGAME_HIDE_SUPPORT_PROMPT\"] = \"hide\"\n\nfrom vgdl.interfaces.gym import VGDLEnv\nimport numpy as np\n\nfrom messenger.envs.base import MessengerEnv, Grid, Position\nimport messenger.envs.config as config\nfrom messenger.envs.manual import TextManual\nfrom messenger.envs.utils import games_from_json\n\n\n# specifies the game variant (e.g. chasing enemy, fleeing message, stationary goal)\n# path is path to the vgdl domain file describing the variant.\nGameVariant = namedtuple(\n    \"GameVariant\", [\"path\", \"enemy_type\", \"message_type\", \"goal_type\"]\n)\n\n\nclass StageTwo(MessengerEnv):\n    '''\n    Full messenger environment with mobile sprites. Uses Py-VGDL as game engine.\n    To avoid the need to instantiate a large number of games, (since there are\n    P(12,3) = 1320 possible entity to role assignments) We apply a wrapper on top\n    of the text and game state which masks the role archetypes (enemy, message goal)\n    into entities (e.g. alien, knight, mage).\n    '''\n\n    def __init__(self, split:str, shuffle_obs=True):\n        super().__init__()\n        self.shuffle_obs = shuffle_obs # shuffle the entity layers\n\n        this_folder = Path(__file__).parent\n        # Get the games and manual\n        games_json_path = this_folder.joinpath(\"games.json\")\n        if \"train\" in split and \"mc\" in split: # multi-combination games\n            game_split = \"train_multi_comb\"\n            text_json_path = this_folder.joinpath(\"texts\", \"text_train.json\")\n        elif \"train\" in split and \"sc\" in split: # single-combination games\n            game_split = \"train_single_comb\"\n            text_json_path = this_folder.joinpath(\"texts\", \"text_train.json\")\n        elif \"val\" in split:\n            game_split = \"val\"\n            text_json_path = this_folder.joinpath(\"texts\", \"text_val.json\")\n        elif \"test\" in split:\n            game_split = \"test\"\n            text_json_path = this_folder.joinpath(\"texts\", \"text_test.json\")\n        else:\n            raise Exception(f\"Split: {split} not understood.\")\n\n        # list of Game namedtuples\n        self.all_games = games_from_json(json_path=games_json_path, split=game_split)\n        self.text_manual = TextManual(json_path=text_json_path)\n\n        # get the folder that has the game variants and init_states\n        if \"test\" in split and \"se\" not in split: # new dynamics (se for state estimation)\n            vgdl_files = this_folder.joinpath(\"vgdl_files\", \"stage_2_nd\")\n        else: # training dynamics\n            vgdl_files = this_folder.joinpath(\"vgdl_files\", \"stage_2\")\n        \n        # get the file paths to possible starting states\n        self.init_states = [\n            str(path) for path in vgdl_files.joinpath(\"init_states\").glob(\"*.txt\")\n        ]\n        # get all the game variants\n        self.game_variants = [\n            self._get_variant(path) for path in vgdl_files.joinpath(\"variants\").glob(\"*.txt\")\n        ]\n\n        # entities tracked by VGDLEnv\n        self.notable_sprites = [\"enemy\", \"message\", \"goal\", \"no_message\", \"with_message\"]\n        self.env = None # the VGDLEnv\n\n    def _get_variant(self, variant_file:Path) -> GameVariant:\n        '''\n        Return the GameVariant for the variant specified by variant_file. \n        Searches through the vgdl code to find the correct type:\n        {chaser, fleeing, immovable}\n        '''\n\n        code = variant_file.read_text()\n        return GameVariant(\n            path = str(variant_file),\n            enemy_type = re.search(r'enemy > (\\S+)', code)[1].lower(),\n            message_type = re.search(r'message > (\\S+)', code)[1].lower(),\n            goal_type = re.search(r'goal > (\\S+)', code)[1].lower()\n        )\n\n    def _convert_obs(self, vgdl_obs):\n        '''\n        Return a grid built from the vgdl observation which is a\n        KeyValueObservation object (see vgdl code for details).\n        '''\n        entity_locs = Grid(layers=3, shuffle=self.shuffle_obs)\n        avatar_locs = Grid(layers=1)\n\n        # try to add each entity one by one, if it's not there move on.\n        if 'enemy.1' in vgdl_obs:\n            entity_locs.add(self.game.enemy, Position(*vgdl_obs['enemy.1']['position']))\n        if 'message.1' in vgdl_obs:\n            entity_locs.add(self.game.message, Position(*vgdl_obs['message.1']['position']))\n        else:\n            # advance the entity counter, Oracle model requires special order.\n            # TODO: maybe used named layers to make this more understandable.\n            entity_locs.entity_count += 1\n        if 'goal.1' in vgdl_obs:\n            entity_locs.add(self.game.goal, Position(*vgdl_obs['goal.1']['position']))\n\n        if 'no_message.1' in vgdl_obs:\n            '''\n            Due to a quirk in VGDL, the avatar is no_message if it starts as no_message\n            even if the avatar may have acquired the message at a later point.\n            To check, if it has a message, check that the class vector corresponding to\n            with_message is == 1.\n            '''\n            avatar_pos = Position(*vgdl_obs['no_message.1']['position'])\n            # with_key is last position, see self.notable_sprites\n            if vgdl_obs['no_message.1']['class'][-1] == 1:\n                avatar = config.WITH_MESSAGE\n            else:\n                avatar = config.NO_MESSAGE\n\n        elif \"with_message.1\" in vgdl_obs:\n            # this case only occurs if avatar begins as with_message at start of episode\n            avatar_pos = Position(*vgdl_obs['with_message.1']['position'])\n            avatar = config.WITH_MESSAGE\n\n        else: # the avatar is not in observation, so is probably dead\n            return {\"entities\": entity_locs.grid, \"avatar\": avatar_locs.grid}\n\n        avatar_locs.add(avatar, avatar_pos) # if not dead, add it.\n\n        return {\"entities\": entity_locs.grid, \"avatar\": avatar_locs.grid}\n\n    def reset(self, **kwargs):\n        '''\n        Resets the current environment. NOTE: We remake the environment each time.\n        This is a workaround to a bug in py-vgdl, where env.reset() does not\n        properly reset the environment. kwargs go to get_document().\n        '''\n\n        self.game = random.choice(self.all_games) # (e.g. enemy-alien, message-knight, goal - bear)\n\n        # choose the game variant (e.g. enmey-chasing, message-fleeing, goal-static)\n        # and initial starting location of the entities.\n        variant = random.choice(self.game_variants)\n        init_state = random.choice(self.init_states) # initial state file\n\n        # args that will go into VGDL Env.\n        self._envargs = {\n            'game_file': variant.path,\n            'level_file': init_state,\n            'notable_sprites': self.notable_sprites.copy(),\n            'obs_type': 'objects', # track the objects\n            'block_size': 34  # rendering block size\n        }\n        self.env = VGDLEnv(**self._envargs)\n        vgdl_obs = self.env.reset()\n\n        manual = self.text_manual.get_document(\n            enemy=self.game.enemy.name,\n            message=self.game.message.name,\n            goal=self.game.goal.name,\n            enemy_type=variant.enemy_type,\n            message_type=variant.message_type,\n            goal_type=variant.goal_type,\n            **kwargs\n        )\n\n        if self.shuffle_obs:\n            random.shuffle(manual)\n            \n        return self._convert_obs(vgdl_obs), manual\n\n    def step(self, action):\n        vgdl_obs, reward, done, info = self.env.step(action)\n        return self._convert_obs(vgdl_obs), reward, done, info\n    \n"}
{"type": "source_file", "path": "src/messenger/envs/wrappers.py", "content": "'''\nImplements wrappers on top of the basic messenger environments\n'''\nimport random\n\nfrom messenger.envs.base import MessengerEnv\nfrom messenger.envs.stage_one import StageOne\nfrom messenger.envs.stage_two import StageTwo\nfrom messenger.envs.stage_three import StageThree\n\n\nclass TwoEnvWrapper(MessengerEnv):\n    '''\n    Switches between two Messenger environments\n    '''\n    def __init__(self, stage:int, split_1:str, split_2:str, prob_env_1=0.5, **kwargs):\n        super().__init__()\n        if stage == 1:\n            self.env_1 = StageOne(split=split_1, **kwargs)\n            self.env_2 = StageOne(split=split_2, **kwargs)\n        elif stage == 2:\n            self.env_1 = StageTwo(split=split_1, **kwargs)\n            self.env_2 = StageTwo(split=split_2, **kwargs)\n        elif stage == 3:\n            self.env_1 = StageThree(split=split_1, **kwargs)\n            self.env_2 = StageThree(split=split_2, **kwargs)\n        \n        self.prob_env_1 = prob_env_1\n        self.cur_env = None\n        \n    def reset(self, **kwargs):\n        if random.random() < self.prob_env_1:\n            self.cur_env = self.env_1\n        else:\n            self.cur_env = self.env_2\n        return self.cur_env.reset(**kwargs)\n    \n    def step(self, action):\n        return self.cur_env.step(action)"}
{"type": "source_file", "path": "src/smartplay/crafter/crafter/env.py", "content": "import collections\n\nimport numpy as np\n\nfrom . import constants\nfrom . import engine\nfrom . import objects\nfrom . import worldgen\n\n\n# Gym is an optional dependency.\ntry:\n  import gym\n  DiscreteSpace = gym.spaces.Discrete\n  BoxSpace = gym.spaces.Box\n  DictSpace = gym.spaces.Dict\n  BaseClass = gym.Env\nexcept ImportError:\n  DiscreteSpace = collections.namedtuple('DiscreteSpace', 'n')\n  BoxSpace = collections.namedtuple('BoxSpace', 'low, high, shape, dtype')\n  DictSpace = collections.namedtuple('DictSpace', 'spaces')\n  BaseClass = object\n\n\nclass Env(BaseClass):\n\n  def __init__(\n      self, area=(64, 64), view=(9, 9), size=(64, 64),\n      reward=True, length=10000, seed=None):\n    view = np.array(view if hasattr(view, '__len__') else (view, view))\n    size = np.array(size if hasattr(size, '__len__') else (size, size))\n    seed = np.random.randint(0, 2**31 - 1) if seed is None else seed\n    self._area = area\n    self._view = view\n    self._size = size\n    self._reward = reward\n    self._length = length\n    self._seed = seed\n    self._episode = 0\n    self._world = engine.World(area, constants.materials, (12, 12))\n    self._textures = engine.Textures(constants.root / 'assets')\n    item_rows = int(np.ceil(len(constants.items) / view[0]))\n    self._local_view = engine.LocalView(\n        self._world, self._textures, [view[0], view[1] - item_rows])\n    self._item_view = engine.ItemView(\n        self._textures, [view[0], item_rows])\n    self._sem_view = engine.SemanticView(self._world, [\n        objects.Player, objects.Cow, objects.Zombie,\n        objects.Skeleton, objects.Arrow, objects.Plant])\n    self._step = None\n    self._player = None\n    self._last_health = None\n    self._unlocked = None\n    # Some libraries expect these attributes to be set.\n    self.reward_range = None\n    self.metadata = None\n\n  @property\n  def observation_space(self):\n    return BoxSpace(0, 255, tuple(self._size) + (3,), np.uint8)\n\n  @property\n  def action_space(self):\n    return DiscreteSpace(len(constants.actions))\n\n  @property\n  def action_names(self):\n    return constants.actions\n\n  def reset(self):\n    center = (self._world.area[0] // 2, self._world.area[1] // 2)\n    self._episode += 1\n    self._step = 0\n    self._world.reset(seed=hash((self._seed, self._episode)) % (2 ** 31 - 1))\n    self._update_time()\n    self._player = objects.Player(self._world, center)\n    self._last_health = self._player.health\n    self._world.add(self._player)\n    self._unlocked = set()\n    worldgen.generate_world(self._world, self._player)\n    return self._obs()\n\n  def step(self, action):\n    self._step += 1\n    self._update_time()\n    self._player.action = constants.actions[action]\n    for obj in self._world.objects:\n      if self._player.distance(obj) < 2 * max(self._view):\n        obj.update()\n    if self._step % 10 == 0:\n      for chunk, objs in self._world.chunks.items():\n        # xmin, xmax, ymin, ymax = chunk\n        # center = (xmax - xmin) // 2, (ymax - ymin) // 2\n        # if self._player.distance(center) < 4 * max(self._view):\n        self._balance_chunk(chunk, objs)\n    obs = self._obs()\n    reward = (self._player.health - self._last_health) / 10\n    self._last_health = self._player.health\n    unlocked = {\n        name for name, count in self._player.achievements.items()\n        if count > 0 and name not in self._unlocked}\n    if unlocked:\n      self._unlocked |= unlocked\n      reward += 1.0\n    dead = self._player.health <= 0\n    over = self._length and self._step >= self._length\n    done = dead or over\n    info = {\n        'inventory': self._player.inventory.copy(),\n        'achievements': self._player.achievements.copy(),\n        'sleeping': self._player.sleeping,\n        'discount': 1 - float(dead),\n        'semantic': self._sem_view(),\n        'player_pos': self._player.pos,\n        'player_facing': self._player.facing,\n        'reward': reward,\n        'dead': dead,\n        'unlocked': unlocked,\n        'action': self._player.action,\n        'view': self._view,\n    }\n    if not self._reward:\n      reward = 0.0\n    return obs, reward, done, info\n\n  def render(self, size=None):\n    size = size or self._size\n    unit = size // self._view\n    canvas = np.zeros(tuple(size) + (3,), np.uint8)\n    local_view = self._local_view(self._player, unit)\n    item_view = self._item_view(self._player.inventory, unit)\n    view = np.concatenate([local_view, item_view], 1)\n    border = (size - (size // self._view) * self._view) // 2\n    (x, y), (w, h) = border, view.shape[:2]\n    canvas[x: x + w, y: y + h] = view\n    return canvas.transpose((1, 0, 2))\n\n  def _obs(self):\n    return self.render()\n\n  def _update_time(self):\n    # https://www.desmos.com/calculator/grfbc6rs3h\n    progress = (self._step / 300) % 1 + 0.3\n    daylight = 1 - np.abs(np.cos(np.pi * progress)) ** 3\n    self._world.daylight = daylight\n\n  def _balance_chunk(self, chunk, objs):\n    light = self._world.daylight\n    self._balance_object(\n        chunk, objs, objects.Zombie, 'grass', 6, 0, 0.3, 0.4,\n        lambda pos: objects.Zombie(self._world, pos, self._player),\n        lambda num, space: (\n            0 if space < 50 else 3.5 - 3 * light, 3.5 - 3 * light))\n    self._balance_object(\n        chunk, objs, objects.Skeleton, 'path', 7, 7, 0.1, 0.1,\n        lambda pos: objects.Skeleton(self._world, pos, self._player),\n        lambda num, space: (0 if space < 6 else 1, 2))\n    self._balance_object(\n        chunk, objs, objects.Cow, 'grass', 5, 5, 0.01, 0.1,\n        lambda pos: objects.Cow(self._world, pos),\n        lambda num, space: (0 if space < 30 else 1, 1.5 + light))\n\n  def _balance_object(\n      self, chunk, objs, cls, material, span_dist, despan_dist,\n      spawn_prob, despawn_prob, ctor, target_fn):\n    xmin, xmax, ymin, ymax = chunk\n    random = self._world.random\n    creatures = [obj for obj in objs if isinstance(obj, cls)]\n    mask = self._world.mask(*chunk, material)\n    target_min, target_max = target_fn(len(creatures), mask.sum())\n    if len(creatures) < int(target_min) and random.uniform() < spawn_prob:\n      xs = np.tile(np.arange(xmin, xmax)[:, None], [1, ymax - ymin])\n      ys = np.tile(np.arange(ymin, ymax)[None, :], [xmax - xmin, 1])\n      xs, ys = xs[mask], ys[mask]\n      i = random.randint(0, len(xs))\n      pos = np.array((xs[i], ys[i]))\n      empty = self._world[pos][1] is None\n      away = self._player.distance(pos) >= span_dist\n      if empty and away:\n        self._world.add(ctor(pos))\n    elif len(creatures) > int(target_max) and random.uniform() < despawn_prob:\n      obj = creatures[random.randint(0, len(creatures))]\n      away = self._player.distance(obj.pos) >= despan_dist\n      if away:\n        self._world.remove(obj)\n\nclass EnvSample(BaseClass):\n\n  def __init__(\n      self, area=(64, 64), view=(9, 9), size=(64, 64),\n      reward=True, length=10000, seed=None, steps=20):\n    view = np.array(view if hasattr(view, '__len__') else (view, view))\n    size = np.array(size if hasattr(size, '__len__') else (size, size))\n    seed = np.random.randint(0, 2**31 - 1) if seed is None else seed\n    self._area = area\n    self._view = view\n    self._size = size\n    self._reward = reward\n    self._length = length\n    self._seed = seed\n    self._episode = 0\n    self._world = engine.World(area, constants.materials, (12, 12))\n    self._textures = engine.Textures(constants.root / 'assets')\n    item_rows = int(np.ceil(len(constants.items) / view[0]))\n    self._local_view = engine.LocalView(\n        self._world, self._textures, [view[0], view[1] - item_rows])\n    self._item_view = engine.ItemView(\n        self._textures, [view[0], item_rows])\n    self._sem_view = engine.SemanticView(self._world, [\n        objects.Player, objects.Cow, objects.Zombie,\n        objects.Skeleton, objects.Arrow, objects.Plant])\n    self._step = None\n    self._nsteps = steps\n    self._player = None\n    self._last_health = None\n    self._unlocked = None\n    # Some libraries expect these attributes to be set.\n    self.reward_range = None\n    self.metadata = None\n\n  @property\n  def observation_space(self):\n    return BoxSpace(0, 255, tuple(self._size) + (3,), np.uint8)\n\n  @property\n  def action_space(self):\n    return DiscreteSpace(len(constants.actions))\n\n  @property\n  def action_names(self):\n    return constants.actions\n\n  def reset(self):\n    center = (self._world.area[0] // 2, self._world.area[1] // 2)\n    self._episode += 1\n    self._step = 0\n    self._world.reset(seed=hash((self._seed, self._episode)) % (2 ** 31 - 1))\n    self._update_time()\n    self._player = objects.Player(self._world, center)\n    self._last_health = self._player.health\n    self._world.add(self._player)\n    self._unlocked = set()\n    worldgen.generate_world(self._world, self._player)\n    return self._obs()\n\n  def __step(self, action):\n    self._step += 1\n    self._update_time()\n    self._player.action = constants.actions[action]\n    for obj in self._world.objects:\n      if self._player.distance(obj) < 2 * max(self._view):\n        obj.update()\n    if self._step % 10 == 0:\n      for chunk, objs in self._world.chunks.items():\n        # xmin, xmax, ymin, ymax = chunk\n        # center = (xmax - xmin) // 2, (ymax - ymin) // 2\n        # if self._player.distance(center) < 4 * max(self._view):\n        self._balance_chunk(chunk, objs)\n    obs = self._obs()\n    reward = (self._player.health - self._last_health) / 10\n    self._last_health = self._player.health\n    unlocked = {\n        name for name, count in self._player.achievements.items()\n        if count > 0 and name not in self._unlocked}\n    if unlocked:\n      self._unlocked |= unlocked\n      reward += 1.0\n    dead = self._player.health <= 0\n    over = self._length and self._step >= self._length\n    done = dead or over\n    info = {\n        'inventory': self._player.inventory.copy(),\n        'achievements': self._player.achievements.copy(),\n        'discount': 1 - float(dead),\n        'semantic': self._sem_view(),\n        'player_pos': self._player.pos,\n        'player_facing': self._player.facing,\n        'reward': reward,\n        'dead': dead,\n        'unlocked': unlocked,\n        'action': self._player.action,\n        'view': self._view,\n    }\n    if not self._reward:\n      reward = 0.0\n    return obs, reward, done, info\n\n  def step(self, action):\n    frames = max(int(np.random.normal(loc = self._nsteps, scale = self._nsteps/2)), 1)\n    for _ in range(frames):\n      obs, reward, done, info = self.__step(np.random.randint(len(constants.actions)))\n      if done:\n        self.reset()\n        obs, reward, done, info = self.__step(np.random.randint(len(constants.actions)))\n    return obs, reward, done, info\n\n  def render(self, size=None):\n    size = size or self._size\n    unit = size // self._view\n    canvas = np.zeros(tuple(size) + (3,), np.uint8)\n    local_view = self._local_view(self._player, unit)\n    item_view = self._item_view(self._player.inventory, unit)\n    view = np.concatenate([local_view, item_view], 1)\n    border = (size - (size // self._view) * self._view) // 2\n    (x, y), (w, h) = border, view.shape[:2]\n    canvas[x: x + w, y: y + h] = view\n    return canvas.transpose((1, 0, 2))\n\n  def _obs(self):\n    return self.render()\n\n  def _update_time(self):\n    # https://www.desmos.com/calculator/grfbc6rs3h\n    progress = (self._step / 300) % 1 + 0.3\n    daylight = 1 - np.abs(np.cos(np.pi * progress)) ** 3\n    self._world.daylight = daylight\n\n  def _balance_chunk(self, chunk, objs):\n    light = self._world.daylight\n    self._balance_object(\n        chunk, objs, objects.Zombie, 'grass', 6, 0, 0.3, 0.4,\n        lambda pos: objects.Zombie(self._world, pos, self._player),\n        lambda num, space: (\n            0 if space < 50 else 3.5 - 3 * light, 3.5 - 3 * light))\n    self._balance_object(\n        chunk, objs, objects.Skeleton, 'path', 7, 7, 0.1, 0.1,\n        lambda pos: objects.Skeleton(self._world, pos, self._player),\n        lambda num, space: (0 if space < 6 else 1, 2))\n    self._balance_object(\n        chunk, objs, objects.Cow, 'grass', 5, 5, 0.01, 0.1,\n        lambda pos: objects.Cow(self._world, pos),\n        lambda num, space: (0 if space < 30 else 1, 1.5 + light))\n\n  def _balance_object(\n      self, chunk, objs, cls, material, span_dist, despan_dist,\n      spawn_prob, despawn_prob, ctor, target_fn):\n    xmin, xmax, ymin, ymax = chunk\n    random = self._world.random\n    creatures = [obj for obj in objs if isinstance(obj, cls)]\n    mask = self._world.mask(*chunk, material)\n    target_min, target_max = target_fn(len(creatures), mask.sum())\n    if len(creatures) < int(target_min) and random.uniform() < spawn_prob:\n      xs = np.tile(np.arange(xmin, xmax)[:, None], [1, ymax - ymin])\n      ys = np.tile(np.arange(ymin, ymax)[None, :], [xmax - xmin, 1])\n      xs, ys = xs[mask], ys[mask]\n      i = random.randint(0, len(xs))\n      pos = np.array((xs[i], ys[i]))\n      empty = self._world[pos][1] is None\n      away = self._player.distance(pos) >= span_dist\n      if empty and away:\n        self._world.add(ctor(pos))\n    elif len(creatures) > int(target_max) and random.uniform() < despawn_prob:\n      obj = creatures[random.randint(0, len(creatures))]\n      away = self._player.distance(obj.pos) >= despan_dist\n      if away:\n        self._world.remove(obj)\n"}
{"type": "source_file", "path": "src/messenger/envs/manual.py", "content": "import random\nimport json\nfrom collections import namedtuple\n\nimport messenger.envs.config as config\n\nDescr = namedtuple(\"Description\", ['entity', 'role', 'type'])\n\nclass TextManual:\n    '''\n    Class which implements methods that allow environments to construct\n    text manuals for games in Messenger.\n    '''\n    \n    def __init__(self, json_path):\n        with open(json_path, \"r\") as f:\n            self.descriptors = json.load(f)\n\n    def get_descriptor(self, entity, role, entity_type, no_type_p=0.15):\n        '''\n        Get a descriptor using the templates.\n        Parameters:\n        entity: The object that is being described (e.g. alien)\n        role: The role the the object plays in the env. (e.g. enemy)\n        entity_type:\n            The object type (e.g. chaser)\n        no_type_p:\n            The probability of returning a descriptor that does not have\n            any type information (only if entity_type is not None).\n        '''\n        if random.random() < no_type_p: # no type information\n            return random.choice(self.descriptors[entity][role][\"unknown\"])\n\n        else:\n            return random.choice(self.descriptors[entity][role][entity_type])\n\n    def get_document(self, enemy, message, goal, shuffle=False, \n                enemy_type=None, message_type=None, goal_type=None,\n                append=False, delete=False, **kwargs):\n        '''\n        Makes a document for Messenger using the specified parameters.\n        If no type is provided, a random type will be selected.\n\n        Parameters:\n        append: \n            If True, append an extraneous sentence to the document describing a \n            random object that is not in {enemy, message, goal}.\n        delete: If True, Delete a random descriptor from the document.\n        shuffle: \n            If True, shuffles the order of the descriptors\n        kwargs:\n            All other kwargs go to get_descriptor()\n        '''\n\n        document = [\n            self.get_descriptor(entity=enemy, entity_type=enemy_type, role=\"enemy\", **kwargs),\n            self.get_descriptor(entity=message, entity_type=message_type, role=\"message\", **kwargs),\n            self.get_descriptor(entity=goal, entity_type=goal_type, role=\"goal\", **kwargs)\n        ]\n\n        if delete: # delete a random descriptor\n            document = random.sample(document, 2)\n\n        if append:\n            # choose an object not in {enemy, message, goal}\n            valid_objs = [obj.name for obj in config.NPCS if obj.name not in [enemy, message, goal]]\n            rand_obj = random.choice(valid_objs)\n            result = None\n            while result is None:\n                try:\n                    result = self.get_descriptor(\n                        entity=rand_obj,\n                        role=random.choice((\"enemy\", \"message\", \"goal\")),\n                        entity_type=random.choice((\"chaser\", \"fleeing\", \"immovable\")),\n                        **kwargs\n                    )\n                except:\n                    pass\n            document.append(result)\n            \n        if shuffle:\n            document = random.sample(document, len(document))\n\n        return document\n\n    def get_document_plus(self, *args, **kwargs):\n        '''\n        Makes a document for Messenger using the specified parameters.\n\n        Parameters:\n        args: List of Descrip namedtuples\n        '''\n\n        document = []\n\n        for descrip in args:\n            document.append(\n                self.get_descriptor(\n                    entity=descrip.entity,\n                    role=descrip.role,\n                    entity_type=descrip.type,\n                    no_type_p=0,\n                    **kwargs\n                )\n            )\n\n        return document\n\n    def get_decoy_descriptor(self, entity, not_of_role, not_of_type, **kwargs):\n        '''\n        Get a description about the entity where the entity is not of role not_of_role\n        and not of type not_of_type\n        '''\n        possible_roles = [x for x in ('message', 'goal', 'enemy') if x != not_of_role]\n        random.shuffle(possible_roles)\n        selected_type = random.choice([x for x in ('chaser', 'fleeing', 'immovable') if x != not_of_type])\n\n        for role in possible_roles:\n            try:\n                return self.get_descriptor(\n                    entity=entity,\n                    role=role,\n                    entity_type=selected_type,\n                    no_type_p=0,\n                    **kwargs\n                )\n            except:\n                continue\n        raise Exception('decoy description with impossible constraints')\n\n\nif __name__ == \"__main__\":\n    # just some quick and dirty tests\n    from pathlib import Path\n    text_json = Path(__file__).parent.joinpath('texts', 'text_train.json')\n    manual = TextManual(json_path=text_json)\n    descriptions = (\n        Descr(entity=\"airplane\", role='goal', type='chaser'),\n        Descr(entity=\"airplane\", role='message', type='fleeing'),\n        Descr(entity=\"dog\", role='enemy', type='immovable'),\n        Descr(entity=\"dog\", role='goal', type='fleeing'),\n        Descr(entity=\"mage\", role='goal', type='chaser'),\n        Descr(entity=\"mage\", role='message', type='immovable'),\n    )\n    print(manual.get_document_plus(*descriptions))"}
{"type": "source_file", "path": "src/messenger/__init__.py", "content": "from gym.envs.registration import register\n\nregister(\n    id = \"msgr-train-v1\",\n    entry_point=\"messenger.envs:TwoEnvWrapper\",\n    kwargs = dict(\n        stage=1,\n        split_1=\"train_mc\",\n        split_2=\"train_sc\",\n        prob_env_1=0.75\n    )\n)\n\nregister(\n    id = \"msgr-train-sc-v1\",\n    entry_point=\"messenger.envs:StageOne\",\n    kwargs = dict(\n        split=\"train_sc\",\n    )\n)\n\nregister(\n    id = \"msgr-train-mc-v1\",\n    entry_point=\"messenger.envs:StageOne\",\n    kwargs = dict(\n        split=\"train_mc\",\n    )\n)\n\nregister(\n    id = \"msgr-val-v1\",\n    entry_point=\"messenger.envs:StageOne\",\n    kwargs = dict(\n        split=\"val\",\n    )\n)\n\nregister(\n    id = \"msgr-test-v1\",\n    entry_point=\"messenger.envs:StageOne\",\n    kwargs = dict(\n        split=\"test\",\n    )\n)\n\nregister(\n    id = \"msgr-train-v2\",\n    entry_point=\"messenger.envs:TwoEnvWrapper\",\n    kwargs = dict(\n        stage=2,\n        split_1=\"train_mc\",\n        split_2=\"train_sc\",\n        prob_env_1=0.75\n    )\n)\n\nregister(\n    id = \"msgr-train-sc-v2\",\n    entry_point=\"messenger.envs:StageTwo\",\n    kwargs = dict(\n        split=\"train_sc\",\n    )\n)\n\nregister(\n    id = \"msgr-train-mc-v2\",\n    entry_point=\"messenger.envs:StageTwo\",\n    kwargs = dict(\n        split=\"train_mc\",\n    )\n)\n\nregister(\n    id = \"msgr-val-v2\",\n    entry_point=\"messenger.envs:StageTwo\",\n    kwargs = dict(\n        split=\"val\",\n    )\n)\n\nregister(\n    id = \"msgr-test-v2\",\n    entry_point=\"messenger.envs:StageTwo\",\n    kwargs = dict(\n        split=\"test\",\n    )\n)\n\nregister(\n    id = \"msgr-test-se-v2\",\n    entry_point=\"messenger.envs:StageTwo\",\n    kwargs = dict(\n        split=\"test_se\",\n    )\n)\n\nregister(\n    id = \"msgr-train-v3\",\n    entry_point=\"messenger.envs:TwoEnvWrapper\",\n    kwargs = dict(\n        stage=3,\n        split_1=\"train_mc\",\n        split_2=\"train_sc\",\n        prob_env_1=0.75,\n    )\n)\n\nregister(\n    id = \"msgr-train-sc-v3\",\n    entry_point=\"messenger.envs:StageThree\",\n    kwargs = dict(\n        split=\"train_sc\",\n    )\n)\n\nregister(\n    id = \"msgr-train-mc-v3\",\n    entry_point=\"messenger.envs:StageThree\",\n    kwargs = dict(\n        split=\"train_mc\",\n    )\n)\n\n\nregister(\n    id = \"msgr-val-v3\",\n    entry_point=\"messenger.envs:StageThree\",\n    kwargs = dict(\n        split=\"val\",\n    )\n)\n\nregister(\n    id = \"msgr-test-v3\",\n    entry_point=\"messenger.envs:StageThree\",\n    kwargs = dict(\n        split=\"test\",\n    )\n)"}
{"type": "source_file", "path": "src/smartplay/crafter/crafter/engine.py", "content": "import collections\nimport functools\nimport pathlib\n\nimport imageio\nimport numpy as np\nfrom PIL import Image, ImageEnhance\n\n\nclass AttrDict(dict):\n\n  __getattr__ = dict.__getitem__\n\n\nclass staticproperty:\n\n  def __init__(self, function):\n    self.function = function\n\n  def __get__(self, instance, owner=None):\n    return self.function()\n\n\nclass World:\n\n  def __init__(self, area, materials, chunk_size):\n    self.area = area\n    self._chunk_size = chunk_size\n    self._mat_names = {i: x for i, x in enumerate([None] + materials)}\n    self._mat_ids = {x: i for i, x in enumerate([None] + materials)}\n    self.reset()\n\n  def reset(self, seed=None):\n    self.random = np.random.RandomState(seed)\n    self.daylight = 0.0\n    self._chunks = collections.defaultdict(set)\n    self._objects = [None]\n    self._mat_map = np.zeros(self.area, np.uint8)\n    self._obj_map = np.zeros(self.area, np.uint32)\n\n  @property\n  def objects(self):\n    # Return a new list so the objects cannot change while being iterated over.\n    return [obj for obj in self._objects if obj]\n\n  @property\n  def chunks(self):\n    return self._chunks.copy()\n\n  def add(self, obj):\n    assert hasattr(obj, 'pos')\n    obj.pos = np.array(obj.pos)\n    assert self._obj_map[tuple(obj.pos)] == 0\n    index = len(self._objects)\n    self._objects.append(obj)\n    self._obj_map[tuple(obj.pos)] = index\n    self._chunks[self.chunk_key(obj.pos)].add(obj)\n\n  def remove(self, obj):\n    if obj.removed:\n      return\n    self._objects[self._obj_map[tuple(obj.pos)]] = None\n    self._obj_map[tuple(obj.pos)] = 0\n    self._chunks[self.chunk_key(obj.pos)].remove(obj)\n    obj.removed = True\n\n  def move(self, obj, pos):\n    if obj.removed:\n      return\n    pos = np.array(pos)\n    assert self._obj_map[tuple(pos)] == 0\n    index = self._obj_map[tuple(obj.pos)]\n    self._obj_map[tuple(pos)] = index\n    self._obj_map[tuple(obj.pos)] = 0\n    old_chunk = self.chunk_key(obj.pos)\n    new_chunk = self.chunk_key(pos)\n    if old_chunk != new_chunk:\n      self._chunks[old_chunk].remove(obj)\n      self._chunks[new_chunk].add(obj)\n    obj.pos = pos\n\n  def __setitem__(self, pos, material):\n    if material not in self._mat_ids:\n      id_ = len(self._mat_ids)\n      self._mat_ids[material] = id_\n    self._mat_map[tuple(pos)] = self._mat_ids[material]\n\n  def __getitem__(self, pos):\n    if not _inside((0, 0), pos, self.area):\n      return None, None\n    material = self._mat_names[self._mat_map[tuple(pos)]]\n    obj = self._objects[self._obj_map[tuple(pos)]]\n    return material, obj\n\n  def nearby(self, pos, distance):\n    (x, y), d = pos, distance\n    ids = set(self._mat_map[\n        x - d: x + d + 1, y - d: y + d + 1].flatten().tolist())\n    materials = tuple(self._mat_names[x] for x in ids)\n    indices = self._obj_map[\n        x - d: x + d + 1, y - d: y + d + 1].flatten().tolist()\n    objs = {self._objects[i] for i in indices if i > 0}\n    return materials, objs\n\n  def mask(self, xmin, xmax, ymin, ymax, material):\n    region = self._mat_map[xmin: xmax, ymin: ymax]\n    return (region == self._mat_ids[material])\n\n  def count(self, material):\n    return (self._mat_map == self._mat_ids[material]).sum()\n\n  def chunk_key(self, pos):\n    (x, y), (csx, csy) = pos, self._chunk_size\n    xmin, ymin = (x // csx) * csx, (y // csy) * csy\n    xmax = min(xmin + csx, self.area[0])\n    ymax = min(ymin + csy, self.area[1])\n    return (xmin, xmax, ymin, ymax)\n\n\nclass Textures:\n\n  def __init__(self, directory):\n    self._originals = {}\n    self._textures = {}\n    for filename in pathlib.Path(directory).glob('*.png'):\n      image = imageio.imread(filename.read_bytes())\n      image = image.transpose((1, 0) + tuple(range(2, len(image.shape))))\n      self._originals[filename.stem] = image\n      self._textures[(filename.stem, image.shape[:2])] = image\n\n  def get(self, name, size):\n    if name is None:\n      name = 'unknown'\n    size = int(size[0]), int(size[1])\n    key = name, size\n    if key not in self._textures:\n      image = self._originals[name]\n      image = Image.fromarray(image)\n      image = image.resize(size[::-1], resample=Image.NEAREST)\n      image = np.array(image)\n      self._textures[key] = image\n    return self._textures[key]\n\n\nclass GlobalView:\n\n  pass\n\n\nclass UncoverView:\n\n  pass\n\n\nclass LocalView:\n\n  def __init__(self, world, textures, grid):\n    self._world = world\n    self._textures = textures\n    self._grid = np.array(grid)\n    self._offset = self._grid // 2\n    self._area = np.array(self._world.area)\n    self._center = None\n\n  def __call__(self, player, unit):\n    self._unit = np.array(unit)\n    self._center = np.array(player.pos)\n    canvas = np.zeros(tuple(self._grid * unit) + (3,), np.uint8) + 127\n    for x in range(self._grid[0]):\n      for y in range(self._grid[1]):\n        pos = self._center + np.array([x, y]) - self._offset\n        if not _inside((0, 0), pos, self._area):\n          continue\n        texture = self._textures.get(self._world[pos][0], unit)\n        _draw(canvas, np.array([x, y]) * unit, texture)\n    for obj in self._world.objects:\n      pos = obj.pos - self._center + self._offset\n      if not _inside((0, 0), pos, self._grid):\n        continue\n      texture = self._textures.get(obj.texture, unit)\n      _draw_alpha(canvas, pos * unit, texture)\n    canvas = self._light(canvas, self._world.daylight)\n    if player.sleeping:\n      canvas = self._sleep(canvas)\n    # if player.health < 1:\n    #   canvas = self._tint(canvas, (128, 0, 0), 0.6)\n    return canvas\n\n  def _light(self, canvas, daylight):\n    night = canvas\n    if daylight < 0.5:\n      night = self._noise(night, 2 * (0.5 - daylight), 0.5)\n    night = np.array(ImageEnhance.Color(\n        Image.fromarray(night.astype(np.uint8))).enhance(0.4))\n    night = self._tint(night, (0, 16, 64), 0.5)\n    return daylight * canvas + (1 - daylight) * night\n\n  def _sleep(self, canvas):\n    canvas = np.array(ImageEnhance.Color(\n        Image.fromarray(canvas.astype(np.uint8))).enhance(0.0))\n    canvas = self._tint(canvas, (0, 0, 16), 0.5)\n    return canvas\n\n  def _tint(self, canvas, color, amount):\n    color = np.array(color)\n    return (1 - amount) * canvas + amount * color\n\n  def _noise(self, canvas, amount, stddev):\n    noise = self._world.random.uniform(32, 127, canvas.shape[:2])[..., None]\n    mask = amount * self._vignette(canvas.shape, stddev)[..., None]\n    return (1 - mask) * canvas + mask * noise\n\n  @functools.lru_cache(10)\n  def _vignette(self, shape, stddev):\n    xs, ys = np.meshgrid(\n        np.linspace(-1, 1, shape[0]),\n        np.linspace(-1, 1, shape[1]))\n    return 1 - np.exp(-0.5 * (xs ** 2 + ys ** 2) / (stddev ** 2)).T\n\n\nclass ItemView:\n\n  def __init__(self, textures, grid):\n    self._textures = textures\n    self._grid = np.array(grid)\n\n  def __call__(self, inventory, unit):\n    unit = np.array(unit)\n    canvas = np.zeros(tuple(self._grid * unit) + (3,), np.uint8)\n    for index, (item, amount) in enumerate(inventory.items()):\n      if amount < 1:\n        continue\n      self._item(canvas, index, item, unit)\n      self._amount(canvas, index, amount, unit)\n    return canvas\n\n  def _item(self, canvas, index, item, unit):\n    pos = index % self._grid[0], index // self._grid[0]\n    pos = (pos * unit + 0.1 * unit).astype(np.int32)\n    texture = self._textures.get(item, 0.8 * unit)\n    _draw_alpha(canvas, pos, texture)\n\n  def _amount(self, canvas, index, amount, unit):\n    pos = index % self._grid[0], index // self._grid[0]\n    pos = (pos * unit + 0.4 * unit).astype(np.int32)\n    text = str(amount) if amount in list(range(10)) else 'unknown'\n    texture = self._textures.get(text, 0.6 * unit)\n    _draw_alpha(canvas, pos, texture)\n\n\nclass SemanticView:\n\n  def __init__(self, world, obj_types):\n    self._world = world\n    self._mat_ids = world._mat_ids.copy()\n    self._obj_ids = {\n        c: len(self._mat_ids) + i\n        for i, c in enumerate(obj_types)}\n\n  def __call__(self):\n    canvas = self._world._mat_map.copy()\n    for obj in self._world.objects:\n      canvas[tuple(obj.pos)] = self._obj_ids[type(obj)]\n    return canvas\n\n\ndef _inside(lhs, mid, rhs):\n  return (lhs[0] <= mid[0] < rhs[0]) and (lhs[1] <= mid[1] < rhs[1])\n\ndef _draw(canvas, pos, texture):\n  (x, y), (w, h) = pos, texture.shape[:2]\n  if texture.shape[-1] == 4:\n    texture = texture[..., :3]\n  canvas[x: x + w, y: y + h] = texture\n\ndef _draw_alpha(canvas, pos, texture):\n  (x, y), (w, h) = pos, texture.shape[:2]\n  if texture.shape[-1] == 4:\n    alpha = texture[..., 3:].astype(np.float32) / 255\n    texture = texture[..., :3].astype(np.float32) / 255\n    current = canvas[x: x + w, y: y + h].astype(np.float32) / 255\n    blended = alpha * texture + (1 - alpha) * current\n    texture = (255 * blended).astype(np.uint8)\n  canvas[x: x + w, y: y + h] = texture\n"}
{"type": "source_file", "path": "src/smartplay/crafter/crafter/recorder.py", "content": "import datetime\nimport json\nimport pathlib\n\nimport imageio\nimport numpy as np\n\n\nclass Recorder:\n\n  def __init__(\n      self, env, directory, save_stats=True, save_video=True,\n      save_episode=True, video_size=(512, 512)):\n    if directory and save_stats:\n      env = StatsRecorder(env, directory)\n    if directory and save_video:\n      env = VideoRecorder(env, directory, video_size)\n    if directory and save_episode:\n      env = EpisodeRecorder(env, directory)\n    self._env = env\n\n  def __getattr__(self, name):\n    if name.startswith('__'):\n      raise AttributeError(name)\n    return getattr(self._env, name)\n\n\nclass StatsRecorder:\n\n  def __init__(self, env, directory):\n    self._env = env\n    self._directory = pathlib.Path(directory).expanduser()\n    self._directory.mkdir(exist_ok=True, parents=True)\n    self._file = (self._directory / 'stats.jsonl').open('a')\n    self._length = None\n    self._reward = None\n    self._unlocked = None\n    self._stats = None\n\n  def __getattr__(self, name):\n    if name.startswith('__'):\n      raise AttributeError(name)\n    return getattr(self._env, name)\n\n  def reset(self):\n    obs = self._env.reset()\n    self._length = 0\n    self._reward = 0\n    self._unlocked = None\n    self._stats = None\n    return obs\n\n  def step(self, action):\n    obs, reward, done, info = self._env.step(action)\n    self._length += 1\n    self._reward += info['reward']\n    if done:\n      self._stats = {'length': self._length, 'reward': round(self._reward, 1)}\n      for key, value in info['achievements'].items():\n        self._stats[f'achievement_{key}'] = value\n      self._save()\n    return obs, reward, done, info\n\n  def _save(self):\n    self._file.write(json.dumps(self._stats) + '\\n')\n    self._file.flush()\n\n\nclass VideoRecorder:\n\n  def __init__(self, env, directory, size=(512, 512)):\n    if not hasattr(env, 'episode_name'):\n      env = EpisodeName(env)\n    self._env = env\n    self._directory = pathlib.Path(directory).expanduser()\n    self._directory.mkdir(exist_ok=True, parents=True)\n    self._size = size\n    self._frames = None\n\n  def __getattr__(self, name):\n    if name.startswith('__'):\n      raise AttributeError(name)\n    return getattr(self._env, name)\n\n  def reset(self):\n    obs = self._env.reset()\n    self._frames = [self._env.render(self._size)]\n    return obs\n\n  def step(self, action):\n    obs, reward, done, info = self._env.step(action)\n    self._frames.append(self._env.render(self._size))\n    if done:\n      self._save()\n    return obs, reward, done, info\n\n  def _save(self):\n    filename = str(self._directory / (self._env.episode_name + '.mp4'))\n    imageio.mimsave(filename, self._frames)\n\n\nclass EpisodeRecorder:\n\n  def __init__(self, env, directory):\n    if not hasattr(env, 'episode_name'):\n      env = EpisodeName(env)\n    self._env = env\n    self._directory = pathlib.Path(directory).expanduser()\n    self._directory.mkdir(exist_ok=True, parents=True)\n    self._episode = None\n\n  def __getattr__(self, name):\n    if name.startswith('__'):\n      raise AttributeError(name)\n    return getattr(self._env, name)\n\n  def reset(self):\n    obs = self._env.reset()\n    self._episode = [{'image': obs}]\n    return obs\n\n  def step(self, action):\n    # Transitions are defined from the environment perspective, meaning that a\n    # transition contains the action and the resulting reward and next\n    # observation produced by the environment in response to said action.\n    obs, reward, done, info = self._env.step(action)\n    transition = {\n        'action': action, 'image': obs, 'reward': reward, 'done': done,\n    }\n    for key, value in info.items():\n      if key in ('inventory', 'achievements'):\n        continue\n      transition[key] = value\n    for key, value in info['achievements'].items():\n      transition[f'achievement_{key}'] = value\n    for key, value in info['inventory'].items():\n      transition[f'ainventory_{key}'] = value\n    self._episode.append(transition)\n    if done:\n      self._save()\n    return obs, reward, done, info\n\n  def _save(self):\n    filename = str(self._directory / (self._env.episode_name + '.npz'))\n    # Fill in zeros for keys missing at the first time step.\n    for key, value in self._episode[1].items():\n      if key not in self._episode[0]:\n        self._episode[0][key] = np.zeros_like(value)\n    episode = {\n        k: np.array([step[k] for step in self._episode])\n        for k in self._episode[0]}\n    np.savez_compressed(filename, **episode)\n\n\nclass EpisodeName:\n\n  def __init__(self, env):\n    self._env = env\n    self._timestamp = None\n    self._unlocked = None\n    self._length = None\n\n  def __getattr__(self, name):\n    if name.startswith('__'):\n      raise AttributeError(name)\n    return getattr(self._env, name)\n\n  def reset(self):\n    obs = self._env.reset()\n    self._timestamp = None\n    self._unlocked = None\n    self._length = 0\n    return obs\n\n  def step(self, action):\n    obs, reward, done, info = self._env.step(action)\n    self._length += 1\n    if done:\n      self._timestamp = datetime.datetime.now().strftime('%Y%m%dT%H%M%S')\n      self._unlocked = sum(int(v >= 1) for v in info['achievements'].values())\n    return obs, reward, done, info\n\n  @property\n  def episode_name(self):\n    return f'{self._timestamp}-ach{self._unlocked}-len{self._length}'\n"}
{"type": "source_file", "path": "src/smartplay/hanoi/__init__.py", "content": "from gym.envs.registration import register\n\nfrom .hanoi_env import Hanoi3Disk, Hanoi4Disk\n\nenvironments = [\n    ['Hanoi3Disk', 'v0'],\n    ['Hanoi4Disk', 'v0'],\n]\n\nfor environment in environments:\n    register(\n        id='{}-{}'.format(environment[0], environment[1]),\n        entry_point='smartplay.hanoi:{}'.format(environment[0]),\n        # group='smartplay',\n    )\n"}
{"type": "source_file", "path": "src/smartplay/crafter/crafter_env.py", "content": "import gym\nfrom gym import error, spaces, utils\nfrom gym.utils import seeding\nfrom ..utils import HistoryTracker\nfrom .crafter import Env\nimport numpy as np\n\nid_to_item = [0]*19\nimport itertools\ndummyenv = Env()\nfor name, ind in itertools.chain(dummyenv._world._mat_ids.items(), dummyenv._sem_view._obj_ids.items()):\n    name = str(name)[str(name).find('objects.')+len('objects.'):-2].lower() if 'objects.' in str(name) else str(name)\n    id_to_item[ind] = name\nplayer_idx = id_to_item.index('player')\ndel dummyenv\n\nvitals = [\"health\",\"food\",\"drink\",\"energy\",]\n\nrot = np.array([[0,-1],[1,0]])\ndirections = ['front', 'right', 'back', 'left']\n\ndef describe_inventory(info):\n    result = \"\"\n    \n    status_str = \"Your status:\\n{}\".format(\"\\n\".join([\"- {}: {}/9\".format(v, info['inventory'][v]) for v in vitals]))\n    result += status_str + \"\\n\\n\"\n    \n    inventory_str = \"\\n\".join([\"- {}: {}\".format(i, num) for i,num in info['inventory'].items() if i not in vitals and num!=0])\n    inventory_str = \"Your inventory:\\n{}\".format(inventory_str) if inventory_str else \"You have nothing in your inventory.\"\n    result += inventory_str #+ \"\\n\\n\"\n    \n    return result.strip()\n\n\nREF = np.array([0, 1])\n\ndef rotation_matrix(v1, v2):\n    dot = np.dot(v1,v2)\n    cross = np.cross(v1,v2)\n    rotation_matrix = np.array([[dot, -cross],[cross, dot]])\n    return rotation_matrix\n\ndef describe_loc(ref, P):\n    desc = []\n    if ref[1] > P[1]:\n        desc.append(\"north\")\n    elif ref[1] < P[1]:\n        desc.append(\"south\")\n    if ref[0] > P[0]:\n        desc.append(\"west\")\n    elif ref[0] < P[0]:\n        desc.append(\"east\")\n\n    return \"-\".join(desc)\n\n\ndef describe_env(info):\n    assert(info['semantic'][info['player_pos'][0],info['player_pos'][1]] == player_idx)\n    semantic = info['semantic'][info['player_pos'][0]-info['view'][0]//2:info['player_pos'][0]+info['view'][0]//2+1, info['player_pos'][1]-info['view'][1]//2+1:info['player_pos'][1]+info['view'][1]//2]\n    center = np.array([info['view'][0]//2,info['view'][1]//2-1])\n    result = \"\"\n    x = np.arange(semantic.shape[1])\n    y = np.arange(semantic.shape[0])\n    x1, y1 = np.meshgrid(x,y)\n    loc = np.stack((y1, x1),axis=-1)\n    dist = np.absolute(center-loc).sum(axis=-1)\n    obj_info_list = []\n    \n    facing = info['player_facing']\n    target = (center[0] + facing[0], center[1] + facing[1])\n    target = id_to_item[semantic[target]]\n    obs = \"You face {} at your front.\".format(target, describe_loc(np.array([0,0]),facing))\n    \n    for idx in np.unique(semantic):\n        if idx==player_idx:\n            continue\n\n        smallest = np.unravel_index(np.argmin(np.where(semantic==idx, dist, np.inf)), semantic.shape)\n        obj_info_list.append((id_to_item[idx], dist[smallest], describe_loc(np.array([0,0]), smallest-center)))\n\n    if len(obj_info_list)>0:\n        status_str = \"You see:\\n{}\".format(\"\\n\".join([\"- {} {} steps to your {}\".format(name, dist, loc) for name, dist, loc in obj_info_list]))\n    else:\n        status_str = \"You see nothing away from you.\"\n    result += status_str + \"\\n\\n\"\n    result += obs.strip()\n    \n    return result.strip()\n\n\ndef describe_act(info):\n    result = \"\"\n    \n    action_str = info['action'].replace('do_', 'interact_')\n    action_str = action_str.replace('move_up', 'move_north')\n    action_str = action_str.replace('move_down', 'move_south')\n    action_str = action_str.replace('move_left', 'move_west')\n    action_str = action_str.replace('move_right', 'move_east')\n    \n    act = \"You took action {}.\".format(action_str) \n    result+= act\n    \n    return result.strip()\n\n\ndef describe_status(info):\n    if info['sleeping']:\n        return \"You are sleeping, and will not be able take actions until energy is full.\\n\\n\"\n    elif info['dead']:\n        return \"You died.\\n\\n\"\n    else:\n        return \"\"\n\n    \ndef describe_frame(info, action):\n    try:\n        result = \"\"\n        \n        if action is not None:\n            result+=describe_act(info)\n        result+=describe_status(info)\n        result+=\"\\n\\n\"\n        result+=describe_env(info)\n        result+=\"\\n\\n\"\n        result+=describe_inventory(info)\n        \n        return result.strip()\n    except:\n        return \"Error, you are out of the map.\"\n\nclass Crafter(Env):\n\n    default_iter = 10\n    default_steps = 10000\n\n    def __init__(self, area=(64, 64), view=(9, 9), size=(64, 64), reward=True, length=10000, seed=None, max_steps=2):\n        self.history = HistoryTracker(max_steps)\n        self.action_list = [\"Noop\", \"Move West\", \"Move East\", \"Move North\", \"Move South\", \"Do\", \\\n    \"Sleep\", \"Place Stone\", \"Place Table\", \"Place Furnace\", \"Place Plant\", \\\n    \"Make Wood Pickaxe\", \"Make Stone Pickaxe\", \"Make Iron Pickaxe\", \"Make Wood Sword\", \\\n    \"Make Stone Sword\", \"Make Iron Sword\"]\n        import pickle\n        import pathlib\n        root = pathlib.Path(__file__).parent\n        with open(root / \"assets/crafter_ctxt.pkl\", 'rb') as f: # Context extracted using text-davinci-003 following https://arxiv.org/abs/2305.15486\n            CTXT = pickle.load(f)\n        CTXT = CTXT.replace(\"DO NOT answer in LaTeX.\", \"\")\n        CTXT = CTXT.replace(\"Move Up: Flat ground above the agent.\", \"Move North: Flat ground north of the agent.\")\n        CTXT = CTXT.replace(\"Move Down: Flat ground below the agent.\", \"Move South: Flat ground south of the agent.\")\n        CTXT = CTXT.replace(\"Move Left: Flat ground left to the agent.\", \"Move West: Flat ground west of the agent.\")\n        CTXT = CTXT.replace(\"Move Right: Flat ground right to the agent.\", \"Move East: Flat ground east of the agent.\")\n        self.desc = CTXT\n        self.score_tracker = 0\n        super().__init__(area, view, size, reward, length, seed)\n\n    def reset(self):\n        self.history.reset()\n        super().reset()\n        obs, reward, done, info = self.step(0)\n        self.score_tracker = 0 + sum([1. for k,v in info['achievements'].items() if v>0])\n        info.update({'manual': self.desc,\n                'obs': describe_frame(info, None),\n                'history': self.history.describe(),\n                'score': self.score_tracker,\n                'done': done,\n                'completed': 0,\n                })\n        self.history.step(info)\n        return obs, info\n    \n    def step(self, action):\n        obs, reward, done, info = super().step(action)\n        self.score_tracker = self.score_tracker + sum([1. for k,v in info['achievements'].items() if v>0])\n        info.update({'manual': self.desc,\n                'obs': describe_frame(info, self.action_list[action]),\n                'history': self.history.describe(),\n                'score': self.score_tracker,\n                'done': done,\n                'completed': 0,\n                })\n        self.history.step(info)\n        return obs, reward, done, info\n"}
{"type": "source_file", "path": "src/smartplay/crafter/crafter/run_terrain.py", "content": "import argparse\n\nimport imageio\nimport numpy as np\n\nimport crafter\n\n\ndef main():\n  parser = argparse.ArgumentParser()\n  parser.add_argument('--seed', type=int, default=None)\n  parser.add_argument('--amount', type=int, default=4)\n  parser.add_argument('--cols', type=int, default=4)\n  parser.add_argument('--area', nargs=2, type=int, default=(64, 64))\n  parser.add_argument('--size', type=int, default=1024)\n  parser.add_argument('--filename', type=str, default='terrain.png')\n  args = parser.parse_args()\n\n  env = crafter.Env(args.area, args.area, args.size, seed=args.seed)\n  images = []\n  for index in range(args.amount):\n    images.append(env.reset())\n    diamonds = env._world.count('diamond')\n    print(f'Map: {index:>2}, diamonds: {diamonds:>2}')\n\n  rows = len(images) // args.cols\n  strips = []\n  for row in range(rows):\n    strip = []\n    for col in range(args.cols):\n      try:\n        strip.append(images[row * args.cols + col])\n      except IndexError:\n        strip.append(np.zeros_like(strip[-1]))\n    strips.append(np.concatenate(strip, 1))\n  grid = np.concatenate(strips, 0)\n\n  imageio.imsave(args.filename, grid)\n  print('Saved', args.filename)\n\n\nif __name__ == '__main__':\n  main()\n"}
{"type": "source_file", "path": "src/smartplay/messenger_emma/__init__.py", "content": "from gym.envs.registration import register\n\nfrom .messenger_env import MessengerEnv\n\nenvironments = [\n    ['MessengerL1', 'v0'],\n    ['MessengerL2', 'v0'],\n    ['MessengerL3', 'v0'],\n]\n\nfor environment in environments:\n    register(\n        id='{}-{}'.format(environment[0], environment[1]),\n        entry_point='smartplay.messenger_emma:MessengerEnv',\n        kwargs={'lvl': int(environment[0][-1])}\n        # group='smartplay',\n    )\n"}
{"type": "source_file", "path": "src/smartplay/crafter/crafter/run_gui.py", "content": "import argparse\n\nimport numpy as np\ntry:\n  import pygame\nexcept ImportError:\n  print('Please install the pygame package to use the GUI.')\n  raise\nfrom PIL import Image\n\nimport crafter\n\n\ndef main():\n  boolean = lambda x: bool(['False', 'True'].index(x))\n  parser = argparse.ArgumentParser()\n  parser.add_argument('--seed', type=int, default=None)\n  parser.add_argument('--area', nargs=2, type=int, default=(64, 64))\n  parser.add_argument('--view', type=int, nargs=2, default=(9, 9))\n  parser.add_argument('--length', type=int, default=None)\n  parser.add_argument('--health', type=int, default=9)\n  parser.add_argument('--window', type=int, nargs=2, default=(600, 600))\n  parser.add_argument('--size', type=int, nargs=2, default=(0, 0))\n  parser.add_argument('--record', type=str, default=None)\n  parser.add_argument('--fps', type=int, default=5)\n  parser.add_argument('--wait', type=boolean, default=False)\n  parser.add_argument('--death', type=str, default='reset', choices=[\n      'continue', 'reset', 'quit'])\n  args = parser.parse_args()\n\n  keymap = {\n      pygame.K_a: 'move_left',\n      pygame.K_d: 'move_right',\n      pygame.K_w: 'move_up',\n      pygame.K_s: 'move_down',\n      pygame.K_SPACE: 'do',\n      pygame.K_TAB: 'sleep',\n\n      pygame.K_r: 'place_stone',\n      pygame.K_t: 'place_table',\n      pygame.K_f: 'place_furnace',\n      pygame.K_p: 'place_plant',\n\n      pygame.K_1: 'make_wood_pickaxe',\n      pygame.K_2: 'make_stone_pickaxe',\n      pygame.K_3: 'make_iron_pickaxe',\n      pygame.K_4: 'make_wood_sword',\n      pygame.K_5: 'make_stone_sword',\n      pygame.K_6: 'make_iron_sword',\n  }\n  print('Actions:')\n  for key, action in keymap.items():\n    print(f'  {pygame.key.name(key)}: {action}')\n\n  crafter.constants.items['health']['max'] = args.health\n  crafter.constants.items['health']['initial'] = args.health\n\n  size = list(args.size)\n  size[0] = size[0] or args.window[0]\n  size[1] = size[1] or args.window[1]\n\n  env = crafter.Env(\n      area=args.area, view=args.view, length=args.length, seed=args.seed)\n  env = crafter.Recorder(env, args.record)\n  env.reset()\n  achievements = set()\n  duration = 0\n  return_ = 0\n  was_done = False\n  print('Diamonds exist:', env._world.count('diamond'))\n\n  pygame.init()\n  screen = pygame.display.set_mode(args.window)\n  clock = pygame.time.Clock()\n  running = True\n  while running:\n\n    # Rendering.\n    image = env.render(size)\n    if size != args.window:\n      image = Image.fromarray(image)\n      image = image.resize(args.window, resample=Image.NEAREST)\n      image = np.array(image)\n    surface = pygame.surfarray.make_surface(image.transpose((1, 0, 2)))\n    screen.blit(surface, (0, 0))\n    pygame.display.flip()\n    clock.tick(args.fps)\n\n    # Keyboard input.\n    action = None\n    pygame.event.pump()\n    for event in pygame.event.get():\n      if event.type == pygame.QUIT:\n        running = False\n      elif event.type == pygame.KEYDOWN and event.key == pygame.K_ESCAPE:\n        running = False\n      elif event.type == pygame.KEYDOWN and event.key in keymap.keys():\n        action = keymap[event.key]\n    if action is None:\n      pressed = pygame.key.get_pressed()\n      for key, action in keymap.items():\n        if pressed[key]:\n          break\n      else:\n        if args.wait and not env._player.sleeping:\n          continue\n        else:\n          action = 'noop'\n\n    # Environment step.\n    _, reward, done, _ = env.step(env.action_names.index(action))\n    duration += 1\n\n    # Achievements.\n    unlocked = {\n        name for name, count in env._player.achievements.items()\n        if count > 0 and name not in achievements}\n    for name in unlocked:\n      achievements |= unlocked\n      total = len(env._player.achievements.keys())\n      print(f'Achievement ({len(achievements)}/{total}): {name}')\n    if env._step > 0 and env._step % 100 == 0:\n      print(f'Time step: {env._step}')\n    if reward:\n      print(f'Reward: {reward}')\n      return_ += reward\n\n    # Episode end.\n    if done and not was_done:\n      was_done = True\n      print('Episode done!')\n      print('Duration:', duration)\n      print('Return:', return_)\n      if args.death == 'quit':\n        running = False\n      if args.death == 'reset':\n        print('\\nStarting a new episode.')\n        env.reset()\n        achievements = set()\n        was_done = False\n        duration = 0\n        return_ = 0\n      if args.death == 'continue':\n        pass\n\n  pygame.quit()\n\n\nif __name__ == '__main__':\n  main()\n"}
{"type": "source_file", "path": "src/smartplay/minedojo/__init__.py", "content": "from gym.envs.registration import register\n\nfrom .minedojo_env import MineDojoEnv\n\nenvironments = [\n    ['MinedojoCreative0', 'v0'],\n    ['MinedojoCreative1', 'v0'],\n    ['MinedojoCreative2', 'v0'],\n    ['MinedojoCreative4', 'v0'],\n    ['MinedojoCreative5', 'v0'],\n    ['MinedojoCreative7', 'v0'],\n    ['MinedojoCreative8', 'v0'],\n    ['MinedojoCreative9', 'v0'],\n]\n\nfor environment in environments:\n    register(\n        id='{}-{}'.format(environment[0], environment[1]),\n        entry_point='smartplay.minedojo:MineDojoEnv',\n        kwargs={'task_id': environment[0][-1]}\n    )\n"}
{"type": "source_file", "path": "src/smartplay/crafter/crafter/objects.py", "content": "import numpy as np\n\nfrom . import constants\nfrom . import engine\n\n\nclass Object:\n\n  def __init__(self, world, pos):\n    self.world = world\n    self.pos = np.array(pos)\n    self.random = world.random\n    self.inventory = {'health': 0}\n    self.removed = False\n\n  @property\n  def texture(self):\n    raise 'unknown'\n\n  @property\n  def walkable(self):\n    return constants.walkable\n\n  @property\n  def health(self):\n    return self.inventory['health']\n\n  @health.setter\n  def health(self, value):\n    self.inventory['health'] = max(0, value)\n\n  @property\n  def all_dirs(self):\n    return ((-1, 0), (+1, 0), (0, -1), (0, +1))\n\n  def move(self, direction):\n    direction = np.array(direction)\n    target = self.pos + direction\n    if self.is_free(target):\n      self.world.move(self, target)\n      return True\n    return False\n\n  def is_free(self, target, materials=None):\n    materials = self.walkable if materials is None else materials\n    material, obj = self.world[target]\n    return obj is None and material in materials\n\n  def distance(self, target):\n    if hasattr(target, 'pos'):\n      target = target.pos\n    return np.abs(target - self.pos).sum()\n\n  def toward(self, target, long_axis=True):\n    if hasattr(target, 'pos'):\n      target = target.pos\n    offset = target - self.pos\n    dists = np.abs(offset)\n    if (dists[0] > dists[1] if long_axis else dists[0] <= dists[1]):\n      return np.array((np.sign(offset[0]), 0))\n    else:\n      return np.array((0, np.sign(offset[1])))\n\n  def random_dir(self):\n    return self.all_dirs[self.random.randint(0, 4)]\n\n\nclass Player(Object):\n\n  def __init__(self, world, pos):\n    super().__init__(world, pos)\n    self.facing = (0, 1)\n    self.inventory = {\n        name: info['initial'] for name, info in constants.items.items()}\n    self.achievements = {name: 0 for name in constants.achievements}\n    self.action = 'noop'\n    self.sleeping = False\n    self._last_health = self.health\n    self._hunger = 0\n    self._thirst = 0\n    self._fatigue = 0\n    self._recover = 0\n\n  @property\n  def texture(self):\n    if self.sleeping:\n      return 'player-sleep'\n    return {\n        (-1, 0): 'player-left',\n        (+1, 0): 'player-right',\n        (0, -1): 'player-up',\n        (0, +1): 'player-down',\n    }[tuple(self.facing)]\n\n  @property\n  def walkable(self):\n    return constants.walkable + ['lava']\n\n  def update(self):\n    target = (self.pos[0] + self.facing[0], self.pos[1] + self.facing[1])\n    material, obj = self.world[target]\n    action = self.action\n    if self.sleeping:\n      if self.inventory['energy'] < constants.items['energy']['max']:\n        action = 'sleep'\n      else:\n        self.sleeping = False\n        self.achievements['wake_up'] += 1\n    if action == 'noop':\n      pass\n    elif action.startswith('move_'):\n      self._move(action[len('move_'):])\n    elif action == 'do' and obj:\n      self._do_object(obj)\n    elif action == 'do':\n      self._do_material(target, material)\n    elif action == 'sleep':\n      if self.inventory['energy'] < constants.items['energy']['max']:\n        self.sleeping = True\n    elif action.startswith('place_'):\n      self._place(action[len('place_'):], target, material)\n    elif action.startswith('make_'):\n      self._make(action[len('make_'):])\n    self._update_life_stats()\n    self._degen_or_regen_health()\n    for name, amount in self.inventory.items():\n      maxmium = constants.items[name]['max']\n      self.inventory[name] = max(0, min(amount, maxmium))\n    # This needs to happen after the inventory states are clamped\n    # because it involves the health water inventory count.\n    self._wake_up_when_hurt()\n\n  def _update_life_stats(self):\n    self._hunger += 0.5 if self.sleeping else 1\n    if self._hunger > 25:\n      self._hunger = 0\n      self.inventory['food'] -= 1\n    self._thirst += 0.5 if self.sleeping else 1\n    if self._thirst > 20:\n      self._thirst = 0\n      self.inventory['drink'] -= 1\n    if self.sleeping:\n      self._fatigue = min(self._fatigue - 1, 0)\n    else:\n      self._fatigue += 1\n    if self._fatigue < -10:\n      self._fatigue = 0\n      self.inventory['energy'] += 1\n    if self._fatigue > 30:\n      self._fatigue = 0\n      self.inventory['energy'] -= 1\n\n  def _degen_or_regen_health(self):\n    necessities = (\n        self.inventory['food'] > 0,\n        self.inventory['drink'] > 0,\n        self.inventory['energy'] > 0 or self.sleeping)\n    if all(necessities):\n      self._recover += 2 if self.sleeping else 1\n    else:\n      self._recover -= 0.5 if self.sleeping else 1\n    if self._recover > 25:\n      self._recover = 0\n      self.health += 1\n    if self._recover < -15:\n      self._recover = 0\n      self.health -= 1\n\n  def _wake_up_when_hurt(self):\n    if self.health < self._last_health:\n      self.sleeping = False\n    self._last_health = self.health\n\n  def _move(self, direction):\n    directions = dict(left=(-1, 0), right=(+1, 0), up=(0, -1), down=(0, +1))\n    self.facing = directions[direction]\n    self.move(self.facing)\n    if self.world[self.pos][0] == 'lava':\n      self.health = 0\n\n  def _do_object(self, obj):\n    damage = max([\n        1,\n        self.inventory['wood_sword'] and 2,\n        self.inventory['stone_sword'] and 3,\n        self.inventory['iron_sword'] and 5,\n    ])\n    if isinstance(obj, Plant):\n      if obj.ripe:\n        obj.grown = 0\n        self.inventory['food'] += 4\n        self.achievements['eat_plant'] += 1\n    if isinstance(obj, Fence):\n      self.world.remove(obj)\n      self.inventory['fence'] += 1\n      self.achievements['collect_fence'] += 1\n    if isinstance(obj, Zombie):\n      obj.health -= damage\n      if obj.health <= 0:\n        self.achievements['defeat_zombie'] += 1\n    if isinstance(obj, Skeleton):\n      obj.health -= damage\n      if obj.health <= 0:\n        self.achievements['defeat_skeleton'] += 1\n    if isinstance(obj, Cow):\n      obj.health -= damage\n      if obj.health <= 0:\n        self.inventory['food'] += 6\n        self.achievements['eat_cow'] += 1\n        # TODO: Keep track of previous inventory state to do this in a more\n        # general way.\n        self._hunger = 0\n\n  def _do_material(self, target, material):\n    if material == 'water':\n      # TODO: Keep track of previous inventory state to do this in a more\n      # general way.\n      self._thirst = 0\n    info = constants.collect.get(material)\n    if not info:\n      return\n    for name, amount in info['require'].items():\n      if self.inventory[name] < amount:\n        return\n    self.world[target] = info['leaves']\n    if self.random.uniform() <= info.get('probability', 1):\n      for name, amount in info['receive'].items():\n        self.inventory[name] += amount\n        self.achievements[f'collect_{name}'] += 1\n\n  def _place(self, name, target, material):\n    if self.world[target][1]:\n      return\n    info = constants.place[name]\n    if material not in info['where']:\n      return\n    if any(self.inventory[k] < v for k, v in info['uses'].items()):\n      return\n    for item, amount in info['uses'].items():\n      self.inventory[item] -= amount\n    if info['type'] == 'material':\n      self.world[target] = name\n    elif info['type'] == 'object':\n      cls = {\n          'fence': Fence,\n          'plant': Plant,\n      }[name]\n      self.world.add(cls(self.world, target))\n    self.achievements[f'place_{name}'] += 1\n\n  def _make(self, name):\n    nearby, _ = self.world.nearby(self.pos, 1)\n    info = constants.make[name]\n    if not all(util in nearby for util in info['nearby']):\n      return\n    if any(self.inventory[k] < v for k, v in info['uses'].items()):\n      return\n    for item, amount in info['uses'].items():\n      self.inventory[item] -= amount\n    self.inventory[name] += info['gives']\n    self.achievements[f'make_{name}'] += 1\n\n\nclass Cow(Object):\n\n  def __init__(self, world, pos):\n    super().__init__(world, pos)\n    self.health = 3\n\n  @property\n  def texture(self):\n    return 'cow'\n\n  def update(self):\n    if self.health <= 0:\n      self.world.remove(self)\n    if self.random.uniform() < 0.5:\n      direction = self.random_dir()\n      self.move(direction)\n\n\nclass Zombie(Object):\n\n  def __init__(self, world, pos, player):\n    super().__init__(world, pos)\n    self.player = player\n    self.health = 5\n    self.cooldown = 0\n\n  @property\n  def texture(self):\n    return 'zombie'\n\n  def update(self):\n    if self.health <= 0:\n      self.world.remove(self)\n    dist = self.distance(self.player)\n    if dist <= 8 and self.random.uniform() < 0.9:\n      self.move(self.toward(self.player, self.random.uniform() < 0.8))\n    else:\n      self.move(self.random_dir())\n    dist = self.distance(self.player)\n    if dist <= 1:\n      if self.cooldown:\n        self.cooldown -= 1\n      else:\n        if self.player.sleeping:\n          damage = 7\n        else:\n          damage = 2\n        self.player.health -= damage\n        self.cooldown = 5\n\n\nclass Skeleton(Object):\n\n  def __init__(self, world, pos, player):\n    super().__init__(world, pos)\n    self.player = player\n    self.health = 3\n    self.reload = 0\n\n  @property\n  def texture(self):\n    return 'skeleton'\n\n  def update(self):\n    if self.health <= 0:\n      self.world.remove(self)\n    self.reload = max(0, self.reload - 1)\n    dist = self.distance(self.player.pos)\n    if dist <= 3:\n      moved = self.move(-self.toward(self.player, self.random.uniform() < 0.6))\n      if moved:\n        return\n    if dist <= 5 and self.random.uniform() < 0.5:\n      self._shoot(self.toward(self.player))\n    elif dist <= 8 and self.random.uniform() < 0.3:\n      self.move(self.toward(self.player, self.random.uniform() < 0.6))\n    elif self.random.uniform() < 0.2:\n      self.move(self.random_dir())\n\n  def _shoot(self, direction):\n    if self.reload > 0:\n      return\n    if direction[0] == 0 and direction[1] == 0:\n      return\n    pos = self.pos + direction\n    if self.is_free(pos, Arrow.walkable):\n      self.world.add(Arrow(self.world, pos, direction))\n      self.reload = 4\n\n\nclass Arrow(Object):\n\n  def __init__(self, world, pos, facing):\n    super().__init__(world, pos)\n    self.facing = facing\n\n  @property\n  def texture(self):\n    return {\n        (-1, 0): 'arrow-left',\n        (+1, 0): 'arrow-right',\n        (0, -1): 'arrow-up',\n        (0, +1): 'arrow-down',\n    }[tuple(self.facing)]\n\n  @engine.staticproperty\n  def walkable():\n    return constants.walkable + ['water', 'lava']\n\n  def update(self):\n    target = self.pos + self.facing\n    material, obj = self.world[target]\n    if obj:\n      obj.health -= 2\n      self.world.remove(self)\n    elif material not in self.walkable:\n      self.world.remove(self)\n      if material in ['table', 'furnace']:\n        self.world[target] = 'path'\n    else:\n      self.move(self.facing)\n\n\nclass Plant(Object):\n\n  def __init__(self, world, pos):\n    super().__init__(world, pos)\n    self.health = 1\n    self.grown = 0\n\n  @property\n  def texture(self):\n    if self.ripe:\n      return 'plant-ripe'\n    else:\n      return 'plant'\n\n  @property\n  def ripe(self):\n    return self.grown > 300\n\n  def update(self):\n    self.grown += 1\n    objs = [self.world[self.pos + dir_][1] for dir_ in self.all_dirs]\n    if any(isinstance(obj, (Zombie, Skeleton, Cow)) for obj in objs):\n      self.health -= 1\n    if self.health <= 0:\n      self.world.remove(self)\n\n\nclass Fence(Object):\n\n  def __init__(self, world, pos):\n    super().__init__(world, pos)\n\n  @property\n  def texture(self):\n    return 'fence'\n\n  def update(self):\n    pass\n"}
{"type": "source_file", "path": "src/smartplay/crafter/crafter/worldgen.py", "content": "import functools\n\nimport numpy as np\nimport opensimplex\n\nfrom . import constants\nfrom . import objects\n\n\ndef generate_world(world, player):\n  simplex = opensimplex.OpenSimplex(seed=world.random.randint(0, 2 ** 31 - 1))\n  tunnels = np.zeros(world.area, bool)\n  for x in range(world.area[0]):\n    for y in range(world.area[1]):\n      _set_material(world, (x, y), player, tunnels, simplex)\n  for x in range(world.area[0]):\n    for y in range(world.area[1]):\n      _set_object(world, (x, y), player, tunnels)\n\n\ndef _set_material(world, pos, player, tunnels, simplex):\n  x, y = pos\n  simplex = functools.partial(_simplex, simplex)\n  uniform = world.random.uniform\n  start = 4 - np.sqrt((x - player.pos[0]) ** 2 + (y - player.pos[1]) ** 2)\n  start += 2 * simplex(x, y, 8, 3)\n  start = 1 / (1 + np.exp(-start))\n  water = simplex(x, y, 3, {15: 1, 5: 0.15}, False) + 0.1\n  water -= 2 * start\n  mountain = simplex(x, y, 0, {15: 1, 5: 0.3})\n  mountain -= 4 * start + 0.3 * water\n  if start > 0.5:\n    world[x, y] = 'grass'\n  elif mountain > 0.15:\n    if (simplex(x, y, 6, 7) > 0.15 and mountain > 0.3):  # cave\n      world[x, y] = 'path'\n    elif simplex(2 * x, y / 5, 7, 3) > 0.4:  # horizonal tunnle\n      world[x, y] = 'path'\n      tunnels[x, y] = True\n    elif simplex(x / 5, 2 * y, 7, 3) > 0.4:  # vertical tunnle\n      world[x, y] = 'path'\n      tunnels[x, y] = True\n    elif simplex(x, y, 1, 8) > 0 and uniform() > 0.85:\n      world[x, y] = 'coal'\n    elif simplex(x, y, 2, 6) > 0.4 and uniform() > 0.75:\n      world[x, y] = 'iron'\n    elif mountain > 0.18 and uniform() > 0.994:\n      world[x, y] = 'diamond'\n    elif mountain > 0.3 and simplex(x, y, 6, 5) > 0.35:\n      world[x, y] = 'lava'\n    else:\n      world[x, y] = 'stone'\n  elif 0.25 < water <= 0.35 and simplex(x, y, 4, 9) > -0.2:\n    world[x, y] = 'sand'\n  elif 0.3 < water:\n    world[x, y] = 'water'\n  else:  # grassland\n    if simplex(x, y, 5, 7) > 0 and uniform() > 0.8:\n      world[x, y] = 'tree'\n    else:\n      world[x, y] = 'grass'\n\n\ndef _set_object(world, pos, player, tunnels):\n  x, y = pos\n  uniform = world.random.uniform\n  dist = np.sqrt((x - player.pos[0]) ** 2 + (y - player.pos[1]) ** 2)\n  material, _ = world[x, y]\n  if material not in constants.walkable:\n    pass\n  elif dist > 3 and material == 'grass' and uniform() > 0.985:\n    world.add(objects.Cow(world, (x, y)))\n  elif dist > 10 and uniform() > 0.993:\n    world.add(objects.Zombie(world, (x, y), player))\n  elif material == 'path' and tunnels[x, y] and uniform() > 0.95:\n    world.add(objects.Skeleton(world, (x, y), player))\n\n\ndef _simplex(simplex, x, y, z, sizes, normalize=True):\n  if not isinstance(sizes, dict):\n    sizes = {sizes: 1}\n  value = 0\n  for size, weight in sizes.items():\n    if hasattr(simplex, 'noise3d'):\n      noise = simplex.noise3d(x / size, y / size, z)\n    else:\n      noise = simplex.noise3(x / size, y / size, z)\n    value += weight * noise\n  if normalize:\n    value /= sum(sizes.values())\n  return value\n"}
{"type": "source_file", "path": "src/smartplay/minedojo/minedojo_env.py", "content": "import gym\nfrom gym import error, spaces, utils\nfrom gym.utils import seeding\nfrom ..utils import HistoryTracker, describe_act\n\nimport random\nimport itertools\nimport numpy as np\nimport math\nimport cc3d\nimport minedojo\n\nbiomes_dict = {\n    0: 'Ocean',\n    1: 'Plains',\n    2: 'Desert',\n    3: 'Extreme Hills',\n    4: 'Forest',\n    5: 'Taiga',\n    6: 'Swampland',\n    7: 'River',\n    8: 'Hell (The Nether)',\n    9: 'The End',\n    10: 'FrozenOcean',\n    11: 'FrozenRiver',\n    12: 'Ice Plains',\n    13: 'Ice Mountains',\n    14: 'MushroomIsland',\n    15: 'MushroomIslandShore',\n    16: 'Beach',\n    17: 'DesertHills',\n    18: 'ForestHills',\n    19: 'TaigaHills',\n    20: 'Extreme Hills Edge',\n    21: 'Jungle',\n    22: 'JungleHills',\n    23: 'JungleEdge',\n    24: 'Deep Ocean',\n    25: 'Stone Beach',\n    26: 'Cold Beach',\n    27: 'Birch Forest',\n    28: 'Birch Forest Hills',\n    29: 'Roofed Forest',\n    30: 'Cold Taiga',\n    31: 'Cold Taiga Hills',\n    32: 'Mega Taiga',\n    33: 'Mega Taiga Hills',\n    34: 'Extreme Hills+',\n    35: 'Savanna',\n    36: 'Savanna Plateau',\n    37: 'Mesa',\n    38: 'Mesa Plateau F',\n    39: 'Mesa Plateau',\n    127: 'The Void',\n    129: 'Sunflower Plains',\n    130: 'Desert M',\n    131: 'Extreme Hills M',\n    132: 'Flower Forest',\n    133: 'Taiga M',\n    134: 'Swampland M',\n    140: 'Ice Plains Spikes',\n    149: 'Jungle M',\n    151: 'JungleEdge M',\n    155: 'Birch Forest M',\n    156: 'Birch Forest Hills M',\n    157: 'Roofed Forest M',\n    158: 'Cold Taiga M',\n    160: 'Mega Spruce Taiga',\n    161: 'Redwood Taiga Hills M',\n    162: 'Extreme Hills+ M',\n    163: 'Savanna M',\n    164: 'Savanna Plateau M',\n    165: 'Mesa (Bryce)',\n    166: 'Mesa Plateau F M',\n    167: 'Mesa Plateau M'\n}\n\nyaw_granularity = 6\npitch_granularity = 6\nFOV = 96\n\npitch_cnt = len(np.arange(-FOV//2, FOV//2+1, pitch_granularity))\nyaw_cnt = len(np.arange(-FOV//2, FOV//2+1, yaw_granularity))\n\ndef get_direction(yaw, pitch):\n    # Convert yaw and pitch to radians\n    yaw_rad = math.radians(yaw)\n    pitch_rad = math.radians(pitch)\n    direction_list = []\n\n    # Calculate the x, y, and z components of the direction vector\n    x = math.sin(yaw_rad)* math.cos(pitch_rad)\n    y = math.sin(pitch_rad)\n    z = math.cos(yaw_rad) * math.cos(pitch_rad)\n\n    # Determine the direction based on the sign of the x, y, and z components\n    if y > 0.2:\n        direction = \"above you\"\n    elif y < -0.2:\n        direction = \"below you\"\n    else:\n        direction = \"at your level\"\n\n    if z > 0.2:\n        direction_list.append(\"north\")\n    elif z < -0.2:\n        direction_list.append(\"south\")\n    \n    if x > 0.2:\n        direction_list.append(\"east\")\n    elif x < -0.2:\n        direction_list.append(\"west\")\n\n    return direction + \" to \" + (\"front\" if \"-\".join(direction_list) == \"\" else \"-\".join(direction_list))\n\ndef describe_surround(obs):\n    # print(\"What blocks are around me?\")\n    # print()\n    result = \"Around you:\\n\"\n    \n    # Voxel size (Can be changed based on the voxel space)\n    voxel_x_size = 9\n    voxel_y_size = 9\n    voxel_z_size = 9\n    voxel_center_x = voxel_x_size // 2\n    voxel_center_y = voxel_y_size // 2\n    voxel_center_z = voxel_z_size // 2\n    \n    # Convert the 3D block name list into np array\n    block_names = np.array(obs['voxels']['block_name'])\n    \n    # Only preserve blocks 1 meter below me\n    block_names = block_names[:, voxel_z_size // 2 - 1:, :]\n\n    # Encode the block name using its unique index\n    unique_block_names, unique_block_indices = np.unique(block_names, return_inverse=True)\n    block_names_int = np.array(unique_block_indices).reshape(\n        block_names.shape)\n\n    # Apply 3D connected component and extract the number of labels\n    block_names_labels, labels_count = cc3d.connected_components(block_names_int, return_N = True, connectivity = 6)\n\n    # Get the centroids of each label\n    centroids = cc3d.statistics(block_names_labels)['centroids']\n\n    # Get the map: label -> block name\n    label_2_block_name = [\"\"] * (labels_count + 1)\n    for i in range(voxel_x_size):\n        for j in range(block_names.shape[1]):\n            for k in range(voxel_y_size):\n                label = block_names_labels[i][j][k]\n                block_name = block_names[i][j][k]\n                label_2_block_name[label] = block_name\n\n    # Describe the surrounding environment based on the connected component labels\n    # Helper lists for printing\n    x_axis_list = [\"west\", \"east\"]\n    y_axis_list = [\"north\", \"south\"]\n    z_axis_list = [\"below you\", \"above you\"] # The height of me is defined as the height of my feet\n    \n    # The map: block name -> min distance\n    block2dist = dict()\n    \n    # The map: block name -> direction\n    block2dir = dict()\n    \n    # Get the direction of each label based on the centroid\n    for i in range(labels_count + 1):\n        middle_point = centroids[i]\n        block_name = label_2_block_name[i]\n\n        # Skip the air\n        if block_name == 'air': continue\n\n        # Get the indices to the printing help list\n        x_coord = 1 if middle_point[0] > voxel_center_x else 0\n        z_coord = 1 if middle_point[1] > 1 else 0\n        y_coord = 1 if middle_point[2] > voxel_center_y else 0\n\n        is_x_center = math.isclose(middle_point[0], voxel_center_x)\n        is_z_center = math.isclose(middle_point[1], 1)\n        is_y_center = math.isclose(middle_point[2], voxel_center_y)\n\n        # If the component is exactly centered, skip\n        if is_x_center and is_y_center and is_z_center:\n            continue\n        \n        # Update the min distance and correspoding direction for the same block name\n        distance = math.sqrt((middle_point[0] - voxel_center_x)**2 + (middle_point[1] - 1)**2 + (middle_point[2] - voxel_center_y)**2)\n        if block_name not in block2dist or distance < block2dist[block_name]:\n            block2dist[block_name] = distance\n            \n            # Record the direction of this component\n            direction_str_list = []\n\n            if not is_z_center:\n                direction_str_list.append(z_axis_list[z_coord])\n            else:\n                direction_str_list.append(\"\")\n\n            if not is_y_center:\n                direction_str_list.append(y_axis_list[y_coord])\n\n            # Skip when the coordinate is at the voxel center\n            if not is_x_center:\n                direction_str_list.append(x_axis_list[x_coord])\n\n            direction_str = direction_str_list[0] + \" to \"+ ('-'.join(direction_str_list[1:]) if '-'.join(direction_str_list[1:]) != \"\" else \"front\")\n            \n            block2dir[block_name] = direction_str.strip()\n        \n    # Finally print the blocks\n    for block_name in block2dist:\n        direction_str = block2dir[block_name]\n        result += f\" - {block_name}, {'%.2f' % block2dist[block_name]} blocks away, {direction_str}\\n\"\n        \n    return result.strip()\n\n# Describe the block the agent is facing to\n\ndef describe_cursor(obs):\n    \n    # Track the cursor using lidar\n    cursor_block = obs[\"rays\"][\"block_name\"][0]\n    \n    # Skip if the cursor is pointing to air\n    if cursor_block == 'air':\n        return \"You're not aiming at any block.\"\n    else:\n        return f\"You're aiming at {cursor_block}.\"\n\n# Describe the surrounding entities around the agent\n\ndef describe_entity(obs):\n    result = \"\"\n    my_yaw = obs[\"location_stats\"][\"yaw\"]\n    my_pitch = obs[\"location_stats\"][\"pitch\"]\n    \n    # Number of pitch/yaw rays (Can be changed based on the lidar rays)\n    \n    # A flag indicating where there are entities around\n    see_entity = False\n\n    # Reshape and convert the list into np array\n    entity_names = np.array(obs[\"rays\"][\"entity_name\"][1:].reshape(pitch_cnt,yaw_cnt))\n\n    # Encode the block name using its unique index\n    unique_entity_names, unique_entity_indices = np.unique(entity_names, return_inverse=True)\n    entity_names_int = np.array(unique_entity_indices).reshape(entity_names.shape)\n\n    # Apply 2D connected components\n    entity_names_labels, labels_count = cc3d.connected_components(entity_names_int, return_N = True, connectivity = 8)\n\n    # Get the map: label -> entity name\n    label_2_entity_name = [\"\"] * (labels_count + 1)\n    for i in range(pitch_cnt):\n        for j in range(yaw_cnt):\n            label = entity_names_labels[i][j]\n            entity_name = entity_names[i][j]\n            label_2_entity_name[label] = entity_name\n\n    # Describe each component\n    for i in range(labels_count + 1):\n        entity_name = label_2_entity_name[i]\n\n        # Skip the null entity\n        if entity_name == \"null\": \n            continue\n        else: \n            see_entity = True\n\n        # Find all the indices of this component\n        all_idx = np.where(entity_names_labels == i)\n        amount = np.sum(entity_names_labels == i)\n\n        # Find the minimum distance inside each component\n        min_distance = math.inf\n        direction = \"\"\n        for row, col in zip(all_idx[0], all_idx[1]):\n            index = row * yaw_cnt + col\n            distance = obs[\"rays\"][\"entity_distance\"][index+1]\n            if distance < min_distance:\n                min_distance = distance\n                yaw = ((col / (yaw_cnt-1)) * FOV - FOV/2 + my_yaw + 540) % 360 - 180\n                pitch = (row / (pitch_cnt-1)) * 180 - 90 + my_pitch\n                direction = get_direction(yaw, pitch)\n\n        amount_description = \"taking {0:.0f}% of screen\".format(amount/(pitch_cnt*yaw_cnt)*100)\n\n        result+=f\" - {entity_name}, {'%.2f' % min_distance} blocks away, {direction}, {amount_description}\\n\"\n        \n    # If there are no surrounding entities that the agent can see\n    if not see_entity:\n        result = \"\"\n    \n    return result\n\n# Describe the surrounding entities around the agent\n\ndef describe_obj(obs):\n    result = \"You see:\\n\"\n    my_yaw = obs[\"location_stats\"][\"yaw\"]\n    my_pitch = obs[\"location_stats\"][\"pitch\"]\n    \n    # Number of pitch/yaw rays (Can be changed based on the lidar rays)\n    \n    # A flag indicating where there are entities around\n    see_entity = False\n\n    # Reshape and convert the list into np array\n    entity_names = np.array(obs[\"rays\"][\"block_name\"][1:].reshape(pitch_cnt,yaw_cnt))\n\n    # Encode the block name using its unique index\n    unique_entity_names, unique_entity_indices = np.unique(entity_names, return_inverse=True)\n    entity_names_int = np.array(unique_entity_indices).reshape(entity_names.shape)\n\n    # Apply 2D connected components\n    entity_names_labels, labels_count = cc3d.connected_components(entity_names_int, return_N = True, connectivity = 8)\n\n    # Get the map: label -> entity name\n    label_2_entity_name = [\"\"] * (labels_count + 1)\n    for i in range(pitch_cnt):\n        for j in range(yaw_cnt):\n            label = entity_names_labels[i][j]\n            entity_name = entity_names[i][j]\n            label_2_entity_name[label] = entity_name\n\n    # Describe each component\n    for i in range(labels_count + 1):\n        entity_name = label_2_entity_name[i]\n\n        # Skip the null entity\n        if entity_name in (\"null\", \"air\"): \n            continue\n        else: \n            see_entity = True\n\n        # Find all the indices of this component\n        all_idx = np.where(entity_names_labels == i)\n        amount = np.sum(entity_names_labels == i)\n\n        # Find the minimum distance inside each component\n        min_distance = math.inf\n        direction = \"\"\n        for row, col in zip(all_idx[0], all_idx[1]):\n            index = row * yaw_cnt + col\n            distance = obs[\"rays\"][\"block_distance\"][index+1]\n            if distance < min_distance:\n                min_distance = distance\n                yaw = ((col / (yaw_cnt-1)) * FOV - FOV/2 + my_yaw + 540) % 360 - 180\n                pitch = ((row / (pitch_cnt-1)) * 180 - 90) + my_pitch\n                direction = get_direction(yaw, pitch)\n        \n        amount_description = \"taking {0:.0f}% of screen\".format(amount/(pitch_cnt*yaw_cnt)*100)\n\n        result+=f\" - {entity_name}, {'%.2f' % min_distance} blocks away, {direction}, {amount_description}\\n\"\n        \n    # If there are no surrounding entities that the agent can see\n    if not see_entity:\n        result = \"\"\n\n    return result.strip()\n\n# Describe the exact coordinate the agent is in\ndef describe_location(obs):\n    result = \"\"\n\n    coord_list = obs[\"location_stats\"][\"pos\"]\n    \n    # Describe the direction the agent is currently facing\n    yaw = (obs[\"location_stats\"][\"yaw\"]+180) % 360 - 180\n    \n    direction_list = [\"north\", \"north-east\", \"east\", \"south-east\", \"south\", \"south-west\", \"west\", \"north-west\"]\n    direction_index = (int((yaw // 22.5) + 1)) // 2 # Calculate the index mapping to the direction_list above\n    if direction_index == 4:\n        direction_index *= -1\n    direction_index += 4\n\n    result = f\"Coordinate ({'%.2f' % coord_list[0]},{'%.2f' % coord_list[1]},{'%.2f' % coord_list[2]}). Facing {direction_list[direction_index]}.\"    # Describe whether the agent is looking up or down\n    pitch = obs[\"location_stats\"][\"pitch\"]\n\n    if pitch < 0:\n        result += \" Looking up.\"\n    elif pitch > 0:\n        result += \" Looking down.\"\n\n    return result.strip()\n\ndef describe_frame(obs):\n    return describe_location(obs)+\"\\n\\n\"+describe_cursor(obs)+\"\\n\"+describe_surround(obs)+\"\\n\"+describe_entity(obs)+\"\\n\"+describe_obj(obs)\n\nclass MineDojoEnv(gym.Env):\n    default_iter = 20\n    default_steps = 200\n\n    def __init__(self, task_id, max_steps=2):\n\n        # Creative task\n        self._env = minedojo.make(\n            task_id=\"creative:{}\".format(task_id),\n            fast_reset=False,\n            image_size=(160, 256),\n            use_voxel=True,\n            voxel_size=dict(xmin=-4, ymin=-4, zmin=-4, xmax=4, ymax=4, zmax=4), # Set voxel space to be 9*9*9\n            use_lidar=True,\n            lidar_rays=[(0,0,3)] + [\n                    (np.pi * pitch / 180, np.pi * yaw / 180, 65)\n                    for pitch in np.arange(-FOV//2, FOV//2+1, pitch_granularity)\n                    for yaw in np.arange(-FOV//2, FOV//2+1, yaw_granularity)\n            ]   # Track the agent's cursor\n        )\n        goal = self._env.task_prompt.replace(\".\", \"\")[self._env.task_prompt.find('find ')+len('find '):]\n        self.goal_set = {k for k,v in biomes_dict.items() if goal in v.lower()}\n        self.action_list = [\"Move North\", \"Move East\", \"Move South\", \"Move West\"]\n        self.noop_step=50\n\n        self.history = HistoryTracker(max_steps)\n        self.desc = \"\"\"\nYou are in Minecraft and your goal is to find a {} biome. You are not allowed to craft anything.\n\nIn your observation, you are provided the amount of space an object takes in your field of view. Note that objects of the same size takes more space when they are closer to you.\n\n{}\n\"\"\".format(goal, describe_act(self.action_list)).strip()\n        \n    def get_turn_action(self, act, obs):\n        yaw = obs[\"location_stats\"][\"yaw\"]\n        # Compute yaw delta to face the yaw indicated by the action 'act'\n        action_yaw = act * 90 - 180\n        yaw_delta = (action_yaw - yaw + 180) % 360 // 15\n        action = self._env.action_space.no_op()\n        action[4] = yaw_delta\n        return action\n\n    def go_forward_action(self):\n        action = self._env.action_space.no_op()\n        action[0] = 1 # Move forward\n        action[1] = 0 if random.random() < 0.5 else (1 if random.random() < 0.5 else 2) # Move left/right with prob=0.5 to get unstuck\n        action[2] = 3 if random.random() < 0.5 else 1 # Sprint with prob=0.5\n        return action\n\n    def describe(self, obs, action=None):\n        if action is not None:\n            return \"You took action {}.\\n\\n{}\".format(self.action_list[action], describe_frame(obs))\n        else:\n            return describe_frame(obs)\n\n    def reset(self):\n        obs = self._env.reset()\n        self.history.reset()\n        info = {\n            \"obs\": self.describe(obs),\n            \"manual\": self.desc,\n            \"history\": self.history.describe(),\n            \"score\": 0,\n            \"completed\": 0,\n            }\n        self.history.step(info)\n        self.obs = obs\n        return obs, info\n\n    def step(self, action):\n        R = 0\n        obs, reward, done, info = self._env.step(self.get_turn_action(action, self.obs))\n        R += reward\n        for _ in range(self.noop_step):\n            obs, reward, done, info = self._env.step(self.go_forward_action())\n            R += reward\n            if obs[\"location_stats\"][\"biome_id\"].item() in self.goal_set:\n                break\n\n        info.update({\n            \"obs\": self.describe(obs, action),\n            \"manual\": self.desc,\n            \"history\": self.history.describe(),\n            \"score\": 0,\n            \"completed\": 0,\n            })\n        if obs[\"location_stats\"][\"biome_id\"].item() in self.goal_set:\n            info[\"completed\"] = 1\n            info[\"score\"] = 1\n            done = True\n        self.history.step(info)\n        self.obs = obs\n        return obs, R, done, info\n"}
{"type": "source_file", "path": "src/smartplay/messenger_emma/messenger_env.py", "content": "import gym\nfrom gym import error, spaces, utils\nfrom gym.utils import seeding\nfrom ..utils import HistoryTracker, describe_act\n\nimport random\nimport itertools\nimport numpy as np\nimport messenger\nimport messenger.envs\n\n\n''' Format function passed to numpy print to make things pretty.\n'''\n\nid_map = {}\n\nfor ent in messenger.envs.config.ALL_ENTITIES:\n    id_map[ent.id] = ent.name\nid_map[0] = '  '\nid_map[15] = 'you (agent) without the message'\nid_map[16] = 'you (agent) with the message'\n\ndef describe_block(i):\n    if i < 17:\n        return id_map[i]\n    else:\n        return 'XX'\n\ndef describe_loc(ref, P):\n    desc = []\n    if ref[0] > P[0]:\n        desc.append(\"north\")\n    elif ref[0] < P[0]:\n        desc.append(\"south\")\n    if ref[1] > P[1]:\n        desc.append(\"west\")\n    elif ref[1] < P[1]:\n        desc.append(\"east\")\n\n    return \"-\".join(desc)\n\n    \ndef describe_frame(info):\n\n    if 15 in np.unique(info['avatar']):\n        obs = \"You (agent) don't have the message.\"\n        agent = 15\n    elif 16 in np.unique(info['avatar']):\n        obs = \"You (agent) already have the message.\"\n        agent = 16\n    else:\n        print(np.unique(info['avatar']))\n        raise NotImplemented(\"Problem with agent\")\n    center = np.array(np.where(info['avatar'].squeeze() == agent)).squeeze()\n    info = info['entities']\n    result = \"\"\n    x = np.arange(info.shape[1])\n    y = np.arange(info.shape[0])\n    x1, y1 = np.meshgrid(x,y)\n    loc = np.stack((y1, x1),axis=-1)\n    dist = np.absolute(center-loc).sum(axis=-1)[:,:,np.newaxis]\n\n    obj_info_list = []\n    \n    \n    for idx in np.unique(info):\n        if idx == 15 or idx == 0:\n            continue\n        smallest = np.unravel_index(np.argmin(np.where(info==idx, dist, np.inf)), info.shape)\n        obj_info_list.append((describe_block(idx), dist[smallest[:2]][0], describe_loc(np.array([0,0]), smallest[:2]-center)))\n\n    if len(obj_info_list)>0:\n        status_str = \"You see:\\n{}\".format(\"\\n\".join([\"- {} {} steps to your {}\".format(name, dist, loc) if dist>0 else \"- {} 0 steps with you\".format(name) for name, dist, loc in obj_info_list]))\n    else:\n        status_str = \"You see nothing away from you.\"\n    result += obs.strip() + \"\\n\\n\" + status_str.strip()\n    \n    return result.strip()\n\nclass MessengerEnv(gym.Env):\n    default_iter = 100\n    default_steps = None\n\n    def __init__(self, lvl=1, max_steps=2, env_noise=0):\n\n        lvl_to_steps = [4, 64, 128]\n        self.default_steps = lvl_to_steps[lvl-1]\n        env_id = 'msgr-test-v{}'.format(lvl)\n        self._env = gym.make(env_id)\n        self.action_list = [\"Move North\", \"Move South\", \"Move West\", \"Move East\", \"Do Nothing\"]\n\n        self.history = HistoryTracker(max_steps)\n        self.game_context = \"\"\"In the game, MESSENGER, each entity can take on one of three roles: an enemy, message, or goal. The agentâ€™s objective is to bring the message to the goal while avoiding the enemies. If the agent encounters an enemy at any point in the game, or the goal without first obtaining the message, it loses the game and obtains a reward of âˆ’1.\"\"\".strip()\n        self.advice = \"\"\"To solve a game, you may find it helpful to list the objects that you see. Then for each object, match it with an entity description, and identify whether it is good or bad to interact with the object.\nThe name specifications of in-game objects may not be exact matches. Please try identifying with synonyms.\n\"\"\".strip()\n\n    def _update_manual(self, manual):\n        self.desc = \"{}\\n\\n{}\\n\\n{}\\n\\n{}\".format(self.game_context, \"\\n\".join(manual), self.advice, describe_act(self.action_list)).strip()\n\n    def describe(self, obs, action=None):\n        if action is not None:\n            return \"You took action {}.\\n\\n{}\".format(self.action_list[action], describe_frame(obs))\n        else:\n            return describe_frame(obs)\n\n    def reset(self):\n        obs, manual = self._env.reset()\n        self._update_manual(manual)\n        self.history.reset()\n        info = {\n            \"obs\": self.describe(obs),\n            \"manual\": self.desc,\n            \"history\": self.history.describe(),\n            \"score\": 0,\n            \"completed\": 0,\n            }\n        self.history.step(info)\n        return obs, info\n\n    def step(self, action):\n        obs, reward, done, info = self._env.step(action)\n        try:\n            description = self.describe(obs, action)\n        except:\n            description = \"Environment Error.\"\n        info={\n            \"obs\": description,\n            \"manual\": self.desc,\n            \"history\": self.history.describe(),\n            \"score\": reward,\n            \"completed\": int(reward==1 and done),\n            }\n        self.history.step(info)\n        return obs, reward, done, info\n"}
{"type": "source_file", "path": "src/smartplay/hanoi/hanoi_env.py", "content": "import gym\nfrom gym import error, spaces, utils\nfrom gym.utils import seeding\nfrom ..utils import HistoryTracker, describe_act\n\nimport random\nimport itertools\nimport numpy as np\n\naction_to_move = [(0, 1), (0, 2), (1, 0),\n                  (1, 2), (2, 0), (2, 1)]\n\nclass HanoiEnv(gym.Env):\n    default_iter = 10\n    default_steps = 30\n\n    def __init__(self, max_steps=5, num_disks=4, env_noise=0):\n        self.num_disks = num_disks\n        self.env_noise = env_noise\n        self.action_space = spaces.Discrete(6)\n        self.observation_space = spaces.Tuple(self.num_disks*(spaces.Discrete(3),))\n\n        self.current_state = None\n        self.goal_state = self.num_disks*(2,)\n        self.history = HistoryTracker(max_steps)\n\n        self.done = None\n        self.ACTION_LOOKUP = {0 : \"(0,1) - top disk of pole 0 to top of pole 1 \",\n                              1 : \"(0,2) - top disk of pole 0 to top of pole 2 \",\n                              2 : \"(1,0) - top disk of pole 1 to top of pole 0\",\n                              3 : \"(1,2) - top disk of pole 1 to top of pole 2\",\n                              4 : \"(2,0) - top disk of pole 2 to top of pole 0\",\n                              5 : \"(2,1) - top disk of pole 2 to top of pole 1\"}\n\n\n        self.action_list = [\n        \"Move the top disk of rod A to the top of rod B\",\n        \"Move the top disk of rod A to the top of rod C\",\n        \"Move the top disk of rod B to the top of rod A\",\n        \"Move the top disk of rod B to the top of rod C\",\n        \"Move the top disk of rod C to the top of rod A\",\n        \"Move the top disk of rod C to the top of rod B\",\n        ]\n      \n        self.desc = \"\"\"\nThe game consists of three rods (A,B,C) and a number of disks of various sizes, which can go onto any rod. \nThe game begins with the disks stacked on rod A in order of decreasing size, the smallest at the top (righthand side). \nThe objective is to move the entire stack to rod C, obeying the following rules:\n\n - Only one disk may be moved at a time.\n - Each move consists of taking the top disk from one of the stacks and placing it on top of another stack or on an empty rod.\n - You cannot place a bigger disk on top of a smaller disk.\n\nFor example, considering movements from B under the following setting:\n- A: |bottom, [0], top|\n- B: |bottom, [1], top|\n- C: |bottom, [2], top|\nYou are only allowed to move from B to C but not A, since the top of B (1) is smaller than the top of C (2) but bigger than the top of A (0).\n\nFinally, the starting configuration is:\n- A: |bottom, {}, top|\n- B: |bottom, [], top|\n- C: |bottom, [], top|\n\nand the goal configuration is:\n- A: |bottom, [], top|\n- B: |bottom, [], top|\n- C: |bottom, {}, top|\nwith top on the right and bottom on the left\n\n{}\n\"\"\".format(list(range(num_disks))[::-1],list(range(num_disks))[::-1], describe_act(self.action_list)).strip()\n\n    def step(self, action):\n        \"\"\"\n        * Inputs:\n            - action: integer from 0 to 5 (see ACTION_LOOKUP)\n        * Outputs:\n            - current_state: state after transition\n            - reward: reward from transition\n            - done: episode state\n            - info: dict of booleans (noisy?/invalid action?)\n        0. Check if transition is noisy or not\n        1. Transform action (0 to 5 integer) to tuple move - see Lookup\n        2. Check if move is allowed\n        3. If it is change corresponding entry | If not return same state\n        4. Check if episode completed and return\n        \"\"\"\n        if self.done:\n            raise RuntimeError(\"Episode has finished. Call env.reset() to start a new episode.\")\n\n        info = {\"transition_failure\": False,\n                \"invalid_action\": False}\n\n        if self.env_noise > 0:\n            r_num = random.random()\n            if r_num <= self.env_noise:\n                action = random.randint(0, self.action_space.n-1)\n                info[\"transition_failure\"] = True\n\n        move = action_to_move[action]\n\n        if self.move_allowed(move):\n            disk_to_move = min(self.disks_on_peg(move[0]))\n            moved_state = list(self.current_state)\n            moved_state[disk_to_move] = move[1]\n            self.current_state = tuple(moved_state)\n        else:\n            info[\"invalid_action\"] = True\n\n        if self.current_state == self.goal_state:\n            reward = 100\n            self.done = True\n        elif info[\"invalid_action\"] == True:\n            reward = -1\n        else:\n            reward = 0\n\n        info[\"state\"] = (self.disks_on_peg(0), self.disks_on_peg(1), self.disks_on_peg(2))\n        info[\"score\"] = len(self.disks_on_peg(2))\n        info[\"manual\"] = self.desc\n        info[\"obs\"] = self.describe_state(info, action)\n        info[\"history\"] = self.history.describe()\n        info[\"completed\"] = 0 if not self.done else 1\n        self.history.step(info)\n\n        return self.current_state, reward, self.done, info\n\n    def describe_state(self, state, action=None):\n        index = ['A', 'B', 'C']\n        if action is not None:\n            result = \"You tried to {}. Current configuration:\".format(self.action_list[action].lower())\n        else:\n            result = \"Current configuration:\"\n        for i in range(3):\n            result += \"\\n- {}: |bottom, {}, top|\".format(index[i], state['state'][i][::-1])\n        return result.strip()\n\n    def disks_on_peg(self, peg):\n        \"\"\"\n        * Inputs:\n            - peg: pole to check how many/which disks are in it\n        * Outputs:\n            - list of disk numbers that are allocated on pole\n        \"\"\"\n        return [disk for disk in range(self.num_disks) if self.current_state[disk] == peg]\n\n    def move_allowed(self, move):\n        \"\"\"\n        * Inputs:\n            - move: tuple of state transition (see ACTION_LOOKUP)\n        * Outputs:\n            - boolean indicating whether action is allowed from state!\n        move[0] - peg from which we want to move disc\n        move[1] - peg we want to move disc to\n        Allowed if:\n            * discs_to is empty (no disc of peg) set to true\n            * Smallest disc on target pole larger than smallest on prev\n        \"\"\"\n        disks_from = self.disks_on_peg(move[0])\n        disks_to = self.disks_on_peg(move[1])\n\n        if disks_from:\n            return (min(disks_to) > min(disks_from)) if disks_to else True\n        else:\n            return False\n\n    def reset(self):\n        self.current_state = self.num_disks * (0,)\n        self.done = False\n        self.history.reset()\n        info = {\"state\":(self.disks_on_peg(0), self.disks_on_peg(1), self.disks_on_peg(2))}\n        info[\"score\"] = len(self.disks_on_peg(2))\n        info[\"manual\"] = self.desc\n        info[\"obs\"] = self.describe_state(info)\n        info[\"history\"] = self.history.describe()\n        info[\"completed\"] = 0\n        self.history.step(info)\n        return self.current_state, info\n\n    def render(self, mode='human', close=False):\n        return\n\n    def set_env_parameters(self, num_disks=4, env_noise=0, verbose=True):\n        self.num_disks = num_disks\n        self.env_noise = env_noise\n        self.observation_space = spaces.Tuple(self.num_disks*(spaces.Discrete(3),))\n        self.goal_state = self.num_disks*(2,)\n\n        if verbose:\n            print(\"Hanoi Environment Parameters have been set to:\")\n            print(\"\\t Number of Disks: {}\".format(self.num_disks))\n            print(\"\\t Transition Failure Probability: {}\".format(self.env_noise))\n\n    def get_movability_map(self, fill=False):\n        # Initialize movability map\n        mov_map = np.zeros(self.num_disks*(3, ) + (6,))\n\n        if fill:\n            # Get list of all states as tuples\n            id_list = self.num_disks*[0] + self.num_disks*[1] + self.num_disks*[2]\n            states = list(itertools.permutations(id_list, self.num_disks))\n\n            for state in states:\n                for action in range(6):\n                    move = action_to_move[action]\n                    disks_from = []\n                    disks_to = []\n                    for d in range(self.num_disks):\n                        if state[d] == move[0]: disks_from.append(d)\n                        elif state[d] == move[1]: disks_to.append(d)\n\n                    if disks_from: valid = (min(disks_to) > min(disks_from)) if disks_to else True\n                    else: valid = False\n\n                    if not valid: mov_map[state][action] = -np.inf\n\n                    move_from = [m[0] for m in action_to_move]\n                    move_to = [m[1] for m in action_to_move]\n\n        return mov_map\n\nclass Hanoi3Disk(HanoiEnv):\n    \"\"\"Basic 3 disk Hanoi Environment\"\"\"\n    def __init__(self):\n        HanoiEnv.__init__(self, num_disks=3)\n\nclass Hanoi4Disk(HanoiEnv):\n    \"\"\"Basic 4 disk Hanoi Environment\"\"\"\n    def __init__(self):\n        HanoiEnv.__init__(self, num_disks=4)\n"}
{"type": "source_file", "path": "src/smartplay/eval.py", "content": "# Benchmark games used in the paper\nbenchmark_games_v0 = [\n    'RockPaperScissorBasic',\n    'BanditTwoArmedHighLowFixed',\n    'MessengerL1',\n    'MessengerL2',\n    'Hanoi3Disk',\n    'Crafter',\n    'MinedojoCreative0',\n]\n\n\nfrom . import recorded_settings, game_challenges\nimport pandas as pd\nimport numpy as np\n\n\ndef normalize_score(game_name, score):\n    # Normalize score to [0, 1] for a given environment\n\n    if game_name in recorded_settings.keys():\n        return (score - recorded_settings[game_name]['min score']) / (recorded_settings[game_name]['human score'] - recorded_settings[game_name]['min score'])\n    else:\n        raise ValueError('Environment `{}` does not have recorded settings.'.format(game_name))\n\n\ndef analyze_capabilities(score_dict):\n    # Analyze capabilities of a model based on the metric dictionary.\n    # The metric dictionary should have the following format:\n    # {\n    #     'game_name': score,\n    #     ...\n    # }\n    # The output is a dictionary with the following format:\n    # {\n    #     'game_name': capability,\n    #     ...\n    # }\n\n    challenges_df = pd.DataFrame(game_challenges)\n    challenges_df = challenges_df.loc[:, list(score_dict.keys())] - 1\n\n    normalized_scores = np.array(list(score_dict.values())).reshape(-1, 1)\n    scores = (challenges_df.values @ normalized_scores).squeeze() / np.sum(challenges_df.values, axis=1)\n\n    return dict(zip(challenges_df.index, scores.flatten()))"}
{"type": "source_file", "path": "src/smartplay/rock_paper_scissors/rock_paper_scissor.py", "content": "import numpy as np\nimport gym\nfrom gym import spaces\nfrom gym.utils import seeding\nfrom ..utils import HistoryTracker, describe_act\nimport random\n\n\nclass RPSEnv(gym.Env):\n    \"\"\"\n    Bandit environment base to allow agents to interact with the class n-armed bandit\n    in different variations\n\n    p_dist:\n        A list of probabilities of the likelihood that a particular bandit will pay out\n    r_dist:\n        A list of either rewards (if number) or means and standard deviations (if list)\n        of the payout that bandit has\n    \"\"\"\n\n    default_iter = 20\n    default_steps = 50\n\n    def __init__(self, probs, reward = [1, 1, 1], max_steps=50):\n\n        if min(probs) < 0 or max(probs) > 1 or sum(probs) != 1:\n            raise ValueError(\"All probabilities must be between 0 and 1, and sum to 1\")\n\n        dist = list(zip(probs, reward))\n        random.shuffle(dist)\n        self.probs = [d[0] for d in dist]\n\n        self.action_list = [\"Rock\", \"Paper\", \"Scissor\"]\n        self.reward = [d[1] for d in dist]\n        self._update_manual()\n        self.observation_space = spaces.Discrete(1)\n        self.action_space = spaces.Discrete(len(self.probs))\n        self.history = HistoryTracker(max_steps)\n        self.score_tracker = 0\n\n        self._seed()\n\n    def _update_manual(self):\n        self.desc = \"\"\"\nFor the game Rock Paper Scissors, you and the opponent choose one of three options: rock, paper, or scissors.\nAfter both players have chosen, the winner is determined as follows:\nRock crushes scissors (Rock wins, score {})\nScissors cut paper (Scissors win, score {})\nPaper covers rock (Paper wins, score {})\nIf you lose, your score is the negative of the winner's score.\nIf both players choose the same option, it's a draw (score 0).\nYour goal is to maximize your score.\n\n{}\n\"\"\".strip().format(*self.reward, describe_act(self.action_list))\n\n    def compute_optimal_action(self):\n        n_actions = len(self.probs)\n        expected_scores = np.zeros(n_actions)\n\n        for action in range(n_actions):\n            expected_score = 0\n            for opponent_action in range(n_actions):\n                expected_score += self.probs[opponent_action] * self.reward[opponent_action]\n                if (action == 0 and opponent_action == 1) or (action == 1 and opponent_action == 2) or (action == 2 and opponent_action == 0):\n                    expected_score -= self.probs[opponent_action] * self.reward[opponent_action]\n                elif action == opponent_action:\n                    pass\n                else:\n                    expected_score += self.probs[opponent_action] * self.reward[action]\n            expected_scores[action] = expected_score\n\n        optimal_action = np.argmax(expected_scores)\n        expected_score = expected_scores[optimal_action]\n\n        return optimal_action, expected_score\n\n    def sample_opponent_action(self, probs):\n        return np.random.choice(len(probs), p=probs)\n\n    def _seed(self, seed=None):\n        self.np_random, seed = seeding.np_random(seed)\n        return [seed]\n\n    def step(self, action):\n        assert self.action_space.contains(action)\n        opponent_action = self.sample_opponent_action(self.probs)\n        \n        if action == 0 and opponent_action == 1:\n            result = \"lost\"\n        elif action == 1 and opponent_action == 2:\n            result = \"lost\"\n        elif action == 2 and opponent_action == 0:\n            result = \"lost\"\n        elif action == opponent_action:\n            result = \"tied\"\n        else:\n            result = \"won\"\n\n        reward = self.reward[action] if result == \"won\" else -self.reward[opponent_action] if result == \"lost\" else 0\n        done = False\n        optimal_action, expected_score = self.compute_optimal_action()\n        self.score_tracker += int(action == optimal_action)\n        info = {\n            \"obs\": \"You chose {}, and the opponent chose {}. You {} and received score {}.\\nNew round begins.\".format(\n                self.action_list[action], \n                self.action_list[opponent_action], \n                result, reward),\n            \"manual\": self.desc,\n            \"history\": self.history.describe(),\n            \"score\": self.score_tracker,\n            \"result\": result,\n            \"completed\": 0,\n            }\n        self.history.step(info)\n\n        return 0, reward, done, info\n\n    def reset(self):\n        probs, reward = self.probs, self.reward\n        dist = list(zip(probs, reward))\n        random.shuffle(dist)\n        self.probs = [d[0] for d in dist]\n\n        self.reward = [d[1] for d in dist]\n        self._update_manual()\n        self.history.reset()\n        self.score_tracker = 0\n        info = {\n            \"obs\":\"New round begins.\",\n            \"manual\": self.desc,\n            \"history\": self.history.describe(),\n            \"score\": self.score_tracker,\n            \"completed\": 0,\n            }\n        self.history.step(info)\n        return 0, info\n\n    def render(self, mode='human', close=False):\n        pass\n\n\nclass RockPaperScissorBasic(RPSEnv):\n    \"\"\"Basic version of the game with biased opponent\"\"\"\n    def __init__(self):\n        RPSEnv.__init__(self, [0.2, 0.2, 0.6])\n\n\nclass RockPaperScissorDifferentScore(RPSEnv):\n    \"\"\"Biased opponent and different scores\"\"\"\n    def __init__(self):\n        RPSEnv.__init__(self, p_dist=[0.3, 0.3, 0.4], r_dist=[1, 2, 1])\n"}
{"type": "source_file", "path": "src/smartplay/utils.py", "content": "import copy\nclass HistoryTracker:\n\n    def __init__(self, max_steps) -> None:\n        self.max_steps = max_steps\n        self.game_step = 0\n        self.reset()\n\n    def step(self, info) -> None:\n        self.info.append(copy.copy(info))\n        if len(self.info) > self.max_steps:\n            self.info.pop(0)\n        self.game_step += 1\n\n    def reset(self) -> None:\n        self.info = []\n        self.game_step = 0\n    \n    def describe(self, game_step=None):\n        if len(self.info) == 0:\n            return \"\"\n        game_step = self.game_step if game_step is None else game_step\n        result = \"Most recent {} steps of the player's in-game observation:\\n\\n\".format(len(self.info))\n        for i, info in enumerate(self.info):\n            result += \"Player Observation Step {}:\\n\".format(game_step - len(self.info) + i)\n            result += info[\"obs\"] + \"\\n\\n\"\n        return result.strip()\n    \n    def score(self):\n        return sum([info[\"score\"] for info in self.info])\n\n\ndef describe_act(action_list):\n    return \"List of all actions:\\n\" + \"\\n\".join([\"{}. {}\".format(i+1, s) for i,s in enumerate(action_list)])\n"}
{"type": "source_file", "path": "src/smartplay/crafter/crafter/run_random.py", "content": "import argparse\nimport pathlib\nimport time\n\nimport numpy as np\n\nimport crafter\n\n\ndef main():\n  parser = argparse.ArgumentParser()\n  parser.add_argument('--seed', type=int, default=None)\n  parser.add_argument('--area', nargs=2, type=int, default=(64, 64))\n  parser.add_argument('--length', type=int, default=10000)\n  parser.add_argument('--health', type=int, default=9)\n  parser.add_argument('--record', type=pathlib.Path, default=None)\n  parser.add_argument('--episodes', type=int, default=1)\n  args = parser.parse_args()\n\n  random = np.random.RandomState(args.seed)\n  crafter.constants.items['health']['max'] = args.health\n  crafter.constants.items['health']['initial'] = args.health\n  env = crafter.Env(area=args.area, length=args.length, seed=args.seed)\n  env = crafter.Recorder(env, args.record)\n\n  for _ in range(args.episodes):\n\n    start = time.time()\n    obs = env.reset()\n    print('')\n    print(f'Reset time: {1000*(time.time()-start):.2f}ms')\n    print('Coal exist:    ', env._world.count('coal'))\n    print('Iron exist:    ', env._world.count('iron'))\n    print('Diamonds exist:', env._world.count('diamond'))\n\n    start = time.time()\n    done = False\n    while not done:\n      action = random.randint(0, env.action_space.n)\n      obs, reward, done, info = env.step(action)\n    duration = time.time() - start\n    step = env._step\n    print(f'Step time: {1000*duration/step:.2f}ms ({int(step/duration)} FPS)')\n    print('Episode length:', step)\n\n\nif __name__ == '__main__':\n  main()\n"}
{"type": "source_file", "path": "src/smartplay/rock_paper_scissors/__init__.py", "content": "from gym.envs.registration import register\n\nfrom .rock_paper_scissor import RockPaperScissorBasic\nfrom .rock_paper_scissor import RockPaperScissorDifferentScore\n\nenvironments = [\n    ['RockPaperScissorBasic', 'v0'],\n    ['RockPaperScissorDifferentScore', 'v0'],\n]\n\nfor environment in environments:\n    register(\n        id='{}-{}'.format(environment[0], environment[1]),\n        entry_point='smartplay.rock_paper_scissors:{}'.format(environment[0]),\n        # group='smartplay',\n    )\n"}
