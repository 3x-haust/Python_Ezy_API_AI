{"repo_info": {"repo_name": "flexo", "repo_owner": "IBM", "repo_url": "https://github.com/IBM/flexo"}}
{"type": "test_file", "path": "tests/__init__.py", "content": ""}
{"type": "test_file", "path": "tests/test_json_formatter.py", "content": "# tests/test_json_formatter.py\n\nimport pytest\nfrom src.utils.json_formatter import format_json_to_document\n\n\ndef test_basic_structures():\n    \"\"\"Test basic data structure formatting\"\"\"\n    # Simple dictionary\n    assert format_json_to_document({\"a\": 1, \"b\": 2}) == \"a: 1\\nb: 2\\n\"\n\n    # Simple list\n    assert format_json_to_document([1, 2, 3]) == \"0: 1\\n1: 2\\n2: 3\\n\"\n\n    # Mixed types in dictionary\n    result = format_json_to_document({\n        \"string\": \"text\",\n        \"integer\": 42,\n        \"float\": 3.14,\n        \"boolean\": True\n    })\n    assert \"string: text\\n\" in result\n    assert \"integer: 42\\n\" in result\n    assert \"float: 3.14\\n\" in result\n    assert \"boolean: True\\n\" in result\n\n\ndef test_nested_structures():\n    \"\"\"Test nested data structure formatting\"\"\"\n    test_data = {\n        \"person\": {\n            \"name\": \"John Doe\",\n            \"age\": 30,\n            \"contact\": {\n                \"email\": \"john@example.com\",\n                \"phones\": [\"+1234567890\", \"+0987654321\"]\n            },\n            \"interests\": [\"reading\", \"hiking\", {\"sports\": [\"football\", \"tennis\"]}],\n            \"metadata\": None,\n            \"notes\": \"A very long note that should be truncated...\" * 20,\n            \"empty_list\": [],\n            \"empty_dict\": {}\n        }\n    }\n\n    formatted = format_json_to_document(\n        test_data,\n        indent_size=2,\n        preview_length=50,\n        max_length=100\n    )\n\n    # Test structure and indentation\n    assert \"person:\\n\" in formatted\n    assert \"  name: John Doe\\n\" in formatted\n    assert \"  contact:\\n\" in formatted\n    assert \"    email:\" in formatted  # Test nested indentation\n    assert \"      0: +1234567890\\n\" in formatted  # Test list indentation\n\n\ndef test_edge_cases():\n    \"\"\"Test edge cases and special values\"\"\"\n    # None values\n    assert format_json_to_document(None) == \"null\\n\"\n    assert format_json_to_document({\"key\": None}) == \"key: null\\n\"\n    assert format_json_to_document([None, None]) == \"0: null\\n1: null\\n\"\n\n    # Empty structures\n    assert format_json_to_document({}) == \"[empty]\\n\"\n    assert format_json_to_document([]) == \"[empty]\\n\"\n    assert format_json_to_document({\"empty\": []}) == \"empty: [empty]\\n\"\n\n    # Nested empty structures\n    assert \"nested: [empty]\\n\" in format_json_to_document({\"outer\": {\"nested\": []}})\n\n    # Special characters in strings\n    special_chars = format_json_to_document({\n        \"newline\": \"hello\\nworld\",\n        \"tab\": \"hello\\tworld\",\n        \"unicode\": \"hello üåç\"\n    })\n    assert \"newline: hello\\nworld\\n\" in special_chars\n    assert \"tab: hello\\tworld\\n\" in special_chars\n    assert \"unicode: hello üåç\\n\" in special_chars\n\n\ndef test_formatting_options():\n    \"\"\"Test different formatting options\"\"\"\n    data = {\"a\": [1, 2, {\"b\": 3}]}\n\n    # Test indent size\n    indent_result = format_json_to_document(data, indent_size=4)\n    assert \"    b: 3\\n\" in indent_result\n\n    # Test without list indices\n    no_indices = format_json_to_document(data, show_list_indices=False)\n    assert \"1\\n\" in no_indices\n    assert \"0: 1\" not in no_indices\n\n    # Test custom placeholders\n    custom = format_json_to_document(\n        {\"a\": None, \"b\": []},\n        null_placeholder=\"NULL\",\n        empty_placeholder=\"EMPTY\"\n    )\n    assert \"a: NULL\\n\" in custom\n    assert \"b: EMPTY\\n\" in custom\n\n    # Test truncation\n    long_text = \"x\" * 200\n    truncated = format_json_to_document(\n        {\"long\": long_text},\n        preview_length=10,\n        max_length=20\n    )\n    assert \"...\" in truncated\n    assert len(truncated.split(\"long: \")[1].strip()) < 15\n\n\ndef test_large_data_performance():\n    \"\"\"Test performance with large nested structures\"\"\"\n    # Create a large nested structure\n    large_data = {\n        \"level1\": {\n            f\"key{i}\": {\n                f\"subkey{j}\": f\"value{j}\"\n                for j in range(100)\n            }\n            for i in range(100)\n        }\n    }\n\n    # Test performance and memory usage\n    formatted = format_json_to_document(large_data)\n    assert formatted.startswith(\"level1:\\n\")\n    assert \"key0:\\n\" in formatted\n    assert \"subkey0: value0\\n\" in formatted\n\n    # Test with large lists\n    large_list = list(range(10000))\n    formatted_list = format_json_to_document(large_list)\n    assert \"0: 0\\n\" in formatted_list\n    assert \"9999: 9999\\n\" in formatted_list\n\n\ndef test_error_handling():\n    \"\"\"Test error handling and invalid inputs\"\"\"\n    # Test with invalid indent size\n    with pytest.raises(ValueError):\n        format_json_to_document({}, indent_size=-1)\n\n    # Test with invalid preview length\n    with pytest.raises(ValueError):\n        format_json_to_document({}, preview_length=0)\n\n    # Test with circular reference\n    d = {}\n    d[\"self\"] = d\n    with pytest.raises(RecursionError):\n        format_json_to_document(d)\n\n\nif __name__ == \"__main__\":\n    pytest.main([__file__])\n"}
{"type": "test_file", "path": "tests/test_streamlit_app.py", "content": "import uuid\nimport json\nimport requests\nimport streamlit as st\nfrom typing import List, Dict, Optional\n\n# Set the new endpoint URL\nSTREAMING_URL = \"http://localhost:8000/v1/chat/completions\"\n\nst.title(\"For Testing - Chat Interface\")\n\n# Initialize messages in session state\nif \"messages\" not in st.session_state:\n    st.session_state.messages = []\n\n\n# Function to display messages with appropriate styling\ndef get_status_icon(status: str) -> str:\n    \"\"\"Get the appropriate icon for a status message.\"\"\"\n    status_icons = {\n        \"starting_generation\": \"üü¢\",\n        \"tool_call_detected\": \"üîç\",\n        \"tools_executed\": \"‚ö°\",\n        \"continuing_generation\": \"üìù\",\n        \"generation_complete\": \"‚úÖ\"\n    }\n    return status_icons.get(status, \"üîß\")\n\n\ndef display_message(message: dict):\n    \"\"\"Display a message with appropriate styling and icons.\"\"\"\n    if message.get(\"is_status\"):\n        status = message.get(\"status\", \"unknown\")\n        icon = get_status_icon(status)\n\n        with st.chat_message(\"assistant\", avatar=icon):\n            st.markdown(message[\"content\"])\n    else:\n        with st.chat_message(message[\"role\"]):\n            st.markdown(message[\"content\"])\n\n\n# Display existing messages, filtering out status updates for the API\ndef get_api_messages() -> List[Dict[str, str]]:\n    \"\"\"Filter and return messages suitable for the API.\"\"\"\n    return [\n        {\"role\": msg[\"role\"], \"content\": msg[\"content\"]}\n        for msg in st.session_state.messages\n        if not msg.get(\"is_status\", False)\n    ]\n\n\n# Display all messages in the UI\nfor message in st.session_state.messages:\n    display_message(message)\n\n# Sidebar configuration\nwith st.sidebar:\n    st.header(\"Example Additional Context\")\n    field1 = st.text_input(\"Example Value 1\", value=\"Context Value\")\n    field2 = st.text_input(\"Example Value 2\", value=\"Context Value\")\n    include_status = st.checkbox(\"Show status updates\", value=False)\n\n    context = {\n        \"values\": [\n            {\"key\": \"example field1\", \"value\": field1},\n            {\"key\": \"example field2\", \"value\": field2}\n        ]\n    }\n\n# Generate a unique thread ID for the session\nif \"thread_id\" not in st.session_state:\n    st.session_state.thread_id = str(uuid.uuid4())\n\n\ndef stream_response(conversation_input: Dict[str, any]) -> Optional[str]:\n    \"\"\"Stream the response from the /chat/completions API.\"\"\"\n    headers = {\"X-IBM-THREAD-ID\": st.session_state.thread_id}\n\n    # Create placeholder for status messages\n    status_placeholder = st.empty()\n    response_container = st.empty()\n\n    try:\n        with requests.post(STREAMING_URL, json=conversation_input, headers=headers, stream=True) as response:\n            if response.status_code == 200:\n                assistant_response = \"\"\n\n                for line in response.iter_lines(chunk_size=5, decode_unicode=True):\n                    if line and line.startswith(\"data: \"):\n                        try:\n                            data = json.loads(line[6:])\n                            delta = data[\"choices\"][0][\"delta\"]\n\n                            # Handle status updates\n                            if \"metadata\" in delta and \"status\" in delta[\"metadata\"]:\n                                status = delta[\"metadata\"][\"status\"]\n                                if include_status:\n                                    # Build status message content\n                                    content = status\n                                    tools = None\n\n                                    # Handle tool calls if present\n                                    if status == \"tool_call_detected\" and \"tools\" in delta[\"metadata\"]:\n                                        tools = delta[\"metadata\"][\"tools\"]\n                                        tools_list = \"\\n\".join([f\"- üõ† {tool}\" for tool in tools])\n                                        content = f\"{status}\\n{tools_list}\"\n\n                                    status_msg = {\n                                        \"role\": \"assistant\",\n                                        \"content\": content,\n                                        \"is_status\": True,\n                                        \"status\": status,\n                                        \"tools\": tools if tools else None\n                                    }\n\n                                    status_placeholder.markdown(content)\n\n                                    # Add status to message history\n                                    st.session_state.messages.append(status_msg)\n\n                            # Handle content updates\n                            elif \"content\" in delta:\n                                content = delta[\"content\"]\n                                assistant_response += content\n                                response_container.markdown(assistant_response)\n\n                        except (json.JSONDecodeError, KeyError) as e:\n                            st.error(f\"Error processing response: {e}\")\n\n                # Clear status placeholder after completion\n                status_placeholder.empty()\n                return assistant_response\n            else:\n                st.error(f\"Error: {response.status_code} - {response.reason}\")\n                return None\n    except requests.RequestException as e:\n        st.error(f\"Request error: {e}\")\n        return None\n\n\n# Handle user input from chat\nif prompt := st.chat_input(\"Type your message here...\"):\n    # Add user's message to session state and display it\n    user_message = {\"role\": \"user\", \"content\": prompt}\n    st.session_state.messages.append(user_message)\n    display_message(user_message)\n\n    # Prepare conversation input\n    conversation_input = {\n        \"model\": \"agent-01\",\n        \"messages\": get_api_messages(),\n        \"stream\": True\n    }\n\n    # Stream the response\n    full_response = stream_response(conversation_input)\n\n    # Add assistant's final response to conversation history\n    if full_response:\n        st.session_state.messages.append({\n            \"role\": \"assistant\",\n            \"content\": full_response,\n            \"is_status\": False\n        })\n\n        # Force a rerun to update the message history display\n        st.rerun()\n"}
{"type": "source_file", "path": "src/data_models/chat_completions.py", "content": "# src/data_models/chat_completions.py\n\nimport json\nfrom pydantic import BaseModel, Field, ConfigDict\nfrom typing import List, Union, Optional, Literal, Annotated, Dict\n\n\nclass MessageBase(BaseModel):\n    \"\"\"Base class for all message types in the chat completion system.\n\n    This class serves as the foundation for all message types, enforcing a common structure\n    and validation rules. It uses Pydantic's strict mode to forbid extra attributes.\n\n    Attributes:\n        role (str): The role of the message sender. Must be implemented by child classes.\n\n    Note:\n        This class should not be used directly but rather inherited by specific message types.\n    \"\"\"\n    role: str\n\n    model_config = ConfigDict(extra='forbid')\n\n\nclass UserTextContent(BaseModel):\n    \"\"\"Represents the text content structure for user messages.\n\n    This model defines the format for text-based content in user messages, ensuring\n    proper typing and validation.\n\n    Attributes:\n        type (Literal[\"text\"]): Content type identifier, always set to \"text\".\n        text (str): The actual text content of the user message.\n    \"\"\"\n    type: Literal[\"text\"] = Field(default=\"text\", description=\"Content type, fixed to 'text' for user messages\")\n    text: str = Field(..., description=\"The text content of the user message\")\n\n\nclass UserImageURLContent(BaseModel):\n    \"\"\"Represents the image URL content structure for user messages.\n\n    This model defines the format for image-based content in user messages, supporting\n    base64 encoded images with configurable processing detail levels.\n\n    Attributes:\n        type (Literal[\"image_url\"]): Content type identifier, always set to \"image_url\".\n        image_url (dict): Dictionary containing a 'url' field with a base64 encoded image string.\n        detail (Optional[Literal[\"low\", \"high\", \"auto\"]]): Processing detail level for the image.\n            Defaults to \"auto\".\n    \"\"\"\n    type: Literal[\"image_url\"] = Field(\n        default=\"image_url\",\n        description=\"Content type, fixed to 'image_url' for user messages\"\n    )\n    image_url: dict = Field(\n        ...,\n        description=\"The image URL as a dictionary containing a 'url' field with a base64 encoded string\"\n    )\n    detail: Optional[Literal[\"low\", \"high\", \"auto\"]] = Field(\n        default=\"auto\",\n        description=\"Detail level for image processing\"\n    )\n\n\nclass UserMessage(MessageBase):\n    \"\"\"Represents a message from the user in the chat completion system.\n\n    This model handles both simple text messages and complex content types including\n    images. It supports single string content or a list of mixed content types.\n\n    Attributes:\n        role (Literal[\"user\"]): Role identifier, always set to \"user\".\n        content (Union[str, List[Union[UserTextContent, UserImageURLContent]]]):\n            The message content, either as a simple string or a list of content objects.\n    \"\"\"\n    role: Literal[\"user\"] = Field(default=\"user\", description=\"Role is fixed to 'user' for user messages\")\n    content: Union[str, List[Union[UserTextContent, UserImageURLContent]]] = Field(\n        ..., description=\"String or detailed content of the user message\"\n    )\n\n\nclass FunctionDetail(BaseModel):\n    \"\"\"Defines the structure for function call details in tool calls.\n\n    This model contains the essential information needed to execute a function\n    through the tool calling system.\n\n    Attributes:\n        name (str): The name of the function to be called.\n        arguments (Dict): Dictionary of arguments to be passed to the function.\n    \"\"\"\n    name: str = Field(..., description=\"Name of the function\")\n    arguments: str = Field(..., description=\"Arguments for the function\")\n\n\nclass ToolCall(BaseModel):\n    \"\"\"Represents a tool call made by the assistant.\n\n    This model handles the structure and formatting of tool calls, including\n    custom serialization of function arguments to JSON.\n\n    Attributes:\n        id (str): Unique identifier for the tool call.\n        type (Literal[\"function\"]): Type of tool call, currently only supports \"function\".\n        function (FunctionDetail): Detailed information about the function to be called.\n\n    Methods:\n        model_dump(*args, **kwargs) -> dict:\n            Custom serialization method that converts function arguments to JSON string.\n\n        format_tool_calls() -> str:\n            Formats the tool call as a JSON array string for API compatibility.\n    \"\"\"\n    id: str = Field(..., description=\"ID of the tool call\")\n    type: Literal[\"function\"] = Field(default=\"function\", description=\"Tool type, currently only 'function' is allowed\")\n    function: FunctionDetail = Field(..., description=\"Details of the function call, including name and arguments\")\n\n    def model_dump(self, *args, **kwargs) -> dict:\n        \"\"\"Custom model_dump to convert 'arguments' in function to a JSON string.\"\"\"\n        # Call the original model_dump\n        data = super().model_dump(*args, **kwargs)\n\n        # Convert 'arguments' to a JSON string within 'function'\n        if \"function\" in data:\n            data[\"function\"][\"arguments\"] = json.dumps(data[\"function\"][\"arguments\"])\n\n        return data\n\n    def format_tool_calls(self) -> str:\n        \"\"\"Format tool call as a JSON array string.\"\"\"\n        formatted_call = {\n            \"name\": self.function.name,\n            \"arguments\": self.function.arguments\n        }\n        return json.dumps(formatted_call)\n\n\nclass AssistantMessage(MessageBase):\n    \"\"\"Represents a message from the assistant in the chat completion system.\n\n    This model handles various types of assistant responses, including regular messages,\n    tool calls, and refusals. It includes custom serialization logic to handle None values.\n\n    Attributes:\n        role (Literal[\"assistant\"]): Role identifier, always set to \"assistant\".\n        content (Optional[Union[str, List[dict]]]): The content of the assistant's message.\n        refusal (Optional[str]): Optional refusal message if the assistant declines to respond.\n        tool_calls (Optional[List[ToolCall]]): List of tool calls made by the assistant.\n\n    Methods:\n        model_dump(*args, **kwargs) -> dict:\n            Custom serialization method that excludes None values and properly formats tool calls.\n    \"\"\"\n    role: Literal[\"assistant\"] = Field(\n        default=\"assistant\", description=\"Role is fixed to 'assistant' for assistant messages\"\n    )\n    content: Optional[Union[str, List[dict]]] = Field(None, description=\"The content of the assistant message\")\n    refusal: Optional[str] = Field(None, description=\"The refusal message by the assistant\")\n    tool_calls: Optional[List[ToolCall]] = Field(None, description=\"List of tool calls made by the assistant\")\n\n    def model_dump(self, *args, **kwargs) -> dict:\n        \"\"\"Custom model_dump that excludes fields with None values and calls model_dump on nested ToolCall models.\"\"\"\n        data = super().model_dump(*args, **kwargs)\n        if self.tool_calls:\n            data[\"tool_calls\"] = [call.model_dump() for call in self.tool_calls]\n        return {key: value for key, value in data.items() if value is not None}\n\n\nclass SystemMessage(MessageBase):\n    \"\"\"Represents a system message in the chat completion system.\n\n    This model handles system-level instructions and context that guide the\n    conversation behavior.\n\n    Attributes:\n        role (Literal[\"system\"]): Role identifier, always set to \"system\".\n        content (str): The content of the system message.\n    \"\"\"\n    role: Literal[\"system\"] = Field(default=\"system\", description=\"Role is fixed to 'system' for system messages\")\n    content: str = Field(..., description=\"The content of the system message\")\n\n\nclass ToolMessage(BaseModel):\n    \"\"\"Represents a message from a tool in the chat completion system.\n\n    This model handles responses from tool calls, including function executions\n    and their results.\n\n    Attributes:\n        role (Literal[\"tool\"]): Role identifier, always set to \"tool\".\n        name (str): The name of the tool that generated the message.\n        content (str): The content/result of the tool execution.\n        tool_call_id (Optional[str]): Optional identifier linking to the original tool call.\n            Defaults to '123abcdef'.\n    \"\"\"\n    role: Literal[\"tool\"] = Field(default=\"tool\", description=\"Role is fixed to 'tool' for tool messages\")\n    content: str = Field(..., description=\"The content of the tool message\")\n    tool_call_id: Optional[str] = Field(default='123abcdef', description=\"Tool call ID\")\n\n\n\"\"\"Represents all possible message types in the chat completion system.\n\nThe TextChatMessage type uses Pydantic's discriminated unions to automatically\ndetermine the correct message type based on the 'role' field during validation.\n\nTypes:\n    - UserMessage: Messages from users with role=\"user\"\n    - AssistantMessage: Messages from the assistant with role=\"assistant\"\n    - SystemMessage: System-level messages with role=\"system\"\n    - ToolMessage: Messages from tools with role=\"tool\"\n\nNote:\n    This uses Pydantic's discriminated union feature to ensure proper validation\n    and serialization of different message types.\n\"\"\"\nTextChatMessage = Annotated[\n    Union[UserMessage, AssistantMessage, SystemMessage, ToolMessage],\n    Field(discriminator='role')\n]\n"}
{"type": "source_file", "path": "src/agent/__init__.py", "content": "from .chat_agent_streaming import StreamingChatAgent\n"}
{"type": "source_file", "path": "src/data_models/streaming.py", "content": "# src/data_models/streaming.py\n\nfrom typing import Optional\nfrom pydantic import BaseModel, Field, computed_field\n\n\nclass PatternMatchResult(BaseModel):\n    \"\"\"Result of a pattern matching operation in stream processing.\n\n    This class encapsulates all possible outcomes and data from a pattern\n    matching operation, including matched patterns, processed output,\n    and any errors that occurred.\n\n    Attributes:\n        output (Optional[str]): The processed text after pattern matching.\n            None if no processing occurred.\n        pattern_name (Optional[str]): Name of the matched pattern.\n            None if no pattern was matched.\n        matched (bool): Whether a pattern was successfully matched.\n            Defaults to False.\n        text_with_tool_call (Optional[str]): Complete text containing the tool call\n            if a match was found. None otherwise.\n        tool_call_message (Optional[str]): Message associated with the tool call.\n            None if no tool call was detected.\n        error (Optional[str]): Error message if pattern matching failed.\n            None if processing was successful.\n\n    Example:\n        ```python\n        result = PatternMatchResult(\n            output=\"Processed text\",\n            pattern_name=\"MistralPattern0\",\n            matched=True,\n            text_with_tool_call=\"Complete tool call text\",\n            tool_call_message=\"Tool call detected\"\n        )\n        ```\n    \"\"\"\n    output: Optional[str] = Field(\n        default=None,\n        description=\"The processed output text after pattern matching\"\n    )\n    pattern_name: Optional[str] = Field(\n        default=None,\n        description=\"The name of the matched pattern\"\n    )\n    matched: bool = Field(\n        default=False,\n        description=\"Indicates whether a pattern was successfully matched\"\n    )\n    text_with_tool_call: Optional[str] = Field(\n        default=None,\n        description=\"The complete text containing the tool call if matched\"\n    )\n    tool_call_message: Optional[str] = Field(\n        default=None,\n        description=\"Any message associated with the tool call\"\n    )\n    error: Optional[str] = Field(\n        default=None,\n        description=\"Error message if pattern matching failed\"\n    )\n\n\nclass StreamConfig(BaseModel):\n    \"\"\"Configuration settings for stream processing operations.\n\n    This class defines the configuration parameters that control how streaming\n    data is processed, buffered, and chunked. It uses Pydantic for validation\n    and provides several factory methods for common configurations.\n\n    Attributes:\n        buffer_size (int): Size of the buffer in characters. Zero disables buffering.\n            Must be greater than or equal to 0.\n        chunk_separator (str): String used to separate chunks when combining buffered content.\n        strip_whitespace (bool): Whether to remove whitespace from chunks before processing.\n        buffering_enabled (bool): Computed field indicating if buffering is active.\n\n    Example:\n        ```python\n        # Create a default configuration\n        config = StreamConfig.create_default()\n\n        # Create a buffered configuration\n        buffered_config = StreamConfig.create_buffered(buffer_size=100)\n\n        # Create configuration with custom separator\n        custom_config = StreamConfig.create_with_separator(\n            buffer_size=50,\n            separator=\"\\\\n\"\n        )\n        ```\n    \"\"\"\n    buffer_size: int = Field(\n        default=0,\n        description=\"Size of the buffer in characters. Set to 0 to disable buffering.\",\n        ge=0\n    )\n    chunk_separator: str = Field(\n        default=\"\",\n        description=\"Separator to use between chunks when combining buffered content.\"\n    )\n    strip_whitespace: bool = Field(\n        default=False,\n        description=\"Whether to strip whitespace from chunks before processing.\"\n    )\n\n    @computed_field\n    @property\n    def buffering_enabled(self) -> bool:\n        \"\"\"Indicates whether buffering is enabled based on buffer size.\"\"\"\n        return self.buffer_size > 0\n\n    model_config = {\n        \"frozen\": True,\n        \"extra\": \"forbid\",\n        \"json_schema_extra\": {\n            \"examples\": [\n                {\n                    \"buffer_size\": 10,\n                    \"chunk_separator\": \"\\n\",\n                    \"strip_whitespace\": True\n                }\n            ]\n        }\n    }\n\n    @classmethod\n    def create_default(cls) -> \"StreamConfig\":\n        \"\"\"Create a StreamConfig with default values.\"\"\"\n        return cls()\n\n    @classmethod\n    def create_buffered(cls, buffer_size: int) -> \"StreamConfig\":\n        \"\"\"Create a StreamConfig with specified buffer size.\"\"\"\n        return cls(buffer_size=buffer_size)\n\n    @classmethod\n    def create_with_separator(\n            cls,\n            buffer_size: int = 0,\n            separator: str = \"\\n\"\n    ) -> \"StreamConfig\":\n        \"\"\"Create a StreamConfig with specified buffer size and separator.\"\"\"\n        return cls(\n            buffer_size=buffer_size,\n            chunk_separator=separator\n        )\n"}
{"type": "source_file", "path": "src/agent/chat_agent_streaming.py", "content": "# src/agent/chat_streaming_agent.py\n\nimport os\nimport json5\nimport yaml\nimport asyncio\nimport logging\nfrom functools import wraps\nfrom typing import List, AsyncGenerator, Dict, Optional, Any, Callable\n\nfrom src.api import SSEChunk, AgentStatus\nfrom src.data_models.agent import StreamState, StreamContext\nfrom src.data_models.chat_completions import (\n    ToolCall,\n    ToolMessage,\n    SystemMessage,\n    TextChatMessage,\n    AssistantMessage,\n)\nfrom src.llm import LLMFactory\nfrom src.tools import ToolRegistry\nfrom src.prompt_builders import PromptPayload, PromptBuilderOutput, BasePromptBuilder\nfrom src.utils.factory import PromptBuilderFactory, ToolCallParserFactory, FormatType\nfrom src.llm.tool_detection.detection_result import DetectionState, DetectionResult\nfrom src.llm.tool_detection import ManualToolCallDetectionStrategy, VendorToolCallDetectionStrategy\n\n\n# ----------------------------------------------------------------\n#  Setup Standard Error Handler for State Handlers\n# ----------------------------------------------------------------\n\ndef handle_streaming_errors(func: Callable[..., AsyncGenerator[SSEChunk, None]]) \\\n        -> Callable[..., AsyncGenerator[SSEChunk, None]]:\n    @wraps(func)\n    async def wrapper(self, *args, **kwargs) -> AsyncGenerator[SSEChunk, None]:\n        try:\n            async for item in func(self, *args, **kwargs):\n                yield item\n        except Exception:\n            self.logger.error(f\"Error in {func.__name__}\", exc_info=True)\n            yield await SSEChunk.make_stop_chunk(\n                content=\"I apologize - I've encountered an unexpected error. Please try your request again.\")\n            return\n\n    return wrapper\n\n\n# ----------------------------------------------------------------\n#  Create Streaming Chat Agent\n# ----------------------------------------------------------------\n\n\nclass StreamingChatAgent:\n    \"\"\"An agent that handles streaming chat interactions with support for tool execution.\n\n    This agent processes conversations in a streaming fashion, handling message generation,\n    tool detection, and tool execution. It supports both vendor-specific and manual tool\n    detection strategies.\n\n    Args:\n        config (Dict): Configuration dictionary containing:\n\n            - `history_limit` (int): Maximum number of historical messages to consider\n            - `system_prompt` (str): System prompt to prepend to conversations\n            - `detection_mode` (str): Mode for tool detection ('vendor' or 'manual')\n            - `use_vendor_chat_completions` (bool): Whether to use vendor chat completions\n            - `models_config` (Dict): Configuration for language models\n            - `tools_config` (Dict): Configuration for available tools\n            - `logging_level` (str): Logging level (default: 'INFO')\n            - `max_streaming_iterations` (int): Maximum number of streaming iterations\n\n    Attributes:\n        response_model_name (str): Name of the main chat model\n        history_limit (int): Maximum number of historical messages to consider\n        system_prompt (str): System prompt prepended to conversations\n        logger (logging.Logger): Logger instance for the agent\n        detection_mode (str): Current tool detection mode\n        use_vendor_chat_completions (bool): Whether vendor chat completions are enabled\n    \"\"\"\n    def __init__(self, config: Dict) -> None:\n        self.config = config\n        self.response_model_name = \"main_chat_model\"\n        self.history_limit = self.config.get('history_limit', 3)\n        self.system_prompt = self.config.get('system_prompt')\n\n        # Initialize logger\n        self.logger = logging.getLogger(self.__class__.__name__)\n        logging_level = os.getenv(\"LOG_LEVEL\", \"INFO\")\n        logging.basicConfig(level=getattr(logging, logging_level.upper(), None))\n        self.logger.info(f'Logger set to {logging_level}')\n\n        # Initialize vendor-specific components\n        self.main_chat_model_config = self.config.get('models_config').get('main_chat_model')\n        self.llm_factory = LLMFactory(config=self.config.get('models_config'))\n\n        # Determine detection strategy first\n        self.detection_mode = self.config.get(\"detection_mode\", \"vendor\")\n        self.use_vendor_chat_completions = self.config.get(\"use_vendor_chat_completions\", True)\n\n        # Load parser config if needed for manual detection\n        self.logger.info(\"\\n\\n\" + \"=\" * 60 + \"\\n\" + \"Agent Configuration Summary\" + \"\\n\" + \"=\" * 60 + \"\\n\" +\n                         f\"Main Chat Model Config:\\n{json5.dumps(self.main_chat_model_config, indent=4)}\\n\" +\n                         f\"Tool Detection Mode: {self.detection_mode}\\n\" +\n                         f\"Vendor Chat API Mode: {self.use_vendor_chat_completions}\\n\" +\n                         \"\\n\" + \"=\" * 60 + \"\\n\")\n        if self.detection_mode == \"manual\":\n            with open(\"src/configs/parsing.yaml\", \"r\") as f:\n                parser_config = yaml.safe_load(f)\n            self.tool_call_parser = ToolCallParserFactory.get_parser(\n                FormatType.JSON,\n                parser_config\n            )\n            self.detection_strategy = ManualToolCallDetectionStrategy(\n                parser=self.tool_call_parser\n            )\n        else:\n            self.detection_strategy = VendorToolCallDetectionStrategy()\n            self.tool_call_parser = None\n\n        # Initialize prompt builder with inject_tools flag based on detection mode\n        self.prompt_builder: BasePromptBuilder = PromptBuilderFactory.get_prompt_builder(\n            vendor=self.main_chat_model_config.get('vendor')\n        )\n\n        # Initialize tool registry\n        self.tool_registry = ToolRegistry(\n            tools_config=self.config.get(\"tools_config\"),\n            mcp_config=self.config.get(\"mcp_config\")\n        )\n        asyncio.create_task(self.tool_registry.initialize_all_tools())\n\n    @handle_streaming_errors\n    async def stream_step(\n            self,\n            conversation_history: List[TextChatMessage],\n            api_passed_context: Optional[Dict[str, Any]] = None\n    ) -> AsyncGenerator[SSEChunk, None]:\n        \"\"\"Process a conversation step in a streaming fashion.\n\n        Handles the main conversation flow, including message generation, tool detection,\n        and tool execution. The process flows through different states until completion.\n\n        Args:\n            conversation_history (List[TextChatMessage]): List of previous conversation messages\n            api_passed_context (Optional[Dict[str, Any]]): Additional context passed from the API\n\n        Yields:\n            SSEChunk: Server-Sent Events chunks containing response content or status updates\n\n        States:\n            - STREAMING: Generating response content\n            - TOOL_DETECTION: Detecting tool calls in the response\n            - EXECUTING_TOOLS: Running detected tools\n            - INTERMEDIATE: Handling intermediate steps\n            - COMPLETING: Finalizing the response\n        \"\"\"\n        self.logger.debug(\"Starting streaming agent processing\")\n\n        context = await self._initialize_context(conversation_history, api_passed_context)\n\n        while context.current_state != StreamState.COMPLETED:\n            match context.current_state:\n\n                case StreamState.STREAMING:\n                    self.logger.info(f\"--- Entering Streaming State ---\")\n                    async for item in self._handle_streaming(context):\n                        yield item\n\n                case StreamState.TOOL_DETECTION:\n                    self.logger.info(f\"--- Entering Tool Detection State ---\")\n                    async for item in self._handle_tool_detection(context):\n                        yield item\n\n                case StreamState.EXECUTING_TOOLS:\n                    self.logger.info(f\"--- Entering Executing Tools State ---\")\n                    async for item in self._handle_tool_execution(context):\n                        yield item\n\n                case StreamState.INTERMEDIATE:\n                    self.logger.info(f\"--- Entering Intermediate State ---\")\n                    async for item in self._handle_intermediate(context):\n                        yield item\n\n                case StreamState.COMPLETING:\n                    self.logger.info(f\"--- Entering Completing State ---\")\n                    async for item in self._handle_completing(context):\n                        yield item\n                    context.current_state = StreamState.COMPLETED\n\n    # ----------------------------------------------------------------\n    #  STATE HANDLERS\n    # ----------------------------------------------------------------\n\n    @handle_streaming_errors\n    async def _handle_streaming(self, context: StreamContext) -> AsyncGenerator[SSEChunk, None]:\n        \"\"\"Handle the streaming state of the conversation.\n\n        Processes the main content generation phase, monitoring for tool calls and\n        managing the response stream.\n\n        Args:\n            context (StreamContext): Current streaming context\n\n        Yields:\n            SSEChunk: Content chunks and status updates\n\n        Raises:\n            Exception: If maximum streaming iterations are exceeded\n        \"\"\"\n        self.detection_strategy.reset()\n        context.streaming_entry_count += 1\n        if context.streaming_entry_count > context.max_streaming_iterations:\n            self.logger.error(\"Maximum streaming iterations reached. Aborting further streaming.\")\n            yield await SSEChunk.make_stop_chunk(\n                content=\"Maximum streaming depth reached. Please try your request again.\"\n            )\n            context.current_state = StreamState.COMPLETING\n            return\n\n        prompt_payload = PromptPayload(\n            conversation_history=context.conversation_history,\n            tool_definitions=context.tool_definitions if self.detection_mode == \"manual\" else None\n        )\n\n        prompt_output: PromptBuilderOutput = (\n            await self.prompt_builder.build_chat(prompt_payload) if self.use_vendor_chat_completions\n            else await self.prompt_builder.build_text(prompt_payload)\n        )\n        llm_input = prompt_output.get_output()\n\n        stream_kwargs = {\n            'prompt': llm_input if isinstance(llm_input, str) else None,\n            'messages': llm_input if isinstance(llm_input, list) else None,\n            'tools': context.tool_definitions if self.detection_mode != \"manual\" else None\n        }\n        stream_kwargs = {k: v for k, v in stream_kwargs.items() if v is not None}\n\n        self.logger.debug(f\"stream_kwargs: {stream_kwargs}\")\n        llm_adapter = context.llm_factory.get_adapter(self.response_model_name)\n        stream_gen = llm_adapter.gen_sse_stream if isinstance(llm_input, str) else llm_adapter.gen_chat_sse_stream\n\n        accumulated_content = []\n        async for sse_chunk in stream_gen(**stream_kwargs):\n            detection_result = await self.detection_strategy.detect_chunk(sse_chunk, context)\n            self.logger.debug(f\"Detection result: {detection_result}\")\n\n            if detection_result.state in [DetectionState.NO_MATCH, DetectionState.PARTIAL_MATCH]:\n                if detection_result.content:\n                    accumulated_content.append(detection_result.content)\n                    yield SSEChunk.make_text_chunk(detection_result.content)\n\n            elif detection_result.state == DetectionState.COMPLETE_MATCH:\n                async for chunk in self._handle_complete_match(context, detection_result, accumulated_content):\n                    yield chunk\n                return\n\n        final_result = await self.detection_strategy.finalize_detection(context)\n        self.logger.debug(f\"Final detection result: {final_result}\")\n\n        if final_result.state == DetectionState.COMPLETE_MATCH:\n            async for chunk in self._handle_complete_match(context, final_result, accumulated_content):\n                yield chunk\n        else:\n            if final_result.content:\n                accumulated_content.append(final_result.content)\n                yield SSEChunk.make_text_chunk(final_result.content)\n\n            if accumulated_content:\n                context.conversation_history.append(\n                    AssistantMessage(content=\"\".join(accumulated_content))\n                )\n\n            yield await SSEChunk.make_stop_chunk()\n            context.current_state = StreamState.COMPLETING\n\n    @handle_streaming_errors\n    async def _handle_tool_detection(self, context: StreamContext) -> AsyncGenerator[SSEChunk, None]:\n        \"\"\"Handle the tool detection state.\n\n        Processes detected tool calls and transitions to tool execution.\n\n        Args:\n            context (StreamContext): Current streaming context\n\n        Yields:\n            SSEChunk: Tool detection status updates\n        \"\"\"\n        self.logger.debug(\"Tool calls detected, transitioning to EXECUTING_TOOLS\")\n        context.current_state = StreamState.EXECUTING_TOOLS\n        yield await SSEChunk.make_status_chunk(\n            AgentStatus.TOOL_DETECTED,\n            {\"tools\": [tc.format_tool_calls() for tc in context.current_tool_call]}\n        )\n\n    @handle_streaming_errors\n    async def _handle_tool_execution(\n            self,\n            context: StreamContext\n    ) -> AsyncGenerator[SSEChunk, None]:\n        \"\"\"Execute detected tools and process their results.\n\n        Runs the detected tools concurrently and handles their results, including\n        error cases.\n\n        Args:\n            context (StreamContext): Current streaming context\n\n        Yields:\n            SSEChunk: Tool execution status updates\n        \"\"\"\n        if context.message_buffer.strip():\n            context.conversation_history.append(\n                AssistantMessage(content=context.message_buffer)\n            )\n            context.message_buffer = \"\"\n\n        results = await self._execute_tools_concurrently(context)\n\n        tool_results = []\n        for call, result in zip(context.current_tool_call, results):\n            context.conversation_history.append(\n                AssistantMessage(tool_calls=[call])\n            )\n            if isinstance(result, Exception):\n                context.conversation_history.append(\n                    AssistantMessage(\n                        content=f\"Error executing tool {call.function.name}: {str(result)}\"\n                    )\n                )\n            else:\n                tool_results.append(result)\n                context.conversation_history.append(\n                    ToolMessage(\n                        content=result[\"result\"],\n                        tool_call_id=call.id\n                    )\n                )\n\n        self.logger.debug(\"Tool execution results: %s\", tool_results)\n        context.current_state = StreamState.INTERMEDIATE\n        yield await SSEChunk.make_status_chunk(AgentStatus.TOOLS_EXECUTED)\n\n    @handle_streaming_errors\n    async def _handle_intermediate(self, context: StreamContext) -> AsyncGenerator[SSEChunk, None]:\n        \"\"\"Handle the intermediate state between tool executions.\n\n        Resets the message buffer and detection strategy for the next iteration.\n\n        Args:\n            context (StreamContext): Current streaming context\n\n        Yields:\n            SSEChunk: Status updates for continuation\n        \"\"\"\n        context.message_buffer = \"\"\n        self.detection_strategy.reset()\n        context.current_state = StreamState.STREAMING\n        yield await SSEChunk.make_status_chunk(AgentStatus.CONTINUING)\n\n    @handle_streaming_errors\n    async def _handle_completing(self, context: StreamContext) -> AsyncGenerator[SSEChunk, None]:\n        \"\"\"Handle the completion state of the conversation.\n\n        Finalizes the conversation and sends the stop signal.\n\n        Args:\n            context (StreamContext): Current streaming context\n\n        Yields:\n            SSEChunk: Final stop chunk\n        \"\"\"\n        self.logger.info(f\"--- Entering COMPLETING State ---\")\n\n        yield await SSEChunk.make_stop_chunk()\n        self.logger.info(f\"Streaming process completed.\")\n\n    # ----------------------------------------------------------------\n    #  HELPER METHODS\n    # ----------------------------------------------------------------\n\n    async def _initialize_context(\n            self,\n            conversation_history: List[TextChatMessage],\n            api_passed_context: Optional[Dict]\n    ) -> StreamContext:\n        \"\"\"Initialize the streaming context for a new conversation step.\n\n        Sets up the conversation history with system prompt and creates the\n        streaming context with necessary configurations.\n\n        Args:\n            conversation_history (List[TextChatMessage]): Previous conversation messages\n            api_passed_context (Optional[Dict]): Additional context from the API\n\n        Returns:\n            StreamContext: Initialized streaming context\n        \"\"\"\n        selected_history = (\n            conversation_history[-self.history_limit:]\n            if self.history_limit > 0\n            else conversation_history\n        )\n\n        if self.system_prompt:\n            system_message = SystemMessage(content=self.system_prompt)\n            selected_history.insert(0, system_message)\n\n        return StreamContext(\n            conversation_history=selected_history,\n            tool_definitions=await self.tool_registry.get_tool_definitions(),\n            context=api_passed_context,\n            llm_factory=self.llm_factory,\n            current_state=StreamState.STREAMING,\n            max_streaming_iterations=self.config.get(\"max_streaming_iterations\", 1),\n            streaming_entry_count=0\n        )\n\n    async def _handle_complete_match(\n            self,\n            context: StreamContext,\n            result: DetectionResult,\n            accumulated_content: List[str]\n    ) -> AsyncGenerator[SSEChunk, None]:\n        \"\"\"Handle a complete tool call match detection.\n\n        Process detected tool calls and update the conversation history.\n\n        Args:\n            context (StreamContext): Current streaming context\n            result (DetectionResult): Tool detection result\n            accumulated_content (List[str]): Accumulated response content\n\n        Yields:\n            SSEChunk: Content chunks and updates\n        \"\"\"\n        if result.content:\n            accumulated_content.append(result.content)\n            yield SSEChunk.make_text_chunk(result.content)\n\n        context.current_tool_call = result.tool_calls or []\n        if accumulated_content:\n            context.conversation_history.append(\n                AssistantMessage(content=\"\".join(accumulated_content))\n            )\n        context.current_state = StreamState.TOOL_DETECTION\n\n    async def _execute_tools_concurrently(self, context: StreamContext) -> List[Any]:\n        \"\"\"Execute multiple tools concurrently.\n\n        Run detected tools in parallel and handle their results or errors.\n\n        Args:\n            context (StreamContext): Current streaming context\n\n        Returns:\n            List[Any]: List of tool execution results or exceptions\n\n        Raises:\n            RuntimeError: If a requested tool is not found\n        \"\"\"\n        async def run_tool(tool_call: ToolCall):\n            try:\n                tool = await self.tool_registry.get_tool(tool_call.function.name)\n                if not tool:\n                    raise RuntimeError(f\"Tool {tool_call.function.name} not found\")\n                tool_args = json5.loads(tool_call.function.arguments)\n                self.logger.info(f\"Running tool {tool_call.function.name} with arguments: {tool_args}\")\n                result = await tool.execute(\n                    context=context,\n                    **tool_args\n                )\n                return {\"tool_name\": tool_call.function.name, \"result\": result.result}\n\n            except Exception as e:\n                self.logger.error(f\"Error executing tool {tool_call.function.name}\", exc_info=True)\n                return e\n\n        tasks = [run_tool(tc) for tc in context.current_tool_call]\n        return await asyncio.gather(*tasks, return_exceptions=True)\n"}
{"type": "source_file", "path": "src/api/__init__.py", "content": "from .sse_models import *\n"}
{"type": "source_file", "path": "src/api/request_models.py", "content": "# src/api/request_models.py\n\nfrom typing import Optional, List\nfrom pydantic import BaseModel, Field\nfrom src.data_models.chat_completions import TextChatMessage\n\n\nclass ContextValue(BaseModel):\n    \"\"\"Represents a key-value pair for contextual information.\n\n    This model is used to store individual pieces of context data\n    that can be passed to tools or used in conversation.\n\n    Attributes:\n        key (str): The identifier for this piece of context\n        value (str): The actual context value\n    \"\"\"\n    key: str\n    value: str\n\n\nclass ContextModel(BaseModel):\n    \"\"\"Container for multiple context values.\n\n    This model serves as a collection of context values that can be\n    passed around the system to provide additional information to\n    tools and conversations.\n\n    Attributes:\n        values (List[ContextValue]): List of context key-value pairs\n\n    Note: This model is used to define a structure for exposing in the API for allowing additional context to be passed through the system. The keys and values are used to create a dictionary which is passed to the streaming agent and anything downstream.\n    \"\"\"\n    values: Optional[List[ContextValue]] = Field(None, description=\"Additional context values (e.g. for API tools)\")\n\n\nclass ChatCompletionRequest(BaseModel):\n    \"\"\"Request model for chat completion endpoints.\n\n    Attributes:\n        model (Optional[str]): ID of the model to use for completion.\n        messages (List[dict]): Array of message objects with role and content.\n        context (Optional[ContextModel]): Additional context for API tools.\n    \"\"\"\n    model: Optional[str] = Field(None, description=\"ID of the model to use\")\n    messages: List[TextChatMessage] = Field(..., description=\"Array of messages (role/content)\")\n    context: Optional[ContextModel] = Field(None, description=\"Additional context values (e.g. for API tools)\")\n"}
{"type": "source_file", "path": "src/api/sse_models.py", "content": "# src/api/sse_models.py\nfrom __future__ import annotations\n\nimport time\nfrom enum import Enum\nfrom typing import Dict\nfrom typing import List, Optional, Any\nfrom pydantic import BaseModel\n\n\nclass AgentStatus(str, Enum):\n    STARTING = \"starting_generation\"\n    TOOL_DETECTED = \"tool_call_detected\"\n    TOOLS_EXECUTED = \"tools_executed\"\n    MAX_DEPTH = \"max_depth_reached\"\n    CONTINUING = \"continuing_generation\"\n\n\nclass SSEStatus(BaseModel):\n    status: AgentStatus\n    details: Optional[Dict[str, Any]] = None\n\n\nclass SSEFunction(BaseModel):\n    \"\"\"Model for function calls in SSE responses, with support for streaming chunks.\"\"\"\n    name: str = \"\"\n    arguments: str = \"\"\n\n\nclass SSEToolCall(BaseModel):\n    \"\"\"Model for tool calls in SSE responses.\"\"\"\n    index: int = 0\n    id: Optional[str] = None\n    type: str = \"function\"\n    function: Optional[SSEFunction] = None\n\n\nclass SSEDelta(BaseModel):\n    \"\"\"Model for delta content in SSE responses.\"\"\"\n    role: Optional[str] = None\n    content: Optional[str] = None\n    tool_calls: Optional[List[SSEToolCall]] = None\n    refusal: Optional[str] = None\n    status: Optional[str] = None\n    metadata: Optional[dict] = None\n\n\nclass SSEChoice(BaseModel):\n    \"\"\"Model for choices in SSE responses.\"\"\"\n    index: int\n    delta: SSEDelta\n    logprobs: Optional[dict] = None\n    finish_reason: Optional[str] = None\n\n\nclass SSEChunk(BaseModel):\n    \"\"\"Model for SSE chunks.\"\"\"\n    id: str\n    object: str\n    created: int\n    model: str\n    service_tier: Optional[str] = None\n    system_fingerprint: Optional[str] = None\n    choices: List[SSEChoice]\n    thread_id: Optional[str] = None  # this is an IBM wxO thing\n\n    @staticmethod\n    def make_text_chunk(text: str) -> 'SSEChunk':\n        \"\"\"\n        Utility to create a minimal SSEChunk that only has user-visible 'content'.\n        This ensures we never leak partial function-call details back to the user.\n        \"\"\"\n        return SSEChunk(\n            id=f\"chatcmpl-{time.time()}\",\n            object=\"chat.completion.chunk\",\n            created=int(time.time()),\n            model=\"agent-01\",\n            choices=[\n                SSEChoice(\n                    index=0,\n                    delta=SSEDelta(role=\"assistant\", content=text),\n                    finish_reason=None\n                )\n            ]\n        )\n\n    @staticmethod\n    async def make_status_chunk(status: str, extra_info: Optional[Dict] = None) -> 'SSEChunk':\n        metadata = {\"status\": status}\n        if extra_info:\n            metadata.update(extra_info)\n\n        return SSEChunk(\n            id=f\"status_{time.time()}\",\n            object=\"chat.completion.chunk\",\n            created=int(time.time()),\n            model=\"agent-01\",\n            choices=[\n                SSEChoice(\n                    index=0,\n                    delta=SSEDelta(\n                        role=\"system\",\n                        metadata=metadata\n                    ),\n                    finish_reason=None\n                )\n            ]\n        )\n\n    @staticmethod\n    async def make_stop_chunk(content=None, refusal=None) -> 'SSEChunk':\n        return SSEChunk(\n            id=f\"chatcmpl-{time.time()}\",\n            object=\"chat.completion.chunk\",\n            created=int(time.time()),\n            model=\"agent-01\",\n            choices=[\n                SSEChoice(\n                    index=0,\n                    delta=SSEDelta(role=\"assistant\", content=content, refusal=refusal),\n                    finish_reason=\"stop\"\n                )\n            ]\n        )\n"}
{"type": "source_file", "path": "src/data_models/__init__.py", "content": "from .wx_assistant import WxAssistantMessage, WxAssistantConversationInput\nfrom .tools import Tool\n"}
{"type": "source_file", "path": "src/__init__.py", "content": ""}
{"type": "source_file", "path": "src/api/routes/chat_completions_api.py", "content": "\"\"\"Chat Completions API Module.\n\nThis module implements a FastAPI router for streaming chat completion functionality,\nwith authentication, message handling, and integration with a streaming chat agent.\n\nThe module provides:\n- API key validation\n- Message format conversion\n- Streaming chat completions\n- SSE (Server-Sent Events) response handling\n- Agent info endpoint (compatible with OpenAI's /models format)\n\"\"\"\n\nimport os\nimport time\nimport yaml\nimport logging\nfrom typing import List, Optional\nfrom fastapi.responses import StreamingResponse\nfrom starlette.status import HTTP_403_FORBIDDEN\nfrom fastapi.security.api_key import APIKeyHeader\nfrom fastapi import APIRouter, Body, Depends, Header, HTTPException\n\nfrom src.agent import StreamingChatAgent\nfrom src.api.request_models import ChatCompletionRequest\nfrom src.data_models.chat_completions import (\n    TextChatMessage,\n    UserMessage,\n    UserTextContent\n)\n\nlogger = logging.getLogger(__name__)\nrouter = APIRouter()\n\n# Capture the agent's start time once when the app starts.\nAGENT_START_TIME = int(time.time())\n\n# Security setup\nAPI_KEY_NAME = \"X-API-KEY\"\napi_key_header = APIKeyHeader(name=API_KEY_NAME, auto_error=False)\nFLEXO_API_KEY = os.getenv(\"FLEXO_API_KEY\")\nENABLE_API_KEY = os.getenv(\"ENABLE_API_KEY\", \"true\").lower() != \"false\"\n\nif not FLEXO_API_KEY:\n    logger.error(\"FLEXO_API_KEY environment variable not set\")\n    raise ValueError(\"FLEXO_API_KEY is not set. Please set it in your environment or .env file.\")\n\nlogger.info(\"API authentication %s\", \"enabled\" if ENABLE_API_KEY else \"disabled\")\n\n\nasync def get_api_key(api_key: str = Depends(api_key_header)):\n    \"\"\"Validate the incoming API key against environment configuration.\n\n    Args:\n        api_key (str): The API key from the request header.\n\n    Returns:\n        str: The validated API key.\n\n    Raises:\n        HTTPException: If authentication is enabled and the API key is invalid.\n    \"\"\"\n    if ENABLE_API_KEY:\n        logger.debug(\"Validating API key\")\n        if not api_key or api_key != FLEXO_API_KEY:\n            logger.warning(\"Invalid API key attempted\")\n            raise HTTPException(\n                status_code=HTTP_403_FORBIDDEN,\n                detail=\"Unauthorized\"\n            )\n    return api_key\n\n\n# Agent initialization\ntry:\n    logger.debug(\"Loading agent configuration from yaml\")\n    with open('src/configs/agent.yaml', 'r') as file:\n        config = yaml.safe_load(file)\n    streaming_chat_agent_instance = StreamingChatAgent(config=config)\n    logger.info(\"StreamingChatAgent successfully initialized\")\nexcept Exception as e:\n    logger.error(\"Failed to load agent config: %s\", str(e), exc_info=True)\n    raise\n\n\ndef get_streaming_agent() -> StreamingChatAgent:\n    \"\"\"Dependency provider for the streaming agent instance.\n\n    Returns:\n        StreamingChatAgent: The global streaming agent instance.\n    \"\"\"\n    return streaming_chat_agent_instance\n\n\ndef convert_message_content(messages: List[TextChatMessage]) -> List[TextChatMessage]:\n    \"\"\"Convert plain string user messages to structured UserTextContent objects.\n\n    Args:\n        messages (List[TextChatMessage]): List of messages to convert.\n\n    Returns:\n        List[TextChatMessage]: Converted messages with proper content structure.\n    \"\"\"\n    logger.debug(\"Converting %d messages\", len(messages))\n    converted = []\n    for msg in messages:\n        if msg.role == \"user\" and isinstance(msg.content, str):\n            converted.append(\n                UserMessage(\n                    role=\"user\",\n                    content=[UserTextContent(text=msg.content.strip())]\n                )\n            )\n        else:\n            converted.append(msg)\n    return converted\n\n\ndef get_non_sensitive_config(config: dict) -> dict:\n    \"\"\"Filter the agent configuration to include only non-sensitive parameters.\n\n    Args:\n        config (dict): The full agent configuration.\n\n    Returns:\n        dict: Filtered configuration excluding keys that may be sensitive.\n    \"\"\"\n    # Exclude keys that include 'key', 'secret', or 'password' (case insensitive)\n    return {\n        k: v for k, v in config.items()\n        if not any(sensitive in k.lower() for sensitive in [\"key\", \"secret\", \"password\"])\n    }\n\n\n@router.post(\n    \"/chat/completions\",\n    summary=\"Generate streaming chat completions\",\n    description=\"Generate a streaming response from the agent based on user input.\",\n    tags=[\"Agent Chat\"],\n    operation_id=\"chat\",\n    response_class=StreamingResponse\n)\nasync def chat_completions(\n        request_body: ChatCompletionRequest = Body(...),\n        x_ibm_thread_id: Optional[str] = Header(None),\n        agent: StreamingChatAgent = Depends(get_streaming_agent),\n        api_key: Optional[str] = Depends(get_api_key)\n):\n    \"\"\"Generate streaming chat completions from the agent.\n\n    Processes incoming chat messages and returns a streaming response with\n    token-by-token updates.\n\n    Args:\n        request_body (ChatCompletionRequest): The chat completion request.\n        x_ibm_thread_id (Optional[str]): Thread ID for response correlation.\n        agent (StreamingChatAgent): The chat agent instance.\n        api_key (Optional[str]): Validated API key.\n\n    Returns:\n        StreamingResponse: Server-sent events stream of completion tokens.\n\n    Raises:\n        HTTPException: If processing fails or invalid input is provided.\n    \"\"\"\n    try:\n        logger.debug(f\"Processing chat completion request with {len(request_body.messages)} messages\")\n        processed_messages = convert_message_content(request_body.messages)\n\n        logger.debug(\"Initiating streaming response\")\n        response_stream = agent.stream_step(\n            conversation_history=processed_messages,\n            api_passed_context=request_body.context.model_dump() if request_body.context else None\n        )\n\n        async def sse_generator():\n            \"\"\"Generate SSE chunks from the response stream.\"\"\"\n            try:\n                async for sse_chunk in response_stream:\n                    if sse_chunk:\n                        if x_ibm_thread_id:\n                            sse_chunk.thread_id = x_ibm_thread_id\n                        sse_chunk.object = \"thread.message.delta\"\n\n                        logger.debug(\"Sending SSE chunk\")\n                        yield f\"data: {sse_chunk.model_dump_json(exclude_none=True)}\\n\\n\"\n\n                        if any(choice.finish_reason in [\"stop\", \"tool_calls\"]\n                               for choice in sse_chunk.choices):\n                            logger.debug(\"Stream completed\")\n                            return\n            except Exception as e:\n                logger.error(\"Error in SSE generator: %s\", str(e), exc_info=True)\n                return\n\n        logger.debug(\"Initializing StreamingResponse\")\n        return StreamingResponse(\n            sse_generator(),\n            media_type=\"text/event-stream\",\n            headers={\n                \"Cache-Control\": \"no-cache\",\n                \"Connection\": \"keep-alive\",\n                \"X-Accel-Buffering\": \"no\"\n            }\n        )\n\n    except Exception as e:\n        logger.error(\"Error in /chat/completions: %s\", str(e), exc_info=True)\n        raise HTTPException(status_code=500, detail=str(e))\n\n\n@router.get(\n    \"/models\",\n    summary=\"Agent information\",\n    description=\"Returns information about the running agent including its default model and non-sensitive configuration parameters.\",\n    tags=[\"Models\"],\n    operation_id=\"getModels\"\n)\nasync def get_models(\n    agent: StreamingChatAgent = Depends(get_streaming_agent),\n    api_key: Optional[str] = Depends(get_api_key)\n):\n    \"\"\"Retrieve agent information in a format similar to OpenAI's models endpoint.\n\n    This endpoint returns the agent's name, default model, creation time,\n    owner, and a selection of non-sensitive configuration parameters.\n    \"\"\"\n    agent_name = agent.config.get(\"name\", \"default-agent\")\n    default_model = agent.config.get(\"model\", \"default-model\")\n    created = agent.config.get(\"created\", AGENT_START_TIME)\n    owned_by = agent.config.get(\"owned_by\", \"user\")\n    non_sensitive_params = get_non_sensitive_config(agent.config)\n\n    agent_info = {\n        \"id\": agent_name,\n        \"object\": \"agent\",\n        \"default_model\": default_model,\n        \"created\": created,\n        \"owned_by\": owned_by,\n        \"params\": non_sensitive_params\n    }\n\n    return {\"data\": [agent_info], \"object\": \"list\"}\n"}
{"type": "source_file", "path": "src/api/routes/__init__.py", "content": ""}
{"type": "source_file", "path": "src/data_models/agent.py", "content": "# src/data_models/agent.py\n\nfrom enum import Enum\nfrom pydantic import BaseModel, Field\nfrom typing import List, Optional, Any, Dict\n\nfrom src.llm import LLMFactory\nfrom src.data_models.tools import Tool\nfrom src.data_models.chat_completions import TextChatMessage, ToolCall\n\n\nclass StreamState(Enum):\n    \"\"\"Defines the possible states of the streaming agent during processing.\n\n    The StreamState enum represents different operational states that the streaming agent\n    can be in at any given time during message processing and tool execution.\n\n    Attributes:\n        STREAMING: Currently streaming response content\n        TOOL_DETECTION: Analyzing stream for potential tool calls\n        EXECUTING_TOOLS: Currently executing detected tools\n        INTERMEDIATE: Temporary state between major operations\n\n    Example:\n        ```python\n        state = StreamState.IDLE\n        if processing_started:\n            state = StreamState.STREAMING\n        ```\n    \"\"\"\n    STREAMING = \"streaming\"\n    TOOL_DETECTION = \"detection\"\n    EXECUTING_TOOLS = \"executing\"\n    INTERMEDIATE = \"intermediate\"\n    COMPLETING = \"completing\"\n    COMPLETED = \"completed\"\n\n\nclass StreamResult(BaseModel):\n    \"\"\"Represents the result of processing an individual stream chunk.\n\n    This class encapsulates the various pieces of information that can be produced\n    when processing a single chunk of a stream, including content, errors, and status updates.\n\n    Attributes:\n        content (Optional[str]): The actual content from the stream chunk. May be None if\n            chunk contained no content (e.g., only status updates).\n        error (Optional[str]): Error message if any issues occurred during processing.\n            None if processing was successful.\n        status (Optional[str]): Status message indicating state changes or completion.\n            Used to communicate processing progress.\n        should_continue (bool): Flag indicating if streaming should continue.\n            Defaults to True, set to False for terminating streaming.\n\n    Example:\n        ```python\n        result = StreamResult(\n            content=\"Generated text response\",\n            status=\"Processing complete\",\n            should_continue=True\n        )\n        ```\n    \"\"\"\n    content: Optional[str] = Field(\n        default=None,\n        description=\"The content of the stream chunk\"\n    )\n    error: Optional[str] = Field(\n        default=None,\n        description=\"Error message if processing failed\"\n    )\n    status: Optional[str] = Field(\n        default=None,\n        description=\"Status message indicating state changes or completion\"\n    )\n    should_continue: bool = Field(\n        default=True,\n        description=\"Flag indicating if streaming should continue\"\n    )\n\n\nclass StreamContext(BaseModel):\n    \"\"\"Context and state for a streaming conversation session.\n\n    This class stores all necessary information for managing a streaming conversation,\n    including conversation history, available tool definitions, state buffers, and\n    session metadata. It also tracks the number of times the streaming state has been\n    initiated.\n\n    Attributes:\n        conversation_history (List[TextChatMessage]): The full conversation history,\n            including a system message at the start if available.\n        tool_definitions (List[Tool]): Definitions of available tools for execution.\n        message_buffer (str): Buffer for accumulating generated response text.\n        tool_call_buffer (str): Buffer for accumulating potential tool call text until parsing.\n        current_tool_call (Optional[List[ToolCall]]): The currently processing tool calls, if any.\n        current_state (StreamState): The current state of the stream processing.\n        streaming_entry_count (int): Counter tracking the number of times the streaming state has been entered.\n        max_streaming_iterations (int): The maximum allowed number of times the streaming state can be initiated.\n        context (Optional[Dict[str, Any]]): Additional metadata associated with the streaming session.\n        llm_factory (Optional[LLMFactory]): LLM factory associated with the streaming agent.\n    \"\"\"\n\n    conversation_history: List[TextChatMessage] = Field(\n        default_factory=list,\n        description=\"Full conversation history with system message at the start.\"\n    )\n    tool_definitions: List[Tool] = Field(\n        default_factory=list,\n        description=\"Definitions of available tools.\"\n    )\n    message_buffer: str = Field(\n        default=\"\",\n        description=\"Buffer for accumulating generated response text.\"\n    )\n    tool_call_buffer: str = Field(\n        default=\"\",\n        description=\"Buffer for accumulating tool call text until parsing.\"\n    )\n    current_tool_call: Optional[List[ToolCall]] = Field(\n        default=None,\n        description=\"Currently processing tool calls.\"\n    )\n    current_state: StreamState = Field(\n        default=None,\n        description=\"Current state of the stream processing.\"\n    )\n    streaming_entry_count: int = Field(\n        default=0,\n        description=\"Tracks how many times the streaming state has been entered.\"\n    )\n    max_streaming_iterations: int = Field(\n        default=3,\n        description=\"The maximum allowed number of times the streaming state can be initiated.\"\n    )\n    context: Optional[Dict[str, Any]] = Field(\n        default=None,\n        description=\"Optional metadata associated with the streaming session.\"\n    )\n    llm_factory: Optional[LLMFactory] = Field(\n        default=None,\n        description=\"LLM Model factory for retrieving LLM adapters.\"\n    )\n\n    class Config:\n        \"\"\"Pydantic model configuration.\"\"\"\n        arbitrary_types_allowed = True\n"}
{"type": "source_file", "path": "src/llm/__init__.py", "content": "from .llm_factory import LLMFactory\n"}
{"type": "source_file", "path": "src/database/base_adapter.py", "content": "# src/database/base_adapter.py\n\nfrom abc import ABC, abstractmethod\n\n\nclass DatabaseAdapter(ABC):\n    \"\"\"Abstract base class defining the interface for database operations.\n\n    This class serves as a template for implementing database adapters,\n    ensuring consistent interface across different database backends.\n    All database implementations must inherit from this class and\n    implement its abstract methods.\n\n    Methods:\n        add: Add data to the database\n        search: Search for data in the database\n        reset: Reset or clear the database\n\n    Example:\n        ```python\n        class MyDatabaseClient(DatabaseAdapter):\n            async def add(self, document, index_name):\n                # Implementation for adding documents\n                pass\n\n            async def search(self, query, index_name):\n                # Implementation for searching documents\n                pass\n\n            async def reset(self, index_name):\n                # Implementation for resetting the database\n                pass\n        ```\n    \"\"\"\n\n    @abstractmethod\n    async def add(self, *args, **kwargs):\n        \"\"\"Add data to the database.\n\n        This abstract method must be implemented by concrete database adapters\n        to handle data insertion operations.\n\n        Args:\n            *args: Variable length argument list for flexibility across implementations\n            **kwargs: Arbitrary keyword arguments for flexibility across implementations\n\n        Raises:\n            NotImplementedError: If the concrete class doesn't implement this method\n        \"\"\"\n        pass\n\n    @abstractmethod\n    async def search(self, *args, **kwargs):\n        \"\"\"Search for data in the database.\n\n        This abstract method must be implemented by concrete database adapters\n        to handle search operations.\n\n        Args:\n            *args: Variable length argument list for flexibility across implementations\n            **kwargs: Arbitrary keyword arguments for flexibility across implementations\n\n        Raises:\n            NotImplementedError: If the concrete class doesn't implement this method\n        \"\"\"\n        pass\n\n    @abstractmethod\n    async def reset(self, *args, **kwargs):\n        \"\"\"Reset or clear data in the database.\n\n        This abstract method must be implemented by concrete database adapters\n        to handle database reset operations.\n\n        Args:\n            *args: Variable length argument list for flexibility across implementations\n            **kwargs: Arbitrary keyword arguments for flexibility across implementations\n\n        Raises:\n            NotImplementedError: If the concrete class doesn't implement this method\n        \"\"\"\n        pass\n"}
{"type": "source_file", "path": "src/llm/adapters/xai_adapter.py", "content": "# src/llm/adapters/xai_adapter.py\n\nimport os\nimport logging\nfrom typing import AsyncGenerator, List, Optional\n\nfrom openai import AsyncOpenAI\nfrom src.llm.adapters.base_vendor_adapter import BaseVendorAdapter\nfrom src.data_models.tools import Tool\nfrom src.data_models.chat_completions import TextChatMessage\nfrom src.api.sse_models import SSEChunk, SSEChoice, SSEDelta, SSEToolCall, SSEFunction\n\nlogger = logging.getLogger(__name__)\n\n\nclass XAIAdapter(BaseVendorAdapter):\n    \"\"\"Adapter for interacting with xAI's API.\n\n    Utilizes the OpenAI client for compatibility with xAI API endpoints.\n    Supports streaming responses and converts them to standardized SSE chunks.\n\n    Attributes:\n        model_name (str): The model identifier being served\n        api_key (str): xAI API key for authentication\n        base_url (str): URL of the xAI API server\n        client (AsyncOpenAI): Configured OpenAI-compatible client for xAI\n    \"\"\"\n\n    def __init__(\n            self,\n            model_name: str,\n            base_url: str = \"https://api.x.ai/v1\",\n            api_key: str = None,\n            **default_params\n    ):\n        \"\"\"Initialize the xAI adapter.\n\n        Args:\n            model_name (str): Name of the xAI model to use\n            base_url (str): URL of the xAI API server\n            api_key (str): xAI API key for authentication\n            **default_params: Additional parameters for generation (temperature etc.)\n        \"\"\"\n        self.model_name = model_name\n        self.base_url = base_url\n\n        # Get API key from environment or parameter\n        self.api_key = api_key or os.getenv(\"XAI_API_KEY\")\n        if not self.api_key:\n            raise ValueError(\"xAI API key is required. Provide as parameter or set `XAI_API_KEY` environment variable.\")\n\n        self.default_params = default_params\n\n        # Configure OpenAI-compatible client for X.AI\n        self.client = AsyncOpenAI(\n            base_url=self.base_url,\n            api_key=self.api_key\n        )\n\n        logger.info(f\"Initialized xAI adapter for model: {self.model_name}\")\n        logger.debug(f\"Using X.AI server at: {self.base_url}\")\n        logger.debug(f\"Default parameters: {default_params}\")\n\n    async def gen_chat_sse_stream(\n            self,\n            messages: List[TextChatMessage],\n            tools: Optional[List[Tool]] = None,\n            **kwargs\n    ) -> AsyncGenerator[SSEChunk, None]:\n        \"\"\"Generate streaming chat completion.\n\n        Uses xAI's chat completions endpoint with streaming enabled.\n\n        Args:\n            messages (List[TextChatMessage]): List of chat messages\n            tools (Optional[List[Tool]]): Optional tools/functions definitions\n            **kwargs: Additional parameters to override defaults\n\n        Yields:\n            SSEChunk: Standardized chunks of the streaming response\n        \"\"\"\n        try:\n            # Convert messages to OpenAI format\n            openai_messages = [msg.model_dump() for msg in messages]\n\n            # Prepare request payload\n            request_params = {\n                \"model\": self.model_name,\n                \"messages\": openai_messages,\n                \"stream\": True,\n                **self.default_params,\n                **kwargs\n            }\n\n            # Add tools if provided\n            if tools:\n                request_params[\"tools\"] = [tool.model_dump() for tool in tools]\n                request_params[\"tool_choice\"] = \"auto\"\n\n            # Stream response\n            async for chunk in await self.client.chat.completions.create(**request_params):\n                yield self._convert_to_sse_chunk(chunk)\n\n        except Exception as e:\n            logger.error(f\"Error in xAI chat stream: {str(e)}\", exc_info=True)\n            raise RuntimeError(f\"xAI chat completion failed: {str(e)}\") from e\n\n    async def gen_sse_stream(\n            self,\n            prompt: str,\n            **kwargs\n    ) -> AsyncGenerator[SSEChunk, None]:\n        \"\"\"Generate streaming completion from a text prompt.\n\n        For xAI, this simply converts the text prompt to a chat message and calls gen_chat_sse_stream.\n\n        Args:\n            prompt (str): Input text prompt\n            **kwargs: Additional parameters to override defaults\n\n        Yields:\n            SSEChunk: Standardized chunks of the streaming response\n        \"\"\"\n        # Convert the prompt to a single user message\n        messages = [{\"role\": \"user\", \"content\": prompt}]\n\n        # Use the chat completions endpoint\n        async for chunk in self.gen_chat_sse_stream(messages, **kwargs):\n            yield chunk\n\n    def _convert_to_sse_chunk(self, raw_chunk) -> SSEChunk:\n        \"\"\"Convert xAI API response chunk to standardized SSE format.\n\n        Args:\n            raw_chunk: Raw chunk from xAI API\n\n        Returns:\n            SSEChunk: Standardized chunk format\n        \"\"\"\n        try:\n            choices = []\n\n            # Handle chat completion format\n            for choice in raw_chunk.choices:\n                tool_calls = None\n\n                # Handle tool calls if present\n                if hasattr(choice.delta, 'tool_calls') and choice.delta.tool_calls:\n                    tool_calls = []\n                    for tc in choice.delta.tool_calls:\n                        function = None\n                        if tc.function:\n                            function = SSEFunction(\n                                name=tc.function.name or \"\",\n                                arguments=tc.function.arguments or \"\"\n                            )\n                        tool_calls.append(SSEToolCall(\n                            index=tc.index or 0,\n                            id=tc.id,\n                            type=tc.type or \"function\",\n                            function=function\n                        ))\n\n                choices.append(SSEChoice(\n                    index=choice.index,\n                    delta=SSEDelta(\n                        role=choice.delta.role if hasattr(choice.delta, 'role') else None,\n                        content=choice.delta.content if hasattr(choice.delta, 'content') else None,\n                        tool_calls=tool_calls\n                    ),\n                    finish_reason=choice.finish_reason\n                ))\n\n            return SSEChunk(\n                id=raw_chunk.id,\n                object=raw_chunk.object,\n                created=raw_chunk.created,\n                model=raw_chunk.model,\n                choices=choices\n            )\n\n        except Exception as e:\n            logger.error(f\"Error converting xAI chunk: {raw_chunk}\", exc_info=True)\n            raise ValueError(f\"Failed to convert xAI response: {str(e)}\") from e\n"}
{"type": "source_file", "path": "src/llm/adapters/watsonx/watsonx_adapter.py", "content": "import time\nimport json\nimport logging\nimport asyncio\nimport aiohttp\nfrom aiohttp import ClientError\nfrom asyncio import TimeoutError\nfrom tenacity import (\n    retry,\n    stop_after_attempt,\n    wait_exponential,\n    retry_if_exception_type,\n    before_sleep_log\n)\nfrom contextlib import asynccontextmanager\nfrom typing import AsyncGenerator, List, Optional, Dict, Any\n\nfrom src.data_models.tools import Tool\nfrom src.llm.adapters import BaseVendorAdapter\nfrom src.data_models.chat_completions import TextChatMessage\nfrom src.llm.adapters.watsonx.watsonx_config import WatsonXConfig\nfrom src.llm.adapters.watsonx.ibm_token_manager import IBMTokenManager\nfrom src.api.sse_models import SSEChunk, SSEChoice, SSEDelta, SSEToolCall, SSEFunction\n\nlogger = logging.getLogger(__name__)\n\nMAX_RETRIES = 1  # no retries\nMIN_RETRY_WAIT = 1  # seconds\nMAX_RETRY_WAIT = 10  # seconds\n\nDEFAULT_TIMEOUT = aiohttp.ClientTimeout(\n    total=300,  # 5 minutes total timeout\n    connect=60,  # 60 seconds connection timeout\n    sock_read=60  # 60 seconds socket read timeout\n)\n\n\nclass WatsonXAdapter(BaseVendorAdapter):\n    \"\"\"Adapter for interacting with IBM WatsonX's API.\n\n    This class implements the BaseVendorAdapter interface for WatsonX language models,\n    handling authentication, streaming requests, and response parsing. It converts\n    WatsonX-specific formats into standardized SSE chunks for consistent handling\n    across different LLM providers.\n\n    Attributes:\n        model_id (str): The WatsonX model identifier.\n        model_params (dict): Default parameters for model requests.\n        token_manager (IBMTokenManager): Manager for IBM Cloud authentication tokens.\n        project_id (str): WatsonX project identifier.\n        base_url (str): Base URL for WatsonX API endpoints.\n        timeout (aiohttp.ClientTimeout): Timeout configuration for requests.\n        _session (Optional[aiohttp.ClientSession]): Reusable HTTP session.\n    \"\"\"\n\n    def __init__(self,\n                 model_name: str,\n                 token_manager: IBMTokenManager,\n                 timeout: Optional[aiohttp.ClientTimeout] = None,\n                 **model_params):\n        \"\"\"Initialize the WatsonX Adapter with model configuration.\n\n        Args:\n            model_name (str): The identifier of the WatsonX model to use.\n            token_manager (IBMTokenManager): Manager for handling IBM authentication.\n            timeout (Optional[aiohttp.ClientTimeout]): Custom timeout configuration.\n            **model_params: Additional parameters to include in model requests.\n\n        Raises:\n            ValueError: If required configuration is missing.\n        \"\"\"\n        self.model_id = model_name\n        self.model_params = model_params or {}\n        self.token_manager = token_manager\n        self.project_id = WatsonXConfig.PROJECT_ID\n        self.base_url = \"https://us-south.ml.cloud.ibm.com/ml/v1/text\"\n        self._session: Optional[aiohttp.ClientSession] = None\n        self.timeout = timeout or DEFAULT_TIMEOUT\n        logger.info(f\"WatsonX Adapter initialized with model: {self.model_id}\")\n        logger.debug(f\"Model parameters configured: {model_params}\")\n        logger.debug(f\"Timeout configuration: {self.timeout}\")\n\n    @asynccontextmanager\n    async def _session_context(self):\n        \"\"\"Manage the lifecycle of an HTTP session.\n\n        Yields:\n            aiohttp.ClientSession: Active HTTP session for making requests.\n        \"\"\"\n        logger.debug(\"Creating new HTTP session\")\n        session = aiohttp.ClientSession(timeout=self.timeout)\n        try:\n            yield session\n        finally:\n            await session.close()\n            logger.debug(\"HTTP session closed\")\n\n    async def gen_chat_sse_stream(\n            self,\n            messages: List[TextChatMessage],\n            tools: Optional[List[Tool]] = None,\n            timeout: Optional[aiohttp.ClientTimeout] = None,\n            **kwargs\n    ) -> AsyncGenerator[SSEChunk, None]:\n        \"\"\"Generate a streaming chat response from a sequence of messages.\n\n        Args:\n            messages (List[TextChatMessage]): List of chat messages for context.\n            tools (Optional[List[Tool]]): List of tools available to the model.\n            timeout (Optional[aiohttp.ClientTimeout]): Optional request-specific timeout.\n            **kwargs: Additional parameters to override defaults.\n\n        Yields:\n            SSEChunk: Standardized chunks of the streaming response.\n\n        Raises:\n            RuntimeError: If the WatsonX API request fails.\n            TimeoutError: If the request times out.\n        \"\"\"\n        logger.debug(f\"Processing chat stream request with {len(messages)} messages\")\n        serialized_messages = [msg.model_dump() for msg in messages]\n        serialized_tools = [tool.model_dump() for tool in tools] if tools else None\n\n        payload = {\n            \"model_id\": self.model_id,\n            \"project_id\": self.project_id,\n            \"messages\": serialized_messages,\n            **kwargs\n        }\n        if serialized_tools:\n            logger.debug(f\"Adding {len(serialized_tools)} tools to request\")\n            payload[\"tools\"] = serialized_tools\n\n        async for raw_chunk in self._make_sse_request(\"chat_stream\", payload, timeout):\n            sse_chunk = self._convert_to_sse_chunk(raw_chunk)\n            yield sse_chunk\n\n    async def gen_sse_stream(\n            self,\n            prompt: str,\n            timeout: Optional[aiohttp.ClientTimeout] = None,\n            **kwargs\n    ) -> AsyncGenerator[SSEChunk, None]:\n        \"\"\"Generate text using WatsonX's generation_stream endpoint.\n\n        Args:\n            prompt (str): The input text prompt.\n            timeout (Optional[aiohttp.ClientTimeout]): Optional request-specific timeout.\n            **kwargs: Additional parameters to pass to the API.\n\n        Yields:\n            SSEChunk: Server-sent event chunks containing generated text.\n\n        Raises:\n            RuntimeError: If the streaming request fails.\n            TimeoutError: If the request times out.\n        \"\"\"\n        logger.debug(f\"Processing generation stream request. Prompt: {prompt[:50]}...\")\n        payload = {\n            \"model_id\": self.model_id,\n            \"project_id\": self.project_id,\n            \"input\": prompt,\n            \"parameters\": {\n                **self.model_params,\n                **kwargs\n            }\n        }\n\n        async for raw_chunk in self._make_sse_request(\"generation_stream\", payload, timeout):\n            sse_chunk = self._convert_to_sse_chunk(raw_chunk)\n            yield sse_chunk\n\n    @retry(\n        stop=stop_after_attempt(MAX_RETRIES),\n        wait=wait_exponential(multiplier=MIN_RETRY_WAIT, max=MAX_RETRY_WAIT),\n        retry=retry_if_exception_type((ClientError, TimeoutError)),\n        before_sleep=before_sleep_log(logger, logging.WARNING),\n        reraise=True\n    )\n    async def _make_sse_request(self,\n                                endpoint: str,\n                                payload: Dict[str, Any],\n                                timeout: Optional[aiohttp.ClientTimeout] = None) \\\n            -> AsyncGenerator[Dict[str, Any], None]:\n        \"\"\"Make a streaming request to WatsonX API with retry logic.\n\n        Args:\n            endpoint (str): API endpoint to call.\n            payload (Dict[str, Any]): Request payload data.\n            timeout (Optional[aiohttp.ClientTimeout]): Optional request-specific timeout.\n\n        Yields:\n            Dict[str, Any]: Raw response chunks from the API.\n\n        Raises:\n            aiohttp.ClientError: If all retry attempts fail with HTTP errors.\n            ValueError: If response parsing fails.\n            TimeoutError: If all retry attempts timeout.\n            Exception: If all retry attempts fail for other reasons.\n        \"\"\"\n        token = await self.token_manager.get_token()\n        url = f\"{self.base_url}/{endpoint}?version=2023-05-29\"\n        headers = {\n            \"Authorization\": f\"Bearer {token}\",\n            \"Content-Type\": \"application/json\",\n            \"Accept\": \"application/json\",\n        }\n\n        logger.debug(f\"Making request to endpoint: {endpoint}\")\n        logger.debug(f\"Request payload: {json.dumps(payload, indent=2)}\")\n\n        try:\n            async with self._session_context() as session:\n                async with session.post(url,\n                                        json=payload,\n                                        headers=headers,\n                                        timeout=timeout or self.timeout) as resp:\n                    resp.raise_for_status()\n                    logger.debug(f\"Stream connected, status: {resp.status}\")\n\n                    buffer = []\n                    async for raw_line in resp.content:\n                        line = raw_line.decode(\"utf-8\").strip()\n\n                        if not line:\n                            event_data = self._parse_sse_event(buffer)\n                            buffer = []\n\n                            if \"data\" in event_data:\n                                try:\n                                    data_parsed = json.loads(event_data[\"data\"])\n                                    yield data_parsed\n                                except json.JSONDecodeError:\n                                    logger.warning(f\"Skipping invalid SSE data: {event_data['data']}\")\n                            continue\n\n                        buffer.append(line)\n\n        except aiohttp.ClientError as e:\n            logger.error(f\"HTTP request failed: {str(e)}\", exc_info=True)\n            raise\n        except asyncio.TimeoutError as e:\n            logger.error(f\"Request timed out: {str(e)}\", exc_info=True)\n            raise TimeoutError(f\"Request to {endpoint} timed out\") from e\n\n    def _parse_sse_event(self, lines: List[str]) -> Dict[str, str]:\n        \"\"\"Parse Server-Sent Events format into structured data.\n\n        Args:\n            lines (List[str]): Raw SSE message lines.\n\n        Returns:\n            Dict[str, str]: Parsed event data.\n        \"\"\"\n        event = {}\n        for line in lines:\n            if line.startswith(\"id:\"):\n                event[\"id\"] = line[len(\"id:\"):].strip()\n            elif line.startswith(\"event:\"):\n                event[\"event\"] = line[len(\"event:\"):].strip()\n            elif line.startswith(\"data:\"):\n                data_str = line[len(\"data:\"):].strip()\n                event[\"data\"] = event.get(\"data\", \"\") + data_str\n        return event\n\n    def _convert_to_sse_chunk(self, raw_chunk: dict) -> SSEChunk:\n        \"\"\"Convert WatsonX response format to standardized SSE chunk.\n\n        Handles both generation_stream and chat_stream response formats.\n\n        Args:\n            raw_chunk (dict): Raw response data from WatsonX.\n\n        Returns:\n            SSEChunk: Standardized chunk format.\n\n        Raises:\n            ValueError: If chunk conversion fails.\n        \"\"\"\n        try:\n            logger.debug(f\"Converting chunk: {json.dumps(raw_chunk, indent=2)}\")\n            # Handle generation_stream format\n            if \"results\" in raw_chunk:\n                result = raw_chunk[\"results\"][0]\n                choices = [\n                    SSEChoice(\n                        index=0,\n                        delta=SSEDelta(\n                            content=result.get(\"generated_text\"),\n                            role=\"assistant\"\n                        ),\n                        logprobs=None,\n                        finish_reason=result.get(\"stop_reason\")\n                    )\n                ]\n            # Handle chat_stream format\n            else:\n                choices = []\n                for choice_dict in raw_chunk.get('choices', []):\n                    delta_data = choice_dict.get('delta', {})\n\n                    tool_calls = None\n                    if \"tool_calls\" in delta_data:\n                        tool_calls = [\n                            SSEToolCall(\n                                index=tc.get(\"index\", 0),\n                                id=tc.get(\"id\"),\n                                type=tc.get(\"type\", \"function\"),\n                                function=SSEFunction(\n                                    name=tc[\"function\"][\"name\"],\n                                    arguments=tc[\"function\"].get(\"arguments\", \"\")\n                                ) if tc.get(\"function\") else None\n                            ) for tc in delta_data[\"tool_calls\"]\n                        ]\n\n                    delta = SSEDelta(\n                        role=delta_data.get(\"role\"),\n                        content=delta_data.get(\"content\"),\n                        tool_calls=tool_calls,\n                        refusal=delta_data.get(\"refusal\"),\n                        status=delta_data.get(\"status\"),\n                        metadata=delta_data.get(\"metadata\")\n                    )\n\n                    choices.append(SSEChoice(\n                        index=choice_dict.get(\"index\", 0),\n                        delta=delta,\n                        logprobs=choice_dict.get(\"logprobs\"),\n                        finish_reason=choice_dict.get(\"finish_reason\")\n                    ))\n\n            return SSEChunk(\n                id=raw_chunk.get(\"id\", f\"watsonx-{int(time.time())}\"),\n                object=raw_chunk.get(\"object\", \"chat.completion.chunk\"),\n                created=raw_chunk.get(\"created\", int(time.time())),\n                model=raw_chunk.get(\"model\", self.model_id),\n                choices=choices\n            )\n        except Exception as e:\n            logger.error(f\"Error converting WatsonX chunk: {raw_chunk}\", exc_info=True)\n            raise ValueError(f\"Failed to convert WatsonX response to SSEChunk: {str(e)}\") from e\n"}
{"type": "source_file", "path": "src/data_models/wx_assistant.py", "content": "# src/data_models/wx_assistant.py\n\nfrom pydantic import BaseModel\nfrom typing import List, Optional\nfrom src.api.request_models import ContextModel\nfrom src.data_models.chat_completions import TextChatMessage, UserMessage, AssistantMessage, UserTextContent\n\n\nclass WxAssistantMessage(BaseModel):\n    \"\"\"\n    This is a class for WxAssistant messages.\n\n    Attributes:\n        u (Optional[str]): Represents the user input.\n        a (Optional[str]): Represents the assistant response.\n        n (Optional[bool]): An optional boolean for additional context.\n    \"\"\"\n    u: Optional[str] = None\n    a: Optional[str] = None\n    n: Optional[bool] = None\n\n    def to_dict(self):\n        return {\"u\": self.u, \"a\": self.a, \"n\": self.n}\n\n\nclass WxAssistantConversationInput(BaseModel):\n    \"\"\"\n    This is a class for WxAssistant conversation input.\n\n    Attributes:\n        messages (List[WxAssistantMessage]): A list of WxAssistantMessage instances.\n        context (Optional[ContextModel]): Optional metadata for additional context.\n    \"\"\"\n    messages: List[WxAssistantMessage]\n    context: Optional[ContextModel] = None\n\n\ndef convert_wx_to_conversation(wx_input: WxAssistantConversationInput) -> List[TextChatMessage]:\n    \"\"\"Convert Watson Assistant messages to standard conversation format.\n\n    Converts messages from Watson Assistant format to a list of TextChatMessage\n    objects suitable for general conversation processing.\n\n    Args:\n        wx_input (WxAssistantConversationInput): Watson Assistant conversation input.\n\n    Returns:\n        List[TextChatMessage]: List of converted messages in standard format.\n\n    Example:\n        ```python\n        wx_input = WxAssistantConversationInput(messages=[...])\n        messages = convert_wx_to_conversation(wx_input)\n        ```\n    \"\"\"\n    messages: List[TextChatMessage] = []\n\n    for wx_message in wx_input.messages:\n        if wx_message.u:\n            messages.append(\n                UserMessage(\n                    role='user',\n                    content=[UserTextContent(text=wx_message.u)]\n                )\n            )\n        if wx_message.a:\n            messages.append(\n                AssistantMessage(content=wx_message.a))\n        if wx_message.n:\n            pass\n\n    return messages\n"}
{"type": "source_file", "path": "src/llm/pattern_detection/base_buffered_processor.py", "content": "# src/llm/pattern_detection/base_buffered_processor.py\n\nfrom abc import abstractmethod\nfrom src.data_models.streaming import PatternMatchResult\n\n\nclass BaseBufferedProcessor:\n    \"\"\"Base class for buffered text processors.\n\n    This abstract base class implements common buffering logic for streaming text\n    processing. Subclasses must implement process_chunk_impl to define specific\n    matching behavior.\n\n    Attributes:\n        `tool_call_message`: Message to include when a tool call is detected.\n        `trailing_buffer_original`: Buffer containing text carried over from previous chunks.\n\n    Args:\n        `tool_call_message`: Optional message to use when a tool call is detected.\n            Defaults to \"Tool call detected.\"\n    \"\"\"\n\n    def __init__(self, tool_call_message: str = \"Tool call detected.\"):\n        self.tool_call_message = tool_call_message\n        self.trailing_buffer_original = \"\"\n\n    def reset_states(self):\n        \"\"\"Resets the processor's internal state.\n\n        Clears the trailing buffer and resets any internal state to initial values.\n        Should be called before starting to process a new stream of text.\n        \"\"\"\n        self.trailing_buffer_original = \"\"\n\n    async def process_chunk(self, chunk: str) -> PatternMatchResult:\n        \"\"\"Processes a chunk of text with buffering.\n\n        Combines the new chunk with any trailing text from previous chunks and\n        delegates the actual processing to process_chunk_impl.\n\n        Args:\n            `chunk`: The new text chunk to process.\n\n        Returns:\n            `PatternMatchResult` containing the processed output and any match information.\n\n        ``` python title=\"Example usage\"\n        processor = MyProcessor()\n        result = await processor.process_chunk(\"some text\")\n        print(result.output)  # some text\n        ```\n        \"\"\"\n        # Combine the trailing buffer with new chunk\n        combined_original = self.trailing_buffer_original + chunk\n\n        # Let the subclass handle the matching logic on combined_original.\n        result, new_trailing = self.process_chunk_impl(combined_original)\n\n        # Update the trailing buffer with what's left\n        self.trailing_buffer_original = new_trailing\n        return result\n\n    async def flush_buffer(self) -> PatternMatchResult:\n        \"\"\"Flushes any remaining text in the trailing buffer.\n\n        Should be called after processing the final chunk to handle any remaining\n        buffered text.\n\n        Returns:\n            `PatternMatchResult` containing any remaining buffered text.\n\n        ``` python title=\"Example usage\"\n        processor = MyProcessor()\n        result = await processor.flush_buffer()\n        print(len(result.output))  # 0\n        ```\n        \"\"\"\n        from src.data_models.streaming import PatternMatchResult\n        result = PatternMatchResult()\n        if self.trailing_buffer_original:\n            result.output = self.trailing_buffer_original\n            self.trailing_buffer_original = \"\"\n        return result\n\n    @abstractmethod\n    def process_chunk_impl(self, combined_original: str):\n        \"\"\"Processes combined text to find pattern matches.\n\n        This abstract method must be implemented by subclasses to define specific\n        pattern matching behavior.\n\n        Args:\n            `combined_original`: Text to process, including any trailing text from\n                previous chunks.\n\n        Returns:\n            A tuple containing:\n\n                - `PatternMatchResult`: Result object with match information and\n                    processed text.\n                - `str`: Any trailing text to carry over to the next chunk.\n        \"\"\"\n        pass\n"}
{"type": "source_file", "path": "src/llm/adapters/anthropic_adapter.py", "content": "# src/llm/adapters/anthropic_adapter.py\n\nimport os\nimport time\nimport logging\nfrom typing import AsyncGenerator, List, Optional, Union, Dict, Any\n\nfrom anthropic import AsyncAnthropic\nfrom src.data_models.tools import Tool\nfrom src.llm.adapters.base_vendor_adapter import BaseVendorAdapter\nfrom src.api import SSEChunk, SSEChoice, SSEDelta, SSEToolCall, SSEFunction\nfrom src.data_models.chat_completions import (\n    UserMessage,\n    SystemMessage,\n    AssistantMessage,\n    ToolMessage,\n    TextChatMessage,\n    UserTextContent,\n    UserImageURLContent,\n)\n\nlogger = logging.getLogger(__name__)\n\n\n# --------------------------------------------------\n# Content Conversion Functions\n# --------------------------------------------------\ndef convert_content(\n    content: Union[str, List[Union[UserTextContent, UserImageURLContent, Dict[str, Any]]]]\n) -> List[Dict[str, Any]]:\n    \"\"\"Convert message content into Anthropic content blocks.\n\n    Args:\n        content (Union[str, List[Union[UserTextContent, UserImageURLContent, Dict[str, Any]]]]):\n            Either a simple text string or a list of content blocks. The list can contain:\n\n            - `UserTextContent`: A text content block.\n            - `UserImageURLContent`: An image content block.\n            - `Dict`: A dictionary representing other content types (e.g., for assistant messages).\n\n    Returns:\n        List[Dict[str, Any]]: A list of content blocks in Anthropic-compatible format.\n\n    Raises:\n        ValueError: If the content is neither a string nor a list, or if a block is of an invalid type.\n    \"\"\"\n    if isinstance(content, str):\n        return [{\"type\": \"text\", \"text\": content}]\n\n    if not isinstance(content, list):\n        raise ValueError(f\"Content must be a string or a list, got {type(content)}\")\n\n    blocks = []\n    for block in content:\n        if isinstance(block, UserTextContent):\n            blocks.append({\"type\": \"text\", \"text\": block.text})\n        elif isinstance(block, UserImageURLContent):\n            blocks.append({\n                \"type\": \"image\",\n                \"source\": {\n                    \"type\": \"base64\",\n                    \"media_type\": \"image/jpeg\",\n                    \"data\": block.image_url.get(\"url\", \"\"),\n                }\n            })\n        elif isinstance(block, dict):\n            block_type = block.get(\"type\")\n            if block_type == \"text\":\n                blocks.append({\"type\": \"text\", \"text\": block.get(\"text\", \"\")})\n            elif block_type == \"image_url\":\n                blocks.append({\n                    \"type\": \"image\",\n                    \"source\": {\n                        \"type\": \"base64\",\n                        \"media_type\": \"image/jpeg\",\n                        \"data\": block.get(\"image_url\", {}).get(\"url\", \"\"),\n                    }\n                })\n            else:\n                # Pass through other block types (e.g., tool_use or tool_result)\n                blocks.append(block)\n        else:\n            raise ValueError(f\"Invalid content block type: {type(block)}\")\n\n    return blocks\n\n\n# --------------------------------------------------\n# Tool Conversion Functions\n# --------------------------------------------------\ndef convert_tool_to_anthropic_format(tool: Tool) -> dict:\n    \"\"\"Convert our Tool model to Anthropic's format.\n\n    Args:\n        tool (Tool): The tool to convert.\n\n    Returns:\n        dict: A dictionary representing the tool in Anthropic's expected format.\n    \"\"\"\n    input_schema = tool.function.parameters.model_dump() if tool.function.parameters else {}\n    return {\n        \"name\": tool.function.name,\n        \"description\": tool.function.description or \"\",\n        \"input_schema\": input_schema,\n    }\n\n\n# --------------------------------------------------\n# Message Conversion Functions\n# --------------------------------------------------\ndef convert_user_message(msg: UserMessage) -> Dict[str, Any]:\n    \"\"\"Convert a UserMessage to Anthropic format.\n\n    Args:\n        msg (UserMessage): The user message to convert.\n\n    Returns:\n        Dict[str, Any]: The converted message in Anthropic format.\n    \"\"\"\n    return {\"role\": \"user\", \"content\": convert_content(msg.content)}\n\n\ndef convert_assistant_message(msg: AssistantMessage) -> Dict[str, Any]:\n    \"\"\"Convert an AssistantMessage to Anthropic format.\n\n    Args:\n        msg (AssistantMessage): The assistant message to convert.\n\n    Returns:\n        Dict[str, Any]: The converted message in Anthropic format.\n    \"\"\"\n    blocks = []\n    if msg.content:\n        blocks.extend(convert_content(msg.content))\n    if msg.refusal:\n        blocks.append({\"type\": \"text\", \"text\": msg.refusal})\n    if msg.tool_calls:\n        for call in msg.tool_calls:\n            blocks.append({\n                \"type\": \"tool_use\",\n                \"id\": call.id,\n                \"name\": call.function.name,\n                \"input\": call.function.arguments,\n            })\n    return {\"role\": \"assistant\", \"content\": blocks}\n\n\ndef convert_tool_message(msg: ToolMessage) -> Dict[str, Any]:\n    \"\"\"Convert a ToolMessage to Anthropic format.\n\n    Note:\n        Tool results must be returned as user messages for Anthropic.\n\n    Args:\n        msg (ToolMessage): The tool message to convert.\n\n    Returns:\n        Dict[str, Any]: The converted tool message in Anthropic format.\n    \"\"\"\n    return {\n        \"role\": \"user\",\n        \"content\": [{\n            \"type\": \"tool_result\",\n            \"tool_use_id\": msg.tool_call_id,\n            \"content\": msg.content,\n        }],\n    }\n\n\ndef convert_messages_to_anthropic(messages: List[TextChatMessage]) -> Dict[str, Any]:\n    \"\"\"Convert a list of messages to Anthropic's format.\n\n    Args:\n        messages (List[TextChatMessage]): A list of chat messages.\n\n    Returns:\n        Dict[str, Any]: A dictionary containing the converted messages and an optional system prompt.\n    \"\"\"\n    system_prompt = None\n    converted = []\n\n    for msg in messages:\n        if isinstance(msg, SystemMessage):\n            system_prompt = (system_prompt + \"\\n\" + msg.content) if system_prompt else msg.content\n        elif isinstance(msg, UserMessage):\n            converted.append(convert_user_message(msg))\n        elif isinstance(msg, AssistantMessage):\n            converted.append(convert_assistant_message(msg))\n        elif isinstance(msg, ToolMessage):\n            converted.append(convert_tool_message(msg))\n        else:\n            raise ValueError(f\"Unsupported message type: {type(msg)}\")\n\n    result = {\"messages\": converted}\n    if system_prompt:\n        result[\"system\"] = system_prompt\n    return result\n\n\n# --------------------------------------------------\n# Anthropic Adapter\n# --------------------------------------------------\nclass AnthropicAdapter(BaseVendorAdapter):\n    \"\"\"Adapter for interacting with Anthropic's API.\"\"\"\n\n    def __init__(self, model_name: str, **default_params):\n        \"\"\"Initialize Anthropic Adapter.\n\n        Args:\n            model_name (str): The name of the model to use.\n            **default_params: Additional default parameters for the adapter.\n\n        Raises:\n            ValueError: If the Anthropic API key is missing.\n        \"\"\"\n        self.api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n        if not self.api_key:\n            raise ValueError(\n                \"Missing Anthropic API key. Set the ANTHROPIC_API_KEY environment variable.\"\n            )\n        self.client = AsyncAnthropic(api_key=self.api_key)\n        self.model_name = model_name\n        self.default_params = default_params\n        logger.info(f\"Anthropic Adapter initialized with model: {self.model_name}\")\n\n    async def gen_sse_stream(\n        self, prompt: str, **kwargs\n    ) -> AsyncGenerator[SSEChunk, None]:\n        \"\"\"Generate an SSE stream from a text prompt.\n\n        Args:\n            prompt (str): The text prompt to generate an SSE stream for.\n            **kwargs: Additional keyword arguments for generation.\n\n        Yields:\n            AsyncGenerator[SSEChunk, None]: A generator yielding SSEChunk objects.\n        \"\"\"\n        async for chunk in self.gen_chat_sse_stream([{\"role\": \"user\", \"content\": prompt}], **kwargs):\n            yield chunk\n\n    async def gen_chat_sse_stream(\n        self,\n        messages: List[TextChatMessage],\n        tools: Optional[List[Tool]] = None,\n        **kwargs,\n    ) -> AsyncGenerator[SSEChunk, None]:\n        \"\"\"Generate a streaming chat response.\n\n        Args:\n            `messages` (List[TextChatMessage]): A list of chat messages.\n            `tools` (Optional[List[Tool]], optional): A list of Tool objects. Defaults to None.\n            `**kwargs`: Additional keyword arguments for generation.\n\n        Yields:\n            AsyncGenerator[SSEChunk, None]: A generator yielding SSEChunk objects.\n        \"\"\"\n        request_payload = {\n            \"model\": self.model_name,\n            \"max_tokens\": self.default_params.get(\"max_tokens\", 1024),\n            \"stream\": True,\n            **self.default_params,\n            **kwargs,\n            **convert_messages_to_anthropic(messages),\n        }\n\n        if tools:\n            anthropic_tools = [convert_tool_to_anthropic_format(tool) for tool in tools]\n            request_payload[\"tools\"] = anthropic_tools\n            request_payload[\"tool_choice\"] = {\"type\": \"auto\"}\n\n        try:\n            stream = await self.client.messages.create(**request_payload)\n            async for event in stream:\n                yield await self._convert_to_sse_chunk(event)\n        except Exception as e:\n            logger.error(f\"Error in Anthropic streaming: {str(e)}\", exc_info=True)\n            raise RuntimeError(f\"Anthropic API streaming failed: {str(e)}\") from e\n\n    async def _convert_to_sse_chunk(self, raw_event: Any) -> SSEChunk:\n        \"\"\"Convert an Anthropic event to an SSEChunk.\n\n        Args:\n            `raw_event` (Any): The raw event from the Anthropic API.\n\n        Returns:\n            SSEChunk: The converted SSEChunk object.\n\n        Raises:\n            ValueError: If conversion of the event fails.\n        \"\"\"\n        try:\n            event_type = raw_event.type\n            current_time = int(time.time())\n\n            match event_type:\n                case \"content_block_start\":\n                    content_block = raw_event.content_block\n                    if content_block.type == \"text\":\n                        delta = SSEDelta(\n                            role=\"assistant\",\n                            content=getattr(content_block, \"text\", \"\"),\n                        )\n                    elif content_block.type == \"tool_use\":\n                        delta = SSEDelta(\n                            role=\"assistant\",\n                            content=\"\",\n                            tool_calls=[SSEToolCall(\n                                id=content_block.id,\n                                type=\"function\",\n                                function=SSEFunction(name=content_block.name, arguments=\"\"),\n                            )],\n                        )\n                    else:\n                        delta = SSEDelta(role=\"assistant\", content=\"\")\n                    choice = SSEChoice(index=raw_event.index, delta=delta)\n                    return SSEChunk(\n                        id=f\"content_block_start_{raw_event.index}\",\n                        object=\"chat.completion.chunk\",\n                        created=current_time,\n                        model=self.model_name,\n                        choices=[choice],\n                    )\n\n                case \"content_block_delta\":\n                    delta_info = raw_event.delta\n                    if delta_info.type == \"text_delta\":\n                        delta = SSEDelta(\n                            role=\"assistant\",\n                            content=delta_info.text,\n                        )\n                    elif delta_info.type == \"input_json_delta\":\n                        delta = SSEDelta(\n                            role=\"assistant\",\n                            content=\"\",\n                            tool_calls=[SSEToolCall(\n                                type=\"function\",\n                                function=SSEFunction(\n                                    name=\"\",\n                                    arguments=delta_info.partial_json,\n                                ),\n                            )],\n                        )\n                    else:\n                        delta = SSEDelta(role=\"assistant\", content=\"\")\n                    choice = SSEChoice(index=raw_event.index, delta=delta)\n                    return SSEChunk(\n                        id=f\"delta_{raw_event.index}\",\n                        object=\"chat.completion.chunk\",\n                        created=current_time,\n                        model=self.model_name,\n                        choices=[choice],\n                    )\n\n                case \"content_block_stop\":\n                    delta = SSEDelta(role=\"assistant\", content=\"\")\n                    choice = SSEChoice(index=raw_event.index, delta=delta)\n                    return SSEChunk(\n                        id=f\"block_stop_{raw_event.index}\",\n                        object=\"chat.completion.chunk\",\n                        created=current_time,\n                        model=self.model_name,\n                        choices=[choice],\n                    )\n\n                case \"message_delta\":\n                    delta = SSEDelta(role=\"assistant\", content=\"\")\n                    choice = SSEChoice(\n                        index=0,\n                        delta=delta,\n                        finish_reason=getattr(raw_event.delta, \"stop_reason\", None),\n                    )\n                    return SSEChunk(\n                        id=\"message_delta\",\n                        object=\"chat.completion.chunk\",\n                        created=current_time,\n                        model=self.model_name,\n                        choices=[choice],\n                    )\n\n                case \"message_stop\":\n                    delta = SSEDelta(role=\"assistant\", content=\"\")\n                    choice = SSEChoice(index=0, delta=delta)\n                    return SSEChunk(\n                        id=\"message_stop\",\n                        object=\"chat.completion.chunk\",\n                        created=current_time,\n                        model=self.model_name,\n                        choices=[choice],\n                    )\n\n                case _:\n                    delta = SSEDelta(role=\"assistant\", content=\"\")\n                    choice = SSEChoice(index=0, delta=delta)\n                    return SSEChunk(\n                        id=f\"unknown_{event_type}\",\n                        object=\"chat.completion.chunk\",\n                        created=current_time,\n                        model=self.model_name,\n                        choices=[choice],\n                    )\n\n        except Exception as e:\n            logger.error(f\"Error converting Anthropic event: {raw_event}\", exc_info=True)\n            raise ValueError(f\"Failed to convert Anthropic response to SSEChunk: {str(e)}\") from e\n"}
{"type": "source_file", "path": "src/database/query_builder.py", "content": "# src/database/query_builder.py\n\nimport json\nfrom typing import Dict, Any\nfrom string import Template\n\n\nclass ElasticQueryBuilder:\n    def __init__(self, config: Dict[str, Any]):\n        \"\"\"\n        Initialize the QueryBuilder with the Elasticsearch-related config.\n\n        Args:\n            config (Dict[str, Any]): Elasticsearch config, including the query templates and other parameters.\n        \"\"\"\n        if not config or 'query_body' not in config:\n            raise ValueError(\"Elasticsearch config must include 'query_body'.\")\n\n        self.query_body: Dict[str, Any] = config.get('query_body')\n        self.timeout: int = config.get('timeout', 30)\n        self.max_retries: int = config.get('max_retries', 3)\n        self.overfetch_buffer: int = config.get('overfetch_buffer', 50)\n\n    def get_query(self, user_input: str) -> Dict[str, Any]:\n        \"\"\"\n        Retrieve and process the query template with user input.\n\n        Args:\n            user_input (str): The user input to inject into the query.\n\n        Returns:\n            Dict[str, Any]: The processed query body.\n        \"\"\"\n        if not self.query_body:\n            raise ValueError(\"No query body found in the Elasticsearch configuration.\")\n\n        # Convert the query body to JSON string for template substitution\n        query_body_str = json.dumps(self.query_body)\n\n        # Escape user input properly using json.dumps\n        escaped_user_input = json.dumps(user_input)[1:-1]\n\n        # Replace placeholders using Python's Template\n        processed_query_str = Template(query_body_str).substitute(USER_INPUT=escaped_user_input)\n\n        return json.loads(processed_query_str)\n"}
{"type": "source_file", "path": "src/llm/adapters/watsonx/__init__.py", "content": ""}
{"type": "source_file", "path": "src/llm/adapters/base_vendor_adapter.py", "content": "# src/llm/adapters/base_vendor_adapter.py\n\nfrom abc import abstractmethod\nfrom typing import AsyncGenerator, Optional, List\nfrom ...api.sse_models import SSEChunk\nfrom ...data_models.tools import Tool\nfrom ...data_models.chat_completions import TextChatMessage\n\n\nclass BaseVendorAdapter:\n    \"\"\"\n    Abstract base class for any LLM vendor adapter.\n    Must produce SSEChunk objects when streaming text.\n    \"\"\"\n\n    @abstractmethod\n    async def gen_sse_stream(self, prompt: str) -> AsyncGenerator[SSEChunk, None]:\n        \"\"\"\n        Generate SSEChunk objects in a streaming manner from the given prompt.\n        \"\"\"\n        pass\n\n    # Optionally, you can define a chat method if you differentiate chat vs text\n    @abstractmethod\n    async def gen_chat_sse_stream(self, messages: List[TextChatMessage], tools: Optional[List[Tool]]) -> AsyncGenerator[SSEChunk, None]:\n        pass\n"}
{"type": "source_file", "path": "src/database/milvus_client.py", "content": "import yaml\nfrom pymilvus import (\n    connections,\n    Collection,\n    CollectionSchema,\n    FieldSchema,\n    DataType,\n    list_collections,\n    drop_collection,\n)\nfrom .base_adapter import DatabaseAdapter\n\n\nclass MilvusClient(DatabaseAdapter):\n    def __init__(\n            self,\n            config_file,\n            collection_name,\n            vector_dim,\n            additional_fields=None,\n            index_params=None,\n    ):\n        \"\"\"Initializes a Milvus client for vector database operations.\n\n        Args:\n            config_file (str): Path to the YAML configuration file.\n            collection_name (str): Name of the Milvus collection to be used or created.\n            vector_dim (int): Dimension of the vectors stored in the collection.\n            additional_fields (list, optional): Additional fields for the collection schema.\n        \"\"\"\n        self._load_config(config_file)\n        self.collection_name = collection_name\n        self.vector_dim = vector_dim\n        self.additional_fields = additional_fields if additional_fields else []\n        self.index_params = (\n            index_params\n            if index_params\n            else {\n                \"index_type\": \"IVF_FLAT\",\n                \"metric_type\": \"L2\",\n                \"params\": {\"nlist\": 1024},\n            }\n        )\n        self._connect()\n        self._create_collection()\n\n    def add(self, vector, metadata, check_dup=False):\n        \"\"\"Adds a vector and its associated metadata to the collection.\n\n        Args:\n            vector (list[float]): The vector to be added.\n            metadata (dict): Metadata associated with the vector.\n            check_dup (bool): Flag to check for duplicates before insertion (currently not implemented).\n        \"\"\"\n        data = [vector] + [metadata[field] for field in metadata]\n        self.collection.insert(data)\n        self.collection.flush()  # Ensures data persistence\n\n    def search(\n            self,\n            vector,\n            top_k,\n            distance_range=None,\n            search_params=None,\n            output_fields=None,\n            filter_expr=None,\n    ):\n        \"\"\"Performs a vector search in the collection.\n\n        Args:\n            vector (list[float]): The query vector.\n            top_k (int): Number of top results to return.\n            distance_range (list[int, int], optional): Minimum and maximum distances for filtering results.\n            search_params (dict, optional): Parameters for the search.\n            output_fields (list, optional): Fields to include in the returned results.\n            filter_expr (str, optional): Filter expression for conditional search.\n\n        Returns:\n            SearchResult: Search results from Milvus.\n        \"\"\"\n        self.collection.load()\n        if search_params is None:\n            search_params = {\"metric_type\": \"L2\", \"params\": {\"nprobe\": 10}}\n        if distance_range:\n            search_params[\"params\"].update(\n                {\"range_filter\": distance_range[0], \"radius\": distance_range[1]}\n            )\n\n        return self.collection.search(\n            data=[vector],\n            anns_field=\"vector\",\n            param=search_params,\n            limit=top_k,\n            expr=filter_expr,\n            output_fields=output_fields,\n        )\n\n    def reset(self):\n        \"\"\"Drops the current collection and creates a new one.\"\"\"\n        self.reset_collection()\n\n    def reset_collection(self):\n        \"\"\"Drops the current collection and creates a new one.\"\"\"\n        if self.collection_name in list_collections():\n            drop_collection(self.collection_name)\n        self._create_collection()\n\n    def _load_config(self, config_file):\n        \"\"\"Loads configuration settings from a YAML file.\"\"\"\n        with open(config_file, \"r\") as file:\n            self.config = yaml.safe_load(file)\n\n    def _connect(self, secure: bool = True):\n        \"\"\"Connects to the Milvus server using loaded configuration.\"\"\"\n        # Connect using only the non-None configuration parameters\n        connections.connect(**{k: v for k, v in self.config.items() if v is not None}, )\n\n    def _create_collection(\n            self, **kwargs\n    ):  # add ability to enable enable_dynamic_field and other Collection params\n        \"\"\"Creates a new collection in Milvus or loads an existing one.\"\"\"\n        if self.collection_name not in list_collections():\n            fields = [\n                         FieldSchema(\n                             name=\"id\", dtype=DataType.INT64, is_primary=True, auto_id=True\n                         ),\n                         FieldSchema(\n                             name=\"vector\", dtype=DataType.FLOAT_VECTOR, dim=self.vector_dim\n                         ),\n                     ] + [\n                         FieldSchema(\n                             name=field[\"name\"],\n                             dtype=field[\"dtype\"],\n                             max_length=field.get(\"max_length\"),\n                         )\n                         for field in self.additional_fields\n                     ]\n            schema = CollectionSchema(\n                fields, \"Vector collection with additional metadata\"\n            )\n            self.collection = Collection(name=self.collection_name, schema=schema)\n            self._create_index()\n        else:\n            self.collection = Collection(name=self.collection_name)\n\n    def _print_collection_schema(self):\n        \"\"\"Prints the schema of the current collection.\"\"\"\n        if self.collection_name in list_collections():\n            collection = Collection(name=self.collection_name)\n            print(f\"Schema for collection '{self.collection_name}':\")\n            for field in collection.schema.fields:\n                print(\n                    f\"Field Name: {field.name}, Data Type: {field.dtype}, Description: {field.description}\"\n                )\n        else:\n            print(f\"Collection '{self.collection_name}' does not exist.\")\n\n    def _create_index(self, index_params=None):\n        \"\"\"Creates an index for efficient search in the collection.\"\"\"\n        self.collection.create_index(\n            field_name=\"vector\", index_params=self.index_params\n        )\n"}
{"type": "source_file", "path": "src/database/__init__.py", "content": "from .base_adapter import DatabaseAdapter\nfrom .milvus_client import MilvusClient\nfrom .elastic_client import ElasticsearchClient\nfrom .query_builder import ElasticQueryBuilder\n"}
{"type": "source_file", "path": "src/llm/pattern_detection/__init__.py", "content": "from .buffered_processor_standard import AhoCorasickBufferedProcessor\nfrom .buffered_processor_normalized import AhoCorasickBufferedProcessorNormalized\n"}
{"type": "source_file", "path": "src/llm/adapters/openai_adapter.py", "content": "# src/llm/adapters/openai_adapter.py\n\nimport os\nimport logging\nfrom openai import AsyncOpenAI\nfrom typing import AsyncGenerator, List, Optional\nfrom openai.types.chat.chat_completion_chunk import ChatCompletionChunk\nfrom src.data_models.tools import Tool\nfrom src.llm.adapters import BaseVendorAdapter\nfrom src.data_models.chat_completions import TextChatMessage\nfrom src.api import SSEChunk, SSEChoice, SSEDelta, SSEToolCall, SSEFunction\n\nlogger = logging.getLogger(__name__)\n\n\nclass OpenAIAdapter(BaseVendorAdapter):\n    \"\"\"Adapter for interacting with OpenAI's API.\n\n    This class implements the BaseVendorAdapter interface for OpenAI's chat models,\n    handling authentication, request formatting, and response streaming. It converts\n    OpenAI-specific response formats into standardized SSE chunks for consistent\n    handling across different LLM providers.\n\n    Attributes:\n        api_key (str): OpenAI API key loaded from environment variables.\n        client (AsyncOpenAI): Authenticated OpenAI client instance.\n        model_name (str): The OpenAI model identifier (e.g., \"gpt-4\").\n        default_params (dict): Default parameters for OpenAI API calls.\n    \"\"\"\n\n    def __init__(self, model_name: str, **default_params):\n        \"\"\"Initialize the OpenAI Adapter with model configuration.\n\n        Args:\n            model_name (str): The identifier of the OpenAI model to use (e.g., \"gpt-4\").\n            **default_params: Additional parameters to include in all API calls.\n                Common parameters include temperature, max_tokens, etc.\n\n        Raises:\n            ValueError: If OPENAI_API_KEY environment variable is not set.\n        \"\"\"\n        self.api_key = os.getenv(\"OPENAI_API_KEY\")\n        if not self.api_key:\n            raise ValueError(\"Missing OpenAI API key. Set the OPENAI_API_KEY environment variable.\")\n\n        self.client = AsyncOpenAI()\n        self.client.api_key = self.api_key\n\n        self.model_name = model_name\n        self.default_params = default_params\n        logger.info(f\"OpenAI Adapter initialized with model: {self.model_name}\")\n        logger.debug(f\"Default parameters configured: {default_params}\")\n\n    async def gen_sse_stream(\n            self,\n            prompt: str,\n            **kwargs\n    ) -> AsyncGenerator[SSEChunk, None]:\n        \"\"\"Generate SSE stream from a single text prompt.\n\n        Converts a single prompt into a chat message and streams the response.\n\n        Args:\n            prompt (str): The text prompt to send to the model.\n            **kwargs: Additional parameters to override defaults for this request.\n\n        Yields:\n            SSEChunk: Standardized chunks of the streaming response.\n\n        Raises:\n            RuntimeError: If the streaming request fails.\n        \"\"\"\n        logger.debug(f\"Converting single prompt to chat format: {prompt[:50]}...\")\n        messages = [{\"role\": \"user\", \"content\": prompt}]\n        async for chunk in self.gen_chat_sse_stream(messages, **kwargs):\n            yield chunk\n\n    async def gen_chat_sse_stream(\n            self,\n            messages: List[TextChatMessage],\n            tools: Optional[List[Tool]] = None,\n            **kwargs,\n    ) -> AsyncGenerator[SSEChunk, None]:\n        \"\"\"Generate a streaming chat response from a sequence of messages.\n\n        Args:\n            messages (List[TextChatMessage]): List of chat messages for context.\n            tools (Optional[List[Tool]]): List of tools available to the model.\n            **kwargs: Additional parameters to override defaults for this request.\n\n        Yields:\n            SSEChunk: Standardized chunks of the streaming response.\n\n        Raises:\n            RuntimeError: If the OpenAI API request fails.\n        \"\"\"\n        openai_messages = [msg.model_dump() for msg in messages]\n        logger.debug(f\"Processing chat stream request with {len(messages)} messages\")\n\n        request_payload = {\n            \"model\": self.model_name,\n            \"messages\": openai_messages,\n            \"stream\": True,\n            **self.default_params,\n            **kwargs,\n        }\n\n        if tools:\n            logger.debug(f\"Adding {len(tools)} tools to request\")\n            request_payload[\"tools\"] = [tool.model_dump() for tool in tools]\n            request_payload[\"tool_choice\"] = \"auto\"\n\n        try:\n            logger.debug(\"Initiating OpenAI streaming request\")\n            async for chunk in await self.client.chat.completions.create(**request_payload):\n                yield await self._convert_to_sse_chunk(chunk)\n        except Exception as e:\n            logger.error(f\"Error in OpenAI streaming: {str(e)}\", exc_info=True)\n            raise RuntimeError(f\"OpenAI API streaming failed: {str(e)}\") from e\n\n    async def _convert_to_sse_chunk(self, raw_chunk: ChatCompletionChunk) -> SSEChunk:\n        \"\"\"Convert OpenAI's response chunk to standardized SSE format.\n\n        Transforms OpenAI's ChatCompletionChunk into the application's\n        standardized SSEChunk format, handling all possible response fields\n        including tool calls, content, and metadata.\n\n        Args:\n            raw_chunk (ChatCompletionChunk): Raw chunk from OpenAI's API.\n\n        Returns:\n            SSEChunk: Standardized chunk format for consistent handling.\n\n        Raises:\n            ValueError: If chunk conversion fails due to unexpected format.\n        \"\"\"\n        try:\n            logger.debug(f\"Converting chunk ID: {raw_chunk.id}\")\n            choices = []\n            for choice in raw_chunk.choices:\n                tool_calls = None\n                if choice.delta.tool_calls:\n                    tool_calls = []\n                    for tc in choice.delta.tool_calls:\n                        function = None\n                        if tc.function:\n                            function = SSEFunction(\n                                name=\"\" if tc.function.name is None else tc.function.name,\n                                arguments=\"\" if tc.function.arguments is None else tc.function.arguments\n                            )\n\n                        tool_calls.append(SSEToolCall(\n                            index=tc.index if tc.index is not None else 0,\n                            id=tc.id,\n                            type=tc.type if tc.type else \"function\",\n                            function=function\n                        ))\n\n                delta = SSEDelta(\n                    role=choice.delta.role,\n                    content=choice.delta.content,\n                    tool_calls=tool_calls,\n                    refusal=choice.delta.refusal\n                )\n\n                choices.append(SSEChoice(\n                    index=choice.index,\n                    delta=delta,\n                    logprobs=choice.logprobs,\n                    finish_reason=choice.finish_reason\n                ))\n\n            return SSEChunk(\n                id=raw_chunk.id,\n                object=raw_chunk.object,\n                created=raw_chunk.created,\n                model=raw_chunk.model,\n                service_tier=raw_chunk.service_tier,\n                system_fingerprint=raw_chunk.system_fingerprint,\n                choices=choices\n            )\n\n        except Exception as e:\n            logger.error(f\"Error converting OpenAI chunk: {raw_chunk}\", exc_info=True)\n            raise ValueError(f\"Failed to convert OpenAI response to SSEChunk: {str(e)}\") from e\n"}
{"type": "source_file", "path": "src/llm/adapters/watsonx/ibm_token_manager.py", "content": "# src/llm/adapters/watsonx/ibm_token_manager.py\n\nimport os\nimport time\nimport asyncio\nimport aiohttp\nimport logging\nfrom typing import Optional\n\nlogger = logging.getLogger(__name__)\n\n\nclass IBMTokenManager:\n    \"\"\"Manages IBM Cloud OAuth2 token lifecycle for WatsonX API access.\n\n    This class handles authentication token management for IBM Cloud services,\n    including automatic token refresh and thread-safe token access. It implements\n    a singleton pattern to maintain one token instance across the application.\n\n    Attributes:\n        api_key (str): IBM Cloud API key for authentication.\n        token_url (str): IBM IAM authentication endpoint URL.\n        refresh_buffer (int): Time buffer in seconds before token expiry to trigger refresh.\n        access_token (Optional[str]): Current valid access token.\n        expiry_time (float): Unix timestamp when the current token expires.\n        lock (asyncio.Lock): Async lock for thread-safe token refresh operations.\n    \"\"\"\n\n    def __init__(self, api_key: str, refresh_buffer: int = 60):\n        \"\"\"Initialize the IBM Token Manager.\n\n        Args:\n            api_key (str): IBM WatsonX API key for authentication.\n            refresh_buffer (int, optional): Buffer time in seconds before token expiry\n                to trigger a refresh. Defaults to 60 seconds.\n\n        Raises:\n            ValueError: If api_key is empty or None.\n            EnvironmentError: If IBM_AUTH_URL environment variable is not set.\n        \"\"\"\n        if not api_key:\n            raise ValueError(\"API key cannot be empty or None\")\n\n        self.api_key = api_key\n        self.token_url = os.getenv(\"IBM_AUTH_URL\")\n        if not self.token_url:\n            raise EnvironmentError(\"IBM_AUTH_URL environment variable not set\")\n\n        self.refresh_buffer = refresh_buffer\n        self.access_token: Optional[str] = None\n        self.expiry_time: float = 0\n        self.lock = asyncio.Lock()\n\n        logger.debug(\"Initialized IBMTokenManager with refresh buffer of %d seconds\", refresh_buffer)\n\n    async def _is_token_expired(self) -> bool:\n        \"\"\"Check if the current token is expired or approaching expiry.\n\n        Returns:\n            bool: True if token is expired or will expire soon, False otherwise.\n        \"\"\"\n        current_time = time.time()\n        is_expired = self.access_token is None or current_time > (self.expiry_time - self.refresh_buffer)\n\n        if is_expired:\n            logger.debug(\"Token is expired or approaching expiry\")\n        return is_expired\n\n    async def _refresh_token(self) -> None:\n        \"\"\"Fetch a new OAuth token from IBM IAM.\n\n        Raises:\n            aiohttp.ClientError: If the token refresh request fails.\n            ValueError: If the response doesn't contain expected token information.\n            Exception: For any other unexpected errors during token refresh.\n        \"\"\"\n        async with self.lock:\n            if not await self._is_token_expired():\n                logger.debug(\"Token refresh skipped - current token still valid\")\n                return\n\n            logger.debug(\"Starting token refresh process\")\n            try:\n                headers = {\"Content-Type\": \"application/x-www-form-urlencoded\"}\n                payload = {\n                    \"grant_type\": \"urn:ibm:params:oauth:grant-type:apikey\",\n                    \"apikey\": self.api_key\n                }\n\n                async with aiohttp.ClientSession() as session:\n                    logger.debug(\"Making token refresh request to IBM IAM\")\n                    async with session.post(self.token_url, headers=headers, data=payload) as response:\n                        response.raise_for_status()\n                        token_info = await response.json()\n\n                        if \"access_token\" not in token_info or \"expires_in\" not in token_info:\n                            raise ValueError(\"Invalid token response from IBM IAM\")\n\n                        self.access_token = token_info[\"access_token\"]\n                        expires_in = int(token_info[\"expires_in\"])\n                        self.expiry_time = time.time() + expires_in\n\n                        logger.info(\"Successfully refreshed IBM token. Expires in %d seconds\", expires_in)\n                        logger.debug(\"Token expiry time set to %s\", time.ctime(self.expiry_time))\n\n            except aiohttp.ClientError as e:\n                logger.error(\"HTTP error during token refresh: %s\", str(e))\n                self.access_token = None\n                raise\n            except Exception as e:\n                logger.error(\"Unexpected error during token refresh: %s\", str(e))\n                self.access_token = None\n                raise\n\n    async def get_token(self) -> Optional[str]:\n        \"\"\"Retrieve a valid access token, refreshing if necessary.\n\n        Returns:\n            Optional[str]: Valid access token if successful, None if token\n                refresh fails.\n\n        Raises:\n            Exception: If token refresh fails when attempted.\n        \"\"\"\n        logger.debug(\"Token requested\")\n        if await self._is_token_expired():\n            logger.debug(\"Token refresh needed before returning\")\n            await self._refresh_token()\n        return self.access_token\n"}
{"type": "source_file", "path": "src/database/elastic_client.py", "content": "import os\nfrom elasticsearch import AsyncElasticsearch\nfrom src.database.base_adapter import DatabaseAdapter\n\n\nclass ElasticsearchClient(DatabaseAdapter):\n    \"\"\"Asynchronous Elasticsearch client adapter for database operations.\n\n    This class implements the DatabaseAdapter interface to provide asynchronous\n    interaction with Elasticsearch. It handles document indexing, searching,\n    index management, and other essential Elasticsearch operations.\n\n    The client requires Elasticsearch endpoint and API key to be set in environment\n    variables ES_ENDPOINT and ES_API_KEY respectively.\n\n    Attributes:\n        client (AsyncElasticsearch): Async Elasticsearch client instance configured\n            with the provided endpoint and API key.\n\n    Raises:\n        ValueError: If either ES_ENDPOINT or ES_API_KEY environment variables are not set.\n\n    Example:\n        ```python\n        # Initialize the client\n        es_client = ElasticsearchClient()\n\n        # Add a document\n        document = {\"title\": \"Example\", \"content\": \"Sample text\"}\n        await es_client.add(document, \"my_index\")\n\n        # Search for documents\n        query = {\"query\": {\"match\": {\"content\": \"sample\"}}}\n        results = await es_client.search(query, \"my_index\")\n        ```\n    \"\"\"\n    def __init__(self, verify_certs: bool = True):\n        \"\"\"Initialize the Elasticsearch client with credentials from environment variables.\n\n        Establishes connection to Elasticsearch using endpoint and API key from\n        environment variables. Sets up the async client with specific configurations\n        for SSL verification and request timeout.\n\n        Raises:\n            ValueError: If required environment variables are not set.\n        \"\"\"\n        es_endpoint = os.getenv(\"ES_ENDPOINT\")\n        es_api_key = os.getenv(\"ES_API_KEY\")\n\n        if not es_endpoint or not es_api_key:\n            raise ValueError(\"Elasticsearch endpoint and API key must be provided.\")\n\n        self.client = AsyncElasticsearch(\n            es_endpoint,\n            api_key=es_api_key,\n            verify_certs=verify_certs,\n            request_timeout=60,\n        )\n\n    async def add(self, document, index_name):\n        \"\"\"Add a document to the specified Elasticsearch index.\n\n        Args:\n            document: Dictionary containing the document data to be indexed.\n            index_name (str): Name of the index to add the document to.\n\n        Returns:\n            dict: Elasticsearch response containing the indexing result.\n\n        Example:\n            ```python\n            response = await client.add(\n                {\"title\": \"Test\", \"content\": \"Content\"},\n                \"my_index\"\n            )\n            ```\n        \"\"\"\n        response = await self.client.index(index=index_name, body=document)\n        return response\n\n    async def search(self, query_body: dict, index_name: str, size: int = 5):\n        \"\"\"Search for documents in the specified index.\n\n        Args:\n            query_body (dict): Elasticsearch query DSL dictionary.\n            index_name (str): Name of the index to search in.\n            size (int, optional): Maximum number of results to return. Defaults to 5.\n\n        Returns:\n            dict: Elasticsearch response containing search results.\n\n        Example:\n            ```python\n            query = {\n                \"query\": {\n                    \"match\": {\n                        \"content\": \"search text\"\n                    }\n                }\n            }\n            results = await client.search(query, \"my_index\", size=10)\n            ```\n        \"\"\"\n        response = await self.client.search(\n            index=index_name,\n            body=query_body,\n            size=size,\n        )\n        return response\n\n    async def reset(self, index_name):\n        \"\"\"Delete all documents from the specified index.\n\n        Args:\n            index_name (str): Name of the index to reset.\n\n        Example:\n            ```python\n            await client.reset(\"my_index\")\n            ```\n        \"\"\"\n        await self.client.delete_by_query(\n            index=index_name,\n            body={\"query\": {\"match_all\": {}}},\n        )\n\n    async def create_index(self, index_name, settings=None, mappings=None):\n        \"\"\"Create a new Elasticsearch index if it doesn't exist.\n\n        Args:\n            index_name (str): Name of the index to create.\n            settings (dict, optional): Index settings configuration. Defaults to empty dict.\n            mappings (dict, optional): Index mappings configuration. Defaults to empty dict.\n\n        Example:\n            ```python\n            settings = {\"number_of_shards\": 1}\n            mappings = {\n                \"properties\": {\n                    \"title\": {\"type\": \"text\"},\n                    \"content\": {\"type\": \"text\"}\n                }\n            }\n            await client.create_index(\"my_index\", settings, mappings)\n            ```\n        \"\"\"\n        if not await self.client.indices.exists(index=index_name):\n            await self.client.indices.create(index=index_name, body={\n                \"settings\": settings if settings else {},\n                \"mappings\": mappings if mappings else {}\n            })\n\n    async def index_exists(self, index_name: str) -> bool:\n        \"\"\"Check if an index exists.\n\n        Args:\n            index_name (str): Name of the index to check.\n\n        Returns:\n            bool: True if index exists, False otherwise.\n\n        Example:\n            ```python\n            exists = await client.index_exists(\"my_index\")\n            if not exists:\n                await client.create_index(\"my_index\")\n            ```\n        \"\"\"\n        return await self.client.indices.exists(index=index_name)\n"}
{"type": "source_file", "path": "src/llm/adapters/watsonx/watsonx_config.py", "content": "# src/llm/watsonx_config.py\n\nimport os\nimport logging\nfrom dotenv import load_dotenv\n\nload_dotenv()\nlogger = logging.getLogger(__name__)\n\n\nclass WatsonXConfig:\n    \"\"\"Configuration management for WatsonX credentials and settings.\n\n    This class handles loading and validation of required WatsonX credentials\n    from environment variables.\n\n    Attributes:\n        CREDS (dict): Dictionary containing API key and URL for WatsonX.\n        PROJECT_ID (str): WatsonX project identifier.\n\n    Example:\n        ```python\n        # Validate credentials before use\n        WatsonXConfig.validate_credentials()\n\n        # Access credentials\n        credentials = WatsonXConfig.CREDS\n        project_id = WatsonXConfig.PROJECT_ID\n        ```\n    \"\"\"\n    CREDS = {'apikey': os.getenv(\"WXAI_API_KEY\"), 'url': os.getenv(\"WXAI_URL\")}\n    PROJECT_ID = os.getenv(\"WXAI_PROJECT_ID\")\n\n    @classmethod\n    def validate_credentials(cls):\n        \"\"\"Validate the presence of required WatsonX credentials.\n\n        Checks for the presence of all required credentials and logs appropriate\n        messages for missing values.\n\n        Raises:\n            ValueError: If any required credential is missing.\n        \"\"\"\n        missing = [key for key, value in {\n            'WXAI_API_KEY': cls.CREDS['apikey'],\n            'WXAI_URL': cls.CREDS['url'],\n            'WXAI_PROJECT_ID': cls.PROJECT_ID\n        }.items() if not value]\n\n        if missing:\n            logger.error(f\"Missing WatsonX credentials: {', '.join(missing)}\")\n            raise ValueError(f\"Missing WatsonX credentials: {', '.join(missing)}\")\n\n        logger.info(\"All WatsonX credentials loaded successfully.\")\n"}
{"type": "source_file", "path": "src/llm/pattern_detection/aho_corasick.py", "content": "# src/llm/pattern_detection/aho_corasick.py\n\n\"\"\"Implements the Aho-Corasick string matching algorithm for pattern detection.\n\nThis module provides an implementation of the Aho-Corasick automaton, which enables\nefficient multiple pattern string matching. The algorithm constructs a finite state\nmachine from a set of patterns and can find all occurrences of any pattern in a\ngiven text in O(n + m + k) time, where n is the length of the text, m is the total\nlength of patterns, and k is the number of pattern occurrences.\n\nExample:\n    patterns = {'pattern1': 'abc', 'pattern2': 'def'}\n    automaton = AhoCorasickAutomaton(patterns)\n    matches = automaton.search_chunk('abcdef')\n\"\"\"\n\nfrom collections import deque\nfrom typing import Dict, List, Tuple\n\n\nclass AhoCorasickAutomaton:\n    \"\"\"An implementation of the Aho-Corasick string matching automaton.\n\n    This class implements a finite state machine that can efficiently match multiple\n    patterns simultaneously in a given text. It uses a trie data structure augmented\n    with failure links to achieve linear-time pattern matching.\n\n    Attributes:\n        patterns: A dictionary mapping pattern names to their string values.\n        next_states: A list of dictionaries representing state transitions.\n        fail: A list of failure link states.\n        output: A list of pattern names associated with each state.\n        current_state: The current state of the automaton.\n\n    Args:\n        patterns: A dictionary where keys are pattern names and values are the\n            pattern strings to match.\n    \"\"\"\n\n    def __init__(self, patterns: Dict[str, str]):\n        \"\"\"Initializes the Aho-Corasick automaton with the given patterns.\n\n        Args:\n            patterns: A dictionary mapping pattern names to their string values.\n        \"\"\"\n        self.patterns = patterns\n        self.next_states: List[Dict[str, int]] = []\n        self.fail: List[int] = []\n        self.output: List[List[str]] = []\n        self.current_state = 0\n        self._build_machine()\n\n    def _build_machine(self):\n        \"\"\"Builds the Aho-Corasick automaton.\n\n        Constructs the trie structure, sets up failure links, and computes output\n        functions for the automaton. This is called automatically during initialization.\n        \"\"\"\n        # Initialize root state\n        self.next_states.append({})\n        self.fail.append(0)\n        self.output.append([])\n\n        # Build the trie from patterns\n        for pattern_name, pattern_str in self.patterns.items():\n            self._insert(pattern_str, pattern_name)\n\n        # Build failure links using BFS\n        queue = deque()\n        for char, nxt_state in self.next_states[0].items():\n            self.fail[nxt_state] = 0\n            queue.append(nxt_state)\n\n        while queue:\n            state = queue.popleft()\n            for char, nxt_state in self.next_states[state].items():\n                queue.append(nxt_state)\n                f = self.fail[state]\n                while f > 0 and char not in self.next_states[f]:\n                    f = self.fail[f]\n                f = self.next_states[f].get(char, 0)\n                self.fail[nxt_state] = f\n                self.output[nxt_state].extend(self.output[f])\n\n    def _insert(self, pattern_str: str, pattern_name: str):\n        \"\"\"Inserts a pattern into the trie structure of the automaton.\n\n         Args:\n             pattern_str: The string pattern to insert.\n             pattern_name: The name associated with the pattern.\n         \"\"\"\n        current_state = 0\n        for char in pattern_str:\n            if char not in self.next_states[current_state]:\n                self.next_states.append({})\n                self.fail.append(0)\n                self.output.append([])\n                self.next_states[current_state][char] = len(self.next_states) - 1\n            current_state = self.next_states[current_state][char]\n        self.output[current_state].append(pattern_name)\n\n    def reset_state(self):\n        \"\"\"Resets the automaton to its initial state.\n\n        This method should be called before starting a new search if the automaton\n        has been used previously.\n        \"\"\"\n        self.current_state = 0\n\n    def search_chunk(self, chunk: str) -> List[Tuple[int, str]]:\n        \"\"\"Searches for pattern matches in the given text chunk.\n\n        Args:\n            chunk: The text string to search for pattern matches.\n\n        Returns:\n            A list of tuples, where each tuple contains:\n                - The ending index of the match in the chunk (int)\n                - The name of the matched pattern (str)\n\n        ``` python title=\"Example usage\"\n        automaton = AhoCorasickAutomaton({'pat1': 'abc', 'pat2': 'bc'})\n        automaton.search_chunk('abc')  # [(1, 'pat2'), (2, 'pat1')]\n        ```\n        \"\"\"\n        found_patterns = []\n        for i, char in enumerate(chunk):\n            while self.current_state > 0 and char not in self.next_states[self.current_state]:\n                self.current_state = self.fail[self.current_state]\n            self.current_state = self.next_states[self.current_state].get(char, 0)\n            if self.output[self.current_state]:\n                for pattern_name in self.output[self.current_state]:\n                    found_patterns.append((i, pattern_name))\n        return found_patterns\n"}
{"type": "source_file", "path": "src/llm/adapters/openai_compat_adapter.py", "content": "# src/llm/adapters/openai_compat_adapter.py\n\nimport logging\nfrom typing import AsyncGenerator, List, Optional\nfrom openai import AsyncOpenAI\n\nfrom src.data_models.tools import Tool\nfrom src.llm.adapters import BaseVendorAdapter\nfrom src.data_models.chat_completions import TextChatMessage\nfrom src.api.sse_models import SSEChunk, SSEChoice, SSEDelta, SSEToolCall, SSEFunction\n\nlogger = logging.getLogger(__name__)\n\n\nclass OpenAICompatAdapter(BaseVendorAdapter):\n    \"\"\"Adapter for interacting with Open AI compatible APIs.\n\n    Supports both chat completions and completions endpoints. Handles streaming responses and converts\n    them to standardized SSE chunks.\n\n    Attributes:\n        model_name (str): The model identifier being served\n        base_url (str): URL of the server (default: http://localhost:8000/v1)\n        api_key (str): API key for server authentication\n        client (AsyncOpenAI): Configured OpenAI client\n    \"\"\"\n\n    def __init__(\n            self,\n            model_name: str,\n            base_url: str = \"http://localhost:8000/v1\",\n            api_key: str = \"dummy-key\",\n            **default_params\n    ):\n        \"\"\"Initialize the adapter.\n\n        Args:\n            model_name (str): Name of the model being served (e.g. \"NousResearch/Llama-2-7b\")\n            base_url (str): URL of the server\n            api_key (str): API key for authentication\n            **default_params: Additional parameters for generation (temperature etc.)\n        \"\"\"\n        self.model_name = model_name\n        self.base_url = base_url\n        self.api_key = api_key\n        self.default_params = default_params\n\n        # Configure OpenAI client for server\n        self.client = AsyncOpenAI(\n            base_url=self.base_url,\n            api_key=self.api_key\n        )\n\n        logger.info(f\"Initialized adapter for model: {self.model_name}\")\n        logger.debug(f\"Using server at: {self.base_url}\")\n        logger.debug(f\"Default parameters: {default_params}\")\n\n    async def gen_chat_sse_stream(\n            self,\n            messages: List[TextChatMessage],\n            tools: Optional[List[Tool]] = None,\n            **kwargs\n    ) -> AsyncGenerator[SSEChunk, None]:\n        \"\"\"Generate streaming chat completion using chat endpoint.\n\n        Args:\n            messages (List[TextChatMessage]): List of chat messages\n            tools (Optional[List[Tool]]): Optional tools/functions\n            **kwargs: Additional parameters to override defaults\n\n        Yields:\n            SSEChunk: Standardized chunks of the streaming response\n        \"\"\"\n        try:\n            # Convert messages to OpenAI format\n            openai_messages = [msg.model_dump() for msg in messages]\n\n            # Prepare request payload\n            request_params = {\n                \"model\": self.model_name,\n                \"messages\": openai_messages,\n                \"stream\": True,\n                **self.default_params,\n                **kwargs\n            }\n\n            # Add tools if provided\n            if tools:\n                request_params[\"tools\"] = [tool.model_dump() for tool in tools]\n                request_params[\"tool_choice\"] = \"auto\"\n\n            # Stream response\n            async for chunk in await self.client.chat.completions.create(**request_params):\n                yield self._convert_to_sse_chunk(chunk)\n\n        except Exception as e:\n            logger.error(f\"Error in chat stream: {str(e)}\", exc_info=True)\n            raise RuntimeError(f\"Chat completion failed: {str(e)}\") from e\n\n    async def gen_sse_stream(\n            self,\n            prompt: str,\n            **kwargs\n    ) -> AsyncGenerator[SSEChunk, None]:\n        \"\"\"Generate streaming completion using completions endpoint.\n\n        Args:\n            prompt (str): Input text prompt\n            **kwargs: Additional parameters to override defaults\n\n        Yields:\n            SSEChunk: Standardized chunks of the streaming response\n        \"\"\"\n        try:\n            # Prepare request payload\n            request_params = {\n                \"model\": self.model_name,\n                \"prompt\": prompt,\n                \"stream\": True,\n                **self.default_params,\n                **kwargs\n            }\n\n            logger.debug(f\"Making completions request with prompt: {prompt[:50]}...\")\n\n            # Use completions endpoint directly\n            async for chunk in await self.client.completions.create(**request_params):\n                yield self._convert_to_sse_chunk(chunk)\n\n        except Exception as e:\n            logger.error(f\"Error in completion stream: {str(e)}\", exc_info=True)\n            raise RuntimeError(f\"Completion failed: {str(e)}\") from e\n\n    def _convert_to_sse_chunk(self, raw_chunk) -> SSEChunk:\n        \"\"\"Convert response chunk to standardized SSE format.\n\n        Args:\n            raw_chunk: Raw chunk from API\n\n        Returns:\n            SSEChunk: Standardized chunk format\n        \"\"\"\n        try:\n            choices = []\n\n            # Check if this is a text completion or chat completion by looking at the object type\n            if raw_chunk.object == 'text_completion':\n                # Handle text completion format\n                for choice in raw_chunk.choices:\n                    choices.append(SSEChoice(\n                        index=choice.index,\n                        delta=SSEDelta(\n                            content=choice.text,\n                            role=\"assistant\"\n                        ),\n                        finish_reason=choice.finish_reason\n                    ))\n            else:\n                # Handle chat completion format\n                for choice in raw_chunk.choices:\n                    tool_calls = None\n                    if hasattr(choice.delta, 'tool_calls') and choice.delta.tool_calls:\n                        tool_calls = []\n                        for tc in choice.delta.tool_calls:\n                            function = None\n                            if tc.function:\n                                function = SSEFunction(\n                                    name=tc.function.name or \"\",\n                                    arguments=tc.function.arguments or \"\"\n                                )\n                            tool_calls.append(SSEToolCall(\n                                index=tc.index or 0,\n                                id=tc.id,\n                                type=tc.type or \"function\",\n                                function=function\n                            ))\n\n                    choices.append(SSEChoice(\n                        index=choice.index,\n                        delta=SSEDelta(\n                            role=choice.delta.role if hasattr(choice.delta, 'role') else None,\n                            content=choice.delta.content if hasattr(choice.delta, 'content') else None,\n                            tool_calls=tool_calls\n                        ),\n                        finish_reason=choice.finish_reason\n                    ))\n\n            return SSEChunk(\n                id=raw_chunk.id,\n                object=raw_chunk.object,\n                created=raw_chunk.created,\n                model=raw_chunk.model,\n                choices=choices\n            )\n\n        except Exception as e:\n            logger.error(f\"Error converting chunk: {raw_chunk}\", exc_info=True)\n            raise ValueError(f\"Failed to convert response: {str(e)}\") from e\n"}
{"type": "source_file", "path": "src/data_models/tools.py", "content": "# src/data_models/tools.py\n\nfrom pydantic import BaseModel, Field\nfrom typing import Optional, List, Dict, Any, Literal\n\n\nclass FunctionParameters(BaseModel):\n    \"\"\"Schema for function parameters following JSON Schema specification.\n\n    This class defines the structure for function parameters using JSON Schema.\n    It specifies the type, properties, required fields, and whether additional\n    properties are allowed.\n\n    Attributes:\n        type (str): The type of the parameters object, always \"object\".\n        properties (Dict[str, Dict[str, Any]]): Mapping of parameter names to their\n            JSON Schema definitions.\n        required (Optional[List[str]]): List of required parameter names.\n        additionalProperties (Optional[bool]): Whether additional properties beyond\n            those specified are allowed.\n\n    Example:\n        ```python\n        parameters = FunctionParameters(\n            type=\"object\",\n            properties={\n                \"location\": {\n                    \"type\": \"string\",\n                    \"description\": \"City name or coordinates\"\n                }\n            },\n            required=[\"location\"]\n        )\n        ```\n    \"\"\"\n    type: str = \"object\"\n    properties: Dict[str, Dict[str, Any]]\n    required: Optional[List[str]] = None\n    additionalProperties: Optional[bool] = None\n\n\nclass Function(BaseModel):\n    \"\"\"Represents a function that can be called by the model.\n\n    Defines the structure of a callable function, including its name,\n    description, parameters, and validation settings.\n\n    Attributes:\n        name (str): Function identifier, must be 1-64 characters and contain only\n            alphanumeric characters, underscores, and hyphens.\n        description (Optional[str]): Human-readable description of what the\n            function does.\n        parameters (Optional[FunctionParameters]): Schema defining the function's\n            parameters.\n        strict (Optional[bool]): Whether to enforce strict parameter validation.\n            Defaults to False.\n\n    Example:\n        ```python\n        function = Function(\n            name=\"get_weather\",\n            description=\"Get current weather for a location\",\n            parameters=FunctionParameters(...),\n            strict=True\n        )\n        ```\n    \"\"\"\n    name: str = Field(..., max_length=64, pattern=\"^[a-zA-Z0-9_-]+$\")\n    description: Optional[str] = None\n    parameters: Optional[FunctionParameters] = None\n    strict: Optional[bool] = Field(default=False)\n\n\nclass Tool(BaseModel):\n    \"\"\"Represents a tool that the model can use.\n\n    A tool is a wrapper around a function that can be called by the model.\n    Currently, only function-type tools are supported.\n\n    Attributes:\n        type (Literal[\"function\"]): The type of tool, currently only \"function\"\n            is supported.\n        function (Function): The function definition for this tool.\n\n    Example:\n        ```python\n        tool = Tool(\n            type=\"function\",\n            function=Function(\n                name=\"get_weather\",\n                description=\"Get current weather\",\n                parameters=FunctionParameters(...),\n                strict=True\n            )\n        )\n        ```\n    \"\"\"\n    type: Literal[\"function\"] = \"function\"\n    function: Function\n\n\nclass ToolsList(BaseModel):\n    \"\"\"Container for a list of tools.\n\n    Manages a collection of tools that can be provided to the model,\n    with a maximum limit of 128 tools.\n\n    Attributes:\n        tools (List[Tool]): List of tool definitions, maximum length of 128.\n    \"\"\"\n    tools: List[Tool] = Field(..., max_length=128)\n\n\nclass ToolResponse(BaseModel):\n    \"\"\"Represents the standardized output of a tool execution.\"\"\"\n\n    result: str = Field(\n        description=\"The main output or result of the tool execution\"\n    )\n    context: Optional[Dict] = Field(\n        default=None,\n        description=\"Additional contextual information or metadata about the execution\"\n    )\n"}
{"type": "source_file", "path": "src/llm/adapters/mistral_ai_adapter.py", "content": "# src/llm/adapters/mistral_ai_adapter.py\n\nimport os\nimport json\nimport logging\nfrom datetime import datetime\nfrom mistralai import Mistral\nfrom typing import AsyncGenerator, List, Optional\n\nfrom src.data_models.tools import Tool\nfrom src.llm.adapters import BaseVendorAdapter\nfrom src.data_models.chat_completions import TextChatMessage\nfrom src.api import SSEChunk, SSEChoice, SSEDelta, SSEToolCall, SSEFunction\n\nlogger = logging.getLogger(__name__)\n\n\nclass MistralAIAdapter(BaseVendorAdapter):\n    \"\"\"Adapter for interacting with Mistral AI's API.\n\n    This class implements the BaseVendorAdapter interface for Mistral's chat models,\n    handling authentication, request formatting, and response streaming. It converts\n    Mistral-specific response formats into standardized SSE chunks for consistent\n    handling across different LLM providers.\n\n    Attributes:\n        `api_key` (str): Mistral API key loaded from environment variables.\n        `client` (Mistral): Authenticated Mistral client instance.\n        `model_name` (str): The Mistral model identifier (e.g., \"mistral-tiny\").\n        `default_params` (dict): Default parameters for Mistral API calls.\n    \"\"\"\n\n    def __init__(self, model_name: str, **default_params):\n        \"\"\"Initialize the Mistral AI Adapter with model configuration.\n\n        Args:\n            `model_name` (str): The identifier of the Mistral model to use (e.g., \"mistral-tiny\").\n            `**default_params`: Additional parameters to include in all API calls.\n                Common parameters include temperature, max_tokens, etc.\n\n        Raises:\n            `ValueError`: If MISTRAL_API_KEY environment variable is not set.\n        \"\"\"\n        self.api_key = os.getenv(\"MISTRAL_API_KEY\")\n        if not self.api_key:\n            raise ValueError(\"Missing Mistral API key. Set the MISTRAL_API_KEY environment variable.\")\n\n        self.client = Mistral(api_key=self.api_key)\n        self.model_name = model_name\n        self.default_params = default_params\n        logger.info(f\"Mistral AI Adapter initialized with model: {self.model_name}\")\n        logger.debug(f\"Default parameters configured: {default_params}\")\n\n    async def gen_sse_stream(\n            self,\n            prompt: str,\n            **kwargs\n    ) -> AsyncGenerator[SSEChunk, None]:\n        \"\"\"Generate SSE stream from a single text prompt.\n\n        Converts a single prompt into a chat message and streams the response.\n\n        Args:\n            `prompt` (str): The text prompt to send to the model.\n            `**kwargs`: Additional parameters to override defaults for this request.\n\n        Yields:\n            `SSEChunk`: Standardized chunks of the streaming response.\n\n        Raises:\n            `RuntimeError`: If the streaming request fails.\n        \"\"\"\n        logger.debug(f\"Converting single prompt to chat format: {prompt[:50]}...\")\n        messages = [{\"role\": \"user\", \"content\": prompt}]\n        async for chunk in self.gen_chat_sse_stream(messages, **kwargs):\n            yield chunk\n\n    async def gen_chat_sse_stream(\n            self,\n            messages: List[TextChatMessage],\n            tools: Optional[List[Tool]] = None,\n            **kwargs,\n    ) -> AsyncGenerator[SSEChunk, None]:\n        \"\"\"Generate a streaming chat response from a sequence of messages.\n\n        Args:\n            `messages` (List[TextChatMessage]): List of chat messages for context.\n            `tools` (Optional[List[Tool]]): List of tools available to the model.\n            `**kwargs`: Additional parameters to override defaults for this request.\n\n        Yields:\n            `SSEChunk`: Standardized chunks of the streaming response.\n\n        Raises:\n            `RuntimeError`: If the Mistral API request fails.\n        \"\"\"\n        mistral_messages = [msg.model_dump() for msg in messages]\n        logger.debug(f\"Processing chat stream request with {len(messages)} messages\")\n\n        request_payload = {\n            \"model\": self.model_name,\n            \"messages\": mistral_messages,\n            **self.default_params,\n            **kwargs,\n        }\n\n        if tools:\n            logger.debug(f\"Adding {len(tools)} tools to request\")\n            request_payload[\"tools\"] = [tool.model_dump() for tool in tools]\n            request_payload[\"tool_choice\"] = \"auto\"\n\n        try:\n            logger.debug(\"Initiating Mistral streaming request\")\n            response = await self.client.chat.stream_async(**request_payload)\n            async for chunk in response:\n                yield await self._convert_to_sse_chunk(chunk)\n        except Exception as e:\n            logger.error(f\"Error in Mistral streaming: {str(e)}\", exc_info=True)\n            raise RuntimeError(f\"Mistral API streaming failed: {str(e)}\") from e\n\n    async def _convert_to_sse_chunk(self, raw_chunk) -> SSEChunk:\n        \"\"\"Convert Mistral's response chunk to standardized SSE format.\n\n        Uses model_dump_json() to convert Mistral's response to a clean JSON representation,\n        then constructs an SSEChunk from the parsed data.\n\n        Args:\n            raw_chunk: Raw chunk from Mistral's API, could be CompletionChunk or CompletionEvent.\n\n        Returns:\n            SSEChunk: Standardized chunk format for consistent handling.\n\n        Raises:\n            ValueError: If chunk conversion fails due to unexpected format.\n        \"\"\"\n        try:\n            # Use model_dump_json() to get a clean JSON representation where Unset values are omitted\n            chunk_json = raw_chunk.model_dump_json()\n            chunk_data = json.loads(chunk_json)\n\n            # Extract the 'data' field if present (based on the provided example)\n            if 'data' in chunk_data:\n                chunk_data = chunk_data['data']\n\n            logger.debug(f\"Converting chunk ID: {chunk_data.get('id', 'unknown')}\")\n\n            # Process choices\n            choices = []\n            for choice_data in chunk_data.get('choices', []):\n                delta_data = choice_data.get('delta', {})\n\n                # Process tool calls if present\n                tool_calls = None\n                if 'tool_calls' in delta_data:\n                    tool_calls = []\n                    for tc_data in delta_data['tool_calls']:\n                        function = None\n                        if 'function' in tc_data:\n                            # Ensure name and arguments are valid strings, defaulting to empty strings if missing or None\n                            fn_name = tc_data['function'].get('name')\n                            fn_name = '' if fn_name is None else fn_name\n\n                            fn_args = tc_data['function'].get('arguments')\n                            fn_args = '' if fn_args is None else fn_args\n\n                            function = SSEFunction(\n                                name=fn_name,\n                                arguments=fn_args\n                            )\n\n                        # Ensure type is always a string, defaulting to 'function' if missing or None\n                        tool_call_type = tc_data.get('type')\n                        if tool_call_type is None:\n                            tool_call_type = 'function'\n\n                        tool_calls.append(SSEToolCall(\n                            index=tc_data.get('index', 0),\n                            id=tc_data.get('id'),\n                            type=tool_call_type,\n                            function=function\n                        ))\n\n                # Create delta\n                delta = SSEDelta(\n                    role=delta_data.get('role'),\n                    content=delta_data.get('content'),\n                    tool_calls=tool_calls,\n                    refusal=delta_data.get('refusal')\n                )\n\n                # Create choice\n                choices.append(SSEChoice(\n                    index=choice_data.get('index', 0),\n                    delta=delta,\n                    logprobs=choice_data.get('logprobs'),\n                    finish_reason=choice_data.get('finish_reason')\n                ))\n\n            # Create and return the SSEChunk\n            return SSEChunk(\n                id=chunk_data.get('id', f\"gen-{id(chunk_data)}\"),\n                object=chunk_data.get('object', 'chat.completion.chunk'),\n                created=chunk_data.get('created', int(datetime.now().timestamp())),\n                model=chunk_data.get('model', self.model_name),\n                service_tier=None,  # Default to None if not provided by Mistral\n                system_fingerprint=None,  # Default to None if not provided by Mistral\n                choices=choices\n            )\n\n        except Exception as e:\n            logger.error(f\"Error converting Mistral chunk: {e}\", exc_info=True)\n            raise ValueError(f\"Failed to convert Mistral response to SSEChunk: {str(e)}\") from e\n"}
{"type": "source_file", "path": "src/llm/pattern_detection/buffered_processor_normalized.py", "content": "# src/llm/pattern_detection/buffered_processor_normalized.py\n\nfrom src.data_models.streaming import PatternMatchResult\nfrom src.llm.pattern_detection.pattern_utils import load_patterns\nfrom src.llm.pattern_detection.pattern_utils import normalize_and_map\nfrom src.llm.pattern_detection.base_buffered_processor import BaseBufferedProcessor\nfrom src.llm.pattern_detection.aho_corasick_normalized import AhoCorasickAutomatonNormalized\n\n\nclass AhoCorasickBufferedProcessorNormalized(BaseBufferedProcessor):\n    \"\"\"A buffered processor that performs normalized pattern matching ignoring whitespace.\n\n    This class implements pattern matching that is insensitive to whitespace variations\n    by normalizing both patterns and input text. It uses the Aho-Corasick algorithm\n    for efficient multiple pattern matching.\n\n    Attributes:\n        automaton: An instance of AhoCorasickAutomatonNormalized for pattern matching.\n        max_pattern_len: The length of the longest pattern in the normalized patterns.\n        tool_call_message: Message to include when a tool call is detected.\n\n    Args:\n        yaml_path: Path to the YAML file containing pattern definitions.\n        tool_call_message: Optional message to use when a tool call is detected.\n            Defaults to \"Tool call detected.\"\n    \"\"\"\n\n    def __init__(self, yaml_path: str, tool_call_message: str = \"Tool call detected.\"):\n        super().__init__(tool_call_message)\n        raw_patterns = load_patterns(yaml_path)\n        self.automaton = AhoCorasickAutomatonNormalized(raw_patterns)\n        self.max_pattern_len = max(len(p) for p in self.automaton.normalized_patterns.values())\n        self.automaton.reset_state()\n\n    def process_chunk_impl(self, combined_original: str):\n        \"\"\"Processes a chunk of text to find pattern matches while ignoring whitespace.\n\n        This method normalizes the input text, performs pattern matching, and returns\n        the earliest match found along with any safe text that can be output.\n\n        Args:\n            `combined_original`: The original text chunk to process.\n\n        Returns:\n            A tuple containing:\n\n                - `PatternMatchResult`: Result object containing match information and\n                    processed text.\n                - `str`: Any trailing text that needs to be carried over to the next\n                    chunk.\n\n        ``` python title=\"Example usage\"\n        processor = AhoCorasickBufferedProcessorNormalized('patterns.yaml')\n        result, trailing = processor.process_chunk_impl('some text')\n        print(result.matched, result.pattern_name)  # False None\n        ```\n        \"\"\"\n        result = PatternMatchResult()\n        # Normalize the entire combined_original and get index mapping.\n        norm_combined, index_map = normalize_and_map(combined_original)\n        matches = self.automaton.search_chunk(norm_combined)\n\n        if not matches:\n            keep_len = min(self.max_pattern_len - 1, len(norm_combined))\n            if keep_len > 0:\n                orig_keep_start = index_map[len(norm_combined) - keep_len]\n                safe_text = combined_original[:orig_keep_start]\n                new_trailing = combined_original[orig_keep_start:]\n            else:\n                safe_text = combined_original\n                new_trailing = \"\"\n            result.output = safe_text\n            return result, new_trailing\n\n        # Find the earliest match\n        earliest_original_index = None\n        earliest_match_pattern = None\n        earliest_norm_start_idx = None\n        for norm_end_idx, pattern_name in matches:\n            pat_len = self.automaton.get_pattern_length(pattern_name)\n            norm_start_idx = norm_end_idx - pat_len + 1\n            original_end = index_map[norm_end_idx]\n            original_start = index_map[norm_start_idx]\n            if earliest_original_index is None or original_end < earliest_original_index:\n                earliest_original_index = original_end\n                earliest_match_pattern = pattern_name\n                earliest_norm_start_idx = norm_start_idx\n\n        original_start = index_map[earliest_norm_start_idx]\n        result.matched = True\n        result.pattern_name = earliest_match_pattern\n        result.tool_call_message = self.tool_call_message\n        result.output = combined_original[:original_start]\n        result.text_with_tool_call = combined_original[original_start:]\n        new_trailing = \"\"\n        self.automaton.reset_state()\n        return result, new_trailing\n"}
{"type": "source_file", "path": "src/llm/pattern_detection/aho_corasick_normalized.py", "content": "# src/llm/pattern_detection/aho_corasick_normalized.py\n\n\"\"\"Implements a normalized version of the Aho-Corasick string matching algorithm.\n\nThis module provides a wrapper around the base Aho-Corasick automaton that handles\ntext normalization for pattern matching. It normalizes patterns during initialization\nand provides methods to search in normalized text.\n\"\"\"\n\nfrom typing import Dict, List, Tuple\nfrom src.llm.pattern_detection.aho_corasick import AhoCorasickAutomaton\nfrom src.llm.pattern_detection.pattern_utils import normalize_and_map\n\n\nclass AhoCorasickAutomatonNormalized:\n    \"\"\"A wrapper for normalized pattern matching using the Aho-Corasick algorithm.\n\n    This class normalizes patterns by removing whitespace variations before building\n    the underlying Aho-Corasick automaton. This allows for pattern matching that is\n    insensitive to whitespace differences.\n\n    Attributes:\n        `normalized_patterns`: Dictionary mapping pattern names to their normalized forms.\n        `pattern_lengths`: Dictionary storing the lengths of normalized patterns.\n        `automaton`: The underlying AhoCorasickAutomaton instance.\n\n    Args:\n        patterns: Dictionary mapping pattern names to their original string patterns.\n    \"\"\"\n\n    def __init__(self, patterns: Dict[str, str]):\n        self.normalized_patterns = {}\n        self.pattern_lengths = {}\n        for name, pat in patterns.items():\n            norm_pat, _ = normalize_and_map(pat)\n            self.normalized_patterns[name] = norm_pat\n            self.pattern_lengths[name] = len(norm_pat)\n\n        self.automaton = AhoCorasickAutomaton(self.normalized_patterns)\n\n    def reset_state(self):\n        \"\"\"Resets the automaton to its initial state.\n\n        Should be called before starting a new search if the automaton has been\n        used previously.\n        \"\"\"\n        self.automaton.reset_state()\n\n    def search_chunk(self, norm_chunk: str) -> List[Tuple[int, str]]:\n        \"\"\"Searches for pattern matches in normalized text.\n\n        Args:\n            `norm_chunk`: The normalized text chunk to search in. Should be\n                pre-normalized before calling this method.\n\n        Returns:\n            A list of tuples, where each tuple contains:\n\n                - The ending index of the match in the normalized text (int)\n                - The name of the matched pattern (str)\n\n        ``` python title=\"Example usage\"\n        automaton = AhoCorasickAutomatonNormalized({'pat1': 'hello world'})\n        matches = automaton.search_chunk('helloworld')\n        print(len(matches))  # 1\n        ```\n        \"\"\"\n        return self.automaton.search_chunk(norm_chunk)\n\n    def get_pattern_length(self, pattern_name: str) -> int:\n        \"\"\"Returns the length of a normalized pattern.\n\n        Args:\n            `pattern_name`: The name of the pattern whose length is required.\n\n        Returns:\n            The length of the normalized pattern as an integer.\n\n        Raises:\n            `KeyError`: Error raised if the `pattern_name` is not found in the patterns dictionary.\n        \"\"\"\n        return self.pattern_lengths[pattern_name]\n"}
{"type": "source_file", "path": "src/llm/tool_detection/__init__.py", "content": "from .base_detection_strategy import BaseToolCallDetectionStrategy\nfrom .manual_detection_strategy import ManualToolCallDetectionStrategy\nfrom .vendor_detection_strategy import VendorToolCallDetectionStrategy\n"}
{"type": "source_file", "path": "src/main.py", "content": "\"\"\"\nMain module for initializing and configuring the FastAPI application.\n\nThis module sets up logging, loads environment variables, configures the FastAPI app,\nand registers routes and middleware. It also defines global exception handlers to provide\nconsistent error responses.\n\nExample:\n    To run the application:\n        $ uvicorn src.main:app --reload\n\nEnvironment Variables:\n    LOG_LEVEL (str): Logging level for the application (default: 'INFO').\n\"\"\"\n\nimport os\nimport yaml\nimport logging\nfrom fastapi import FastAPI\nfrom starlette import status\nfrom dotenv import load_dotenv\nfrom fastapi.responses import JSONResponse\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom fastapi.exceptions import RequestValidationError\n\nfrom src.api.sse_models import SSEChunk\nfrom src.api.routes.chat_completions_api import router as chat_completions_router\n\n# Load environment variables\nload_dotenv()\n\n# Configure logging\nlog_level = os.getenv(\"LOG_LEVEL\", \"INFO\").upper()\nlogging.basicConfig(\n    level=getattr(logging, log_level),\n    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n)\nlogger = logging.getLogger(__name__)\nlogger.info(\"Initializing application with log level: %s\", log_level)\n\n# Initialize FastAPI app\napp = FastAPI(\n    title=\"Chat Completions API\",\n    description=\"API for handling chat completions with streaming support\",\n    version=\"1.0.0\",\n)\n\n# Load agent config to check for allowed_origins\ntry:\n    with open(\"src/configs/agent.yaml\", \"r\") as file:\n        agent_config = yaml.safe_load(file)\nexcept Exception as e:\n    logger.warning(\"Failed to load agent configuration for CORS: %s\", str(e))\n    agent_config = {}\n\nallowed_origins = agent_config.get(\"allowed_origins\")\nif allowed_origins:\n    # If allowed_origins is provided as a string, convert it to a list.\n    if isinstance(allowed_origins, str):\n        allowed_origins = [allowed_origins]\n    app.add_middleware(\n        CORSMiddleware,\n        allow_origins=allowed_origins,\n        allow_credentials=True,\n        allow_methods=[\"GET\", \"POST\", \"OPTIONS\"],\n        allow_headers=[\"Authorization\", \"Content-Type\", \"X-API-KEY\"],\n    )\n    logger.info(\"CORS middleware enabled with allowed origins: %s\", allowed_origins)\nelse:\n    logger.info(\"CORS middleware not enabled because no allowed_origins were provided in the config.\")\n\n# Include the chat completions router\nlogger.info(\"Registering chat completions router at /v1\")\napp.include_router(chat_completions_router, prefix=\"/v1\")\n\n\n# --------------------\n# EXCEPTION HANDLERS\n# --------------------\n\n@app.exception_handler(Exception)\nasync def generic_exception_handler(request, exc: Exception):\n    \"\"\"Handle all unhandled exceptions globally.\n\n    This function catches any exception that is not explicitly handled by other exception\n    handlers and returns a JSON response with the error details.\n\n    Args:\n        request (Request): The incoming request.\n        exc (Exception): The exception that was raised.\n\n    Returns:\n        JSONResponse: A JSON response with an appropriate HTTP status code and error message.\n    \"\"\"\n    error_msg = str(exc)\n    status_code = status.HTTP_500_INTERNAL_SERVER_ERROR\n\n    if isinstance(exc, ValueError):\n        logger.warning(\"ValueError occurred: %s\", error_msg)\n        status_code = status.HTTP_400_BAD_REQUEST\n    elif isinstance(exc, TimeoutError):\n        logger.error(\"Request timed out: %s\", error_msg)\n        status_code = status.HTTP_504_GATEWAY_TIMEOUT\n        error_msg = \"Request timed out. Please try again.\"\n    else:\n        logger.error(\"Unhandled exception occurred\", exc_info=True)\n\n    logger.debug(\"Generating error response chunk with status %d\", status_code)\n    error_response = await SSEChunk.make_stop_chunk(\n        refusal=error_msg,\n        content=\"I apologize, but an error occurred while processing your request.\",\n    )\n\n    return JSONResponse(\n        status_code=status_code,\n        content=error_response.model_dump(),\n    )\n\n\n@app.exception_handler(RequestValidationError)\nasync def validation_exception_handler(request, exc: RequestValidationError):\n    \"\"\"Handle request validation errors.\n\n    This function catches errors that occur during request validation (e.g., invalid format,\n    missing required fields) and returns a JSON response with details about the validation failure.\n\n    Args:\n        request (Request): The incoming request.\n        exc (RequestValidationError): The exception raised during request validation.\n\n    Returns:\n        JSONResponse: A JSON response with a 422 status code and details about the validation errors.\n    \"\"\"\n    logger.warning(\"Request validation failed: %s\", str(exc))\n    logger.debug(\"Validation error details: %s\", exc.errors())\n\n    error_response = await SSEChunk.make_stop_chunk(\n        refusal=str(exc),\n        content=\"There seems to be an issue with the provided request format. Please check your input and try again.\",\n    )\n\n    return JSONResponse(\n        status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,\n        content=error_response.model_dump(),\n    )\n"}
{"type": "source_file", "path": "src/llm/pattern_detection/pattern_utils.py", "content": "# src/llm/streaming/pattern_utils.py\n\nimport yaml\nfrom typing import Dict\n\n\ndef load_patterns(yaml_path: str) -> Dict[str, str]:\n    \"\"\"\n    Load tool call patterns from a YAML file.\n\n    Args:\n        yaml_path: Path to the YAML file containing patterns\n\n    Returns:\n        Dictionary mapping pattern names to pattern strings\n    \"\"\"\n    with open(yaml_path, 'r') as f:\n        config = yaml.safe_load(f)\n    return config.get('patterns', {})\n\n\ndef normalize_and_map(text: str):\n    \"\"\"Returns a tuple of normalized text with whitespace removed and an index mapping.\n\n    Processes the input text by removing all whitespace characters and creating\n    a mapping that tracks the original position of each character.\n\n    Args:\n        text: The input string to be normalized.\n\n    Returns:\n        tuple: A tuple containing two elements:\n\n            - `normalized_text` (str): The input text with all whitespace removed.\n            - `index_map` (list): A list where index_map[i] is the original index\n              of the i-th character in normalized_text.\n\n    ``` python title=\"Example usage\"\n    normalize_and_map(\"a b c\")  # ('abc', [0, 2, 4])\n    ```\n    \"\"\"\n    normalized_chars = []\n    index_map = []\n    for idx, ch in enumerate(text):\n        if not ch.isspace():\n            normalized_chars.append(ch)\n            index_map.append(idx)\n    return \"\".join(normalized_chars), index_map\n"}
{"type": "source_file", "path": "src/llm/tool_detection/vendor_detection_strategy.py", "content": "# src/llm/tool_detection/vendor_detection_strategy.py\n\nimport json\nimport logging\nfrom src.api import SSEChunk\nfrom src.data_models.agent import StreamContext\nfrom src.data_models.chat_completions import ToolCall, FunctionDetail\nfrom src.llm.tool_detection import BaseToolCallDetectionStrategy\nfrom src.llm.tool_detection.detection_result import DetectionResult, DetectionState\n\n\nclass VendorToolCallDetectionStrategy(BaseToolCallDetectionStrategy):\n    \"\"\"A strategy for detecting tool calls using vendor-provided metadata in SSE chunks.\n\n    This strategy processes tool calls by accumulating function names and arguments\n    across multiple Server-Sent Events (SSE) chunks. It relies on vendor-specific\n    metadata in the chunks to identify tool calls and their completion status.\n\n    Attributes:\n        found_complete_call (bool): Flag indicating if a complete tool call was found.\n        collected_tool_calls (List[ToolCall]): List of fully collected tool calls.\n        partial_name (str): Buffer for accumulating function name.\n        partial_args (str): Buffer for accumulating function arguments.\n\n    Example:\n        ```python\n        detector = VendorToolCallDetectionStrategy()\n\n        async for chunk in stream:\n            result = await detector.detect_chunk(chunk, context)\n            if result.state == DetectionState.COMPLETE_MATCH:\n                tool_calls = result.tool_calls\n                # Process the complete tool calls\n        ```\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"Initialize the vendor tool call detection strategy.\"\"\"\n        self.logger = logging.getLogger(self.__class__.__name__)\n        self.logger.debug(\"Initializing VendorToolCallDetectionStrategy\")\n        self.found_complete_call = None\n        self.collected_tool_calls = None\n        self.partial_name = None\n        self.partial_args = None\n        self.reset()\n\n    def reset(self) -> None:\n        \"\"\"Reset all stored tool call information to initial state.\n\n        This method clears all accumulated data and resets flags, preparing the\n        detector for processing a new stream of chunks.\n        \"\"\"\n        self.logger.debug(\"Resetting detector state\")\n        self.partial_name = None\n        self.partial_args = \"\"  # Accumulates streamed arguments\n        self.collected_tool_calls = []\n        self.found_complete_call = False\n\n    async def detect_chunk(\n            self,\n            sse_chunk: SSEChunk,\n            context: StreamContext\n    ) -> DetectionResult:\n        \"\"\"Process an SSE chunk for tool call detection.\n\n        Analyzes the chunk for tool call information using vendor-provided metadata.\n        Accumulates partial tool calls across multiple chunks until a complete call\n        is detected.\n\n        Args:\n            sse_chunk (SSEChunk): The chunk of streaming content to process.\n            context (StreamContext): Context information for the current stream.\n\n        Returns:\n            DetectionResult: Result of processing the chunk, including detection state\n                and any content or tool calls found.\n\n        Note:\n            This method maintains state between calls detect_chunk() to properly handle tool calls\n            that span multiple chunks. It relies on the finish_reason field to\n            determine when a tool call is complete.\n        \"\"\"\n        tool_call_data = None\n        if not sse_chunk.choices:\n            return DetectionResult(state=DetectionState.NO_MATCH)\n\n        delta = sse_chunk.choices[0].delta\n        finish_reason = sse_chunk.choices[0].finish_reason\n\n        text_content = delta.content if delta.content else None\n\n        # Check if tool call data is present in this chunk\n        tool_calls = delta.tool_calls if delta.tool_calls else None\n\n        if tool_calls:\n            tool_call_data = tool_calls[0]  # Assuming index 0 for simplicity\n            function_name = tool_call_data.function.name if tool_call_data.function else None\n            arguments = tool_call_data.function.arguments if tool_call_data.function else None\n\n            # If this chunk contains a function name, store it\n            if function_name:\n                self.partial_name = function_name\n\n            # If arguments are being streamed, accumulate them\n            if arguments:\n                self.partial_args += arguments\n\n        # If finish_reason indicates the tool call is complete, finalize it\n        if finish_reason in [\"tool_calls\", \"tool_use\"]:\n            if self.partial_name:\n                try:\n                    parsed_args = json.loads(self.partial_args) if self.partial_args else {}\n                except json.JSONDecodeError:\n                    self.logger.warning(\"Failed to parse arguments as JSON: %s\", self.partial_args[:50])\n                    parsed_args = {\"_malformed\": self.partial_args}\n\n                tool_call = ToolCall(\n                    id=(tool_call_data and tool_call_data.id) or \"call_generated\",\n                    function=FunctionDetail(\n                        name=self.partial_name,\n                        arguments=str(parsed_args)\n                    )\n                )\n                self.collected_tool_calls.append(tool_call)\n                self.found_complete_call = True\n\n                return DetectionResult(\n                    state=DetectionState.COMPLETE_MATCH,\n                    tool_calls=[tool_call],\n                    content=text_content\n                )\n\n        # If we're still collecting tool call data, return PARTIAL_MATCH\n        if self.partial_name or self.partial_args:\n            return DetectionResult(\n                state=DetectionState.PARTIAL_MATCH,\n                content=text_content\n            )\n\n        # Otherwise, just return NO_MATCH and pass the text through\n        return DetectionResult(\n            state=DetectionState.NO_MATCH,\n            content=text_content\n        )\n\n    async def finalize_detection(self, context: StreamContext) -> DetectionResult:\n        \"\"\"Finalize the detection process and handle any accumulated tool calls.\n\n        This method is called at the end of the SSE stream to process any remaining\n        tool call data and return final results.\n\n        Args:\n            context (StreamContext): Context information for the current stream.\n\n        Returns:\n            DetectionResult: Final result of the detection process, including any\n                complete tool calls or remaining content.\n\n        Note:\n            This method handles cleanup of partial tool calls that were never\n            completed due to stream termination.\n        \"\"\"\n        self.logger.debug(\"Finalizing detection\")\n        if self.found_complete_call:\n            self.logger.debug(\"Returning %d collected tool calls\", len(self.collected_tool_calls))\n            return DetectionResult(\n                state=DetectionState.COMPLETE_MATCH,\n                tool_calls=self.collected_tool_calls\n            )\n\n        if self.partial_name or self.partial_args:\n            self.logger.debug(\"Incomplete tool call data at stream end\")\n            self.logger.debug(f\"Name: {self.partial_name}, Args: {self.partial_args}\")\n            return DetectionResult(state=DetectionState.NO_MATCH)\n\n        self.logger.debug(\"No tool calls to finalize\")\n        return DetectionResult(state=DetectionState.NO_MATCH)\n"}
{"type": "source_file", "path": "src/llm/llm_factory.py", "content": "# src/llm/llm_factory.py\n\nimport logging\nimport os\nfrom typing import Dict, Any, Type, Optional, TypeVar\n\nfrom .adapters.base_vendor_adapter import BaseVendorAdapter\nfrom .adapters.watsonx.watsonx_config import WatsonXConfig\nfrom .adapters.watsonx.ibm_token_manager import IBMTokenManager\nfrom .adapters import (\n    OpenAIAdapter,\n    WatsonXAdapter,\n    AnthropicAdapter,\n    MistralAIAdapter,\n    OpenAICompatAdapter,\n    XAIAdapter,\n)\n\nlogger = logging.getLogger(__name__)\n\n# Type variable for adapter instances\nT = TypeVar('T', bound=BaseVendorAdapter)\n\n\nclass LLMFactory:\n    \"\"\"Factory for creating and managing LLM adapters for different vendors.\n\n    This class implements the Factory pattern to instantiate and manage different\n    LLM adapters based on configuration. It maintains a singleton-like pattern\n    for adapter instances and service-specific token managers.\n\n    Attributes:\n        _adapters (Dict[str, BaseVendorAdapter]): Class-level dictionary storing instantiated adapters.\n        _token_manager (IBMTokenManager): Class-level token manager instance for WatsonX.\n        _adapter_registry (Dict[str, Type[BaseVendorAdapter]]): Mapping of vendor names to adapter classes.\n    \"\"\"\n\n    _adapters: Optional[Dict[str, BaseVendorAdapter]] = None\n    _token_manager: Optional[IBMTokenManager] = None\n\n    # Registry of standard adapter classes by vendor name\n    _adapter_registry: Dict[str, Type[BaseVendorAdapter]] = {\n        \"openai\": OpenAIAdapter,\n        \"anthropic\": AnthropicAdapter,\n        \"mistral-ai\": MistralAIAdapter,\n        \"xai\": XAIAdapter,\n        \"openai-compat\": OpenAICompatAdapter,\n    }\n\n    def __init__(self, config: Dict[str, Dict[str, Any]]):\n        \"\"\"Initialize the LLM Factory with configuration.\n\n        Args:\n            config (Dict[str, Dict[str, Any]]): Configuration dictionary containing model configurations.\n                Expected format:\n                {\n                    \"model_name\": {\n                        \"vendor\": str,\n                        \"model_id\": str,\n                        ...additional_config\n                    }\n                }\n\n        Raises:\n            ValueError: If the configuration format is invalid.\n        \"\"\"\n        if LLMFactory._adapters is None:\n            self._initialize_adapters(config)\n\n    @classmethod\n    def _initialize_adapters(cls, config: Dict[str, Dict[str, Any]]) -> None:\n        \"\"\"Initialize adapters based on the provided configuration.\n\n        This method creates adapter instances for each model in the config.\n\n        Args:\n            config (Dict[str, Dict[str, Any]]): Configuration dictionary for all models.\n\n        Raises:\n            ValueError: If an unknown vendor is specified or if required configuration is missing.\n        \"\"\"\n        cls._adapters = {}\n        logger.debug(\"Initializing LLM adapters\")\n\n        # Initialize service-specific components once if needed\n        cls._initialize_service_components(config)\n\n        # Process each model in the configuration\n        for model_name, model_config in config.items():\n            try:\n                # Validate and extract configuration\n                validated_config = cls._validate_model_config(model_name, model_config)\n                vendor = validated_config[\"vendor\"]\n                model_id = validated_config[\"model_id\"]\n                adapter_params = validated_config[\"adapter_params\"]\n\n                # Create the adapter\n                adapter = cls._create_adapter(vendor, model_id, **adapter_params)\n                cls._adapters[model_name] = adapter\n                logger.debug(f\"Initialized {vendor} adapter for model: {model_name}\")\n\n            except Exception as e:\n                logger.error(f\"Failed to initialize adapter for {model_name}: {str(e)}\")\n                # Add context to the exception\n                raise ValueError(f\"Adapter initialization failed for {model_name}\") from e\n\n    @classmethod\n    def _initialize_service_components(cls, config: Dict[str, Dict[str, Any]]) -> None:\n        \"\"\"Initialize service-specific components required by adapters.\n\n        Args:\n            config (Dict[str, Dict[str, Any]]): Configuration dictionary for all models.\n\n        Raises:\n            ValueError: If initialization of a service component fails.\n        \"\"\"\n        # Initialize WatsonX Token Manager if needed\n        if any(\"watsonx\" in model_config.get(\"vendor\", \"\") for model_config in config.values()):\n            try:\n                cls._token_manager = IBMTokenManager(api_key=WatsonXConfig.CREDS.get('apikey'))\n                logger.debug(\"Initialized WatsonX Token Manager\")\n            except Exception as e:\n                logger.error(f\"Failed to initialize WatsonX Token Manager: {str(e)}\")\n                raise ValueError(\"Failed to initialize WatsonX Token Manager\") from e\n\n    @classmethod\n    def _create_adapter(cls, vendor: str, model_id: str, **kwargs) -> BaseVendorAdapter:\n        \"\"\"Create an adapter instance based on vendor and model ID.\n\n        Args:\n            vendor (str): The vendor identifier.\n            model_id (str): The model identifier.\n            **kwargs: Additional parameters for the adapter.\n\n        Returns:\n            BaseVendorAdapter: The created adapter instance.\n\n        Raises:\n            ValueError: If the vendor is unknown or if adapter creation fails.\n        \"\"\"\n        # Handle special case for WatsonX\n        if \"watsonx\" in vendor:\n            if cls._token_manager is None:\n                raise ValueError(\"IBMTokenManager was not initialized for WatsonX models.\")\n            return WatsonXAdapter(\n                model_name=model_id,\n                token_manager=cls._token_manager,\n                **kwargs\n            )\n\n        # Handle case for xAI\n        if vendor == \"xai\":\n            # Get API key from config or environment\n            api_key = kwargs.pop(\"api_key\", None) or os.getenv(\"XAI_API_KEY\")\n            if not api_key:\n                logger.warning(f\"No XAI API key found for model {model_id}. Set XAI_API_KEY environment variable.\")\n\n            # Use the standard X.AI base URL unless overridden\n            base_url = kwargs.pop(\"base_url\", \"https://api.x.ai/v1\")\n\n            return XAIAdapter(\n                model_name=model_id,\n                api_key=api_key,\n                base_url=base_url,\n                **kwargs\n            )\n\n        # Check for OpenAI compatibility vendors (partial match)\n        if \"openai-compat\" in vendor:\n            return OpenAICompatAdapter(model_name=model_id, **kwargs)\n\n        # Handle standard adapters from registry with exact match\n        adapter_class = cls._adapter_registry.get(vendor)\n        if adapter_class:\n            return adapter_class(model_name=model_id, **kwargs)\n\n        # If we get here, the vendor is unknown\n        raise ValueError(f\"Unknown vendor '{vendor}'\")\n\n    @staticmethod\n    def _validate_model_config(model_name: str, config: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Validate model configuration and extract adapter parameters.\n\n        Args:\n            model_name (str): The name of the model.\n            config (Dict[str, Any]): The model configuration.\n\n        Returns:\n            Dict[str, Any]: Validated configuration with extracted parameters.\n\n        Raises:\n            ValueError: If required fields are missing.\n        \"\"\"\n        # Check required fields\n        required_fields = [\"vendor\", \"model_id\"]\n        for field in required_fields:\n            if field not in config:\n                raise ValueError(f\"Missing required field '{field}' for model '{model_name}'\")\n\n        # Extract and return relevant configuration\n        return {\n            \"vendor\": config[\"vendor\"],\n            \"model_id\": config[\"model_id\"],\n            \"adapter_params\": {k: v for k, v in config.items() if k not in [\"vendor\", \"model_id\"]}\n        }\n\n    @classmethod\n    def get_adapter(cls, model_name: str, config: Optional[Dict[str, Dict[str, Any]]] = None) -> BaseVendorAdapter:\n        \"\"\"Retrieve an adapter instance for a specific model with lazy initialization if needed.\n\n        Args:\n            model_name (str): Name of the model to retrieve the adapter for.\n            config (Optional[Dict[str, Dict[str, Any]]]): Configuration to use if factory is not initialized.\n\n        Returns:\n            BaseVendorAdapter: The adapter instance for the specified model.\n\n        Raises:\n            ValueError: If adapters haven't been initialized or if the\n                requested model adapter is not found.\n        \"\"\"\n        # Lazy initialization if needed\n        if cls._adapters is None:\n            if not config:\n                raise ValueError(\"Adapters have not been initialized. Initialize the factory with a config first.\")\n            cls._initialize_adapters(config)\n\n        adapter = cls._adapters.get(model_name)\n        if adapter:\n            logger.debug(f\"Retrieved adapter for model: {model_name}\")\n            return adapter\n        else:\n            raise ValueError(f\"Adapter for model '{model_name}' not found.\")\n\n    @classmethod\n    def has_adapter(cls, model_name: str) -> bool:\n        \"\"\"Check if an adapter is available for a model without raising exceptions.\n\n        Args:\n            model_name (str): The name of the model to check.\n\n        Returns:\n            bool: True if the adapter exists, False otherwise.\n        \"\"\"\n        return cls._adapters is not None and model_name in cls._adapters\n\n    @classmethod\n    def list_available_models(cls) -> list:\n        \"\"\"List all available model names that have initialized adapters.\n\n        Returns:\n            list: List of model names with initialized adapters.\n\n        Raises:\n            ValueError: If adapters haven't been initialized.\n        \"\"\"\n        if cls._adapters is None:\n            raise ValueError(\"Adapters have not been initialized.\")\n\n        return list(cls._adapters.keys())\n"}
{"type": "source_file", "path": "src/llm/adapters/__init__.py", "content": "from .anthropic_adapter import AnthropicAdapter\nfrom .base_vendor_adapter import BaseVendorAdapter\nfrom .openai_adapter import OpenAIAdapter\nfrom .mistral_ai_adapter import MistralAIAdapter\nfrom .watsonx.watsonx_adapter import WatsonXAdapter\nfrom .openai_compat_adapter import OpenAICompatAdapter\nfrom .xai_adapter import XAIAdapter\n"}
{"type": "source_file", "path": "src/llm/tool_detection/base_detection_strategy.py", "content": "# src/llm/tool_detection/base_detection_strategy.py\n\nfrom abc import ABC, abstractmethod\n\nfrom src.api import SSEChunk\nfrom src.data_models.agent import StreamContext\nfrom src.llm.tool_detection.detection_result import DetectionResult\n\n\nclass BaseToolCallDetectionStrategy(ABC):\n    \"\"\"Abstract base class for implementing tool call detection strategies.\n\n    This class defines the interface for strategies that detect when an LLM\n    wants to make tool calls within its response stream. Implementations should\n    handle parsing of SSE chunks to identify tool call patterns and maintain\n    any necessary state between chunks.\n\n    The detection process happens in three phases:\n    1. Reset - Clear any accumulated state\n    2. Detect - Process incoming chunks sequentially\n    3. Finalize - Handle any remaining state and make final determination\n    \"\"\"\n\n    @abstractmethod\n    def reset(self) -> None:\n        \"\"\"Reset the strategy's internal state.\n\n        This method should be called before starting a new detection sequence\n        to ensure no state is carried over from previous detections.\n\n        Implementation should clear any accumulated buffers, counters, or other\n        state variables used during detection.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    async def detect_chunk(\n            self,\n            sse_chunk: SSEChunk,\n            context: StreamContext\n    ) -> DetectionResult:\n        \"\"\"Process an SSE chunk to detect potential tool calls.\n\n        Args:\n            sse_chunk (SSEChunk): The chunk of streaming response to analyze.\n                Contains delta updates and choice information.\n            context (StreamContext): Contextual information about the current\n                stream, including conversation history and available tools.\n\n        Returns:\n            DetectionResult: The result of analyzing this chunk, including\n                whether a tool call was detected and any extracted tool\n                call information.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    async def finalize_detection(\n            self,\n            context: StreamContext\n    ) -> DetectionResult:\n        \"\"\"Complete the detection process and handle any remaining state.\n\n        This method should be called after all chunks have been processed\n        to handle any buffered content or partial tool calls that may need\n        final processing.\n\n        Args:\n            context (StreamContext): Contextual information about the current\n                stream, including conversation history and available tools.\n\n        Returns:\n            DetectionResult: Final detection result, including any tool calls\n                that were detected from accumulated state.\n        \"\"\"\n        pass\n"}
{"type": "source_file", "path": "src/llm/tool_detection/detection_result.py", "content": "# src/llm/tool_detection/detection_result.py\n\nfrom enum import Enum\nfrom typing import Optional, List\nfrom pydantic import BaseModel\n\nfrom src.api import SSEChunk\nfrom src.data_models.chat_completions import ToolCall\n\n\nclass DetectionState(str, Enum):\n    \"\"\"Enumeration of possible tool call detection states.\n\n    Represents the different states a tool call detection can be in during\n    the processing of streaming chunks.\n\n    Attributes:\n        NO_MATCH: No tool call pattern was detected in the current content.\n        PARTIAL_MATCH: A potential tool call was detected but is incomplete.\n        COMPLETE_MATCH: A complete and valid tool call was detected.\n    \"\"\"\n    NO_MATCH = \"no_match\"\n    PARTIAL_MATCH = \"partial_match\"\n    COMPLETE_MATCH = \"complete_match\"\n\n\nclass DetectionResult(BaseModel):\n    \"\"\"Container for tool call detection results.\n\n    Encapsulates the result of processing an SSE chunk for tool calls,\n    including the detection state, any found tool calls, accumulated content,\n    and the original chunk.\n\n    Attributes:\n        state (DetectionState): The current state of tool call detection.\n            Indicates whether a tool call was found and if it's complete.\n        tool_calls (Optional[List[ToolCall]]): List of tool calls found in the\n            content. Only present when state is COMPLETE_MATCH.\n        content (Optional[str]): Accumulated text content from the chunk,\n            excluding any tool call syntax.\n        sse_chunk (Optional[SSEChunk]): The original SSE chunk that was\n            processed, preserved for reference or further processing.\n    \"\"\"\n    state: DetectionState\n    tool_calls: Optional[List[ToolCall]] = None\n    content: Optional[str] = None\n    sse_chunk: Optional[SSEChunk] = None\n"}
{"type": "source_file", "path": "src/mcp/__init__.py", "content": ""}
{"type": "source_file", "path": "src/llm/pattern_detection/buffered_processor_standard.py", "content": "# src/llm/pattern_detection/buffered_processor_standard.py\n\nfrom src.data_models.streaming import PatternMatchResult\nfrom src.llm.pattern_detection.pattern_utils import load_patterns\nfrom src.llm.pattern_detection.aho_corasick import AhoCorasickAutomaton\nfrom src.llm.pattern_detection.base_buffered_processor import BaseBufferedProcessor\n\n\nclass AhoCorasickBufferedProcessor(BaseBufferedProcessor):\n    \"\"\"A buffered processor that performs exact pattern matching.\n\n    This class implements exact pattern matching using the Aho-Corasick algorithm\n    for efficient multiple pattern matching. Unlike the normalized version, this\n    processor is sensitive to whitespace and performs exact string matching.\n\n    Attributes:\n        `automaton`: An instance of AhoCorasickAutomaton for pattern matching.\n        `max_pattern_len`: The length of the longest pattern in the raw patterns.\n        `tool_call_message`: Message to include when a tool call is detected.\n\n    Args:\n        `yaml_path`: Path to the YAML file containing pattern definitions.\n        `tool_call_message`: Optional message to use when a tool call is detected.\n            Defaults to \"Tool call detected.\"\n    \"\"\"\n\n    def __init__(self, yaml_path: str, tool_call_message: str = \"Tool call detected.\"):\n        super().__init__(tool_call_message)\n        raw_patterns = load_patterns(yaml_path)\n        self.automaton = AhoCorasickAutomaton(raw_patterns)\n        self.max_pattern_len = max(len(p) for p in raw_patterns.values())\n        self.automaton.reset_state()\n\n    def process_chunk_impl(self, combined_original: str):\n        \"\"\"Processes a chunk of text to find exact pattern matches.\n\n        This method performs exact pattern matching on the input text and returns\n        the earliest match found along with any safe text that can be output.\n\n        Args:\n            `combined_original`: The text chunk to process.\n\n        Returns:\n            A tuple containing:\n\n                - `PatternMatchResult`: Result object containing match information and\n                    processed text.\n                - `str`: Any trailing text that needs to be carried over to the next\n                    chunk.\n\n        ``` python title=\"Example usage\"\n        processor = AhoCorasickBufferedProcessor('patterns.yaml')\n        result, trailing = processor.process_chunk_impl('some text')\n        print(result.matched, result.pattern_name)  # False None\n        ```\n        \"\"\"\n        result = PatternMatchResult()\n        # Search in the original text\n        matches = self.automaton.search_chunk(combined_original)\n\n        if not matches:\n            # Keep up to max_pattern_len - 1 characters for partial match\n            keep_len = min(self.max_pattern_len - 1, len(combined_original))\n            if keep_len > 0:\n                safe_text = combined_original[:-keep_len]\n                new_trailing = combined_original[-keep_len:]\n            else:\n                safe_text = combined_original\n                new_trailing = \"\"\n            result.output = safe_text\n            return result, new_trailing\n\n        # Otherwise, use the earliest match\n        earliest_end, pattern_name = min(matches, key=lambda x: x[0])\n        pattern_str = self.automaton.patterns[pattern_name]\n        match_start = earliest_end - len(pattern_str) + 1\n\n        result.matched = True\n        result.pattern_name = pattern_name\n        result.tool_call_message = self.tool_call_message\n        result.output = combined_original[:match_start]\n        result.text_with_tool_call = combined_original[match_start:]\n        new_trailing = \"\"\n        self.automaton.reset_state()\n        return result, new_trailing\n"}
{"type": "source_file", "path": "src/mcp/client.py", "content": "# src/mcp/client.py\n\nimport anyio\nimport logging\nfrom typing import Optional, Any, Dict, Callable, Awaitable\n\nfrom mcp import types\nfrom mcp import ClientSession\nfrom mcp.client.sse import sse_client\nfrom mcp.client.stdio import stdio_client, StdioServerParameters\n\nfrom src.tools.core.observer import MCPToolObserver\n\nlogger = logging.getLogger(__name__)\n\n\nclass FlexoMCPClient:\n    \"\"\"A client wrapper for the MCP SDK's ClientSession, supporting both SSE and Stdio transports.\n\n    This class manages the connection, message processing, and notification handling with the MCP server.\n\n    Attributes:\n        config (dict[str, Any]): Configuration dictionary.\n        observer (MCPToolObserver): Observer instance for handling tool notifications.\n    \"\"\"\n\n    def __init__(self, config: dict[str, Any]):\n        \"\"\"Initializes the FlexoMCPClient.\n\n        Args:\n            config (dict[str, Any]): A dictionary that may contain:\n                {\n                    \"transport\": \"sse\" or \"stdio\",\n                    \"sse_url\": <SSE endpoint URL>,\n                    \"command\": <stdio command, e.g. 'python'>,\n                    \"args\": <list of arguments for stdio process>,\n                    \"env\": <dict of environment variables for stdio process>,\n                    ...\n                }\n        \"\"\"\n        self.config = config\n        self._session: Optional[ClientSession] = None\n        self._streams = None\n        self._transport_cm = None\n        self._connected = False\n        self._task_group = None\n        self._notification_handlers: Dict[str, Callable[[Any], Awaitable[None]]] = {}\n        self.observer = MCPToolObserver()\n\n    async def __aenter__(self) -> \"FlexoMCPClient\":\n        \"\"\"Asynchronous context manager entry.\n\n        Returns:\n            FlexoMCPClient: The connected MCP client instance.\n        \"\"\"\n        await self.connect()\n        return self\n\n    async def __aexit__(self, exc_type, exc_val, exc_tb) -> None:\n        \"\"\"Asynchronous context manager exit.\n\n        Closes the MCP client connection.\n\n        Args:\n            exc_type: Exception type.\n            exc_val: Exception value.\n            exc_tb: Exception traceback.\n        \"\"\"\n        await self.close()\n\n    async def connect(self) -> None:\n        \"\"\"Establishes the connection to the MCP server using SSE or Stdio transport.\n\n        Raises:\n            ValueError: If an unsupported transport type is specified.\n        \"\"\"\n        transport = self.config.get(\"transport\", \"sse\")\n        if transport == \"sse\":\n            await self._connect_sse()\n        elif transport == \"stdio\":\n            await self._connect_stdio()\n        else:\n            raise ValueError(f\"Unsupported transport: {transport}\")\n\n        # Enter the session context.\n        await self._session.__aenter__()\n\n        # Initialize the session.\n        init_result = await self._session.initialize()\n        logger.info(f\"Connected to server: {init_result.serverInfo.name} {init_result.serverInfo.version}\")\n\n        self._register_default_notification_handlers()\n\n        # Start message processor in a task group.\n        self._task_group = anyio.create_task_group()\n        await self._task_group.__aenter__()\n        self._task_group.start_soon(self._process_incoming_messages)\n\n        self._connected = True\n        logger.info(\"MCP client session initialized successfully.\")\n\n        # Register observer with this client.\n        self.observer.register_with_mcp_client(self)\n\n    async def _connect_sse(self) -> None:\n        \"\"\"Connects to the MCP server using SSE transport.\n\n        Uses the 'sse_url' from the configuration.\n\n        Raises:\n            Exception: If connection fails.\n        \"\"\"\n        sse_url = self.config.get(\"sse_url\", \"http://localhost:8080/sse\")\n        logger.info(f\"Connecting to SSE server at {sse_url}...\")\n        self._transport_cm = sse_client(sse_url)\n        self._streams = await self._transport_cm.__aenter__()\n        self._session = ClientSession(self._streams[0], self._streams[1])\n\n    async def _connect_stdio(self) -> None:\n        \"\"\"Connects to the MCP server using Stdio transport.\n\n        Uses the 'command', 'args', and 'env' from the configuration.\n        \"\"\"\n        command = self.config.get(\"command\", \"python\")\n        args = self.config.get(\"args\", [])\n        env = self.config.get(\"env\", None)\n        server_params = StdioServerParameters(command=command, args=args, env=env)\n        logger.info(f\"Connecting to stdio server with: {command} {args} ...\")\n        self._transport_cm = stdio_client(server_params)\n        self._streams = await self._transport_cm.__aenter__()\n        self._session = ClientSession(self._streams[0], self._streams[1])\n\n    def _register_default_notification_handlers(self) -> None:\n        \"\"\"Registers the built-in notification handlers.\"\"\"\n        handlers = {\n            \"notifications/tools/list_changed\": self._handle_tools_list_changed,\n            \"notifications/resources/list_changed\": self._handle_resources_list_changed,\n            \"notifications/prompts/list_changed\": self._handle_prompts_list_changed,\n            \"notifications/resources/updated\": self._handle_resource_updated,\n            \"notifications/logging/message\": self._handle_logging_message,\n        }\n        for method, handler in handlers.items():\n            self.register_notification_handler(method, handler)\n\n    async def close(self) -> None:\n        \"\"\"Gracefully closes the MCP client session and transport context.\"\"\"\n        if self._task_group is not None:\n            self._task_group.cancel_scope.cancel()\n            await self._task_group.__aexit__(None, None, None)\n            self._task_group = None\n\n        if self._connected and self._session:\n            await self._session.__aexit__(None, None, None)\n        if self._transport_cm:\n            await self._transport_cm.__aexit__(None, None, None)\n        self._connected = False\n        self._session = None\n        self._streams = None\n        logger.info(\"MCP client closed successfully.\")\n\n    @property\n    def session(self) -> ClientSession:\n        \"\"\"Gets the underlying ClientSession.\n\n        Returns:\n            ClientSession: The active client session.\n\n        Raises:\n            RuntimeError: If the session is not connected.\n        \"\"\"\n        if not self._session:\n            raise RuntimeError(\"MCP client session not connected. Call connect() first.\")\n        return self._session\n\n    async def _process_incoming_messages(self) -> None:\n        \"\"\"Processes incoming messages from the MCP server, including notifications.\"\"\"\n        logger.info(\"Starting message processor...\")\n        try:\n            async for message in self.session.incoming_messages:\n                if isinstance(message, Exception):\n                    logger.error(f\"Error in MCP communication: {message}\")\n                elif isinstance(message, types.ServerNotification):\n                    # Attach the session to the notification.\n                    message.session = self.session\n                    await self._handle_notification(message)\n                # Additional message types (e.g., RequestResponder) can be handled here.\n        except Exception as e:\n            logger.exception(f\"Error in message processor: {e}\")\n        finally:\n            logger.info(\"Message processor stopped\")\n\n    async def _handle_notification(self, notification: types.ServerNotification) -> None:\n        \"\"\"Handles server notifications based on their type.\n\n        Args:\n            notification (types.ServerNotification): The notification received from the server.\n        \"\"\"\n        method = notification.root.method\n        logger.debug(f\"Received notification: {method}\")\n\n        handler = self._notification_handlers.get(method)\n        if handler:\n            try:\n                await handler(notification)\n            except Exception as e:\n                logger.exception(f\"Error in notification handler for {method}: {e}\")\n        else:\n            logger.info(f\"Received unhandled notification: {method}\")\n\n    def register_notification_handler(\n        self, method: str, handler: Callable[[Any], Awaitable[None]]\n    ) -> None:\n        \"\"\"Registers a handler for a specific notification type.\n\n        Args:\n            method (str): The notification method name.\n            handler (Callable[[Any], Awaitable[None]]): The asynchronous handler function.\n        \"\"\"\n        self._notification_handlers[method] = handler\n        logger.debug(f\"Registered handler for {method}\")\n\n    # --- Notification Handlers ---\n    async def _handle_tools_list_changed(self, notification: types.ServerNotification) -> None:\n        \"\"\"Handles notifications for tools list changes by delegating to the observer.\n\n        Args:\n            notification (types.ServerNotification): The notification for tools list change.\n        \"\"\"\n        logger.info(\"Tools list changed notification received\")\n        # Implementation pending\n\n    async def _handle_resources_list_changed(self, notification: types.ServerNotification) -> None:\n        \"\"\"Handles notifications for resources list changes.\n\n        Args:\n            notification (types.ServerNotification): The notification for resources list change.\n        \"\"\"\n        logger.info(\"Resources list changed notification received\")\n        # Implementation pending\n\n    async def _handle_prompts_list_changed(self, notification: types.ServerNotification) -> None:\n        \"\"\"Handles notifications for prompts list changes.\n\n        Args:\n            notification (types.ServerNotification): The notification for prompts list change.\n        \"\"\"\n        logger.info(\"Prompts list changed notification received\")\n        # Implementation pending\n\n    async def _handle_resource_updated(self, notification: types.ServerNotification) -> None:\n        \"\"\"Handles notifications for resource updates.\n\n        Args:\n            notification (types.ServerNotification): The notification for resource update.\n        \"\"\"\n        logger.info(\"Resource updated notification received\")\n        # Implementation pending\n\n    async def _handle_logging_message(self, notification: types.ServerNotification) -> None:\n        \"\"\"Handles logging message notifications from the server.\n\n        Args:\n            notification (types.ServerNotification): The notification containing logging info.\n        \"\"\"\n        params = getattr(notification, \"params\", None)\n        if params and hasattr(params, \"level\") and hasattr(params, \"message\"):\n            level = params.level\n            message = params.message\n            if level == \"error\":\n                logger.error(f\"MCP server log: {message}\")\n            elif level == \"warning\":\n                logger.warning(f\"MCP server log: {message}\")\n            else:\n                logger.info(f\"MCP server log: {message}\")\n        else:\n            logger.info(f\"MCP server log (format unknown): {params}\")\n\n    # --- Convenience Methods for Common Operations ---\n    async def list_tools(self) -> types.ListToolsResult:\n        \"\"\"Lists available tools from the MCP server.\n\n        Returns:\n            types.ListToolsResult: The result containing available tools.\n        \"\"\"\n        return await self.session.list_tools()\n\n    async def call_tool(self, name: str, arguments: dict[str, Any]) -> types.CallToolResult:\n        \"\"\"Calls a tool on the MCP server.\n\n        Args:\n            name (str): The name of the tool to call.\n            arguments (dict[str, Any]): A dictionary of arguments for the tool.\n\n        Returns:\n            types.CallToolResult: The result from calling the tool.\n        \"\"\"\n        return await self.session.call_tool(name, arguments)\n\n    async def subscribe_resource(self, uri: str) -> Any:\n        \"\"\"Subscribes to updates for a specific resource.\n\n        Args:\n            uri (str): The URI of the resource to subscribe to.\n\n        Returns:\n            Any: The subscription result.\n        \"\"\"\n        return await self.session.subscribe_resource(uri)\n\n    async def unsubscribe_resource(self, uri: str) -> Any:\n        \"\"\"Unsubscribes from updates for a specific resource.\n\n        Args:\n            uri (str): The URI of the resource to unsubscribe from.\n\n        Returns:\n            Any: The unsubscription result.\n        \"\"\"\n        return await self.session.unsubscribe_resource(uri)\n"}
{"type": "source_file", "path": "src/mcp/mcp_tool_adapter.py", "content": "# src/mcp/mcp_tool_adapter.py\n\nfrom typing import Any, Dict, Optional\nfrom mcp.types import Tool as MCPToolDefinition\nfrom src.tools.core.base_mcp_tool import BaseMCPTool\nfrom src.data_models.tools import ToolResponse\nfrom src.data_models.agent import StreamContext\n\n\nclass DefaultMCPTool(BaseMCPTool):\n    \"\"\"\n    A default implementation of an MCP-based tool.\n    This class automatically initializes from an MCP tool definition and provides\n    a basic execution behavior (which you can later customize).\n    \"\"\"\n\n    async def execute(self, context: Optional[StreamContext] = None, **kwargs) -> ToolResponse:\n        # Default behavior: simply echo the provided arguments.\n        result_text = f\"Executed {self.name} with arguments: {kwargs}\"\n        # Construct and return a ToolResponse (adapt to your actual structure)\n        return ToolResponse(result=result_text)\n\n    def parse_output(self, output: str) -> Dict[str, Any]:\n        # Default parsing: wrap the output in a dictionary.\n        return {\"result\": output}\n\n\ndef convert_mcp_tool_to_flexo_tool(mcp_tool_def: MCPToolDefinition, config: Optional[Dict] = None) -> BaseMCPTool:\n    \"\"\"\n    Convert an MCP tool definition into a Flexo tool instance.\n\n    Args:\n        mcp_tool_def: An MCP tool definition instance (from mcp.types).\n        config: Optional configuration for the tool.\n\n    Returns:\n        An instance of BaseMCPTool (or a subclass thereof) that represents the tool.\n    \"\"\"\n    return DefaultMCPTool(mcp_tool_def, config)\n"}
{"type": "source_file", "path": "src/prompt_builders/__init__.py", "content": "from .base_prompt_builder import BasePromptBuilder\nfrom .openai.openai_prompt_builder import OpenAIPromptBuilder\nfrom .anthropic.anthropic_prompt_builder import AnthropicPromptBuilder\nfrom .watsonx.llama.llama_prompt_builder import WatsonXLlamaPromptBuilder\nfrom .watsonx.mistral.mistral_prompt_builder import WatsonXMistralPromptBuilder\nfrom .watsonx.granite.granite_prompt_builder import WatsonXGranitePromptBuilder\nfrom .mistral_ai.mistral_ai_prompt_builder import MistralAIPromptBuilder\nfrom .prompt_models import PromptPayload, PromptBuilderOutput\nfrom .openai_compat.granite.granite_prompt_builder import OpenAICompatGranitePromptBuilder\nfrom .openai_compat.llama.llama_prompt_builder import OpenAICompatLlamaPromptBuilder\nfrom .xai.xai_prompt_builder import XAIPromptBuilder\n"}
{"type": "source_file", "path": "src/llm/tool_detection/manual_detection_strategy.py", "content": "# src/llm/tool_detection/manual_tool_call_detection.py\n\nimport logging\nfrom typing import List\n\nfrom src.api import SSEChunk\nfrom src.data_models.agent import StreamContext\nfrom src.tools.core.parsers import BaseToolCallParser\nfrom src.data_models.chat_completions import ToolCall, FunctionDetail\nfrom src.llm.tool_detection import BaseToolCallDetectionStrategy\nfrom src.llm.pattern_detection import AhoCorasickBufferedProcessorNormalized, AhoCorasickBufferedProcessor\nfrom src.llm.tool_detection.detection_result import DetectionResult, DetectionState\n\n\nclass ManualToolCallDetectionStrategy(BaseToolCallDetectionStrategy):\n    \"\"\"A strategy for detecting tool calls in streaming LLM output using pattern matching.\n\n    This class implements manual detection of tool calls by processing streaming chunks\n    of text using the Aho-Corasick algorithm for pattern matching. It maintains internal\n    state to track tool call boundaries and accumulate content.\n\n    Args:\n        parser (BaseToolCallParser): Parser instance for processing detected tool calls.\n        pattern_config_path (str, optional): Path to YAML config file containing tool call patterns.\n            Defaults to \"src/configs/tool_call_patterns.yaml\".\n\n    Attributes:\n        tool_call_parser (BaseToolCallParser): Parser for processing tool calls.\n        pattern_detector (AhoCorasickBufferedProcessor): Pattern matching processor.\n        pre_tool_call_content (List[str]): Buffer for content before tool call.\n        tool_call_buffer (str): Buffer for accumulating tool call content.\n        in_tool_call (bool): Flag indicating if currently processing a tool call.\n        accumulation_mode (bool): Flag for content accumulation mode.\n\n    Example:\n        ```python\n        parser = JSONToolCallParser()\n        detector = ManualToolCallDetectionStrategy(parser)\n\n        # Process streaming chunks\n        async for chunk in stream:\n            result = await detector.detect_chunk(chunk, context)\n            if result.state == DetectionState.COMPLETE_MATCH:\n                # Handle detected tool call\n                process_tool_calls(result.tool_calls)\n        ```\n    \"\"\"\n\n    def __init__(self, parser: BaseToolCallParser, pattern_config_path: str = \"src/configs/tool_call_patterns.yaml\"):\n        self.logger = logging.getLogger(self.__class__.__name__)\n        self.logger.debug(\"Initializing ManualToolCallDetectionStrategy with config: %s\", pattern_config_path)\n        self.tool_call_parser = parser\n        self.pattern_detector = AhoCorasickBufferedProcessorNormalized(pattern_config_path)\n\n        self.pre_tool_call_content: List[str] = []\n        self.tool_call_buffer: str = \"\"\n        self.in_tool_call: bool = False\n        self.accumulation_mode: bool = False\n\n    def reset(self) -> None:\n        \"\"\"Reset all internal state of the detection strategy.\n\n        This method clears all buffers and resets flags to their initial state.\n        Should be called between processing different streams or after errors.\n        \"\"\"\n        self.logger.debug(\"Resetting detector state\")\n        self.pattern_detector.reset_states()\n        self.pre_tool_call_content = []\n        self.tool_call_buffer = \"\"\n        self.in_tool_call = False\n        self.accumulation_mode = False\n\n    async def detect_chunk(self, sse_chunk: SSEChunk, context: StreamContext) -> DetectionResult:\n        \"\"\"\n        Process a single chunk of streaming content for tool call detection.\n\n        Args:\n            sse_chunk (SSEChunk): The chunk of streaming content to process.\n            context (StreamContext): Context information for the current stream.\n\n        Returns:\n            DetectionResult: Result of processing the chunk, including detection state\n                and any content or tool calls found.\n        \"\"\"\n        # If the chunk has no valid delta content, return NO_MATCH.\n        if not sse_chunk.choices or not sse_chunk.choices[0].delta:\n            return DetectionResult(state=DetectionState.NO_MATCH, sse_chunk=sse_chunk)\n\n        chunk_content = sse_chunk.choices[0].delta.content\n        if not chunk_content:\n            return DetectionResult(state=DetectionState.NO_MATCH, sse_chunk=sse_chunk)\n\n        # If already in tool call detection mode, continue accumulating and report a partial match.\n        if self.in_tool_call:\n            self.tool_call_buffer += chunk_content\n            return DetectionResult(\n                state=DetectionState.PARTIAL_MATCH,\n                sse_chunk=sse_chunk\n            )\n\n        # Process the chunk through the pattern detector.\n        result = await self.pattern_detector.process_chunk(chunk_content)\n\n        if result.error:\n            return DetectionResult(state=DetectionState.NO_MATCH, sse_chunk=sse_chunk)\n\n        # If a pattern is matched, switch into tool call mode.\n        # Return any content remaining that could have been in the same chunk as the patter or in a buffer\n        if result.matched:\n            self.in_tool_call = True\n            self.tool_call_buffer = result.text_with_tool_call\n            return DetectionResult(\n                state=DetectionState.PARTIAL_MATCH,\n                content=result.output,\n                sse_chunk=sse_chunk\n            )\n\n        # For regular content, if there is any output, return it as PARTIAL_MATCH.\n        if result.output:\n            return DetectionResult(\n                state=DetectionState.PARTIAL_MATCH,\n                content=result.output,\n                sse_chunk=sse_chunk\n            )\n\n        # If nothing of interest is found, return NO_MATCH.\n        return DetectionResult(state=DetectionState.NO_MATCH, sse_chunk=sse_chunk)\n\n    async def finalize_detection(self, context: StreamContext) -> DetectionResult:\n        \"\"\"Finalize the detection process and handle any accumulated content.\n\n        This method is called at the end of a stream to process any remaining content\n        in the buffers and return final detection results.\n\n        Args:\n            context (StreamContext): Context information for the current stream.\n\n        Returns:\n            DetectionResult: Final result of the detection process, including any\n                complete tool calls or remaining content.\n        \"\"\"\n        self.logger.debug(\"Finalizing detection\")\n        # Flush any remaining content from pattern detector\n        final_result = await self.pattern_detector.flush_buffer()\n\n        if self.in_tool_call:\n            self.logger.debug(\"Processing final tool call buffer\")\n            if final_result.output:\n                self.tool_call_buffer += final_result.output\n\n            # Parse accumulated tool call\n            parsed_tool_call_data = self.tool_call_parser.parse(self.tool_call_buffer)\n            self.logger.debug(f\"Tool call buffer: {self.tool_call_buffer}\")\n            self.logger.debug(f\"Parsed tool call data: {parsed_tool_call_data}\")\n\n            if \"error\" in parsed_tool_call_data:\n                self.logger.error(f\"Tool call parsing failed: {parsed_tool_call_data['error']}\")\n                return DetectionResult(\n                    state=DetectionState.NO_MATCH,\n                    content=\"Sorry, but I was unable to complete your request. Please try again.\",\n                )\n\n            parsed_tool_calls = self._extract_tool_calls(parsed_tool_call_data)\n            self.logger.debug(\"Successfully parsed %d tool calls\", len(parsed_tool_calls))\n\n            return DetectionResult(\n                state=DetectionState.COMPLETE_MATCH,\n                tool_calls=parsed_tool_calls\n            )\n\n        # No tool call detected, return any final content\n        if final_result.output:\n            self.logger.debug(\"Returning final content: %s\", final_result.output[:50])\n            return DetectionResult(\n                state=DetectionState.PARTIAL_MATCH,\n                content=final_result.output\n            )\n\n        self.logger.debug(\"No final content to return\")\n        return DetectionResult(state=DetectionState.NO_MATCH)\n\n    def _extract_tool_calls(self, parsed_output: dict) -> List[ToolCall]:\n        \"\"\"Extract structured tool calls from parsed JSON output.\n\n        Converts the parsed JSON format into a list of ToolCall objects with\n        appropriate structure and typing.\n\n        Args:\n            parsed_output (dict): The parsed JSON output containing tool call data.\n\n        Returns:\n            List[ToolCall]: List of structured tool call objects ready for processing.\n        \"\"\"\n        tool_calls = []\n        for tool_call_dict in parsed_output.get(\"tool_calls\", []):\n            tool_call_args = tool_call_dict.get(\"parameters\", tool_call_dict.get(\"arguments\"))\n            self.logger.debug(\"Extracting tool call arguments: %s\", tool_call_args)\n            tool_calls.append(ToolCall(\n                id='123456789',  # Placeholder ID; modify as needed\n                type=tool_call_dict.get(\"type\", \"function\"),\n                function=FunctionDetail(\n                    name=tool_call_dict.get(\"name\"),\n                    arguments=str(tool_call_args)\n                )\n            ))\n        return tool_calls\n"}
{"type": "source_file", "path": "src/prompt_builders/anthropic/__init__.py", "content": "from .anthropic_prompt_builder import AnthropicPromptBuilder\n"}
