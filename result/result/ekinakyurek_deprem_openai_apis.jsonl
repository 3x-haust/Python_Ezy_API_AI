{"repo_info": {"repo_name": "deprem_openai_apis", "repo_owner": "ekinakyurek", "repo_url": "https://github.com/ekinakyurek/deprem_openai_apis"}}
{"type": "test_file", "path": "tests/__init__.py", "content": ""}
{"type": "test_file", "path": "tests/test_ai.py", "content": "import logging\nimport os\n\nfrom fastapi.testclient import TestClient\nfrom main import app\n\nlogger = logging.getLogger(__name__)\n\nclient = TestClient(app=app)\n\nPAYLOAD = {\n    \"inputs\": [\n        \"İskenderun Hatay Mustafa Kemal mahallesi 544 sokak no:11 (Batı Göz hastanesi\"\n        \" sokağı) Selahattin Yurt Dudu Yurt Sezer Yurt GÖÇÜK ALTINDALAR!!! #DEPREMOLDU\"\n        \" #depremhatay #deprem #Hatay #hatayacil #HatayaYardım #hataydepremi\",\n        \"LÜTFEN YAYIN!!!! 8 katlı bina HATAYDA Odabaşı mah. Uğur Mumcu caddesi no 4\"\n        \" Mahmut Karakaş kat 4\",\n    ],\n}\n\n\ndef test_intent():\n    correct_token = os.getenv(\"NEEDS_RESOLVER_API_KEY\")\n    headers = {\"Authorization\": f\"Bearer {correct_token}\"}\n    response = client.post(\"/intent-extractor/\", json=PAYLOAD, headers=headers)\n    assert response.status_code == 200\n    outputs = response.json()[\"response\"]\n    assert isinstance(outputs, list)\n\n    for obj in outputs:\n        logger.debug(obj)\n        assert isinstance(obj, dict)\n        assert \"string\" in obj\n        assert \"processed\" in obj\n        assert isinstance(obj[\"processed\"][\"intent\"], list)\n        assert len(obj[\"processed\"][\"intent\"]) > 0\n        assert isinstance(obj[\"processed\"][\"detailed_intent_tags\"], list)\n        assert len(obj[\"processed\"][\"detailed_intent_tags\"]) > 0\n"}
{"type": "source_file", "path": "main.py", "content": "import asyncio\nimport logging\nimport os\nimport re\nfrom functools import lru_cache\nfrom typing import List, Optional\nfrom fastapi import FastAPI, Request, HTTPException\nimport src.converter as converter\nfrom src.config import Settings\nfrom src.logger import setup_logging\nfrom src.models import IntentResponse, RequestIntent\nfrom src.lm.tokenizer import GPTTokenizer\n\n\nsetup_logging()\napp = FastAPI()\nrotator = 0\nlock = asyncio.Lock()\n\n@lru_cache(maxsize=None)\ndef get_settings(pid: int):\n    settings = Settings()\n\n    with open(settings.address_prompt_file) as handle:\n        settings.address_template = handle.read()\n\n    with open(settings.detailed_intent_prompt_file) as handle:\n        settings.detailed_intent_template = handle.read()\n\n    if settings.geo_location:\n        settings.geo_key = converter.setup_geocoding()\n\n    settings.openai_keys = converter.setup_openai(pid % settings.num_workers)\n\n    logging.warning(f\"Engine {settings.engine}\")\n\n    return settings\n\n\n\n\n\nasync def convert(\n        info: str,\n        inputs: List[str],\n        settings: Settings,\n        api_key: Optional[str] = None,\n):\n    if info == \"address\":\n        template = settings.address_template\n        max_tokens = settings.address_max_tokens\n        completion_params = dict(temperature=0.1, frequency_penalty=0.3)\n    elif info == \"detailed_intent\":\n        template = settings.detailed_intent_template\n        max_tokens = settings.detailed_intent_max_tokens\n        completion_params = dict(temperature=0.0, frequency_penalty=0.0)\n    else:\n        raise ValueError(\"Unknown information extraction requested\")\n\n    text_inputs = []\n    for tweet in inputs:\n        text_inputs.append(converter.create_prompt(text=tweet, template=template, max_tokens=max_tokens))\n\n    outputs = await converter.query_with_retry(\n        text_inputs,\n        api_key=api_key,\n        engine=settings.engine,\n        top_p=1,\n        max_tokens=max_tokens,\n        stop=\"#END\",\n        **completion_params,\n    )\n\n    returned = []\n    for output in outputs:\n        returned_dict = {}\n        returned_dict[\"string\"] = output\n        try:\n            returned_dict[\"processed\"] = converter.postprocess(info, output[0])\n        except Exception as e:\n            returned_dict[\"processed\"] = {\n                \"intent\": [],\n                \"detailed_intent_tags\": [],\n            }\n            logging.warning(f\"Parsing error in {output},\\n {e}\")\n\n        if info == \"address\" and settings.geo_location and returned_dict[\"processed\"]:\n            returned_dict[\"processed\"][\"geo\"] = converter.get_geo_result(\n                settings.geo_key, returned_dict[\"processed\"]\n            )\n        returned.append(returned_dict)\n\n    return returned\n\n\n@app.post(\"/intent-extractor/\", response_model=IntentResponse)\nasync def intent(payload: RequestIntent, req: Request):\n    # correct_token = os.getenv(\"NEEDS_RESOLVER_API_KEY\", None)\n    # if correct_token is None:\n    #     raise Exception(\"token not found in env files!\")\n    # coming_token = req.headers[\"Authorization\"]\n    # # Here your code for verifying the token or whatever you use\n    # if coming_token != 'Bearer ' + correct_token:\n    #     raise HTTPException(\n    #         status_code=401,\n    #         detail=\"Unauthorized\"\n    #     )\n\n    settings = get_settings(os.getpid())\n\n    inputs = payload.dict()[\"inputs\"]\n\n    global rotator\n    async with lock:\n        rotator = (rotator + 1) % len(settings.openai_keys)\n\n    api_key = settings.openai_keys[rotator]\n\n    outputs = await convert(\"detailed_intent\", inputs, settings, api_key=api_key)\n    return {\"response\": outputs}\n\n\n@app.get(\"/health\")\nasync def health():\n    return {\"status\": \"living the dream\"}"}
{"type": "source_file", "path": "src/__init__.py", "content": ""}
{"type": "source_file", "path": "eval.py", "content": "import json\nimport sklearn\nimport sklearn.metrics\nfrom absl import app, flags\nfrom sklearn.preprocessing import MultiLabelBinarizer\n\n\nFLAGS = flags.FLAGS\n\nflags.DEFINE_string(\n    \"input_file\", default=None, help=\"Prompt file to use for the problem\"\n)\n\n\ndef main(_):\n    true_values = []\n    pred_values = []\n\n    FILE_NAME = FLAGS.input_file\n    with open(FILE_NAME.replace(\"jsonl\", \"tsv\"), \"w\") as handle:\n\n        for line in open(FILE_NAME):\n            datum = json.loads(line)\n            y_true = datum[\"label\"]\n            y_pred = datum[\"detailed_intent_json\"][\"intent\"] #.split(\",\")\n            if \"Alakasiz\" in y_true:\n                del y_true[y_true.index(\"Alakasiz\")]\n            if len(y_true) == 0:\n                continue\n            true_values.append(y_true)\n            pred_values.append(y_pred)\n            print(\n                datum[\"image_url\"].replace(\"\\n\", \"\"), \"\\t\", y_true, \"\\t\", y_pred, file=handle\n            )\n\n    binarizer = MultiLabelBinarizer().fit(true_values)\n\n    # pdb.set_trace()\n    true_values = binarizer.transform(true_values)\n    pred_values = binarizer.transform(pred_values)\n\n    # pdb.set_trace()\n    print(\n        sklearn.metrics.classification_report(\n            true_values, pred_values, target_names=binarizer.classes_\n        ),\n    )\n\n\nif __name__ == \"__main__\":\n    app.run(main)\n"}
{"type": "source_file", "path": "src/gpt/network_manager.py", "content": "import asyncio\nimport logging\nfrom functools import wraps\nfrom math import ceil, log2\nfrom random import random\nfrom openai import APIError\nfrom openai.error import (\n    APIConnectionError,\n    AuthenticationError,\n    InvalidRequestError,\n    OpenAIError,\n    RateLimitError,\n    ServiceUnavailableError,\n    TryAgain,\n)\n# from src.concurrent.asynchronous import run_async_tasks\n\n\nlogger = logging.getLogger(__name__)\n\n\nOPENAI_MAX_RETRY = 10\n# quota is reset in every 60 seconds\nOPENAI_REFRESH_QUOTA = 60\nOPENAI_EXP_CAP = int(ceil(log2(OPENAI_REFRESH_QUOTA)))\n\n\nclass OpenAINetworkManager:\n    def __init__(self):\n        raise AssertionError(f\"{type(self).__name__} should not be instantiated.\")\n\n    @staticmethod\n    def async_retry_with_exp_backoff(task):\n        @wraps(task)\n        async def wrapper(*args, **kwargs):\n            for i in range(OPENAI_MAX_RETRY + 1):\n                wait_time = (1 << min(i, OPENAI_EXP_CAP)) + random() / 10\n                try:\n                    return task(*args, **kwargs)\n                except (\n                    RateLimitError,\n                    ServiceUnavailableError,\n                    APIConnectionError,\n                    APIError,\n                    TryAgain,\n                ) as e:\n                    if i == OPENAI_MAX_RETRY:\n                        logger.error(\n                            f\"Retry, TooManyRequests or Server Error. {str(e)}\"\n                        )\n                        raise e\n                    else:\n                        logger.warning(\n                            f\"Waiting {round(wait_time, 2)} seconds for API...\",\n                        )\n                        await asyncio.sleep(wait_time)\n                except AuthenticationError as e:\n                    # No way to handle\n                    logger.error(f\"AuthenticationError: {str(e)}\")\n                    raise Exception(\n                        \"AuthenticationError: Incorrect API key is provided.\",\n                    )\n                except InvalidRequestError as e:\n                    logger.error(f\"InvalidRequestError: {str(e)}\")\n                    raise e\n                except OpenAIError as e:\n                    logger.error(f\"API Request failed. {str(e)}\")\n                    raise e\n                except Exception as e:\n                    logger.error(f\"Error unrelated to API. {str(e)}\")\n                    raise e\n\n        return wrapper\n\n\nasync def interact_with_api(func, *args, **kwargs):\n    @OpenAINetworkManager.async_retry_with_exp_backoff\n    def interact():\n        return func(*args, **kwargs)\n\n    return await interact()\n"}
{"type": "source_file", "path": "src/config.py", "content": "from typing import Optional, List\nfrom pydantic import BaseSettings\n\n\nclass Settings(BaseSettings):\n    address_prompt_file: str = \"prompts/address.txt\"\n    detailed_intent_prompt_file: str = \"prompts/detailed_intent.txt\"\n    address_template: Optional[str] = None\n    detailed_intent_template: Optional[str] = None\n    geo_key: Optional[str] = None\n    openai_keys: Optional[List[str]] = None\n    address_max_tokens: int = 384\n    detailed_intent_max_tokens: int = 100\n    batch_size: int = 20\n    geo_location: bool = False\n    num_workers: int = 5\n    engine: str = \"afet-org\"\n\n    class Config:\n        env_file = \".env\"\n\n\n    \n"}
{"type": "source_file", "path": "src/gpt/__init__.py", "content": ""}
{"type": "source_file", "path": "src/converter.py", "content": "import asyncio\nimport json\nimport os\nimport re\nimport urllib\nfrom typing import List, Optional\nimport openai\nimport requests\nfrom absl import app, flags, logging\nfrom tqdm import tqdm\nfrom src.gpt.network_manager import interact_with_api\nfrom src.lm.tokenizer import GPTTokenizer\n\nFLAGS = flags.FLAGS\n\nflags.DEFINE_string(\n    \"prompt_file\", default=None, help=\"Prompt file to use for the problem\"\n)\n\nflags.DEFINE_string(\"input_file\", default=None, help=\"Input file to read data\")\n\nflags.DEFINE_string(\"output_file\", default=None, help=\"Output file to write to\")\n\nflags.DEFINE_integer(\"max_tokens\", default=384, help=\"LM max generation length\")\n\nflags.DEFINE_integer(\"worker_id\", default=0, help=\"Worker id for the job\")\n\nflags.DEFINE_integer(\"num_workers\", default=1, help=\"number of workers\")\n\nflags.DEFINE_integer(\"batch_size\", default=20, help=\"batch size for OpenAI queries\")\n\nflags.DEFINE_boolean(\n    \"geo_location\", default=False, help=\"whether to add geo location to the output\"\n)\n\nflags.DEFINE_string(\"info\", default=\"address\", help=\"address | intent\")\n\nflags.DEFINE_string(\"engine\", \"code-davinci-002\", help=\"GPT engines\")\n\nGEO_BASE_URL = \"https://maps.googleapis.com/maps/api/geocode/json?\"\n\n# TODO: add more keywords.\nNON_ADDRESS_WORDS = [\n    \"arkadaş\",\n    \"bebek\",\n    \"enkaz\",\n    \"deprem\",\n    \"ekipman\",\n    \"araç\",\n    \"kayıp\",\n    \"acil\",\n    \"yardım\",\n    \"kurtarma\",\n    \"kayıp\",\n    \"aile\",\n    \"baba\",\n]\n\n\ndef postprocess_for_address(address):\n    # a quick rule based filtering for badly parsed outputs.\n    address = json.loads(address)\n    if type(address) == dict:\n        for key in (\n            \"mahallesi | bulvarı\",\n            \"sokak | caddesi | yolu\",\n            \"sitesi | apartmanı\",\n            \"no | blok\",\n            \"kat\",\n            \"phone\",\n        ):\n            if (\n                key in address\n                and len(address[key]) > 50\n                or any(word in address[key] for word in NON_ADDRESS_WORDS)\n            ):\n                address[key] = \"\"\n\n        for key in (\"no | blok\", \"kat\"):\n            if key in address and len(address[key]) > 20:\n                address[key] = \"\"\n\n    return address\n\n\nTAG_MAP = {\n    \"POWER_SOURCE\": \"Elektrik Kaynagi\",\n    \"WATER\": \"Su\",\n    \"LOGISTICS\": \"Lojistik\",\n    \"TRANSPORTATION\": \"Lojistik\",\n    \"FOOD\": \"Yemek\",\n    \"RESCUE\": \"Enkaz Kaldirma\",\n    \"HEALTH\": \"Saglik\",\n    \"UNINFORMATIVE\": \"Alakasiz\",\n    \"SHELTER\": \"Barınma\",\n    \"HEATING\": \"Isinma\",\n    \"RESCUE_ELECTRONICS\": \"Arama Ekipmani\",\n    # \"LOOTING\": \"Yagma\",\n    \"BURIAL\": \"Cenaze\",\n    \"CLOTHES\": \"Giysi\",\n    \"PORTABLE_TOILET\": \"Tuvalet\"\n}\n\n\nCHARMAP = {\n    u\"I\": u\"ı\",\n    u\"İ\": u\"i\",\n}\n\ndef tr_lower(text):\n    for c1, c2 in CHARMAP.items():\n        text = text.replace(c1, c2)\n    return text\n    \ndef postprocess_for_intent(intent):\n    m = re.search(r\"(?<=\\[).+?(?=\\])\", intent)\n    if m:\n        tags = m.group()\n        tags = [TAG_MAP.get(tag.strip(), tag.strip()) for tag in tags.split(\",\")]\n        return {\"intent\": \",\".join(tags)}\n    else:\n        return {\"intent\": \"Diğer\"}\n\n\ndef postprocess_for_intent_v2(intent):\n    matches = re.findall(r\"(?<=\\[).+?(?=\\])\", intent)\n    if matches:\n        detailed_intent = matches[0]\n        detailed_intent_tags = [\n            tr_lower(TAG_MAP.get(tag.strip(), tag.strip())).lower() for tag in detailed_intent.split(\",\")\n        ]\n        if len(matches) > 1:\n            intent = matches[1]\n            intent_tags = [\n                TAG_MAP.get(tag.strip(), tag.strip()) for tag in intent.split(\",\")\n            ]\n        else:\n            intent_tags = []\n\n        return {\n            \"intent\": intent_tags,  # \",\".join(intent_tags),\n            \"detailed_intent_tags\": detailed_intent_tags,  # \",\".join(detailed_intent_tags),\n        }\n    else:\n        return {\n            \"intent\": [],  # \",\".join(intent_tags),\n            \"detailed_intent_tags\": [],  # \",\".join(detailed_intent_tags),\n        }\n\n\ndef postprocess(info, line):\n    if info == \"address\":\n        return postprocess_for_address(line)\n    elif info == \"detailed_intent\":\n        return postprocess_for_intent_v2(line)\n    else:\n        raise ValueError(\"Unknown info type\")\n\n\ndef get_address_str(address):\n    address_str = \"\"\n    for key in (\n        \"mahallesi | bulvarı\",\n        \"sokak | caddesi | yolu\",\n        \"sitesi | apartmanı\",\n        \"no | blok\",\n        \"city\",\n        \"province\",\n    ):\n        address_str += address.get(key, \"\") + \" \"\n\n    return address_str.strip()\n\n\nasync def query_with_retry(inputs: List[str], api_key: Optional[str] = None, **kwargs) -> List[List[str]]:\n    \"\"\"Queries GPT API up to max_retry time to get the responses.\"\"\"\n    if api_key:\n        openai.api_key = api_key\n    try:\n        response = await interact_with_api(openai.Completion.create, prompt=inputs, **kwargs)\n    except Exception:\n        return [['{\"status\": \"ERROR\"}']] * len(inputs)\n\n    return [\n        [line for line in choice[\"text\"].split(\"\\n\") if len(line) > 10]\n        for choice in response[\"choices\"]\n    ]\n\n\ndef setup_openai(worker_id: int = 0) -> List[str]:\n    logging.warning(f\"worker id in open ai keys {worker_id}\")\n\n    try:\n        openai_keys = os.getenv(\"OPENAI_API_KEY_POOL\").split(\",\")\n    except KeyError:\n        logging.error(\n            \"OPENAI_API_KEY_POOL or OPENAI_API_BASE_POOL environment variable is not\"\n            \" specified\"\n        )\n\n    assert len(openai_keys) > 0, \"No keys specified in the environment variable\"\n\n    # set the default key\n    openai.api_key = openai_keys[worker_id % len(openai_keys)].strip()\n\n    try:\n        openai_bases = os.getenv(\"OPENAI_API_BASE_POOL\").split(\",\")\n        assert len(openai_bases) == len(openai_keys)\n        openai.api_type = \"azure\"\n        openai.api_version = \"2022-12-01\"\n        openai.api_base = openai_bases[worker_id % len(openai_bases)].strip()\n    except (KeyError, AttributeError):\n        logging.warning(\"OPENAI_API_BASE_POOL is not specified in the environment\")\n    except AssertionError as msg:\n        logging.error(\n            \"Env variables OPENAI_API_KEY_POOL and OPENAI_API_BASE_POOL has\"\n            f\" incosistent shapes, {msg}\"\n        )\n\n    return openai_keys\n\ndef setup_geocoding(worker_id: int = 0):\n    try:\n        geo_keys = os.getenv(\"GEO_KEY_POOL\").split(\",\")\n    except KeyError:\n        logging.error(\"GEO_KEY_POOL environment variable is not specified\")\n\n    assert len(geo_keys) > 0, \"No keys specified in the environment variable\"\n\n    worker_geo_key = geo_keys[worker_id % len(geo_keys)].strip()\n\n    return worker_geo_key\n\n\ndef get_geo_result(key, address):\n    address_str = get_address_str(address)\n    parameters = {\"address\": address_str, \"key\": key}\n    response = requests.get(f\"{GEO_BASE_URL}{urllib.parse.urlencode(parameters)}\")\n\n    if response.status_code == 200:\n        results = json.loads(response.content)[\"results\"]\n        if results:\n            for result in results:\n                if \"geometry\" in result and \"location\" in result[\"geometry\"]:\n                    loc = result[\"geometry\"][\"location\"]\n                    link = \"https://maps.google.com/?q={lat},{lng}\".format(\n                        lat=loc[\"lat\"], lng=loc[\"lng\"]\n                    )\n                    result[\"gmaps_link\"] = link\n        return results\n    else:\n        logging.warning(response.content)\n\n\ndef preprocess_tweet(text: str) -> str:\n    mention_pattern = r\"@\\w+\"\n    url_pattern = r\"(\\w+?://)?(?:www\\.)?[-a-zA-Z0-9@:%._\\\\+~#=]{1,256}\\.[a-zA-Z]{1,10}\\b(?:[-a-zA-Z0-9()@:%_\\\\+.~#?&\\\\/=]*)\"\n    # remove mentions\n    mentions_removed = re.sub(mention_pattern, \" \", text)\n    # remove urls\n    url_removed = re.sub(url_pattern, \"\", mentions_removed)\n    # remove consequent spaces\n    return re.sub(r\"\\s+\", \" \", url_removed)\n\n\ndef create_prompt(text: str, template: str, max_tokens: int) -> str:\n    template_token_count = GPTTokenizer.token_count(template)\n\n    preprocessed_text = preprocess_tweet(text)\n\n    truncated_text = GPTTokenizer.truncate(\n        preprocessed_text,\n        max_tokens=GPTTokenizer.MAX_TOKENS - max_tokens - template_token_count,\n    )\n\n    return template.format(ocr_input=truncated_text)\n\n\ndef main(_):\n    setup_openai(FLAGS.worker_id)\n    if FLAGS.geo_location:\n        geo_key = setup_geocoding(FLAGS.worker_id)\n\n    with open(FLAGS.prompt_file) as handle:\n        template = handle.read()\n\n    if FLAGS.info == \"address\":\n        completion_params = dict(temperature=0.1, frequency_penalty=0.3)\n    elif \"intent\" in FLAGS.info:\n        completion_params = dict(temperature=0.0, frequency_penalty=0.0)\n    else:\n        raise ValueError(\"Unknown info\")\n\n    logging.info(f\"Engine {FLAGS.engine}\")\n\n    loop = asyncio.get_event_loop()\n\n    with open(FLAGS.input_file) as handle:\n        # raw_data = [json.loads(line.strip()) for line in handle]\n        raw_data = json.load(handle)\n        split_size = len(raw_data) // FLAGS.num_workers\n        raw_data = raw_data[\n            FLAGS.worker_id * split_size : (FLAGS.worker_id + 1) * split_size\n        ]\n\n    logging.info(f\"Length of the data for this worker is {len(raw_data)}\")\n    text_inputs = []\n    raw_inputs = []\n\n    for index, row in tqdm(enumerate(raw_data)):\n        # text_inputs.append(template.format(ocr_input=row[\"Tweet\"]))\n        text_inputs.append(create_prompt(text=row[\"image_url\"], template=template, max_tokens=FLAGS.max_tokens))\n        raw_inputs.append(row)\n\n        if (index + 1) % FLAGS.batch_size == 0 or index == len(raw_data) - 1:\n            # to not throttle api key limits with parallel queries?\n            outputs = loop.run_until_complete(query_with_retry(\n                text_inputs,\n                engine=FLAGS.engine,\n                max_tokens=FLAGS.max_tokens,\n                top_p=1,\n                presence_penalty=0,\n                stop=\"#END\",\n                **completion_params,\n            ))\n\n            with open(FLAGS.output_file, \"a+\") as handle:\n                for inp, output_lines in zip(raw_inputs, outputs):\n                    # for output_line in output_lines:\n                    output_line = output_lines[0]\n                    current_input = inp.copy()\n                    try:\n                        current_input[FLAGS.info + \"_json\"] = postprocess(\n                            FLAGS.info, output_line\n                        )\n                        current_input[FLAGS.info + \"_str\"] = output_line\n                    except Exception as e:\n                        logging.warning(f\"Parsing error in {output_line},\\n {e}\")\n                        current_input[FLAGS.info + \"_json\"] = {}\n                        current_input[FLAGS.info + \"_str\"] = output_line\n\n                    if (\n                        FLAGS.info == \"address\"\n                        and FLAGS.geo_location\n                        and type(current_input[FLAGS.info + \"_json\"]) == dict\n                    ):\n                        current_input[\"geo\"] = get_geo_result(\n                            geo_key, current_input[FLAGS.info + \"_json\"]\n                        )\n\n                    json_output = json.dumps(current_input)\n                    handle.write(json_output + \"\\n\")\n\n            text_inputs = []\n            raw_inputs = []\n\n\nif __name__ == \"__main__\":\n    app.run(main)\n"}
{"type": "source_file", "path": "src/logger.py", "content": "import logging\nimport sys\n\n\ndef setup_logging():\n    handler = logging.StreamHandler(sys.stdout)\n    formatter = logging.Formatter(\n        '%(asctime)s [%(levelname)s] %(name)s: %(message)s'\n    )\n    handler.setFormatter(formatter)\n\n    # default logger\n    default_logger = logging.getLogger()\n    # remove default handler with formatter\n    default_logger.handlers.clear()\n    default_logger.setLevel(logging.INFO)\n    default_logger.addHandler(handler)\n\n"}
{"type": "source_file", "path": "src/lm/tokenizer.py", "content": "from transformers import AutoTokenizer\n\n\ntokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n\n\nclass GPTTokenizer:\n    MAX_TOKENS = 4096\n\n    @classmethod\n    def token_count(cls, text: str) -> int:\n        return len(tokenizer(text, truncation=False)[\"input_ids\"])\n\n    @classmethod\n    def truncate(self, text: str, max_tokens: int) -> str:\n        encoded = tokenizer(text, truncation=True, max_length=max_tokens)\n        return tokenizer.decode(encoded[\"input_ids\"])\n"}
{"type": "source_file", "path": "src/lm/__init__.py", "content": ""}
{"type": "source_file", "path": "src/models.py", "content": "from pydantic import BaseModel, Field\nfrom typing import List\n\n\nclass RequestIntent(BaseModel):\n    inputs: List[str] = Field(\n        description=\"list of tweets to classify or parse\",\n        default=\"\"\" [\"İskenderun Hatay Mustafa Kemal mahallesi 544 sokak no:11 (Batı Göz hastanesi sokağı) Selahattin Yurt Dudu Yurt Sezer Yurt GÖÇÜK ALTINDALAR!!! #DEPREMOLDU #depremhatay #deprem #Hatay #hatayacil #HatayaYardım #hataydepremi\", \"LÜTFEN YAYIN!!!! 8 katlı bina HATAYDA Odabaşı mah. Uğur Mumcu caddesi no 4 Mahmut Karakaş kat 4\"]\"\"\",\n    )\n\n\nclass IntentRequest(BaseModel):\n    inputs: List[str] = Field(\n        description=\"list of tweets to classify or parse\",\n        default=\"\"\" [\"İskenderun Hatay Mustafa Kemal mahallesi 544 sokak no:11 (Batı Göz hastanesi sokağı) Selahattin Yurt Dudu Yurt Sezer Yurt GÖÇÜK ALTINDALAR!!! #DEPREMOLDU #depremhatay #deprem #Hatay #hatayacil #HatayaYardım #hataydepremi\", \"LÜTFEN YAYIN!!!! 8 katlı bina HATAYDA Odabaşı mah. Uğur Mumcu caddesi no 4 Mahmut Karakaş kat 4\"]\"\"\",\n    )\n\n\nclass IntentResponse(BaseModel):\n    response: List[dict]\n"}
