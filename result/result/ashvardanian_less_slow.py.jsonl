{"repo_info": {"repo_name": "less_slow.py", "repo_owner": "ashvardanian", "repo_url": "https://github.com/ashvardanian/less_slow.py"}}
{"type": "source_file", "path": "less_slow.py", "content": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n\"\"\"\nless_slow.py\n============\n\nMicro-benchmarks to build a performance-first mindset in Python.\n\nThis project is a spiritual brother to `less_slow.cpp` for C++20,\nand `less_slow.rs` for Rust. Unlike low-level systems languages,\nPython is a high-level with significant runtime overheads, and\nno obvious way to predict the performance of code.\n\nMoreover, the performance of different language and library components\ncan vary significantly between consecutive Python versions. That's\ntrue for both small numeric operations, and high-level abstractions,\nlike iterators, generators, and async code.\n\"\"\"\n\nimport pytest\nimport numpy as np\n\n# region: Numerics\n\n# region: Accuracy vs Efficiency of Standard Libraries\n\n# ? Numerical computing is a core subject in high-performance computing (HPC)\n# ? research and graduate studies, yet its foundational concepts are more\n# ? accessible than they seem. Let's start with one of the most basic\n# ? operations — computing the __sine__ of a number.\n\nimport math\n\n\ndef f64_sine_math(x: float) -> float:\n    return math.sin(x)\n\n\ndef f64_sine_math_cached(x: float) -> float:\n    # Cache the math.sin function lookup\n    local_sin = math.sin\n    return local_sin(x)\n\n\ndef f64_sine_numpy(x: float) -> float:\n    return np.sin(x)\n\n\n# ? NumPy is the de-facto standard for numerical computing in Python, and\n# ? it's known for its speed and simplicity. However, it's not always the\n# ? fastest option for simple operations like sine, cosine, or exponentials.\n# ?\n# ? NumPy lacks hot-path optimizations if the input is a single scalar value,\n# ? and it's often slower than the standard math library for such cases:\n# ?\n# ?  - math.sin:  620us\n# ?  - np.sin:    4200us\n# ?\n# ? NumPy, of course, becomes much faster when the input is a large array,\n# ? as opposed to a single scalar value.\n# ?\n# ? When absolute bit-accuracy is not required, it's often possible to\n# ? approximate mathematical functions using simpler, faster operations.\n# ? For example, the Maclaurin series for sine:\n# ?\n# ?   sin(x) ≈ x - (x^3)/3! + (x^5)/5! - (x^7)/7! + ...\n# ?\n# ? is a simple polynomial approximation that converges quickly for small x.\n# ? Both can be implemented in Python, NumPy, or Numba JIT, and benchmarked.\n\n\ndef f64_sine_math_maclaurin(x: float) -> float:\n    return x - math.pow(x, 3) / 6.0 + math.pow(x, 5) / 120.0\n\n\ndef f64_sine_numpy_maclaurin(x: float) -> float:\n    return x - np.pow(x, 3) / 6.0 + np.pow(x, 5) / 120.0\n\n\ndef f64_sine_maclaurin_powless(x: float) -> float:\n    x2 = x * x\n    x3 = x2 * x\n    x5 = x3 * x2\n    return x - (x3 / 6.0) + (x5 / 120.0)\n\n\ndef f64_sine_maclaurin_multiply(x: float) -> float:\n    x2 = x * x\n    x3 = x2 * x\n    x5 = x3 * x2\n    return x - (x3 * 0.1666666667) + (x5 * 0.008333333333)\n\n\n# ? Let's define a couple of helper functions to run benchmarks on these functions,\n# ? and compare their performance on 10k random floats in [0, 2π]. We can also\n# ? use Numba JIT to compile these functions to machine code, and compare the\n# ? performance of the JIT-compiled functions with the standard Python functions.\n\nnumba_installed = False\ntry:\n    import numba\n\n    numba_installed = True\nexcept ImportError:\n    pass  # skip if numba is not installed\n\n\ndef _f64_sine_run_benchmark_on_each(benchmark, sin_fn):\n    \"\"\"Applies `sin_fn` to 10k random floats in [0, 2π] individually.\"\"\"\n    inputs = np.random.rand(10_000)  # 10k random floats\n    inputs = inputs.astype(np.float64) * 2 * np.pi  # [0, 2π]\n\n    def call_sin_on_all():\n        for x in inputs:\n            sin_fn(x)\n\n    result = benchmark(call_sin_on_all)\n    return result\n\n\ndef _f64_sine_run_benchmark_on_all(benchmark, sin_fn):\n    \"\"\"Applies `sin_fn` to 10k random floats in [0, 2π] all at once.\"\"\"\n    inputs = np.random.rand(10_000)  # 10k random floats\n    inputs = inputs.astype(np.float64) * 2 * np.pi  # [0, 2π]\n    call_sin_on_all = lambda: sin_fn(inputs)  # noqa: E731\n    result = benchmark(call_sin_on_all)\n    return result\n\n\n@pytest.mark.benchmark(group=\"sin\")\ndef test_f64_sine_math(benchmark):\n    _f64_sine_run_benchmark_on_each(benchmark, f64_sine_math)\n\n\n@pytest.mark.benchmark(group=\"sin\")\ndef test_f64_sine_math_cached(benchmark):\n    _f64_sine_run_benchmark_on_each(benchmark, f64_sine_math_cached)\n\n\n@pytest.mark.benchmark(group=\"sin\")\ndef test_f64_sine_numpy(benchmark):\n    _f64_sine_run_benchmark_on_each(benchmark, f64_sine_numpy)\n\n\n@pytest.mark.benchmark(group=\"sin\")\ndef test_f64_sine_maclaurin_math(benchmark):\n    _f64_sine_run_benchmark_on_each(benchmark, f64_sine_math_maclaurin)\n\n\n@pytest.mark.benchmark(group=\"sin\")\ndef test_f64_sine_maclaurin_numpy(benchmark):\n    _f64_sine_run_benchmark_on_each(benchmark, f64_sine_numpy_maclaurin)\n\n\n@pytest.mark.benchmark(group=\"sin\")\ndef test_f64_sine_maclaurin_powless(benchmark):\n    _f64_sine_run_benchmark_on_each(benchmark, f64_sine_maclaurin_powless)\n\n\n@pytest.mark.benchmark(group=\"sin\")\ndef test_f64_sine_maclaurin_multiply(benchmark):\n    _f64_sine_run_benchmark_on_each(benchmark, f64_sine_maclaurin_multiply)\n\n\n@pytest.mark.skipif(not numba_installed, reason=\"Numba not installed\")\n@pytest.mark.benchmark(group=\"sin\")\ndef test_f64_sine_jit(benchmark):\n    sin_fn = numba.njit(f64_sine_math)\n    sin_fn(0.0)  # trigger compilation\n    _f64_sine_run_benchmark_on_each(benchmark, sin_fn)\n\n\n@pytest.mark.skipif(not numba_installed, reason=\"Numba not installed\")\n@pytest.mark.benchmark(group=\"sin\")\ndef test_f64_sine_maclaurin_jit(benchmark):\n    sin_fn = numba.njit(f64_sine_math_maclaurin)\n    sin_fn(0.0)  # trigger compilation\n    _f64_sine_run_benchmark_on_each(benchmark, sin_fn)\n\n\n@pytest.mark.skipif(not numba_installed, reason=\"Numba not installed\")\n@pytest.mark.benchmark(group=\"sin\")\ndef test_f64_sine_maclaurin_powless_jit(benchmark):\n    sin_fn = numba.njit(f64_sine_maclaurin_powless)\n    sin_fn(0.0)  # trigger compilation\n    _f64_sine_run_benchmark_on_each(benchmark, sin_fn)\n\n\n@pytest.mark.benchmark(group=\"sin\")\ndef test_f64_sines_numpy(benchmark):\n    _f64_sine_run_benchmark_on_all(benchmark, f64_sine_numpy)\n\n\n@pytest.mark.benchmark(group=\"sin\")\ndef test_f64_sines_maclaurin_numpy(benchmark):\n    _f64_sine_run_benchmark_on_all(benchmark, f64_sine_numpy_maclaurin)\n\n\n@pytest.mark.benchmark(group=\"sin\")\ndef test_f64_sines_maclaurin_powless(benchmark):\n    _f64_sine_run_benchmark_on_all(benchmark, f64_sine_maclaurin_powless)\n\n\n# ? The results are somewhat shocking!\n# ?\n# ? `f64_sine_maclaurin_powless` and `test_f64_sine_maclaurin_numpy` are\n# ? both the fastest and one of the slowest implementations, depending on\n# ? the input shape - scalar or array: 29us vs 2610us.\n# ?\n# ? This little benchmark is enough to understand, why Python is broadly\n# ? considered a \"glue\" language for various native languages and pre-compiled\n# ? libraries for batch/bulk processing.\n\n# endregion: Accuracy vs Efficiency of Standard Libraries\n\n# region: Approximations and Benchmarks\n\n# ? Python benefits greatly from a wide ecosystem of numerics libraries.\n# ? If you know Linear Algebra, there is a wide range of tricks you can\n# ? use to speed up your code.\n# ?\n# ? For example, the Singular Value Decomposition (SVD) is a fundamental\n# ? operation proven by the Eckart-Young-Mirsky theorem to be the best\n# ? low-rank approximation of a matrix w.r.t. the Frobenius norm.\n# ?\n# ? https://en.wikipedia.org/wiki/Singular_value_decomposition\n# ? https://en.wikipedia.org/wiki/Matrix_norm#Frobenius_norm\n\n\n@pytest.mark.benchmark(group=\"decomposition\")\n@pytest.mark.parametrize(\"n_dim\", [1000, 5000])\n@pytest.mark.parametrize(\"k_percent\", [\"100%\", \"20%\", \"4%\"])\n@pytest.mark.parametrize(\"dtype\", [np.float32, np.float64])\ndef test_svd(benchmark, n_dim: int, k_percent: str, dtype: np.dtype):\n    k = int(n_dim * int(k_percent[:-1]) / 100)\n    m = 10  # Number of columns in the multiplication circuit\n\n    # ? Just to prove a point, let's make the second dimension slightly\n    # ? larger to show that this works for non-square matrices too.\n    A = np.random.rand(n_dim, n_dim + 1).astype(dtype)\n    X = np.random.rand(n_dim + 1, m).astype(dtype)\n\n    def baseline():\n        return A @ X\n\n    def frobenius_norm(A: np.ndarray) -> float:\n        return np.linalg.norm(A, ord=\"fro\")\n\n    def low_rank_approximation(A, k: int) -> tuple:\n        U, S, VT = np.linalg.svd(A, full_matrices=False)\n        return U[:, :k], S[:k], VT[:k, :]\n\n    # Compute the low-rank approximation of A just once\n    U_k, S_k, VT_k = low_rank_approximation(A, k)\n    S_truncated = np.zeros((k, k))\n    np.fill_diagonal(S_truncated, S_k)\n\n    # Estimate the error of the low-rank approximation\n    mean_recovery_error = frobenius_norm(A - U_k @ S_truncated @ VT_k) / A.size\n    expected_result = baseline()\n    max_circuit_error = 0.0  # ? We will update this later\n\n    # Decomposed multiplication: compute A_approx @ X where A_approx = U_k * diag(S_k) * VT_k.\n    # This is computed in two steps: first compute (VT_k @ X), then scale by S_k, and finally multiply by U_k.\n    def decomposed():\n        temp = VT_k @ X  # (k x m)\n        # Scale each row by the corresponding singular value without explicitly constructing\n        # the diagonal matrix.\n        temp = S_k[:, np.newaxis] * temp\n        return U_k @ temp  # (n_dim x m)\n\n    def bench_svd():\n        product = baseline if k_percent == \"100%\" else decomposed\n        result = product()\n        mean_error = frobenius_norm(result - expected_result) / result.size\n\n        # Update the `max_circuit_error`\n        nonlocal max_circuit_error\n        max_circuit_error = max(max_circuit_error, mean_error)\n        return mean_error\n\n    # Estimate FLOPs:\n    # Full multiplication: A (n_dim x (n_dim+1)) times X ((n_dim+1) x m)\n    # roughly requires 2 * n_dim * (n_dim+1) * m floating-point operations.\n    full_flops = 2 * n_dim * (n_dim + 1) * m\n\n    # Decomposed multiplication consists of:\n    # 1. VT_k @ X: (k x (n_dim+1)) * ((n_dim+1) x m) → 2 * k * (n_dim+1) * m flops.\n    # 2. Scaling by S_k: k * m flops (one multiplication per element).\n    # 3. U_k @ (result): (n_dim x k) * (k x m) → 2 * n_dim * k * m flops.\n    decomposed_flops = 2 * k * (n_dim + 1) * m + k * m + 2 * n_dim * k * m\n\n    benchmark(bench_svd)\n    benchmark.extra_info[\"mean_recovery_error\"] = mean_recovery_error\n    benchmark.extra_info[\"flops_reduction\"] = full_flops / decomposed_flops\n\n\n# ? SVD is not the only decomposition method. Less theoretically sound but\n# ? often faster is the QR decomposition, which is applicable to any full-rank.\n# ? It produces an orthogonal matrix Q and an upper triangular matrix R such that\n# ? A = Q @ R.\n\n\n@pytest.mark.benchmark(group=\"decomposition\")\n@pytest.mark.parametrize(\"n_dim\", [1000, 5000])\n@pytest.mark.parametrize(\"k_percent\", [\"100%\", \"20%\", \"4%\"])\n@pytest.mark.parametrize(\"dtype\", [np.float32, np.float64])\ndef test_qr(benchmark, n_dim: int, k_percent: float, dtype: np.dtype):\n    k = int(n_dim * int(k_percent[:-1]) / 100)\n    m = 10  # Number of columns in the multiplication circuit\n\n    # ? Just to prove a point, let's make the second dimension slightly\n    # ? larger to show that this works for non-square matrices too.\n    A = np.random.rand(n_dim, n_dim + 1).astype(dtype)\n    X = np.random.rand(n_dim + 1, m).astype(dtype)\n\n    def baseline():\n        return A @ X\n\n    def frobenius_norm(mat: np.ndarray) -> float:\n        return np.linalg.norm(mat, ord=\"fro\")\n\n    # Compute the reduced QR decomposition of A.\n    Q, R = np.linalg.qr(A, mode=\"reduced\")\n    Q_k = Q[:, :k]\n    R_k = R[:k, :]\n\n    # Estimate the recovery error of the truncated QR approximation.\n    A_approx = Q_k @ R_k\n    mean_recovery_error = frobenius_norm(A - A_approx) / A.size\n    expected_result = baseline()\n    max_circuit_error = 0.0  # ? We will update this later\n\n    # Decomposed multiplication using the truncated QR factors.\n    def decomposed():\n        return Q_k @ (R_k @ X)\n\n    def bench_qr():\n        product = baseline if k_percent == \"100%\" else decomposed\n        result = product()\n        mean_error = frobenius_norm(result - expected_result) / result.size\n\n        # Update the `max_circuit_error`\n        nonlocal max_circuit_error\n        max_circuit_error = max(max_circuit_error, mean_error)\n        return mean_error\n\n    # Estimate FLOPs:\n    # Full multiplication: A (n_dim x (n_dim+1)) times X ((n_dim+1) x m)\n    full_flops = 2 * n_dim * (n_dim + 1) * m\n    # Decomposed multiplication:\n    # 1. R_k @ X: 2 * k * (n_dim+1) * m flops.\n    # 2. Q_k @ (result): 2 * n_dim * k * m flops.\n    decomposed_flops = 2 * k * (n_dim + 1) * m + 2 * n_dim * k * m\n\n    benchmark(bench_qr)\n    benchmark.extra_info[\"mean_recovery_error\"] = mean_recovery_error\n    benchmark.extra_info[\"flops_reduction\"] = full_flops / decomposed_flops\n\n\n# ? For narrower classes of matrices we can do even better!\n# ? SciPy also provides LU and Cholesky decompositions, but LU only works for square\n# ? matrices, and Cholesky only works for symmetric positive-definite matrices (SPD).\n# ? SPD means that A = A^T and x^T @ A @ x > 0 for all non-zero vectors x.\n\n\n@pytest.mark.benchmark(group=\"decomposition\")\n@pytest.mark.parametrize(\"n_dim\", [1000, 5000])\n@pytest.mark.parametrize(\"k_percent\", [\"100%\", \"20%\", \"4%\"])\n@pytest.mark.parametrize(\"dtype\", [np.float32, np.float64])\ndef test_cholesky(benchmark, n_dim: int, k_percent: float, dtype: np.dtype):\n    k = int(n_dim * int(k_percent[:-1]) / 100)\n    m = 10  # Number of columns in the multiplication circuit\n\n    # ? Generate a random matrix and form a symmetric positive definite matrix.\n    A = np.random.rand(n_dim, n_dim).astype(dtype)\n    A_spd = A @ A.T + np.eye(n_dim).astype(dtype) / 1000\n    X = np.random.rand(n_dim, m).astype(dtype)\n\n    def baseline():\n        return A_spd @ X\n\n    def frobenius_norm(mat: np.ndarray) -> float:\n        return np.linalg.norm(mat, ord=\"fro\")\n\n    #  Compute the Cholesky decomposition of the SPD matrix\n    L = np.linalg.cholesky(A_spd)\n    L_k = L[:, :k]\n    A_approx = L_k @ L_k.T\n\n    mean_recovery_error = frobenius_norm(A_spd - A_approx) / A_spd.size\n    expected_result = baseline()\n    max_circuit_error = 0.0\n\n    def decomposed():\n        return L_k @ (L_k.T @ X)\n\n    def bench():\n        product = baseline if k_percent == \"100%\" else decomposed\n        result = product()\n        mean_error = frobenius_norm(result - expected_result) / result.size\n\n        # Update the `max_circuit_error`\n        nonlocal max_circuit_error\n        max_circuit_error = max(max_circuit_error, mean_error)\n        return mean_error\n\n    # Estimate FLOPs:\n    # Full multiplication: A_spd (n_dim x n_dim) times X (n_dim x m)\n    full_flops = 2 * n_dim * n_dim * m\n    # Decomposed multiplication:\n    # 1. L_k.T @ X: 2 * k * n_dim * m flops.\n    # 2. L_k @ (result): 2 * n_dim * k * m flops.\n    decomposed_flops = 4 * n_dim * k * m\n\n    benchmark(bench)\n    benchmark.extra_info[\"mean_recovery_error\"] = mean_recovery_error\n    benchmark.extra_info[\"flops_reduction\"] = full_flops / decomposed_flops\n\n\n# endregion: Approximations and Benchmarks\n\n# endregion: Numerics\n\n# region: Pipelines and Abstractions\n\n# ? Designing efficient micro-kernels is hard, but composing them into\n# ? high-level pipelines without losing performance is just as difficult.\n# ?\n# ? Consider a hypothetical numeric processing pipeline:\n# ?\n# ?   1. Generate all integers in a given range (e.g., [1, 49]).\n# ?   2. Filter out integers that are perfect squares.\n# ?   3. Expand each remaining number into its prime factors.\n# ?   4. Sum all prime factors from the filtered numbers.\n# ?\n# ? We'll explore four implementations of this pipeline:\n# ?\n# ?  - __Callback-based__ pipeline using lambdas,\n# ?  - __Generators__, `yield`-ing values at each stage,\n# ?  - __Range-based__ pipeline using a custom `PrimeFactors` iterator,\n# ?  - __Polymorphic__ pipeline with a shared base class,\n# ?  - __Async Generators__ with `async for` loops.\n\nPIPE_START = 3\nPIPE_END = 49\n\n\ndef is_power_of_two(x: int) -> bool:\n    \"\"\"Return True if x is a power of two, False otherwise.\"\"\"\n    return x > 0 and (x & (x - 1)) == 0\n\n\ndef is_power_of_three(x: int) -> bool:\n    \"\"\"Return True if x is a power of three, False otherwise.\"\"\"\n    MAX_POWER_OF_THREE = 12157665459056928801\n    return x > 0 and (MAX_POWER_OF_THREE % x == 0)\n\n\n# region: Callbacks\n\nfrom typing import Callable, Tuple  # noqa: E402\n\n\ndef prime_factors_callback(number: int, callback: Callable[[int], None]) -> None:\n    \"\"\"Factorize `number` into primes, invoking `callback(factor)` for each factor.\"\"\"\n    factor = 2\n    while number > 1:\n        if number % factor == 0:\n            callback(factor)\n            number //= factor\n        else:\n            factor += 1 if factor == 2 else 2\n\n\ndef pipeline_callbacks() -> Tuple[int, int]:\n    sum_factors = 0\n    count = 0\n\n    def record_factor(factor: int) -> None:\n        nonlocal sum_factors, count\n        sum_factors += factor\n        count += 1\n\n    for value in range(PIPE_START, PIPE_END + 1):\n        if not is_power_of_two(value) and not is_power_of_three(value):\n            prime_factors_callback(value, record_factor)\n\n    return sum_factors, count\n\n\n# endregion: Callbacks\n\n# region: Generators\nfrom typing import Generator  # noqa: E402\nfrom itertools import chain  # noqa: E402\nfrom functools import reduce  # noqa: E402\n\n\ndef prime_factors_generator(number: int) -> Generator[int, None, None]:\n    \"\"\"Yield prime factors of `number` one by one, lazily.\"\"\"\n    factor = 2\n    while number > 1:\n        if number % factor == 0:\n            yield factor\n            number //= factor\n        else:\n            factor += 1 if factor == 2 else 2\n\n\ndef pipeline_generators() -> Tuple[int, int]:\n    values = range(PIPE_START, PIPE_END + 1)\n    values = filter(lambda x: not is_power_of_two(x), values)\n    values = filter(lambda x: not is_power_of_three(x), values)\n\n    values_factors = map(prime_factors_generator, values)\n    all_factors = chain.from_iterable(values_factors)\n\n    # Use `reduce` to do a single-pass accumulation of (sum, count)\n    sum_factors, count = reduce(\n        lambda acc, factor: (acc[0] + factor, acc[1] + 1),\n        all_factors,\n        (0, 0),\n    )\n    return sum_factors, count\n\n\n# endregion: Generators\n\n# region: Iterators\n\n\nclass PrimeFactors:\n    \"\"\"An iterator to lazily compute the prime factors of a single number.\"\"\"\n\n    def __init__(self, number: int) -> None:\n        self.number = number\n        self.factor = 2\n\n    def __iter__(self) -> \"PrimeFactors\":\n        return self\n\n    def __next__(self) -> int:\n        while self.number > 1:\n            if self.number % self.factor == 0:\n                self.number //= self.factor\n                return self.factor\n            self.factor += 1 if self.factor == 2 else 2\n\n        raise StopIteration\n\n\ndef pipeline_iterators() -> Tuple[int, int]:\n    sum_factors = 0\n    count = 0\n\n    for value in range(PIPE_START, PIPE_END + 1):\n        if not is_power_of_two(value) and not is_power_of_three(value):\n            for factor in PrimeFactors(value):\n                sum_factors += factor\n                count += 1\n\n    return sum_factors, count\n\n\n# endregion: Iterators\n\n# region: Polymorphic\nfrom typing import List  # noqa: E402\nfrom abc import ABC, abstractmethod  # noqa: E402\n\n\nclass PipelineStage(ABC):\n    \"\"\"Base pipeline stage, mimicking a C++-style virtual interface.\"\"\"\n\n    @abstractmethod\n    def process(self, data: List[int]) -> None: ...\n\n\nclass ForRangeStage(PipelineStage):\n    \"\"\"Stage that appends [start..end] to `data`.\"\"\"\n\n    def __init__(self, start: int, end: int) -> None:\n        self.start = start\n        self.end = end\n\n    def process(self, data: List[int]) -> None:\n        data.clear()\n        data.extend(range(self.start, self.end + 1))\n\n\nclass FilterStage(PipelineStage):\n    \"\"\"Stage that filters out values in-place using a predicate.\"\"\"\n\n    def __init__(self, predicate: Callable[[int], bool]) -> None:\n        self.predicate = predicate\n\n    def process(self, data: List[int]) -> None:\n        data[:] = [x for x in data if not self.predicate(x)]\n\n\nclass PrimeFactorsStage(PipelineStage):\n    \"\"\"Stage that expands each value into prime factors, storing them back into data.\"\"\"\n\n    def process(self, data: List[int]) -> None:\n        result = []\n        for val in data:\n            # Use the generator-based prime factors\n            result.extend(PrimeFactors(val))\n        data[:] = result\n\n\ndef pipeline_dynamic_dispatch() -> Tuple[int, int]:\n    pipeline_stages = [\n        ForRangeStage(PIPE_START, PIPE_END),\n        FilterStage(is_power_of_two),\n        FilterStage(is_power_of_three),\n        PrimeFactorsStage(),\n    ]\n\n    data: List[int] = []\n    for stage in pipeline_stages:\n        stage.process(data)\n\n    return sum(data), len(data)\n\n\n# endregion: Polymorphic\n\n# region: Async Generators\n\nimport asyncio  # noqa: E402\nfrom typing import AsyncGenerator  # noqa: E402\n\n\nasync def for_range_async(start: int, end: int) -> AsyncGenerator[int, None]:\n    \"\"\"Async generator that yields [start..end].\"\"\"\n    for value in range(start, end + 1):\n        yield value\n\n\nasync def filter_async(generator, predicate: Callable[[int], bool]):\n    \"\"\"Async generator that forwards `generator` outputs NOT satisfying `predicate`.\"\"\"\n    async for value in generator:\n        if not predicate(value):\n            yield value\n\n\nasync def prime_factors_async(generator):\n    \"\"\"Async generator that yields prime factors for outputs of `generator`.\"\"\"\n    async for val in generator:\n        for factor in prime_factors_generator(val):\n            yield factor\n\n\nasync def pipeline_async() -> Tuple[int, int]:\n    values = for_range_async(PIPE_START, PIPE_END)\n    values = filter_async(values, is_power_of_two)\n    values = filter_async(values, is_power_of_three)\n    values = prime_factors_async(values)\n\n    sum_factors = 0\n    count = 0\n    async for factor in values:\n        sum_factors += factor\n        count += 1\n\n    return sum_factors, count\n\n\n# endregion: Async Generators\n\nPIPE_EXPECTED_SUM = 645  # sum of prime factors from the final data\nPIPE_EXPECTED_COUNT = 84  # total prime factors from the final data\n\n\n@pytest.mark.benchmark(group=\"pipelines\")\ndef test_pipeline_callbacks(benchmark):\n    result = benchmark(pipeline_callbacks)\n    assert result == (PIPE_EXPECTED_SUM, PIPE_EXPECTED_COUNT)\n\n\n@pytest.mark.benchmark(group=\"pipelines\")\ndef test_pipeline_generators(benchmark):\n    result = benchmark(pipeline_generators)\n    assert result == (PIPE_EXPECTED_SUM, PIPE_EXPECTED_COUNT)\n\n\n@pytest.mark.benchmark(group=\"pipelines\")\ndef test_pipeline_iterators(benchmark):\n    result = benchmark(pipeline_iterators)\n    assert result == (PIPE_EXPECTED_SUM, PIPE_EXPECTED_COUNT)\n\n\n@pytest.mark.benchmark(group=\"pipelines\")\ndef test_pipeline_dynamic_dispatch(benchmark):\n    \"\"\"Benchmark the dynamic-dispatch (trait-object) pipeline.\"\"\"\n    result = benchmark(pipeline_dynamic_dispatch)\n    assert result == (PIPE_EXPECTED_SUM, PIPE_EXPECTED_COUNT)\n\n\n@pytest.mark.benchmark(group=\"pipelines\")\ndef test_pipeline_async(benchmark):\n    \"\"\"Benchmark the async-generators pipeline.\"\"\"\n    run_async = lambda: asyncio.run(pipeline_async())  # noqa: E731\n    result = benchmark(run_async)\n    assert result == (PIPE_EXPECTED_SUM, PIPE_EXPECTED_COUNT)\n\n\n# ? The results, as expected, are much slower than in similar pipelines\n# ? written in C++ or Rust. However, the iterators, don't seem like a\n# ? good design choice in Python!\n# ?\n# ?  - Callbacks: 16.8ms\n# ?  - Generators: 23.3ms\n# ?  - Iterators: 31.8ms\n# ?  - Polymorphic: 33.2ms\n# ?  - Async: 97.0ms\n# ?\n# ? For comparison, a fast C++/Rust implementation would take 200ns,\n# ? or __84x__ faster than the fastest Python implementation here.\n\n# endregion: Pipelines and Abstractions\n\n# region: Structures, Tuples, ADTs, AOS, SOA\n\n# region: Composite Structs\n\n# ? Python has many ways of defining composite objects. The most common\n# ? are tuples, dictionaries, named-tuples, dataclasses, and classes.\n# ? Let's compare them by assembling a simple composite of numeric values.\n# ?\n# ? We prefer `float` and `bool` fields as the most predictable Python types,\n# ? as the `int` integers in Python involve a lot of additional logic for\n# ? arbitrary-precision arithmetic, which can affect the latencies.\n\nfrom dataclasses import dataclass  # noqa: E402\nfrom collections import namedtuple  # noqa: E402\n\n\n@pytest.mark.benchmark(group=\"composite-structs\")\ndef test_structs_dict(benchmark):\n    def kernel():\n        point = {\"x\": 1.0, \"y\": 2.0, \"flag\": True}\n        return point[\"x\"] + point[\"y\"]\n\n    result = benchmark(kernel)\n    assert result == 3.0\n\n\n@pytest.mark.benchmark(group=\"composite-structs\")\ndef test_structs_dict_fun(benchmark):\n    def kernel():\n        point = dict(x=1.0, y=2.0, flag=True)\n        return point[\"x\"] + point[\"y\"]\n\n    result = benchmark(kernel)\n    assert result == 3.0\n\n\n# ? Python's dynamic nature allows users to override builtin functions, this means that\n# ? calls to `dict` (or `list`, `tuple`, etc) require a lookup before execution.\n# ? However, that does not apply to `{}` (or `[]`, etc) since it is a language construct\n# ? which users are unable to override, as such, the Python interpreter is free to\n# ? *simply* create the dictionary instead of performing a lookup before.\n# ? This is even clearer if you compare the bytecode for `{\"a\": 1}` vs `dict(a=1)`:\n# ?\n# ? Bytecode for `{\"a\": 1}`:\n# ? ```\n# ? LOAD_CONST               1 ('a')\n# ? LOAD_CONST               2 (1)\n# ? BUILD_MAP                1\n# ? RETURN_VALUE\n# ? ```\n# ?\n# ? Bytecode for `dict(a=1)`\n# ? ```\n# ? LOAD_GLOBAL              1 (dict + NULL)\n# ? LOAD_CONST               1 (1)\n# ? LOAD_CONST               2 (('a',))\n# ? CALL_KW                  1\n# ? RETURN_VALUE\n# ? ```\n# ?\n# ? This results in a measureable difference between the two, running the\n# ? `test_structs_dict` and `test_structs_dict_fun` benchmarks on a Apple M2\n# ? gives us the following results:\n# ?\n# ?  - test_structs_dict (mean): 106 ms\n# ?  - test_structs_dict_fun (mean): 130 ms\n\n\nclass PointClass:\n    def __init__(self, x: float, y: float, flag: bool) -> None:\n        self.x = x\n        self.y = y\n        self.flag = flag\n\n\n@pytest.mark.benchmark(group=\"composite-structs\")\ndef test_structs_class(benchmark):\n    def kernel():\n        point = PointClass(1.0, 2.0, True)\n        return point.x + point.y\n\n    result = benchmark(kernel)\n    assert result == 3.0\n\n\n@dataclass\nclass PointDataclass:\n    x: float\n    y: float\n    flag: bool\n\n\n@pytest.mark.benchmark(group=\"composite-structs\")\ndef test_structs_dataclass(benchmark):\n    def kernel():\n        point = PointDataclass(1.0, 2.0, True)\n        return point.x + point.y\n\n    result = benchmark(kernel)\n    assert result == 3.0\n\n\n@dataclass\nclass PointSlotsDataclass:\n    __slots__ = (\"x\", \"y\", \"flag\")\n    x: float\n    y: float\n    flag: bool\n\n\n@pytest.mark.benchmark(group=\"composite-structs\")\ndef test_structs_slots_dataclass(benchmark):\n    def kernel():\n        point = PointSlotsDataclass(1.0, 2.0, True)\n        return point.x + point.y\n\n    result = benchmark(kernel)\n    assert result == 3.0\n\n\nPointNamedtuple = namedtuple(\"PointNamedtuple\", [\"x\", \"y\", \"flag\"])\n\n\n@pytest.mark.benchmark(group=\"composite-structs\")\ndef test_structs_namedtuple(benchmark):\n    def kernel():\n        point = PointNamedtuple(1.0, 2.0, True)\n        return point.x + point.y\n\n    result = benchmark(kernel)\n    assert result == 3.0\n\n\n@pytest.mark.benchmark(group=\"composite-structs\")\ndef test_structs_tuple_indexing(benchmark):\n    def kernel():\n        point = (1.0, 2.0, True)\n        return point[0] + point[1]\n\n    result = benchmark(kernel)\n    assert result == 3.0\n\n\n@pytest.mark.benchmark(group=\"composite-structs\")\ndef test_structs_tuple_unpacking(benchmark):\n    def kernel():\n        x, y, _ = (1.0, 2.0, True)\n        return x + y\n\n    result = benchmark(kernel)\n    assert result == 3.0\n\n\n# ? Interestingly, the `namedtuple`, that is often believed to be a\n# ? performance-oriented choice, is 50% slower than both `dataclass` and\n# ? the custom class... which are in turn slower than a simple `dict`\n# ? with the same string fields!\n# ?\n# ? - Tuple: 47ns (indexing) vs 43ns (unpacking)\n# ? - Dict:  101ns\n# ? - Slots Dataclass: 112ns\n# ? - Dataclass: 122ns\n# ? - Class: 125ns\n# ? - Namedtuple: 183ns\n# ?\n# ? None of those structures validates the types of the fields, so\n# ? many Python developers resort to external libraries for that.\n# ?\n# ? - pydantic: over 6 Million downloads per day\n# ? - attrs: over 5 Million downloads per day\n\npydantic_installed = False\ntry:\n    from pydantic import BaseModel  # noqa: E402\n\n    pydantic_installed = True\nexcept ImportError:\n    BaseModel = dict\n\n\nfrom attrs import define, field, validators  # noqa: E402\n\n\nclass PointPydantic(BaseModel):\n    x: float\n    y: float\n    flag: bool\n\n\n@pytest.mark.skipif(not pydantic_installed, reason=\"Pydantic not installed\")\n@pytest.mark.benchmark(group=\"composite-structs\")\ndef test_structs_pydantic(benchmark):\n    def kernel():\n        point = PointPydantic(x=1.0, y=2.0, flag=True)\n        return point.x + point.y\n\n    result = benchmark(kernel)\n    assert result == 3.0\n\n\n@define\nclass PointAttrs:\n    x: float = field(validator=validators.instance_of(float))\n    y: float = field(validator=validators.instance_of(float))\n    flag: bool = field(validator=validators.instance_of(bool))\n\n\n@pytest.mark.benchmark(group=\"composite-structs\")\ndef test_structs_attrs(benchmark):\n    def kernel():\n        point = PointAttrs(1.0, 2.0, True)\n        return point.x + point.y\n\n    result = benchmark(kernel)\n    assert result == 3.0\n\n\n# endregion: Composite Structs\n\n# region: Heterogenous Collections\n\n# ? Python is a dynamically typed language, and it allows mixing different\n# ? types in a single collection. However, the performance of such collections\n# ? can vary significantly, depending on the types and their distribution.\n\n\n# endregion: Heterogenous Collections\n\n# region: Tables and Arrays\n\nimport pandas as pd  # noqa: E402\n\ntry:\n    import pyarrow as pa  # noqa: E402\nexcept ImportError:\n    pass\n\n# endregion: Tables and Arrays\n\n# endregion: Structures, Tuples, ADTs, AOS, SOA\n\n# region: Exceptions, Backups, Logging\n\n# region: Errors\n\n# ?  In the real world, control-flow gets messy, as different methods will\n# ?  break in different places. Let's imagine a system, that:\n# ?\n# ?  - Reads an integer from a text file.\n# ?  - Increments it.\n# ?  - Saves it back to the text file.\n# ?\n# ?  As soon as we start dealing with \"external devices\", as opposed to the CPU itself,\n# ?  failures become regular. The file may not exist, the integer may not be a number,\n# ?  the file may be read-only, the disk may be full, the file may be locked, etc.\n\nfail_period_read_integer = 6\nfail_period_convert_to_integer = 11\nfail_period_next_string = 17\nfail_period_write_back = 23\n\n\ndef read_integer_from_file_or_raise(file: str, iteration: int) -> str:\n    # Simulate a file-read failure\n    if iteration % fail_period_read_integer == 0:\n        raise RuntimeError(f\"File read failed at iteration {iteration}\")\n    # Simulate a bad string that cannot be converted\n    if iteration % fail_period_convert_to_integer == 0:\n        return \"abc\"\n    # Otherwise, pretend the file contains \"1\"\n    return \"1\"\n\n\ndef string_to_integer_or_raise(value: str, iteration: int) -> int:\n    try:\n        return int(value)\n    except ValueError:\n        raise ValueError(f\"Conversion failed at iteration {iteration}\")\n\n\ndef integer_to_next_string_or_raise(value: int, iteration: int) -> str:\n    if iteration % fail_period_next_string == 0:\n        raise RuntimeError(f\"Increment failed at iteration {iteration}\")\n    return str(value + 1)\n\n\ndef write_to_file_or_raise(file: str, value: str, iteration: int) -> None:\n    if iteration % fail_period_write_back == 0:\n        raise RuntimeError(f\"File write failed at iteration {iteration}\")\n    # Otherwise, success (do nothing).\n\n\ndef increment_file_or_raise(file: str, iteration: int) -> None:\n    read_value = read_integer_from_file_or_raise(file, iteration)\n    int_value = string_to_integer_or_raise(read_value, iteration)\n    next_value = integer_to_next_string_or_raise(int_value, iteration)\n    write_to_file_or_raise(file, next_value, iteration)\n\n\n@pytest.mark.benchmark(group=\"errors\")\ndef test_errors_raise(benchmark):\n    def runner():\n        file_path = \"test.txt\"\n        iteration = 0\n        for _ in range(1_000):\n            iteration += 1\n            try:\n                increment_file_or_raise(file_path, iteration)\n            except Exception:\n                pass\n\n    benchmark(runner)\n\n\n# ? Now let’s define a simple status-based approach, akin to `std::expected`\n# ? or a custom status enum in C++. It's not a common pattern in Python.\n\nfrom enum import Enum, auto  # noqa: E402\n\n\nclass Status(Enum):\n    SUCCESS = auto()\n    READ_FAILED = auto()\n    CONVERT_FAILED = auto()\n    INCREMENT_FAILED = auto()\n    WRITE_FAILED = auto()\n\n\nclass Expected:\n    \"\"\"\n    A simple 'expected' type in Python.\n    - If success, `error` is None and `value` holds the data.\n    - If error, `error` is a Status, and `value` may be None or partial data.\n    \"\"\"\n\n    __slots__ = (\"value\", \"error\")\n\n    def __init__(self, value=None, error: Status = None):\n        self.value = value\n        self.error = error\n\n    def is_ok(self) -> bool:\n        return self.error is None\n\n\ndef read_integer_from_file_expected(file: str, iteration: int) -> Expected:\n    if iteration % fail_period_read_integer == 0:\n        return Expected(error=Status.READ_FAILED)\n    if iteration % fail_period_convert_to_integer == 0:\n        # Return \"abc\" with success => which triggers the \"convert failed\" later\n        return Expected(value=\"abc\", error=None)\n    # Otherwise, pretend the file contains \"1\"\n    return Expected(value=\"1\", error=None)\n\n\ndef string_to_integer_expected(value: str, iteration: int) -> Expected:\n    if not value.isnumeric():\n        return Expected(error=Status.CONVERT_FAILED)\n    return Expected(value=int(value), error=None)\n\n\ndef integer_to_next_string_expected(value: int, iteration: int) -> Expected:\n    if iteration % fail_period_next_string == 0:\n        return Expected(error=Status.INCREMENT_FAILED)\n    return Expected(value=str(value + 1), error=None)\n\n\ndef write_to_file_expected(file: str, value: str, iteration: int) -> Status:\n    if iteration % fail_period_write_back == 0:\n        return Status.WRITE_FAILED\n    return Status.SUCCESS\n\n\ndef increment_file_expected(file: str, iteration: int) -> Status:\n    res_read = read_integer_from_file_expected(file, iteration)\n    if not res_read.is_ok():\n        return res_read.error\n    res_int = string_to_integer_expected(res_read.value, iteration)\n    if not res_int.is_ok():\n        return res_int.error\n    res_incr = integer_to_next_string_expected(res_int.value, iteration)\n    if not res_incr.is_ok():\n        return res_incr.error\n\n    return write_to_file_expected(file, res_incr.value, iteration)\n\n\n@pytest.mark.benchmark(group=\"errors\")\ndef test_errors_expected(benchmark):\n    def runner():\n        file_path = \"test.txt\"\n        iteration = 0\n        for _ in range(1_000):\n            iteration += 1\n            increment_file_expected(file_path, iteration)\n\n    benchmark(runner)\n\n\n# ? As we know, classes and `__slots__` may add a noticeable overhead.\n# ? So let's explore the less common Go-style approach of returning tuples\n# ? and unpacking them on the fly.\n\nStatusCode = int\nSTATUS_SUCCESS = 0\nSTATUS_READ_FAILED = 1\nSTATUS_CONVERT_FAILED = 2\nSTATUS_INCREMENT_FAILED = 3\nSTATUS_WRITE_FAILED = 4\n\n\ndef read_integer_from_file_status(file: str, iteration: int) -> Tuple[str, StatusCode]:\n    if iteration % fail_period_read_integer == 0:\n        return None, STATUS_READ_FAILED\n    if iteration % fail_period_convert_to_integer == 0:\n        # Return \"abc\" with success => which triggers the \"convert failed\" later\n        return \"abc\", STATUS_SUCCESS\n    # Otherwise, pretend the file contains \"1\"\n    return \"1\", STATUS_SUCCESS\n\n\ndef string_to_integer_status(value: str, iteration: int) -> Tuple[int, StatusCode]:\n    if not value.isnumeric():\n        return None, STATUS_CONVERT_FAILED\n    return int(value), STATUS_SUCCESS\n\n\ndef integer_to_next_string_status(value: int, iteration: int) -> Tuple[str, StatusCode]:\n    if iteration % fail_period_next_string == 0:\n        return None, STATUS_INCREMENT_FAILED\n    return str(value + 1), STATUS_SUCCESS\n\n\ndef write_to_file_status(file: str, value: str, iteration: int) -> Status:\n    if iteration % fail_period_write_back == 0:\n        return STATUS_WRITE_FAILED\n    return STATUS_SUCCESS\n\n\ndef increment_file_status(file: str, iteration: int) -> Status:\n    read_value, read_status = read_integer_from_file_status(file, iteration)\n    if read_status != STATUS_SUCCESS:\n        return read_status\n    int_value, int_status = string_to_integer_status(read_value, iteration)\n    if int_status != STATUS_SUCCESS:\n        return int_status\n    next_value, next_status = integer_to_next_string_status(int_value, iteration)\n    if next_status != STATUS_SUCCESS:\n        return next_status\n    write_status = write_to_file_status(file, next_value, iteration)\n    return write_status\n\n\n@pytest.mark.benchmark(group=\"errors\")\ndef test_errors_status(benchmark):\n    def runner():\n        file_path = \"test.txt\"\n        iteration = 0\n        for _ in range(1_000):\n            iteration += 1\n            increment_file_status(file_path, iteration)\n\n    benchmark(runner)\n\n\n# ? The results are quite interesting! Raising exceptions beats the more\n# ? explicit `Expected` approach by 2x, but loses to tuple-based status\n# ? codes by 50%.\n# ?\n# ? - Raise: 329us\n# ? - Expected: 660us\n# ? - Status: 236us\n# ?\n# ? That difference could grow further we get a `noexcept`-like mechanism\n# ? to annotate functions that never raise exceptions, and need no stack\n# ? tracing logic: https://github.com/python/typing/issues/604\n# ?\n# ? Stick to `tuple`-s with unpacking for the best performance!\n\n# endregion: Errors\n\n# regions: Logs\n\n# endregion: Logs\n\n# endregion: Exceptions, Backups, Logging\n\n# region: Dynamic Code\n\n# region: Reflection, Inspection\n\n# endregion: Reflection, Inspection\n\n# region: Evaluating Strings\n\n# endregion: Evaluating Strings\n\n# endregion: Dynamic Code\n\n# region: Networking and Databases\n\n# ? When implementing web-applications, Python developers often rush to\n# ? use overloaded high-level frameworks, like Django, Flask, or FastAPI,\n# ? without ever considering a lower-level route.\n# ?\n# ? Let's implement a simple \"echo\" client and server using Python's\n# ? built-in `socket` module, and compare its performance with a similar\n# ? implementation in the `asyncio` module and FastAPI.\n\nimport socket  # for TCP and UDP servers # noqa: E402\nimport inspect  # to get the source code of a function # noqa: E402\nimport subprocess  # to start a server in a subprocess # noqa: E402\nimport sys  # to get the Python executable # noqa: E402\nimport time  # sleep for a bit until the socket binds # noqa: E402\nfrom abc import ABC, abstractmethod  # to define abstract classes # noqa: E402\nfrom typing import Literal  # noqa: E402\n\n# ? The User Datagram Protocol (UDP) is OSI Layer 4 \"Transport protocol\", and\n# ? should be able to operate on top of any OSI Layer 3 \"Network protocol\".\n# ?\n# ? In most cases, it operates on top of the Internet Protocol (IP), which can\n# ? have Maximum Transmission Unit (MTU) ranging 20 for IPv4 and 40 for IPv6\n# ? to 65535 bytes. In our case, however, the OSI Layer 2 \"Data Link Layer\" is\n# ? likely to be Ethernet, which has a MTU of 1500 bytes, but most routers are\n# ? configured to fragment packets larger than 1460 bytes. Hence, our choice!\nRPC_MTU = 1460\nRPC_PORT = 12345\nRPC_PACKET_TIMEOUT_SEC = 0.05\nRPC_BATCH_TIMEOUT_SEC = 0.5\n\n\ndef fetch_public_ip() -> str:\n    \"\"\"\n    Returns the 'default' (outbound) IP address of the current machine.\n    Note that this may be a private IP if behind NAT (it won't be your\n    real public-facing IP if you are behind a router/firewall).\n    \"\"\"\n    with socket.socket(socket.AF_INET, socket.SOCK_DGRAM) as s:\n        # The IP/port here doesn't need to be reachable (we never send data);\n        # we just need the OS to pick a default interface for this \"outbound\" connection.\n        s.connect((\"8.8.8.8\", 80))\n        return s.getsockname()[0]\n\n\nclass EchoServer(ABC):\n    \"\"\"Abstract base class for echo servers.\"\"\"\n\n    def __init__(self, host: str = \"0.0.0.0\", port: int = RPC_PORT):\n        \"\"\"\n        :param host: The host to bind the server to. Set to '0.0.0.0' to listen on all\n            interfaces. Set to 'localhost' or '127.0.0.1' to listen on the loopback\n            interface.\n        :param port: The port to bind the server to.\n        \"\"\"\n        self.host = host\n        self.port = port\n\n    @abstractmethod\n    def run(self):\n        \"\"\"Run the echo server (blocking call).\"\"\"\n        pass\n\n\nclass TCPEchoServer(EchoServer):\n    \"\"\"Simple TCP Echo Server.\"\"\"\n\n    def run(self):\n        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as server:\n            server.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n            server.bind((self.host, self.port))\n            server.listen()\n            while True:\n                conn, _ = server.accept()\n                with conn:\n                    while True:\n                        data = conn.recv(RPC_MTU)\n                        if not data:\n                            break\n                        conn.sendall(data)\n\n\nclass UDPEchoServer(EchoServer):\n    \"\"\"Simple UDP Echo Server.\"\"\"\n\n    def run(self):\n        with socket.socket(socket.AF_INET, socket.SOCK_DGRAM) as server:\n            server.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n            server.bind((self.host, self.port))\n            while True:\n                data, addr = server.recvfrom(RPC_MTU)\n                if not data:\n                    break\n                server.sendto(data, addr)\n\n\nclass EchoClient(ABC):\n    \"\"\"Abstract base class for echo clients.\"\"\"\n\n    def __init__(\n        self,\n        host: str = \"localhost\",\n        port: int = RPC_PORT,\n        timeout: float = RPC_PACKET_TIMEOUT_SEC,\n    ):\n        self.host = host\n        self.port = port\n        self.timeout = timeout\n\n    @abstractmethod\n    def connect(self):\n        \"\"\"Establish or prepare the client socket (TCP connect, or just open a UDP socket).\"\"\"\n        pass\n\n    @abstractmethod\n    def send_and_receive(self, data: bytes) -> bytes:\n        \"\"\"Send data and receive its echo.\"\"\"\n        pass\n\n    def send_and_receive_batch(self, messages: List[bytes]) -> List[bytes]:\n        \"\"\"Send a batch of messages and receive their echoes.\"\"\"\n        return [self.send_and_receive(m) for m in messages]\n\n    @abstractmethod\n    def close(self):\n        \"\"\"Close the underlying socket.\"\"\"\n        pass\n\n\nclass TCPEchoClient(EchoClient):\n    \"\"\"TCP Echo Client implementation.\"\"\"\n\n    def __init__(\n        self,\n        host=\"localhost\",\n        port=RPC_PORT,\n        timeout=RPC_PACKET_TIMEOUT_SEC,\n    ):\n        super().__init__(host, port, timeout)\n        self._sock = None\n\n    def connect(self):\n        self._sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        self._sock.settimeout(self.timeout)\n        self._sock.connect((self.host, self.port))\n\n    def send_and_receive(self, data: bytes) -> bytes:\n        self._sock.sendall(data)\n        return self._sock.recv(RPC_MTU)\n\n    def close(self):\n        if self._sock:\n            self._sock.close()\n            self._sock = None\n\n\nclass UDPEchoClient(EchoClient):\n    \"\"\"UDP Echo Client implementation.\"\"\"\n\n    def __init__(\n        self,\n        host=\"localhost\",\n        port=RPC_PORT,\n        timeout=RPC_PACKET_TIMEOUT_SEC,\n    ):\n        super().__init__(host, port, timeout)\n        self._sock = None\n\n    def connect(self):\n        # For UDP, \"connect\" isn't needed\n        self._sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n        self._sock.settimeout(self.timeout)\n\n    def send_and_receive(self, data: bytes) -> bytes:\n        # For UDP, we must specify the address on sendto unless we've \"connected\" the socket.\n        self._sock.sendto(data, (self.host, self.port))\n        resp, _ = self._sock.recvfrom(RPC_MTU)\n        return resp\n\n    def close(self):\n        if self._sock:\n            self._sock.close()\n            self._sock = None\n\n\nclass ServerProcess:\n    \"\"\"\n    Wraps an EchoServer in a subprocess. On __enter__, spawns the server\n    and returns `self`. On __exit__, kills the subprocess.\n    \"\"\"\n\n    def __init__(self, server: EchoServer):\n        self.server = server\n        self._proc = None\n\n    def __enter__(self):\n        source_code = inspect.getsource(self.server.__class__)\n        # We'll also need the base class if the server references it:\n        base_code = inspect.getsource(EchoServer)\n        # Recreate an identical server instance in another process and call run()\n        script = f\"\"\"\nimport socket\nfrom abc import ABC, abstractmethod\n\nRPC_MTU = {RPC_MTU}\nRPC_PORT = {RPC_PORT}\n\n{base_code}\n{source_code}\n\nif __name__ == \"__main__\":\n    server = {self.server.__class__.__name__}(host={self.server.host!r}, port={self.server.port})\n    server.run()\n\"\"\"\n\n        self._proc = subprocess.Popen([sys.executable, \"-c\", script])\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        if self._proc:\n            self._proc.kill()\n            self._proc.wait()\n\n\ndef profile_echo_latency(\n    benchmark,\n    server_class,\n    client_class,\n    packet_length: int = 1024,\n    rounds: int = 100_000,\n    batch_size: int = 1,\n    use_batching: bool = False,\n    route: Literal[\"loopback\", \"public\"] = \"loopback\",\n):\n    \"\"\"\n    A generic echo latency profiler that uses class-based server/client.\n    \"\"\"\n\n    packet = b\"ping\" * (packet_length // 4)\n    address_to_listen = \"127.0.0.1\" if route == \"loopback\" else \"0.0.0.0\"\n    address_to_talk = \"127.0.0.1\" if route == \"loopback\" else fetch_public_ip()\n    lost_packets = 0\n\n    # Initialize server and run in a subprocess context\n    server = server_class(host=address_to_listen)\n    context = ServerProcess(server).__enter__()\n    time.sleep(0.5)  # Short wait to ensure server is listening\n\n    # Create client\n    client = client_class(host=address_to_talk)\n    client.connect()\n\n    # We may want to allow executing the requests within the batch asynchronously\n    send_many: callable = client_class.send_and_receive_batch\n    emulate_sending_many: callable = EchoClient.send_and_receive_batch\n    supports_batching: bool = emulate_sending_many is not send_many\n    packets = [packet] * batch_size\n    if use_batching:\n        assert supports_batching, \"Client does not support batching!\"\n\n    def runner():\n        nonlocal lost_packets\n        try:\n            responses = send_many(client, packets)\n            if any(r != packet for r in responses):\n                raise ValueError(\"Mismatched echo response!\")\n        except socket.timeout:\n            lost_packets += batch_size\n\n    benchmark.pedantic(runner, iterations=1, rounds=rounds)\n    benchmark.extra_info[\"lost_packets\"] = lost_packets\n\n    client.close()\n    context.__exit__(None, None, None)  # kill the server\n\n\n@pytest.mark.benchmark(group=\"echo\")\ndef test_rpc_tcp_loopback(benchmark):\n    profile_echo_latency(benchmark, TCPEchoServer, TCPEchoClient, route=\"loopback\")\n\n\n@pytest.mark.benchmark(group=\"echo\")\ndef test_rpc_udp_loopback(benchmark):\n    profile_echo_latency(benchmark, UDPEchoServer, UDPEchoClient, route=\"loopback\")\n\n\n@pytest.mark.benchmark(group=\"echo\")\ndef test_rpc_tcp_public(benchmark):\n    profile_echo_latency(benchmark, TCPEchoServer, TCPEchoClient, route=\"public\")\n\n\n@pytest.mark.benchmark(group=\"echo\")\ndef test_rpc_udp_public(benchmark):\n    profile_echo_latency(benchmark, UDPEchoServer, UDPEchoClient, route=\"public\")\n\n\n# ? There's a clear difference between sending packets via `127.0.0.1` (loopback)\n# ? versus the machine's \"public\" IP. Loopback is effectively short-circuited in\n# ? software, yielding minimal overhead and tighter latency distributions.\n# ? By contrast, using the \"public\" IP can trigger NAT hairpin or firewall checks,\n# ? resulting in higher average and more variable latency, especially for UDP.\n# ?\n# ? - TCP Loopback: from 11 us to 319 us worst-case, average 18 us\n# ? - TCP Public: from 13 us to 2'773 us worst-case, average 19 us\n# ? - UDP Loopback: from 15 us to 542 us worst-case, average 20 us\n# ? - UDP Public: from 27 us to 4'790 us worst-case, average 34 us\n# ?\n# ? Sounds interesting? I suggest reading\n# ?\n# ? - \"High Performance Browser Networking\" by Ilya Grigorik:\n# ?   https://hpbn.co/\n# ? - \"Moving past TCP in the data center, part 2\" by Jake Edge:\n# ?   https://lwn.net/Articles/914030/\n\n\nclass AsyncioTCPEchoServer(EchoServer):\n    \"\"\"Asyncio-based TCP Echo Server.\"\"\"\n\n    def run(self):\n        import asyncio\n\n        async def handle_echo(reader, writer):\n            while True:\n                data = await reader.read(RPC_MTU)\n                if not data:\n                    break\n                writer.write(data)\n                await writer.drain()\n            writer.close()\n            await writer.wait_closed()\n\n        async def main_loop():\n            server = await asyncio.start_server(handle_echo, self.host, self.port)\n            async with server:\n                # Serve forever (blocking)\n                await server.serve_forever()\n\n        asyncio.run(main_loop())\n\n\nclass AsyncioTCPEchoClient(ABC):\n    \"\"\"\n    Since your framework expects .connect(), .send_and_receive(), .close()\n    in a synchronous style, we internally run an event loop and call asyncio\n    functions with run_until_complete().\n    \"\"\"\n\n    def __init__(self, host=\"localhost\", port=RPC_PORT, timeout=RPC_PACKET_TIMEOUT_SEC):\n        self.host = host\n        self.port = port\n        self.timeout = timeout\n        self._loop = None\n        self._reader = None\n        self._writer = None\n\n    def connect(self):\n        import asyncio\n\n        self._loop = asyncio.new_event_loop()\n\n        async def _connect():\n            reader, writer = await asyncio.open_connection(self.host, self.port)\n            # Optionally, we can set socket timeouts or other config here.\n            return reader, writer\n\n        self._reader, self._writer = self._loop.run_until_complete(_connect())\n\n    def send_and_receive(self, data: bytes) -> bytes:\n        async def _send_and_receive(d):\n            self._writer.write(d)\n            await self._writer.drain()\n            resp = await self._reader.read(RPC_MTU)\n            return resp\n\n        return self._loop.run_until_complete(_send_and_receive(data))\n\n    def send_and_receive_batch(self, messages: List[bytes]) -> List[bytes]:\n        async def _send_and_receive_batch(msgs: List[bytes]):\n            results = []\n            for m in msgs:\n                self._writer.write(m)\n                await self._writer.drain()\n                resp = await self._reader.read(RPC_MTU)\n                results.append(resp)\n            return results\n\n        return self._loop.run_until_complete(_send_and_receive_batch(messages))\n\n    def close(self):\n        async def _close():\n            if self._writer:\n                self._writer.close()\n                await self._writer.wait_closed()\n\n        if self._loop:\n            self._loop.run_until_complete(_close())\n            self._loop.close()\n            self._loop = None\n\n\n@pytest.mark.benchmark(group=\"echo\")\ndef test_batch16_rpc_asyncio_ordered(benchmark):\n    profile_echo_latency(\n        benchmark,\n        AsyncioTCPEchoServer,\n        AsyncioTCPEchoClient,\n        route=\"loopback\",\n        batch_size=16,\n        use_batching=True,\n        rounds=1_000,\n    )\n\n\n@pytest.mark.benchmark(group=\"echo\")\ndef test_batch16_rpc_asyncio_unordered(benchmark):\n    profile_echo_latency(\n        benchmark,\n        AsyncioTCPEchoServer,\n        AsyncioTCPEchoClient,\n        route=\"loopback\",\n        batch_size=16,\n        use_batching=True,\n        rounds=1_000,\n    )\n\n\n# ? The results are unsettling. The promise of `asyncio` is to provide a\n# ? high-performance, non-blocking I/O framework. However, the overhead\n# ? of the event loop, the context switches, and the additional buffering\n# ? can make it slower than the synchronous TCP client per call.\n# ?\n# ? For 16 calls in a batch, using the 'loopback' interface, the latency is:\n# ? - Asyncio Ordered: from 579 us to 2'909 us worst-case, average 627 us\n# ? - Asyncio Unordered: from 582 us to 2'598 us worst-case, average 631 us\n# ?\n# ? First, we don't see a significant improvement in latency when allowing\n# ? out-of-order processing. Second, when normalizing throughput, the\n# ? original blocking TCP client ends up being faster:\n# ?\n# ? - Asyncio Ordered: average 39 us\n# ? - Asyncio Unordered: average 39 us\n# ? - TCP Loopback: average 18 us\n# ?\n# ? This, however, may not be as bad as higher-level frameworks like FastAPI,\n# ? and one of the most common underlying ASGI servers, Uvicorn.\n\n\nclass FastAPIEchoServer(EchoServer):\n    \"\"\"\n    Minimal FastAPI-based HTTP echo server. It exposes a POST /echo endpoint\n    that simply returns the raw request body as-is (using a binary media type).\n    \"\"\"\n\n    def run(self):\n        import uvicorn\n        from fastapi import FastAPI, Request\n        from fastapi.responses import Response\n\n        app = FastAPI()\n\n        @app.post(\"/echo\")\n        async def echo_endpoint(req: Request):\n            data = await req.body()\n            return Response(content=data, media_type=\"application/octet-stream\")\n\n        uvicorn.run(app, host=self.host, port=self.port, log_level=\"error\")\n\n\nclass UvicornEchoServer(EchoServer):\n    \"\"\"\n    Minimal raw ASGI echo server on /echo. No FastAPI or Starlette, just\n    uvicorn + a single scope check for POST /echo. Returns the request\n    body verbatim with content-type=application/octet-stream.\n    \"\"\"\n\n    def run(self):\n        import uvicorn\n\n        async def app(scope, receive, send):\n            if scope[\"type\"] == \"http\":\n                # Check path; if not /echo, return 404\n                if scope.get(\"path\", \"\") != \"/echo\":\n                    await send(\n                        {\"type\": \"http.response.start\", \"status\": 404, \"headers\": []}\n                    )\n                    await send(\n                        {\n                            \"type\": \"http.response.body\",\n                            \"body\": b\"Not Found\",\n                            \"more_body\": False,\n                        }\n                    )\n                    return\n\n                body = b\"\"\n                more_body = True\n                while more_body:\n                    event = await receive()\n                    if event[\"type\"] == \"http.request\":\n                        body += event.get(\"body\", b\"\")\n                        more_body = event.get(\"more_body\", False)\n\n                # Echo the body\n                await send(\n                    {\n                        \"type\": \"http.response.start\",\n                        \"status\": 200,\n                        \"headers\": [\n                            (b\"content-type\", b\"application/octet-stream\"),\n                        ],\n                    }\n                )\n                await send(\n                    {\n                        \"type\": \"http.response.body\",\n                        \"body\": body,\n                        \"more_body\": False,\n                    }\n                )\n\n        uvicorn.run(app, host=self.host, port=self.port, log_level=\"error\")\n\n\nclass RequestsClient(EchoClient):\n    \"\"\"\n    A simple requests-based client, calling POST /echo with the raw data in the\n    request body, and returning the response body as bytes.\n    \"\"\"\n\n    def __init__(self, host=\"localhost\", port=RPC_PORT, timeout=RPC_PACKET_TIMEOUT_SEC):\n        super().__init__(host, port, timeout)\n        self._session = None\n\n    def connect(self):\n        import requests\n\n        self._session = requests.Session()\n        self._session.headers.update({\"Content-Type\": \"application/octet-stream\"})\n\n    def send_and_receive(self, data: bytes) -> bytes:\n        url = f\"http://{self.host}:{self.port}/echo\"\n        resp = self._session.post(url, data=data, timeout=self.timeout)\n        resp.raise_for_status()\n        return resp.content\n\n    def close(self):\n        if self._session:\n            self._session.close()\n            self._session = None\n\n\nclass HTTPXAsyncEchoClient(EchoClient):\n    \"\"\"\n    Uses the httpx library in async mode to talk to the /echo endpoint.\n    Batching is done concurrently with asyncio.gather.\n    \"\"\"\n\n    def __init__(self, host=\"localhost\", port=RPC_PORT, timeout=RPC_PACKET_TIMEOUT_SEC):\n        super().__init__(host, port, timeout)\n        self._loop = None\n        self._client = None\n\n    def connect(self):\n        import httpx\n        import asyncio\n\n        # We'll create a dedicated event loop for this client and\n        # instantiate the AsyncClient inside it.\n        self._loop = asyncio.new_event_loop()\n\n        async def _setup():\n            # Create an AsyncClient with the given timeout and\n            # set headers for sending binary data.\n            client = httpx.AsyncClient(timeout=self.timeout)\n            client.headers.update({\"Content-Type\": \"application/octet-stream\"})\n            return client\n\n        self._client = self._loop.run_until_complete(_setup())\n\n    def send_and_receive(self, data: bytes) -> bytes:\n        \"\"\"\n        Sends a single request and awaits the response using AsyncClient.\n        We wrap it in run_until_complete() for synchronous code compatibility.\n        \"\"\"\n\n        async def _send_and_receive(d):\n            url = f\"http://{self.host}:{self.port}/echo\"\n            resp = await self._client.post(url, content=d)\n            resp.raise_for_status()\n            return resp.content\n\n        return self._loop.run_until_complete(_send_and_receive(data))\n\n    def send_and_receive_batch(self, messages: List[bytes]) -> List[bytes]:\n        \"\"\"\n        Demonstrates concurrent batch logic using asyncio.gather.\n        All requests are fired off in parallel, then we await all responses.\n        \"\"\"\n        import asyncio\n\n        async def _send_and_receive_batch(msgs: List[bytes]) -> List[bytes]:\n            url = f\"http://{self.host}:{self.port}/echo\"\n\n            # Build a coroutine for each message\n            async def post(msg: bytes):\n                resp = await self._client.post(url, content=msg)\n                resp.raise_for_status()\n                return resp.content\n\n            # Fire them off concurrently\n            tasks = [post(m) for m in msgs]\n            results = await asyncio.gather(*tasks)\n            return list(results)\n\n        return self._loop.run_until_complete(_send_and_receive_batch(messages))\n\n    def close(self):\n        \"\"\"\n        Closes the AsyncClient and event loop.\n        \"\"\"\n        import asyncio\n\n        async def _close():\n            if self._client:\n                await self._client.aclose()\n\n        if self._loop:\n            self._loop.run_until_complete(_close())\n            self._loop.close()\n            self._loop = None\n\n\n@pytest.mark.benchmark(group=\"echo\")\ndef test_batch16_rpc_fastapi_requests(benchmark):\n    profile_echo_latency(\n        benchmark,\n        FastAPIEchoServer,\n        RequestsClient,\n        route=\"loopback\",\n        batch_size=16,\n        use_batching=False,  # ! Requests are typically synchronous\n        rounds=1_000,\n    )\n\n\n@pytest.mark.benchmark(group=\"echo\")\ndef test_batch16_rpc_fastapi_httpx(benchmark):\n    profile_echo_latency(\n        benchmark,\n        FastAPIEchoServer,\n        HTTPXAsyncEchoClient,\n        route=\"loopback\",\n        batch_size=16,\n        use_batching=True,\n        rounds=1_000,\n    )\n\n\n@pytest.mark.benchmark(group=\"echo\")\ndef test_batch16_rpc_uvicorn_requests(benchmark):\n    profile_echo_latency(\n        benchmark,\n        FastAPIEchoServer,\n        RequestsClient,\n        route=\"loopback\",\n        batch_size=16,\n        use_batching=False,  # ! Requests are typically synchronous\n        rounds=1_000,\n    )\n\n\n@pytest.mark.benchmark(group=\"echo\")\ndef test_batch16_rpc_uvicorn_httpx(benchmark):\n    profile_echo_latency(\n        benchmark,\n        FastAPIEchoServer,\n        HTTPXAsyncEchoClient,\n        route=\"loopback\",\n        batch_size=16,\n        use_batching=True,\n        rounds=1_000,\n    )\n\n\n# ? The benchmark results are striking. For batch sizes of 16 messages:\n# ?\n# ? - Raw TCP with asyncio: 0.95 milliseconds per batch (59 us per message)\n# ? - Requests+FastAPI/Uvicorn: 7.7 milliseconds per batch (0.5 ms per message)\n# ? - Async HTTPX+FastAPI/Uvicorn: 12.5 milliseconds per batch (0.8 ms per message)\n# ?\n# ? This demonstrates why low-latency systems often avoid HTTP and high-level\n# ? frameworks in favor of raw TCP/UDP, especially for internal services. The\n# ? arguable convenience of FastAPI comes at a significant performance cost -\n# ? about 10x slower than already slow IO stack of Python.\n\n# endregion: Networking and Databases\n"}
