{"repo_info": {"repo_name": "podcastfy", "repo_owner": "souzatharsis", "repo_url": "https://github.com/souzatharsis/podcastfy"}}
{"type": "test_file", "path": "tests/__init__.py", "content": "# This file can be left empty\n"}
{"type": "test_file", "path": "tests/test_content_parser.py", "content": "import unittest\nimport pytest\nfrom podcastfy.utils.config import load_config\nfrom podcastfy.content_parser.content_extractor import ContentExtractor\nfrom podcastfy.content_parser.youtube_transcriber import YouTubeTranscriber\nfrom podcastfy.content_parser.website_extractor import WebsiteExtractor\nfrom podcastfy.content_parser.pdf_extractor import PDFExtractor\n\n\nclass TestContentParser(unittest.TestCase):\n    def test_content_extractor(self):\n        # Add tests for ContentExtractor\n        pass\n\n    @pytest.mark.skip(\n        reason=\"IP getting blocked by YouTube when running from GitHub Actions\"\n    )\n    def test_youtube_transcriber(self):\n        \"\"\"\n        Test the YouTubeTranscriber class to ensure it correctly extracts and cleans transcripts from a YouTube video.\n        \"\"\"\n        # Initialize YouTubeTranscriber\n        transcriber = YouTubeTranscriber()\n\n        # Test URL\n        test_url = \"https://www.youtube.com/watch?v=m3kJo5kEzTQ\"\n\n        # Extract transcript\n        extracted_transcript = transcriber.extract_transcript(test_url)\n\n        # Load expected transcript from youtube.txt file\n        with open(\"./tests/data/mock/youtube.txt\", \"r\") as f:\n            expected_transcript = f.read()\n\n        # Assert that the first 100 characters of the extracted transcript match the expected transcript\n        self.assertEqual(\n            extracted_transcript[:100].strip(), expected_transcript[:100].strip()\n        )\n\n    def test_website_extractor(self):\n        \"\"\"\n        Test the WebsiteExtractor class to ensure it correctly extracts content from a website.\n        \"\"\"\n        # pass #TODO remove pass when testing. Keeping it here to avoid running out of quota.\n\n        # Initialize WebsiteExtractor\n        config = load_config()\n        extractor = WebsiteExtractor()\n\n        # Test URL\n        test_url = \"http://www.souzatharsis.com\"\n\n        # Extract content\n        extracted_content = extractor.extract_content(test_url)\n        print(extracted_content.strip())\n        # Load expected content from website.md file\n        with open(\"./tests/data/mock/website.md\", \"r\") as f:\n            expected_content = f.read()\n        print(expected_content.strip())\n        # Assert that the extracted content matches the expected content\n        self.assertEqual(extracted_content.strip(), expected_content.strip())\n\n    def test_pdf_extractor(self):\n        \"\"\"\n        Test the PDFExtractor class to ensure it correctly extracts content from a PDF file.\n        \"\"\"\n        # Initialize PDFExtractor\n        extractor = PDFExtractor()\n\n        # Path to the test PDF file\n        pdf_path = \"./tests/data/pdf/file.pdf\"\n\n        # Extract content from PDF\n        extracted_content = extractor.extract_content(pdf_path)\n\n        # Load expected content from file.txt\n        with open(\"./tests/data/mock/file.txt\", \"r\") as f:\n            expected_content = f.read()\n\n        # Assert that the first 500 characters of the extracted content match the expected content\n        self.assertEqual(\n            extracted_content[:500].strip(), expected_content[:500].strip()\n        )\n\n    @pytest.mark.skip(reason=\"Too expensive to be auto tested on Github Actions\")\n    def test_generate_topic_content(self):\n        \"\"\"Test generating content for a specific topic.\"\"\"\n        extractor = ContentExtractor()\n        topic = \"Latest news about OpenAI\"\n\n        # Generate content for the topic\n        content = extractor.generate_topic_content(topic)\n\n        # Verify the content\n        self.assertIsNotNone(content)\n        self.assertIsInstance(content, str)\n        self.assertGreater(len(content), 100)  # Content should be substantial\n\n        # Check if content is relevant to the topic\n        lower_content = content.lower()\n        self.assertTrue(\n            any(term in lower_content for term in [\"openai\"]),\n            \"Generated content should be relevant to the topic\",\n        )\n\n\nif __name__ == \"__main__\":\n    unittest.main()\n"}
{"type": "test_file", "path": "tests/test_api.py", "content": "import os\nimport pytest\nfrom podcastfy.api.fast_app import app\nfrom httpx import WSGITransport\nfrom fastapi.testclient import TestClient\n\nclient = TestClient(app, transport=WSGITransport(app=app))\n\n@pytest.fixture\ndef sample_config():\n    return {\n        \"generate_podcast\": True,\n        \"urls\": [\"https://www.phenomenalworld.org/interviews/swap-structure/\"],\n        \"name\": \"Central Clearing Risks\",\n        \"tagline\": \"Exploring the complexities of financial systemic risk\",\n        \"creativity\": 0.8,\n        \"conversation_style\": [\"engaging\", \"informative\"],\n        \"roles_person1\": \"main summarizer\",\n        \"roles_person2\": \"questioner\",\n        \"dialogue_structure\": [\"Introduction\", \"Content\", \"Conclusion\"],\n        \"tts_model\": \"edge\",\n        \"is_long_form\": False,\n        \"engagement_techniques\": [\"questions\", \"examples\", \"analogies\"],\n        \"user_instructions\": \"Don't use the word Dwelve\",\n        \"output_language\": \"English\"\n    }\n\n@pytest.mark.skip(reason=\"Trying to understand if other tests are passing\")\ndef test_generate_podcast_with_edge_tts(sample_config):\n    response = client.post(\"/generate\", json=sample_config)\n    assert response.status_code == 200\n    assert \"audioUrl\" in response.json()\n    assert response.json()[\"audioUrl\"].startswith(\"http://testserver\")\n\ndef test_healthcheck():\n    response = client.get(\"/health\")\n    assert response.status_code == 200\n    assert response.json() == {\"status\": \"healthy\"}\n\n\nif __name__ == \"__main__\":\n    pytest.main()"}
{"type": "test_file", "path": "tests/test_genai_podcast.py", "content": "import unittest\nimport pytest\nfrom unittest.mock import patch, MagicMock\nimport tempfile\nimport os\nfrom podcastfy.content_generator import ContentGenerator\nfrom podcastfy.utils.config import Config\nfrom podcastfy.utils.config_conversation import ConversationConfig\nfrom podcastfy.content_parser.pdf_extractor import PDFExtractor\nfrom podcastfy.content_parser.content_extractor import ContentExtractor\n\n\nMOCK_IMAGE_PATHS = [\n    \"https://raw.githubusercontent.com/souzatharsis/podcastfy/refs/heads/main/data/images/Senecio.jpeg\",\n    \"https://raw.githubusercontent.com/souzatharsis/podcastfy/refs/heads/main/data/images/connection.jpg\",\n]\n\nMODEL_NAME = \"gemini-1.5-pro-latest\"\nAPI_KEY_LABEL = \"GEMINI_API_KEY\"\n\n\n# TODO: Should be a fixture\ndef sample_conversation_config():\n    conversation_config = {\n        \"word_count\": 500,\n        \"roles_person1\": \"professor\",\n        \"roles_person2\": \"student\",\n        \"podcast_name\": \"Teachfy\",\n        \"podcast_tagline\": \"Learning Through Conversation\",\n    }\n    return conversation_config\n\n\nclass TestGenAIPodcast(unittest.TestCase):\n    def setUp(self):\n        \"\"\"\n        Set up the test environment.\n        \"\"\"\n        config = Config()\n        self.api_key = config.GEMINI_API_KEY\n        self.config = config\n\n    def test_generate_qa_content(self):\n        \"\"\"\n        Test the generate_qa_content method of ContentGenerator.\n        \"\"\"\n        content_generator = ContentGenerator(model_name=MODEL_NAME, api_key_label=API_KEY_LABEL)\n        input_text = \"United States of America\"\n        result = content_generator.generate_qa_content(input_text)\n        self.assertIsNotNone(result)\n        self.assertNotEqual(result, \"\")\n        self.assertIsInstance(result, str)\n\n    def test_custom_conversation_config(self):\n        \"\"\"\n        Test the generation of content using a custom conversation configuration file.\n        \"\"\"\n        conversation_config = sample_conversation_config()\n        content_generator = ContentGenerator(model_name=MODEL_NAME, api_key_label=API_KEY_LABEL, conversation_config=conversation_config)\n        input_text = \"United States of America\"\n\n        result = content_generator.generate_qa_content(input_text)\n\n        self.assertIsNotNone(result)\n        self.assertNotEqual(result, \"\")\n        self.assertIsInstance(result, str)\n\n        # Check for elements from the custom config\n        self.assertIn(conversation_config[\"podcast_name\"].lower(), result.lower())\n        self.assertIn(conversation_config[\"podcast_tagline\"].lower(), result.lower())\n\n    def test_generate_qa_content_from_images(self):\n        \"\"\"Test generating Q&A content from two input images.\"\"\"\n        image_paths = MOCK_IMAGE_PATHS\n\n        content_generator = ContentGenerator(model_name=MODEL_NAME, api_key_label=API_KEY_LABEL)\n\n        with tempfile.NamedTemporaryFile(\n            mode=\"w+\", suffix=\".txt\", delete=False\n        ) as temp_file:\n            result = content_generator.generate_qa_content(\n                input_texts=\"\",  # Empty string for input_texts\n                image_file_paths=image_paths,\n                output_filepath=temp_file.name,\n            )\n\n        self.assertIsNotNone(result)\n        self.assertNotEqual(result, \"\")\n        self.assertIsInstance(result, str)\n\n        # Check if the output file was created and contains the same content\n        with open(temp_file.name, \"r\") as f:\n            file_content = f.read()\n\n        self.assertEqual(result, file_content)\n\n        # Clean up the temporary file\n        os.unlink(temp_file.name)\n\n    def test_generate_qa_content_from_pdf(self):\n        \"\"\"Test generating Q&A content from a PDF file.\"\"\"\n        pdf_file = \"tests/data/pdf/file.pdf\"\n        content_generator = ContentGenerator(model_name=MODEL_NAME, api_key_label=API_KEY_LABEL)\n        pdf_extractor = PDFExtractor()\n\n        # Extract content from the PDF file\n        extracted_content = pdf_extractor.extract_content(pdf_file)\n\n        # Generate Q&A content from the extracted text\n        result = content_generator.generate_qa_content(input_texts=extracted_content)\n\n        self.assertIsNotNone(result)\n        self.assertNotEqual(result, \"\")\n        self.assertIsInstance(result, str)\n\n    def test_generate_qa_content_from_raw_text(self):\n        \"\"\"Test generating Q&A content from raw input text.\"\"\"\n        raw_text = \"The wonderful world of LLMs.\"\n        content_generator = ContentGenerator(model_name=MODEL_NAME, api_key_label=API_KEY_LABEL)\n\n        result = content_generator.generate_qa_content(input_texts=raw_text)\n\n        self.assertIsNotNone(result)\n        self.assertNotEqual(result, \"\")\n        self.assertIsInstance(result, str)\n\n    @pytest.mark.skip(reason=\"Too expensive to be auto tested on Github Actions\")\n    def test_generate_qa_content_from_topic(self):\n        \"\"\"Test generating Q&A content from a specific topic.\"\"\"\n        topic = \"Latest news about OpenAI\"\n        content_generator = ContentGenerator(model_name=MODEL_NAME, api_key_label=API_KEY_LABEL)\n        extractor = ContentExtractor()\n        topic = \"Latest news about OpenAI\"\n\n        # Generate content for the topic\n        content = extractor.generate_topic_content(topic)\n\n        result = content_generator.generate_qa_content(input_texts=content)\n\n        self.assertIsNotNone(result)\n        self.assertNotEqual(result, \"\")\n        self.assertIsInstance(result, str)\n\n        # Verify Q&A format\n        self.assertIn(\"<Person1>\", result)\n        self.assertIn(\"<Person2>\", result)\n\n        # Verify content relevance\n        lower_result = result.lower()\n        self.assertTrue(\n            any(term in lower_result for term in [\"openai\"]),\n            \"Generated content should be relevant to the topic\",\n        )\n\n\nif __name__ == \"__main__\":\n    unittest.main()\n"}
{"type": "test_file", "path": "tests/test_generate_podcast.py", "content": "import os\nimport pytest\nimport tempfile\nfrom podcastfy.client import generate_podcast\nfrom podcastfy.utils.config import load_config\nfrom podcastfy.utils.config_conversation import load_conversation_config\n\n\nTEST_URL = \"https://en.wikipedia.org/wiki/Friends\"\nMOCK_IMAGE_PATHS = [\n    \"https://raw.githubusercontent.com/souzatharsis/podcastfy/refs/heads/main/data/images/Senecio.jpeg\",\n    \"https://raw.githubusercontent.com/souzatharsis/podcastfy/refs/heads/main/data/images/connection.jpg\",\n]\n\n\n@pytest.fixture\ndef sample_config():\n    config = load_config()\n    return config\n\n\n@pytest.fixture\ndef default_conversation_config():\n    config = load_conversation_config()\n    return config\n\n\n@pytest.fixture\ndef sample_conversation_config():\n    \"\"\"\n    Fixture to provide a sample conversation configuration for testing.\n\n    Returns:\n            dict: A dictionary containing sample conversation configuration parameters.\n    \"\"\"\n    conversation_config = {\n        \"word_count\": 300,\n        \"conversation_style\": [\"formal\", \"educational\"],\n        \"roles_person1\": \"professor\",\n        \"roles_person2\": \"student\",\n        \"dialogue_structure\": [\n            \"Introduction\",\n            \"Main Points\",\n            \"Case Studies\",\n            \"Quiz\",\n            \"Conclusion\",\n        ],\n        \"podcast_name\": \"Teachfy\",\n        \"podcast_tagline\": \"Learning Through Conversation\",\n        \"output_language\": \"English\",\n        \"engagement_techniques\": [\"examples\", \"questions\"],\n        \"creativity\": 0,\n        \"text_to_speech\": {\n            \"output_directories\": {\n                \"transcripts\": \"tests/data/transcriptsTEST\",\n                \"audio\": \"tests/data/audioTEST\",\n            },\n            \"temp_audio_dir\": \"tests/data/audio/tmpTEST/\",\n            \"ending_message\": \"Bye Bye!\",\n        },\n    }\n    return conversation_config\n\n\n@pytest.fixture(autouse=True)\ndef setup_test_directories(sample_conversation_config):\n    \"\"\"Create test directories if they don't exist.\"\"\"\n    output_dirs = sample_conversation_config.get(\"text_to_speech\", {}).get(\n        \"output_directories\", {}\n    )\n    for directory in output_dirs.values():\n        os.makedirs(directory, exist_ok=True)\n    temp_dir = sample_conversation_config.get(\"text_to_speech\", {}).get(\n        \"temp_audio_dir\"\n    )\n    if temp_dir:\n        os.makedirs(temp_dir, exist_ok=True)\n\n\n@pytest.mark.skip(reason=\"Testing edge only on Github Action as it's free\")\ndef test_generate_podcast_from_urls_11labs(default_conversation_config):\n    \"\"\"Test generating a podcast from a list of URLs.\"\"\"\n    urls = [TEST_URL]\n\n    audio_file = generate_podcast(urls=urls, tts_model=\"elevenlabs\")\n    print(f\"Audio file generated using ElevenLabs model: {audio_file}\")\n    assert audio_file is not None\n    assert os.path.exists(audio_file)\n    assert audio_file.endswith(\".mp3\")\n    assert os.path.getsize(audio_file) > 1024  # Check if larger than 1KB\n    assert os.path.dirname(audio_file) == default_conversation_config.get(\n        \"text_to_speech\", {}\n    ).get(\"output_directories\", {}).get(\"audio\")\n\n\n@pytest.mark.skip(reason=\"Testing edge only on Github Action as it's free\")\ndef test_generate_podcast_from_urls_openai(default_conversation_config):\n    \"\"\"Test generating a podcast from a list of URLs.\"\"\"\n    urls = [\n        TEST_URL,\n    ]\n\n    audio_file = generate_podcast(urls=urls, tts_model=\"openai\")\n    print(f\"Audio file generated using OpenAI model: {audio_file}\")\n\n    assert audio_file is not None\n    assert os.path.exists(audio_file)\n    assert audio_file.endswith(\".mp3\")\n    assert os.path.getsize(audio_file) > 1024  # Check if larger than 1KB\n    assert os.path.dirname(audio_file) == default_conversation_config.get(\n        \"text_to_speech\", {}\n    ).get(\"output_directories\", {}).get(\"audio\")\n\n\n@pytest.mark.skip(reason=\"Testing edge only on Github Action as it's free\")\ndef test_generate_podcast_from_urls_gemini(default_conversation_config):\n    \"\"\"Test generating a podcast from a list of URLs.\"\"\"\n    urls = [\n        TEST_URL,\n    ]\n\n    audio_file = generate_podcast(urls=urls, tts_model=\"gemini\")\n    print(f\"Audio file generated using Gemini model: {audio_file}\")\n\n    assert audio_file is not None\n    assert os.path.exists(audio_file)\n    assert audio_file.endswith(\".mp3\")\n    assert os.path.getsize(audio_file) > 1024  # Check if larger than 1KB\n    assert os.path.dirname(audio_file) == default_conversation_config.get(\n        \"text_to_speech\", {}\n    ).get(\"output_directories\", {}).get(\"audio\")\n\n\ndef test_generate_podcast_from_urls_edge(default_conversation_config):\n    \"\"\"Test generating a podcast from a list of URLs.\"\"\"\n    urls = [TEST_URL]\n\n    audio_file = generate_podcast(urls=urls, tts_model=\"edge\")\n    print(f\"Audio file generated using Edge model: {audio_file}\")\n    assert audio_file is not None\n    assert os.path.exists(audio_file)\n    assert audio_file.endswith(\".mp3\")\n    assert os.path.getsize(audio_file) > 1024  # Check if larger than 1KB\n    assert os.path.dirname(audio_file) == default_conversation_config.get(\n        \"text_to_speech\", {}\n    ).get(\"output_directories\", {}).get(\"audio\")\n\n\ndef test_generate_transcript_only(default_conversation_config):\n    \"\"\"Test generating only a transcript without audio.\"\"\"\n    urls = [TEST_URL]\n\n    result = generate_podcast(urls=urls, transcript_only=True)\n    print(f\"Transcript file generated: {result}\")\n\n    assert result is not None\n    assert os.path.exists(result)\n    assert result.endswith(\".txt\")\n    assert os.path.dirname(result) == default_conversation_config.get(\n        \"text_to_speech\", {}\n    ).get(\"output_directories\", {}).get(\"transcripts\")\n\n\ndef test_generate_podcast_from_transcript_file(sample_conversation_config):\n    \"\"\"Test generating a podcast from an existing transcript file.\"\"\"\n    # First, generate a transcript\n    transcript_file = os.path.join(\n        sample_conversation_config.get(\"text_to_speech\", {})\n        .get(\"output_directories\", {})\n        .get(\"transcripts\"),\n        \"test_transcript.txt\",\n    )\n    with open(transcript_file, \"w\") as f:\n        f.write(\n            \"<Person1>Joe Biden and the US Politics</Person1><Person2>Joe Biden is the current president of the United States of America</Person2>\"\n        )\n\n    # Now use this transcript to generate a podcast\n    audio_file = generate_podcast(\n        transcript_file=transcript_file,\n        tts_model=\"edge\",\n        conversation_config=sample_conversation_config,\n    )\n\n    assert audio_file is not None\n    assert os.path.exists(audio_file)\n    assert audio_file.endswith(\".mp3\")\n    assert os.path.getsize(audio_file) > 1024  # Check if larger than 1KB\n    assert os.path.dirname(audio_file) == sample_conversation_config.get(\n        \"text_to_speech\", {}\n    ).get(\"output_directories\", {}).get(\"audio\")\n\n\ndef test_generate_podcast_with_custom_config(sample_config, sample_conversation_config):\n    \"\"\"Test generating a podcast with a custom conversation config.\"\"\"\n    urls = [\"https://en.wikipedia.org/wiki/Artificial_intelligence\"]\n\n    audio_file = generate_podcast(\n        urls=urls,\n        config=sample_config,\n        conversation_config=sample_conversation_config,\n        tts_model=\"edge\",\n    )\n\n    assert audio_file is not None\n    assert os.path.exists(audio_file)\n    assert audio_file.endswith(\".mp3\")\n    assert os.path.getsize(audio_file) > 1024  # Check if larger than 1KB\n    assert (\n        os.path.dirname(audio_file)\n        == sample_conversation_config[\"text_to_speech\"][\"output_directories\"][\"audio\"]\n    )\n\n\ndef test_generate_from_local_pdf(sample_config):\n    \"\"\"Test generating a podcast from a local PDF file.\"\"\"\n    pdf_file = \"tests/data/pdf/file.pdf\"\n    audio_file = generate_podcast(\n        urls=[pdf_file], config=sample_config, tts_model=\"edge\"\n    )\n    assert audio_file is not None\n    assert os.path.exists(audio_file)\n    assert audio_file.endswith(\".mp3\")\n    assert os.path.getsize(audio_file) > 1024  # Check if larger than 1KB\n\n@pytest.mark.skip(reason=\"Testing edge only on Github Action as it's free\")\ndef test_generate_from_local_pdf_multispeaker(sample_config):\n    \"\"\"Test generating a podcast from a local PDF file.\"\"\"\n    pdf_file = \"tests/data/pdf/file.pdf\"\n    audio_file = generate_podcast(\n        urls=[pdf_file], config=sample_config, tts_model=\"geminimulti\"\n    )\n    assert audio_file is not None\n    assert os.path.exists(audio_file)\n    assert audio_file.endswith(\".mp3\")\n    assert os.path.getsize(audio_file) > 1024  # Check if larger than 1KB\n\n@pytest.mark.skip(reason=\"Testing edge only on Github Action as it's free\")\ndef test_generate_from_local_pdf_multispeaker_longform(sample_config):\n    \"\"\"Test generating a podcast from a local PDF file.\"\"\"\n    pdf_file = \"tests/data/pdf/file.pdf\"\n    audio_file = generate_podcast(\n        urls=[pdf_file], config=sample_config, tts_model=\"geminimulti\", longform=True\n    )\n    assert audio_file is not None\n    assert os.path.exists(audio_file)\n    assert audio_file.endswith(\".mp3\")\n    assert os.path.getsize(audio_file) > 1024  # Check if larger than 1KB\n\ndef test_generate_podcast_no_urls_or_transcript():\n    \"\"\"Test that an error is raised when no URLs or transcript file is provided.\"\"\"\n    with pytest.raises(ValueError):\n        generate_podcast()\n\n\ndef test_generate_podcast_from_images(sample_config, default_conversation_config):\n    \"\"\"Test generating a podcast from two input images.\"\"\"\n    image_paths = MOCK_IMAGE_PATHS\n\n    audio_file = generate_podcast(\n        image_paths=image_paths, tts_model=\"edge\", config=sample_config\n    )\n\n    assert audio_file is not None\n    assert os.path.exists(audio_file)\n    assert audio_file.endswith(\".mp3\")\n    assert os.path.getsize(audio_file) > 1024  # Check if larger than 1KB\n\n    # Check if a transcript was generated\n    transcript_dir = (\n        default_conversation_config.get(\"text_to_speech\", {})\n        .get(\"output_directories\", {})\n        .get(\"transcripts\")\n    )\n    transcript_files = [\n        f\n        for f in os.listdir(transcript_dir)\n        if f.startswith(\"transcript_\") and f.endswith(\".txt\")\n    ]\n    assert len(transcript_files) > 0\n\n\ndef test_generate_podcast_from_raw_text(sample_config, default_conversation_config):\n    \"\"\"Test generating a podcast from raw input text.\"\"\"\n    raw_text = \"The wonderful world of LLMs.\"\n\n    audio_file = generate_podcast(text=raw_text, tts_model=\"edge\", config=sample_config)\n\n    assert audio_file is not None\n    assert os.path.exists(audio_file)\n    assert audio_file.endswith(\".mp3\")\n    assert os.path.getsize(audio_file) > 1024  # Check if larger than 1KB\n    assert os.path.dirname(audio_file) == default_conversation_config.get(\n        \"text_to_speech\", {}\n    ).get(\"output_directories\", {}).get(\"audio\")\n\n\ndef test_generate_transcript_with_user_instructions(\n    sample_config, default_conversation_config\n):\n    \"\"\"Test generating a transcript with specific user instructions in the conversation config.\"\"\"\n    url = \"https://en.wikipedia.org/wiki/Artificial_intelligence\"\n\n    # Create a custom conversation config with user instructions\n    conversation_config = {\n        \"word_count\": 2000,\n        \"conversation_style\": [\"formal\", \"educational\"],\n        \"roles_person1\": \"professor\",\n        \"roles_person2\": \"student\",\n        \"dialogue_structure\": [\n            \"Introduction\",\n            \"Main Points\",\n            \"Case Studies\",\n            \"Quiz\",\n            \"Conclusion\",\n        ],\n        \"podcast_name\": \"Teachfy\",\n        \"podcast_tagline\": \"Learning Through Teaching\",\n        \"output_language\": \"English\",\n        \"engagement_techniques\": [\"examples\", \"questions\"],\n        \"creativity\": 0,\n        \"user_instructions\": \"Make a connection with a related topic: Knowledge Graphs.\",\n    }\n\n    # Generate transcript with the custom config\n    result = generate_podcast(\n        urls=[url],\n        transcript_only=True,\n        config=sample_config,\n        conversation_config=conversation_config,\n        tts_model=\"edge\",\n    )\n\n    assert result is not None\n    assert os.path.exists(result)\n    assert result.endswith(\".txt\")\n    assert os.path.dirname(result) == default_conversation_config.get(\n        \"text_to_speech\", {}\n    ).get(\"output_directories\", {}).get(\"transcripts\")\n\n    # Read the generated transcript\n    with open(result, \"r\") as f:\n        content = f.read()\n\n    assert (\n        conversation_config[\"podcast_name\"].lower() in content.lower()\n    ), f\"Expected to find podcast name '{conversation_config['podcast_name']}' in transcript\"\n    assert (\n        conversation_config[\"podcast_tagline\"].lower() in content.lower()\n    ), f\"Expected to find podcast tagline '{conversation_config['podcast_tagline']}' in transcript\"\n\n\ndef test_generate_podcast_with_custom_llm(sample_config, default_conversation_config):\n    \"\"\"Test generating a podcast with a custom LLM model.\"\"\"\n    urls = [\"https://en.wikipedia.org/wiki/Artificial_intelligence\"]\n\n    audio_file = generate_podcast(\n        urls=urls,\n        tts_model=\"edge\",\n        config=sample_config,\n        llm_model_name=\"gemini-1.5-pro-latest\",\n        api_key_label=\"GEMINI_API_KEY\",\n    )\n\n    assert audio_file is not None\n    assert os.path.exists(audio_file)\n    assert audio_file.endswith(\".mp3\")\n    assert os.path.getsize(audio_file) > 1024\n    assert os.path.dirname(audio_file) == default_conversation_config.get(\n        \"text_to_speech\", {}\n    ).get(\"output_directories\", {}).get(\"audio\")\n\n\ndef test_generate_transcript_only_with_custom_llm(\n    sample_config, default_conversation_config\n):\n    \"\"\"Test generating only a transcript with a custom LLM model.\"\"\"\n    urls = [\"https://en.wikipedia.org/wiki/Artificial_intelligence\"]\n\n    # Generate transcript with custom LLM settings\n    result = generate_podcast(\n        urls=urls,\n        transcript_only=True,\n        config=sample_config,\n        llm_model_name=\"gemini-1.5-pro-latest\",\n        api_key_label=\"GEMINI_API_KEY\",\n    )\n\n    assert result is not None\n    assert os.path.exists(result)\n    assert result.endswith(\".txt\")\n    assert os.path.dirname(result) == default_conversation_config.get(\n        \"text_to_speech\", {}\n    ).get(\"output_directories\", {}).get(\"transcripts\")\n\n    # Read and verify the content\n    with open(result, \"r\") as f:\n        content = f.read()\n\n    # Verify the content follows the Person1/Person2 format\n    assert \"<Person1>\" in content\n    assert \"<Person2>\" in content\n    assert len(content.split(\"<Person1>\")) > 1  # At least one question\n    assert len(content.split(\"<Person2>\")) > 1  # At least one answer\n\n    # Verify the content is substantial\n    min_length = 500  # Minimum expected length in characters\n    assert (\n        len(content) > min_length\n    ), f\"Content length ({len(content)}) is less than minimum expected ({min_length})\"\n\n\ndef test_generate_longform_transcript(sample_config, default_conversation_config):\n    \"\"\"Test generating a longform podcast transcript from a PDF file.\"\"\"\n    pdf_file = \"tests/data/pdf/file.pdf\"\n    \n    # Generate transcript with longform=True\n    result = generate_podcast(\n        urls=[pdf_file],\n        config=sample_config,\n        transcript_only=True,\n        longform=True\n    )\n\n    assert result is not None\n    assert os.path.exists(result)\n    assert result.endswith(\".txt\")\n    \n    # Read and verify the content\n    with open(result, \"r\") as f:\n        content = f.read()\n    \n    # Verify the content follows the Person1/Person2 format\n    assert \"<Person1>\" in content\n    assert \"<Person2>\" in content\n    \n    # Verify it's a long-form transcript (>1000 characters)\n    assert len(content) > 1000, f\"Content length ({len(content)}) is less than minimum expected for longform (1000)\"\n    \n    # Verify multiple discussion rounds exist (characteristic of longform)\n    person1_segments = content.count(\"<Person1>\")\n    assert person1_segments > 3, f\"Expected more than 3 discussion rounds, got {person1_segments}\"\n\n\nif __name__ == \"__main__\":\n    pytest.main()\n"}
{"type": "test_file", "path": "tests/test_audio.py", "content": "import unittest\nimport pytest\nimport os\nfrom podcastfy.text_to_speech import TextToSpeech\nfrom podcastfy.utils.config_conversation import load_conversation_config\n\n\nclass TestAudio(unittest.TestCase):\n    def setUp(self):\n        self.test_text = \"<Person1>Hello, how are you?</Person1><Person2>I'm doing great, thanks for asking!</Person2>\"\n        self.output_dir = \"tests/data/audio\"\n        os.makedirs(self.output_dir, exist_ok=True)\n\n    @pytest.mark.skip(reason=\"Testing edge only on Github Action as it's free\")\n    def test_text_to_speech_openai(self):\n        tts = TextToSpeech(model=\"openai\")\n        output_file = os.path.join(self.output_dir, \"test_openai.mp3\")\n        tts.convert_to_speech(self.test_text, output_file)\n\n        self.assertTrue(os.path.exists(output_file))\n        self.assertGreater(os.path.getsize(output_file), 1024)\n\n        # Clean up\n        os.remove(output_file)\n\n    @pytest.mark.skip(reason=\"Testing edge only on Github Action as it's free\")\n    def test_text_to_speech_elevenlabs(self):\n        tts = TextToSpeech(model=\"elevenlabs\")\n        output_file = os.path.join(self.output_dir, \"test_elevenlabs.mp3\")\n        tts.convert_to_speech(self.test_text, output_file)\n\n        self.assertTrue(os.path.exists(output_file))\n        self.assertGreater(os.path.getsize(output_file), 1024)\n\n        # Clean up\n        os.remove(output_file)\n\n    def test_text_to_speech_edge(self):\n        tts = TextToSpeech(model=\"edge\")\n        output_file = os.path.join(self.output_dir, \"test_edge.mp3\")\n        tts.convert_to_speech(self.test_text, output_file)\n\n        self.assertTrue(os.path.exists(output_file))\n        self.assertGreater(os.path.getsize(output_file), 1024)\n\n        # Clean up\n        os.remove(output_file)\n\n    @pytest.mark.skip(reason=\"Testing edge only on Github Action as it's free\")\n    def test_text_to_speech_google(self):\n        tts = TextToSpeech(model=\"gemini\")\n        output_file = os.path.join(self.output_dir, \"test_google.mp3\")\n        tts.convert_to_speech(self.test_text, output_file)\n\n        self.assertTrue(os.path.exists(output_file))\n        self.assertGreater(os.path.getsize(output_file), 1024)\n\n        # Clean up\n        os.remove(output_file)\n\n    @pytest.mark.skip(reason=\"Testing edge only on Github Action as it's free\")\n    def test_text_to_speech_google_multi(self):\n        tts = TextToSpeech(model=\"gemini_multi\")\n        output_file = os.path.join(self.output_dir, \"test_google_multi.mp3\")\n        tts.convert_to_speech(self.test_text, output_file)\n\n        self.assertTrue(os.path.exists(output_file))\n        self.assertGreater(os.path.getsize(output_file), 1024)\n\n        # Clean up\n        os.remove(output_file)\n\n\nif __name__ == \"__main__\":\n    unittest.main()\n"}
{"type": "test_file", "path": "tests/test_client.py", "content": "\"\"\"\nUnit tests for the Podcastfy CLI client.\n\"\"\"\n\nimport os\nimport pytest\nimport re\nfrom typer.testing import CliRunner\nfrom podcastfy.client import app\n\nrunner = CliRunner()\n\n\n# Mock data\nMOCK_URLS = [\n    \"https://en.wikipedia.org/wiki/Podcast\",\n    \"https://en.wikipedia.org/wiki/Text-to-speech\",\n]\nMOCK_FILE_CONTENT = \"\\n\".join(MOCK_URLS)\nMOCK_TRANSCRIPT = \"<Person1>Joe Biden and the US Politics</Person1><Person2>Joe Biden is the current president of the United States of America</Person2>\"\nMOCK_IMAGE_PATHS = [\n    \"https://raw.githubusercontent.com/souzatharsis/podcastfy/refs/heads/main/data/images/Senecio.jpeg\",\n    \"https://raw.githubusercontent.com/souzatharsis/podcastfy/refs/heads/main/data/images/connection.jpg\",\n]\nMOCK_CONVERSATION_CONFIG = \"\"\"\nword_count: 300\nconversation_style: \n  - formal\n  - educational\nroles_person1: professor\nroles_person2: student\ndialogue_structure: \n  - Introduction\n  - Main Points\n  - Case Studies\n  - Quiz\n  - Conclusion\npodcast_name: Teachfy\npodcast_tagline: Learning Through Conversation\noutput_language: English\nengagement_techniques: \n  - examples\n  - questions\ncreativity: 0\ntext_to_speech:\n\tmodel: edge\n\"\"\"\n\n\n@pytest.fixture\ndef mock_files(tmp_path):\n    # Create mock files\n    url_file = tmp_path / \"urls.txt\"\n    url_file.write_text(MOCK_FILE_CONTENT)\n\n    transcript_file = tmp_path / \"transcript.txt\"\n    transcript_file.write_text(MOCK_TRANSCRIPT)\n\n    config_file = tmp_path / \"custom_config.yaml\"\n    config_file.write_text(MOCK_CONVERSATION_CONFIG)\n\n    return {\n        \"url_file\": str(url_file),\n        \"transcript_file\": str(transcript_file),\n        \"config_file\": str(config_file),\n    }\n\n\n@pytest.fixture\ndef sample_config():\n    \"\"\"\n    Fixture to provide a sample conversation configuration for testing.\n\n    Returns:\n            dict: A dictionary containing sample conversation configuration parameters.\n    \"\"\"\n    conversation_config = {\n        \"word_count\": 300,\n        \"text_to_speech\": {\n            \"output_directories\": {\n                \"transcripts\": \"tests/data/transcripts\",\n                \"audio\": \"tests/data/audio\",\n            },\n            \"temp_audio_dir\": \"tests/data/audio/tmp\",\n            \"ending_message\": \"Bye Bye!\",\n        },\n    }\n    return conversation_config\n\n\ndef test_generate_podcast_from_urls(sample_config):\n    result = runner.invoke(\n        app, [\"--url\", MOCK_URLS[0], \"--url\", MOCK_URLS[1], \"--tts-model\", \"edge\"]\n    )\n    assert result.exit_code == 0\n    assert \"Podcast generated successfully using edge TTS model\" in result.stdout\n    audio_path = result.stdout.split(\": \")[-1].strip()\n    assert os.path.exists(audio_path)\n    assert audio_path.endswith(\".mp3\")\n    assert os.path.getsize(audio_path) > 1024  # Check if larger than 1KB\n\n\ndef test_generate_podcast_from_file(mock_files, sample_config):\n    result = runner.invoke(\n        app, [\"--file\", mock_files[\"url_file\"], \"--tts-model\", \"edge\"]\n    )\n    assert result.exit_code == 0\n    assert \"Podcast generated successfully using edge TTS model\" in result.stdout\n    assert os.path.exists(result.stdout.split(\": \")[-1].strip())\n    assert result.stdout.split(\": \")[-1].strip().endswith(\".mp3\")\n    assert (\n        os.path.getsize(result.stdout.split(\": \")[-1].strip()) > 1024\n    )  # Check if larger than 1KB\n\n\ndef test_generate_podcast_from_transcript(mock_files, sample_config):\n    result = runner.invoke(\n        app, [\"--transcript\", mock_files[\"transcript_file\"], \"--tts-model\", \"edge\"]\n    )\n    assert result.exit_code == 0\n    assert \"Podcast generated successfully using edge TTS model\" in result.stdout\n    assert os.path.exists(result.stdout.split(\": \")[-1].strip())\n    assert result.stdout.split(\": \")[-1].strip().endswith(\".mp3\")\n    assert (\n        os.path.getsize(result.stdout.split(\": \")[-1].strip()) > 1024\n    )  # Check if larger than 1KB\n\n\ndef test_generate_transcript_only(sample_config):\n    result = runner.invoke(app, [\"--url\", MOCK_URLS[0], \"--transcript-only\"])\n    assert result.exit_code == 0\n    assert \"Transcript generated successfully\" in result.stdout\n\n    # Extract the transcript path\n    transcript_path = result.stdout.split(\": \")[-1].strip()\n\n    assert transcript_path, \"Transcript path is empty\"\n    assert os.path.exists(\n        transcript_path\n    ), f\"Transcript file does not exist at path: {transcript_path}\"\n\n    with open(transcript_path, \"r\") as f:\n        content = f.read()\n        assert content != \"\"\n        assert isinstance(content, str)\n        assert all(\n            \"<Person1>\" in tag and \"</Person1>\" in tag\n            for tag in re.findall(r\"<Person1>.*?</Person1>\", content)\n        )\n        assert all(\n            \"<Person2>\" in tag and \"</Person2>\" in tag\n            for tag in re.findall(r\"<Person2>.*?</Person2>\", content)\n        )\n\n\n@pytest.mark.skip(reason=\"Not supported yet\")\ndef test_generate_podcast_from_urls_and_file(mock_files, sample_config):\n    result = runner.invoke(\n        app,\n        [\n            \"--url\",\n            MOCK_URLS[0],\n            \"--file\",\n            mock_files[\"url_file\"],\n            \"--tts-model\",\n            \"edge\",\n        ],\n    )\n    assert result.exit_code == 0\n    assert \"Podcast generated successfully using edge TTS model\" in result.stdout\n    assert os.path.exists(result.stdout.split(\": \")[-1].strip())\n    assert result.stdout.split(\": \")[-1].strip().endswith(\".mp3\")\n    assert (\n        os.path.getsize(result.stdout.split(\": \")[-1].strip()) > 1024\n    )  # Check if larger than 1KB\n\n\ndef test_generate_podcast_from_image(sample_config):\n    result = runner.invoke(app, [\"--image\", MOCK_IMAGE_PATHS[0], \"--tts-model\", \"edge\"])\n    assert result.exit_code == 0\n    assert \"Podcast generated successfully using edge TTS model\" in result.stdout\n    assert os.path.exists(result.stdout.split(\": \")[-1].strip())\n    assert result.stdout.split(\": \")[-1].strip().endswith(\".mp3\")\n    assert (\n        os.path.getsize(result.stdout.split(\": \")[-1].strip()) > 1024\n    )  # Check if larger than 1KB\n\n\n@pytest.mark.skip(reason=\"To be further tested\")\ndef test_generate_podcast_with_custom_config(mock_files, sample_config):\n    result = runner.invoke(\n        app,\n        [\n            \"--url\",\n            MOCK_URLS[0],\n            \"--conversation-config\",\n            mock_files[\"config_file\"],\n            \"--tts-model\",\n            \"edge\",\n        ],\n    )\n    assert result.exit_code == 0\n    assert \"Podcast generated successfully using edge TTS model\" in result.stdout\n    audio_path = result.stdout.split(\": \")[-1].strip()\n    assert os.path.exists(audio_path)\n    assert audio_path.endswith(\".mp3\")\n    assert os.path.getsize(audio_path) > 1024  # Check if larger than 1KB\n\n    # Check for elements from the custom config in the transcript\n    transcript_path = audio_path.replace(\".mp3\", \".txt\")\n    assert os.path.exists(transcript_path)\n    with open(transcript_path, \"r\") as f:\n        content = f.read()\n        assert \"Teachfy\" in content\n        assert \"Learning Through Conversation\" in content\n\n\ndef test_generate_podcast_from_urls_and_images(sample_config):\n    result = runner.invoke(\n        app,\n        [\"--url\", MOCK_URLS[0], \"--image\", MOCK_IMAGE_PATHS[0], \"--tts-model\", \"edge\"],\n    )\n    assert result.exit_code == 0\n    assert \"Podcast generated successfully using edge TTS model\" in result.stdout\n    assert os.path.exists(result.stdout.split(\": \")[-1].strip())\n    assert result.stdout.split(\": \")[-1].strip().endswith(\".mp3\")\n    assert (\n        os.path.getsize(result.stdout.split(\": \")[-1].strip()) > 1024\n    )  # Check if larger than 1KB\n\n\n@pytest.mark.skip(reason=\"Requires local LLM running\")\ndef test_generate_transcript_with_local_llm(sample_config):\n    result = runner.invoke(\n        app,\n        [\"--url\", MOCK_URLS[0], \"--transcript-only\", \"--local\", \"--tts-model\", \"edge\"],\n    )\n    assert result.exit_code == 0\n    assert \"Transcript generated successfully\" in result.stdout\n    transcript_path = result.stdout.split(\": \")[-1].strip()\n    assert os.path.exists(transcript_path)\n    with open(transcript_path, \"r\") as f:\n        content = f.read()\n        assert content != \"\"\n        assert isinstance(content, str)\n        assert re.match(\n            r\"(<Person1>.*?</Person1>\\s*<Person2>.*?</Person2>\\s*)+\", content\n        )\n\n\ndef test_generate_podcast_from_raw_text():\n    \"\"\"Test generating a podcast from raw input text using the CLI.\"\"\"\n    raw_text = \"The wonderful world of LLMs.\"\n    result = runner.invoke(app, [\"--text\", raw_text, \"--tts-model\", \"edge\"])\n    assert result.exit_code == 0\n    assert \"Podcast generated successfully using edge TTS model\" in result.stdout\n    audio_path = result.stdout.split(\": \")[-1].strip()\n    assert os.path.exists(audio_path)\n    assert audio_path.endswith(\".mp3\")\n    assert os.path.getsize(audio_path) > 1024  # Check if larger than 1KB\n\n\ndef test_cli_help():\n    result = runner.invoke(app, [\"--help\"])\n    assert result.exit_code == 0\n    assert \"Generate a podcast or transcript from a list of URLs\" in result.stdout\n\n\ndef test_no_input_provided():\n    result = runner.invoke(app)\n    assert result.exit_code != 0\n    assert \"No input provided\" in result.stdout\n\n\ndef test_generate_podcast_with_custom_llm():\n    \"\"\"Test generating a podcast with a custom LLM model using CLI.\"\"\"\n    result = runner.invoke(\n        app,\n        [\n            \"--url\",\n            MOCK_URLS[0],\n            \"--tts-model\",\n            \"edge\",\n            \"--llm-model-name\",\n            \"gemini-1.5-pro-latest\",\n            \"--api-key-label\",\n            \"GEMINI_API_KEY\",\n        ],\n    )\n\n    assert result.exit_code == 0\n    assert \"Podcast generated successfully using edge TTS model\" in result.stdout\n\n    # Extract and verify the audio file\n    audio_path = result.stdout.split(\": \")[-1].strip()\n    assert os.path.exists(audio_path)\n    assert audio_path.endswith(\".mp3\")\n    assert os.path.getsize(audio_path) > 1024  # Check if larger than 1KB\n\n    # Clean up\n    os.remove(audio_path)\n\n\ndef test_generate_transcript_only_with_custom_llm():\n    \"\"\"Test generating only a transcript with a custom LLM model using CLI.\"\"\"\n    result = runner.invoke(\n        app,\n        [\n            \"--url\",\n            MOCK_URLS[0],\n            \"--transcript-only\",\n            \"--llm-model-name\",\n            \"gemini-1.5-pro-latest\",\n            \"--api-key-label\",\n            \"GEMINI_API_KEY\",\n        ],\n    )\n\n    assert result.exit_code == 0\n    assert \"Transcript generated successfully\" in result.stdout\n\n    # Extract and verify the transcript file\n    transcript_path = result.stdout.split(\": \")[-1].strip()\n    assert os.path.exists(transcript_path)\n    assert transcript_path.endswith(\".txt\")\n\n    # Verify transcript content\n    with open(transcript_path, \"r\") as f:\n        content = f.read()\n        assert content != \"\"\n        assert isinstance(content, str)\n        assert \"<Person1>\" in content\n        assert \"<Person2>\" in content\n        assert len(content.split(\"<Person1>\")) > 1  # At least one question\n        assert len(content.split(\"<Person2>\")) > 1  # At least one answer\n\n        # Verify content is substantial\n        min_length = 500  # Minimum expected length in characters\n        assert (\n            len(content) > min_length\n        ), f\"Content length ({len(content)}) is less than minimum expected ({min_length})\"\n\n    # Clean up\n    os.remove(transcript_path)\n\n\n@pytest.mark.skip(reason=\"Too expensive to be auto tested on Github Actions\")\ndef test_generate_podcast_from_topic():\n    \"\"\"Test generating a podcast from a topic using CLI.\"\"\"\n    result = runner.invoke(\n        app, [\"--topic\", \"Artificial Intelligence Ethics\", \"--tts-model\", \"edge\"]\n    )\n\n    assert result.exit_code == 0\n    assert \"Podcast generated successfully using edge TTS model\" in result.stdout\n\n    # Extract and verify the audio file\n    audio_path = result.stdout.split(\": \")[-1].strip()\n    assert os.path.exists(audio_path)\n    assert audio_path.endswith(\".mp3\")\n    assert os.path.getsize(audio_path) > 1024  # Check if larger than 1KB\n\n    # Clean up\n    os.remove(audio_path)\n\n\nif __name__ == \"__main__\":\n    pytest.main()\n"}
{"type": "source_file", "path": "build_docs.py", "content": "import os\nimport sys\nfrom sphinx.cmd.build import main as sphinx_main\n\ndef main():\n\t\"\"\"\n\tWrapper function to build Sphinx documentation.\n\t\"\"\"\n\t# Change to the docs directory\n\tos.chdir('docs')\n\t\n\t# Run Sphinx build command\n\tsys.exit(sphinx_main(['-b', 'html', 'source', '_build/html']))\n\nif __name__ == '__main__':\n\tmain()"}
{"type": "source_file", "path": "docs/source/conf.py", "content": "# Configuration file for the Sphinx documentation builder.\n#\n# For the full list of built-in configuration values, see the documentation:\n# https://www.sphinx-doc.org/en/master/usage/configuration.html\n\n# -- Project information -----------------------------------------------------\n# https://www.sphinx-doc.org/en/master/usage/configuration.html#project-information\n\nproject = 'podcastfy'\ncopyright = '2024, Tharsis T. P. Souza'\nauthor = 'Tharsis T. P. Souza'\nrelease = 'v0.4.0'\n\n# -- General configuration ---------------------------------------------------\n# https://www.sphinx-doc.org/en/master/usage/configuration.html#general-configuration\n\nimport os\nimport sys\nsys.path.insert(0, os.path.abspath('../..'))\n\nextensions = [\n    \"sphinx.ext.autodoc\",\n    \"sphinx.ext.napoleon\",\n    \"sphinx.ext.viewcode\",\n    \"nbsphinx\",\n]\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\"_templates\"]\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This pattern also affects html_static_path and html_extra_path.\nexclude_patterns = []\n\n\n# -- Options for HTML output -------------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\nhtml_theme = \"sphinx_rtd_theme\"\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named \"default.css\" will overwrite the builtin \"default.css\".\nhtml_static_path = [\"_static\"]\nmaster_doc = \"index\""}
{"type": "source_file", "path": "docs/generate_api_docs.py", "content": "import os\nimport pkgutil\n\ndef generate_api_docs(package_name):\n\t# Get the package\n\tpackage = __import__(package_name)\n\n\t# Create the api directory if it doesn't exist\n\tapi_dir = 'docs/source/api'\n\tos.makedirs(api_dir, exist_ok=True)\n\n\t# Generate the main API page\n\twith open(f'{api_dir}/index.rst', 'w') as f:\n\t\tf.write(f\"{package_name} API\\n\")\n\t\tf.write(\"=\" * (len(package_name) + 4) + \"\\n\\n\")\n\t\tf.write(\".. toctree::\\n\")\n\t\tf.write(\"   :maxdepth: 2\\n\\n\")\n\n\t# Iterate through all modules in the package\n\tfor _, module_name, _ in pkgutil.walk_packages(package.__path__, package.__name__ + '.'):\n\t\twith open(f'{api_dir}/{module_name}.rst', 'w') as f:\n\t\t\tf.write(f\"{module_name}\\n\")\n\t\t\tf.write(\"=\" * len(module_name) + \"\\n\\n\")\n\t\t\tf.write(f\".. automodule:: {module_name}\\n\")\n\t\t\tf.write(\"   :members:\\n\")\n\t\t\tf.write(\"   :undoc-members:\\n\")\n\t\t\tf.write(\"   :show-inheritance:\\n\")\n\n\t\t# Add the module to the main API page\n\t\twith open(f'{api_dir}/index.rst', 'a') as f:\n\t\t\tf.write(f\"   {module_name}\\n\")\n\ndef main():\n\tgenerate_api_docs('podcastfy')  # Replace 'podcastfy' with your actual package name\n\nif __name__ == \"__main__\":\n\tmain()"}
{"type": "source_file", "path": "podcastfy/api/fast_app.py", "content": "\"\"\"\nFastAPI implementation for Podcastify podcast generation service.\n\nThis module provides REST endpoints for podcast generation and audio serving,\nwith configuration management and temporary file handling.\n\"\"\"\n\nfrom fastapi import FastAPI, HTTPException\nfrom fastapi.responses import FileResponse, JSONResponse\nimport os\nimport shutil\nimport yaml\nfrom typing import Dict, Any\nfrom pathlib import Path\nfrom ..client import generate_podcast\nimport uvicorn\n\n\ndef load_base_config() -> Dict[Any, Any]:\n    config_path = Path(__file__).parent / \"podcastfy\" / \"conversation_config.yaml\"\n    try:\n        with open(config_path, 'r') as file:\n            return yaml.safe_load(file)\n    except Exception as e:\n        print(f\"Warning: Could not load base config: {e}\")\n        return {}\n\ndef merge_configs(base_config: Dict[Any, Any], user_config: Dict[Any, Any]) -> Dict[Any, Any]:\n    \"\"\"Merge user configuration with base configuration, preferring user values.\"\"\"\n    merged = base_config.copy()\n    \n    # Handle special cases for nested dictionaries\n    if 'text_to_speech' in merged and 'text_to_speech' in user_config:\n        merged['text_to_speech'].update(user_config.get('text_to_speech', {}))\n    \n    # Update top-level keys\n    for key, value in user_config.items():\n        if key != 'text_to_speech':  # Skip text_to_speech as it's handled above\n            if value is not None:  # Only update if value is not None\n                merged[key] = value\n                \n    return merged\n\napp = FastAPI()\n\nTEMP_DIR = os.path.join(os.path.dirname(__file__), \"temp_audio\")\nos.makedirs(TEMP_DIR, exist_ok=True)\n\n@app.post(\"/generate\")\nasync def generate_podcast_endpoint(data: dict):\n    \"\"\"\"\"\"\n    try:\n        # Set environment variables\n        os.environ['OPENAI_API_KEY'] = data.get('openai_key')\n        os.environ['GEMINI_API_KEY'] = data.get('google_key')\n        os.environ['ELEVENLABS_API_KEY'] = data.get('elevenlabs_key')\n\n        # Load base configuration\n        base_config = load_base_config()\n        \n        # Get TTS model and its configuration from base config\n        tts_model = data.get('tts_model', base_config.get('text_to_speech', {}).get('default_tts_model', 'openai'))\n        tts_base_config = base_config.get('text_to_speech', {}).get(tts_model, {})\n        \n        # Get voices (use user-provided voices or fall back to defaults)\n        voices = data.get('voices', {})\n        default_voices = tts_base_config.get('default_voices', {})\n        \n        # Prepare user configuration\n        user_config = {\n            'creativity': float(data.get('creativity', base_config.get('creativity', 0.7))),\n            'conversation_style': data.get('conversation_style', base_config.get('conversation_style', [])),\n            'roles_person1': data.get('roles_person1', base_config.get('roles_person1')),\n            'roles_person2': data.get('roles_person2', base_config.get('roles_person2')),\n            'dialogue_structure': data.get('dialogue_structure', base_config.get('dialogue_structure', [])),\n            'podcast_name': data.get('name', base_config.get('podcast_name')),\n            'podcast_tagline': data.get('tagline', base_config.get('podcast_tagline')),\n            'output_language': data.get('output_language', base_config.get('output_language', 'English')),\n            'user_instructions': data.get('user_instructions', base_config.get('user_instructions', '')),\n            'engagement_techniques': data.get('engagement_techniques', base_config.get('engagement_techniques', [])),\n            'text_to_speech': {\n                'default_tts_model': tts_model,\n                'model': tts_base_config.get('model'),\n                'default_voices': {\n                    'question': voices.get('question', default_voices.get('question')),\n                    'answer': voices.get('answer', default_voices.get('answer'))\n                }\n            }\n        }\n\n        # print(user_config)\n\n        # Merge configurations\n        conversation_config = merge_configs(base_config, user_config)\n\n        # print(conversation_config)\n        \n\n        # Generate podcast\n        result = generate_podcast(\n            urls=data.get('urls', []),\n            conversation_config=conversation_config,\n            tts_model=tts_model,\n            longform=bool(data.get('is_long_form', False)),\n        )\n        # Handle the result\n        if isinstance(result, str) and os.path.isfile(result):\n            filename = f\"podcast_{os.urandom(8).hex()}.mp3\"\n            output_path = os.path.join(TEMP_DIR, filename)\n            shutil.copy2(result, output_path)\n            return {\"audioUrl\": f\"/audio/{filename}\"}\n        elif hasattr(result, 'audio_path'):\n            filename = f\"podcast_{os.urandom(8).hex()}.mp3\"\n            output_path = os.path.join(TEMP_DIR, filename)\n            shutil.copy2(result.audio_path, output_path)\n            return {\"audioUrl\": f\"/audio/{filename}\"}\n        else:\n            raise HTTPException(status_code=500, detail=\"Invalid result format\")\n\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n\n@app.get(\"/audio/{filename}\")\nasync def serve_audio(filename: str):\n    \"\"\" Get File Audio From ther Server\"\"\"\n    file_path = os.path.join(TEMP_DIR, filename)\n    if not os.path.exists(file_path):\n        raise HTTPException(status_code=404, detail=\"File not found\")\n    return FileResponse(file_path)\n\n@app.get(\"/health\")\nasync def healthcheck():\n    return {\"status\": \"healthy\"}\n\nif __name__ == \"__main__\":\n    host = os.getenv(\"HOST\", \"127.0.0.1\")\n    port = int(os.getenv(\"PORT\", 8080))\n    uvicorn.run(app, host=host, port=port)\n"}
{"type": "source_file", "path": "podcastfy/__init__.py", "content": "# This file can be left empty for now\n__version__ = \"0.4.1\"  # or whatever version you're on\n"}
{"type": "source_file", "path": "podcastfy/content_parser/pdf_extractor.py", "content": "\"\"\"\nPDF Extractor Module\n\nThis module provides functionality to extract text content from PDF files.\nIt handles the reading of PDF files, text extraction, and normalization of\nthe extracted content, including handling of special characters and accents.\n\"\"\"\n\nimport pymupdf\nimport logging\nimport os\nimport unicodedata\n\nlogger = logging.getLogger(__name__)\n\nclass PDFExtractor:\n\tdef extract_content(self, file_path: str) -> str:\n\t\t\"\"\"\n\t\tExtract text content from a PDF file, handling foreign characters and special characters.\n\t\tAccents are removed from the text.\n\n\t\tArgs:\n\t\t\tfile_path (str): Path to the PDF file.\n\n\t\tReturns:\n\t\t\tstr: Extracted text content with accents removed and properly handled characters.\n\t\t\"\"\"\n\t\ttry:\n\t\t\tdoc = pymupdf.open(file_path)\n\t\t\tcontent = \" \".join(page.get_text() for page in doc)\n\t\t\tdoc.close()\n\t\t\t\n\t\t\t# Normalize the text to handle special characters and remove accents\n\t\t\tnormalized_content = unicodedata.normalize('NFKD', content)\n\n\t\t\treturn normalized_content\n\t\texcept Exception as e:\n\t\t\tlogger.error(f\"Error extracting PDF content: {str(e)}\")\n\t\t\traise\n\ndef main(seed: int = 42) -> None:\n\t\"\"\"\n\tTest the PDFExtractor class with a specific PDF file.\n\n\tArgs:\n\t\tseed (int): Random seed for reproducibility. Defaults to 42.\n\t\"\"\"\n\t# Set the random seed\n\timport random\n\trandom.seed(seed)\n\n\t# Get the absolute path of the script\n\tscript_dir = os.path.dirname(os.path.abspath(__file__))\n\t\n\t# Construct the path to the PDF file\n\tpdf_path = os.path.join(script_dir, '..', '..', 'tests', 'data', 'file.pdf')\n\t\n\textractor = PDFExtractor()\n\n\ttry:\n\t\tcontent = extractor.extract_content(pdf_path)\n\t\tprint(\"PDF content extracted successfully:\")\n\t\tprint(content[:500] + \"...\" if len(content) > 500 else content)\n\texcept Exception as e:\n\t\tprint(f\"An error occurred: {str(e)}\")\n\nif __name__ == \"__main__\":\n\tmain()"}
{"type": "source_file", "path": "podcastfy/content_parser/youtube_transcriber.py", "content": "\"\"\"\nYouTube Transcriber Module\n\nThis module is responsible for extracting and cleaning transcripts from YouTube videos.\nIt uses the YouTube Transcript API to fetch transcripts and provides functionality\nto clean and format the extracted text.\n\"\"\"\n\nfrom youtube_transcript_api import YouTubeTranscriptApi\nimport logging\nfrom podcastfy.utils.config import load_config\n\nlogger = logging.getLogger(__name__)\n\nclass YouTubeTranscriber:\n\tdef __init__(self):\n\t\tself.config = load_config()\n\t\tself.youtube_transcriber_config = self.config.get('youtube_transcriber')\n\n\tdef extract_transcript(self, url: str) -> str:\n\t\t\"\"\"\n\t\tExtract transcript from a YouTube video and remove '[music]' tags (case-insensitive).\n\n\t\tArgs:\n\t\t\turl (str): YouTube video URL.\n\n\t\tReturns:\n\t\t\tstr: Cleaned and extracted transcript.\n\t\t\"\"\"\n\t\ttry:\n\t\t\tvideo_id = url.split(\"v=\")[-1]\n\t\t\ttranscript = YouTubeTranscriptApi.get_transcript(video_id)\n\t\t\tcleaned_transcript = \" \".join([\n\t\t\t\tentry['text'] for entry in transcript \n\t\t\t\tif entry['text'].lower() not in self.youtube_transcriber_config['remove_phrases']\n\t\t\t])\n\t\t\treturn cleaned_transcript\n\t\texcept Exception as e:\n\t\t\tlogger.error(f\"Error extracting YouTube transcript: {str(e)}\")\n\t\t\traise\n\ndef main(seed: int = 42) -> None:\n\t\"\"\"\n\tTest the YouTubeTranscriber class with a specific URL and save the transcript.\n\n\tArgs:\n\t\tseed (int): Random seed for reproducibility. Defaults to 42.\n\t\"\"\"\n\turl = \"https://www.youtube.com/watch?v=nFbJCoTK0_g\"\n\ttranscriber = YouTubeTranscriber()\n\n\ttry:\n\t\ttranscript = transcriber.extract_transcript(url)\n\t\tprint(\"Transcript extracted successfully.\")\n\t\t\n\t\t# Save transcript to file\n\t\toutput_file = 'tests/data/transcripts/youtube_transcript2.txt'\n\t\twith open(output_file, 'w') as file:\n\t\t\tfile.write(transcript)\n\t\t\n\t\tprint(f\"Transcript saved to {output_file}\")\n\t\tprint(\"First 500 characters of the transcript:\")\n\t\tprint(transcript[:500] + \"...\" if len(transcript) > 500 else transcript)\n\texcept Exception as e:\n\t\tlogger.error(f\"An error occurred: {str(e)}\")\n\t\traise\n\nif __name__ == \"__main__\":\n\tmain()"}
{"type": "source_file", "path": "podcastfy/text_to_speech.py", "content": "\"\"\"\nText-to-Speech Module for converting text into speech using various providers.\n\nThis module provides functionality to convert text into speech using various TTS models.\nIt supports ElevenLabs, Google, OpenAI and Edge TTS services and handles the conversion process,\nincluding cleaning of input text and merging of audio files.\n\"\"\"\n\nimport io\nimport logging\nimport os\nimport re\nimport tempfile\nfrom typing import List, Tuple, Optional, Dict, Any\nfrom pydub import AudioSegment\n\nfrom .tts.factory import TTSProviderFactory\nfrom .utils.config import load_config\nfrom .utils.config_conversation import load_conversation_config\n\nlogger = logging.getLogger(__name__)\n\n\nclass TextToSpeech:\n    def __init__(\n        self,\n        model: str = None,\n        api_key: Optional[str] = None,\n        conversation_config: Optional[Dict[str, Any]] = None,\n    ):\n        \"\"\"\n        Initialize the TextToSpeech class.\n\n        Args:\n                        model (str): The model to use for text-to-speech conversion.\n                                                Options are 'elevenlabs', 'gemini', 'openai', 'edge' or 'geminimulti'. Defaults to 'openai'.\n                        api_key (Optional[str]): API key for the selected text-to-speech service.\n                        conversation_config (Optional[Dict]): Configuration for conversation settings.\n        \"\"\"\n        self.config = load_config()\n        self.conversation_config = load_conversation_config(conversation_config)\n        self.tts_config = self.conversation_config.get(\"text_to_speech\", {})\n\n        # Get API key from config if not provided\n        if not api_key:\n            api_key = getattr(self.config, f\"{model.upper().replace('MULTI', '')}_API_KEY\", None)\n\n        # Initialize provider using factory\n        self.provider = TTSProviderFactory.create(\n            provider_name=model, api_key=api_key, model=model\n        )\n\n        # Setup directories and config\n        self._setup_directories()\n        self.audio_format = self.tts_config.get(\"audio_format\", \"mp3\")\n        self.ending_message = self.tts_config.get(\"ending_message\", \"\")\n\n    def _get_provider_config(self) -> Dict[str, Any]:\n        \"\"\"Get provider-specific configuration.\"\"\"\n        # Get provider name in lowercase without 'TTS' suffix\n        provider_name = self.provider.__class__.__name__.lower().replace(\"tts\", \"\")\n\n        # Get provider config from tts_config\n        provider_config = self.tts_config.get(provider_name, {})\n\n        # If provider config is empty, try getting from default config\n        if not provider_config:\n            provider_config = {\n                \"model\": self.tts_config.get(\"default_model\"),\n                \"default_voices\": {\n                    \"question\": self.tts_config.get(\"default_voice_question\"),\n                    \"answer\": self.tts_config.get(\"default_voice_answer\"),\n                },\n            }\n\n        logger.debug(f\"Using provider config: {provider_config}\")\n        return provider_config\n\n    def convert_to_speech(self, text: str, output_file: str) -> None:\n        \"\"\"\n        Convert input text to speech and save as an audio file.\n\n        Args:\n                text (str): Input text to convert to speech.\n                output_file (str): Path to save the output audio file.\n\n        Raises:\n            ValueError: If the input text is not properly formatted\n        \"\"\"\n        # Validate transcript format\n        # self._validate_transcript_format(text)\n\n        cleaned_text = text\n\n        try:\n\n            if (\n                \"multi\" in self.provider.model.lower()\n            ):  # refactor: We should have instead MultiSpeakerTTS and SingleSpeakerTTS classes\n                provider_config = self._get_provider_config()\n                voice = provider_config.get(\"default_voices\", {}).get(\"question\")\n                voice2 = provider_config.get(\"default_voices\", {}).get(\"answer\")\n                model = provider_config.get(\"model\")\n                audio_data_list = self.provider.generate_audio(\n                    cleaned_text,\n                    voice=\"S\",\n                    model=\"en-US-Studio-MultiSpeaker\",\n                    voice2=\"R\",\n                    ending_message=self.ending_message,\n                )\n\n                try:\n                    # First verify we have data\n                    if not audio_data_list:\n                        raise ValueError(\"No audio data chunks provided\")\n\n                    logger.info(f\"Starting audio processing with {len(audio_data_list)} chunks\")\n                    combined = AudioSegment.empty()\n                    \n                    for i, chunk in enumerate(audio_data_list):\n                        # Save chunk to temporary file\n                        #temp_file = \"./tmp.mp3\"\n                        #with open(temp_file, \"wb\") as f:\n                        #    f.write(chunk)\n                        \n                        segment = AudioSegment.from_file(io.BytesIO(chunk))\n                        logger.info(f\"################### Loaded chunk {i}, duration: {len(segment)}ms\")\n                        \n                        combined += segment\n                    \n                    # Export with high quality settings\n                    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n                    combined.export(\n                        output_file, \n                        format=self.audio_format,\n                        codec=\"libmp3lame\",\n                        bitrate=\"320k\"\n                    )\n                    \n                except Exception as e:\n                    logger.error(f\"Error during audio processing: {str(e)}\")\n                    raise\n            else:\n                with tempfile.TemporaryDirectory(dir=self.temp_audio_dir) as temp_dir:\n                    audio_segments = self._generate_audio_segments(\n                        cleaned_text, temp_dir\n                    )\n                    self._merge_audio_files(audio_segments, output_file)\n                    logger.info(f\"Audio saved to {output_file}\")\n\n        except Exception as e:\n            logger.error(f\"Error converting text to speech: {str(e)}\")\n            raise\n\n    def _generate_audio_segments(self, text: str, temp_dir: str) -> List[str]:\n        \"\"\"Generate audio segments for each Q&A pair.\"\"\"\n        qa_pairs = self.provider.split_qa(\n            text, self.ending_message, self.provider.get_supported_tags()\n        )\n        audio_files = []\n        provider_config = self._get_provider_config()\n\n        for idx, (question, answer) in enumerate(qa_pairs, 1):\n            for speaker_type, content in [(\"question\", question), (\"answer\", answer)]:\n                temp_file = os.path.join(\n                    temp_dir, f\"{idx}_{speaker_type}.{self.audio_format}\"\n                )\n                voice = provider_config.get(\"default_voices\", {}).get(speaker_type)\n                model = provider_config.get(\"model\")\n\n                audio_data = self.provider.generate_audio(content, voice, model)\n                with open(temp_file, \"wb\") as f:\n                    f.write(audio_data)\n                audio_files.append(temp_file)\n\n        return audio_files\n\n    def _merge_audio_files(self, audio_files: List[str], output_file: str) -> None:\n        \"\"\"\n        Merge the provided audio files sequentially, ensuring questions come before answers.\n\n        Args:\n                audio_files: List of paths to audio files to merge\n                output_file: Path to save the merged audio file\n        \"\"\"\n        try:\n\n            def get_sort_key(file_path: str) -> Tuple[int, int]:\n                \"\"\"\n                Create sort key from filename that puts questions before answers.\n                Example filenames: \"1_question.mp3\", \"1_answer.mp3\"\n                \"\"\"\n                basename = os.path.basename(file_path)\n                # Extract the index number and type (question/answer)\n                idx = int(basename.split(\"_\")[0])\n                is_answer = basename.split(\"_\")[1].startswith(\"answer\")\n                return (\n                    idx,\n                    1 if is_answer else 0,\n                )  # Questions (0) come before answers (1)\n\n            # Sort files by index and type (question/answer)\n            audio_files.sort(key=get_sort_key)\n\n            # Create empty audio segment\n            combined = AudioSegment.empty()\n\n            # Add each audio file to the combined segment\n            for file_path in audio_files:\n                combined += AudioSegment.from_file(file_path, format=self.audio_format)\n\n            # Ensure output directory exists\n            os.makedirs(os.path.dirname(output_file), exist_ok=True)\n\n            # Export the combined audio\n            combined.export(output_file, format=self.audio_format)\n            logger.info(f\"Merged audio saved to {output_file}\")\n\n        except Exception as e:\n            logger.error(f\"Error merging audio files: {str(e)}\")\n            raise\n\n    def _setup_directories(self) -> None:\n        \"\"\"Setup required directories for audio processing.\"\"\"\n        self.output_directories = self.tts_config.get(\"output_directories\", {})\n        temp_dir = self.tts_config.get(\"temp_audio_dir\", \"data/audio/tmp/\").rstrip(\"/\").split(\"/\")\n        self.temp_audio_dir = os.path.join(*temp_dir)\n        base_dir = os.path.abspath(os.path.dirname(__file__))\n        self.temp_audio_dir = os.path.join(base_dir, self.temp_audio_dir)\n\n        os.makedirs(self.temp_audio_dir, exist_ok=True)\n\n        # Create directories if they don't exist\n        for dir_path in [\n            self.output_directories.get(\"transcripts\"),\n            self.output_directories.get(\"audio\"),\n            self.temp_audio_dir,\n        ]:\n            if dir_path and not os.path.exists(dir_path):\n                os.makedirs(dir_path)\n\n    def _validate_transcript_format(self, text: str) -> None:\n        \"\"\"\n        Validate that the input text follows the correct transcript format.\n\n        Args:\n            text (str): Input text to validate\n\n        Raises:\n            ValueError: If the text is not properly formatted\n\n        The text should:\n        1. Have alternating Person1 and Person2 tags\n        2. Each opening tag should have a closing tag\n        3. Tags should be properly nested\n        \"\"\"\n        try:\n            # Check for empty text\n            if not text.strip():\n                raise ValueError(\"Input text is empty\")\n\n            # Check for matching opening and closing tags\n            person1_open = text.count(\"<Person1>\")\n            person1_close = text.count(\"</Person1>\")\n            person2_open = text.count(\"<Person2>\")\n            person2_close = text.count(\"</Person2>\")\n\n            if person1_open != person1_close:\n                raise ValueError(\n                    f\"Mismatched Person1 tags: {person1_open} opening tags and {person1_close} closing tags\"\n                )\n            if person2_open != person2_close:\n                raise ValueError(\n                    f\"Mismatched Person2 tags: {person2_open} opening tags and {person2_close} closing tags\"\n                )\n\n            # Check for alternating pattern using regex\n            pattern = r\"<Person1>.*?</Person1>\\s*<Person2>.*?</Person2>\"\n            matches = re.findall(pattern, text, re.DOTALL)\n\n            # Calculate expected number of pairs\n            expected_pairs = min(person1_open, person2_open)\n\n            if len(matches) != expected_pairs:\n                raise ValueError(\n                    \"Tags are not properly alternating between Person1 and Person2. \"\n                    \"Each Person1 section should be followed by a Person2 section.\"\n                )\n\n                # Check for malformed tags (unclosed or improperly nested)\n                stack = []\n                for match in re.finditer(r\"<(/?)Person([12])>\", text):\n                    tag = match.group(0)\n                    if tag.startswith(\"</\"):\n                        if not stack or stack[-1] != tag[2:-1]:\n                            raise ValueError(f\"Improperly nested tags near: {tag}\")\n                        stack.pop()\n                    else:\n                        stack.append(tag[1:-1])\n\n                if stack:\n                    raise ValueError(f\"Unclosed tags: {', '.join(stack)}\")\n\n            logger.debug(\"Transcript format validation passed\")\n\n        except ValueError as e:\n            logger.error(f\"Transcript format validation failed: {str(e)}\")\n            raise\n        except Exception as e:\n            logger.error(f\"Unexpected error during transcript validation: {str(e)}\")\n            raise ValueError(f\"Invalid transcript format: {str(e)}\")\n\n\ndef main(seed: int = 42) -> None:\n    \"\"\"\n    Main function to test the TextToSpeech class.\n\n    Args:\n            seed (int): Random seed for reproducibility. Defaults to 42.\n    \"\"\"\n    try:\n        # Load configuration\n        config = load_config()\n\n        # Override default TTS model to use edge for tests\n        test_config = {\"text_to_speech\": {\"default_tts_model\": \"edge\"}}\n\n        # Read input text from file\n        with open(\n            \"tests/data/transcript_336aa9f955cd4019bc1287379a5a2820.txt\", \"r\"\n        ) as file:\n            input_text = file.read()\n\n        # Test ElevenLabs\n        tts_elevenlabs = TextToSpeech(model=\"elevenlabs\")\n        elevenlabs_output_file = \"tests/data/response_elevenlabs.mp3\"\n        tts_elevenlabs.convert_to_speech(input_text, elevenlabs_output_file)\n        logger.info(\n            f\"ElevenLabs TTS completed. Output saved to {elevenlabs_output_file}\"\n        )\n\n        # Test OpenAI\n        tts_openai = TextToSpeech(model=\"openai\")\n        openai_output_file = \"tests/data/response_openai.mp3\"\n        tts_openai.convert_to_speech(input_text, openai_output_file)\n        logger.info(f\"OpenAI TTS completed. Output saved to {openai_output_file}\")\n\n        # Test Edge\n        tts_edge = TextToSpeech(model=\"edge\")\n        edge_output_file = \"tests/data/response_edge.mp3\"\n        tts_edge.convert_to_speech(input_text, edge_output_file)\n        logger.info(f\"Edge TTS completed. Output saved to {edge_output_file}\")\n\n    except Exception as e:\n        logger.error(f\"An error occurred during text-to-speech conversion: {str(e)}\")\n        raise\n\n\nif __name__ == \"__main__\":\n    main(seed=42)\n"}
{"type": "source_file", "path": "podcastfy/client.py", "content": "\"\"\"\nPodcastfy CLI\n\nThis module provides a command-line interface for generating podcasts or transcripts\nfrom URLs or existing transcript files. It orchestrates the content extraction,\ngeneration, and text-to-speech conversion processes.\n\"\"\"\n\nimport os\nimport uuid\nimport typer\nimport yaml\nfrom podcastfy.content_parser.content_extractor import ContentExtractor\nfrom podcastfy.content_generator import ContentGenerator\nfrom podcastfy.text_to_speech import TextToSpeech\nfrom podcastfy.utils.config import Config, load_config\nfrom podcastfy.utils.config_conversation import load_conversation_config\nfrom podcastfy.utils.logger import setup_logger\nfrom typing import List, Optional, Dict, Any\nimport copy\n\nimport logging\n\n# Configure logging to show all levels and write to both file and console\n\"\"\" logging.basicConfig(\n    level=logging.DEBUG,  # Show all levels of logs\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n    handlers=[\n        logging.FileHandler('podcastfy.log'),  # Save to file\n        logging.StreamHandler()  # Print to console\n    ]\n) \"\"\"\n\n\nlogger = setup_logger(__name__)\n\napp = typer.Typer()\n\nos.environ[\"LANGCHAIN_TRACING_V2\"] = \"False\"\n\n\ndef process_content(\n    urls: Optional[List[str]] = None,\n    transcript_file: Optional[str] = None,\n    tts_model: Optional[str] = None,\n    generate_audio: bool = True,\n    config: Optional[Dict[str, Any]] = None,\n    conversation_config: Optional[Dict[str, Any]] = None,\n    image_paths: Optional[List[str]] = None,\n    is_local: bool = False,\n    text: Optional[str] = None,\n    model_name: Optional[str] = None,\n    api_key_label: Optional[str] = None,\n    topic: Optional[str] = None,\n    longform: bool = False\n):\n    \"\"\"\n    Process URLs, a transcript file, image paths, or raw text to generate a podcast or transcript.\n    \"\"\"\n    try:\n        if config is None:\n            config = load_config()\n\n        # Load default conversation config\n        conv_config = load_conversation_config()\n\n        # Update with provided config if any\n        if conversation_config:\n            conv_config.configure(conversation_config)\n        # Get output directories from conversation config\n        tts_config = conv_config.get(\"text_to_speech\", {})\n        output_directories = tts_config.get(\"output_directories\", {})\n\n        if transcript_file:\n            logger.info(f\"Using transcript file: {transcript_file}\")\n            with open(transcript_file, \"r\") as file:\n                qa_content = file.read()\n        else:\n            # Initialize content_extractor if needed\n            content_extractor = None\n            if urls or topic or (text and longform and len(text.strip()) < 100):\n                content_extractor = ContentExtractor()\n\n            content_generator = ContentGenerator(\n                is_local=is_local,\n                model_name=model_name,\n                api_key_label=api_key_label,\n                conversation_config=conv_config.to_dict()\n            )\n\n            combined_content = \"\"\n            \n            if urls:\n                logger.info(f\"Processing {len(urls)} links\")\n                contents = [content_extractor.extract_content(link) for link in urls]\n                combined_content += \"\\n\\n\".join(contents)\n\n            if text:\n                if longform and len(text.strip()) < 100:\n                    logger.info(\"Text too short for direct long-form generation. Extracting context...\")\n                    expanded_content = content_extractor.generate_topic_content(text)\n                    combined_content += f\"\\n\\n{expanded_content}\"\n                else:\n                    combined_content += f\"\\n\\n{text}\"\n\n            if topic:\n                topic_content = content_extractor.generate_topic_content(topic)\n                combined_content += f\"\\n\\n{topic_content}\"\n\n            # Generate Q&A content using output directory from conversation config\n            random_filename = f\"transcript_{uuid.uuid4().hex}.txt\"\n            transcript_filepath = os.path.join(\n                output_directories.get(\"transcripts\", \"data/transcripts\"),\n                random_filename,\n            )\n            qa_content = content_generator.generate_qa_content(\n                combined_content,\n                image_file_paths=image_paths or [],\n                output_filepath=transcript_filepath,\n                longform=longform\n            )\n\n        if generate_audio:\n            api_key = None\n            if tts_model != \"edge\":\n                api_key = getattr(config, f\"{tts_model.upper().replace('MULTI', '')}_API_KEY\")\n\n            text_to_speech = TextToSpeech(\n                model=tts_model,\n                api_key=api_key,\n                conversation_config=conv_config.to_dict(),\n            )\n\n            random_filename = f\"podcast_{uuid.uuid4().hex}.mp3\"\n            audio_file = os.path.join(\n                output_directories.get(\"audio\", \"data/audio\"), random_filename\n            )\n            text_to_speech.convert_to_speech(qa_content, audio_file)\n            logger.info(f\"Podcast generated successfully using {tts_model} TTS model\")\n            return audio_file\n        else:\n            logger.info(f\"Transcript generated successfully: {transcript_filepath}\")\n            return transcript_filepath\n\n    except Exception as e:\n        logger.error(f\"An error occurred in the process_content function: {str(e)}\")\n        raise\n\n\n@app.command()\ndef main(\n    urls: list[str] = typer.Option(None, \"--url\", \"-u\", help=\"URLs to process\"),\n    file: typer.FileText = typer.Option(\n        None, \"--file\", \"-f\", help=\"File containing URLs, one per line\"\n    ),\n    transcript: typer.FileText = typer.Option(\n        None, \"--transcript\", \"-t\", help=\"Path to a transcript file\"\n    ),\n    tts_model: str = typer.Option(\n        None,\n        \"--tts-model\",\n        \"-tts\",\n        help=\"TTS model to use (openai, elevenlabs, edge, or gemini)\",\n    ),\n    transcript_only: bool = typer.Option(\n        False, \"--transcript-only\", help=\"Generate only a transcript without audio\"\n    ),\n    conversation_config_path: str = typer.Option(\n        None,\n        \"--conversation-config\",\n        \"-cc\",\n        help=\"Path to custom conversation configuration YAML file\",\n    ),\n    image_paths: List[str] = typer.Option(\n        None, \"--image\", \"-i\", help=\"Paths to image files to process\"\n    ),\n    is_local: bool = typer.Option(\n        False,\n        \"--local\",\n        \"-l\",\n        help=\"Use a local LLM instead of a remote one (http://localhost:8080)\",\n    ),\n    text: str = typer.Option(\n        None, \"--text\", \"-txt\", help=\"Raw text input to be processed\"\n    ),\n    llm_model_name: str = typer.Option(\n        None, \"--llm-model-name\", \"-m\", help=\"LLM model name for transcript generation\"\n    ),\n    api_key_label: str = typer.Option(\n        None, \"--api-key-label\", \"-k\", help=\"Environment variable name for LLMAPI key\"\n    ),\n    topic: str = typer.Option(\n        None, \"--topic\", \"-tp\", help=\"Topic to generate podcast about\"\n    ),\n    longform: bool = typer.Option(\n        False, \n        \"--longform\", \n        \"-lf\", \n        help=\"Generate long-form content (only available for text input without images)\"\n    ),\n):\n    \"\"\"\n    Generate a podcast or transcript from a list of URLs, a file containing URLs, a transcript file, image files, or raw text.\n    \"\"\"\n    try:\n        config = load_config()\n        main_config = config.get(\"main\", {})\n\n        conversation_config = None\n        # Load conversation config if provided\n        if conversation_config_path:\n            with open(conversation_config_path, \"r\") as f:\n                conversation_config: Dict[str, Any] | None = yaml.safe_load(f)\n\n        # Use default TTS model from conversation config if not specified\n        if tts_model is None:\n            tts_config = load_conversation_config().get(\"text_to_speech\", {})\n            tts_model = tts_config.get(\"default_tts_model\", \"openai\")\n\n        if transcript:\n            if image_paths:\n                logger.warning(\"Image paths are ignored when using a transcript file.\")\n            final_output = process_content(\n                transcript_file=transcript.name,\n                tts_model=tts_model,\n                generate_audio=not transcript_only,\n                conversation_config=conversation_config,\n                config=config,\n                is_local=is_local,\n                text=text,\n                model_name=llm_model_name,\n                api_key_label=api_key_label,\n                topic=topic,\n                longform=longform\n            )\n        else:\n            urls_list = urls or []\n            if file:\n                urls_list.extend([line.strip() for line in file if line.strip()])\n\n            if not urls_list and not image_paths and not text and not topic:\n                raise typer.BadParameter(\n                    \"No input provided. Use --url, --file, --transcript, --image, --text, or --topic.\"\n                )\n\n            final_output = process_content(\n                urls=urls_list,\n                tts_model=tts_model,\n                generate_audio=not transcript_only,\n                config=config,\n                conversation_config=conversation_config,\n                image_paths=image_paths,\n                is_local=is_local,\n                text=text,\n                model_name=llm_model_name,\n                api_key_label=api_key_label,\n                topic=topic,\n                longform=longform\n            )\n\n        if transcript_only:\n            typer.echo(f\"Transcript generated successfully: {final_output}\")\n        else:\n            typer.echo(\n                f\"Podcast generated successfully using {tts_model} TTS model: {final_output}\"\n            )\n\n    except Exception as e:\n        typer.echo(f\"An error occurred: {str(e)}\", err=True)\n        raise typer.Exit(code=1)\n\n\nif __name__ == \"__main__\":\n    app()\n\n\ndef generate_podcast(\n    urls: Optional[List[str]] = None,\n    url_file: Optional[str] = None,\n    transcript_file: Optional[str] = None,\n    tts_model: Optional[str] = None,\n    transcript_only: bool = False,\n    config: Optional[Dict[str, Any]] = None,\n    conversation_config: Optional[Dict[str, Any]] = None,\n    image_paths: Optional[List[str]] = None,\n    is_local: bool = False,\n    text: Optional[str] = None,\n    llm_model_name: Optional[str] = None,\n    api_key_label: Optional[str] = None,\n    topic: Optional[str] = None,\n    longform: bool = False,\n) -> Optional[str]:\n    \"\"\"\n    Generate a podcast or transcript from a list of URLs, a file containing URLs, a transcript file, or image files.\n\n    Args:\n        urls (Optional[List[str]]): List of URLs to process.\n        url_file (Optional[str]): Path to a file containing URLs, one per line.\n        transcript_file (Optional[str]): Path to a transcript file.\n        tts_model (Optional[str]): TTS model to use ('openai' [default], 'elevenlabs', 'edge', or 'gemini').\n        transcript_only (bool): Generate only a transcript without audio. Defaults to False.\n        config (Optional[Dict[str, Any]]): User-provided configuration dictionary.\n        conversation_config (Optional[Dict[str, Any]]): User-provided conversation configuration dictionary.\n        image_paths (Optional[List[str]]): List of image file paths to process.\n        is_local (bool): Whether to use a local LLM. Defaults to False.\n        text (Optional[str]): Raw text input to be processed.\n        llm_model_name (Optional[str]): LLM model name for content generation.\n        api_key_label (Optional[str]): Environment variable name for LLM API key.\n        topic (Optional[str]): Topic to generate podcast about.\n\n    Returns:\n        Optional[str]: Path to the final podcast audio file, or None if only generating a transcript.\n    \"\"\"\n    try:\n        print(\"Generating podcast...\")\n        # Load default config\n        default_config = load_config()\n\n        # Update config if provided\n        if config:\n            if isinstance(config, dict):\n                # Create a deep copy of the default config\n                updated_config = copy.deepcopy(default_config)\n                # Update the copy with user-provided values\n                updated_config.configure(**config)\n                default_config = updated_config\n            elif isinstance(config, Config):\n                # If it's already a Config object, use it directly\n                default_config = config\n            else:\n                raise ValueError(\n                    \"Config must be either a dictionary or a Config object\"\n                )\n\n        if not conversation_config:\n            conversation_config = load_conversation_config().to_dict()\n\n        main_config = default_config.config.get(\"main\", {})\n\n        # Use provided tts_model if specified, otherwise use the one from config\n        if tts_model is None:\n            tts_model = conversation_config.get(\"default_tts_model\", \"openai\")\n\n        if transcript_file:\n            if image_paths:\n                logger.warning(\"Image paths are ignored when using a transcript file.\")\n            return process_content(\n                transcript_file=transcript_file,\n                tts_model=tts_model,\n                generate_audio=not transcript_only,\n                config=default_config,\n                conversation_config=conversation_config,\n                is_local=is_local,\n                text=text,\n                model_name=llm_model_name,\n                api_key_label=api_key_label,\n                topic=topic,\n                longform=longform\n            )\n        else:\n            urls_list = urls or []\n            if url_file:\n                with open(url_file, \"r\") as file:\n                    urls_list.extend([line.strip() for line in file if line.strip()])\n\n            if not urls_list and not image_paths and not text and not topic:\n                raise ValueError(\n                    \"No input provided. Please provide either 'urls', 'url_file', \"\n                    \"'transcript_file', 'image_paths', 'text', or 'topic'.\"\n                )\n\n            return process_content(\n                urls=urls_list,\n                tts_model=tts_model,\n                generate_audio=not transcript_only,\n                config=default_config,\n                conversation_config=conversation_config,\n                image_paths=image_paths,\n                is_local=is_local,\n                text=text,\n                model_name=llm_model_name,\n                api_key_label=api_key_label,\n                topic=topic,\n                longform=longform\n            )\n\n    except Exception as e:\n        logger.error(f\"An error occurred: {str(e)}\")\n        raise\n"}
{"type": "source_file", "path": "podcastfy/content_parser/content_extractor.py", "content": "\"\"\"\nContent Extractor Module\n\nThis module provides functionality to extract content from various sources including\nwebsites, YouTube videos, and PDF files. It serves as a central hub for content\nextraction, delegating to specialized extractors based on the source type.\n\"\"\"\n\nimport logging\nimport re\nfrom typing import List, Union\nfrom urllib.parse import urlparse\nfrom .youtube_transcriber import YouTubeTranscriber\nfrom .website_extractor import WebsiteExtractor\nfrom .pdf_extractor import PDFExtractor\nfrom podcastfy.utils.config import load_config\n\nlogger = logging.getLogger(__name__)\n\nclass ContentExtractor:\n\tdef __init__(self):\n\t\t\"\"\"\n\t\tInitialize the ContentExtractor.\n\t\t\"\"\"\n\t\tself.youtube_transcriber = YouTubeTranscriber()\n\t\tself.website_extractor = WebsiteExtractor()\n\t\tself.pdf_extractor = PDFExtractor()\n\t\tself.config = load_config()\n\t\tself.content_extractor_config = self.config.get('content_extractor', {})\n\n\tdef is_url(self, source: str) -> bool:\n\t\t\"\"\"\n\t\tCheck if the given source is a valid URL.\n\n\t\tArgs:\n\t\t\tsource (str): The source to check.\n\n\t\tReturns:\n\t\t\tbool: True if the source is a valid URL, False otherwise.\n\t\t\"\"\"\n\t\ttry:\n\t\t\t# If the source doesn't start with a scheme, add 'https://'\n\t\t\tif not source.startswith(('http://', 'https://')):\n\t\t\t\tsource = 'https://' + source\n\n\t\t\tresult = urlparse(source)\n\t\t\treturn all([result.scheme, result.netloc])\n\t\texcept ValueError:\n\t\t\treturn False\n\n\tdef extract_content(self, source: str) -> str:\n\t\t\"\"\"\n\t\tExtract content from various sources.\n\n\t\tArgs:\n\t\t\tsource (str): URL or file path of the content source.\n\n\t\tReturns:\n\t\t\tstr: Extracted text content.\n\n\t\tRaises:\n\t\t\tValueError: If the source type is unsupported.\n\t\t\"\"\"\n\t\ttry:\n\t\t\tif source.lower().endswith('.pdf'):\n\t\t\t\treturn self.pdf_extractor.extract_content(source)\n\t\t\telif self.is_url(source):\n\t\t\t\tif any(pattern in source for pattern in self.content_extractor_config['youtube_url_patterns']):\n\t\t\t\t\treturn self.youtube_transcriber.extract_transcript(source)\n\t\t\t\telse:\n\t\t\t\t\treturn self.website_extractor.extract_content(source)\n\t\t\telse:\n\t\t\t\traise ValueError(\"Unsupported source type\")\n\t\texcept Exception as e:\n\t\t\tlogger.error(f\"Error extracting content from {source}: {str(e)}\")\n\t\t\traise\n\t\n\tdef generate_topic_content(self, topic: str) -> str:\n\t\t\"\"\"\n\t\tGenerate content based on a given topic using a generative model.\n\n\t\tArgs:\n\t\t\ttopic (str): The topic to generate content for.\n\n\t\tReturns:\n\t\t\tstr: Generated content based on the topic.\n\t\t\"\"\"\n\t\ttry:\n\t\t\timport google.generativeai as genai\n\n\t\t\tmodel = genai.GenerativeModel('models/gemini-1.5-flash-002')\n\t\t\ttopic_prompt = f'Be detailed. Search for {topic}'\n\t\t\tresponse = model.generate_content(contents=topic_prompt, tools='google_search_retrieval')\n\t\t\t\n\t\t\treturn response.candidates[0].content.parts[0].text\n\t\texcept Exception as e:\n\t\t\tlogger.error(f\"Error generating content for topic '{topic}': {str(e)}\")\n\t\t\traise\n\t\t\n\ndef main(seed: int = 42) -> None:\n\t\"\"\"\n\tMain function to test the ContentExtractor class.\n\t\"\"\"\n\tlogging.basicConfig(level=logging.INFO)\n\n\t# Create an instance of ContentExtractor\n\textractor = ContentExtractor()\n\n\t# Test sources\n\ttest_sources: List[str] = [\n\t\t\"www.souzatharsis.com\",\n\t\t\"https://www.youtube.com/watch?v=dQw4w9WgXcQ\",\n\t\t\"path/to/sample.pdf\"\n\t]\n\n\tfor source in test_sources:\n\t\ttry:\n\t\t\tlogger.info(f\"Extracting content from: {source}\")\n\t\t\tcontent = extractor.extract_content(source)\n\n\t\t\t# Print the first 500 characters of the extracted content\n\t\t\tlogger.info(f\"Extracted content (first 500 characters):\\n{content[:500]}...\")\n\n\t\t\t# Print the total length of the extracted content\n\t\t\tlogger.info(f\"Total length of extracted content: {len(content)} characters\")\n\t\t\tlogger.info(\"-\" * 50)\n\n\t\texcept Exception as e:\n\t\t\tlogger.error(f\"An error occurred while processing {source}: {str(e)}\")\n\nif __name__ == \"__main__\":\n\tmain()\n"}
{"type": "source_file", "path": "podcastfy/content_generator.py", "content": "\"\"\"\nContent Generator Module\n\nThis module is responsible for generating Q&A content based on input texts using\nLangChain and various LLM backends. It handles the interaction with the AI model and\nprovides methods to generate and save the generated content.\n\"\"\"\n\nimport os\nfrom typing import Optional, Dict, Any, List\nimport re\n\n\nfrom langchain_community.chat_models import ChatLiteLLM\nfrom langchain_google_genai import ChatGoogleGenerativeAI\nfrom langchain_community.llms.llamafile import Llamafile\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain import hub\nfrom podcastfy.utils.config_conversation import load_conversation_config\nfrom podcastfy.utils.config import load_config\nimport logging\nfrom langchain.prompts import HumanMessagePromptTemplate\nfrom abc import ABC, abstractmethod\n\nlogger = logging.getLogger(__name__)\n\n\nclass LLMBackend:\n    def __init__(\n        self,\n        is_local: bool,\n        temperature: float,\n        max_output_tokens: int,\n        model_name: str,\n        api_key_label: str = \"GEMINI_API_KEY\",\n    ):\n        \"\"\"\n        Initialize the LLMBackend.\n\n        Args:\n                is_local (bool): Whether to use a local LLM or not.\n                temperature (float): The temperature for text generation.\n                max_output_tokens (int): The maximum number of output tokens.\n                model_name (str): The name of the model to use.\n        \"\"\"\n        self.is_local = is_local\n        self.temperature = temperature\n        self.max_output_tokens = max_output_tokens\n        self.model_name = model_name\n        self.is_multimodal = not is_local  # Does not assume local LLM is multimodal\n\n        common_params = {\n            \"temperature\": temperature,\n            \"presence_penalty\": 0.75,  # Encourage diverse content\n            \"frequency_penalty\": 0.75,  # Avoid repetition\n        }\n\n        if is_local:\n            self.llm = Llamafile() # replace with ollama\n        elif (\n            \"gemini\" in self.model_name.lower()\n        ):  # keeping original gemini as a special case while we build confidence on LiteLLM\n\n            self.llm = ChatGoogleGenerativeAI(\n                api_key=os.environ[\"GEMINI_API_KEY\"],\n                model=model_name,\n                max_output_tokens=max_output_tokens,\n                **common_params,\n            )\n        else:  # user should set api_key_label from input\n            self.llm = ChatLiteLLM(\n                model=self.model_name,\n                temperature=temperature,\n                api_key=os.environ[api_key_label],\n            )\n\n\nclass LongFormContentGenerator:\n    \"\"\"\n    Handles generation of long-form podcast conversations by breaking content into manageable chunks.\n    \n    Uses a \"Content Chunking with Contextual Linking\" strategy to maintain context between segments\n    while generating longer conversations.\n    \n    Attributes:\n        LONGFORM_INSTRUCTIONS (str): Constant containing instructions for long-form generation\n        llm_chain: The LangChain chain used for content generation\n    \"\"\"\n    # Add constant for long-form instructions\n    LONGFORM_INSTRUCTIONS = \"\"\"\n    Additional Instructions:\n        1. Provide extensive examples and real-world applications\n        2. Include detailed analysis and multiple perspectives\n        3. Use the \"yes, and\" technique to build upon points\n        4. Incorporate relevant anecdotes and case studies\n        5. Balance detailed explanations with engaging dialogue\n        6. Maintain consistent voice throughout the extended discussion\n        7. Generate a long conversation - output max_output_tokens tokens\n    \"\"\"\n    \n    def __init__(self, chain, llm, config_conversation: Dict[str, Any], ):\n        \"\"\"\n        Initialize ConversationGenerator.\n        \n        Args:\n            llm_chain: The LangChain chain to use for generation\n            config_conversation: Conversation configuration dictionary\n        \"\"\"\n        self.llm_chain = chain\n        self.llm = llm\n        self.max_num_chunks = config_conversation.get(\"max_num_chunks\", 10)  # Default if not in config\n        self.min_chunk_size = config_conversation.get(\"min_chunk_size\", 200)  # Default if not in config\n\n    def __calculate_chunk_size(self, input_content: str) -> int:\n        \"\"\"\n        Calculate chunk size based on input content length.\n        \n        Args:\n            input_content: Input text content\n                \n        Returns:\n            Calculated chunk size that ensures:\n            - Returns 1 if content length <= min_chunk_size\n            - Each chunk has at least min_chunk_size characters\n            - Number of chunks is at most max_num_chunks\n        \"\"\"\n        input_length = len(input_content)\n        if input_length <= self.min_chunk_size:\n            return input_length\n        \n        maximum_chunk_size = input_length // self.max_num_chunks\n        if maximum_chunk_size >= self.min_chunk_size:\n            return maximum_chunk_size\n        \n        # Calculate chunk size that maximizes size while maintaining minimum chunks\n        return input_length // (input_length // self.min_chunk_size)\n\n    def chunk_content(self, input_content: str, chunk_size: int) -> List[str]:\n        \"\"\"\n        Split input content into manageable chunks while preserving context.\n        \n        Args:\n            input_content (str): The input text to chunk\n            chunk_size (int): Maximum size of each chunk\n            \n        Returns:\n            List[str]: List of content chunks\n        \"\"\"\n        sentences = input_content.split('. ')\n        chunks = []\n        current_chunk = []\n        current_length = 0\n        \n        for sentence in sentences:\n            sentence_length = len(sentence)\n            if current_length + sentence_length > chunk_size and current_chunk:\n                chunks.append('. '.join(current_chunk) + '.')\n                current_chunk = []\n                current_length = 0\n            current_chunk.append(sentence)\n            current_length += sentence_length\n            \n        if current_chunk:\n            chunks.append('. '.join(current_chunk) + '.')\n        return chunks\n\n    def enhance_prompt_params(self, prompt_params: Dict, \n                              part_idx: int, \n                              total_parts: int,\n                              chat_context: str) -> Dict:\n        \"\"\"\n        Enhance prompt parameters for long-form content generation.\n        \n        Args:\n            prompt_params (Dict): Original prompt parameters\n            part_idx (int): Index of current conversation part\n            total_parts (int): Total number of conversation parts\n            chat_context (str): Chat context from previous parts\n            \n        Returns:\n            Dict: Enhanced prompt parameters with part-specific instructions\n        \"\"\"\n        enhanced_params = prompt_params.copy()\n\t\t# Initialize part_instructions with chat context\n        enhanced_params[\"context\"] = chat_context\n        \n        COMMON_INSTRUCTIONS = \"\"\"\n            Podcast conversation so far is given in CONTEXT.\n            Continue the natural flow of conversation. Follow-up on the very previous point/question without repeating topics or points already discussed!\n            Hence, the transition should be smooth and natural. Avoid abrupt transitions.\n            Make sure the first to speak is different from the previous speaker. Look at the last tag in CONTEXT to determine the previous speaker. \n            If last tag in CONTEXT is <Person1>, then the first to speak now should be <Person2>.\n            If last tag in CONTEXT is <Person2>, then the first to speak now should be <Person1>.\n            This is a live conversation without any breaks.\n            Hence, avoid statemeents such as \"we'll discuss after a short break.  Stay tuned\" or \"Okay, so, picking up where we left off\".\n        \"\"\" \n\n        # Add part-specific instructions\n        if part_idx == 0:\n            enhanced_params[\"instruction\"] = f\"\"\"\n            ALWAYS START THE CONVERSATION GREETING THE AUDIENCE: Welcome to {enhanced_params[\"podcast_name\"]} - {enhanced_params[\"podcast_tagline\"]}.\n            You are generating the Introduction part of a long podcast conversation.\n            Don't cover any topics yet, just introduce yourself and the topic. Leave the rest for later parts, following these guidelines:\n            \"\"\"\n        elif part_idx == total_parts - 1:\n            enhanced_params[\"instruction\"] = f\"\"\"\n            You are generating the last part of a long podcast conversation. \n            {COMMON_INSTRUCTIONS}\n            For this part, discuss the below INPUT and then make concluding remarks in a podcast conversation format and END THE CONVERSATION GREETING THE AUDIENCE WITH PERSON1 ALSO SAYING A GOOD BYE MESSAGE, following these guidelines:\n            \"\"\"\n        else:\n            enhanced_params[\"instruction\"] = f\"\"\"\n            You are generating part {part_idx+1} of {total_parts} parts of a long podcast conversation.\n            {COMMON_INSTRUCTIONS}\n            For this part, discuss the below INPUT in a podcast conversation format, following these guidelines:\n            \"\"\"\n        \n        return enhanced_params\n\n    def generate_long_form(\n        self, \n        input_content: str, \n        prompt_params: Dict\n    ) -> str:\n        \"\"\"\n        Generate a complete long-form conversation using chunked content.\n        \n        Args:\n            input_content (str): Input text for conversation\n            prompt_params (Dict): Base prompt parameters\n            \n        Returns:\n            str: Generated long-form conversation\n        \"\"\"\n        # Add long-form instructions once at the beginning\n        prompt_params[\"user_instructions\"] = prompt_params.get(\"user_instructions\", \"\") + self.LONGFORM_INSTRUCTIONS\n        \n        # Get chunk size\n        chunk_size = self.__calculate_chunk_size(input_content)\n\n        chunks = self.chunk_content(input_content, chunk_size)\n        conversation_parts = []\n        chat_context = input_content\n        num_parts = len(chunks)\n        print(f\"Generating {num_parts} parts\")\n        \n        for i, chunk in enumerate(chunks):\n            enhanced_params = self.enhance_prompt_params(\n                prompt_params,\n                part_idx=i,\n                total_parts=num_parts,\n                chat_context=chat_context\n            )\n            enhanced_params[\"input_text\"] = chunk\n            response = self.llm_chain.invoke(enhanced_params)\n            if i == 0:\n                chat_context = response\n            else:\n                chat_context = chat_context + response\n            print(f\"Generated part {i+1}/{num_parts}: Size {len(chunk)} characters.\")\n            #print(f\"[LLM-START] Step: {i+1} ##############################\")\n            #print(response)\n            #print(f\"[LLM-END] Step: {i+1} ##############################\")\n            conversation_parts.append(response)\n\n        return self.stitch_conversations(conversation_parts)\n    \n    def stitch_conversations(self, parts: List[str]) -> str:\n        \"\"\"\n        Combine conversation parts with smooth transitions.\n        \n        Args:\n            parts (List[str]): List of conversation parts\n            \n        Returns:\n            str: Combined conversation\n        \"\"\"\n        # Simply join the parts, preserving all markup\n        return \"\\n\".join(parts)\n\n\n# Make BaseContentCleaner a mixin class\nclass ContentCleanerMixin:\n    \"\"\"\n    Mixin class containing common transcript cleaning operations.\n    \n    Provides reusable cleaning methods that can be used by different content generation strategies.\n    Methods use protected naming convention (_method_name) as they are intended for internal use\n    by the strategies.\n    \"\"\"\n    \n    @staticmethod\n    def _clean_scratchpad(text: str) -> str:\n        \"\"\"\n        Remove scratchpad blocks, plaintext blocks, standalone triple backticks, any string enclosed in brackets, and underscores around words.\n        \"\"\"\n        try:\n            import re\n            pattern = r'```scratchpad\\n.*?```\\n?|```plaintext\\n.*?```\\n?|```\\n?|\\[.*?\\]'\n            cleaned_text = re.sub(pattern, '', text, flags=re.DOTALL)\n            # Remove \"xml\" if followed by </Person1> or </Person2>\n            cleaned_text = re.sub(r\"xml(?=\\s*</Person[12]>)\", \"\", cleaned_text)\n            # Remove underscores around words\n            cleaned_text = re.sub(r'_(.*?)_', r'\\1', cleaned_text)\n            return cleaned_text.strip()\n        except Exception as e:\n            logger.error(f\"Error cleaning scratchpad content: {str(e)}\")\n            return text\n\n    @staticmethod\n    def _clean_tss_markup(\n        input_text: str, \n        additional_tags: List[str] = [\"Person1\", \"Person2\"]\n    ) -> str:\n        \"\"\"\n        Remove unsupported TSS markup tags while preserving supported ones.\n        \"\"\"\n        try:\n            input_text = ContentCleanerMixin._clean_scratchpad(input_text)\n            supported_tags = [\"speak\", \"lang\", \"p\", \"phoneme\", \"s\", \"sub\"]\n            supported_tags.extend(additional_tags)\n\n            pattern = r\"</?(?!(?:\" + \"|\".join(supported_tags) + r\")\\b)[^>]+>\"\n            cleaned_text = re.sub(pattern, \"\", input_text)\n            cleaned_text = re.sub(r\"\\n\\s*\\n\", \"\\n\", cleaned_text)\n            cleaned_text = re.sub(r\"\\*\", \"\", cleaned_text)\n\n            for tag in additional_tags:\n                cleaned_text = re.sub(\n                    f'<{tag}>(.*?)(?=<(?:{\"|\".join(additional_tags)})>|$)',\n                    f\"<{tag}>\\\\1</{tag}>\",\n                    cleaned_text,\n                    flags=re.DOTALL,\n                )\n            \n\n\n            return cleaned_text.strip()\n            \n        except Exception as e:\n            logger.error(f\"Error cleaning TSS markup: {str(e)}\")\n            return input_text\n\n\nclass ContentGenerationStrategy(ABC):\n    \"\"\"\n    Abstract base class defining the interface for content generation strategies.\n    \n    Defines the contract that all concrete strategies must implement, including\n    validation, generation, and cleaning operations.\n    \"\"\"\n    \n    @abstractmethod\n    def validate(self, input_texts: str, image_file_paths: List[str]) -> None:\n        \"\"\"Validate inputs for this strategy.\"\"\"\n        pass\n        \n    @abstractmethod\n    def generate(self, \n                chain,\n                input_texts: str,\n                prompt_params: Dict[str, Any],\n                **kwargs) -> str:\n        \"\"\"Generate content using this strategy.\"\"\"\n        pass\n        \n    @abstractmethod\n    def clean(self, \n             response: str,\n             config: Dict[str, Any]) -> str:\n        \"\"\"Clean the generated response according to strategy.\"\"\"\n        pass\n\n    @abstractmethod\n    def compose_prompt_params(self,\n                            config_conversation: Dict[str, Any],\n                            image_file_paths: List[str] = [],\n                            image_path_keys: List[str] = [],\n                            input_texts: str = \"\") -> Dict[str, Any]:\n        \"\"\"Compose prompt parameters according to strategy.\"\"\"\n        pass\n\n\nclass StandardContentStrategy(ContentGenerationStrategy, ContentCleanerMixin):\n    \"\"\"\n    Strategy for generating standard-length content.\n    \n    Implements basic content generation without chunking or special handling.\n    Uses common cleaning operations from ContentCleanerMixin.\n    \"\"\"\n    \n    def __init__(self, llm, content_generator_config: Dict[str, Any], config_conversation: Dict[str, Any]):\n        \"\"\"\n        Initialize StandardContentStrategy.\n        \n        Args:\n            content_generator_config (Dict[str, Any]): Configuration for content generation\n            config_conversation (Dict[str, Any]): Conversation configuration\n        \"\"\"\n        self.llm = llm\n        self.content_generator_config = content_generator_config\n        self.config_conversation = config_conversation\n    \n    def validate(self, input_texts: str, image_file_paths: List[str]) -> None:\n        \"\"\"No specific validation needed for standard content.\"\"\"\n        pass\n        \n    def generate(self, \n                chain,\n                input_texts: str,\n                prompt_params: Dict[str, Any],\n                **kwargs) -> str:\n        \"\"\"Generate standard-length content.\"\"\"\n        return chain.invoke(prompt_params)\n        \n    def clean(self, \n             response: str,\n             config: Dict[str, Any]) -> str:\n        \"\"\"Apply basic TSS markup cleaning.\"\"\"\n        return self._clean_tss_markup(response)\n\n    def compose_prompt_params(self,\n                            config_conversation: Dict[str, Any],\n                            image_file_paths: List[str] = [],\n                            image_path_keys: List[str] = [],\n                            input_texts: str = \"\") -> Dict[str, Any]:\n        \"\"\"Compose prompt parameters for standard content generation.\"\"\"\n        prompt_params = {\n            \"input_text\": input_texts,\n            \"conversation_style\": \", \".join(\n                config_conversation.get(\"conversation_style\", [])\n            ),\n            \"roles_person1\": config_conversation.get(\"roles_person1\"),\n            \"roles_person2\": config_conversation.get(\"roles_person2\"),\n            \"dialogue_structure\": \", \".join(\n                config_conversation.get(\"dialogue_structure\", [])\n            ),\n            \"podcast_name\": config_conversation.get(\"podcast_name\"),\n            \"podcast_tagline\": config_conversation.get(\"podcast_tagline\"),\n            \"output_language\": config_conversation.get(\"output_language\"),\n            \"engagement_techniques\": \", \".join(\n                config_conversation.get(\"engagement_techniques\", [])\n            ),\n        }\n\n        # Add image paths to parameters if any\n        for key, path in zip(image_path_keys, image_file_paths):\n            prompt_params[key] = path\n\n        return prompt_params\n\n\nclass LongFormContentStrategy(ContentGenerationStrategy, ContentCleanerMixin):\n    \"\"\"\n    Strategy for generating long-form content.\n    \n    Implements advanced content generation using chunking and context maintenance.\n    Includes additional cleaning operations specific to long-form content.\n    \n    Note:\n        - Only works with text input (no images)\n        - Requires non-empty input text\n    \"\"\"\n    \n    def __init__(self, llm, content_generator_config: Dict[str, Any], config_conversation: Dict[str, Any]):\n        \"\"\"\n        Initialize LongFormContentStrategy.\n        \n        Args:\n            content_generator_config (Dict[str, Any]): Configuration for content generation\n            config_conversation (Dict[str, Any]): Conversation configuration\n        \"\"\"\n        self.llm = llm\n        self.content_generator_config = content_generator_config\n        self.config_conversation = config_conversation\n    \n    def validate(self, input_texts: str, image_file_paths: List[str]) -> None:\n        \"\"\"Validate inputs for long-form generation.\"\"\"\n        if not input_texts.strip():\n            raise ValueError(\"Long-form generation requires non-empty input text\")\n        if image_file_paths:\n            raise ValueError(\"Long-form generation is not available with image inputs\")\n            \n    def generate(self, \n                chain,\n                input_texts: str,\n                prompt_params: Dict[str, Any],\n                **kwargs) -> str:\n        \"\"\"Generate long-form content.\"\"\"\n        generator = LongFormContentGenerator(chain, self.llm, self.config_conversation)\n        return generator.generate_long_form(\n            input_texts,\n            prompt_params\n        )\n        \n    def clean(self, \n             response: str,\n             config: Dict[str, Any]) -> str:\n        \"\"\"Apply enhanced cleaning for long-form content.\"\"\"\n        # First apply standard cleaning using common method\n        standard_clean = self._clean_tss_markup(response)\n        # Then apply additional long-form specific cleaning\n        return self._clean_transcript_response(standard_clean, config)\n    \n    def _clean_transcript_response(self, transcript: str, config: Dict[str, Any]) -> str:\n        \"\"\"\n        Clean transcript using a two-step process with LLM-based cleaning.\n        \n        First cleans the markup using a specialized prompt template, then rewrites\n        for better flow and consistency using a second prompt template.\n        \n        Args:\n            transcript (str): Raw transcript text that may contain scratchpad blocks\n            config (Dict[str, Any]): Configuration dictionary containing LLM and prompt settings\n            \n        Returns:\n            str: Cleaned and rewritten transcript with proper tags and improved flow\n            \n        Note:\n            Falls back to original or partially cleaned transcript if any cleaning step fails\n        \"\"\"\n        logger.debug(\"Starting transcript cleaning process\")\n\n        final_transcript = self._fix_alternating_tags(transcript)\n        \n        logger.debug(\"Completed transcript cleaning process\")\n        \n        return final_transcript\n\n         \n    def _clean_transcript_response_DEPRECATED(self, transcript: str, config: Dict[str, Any]) -> str:\n        \"\"\"\n        Clean transcript using a two-step process with LLM-based cleaning.\n        \n        First cleans the markup using a specialized prompt template, then rewrites\n        for better flow and consistency using a second prompt template.\n        \n        Args:\n            transcript (str): Raw transcript text that may contain scratchpad blocks\n            config (Dict[str, Any]): Configuration dictionary containing LLM and prompt settings\n            \n        Returns:\n            str: Cleaned and rewritten transcript with proper tags and improved flow\n            \n        Note:\n            Falls back to original or partially cleaned transcript if any cleaning step fails\n        \"\"\"\n        logger.debug(\"Starting transcript cleaning process\")\n        try:\n            logger.debug(\"Initializing LLM model for cleaning\")\n            # Initialize model with config values for consistent cleaning\n            #llm = ChatGoogleGenerativeAI(\n            #    model=self.content_generator_config[\"meta_llm_model\"],\n            #    temperature=0,\n            #    presence_penalty=0.75,  # Encourage diverse content\n            #    frequency_penalty=0.75  # Avoid repetition\n            #)\n            llm = self.llm\n            logger.debug(\"LLM model initialized successfully\")\n\n            # Get prompt templates from hub\n            logger.debug(\"Pulling prompt templates from hub\")\n            try:\n                clean_transcript_prompt = hub.pull(f\"{self.content_generator_config['cleaner_prompt_template']}:{self.content_generator_config['cleaner_prompt_commit']}\")\n                rewrite_prompt = hub.pull(f\"{self.content_generator_config['rewriter_prompt_template']}:{self.content_generator_config['rewriter_prompt_commit']}\")\n                logger.debug(\"Successfully pulled prompt templates\")\n            except Exception as e:\n                logger.error(f\"Error pulling prompt templates: {str(e)}\")\n                return transcript\n            \n            logger.debug(\"Creating cleaning and rewriting chains\")\n            # Create chains\n            clean_chain = clean_transcript_prompt | llm | StrOutputParser()\n            rewrite_chain = rewrite_prompt | llm | StrOutputParser()\n            \n            # Run cleaning chain\n            logger.debug(\"Executing cleaning chain\")\n            try:\n                cleaned_response = clean_chain.invoke({\"transcript\": transcript})\n                if not cleaned_response:\n                    logger.warning(\"Cleaning chain returned empty response\")\n                    return transcript\n                logger.debug(\"Successfully cleaned transcript\")\n            except Exception as e:\n                logger.error(f\"Error in cleaning chain: {str(e)}\")\n                return transcript\n            \n            # Run rewriting chain\n            logger.debug(\"Executing rewriting chain\")\n            try:\n                rewritten_response = rewrite_chain.invoke({\"transcript\": cleaned_response})\n                if not rewritten_response:\n                    logger.warning(\"Rewriting chain returned empty response\")\n                    return cleaned_response  # Fall back to cleaned version\n                logger.debug(\"Successfully rewrote transcript\")\n            except Exception as e:\n                logger.error(f\"Error in rewriting chain: {str(e)}\")\n                return cleaned_response  # Fall back to cleaned version\n                \n            # Fix alternating tags in the final response\n            logger.debug(\"Fixing alternating tags\")\n            final_transcript = self._fix_alternating_tags(rewritten_response)\n            logger.debug(\"Completed transcript cleaning process\")\n            \n            return final_transcript\n            \n        except Exception as e:\n            logger.error(f\"Error in transcript cleaning process: {str(e)}\")\n            return transcript  # Return original if cleaning fails\n\n    def _fix_alternating_tags(self, transcript: str) -> str:\n        \"\"\"\n        Ensures transcript has properly alternating Person1 and Person2 tags.\n        \n        Merges consecutive same-person tags and ensures proper tag alternation\n        throughout the transcript.\n        \n        Args:\n            transcript (str): Input transcript text that may have consecutive same-person tags\n            \n        Returns:\n            str: Transcript with properly alternating tags and merged content\n            \n        Example:\n            Input:\n                <Person1>Hello</Person1>\n                <Person1>World</Person1>\n                <Person2>Hi</Person2>\n            Output:\n                <Person1>Hello World</Person1>\n                <Person2>Hi</Person2>\n                \n        Note:\n            Returns original transcript if cleaning fails\n        \"\"\"\n        try:\n            # Split into individual tag blocks while preserving tags\n            pattern = r'(<Person[12]>.*?</Person[12]>)'\n            blocks = re.split(pattern, transcript, flags=re.DOTALL)\n            \n            # Filter out empty/whitespace blocks\n            blocks = [b.strip() for b in blocks if b.strip()]\n            \n            merged_blocks = []\n            current_content = []\n            current_person = None\n            \n            for block in blocks:\n                # Extract person number and content\n                match = re.match(r'<Person([12])>(.*?)</Person\\1>', block, re.DOTALL)\n                if not match:\n                    continue\n                    \n                person_num, content = match.groups()\n                content = content.strip()\n                \n                if current_person == person_num:\n                    # Same person - append content\n                    current_content.append(content)\n                else:\n                    # Different person - flush current content if any\n                    if current_content:\n                        merged_text = \" \".join(current_content)\n                        merged_blocks.append(f\"<Person{current_person}>{merged_text}</Person{current_person}>\")\n                    # Start new person\n                    current_person = person_num\n                    current_content = [content]\n            \n            # Flush final content\n            if current_content:\n                merged_text = \" \".join(current_content)\n                merged_blocks.append(f\"<Person{current_person}>{merged_text}</Person{current_person}>\")\n                \n            return \"\\n\".join(merged_blocks)\n            \n        except Exception as e:\n            logger.error(f\"Error fixing alternating tags: {str(e)}\")\n            return transcript  # Return original if fixing fails\n\n    def compose_prompt_params(self,\n                            config_conversation: Dict[str, Any],\n                            image_file_paths: List[str] = [],\n                            image_path_keys: List[str] = [],\n                            input_texts: str = \"\") -> Dict[str, Any]:\n        \"\"\"Compose prompt parameters for long-form content generation.\"\"\"\n        return {\n            \"conversation_style\": \", \".join(\n                config_conversation.get(\"conversation_style\", [])\n            ),\n            \"roles_person1\": config_conversation.get(\"roles_person1\"),\n            \"roles_person2\": config_conversation.get(\"roles_person2\"),\n            \"dialogue_structure\": \", \".join(\n                config_conversation.get(\"dialogue_structure\", [])\n            ),\n            \"podcast_name\": config_conversation.get(\"podcast_name\"),\n            \"podcast_tagline\": config_conversation.get(\"podcast_tagline\"),\n            \"output_language\": config_conversation.get(\"output_language\"),\n            \"engagement_techniques\": \", \".join(\n                config_conversation.get(\"engagement_techniques\", [])\n            ),\n        }\n\n\nclass ContentGenerator:\n    def __init__(\n        self, \n        is_local: bool=False, \n        model_name: str=\"gemini-1.5-pro-latest\", \n        api_key_label: str=\"GEMINI_API_KEY\",\n        conversation_config: Optional[Dict[str, Any]] = None\n    ):\n        \"\"\"\n        Initialize the ContentGenerator.\n\n        Args:\n                api_key (str): API key for Google's Generative AI.\n                conversation_config (Optional[Dict[str, Any]]): Custom conversation configuration.\n        \"\"\"\n        #os.environ[\"GOOGLE_API_KEY\"] = api_key\n        self.config = load_config()\n        self.content_generator_config = self.config.get(\"content_generator\", {})\n\n        self.config_conversation = load_conversation_config(conversation_config)\n        self.tts_config = self.config_conversation.get(\"text_to_speech\", {})\n\n        # Get output directories from conversation config\n        self.output_directories = self.tts_config.get(\"output_directories\", {})\n\n        # Create output directories if they don't exist\n        transcripts_dir = self.output_directories.get(\"transcripts\")\n\n        if transcripts_dir and not os.path.exists(transcripts_dir):\n            os.makedirs(transcripts_dir)\n        \n        self.is_local = is_local\n\n                # Initialize LLM backend\n        if not model_name:\n            model_name = self.content_generator_config.get(\"llm_model\")\n        if is_local:\n            model_name = \"User provided local model\"\n\n        llm_backend = LLMBackend(\n            is_local=is_local,\n            temperature=self.config_conversation.get(\"creativity\", 1),\n            max_output_tokens=self.content_generator_config.get(\n                \"max_output_tokens\", 8192\n            ),\n            model_name=model_name,\n            api_key_label=api_key_label,\n        )\n\n        self.llm = llm_backend.llm\n\n\n\n        # Initialize strategies with configs\n        self.strategies = {\n            True: LongFormContentStrategy(\n                self.llm,\n                self.content_generator_config,\n                self.config_conversation\n            ),\n            False: StandardContentStrategy(\n                self.llm,\n                self.content_generator_config,\n                self.config_conversation\n            )\n        }\n\n    def __compose_prompt(self, num_images: int, longform: bool=False):\n        \"\"\"\n        Compose the prompt for the LLM based on the content list.\n        \"\"\"\n        content_generator_config = self.config.get(\"content_generator\", {})\n        \n        # Get base template and commit values\n        base_template = content_generator_config.get(\"prompt_template\")\n        base_commit = content_generator_config.get(\"prompt_commit\")\n        \n        # Modify template and commit for longform if configured\n        if longform:\n            template = content_generator_config.get(\"longform_prompt_template\")\n            commit = content_generator_config.get(\"longform_prompt_commit\")\n        else:\n            template = base_template\n            commit = base_commit\n\n        prompt_template = hub.pull(f\"{template}:{commit}\")\n\n        image_path_keys = []\n        messages = []\n\n        # Only add text content if input_text is not empty\n        text_content = {\n            \"type\": \"text\",\n            \"text\": \"Please analyze this input and generate a conversation. {input_text}\",\n        }\n        messages.append(text_content)\n\n        for i in range(num_images):\n            key = f\"image_path_{i}\"\n            image_content = {\n                \"image_url\": {\"url\": f\"{{{key}}}\", \"detail\": \"high\"},\n                \"type\": \"image_url\",\n            }\n            image_path_keys.append(key)\n            messages.append(image_content)\n\n        user_prompt_template = ChatPromptTemplate.from_messages(\n            messages=[HumanMessagePromptTemplate.from_template(messages)]\n        )\n        user_instructions = self.config_conversation.get(\"user_instructions\", \"\")\n\n        user_instructions = (\n            \"[[MAKE SURE TO FOLLOW THESE INSTRUCTIONS OVERRIDING THE PROMPT TEMPLATE IN CASE OF CONFLICT: \"\n            + user_instructions\n            + \"]]\"\n        )\n\n        new_system_message = (\n            prompt_template.messages[0].prompt.template + \"\\n\" + user_instructions\n        )\n\n        # Compose messages from podcastfy_prompt_template and user_prompt_template\n        combined_messages = (\n            ChatPromptTemplate.from_messages([new_system_message]).messages\n            + user_prompt_template.messages\n        )\n\n        # Create a new ChatPromptTemplate object with the combined messages\n        composed_prompt_template = ChatPromptTemplate.from_messages(combined_messages)\n\n        return composed_prompt_template, image_path_keys\n\n    def generate_qa_content(\n        self,\n        input_texts: str = \"\",\n        image_file_paths: List[str] = [],\n        output_filepath: Optional[str] = None,\n        longform: bool = False\n    ) -> str:\n        \"\"\"\n        Generate Q&A content based on input texts.\n\n        Args:\n            input_texts (str): Input texts to generate content from.\n            image_file_paths (List[str]): List of image file paths.\n            output_filepath (Optional[str]): Filepath to save the response content.\n            is_local (bool): Whether to use a local LLM or not.\n            model_name (str): Model name to use for generation.\n            api_key_label (str): Environment variable name for API key.\n            longform (bool): Whether to generate long-form content. Defaults to False.\n\n        Returns:\n            str: Generated conversation content\n\n        Raises:\n            ValueError: If strategy validation fails\n            Exception: If there's an error in generating content.\n        \"\"\"\n        try:\n            # Get appropriate strategy\n            strategy = self.strategies[longform]\n            \n            # Validate inputs for chosen strategy\n            strategy.validate(input_texts, image_file_paths)\n\n            # Setup chain\n            num_images = 0 if self.is_local else len(image_file_paths)\n            self.prompt_template, image_path_keys = self.__compose_prompt(num_images, longform)\n            self.parser = StrOutputParser()\n            self.chain = self.prompt_template | self.llm | self.parser\n\n\n            # Prepare parameters using strategy\n            prompt_params = strategy.compose_prompt_params(\n                self.config_conversation,\n                image_file_paths,\n                image_path_keys,\n                input_texts\n            )\n\n            # Generate content using selected strategy\n            self.response = strategy.generate(\n                self.chain,\n                input_texts,\n                prompt_params\n            )\n\n            # Clean response using the same strategy\n            self.response = strategy.clean(\n                self.response,\n                self.content_generator_config\n            )\n                \n            logger.info(f\"Content generated successfully\")\n\n            # Save output if requested\n            if output_filepath:\n                with open(output_filepath, \"w\") as file:\n                    file.write(self.response)\n                logger.info(f\"Response content saved to {output_filepath}\")\n                print(f\"Transcript saved to {output_filepath}\")\n\n            return self.response\n            \n        except Exception as e:\n            logger.error(f\"Error generating content: {str(e)}\")\n            raise\n"}
{"type": "source_file", "path": "podcastfy/content_parser/__init__.py", "content": "# This file can be left empty for now"}
{"type": "source_file", "path": "podcastfy/content_parser/website_extractor.py", "content": "\"\"\"\nWebsite Extractor Module\n\nThis module is responsible for extracting clean text content from websites using\nBeautifulSoup for local HTML parsing instead of the Jina AI API.\n\"\"\"\n\nimport requests\nimport re\nimport html\nimport logging\nfrom urllib.parse import urlparse\nfrom bs4 import BeautifulSoup\nfrom podcastfy.utils.config import load_config\nfrom typing import List\n\nlogger = logging.getLogger(__name__)\n\nclass WebsiteExtractor:\n\tdef __init__(self):\n\t\t\"\"\"\n\t\tInitialize the WebsiteExtractor.\n\t\t\"\"\"\n\t\tself.config = load_config()\n\t\tself.website_extractor_config = self.config.get('website_extractor', {})\n\t\tself.unwanted_tags = self.website_extractor_config.get('unwanted_tags', [])\n\t\tself.user_agent = self.website_extractor_config.get('user_agent', 'Mozilla/5.0')\n\t\tself.timeout = self.website_extractor_config.get('timeout', 10)\n\t\tself.remove_patterns = self.website_extractor_config.get('markdown_cleaning', {}).get('remove_patterns', [])\n\n\tdef extract_content(self, url: str) -> str:\n\t\t\"\"\"\n\t\tExtract clean text content from a website using BeautifulSoup.\n\n\t\tArgs:\n\t\t\turl (str): Website URL.\n\n\t\tReturns:\n\t\t\tstr: Extracted clean text content.\n\n\t\tRaises:\n\t\t\tException: If there's an error in extracting the content.\n\t\t\"\"\"\n\t\ttry:\n\t\t\t# Normalize the URL\n\t\t\tnormalized_url = self.normalize_url(url)\n\n\t\t\t# Request the webpage\n\t\t\theaders = {'User-Agent': self.user_agent}\n\t\t\tresponse = requests.get(normalized_url, headers=headers, timeout=self.timeout)\n\t\t\tresponse.raise_for_status()  # Raise an exception for bad status codes\n\n\t\t\t# Parse the page content with BeautifulSoup\n\t\t\tsoup = BeautifulSoup(response.text, 'html.parser')\n\n\t\t\t# Remove unwanted elements\n\t\t\tself.remove_unwanted_elements(soup)\n\n\t\t\t# Extract and clean the text content\n\t\t\traw_text = soup.get_text(separator=\"\\n\")  # Get all text content\n\t\t\tcleaned_content = self.clean_content(raw_text)\n\n\t\t\treturn cleaned_content\n\t\texcept requests.RequestException as e:\n\t\t\tlogger.error(f\"Failed to extract content from {url}: {str(e)}\")\n\t\t\traise Exception(f\"Failed to extract content from {url}: {str(e)}\")\n\t\texcept Exception as e:\n\t\t\tlogger.error(f\"An unexpected error occurred while extracting content from {url}: {str(e)}\")\n\t\t\traise Exception(f\"An unexpected error occurred while extracting content from {url}: {str(e)}\")\n\n\tdef normalize_url(self, url: str) -> str:\n\t\t\"\"\"\n\t\tNormalize the given URL by adding scheme if missing and ensuring it's a valid URL.\n\n\t\tArgs:\n\t\t\turl (str): The URL to normalize.\n\n\t\tReturns:\n\t\t\tstr: The normalized URL.\n\n\t\tRaises:\n\t\t\tValueError: If the URL is invalid after normalization attempts.\n\t\t\"\"\"\n\t\t# If the URL doesn't start with a scheme, add 'https://'\n\t\tif not url.startswith(('http://', 'https://')):\n\t\t\turl = 'https://' + url\n\n\t\t# Parse the URL\n\t\tparsed = urlparse(url)\n\n\t\t# Ensure the URL has a valid scheme and netloc\n\t\tif not all([parsed.scheme, parsed.netloc]):\n\t\t\traise ValueError(f\"Invalid URL: {url}\")\n\n\t\treturn parsed.geturl()\n\n\tdef remove_unwanted_elements(self, soup: BeautifulSoup) -> None:\n\t\t\"\"\"\n\t\tRemove unwanted elements from the BeautifulSoup object.\n\n\t\tArgs:\n\t\t\tsoup (BeautifulSoup): The BeautifulSoup object to clean.\n\t\t\"\"\"\n\t\tfor tag in self.unwanted_tags:\n\t\t\tfor element in soup.find_all(tag):\n\t\t\t\telement.decompose()\n\n\tdef clean_content(self, content: str) -> str:\n\t\t\"\"\"\n\t\tClean the extracted content by removing unnecessary whitespace and applying\n\t\tcustom cleaning patterns.\n\n\t\tArgs:\n\t\t\tcontent (str): The content to clean.\n\n\t\tReturns:\n\t\t\tstr: Cleaned text content.\n\t\t\"\"\"\n\t\t# Decode HTML entities\n\t\tcleaned_content = html.unescape(content)\n\n\t\t# Remove extra whitespace\n\t\tcleaned_content = re.sub(r'\\s+', ' ', cleaned_content)\n\n\t\t# Remove extra newlines\n\t\tcleaned_content = re.sub(r'\\n{3,}', '\\n\\n', cleaned_content)\n\n\t\t# Apply custom cleaning patterns from config\n\t\tfor pattern in self.remove_patterns:\n\t\t\tcleaned_content = re.sub(pattern, '', cleaned_content)\n\n\t\treturn cleaned_content.strip()\n\ndef main(seed: int = 42) -> None:\n\t\"\"\"\n\tMain function to test the WebsiteExtractor class.\n\t\"\"\"\n\tlogging.basicConfig(level=logging.INFO)\n\n\t# Create an instance of WebsiteExtractor\n\textractor = WebsiteExtractor()\n\n\t# Test URLs\n\ttest_urls: List[str] = [\n\t\t\"www.souzatharsis.com\",\n\t\t\"https://en.wikipedia.org/wiki/Web_scraping\"\n\t]\n\n\tfor url in test_urls:\n\t\ttry:\n\t\t\tlogger.info(f\"Extracting content from: {url}\")\n\t\t\tcontent = extractor.extract_content(url)\n\n\t\t\t# Print the first 500 characters of the extracted content\n\t\t\tlogger.info(f\"Extracted content (first 500 characters):\\n{content[:500]}...\")\n\n\t\t\t# Print the total length of the extracted content\n\t\t\tlogger.info(f\"Total length of extracted content: {len(content)} characters\")\n\t\t\tlogger.info(\"-\" * 50)\n\n\t\texcept Exception as e:\n\t\t\tlogger.error(f\"An error occurred while processing {url}: {str(e)}\")\n\nif __name__ == \"__main__\":\n\tmain()\n"}
{"type": "source_file", "path": "podcastfy/tts/providers/edge.py", "content": "\"\"\"Edge TTS provider implementation.\"\"\"\n\nimport edge_tts\nimport os\nimport tempfile\nfrom typing import List\nfrom ..base import TTSProvider\n\nclass EdgeTTS(TTSProvider):\n    def __init__(self, api_key: str = None, model: str = None):\n        \"\"\"\n        Initialize Edge TTS provider.\n        \n        Args:\n            api_key (str): Not used for Edge TTS\n            model (str): Model name to use\n        \"\"\"\n        self.model = model or \"default\"  # Edge TTS doesn't use models, but we set it for consistency\n\n    def generate_audio(self, text: str, voice: str, model: str, voice2: str = None) -> bytes:\n        \"\"\"Generate audio using Edge TTS.\"\"\"\n        import nest_asyncio\n        import asyncio\n        \n        # Apply nest_asyncio to allow nested event loops\n        nest_asyncio.apply()\n        \n        async def _generate():\n            communicate = edge_tts.Communicate(text, voice)\n            # Create a temporary file with proper context management\n            with tempfile.NamedTemporaryFile(suffix='.mp3', delete=False) as tmp_file:\n                temp_path = tmp_file.name\n                \n            try:\n                # Save audio to temporary file\n                await communicate.save(temp_path)\n                # Read the audio data\n                with open(temp_path, 'rb') as f:\n                    return f.read()\n            finally:\n                # Clean up temporary file\n                if os.path.exists(temp_path):\n                    os.remove(temp_path)\n\n        # Use nest_asyncio to handle nested event loops\n        loop = asyncio.get_event_loop()\n        return loop.run_until_complete(_generate())\n        \n    def get_supported_tags(self) -> List[str]:\n        \"\"\"Get supported SSML tags.\"\"\"\n        return self.COMMON_SSML_TAGS"}
{"type": "source_file", "path": "podcastfy/tts/providers/elevenlabs.py", "content": "\"\"\"ElevenLabs TTS provider implementation.\"\"\"\n\nfrom elevenlabs import client as elevenlabs_client\nfrom ..base import TTSProvider\nfrom typing import List\n\nclass ElevenLabsTTS(TTSProvider):\n    def __init__(self, api_key: str, model: str = \"eleven_multilingual_v2\"):\n        \"\"\"\n        Initialize ElevenLabs TTS provider.\n        \n        Args:\n            api_key (str): ElevenLabs API key\n            model (str): Model name to use. Defaults to \"eleven_multilingual_v2\"\n        \"\"\"\n        self.client = elevenlabs_client.ElevenLabs(api_key=api_key)\n        self.model = model\n        \n    def generate_audio(self, text: str, voice: str, model: str, voice2: str = None) -> bytes:\n        \"\"\"Generate audio using ElevenLabs API.\"\"\"\n        audio = self.client.generate(\n            text=text,\n            voice=voice,\n            model=model\n        )\n        return b''.join(chunk for chunk in audio if chunk)\n        \n    def get_supported_tags(self) -> List[str]:\n        \"\"\"Get supported SSML tags.\"\"\"\n        return ['lang', 'p', 'phoneme', 's', 'sub'] "}
{"type": "source_file", "path": "podcastfy/tts/base.py", "content": "\"\"\"Abstract base class for Text-to-Speech providers.\"\"\"\n\nfrom abc import ABC, abstractmethod\nfrom typing import List, ClassVar, Tuple\nimport re\n\nclass TTSProvider(ABC):\n    \"\"\"Abstract base class that defines the interface for TTS providers.\"\"\"\n    \n    # Common SSML tags supported by most providers\n    COMMON_SSML_TAGS: ClassVar[List[str]] = [\n        'lang', 'p', 'phoneme', 's', 'sub'\n    ]\n    \n    @abstractmethod\n    def generate_audio(self, text: str, voice: str, model: str, voice2: str) -> bytes:\n        \"\"\"\n        Generate audio from text using the provider's API.\n        \n        Args:\n            text: Text to convert to speech\n            voice: Voice ID/name to use\n            model: Model ID/name to use\n            \n        Returns:\n            Audio data as bytes\n            \n        Raises:\n            ValueError: If invalid parameters are provided\n            RuntimeError: If audio generation fails\n        \"\"\"\n        pass\n\n    def get_supported_tags(self) -> List[str]:\n        \"\"\"\n        Get set of SSML tags supported by this provider.\n        \n        Returns:\n            Set of supported SSML tag names\n        \"\"\"\n        return self.COMMON_SSML_TAGS.copy()\n    \n    def validate_parameters(self, text: str, voice: str, model: str, voice2: str = None) -> None:\n        \"\"\"\n        Validate input parameters before generating audio.\n        \n        Raises:\n            ValueError: If any parameter is invalid\n        \"\"\"\n        if not text:\n            raise ValueError(\"Text cannot be empty\")\n        if not voice:\n            raise ValueError(\"Voice must be specified\")\n        if not model:\n            raise ValueError(\"Model must be specified\")\n        \n    def split_qa(self, input_text: str, ending_message: str, supported_tags: List[str] = None) -> List[Tuple[str, str]]:\n        \"\"\"\n        Split the input text into question-answer pairs.\n\n        Args:\n            input_text (str): The input text containing Person1 and Person2 dialogues.\n            ending_message (str): The ending message to add to the end of the input text.\n\n        Returns:\n                List[Tuple[str, str]]: A list of tuples containing (Person1, Person2) dialogues.\n        \"\"\"\n        input_text = self.clean_tss_markup(input_text, supported_tags=supported_tags)\n        \n        # Add placeholder if input_text starts with <Person2>\n        if input_text.strip().startswith(\"<Person2>\"):\n            input_text = \"<Person1> Humm... </Person1>\" + input_text\n\n        # Add ending message to the end of input_text\n        if input_text.strip().endswith(\"</Person1>\"):\n            input_text += f\"<Person2>{ending_message}</Person2>\"\n\n        # Regular expression pattern to match Person1 and Person2 dialogues\n        pattern = r\"<Person1>(.*?)</Person1>\\s*<Person2>(.*?)</Person2>\"\n\n        # Find all matches in the input text\n        matches = re.findall(pattern, input_text, re.DOTALL)\n\n        # Process the matches to remove extra whitespace and newlines\n        processed_matches = [\n            (\" \".join(person1.split()).strip(), \" \".join(person2.split()).strip())\n            for person1, person2 in matches\n        ]\n        return processed_matches\n\n    def clean_tss_markup(self, input_text: str, additional_tags: List[str] = [\"Person1\", \"Person2\"], supported_tags: List[str] = None) -> str:\n        \"\"\"\n        Remove unsupported TSS markup tags from the input text while preserving supported SSML tags.\n\n        Args:\n            input_text (str): The input text containing TSS markup tags.\n            additional_tags (List[str]): Optional list of additional tags to preserve. Defaults to [\"Person1\", \"Person2\"].\n            supported_tags (List[str]): Optional list of supported tags. If None, use COMMON_SSML_TAGS.\n        Returns:\n            str: Cleaned text with unsupported TSS markup tags removed.\n        \"\"\"\n        if supported_tags is None:\n            supported_tags = self.COMMON_SSML_TAGS.copy()\n\n        # Append additional tags to the supported tags list\n        supported_tags.extend(additional_tags)\n\n        # Create a pattern that matches any tag not in the supported list\n        pattern = r'</?(?!(?:' + '|'.join(supported_tags) + r')\\b)[^>]+>'\n\n        # Remove unsupported tags\n        cleaned_text = re.sub(pattern, '', input_text)\n\n        # Remove any leftover empty lines\n        cleaned_text = re.sub(r'\\n\\s*\\n', '\\n', cleaned_text)\n\n        # Ensure closing tags for additional tags are preserved\n        for tag in additional_tags:\n            cleaned_text = re.sub(f'<{tag}>(.*?)(?=<(?:{\"|\".join(additional_tags)})>|$)', \n                                f'<{tag}>\\\\1</{tag}>', \n                                cleaned_text, \n                                flags=re.DOTALL)\n\n        return cleaned_text.strip()"}
{"type": "source_file", "path": "podcastfy/tts/factory.py", "content": "\"\"\"Factory for creating TTS providers.\"\"\"\n\nfrom typing import Dict, Type, Optional\nfrom .base import TTSProvider\nfrom .providers.elevenlabs import ElevenLabsTTS\nfrom .providers.openai import OpenAITTS\nfrom .providers.edge import EdgeTTS\nfrom .providers.gemini import GeminiTTS\nfrom .providers.geminimulti import GeminiMultiTTS\nclass TTSProviderFactory:\n    \"\"\"Factory class for creating TTS providers.\"\"\"\n    \n    _providers: Dict[str, Type[TTSProvider]] = {\n        'elevenlabs': ElevenLabsTTS,\n        'openai': OpenAITTS,\n        'edge': EdgeTTS,\n        'gemini': GeminiTTS,\n        'geminimulti': GeminiMultiTTS\n    }\n    \n    @classmethod\n    def create(cls, provider_name: str, api_key: Optional[str] = None, model: Optional[str] = None) -> TTSProvider:\n        \"\"\"\n        Create a TTS provider instance.\n        \n        Args:\n            provider_name: Name of the provider to create\n            api_key: Optional API key for the provider\n            model: Optional model name for the provider\n            \n        Returns:\n            TTSProvider instance\n            \n        Raises:\n            ValueError: If provider_name is not supported\n        \"\"\"\n        provider_class = cls._providers.get(provider_name.lower())\n        if not provider_class:\n            raise ValueError(f\"Unsupported provider: {provider_name}. \"\n                           f\"Choose from: {', '.join(cls._providers.keys())}\")\n                           \n        return provider_class(api_key, model) if api_key else provider_class(model=model)\n    \n    @classmethod\n    def register_provider(cls, name: str, provider_class: Type[TTSProvider]) -> None:\n        \"\"\"Register a new provider class.\"\"\"\n        cls._providers[name.lower()] = provider_class "}
{"type": "source_file", "path": "podcastfy/tts/providers/openai.py", "content": "\"\"\"OpenAI TTS provider implementation.\"\"\"\n\nimport openai\nfrom typing import List, Optional\nfrom ..base import TTSProvider\n\nclass OpenAITTS(TTSProvider):\n    \"\"\"OpenAI Text-to-Speech provider.\"\"\"\n    \n    # Provider-specific SSML tags\n    PROVIDER_SSML_TAGS: List[str] = ['break', 'emphasis']\n    \n    def __init__(self, api_key: Optional[str] = None, model: str = \"tts-1-hd\"):\n        \"\"\"\n        Initialize OpenAI TTS provider.\n        \n        Args:\n            api_key: OpenAI API key. If None, expects OPENAI_API_KEY env variable\n            model: Model name to use. Defaults to \"tts-1-hd\"\n        \"\"\"\n        if api_key:\n            openai.api_key = api_key\n        elif not openai.api_key:\n            raise ValueError(\"OpenAI API key must be provided or set in environment\")\n        self.model = model\n            \n    def get_supported_tags(self) -> List[str]:\n        \"\"\"Get all supported SSML tags including provider-specific ones.\"\"\"\n        return self.PROVIDER_SSML_TAGS\n        \n    def generate_audio(self, text: str, voice: str, model: str, voice2: str = None) -> bytes:\n        \"\"\"Generate audio using OpenAI API.\"\"\"\n        self.validate_parameters(text, voice, model)\n        \n        try:\n            response = openai.audio.speech.create(\n                model=model,\n                voice=voice,\n                input=text\n            )\n            return response.content\n        except Exception as e:\n            raise RuntimeError(f\"Failed to generate audio: {str(e)}\") from e"}
{"type": "source_file", "path": "podcastfy/utils/config.py", "content": "\"\"\"\nConfiguration Module\n\nThis module handles the loading and management of configuration settings for the Podcastfy application.\nIt uses environment variables to securely store and access API keys and other sensitive information,\nand a YAML file for non-sensitive configuration settings.\n\"\"\"\n\nimport os\nfrom dotenv import load_dotenv, find_dotenv\nfrom typing import Any, Dict, Optional\nimport yaml\n\ndef get_config_path(config_file: str = 'config.yaml'):\n\t\"\"\"\n\tGet the path to the config.yaml file.\n\t\n\tReturns:\n\t\tstr: The path to the config.yaml file.\n\t\"\"\"\n\ttry:\n\t\tbase_path = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n\t\t\n\t\t# Look for config.yaml in the package root\n\t\tconfig_path = os.path.join(base_path, config_file)\n\t\tif os.path.exists(config_path):\n\t\t\treturn config_path\n\t\t\n\t\t# If not found, look in the current working directory\n\t\tconfig_path = os.path.join(os.getcwd(), config_file)\n\t\tif os.path.exists(config_path):\n\t\t\treturn config_path\n\t\t\n\t\traise FileNotFoundError(f\"{config_file} not found\")\n\t\n\texcept Exception as e:\n\t\tprint(f\"Error locating {config_file}: {str(e)}\")\n\t\treturn None\n\nclass Config:\n\tdef __init__(self, config_file: str = 'config.yaml'):\n\t\t\"\"\"\n\t\tInitialize the Config class by loading environment variables and YAML configuration.\n\n\t\tArgs:\n\t\t\tconfig_file (str): Path to the YAML configuration file. Defaults to 'config.yaml'.\n\t\t\"\"\"\n\t\t# Try to find .env file\n\t\tdotenv_path = find_dotenv(usecwd=True)\n\t\tif dotenv_path:\n\t\t\tload_dotenv(dotenv_path)\n\t\telse:\n\t\t\tprint(\"Warning: .env file not found. Using environment variables if available.\")\n\t\t\n\t\t# Load API keys from environment variables\n\t\tself.GEMINI_API_KEY: str = os.getenv(\"GEMINI_API_KEY\", \"\")\n\t\tself.OPENAI_API_KEY: str = os.getenv(\"OPENAI_API_KEY\", \"\")\n\t\tself.ELEVENLABS_API_KEY: str = os.getenv(\"ELEVENLABS_API_KEY\", \"\")\n\t\t\n\t\tconfig_path = get_config_path(config_file)\n\t\tif config_path:\n\t\t\twith open(config_path, 'r') as file:\n\t\t\t\tself.config: Dict[str, Any] = yaml.safe_load(file)\n\t\telse:\n\t\t\tprint(\"Could not locate config.yaml\")\n\t\t\tself.config = {}\n\t\t\n\t\t# Set attributes based on YAML config\n\t\tself._set_attributes()\n\n\tdef _set_attributes(self):\n\t\t\"\"\"Set attributes based on the current configuration.\"\"\"\n\t\tfor key, value in self.config.items():\n\t\t\tsetattr(self, key.upper(), value)\n\n\t\t# Ensure output directories exist\n\t\tif 'output_directories' in self.config:\n\t\t\tfor dir_type, dir_path in self.config['output_directories'].items():\n\t\t\t\tos.makedirs(dir_path, exist_ok=True)\n\n\tdef configure(self, **kwargs):\n\t\t\"\"\"\n\t\tConfigure the settings by updating the config dictionary and relevant attributes.\n\n\t\tArgs:\n\t\t\t**kwargs: Keyword arguments representing configuration keys and values to update.\n\t\t\"\"\"\n\t\tfor key, value in kwargs.items():\n\t\t\tif key in self.config:\n\t\t\t\tself.config[key] = value\n\t\t\telif key in ['JINA_API_KEY', 'GEMINI_API_KEY', 'OPENAI_API_KEY', 'ELEVENLABS_API_KEY']:\n\t\t\t\tsetattr(self, key, value)\n\t\t\telse:\n\t\t\t\traise ValueError(f\"Unknown configuration key: {key}\")\n\n\t\t# Update attributes based on the new configuration\n\t\tself._set_attributes()\n\n\tdef get(self, key: str, default: Optional[Any] = None) -> Any:\n\t\t\"\"\"\n\t\tGet a configuration value by key.\n\n\t\tArgs:\n\t\t\tkey (str): The configuration key to retrieve.\n\t\t\tdefault (Optional[Any]): The default value if the key is not found.\n\n\t\tReturns:\n\t\t\tAny: The value associated with the key, or the default value if not found.\n\t\t\"\"\"\n\t\treturn self.config.get(key, default)\n\ndef load_config() -> Config:\n\t\"\"\"\n\tLoad and return a Config instance.\n\n\tReturns:\n\t\tConfig: An instance of the Config class.\n\t\"\"\"\n\treturn Config()\n\ndef main() -> None:\n\t\"\"\"\n\tTest the Config class and print configuration status.\n\t\"\"\"\n\t# Create an instance of the Config class\n\tconfig = load_config()\n\t\n\t# Test each configuration value\n\tprint(\"Testing Config class:\")\n\tprint(f\"JINA_API_KEY: {'Set' if config.JINA_API_KEY else 'Not set'}\")\n\tprint(f\"GEMINI_API_KEY: {'Set' if config.GEMINI_API_KEY else 'Not set'}\")\n\tprint(f\"OPENAI_API_KEY: {'Set' if config.OPENAI_API_KEY else 'Not set'}\")\n\tprint(f\"ELEVENLABS_API_KEY: {'Set' if config.ELEVENLABS_API_KEY else 'Not set'}\")\n\n\t# Print a warning for any missing configuration\n\tmissing_config = []\n\tfor key in ['JINA_API_KEY', 'GEMINI_API_KEY', 'OPENAI_API_KEY', 'ELEVENLABS_API_KEY']:\n\t\tif not getattr(config, key):\n\t\t\tmissing_config.append(key)\n\n\tif missing_config:\n\t\tprint(\"\\nWarning: The following configuration values are missing:\")\n\t\tfor config_name in missing_config:\n\t\t\tprint(f\"- {config_name}\")\n\t\tprint(\"Please ensure these are set in your .env file.\")\n\telse:\n\t\tprint(\"\\nAll configuration values are set.\")\n\n\t# Test the get method with a default value\n\tprint(f\"\\nTesting get method with default value:\")\n\tprint(f\"NON_EXISTENT_KEY: {config.get('NON_EXISTENT_KEY', 'Default Value')}\")\n\nif __name__ == \"__main__\":\n\tmain()"}
{"type": "source_file", "path": "podcastfy/tts/providers/geminimulti.py", "content": "\"\"\"Google Cloud Text-to-Speech provider implementation.\"\"\"\n\nfrom google.cloud import texttospeech_v1beta1\nfrom typing import List\nfrom ..base import TTSProvider\nimport re\nimport logging\nfrom io import BytesIO\nfrom pydub import AudioSegment\n\nlogger = logging.getLogger(__name__)\n\nclass GeminiMultiTTS(TTSProvider):\n    \"\"\"Google Cloud Text-to-Speech provider with multi-speaker support.\"\"\"\n    \n    def __init__(self, api_key: str = None, model: str = \"en-US-Studio-MultiSpeaker\"):\n        \"\"\"\n        Initialize Google Cloud TTS provider.\n        \n        Args:\n            api_key (str): Google Cloud API key\n        \"\"\"\n        self.model = model\n        try:\n            self.client = texttospeech_v1beta1.TextToSpeechClient(\n                client_options={'api_key': api_key} if api_key else None\n            )\n            logger.info(\"Successfully initialized GeminiMultiTTS client\")\n        except Exception as e:\n            logger.error(f\"Failed to initialize GeminiMultiTTS client: {str(e)}\")\n            raise\n            \n    def chunk_text(self, text: str, max_bytes: int = 1300) -> List[str]:\n        \"\"\"\n        Split text into chunks that fit within Google TTS byte limit while preserving speaker tags.\n        \n        Args:\n            text (str): Input text with Person1/Person2 tags\n            max_bytes (int): Maximum bytes per chunk\n            \n        Returns:\n            List[str]: List of text chunks with proper speaker tags preserved\n        \"\"\"\n        logger.debug(f\"Starting chunk_text with text length: {len(text)} bytes\")\n        \n        # Split text into tagged sections, preserving both Person1 and Person2 tags\n        pattern = r'(<Person[12]>.*?</Person[12]>)'\n        sections = re.split(pattern, text, flags=re.DOTALL)\n        sections = [s.strip() for s in sections if s.strip()]\n        logger.debug(f\"Split text into {len(sections)} sections\")\n        \n        chunks = []\n        current_chunk = \"\"\n        \n        for section in sections:\n            # Extract speaker tag and content if this is a tagged section\n            tag_match = re.match(r'<(Person[12])>(.*?)</Person[12]>', section, flags=re.DOTALL)\n            \n            if tag_match:\n                speaker_tag = tag_match.group(1)  # Will be either Person1 or Person2\n                content = tag_match.group(2).strip()\n                \n                # Test if adding this entire section would exceed limit\n                test_chunk = current_chunk\n                if current_chunk:\n                    test_chunk += f\"<{speaker_tag}>{content}</{speaker_tag}>\"\n                else:\n                    test_chunk = f\"<{speaker_tag}>{content}</{speaker_tag}>\"\n                    \n                if len(test_chunk.encode('utf-8')) > max_bytes and current_chunk:\n                    # Store current chunk and start new one\n                    chunks.append(current_chunk)\n                    current_chunk = f\"<{speaker_tag}>{content}</{speaker_tag}>\"\n                else:\n                    # Add to current chunk\n                    current_chunk = test_chunk\n        \n        # Add final chunk if it exists\n        if current_chunk:\n            chunks.append(current_chunk)\n            \n        logger.info(f\"Created {len(chunks)} chunks from input text\")\n        return chunks\n\n    def split_turn_text(self, text: str, max_chars: int = 500) -> List[str]:\n        \"\"\"\n        Split turn text into smaller chunks at sentence boundaries.\n        \n        Args:\n            text (str): Text content of a single turn\n            max_chars (int): Maximum characters per chunk\n            \n        Returns:\n            List[str]: List of text chunks\n        \"\"\"\n        #print(f\"### TEXT: {text}\" )\n        #print(f\"### LENGTH: {len(text)}\")\n        if len(text) <= max_chars:\n            return [text]\n        \n        chunks = []\n        sentences = re.split(r'([.!?]+(?:\\s+|$))', text)\n        sentences = [s for s in sentences if s]\n        \n        current_chunk = \"\"\n        for i in range(0, len(sentences), 2):\n            sentence = sentences[i]\n            separator = sentences[i + 1] if i + 1 < len(sentences) else \"\"\n            complete_sentence = sentence + separator\n            \n            if len(current_chunk) + len(complete_sentence) > max_chars:\n                if current_chunk:\n                    chunks.append(current_chunk.strip())\n                    current_chunk = complete_sentence\n                else:\n                    # If a single sentence is too long, split at word boundaries\n                    words = complete_sentence.split()\n                    temp_chunk = \"\"\n                    for word in words:\n                        if len(temp_chunk) + len(word) + 1 > max_chars:\n                            chunks.append(temp_chunk.strip())\n                            temp_chunk = word\n                        else:\n                            temp_chunk += \" \" + word if temp_chunk else word\n                    current_chunk = temp_chunk\n            else:\n                current_chunk += complete_sentence\n                \n        if current_chunk:\n            chunks.append(current_chunk.strip())\n            \n        return chunks\n\n    def merge_audio(self, audio_chunks: List[bytes]) -> bytes:\n        \"\"\"\n        Merge multiple MP3 audio chunks into a single audio file.\n        \n        Args:\n            audio_chunks (List[bytes]): List of MP3 audio data\n            \n        Returns:\n            bytes: Combined MP3 audio data\n        \"\"\"\n        if not audio_chunks:\n            return b\"\"\n        \n        if len(audio_chunks) == 1:\n            return audio_chunks[0]\n        \n        try:\n            # Initialize combined audio with first chunk\n            combined = None\n            valid_chunks = []\n            \n            for i, chunk in enumerate(audio_chunks):\n                try:\n                    # Ensure chunk is not empty\n                    if not chunk or len(chunk) == 0:\n                        logger.warning(f\"Skipping empty chunk {i}\")\n                        continue\n                    \n                    # Save chunk to temporary file for ffmpeg to process\n                    temp_file = f\"temp_chunk_{i}.mp3\"\n                    with open(temp_file, \"wb\") as f:\n                        f.write(chunk)\n                    \n                    # Create audio segment from temp file\n                    try:\n                        segment = AudioSegment.from_file(temp_file, format=\"mp3\")\n                        if len(segment) > 0:\n                            valid_chunks.append(segment)\n                            logger.debug(f\"Successfully processed chunk {i}\")\n                        else:\n                            logger.warning(f\"Zero-length segment in chunk {i}\")\n                    except Exception as e:\n                        logger.error(f\"Error processing chunk {i}: {str(e)}\")\n                    \n                    # Clean up temp file\n                    import os\n                    try:\n                        os.remove(temp_file)\n                    except Exception as e:\n                        logger.warning(f\"Failed to remove temp file {temp_file}: {str(e)}\")\n                    \n                except Exception as e:\n                    logger.error(f\"Error handling chunk {i}: {str(e)}\")\n                    continue\n            \n            if not valid_chunks:\n                raise RuntimeError(\"No valid audio chunks to merge\")\n            \n            # Merge valid chunks\n            combined = valid_chunks[0]\n            for segment in valid_chunks[1:]:\n                combined = combined + segment\n            \n            # Export with specific parameters\n            output = BytesIO()\n            combined.export(\n                output,\n                format=\"mp3\",\n                codec=\"libmp3lame\",\n                bitrate=\"320k\"\n            )\n            \n            result = output.getvalue()\n            if len(result) == 0:\n                raise RuntimeError(\"Export produced empty output\")\n            \n            return result\n            \n        except Exception as e:\n            logger.error(f\"Audio merge failed: {str(e)}\", exc_info=True)\n            # If merging fails, return the first valid chunk as fallback\n            if audio_chunks:\n                return audio_chunks[0]\n            raise RuntimeError(f\"Failed to merge audio chunks and no valid fallback found: {str(e)}\")\n\n    def generate_audio(self, text: str, voice: str = \"R\", model: str = \"en-US-Studio-MultiSpeaker\", \n                       voice2: str = \"S\", ending_message: str = \"\"):\n        \"\"\"\n        Generate audio using Google Cloud TTS API with multi-speaker support.\n        Handles text longer than 5000 bytes by chunking and merging.\n        \"\"\"\n        logger.info(f\"Starting audio generation for text of length: {len(text)}\")\n        logger.debug(f\"Parameters: voice={voice}, voice2={voice2}, model={model}\")\n        #print(\"######################### TEXT #########################\")\n        #print(text)\n        #print(\"######################### END TEXT #########################\")\n        try:\n            # Split text into chunks if needed\n            text_chunks = self.chunk_text(text)\n            logger.info(f\"#########################33 Text split into {len(text_chunks)} chunks\")\n            audio_chunks = []\n            #print(text_chunks[0])\n            \n            # Process each chunk\n            for i, chunk in enumerate(text_chunks, 1):\n                logger.debug(f\"Processing chunk {i}/{len(text_chunks)}\")\n                # Create multi-speaker markup\n                multi_speaker_markup = texttospeech_v1beta1.MultiSpeakerMarkup()\n                #print(\"######################### CHUNK #########################\")\n                #print(chunk)\n                # Get Q&A pairs for this chunk\n                qa_pairs = self.split_qa(chunk, \"\", self.get_supported_tags())\n                logger.debug(f\"Found {len(qa_pairs)} Q&A pairs in chunk {i}\")\n                #print(\"######################### QA PAIRS #########################\")\n                #print(qa_pairs)\n                # Add turns for each Q&A pair\n                for j, (question, answer) in enumerate(qa_pairs, 1):\n                    logger.debug(f\"Processing Q&A pair {j}/{len(qa_pairs)}\")\n                    \n                    # Split question into smaller chunks if needed\n                    question_chunks = self.split_turn_text(question.strip())\n                    logger.debug(f\"Question split into {len(question_chunks)} chunks\")\n                    logger.debug(f\"######################### Question chunks: {question_chunks}\")\n                    for q_chunk in question_chunks:\n                        logger.debug(f\"Adding question turn: '{q_chunk[:50]}...' (length: {len(q_chunk)})\")\n                        q_turn = texttospeech_v1beta1.MultiSpeakerMarkup.Turn()\n                        q_turn.text = q_chunk\n                        q_turn.speaker = voice\n                        multi_speaker_markup.turns.append(q_turn)\n                    \n                    # Split answer into smaller chunks if needed\n                    if answer:\n                        answer_chunks = self.split_turn_text(answer.strip())\n                        logger.debug(f\"Answer split into {len(answer_chunks)} chunks\")\n                        logger.debug(f\"######################### Answer chunks: {answer_chunks}\")\n                        for a_chunk in answer_chunks:\n                            logger.debug(f\"Adding answer turn: '{a_chunk[:50]}...' (length: {len(a_chunk)})\")\n                            a_turn = texttospeech_v1beta1.MultiSpeakerMarkup.Turn()\n                            a_turn.text = a_chunk\n                            a_turn.speaker = voice2\n                            multi_speaker_markup.turns.append(a_turn)\n                \n                logger.debug(f\"Created markup with {len(multi_speaker_markup.turns)} turns\")\n                \n                # Create synthesis input with multi-speaker markup\n                synthesis_input = texttospeech_v1beta1.SynthesisInput(\n                    multi_speaker_markup=multi_speaker_markup\n                )\n                \n                logger.debug(\"Calling synthesize_speech API\")\n                # Set voice parameters\n                voice_params = texttospeech_v1beta1.VoiceSelectionParams(\n                    language_code=\"en-US\",\n                    name=model\n                )\n                \n                # Set audio config\n                audio_config = texttospeech_v1beta1.AudioConfig(\n                    audio_encoding=texttospeech_v1beta1.AudioEncoding.MP3,\n                    #sample_rate_hertz=44100,  # Specify sample rate\n                    #effects_profile_id=['headphone-class-device'],  # Optimize for headphones\n                    #speaking_rate=1.0,  # Normal speaking rate\n                )\n                \n                # Generate speech for this chunk\n                response = self.client.synthesize_speech(\n                    input=synthesis_input,\n                    voice=voice_params,\n                    audio_config=audio_config\n                )\n\n                audio_chunks.append(response.audio_content)\n            #print(f\"#### Audio chunks: {audio_chunks}\")\n            #print(f\"#### Audio chunks length: {len(audio_chunks)}\")\n            return audio_chunks\n        \n            \n        except Exception as e:\n            logger.error(f\"Failed to generate audio: {str(e)}\", exc_info=True)\n            raise RuntimeError(f\"Failed to generate audio: {str(e)}\") from e\n    \n    def get_supported_tags(self) -> List[str]:\n        \"\"\"Get supported SSML tags.\"\"\"\n        # Add any Google-specific SSML tags to the common ones\n        return self.COMMON_SSML_TAGS\n        \n    def validate_parameters(self, text: str, voice: str, model: str) -> None:\n        \"\"\"\n        Validate input parameters before generating audio.\n        \n        Args:\n            text (str): Input text\n            voice (str): Voice ID\n            model (str): Model name\n            \n        Raises:\n            ValueError: If parameters are invalid\n        \"\"\"\n        super().validate_parameters(text, voice, model)\n        \n        # Additional validation for multi-speaker model\n        if model != \"en-US-Studio-MultiSpeaker\":\n            raise ValueError(\n                \"Google Multi-speaker TTS requires model='en-US-Studio-MultiSpeaker'\"\n            )"}
{"type": "source_file", "path": "podcastfy/tts/providers/gemini.py", "content": "\"\"\"Google Cloud Text-to-Speech provider implementation for single speaker.\"\"\"\n\nfrom google.cloud import texttospeech_v1beta1\nfrom typing import List\nfrom ..base import TTSProvider\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nclass GeminiTTS(TTSProvider):\n    \"\"\"Google Cloud Text-to-Speech provider for single speaker.\"\"\"\n    \n    def __init__(self, api_key: str = None, model: str = \"en-US-Journey-F\"):\n        \"\"\"\n        Initialize Google Cloud TTS provider.\n        \n        Args:\n            api_key (str): Google Cloud API key\n            model (str): Default voice model to use\n        \"\"\"\n        self.model = model\n        try:\n            self.client = texttospeech_v1beta1.TextToSpeechClient(\n                client_options={'api_key': api_key} if api_key else None\n            )\n        except Exception as e:\n            logger.error(f\"Failed to initialize Google TTS client: {str(e)}\")\n            raise\n\n    def generate_audio(self, text: str, voice: str = \"en-US-Journey-F\", \n                      model: str = None, **kwargs) -> bytes:\n        \"\"\"\n        Generate audio using Google Cloud TTS API.\n        \n        Args:\n            text (str): Text to convert to speech\n            voice (str): Voice ID/name to use (format: \"{language-code}-{name}-{gender}\")\n            model (str): Optional model override\n            \n        Returns:\n            bytes: Audio data\n            \n        Raises:\n            ValueError: If parameters are invalid\n            RuntimeError: If audio generation fails\n        \"\"\"\n        self.validate_parameters(text, voice, model or self.model)\n        \n        try:\n            # Create synthesis input\n            synthesis_input = texttospeech_v1beta1.SynthesisInput(\n                text=text\n            )\n            \n            # Parse language code from voice ID (e.g., \"en-IN\" from \"en-IN-Journey-D\")\n            language_code = \"-\".join(voice.split(\"-\")[:2])\n\n            voice_params = texttospeech_v1beta1.VoiceSelectionParams(\n                language_code=language_code,\n                name=voice,\n            )\n            \n            # Set audio config\n            audio_config = texttospeech_v1beta1.AudioConfig(\n                audio_encoding=texttospeech_v1beta1.AudioEncoding.MP3\n            )\n            \n            # Generate speech\n            response = self.client.synthesize_speech(\n                input=synthesis_input,\n                voice=voice_params,\n                audio_config=audio_config\n            )\n            \n            return response.audio_content\n            \n        except Exception as e:\n            logger.error(f\"Failed to generate audio: {str(e)}\")\n            raise RuntimeError(f\"Failed to generate audio: {str(e)}\") from e\n    \n    def get_supported_tags(self) -> List[str]:\n        \"\"\"Get supported SSML tags.\"\"\"\n        return self.COMMON_SSML_TAGS\n        \n    def validate_parameters(self, text: str, voice: str, model: str) -> None:\n        \"\"\"\n        Validate input parameters before generating audio.\n        \n        Args:\n            text (str): Input text\n            voice (str): Voice ID/name\n            model (str): Model name\n            \n        Raises:\n            ValueError: If parameters are invalid\n        \"\"\"\n        super().validate_parameters(text, voice, model)\n        \n        if not text:\n            raise ValueError(\"Text cannot be empty\")\n        \n        if not voice:\n            raise ValueError(\"Voice must be specified\")"}
{"type": "source_file", "path": "podcastfy/utils/config_conversation.py", "content": "\"\"\"\nConversation Configuration Module\n\nThis module handles the loading and management of conversation configuration settings\nfor the Podcastfy application. It uses a YAML file for conversation-specific configuration settings.\n\"\"\"\n\nimport os\nimport sys\nfrom typing import Any, Dict, Optional, List\nimport yaml\n\ndef get_conversation_config_path(config_file: str = 'conversation_config.yaml'):\n\t\"\"\"\n\tGet the path to the conversation_config.yaml file.\n\t\n\tReturns:\n\t\tstr: The path to the conversation_config.yaml file.\n\t\"\"\"\n\ttry:\n\t\t# Check if the script is running in a PyInstaller bundle\n\t\tif getattr(sys, 'frozen', False):\n\t\t\tbase_path = sys._MEIPASS\n\t\telse:\n\t\t\tbase_path = os.path.dirname(os.path.abspath(__file__))\n\t\t\n\t\t# Look for conversation_config.yaml in the same directory as the script\n\t\tconfig_path = os.path.join(base_path, config_file)\n\t\tif os.path.exists(config_path):\n\t\t\treturn config_path\n\t\t\n\t\t# If not found, look in the parent directory (package root)\n\t\tconfig_path = os.path.join(os.path.dirname(base_path), config_file)\n\t\tif os.path.exists(config_path):\n\t\t\treturn config_path\n\t\t\n\t\t# If still not found, look in the current working directory\n\t\tconfig_path = os.path.join(os.getcwd(), config_file)\n\t\tif os.path.exists(config_path):\n\t\t\treturn config_path\n\t\t\n\t\traise FileNotFoundError(f\"{config_file} not found\")\n\t\n\texcept Exception as e:\n\t\tprint(f\"Error locating {config_file}: {str(e)}\")\n\t\treturn None\n\nclass NestedConfig:\n\t\"\"\"\n\tA class to handle nested configuration objects with proper method inheritance.\n\t\"\"\"\n\tdef __init__(self, config_dict: Dict[str, Any]):\n\t\t\"\"\"\n\t\tInitialize a nested configuration object.\n\n\t\tArgs:\n\t\t\tconfig_dict (Dict[str, Any]): Dictionary containing the nested configuration\n\t\t\"\"\"\n\t\tfor key, value in config_dict.items():\n\t\t\tif isinstance(value, dict):\n\t\t\t\tsetattr(self, key, NestedConfig(value))\n\t\t\telse:\n\t\t\t\tsetattr(self, key, value)\n\t\n\tdef to_dict(self) -> Dict[str, Any]:\n\t\t\"\"\"\n\t\tConvert the NestedConfig object to a dictionary, preserving nested structure.\n\n\t\tReturns:\n\t\t\tDict[str, Any]: A dictionary representation of the configuration\n\t\t\"\"\"\n\t\tresult = {}\n\t\tfor key, value in self.__dict__.items():\n\t\t\tif not key.startswith('_'):\n\t\t\t\tif isinstance(value, NestedConfig):\n\t\t\t\t\tresult[key] = value.to_dict()\n\t\t\t\telse:\n\t\t\t\t\tresult[key] = value\n\t\treturn result\n\t\n\tdef get(self, key: str, default: Optional[Any] = None) -> Any:\n\t\t\"\"\"\n\t\tGet a configuration value by key, supporting nested keys with dot notation.\n\n\t\tArgs:\n\t\t\tkey (str): The configuration key to retrieve (e.g., 'child.value')\n\t\t\tdefault (Optional[Any]): The default value if the key is not found.\n\n\t\tReturns:\n\t\t\tAny: The value associated with the key, or the default value if not found.\n\t\t\"\"\"\n\t\tcurrent = self\n\t\ttry:\n\t\t\tfor part in key.split('.'):\n\t\t\t\tif isinstance(current, dict):\n\t\t\t\t\tcurrent = current[part]\n\t\t\t\telse:\n\t\t\t\t\tcurrent = getattr(current, part)\n\t\t\treturn current\n\t\texcept (AttributeError, KeyError):\n\t\t\treturn default\n\n\tdef get_list(self, key: str, default: Optional[List[str]] = None) -> List[str]:\n\t\t\"\"\"\n\t\tGet a list configuration value by key, supporting nested keys with dot notation.\n\n\t\tArgs:\n\t\t\tkey (str): The configuration key to retrieve (e.g., 'child.list')\n\t\t\tdefault (Optional[List[str]]): The default value if the key is not found.\n\n\t\tReturns:\n\t\t\tList[str]: The list associated with the key, or the default value if not found.\n\t\t\"\"\"\n\t\tvalue = self.get(key, default)\n\t\tif isinstance(value, str):\n\t\t\treturn [item.strip() for item in value.split(',')]\n\t\treturn value if isinstance(value, list) else default or []\n\n\tdef configure(self, config: Dict[str, Any]) -> None:\n\t\t\"\"\"\n\t\tConfigure the settings with the provided dictionary.\n\n\t\tArgs:\n\t\t\tconfig (Dict[str, Any]): Configuration dictionary to update the settings.\n\t\t\"\"\"\n\t\tfor key, value in config.items():\n\t\t\tif isinstance(value, dict) and hasattr(self, key) and isinstance(getattr(self, key), NestedConfig):\n\t\t\t\tgetattr(self, key).configure(value)\n\t\t\telse:\n\t\t\t\tsetattr(self, key, value)\n\nclass ConversationConfig(NestedConfig):\n\tdef __init__(self, config_conversation: Optional[Dict[str, Any]] = None):\n\t\t\"\"\"\n\t\tInitialize the ConversationConfig class with a dictionary configuration.\n\n\t\tArgs:\n\t\t\tconfig_conversation (Optional[Dict[str, Any]]): Configuration dictionary. If None, default config will be used.\n\t\t\"\"\"\n\t\t# Load default configuration\n\t\tself.config_conversation = self._load_default_config()\n\t\tif config_conversation is not None:\n\t\t\timport copy\n\t\t\t\n\t\t\t# Create a deep copy of the default configuration\n\t\t\tself.config_conversation = copy.deepcopy(self.config_conversation)\n\t\t\t\n\t\t\t# Update the configuration with provided values\n\t\t\tif isinstance(config_conversation, dict):\n\t\t\t\tself._deep_update(self.config_conversation, config_conversation)\n\t\t\telse:\n\t\t\t\tprint(\"Warning: config_conversation should be a dictionary.\")\n\t\t\n\t\t# Initialize the NestedConfig with the configuration\n\t\tsuper().__init__(self.config_conversation)\n\n\tdef _load_default_config(self) -> Dict[str, Any]:\n\t\t\"\"\"Load the default configuration from conversation_config.yaml.\"\"\"\n\t\tconfig_path = get_conversation_config_path()\n\t\tif config_path:\n\t\t\twith open(config_path, 'r') as file:\n\t\t\t\treturn yaml.safe_load(file)\n\t\telse:\n\t\t\traise FileNotFoundError(\"conversation_config.yaml not found\")\n\n\tdef _deep_update(self, target: Dict[str, Any], source: Dict[str, Any]) -> None:\n\t\t\"\"\"\n\t\tRecursively update a nested dictionary.\n\n\t\tArgs:\n\t\t\ttarget (Dict[str, Any]): The dictionary to update\n\t\t\tsource (Dict[str, Any]): The dictionary containing updates\n\t\t\"\"\"\n\t\tfor key, value in source.items():\n\t\t\tif key == 'config_conversation':\n\t\t\t\tself._deep_update(target, value)\n\t\t\telif isinstance(value, dict) and key in target and isinstance(target[key], dict):\n\t\t\t\tself._deep_update(target[key], value)\n\t\t\telse:\n\t\t\t\ttarget[key] = value\n\n\tdef to_dict(self) -> Dict[str, Any]:\n\t\t\"\"\"\n\t\tConvert the ConversationConfig object to a dictionary, preserving nested structure.\n\n\t\tReturns:\n\t\t\tDict[str, Any]: A dictionary representation of the configuration\n\t\t\"\"\"\n\t\tresult = {}\n\t\tfor key, value in self.__dict__.items():\n\t\t\tif not key.startswith('_'):\n\t\t\t\tif isinstance(value, (ConversationConfig, NestedConfig)):\n\t\t\t\t\tresult[key] = value.to_dict()\n\t\t\t\telse:\n\t\t\t\t\tresult[key] = value\n\t\treturn result\n\ndef load_conversation_config(config_conversation: Optional[Dict[str, Any]] = None) -> ConversationConfig:\n\t\"\"\"\n\tLoad and return a ConversationConfig instance.\n\n\tArgs:\n\t\tconfig_conversation (Optional[Dict[str, Any]]): Configuration dictionary to use. If None, default config will be used.\n\n\tReturns:\n\t\tConversationConfig: An instance of the ConversationConfig class.\n\t\"\"\"\n\treturn ConversationConfig(config_conversation)\n\ndef main() -> None:\n\t\"\"\"\n\tTest the ConversationConfig class and print configuration status.\n\t\"\"\"\n\ttry:\n\t\t# Create an instance of the ConversationConfig class with default settings\n\t\tdefault_config = load_conversation_config()\n\t\t\n\t\tprint(\"Default Configuration:\")\n\t\tfor key, value in default_config.config_conversation.items():\n\t\t\tprint(f\"{key}: {value}\")\n\n\t\t# Test with custom configuration\n\t\tcustom_config = {\n\t\t\t\"word_count\": 1500,\n\t\t\t\"podcast_name\": \"Custom Podcast\",\n\t\t\t\"output_language\": \"Spanish\"\n\t\t}\n\t\tcustom_config_instance = load_conversation_config(custom_config)\n\n\t\tprint(\"\\nCustom Configuration:\")\n\t\tfor key, value in custom_config_instance.config_conversation.items():\n\t\t\tprint(f\"{key}: {value}\")\n\n\t\t# Test the get method with a default value\n\t\tprint(f\"\\nTesting get method with default value:\")\n\t\tprint(f\"NON_EXISTENT_KEY: {custom_config_instance.get('NON_EXISTENT_KEY', 'Default Value')}\")\n\n\texcept FileNotFoundError as e:\n\t\tprint(f\"Error: {str(e)}\")\n\texcept Exception as e:\n\t\tprint(f\"An unexpected error occurred: {str(e)}\")\n\nif __name__ == \"__main__\":\n\tmain()\n"}
{"type": "source_file", "path": "podcastfy/utils/logger.py", "content": "\"\"\"\nLogger Module\n\nThis module provides a utility function to set up and configure a logger for the Podcastfy application.\nIt ensures consistent logging format and configuration across the application.\n\"\"\"\n\nimport logging\nfrom typing import Any\nfrom podcastfy.utils.config import load_config\n\ndef setup_logger(name: str) -> logging.Logger:\n    \"\"\"\n    Set up and configure a logger.\n\n    Args:\n        name (str): The name of the logger.\n\n    Returns:\n        logging.Logger: A configured logger instance.\n    \"\"\"\n    config = load_config()\n    logging_config = config.get('logging')\n\n    logger = logging.getLogger(name)\n    logger.setLevel(logging_config['level'])\n    \n    formatter = logging.Formatter(logging_config['format'])\n    \n    console_handler = logging.StreamHandler()\n    console_handler.setFormatter(formatter)\n    \n    logger.addHandler(console_handler)\n    \n    return logger"}
{"type": "source_file", "path": "usage/fast_api_example.py", "content": "\"\"\"\nExample implementation of the Podcastify FastAPI client.\n\nThis module demonstrates how to interact with the Podcastify API\nto generate and download podcasts.\n\"\"\"\n\nimport asyncio\nimport aiohttp\nimport json\nimport os\nfrom pathlib import Path\nfrom typing import Dict, Any\n\n\ndef get_default_config() -> Dict[str, Any]:\n\t\"\"\"\n\tReturns default configuration for podcast generation.\n\n\tReturns:\n\t\tDict[str, Any]: Default configuration dictionary\n\t\"\"\"\n\treturn {\n\t\t\"generate_podcast\": True,\n\t\t\"google_key\": \"YOUR_GEMINI_API_KEY\",\n\t\t\"openai_key\": \"YOUR_OPENAI_API_KEY\",\n\t\t\"urls\": [\"https://www.phenomenalworld.org/interviews/swap-structure/\"],\n\t\t\"name\": \"Central Clearing Risks\",\n\t\t\"tagline\": \"Exploring the complexities of financial systemic risk\",\n\t\t\"creativity\": 0.8,\n\t\t\"conversation_style\": [\"engaging\", \"informative\"],\n\t\t\"roles_person1\": \"main summarizer\",\n\t\t\"roles_person2\": \"questioner\",\n\t\t\"dialogue_structure\": [\"Introduction\", \"Content\", \"Conclusion\"],\n\t\t\"tts_model\": \"openai\",\n\t\t\"is_long_form\": False,\n\t\t\"engagement_techniques\": [\"questions\", \"examples\", \"analogies\"],\n\t\t\"user_instructions\": \"Dont use the world Dwelve\",\n\t\t\"output_language\": \"English\"\n\t}\n\n\nasync def generate_podcast() -> None:\n\t\"\"\"\n\tGenerates a podcast using the Podcastify API and downloads the result.\n\t\"\"\"\n\tasync with aiohttp.ClientSession() as session:\n\t\ttry:\n\t\t\tprint(\"Starting podcast generation...\")\n\t\t\tasync with session.post(\n\t\t\t\t\"http://localhost:8080/generate\",\n\t\t\t\tjson=get_default_config()\n\t\t\t) as response:\n\t\t\t\tif response.status != 200:\n\t\t\t\t\tprint(f\"Error: Server returned status {response.status}\")\n\t\t\t\t\treturn\n\t\t\t\t\n\t\t\t\tresult = await response.json()\n\t\t\t\tif \"error\" in result:\n\t\t\t\t\tprint(f\"Error: {result['error']}\")\n\t\t\t\t\treturn\n\n\t\t\t\tawait download_podcast(session, result)\n\n\t\texcept aiohttp.ClientError as e:\n\t\t\tprint(f\"Network error: {str(e)}\")\n\t\texcept Exception as e:\n\t\t\tprint(f\"Unexpected error: {str(e)}\")\n\n\nasync def download_podcast(session: aiohttp.ClientSession, result: Dict[str, str]) -> None:\n\t\"\"\"\n\tDownloads the generated podcast file.\n\n\tArgs:\n\t\tsession (aiohttp.ClientSession): Active client session\n\t\tresult (Dict[str, str]): API response containing audioUrl\n\t\"\"\"\n\taudio_url = f\"http://localhost:8080{result['audioUrl']}\"\n\tprint(f\"Podcast generated! Downloading from: {audio_url}\")\n\n\tasync with session.get(audio_url) as audio_response:\n\t\tif audio_response.status == 200:\n\t\t\tfilename = os.path.join(\n\t\t\t\tstr(Path.home() / \"Downloads\"), \n\t\t\t\tresult['audioUrl'].split('/')[-1]\n\t\t\t)\n\t\t\twith open(filename, 'wb') as f:\n\t\t\t\tf.write(await audio_response.read())\n\t\t\tprint(f\"Downloaded to: {filename}\")\n\t\telse:\n\t\t\tprint(f\"Failed to download audio. Status: {audio_response.status}\")\n\n\nif __name__ == \"__main__\":\n\ttry:\n\t\tasyncio.run(generate_podcast())\n\texcept KeyboardInterrupt:\n\t\tprint(\"\\nProcess interrupted by user\")\n\texcept Exception as e:\n\t\tprint(f\"Error: {str(e)}\")"}
