{"repo_info": {"repo_name": "HiGPT", "repo_owner": "HKUDS", "repo_url": "https://github.com/HKUDS/HiGPT"}}
{"type": "test_file", "path": "higpt/serve/test_message.py", "content": "\"\"\"Send a test message.\"\"\"\nimport argparse\nimport json\n\nimport requests\n\nfrom fastchat.model.model_adapter import get_conversation_template\n\n\ndef main():\n    model_name = args.model_name\n\n    if args.worker_address:\n        worker_addr = args.worker_address\n    else:\n        controller_addr = args.controller_address\n        ret = requests.post(controller_addr + \"/refresh_all_workers\")\n        ret = requests.post(controller_addr + \"/list_models\")\n        models = ret.json()[\"models\"]\n        models.sort()\n        print(f\"Models: {models}\")\n\n        ret = requests.post(\n            controller_addr + \"/get_worker_address\", json={\"model\": model_name}\n        )\n        worker_addr = ret.json()[\"address\"]\n        print(f\"worker_addr: {worker_addr}\")\n\n    if worker_addr == \"\":\n        print(f\"No available workers for {model_name}\")\n        return\n\n    conv = get_conversation_template(model_name)\n    conv.append_message(conv.roles[0], args.message)\n    conv.append_message(conv.roles[1], None)\n    prompt = conv.get_prompt()\n\n    headers = {\"User-Agent\": \"FastChat Client\"}\n    gen_params = {\n        \"model\": model_name,\n        \"prompt\": prompt,\n        \"temperature\": args.temperature,\n        \"max_new_tokens\": args.max_new_tokens,\n        \"stop\": conv.stop_str,\n        \"stop_token_ids\": conv.stop_token_ids,\n        \"echo\": False,\n    }\n    response = requests.post(\n        worker_addr + \"/worker_generate_stream\",\n        headers=headers,\n        json=gen_params,\n        stream=True,\n    )\n\n    print(f\"{conv.roles[0]}: {args.message}\")\n    print(f\"{conv.roles[1]}: \", end=\"\")\n    prev = 0\n    for chunk in response.iter_lines(decode_unicode=False, delimiter=b\"\\0\"):\n        if chunk:\n            data = json.loads(chunk.decode())\n            output = data[\"text\"].strip()\n            print(output[prev:], end=\"\", flush=True)\n            prev = len(output)\n    print(\"\")\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--controller-address\", type=str, default=\"http://localhost:21001\"\n    )\n    parser.add_argument(\"--worker-address\", type=str)\n    parser.add_argument(\"--model-name\", type=str, required=True)\n    parser.add_argument(\"--temperature\", type=float, default=0.0)\n    parser.add_argument(\"--max-new-tokens\", type=int, default=32)\n    parser.add_argument(\n        \"--message\", type=str, default=\"Tell me a story with more than 1000 words.\"\n    )\n    args = parser.parse_args()\n\n    main()\n"}
{"type": "test_file", "path": "higpt/serve/test_throughput.py", "content": "\"\"\"Benchmarking script to test the throughput of serving workers.\"\"\"\nimport argparse\nimport json\n\nimport requests\nimport threading\nimport time\n\nfrom fastchat.conversation import default_conversation\n\n\ndef main():\n    if args.worker_address:\n        worker_addr = args.worker_address\n    else:\n        controller_addr = args.controller_address\n        ret = requests.post(controller_addr + \"/refresh_all_workers\")\n        ret = requests.post(controller_addr + \"/list_models\")\n        models = ret.json()[\"models\"]\n        models.sort()\n        print(f\"Models: {models}\")\n\n        ret = requests.post(\n            controller_addr + \"/get_worker_address\", json={\"model\": args.model_name}\n        )\n        worker_addr = ret.json()[\"address\"]\n        print(f\"worker_addr: {worker_addr}\")\n\n    if worker_addr == \"\":\n        return\n\n    conv = default_conversation.copy()\n    conv.append_message(conv.roles[0], \"Tell me a story with more than 1000 words\")\n    prompt_template = conv.get_prompt()\n    prompts = [prompt_template for _ in range(args.n_thread)]\n\n    headers = {\"User-Agent\": \"fastchat Client\"}\n    ploads = [\n        {\n            \"model\": args.model_name,\n            \"prompt\": prompts[i],\n            \"max_new_tokens\": args.max_new_tokens,\n            \"temperature\": 0.0,\n            # \"stop\": conv.sep,\n        }\n        for i in range(len(prompts))\n    ]\n\n    def send_request(results, i):\n        if args.test_dispatch:\n            ret = requests.post(\n                controller_addr + \"/get_worker_address\", json={\"model\": args.model_name}\n            )\n            thread_worker_addr = ret.json()[\"address\"]\n        else:\n            thread_worker_addr = worker_addr\n        print(f\"thread {i} goes to {thread_worker_addr}\")\n        response = requests.post(\n            thread_worker_addr + \"/worker_generate_stream\",\n            headers=headers,\n            json=ploads[i],\n            stream=False,\n        )\n        k = list(\n            response.iter_lines(chunk_size=8192, decode_unicode=False, delimiter=b\"\\0\")\n        )\n        # print(k)\n        response_new_words = json.loads(k[-2].decode(\"utf-8\"))[\"text\"]\n        error_code = json.loads(k[-2].decode(\"utf-8\"))[\"error_code\"]\n        # print(f\"=== Thread {i} ===, words: {1}, error code: {error_code}\")\n        results[i] = len(response_new_words.split(\" \")) - len(prompts[i].split(\" \"))\n\n    # use N threads to prompt the backend\n    tik = time.time()\n    threads = []\n    results = [None] * args.n_thread\n    for i in range(args.n_thread):\n        t = threading.Thread(target=send_request, args=(results, i))\n        t.start()\n        # time.sleep(0.5)\n        threads.append(t)\n\n    for t in threads:\n        t.join()\n\n    print(f\"Time (POST): {time.time() - tik} s\")\n    # n_words = 0\n    # for i, response in enumerate(results):\n    #     # print(prompt[i].replace(conv.sep, \"\\n\"), end=\"\")\n    #     # make sure the streaming finishes at EOS or stopping criteria\n    #     k = list(response.iter_lines(chunk_size=8192, decode_unicode=False, delimiter=b\"\\0\"))\n    #     response_new_words = json.loads(k[-2].decode(\"utf-8\"))[\"text\"]\n    #     # print(response_new_words)\n    #     n_words += len(response_new_words.split(\" \")) - len(prompts[i].split(\" \"))\n    n_words = sum(results)\n    time_seconds = time.time() - tik\n    print(\n        f\"Time (Completion): {time_seconds}, n threads: {args.n_thread}, \"\n        f\"throughput: {n_words / time_seconds} words/s.\"\n    )\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--controller-address\", type=str, default=\"http://localhost:21001\"\n    )\n    parser.add_argument(\"--worker-address\", type=str)\n    parser.add_argument(\"--model-name\", type=str, default=\"vicuna\")\n    parser.add_argument(\"--max-new-tokens\", type=int, default=2048)\n    parser.add_argument(\"--n-thread\", type=int, default=8)\n    parser.add_argument(\"--test-dispatch\", action=\"store_true\")\n    args = parser.parse_args()\n\n    main()\n"}
{"type": "test_file", "path": "playground/test_openai_api/openai_api.py", "content": "import os\n\nfrom fastchat.model import get_conversation_template\n\ndef chatgpt():\n    import openai\n    model = \"gpt-3.5-turbo\"\n    conv = get_conversation_template(model)\n    conv.append_message(conv.roles[0], \"Hello!\")\n    conv.append_message(conv.roles[1], None)\n\n    messages = conv.to_openai_api_messages()\n    print(messages)\n\n    res = openai.ChatCompletion.create(model=model, messages=messages)\n    msg = res[\"choices\"][0][\"message\"][\"content\"]\n    print(msg)\n\n    res = openai.ChatCompletion.create(model=model, messages=messages, stream=True)\n    msg = \"\"\n    for chunk in res:\n        msg += chunk[\"choices\"][0][\"delta\"].get(\"content\", \"\")\n    print(msg)\n\n\nchatgpt()\n"}
{"type": "test_file", "path": "playground/test_embedding/test_sentence_similarity.py", "content": "import json\nimport os\n\nimport numpy as np\nimport openai\nimport requests\nfrom scipy.spatial.distance import cosine\n\n\ndef get_embedding_from_api(word, model=\"vicuna-7b-v1.1\"):\n    if \"ada\" in model:\n        resp = openai.Embedding.create(\n            model=model,\n            input=word,\n        )\n        embedding = np.array(resp[\"data\"][0][\"embedding\"])\n        return embedding\n\n    url = \"http://localhost:8000/v1/embeddings\"\n    headers = {\"Content-Type\": \"application/json\"}\n    data = json.dumps({\"model\": model, \"input\": word})\n\n    response = requests.post(url, headers=headers, data=data)\n    if response.status_code == 200:\n        embedding = np.array(response.json()[\"data\"][0][\"embedding\"])\n        return embedding\n    else:\n        print(f\"Error: {response.status_code} - {response.text}\")\n        return None\n\n\ndef cosine_similarity(vec1, vec2):\n    return 1 - cosine(vec1, vec2)\n\n\ndef print_cosine_similarity(embeddings, texts):\n    for i in range(len(texts)):\n        for j in range(i + 1, len(texts)):\n            sim = cosine_similarity(embeddings[texts[i]], embeddings[texts[j]])\n            print(f\"Cosine similarity between '{texts[i]}' and '{texts[j]}': {sim:.2f}\")\n\n\ntexts = [\n    \"The quick brown fox\",\n    \"The quick brown dog\",\n    \"The fast brown fox\",\n    \"A completely different sentence\",\n]\n\nembeddings = {}\nfor text in texts:\n    embeddings[text] = get_embedding_from_api(text)\n\nprint(\"Vicuna-7B:\")\nprint_cosine_similarity(embeddings, texts)\n\nfor text in texts:\n    embeddings[text] = get_embedding_from_api(text, model=\"text-similarity-ada-001\")\n\nprint(\"text-similarity-ada-001:\")\nprint_cosine_similarity(embeddings, texts)\n\nfor text in texts:\n    embeddings[text] = get_embedding_from_api(text, model=\"text-embedding-ada-002\")\n\nprint(\"text-embedding-ada-002:\")\nprint_cosine_similarity(embeddings, texts)\n"}
{"type": "test_file", "path": "tests/test_openai_sdk.py", "content": "import openai\n\nopenai.api_key = \"EMPTY\"  # Not support yet\nopenai.api_base = \"http://localhost:8000/v1\"\n\nmodel = \"vicuna-7b-v1.1\"\n\n\ndef test_list_models():\n    model_list = openai.Model.list()\n    print(model_list[\"data\"][0][\"id\"])\n\n\ndef test_completion():\n    prompt = \"Once upon a time\"\n    completion = openai.Completion.create(model=model, prompt=prompt, max_tokens=64)\n    print(prompt + completion.choices[0].text)\n\n\ndef test_embedding():\n    embedding = openai.Embedding.create(model=model, input=\"Hello world!\")\n    print(len(embedding[\"data\"][0][\"embedding\"]))\n\n\ndef test_chat_completion():\n    completion = openai.ChatCompletion.create(\n        model=model, messages=[{\"role\": \"user\", \"content\": \"Hello! What is your name?\"}]\n    )\n    print(completion.choices[0].message.content)\n\n\ndef test_chat_completion_stream():\n    messages = [{\"role\": \"user\", \"content\": \"Hello! What is your name?\"}]\n    res = openai.ChatCompletion.create(model=model, messages=messages, stream=True)\n    for chunk in res:\n        content = chunk[\"choices\"][0][\"delta\"].get(\"content\", \"\")\n        print(content, end=\"\", flush=True)\n    print()\n\n\nif __name__ == \"__main__\":\n    test_list_models()\n    test_completion()\n    test_embedding()\n    test_chat_completion()\n    test_chat_completion_stream()\n"}
{"type": "test_file", "path": "playground/test_openai_api/anthropic_api.py", "content": "import os\n\nfrom fastchat.model import get_conversation_template\n\n\ndef claude():\n    import anthropic\n    c = anthropic.Client(os.environ[\"ANTHROPIC_API_KEY\"])\n\n    model = \"claude-v1\"\n    conv = get_conversation_template(model)\n    conv.append_message(conv.roles[0], \"Hello!\")\n    conv.append_message(conv.roles[1], None)\n    prompt = conv.get_prompt()\n\n    response = c.completion_stream(\n        prompt=prompt,\n        stop_sequences=[anthropic.HUMAN_PROMPT],\n        max_tokens_to_sample=256,\n        model=model,\n        stream=True,\n    )\n    for data in response:\n        print(data[\"completion\"])\n\n\nclaude()\n"}
{"type": "test_file", "path": "tests/test_openai_langchain.py", "content": "# export OPENAI_API_BASE=http://localhost:8000/v1\n# export OPENAI_API_KEY=EMPTY\n\nfrom langchain import OpenAI, LLMChain, PromptTemplate\nfrom langchain.memory import ConversationBufferWindowMemory\nfrom langchain.embeddings import OpenAIEmbeddings\nimport numpy as np\n\ntemplate = \"\"\"{history}\nHuman: {human_input}\nAssistant:\"\"\"\n\ndef test_embedding():\n    embeddings = OpenAIEmbeddings()\n    texts = [\"Why does the chicken cross the road\", \"To be honest\", \"Long time ago\"]\n    query_result = embeddings.embed_query(texts[0])\n    doc_result = embeddings.embed_documents(texts)\n    assert np.allclose(query_result, doc_result[0], atol=1e-3)\n\ndef test_chain():\n\n    prompt = PromptTemplate(\n        input_variables=[\"history\", \"human_input\"],\n        template=template\n    )\n    chain = LLMChain(\n        llm=OpenAI(model=\"text-embedding-ada-002\", temperature=1), \n        prompt=prompt, \n        verbose=True, \n        memory=ConversationBufferWindowMemory(k=2),\n    )\n    output = chain.predict(human_input=\"ls ~\")\n    print(output)\n\nif __name__ == \"__main__\":\n    test_embedding()\n    test_chain()\n\n"}
{"type": "test_file", "path": "playground/test_embedding/test_semantic_search.py", "content": "import json\nimport os\n\nimport numpy as np\nimport openai\nimport pandas as pd\nimport requests\nfrom scipy.spatial.distance import cosine\n\n\ndef cosine_similarity(vec1, vec2):\n    try:\n        return 1 - cosine(vec1, vec2)\n    except:\n        print(vec1.shape, vec2.shape)\n\n\ndef get_embedding_from_api(word, model=\"vicuna-7b-v1.1\"):\n    if \"ada\" in model:\n        resp = openai.Embedding.create(\n            model=model,\n            input=word,\n        )\n        embedding = np.array(resp[\"data\"][0][\"embedding\"])\n        return embedding\n\n    url = \"http://localhost:8000/v1/embeddings\"\n    headers = {\"Content-Type\": \"application/json\"}\n    data = json.dumps({\"model\": model, \"input\": word})\n\n    response = requests.post(url, headers=headers, data=data)\n    if response.status_code == 200:\n        embedding = np.array(response.json()[\"data\"][0][\"embedding\"])\n        return embedding\n    else:\n        print(f\"Error: {response.status_code} - {response.text}\")\n        return None\n\n\ndef create_embedding_data_frame(data_path, model, max_tokens=500):\n    df = pd.read_csv(data_path, index_col=0)\n    df = df[[\"Time\", \"ProductId\", \"UserId\", \"Score\", \"Summary\", \"Text\"]]\n    df = df.dropna()\n    df[\"combined\"] = (\n        \"Title: \" + df.Summary.str.strip() + \"; Content: \" + df.Text.str.strip()\n    )\n    top_n = 1000\n    df = df.sort_values(\"Time\").tail(top_n * 2)\n    df.drop(\"Time\", axis=1, inplace=True)\n\n    df[\"n_tokens\"] = df.combined.apply(lambda x: len(x))\n    df = df[df.n_tokens <= max_tokens].tail(top_n)\n    df[\"embedding\"] = df.combined.apply(lambda x: get_embedding_from_api(x, model))\n    return df\n\n\ndef search_reviews(df, product_description, n=3, pprint=False, model=\"vicuna-7b-v1.1\"):\n    product_embedding = get_embedding_from_api(product_description, model=model)\n    df[\"similarity\"] = df.embedding.apply(\n        lambda x: cosine_similarity(x, product_embedding)\n    )\n\n    results = (\n        df.sort_values(\"similarity\", ascending=False)\n        .head(n)\n        .combined.str.replace(\"Title: \", \"\")\n        .str.replace(\"; Content:\", \": \")\n    )\n    if pprint:\n        for r in results:\n            print(r[:200])\n            print()\n    return results\n\n\ndef print_model_search(input_path, model):\n    print(f\"Model: {model}\")\n    df = create_embedding_data_frame(input_path, model)\n    print(\"search: delicious beans\")\n    results = search_reviews(df, \"delicious beans\", n=5, model=model)\n    print(results)\n    print(\"search: whole wheat pasta\")\n    results = search_reviews(df, \"whole wheat pasta\", n=5, model=model)\n    print(results)\n    print(\"search: bad delivery\")\n    results = search_reviews(df, \"bad delivery\", n=5, model=model)\n    print(results)\n\n\ninput_datapath = \"amazon_fine_food_review.csv\"\nif not os.path.exists(input_datapath):\n    raise Exception(\n        f\"Please download data from: https://www.kaggle.com/datasets/snap/amazon-fine-food-reviews\"\n    )\n\n\nprint_model_search(input_datapath, \"vicuna-7b-v1.1\")\nprint_model_search(input_datapath, \"text-similarity-ada-001\")\nprint_model_search(input_datapath, \"text-embedding-ada-002\")\n"}
{"type": "test_file", "path": "playground/test_embedding/test_classification.py", "content": "import json\nimport os\n\nimport numpy as np\nimport openai\nimport pandas as pd\nimport requests\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, accuracy_score\n\n\nnp.set_printoptions(threshold=10000)\n\n\ndef get_embedding_from_api(word, model=\"vicuna-7b-v1.1\"):\n    if \"ada\" in model:\n        resp = openai.Embedding.create(\n            model=model,\n            input=word,\n        )\n        embedding = np.array(resp[\"data\"][0][\"embedding\"])\n        return embedding\n\n    url = \"http://localhost:8000/v1/embeddings\"\n    headers = {\"Content-Type\": \"application/json\"}\n    data = json.dumps({\"model\": model, \"input\": word})\n\n    response = requests.post(url, headers=headers, data=data)\n    if response.status_code == 200:\n        embedding = np.array(response.json()[\"data\"][0][\"embedding\"])\n        return embedding\n    else:\n        print(f\"Error: {response.status_code} - {response.text}\")\n        return None\n\n\ndef create_embedding_data_frame(data_path, model, max_tokens=500):\n    df = pd.read_csv(data_path, index_col=0)\n    df = df[[\"Time\", \"ProductId\", \"UserId\", \"Score\", \"Summary\", \"Text\"]]\n    df = df.dropna()\n    df[\"combined\"] = (\n        \"Title: \" + df.Summary.str.strip() + \"; Content: \" + df.Text.str.strip()\n    )\n    top_n = 1000\n    df = df.sort_values(\"Time\").tail(top_n * 2)\n    df.drop(\"Time\", axis=1, inplace=True)\n\n    df[\"n_tokens\"] = df.combined.apply(lambda x: len(x))\n    df = df[df.n_tokens <= max_tokens].tail(top_n)\n    df[\"embedding\"] = df.combined.apply(lambda x: get_embedding_from_api(x, model))\n    return df\n\n\ndef train_random_forest(df):\n    X_train, X_test, y_train, y_test = train_test_split(\n        list(df.embedding.values), df.Score, test_size=0.2, random_state=42\n    )\n\n    clf = RandomForestClassifier(n_estimators=100)\n    clf.fit(X_train, y_train)\n    preds = clf.predict(X_test)\n\n    report = classification_report(y_test, preds)\n    accuracy = accuracy_score(y_test, preds)\n    return clf, accuracy, report\n\n\ninput_datapath = \"amazon_fine_food_review.csv\"\nif not os.path.exists(input_datapath):\n    raise Exception(\n        f\"Please download data from: https://www.kaggle.com/datasets/snap/amazon-fine-food-reviews\"\n    )\n\ndf = create_embedding_data_frame(input_datapath, \"vicuna-7b-v1.1\")\nclf, accuracy, report = train_random_forest(df)\nprint(f\"Vicuna-7b-v1.1 accuracy:{accuracy}\")\ndf = create_embedding_data_frame(input_datapath, \"text-similarity-ada-001\")\nclf, accuracy, report = train_random_forest(df)\nprint(f\"text-similarity-ada-001 accuracy:{accuracy}\")\ndf = create_embedding_data_frame(input_datapath, \"text-embedding-ada-002\")\nclf, accuracy, report = train_random_forest(df)\nprint(f\"text-embedding-ada-002 accuracy:{accuracy}\")\n"}
{"type": "source_file", "path": "higpt/model/graph_layers/graph_transformer.py", "content": "import torch as t\nfrom torch import nn\nimport torch.nn.functional as F\nimport math\nfrom transformers.configuration_utils import PretrainedConfig\n\ninit = nn.init.xavier_uniform_\nuniformInit = nn.init.uniform\n\ndef PositionalEncoding(q_len, d_model, normalize=True):\n    pe = t.zeros(q_len, d_model)\n    position = t.arange(0, q_len).unsqueeze(1)\n    div_term = t.exp(t.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n    pe[:, 0::2] = t.sin(position * div_term)\n    pe[:, 1::2] = t.cos(position * div_term)\n    if normalize:\n        pe = pe - pe.mean()\n        pe = pe / (pe.std() * 10)\n    return pe\n\n\ndef pos_encoding(pe, learn_pe, nvar, d_model):\n    # Positional encoding\n    if pe == None:\n        W_pos = t.empty((nvar, d_model)) # pe = None and learn_pe = False can be used to measure impact of pe\n        nn.init.uniform_(W_pos, -0.02, 0.02)\n        learn_pe = False\n    elif pe == 'zero':\n        W_pos = t.empty((nvar, 1))\n        nn.init.uniform_(W_pos, -0.02, 0.02)\n    elif pe == 'zeros':\n        W_pos = t.empty((nvar, d_model))\n        nn.init.uniform_(W_pos, -0.02, 0.02)\n    elif pe == 'normal' or pe == 'gauss':\n        W_pos = t.zeros((nvar, 1))\n        t.nn.init.normal_(W_pos, mean=0.0, std=0.1)\n    elif pe == 'uniform':\n        W_pos = t.zeros((nvar, 1))\n        nn.init.uniform_(W_pos, a=0.0, b=0.1)\n    elif pe == 'sincos': W_pos = PositionalEncoding(nvar, d_model, normalize=True)\n    else: raise ValueError(f\"{pe} is not a valid pe (positional encoder. Available types: 'gauss'=='normal', \\\n        'zeros', 'zero', uniform', 'sincos', None.)\")\n    return nn.Parameter(W_pos, requires_grad=learn_pe)\n\n\nclass graph_transformer(nn.Module):\n    def __init__(self, args):\n        super(graph_transformer, self).__init__()\n        self.config = PretrainedConfig()\n        self.gtLayers = nn.Sequential(*[GTLayer(args) for i in range(args.gt_layers)])\n\n        self.W_pos = pos_encoding('zeros', True, 1, args.att_d_model)\n                \n        self.W_P = nn.Linear(args.gnn_input, args.att_d_model)\n        self.dropout = nn.Dropout(0.1)\n        self.inverW_P = nn.Linear(args.att_d_model, args.gnn_output)\n        self.args = args\n\n    def forward(self, g):\n        # Adj: sp adj\n        # x: bs * n * d_model * num_patch\n        \n        # print(edge_index)\n        device = self.parameters().__next__().device\n        g = g.to(device)\n        \n        x = g.graph_node\n        \n        # x, W_P_weight, W_P_bias= Mv2Samedevice([x, self.W_P.weight, self.W_P.bias])\n        # self.W_P.weight = nn.Parameter(W_P_weight.to(x.dtype))\n        # self.W_P.bias = nn.Parameter(W_P_bias.to(x.dtype))\n        # print(self.W_P.dtype, x.dtype)\n        z = self.W_P(x)\n        if self.args.if_pos: \n            embeds = self.dropout(z + self.W_pos) \n        else: \n            embeds = self.dropout(z) \n        for gt in self.gtLayers:\n            embeds = gt(g, embeds) # bs * num_patch * n * d_model\n        # embeds, inverW_P_weight, inverW_P_bias = Mv2Samedevice([embeds, self.inverW_P.weight, self.inverW_P.bias])\n        # self.inverW_P.weight = nn.Parameter(inverW_P_weight.to(embeds.dtype))\n        # self.inverW_P.bias = nn.Parameter(inverW_P_bias.to(embeds.dtype))\n        ret = self.inverW_P(embeds)\n        return ret\ndef Mv2Samedevice(vars): \n    return [var.to(vars[0].device) for var in vars]\n\nclass GTLayer(nn.Module):\n    def __init__(self, args):\n        super(GTLayer, self).__init__()\n        self.qTrans = nn.Parameter(init(t.empty(args.att_d_model, args.att_d_model)))\n        self.kTrans = nn.Parameter(init(t.empty(args.att_d_model, args.att_d_model)))\n        self.vTrans = nn.Parameter(init(t.empty(args.att_d_model, args.att_d_model)))\n        if args.att_norm: \n            self.norm = nn.LayerNorm(args.att_d_model, eps=1e-6)\n        self.args = args\n        \n        \n    \n    def forward(self, g, embeds):\n        # Adj: adj\n        # x: n * d_model\n        rows, cols = g.edge_index\n        nvar, _ = embeds.shape\n        # print(rows)\n        # print(cols)\n\n        rowEmbeds = embeds[rows, :]\n        colEmbeds = embeds[cols, :]\n        evar, _ = rowEmbeds.shape\n\n        # rowEmbeds, qTrans, kTrans, vTrans = Mv2Samedevice([rowEmbeds, self.qTrans, self.kTrans, self.vTrans])\n        # self.qTrans = nn.Parameter(qTrans.to(rowEmbeds.dtype))\n        # self.kTrans = nn.Parameter(kTrans.to(rowEmbeds.dtype))\n        # self.vTrans = nn.Parameter(vTrans.to(rowEmbeds.dtype))\n        qEmbeds = (rowEmbeds @ self.qTrans).view([evar, self.args.head, self.args.att_d_model // self.args.head])\n        kEmbeds = (colEmbeds @ self.kTrans).view([evar, self.args.head, self.args.att_d_model // self.args.head])\n        vEmbeds = (colEmbeds @ self.vTrans).view([evar, self.args.head, self.args.att_d_model // self.args.head])\n        \n        att = t.einsum('ehd, ehd -> eh', qEmbeds, kEmbeds)\n        att = t.clamp(att, -10.0, 10.0)\n        expAtt = t.exp(att)\n        \n        tem = t.zeros([nvar, self.args.head]).to(expAtt.device, dtype=expAtt.dtype)\n        # print(tem.device, expAtt.device, rows.device)\n        rows = rows.to(expAtt.device)\n        attNorm = (tem.index_add_(0, rows, expAtt))[rows, :]\n        att = expAtt / (attNorm + 1e-8) # bleh\n        \n        resEmbeds = t.einsum('eh, ehd -> ehd', att, vEmbeds).view([evar, self.args.att_d_model])\n        tem = t.zeros([nvar, self.args.att_d_model]).to(resEmbeds.device, dtype=resEmbeds.dtype)\n        rows = rows.to(resEmbeds.device)\n        tem = tem.to(resEmbeds.dtype)\n        resEmbeds = tem.index_add_(0, rows, resEmbeds) # nd\n        resEmbeds = resEmbeds + embeds\n        if self.args.att_norm: \n            # resEmbeds, norm_weight, norm_bias = Mv2Samedevice([resEmbeds, self.norm.weight, self.norm.bias])\n            # self.norm.weight = nn.Parameter(norm_weight.to(resEmbeds.dtype))\n            # self.norm.bias = nn.Parameter(norm_bias.to(resEmbeds.dtype))\n            resEmbeds = self.norm(resEmbeds)\n\n        return resEmbeds"}
{"type": "source_file", "path": "higpt/model/graph_layers/simple_tokenizer.py", "content": "import gzip\nimport html\nimport os\nfrom functools import lru_cache\n\nimport ftfy\nimport regex as re\n\n\n@lru_cache()\ndef default_bpe():\n    return os.path.join(os.path.dirname(os.path.abspath(__file__)), \"bpe_simple_vocab_16e6.txt.gz\")\n\n\n@lru_cache()\ndef bytes_to_unicode():\n    \"\"\"\n    Returns list of utf-8 byte and a corresponding list of unicode strings.\n    The reversible bpe codes work on unicode strings.\n    This means you need a large # of unicode characters in your vocab if you want to avoid UNKs.\n    When you're at something like a 10B token dataset you end up needing around 5K for decent coverage.\n    This is a signficant percentage of your normal, say, 32K bpe vocab.\n    To avoid that, we want lookup tables between utf-8 bytes and unicode strings.\n    And avoids mapping to whitespace/control characters the bpe code barfs on.\n    \"\"\"\n    bs = list(range(ord(\"!\"), ord(\"~\")+1))+list(range(ord(\"¡\"), ord(\"¬\")+1))+list(range(ord(\"®\"), ord(\"ÿ\")+1))\n    cs = bs[:]\n    n = 0\n    for b in range(2**8):\n        if b not in bs:\n            bs.append(b)\n            cs.append(2**8+n)\n            n += 1\n    cs = [chr(n) for n in cs]\n    return dict(zip(bs, cs))\n\n\ndef get_pairs(word):\n    \"\"\"Return set of symbol pairs in a word.\n    Word is represented as tuple of symbols (symbols being variable-length strings).\n    \"\"\"\n    pairs = set()\n    prev_char = word[0]\n    for char in word[1:]:\n        pairs.add((prev_char, char))\n        prev_char = char\n    return pairs\n\n\ndef basic_clean(text):\n    text = ftfy.fix_text(text)\n    text = html.unescape(html.unescape(text))\n    return text.strip()\n\n\ndef whitespace_clean(text):\n    text = re.sub(r'\\s+', ' ', text)\n    text = text.strip()\n    return text\n\n\nclass SimpleTokenizer(object):\n    def __init__(self, bpe_path: str = default_bpe()):\n        self.byte_encoder = bytes_to_unicode()\n        self.byte_decoder = {v: k for k, v in self.byte_encoder.items()}\n        merges = gzip.open(bpe_path).read().decode(\"utf-8\").split('\\n')\n        merges = merges[1:49152-256-2+1]\n        merges = [tuple(merge.split()) for merge in merges]\n        vocab = list(bytes_to_unicode().values())\n        vocab = vocab + [v+'</w>' for v in vocab]\n        for merge in merges:\n            vocab.append(''.join(merge))\n        vocab.extend(['<|startoftext|>', '<|endoftext|>'])\n        self.encoder = dict(zip(vocab, range(len(vocab))))\n        self.decoder = {v: k for k, v in self.encoder.items()}\n        self.bpe_ranks = dict(zip(merges, range(len(merges))))\n        self.cache = {'<|startoftext|>': '<|startoftext|>', '<|endoftext|>': '<|endoftext|>'}\n        self.pat = re.compile(r\"\"\"<\\|startoftext\\|>|<\\|endoftext\\|>|'s|'t|'re|'ve|'m|'ll|'d|[\\p{L}]+|[\\p{N}]|[^\\s\\p{L}\\p{N}]+\"\"\", re.IGNORECASE)\n\n    def bpe(self, token):\n        if token in self.cache:\n            return self.cache[token]\n        word = tuple(token[:-1]) + ( token[-1] + '</w>',)\n        pairs = get_pairs(word)\n\n        if not pairs:\n            return token+'</w>'\n\n        while True:\n            bigram = min(pairs, key = lambda pair: self.bpe_ranks.get(pair, float('inf')))\n            if bigram not in self.bpe_ranks:\n                break\n            first, second = bigram\n            new_word = []\n            i = 0\n            while i < len(word):\n                try:\n                    j = word.index(first, i)\n                    new_word.extend(word[i:j])\n                    i = j\n                except:\n                    new_word.extend(word[i:])\n                    break\n\n                if word[i] == first and i < len(word)-1 and word[i+1] == second:\n                    new_word.append(first+second)\n                    i += 2\n                else:\n                    new_word.append(word[i])\n                    i += 1\n            new_word = tuple(new_word)\n            word = new_word\n            if len(word) == 1:\n                break\n            else:\n                pairs = get_pairs(word)\n        word = ' '.join(word)\n        self.cache[token] = word\n        return word\n\n    def encode(self, text):\n        bpe_tokens = []\n        text = whitespace_clean(basic_clean(text)).lower()\n        for token in re.findall(self.pat, text):\n            token = ''.join(self.byte_encoder[b] for b in token.encode('utf-8'))\n            bpe_tokens.extend(self.encoder[bpe_token] for bpe_token in self.bpe(token).split(' '))\n        return bpe_tokens\n\n    def decode(self, tokens):\n        text = ''.join([self.decoder[token] for token in tokens])\n        text = bytearray([self.byte_decoder[c] for c in text]).decode('utf-8', errors=\"replace\").replace('</w>', ' ')\n        return text\n"}
{"type": "source_file", "path": "higpt/model/convert_fp16.py", "content": "\"\"\"\nUsage:\npython3 -m fastchat.model.convert_fp16 --in in-folder --out out-folder\n\"\"\"\nimport argparse\n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\n\n\ndef convert_fp16(in_checkpoint, out_checkpoint):\n    tokenizer = AutoTokenizer.from_pretrained(in_checkpoint, use_fast=False)\n    model = AutoModelForCausalLM.from_pretrained(\n        in_checkpoint, torch_dtype=torch.float16, low_cpu_mem_usage=True\n    )\n    model.save_pretrained(out_checkpoint)\n    tokenizer.save_pretrained(out_checkpoint)\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--in-checkpoint\", type=str, help=\"Path to the model\")\n    parser.add_argument(\"--out-checkpoint\", type=str, help=\"Path to the output model\")\n    args = parser.parse_args()\n\n    convert_fp16(args.in_checkpoint, args.out_checkpoint)\n"}
{"type": "source_file", "path": "higpt/model/heteclip_models/clip_outputs.py", "content": "\"\"\"\n Copyright (c) 2022, salesforce.com, inc.\n All rights reserved.\n SPDX-License-Identifier: BSD-3-Clause\n For full license text, see the LICENSE file in the repo root or https://opensource.org/licenses/BSD-3-Clause\n\n Based on https://github.com/mlfoundations/open_clip\n\"\"\"\n\nfrom dataclasses import dataclass\n\nfrom typing import Optional\n\nimport torch\nfrom transformers.modeling_outputs import ModelOutput\n\n\n@dataclass\nclass ClipOutputFeatures(ModelOutput):\n    \"\"\"\n    Data class of features from AlbefFeatureExtractor.\n\n    Args:\n        image_embeds: `torch.FloatTensor` of shape `(batch_size, 1, embed_dim)`, `optional`\n        image_features: `torch.FloatTensor` of shape `(batch_size, 1, feature_dim)`, `optional`\n        text_embeds: `torch.FloatTensor` of shape `(batch_size, 1, embed_dim)`, `optional`\n        text_features: `torch.FloatTensor` of shape `(batch_size, 1, feature_dim)`, `optional`\n    \"\"\"\n\n    image_embeds: Optional[torch.FloatTensor] = None\n    image_embeds_proj: Optional[torch.FloatTensor] = None\n\n    text_embeds: Optional[torch.FloatTensor] = None\n    text_embeds_proj: Optional[torch.FloatTensor] = None\n\n\n@dataclass\nclass ClipOutput(ModelOutput):\n    intermediate_output: Optional[ClipOutputFeatures] = None\n\n    logit_scale_exp: Optional[torch.FloatTensor] = None\n\n    loss: Optional[torch.FloatTensor] = None\n\n\n@dataclass\nclass HeteClipOutputFeatures(ModelOutput):\n\n    graph_embeds: Optional[torch.FloatTensor] = None\n    graph_embeds_proj: Optional[torch.FloatTensor] = None\n\n    text_embeds: Optional[torch.FloatTensor] = None\n    text_embeds_proj: Optional[torch.FloatTensor] = None"}
{"type": "source_file", "path": "higpt/model/heteclip_models/__init__.py", "content": "\"\"\"\n Copyright (c) 2022, salesforce.com, inc.\n All rights reserved.\n SPDX-License-Identifier: BSD-3-Clause\n For full license text, see the LICENSE file in the repo root or https://opensource.org/licenses/BSD-3-Clause\n\n Based on https://github.com/mlfoundations/open_clip\n\"\"\"\n\n\"\"\" OpenAI pretrained model functions\nAdapted from https://github.com/mlfoundations/open_clip and https://github.com/openai/CLIP.\n\nOriginally MIT License, Copyright (c) 2021 OpenAI.\n\"\"\"\nfrom higpt.model.heteclip_models.model import Transformer, LayerNorm, CLIPTextCfg"}
{"type": "source_file", "path": "higpt/model/graph_layers/mpnn.py", "content": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch_geometric.utils import remove_self_loops, add_self_loops, degree\nimport json\nimport copy\nfrom transformers import AutoTokenizer \nimport transformers\nfrom transformers.configuration_utils import PretrainedConfig\nimport os\n\ndef gcn_conv(h, edge_index):\n    # print(edge_index)\n    N, node_feas = h.shape\n    edge_index, _ = remove_self_loops(edge_index)\n    edge_index, _ = add_self_loops(edge_index, num_nodes=N)\n    \n    src, dst = edge_index\n    deg = degree(dst, num_nodes=N)\n\n    deg_src = deg[src].pow(-0.5) \n    deg_src.masked_fill_(deg_src == float('inf'), 0)\n    deg_dst = deg[dst].pow(-0.5)\n    deg_dst.masked_fill_(deg_dst == float('inf'), 0)\n    edge_weight = deg_src * deg_dst\n\n    a = torch.sparse_coo_tensor(edge_index, edge_weight, torch.Size([N, N])).t()\n    rows, cols = edge_index\n    edge_msg = h[rows, :] * torch.unsqueeze(edge_weight, dim=-1)\n    col_embeds = h[cols, :]\n    tem = torch.zeros([N, node_feas]).to(edge_msg.device)\n    rows = rows.to(edge_msg.device)\n    h_prime = tem.index_add_(0, rows, edge_msg) # nd\n    # h = h.float() \n    # h_prime = a @ h \n    # h_prime = h_prime.bfloat16()\n    return h_prime\n\n# Implementation of MPNN, which can become MLP or GCN depending on whether using message passing\nclass MPNN(nn.Module): \n    def __init__(self, in_channels, hidden_channels, out_channels, **kwargs):\n        super(MPNN, self).__init__()\n        self.config = PretrainedConfig()\n        self.dropout = kwargs.get('dropout')# args.dropout\n        self.num_layers = kwargs.get('num_layers')# args.num_layers\n        self.ff_bias = True  # Use bias for FF layers in default\n\n        self.bns = nn.BatchNorm1d(hidden_channels, affine=False, track_running_stats=False)\n        self.activation = F.relu\n        self.if_param = kwargs.get('if_param')\n\n        if self.if_param: \n            self.fcs = nn.ModuleList([])\n            self.fcs.append(nn.Linear(in_channels, hidden_channels, bias=self.ff_bias))\n            for _ in range(self.num_layers - 2): self.fcs.append(nn.Linear(hidden_channels, hidden_channels, bias=self.ff_bias)) #1s\n            self.fcs.append(nn.Linear(hidden_channels, out_channels, bias=self.ff_bias)) #1\n            self.reset_parameters()\n    \n\n    def reset_parameters(self):\n        for mlp in self.fcs: \n            nn.init.xavier_uniform_(mlp.weight, gain=1.414)\n            nn.init.zeros_(mlp.bias)\n\n    def forward(self, g, use_conv=True):\n        \n        x = g.graph_node\n        edge_index = g.edge_index\n        try:\n            device = self.parameters().__next__().device\n        except: \n            device = x.device\n        x = x.to(device)\n        edge_index = edge_index.to(device)\n        for i in range(self.num_layers - 1):\n            if self.if_param: x = x @ self.fcs[i].weight.t() \n            if use_conv: x = gcn_conv(x, edge_index)  # Optionally replace 'gcn_conv' with other conv functions in conv.py\n            if self.ff_bias and self.if_param: x = x + self.fcs[i].bias\n            try: \n                x = self.activation(self.bns(x))\n            except: \n                x = self.activation((x))\n            x = F.dropout(x, p=self.dropout, training=self.training)\n\n        if self.if_param: x = x @ self.fcs[-1].weight.t() \n        if use_conv: x = gcn_conv(x, edge_index)\n        if self.ff_bias and self.if_param: x = x + self.fcs[-1].bias\n        return x\n\nif __name__ == '__main__': \n    list_data_dict = json.load(open('./GraphChat/playground/data/arxiv_train_instruct.json', \"r\"))\n    graph_dict = list_data_dict[0]['graph']\n    graph_edge_index = torch.Tensor(copy.deepcopy(graph_dict['edge_index'])).long()\n    graph_node_list = copy.deepcopy(graph_dict['node_list'])\n    target_node = copy.deepcopy(graph_dict['node_idx'])\n    graph_type = copy.deepcopy(list_data_dict[0]['id']).split('_')[0]\n    graph_data_all = torch.load('./GraphChat/playground/data/graph_data.pt')\n    graph_node_rep = graph_data_all[graph_type].x[graph_node_list] ##\n\n    print(graph_node_rep.shape)   \n        \n    mpnn_net = MPNN(in_channels = 128, hidden_channels = 256, out_channels = 128, dropout = 0.1, num_layers = 2)\n\n    output_rep = mpnn_net(graph_node_rep, graph_edge_index)\n    print(output_rep.shape)"}
{"type": "source_file", "path": "higpt/model/graph_layers/clip_graph.py", "content": "from collections import OrderedDict\nfrom typing import Tuple, Union\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\nfrom typing import Any, Union, List\nfrom higpt.model.graph_layers.simple_tokenizer import SimpleTokenizer as _Tokenizer\nfrom torch_geometric.nn.conv import MessagePassing\nfrom torch_scatter import scatter_add\nfrom torch_geometric.utils import add_remaining_self_loops\nfrom torch.nn import Parameter\nfrom torch import nn, optim\nfrom higpt.model.graph_layers.graph_transformer import graph_transformer\nfrom transformers.configuration_utils import PretrainedConfig\n\n_tokenizer = _Tokenizer()\n\n\nclass LayerNorm(nn.LayerNorm):\n    \"\"\"Subclass torch's LayerNorm to handle fp16.\"\"\"\n\n    def forward(self, x: torch.Tensor):\n        orig_type = x.dtype\n        ret = super().forward(x.type(torch.float32))\n        return ret.type(orig_type)\n\n\nclass QuickGELU(nn.Module):\n    def forward(self, x: torch.Tensor):\n        return x * torch.sigmoid(1.702 * x)\n\n\nclass ResidualAttentionBlock(nn.Module):\n    def __init__(self, d_model: int, n_head: int, attn_mask: torch.Tensor = None):\n        super().__init__()\n\n        self.attn = nn.MultiheadAttention(d_model, n_head)\n        self.ln_1 = LayerNorm(d_model)\n        self.mlp = nn.Sequential(OrderedDict([\n            (\"c_fc\", nn.Linear(d_model, d_model * 4)),\n            (\"gelu\", QuickGELU()),\n            (\"c_proj\", nn.Linear(d_model * 4, d_model))\n        ]))\n        self.ln_2 = LayerNorm(d_model)\n        self.attn_mask = attn_mask\n\n    def attention(self, x: torch.Tensor):\n        self.attn_mask = self.attn_mask.to(dtype=x.dtype, device=x.device) if self.attn_mask is not None else None\n        return self.attn(x, x, x, need_weights=False, attn_mask=self.attn_mask)[0]\n\n    def forward(self, x: torch.Tensor):\n        x = x + self.attention(self.ln_1(x))\n        x = x + self.mlp(self.ln_2(x))\n        return x\n\n\nclass Transformer(nn.Module):\n    def __init__(self, width: int, layers: int, heads: int, attn_mask: torch.Tensor = None):\n        super().__init__()\n        self.width = width\n        self.layers = layers\n        self.resblocks = nn.Sequential(*[ResidualAttentionBlock(width, heads, attn_mask) for _ in range(layers)])\n\n    def forward(self, x: torch.Tensor):\n        return self.resblocks(x)\n\n\nclass GNN(MessagePassing):\n    def __init__(self, args, **kwargs):\n        super(GNN, self).__init__(aggr='add', **kwargs)\n        self.config = PretrainedConfig()\n        self.vars = nn.ParameterList()\n\n        w = nn.Parameter(torch.ones([args.gnn_hid, args.gnn_input]))\n        torch.nn.init.xavier_uniform_(w)\n        self.vars.append(w)\n        self.vars.append(nn.Parameter(torch.zeros(args.gnn_hid)))\n\n        w = nn.Parameter(torch.ones([args.gnn_output, args.gnn_hid]))\n        torch.nn.init.xavier_uniform_(w)\n        self.vars.append(w)\n        self.vars.append(nn.Parameter(torch.zeros(args.gnn_output)))\n\n    @staticmethod\n    def norm(edge_index, num_nodes, improved=False, dtype=None):\n        edge_weight = torch.ones((edge_index.size(1),), dtype=dtype,\n                                 device=edge_index.device)\n\n        fill_value = 1.0 if not improved else 2.0\n        edge_index, edge_weight = add_remaining_self_loops(\n            edge_index, edge_weight, fill_value, num_nodes)\n\n        row, col = edge_index\n        deg = scatter_add(edge_weight, row, dim=0, dim_size=num_nodes)\n        deg_inv_sqrt = deg.pow(-0.5)\n        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n\n        return edge_index, deg_inv_sqrt[row] * edge_weight * deg_inv_sqrt[col]\n\n    def forward(self, g, vars=None):\n        device = self.parameters()[0].device\n        g = g.to(device)\n        \n        edge_index = g.edge_index\n        x = g.graph_node\n        if vars is None:\n            vars = self.vars\n        improved = False\n\n        w, b = vars[0], vars[1]\n        edge_index, norm = self.norm(edge_index, x.size(self.node_dim), improved, x.dtype)\n        x = self.propagate(edge_index, x=x, norm=norm)\n        w = w.to(x.device)\n        b = b.to(x.device)\n        x = F.linear(x, w, b)\n        x = F.leaky_relu(x)\n\n        w, b = vars[2], vars[3]\n        edge_index, norm = self.norm(edge_index, x.size(self.node_dim), improved, x.dtype)\n        x = self.propagate(edge_index, x=x, norm=norm)\n        w = w.to(x.device)\n        b = b.to(x.device)\n        x = F.linear(x, w, b)\n\n        return x\n\n    def parameters(self):\n        return self.vars\n\n\n\ndef Mv2SameDevice(var_list):\n    for vid in range(1, len(var_list)):\n        var_list[vid] = var_list[vid].to(var_list[0].device)\n    return var_list\n\n\nclass CLIP(nn.Module):\n    def __init__(self,\n                 args\n                 ):\n        super().__init__()\n\n        self.context_length = args.context_length\n        self.args = args\n        self.edge_coef = args.edge_coef\n\n        if args.gnn_type == 'gcn':\n            self.gnn = GNN(args)\n        elif args.gnn_type == 'gt': \n            self.gnn = graph_transformer(args)\n        self.transformer = Transformer(\n            width=args.transformer_width,\n            layers=args.transformer_layers,\n            heads=args.transformer_heads,\n            attn_mask=self.build_attention_mask()\n        )\n\n        self.vocab_size = args.vocab_size\n        self.token_embedding = nn.Embedding(args.vocab_size,\n                                            args.transformer_width)  # the embedding for all possible tokens\n        self.positional_embedding = nn.Parameter(torch.empty(self.context_length, args.transformer_width))\n        self.ln_final = LayerNorm(args.transformer_width)\n\n        self.text_projection = nn.Parameter(torch.empty(args.transformer_width, args.embed_dim))\n        # self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))\n\n        if args.gnn_type == 'gcn':\n            self.dtype = self.gnn.vars[0].dtype\n        elif args.gnn_type == 'gt': \n            self.dtype = self.gnn.W_pos.dtype\n\n        self.optim = optim.Adam([{'params': self.token_embedding.weight},\n                                 {'params': self.positional_embedding},\n                                 {'params': self.transformer.parameters()},\n                                 {'params': self.text_projection},\n                                 {'params': self.gnn.parameters()}\n                                 ], lr=args.lr)\n\n        self.initialize_parameters()\n\n    def initialize_parameters(self):\n        nn.init.normal_(self.token_embedding.weight, std=0.02)\n        nn.init.normal_(self.positional_embedding, std=0.01)\n\n        proj_std = (self.transformer.width ** -0.5) * ((2 * self.transformer.layers) ** -0.5)\n        attn_std = self.transformer.width ** -0.5\n        fc_std = (2 * self.transformer.width) ** -0.5\n        for block in self.transformer.resblocks:\n            nn.init.normal_(block.attn.in_proj_weight, std=attn_std)\n            nn.init.normal_(block.attn.out_proj.weight, std=proj_std)\n            nn.init.normal_(block.mlp.c_fc.weight, std=fc_std)\n            nn.init.normal_(block.mlp.c_proj.weight, std=proj_std)\n\n        if self.text_projection is not None:\n            nn.init.normal_(self.text_projection, std=self.transformer.width ** -0.5)\n\n    def build_attention_mask(self):\n        # lazily create causal attention mask, with full attention between the vision tokens\n        # pytorch uses additive attention mask; fill with -inf\n        mask = torch.empty(self.context_length, self.context_length)\n        mask.fill_(float(\"-inf\"))\n        mask.triu_(1)  # zero out the lower diagonal\n        return mask\n\n    def encode_image(self, idx_train, g):\n        embs = self.gnn(g)\n        idx_train = idx_train.to(embs.device)\n        idx_train = idx_train\n        train_embs = embs[idx_train]\n        return train_embs\n\n    def encode_text(self, text):\n        x = self.token_embedding(text).type(self.dtype)  # [batch_size, n_ctx, d_model]\n\n        x = x + self.positional_embedding.type(self.dtype)\n        x = x.permute(1, 0,\n                      2)  # NLD -> LND, batch_size * context_length *emb_dim -> context_length * batch_size  *emb_dim\n        x = self.transformer(x)\n        x = x.permute(1, 0,\n                      2)  # LND -> NLD, context_length * batch_size *emb_dim -> batch_size * context_length *emb_dim\n        x = self.ln_final(x).type(self.dtype)\n        # x.shape = [batch_size, n_ctx, transformer.width]\n        # take features from the eot （end of token） embedding (eot_token is the highest number in each sequence)\n        # so there is node need to shorten the context length\n        x = x[torch.arange(x.shape[0]), text.argmax(dim=-1)]  #\n        x = x @ self.text_projection\n        return x\n\n    def forward(self, g, s_n, t_n, s_n_text, t_n_text, training=True):\n\n        s_image_features = self.encode_image(s_n, g)\n\n        s_text_features = self.encode_text(s_n_text)\n\n        t_text_features = self.encode_text(t_n_text)\n        t_text_features = t_text_features.reshape(s_image_features.shape[0], self.args.neigh_num, self.args.gnn_output)\n        t_text_features = torch.mean(t_text_features, dim=1, keepdim=False)\n        # normalized features\n        s_image_features = s_image_features / s_image_features.norm(dim=-1, keepdim=True)\n        s_text_features = s_text_features / s_text_features.norm(dim=-1, keepdim=True)\n        t_text_features = t_text_features / t_text_features.norm(dim=-1, keepdim=True)\n\n        # cosine similarity as logits\n\n        labels = torch.arange(s_image_features.shape[0]).cuda()\n\n        # logit_scale = self.logit_scale.exp()  # the temporature hyperparameter\n        # logit_scale, s_image_features, s_text_features = Mv2SameDevice([logit_scale, s_image_features, s_text_features])\n        # logits = logit_scale * s_image_features @ s_text_features.t()\n        # loss_i = F.cross_entropy(logits, labels)\n        # loss_t = F.cross_entropy(logits.T, labels)\n        # node_loss = (loss_i + loss_t) / 2\n\n        # logit_scale, s_image_features, t_text_features = Mv2SameDevice([logit_scale, s_image_features, t_text_features])\n        # logits = logit_scale * s_image_features @ t_text_features.t()\n        # loss_i = F.cross_entropy(logits, labels)\n        # loss_t = F.cross_entropy(logits.T, labels)\n        # gt_loss = (loss_i + loss_t)/2\n\n        # logit_scale, s_text_features, t_text_features = Mv2SameDevice([logit_scale, s_text_features, t_text_features])\n        # logits = logit_scale * s_text_features @ t_text_features.t()\n        # loss_i = F.cross_entropy(logits, labels)\n        # loss_t = F.cross_entropy(logits.T, labels)\n        # tt_loss = (loss_i + loss_t)/2\n\n        \n\n        # shape = [global_batch_size, global_batch_size]\n        # return all_loss\n        return s_image_features, s_text_features, t_text_features, labels\n\n\ndef tokenize(texts: Union[str, List[str]], context_length: int = 128, truncate: bool = True) -> torch.LongTensor:\n\n    \"\"\"\n    Returns the tokenized representation of given input string(s)\n\n    Parameters\n    ----------\n    texts : Union[str, List[str]]\n        An input string or a list of input strings to tokenize\n\n    context_length : int\n        The context length to use; all CLIP models use 77 as the context length\n\n    truncate: bool\n        Whether to truncate the text in case its encoding is longer than the context length\n\n    Returns\n    -------\n    A two-dimensional tensor containing the resulting tokens, shape = [number of input strings, context_length]\n    \"\"\"\n    if isinstance(texts, str):\n        texts = [texts]\n\n    sot_token = _tokenizer.encoder[\"<|startoftext|>\"]\n    eot_token = _tokenizer.encoder[\"<|endoftext|>\"]\n    all_tokens = [[sot_token] + _tokenizer.encode(text) + [eot_token] for text in texts]\n    result = torch.zeros(len(all_tokens), context_length, dtype=torch.long)\n\n    for i, tokens in enumerate(all_tokens):\n        if len(tokens) > context_length:\n            if truncate:\n                tokens = tokens[:context_length]\n                tokens[-1] = eot_token\n            else:\n                raise RuntimeError(f\"Input {texts[i]} is too long for context length {context_length}\")\n        result[i, :len(tokens)] = torch.tensor(tokens)\n\n    return result\n\n"}
{"type": "source_file", "path": "higpt/model/graph_layers/__init__.py", "content": "from higpt.model.graph_layers.mpnn import MPNN\nfrom higpt.model.graph_layers.clip_graph import CLIP, GNN\nfrom higpt.model.graph_layers.graph_transformer import graph_transformer"}
{"type": "source_file", "path": "HG_grounding/dataset/base_dataset_graph.py", "content": "\"\"\"\n Copyright (c) 2022, salesforce.com, inc.\n All rights reserved.\n SPDX-License-Identifier: BSD-3-Clause\n For full license text, see the LICENSE file in the repo root or https://opensource.org/licenses/BSD-3-Clause\n\"\"\"\n\nimport json\nfrom typing import Iterable\n\nfrom torch.utils.data import Dataset, ConcatDataset\nfrom torch.utils.data.dataloader import default_collate\nimport os.path as osp\n\n\nclass BaseDataset(Dataset):\n    def __init__(\n        self, graph_processor=None, text_processor=None, datasets_root=None, ann_paths=[]\n    ):\n        \"\"\"\n        vis_root (string): Root directory of images (e.g. coco/images/)\n        ann_root (string): directory to store the annotation file\n        \"\"\"\n        self.datasets_root = datasets_root\n\n        self.annotation = []\n        for ann_path in ann_paths:\n            ann_data = json.load(open(ann_path, \"r\"))\n            self.annotation.extend(ann_data)\n            # handle graph path\n            for ann_item in ann_data: \n                ori_graph_path = ann_item[\"graph\"][\"graph\"]\n                # graph_file = ori_graph_path.split(\"/\")[-1]\n                # dsname = ann_path.split(\"/\")[-3]\n                graph_path = osp.join(self.datasets_root, ori_graph_path)\n                assert osp.exists(graph_path), f\"Graph file {graph_path} does not exist!\"\n                ann_item[\"graph\"][\"graph\"] = graph_path\n\n        self.graph_processor = graph_processor\n        self.text_processor = text_processor\n        print(self.annotation[0]['graph'])\n        print(self.annotation[1]['graph'])\n        print(self.annotation[2]['graph'])\n\n        self._add_instance_ids()\n\n    def __len__(self):\n        return len(self.annotation)\n\n    def collater(self, samples):\n        return default_collate(samples)\n\n    def set_processors(self, graph_processor, text_processor):\n        self.graph_processor = graph_processor\n        self.text_processor = text_processor\n\n    def _add_instance_ids(self, key=\"instance_id\"):\n        for idx, ann in enumerate(self.annotation):\n            ann[key] = str(idx)\n\n\nclass ConcatDataset(ConcatDataset):\n    def __init__(self, datasets: Iterable[Dataset]) -> None:\n        super().__init__(datasets)\n\n    def collater(self, samples):\n        # TODO For now only supports datasets with same underlying collater implementations\n\n        all_keys = set()\n        for s in samples:\n            all_keys.update(s)\n\n        shared_keys = all_keys\n        for s in samples:\n            shared_keys = shared_keys & set(s.keys())\n\n        samples_shared_keys = []\n        for s in samples:\n            samples_shared_keys.append({k: s[k] for k in s.keys() if k in shared_keys})\n\n        return self.datasets[0].collater(samples_shared_keys)\n"}
{"type": "source_file", "path": "HG_grounding/logger.py", "content": "\"\"\"\n Copyright (c) 2022, salesforce.com, inc.\n All rights reserved.\n SPDX-License-Identifier: BSD-3-Clause\n For full license text, see the LICENSE file in the repo root or https://opensource.org/licenses/BSD-3-Clause\n\"\"\"\n\nimport datetime\nimport logging\nimport time\nfrom collections import defaultdict, deque\n\nimport torch\nimport torch.distributed as dist\n\nimport dist_utils\n\n\nclass SmoothedValue(object):\n    \"\"\"Track a series of values and provide access to smoothed values over a\n    window or the global series average.\n    \"\"\"\n\n    def __init__(self, window_size=20, fmt=None):\n        if fmt is None:\n            fmt = \"{median:.4f} ({global_avg:.4f})\"\n        self.deque = deque(maxlen=window_size)\n        self.total = 0.0\n        self.count = 0\n        self.fmt = fmt\n\n    def update(self, value, n=1):\n        self.deque.append(value)\n        self.count += n\n        self.total += value * n\n\n    def synchronize_between_processes(self):\n        \"\"\"\n        Warning: does not synchronize the deque!\n        \"\"\"\n        if not dist_utils.is_dist_avail_and_initialized():\n            return\n        t = torch.tensor([self.count, self.total], dtype=torch.float64, device=\"cuda\")\n        dist.barrier()\n        dist.all_reduce(t)\n        t = t.tolist()\n        self.count = int(t[0])\n        self.total = t[1]\n\n    @property\n    def median(self):\n        d = torch.tensor(list(self.deque))\n        return d.median().item()\n\n    @property\n    def avg(self):\n        d = torch.tensor(list(self.deque), dtype=torch.float32)\n        return d.mean().item()\n\n    @property\n    def global_avg(self):\n        return self.total / self.count\n\n    @property\n    def max(self):\n        return max(self.deque)\n\n    @property\n    def value(self):\n        return self.deque[-1]\n\n    def __str__(self):\n        return self.fmt.format(\n            median=self.median,\n            avg=self.avg,\n            global_avg=self.global_avg,\n            max=self.max,\n            value=self.value,\n        )\n\n\nclass MetricLogger(object):\n    def __init__(self, delimiter=\"\\t\"):\n        self.meters = defaultdict(SmoothedValue)\n        self.delimiter = delimiter\n\n    def update(self, **kwargs):\n        for k, v in kwargs.items():\n            if isinstance(v, torch.Tensor):\n                v = v.item()\n            assert isinstance(v, (float, int))\n            self.meters[k].update(v)\n\n    def __getattr__(self, attr):\n        if attr in self.meters:\n            return self.meters[attr]\n        if attr in self.__dict__:\n            return self.__dict__[attr]\n        raise AttributeError(\n            \"'{}' object has no attribute '{}'\".format(type(self).__name__, attr)\n        )\n\n    def __str__(self):\n        loss_str = []\n        for name, meter in self.meters.items():\n            loss_str.append(\"{}: {}\".format(name, str(meter)))\n        return self.delimiter.join(loss_str)\n\n    def global_avg(self):\n        loss_str = []\n        for name, meter in self.meters.items():\n            loss_str.append(\"{}: {:.4f}\".format(name, meter.global_avg))\n        return self.delimiter.join(loss_str)\n\n    def synchronize_between_processes(self):\n        for meter in self.meters.values():\n            meter.synchronize_between_processes()\n\n    def add_meter(self, name, meter):\n        self.meters[name] = meter\n\n    def log_every(self, iterable, print_freq, header=None):\n        i = 0\n        if not header:\n            header = \"\"\n        start_time = time.time()\n        end = time.time()\n        iter_time = SmoothedValue(fmt=\"{avg:.4f}\")\n        data_time = SmoothedValue(fmt=\"{avg:.4f}\")\n        space_fmt = \":\" + str(len(str(len(iterable)))) + \"d\"\n        log_msg = [\n            header,\n            \"[{0\" + space_fmt + \"}/{1}]\",\n            \"eta: {eta}\",\n            \"{meters}\",\n            \"time: {time}\",\n            \"data: {data}\",\n        ]\n        if torch.cuda.is_available():\n            log_msg.append(\"max mem: {memory:.0f}\")\n        log_msg = self.delimiter.join(log_msg)\n        MB = 1024.0 * 1024.0\n        for obj in iterable:\n            data_time.update(time.time() - end)\n            yield obj\n            iter_time.update(time.time() - end)\n            if i % print_freq == 0 or i == len(iterable) - 1:\n                eta_seconds = iter_time.global_avg * (len(iterable) - i)\n                eta_string = str(datetime.timedelta(seconds=int(eta_seconds)))\n                if torch.cuda.is_available():\n                    print(\n                        log_msg.format(\n                            i,\n                            len(iterable),\n                            eta=eta_string,\n                            meters=str(self),\n                            time=str(iter_time),\n                            data=str(data_time),\n                            memory=torch.cuda.max_memory_allocated() / MB,\n                        )\n                    )\n                else:\n                    print(\n                        log_msg.format(\n                            i,\n                            len(iterable),\n                            eta=eta_string,\n                            meters=str(self),\n                            time=str(iter_time),\n                            data=str(data_time),\n                        )\n                    )\n            i += 1\n            end = time.time()\n        total_time = time.time() - start_time\n        total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n        print(\n            \"{} Total time: {} ({:.4f} s / it)\".format(\n                header, total_time_str, total_time / len(iterable)\n            )\n        )\n\n\nclass AttrDict(dict):\n    def __init__(self, *args, **kwargs):\n        super(AttrDict, self).__init__(*args, **kwargs)\n        self.__dict__ = self\n\n\ndef setup_logger():\n    logging.basicConfig(\n        level=logging.INFO if dist_utils.is_main_process() else logging.WARN,\n        format=\"%(asctime)s [%(levelname)s] %(message)s\",\n        handlers=[logging.StreamHandler()],\n    )\n"}
{"type": "source_file", "path": "HG_grounding/base_model.py", "content": "\"\"\"\n Copyright (c) 2022, salesforce.com, inc.\n All rights reserved.\n SPDX-License-Identifier: BSD-3-Clause\n For full license text, see the LICENSE file in the repo root or https://opensource.org/licenses/BSD-3-Clause\n\"\"\"\n\nimport logging\nimport os\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom dist_utils import download_cached_file, is_dist_avail_and_initialized\nfrom utils import get_abs_path, is_url\nfrom omegaconf import OmegaConf\n\n\nclass BaseModel(nn.Module):\n    \"\"\"Base class for models.\"\"\"\n\n    def __init__(self):\n        super().__init__()\n\n    @property\n    def device(self):\n        return list(self.parameters())[0].device\n\n    def load_checkpoint(self, url_or_filename):\n        \"\"\"\n        Load from a finetuned checkpoint.\n\n        This should expect no mismatch in the model keys and the checkpoint keys.\n        \"\"\"\n\n        if is_url(url_or_filename):\n            cached_file = download_cached_file(\n                url_or_filename, check_hash=False, progress=True\n            )\n            checkpoint = torch.load(cached_file, map_location=\"cpu\")\n        elif os.path.isfile(url_or_filename):\n            checkpoint = torch.load(url_or_filename, map_location=\"cpu\")\n        else:\n            raise RuntimeError(\"checkpoint url or path is invalid\")\n\n        if \"model\" in checkpoint.keys():\n            state_dict = checkpoint[\"model\"]\n        else:\n            state_dict = checkpoint\n\n        msg = self.load_state_dict(state_dict, strict=False)\n\n        logging.info(\"Missing keys {}\".format(msg.missing_keys))\n        logging.info(\"load checkpoint from %s\" % url_or_filename)\n\n        return msg\n\n    @classmethod\n    def from_pretrained(cls, model_type):\n        \"\"\"\n        Build a pretrained model from default configuration file, specified by model_type.\n\n        Args:\n            - model_type (str): model type, specifying architecture and checkpoints.\n\n        Returns:\n            - model (nn.Module): pretrained or finetuned model, depending on the configuration.\n        \"\"\"\n        model_cfg = OmegaConf.load(cls.default_config_path(model_type)).model\n        model = cls.from_config(model_cfg)\n\n        return model\n\n    @classmethod\n    def default_config_path(cls, model_type):\n        assert (\n            model_type in cls.PRETRAINED_MODEL_CONFIG_DICT\n        ), \"Unknown model type {}\".format(model_type)\n        return get_abs_path(cls.PRETRAINED_MODEL_CONFIG_DICT[model_type])\n\n    def load_checkpoint_from_config(self, cfg, **kwargs):\n        \"\"\"\n        Load checkpoint as specified in the config file.\n\n        If load_finetuned is True, load the finetuned model; otherwise, load the pretrained model.\n        When loading the pretrained model, each task-specific architecture may define their\n        own load_from_pretrained() method.\n        \"\"\"\n        load_finetuned = cfg.get(\"load_finetuned\", True)\n        if load_finetuned:\n            finetune_path = cfg.get(\"finetuned\", None)\n            assert (\n                finetune_path is not None\n            ), \"Found load_finetuned is True, but finetune_path is None.\"\n            self.load_checkpoint(url_or_filename=finetune_path)\n        else:\n            load_pretrained = cfg.get(\"load_pretrained\", True)\n            if load_pretrained:\n                # load pre-trained weights\n                pretrain_path = cfg.get(\"pretrained\", None)\n                assert \"Found load_finetuned is False, but pretrain_path is None.\"\n                self.load_from_pretrained(url_or_filename=pretrain_path, **kwargs)\n\n    def before_training(self, **kwargs):\n        pass\n\n    def get_optimizer_params(self, weight_decay, lr_scale=1):\n        p_wd, p_non_wd = [], []\n        for n, p in self.named_parameters():\n            if not p.requires_grad:\n                continue  # frozen weights\n            if p.ndim < 2 or \"bias\" in n or \"ln\" in n or \"bn\" in n:\n                p_non_wd.append(p)\n            else:\n                p_wd.append(p)        \n        optim_params = [\n            {\"params\": p_wd, \"weight_decay\": weight_decay, \"lr_scale\": lr_scale},\n            {\"params\": p_non_wd, \"weight_decay\": 0, \"lr_scale\": lr_scale},\n        ]                \n        return optim_params\n    \n    def before_evaluation(self, **kwargs):\n        pass\n\n    def show_n_params(self, return_str=True):\n        tot = 0\n        for p in self.parameters():\n            w = 1\n            for x in p.shape:\n                w *= x\n            tot += w\n        if return_str:\n            if tot >= 1e6:\n                return \"{:.1f}M\".format(tot / 1e6)\n            else:\n                return \"{:.1f}K\".format(tot / 1e3)\n        else:\n            return tot\n\n\nclass BaseEncoder(nn.Module):\n    \"\"\"\n    Base class for primitive encoders, such as ViT, TimeSformer, etc.\n    \"\"\"\n\n    def __init__(self):\n        super().__init__()\n\n    def forward_features(self, samples, **kwargs):\n        raise NotImplementedError\n\n    @property\n    def device(self):\n        return list(self.parameters())[0].device\n\n\nclass SharedQueueMixin:\n    @torch.no_grad()\n    def _dequeue_and_enqueue(self, image_feat, text_feat, idxs=None):\n        # gather keys before updating queue\n        image_feats = concat_all_gather(image_feat)\n        text_feats = concat_all_gather(text_feat)\n\n        batch_size = image_feats.shape[0]\n\n        ptr = int(self.queue_ptr)\n        assert self.queue_size % batch_size == 0  # for simplicity\n\n        # replace the keys at ptr (dequeue and enqueue)\n        self.image_queue[:, ptr : ptr + batch_size] = image_feats.T\n        self.text_queue[:, ptr : ptr + batch_size] = text_feats.T\n\n        if idxs is not None:\n            idxs = concat_all_gather(idxs)\n            self.idx_queue[:, ptr : ptr + batch_size] = idxs.T\n\n        ptr = (ptr + batch_size) % self.queue_size  # move pointer\n        self.queue_ptr[0] = ptr\n\n\nclass MomentumDistilationMixin:\n    @torch.no_grad()\n    def copy_params(self):\n        for model_pair in self.model_pairs:\n            for param, param_m in zip(\n                model_pair[0].parameters(), model_pair[1].parameters()\n            ):\n                param_m.data.copy_(param.data)  # initialize\n                param_m.requires_grad = False  # not update by gradient\n\n    @torch.no_grad()\n    def _momentum_update(self):\n        for model_pair in self.model_pairs:\n            for param, param_m in zip(\n                model_pair[0].parameters(), model_pair[1].parameters()\n            ):\n                param_m.data = param_m.data * self.momentum + param.data * (\n                    1.0 - self.momentum\n                )\n\n\nclass GatherLayer(torch.autograd.Function):\n    \"\"\"\n    Gather tensors from all workers with support for backward propagation:\n    This implementation does not cut the gradients as torch.distributed.all_gather does.\n    \"\"\"\n\n    @staticmethod\n    def forward(ctx, x):\n        output = [\n            torch.zeros_like(x) for _ in range(torch.distributed.get_world_size())\n        ]\n        torch.distributed.all_gather(output, x)\n        return tuple(output)\n\n    @staticmethod\n    def backward(ctx, *grads):\n        all_gradients = torch.stack(grads)\n        torch.distributed.all_reduce(all_gradients)\n        return all_gradients[torch.distributed.get_rank()]\n\n\ndef all_gather_with_grad(tensors):\n    \"\"\"\n    Performs all_gather operation on the provided tensors.\n    Graph remains connected for backward grad computation.\n    \"\"\"\n    # Queue the gathered tensors\n    world_size = torch.distributed.get_world_size()\n    # There is no need for reduction in the single-proc case\n    if world_size == 1:\n        return tensors\n\n    # tensor_all = GatherLayer.apply(tensors)\n    tensor_all = GatherLayer.apply(tensors)\n\n    return torch.cat(tensor_all, dim=0)\n\n\n@torch.no_grad()\ndef concat_all_gather(tensor):\n    \"\"\"\n    Performs all_gather operation on the provided tensors.\n    *** Warning ***: torch.distributed.all_gather has no gradient.\n    \"\"\"\n    # if use distributed training\n    if not is_dist_avail_and_initialized():\n        return tensor\n\n    tensors_gather = [\n        torch.ones_like(tensor) for _ in range(torch.distributed.get_world_size())\n    ]\n    torch.distributed.all_gather(tensors_gather, tensor, async_op=False)\n\n    output = torch.cat(tensors_gather, dim=0)\n    return output\n\n\ndef tile(x, dim, n_tile):\n    init_dim = x.size(dim)\n    repeat_idx = [1] * x.dim()\n    repeat_idx[dim] = n_tile\n    x = x.repeat(*(repeat_idx))\n    order_index = torch.LongTensor(\n        np.concatenate([init_dim * np.arange(n_tile) + i for i in range(init_dim)])\n    )\n    return torch.index_select(x, dim, order_index.to(x.device))\n"}
{"type": "source_file", "path": "HG_grounding/models/clip_models/homo_clip.py", "content": "\"\"\"\n Copyright (c) 2022, salesforce.com, inc.\n All rights reserved.\n SPDX-License-Identifier: BSD-3-Clause\n For full license text, see the LICENSE file in the repo root or https://opensource.org/licenses/BSD-3-Clause\n\n Based on https://github.com/mlfoundations/open_clip\n\"\"\"\n\n\"\"\" CLIP Model\nAdapted from https://github.com/openai/CLIP. Originally MIT License, Copyright (c) 2021 OpenAI.\n\"\"\"\n\nimport datetime\nimport json\nimport logging\nimport os\nimport re\nimport time\nimport warnings\nfrom collections import OrderedDict\nfrom copy import deepcopy\nfrom dataclasses import dataclass\nfrom pathlib import Path\nfrom typing import Callable, List, Optional, Tuple, Union, Dict\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\n# from lavis.common.registry import registry\n# from lavis.common.utils import get_abs_path\nfrom base_model import BaseModel\nfrom models.clip_models.clip_outputs import ClipOutput, HeteClipOutputFeatures\n# from models.clip_models.timm_model import TimmModel\nfrom models.clip_models.transform import image_transform\nfrom models.clip_models.utils import freeze_batch_norm_2d\n# from lavis.tasks.multimodal_classification import MultimodalClassificationTask\nfrom torch import nn\nfrom models.meta_hgt.meta_hgtconv_bert_all import MetaHGTConv, MetaHGTConvCfg\n# from models.homo_hgt.homo_gt_moe import HomoGTMoE, HomoGTMoEConfig\nfrom torch_geometric.typing import Adj, EdgeType, Metadata, NodeType\nfrom .pretrained import (\n    download_pretrained,\n    get_pretrained_url,\n    list_pretrained_tag_models,\n)\n\n_MODEL_CONFIG_PATHS = [Path(__file__).parent.parent.parent / f\"configs/models/clip/\"]\n_MODEL_CONFIGS = {}  # directory (model_name: config) of model architecture configs\n\n\nclass LayerNorm(nn.LayerNorm):\n    \"\"\"Subclass torch's LayerNorm to handle fp16.\"\"\"\n\n    def forward(self, x: torch.Tensor):\n        orig_type = x.dtype\n        x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n        return x.to(orig_type)\n\n\nclass QuickGELU(nn.Module):\n    # NOTE This is slower than nn.GELU or nn.SiLU and uses more GPU memory\n    def forward(self, x: torch.Tensor):\n        return x * torch.sigmoid(1.702 * x)\n\n\nclass ResidualAttentionBlock(nn.Module):\n    def __init__(self, d_model: int, n_head: int, act_layer: Callable = nn.GELU):\n        super().__init__()\n\n        self.attn = nn.MultiheadAttention(d_model, n_head)\n        self.ln_1 = LayerNorm(d_model)\n        self.mlp = nn.Sequential(\n            OrderedDict(\n                [\n                    (\"c_fc\", nn.Linear(d_model, d_model * 4)),\n                    (\"gelu\", act_layer()),\n                    (\"c_proj\", nn.Linear(d_model * 4, d_model)),\n                ]\n            )\n        )\n        self.ln_2 = LayerNorm(d_model)\n\n    def attention(self, x: torch.Tensor, attn_mask: Optional[torch.Tensor] = None):\n        return self.attn(x, x, x, need_weights=False, attn_mask=attn_mask)[0]\n\n    def forward(self, x: torch.Tensor, attn_mask: Optional[torch.Tensor] = None):\n        x = x + self.attention(self.ln_1(x), attn_mask=attn_mask)\n        x = x + self.mlp(self.ln_2(x))\n        return x\n\n\nclass Transformer(nn.Module):\n    def __init__(\n        self, width: int, layers: int, heads: int, act_layer: Callable = nn.GELU\n    ):\n        super().__init__()\n        self.width = width\n        self.layers = layers\n        self.resblocks = nn.ModuleList(\n            [\n                ResidualAttentionBlock(width, heads, act_layer=act_layer)\n                for _ in range(layers)\n            ]\n        )\n\n    def forward(self, x: torch.Tensor, attn_mask: Optional[torch.Tensor] = None):\n        for r in self.resblocks:\n            x = r(x, attn_mask=attn_mask)\n        return x\n\n\n@dataclass\nclass CLIPTextCfg:\n    context_length: int\n    vocab_size: int\n    width: int\n    heads: int\n    layers: int\n\n\n# @registry.register_model(\"clip\")\n# @registry.register_model(\"clip_feature_extractor\")\nclass CLIP(BaseModel):\n\n    def __init__(\n        self,\n        embed_dim: int,\n        graph_cfg: MetaHGTConvCfg,\n        text_cfg: CLIPTextCfg,\n        quick_gelu: bool = False,\n        gnn_type: str = \"meta\", # \"meta\", \"homo\"\n    ):\n        from .tokenizer import tokenize\n\n        super().__init__()\n\n        self.tokenizer = tokenize\n        self._loss = None\n\n        if isinstance(graph_cfg, dict):\n            if gnn_type == \"meta\":\n                graph_cfg = MetaHGTConvCfg(**graph_cfg)\n            elif gnn_type == \"homo\":\n                graph_cfg = HomoGTMoEConfig(**graph_cfg)\n            else: \n                raise NotImplementedError\n        if isinstance(text_cfg, dict):\n            text_cfg = CLIPTextCfg(**text_cfg)\n\n        self.context_length = text_cfg.context_length\n\n        # OpenAI models are pretrained w/ QuickGELU but native nn.GELU is both faster and more\n        # memory efficient in recent PyTorch releases (>= 1.10).\n        # NOTE: timm models always use native GELU regardless of quick_gelu flag.\n        act_layer = QuickGELU if quick_gelu else nn.GELU\n\n        if gnn_type == \"meta\":\n            self.graph_encoder = MetaHGTConv(\n                in_channels = graph_cfg.in_channels,\n                out_channels = graph_cfg.out_channels,\n                heads = graph_cfg.heads,\n                dynamic = graph_cfg.dynamic,\n                text_cfg = text_cfg, \n                layernorm = LayerNorm \n            )\n        elif gnn_type == \"homo\":\n            self.graph_encoder = HomoGTMoE(\n                graph_cfg, \n                text_transformer=Transformer, \n                text_cfg = text_cfg, \n                layernorm = LayerNorm\n            )\n        else:\n            raise NotImplementedError\n\n        self.transformer = Transformer(\n            width=text_cfg.width,\n            layers=text_cfg.layers,\n            heads=text_cfg.heads,\n            act_layer=act_layer,\n        )\n\n        self.vocab_size = text_cfg.vocab_size\n        self.token_embedding = nn.Embedding(text_cfg.vocab_size, text_cfg.width)\n        self.positional_embedding = nn.Parameter(\n            torch.empty(self.context_length, text_cfg.width)\n        )\n        self.ln_final = LayerNorm(text_cfg.width)\n\n        self.text_projection = nn.Parameter(torch.empty(text_cfg.width, embed_dim))\n        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07), requires_grad = False)\n        self.register_buffer(\"attn_mask\", self.build_attention_mask(), persistent=False)\n\n        self.prompt_templates = openai_imagenet_template\n        self.classifier = None\n\n        self.init_parameters()\n\n    @property\n    def loss(self):\n        if self._loss is None:\n            from models.clip_models.loss import HeteClipLoss\n\n            self._loss = HeteClipLoss()\n\n        return self._loss\n\n    def init_parameters(self):\n        nn.init.normal_(self.token_embedding.weight, std=0.02)\n        nn.init.normal_(self.positional_embedding, std=0.01)\n        nn.init.constant_(self.logit_scale, np.log(1 / 0.07))\n\n\n        proj_std = (self.transformer.width**-0.5) * (\n            (2 * self.transformer.layers) ** -0.5\n        )\n        attn_std = self.transformer.width**-0.5\n        fc_std = (2 * self.transformer.width) ** -0.5\n        for block in self.transformer.resblocks:\n            nn.init.normal_(block.attn.in_proj_weight, std=attn_std)\n            nn.init.normal_(block.attn.out_proj.weight, std=proj_std)\n            nn.init.normal_(block.mlp.c_fc.weight, std=fc_std)\n            nn.init.normal_(block.mlp.c_proj.weight, std=proj_std)\n\n        if self.text_projection is not None:\n            nn.init.normal_(self.text_projection, std=self.transformer.width**-0.5)\n\n    def build_attention_mask(self):\n        # lazily create causal attention mask, with full attention between the vision tokens\n        # pytorch uses additive attention mask; fill with -inf\n        mask = torch.empty(self.context_length, self.context_length)\n        mask.fill_(float(\"-inf\"))\n        mask.triu_(1)  # zero out the lower diagonal\n        return mask\n\n    def lock_image_tower(self, unlocked_groups=0, freeze_bn_stats=False):\n        # lock image tower as per LiT - https://arxiv.org/abs/2111.07991\n        self.visual.lock(\n            unlocked_groups=unlocked_groups, freeze_bn_stats=freeze_bn_stats\n        )\n\n    def encode_graph(self, graph: List[Dict[str, torch.Tensor]], des_order: List[List[str]], node_type_feas_dict: List[Dict[NodeType, torch.Tensor]] = None,\n        edge_type_feas_dict: List[Dict[EdgeType, torch.Tensor]] = None,):\n        graph_list = []\n        for idx, graph_dict in enumerate(graph): \n            graph_list.append(self.graph_encoder(graph_dict.x_dict, graph_dict.edge_index_dict, node_type_feas_dict = node_type_feas_dict[idx], edge_type_feas_dict = edge_type_feas_dict[idx]))\n        graph_embeds = []\n        assert len(graph_list) == len(des_order)\n        for idx, order in enumerate(des_order): \n            graph_embeds.extend([graph_list[idx][o] for o in order])\n        graph_embeds = torch.cat(graph_embeds, dim = 0)\n        return graph_embeds\n\n    def encode_text(self, text):\n        x = self.token_embedding(text)  # [batch_size, n_ctx, d_model]\n\n        x = x + self.positional_embedding\n        x = x.permute(1, 0, 2)  # NLD -> LND\n        x = self.transformer(x, attn_mask=self.attn_mask)\n        x = x.permute(1, 0, 2)  # LND -> NLD\n        x = self.ln_final(x)\n\n        # x.shape = [batch_size, n_ctx, transformer.width]\n        # take features from the eot embedding (eot_token is the highest number in each sequence)\n        x = x[torch.arange(x.shape[0]), text.argmax(dim=-1)] @ self.text_projection\n\n        return x\n\n    # def forward(self, image, text):\n    def forward(self, samples):\n        r'''\n        samples: \n        \"graph\": List[Dict],\n        \"text_input\": List[str],\n        \"des_order\": List[str], \n        \"graph_id\": graph_ids\n        '''\n        graph: List[Dict] = samples.get(\"graph\")\n        text: List[str] = samples.get(\"text_input\")\n        des_order: List[List[str]] = samples.get(\"des_order\")\n\n        if text is not None:\n            text = self.tokenizer(text, self.context_length).to(self.token_embedding.weight.device)\n\n        if graph is None:\n            return self.encode_text(text) # N, dim\n        elif text is None:\n            return self.encode_graph(graph, des_order)\n        edge_type_feas_dict = samples.get('edge_type_feas_dict')\n        node_type_feas_dict = samples.get('node_type_feas_dict')\n        graph_embeds = self.encode_graph(graph, des_order, node_type_feas_dict = node_type_feas_dict, edge_type_feas_dict = edge_type_feas_dict)\n        graph_features = F.normalize(graph_embeds, dim=-1)\n\n        text_embeds = self.encode_text(text)\n        text_features = F.normalize(text_embeds, dim=-1)\n        assert graph_features.shape == text_features.shape, f'{graph_features.shape} != {text_features.shape}'\n\n        loss = self.loss(graph_features, text_features, self.logit_scale.exp())\n\n        # return {\"loss\": loss}\n        return ClipOutput(\n            intermediate_output=HeteClipOutputFeatures(\n                graph_embeds=graph_embeds,\n                graph_embeds_proj=graph_features,\n                text_embeds=text_embeds,\n                text_embeds_proj=text_features,\n            ),\n            loss=loss,\n            logit_scale_exp=self.logit_scale.exp(),\n        )\n\n    def extract_features(self, samples):\n        \"\"\"\n        Extract features from the model for samples.\n\n        Keys allowed are \"image\" and \"text_input\" in samples.\n        If either key is missing, the corresponding features are not extracted.\n\n        Args:\n            samples: dict of samples to extract features from.\n\n        Returns:\n            ClipOutputFeatures object with features for the samples.\n        \"\"\"\n        graph: List[Dict] = samples.get(\"graph\")\n        text: List[str] = samples.get(\"text_input\")\n        des_order: List[List[str]] = samples.get(\"des_order\")\n\n        if text is not None:\n            text = self.tokenizer(text)\n\n        if graph is None:\n            return self.encode_text(text) # N, dim\n        elif text is None:\n            return self.encode_graph(graph, des_order)\n        graph_embeds = self.encode_graph(graph, des_order)\n        graph_features = F.normalize(graph_embeds, dim=-1)\n\n        text_embeds = self.encode_text(text)\n        text_features = F.normalize(text_embeds, dim=-1)\n        assert graph_features.shape == text_features.shape\n\n        return HeteClipOutputFeatures(\n                graph_embeds=graph_embeds,\n                graph_embeds_proj=graph_features,\n                text_embeds=text_embeds,\n                text_embeds_proj=text_features,\n            )\n\n    def predict(self, samples):\n        image = samples[\"image\"]\n        targets = samples[\"label\"]\n\n        image_features = self.encode_image(image)\n        image_features = F.normalize(image_features, dim=-1)\n\n        logits = 100.0 * image_features @ self.classifier\n\n        return {\"predictions\": logits, \"targets\": targets}\n\n    def before_evaluation(self, dataset, task_type, **kwargs):\n        if task_type == MultimodalClassificationTask:\n            self.classifier = self.zero_shot_classifier(\n                classnames=dataset.classnames,\n                templates=self.prompt_templates,\n            )\n\n    def zero_shot_classifier(self, classnames, templates):\n        with torch.no_grad():\n            zeroshot_weights = []\n            for classname in classnames:\n                texts = [\n                    template(classname) for template in templates\n                ]  # format with class\n                texts = self.tokenizer(texts).to(self.device)  # tokenize\n\n                class_embeddings = self.encode_text(texts)\n                class_embedding = F.normalize(class_embeddings, dim=-1).mean(dim=0)\n                class_embedding /= class_embedding.norm()\n                zeroshot_weights.append(class_embedding)\n            zeroshot_weights = torch.stack(zeroshot_weights, dim=1).to(self.device)\n        return zeroshot_weights\n\n    @classmethod\n    def default_config_path(cls, model_type=\"base\"):\n        model_type = \"ViT-B-32\" if model_type == \"base\" else model_type\n\n        assert (\n            model_type in cls.PRETRAINED_MODEL_CONFIG_DICT\n        ), \"Unknown model type {}. \\n Available types: {}\".format(\n            model_type, cls.PRETRAINED_MODEL_CONFIG_DICT.keys()\n        )\n        return get_abs_path(cls.PRETRAINED_MODEL_CONFIG_DICT[model_type])\n\n    @classmethod\n    def from_config(cls, cfg=None):\n        model_name = cfg.model_type\n        pretrained = cfg.pretrained\n\n        precision = cfg.get(\"precision\", \"fp32\")\n\n        return create_model(\n            model_name=model_name, pretrained=pretrained, precision=precision\n        )\n\n    def zero_shot_predict(self, image_path, categories):\n        assert isinstance(\n            categories, list\n        ), f\"categories must be a list, got {type(categories)}.\"\n        assert os.path.exists(image_path), f\"File {image_path} does not exist.\"\n\n        from lavis.processors.clip_processors import ClipImageEvalProcessor\n        from PIL import Image\n\n        image_preprocess = ClipImageEvalProcessor()\n        image = image_preprocess(Image.open(image_path)).unsqueeze(0)\n\n        text = self.tokenizer(categories)\n\n        with torch.no_grad():\n            image_features = self.encode_image(image)\n            text_features = self.encode_text(text)\n            image_features /= image_features.norm(dim=-1, keepdim=True)\n            text_features /= text_features.norm(dim=-1, keepdim=True)\n\n            text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n\n            print(\"Label probs:\", text_probs)  # prints: [[1., 0., 0.]]\n\n    def compute_sim_matrix(self, data_loader, **kwargs):\n        logging.info(\"Computing features for evaluation...\")\n        start_time = time.time()\n\n        texts = data_loader.dataset.text\n        num_text = len(texts)\n        text_bs = 256\n        text_features = []\n\n        for i in range(0, num_text, text_bs):\n\n            text = texts[i : min(num_text, i + text_bs)]\n            text_input = self.tokenizer(text).to(self.device)\n\n            text_feat = self.encode_text(text_input)\n            text_feat = F.normalize(text_feat, dim=-1)\n\n            text_features.append(text_feat)\n\n        text_features = torch.cat(text_features, dim=0)\n\n        image_features = []\n        for samples in data_loader:\n            image = samples[\"image\"]\n\n            image = image.to(self.device)\n            image_feat = self.encode_image(image)\n            image_feat = F.normalize(image_feat, dim=-1)\n\n            image_features.append(image_feat)\n\n        image_features = torch.cat(image_features, dim=0)\n\n        sims_matrix_i2t = image_features @ text_features.t()\n        sims_matrix_t2i = sims_matrix_i2t.t()\n\n        total_time = time.time() - start_time\n        total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n        logging.info(\"Evaluation time {}\".format(total_time_str))\n\n        return sims_matrix_i2t.cpu().numpy(), sims_matrix_t2i.cpu().numpy()\n\n\ndef convert_weights_to_fp16(model: nn.Module):\n    \"\"\"Convert applicable model parameters to fp16\"\"\"\n\n    def _convert_weights_to_fp16(l):\n        if isinstance(l, (nn.Conv1d, nn.Conv2d, nn.Linear)):\n            l.weight.data = l.weight.data.half()\n            if l.bias is not None:\n                l.bias.data = l.bias.data.half()\n\n        if isinstance(l, nn.MultiheadAttention):\n            for attr in [\n                *[f\"{s}_proj_weight\" for s in [\"in\", \"q\", \"k\", \"v\"]],\n                \"in_proj_bias\",\n                \"bias_k\",\n                \"bias_v\",\n            ]:\n                tensor = getattr(l, attr)\n                if tensor is not None:\n                    tensor.data = tensor.data.half()\n\n        for name in [\"text_projection\", \"proj\"]:\n            if hasattr(l, name):\n                attr = getattr(l, name)\n                if attr is not None:\n                    attr.data = attr.data.half()\n\n    model.apply(_convert_weights_to_fp16)\n\n\ndef build_model_from_openai_state_dict(state_dict: dict):\n    vit = \"visual.proj\" in state_dict\n\n    if vit:\n        vision_width = state_dict[\"visual.conv1.weight\"].shape[0]\n        vision_layers = len(\n            [\n                k\n                for k in state_dict.keys()\n                if k.startswith(\"visual.\") and k.endswith(\".attn.in_proj_weight\")\n            ]\n        )\n        vision_patch_size = state_dict[\"visual.conv1.weight\"].shape[-1]\n        grid_size = round(\n            (state_dict[\"visual.positional_embedding\"].shape[0] - 1) ** 0.5\n        )\n        image_size = vision_patch_size * grid_size\n    else:\n        counts: list = [\n            len(\n                set(\n                    k.split(\".\")[2]\n                    for k in state_dict\n                    if k.startswith(f\"visual.layer{b}\")\n                )\n            )\n            for b in [1, 2, 3, 4]\n        ]\n        vision_layers = tuple(counts)\n        vision_width = state_dict[\"visual.layer1.0.conv1.weight\"].shape[0]\n        output_width = round(\n            (state_dict[\"visual.attnpool.positional_embedding\"].shape[0] - 1) ** 0.5\n        )\n        vision_patch_size = None\n        assert (\n            output_width**2 + 1\n            == state_dict[\"visual.attnpool.positional_embedding\"].shape[0]\n        )\n        image_size = output_width * 32\n\n    embed_dim = state_dict[\"text_projection\"].shape[1]\n    context_length = state_dict[\"positional_embedding\"].shape[0]\n    vocab_size = state_dict[\"token_embedding.weight\"].shape[0]\n    transformer_width = state_dict[\"ln_final.weight\"].shape[0]\n    transformer_heads = transformer_width // 64\n    transformer_layers = len(\n        set(\n            k.split(\".\")[2]\n            for k in state_dict\n            if k.startswith(f\"transformer.resblocks\")\n        )\n    )\n\n    vision_cfg = CLIPVisionCfg(\n        layers=vision_layers,\n        width=vision_width,\n        patch_size=vision_patch_size,\n        image_size=image_size,\n    )\n    text_cfg = CLIPTextCfg(\n        context_length=context_length,\n        vocab_size=vocab_size,\n        width=transformer_width,\n        heads=transformer_heads,\n        layers=transformer_layers,\n    )\n    model = CLIP(\n        embed_dim,\n        vision_cfg=vision_cfg,\n        text_cfg=text_cfg,\n        quick_gelu=True,  # OpenAI models were trained with QuickGELU\n    )\n\n    for key in [\"input_resolution\", \"context_length\", \"vocab_size\"]:\n        state_dict.pop(key, None)\n\n    convert_weights_to_fp16(model)\n    model.load_state_dict(state_dict)\n    return model.eval()\n\n\ndef trace_model(model, batch_size=256, device=torch.device(\"cpu\")):\n    model.eval()\n    image_size = model.visual.image_size\n    example_images = torch.ones((batch_size, 3, image_size, image_size), device=device)\n    example_text = torch.zeros(\n        (batch_size, model.context_length), dtype=torch.int, device=device\n    )\n    model = torch.jit.trace_module(\n        model,\n        inputs=dict(\n            forward=(example_images, example_text),\n            encode_text=(example_text,),\n            encode_image=(example_images,),\n        ),\n    )\n    model.visual.image_size = image_size\n    return\n\n\ndef _natural_key(string_):\n    return [int(s) if s.isdigit() else s for s in re.split(r\"(\\d+)\", string_.lower())]\n\n\ndef _rescan_model_configs():\n    global _MODEL_CONFIGS\n\n    config_ext = (\".json\",)\n    config_files = []\n    for config_path in _MODEL_CONFIG_PATHS:\n        if config_path.is_file() and config_path.suffix in config_ext:\n            config_files.append(config_path)\n        elif config_path.is_dir():\n            for ext in config_ext:\n                config_files.extend(config_path.glob(f\"*{ext}\"))\n\n    for cf in config_files:\n        with open(cf, \"r\") as f:\n            model_cfg = json.load(f)\n            if all(a in model_cfg for a in (\"embed_dim\", \"vision_cfg\", \"text_cfg\")):\n                _MODEL_CONFIGS[cf.stem] = model_cfg\n\n    _MODEL_CONFIGS = {\n        k: v\n        for k, v in sorted(_MODEL_CONFIGS.items(), key=lambda x: _natural_key(x[0]))\n    }\n\n\n_rescan_model_configs()  # initial populate of model config registry\n\n\ndef load_state_dict(checkpoint_path: str, map_location=\"cpu\"):\n    checkpoint = torch.load(checkpoint_path, map_location=map_location)\n    if isinstance(checkpoint, dict) and \"state_dict\" in checkpoint:\n        state_dict = checkpoint[\"state_dict\"]\n    else:\n        state_dict = checkpoint\n    if next(iter(state_dict.items()))[0].startswith(\"module\"):\n        state_dict = {k[7:]: v for k, v in state_dict.items()}\n    return state_dict\n\n\ndef create_model(\n    model_name: str,\n    pretrained: str = \"\",\n    precision: str = \"fp32\",\n    device: torch.device = torch.device(\"cpu\"),\n    jit: bool = False,\n    force_quick_gelu: bool = False,\n    pretrained_image: bool = False,\n):\n    model_name = model_name.replace(\n        \"/\", \"-\"\n    )  # for callers using old naming with / in ViT names\n\n    if pretrained.lower() == \"openai\":\n        logging.info(f\"Loading pretrained {model_name} from OpenAI.\")\n        model = load_openai_model(model_name, device=device, jit=jit)\n        # See https://discuss.pytorch.org/t/valueerror-attemting-to-unscale-fp16-gradients/81372\n        if precision == \"amp\" or precision == \"fp32\":\n            model = model.float()\n    else:\n        logging.info(f\"No pretrained weights loaded for {model_name} model.\")\n        if model_name in _MODEL_CONFIGS:\n            logging.info(f\"Loading {model_name} model config.\")\n            model_cfg = deepcopy(_MODEL_CONFIGS[model_name])\n        else:\n            logging.error(\n                f\"Model config for {model_name} not found; available models {list_models()}.\"\n            )\n            raise RuntimeError(f\"Model config for {model_name} not found.\")\n\n        if force_quick_gelu:\n            # override for use of QuickGELU on non-OpenAI transformer models\n            model_cfg[\"quick_gelu\"] = True\n\n        if pretrained_image:\n            if \"timm_model_name\" in model_cfg.get(\"vision_cfg\", {}):\n                # pretrained weight loading for timm models set via vision_cfg\n                model_cfg[\"vision_cfg\"][\"timm_model_pretrained\"] = True\n            else:\n                assert (\n                    False\n                ), \"pretrained image towers currently only supported for timm models\"\n\n        model = CLIP(**model_cfg)\n\n        if pretrained:\n            checkpoint_path = \"\"\n            url = get_pretrained_url(model_name, pretrained)\n            if url:\n                checkpoint_path = download_pretrained(url)\n            elif os.path.exists(pretrained):\n                checkpoint_path = pretrained\n\n            if checkpoint_path:\n                logging.info(f\"Loading pretrained {model_name} weights ({pretrained}).\")\n                model.load_state_dict(load_state_dict(checkpoint_path))\n            else:\n                logging.warning(\n                    f\"Pretrained weights ({pretrained}) not found for model {model_name}.\"\n                )\n                raise RuntimeError(\n                    f\"Pretrained weights ({pretrained}) not found for model {model_name}.\"\n                )\n\n        model.to(device=device)\n        if precision == \"fp16\":\n            assert device.type != \"cpu\"\n            convert_weights_to_fp16(model)\n\n        if jit:\n            model = torch.jit.script(model)\n\n    return model\n\n\ndef create_model_and_transforms(\n    model_name: str,\n    pretrained: str = \"\",\n    precision: str = \"fp32\",\n    device: torch.device = torch.device(\"cpu\"),\n    jit: bool = False,\n    force_quick_gelu: bool = False,\n    pretrained_image: bool = False,\n):\n    model = create_model(\n        model_name,\n        pretrained,\n        precision,\n        device,\n        jit,\n        force_quick_gelu=force_quick_gelu,\n        pretrained_image=pretrained_image,\n    )\n    preprocess_train = image_transform(model.visual.image_size, is_train=True)\n    preprocess_val = image_transform(model.visual.image_size, is_train=False)\n    return model, preprocess_train, preprocess_val\n\n\ndef list_models():\n    \"\"\"enumerate available model architectures based on config files\"\"\"\n    return list(_MODEL_CONFIGS.keys())\n\n\ndef add_model_config(path):\n    \"\"\"add model config path or file and update registry\"\"\"\n    if not isinstance(path, Path):\n        path = Path(path)\n    _MODEL_CONFIG_PATHS.append(path)\n    _rescan_model_configs()\n\n\ndef list_openai_models() -> List[str]:\n    \"\"\"Returns the names of available CLIP models\"\"\"\n    return list_pretrained_tag_models(\"openai\")\n\n\ndef load_openai_model(\n    name: str,\n    device: Union[str, torch.device] = \"cuda\" if torch.cuda.is_available() else \"cpu\",\n    jit=True,\n):\n    \"\"\"Load a CLIP model\n    Parameters\n    ----------\n    name : str\n        A model name listed by `clip.available_models()`, or the path to a model checkpoint containing the state_dict\n    device : Union[str, torch.device]\n        The device to put the loaded model\n    jit : bool\n        Whether to load the optimized JIT model (default) or more hackable non-JIT model.\n    Returns\n    -------\n    model : torch.nn.Module\n        The CLIP model\n    preprocess : Callable[[PIL.Image], torch.Tensor]\n        A torchvision transform that converts a PIL image into a tensor that the returned model can take as its input\n    \"\"\"\n    if get_pretrained_url(name, \"openai\"):\n        model_path = download_pretrained(get_pretrained_url(name, \"openai\"))\n    elif os.path.isfile(name):\n        model_path = name\n    else:\n        raise RuntimeError(\n            f\"Model {name} not found; available models = {list_openai_models()}\"\n        )\n\n    try:\n        # loading JIT archive\n        model = torch.jit.load(model_path, map_location=device if jit else \"cpu\").eval()\n        state_dict = None\n    except RuntimeError:\n        # loading saved state dict\n        if jit:\n            warnings.warn(\n                f\"File {model_path} is not a JIT archive. Loading as a state dict instead\"\n            )\n            jit = False\n        state_dict = torch.load(model_path, map_location=\"cpu\")\n\n    if not jit:\n        try:\n            model = build_model_from_openai_state_dict(\n                state_dict or model.state_dict()\n            ).to(device)\n        except KeyError:\n            sd = {k[7:]: v for k, v in state_dict[\"state_dict\"].items()}\n            model = build_model_from_openai_state_dict(sd).to(device)\n\n        if str(device) == \"cpu\":\n            model.float()\n        return model\n\n    # patch the device names\n    device_holder = torch.jit.trace(\n        lambda: torch.ones([]).to(torch.device(device)), example_inputs=[]\n    )\n    device_node = [\n        n\n        for n in device_holder.graph.findAllNodes(\"prim::Constant\")\n        if \"Device\" in repr(n)\n    ][-1]\n\n    def patch_device(module):\n        try:\n            graphs = [module.graph] if hasattr(module, \"graph\") else []\n        except RuntimeError:\n            graphs = []\n\n        if hasattr(module, \"forward1\"):\n            graphs.append(module.forward1.graph)\n\n        for graph in graphs:\n            for node in graph.findAllNodes(\"prim::Constant\"):\n                if \"value\" in node.attributeNames() and str(node[\"value\"]).startswith(\n                    \"cuda\"\n                ):\n                    node.copyAttributes(device_node)\n\n    model.apply(patch_device)\n    patch_device(model.encode_image)\n    patch_device(model.encode_text)\n\n    # patch dtype to float32 on CPU\n    if str(device) == \"cpu\":\n        float_holder = torch.jit.trace(\n            lambda: torch.ones([]).float(), example_inputs=[]\n        )\n        float_input = list(float_holder.graph.findNode(\"aten::to\").inputs())[1]\n        float_node = float_input.node()\n\n        def patch_float(module):\n            try:\n                graphs = [module.graph] if hasattr(module, \"graph\") else []\n            except RuntimeError:\n                graphs = []\n\n            if hasattr(module, \"forward1\"):\n                graphs.append(module.forward1.graph)\n\n            for graph in graphs:\n                for node in graph.findAllNodes(\"aten::to\"):\n                    inputs = list(node.inputs())\n                    for i in [\n                        1,\n                        2,\n                    ]:  # dtype can be the second or third argument to aten::to()\n                        if inputs[i].node()[\"value\"] == 5:\n                            inputs[i].node().copyAttributes(float_node)\n\n        model.apply(patch_float)\n        patch_float(model.encode_image)\n        patch_float(model.encode_text)\n        model.float()\n\n    # ensure image_size attr available at consistent location for both jit and non-jit\n    model.visual.image_size = model.input_resolution.item()\n    return model\n\n\nopenai_imagenet_template = [\n    lambda c: f\"a bad photo of a {c}.\",\n    lambda c: f\"a photo of many {c}.\",\n    lambda c: f\"a sculpture of a {c}.\",\n    lambda c: f\"a photo of the hard to see {c}.\",\n    lambda c: f\"a low resolution photo of the {c}.\",\n    lambda c: f\"a rendering of a {c}.\",\n    lambda c: f\"graffiti of a {c}.\",\n    lambda c: f\"a bad photo of the {c}.\",\n    lambda c: f\"a cropped photo of the {c}.\",\n    lambda c: f\"a tattoo of a {c}.\",\n    lambda c: f\"the embroidered {c}.\",\n    lambda c: f\"a photo of a hard to see {c}.\",\n    lambda c: f\"a bright photo of a {c}.\",\n    lambda c: f\"a photo of a clean {c}.\",\n    lambda c: f\"a photo of a dirty {c}.\",\n    lambda c: f\"a dark photo of the {c}.\",\n    lambda c: f\"a drawing of a {c}.\",\n    lambda c: f\"a photo of my {c}.\",\n    lambda c: f\"the plastic {c}.\",\n    lambda c: f\"a photo of the cool {c}.\",\n    lambda c: f\"a close-up photo of a {c}.\",\n    lambda c: f\"a black and white photo of the {c}.\",\n    lambda c: f\"a painting of the {c}.\",\n    lambda c: f\"a painting of a {c}.\",\n    lambda c: f\"a pixelated photo of the {c}.\",\n    lambda c: f\"a sculpture of the {c}.\",\n    lambda c: f\"a bright photo of the {c}.\",\n    lambda c: f\"a cropped photo of a {c}.\",\n    lambda c: f\"a plastic {c}.\",\n    lambda c: f\"a photo of the dirty {c}.\",\n    lambda c: f\"a jpeg corrupted photo of a {c}.\",\n    lambda c: f\"a blurry photo of the {c}.\",\n    lambda c: f\"a photo of the {c}.\",\n    lambda c: f\"a good photo of the {c}.\",\n    lambda c: f\"a rendering of the {c}.\",\n    lambda c: f\"a {c} in a video game.\",\n    lambda c: f\"a photo of one {c}.\",\n    lambda c: f\"a doodle of a {c}.\",\n    lambda c: f\"a close-up photo of the {c}.\",\n    lambda c: f\"a photo of a {c}.\",\n    lambda c: f\"the origami {c}.\",\n    lambda c: f\"the {c} in a video game.\",\n    lambda c: f\"a sketch of a {c}.\",\n    lambda c: f\"a doodle of the {c}.\",\n    lambda c: f\"a origami {c}.\",\n    lambda c: f\"a low resolution photo of a {c}.\",\n    lambda c: f\"the toy {c}.\",\n    lambda c: f\"a rendition of the {c}.\",\n    lambda c: f\"a photo of the clean {c}.\",\n    lambda c: f\"a photo of a large {c}.\",\n    lambda c: f\"a rendition of a {c}.\",\n    lambda c: f\"a photo of a nice {c}.\",\n    lambda c: f\"a photo of a weird {c}.\",\n    lambda c: f\"a blurry photo of a {c}.\",\n    lambda c: f\"a cartoon {c}.\",\n    lambda c: f\"art of a {c}.\",\n    lambda c: f\"a sketch of the {c}.\",\n    lambda c: f\"a embroidered {c}.\",\n    lambda c: f\"a pixelated photo of a {c}.\",\n    lambda c: f\"itap of the {c}.\",\n    lambda c: f\"a jpeg corrupted photo of the {c}.\",\n    lambda c: f\"a good photo of a {c}.\",\n    lambda c: f\"a plushie {c}.\",\n    lambda c: f\"a photo of the nice {c}.\",\n    lambda c: f\"a photo of the small {c}.\",\n    lambda c: f\"a photo of the weird {c}.\",\n    lambda c: f\"the cartoon {c}.\",\n    lambda c: f\"art of the {c}.\",\n    lambda c: f\"a drawing of the {c}.\",\n    lambda c: f\"a photo of the large {c}.\",\n    lambda c: f\"a black and white photo of a {c}.\",\n    lambda c: f\"the plushie {c}.\",\n    lambda c: f\"a dark photo of a {c}.\",\n    lambda c: f\"itap of a {c}.\",\n    lambda c: f\"graffiti of the {c}.\",\n    lambda c: f\"a toy {c}.\",\n    lambda c: f\"itap of my {c}.\",\n    lambda c: f\"a photo of a cool {c}.\",\n    lambda c: f\"a photo of a small {c}.\",\n    lambda c: f\"a tattoo of the {c}.\",\n]\n"}
{"type": "source_file", "path": "HG_grounding/dist_utils.py", "content": "\"\"\"\n Copyright (c) 2022, salesforce.com, inc.\n All rights reserved.\n SPDX-License-Identifier: BSD-3-Clause\n For full license text, see the LICENSE file in the repo root or https://opensource.org/licenses/BSD-3-Clause\n\"\"\"\n\nimport datetime\nimport functools\nimport os\n\nimport torch\nimport torch.distributed as dist\nimport timm.models.hub as timm_hub\n\n\ndef setup_for_distributed(is_master):\n    \"\"\"\n    This function disables printing when not in master process\n    \"\"\"\n    import builtins as __builtin__\n\n    builtin_print = __builtin__.print\n\n    def print(*args, **kwargs):\n        force = kwargs.pop(\"force\", False)\n        if is_master or force:\n            builtin_print(*args, **kwargs)\n\n    __builtin__.print = print\n\n\ndef is_dist_avail_and_initialized():\n    if not dist.is_available():\n        return False\n    if not dist.is_initialized():\n        return False\n    return True\n\n\ndef get_world_size():\n    if not is_dist_avail_and_initialized():\n        return 1\n    return dist.get_world_size()\n\n\ndef get_rank():\n    if not is_dist_avail_and_initialized():\n        return 0\n    return dist.get_rank()\n\n\ndef is_main_process():\n    return get_rank() == 0\n\n\ndef init_distributed_mode(args):\n    if \"RANK\" in os.environ and \"WORLD_SIZE\" in os.environ:\n        args.rank = int(os.environ[\"RANK\"])\n        args.world_size = int(os.environ[\"WORLD_SIZE\"])\n        args.gpu = int(os.environ[\"LOCAL_RANK\"])\n    elif \"SLURM_PROCID\" in os.environ:\n        args.rank = int(os.environ[\"SLURM_PROCID\"])\n        args.gpu = args.rank % torch.cuda.device_count()\n    else:\n        print(\"Not using distributed mode\")\n        args.distributed = False\n        return\n\n    args.distributed = True\n\n    torch.cuda.set_device(args.gpu)\n    args.dist_backend = \"nccl\"\n    print(\n        \"| distributed init (rank {}, world {}): {}\".format(\n            args.rank, args.world_size, args.dist_url\n        ),\n        flush=True,\n    )\n    torch.distributed.init_process_group(\n        backend=args.dist_backend,\n        init_method=args.dist_url,\n        world_size=args.world_size,\n        rank=args.rank,\n        timeout=datetime.timedelta(\n            days=365\n        ),  # allow auto-downloading and de-compressing\n    )\n    torch.distributed.barrier()\n    setup_for_distributed(args.rank == 0)\n\n\ndef get_dist_info():\n    if torch.__version__ < \"1.0\":\n        initialized = dist._initialized\n    else:\n        initialized = dist.is_initialized()\n    if initialized:\n        rank = dist.get_rank()\n        world_size = dist.get_world_size()\n    else:  # non-distributed training\n        rank = 0\n        world_size = 1\n    return rank, world_size\n\n\ndef main_process(func):\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        rank, _ = get_dist_info()\n        if rank == 0:\n            return func(*args, **kwargs)\n\n    return wrapper\n\n\ndef download_cached_file(url, check_hash=True, progress=False):\n    \"\"\"\n    Download a file from a URL and cache it locally. If the file already exists, it is not downloaded again.\n    If distributed, only the main process downloads the file, and the other processes wait for the file to be downloaded.\n    \"\"\"\n\n    def get_cached_file_path():\n        # a hack to sync the file path across processes\n        parts = torch.hub.urlparse(url)\n        filename = os.path.basename(parts.path)\n        cached_file = os.path.join(timm_hub.get_cache_dir(), filename)\n\n        return cached_file\n\n    if is_main_process():\n        timm_hub.download_cached_file(url, check_hash, progress)\n\n    if is_dist_avail_and_initialized():\n        dist.barrier()\n\n    return get_cached_file_path()\n"}
{"type": "source_file", "path": "HG_grounding/models/clip_models/__init__.py", "content": "\"\"\"\n Copyright (c) 2022, salesforce.com, inc.\n All rights reserved.\n SPDX-License-Identifier: BSD-3-Clause\n For full license text, see the LICENSE file in the repo root or https://opensource.org/licenses/BSD-3-Clause\n\n Based on https://github.com/mlfoundations/open_clip\n\"\"\"\n\n\"\"\" OpenAI pretrained model functions\nAdapted from https://github.com/mlfoundations/open_clip and https://github.com/openai/CLIP.\n\nOriginally MIT License, Copyright (c) 2021 OpenAI.\n\"\"\"\n"}
{"type": "source_file", "path": "HG_grounding/dataset/caption_datasets_hgt.py", "content": "\"\"\"\n Copyright (c) 2022, salesforce.com, inc.\n All rights reserved.\n SPDX-License-Identifier: BSD-3-Clause\n For full license text, see the LICENSE file in the repo root or https://opensource.org/licenses/BSD-3-Clause\n\"\"\"\n\nimport os\nfrom collections import OrderedDict\n\nfrom dataset.base_dataset_graph import BaseDataset\nfrom PIL import Image\nimport torch\nfrom torch.utils.data.dataloader import default_collate\nimport math\n\n\nclass __DisplMixin:\n    def displ_item(self, index):\n        sample, ann = self.__getitem__(index), self.annotation[index]\n\n        return OrderedDict(\n            {\n                \"file\": ann[\"graph\"],\n                \"caption\": ann[\"caption\"],\n                \"graph\": sample[\"graph\"],\n            }\n        )\n\nclass HeteCaptionDataset(BaseDataset, __DisplMixin):\n    def __init__(self, graph_processor, text_processor, datasets_root, ann_paths):\n        \"\"\"\n        vis_root (string): Root directory of images (e.g. coco/images/)\n        ann_root (string): directory to store the annotation file\n        \"\"\"\n        super().__init__(graph_processor, text_processor, datasets_root, ann_paths)\n\n        self.graph_ids = {}\n        n = 0\n        for ann in self.annotation:\n            g_id = ann[\"graph_id\"]\n            if g_id not in self.graph_ids.keys():\n                self.graph_ids[g_id] = n\n                n += 1\n        # handle graph \n        self.node_type_feas_dict_dblp = torch.load('./models/meta_hgt/meta_dict/dblp/node_type.pt')\n        for k in self.node_type_feas_dict_dblp.keys(): \n            self.node_type_feas_dict_dblp[k] = torch.Tensor(self.node_type_feas_dict_dblp[k])\n        self.edge_type_feas_dict_dblp = torch.load('./models/meta_hgt/meta_dict/dblp/edge_type.pt')\n        for k in self.edge_type_feas_dict_dblp.keys(): \n            self.edge_type_feas_dict_dblp[k] = torch.Tensor(self.edge_type_feas_dict_dblp[k])\n\n        self.node_type_feas_dict_acm = torch.load('./models/meta_hgt/meta_dict/acm/node_type.pt')\n        for k in self.node_type_feas_dict_acm.keys(): \n            self.node_type_feas_dict_acm[k] = torch.Tensor(self.node_type_feas_dict_acm[k])\n        self.edge_type_feas_dict_acm = torch.load('./models/meta_hgt/meta_dict/acm/edge_type.pt')\n        for k in self.edge_type_feas_dict_acm.keys(): \n            self.edge_type_feas_dict_acm[k] = torch.Tensor(self.edge_type_feas_dict_acm[k])\n\n        self.node_type_feas_dict_imdb = torch.load('./models/meta_hgt/meta_dict/imdb/node_type.pt')\n        for k in self.node_type_feas_dict_imdb.keys(): \n            self.node_type_feas_dict_imdb[k] = torch.Tensor(self.node_type_feas_dict_imdb[k])\n        self.edge_type_feas_dict_imdb = torch.load('./models/meta_hgt/meta_dict/imdb/edge_type.pt')\n        for k in self.edge_type_feas_dict_imdb.keys(): \n            self.edge_type_feas_dict_imdb[k] = torch.Tensor(self.edge_type_feas_dict_imdb[k])\n        \n\n    def __getitem__(self, index):\n\n        # TODO this assumes image input, not general enough\n        \n        ann = self.annotation[index]\n\n        dsname = ann['graph_id'].split('_')[0]\n\n        # print(self.datasets_root, dsname, ann[\"graph\"])\n\n        graph_path = os.path.join(ann[\"graph\"]['graph'])\n        graph_dict = torch.load(graph_path)\n        des_dict = ann['des_dict']\n        \n        if 'subject' in graph_dict.x_dict.keys(): \n            edge_type_feas_dict = self.edge_type_feas_dict_acm\n            node_type_feas_dict = self.node_type_feas_dict_acm\n            des_dict.pop('author') if 'author' in des_dict else None\n        elif 'movie' in graph_dict.x_dict.keys(): \n            edge_type_feas_dict = self.edge_type_feas_dict_imdb\n            node_type_feas_dict = self.node_type_feas_dict_imdb\n        elif 'paper' in graph_dict.x_dict.keys(): \n            edge_type_feas_dict = self.edge_type_feas_dict_dblp\n            node_type_feas_dict = self.node_type_feas_dict_dblp\n            new_conf_reps = torch.ones(graph_dict['conference'].num_nodes, 768)\n            graph_dict['conference'].x = new_conf_reps\n            des_dict.pop('term') if 'term' in des_dict else None\n            des_dict.pop('paper') if 'paper' in des_dict else None\n        else: \n            raise NotImplementedError\n\n        graph_id = ann['graph_id']\n\n        # print(dsname)\n        # print(ann[\"description\"])\n        \n        # des_dict.pop('paper') if 'paper' in des_dict else None\n\n        des_order = des_dict.keys()\n        \n        # caption = self.text_processor(ann[\"description\"])\n        caption = []\n        for k in des_order: \n            des_list = des_dict[k]\n            # replaced_des_list = ['nan' if type(x) != str else x for x in des_list]\n            caption_tmp = [self.text_processor(des) for des in des_list]\n            \n            caption.extend(caption_tmp)\n\n        return {\n            \"graph\": graph_dict,\n            \"text_input\": caption,\n            \"des_order\": des_order, \n            \"graph_id\": graph_id, \n            \"edge_type_feas_dict\": edge_type_feas_dict, \n            \"node_type_feas_dict\": node_type_feas_dict\n            # \"graph\": self.img_ids[ann[\"image_id\"]],\n        }\n    def collater(self, samples):\n        # print(type(samples))\n        graph_dict = [sample['graph'] for sample in samples]\n        \n        caption = []\n        for sample in samples: \n            caption.extend(sample['text_input'])\n        des_order = [sample['des_order'] for sample in samples]\n        graph_ids = [sample['graph_id'] for sample in samples]\n        node_type_feas_dict = [sample['node_type_feas_dict'] for sample in samples]\n        edge_type_feas_dict = [sample['edge_type_feas_dict'] for sample in samples]\n        \n        return {\n            \"graph\": graph_dict,\n            \"text_input\": caption,\n            \"des_order\": des_order, \n            \"graph_id\": graph_ids, \n            # \"edge_type_feas_dict\": samples[0]['edge_type_feas_dict'],\n            # \"node_type_feas_dict\": samples[0]['node_type_feas_dict']\n            \"edge_type_feas_dict\": edge_type_feas_dict,\n            \"node_type_feas_dict\": node_type_feas_dict\n        }\n\n\n\nclass CaptionEvalDataset(BaseDataset, __DisplMixin):\n    def __init__(self, graph_processor, text_processor, graph_root, ann_paths):\n        \"\"\"\n        vis_root (string): Root directory of images (e.g. coco/images/)\n        ann_root (string): directory to store the annotation file\n        split (string): val or test\n        \"\"\"\n        super().__init__(graph_processor, text_processor, graph_root, ann_paths)\n\n    def __getitem__(self, index):\n\n        ann = self.annotation[index]\n\n        graph_path = os.path.join(self.graph_root, ann[\"graph\"])\n        graph = torch.load(graph_path)\n        graph = self.graph_processor(graph)\n\n        # image = self.vis_processor(image)\n\n        return {\n            \"graph\": graph,\n            \"graph_id\": ann[\"graph_id\"],\n            \"instance_id\": ann[\"instance_id\"],\n        }\n    def collater(self, samples):\n        samples['graph'] = self.graph_collator(samples['graph'])\n        return default_collate(samples)\n"}
{"type": "source_file", "path": "higpt/__init__.py", "content": "__version__ = \"0.2.11\"\n"}
{"type": "source_file", "path": "HG_grounding/models/clip_models/transform.py", "content": "\"\"\"\n Copyright (c) 2022, salesforce.com, inc.\n All rights reserved.\n SPDX-License-Identifier: BSD-3-Clause\n For full license text, see the LICENSE file in the repo root or https://opensource.org/licenses/BSD-3-Clause\n\n Based on https://github.com/mlfoundations/open_clip\n\"\"\"\n\nfrom typing import Optional, Sequence, Tuple\n\nimport torch\nimport torch.nn as nn\nimport torchvision.transforms.functional as F\n\n\nfrom torchvision.transforms import (\n    Normalize,\n    Compose,\n    RandomResizedCrop,\n    InterpolationMode,\n    ToTensor,\n    Resize,\n    CenterCrop,\n)\n\n\nclass ResizeMaxSize(nn.Module):\n    def __init__(\n        self, max_size, interpolation=InterpolationMode.BICUBIC, fn=\"max\", fill=0\n    ):\n        super().__init__()\n        if not isinstance(max_size, int):\n            raise TypeError(f\"Size should be int. Got {type(max_size)}\")\n        self.max_size = max_size\n        self.interpolation = interpolation\n        self.fn = min if fn == \"min\" else min\n        self.fill = fill\n\n    def forward(self, img):\n        if isinstance(img, torch.Tensor):\n            height, width = img.shape[:2]\n        else:\n            width, height = img.size\n        scale = self.max_size / float(max(height, width))\n        if scale != 1.0:\n            new_size = tuple(round(dim * scale) for dim in (height, width))\n            img = F.resize(img, new_size, self.interpolation)\n            pad_h = self.max_size - new_size[0]\n            pad_w = self.max_size - new_size[1]\n            img = F.pad(\n                img,\n                padding=[\n                    pad_w // 2,\n                    pad_h // 2,\n                    pad_w - pad_w // 2,\n                    pad_h - pad_h // 2,\n                ],\n                fill=self.fill,\n            )\n        return img\n\n\ndef _convert_to_rgb(image):\n    return image.convert(\"RGB\")\n\n\ndef image_transform(\n    image_size: int,\n    is_train: bool,\n    mean: Optional[Tuple[float, ...]] = None,\n    std: Optional[Tuple[float, ...]] = None,\n    resize_longest_max: bool = False,\n    fill_color: int = 0,\n):\n    mean = mean or (0.48145466, 0.4578275, 0.40821073)  # OpenAI dataset mean\n    std = std or (0.26862954, 0.26130258, 0.27577711)  # OpenAI dataset std\n    if isinstance(image_size, (list, tuple)) and image_size[0] == image_size[1]:\n        # for square size, pass size as int so that Resize() uses aspect preserving shortest edge\n        image_size = image_size[0]\n\n    normalize = Normalize(mean=mean, std=std)\n    if is_train:\n        return Compose(\n            [\n                RandomResizedCrop(\n                    image_size,\n                    scale=(0.9, 1.0),\n                    interpolation=InterpolationMode.BICUBIC,\n                ),\n                _convert_to_rgb,\n                ToTensor(),\n                normalize,\n            ]\n        )\n    else:\n        if resize_longest_max:\n            transforms = [ResizeMaxSize(image_size, fill=fill_color)]\n        else:\n            transforms = [\n                Resize(image_size, interpolation=InterpolationMode.BICUBIC),\n                CenterCrop(image_size),\n            ]\n        transforms.extend(\n            [\n                _convert_to_rgb,\n                ToTensor(),\n                normalize,\n            ]\n        )\n        return Compose(transforms)\n"}
{"type": "source_file", "path": "HG_grounding/models/meta_hgt/meta_linear.py", "content": "import copy\nimport math\nfrom typing import Any, Dict, Optional, Union, List\n\nimport torch\nimport torch.nn.functional as F\nfrom torch import Tensor\nfrom torch.nn.parameter import Parameter\n\nimport torch_geometric.backend\nimport torch_geometric.typing\nfrom torch_geometric.nn import inits\nfrom torch_geometric.typing import pyg_lib\nfrom torch_geometric.utils import index_sort, scatter\nfrom torch_geometric.utils.sparse import index2ptr\nimport torch.nn as nn\n\ninit = nn.init.xavier_uniform_\n\nclass ParameterGenerator(nn.Module):\n    def __init__(self, memory_size, in_channels, out_channels, hidden_channels, dynamic):\n        super(ParameterGenerator, self).__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.hidden_channels = hidden_channels\n        self.dynamic = dynamic\n\n        if self.dynamic:\n            print('Using DYNAMIC')\n            self.weight_generator = nn.Sequential(*[\n                nn.Linear(memory_size, hidden_channels),\n                nn.ReLU(),\n                nn.Linear(hidden_channels, hidden_channels),\n                nn.ReLU(),\n                nn.Linear(hidden_channels, in_channels * out_channels)\n            ])\n            self.bias_generator = nn.Sequential(*[\n                nn.Linear(memory_size, hidden_channels),\n                nn.ReLU(),\n                nn.Linear(hidden_channels, hidden_channels),\n                nn.ReLU(),\n                nn.Linear(hidden_channels, out_channels)\n            ])\n        else:\n            print('Using FC')\n            self.weights = nn.Parameter(init(torch.empty(in_channels, out_channels)), requires_grad=True)\n            self.biases = nn.Parameter(init(torch.empty(out_channels)), requires_grad=True)\n\n    def forward(self, memory=None):\n        if self.dynamic:\n            weights = self.weight_generator(memory).view(self.in_channels, self.out_channels)\n            biases = self.bias_generator(memory).view(self.out_channels)\n        else:\n            weights = self.weights\n            biases = self.biases\n        return weights, biases\n\nclass LinearCustom(nn.Module):\n\n    def __init__(self, in_channels, out_channels):\n        super(LinearCustom, self).__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n\n    def forward(self, inputs, parameters: List[Tensor]):\n        weights, biases = parameters[0], parameters[1]\n        assert weights.shape == torch.Size([self.in_channels, self.out_channels]) and biases.shape == torch.Size([self.out_channels])\n        return torch.matmul(inputs, weights) + biases\n\nclass MetaHeteroLinear(torch.nn.Module):\n    def __init__(\n        self,\n        memory_size: int,\n        in_channels: int,\n        out_channels: int,\n        dynamic: bool = True,\n        **kwargs,\n    ):\n        super().__init__()\n        self.memory_size = memory_size\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.kwargs = kwargs\n\n        self.meta_lin = LinearCustom(self.in_channels, self.out_channels)\n        self.lin_gen = ParameterGenerator(self.memory_size, self.in_channels, self.out_channels, self.memory_size //2, dynamic)\n\n    def forward(self, x: Tensor, type_vec: Tensor, edge_feas_dict: Dict) -> Tensor:\n        r\"\"\"\n        Args:\n            x (torch.Tensor): The input features.\n            type_vec (torch.Tensor): A vector that maps each entry to a type.\n        \"\"\"\n        out = x.new_empty(x.size(0), self.out_channels)\n        for i in edge_feas_dict.keys():\n            mask = type_vec == i\n            if mask.numel() == 0:\n                continue\n            params = self.lin_gen(edge_feas_dict[i])\n            subset_out = self.meta_lin(x[mask], params)\n            # The data type may have changed with mixed precision:\n            out[mask] = subset_out.to(out.dtype)\n\n        return out\n\n\n    def __repr__(self) -> str:\n        return (f'{self.__class__.__name__}({self.in_channels}, '\n                f'{self.out_channels}, num_types={self.num_types}, '\n                f'bias={self.kwargs.get(\"bias\", True)})')\n\n\nclass MetaHeteroDictLinear(torch.nn.Module):\n    def __init__(\n        self,\n        memory_size: int,\n        in_channels: int,\n        out_channels: int,\n        dynamic: bool = True,\n        **kwargs,\n    ):\n        super().__init__()\n        self.memory_size = memory_size\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.kwargs = kwargs\n\n        self.lin_gen = ParameterGenerator(self.memory_size, self.in_channels, self.out_channels, self.memory_size //2, dynamic)\n        \n        self.meta_lin = LinearCustom(self.in_channels, self.out_channels)\n\n    def forward(\n        self,\n        x_dict: Dict[str, Tensor],\n        node_feas_dict: Dict, \n    ) -> Dict[str, Tensor]:\n        r\"\"\"\n        Args:\n            x_dict (Dict[Any, torch.Tensor]): A dictionary holding input\n                features for each individual type.\n        \"\"\"\n        out_dict = {}\n\n        for key, node_feas in node_feas_dict.items():\n            if key in x_dict:\n                params = self.lin_gen(node_feas)\n                out_dict[key] = self.meta_lin(x_dict[key], params)\n\n        return out_dict\n\n    def __repr__(self) -> str:\n        return (f'{self.__class__.__name__}({self.in_channels}, '\n                f'{self.out_channels}, bias={self.kwargs.get(\"bias\", True)})')\n\n\n"}
{"type": "source_file", "path": "HG_grounding/models/clip_models/utils.py", "content": "\"\"\"\n Copyright (c) 2022, salesforce.com, inc.\n All rights reserved.\n SPDX-License-Identifier: BSD-3-Clause\n For full license text, see the LICENSE file in the repo root or https://opensource.org/licenses/BSD-3-Clause\n\n Based on https://github.com/mlfoundations/open_clip\n\"\"\"\n\nfrom torch import nn as nn\nfrom torchvision.ops.misc import FrozenBatchNorm2d\n\n\ndef freeze_batch_norm_2d(module, module_match={}, name=\"\"):\n    \"\"\"\n    Converts all `BatchNorm2d` and `SyncBatchNorm` layers of provided module into `FrozenBatchNorm2d`. If `module` is\n    itself an instance of either `BatchNorm2d` or `SyncBatchNorm`, it is converted into `FrozenBatchNorm2d` and\n    returned. Otherwise, the module is walked recursively and submodules are converted in place.\n    Args:\n        module (torch.nn.Module): Any PyTorch module.\n        module_match (dict): Dictionary of full module names to freeze (all if empty)\n        name (str): Full module name (prefix)\n    Returns:\n        torch.nn.Module: Resulting module\n    Inspired by https://github.com/pytorch/pytorch/blob/a5895f85be0f10212791145bfedc0261d364f103/torch/nn/modules/batchnorm.py#L762\n    \"\"\"\n    res = module\n    is_match = True\n    if module_match:\n        is_match = name in module_match\n    if is_match and isinstance(\n        module, (nn.modules.batchnorm.BatchNorm2d, nn.modules.batchnorm.SyncBatchNorm)\n    ):\n        res = FrozenBatchNorm2d(module.num_features)\n        res.num_features = module.num_features\n        res.affine = module.affine\n        if module.affine:\n            res.weight.data = module.weight.data.clone().detach()\n            res.bias.data = module.bias.data.clone().detach()\n        res.running_mean.data = module.running_mean.data\n        res.running_var.data = module.running_var.data\n        res.eps = module.eps\n    else:\n        for child_name, child in module.named_children():\n            full_child_name = \".\".join([name, child_name]) if name else child_name\n            new_child = freeze_batch_norm_2d(child, module_match, full_child_name)\n            if new_child is not child:\n                res.add_module(child_name, new_child)\n    return res\n"}
{"type": "source_file", "path": "HG_grounding/lit_models/lit_hgt.py", "content": "from typing import Any\nfrom lightning import LightningModule, LightningDataModule\nfrom lightning.pytorch.utilities.types import STEP_OUTPUT, OptimizerLRScheduler\n# from models.clip_models.model import CLIP\nfrom models.clip_models.homo_clip import CLIP\nimport torch\nimport torch.nn.functional as F\nimport re\nimport numpy as np\nfrom omegaconf import OmegaConf\nfrom transformers.utils.import_utils import is_cython_available\nfrom pl_bolts.optimizers.lr_scheduler import LinearWarmupCosineAnnealingLR\nimport os.path as osp\n\nclass HeteCLIP(LightningModule):\n    def __init__(self, training_args, model_args, data_args) -> None:\n        super().__init__()\n        self.training_args = training_args\n        self.model_args = model_args\n        self.data_args = data_args\n        if model_args.gnn_type == 'meta':\n            graph_cfg = dict(\n                in_channels=model_args.graph_in_channels, \n                out_channels=model_args.graph_out_channels, \n                heads=model_args.graph_heads, \n                dynamic= model_args.graph_dynamic, \n            )\n        elif model_args.gnn_type == 'homo':\n            graph_cfg = dict(\n                att_d_model = model_args.graph_in_channels, \n                head = model_args.graph_heads, \n                att_norm = True\n            )\n        text_cfg = dict(\n            context_length = model_args.context_length, \n            vocab_size = model_args.vocab_size, \n            width = model_args.text_width, \n            heads = model_args.text_heads, \n            layers = model_args.text_layers, \n        )\n\n        self.model = CLIP(\n        embed_dim = model_args.embed_dim,\n        graph_cfg = graph_cfg,\n        text_cfg = text_cfg,\n        quick_gelu = model_args.quick_gelu,\n        gnn_type = model_args.gnn_type\n        )\n        self.training_step_outputs = []\n\n    def configure_optimizers(self) -> OptimizerLRScheduler:\n        p_wd, p_non_wd = [], []\n        for n, p in self.model.named_parameters():\n            if not p.requires_grad:\n                continue  # frozen weights\n            else:\n                p_wd.append(p)        \n        optim_params = [\n            {\"params\": p_wd, \"weight_decay\": 0.05, \"lr_scale\": [1e-7, 1e-6]},\n        ]                \n        \n        optim = torch.optim.AdamW(optim_params, lr=self.training_args.learning_rate, betas=(0.9, 0.999))\n        lr_sched = LinearWarmupCosineAnnealingLR(optimizer = optim, warmup_epochs = 1, max_epochs = self.training_args.max_epochs, warmup_start_lr=1e-6, eta_min=0.0, last_epoch=- 1)\n        return [optim], [lr_sched]\n\n    def training_step(self, batch_sample) -> STEP_OUTPUT:\n        clip_output = self.forward(batch_sample)\n        self.log('loss', clip_output.loss.item(), on_step=True, on_epoch=True, prog_bar=True, batch_size=self.data_args.micro_batch_size)\n        self.training_step_outputs.append(clip_output.loss.item())\n        return clip_output.loss\n    def on_train_epoch_end(self):\n        epoch_average = np.array(self.training_step_outputs).mean()\n        self.log(\"training_epoch_average\", epoch_average, sync_dist=True)\n        self.training_step_outputs.clear()  # free memory\n\n    def forward(self, batch_sample) -> Any:\n        return self.model(batch_sample)\n    def on_train_batch_end(self, outputs: STEP_OUTPUT, batch: Any, batch_idx: int) -> None:\n        # print('*'*20, 'grad none', '*'*20)\n        # for n, p in self.named_parameters():\n        #     if p.grad is None:\n        #         print(n)\n        pass\n"}
{"type": "source_file", "path": "HG_grounding/models/meta_hgt/meta_hgtconv.py", "content": "import math\nfrom typing import Dict, List, Optional, Tuple, Union\n\nimport torch\nfrom torch import Tensor\nfrom torch.nn import Parameter\n\nfrom torch_geometric.nn.conv import MessagePassing\n# from torch_geometric.nn.dense import HeteroDictLinear, HeteroLinear\nfrom models.meta_hgt.meta_linear import MetaHeteroDictLinear, MetaHeteroLinear\nfrom torch_geometric.nn.inits import ones\nfrom torch_geometric.nn.parameter_dict import ParameterDict\nfrom torch_geometric.typing import Adj, EdgeType, Metadata, NodeType\nfrom torch_geometric.utils import softmax\nfrom torch_geometric.utils.hetero import construct_bipartite_edge_index\nfrom models.meta_hgt.hgt_constants import NODE_TYPE_DICT, EDGE_TYPE_DICT\nfrom dataclasses import dataclass\nimport torch.nn as nn\nfrom models.clip_models.tokenizer import tokenize\n\n@dataclass\nclass MetaHGTConvCfg:\n    in_channels: int \n    out_channels: int\n    heads: int\n    dynamic: bool = True\n\n\nclass MetaHGTConv(MessagePassing):\n    \n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        heads: int = 1,\n        dynamic: bool = False,\n        text_transformer = None, \n        text_cfg = None, \n        layernorm = None, \n        **kwargs,\n    ):\n        super().__init__(aggr='add', node_dim=0, **kwargs)\n\n        if out_channels % heads != 0:\n            raise ValueError(f\"'out_channels' (got {out_channels}) must be \"\n                             f\"divisible by the number of heads (got {heads})\")\n\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.heads = heads\n\n        self.kqv_lin = MetaHeteroDictLinear(text_cfg.width, self.in_channels,\n                                        self.out_channels * 3, dynamic)\n\n        self.out_lin = MetaHeteroDictLinear(text_cfg.width, self.out_channels, self.out_channels, dynamic)\n\n        dim = out_channels // heads\n\n        self.k_rel = MetaHeteroLinear(text_cfg.width, dim, dim, dynamic)\n        self.v_rel = MetaHeteroLinear(text_cfg.width, dim, dim, dynamic)\n\n        self.skipTrans = nn.Linear(text_cfg.width, 1) # node aware, skip: 1\n\n        self.p_relTrans = nn.Linear(text_cfg.width, heads) # edge aware, p_rel: 1, heads\n\n        self.tokenizer = tokenize\n\n        act_layer = nn.GELU\n\n        self.transformer = text_transformer(\n            width=text_cfg.width,\n            layers=text_cfg.layers,\n            heads=text_cfg.heads,\n            act_layer=act_layer,\n        )\n\n        self.context_length = text_cfg.context_length\n        self.vocab_size = text_cfg.vocab_size\n        self.token_embedding = nn.Embedding(text_cfg.vocab_size, text_cfg.width)\n        self.positional_embedding = nn.Parameter(\n            torch.empty(self.context_length, text_cfg.width)\n        )\n        self.ln_final = layernorm(text_cfg.width)\n\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        super().reset_parameters()\n\n        nn.init.normal_(self.token_embedding.weight, std=0.02)\n        nn.init.normal_(self.positional_embedding, std=0.01)\n\n        proj_std = (self.transformer.width**-0.5) * (\n            (2 * self.transformer.layers) ** -0.5\n        )\n        attn_std = self.transformer.width**-0.5\n        fc_std = (2 * self.transformer.width) ** -0.5\n        for block in self.transformer.resblocks:\n            nn.init.normal_(block.attn.in_proj_weight, std=attn_std)\n            nn.init.normal_(block.attn.out_proj.weight, std=proj_std)\n            nn.init.normal_(block.mlp.c_fc.weight, std=fc_std)\n            nn.init.normal_(block.mlp.c_proj.weight, std=proj_std)\n\n    def _cat(self, x_dict: Dict[str, Tensor]) -> Tuple[Tensor, Dict[str, int]]:\n        \"\"\"Concatenates a dictionary of features.\"\"\"\n        cumsum = 0\n        outs: List[Tensor] = []\n        offset: Dict[str, int] = {}\n        for key, x in x_dict.items():\n            outs.append(x)\n            offset[key] = cumsum\n            cumsum += x.size(0)\n        return torch.cat(outs, dim=0), offset\n\n    def _construct_src_node_feat(\n        self, k_dict: Dict[str, Tensor], v_dict: Dict[str, Tensor],\n        edge_index_dict: Dict[EdgeType, Adj], \n        edge_type_feas_dict: Dict[EdgeType, Tensor], \n    ) -> Tuple[Tensor, Tensor, Dict[EdgeType, int]]:\n        \"\"\"Constructs the source node representations.\"\"\"\n        cumsum = 0\n        num_edge_types = len(edge_index_dict.keys())\n        H, D = self.heads, self.out_channels // self.heads\n\n        # Flatten into a single tensor with shape [num_edge_types * heads, D]:\n        ks: List[Tensor] = []\n        vs: List[Tensor] = []\n        type_list: List[Tensor] = []\n        offset: Dict[EdgeType] = {}\n\n        edge_types_map = {\n            edge_type: i\n            for i, edge_type in enumerate(edge_index_dict.keys())\n        }\n        for edge_type in edge_index_dict.keys():\n            src = edge_type[0]\n            N = k_dict[src].size(0)\n            offset[edge_type] = cumsum\n            cumsum += N\n\n            # construct type_vec for curr edge_type with shape [H, D]\n            edge_type_offset = edge_types_map[edge_type]\n            type_vec = torch.arange(H, dtype=torch.long).view(-1, 1).repeat(\n                1, N) * num_edge_types + edge_type_offset\n\n            type_list.append(type_vec)\n            ks.append(k_dict[src])\n            vs.append(v_dict[src])\n\n        ks = torch.cat(ks, dim=0).transpose(0, 1).reshape(-1, D)\n        vs = torch.cat(vs, dim=0).transpose(0, 1).reshape(-1, D)\n        type_vec = torch.cat(type_list, dim=1).flatten()\n\n        edge_feas_dict = {edge_types_map[k]: v for k, v in edge_type_feas_dict.items()}\n\n        k = self.k_rel(ks, type_vec, edge_feas_dict).view(H, -1, D).transpose(0, 1)\n        v = self.v_rel(vs, type_vec, edge_feas_dict).view(H, -1, D).transpose(0, 1)\n\n        return k, v, offset\n    def encode_text(self, text): \n        x = self.token_embedding(text)  # [batch_size, n_ctx, d_model]\n\n        x = x + self.positional_embedding\n        x = x.permute(1, 0, 2)  # NLD -> LND\n        x = self.transformer(x, attn_mask=None)\n        x = x.permute(1, 0, 2)  # LND -> NLD\n        x = self.ln_final(x)\n\n        # x.shape = [batch_size, n_ctx, transformer.width]\n        # take features from the eot embedding (eot_token is the highest number in each sequence)\n        x = x[torch.arange(x.shape[0]), text.argmax(dim=-1)] # 1, width\n\n        return x\n\n    def _construct_p_rel(self, edge_type_feas_dict: Dict[EdgeType, Tensor]):\n        p_rel = {k: self.p_relTrans(v).unsqueeze(0) for k, v in edge_type_feas_dict.items()}\n        return p_rel\n    def _construct_skip(self, node_type_feas_dict: Dict[EdgeType, Tensor]):\n        skip = {k: self.skipTrans(v) for k, v in node_type_feas_dict.items()}\n        return skip\n\n    def forward(\n        self,\n        x_dict: Dict[NodeType, Tensor],\n        edge_index_dict: Dict[EdgeType, Adj]  # Support both.\n    ) -> Dict[NodeType, Optional[Tensor]]:\n        F = self.out_channels\n        H = self.heads\n        D = F // H\n\n        node_type_feas_dict = {k: self.encode_text(self.tokenizer(NODE_TYPE_DICT[k], self.context_length).to(self.token_embedding.weight.device)).squeeze(0) for k in x_dict.keys()}\n\n        edge_type_feas_dict = {k: self.encode_text(self.tokenizer(EDGE_TYPE_DICT[k], self.context_length).to(self.token_embedding.weight.device)).squeeze(0) for k in edge_index_dict.keys()}\n\n        k_dict, q_dict, v_dict, out_dict = {}, {}, {}, {}\n\n        # Compute K, Q, V over node types:\n        kqv_dict = self.kqv_lin(x_dict, node_type_feas_dict)\n        for key, val in kqv_dict.items():\n            k, q, v = torch.tensor_split(val, 3, dim=1)\n            k_dict[key] = k.view(-1, H, D)\n            q_dict[key] = q.view(-1, H, D)\n            v_dict[key] = v.view(-1, H, D)\n\n        q, dst_offset = self._cat(q_dict)\n        k, v, src_offset = self._construct_src_node_feat(\n            k_dict, v_dict, edge_index_dict, edge_type_feas_dict)\n        p_rel = self._construct_p_rel(edge_type_feas_dict)\n        edge_index, edge_attr = construct_bipartite_edge_index(\n            edge_index_dict, src_offset, dst_offset, edge_attr_dict=p_rel)\n\n        out = self.propagate(edge_index, k=k, q=q, v=v, edge_attr=edge_attr,\n                             size=None)\n\n        dst_node_types = set([key[-1] for key in edge_index_dict.keys()])\n\n        # Reconstruct output node embeddings dict:\n        for node_type, start_offset in dst_offset.items():\n            end_offset = start_offset + q_dict[node_type].size(0)\n            if node_type in dst_node_types:\n                out_dict[node_type] = out[start_offset:end_offset]\n\n        # Transform output node embeddings:\n        a_dict = self.out_lin({\n            k:\n            torch.nn.functional.gelu(v) if v is not None else v\n            for k, v in out_dict.items()\n        }, node_type_feas_dict)\n\n        skip = self._construct_skip(node_type_feas_dict)\n        # Iterate over node types:\n        for node_type, out in out_dict.items():\n            out = a_dict[node_type]\n\n            if out.size(-1) == x_dict[node_type].size(-1):\n                alpha = skip[node_type].sigmoid()\n                out = alpha * out + (1 - alpha) * x_dict[node_type]\n            out_dict[node_type] = out\n\n        return out_dict\n\n    def message(self, k_j: Tensor, q_i: Tensor, v_j: Tensor, edge_attr: Tensor,\n                index: Tensor, ptr: Optional[Tensor],\n                size_i: Optional[int]) -> Tensor:\n        alpha = (q_i * k_j).sum(dim=-1) * edge_attr\n        alpha = alpha / math.sqrt(q_i.size(-1))\n        alpha = softmax(alpha, index, ptr, size_i)\n        out = v_j * alpha.view(-1, self.heads, 1)\n        return out.view(-1, self.out_channels)\n\n    def __repr__(self) -> str:\n        return (f'{self.__class__.__name__}(-1, {self.out_channels}, '\n                f'heads={self.heads})')\n"}
{"type": "source_file", "path": "HG_grounding/utils.py", "content": "\"\"\"\n Copyright (c) 2022, salesforce.com, inc.\n All rights reserved.\n SPDX-License-Identifier: BSD-3-Clause\n For full license text, see the LICENSE file in the repo root or https://opensource.org/licenses/BSD-3-Clause\n\"\"\"\n\nimport io\nimport json\nimport logging\nimport os\nimport pickle\nimport re\nimport shutil\nimport tarfile\nimport urllib\nimport urllib.error\nimport urllib.request\nfrom typing import Optional\nfrom urllib.parse import urlparse\n\nimport numpy as np\nimport pandas as pd\nimport yaml\nfrom iopath.common.download import download\nfrom iopath.common.file_io import file_lock, g_pathmgr\nfrom dist_utils import download_cached_file\nfrom torch.utils.model_zoo import tqdm\nfrom torchvision.datasets.utils import (\n    check_integrity,\n    download_file_from_google_drive,\n    extract_archive,\n)\n\n\ndef now():\n    from datetime import datetime\n\n    return datetime.now().strftime(\"%Y%m%d%H%M\")[:-1]\n\n\ndef is_url(url_or_filename):\n    parsed = urlparse(url_or_filename)\n    return parsed.scheme in (\"http\", \"https\")\n\n\ndef get_cache_path(rel_path):\n    return os.path.expanduser(os.path.join(registry.get_path(\"cache_root\"), rel_path))\n\n\ndef get_abs_path(rel_path):\n    return os.path.join(registry.get_path(\"library_root\"), rel_path)\n\n\ndef load_json(filename):\n    with open(filename, \"r\") as f:\n        return json.load(f)\n\n\n# The following are adapted from torchvision and vissl\n# torchvision: https://github.com/pytorch/vision\n# vissl: https://github.com/facebookresearch/vissl/blob/main/vissl/utils/download.py\n\n\ndef makedir(dir_path):\n    \"\"\"\n    Create the directory if it does not exist.\n    \"\"\"\n    is_success = False\n    try:\n        if not g_pathmgr.exists(dir_path):\n            g_pathmgr.mkdirs(dir_path)\n        is_success = True\n    except BaseException:\n        print(f\"Error creating directory: {dir_path}\")\n    return is_success\n\n\ndef get_redirected_url(url: str):\n    \"\"\"\n    Given a URL, returns the URL it redirects to or the\n    original URL in case of no indirection\n    \"\"\"\n    import requests\n\n    with requests.Session() as session:\n        with session.get(url, stream=True, allow_redirects=True) as response:\n            if response.history:\n                return response.url\n            else:\n                return url\n\n\ndef to_google_drive_download_url(view_url: str) -> str:\n    \"\"\"\n    Utility function to transform a view URL of google drive\n    to a download URL for google drive\n    Example input:\n        https://drive.google.com/file/d/137RyRjvTBkBiIfeYBNZBtViDHQ6_Ewsp/view\n    Example output:\n        https://drive.google.com/uc?export=download&id=137RyRjvTBkBiIfeYBNZBtViDHQ6_Ewsp\n    \"\"\"\n    splits = view_url.split(\"/\")\n    assert splits[-1] == \"view\"\n    file_id = splits[-2]\n    return f\"https://drive.google.com/uc?export=download&id={file_id}\"\n\n\ndef download_google_drive_url(url: str, output_path: str, output_file_name: str):\n    \"\"\"\n    Download a file from google drive\n    Downloading an URL from google drive requires confirmation when\n    the file of the size is too big (google drive notifies that\n    anti-viral checks cannot be performed on such files)\n    \"\"\"\n    import requests\n\n    with requests.Session() as session:\n\n        # First get the confirmation token and append it to the URL\n        with session.get(url, stream=True, allow_redirects=True) as response:\n            for k, v in response.cookies.items():\n                if k.startswith(\"download_warning\"):\n                    url = url + \"&confirm=\" + v\n\n        # Then download the content of the file\n        with session.get(url, stream=True, verify=True) as response:\n            makedir(output_path)\n            path = os.path.join(output_path, output_file_name)\n            total_size = int(response.headers.get(\"Content-length\", 0))\n            with open(path, \"wb\") as file:\n                from tqdm import tqdm\n\n                with tqdm(total=total_size) as progress_bar:\n                    for block in response.iter_content(\n                        chunk_size=io.DEFAULT_BUFFER_SIZE\n                    ):\n                        file.write(block)\n                        progress_bar.update(len(block))\n\n\ndef _get_google_drive_file_id(url: str) -> Optional[str]:\n    parts = urlparse(url)\n\n    if re.match(r\"(drive|docs)[.]google[.]com\", parts.netloc) is None:\n        return None\n\n    match = re.match(r\"/file/d/(?P<id>[^/]*)\", parts.path)\n    if match is None:\n        return None\n\n    return match.group(\"id\")\n\n\ndef _urlretrieve(url: str, filename: str, chunk_size: int = 1024) -> None:\n    with open(filename, \"wb\") as fh:\n        with urllib.request.urlopen(\n            urllib.request.Request(url, headers={\"User-Agent\": \"vissl\"})\n        ) as response:\n            with tqdm(total=response.length) as pbar:\n                for chunk in iter(lambda: response.read(chunk_size), \"\"):\n                    if not chunk:\n                        break\n                    pbar.update(chunk_size)\n                    fh.write(chunk)\n\n\ndef download_url(\n    url: str,\n    root: str,\n    filename: Optional[str] = None,\n    md5: Optional[str] = None,\n) -> None:\n    \"\"\"Download a file from a url and place it in root.\n    Args:\n        url (str): URL to download file from\n        root (str): Directory to place downloaded file in\n        filename (str, optional): Name to save the file under.\n                                  If None, use the basename of the URL.\n        md5 (str, optional): MD5 checksum of the download. If None, do not check\n    \"\"\"\n    root = os.path.expanduser(root)\n    if not filename:\n        filename = os.path.basename(url)\n    fpath = os.path.join(root, filename)\n\n    makedir(root)\n\n    # check if file is already present locally\n    if check_integrity(fpath, md5):\n        print(\"Using downloaded and verified file: \" + fpath)\n        return\n\n    # expand redirect chain if needed\n    url = get_redirected_url(url)\n\n    # check if file is located on Google Drive\n    file_id = _get_google_drive_file_id(url)\n    if file_id is not None:\n        return download_file_from_google_drive(file_id, root, filename, md5)\n\n    # download the file\n    try:\n        print(\"Downloading \" + url + \" to \" + fpath)\n        _urlretrieve(url, fpath)\n    except (urllib.error.URLError, IOError) as e:  # type: ignore[attr-defined]\n        if url[:5] == \"https\":\n            url = url.replace(\"https:\", \"http:\")\n            print(\n                \"Failed download. Trying https -> http instead.\"\n                \" Downloading \" + url + \" to \" + fpath\n            )\n            _urlretrieve(url, fpath)\n        else:\n            raise e\n\n    # check integrity of downloaded file\n    if not check_integrity(fpath, md5):\n        raise RuntimeError(\"File not found or corrupted.\")\n\n\ndef download_and_extract_archive(\n    url: str,\n    download_root: str,\n    extract_root: Optional[str] = None,\n    filename: Optional[str] = None,\n    md5: Optional[str] = None,\n    remove_finished: bool = False,\n) -> None:\n    download_root = os.path.expanduser(download_root)\n    if extract_root is None:\n        extract_root = download_root\n    if not filename:\n        filename = os.path.basename(url)\n\n    download_url(url, download_root, filename, md5)\n\n    archive = os.path.join(download_root, filename)\n    print(\"Extracting {} to {}\".format(archive, extract_root))\n    extract_archive(archive, extract_root, remove_finished)\n\n\ndef cache_url(url: str, cache_dir: str) -> str:\n    \"\"\"\n    This implementation downloads the remote resource and caches it locally.\n    The resource will only be downloaded if not previously requested.\n    \"\"\"\n    parsed_url = urlparse(url)\n    dirname = os.path.join(cache_dir, os.path.dirname(parsed_url.path.lstrip(\"/\")))\n    makedir(dirname)\n    filename = url.split(\"/\")[-1]\n    cached = os.path.join(dirname, filename)\n    with file_lock(cached):\n        if not os.path.isfile(cached):\n            logging.info(f\"Downloading {url} to {cached} ...\")\n            cached = download(url, dirname, filename=filename)\n    logging.info(f\"URL {url} cached in {cached}\")\n    return cached\n\n\n# TODO (prigoyal): convert this into RAII-style API\ndef create_file_symlink(file1, file2):\n    \"\"\"\n    Simply create the symlinks for a given file1 to file2.\n    Useful during model checkpointing to symlinks to the\n    latest successful checkpoint.\n    \"\"\"\n    try:\n        if g_pathmgr.exists(file2):\n            g_pathmgr.rm(file2)\n        g_pathmgr.symlink(file1, file2)\n    except Exception as e:\n        logging.info(f\"Could NOT create symlink. Error: {e}\")\n\n\ndef save_file(data, filename, append_to_json=True, verbose=True):\n    \"\"\"\n    Common i/o utility to handle saving data to various file formats.\n    Supported:\n        .pkl, .pickle, .npy, .json\n    Specifically for .json, users have the option to either append (default)\n    or rewrite by passing in Boolean value to append_to_json.\n    \"\"\"\n    if verbose:\n        logging.info(f\"Saving data to file: {filename}\")\n    file_ext = os.path.splitext(filename)[1]\n    if file_ext in [\".pkl\", \".pickle\"]:\n        with g_pathmgr.open(filename, \"wb\") as fopen:\n            pickle.dump(data, fopen, pickle.HIGHEST_PROTOCOL)\n    elif file_ext == \".npy\":\n        with g_pathmgr.open(filename, \"wb\") as fopen:\n            np.save(fopen, data)\n    elif file_ext == \".json\":\n        if append_to_json:\n            with g_pathmgr.open(filename, \"a\") as fopen:\n                fopen.write(json.dumps(data, sort_keys=True) + \"\\n\")\n                fopen.flush()\n        else:\n            with g_pathmgr.open(filename, \"w\") as fopen:\n                fopen.write(json.dumps(data, sort_keys=True) + \"\\n\")\n                fopen.flush()\n    elif file_ext == \".yaml\":\n        with g_pathmgr.open(filename, \"w\") as fopen:\n            dump = yaml.dump(data)\n            fopen.write(dump)\n            fopen.flush()\n    else:\n        raise Exception(f\"Saving {file_ext} is not supported yet\")\n\n    if verbose:\n        logging.info(f\"Saved data to file: {filename}\")\n\n\ndef load_file(filename, mmap_mode=None, verbose=True, allow_pickle=False):\n    \"\"\"\n    Common i/o utility to handle loading data from various file formats.\n    Supported:\n        .pkl, .pickle, .npy, .json\n    For the npy files, we support reading the files in mmap_mode.\n    If the mmap_mode of reading is not successful, we load data without the\n    mmap_mode.\n    \"\"\"\n    if verbose:\n        logging.info(f\"Loading data from file: {filename}\")\n\n    file_ext = os.path.splitext(filename)[1]\n    if file_ext == \".txt\":\n        with g_pathmgr.open(filename, \"r\") as fopen:\n            data = fopen.readlines()\n    elif file_ext in [\".pkl\", \".pickle\"]:\n        with g_pathmgr.open(filename, \"rb\") as fopen:\n            data = pickle.load(fopen, encoding=\"latin1\")\n    elif file_ext == \".npy\":\n        if mmap_mode:\n            try:\n                with g_pathmgr.open(filename, \"rb\") as fopen:\n                    data = np.load(\n                        fopen,\n                        allow_pickle=allow_pickle,\n                        encoding=\"latin1\",\n                        mmap_mode=mmap_mode,\n                    )\n            except ValueError as e:\n                logging.info(\n                    f\"Could not mmap {filename}: {e}. Trying without g_pathmgr\"\n                )\n                data = np.load(\n                    filename,\n                    allow_pickle=allow_pickle,\n                    encoding=\"latin1\",\n                    mmap_mode=mmap_mode,\n                )\n                logging.info(\"Successfully loaded without g_pathmgr\")\n            except Exception:\n                logging.info(\"Could not mmap without g_pathmgr. Trying without mmap\")\n                with g_pathmgr.open(filename, \"rb\") as fopen:\n                    data = np.load(fopen, allow_pickle=allow_pickle, encoding=\"latin1\")\n        else:\n            with g_pathmgr.open(filename, \"rb\") as fopen:\n                data = np.load(fopen, allow_pickle=allow_pickle, encoding=\"latin1\")\n    elif file_ext == \".json\":\n        with g_pathmgr.open(filename, \"r\") as fopen:\n            data = json.load(fopen)\n    elif file_ext == \".yaml\":\n        with g_pathmgr.open(filename, \"r\") as fopen:\n            data = yaml.load(fopen, Loader=yaml.FullLoader)\n    elif file_ext == \".csv\":\n        with g_pathmgr.open(filename, \"r\") as fopen:\n            data = pd.read_csv(fopen)\n    else:\n        raise Exception(f\"Reading from {file_ext} is not supported yet\")\n    return data\n\n\ndef abspath(resource_path: str):\n    \"\"\"\n    Make a path absolute, but take into account prefixes like\n    \"http://\" or \"manifold://\"\n    \"\"\"\n    regex = re.compile(r\"^\\w+://\")\n    if regex.match(resource_path) is None:\n        return os.path.abspath(resource_path)\n    else:\n        return resource_path\n\n\ndef makedir(dir_path):\n    \"\"\"\n    Create the directory if it does not exist.\n    \"\"\"\n    is_success = False\n    try:\n        if not g_pathmgr.exists(dir_path):\n            g_pathmgr.mkdirs(dir_path)\n        is_success = True\n    except BaseException:\n        logging.info(f\"Error creating directory: {dir_path}\")\n    return is_success\n\n\ndef is_url(input_url):\n    \"\"\"\n    Check if an input string is a url. look for http(s):// and ignoring the case\n    \"\"\"\n    is_url = re.match(r\"^(?:http)s?://\", input_url, re.IGNORECASE) is not None\n    return is_url\n\n\ndef download_and_untar(url):\n    cached_file = download_cached_file(\n        url, check_hash=False, progress=True\n    )\n    # get path to untarred directory\n    untarred_dir = os.path.basename(url).split(\".\")[0]\n    parent_dir = os.path.dirname(cached_file)\n\n    full_dir = os.path.join(parent_dir, untarred_dir)\n\n    if not os.path.exists(full_dir):\n        with tarfile.open(cached_file) as tar:\n            tar.extractall(parent_dir)\n\n    return full_dir\n\ndef cleanup_dir(dir):\n    \"\"\"\n    Utility for deleting a directory. Useful for cleaning the storage space\n    that contains various training artifacts like checkpoints, data etc.\n    \"\"\"\n    if os.path.exists(dir):\n        logging.info(f\"Deleting directory: {dir}\")\n        shutil.rmtree(dir)\n    logging.info(f\"Deleted contents of directory: {dir}\")\n\n\ndef get_file_size(filename):\n    \"\"\"\n    Given a file, get the size of file in MB\n    \"\"\"\n    size_in_mb = os.path.getsize(filename) / float(1024**2)\n    return size_in_mb\n"}
{"type": "source_file", "path": "HG_grounding/lit_models/lit_hgt_data.py", "content": "from typing import Any\nfrom lightning import LightningModule, LightningDataModule\nfrom lightning.pytorch.utilities.types import STEP_OUTPUT, OptimizerLRScheduler\nfrom models.clip_models.model import CLIP\nimport torch\nimport torch.nn.functional as F\nimport re\nimport numpy as np\nfrom omegaconf import OmegaConf\nfrom transformers.utils.import_utils import is_cython_available\nfrom pl_bolts.optimizers.lr_scheduler import LinearWarmupCosineAnnealingLR\nimport os.path as osp\nfrom dataset.caption_datasets_hgt import HeteCaptionDataset\nfrom lit_module_graph import BlipCaptionProcessor\nimport glob\n\nclass HeteLitDataModule(LightningDataModule):\n    def __init__(self, data_args) -> None:\n        super().__init__()\n        datasets_root = data_args.dataset_dir\n        datasets_names = data_args.datalist.split(\",\")\n        ann_paths = []\n        split_type = 'train'\n        for dsname in datasets_names: \n            json_files = glob.glob(osp.join(datasets_root, dsname, 'ann', '**/*.json'), recursive=True)\n            # assert len(json_files) == 1, f'{dsname} has more than one json file.'\n            ann_paths.extend(json_files)\n        self.train_data = HeteCaptionDataset(graph_processor = None, text_processor = BlipCaptionProcessor(), datasets_root = datasets_root, ann_paths = ann_paths)\n        \n        self.batch_size = data_args.micro_batch_size\n    \n    def train_dataloader(self):\n        return torch.utils.data.DataLoader(self.train_data, batch_size=self.batch_size, shuffle=True, collate_fn=self.train_data.collater)\n    \n    def val_dataloader(self):\n        return torch.utils.data.DataLoader(self.val_data, batch_size=self.batch_size, shuffle=False, collate_fn=self.val_data.collater)\n    \n    def test_dataloader(self):\n        return torch.utils.data.DataLoader(self.test_data, batch_size=self.batch_size, shuffle=False, collate_fn=self.test_data.collater)"}
{"type": "source_file", "path": "HG_grounding/models/clip_models/loss.py", "content": "\"\"\"\n Copyright (c) 2022, salesforce.com, inc.\n All rights reserved.\n SPDX-License-Identifier: BSD-3-Clause\n For full license text, see the LICENSE file in the repo root or https://opensource.org/licenses/BSD-3-Clause\n\"\"\"\n\nimport logging\nimport torch\nimport torch.distributed.nn\nfrom torch import distributed as dist, nn as nn\nfrom torch.nn import functional as F\n\ntry:\n    import horovod.torch as hvd\nexcept ImportError:\n    hvd = None\n\n\ndef gather_features(\n    image_features,\n    text_features,\n    local_loss=False,\n    gather_with_grad=False,\n    rank=0,\n    world_size=1,\n    use_horovod=False,\n):\n    if use_horovod:\n        assert hvd is not None, \"Please install horovod\"\n        if gather_with_grad:\n            all_image_features = hvd.allgather(image_features)\n            all_text_features = hvd.allgather(text_features)\n        else:\n            with torch.no_grad():\n                all_image_features = hvd.allgather(image_features)\n                all_text_features = hvd.allgather(text_features)\n            if not local_loss:\n                # ensure grads for local rank when all_* features don't have a gradient\n                gathered_image_features = list(\n                    all_image_features.chunk(world_size, dim=0)\n                )\n                gathered_text_features = list(\n                    all_text_features.chunk(world_size, dim=0)\n                )\n                gathered_image_features[rank] = image_features\n                gathered_text_features[rank] = text_features\n                all_image_features = torch.cat(gathered_image_features, dim=0)\n                all_text_features = torch.cat(gathered_text_features, dim=0)\n    else:\n        # We gather tensors from all gpus\n        if gather_with_grad:\n            all_image_features = torch.cat(\n                torch.distributed.nn.all_gather(image_features), dim=0\n            )\n            all_text_features = torch.cat(\n                torch.distributed.nn.all_gather(text_features), dim=0\n            )\n        else:\n            gathered_image_features = [\n                torch.zeros_like(image_features) for _ in range(world_size)\n            ]\n            gathered_text_features = [\n                torch.zeros_like(text_features) for _ in range(world_size)\n            ]\n            dist.all_gather(gathered_image_features, image_features)\n            dist.all_gather(gathered_text_features, text_features)\n            if not local_loss:\n                # ensure grads for local rank when all_* features don't have a gradient\n                gathered_image_features[rank] = image_features\n                gathered_text_features[rank] = text_features\n            all_image_features = torch.cat(gathered_image_features, dim=0)\n            all_text_features = torch.cat(gathered_text_features, dim=0)\n\n    return all_image_features, all_text_features\n\n\nclass ClipLoss(nn.Module):\n    def __init__(\n        self,\n        local_loss=False,\n        gather_with_grad=False,\n        cache_labels=False,\n        rank=0,\n        world_size=1,\n        use_horovod=False,\n    ):\n        super().__init__()\n        self.local_loss = local_loss\n        self.gather_with_grad = gather_with_grad\n        self.cache_labels = cache_labels\n        self.rank = rank\n        self.world_size = world_size\n        self.use_horovod = use_horovod\n\n        # cache state\n        self.prev_num_logits = 0\n        self.labels = {}\n\n    def forward(self, image_features, text_features, logit_scale):\n        device = image_features.device\n        if self.world_size > 1:\n            all_image_features, all_text_features = gather_features(\n                image_features,\n                text_features,\n                self.local_loss,\n                self.gather_with_grad,\n                self.rank,\n                self.world_size,\n                self.use_horovod,\n            )\n\n            if self.local_loss:\n                logits_per_image = logit_scale * image_features @ all_text_features.T\n                logits_per_text = logit_scale * text_features @ all_image_features.T\n            else:\n                logits_per_image = (\n                    logit_scale * all_image_features @ all_text_features.T\n                )\n                logits_per_text = logits_per_image.T\n        else:\n            logits_per_image = logit_scale * image_features @ text_features.T\n            logits_per_text = logit_scale * text_features @ image_features.T\n\n        # calculated ground-truth and cache if enabled\n        num_logits = logits_per_image.shape[0]\n        if self.prev_num_logits != num_logits or device not in self.labels:\n            labels = torch.arange(num_logits, device=device, dtype=torch.long)\n            if self.world_size > 1 and self.local_loss:\n                labels = labels + num_logits * self.rank\n            if self.cache_labels:\n                self.labels[device] = labels\n                self.prev_num_logits = num_logits\n        else:\n            labels = self.labels[device]\n\n        total_loss = (\n            F.cross_entropy(logits_per_image, labels)\n            + F.cross_entropy(logits_per_text, labels)\n        ) / 2\n        return total_loss\n\nclass HeteClipLoss(nn.Module):\n    def __init__(\n        self,\n    ):\n        super().__init__()\n\n    def forward(self, image_features, text_features, logit_scale):\n        device = image_features.device\n        logits_per_image = logit_scale * image_features @ text_features.T\n        logits_per_text = logit_scale * text_features @ image_features.T\n\n        # calculated ground-truth and cache if enabled\n        num_logits = logits_per_image.shape[0]\n        labels = torch.arange(num_logits, device=device, dtype=torch.long)\n\n        total_loss = (\n            F.cross_entropy(logits_per_image, labels)\n            + F.cross_entropy(logits_per_text, labels)\n        ) / 2\n        return total_loss"}
{"type": "source_file", "path": "HG_grounding/models/clip_models/clip_outputs.py", "content": "\"\"\"\n Copyright (c) 2022, salesforce.com, inc.\n All rights reserved.\n SPDX-License-Identifier: BSD-3-Clause\n For full license text, see the LICENSE file in the repo root or https://opensource.org/licenses/BSD-3-Clause\n\n Based on https://github.com/mlfoundations/open_clip\n\"\"\"\n\nfrom dataclasses import dataclass\n\nfrom typing import Optional\n\nimport torch\nfrom transformers.modeling_outputs import ModelOutput\n\n\n@dataclass\nclass ClipOutputFeatures(ModelOutput):\n    \"\"\"\n    Data class of features from AlbefFeatureExtractor.\n\n    Args:\n        image_embeds: `torch.FloatTensor` of shape `(batch_size, 1, embed_dim)`, `optional`\n        image_features: `torch.FloatTensor` of shape `(batch_size, 1, feature_dim)`, `optional`\n        text_embeds: `torch.FloatTensor` of shape `(batch_size, 1, embed_dim)`, `optional`\n        text_features: `torch.FloatTensor` of shape `(batch_size, 1, feature_dim)`, `optional`\n    \"\"\"\n\n    image_embeds: Optional[torch.FloatTensor] = None\n    image_embeds_proj: Optional[torch.FloatTensor] = None\n\n    text_embeds: Optional[torch.FloatTensor] = None\n    text_embeds_proj: Optional[torch.FloatTensor] = None\n\n\n@dataclass\nclass ClipOutput(ModelOutput):\n    intermediate_output: Optional[ClipOutputFeatures] = None\n\n    logit_scale_exp: Optional[torch.FloatTensor] = None\n\n    loss: Optional[torch.FloatTensor] = None\n\n\n@dataclass\nclass HeteClipOutputFeatures(ModelOutput):\n\n    graph_embeds: Optional[torch.FloatTensor] = None\n    graph_embeds_proj: Optional[torch.FloatTensor] = None\n\n    text_embeds: Optional[torch.FloatTensor] = None\n    text_embeds_proj: Optional[torch.FloatTensor] = None"}
{"type": "source_file", "path": "HG_grounding/lit_train/lit_hgt_train.py", "content": "import sys\nfrom pathlib import Path\n# support running without installing as a package\n# wd = Path(__file__).parent.parent.resolve()\n# sys.path.append(str(wd))\n\nfrom lightning import Trainer, seed_everything\nfrom lit_models.lit_hgt import HeteCLIP\nfrom lit_models.lit_hgt_data import HeteLitDataModule\n# from gpt_config import Config as GPTConfig\nimport lightning.pytorch.callbacks as plc\nfrom typing import Any, Optional, Dict, List, Sequence\nfrom dataclasses import dataclass, field\nimport transformers\n\n\n@dataclass\nclass TrainingArguments:\n    max_epochs: Optional[int] = field(default=100)\n    learning_rate: Optional[float] = field(default=1e-5)\n    devices: Optional[int] = field(default=4)\n\n@dataclass\nclass ModelArguments:\n    # graph\n    graph_in_channels: Optional[int] = field(default=768)\n    graph_out_channels: Optional[int] = field(default=768)\n    graph_heads: Optional[int] = field(default=12)\n    graph_dynamic: Optional[bool] = field(default=True)\n\n    # transformer\n    context_length: Optional[int] = field(default=128)\n    vocab_size: Optional[int] = field(default=49408)\n    text_width: Optional[int] = field(default=768)\n    text_heads: Optional[int] = field(default=8)\n    text_layers: Optional[int] = field(default=6)\n    \n    # clip\n    embed_dim: Optional[int] = field(default=768)\n    quick_gelu: Optional[bool] = field(default=False)\n\n    gnn_type: Optional[str] = field(default='meta')\n    \n@dataclass\nclass DataArguments:\n    batch_size: Optional[int] = field(default=1)\n    micro_batch_size: Optional[int] = field(default=1)\n    dataset_dir: Optional[str] = field(default='datasets')\n    datalist: Optional[str] = field(default='HetOAG')\n\ndef load_callbacks():\n    callbacks = []\n    callbacks.append(plc.EarlyStopping(\n        monitor='loss',\n        mode='min',\n        patience=5,\n        min_delta=0.001\n    ))\n\n    callbacks.append(plc.ModelCheckpoint(\n        monitor='loss',\n        filename='best-{epoch:02d}-{loss:.3f}',\n        save_top_k=1,\n        mode='min',\n        save_last=True\n    ))\n\n    callbacks.append(plc.LearningRateMonitor(\n            logging_interval='epoch'))\n    return callbacks\n\ndef train():\n    seed_everything(42)\n    parser = transformers.HfArgumentParser((ModelArguments, DataArguments, TrainingArguments))\n    model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n    data_args.gradient_accumulation_iters = data_args.batch_size // data_args.micro_batch_size\n    trainer = Trainer(\n        devices=training_args.devices,\n        max_epochs=training_args.max_epochs,\n        num_sanity_val_steps=0,\n        accelerator=\"auto\",\n        precision=\"32-true\",\n        enable_progress_bar=True,\n        callbacks=load_callbacks(),\n        strategy='ddp', \n        gradient_clip_val=0.5, gradient_clip_algorithm=\"value\"\n    )\n\n    pl_module = HeteCLIP(training_args, model_args, data_args)\n    data_module = HeteLitDataModule(data_args)\n\n    # print('*'*20 ,'grad none', '*'*20)\n    # for n, p in pl_module.named_parameters():\n    #     if p.grad is None: \n    #         print(n)\n\n    trainer.fit(pl_module, data_module)\n\nif __name__ == \"__main__\":\n    train()"}
{"type": "source_file", "path": "HG_grounding/models/clip_models/pretrained.py", "content": "\"\"\"\n Copyright (c) 2022, salesforce.com, inc.\n All rights reserved.\n SPDX-License-Identifier: BSD-3-Clause\n For full license text, see the LICENSE file in the repo root or https://opensource.org/licenses/BSD-3-Clause\n\n Based on https://github.com/mlfoundations/open_clip\n\"\"\"\n\nimport hashlib\nimport os\nimport urllib\nimport warnings\n\nfrom tqdm import tqdm\n\n_RN50 = dict(\n    openai=\"https://openaipublic.azureedge.net/clip/models/afeb0e10f9e5a86da6080e35cf09123aca3b358a0c3e3b6c78a7b63bc04b6762/RN50.pt\",\n    yfcc15m=\"https://github.com/mlfoundations/open_clip/releases/download/v0.2-weights/rn50-quickgelu-yfcc15m-455df137.pt\",\n    cc12m=\"https://github.com/mlfoundations/open_clip/releases/download/v0.2-weights/rn50-quickgelu-cc12m-f000538c.pt\",\n)\n\n_RN50_quickgelu = dict(\n    openai=\"https://openaipublic.azureedge.net/clip/models/afeb0e10f9e5a86da6080e35cf09123aca3b358a0c3e3b6c78a7b63bc04b6762/RN50.pt\",\n    yfcc15m=\"https://github.com/mlfoundations/open_clip/releases/download/v0.2-weights/rn50-quickgelu-yfcc15m-455df137.pt\",\n    cc12m=\"https://github.com/mlfoundations/open_clip/releases/download/v0.2-weights/rn50-quickgelu-cc12m-f000538c.pt\",\n)\n\n_RN101 = dict(\n    openai=\"https://openaipublic.azureedge.net/clip/models/8fa8567bab74a42d41c5915025a8e4538c3bdbe8804a470a72f30b0d94fab599/RN101.pt\",\n    yfcc15m=\"https://github.com/mlfoundations/open_clip/releases/download/v0.2-weights/rn101-quickgelu-yfcc15m-3e04b30e.pt\",\n)\n\n_RN101_quickgelu = dict(\n    openai=\"https://openaipublic.azureedge.net/clip/models/8fa8567bab74a42d41c5915025a8e4538c3bdbe8804a470a72f30b0d94fab599/RN101.pt\",\n    yfcc15m=\"https://github.com/mlfoundations/open_clip/releases/download/v0.2-weights/rn101-quickgelu-yfcc15m-3e04b30e.pt\",\n)\n\n_RN50x4 = dict(\n    openai=\"https://openaipublic.azureedge.net/clip/models/7e526bd135e493cef0776de27d5f42653e6b4c8bf9e0f653bb11773263205fdd/RN50x4.pt\",\n)\n\n_RN50x16 = dict(\n    openai=\"https://openaipublic.azureedge.net/clip/models/52378b407f34354e150460fe41077663dd5b39c54cd0bfd2b27167a4a06ec9aa/RN50x16.pt\",\n)\n\n_RN50x64 = dict(\n    openai=\"https://openaipublic.azureedge.net/clip/models/be1cfb55d75a9666199fb2206c106743da0f6468c9d327f3e0d0a543a9919d9c/RN50x64.pt\",\n)\n\n_VITB32 = dict(\n    openai=\"https://openaipublic.azureedge.net/clip/models/40d365715913c9da98579312b702a82c18be219cc2a73407c4526f58eba950af/ViT-B-32.pt\",\n    laion400m_e31=\"https://github.com/mlfoundations/open_clip/releases/download/v0.2-weights/vit_b_32-quickgelu-laion400m_e31-d867053b.pt\",\n    laion400m_e32=\"https://github.com/mlfoundations/open_clip/releases/download/v0.2-weights/vit_b_32-quickgelu-laion400m_e32-46683a32.pt\",\n    laion400m_avg=\"https://github.com/mlfoundations/open_clip/releases/download/v0.2-weights/vit_b_32-quickgelu-laion400m_avg-8a00ab3c.pt\",\n)\n\n_VITB32_quickgelu = dict(\n    openai=\"https://openaipublic.azureedge.net/clip/models/40d365715913c9da98579312b702a82c18be219cc2a73407c4526f58eba950af/ViT-B-32.pt\",\n    laion400m_e31=\"https://github.com/mlfoundations/open_clip/releases/download/v0.2-weights/vit_b_32-quickgelu-laion400m_e31-d867053b.pt\",\n    laion400m_e32=\"https://github.com/mlfoundations/open_clip/releases/download/v0.2-weights/vit_b_32-quickgelu-laion400m_e32-46683a32.pt\",\n    laion400m_avg=\"https://github.com/mlfoundations/open_clip/releases/download/v0.2-weights/vit_b_32-quickgelu-laion400m_avg-8a00ab3c.pt\",\n)\n\n_VITB16 = dict(\n    openai=\"https://openaipublic.azureedge.net/clip/models/5806e77cd80f8b59890b7e101eabd078d9fb84e6937f9e85e4ecb61988df416f/ViT-B-16.pt\",\n)\n\n_VITL14 = dict(\n    openai=\"https://openaipublic.azureedge.net/clip/models/b8cca3fd41ae0c99ba7e8951adf17d267cdb84cd88be6f7c2e0eca1737a03836/ViT-L-14.pt\",\n)\n\n_VITL14_336 = dict(\n    openai=\"https://openaipublic.azureedge.net/clip/models/3035c92b350959924f9f00213499208652fc7ea050643e8b385c2dac08641f02/ViT-L-14-336px.pt\"\n)\n\n_PRETRAINED = {\n    \"RN50\": _RN50,\n    \"RN50-quickgelu\": _RN50_quickgelu,\n    \"RN101\": _RN101,\n    \"RN101-quickgelu\": _RN101_quickgelu,\n    \"RN50x4\": _RN50x4,\n    \"RN50x16\": _RN50x16,\n    \"ViT-B-32\": _VITB32,\n    \"ViT-B-32-quickgelu\": _VITB32_quickgelu,\n    \"ViT-B-16\": _VITB16,\n    \"ViT-L-14\": _VITL14,\n    \"ViT-L-14-336\": _VITL14_336,\n}\n\n\ndef list_pretrained(as_str: bool = False):\n    \"\"\"returns list of pretrained models\n    Returns a tuple (model_name, pretrain_tag) by default or 'name:tag' if as_str == True\n    \"\"\"\n    return [\n        \":\".join([k, t]) if as_str else (k, t)\n        for k in _PRETRAINED.keys()\n        for t in _PRETRAINED[k].keys()\n    ]\n\n\ndef list_pretrained_tag_models(tag: str):\n    \"\"\"return all models having the specified pretrain tag\"\"\"\n    models = []\n    for k in _PRETRAINED.keys():\n        if tag in _PRETRAINED[k]:\n            models.append(k)\n    return models\n\n\ndef list_pretrained_model_tags(model: str):\n    \"\"\"return all pretrain tags for the specified model architecture\"\"\"\n    tags = []\n    if model in _PRETRAINED:\n        tags.extend(_PRETRAINED[model].keys())\n    return tags\n\n\ndef get_pretrained_url(model: str, tag: str):\n    if model not in _PRETRAINED:\n        return \"\"\n    model_pretrained = _PRETRAINED[model]\n    tag = tag.lower()\n    if tag not in model_pretrained:\n        return \"\"\n    return model_pretrained[tag]\n\n\ndef download_pretrained(url: str, root: str = os.path.expanduser(\"~/.cache/clip\")):\n    os.makedirs(root, exist_ok=True)\n    filename = os.path.basename(url)\n\n    if \"openaipublic\" in url:\n        expected_sha256 = url.split(\"/\")[-2]\n    else:\n        expected_sha256 = \"\"\n\n    download_target = os.path.join(root, filename)\n\n    if os.path.exists(download_target) and not os.path.isfile(download_target):\n        raise RuntimeError(f\"{download_target} exists and is not a regular file\")\n\n    if os.path.isfile(download_target):\n        if expected_sha256:\n            if (\n                hashlib.sha256(open(download_target, \"rb\").read()).hexdigest()\n                == expected_sha256\n            ):\n                return download_target\n            else:\n                warnings.warn(\n                    f\"{download_target} exists, but the SHA256 checksum does not match; re-downloading the file\"\n                )\n        else:\n            return download_target\n\n    with urllib.request.urlopen(url) as source, open(download_target, \"wb\") as output:\n        with tqdm(\n            total=int(source.info().get(\"Content-Length\")),\n            ncols=80,\n            unit=\"iB\",\n            unit_scale=True,\n        ) as loop:\n            while True:\n                buffer = source.read(8192)\n                if not buffer:\n                    break\n\n                output.write(buffer)\n                loop.update(len(buffer))\n\n    if (\n        expected_sha256\n        and hashlib.sha256(open(download_target, \"rb\").read()).hexdigest()\n        != expected_sha256\n    ):\n        raise RuntimeError(\n            f\"Model has been downloaded but the SHA256 checksum does not not match\"\n        )\n\n    return download_target\n"}
{"type": "source_file", "path": "HG_grounding/models/clip_models/tokenizer.py", "content": "\"\"\"\n Copyright (c) 2022, salesforce.com, inc.\n All rights reserved.\n SPDX-License-Identifier: BSD-3-Clause\n For full license text, see the LICENSE file in the repo root or https://opensource.org/licenses/BSD-3-Clause\n\n Based on https://github.com/mlfoundations/open_clip\n\"\"\"\n\n\"\"\" CLIP tokenizer\nCopied from https://github.com/openai/CLIP. Originally MIT License, Copyright (c) 2021 OpenAI.\n\"\"\"\nimport gzip\nimport html\nimport os\nfrom functools import lru_cache\nfrom typing import Union, List\n\nimport ftfy\nimport regex as re\nimport torch\n\n\n@lru_cache()\ndef default_bpe():\n    return os.path.join(\n        os.path.dirname(os.path.abspath(__file__)), \"bpe_simple_vocab_16e6.txt.gz\"\n    )\n\n\n@lru_cache()\ndef bytes_to_unicode():\n    \"\"\"\n    Returns list of utf-8 byte and a corresponding list of unicode strings.\n    The reversible bpe codes work on unicode strings.\n    This means you need a large # of unicode characters in your vocab if you want to avoid UNKs.\n    When you're at something like a 10B token dataset you end up needing around 5K for decent coverage.\n    This is a signficant percentage of your normal, say, 32K bpe vocab.\n    To avoid that, we want lookup tables between utf-8 bytes and unicode strings.\n    And avoids mapping to whitespace/control characters the bpe code barfs on.\n    \"\"\"\n    bs = (\n        list(range(ord(\"!\"), ord(\"~\") + 1))\n        + list(range(ord(\"¡\"), ord(\"¬\") + 1))\n        + list(range(ord(\"®\"), ord(\"ÿ\") + 1))\n    )\n    cs = bs[:]\n    n = 0\n    for b in range(2**8):\n        if b not in bs:\n            bs.append(b)\n            cs.append(2**8 + n)\n            n += 1\n    cs = [chr(n) for n in cs]\n    return dict(zip(bs, cs))\n\n\ndef get_pairs(word):\n    \"\"\"Return set of symbol pairs in a word.\n    Word is represented as tuple of symbols (symbols being variable-length strings).\n    \"\"\"\n    pairs = set()\n    prev_char = word[0]\n    for char in word[1:]:\n        pairs.add((prev_char, char))\n        prev_char = char\n    return pairs\n\n\ndef basic_clean(text):\n    text = ftfy.fix_text(text)\n    text = html.unescape(html.unescape(text))\n    return text.strip()\n\n\ndef whitespace_clean(text):\n    text = re.sub(r\"\\s+\", \" \", text)\n    text = text.strip()\n    return text\n\n\nclass SimpleTokenizer(object):\n    def __init__(self, bpe_path: str = default_bpe(), special_tokens=None):\n        self.byte_encoder = bytes_to_unicode()\n        self.byte_decoder = {v: k for k, v in self.byte_encoder.items()}\n        merges = gzip.open(bpe_path).read().decode(\"utf-8\").split(\"\\n\")\n        merges = merges[1 : 49152 - 256 - 2 + 1]\n        merges = [tuple(merge.split()) for merge in merges]\n        vocab = list(bytes_to_unicode().values())\n        vocab = vocab + [v + \"</w>\" for v in vocab]\n        for merge in merges:\n            vocab.append(\"\".join(merge))\n        if not special_tokens:\n            special_tokens = [\"<start_of_text>\", \"<end_of_text>\"]\n        else:\n            special_tokens = [\"<start_of_text>\", \"<end_of_text>\"] + special_tokens\n        vocab.extend(special_tokens)\n        self.encoder = dict(zip(vocab, range(len(vocab))))\n        self.decoder = {v: k for k, v in self.encoder.items()}\n        self.bpe_ranks = dict(zip(merges, range(len(merges))))\n        self.cache = {t: t for t in special_tokens}\n        special = \"|\".join(special_tokens)\n        self.pat = re.compile(\n            special + r\"\"\"|'s|'t|'re|'ve|'m|'ll|'d|[\\p{L}]+|[\\p{N}]|[^\\s\\p{L}\\p{N}]+\"\"\",\n            re.IGNORECASE,\n        )\n\n        self.vocab_size = len(self.encoder)\n        self.all_special_ids = [self.encoder[t] for t in special_tokens]\n\n    def bpe(self, token):\n        if token in self.cache:\n            return self.cache[token]\n        word = tuple(token[:-1]) + (token[-1] + \"</w>\",)\n        pairs = get_pairs(word)\n\n        if not pairs:\n            return token + \"</w>\"\n\n        while True:\n            bigram = min(pairs, key=lambda pair: self.bpe_ranks.get(pair, float(\"inf\")))\n            if bigram not in self.bpe_ranks:\n                break\n            first, second = bigram\n            new_word = []\n            i = 0\n            while i < len(word):\n                try:\n                    j = word.index(first, i)\n                    new_word.extend(word[i:j])\n                    i = j\n                except:\n                    new_word.extend(word[i:])\n                    break\n\n                if word[i] == first and i < len(word) - 1 and word[i + 1] == second:\n                    new_word.append(first + second)\n                    i += 2\n                else:\n                    new_word.append(word[i])\n                    i += 1\n            new_word = tuple(new_word)\n            word = new_word\n            if len(word) == 1:\n                break\n            else:\n                pairs = get_pairs(word)\n        word = \" \".join(word)\n        self.cache[token] = word\n        return word\n\n    def encode(self, text):\n        bpe_tokens = []\n        text = whitespace_clean(basic_clean(text)).lower()\n        for token in re.findall(self.pat, text):\n            token = \"\".join(self.byte_encoder[b] for b in token.encode(\"utf-8\"))\n            bpe_tokens.extend(\n                self.encoder[bpe_token] for bpe_token in self.bpe(token).split(\" \")\n            )\n        return bpe_tokens\n\n    def decode(self, tokens):\n        text = \"\".join([self.decoder[token] for token in tokens])\n        text = (\n            bytearray([self.byte_decoder[c] for c in text])\n            .decode(\"utf-8\", errors=\"replace\")\n            .replace(\"</w>\", \" \")\n        )\n        return text\n\n\n_tokenizer = SimpleTokenizer()\n\n\ndef tokenize(\n    texts: Union[str, List[str]], context_length: int = 77\n) -> torch.LongTensor:\n    \"\"\"\n    Returns the tokenized representation of given input string(s)\n    Parameters\n    ----------\n    texts : Union[str, List[str]]\n        An input string or a list of input strings to tokenize\n    context_length : int\n        The context length to use; all CLIP models use 77 as the context length\n    Returns\n    -------\n    A two-dimensional tensor containing the resulting tokens, shape = [number of input strings, context_length]\n    \"\"\"\n    if isinstance(texts, str):\n        texts = [texts]\n\n    sot_token = _tokenizer.encoder[\"<start_of_text>\"]\n    eot_token = _tokenizer.encoder[\"<end_of_text>\"]\n    all_tokens = [[sot_token] + _tokenizer.encode(text) + [eot_token] for text in texts]\n    result = torch.zeros(len(all_tokens), context_length, dtype=torch.long)\n\n    for i, tokens in enumerate(all_tokens):\n        if len(tokens) > context_length:\n            tokens = tokens[:context_length]  # Truncate\n        result[i, : len(tokens)] = torch.tensor(tokens)\n\n    return result\n"}
{"type": "source_file", "path": "HG_grounding/models/meta_hgt/ori_hgt.py", "content": "from torch_geometric.nn import HGTConv, Linear"}
{"type": "source_file", "path": "higpt/constants.py", "content": "from enum import IntEnum\nimport os\n\n# For the gradio web server\nSERVER_ERROR_MSG = (\n    \"**NETWORK ERROR DUE TO HIGH TRAFFIC. PLEASE REGENERATE OR REFRESH THIS PAGE.**\"\n)\nMODERATION_MSG = \"YOUR INPUT VIOLATES OUR CONTENT MODERATION GUIDELINES. PLEASE FIX YOUR INPUT AND TRY AGAIN.\"\nCONVERSATION_LIMIT_MSG = \"YOU HAVE REACHED THE CONVERSATION LENGTH LIMIT. PLEASE CLEAR HISTORY AND START A NEW CONVERSATION.\"\nINPUT_CHAR_LEN_LIMIT = 2560\nCONVERSATION_LEN_LIMIT = 50\nLOGDIR = \".\"\n\n# For the controller and workers(could be overwritten through ENV variables.)\nCONTROLLER_HEART_BEAT_EXPIRATION = int(\n    os.getenv(\"FASTCHAT_CONTROLLER_HEART_BEAT_EXPIRATION\", 90)\n)\nWORKER_HEART_BEAT_INTERVAL = int(os.getenv(\"FASTCHAT_WORKER_HEART_BEAT_INTERVAL\", 30))\nWORKER_API_TIMEOUT = int(os.getenv(\"FASTCHAT_WORKER_API_TIMEOUT\", 100))\nWORKER_API_EMBEDDING_BATCH_SIZE = int(os.getenv(\"WORKER_API_EMBEDDING_BATCH_SIZE\", 4))\n\n\nclass ErrorCode(IntEnum):\n    \"\"\"\n    https://platform.openai.com/docs/guides/error-codes/api-errors\n    \"\"\"\n\n    VALIDATION_TYPE_ERROR = 40001\n\n    INVALID_AUTH_KEY = 40101\n    INCORRECT_AUTH_KEY = 40102\n    NO_PERMISSION = 40103\n\n    INVALID_MODEL = 40301\n    PARAM_OUT_OF_RANGE = 40302\n    CONTEXT_OVERFLOW = 40303\n\n    RATE_LIMIT = 42901\n    QUOTA_EXCEEDED = 42902\n    ENGINE_OVERLOADED = 42903\n\n    INTERNAL_ERROR = 50001\n    CUDA_OUT_OF_MEMORY = 50002\n    GRADIO_REQUEST_ERROR = 50003\n    GRADIO_STREAM_UNKNOWN_ERROR = 50004\n    CONTROLLER_NO_WORKER = 50005\n    CONTROLLER_WORKER_TIMEOUT = 50006\n\nDEFAULT_GRAPH_TOKEN = \"<graph>\"\nDEFAULT_GRAPH_PATCH_TOKEN = \"<g_patch>\"\nDEFAULT_G_START_TOKEN = \"<g_start>\"\nDEFAULT_G_END_TOKEN = \"<g_end>\""}
{"type": "source_file", "path": "base_model.py", "content": "\"\"\"\n Copyright (c) 2022, salesforce.com, inc.\n All rights reserved.\n SPDX-License-Identifier: BSD-3-Clause\n For full license text, see the LICENSE file in the repo root or https://opensource.org/licenses/BSD-3-Clause\n\"\"\"\n\nimport logging\nimport os\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom dist_utils import download_cached_file, is_dist_avail_and_initialized\nfrom utils import get_abs_path, is_url\nfrom omegaconf import OmegaConf\n\n\nclass BaseModel(nn.Module):\n    \"\"\"Base class for models.\"\"\"\n\n    def __init__(self):\n        super().__init__()\n\n    @property\n    def device(self):\n        return list(self.parameters())[0].device\n\n    def load_checkpoint(self, url_or_filename):\n        \"\"\"\n        Load from a finetuned checkpoint.\n\n        This should expect no mismatch in the model keys and the checkpoint keys.\n        \"\"\"\n\n        if is_url(url_or_filename):\n            cached_file = download_cached_file(\n                url_or_filename, check_hash=False, progress=True\n            )\n            checkpoint = torch.load(cached_file, map_location=\"cpu\")\n        elif os.path.isfile(url_or_filename):\n            checkpoint = torch.load(url_or_filename, map_location=\"cpu\")\n        else:\n            raise RuntimeError(\"checkpoint url or path is invalid\")\n\n        if \"model\" in checkpoint.keys():\n            state_dict = checkpoint[\"model\"]\n        else:\n            state_dict = checkpoint\n\n        msg = self.load_state_dict(state_dict, strict=False)\n\n        logging.info(\"Missing keys {}\".format(msg.missing_keys))\n        logging.info(\"load checkpoint from %s\" % url_or_filename)\n\n        return msg\n\n    @classmethod\n    def from_pretrained(cls, model_type):\n        \"\"\"\n        Build a pretrained model from default configuration file, specified by model_type.\n\n        Args:\n            - model_type (str): model type, specifying architecture and checkpoints.\n\n        Returns:\n            - model (nn.Module): pretrained or finetuned model, depending on the configuration.\n        \"\"\"\n        model_cfg = OmegaConf.load(cls.default_config_path(model_type)).model\n        model = cls.from_config(model_cfg)\n\n        return model\n\n    @classmethod\n    def default_config_path(cls, model_type):\n        assert (\n            model_type in cls.PRETRAINED_MODEL_CONFIG_DICT\n        ), \"Unknown model type {}\".format(model_type)\n        return get_abs_path(cls.PRETRAINED_MODEL_CONFIG_DICT[model_type])\n\n    def load_checkpoint_from_config(self, cfg, **kwargs):\n        \"\"\"\n        Load checkpoint as specified in the config file.\n\n        If load_finetuned is True, load the finetuned model; otherwise, load the pretrained model.\n        When loading the pretrained model, each task-specific architecture may define their\n        own load_from_pretrained() method.\n        \"\"\"\n        load_finetuned = cfg.get(\"load_finetuned\", True)\n        if load_finetuned:\n            finetune_path = cfg.get(\"finetuned\", None)\n            assert (\n                finetune_path is not None\n            ), \"Found load_finetuned is True, but finetune_path is None.\"\n            self.load_checkpoint(url_or_filename=finetune_path)\n        else:\n            load_pretrained = cfg.get(\"load_pretrained\", True)\n            if load_pretrained:\n                # load pre-trained weights\n                pretrain_path = cfg.get(\"pretrained\", None)\n                assert \"Found load_finetuned is False, but pretrain_path is None.\"\n                self.load_from_pretrained(url_or_filename=pretrain_path, **kwargs)\n\n    def before_training(self, **kwargs):\n        pass\n\n    def get_optimizer_params(self, weight_decay, lr_scale=1):\n        p_wd, p_non_wd = [], []\n        for n, p in self.named_parameters():\n            if not p.requires_grad:\n                continue  # frozen weights\n            if p.ndim < 2 or \"bias\" in n or \"ln\" in n or \"bn\" in n:\n                p_non_wd.append(p)\n            else:\n                p_wd.append(p)        \n        optim_params = [\n            {\"params\": p_wd, \"weight_decay\": weight_decay, \"lr_scale\": lr_scale},\n            {\"params\": p_non_wd, \"weight_decay\": 0, \"lr_scale\": lr_scale},\n        ]                \n        return optim_params\n    \n    def before_evaluation(self, **kwargs):\n        pass\n\n    def show_n_params(self, return_str=True):\n        tot = 0\n        for p in self.parameters():\n            w = 1\n            for x in p.shape:\n                w *= x\n            tot += w\n        if return_str:\n            if tot >= 1e6:\n                return \"{:.1f}M\".format(tot / 1e6)\n            else:\n                return \"{:.1f}K\".format(tot / 1e3)\n        else:\n            return tot\n\n\nclass BaseEncoder(nn.Module):\n    \"\"\"\n    Base class for primitive encoders, such as ViT, TimeSformer, etc.\n    \"\"\"\n\n    def __init__(self):\n        super().__init__()\n\n    def forward_features(self, samples, **kwargs):\n        raise NotImplementedError\n\n    @property\n    def device(self):\n        return list(self.parameters())[0].device\n\n\nclass SharedQueueMixin:\n    @torch.no_grad()\n    def _dequeue_and_enqueue(self, image_feat, text_feat, idxs=None):\n        # gather keys before updating queue\n        image_feats = concat_all_gather(image_feat)\n        text_feats = concat_all_gather(text_feat)\n\n        batch_size = image_feats.shape[0]\n\n        ptr = int(self.queue_ptr)\n        assert self.queue_size % batch_size == 0  # for simplicity\n\n        # replace the keys at ptr (dequeue and enqueue)\n        self.image_queue[:, ptr : ptr + batch_size] = image_feats.T\n        self.text_queue[:, ptr : ptr + batch_size] = text_feats.T\n\n        if idxs is not None:\n            idxs = concat_all_gather(idxs)\n            self.idx_queue[:, ptr : ptr + batch_size] = idxs.T\n\n        ptr = (ptr + batch_size) % self.queue_size  # move pointer\n        self.queue_ptr[0] = ptr\n\n\nclass MomentumDistilationMixin:\n    @torch.no_grad()\n    def copy_params(self):\n        for model_pair in self.model_pairs:\n            for param, param_m in zip(\n                model_pair[0].parameters(), model_pair[1].parameters()\n            ):\n                param_m.data.copy_(param.data)  # initialize\n                param_m.requires_grad = False  # not update by gradient\n\n    @torch.no_grad()\n    def _momentum_update(self):\n        for model_pair in self.model_pairs:\n            for param, param_m in zip(\n                model_pair[0].parameters(), model_pair[1].parameters()\n            ):\n                param_m.data = param_m.data * self.momentum + param.data * (\n                    1.0 - self.momentum\n                )\n\n\nclass GatherLayer(torch.autograd.Function):\n    \"\"\"\n    Gather tensors from all workers with support for backward propagation:\n    This implementation does not cut the gradients as torch.distributed.all_gather does.\n    \"\"\"\n\n    @staticmethod\n    def forward(ctx, x):\n        output = [\n            torch.zeros_like(x) for _ in range(torch.distributed.get_world_size())\n        ]\n        torch.distributed.all_gather(output, x)\n        return tuple(output)\n\n    @staticmethod\n    def backward(ctx, *grads):\n        all_gradients = torch.stack(grads)\n        torch.distributed.all_reduce(all_gradients)\n        return all_gradients[torch.distributed.get_rank()]\n\n\ndef all_gather_with_grad(tensors):\n    \"\"\"\n    Performs all_gather operation on the provided tensors.\n    Graph remains connected for backward grad computation.\n    \"\"\"\n    # Queue the gathered tensors\n    world_size = torch.distributed.get_world_size()\n    # There is no need for reduction in the single-proc case\n    if world_size == 1:\n        return tensors\n\n    # tensor_all = GatherLayer.apply(tensors)\n    tensor_all = GatherLayer.apply(tensors)\n\n    return torch.cat(tensor_all, dim=0)\n\n\n@torch.no_grad()\ndef concat_all_gather(tensor):\n    \"\"\"\n    Performs all_gather operation on the provided tensors.\n    *** Warning ***: torch.distributed.all_gather has no gradient.\n    \"\"\"\n    # if use distributed training\n    if not is_dist_avail_and_initialized():\n        return tensor\n\n    tensors_gather = [\n        torch.ones_like(tensor) for _ in range(torch.distributed.get_world_size())\n    ]\n    torch.distributed.all_gather(tensors_gather, tensor, async_op=False)\n\n    output = torch.cat(tensors_gather, dim=0)\n    return output\n\n\ndef tile(x, dim, n_tile):\n    init_dim = x.size(dim)\n    repeat_idx = [1] * x.dim()\n    repeat_idx[dim] = n_tile\n    x = x.repeat(*(repeat_idx))\n    order_index = torch.LongTensor(\n        np.concatenate([init_dim * np.arange(n_tile) + i for i in range(init_dim)])\n    )\n    return torch.index_select(x, dim, order_index.to(x.device))\n"}
{"type": "source_file", "path": "HG_grounding/models/meta_hgt/hgt_constants.py", "content": "NODE_TYPE_DICT = {\n    'paper': 'This node represents a paper',\n    'venue': 'This node represents a venue (could be a journal, conference, repository or patent)',\n    'field': 'This node represents a field of study (could be L0, L1, L2, L3, L4 or L5)',\n    'author': 'This node represents an author',\n    'affiliation': 'This node represents an affiliation (could be a university, company or institution)',\n}\n\nEDGE_TYPE_DICT = {\n('paper', 'PV_Conference', 'venue'): 'The paper is published in the conference', \n('venue', 'Inverse_PV_Conference', 'paper'): 'The conference has the paper', \n('paper', 'PV_Journal', 'venue'): 'The paper is published in the journal', \n('venue', 'Inverse_PV_Journal', 'paper'): 'The journal has the paper', \n('paper', 'PV_Repository', 'venue'): 'The paper is published in the repository', \n('venue', 'Inverse_PV_Repository', 'paper'): 'The repository has the paper', \n('paper', 'PV_Patent', 'venue'): 'The paper is published in the patent', \n('venue', 'Inverse_PV_Patent', 'paper'): 'The patent has the paper', \n('paper', 'PP_cite', 'paper'): 'The paper cites the paper', \n('paper', 'Inverse_PP_cite', 'paper'): 'The paper is cited by the paper', \n('author', 'AP_write_last', 'paper'): 'The author is the last author of the paper', \n('paper', 'Inverse_AP_write_last', 'author'): 'The paper lists the author as its last author', \n('author', 'AP_write_other', 'paper'): 'The author is neither the first nor the last author of the paper', \n('paper', 'Inverse_AP_write_other', 'author'): 'The paper lists the author as neither its first nor its last author', \n('author', 'AP_write_first', 'paper'): 'The author is the first author of the paper', \n('paper', 'Inverse_AP_write_first', 'author'): 'The paper lists the author as its first author', \n('field', 'FF_in', 'field'): 'The field is in the field', \n('field', 'Inverse_FF_in', 'field'): 'The field includes the field', \n('paper', 'PF_in_L0', 'field'): 'The paper belongs to the L0 field', \n('field', 'Inverse_PF_in_L0', 'paper'): 'The L0 field includes the paper', \n('paper', 'PF_in_L3', 'field'): 'The paper belongs to the L3 field', \n('field', 'Inverse_PF_in_L3', 'paper'): 'The L3 field includes the paper', \n('paper', 'PF_in_L2', 'field'): 'The paper belongs to the L2 field', \n('field', 'Inverse_PF_in_L2', 'paper'): 'The L2 field includes the paper', \n('paper', 'PF_in_L5', 'field'): 'The paper belongs to the L5 field', \n('field', 'Inverse_PF_in_L5', 'paper'): 'The L5 field includes the paper', \n('paper', 'PF_in_L4', 'field'): 'The paper belongs to the L4 field', \n('field', 'Inverse_PF_in_L4', 'paper'): 'The L4 field includes the paper', \n('author', 'in', 'affiliation'): 'The author is affiliated with the affiliation', \n('affiliation', 'Inverse_in', 'author'): 'The affiliation has the author', \n}"}
{"type": "source_file", "path": "HG_grounding/models/clip_models/timm_model.py", "content": "\"\"\"\n Copyright (c) 2022, salesforce.com, inc.\n All rights reserved.\n SPDX-License-Identifier: BSD-3-Clause\n For full license text, see the LICENSE file in the repo root or https://opensource.org/licenses/BSD-3-Clause\n\n Based on https://github.com/mlfoundations/open_clip\n\"\"\"\n\n\"\"\" timm model adapter\nWraps timm (https://github.com/rwightman/pytorch-image-models) models for use as a vision tower in CLIP model.\n\"\"\"\nimport math\nimport warnings\nfrom collections import OrderedDict\nfrom typing import List, Optional, Tuple, Union\n\nimport torch\nimport torch.nn as nn\nfrom torch import nn as nn\n\ntry:\n    import timm\n    from timm.models.layers import Mlp, to_2tuple\n\n    # from timm.models.layers.attention_pool2d import RotAttentionPool2d\n    # from timm.models.layers.attention_pool2d import (\n    #     AttentionPool2d as AbsAttentionPool2d,\n    # )\n\nexcept ImportError as e:\n    timm = None\n\nfrom lavis.models.clip_models.utils import freeze_batch_norm_2d\n\n\nclass TimmModel(nn.Module):\n    \"\"\"timm model adapter\n    # FIXME this adapter is a work in progress, may change in ways that break weight compat\n    \"\"\"\n\n    def __init__(\n        self,\n        model_name,\n        embed_dim,\n        image_size=224,\n        pool=\"avg\",\n        proj=\"linear\",\n        drop=0.0,\n        pretrained=False,\n    ):\n        super().__init__()\n        if timm is None:\n            raise RuntimeError(\"Please `pip install timm` to use timm models.\")\n\n        self.image_size = to_2tuple(image_size)\n        self.trunk = timm.create_model(model_name, pretrained=pretrained)\n        feat_size = self.trunk.default_cfg.get(\"pool_size\", None)\n        feature_ndim = 1 if not feat_size else 2\n        if pool in (\"abs_attn\", \"rot_attn\"):\n            assert feature_ndim == 2\n            # if attn pooling used, remove both classifier and default pool\n            self.trunk.reset_classifier(0, global_pool=\"\")\n        else:\n            # reset global pool if pool config set, otherwise leave as network default\n            reset_kwargs = dict(global_pool=pool) if pool else {}\n            self.trunk.reset_classifier(0, **reset_kwargs)\n        prev_chs = self.trunk.num_features\n\n        head_layers = OrderedDict()\n        if pool == \"abs_attn\":\n            head_layers[\"pool\"] = AttentionPool2d(\n                prev_chs, feat_size=feat_size, out_features=embed_dim\n            )\n            prev_chs = embed_dim\n        elif pool == \"rot_attn\":\n            head_layers[\"pool\"] = RotAttentionPool2d(prev_chs, out_features=embed_dim)\n            prev_chs = embed_dim\n        else:\n            assert proj, \"projection layer needed if non-attention pooling is used.\"\n\n        # NOTE attention pool ends with a projection layer, so proj should usually be set to '' if such pooling is used\n        if proj == \"linear\":\n            head_layers[\"drop\"] = nn.Dropout(drop)\n            head_layers[\"proj\"] = nn.Linear(prev_chs, embed_dim)\n        elif proj == \"mlp\":\n            head_layers[\"mlp\"] = Mlp(prev_chs, 2 * embed_dim, embed_dim, drop=drop)\n\n        self.head = nn.Sequential(head_layers)\n\n    def lock(self, unlocked_groups=0, freeze_bn_stats=False):\n        \"\"\"lock modules\n        Args:\n            unlocked_groups (int): leave last n layer groups unlocked (default: 0)\n        \"\"\"\n        if not unlocked_groups:\n            # lock full model\n            for param in self.trunk.parameters():\n                param.requires_grad = False\n            if freeze_bn_stats:\n                freeze_batch_norm_2d(self.trunk)\n        else:\n            # NOTE: partial freeze requires latest timm (master) branch and is subject to change\n            try:\n                # FIXME import here until API stable and in an official release\n                from timm.models.helpers import group_modules, group_parameters\n            except ImportError:\n                raise RuntimeError(\n                    \"Please install latest timm `pip install git+https://github.com/rwightman/pytorch-image-models`\"\n                )\n            matcher = self.trunk.group_matcher()\n            gparams = group_parameters(self.trunk, matcher)\n            max_layer_id = max(gparams.keys())\n            max_layer_id = max_layer_id - unlocked_groups\n            for group_idx in range(max_layer_id + 1):\n                group = gparams[group_idx]\n                for param in group:\n                    self.trunk.get_parameter(param).requires_grad = False\n            if freeze_bn_stats:\n                gmodules = group_modules(self.trunk, matcher, reverse=True)\n                gmodules = {k for k, v in gmodules.items() if v <= max_layer_id}\n                freeze_batch_norm_2d(self.trunk, gmodules)\n\n    def forward(self, x):\n        x = self.trunk(x)\n        x = self.head(x)\n        return x\n\n\nclass RotAttentionPool2d(nn.Module):\n    \"\"\"Attention based 2D feature pooling w/ rotary (relative) pos embedding.\n    This is a multi-head attention based replacement for (spatial) average pooling in NN architectures.\n    Adapted from the AttentionPool2d in CLIP w/ rotary embedding instead of learned embed.\n    https://github.com/openai/CLIP/blob/3b473b0e682c091a9e53623eebc1ca1657385717/clip/model.py\n    NOTE: While this impl does not require a fixed feature size, performance at differeing resolutions from\n    train varies widely and falls off dramatically. I'm not sure if there is a way around this... -RW\n    \"\"\"\n\n    def __init__(\n        self,\n        in_features: int,\n        out_features: int = None,\n        embed_dim: int = None,\n        num_heads: int = 4,\n        qkv_bias: bool = True,\n    ):\n        super().__init__()\n        embed_dim = embed_dim or in_features\n        out_features = out_features or in_features\n        self.qkv = nn.Linear(in_features, embed_dim * 3, bias=qkv_bias)\n        self.proj = nn.Linear(embed_dim, out_features)\n        self.num_heads = num_heads\n        assert embed_dim % num_heads == 0\n        self.head_dim = embed_dim // num_heads\n        self.scale = self.head_dim**-0.5\n        self.pos_embed = RotaryEmbedding(self.head_dim)\n\n        trunc_normal_(self.qkv.weight, std=in_features**-0.5)\n        nn.init.zeros_(self.qkv.bias)\n\n    def forward(self, x):\n        B, _, H, W = x.shape\n        N = H * W\n        x = x.reshape(B, -1, N).permute(0, 2, 1)\n\n        x = torch.cat([x.mean(1, keepdim=True), x], dim=1)\n\n        x = (\n            self.qkv(x)\n            .reshape(B, N + 1, 3, self.num_heads, self.head_dim)\n            .permute(2, 0, 3, 1, 4)\n        )\n        q, k, v = x[0], x[1], x[2]\n\n        qc, q = q[:, :, :1], q[:, :, 1:]\n        sin_emb, cos_emb = self.pos_embed.get_embed((H, W))\n        q = apply_rot_embed(q, sin_emb, cos_emb)\n        q = torch.cat([qc, q], dim=2)\n\n        kc, k = k[:, :, :1], k[:, :, 1:]\n        k = apply_rot_embed(k, sin_emb, cos_emb)\n        k = torch.cat([kc, k], dim=2)\n\n        attn = (q @ k.transpose(-2, -1)) * self.scale\n        attn = attn.softmax(dim=-1)\n\n        x = (attn @ v).transpose(1, 2).reshape(B, N + 1, -1)\n        x = self.proj(x)\n        return x[:, 0]\n\n\nclass AttentionPool2d(nn.Module):\n    \"\"\"Attention based 2D feature pooling w/ learned (absolute) pos embedding.\n    This is a multi-head attention based replacement for (spatial) average pooling in NN architectures.\n    It was based on impl in CLIP by OpenAI\n    https://github.com/openai/CLIP/blob/3b473b0e682c091a9e53623eebc1ca1657385717/clip/model.py\n    NOTE: This requires feature size upon construction and well prevent adaptive sizing of the network.\n    \"\"\"\n\n    def __init__(\n        self,\n        in_features: int,\n        feat_size: Union[int, Tuple[int, int]],\n        out_features: int = None,\n        embed_dim: int = None,\n        num_heads: int = 4,\n        qkv_bias: bool = True,\n    ):\n        super().__init__()\n\n        embed_dim = embed_dim or in_features\n        out_features = out_features or in_features\n        assert embed_dim % num_heads == 0\n        self.feat_size = to_2tuple(feat_size)\n        self.qkv = nn.Linear(in_features, embed_dim * 3, bias=qkv_bias)\n        self.proj = nn.Linear(embed_dim, out_features)\n        self.num_heads = num_heads\n        self.head_dim = embed_dim // num_heads\n        self.scale = self.head_dim**-0.5\n\n        spatial_dim = self.feat_size[0] * self.feat_size[1]\n        self.pos_embed = nn.Parameter(torch.zeros(spatial_dim + 1, in_features))\n        trunc_normal_(self.pos_embed, std=in_features**-0.5)\n        trunc_normal_(self.qkv.weight, std=in_features**-0.5)\n        nn.init.zeros_(self.qkv.bias)\n\n    def forward(self, x):\n        B, _, H, W = x.shape\n        N = H * W\n        assert self.feat_size[0] == H\n        assert self.feat_size[1] == W\n        x = x.reshape(B, -1, N).permute(0, 2, 1)\n        x = torch.cat([x.mean(1, keepdim=True), x], dim=1)\n        x = x + self.pos_embed.unsqueeze(0).to(x.dtype)\n\n        x = (\n            self.qkv(x)\n            .reshape(B, N + 1, 3, self.num_heads, self.head_dim)\n            .permute(2, 0, 3, 1, 4)\n        )\n        q, k, v = x[0], x[1], x[2]\n        attn = (q @ k.transpose(-2, -1)) * self.scale\n        attn = attn.softmax(dim=-1)\n\n        x = (attn @ v).transpose(1, 2).reshape(B, N + 1, -1)\n        x = self.proj(x)\n        return x[:, 0]\n\n\ndef pixel_freq_bands(\n    num_bands: int,\n    max_freq: float = 224.0,\n    linear_bands: bool = True,\n    dtype: torch.dtype = torch.float32,\n    device: Optional[torch.device] = None,\n):\n    if linear_bands:\n        bands = torch.linspace(1.0, max_freq / 2, num_bands, dtype=dtype, device=device)\n    else:\n        bands = 2 ** torch.linspace(\n            0, math.log(max_freq, 2) - 1, num_bands, dtype=dtype, device=device\n        )\n    return bands * torch.pi\n\n\ndef inv_freq_bands(\n    num_bands: int,\n    temperature: float = 100000.0,\n    step: int = 2,\n    dtype: torch.dtype = torch.float32,\n    device: Optional[torch.device] = None,\n) -> torch.Tensor:\n    inv_freq = 1.0 / (\n        temperature\n        ** (torch.arange(0, num_bands, step, dtype=dtype, device=device) / num_bands)\n    )\n    return inv_freq\n\n\ndef build_sincos2d_pos_embed(\n    feat_shape: List[int],\n    dim: int = 64,\n    temperature: float = 10000.0,\n    reverse_coord: bool = False,\n    interleave_sin_cos: bool = False,\n    dtype: torch.dtype = torch.float32,\n    device: Optional[torch.device] = None,\n) -> torch.Tensor:\n    \"\"\"\n    Args:\n        feat_shape:\n        dim:\n        temperature:\n        reverse_coord: stack grid order W, H instead of H, W\n        interleave_sin_cos: sin, cos, sin, cos stack instead of sin, sin, cos, cos\n        dtype:\n        device:\n    Returns:\n    \"\"\"\n    assert (\n        dim % 4 == 0\n    ), \"Embed dimension must be divisible by 4 for sin-cos 2D position embedding\"\n    pos_dim = dim // 4\n    bands = inv_freq_bands(\n        pos_dim, temperature=temperature, step=1, dtype=dtype, device=device\n    )\n\n    if reverse_coord:\n        feat_shape = feat_shape[::-1]  # stack W, H instead of H, W\n    grid = (\n        torch.stack(\n            torch.meshgrid(\n                [torch.arange(s, device=device, dtype=dtype) for s in feat_shape]\n            )\n        )\n        .flatten(1)\n        .transpose(0, 1)\n    )\n    pos2 = grid.unsqueeze(-1) * bands.unsqueeze(0)\n    # FIXME add support for unflattened spatial dim?\n\n    stack_dim = (\n        2 if interleave_sin_cos else 1\n    )  # stack sin, cos, sin, cos  instead of sin sin cos cos\n    pos_emb = torch.stack([torch.sin(pos2), torch.cos(pos2)], dim=stack_dim).flatten(1)\n    return pos_emb\n\n\ndef build_fourier_pos_embed(\n    feat_shape: List[int],\n    bands: Optional[torch.Tensor] = None,\n    num_bands: int = 64,\n    max_res: int = 224,\n    linear_bands: bool = False,\n    include_grid: bool = False,\n    concat_out: bool = True,\n    in_pixels: bool = True,\n    dtype: torch.dtype = torch.float32,\n    device: Optional[torch.device] = None,\n) -> List[torch.Tensor]:\n    if bands is None:\n        if in_pixels:\n            bands = pixel_freq_bands(\n                num_bands,\n                float(max_res),\n                linear_bands=linear_bands,\n                dtype=dtype,\n                device=device,\n            )\n        else:\n            bands = inv_freq_bands(num_bands, step=1, dtype=dtype, device=device)\n    else:\n        if device is None:\n            device = bands.device\n        if dtype is None:\n            dtype = bands.dtype\n\n    if in_pixels:\n        grid = torch.stack(\n            torch.meshgrid(\n                [\n                    torch.linspace(-1.0, 1.0, steps=s, device=device, dtype=dtype)\n                    for s in feat_shape\n                ]\n            ),\n            dim=-1,\n        )\n    else:\n        grid = torch.stack(\n            torch.meshgrid(\n                [torch.arange(s, device=device, dtype=dtype) for s in feat_shape]\n            ),\n            dim=-1,\n        )\n    grid = grid.unsqueeze(-1)\n    pos = grid * bands\n\n    pos_sin, pos_cos = pos.sin(), pos.cos()\n    out = (grid, pos_sin, pos_cos) if include_grid else (pos_sin, pos_cos)\n    # FIXME torchscript doesn't like multiple return types, probably need to always cat?\n    if concat_out:\n        out = torch.cat(out, dim=-1)\n    return out\n\n\nclass FourierEmbed(nn.Module):\n    def __init__(\n        self,\n        max_res: int = 224,\n        num_bands: int = 64,\n        concat_grid=True,\n        keep_spatial=False,\n    ):\n        super().__init__()\n        self.max_res = max_res\n        self.num_bands = num_bands\n        self.concat_grid = concat_grid\n        self.keep_spatial = keep_spatial\n        self.register_buffer(\n            \"bands\", pixel_freq_bands(max_res, num_bands), persistent=False\n        )\n\n    def forward(self, x):\n        B, C = x.shape[:2]\n        feat_shape = x.shape[2:]\n        emb = build_fourier_pos_embed(\n            feat_shape,\n            self.bands,\n            include_grid=self.concat_grid,\n            dtype=x.dtype,\n            device=x.device,\n        )\n        emb = emb.transpose(-1, -2).flatten(len(feat_shape))\n        batch_expand = (B,) + (-1,) * (x.ndim - 1)\n\n        # FIXME support nD\n        if self.keep_spatial:\n            x = torch.cat(\n                [x, emb.unsqueeze(0).expand(batch_expand).permute(0, 3, 1, 2)], dim=1\n            )\n        else:\n            x = torch.cat(\n                [x.permute(0, 2, 3, 1), emb.unsqueeze(0).expand(batch_expand)], dim=-1\n            )\n            x = x.reshape(B, feat_shape.numel(), -1)\n\n        return x\n\n\ndef rot(x):\n    return torch.stack([-x[..., 1::2], x[..., ::2]], -1).reshape(x.shape)\n\n\ndef apply_rot_embed(x: torch.Tensor, sin_emb, cos_emb):\n    return x * cos_emb + rot(x) * sin_emb\n\n\ndef apply_rot_embed_list(x: List[torch.Tensor], sin_emb, cos_emb):\n    if isinstance(x, torch.Tensor):\n        x = [x]\n    return [t * cos_emb + rot(t) * sin_emb for t in x]\n\n\ndef apply_rot_embed_split(x: torch.Tensor, emb):\n    split = emb.shape[-1] // 2\n    return x * emb[:, :split] + rot(x) * emb[:, split:]\n\n\ndef build_rotary_pos_embed(\n    feat_shape: List[int],\n    bands: Optional[torch.Tensor] = None,\n    dim: int = 64,\n    max_freq: float = 224,\n    linear_bands: bool = False,\n    dtype: torch.dtype = torch.float32,\n    device: Optional[torch.device] = None,\n):\n    \"\"\"\n    NOTE: shape arg should include spatial dim only\n    \"\"\"\n    feat_shape = torch.Size(feat_shape)\n\n    sin_emb, cos_emb = build_fourier_pos_embed(\n        feat_shape,\n        bands=bands,\n        num_bands=dim // 4,\n        max_res=max_freq,\n        linear_bands=linear_bands,\n        concat_out=False,\n        device=device,\n        dtype=dtype,\n    )\n    N = feat_shape.numel()\n    sin_emb = sin_emb.reshape(N, -1).repeat_interleave(2, -1)\n    cos_emb = cos_emb.reshape(N, -1).repeat_interleave(2, -1)\n    return sin_emb, cos_emb\n\n\nclass RotaryEmbedding(nn.Module):\n    \"\"\"Rotary position embedding\n    NOTE: This is my initial attempt at impl rotary embedding for spatial use, it has not\n    been well tested, and will likely change. It will be moved to its own file.\n    The following impl/resources were referenced for this impl:\n    * https://github.com/lucidrains/vit-pytorch/blob/6f3a5fcf0bca1c5ec33a35ef48d97213709df4ba/vit_pytorch/rvt.py\n    * https://blog.eleuther.ai/rotary-embeddings/\n    \"\"\"\n\n    def __init__(self, dim, max_res=224, linear_bands: bool = False):\n        super().__init__()\n        self.dim = dim\n        self.register_buffer(\n            \"bands\",\n            pixel_freq_bands(dim // 4, max_res, linear_bands=linear_bands),\n            persistent=False,\n        )\n\n    def get_embed(self, shape: List[int]):\n        return build_rotary_pos_embed(shape, self.bands)\n\n    def forward(self, x):\n        # assuming channel-first tensor where spatial dim are >= 2\n        sin_emb, cos_emb = self.get_embed(x.shape[2:])\n        return apply_rot_embed(x, sin_emb, cos_emb)\n\n\ndef _no_grad_trunc_normal_(tensor, mean, std, a, b):\n    # Cut & paste from PyTorch official master until it's in a few official releases - RW\n    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf\n    def norm_cdf(x):\n        # Computes standard normal cumulative distribution function\n        return (1.0 + math.erf(x / math.sqrt(2.0))) / 2.0\n\n    if (mean < a - 2 * std) or (mean > b + 2 * std):\n        warnings.warn(\n            \"mean is more than 2 std from [a, b] in nn.init.trunc_normal_. \"\n            \"The distribution of values may be incorrect.\",\n            stacklevel=2,\n        )\n\n    with torch.no_grad():\n        # Values are generated by using a truncated uniform distribution and\n        # then using the inverse CDF for the normal distribution.\n        # Get upper and lower cdf values\n        l = norm_cdf((a - mean) / std)\n        u = norm_cdf((b - mean) / std)\n\n        # Uniformly fill tensor with values from [l, u], then translate to\n        # [2l-1, 2u-1].\n        tensor.uniform_(2 * l - 1, 2 * u - 1)\n\n        # Use inverse cdf transform for normal distribution to get truncated\n        # standard normal\n        tensor.erfinv_()\n\n        # Transform to proper mean, std\n        tensor.mul_(std * math.sqrt(2.0))\n        tensor.add_(mean)\n\n        # Clamp to ensure it's in the proper range\n        tensor.clamp_(min=a, max=b)\n        return tensor\n\n\ndef trunc_normal_(tensor, mean=0.0, std=1.0, a=-2.0, b=2.0):\n    r\"\"\"Fills the input Tensor with values drawn from a truncated\n    normal distribution. The values are effectively drawn from the\n    normal distribution :math:`\\mathcal{N}(\\text{mean}, \\text{std}^2)`\n    with values outside :math:`[a, b]` redrawn until they are within\n    the bounds. The method used for generating the random values works\n    best when :math:`a \\leq \\text{mean} \\leq b`.\n    Args:\n        tensor: an n-dimensional `torch.Tensor`\n        mean: the mean of the normal distribution\n        std: the standard deviation of the normal distribution\n        a: the minimum cutoff value\n        b: the maximum cutoff value\n    Examples:\n        >>> w = torch.empty(3, 5)\n        >>> nn.init.trunc_normal_(w)\n    \"\"\"\n    return _no_grad_trunc_normal_(tensor, mean, std, a, b)\n"}
{"type": "source_file", "path": "HG_grounding/models/meta_hgt/meta_hgtconv_bert_all.py", "content": "import math\nfrom typing import Dict, List, Optional, Tuple, Union\n\nimport torch\nfrom torch import Tensor\nfrom torch.nn import Parameter\n\nfrom torch_geometric.nn.conv import MessagePassing\n# from torch_geometric.nn.dense import HeteroDictLinear, HeteroLinear\nfrom models.meta_hgt.meta_linear import MetaHeteroDictLinear, MetaHeteroLinear\nfrom torch_geometric.nn.inits import ones\nfrom torch_geometric.nn.parameter_dict import ParameterDict\nfrom torch_geometric.typing import Adj, EdgeType, Metadata, NodeType\nfrom torch_geometric.utils import softmax\nfrom torch_geometric.utils.hetero import construct_bipartite_edge_index\n# from models.meta_hgt.hgt_constants import NODE_TYPE_DICT, EDGE_TYPE_DICT\nfrom dataclasses import dataclass\nimport torch.nn as nn\nfrom models.clip_models.tokenizer import tokenize\nfrom transformers import BertTokenizer, BertModel\nfrom pathlib import Path\n\nwd = Path(__file__).resolve().parent\n@dataclass\nclass MetaHGTConvCfg:\n    in_channels: int \n    out_channels: int\n    heads: int\n    dynamic: bool = True\n\n\nclass MetaHGTConv(MessagePassing):\n    \n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        heads: int = 1,\n        dynamic: bool = False,\n        text_cfg = None, \n        layernorm = None, \n        **kwargs,\n    ):\n        super().__init__(aggr='add', node_dim=0, **kwargs)\n\n        if out_channels % heads != 0:\n            raise ValueError(f\"'out_channels' (got {out_channels}) must be \"\n                             f\"divisible by the number of heads (got {heads})\")\n\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.heads = heads\n        \n\n        self.kqv_lin = MetaHeteroDictLinear(text_cfg.width, self.in_channels,\n                                        self.out_channels * 3, dynamic)\n\n        self.out_lin = MetaHeteroDictLinear(text_cfg.width, self.out_channels, self.out_channels, dynamic)\n        self.context_length = text_cfg.context_length\n\n        dim = out_channels // heads\n\n        self.k_rel = MetaHeteroLinear(text_cfg.width, dim, dim, dynamic)\n        self.v_rel = MetaHeteroLinear(text_cfg.width, dim, dim, dynamic)\n\n        self.skipTrans = nn.Linear(text_cfg.width, 1) # node aware, skip: 1\n\n        self.p_relTrans = nn.Linear(text_cfg.width, heads) # edge aware, p_rel: 1, heads\n        # self.ln_final = layernorm(text_cfg.width)\n\n        self.norm = nn.LayerNorm(self.out_channels, eps=1e-6)\n\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        super().reset_parameters()\n\n    def _cat(self, x_dict: Dict[str, Tensor]) -> Tuple[Tensor, Dict[str, int]]:\n        \"\"\"Concatenates a dictionary of features.\"\"\"\n        cumsum = 0\n        outs: List[Tensor] = []\n        offset: Dict[str, int] = {}\n        for key, x in x_dict.items():\n            outs.append(x)\n            offset[key] = cumsum\n            cumsum += x.size(0)\n        return torch.cat(outs, dim=0), offset\n\n    def _construct_src_node_feat(\n        self, k_dict: Dict[str, Tensor], v_dict: Dict[str, Tensor],\n        edge_index_dict: Dict[EdgeType, Adj], \n        edge_type_feas_dict: Dict[EdgeType, Tensor], \n    ) -> Tuple[Tensor, Tensor, Dict[EdgeType, int]]:\n        \"\"\"Constructs the source node representations.\"\"\"\n        cumsum = 0\n        num_edge_types = len(edge_index_dict.keys())\n        H, D = self.heads, self.out_channels // self.heads\n\n        # Flatten into a single tensor with shape [num_edge_types * heads, D]:\n        ks: List[Tensor] = []\n        vs: List[Tensor] = []\n        type_list: List[Tensor] = []\n        offset: Dict[EdgeType] = {}\n\n        edge_types_map = {\n            edge_type: i\n            for i, edge_type in enumerate(edge_index_dict.keys())\n        }\n        for edge_type in edge_index_dict.keys():\n            src = edge_type[0]\n            N = k_dict[src].size(0)\n            offset[edge_type] = cumsum\n            cumsum += N\n\n            # construct type_vec for curr edge_type with shape [H, D]\n            edge_type_offset = edge_types_map[edge_type]\n            type_vec = torch.arange(H, dtype=torch.long).view(-1, 1).repeat(\n                1, N) * num_edge_types + edge_type_offset\n\n            type_list.append(type_vec)\n            ks.append(k_dict[src])\n            vs.append(v_dict[src])\n\n        ks = torch.cat(ks, dim=0).transpose(0, 1).reshape(-1, D)\n        vs = torch.cat(vs, dim=0).transpose(0, 1).reshape(-1, D)\n        type_vec = torch.cat(type_list, dim=1).flatten()\n\n        edge_feas_dict = {edge_types_map[k]: v for k, v in edge_type_feas_dict.items()}\n\n        k = self.k_rel(ks, type_vec, edge_feas_dict).view(H, -1, D).transpose(0, 1)\n        v = self.v_rel(vs, type_vec, edge_feas_dict).view(H, -1, D).transpose(0, 1)\n\n        return k, v, offset\n\n    def _construct_p_rel(self, edge_type_feas_dict: Dict[EdgeType, Tensor]):\n        p_rel = {k: self.p_relTrans(v).unsqueeze(0) for k, v in edge_type_feas_dict.items()}\n        return p_rel\n    def _construct_skip(self, node_type_feas_dict: Dict[EdgeType, Tensor]):\n        skip = {k: self.skipTrans(v) for k, v in node_type_feas_dict.items()}\n        return skip\n\n    def forward(\n        self,\n        x_dict: Dict[NodeType, Tensor],\n        edge_index_dict: Dict[EdgeType, Adj],  # Support both.\n        data_type: str = 'dblp', \n        node_type_feas_dict: Dict[NodeType, Tensor] = None,\n        edge_type_feas_dict: Dict[EdgeType, Tensor] = None,\n    ) -> Dict[NodeType, Optional[Tensor]]:\n\n        \n        F = self.out_channels\n        H = self.heads\n        D = F // H\n\n        k_dict, q_dict, v_dict, out_dict = {}, {}, {}, {}\n\n        # Compute K, Q, V over node types:\n        kqv_dict = self.kqv_lin(x_dict, node_type_feas_dict)\n        for key, val in kqv_dict.items():\n            k, q, v = torch.tensor_split(val, 3, dim=1)\n            k_dict[key] = k.view(-1, H, D)\n            q_dict[key] = q.view(-1, H, D)\n            v_dict[key] = v.view(-1, H, D)\n\n        q, dst_offset = self._cat(q_dict)\n        k, v, src_offset = self._construct_src_node_feat(\n            k_dict, v_dict, edge_index_dict, edge_type_feas_dict)\n        p_rel = self._construct_p_rel(edge_type_feas_dict)\n        edge_index, edge_attr = construct_bipartite_edge_index(\n            edge_index_dict, src_offset, dst_offset, edge_attr_dict=p_rel)\n\n        out = self.propagate(edge_index, k=k, q=q, v=v, edge_attr=edge_attr,\n                             size=None)\n\n        dst_node_types = set([key[-1] for key in edge_index_dict.keys()])\n\n        # Reconstruct output node embeddings dict:\n        for node_type, start_offset in dst_offset.items():\n            end_offset = start_offset + q_dict[node_type].size(0)\n            if node_type in dst_node_types:\n                out_dict[node_type] = out[start_offset:end_offset]\n\n        # Transform output node embeddings:\n        a_dict = self.out_lin({\n            k:\n            torch.nn.functional.gelu(v) if v is not None else v\n            for k, v in out_dict.items()\n        }, node_type_feas_dict)\n\n        skip = self._construct_skip(node_type_feas_dict)\n        # Iterate over node types:\n        for node_type, out in out_dict.items():\n            out = a_dict[node_type]\n\n            if out.size(-1) == x_dict[node_type].size(-1):\n                alpha = skip[node_type].sigmoid()\n                out = alpha * out + (1 - alpha) * x_dict[node_type]\n            out = torch.clamp(out, -100.0, 100.0)\n            out = self.norm(out)\n            out_dict[node_type] = out\n\n        return out_dict\n\n    def message(self, k_j: Tensor, q_i: Tensor, v_j: Tensor, edge_attr: Tensor,\n                index: Tensor, ptr: Optional[Tensor],\n                size_i: Optional[int]) -> Tensor:\n        alpha = (q_i * k_j).sum(dim=-1) * edge_attr\n        alpha = alpha / math.sqrt(q_i.size(-1))\n        alpha = softmax(alpha, index, ptr, size_i)\n        out = v_j * alpha.view(-1, self.heads, 1)\n        return out.view(-1, self.out_channels)\n\n    def __repr__(self) -> str:\n        return (f'{self.__class__.__name__}(-1, {self.out_channels}, '\n                f'heads={self.heads})')\n"}
{"type": "source_file", "path": "HG_grounding/models/clip_models/model.py", "content": "\"\"\"\n Copyright (c) 2022, salesforce.com, inc.\n All rights reserved.\n SPDX-License-Identifier: BSD-3-Clause\n For full license text, see the LICENSE file in the repo root or https://opensource.org/licenses/BSD-3-Clause\n\n Based on https://github.com/mlfoundations/open_clip\n\"\"\"\n\n\"\"\" CLIP Model\nAdapted from https://github.com/openai/CLIP. Originally MIT License, Copyright (c) 2021 OpenAI.\n\"\"\"\n\nimport datetime\nimport json\nimport logging\nimport os\nimport re\nimport time\nimport warnings\nfrom collections import OrderedDict\nfrom copy import deepcopy\nfrom dataclasses import dataclass\nfrom pathlib import Path\nfrom typing import Callable, List, Optional, Tuple, Union, Dict\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\n# from lavis.common.registry import registry\n# from lavis.common.utils import get_abs_path\nfrom base_model import BaseModel\nfrom models.clip_models.clip_outputs import ClipOutput, HeteClipOutputFeatures\n# from models.clip_models.timm_model import TimmModel\nfrom models.clip_models.transform import image_transform\nfrom models.clip_models.utils import freeze_batch_norm_2d\n# from lavis.tasks.multimodal_classification import MultimodalClassificationTask\nfrom torch import nn\nfrom models.meta_hgt.meta_hgtconv_bert_all import MetaHGTConv, MetaHGTConvCfg\n\nfrom .pretrained import (\n    download_pretrained,\n    get_pretrained_url,\n    list_pretrained_tag_models,\n)\n\n_MODEL_CONFIG_PATHS = [Path(__file__).parent.parent.parent / f\"configs/models/clip/\"]\n_MODEL_CONFIGS = {}  # directory (model_name: config) of model architecture configs\n\n\nclass LayerNorm(nn.LayerNorm):\n    \"\"\"Subclass torch's LayerNorm to handle fp16.\"\"\"\n\n    def forward(self, x: torch.Tensor):\n        orig_type = x.dtype\n        x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n        return x.to(orig_type)\n\n\nclass QuickGELU(nn.Module):\n    # NOTE This is slower than nn.GELU or nn.SiLU and uses more GPU memory\n    def forward(self, x: torch.Tensor):\n        return x * torch.sigmoid(1.702 * x)\n\n\nclass ResidualAttentionBlock(nn.Module):\n    def __init__(self, d_model: int, n_head: int, act_layer: Callable = nn.GELU):\n        super().__init__()\n\n        self.attn = nn.MultiheadAttention(d_model, n_head)\n        self.ln_1 = LayerNorm(d_model)\n        self.mlp = nn.Sequential(\n            OrderedDict(\n                [\n                    (\"c_fc\", nn.Linear(d_model, d_model * 4)),\n                    (\"gelu\", act_layer()),\n                    (\"c_proj\", nn.Linear(d_model * 4, d_model)),\n                ]\n            )\n        )\n        self.ln_2 = LayerNorm(d_model)\n\n    def attention(self, x: torch.Tensor, attn_mask: Optional[torch.Tensor] = None):\n        return self.attn(x, x, x, need_weights=False, attn_mask=attn_mask)[0]\n\n    def forward(self, x: torch.Tensor, attn_mask: Optional[torch.Tensor] = None):\n        x = x + self.attention(self.ln_1(x), attn_mask=attn_mask)\n        x = x + self.mlp(self.ln_2(x))\n        return x\n\n\nclass Transformer(nn.Module):\n    def __init__(\n        self, width: int, layers: int, heads: int, act_layer: Callable = nn.GELU\n    ):\n        super().__init__()\n        self.width = width\n        self.layers = layers\n        self.resblocks = nn.ModuleList(\n            [\n                ResidualAttentionBlock(width, heads, act_layer=act_layer)\n                for _ in range(layers)\n            ]\n        )\n\n    def forward(self, x: torch.Tensor, attn_mask: Optional[torch.Tensor] = None):\n        for r in self.resblocks:\n            x = r(x, attn_mask=attn_mask)\n        return x\n\n\n@dataclass\nclass CLIPTextCfg:\n    context_length: int\n    vocab_size: int\n    width: int\n    heads: int\n    layers: int\n\n\n# @registry.register_model(\"clip\")\n# @registry.register_model(\"clip_feature_extractor\")\nclass CLIP(BaseModel):\n\n    def __init__(\n        self,\n        embed_dim: int,\n        graph_cfg: MetaHGTConvCfg,\n        text_cfg: CLIPTextCfg,\n        quick_gelu: bool = False,\n    ):\n        from .tokenizer import tokenize\n\n        super().__init__()\n\n        self.tokenizer = tokenize\n        self._loss = None\n\n        if isinstance(graph_cfg, dict):\n            graph_cfg = MetaHGTConvCfg(**graph_cfg)\n        if isinstance(text_cfg, dict):\n            text_cfg = CLIPTextCfg(**text_cfg)\n\n        self.context_length = text_cfg.context_length\n\n        # OpenAI models are pretrained w/ QuickGELU but native nn.GELU is both faster and more\n        # memory efficient in recent PyTorch releases (>= 1.10).\n        # NOTE: timm models always use native GELU regardless of quick_gelu flag.\n        act_layer = QuickGELU if quick_gelu else nn.GELU\n\n        self.graph_encoder = MetaHGTConv(\n            in_channels = graph_cfg.in_channels,\n            out_channels = graph_cfg.out_channels,\n            heads = graph_cfg.heads,\n            dynamic = graph_cfg.dynamic,\n            text_cfg = text_cfg, \n            layernorm = LayerNorm \n        )\n\n        self.transformer = Transformer(\n            width=text_cfg.width,\n            layers=text_cfg.layers,\n            heads=text_cfg.heads,\n            act_layer=act_layer,\n        )\n\n        self.vocab_size = text_cfg.vocab_size\n        self.token_embedding = nn.Embedding(text_cfg.vocab_size, text_cfg.width)\n        self.positional_embedding = nn.Parameter(\n            torch.empty(self.context_length, text_cfg.width)\n        )\n        self.ln_final = LayerNorm(text_cfg.width)\n\n        self.text_projection = nn.Parameter(torch.empty(text_cfg.width, embed_dim))\n        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))\n        self.register_buffer(\"attn_mask\", self.build_attention_mask(), persistent=False)\n\n        self.prompt_templates = openai_imagenet_template\n        self.classifier = None\n\n        self.init_parameters()\n\n    @property\n    def loss(self):\n        if self._loss is None:\n            from models.clip_models.loss import HeteClipLoss\n\n            self._loss = HeteClipLoss()\n\n        return self._loss\n\n    def init_parameters(self):\n        nn.init.normal_(self.token_embedding.weight, std=0.02)\n        nn.init.normal_(self.positional_embedding, std=0.01)\n        nn.init.constant_(self.logit_scale, np.log(1 / 0.07))\n\n\n        proj_std = (self.transformer.width**-0.5) * (\n            (2 * self.transformer.layers) ** -0.5\n        )\n        attn_std = self.transformer.width**-0.5\n        fc_std = (2 * self.transformer.width) ** -0.5\n        for block in self.transformer.resblocks:\n            nn.init.normal_(block.attn.in_proj_weight, std=attn_std)\n            nn.init.normal_(block.attn.out_proj.weight, std=proj_std)\n            nn.init.normal_(block.mlp.c_fc.weight, std=fc_std)\n            nn.init.normal_(block.mlp.c_proj.weight, std=proj_std)\n\n        if self.text_projection is not None:\n            nn.init.normal_(self.text_projection, std=self.transformer.width**-0.5)\n\n    def build_attention_mask(self):\n        # lazily create causal attention mask, with full attention between the vision tokens\n        # pytorch uses additive attention mask; fill with -inf\n        mask = torch.empty(self.context_length, self.context_length)\n        mask.fill_(float(\"-inf\"))\n        mask.triu_(1)  # zero out the lower diagonal\n        return mask\n\n    def lock_image_tower(self, unlocked_groups=0, freeze_bn_stats=False):\n        # lock image tower as per LiT - https://arxiv.org/abs/2111.07991\n        self.visual.lock(\n            unlocked_groups=unlocked_groups, freeze_bn_stats=freeze_bn_stats\n        )\n\n    def encode_graph(self, graph: List[Dict[str, torch.Tensor]], des_order: List[List[str]]):\n        graph_list = []\n        for graph_dict in graph: \n            if 'subject' in graph_dict.keys():\n                data_type = 'acm'\n            elif 'movie' in graph_dict.keys():\n                data_type = 'imdb'\n            elif 'paper' in graph_dict.keys():\n                data_type = 'dblp'\n                new_conf_reps = torch.ones(graph_dict['conference'].num_nodes, 768)\n                graph_dict['conference'].x = new_conf_reps\n            graph_list.append(self.graph_encoder(graph_dict.x_dict, graph_dict.edge_index_dict, data_type=data_type))\n        graph_embeds = []\n        assert len(graph_list) == len(des_order)\n        for idx, order in enumerate(des_order): \n            graph_embeds.extend([graph_list[idx][o] for o in order])\n        graph_embeds = torch.cat(graph_embeds, dim = 0)\n        return graph_embeds\n\n    def encode_text(self, text):\n        x = self.token_embedding(text)  # [batch_size, n_ctx, d_model]\n\n        x = x + self.positional_embedding\n        x = x.permute(1, 0, 2)  # NLD -> LND\n        x = self.transformer(x, attn_mask=self.attn_mask)\n        x = x.permute(1, 0, 2)  # LND -> NLD\n        x = self.ln_final(x)\n\n        # x.shape = [batch_size, n_ctx, transformer.width]\n        # take features from the eot embedding (eot_token is the highest number in each sequence)\n        x = x[torch.arange(x.shape[0]), text.argmax(dim=-1)] @ self.text_projection\n\n        return x\n\n    # def forward(self, image, text):\n    def forward(self, samples):\n        r'''\n        samples: \n        \"graph\": List[Dict],\n        \"text_input\": List[str],\n        \"des_order\": List[str], \n        \"graph_id\": graph_ids\n        '''\n        graph: List[Dict] = samples.get(\"graph\")\n        text: List[str] = samples.get(\"text_input\")\n        des_order: List[List[str]] = samples.get(\"des_order\")\n\n        if text is not None:\n            text = self.tokenizer(text, self.context_length).to(self.token_embedding.weight.device)\n\n        if graph is None:\n            return self.encode_text(text) # N, dim\n        elif text is None:\n            return self.encode_graph(graph, des_order)\n        graph_embeds = self.encode_graph(graph, des_order)\n        graph_features = F.normalize(graph_embeds, dim=-1)\n\n        text_embeds = self.encode_text(text)\n        text_features = F.normalize(text_embeds, dim=-1)\n        assert graph_features.shape == text_features.shape\n\n        loss = self.loss(graph_features, text_features, self.logit_scale.exp())\n\n        # return {\"loss\": loss}\n        return ClipOutput(\n            intermediate_output=HeteClipOutputFeatures(\n                graph_embeds=graph_embeds,\n                graph_embeds_proj=graph_features,\n                text_embeds=text_embeds,\n                text_embeds_proj=text_features,\n            ),\n            loss=loss,\n            logit_scale_exp=self.logit_scale.exp(),\n        )\n\n    def extract_features(self, samples):\n        \"\"\"\n        Extract features from the model for samples.\n\n        Keys allowed are \"image\" and \"text_input\" in samples.\n        If either key is missing, the corresponding features are not extracted.\n\n        Args:\n            samples: dict of samples to extract features from.\n\n        Returns:\n            ClipOutputFeatures object with features for the samples.\n        \"\"\"\n        graph: List[Dict] = samples.get(\"graph\")\n        text: List[str] = samples.get(\"text_input\")\n        des_order: List[List[str]] = samples.get(\"des_order\")\n\n        if text is not None:\n            text = self.tokenizer(text)\n\n        if graph is None:\n            return self.encode_text(text) # N, dim\n        elif text is None:\n            return self.encode_graph(graph, des_order)\n        graph_embeds = self.encode_graph(graph, des_order)\n        graph_features = F.normalize(graph_embeds, dim=-1)\n\n        text_embeds = self.encode_text(text)\n        text_features = F.normalize(text_embeds, dim=-1)\n        assert graph_features.shape == text_features.shape\n\n        return HeteClipOutputFeatures(\n                graph_embeds=graph_embeds,\n                graph_embeds_proj=graph_features,\n                text_embeds=text_embeds,\n                text_embeds_proj=text_features,\n            )\n\n    def predict(self, samples):\n        image = samples[\"image\"]\n        targets = samples[\"label\"]\n\n        image_features = self.encode_image(image)\n        image_features = F.normalize(image_features, dim=-1)\n\n        logits = 100.0 * image_features @ self.classifier\n\n        return {\"predictions\": logits, \"targets\": targets}\n\n    def before_evaluation(self, dataset, task_type, **kwargs):\n        if task_type == MultimodalClassificationTask:\n            self.classifier = self.zero_shot_classifier(\n                classnames=dataset.classnames,\n                templates=self.prompt_templates,\n            )\n\n    def zero_shot_classifier(self, classnames, templates):\n        with torch.no_grad():\n            zeroshot_weights = []\n            for classname in classnames:\n                texts = [\n                    template(classname) for template in templates\n                ]  # format with class\n                texts = self.tokenizer(texts).to(self.device)  # tokenize\n\n                class_embeddings = self.encode_text(texts)\n                class_embedding = F.normalize(class_embeddings, dim=-1).mean(dim=0)\n                class_embedding /= class_embedding.norm()\n                zeroshot_weights.append(class_embedding)\n            zeroshot_weights = torch.stack(zeroshot_weights, dim=1).to(self.device)\n        return zeroshot_weights\n\n    @classmethod\n    def default_config_path(cls, model_type=\"base\"):\n        model_type = \"ViT-B-32\" if model_type == \"base\" else model_type\n\n        assert (\n            model_type in cls.PRETRAINED_MODEL_CONFIG_DICT\n        ), \"Unknown model type {}. \\n Available types: {}\".format(\n            model_type, cls.PRETRAINED_MODEL_CONFIG_DICT.keys()\n        )\n        return get_abs_path(cls.PRETRAINED_MODEL_CONFIG_DICT[model_type])\n\n    @classmethod\n    def from_config(cls, cfg=None):\n        model_name = cfg.model_type\n        pretrained = cfg.pretrained\n\n        precision = cfg.get(\"precision\", \"fp32\")\n\n        return create_model(\n            model_name=model_name, pretrained=pretrained, precision=precision\n        )\n\n    def zero_shot_predict(self, image_path, categories):\n        assert isinstance(\n            categories, list\n        ), f\"categories must be a list, got {type(categories)}.\"\n        assert os.path.exists(image_path), f\"File {image_path} does not exist.\"\n\n        from lavis.processors.clip_processors import ClipImageEvalProcessor\n        from PIL import Image\n\n        image_preprocess = ClipImageEvalProcessor()\n        image = image_preprocess(Image.open(image_path)).unsqueeze(0)\n\n        text = self.tokenizer(categories)\n\n        with torch.no_grad():\n            image_features = self.encode_image(image)\n            text_features = self.encode_text(text)\n            image_features /= image_features.norm(dim=-1, keepdim=True)\n            text_features /= text_features.norm(dim=-1, keepdim=True)\n\n            text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n\n            print(\"Label probs:\", text_probs)  # prints: [[1., 0., 0.]]\n\n    def compute_sim_matrix(self, data_loader, **kwargs):\n        logging.info(\"Computing features for evaluation...\")\n        start_time = time.time()\n\n        texts = data_loader.dataset.text\n        num_text = len(texts)\n        text_bs = 256\n        text_features = []\n\n        for i in range(0, num_text, text_bs):\n\n            text = texts[i : min(num_text, i + text_bs)]\n            text_input = self.tokenizer(text).to(self.device)\n\n            text_feat = self.encode_text(text_input)\n            text_feat = F.normalize(text_feat, dim=-1)\n\n            text_features.append(text_feat)\n\n        text_features = torch.cat(text_features, dim=0)\n\n        image_features = []\n        for samples in data_loader:\n            image = samples[\"image\"]\n\n            image = image.to(self.device)\n            image_feat = self.encode_image(image)\n            image_feat = F.normalize(image_feat, dim=-1)\n\n            image_features.append(image_feat)\n\n        image_features = torch.cat(image_features, dim=0)\n\n        sims_matrix_i2t = image_features @ text_features.t()\n        sims_matrix_t2i = sims_matrix_i2t.t()\n\n        total_time = time.time() - start_time\n        total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n        logging.info(\"Evaluation time {}\".format(total_time_str))\n\n        return sims_matrix_i2t.cpu().numpy(), sims_matrix_t2i.cpu().numpy()\n\n\ndef convert_weights_to_fp16(model: nn.Module):\n    \"\"\"Convert applicable model parameters to fp16\"\"\"\n\n    def _convert_weights_to_fp16(l):\n        if isinstance(l, (nn.Conv1d, nn.Conv2d, nn.Linear)):\n            l.weight.data = l.weight.data.half()\n            if l.bias is not None:\n                l.bias.data = l.bias.data.half()\n\n        if isinstance(l, nn.MultiheadAttention):\n            for attr in [\n                *[f\"{s}_proj_weight\" for s in [\"in\", \"q\", \"k\", \"v\"]],\n                \"in_proj_bias\",\n                \"bias_k\",\n                \"bias_v\",\n            ]:\n                tensor = getattr(l, attr)\n                if tensor is not None:\n                    tensor.data = tensor.data.half()\n\n        for name in [\"text_projection\", \"proj\"]:\n            if hasattr(l, name):\n                attr = getattr(l, name)\n                if attr is not None:\n                    attr.data = attr.data.half()\n\n    model.apply(_convert_weights_to_fp16)\n\n\ndef build_model_from_openai_state_dict(state_dict: dict):\n    vit = \"visual.proj\" in state_dict\n\n    if vit:\n        vision_width = state_dict[\"visual.conv1.weight\"].shape[0]\n        vision_layers = len(\n            [\n                k\n                for k in state_dict.keys()\n                if k.startswith(\"visual.\") and k.endswith(\".attn.in_proj_weight\")\n            ]\n        )\n        vision_patch_size = state_dict[\"visual.conv1.weight\"].shape[-1]\n        grid_size = round(\n            (state_dict[\"visual.positional_embedding\"].shape[0] - 1) ** 0.5\n        )\n        image_size = vision_patch_size * grid_size\n    else:\n        counts: list = [\n            len(\n                set(\n                    k.split(\".\")[2]\n                    for k in state_dict\n                    if k.startswith(f\"visual.layer{b}\")\n                )\n            )\n            for b in [1, 2, 3, 4]\n        ]\n        vision_layers = tuple(counts)\n        vision_width = state_dict[\"visual.layer1.0.conv1.weight\"].shape[0]\n        output_width = round(\n            (state_dict[\"visual.attnpool.positional_embedding\"].shape[0] - 1) ** 0.5\n        )\n        vision_patch_size = None\n        assert (\n            output_width**2 + 1\n            == state_dict[\"visual.attnpool.positional_embedding\"].shape[0]\n        )\n        image_size = output_width * 32\n\n    embed_dim = state_dict[\"text_projection\"].shape[1]\n    context_length = state_dict[\"positional_embedding\"].shape[0]\n    vocab_size = state_dict[\"token_embedding.weight\"].shape[0]\n    transformer_width = state_dict[\"ln_final.weight\"].shape[0]\n    transformer_heads = transformer_width // 64\n    transformer_layers = len(\n        set(\n            k.split(\".\")[2]\n            for k in state_dict\n            if k.startswith(f\"transformer.resblocks\")\n        )\n    )\n\n    vision_cfg = CLIPVisionCfg(\n        layers=vision_layers,\n        width=vision_width,\n        patch_size=vision_patch_size,\n        image_size=image_size,\n    )\n    text_cfg = CLIPTextCfg(\n        context_length=context_length,\n        vocab_size=vocab_size,\n        width=transformer_width,\n        heads=transformer_heads,\n        layers=transformer_layers,\n    )\n    model = CLIP(\n        embed_dim,\n        vision_cfg=vision_cfg,\n        text_cfg=text_cfg,\n        quick_gelu=True,  # OpenAI models were trained with QuickGELU\n    )\n\n    for key in [\"input_resolution\", \"context_length\", \"vocab_size\"]:\n        state_dict.pop(key, None)\n\n    convert_weights_to_fp16(model)\n    model.load_state_dict(state_dict)\n    return model.eval()\n\n\ndef trace_model(model, batch_size=256, device=torch.device(\"cpu\")):\n    model.eval()\n    image_size = model.visual.image_size\n    example_images = torch.ones((batch_size, 3, image_size, image_size), device=device)\n    example_text = torch.zeros(\n        (batch_size, model.context_length), dtype=torch.int, device=device\n    )\n    model = torch.jit.trace_module(\n        model,\n        inputs=dict(\n            forward=(example_images, example_text),\n            encode_text=(example_text,),\n            encode_image=(example_images,),\n        ),\n    )\n    model.visual.image_size = image_size\n    return\n\n\ndef _natural_key(string_):\n    return [int(s) if s.isdigit() else s for s in re.split(r\"(\\d+)\", string_.lower())]\n\n\ndef _rescan_model_configs():\n    global _MODEL_CONFIGS\n\n    config_ext = (\".json\",)\n    config_files = []\n    for config_path in _MODEL_CONFIG_PATHS:\n        if config_path.is_file() and config_path.suffix in config_ext:\n            config_files.append(config_path)\n        elif config_path.is_dir():\n            for ext in config_ext:\n                config_files.extend(config_path.glob(f\"*{ext}\"))\n\n    for cf in config_files:\n        with open(cf, \"r\") as f:\n            model_cfg = json.load(f)\n            if all(a in model_cfg for a in (\"embed_dim\", \"vision_cfg\", \"text_cfg\")):\n                _MODEL_CONFIGS[cf.stem] = model_cfg\n\n    _MODEL_CONFIGS = {\n        k: v\n        for k, v in sorted(_MODEL_CONFIGS.items(), key=lambda x: _natural_key(x[0]))\n    }\n\n\n_rescan_model_configs()  # initial populate of model config registry\n\n\ndef load_state_dict(checkpoint_path: str, map_location=\"cpu\"):\n    checkpoint = torch.load(checkpoint_path, map_location=map_location)\n    if isinstance(checkpoint, dict) and \"state_dict\" in checkpoint:\n        state_dict = checkpoint[\"state_dict\"]\n    else:\n        state_dict = checkpoint\n    if next(iter(state_dict.items()))[0].startswith(\"module\"):\n        state_dict = {k[7:]: v for k, v in state_dict.items()}\n    return state_dict\n\n\ndef create_model(\n    model_name: str,\n    pretrained: str = \"\",\n    precision: str = \"fp32\",\n    device: torch.device = torch.device(\"cpu\"),\n    jit: bool = False,\n    force_quick_gelu: bool = False,\n    pretrained_image: bool = False,\n):\n    model_name = model_name.replace(\n        \"/\", \"-\"\n    )  # for callers using old naming with / in ViT names\n\n    if pretrained.lower() == \"openai\":\n        logging.info(f\"Loading pretrained {model_name} from OpenAI.\")\n        model = load_openai_model(model_name, device=device, jit=jit)\n        # See https://discuss.pytorch.org/t/valueerror-attemting-to-unscale-fp16-gradients/81372\n        if precision == \"amp\" or precision == \"fp32\":\n            model = model.float()\n    else:\n        logging.info(f\"No pretrained weights loaded for {model_name} model.\")\n        if model_name in _MODEL_CONFIGS:\n            logging.info(f\"Loading {model_name} model config.\")\n            model_cfg = deepcopy(_MODEL_CONFIGS[model_name])\n        else:\n            logging.error(\n                f\"Model config for {model_name} not found; available models {list_models()}.\"\n            )\n            raise RuntimeError(f\"Model config for {model_name} not found.\")\n\n        if force_quick_gelu:\n            # override for use of QuickGELU on non-OpenAI transformer models\n            model_cfg[\"quick_gelu\"] = True\n\n        if pretrained_image:\n            if \"timm_model_name\" in model_cfg.get(\"vision_cfg\", {}):\n                # pretrained weight loading for timm models set via vision_cfg\n                model_cfg[\"vision_cfg\"][\"timm_model_pretrained\"] = True\n            else:\n                assert (\n                    False\n                ), \"pretrained image towers currently only supported for timm models\"\n\n        model = CLIP(**model_cfg)\n\n        if pretrained:\n            checkpoint_path = \"\"\n            url = get_pretrained_url(model_name, pretrained)\n            if url:\n                checkpoint_path = download_pretrained(url)\n            elif os.path.exists(pretrained):\n                checkpoint_path = pretrained\n\n            if checkpoint_path:\n                logging.info(f\"Loading pretrained {model_name} weights ({pretrained}).\")\n                model.load_state_dict(load_state_dict(checkpoint_path))\n            else:\n                logging.warning(\n                    f\"Pretrained weights ({pretrained}) not found for model {model_name}.\"\n                )\n                raise RuntimeError(\n                    f\"Pretrained weights ({pretrained}) not found for model {model_name}.\"\n                )\n\n        model.to(device=device)\n        if precision == \"fp16\":\n            assert device.type != \"cpu\"\n            convert_weights_to_fp16(model)\n\n        if jit:\n            model = torch.jit.script(model)\n\n    return model\n\n\ndef create_model_and_transforms(\n    model_name: str,\n    pretrained: str = \"\",\n    precision: str = \"fp32\",\n    device: torch.device = torch.device(\"cpu\"),\n    jit: bool = False,\n    force_quick_gelu: bool = False,\n    pretrained_image: bool = False,\n):\n    model = create_model(\n        model_name,\n        pretrained,\n        precision,\n        device,\n        jit,\n        force_quick_gelu=force_quick_gelu,\n        pretrained_image=pretrained_image,\n    )\n    preprocess_train = image_transform(model.visual.image_size, is_train=True)\n    preprocess_val = image_transform(model.visual.image_size, is_train=False)\n    return model, preprocess_train, preprocess_val\n\n\ndef list_models():\n    \"\"\"enumerate available model architectures based on config files\"\"\"\n    return list(_MODEL_CONFIGS.keys())\n\n\ndef add_model_config(path):\n    \"\"\"add model config path or file and update registry\"\"\"\n    if not isinstance(path, Path):\n        path = Path(path)\n    _MODEL_CONFIG_PATHS.append(path)\n    _rescan_model_configs()\n\n\ndef list_openai_models() -> List[str]:\n    \"\"\"Returns the names of available CLIP models\"\"\"\n    return list_pretrained_tag_models(\"openai\")\n\n\ndef load_openai_model(\n    name: str,\n    device: Union[str, torch.device] = \"cuda\" if torch.cuda.is_available() else \"cpu\",\n    jit=True,\n):\n    \"\"\"Load a CLIP model\n    Parameters\n    ----------\n    name : str\n        A model name listed by `clip.available_models()`, or the path to a model checkpoint containing the state_dict\n    device : Union[str, torch.device]\n        The device to put the loaded model\n    jit : bool\n        Whether to load the optimized JIT model (default) or more hackable non-JIT model.\n    Returns\n    -------\n    model : torch.nn.Module\n        The CLIP model\n    preprocess : Callable[[PIL.Image], torch.Tensor]\n        A torchvision transform that converts a PIL image into a tensor that the returned model can take as its input\n    \"\"\"\n    if get_pretrained_url(name, \"openai\"):\n        model_path = download_pretrained(get_pretrained_url(name, \"openai\"))\n    elif os.path.isfile(name):\n        model_path = name\n    else:\n        raise RuntimeError(\n            f\"Model {name} not found; available models = {list_openai_models()}\"\n        )\n\n    try:\n        # loading JIT archive\n        model = torch.jit.load(model_path, map_location=device if jit else \"cpu\").eval()\n        state_dict = None\n    except RuntimeError:\n        # loading saved state dict\n        if jit:\n            warnings.warn(\n                f\"File {model_path} is not a JIT archive. Loading as a state dict instead\"\n            )\n            jit = False\n        state_dict = torch.load(model_path, map_location=\"cpu\")\n\n    if not jit:\n        try:\n            model = build_model_from_openai_state_dict(\n                state_dict or model.state_dict()\n            ).to(device)\n        except KeyError:\n            sd = {k[7:]: v for k, v in state_dict[\"state_dict\"].items()}\n            model = build_model_from_openai_state_dict(sd).to(device)\n\n        if str(device) == \"cpu\":\n            model.float()\n        return model\n\n    # patch the device names\n    device_holder = torch.jit.trace(\n        lambda: torch.ones([]).to(torch.device(device)), example_inputs=[]\n    )\n    device_node = [\n        n\n        for n in device_holder.graph.findAllNodes(\"prim::Constant\")\n        if \"Device\" in repr(n)\n    ][-1]\n\n    def patch_device(module):\n        try:\n            graphs = [module.graph] if hasattr(module, \"graph\") else []\n        except RuntimeError:\n            graphs = []\n\n        if hasattr(module, \"forward1\"):\n            graphs.append(module.forward1.graph)\n\n        for graph in graphs:\n            for node in graph.findAllNodes(\"prim::Constant\"):\n                if \"value\" in node.attributeNames() and str(node[\"value\"]).startswith(\n                    \"cuda\"\n                ):\n                    node.copyAttributes(device_node)\n\n    model.apply(patch_device)\n    patch_device(model.encode_image)\n    patch_device(model.encode_text)\n\n    # patch dtype to float32 on CPU\n    if str(device) == \"cpu\":\n        float_holder = torch.jit.trace(\n            lambda: torch.ones([]).float(), example_inputs=[]\n        )\n        float_input = list(float_holder.graph.findNode(\"aten::to\").inputs())[1]\n        float_node = float_input.node()\n\n        def patch_float(module):\n            try:\n                graphs = [module.graph] if hasattr(module, \"graph\") else []\n            except RuntimeError:\n                graphs = []\n\n            if hasattr(module, \"forward1\"):\n                graphs.append(module.forward1.graph)\n\n            for graph in graphs:\n                for node in graph.findAllNodes(\"aten::to\"):\n                    inputs = list(node.inputs())\n                    for i in [\n                        1,\n                        2,\n                    ]:  # dtype can be the second or third argument to aten::to()\n                        if inputs[i].node()[\"value\"] == 5:\n                            inputs[i].node().copyAttributes(float_node)\n\n        model.apply(patch_float)\n        patch_float(model.encode_image)\n        patch_float(model.encode_text)\n        model.float()\n\n    # ensure image_size attr available at consistent location for both jit and non-jit\n    model.visual.image_size = model.input_resolution.item()\n    return model\n\n\nopenai_imagenet_template = [\n    lambda c: f\"a bad photo of a {c}.\",\n    lambda c: f\"a photo of many {c}.\",\n    lambda c: f\"a sculpture of a {c}.\",\n    lambda c: f\"a photo of the hard to see {c}.\",\n    lambda c: f\"a low resolution photo of the {c}.\",\n    lambda c: f\"a rendering of a {c}.\",\n    lambda c: f\"graffiti of a {c}.\",\n    lambda c: f\"a bad photo of the {c}.\",\n    lambda c: f\"a cropped photo of the {c}.\",\n    lambda c: f\"a tattoo of a {c}.\",\n    lambda c: f\"the embroidered {c}.\",\n    lambda c: f\"a photo of a hard to see {c}.\",\n    lambda c: f\"a bright photo of a {c}.\",\n    lambda c: f\"a photo of a clean {c}.\",\n    lambda c: f\"a photo of a dirty {c}.\",\n    lambda c: f\"a dark photo of the {c}.\",\n    lambda c: f\"a drawing of a {c}.\",\n    lambda c: f\"a photo of my {c}.\",\n    lambda c: f\"the plastic {c}.\",\n    lambda c: f\"a photo of the cool {c}.\",\n    lambda c: f\"a close-up photo of a {c}.\",\n    lambda c: f\"a black and white photo of the {c}.\",\n    lambda c: f\"a painting of the {c}.\",\n    lambda c: f\"a painting of a {c}.\",\n    lambda c: f\"a pixelated photo of the {c}.\",\n    lambda c: f\"a sculpture of the {c}.\",\n    lambda c: f\"a bright photo of the {c}.\",\n    lambda c: f\"a cropped photo of a {c}.\",\n    lambda c: f\"a plastic {c}.\",\n    lambda c: f\"a photo of the dirty {c}.\",\n    lambda c: f\"a jpeg corrupted photo of a {c}.\",\n    lambda c: f\"a blurry photo of the {c}.\",\n    lambda c: f\"a photo of the {c}.\",\n    lambda c: f\"a good photo of the {c}.\",\n    lambda c: f\"a rendering of the {c}.\",\n    lambda c: f\"a {c} in a video game.\",\n    lambda c: f\"a photo of one {c}.\",\n    lambda c: f\"a doodle of a {c}.\",\n    lambda c: f\"a close-up photo of the {c}.\",\n    lambda c: f\"a photo of a {c}.\",\n    lambda c: f\"the origami {c}.\",\n    lambda c: f\"the {c} in a video game.\",\n    lambda c: f\"a sketch of a {c}.\",\n    lambda c: f\"a doodle of the {c}.\",\n    lambda c: f\"a origami {c}.\",\n    lambda c: f\"a low resolution photo of a {c}.\",\n    lambda c: f\"the toy {c}.\",\n    lambda c: f\"a rendition of the {c}.\",\n    lambda c: f\"a photo of the clean {c}.\",\n    lambda c: f\"a photo of a large {c}.\",\n    lambda c: f\"a rendition of a {c}.\",\n    lambda c: f\"a photo of a nice {c}.\",\n    lambda c: f\"a photo of a weird {c}.\",\n    lambda c: f\"a blurry photo of a {c}.\",\n    lambda c: f\"a cartoon {c}.\",\n    lambda c: f\"art of a {c}.\",\n    lambda c: f\"a sketch of the {c}.\",\n    lambda c: f\"a embroidered {c}.\",\n    lambda c: f\"a pixelated photo of a {c}.\",\n    lambda c: f\"itap of the {c}.\",\n    lambda c: f\"a jpeg corrupted photo of the {c}.\",\n    lambda c: f\"a good photo of a {c}.\",\n    lambda c: f\"a plushie {c}.\",\n    lambda c: f\"a photo of the nice {c}.\",\n    lambda c: f\"a photo of the small {c}.\",\n    lambda c: f\"a photo of the weird {c}.\",\n    lambda c: f\"the cartoon {c}.\",\n    lambda c: f\"art of the {c}.\",\n    lambda c: f\"a drawing of the {c}.\",\n    lambda c: f\"a photo of the large {c}.\",\n    lambda c: f\"a black and white photo of a {c}.\",\n    lambda c: f\"the plushie {c}.\",\n    lambda c: f\"a dark photo of a {c}.\",\n    lambda c: f\"itap of a {c}.\",\n    lambda c: f\"graffiti of the {c}.\",\n    lambda c: f\"a toy {c}.\",\n    lambda c: f\"itap of my {c}.\",\n    lambda c: f\"a photo of a cool {c}.\",\n    lambda c: f\"a photo of a small {c}.\",\n    lambda c: f\"a tattoo of the {c}.\",\n]\n"}
{"type": "source_file", "path": "dist_utils.py", "content": "\"\"\"\n Copyright (c) 2022, salesforce.com, inc.\n All rights reserved.\n SPDX-License-Identifier: BSD-3-Clause\n For full license text, see the LICENSE file in the repo root or https://opensource.org/licenses/BSD-3-Clause\n\"\"\"\n\nimport datetime\nimport functools\nimport os\n\nimport torch\nimport torch.distributed as dist\nimport timm.models.hub as timm_hub\n\n\ndef setup_for_distributed(is_master):\n    \"\"\"\n    This function disables printing when not in master process\n    \"\"\"\n    import builtins as __builtin__\n\n    builtin_print = __builtin__.print\n\n    def print(*args, **kwargs):\n        force = kwargs.pop(\"force\", False)\n        if is_master or force:\n            builtin_print(*args, **kwargs)\n\n    __builtin__.print = print\n\n\ndef is_dist_avail_and_initialized():\n    if not dist.is_available():\n        return False\n    if not dist.is_initialized():\n        return False\n    return True\n\n\ndef get_world_size():\n    if not is_dist_avail_and_initialized():\n        return 1\n    return dist.get_world_size()\n\n\ndef get_rank():\n    if not is_dist_avail_and_initialized():\n        return 0\n    return dist.get_rank()\n\n\ndef is_main_process():\n    return get_rank() == 0\n\n\ndef init_distributed_mode(args):\n    if \"RANK\" in os.environ and \"WORLD_SIZE\" in os.environ:\n        args.rank = int(os.environ[\"RANK\"])\n        args.world_size = int(os.environ[\"WORLD_SIZE\"])\n        args.gpu = int(os.environ[\"LOCAL_RANK\"])\n    elif \"SLURM_PROCID\" in os.environ:\n        args.rank = int(os.environ[\"SLURM_PROCID\"])\n        args.gpu = args.rank % torch.cuda.device_count()\n    else:\n        print(\"Not using distributed mode\")\n        args.distributed = False\n        return\n\n    args.distributed = True\n\n    torch.cuda.set_device(args.gpu)\n    args.dist_backend = \"nccl\"\n    print(\n        \"| distributed init (rank {}, world {}): {}\".format(\n            args.rank, args.world_size, args.dist_url\n        ),\n        flush=True,\n    )\n    torch.distributed.init_process_group(\n        backend=args.dist_backend,\n        init_method=args.dist_url,\n        world_size=args.world_size,\n        rank=args.rank,\n        timeout=datetime.timedelta(\n            days=365\n        ),  # allow auto-downloading and de-compressing\n    )\n    torch.distributed.barrier()\n    setup_for_distributed(args.rank == 0)\n\n\ndef get_dist_info():\n    if torch.__version__ < \"1.0\":\n        initialized = dist._initialized\n    else:\n        initialized = dist.is_initialized()\n    if initialized:\n        rank = dist.get_rank()\n        world_size = dist.get_world_size()\n    else:  # non-distributed training\n        rank = 0\n        world_size = 1\n    return rank, world_size\n\n\ndef main_process(func):\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        rank, _ = get_dist_info()\n        if rank == 0:\n            return func(*args, **kwargs)\n\n    return wrapper\n\n\ndef download_cached_file(url, check_hash=True, progress=False):\n    \"\"\"\n    Download a file from a URL and cache it locally. If the file already exists, it is not downloaded again.\n    If distributed, only the main process downloads the file, and the other processes wait for the file to be downloaded.\n    \"\"\"\n\n    def get_cached_file_path():\n        # a hack to sync the file path across processes\n        parts = torch.hub.urlparse(url)\n        filename = os.path.basename(parts.path)\n        cached_file = os.path.join(timm_hub.get_cache_dir(), filename)\n\n        return cached_file\n\n    if is_main_process():\n        timm_hub.download_cached_file(url, check_hash, progress)\n\n    if is_dist_avail_and_initialized():\n        dist.barrier()\n\n    return get_cached_file_path()\n"}
{"type": "source_file", "path": "higpt/conversation.py", "content": "import dataclasses\nfrom enum import auto, Enum\nfrom typing import List, Tuple\n\n\nclass SeparatorStyle(Enum):\n    \"\"\"Different separator style.\"\"\"\n    SINGLE = auto()\n    TWO = auto()\n    MPT = auto()\n\n\n@dataclasses.dataclass\nclass Conversation:\n    \"\"\"A class that keeps all conversation history.\"\"\"\n    system: str\n    roles: List[str]\n    messages: List[List[str]]\n    offset: int\n    sep_style: SeparatorStyle = SeparatorStyle.SINGLE\n    sep: str = \"###\"\n    sep2: str = None\n    version: str = \"Unknown\"\n\n    skip_next: bool = False\n\n    def get_prompt(self):\n        if self.sep_style == SeparatorStyle.SINGLE:\n            ret = self.system + self.sep\n            for role, message in self.messages:\n                if message:\n                    if type(message) is tuple:\n                        message, _, _ = message\n                    ret += role + \": \" + message + self.sep\n                else:\n                    ret += role + \":\"\n            return ret\n        elif self.sep_style == SeparatorStyle.TWO:\n            seps = [self.sep, self.sep2]\n            ret = self.system + seps[0]\n            for i, (role, message) in enumerate(self.messages):\n                if message:\n                    if type(message) is tuple:\n                        message, _, _ = message\n                    ret += role + \": \" + message + seps[i % 2]\n                else:\n                    ret += role + \":\"\n            return ret\n        if self.sep_style == SeparatorStyle.MPT:\n            ret = self.system + self.sep\n            for role, message in self.messages:\n                if message:\n                    if type(message) is tuple:\n                        message, _, _ = message\n                    ret += role + message + self.sep\n                else:\n                    ret += role\n            return ret\n        else:\n            raise ValueError(f\"Invalid style: {self.sep_style}\")\n\n    def append_message(self, role, message):\n        self.messages.append([role, message])\n\n    def get_images(self, return_pil=False):\n        images = []\n        for i, (role, msg) in enumerate(self.messages[self.offset:]):\n            if i % 2 == 0:\n                if type(msg) is tuple:\n                    import base64\n                    from io import BytesIO\n                    from PIL import Image\n                    msg, image, image_process_mode = msg\n                    if image_process_mode == \"Pad\":\n                        def expand2square(pil_img, background_color=(122, 116, 104)):\n                            width, height = pil_img.size\n                            if width == height:\n                                return pil_img\n                            elif width > height:\n                                result = Image.new(pil_img.mode, (width, width), background_color)\n                                result.paste(pil_img, (0, (width - height) // 2))\n                                return result\n                            else:\n                                result = Image.new(pil_img.mode, (height, height), background_color)\n                                result.paste(pil_img, ((height - width) // 2, 0))\n                                return result\n                        image = expand2square(image)\n                    elif image_process_mode == \"Crop\":\n                        pass\n                    elif image_process_mode == \"Resize\":\n                        image = image.resize((224, 224))\n                    else:\n                        raise ValueError(f\"Invalid image_process_mode: {image_process_mode}\")\n                    max_hw, min_hw = max(image.size), min(image.size)\n                    aspect_ratio = max_hw / min_hw\n                    max_len, min_len = 800, 400\n                    shortest_edge = int(min(max_len / aspect_ratio, min_len, min_hw))\n                    longest_edge = int(shortest_edge * aspect_ratio)\n                    W, H = image.size\n                    if H > W:\n                        H, W = longest_edge, shortest_edge\n                    else:\n                        H, W = shortest_edge, longest_edge\n                    image = image.resize((W, H))\n                    if return_pil:\n                        images.append(image)\n                    else:\n                        buffered = BytesIO()\n                        image.save(buffered, format=\"JPEG\")\n                        img_b64_str = base64.b64encode(buffered.getvalue()).decode()\n                        images.append(img_b64_str)\n        return images\n\n    def to_gradio_chatbot(self):\n        ret = []\n        for i, (role, msg) in enumerate(self.messages[self.offset:]):\n            if i % 2 == 0:\n                if type(msg) is tuple:\n                    import base64\n                    from io import BytesIO\n                    msg, image, image_process_mode = msg\n                    max_hw, min_hw = max(image.size), min(image.size)\n                    aspect_ratio = max_hw / min_hw\n                    max_len, min_len = 800, 400\n                    shortest_edge = int(min(max_len / aspect_ratio, min_len, min_hw))\n                    longest_edge = int(shortest_edge * aspect_ratio)\n                    W, H = image.size\n                    if H > W:\n                        H, W = longest_edge, shortest_edge\n                    else:\n                        H, W = shortest_edge, longest_edge\n                    image = image.resize((W, H))\n                    # image = image.resize((224, 224))\n                    buffered = BytesIO()\n                    image.save(buffered, format=\"JPEG\")\n                    img_b64_str = base64.b64encode(buffered.getvalue()).decode()\n                    img_str = f'<img src=\"data:image/png;base64,{img_b64_str}\" alt=\"user upload image\" />'\n                    msg = msg.replace('<image>', img_str)\n                ret.append([msg, None])\n            else:\n                ret[-1][-1] = msg\n        return ret\n\n    def copy(self):\n        return Conversation(\n            system=self.system,\n            roles=self.roles,\n            messages=[[x, y] for x, y in self.messages],\n            offset=self.offset,\n            sep_style=self.sep_style,\n            sep=self.sep,\n            sep2=self.sep2)\n\n    def dict(self):\n        if len(self.get_images()) > 0:\n            return {\n                \"system\": self.system,\n                \"roles\": self.roles,\n                \"messages\": [[x, y[0] if type(y) is tuple else y] for x, y in self.messages],\n                \"offset\": self.offset,\n                \"sep\": self.sep,\n                \"sep2\": self.sep2,\n            }\n        return {\n            \"system\": self.system,\n            \"roles\": self.roles,\n            \"messages\": self.messages,\n            \"offset\": self.offset,\n            \"sep\": self.sep,\n            \"sep2\": self.sep2,\n        }\n\n\nconv_v1 = Conversation(\n    system=\"A chat between a curious human and an artificial intelligence assistant. \"\n           \"The assistant gives helpful, detailed, and polite answers to the human's questions.\",\n    roles=(\"Human\", \"Assistant\"),\n    messages=(\n        (\"Human\", \"Give three tips for staying healthy.\"),\n        (\"Assistant\",\n            \"Sure, here are three tips for staying healthy:\\n\"\n            \"1. Exercise regularly: Regular physical activity can help improve your overall health and wellbeing. \"\n            \"It can also help reduce your risk of chronic conditions such as obesity, diabetes, heart disease, \"\n            \"and certain cancers. Aim for at least 150 minutes of moderate-intensity aerobic exercise or \"\n            \"75 minutes of vigorous-intensity aerobic exercise per week, along with muscle-strengthening \"\n            \"activities at least two days per week.\\n\"\n            \"2. Eat a balanced diet: Eating a balanced diet that is rich in fruits, \"\n            \"vegetables, whole grains, lean proteins, and healthy fats can help support \"\n            \"your overall health. Try to limit your intake of processed and high-sugar foods, \"\n            \"and aim to drink plenty of water throughout the day.\\n\"\n            \"3. Get enough sleep: Getting enough quality sleep is essential for your physical \"\n            \"and mental health. Adults should aim for seven to nine hours of sleep per night. \"\n            \"Establish a regular sleep schedule and try to create a relaxing bedtime routine to \"\n            \"help improve the quality of your sleep.\")\n    ),\n    offset=2,\n    sep_style=SeparatorStyle.SINGLE,\n    sep=\"###\",\n)\n\nconv_v1_2 = Conversation(\n    system=\"A chat between a curious human and an artificial intelligence assistant. \"\n           \"The assistant gives helpful, detailed, and polite answers to the human's questions.\",\n    roles=(\"Human\", \"Assistant\"),\n    messages=(\n        (\"Human\", \"What are the key differences between renewable and non-renewable energy sources?\"),\n        (\"Assistant\",\n            \"Renewable energy sources are those that can be replenished naturally in a relatively \"\n            \"short amount of time, such as solar, wind, hydro, geothermal, and biomass. \"\n            \"Non-renewable energy sources, on the other hand, are finite and will eventually be \"\n            \"depleted, such as coal, oil, and natural gas. Here are some key differences between \"\n            \"renewable and non-renewable energy sources:\\n\"\n            \"1. Availability: Renewable energy sources are virtually inexhaustible, while non-renewable \"\n            \"energy sources are finite and will eventually run out.\\n\"\n            \"2. Environmental impact: Renewable energy sources have a much lower environmental impact \"\n            \"than non-renewable sources, which can lead to air and water pollution, greenhouse gas emissions, \"\n            \"and other negative effects.\\n\"\n            \"3. Cost: Renewable energy sources can be more expensive to initially set up, but they typically \"\n            \"have lower operational costs than non-renewable sources.\\n\"\n            \"4. Reliability: Renewable energy sources are often more reliable and can be used in more remote \"\n            \"locations than non-renewable sources.\\n\"\n            \"5. Flexibility: Renewable energy sources are often more flexible and can be adapted to different \"\n            \"situations and needs, while non-renewable sources are more rigid and inflexible.\\n\"\n            \"6. Sustainability: Renewable energy sources are more sustainable over the long term, while \"\n            \"non-renewable sources are not, and their depletion can lead to economic and social instability.\\n\")\n    ),\n    offset=2,\n    sep_style=SeparatorStyle.SINGLE,\n    sep=\"###\",\n)\n\nconv_vicuna_v1_1 = Conversation(\n    system=\"A chat between a curious user and an artificial intelligence assistant. \"\n    \"The assistant gives helpful, detailed, and polite answers to the user's questions.\",\n    roles=(\"USER\", \"ASSISTANT\"),\n    version=\"v1\",\n    messages=(),\n    offset=0,\n    sep_style=SeparatorStyle.TWO,\n    sep=\" \",\n    sep2=\"</s>\",\n)\n\nconv_mpt = Conversation(\n    system=\"\"\"<|im_start|>system\n- You are a helpful language and vision assistant.\n- You are able to understand the visual content that the user provides, and assist the user with a variety of tasks using natural language.\n- You should follow the instructions carefully and explain your answers in detail.\"\"\",\n    roles=(\"<|im_start|>user\\n\", \"<|im_start|>assistant\\n\"),\n    version=\"mpt\",\n    messages=(),\n    offset=0,\n    sep_style=SeparatorStyle.MPT,\n    sep=\"<|im_end|>\",\n)\n\nconv_mpt_text = Conversation(\n    system=\"\"\"<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.\"\"\",\n    roles=(\"<|im_start|>user\\n\", \"<|im_start|>assistant\\n\"),\n    version=\"mpt\",\n    messages=(),\n    offset=0,\n    sep_style=SeparatorStyle.MPT,\n    sep=\"<|im_end|>\",\n)\n\nconv_bair_v1 = Conversation(\n    system=\"BEGINNING OF CONVERSATION:\",\n    roles=(\"USER\", \"GPT\"),\n    messages=(),\n    offset=0,\n    sep_style=SeparatorStyle.TWO,\n    sep=\" \",\n    sep2=\"</s>\",\n)\n\nsimple_conv = Conversation(\n    system=\"A chat between a curious human and an artificial intelligence assistant. \"\n           \"The assistant gives helpful, detailed, and polite answers to the human's questions.\",\n    roles=(\"Human\", \"Assistant\"),\n    messages=(\n        (\"Human\", \"Hi!\"),\n        (\"Assistant\", \"Hi there! How can I help you today?\")\n    ),\n    offset=2,\n    sep_style=SeparatorStyle.SINGLE,\n    sep=\"###\",\n)\n\nsimple_conv_multimodal = Conversation(\n    system=\"You are LLaVA, a large language and vision assistant trained by UW Madison WAIV Lab.\"\n           \"You are able to understand the visual content that the user provides, and assist the user with a variety of tasks using natural language.\"\n           \"Follow the instructions carefully and explain your answers in detail.\",\n    roles=(\"Human\", \"Assistant\"),\n    messages=(\n        (\"Human\", \"Hi!\"),\n        (\"Assistant\", \"Hi there!  How can I help you today?\\n\")\n    ),\n    offset=2,\n    sep_style=SeparatorStyle.SINGLE,\n    sep=\"###\",\n)\n\nsimple_conv_mpt_multimodal = Conversation(\n    system=\"\"\"<|im_start|>system\n- You are LLaVA, a large language and vision assistant trained by UW Madison WAIV Lab.\n- You are able to understand the visual content that the user provides, and assist the user with a variety of tasks using natural language.\n- You should follow the instructions carefully and explain your answers in detail.\"\"\",\n    roles=(\"<|im_start|>user\\n\", \"<|im_start|>assistant\\n\"),\n    version=\"mpt\",\n    messages=(),\n    offset=0,\n    sep_style=SeparatorStyle.MPT,\n    sep=\"<|im_end|>\",\n)\n\nsimple_conv_legacy = Conversation(\n    system=\"You are LLaVA, a large language model trained by UW Madison WAIV Lab.\"\n           \"You are designed to assist human with a variety of tasks using natural language.\"\n           \"Follow the instructions carefully.\",\n    roles=(\"Human\", \"Assistant\"),\n    messages=(\n        (\"Human\", \"Hi!\\n\\n### Response:\"),\n        (\"Assistant\", \"Hi there!  How can I help you today?\\n\")\n    ),\n    offset=2,\n    sep_style=SeparatorStyle.SINGLE,\n    sep=\"###\",\n)\n\nconv_llava_v1 = Conversation(\n    system=\"You are LLaVA, a large language and vision assistant trained by UW Madison WAIV Lab.\"\n           \"You are able to understand the visual content that the user provides, and assist the user with a variety of tasks using natural language.\"\n           \"Follow the instructions carefully and explain your answers in detail.\",\n    roles=(\"USER\", \"ASSISTANT\"),\n    version=\"v1\",\n    messages=(),\n    offset=0,\n    sep_style=SeparatorStyle.TWO,\n    sep=\" \",\n    sep2=\"</s>\",\n)\n\nconv_graphchat_v1 = Conversation(\n    system=\"You are GraphChat, a large language and graph-structral assistant trained by HKUDS Lab.\"\n           \"You are able to understand the graph structures that the user provides, and assist the user with a variety of tasks using natural language.\"\n           \"Follow the instructions carefully and explain your answers in detail.\",\n    roles=(\"USER\", \"ASSISTANT\"),\n    version=\"v1\",\n    messages=(),\n    offset=0,\n    sep_style=SeparatorStyle.TWO,\n    sep=\" \",\n    sep2=\"</s>\",\n)\n\ndefault_conversation = conv_v1_2\nconv_templates = {\n    \"default\": conv_v1_2,\n    \"simple\": simple_conv,\n    \"simple_legacy\": simple_conv_legacy,\n    \"multimodal\": simple_conv_multimodal,\n    \"mpt_multimodal\": simple_conv_mpt_multimodal,\n    \"llava_v1\": conv_llava_v1, \n    \"graphchat_v1\": conv_graphchat_v1, \n\n\n    # fastchat\n    \"v1\": conv_v1_2,\n    \"bair_v1\": conv_bair_v1,\n    \"vicuna_v1_1\": conv_vicuna_v1_1,\n    \"mpt\": conv_mpt,\n    \"mpt_text\": conv_mpt_text,\n}\n\n\nif __name__ == \"__main__\":\n    print(default_conversation.get_prompt())\n"}
{"type": "source_file", "path": "higpt/eval/run_higpt.py", "content": "import argparse\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\nimport os\nfrom higpt.conversation import conv_templates, SeparatorStyle\nfrom higpt.utils import disable_torch_init\nfrom transformers import CLIPVisionModel, CLIPImageProcessor, StoppingCriteria\nfrom higpt.model import *\nfrom higpt.model.utils import KeywordsStoppingCriteria\nfrom torch_geometric.data import Data\nimport json\nimport copy\nfrom higpt.model.meta_hgt import MetaHGTConvCfg, MetaHGTConv\n\n\nimport os\nimport requests\nfrom PIL import Image\nfrom io import BytesIO\n\nfrom tqdm import tqdm\nimport json\nimport os.path as osp\n\nimport ray\n\n# os.environ['PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION'] = 'python'\n\nDEFAULT_GRAPH_TOKEN = \"<graph>\"\nDEFAULT_GRAPH_PATCH_TOKEN = \"<g_patch>\"\nDEFAULT_G_START_TOKEN = \"<g_start>\"\nDEFAULT_G_END_TOKEN = \"<g_end>\"\n\nnode_feas_dict_dblp = torch.load('./higpt/model/meta_hgt/meta_dict/dblp/node_type.pt')\nfor k in node_feas_dict_dblp.keys():\n    node_feas_dict_dblp[k] = torch.Tensor(node_feas_dict_dblp[k])\nedge_feas_dict_dblp = torch.load('./higpt/model/meta_hgt/meta_dict/dblp/edge_type.pt')\nfor k in edge_feas_dict_dblp.keys():\n    edge_feas_dict_dblp[k] = torch.Tensor(edge_feas_dict_dblp[k])\n\nnode_feas_dict_acm = torch.load('./higpt/model/meta_hgt/meta_dict/acm/node_type.pt')\nfor k in node_feas_dict_acm.keys():\n    node_feas_dict_acm[k] = torch.Tensor(node_feas_dict_acm[k])\nedge_feas_dict_acm = torch.load('./higpt/model/meta_hgt/meta_dict/acm/edge_type.pt')\nfor k in node_feas_dict_acm.keys():\n    node_feas_dict_acm[k] = torch.Tensor(node_feas_dict_acm[k])\n\nnode_feas_dict_imdb = torch.load('./higpt/model/meta_hgt/meta_dict/imdb/node_type.pt')\nfor k in node_feas_dict_imdb.keys():\n    node_feas_dict_imdb[k] = torch.Tensor(node_feas_dict_imdb[k])\nedge_feas_dict_imdb = torch.load('./higpt/model/meta_hgt/meta_dict/imdb/edge_type.pt')\nfor k in node_feas_dict_imdb.keys():\n    node_feas_dict_imdb[k] = torch.Tensor(node_feas_dict_imdb[k])\n\ndef load_graph(instruct_item, graph_root): \n    graph_path = osp.join(graph_root, instruct_item['graph']['graph'])\n    graph_dict = torch.load(graph_path)\n    hetero_key_order = instruct_item['graph']['keys_order']\n    if 'subject' in graph_dict.x_dict.keys(): \n        graph_dict['edge_feas_dict'] = edge_feas_dict_acm\n        graph_dict['node_feas_dict'] = node_feas_dict_acm\n    elif 'movie' in graph_dict.x_dict.keys(): \n        graph_dict['edge_feas_dict'] = edge_feas_dict_imdb\n        graph_dict['node_feas_dict'] = node_feas_dict_imdb\n    elif 'paper' in graph_dict.x_dict.keys(): \n        graph_dict['edge_feas_dict'] = edge_feas_dict_dblp\n        graph_dict['node_feas_dict'] = node_feas_dict_dblp\n        new_conf_reps = torch.ones(graph_dict['conference'].num_nodes, 768)\n        graph_dict['conference'].x = new_conf_reps\n    else: \n        raise NotImplementedError\n    cur_token_lens = []\n    for key in hetero_key_order:\n        cur_token_lens.append(graph_dict.x_dict[key].shape[0])\n\n    return {\n        'graph_data': graph_dict, \n        'graph_token_len': cur_token_lens\n    }\n\n\ndef load_prompting_file(file_path): \n    with open(file_path, 'r', encoding='utf-8') as f:\n        data = json.load(f)\n    return data\n\n# def prepare_query(instruct_item): \n\n\ndef run_eval(args, num_gpus):\n    # split question file into num_gpus files\n    prompt_file = load_prompting_file(args.prompting_file)\n    prompt_file = prompt_file[args.start_id:args.end_id]\n    chunk_size = len(prompt_file) // num_gpus\n    ans_handles = []\n    split_list = list(range(args.start_id, args.end_id, chunk_size))\n    idx_list = list(range(0, len(prompt_file), chunk_size))\n    if len(split_list) == num_gpus: \n        split_list.append(args.end_id)\n        idx_list.append(len(prompt_file))\n    elif len(split_list) == num_gpus + 1: \n        split_list[-1] = args.end_id\n        idx_list[-1] = len(prompt_file)\n    else: \n        raise ValueError('error in the number of list')\n\n    if osp.exists(args.output_res_path) is False: \n        os.makedirs(args.output_res_path, exist_ok = True)\n    \n    for idx in range(len(idx_list) - 1):\n        start_idx = idx_list[idx]\n        end_idx = idx_list[idx + 1]\n        \n        start_split = split_list[idx]\n        end_split = split_list[idx + 1]\n        ans_handles.append(\n            eval_model.remote(\n                args, prompt_file[start_idx:end_idx], start_split, end_split\n            )\n        )\n\n    ans_jsons = []\n    for ans_handle in ans_handles:\n        ans_jsons.extend(ray.get(ans_handle))\n\n    # with open(args.output_res_path, \"w\") as ans_file:\n    #     for line in ans_jsons:\n    #         ans_file.write(json.dumps(line) + \"\\n\")\n\n\n@ray.remote(num_gpus=1)\n@torch.inference_mode()\ndef eval_model(args, prompt_file, start_idx, end_idx):\n    # load prompting file\n    # prompt_file = load_prompting_file(args.prompting_file)\n\n\n    # Model\n    disable_torch_init()\n    # model_name = os.path.expanduser(args.model_name)\n    print('start loading')\n    tokenizer = AutoTokenizer.from_pretrained(args.model_name)\n    print('finish loading')\n\n    print('start loading')\n    model = HeteroLlamaForCausalLM.from_pretrained(args.model_name, torch_dtype=torch.float32, use_cache=True, low_cpu_mem_usage=True).cuda()\n    print('finish loading')\n\n    use_graph_start_end = getattr(model.config, \"use_graph_start_end\", False)\n    tokenizer.add_tokens([DEFAULT_GRAPH_PATCH_TOKEN], special_tokens=True)\n    if use_graph_start_end:\n        tokenizer.add_tokens([DEFAULT_G_START_TOKEN, DEFAULT_G_END_TOKEN], special_tokens=True)\n\n    graph_tower = model.get_model().graph_tower\n    \n    # TODO: add graph tower\n    # if graph_tower.device.type == 'meta':\n    #     print('meta')\n    graph_tower= load_metahgt_pretrained(MetaHGTConv, './MetaHGT_imdb_dblp_epoch5')\n    \n    model.get_model().graph_tower = graph_tower.cuda()\n    # else:\n    #     print('other')\n    # print(next(graph_tower.parameters()).dtype)\n    graph_tower.to(device='cuda', dtype=torch.float32)\n    graph_config = graph_tower.config\n    graph_config.graph_patch_token = tokenizer.convert_tokens_to_ids([DEFAULT_GRAPH_PATCH_TOKEN])[0]\n    graph_config.use_graph_start_end = use_graph_start_end\n    if use_graph_start_end:\n        graph_config.graph_start_token, graph_config.graph_end_token = tokenizer.convert_tokens_to_ids([DEFAULT_G_START_TOKEN, DEFAULT_G_END_TOKEN])\n    # TODO: add graph token len\n\n    res_data = []\n    print(f'total: {len(prompt_file)}')\n    for idx, instruct_item in tqdm(enumerate(prompt_file)):\n        # instruct_item = prompt_file[0]\n        # if idx >= 3: \n        #     break\n        graph_dict = load_graph(instruct_item, args.graph_root)\n        graph_token_len = graph_dict['graph_token_len']\n        graph_data = graph_dict['graph_data']\n\n        qs = instruct_item[\"conversations\"][0][\"value\"]\n        \n        if DEFAULT_GRAPH_TOKEN in qs:\n            # build replace_tokens\n            replace_tokens = []\n            for i, token_len in enumerate(graph_token_len):\n                replace_token = DEFAULT_GRAPH_PATCH_TOKEN * token_len\n                if use_graph_start_end:\n                    replace_token = DEFAULT_G_START_TOKEN + replace_token + DEFAULT_G_END_TOKEN\n                replace_tokens.append(replace_token)\n\n            for i, replace_token in enumerate(replace_tokens):\n                index = qs.find(DEFAULT_GRAPH_TOKEN)\n                qs = qs[:index] + replace_token + qs[index+len(DEFAULT_GRAPH_TOKEN):]\n        conv_mode = \"graphchat_v1\"\n\n        if args.conv_mode is not None and conv_mode != args.conv_mode:\n            print('[WARNING] the auto inferred conversation mode is {}, while `--conv-mode` is {}, using {}'.format(conv_mode, args.conv_mode, args.conv_mode))\n        else:\n            args.conv_mode = conv_mode\n\n        conv = conv_templates[args.conv_mode].copy()\n        conv.append_message(conv.roles[0], qs)\n        conv.append_message(conv.roles[1], None)\n        prompt = conv.get_prompt()\n        inputs = tokenizer([prompt])\n\n        \n\n        input_ids = torch.as_tensor(inputs.input_ids).cuda()\n\n        stop_str = conv.sep if conv.sep_style != SeparatorStyle.TWO else conv.sep2\n        keywords = [stop_str]\n        stopping_criteria = KeywordsStoppingCriteria(keywords, tokenizer, input_ids)\n\n        # graph_data.graph_node = graph_data.graph_node.to(torch.float16)\n        # graph_data.edge_index = graph_data.edge_index.to(torch.float16)\n\n        with torch.inference_mode():\n            output_ids = model.generate(\n                input_ids,\n                graph_data=graph_data.cuda(),\n                hetero_key_order = instruct_item['graph']['keys_order'], \n                do_sample=True,\n                temperature=0.2,\n                max_new_tokens=1024,\n                stopping_criteria=[stopping_criteria])\n\n        input_token_len = input_ids.shape[1]\n        n_diff_input_output = (input_ids != output_ids[:, :input_token_len]).sum().item()\n        if n_diff_input_output > 0:\n            print(f'[Warning] {n_diff_input_output} output_ids are not the same as the input_ids')\n        outputs = tokenizer.batch_decode(output_ids[:, input_token_len:], skip_special_tokens=True)[0]\n        outputs = outputs.strip()\n        if outputs.endswith(stop_str):\n            outputs = outputs[:-len(stop_str)]\n        outputs = outputs.strip()\n        # print(outputs)\n\n        res_data.append({\"id\": instruct_item[\"id\"], \"node_idx\": instruct_item[\"graph\"][\"node_idx\"], \"res\": outputs}.copy())\n        with open(osp.join(args.output_res_path, 'arxiv_test_res_{}_{}.json'.format(start_idx, end_idx)), \"w\") as fout:\n            json.dump(res_data, fout, indent=4)\n    return res_data\n    # with open(args.output_res_path, \"w\") as fout:\n    #     json.dump(res_data, fout, indent=4)\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--model-name\", type=str, default=\"facebook/opt-350m\")\n    # parser.add_argument(\"--image-file\", type=str, required=True)\n    # parser.add_argument(\"--query\", type=str, required=True)\n    parser.add_argument(\"--prompting_file\", type=str, default=None)\n    parser.add_argument(\"--conv-mode\", type=str, default=None)\n    parser.add_argument(\"--graph_root\", type=str, default=None)\n\n    parser.add_argument(\"--output_res_path\", type=str, default=None)\n    parser.add_argument(\"--num_gpus\", type=int, default=4)\n\n    parser.add_argument(\"--start_id\", type=int, default=0)\n    parser.add_argument(\"--end_id\", type=int, default=1000)\n\n    args = parser.parse_args()\n\n    # eval_model(args)\n\n    ray.init()\n    run_eval(args, args.num_gpus)\n\n\n# protobuf             4.22.3"}
{"type": "source_file", "path": "higpt/model/apply_delta.py", "content": "\"\"\"\nApply the delta weights on top of a base model.\n\nUsage:\npython3 -m fastchat.model.apply_delta --base ~/model_weights/llama-7b --target ~/model_weights/vicuna-7b --delta lmsys/vicuna-7b-delta-v1.1\n\"\"\"\nimport argparse\nimport gc\nimport glob\nimport json\nimport os\nimport shutil\nimport tempfile\n\nfrom huggingface_hub import snapshot_download\nimport torch\nfrom torch import nn\nfrom tqdm import tqdm\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig\n\n\nGB = 1 << 30\n\n\ndef split_files(model_path, tmp_path, split_size):\n    if not os.path.exists(model_path):\n        model_path = snapshot_download(repo_id=model_path)\n    if not os.path.exists(tmp_path):\n        os.makedirs(tmp_path)\n\n    file_pattern = os.path.join(model_path, \"pytorch_model-*.bin\")\n    files = glob.glob(file_pattern)\n\n    part = 0\n    try:\n        for file_path in tqdm(files):\n            state_dict = torch.load(file_path)\n            new_state_dict = {}\n\n            current_size = 0\n            for name, param in state_dict.items():\n                param_size = param.numel() * param.element_size()\n\n                if current_size + param_size > split_size:\n                    new_file_name = f\"pytorch_model-{part}.bin\"\n                    new_file_path = os.path.join(tmp_path, new_file_name)\n                    torch.save(new_state_dict, new_file_path)\n                    current_size = 0\n                    new_state_dict = None\n                    gc.collect()\n                    new_state_dict = {}\n                    part += 1\n\n                new_state_dict[name] = param\n                current_size += param_size\n\n            new_file_name = f\"pytorch_model-{part}.bin\"\n            new_file_path = os.path.join(tmp_path, new_file_name)\n            torch.save(new_state_dict, new_file_path)\n            new_state_dict = None\n            gc.collect()\n            new_state_dict = {}\n            part += 1\n    except Exception as e:\n        print(f\"An error occurred during split_files: {e}\")\n        shutil.rmtree(tmp_path)\n        raise\n\n\ndef apply_delta_low_cpu_mem(base_model_path, target_model_path, delta_path):\n    delta_tokenizer = AutoTokenizer.from_pretrained(delta_path, use_fast=False)\n    delta_config = AutoConfig.from_pretrained(delta_path)\n\n    if os.path.exists(target_model_path):\n        shutil.rmtree(target_model_path)\n    os.makedirs(target_model_path)\n\n    split_size = 4 * GB\n\n    with tempfile.TemporaryDirectory() as tmp_base_path, tempfile.TemporaryDirectory() as tmp_delta_path:\n        print(f\"Split files for the base model to {tmp_base_path}\")\n        split_files(base_model_path, tmp_base_path, split_size)\n        print(f\"Split files for the delta weights to {tmp_delta_path}\")\n        split_files(delta_path, tmp_delta_path, split_size)\n\n        base_pattern = os.path.join(tmp_base_path, \"pytorch_model-*.bin\")\n        base_files = glob.glob(base_pattern)\n        delta_pattern = os.path.join(tmp_delta_path, \"pytorch_model-*.bin\")\n        delta_files = glob.glob(delta_pattern)\n        delta_state_dict = torch.load(delta_files[0])\n\n        print(\"Applying the delta\")\n        weight_map = {}\n        total_size = 0\n\n        for i, base_file in tqdm(enumerate(base_files)):\n            state_dict = torch.load(base_file)\n            file_name = f\"pytorch_model-{i}.bin\"\n            for name, param in state_dict.items():\n                if name not in delta_state_dict:\n                    for delta_file in delta_files:\n                        delta_state_dict = torch.load(delta_file)\n                        gc.collect()\n                        if name in delta_state_dict:\n                            break\n\n                state_dict[name] += delta_state_dict[name]\n                weight_map[name] = file_name\n                total_size += param.numel() * param.element_size()\n                gc.collect()\n            torch.save(state_dict, os.path.join(target_model_path, file_name))\n\n        with open(\n            os.path.join(target_model_path, \"pytorch_model.bin.index.json\"), \"w\"\n        ) as f:\n            json.dump(\n                {\"weight_map\": weight_map, \"metadata\": {\"total_size\": total_size}}, f\n            )\n\n    print(f\"Saving the target model to {target_model_path}\")\n    delta_tokenizer.save_pretrained(target_model_path)\n    delta_config.save_pretrained(target_model_path)\n\n\ndef apply_delta(base_model_path, target_model_path, delta_path):\n    print(f\"Loading the delta weights from {delta_path}\")\n    delta_tokenizer = AutoTokenizer.from_pretrained(delta_path, use_fast=False)\n    delta = AutoModelForCausalLM.from_pretrained(\n        delta_path, torch_dtype=torch.float16, low_cpu_mem_usage=True\n    )\n\n    print(f\"Loading the base model from {base_model_path}\")\n    base = AutoModelForCausalLM.from_pretrained(\n        base_model_path, torch_dtype=torch.float16, low_cpu_mem_usage=True\n    )\n\n    print(\"Applying the delta\")\n    for name, param in tqdm(base.state_dict().items(), desc=\"Applying delta\"):\n        assert name in delta.state_dict()\n        param.data += delta.state_dict()[name]\n\n    print(f\"Saving the target model to {target_model_path}\")\n    base.save_pretrained(target_model_path)\n    delta_tokenizer.save_pretrained(target_model_path)\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--base-model-path\", type=str, required=True)\n    parser.add_argument(\"--target-model-path\", type=str, required=True)\n    parser.add_argument(\"--delta-path\", type=str, required=True)\n    parser.add_argument(\n        \"--low-cpu-mem\",\n        action=\"store_true\",\n        help=\"Lower the cpu memory usage. This will split large files and use \"\n        \"disk as swap to reduce the memory usage below 10GB.\",\n    )\n    args = parser.parse_args()\n\n    if args.low_cpu_mem:\n        apply_delta_low_cpu_mem(\n            args.base_model_path, args.target_model_path, args.delta_path\n        )\n    else:\n        apply_delta(args.base_model_path, args.target_model_path, args.delta_path)\n"}
{"type": "source_file", "path": "higpt/model/GraphLlama_pl.py", "content": "import os\nimport random\nfrom typing import Any, Optional, Dict, List\nimport logging\nimport torch\nfrom lightning.pytorch import LightningModule\nfrom transformers import get_cosine_schedule_with_warmup\nfrom torch.optim import AdamW\nimport torch.nn as nn\nfrom higpt.model.GraphLlama import GraphLlamaForCausalLM\nimport transformers\n\ndef find_all_linear_names(model):\n    cls = torch.nn.Linear\n    lora_module_names = set()\n    for name, module in model.named_modules():\n        if isinstance(module, cls):\n            names = name.split('.')\n            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n\n\n    if 'lm_head' in lora_module_names: # needed for 16-bit\n        lora_module_names.remove('lm_head')\n    return list(lora_module_names)\n\nclass GraphGPT_pl(LightningModule): \n    def __init__(self,\n        training_args, model_args, data_args, tokenizer, \n        **kwargs,\n    ):\n        super().__init__()\n        self.training_args = training_args\n        self.model_args = model_args\n        self.data_args = data_args\n        compute_dtype = (torch.float16 if training_args.fp16 else (torch.bfloat16 if training_args.bf16 else torch.float32))\n\n        bnb_model_from_pretrained_args = {}\n\n    ## load 4 8 bit \n        if training_args.bits in [4, 8]:\n            from transformers import BitsAndBytesConfig\n            from peft import prepare_model_for_int8_training\n            bnb_model_from_pretrained_args.update(dict(\n                device_map={\"\": training_args.device},\n                load_in_4bit=training_args.bits == 4,\n                load_in_8bit=training_args.bits == 8,\n                quantization_config=BitsAndBytesConfig(\n                    load_in_4bit=training_args.bits == 4,\n                    load_in_8bit=training_args.bits == 8,\n                    llm_int8_threshold=6.0,\n                    llm_int8_has_fp16_weight=False,\n                    bnb_4bit_compute_dtype=compute_dtype,\n                    bnb_4bit_use_double_quant=training_args.double_quant,\n                    bnb_4bit_quant_type=training_args.quant_type # {'fp4', 'nf4'}\n                )\n            ))\n\n        if model_args.graph_tower is not None:\n            self.model = GraphLlamaForCausalLM.from_pretrained(\n                    model_args.model_name_or_path,\n                    cache_dir=training_args.cache_dir,\n                    **bnb_model_from_pretrained_args\n                ) ## TODO: add real Graph Llama model \n        else:\n            self.model = transformers.LlamaForCausalLM.from_pretrained(\n                model_args.model_name_or_path,\n                cache_dir=training_args.cache_dir,\n                **bnb_model_from_pretrained_args\n            )\n        self.model.config.pretrain_graph_model_path = self.model.config.pretrain_graph_model_path + model_args.graph_tower\n        self.model.config.use_cache = False\n        if model_args.freeze_backbone:\n            self.model.model.requires_grad_(False)\n\n        if training_args.bits in [4, 8]:\n            self.model.config.torch_dtype=(torch.float32 if training_args.fp16 else (torch.bfloat16 if training_args.bf16 else torch.float32))\n            self.model = prepare_model_for_int8_training(self.model, use_gradient_checkpointing=training_args.gradient_checkpointing)\n\n        if training_args.gradient_checkpointing and model_args.graph_tower is None:\n            if hasattr(self.model, \"enable_input_require_grads\"):\n                self.model.enable_input_require_grads()\n            else:\n                def make_inputs_require_grad(module, input, output):\n                    output.requires_grad_(True)\n                self.model.get_input_embeddings().register_forward_hook(make_inputs_require_grad)\n\n        if training_args.lora_enable:\n            from peft import LoraConfig, get_peft_model\n            lora_config = LoraConfig(\n                r=training_args.lora_r,\n                lora_alpha=training_args.lora_alpha,\n                target_modules=find_all_linear_names(model),\n                lora_dropout=training_args.lora_dropout,\n                bias=training_args.lora_bias,\n                task_type=\"CAUSAL_LM\",\n            )\n            if training_args.bits == 16:\n                if training_args.bf16:\n                    model.to(torch.bfloat16)\n                if training_args.fp16:\n                    model.to(torch.float16)\n            logging.warning(\"Adding LoRA adapters...\")\n            model = get_peft_model(model, lora_config)\n        \n        if model_args.graph_tower is not None:\n            model_graph_dict = self.model.get_model().initialize_graph_modules(\n                graph_tower=model_args.graph_tower,\n                graph_select_layer=model_args.graph_select_layer,\n                pretrain_graph_mlp_adapter=model_args.pretrain_graph_mlp_adapter,\n                fsdp=None\n            )\n            self.model.get_graph_tower().to(dtype=compute_dtype)\n            # graph_config = model_graph_dict['graph_config']\n\n            # data_args.graph_token_len = model_graph_dict['graph_token_len']\n            # data_args.graph_processor = model_graph_dict['graph_processor']\n            data_args.is_graph = True\n\n            self.model.config.tune_graph_mlp_adapter = training_args.tune_graph_mlp_adapter = model_args.tune_graph_mlp_adapter\n            if model_args.tune_graph_mlp_adapter:\n                self.model.requires_grad_(False)\n                for p in self.model.get_model().graph_projector.parameters():\n                    p.requires_grad = True\n\n            self.model.config.freeze_graph_mlp_adapter = training_args.freeze_graph_mlp_adapter\n            if training_args.freeze_graph_mlp_adapter:\n                for p in self.model.get_model().graph_projector.parameters():\n                    p.requires_grad = False\n\n            if training_args.bits in [4, 8]:\n                self.model.get_model().graph_projector.to(dtype=compute_dtype, device=training_args.device)\n\n            self.model.config.use_graph_start_end = data_args.use_graph_start_end = model_args.use_graph_start_end\n            # graph_config.use_graph_start_end = training_args.use_graph_start_end = model_args.use_graph_start_end\n            training_args.use_graph_start_end = model_args.use_graph_start_end\n            self.model.config.sep_graph_conv_front = data_args.sep_graph_conv_front\n            self.model.initialize_graph_tokenizer(use_graph_start_end=model_args.use_graph_start_end, tokenizer=tokenizer, device='cuda',\n                                            tune_graph_mlp_adapter=model_args.tune_graph_mlp_adapter, pretrain_graph_mlp_adapter=model_args.pretrain_graph_mlp_adapter)\n\n            params_no_grad = [n for n, p in self.model.named_parameters() if not p.requires_grad]\n            if training_args.bits in [4, 8]:\n                from peft.tuners.lora import LoraLayer\n                for name, module in self.model.named_modules():\n                    if isinstance(module, LoraLayer):\n                        if training_args.bf16:\n                            module = module.to(torch.bfloat16)\n                    if 'norm' in name:\n                        module = module.to(torch.float32)\n                    if 'lm_head' in name or 'embed_tokens' in name:\n                        if hasattr(module, 'weight'):\n                            if training_args.bf16 and module.weight.dtype == torch.float32:\n                                module = module.to(torch.bfloat16)\n\n            print('************************** parameters: #', sum(p.numel() for p in self.model.parameters() if p.requires_grad))\n            tuned_params = []\n            for name, param in self.model.named_parameters():\n                if param.requires_grad:\n                    tuned_params.append(name)\n            print(tuned_params)\n        \n    def training_step(self, batch, batch_idx):\n        bs = len(batch[\"input_ids\"])\n        loss_dict = self.model(**batch)\n        loss = loss_dict['loss']\n        \n        log_dict = {f'train_loss': loss.item()}\n        self.log_dict(log_dict, on_step=True, on_epoch=True, prog_bar=True, sync_dist=True, batch_size=bs)\n        return loss\n\n    def configure_optimizers(self):\n        \"\"\"Prepare optimizer and schedule (linear warmup and decay)\"\"\"\n        # no_decay = [\"bias\", \"LayerNorm.weight\"]\n        # if IS_STAGE2:\n        \n        optimizer_grouped_parameters = [\n            {\n                \"params\": [p for n, p in self.model.named_parameters()], \"lr_scale\": [1e-5, 1e-4]\n            }\n        ]\n        \n        optimizer = AdamW(optimizer_grouped_parameters, lr=self.training_args.learning_rate)\n\n        # scheduler = get_linear_schedule_with_warmup(\n        #     optimizer,\n        #     num_warmup_steps=self.training_args.warmup_steps,\n        #     num_training_steps=self.trainer.estimated_stepping_batches,\n        # )\n        scheduler = get_cosine_schedule_with_warmup(\n            optimizer,\n            num_warmup_steps=self.training_args.warmup_steps,\n            num_training_steps=self.trainer.estimated_stepping_batches,\n        )\n        scheduler = {\"scheduler\": scheduler, \"interval\": \"step\", \"frequency\": 1}\n        return [optimizer], [scheduler]"}
{"type": "source_file", "path": "higpt/model/chatglm_model.py", "content": "import torch\nfrom typing import List, Tuple\n\n\ndef stream_chat_token_num(tokenizer, query: str, history: List[Tuple[str, str]] = None):\n    if history is None:\n        history = []\n    if not history:\n        prompt = query\n    else:\n        prompt = \"\"\n        for i, (old_query, response) in enumerate(history):\n            prompt += \"[Round {}]\\n问：{}\\n答：{}\\n\".format(i, old_query, response)\n        prompt += \"[Round {}]\\n问：{}\\n答：\".format(len(history), query)\n    inputs = tokenizer([prompt])\n    return sum([len(x) for x in inputs[\"input_ids\"]])\n\n\n@torch.inference_mode()\ndef chatglm_generate_stream(\n    model, tokenizer, params, device, context_len=2048, stream_interval=2\n):\n    \"\"\"Generate text using model's chat api\"\"\"\n    messages = params[\"prompt\"]\n    max_new_tokens = int(params.get(\"max_new_tokens\", 256))\n    temperature = float(params.get(\"temperature\", 1.0))\n    top_p = float(params.get(\"top_p\", 1.0))\n    repetition_penalty = float(params.get(\"repetition_penalty\", 1.0))\n    echo = params.get(\"echo\", True)\n\n    gen_kwargs = {\n        # \"max_new_tokens\": max_new_tokens,  disabled due to a warning.\n        \"do_sample\": True if temperature > 1e-5 else False,\n        \"top_p\": top_p,\n        \"repetition_penalty\": repetition_penalty,\n        \"logits_processor\": None,\n    }\n    if temperature > 1e-5:\n        gen_kwargs[\"temperature\"] = temperature\n\n    hist = []\n    for i in range(0, len(messages) - 2, 2):\n        hist.append((messages[i][1], messages[i + 1][1]))\n    query = messages[-2][1]\n\n    input_echo_len = stream_chat_token_num(tokenizer, query, hist)\n\n    output = \"\"\n    i = 0\n    for i, (response, new_hist) in enumerate(\n        model.stream_chat(tokenizer, query, hist, **gen_kwargs)\n    ):\n        if echo:\n            output = query + \" \" + response\n        else:\n            output = response\n\n        yield {\n            \"text\": output,\n            \"usage\": {\n                \"prompt_tokens\": input_echo_len,\n                \"completion_tokens\": i,\n                \"total_tokens\": input_echo_len + i,\n            },\n            \"finish_reason\": None,\n        }\n\n    # TODO: ChatGLM stop when it reach max length\n    # Only last stream result contains finish_reason, we set finish_reason as stop\n    ret = {\n        \"text\": output,\n        \"usage\": {\n            \"prompt_tokens\": input_echo_len,\n            \"completion_tokens\": i,\n            \"total_tokens\": input_echo_len + i,\n        },\n        \"finish_reason\": \"stop\",\n    }\n    yield ret\n"}
{"type": "source_file", "path": "higpt/model/HeteroLlama.py", "content": "#    Copyright 2023 Haotian Liu\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\");\n#    you may not use this file except in compliance with the License.\n#    You may obtain a copy of the License at\n#\n#        http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS,\n#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#    See the License for the specific language governing permissions and\n#    limitations under the License.\n\nfrom typing import List, Optional, Tuple, Union\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn import CrossEntropyLoss\n\nfrom transformers import AutoConfig, AutoModelForCausalLM, \\\n                         LlamaConfig, LlamaModel, LlamaForCausalLM, \\\n                         CLIPVisionModel, CLIPImageProcessor\n\nfrom transformers.modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n\nfrom higpt.model.graph_layers import MPNN, GNN, CLIP, graph_transformer\nfrom higpt.model.meta_hgt import MetaHGTConvCfg, MetaHGTConv\nfrom higpt.model.heteclip_models import Transformer, LayerNorm, CLIPTextCfg\nfrom torch_geometric.data import Data\nimport json\nimport os.path as osp\nimport glob\n\nDEFAULT_GRAPH_TOKEN = \"<graph>\"\nDEFAULT_GRAPH_PATCH_TOKEN = \"<g_patch>\"\nDEFAULT_G_START_TOKEN = \"<g_start>\"\nDEFAULT_G_END_TOKEN = \"<g_end>\"\n\n\nclass HeteroLlamaConfig(LlamaConfig):\n    model_type = \"HeteroLlama\"\n\n\ndef load_metahgt_pretrained(model_name, pretrain_model_path): \n    # load conig json\n    \n    assert osp.exists(osp.join(pretrain_model_path, 'graph_config.json')), 'graph_config.json missing'\n    with open(osp.join(pretrain_model_path, 'graph_config.json'), 'r') as f:\n        graph_config_dict = json.load(f)\n    graph_cfg = MetaHGTConvCfg(**graph_config_dict)\n\n    assert osp.exists(osp.join(pretrain_model_path, 'text_config.json')), 'text_config.json missing'\n    with open(osp.join(pretrain_model_path, 'text_config.json'), 'r') as f:\n        text_config_dict = json.load(f)\n    text_cfg = CLIPTextCfg(**text_config_dict)\n    \n    assert model_name == MetaHGTConv\n    model = model_name(in_channels = graph_cfg.in_channels,\n        out_channels = graph_cfg.out_channels,\n        heads = graph_cfg.heads,\n        dynamic = graph_cfg.dynamic, \n        text_cfg = text_cfg,)\n\n    pkl_files = glob.glob(osp.join(pretrain_model_path, '*.ckpt'))\n    state_dict = torch.load(pkl_files[0], map_location = 'cpu')['state_dict']\n    print('loading graph pre train model ...')\n    gnn_state_dict = {}\n    for key, value in state_dict.items():\n        if key.startswith('model.graph_encoder'):\n            new_key = key.split('model.graph_encoder.')[1]\n            gnn_state_dict[new_key] = value\n    model.load_state_dict(gnn_state_dict, strict=False)\n\n    return model\n\n\nclass HeteroLlamaModel(LlamaModel):\n    config_class = HeteroLlamaConfig\n\n    def __init__(self, config: LlamaConfig):\n        super(HeteroLlamaModel, self).__init__(config)\n\n        if hasattr(config, \"graph_tower\"):\n            # HACK: for FSDP\n\n            if config.graph_tower.split('_')[0] == \"MetaHGT\": \n                self.graph_tower = load_metahgt_pretrained(MetaHGTConv, config.pretrain_graph_model_path)\n\n            # self.vision_tower = CLIPVisionModel.from_pretrained(config.mm_vision_tower)\n\n        if hasattr(config, \"use_graph_proj\"):\n            self.graph_projector = nn.Linear(config.graph_hidden_size, config.hidden_size)\n\n    def get_graph_tower(self):\n        graph_tower = getattr(self, 'graph_tower', None)\n        if type(graph_tower) is list:\n            graph_tower = graph_tower[0]\n        return graph_tower\n\n    def initialize_graph_modules(self, graph_tower, graph_select_layer,\n                                  pretrain_graph_mlp_adapter=None, fsdp=None): # TODO: modify this function\n        self.config.graph_tower = graph_tower\n\n\n        if not hasattr(self, 'graph_tower'):\n            if self.config.graph_tower.split('_')[0] == \"MetaHGT\":\n                graph_tower = load_metahgt_pretrained(MetaHGTConv, self.config.pretrain_graph_model_path)\n        else:\n            graph_tower = self.graph_tower\n        graph_tower.requires_grad_(False)\n\n        if fsdp is not None and len(fsdp) > 0:\n            self.graph_tower = [graph_tower]\n        else:\n            self.graph_tower = graph_tower\n\n        \n\n        self.config.use_graph_proj = True\n        self.config.graph_select_layer = graph_select_layer\n\n        if not hasattr(self, 'graph_projector'):\n            self.graph_projector = nn.Linear(self.config.graph_hidden_size, self.config.hidden_size)\n\n        if pretrain_graph_mlp_adapter is not None:\n            graph_projector_weights = torch.load(pretrain_graph_mlp_adapter, map_location='cpu')\n            self.graph_projector.load_state_dict({k.split('.')[-1]: v for k, v in graph_projector_weights.items()})\n\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        past_key_values: Optional[List[torch.FloatTensor]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        # graph_node_reps: Optional[torch.FloatTensor] = None,\n        # edge_index_reps: Optional[torch.FloatTensor] = None,\n        graph_data: Optional[Data] = None,\n        return_dict: Optional[bool] = None,\n        hetero_key_order: Optional[List[List[str]]] =None\n    ) -> Union[Tuple, BaseModelOutputWithPast]:\n\n        # HACK: replace back original embeddings for LLaVA pretraining\n        orig_embeds_params = getattr(self, 'orig_embeds_params', None)\n        # if orig_embeds_params is not None:\n        #     orig_embeds_params = orig_embeds_params[0]\n        #     with torch.no_grad():\n        #         self.get_input_embeddings().weight.data[:-2] = orig_embeds_params[:-2].data\n\n        if inputs_embeds is None:\n            inputs_embeds = self.embed_tokens(input_ids)\n\n        # graph_tower = self.get_graph_tower()\n        if self.graph_tower is not None and (input_ids.shape[1] != 1 or self.training) and graph_data is not None:\n            # TODO: this is a modified multimodal LLM -- Haotian Liu\n\n            if type(graph_data[0]) is list:\n                graph_node_features = []\n                for g_list, keys in zip(graph_data, hetero_key_order): \n                    if type(keys[0]) is list: \n                        for g, g_k in zip(g_list, keys):\n                            ret_g = g.x_dict\n                            for k in g_k:\n                                if torch.any(torch.isnan(ret_g[k])): \n                                    print(k, ret_g[k])\n                                    raise ValueError\n                                graph_node_features.append(ret_g[k])\n                    else:\n                        for g in g_list:\n                            ret_g = g.x_dict\n                            for k in keys:\n                                if torch.any(torch.isnan(ret_g[k])): \n                                    print(k, ret_g[k])\n                                    raise ValueError\n                                graph_node_features.append(ret_g[k])\n            elif type(graph_data) is list:\n                # variable length images\n                graph_node_features = []\n                for g, keys in zip(graph_data, hetero_key_order):\n                    # print(g)\n                    # print(self.graph_tower.parameters().__next__().device)\n                    # print(g.x_dict['paper'].device)\n                    # cur_device = self.graph_tower.parameters().__next__().device\n                    # self.graph_tower = self.graph_tower.to(cur_device)\n                    # g = g.to(cur_device)\n                    # with torch.no_grad():\n                    #     ret_g = self.graph_tower(g.x_dict, g.edge_index_dict, node_type_feas_dict = g['node_feas_dict'], edge_type_feas_dict = g['edge_feas_dict'])\n                    # for k, v in ret_g.items():\n                    #     ret_g[k] = v.to(cur_device)\n                    ret_g = g.x_dict\n                    for k in keys:\n                        if torch.any(torch.isnan(ret_g[k])): \n                            print(k, ret_g[k])\n                            raise ValueError\n                        graph_node_features.append(ret_g[k])\n            else:\n                raise ValueError(f'graph_node_reps is expected to be a list but got {type(graph_data)}')\n            if type(graph_data) is list:\n                # if type(graph_node_features[0]) is not dict:\n                graph_node_features = [self.graph_projector(node_feature) for node_feature in graph_node_features]\n            else:\n                raise ValueError(f'graph_node_reps is expected to be a list but got {type(graph_data)}')\n            dummy_graph_features = torch.zeros(256, 768, device=inputs_embeds.device, dtype=inputs_embeds.dtype)\n\n            new_input_embeds = []\n            cur_graph_idx = 0\n            for cur_input_ids, cur_input_embeds in zip(input_ids, inputs_embeds):\n                if (cur_input_ids == self.graph_tower.config.graph_patch_token).sum() == 0:\n                    # multimodal LLM, but the current sample is not multimodal\n                    cur_input_embeds = cur_input_embeds + (0. * dummy_graph_features).sum()\n                    new_input_embeds.append(cur_input_embeds)\n                    cur_graph_idx += 1\n                    continue\n                if self.graph_tower.config.use_graph_start_end:\n                    cur_graph_features = graph_node_features[cur_graph_idx]\n                    num_patches = cur_graph_features.shape[0]\n                    if (cur_input_ids == self.graph_tower.config.graph_start_token).sum() != (cur_input_ids == self.graph_tower.config.graph_end_token).sum():\n                        raise ValueError(\"The number of graph start tokens and graph end tokens should be the same.\")\n                    graph_start_tokens = torch.where(cur_input_ids == self.graph_tower.config.graph_start_token)[0]\n                    # print(graph_start_tokens)\n                    for graph_start_token_pos in graph_start_tokens:\n                        cur_graph_features = graph_node_features[cur_graph_idx].to(device=cur_input_embeds.device)\n                        num_patches = cur_graph_features.shape[0]\n                        if cur_input_ids[graph_start_token_pos + num_patches + 1] != self.graph_tower.config.graph_end_token:\n                            raise ValueError(\"The graph end token should follow the graph start token.\")\n                        if orig_embeds_params is not None:\n                            cur_new_input_embeds = torch.cat((cur_input_embeds[:graph_start_token_pos].detach(), cur_input_embeds[graph_start_token_pos:graph_start_token_pos+1], cur_graph_features, cur_input_embeds[graph_start_token_pos + num_patches + 1:graph_start_token_pos + num_patches + 2], cur_input_embeds[graph_start_token_pos + num_patches + 2:].detach()), dim=0)\n                        else:\n                            cur_new_input_embeds = torch.cat((cur_input_embeds[:graph_start_token_pos+1], cur_graph_features, cur_input_embeds[graph_start_token_pos + num_patches + 1:]), dim=0)\n                        cur_graph_idx += 1\n                    new_input_embeds.append(cur_new_input_embeds)\n                else:\n                    cur_graph_features = graph_node_features[cur_graph_idx]\n                    num_patches = cur_graph_features.shape[0]\n                    if (cur_input_ids == self.graph_tower.config.graph_patch_token).sum() != num_patches:\n                        raise ValueError(\"The number of graph patch tokens should be the same as the number of graph patches.\")\n                    masked_indices = torch.where(cur_input_ids == self.graph_tower.config.graph_patch_token)[0]\n                    mask_index_start = masked_indices[0]\n                    if (masked_indices != torch.arange(mask_index_start, mask_index_start+num_patches, device=masked_indices.device, dtype=masked_indices.dtype)).any():\n                        raise ValueError(\"The graph patch tokens should be consecutive.\")\n                    if orig_embeds_params is not None:\n                        cur_new_input_embeds = torch.cat((cur_input_embeds[:mask_index_start].detach(), cur_graph_features, cur_input_embeds[mask_index_start+num_patches:].detach()), dim=0)\n                    else:\n                        cur_new_input_embeds = torch.cat((cur_input_embeds[:mask_index_start], cur_graph_features, cur_input_embeds[mask_index_start+num_patches:]), dim=0)\n                    new_input_embeds.append(cur_new_input_embeds)\n                    cur_graph_idx += 1\n\n            # print(cur_graph_idx)\n            # print(len(graph_node_features))\n            assert cur_graph_idx == len(graph_node_features)\n            inputs_embeds = torch.stack(new_input_embeds, dim=0)\n\n        return super(HeteroLlamaModel, self).forward(\n            input_ids=None, attention_mask=attention_mask, past_key_values=past_key_values,\n            inputs_embeds=inputs_embeds, use_cache=use_cache,\n            output_attentions=output_attentions, output_hidden_states=output_hidden_states,\n            return_dict=return_dict\n        )\n\n\nclass HeteroLlamaForCausalLM(LlamaForCausalLM):\n    config_class = HeteroLlamaConfig\n\n    def __init__(self, config):\n        super(LlamaForCausalLM, self).__init__(config)\n        self.model = HeteroLlamaModel(config)\n\n        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_model(self):\n        return self.model\n\n    def get_graph_tower(self):\n        return self.get_model().get_graph_tower()\n\n    def get_vision_tower(self):\n        model = self.get_model()\n        graph_tower = model.graph_tower\n        if type(graph_tower) is list:\n            graph_tower = graph_tower[0]\n        return graph_tower\n\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        past_key_values: Optional[List[torch.FloatTensor]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        labels: Optional[torch.LongTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        # graph_node_reps: Optional[torch.FloatTensor] = None,\n        # edge_index_reps: Optional[torch.FloatTensor] = None,\n        graph_data: Optional[Data] = None,\n        return_dict: Optional[bool] = None,\n        hetero_key_order: Optional[List[List[str]]] =None, \n    ) -> Union[Tuple, CausalLMOutputWithPast]:\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n        outputs = self.model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            past_key_values=past_key_values,\n            inputs_embeds=inputs_embeds,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n            # graph_node_reps=graph_node_reps, \n            # edge_index_reps=edge_index_reps\n            graph_data = graph_data, \n            hetero_key_order = hetero_key_order\n        )\n\n        hidden_states = outputs[0]\n        logits = self.lm_head(hidden_states)\n\n        loss = None\n        if labels is not None:\n            # Shift so that tokens < n predict n\n            shift_logits = logits[..., :-1, :].contiguous()\n            shift_labels = labels[..., 1:].contiguous()\n            # Flatten the tokens\n            loss_fct = CrossEntropyLoss()\n            shift_logits = shift_logits.view(-1, self.config.vocab_size)\n            shift_labels = shift_labels.view(-1)\n            # Enable model/pipeline parallelism\n            shift_labels = shift_labels.to(shift_logits.device)\n            loss = loss_fct(shift_logits, shift_labels)\n\n        if not return_dict:\n            output = (logits,) + outputs[1:]\n            return (loss,) + output if loss is not None else output\n\n        return CausalLMOutputWithPast(\n            loss=loss,\n            logits=logits,\n            past_key_values=outputs.past_key_values,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )\n\n    def prepare_inputs_for_generation(\n        self, input_ids, past_key_values=None, attention_mask=None, inputs_embeds=None, **kwargs\n    ):\n        if past_key_values:\n            input_ids = input_ids[:, -1:]\n\n        # if `inputs_embeds` are passed, we only want to use them in the 1st generation step\n        if inputs_embeds is not None and past_key_values is None:\n            model_inputs = {\"inputs_embeds\": inputs_embeds}\n        else:\n            model_inputs = {\"input_ids\": input_ids}\n        if kwargs.get(\"graph_data\") is None: \n            model_inputs.update(\n            {\n                \"past_key_values\": past_key_values,\n                \"use_cache\": kwargs.get(\"use_cache\"),\n                \"attention_mask\": attention_mask,\n                \"graph_data\": None,\n                # \"edge_index_reps\": kwargs.get(\"edge_index_reps\", None),\n                \"hetero_key_order\": [kwargs.get(\"hetero_key_order\", None)]\n            }\n            )\n        else:\n            model_inputs.update(\n                {\n                    \"past_key_values\": past_key_values,\n                    \"use_cache\": kwargs.get(\"use_cache\"),\n                    \"attention_mask\": attention_mask,\n                    \"graph_data\": [kwargs.get(\"graph_data\", None)],\n                    # \"edge_index_reps\": kwargs.get(\"edge_index_reps\", None),\n                    \"hetero_key_order\": [kwargs.get(\"hetero_key_order\", None)]\n                }\n            )\n        return model_inputs\n\n    def initialize_graph_tokenizer(self, use_graph_start_end, tokenizer, device,\n                                    tune_graph_mlp_adapter=False, pretrain_graph_mlp_adapter=None):\n        vision_config = self.get_graph_tower().config\n        vision_config.use_graph_start_end = use_graph_start_end\n        tokenizer.add_tokens([DEFAULT_GRAPH_PATCH_TOKEN], special_tokens=True)\n        self.resize_token_embeddings(len(tokenizer))\n\n        if use_graph_start_end:\n            num_new_tokens = tokenizer.add_tokens([DEFAULT_G_START_TOKEN, DEFAULT_G_END_TOKEN], special_tokens=True)\n            self.resize_token_embeddings(len(tokenizer))\n            vision_config.graph_start_token, vision_config.graph_end_token = tokenizer.convert_tokens_to_ids([DEFAULT_G_START_TOKEN, DEFAULT_G_END_TOKEN])\n\n            if num_new_tokens > 0:\n                input_embeddings = self.get_input_embeddings().weight.data\n                output_embeddings = self.get_output_embeddings().weight.data\n\n                input_embeddings_avg = input_embeddings[:-num_new_tokens].mean(\n                    dim=0, keepdim=True)\n                output_embeddings_avg = output_embeddings[:-num_new_tokens].mean(\n                    dim=0, keepdim=True)\n\n                input_embeddings[-num_new_tokens:] = input_embeddings_avg\n                output_embeddings[-num_new_tokens:] = output_embeddings_avg\n\n            if tune_graph_mlp_adapter:\n                # self.get_model().orig_embeds_params = [self.get_input_embeddings().weight.data.clone().to(device=device)]\n                self.get_model().orig_embeds_params = [self.get_input_embeddings().weight.data.clone().to(device=device)]\n                for p in self.get_input_embeddings().parameters():\n                    p.requires_grad = True\n                for p in self.get_output_embeddings().parameters():\n                    p.requires_grad = False\n\n            if pretrain_graph_mlp_adapter:\n                mm_projector_weights = torch.load(pretrain_graph_mlp_adapter, map_location='cpu')\n                embed_tokens_weight = mm_projector_weights['model.embed_tokens.weight']\n                assert num_new_tokens == 2\n                if input_embeddings.shape == embed_tokens_weight.shape:\n                    input_embeddings[-num_new_tokens:] = embed_tokens_weight[-num_new_tokens:]\n                elif embed_tokens_weight.shape[0] == num_new_tokens:\n                    input_embeddings[-num_new_tokens:] = embed_tokens_weight\n                else:\n                    raise ValueError(f\"Unexpected embed_tokens_weight shape. Pretrained: {embed_tokens_weight.shape}. Current: {input_embeddings.shape}. Numer of new tokens: {num_new_tokens}.\")\n\n        vision_config.graph_patch_token = tokenizer.convert_tokens_to_ids([DEFAULT_GRAPH_PATCH_TOKEN])[0]\n\nAutoConfig.register(\"HeteroLlama\", HeteroLlamaConfig)\nAutoModelForCausalLM.register(HeteroLlamaConfig, HeteroLlamaForCausalLM)\n"}
{"type": "source_file", "path": "higpt/model/builder.py", "content": "#    Copyright 2023 Haotian Liu\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\");\n#    you may not use this file except in compliance with the License.\n#    You may obtain a copy of the License at\n#\n#        http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS,\n#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#    See the License for the specific language governing permissions and\n#    limitations under the License.\n\n\nimport os\nimport shutil\n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig, BitsAndBytesConfig\nimport torch\nfrom higpt.model import *\nfrom higpt.constants import DEFAULT_GRAPH_PATCH_TOKEN, DEFAULT_G_START_TOKEN, DEFAULT_G_END_TOKEN\n\n\ndef load_pretrained_model(model_path, model_base, model_name, load_8bit=False, load_4bit=False, device_map=\"auto\"):\n    kwargs = {\"device_map\": device_map}\n\n    if load_8bit:\n        kwargs['load_in_8bit'] = True\n    elif load_4bit:\n        kwargs['load_in_4bit'] = True\n        kwargs['quantization_config'] = BitsAndBytesConfig(\n            load_in_4bit=True,\n            bnb_4bit_compute_dtype=torch.float16,\n            bnb_4bit_use_double_quant=True,\n            bnb_4bit_quant_type='nf4'\n        )\n    else:\n        kwargs['torch_dtype'] = torch.float16\n\n    if 'graphchat' in model_name.lower():\n        # Load LLaVA model\n        if 'lora' in model_name.lower() and model_base is not None:\n            lora_cfg_pretrained = AutoConfig.from_pretrained(model_path)\n            tokenizer = AutoTokenizer.from_pretrained(model_base, use_fast=False)\n            print('Loading LLaVA from base model...')\n            model = GraphLlamaForCausalLM.from_pretrained(model_base, low_cpu_mem_usage=True, config=lora_cfg_pretrained, **kwargs)\n            token_num, tokem_dim = model.lm_head.out_features, model.lm_head.in_features\n            if model.lm_head.weight.shape[0] != token_num:\n                model.lm_head.weight = torch.nn.Parameter(torch.empty(token_num, tokem_dim, device=model.device, dtype=model.dtype))\n                model.model.embed_tokens.weight = torch.nn.Parameter(torch.empty(token_num, tokem_dim, device=model.device, dtype=model.dtype))\n\n            print('Loading additional LLaVA weights...')\n            if os.path.exists(os.path.join(model_path, 'non_lora_trainables.bin')):\n                non_lora_trainables = torch.load(os.path.join(model_path, 'non_lora_trainables.bin'), map_location='cpu')\n            else:\n                # this is probably from HF Hub\n                from huggingface_hub import hf_hub_download\n                def load_from_hf(repo_id, filename, subfolder=None):\n                    cache_file = hf_hub_download(\n                        repo_id=repo_id,\n                        filename=filename,\n                        subfolder=subfolder)\n                    return torch.load(cache_file, map_location='cpu')\n                non_lora_trainables = load_from_hf(model_path, 'non_lora_trainables.bin')\n            non_lora_trainables = {(k[11:] if k.startswith('base_model.') else k): v for k, v in non_lora_trainables.items()}\n            if any(k.startswith('model.model.') for k in non_lora_trainables):\n                non_lora_trainables = {(k[6:] if k.startswith('model.') else k): v for k, v in non_lora_trainables.items()}\n            model.load_state_dict(non_lora_trainables, strict=False)\n\n            from peft import PeftModel\n            print('Loading LoRA weights...')\n            model = PeftModel.from_pretrained(model, model_path)\n            print('Merging LoRA weights...')\n            model = model.merge_and_unload()\n            print('Model is loaded...')\n        elif model_base is not None:\n            # this may be mm projector only\n            print('Loading LLaVA from base model...')\n            if 'mpt' in model_name.lower():\n                if not os.path.isfile(os.path.join(model_path, 'configuration_mpt.py')):\n                    shutil.copyfile(os.path.join(model_base, 'configuration_mpt.py'), os.path.join(model_path, 'configuration_mpt.py'))\n                tokenizer = AutoTokenizer.from_pretrained(model_base, use_fast=True)\n                cfg_pretrained = AutoConfig.from_pretrained(model_path, trust_remote_code=True)\n                model = LlavaMPTForCausalLM.from_pretrained(model_base, low_cpu_mem_usage=True, config=cfg_pretrained, **kwargs)\n            else:\n                tokenizer = AutoTokenizer.from_pretrained(model_base, use_fast=False)\n                cfg_pretrained = AutoConfig.from_pretrained(model_path)\n                model = LlavaLlamaForCausalLM.from_pretrained(model_base, low_cpu_mem_usage=True, config=cfg_pretrained, **kwargs)\n\n            mm_projector_weights = torch.load(os.path.join(model_path, 'mm_projector.bin'), map_location='cpu')\n            mm_projector_weights = {k: v.to(torch.float16) for k, v in mm_projector_weights.items()}\n            model.load_state_dict(mm_projector_weights, strict=False)\n        else:\n            if 'mpt' in model_name.lower():\n                tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=True)\n                model = LlavaMPTForCausalLM.from_pretrained(model_path, low_cpu_mem_usage=True, **kwargs)\n            else:\n                tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)\n                model = LlavaLlamaForCausalLM.from_pretrained(model_path, low_cpu_mem_usage=True, **kwargs)\n    else:\n        # Load language model\n        if model_base is not None:\n            # PEFT model\n            from peft import PeftModel\n            tokenizer = AutoTokenizer.from_pretrained(model_base, use_fast=False)\n            model = AutoModelForCausalLM.from_pretrained(model_base, torch_dtype=torch.float16, low_cpu_mem_usage=True, device_map=\"auto\")\n            print(f\"Loading LoRA weights from {model_path}\")\n            model = PeftModel.from_pretrained(model, model_path)\n            print(f\"Merging weights\")\n            model = model.merge_and_unload()\n            print('Convert to FP16...')\n            model.to(torch.float16)\n        else:\n            use_fast = False\n            if 'mpt' in model_name.lower():\n                tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=True)\n                model = AutoModelForCausalLM.from_pretrained(model_path, low_cpu_mem_usage=True, trust_remote_code=True, **kwargs)\n            else:\n                tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)\n                model = AutoModelForCausalLM.from_pretrained(model_path, low_cpu_mem_usage=True, **kwargs)\n\n    image_processor = None\n\n    if 'llava' in model_name.lower():\n        mm_use_im_start_end = getattr(model.config, \"mm_use_im_start_end\", False)\n        mm_use_im_patch_token = getattr(model.config, \"mm_use_im_patch_token\", True)\n        if mm_use_im_patch_token:\n            tokenizer.add_tokens([DEFAULT_IMAGE_PATCH_TOKEN], special_tokens=True)\n        if mm_use_im_start_end:\n            tokenizer.add_tokens([DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN], special_tokens=True)\n        model.resize_token_embeddings(len(tokenizer))\n\n        vision_tower = model.get_vision_tower()\n        if not vision_tower.is_loaded:\n            vision_tower.load_model()\n        vision_tower.to(device='cuda', dtype=torch.float16)\n        image_processor = vision_tower.image_processor\n\n    if hasattr(model.config, \"max_sequence_length\"):\n        context_len = model.config.max_sequence_length\n    else:\n        context_len = 2048\n\n    return tokenizer, model, image_processor, context_len"}
{"type": "source_file", "path": "higpt/eval/run_higpt_incontext.py", "content": "import argparse\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\nimport os\nfrom higpt.conversation import conv_templates, SeparatorStyle\nfrom higpt.utils import disable_torch_init\nfrom transformers import CLIPVisionModel, CLIPImageProcessor, StoppingCriteria\nfrom higpt.model import *\nfrom higpt.model.utils import KeywordsStoppingCriteria\nfrom torch_geometric.data import Data\nimport json\nimport copy\nfrom higpt.model.meta_hgt import MetaHGTConvCfg, MetaHGTConv\nimport torch.nn as nn\nfrom transformers.configuration_utils import PretrainedConfig\n\nimport os\nimport requests\nfrom PIL import Image\nfrom io import BytesIO\n\nfrom tqdm import tqdm\nimport json\nimport os.path as osp\n\nimport ray\n\n# os.environ['PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION'] = 'python'\n\nDEFAULT_GRAPH_TOKEN = \"<graph>\"\nDEFAULT_GRAPH_PATCH_TOKEN = \"<g_patch>\"\nDEFAULT_G_START_TOKEN = \"<g_start>\"\nDEFAULT_G_END_TOKEN = \"<g_end>\"\n\nnode_feas_dict_dblp = torch.load('./higpt/model/meta_hgt/meta_dict/dblp/node_type.pt')\nfor k in node_feas_dict_dblp.keys():\n    node_feas_dict_dblp[k] = torch.Tensor(node_feas_dict_dblp[k])\nedge_feas_dict_dblp = torch.load('./higpt/model/meta_hgt/meta_dict/dblp/edge_type.pt')\nfor k in edge_feas_dict_dblp.keys():\n    edge_feas_dict_dblp[k] = torch.Tensor(edge_feas_dict_dblp[k])\n\nnode_feas_dict_acm = torch.load('./higpt/model/meta_hgt/meta_dict/acm/node_type.pt')\nfor k in node_feas_dict_acm.keys():\n    node_feas_dict_acm[k] = torch.Tensor(node_feas_dict_acm[k])\nedge_feas_dict_acm = torch.load('./higpt/model/meta_hgt/meta_dict/acm/edge_type.pt')\nfor k in node_feas_dict_acm.keys():\n    node_feas_dict_acm[k] = torch.Tensor(node_feas_dict_acm[k])\n\nnode_feas_dict_imdb = torch.load('./higpt/model/meta_hgt/meta_dict/imdb/node_type.pt')\nfor k in node_feas_dict_imdb.keys():\n    node_feas_dict_imdb[k] = torch.Tensor(node_feas_dict_imdb[k])\nedge_feas_dict_imdb = torch.load('./higpt/model/meta_hgt/meta_dict/imdb/edge_type.pt')\nfor k in node_feas_dict_imdb.keys():\n    node_feas_dict_imdb[k] = torch.Tensor(node_feas_dict_imdb[k])\n\nclass HGT(nn.Module):\n    \n    def __init__(\n        self,\n    ):\n        super().__init__()\n        self.config = PretrainedConfig()\n\ndef load_graph(instruct_item, graph_root): \n    graph_dict_list = []\n    cur_token_lens = []\n    # hetero_key_order = instruct_item['graph']['keys_order']\n\n    hetero_key_orders = []\n\n    context_graphs = instruct_item['context_graph']\n    for cg_dict in context_graphs: \n        cg_path = osp.join(graph_root, cg_dict['graph'])\n        graph_dict = torch.load(cg_path)\n        if 'subject' in graph_dict.x_dict.keys(): \n            graph_dict['edge_feas_dict'] = edge_feas_dict_acm\n            graph_dict['node_feas_dict'] = node_feas_dict_acm\n        elif 'movie' in graph_dict.x_dict.keys(): \n            graph_dict['edge_feas_dict'] = edge_feas_dict_imdb\n            graph_dict['node_feas_dict'] = node_feas_dict_imdb\n        elif 'paper' in graph_dict.x_dict.keys(): \n            graph_dict['edge_feas_dict'] = edge_feas_dict_dblp\n            graph_dict['node_feas_dict'] = node_feas_dict_dblp\n            new_conf_reps = torch.ones(graph_dict['conference'].num_nodes, 768)\n            graph_dict['conference'].x = new_conf_reps\n        else: \n            raise NotImplementedError\n        graph_dict_list.append(graph_dict)\n        hetero_key_order = cg_dict['keys_order']\n        hetero_key_orders.append(hetero_key_order)\n        for key in hetero_key_order:\n            cur_token_lens.append(graph_dict.x_dict[key].shape[0])\n    graph_path = osp.join(graph_root, instruct_item['graph']['graph'])\n    graph_dict = torch.load(graph_path)\n    if 'subject' in graph_dict.x_dict.keys(): \n        graph_dict['edge_feas_dict'] = edge_feas_dict_acm\n        graph_dict['node_feas_dict'] = node_feas_dict_acm\n    elif 'movie' in graph_dict.x_dict.keys(): \n        graph_dict['edge_feas_dict'] = edge_feas_dict_imdb\n        graph_dict['node_feas_dict'] = node_feas_dict_imdb\n    elif 'paper' in graph_dict.x_dict.keys(): \n        graph_dict['edge_feas_dict'] = edge_feas_dict_dblp\n        graph_dict['node_feas_dict'] = node_feas_dict_dblp\n        new_conf_reps = torch.ones(graph_dict['conference'].num_nodes, 768)\n        graph_dict['conference'].x = new_conf_reps\n    else: \n        raise NotImplementedError\n    hetero_key_order = instruct_item['graph']['keys_order']\n    hetero_key_orders.append(hetero_key_order)\n    for key in hetero_key_order:\n        cur_token_lens.append(graph_dict.x_dict[key].shape[0])\n    graph_dict_list.append(graph_dict)\n        \n\n    return {\n        'graph_data': graph_dict_list, \n        'graph_token_len': cur_token_lens, \n        'hetero_key_orders': hetero_key_orders\n    }\n\n\ndef load_prompting_file(file_path): \n    with open(file_path, 'r', encoding='utf-8') as f:\n        data = json.load(f)\n    return data\n\n# def prepare_query(instruct_item): \n\n\ndef run_eval(args, num_gpus):\n    # split question file into num_gpus files\n    prompt_file = load_prompting_file(args.prompting_file)\n    prompt_file = prompt_file[args.start_id:args.end_id]\n    chunk_size = len(prompt_file) // num_gpus\n    ans_handles = []\n    split_list = list(range(args.start_id, args.end_id, chunk_size))\n    idx_list = list(range(0, len(prompt_file), chunk_size))\n    if len(split_list) == num_gpus: \n        split_list.append(args.end_id)\n        idx_list.append(len(prompt_file))\n    elif len(split_list) == num_gpus + 1: \n        split_list[-1] = args.end_id\n        idx_list[-1] = len(prompt_file)\n    else: \n        raise ValueError('error in the number of list')\n\n    if osp.exists(args.output_res_path) is False: \n        os.makedirs(args.output_res_path, exist_ok = True)\n    \n    for idx in range(len(idx_list) - 1):\n        start_idx = idx_list[idx]\n        end_idx = idx_list[idx + 1]\n        \n        start_split = split_list[idx]\n        end_split = split_list[idx + 1]\n        ans_handles.append(\n            eval_model.remote(\n                args, prompt_file[start_idx:end_idx], start_split, end_split\n            )\n        )\n\n    ans_jsons = []\n    for ans_handle in ans_handles:\n        ans_jsons.extend(ray.get(ans_handle))\n\n    # with open(args.output_res_path, \"w\") as ans_file:\n    #     for line in ans_jsons:\n    #         ans_file.write(json.dumps(line) + \"\\n\")\n\n\n@ray.remote(num_gpus=1)\n@torch.inference_mode()\ndef eval_model(args, prompt_file, start_idx, end_idx):\n    # load prompting file\n    # prompt_file = load_prompting_file(args.prompting_file)\n\n\n    # Model\n    disable_torch_init()\n    # model_name = os.path.expanduser(args.model_name)\n    print('start loading')\n    tokenizer = AutoTokenizer.from_pretrained(args.model_name)\n    print('finish loading')\n\n    print('start loading')\n    model = HeteroLlamaForCausalLM.from_pretrained(args.model_name, torch_dtype=torch.float32, use_cache=True, low_cpu_mem_usage=True)\n    print('finish loading')\n\n    use_graph_start_end = getattr(model.config, \"use_graph_start_end\", False)\n    tokenizer.add_tokens([DEFAULT_GRAPH_PATCH_TOKEN], special_tokens=True)\n    if use_graph_start_end:\n        tokenizer.add_tokens([DEFAULT_G_START_TOKEN, DEFAULT_G_END_TOKEN], special_tokens=True)\n\n    graph_tower = model.get_model().graph_tower\n    \n    # TODO: add graph tower\n    # if graph_tower.device.type == 'meta':\n    #     print('meta')\n    graph_tower= load_metahgt_pretrained(MetaHGTConv, './MetaHGT_imdb_dblp_epoch5')\n    new_graph_tower = HGT()\n    new_graph_tower.config = graph_tower.config\n    \n    model.get_model().graph_tower = new_graph_tower.cuda()\n    # else:\n    #     print('other')\n    # print(next(graph_tower.parameters()).dtype)\n    # graph_tower.to(device='cuda', dtype=torch.float32)\n    model = model.cuda()\n    graph_config = graph_tower.config\n    graph_config.graph_patch_token = tokenizer.convert_tokens_to_ids([DEFAULT_GRAPH_PATCH_TOKEN])[0]\n    graph_config.use_graph_start_end = use_graph_start_end\n    if use_graph_start_end:\n        graph_config.graph_start_token, graph_config.graph_end_token = tokenizer.convert_tokens_to_ids([DEFAULT_G_START_TOKEN, DEFAULT_G_END_TOKEN])\n    # TODO: add graph token len\n\n    res_data = []\n    print(f'total: {len(prompt_file)}')\n    for idx, instruct_item in tqdm(enumerate(prompt_file)):\n        # instruct_item = prompt_file[0]\n        # if idx >= 3: \n        #     break\n        graph_dict = load_graph(instruct_item, args.graph_root)\n        graph_token_len = graph_dict['graph_token_len']\n        graph_data = graph_dict['graph_data']\n        hetero_key_orders = graph_dict['hetero_key_orders']\n\n        qs = instruct_item[\"conversations\"][0][\"value\"]\n        \n        if DEFAULT_GRAPH_TOKEN in qs:\n            # build replace_tokens\n            replace_tokens = []\n            for i, token_len in enumerate(graph_token_len):\n                replace_token = DEFAULT_GRAPH_PATCH_TOKEN * token_len\n                if use_graph_start_end:\n                    replace_token = DEFAULT_G_START_TOKEN + replace_token + DEFAULT_G_END_TOKEN\n                replace_tokens.append(replace_token)\n\n            for i, replace_token in enumerate(replace_tokens):\n                index = qs.find(DEFAULT_GRAPH_TOKEN)\n                qs = qs[:index] + replace_token + qs[index+len(DEFAULT_GRAPH_TOKEN):]\n        conv_mode = \"graphchat_v1\"\n\n        if args.conv_mode is not None and conv_mode != args.conv_mode:\n            print('[WARNING] the auto inferred conversation mode is {}, while `--conv-mode` is {}, using {}'.format(conv_mode, args.conv_mode, args.conv_mode))\n        else:\n            args.conv_mode = conv_mode\n\n        conv = conv_templates[args.conv_mode].copy()\n        conv.append_message(conv.roles[0], qs)\n        conv.append_message(conv.roles[1], None)\n        prompt = conv.get_prompt()\n        inputs = tokenizer([prompt])\n\n        \n\n        input_ids = torch.as_tensor(inputs.input_ids).cuda()\n\n        stop_str = conv.sep if conv.sep_style != SeparatorStyle.TWO else conv.sep2\n        keywords = [stop_str]\n        stopping_criteria = KeywordsStoppingCriteria(keywords, tokenizer, input_ids)\n\n        # graph_data.graph_node = graph_data.graph_node.to(torch.float16)\n        # graph_data.edge_index = graph_data.edge_index.to(torch.float16)\n        for idx in range(len(graph_data)):\n            graph_data[idx].cuda()\n\n        with torch.inference_mode():\n            output_ids = model.generate(\n                input_ids,\n                graph_data=graph_data,\n                hetero_key_order = hetero_key_orders, \n                do_sample=True,\n                temperature=0.2,\n                max_new_tokens=1024,\n                stopping_criteria=[stopping_criteria])\n\n        input_token_len = input_ids.shape[1]\n        n_diff_input_output = (input_ids != output_ids[:, :input_token_len]).sum().item()\n        if n_diff_input_output > 0:\n            print(f'[Warning] {n_diff_input_output} output_ids are not the same as the input_ids')\n        outputs = tokenizer.batch_decode(output_ids[:, input_token_len:], skip_special_tokens=True)[0]\n        outputs = outputs.strip()\n        if outputs.endswith(stop_str):\n            outputs = outputs[:-len(stop_str)]\n        outputs = outputs.strip()\n        # print(outputs)\n\n        res_data.append({\"id\": instruct_item[\"id\"], \"node_idx\": instruct_item[\"graph\"][\"node_idx\"], \"res\": outputs}.copy())\n        with open(osp.join(args.output_res_path, 'arxiv_test_res_{}_{}.json'.format(start_idx, end_idx)), \"w\") as fout:\n            json.dump(res_data, fout, indent=4)\n    return res_data\n    # with open(args.output_res_path, \"w\") as fout:\n    #     json.dump(res_data, fout, indent=4)\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--model-name\", type=str, default=\"facebook/opt-350m\")\n    # parser.add_argument(\"--image-file\", type=str, required=True)\n    # parser.add_argument(\"--query\", type=str, required=True)\n    parser.add_argument(\"--prompting_file\", type=str, default=None)\n    parser.add_argument(\"--conv-mode\", type=str, default=None)\n    parser.add_argument(\"--graph_root\", type=str, default=None)\n\n    parser.add_argument(\"--output_res_path\", type=str, default=None)\n    parser.add_argument(\"--num_gpus\", type=int, default=4)\n\n    parser.add_argument(\"--start_id\", type=int, default=0)\n    parser.add_argument(\"--end_id\", type=int, default=1000)\n\n    args = parser.parse_args()\n\n    # eval_model(args)\n\n    ray.init()\n    run_eval(args, args.num_gpus)\n\n\n# protobuf             4.22.3"}
{"type": "source_file", "path": "higpt/model/apply_lora.py", "content": "\"\"\"\nApply the LoRA weights on top of a base model.\n\nUsage:\npython3 -m fastchat.model.apply_lora --base ~/model_weights/llama-7b --target ~/model_weights/baize-7b --lora project-baize/baize-lora-7B\n\nDependency:\npip3 install git+https://github.com/huggingface/peft.git@2822398fbe896f25d4dac5e468624dc5fd65a51b\n\"\"\"\nimport argparse\n\nimport torch\nfrom peft import PeftModel\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\n\ndef apply_lora(base_model_path, target_model_path, lora_path):\n    print(f\"Loading the base model from {base_model_path}\")\n    base = AutoModelForCausalLM.from_pretrained(\n        base_model_path, torch_dtype=torch.float16, low_cpu_mem_usage=True\n    )\n    base_tokenizer = AutoTokenizer.from_pretrained(base_model_path, use_fast=False)\n\n    print(f\"Loading the LoRA adapter from {lora_path}\")\n\n    lora_model = PeftModel.from_pretrained(\n        base,\n        lora_path,\n        torch_dtype=torch.float16,\n    )\n\n    print(\"Applying the LoRA\")\n    model = lora_model.merge_and_unload()\n\n    print(f\"Saving the target model to {target_model_path}\")\n    model.save_pretrained(target_model_path)\n    base_tokenizer.save_pretrained(target_model_path)\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--base-model-path\", type=str, required=True)\n    parser.add_argument(\"--target-model-path\", type=str, required=True)\n    parser.add_argument(\"--lora-path\", type=str, required=True)\n\n    args = parser.parse_args()\n\n    apply_lora(args.base_model_path, args.target_model_path, args.lora_path)\n"}
{"type": "source_file", "path": "higpt/model/HeteroLlama_pl.py", "content": "import os\nimport random\nfrom typing import Any, Optional, Dict, List, Union\nimport logging\nimport torch\nfrom lightning.pytorch import LightningModule\nfrom transformers import get_cosine_schedule_with_warmup\nfrom torch.optim import AdamW\nimport torch.nn as nn\nfrom higpt.model.HeteroLlama import HeteroLlamaForCausalLM\nimport transformers\nimport numpy as np\n\ndef find_all_linear_names(model):\n    cls = torch.nn.Linear\n    lora_module_names = set()\n    for name, module in model.named_modules():\n        if isinstance(module, cls):\n            names = name.split('.')\n            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n\n\n    if 'lm_head' in lora_module_names: # needed for 16-bit\n        lora_module_names.remove('lm_head')\n    return list(lora_module_names)\n\nclass HeteroGPT_pl(LightningModule): \n    def __init__(self,\n        training_args, model_args, data_args, tokenizer, \n        **kwargs,\n    ):\n        super().__init__()\n        self.training_args = training_args\n        self.model_args = model_args\n        self.data_args = data_args\n        compute_dtype = (torch.float16 if training_args.fp16 else (torch.bfloat16 if training_args.bf16 else torch.float32))\n\n        bnb_model_from_pretrained_args = {}\n\n    ## load 4 8 bit \n        if training_args.bits in [4, 8]:\n            from transformers import BitsAndBytesConfig\n            from peft import prepare_model_for_int8_training\n            bnb_model_from_pretrained_args.update(dict(\n                device_map={\"\": training_args.device},\n                load_in_4bit=training_args.bits == 4,\n                load_in_8bit=training_args.bits == 8,\n                quantization_config=BitsAndBytesConfig(\n                    load_in_4bit=training_args.bits == 4,\n                    load_in_8bit=training_args.bits == 8,\n                    llm_int8_threshold=6.0,\n                    llm_int8_has_fp16_weight=False,\n                    bnb_4bit_compute_dtype=compute_dtype,\n                    bnb_4bit_use_double_quant=training_args.double_quant,\n                    bnb_4bit_quant_type=training_args.quant_type # {'fp4', 'nf4'}\n                )\n            ))\n\n        if model_args.graph_tower is not None:\n            self.model = HeteroLlamaForCausalLM.from_pretrained(\n                    model_args.model_name_or_path,\n                    cache_dir=training_args.cache_dir,\n\n                    **bnb_model_from_pretrained_args\n                ) ## TODO: add real Graph Llama model \n        else:\n            self.model = transformers.LlamaForCausalLM.from_pretrained(\n                model_args.model_name_or_path,\n                cache_dir=training_args.cache_dir,\n                **bnb_model_from_pretrained_args\n            )\n        self.model.config.pretrain_graph_model_path = self.model.config.pretrain_graph_model_path + model_args.graph_tower\n        self.model.config.use_cache = False\n        if model_args.freeze_backbone:\n            self.model.model.requires_grad_(False)\n\n        if training_args.bits in [4, 8]:\n            self.model.config.torch_dtype=(torch.float32 if training_args.fp16 else (torch.bfloat16 if training_args.bf16 else torch.float32))\n            self.model = prepare_model_for_int8_training(self.model, use_gradient_checkpointing=training_args.gradient_checkpointing)\n\n        if training_args.gradient_checkpointing and model_args.graph_tower is None:\n            if hasattr(self.model, \"enable_input_require_grads\"):\n                self.model.enable_input_require_grads()\n            else:\n                def make_inputs_require_grad(module, input, output):\n                    output.requires_grad_(True)\n                self.model.get_input_embeddings().register_forward_hook(make_inputs_require_grad)\n\n        if training_args.lora_enable:\n            from peft import LoraConfig, get_peft_model\n            lora_config = LoraConfig(\n                r=training_args.lora_r,\n                lora_alpha=training_args.lora_alpha,\n                target_modules=find_all_linear_names(model),\n                lora_dropout=training_args.lora_dropout,\n                bias=training_args.lora_bias,\n                task_type=\"CAUSAL_LM\",\n            )\n            if training_args.bits == 16:\n                if training_args.bf16:\n                    model.to(torch.bfloat16)\n                if training_args.fp16:\n                    model.to(torch.float16)\n            logging.warning(\"Adding LoRA adapters...\")\n            model = get_peft_model(model, lora_config)\n        \n        if model_args.graph_tower is not None:\n            model_graph_dict = self.model.get_model().initialize_graph_modules(\n                graph_tower=model_args.graph_tower,\n                graph_select_layer=model_args.graph_select_layer,\n                pretrain_graph_mlp_adapter=model_args.pretrain_graph_mlp_adapter,\n                fsdp=None\n            )\n            self.model.get_graph_tower().to(dtype=compute_dtype)\n            # graph_config = model_graph_dict['graph_config']\n\n            # data_args.graph_token_len = model_graph_dict['graph_token_len']\n            # data_args.graph_processor = model_graph_dict['graph_processor']\n            data_args.is_graph = True\n\n            self.model.config.tune_graph_mlp_adapter = training_args.tune_graph_mlp_adapter = model_args.tune_graph_mlp_adapter\n            if model_args.tune_graph_mlp_adapter:\n                self.model.requires_grad_(False)\n                for p in self.model.get_model().graph_projector.parameters():\n                    p.requires_grad = True\n                if model_args.tune_gnn:\n                    for p in self.model.get_model().graph_tower.parameters():\n                        p.requires_grad = True\n                \n\n            self.model.config.freeze_graph_mlp_adapter = training_args.freeze_graph_mlp_adapter\n            if training_args.freeze_graph_mlp_adapter:\n                for p in self.model.get_model().graph_projector.parameters():\n                    p.requires_grad = False\n\n            if training_args.bits in [4, 8]:\n                self.model.get_model().graph_projector.to(dtype=compute_dtype, device=training_args.device)\n\n            self.model.config.use_graph_start_end = data_args.use_graph_start_end = model_args.use_graph_start_end\n            # graph_config.use_graph_start_end = training_args.use_graph_start_end = model_args.use_graph_start_end\n            training_args.use_graph_start_end = model_args.use_graph_start_end\n            self.model.config.sep_graph_conv_front = data_args.sep_graph_conv_front\n            self.model.initialize_graph_tokenizer(use_graph_start_end=model_args.use_graph_start_end, tokenizer=tokenizer, device='cuda',\n                                            tune_graph_mlp_adapter=model_args.tune_graph_mlp_adapter, pretrain_graph_mlp_adapter=model_args.pretrain_graph_mlp_adapter)\n\n            params_no_grad = [n for n, p in self.model.named_parameters() if not p.requires_grad]\n            if training_args.bits in [4, 8]:\n                from peft.tuners.lora import LoraLayer\n                for name, module in self.model.named_modules():\n                    if isinstance(module, LoraLayer):\n                        if training_args.bf16:\n                            module = module.to(torch.bfloat16)\n                    if 'norm' in name:\n                        module = module.to(torch.float32)\n                    if 'lm_head' in name or 'embed_tokens' in name:\n                        if hasattr(module, 'weight'):\n                            if training_args.bf16 and module.weight.dtype == torch.float32:\n                                module = module.to(torch.bfloat16)\n\n            print('************************** parameters: #', sum(p.numel() for p in self.model.parameters() if p.requires_grad))\n            tuned_params = []\n            for name, param in self.model.named_parameters():\n                if param.requires_grad:\n                    tuned_params.append(name)\n            print(tuned_params)\n        \n    def training_step(self, batch, batch_idx):\n        bs = len(batch[\"input_ids\"])\n        loss_dict = self.model(**batch)\n        loss = loss_dict['loss']\n        if np.isnan(loss.item()):\n            print(batch['input_ids'])\n            print(batch['labels'])\n            raise ValueError('loss is nan')\n        log_dict = {f'train_loss': loss.item()}\n        self.log_dict(log_dict, on_step=True, on_epoch=True, prog_bar=True, sync_dist=True, batch_size=bs)\n        return loss\n\n    def configure_optimizers(self):\n        \"\"\"Prepare optimizer and schedule (linear warmup and decay)\"\"\"\n        # no_decay = [\"bias\", \"LayerNorm.weight\"]\n        # if IS_STAGE2:\n        \n        # optimizer_grouped_parameters = [\n        #     {\n        #         \"params\": [p for n, p in self.model.named_parameters()], \"lr_scale\": [1e-5, 1e-4]\n        #     }\n        # ]\n\n        optimizer_grouped_parameters = [\n            {\n                \"params\": [p for n, p in self.model.named_parameters() if p.requires_grad],\n            }\n        ]\n        \n        optimizer = AdamW(optimizer_grouped_parameters, lr=self.training_args.learning_rate, eps=1e-3)\n\n        # scheduler = get_linear_schedule_with_warmup(\n        #     optimizer,\n        #     num_warmup_steps=self.training_args.warmup_steps,\n        #     num_training_steps=self.trainer.estimated_stepping_batches,\n        # )\n        scheduler = get_cosine_schedule_with_warmup(\n            optimizer,\n            num_warmup_steps=self.training_args.warmup_steps,\n            num_training_steps=self.trainer.estimated_stepping_batches,\n        )\n        scheduler = {\"scheduler\": scheduler, \"interval\": \"step\", \"frequency\": 1}\n        return [optimizer], [scheduler]\n    # def configure_gradient_clipping(\n    #         self,\n    #         optimizer,\n    #         optimizer_idx: int,\n    #         gradient_clip_val: Optional[Union[int, float]] = None,\n    #         gradient_clip_algorithm: Optional[str] = None,\n    # ):\n    #     assert gradient_clip_algorithm in ('norm', None), gradient_clip_algorithm\n    #     self.model.clip_grad_norm_(gradient_clip_val)\n    # def on_save_checkpoint(self, checkpoint: Dict[str, Any]) -> None:\n    #     trainable_param_names = [n for n, p in self.named_parameters() if p.requires_grad]\n    #     # remove untrainable params\n    #     print('trainable_param_names:', trainable_param_names)\n    #     for k in list(checkpoint[\"state_dict\"].keys()):\n    #         if k not in trainable_param_names:\n    #             del checkpoint[\"state_dict\"][k]\n    # def on_train_batch_end(self, outputs, batch: Any, batch_idx: int) -> None:\n    #     print('*'*20, 'grad not none', '*'*20)\n    #     for n, p in self.named_parameters():\n    #         if p.grad is not None:\n    #             print(n)\n        # pass"}
{"type": "source_file", "path": "higpt/model/MetaHGTConv_pl.py", "content": "import os\nimport random\nfrom typing import Any, Optional, Dict, List\nimport logging\nimport torch\nfrom lightning.pytorch import LightningModule\nfrom transformers import get_cosine_schedule_with_warmup\nfrom torch.optim import AdamW\nimport torch.nn as nn\nfrom higpt.model.HeteroLlama import HeteroLlamaForCausalLM\nimport transformers\nimport numpy as np\nfrom higpt.model.meta_hgt import MetaHGTConvCfg, MetaHGTConv\nimport os.path as osp\nimport json\nimport glob\nfrom higpt.model.heteclip_models import Transformer, LayerNorm, CLIPTextCfg\n\ndef load_metahgt_pretrained(model_name, pretrain_model_path): \n    # load conig json\n    \n    assert osp.exists(osp.join(pretrain_model_path, 'graph_config.json')), 'graph_config.json missing'\n    with open(osp.join(pretrain_model_path, 'graph_config.json'), 'r') as f:\n        graph_config_dict = json.load(f)\n    graph_cfg = MetaHGTConvCfg(**graph_config_dict)\n\n    assert osp.exists(osp.join(pretrain_model_path, 'text_config.json')), 'text_config.json missing'\n    with open(osp.join(pretrain_model_path, 'text_config.json'), 'r') as f:\n        text_config_dict = json.load(f)\n    text_cfg = CLIPTextCfg(**text_config_dict)\n    \n    assert model_name == MetaHGTConv\n    model = model_name(in_channels = graph_cfg.in_channels,\n        out_channels = graph_cfg.out_channels,\n        heads = graph_cfg.heads,\n        dynamic = graph_cfg.dynamic,\n        text_transformer = Transformer, \n        text_cfg = text_cfg, \n        layernorm = LayerNorm)\n\n    pkl_files = glob.glob(osp.join(pretrain_model_path, '*.ckpt'))\n    state_dict = torch.load(pkl_files[0], map_location = 'cpu')['state_dict']\n    print('loading graph pre train model ...')\n    gnn_state_dict = {}\n    for key, value in state_dict.items():\n        if key.startswith('model.graph_encoder'):\n            new_key = key.split('model.graph_encoder.')[1]\n            gnn_state_dict[new_key] = value\n    model.load_state_dict(gnn_state_dict)\n\n    return model\n\nclass MetaHGT_pl(LightningModule): \n    def __init__(self,\n        \n    ):\n        super().__init__()\n        self.model = load_metahgt_pretrained(MetaHGTConv, '/root/paddlejob/workspace/env_run/output/HeteGPT/MetaHGT')\n        "}
{"type": "source_file", "path": "higpt/model/compression.py", "content": "import dataclasses\nimport gc\nimport glob\nimport os\n\nfrom accelerate import init_empty_weights\nfrom accelerate.utils import set_module_tensor_to_device\nimport torch\nfrom torch import Tensor\nimport torch.nn as nn\nfrom torch.nn import functional as F\nfrom tqdm import tqdm\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig\n\n\n@dataclasses.dataclass\nclass CompressionConfig:\n    \"\"\"Group-wise quantization.\"\"\"\n\n    num_bits: int\n    group_size: int\n    group_dim: int\n    symmetric: bool\n    enabled: bool = True\n\n\ndefault_compression_config = CompressionConfig(\n    num_bits=8, group_size=256, group_dim=1, symmetric=True, enabled=True\n)\n\n\nclass CLinear(nn.Module):\n    \"\"\"Compressed Linear Layer.\"\"\"\n\n    def __init__(self, weight=None, bias=None, device=None):\n        super().__init__()\n        if weight is None:\n            self.weight = None\n        elif isinstance(weight, Tensor):\n            self.weight = compress(weight.data.to(device), default_compression_config)\n        else:\n            self.weight = weight\n        self.bias = bias\n\n    def forward(self, input: Tensor) -> Tensor:\n        weight = decompress(self.weight, default_compression_config)\n        return F.linear(input.to(weight.dtype), weight, self.bias)\n\n\ndef compress_module(module, target_device):\n    for attr_str in dir(module):\n        target_attr = getattr(module, attr_str)\n        if type(target_attr) == torch.nn.Linear:\n            setattr(\n                module,\n                attr_str,\n                CLinear(target_attr.weight, target_attr.bias, target_device),\n            )\n    for name, child in module.named_children():\n        compress_module(child, target_device)\n\n\ndef get_compressed_list(module, prefix=\"\"):\n    compressed_list = []\n    for attr_str in dir(module):\n        target_attr = getattr(module, attr_str)\n        if type(target_attr) == torch.nn.Linear:\n            full_name = (\n                f\"{prefix}.{attr_str}.weight\" if prefix else f\"{attr_str}.weight\"\n            )\n            compressed_list.append(full_name)\n    for name, child in module.named_children():\n        child_prefix = f\"{prefix}.{name}\" if prefix else name\n        for each in get_compressed_list(child, child_prefix):\n            compressed_list.append(each)\n    return compressed_list\n\n\ndef apply_compressed_weight(module, compressed_state_dict, target_device, prefix=\"\"):\n    for attr_str in dir(module):\n        target_attr = getattr(module, attr_str)\n        if type(target_attr) == torch.nn.Linear:\n            full_name = (\n                f\"{prefix}.{attr_str}.weight\" if prefix else f\"{attr_str}.weight\"\n            )\n            setattr(\n                module,\n                attr_str,\n                CLinear(\n                    compressed_state_dict[full_name], target_attr.bias, target_device\n                ),\n            )\n    for name, child in module.named_children():\n        child_prefix = f\"{prefix}.{name}\" if prefix else name\n        apply_compressed_weight(\n            child, compressed_state_dict, target_device, child_prefix\n        )\n\n\ndef load_compress_model(model_path, device, torch_dtype):\n    # partially load model\n    tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)\n    base_pattern = os.path.join(model_path, \"pytorch_model-*.bin\")\n    files = glob.glob(base_pattern)\n\n    with init_empty_weights():\n        config = AutoConfig.from_pretrained(\n            model_path, low_cpu_mem_usage=True, torch_dtype=torch_dtype\n        )\n        model = AutoModelForCausalLM.from_config(config)\n        linear_weights = get_compressed_list(model)\n\n    compressed_state_dict = {}\n\n    for filename in tqdm(files):\n        tmp_state_dict = torch.load(filename)\n        for name in tmp_state_dict:\n            if name in linear_weights:\n                tensor = tmp_state_dict[name].to(device).data.to(torch_dtype)\n                compressed_state_dict[name] = compress(\n                    tensor, default_compression_config\n                )\n            else:\n                compressed_state_dict[name] = tmp_state_dict[name].to(device)\n            tmp_state_dict[name] = None\n            tensor = None\n            gc.collect()\n            torch.cuda.empty_cache()\n\n    for name in model.state_dict():\n        if name not in linear_weights:\n            set_module_tensor_to_device(\n                model, name, device, value=compressed_state_dict[name]\n            )\n    apply_compressed_weight(model, compressed_state_dict, device)\n\n    model.to(device)\n\n    return model, tokenizer\n\n\ndef compress(tensor, config):\n    \"\"\"Simulate group-wise quantization.\"\"\"\n    if not config.enabled:\n        return tensor\n\n    group_size, num_bits, group_dim, symmetric = (\n        config.group_size,\n        config.num_bits,\n        config.group_dim,\n        config.symmetric,\n    )\n    assert num_bits <= 8\n\n    original_shape = tensor.shape\n    num_groups = (original_shape[group_dim] + group_size - 1) // group_size\n    new_shape = (\n        original_shape[:group_dim]\n        + (num_groups, group_size)\n        + original_shape[group_dim + 1 :]\n    )\n\n    # Pad\n    pad_len = (group_size - original_shape[group_dim] % group_size) % group_size\n    if pad_len != 0:\n        pad_shape = (\n            original_shape[:group_dim] + (pad_len,) + original_shape[group_dim + 1 :]\n        )\n        tensor = torch.cat(\n            [tensor, torch.zeros(pad_shape, dtype=tensor.dtype, device=tensor.device)],\n            dim=group_dim,\n        )\n    data = tensor.view(new_shape)\n\n    # Quantize\n    if symmetric:\n        B = 2 ** (num_bits - 1) - 1\n        scale = B / torch.max(data.abs(), dim=group_dim + 1, keepdim=True)[0]\n        data = data * scale\n        data = data.clamp_(-B, B).round_().to(torch.int8)\n        return data, scale, original_shape\n    else:\n        B = 2**num_bits - 1\n        mn = torch.min(data, dim=group_dim + 1, keepdim=True)[0]\n        mx = torch.max(data, dim=group_dim + 1, keepdim=True)[0]\n\n        scale = B / (mx - mn)\n        data = data - mn\n        data.mul_(scale)\n\n        data = data.clamp_(0, B).round_().to(torch.uint8)\n        return data, mn, scale, original_shape\n\n\ndef decompress(packed_data, config):\n    \"\"\"Simulate group-wise dequantization.\"\"\"\n    if not config.enabled:\n        return packed_data\n\n    group_size, num_bits, group_dim, symmetric = (\n        config.group_size,\n        config.num_bits,\n        config.group_dim,\n        config.symmetric,\n    )\n\n    # Dequantize\n    if symmetric:\n        data, scale, original_shape = packed_data\n        data = data / scale\n    else:\n        data, mn, scale, original_shape = packed_data\n        data = data / scale\n        data.add_(mn)\n\n    # Unpad\n    pad_len = (group_size - original_shape[group_dim] % group_size) % group_size\n    if pad_len:\n        padded_original_shape = (\n            original_shape[:group_dim]\n            + (original_shape[group_dim] + pad_len,)\n            + original_shape[group_dim + 1 :]\n        )\n        data = data.reshape(padded_original_shape)\n        indices = [slice(0, x) for x in original_shape]\n        return data[indices].contiguous()\n    else:\n        return data.view(original_shape)\n"}
{"type": "source_file", "path": "higpt/model/__init__.py", "content": "from higpt.model.model_adapter import (\n    load_model,\n    get_conversation_template,\n    add_model_args,\n)\n\nfrom higpt.model.GraphLlama import GraphLlamaForCausalLM, load_model_pretrained, transfer_param_tograph\nfrom higpt.model.graph_layers.clip_graph import GNN, graph_transformer, CLIP\nfrom higpt.model.HeteroLlama import HeteroLlamaForCausalLM, load_metahgt_pretrained\n"}
{"type": "source_file", "path": "higpt/model/GraphLlama.py", "content": "#    Copyright 2023 Haotian Liu\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\");\n#    you may not use this file except in compliance with the License.\n#    You may obtain a copy of the License at\n#\n#        http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS,\n#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#    See the License for the specific language governing permissions and\n#    limitations under the License.\n\n\nfrom typing import List, Optional, Tuple, Union\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn import CrossEntropyLoss\n\nfrom transformers import AutoConfig, AutoModelForCausalLM, \\\n                         LlamaConfig, LlamaModel, LlamaForCausalLM, \\\n                         CLIPVisionModel, CLIPImageProcessor\n\nfrom transformers.modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n\nfrom higpt.model.graph_layers import MPNN, GNN, CLIP, graph_transformer\nfrom torch_geometric.data import Data\nimport json\nimport os.path as osp\nimport glob\n\nDEFAULT_GRAPH_TOKEN = \"<graph>\"\nDEFAULT_GRAPH_PATCH_TOKEN = \"<g_patch>\"\nDEFAULT_G_START_TOKEN = \"<g_start>\"\nDEFAULT_G_END_TOKEN = \"<g_end>\"\n\n\nclass GraphLlamaConfig(LlamaConfig):\n    model_type = \"GraphLlama\"\n\nclass GraphPretrainConfig:\n    def __init__(self, dictionary):\n        for key, value in dictionary.items():\n            setattr(self, key, value)\n\ndef load_model_pretrained(model_name, pretrain_model_path): \n    # load conig json\n    \n    assert osp.exists(osp.join(pretrain_model_path, 'config.json')), 'config.json missing'\n    with open(osp.join(pretrain_model_path, 'config.json'), 'r') as f:\n        config_dict = json.load(f)\n    args = GraphPretrainConfig(config_dict)\n    model = model_name(args)\n    pkl_files = glob.glob(osp.join(pretrain_model_path, '*.pkl'))\n    state_dict = torch.load(pkl_files[0])\n    # print(state_dict.keys())\n    if 'logit_scale' in state_dict.keys(): \n        state_dict.pop('logit_scale')\n    print('loading graph pre train model')\n    model.load_state_dict(state_dict)\n\n    # model.load_state_dict(torch.load('/root/paddlejob/workspace/env_run/llm/GraphChat/clip_gt/node_ttgt_8and12_0.1_gt_no_pos.pkl'))\n\n    # model = torch.load(pkl_files[0])\n\n    return model, args\ndef transfer_param_tograph(clip_graph, gnn):\n    # state_dict = model.state_dict()\n    #print('new_state_dict',new_state_dict)\n    # matched_layers = 0\n    # unmatched_layers = []\n    # for name, param in gnn.state_dict().items():        \n    #     # if exclude_head and 'head' in name: continue\n    #     if name in clip_graph:            \n    #         matched_layers += 1\n    #         input_param = clip_graph[name]\n    #         print(f'name: {name}, inp: {input_param.shape}, para: {param.shape}')\n    #         if input_param.shape == param.shape: param.copy_(input_param)\n    #         else: unmatched_layers.append(name)\n    #     else:\n    #         unmatched_layers.append(name)\n    #         pass # these are weights that weren't in the original model, such as a new head\n    # if matched_layers == 0: raise Exception(\"No shared weight names were found between the models\")\n    # else:\n    #     if len(unmatched_layers) > 0:\n    #         print(f'check unmatched_layers: {unmatched_layers}')\n    #     else:\n    #         print(f\"weights from clip_graph successfully transferred!\\n\")\n    print(clip_graph)\n    gnn_state_dict = clip_graph.gnn.state_dict()\n    gnn.load_state_dict(gnn_state_dict)\n    return gnn\n\n\nclass GraphLlamaModel(LlamaModel):\n    config_class = GraphLlamaConfig\n\n    def __init__(self, config: LlamaConfig):\n        super(GraphLlamaModel, self).__init__(config)\n\n        if hasattr(config, \"graph_tower\"):\n            # HACK: for FSDP\n            # self.vision_tower = [CLIPVisionModel.from_pretrained(config.graph_tower)]\n            # self.arxiv_projector = nn.Linear(config.graph_hidden_size, config.hidden_size)\n            if config.graph_tower == 'MPNN': \n                self.graph_tower = MPNN(in_channels = config.graph_hidden_size, hidden_channels = config.graph_hidden_size * 2, out_channels = config.graph_hidden_size, dropout = 0.1, num_layers = 2, if_param = False)\n            elif config.graph_tower == \"clip_gcn_arxiv\": \n\n                clip_graph, args= load_model_pretrained(CLIP, config.pretrain_graph_model_path)\n                self.graph_tower = GNN(args)\n                self.graph_tower = transfer_param_tograph(clip_graph, self.graph_tower)\n            elif config.graph_tower == \"clip_gt\":\n                clip_graph, args= load_model_pretrained(CLIP, config.pretrain_graph_model_path) \n                self.graph_tower = graph_transformer(args)\n                self.graph_tower = transfer_param_tograph(clip_graph, self.graph_tower)\n            elif config.graph_tower == \"clip_gt_arxiv\": \n                clip_graph, args= load_model_pretrained(CLIP, config.pretrain_graph_model_path) \n                self.graph_tower = graph_transformer(args)\n                self.graph_tower = transfer_param_tograph(clip_graph, self.graph_tower)\n            elif config.graph_tower == \"clip_gt_arxiv_pub\": \n                clip_graph, args= load_model_pretrained(CLIP, config.pretrain_graph_model_path) \n                self.graph_tower = graph_transformer(args)\n                self.graph_tower = transfer_param_tograph(clip_graph, self.graph_tower)\n\n            \n\n            # self.vision_tower = CLIPVisionModel.from_pretrained(config.mm_vision_tower)\n\n        if hasattr(config, \"use_graph_proj\"):\n            self.graph_projector = nn.Linear(config.graph_hidden_size, config.hidden_size)\n\n    def get_graph_tower(self):\n        graph_tower = getattr(self, 'graph_tower', None)\n        if type(graph_tower) is list:\n            graph_tower = graph_tower[0]\n        return graph_tower\n\n    def initialize_graph_modules(self, graph_tower, graph_select_layer,\n                                  pretrain_graph_mlp_adapter=None, fsdp=None): # TODO: modify this function\n        self.config.graph_tower = graph_tower\n\n\n        if not hasattr(self, 'graph_tower'):\n            if self.config.graph_tower == 'MPNN': \n                graph_tower = MPNN(in_channels = self.config.graph_hidden_size, hidden_channels = self.config.graph_hidden_size * 2, out_channels = self.config.graph_hidden_size, dropout = 0.1, num_layers = 2, if_param = False)\n            elif self.config.graph_tower == \"clip_gcn_arxiv\": \n\n                clip_graph, args= load_model_pretrained(CLIP, self.config.pretrain_graph_model_path)\n                graph_tower = GNN(args)\n                graph_tower = transfer_param_tograph(clip_graph, graph_tower)\n            elif self.config.graph_tower == \"clip_gt\":\n                clip_graph, args= load_model_pretrained(CLIP, self.config.pretrain_graph_model_path) \n                graph_tower = graph_transformer(args)\n                graph_tower = transfer_param_tograph(clip_graph, graph_tower)\n            # graph_tower = MPNN(in_channels = self.config.graph_hidden_size, hidden_channels = self.config.graph_hidden_size * 2, out_channels = self.config.graph_hidden_size, dropout = 0.1, num_layers = 2)\n            elif self.config.graph_tower == \"clip_gt_arxiv\":\n                clip_graph, args= load_model_pretrained(CLIP, self.config.pretrain_graph_model_path) \n                graph_tower = graph_transformer(args)\n                graph_tower = transfer_param_tograph(clip_graph, graph_tower)\n            elif self.config.graph_tower == \"clip_gt_arxiv_pub\":\n                clip_graph, args= load_model_pretrained(CLIP, self.config.pretrain_graph_model_path) \n                graph_tower = graph_transformer(args)\n                graph_tower = transfer_param_tograph(clip_graph, graph_tower)\n        else:\n            graph_tower = self.graph_tower\n        graph_tower.requires_grad_(False)\n\n        if fsdp is not None and len(fsdp) > 0:\n            self.graph_tower = [graph_tower]\n        else:\n            self.graph_tower = graph_tower\n\n        \n\n        self.config.use_graph_proj = True\n        self.config.graph_select_layer = graph_select_layer\n\n        if not hasattr(self, 'graph_projector'):\n            self.graph_projector = nn.Linear(self.config.graph_hidden_size, self.config.hidden_size)\n\n        if pretrain_graph_mlp_adapter is not None:\n            graph_projector_weights = torch.load(pretrain_graph_mlp_adapter, map_location='cpu')\n            self.graph_projector.load_state_dict({k.split('.')[-1]: v for k, v in graph_projector_weights.items()})\n\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        past_key_values: Optional[List[torch.FloatTensor]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        # graph_node_reps: Optional[torch.FloatTensor] = None,\n        # edge_index_reps: Optional[torch.FloatTensor] = None,\n        graph_data: Optional[Data] = None,\n        return_dict: Optional[bool] = None,\n    ) -> Union[Tuple, BaseModelOutputWithPast]:\n\n        # HACK: replace back original embeddings for LLaVA pretraining\n        orig_embeds_params = getattr(self, 'orig_embeds_params', None)\n        # if orig_embeds_params is not None:\n        #     orig_embeds_params = orig_embeds_params[0]\n        #     with torch.no_grad():\n        #         self.get_input_embeddings().weight.data[:-2] = orig_embeds_params[:-2].data\n\n        if inputs_embeds is None:\n            inputs_embeds = self.embed_tokens(input_ids)\n\n        graph_tower = self.get_graph_tower()\n        if graph_tower is not None and (input_ids.shape[1] != 1 or self.training) and graph_data is not None:\n            # TODO: this is a modified multimodal LLM -- Haotian Liu\n            with torch.no_grad():\n                if type(graph_data) is list:\n                    # variable length images\n                    graph_node_features = []\n                    if type(graph_data[0]) is Data:\n                        for g in graph_data:\n                            # print(g)\n                            node_forward_out = graph_tower(g)\n                            graph_node_features.append(node_forward_out)\n                    elif type(graph_data[0]) is dict:\n                        for g_dict in graph_data:\n                            node_forward_out_1 = graph_tower(g_dict['graph_1'])\n                            node_forward_out_2 = graph_tower(g_dict['graph_2'])\n                            graph_node_features.append(node_forward_out_1)\n                            graph_node_features.append(node_forward_out_2)\n                else:\n                    raise ValueError(f'graph_node_reps is expected to be a list but got {type(graph_data)}')\n            if type(graph_data) is list:\n                # if type(graph_node_features[0]) is not dict:\n                graph_node_features = [self.graph_projector(node_feature) for node_feature in graph_node_features]\n                # else: \n                #     graph_node_features = [{'graph_1': self.graph_projector(node_feature['graph_1']), 'graph_2': self.graph_projector(node_feature['graph_2'])} for node_feature in graph_node_features]\n            else:\n                raise ValueError(f'graph_node_reps is expected to be a list but got {type(graph_data)}')\n            dummy_graph_features = torch.zeros(256, 128, device=inputs_embeds.device, dtype=inputs_embeds.dtype)\n            dummy_graph_features = self.graph_projector(dummy_graph_features)\n\n            new_input_embeds = []\n            cur_graph_idx = 0\n            for cur_input_ids, cur_input_embeds in zip(input_ids, inputs_embeds):\n                if (cur_input_ids == graph_tower.config.graph_patch_token).sum() == 0:\n                    # multimodal LLM, but the current sample is not multimodal\n                    cur_input_embeds = cur_input_embeds + (0. * dummy_graph_features).sum()\n                    new_input_embeds.append(cur_input_embeds)\n                    cur_graph_idx += 1\n                    continue\n                if graph_tower.config.use_graph_start_end:\n                    cur_graph_features = graph_node_features[cur_graph_idx]\n                    num_patches = cur_graph_features.shape[0]\n                    if (cur_input_ids == graph_tower.config.graph_start_token).sum() != (cur_input_ids == graph_tower.config.graph_end_token).sum():\n                        raise ValueError(\"The number of graph start tokens and graph end tokens should be the same.\")\n                    graph_start_tokens = torch.where(cur_input_ids == graph_tower.config.graph_start_token)[0]\n                    # print(graph_start_tokens)\n                    for graph_start_token_pos in graph_start_tokens:\n                        cur_graph_features = graph_node_features[cur_graph_idx].to(device=cur_input_embeds.device)\n                        num_patches = cur_graph_features.shape[0]\n                        if cur_input_ids[graph_start_token_pos + num_patches + 1] != graph_tower.config.graph_end_token:\n                            raise ValueError(\"The graph end token should follow the graph start token.\")\n                        if orig_embeds_params is not None:\n                            cur_new_input_embeds = torch.cat((cur_input_embeds[:graph_start_token_pos].detach(), cur_input_embeds[graph_start_token_pos:graph_start_token_pos+1], cur_graph_features, cur_input_embeds[graph_start_token_pos + num_patches + 1:graph_start_token_pos + num_patches + 2], cur_input_embeds[graph_start_token_pos + num_patches + 2:].detach()), dim=0)\n                        else:\n                            cur_new_input_embeds = torch.cat((cur_input_embeds[:graph_start_token_pos+1], cur_graph_features, cur_input_embeds[graph_start_token_pos + num_patches + 1:]), dim=0)\n                        cur_graph_idx += 1\n                    new_input_embeds.append(cur_new_input_embeds)\n                else:\n                    cur_graph_features = graph_node_features[cur_graph_idx]\n                    num_patches = cur_graph_features.shape[0]\n                    if (cur_input_ids == graph_tower.config.graph_patch_token).sum() != num_patches:\n                        raise ValueError(\"The number of graph patch tokens should be the same as the number of graph patches.\")\n                    masked_indices = torch.where(cur_input_ids == graph_tower.config.graph_patch_token)[0]\n                    mask_index_start = masked_indices[0]\n                    if (masked_indices != torch.arange(mask_index_start, mask_index_start+num_patches, device=masked_indices.device, dtype=masked_indices.dtype)).any():\n                        raise ValueError(\"The graph patch tokens should be consecutive.\")\n                    if orig_embeds_params is not None:\n                        cur_new_input_embeds = torch.cat((cur_input_embeds[:mask_index_start].detach(), cur_graph_features, cur_input_embeds[mask_index_start+num_patches:].detach()), dim=0)\n                    else:\n                        cur_new_input_embeds = torch.cat((cur_input_embeds[:mask_index_start], cur_graph_features, cur_input_embeds[mask_index_start+num_patches:]), dim=0)\n                    new_input_embeds.append(cur_new_input_embeds)\n                    cur_graph_idx += 1\n\n            # print(cur_graph_idx)\n            # print(len(graph_node_features))\n            assert cur_graph_idx == len(graph_node_features)\n            inputs_embeds = torch.stack(new_input_embeds, dim=0)\n\n        return super(GraphLlamaModel, self).forward(\n            input_ids=None, attention_mask=attention_mask, past_key_values=past_key_values,\n            inputs_embeds=inputs_embeds, use_cache=use_cache,\n            output_attentions=output_attentions, output_hidden_states=output_hidden_states,\n            return_dict=return_dict\n        )\n\n\nclass GraphLlamaForCausalLM(LlamaForCausalLM):\n    config_class = GraphLlamaConfig\n\n    def __init__(self, config):\n        super(LlamaForCausalLM, self).__init__(config)\n        self.model = GraphLlamaModel(config)\n\n        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_model(self):\n        return self.model\n\n    def get_graph_tower(self):\n        return self.get_model().get_graph_tower()\n\n    def get_vision_tower(self):\n        model = self.get_model()\n        graph_tower = model.graph_tower\n        if type(graph_tower) is list:\n            graph_tower = graph_tower[0]\n        return graph_tower\n\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        past_key_values: Optional[List[torch.FloatTensor]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        labels: Optional[torch.LongTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        # graph_node_reps: Optional[torch.FloatTensor] = None,\n        # edge_index_reps: Optional[torch.FloatTensor] = None,\n        graph_data: Optional[Data] = None,\n        return_dict: Optional[bool] = None,\n    ) -> Union[Tuple, CausalLMOutputWithPast]:\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n        outputs = self.model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            past_key_values=past_key_values,\n            inputs_embeds=inputs_embeds,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n            # graph_node_reps=graph_node_reps, \n            # edge_index_reps=edge_index_reps\n            graph_data = graph_data\n        )\n\n        hidden_states = outputs[0]\n        logits = self.lm_head(hidden_states)\n\n        loss = None\n        if labels is not None:\n            # Shift so that tokens < n predict n\n            shift_logits = logits[..., :-1, :].contiguous()\n            shift_labels = labels[..., 1:].contiguous()\n            # Flatten the tokens\n            loss_fct = CrossEntropyLoss()\n            shift_logits = shift_logits.view(-1, self.config.vocab_size)\n            shift_labels = shift_labels.view(-1)\n            # Enable model/pipeline parallelism\n            shift_labels = shift_labels.to(shift_logits.device)\n            loss = loss_fct(shift_logits, shift_labels)\n\n        if not return_dict:\n            output = (logits,) + outputs[1:]\n            return (loss,) + output if loss is not None else output\n\n        return CausalLMOutputWithPast(\n            loss=loss,\n            logits=logits,\n            past_key_values=outputs.past_key_values,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )\n\n    def prepare_inputs_for_generation(\n        self, input_ids, past_key_values=None, attention_mask=None, inputs_embeds=None, **kwargs\n    ):\n        if past_key_values:\n            input_ids = input_ids[:, -1:]\n\n        # if `inputs_embeds` are passed, we only want to use them in the 1st generation step\n        if inputs_embeds is not None and past_key_values is None:\n            model_inputs = {\"inputs_embeds\": inputs_embeds}\n        else:\n            model_inputs = {\"input_ids\": input_ids}\n\n        model_inputs.update(\n            {\n                \"past_key_values\": past_key_values,\n                \"use_cache\": kwargs.get(\"use_cache\"),\n                \"attention_mask\": attention_mask,\n                \"graph_data\": [kwargs.get(\"graph_data\", None)],\n                # \"edge_index_reps\": kwargs.get(\"edge_index_reps\", None),\n            }\n        )\n        return model_inputs\n\n    def initialize_graph_tokenizer(self, use_graph_start_end, tokenizer, device,\n                                    tune_graph_mlp_adapter=False, pretrain_graph_mlp_adapter=None):\n        vision_config = self.get_graph_tower().config\n        vision_config.use_graph_start_end = use_graph_start_end\n        tokenizer.add_tokens([DEFAULT_GRAPH_PATCH_TOKEN], special_tokens=True)\n        self.resize_token_embeddings(len(tokenizer))\n\n        if use_graph_start_end:\n            num_new_tokens = tokenizer.add_tokens([DEFAULT_G_START_TOKEN, DEFAULT_G_END_TOKEN], special_tokens=True)\n            self.resize_token_embeddings(len(tokenizer))\n            vision_config.graph_start_token, vision_config.graph_end_token = tokenizer.convert_tokens_to_ids([DEFAULT_G_START_TOKEN, DEFAULT_G_END_TOKEN])\n\n            if num_new_tokens > 0:\n                input_embeddings = self.get_input_embeddings().weight.data\n                output_embeddings = self.get_output_embeddings().weight.data\n\n                input_embeddings_avg = input_embeddings[:-num_new_tokens].mean(\n                    dim=0, keepdim=True)\n                output_embeddings_avg = output_embeddings[:-num_new_tokens].mean(\n                    dim=0, keepdim=True)\n\n                input_embeddings[-num_new_tokens:] = input_embeddings_avg\n                output_embeddings[-num_new_tokens:] = output_embeddings_avg\n\n            if tune_graph_mlp_adapter:\n                # self.get_model().orig_embeds_params = [self.get_input_embeddings().weight.data.clone().to(device=device)]\n                self.get_model().orig_embeds_params = [self.get_input_embeddings().weight.data.clone().to(device=device)]\n                for p in self.get_input_embeddings().parameters():\n                    p.requires_grad = True\n                for p in self.get_output_embeddings().parameters():\n                    p.requires_grad = False\n\n            if pretrain_graph_mlp_adapter:\n                mm_projector_weights = torch.load(pretrain_graph_mlp_adapter, map_location='cpu')\n                embed_tokens_weight = mm_projector_weights['model.embed_tokens.weight']\n                assert num_new_tokens == 2\n                if input_embeddings.shape == embed_tokens_weight.shape:\n                    input_embeddings[-num_new_tokens:] = embed_tokens_weight[-num_new_tokens:]\n                elif embed_tokens_weight.shape[0] == num_new_tokens:\n                    input_embeddings[-num_new_tokens:] = embed_tokens_weight\n                else:\n                    raise ValueError(f\"Unexpected embed_tokens_weight shape. Pretrained: {embed_tokens_weight.shape}. Current: {input_embeddings.shape}. Numer of new tokens: {num_new_tokens}.\")\n\n        vision_config.graph_patch_token = tokenizer.convert_tokens_to_ids([DEFAULT_GRAPH_PATCH_TOKEN])[0]\n\nAutoConfig.register(\"GraphLlama\", GraphLlamaConfig)\nAutoModelForCausalLM.register(GraphLlamaConfig, GraphLlamaForCausalLM)\n"}
