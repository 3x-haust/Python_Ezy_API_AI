{"repo_info": {"repo_name": "MM-RLHF", "repo_owner": "Kwai-YuanQi", "repo_url": "https://github.com/Kwai-YuanQi/MM-RLHF"}}
{"type": "test_file", "path": "llava/serve/test_message.py", "content": "import argparse\nimport json\n\nimport requests\n\nfrom llava.conversation import default_conversation\n\n\ndef main():\n    if args.worker_address:\n        worker_addr = args.worker_address\n    else:\n        controller_addr = args.controller_address\n        ret = requests.post(controller_addr + \"/refresh_all_workers\")\n        ret = requests.post(controller_addr + \"/list_models\")\n        models = ret.json()[\"models\"]\n        models.sort()\n        print(f\"Models: {models}\")\n\n        ret = requests.post(controller_addr + \"/get_worker_address\", json={\"model\": args.model_name})\n        worker_addr = ret.json()[\"address\"]\n        print(f\"worker_addr: {worker_addr}\")\n\n    if worker_addr == \"\":\n        return\n\n    conv = default_conversation.copy()\n    conv.append_message(conv.roles[0], args.message)\n    prompt = conv.get_prompt()\n\n    headers = {\"User-Agent\": \"LLaVA Client\"}\n    pload = {\n        \"model\": args.model_name,\n        \"prompt\": prompt,\n        \"max_new_tokens\": args.max_new_tokens,\n        \"temperature\": 0.7,\n        \"stop\": conv.sep,\n    }\n    response = requests.post(worker_addr + \"/worker_generate_stream\", headers=headers, json=pload, stream=True)\n\n    print(prompt.replace(conv.sep, \"\\n\"), end=\"\")\n    for chunk in response.iter_lines(chunk_size=8192, decode_unicode=False, delimiter=b\"\\0\"):\n        if chunk:\n            data = json.loads(chunk.decode(\"utf-8\"))\n            output = data[\"text\"].split(conv.sep)[-1]\n            print(output, end=\"\\r\")\n    print(\"\")\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--controller-address\", type=str, default=\"http://localhost:21001\")\n    parser.add_argument(\"--worker-address\", type=str)\n    parser.add_argument(\"--model-name\", type=str, default=\"facebook/opt-350m\")\n    parser.add_argument(\"--max-new-tokens\", type=int, default=32)\n    parser.add_argument(\"--message\", type=str, default=\"Tell me a story with more than 1000 words.\")\n    args = parser.parse_args()\n\n    main()\n"}
{"type": "source_file", "path": "llava/model/multimodal_encoder/dev_eva_clip/eva_clip/__init__.py", "content": "from .constants import OPENAI_DATASET_MEAN, OPENAI_DATASET_STD\nfrom .factory import create_model, create_model_and_transforms, create_model_from_pretrained, get_tokenizer\nfrom .factory import list_models, add_model_config, get_model_config, load_checkpoint\nfrom .loss import ClipLoss\nfrom .model import CLIP, CustomCLIP, CLIPTextCfg, CLIPVisionCfg, convert_weights_to_lp, convert_weights_to_fp16, trace_model, get_cast_dtype\nfrom .openai import load_openai_model, list_openai_models\nfrom .pretrained import list_pretrained, list_pretrained_models_by_tag, list_pretrained_tags_by_model, get_pretrained_url, download_pretrained_from_url, is_pretrained_cfg, get_pretrained_cfg, download_pretrained\nfrom .tokenizer import SimpleTokenizer, tokenize\nfrom .transform import image_transform\n"}
{"type": "source_file", "path": "llava/model/multimodal_encoder/builder.py", "content": "import os\nfrom .clip_encoder import CLIPVisionTower\nfrom .imagebind import ImageBindWrapper\nfrom .open_clip_encoder import OpenCLIPVisionTower\nfrom .hf_vision import HFVisionTower\nfrom .siglip_encoder import SigLipVisionTower\nfrom .clip_encoder import CLIPVisionTower, CLIPVisionTowerS2\n\n# from .eva_clip.eva_clip_encoder import EvaClipVisionTower\n# from .dev_eva_clip.eva_vit import EvaViTWrapper\n\n\ndef build_vision_tower(vision_tower_cfg, **kwargs):\n    vision_tower = getattr(vision_tower_cfg, \"mm_vision_tower\", getattr(vision_tower_cfg, \"vision_tower\", None))\n    is_absolute_path_exists = os.path.exists(vision_tower)\n    use_s2 = getattr(vision_tower_cfg, \"s2\", False)\n    if is_absolute_path_exists or vision_tower.startswith(\"openai\") or vision_tower.startswith(\"laion\") or \"ShareGPT4V\" in vision_tower:\n        if use_s2:\n            return CLIPVisionTowerS2(vision_tower, args=vision_tower_cfg, **kwargs)\n        else:\n            return CLIPVisionTower(vision_tower, args=vision_tower_cfg, **kwargs)\n    elif \"siglip\" in vision_tower:\n        return SigLipVisionTower(vision_tower, vision_tower_cfg=vision_tower_cfg, **kwargs)\n    elif vision_tower.startswith(\"hf:\"):\n        return HFVisionTower(vision_tower, args=vision_tower_cfg, **kwargs)\n    elif vision_tower in [\"imagebind_huge\"]:\n        return ImageBindWrapper(vision_tower, args=vision_tower_cfg, **kwargs)\n    elif vision_tower.startswith(\"open_clip_hub\"):\n        return OpenCLIPVisionTower(vision_tower, args=vision_tower_cfg, **kwargs)\n    # elif \"internal-eva\" in vision_tower.lower() or \"eva02\" in vision_tower.lower():\n    #     return EvaClipVisionTower(vision_tower, args=vision_tower_cfg, **kwargs)\n    # elif vision_tower in [\"EVA-CLIP-8B\", \"EVA-CLIP-8B-plus\"]:\n    #     return EvaViTWrapper(vision_tower, args=vision_tower_cfg, **kwargs)\n\n    raise ValueError(f\"Unknown vision tower: {vision_tower}\")\n"}
{"type": "source_file", "path": "llava/eval/evaluate_interleave.py", "content": "import re\nfrom rouge import Rouge\nimport argparse\nimport os\nimport json\nimport numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n\nspot_the_diff = [\"Spot-the-Diff\", \"Birds-to-Words\", \"CLEVR-Change\"]\nimage_edit_instruct = [\"IEdit\", \"HQ-Edit\", \"MagicBrush\"]\nvisual_story_telling = [\"AESOP\", \"FlintstonesSV\", \"PororoSV\", \"VIST\"]\nvisual_cloze = [\"COMICS_Dialogue\", \"RecipeQA_VisualCloze\"]\ntext_rich_vqa = [\"WebQA\", \"TQA\", \"OCR-VQA\", \"DocVQA\"]\nmulti_image_vqa = [\"MIT-States_StateCoherence\", \"MIT-States_PropertyCoherence\", \"VISION\", \"RecipeQA_ImageCoherence\"]\n\npuzzle = [\"RAVEN\"]\nnlrv2 = [\"NLVR2_Mantis\"]\nqbench = [\"QBench\"]\n\nclass Eval:\n    def __init__(self):\n        self.periodStrip = re.compile(\"(?!<=\\d)(\\.)(?!\\d)\")\n        self.commaStrip = re.compile(\"(\\d)(\\,)(\\d)\")\n        self.punct = [\n            \";\",\n            r\"/\",\n            \"[\",\n            \"]\",\n            '\"',\n            \"{\",\n            \"}\",\n            \"(\",\n            \")\",\n            \"=\",\n            \"+\",\n            \"\\\\\",\n            \"_\",\n            \"-\",\n            \">\",\n            \"<\",\n            \"@\",\n            \"`\",\n            \",\",\n            \"?\",\n            \"!\",\n        ]\n        \n    def processPunctuation(self, inText):\n        outText = inText\n        for p in self.punct:\n            if (p + \" \" in inText or \" \" + p in inText) or (\n                re.search(self.commaStrip, inText) != None\n            ):\n                outText = outText.replace(p, \"\")\n            else:\n                outText = outText.replace(p, \" \")\n        outText = self.periodStrip.sub(\"\", outText, re.UNICODE)\n        return outText\n    \n    def process(self, answer):\n        answer = answer.replace(\"\\n\", \" \")\n        answer = answer.replace(\"\\t\", \" \")\n        answer = answer.strip()\n        answer = self.processPunctuation(answer)\n        answer = answer.strip('\\'')\n        answer = answer.strip('\\\"')\n        answer = answer.strip(')')\n        answer = answer.strip('(')\n        answer = answer.strip().lower()\n        return answer\n\n    def evaluate_rouge(self,preds):\n        rouge = Rouge()\n        acc = {'f': []}\n        eval_list = []\n        for i, res in enumerate(preds):\n            sample_id = res['sample_id']\n            # print(sample_id)\n            gt_ans = self.process(res[\"gt_response\"])\n            pred_ans = self.process(res[\"pred_response\"])\n            # assert gt_ans != ''\n\n            if gt_ans == '':\n                continue\n            \n            if pred_ans == '':\n                s = 0\n            else:\n                if len(pred_ans) > 512:\n                    pred_ans = pred_ans[0: 512]\n                s = rouge.get_scores(pred_ans, gt_ans)[0]['rouge-l']['f']\n            acc['f'].append(s)\n            eval_list.append({'id':str(sample_id),'score':str(round(s,3))})\n        results = {'Rouge-L f': np.mean(acc['f'])}\n        return results,eval_list\n\n\n    def judge_multi_choice(self,sample):\n        sample_id = sample['sample_id']\n        gt_ans = sample[\"gt_response\"]\n        pred_ans = sample[\"pred_response\"]\n\n        if \":\" in pred_ans:\n            a_list = pred_ans.split(\":\")\n            a_list = [a.strip() for a in a_list ]\n            for a in a_list:\n                if len(a) == 1 and a[-1] in [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\"]:\n                    pred_ans = a\n\n        if pred_ans == gt_ans:\n            return 1\n        else:\n            return 0\n\n    def process_sample(self,sample):\n        sample[\"gt_response\"] = self.process(sample[\"gt_response\"])\n        sample[\"pred_response\"] = self.process(sample[\"pred_response\"])\n\n    def evaluate_multichoice(self, preditions):\n        correct = 0\n        eval_list = []\n        for i, sample in enumerate(preditions):\n            self.process_sample(sample)\n            score = self.judge_multi_choice(sample)\n            sample_id = sample['sample_id']\n            sample['result'] = score\n            eval_list.append({'id':str(sample_id),'score':str(score)})\n            correct+=score\n        return {'Accuracy':correct/len(preditions)},eval_list\n\n    def evaluate_multi_choice_image(self,preditions):\n        correct = 0\n        eval_list = []\n        for i,sample in enumerate(preditions):\n            gt_ans = self.process(sample[\"gt_response\"])\n            pred_ans = self.process(sample[\"pred_response\"])\n            sample_id = sample['sample_id']\n\n            if \":\" in pred_ans:\n                a_list = pred_ans.split(\":\")\n                a_list = [a.strip() for a in a_list ]\n                for a in a_list:\n                    if len(a) == 1 and a[-1] in [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\"]:\n                        pred_ans = a\n\n            if gt_ans == pred_ans:\n                score = 1\n            else:\n                score = 0\n            sample_id = sample['sample_id']\n            sample['result'] = score\n            eval_list.append({'id':str(sample_id),'score':str(score)})\n            correct+=score\n        return {'Accuracy':correct/len(preditions)},eval_list\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--result-dir', type=str, required=True)\n\n    args = parser.parse_args()\n    \n    result_file = os.path.join(args.result_dir, \"result.jsonl\")\n\n    if not os.path.exists(result_file):\n        print('No prediction file found')\n        exit(0)\n    with open(result_file, 'r') as f:\n        preds_all = [json.loads(line) for line in f]\n    \n    preds_all_dict = dict()\n    for pred in preds_all:\n        if pred[\"dataset\"] not in preds_all_dict:\n            preds_all_dict[pred[\"dataset\"]] = list()\n        preds_all_dict[pred[\"dataset\"]].append(pred)\n\n    image_choice_dataset_list = [\"recipeqa-RecipeQA_VisualCloze\", \"RecipeQA_ImageCoherence\", \"COMICS_Panel\"]\n    E = Eval()\n\n    eval_result_list = dict()\n    eval_result_list_detail = dict()\n\n    for dataset in preds_all_dict:\n        \n        preds = preds_all_dict[dataset]\n        question_type = preds[0][\"question_type\"]\n   \n        if question_type == 'open-ended':\n            eval_result, eval_list = E.evaluate_rouge(preds)\n\n        elif question_type == 'multi-choice' or dataset == 'nlrv2':\n            if dataset in image_choice_dataset_list:\n                eval_result, eval_list = E.evaluate_multi_choice_image(preds)\n            else:\n                eval_result, eval_list = E.evaluate_multichoice(preds)\n\n        else:\n            eval_result = 'Dataset not supported'\n            print('Dataset not supported')\n            exit(0)\n\n        print(dataset, end = ':  ')\n        print(eval_result)\n\n        eval_result_list[dataset] = eval_result\n        eval_result_list_detail[dataset] = eval_list\n\n    os.makedirs(args.result_dir, exist_ok=True)\n    with open(os.path.join(args.result_dir, 'eval_dataset.json'), 'w') as f:\n        json.dump(eval_result_list, f, indent=4)\n\n    with open(os.path.join(args.result_dir,'eval_dataset_details.json'), 'w') as f:\n        json.dump(eval_result_list_detail, f, indent=4)\n\n\n    eval_cat_list = dict()\n    print()\n\n    # spot_the_diff\n    score = 0\n    count = 0\n    for dataset in eval_result_list:\n        if dataset in spot_the_diff:\n            count += 1\n            score += list(eval_result_list[dataset].values())[0]\n    if count > 0:\n        score /= count\n        eval_cat_list[\"spot_the_diff\"] = score\n        print(\"spot_the_diff\", end = ':  ')\n        print('{:.2f}'.format(100 * score))\n\n    # image_edit_instruct\n    score = 0\n    count = 0\n    for dataset in eval_result_list:\n        if dataset in image_edit_instruct:\n            count += 1\n            score += list(eval_result_list[dataset].values())[0]\n    if count > 0:\n        score /= count\n        eval_cat_list[\"image_edit_instruct\"] = score\n        print(\"image_edit_instruct\", end = ':  ')\n        print('{:.2f}'.format(100 * score))\n\n    # visual_story_telling\n    score = 0\n    count = 0\n    for dataset in eval_result_list:\n        if dataset in visual_story_telling:\n            count += 1\n            score += list(eval_result_list[dataset].values())[0]\n    if count > 0:\n        score /= count\n        eval_cat_list[\"visual_story_telling\"] = score\n        print(\"visual_story_telling\", end = ':  ')\n        print('{:.2f}'.format(100 * score))\n\n    # visual_cloze\n    score = 0\n    count = 0\n    for dataset in eval_result_list:\n        if dataset in visual_cloze:\n            count += 1\n            score += list(eval_result_list[dataset].values())[0]\n    if count > 0:\n        score /= count\n        eval_cat_list[\"visual_cloze\"] = score\n        print(\"visual_cloze\", end = ':  ')\n        print('{:.2f}'.format(100 * score))\n\n    # text_rich_vqa\n    score = 0\n    count = 0\n    for dataset in eval_result_list:\n        if dataset in text_rich_vqa:\n            count += 1\n            score += list(eval_result_list[dataset].values())[0]\n    if count > 0:\n        score /= count\n        eval_cat_list[\"text_rich_vqa\"] = score\n        print(\"text_rich_vqa\", end = ':  ')\n        print('{:.2f}'.format(100 * score))\n\n    # multi_image_vqa\n    score = 0\n    count = 0\n    for dataset in eval_result_list:\n        if dataset in multi_image_vqa:\n            count += 1\n            score += list(eval_result_list[dataset].values())[0]\n    if count > 0:\n        score /= count\n        eval_cat_list[\"multi_image_vqa\"] = score\n        print(\"multi_image_vqa\", end = ':  ')\n        print('{:.2f}'.format(100 * score))\n\n    # puzzle\n    score = 0\n    count = 0\n    for dataset in eval_result_list:\n        if dataset in puzzle:\n            count += 1\n            score += list(eval_result_list[dataset].values())[0]\n    if count > 0:\n        score /= count\n        eval_cat_list[\"puzzle\"] = score\n        print(\"puzzle\", end = ':  ')\n        print('{:.2f}'.format(100 * score))\n\n    # nlrv2\n    score = 0\n    count = 0\n    for dataset in eval_result_list:\n        if dataset in nlrv2:\n            count += 1\n            score += list(eval_result_list[dataset].values())[0]\n    if count > 0:\n        score /= count\n        eval_cat_list[\"nlrv2\"] = score\n        print(\"nlrv2\", end = ':  ')\n        print('{:.2f}'.format(100 * score))\n\n    # qbench\n    score = 0\n    count = 0\n    for dataset in eval_result_list:\n        if dataset in qbench:\n            count += 1\n            score += list(eval_result_list[dataset].values())[0]\n    if count > 0:\n        score /= count\n        eval_cat_list[\"qbench\"] = score\n        print(\"qbench\", end = ':  ')\n        print('{:.2f}'.format(100 * score))\n\n    with open(os.path.join(args.result_dir,'eval_cat.json'), 'w') as f:\n        json.dump(eval_cat_list, f, indent=4)"}
{"type": "source_file", "path": "llava/__init__.py", "content": "from .model import LlavaLlamaForCausalLM\n"}
{"type": "source_file", "path": "llava/model/apply_delta.py", "content": "\"\"\"\nUsage:\npython3 -m fastchat.model.apply_delta --base ~/model_weights/llama-7b --target ~/model_weights/vicuna-7b --delta lmsys/vicuna-7b-delta\n\"\"\"\n\nimport argparse\n\nimport torch\nfrom tqdm import tqdm\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom llava import LlavaLlamaForCausalLM\n\n\ndef apply_delta(base_model_path, target_model_path, delta_path):\n    print(\"Loading base model\")\n    base = AutoModelForCausalLM.from_pretrained(base_model_path, torch_dtype=torch.float16, low_cpu_mem_usage=True)\n\n    print(\"Loading delta\")\n    delta = LlavaLlamaForCausalLM.from_pretrained(delta_path, torch_dtype=torch.float16, low_cpu_mem_usage=True)\n    delta_tokenizer = AutoTokenizer.from_pretrained(delta_path)\n\n    print(\"Applying delta\")\n    for name, param in tqdm(delta.state_dict().items(), desc=\"Applying delta\"):\n        if name not in base.state_dict():\n            assert name in [\"model.mm_projector.weight\", \"model.mm_projector.bias\"], f\"{name} not in base model\"\n            continue\n        if param.data.shape == base.state_dict()[name].shape:\n            param.data += base.state_dict()[name]\n        else:\n            assert name in [\"model.embed_tokens.weight\", \"lm_head.weight\"], f\"{name} dimension mismatch: {param.data.shape} vs {base.state_dict()[name].shape}\"\n            bparam = base.state_dict()[name]\n            param.data[: bparam.shape[0], : bparam.shape[1]] += bparam\n\n    print(\"Saving target model\")\n    delta.save_pretrained(target_model_path)\n    delta_tokenizer.save_pretrained(target_model_path)\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--base-model-path\", type=str, required=True)\n    parser.add_argument(\"--target-model-path\", type=str, required=True)\n    parser.add_argument(\"--delta-path\", type=str, required=True)\n\n    args = parser.parse_args()\n\n    apply_delta(args.base_model_path, args.target_model_path, args.delta_path)\n"}
{"type": "source_file", "path": "llava/model/language_model/llava_gemma.py", "content": "#    Copyright 2024 Duc Q. Nguyen, Haotian Liu and Bo Li\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\");\n#    you may not use this file except in compliance with the License.\n#    You may obtain a copy of the License at\n#\n#        http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS,\n#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#    See the License for the specific language governing permissions and\n#    limitations under the License.\n\n\nfrom typing import List, Optional, Tuple, Union\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import CrossEntropyLoss\n\nfrom transformers import AutoConfig, AutoModelForCausalLM, GemmaConfig, GemmaModel, GemmaForCausalLM\n\nfrom transformers.modeling_outputs import CausalLMOutputWithPast\nfrom transformers.generation.utils import GenerateOutput\n\nfrom ..llava_arch import LlavaMetaModel, LlavaMetaForCausalLM\n\n\nclass LlavaGemmaConfig(GemmaConfig):\n    model_type = \"llava_gemma\"\n\n\nclass LlavaGemmaModel(LlavaMetaModel, GemmaModel):\n    config_class = LlavaGemmaConfig\n\n    def __init__(self, config: GemmaConfig):\n        super(LlavaGemmaModel, self).__init__(config)\n\n\nclass LlavaGemmaForCausalLM(GemmaForCausalLM, LlavaMetaForCausalLM):\n    config_class = LlavaGemmaConfig\n\n    def __init__(self, config):\n        super(GemmaForCausalLM, self).__init__(config)\n        self.model = LlavaGemmaModel(config)\n\n        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_model(self):\n        return self.model\n\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_values: Optional[List[torch.FloatTensor]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        labels: Optional[torch.LongTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        images: Optional[torch.FloatTensor] = None,\n        image_sizes: Optional[List[List[int]]] = None,\n        return_dict: Optional[bool] = None,\n        cache_position: Optional[torch.LongTensor] = None,\n    ) -> Union[Tuple, CausalLMOutputWithPast]:\n\n        if inputs_embeds is None:\n            (input_ids, position_ids, attention_mask, past_key_values, inputs_embeds, labels) = self.prepare_inputs_labels_for_multimodal(input_ids, position_ids, attention_mask, past_key_values, labels, images, image_sizes)\n\n        return super().forward(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            past_key_values=past_key_values,\n            inputs_embeds=inputs_embeds,\n            labels=labels,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n            cache_position=cache_position,\n        )\n\n    @torch.no_grad()\n    def generate(\n        self,\n        inputs: Optional[torch.Tensor] = None,\n        images: Optional[torch.Tensor] = None,\n        image_sizes: Optional[torch.Tensor] = None,\n        **kwargs,\n    ) -> Union[GenerateOutput, torch.LongTensor]:\n        position_ids = kwargs.pop(\"position_ids\", None)\n        attention_mask = kwargs.pop(\"attention_mask\", None)\n        if \"inputs_embeds\" in kwargs:\n            raise NotImplementedError(\"`inputs_embeds` is not supported\")\n\n        if images is not None:\n            (inputs, position_ids, attention_mask, _, inputs_embeds, _) = self.prepare_inputs_labels_for_multimodal(inputs, position_ids, attention_mask, None, None, images, image_sizes=image_sizes)\n        else:\n            inputs_embeds = self.get_model().embed_tokens(inputs)\n\n        return super().generate(position_ids=position_ids, attention_mask=attention_mask, inputs_embeds=inputs_embeds, **kwargs)\n\n    def prepare_inputs_for_generation(self, input_ids, past_key_values=None, inputs_embeds=None, **kwargs):\n        images = kwargs.pop(\"images\", None)\n        image_sizes = kwargs.pop(\"image_sizes\", None)\n        inputs = super().prepare_inputs_for_generation(input_ids, past_key_values=past_key_values, inputs_embeds=inputs_embeds, **kwargs)\n        if images is not None:\n            inputs[\"images\"] = images\n        if image_sizes is not None:\n            inputs[\"image_sizes\"] = image_sizes\n        return inputs\n\n\nAutoConfig.register(\"llava_gemma\", LlavaGemmaConfig)\nAutoModelForCausalLM.register(LlavaGemmaConfig, LlavaGemmaForCausalLM)\n"}
{"type": "source_file", "path": "llava/constants.py", "content": "CONTROLLER_HEART_BEAT_EXPIRATION = 30\nWORKER_HEART_BEAT_INTERVAL = 15\n\nLOGDIR = \".\"\n\n# Model Constants\nIGNORE_INDEX = -100\nIMAGE_TOKEN_INDEX = -200\nDEFAULT_IMAGE_TOKEN = \"<image>\"\nDEFAULT_IMAGE_PATCH_TOKEN = \"<im_patch>\"\nDEFAULT_IM_START_TOKEN = \"<im_start>\"\nDEFAULT_IM_END_TOKEN = \"<im_end>\"\n"}
{"type": "source_file", "path": "llava/model/make_delta.py", "content": "\"\"\"\nUsage:\npython3 -m llava.model.make_delta --base ~/model_weights/llama-7b --target ~/model_weights/llava-7b --delta ~/model_weights/llava-7b-delta --hub-repo-id liuhaotian/llava-7b-delta\n\"\"\"\n\nimport argparse\n\nimport torch\nfrom tqdm import tqdm\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom llava.model.utils import auto_upgrade\n\n\ndef make_delta(base_model_path, target_model_path, delta_path, hub_repo_id):\n    print(\"Loading base model\")\n    base = AutoModelForCausalLM.from_pretrained(base_model_path, torch_dtype=torch.float16, low_cpu_mem_usage=True)\n\n    print(\"Loading target model\")\n    auto_upgrade(target_model_path)\n    target = AutoModelForCausalLM.from_pretrained(target_model_path, torch_dtype=torch.float16, low_cpu_mem_usage=True)\n\n    print(\"Calculating delta\")\n    for name, param in tqdm(target.state_dict().items(), desc=\"Calculating delta\"):\n        if name not in base.state_dict():\n            assert name in [\"model.mm_projector.weight\", \"model.mm_projector.bias\"], f\"{name} not in base model\"\n            continue\n        if param.data.shape == base.state_dict()[name].shape:\n            param.data -= base.state_dict()[name]\n        else:\n            assert name in [\"model.embed_tokens.weight\", \"lm_head.weight\"], f\"{name} dimension mismatch: {param.data.shape} vs {base.state_dict()[name].shape}\"\n            bparam = base.state_dict()[name]\n            param.data[: bparam.shape[0], : bparam.shape[1]] -= bparam\n\n    print(\"Saving delta\")\n    if hub_repo_id:\n        kwargs = {\"push_to_hub\": True, \"repo_id\": hub_repo_id}\n    else:\n        kwargs = {}\n    target.save_pretrained(delta_path, **kwargs)\n    target_tokenizer = AutoTokenizer.from_pretrained(target_model_path)\n    target_tokenizer.save_pretrained(delta_path, **kwargs)\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--base-model-path\", type=str, required=True)\n    parser.add_argument(\"--target-model-path\", type=str, required=True)\n    parser.add_argument(\"--delta-path\", type=str, required=True)\n    parser.add_argument(\"--hub-repo-id\", type=str, default=None)\n    args = parser.parse_args()\n\n    make_delta(args.base_model_path, args.target_model_path, args.delta_path, args.hub_repo_id)\n"}
{"type": "source_file", "path": "llava/conversation.py", "content": "import dataclasses\nfrom enum import auto, Enum\nfrom typing import List, Any, Dict, Union, Tuple\nimport re\nimport base64\nfrom io import BytesIO\nfrom PIL import Image\nfrom transformers import AutoTokenizer\n\n\nclass SeparatorStyle(Enum):\n    \"\"\"Different separator style.\"\"\"\n\n    SINGLE = auto()\n    TWO = auto()\n    MPT = auto()\n    PLAIN = auto()\n    CHATML = auto()\n    LLAMA_2 = auto()\n    LLAMA_3 = auto()\n    QWEN = auto()\n    GEMMA = auto()\n\n\n@dataclasses.dataclass\nclass Conversation:\n    \"\"\"A class that keeps all conversation history.\"\"\"\n\n    system: str\n    roles: List[str]\n    messages: List[List[str]]\n    offset: int\n    sep_style: SeparatorStyle = SeparatorStyle.SINGLE\n    sep: str = \"###\"\n    sep2: str = None\n    version: str = \"Unknown\"\n\n    tokenizer_id: str = \"\"\n    tokenizer: Any = None\n    # Stop criteria (the default one is EOS token)\n    stop_str: Union[str, List[str]] = None\n    # Stops generation if meeting any token in this list\n    stop_token_ids: List[int] = None\n\n    skip_next: bool = False\n\n    def get_prompt(self):\n        messages = self.messages\n        if len(messages) > 0 and type(messages[0][1]) is tuple:\n            messages = self.messages.copy()\n            init_role, init_msg = messages[0].copy()\n            init_msg = init_msg[0]\n            if \"mmtag\" in self.version:\n                init_msg = init_msg.replace(\"<image>\", \"\").strip()\n                messages[0] = (init_role, init_msg)\n                messages.insert(0, (self.roles[0], \"<Image><image></Image>\"))\n                messages.insert(1, (self.roles[1], \"Received.\"))\n            elif not init_msg.startswith(\"<image>\"):\n                init_msg = init_msg.replace(\"<image>\", \"\").strip()\n                messages[0] = (init_role, \"<image>\\n\" + init_msg)\n            else:\n                messages[0] = (init_role, init_msg)\n\n        if self.sep_style == SeparatorStyle.SINGLE:\n            ret = self.system + self.sep\n            for role, message in messages:\n                if message:\n                    if type(message) is tuple:\n                        message, _, _ = message\n                    ret += role + \": \" + message + self.sep\n                else:\n                    ret += role + \":\"\n\n        elif self.sep_style == SeparatorStyle.TWO:\n            seps = [self.sep, self.sep2]\n            ret = self.system + seps[0]\n            for i, (role, message) in enumerate(messages):\n                if message:\n                    if type(message) is tuple:\n                        message, _, _ = message\n                    ret += role + \": \" + message + seps[i % 2]\n                else:\n                    ret += role + \":\"\n\n        elif self.sep_style == SeparatorStyle.CHATML:\n            ret = \"\" if self.system == \"\" else self.system + self.sep + \"\\n\"\n            for role, message in messages:\n                if message:\n                    if type(message) is tuple:\n                        message, images, _ = message\n                        message = \"<image>\" * len(images) + message\n                    ret += role + \"\\n\" + message + self.sep + \"\\n\"\n                else:\n                    ret += role + \"\\n\"\n            return ret\n\n        elif self.sep_style == SeparatorStyle.LLAMA_3:\n            if self.tokenizer is None:\n                raise ValueError(\"Llama 3 tokenizer is not available. Make sure you have the necessary permissions.\")\n            chat_template_messages = [{\"role\": \"system\", \"content\": self.system}]\n            for role, message in messages:\n                if message:\n                    if type(message) is tuple:\n                        message, images = message\n                        message = \"<image>\" * len(images) + message\n                    chat_template_messages.append({\"role\": role, \"content\": message})\n\n            # print(chat_template_messages)\n            return self.tokenizer.apply_chat_template(chat_template_messages, tokenize=False, add_generation_prompt=True)\n            # ret = \"\" if self.system == \"\" else self.system + self.sep + \"\\n\"\n            # for role, message in messages:\n            #     if message:\n            #         if type(message) is tuple:\n            #             message, images = message\n            #             message = \"<image>\" * len(images) + message\n            #         ret += role + \"\\n\" + message + self.sep + \"\\n\"\n            #     else:\n            #         ret += role + \"\\n\"\n            # return ret\n\n        elif self.sep_style == SeparatorStyle.MPT:\n            ret = self.system + self.sep\n            for role, message in messages:\n                if message:\n                    if type(message) is tuple:\n                        message, _, _ = message\n                    ret += role + message + self.sep\n                else:\n                    ret += role\n\n        elif self.sep_style == SeparatorStyle.GEMMA:\n            ret = \"\"\n            for i, (role, message) in enumerate(messages):\n                assert role == self.roles[i % 2], \"Conversation should alternate user/assistant/user/assistant/...\"\n                if message:\n                    if type(message) is tuple:\n                        message, _, _ = message\n                    ret += role + message + self.sep\n                else:\n                    ret += role\n\n        elif self.sep_style == SeparatorStyle.LLAMA_2:\n            wrap_sys = lambda msg: f\"<<SYS>>\\n{msg}\\n<</SYS>>\\n\\n\" if len(msg) > 0 else msg\n            wrap_inst = lambda msg: f\"[INST] {msg} [/INST]\"\n            ret = \"\"\n\n            for i, (role, message) in enumerate(messages):\n                if i == 0:\n                    assert message, \"first message should not be none\"\n                    assert role == self.roles[0], \"first message should come from user\"\n                if message:\n                    if type(message) is tuple:\n                        message, _, _ = message\n                    if i == 0:\n                        message = wrap_sys(self.system) + message\n                    if i % 2 == 0:\n                        message = wrap_inst(message)\n                        ret += self.sep + message\n                    else:\n                        ret += \" \" + message + \" \" + self.sep2\n                else:\n                    ret += \"\"\n            ret = ret.lstrip(self.sep)\n\n        elif self.sep_style == SeparatorStyle.PLAIN:\n            seps = [self.sep, self.sep2]\n            ret = self.system\n            for i, (role, message) in enumerate(messages):\n                if message:\n                    if type(message) is tuple:\n                        message, _, _ = message\n                    ret += message + seps[i % 2]\n                else:\n                    ret += \"\"\n        else:\n            raise ValueError(f\"Invalid style: {self.sep_style}\")\n\n        return ret\n\n    def append_message(self, role, message):\n        self.messages.append([role, message])\n\n    def process_image(self, image, image_process_mode, return_pil=False, image_format=\"PNG\"):\n        if image_process_mode == \"Pad\":\n\n            def expand2square(pil_img, background_color=(122, 116, 104)):\n                width, height = pil_img.size\n                if width == height:\n                    return pil_img\n                elif width > height:\n                    result = Image.new(pil_img.mode, (width, width), background_color)\n                    result.paste(pil_img, (0, (width - height) // 2))\n                    return result\n                else:\n                    result = Image.new(pil_img.mode, (height, height), background_color)\n                    result.paste(pil_img, ((height - width) // 2, 0))\n                    return result\n\n            image = expand2square(image)\n        elif image_process_mode in [\"Default\", \"Crop\"]:\n            pass\n        elif image_process_mode == \"Resize\":\n            image = image.resize((336, 336))\n        else:\n            raise ValueError(f\"Invalid image_process_mode: {image_process_mode}\")\n\n        if type(image) is not Image.Image:\n            image = Image.open(image).convert(\"RGB\")\n\n        max_hw, min_hw = max(image.size), min(image.size)\n        aspect_ratio = max_hw / min_hw\n        max_len, min_len = 672, 448\n        shortest_edge = int(min(max_len / aspect_ratio, min_len, min_hw))\n        longest_edge = int(shortest_edge * aspect_ratio)\n        W, H = image.size\n        if H > W:\n            H, W = longest_edge, shortest_edge\n        else:\n            H, W = shortest_edge, longest_edge\n        image = image.resize((W, H))\n        if return_pil:\n            return image\n        else:\n            buffered = BytesIO()\n            image.save(buffered, format=image_format)\n            img_b64_str = base64.b64encode(buffered.getvalue()).decode()\n            return img_b64_str\n\n    def get_images(self, return_pil=False, return_path=False):\n        images = []\n        for i, (role, msg) in enumerate(self.messages[self.offset :]):\n            if i % 2 == 0:\n                if type(msg) is tuple:\n                    msg, image, image_process_mode = msg\n                    if type(image) != list:\n                        image = [image]\n                    for img in image:\n                        if not return_path and self.is_image_file(img):\n                            img = self.process_image(img, image_process_mode, return_pil=return_pil)\n                        else:\n                            images.append(img)\n        return images\n\n    def is_image_file(self, filename):\n        image_extensions = [\".png\", \".jpg\", \".jpeg\", \".gif\", \".bmp\", \".tiff\", \".webp\"]\n        return any(filename.lower().endswith(ext) for ext in image_extensions)\n\n    def is_video_file(self, filename):\n        video_extensions = [\".mp4\", \".mov\", \".avi\", \".mkv\", \".wmv\", \".flv\", \".mpeg\", \".mpg\"]\n        return any(filename.lower().endswith(ext) for ext in video_extensions)\n\n    def to_gradio_chatbot(self):\n        ret = []\n        for i, (role, msg) in enumerate(self.messages[self.offset :]):\n            if i % 2 == 0:\n                if type(msg) is tuple:\n                    msg, image, image_process_mode = msg\n                    if type(image) != list:\n                        image = [image]\n                    if len(image) == 1:\n                        msg = \"<image>\\n\" + msg.replace(\"<image>\", \"\").strip()\n                    else:\n                        msg = re.sub(r\"(<image>)\\n(?=<image>)\", r\"\\1 \", msg)\n\n                    img_str_list = []                         \n                    for img in image:\n                        if self.is_image_file(img):\n                            img_b64_str = self.process_image(img, \"Default\", return_pil=False, image_format=\"JPEG\")\n                            img_str = f'<img src=\"data:image/jpeg;base64,{img_b64_str}\" style=\"max-width: 256px; max-height: 256px; width: auto; height: auto; object-fit: contain;\"/>'\n                            img_str_list.append(img_str)\n                        elif self.is_video_file(img):\n                            ret.append(((img,), None))\n\n                    msg = msg.strip()\n                    img_place_holder = \"\"\n                    for img_str in img_str_list:\n                        img_place_holder += f\"{img_str}\\n\\n\"\n\n                    if len(img_str_list) > 0:\n                        msg = f\"{img_place_holder}\\n\\n{msg}\"\n\n                    if len(msg) > 0:\n                        ret.append([msg, None])\n                else:\n                    ret.append([msg, None])\n            else:\n                ret[-1][-1] = msg\n        return ret\n\n    def copy(self):\n        return Conversation(system=self.system, roles=self.roles, messages=[[x, y] for x, y in self.messages], offset=self.offset, sep_style=self.sep_style, sep=self.sep, sep2=self.sep2, version=self.version)\n\n    def dict(self):\n        if len(self.get_images()) > 0:\n            return {\n                \"system\": self.system,\n                \"roles\": self.roles,\n                \"messages\": [[x, y[0] if type(y) is tuple else y] for x, y in self.messages],\n                \"offset\": self.offset,\n                \"sep\": self.sep,\n                \"sep2\": self.sep2,\n            }\n        return {\n            \"system\": self.system,\n            \"roles\": self.roles,\n            \"messages\": self.messages,\n            \"offset\": self.offset,\n            \"sep\": self.sep,\n            \"sep2\": self.sep2,\n        }\n\n\nconv_vicuna_v0 = Conversation(\n    system=\"A chat between a curious human and an artificial intelligence assistant. \" \"The assistant gives helpful, detailed, and polite answers to the human's questions.\",\n    roles=(\"Human\", \"Assistant\"),\n    messages=[\n        [\"Human\", \"What are the key differences between renewable and non-renewable energy sources?\"],\n        [\n            \"Assistant\",\n            \"Renewable energy sources are those that can be replenished naturally in a relatively \"\n            \"short amount of time, such as solar, wind, hydro, geothermal, and biomass. \"\n            \"Non-renewable energy sources, on the other hand, are finite and will eventually be \"\n            \"depleted, such as coal, oil, and natural gas. Here are some key differences between \"\n            \"renewable and non-renewable energy sources:\\n\"\n            \"1. Availability: Renewable energy sources are virtually inexhaustible, while non-renewable \"\n            \"energy sources are finite and will eventually run out.\\n\"\n            \"2. Environmental impact: Renewable energy sources have a much lower environmental impact \"\n            \"than non-renewable sources, which can lead to air and water pollution, greenhouse gas emissions, \"\n            \"and other negative effects.\\n\"\n            \"3. Cost: Renewable energy sources can be more expensive to initially set up, but they typically \"\n            \"have lower operational costs than non-renewable sources.\\n\"\n            \"4. Reliability: Renewable energy sources are often more reliable and can be used in more remote \"\n            \"locations than non-renewable sources.\\n\"\n            \"5. Flexibility: Renewable energy sources are often more flexible and can be adapted to different \"\n            \"situations and needs, while non-renewable sources are more rigid and inflexible.\\n\"\n            \"6. Sustainability: Renewable energy sources are more sustainable over the long term, while \"\n            \"non-renewable sources are not, and their depletion can lead to economic and social instability.\\n\",\n        ],\n    ],\n    offset=2,\n    sep_style=SeparatorStyle.SINGLE,\n    sep=\"###\",\n)\n\nconv_vicuna_v1 = Conversation(\n    system=\"A chat between a curious user and an artificial intelligence assistant. \" \"The assistant gives helpful, detailed, and polite answers to the user's questions.\",\n    roles=(\"USER\", \"ASSISTANT\"),\n    version=\"v1\",\n    messages=[],\n    offset=0,\n    sep_style=SeparatorStyle.TWO,\n    sep=\" \",\n    sep2=\"</s>\",\n)\n\nconv_llama_2 = Conversation(\n    system=\"\"\"You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\"\"\",\n    roles=(\"USER\", \"ASSISTANT\"),\n    version=\"llama_v2\",\n    messages=[],\n    offset=0,\n    sep_style=SeparatorStyle.LLAMA_2,\n    sep=\"<s>\",\n    sep2=\"</s>\",\n)\n\nconv_llava_llama_2 = Conversation(\n    system=\"You are a helpful language and vision assistant. \" \"You are able to understand the visual content that the user provides, \" \"and assist the user with a variety of tasks using natural language.\",\n    roles=(\"USER\", \"ASSISTANT\"),\n    version=\"llama_v2\",\n    messages=[],\n    offset=0,\n    sep_style=SeparatorStyle.LLAMA_2,\n    sep=\"<s>\",\n    sep2=\"</s>\",\n)\n\ndef safe_load_tokenizer(tokenizer_id):\n    try:\n        return AutoTokenizer.from_pretrained(tokenizer_id)\n    except Exception:\n        return None\n\nconv_llava_llama_3 = Conversation(\n    system=\"You are a helpful language and vision assistant. \" \"You are able to understand the visual content that the user provides, \" \"and assist the user with a variety of tasks using natural language.\",\n    roles=(\"user\", \"assistant\"),\n    version=\"llama_v3\",\n    messages=[],\n    offset=0,\n    sep=\"<|eot_id|>\",\n    sep_style=SeparatorStyle.LLAMA_3,\n    tokenizer_id=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n    tokenizer=safe_load_tokenizer(\"meta-llama/Meta-Llama-3-8B-Instruct\"),\n    stop_token_ids=[128009],\n)\n\nconv_mistral_instruct = Conversation(\n    system=\"\",\n    roles=(\"USER\", \"ASSISTANT\"),\n    version=\"llama_v2\",\n    messages=[],\n    offset=0,\n    sep_style=SeparatorStyle.LLAMA_2,\n    sep=\"\",\n    sep2=\"</s>\",\n)\n\nconv_llava_llama_2_simple = Conversation(\n    system=\"Answer the questions about the visual content that the user provides.\",\n    roles=(\"USER\", \"ASSISTANT\"),\n    version=\"llama_v2\",\n    messages=[],\n    offset=0,\n    sep_style=SeparatorStyle.LLAMA_2,\n    sep=\"<s>\",\n    sep2=\"</s>\",\n)\n\nconv_llava_llama_2_mmtag = Conversation(\n    system=\"Answer the questions about the visual content that the user provides.\" \"The visual content will be provided with the following format: <Image>visual content</Image>.\",\n    roles=(\"USER\", \"ASSISTANT\"),\n    version=\"llama_v2_mmtag\",\n    messages=[],\n    offset=0,\n    sep_style=SeparatorStyle.LLAMA_2,\n    sep=\"<s>\",\n    sep2=\"</s>\",\n)\n\nconv_mpt = Conversation(\n    system=\"\"\"<|im_start|>system\nA conversation between a user and an LLM-based AI assistant. The assistant gives helpful and honest answers.\"\"\",\n    roles=(\"<|im_start|>user\\n\", \"<|im_start|>assistant\\n\"),\n    version=\"mpt\",\n    messages=[],\n    offset=0,\n    sep_style=SeparatorStyle.MPT,\n    sep=\"<|im_end|>\",\n)\n\nconv_qwen = Conversation(\n    system=\"\"\"<|im_start|>system\nYou are a helpful assistant.\"\"\",\n    roles=(\"<|im_start|>user\", \"<|im_start|>assistant\"),\n    version=\"qwen\",\n    messages=[],\n    offset=0,\n    sep_style=SeparatorStyle.CHATML,\n    sep=\"<|im_end|>\",\n)\n\nconv_gemma_instruct = Conversation(system=\"\", roles=(\"<start_of_turn>user\\n\", \"<start_of_turn>model\\n\"), version=\"gemma\", messages=[], offset=0, sep_style=SeparatorStyle.GEMMA, sep=\"<end_of_turn>\\n\")\n\nconv_llava_plain = Conversation(\n    system=\"\",\n    roles=(\"\", \"\"),\n    messages=[],\n    offset=0,\n    sep_style=SeparatorStyle.PLAIN,\n    sep=\"\\n\",\n)\n\nconv_llava_v0 = Conversation(\n    system=\"A chat between a curious human and an artificial intelligence assistant. \" \"The assistant gives helpful, detailed, and polite answers to the human's questions.\",\n    roles=(\"Human\", \"Assistant\"),\n    messages=[],\n    offset=0,\n    sep_style=SeparatorStyle.SINGLE,\n    sep=\"###\",\n)\n\nconv_llava_v0_mmtag = Conversation(\n    system=\"A chat between a curious user and an artificial intelligence assistant. \"\n    \"The assistant is able to understand the visual content that the user provides, and assist the user with a variety of tasks using natural language.\"\n    \"The visual content will be provided with the following format: <Image>visual content</Image>.\",\n    roles=(\"Human\", \"Assistant\"),\n    messages=[],\n    offset=0,\n    sep_style=SeparatorStyle.SINGLE,\n    sep=\"###\",\n    version=\"v0_mmtag\",\n)\n\nconv_llava_v1 = Conversation(\n    system=\"A chat between a curious human and an artificial intelligence assistant. \" \"The assistant gives helpful, detailed, and polite answers to the human's questions.\",\n    roles=(\"USER\", \"ASSISTANT\"),\n    version=\"v1\",\n    messages=[],\n    offset=0,\n    sep_style=SeparatorStyle.TWO,\n    sep=\" \",\n    sep2=\"</s>\",\n)\n\nconv_llava_v1_mmtag = Conversation(\n    system=\"A chat between a curious user and an artificial intelligence assistant. \"\n    \"The assistant is able to understand the visual content that the user provides, and assist the user with a variety of tasks using natural language.\"\n    \"The visual content will be provided with the following format: <Image>visual content</Image>.\",\n    roles=(\"USER\", \"ASSISTANT\"),\n    messages=[],\n    offset=0,\n    sep_style=SeparatorStyle.TWO,\n    sep=\" \",\n    sep2=\"</s>\",\n    version=\"v1_mmtag\",\n)\n\nconv_mistral_orca = Conversation(\n    system=\"\"\"<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI. Write out your reasoning step-by-step to be sure you get the right answers!\"\"\",\n    roles=(\"<|im_start|>user\\n\", \"<|im_start|>assistant\\n\"),\n    version=\"mpt\",\n    messages=[],\n    offset=0,\n    sep_style=SeparatorStyle.MPT,\n    sep=\"<|im_end|>\",\n)\n\nconv_mistral_zephyr = Conversation(\n    system=\"\"\"<|system|>\nYou are a helpful AI assistant.\"\"\",\n    roles=(\"<|user|>\\n\", \"<|assistant|>\\n\"),\n    version=\"mpt\",\n    messages=[],\n    offset=0,\n    sep_style=SeparatorStyle.MPT,\n    sep=\"</s>\",\n)\n\nconv_mistral_direct = Conversation(\n    system=\"\"\"<|im_start|>system\nAnswer the questions.\"\"\",\n    roles=(\"<|im_start|>user\\n\", \"<|im_start|>assistant\\n\"),\n    version=\"mpt\",\n    messages=[],\n    offset=0,\n    sep_style=SeparatorStyle.MPT,\n    sep=\"<|im_end|>\",\n)\n\nconv_chatml_direct = Conversation(\n    system=\"\"\"<|im_start|>system\nAnswer the questions.\"\"\",\n    roles=(\"<|im_start|>user\\n\", \"<|im_start|>assistant\\n\"),\n    version=\"mpt\",\n    messages=[],\n    offset=0,\n    sep_style=SeparatorStyle.MPT,\n    sep=\"<|im_end|>\",\n)\n\ndefault_conversation = conv_vicuna_v0\nconv_templates = {\n    \"default\": conv_vicuna_v0,\n    \"v0\": conv_vicuna_v0,\n    \"v1\": conv_vicuna_v1,\n    \"vicuna_v1\": conv_vicuna_v1,\n    \"llama_2\": conv_llama_2,\n    \"mistral_instruct\": conv_mistral_instruct,\n    \"mistral_orca\": conv_mistral_orca,\n    \"mistral_zephyr\": conv_mistral_zephyr,\n    \"mistral_direct\": conv_mistral_direct,\n    \"plain\": conv_llava_plain,\n    \"v0_plain\": conv_llava_plain,\n    \"chatml_direct\": conv_chatml_direct,\n    \"llava_v0\": conv_llava_v0,\n    \"llava_v0_mmtag\": conv_llava_v0_mmtag,\n    \"llava_v1\": conv_llava_v1,\n    \"llava_v1_mmtag\": conv_llava_v1_mmtag,\n    \"llava_llama_2\": conv_llava_llama_2,\n    \"llava_llama_3\": conv_llava_llama_3,\n    \"llava_llama_2_simple\": conv_llava_llama_2_simple,\n    \"llava_llama_2_mmtag\": conv_llava_llama_2_mmtag,\n    \"llava_mistral_instruct\": conv_mistral_instruct,\n    \"mpt\": conv_mpt,\n    \"qwen_1_5\": conv_qwen,\n    \"qwen_2\": conv_qwen,\n    \"gemma_instruct\": conv_gemma_instruct,\n}\n\n\nif __name__ == \"__main__\":\n    print(default_conversation.get_prompt())\n"}
{"type": "source_file", "path": "llava/model/language_model/llava_mistral.py", "content": "#    Copyright 2023 Haotian Liu\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\");\n#    you may not use this file except in compliance with the License.\n#    You may obtain a copy of the License at\n#\n#        http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS,\n#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#    See the License for the specific language governing permissions and\n#    limitations under the License.\n\n\nfrom typing import List, Optional, Tuple, Union\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import CrossEntropyLoss\n\nfrom transformers import AutoConfig, AutoModelForCausalLM, MistralConfig, MistralModel, MistralForCausalLM, GenerationConfig\n\nfrom transformers.modeling_outputs import CausalLMOutputWithPast\nfrom transformers.generation.utils import GenerateOutput\n\nfrom ..llava_arch import LlavaMetaModel, LlavaMetaForCausalLM\n\n\nclass LlavaMistralConfig(MistralConfig):\n    model_type = \"llava_mistral\"\n    temperature: float = 0.0  # reset to 0.0, previously 0.9 for Vicuna\n    max_new_tokens: int = 1024\n    do_sample: bool = False\n    top_p: Optional[float] = None\n\n\nclass LlavaMistralModel(LlavaMetaModel, MistralModel):\n    config_class = LlavaMistralConfig\n\n    def __init__(self, config: MistralConfig):\n        super(LlavaMistralModel, self).__init__(config)\n\n\nclass LlavaMistralForCausalLM(MistralForCausalLM, LlavaMetaForCausalLM):\n    config_class = LlavaMistralConfig\n\n    def __init__(self, config):\n        super(MistralForCausalLM, self).__init__(config)\n\n        config.model_type = \"llava_mistral\"\n        config.rope_scaling = None\n\n        self.model = LlavaMistralModel(config)\n        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_model(self):\n        return self.model\n\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_values: Optional[List[torch.FloatTensor]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        labels: Optional[torch.LongTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        images: Optional[torch.FloatTensor] = None,\n        image_sizes: Optional[List[List[int]]] = None,\n        return_dict: Optional[bool] = None,\n        cache_position=None,\n    ) -> Union[Tuple, CausalLMOutputWithPast]:\n\n        if inputs_embeds is None:\n            (input_ids, position_ids, attention_mask, past_key_values, inputs_embeds, labels) = self.prepare_inputs_labels_for_multimodal(input_ids, position_ids, attention_mask, past_key_values, labels, images, image_sizes)\n\n        return super().forward(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            past_key_values=past_key_values,\n            inputs_embeds=inputs_embeds,\n            labels=labels,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n    @torch.no_grad()\n    def generate(\n        self,\n        inputs: Optional[torch.Tensor] = None,\n        images: Optional[torch.Tensor] = None,\n        image_sizes: Optional[torch.Tensor] = None,\n        **kwargs,\n    ) -> Union[GenerateOutput, torch.LongTensor]:\n        position_ids = kwargs.pop(\"position_ids\", None)\n        attention_mask = kwargs.pop(\"attention_mask\", None)\n        if \"inputs_embeds\" in kwargs:\n            raise NotImplementedError(\"`inputs_embeds` is not supported\")\n\n        if images is not None:\n            (inputs, position_ids, attention_mask, _, inputs_embeds, _) = self.prepare_inputs_labels_for_multimodal(inputs, position_ids, attention_mask, None, None, images, image_sizes=image_sizes)\n        else:\n            inputs_embeds = self.get_model().embed_tokens(inputs)\n\n        return super().generate(position_ids=position_ids, attention_mask=attention_mask, inputs_embeds=inputs_embeds, **kwargs)\n\n    def prepare_inputs_for_generation(self, input_ids, past_key_values=None, inputs_embeds=None, **kwargs):\n        images = kwargs.pop(\"images\", None)\n        image_sizes = kwargs.pop(\"image_sizes\", None)\n        inputs = super().prepare_inputs_for_generation(input_ids, past_key_values=past_key_values, inputs_embeds=inputs_embeds, **kwargs)\n        if images is not None:\n            inputs[\"images\"] = images\n        if image_sizes is not None:\n            inputs[\"image_sizes\"] = image_sizes\n        return inputs\n\n\nAutoConfig.register(\"llava_mistral\", LlavaMistralConfig)\nAutoModelForCausalLM.register(LlavaMistralConfig, LlavaMistralForCausalLM)\n"}
{"type": "source_file", "path": "llava/mm_utils.py", "content": "from PIL import Image\nfrom io import BytesIO\nimport base64\nimport math\nimport ast\nimport re\nimport torch\nfrom transformers import StoppingCriteria\nfrom llava.constants import IMAGE_TOKEN_INDEX\n\n\ndef resize_and_center_crop(image, shortest_edge_length):\n    # Calculate new dimensions and resize\n    aspect_ratio = float(image.width) / float(image.height)\n    if aspect_ratio > 1:\n        new_width = int(shortest_edge_length * aspect_ratio)\n        new_height = shortest_edge_length\n    else:\n        new_width = shortest_edge_length\n        new_height = int(shortest_edge_length / aspect_ratio)\n    resized_image = image.resize((new_width, new_height), Image.ANTIALIAS)\n\n    # Calculate the position and perform the center crop\n    left = (new_width - shortest_edge_length) / 2\n    top = (new_height - shortest_edge_length) / 2\n    right = (new_width + shortest_edge_length) / 2\n    bottom = (new_height + shortest_edge_length) / 2\n    cropped_image = resized_image.crop((left, top, right, bottom))\n\n    return cropped_image\n\n\ndef auto_pad_images(image, grid_params):\n    assert isinstance(image, Image.Image), \"Input should be a Pillow Image\"\n    assert len(grid_params) > 0, \"Grid parameters should not be empty\"\n\n    # Step 1: Calculate and find the closest aspect ratio\n    input_width, input_height = image.size\n    input_aspect_ratio = input_width / input_height\n    candidate_resolutions = [(w / h, w, h) for w in grid_params for h in grid_params]\n    closest_aspect_ratio = min(candidate_resolutions, key=lambda x: abs(input_aspect_ratio - x[0]))\n\n    candidate_resolutions = [(x[1], x[2]) for x in candidate_resolutions if abs(x[0] - closest_aspect_ratio[0]) < 1e-3]\n\n    target_resolution = min(candidate_resolutions, key=lambda res: abs(max(input_width, input_height) / max(res) - 1))\n\n    resize_width, resize_height = target_resolution\n    if input_width > input_height:\n        resize_height = int(resize_width / input_aspect_ratio)\n    else:\n        resize_width = int(resize_height * input_aspect_ratio)\n    resized_image = image.resize((resize_width, resize_height), Image.ANTIALIAS)\n\n    # Step 5: Pad the resized image if necessary to match the target resolution\n    pad_width = target_resolution[0] - resize_width\n    pad_height = target_resolution[1] - resize_height\n    padded_image = Image.new(\"RGB\", target_resolution, color=(0, 0, 0))\n    padded_image.paste(resized_image, (pad_width // 2, pad_height // 2))\n\n    return padded_image\n\n\ndef extract_patches(image, patch_size, overlap_ratio):\n    assert isinstance(image, Image.Image), \"Input should be a Pillow Image\"\n    assert patch_size > 0, \"Patch size should be greater than 0\"\n    assert 0 <= overlap_ratio < 1, \"Overlap ratio should be between 0 and 1\"\n\n    W, H = image.size\n    patches = []\n\n    stride = int(patch_size * (1 - overlap_ratio))\n\n    num_patches_y = (H - patch_size) // stride + 1\n    num_patches_x = (W - patch_size) // stride + 1\n\n    y_start = (H - (num_patches_y - 1) * stride - patch_size) // 2\n    x_start = (W - (num_patches_x - 1) * stride - patch_size) // 2\n\n    for y in range(y_start, y_start + num_patches_y * stride, stride):\n        for x in range(x_start, x_start + num_patches_x * stride, stride):\n            patch = image.crop((x, y, x + patch_size, y + patch_size))\n            patches.append(patch)\n\n    return patches\n\n\ndef process_highres_image_crop_split(image, data_args, processor=None):\n    crop_resolution = data_args.image_crop_resolution\n    split_resolution = data_args.image_split_resolution\n    if processor is None:\n        processor = data_args.image_processor\n    image_crop = resize_and_center_crop(image, crop_resolution)\n    image_patches = extract_patches(image_crop, patch_size=split_resolution, overlap_ratio=0)\n    image_patches = [processor.preprocess(image_patch, return_tensors=\"pt\")[\"pixel_values\"][0] for image_patch in image_patches]\n    return torch.stack(image_patches, dim=0)\n\n\ndef process_highres_image(image, processor, grid_pinpoints):\n    grid_params = [int(x) for x in grid_pinpoints.split(\",\")]\n    width_height = max(image.size)\n    fit_grid_params = [x for x in grid_params if x >= width_height]\n    if len(fit_grid_params) == 0:\n        select_size = max(grid_params)\n    else:\n        select_size = min(fit_grid_params)\n    # FIXME: always select the 448\n    select_size = max(grid_params)\n    image_padded = expand2square(image, tuple(int(x * 255) for x in processor.image_mean))\n\n    # FIXME: this seems to be a bug that it always resizes instead of padding\n    image_original_resize = image.resize((processor.size[\"shortest_edge\"], processor.size[\"shortest_edge\"]))\n    image_padded = image_padded.resize((select_size, select_size))\n    image_patches = extract_patches(image_padded, patch_size=processor.size[\"shortest_edge\"], overlap_ratio=0)\n    image_patches = [image_original_resize] + image_patches\n    image_patches = [processor.preprocess(image_patch, return_tensors=\"pt\")[\"pixel_values\"][0] for image_patch in image_patches]\n    return torch.stack(image_patches, dim=0)\n\n\ndef select_best_resolution(original_size, possible_resolutions):\n    \"\"\"\n    Selects the best resolution from a list of possible resolutions based on the original size.\n\n    Args:\n        original_size (tuple): The original size of the image in the format (width, height).\n        possible_resolutions (list): A list of possible resolutions in the format [(width1, height1), (width2, height2), ...].\n\n    Returns:\n        tuple: The best fit resolution in the format (width, height).\n    \"\"\"\n    original_width, original_height = original_size\n    best_fit = None\n    max_effective_resolution = 0\n    min_wasted_resolution = float(\"inf\")\n\n    for width, height in possible_resolutions:\n        # Calculate the downscaled size to keep the aspect ratio\n        scale = min(width / original_width, height / original_height)\n        downscaled_width, downscaled_height = int(original_width * scale), int(original_height * scale)\n\n        # Calculate effective and wasted resolutions\n        effective_resolution = min(downscaled_width * downscaled_height, original_width * original_height)\n        wasted_resolution = (width * height) - effective_resolution\n\n        if effective_resolution > max_effective_resolution or (effective_resolution == max_effective_resolution and wasted_resolution < min_wasted_resolution):\n            max_effective_resolution = effective_resolution\n            min_wasted_resolution = wasted_resolution\n            best_fit = (width, height)\n\n    return best_fit\n\n\ndef resize_and_pad_image(image, target_resolution):\n    \"\"\"\n    Resize and pad an image to a target resolution while maintaining aspect ratio.\n\n    Args:\n        image (PIL.Image.Image): The input image.\n        target_resolution (tuple): The target resolution (width, height) of the image.\n\n    Returns:\n        PIL.Image.Image: The resized and padded image.\n    \"\"\"\n    original_width, original_height = image.size\n    target_width, target_height = target_resolution\n\n    # Determine which dimension (width or height) to fill\n    scale_w = target_width / original_width\n    scale_h = target_height / original_height\n\n    if scale_w < scale_h:\n        # Width will be filled completely\n        new_width = target_width\n        new_height = min(math.ceil(original_height * scale_w), target_height)\n    else:\n        # Height will be filled completely\n        new_height = target_height\n        new_width = min(math.ceil(original_width * scale_h), target_width)\n\n    # Resize the image\n    resized_image = image.resize((new_width, new_height))\n\n    # Create a new image with the target size and paste the resized image onto it\n    new_image = Image.new(\"RGB\", (target_width, target_height), (0, 0, 0))\n    paste_x = (target_width - new_width) // 2\n    paste_y = (target_height - new_height) // 2\n    new_image.paste(resized_image, (paste_x, paste_y))\n\n    return new_image\n\n\ndef divide_to_patches(image, patch_size):\n    \"\"\"\n    Divides an image into patches of a specified size.\n\n    Args:\n        image (PIL.Image.Image): The input image.\n        patch_size (int): The size of each patch.\n\n    Returns:\n        list: A list of PIL.Image.Image objects representing the patches.\n    \"\"\"\n    patches = []\n    width, height = image.size\n    for i in range(0, height, patch_size):\n        for j in range(0, width, patch_size):\n            box = (j, i, j + patch_size, i + patch_size)\n            patch = image.crop(box)\n            patches.append(patch)\n\n    return patches\n\n\ndef get_anyres_image_grid_shape(image_size, grid_pinpoints, patch_size):\n    \"\"\"\n    Calculate the shape of the image patch grid after the preprocessing for images of any resolution.\n\n    Args:\n        image_size (tuple): The size of the input image in the format (width, height).\n        grid_pinpoints (str): A string representation of a list of possible resolutions.\n        patch_size (int): The size of each image patch.\n\n    Returns:\n        tuple: The shape of the image patch grid in the format (width, height).\n    \"\"\"\n    if isinstance(grid_pinpoints, str) and \"x\" in grid_pinpoints:\n        assert patch_size in [224, 336, 384, 448, 512], \"patch_size should be in [224, 336, 384, 448, 512]\"\n        # Use regex to extract the range from the input string\n        matches = re.findall(r\"\\((\\d+)x(\\d+)\\)\", grid_pinpoints)\n        range_start = tuple(map(int, matches[0]))\n        range_end = tuple(map(int, matches[-1]))\n        # Generate a matrix of tuples from (range_start[0], range_start[1]) to (range_end[0], range_end[1])\n        grid_pinpoints = [(i, j) for i in range(range_start[0], range_end[0] + 1) for j in range(range_start[1], range_end[1] + 1)]\n        # Multiply all elements by patch_size\n        grid_pinpoints = [[dim * patch_size for dim in pair] for pair in grid_pinpoints]\n    if type(grid_pinpoints) is list:\n        possible_resolutions = grid_pinpoints\n    else:\n        possible_resolutions = ast.literal_eval(grid_pinpoints)\n    width, height = select_best_resolution(image_size, possible_resolutions)\n    return width // patch_size, height // patch_size\n\n\ndef process_anyres_image(image, processor, grid_pinpoints):\n    \"\"\"\n    Process an image with variable resolutions.\n\n    Args:\n        image (PIL.Image.Image): The input image to be processed.\n        processor: The image processor object.\n        grid_pinpoints (str): A string representation of a list of possible resolutions.\n\n    Returns:\n        torch.Tensor: A tensor containing the processed image patches.\n    \"\"\"\n    # Convert grid_pinpoints from string to list\n    if isinstance(grid_pinpoints, str) and \"x\" in grid_pinpoints:\n        try:\n            patch_size = processor.size[0]\n        except Exception as e:\n            patch_size = processor.size[\"shortest_edge\"]\n        assert patch_size in [224, 336, 384, 448, 512], \"patch_size should be in [224, 336, 384, 448, 512]\"\n        # Use regex to extract the range from the input string\n        matches = re.findall(r\"\\((\\d+)x(\\d+)\\)\", grid_pinpoints)\n        range_start = tuple(map(int, matches[0]))\n        range_end = tuple(map(int, matches[-1]))\n        # Generate a matrix of tuples from (range_start[0], range_start[1]) to (range_end[0], range_end[1])\n        grid_pinpoints = [(i, j) for i in range(range_start[0], range_end[0] + 1) for j in range(range_start[1], range_end[1] + 1)]\n        # Multiply all elements by patch_size\n        grid_pinpoints = [[dim * patch_size for dim in pair] for pair in grid_pinpoints]\n\n    if type(grid_pinpoints) is list:\n        possible_resolutions = grid_pinpoints\n    else:\n        possible_resolutions = ast.literal_eval(grid_pinpoints)\n    best_resolution = select_best_resolution(image.size, possible_resolutions)\n    image_padded = resize_and_pad_image(image, best_resolution)\n\n    patches = divide_to_patches(image_padded, processor.crop_size[\"height\"])\n\n    # FIXME: this seems to be a bug that it resizes instead of pad.\n    # but to keep it consistent with previous, i will keep it as it is\n    # TODO: uncomment below to ablate with the padding\n    if isinstance(processor.size, dict):\n        shortest_edge = processor.size[\"shortest_edge\"]\n    else:\n        shortest_edge = min(processor.size)\n    image_original_resize = image.resize((shortest_edge, shortest_edge))\n    # image_padded_square = expand2square(image, tuple(int(x*255) for x in processor.image_mean))\n    # image_original_resize = image_padded_square.resize((processor.size['shortest_edge'], processor.size['shortest_edge']))\n\n    image_patches = [image_original_resize] + patches\n    image_patches = [processor.preprocess(image_patch, return_tensors=\"pt\")[\"pixel_values\"][0] for image_patch in image_patches]\n    return torch.stack(image_patches, dim=0)\n\n\ndef load_image_from_base64(image):\n    return Image.open(BytesIO(base64.b64decode(image)))\n\n\ndef expand2square(pil_img, background_color):\n    width, height = pil_img.size\n    if width == height:\n        return pil_img\n    elif width > height:\n        result = Image.new(pil_img.mode, (width, width), background_color)\n        result.paste(pil_img, (0, (width - height) // 2))\n        return result\n    else:\n        result = Image.new(pil_img.mode, (height, height), background_color)\n        result.paste(pil_img, ((height - width) // 2, 0))\n        return result\n\n\ndef process_images(images, image_processor, model_cfg):\n    image_aspect_ratio = getattr(model_cfg, \"image_aspect_ratio\", None)\n    new_images = []\n    if image_aspect_ratio == \"highres\":\n        for image in images:\n            image = process_highres_image(image, image_processor, model_cfg.image_grid_pinpoints)\n            new_images.append(image)\n    elif image_aspect_ratio == \"anyres\" or \"anyres_max\" in image_aspect_ratio:\n        for image in images:\n            image = process_anyres_image(image, image_processor, model_cfg.image_grid_pinpoints)\n            new_images.append(image)\n    elif image_aspect_ratio == \"crop_split\":\n        for image in images:\n            image = process_highres_image_crop_split(image, model_cfg, image_processor)\n            new_images.append(image)\n    elif image_aspect_ratio == \"pad\":\n        for image in images:\n            image = expand2square(image, tuple(int(x * 255) for x in image_processor.image_mean))\n            image = image_processor.preprocess(image, return_tensors=\"pt\")[\"pixel_values\"][0]\n            new_images.append(image)\n    else:\n        return image_processor.preprocess(images, return_tensors=\"pt\")[\"pixel_values\"]\n    if all(x.shape == new_images[0].shape for x in new_images):\n        new_images = torch.stack(new_images, dim=0)\n    return new_images\n\n\ndef tokenizer_image_token(prompt, tokenizer, image_token_index=IMAGE_TOKEN_INDEX, return_tensors=None):\n    prompt_chunks = [tokenizer(chunk).input_ids for chunk in prompt.split(\"<image>\")]\n\n    def insert_separator(X, sep):\n        return [ele for sublist in zip(X, [sep] * len(X)) for ele in sublist][:-1]\n\n    input_ids = []\n    offset = 0\n    if len(prompt_chunks) > 0 and len(prompt_chunks[0]) > 0 and prompt_chunks[0][0] == tokenizer.bos_token_id:\n        offset = 1\n        input_ids.append(prompt_chunks[0][0])\n\n    for x in insert_separator(prompt_chunks, [image_token_index] * (offset + 1)):\n        input_ids.extend(x[offset:])\n\n    if return_tensors is not None:\n        if return_tensors == \"pt\":\n            return torch.tensor(input_ids, dtype=torch.long)\n        raise ValueError(f\"Unsupported tensor type: {return_tensors}\")\n    return input_ids\n\n\ndef get_model_name_from_path(model_path):\n    model_path = model_path.strip(\"/\")\n    model_paths = model_path.split(\"/\")\n    if model_paths[-1].startswith(\"checkpoint-\"):\n        return model_paths[-2] + \"_\" + model_paths[-1]\n    else:\n        return model_paths[-1]\n\n\nclass KeywordsStoppingCriteria(StoppingCriteria):\n    def __init__(self, keywords, tokenizer, input_ids):\n        self.keywords = keywords\n        self.keyword_ids = []\n        for keyword in keywords:\n            cur_keyword_ids = tokenizer(keyword).input_ids\n            if len(cur_keyword_ids) > 1 and cur_keyword_ids[0] == tokenizer.bos_token_id:\n                cur_keyword_ids = cur_keyword_ids[1:]\n            self.keyword_ids.append(torch.tensor(cur_keyword_ids))\n        self.tokenizer = tokenizer\n        self.start_len = input_ids.shape[1]\n\n    def __call__(self, output_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n        assert output_ids.shape[0] == 1, \"Only support batch size 1 (yet)\"  # TODO\n        offset = min(output_ids.shape[1] - self.start_len, 3)\n        self.keyword_ids = [keyword_id.to(output_ids.device) for keyword_id in self.keyword_ids]\n        for keyword_id in self.keyword_ids:\n            if output_ids[0, -keyword_id.shape[0] :] == keyword_id:\n                return True\n        outputs = self.tokenizer.batch_decode(output_ids[:, -offset:], skip_special_tokens=True)[0]\n        for keyword in self.keywords:\n            if keyword in outputs:\n                return True\n        return False\n"}
{"type": "source_file", "path": "llava/model/builder.py", "content": "#    Copyright 2023 Haotian Liu\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\");\n#    you may not use this file except in compliance with the License.\n#    You may obtain a copy of the License at\n#\n#        http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS,\n#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#    See the License for the specific language governing permissions and\n#    limitations under the License.\n\n\nimport os\nimport warnings\nimport shutil\n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig, BitsAndBytesConfig\nimport torch\nfrom llava.model import *\nfrom llava.constants import DEFAULT_IMAGE_PATCH_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN\nfrom llava.utils import rank0_print\n\n\ndef load_pretrained_model(model_path, model_base, model_name, load_8bit=False, load_4bit=False, device_map=\"auto\", torch_dtype=\"float16\",attn_implementation=\"flash_attention_2\", customized_config=None, overwrite_config=None, **kwargs):\n    kwargs[\"device_map\"] = device_map\n\n    if load_8bit:\n        kwargs[\"load_in_8bit\"] = True\n    elif load_4bit:\n        kwargs[\"load_in_4bit\"] = True\n        kwargs[\"quantization_config\"] = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.float16, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\")\n    elif torch_dtype == \"float16\":\n        kwargs[\"torch_dtype\"] = torch.float16\n    elif torch_dtype == \"bfloat16\":\n        kwargs[\"torch_dtype\"] = torch.bfloat16\n    else:\n        import pdb;pdb.set_trace()\n\n    if customized_config is not None:\n        kwargs[\"config\"] = customized_config\n\n    if \"multimodal\" in kwargs:\n        if kwargs[\"multimodal\"] is True:\n            is_multimodal = True\n            kwargs.pop(\"multimodal\")\n    else:\n        is_multimodal = False\n\n    if \"llava\" in model_name.lower() or is_multimodal:\n        # Load LLaVA model\n        if \"lora\" in model_name.lower() and model_base is None:\n            warnings.warn(\n                \"There is `lora` in model name but no `model_base` is provided. If you are loading a LoRA model, please provide the `model_base` argument. Detailed instruction: https://github.com/haotian-liu/LLaVA#launch-a-model-worker-lora-weights-unmerged.\"\n            )\n        if \"lora\" in model_name.lower() and model_base is not None:\n            lora_cfg_pretrained = AutoConfig.from_pretrained(model_path)\n            tokenizer = AutoTokenizer.from_pretrained(model_base, use_fast=False)\n            rank0_print(\"Loading LLaVA from base model...\")\n            if \"mixtral\" in model_name.lower():\n                from llava.model.language_model.llava_mixtral import LlavaMixtralConfig\n\n                lora_cfg_pretrained = LlavaMixtralConfig.from_pretrained(model_path)\n                tokenizer = AutoTokenizer.from_pretrained(model_base, use_fast=False)\n                model = LlavaMixtralForCausalLM.from_pretrained(model_base, low_cpu_mem_usage=True, config=lora_cfg_pretrained, attn_implementation=attn_implementation, **kwargs)\n            elif \"mistral\" in model_name.lower():\n                from llava.model.language_model.llava_mistral import LlavaMistralConfig\n\n                lora_cfg_pretrained = LlavaMistralConfig.from_pretrained(model_path)\n                tokenizer = AutoTokenizer.from_pretrained(model_base, use_fast=False)\n                model = LlavaMistralForCausalLM.from_pretrained(model_base, low_cpu_mem_usage=True, config=lora_cfg_pretrained, attn_implementation=attn_implementation, **kwargs)\n            elif \"gemma\" in model_name.lower():\n                from llava.model.language_model.llava_gemma import LlavaGemmaConfig\n\n                lora_cfg_pretrained = LlavaGemmaConfig.from_pretrained(model_path)\n                tokenizer = AutoTokenizer.from_pretrained(model_base, use_fast=False)\n                model = LlavaGemmaForCausalLM.from_pretrained(model_base, low_cpu_mem_usage=True, config=lora_cfg_pretrained, attn_implementation=attn_implementation, **kwargs)\n            elif \"qwen\" in model_name.lower():\n                from llava.model.language_model.llava_qwen import LlavaQwenConfig\n                llava_cfg = LlavaQwenConfig.from_pretrained(model_path)\n                # rank0_print(f\"Overwriting config with {overwrite_config}\")\n                # for k, v in overwrite_config.items():\n                #     setattr(llava_cfg, k, v)\n                model = LlavaQwenForCausalLM.from_pretrained(model_base, low_cpu_mem_usage=True, attn_implementation=attn_implementation, config=llava_cfg, **kwargs)\n\n            else:\n                from llava.model.language_model.llava_llama import LlavaConfig\n\n                lora_cfg_pretrained = LlavaConfig.from_pretrained(model_path)\n                tokenizer = AutoTokenizer.from_pretrained(model_base, use_fast=False)\n                model = LlavaLlamaForCausalLM.from_pretrained(model_base, low_cpu_mem_usage=True, config=lora_cfg_pretrained, attn_implementation=attn_implementation, **kwargs)\n\n            token_num, tokem_dim = model.lm_head.out_features, model.lm_head.in_features\n            if model.lm_head.weight.shape[0] != token_num:\n                model.lm_head.weight = torch.nn.Parameter(torch.empty(token_num, tokem_dim, device=model.device, dtype=model.dtype))\n                model.model.embed_tokens.weight = torch.nn.Parameter(torch.empty(token_num, tokem_dim, device=model.device, dtype=model.dtype))\n\n            rank0_print(\"Loading additional LLaVA weights...\")\n            if os.path.exists(os.path.join(model_path, \"non_lora_trainables.bin\")):\n                non_lora_trainables = torch.load(os.path.join(model_path, \"non_lora_trainables.bin\"), map_location=\"cpu\")\n            else:\n                # this is probably from HF Hub\n                from huggingface_hub import hf_hub_download\n\n                def load_from_hf(repo_id, filename, subfolder=None):\n                    cache_file = hf_hub_download(repo_id=repo_id, filename=filename, subfolder=subfolder)\n                    return torch.load(cache_file, map_location=\"cpu\")\n\n                non_lora_trainables = load_from_hf(model_path, \"non_lora_trainables.bin\")\n            non_lora_trainables = {(k[11:] if k.startswith(\"base_model.\") else k): v for k, v in non_lora_trainables.items()}\n            if any(k.startswith(\"model.model.\") for k in non_lora_trainables):\n                non_lora_trainables = {(k[6:] if k.startswith(\"model.\") else k): v for k, v in non_lora_trainables.items()}\n            model.load_state_dict(non_lora_trainables, strict=False)\n\n            from peft import PeftModel\n\n            rank0_print(\"Loading LoRA weights...\")\n            model = PeftModel.from_pretrained(model, model_path)\n            rank0_print(\"Merging LoRA weights...\")\n            model = model.merge_and_unload()\n            rank0_print(\"Model is loaded...\")\n        elif model_base is not None:  # this may be mm projector only, loading projector with preset language mdoel\n            rank0_print(f\"Loading LLaVA from base model {model_base}...\")\n            if \"mixtral\" in model_name.lower():\n                tokenizer = AutoTokenizer.from_pretrained(model_base, use_fast=False)\n                cfg_pretrained = AutoConfig.from_pretrained(model_path)\n                model = LlavaMixtralForCausalLM.from_pretrained(model_base, low_cpu_mem_usage=True, config=cfg_pretrained, attn_implementation=attn_implementation, **kwargs)\n            elif \"mistral\" in model_name.lower() or \"zephyr\" in model_name.lower():\n                tokenizer = AutoTokenizer.from_pretrained(model_base, use_fast=False)\n                cfg_pretrained = AutoConfig.from_pretrained(model_path)\n                model = LlavaMistralForCausalLM.from_pretrained(model_base, low_cpu_mem_usage=True, config=cfg_pretrained, attn_implementation=attn_implementation, **kwargs)\n            elif \"gemma\" in model_name.lower():\n                tokenizer = AutoTokenizer.from_pretrained(model_base, use_fast=False)\n                cfg_pretrained = AutoConfig.from_pretrained(model_path)\n                model = LlavaGemmaForCausalLM.from_pretrained(model_base, low_cpu_mem_usage=True, config=cfg_pretrained, attn_implementation=attn_implementation, **kwargs)\n            elif (\n                \"wizardlm-2\" in model_name.lower()\n                and \"vicuna\" in model_name.lower()\n                or \"llama\" in model_name.lower()\n                or \"yi\" in model_name.lower()\n                or \"nous-hermes\" in model_name.lower()\n                or \"llava-v1.6-34b\" in model_name.lower()\n                or \"llava-v1.5\" in model_name.lower()\n            ):\n                from llava.model.language_model.llava_llama import LlavaConfig\n\n                tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)\n                if customized_config is None:\n                    llava_cfg = LlavaConfig.from_pretrained(model_path)\n                    if \"v1.5\" in model_name.lower():\n                        llava_cfg.delay_load = True  # a workaround for correctly loading v1.5 models\n                else:\n                    llava_cfg = customized_config\n\n                tokenizer = AutoTokenizer.from_pretrained(model_base, use_fast=False)\n                llava_cfg = LlavaConfig.from_pretrained(model_path)\n                model = LlavaLlamaForCausalLM.from_pretrained(model_base, low_cpu_mem_usage=True, config=llava_cfg, **kwargs)\n            else:\n                raise ValueError(f\"Model {model_name} not supported\")\n\n            mm_projector_weights = torch.load(os.path.join(model_path, \"mm_projector.bin\"), map_location=\"cpu\")\n            mm_projector_weights = {k: v.to(torch.float16) for k, v in mm_projector_weights.items()}\n            model.load_state_dict(mm_projector_weights, strict=False)\n        else:\n            rank0_print(f\"Loaded LLaVA model: {model_path}\")\n            if \"mixtral\" in model_name.lower():\n                from llava.model.language_model.llava_mixtral import LlavaMixtralConfig\n\n                tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)\n                if customized_config is None:\n                    llava_cfg = LlavaMixtralConfig.from_pretrained(model_path)\n                else:\n                    llava_cfg = customized_config\n\n                if overwrite_config is not None:\n                    rank0_print(f\"Overwriting config with {overwrite_config}\")\n                    for k, v in overwrite_config.items():\n                        setattr(llava_cfg, k, v)\n\n                tokenizer = AutoTokenizer.from_pretrained(model_path)\n                model = LlavaMixtralForCausalLM.from_pretrained(model_path, low_cpu_mem_usage=True, attn_implementation=attn_implementation, config=llava_cfg, **kwargs)\n\n            elif \"mistral\" in model_name.lower() or \"zephyr\" in model_name.lower():\n                tokenizer = AutoTokenizer.from_pretrained(model_path)\n                model = LlavaMistralForCausalLM.from_pretrained(model_path, low_cpu_mem_usage=True, attn_implementation=attn_implementation, **kwargs)\n            elif (\n                \"wizardlm-2\" in model_name.lower()\n                and \"vicuna\" in model_name.lower()\n                or \"llama\" in model_name.lower()\n                or \"yi\" in model_name.lower()\n                or \"nous-hermes\" in model_name.lower()\n                or \"llava-v1.6-34b\" in model_name.lower()\n                or \"llava-v1.5\" in model_name.lower()\n                or \"llava-v1.6\" in model_name.lower()\n                or \"llava16\" in model_name.lower()\n                or \"llavanext\" in model_name.lower()\n            ):\n                from llava.model.language_model.llava_llama import LlavaConfig\n\n                tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)\n                if customized_config is None:\n                    llava_cfg = LlavaConfig.from_pretrained(model_path)\n                    if \"v1.5\" in model_name.lower():\n                        llava_cfg.delay_load = True  # a workaround for correctly loading v1.5 models\n                else:\n                    llava_cfg = customized_config\n\n                if overwrite_config is not None:\n                    rank0_print(f\"Overwriting config with {overwrite_config}\")\n                    for k, v in overwrite_config.items():\n                        setattr(llava_cfg, k, v)\n\n                model = LlavaLlamaForCausalLM.from_pretrained(model_path, low_cpu_mem_usage=True, attn_implementation=attn_implementation, config=llava_cfg, **kwargs)\n\n            elif \"qwen\" in model_name.lower() or \"quyen\" in model_name.lower():\n                tokenizer = AutoTokenizer.from_pretrained(model_path)\n                if \"moe\" in model_name.lower() or \"A14B\" in model_name.lower():\n                    from llava.model.language_model.llava_qwen_moe import LlavaQwenMoeConfig\n                    if overwrite_config is not None:\n                        llava_cfg = LlavaQwenMoeConfig.from_pretrained(model_path)\n                        rank0_print(f\"Overwriting config with {overwrite_config}\")\n                        for k, v in overwrite_config.items():\n                            setattr(llava_cfg, k, v)\n                        model = LlavaQwenMoeForCausalLM.from_pretrained(model_path, low_cpu_mem_usage=True, attn_implementation=attn_implementation, config=llava_cfg, **kwargs)\n                    else:\n                        model = LlavaQwenMoeForCausalLM.from_pretrained(model_path, low_cpu_mem_usage=True, attn_implementation=attn_implementation, **kwargs)\n\n                else:\n                    from llava.model.language_model.llava_qwen import LlavaQwenConfig\n                    if overwrite_config is not None:\n                        llava_cfg = LlavaQwenConfig.from_pretrained(model_path)\n                        rank0_print(f\"Overwriting config with {overwrite_config}\")\n                        for k, v in overwrite_config.items():\n                            setattr(llava_cfg, k, v)\n                        model = LlavaQwenForCausalLM.from_pretrained(model_path, low_cpu_mem_usage=True, attn_implementation=attn_implementation, config=llava_cfg, **kwargs)\n                    else:\n                        model = LlavaQwenForCausalLM.from_pretrained(model_path, low_cpu_mem_usage=True, attn_implementation=attn_implementation, **kwargs)\n\n            elif \"gemma\" in model_name.lower():\n                tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)\n                cfg_pretrained = AutoConfig.from_pretrained(model_path)\n                model = LlavaGemmaForCausalLM.from_pretrained(model_path, low_cpu_mem_usage=True, config=cfg_pretrained, attn_implementation=attn_implementation, **kwargs)\n            else:\n                try:\n                    from llava.model.language_model.llava_llama import LlavaConfig\n\n                    tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)\n                    if customized_config is None:\n                        llava_cfg = LlavaConfig.from_pretrained(model_path)\n                        if \"v1.5\" in model_path.lower():\n                            llava_cfg.delay_load = True  # a workaround for correctly loading v1.5 models\n                    else:\n                        llava_cfg = customized_config\n\n                    if overwrite_config is not None:\n                        rank0_print(f\"Overwriting config with {overwrite_config}\")\n                        for k, v in overwrite_config.items():\n                            setattr(llava_cfg, k, v)\n                    model = LlavaLlamaForCausalLM.from_pretrained(model_path, low_cpu_mem_usage=True, attn_implementation=attn_implementation, config=llava_cfg, **kwargs)\n                except:\n                    raise ValueError(f\"Model {model_name} not supported\")\n\n    else:\n        # Load language model\n        if model_base is not None:\n            # PEFT model\n            from peft import PeftModel\n\n            tokenizer = AutoTokenizer.from_pretrained(model_base, use_fast=False)\n            model = AutoModelForCausalLM.from_pretrained(model_base, torch_dtype=torch.float16, low_cpu_mem_usage=True, device_map=\"auto\")\n            print(f\"Loading LoRA weights from {model_path}\")\n            model = PeftModel.from_pretrained(model, model_path)\n            print(f\"Merging weights\")\n            model = model.merge_and_unload()\n            print(\"Convert to FP16...\")\n            model.to(torch.float16)\n        else:\n            use_fast = False\n            if \"mpt\" in model_name.lower().replace(\"prompt\", \"\"):\n                tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=True)\n                model = AutoModelForCausalLM.from_pretrained(model_path, low_cpu_mem_usage=True, trust_remote_code=True, **kwargs)\n            else:\n                tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)\n                model = AutoModelForCausalLM.from_pretrained(model_path, low_cpu_mem_usage=True, **kwargs)\n\n    rank0_print(f\"Model Class: {model.__class__.__name__}\")\n    image_processor = None\n\n    if \"llava\" in model_name.lower() or is_multimodal:\n        mm_use_im_start_end = getattr(model.config, \"mm_use_im_start_end\", False)\n        mm_use_im_patch_token = getattr(model.config, \"mm_use_im_patch_token\", True)\n        if mm_use_im_patch_token:\n            tokenizer.add_tokens([DEFAULT_IMAGE_PATCH_TOKEN], special_tokens=True)\n        if mm_use_im_start_end:\n            tokenizer.add_tokens([DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN], special_tokens=True)\n        \n        if 'reward' not in model_name.lower():\n            model.resize_token_embeddings(len(tokenizer))\n\n        vision_tower = model.get_vision_tower()\n        if not vision_tower.is_loaded:\n            vision_tower.load_model(device_map=device_map)\n        if device_map != \"auto\":\n            vision_tower.to(device=\"cuda\", dtype=torch.float16)\n        image_processor = vision_tower.image_processor\n\n    if hasattr(model.config, \"max_sequence_length\"):\n        context_len = model.config.max_sequence_length\n    elif hasattr(model.config, \"max_position_embeddings\"):\n        context_len = model.config.max_position_embeddings\n    elif hasattr(model.config, \"tokenizer_model_max_length\"):\n        context_len = model.config.tokenizer_model_max_length\n    else:\n        context_len = 2048\n\n    return tokenizer, model, image_processor, context_len\n"}
{"type": "source_file", "path": "llava/eval/model_vqa.py", "content": "import argparse\nimport torch\nimport os\nimport json\nfrom tqdm import tqdm\nimport shortuuid\n\nfrom llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN\nfrom llava.conversation import conv_templates, SeparatorStyle\nfrom llava.model.builder import load_pretrained_model\nfrom llava.utils import disable_torch_init\nfrom llava.mm_utils import tokenizer_image_token, get_model_name_from_path, KeywordsStoppingCriteria\n\nfrom llava.constants import IGNORE_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN, IMAGE_TOKEN_INDEX\nfrom typing import Dict, Optional, Sequence, List\nimport transformers\nimport re\n\nfrom PIL import Image\nimport math\n\n\ndef split_list(lst, n):\n    \"\"\"Split a list into n (roughly) equal-sized chunks\"\"\"\n    chunk_size = math.ceil(len(lst) / n)  # integer division\n    return [lst[i:i+chunk_size] for i in range(0, len(lst), chunk_size)]\n\n\ndef get_chunk(lst, n, k):\n    chunks = split_list(lst, n)\n    return chunks[k]\n\ndef preprocess_qwen(sources, tokenizer: transformers.PreTrainedTokenizer, has_image: bool = False, max_len=2048, system_message: str = \"You are a helpful assistant.\") -> Dict:\n    roles = {\"human\": \"<|im_start|>user\", \"gpt\": \"<|im_start|>assistant\"}\n\n    im_start, im_end = tokenizer.additional_special_tokens_ids\n    nl_tokens = tokenizer(\"\\n\").input_ids\n    _system = tokenizer(\"system\").input_ids + nl_tokens\n    _user = tokenizer(\"user\").input_ids + nl_tokens\n    _assistant = tokenizer(\"assistant\").input_ids + nl_tokens\n\n    # Apply prompt templates\n    input_ids, targets = [], []\n\n    source = sources\n    if roles[source[0][\"from\"]] != roles[\"human\"]:\n        source = source[1:]\n\n    input_id, target = [], []\n    system = [im_start] + _system + tokenizer(system_message).input_ids + [im_end] + nl_tokens\n    input_id += system\n    target += [im_start] + [IGNORE_INDEX] * (len(system) - 3) + [im_end] + nl_tokens\n    assert len(input_id) == len(target)\n    for j, sentence in enumerate(source):\n        role = roles[sentence[\"from\"]]\n        if has_image and sentence[\"value\"] is not None and \"<image>\" in sentence[\"value\"]:\n            num_image = len(re.findall(DEFAULT_IMAGE_TOKEN, sentence[\"value\"]))\n            texts = sentence[\"value\"].split('<image>')\n            _input_id = tokenizer(role).input_ids + nl_tokens \n            for i,text in enumerate(texts):\n                _input_id += tokenizer(text).input_ids \n                if i<len(texts)-1:\n                    _input_id += [IMAGE_TOKEN_INDEX] + nl_tokens\n            _input_id += [im_end] + nl_tokens\n            assert sum([i==IMAGE_TOKEN_INDEX for i in _input_id])==num_image\n        else:\n            if sentence[\"value\"] is None:\n                _input_id = tokenizer(role).input_ids + nl_tokens\n            else:\n                _input_id = tokenizer(role).input_ids + nl_tokens + tokenizer(sentence[\"value\"]).input_ids + [im_end] + nl_tokens\n        input_id += _input_id\n        if role == \"<|im_start|>user\":\n            _target = [im_start] + [IGNORE_INDEX] * (len(_input_id) - 3) + [im_end] + nl_tokens\n        elif role == \"<|im_start|>assistant\":\n            _target = [im_start] + [IGNORE_INDEX] * len(tokenizer(role).input_ids) + _input_id[len(tokenizer(role).input_ids) + 1 : -2] + [im_end] + nl_tokens\n        else:\n            raise NotImplementedError\n        target += _target\n\n    input_ids.append(input_id)\n    targets.append(target)\n    input_ids = torch.tensor(input_ids, dtype=torch.long)\n    targets = torch.tensor(targets, dtype=torch.long)\n    return input_ids\n\ndef eval_model(args):\n    \n    # Model\n    disable_torch_init()\n    model_path = os.path.expanduser(args.model_path)\n    model_name = get_model_name_from_path(model_path)\n    tokenizer, model, image_processor, context_len = load_pretrained_model(model_path, args.model_base, model_name)\n\n    # Data\n    with open(os.path.expanduser(args.question_file)) as f:\n        questions = json.load(f)\n    questions = get_chunk(questions, args.num_chunks, args.chunk_idx)\n    answers_file = os.path.expanduser(args.answers_file)\n    os.makedirs(os.path.dirname(answers_file), exist_ok=True)\n    ans_file = open(answers_file, \"w\")\n    \n    for line in tqdm(questions):\n        idx = line[\"sample_id\"]\n        question_type = line[\"metadata\"][\"question_type\"]\n        dataset_name = line[\"metadata\"][\"dataset\"]\n        gt = line[\"conversations\"][1][\"value\"]\n\n        image_files = line[\"image\"]\n        qs = line[\"conversations\"][0][\"value\"]\n        cur_prompt = args.extra_prompt + qs\n\n        args.conv_mode = \"qwen_1_5\"\n\n        conv = conv_templates[args.conv_mode].copy()\n        conv.append_message(conv.roles[0], qs)\n        conv.append_message(conv.roles[1], None)\n        prompt = conv.get_prompt()\n\n        input_ids = preprocess_qwen([line[\"conversations\"][0],{'from': 'gpt','value': None}], tokenizer, has_image=True).cuda()\n        img_num = list(input_ids.squeeze()).count(IMAGE_TOKEN_INDEX)\n\n        image_tensors = []\n        for image_file in image_files:\n            image = Image.open(os.path.join(args.image_folder, image_file))\n            image_tensor = image_processor.preprocess(image, return_tensors='pt')['pixel_values']\n            image_tensors.append(image_tensor.half().cuda())\n        # image_tensors = torch.cat(image_tensors, dim=0)\n\n        stop_str = conv.sep if conv.sep_style != SeparatorStyle.TWO else conv.sep2\n        keywords = [stop_str]\n        stopping_criteria = KeywordsStoppingCriteria(keywords, tokenizer, input_ids)\n\n        with torch.inference_mode():\n            output_ids = model.generate(\n                input_ids,\n                images=image_tensors,\n                do_sample=True if args.temperature > 0 else False,\n                temperature=args.temperature,\n                top_p=args.top_p,\n                num_beams=args.num_beams,\n                # no_repeat_ngram_size=3,\n                max_new_tokens=1024,\n                use_cache=True)\n\n        \n        outputs = tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0]\n        outputs = outputs.strip()\n        if outputs.endswith(stop_str):\n            outputs = outputs[:-len(stop_str)]\n        outputs = outputs.strip()\n\n        ans_id = shortuuid.uuid()\n        ans_file.write(json.dumps({\n                                   \"dataset\": dataset_name,\n                                   \"sample_id\": idx,\n                                   \"prompt\": cur_prompt,\n                                   \"pred_response\": outputs,\n                                   \"gt_response\": gt,\n                                   \"shortuuid\": ans_id,\n                                   \"model_id\": model_name,\n                                   \"question_type\": question_type,\n                                   }) + \"\\n\")\n        ans_file.flush()\n\n        if len(line[\"conversations\"]) > 2:\n\n            for i in range(2, len(line[\"conversations\"]), 2):\n                input_ids = torch.cat((input_ids, output_ids), dim=1)\n\n                gt = line[\"conversations\"][i + 1][\"value\"]\n                qs = line[\"conversations\"][i][\"value\"]\n                cur_prompt = args.extra_prompt + qs\n\n                args.conv_mode = \"qwen_1_5\"\n\n                conv = conv_templates[args.conv_mode].copy()\n                conv.append_message(conv.roles[0], qs)\n                conv.append_message(conv.roles[1], None)\n                prompt = conv.get_prompt()\n\n                input_ids_new = preprocess_qwen([line[\"conversations\"][i],{'from': 'gpt','value': None}], tokenizer, has_image=True).cuda()\n                input_ids = torch.cat((input_ids, input_ids_new), dim=1)\n                img_num = list(input_ids_new.squeeze()).count(IMAGE_TOKEN_INDEX)\n\n                stop_str = conv.sep if conv.sep_style != SeparatorStyle.TWO else conv.sep2\n                keywords = [stop_str]\n                stopping_criteria = KeywordsStoppingCriteria(keywords, tokenizer, input_ids)\n\n                with torch.inference_mode():\n                    output_ids = model.generate(\n                        input_ids,\n                        images=image_tensors,\n                        do_sample=True if args.temperature > 0 else False,\n                        temperature=args.temperature,\n                        top_p=args.top_p,\n                        num_beams=args.num_beams,\n                        # no_repeat_ngram_size=3,\n                        max_new_tokens=1024,\n                        use_cache=True)\n        \n                outputs = tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0]\n                outputs = outputs.strip()\n                if outputs.endswith(stop_str):\n                    outputs = outputs[:-len(stop_str)]\n                outputs = outputs.strip()\n\n                ans_id = shortuuid.uuid()\n                ans_file.write(json.dumps({\n                                        \"dataset\": dataset_name,\n                                        \"sample_id\": idx,\n                                        \"prompt\": cur_prompt,\n                                        \"pred_response\": outputs,\n                                        \"gt_response\": gt,\n                                        \"shortuuid\": ans_id,\n                                        \"model_id\": model_name,\n                                        \"question_type\": question_type,\n                                        }) + \"\\n\")\n                ans_file.flush()\n\n\n    ans_file.close()\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--model-path\", type=str, default=\"facebook/opt-350m\")\n    parser.add_argument(\"--model-base\", type=str, default=None)\n    parser.add_argument(\"--image-folder\", type=str, default=\"\")\n    parser.add_argument(\"--extra-prompt\", type=str, default=\"\")\n    parser.add_argument(\"--question-file\", type=str, default=\"tables/question.jsonl\")\n    parser.add_argument(\"--answers-file\", type=str, default=\"answer.jsonl\")\n    parser.add_argument(\"--conv-mode\", type=str, default=\"llava_v1\")\n    parser.add_argument(\"--num-chunks\", type=int, default=1)\n    parser.add_argument(\"--chunk-idx\", type=int, default=0)\n    parser.add_argument(\"--temperature\", type=float, default=0.2)\n    parser.add_argument(\"--top_p\", type=float, default=None)\n    parser.add_argument(\"--num_beams\", type=int, default=1)\n    parser.add_argument(\"--test_size\", type=int, default=10000000)\n    args = parser.parse_args()\n\n    eval_model(args)"}
{"type": "source_file", "path": "llava/model/__init__.py", "content": "import os\n\nAVAILABLE_MODELS = {\n    \"llava_llama\": \"LlavaLlamaForCausalLM, LlavaConfig\",\n    \"llava_qwen\": \"LlavaQwenForCausalLM, LlavaQwenConfig\",\n    \"llava_mistral\": \"LlavaMistralForCausalLM, LlavaMistralConfig\",\n    \"llava_mixtral\": \"LlavaMixtralForCausalLM, LlavaMixtralConfig\",\n    # \"llava_qwen_moe\": \"LlavaQwenMoeForCausalLM, LlavaQwenMoeConfig\",    \n    # Add other models as needed\n}\n\nfor model_name, model_classes in AVAILABLE_MODELS.items():\n    try:\n        exec(f\"from .language_model.{model_name} import {model_classes}\")\n    except Exception as e:\n        print(f\"Failed to import {model_name} from llava.language_model.{model_name}. Error: {e}\")\n"}
{"type": "source_file", "path": "llava/model/language_model/llava_qwen_moe.py", "content": "#    Copyright 2024 Hao Zhang\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\");\n#    you may not use this file except in compliance with the License.\n#    You may obtain a copy of the License at\n#\n#        http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS,\n#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#    See the License for the specific language governing permissions and\n#    limitations under the License.\n\n\nfrom typing import List, Optional, Tuple, Union, Dict\nimport torch\nimport torch.nn as nn\nfrom torch.nn import CrossEntropyLoss\n\nimport transformers\nfrom transformers import AutoConfig, AutoModelForCausalLM\n\nfrom transformers.modeling_outputs import CausalLMOutputWithPast\nfrom transformers.generation.utils import GenerateOutput\n\n# from ...constants import IGNORE_INDEX, IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN\nfrom llava.model.llava_arch import LlavaMetaModel, LlavaMetaForCausalLM\nfrom transformers import Qwen2MoeConfig, Qwen2MoeModel, Qwen2MoeForCausalLM\n\n# from .qwen.modeling_qwen import QWenLMHeadModel, QWenModel\n# from .qwen.configuration_qwen import QWenConfig\n\n\nclass LlavaQwenMoeConfig(Qwen2MoeConfig):\n    model_type = \"llava_qwen_moe\"\n\n\nclass LlavaQwenMoeModel(LlavaMetaModel, Qwen2MoeModel):\n    config_class = LlavaQwenMoeConfig\n\n    def __init__(self, config: Qwen2MoeConfig):\n        super(LlavaQwenMoeModel, self).__init__(config)\n\n\nclass LlavaQwenMoeForCausalLM(Qwen2MoeForCausalLM, LlavaMetaForCausalLM):\n    config_class = LlavaQwenMoeConfig\n\n    def __init__(self, config):\n        # super(Qwen2MoeForCausalLM, self).__init__(config)\n        Qwen2MoeForCausalLM.__init__(self, config)\n        config.model_type = \"llava_qwen_moe\"\n        config.rope_scaling = None\n\n        self.model = LlavaQwenMoeModel(config)\n        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_model(self):\n        return self.model\n\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_values: Optional[List[torch.FloatTensor]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        labels: Optional[torch.LongTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        images: Optional[torch.FloatTensor] = None,\n        image_sizes: Optional[List[List[int]]] = None,\n        return_dict: Optional[bool] = None,\n        modalities: Optional[List[str]] = [\"image\"],\n        dpo_forward: Optional[bool] = False,\n        cache_position=None,\n    ) -> Union[Tuple, CausalLMOutputWithPast]:\n\n        if inputs_embeds is None:\n            (input_ids, position_ids, attention_mask, past_key_values, inputs_embeds, labels) = self.prepare_inputs_labels_for_multimodal(input_ids, position_ids, attention_mask, past_key_values, labels, images, modalities, image_sizes)\n\n        if dpo_forward:\n            outputs = self.model(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                position_ids=position_ids,\n                past_key_values=past_key_values,\n                inputs_embeds=inputs_embeds,\n                use_cache=use_cache,\n                output_attentions=output_attentions,\n                output_hidden_states=output_hidden_states,\n                return_dict=return_dict,\n            )\n\n            hidden_states = outputs[0]\n            logits = self.lm_head(hidden_states)\n            return logits, labels\n\n        else:\n            return super().forward(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                position_ids=position_ids,\n                past_key_values=past_key_values,\n                inputs_embeds=inputs_embeds,\n                labels=labels,\n                use_cache=use_cache,\n                output_attentions=output_attentions,\n                output_hidden_states=output_hidden_states,\n                return_dict=return_dict,\n            )\n\n    @torch.no_grad()\n    def generate(\n        self,\n        inputs: Optional[torch.Tensor] = None,\n        images: Optional[torch.Tensor] = None,\n        image_sizes: Optional[torch.Tensor] = None,\n        modalities: Optional[List[str]] = [\"image\"],\n        **kwargs,\n    ) -> Union[GenerateOutput, torch.LongTensor]:\n        position_ids = kwargs.pop(\"position_ids\", None)\n        attention_mask = kwargs.pop(\"attention_mask\", None)\n        if \"inputs_embeds\" in kwargs:\n            raise NotImplementedError(\"`inputs_embeds` is not supported\")\n\n        if images is not None:\n            (inputs, position_ids, attention_mask, _, inputs_embeds, _) = self.prepare_inputs_labels_for_multimodal(inputs, position_ids, attention_mask, None, None, images, modalities, image_sizes=image_sizes)\n        else:\n            inputs_embeds = self.get_model().embed_tokens(inputs)\n\n        return super().generate(position_ids=position_ids, attention_mask=attention_mask, inputs_embeds=inputs_embeds, **kwargs)\n\n    def prepare_inputs_for_generation(self, input_ids, past_key_values=None, inputs_embeds=None, **kwargs):\n        images = kwargs.pop(\"images\", None)\n        image_sizes = kwargs.pop(\"image_sizes\", None)\n        inputs = super().prepare_inputs_for_generation(input_ids, past_key_values=past_key_values, inputs_embeds=inputs_embeds, **kwargs)\n        if images is not None:\n            inputs[\"images\"] = images\n        if image_sizes is not None:\n            inputs[\"image_sizes\"] = image_sizes\n        return inputs\n\n\nAutoConfig.register(\"llava_qwen_moe\", LlavaQwenMoeConfig)\nAutoModelForCausalLM.register(LlavaQwenMoeConfig, LlavaQwenMoeForCausalLM)\n"}
{"type": "source_file", "path": "llava/eval/cal_performance_mmreweard_bench.py", "content": "import json\nfrom collections import defaultdict\nimport argparse\n\nparser = argparse.ArgumentParser(description='Calculate metrics from JSONL data')\nparser.add_argument('input_file', type=str, help='Path to input JSONL file')\nargs = parser.parse_args()\n\ninput_file = args.input_file\n\n# 类别关键词\ncategory_keywords = [\"mcq\", \"long\", \"short\", \"safety\", \"video\"]\n\n# 初始化统计\ncategory_stats = {keyword: {\"accuracy\": 0, \"acc_plus\": 0, \"total\": 0} for keyword in category_keywords}\noverall_stats = {\"accuracy\": 0, \"acc_plus\": 0, \"total\": 0}\n\n# 用于存储每个id的items\nid_to_items = defaultdict(list)\n\n# 读取数据并分类\nwith open(input_file, \"r\") as infile:\n    for line in infile:\n        item = json.loads(line.strip())\n        image_path = item.get(\"image\", \"\") or item.get(\"video\", \"\")\n        item_id = item.get(\"id\", \"\")\n        id_to_items[item_id].append(item)\n\n        # 分类到相应类别\n        for keyword in category_keywords:\n            if keyword in image_path:\n                category_stats[keyword][\"total\"] += 1\n                break\n\n        # 更新总计\n        overall_stats[\"total\"] += 1\n\n# 计算accuracy和acc_plus\nfor item_id, items in id_to_items.items():\n    # 统计单个id是否满足acc+\n    all_correct = True\n    for item in items:\n        reward_0 = item[\"rewards\"][0]\n        reward_1 = item[\"rewards\"][1]\n        correct = reward_0 > reward_1\n\n        # 分类统计accuracy\n        for keyword in category_keywords:\n            if keyword in item.get(\"image\", \"\") or keyword in item.get(\"video\", \"\"):\n                if correct:\n                    category_stats[keyword][\"accuracy\"] += 1\n                else:\n                    all_correct = False\n                break\n\n        # 总体统计accuracy\n        if correct:\n            overall_stats[\"accuracy\"] += 1\n        else:\n            all_correct = False\n\n    # 更新acc+统计\n    if all_correct:\n        for keyword in category_keywords:\n            if any(keyword in item.get(\"image\", \"\") or keyword in item.get(\"video\", \"\") for item in items):\n                category_stats[keyword][\"acc_plus\"] += 1\n                break\n        overall_stats[\"acc_plus\"] += 1\n\n# 计算每个类别的accuracy和acc+\nfor keyword, stats in category_stats.items():\n    if stats[\"total\"] > 0:\n        stats[\"accuracy\"] = stats[\"accuracy\"] / stats[\"total\"]\n        stats[\"acc_plus\"] = stats[\"acc_plus\"] / len(\n            [item_id for item_id in id_to_items if any(keyword in (item.get(\"image\", \"\") + item.get(\"video\", \"\")) for item in id_to_items[item_id])]\n        )\n\n# 计算总体accuracy和acc+\nif overall_stats[\"total\"] > 0:\n    overall_stats[\"accuracy\"] = overall_stats[\"accuracy\"] / overall_stats[\"total\"]\n    overall_stats[\"acc_plus\"] = overall_stats[\"acc_plus\"] / len(id_to_items)\n\n# 输出结果\ndef print_metrics():\n    print(\"\\nCategory-wise Metrics:\")\n    for keyword, stats in category_stats.items():\n        print(f\"Category: {keyword}\")\n        print(f\"  Accuracy: {stats['accuracy']:.2f}\")\n        print(f\"  ACC+: {stats['acc_plus']:.2f}\")\n        print(f\"  Total: {stats['total']}\")\n\n    print(\"\\nOverall Metrics:\")\n    print(f\"Overall Accuracy: {overall_stats['accuracy']:.2f}\")\n    print(f\"Overall ACC+: {overall_stats['acc_plus']:.2f}\")\n    print(f\"Total Items: {overall_stats['total']}\")\n\n# 输出\nprint_metrics()\n"}
{"type": "source_file", "path": "llava/model/language_model/modeling_llama.py", "content": "# coding=utf-8\n# Copyright 2022 EleutherAI and the HuggingFace Inc. team. All rights reserved.\n#\n# This code is based on EleutherAI's GPT-NeoX library and the GPT-NeoX\n# and OPT implementations in this library. It has been modified from its\n# original forms to accommodate minor architectural differences compared\n# to GPT-NeoX and OPT used by the Meta AI team that trained the model.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\" PyTorch LLaMA model.\"\"\"\nimport math\nimport warnings\nfrom typing import List, Optional, Tuple, Union\n\nimport torch\nimport torch.nn.functional as F\nimport torch.utils.checkpoint\nfrom torch import nn\nfrom torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n\nfrom transformers.activations import ACT2FN\nfrom transformers.cache_utils import Cache, DynamicCache, StaticCache\nfrom transformers.modeling_outputs import (\n    BaseModelOutputWithPast,\n    CausalLMOutputWithPast,\n    QuestionAnsweringModelOutput,\n    SequenceClassifierOutputWithPast,\n)\nfrom transformers.modeling_utils import PreTrainedModel\nfrom transformers.pytorch_utils import ALL_LAYERNORM_LAYERS\nfrom transformers.utils import (\n    add_start_docstrings,\n    add_start_docstrings_to_model_forward,\n    is_flash_attn_2_available,\n    is_flash_attn_greater_or_equal_2_10,\n    logging,\n    replace_return_docstrings,\n)\nfrom transformers.models.llama.configuration_llama import LlamaConfig\n\nif is_flash_attn_2_available():\n    from flash_attn import flash_attn_func, flash_attn_varlen_func\n    from flash_attn.bert_padding import index_first_axis, pad_input, unpad_input  # noqa\n\n\nlogger = logging.get_logger(__name__)\n\n_CONFIG_FOR_DOC = \"LlamaConfig\"\n\n\ndef _get_unpad_data(attention_mask):\n    seqlens_in_batch = attention_mask.sum(dim=-1, dtype=torch.int32)\n    indices = torch.nonzero(attention_mask.flatten(), as_tuple=False).flatten()\n    max_seqlen_in_batch = seqlens_in_batch.max().item()\n    cu_seqlens = F.pad(torch.cumsum(seqlens_in_batch, dim=0, dtype=torch.int32), (1, 0))\n    return (\n        indices,\n        cu_seqlens,\n        max_seqlen_in_batch,\n    )\n\n\nclass LlamaRMSNorm(nn.Module):\n    def __init__(self, hidden_size, eps=1e-6):\n        \"\"\"\n        LlamaRMSNorm is equivalent to T5LayerNorm\n        \"\"\"\n        super().__init__()\n        self.weight = nn.Parameter(torch.ones(hidden_size))\n        self.variance_epsilon = eps\n\n    def forward(self, hidden_states):\n        input_dtype = hidden_states.dtype\n        hidden_states = hidden_states.to(torch.float32)\n        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n        return self.weight * hidden_states.to(input_dtype)\n\n\nALL_LAYERNORM_LAYERS.append(LlamaRMSNorm)\n\n\nclass LlamaRotaryEmbedding(nn.Module):\n    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None, scaling_factor=1.0):\n        super().__init__()\n        self.scaling_factor = scaling_factor\n        self.dim = dim\n        self.max_position_embeddings = max_position_embeddings\n        self.base = base\n        inv_freq = 1.0 / (self.base ** (torch.arange(0, self.dim, 2, dtype=torch.int64).float().to(device) / self.dim))\n        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n        # For BC we register cos and sin cached\n        self.max_seq_len_cached = max_position_embeddings\n        t = torch.arange(self.max_seq_len_cached, device=device, dtype=torch.int64).type_as(self.inv_freq)\n        t = t / self.scaling_factor\n        freqs = torch.outer(t, self.inv_freq)\n        # Different from paper, but it uses a different permutation in order to obtain the same calculation\n        emb = torch.cat((freqs, freqs), dim=-1)\n        self.register_buffer(\"_cos_cached\", emb.cos().to(torch.get_default_dtype()), persistent=False)\n        self.register_buffer(\"_sin_cached\", emb.sin().to(torch.get_default_dtype()), persistent=False)\n\n    @property\n    def sin_cached(self):\n        logger.warning_once(\"The sin_cached attribute will be removed in 4.39. Bear in mind that its contents changed in v4.38. Use \" \"the forward method of RoPE from now on instead. It is not used in the `LlamaAttention` class\")\n        return self._sin_cached\n\n    @property\n    def cos_cached(self):\n        logger.warning_once(\"The cos_cached attribute will be removed in 4.39. Bear in mind that its contents changed in v4.38. Use \" \"the forward method of RoPE from now on instead. It is not used in the `LlamaAttention` class\")\n        return self._cos_cached\n\n    @torch.no_grad()\n    def forward(self, x, position_ids, seq_len=None):\n        if seq_len is not None:\n            logger.warning_once(\"The `seq_len` argument is deprecated and unused. It will be removed in v4.39.\")\n\n        # x: [bs, num_attention_heads, seq_len, head_size]\n        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n        position_ids_expanded = position_ids[:, None, :].float()\n        # Force float32 since bfloat16 loses precision on long contexts\n        # See https://github.com/huggingface/transformers/pull/29285\n        device_type = x.device.type\n        device_type = device_type if isinstance(device_type, str) else \"cpu\"\n        with torch.autocast(device_type=device_type, enabled=False):\n            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n            emb = torch.cat((freqs, freqs), dim=-1)\n            cos = emb.cos()\n            sin = emb.sin()\n        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n\n\nclass LlamaLinearScalingRotaryEmbedding(LlamaRotaryEmbedding):\n    \"\"\"LlamaRotaryEmbedding extended with linear scaling. Credits to the Reddit user /u/kaiokendev\"\"\"\n\n    def forward(self, x, position_ids, seq_len=None):\n        # difference to the original RoPE: a scaling factor is aplied to the position ids\n        position_ids = position_ids.float() / self.scaling_factor\n        cos, sin = super().forward(x, position_ids, seq_len)\n        return cos, sin\n\n\nclass LlamaDynamicNTKScalingRotaryEmbedding(LlamaRotaryEmbedding):\n    \"\"\"LlamaRotaryEmbedding extended with Dynamic NTK scaling. Credits to the Reddit users /u/bloc97 and /u/emozilla\"\"\"\n\n    def forward(self, x, position_ids, seq_len=None):\n        # difference to the original RoPE: inv_freq is recomputed when the sequence length > original length\n        seq_len = torch.max(position_ids) + 1\n        if seq_len > self.max_position_embeddings:\n            base = self.base * ((self.scaling_factor * seq_len / self.max_position_embeddings) - (self.scaling_factor - 1)) ** (self.dim / (self.dim - 2))\n            inv_freq = 1.0 / (base ** (torch.arange(0, self.dim, 2, dtype=torch.int64).float().to(x.device) / self.dim))\n            self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: this may break with compilation\n\n        cos, sin = super().forward(x, position_ids, seq_len)\n        return cos, sin\n\n\ndef rotate_half(x):\n    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n    x1 = x[..., : x.shape[-1] // 2]\n    x2 = x[..., x.shape[-1] // 2 :]\n    return torch.cat((-x2, x1), dim=-1)\n\n\ndef apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n\n    Args:\n        q (`torch.Tensor`): The query tensor.\n        k (`torch.Tensor`): The key tensor.\n        cos (`torch.Tensor`): The cosine part of the rotary embedding.\n        sin (`torch.Tensor`): The sine part of the rotary embedding.\n        position_ids (`torch.Tensor`, *optional*):\n            Deprecated and unused.\n        unsqueeze_dim (`int`, *optional*, defaults to 1):\n            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n    Returns:\n        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n    \"\"\"\n    cos = cos.unsqueeze(unsqueeze_dim)\n    sin = sin.unsqueeze(unsqueeze_dim)\n    q_embed = (q * cos) + (rotate_half(q) * sin)\n    k_embed = (k * cos) + (rotate_half(k) * sin)\n    return q_embed, k_embed\n\n\nclass LlamaMLP(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.hidden_size = config.hidden_size\n        self.intermediate_size = config.intermediate_size\n        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)\n        self.act_fn = ACT2FN[config.hidden_act]\n\n    def forward(self, x):\n        if self.config.pretraining_tp > 1:\n            slice = self.intermediate_size // self.config.pretraining_tp\n            gate_proj_slices = self.gate_proj.weight.split(slice, dim=0)\n            up_proj_slices = self.up_proj.weight.split(slice, dim=0)\n            down_proj_slices = self.down_proj.weight.split(slice, dim=1)\n\n            gate_proj = torch.cat([F.linear(x, gate_proj_slices[i]) for i in range(self.config.pretraining_tp)], dim=-1)\n            up_proj = torch.cat([F.linear(x, up_proj_slices[i]) for i in range(self.config.pretraining_tp)], dim=-1)\n\n            intermediate_states = (self.act_fn(gate_proj) * up_proj).split(slice, dim=2)\n            down_proj = [F.linear(intermediate_states[i], down_proj_slices[i]) for i in range(self.config.pretraining_tp)]\n            down_proj = sum(down_proj)\n        else:\n            down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n\n        return down_proj\n\n\ndef repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n    \"\"\"\n    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n    \"\"\"\n    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n    if n_rep == 1:\n        return hidden_states\n    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n\n\nclass LlamaAttention(nn.Module):\n    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n\n    def __init__(self, config: LlamaConfig, layer_idx: Optional[int] = None):\n        super().__init__()\n        self.config = config\n        self.layer_idx = layer_idx\n        if layer_idx is None:\n            logger.warning_once(\n                f\"Instantiating {self.__class__.__name__} without passing a `layer_idx` is not recommended and will \"\n                \"lead to errors during the forward call if caching is used. Please make sure to provide a `layer_idx` \"\n                \"when creating this class.\"\n            )\n\n        self.attention_dropout = config.attention_dropout\n        self.hidden_size = config.hidden_size\n        self.num_heads = config.num_attention_heads\n        self.head_dim = self.hidden_size // self.num_heads\n        self.num_key_value_heads = config.num_key_value_heads\n        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n        self.max_position_embeddings = config.max_position_embeddings\n        self.rope_theta = config.rope_theta\n        self.is_causal = True\n\n        if (self.head_dim * self.num_heads) != self.hidden_size:\n            raise ValueError(f\"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}\" f\" and `num_heads`: {self.num_heads}).\")\n\n        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=config.attention_bias)\n        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n        self.o_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=config.attention_bias)\n        self._init_rope()\n\n    def _init_rope(self):\n        if self.config.rope_scaling is None:\n            self.rotary_emb = LlamaRotaryEmbedding(\n                self.head_dim,\n                max_position_embeddings=self.max_position_embeddings,\n                base=self.rope_theta,\n            )\n        else:\n            scaling_type = self.config.rope_scaling[\"type\"]\n            scaling_factor = self.config.rope_scaling[\"factor\"]\n            if scaling_type == \"linear\":\n                self.rotary_emb = LlamaLinearScalingRotaryEmbedding(\n                    self.head_dim,\n                    max_position_embeddings=self.max_position_embeddings,\n                    scaling_factor=scaling_factor,\n                    base=self.rope_theta,\n                )\n            elif scaling_type == \"dynamic\":\n                self.rotary_emb = LlamaDynamicNTKScalingRotaryEmbedding(\n                    self.head_dim,\n                    max_position_embeddings=self.max_position_embeddings,\n                    scaling_factor=scaling_factor,\n                    base=self.rope_theta,\n                )\n            else:\n                raise ValueError(f\"Unknown RoPE scaling type {scaling_type}\")\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_value: Optional[Cache] = None,\n        output_attentions: bool = False,\n        use_cache: bool = False,\n        cache_position: Optional[torch.LongTensor] = None,\n        **kwargs,\n    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n        bsz, q_len, _ = hidden_states.size()\n\n        if self.config.pretraining_tp > 1:\n            key_value_slicing = (self.num_key_value_heads * self.head_dim) // self.config.pretraining_tp\n            query_slices = self.q_proj.weight.split((self.num_heads * self.head_dim) // self.config.pretraining_tp, dim=0)\n            key_slices = self.k_proj.weight.split(key_value_slicing, dim=0)\n            value_slices = self.v_proj.weight.split(key_value_slicing, dim=0)\n\n            query_states = [F.linear(hidden_states, query_slices[i]) for i in range(self.config.pretraining_tp)]\n            query_states = torch.cat(query_states, dim=-1)\n\n            key_states = [F.linear(hidden_states, key_slices[i]) for i in range(self.config.pretraining_tp)]\n            key_states = torch.cat(key_states, dim=-1)\n\n            value_states = [F.linear(hidden_states, value_slices[i]) for i in range(self.config.pretraining_tp)]\n            value_states = torch.cat(value_states, dim=-1)\n\n        else:\n            query_states = self.q_proj(hidden_states)\n            key_states = self.k_proj(hidden_states)\n            value_states = self.v_proj(hidden_states)\n\n        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n\n        past_key_value = getattr(self, \"past_key_value\", past_key_value)\n        cos, sin = self.rotary_emb(value_states, position_ids)\n        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n\n        if past_key_value is not None:\n            # sin and cos are specific to RoPE models; position_ids needed for the static cache\n            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n\n        key_states = repeat_kv(key_states, self.num_key_value_groups)\n        value_states = repeat_kv(value_states, self.num_key_value_groups)\n\n        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n\n        if attention_mask is not None:  # no matter the length, we just slice it\n            causal_mask = attention_mask\n            if cache_position is not None:\n                causal_mask = attention_mask[:, :, cache_position, : key_states.shape[-2]]\n            attn_weights = attn_weights + causal_mask\n\n        # upcast attention to fp32\n        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n        attn_weights = nn.functional.dropout(attn_weights, p=self.attention_dropout, training=self.training)\n        attn_output = torch.matmul(attn_weights, value_states)\n\n        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n            raise ValueError(f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\" f\" {attn_output.size()}\")\n\n        attn_output = attn_output.transpose(1, 2).contiguous()\n\n        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n\n        if self.config.pretraining_tp > 1:\n            attn_output = attn_output.split(self.hidden_size // self.config.pretraining_tp, dim=2)\n            o_proj_slices = self.o_proj.weight.split(self.hidden_size // self.config.pretraining_tp, dim=1)\n            attn_output = sum([F.linear(attn_output[i], o_proj_slices[i]) for i in range(self.config.pretraining_tp)])\n        else:\n            attn_output = self.o_proj(attn_output)\n\n        if not output_attentions:\n            attn_weights = None\n\n        return attn_output, attn_weights, past_key_value\n\n\nclass LlamaRingFlashAttention2(LlamaAttention):\n    \"\"\"\n    Llama flash attention module. This module inherits from `LlamaAttention` as the weights of the module stays\n    untouched. The only required change would be on the forward pass where it needs to correctly call the public API of\n    flash attention and deal with padding tokens in case the input contains any of them.\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n        # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignement, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n        # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n        self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.LongTensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_value: Optional[Cache] = None,\n        output_attentions: bool = False,\n        use_cache: bool = False,\n        cache_position: Optional[torch.LongTensor] = None,\n        **kwargs,\n    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n        output_attentions = False\n\n        bsz, q_len, _ = hidden_states.size()\n\n        query_states = self.q_proj(hidden_states)\n        key_states = self.k_proj(hidden_states)\n        value_states = self.v_proj(hidden_states)\n\n        # Flash attention requires the input to have the shape\n        # batch_size x seq_length x head_dim x hidden_dim\n        # therefore we just need to keep the original shape\n        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n\n        cos, sin = self.rotary_emb(value_states, position_ids)\n        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n\n        past_key_value = getattr(self, \"past_key_value\", past_key_value)\n\n        if past_key_value is not None:\n            # sin and cos are specific to RoPE models; position_ids needed for the static cache\n            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n\n        # TODO: These transpose are quite inefficient but Flash Attention requires the layout [batch_size, sequence_length, num_heads, head_dim]. We would need to refactor the KV cache\n        # to be able to avoid many of these transpose/reshape/view.\n        query_states = query_states.transpose(1, 2)\n        key_states = key_states.transpose(1, 2)\n        value_states = value_states.transpose(1, 2)\n\n        dropout_rate = self.attention_dropout if self.training else 0.0\n\n        # In PEFT, usually we cast the layer norms in float32 for training stability reasons\n        # therefore the input hidden states gets silently casted in float32. Hence, we need\n        # cast them back in the correct dtype just to be sure everything works as expected.\n        # This might slowdown training & inference so it is recommended to not cast the LayerNorms\n        # in fp32. (LlamaRMSNorm handles it correctly)\n\n        input_dtype = query_states.dtype\n        if input_dtype == torch.float32:\n            if torch.is_autocast_enabled():\n                target_dtype = torch.get_autocast_gpu_dtype()\n            # Handle the case where the model is quantized\n            elif hasattr(self.config, \"_pre_quantization_dtype\"):\n                target_dtype = self.config._pre_quantization_dtype\n            else:\n                target_dtype = self.q_proj.weight.dtype\n\n            logger.warning_once(\n                f\"The input hidden states seems to be silently casted in float32, this might be related to\" f\" the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in\" f\" {target_dtype}.\"\n            )\n\n            query_states = query_states.to(target_dtype)\n            key_states = key_states.to(target_dtype)\n            value_states = value_states.to(target_dtype)\n\n        attn_output = self._flash_attention_forward(query_states, key_states, value_states, attention_mask, q_len, dropout=dropout_rate)\n\n        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size).contiguous()\n        attn_output = self.o_proj(attn_output)\n\n        if not output_attentions:\n            attn_weights = None\n\n        return attn_output, attn_weights, past_key_value\n\n    def _flash_attention_forward(self, query_states, key_states, value_states, attention_mask, query_length, dropout=0.0, softmax_scale=None):\n        \"\"\"\n        Calls the forward method of Flash Attention - if the input hidden states contain at least one padding token\n        first unpad the input, then computes the attention scores and pad the final attention scores.\n\n        Args:\n            query_states (`torch.Tensor`):\n                Input query states to be passed to Flash Attention API\n            key_states (`torch.Tensor`):\n                Input key states to be passed to Flash Attention API\n            value_states (`torch.Tensor`):\n                Input value states to be passed to Flash Attention API\n            attention_mask (`torch.Tensor`):\n                The padding mask - corresponds to a tensor of size `(batch_size, seq_len)` where 0 stands for the\n                position of padding tokens and 1 for the position of non-padding tokens.\n            dropout (`int`, *optional*):\n                Attention dropout\n            softmax_scale (`float`, *optional*):\n                The scaling of QK^T before applying softmax. Default to 1 / sqrt(head_dim)\n        \"\"\"\n        if not self._flash_attn_uses_top_left_mask:\n            causal = self.is_causal\n        else:\n            # TODO: Remove the `query_length != 1` check once Flash Attention for RoCm is bumped to 2.1. For details, please see the comment in LlamaFlashAttention2 __init__.\n            causal = self.is_causal and query_length != 1\n\n        # Contains at least one padding token in the sequence\n        if attention_mask is not None:\n            batch_size = query_states.shape[0]\n            query_states, key_states, value_states, indices_q, cu_seq_lens, max_seq_lens = self._upad_input(query_states, key_states, value_states, attention_mask, query_length)\n\n            cu_seqlens_q, cu_seqlens_k = cu_seq_lens\n            max_seqlen_in_batch_q, max_seqlen_in_batch_k = max_seq_lens\n\n            attn_output_unpad = zigzag_ring_flash_attn_varlen_func(\n                query_states,\n                key_states,\n                value_states,\n                cu_seqlens_q=cu_seqlens_q,\n                cu_seqlens_k=cu_seqlens_k,\n                max_seqlen_q=max_seqlen_in_batch_q,\n                max_seqlen_k=max_seqlen_in_batch_k,\n                dropout_p=dropout,\n                softmax_scale=softmax_scale,\n                causal=causal,\n            )\n\n            attn_output = pad_input(attn_output_unpad, indices_q, batch_size, query_length)\n        else:\n            # pack qkv\n            # query_states: (batch_size, seqlen, nheads, headdim)\n            # qkv: (batch_size, seqlen, 3, nheads, headdim)\n            qkv = torch.stack([query_states, key_states, value_states], dim=2)\n            attn_output = zigzag_ring_flash_attn_qkvpacked_func(qkv, dropout, softmax_scale, causal=causal)\n\n        return attn_output\n\n    def _upad_input(self, query_layer, key_layer, value_layer, attention_mask, query_length):\n        indices_k, cu_seqlens_k, max_seqlen_in_batch_k = _get_unpad_data(attention_mask)\n        batch_size, kv_seq_len, num_key_value_heads, head_dim = key_layer.shape\n\n        key_layer = index_first_axis(key_layer.reshape(batch_size * kv_seq_len, num_key_value_heads, head_dim), indices_k)\n        value_layer = index_first_axis(value_layer.reshape(batch_size * kv_seq_len, num_key_value_heads, head_dim), indices_k)\n        if query_length == kv_seq_len:\n            query_layer = index_first_axis(query_layer.reshape(batch_size * kv_seq_len, self.num_heads, head_dim), indices_k)\n            cu_seqlens_q = cu_seqlens_k\n            max_seqlen_in_batch_q = max_seqlen_in_batch_k\n            indices_q = indices_k\n        elif query_length == 1:\n            max_seqlen_in_batch_q = 1\n            cu_seqlens_q = torch.arange(batch_size + 1, dtype=torch.int32, device=query_layer.device)  # There is a memcpy here, that is very bad.\n            indices_q = cu_seqlens_q[:-1]\n            query_layer = query_layer.squeeze(1)\n        else:\n            # The -q_len: slice assumes left padding.\n            attention_mask = attention_mask[:, -query_length:]\n            query_layer, indices_q, cu_seqlens_q, max_seqlen_in_batch_q = unpad_input(query_layer, attention_mask)\n\n        return (\n            query_layer,\n            key_layer,\n            value_layer,\n            indices_q,\n            (cu_seqlens_q, cu_seqlens_k),\n            (max_seqlen_in_batch_q, max_seqlen_in_batch_k),\n        )\n\n\nclass LlamaFlashAttention2(LlamaAttention):\n    \"\"\"\n    Llama flash attention module. This module inherits from `LlamaAttention` as the weights of the module stays\n    untouched. The only required change would be on the forward pass where it needs to correctly call the public API of\n    flash attention and deal with padding tokens in case the input contains any of them.\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n        # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignement, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n        # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n        self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.LongTensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_value: Optional[Cache] = None,\n        output_attentions: bool = False,\n        use_cache: bool = False,\n        cache_position: Optional[torch.LongTensor] = None,\n        **kwargs,\n    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n        output_attentions = False\n\n        bsz, q_len, _ = hidden_states.size()\n\n        query_states = self.q_proj(hidden_states)\n        key_states = self.k_proj(hidden_states)\n        value_states = self.v_proj(hidden_states)\n\n        # Flash attention requires the input to have the shape\n        # batch_size x seq_length x head_dim x hidden_dim\n        # therefore we just need to keep the original shape\n        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n\n        cos, sin = self.rotary_emb(value_states, position_ids)\n        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n\n        past_key_value = getattr(self, \"past_key_value\", past_key_value)\n\n        if past_key_value is not None:\n            # sin and cos are specific to RoPE models; position_ids needed for the static cache\n            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n\n        # TODO: These transpose are quite inefficient but Flash Attention requires the layout [batch_size, sequence_length, num_heads, head_dim]. We would need to refactor the KV cache\n        # to be able to avoid many of these transpose/reshape/view.\n        query_states = query_states.transpose(1, 2)\n        key_states = key_states.transpose(1, 2)\n        value_states = value_states.transpose(1, 2)\n\n        dropout_rate = self.attention_dropout if self.training else 0.0\n\n        # In PEFT, usually we cast the layer norms in float32 for training stability reasons\n        # therefore the input hidden states gets silently casted in float32. Hence, we need\n        # cast them back in the correct dtype just to be sure everything works as expected.\n        # This might slowdown training & inference so it is recommended to not cast the LayerNorms\n        # in fp32. (LlamaRMSNorm handles it correctly)\n\n        input_dtype = query_states.dtype\n        if input_dtype == torch.float32:\n            if torch.is_autocast_enabled():\n                target_dtype = torch.get_autocast_gpu_dtype()\n            # Handle the case where the model is quantized\n            elif hasattr(self.config, \"_pre_quantization_dtype\"):\n                target_dtype = self.config._pre_quantization_dtype\n            else:\n                target_dtype = self.q_proj.weight.dtype\n\n            logger.warning_once(\n                f\"The input hidden states seems to be silently casted in float32, this might be related to\" f\" the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in\" f\" {target_dtype}.\"\n            )\n\n            query_states = query_states.to(target_dtype)\n            key_states = key_states.to(target_dtype)\n            value_states = value_states.to(target_dtype)\n\n        attn_output = self._flash_attention_forward(query_states, key_states, value_states, attention_mask, q_len, dropout=dropout_rate)\n\n        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size).contiguous()\n        attn_output = self.o_proj(attn_output)\n\n        if not output_attentions:\n            attn_weights = None\n\n        return attn_output, attn_weights, past_key_value\n\n    def _flash_attention_forward(self, query_states, key_states, value_states, attention_mask, query_length, dropout=0.0, softmax_scale=None):\n        \"\"\"\n        Calls the forward method of Flash Attention - if the input hidden states contain at least one padding token\n        first unpad the input, then computes the attention scores and pad the final attention scores.\n\n        Args:\n            query_states (`torch.Tensor`):\n                Input query states to be passed to Flash Attention API\n            key_states (`torch.Tensor`):\n                Input key states to be passed to Flash Attention API\n            value_states (`torch.Tensor`):\n                Input value states to be passed to Flash Attention API\n            attention_mask (`torch.Tensor`):\n                The padding mask - corresponds to a tensor of size `(batch_size, seq_len)` where 0 stands for the\n                position of padding tokens and 1 for the position of non-padding tokens.\n            dropout (`int`, *optional*):\n                Attention dropout\n            softmax_scale (`float`, *optional*):\n                The scaling of QK^T before applying softmax. Default to 1 / sqrt(head_dim)\n        \"\"\"\n        if not self._flash_attn_uses_top_left_mask:\n            causal = self.is_causal\n        else:\n            # TODO: Remove the `query_length != 1` check once Flash Attention for RoCm is bumped to 2.1. For details, please see the comment in LlamaFlashAttention2 __init__.\n            causal = self.is_causal and query_length != 1\n\n        # Contains at least one padding token in the sequence\n        if attention_mask is not None:\n            batch_size = query_states.shape[0]\n            query_states, key_states, value_states, indices_q, cu_seq_lens, max_seq_lens = self._upad_input(query_states, key_states, value_states, attention_mask, query_length)\n\n            cu_seqlens_q, cu_seqlens_k = cu_seq_lens\n            max_seqlen_in_batch_q, max_seqlen_in_batch_k = max_seq_lens\n\n            attn_output_unpad = flash_attn_varlen_func(\n                query_states,\n                key_states,\n                value_states,\n                cu_seqlens_q=cu_seqlens_q,\n                cu_seqlens_k=cu_seqlens_k,\n                max_seqlen_q=max_seqlen_in_batch_q,\n                max_seqlen_k=max_seqlen_in_batch_k,\n                dropout_p=dropout,\n                softmax_scale=softmax_scale,\n                causal=causal,\n            )\n\n            attn_output = pad_input(attn_output_unpad, indices_q, batch_size, query_length)\n        else:\n            attn_output = flash_attn_func(query_states, key_states, value_states, dropout, softmax_scale=softmax_scale, causal=causal)\n\n        return attn_output\n\n    def _upad_input(self, query_layer, key_layer, value_layer, attention_mask, query_length):\n        indices_k, cu_seqlens_k, max_seqlen_in_batch_k = _get_unpad_data(attention_mask)\n        batch_size, kv_seq_len, num_key_value_heads, head_dim = key_layer.shape\n\n        key_layer = index_first_axis(key_layer.reshape(batch_size * kv_seq_len, num_key_value_heads, head_dim), indices_k)\n        value_layer = index_first_axis(value_layer.reshape(batch_size * kv_seq_len, num_key_value_heads, head_dim), indices_k)\n        if query_length == kv_seq_len:\n            query_layer = index_first_axis(query_layer.reshape(batch_size * kv_seq_len, self.num_heads, head_dim), indices_k)\n            cu_seqlens_q = cu_seqlens_k\n            max_seqlen_in_batch_q = max_seqlen_in_batch_k\n            indices_q = indices_k\n        elif query_length == 1:\n            max_seqlen_in_batch_q = 1\n            cu_seqlens_q = torch.arange(batch_size + 1, dtype=torch.int32, device=query_layer.device)  # There is a memcpy here, that is very bad.\n            indices_q = cu_seqlens_q[:-1]\n            query_layer = query_layer.squeeze(1)\n        else:\n            # The -q_len: slice assumes left padding.\n            attention_mask = attention_mask[:, -query_length:]\n            query_layer, indices_q, cu_seqlens_q, max_seqlen_in_batch_q = unpad_input(query_layer, attention_mask)\n\n        return (\n            query_layer,\n            key_layer,\n            value_layer,\n            indices_q,\n            (cu_seqlens_q, cu_seqlens_k),\n            (max_seqlen_in_batch_q, max_seqlen_in_batch_k),\n        )\n\n\nclass LlamaSdpaAttention(LlamaAttention):\n    \"\"\"\n    Llama attention module using torch.nn.functional.scaled_dot_product_attention. This module inherits from\n    `LlamaAttention` as the weights of the module stays untouched. The only changes are on the forward pass to adapt to\n    SDPA API.\n    \"\"\"\n\n    # Adapted from LlamaAttention.forward\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_value: Optional[Cache] = None,\n        output_attentions: bool = False,\n        use_cache: bool = False,\n        cache_position: Optional[torch.LongTensor] = None,\n    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n        if output_attentions:\n            # TODO: Improve this warning with e.g. `model.config.attn_implementation = \"manual\"` once this is implemented.\n            logger.warning_once(\n                \"LlamaModel is using LlamaSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, \"\n                'but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n            )\n            return super().forward(\n                hidden_states=hidden_states,\n                attention_mask=attention_mask,\n                position_ids=position_ids,\n                past_key_value=past_key_value,\n                output_attentions=output_attentions,\n                use_cache=use_cache,\n                cache_position=cache_position,\n            )\n\n        bsz, q_len, _ = hidden_states.size()\n\n        query_states = self.q_proj(hidden_states)\n        key_states = self.k_proj(hidden_states)\n        value_states = self.v_proj(hidden_states)\n\n        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n\n        cos, sin = self.rotary_emb(value_states, position_ids)\n        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n\n        # In case static cache is used, it is an instance attribute.\n        past_key_value = getattr(self, \"past_key_value\", past_key_value)\n\n        if past_key_value is not None:\n            # sin and cos are specific to RoPE models; position_ids needed for the static cache\n            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n\n        key_states = repeat_kv(key_states, self.num_key_value_groups)\n        value_states = repeat_kv(value_states, self.num_key_value_groups)\n\n        causal_mask = attention_mask\n        if attention_mask is not None and cache_position is not None:\n            causal_mask = causal_mask[:, :, cache_position, : key_states.shape[-2]]\n\n        # SDPA with memory-efficient backend is currently (torch==2.1.2) bugged with non-contiguous inputs with custom attn_mask,\n        # Reference: https://github.com/pytorch/pytorch/issues/112577.\n        if query_states.device.type == \"cuda\" and causal_mask is not None:\n            query_states = query_states.contiguous()\n            key_states = key_states.contiguous()\n            value_states = value_states.contiguous()\n\n        attn_output = torch.nn.functional.scaled_dot_product_attention(\n            query_states,\n            key_states,\n            value_states,\n            attn_mask=causal_mask,\n            dropout_p=self.attention_dropout if self.training else 0.0,\n        )\n\n        attn_output = attn_output.transpose(1, 2).contiguous()\n        attn_output = attn_output.view(bsz, q_len, self.hidden_size)\n\n        attn_output = self.o_proj(attn_output)\n\n        return attn_output, None, past_key_value\n\n\ntry:\n    from ring_flash_attn import zigzag_ring_flash_attn_qkvpacked_func, zigzag_ring_flash_attn_varlen_func\nexcept ImportError:\n    print(\"Please install the ring-flash-attn package\")\n\nLLAMA_ATTENTION_CLASSES = {\n    \"eager\": LlamaAttention,\n    \"flash_attention_2\": LlamaFlashAttention2,\n    \"ring_flash_attention_2\": LlamaRingFlashAttention2,\n    \"sdpa\": LlamaSdpaAttention,\n}\n\n\nclass LlamaDecoderLayer(nn.Module):\n    def __init__(self, config: LlamaConfig, layer_idx: int):\n        super().__init__()\n        self.hidden_size = config.hidden_size\n\n        self.self_attn = LLAMA_ATTENTION_CLASSES[config._attn_implementation](config=config, layer_idx=layer_idx)\n\n        self.mlp = LlamaMLP(config)\n        self.input_layernorm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n        self.post_attention_layernorm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n        output_attentions: Optional[bool] = False,\n        use_cache: Optional[bool] = False,\n        cache_position: Optional[torch.LongTensor] = None,\n        **kwargs,\n    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n        \"\"\"\n        Args:\n            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n            attention_mask (`torch.FloatTensor`, *optional*):\n                attention mask of size `(batch_size, sequence_length)` if flash attention is used or `(batch_size, 1,\n                query_sequence_length, key_sequence_length)` if default attention is used.\n            output_attentions (`bool`, *optional*):\n                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                returned tensors for more detail.\n            use_cache (`bool`, *optional*):\n                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n                (see `past_key_values`).\n            past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n        \"\"\"\n        if \"padding_mask\" in kwargs:\n            warnings.warn(\"Passing `padding_mask` is deprecated and will be removed in v4.37. Please make sure use `attention_mask` instead.`\")\n\n        residual = hidden_states\n\n        hidden_states = self.input_layernorm(hidden_states)\n\n        # Self Attention\n        hidden_states, self_attn_weights, present_key_value = self.self_attn(\n            hidden_states=hidden_states,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            past_key_value=past_key_value,\n            output_attentions=output_attentions,\n            use_cache=use_cache,\n            cache_position=cache_position,\n            **kwargs,\n        )\n        hidden_states = residual + hidden_states\n\n        # Fully Connected\n        residual = hidden_states\n        hidden_states = self.post_attention_layernorm(hidden_states)\n        hidden_states = self.mlp(hidden_states)\n        hidden_states = residual + hidden_states\n\n        outputs = (hidden_states,)\n\n        if output_attentions:\n            outputs += (self_attn_weights,)\n\n        if use_cache:\n            outputs += (present_key_value,)\n\n        return outputs\n\n\nLLAMA_START_DOCSTRING = r\"\"\"\n    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n    etc.)\n\n    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n    and behavior.\n\n    Parameters:\n        config ([`LlamaConfig`]):\n            Model configuration class with all the parameters of the model. Initializing with a config file does not\n            load the weights associated with the model, only the configuration. Check out the\n            [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n\"\"\"\n\n\n@add_start_docstrings(\n    \"The bare LLaMA Model outputting raw hidden-states without any specific head on top.\",\n    LLAMA_START_DOCSTRING,\n)\nclass LlamaPreTrainedModel(PreTrainedModel):\n    config_class = LlamaConfig\n    base_model_prefix = \"model\"\n    supports_gradient_checkpointing = True\n    _no_split_modules = [\"LlamaDecoderLayer\"]\n    _skip_keys_device_placement = [\"past_key_values\", \"causal_mask\"]\n    _supports_flash_attn_2 = True\n    _supports_sdpa = True\n    _supports_cache_class = True\n\n    def _init_weights(self, module):\n        std = self.config.initializer_range\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=std)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=std)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n\n    def _setup_cache(self, cache_cls, max_batch_size, max_cache_len: Optional[int] = None):\n        if self.config._attn_implementation == \"flash_attention_2\" and cache_cls == StaticCache:\n            raise ValueError(\"`static` cache implementation is not compatible with `attn_implementation==flash_attention_2` \" \"make sure to use `sdpa` in the mean time, and open an issue at https://github.com/huggingface/transformers\")\n\n        if max_cache_len > self.model.causal_mask.shape[-1] or self.device != self.model.causal_mask.device:\n            causal_mask = torch.full((max_cache_len, max_cache_len), fill_value=True, device=self.device, dtype=torch.bool)\n            self.register_buffer(\"causal_mask\", torch.triu(causal_mask, diagonal=1), persistent=False)\n\n        for layer in self.model.layers:\n            device = layer.input_layernorm.weight.device\n            if hasattr(self.config, \"_pre_quantization_dtype\"):\n                dtype = self.config._pre_quantization_dtype\n            else:\n                dtype = layer.self_attn.o_proj.weight.dtype\n            layer.self_attn.past_key_value = cache_cls(self.config, max_batch_size, max_cache_len, device=device, dtype=dtype)\n\n    def _reset_cache(self):\n        for layer in self.model.layers:\n            layer.self_attn.past_key_value = None\n\n\nLLAMA_INPUTS_DOCSTRING = r\"\"\"\n    Args:\n        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n            it.\n\n            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n            [`PreTrainedTokenizer.__call__`] for details.\n\n            [What are input IDs?](../glossary#input-ids)\n        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n\n            - 1 for tokens that are **not masked**,\n            - 0 for tokens that are **masked**.\n\n            [What are attention masks?](../glossary#attention-mask)\n\n            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n            [`PreTrainedTokenizer.__call__`] for details.\n\n            If `past_key_values` is used, optionally only the last `input_ids` have to be input (see\n            `past_key_values`).\n\n            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]\n            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more\n            information on the default strategy.\n\n            - 1 indicates the head is **not masked**,\n            - 0 indicates the head is **masked**.\n        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n            config.n_positions - 1]`.\n\n            [What are position IDs?](../glossary#position-ids)\n        past_key_values (`Cache` or `tuple(tuple(torch.FloatTensor))`, *optional*):\n            Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n            blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`\n            returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.\n\n            Two formats are allowed:\n            - a [`~cache_utils.Cache`] instance;\n            - Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n            shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`). This is also known as the legacy\n            cache format.\n\n            The model will output the same cache format that is fed as input. If no `past_key_values` are passed, the\n            legacy cache format will be returned.\n\n            If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't\n            have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`\n            of shape `(batch_size, sequence_length)`.\n        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n            model's internal embedding lookup matrix.\n        use_cache (`bool`, *optional*):\n            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n            `past_key_values`).\n        output_attentions (`bool`, *optional*):\n            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n            tensors for more detail.\n        output_hidden_states (`bool`, *optional*):\n            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n            more detail.\n        return_dict (`bool`, *optional*):\n            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n\"\"\"\n\n\n@add_start_docstrings(\n    \"The bare LLaMA Model outputting raw hidden-states without any specific head on top.\",\n    LLAMA_START_DOCSTRING,\n)\nclass LlamaModel(LlamaPreTrainedModel):\n    \"\"\"\n    Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer is a [`LlamaDecoderLayer`]\n\n    Args:\n        config: LlamaConfig\n    \"\"\"\n\n    def __init__(self, config: LlamaConfig):\n        super().__init__(config)\n        self.padding_idx = config.pad_token_id\n        self.vocab_size = config.vocab_size\n\n        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)\n        self.layers = nn.ModuleList([LlamaDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)])\n        self.norm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n        self.gradient_checkpointing = False\n\n        # Register a causal mask to separate causal and padding mask creation. Merging happens in the attention class.\n        # NOTE: This is not friendly with TorchScript, ONNX, ExportedProgram serialization for very large `max_position_embeddings`.\n        causal_mask = torch.full((config.max_position_embeddings, config.max_position_embeddings), fill_value=True, dtype=torch.bool)\n        self.register_buffer(\"causal_mask\", torch.triu(causal_mask, diagonal=1), persistent=False)\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_input_embeddings(self):\n        return self.embed_tokens\n\n    def set_input_embeddings(self, value):\n        self.embed_tokens = value\n\n    @add_start_docstrings_to_model_forward(LLAMA_INPUTS_DOCSTRING)\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_values: Optional[List[torch.FloatTensor]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n        cache_position: Optional[torch.LongTensor] = None,\n    ) -> Union[Tuple, BaseModelOutputWithPast]:\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        use_cache = use_cache if use_cache is not None else self.config.use_cache\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        if (input_ids is None) ^ (inputs_embeds is not None):\n            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time, and must specify either one\")\n\n        if self.gradient_checkpointing and self.training and use_cache:\n            logger.warning_once(\"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\")\n            use_cache = False\n\n        if inputs_embeds is None:\n            inputs_embeds = self.embed_tokens(input_ids)\n\n        past_seen_tokens = 0\n        if use_cache:  # kept for BC (cache positions)\n            if not isinstance(past_key_values, StaticCache):\n                past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n                past_seen_tokens = past_key_values.get_seq_length()\n\n        if cache_position is None:\n            if isinstance(past_key_values, StaticCache):\n                raise ValueError(\"cache_position is a required argument when using StaticCache.\")\n            cache_position = torch.arange(past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device)\n\n        if position_ids is None:\n            position_ids = cache_position.unsqueeze(0)\n\n        causal_mask = self._update_causal_mask(attention_mask, inputs_embeds)\n\n        # embed positions\n        hidden_states = inputs_embeds\n\n        # decoder layers\n        all_hidden_states = () if output_hidden_states else None\n        all_self_attns = () if output_attentions else None\n        next_decoder_cache = None\n\n        for decoder_layer in self.layers:\n            if output_hidden_states:\n                all_hidden_states += (hidden_states,)\n\n            if self.gradient_checkpointing and self.training:\n                layer_outputs = self._gradient_checkpointing_func(\n                    decoder_layer.__call__,\n                    hidden_states,\n                    causal_mask,\n                    position_ids,\n                    past_key_values,\n                    output_attentions,\n                    use_cache,\n                    cache_position,\n                )\n            else:\n                layer_outputs = decoder_layer(\n                    hidden_states,\n                    attention_mask=causal_mask,\n                    position_ids=position_ids,\n                    past_key_value=past_key_values,\n                    output_attentions=output_attentions,\n                    use_cache=use_cache,\n                    cache_position=cache_position,\n                )\n\n            hidden_states = layer_outputs[0]\n\n            if use_cache:\n                next_decoder_cache = layer_outputs[2 if output_attentions else 1]\n\n            if output_attentions:\n                all_self_attns += (layer_outputs[1],)\n\n        hidden_states = self.norm(hidden_states)\n\n        # add hidden states from the last decoder layer\n        if output_hidden_states:\n            all_hidden_states += (hidden_states,)\n\n        next_cache = None\n        if use_cache:\n            next_cache = next_decoder_cache.to_legacy_cache() if isinstance(next_decoder_cache, Cache) else next_decoder_cache\n        if not return_dict:\n            return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)\n        return BaseModelOutputWithPast(\n            last_hidden_state=hidden_states,\n            past_key_values=next_cache,\n            hidden_states=all_hidden_states,\n            attentions=all_self_attns,\n        )\n\n    # TODO: As of torch==2.2.0, the `attention_mask` passed to the model in `generate` is 2D and of dynamic length even when the static\n    # KV cache is used. This is an issue for torch.compile which then recaptures cudagraphs at each decode steps due to the dynamic shapes.\n    # (`recording cudagraph tree for symint key 13`, etc.), which is VERY slow. A workaround is `@torch.compiler.disable`, but this prevents using\n    # `fullgraph=True`. See more context in https://github.com/huggingface/transformers/pull/29114\n    def _update_causal_mask(self, attention_mask, input_tensor):\n        if self.config._attn_implementation == \"flash_attention_2\":\n            if attention_mask is not None and 0.0 in attention_mask:\n                return attention_mask\n            return None\n\n        batch_size, seq_length = input_tensor.shape[:2]\n        dtype = input_tensor.dtype\n        device = input_tensor.device\n\n        # support going beyond cached `max_position_embedding`\n        if seq_length > self.causal_mask.shape[-1]:\n            causal_mask = torch.full((2 * self.causal_mask.shape[-1], 2 * self.causal_mask.shape[-1]), fill_value=1)\n            self.register_buffer(\"causal_mask\", torch.triu(causal_mask, diagonal=1), persistent=False)\n\n        # We use the current dtype to avoid any overflows\n        min_dtype = torch.finfo(dtype).min\n        causal_mask = self.causal_mask[None, None, :, :].repeat(batch_size, 1, 1, 1).to(dtype) * min_dtype\n\n        causal_mask = causal_mask.to(dtype=dtype, device=device)\n        if attention_mask is not None and attention_mask.dim() == 2:\n            mask_length = attention_mask.shape[-1]\n            padding_mask = causal_mask[..., :mask_length].eq(0.0) * attention_mask[:, None, None, :].eq(0.0)\n            causal_mask[..., :mask_length] = causal_mask[..., :mask_length].masked_fill(padding_mask, min_dtype)\n\n        if self.config._attn_implementation == \"sdpa\" and attention_mask is not None and attention_mask.device.type == \"cuda\":\n            # TODO: For dynamo, rather use a check on fullgraph=True once this is possible (https://github.com/pytorch/pytorch/pull/120400).\n            is_tracing = torch.jit.is_tracing() or isinstance(input_tensor, torch.fx.Proxy) or (hasattr(torch, \"_dynamo\") and torch._dynamo.is_compiling())\n            if not is_tracing and torch.any(attention_mask != 1):\n                # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n                # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n                # Details: https://github.com/pytorch/pytorch/issues/110213\n                causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)\n\n        return causal_mask\n\n\nclass LlamaForCausalLM(LlamaPreTrainedModel):\n    _tied_weights_keys = [\"lm_head.weight\"]\n\n    def __init__(self, config):\n        super().__init__(config)\n        self.model = LlamaModel(config)\n        self.vocab_size = config.vocab_size\n        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_input_embeddings(self):\n        return self.model.embed_tokens\n\n    def set_input_embeddings(self, value):\n        self.model.embed_tokens = value\n\n    def get_output_embeddings(self):\n        return self.lm_head\n\n    def set_output_embeddings(self, new_embeddings):\n        self.lm_head = new_embeddings\n\n    def set_decoder(self, decoder):\n        self.model = decoder\n\n    def get_decoder(self):\n        return self.model\n\n    @add_start_docstrings_to_model_forward(LLAMA_INPUTS_DOCSTRING)\n    @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_values: Optional[List[torch.FloatTensor]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        labels: Optional[torch.LongTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n        cache_position: Optional[torch.LongTensor] = None,\n    ) -> Union[Tuple, CausalLMOutputWithPast]:\n        r\"\"\"\n        Args:\n            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n\n        Returns:\n\n        Example:\n\n        ```python\n        >>> from transformers import AutoTokenizer, LlamaForCausalLM\n\n        >>> model = LlamaForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n        >>> tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n\n        >>> prompt = \"Hey, are you conscious? Can you talk to me?\"\n        >>> inputs = tokenizer(prompt, return_tensors=\"pt\")\n\n        >>> # Generate\n        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)\n        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n        \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\n        ```\"\"\"\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n        outputs = self.model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            past_key_values=past_key_values,\n            inputs_embeds=inputs_embeds,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n            cache_position=cache_position,\n        )\n\n        hidden_states = outputs[0]\n        if self.config.pretraining_tp > 1:\n            lm_head_slices = self.lm_head.weight.split(self.vocab_size // self.config.pretraining_tp, dim=0)\n            logits = [F.linear(hidden_states, lm_head_slices[i]) for i in range(self.config.pretraining_tp)]\n            logits = torch.cat(logits, dim=-1)\n        else:\n            logits = self.lm_head(hidden_states)\n        logits = logits.float()\n\n        loss = None\n        if labels is not None:\n            # Shift so that tokens < n predict n\n            shift_logits = logits[..., :-1, :].contiguous()\n            shift_labels = labels[..., 1:].contiguous()\n            # Flatten the tokens\n            loss_fct = CrossEntropyLoss()\n            shift_logits = shift_logits.view(-1, self.config.vocab_size)\n            shift_labels = shift_labels.view(-1)\n            # Enable model parallelism\n            shift_labels = shift_labels.to(shift_logits.device)\n            loss = loss_fct(shift_logits, shift_labels)\n\n        if not return_dict:\n            output = (logits,) + outputs[1:]\n            return (loss,) + output if loss is not None else output\n\n        return CausalLMOutputWithPast(\n            loss=loss,\n            logits=logits,\n            past_key_values=outputs.past_key_values,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )\n\n    def prepare_inputs_for_generation(self, input_ids, past_key_values=None, attention_mask=None, inputs_embeds=None, **kwargs):\n        past_length = 0\n        if past_key_values is not None:\n            if isinstance(past_key_values, Cache):\n                cache_length = past_key_values.get_seq_length()\n                past_length = past_key_values.seen_tokens\n                max_cache_length = past_key_values.get_max_length()\n            else:\n                cache_length = past_length = past_key_values[0][0].shape[2]\n                max_cache_length = None\n\n            # Keep only the unprocessed tokens:\n            # 1 - If the length of the attention_mask exceeds the length of input_ids, then we are in a setting where\n            # some of the inputs are exclusively passed as part of the cache (e.g. when passing input_embeds as\n            # input)\n            if attention_mask is not None and attention_mask.shape[1] > input_ids.shape[1]:\n                input_ids = input_ids[:, -(attention_mask.shape[1] - past_length) :]\n            # 2 - If the past_length is smaller than input_ids', then input_ids holds all input tokens. We can discard\n            # input_ids based on the past_length.\n            elif past_length < input_ids.shape[1]:\n                input_ids = input_ids[:, past_length:]\n            # 3 - Otherwise (past_length >= input_ids.shape[1]), let's assume input_ids only has unprocessed tokens.\n\n            # If we are about to go beyond the maximum cache length, we need to crop the input attention mask.\n            if max_cache_length is not None and attention_mask is not None and cache_length + input_ids.shape[1] > max_cache_length:\n                attention_mask = attention_mask[:, -max_cache_length:]\n\n        position_ids = kwargs.get(\"position_ids\", None)\n        if attention_mask is not None and position_ids is None:\n            # create position_ids on the fly for batch generation\n            position_ids = attention_mask.long().cumsum(-1) - 1\n            position_ids.masked_fill_(attention_mask == 0, 1)\n            if past_key_values:\n                position_ids = position_ids[:, -input_ids.shape[1] :]\n\n        if self.generation_config.cache_implementation == \"static\":\n            # generation with static cache\n            cache_position = kwargs.get(\"cache_position\", None)\n            if cache_position is None:\n                past_length = 0\n            else:\n                past_length = cache_position[-1] + 1\n            input_ids = input_ids[:, past_length:]\n            position_ids = position_ids[:, past_length:]\n\n        # TODO @gante we should only keep a `cache_position` in generate, and do +=1.\n        # same goes for position ids. Could also help with continued generation.\n        input_length = position_ids.shape[-1] if position_ids is not None else input_ids.shape[-1]\n        cache_position = torch.arange(past_length, past_length + input_length, device=input_ids.device)\n        position_ids = position_ids.contiguous() if position_ids is not None else None\n\n        # if `inputs_embeds` are passed, we only want to use them in the 1st generation step\n        if inputs_embeds is not None and past_key_values is None:\n            model_inputs = {\"inputs_embeds\": inputs_embeds}\n        else:\n            # The `contiguous()` here is necessary to have a static stride during decoding. torchdynamo otherwise\n            # recompiles graphs as the stride of the inputs is a guard. Ref: https://github.com/huggingface/transformers/pull/29114\n            # TODO: use `next_tokens` directly instead.\n            model_inputs = {\"input_ids\": input_ids.contiguous()}\n\n        model_inputs.update(\n            {\n                \"position_ids\": position_ids,\n                \"cache_position\": cache_position,\n                \"past_key_values\": past_key_values,\n                \"use_cache\": kwargs.get(\"use_cache\"),\n                \"attention_mask\": attention_mask,\n            }\n        )\n        return model_inputs\n\n    @staticmethod\n    def _reorder_cache(past_key_values, beam_idx):\n        reordered_past = ()\n        for layer_past in past_key_values:\n            reordered_past += (tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past),)\n        return reordered_past\n\n\n@add_start_docstrings(\n    \"\"\"\n    The LLaMa Model transformer with a sequence classification head on top (linear layer).\n\n    [`LlamaForSequenceClassification`] uses the last token in order to do the classification, as other causal models\n    (e.g. GPT-2) do.\n\n    Since it does classification on the last token, it requires to know the position of the last token. If a\n    `pad_token_id` is defined in the configuration, it finds the last token that is not a padding token in each row. If\n    no `pad_token_id` is defined, it simply takes the last value in each row of the batch. Since it cannot guess the\n    padding tokens when `inputs_embeds` are passed instead of `input_ids`, it does the same (take the last value in\n    each row of the batch).\n    \"\"\",\n    LLAMA_START_DOCSTRING,\n)\nclass LlamaForSequenceClassification(LlamaPreTrainedModel):\n    def __init__(self, config):\n        super().__init__(config)\n        self.num_labels = config.num_labels\n        self.model = LlamaModel(config)\n        self.score = nn.Linear(config.hidden_size, self.num_labels, bias=False)\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_input_embeddings(self):\n        return self.model.embed_tokens\n\n    def set_input_embeddings(self, value):\n        self.model.embed_tokens = value\n\n    @add_start_docstrings_to_model_forward(LLAMA_INPUTS_DOCSTRING)\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_values: Optional[List[torch.FloatTensor]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        labels: Optional[torch.LongTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ) -> Union[Tuple, SequenceClassifierOutputWithPast]:\n        r\"\"\"\n        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n        \"\"\"\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        transformer_outputs = self.model(\n            input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            past_key_values=past_key_values,\n            inputs_embeds=inputs_embeds,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n        hidden_states = transformer_outputs[0]\n        logits = self.score(hidden_states)\n\n        if input_ids is not None:\n            batch_size = input_ids.shape[0]\n        else:\n            batch_size = inputs_embeds.shape[0]\n\n        if self.config.pad_token_id is None and batch_size != 1:\n            raise ValueError(\"Cannot handle batch sizes > 1 if no padding token is defined.\")\n        if self.config.pad_token_id is None:\n            sequence_lengths = -1\n        else:\n            if input_ids is not None:\n                # if no pad token found, use modulo instead of reverse indexing for ONNX compatibility\n                sequence_lengths = torch.eq(input_ids, self.config.pad_token_id).int().argmax(-1) - 1\n                sequence_lengths = sequence_lengths % input_ids.shape[-1]\n                sequence_lengths = sequence_lengths.to(logits.device)\n            else:\n                sequence_lengths = -1\n\n        pooled_logits = logits[torch.arange(batch_size, device=logits.device), sequence_lengths]\n\n        loss = None\n        if labels is not None:\n            labels = labels.to(logits.device)\n            if self.config.problem_type is None:\n                if self.num_labels == 1:\n                    self.config.problem_type = \"regression\"\n                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n                    self.config.problem_type = \"single_label_classification\"\n                else:\n                    self.config.problem_type = \"multi_label_classification\"\n\n            if self.config.problem_type == \"regression\":\n                loss_fct = MSELoss()\n                if self.num_labels == 1:\n                    loss = loss_fct(pooled_logits.squeeze(), labels.squeeze())\n                else:\n                    loss = loss_fct(pooled_logits, labels)\n            elif self.config.problem_type == \"single_label_classification\":\n                loss_fct = CrossEntropyLoss()\n                loss = loss_fct(pooled_logits.view(-1, self.num_labels), labels.view(-1))\n            elif self.config.problem_type == \"multi_label_classification\":\n                loss_fct = BCEWithLogitsLoss()\n                loss = loss_fct(pooled_logits, labels)\n        if not return_dict:\n            output = (pooled_logits,) + transformer_outputs[1:]\n            return ((loss,) + output) if loss is not None else output\n\n        return SequenceClassifierOutputWithPast(\n            loss=loss,\n            logits=pooled_logits,\n            past_key_values=transformer_outputs.past_key_values,\n            hidden_states=transformer_outputs.hidden_states,\n            attentions=transformer_outputs.attentions,\n        )\n\n\n@add_start_docstrings(\n    \"\"\"\nThe Llama Model transformer with a span classification head on top for extractive question-answering tasks like\nSQuAD (a linear layer on top of the hidden-states output to compute `span start logits` and `span end logits`).\n    \"\"\",\n    LLAMA_START_DOCSTRING,\n)\nclass LlamaForQuestionAnswering(LlamaPreTrainedModel):\n    base_model_prefix = \"transformer\"\n\n    # Copied from transformers.models.bloom.modeling_bloom.BloomForQuestionAnswering.__init__ with Bloom->Llama\n    def __init__(self, config):\n        super().__init__(config)\n        self.transformer = LlamaModel(config)\n        self.qa_outputs = nn.Linear(config.hidden_size, 2)\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_input_embeddings(self):\n        return self.transformer.embed_tokens\n\n    def set_input_embeddings(self, value):\n        self.transformer.embed_tokens = value\n\n    @add_start_docstrings_to_model_forward(LLAMA_INPUTS_DOCSTRING)\n    def forward(\n        self,\n        input_ids: Optional[torch.LongTensor] = None,\n        attention_mask: Optional[torch.FloatTensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_values: Optional[List[torch.FloatTensor]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        start_positions: Optional[torch.LongTensor] = None,\n        end_positions: Optional[torch.LongTensor] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ) -> Union[Tuple, QuestionAnsweringModelOutput]:\n        r\"\"\"\n        start_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n            Labels for position (index) of the start of the labelled span for computing the token classification loss.\n            Positions are clamped to the length of the sequence (`sequence_length`). Position outside of the sequence\n            are not taken into account for computing the loss.\n        end_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n            Labels for position (index) of the end of the labelled span for computing the token classification loss.\n            Positions are clamped to the length of the sequence (`sequence_length`). Position outside of the sequence\n            are not taken into account for computing the loss.\n        \"\"\"\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        outputs = self.transformer(\n            input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            past_key_values=past_key_values,\n            inputs_embeds=inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        sequence_output = outputs[0]\n\n        logits = self.qa_outputs(sequence_output)\n        start_logits, end_logits = logits.split(1, dim=-1)\n        start_logits = start_logits.squeeze(-1).contiguous()\n        end_logits = end_logits.squeeze(-1).contiguous()\n\n        total_loss = None\n        if start_positions is not None and end_positions is not None:\n            # If we are on multi-GPU, split add a dimension\n            if len(start_positions.size()) > 1:\n                start_positions = start_positions.squeeze(-1).to(start_logits.device)\n            if len(end_positions.size()) > 1:\n                end_positions = end_positions.squeeze(-1).to(end_logits.device)\n            # sometimes the start/end positions are outside our model inputs, we ignore these terms\n            ignored_index = start_logits.size(1)\n            start_positions = start_positions.clamp(0, ignored_index)\n            end_positions = end_positions.clamp(0, ignored_index)\n\n            loss_fct = CrossEntropyLoss(ignore_index=ignored_index)\n            start_loss = loss_fct(start_logits, start_positions)\n            end_loss = loss_fct(end_logits, end_positions)\n            total_loss = (start_loss + end_loss) / 2\n\n        if not return_dict:\n            output = (start_logits, end_logits) + outputs[2:]\n            return ((total_loss,) + output) if total_loss is not None else output\n\n        return QuestionAnsweringModelOutput(\n            loss=total_loss,\n            start_logits=start_logits,\n            end_logits=end_logits,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )\n"}
{"type": "source_file", "path": "llava/eval/eval_mm_reward_bench.py", "content": "import argparse\nimport torch\nimport os\n# os.environ['CUDA_VISIBLE_DEVICES'] = '1,2,3'\nimport json\nfrom tqdm import tqdm\nimport shortuuid\n\nfrom llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN\nfrom llava.conversation import conv_templates, SeparatorStyle\nfrom llava.model.builder import load_pretrained_model\nfrom llava.utils import disable_torch_init\nfrom llava.mm_utils import tokenizer_image_token, process_images, get_model_name_from_path\n\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom PIL import Image\nimport math\nimport numpy as np\n# import debugpy\n\n\ndef split_list(lst, n):\n    \"\"\"Split a list into n (roughly) equal-sized chunks\"\"\"\n    chunk_size = math.ceil(len(lst) / n)  # integer division\n    return [lst[i:i+chunk_size] for i in range(0, len(lst), chunk_size)]\n\n\ndef get_chunk(lst, n, k):\n    chunks = split_list(lst, n)\n    return chunks[k]\n\ndef process_video(video_file, image_processor, config):\n    device = 'cuda'\n    frame_files = [os.path.join(video_file, f) for f in os.listdir(video_file) if os.path.isfile(os.path.join(video_file, f))]\n    frame_files.sort()\n    sample_frames = 32\n    sample_indices = np.linspace(0, len(frame_files) - 1, sample_frames, dtype=int)\n\n    # 确保所有索引都在合法范围内（使用 np.clip）\n    sample_indices = np.clip(sample_indices, 0, len(frame_files) - 1)\n\n    # 获取采样的 frame_files\n    frame_files = [frame_files[i] for i in sample_indices]\n\n    # 打开并转换为 RGB 图像\n    frame_files = [Image.open(item).convert(\"RGB\") for item in frame_files]\n    \n    config.image_aspect_ratio = 'pad'\n    image_tensor = process_images(frame_files, image_processor, config)\n    \n    return image_tensor\n\ndef make_conv_rm(prompt, answer, has_image=True):\n    critic_prompt = (\n        \"You are an unbiased and fair evaluator tasked with assessing the quality of answers provided by a Large Multimodal Model (LMM). \"\n        \"Given the following question and the LMM's response, please evaluate the correctness, clarity, and completeness of the answer. \"\n        \"Provide a detailed explanation of your assessment, including specific points of strength and areas for improvement.\\n\\n\"\n        \"[Question]: {question}\\n\"\n        \"[LMM Response]: {answer}\\n\\n\"\n        \"[Evaluation]:\"\n    )\n    formatted_prompt = critic_prompt.format(question=prompt, answer=answer)\n    if has_image:\n        formatted_prompt = formatted_prompt.replace(DEFAULT_IMAGE_TOKEN, \"\").strip()\n        formatted_prompt = DEFAULT_IMAGE_TOKEN + \"\\n\" + formatted_prompt\n        formatted_prompt = formatted_prompt.strip()\n    return formatted_prompt\n\ndef eval_model(args):\n    # Model\n    disable_torch_init()\n    # model_name = \"llava\"\n    device_map = \"auto\"\n    model_path = os.path.expanduser(args.model_path)\n    #yt\n    model_name = \"llava_qwen\"\n    device = \"cuda\"\n\n    tokenizer, model, image_processor, max_length = load_pretrained_model(model_path, None, model_name, device_map=device_map, attn_implementation=None)\n    model.eval()\n\n    questions = []\n    with open(args.question_file, 'r') as file:\n        for line in file:\n            questions.append(json.loads(line))\n    questions = get_chunk(questions, args.num_chunks, args.chunk_idx)\n    answers_file_with_chunks = f\"{args.answers_file.rsplit('.', 1)[0]}_chunk{args.chunk_idx}_of_{args.num_chunks}.jsonl\"\n    existing_ids = set()\n    try:\n        with open(answers_file_with_chunks, 'r') as file:\n            for line in file:\n                answer = json.loads(line)\n                existing_ids.add(answer.get(\"id\"))\n    except FileNotFoundError:\n        print(f\"No existing file found: {answers_file_with_chunks}, proceeding with empty set of existing IDs.\")\n\n    questions = [q for q in questions if q.get(\"id\") not in existing_ids]\n    print(f'saving all the answers to {answers_file_with_chunks}')\n    answers_file = os.path.expanduser(answers_file_with_chunks)\n    os.makedirs(os.path.dirname(answers_file), exist_ok=True)\n    ans_file = open(answers_file, \"a+\")\n\n    if 'plain' in model_name and 'finetune' not in model_name.lower() and 'mmtag' not in args.conv_mode:\n        args.conv_mode = args.conv_mode + '_mmtag'\n        print(f'It seems that this is a plain model, but it is not using a mmtag prompt, auto switching to {args.conv_mode}.')\n\n    index, cnt_images = 0, []\n    for line in tqdm(questions, total=len(questions)):\n        video_file = line.get(\"video\", None)\n        image_file = line.get(\"image\", None)\n        # image_file = line[\"image\"]\n        \n        if image_file:\n            image = Image.open(os.path.join(args.image_folder, image_file)).convert('RGB')\n            image_tensor = process_images([image], image_processor, model.config)[0]\n            size = [image.size]\n        else:\n            image_tensor = process_video(video_file, image_processor, model.config)\n            size = None\n\n        if video_file:\n            modalities=[\"video\"]\n        if image_file:\n            modalities=[\"image\"]\n\n        if isinstance(image_tensor, tuple):\n            image_tensor = image_tensor[0]\n        if isinstance(image_tensor, list):\n            image_tensor = [_image.to(dtype=torch.float16, device=device) for _image in image_tensor]\n        else:\n            image_tensor = image_tensor.to(dtype=torch.float16, device=device)\n        answers = [line['chosen'], line['rejected']]\n        critics = [line['chosen_reason'], line['rejected_reason']]\n        rewards = []\n        critic_texts = []\n        for t in range(len(answers)):\n            qs = make_conv_rm(line[\"prompt\"], answers[t], video_file or image_file)\n            \n            conv = conv_templates[args.conv_mode].copy()\n            conv.append_message(conv.roles[0], qs)\n            conv.append_message(conv.roles[1], None)\n            prompt = conv.get_prompt()\n\n            input_ids = tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt').unsqueeze(0).to(device)\n            \n            critic_reward = []\n            \n            for critic_t in range(args.num_critic):\n                if not args.use_gt_critic and not args.wo_critic:\n                    with torch.inference_mode():\n                        output_ids = model.generate(\n                            input_ids,\n                            images=image_tensor,\n                            image_sizes=size,\n                            do_sample=True if args.temperature > 0 else False,\n                            modalities=modalities,\n                            temperature=args.temperature,\n                            top_p=args.top_p,\n                            num_beams=args.num_beams,\n                            max_new_tokens=args.max_new_tokens,\n                            use_cache=True)\n                        \n                    critic = tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0].strip()\n                    critic_texts.append(critic)\n                elif not args.wo_critic:\n                    critic = critics[t]\n                else:\n                    critic = None\n                \n                conv = conv_templates[args.conv_mode].copy()\n                conv.append_message(conv.roles[0], qs)\n                conv.append_message(conv.roles[1], critic)\n                prompt = conv.get_prompt()\n\n                input_ids_reward = tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt').unsqueeze(0).to(device)\n                with torch.inference_mode():\n                    reward, _ = model(\n                        input_ids_reward,\n                        images=image_tensor,\n                        image_sizes=size,\n                        rm_forward=True)\n                    critic_reward.append(reward[0].item())\n            \n            rewards.append(np.mean(critic_reward))\n        \n        # import pdb;pdb.set_trace()\n        index += 1\n        line['rewards'] = rewards\n        line['critic'] = critic_texts\n        ans_file.write(json.dumps(line) + \"\\n\")\n        ans_file.flush()\n    ans_file.close()\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    # sampling strategy follows simpo\n    # nohup python scripts/reward_model/ours_cirtic_eval.py --num-chunks 4 --chunk-idx 0 >> /mllm_hdd/mllm_hdd/yfzhang/data/alignment/vl_reward/0_4.log 2>&1 &\n    # parser.add_argument(\"--rm-model-path\", type=str, default=\"../model/alignment/llava16_llava_rlhf_reward_model_lr1e_5_bsz128_freevision_reward_model_coefficent\")\n    parser.add_argument(\"--model-path\", type=str, default=\"yifanzhang114/MM-RLHF-Reward-7B-llava-ov-qwen\")\n    parser.add_argument(\"--model-base\", type=str, default=None)\n    parser.add_argument(\"--image-folder\", type=str, default=\"/mllm_hdd/mllm_hdd/yfzhang/data/alignment/\")\n    parser.add_argument(\"--question-file\", type=str, default=\"./mm_reward_bench.jsonl\")\n    parser.add_argument(\"--answers-file\", type=str, default=\"./mm_reward_bench_result.jsonl\")\n    parser.add_argument(\"--conv-mode\", type=str, default=\"qwen_1_5\")\n    parser.add_argument(\"--num-chunks\", type=int, default=1)\n    parser.add_argument(\"--chunk-idx\", type=int, default=0)\n    parser.add_argument(\"--temperature\", type=float, default=0.)\n    parser.add_argument(\"--top_p\", type=float, default=None)\n    parser.add_argument(\"--num_beams\", type=int, default=1)\n    parser.add_argument(\"--num_critic\", type=int, default=1)\n    parser.add_argument(\"--max_new_tokens\", type=int, default=4096)\n\n    parser.add_argument(\"--use-qlora\", type=bool, default=False)\n    parser.add_argument(\"--use_gt_critic\", type=bool, default=False)\n    parser.add_argument(\"--wo_critic\", type=bool, default=False)\n    parser.add_argument(\"--qlora-path\", type=str, default=\"\")\n\n    parser.add_argument(\n        \"--test-prompt\",\n        type=str,\n        default=\"\",\n    )\n    args = parser.parse_args()\n    print(args)\n\n    eval_model(args)"}
{"type": "source_file", "path": "llava/model/multimodal_encoder/clip_encoder.py", "content": "import torch\nimport torch.nn as nn\nfrom llava.utils import rank0_print\nfrom transformers import CLIPVisionModel, CLIPImageProcessor, CLIPVisionConfig\n\ntry:\n    from s2wrapper import forward as multiscale_forward\nexcept:\n    pass\n\n\nclass CLIPVisionTower(nn.Module):\n    def __init__(self, vision_tower, args, delay_load=False):\n        super().__init__()\n\n        self.is_loaded = False\n\n        self.vision_tower_name = vision_tower\n        self.select_layer = args.mm_vision_select_layer\n        self.select_feature = getattr(args, \"mm_vision_select_feature\", \"patch\")\n\n        if not delay_load:\n            rank0_print(f\"Loading vision tower: {vision_tower}\")\n            self.load_model()\n        elif getattr(args, \"unfreeze_mm_vision_tower\", False):\n            # TODO: better detector is needed.\n            rank0_print(f\"The checkpoint seems to contain `vision_tower` weights: `unfreeze_mm_vision_tower`: True.\")\n            self.load_model()\n        elif hasattr(args, \"mm_tunable_parts\") and \"mm_vision_tower\" in args.mm_tunable_parts:\n            rank0_print(f\"The checkpoint seems to contain `vision_tower` weights: `mm_tunable_parts` contains `mm_vision_tower`.\")\n            self.load_model()\n        else:\n            self.cfg_only = CLIPVisionConfig.from_pretrained(self.vision_tower_name)\n\n    def load_model(self, device_map=None):\n        if self.is_loaded:\n            rank0_print(\"{} is already loaded, `load_model` called again, skipping.\".format(self.vision_tower_name))\n            return\n\n        self.image_processor = CLIPImageProcessor.from_pretrained(self.vision_tower_name)\n        self.vision_tower = CLIPVisionModel.from_pretrained(self.vision_tower_name, device_map=device_map)\n        self.vision_tower.requires_grad_(False)\n\n        self.is_loaded = True\n\n    def feature_select(self, image_forward_outs):\n        select_feature_type = self.select_feature\n\n        if self.select_feature in [\"slicefour_patch\", \"slicefour_cls_patch\"]:\n            select_every_k_layer = len(image_forward_outs.hidden_states) // 4\n            image_features = torch.cat([image_forward_outs.hidden_states[i] for i in range(select_every_k_layer + self.select_layer, len(image_forward_outs.hidden_states), select_every_k_layer)], dim=-1)\n            select_feature_type = select_feature_type.replace(\"slicefour_\", \"\")\n        elif self.select_feature in [\"slice_m25811_f6_patch\", \"slice_m25811_f6_cls_patch\"]:\n            select_layers = [-2, -5, -8, -11, 6]\n            image_features = torch.cat([image_forward_outs.hidden_states[i] for i in select_layers], dim=-1)\n            select_feature_type = select_feature_type.replace(\"slice_m25811_f6_\", \"\")\n        else:\n            image_features = image_forward_outs.hidden_states[self.select_layer]\n\n        if select_feature_type == \"patch\":\n            image_features = image_features[:, 1:]\n        elif select_feature_type == \"cls_patch\":\n            image_features = image_features\n        else:\n            raise ValueError(f\"Unexpected select feature: {select_feature_type}\")\n        return image_features\n\n    def forward(self, images):\n        if type(images) is list:\n            image_features = []\n            for image in images:\n                image_forward_out = self.vision_tower(image.to(device=self.device, dtype=self.dtype).unsqueeze(0), output_hidden_states=True)\n                image_feature = self.feature_select(image_forward_out).to(image.dtype)\n                image_features.append(image_feature)\n        else:\n            image_forward_outs = self.vision_tower(images.to(device=self.device, dtype=self.dtype), output_hidden_states=True)\n            image_features = self.feature_select(image_forward_outs).to(images.dtype)\n\n        return image_features\n\n    @property\n    def dummy_feature(self):\n        return torch.zeros(1, self.hidden_size, device=self.device, dtype=self.dtype)\n\n    @property\n    def dtype(self):\n        return self.vision_tower.dtype\n\n    @property\n    def device(self):\n        return self.vision_tower.device\n\n    @property\n    def config(self):\n        if self.is_loaded:\n            return self.vision_tower.config\n        else:\n            return self.cfg_only\n\n    @property\n    def hidden_size(self):\n        _hidden_size = self.config.hidden_size\n        if \"slicefour\" in self.select_feature:\n            _hidden_size *= 4\n        if \"slice_m25811_f6\" in self.select_feature:\n            _hidden_size *= 5\n        return _hidden_size\n\n    @property\n    def num_patches_per_side(self):\n        return self.config.image_size // self.config.patch_size\n\n    @property\n    def num_patches(self):\n        _num_patches = (self.config.image_size // self.config.patch_size) ** 2\n        if \"cls_patch\" in self.select_feature:\n            _num_patches += 1\n        return _num_patches\n\n    @property\n    def image_size(self):\n        return self.config.image_size\n\n\nclass CLIPVisionTowerS2(CLIPVisionTower):\n    def __init__(self, vision_tower, args, delay_load=False):\n\n        self.s2_scales = getattr(args, \"s2_scales\", \"336,672,1008\")\n        self.s2_scales = list(map(int, self.s2_scales.split(\",\")))\n        self.s2_scales.sort()\n        self.s2_split_size = self.s2_scales[0]\n        self.s2_image_size = self.s2_scales[-1]\n\n        super().__init__(vision_tower, args, delay_load)\n\n        # change resize/crop size in preprocessing to the largest image size in s2_scale\n        if not delay_load or getattr(args, \"unfreeze_mm_vision_tower\", False):\n            self.image_processor.size[\"shortest_edge\"] = self.s2_image_size\n            self.image_processor.crop_size[\"height\"] = self.image_processor.crop_size[\"width\"] = self.s2_image_size\n\n    def load_model(self, device_map=None):\n        if self.is_loaded:\n            rank0_print(\"{} is already loaded, `load_model` called again, skipping.\".format(self.vision_tower_name))\n            return\n\n        self.image_processor = CLIPImageProcessor.from_pretrained(self.vision_tower_name)\n        self.vision_tower = CLIPVisionModel.from_pretrained(self.vision_tower_name, device_map=device_map)\n        self.vision_tower.requires_grad_(False)\n\n        self.image_processor.size[\"shortest_edge\"] = self.s2_image_size\n        self.image_processor.crop_size[\"height\"] = self.image_processor.crop_size[\"width\"] = self.s2_image_size\n\n        self.is_loaded = True\n\n    def forward_feature(self, images):\n        image_forward_outs = self.vision_tower(images.to(device=self.device, dtype=self.dtype), output_hidden_states=True)\n        image_features = self.feature_select(image_forward_outs).to(images.dtype)\n        return image_features\n\n    def forward(self, images):\n        if type(images) is list:\n            image_features = []\n            for image in images:\n                image_feature = multiscale_forward(self.forward_feature, image.unsqueeze(0), img_sizes=self.s2_scales, max_split_size=self.s2_split_size, split_forward=True)\n                image_features.append(image_feature)\n        else:\n            image_features = multiscale_forward(self.forward_feature, images, img_sizes=self.s2_scales, max_split_size=self.s2_split_size, split_forward=True)\n\n        return image_features\n\n    @property\n    def hidden_size(self):\n        return self.config.hidden_size * len(self.s2_scales)\n"}
{"type": "source_file", "path": "llava/model/language_model/llava_llama.py", "content": "#    Copyright 2023 Haotian Liu\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\");\n#    you may not use this file except in compliance with the License.\n#    You may obtain a copy of the License at\n#\n#        http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS,\n#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#    See the License for the specific language governing permissions and\n#    limitations under the License.\n\n\nfrom typing import List, Optional, Tuple, Union\n\nimport torch\nimport torch.nn as nn\n\nfrom transformers import AutoConfig, AutoModelForCausalLM, LlamaConfig\n\nfrom torch.nn import CrossEntropyLoss\n\n\nimport copy\nfrom transformers import LlamaModel, LlamaForCausalLM\nfrom transformers.modeling_outputs import CausalLMOutputWithPast\nfrom transformers.generation.utils import GenerateOutput\n\nfrom llava.model.llava_arch import LlavaMetaModel, LlavaMetaForCausalLM\n\n\nclass LlavaConfig(LlamaConfig):\n    model_type = \"llava_llama\"\n    temperature: float = 0.0  # reset to 0.0, previously 0.9 for Vicuna\n    max_new_tokens: int = 1024\n    do_sample: bool = False\n    top_p: Optional[float] = None\n    # rope_scaling: Optional[dict] = {}\n\n\nclass LlavaLlamaModel(LlavaMetaModel, LlamaModel):\n    config_class = LlavaConfig\n\n    def __init__(self, config: LlamaConfig):\n        super(LlavaLlamaModel, self).__init__(config)\n\n\nclass LlavaLlamaForCausalLM(LlamaForCausalLM, LlavaMetaForCausalLM):\n    config_class = LlavaConfig\n\n    def __init__(self, config):\n        LlamaForCausalLM.__init__(self, config)\n\n        # configure default generation settings\n        config.model_type = \"llava_llama\"\n        # config.rope_scaling = None\n\n        self.model = LlavaLlamaModel(config)\n        if hasattr(config, 'is_rm') and config.is_rm:\n            self.lm_head = nn.Linear(config.hidden_size, 1, bias=True)\n        else:\n            self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_model(self):\n        return self.model\n\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_values: Optional[List[torch.FloatTensor]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        labels: Optional[torch.LongTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        images: Optional[torch.FloatTensor] = None,\n        image_sizes: Optional[List[List[int]]] = None,\n        return_dict: Optional[bool] = None,\n        modalities: Optional[List[str]] = [\"image\"],\n        dpo_forward: Optional[bool] = False,\n        rm_forward: Optional[bool] = False,\n        cache_position=None,\n    ) -> Union[Tuple, CausalLMOutputWithPast]:\n\n        input_ids_tmp = copy.deepcopy(input_ids)\n        if inputs_embeds is None:\n            (input_ids, position_ids, attention_mask, past_key_values, inputs_embeds, labels) = self.prepare_inputs_labels_for_multimodal(input_ids, position_ids, attention_mask, past_key_values, labels, images, modalities, image_sizes)\n\n        if dpo_forward or rm_forward:\n            outputs = self.model(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                position_ids=position_ids,\n                past_key_values=past_key_values,\n                inputs_embeds=inputs_embeds,\n                use_cache=use_cache,\n                output_attentions=output_attentions,\n                output_hidden_states=output_hidden_states,\n                return_dict=return_dict,\n            )\n            \n            if rm_forward:\n                hidden_states = self.lm_head(outputs[0])\n                batch_size = input_ids_tmp.shape[0]\n                if self.config.pad_token_id is None and batch_size != 1:\n                    raise ValueError(\"Cannot handle batch sizes > 1 if no padding token is defined.\")\n                if self.config.pad_token_id is None or attention_mask is None:\n                    sequence_lengths = -1\n                else:\n                    # Pad token defined: Convert attention_mask to integer and compute cumulative sum\n                    # The last valid token is at the last non-zero position\n                    sequence_lengths = attention_mask.long().cumsum(dim=-1)[:, -1] - 1\n\n                hidden_states = hidden_states[torch.arange(batch_size, device=hidden_states.device), sequence_lengths]\n                return hidden_states, labels\n            else:\n                hidden_states = outputs[0]\n                logits = self.lm_head(hidden_states)\n                return logits, labels\n\n        else:\n            return super().forward(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                position_ids=position_ids,\n                past_key_values=past_key_values,\n                inputs_embeds=inputs_embeds,\n                labels=labels,\n                use_cache=use_cache,\n                output_attentions=output_attentions,\n                output_hidden_states=output_hidden_states,\n                return_dict=return_dict,\n            )\n\n    @torch.no_grad()\n    def generate(\n        self,\n        inputs: Optional[torch.Tensor] = None,\n        images: Optional[torch.Tensor] = None,\n        image_sizes: Optional[torch.Tensor] = None,\n        modalities: Optional[List[str]] = [\"image\"],\n        **kwargs,\n    ) -> Union[GenerateOutput, torch.LongTensor]:\n        modalities = kwargs.pop(\"modalities\", None) if \"modalities\" in kwargs and modalities is None else modalities\n        position_ids = kwargs.pop(\"position_ids\", None)\n        attention_mask = kwargs.pop(\"attention_mask\", None)\n        if \"inputs_embeds\" in kwargs:\n            raise NotImplementedError(\"`inputs_embeds` is not supported\")\n\n        if images is not None:\n            (inputs, position_ids, attention_mask, _, inputs_embeds, _) = self.prepare_inputs_labels_for_multimodal(inputs, position_ids, attention_mask, None, None, images, modalities, image_sizes=image_sizes)\n        else:\n            inputs_embeds = self.get_model().embed_tokens(inputs)\n\n        return super().generate(position_ids=position_ids, attention_mask=attention_mask, inputs_embeds=inputs_embeds, **kwargs)\n\n    def prepare_inputs_for_generation(self, input_ids, past_key_values=None, inputs_embeds=None, **kwargs):\n        images = kwargs.pop(\"images\", None)\n        image_sizes = kwargs.pop(\"image_sizes\", None)\n        inputs = super().prepare_inputs_for_generation(input_ids, past_key_values=past_key_values, inputs_embeds=inputs_embeds, **kwargs)\n        if images is not None:\n            inputs[\"images\"] = images\n        if image_sizes is not None:\n            inputs[\"image_sizes\"] = image_sizes\n        return inputs\n\n\nAutoConfig.register(\"llava_llama\", LlavaConfig)\nAutoModelForCausalLM.register(LlavaConfig, LlavaLlamaForCausalLM)\n"}
{"type": "source_file", "path": "llava/model/multimodal_encoder/dev_eva_clip/eva_clip/constants.py", "content": "OPENAI_DATASET_MEAN = (0.48145466, 0.4578275, 0.40821073)\nOPENAI_DATASET_STD = (0.26862954, 0.26130258, 0.27577711)\n"}
{"type": "source_file", "path": "llava/model/consolidate.py", "content": "\"\"\"\nUsage:\npython3 -m llava.model.consolidate --src ~/model_weights/llava-7b --dst ~/model_weights/llava-7b_consolidate\n\"\"\"\n\nimport argparse\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom llava.model import *\nfrom llava.model.utils import auto_upgrade\n\n\ndef consolidate_ckpt(src_path, dst_path):\n    print(\"Loading model\")\n    auto_upgrade(src_path)\n    src_model = AutoModelForCausalLM.from_pretrained(src_path, torch_dtype=torch.float16, low_cpu_mem_usage=True)\n    src_tokenizer = AutoTokenizer.from_pretrained(src_path, use_fast=False)\n    src_model.save_pretrained(dst_path)\n    src_tokenizer.save_pretrained(dst_path)\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--src\", type=str, required=True)\n    parser.add_argument(\"--dst\", type=str, required=True)\n\n    args = parser.parse_args()\n\n    consolidate_ckpt(args.src, args.dst)\n"}
{"type": "source_file", "path": "llava/model/language_model/llava_qwen.py", "content": "#    Copyright 2024 Hao Zhang\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\");\n#    you may not use this file except in compliance with the License.\n#    You may obtain a copy of the License at\n#\n#        http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS,\n#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#    See the License for the specific language governing permissions and\n#    limitations under the License.\n\n\nfrom typing import List, Optional, Tuple, Union, Dict\nimport torch\nimport torch.nn as nn\nfrom torch.nn import CrossEntropyLoss\n\nimport copy\nfrom transformers import AutoConfig, AutoModelForCausalLM, PretrainedConfig, LlamaModel, LlamaForCausalLM\n\nfrom transformers.modeling_outputs import CausalLMOutputWithPast\nfrom transformers.generation.utils import GenerateOutput\n\n# from ...constants import IGNORE_INDEX, IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN\nfrom llava.model.llava_arch import LlavaMetaModel, LlavaMetaForCausalLM\nfrom transformers import Qwen2Config, Qwen2Model, Qwen2ForCausalLM\nfrom typing import Any\n\n# from .qwen.modeling_qwen import QWenLMHeadModel, QWenModel\n# from .qwen.configuration_qwen import QWenConfig\n\n\nclass LlavaQwenConfig(Qwen2Config):\n    model_type = \"llava_qwen\"\n\n\nclass LlavaQwenModel(LlavaMetaModel, Qwen2Model):\n    config_class = LlavaQwenConfig\n\n    def __init__(self, config: Qwen2Config):\n        super(LlavaQwenModel, self).__init__(config)\n\nclass RewardHead(nn.Module):\n\n    def __init__(self, cfg: PretrainedConfig, n_labels: int):\n        super().__init__()\n        self.dense = nn.Linear(cfg.hidden_size, cfg.hidden_size)\n        # use same dropout as attention dropout\n        self.dropout = nn.Dropout(cfg.attention_dropout)\n        self.out_proj = nn.Linear(cfg.hidden_size, n_labels)\n\n    def forward(self, hidden_states: torch.Tensor, **kwargs: Any):\n        hidden_states = self.dropout(hidden_states)\n        hidden_states = self.dense(hidden_states)\n        hidden_states = torch.tanh(hidden_states)\n        hidden_states = self.dropout(hidden_states)\n        output = self.out_proj(hidden_states)\n        return output\n    \nclass LlavaQwenForCausalLM(Qwen2ForCausalLM, LlavaMetaForCausalLM):\n    config_class = LlavaQwenConfig\n\n    def __init__(self, config):\n        # super(Qwen2ForCausalLM, self).__init__(config)\n        Qwen2ForCausalLM.__init__(self, config)\n        config.model_type = \"llava_qwen\"\n        config.rope_scaling = None\n\n        self.model = LlavaQwenModel(config)\n        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n        if hasattr(config, 'is_rm') and config.is_rm:\n            self.rm_head = RewardHead(config, 1)\n            \n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_model(self):\n        return self.model\n\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_values: Optional[List[torch.FloatTensor]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        labels: Optional[torch.LongTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        images: Optional[torch.FloatTensor] = None,\n        image_sizes: Optional[List[List[int]]] = None,\n        return_dict: Optional[bool] = None,\n        modalities: Optional[List[str]] = [\"image\"],\n        dpo_forward: Optional[bool] = False,\n        rm_forward: Optional[bool] = False,\n        cache_position=None,\n    ) -> Union[Tuple, CausalLMOutputWithPast]:\n\n        input_ids_tmp = copy.deepcopy(input_ids)\n        if inputs_embeds is None:\n            (input_ids, position_ids, attention_mask, past_key_values, inputs_embeds, labels) = self.prepare_inputs_labels_for_multimodal(input_ids, position_ids, attention_mask, past_key_values, labels, images, modalities, image_sizes)\n\n        if dpo_forward or rm_forward:\n            outputs = super().forward(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                position_ids=position_ids,\n                past_key_values=past_key_values,\n                inputs_embeds=inputs_embeds,\n                use_cache=use_cache,\n                labels=labels,\n                output_attentions=output_attentions,\n                output_hidden_states=True,\n                return_dict=True,\n            )\n\n            if rm_forward:\n                hidden_states = outputs.hidden_states[-1]\n                rewards = self.rm_head(hidden_states)\n                batch_size = input_ids_tmp.shape[0]\n                if self.config.pad_token_id is None and batch_size != 1:\n                    raise ValueError(\"Cannot handle batch sizes > 1 if no padding token is defined.\")\n                if self.config.pad_token_id is None or attention_mask is None:\n                    sequence_lengths = -1\n                else:\n                    sequence_lengths = attention_mask.long().cumsum(dim=-1)[:, -1] - 1\n\n                rewards = rewards[torch.arange(batch_size, device=rewards.device), sequence_lengths]\n                return rewards, outputs.loss\n            else:\n                hidden_states = outputs.hidden_states[-1]\n                logits = self.lm_head(hidden_states)\n                return logits, labels\n\n        else:\n            return super().forward(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                position_ids=position_ids,\n                past_key_values=past_key_values,\n                inputs_embeds=inputs_embeds,\n                labels=labels,\n                use_cache=use_cache,\n                output_attentions=output_attentions,\n                output_hidden_states=output_hidden_states,\n                return_dict=return_dict,\n            )\n\n    @torch.no_grad()\n    def generate(\n        self,\n        inputs: Optional[torch.Tensor] = None,\n        images: Optional[torch.Tensor] = None,\n        image_sizes: Optional[torch.Tensor] = None,\n        modalities: Optional[List[str]] = [\"image\"],\n        **kwargs,\n    ) -> Union[GenerateOutput, torch.LongTensor]:\n        position_ids = kwargs.pop(\"position_ids\", None)\n        attention_mask = kwargs.pop(\"attention_mask\", None)\n        if \"inputs_embeds\" in kwargs:\n            raise NotImplementedError(\"`inputs_embeds` is not supported\")\n\n        if images is not None:\n            (inputs, position_ids, attention_mask, _, inputs_embeds, _) = self.prepare_inputs_labels_for_multimodal(inputs, position_ids, attention_mask, None, None, images, modalities, image_sizes=image_sizes)\n        else:\n            inputs_embeds = self.get_model().embed_tokens(inputs)\n\n        return super().generate(position_ids=position_ids, attention_mask=attention_mask, inputs_embeds=inputs_embeds, **kwargs)\n\n    def prepare_inputs_for_generation(self, input_ids, past_key_values=None, inputs_embeds=None, **kwargs):\n        images = kwargs.pop(\"images\", None)\n        image_sizes = kwargs.pop(\"image_sizes\", None)\n        inputs = super().prepare_inputs_for_generation(input_ids, past_key_values=past_key_values, inputs_embeds=inputs_embeds, **kwargs)\n        if images is not None:\n            inputs[\"images\"] = images\n        if image_sizes is not None:\n            inputs[\"image_sizes\"] = image_sizes\n        return inputs\n\n\nAutoConfig.register(\"llava_qwen\", LlavaQwenConfig)\nAutoModelForCausalLM.register(LlavaQwenConfig, LlavaQwenForCausalLM)\n"}
{"type": "source_file", "path": "llava/model/language_model/llava_mixtral.py", "content": "#    Copyright 2023 Haotian Liu\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\");\n#    you may not use this file except in compliance with the License.\n#    You may obtain a copy of the License at\n#\n#        http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS,\n#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#    See the License for the specific language governing permissions and\n#    limitations under the License.\n\n\nfrom typing import List, Optional, Tuple, Union\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import CrossEntropyLoss\n\nfrom transformers import AutoConfig, AutoModelForCausalLM, MixtralConfig, MixtralModel, MixtralForCausalLM, GenerationConfig\n\nfrom transformers.modeling_outputs import CausalLMOutputWithPast\nfrom transformers.generation.utils import GenerateOutput\n\nfrom ..llava_arch import LlavaMetaModel, LlavaMetaForCausalLM\n\n\nclass LlavaMixtralConfig(MixtralConfig):\n    model_type = \"llava_mixtral\"\n\n\nclass LlavaMixtralModel(LlavaMetaModel, MixtralModel):\n    config_class = LlavaMixtralConfig\n\n    def __init__(self, config: MixtralConfig):\n        super(LlavaMixtralModel, self).__init__(config)\n\n\nclass LlavaMixtralForCausalLM(MixtralForCausalLM, LlavaMetaForCausalLM):\n    config_class = LlavaMixtralConfig\n\n    def __init__(self, config):\n        super(MixtralForCausalLM, self).__init__(config)\n\n        config.model_type = \"llava_mixtral\"\n        config.rope_scaling = None\n        self.model = LlavaMixtralModel(config)\n        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_model(self):\n        return self.model\n\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_values: Optional[List[torch.FloatTensor]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        labels: Optional[torch.LongTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        images: Optional[torch.FloatTensor] = None,\n        image_sizes: Optional[List[List[int]]] = None,\n        return_dict: Optional[bool] = None,\n        modalities: Optional[List[str]] = [\"image\"],\n        dpo_forward: Optional[bool] = None,\n        cache_position=None,\n    ) -> Union[Tuple, CausalLMOutputWithPast]:\n\n        if inputs_embeds is None:\n            (input_ids, position_ids, attention_mask, past_key_values, inputs_embeds, labels) = self.prepare_inputs_labels_for_multimodal(input_ids, position_ids, attention_mask, past_key_values, labels, images, modalities, image_sizes)\n\n        if dpo_forward:\n            outputs = self.model(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                position_ids=position_ids,\n                past_key_values=past_key_values,\n                inputs_embeds=inputs_embeds,\n                use_cache=use_cache,\n                output_attentions=output_attentions,\n                output_hidden_states=output_hidden_states,\n                return_dict=return_dict,\n            )\n\n            hidden_states = outputs[0]\n            logits = self.lm_head(hidden_states)\n            return logits, labels\n\n        else:\n            return super().forward(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                position_ids=position_ids,\n                past_key_values=past_key_values,\n                inputs_embeds=inputs_embeds,\n                labels=labels,\n                use_cache=use_cache,\n                output_attentions=output_attentions,\n                output_hidden_states=output_hidden_states,\n                return_dict=return_dict,\n            )\n\n    @torch.no_grad()\n    def generate(\n        self,\n        inputs: Optional[torch.Tensor] = None,\n        images: Optional[torch.Tensor] = None,\n        image_sizes: Optional[torch.Tensor] = None,\n        modalities: Optional[List[str]] = [\"image\"],\n        **kwargs,\n    ) -> Union[GenerateOutput, torch.LongTensor]:\n        position_ids = kwargs.pop(\"position_ids\", None)\n        attention_mask = kwargs.pop(\"attention_mask\", None)\n        if \"inputs_embeds\" in kwargs:\n            raise NotImplementedError(\"`inputs_embeds` is not supported\")\n\n        if images is not None:\n            (inputs, position_ids, attention_mask, _, inputs_embeds, _) = self.prepare_inputs_labels_for_multimodal(inputs, position_ids, attention_mask, None, None, images, modalities, image_sizes=image_sizes)\n        else:\n            inputs_embeds = self.get_model().embed_tokens(inputs)\n\n        return super().generate(position_ids=position_ids, attention_mask=attention_mask, inputs_embeds=inputs_embeds, **kwargs)\n\n    def prepare_inputs_for_generation(self, input_ids, past_key_values=None, inputs_embeds=None, **kwargs):\n        images = kwargs.pop(\"images\", None)\n        image_sizes = kwargs.pop(\"image_sizes\", None)\n        inputs = super().prepare_inputs_for_generation(input_ids, past_key_values=past_key_values, inputs_embeds=inputs_embeds, **kwargs)\n        if images is not None:\n            inputs[\"images\"] = images\n        if image_sizes is not None:\n            inputs[\"image_sizes\"] = image_sizes\n        return inputs\n\n\nAutoConfig.register(\"llava_mixtral\", LlavaMixtralConfig)\nAutoModelForCausalLM.register(LlavaMixtralConfig, LlavaMixtralForCausalLM)\n"}
{"type": "source_file", "path": "llava/model/llava_arch.py", "content": "#    Copyright 2023 Haotian Liu\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\");\n#    you may not use this file except in compliance with the License.\n#    You may obtain a copy of the License at\n#\n#        http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS,\n#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#    See the License for the specific language governing permissions and\n#    limitations under the License.\n\n\nfrom abc import ABC, abstractmethod\n\nimport math\nimport re\nimport time\nimport torch\nimport torch.nn as nn\nfrom .multimodal_encoder.builder import build_vision_tower\nfrom .multimodal_resampler.builder import build_vision_resampler\nfrom .multimodal_projector.builder import build_vision_projector\n\nfrom llava.constants import IGNORE_INDEX, IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_PATCH_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN\n\nfrom llava.mm_utils import get_anyres_image_grid_shape\nfrom llava.utils import rank0_print, rank_print\nimport random\n\n\nclass LlavaMetaModel:\n\n    def __init__(self, config):\n        super(LlavaMetaModel, self).__init__(config)\n\n        if hasattr(config, \"mm_vision_tower\"):\n            delay_load = getattr(config, \"delay_load\", False)\n            self.vision_tower = build_vision_tower(config, delay_load=delay_load)\n            self.vision_resampler = build_vision_resampler(config, vision_tower=self.vision_tower)\n            self.mm_projector = build_vision_projector(config, vision_cfg=self.vision_tower.config)\n\n            if \"unpad\" in getattr(config, \"mm_patch_merge_type\", \"\"):\n                self.image_newline = nn.Parameter(torch.empty(config.hidden_size, dtype=self.dtype))\n\n    def get_vision_tower(self):\n        vision_tower = getattr(self, \"vision_tower\", None)\n        if type(vision_tower) is list:\n            vision_tower = vision_tower[0]\n        return vision_tower\n\n    def initialize_vision_modules(self, model_args, fsdp=None):\n        vision_tower = model_args.vision_tower\n        mm_vision_select_layer = model_args.mm_vision_select_layer\n        mm_vision_select_feature = model_args.mm_vision_select_feature\n        pretrain_mm_mlp_adapter = model_args.pretrain_mm_mlp_adapter\n        mm_patch_merge_type = model_args.mm_patch_merge_type\n\n        self.config.mm_vision_tower = vision_tower\n        self.config.vision_tower_pretrained = getattr(model_args, \"vision_tower_pretrained\", \"\")\n\n        if self.get_vision_tower() is None:\n            vision_tower = build_vision_tower(model_args)\n            vision_resampler = build_vision_resampler(model_args, vision_tower=vision_tower)\n            for k, v in vision_resampler.config.items():\n                setattr(self.config, k, v)\n\n            if fsdp is not None and len(fsdp) > 0:\n                self.vision_tower = [vision_tower]\n                self.vision_resampler = [vision_resampler]\n            else:\n                self.vision_tower = vision_tower\n                self.vision_resampler = vision_resampler\n        else:\n            if fsdp is not None and len(fsdp) > 0:\n                vision_resampler = self.vision_resampler[0]\n                vision_tower = self.vision_tower[0]\n            else:\n                vision_resampler = self.vision_resampler\n                vision_tower = self.vision_tower\n            vision_tower.load_model()\n\n            # In case it is frozen by LoRA\n            for p in self.vision_resampler.parameters():\n                p.requires_grad = True\n\n        self.config.use_mm_proj = True\n        self.config.mm_projector_type = getattr(model_args, \"mm_projector_type\", \"linear\")\n        self.config.mm_hidden_size = getattr(vision_resampler, \"hidden_size\", vision_tower.hidden_size)\n        self.config.mm_vision_select_layer = mm_vision_select_layer\n        self.config.mm_vision_select_feature = mm_vision_select_feature\n        self.config.mm_patch_merge_type = mm_patch_merge_type\n\n        \n        if not hasattr(self.config, 'add_faster_video'):\n            if model_args.add_faster_video:\n                embed_std = 1 / torch.sqrt(torch.tensor(self.config.hidden_size, dtype=self.dtype))\n                self.faster_token = nn.Parameter(\n                    torch.randn(self.config.hidden_size, dtype=self.dtype) * embed_std\n                )\n\n        if getattr(self, \"mm_projector\", None) is None:\n            self.mm_projector = build_vision_projector(self.config, vision_cfg=vision_tower.config)\n\n            if \"unpad\" in mm_patch_merge_type:\n                embed_std = 1 / torch.sqrt(torch.tensor(self.config.hidden_size, dtype=self.dtype))\n                self.image_newline = nn.Parameter(torch.randn(self.config.hidden_size, dtype=self.dtype) * embed_std)\n        else:\n            # In case it is frozen by LoRA\n            for p in self.mm_projector.parameters():\n                p.requires_grad = True\n\n        if pretrain_mm_mlp_adapter is not None:\n            mm_projector_weights = torch.load(pretrain_mm_mlp_adapter, map_location=\"cpu\")\n\n            def get_w(weights, keyword):\n                return {k.split(keyword + \".\")[1]: v for k, v in weights.items() if keyword in k}\n\n            incompatible_keys = self.mm_projector.load_state_dict(get_w(mm_projector_weights, \"mm_projector\"))\n            rank0_print(f\"Loaded mm projector weights from {pretrain_mm_mlp_adapter}. Incompatible keys: {incompatible_keys}\")\n            incompatible_keys = self.vision_resampler.load_state_dict(get_w(mm_projector_weights, \"vision_resampler\"), strict=False)\n            rank0_print(f\"Loaded vision resampler weights from {pretrain_mm_mlp_adapter}. Incompatible keys: {incompatible_keys}\")\n\n\ndef unpad_image(tensor, original_size):\n    \"\"\"\n    Unpads a PyTorch tensor of a padded and resized image.\n\n    Args:\n    tensor (torch.Tensor): The image tensor, assumed to be in CxHxW format.\n    original_size (tuple): The original size of the image (height, width).\n\n    Returns:\n    torch.Tensor: The unpadded image tensor.\n    \"\"\"\n    original_width, original_height = original_size\n    current_height, current_width = tensor.shape[1:]\n\n    # Compute aspect ratios\n    original_aspect_ratio = original_width / original_height\n    current_aspect_ratio = current_width / current_height\n\n    # Determine padding size and direction\n    if original_aspect_ratio > current_aspect_ratio:\n        # Padding was added to the height\n        scale_factor = current_width / original_width\n        new_height = int(original_height * scale_factor)\n        padding = (current_height - new_height) // 2\n        unpadded_tensor = tensor[:, padding : current_height - padding, :]\n    else:\n        # Padding was added to the width\n        scale_factor = current_height / original_height\n        new_width = int(original_width * scale_factor)\n        padding = (current_width - new_width) // 2\n        unpadded_tensor = tensor[:, :, padding : current_width - padding]\n\n    return unpadded_tensor\n\n\nclass LlavaMetaForCausalLM(ABC):\n\n    @abstractmethod\n    def get_model(self):\n        pass\n\n    def get_vision_tower(self):\n        return self.get_model().get_vision_tower()\n\n    def get_2dPool(self, image_feature, stride=2):\n        height = width = self.get_vision_tower().num_patches_per_side\n        num_frames, num_tokens, num_dim = image_feature.shape\n        image_feature = image_feature.view(num_frames, height, width, -1)\n        image_feature = image_feature.permute(0, 3, 1, 2).contiguous()\n        # image_feature = nn.functional.max_pool2d(image_feature, self.config.mm_spatial_pool_stride)\n        if self.config.mm_spatial_pool_mode == \"average\":\n            image_feature = nn.functional.avg_pool2d(image_feature, stride)\n        elif self.config.mm_spatial_pool_mode == \"max\":\n            image_feature = nn.functional.max_pool2d(image_feature, stride)\n        elif self.config.mm_spatial_pool_mode == \"bilinear\":\n            height, width = image_feature.shape[2:]\n            scaled_shape = [math.ceil(height / stride), math.ceil(width / stride)]\n            image_feature = nn.functional.interpolate(image_feature, size=scaled_shape, mode='bilinear')\n\n        else:\n            raise ValueError(f\"Unexpected mm_spatial_pool_mode: {self.config.mm_spatial_pool_mode}\")\n        image_feature = image_feature.permute(0, 2, 3, 1)\n        image_feature = image_feature.view(num_frames, -1, num_dim)\n        return image_feature\n\n    def encode_images(self, images):\n        image_features = self.get_model().get_vision_tower()(images)\n        # image_features = self.get_model().vision_resampler(image_features, images=images)\n        image_features = self.get_model().mm_projector(image_features)\n        return image_features\n    \n    def encode_multimodals(self, videos_or_images, video_idx_in_batch, split_sizes=None):\n        videos_or_images_features = self.get_model().get_vision_tower()(videos_or_images)\n        per_videos_or_images_features = torch.split(videos_or_images_features, split_sizes, dim=0)  # tuple, (dim_1, 576, 4096)\n        all_videos_or_images_features = []\n        all_faster_video_features = []\n        cur_mm_spatial_pool_stride = self.config.mm_spatial_pool_stride\n\n        for idx, feat in enumerate(per_videos_or_images_features):\n            \n            feat = self.get_model().mm_projector(feat)\n            faster_video_feature = 0\n            slower_img_feat = 0\n            if idx in video_idx_in_batch and cur_mm_spatial_pool_stride > 1:\n                slower_img_feat = self.get_2dPool(feat,cur_mm_spatial_pool_stride)\n                if self.config.add_faster_video:\n                    cur_mm_spatial_pool_stride = cur_mm_spatial_pool_stride * 2\n                    faster_video_feature = self.get_2dPool(feat,cur_mm_spatial_pool_stride)\n            if slower_img_feat is not 0:\n                all_videos_or_images_features.append(slower_img_feat)\n            else:\n                all_videos_or_images_features.append(feat)\n            all_faster_video_features.append(faster_video_feature)\n        return all_videos_or_images_features,all_faster_video_features\n\n    def add_token_per_grid(self, image_feature):\n        resize_h = int(math.sqrt(image_feature.shape[1]))\n        num_frames = image_feature.shape[0]\n        feature_dim = image_feature.shape[-1]\n\n        image_feature = image_feature.view(num_frames, 1, resize_h, resize_h, -1)\n        image_feature = image_feature.permute(4, 0, 2, 1, 3).contiguous()\n        image_feature = image_feature.flatten(1, 2).flatten(2, 3)\n        image_feature = torch.cat((image_feature, self.model.image_newline[:, None, None].expand(*image_feature.shape[:-1], 1).to(image_feature.device)), dim=-1)\n        if getattr(self.config, \"add_faster_video\", False):\n            # import pdb; pdb.set_trace()\n            # (3584, 832, 14) -> (3584, 64, 13, 14)\n            image_feature = image_feature.view(feature_dim, num_frames,resize_h, -1)\n            #  (3584, 64, 13, 14) -> (64, 13, 14, 3584)\n            image_feature = image_feature.permute(1, 2, 3, 0).contiguous()\n            # (64, 13, 14, 3584) -> (64, 13*14, 3584)\n            image_feature = image_feature.flatten(1, 2)\n            # import pdb; pdb.set_trace()\n            return image_feature\n        # import pdb; pdb.set_trace()\n        image_feature = image_feature.flatten(1, 2).transpose(0, 1)\n        return image_feature\n\n    def add_token_per_frame(self, image_feature):\n        image_feature = image_feature.permute(2, 0, 1).contiguous()\n        image_feature =  torch.cat((image_feature, self.model.image_newline[:, None, None].expand(*image_feature.shape[:-1], 1).to(image_feature.device)), dim=-1)\n        image_feature = image_feature.permute(1, 2, 0).contiguous()\n        return image_feature\n\n    def prepare_inputs_labels_for_multimodal(self, input_ids, position_ids, attention_mask, past_key_values, labels, images, modalities=[\"image\"], image_sizes=None):\n        vision_tower = self.get_vision_tower()\n        # rank_print(modalities)\n        if vision_tower is None or images is None or input_ids.shape[1] == 1:\n            return input_ids, position_ids, attention_mask, past_key_values, None, labels\n\n        if isinstance(modalities, str):\n            modalities = [modalities]\n\n        # import pdb; pdb.set_trace()\n        if type(images) is list or images.ndim == 5:\n            if type(images) is list:\n                images = [x.unsqueeze(0) if x.ndim == 3 else x for x in images]\n\n            video_idx_in_batch = []\n            for _ in range(len(modalities)):\n                if modalities[_] == \"video\":\n                    video_idx_in_batch.append(_)\n\n            images_list = []\n            for image in images:\n                if image.ndim == 4:\n                    images_list.append(image)\n                else:\n                    images_list.append(image.unsqueeze(0))\n\n            concat_images = torch.cat([image for image in images_list], dim=0)\n            split_sizes = [image.shape[0] for image in images_list]\n            encoded_image_features = self.encode_images(concat_images)\n            # image_features,all_faster_video_features = self.encode_multimodals(concat_images, video_idx_in_batch, split_sizes)\n\n            # This is a list, each element is [num_images, patch * patch, dim]\n            # rank_print(f\"Concat images : {concat_images.shape}\")\n            encoded_image_features = torch.split(encoded_image_features, split_sizes)\n            image_features = []\n            for idx, image_feat in enumerate(encoded_image_features):\n                if idx in video_idx_in_batch:\n                    image_features.append(self.get_2dPool(image_feat))\n                else:\n                    image_features.append(image_feat)\n            # image_features = self.encode_multimodals(concat_images, video_idx_in_batch, split_sizes)\n            # rank_print(f\"Encoded image feats : {[x.shape for x in image_features]}\")\n            # image_features = torch.split(image_features, split_sizes, dim=0)\n            mm_patch_merge_type = getattr(self.config, \"mm_patch_merge_type\", \"flat\")\n            image_aspect_ratio = getattr(self.config, \"image_aspect_ratio\", \"square\")\n            mm_newline_position = getattr(self.config, \"mm_newline_position\", \"one_token\")\n\n            if mm_patch_merge_type == \"flat\":\n                image_features = [x.flatten(0, 1) for x in image_features]\n\n            elif mm_patch_merge_type.startswith(\"spatial\"):\n                new_image_features = []\n                for image_idx, image_feature in enumerate(image_features):\n                    # FIXME: now assume the image is square, and split to 2x2 patches\n                    # num_patches = h * w, where h = w = sqrt(num_patches)\n                    # currently image_feature is a tensor of shape (4, num_patches, hidden_size)\n                    # we want to first unflatten it to (2, 2, h, w, hidden_size)\n                    # rank0_print(\"At least we are reaching here\")\n                    # import pdb; pdb.set_trace()\n                    if image_idx in video_idx_in_batch:  # video operations\n                        # rank0_print(\"Video\")\n                        if mm_newline_position == \"grid\":\n                            # Grid-wise\n                            image_feature = self.add_token_per_grid(image_feature)\n                            if getattr(self.config, \"add_faster_video\", False):\n                                faster_video_feature = self.add_token_per_grid(all_faster_video_features[image_idx])\n                                # Add a token for each frame\n                                concat_slow_fater_token = []\n                                # import pdb; pdb.set_trace()\n                                for _ in range(image_feature.shape[0]):\n                                    if _ % self.config.faster_token_stride == 0:\n                                        concat_slow_fater_token.append(torch.cat((image_feature[_], self.model.faster_token[None].to(image_feature.device)), dim=0))\n                                    else:\n                                        concat_slow_fater_token.append(torch.cat((faster_video_feature[_], self.model.faster_token[None].to(image_feature.device)), dim=0))\n                                # import pdb; pdb.set_trace()\n                                image_feature = torch.cat(concat_slow_fater_token)\n\n                                # print(\"!!!!!!!!!!!!\")\n                        \n                            new_image_features.append(image_feature)\n                        elif mm_newline_position == \"frame\":\n                            # Frame-wise\n                            image_feature = self.add_token_per_frame(image_feature)\n\n                            new_image_features.append(image_feature.flatten(0, 1))\n                            \n                        elif mm_newline_position == \"one_token\":\n                            # one-token\n                            image_feature = image_feature.flatten(0, 1)\n                            if 'unpad' in mm_patch_merge_type:\n                                image_feature = torch.cat((\n                                    image_feature,\n                                    self.model.image_newline[None].to(image_feature.device)\n                                ), dim=0)\n                            new_image_features.append(image_feature)      \n                        elif mm_newline_position == \"no_token\":\n                            new_image_features.append(image_feature.flatten(0, 1))\n                        else:\n                            raise ValueError(f\"Unexpected mm_newline_position: {mm_newline_position}\")\n                    elif image_feature.shape[0] > 1:  # multi patches and multi images operations\n                        # rank0_print(\"Single-images\")\n                        base_image_feature = image_feature[0]\n                        image_feature = image_feature[1:]\n                        height = width = self.get_vision_tower().num_patches_per_side\n                        assert height * width == base_image_feature.shape[0]\n\n                        if \"anyres_max\" in image_aspect_ratio:\n                            matched_anyres_max_num_patches = re.match(r\"anyres_max_(\\d+)\", image_aspect_ratio)\n                            if matched_anyres_max_num_patches:\n                                max_num_patches = int(matched_anyres_max_num_patches.group(1))\n\n                        if image_aspect_ratio == \"anyres\" or \"anyres_max\" in image_aspect_ratio:\n                            if hasattr(self.get_vision_tower(), \"image_size\"):\n                                vision_tower_image_size = self.get_vision_tower().image_size\n                            else:\n                                raise ValueError(\"vision_tower_image_size is not found in the vision tower.\")\n                            try:\n                                num_patch_width, num_patch_height = get_anyres_image_grid_shape(image_sizes[image_idx], self.config.image_grid_pinpoints, vision_tower_image_size)\n                            except Exception as e:\n                                rank0_print(f\"Error: {e}\")\n                                num_patch_width, num_patch_height = 2, 2\n                            image_feature = image_feature.view(num_patch_height, num_patch_width, height, width, -1)\n                        else:\n                            image_feature = image_feature.view(2, 2, height, width, -1)\n\n                        if \"maxpool2x2\" in mm_patch_merge_type:\n                            image_feature = image_feature.permute(4, 0, 2, 1, 3).contiguous()\n                            image_feature = image_feature.flatten(1, 2).flatten(2, 3)\n                            image_feature = nn.functional.max_pool2d(image_feature, 2)\n                            image_feature = image_feature.flatten(1, 2).transpose(0, 1)\n                        elif \"unpad\" in mm_patch_merge_type and \"anyres_max\" in image_aspect_ratio and matched_anyres_max_num_patches:\n                            unit = image_feature.shape[2]\n                            image_feature = image_feature.permute(4, 0, 2, 1, 3).contiguous()\n                            image_feature = image_feature.flatten(1, 2).flatten(2, 3)\n                            image_feature = unpad_image(image_feature, image_sizes[image_idx])\n                            c, h, w = image_feature.shape\n                            times = math.sqrt(h * w / (max_num_patches * unit**2))\n                            if times > 1.1:\n                                image_feature = image_feature[None]\n                                image_feature = nn.functional.interpolate(image_feature, [int(h // times), int(w // times)], mode=\"bilinear\")[0]\n                            image_feature = torch.cat((image_feature, self.model.image_newline[:, None, None].expand(*image_feature.shape[:-1], 1).to(image_feature.device)), dim=-1)\n                            image_feature = image_feature.flatten(1, 2).transpose(0, 1)\n                        elif \"unpad\" in mm_patch_merge_type:\n                            image_feature = image_feature.permute(4, 0, 2, 1, 3).contiguous()\n                            image_feature = image_feature.flatten(1, 2).flatten(2, 3)\n                            image_feature = unpad_image(image_feature, image_sizes[image_idx])\n                            image_feature = torch.cat((image_feature, self.model.image_newline[:, None, None].expand(*image_feature.shape[:-1], 1).to(image_feature.device)), dim=-1)\n                            image_feature = image_feature.flatten(1, 2).transpose(0, 1)\n                        else:\n                            image_feature = image_feature.permute(0, 2, 1, 3, 4).contiguous()\n                            image_feature = image_feature.flatten(0, 3)\n                        if \"nobase\" in mm_patch_merge_type:\n                            pass\n                        else:\n                            image_feature = torch.cat((base_image_feature, image_feature), dim=0)\n                        new_image_features.append(image_feature)\n                    else:  # single image operations\n                        image_feature = image_feature[0]\n                        if \"unpad\" in mm_patch_merge_type:\n                            image_feature = torch.cat((image_feature, self.model.image_newline[None]), dim=0)\n\n                        new_image_features.append(image_feature)\n                image_features = new_image_features\n            else:\n                raise ValueError(f\"Unexpected mm_patch_merge_type: {self.config.mm_patch_merge_type}\")\n        else:\n            image_features = self.encode_images(images)\n\n        # TODO: image start / end is not implemented here to support pretraining.\n        if getattr(self.config, \"tune_mm_mlp_adapter\", False) and getattr(self.config, \"mm_use_im_start_end\", False):\n            raise NotImplementedError\n        # rank_print(f\"Total images : {len(image_features)}\")\n\n        # Let's just add dummy tensors if they do not exist,\n        # it is a headache to deal with None all the time.\n        # But it is not ideal, and if you have a better idea,\n        # please open an issue / submit a PR, thanks.\n        _labels = labels\n        _position_ids = position_ids\n        _attention_mask = attention_mask\n        if attention_mask is None:\n            attention_mask = torch.ones_like(input_ids, dtype=torch.bool)\n        else:\n            attention_mask = attention_mask.bool()\n        if position_ids is None:\n            position_ids = torch.arange(0, input_ids.shape[1], dtype=torch.long, device=input_ids.device)\n        if labels is None:\n            labels = torch.full_like(input_ids, IGNORE_INDEX)\n\n        # remove the padding using attention_mask -- FIXME\n        _input_ids = input_ids\n        input_ids = [cur_input_ids[cur_attention_mask] for cur_input_ids, cur_attention_mask in zip(input_ids, attention_mask)]\n        labels = [cur_labels[cur_attention_mask] for cur_labels, cur_attention_mask in zip(labels, attention_mask)]\n\n        new_input_embeds = []\n        new_labels = []\n        cur_image_idx = 0\n        # rank_print(\"Inserting Images embedding\")\n        for batch_idx, cur_input_ids in enumerate(input_ids):\n            num_images = (cur_input_ids == IMAGE_TOKEN_INDEX).sum()\n            # rank0_print(num_images)\n            if num_images == 0:\n                cur_image_features = image_features[cur_image_idx]\n                cur_input_embeds_1 = self.get_model().embed_tokens(cur_input_ids)\n                cur_input_embeds = torch.cat([cur_input_embeds_1, cur_image_features[0:0]], dim=0)\n                new_input_embeds.append(cur_input_embeds)\n                new_labels.append(labels[batch_idx])\n                cur_image_idx += 1\n                continue\n\n            image_token_indices = [-1] + torch.where(cur_input_ids == IMAGE_TOKEN_INDEX)[0].tolist() + [cur_input_ids.shape[0]]\n            cur_input_ids_noim = []\n            cur_labels = labels[batch_idx]\n            cur_labels_noim = []\n            for i in range(len(image_token_indices) - 1):\n                cur_input_ids_noim.append(cur_input_ids[image_token_indices[i] + 1 : image_token_indices[i + 1]])\n                cur_labels_noim.append(cur_labels[image_token_indices[i] + 1 : image_token_indices[i + 1]])\n            split_sizes = [x.shape[0] for x in cur_labels_noim]\n            cur_input_embeds = self.get_model().embed_tokens(torch.cat(cur_input_ids_noim))\n            cur_input_embeds_no_im = torch.split(cur_input_embeds, split_sizes, dim=0)\n            cur_new_input_embeds = []\n            cur_new_labels = []\n\n            for i in range(num_images + 1):\n                cur_new_input_embeds.append(cur_input_embeds_no_im[i])\n                cur_new_labels.append(cur_labels_noim[i])\n                if i < num_images:\n                    try:\n                        cur_image_features = image_features[cur_image_idx]\n                    except IndexError:\n                        cur_image_features = image_features[cur_image_idx - 1]\n                    cur_image_idx += 1\n                    cur_new_input_embeds.append(cur_image_features)\n                    cur_new_labels.append(torch.full((cur_image_features.shape[0],), IGNORE_INDEX, device=cur_labels.device, dtype=cur_labels.dtype))\n\n            cur_new_input_embeds = [x.to(self.device) for x in cur_new_input_embeds]\n\n            # import pdb; pdb.set_trace()\n            cur_new_input_embeds = torch.cat(cur_new_input_embeds)\n            cur_new_labels = torch.cat(cur_new_labels)\n\n            new_input_embeds.append(cur_new_input_embeds)\n            new_labels.append(cur_new_labels)\n\n        # Truncate sequences to max length as image embeddings can make the sequence longer\n        tokenizer_model_max_length = getattr(self.config, \"tokenizer_model_max_length\", None)\n        # rank_print(\"Finishing Inserting\")\n\n        new_input_embeds = [x[:tokenizer_model_max_length] for x, modality in zip(new_input_embeds, modalities)]\n        new_labels = [x[:tokenizer_model_max_length] for x, modality in zip(new_labels, modalities)]\n        # TODO: Hard code for control loss spike\n        # if tokenizer_model_max_length is not None:\n        #     new_input_embeds = [x[:4096] if modality != \"video\" else x[:tokenizer_model_max_length] for x, modality in zip(new_input_embeds, modalities)]\n        #     new_labels = [x[:4096] if modality != \"video\" else x[:tokenizer_model_max_length] for x, modality in zip(new_labels, modalities)]\n\n        # Combine them\n        max_len = max(x.shape[0] for x in new_input_embeds)\n        batch_size = len(new_input_embeds)\n\n        new_input_embeds_padded = []\n        new_labels_padded = torch.full((batch_size, max_len), IGNORE_INDEX, dtype=new_labels[0].dtype, device=new_labels[0].device)\n        attention_mask = torch.zeros((batch_size, max_len), dtype=attention_mask.dtype, device=attention_mask.device)\n        position_ids = torch.zeros((batch_size, max_len), dtype=position_ids.dtype, device=position_ids.device)\n        # rank0_print(\"Prepare pos id\")\n\n        for i, (cur_new_embed, cur_new_labels) in enumerate(zip(new_input_embeds, new_labels)):\n            cur_len = cur_new_embed.shape[0]\n            if getattr(self.config, \"tokenizer_padding_side\", \"right\") == \"left\":\n                new_input_embeds_padded.append(torch.cat((torch.zeros((max_len - cur_len, cur_new_embed.shape[1]), dtype=cur_new_embed.dtype, device=cur_new_embed.device), cur_new_embed), dim=0))\n                if cur_len > 0:\n                    new_labels_padded[i, -cur_len:] = cur_new_labels\n                    attention_mask[i, -cur_len:] = True\n                    position_ids[i, -cur_len:] = torch.arange(0, cur_len, dtype=position_ids.dtype, device=position_ids.device)\n            else:\n                new_input_embeds_padded.append(torch.cat((cur_new_embed, torch.zeros((max_len - cur_len, cur_new_embed.shape[1]), dtype=cur_new_embed.dtype, device=cur_new_embed.device)), dim=0))\n                if cur_len > 0:\n                    new_labels_padded[i, :cur_len] = cur_new_labels\n                    attention_mask[i, :cur_len] = True\n                    position_ids[i, :cur_len] = torch.arange(0, cur_len, dtype=position_ids.dtype, device=position_ids.device)\n\n        new_input_embeds = torch.stack(new_input_embeds_padded, dim=0)\n        # rank0_print(\"tokenizer padding\")\n\n        if _labels is None:\n            new_labels = None\n        else:\n            new_labels = new_labels_padded\n\n        if _attention_mask is None:\n            attention_mask = None\n        else:\n            attention_mask = attention_mask.to(dtype=_attention_mask.dtype)\n\n        if _position_ids is None:\n            position_ids = None\n        if getattr(self.config, \"use_pos_skipping\", False) and self.training:\n            position_ids = torch.arange(new_input_embeds.size(1), device=new_input_embeds.device).unsqueeze(0).to(new_input_embeds.device)\n            split_position = random.randint(0, new_input_embeds.size(1))\n            left_add = random.randint(0, self.config.pos_skipping_range)\n            right_add = random.randint(left_add, self.config.pos_skipping_range)\n            position_ids[:, :split_position] += left_add\n            position_ids[:, split_position:] += right_add\n        # import pdb; pdb.set_trace()\n        # rank0_print(\"Finish preparing\")\n        return None, position_ids, attention_mask, past_key_values, new_input_embeds, new_labels\n\n    def initialize_vision_tokenizer(self, model_args, tokenizer):\n        if model_args.mm_use_im_patch_token:\n            tokenizer.add_tokens([DEFAULT_IMAGE_PATCH_TOKEN], special_tokens=True)\n            self.resize_token_embeddings(len(tokenizer))\n\n        if model_args.mm_use_im_start_end:\n            num_new_tokens = tokenizer.add_tokens([DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN], special_tokens=True)\n            self.resize_token_embeddings(len(tokenizer))\n\n            if num_new_tokens > 0:\n                input_embeddings = self.get_input_embeddings().weight.data\n                output_embeddings = self.get_output_embeddings().weight.data\n\n                input_embeddings_avg = input_embeddings[:-num_new_tokens].mean(dim=0, keepdim=True)\n                output_embeddings_avg = output_embeddings[:-num_new_tokens].mean(dim=0, keepdim=True)\n\n                input_embeddings[-num_new_tokens:] = input_embeddings_avg\n                output_embeddings[-num_new_tokens:] = output_embeddings_avg\n\n            if model_args.tune_mm_mlp_adapter:\n                for p in self.get_input_embeddings().parameters():\n                    p.requires_grad = True\n                for p in self.get_output_embeddings().parameters():\n                    p.requires_grad = False\n\n            if model_args.pretrain_mm_mlp_adapter:\n                mm_projector_weights = torch.load(model_args.pretrain_mm_mlp_adapter, map_location=\"cpu\")\n                embed_tokens_weight = mm_projector_weights[\"model.embed_tokens.weight\"]\n                assert num_new_tokens == 2\n                if input_embeddings.shape == embed_tokens_weight.shape:\n                    input_embeddings[-num_new_tokens:] = embed_tokens_weight[-num_new_tokens:]\n                elif embed_tokens_weight.shape[0] == num_new_tokens:\n                    input_embeddings[-num_new_tokens:] = embed_tokens_weight\n                else:\n                    raise ValueError(f\"Unexpected embed_tokens_weight shape. Pretrained: {embed_tokens_weight.shape}. Current: {input_embeddings.shape}. Numer of new tokens: {num_new_tokens}.\")\n        elif model_args.mm_use_im_patch_token:\n            if model_args.tune_mm_mlp_adapter:\n                for p in self.get_input_embeddings().parameters():\n                    p.requires_grad = False\n                for p in self.get_output_embeddings().parameters():\n                    p.requires_grad = False\n"}
{"type": "source_file", "path": "llava/model/language_model/llava_mpt.py", "content": "#    Copyright 2023 Haotian Liu\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\");\n#    you may not use this file except in compliance with the License.\n#    You may obtain a copy of the License at\n#\n#        http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS,\n#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#    See the License for the specific language governing permissions and\n#    limitations under the License.\n\n\nfrom typing import Optional, Tuple\n\nimport torch\n\nfrom transformers import AutoConfig, AutoModelForCausalLM, MptConfig, MptForCausalLM, MptModel, GenerationConfig\nfrom llava.model.llava_arch import LlavaMetaModel, LlavaMetaForCausalLM\n\n\nclass LlavaMptConfig(MptConfig):\n    model_type = \"llava_mpt\"\n\n\nclass LlavaMptModel(LlavaMetaModel, MptModel):\n    config_class = LlavaMptConfig\n\n    def __init__(self, config: MptConfig):\n        config.hidden_size = config.d_model\n        super(LlavaMptModel, self).__init__(config)\n\n    def embed_tokens(self, x):\n        return self.wte(x)\n\n\nclass LlavaMptForCausalLM(MptForCausalLM, LlavaMetaForCausalLM):\n    config_class = LlavaMptConfig\n    supports_gradient_checkpointing = True\n\n    def __init__(self, config):\n        super(MptForCausalLM, self).__init__(config)\n\n        config.model_type = \"llava_mpt\"\n        config.rope_scaling = None\n        self.generation_config = GenerationConfig(\n            temperature=0.0,\n            max_new_tokens=1024,\n            do_sample=False,\n            top_p=None,\n        )\n\n        self.transformer = LlavaMptModel(config)\n        self.lm_head = torch.nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_model(self):\n        return self.transformer\n\n    def _set_gradient_checkpointing(self, module, value=False):\n        if isinstance(module, LlavaMptModel):\n            module.gradient_checkpointing = value\n\n    def forward(\n        self,\n        input_ids: Optional[torch.LongTensor] = None,\n        past_key_values: Optional[Tuple[Tuple[torch.Tensor, torch.Tensor], ...]] = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        inputs_embeds: Optional[torch.Tensor] = None,\n        labels: Optional[torch.Tensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n        cache_position=None,\n        images=None,\n    ):\n\n        input_ids, attention_mask, past_key_values, inputs_embeds, labels = self.prepare_inputs_labels_for_multimodal(input_ids, attention_mask, past_key_values, labels, images)\n\n        return super().forward(\n            input_ids,\n            past_key_values=past_key_values,\n            attention_mask=attention_mask,\n            inputs_embeds=inputs_embeds,\n            labels=labels,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n    def prepare_inputs_for_generation(self, input_ids, past_key_values=None, inputs_embeds=None, **kwargs):\n        images = kwargs.pop(\"images\", None)\n        _inputs = super().prepare_inputs_for_generation(input_ids, past_key_values=past_key_values, inputs_embeds=inputs_embeds, **kwargs)\n        _inputs[\"images\"] = images\n        return _inputs\n\n\nAutoConfig.register(\"llava_mpt\", LlavaMptConfig)\nAutoModelForCausalLM.register(LlavaMptConfig, LlavaMptForCausalLM)\n"}
{"type": "source_file", "path": "llava/model/multimodal_encoder/dev_eva_clip/eva_clip/loss.py", "content": "import math\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\n\ntry:\n    import torch.distributed.nn\n    from torch import distributed as dist\n\n    has_distributed = True\nexcept ImportError:\n    has_distributed = False\n\ntry:\n    import horovod.torch as hvd\nexcept ImportError:\n    hvd = None\n\nfrom timm.loss import LabelSmoothingCrossEntropy\n\n\ndef gather_features(image_features, text_features, local_loss=False, gather_with_grad=False, rank=0, world_size=1, use_horovod=False):\n    assert has_distributed, \"torch.distributed did not import correctly, please use a PyTorch version with support.\"\n    if use_horovod:\n        assert hvd is not None, \"Please install horovod\"\n        if gather_with_grad:\n            all_image_features = hvd.allgather(image_features)\n            all_text_features = hvd.allgather(text_features)\n        else:\n            with torch.no_grad():\n                all_image_features = hvd.allgather(image_features)\n                all_text_features = hvd.allgather(text_features)\n            if not local_loss:\n                # ensure grads for local rank when all_* features don't have a gradient\n                gathered_image_features = list(all_image_features.chunk(world_size, dim=0))\n                gathered_text_features = list(all_text_features.chunk(world_size, dim=0))\n                gathered_image_features[rank] = image_features\n                gathered_text_features[rank] = text_features\n                all_image_features = torch.cat(gathered_image_features, dim=0)\n                all_text_features = torch.cat(gathered_text_features, dim=0)\n    else:\n        # We gather tensors from all gpus\n        if gather_with_grad:\n            all_image_features = torch.cat(torch.distributed.nn.all_gather(image_features), dim=0)\n            all_text_features = torch.cat(torch.distributed.nn.all_gather(text_features), dim=0)\n            # all_image_features = torch.cat(torch.distributed.nn.all_gather(image_features, async_op=True), dim=0)\n            # all_text_features = torch.cat(torch.distributed.nn.all_gather(text_features, async_op=True), dim=0)\n        else:\n            gathered_image_features = [torch.zeros_like(image_features) for _ in range(world_size)]\n            gathered_text_features = [torch.zeros_like(text_features) for _ in range(world_size)]\n            dist.all_gather(gathered_image_features, image_features)\n            dist.all_gather(gathered_text_features, text_features)\n            if not local_loss:\n                # ensure grads for local rank when all_* features don't have a gradient\n                gathered_image_features[rank] = image_features\n                gathered_text_features[rank] = text_features\n            all_image_features = torch.cat(gathered_image_features, dim=0)\n            all_text_features = torch.cat(gathered_text_features, dim=0)\n\n    return all_image_features, all_text_features\n\n\nclass ClipLoss(nn.Module):\n\n    def __init__(\n        self,\n        local_loss=False,\n        gather_with_grad=False,\n        cache_labels=False,\n        rank=0,\n        world_size=1,\n        use_horovod=False,\n        smoothing=0.0,\n    ):\n        super().__init__()\n        self.local_loss = local_loss\n        self.gather_with_grad = gather_with_grad\n        self.cache_labels = cache_labels\n        self.rank = rank\n        self.world_size = world_size\n        self.use_horovod = use_horovod\n        self.label_smoothing_cross_entropy = LabelSmoothingCrossEntropy(smoothing=smoothing) if smoothing > 0 else None\n\n        # cache state\n        self.prev_num_logits = 0\n        self.labels = {}\n\n    def forward(self, image_features, text_features, logit_scale=1.0):\n        device = image_features.device\n        if self.world_size > 1:\n            all_image_features, all_text_features = gather_features(image_features, text_features, self.local_loss, self.gather_with_grad, self.rank, self.world_size, self.use_horovod)\n\n            if self.local_loss:\n                logits_per_image = logit_scale * image_features @ all_text_features.T\n                logits_per_text = logit_scale * text_features @ all_image_features.T\n            else:\n                logits_per_image = logit_scale * all_image_features @ all_text_features.T\n                logits_per_text = logits_per_image.T\n        else:\n            logits_per_image = logit_scale * image_features @ text_features.T\n            logits_per_text = logit_scale * text_features @ image_features.T\n        # calculated ground-truth and cache if enabled\n        num_logits = logits_per_image.shape[0]\n        if self.prev_num_logits != num_logits or device not in self.labels:\n            labels = torch.arange(num_logits, device=device, dtype=torch.long)\n            if self.world_size > 1 and self.local_loss:\n                labels = labels + num_logits * self.rank\n            if self.cache_labels:\n                self.labels[device] = labels\n                self.prev_num_logits = num_logits\n        else:\n            labels = self.labels[device]\n\n        if self.label_smoothing_cross_entropy:\n            total_loss = (self.label_smoothing_cross_entropy(logits_per_image, labels) + self.label_smoothing_cross_entropy(logits_per_text, labels)) / 2\n        else:\n            total_loss = (F.cross_entropy(logits_per_image, labels) + F.cross_entropy(logits_per_text, labels)) / 2\n\n        acc = None\n        i2t_acc = (logits_per_image.argmax(-1) == labels).sum() / len(logits_per_image)\n        t2i_acc = (logits_per_text.argmax(-1) == labels).sum() / len(logits_per_text)\n        acc = {\"i2t\": i2t_acc, \"t2i\": t2i_acc}\n        return total_loss, acc\n"}
{"type": "source_file", "path": "llava/model/multimodal_encoder/dev_eva_clip/eva_clip/utils.py", "content": "from itertools import repeat\nimport collections.abc\nimport logging\nimport math\nimport numpy as np\n\nimport torch\nfrom torch import nn as nn\nfrom torchvision.ops.misc import FrozenBatchNorm2d\nimport torch.nn.functional as F\n\n\n# open CLIP\ndef resize_clip_pos_embed(state_dict, model, interpolation: str = \"bicubic\", seq_dim=1):\n    # Rescale the grid of position embeddings when loading from state_dict\n    old_pos_embed = state_dict.get(\"visual.positional_embedding\", None)\n    if old_pos_embed is None or not hasattr(model.visual, \"grid_size\"):\n        return\n    grid_size = to_2tuple(model.visual.grid_size)\n    extra_tokens = 1  # FIXME detect different token configs (ie no class token, or more)\n    new_seq_len = grid_size[0] * grid_size[1] + extra_tokens\n    if new_seq_len == old_pos_embed.shape[0]:\n        return\n\n    if extra_tokens:\n        pos_emb_tok, pos_emb_img = old_pos_embed[:extra_tokens], old_pos_embed[extra_tokens:]\n    else:\n        pos_emb_tok, pos_emb_img = None, old_pos_embed\n    old_grid_size = to_2tuple(int(math.sqrt(len(pos_emb_img))))\n\n    logging.info(\"Resizing position embedding grid-size from %s to %s\", old_grid_size, grid_size)\n    pos_emb_img = pos_emb_img.reshape(1, old_grid_size[0], old_grid_size[1], -1).permute(0, 3, 1, 2)\n    pos_emb_img = F.interpolate(\n        pos_emb_img,\n        size=grid_size,\n        mode=interpolation,\n        align_corners=True,\n    )\n    pos_emb_img = pos_emb_img.permute(0, 2, 3, 1).reshape(1, grid_size[0] * grid_size[1], -1)[0]\n    if pos_emb_tok is not None:\n        new_pos_embed = torch.cat([pos_emb_tok, pos_emb_img], dim=0)\n    else:\n        new_pos_embed = pos_emb_img\n    state_dict[\"visual.positional_embedding\"] = new_pos_embed\n\n\ndef resize_visual_pos_embed(state_dict, model, interpolation: str = \"bicubic\", seq_dim=1):\n    # Rescale the grid of position embeddings when loading from state_dict\n    old_pos_embed = state_dict.get(\"positional_embedding\", None)\n    if old_pos_embed is None or not hasattr(model.visual, \"grid_size\"):\n        return\n    grid_size = to_2tuple(model.visual.grid_size)\n    extra_tokens = 1  # FIXME detect different token configs (ie no class token, or more)\n    new_seq_len = grid_size[0] * grid_size[1] + extra_tokens\n    if new_seq_len == old_pos_embed.shape[0]:\n        return\n\n    if extra_tokens:\n        pos_emb_tok, pos_emb_img = old_pos_embed[:extra_tokens], old_pos_embed[extra_tokens:]\n    else:\n        pos_emb_tok, pos_emb_img = None, old_pos_embed\n    old_grid_size = to_2tuple(int(math.sqrt(len(pos_emb_img))))\n\n    logging.info(\"Resizing position embedding grid-size from %s to %s\", old_grid_size, grid_size)\n    pos_emb_img = pos_emb_img.reshape(1, old_grid_size[0], old_grid_size[1], -1).permute(0, 3, 1, 2)\n    pos_emb_img = F.interpolate(\n        pos_emb_img,\n        size=grid_size,\n        mode=interpolation,\n        align_corners=True,\n    )\n    pos_emb_img = pos_emb_img.permute(0, 2, 3, 1).reshape(1, grid_size[0] * grid_size[1], -1)[0]\n    if pos_emb_tok is not None:\n        new_pos_embed = torch.cat([pos_emb_tok, pos_emb_img], dim=0)\n    else:\n        new_pos_embed = pos_emb_img\n    state_dict[\"positional_embedding\"] = new_pos_embed\n\n\ndef resize_evaclip_pos_embed(state_dict, model, interpolation: str = \"bicubic\", seq_dim=1):\n    all_keys = list(state_dict.keys())\n    # interpolate position embedding\n    if \"visual.pos_embed\" in state_dict:\n        pos_embed_checkpoint = state_dict[\"visual.pos_embed\"]\n        embedding_size = pos_embed_checkpoint.shape[-1]\n        num_patches = model.visual.patch_embed.num_patches\n        # num_extra_tokens = model.visual.pos_embed.shape[-2] - num_patches\n        num_extra_tokens = 1  # FIXME detect different token configs (ie no class token, or more)\n        # height (== width) for the checkpoint position embedding\n        orig_size = int((pos_embed_checkpoint.shape[-2] - num_extra_tokens) ** 0.5)\n        # height (== width) for the new position embedding\n        new_size = int(num_patches**0.5)\n        # class_token and dist_token are kept unchanged\n        if orig_size != new_size:\n            print(\"Position interpolate from %dx%d to %dx%d\" % (orig_size, orig_size, new_size, new_size))\n            extra_tokens = pos_embed_checkpoint[:, :num_extra_tokens]\n            # only the position tokens are interpolated\n            pos_tokens = pos_embed_checkpoint[:, num_extra_tokens:]\n            pos_tokens = pos_tokens.reshape(-1, orig_size, orig_size, embedding_size).permute(0, 3, 1, 2)\n            pos_tokens = torch.nn.functional.interpolate(pos_tokens, size=(new_size, new_size), mode=\"bicubic\", align_corners=False)\n            pos_tokens = pos_tokens.permute(0, 2, 3, 1).flatten(1, 2)\n            new_pos_embed = torch.cat((extra_tokens, pos_tokens), dim=1)\n            state_dict[\"visual.pos_embed\"] = new_pos_embed\n\n            patch_embed_proj = state_dict[\"visual.patch_embed.proj.weight\"]\n            patch_size = model.visual.patch_embed.patch_size\n            state_dict[\"visual.patch_embed.proj.weight\"] = torch.nn.functional.interpolate(patch_embed_proj.float(), size=patch_size, mode=\"bicubic\", align_corners=False)\n\n\ndef resize_eva_pos_embed(state_dict, model, interpolation: str = \"bicubic\", seq_dim=1):\n    all_keys = list(state_dict.keys())\n    # interpolate position embedding\n    if \"pos_embed\" in state_dict:\n        pos_embed_checkpoint = state_dict[\"pos_embed\"]\n        embedding_size = pos_embed_checkpoint.shape[-1]\n        num_patches = model.visual.patch_embed.num_patches\n        # num_extra_tokens = model.visual.pos_embed.shape[-2] - num_patches\n        num_extra_tokens = 1  # FIXME detect different token configs (ie no class token, or more)\n        # height (== width) for the checkpoint position embedding\n        orig_size = int((pos_embed_checkpoint.shape[-2] - num_extra_tokens) ** 0.5)\n        # height (== width) for the new position embedding\n        new_size = int(num_patches**0.5)\n        # class_token and dist_token are kept unchanged\n        if orig_size != new_size:\n            print(\"Position interpolate from %dx%d to %dx%d\" % (orig_size, orig_size, new_size, new_size))\n            extra_tokens = pos_embed_checkpoint[:, :num_extra_tokens]\n            # only the position tokens are interpolated\n            pos_tokens = pos_embed_checkpoint[:, num_extra_tokens:]\n            pos_tokens = pos_tokens.reshape(-1, orig_size, orig_size, embedding_size).permute(0, 3, 1, 2)\n            pos_tokens = torch.nn.functional.interpolate(pos_tokens, size=(new_size, new_size), mode=\"bicubic\", align_corners=False)\n            pos_tokens = pos_tokens.permute(0, 2, 3, 1).flatten(1, 2)\n            new_pos_embed = torch.cat((extra_tokens, pos_tokens), dim=1)\n            state_dict[\"pos_embed\"] = new_pos_embed\n\n            patch_embed_proj = state_dict[\"patch_embed.proj.weight\"]\n            patch_size = model.visual.patch_embed.patch_size\n            state_dict[\"patch_embed.proj.weight\"] = torch.nn.functional.interpolate(patch_embed_proj.float(), size=patch_size, mode=\"bicubic\", align_corners=False)\n\n\ndef resize_rel_pos_embed(state_dict, model, interpolation: str = \"bicubic\", seq_dim=1):\n    all_keys = list(state_dict.keys())\n    for key in all_keys:\n        if \"relative_position_index\" in key:\n            state_dict.pop(key)\n\n        if \"relative_position_bias_table\" in key:\n            rel_pos_bias = state_dict[key]\n            src_num_pos, num_attn_heads = rel_pos_bias.size()\n            dst_num_pos, _ = model.visual.state_dict()[key].size()\n            dst_patch_shape = model.visual.patch_embed.patch_shape\n            if dst_patch_shape[0] != dst_patch_shape[1]:\n                raise NotImplementedError()\n            num_extra_tokens = dst_num_pos - (dst_patch_shape[0] * 2 - 1) * (dst_patch_shape[1] * 2 - 1)\n            src_size = int((src_num_pos - num_extra_tokens) ** 0.5)\n            dst_size = int((dst_num_pos - num_extra_tokens) ** 0.5)\n            if src_size != dst_size:\n                print(\"Position interpolate for %s from %dx%d to %dx%d\" % (key, src_size, src_size, dst_size, dst_size))\n                extra_tokens = rel_pos_bias[-num_extra_tokens:, :]\n                rel_pos_bias = rel_pos_bias[:-num_extra_tokens, :]\n\n                def geometric_progression(a, r, n):\n                    return a * (1.0 - r**n) / (1.0 - r)\n\n                left, right = 1.01, 1.5\n                while right - left > 1e-6:\n                    q = (left + right) / 2.0\n                    gp = geometric_progression(1, q, src_size // 2)\n                    if gp > dst_size // 2:\n                        right = q\n                    else:\n                        left = q\n\n                # if q > 1.090307:\n                #     q = 1.090307\n\n                dis = []\n                cur = 1\n                for i in range(src_size // 2):\n                    dis.append(cur)\n                    cur += q ** (i + 1)\n\n                r_ids = [-_ for _ in reversed(dis)]\n\n                x = r_ids + [0] + dis\n                y = r_ids + [0] + dis\n\n                t = dst_size // 2.0\n                dx = np.arange(-t, t + 0.1, 1.0)\n                dy = np.arange(-t, t + 0.1, 1.0)\n\n                print(\"Original positions = %s\" % str(x))\n                print(\"Target positions = %s\" % str(dx))\n\n                all_rel_pos_bias = []\n\n                for i in range(num_attn_heads):\n                    z = rel_pos_bias[:, i].view(src_size, src_size).float().numpy()\n                    f = F.interpolate.interp2d(x, y, z, kind=\"cubic\")\n                    all_rel_pos_bias.append(torch.Tensor(f(dx, dy)).contiguous().view(-1, 1).to(rel_pos_bias.device))\n\n                rel_pos_bias = torch.cat(all_rel_pos_bias, dim=-1)\n\n                new_rel_pos_bias = torch.cat((rel_pos_bias, extra_tokens), dim=0)\n                state_dict[key] = new_rel_pos_bias\n\n    # interpolate position embedding\n    if \"pos_embed\" in state_dict:\n        pos_embed_checkpoint = state_dict[\"pos_embed\"]\n        embedding_size = pos_embed_checkpoint.shape[-1]\n        num_patches = model.visual.patch_embed.num_patches\n        num_extra_tokens = model.visual.pos_embed.shape[-2] - num_patches\n        # height (== width) for the checkpoint position embedding\n        orig_size = int((pos_embed_checkpoint.shape[-2] - num_extra_tokens) ** 0.5)\n        # height (== width) for the new position embedding\n        new_size = int(num_patches**0.5)\n        # class_token and dist_token are kept unchanged\n        if orig_size != new_size:\n            print(\"Position interpolate from %dx%d to %dx%d\" % (orig_size, orig_size, new_size, new_size))\n            extra_tokens = pos_embed_checkpoint[:, :num_extra_tokens]\n            # only the position tokens are interpolated\n            pos_tokens = pos_embed_checkpoint[:, num_extra_tokens:]\n            pos_tokens = pos_tokens.reshape(-1, orig_size, orig_size, embedding_size).permute(0, 3, 1, 2)\n            pos_tokens = torch.nn.functional.interpolate(pos_tokens, size=(new_size, new_size), mode=\"bicubic\", align_corners=False)\n            pos_tokens = pos_tokens.permute(0, 2, 3, 1).flatten(1, 2)\n            new_pos_embed = torch.cat((extra_tokens, pos_tokens), dim=1)\n            state_dict[\"pos_embed\"] = new_pos_embed\n\n            patch_embed_proj = state_dict[\"patch_embed.proj.weight\"]\n            patch_size = model.visual.patch_embed.patch_size\n            state_dict[\"patch_embed.proj.weight\"] = torch.nn.functional.interpolate(patch_embed_proj.float(), size=patch_size, mode=\"bicubic\", align_corners=False)\n\n\ndef freeze_batch_norm_2d(module, module_match={}, name=\"\"):\n    \"\"\"\n    Converts all `BatchNorm2d` and `SyncBatchNorm` layers of provided module into `FrozenBatchNorm2d`. If `module` is\n    itself an instance of either `BatchNorm2d` or `SyncBatchNorm`, it is converted into `FrozenBatchNorm2d` and\n    returned. Otherwise, the module is walked recursively and submodules are converted in place.\n\n    Args:\n        module (torch.nn.Module): Any PyTorch module.\n        module_match (dict): Dictionary of full module names to freeze (all if empty)\n        name (str): Full module name (prefix)\n\n    Returns:\n        torch.nn.Module: Resulting module\n\n    Inspired by https://github.com/pytorch/pytorch/blob/a5895f85be0f10212791145bfedc0261d364f103/torch/nn/modules/batchnorm.py#L762\n    \"\"\"\n    res = module\n    is_match = True\n    if module_match:\n        is_match = name in module_match\n    if is_match and isinstance(module, (nn.modules.batchnorm.BatchNorm2d, nn.modules.batchnorm.SyncBatchNorm)):\n        res = FrozenBatchNorm2d(module.num_features)\n        res.num_features = module.num_features\n        res.affine = module.affine\n        if module.affine:\n            res.weight.data = module.weight.data.clone().detach()\n            res.bias.data = module.bias.data.clone().detach()\n        res.running_mean.data = module.running_mean.data\n        res.running_var.data = module.running_var.data\n        res.eps = module.eps\n    else:\n        for child_name, child in module.named_children():\n            full_child_name = \".\".join([name, child_name]) if name else child_name\n            new_child = freeze_batch_norm_2d(child, module_match, full_child_name)\n            if new_child is not child:\n                res.add_module(child_name, new_child)\n    return res\n\n\n# From PyTorch internals\ndef _ntuple(n):\n    def parse(x):\n        if isinstance(x, collections.abc.Iterable):\n            return x\n        return tuple(repeat(x, n))\n\n    return parse\n\n\nto_1tuple = _ntuple(1)\nto_2tuple = _ntuple(2)\nto_3tuple = _ntuple(3)\nto_4tuple = _ntuple(4)\nto_ntuple = lambda n, x: _ntuple(n)(x)\n\n\ndef is_logging(args):\n    def is_global_master(args):\n        return args.rank == 0\n\n    def is_local_master(args):\n        return args.local_rank == 0\n\n    def is_master(args, local=False):\n        return is_local_master(args) if local else is_global_master(args)\n\n    return is_master\n\n\nclass AllGather(torch.autograd.Function):\n    \"\"\"An autograd function that performs allgather on a tensor.\n    Performs all_gather operation on the provided tensors.\n    *** Warning ***: torch.distributed.all_gather has no gradient.\n    \"\"\"\n\n    @staticmethod\n    def forward(ctx, tensor, rank, world_size):\n        tensors_gather = [torch.empty_like(tensor) for _ in range(world_size)]\n        torch.distributed.all_gather(tensors_gather, tensor)\n        ctx.rank = rank\n        ctx.batch_size = tensor.shape[0]\n        return torch.cat(tensors_gather, 0)\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        return (grad_output[ctx.batch_size * ctx.rank : ctx.batch_size * (ctx.rank + 1)], None, None)\n\n\nallgather = AllGather.apply\n"}
{"type": "source_file", "path": "llava/model/multimodal_encoder/dev_eva_clip/eva_clip/model.py", "content": "\"\"\" CLIP Model\n\nAdapted from https://github.com/openai/CLIP. Originally MIT License, Copyright (c) 2021 OpenAI.\n\"\"\"\n\nimport os\nfrom dataclasses import dataclass\nfrom typing import Optional, Tuple, Union\nfrom functools import partial\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\n\ntry:\n    from .hf_model import HFTextEncoder\nexcept:\n    HFTextEncoder = None\nfrom .modified_resnet import ModifiedResNet\nfrom .timm_model import TimmModel\nfrom .eva_vit_model import EVAVisionTransformer\nfrom .transformer import LayerNorm, QuickGELU, Attention, VisionTransformer, TextTransformer\n\ntry:\n    from apex.normalization import FusedLayerNorm\nexcept:\n    FusedLayerNorm = LayerNorm\n    # print(\"Please 'pip install apex'\")\n\ntry:\n    import xformers.ops as xops\nexcept ImportError:\n    xops = None\n    # print(\"Please 'pip install xformers'\")\n\n\nclass RMSnorm(nn.Module):\n    \"\"\"\n    adepted from transformers T5LayerNorm\n    \"\"\"\n\n    def __init__(self, hidden_size, eps=1e-6):\n        \"\"\"\n        Construct a layernorm module in the T5 style. No bias and no subtraction of mean.\n        \"\"\"\n        super().__init__()\n        self.weight = nn.Parameter(torch.ones(hidden_size))\n        self.variance_epsilon = eps\n\n    def forward(self, hidden_states):\n        # T5 uses a layer_norm which only scales and doesn't shift, which is also known as Root Mean\n        # Square Layer Normalization https://arxiv.org/abs/1910.07467 thus varience is calculated\n        # w/o mean and there is no bias. Additionally we want to make sure that the accumulation for\n        # half-precision inputs is done in fp32\n\n        variance = hidden_states.to(torch.float32).pow(2).mean(-1, keepdim=True)\n        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n\n        # convert into half-precision if necessary\n        if self.weight.dtype in [torch.float16, torch.bfloat16]:\n            hidden_states = hidden_states.to(self.weight.dtype)\n\n        return self.weight * hidden_states\n\n\n@dataclass\nclass CLIPVisionCfg:\n    layers: Union[Tuple[int, int, int, int], int] = 12\n    width: int = 768\n    head_width: int = 64\n    mlp_ratio: float = 4.0\n    patch_size: int = 16\n    image_size: Union[Tuple[int, int], int] = 224\n    ls_init_value: Optional[float] = None  # layer scale initial value\n    patch_dropout: float = 0.0  # what fraction of patches to dropout during training (0 would mean disabled and no patches dropped) - 0.5 to 0.75 recommended in the paper for optimal results\n    global_average_pool: bool = False  # whether to global average pool the last embedding layer, instead of using CLS token (https://arxiv.org/abs/2205.01580)\n    drop_path_rate: Optional[float] = None  # drop path rate\n    timm_model_name: str = None  # a valid model name overrides layers, width, patch_size\n    timm_model_pretrained: bool = False  # use (imagenet) pretrained weights for named model\n    timm_pool: str = \"avg\"  # feature pooling for timm model ('abs_attn', 'rot_attn', 'avg', '')\n    timm_proj: str = \"linear\"  # linear projection for timm model output ('linear', 'mlp', '')\n    timm_proj_bias: bool = False  # enable bias final projection\n    eva_model_name: str = None  # a valid eva model name overrides layers, width, patch_size\n    qkv_bias: bool = True\n    fusedLN: bool = False\n    xattn: bool = False\n    postnorm: bool = False\n    rope: bool = False\n    pt_hw_seq_len: int = 16  # 224/14\n    intp_freq: bool = False\n    naiveswiglu: bool = False\n    subln: bool = False\n    use_rms_norm: bool = False\n\n\n@dataclass\nclass CLIPTextCfg:\n    context_length: int = 77\n    vocab_size: int = 49408\n    width: int = 512\n    heads: int = 8\n    layers: int = 12\n    ls_init_value: Optional[float] = None  # layer scale initial value\n    hf_model_name: str = None\n    hf_tokenizer_name: str = None\n    hf_model_pretrained: bool = True\n    proj: str = \"mlp\"\n    pooler_type: str = \"mean_pooler\"\n    masked_language_modeling: bool = False\n    fusedLN: bool = False\n    xattn: bool = False\n    attn_mask: bool = True\n\n\ndef get_cast_dtype(precision: str):\n    cast_dtype = None\n    if precision == \"bf16\":\n        cast_dtype = torch.bfloat16\n    elif precision == \"fp16\":\n        cast_dtype = torch.float16\n    return cast_dtype\n\n\ndef _build_vision_tower(embed_dim: int, vision_cfg: CLIPVisionCfg, quick_gelu: bool = False, cast_dtype: Optional[torch.dtype] = None):\n    if isinstance(vision_cfg, dict):\n        vision_cfg = CLIPVisionCfg(**vision_cfg)\n\n    # OpenAI models are pretrained w/ QuickGELU but native nn.GELU is both faster and more\n    # memory efficient in recent PyTorch releases (>= 1.10).\n    # NOTE: timm models always use native GELU regardless of quick_gelu flag.\n    act_layer = QuickGELU if quick_gelu else nn.GELU\n\n    if vision_cfg.eva_model_name:\n        vision_heads = vision_cfg.width // vision_cfg.head_width\n\n        norm_layer = RMSnorm if vision_cfg.use_rms_norm else LayerNorm\n\n        visual = EVAVisionTransformer(\n            img_size=vision_cfg.image_size,\n            patch_size=vision_cfg.patch_size,\n            num_classes=embed_dim,\n            use_mean_pooling=vision_cfg.global_average_pool,  # False\n            init_values=vision_cfg.ls_init_value,\n            patch_dropout=vision_cfg.patch_dropout,\n            embed_dim=vision_cfg.width,\n            depth=vision_cfg.layers,\n            num_heads=vision_heads,\n            mlp_ratio=vision_cfg.mlp_ratio,\n            qkv_bias=vision_cfg.qkv_bias,\n            drop_path_rate=vision_cfg.drop_path_rate,\n            norm_layer=partial(norm_layer, eps=1e-6),\n            xattn=vision_cfg.xattn,\n            rope=vision_cfg.rope,\n            postnorm=vision_cfg.postnorm,\n            pt_hw_seq_len=vision_cfg.pt_hw_seq_len,  # 224/14\n            intp_freq=vision_cfg.intp_freq,\n            naiveswiglu=vision_cfg.naiveswiglu,\n            subln=vision_cfg.subln,\n        )\n    elif vision_cfg.timm_model_name:\n        visual = TimmModel(\n            vision_cfg.timm_model_name, pretrained=vision_cfg.timm_model_pretrained, pool=vision_cfg.timm_pool, proj=vision_cfg.timm_proj, proj_bias=vision_cfg.timm_proj_bias, embed_dim=embed_dim, image_size=vision_cfg.image_size\n        )\n        act_layer = nn.GELU  # so that text transformer doesn't use QuickGELU w/ timm models\n    elif isinstance(vision_cfg.layers, (tuple, list)):\n        vision_heads = vision_cfg.width * 32 // vision_cfg.head_width\n        visual = ModifiedResNet(layers=vision_cfg.layers, output_dim=embed_dim, heads=vision_heads, image_size=vision_cfg.image_size, width=vision_cfg.width)\n    else:\n        vision_heads = vision_cfg.width // vision_cfg.head_width\n        norm_layer = LayerNormFp32 if cast_dtype in (torch.float16, torch.bfloat16) else LayerNorm\n        visual = VisionTransformer(\n            image_size=vision_cfg.image_size,\n            patch_size=vision_cfg.patch_size,\n            width=vision_cfg.width,\n            layers=vision_cfg.layers,\n            heads=vision_heads,\n            mlp_ratio=vision_cfg.mlp_ratio,\n            ls_init_value=vision_cfg.ls_init_value,\n            patch_dropout=vision_cfg.patch_dropout,\n            global_average_pool=vision_cfg.global_average_pool,\n            output_dim=embed_dim,\n            act_layer=act_layer,\n            norm_layer=norm_layer,\n        )\n\n    return visual\n\n\ndef _build_text_tower(\n    embed_dim: int,\n    text_cfg: CLIPTextCfg,\n    quick_gelu: bool = False,\n    cast_dtype: Optional[torch.dtype] = None,\n):\n    if isinstance(text_cfg, dict):\n        text_cfg = CLIPTextCfg(**text_cfg)\n\n    if text_cfg.hf_model_name:\n        text = HFTextEncoder(text_cfg.hf_model_name, output_dim=embed_dim, tokenizer_name=text_cfg.hf_tokenizer_name, proj=text_cfg.proj, pooler_type=text_cfg.pooler_type, masked_language_modeling=text_cfg.masked_language_modeling)\n    else:\n        act_layer = QuickGELU if quick_gelu else nn.GELU\n        norm_layer = LayerNorm\n\n        text = TextTransformer(\n            context_length=text_cfg.context_length,\n            vocab_size=text_cfg.vocab_size,\n            width=text_cfg.width,\n            heads=text_cfg.heads,\n            layers=text_cfg.layers,\n            ls_init_value=text_cfg.ls_init_value,\n            output_dim=embed_dim,\n            act_layer=act_layer,\n            norm_layer=FusedLayerNorm if text_cfg.fusedLN else norm_layer,\n            xattn=text_cfg.xattn,\n            attn_mask=text_cfg.attn_mask,\n        )\n    return text\n\n\nclass CLIP(nn.Module):\n    def __init__(\n        self,\n        embed_dim: int,\n        vision_cfg: CLIPVisionCfg,\n        text_cfg: CLIPTextCfg,\n        quick_gelu: bool = False,\n        cast_dtype: Optional[torch.dtype] = None,\n    ):\n        super().__init__()\n        self.visual = _build_vision_tower(embed_dim, vision_cfg, quick_gelu, cast_dtype)\n\n        text = _build_text_tower(embed_dim, text_cfg, quick_gelu, cast_dtype)\n        self.transformer = text.transformer\n        self.vocab_size = text.vocab_size\n        self.token_embedding = text.token_embedding\n        self.positional_embedding = text.positional_embedding\n        self.ln_final = text.ln_final\n        self.text_projection = text.text_projection\n        self.register_buffer(\"attn_mask\", text.attn_mask, persistent=False)\n\n        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))\n\n    def lock_image_tower(self, unlocked_groups=0, freeze_bn_stats=False):\n        # lock image tower as per LiT - https://arxiv.org/abs/2111.07991\n        self.visual.lock(unlocked_groups=unlocked_groups, freeze_bn_stats=freeze_bn_stats)\n\n    @torch.jit.ignore\n    def set_grad_checkpointing(self, enable=True):\n        self.visual.set_grad_checkpointing(enable)\n        self.transformer.grad_checkpointing = enable\n\n    @torch.jit.ignore\n    def no_weight_decay(self):\n        return {\"logit_scale\"}\n\n    def encode_image(self, image, normalize: bool = False):\n        features = self.visual(image)\n        return F.normalize(features, dim=-1) if normalize else features\n\n    def encode_text(self, text, normalize: bool = False):\n        cast_dtype = self.transformer.get_cast_dtype()\n\n        x = self.token_embedding(text).to(cast_dtype)  # [batch_size, n_ctx, d_model]\n\n        x = x + self.positional_embedding.to(cast_dtype)\n        x = x.permute(1, 0, 2)  # NLD -> LND\n        x = self.transformer(x, attn_mask=self.attn_mask)\n        x = x.permute(1, 0, 2)  # LND -> NLD\n        x = self.ln_final(x)  # [batch_size, n_ctx, transformer.width]\n        # take features from the eot embedding (eot_token is the highest number in each sequence)\n        x = x[torch.arange(x.shape[0]), text.argmax(dim=-1)] @ self.text_projection\n        return F.normalize(x, dim=-1) if normalize else x\n\n    def forward(self, image, text):\n        image_features = self.encode_image(image, normalize=True)\n        text_features = self.encode_text(text, normalize=True)\n        return image_features, text_features, self.logit_scale.exp()\n\n\nclass CustomCLIP(nn.Module):\n    def __init__(\n        self,\n        embed_dim: int,\n        vision_cfg: CLIPVisionCfg,\n        text_cfg: CLIPTextCfg,\n        quick_gelu: bool = False,\n        cast_dtype: Optional[torch.dtype] = None,\n        itm_task: bool = False,\n    ):\n        super().__init__()\n        self.visual = _build_vision_tower(embed_dim, vision_cfg, quick_gelu, cast_dtype)\n        self.text = _build_text_tower(embed_dim, text_cfg, quick_gelu, cast_dtype)\n        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))\n\n    def lock_image_tower(self, unlocked_groups=0, freeze_bn_stats=False):\n        # lock image tower as per LiT - https://arxiv.org/abs/2111.07991\n        self.visual.lock(unlocked_groups=unlocked_groups, freeze_bn_stats=freeze_bn_stats)\n\n    def lock_text_tower(self, unlocked_layers: int = 0, freeze_layer_norm: bool = True):\n        self.text.lock(unlocked_layers, freeze_layer_norm)\n\n    @torch.jit.ignore\n    def set_grad_checkpointing(self, enable=True):\n        self.visual.set_grad_checkpointing(enable)\n        self.text.set_grad_checkpointing(enable)\n\n    @torch.jit.ignore\n    def no_weight_decay(self):\n        return {\"logit_scale\"}\n\n    def encode_image(self, image, normalize: bool = False):\n        features = self.visual(image)\n        return F.normalize(features, dim=-1) if normalize else features\n\n    def encode_text(self, text, normalize: bool = False):\n        features = self.text(text)\n        return F.normalize(features, dim=-1) if normalize else features\n\n    def forward(self, image, text):\n        image_features = self.encode_image(image, normalize=True)\n        text_features = self.encode_text(text, normalize=True)\n        return image_features, text_features, self.logit_scale.exp()\n\n\ndef convert_weights_to_lp(model: nn.Module, dtype=torch.float16):\n    \"\"\"Convert applicable model parameters to low-precision (bf16 or fp16)\"\"\"\n\n    def _convert_weights(l):\n\n        if isinstance(l, (nn.Conv1d, nn.Conv2d, nn.Linear)):\n            l.weight.data = l.weight.data.to(dtype)\n            if l.bias is not None:\n                l.bias.data = l.bias.data.to(dtype)\n\n        if isinstance(l, (nn.MultiheadAttention, Attention)):\n            for attr in [*[f\"{s}_proj_weight\" for s in [\"in\", \"q\", \"k\", \"v\"]], \"in_proj_bias\", \"bias_k\", \"bias_v\"]:\n                tensor = getattr(l, attr, None)\n                if tensor is not None:\n                    tensor.data = tensor.data.to(dtype)\n\n        if isinstance(l, nn.Parameter):\n            l.data = l.data.to(dtype)\n\n        for name in [\"text_projection\", \"proj\"]:\n            if hasattr(l, name) and isinstance(l, nn.Parameter):\n                attr = getattr(l, name, None)\n                if attr is not None:\n                    attr.data = attr.data.to(dtype)\n\n    model.apply(_convert_weights)\n\n\nconvert_weights_to_fp16 = convert_weights_to_lp  # backwards compat\n\n\n# used to maintain checkpoint compatibility\ndef convert_to_custom_text_state_dict(state_dict: dict):\n    if \"text_projection\" in state_dict:\n        # old format state_dict, move text tower -> .text\n        new_state_dict = {}\n        for k, v in state_dict.items():\n            if any(k.startswith(p) for p in (\"text_projection\", \"positional_embedding\", \"token_embedding\", \"transformer\", \"ln_final\", \"logit_scale\")):\n                k = \"text.\" + k\n            new_state_dict[k] = v\n        return new_state_dict\n    return state_dict\n\n\ndef build_model_from_openai_state_dict(\n    state_dict: dict,\n    quick_gelu=True,\n    cast_dtype=torch.float16,\n):\n    vit = \"visual.proj\" in state_dict\n\n    if vit:\n        vision_width = state_dict[\"visual.conv1.weight\"].shape[0]\n        vision_layers = len([k for k in state_dict.keys() if k.startswith(\"visual.\") and k.endswith(\".attn.in_proj_weight\")])\n        vision_patch_size = state_dict[\"visual.conv1.weight\"].shape[-1]\n        grid_size = round((state_dict[\"visual.positional_embedding\"].shape[0] - 1) ** 0.5)\n        image_size = vision_patch_size * grid_size\n    else:\n        counts: list = [len(set(k.split(\".\")[2] for k in state_dict if k.startswith(f\"visual.layer{b}\"))) for b in [1, 2, 3, 4]]\n        vision_layers = tuple(counts)\n        vision_width = state_dict[\"visual.layer1.0.conv1.weight\"].shape[0]\n        output_width = round((state_dict[\"visual.attnpool.positional_embedding\"].shape[0] - 1) ** 0.5)\n        vision_patch_size = None\n        assert output_width**2 + 1 == state_dict[\"visual.attnpool.positional_embedding\"].shape[0]\n        image_size = output_width * 32\n\n    embed_dim = state_dict[\"text_projection\"].shape[1]\n    context_length = state_dict[\"positional_embedding\"].shape[0]\n    vocab_size = state_dict[\"token_embedding.weight\"].shape[0]\n    transformer_width = state_dict[\"ln_final.weight\"].shape[0]\n    transformer_heads = transformer_width // 64\n    transformer_layers = len(set(k.split(\".\")[2] for k in state_dict if k.startswith(f\"transformer.resblocks\")))\n\n    vision_cfg = CLIPVisionCfg(\n        layers=vision_layers,\n        width=vision_width,\n        patch_size=vision_patch_size,\n        image_size=image_size,\n    )\n    text_cfg = CLIPTextCfg(context_length=context_length, vocab_size=vocab_size, width=transformer_width, heads=transformer_heads, layers=transformer_layers)\n    model = CLIP(\n        embed_dim,\n        vision_cfg=vision_cfg,\n        text_cfg=text_cfg,\n        quick_gelu=quick_gelu,  # OpenAI models were trained with QuickGELU\n        cast_dtype=cast_dtype,\n    )\n\n    for key in [\"input_resolution\", \"context_length\", \"vocab_size\"]:\n        state_dict.pop(key, None)\n\n    convert_weights_to_fp16(model)  # OpenAI state dicts are partially converted to float16\n    model.load_state_dict(state_dict)\n    return model.eval()\n\n\ndef trace_model(model, batch_size=256, device=torch.device(\"cpu\")):\n    model.eval()\n    image_size = model.visual.image_size\n    example_images = torch.ones((batch_size, 3, image_size, image_size), device=device)\n    example_text = torch.zeros((batch_size, model.context_length), dtype=torch.int, device=device)\n    model = torch.jit.trace_module(model, inputs=dict(forward=(example_images, example_text), encode_text=(example_text,), encode_image=(example_images,)))\n    model.visual.image_size = image_size\n    return model\n"}
{"type": "source_file", "path": "llava/model/multimodal_encoder/dev_eva_clip/eva_clip/hf_configs.py", "content": "# HF architecture dict:\narch_dict = {\n    # https://huggingface.co/docs/transformers/model_doc/roberta#roberta\n    \"roberta\": {\n        \"config_names\": {\n            \"context_length\": \"max_position_embeddings\",\n            \"vocab_size\": \"vocab_size\",\n            \"width\": \"hidden_size\",\n            \"heads\": \"num_attention_heads\",\n            \"layers\": \"num_hidden_layers\",\n            \"layer_attr\": \"layer\",\n            \"token_embeddings_attr\": \"embeddings\",\n        },\n        \"pooler\": \"mean_pooler\",\n    },\n    # https://huggingface.co/docs/transformers/model_doc/xlm-roberta#transformers.XLMRobertaConfig\n    \"xlm-roberta\": {\n        \"config_names\": {\n            \"context_length\": \"max_position_embeddings\",\n            \"vocab_size\": \"vocab_size\",\n            \"width\": \"hidden_size\",\n            \"heads\": \"num_attention_heads\",\n            \"layers\": \"num_hidden_layers\",\n            \"layer_attr\": \"layer\",\n            \"token_embeddings_attr\": \"embeddings\",\n        },\n        \"pooler\": \"mean_pooler\",\n    },\n    # https://huggingface.co/docs/transformers/model_doc/mt5#mt5\n    \"mt5\": {\n        \"config_names\": {\n            # unlimited seqlen\n            # https://github.com/google-research/text-to-text-transfer-transformer/issues/273\n            # https://github.com/huggingface/transformers/blob/v4.24.0/src/transformers/models/t5/modeling_t5.py#L374\n            \"context_length\": \"\",\n            \"vocab_size\": \"vocab_size\",\n            \"width\": \"d_model\",\n            \"heads\": \"num_heads\",\n            \"layers\": \"num_layers\",\n            \"layer_attr\": \"block\",\n            \"token_embeddings_attr\": \"embed_tokens\",\n        },\n        \"pooler\": \"mean_pooler\",\n    },\n    \"bert\": {\n        \"config_names\": {\n            \"context_length\": \"max_position_embeddings\",\n            \"vocab_size\": \"vocab_size\",\n            \"width\": \"hidden_size\",\n            \"heads\": \"num_attention_heads\",\n            \"layers\": \"num_hidden_layers\",\n            \"layer_attr\": \"layer\",\n            \"token_embeddings_attr\": \"embeddings\",\n        },\n        \"pooler\": \"mean_pooler\",\n    },\n}\n"}
{"type": "source_file", "path": "llava/model/multimodal_encoder/dev_eva_clip/eva_clip/pretrained.py", "content": "import hashlib\nimport os\nimport urllib\nimport warnings\nfrom typing import Dict, Union\n\nfrom tqdm import tqdm\n\ntry:\n    from huggingface_hub import hf_hub_download\n\n    _has_hf_hub = True\nexcept ImportError:\n    hf_hub_download = None\n    _has_hf_hub = False\n\n\ndef _pcfg(url=\"\", hf_hub=\"\", filename=\"\", mean=None, std=None):\n    return dict(\n        url=url,\n        hf_hub=hf_hub,\n        mean=mean,\n        std=std,\n    )\n\n\n_VITB32 = dict(\n    openai=_pcfg(\"https://openaipublic.azureedge.net/clip/models/40d365715913c9da98579312b702a82c18be219cc2a73407c4526f58eba950af/ViT-B-32.pt\"),\n    laion400m_e31=_pcfg(\"https://github.com/mlfoundations/open_clip/releases/download/v0.2-weights/vit_b_32-quickgelu-laion400m_e31-d867053b.pt\"),\n    laion400m_e32=_pcfg(\"https://github.com/mlfoundations/open_clip/releases/download/v0.2-weights/vit_b_32-quickgelu-laion400m_e32-46683a32.pt\"),\n    laion2b_e16=_pcfg(\"https://github.com/mlfoundations/open_clip/releases/download/v0.2-weights/vit_b_32-laion2b_e16-af8dbd0c.pth\"),\n    laion2b_s34b_b79k=_pcfg(hf_hub=\"laion/CLIP-ViT-B-32-laion2B-s34B-b79K/\"),\n)\n\n_VITB32_quickgelu = dict(\n    openai=_pcfg(\"https://openaipublic.azureedge.net/clip/models/40d365715913c9da98579312b702a82c18be219cc2a73407c4526f58eba950af/ViT-B-32.pt\"),\n    laion400m_e31=_pcfg(\"https://github.com/mlfoundations/open_clip/releases/download/v0.2-weights/vit_b_32-quickgelu-laion400m_e31-d867053b.pt\"),\n    laion400m_e32=_pcfg(\"https://github.com/mlfoundations/open_clip/releases/download/v0.2-weights/vit_b_32-quickgelu-laion400m_e32-46683a32.pt\"),\n)\n\n_VITB16 = dict(\n    openai=_pcfg(\"https://openaipublic.azureedge.net/clip/models/5806e77cd80f8b59890b7e101eabd078d9fb84e6937f9e85e4ecb61988df416f/ViT-B-16.pt\"),\n    laion400m_e31=_pcfg(\"https://github.com/mlfoundations/open_clip/releases/download/v0.2-weights/vit_b_16-laion400m_e31-00efa78f.pt\"),\n    laion400m_e32=_pcfg(\"https://github.com/mlfoundations/open_clip/releases/download/v0.2-weights/vit_b_16-laion400m_e32-55e67d44.pt\"),\n    laion2b_s34b_b88k=_pcfg(hf_hub=\"laion/CLIP-ViT-B-16-laion2B-s34B-b88K/\"),\n)\n\n_EVAB16 = dict(\n    eva=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/EVA02_B_psz14to16.pt\"),\n    eva02=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/EVA02_B_psz14to16.pt\"),\n    eva_clip=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/EVA02_CLIP_B_psz16_s8B.pt\"),\n    eva02_clip=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/EVA02_CLIP_B_psz16_s8B.pt\"),\n)\n\n_VITB16_PLUS_240 = dict(\n    laion400m_e31=_pcfg(\"https://github.com/mlfoundations/open_clip/releases/download/v0.2-weights/vit_b_16_plus_240-laion400m_e31-8fb26589.pt\"),\n    laion400m_e32=_pcfg(\"https://github.com/mlfoundations/open_clip/releases/download/v0.2-weights/vit_b_16_plus_240-laion400m_e32-699c4b84.pt\"),\n)\n\n_VITL14 = dict(\n    openai=_pcfg(\"https://openaipublic.azureedge.net/clip/models/b8cca3fd41ae0c99ba7e8951adf17d267cdb84cd88be6f7c2e0eca1737a03836/ViT-L-14.pt\"),\n    laion400m_e31=_pcfg(\"https://github.com/mlfoundations/open_clip/releases/download/v0.2-weights/vit_l_14-laion400m_e31-69988bb6.pt\"),\n    laion400m_e32=_pcfg(\"https://github.com/mlfoundations/open_clip/releases/download/v0.2-weights/vit_l_14-laion400m_e32-3d133497.pt\"),\n    laion2b_s32b_b82k=_pcfg(hf_hub=\"laion/CLIP-ViT-L-14-laion2B-s32B-b82K/\", mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n)\n\n_EVAL14 = dict(\n    eva=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/EVA02_L_psz14.pt\"),\n    eva02=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/EVA02_L_psz14.pt\"),\n    eva_clip=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/EVA02_CLIP_L_psz14_s4B.pt\"),\n    eva02_clip=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/EVA02_CLIP_L_psz14_s4B.pt\"),\n)\n\n_VITL14_336 = dict(\n    openai=_pcfg(\"https://openaipublic.azureedge.net/clip/models/3035c92b350959924f9f00213499208652fc7ea050643e8b385c2dac08641f02/ViT-L-14-336px.pt\"),\n)\n\n_EVAL14_336 = dict(\n    eva_clip=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/EVA02_CLIP_L_336_psz14_s6B.pt\"),\n    eva02_clip=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/EVA02_CLIP_L_336_psz14_s6B.pt\"),\n    eva_clip_224to336=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/EVA02_CLIP_L_psz14_224to336.pt\"),\n    eva02_clip_224to336=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/EVA02_CLIP_L_psz14_224to336.pt\"),\n)\n\n_VITH14 = dict(\n    laion2b_s32b_b79k=_pcfg(hf_hub=\"laion/CLIP-ViT-H-14-laion2B-s32B-b79K/\"),\n)\n\n_VITg14 = dict(\n    laion2b_s12b_b42k=_pcfg(hf_hub=\"laion/CLIP-ViT-g-14-laion2B-s12B-b42K/\"),\n    laion2b_s34b_b88k=_pcfg(hf_hub=\"laion/CLIP-ViT-g-14-laion2B-s34B-b88K/\"),\n)\n\n_EVAg14 = dict(\n    eva=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/\"),\n    eva01=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/EVA01_g_psz14.pt\"),\n    eva_clip=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/EVA01_CLIP_g_14_psz14_s11B.pt\"),\n    eva01_clip=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/EVA01_CLIP_g_14_psz14_s11B.pt\"),\n)\n\n_EVAg14_PLUS = dict(\n    eva=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/\"),\n    eva01=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/EVA01_g_psz14.pt\"),\n    eva_clip=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/EVA01_CLIP_g_14_plus_psz14_s11B.pt\"),\n    eva01_clip=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/EVA01_CLIP_g_14_plus_psz14_s11B.pt\"),\n)\n\n_VITbigG14 = dict(\n    laion2b_s39b_b160k=_pcfg(hf_hub=\"laion/CLIP-ViT-bigG-14-laion2B-39B-b160k/\"),\n)\n\n_EVAbigE14 = dict(\n    eva=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/EVA02_E_psz14.pt\"),\n    eva02=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/EVA02_E_psz14.pt\"),\n    eva_clip=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/EVA02_CLIP_E_psz14_s4B.pt\"),\n    eva02_clip=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/EVA02_CLIP_E_psz14_s4B.pt\"),\n)\n\n_EVAbigE14_PLUS = dict(\n    eva=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/EVA02_E_psz14.pt\"),\n    eva02=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/EVA02_E_psz14.pt\"),\n    eva_clip=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/EVA02_CLIP_E_psz14_plus_s9B.pt\"),\n    eva02_clip=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/EVA02_CLIP_E_psz14_plus_s9B.pt\"),\n)\n\n_EVA_8B = dict(\n    eva=_pcfg(hf_hub=\"BAAI/EVA-CLIP-8B/EVA_8B_psz14.bin\"),\n    eva_clip=_pcfg(hf_hub=\"BAAI/EVA-CLIP-8B/EVA_CLIP_8B_psz14_s9B.pt\"),\n)\n\n_EVA_8B_PLUS = dict(\n    eva_clip=_pcfg(hf_hub=\"BAAI/EVA-CLIP-8B-448/EVA_CLIP_8B_psz14_plus_s0.6B.pt\"),\n)\n\n\n_PRETRAINED = {\n    # \"ViT-B-32\": _VITB32,\n    \"OpenaiCLIP-B-32\": _VITB32,\n    \"OpenCLIP-B-32\": _VITB32,\n    # \"ViT-B-32-quickgelu\": _VITB32_quickgelu,\n    \"OpenaiCLIP-B-32-quickgelu\": _VITB32_quickgelu,\n    \"OpenCLIP-B-32-quickgelu\": _VITB32_quickgelu,\n    # \"ViT-B-16\": _VITB16,\n    \"OpenaiCLIP-B-16\": _VITB16,\n    \"OpenCLIP-B-16\": _VITB16,\n    \"EVA02-B-16\": _EVAB16,\n    \"EVA02-CLIP-B-16\": _EVAB16,\n    # \"ViT-B-16-plus-240\": _VITB16_PLUS_240,\n    \"OpenCLIP-B-16-plus-240\": _VITB16_PLUS_240,\n    # \"ViT-L-14\": _VITL14,\n    \"OpenaiCLIP-L-14\": _VITL14,\n    \"OpenCLIP-L-14\": _VITL14,\n    \"EVA02-L-14\": _EVAL14,\n    \"EVA02-CLIP-L-14\": _EVAL14,\n    # \"ViT-L-14-336\": _VITL14_336,\n    \"OpenaiCLIP-L-14-336\": _VITL14_336,\n    \"EVA02-CLIP-L-14-336\": _EVAL14_336,\n    # \"ViT-H-14\": _VITH14,\n    # \"ViT-g-14\": _VITg14,\n    \"OpenCLIP-H-14\": _VITH14,\n    \"OpenCLIP-g-14\": _VITg14,\n    \"EVA01-CLIP-g-14\": _EVAg14,\n    \"EVA01-CLIP-g-14-plus\": _EVAg14_PLUS,\n    # \"ViT-bigG-14\": _VITbigG14,\n    \"OpenCLIP-bigG-14\": _VITbigG14,\n    \"EVA02-CLIP-bigE-14\": _EVAbigE14,\n    \"EVA02-CLIP-bigE-14-plus\": _EVAbigE14_PLUS,\n    \"EVA-CLIP-8B\": _EVA_8B,\n    \"EVA-CLIP-8B-448\": _EVA_8B_PLUS,\n    \"EVA-CLIP-8B-plus\": _EVA_8B_PLUS,\n}\n\n\ndef _clean_tag(tag: str):\n    # normalize pretrained tags\n    return tag.lower().replace(\"-\", \"_\")\n\n\ndef list_pretrained(as_str: bool = False):\n    \"\"\"returns list of pretrained models\n    Returns a tuple (model_name, pretrain_tag) by default or 'name:tag' if as_str == True\n    \"\"\"\n    return [\":\".join([k, t]) if as_str else (k, t) for k in _PRETRAINED.keys() for t in _PRETRAINED[k].keys()]\n\n\ndef list_pretrained_models_by_tag(tag: str):\n    \"\"\"return all models having the specified pretrain tag\"\"\"\n    models = []\n    tag = _clean_tag(tag)\n    for k in _PRETRAINED.keys():\n        if tag in _PRETRAINED[k]:\n            models.append(k)\n    return models\n\n\ndef list_pretrained_tags_by_model(model: str):\n    \"\"\"return all pretrain tags for the specified model architecture\"\"\"\n    tags = []\n    if model in _PRETRAINED:\n        tags.extend(_PRETRAINED[model].keys())\n    return tags\n\n\ndef is_pretrained_cfg(model: str, tag: str):\n    if model not in _PRETRAINED:\n        return False\n    return _clean_tag(tag) in _PRETRAINED[model]\n\n\ndef get_pretrained_cfg(model: str, tag: str):\n    if model not in _PRETRAINED:\n        return {}\n    model_pretrained = _PRETRAINED[model]\n    return model_pretrained.get(_clean_tag(tag), {})\n\n\ndef get_pretrained_url(model: str, tag: str):\n    cfg = get_pretrained_cfg(model, _clean_tag(tag))\n    return cfg.get(\"url\", \"\")\n\n\ndef download_pretrained_from_url(\n    url: str,\n    cache_dir: Union[str, None] = None,\n):\n    if not cache_dir:\n        cache_dir = os.path.expanduser(\"~/.cache/clip\")\n    os.makedirs(cache_dir, exist_ok=True)\n    filename = os.path.basename(url)\n\n    if \"openaipublic\" in url:\n        expected_sha256 = url.split(\"/\")[-2]\n    elif \"mlfoundations\" in url:\n        expected_sha256 = os.path.splitext(filename)[0].split(\"-\")[-1]\n    else:\n        expected_sha256 = \"\"\n\n    download_target = os.path.join(cache_dir, filename)\n\n    if os.path.exists(download_target) and not os.path.isfile(download_target):\n        raise RuntimeError(f\"{download_target} exists and is not a regular file\")\n\n    if os.path.isfile(download_target):\n        if expected_sha256:\n            if hashlib.sha256(open(download_target, \"rb\").read()).hexdigest().startswith(expected_sha256):\n                return download_target\n            else:\n                warnings.warn(f\"{download_target} exists, but the SHA256 checksum does not match; re-downloading the file\")\n        else:\n            return download_target\n\n    with urllib.request.urlopen(url) as source, open(download_target, \"wb\") as output:\n        with tqdm(total=int(source.headers.get(\"Content-Length\")), ncols=80, unit=\"iB\", unit_scale=True) as loop:\n            while True:\n                buffer = source.read(8192)\n                if not buffer:\n                    break\n\n                output.write(buffer)\n                loop.update(len(buffer))\n\n    if expected_sha256 and not hashlib.sha256(open(download_target, \"rb\").read()).hexdigest().startswith(expected_sha256):\n        raise RuntimeError(f\"Model has been downloaded but the SHA256 checksum does not not match\")\n\n    return download_target\n\n\ndef has_hf_hub(necessary=False):\n    if not _has_hf_hub and necessary:\n        # if no HF Hub module installed, and it is necessary to continue, raise error\n        raise RuntimeError(\"Hugging Face hub model specified but package not installed. Run `pip install huggingface_hub`.\")\n    return _has_hf_hub\n\n\ndef download_pretrained_from_hf(\n    model_id: str,\n    filename: str = \"open_clip_pytorch_model.bin\",\n    revision=None,\n    cache_dir: Union[str, None] = None,\n):\n    has_hf_hub(True)\n    cached_file = hf_hub_download(model_id, filename, revision=revision, cache_dir=cache_dir)\n    return cached_file\n\n\ndef download_pretrained(\n    cfg: Dict,\n    force_hf_hub: bool = False,\n    cache_dir: Union[str, None] = None,\n):\n    target = \"\"\n    if not cfg:\n        return target\n\n    download_url = cfg.get(\"url\", \"\")\n    download_hf_hub = cfg.get(\"hf_hub\", \"\")\n    if download_hf_hub and force_hf_hub:\n        # use HF hub even if url exists\n        download_url = \"\"\n\n    if download_url:\n        target = download_pretrained_from_url(download_url, cache_dir=cache_dir)\n    elif download_hf_hub:\n        has_hf_hub(True)\n        # we assume the hf_hub entries in pretrained config combine model_id + filename in\n        # 'org/model_name/filename.pt' form. To specify just the model id w/o filename and\n        # use 'open_clip_pytorch_model.bin' default, there must be a trailing slash 'org/model_name/'.\n        model_id, filename = os.path.split(download_hf_hub)\n        if filename:\n            target = download_pretrained_from_hf(model_id, filename=filename, cache_dir=cache_dir)\n        else:\n            target = download_pretrained_from_hf(model_id, cache_dir=cache_dir)\n\n    return target\n"}
{"type": "source_file", "path": "llava/model/multimodal_encoder/dev_eva_clip/eva_clip/hf_model.py", "content": "\"\"\" huggingface model adapter\n\nWraps HuggingFace transformers (https://github.com/huggingface/transformers) models for use as a text tower in CLIP model.\n\"\"\"\n\nimport re\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nfrom torch import TensorType\n\ntry:\n    import transformers\n    from transformers import AutoModel, AutoModelForMaskedLM, AutoTokenizer, AutoConfig, PretrainedConfig\n    from transformers.modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling, BaseModelOutputWithPoolingAndCrossAttentions\nexcept ImportError as e:\n    transformers = None\n\n    class BaseModelOutput:\n        pass\n\n    class PretrainedConfig:\n        pass\n\n\nfrom .hf_configs import arch_dict\n\n\n# utils\ndef _camel2snake(s):\n    return re.sub(r\"(?<!^)(?=[A-Z])\", \"_\", s).lower()\n\n\n# TODO: ?last - for gpt-like models\n_POOLERS = {}\n\n\ndef register_pooler(cls):\n    \"\"\"Decorator registering pooler class\"\"\"\n    _POOLERS[_camel2snake(cls.__name__)] = cls\n    return cls\n\n\n@register_pooler\nclass MeanPooler(nn.Module):\n    \"\"\"Mean pooling\"\"\"\n\n    def forward(self, x: BaseModelOutput, attention_mask: TensorType):\n        masked_output = x.last_hidden_state * attention_mask.unsqueeze(-1)\n        return masked_output.sum(dim=1) / attention_mask.sum(-1, keepdim=True)\n\n\n@register_pooler\nclass MaxPooler(nn.Module):\n    \"\"\"Max pooling\"\"\"\n\n    def forward(self, x: BaseModelOutput, attention_mask: TensorType):\n        masked_output = x.last_hidden_state.masked_fill(attention_mask.unsqueeze(-1), -torch.inf)\n        return masked_output.max(1).values\n\n\n@register_pooler\nclass ClsPooler(nn.Module):\n    \"\"\"CLS token pooling\"\"\"\n\n    def __init__(self, use_pooler_output=True):\n        super().__init__()\n        self.cls_token_position = 0\n        self.use_pooler_output = use_pooler_output\n\n    def forward(self, x: BaseModelOutput, attention_mask: TensorType):\n\n        if self.use_pooler_output and isinstance(x, (BaseModelOutputWithPooling, BaseModelOutputWithPoolingAndCrossAttentions)) and (x.pooler_output is not None):\n            return x.pooler_output\n\n        return x.last_hidden_state[:, self.cls_token_position, :]\n\n\nclass HFTextEncoder(nn.Module):\n    \"\"\"HuggingFace model adapter\"\"\"\n\n    def __init__(self, model_name_or_path: str, output_dim: int, tokenizer_name: str = None, config: PretrainedConfig = None, pooler_type: str = None, proj: str = None, pretrained: bool = True, masked_language_modeling: bool = False):\n        super().__init__()\n\n        self.output_dim = output_dim\n\n        # TODO: find better way to get this information\n        uses_transformer_pooler = pooler_type == \"cls_pooler\"\n\n        if transformers is None:\n            raise RuntimeError(\"Please `pip install transformers` to use pre-trained HuggingFace models\")\n        if config is None:\n            self.config = AutoConfig.from_pretrained(model_name_or_path)\n            if masked_language_modeling:\n                create_func, model_args = (AutoModelForMaskedLM.from_pretrained, model_name_or_path) if pretrained else (AutoModelForMaskedLM.from_config, self.config)\n            else:\n                create_func, model_args = (AutoModel.from_pretrained, model_name_or_path) if pretrained else (AutoModel.from_config, self.config)\n            # TODO: do all model configs have this attribute? PretrainedConfig does so yes??\n            if hasattr(self.config, \"is_encoder_decoder\") and self.config.is_encoder_decoder:\n                self.transformer = create_func(model_args)\n                self.transformer = self.transformer.encoder\n            else:\n                self.transformer = create_func(model_args, add_pooling_layer=uses_transformer_pooler)\n        else:\n            self.config = config\n            if masked_language_modeling:\n                self.transformer = AutoModelForMaskedLM.from_config(config)\n            else:\n                self.transformer = AutoModel.from_config(config)\n\n        if pooler_type is None:  # get default arch pooler\n            self.pooler = _POOLERS[(arch_dict[self.config.model_type][\"pooler\"])]()\n        else:\n            self.pooler = _POOLERS[pooler_type]()\n\n        d_model = getattr(self.config, arch_dict[self.config.model_type][\"config_names\"][\"width\"])\n        if (d_model == output_dim) and (proj is None):  # do we always need a proj?\n            self.proj = nn.Identity()\n        elif proj == \"linear\":\n            self.proj = nn.Linear(d_model, output_dim, bias=False)\n        elif proj == \"mlp\":\n            hidden_size = (d_model + output_dim) // 2\n            self.proj = nn.Sequential(\n                nn.Linear(d_model, hidden_size, bias=False),\n                nn.GELU(),\n                nn.Linear(hidden_size, output_dim, bias=False),\n            )\n\n        # self.itm_proj = nn.Linear(d_model, 2, bias=False)\n        # self.mlm_proj = nn.Linear(d_model, self.config.vocab_size), bias=False)\n        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n\n    # def forward_itm(self, x:TensorType, image_embeds:TensorType) -> TensorType:\n    #     image_atts = torch.ones(image_embeds.size()[:-1],dtype=torch.long).to(x.device)\n    #     attn_mask = (x != self.config.pad_token_id).long()\n    #     out = self.transformer(\n    #         input_ids=x,\n    #         attention_mask=attn_mask,\n    #         encoder_hidden_states = image_embeds,\n    #         encoder_attention_mask = image_atts,\n    #         )\n    #     pooled_out = self.pooler(out, attn_mask)\n\n    #     return self.itm_proj(pooled_out)\n\n    def mask(self, input_ids, vocab_size, device, targets=None, masked_indices=None, probability_matrix=None):\n        if masked_indices is None:\n            masked_indices = torch.bernoulli(probability_matrix).bool()\n\n        masked_indices[input_ids == self.tokenizer.pad_token_id] = False\n        masked_indices[input_ids == self.tokenizer.cls_token_id] = False\n\n        if targets is not None:\n            targets[~masked_indices] = -100  # We only compute loss on masked tokens\n\n        # 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n        indices_replaced = torch.bernoulli(torch.full(input_ids.shape, 0.8)).bool() & masked_indices\n        input_ids[indices_replaced] = self.tokenizer.mask_token_id\n\n        # 10% of the time, we replace masked input tokens with random word\n        indices_random = torch.bernoulli(torch.full(input_ids.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n        random_words = torch.randint(vocab_size, input_ids.shape, dtype=torch.long).to(device)\n        input_ids[indices_random] = random_words[indices_random]\n        # The rest of the time (10% of the time) we keep the masked input tokens unchanged\n\n        if targets is not None:\n            return input_ids, targets\n        else:\n            return input_ids\n\n    def forward_mlm(self, input_ids, image_embeds, mlm_probability=0.25):\n        labels = input_ids.clone()\n        attn_mask = (input_ids != self.config.pad_token_id).long()\n        image_atts = torch.ones(image_embeds.size()[:-1], dtype=torch.long).to(input_ids.device)\n        vocab_size = getattr(self.config, arch_dict[self.config.model_type][\"config_names\"][\"vocab_size\"])\n        probability_matrix = torch.full(labels.shape, mlm_probability)\n        input_ids, labels = self.mask(input_ids, vocab_size, input_ids.device, targets=labels, probability_matrix=probability_matrix)\n        mlm_output = self.transformer(\n            input_ids,\n            attention_mask=attn_mask,\n            encoder_hidden_states=image_embeds,\n            encoder_attention_mask=image_atts,\n            return_dict=True,\n            labels=labels,\n        )\n        return mlm_output.loss\n        # mlm_output = self.transformer(input_ids,\n        #                 attention_mask = attn_mask,\n        #                 encoder_hidden_states = image_embeds,\n        #                 encoder_attention_mask = image_atts,\n        #                 return_dict = True,\n        #             ).last_hidden_state\n        # logits = self.mlm_proj(mlm_output)\n\n        # # logits = logits[:, :-1, :].contiguous().view(-1, vocab_size)\n        # logits = logits[:, 1:, :].contiguous().view(-1, vocab_size)\n        # labels = labels[:, 1:].contiguous().view(-1)\n\n        # mlm_loss = F.cross_entropy(\n        #     logits,\n        #     labels,\n        #     # label_smoothing=0.1,\n        # )\n        # return mlm_loss\n\n    def forward(self, x: TensorType) -> TensorType:\n        attn_mask = (x != self.config.pad_token_id).long()\n        out = self.transformer(input_ids=x, attention_mask=attn_mask)\n        pooled_out = self.pooler(out, attn_mask)\n\n        return self.proj(pooled_out)\n\n    def lock(self, unlocked_layers: int = 0, freeze_layer_norm: bool = True):\n        if not unlocked_layers:  # full freezing\n            for n, p in self.transformer.named_parameters():\n                p.requires_grad = (not freeze_layer_norm) if \"LayerNorm\" in n.split(\".\") else False\n            return\n\n        encoder = self.transformer.encoder if hasattr(self.transformer, \"encoder\") else self.transformer\n        layer_list = getattr(encoder, arch_dict[self.config.model_type][\"config_names\"][\"layer_attr\"])\n        print(f\"Unlocking {unlocked_layers}/{len(layer_list) + 1} layers of hf model\")\n        embeddings = getattr(self.transformer, arch_dict[self.config.model_type][\"config_names\"][\"token_embeddings_attr\"])\n        modules = [embeddings, *layer_list][:-unlocked_layers]\n        # freeze layers\n        for module in modules:\n            for n, p in module.named_parameters():\n                p.requires_grad = (not freeze_layer_norm) if \"LayerNorm\" in n.split(\".\") else False\n\n    @torch.jit.ignore\n    def set_grad_checkpointing(self, enable=True):\n        self.transformer.gradient_checkpointing_enable()\n\n    def get_num_layers(self):\n        encoder = self.transformer.encoder if hasattr(self.transformer, \"encoder\") else self.transformer\n        layer_list = getattr(encoder, arch_dict[self.config.model_type][\"config_names\"][\"layer_attr\"])\n        return len(layer_list)\n\n    def init_parameters(self):\n        pass\n"}
{"type": "source_file", "path": "llava/model/multimodal_encoder/dev_eva_clip/eva_vit.py", "content": "# Based on EVA, BEIT, timm and DeiT code bases\n# https://github.com/baaivision/EVA\n# https://github.com/rwightman/pytorch-image-models/tree/master/timm\n# https://github.com/microsoft/unilm/tree/master/beit\n# https://github.com/facebookresearch/deit/\n# https://github.com/facebookresearch/dino\n# --------------------------------------------------------'\n# not tested yet\nimport math\nfrom transformers import CLIPImageProcessor\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.checkpoint as checkpoint\nfrom timm.models.layers import drop_path, to_2tuple, trunc_normal_\nfrom .eva_clip import create_model_and_transforms, get_model_config\nimport torch\nimport torchvision\nimport time\n\nfrom llava.utils import rank0_print\n\n\nclass EvaViTWrapper(nn.Module):\n    def __init__(self, vision_tower, args, delay_load=False):\n        super().__init__()\n\n        self.is_loaded = False\n        self.vision_tower_name = vision_tower\n        self.pretrained = args.vision_tower_pretrained\n        self.args = args\n\n        self.select_layer = args.mm_vision_select_layer\n        if self.select_layer < -1:\n            self.select_layer += 1\n        self.select_feature = getattr(args, \"mm_vision_select_feature\", \"patch\")\n\n        self.model_config = get_model_config(self.vision_tower_name)\n\n        if not delay_load:\n            rank0_print(f\"Loading vision tower: {vision_tower}\")\n            self.load_model()\n        elif getattr(args, \"unfreeze_mm_vision_tower\", False):\n            # TODO: better detector is needed.\n            rank0_print(f\"The checkpoint seems to contain `vision_tower` weights: `unfreeze_mm_vision_tower`: True.\")\n            self.load_model()\n        elif hasattr(args, \"mm_tunable_parts\") and \"mm_vision_tower\" in args.mm_tunable_parts:\n            rank0_print(f\"The checkpoint seems to contain `vision_tower` weights: `mm_tunable_parts` contains `mm_vision_tower`.\")\n            self.load_model()\n\n    def load_model(self):\n        rank0_print(f\"Loading: {self.vision_tower_name}\")\n        rank0_print(f\"Pretrained: {self.pretrained}\")\n        time_start = time.time()\n        model, _, image_processor = create_model_and_transforms(self.vision_tower_name, self.pretrained, force_custom_clip=True, precision=\"fp16\")\n        time_end = time.time()\n        rank0_print(f\"Loaded: {self.vision_tower_name} in {time_end - time_start:.2f}s\")\n        self.device = next(model.parameters()).device\n        self.dtype = next(model.parameters()).dtype\n        if self.device.type != \"meta\":\n            model = model.to(\"cuda\")\n        self.vision_tower = model.visual\n        resize_transform = [t for t in image_processor.transforms if isinstance(t, torchvision.transforms.Resize)][0]\n        normalize_transform = [t for t in image_processor.transforms if isinstance(t, torchvision.transforms.Normalize)][0]\n        self.resize_transform_size = resize_transform.size\n        self.image_processor = CLIPImageProcessor.from_pretrained(\n            \"openai/clip-vit-large-patch14\",\n            crop_size=resize_transform.size,\n            size={\"shortest_edge\": resize_transform.size},\n            image_mean=list(normalize_transform.mean),\n            image_std=list(normalize_transform.std),\n        )\n        rank0_print(f\"Loaded image processor: {self.image_processor}\")\n        self.vision_tower.requires_grad_(False)\n        self.is_loaded = True\n\n    def feature_select(self, image_features):\n        select_feature_type = self.select_feature\n\n        # if self.select_feature in [\"slicefour_patch\", \"slicefour_cls_patch\"]:\n        #     select_every_k_layer = len(image_features) // 4\n        #     image_features = torch.cat([image_features[i] for i in range(select_every_k_layer + self.select_layer, len(image_features), select_every_k_layer)], dim=-1)\n        #     select_feature_type = select_feature_type.replace(\"slicefour_\", \"\")\n        # elif self.select_feature in [\"slice_m25811_f6_patch\", \"slice_m25811_f6_cls_patch\"]:\n        #     select_layers = [-1, -4, -7, -10, 6]\n        #     image_features = torch.cat([image_features[i] for i in select_layers], dim=-1)\n        #     select_feature_type = select_feature_type.replace(\"slice_m25811_f6_\", \"\")\n        # else:\n        #     image_features = image_features[self.select_layer]\n\n        if select_feature_type == \"patch\":\n            image_features = image_features[:, 1:]\n        elif select_feature_type == \"cls_patch\":\n            image_features = image_features\n        else:\n            raise ValueError(f\"Unexpected select feature: {select_feature_type}\")\n        return image_features\n\n    def train(self, mode=True):\n        self.training = mode\n\n        if self.is_loaded:\n            self.vision_tower.eval()\n\n    def forward(self, images):\n        if type(images) is list:\n            image_features = []\n            for image in images:\n                image_features = self.vision_tower.forward_features(image.to(self.dtype), return_all_features=True)\n                image_features = self.feature_select(image_features).to(self.dtype)\n                image_features.append(image_features)\n        else:\n            image_features = self.vision_tower.forward_features(images.to(self.dtype), return_all_features=True)\n            image_features = self.feature_select(image_features).to(self.dtype)\n\n        return image_features\n\n    @property\n    def dummy_feature(self):\n        return torch.zeros(1, self.hidden_size, device=self.device, dtype=self.dtype)\n\n    @property\n    def hidden_size(self):\n        return self.model_config[\"vision_cfg\"][\"width\"]\n\n    @property\n    def num_patches(self):\n        return (self.model_config[\"vision_cfg\"][\"image_size\"] // self.model_config[\"vision_cfg\"][\"patch_size\"]) ** 2\n\n    @property\n    def num_patches_per_side(self):\n        return self.model_config[\"vision_cfg\"][\"image_size\"] // self.model_config[\"vision_cfg\"][\"patch_size\"]\n\n    @property\n    def config(self):\n        return self.model_config\n\n    @property\n    def image_size(self):\n        return self.model_config[\"vision_cfg\"][\"image_size\"]\n"}
{"type": "source_file", "path": "llava/model/multimodal_encoder/dev_eva_clip/eva_clip/modified_resnet.py", "content": "from collections import OrderedDict\n\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nfrom .utils import freeze_batch_norm_2d\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1):\n        super().__init__()\n\n        # all conv layers have stride 1. an avgpool is performed after the second convolution when stride > 1\n        self.conv1 = nn.Conv2d(inplanes, planes, 1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.act1 = nn.ReLU(inplace=True)\n\n        self.conv2 = nn.Conv2d(planes, planes, 3, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.act2 = nn.ReLU(inplace=True)\n\n        self.avgpool = nn.AvgPool2d(stride) if stride > 1 else nn.Identity()\n\n        self.conv3 = nn.Conv2d(planes, planes * self.expansion, 1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n        self.act3 = nn.ReLU(inplace=True)\n\n        self.downsample = None\n        self.stride = stride\n\n        if stride > 1 or inplanes != planes * Bottleneck.expansion:\n            # downsampling layer is prepended with an avgpool, and the subsequent convolution has stride 1\n            self.downsample = nn.Sequential(OrderedDict([(\"-1\", nn.AvgPool2d(stride)), (\"0\", nn.Conv2d(inplanes, planes * self.expansion, 1, stride=1, bias=False)), (\"1\", nn.BatchNorm2d(planes * self.expansion))]))\n\n    def forward(self, x: torch.Tensor):\n        identity = x\n\n        out = self.act1(self.bn1(self.conv1(x)))\n        out = self.act2(self.bn2(self.conv2(out)))\n        out = self.avgpool(out)\n        out = self.bn3(self.conv3(out))\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.act3(out)\n        return out\n\n\nclass AttentionPool2d(nn.Module):\n    def __init__(self, spacial_dim: int, embed_dim: int, num_heads: int, output_dim: int = None):\n        super().__init__()\n        self.positional_embedding = nn.Parameter(torch.randn(spacial_dim**2 + 1, embed_dim) / embed_dim**0.5)\n        self.k_proj = nn.Linear(embed_dim, embed_dim)\n        self.q_proj = nn.Linear(embed_dim, embed_dim)\n        self.v_proj = nn.Linear(embed_dim, embed_dim)\n        self.c_proj = nn.Linear(embed_dim, output_dim or embed_dim)\n        self.num_heads = num_heads\n\n    def forward(self, x):\n        x = x.reshape(x.shape[0], x.shape[1], x.shape[2] * x.shape[3]).permute(2, 0, 1)  # NCHW -> (HW)NC\n        x = torch.cat([x.mean(dim=0, keepdim=True), x], dim=0)  # (HW+1)NC\n        x = x + self.positional_embedding[:, None, :].to(x.dtype)  # (HW+1)NC\n        x, _ = F.multi_head_attention_forward(\n            query=x,\n            key=x,\n            value=x,\n            embed_dim_to_check=x.shape[-1],\n            num_heads=self.num_heads,\n            q_proj_weight=self.q_proj.weight,\n            k_proj_weight=self.k_proj.weight,\n            v_proj_weight=self.v_proj.weight,\n            in_proj_weight=None,\n            in_proj_bias=torch.cat([self.q_proj.bias, self.k_proj.bias, self.v_proj.bias]),\n            bias_k=None,\n            bias_v=None,\n            add_zero_attn=False,\n            dropout_p=0.0,\n            out_proj_weight=self.c_proj.weight,\n            out_proj_bias=self.c_proj.bias,\n            use_separate_proj_weight=True,\n            training=self.training,\n            need_weights=False,\n        )\n\n        return x[0]\n\n\nclass ModifiedResNet(nn.Module):\n    \"\"\"\n    A ResNet class that is similar to torchvision's but contains the following changes:\n    - There are now 3 \"stem\" convolutions as opposed to 1, with an average pool instead of a max pool.\n    - Performs anti-aliasing strided convolutions, where an avgpool is prepended to convolutions with stride > 1\n    - The final pooling layer is a QKV attention instead of an average pool\n    \"\"\"\n\n    def __init__(self, layers, output_dim, heads, image_size=224, width=64):\n        super().__init__()\n        self.output_dim = output_dim\n        self.image_size = image_size\n\n        # the 3-layer stem\n        self.conv1 = nn.Conv2d(3, width // 2, kernel_size=3, stride=2, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(width // 2)\n        self.act1 = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(width // 2, width // 2, kernel_size=3, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(width // 2)\n        self.act2 = nn.ReLU(inplace=True)\n        self.conv3 = nn.Conv2d(width // 2, width, kernel_size=3, padding=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(width)\n        self.act3 = nn.ReLU(inplace=True)\n        self.avgpool = nn.AvgPool2d(2)\n\n        # residual layers\n        self._inplanes = width  # this is a *mutable* variable used during construction\n        self.layer1 = self._make_layer(width, layers[0])\n        self.layer2 = self._make_layer(width * 2, layers[1], stride=2)\n        self.layer3 = self._make_layer(width * 4, layers[2], stride=2)\n        self.layer4 = self._make_layer(width * 8, layers[3], stride=2)\n\n        embed_dim = width * 32  # the ResNet feature dimension\n        self.attnpool = AttentionPool2d(image_size // 32, embed_dim, heads, output_dim)\n\n        self.init_parameters()\n\n    def _make_layer(self, planes, blocks, stride=1):\n        layers = [Bottleneck(self._inplanes, planes, stride)]\n\n        self._inplanes = planes * Bottleneck.expansion\n        for _ in range(1, blocks):\n            layers.append(Bottleneck(self._inplanes, planes))\n\n        return nn.Sequential(*layers)\n\n    def init_parameters(self):\n        if self.attnpool is not None:\n            std = self.attnpool.c_proj.in_features**-0.5\n            nn.init.normal_(self.attnpool.q_proj.weight, std=std)\n            nn.init.normal_(self.attnpool.k_proj.weight, std=std)\n            nn.init.normal_(self.attnpool.v_proj.weight, std=std)\n            nn.init.normal_(self.attnpool.c_proj.weight, std=std)\n\n        for resnet_block in [self.layer1, self.layer2, self.layer3, self.layer4]:\n            for name, param in resnet_block.named_parameters():\n                if name.endswith(\"bn3.weight\"):\n                    nn.init.zeros_(param)\n\n    def lock(self, unlocked_groups=0, freeze_bn_stats=False):\n        assert unlocked_groups == 0, \"partial locking not currently supported for this model\"\n        for param in self.parameters():\n            param.requires_grad = False\n        if freeze_bn_stats:\n            freeze_batch_norm_2d(self)\n\n    @torch.jit.ignore\n    def set_grad_checkpointing(self, enable=True):\n        # FIXME support for non-transformer\n        pass\n\n    def stem(self, x):\n        x = self.act1(self.bn1(self.conv1(x)))\n        x = self.act2(self.bn2(self.conv2(x)))\n        x = self.act3(self.bn3(self.conv3(x)))\n        x = self.avgpool(x)\n        return x\n\n    def forward(self, x):\n        x = self.stem(x)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        x = self.attnpool(x)\n\n        return x\n"}
{"type": "source_file", "path": "llava/model/multimodal_encoder/dev_eva_clip/eva_clip/rope.py", "content": "from math import pi\nimport torch\nfrom torch import nn\nfrom einops import rearrange, repeat\nimport logging\n\n\ndef broadcat(tensors, dim=-1):\n    num_tensors = len(tensors)\n    shape_lens = set(list(map(lambda t: len(t.shape), tensors)))\n    assert len(shape_lens) == 1, \"tensors must all have the same number of dimensions\"\n    shape_len = list(shape_lens)[0]\n    dim = (dim + shape_len) if dim < 0 else dim\n    dims = list(zip(*map(lambda t: list(t.shape), tensors)))\n    expandable_dims = [(i, val) for i, val in enumerate(dims) if i != dim]\n    assert all([*map(lambda t: len(set(t[1])) <= 2, expandable_dims)]), \"invalid dimensions for broadcastable concatentation\"\n    max_dims = list(map(lambda t: (t[0], max(t[1])), expandable_dims))\n    expanded_dims = list(map(lambda t: (t[0], (t[1],) * num_tensors), max_dims))\n    expanded_dims.insert(dim, (dim, dims[dim]))\n    expandable_shapes = list(zip(*map(lambda t: t[1], expanded_dims)))\n    tensors = list(map(lambda t: t[0].expand(*t[1]), zip(tensors, expandable_shapes)))\n    return torch.cat(tensors, dim=dim)\n\n\ndef rotate_half(x):\n    x = rearrange(x, \"... (d r) -> ... d r\", r=2)\n    x1, x2 = x.unbind(dim=-1)\n    x = torch.stack((-x2, x1), dim=-1)\n    return rearrange(x, \"... d r -> ... (d r)\")\n\n\nclass VisionRotaryEmbedding(nn.Module):\n    def __init__(\n        self,\n        dim,\n        pt_seq_len,\n        ft_seq_len=None,\n        custom_freqs=None,\n        freqs_for=\"lang\",\n        theta=10000,\n        max_freq=10,\n        num_freqs=1,\n    ):\n        super().__init__()\n        if custom_freqs:\n            freqs = custom_freqs\n        elif freqs_for == \"lang\":\n            freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n        elif freqs_for == \"pixel\":\n            freqs = torch.linspace(1.0, max_freq / 2, dim // 2) * pi\n        elif freqs_for == \"constant\":\n            freqs = torch.ones(num_freqs).float()\n        else:\n            raise ValueError(f\"unknown modality {freqs_for}\")\n\n        if ft_seq_len is None:\n            ft_seq_len = pt_seq_len\n        t = torch.arange(ft_seq_len) / ft_seq_len * pt_seq_len\n\n        freqs_h = torch.einsum(\"..., f -> ... f\", t, freqs)\n        freqs_h = repeat(freqs_h, \"... n -> ... (n r)\", r=2)\n\n        freqs_w = torch.einsum(\"..., f -> ... f\", t, freqs)\n        freqs_w = repeat(freqs_w, \"... n -> ... (n r)\", r=2)\n\n        freqs = broadcat((freqs_h[:, None, :], freqs_w[None, :, :]), dim=-1)\n\n        self.register_buffer(\"freqs_cos\", freqs.cos())\n        self.register_buffer(\"freqs_sin\", freqs.sin())\n\n        logging.info(f\"Shape of rope freq: {self.freqs_cos.shape}\")\n\n    def forward(self, t, start_index=0):\n        rot_dim = self.freqs_cos.shape[-1]\n        end_index = start_index + rot_dim\n        assert rot_dim <= t.shape[-1], f\"feature dimension {t.shape[-1]} is not of sufficient size to rotate in all the positions {rot_dim}\"\n        t_left, t, t_right = t[..., :start_index], t[..., start_index:end_index], t[..., end_index:]\n        t = (t * self.freqs_cos) + (rotate_half(t) * self.freqs_sin)\n\n        return torch.cat((t_left, t, t_right), dim=-1)\n\n\nclass VisionRotaryEmbeddingFast(nn.Module):\n    def __init__(self, dim, pt_seq_len, ft_seq_len=None, custom_freqs=None, freqs_for=\"lang\", theta=10000, max_freq=10, num_freqs=1, patch_dropout=0.0):\n        super().__init__()\n        if custom_freqs:\n            freqs = custom_freqs\n        elif freqs_for == \"lang\":\n            freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n        elif freqs_for == \"pixel\":\n            freqs = torch.linspace(1.0, max_freq / 2, dim // 2) * pi\n        elif freqs_for == \"constant\":\n            freqs = torch.ones(num_freqs).float()\n        else:\n            raise ValueError(f\"unknown modality {freqs_for}\")\n\n        if ft_seq_len is None:\n            ft_seq_len = pt_seq_len\n        t = torch.arange(ft_seq_len) / ft_seq_len * pt_seq_len\n\n        freqs = torch.einsum(\"..., f -> ... f\", t, freqs)\n        freqs = repeat(freqs, \"... n -> ... (n r)\", r=2)\n        freqs = broadcat((freqs[:, None, :], freqs[None, :, :]), dim=-1)\n\n        freqs_cos = freqs.cos().view(-1, freqs.shape[-1])\n        freqs_sin = freqs.sin().view(-1, freqs.shape[-1])\n\n        self.patch_dropout = patch_dropout\n\n        self.register_buffer(\"freqs_cos\", freqs_cos)\n        self.register_buffer(\"freqs_sin\", freqs_sin)\n\n        logging.info(f\"Shape of rope freq: {self.freqs_cos.shape}\")\n\n    def forward(self, t, patch_indices_keep=None):\n        if patch_indices_keep is not None:\n            batch = t.size()[0]\n            batch_indices = torch.arange(batch)\n            batch_indices = batch_indices[..., None]\n\n            freqs_cos = repeat(self.freqs_cos, \"i j -> n i m j\", n=t.shape[0], m=t.shape[1])\n            freqs_sin = repeat(self.freqs_sin, \"i j -> n i m j\", n=t.shape[0], m=t.shape[1])\n\n            freqs_cos = freqs_cos[batch_indices, patch_indices_keep]\n            freqs_cos = rearrange(freqs_cos, \"n i m j -> n m i j\")\n            freqs_sin = freqs_sin[batch_indices, patch_indices_keep]\n            freqs_sin = rearrange(freqs_sin, \"n i m j -> n m i j\")\n\n            return t * freqs_cos + rotate_half(t) * freqs_sin\n\n        return t * self.freqs_cos + rotate_half(t) * self.freqs_sin\n"}
{"type": "source_file", "path": "llava/model/multimodal_encoder/eva_clip/factory.py", "content": "import json\nimport logging\nimport os\nimport pathlib\nimport re\nfrom copy import deepcopy\nfrom pathlib import Path\nfrom typing import Optional, Tuple, Union, Dict, Any\nimport torch\n\n_MODEL_CONFIG_PATHS = [Path(__file__).parent / f\"model_configs/\"]\n_MODEL_CONFIGS = {}  # directory (model_name: config) of model architecture configs\n\n\ndef _natural_key(string_):\n    return [int(s) if s.isdigit() else s for s in re.split(r\"(\\d+)\", string_.lower())]\n\n\ndef _rescan_model_configs():\n    global _MODEL_CONFIGS\n\n    config_ext = (\".json\",)\n    config_files = []\n    for config_path in _MODEL_CONFIG_PATHS:\n        if config_path.is_file() and config_path.suffix in config_ext:\n            config_files.append(config_path)\n        elif config_path.is_dir():\n            for ext in config_ext:\n                config_files.extend(config_path.glob(f\"*{ext}\"))\n\n    for cf in config_files:\n        with open(cf, \"r\", encoding=\"utf8\") as f:\n            model_cfg = json.load(f)\n            if all(a in model_cfg for a in (\"embed_dim\", \"vision_cfg\", \"text_cfg\")):\n                _MODEL_CONFIGS[cf.stem] = model_cfg\n\n    _MODEL_CONFIGS = dict(sorted(_MODEL_CONFIGS.items(), key=lambda x: _natural_key(x[0])))\n\n\n_rescan_model_configs()  # initial populate of model config registry\n\n\ndef list_models():\n    \"\"\"enumerate available model architectures based on config files\"\"\"\n    return list(_MODEL_CONFIGS.keys())\n\n\ndef add_model_config(path):\n    \"\"\"add model config path or file and update registry\"\"\"\n    if not isinstance(path, Path):\n        path = Path(path)\n    _MODEL_CONFIG_PATHS.append(path)\n    _rescan_model_configs()\n\n\ndef get_model_config(model_name):\n    if model_name in _MODEL_CONFIGS:\n        return deepcopy(_MODEL_CONFIGS[model_name])\n    else:\n        return None\n"}
{"type": "source_file", "path": "llava/model/multimodal_encoder/hf_vision.py", "content": "import torch\nimport torch.nn as nn\n\nfrom transformers import AutoModel, AutoImageProcessor, AutoConfig, CLIPImageProcessor\nfrom llava.utils import rank0_print\n\n\nclass HFVisionTower(nn.Module):\n    def __init__(self, vision_tower, args, delay_load=False):\n        super().__init__()\n\n        self.is_loaded = False\n\n        self.vision_tower_name = vision_tower.replace(\"hf:\", \"\", 1)\n        self.select_layer = args.mm_vision_select_layer\n        self.select_feature = getattr(args, \"mm_vision_select_feature\", \"patch\")\n\n        if not delay_load:\n            self.load_model()\n        else:\n            self.cfg_only = AutoConfig.from_pretrained(self.vision_tower_name)\n\n    def load_model(self):\n        try:\n            self.image_processor = AutoImageProcessor.from_pretrained(self.vision_tower_name)\n        except Exception as e:\n            if \"448\" in self.vision_tower_name:\n                image_size = 448\n                # use image processor with conig\n                self.image_processor = CLIPImageProcessor(size={\"shortest_edge\": image_size}, do_center_crop=True, crop_size=image_size)\n            else:\n                self.image_processor = CLIPImageProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n        rank0_print(f\"Loaded image processor: {self.image_processor}\")\n        self.vision_tower = AutoModel.from_pretrained(self.vision_tower_name, torch_dtype=torch.bfloat16, trust_remote_code=True).to(\"cuda\")\n        self.device = self.vision_tower.device\n        self.dtype = self.vision_tower.dtype\n        self.config = self.vision_tower.config\n\n        if hasattr(self.vision_tower, \"vision_model\"):\n            self.vision_tower = self.vision_tower.vision_model\n        self.vision_tower.requires_grad_(False)\n        # self.vision_tower.eval()\n        self.is_loaded = True\n\n    def feature_select(self, image_forward_outs):\n        select_feature_type = self.select_feature\n\n        if self.select_feature in [\"slicefour_patch\", \"slicefour_cls_patch\"]:\n            select_every_k_layer = len(image_forward_outs.hidden_states) // 4\n            image_features = torch.cat([image_forward_outs.hidden_states[i] for i in range(select_every_k_layer + self.select_layer, len(image_forward_outs.hidden_states), select_every_k_layer)], dim=-1)\n            select_feature_type = select_feature_type.replace(\"slicefour_\", \"\")\n        else:\n            image_features = image_forward_outs.hidden_states[self.select_layer]\n\n        if select_feature_type == \"patch\":\n            image_features = image_features[:, 1:]\n        elif select_feature_type == \"cls_patch\":\n            image_features = image_features\n        else:\n            raise ValueError(f\"Unexpected select feature: {select_feature_type}\")\n        return image_features\n\n    def forward(self, images):\n        if type(images) is list:\n            image_features = []\n            for image in images:\n                image_forward_out = self.vision_tower(image.to(device=self.device, dtype=self.dtype).unsqueeze(0), output_hidden_states=True)\n                image_feature = self.feature_select(image_forward_out).to(image.dtype)\n                image_features.append(image_feature)\n        else:\n            image_forward_outs = self.vision_tower(images.to(device=self.device, dtype=self.dtype), output_hidden_states=True)\n            image_features = self.feature_select(image_forward_outs).to(images.dtype)\n\n        return image_features\n\n    @property\n    def dummy_feature(self):\n        return torch.zeros(1, self.hidden_size, device=self.device, dtype=self.dtype)\n\n    # @property\n    # def dtype(self):\n    #     return self.vision_tower.dtype\n\n    # @property\n    # def device(self):\n    #     return self.vision_tower.device\n\n    @property\n    def hidden_size(self):\n        try:\n            _hidden_size = self.config.hidden_size\n        except:\n            _hidden_size = self.config.vision_config.hidden_size\n        if \"slicefour\" in self.select_feature:\n            _hidden_size *= 4\n        return _hidden_size\n\n    @property\n    def num_patches(self):\n        _num_patches = (self.config.image_size // self.config.patch_size) ** 2\n        if \"cls_patch\" in self.select_feature:\n            _num_patches += 1\n        return _num_patches\n\n    @property\n    def num_patches_per_side(self):\n        return self.config.image_size // self.config.patch_size\n\n    @property\n    def image_size(self):\n        return self.config.image_size\n"}
{"type": "source_file", "path": "llava/model/multimodal_encoder/dev_eva_clip/eva_clip/openai.py", "content": "\"\"\" OpenAI pretrained model functions\n\nAdapted from https://github.com/openai/CLIP. Originally MIT License, Copyright (c) 2021 OpenAI.\n\"\"\"\n\nimport os\nimport warnings\nfrom typing import List, Optional, Union\n\nimport torch\n\nfrom .model import build_model_from_openai_state_dict, convert_weights_to_lp, get_cast_dtype\nfrom .pretrained import get_pretrained_url, list_pretrained_models_by_tag, download_pretrained_from_url\n\n__all__ = [\"list_openai_models\", \"load_openai_model\"]\n\n\ndef list_openai_models() -> List[str]:\n    \"\"\"Returns the names of available CLIP models\"\"\"\n    return list_pretrained_models_by_tag(\"openai\")\n\n\ndef load_openai_model(\n    name: str,\n    precision: Optional[str] = None,\n    device: Optional[Union[str, torch.device]] = None,\n    jit: bool = True,\n    cache_dir: Optional[str] = None,\n):\n    \"\"\"Load a CLIP model\n\n    Parameters\n    ----------\n    name : str\n        A model name listed by `clip.available_models()`, or the path to a model checkpoint containing the state_dict\n    precision: str\n        Model precision, if None defaults to 'fp32' if device == 'cpu' else 'fp16'.\n    device : Union[str, torch.device]\n        The device to put the loaded model\n    jit : bool\n        Whether to load the optimized JIT model (default) or more hackable non-JIT model.\n    cache_dir : Optional[str]\n        The directory to cache the downloaded model weights\n\n    Returns\n    -------\n    model : torch.nn.Module\n        The CLIP model\n    preprocess : Callable[[PIL.Image], torch.Tensor]\n        A torchvision transform that converts a PIL image into a tensor that the returned model can take as its input\n    \"\"\"\n    if device is None:\n        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    if precision is None:\n        precision = \"fp32\" if device == \"cpu\" else \"fp16\"\n\n    if get_pretrained_url(name, \"openai\"):\n        model_path = download_pretrained_from_url(get_pretrained_url(name, \"openai\"), cache_dir=cache_dir)\n    elif os.path.isfile(name):\n        model_path = name\n    else:\n        raise RuntimeError(f\"Model {name} not found; available models = {list_openai_models()}\")\n\n    try:\n        # loading JIT archive\n        model = torch.jit.load(model_path, map_location=device if jit else \"cpu\").eval()\n        state_dict = None\n    except RuntimeError:\n        # loading saved state dict\n        if jit:\n            warnings.warn(f\"File {model_path} is not a JIT archive. Loading as a state dict instead\")\n            jit = False\n        state_dict = torch.load(model_path, map_location=\"cpu\")\n\n    if not jit:\n        # Build a non-jit model from the OpenAI jitted model state dict\n        cast_dtype = get_cast_dtype(precision)\n        try:\n            model = build_model_from_openai_state_dict(state_dict or model.state_dict(), cast_dtype=cast_dtype)\n        except KeyError:\n            sd = {k[7:]: v for k, v in state_dict[\"state_dict\"].items()}\n            model = build_model_from_openai_state_dict(sd, cast_dtype=cast_dtype)\n\n        # model from OpenAI state dict is in manually cast fp16 mode, must be converted for AMP/fp32/bf16 use\n        model = model.to(device)\n        if precision.startswith(\"amp\") or precision == \"fp32\":\n            model.float()\n        elif precision == \"bf16\":\n            convert_weights_to_lp(model, dtype=torch.bfloat16)\n\n        return model\n\n    # patch the device names\n    device_holder = torch.jit.trace(lambda: torch.ones([]).to(torch.device(device)), example_inputs=[])\n    device_node = [n for n in device_holder.graph.findAllNodes(\"prim::Constant\") if \"Device\" in repr(n)][-1]\n\n    def patch_device(module):\n        try:\n            graphs = [module.graph] if hasattr(module, \"graph\") else []\n        except RuntimeError:\n            graphs = []\n\n        if hasattr(module, \"forward1\"):\n            graphs.append(module.forward1.graph)\n\n        for graph in graphs:\n            for node in graph.findAllNodes(\"prim::Constant\"):\n                if \"value\" in node.attributeNames() and str(node[\"value\"]).startswith(\"cuda\"):\n                    node.copyAttributes(device_node)\n\n    model.apply(patch_device)\n    patch_device(model.encode_image)\n    patch_device(model.encode_text)\n\n    # patch dtype to float32 (typically for CPU)\n    if precision == \"fp32\":\n        float_holder = torch.jit.trace(lambda: torch.ones([]).float(), example_inputs=[])\n        float_input = list(float_holder.graph.findNode(\"aten::to\").inputs())[1]\n        float_node = float_input.node()\n\n        def patch_float(module):\n            try:\n                graphs = [module.graph] if hasattr(module, \"graph\") else []\n            except RuntimeError:\n                graphs = []\n\n            if hasattr(module, \"forward1\"):\n                graphs.append(module.forward1.graph)\n\n            for graph in graphs:\n                for node in graph.findAllNodes(\"aten::to\"):\n                    inputs = list(node.inputs())\n                    for i in [1, 2]:  # dtype can be the second or third argument to aten::to()\n                        if inputs[i].node()[\"value\"] == 5:\n                            inputs[i].node().copyAttributes(float_node)\n\n        model.apply(patch_float)\n        patch_float(model.encode_image)\n        patch_float(model.encode_text)\n        model.float()\n\n    # ensure image_size attr available at consistent location for both jit and non-jit\n    model.visual.image_size = model.input_resolution.item()\n    return model\n"}
{"type": "source_file", "path": "llava/model/multimodal_encoder/dev_eva_clip/eva_clip/timm_model.py", "content": "\"\"\" timm model adapter\n\nWraps timm (https://github.com/rwightman/pytorch-image-models) models for use as a vision tower in CLIP model.\n\"\"\"\n\nimport logging\nfrom collections import OrderedDict\n\nimport torch\nimport torch.nn as nn\n\ntry:\n    import timm\n    from timm.models.layers import Mlp, to_2tuple\n\n    try:\n        # old timm imports < 0.8.1\n        from timm.models.layers.attention_pool2d import RotAttentionPool2d\n        from timm.models.layers.attention_pool2d import AttentionPool2d as AbsAttentionPool2d\n    except ImportError:\n        # new timm imports >= 0.8.1\n        from timm.layers import RotAttentionPool2d\n        from timm.layers import AttentionPool2d as AbsAttentionPool2d\nexcept ImportError:\n    timm = None\n\nfrom .utils import freeze_batch_norm_2d\n\n\nclass TimmModel(nn.Module):\n    \"\"\"timm model adapter\n    # FIXME this adapter is a work in progress, may change in ways that break weight compat\n    \"\"\"\n\n    def __init__(self, model_name, embed_dim, image_size=224, pool=\"avg\", proj=\"linear\", proj_bias=False, drop=0.0, pretrained=False):\n        super().__init__()\n        if timm is None:\n            raise RuntimeError(\"Please `pip install timm` to use timm models.\")\n\n        self.image_size = to_2tuple(image_size)\n        self.trunk = timm.create_model(model_name, pretrained=pretrained)\n        feat_size = self.trunk.default_cfg.get(\"pool_size\", None)\n        feature_ndim = 1 if not feat_size else 2\n        if pool in (\"abs_attn\", \"rot_attn\"):\n            assert feature_ndim == 2\n            # if attn pooling used, remove both classifier and default pool\n            self.trunk.reset_classifier(0, global_pool=\"\")\n        else:\n            # reset global pool if pool config set, otherwise leave as network default\n            reset_kwargs = dict(global_pool=pool) if pool else {}\n            self.trunk.reset_classifier(0, **reset_kwargs)\n        prev_chs = self.trunk.num_features\n\n        head_layers = OrderedDict()\n        if pool == \"abs_attn\":\n            head_layers[\"pool\"] = AbsAttentionPool2d(prev_chs, feat_size=feat_size, out_features=embed_dim)\n            prev_chs = embed_dim\n        elif pool == \"rot_attn\":\n            head_layers[\"pool\"] = RotAttentionPool2d(prev_chs, out_features=embed_dim)\n            prev_chs = embed_dim\n        else:\n            assert proj, \"projection layer needed if non-attention pooling is used.\"\n\n        # NOTE attention pool ends with a projection layer, so proj should usually be set to '' if such pooling is used\n        if proj == \"linear\":\n            head_layers[\"drop\"] = nn.Dropout(drop)\n            head_layers[\"proj\"] = nn.Linear(prev_chs, embed_dim, bias=proj_bias)\n        elif proj == \"mlp\":\n            head_layers[\"mlp\"] = Mlp(prev_chs, 2 * embed_dim, embed_dim, drop=drop, bias=(True, proj_bias))\n\n        self.head = nn.Sequential(head_layers)\n\n    def lock(self, unlocked_groups=0, freeze_bn_stats=False):\n        \"\"\"lock modules\n        Args:\n            unlocked_groups (int): leave last n layer groups unlocked (default: 0)\n        \"\"\"\n        if not unlocked_groups:\n            # lock full model\n            for param in self.trunk.parameters():\n                param.requires_grad = False\n            if freeze_bn_stats:\n                freeze_batch_norm_2d(self.trunk)\n        else:\n            # NOTE: partial freeze requires latest timm (master) branch and is subject to change\n            try:\n                # FIXME import here until API stable and in an official release\n                from timm.models.helpers import group_parameters, group_modules\n            except ImportError:\n                raise RuntimeError(\"Please install latest timm `pip install git+https://github.com/rwightman/pytorch-image-models`\")\n            matcher = self.trunk.group_matcher()\n            gparams = group_parameters(self.trunk, matcher)\n            max_layer_id = max(gparams.keys())\n            max_layer_id = max_layer_id - unlocked_groups\n            for group_idx in range(max_layer_id + 1):\n                group = gparams[group_idx]\n                for param in group:\n                    self.trunk.get_parameter(param).requires_grad = False\n            if freeze_bn_stats:\n                gmodules = group_modules(self.trunk, matcher, reverse=True)\n                gmodules = {k for k, v in gmodules.items() if v <= max_layer_id}\n                freeze_batch_norm_2d(self.trunk, gmodules)\n\n    @torch.jit.ignore\n    def set_grad_checkpointing(self, enable=True):\n        try:\n            self.trunk.set_grad_checkpointing(enable)\n        except Exception as e:\n            logging.warning(\"grad checkpointing not supported for this timm image tower, continuing without...\")\n\n    def forward(self, x):\n        x = self.trunk(x)\n        x = self.head(x)\n        return x\n"}
{"type": "source_file", "path": "llava/model/multimodal_encoder/dev_eva_clip/eva_clip/tokenizer.py", "content": "\"\"\" CLIP tokenizer\n\nCopied from https://github.com/openai/CLIP. Originally MIT License, Copyright (c) 2021 OpenAI.\n\"\"\"\n\nimport gzip\nimport html\nimport os\nfrom functools import lru_cache\nfrom typing import Union, List\n\nimport ftfy\nimport regex as re\nimport torch\n\n# https://stackoverflow.com/q/62691279\nimport os\n\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\n\n@lru_cache()\ndef default_bpe():\n    return os.path.join(os.path.dirname(os.path.abspath(__file__)), \"bpe_simple_vocab_16e6.txt.gz\")\n\n\n@lru_cache()\ndef bytes_to_unicode():\n    \"\"\"\n    Returns list of utf-8 byte and a corresponding list of unicode strings.\n    The reversible bpe codes work on unicode strings.\n    This means you need a large # of unicode characters in your vocab if you want to avoid UNKs.\n    When you're at something like a 10B token dataset you end up needing around 5K for decent coverage.\n    This is a signficant percentage of your normal, say, 32K bpe vocab.\n    To avoid that, we want lookup tables between utf-8 bytes and unicode strings.\n    And avoids mapping to whitespace/control characters the bpe code barfs on.\n    \"\"\"\n    bs = list(range(ord(\"!\"), ord(\"~\") + 1)) + list(range(ord(\"¡\"), ord(\"¬\") + 1)) + list(range(ord(\"®\"), ord(\"ÿ\") + 1))\n    cs = bs[:]\n    n = 0\n    for b in range(2**8):\n        if b not in bs:\n            bs.append(b)\n            cs.append(2**8 + n)\n            n += 1\n    cs = [chr(n) for n in cs]\n    return dict(zip(bs, cs))\n\n\ndef get_pairs(word):\n    \"\"\"Return set of symbol pairs in a word.\n    Word is represented as tuple of symbols (symbols being variable-length strings).\n    \"\"\"\n    pairs = set()\n    prev_char = word[0]\n    for char in word[1:]:\n        pairs.add((prev_char, char))\n        prev_char = char\n    return pairs\n\n\ndef basic_clean(text):\n    text = ftfy.fix_text(text)\n    text = html.unescape(html.unescape(text))\n    return text.strip()\n\n\ndef whitespace_clean(text):\n    text = re.sub(r\"\\s+\", \" \", text)\n    text = text.strip()\n    return text\n\n\nclass SimpleTokenizer(object):\n    def __init__(self, bpe_path: str = default_bpe(), special_tokens=None):\n        self.byte_encoder = bytes_to_unicode()\n        self.byte_decoder = {v: k for k, v in self.byte_encoder.items()}\n        merges = gzip.open(bpe_path).read().decode(\"utf-8\").split(\"\\n\")\n        merges = merges[1 : 49152 - 256 - 2 + 1]\n        merges = [tuple(merge.split()) for merge in merges]\n        vocab = list(bytes_to_unicode().values())\n        vocab = vocab + [v + \"</w>\" for v in vocab]\n        for merge in merges:\n            vocab.append(\"\".join(merge))\n        if not special_tokens:\n            special_tokens = [\"<start_of_text>\", \"<end_of_text>\"]\n        else:\n            special_tokens = [\"<start_of_text>\", \"<end_of_text>\"] + special_tokens\n        vocab.extend(special_tokens)\n        self.encoder = dict(zip(vocab, range(len(vocab))))\n        self.decoder = {v: k for k, v in self.encoder.items()}\n        self.bpe_ranks = dict(zip(merges, range(len(merges))))\n        self.cache = {t: t for t in special_tokens}\n        special = \"|\".join(special_tokens)\n        self.pat = re.compile(special + r\"\"\"|'s|'t|'re|'ve|'m|'ll|'d|[\\p{L}]+|[\\p{N}]|[^\\s\\p{L}\\p{N}]+\"\"\", re.IGNORECASE)\n\n        self.vocab_size = len(self.encoder)\n        self.all_special_ids = [self.encoder[t] for t in special_tokens]\n\n    def bpe(self, token):\n        if token in self.cache:\n            return self.cache[token]\n        word = tuple(token[:-1]) + (token[-1] + \"</w>\",)\n        pairs = get_pairs(word)\n\n        if not pairs:\n            return token + \"</w>\"\n\n        while True:\n            bigram = min(pairs, key=lambda pair: self.bpe_ranks.get(pair, float(\"inf\")))\n            if bigram not in self.bpe_ranks:\n                break\n            first, second = bigram\n            new_word = []\n            i = 0\n            while i < len(word):\n                try:\n                    j = word.index(first, i)\n                    new_word.extend(word[i:j])\n                    i = j\n                except:\n                    new_word.extend(word[i:])\n                    break\n\n                if word[i] == first and i < len(word) - 1 and word[i + 1] == second:\n                    new_word.append(first + second)\n                    i += 2\n                else:\n                    new_word.append(word[i])\n                    i += 1\n            new_word = tuple(new_word)\n            word = new_word\n            if len(word) == 1:\n                break\n            else:\n                pairs = get_pairs(word)\n        word = \" \".join(word)\n        self.cache[token] = word\n        return word\n\n    def encode(self, text):\n        bpe_tokens = []\n        text = whitespace_clean(basic_clean(text)).lower()\n        for token in re.findall(self.pat, text):\n            token = \"\".join(self.byte_encoder[b] for b in token.encode(\"utf-8\"))\n            bpe_tokens.extend(self.encoder[bpe_token] for bpe_token in self.bpe(token).split(\" \"))\n        return bpe_tokens\n\n    def decode(self, tokens):\n        text = \"\".join([self.decoder[token] for token in tokens])\n        text = bytearray([self.byte_decoder[c] for c in text]).decode(\"utf-8\", errors=\"replace\").replace(\"</w>\", \" \")\n        return text\n\n\n_tokenizer = SimpleTokenizer()\n\n\ndef tokenize(texts: Union[str, List[str]], context_length: int = 77) -> torch.LongTensor:\n    \"\"\"\n    Returns the tokenized representation of given input string(s)\n\n    Parameters\n    ----------\n    texts : Union[str, List[str]]\n        An input string or a list of input strings to tokenize\n    context_length : int\n        The context length to use; all CLIP models use 77 as the context length\n\n    Returns\n    -------\n    A two-dimensional tensor containing the resulting tokens, shape = [number of input strings, context_length]\n    \"\"\"\n    if isinstance(texts, str):\n        texts = [texts]\n\n    sot_token = _tokenizer.encoder[\"<start_of_text>\"]\n    eot_token = _tokenizer.encoder[\"<end_of_text>\"]\n    all_tokens = [[sot_token] + _tokenizer.encode(text) + [eot_token] for text in texts]\n    result = torch.zeros(len(all_tokens), context_length, dtype=torch.long)\n\n    for i, tokens in enumerate(all_tokens):\n        if len(tokens) > context_length:\n            tokens = tokens[:context_length]  # Truncate\n            tokens[-1] = eot_token\n        result[i, : len(tokens)] = torch.tensor(tokens)\n\n    return result\n\n\nclass HFTokenizer:\n    \"HuggingFace tokenizer wrapper\"\n\n    def __init__(self, tokenizer_name: str):\n        from transformers import AutoTokenizer\n\n        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n\n    def __call__(self, texts: Union[str, List[str]], context_length: int = 77) -> torch.Tensor:\n        # same cleaning as for default tokenizer, except lowercasing\n        # adding lower (for case-sensitive tokenizers) will make it more robust but less sensitive to nuance\n        if isinstance(texts, str):\n            texts = [texts]\n        texts = [whitespace_clean(basic_clean(text)) for text in texts]\n        input_ids = self.tokenizer(texts, return_tensors=\"pt\", max_length=context_length, padding=\"max_length\", truncation=True).input_ids\n        return input_ids\n"}
{"type": "source_file", "path": "llava/model/multimodal_encoder/eva_clip/eva_clip_encoder.py", "content": "import torch\nimport torch.nn as nn\n\nfrom .eva_clip_processors import EvaClipImageTrainProcessor\nfrom .eva_vit import EVAEncoderWrapper\nfrom .factory import list_models, add_model_config, get_model_config\n\nfrom llava.utils import rank0_print\n\n\nclass EvaClipVisionTower(nn.Module):\n    def __init__(self, vision_tower, args, delay_load=False):\n        super().__init__()\n\n        self.is_loaded = False\n        self.vision_tower_name = vision_tower\n        self.vision_tower_pretrained = args.vision_tower_pretrained\n        self.config = get_model_config(vision_tower)\n\n        if not delay_load:\n            rank0_print(f\"Loading EVA ViT: {self.vision_tower_name}\")\n            self.load_model()\n        elif getattr(args, \"unfreeze_mm_vision_tower\", False):\n            # TODO: better detector is needed.\n            rank0_print(f\"The checkpoint seems to contain `vision_tower` weights: `unfreeze_mm_vision_tower`: True.\")\n            self.load_model()\n        elif hasattr(args, \"mm_tunable_parts\") and \"mm_vision_tower\" in args.mm_tunable_parts:\n            rank0_print(f\"The checkpoint seems to contain `vision_tower` weights: `mm_tunable_parts` contains `mm_vision_tower`.\")\n            self.load_model()\n        else:\n            self.cfg_only = self.config\n\n    def load_model(self, device_map=None):\n        rank0_print(f\"Pretrained: {self.vision_tower_pretrained}\")\n        self.image_processor = EvaClipImageTrainProcessor(self.config[\"vision_cfg\"][\"image_size\"])\n        self.vision_tower = EVAEncoderWrapper(self.vision_tower_pretrained, self.config)\n        rank0_print(f\"Loaded image processor: {self.image_processor}\")\n        self.vision_tower.requires_grad_(False)\n        self.is_loaded = True\n\n    def forward(self, images):\n        if type(images) is list:\n            image_features = []\n            for image in images:\n                image_feature = self.vision_tower(image.to(device=self.device, dtype=self.dtype).unsqueeze(0)).to(image.dtype)\n                image_features.append(image_feature)\n        else:\n            image_features = self.vision_tower(images.to(device=self.device, dtype=self.dtype)).to(images.dtype)\n\n        return image_features\n\n    @property\n    def dtype(self):\n        return self.vision_tower.dtype\n\n    @property\n    def device(self):\n        return self.vision_tower.device\n\n    @property\n    def hidden_size(self):\n        return self.config[\"vision_cfg\"][\"width\"]\n\n    @property\n    def num_patches(self):\n        return (self.config[\"vision_cfg\"][\"image_size\"] // self.config[\"vision_cfg\"][\"patch_size\"]) ** 2\n\n    @property\n    def num_patches_per_side(self):\n        return self.config[\"vision_cfg\"][\"image_size\"] // self.config[\"vision_cfg\"][\"patch_size\"]\n\n    @property\n    def image_size(self):\n        return self.config[\"vision_cfg\"][\"image_size\"]\n"}
{"type": "source_file", "path": "llava/model/multimodal_encoder/dev_eva_clip/eva_clip/factory.py", "content": "import json\nimport logging\nimport os\nimport pathlib\nimport re\nfrom copy import deepcopy\nfrom pathlib import Path\nfrom typing import Optional, Tuple, Union, Dict, Any\nimport torch\n\ntry:\n    import deepspeed\nexcept ImportError:\n    deepspeed = None\n\nfrom .constants import OPENAI_DATASET_MEAN, OPENAI_DATASET_STD\nfrom .model import CLIP, CustomCLIP, convert_weights_to_lp, convert_to_custom_text_state_dict, get_cast_dtype\nfrom .openai import load_openai_model\nfrom .pretrained import is_pretrained_cfg, get_pretrained_cfg, download_pretrained, list_pretrained_tags_by_model\nfrom .transform import image_transform\nfrom .tokenizer import HFTokenizer, tokenize\nfrom .utils import resize_clip_pos_embed, resize_evaclip_pos_embed, resize_visual_pos_embed, resize_eva_pos_embed\n\n\n_MODEL_CONFIG_PATHS = [Path(__file__).parent / f\"model_configs/\"]\n_MODEL_CONFIGS = {}  # directory (model_name: config) of model architecture configs\n\n\ndef _natural_key(string_):\n    return [int(s) if s.isdigit() else s for s in re.split(r\"(\\d+)\", string_.lower())]\n\n\ndef _rescan_model_configs():\n    global _MODEL_CONFIGS\n\n    config_ext = (\".json\",)\n    config_files = []\n    for config_path in _MODEL_CONFIG_PATHS:\n        if config_path.is_file() and config_path.suffix in config_ext:\n            config_files.append(config_path)\n        elif config_path.is_dir():\n            for ext in config_ext:\n                config_files.extend(config_path.glob(f\"*{ext}\"))\n\n    for cf in config_files:\n        with open(cf, \"r\", encoding=\"utf8\") as f:\n            model_cfg = json.load(f)\n            if all(a in model_cfg for a in (\"embed_dim\", \"vision_cfg\", \"text_cfg\")):\n                _MODEL_CONFIGS[cf.stem] = model_cfg\n\n    _MODEL_CONFIGS = dict(sorted(_MODEL_CONFIGS.items(), key=lambda x: _natural_key(x[0])))\n\n\n_rescan_model_configs()  # initial populate of model config registry\n\n\ndef list_models():\n    \"\"\"enumerate available model architectures based on config files\"\"\"\n    return list(_MODEL_CONFIGS.keys())\n\n\ndef add_model_config(path):\n    \"\"\"add model config path or file and update registry\"\"\"\n    if not isinstance(path, Path):\n        path = Path(path)\n    _MODEL_CONFIG_PATHS.append(path)\n    _rescan_model_configs()\n\n\ndef get_model_config(model_name):\n    if model_name in _MODEL_CONFIGS:\n        return deepcopy(_MODEL_CONFIGS[model_name])\n    else:\n        return None\n\n\ndef get_tokenizer(model_name):\n    config = get_model_config(model_name)\n    tokenizer = HFTokenizer(config[\"text_cfg\"][\"hf_tokenizer_name\"]) if \"hf_tokenizer_name\" in config[\"text_cfg\"] else tokenize\n    return tokenizer\n\n\n# loading openai CLIP weights when is_openai=True for training\ndef load_state_dict(checkpoint_path: str, map_location: str = \"cpu\", model_key: str = \"model|module|state_dict\", is_openai: bool = False, skip_list: list = []):\n    if is_openai:\n        model = torch.jit.load(checkpoint_path, map_location=\"cpu\").eval()\n        state_dict = model.state_dict()\n        for key in [\"input_resolution\", \"context_length\", \"vocab_size\"]:\n            state_dict.pop(key, None)\n    else:\n        checkpoint = torch.load(checkpoint_path, map_location=map_location)\n        for mk in model_key.split(\"|\"):\n            if isinstance(checkpoint, dict) and mk in checkpoint:\n                state_dict = checkpoint[mk]\n                break\n            else:\n                state_dict = checkpoint\n        if next(iter(state_dict.items()))[0].startswith(\"module\"):\n            state_dict = {k[7:]: v for k, v in state_dict.items()}\n\n    for k in skip_list:\n        if k in list(state_dict.keys()):\n            logging.info(f\"Removing key {k} from pretrained checkpoint\")\n            del state_dict[k]\n\n    if os.getenv(\"RoPE\") == \"1\":\n        for k in list(state_dict.keys()):\n            if \"freqs_cos\" in k or \"freqs_sin\" in k:\n                del state_dict[k]\n    return state_dict\n\n\ndef load_checkpoint(model, checkpoint_path, model_key=\"model|module|state_dict\", strict=True):\n    state_dict = load_state_dict(checkpoint_path, model_key=model_key, is_openai=False)\n    # detect old format and make compatible with new format\n    if \"positional_embedding\" in state_dict and not hasattr(model, \"positional_embedding\"):\n        state_dict = convert_to_custom_text_state_dict(state_dict)\n    if \"text.logit_scale\" in state_dict and hasattr(model, \"logit_scale\"):\n        state_dict[\"logit_scale\"] = state_dict[\"text.logit_scale\"]\n        del state_dict[\"text.logit_scale\"]\n\n    # resize_clip_pos_embed for CLIP and open CLIP\n    if \"visual.positional_embedding\" in state_dict:\n        resize_clip_pos_embed(state_dict, model)\n    # specified to eva_vit_model\n    elif \"visual.pos_embed\" in state_dict:\n        resize_evaclip_pos_embed(state_dict, model)\n\n    # resize_clip_pos_embed(state_dict, model)\n    incompatible_keys = model.load_state_dict(state_dict, strict=strict)\n    logging.info(f\"incompatible_keys.missing_keys: {incompatible_keys.missing_keys}\")\n    return incompatible_keys\n\n\ndef load_clip_visual_state_dict(checkpoint_path: str, map_location: str = \"cpu\", is_openai: bool = False, skip_list: list = []):\n    state_dict = load_state_dict(checkpoint_path, map_location=map_location, is_openai=is_openai, skip_list=skip_list)\n\n    for k in list(state_dict.keys()):\n        if not k.startswith(\"visual.\"):\n            del state_dict[k]\n    for k in list(state_dict.keys()):\n        if k.startswith(\"visual.\"):\n            new_k = k[7:]\n            state_dict[new_k] = state_dict[k]\n            del state_dict[k]\n    return state_dict\n\n\ndef load_clip_text_state_dict(checkpoint_path: str, map_location: str = \"cpu\", is_openai: bool = False, skip_list: list = []):\n    state_dict = load_state_dict(checkpoint_path, map_location=map_location, is_openai=is_openai, skip_list=skip_list)\n\n    for k in list(state_dict.keys()):\n        if k.startswith(\"visual.\"):\n            del state_dict[k]\n    return state_dict\n\n\ndef get_pretrained_tag(pretrained_model):\n    pretrained_model = pretrained_model.lower()\n    if \"laion\" in pretrained_model or \"open_clip\" in pretrained_model:\n        return \"open_clip\"\n    elif \"openai\" in pretrained_model:\n        return \"clip\"\n    elif \"eva\" in pretrained_model and \"clip\" in pretrained_model:\n        return \"eva_clip\"\n    else:\n        return \"other\"\n\n\ndef load_zero_partitions(model, state_dict, is_deepspeed_zero3_enabled, pretrained_model_path, ignore_mismatched_sizes=False):\n    \"\"\"\n    adept from pytorch lightning and transformers\n    with deepspeed.zero.Init():\n        model = MyModel()\n    state_dict = torch.load(model_path, map_location=\"cpu\")\n    load_zero_partitions(model, prefix=\"\")\n    \"\"\"\n\n    # because zero3 puts placeholders in model params, this context\n    # manager gathers (unpartitions) the params of the current layer, then loads from\n    # the state dict and then re-partitions them again\n    model_state_dict = model.state_dict()\n    expected_keys = list(model_state_dict.keys())\n    loaded_keys = list(state_dict.keys())\n    missing_keys = list(set(expected_keys) - set(loaded_keys))\n    unexpected_keys = list(set(loaded_keys) - set(expected_keys))\n\n    # Mistmatched keys contains tuples key/shape1/shape2 of weights in the checkpoint that have a shape not\n    # matching the weights in the model.\n    mismatched_keys = []\n    if ignore_mismatched_sizes:\n        for checkpoint_key in loaded_keys:\n            model_key = checkpoint_key\n\n            if model_key in model_state_dict and state_dict[checkpoint_key].shape != model_state_dict[model_key].shape:\n                mismatched_keys.append((checkpoint_key, state_dict[checkpoint_key].shape, model_state_dict[model_key].shape))\n                del state_dict[checkpoint_key]\n    # copy state_dict so _load_from_state_dict can modify it\n    metadata = getattr(state_dict, \"_metadata\", None)\n    state_dict = state_dict.copy()\n    if metadata is not None:\n        state_dict._metadata = metadata\n\n    error_msgs = []\n\n    # PyTorch's `_load_from_state_dict` does not copy parameters in a module's descendants\n    # so we need to apply the function recursively.\n    def load(module, prefix=\"\"):\n        local_metadata = {} if metadata is None else metadata.get(prefix[:-1], {})\n        args = (state_dict, prefix, local_metadata, True, [], [], error_msgs)\n        if is_deepspeed_zero3_enabled:\n            # because zero3 puts placeholders in model params, this context\n            # manager gathers (unpartitions) the params of the current layer, then loads from\n            # the state dict and then re-partitions them again\n            with deepspeed.zero.GatheredParameters(list(module.parameters(recurse=False)), modifier_rank=0):\n                if torch.distributed.get_rank() == 0:\n                    module._load_from_state_dict(*args)\n        else:\n            module._load_from_state_dict(*args)\n\n        for name, child in module._modules.items():\n            if child is not None:\n                load(child, prefix + name + \".\")\n\n    # Make sure we are able to load base models as well as derived models (with heads)\n    start_prefix = \"\"\n    model_to_load = model\n    load(model_to_load, prefix=start_prefix)\n    del state_dict\n    if len(error_msgs) > 0:\n        error_msg = \"\\n\\t\".join(error_msgs)\n        if \"size mismatch\" in error_msg:\n            error_msg += \"\\n\\tYou may consider adding `ignore_mismatched_sizes=True` in the model `from_pretrained` method.\"\n        raise RuntimeError(f\"Error(s) in loading state_dict for {model.__class__.__name__}:\\n\\t{error_msg}\")\n    if len(unexpected_keys) > 0:\n        logging.warning(\n            f\"Some weights of the model checkpoint at {pretrained_model_path} were not used when\"\n            f\" initializing {model.__class__.__name__}: {unexpected_keys}\\n- This IS expected if you are\"\n            f\" initializing {model.__class__.__name__} from the checkpoint of a model trained on another task or\"\n            \" with another architecture (e.g. initializing a BertForSequenceClassification model from a\"\n            \" BertForPreTraining model).\\n- This IS NOT expected if you are initializing\"\n            f\" {model.__class__.__name__} from the checkpoint of a model that you expect to be exactly identical\"\n            \" (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\"\n        )\n    else:\n        logging.info(f\"All model checkpoint weights were used when initializing {model.__class__.__name__}.\\n\")\n    if len(missing_keys) > 0:\n        logging.warning(\n            f\"Some weights of {model.__class__.__name__} were not initialized from the model checkpoint at\"\n            f\" {pretrained_model_path} and are newly initialized: {missing_keys}\\nYou should probably\"\n            \" TRAIN this model on a down-stream task to be able to use it for predictions and inference.\"\n        )\n    elif len(mismatched_keys) == 0:\n        logging.info(\n            f\"All the weights of {model.__class__.__name__} were initialized from the model checkpoint at\"\n            f\" {pretrained_model_path}.\\nIf your task is similar to the task the model of the checkpoint\"\n            f\" was trained on, you can already use {model.__class__.__name__} for predictions without further\"\n            \" training.\"\n        )\n    if len(mismatched_keys) > 0:\n        mismatched_warning = \"\\n\".join([f\"- {key}: found shape {shape1} in the checkpoint and {shape2} in the model instantiated\" for key, shape1, shape2 in mismatched_keys])\n        logging.warning(\n            f\"Some weights of {model.__class__.__name__} were not initialized from the model checkpoint at\"\n            f\" {pretrained_model_path} and are newly initialized because the shapes did not\"\n            f\" match:\\n{mismatched_warning}\\nYou should probably TRAIN this model on a down-stream task to be able\"\n            \" to use it for predictions and inference.\"\n        )\n\n\ndef load_pretrained_checkpoint(model, visual_checkpoint_path, text_checkpoint_path, strict=True, visual_model=None, text_model=None, model_key=\"model|module|state_dict\", skip_list=[]):\n    visual_tag = get_pretrained_tag(visual_model)\n    text_tag = get_pretrained_tag(text_model)\n\n    logging.info(f\"num of model state_dict keys: {len(model.state_dict().keys())}\")\n    visual_incompatible_keys, text_incompatible_keys = None, None\n    if visual_checkpoint_path:\n        if visual_tag == \"eva_clip\" or visual_tag == \"open_clip\":\n            visual_state_dict = load_clip_visual_state_dict(visual_checkpoint_path, is_openai=False, skip_list=skip_list)\n        elif visual_tag == \"clip\":\n            visual_state_dict = load_clip_visual_state_dict(visual_checkpoint_path, is_openai=True, skip_list=skip_list)\n        else:\n            visual_state_dict = load_state_dict(visual_checkpoint_path, model_key=model_key, is_openai=False, skip_list=skip_list)\n\n        # resize_clip_pos_embed for CLIP and open CLIP\n        if \"positional_embedding\" in visual_state_dict:\n            resize_visual_pos_embed(visual_state_dict, model)\n        # specified to EVA model\n        elif \"pos_embed\" in visual_state_dict:\n            resize_eva_pos_embed(visual_state_dict, model)\n\n        visual_incompatible_keys = model.visual.load_state_dict(visual_state_dict, strict=strict)\n        logging.info(f\"num of loaded visual_state_dict keys: {len(visual_state_dict.keys())}\")\n        logging.info(f\"visual_incompatible_keys.missing_keys: {visual_incompatible_keys.missing_keys}\")\n\n    if text_checkpoint_path:\n        if text_tag == \"eva_clip\" or text_tag == \"open_clip\":\n            text_state_dict = load_clip_text_state_dict(text_checkpoint_path, is_openai=False, skip_list=skip_list)\n        elif text_tag == \"clip\":\n            text_state_dict = load_clip_text_state_dict(text_checkpoint_path, is_openai=True, skip_list=skip_list)\n        else:\n            text_state_dict = load_state_dict(visual_checkpoint_path, model_key=model_key, is_openai=False, skip_list=skip_list)\n\n        text_incompatible_keys = model.text.load_state_dict(text_state_dict, strict=strict)\n\n        logging.info(f\"num of loaded text_state_dict keys: {len(text_state_dict.keys())}\")\n        logging.info(f\"text_incompatible_keys.missing_keys: {text_incompatible_keys.missing_keys}\")\n\n    return visual_incompatible_keys, text_incompatible_keys\n\n\ndef create_model(\n    model_name: str,\n    pretrained: Optional[str] = None,\n    precision: str = \"fp32\",\n    device: Union[str, torch.device] = \"cpu\",\n    jit: bool = False,\n    force_quick_gelu: bool = False,\n    force_custom_clip: bool = False,\n    force_patch_dropout: Optional[float] = None,\n    pretrained_image: str = \"\",\n    pretrained_text: str = \"\",\n    pretrained_hf: bool = True,\n    pretrained_visual_model: str = None,\n    pretrained_text_model: str = None,\n    cache_dir: Optional[str] = None,\n    skip_list: list = [],\n):\n    model_name = model_name.replace(\"/\", \"-\")  # for callers using old naming with / in ViT names\n    if isinstance(device, str):\n        device = torch.device(device)\n\n    if pretrained and pretrained.lower() == \"openai\":\n        logging.info(f\"Loading pretrained {model_name} from OpenAI.\")\n        model = load_openai_model(\n            model_name,\n            precision=precision,\n            device=device,\n            jit=jit,\n            cache_dir=cache_dir,\n        )\n    else:\n        model_cfg = get_model_config(model_name)\n        if model_cfg is not None:\n            logging.info(f\"Loaded {model_name} model config.\")\n        else:\n            logging.error(f\"Model config for {model_name} not found; available models {list_models()}.\")\n            raise RuntimeError(f\"Model config for {model_name} not found.\")\n\n        if \"rope\" in model_cfg.get(\"vision_cfg\", {}):\n            if model_cfg[\"vision_cfg\"][\"rope\"]:\n                os.environ[\"RoPE\"] = \"1\"\n        else:\n            os.environ[\"RoPE\"] = \"0\"\n\n        if force_quick_gelu:\n            # override for use of QuickGELU on non-OpenAI transformer models\n            model_cfg[\"quick_gelu\"] = True\n\n        if force_patch_dropout is not None:\n            # override the default patch dropout value\n            model_cfg[\"vision_cfg\"][\"patch_dropout\"] = force_patch_dropout\n\n        cast_dtype = get_cast_dtype(precision)\n        custom_clip = model_cfg.pop(\"custom_text\", False) or force_custom_clip or (\"hf_model_name\" in model_cfg[\"text_cfg\"])\n\n        if custom_clip:\n            if \"hf_model_name\" in model_cfg.get(\"text_cfg\", {}):\n                model_cfg[\"text_cfg\"][\"hf_model_pretrained\"] = pretrained_hf\n            model = CustomCLIP(**model_cfg, cast_dtype=cast_dtype)\n        else:\n            model = CLIP(**model_cfg, cast_dtype=cast_dtype)\n\n        pretrained_cfg = {}\n        if pretrained:\n            checkpoint_path = \"\"\n            pretrained_cfg = get_pretrained_cfg(model_name, pretrained)\n            if pretrained_cfg:\n                checkpoint_path = download_pretrained(pretrained_cfg, cache_dir=cache_dir)\n            elif os.path.exists(pretrained):\n                checkpoint_path = pretrained\n\n            if checkpoint_path:\n                logging.info(f\"Loading pretrained {model_name} weights ({pretrained}).\")\n                load_checkpoint(model, checkpoint_path, model_key=\"model|module|state_dict\", strict=False)\n            else:\n                error_str = f\"Pretrained weights ({pretrained}) not found for model {model_name}.\" f\"Available pretrained tags ({list_pretrained_tags_by_model(model_name)}.\"\n                logging.warning(error_str)\n                raise RuntimeError(error_str)\n        else:\n            visual_checkpoint_path = \"\"\n            text_checkpoint_path = \"\"\n\n            if pretrained_image:\n                pretrained_visual_model = pretrained_visual_model.replace(\"/\", \"-\")  # for callers using old naming with / in ViT names\n                pretrained_image_cfg = get_pretrained_cfg(pretrained_visual_model, pretrained_image)\n                if \"timm_model_name\" in model_cfg.get(\"vision_cfg\", {}):\n                    # pretrained weight loading for timm models set via vision_cfg\n                    model_cfg[\"vision_cfg\"][\"timm_model_pretrained\"] = True\n                elif pretrained_image_cfg:\n                    visual_checkpoint_path = download_pretrained(pretrained_image_cfg, cache_dir=cache_dir)\n                elif os.path.exists(pretrained_image):\n                    visual_checkpoint_path = pretrained_image\n                else:\n                    logging.warning(f\"Pretrained weights ({visual_checkpoint_path}) not found for model {model_name}.visual.\")\n                    raise RuntimeError(f\"Pretrained weights ({visual_checkpoint_path}) not found for model {model_name}.visual.\")\n\n            if pretrained_text:\n                pretrained_text_model = pretrained_text_model.replace(\"/\", \"-\")  # for callers using old naming with / in ViT names\n                pretrained_text_cfg = get_pretrained_cfg(pretrained_text_model, pretrained_text)\n                if pretrained_image_cfg:\n                    text_checkpoint_path = download_pretrained(pretrained_text_cfg, cache_dir=cache_dir)\n                elif os.path.exists(pretrained_text):\n                    text_checkpoint_path = pretrained_text\n                else:\n                    logging.warning(f\"Pretrained weights ({text_checkpoint_path}) not found for model {model_name}.text.\")\n                    raise RuntimeError(f\"Pretrained weights ({text_checkpoint_path}) not found for model {model_name}.text.\")\n\n            if visual_checkpoint_path:\n                logging.info(f\"Loading pretrained {model_name}.visual weights ({visual_checkpoint_path}).\")\n            if text_checkpoint_path:\n                logging.info(f\"Loading pretrained {model_name}.text weights ({text_checkpoint_path}).\")\n\n            if visual_checkpoint_path or text_checkpoint_path:\n                load_pretrained_checkpoint(model, visual_checkpoint_path, text_checkpoint_path, strict=False, visual_model=pretrained_visual_model, text_model=pretrained_text_model, model_key=\"model|module|state_dict\", skip_list=skip_list)\n\n        if \"fp16\" in precision or \"bf16\" in precision:\n            logging.info(f\"convert precision to {precision}\")\n            model = model.to(torch.bfloat16) if \"bf16\" in precision else model.to(torch.float16)\n\n        # model.to(device=device)\n\n        # set image / mean metadata from pretrained_cfg if available, or use default\n        model.visual.image_mean = pretrained_cfg.get(\"mean\", None) or OPENAI_DATASET_MEAN\n        model.visual.image_std = pretrained_cfg.get(\"std\", None) or OPENAI_DATASET_STD\n\n        if jit:\n            model = torch.jit.script(model)\n\n    return model\n\n\ndef create_model_and_transforms(\n    model_name: str,\n    pretrained: Optional[str] = None,\n    precision: str = \"fp32\",\n    device: Union[str, torch.device] = \"cpu\",\n    jit: bool = False,\n    force_quick_gelu: bool = False,\n    force_custom_clip: bool = False,\n    force_patch_dropout: Optional[float] = None,\n    pretrained_image: str = \"\",\n    pretrained_text: str = \"\",\n    pretrained_hf: bool = True,\n    pretrained_visual_model: str = None,\n    pretrained_text_model: str = None,\n    image_mean: Optional[Tuple[float, ...]] = None,\n    image_std: Optional[Tuple[float, ...]] = None,\n    cache_dir: Optional[str] = None,\n    skip_list: list = [],\n):\n    model = create_model(\n        model_name,\n        pretrained,\n        precision=precision,\n        device=device,\n        jit=jit,\n        force_quick_gelu=force_quick_gelu,\n        force_custom_clip=force_custom_clip,\n        force_patch_dropout=force_patch_dropout,\n        pretrained_image=pretrained_image,\n        pretrained_text=pretrained_text,\n        pretrained_hf=pretrained_hf,\n        pretrained_visual_model=pretrained_visual_model,\n        pretrained_text_model=pretrained_text_model,\n        cache_dir=cache_dir,\n        skip_list=skip_list,\n    )\n\n    image_mean = image_mean or getattr(model.visual, \"image_mean\", None)\n    image_std = image_std or getattr(model.visual, \"image_std\", None)\n    preprocess_train = image_transform(model.visual.image_size, is_train=True, mean=image_mean, std=image_std)\n    preprocess_val = image_transform(model.visual.image_size, is_train=False, mean=image_mean, std=image_std)\n\n    return model, preprocess_train, preprocess_val\n\n\ndef create_model_from_pretrained(\n    model_name: str,\n    pretrained: str,\n    precision: str = \"fp32\",\n    device: Union[str, torch.device] = \"cpu\",\n    jit: bool = False,\n    force_quick_gelu: bool = False,\n    force_custom_clip: bool = False,\n    force_patch_dropout: Optional[float] = None,\n    return_transform: bool = True,\n    image_mean: Optional[Tuple[float, ...]] = None,\n    image_std: Optional[Tuple[float, ...]] = None,\n    cache_dir: Optional[str] = None,\n    is_frozen: bool = False,\n):\n    if not is_pretrained_cfg(model_name, pretrained) and not os.path.exists(pretrained):\n        raise RuntimeError(f\"{pretrained} is not a valid pretrained cfg or checkpoint for {model_name}.\" f\" Use open_clip.list_pretrained() to find one.\")\n\n    model = create_model(\n        model_name,\n        pretrained,\n        precision=precision,\n        device=device,\n        jit=jit,\n        force_quick_gelu=force_quick_gelu,\n        force_custom_clip=force_custom_clip,\n        force_patch_dropout=force_patch_dropout,\n        cache_dir=cache_dir,\n    )\n\n    if is_frozen:\n        for param in model.parameters():\n            param.requires_grad = False\n\n    if not return_transform:\n        return model\n\n    image_mean = image_mean or getattr(model.visual, \"image_mean\", None)\n    image_std = image_std or getattr(model.visual, \"image_std\", None)\n    preprocess = image_transform(model.visual.image_size, is_train=False, mean=image_mean, std=image_std)\n\n    return model, preprocess\n"}
{"type": "source_file", "path": "llava/model/multimodal_encoder/eva_clip/eva_clip_processors.py", "content": "\"\"\"\n# Adapted from https://github.com/baaivision/EVA/tree/master/EVA-CLIP\n\"\"\"\n\nfrom torchvision import transforms\nfrom torchvision.transforms.functional import InterpolationMode\nfrom transformers.image_processing_utils import BatchFeature\nfrom PIL import Image\nfrom transformers.image_transforms import convert_to_rgb\n\n\nclass BaseProcessor:\n    def __init__(self):\n        self.transform = lambda x: x\n        return\n\n    def __call__(self, item):\n        return self.transform(item)\n\n\nclass EvaClipImageBaseProcessor(BaseProcessor):\n    def __init__(self, mean=None, std=None):\n        self.mean = (0.48145466, 0.4578275, 0.40821073) if mean is None else mean\n        self.std = (0.26862954, 0.26130258, 0.27577711) if std is None else std\n\n        self.normalize = transforms.Normalize(self.mean, self.std)\n\n    @property\n    def image_mean(self):\n        return self.mean\n\n\nclass EvaClipImageTrainProcessor(EvaClipImageBaseProcessor):\n    def __init__(self, image_size=224, mean=None, std=None, min_scale=0.5, max_scale=1.0):\n        super().__init__(mean=mean, std=std)\n\n        self.transform = transforms.Compose(\n            [\n                convert_to_rgb,\n                transforms.Resize(\n                    image_size,\n                    interpolation=InterpolationMode.BICUBIC,\n                ),\n                transforms.CenterCrop(image_size),\n                transforms.ToTensor(),\n                self.normalize,\n            ]\n        )\n\n        self.image_size = image_size\n\n    def preprocess(self, images, return_tensors):\n        if isinstance(images, Image.Image):\n            images = [images]\n        else:\n            assert isinstance(images, list)\n\n        transformed_images = [self.transform(image).numpy() for image in images]\n        data = {\"pixel_values\": transformed_images}\n\n        return BatchFeature(data=data, tensor_type=return_tensors)\n\n    def __call__(self, item):\n        return self.transform(item)\n\n    @property\n    def crop_size(self):\n        return {\"height\": self.image_size, \"width\": self.image_size}\n\n    @property\n    def size(self):\n        return {\"shortest_edge\": self.image_size}\n"}
{"type": "source_file", "path": "llava/model/multimodal_encoder/dev_eva_clip/eva_clip/transformer.py", "content": "import os\nimport logging\nfrom collections import OrderedDict\nimport math\nfrom typing import Callable, Optional, Sequence\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\ntry:\n    from timm.models.layers import trunc_normal_\nexcept:\n    from timm.layers import trunc_normal_\n\nfrom .rope import VisionRotaryEmbedding, VisionRotaryEmbeddingFast\nfrom .utils import to_2tuple\n\nif os.getenv(\"ENV_TYPE\") == \"deepspeed\":\n    try:\n        import deepspeed\n        from deepspeed.runtime.activation_checkpointing.checkpointing import checkpoint\n    except:\n        print(\"Please 'pip install deepspeed'\")\n        deepspeed = None\n        from torch.utils.checkpoint import checkpoint\nelse:\n    from torch.utils.checkpoint import checkpoint\n\ntry:\n    import xformers.ops as xops\nexcept ImportError:\n    xops = None\n    # print(\"Please 'pip install xformers'\")\n\n\nclass LayerNormFp32(nn.LayerNorm):\n    \"\"\"Subclass torch's LayerNorm to handle fp16 (by casting to float32 and back).\"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n    def forward(self, x: torch.Tensor):\n        output = F.layer_norm(\n            x.float(),\n            self.normalized_shape,\n            self.weight.float() if self.weight is not None else None,\n            self.bias.float() if self.bias is not None else None,\n            self.eps,\n        )\n        return output.type_as(x)\n\n\nclass LayerNorm(nn.LayerNorm):\n    \"\"\"Subclass torch's LayerNorm (with cast back to input dtype).\"\"\"\n\n    def forward(self, x: torch.Tensor):\n        orig_type = x.dtype\n        x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n        return x.to(orig_type)\n\n\nclass QuickGELU(nn.Module):\n    # NOTE This is slower than nn.GELU or nn.SiLU and uses more GPU memory\n    def forward(self, x: torch.Tensor):\n        return x * torch.sigmoid(1.702 * x)\n\n\nclass LayerScale(nn.Module):\n    def __init__(self, dim, init_values=1e-5, inplace=False):\n        super().__init__()\n        self.inplace = inplace\n        self.gamma = nn.Parameter(init_values * torch.ones(dim))\n\n    def forward(self, x):\n        return x.mul_(self.gamma) if self.inplace else x * self.gamma\n\n\nclass PatchDropout(nn.Module):\n    \"\"\"\n    https://arxiv.org/abs/2212.00794\n    \"\"\"\n\n    def __init__(self, prob, exclude_first_token=True):\n        super().__init__()\n        assert 0 <= prob < 1.0\n        self.prob = prob\n        self.exclude_first_token = exclude_first_token  # exclude CLS token\n        logging.info(f\"os.getenv('RoPE')={os.getenv('RoPE')}\")\n\n    def forward(self, x):\n        if not self.training or self.prob == 0.0:\n            return x\n\n        if self.exclude_first_token:\n            cls_tokens, x = x[:, :1], x[:, 1:]\n        else:\n            cls_tokens = torch.jit.annotate(torch.Tensor, x[:, :1])\n\n        batch = x.size()[0]\n        num_tokens = x.size()[1]\n\n        batch_indices = torch.arange(batch)\n        batch_indices = batch_indices[..., None]\n\n        keep_prob = 1 - self.prob\n        num_patches_keep = max(1, int(num_tokens * keep_prob))\n\n        rand = torch.randn(batch, num_tokens)\n        patch_indices_keep = rand.topk(num_patches_keep, dim=-1).indices\n\n        x = x[batch_indices, patch_indices_keep]\n\n        if self.exclude_first_token:\n            x = torch.cat((cls_tokens, x), dim=1)\n\n        if self.training and os.getenv(\"RoPE\") == \"1\":\n            return x, patch_indices_keep\n\n        return x\n\n\ndef _in_projection_packed(\n    q: torch.Tensor,\n    k: torch.Tensor,\n    v: torch.Tensor,\n    w: torch.Tensor,\n    b: Optional[torch.Tensor] = None,\n):\n    \"\"\"\n    https://github.com/pytorch/pytorch/blob/db2a237763eb8693a20788be94f8c192e762baa8/torch/nn/functional.py#L4726\n    \"\"\"\n    E = q.size(-1)\n    if k is v:\n        if q is k:\n            # self-attention\n            return F.linear(q, w, b).chunk(3, dim=-1)\n        else:\n            # encoder-decoder attention\n            w_q, w_kv = w.split([E, E * 2])\n            if b is None:\n                b_q = b_kv = None\n            else:\n                b_q, b_kv = b.split([E, E * 2])\n            return (F.linear(q, w_q, b_q),) + F.linear(k, w_kv, b_kv).chunk(2, dim=-1)\n    else:\n        w_q, w_k, w_v = w.chunk(3)\n        if b is None:\n            b_q = b_k = b_v = None\n        else:\n            b_q, b_k, b_v = b.chunk(3)\n        return F.linear(q, w_q, b_q), F.linear(k, w_k, b_k), F.linear(v, w_v, b_v)\n\n\nclass Attention(nn.Module):\n    def __init__(self, dim, num_heads=8, qkv_bias=True, scaled_cosine=False, scale_heads=False, logit_scale_max=math.log(1.0 / 0.01), attn_drop=0.0, proj_drop=0.0, xattn=False, rope=False):\n        super().__init__()\n        self.scaled_cosine = scaled_cosine\n        self.scale_heads = scale_heads\n        assert dim % num_heads == 0, \"dim should be divisible by num_heads\"\n        self.num_heads = num_heads\n        self.head_dim = dim // num_heads\n        self.scale = self.head_dim**-0.5\n        self.logit_scale_max = logit_scale_max\n\n        # keeping in_proj in this form (instead of nn.Linear) to match weight scheme of original\n        self.in_proj_weight = nn.Parameter(torch.randn((dim * 3, dim)) * self.scale)\n        if qkv_bias:\n            self.in_proj_bias = nn.Parameter(torch.zeros(dim * 3))\n        else:\n            self.in_proj_bias = None\n\n        if self.scaled_cosine:\n            self.logit_scale = nn.Parameter(torch.log(10 * torch.ones((num_heads, 1, 1))))\n        else:\n            self.logit_scale = None\n        self.attn_drop = nn.Dropout(attn_drop)\n        if self.scale_heads:\n            self.head_scale = nn.Parameter(torch.ones((num_heads, 1, 1)))\n        else:\n            self.head_scale = None\n        self.out_proj = nn.Linear(dim, dim)\n        self.out_drop = nn.Dropout(proj_drop)\n        self.xattn = xattn\n        self.xattn_drop = attn_drop\n        self.rope = rope\n\n    def forward(self, x, attn_mask: Optional[torch.Tensor] = None):\n        L, N, C = x.shape\n        q, k, v = F.linear(x, self.in_proj_weight, self.in_proj_bias).chunk(3, dim=-1)\n        if self.xattn:\n            q = q.contiguous().view(L, N, self.num_heads, -1).transpose(0, 1)\n            k = k.contiguous().view(L, N, self.num_heads, -1).transpose(0, 1)\n            v = v.contiguous().view(L, N, self.num_heads, -1).transpose(0, 1)\n\n            x = xops.memory_efficient_attention(\n                q,\n                k,\n                v,\n                p=self.xattn_drop,\n                scale=self.scale if self.logit_scale is None else None,\n                attn_bias=xops.LowerTriangularMask() if attn_mask is not None else None,\n            )\n        else:\n            q = q.contiguous().view(L, N * self.num_heads, -1).transpose(0, 1)\n            k = k.contiguous().view(L, N * self.num_heads, -1).transpose(0, 1)\n            v = v.contiguous().view(L, N * self.num_heads, -1).transpose(0, 1)\n\n            if self.logit_scale is not None:\n                attn = torch.bmm(F.normalize(q, dim=-1), F.normalize(k, dim=-1).transpose(-1, -2))\n                logit_scale = torch.clamp(self.logit_scale, max=self.logit_scale_max).exp()\n                attn = attn.view(N, self.num_heads, L, L) * logit_scale\n                attn = attn.view(-1, L, L)\n            else:\n                q = q * self.scale\n                attn = torch.bmm(q, k.transpose(-1, -2))\n\n            if attn_mask is not None:\n                if attn_mask.dtype == torch.bool:\n                    new_attn_mask = torch.zeros_like(attn_mask, dtype=q.dtype)\n                    new_attn_mask.masked_fill_(attn_mask, float(\"-inf\"))\n                    attn_mask = new_attn_mask\n                attn += attn_mask\n\n            attn = attn.softmax(dim=-1)\n            attn = self.attn_drop(attn)\n\n            x = torch.bmm(attn, v)\n\n        if self.head_scale is not None:\n            x = x.view(N, self.num_heads, L, C) * self.head_scale\n            x = x.view(-1, L, C)\n        x = x.transpose(0, 1).reshape(L, N, C)\n        x = self.out_proj(x)\n        x = self.out_drop(x)\n        return x\n\n\nclass CustomAttention(nn.Module):\n    def __init__(self, dim, num_heads=8, qkv_bias=True, scaled_cosine=True, scale_heads=False, logit_scale_max=math.log(1.0 / 0.01), attn_drop=0.0, proj_drop=0.0, xattn=False):\n        super().__init__()\n        self.scaled_cosine = scaled_cosine\n        self.scale_heads = scale_heads\n        assert dim % num_heads == 0, \"dim should be divisible by num_heads\"\n        self.num_heads = num_heads\n        self.head_dim = dim // num_heads\n        self.scale = self.head_dim**-0.5\n        self.logit_scale_max = logit_scale_max\n\n        # keeping in_proj in this form (instead of nn.Linear) to match weight scheme of original\n        self.in_proj_weight = nn.Parameter(torch.randn((dim * 3, dim)) * self.scale)\n        if qkv_bias:\n            self.in_proj_bias = nn.Parameter(torch.zeros(dim * 3))\n        else:\n            self.in_proj_bias = None\n\n        if self.scaled_cosine:\n            self.logit_scale = nn.Parameter(torch.log(10 * torch.ones((num_heads, 1, 1))))\n        else:\n            self.logit_scale = None\n        self.attn_drop = nn.Dropout(attn_drop)\n        if self.scale_heads:\n            self.head_scale = nn.Parameter(torch.ones((num_heads, 1, 1)))\n        else:\n            self.head_scale = None\n        self.out_proj = nn.Linear(dim, dim)\n        self.out_drop = nn.Dropout(proj_drop)\n        self.xattn = xattn\n        self.xattn_drop = attn_drop\n\n    def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, attn_mask: Optional[torch.Tensor] = None):\n        q, k, v = _in_projection_packed(query, key, value, self.in_proj_weight, self.in_proj_bias)\n        N_q, B_q, C_q = q.shape\n        N_k, B_k, C_k = k.shape\n        N_v, B_v, C_v = v.shape\n        if self.xattn:\n            # B, N, C -> B, N, num_heads, C\n            q = q.permute(1, 0, 2).reshape(B_q, N_q, self.num_heads, -1)\n            k = k.permute(1, 0, 2).reshape(B_k, N_k, self.num_heads, -1)\n            v = v.permute(1, 0, 2).reshape(B_v, N_v, self.num_heads, -1)\n\n            x = xops.memory_efficient_attention(q, k, v, p=self.xattn_drop, scale=self.scale if self.logit_scale is None else None, attn_bias=xops.LowerTriangularMask() if attn_mask is not None else None)\n        else:\n            # B*H, L, C\n            q = q.contiguous().view(N_q, B_q * self.num_heads, -1).transpose(0, 1)\n            k = k.contiguous().view(N_k, B_k * self.num_heads, -1).transpose(0, 1)\n            v = v.contiguous().view(N_v, B_v * self.num_heads, -1).transpose(0, 1)\n\n            if self.logit_scale is not None:\n                # B*H, N_q, N_k\n                attn = torch.bmm(F.normalize(q, dim=-1), F.normalize(k, dim=-1).transpose(-1, -2))\n                logit_scale = torch.clamp(self.logit_scale, max=self.logit_scale_max).exp()\n                attn = attn.view(B_q, self.num_heads, N_q, N_k) * logit_scale\n                attn = attn.view(-1, N_q, N_k)\n            else:\n                q = q * self.scale\n                attn = torch.bmm(q, k.transpose(-1, -2))\n\n            if attn_mask is not None:\n                if attn_mask.dtype == torch.bool:\n                    new_attn_mask = torch.zeros_like(attn_mask, dtype=q.dtype)\n                    new_attn_mask.masked_fill_(attn_mask, float(\"-inf\"))\n                    attn_mask = new_attn_mask\n                attn += attn_mask\n\n            attn = attn.softmax(dim=-1)\n            attn = self.attn_drop(attn)\n\n            x = torch.bmm(attn, v)\n\n        if self.head_scale is not None:\n            x = x.view(B_q, self.num_heads, N_q, C_q) * self.head_scale\n            x = x.view(-1, N_q, C_q)\n        x = x.transpose(0, 1).reshape(N_q, B_q, C_q)\n        x = self.out_proj(x)\n        x = self.out_drop(x)\n        return x\n\n\nclass CustomResidualAttentionBlock(nn.Module):\n    def __init__(\n        self,\n        d_model: int,\n        n_head: int,\n        mlp_ratio: float = 4.0,\n        ls_init_value: float = None,\n        act_layer: Callable = nn.GELU,\n        norm_layer: Callable = LayerNorm,\n        scale_cosine_attn: bool = False,\n        scale_heads: bool = False,\n        scale_attn: bool = False,\n        scale_fc: bool = False,\n        cross_attn: bool = False,\n        xattn: bool = False,\n    ):\n        super().__init__()\n\n        self.ln_1 = norm_layer(d_model)\n        self.ln_1_k = norm_layer(d_model) if cross_attn else self.ln_1\n        self.ln_1_v = norm_layer(d_model) if cross_attn else self.ln_1\n        self.attn = CustomAttention(d_model, n_head, qkv_bias=True, attn_drop=0.0, proj_drop=0.0, scaled_cosine=scale_cosine_attn, scale_heads=scale_heads, xattn=xattn)\n\n        self.ln_attn = norm_layer(d_model) if scale_attn else nn.Identity()\n        self.ls_1 = LayerScale(d_model, ls_init_value) if ls_init_value is not None else nn.Identity()\n\n        self.ln_2 = norm_layer(d_model)\n        mlp_width = int(d_model * mlp_ratio)\n        self.mlp = nn.Sequential(OrderedDict([(\"c_fc\", nn.Linear(d_model, mlp_width)), (\"ln\", norm_layer(mlp_width) if scale_fc else nn.Identity()), (\"gelu\", act_layer()), (\"c_proj\", nn.Linear(mlp_width, d_model))]))\n\n        self.ls_2 = LayerScale(d_model, ls_init_value) if ls_init_value is not None else nn.Identity()\n\n    def forward(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, attn_mask: Optional[torch.Tensor] = None):\n        q = q + self.ls_1(self.ln_attn(self.attn(self.ln_1(q), self.ln_1_k(k), self.ln_1_v(v), attn_mask=attn_mask)))\n        q = q + self.ls_2(self.mlp(self.ln_2(q)))\n        return q\n\n\nclass CustomTransformer(nn.Module):\n    def __init__(\n        self,\n        width: int,\n        layers: int,\n        heads: int,\n        mlp_ratio: float = 4.0,\n        ls_init_value: float = None,\n        act_layer: Callable = nn.GELU,\n        norm_layer: Callable = LayerNorm,\n        scale_cosine_attn: bool = True,\n        scale_heads: bool = False,\n        scale_attn: bool = False,\n        scale_fc: bool = False,\n        cross_attn: bool = False,\n        xattn: bool = False,\n    ):\n        super().__init__()\n        self.width = width\n        self.layers = layers\n        self.grad_checkpointing = False\n        self.xattn = xattn\n\n        self.resblocks = nn.ModuleList(\n            [\n                CustomResidualAttentionBlock(\n                    width,\n                    heads,\n                    mlp_ratio,\n                    ls_init_value=ls_init_value,\n                    act_layer=act_layer,\n                    norm_layer=norm_layer,\n                    scale_cosine_attn=scale_cosine_attn,\n                    scale_heads=scale_heads,\n                    scale_attn=scale_attn,\n                    scale_fc=scale_fc,\n                    cross_attn=cross_attn,\n                    xattn=xattn,\n                )\n                for _ in range(layers)\n            ]\n        )\n\n    def get_cast_dtype(self) -> torch.dtype:\n        return self.resblocks[0].mlp.c_fc.weight.dtype\n\n    def forward(self, q: torch.Tensor, k: torch.Tensor = None, v: torch.Tensor = None, attn_mask: Optional[torch.Tensor] = None):\n        if k is None and v is None:\n            k = v = q\n        for r in self.resblocks:\n            if self.grad_checkpointing and not torch.jit.is_scripting():\n                q = checkpoint(r, q, k, v, attn_mask)\n            else:\n                q = r(q, k, v, attn_mask=attn_mask)\n        return q\n\n\nclass ResidualAttentionBlock(nn.Module):\n    def __init__(\n        self,\n        d_model: int,\n        n_head: int,\n        mlp_ratio: float = 4.0,\n        ls_init_value: float = None,\n        act_layer: Callable = nn.GELU,\n        norm_layer: Callable = LayerNorm,\n        xattn: bool = False,\n    ):\n        super().__init__()\n\n        self.ln_1 = norm_layer(d_model)\n        if xattn:\n            self.attn = Attention(d_model, n_head, xattn=True)\n        else:\n            self.attn = nn.MultiheadAttention(d_model, n_head)\n        self.ls_1 = LayerScale(d_model, ls_init_value) if ls_init_value is not None else nn.Identity()\n\n        self.ln_2 = norm_layer(d_model)\n        mlp_width = int(d_model * mlp_ratio)\n        self.mlp = nn.Sequential(OrderedDict([(\"c_fc\", nn.Linear(d_model, mlp_width)), (\"gelu\", act_layer()), (\"c_proj\", nn.Linear(mlp_width, d_model))]))\n\n        self.ls_2 = LayerScale(d_model, ls_init_value) if ls_init_value is not None else nn.Identity()\n        self.xattn = xattn\n\n    def attention(self, x: torch.Tensor, attn_mask: Optional[torch.Tensor] = None):\n        attn_mask = attn_mask.to(x.dtype) if attn_mask is not None else None\n        if self.xattn:\n            return self.attn(x, attn_mask=attn_mask)\n        return self.attn(x, x, x, need_weights=False, attn_mask=attn_mask)[0]\n\n    def forward(self, x: torch.Tensor, attn_mask: Optional[torch.Tensor] = None):\n        x = x + self.ls_1(self.attention(self.ln_1(x), attn_mask=attn_mask))\n        x = x + self.ls_2(self.mlp(self.ln_2(x)))\n        return x\n\n\nclass Transformer(nn.Module):\n    def __init__(\n        self,\n        width: int,\n        layers: int,\n        heads: int,\n        mlp_ratio: float = 4.0,\n        ls_init_value: float = None,\n        act_layer: Callable = nn.GELU,\n        norm_layer: Callable = LayerNorm,\n        xattn: bool = False,\n    ):\n        super().__init__()\n        self.width = width\n        self.layers = layers\n        self.grad_checkpointing = False\n\n        self.resblocks = nn.ModuleList([ResidualAttentionBlock(width, heads, mlp_ratio, ls_init_value=ls_init_value, act_layer=act_layer, norm_layer=norm_layer, xattn=xattn) for _ in range(layers)])\n\n    def get_cast_dtype(self) -> torch.dtype:\n        return self.resblocks[0].mlp.c_fc.weight.dtype\n\n    def forward(self, x: torch.Tensor, attn_mask: Optional[torch.Tensor] = None):\n        for r in self.resblocks:\n            if self.grad_checkpointing and not torch.jit.is_scripting():\n                x = checkpoint(r, x, attn_mask)\n            else:\n                x = r(x, attn_mask=attn_mask)\n        return x\n\n\nclass VisionTransformer(nn.Module):\n    def __init__(\n        self,\n        image_size: int,\n        patch_size: int,\n        width: int,\n        layers: int,\n        heads: int,\n        mlp_ratio: float,\n        ls_init_value: float = None,\n        patch_dropout: float = 0.0,\n        global_average_pool: bool = False,\n        output_dim: int = 512,\n        act_layer: Callable = nn.GELU,\n        norm_layer: Callable = LayerNorm,\n        xattn: bool = False,\n    ):\n        super().__init__()\n        self.image_size = to_2tuple(image_size)\n        self.patch_size = to_2tuple(patch_size)\n        self.grid_size = (self.image_size[0] // self.patch_size[0], self.image_size[1] // self.patch_size[1])\n        self.output_dim = output_dim\n        self.conv1 = nn.Conv2d(in_channels=3, out_channels=width, kernel_size=patch_size, stride=patch_size, bias=False)\n\n        scale = width**-0.5\n        self.class_embedding = nn.Parameter(scale * torch.randn(width))\n        self.positional_embedding = nn.Parameter(scale * torch.randn(self.grid_size[0] * self.grid_size[1] + 1, width))\n\n        # setting a patch_dropout of 0. would mean it is disabled and this function would be the identity fn\n        self.patch_dropout = PatchDropout(patch_dropout) if patch_dropout > 0.0 else nn.Identity()\n        self.ln_pre = norm_layer(width)\n\n        self.transformer = Transformer(width, layers, heads, mlp_ratio, ls_init_value=ls_init_value, act_layer=act_layer, norm_layer=norm_layer, xattn=xattn)\n\n        self.global_average_pool = global_average_pool\n        self.ln_post = norm_layer(width)\n        self.proj = nn.Parameter(scale * torch.randn(width, output_dim))\n\n    def lock(self, unlocked_groups=0, freeze_bn_stats=False):\n        for param in self.parameters():\n            param.requires_grad = False\n\n        if unlocked_groups != 0:\n            groups = [\n                [\n                    self.conv1,\n                    self.class_embedding,\n                    self.positional_embedding,\n                    self.ln_pre,\n                ],\n                *self.transformer.resblocks[:-1],\n                [\n                    self.transformer.resblocks[-1],\n                    self.ln_post,\n                ],\n                self.proj,\n            ]\n\n            def _unlock(x):\n                if isinstance(x, Sequence):\n                    for g in x:\n                        _unlock(g)\n                else:\n                    if isinstance(x, torch.nn.Parameter):\n                        x.requires_grad = True\n                    else:\n                        for p in x.parameters():\n                            p.requires_grad = True\n\n            _unlock(groups[-unlocked_groups:])\n\n    def get_num_layers(self):\n        return self.transformer.layers\n\n    @torch.jit.ignore\n    def set_grad_checkpointing(self, enable=True):\n        self.transformer.grad_checkpointing = enable\n\n    @torch.jit.ignore\n    def no_weight_decay(self):\n        return {\"positional_embedding\", \"class_embedding\"}\n\n    def forward(self, x: torch.Tensor, return_all_features: bool = False):\n        x = self.conv1(x)  # shape = [*, width, grid, grid]\n        x = x.reshape(x.shape[0], x.shape[1], -1)  # shape = [*, width, grid ** 2]\n        x = x.permute(0, 2, 1)  # shape = [*, grid ** 2, width]\n        x = torch.cat([self.class_embedding.to(x.dtype) + torch.zeros(x.shape[0], 1, x.shape[-1], dtype=x.dtype, device=x.device), x], dim=1)  # shape = [*, grid ** 2 + 1, width]\n        x = x + self.positional_embedding.to(x.dtype)\n\n        # a patch_dropout of 0. would mean it is disabled and this function would do nothing but return what was passed in\n        x = self.patch_dropout(x)\n        x = self.ln_pre(x)\n\n        x = x.permute(1, 0, 2)  # NLD -> LND\n        x = self.transformer(x)\n        x = x.permute(1, 0, 2)  # LND -> NLD\n\n        if not return_all_features:\n            if self.global_average_pool:\n                x = x.mean(dim=1)  # x = x[:,1:,:].mean(dim=1)\n            else:\n                x = x[:, 0]\n\n            x = self.ln_post(x)\n\n            if self.proj is not None:\n                x = x @ self.proj\n\n        return x\n\n\nclass TextTransformer(nn.Module):\n    def __init__(\n        self,\n        context_length: int = 77,\n        vocab_size: int = 49408,\n        width: int = 512,\n        heads: int = 8,\n        layers: int = 12,\n        ls_init_value: float = None,\n        output_dim: int = 512,\n        act_layer: Callable = nn.GELU,\n        norm_layer: Callable = LayerNorm,\n        xattn: bool = False,\n        attn_mask: bool = True,\n    ):\n        super().__init__()\n        self.context_length = context_length\n        self.vocab_size = vocab_size\n        self.width = width\n        self.output_dim = output_dim\n\n        self.token_embedding = nn.Embedding(vocab_size, width)\n        self.positional_embedding = nn.Parameter(torch.empty(self.context_length, width))\n        self.transformer = Transformer(width=width, layers=layers, heads=heads, ls_init_value=ls_init_value, act_layer=act_layer, norm_layer=norm_layer, xattn=xattn)\n\n        self.xattn = xattn\n        self.ln_final = norm_layer(width)\n        self.text_projection = nn.Parameter(torch.empty(width, output_dim))\n\n        if attn_mask:\n            self.register_buffer(\"attn_mask\", self.build_attention_mask(), persistent=False)\n        else:\n            self.attn_mask = None\n\n        self.init_parameters()\n\n    def init_parameters(self):\n        nn.init.normal_(self.token_embedding.weight, std=0.02)\n        nn.init.normal_(self.positional_embedding, std=0.01)\n\n        proj_std = (self.transformer.width**-0.5) * ((2 * self.transformer.layers) ** -0.5)\n        attn_std = self.transformer.width**-0.5\n        fc_std = (2 * self.transformer.width) ** -0.5\n        for block in self.transformer.resblocks:\n            nn.init.normal_(block.attn.in_proj_weight, std=attn_std)\n            nn.init.normal_(block.attn.out_proj.weight, std=proj_std)\n            nn.init.normal_(block.mlp.c_fc.weight, std=fc_std)\n            nn.init.normal_(block.mlp.c_proj.weight, std=proj_std)\n\n        if self.text_projection is not None:\n            nn.init.normal_(self.text_projection, std=self.transformer.width**-0.5)\n\n    @torch.jit.ignore\n    def set_grad_checkpointing(self, enable=True):\n        self.transformer.grad_checkpointing = enable\n\n    @torch.jit.ignore\n    def no_weight_decay(self):\n        # return {'positional_embedding', 'token_embedding'}\n        return {\"positional_embedding\"}\n\n    def get_num_layers(self):\n        return self.transformer.layers\n\n    def build_attention_mask(self):\n        # lazily create causal attention mask, with full attention between the vision tokens\n        # pytorch uses additive attention mask; fill with -inf\n        mask = torch.empty(self.context_length, self.context_length)\n        mask.fill_(float(\"-inf\"))\n        mask.triu_(1)  # zero out the lower diagonal\n        return mask\n\n    def forward(self, text, return_all_features: bool = False):\n        cast_dtype = self.transformer.get_cast_dtype()\n        x = self.token_embedding(text).to(cast_dtype)  # [batch_size, n_ctx, d_model]\n\n        x = x + self.positional_embedding.to(cast_dtype)\n        x = x.permute(1, 0, 2)  # NLD -> LND\n        x = self.transformer(x, attn_mask=self.attn_mask)\n        # x = self.transformer(x) # no attention mask is applied\n        x = x.permute(1, 0, 2)  # LND -> NLD\n        x = self.ln_final(x)\n\n        if not return_all_features:\n            # x.shape = [batch_size, n_ctx, transformer.width]\n            # take features from the eot embedding (eot_token is the highest number in each sequence)\n            x = x[torch.arange(x.shape[0]), text.argmax(dim=-1)] @ self.text_projection\n        return x\n"}
{"type": "source_file", "path": "llava/model/multimodal_encoder/dev_eva_clip/eva_clip/eva_vit_model.py", "content": "# --------------------------------------------------------\n# Adapted from  https://github.com/microsoft/unilm/tree/master/beit\n# --------------------------------------------------------\nimport math\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\ntry:\n    from timm.models.layers import drop_path, to_2tuple, trunc_normal_\nexcept:\n    from timm.layers import drop_path, to_2tuple, trunc_normal_\n\nfrom .transformer import PatchDropout\nfrom .rope import VisionRotaryEmbedding, VisionRotaryEmbeddingFast\n\nif os.getenv(\"ENV_TYPE\") == \"deepspeed\":\n    try:\n        from deepspeed.runtime.activation_checkpointing.checkpointing import checkpoint\n    except:\n        from torch.utils.checkpoint import checkpoint\nelse:\n    from torch.utils.checkpoint import checkpoint\n\ntry:\n    import xformers.ops as xops\nexcept ImportError:\n    xops = None\n    # print(\"Please 'pip install xformers'\")\n\n\nclass DropPath(nn.Module):\n    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\"\"\"\n\n    def __init__(self, drop_prob=None):\n        super(DropPath, self).__init__()\n        self.drop_prob = drop_prob\n\n    def forward(self, x):\n        return drop_path(x, self.drop_prob, self.training)\n\n    def extra_repr(self) -> str:\n        return \"p={}\".format(self.drop_prob)\n\n\nclass Mlp(nn.Module):\n    def __init__(\n        self,\n        in_features,\n        hidden_features=None,\n        out_features=None,\n        act_layer=nn.GELU,\n        norm_layer=nn.LayerNorm,\n        drop=0.0,\n        subln=False,\n    ):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.act = act_layer()\n\n        self.ffn_ln = norm_layer(hidden_features) if subln else nn.Identity()\n\n        self.fc2 = nn.Linear(hidden_features, out_features)\n        self.drop = nn.Dropout(drop)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        # x = self.drop(x)\n        # commit this for the orignal BERT implement\n        x = self.ffn_ln(x)\n\n        x = self.fc2(x)\n        x = self.drop(x)\n        return x\n\n\nclass SwiGLU(nn.Module):\n    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.SiLU, drop=0.0, norm_layer=nn.LayerNorm, subln=False):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n\n        self.w1 = nn.Linear(in_features, hidden_features)\n        self.w2 = nn.Linear(in_features, hidden_features)\n\n        self.act = act_layer()\n        self.ffn_ln = norm_layer(hidden_features) if subln else nn.Identity()\n        self.w3 = nn.Linear(hidden_features, out_features)\n\n        self.drop = nn.Dropout(drop)\n\n    def forward(self, x):\n        x1 = self.w1(x)\n        x2 = self.w2(x)\n        hidden = self.act(x1) * x2\n        x = self.ffn_ln(hidden)\n        x = self.w3(x)\n        x = self.drop(x)\n        return x\n\n\nclass Attention(nn.Module):\n    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0.0, proj_drop=0.0, window_size=None, attn_head_dim=None, xattn=False, rope=None, subln=False, norm_layer=nn.LayerNorm):\n        super().__init__()\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        if attn_head_dim is not None:\n            head_dim = attn_head_dim\n        all_head_dim = head_dim * self.num_heads\n        self.scale = qk_scale or head_dim**-0.5\n\n        self.subln = subln\n        if self.subln:\n            self.q_proj = nn.Linear(dim, all_head_dim, bias=False)\n            self.k_proj = nn.Linear(dim, all_head_dim, bias=False)\n            self.v_proj = nn.Linear(dim, all_head_dim, bias=False)\n        else:\n            self.qkv = nn.Linear(dim, all_head_dim * 3, bias=False)\n\n        if qkv_bias:\n            self.q_bias = nn.Parameter(torch.zeros(all_head_dim))\n            self.v_bias = nn.Parameter(torch.zeros(all_head_dim))\n        else:\n            self.q_bias = None\n            self.v_bias = None\n\n        if window_size:\n            self.window_size = window_size\n            self.num_relative_distance = (2 * window_size[0] - 1) * (2 * window_size[1] - 1) + 3\n            self.relative_position_bias_table = nn.Parameter(torch.zeros(self.num_relative_distance, num_heads))  # 2*Wh-1 * 2*Ww-1, nH\n            # cls to token & token 2 cls & cls to cls\n\n            # get pair-wise relative position index for each token inside the window\n            coords_h = torch.arange(window_size[0])\n            coords_w = torch.arange(window_size[1])\n            coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n            coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n            relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n            relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n            relative_coords[:, :, 0] += window_size[0] - 1  # shift to start from 0\n            relative_coords[:, :, 1] += window_size[1] - 1\n            relative_coords[:, :, 0] *= 2 * window_size[1] - 1\n            relative_position_index = torch.zeros(size=(window_size[0] * window_size[1] + 1,) * 2, dtype=relative_coords.dtype)\n            relative_position_index[1:, 1:] = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n            relative_position_index[0, 0:] = self.num_relative_distance - 3\n            relative_position_index[0:, 0] = self.num_relative_distance - 2\n            relative_position_index[0, 0] = self.num_relative_distance - 1\n\n            self.register_buffer(\"relative_position_index\", relative_position_index)\n        else:\n            self.window_size = None\n            self.relative_position_bias_table = None\n            self.relative_position_index = None\n\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.inner_attn_ln = norm_layer(all_head_dim) if subln else nn.Identity()\n        # self.proj = nn.Linear(all_head_dim, all_head_dim)\n        self.proj = nn.Linear(all_head_dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n        self.xattn = xattn\n        self.xattn_drop = attn_drop\n\n        self.rope = rope\n\n    def forward(self, x, rel_pos_bias=None, attn_mask=None):\n        B, N, C = x.shape\n        if self.subln:\n            q = F.linear(input=x, weight=self.q_proj.weight, bias=self.q_bias)\n            k = F.linear(input=x, weight=self.k_proj.weight, bias=None)\n            v = F.linear(input=x, weight=self.v_proj.weight, bias=self.v_bias)\n\n            q = q.reshape(B, N, self.num_heads, -1).permute(0, 2, 1, 3)  # B, num_heads, N, C\n            k = k.reshape(B, N, self.num_heads, -1).permute(0, 2, 1, 3)\n            v = v.reshape(B, N, self.num_heads, -1).permute(0, 2, 1, 3)\n        else:\n\n            qkv_bias = None\n            if self.q_bias is not None:\n                qkv_bias = torch.cat((self.q_bias, torch.zeros_like(self.v_bias, requires_grad=False), self.v_bias))\n\n            qkv = F.linear(input=x, weight=self.qkv.weight, bias=qkv_bias)\n            qkv = qkv.reshape(B, N, 3, self.num_heads, -1).permute(2, 0, 3, 1, 4)  # 3, B, num_heads, N, C\n            q, k, v = qkv[0], qkv[1], qkv[2]\n\n        if self.rope:\n            # slightly fast impl\n            q_t = q[:, :, 1:, :]\n            ro_q_t = self.rope(q_t)\n            q = torch.cat((q[:, :, :1, :], ro_q_t), -2).type_as(v)\n\n            k_t = k[:, :, 1:, :]\n            ro_k_t = self.rope(k_t)\n            k = torch.cat((k[:, :, :1, :], ro_k_t), -2).type_as(v)\n\n        if self.xattn:\n            q = q.permute(0, 2, 1, 3)  # B, num_heads, N, C -> B, N, num_heads, C\n            k = k.permute(0, 2, 1, 3)\n            v = v.permute(0, 2, 1, 3)\n\n            x = xops.memory_efficient_attention(\n                q,\n                k,\n                v,\n                p=self.xattn_drop,\n                scale=self.scale,\n            )\n            x = x.reshape(B, N, -1)\n            x = self.inner_attn_ln(x)\n            x = self.proj(x)\n            x = self.proj_drop(x)\n        else:\n            q = q * self.scale\n            attn = q @ k.transpose(-2, -1)\n\n            if self.relative_position_bias_table is not None:\n                relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(self.window_size[0] * self.window_size[1] + 1, self.window_size[0] * self.window_size[1] + 1, -1)  # Wh*Ww,Wh*Ww,nH\n                relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n                attn = attn + relative_position_bias.unsqueeze(0).type_as(attn)\n\n            if rel_pos_bias is not None:\n                attn = attn + rel_pos_bias.type_as(attn)\n\n            if attn_mask is not None:\n                attn_mask = attn_mask.bool()\n                attn = attn.masked_fill(~attn_mask[:, None, None, :], float(\"-inf\"))\n\n            attn = attn.softmax(dim=-1)\n            attn = self.attn_drop(attn)\n\n            x = (attn @ v).transpose(1, 2).reshape(B, N, -1)\n            x = self.inner_attn_ln(x)\n            x = self.proj(x)\n            x = self.proj_drop(x)\n        return x\n\n\nclass Block(nn.Module):\n\n    def __init__(\n        self,\n        dim,\n        num_heads,\n        mlp_ratio=4.0,\n        qkv_bias=False,\n        qk_scale=None,\n        drop=0.0,\n        attn_drop=0.0,\n        drop_path=0.0,\n        init_values=None,\n        act_layer=nn.GELU,\n        norm_layer=nn.LayerNorm,\n        window_size=None,\n        attn_head_dim=None,\n        xattn=False,\n        rope=None,\n        postnorm=False,\n        subln=False,\n        naiveswiglu=False,\n    ):\n        super().__init__()\n        self.norm1 = norm_layer(dim)\n        self.attn = Attention(\n            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop, window_size=window_size, attn_head_dim=attn_head_dim, xattn=xattn, rope=rope, subln=subln, norm_layer=norm_layer\n        )\n        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n        self.drop_path = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n        self.norm2 = norm_layer(dim)\n        mlp_hidden_dim = int(dim * mlp_ratio)\n\n        if naiveswiglu:\n            self.mlp = SwiGLU(\n                in_features=dim,\n                hidden_features=mlp_hidden_dim,\n                subln=subln,\n                norm_layer=norm_layer,\n            )\n        else:\n            self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, subln=subln, drop=drop)\n\n        if init_values is not None and init_values > 0:\n            self.gamma_1 = nn.Parameter(init_values * torch.ones((dim)), requires_grad=True)\n            self.gamma_2 = nn.Parameter(init_values * torch.ones((dim)), requires_grad=True)\n        else:\n            self.gamma_1, self.gamma_2 = None, None\n\n        self.postnorm = postnorm\n\n    def forward(self, x, rel_pos_bias=None, attn_mask=None):\n        if self.gamma_1 is None:\n            if self.postnorm:\n                x = x + self.drop_path(self.norm1(self.attn(x, rel_pos_bias=rel_pos_bias, attn_mask=attn_mask)))\n                x = x + self.drop_path(self.norm2(self.mlp(x)))\n            else:\n                x = x + self.drop_path(self.attn(self.norm1(x), rel_pos_bias=rel_pos_bias, attn_mask=attn_mask))\n                x = x + self.drop_path(self.mlp(self.norm2(x)))\n        else:\n            if self.postnorm:\n                x = x + self.drop_path(self.gamma_1 * self.norm1(self.attn(x, rel_pos_bias=rel_pos_bias, attn_mask=attn_mask)))\n                x = x + self.drop_path(self.gamma_2 * self.norm2(self.mlp(x)))\n            else:\n                x = x + self.drop_path(self.gamma_1 * self.attn(self.norm1(x), rel_pos_bias=rel_pos_bias, attn_mask=attn_mask))\n                x = x + self.drop_path(self.gamma_2 * self.mlp(self.norm2(x)))\n        return x\n\n\nclass PatchEmbed(nn.Module):\n    \"\"\"Image to Patch Embedding\"\"\"\n\n    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n        super().__init__()\n        img_size = to_2tuple(img_size)\n        patch_size = to_2tuple(patch_size)\n        num_patches = (img_size[1] // patch_size[1]) * (img_size[0] // patch_size[0])\n        self.patch_shape = (img_size[0] // patch_size[0], img_size[1] // patch_size[1])\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.num_patches = num_patches\n\n        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n\n    def forward(self, x, **kwargs):\n        B, C, H, W = x.shape\n        # FIXME look at relaxing size constraints\n        assert H == self.img_size[0] and W == self.img_size[1], f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n        x = self.proj(x).flatten(2).transpose(1, 2)\n        return x\n\n\nclass RelativePositionBias(nn.Module):\n\n    def __init__(self, window_size, num_heads):\n        super().__init__()\n        self.window_size = window_size\n        self.num_relative_distance = (2 * window_size[0] - 1) * (2 * window_size[1] - 1) + 3\n        self.relative_position_bias_table = nn.Parameter(torch.zeros(self.num_relative_distance, num_heads))  # 2*Wh-1 * 2*Ww-1, nH\n        # cls to token & token 2 cls & cls to cls\n\n        # get pair-wise relative position index for each token inside the window\n        coords_h = torch.arange(window_size[0])\n        coords_w = torch.arange(window_size[1])\n        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n        coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n        relative_coords[:, :, 0] += window_size[0] - 1  # shift to start from 0\n        relative_coords[:, :, 1] += window_size[1] - 1\n        relative_coords[:, :, 0] *= 2 * window_size[1] - 1\n        relative_position_index = torch.zeros(size=(window_size[0] * window_size[1] + 1,) * 2, dtype=relative_coords.dtype)\n        relative_position_index[1:, 1:] = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n        relative_position_index[0, 0:] = self.num_relative_distance - 3\n        relative_position_index[0:, 0] = self.num_relative_distance - 2\n        relative_position_index[0, 0] = self.num_relative_distance - 1\n\n        self.register_buffer(\"relative_position_index\", relative_position_index)\n\n    def forward(self):\n        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(self.window_size[0] * self.window_size[1] + 1, self.window_size[0] * self.window_size[1] + 1, -1)  # Wh*Ww,Wh*Ww,nH\n        return relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n\n\nclass EVAVisionTransformer(nn.Module):\n    \"\"\"Vision Transformer with support for patch or hybrid CNN input stage\"\"\"\n\n    def __init__(\n        self,\n        img_size=224,\n        patch_size=16,\n        in_chans=3,\n        num_classes=1000,\n        embed_dim=768,\n        depth=12,\n        num_heads=12,\n        mlp_ratio=4.0,\n        qkv_bias=False,\n        qk_scale=None,\n        drop_rate=0.0,\n        attn_drop_rate=0.0,\n        drop_path_rate=0.0,\n        norm_layer=nn.LayerNorm,\n        init_values=None,\n        patch_dropout=0.0,\n        use_abs_pos_emb=True,\n        use_rel_pos_bias=False,\n        use_shared_rel_pos_bias=False,\n        rope=False,\n        use_mean_pooling=True,\n        init_scale=0.001,\n        grad_checkpointing=False,\n        xattn=False,\n        postnorm=False,\n        pt_hw_seq_len=16,\n        intp_freq=False,\n        naiveswiglu=False,\n        subln=False,\n    ):\n        super().__init__()\n        self.image_size = img_size\n        self.num_classes = num_classes\n        self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models\n\n        self.patch_embed = PatchEmbed(img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n        num_patches = self.patch_embed.num_patches\n\n        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n        # self.mask_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n        if use_abs_pos_emb:\n            self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n        else:\n            self.pos_embed = None\n        self.pos_drop = nn.Dropout(p=drop_rate)\n\n        if use_shared_rel_pos_bias:\n            self.rel_pos_bias = RelativePositionBias(window_size=self.patch_embed.patch_shape, num_heads=num_heads)\n        else:\n            self.rel_pos_bias = None\n\n        if rope:\n            half_head_dim = embed_dim // num_heads // 2\n            hw_seq_len = img_size // patch_size\n            self.rope = VisionRotaryEmbeddingFast(\n                dim=half_head_dim,\n                pt_seq_len=pt_hw_seq_len,\n                ft_seq_len=hw_seq_len if intp_freq else None,\n                # patch_dropout=patch_dropout\n            )\n        else:\n            self.rope = None\n\n        self.naiveswiglu = naiveswiglu\n\n        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n        self.use_rel_pos_bias = use_rel_pos_bias\n        self.blocks = nn.ModuleList(\n            [\n                Block(\n                    dim=embed_dim,\n                    num_heads=num_heads,\n                    mlp_ratio=mlp_ratio,\n                    qkv_bias=qkv_bias,\n                    qk_scale=qk_scale,\n                    drop=drop_rate,\n                    attn_drop=attn_drop_rate,\n                    drop_path=dpr[i],\n                    norm_layer=norm_layer,\n                    init_values=init_values,\n                    window_size=self.patch_embed.patch_shape if use_rel_pos_bias else None,\n                    xattn=xattn,\n                    rope=self.rope,\n                    postnorm=postnorm,\n                    subln=subln,\n                    naiveswiglu=naiveswiglu,\n                )\n                for i in range(depth)\n            ]\n        )\n        self.norm = nn.Identity() if use_mean_pooling else norm_layer(embed_dim)\n        self.fc_norm = norm_layer(embed_dim) if use_mean_pooling else None\n        self.head = nn.Linear(embed_dim, num_classes, bias=qkv_bias) if num_classes > 0 else nn.Identity()\n\n        if self.pos_embed is not None:\n            trunc_normal_(self.pos_embed, std=0.02)\n\n        trunc_normal_(self.cls_token, std=0.02)\n\n        self.apply(self._init_weights)\n        self.fix_init_weight()\n\n        if isinstance(self.head, nn.Linear):\n            trunc_normal_(self.head.weight, std=0.02)\n            self.head.weight.data.mul_(init_scale)\n            if self.head.bias is not None:\n                self.head.bias.data.mul_(init_scale)\n\n        # setting a patch_dropout of 0. would mean it is disabled and this function would be the identity fn\n        self.patch_dropout = PatchDropout(patch_dropout) if patch_dropout > 0.0 else nn.Identity()\n\n        self.grad_checkpointing = grad_checkpointing\n\n    def fix_init_weight(self):\n        def rescale(param, layer_id):\n            param.div_(math.sqrt(2.0 * layer_id))\n\n        for layer_id, layer in enumerate(self.blocks):\n            rescale(layer.attn.proj.weight.data, layer_id + 1)\n            if self.naiveswiglu:\n                rescale(layer.mlp.w3.weight.data, layer_id + 1)\n            else:\n                rescale(layer.mlp.fc2.weight.data, layer_id + 1)\n\n    def get_cast_dtype(self) -> torch.dtype:\n        return self.blocks[0].mlp.fc2.weight.dtype\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            trunc_normal_(m.weight, std=0.02)\n            if m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.LayerNorm):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)\n\n    def get_num_layers(self):\n        return len(self.blocks)\n\n    def lock(self, unlocked_groups=0, freeze_bn_stats=False):\n        assert unlocked_groups == 0, \"partial locking not currently supported for this model\"\n        for param in self.parameters():\n            param.requires_grad = False\n\n    @torch.jit.ignore\n    def set_grad_checkpointing(self, enable=True):\n        self.grad_checkpointing = enable\n\n    @torch.jit.ignore\n    def no_weight_decay(self):\n        return {\"pos_embed\", \"cls_token\"}\n\n    def get_classifier(self):\n        return self.head\n\n    def reset_classifier(self, num_classes, global_pool=\"\"):\n        self.num_classes = num_classes\n        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n\n    def forward_features(self, x, return_all_features=False):\n\n        x = self.patch_embed(x)\n        batch_size, seq_len, _ = x.size()\n\n        cls_tokens = self.cls_token.expand(batch_size, -1, -1)  # stole cls_tokens impl from Phil Wang, thanks\n        x = torch.cat((cls_tokens, x), dim=1)\n        if self.pos_embed is not None:\n            x = x + self.pos_embed\n        x = self.pos_drop(x)\n\n        # a patch_dropout of 0. would mean it is disabled and this function would do nothing but return what was passed in\n        # if os.getenv(\"RoPE\") == \"1\":\n        #     if self.training and not isinstance(self.patch_dropout, nn.Identity):\n        #         x, patch_indices_keep = self.patch_dropout(x)\n        #         self.rope.forward = partial(self.rope.forward, patch_indices_keep=patch_indices_keep)\n        #     else:\n        #         self.rope.forward = partial(self.rope.forward, patch_indices_keep=None)\n        #         x = self.patch_dropout(x)\n        # else:\n        x = self.patch_dropout(x)\n\n        rel_pos_bias = self.rel_pos_bias() if self.rel_pos_bias is not None else None\n        for blk in self.blocks:\n            if self.grad_checkpointing:\n                x = checkpoint(blk, x, (rel_pos_bias,))\n            else:\n                x = blk(x, rel_pos_bias=rel_pos_bias)\n\n        if not return_all_features:\n            x = self.norm(x)\n            if self.fc_norm is not None:\n                return self.fc_norm(x.mean(1))\n            else:\n                return x[:, 0]\n        return x\n\n    def forward(self, x, return_all_features=False):\n        if return_all_features:\n            return self.forward_features(x, return_all_features)\n        x = self.forward_features(x)\n        x = self.head(x)\n        return x\n"}
{"type": "source_file", "path": "llava/model/multimodal_encoder/dev_eva_clip/eva_clip/transform.py", "content": "from typing import Optional, Sequence, Tuple\n\nimport torch\nimport torch.nn as nn\nimport torchvision.transforms.functional as F\n\nfrom torchvision.transforms import Normalize, Compose, RandomResizedCrop, InterpolationMode, ToTensor, Resize, CenterCrop\n\nfrom .constants import OPENAI_DATASET_MEAN, OPENAI_DATASET_STD\n\n\nclass ResizeMaxSize(nn.Module):\n\n    def __init__(self, max_size, interpolation=InterpolationMode.BICUBIC, fn=\"max\", fill=0):\n        super().__init__()\n        if not isinstance(max_size, int):\n            raise TypeError(f\"Size should be int. Got {type(max_size)}\")\n        self.max_size = max_size\n        self.interpolation = interpolation\n        self.fn = min if fn == \"min\" else min\n        self.fill = fill\n\n    def forward(self, img):\n        if isinstance(img, torch.Tensor):\n            height, width = img.shape[:2]\n        else:\n            width, height = img.size\n        scale = self.max_size / float(max(height, width))\n        if scale != 1.0:\n            new_size = tuple(round(dim * scale) for dim in (height, width))\n            img = F.resize(img, new_size, self.interpolation)\n            pad_h = self.max_size - new_size[0]\n            pad_w = self.max_size - new_size[1]\n            img = F.pad(img, padding=[pad_w // 2, pad_h // 2, pad_w - pad_w // 2, pad_h - pad_h // 2], fill=self.fill)\n        return img\n\n\ndef _convert_to_rgb(image):\n    return image.convert(\"RGB\")\n\n\n# class CatGen(nn.Module):\n#     def __init__(self, num=4):\n#         self.num = num\n#     def mixgen_batch(image, text):\n#         batch_size = image.shape[0]\n#         index = np.random.permutation(batch_size)\n\n#         cat_images = []\n#         for i in range(batch_size):\n#             # image mixup\n#             image[i,:] = lam * image[i,:] + (1 - lam) * image[index[i],:]\n#             # text concat\n#             text[i] = tokenizer((str(text[i]) + \" \" + str(text[index[i]])))[0]\n#         text = torch.stack(text)\n#         return image, text\n\n\ndef image_transform(\n    image_size: int,\n    is_train: bool,\n    mean: Optional[Tuple[float, ...]] = None,\n    std: Optional[Tuple[float, ...]] = None,\n    resize_longest_max: bool = False,\n    fill_color: int = 0,\n):\n    mean = mean or OPENAI_DATASET_MEAN\n    if not isinstance(mean, (list, tuple)):\n        mean = (mean,) * 3\n\n    std = std or OPENAI_DATASET_STD\n    if not isinstance(std, (list, tuple)):\n        std = (std,) * 3\n\n    if isinstance(image_size, (list, tuple)) and image_size[0] == image_size[1]:\n        # for square size, pass size as int so that Resize() uses aspect preserving shortest edge\n        image_size = image_size[0]\n\n    normalize = Normalize(mean=mean, std=std)\n    if is_train:\n        return Compose(\n            [\n                RandomResizedCrop(image_size, scale=(0.9, 1.0), interpolation=InterpolationMode.BICUBIC),\n                _convert_to_rgb,\n                ToTensor(),\n                normalize,\n            ]\n        )\n    else:\n        if resize_longest_max:\n            transforms = [ResizeMaxSize(image_size, fill=fill_color)]\n        else:\n            transforms = [\n                Resize(image_size, interpolation=InterpolationMode.BICUBIC),\n                CenterCrop(image_size),\n            ]\n        transforms.extend(\n            [\n                _convert_to_rgb,\n                ToTensor(),\n                normalize,\n            ]\n        )\n        return Compose(transforms)\n"}
{"type": "source_file", "path": "llava/model/multimodal_encoder/eva_clip/eva_vit.py", "content": "\"\"\"\n# Adapted from https://github.com/baaivision/EVA/tree/master/EVA-CLIP\n\"\"\"\n\nfrom math import pi\nimport torch\nfrom torch import nn\nfrom einops import rearrange, repeat\nimport logging\nfrom llava.utils import rank0_print\n\n\ndef broadcat(tensors, dim=-1):\n    num_tensors = len(tensors)\n    shape_lens = set(list(map(lambda t: len(t.shape), tensors)))\n    assert len(shape_lens) == 1, \"tensors must all have the same number of dimensions\"\n    shape_len = list(shape_lens)[0]\n    dim = (dim + shape_len) if dim < 0 else dim\n    dims = list(zip(*map(lambda t: list(t.shape), tensors)))\n    expandable_dims = [(i, val) for i, val in enumerate(dims) if i != dim]\n    assert all([*map(lambda t: len(set(t[1])) <= 2, expandable_dims)]), \"invalid dimensions for broadcastable concatentation\"\n    max_dims = list(map(lambda t: (t[0], max(t[1])), expandable_dims))\n    expanded_dims = list(map(lambda t: (t[0], (t[1],) * num_tensors), max_dims))\n    expanded_dims.insert(dim, (dim, dims[dim]))\n    expandable_shapes = list(zip(*map(lambda t: t[1], expanded_dims)))\n    tensors = list(map(lambda t: t[0].expand(*t[1]), zip(tensors, expandable_shapes)))\n    return torch.cat(tensors, dim=dim)\n\n\ndef rotate_half(x):\n    x = rearrange(x, \"... (d r) -> ... d r\", r=2)\n    x1, x2 = x.unbind(dim=-1)\n    x = torch.stack((-x2, x1), dim=-1)\n    return rearrange(x, \"... d r -> ... (d r)\")\n\n\nclass VisionRotaryEmbeddingFast(nn.Module):\n    def __init__(self, dim, pt_seq_len, ft_seq_len=None, custom_freqs=None, freqs_for=\"lang\", theta=10000, max_freq=10, num_freqs=1, patch_dropout=0.0):\n        super().__init__()\n        if custom_freqs:\n            freqs = custom_freqs\n        elif freqs_for == \"lang\":\n            freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n        elif freqs_for == \"pixel\":\n            freqs = torch.linspace(1.0, max_freq / 2, dim // 2) * pi\n        elif freqs_for == \"constant\":\n            freqs = torch.ones(num_freqs).float()\n        else:\n            raise ValueError(f\"unknown modality {freqs_for}\")\n\n        if ft_seq_len is None:\n            ft_seq_len = pt_seq_len\n        t = torch.arange(ft_seq_len) / ft_seq_len * pt_seq_len\n\n        freqs = torch.einsum(\"..., f -> ... f\", t, freqs)\n        freqs = repeat(freqs, \"... n -> ... (n r)\", r=2)\n        freqs = broadcat((freqs[:, None, :], freqs[None, :, :]), dim=-1)\n\n        freqs_cos = freqs.cos().view(-1, freqs.shape[-1])\n        freqs_sin = freqs.sin().view(-1, freqs.shape[-1])\n\n        self.patch_dropout = patch_dropout\n\n        self.register_buffer(\"freqs_cos\", freqs_cos)\n        self.register_buffer(\"freqs_sin\", freqs_sin)\n\n        logging.info(f\"Shape of rope freq: {self.freqs_cos.shape}\")\n\n    def forward(self, t, patch_indices_keep=None):\n        if patch_indices_keep is not None:\n            batch = t.size()[0]\n            batch_indices = torch.arange(batch)\n            batch_indices = batch_indices[..., None]\n\n            freqs_cos = repeat(self.freqs_cos, \"i j -> n i m j\", n=t.shape[0], m=t.shape[1])\n            freqs_sin = repeat(self.freqs_sin, \"i j -> n i m j\", n=t.shape[0], m=t.shape[1])\n\n            freqs_cos = freqs_cos[batch_indices, patch_indices_keep]\n            freqs_cos = rearrange(freqs_cos, \"n i m j -> n m i j\")\n            freqs_sin = freqs_sin[batch_indices, patch_indices_keep]\n            freqs_sin = rearrange(freqs_sin, \"n i m j -> n m i j\")\n\n            return t * freqs_cos + rotate_half(t) * freqs_sin\n\n        return t * self.freqs_cos + rotate_half(t) * self.freqs_sin\n\n\nclass LayerNorm(nn.LayerNorm):\n    \"\"\"Subclass torch's LayerNorm (with cast back to input dtype).\"\"\"\n\n    def forward(self, x: torch.Tensor):\n        orig_type = x.dtype\n        x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n        return x.to(orig_type)\n\n\nclass PatchDropout(nn.Module):\n    \"\"\"\n    https://arxiv.org/abs/2212.00794\n    \"\"\"\n\n    def __init__(self, prob, exclude_first_token=True):\n        super().__init__()\n        assert 0 <= prob < 1.0\n        self.prob = prob\n        self.exclude_first_token = exclude_first_token  # exclude CLS token\n        logging.info(f\"os.getenv('RoPE')={os.getenv('RoPE')}\")\n\n    def forward(self, x):\n        if not self.training or self.prob == 0.0:\n            return x\n\n        if self.exclude_first_token:\n            cls_tokens, x = x[:, :1], x[:, 1:]\n        else:\n            cls_tokens = torch.jit.annotate(torch.Tensor, x[:, :1])\n\n        batch = x.size()[0]\n        num_tokens = x.size()[1]\n\n        batch_indices = torch.arange(batch)\n        batch_indices = batch_indices[..., None]\n\n        keep_prob = 1 - self.prob\n        num_patches_keep = max(1, int(num_tokens * keep_prob))\n\n        rand = torch.randn(batch, num_tokens)\n        patch_indices_keep = rand.topk(num_patches_keep, dim=-1).indices\n\n        x = x[batch_indices, patch_indices_keep]\n\n        if self.exclude_first_token:\n            x = torch.cat((cls_tokens, x), dim=1)\n\n        if self.training and os.getenv(\"RoPE\") == \"1\":\n            return x, patch_indices_keep\n\n        return x\n\n\n# --------------------------------------------------------\n# Adapted from  https://github.com/microsoft/unilm/tree/master/beit\n# --------------------------------------------------------\nimport math\nimport os\nimport torch.nn as nn\nimport torch.nn.functional as F\n\ntry:\n    from timm.models.layers import drop_path, to_2tuple, trunc_normal_\nexcept:\n    from timm.layers import drop_path, to_2tuple, trunc_normal_\n\nif os.getenv(\"ENV_TYPE\") == \"deepspeed\":\n    try:\n        from deepspeed.runtime.activation_checkpointing.checkpointing import checkpoint\n    except:\n        from torch.utils.checkpoint import checkpoint\nelse:\n    from torch.utils.checkpoint import checkpoint\n\ntry:\n    import xformers.ops as xops\nexcept ImportError:\n    xops = None\n    # print(\"Please 'pip install xformers'\")\n\n\nclass DropPath(nn.Module):\n    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\"\"\"\n\n    def __init__(self, drop_prob=None):\n        super(DropPath, self).__init__()\n        self.drop_prob = drop_prob\n\n    def forward(self, x):\n        return drop_path(x, self.drop_prob, self.training)\n\n    def extra_repr(self) -> str:\n        return \"p={}\".format(self.drop_prob)\n\n\nclass Mlp(nn.Module):\n    def __init__(\n        self,\n        in_features,\n        hidden_features=None,\n        out_features=None,\n        act_layer=nn.GELU,\n        norm_layer=nn.LayerNorm,\n        drop=0.0,\n        subln=False,\n    ):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.act = act_layer()\n\n        self.ffn_ln = norm_layer(hidden_features) if subln else nn.Identity()\n\n        self.fc2 = nn.Linear(hidden_features, out_features)\n        self.drop = nn.Dropout(drop)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        # x = self.drop(x)\n        # commit this for the orignal BERT implement\n        x = self.ffn_ln(x)\n\n        x = self.fc2(x)\n        x = self.drop(x)\n        return x\n\n\nclass SwiGLU(nn.Module):\n    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.SiLU, drop=0.0, norm_layer=nn.LayerNorm, subln=False):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n\n        self.w1 = nn.Linear(in_features, hidden_features)\n        self.w2 = nn.Linear(in_features, hidden_features)\n\n        self.act = act_layer()\n        self.ffn_ln = norm_layer(hidden_features) if subln else nn.Identity()\n        self.w3 = nn.Linear(hidden_features, out_features)\n\n        self.drop = nn.Dropout(drop)\n\n    def forward(self, x):\n        x1 = self.w1(x)\n        x2 = self.w2(x)\n        hidden = self.act(x1) * x2\n        x = self.ffn_ln(hidden)\n        x = self.w3(x)\n        x = self.drop(x)\n        return x\n\n\nclass Attention(nn.Module):\n    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0.0, proj_drop=0.0, window_size=None, attn_head_dim=None, xattn=False, rope=None, subln=False, norm_layer=nn.LayerNorm):\n        super().__init__()\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        if attn_head_dim is not None:\n            head_dim = attn_head_dim\n        all_head_dim = head_dim * self.num_heads\n        self.scale = qk_scale or head_dim**-0.5\n\n        self.subln = subln\n        if self.subln:\n            self.q_proj = nn.Linear(dim, all_head_dim, bias=False)\n            self.k_proj = nn.Linear(dim, all_head_dim, bias=False)\n            self.v_proj = nn.Linear(dim, all_head_dim, bias=False)\n        else:\n            self.qkv = nn.Linear(dim, all_head_dim * 3, bias=False)\n\n        if qkv_bias:\n            self.q_bias = nn.Parameter(torch.zeros(all_head_dim))\n            self.v_bias = nn.Parameter(torch.zeros(all_head_dim))\n        else:\n            self.q_bias = None\n            self.v_bias = None\n\n        if window_size:\n            self.window_size = window_size\n            self.num_relative_distance = (2 * window_size[0] - 1) * (2 * window_size[1] - 1) + 3\n            self.relative_position_bias_table = nn.Parameter(torch.zeros(self.num_relative_distance, num_heads))  # 2*Wh-1 * 2*Ww-1, nH\n            # cls to token & token 2 cls & cls to cls\n\n            # get pair-wise relative position index for each token inside the window\n            coords_h = torch.arange(window_size[0])\n            coords_w = torch.arange(window_size[1])\n            coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n            coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n            relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n            relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n            relative_coords[:, :, 0] += window_size[0] - 1  # shift to start from 0\n            relative_coords[:, :, 1] += window_size[1] - 1\n            relative_coords[:, :, 0] *= 2 * window_size[1] - 1\n            relative_position_index = torch.zeros(size=(window_size[0] * window_size[1] + 1,) * 2, dtype=relative_coords.dtype)\n            relative_position_index[1:, 1:] = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n            relative_position_index[0, 0:] = self.num_relative_distance - 3\n            relative_position_index[0:, 0] = self.num_relative_distance - 2\n            relative_position_index[0, 0] = self.num_relative_distance - 1\n\n            self.register_buffer(\"relative_position_index\", relative_position_index)\n        else:\n            self.window_size = None\n            self.relative_position_bias_table = None\n            self.relative_position_index = None\n\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.inner_attn_ln = norm_layer(all_head_dim) if subln else nn.Identity()\n        # self.proj = nn.Linear(all_head_dim, all_head_dim)\n        self.proj = nn.Linear(all_head_dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n        self.xattn = xattn\n        self.xattn_drop = attn_drop\n\n        self.rope = rope\n\n    def forward(self, x, rel_pos_bias=None, attn_mask=None):\n        B, N, C = x.shape\n        if self.subln:\n            q = F.linear(input=x, weight=self.q_proj.weight, bias=self.q_bias)\n            k = F.linear(input=x, weight=self.k_proj.weight, bias=None)\n            v = F.linear(input=x, weight=self.v_proj.weight, bias=self.v_bias)\n\n            q = q.reshape(B, N, self.num_heads, -1).permute(0, 2, 1, 3)  # B, num_heads, N, C\n            k = k.reshape(B, N, self.num_heads, -1).permute(0, 2, 1, 3)\n            v = v.reshape(B, N, self.num_heads, -1).permute(0, 2, 1, 3)\n        else:\n\n            qkv_bias = None\n            if self.q_bias is not None:\n                qkv_bias = torch.cat((self.q_bias, torch.zeros_like(self.v_bias, requires_grad=False), self.v_bias))\n\n            qkv = F.linear(input=x, weight=self.qkv.weight, bias=qkv_bias)\n            qkv = qkv.reshape(B, N, 3, self.num_heads, -1).permute(2, 0, 3, 1, 4)  # 3, B, num_heads, N, C\n            q, k, v = qkv[0], qkv[1], qkv[2]\n\n        if self.rope:\n            # slightly fast impl\n            q_t = q[:, :, 1:, :]\n            ro_q_t = self.rope(q_t)\n            q = torch.cat((q[:, :, :1, :], ro_q_t), -2).type_as(v)\n\n            k_t = k[:, :, 1:, :]\n            ro_k_t = self.rope(k_t)\n            k = torch.cat((k[:, :, :1, :], ro_k_t), -2).type_as(v)\n\n        if self.xattn and xops is not None:\n            q = q.permute(0, 2, 1, 3)  # B, num_heads, N, C -> B, N, num_heads, C\n            k = k.permute(0, 2, 1, 3)\n            v = v.permute(0, 2, 1, 3)\n\n            x = xops.memory_efficient_attention(\n                q,\n                k,\n                v,\n                p=self.xattn_drop,\n                scale=self.scale,\n            )\n            x = x.reshape(B, N, -1)\n            x = self.inner_attn_ln(x)\n            x = self.proj(x)\n            x = self.proj_drop(x)\n        else:\n            q = q * self.scale\n            attn = q @ k.transpose(-2, -1)\n\n            if self.relative_position_bias_table is not None:\n                relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(self.window_size[0] * self.window_size[1] + 1, self.window_size[0] * self.window_size[1] + 1, -1)  # Wh*Ww,Wh*Ww,nH\n                relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n                attn = attn + relative_position_bias.unsqueeze(0).type_as(attn)\n\n            if rel_pos_bias is not None:\n                attn = attn + rel_pos_bias.type_as(attn)\n\n            if attn_mask is not None:\n                attn_mask = attn_mask.bool()\n                attn = attn.masked_fill(~attn_mask[:, None, None, :], float(\"-inf\"))\n\n            attn = attn.softmax(dim=-1)\n            attn = self.attn_drop(attn)\n\n            x = (attn @ v).transpose(1, 2).reshape(B, N, -1)\n            x = self.inner_attn_ln(x)\n            x = self.proj(x)\n            x = self.proj_drop(x)\n        return x\n\n\nclass Block(nn.Module):\n\n    def __init__(\n        self,\n        dim,\n        num_heads,\n        mlp_ratio=4.0,\n        qkv_bias=False,\n        qk_scale=None,\n        drop=0.0,\n        attn_drop=0.0,\n        drop_path=0.0,\n        init_values=None,\n        act_layer=nn.GELU,\n        norm_layer=nn.LayerNorm,\n        window_size=None,\n        attn_head_dim=None,\n        xattn=False,\n        rope=None,\n        postnorm=False,\n        subln=False,\n        naiveswiglu=False,\n    ):\n        super().__init__()\n        self.norm1 = norm_layer(dim)\n        self.attn = Attention(\n            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop, window_size=window_size, attn_head_dim=attn_head_dim, xattn=xattn, rope=rope, subln=subln, norm_layer=norm_layer\n        )\n        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n        self.drop_path = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n        self.norm2 = norm_layer(dim)\n        mlp_hidden_dim = int(dim * mlp_ratio)\n\n        if naiveswiglu:\n            self.mlp = SwiGLU(\n                in_features=dim,\n                hidden_features=mlp_hidden_dim,\n                subln=subln,\n                norm_layer=norm_layer,\n            )\n        else:\n            self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, subln=subln, drop=drop)\n\n        if init_values is not None and init_values > 0:\n            self.gamma_1 = nn.Parameter(init_values * torch.ones((dim)), requires_grad=True)\n            self.gamma_2 = nn.Parameter(init_values * torch.ones((dim)), requires_grad=True)\n        else:\n            self.gamma_1, self.gamma_2 = None, None\n\n        self.postnorm = postnorm\n\n    def forward(self, x, rel_pos_bias=None, attn_mask=None):\n        if self.gamma_1 is None:\n            if self.postnorm:\n                x = x + self.drop_path(self.norm1(self.attn(x, rel_pos_bias=rel_pos_bias, attn_mask=attn_mask)))\n                x = x + self.drop_path(self.norm2(self.mlp(x)))\n            else:\n                x = x + self.drop_path(self.attn(self.norm1(x), rel_pos_bias=rel_pos_bias, attn_mask=attn_mask))\n                x = x + self.drop_path(self.mlp(self.norm2(x)))\n        else:\n            if self.postnorm:\n                x = x + self.drop_path(self.gamma_1 * self.norm1(self.attn(x, rel_pos_bias=rel_pos_bias, attn_mask=attn_mask)))\n                x = x + self.drop_path(self.gamma_2 * self.norm2(self.mlp(x)))\n            else:\n                x = x + self.drop_path(self.gamma_1 * self.attn(self.norm1(x), rel_pos_bias=rel_pos_bias, attn_mask=attn_mask))\n                x = x + self.drop_path(self.gamma_2 * self.mlp(self.norm2(x)))\n        return x\n\n\nclass PatchEmbed(nn.Module):\n    \"\"\"Image to Patch Embedding\"\"\"\n\n    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n        super().__init__()\n        img_size = to_2tuple(img_size)\n        patch_size = to_2tuple(patch_size)\n        num_patches = (img_size[1] // patch_size[1]) * (img_size[0] // patch_size[0])\n        self.patch_shape = (img_size[0] // patch_size[0], img_size[1] // patch_size[1])\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.num_patches = num_patches\n\n        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n\n    def forward(self, x, **kwargs):\n        B, C, H, W = x.shape\n        # FIXME look at relaxing size constraints\n        assert H == self.img_size[0] and W == self.img_size[1], f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n        x = self.proj(x).flatten(2).transpose(1, 2)\n        return x\n\n\nclass RelativePositionBias(nn.Module):\n\n    def __init__(self, window_size, num_heads):\n        super().__init__()\n        self.window_size = window_size\n        self.num_relative_distance = (2 * window_size[0] - 1) * (2 * window_size[1] - 1) + 3\n        self.relative_position_bias_table = nn.Parameter(torch.zeros(self.num_relative_distance, num_heads))  # 2*Wh-1 * 2*Ww-1, nH\n        # cls to token & token 2 cls & cls to cls\n\n        # get pair-wise relative position index for each token inside the window\n        coords_h = torch.arange(window_size[0])\n        coords_w = torch.arange(window_size[1])\n        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n        coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n        relative_coords[:, :, 0] += window_size[0] - 1  # shift to start from 0\n        relative_coords[:, :, 1] += window_size[1] - 1\n        relative_coords[:, :, 0] *= 2 * window_size[1] - 1\n        relative_position_index = torch.zeros(size=(window_size[0] * window_size[1] + 1,) * 2, dtype=relative_coords.dtype)\n        relative_position_index[1:, 1:] = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n        relative_position_index[0, 0:] = self.num_relative_distance - 3\n        relative_position_index[0:, 0] = self.num_relative_distance - 2\n        relative_position_index[0, 0] = self.num_relative_distance - 1\n\n        self.register_buffer(\"relative_position_index\", relative_position_index)\n\n    def forward(self):\n        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(self.window_size[0] * self.window_size[1] + 1, self.window_size[0] * self.window_size[1] + 1, -1)  # Wh*Ww,Wh*Ww,nH\n        return relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n\n\nclass EVAVisionTransformer(nn.Module):\n    \"\"\"Vision Transformer with support for patch or hybrid CNN input stage\"\"\"\n\n    def __init__(\n        self,\n        img_size=224,\n        patch_size=16,\n        in_chans=3,\n        num_classes=1000,\n        embed_dim=768,\n        depth=12,\n        num_heads=12,\n        mlp_ratio=4.0,\n        qkv_bias=False,\n        qk_scale=None,\n        drop_rate=0.0,\n        attn_drop_rate=0.0,\n        drop_path_rate=0.0,\n        norm_layer=nn.LayerNorm,\n        init_values=None,\n        patch_dropout=0.0,\n        use_abs_pos_emb=True,\n        use_rel_pos_bias=False,\n        use_shared_rel_pos_bias=False,\n        rope=False,\n        use_mean_pooling=True,\n        init_scale=0.001,\n        grad_checkpointing=False,\n        xattn=False,\n        postnorm=False,\n        pt_hw_seq_len=16,\n        intp_freq=False,\n        naiveswiglu=False,\n        subln=False,\n    ):\n        super().__init__()\n        self.image_size = img_size\n        self.num_classes = num_classes\n        self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models\n\n        self.patch_embed = PatchEmbed(img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n        num_patches = self.patch_embed.num_patches\n\n        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n        # self.mask_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n        if use_abs_pos_emb:\n            self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n        else:\n            self.pos_embed = None\n        self.pos_drop = nn.Dropout(p=drop_rate)\n\n        if use_shared_rel_pos_bias:\n            self.rel_pos_bias = RelativePositionBias(window_size=self.patch_embed.patch_shape, num_heads=num_heads)\n        else:\n            self.rel_pos_bias = None\n\n        if rope:\n            half_head_dim = embed_dim // num_heads // 2\n            hw_seq_len = img_size // patch_size\n            self.rope = VisionRotaryEmbeddingFast(\n                dim=half_head_dim,\n                pt_seq_len=pt_hw_seq_len,\n                ft_seq_len=hw_seq_len if intp_freq else None,\n                # patch_dropout=patch_dropout\n            )\n        else:\n            self.rope = None\n\n        self.naiveswiglu = naiveswiglu\n\n        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n        self.use_rel_pos_bias = use_rel_pos_bias\n        self.blocks = nn.ModuleList(\n            [\n                Block(\n                    dim=embed_dim,\n                    num_heads=num_heads,\n                    mlp_ratio=mlp_ratio,\n                    qkv_bias=qkv_bias,\n                    qk_scale=qk_scale,\n                    drop=drop_rate,\n                    attn_drop=attn_drop_rate,\n                    drop_path=dpr[i],\n                    norm_layer=norm_layer,\n                    init_values=init_values,\n                    window_size=self.patch_embed.patch_shape if use_rel_pos_bias else None,\n                    xattn=xattn,\n                    rope=self.rope,\n                    postnorm=postnorm,\n                    subln=subln,\n                    naiveswiglu=naiveswiglu,\n                )\n                for i in range(depth)\n            ]\n        )\n        self.norm = nn.Identity() if use_mean_pooling else norm_layer(embed_dim)\n        self.fc_norm = norm_layer(embed_dim) if use_mean_pooling else None\n        self.head = nn.Linear(embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n\n        if self.pos_embed is not None:\n            trunc_normal_(self.pos_embed, std=0.02)\n\n        trunc_normal_(self.cls_token, std=0.02)\n        # trunc_normal_(self.mask_token, std=.02)\n\n        self.apply(self._init_weights)\n        self.fix_init_weight()\n\n        if isinstance(self.head, nn.Linear):\n            trunc_normal_(self.head.weight, std=0.02)\n            self.head.weight.data.mul_(init_scale)\n            self.head.bias.data.mul_(init_scale)\n\n        # setting a patch_dropout of 0. would mean it is disabled and this function would be the identity fn\n        self.patch_dropout = PatchDropout(patch_dropout) if patch_dropout > 0.0 else nn.Identity()\n\n        self.grad_checkpointing = grad_checkpointing\n\n    def fix_init_weight(self):\n        def rescale(param, layer_id):\n            param.div_(math.sqrt(2.0 * layer_id))\n\n        for layer_id, layer in enumerate(self.blocks):\n            rescale(layer.attn.proj.weight.data, layer_id + 1)\n            if self.naiveswiglu:\n                rescale(layer.mlp.w3.weight.data, layer_id + 1)\n            else:\n                rescale(layer.mlp.fc2.weight.data, layer_id + 1)\n\n    def get_cast_dtype(self) -> torch.dtype:\n        return self.blocks[0].mlp.fc2.weight.dtype\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            trunc_normal_(m.weight, std=0.02)\n            if m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.LayerNorm):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)\n\n    def get_num_layers(self):\n        return len(self.blocks)\n\n    def lock(self, unlocked_groups=0, freeze_bn_stats=False):\n        assert unlocked_groups == 0, \"partial locking not currently supported for this model\"\n        for param in self.parameters():\n            param.requires_grad = False\n\n    @torch.jit.ignore\n    def set_grad_checkpointing(self, enable=True):\n        self.grad_checkpointing = enable\n\n    @torch.jit.ignore\n    def no_weight_decay(self):\n        return {\"pos_embed\", \"cls_token\"}\n\n    def get_classifier(self):\n        return self.head\n\n    def reset_classifier(self, num_classes, global_pool=\"\"):\n        self.num_classes = num_classes\n        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n\n    def forward_features(self, x, return_all_features=False):\n\n        x = self.patch_embed(x)\n        batch_size, seq_len, _ = x.size()\n\n        cls_tokens = self.cls_token.expand(batch_size, -1, -1)  # stole cls_tokens impl from Phil Wang, thanks\n        x = torch.cat((cls_tokens, x), dim=1)\n        if self.pos_embed is not None:\n            x = x + self.pos_embed\n        x = self.pos_drop(x)\n\n        # a patch_dropout of 0. would mean it is disabled and this function would do nothing but return what was passed in\n        if os.getenv(\"RoPE\") == \"1\":\n            if self.training and not isinstance(self.patch_dropout, nn.Identity):\n                x, patch_indices_keep = self.patch_dropout(x)\n                # Directly pass patch_indices_keep to self.rope.forward\n                x = self.rope.forward(x, patch_indices_keep=patch_indices_keep)\n            else:\n                # Pass None or omit the patch_indices_keep argument for default behavior\n                x = self.rope.forward(x, patch_indices_keep=None)\n                x = self.patch_dropout(x)\n        else:\n            x = self.patch_dropout(x)\n\n        rel_pos_bias = self.rel_pos_bias() if self.rel_pos_bias is not None else None\n        for i, blk in enumerate(self.blocks):\n            if i == len(self.blocks) - 1:\n                continue\n            if self.grad_checkpointing:\n                x = checkpoint(blk, x, (rel_pos_bias,))\n            else:\n                x = blk(x, rel_pos_bias=rel_pos_bias)\n\n        if not return_all_features:\n            x = self.norm(x)\n            if self.fc_norm is not None:\n                return self.fc_norm(x.mean(1))\n            else:\n                return x[:, 0]\n        return x\n\n    def forward(self, x, return_all_features=False):\n        if return_all_features:\n            return self.forward_features(x, return_all_features)\n        x = self.forward_features(x)\n        x = self.head(x)\n        return x\n\n\ndef load_state_dict(checkpoint_path: str, map_location: str = \"cpu\", model_key: str = \"model|module|state_dict\", is_openai: bool = False, skip_list: list = []):\n    if is_openai:\n        model = torch.jit.load(checkpoint_path, map_location=\"cpu\").eval()\n        state_dict = model.state_dict()\n        for key in [\"input_resolution\", \"context_length\", \"vocab_size\"]:\n            state_dict.pop(key, None)\n    else:\n        checkpoint = torch.load(checkpoint_path, map_location=map_location)\n        for mk in model_key.split(\"|\"):\n            if isinstance(checkpoint, dict) and mk in checkpoint:\n                state_dict = checkpoint[mk]\n                break\n            else:\n                state_dict = checkpoint\n        if next(iter(state_dict.items()))[0].startswith(\"module\"):\n            state_dict = {k[7:]: v for k, v in state_dict.items()}\n\n    for k in skip_list:\n        if k in list(state_dict.keys()):\n            logging.info(f\"Removing key {k} from pretrained checkpoint\")\n            del state_dict[k]\n\n    if os.getenv(\"RoPE\") == \"1\":\n        for k in list(state_dict.keys()):\n            if \"freqs_cos\" in k or \"freqs_sin\" in k:\n                del state_dict[k]\n    return state_dict\n\n\ndef load_clip_visual_state_dict(checkpoint_path: str, map_location: str = \"cpu\", is_openai: bool = False, skip_list: list = []):\n    state_dict = load_state_dict(checkpoint_path, map_location=map_location, is_openai=is_openai, skip_list=skip_list)\n    # for k in list(state_dict.keys()):\n    #     if not k.startswith(\"visual.\"):\n    #         del state_dict[k]\n    # for k in list(state_dict.keys()):\n    #     if k.startswith(\"visual.\"):\n    #         new_k = k[7:]\n    #         state_dict[new_k] = state_dict[k]\n    #         del state_dict[k]\n    return state_dict\n\n\nfrom dataclasses import dataclass\nfrom typing import Optional, Tuple, Union\n\ntry:\n    from apex.normalization import FusedLayerNorm\nexcept:\n    FusedLayerNorm = LayerNorm\n    # print(\"Please build and install Nvidia apex package with option '--cuda_ext' according to https://github.com/NVIDIA/apex#from-source .\")\n\n\n@dataclass\nclass CLIPVisionCfg:\n    layers: Union[Tuple[int, int, int, int], int] = 12\n    width: int = 768\n    head_width: int = 64\n    mlp_ratio: float = 4.0\n    patch_size: int = 16\n    image_size: Union[Tuple[int, int], int] = 224\n    ls_init_value: Optional[float] = None  # layer scale initial value\n    patch_dropout: float = 0.0  # what fraction of patches to dropout during training (0 would mean disabled and no patches dropped) - 0.5 to 0.75 recommended in the paper for optimal results\n    global_average_pool: bool = False  # whether to global average pool the last embedding layer, instead of using CLS token (https://arxiv.org/abs/2205.01580)\n    drop_path_rate: Optional[float] = None  # drop path rate\n    timm_model_name: str = None  # a valid model name overrides layers, width, patch_size\n    timm_model_pretrained: bool = False  # use (imagenet) pretrained weights for named model\n    timm_pool: str = \"avg\"  # feature pooling for timm model ('abs_attn', 'rot_attn', 'avg', '')\n    timm_proj: str = \"linear\"  # linear projection for timm model output ('linear', 'mlp', '')\n    timm_proj_bias: bool = False  # enable bias final projection\n    eva_model_name: str = None  # a valid eva model name overrides layers, width, patch_size\n    qkv_bias: bool = True\n    fusedLN: bool = False\n    xattn: bool = False\n    postnorm: bool = False\n    rope: bool = False\n    pt_hw_seq_len: int = 16  # 224/14\n    intp_freq: bool = False\n    naiveswiglu: bool = False\n    subln: bool = False\n\n\ndef create_norm_layer_factory(use_fused_ln, eps=1e-6):\n    # Otherwise, use the standard LayerNorm\n    return lambda num_features: nn.LayerNorm(num_features, eps=eps)\n\n\ndef _build_vision_tower(vision_tower_path: str, embed_dim: int, vision_cfg: CLIPVisionCfg, **kwargs):\n    if isinstance(vision_cfg, dict):\n        vision_cfg = CLIPVisionCfg(**vision_cfg)\n\n    if vision_cfg.eva_model_name:\n        vision_heads = vision_cfg.width // vision_cfg.head_width\n        # Determine the appropriate norm layer factory based on the configuration\n        norm_layer_factory = create_norm_layer_factory(vision_cfg.fusedLN, eps=1e-6)\n\n        visual = EVAVisionTransformer(\n            img_size=vision_cfg.image_size,\n            patch_size=vision_cfg.patch_size,\n            num_classes=embed_dim,\n            use_mean_pooling=vision_cfg.global_average_pool,  # False\n            init_values=vision_cfg.ls_init_value,\n            patch_dropout=vision_cfg.patch_dropout,\n            embed_dim=vision_cfg.width,\n            depth=vision_cfg.layers,\n            num_heads=vision_heads,\n            mlp_ratio=vision_cfg.mlp_ratio,\n            qkv_bias=vision_cfg.qkv_bias,\n            drop_path_rate=vision_cfg.drop_path_rate,\n            norm_layer=norm_layer_factory,\n            xattn=vision_cfg.xattn,\n            rope=vision_cfg.rope,\n            postnorm=vision_cfg.postnorm,\n            pt_hw_seq_len=vision_cfg.pt_hw_seq_len,  # 224/14\n            intp_freq=vision_cfg.intp_freq,\n            naiveswiglu=vision_cfg.naiveswiglu,\n            subln=vision_cfg.subln,\n        )\n\n        state_dict = load_clip_visual_state_dict(vision_tower_path)\n        incompatible_keys = visual.load_state_dict(state_dict, strict=False)\n        rank0_print(\"EVA-CLIP incompatible_keys:\", incompatible_keys)\n\n    return visual\n\n\nclass EVAEncoderWrapper(nn.Module):\n    def __init__(self, vision_tower_pretrained, config):\n        super(EVAEncoderWrapper, self).__init__()\n        self.config = config\n        self.config[\"vision_tower_path\"] = vision_tower_pretrained\n        self.model = _build_vision_tower(**self.config)\n\n    def forward(self, image, **kwargs):\n        encode = self.model(image, return_all_features=True)[:, 1:, :]  # remove the CLS token\n        return encode\n\n    @property\n    def dtype(self):\n        return list(self.parameters())[-1].dtype\n\n    @property\n    def device(self):\n        return list(self.parameters())[-1].device\n"}
{"type": "source_file", "path": "llava/model/multimodal_encoder/imagebind.py", "content": "import torch\nimport torch.nn as nn\n\nfrom transformers import CLIPImageProcessor\n\ntry:\n    from imagebind.models import imagebind_model\n    from imagebind.models.imagebind_model import ModalityType\n    from imagebind.data import load_and_transform_audio_data\nexcept ImportError:\n    pass\n\n\nclass ImageBindWrapper(nn.Module):\n    def __init__(self, vision_tower, select_layer, select_feature=\"patch\", delay_load=False):\n        super().__init__()\n\n        self.is_loaded = False\n\n        self.vision_tower_name = vision_tower\n        self.select_layer = select_layer\n        self.select_feature = select_feature\n\n        if not delay_load:\n            self.load_model()\n\n    def load_model(self):\n        self.image_processor = CLIPImageProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n        self.vision_tower = imagebind_model.imagebind_huge(pretrained=True)\n        for p in self.vision_tower.parameters():\n            p.requires_grad = False\n        self.vision_tower.eval()\n        self.is_loaded = True\n\n    def train(self, mode=True):\n        self.training = mode\n\n        if self.is_loaded:\n            self.vision_tower.eval()\n\n    @torch.no_grad()\n    def forward(self, x):\n        if type(x) == dict:\n            if x[\"audios\"] is not None:\n                inputs = {ModalityType.AUDIO: load_and_transform_audio_data(x[\"audios\"], device=self.device).half()}\n                embeddings = self.vision_tower(inputs)\n                audio_embedding = embeddings[ModalityType.AUDIO]\n                return audio_embedding.unsqueeze(1)\n        else:\n            inputs = {ModalityType.VISION: x.to(dtype=self.dtype)}\n            embeddings = self.vision_tower(inputs)\n            vision_embedding = embeddings[ModalityType.VISION]\n            if vision_embedding.ndim == 2:\n                return vision_embedding.unsqueeze(1)\n            if vision_embedding.shape[1] == 257:\n                return vision_embedding[:, 1:]\n            raise ValueError(f\"Unexpected shape: {vision_embedding.shape}\")\n\n    @property\n    def dummy_feature(self):\n        return torch.zeros(1, 1024, device=self.device, dtype=self.dtype)\n\n    @property\n    def dtype(self):\n        return self.vision_tower.modality_preprocessors.vision.cls_token.dtype\n\n    @property\n    def device(self):\n        return self.vision_tower.modality_preprocessors.vision.cls_token.device\n\n    @property\n    def hidden_size(self):\n        return 1024\n"}
{"type": "source_file", "path": "llava/model/multimodal_encoder/open_clip_encoder.py", "content": "import torch\nimport torch.nn as nn\nfrom transformers import CLIPImageProcessor\nfrom llava.utils import rank0_print\n\ntry:\n    import open_clip\n    import torchvision\n    from open_clip.transformer import _expand_token\nexcept ImportError:\n    print(\"OpenCLIP not installed\")\n    open_clip = None\n\nHIDDEN_SIZE_DICT = {\n    \"ViT-H-14-378-quickgelu\": 1280,\n}\n\n\nclass OpenCLIPVisionTower(nn.Module):\n    def __init__(self, vision_tower, args, delay_load=False):\n        super().__init__()\n\n        self.is_loaded = False\n        self.model_name = vision_tower.replace(\"open_clip_hub:\", \"\")\n        self.pretrained = args.vision_tower_pretrained\n        self.select_layer = args.mm_vision_select_layer\n        self.select_feature = getattr(args, \"mm_vision_select_feature\", \"patch\")\n\n        if not delay_load:\n            rank0_print(f\"Loading vision tower: {vision_tower}\")\n            self.load_model()\n        elif getattr(args, \"unfreeze_mm_vision_tower\", False):\n            # TODO: better detector is needed.\n            rank0_print(f\"The checkpoint seems to contain `vision_tower` weights: `unfreeze_mm_vision_tower`: True.\")\n            self.load_model()\n        elif hasattr(args, \"mm_tunable_parts\") and \"mm_vision_tower\" in args.mm_tunable_parts:\n            rank0_print(f\"The checkpoint seems to contain `vision_tower` weights: `mm_tunable_parts` contains `mm_vision_tower`.\")\n            self.load_model()\n\n    def load_model(self, device_map=\"auto\"):\n        rank0_print(f\"Loading OpenCLIP model: {self.model_name}\")\n        rank0_print(f\"Pretrained: {self.pretrained}\")\n        vision_tower, _, image_processor = open_clip.create_model_and_transforms(model_name=self.model_name, pretrained=self.pretrained, precision=\"fp32\", device=\"cuda\")\n\n        resize_transform = [t for t in image_processor.transforms if isinstance(t, torchvision.transforms.Resize)][0]\n        normalize_transform = [t for t in image_processor.transforms if isinstance(t, torchvision.transforms.Normalize)][0]\n        self.resize_transform_size = resize_transform.size  # 224 or 384\n        self.patch_size = vision_tower.visual.conv1.kernel_size[0]  # 14 or 16\n\n        self.image_processor = CLIPImageProcessor.from_pretrained(\n            \"openai/clip-vit-large-patch14\",\n            crop_size=resize_transform.size,\n            size={\"shortest_edge\": resize_transform.size},\n            image_mean=list(normalize_transform.mean),\n            image_std=list(normalize_transform.std),\n        )\n        rank0_print(f\"Loaded image processor: {self.image_processor}\")\n        self.vision_tower = vision_tower.visual\n        self.vision_tower.requires_grad_(False)\n\n        self.is_loaded = True\n\n    def feature_select(self, image_forward_outs):\n        image_features = image_forward_outs[self.select_layer]\n        if self.select_feature == \"patch\":\n            image_features = image_features[:, 1:]\n        elif self.select_feature == \"cls_patch\":\n            image_features = image_features\n        elif self.select_feature == \"conv_flatten\":\n            image_features = image_features.flatten(2).transpose(1, 2)\n        else:\n            raise ValueError(f\"Unexpected select feature: {self.select_feature}\")\n        return image_features\n\n    def forward_visual(self, x, output_hidden_states=False):\n        if hasattr(self.vision_tower, \"trunk\") and hasattr(self.vision_tower.trunk, \"_intermediate_layers\"):\n            return self.vision_tower.trunk._intermediate_layers(x, abs(self.select_layer))\n        else:\n\n            def forward_openclip(self, x: torch.Tensor):\n                features = []\n                x = self.conv1(x)  # shape = [*, width, grid, grid]\n                x = x.reshape(x.shape[0], x.shape[1], -1)  # shape = [*, width, grid ** 2]\n                x = x.permute(0, 2, 1)  # shape = [*, grid ** 2, width]\n\n                # class embeddings and positional embeddings\n                x = torch.cat(\n                    [_expand_token(self.class_embedding, x.shape[0]).to(x.dtype), x],\n                    dim=1,\n                )\n                # shape = [*, grid ** 2 + 1, width]\n                x = x + self.positional_embedding.to(x.dtype)\n\n                x = self.patch_dropout(x)\n                x = self.ln_pre(x)\n\n                x = x.permute(1, 0, 2)  # NLD -> LND\n                for r in self.transformer.resblocks:\n                    x = r(x, attn_mask=None)\n                    features.append(x)\n                return features\n\n            return forward_openclip(self.vision_tower, x)\n\n    def forward(self, images):\n        if type(images) is list:\n            image_features = []\n            for image in images:\n                image_forward_out = self.forward_visual(image.to(self.dtype).unsqueeze(0), output_hidden_states=True)\n                image_feature = self.feature_select(image_forward_out).to(image.dtype)\n                image_features.append(image_feature)\n        else:\n            image_forward_outs = self.forward_visual(images.to(self.dtype), output_hidden_states=True)\n            image_features = self.feature_select(image_forward_outs).to(images.dtype)\n\n        return image_features\n\n    @property\n    def dummy_feature(self):\n        return torch.zeros(1, self.hidden_size, device=self.device, dtype=self.dtype)\n\n    @property\n    def dtype(self):\n        if hasattr(self.vision_tower, \"conv1\"):\n            return self.vision_tower.conv1.weight.dtype\n        if hasattr(self.vision_tower, \"trunk\"):\n            return self.vision_tower.trunk.patch_embed.proj.weight.dtype\n        raise NotImplementedError\n\n    @property\n    def device(self):\n        if hasattr(self.vision_tower, \"conv1\"):\n            return self.vision_tower.conv1.weight.device\n        if hasattr(self.vision_tower, \"trunk\"):\n            return self.vision_tower.trunk.patch_embed.proj.weight.device\n        raise NotImplementedError\n\n    @property\n    def config(self):\n        return None\n\n    @property\n    def hidden_size(self):\n        if self.model_name in HIDDEN_SIZE_DICT:\n            return HIDDEN_SIZE_DICT[self.model_name]\n        else:\n            raise NotImplementedError\n\n    @property\n    def num_patches(self):\n        image_size = self.resize_transform_size if isinstance(self.resize_transform_size, int) else self.resize_transform_size[0]\n        _num_patches = (image_size // self.patch_size) ** 2\n        if \"cls_patch\" in self.select_feature:\n            _num_patches += 1\n        return _num_patches\n\n    @property\n    def image_size(self):\n        return self.resize_transform_size\n\n    @property\n    def num_patches_per_side(self):\n        return self.resize_transform_size // self.patch_size\n"}
{"type": "source_file", "path": "llava/model/multimodal_encoder/siglip_encoder.py", "content": "\"\"\"\n# Adapted from https://huggingface.co/MILVLG/imp-v1-3b/blob/main/vision_encoder.py\n\"\"\"\n\nfrom typing import Optional, Tuple, Union, Dict\nfrom dataclasses import dataclass\nfrom functools import partial, reduce\nfrom PIL import Image\nimport torch\nimport torch.utils.checkpoint\nfrom torch import nn\nimport os\nfrom transformers.image_processing_utils import BatchFeature, get_size_dict\nfrom transformers.image_transforms import (\n    convert_to_rgb,\n    normalize,\n    rescale,\n    resize,\n    to_channel_dimension_format,\n)\nfrom transformers.image_utils import (\n    ChannelDimension,\n    PILImageResampling,\n    to_numpy_array,\n)\nfrom transformers.activations import ACT2FN\nfrom transformers.modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling\nfrom transformers.modeling_utils import PreTrainedModel\nfrom transformers import PretrainedConfig\nfrom transformers.utils import ModelOutput\nfrom llava.utils import rank0_print\n\n\nclass SigLipImageProcessor:\n    def __init__(self, image_mean=(0.5, 0.5, 0.5), image_std=(0.5, 0.5, 0.5), size=(384, 384), crop_size: Dict[str, int] = None, resample=PILImageResampling.BICUBIC, rescale_factor=1 / 255, data_format=ChannelDimension.FIRST):\n        crop_size = crop_size if crop_size is not None else {\"height\": 384, \"width\": 384}\n        crop_size = get_size_dict(crop_size, default_to_square=True, param_name=\"crop_size\")\n\n        self.image_mean = image_mean\n        self.image_std = image_std\n        self.size = size\n        self.resample = resample\n        self.rescale_factor = rescale_factor\n        self.data_format = data_format\n        self.crop_size = crop_size\n\n    def preprocess(self, images, return_tensors):\n        if isinstance(images, Image.Image):\n            images = [images]\n        else:\n            # to adapt video data\n            images = [to_numpy_array(image) for image in images]\n            assert isinstance(images, list)\n\n        transforms = [\n            convert_to_rgb,\n            to_numpy_array,\n            partial(resize, size=self.size, resample=self.resample, data_format=self.data_format),\n            partial(rescale, scale=self.rescale_factor, data_format=self.data_format),\n            partial(normalize, mean=self.image_mean, std=self.image_std, data_format=self.data_format),\n            partial(to_channel_dimension_format, channel_dim=self.data_format, input_channel_dim=self.data_format),\n        ]\n\n        images = reduce(lambda x, f: [*map(f, x)], transforms, images)\n        data = {\"pixel_values\": images}\n\n        return BatchFeature(data=data, tensor_type=return_tensors)\n\n\nclass SigLipVisionConfig(PretrainedConfig):\n    model_type = \"siglip_vision_model\"\n\n    def __init__(\n        self,\n        hidden_size=1152,\n        image_mean=(0.5, 0.5, 0.5),\n        intermediate_size=4304,\n        num_hidden_layers=27,\n        num_attention_heads=16,\n        num_channels=3,\n        image_size=384,\n        patch_size=14,\n        hidden_act=\"gelu_pytorch_tanh\",\n        layer_norm_eps=1e-6,\n        attention_dropout=0.0,\n        **kwargs,\n    ):\n        super().__init__(**kwargs)\n\n        self.hidden_size = hidden_size\n        self.intermediate_size = intermediate_size\n        self.num_hidden_layers = num_hidden_layers\n        self.num_attention_heads = num_attention_heads\n        self.num_channels = num_channels\n        self.patch_size = patch_size\n        self.image_size = image_size\n        self.attention_dropout = attention_dropout\n        self.layer_norm_eps = layer_norm_eps\n        self.hidden_act = hidden_act\n        self.image_mean = image_mean\n\n    @classmethod\n    def from_pretrained(cls, pretrained_model_name_or_path: Union[str, os.PathLike], **kwargs) -> \"PretrainedConfig\":\n        cls._set_token_in_kwargs(kwargs)\n\n        config_dict, kwargs = cls.get_config_dict(pretrained_model_name_or_path, **kwargs)\n\n        # get the vision config dict if we are loading from SigLipConfig\n        if config_dict.get(\"model_type\") == \"siglip\":\n            config_dict = config_dict[\"vision_config\"]\n\n        if \"model_type\" in config_dict and hasattr(cls, \"model_type\") and config_dict[\"model_type\"] != cls.model_type:\n            print(f\"You are using a model of type {config_dict['model_type']} to instantiate a model of type \" f\"{cls.model_type}. This is not supported for all configurations of models and can yield errors.\")\n\n        return cls.from_dict(config_dict, **kwargs)\n\n\n@dataclass\n# Copied from transformers.models.clip.modeling_clip.CLIPVisionModelOutput with CLIP->SigLip\nclass SigLipVisionModelOutput(ModelOutput):\n    \"\"\"\n    Base class for vision model's outputs that also contains image embeddings of the pooling of the last hidden states.\n\n    Args:\n        image_embeds (`torch.FloatTensor` of shape `(batch_size, output_dim)` *optional* returned when model is initialized with `with_projection=True`):\n            The image embeddings obtained by applying the projection layer to the pooler_output.\n        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n            Sequence of hidden-states at the output of the last layer of the model.\n        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n\n            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n            sequence_length)`.\n\n            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n            heads.\n    \"\"\"\n\n    image_embeds: Optional[torch.FloatTensor] = None\n    last_hidden_state: torch.FloatTensor = None\n    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n    attentions: Optional[Tuple[torch.FloatTensor]] = None\n\n\nclass SigLipVisionEmbeddings(nn.Module):\n    def __init__(self, config: SigLipVisionConfig):\n        super().__init__()\n        self.config = config\n        self.embed_dim = config.hidden_size\n        self.image_size = config.image_size\n        self.patch_size = config.patch_size\n\n        self.patch_embedding = nn.Conv2d(\n            in_channels=config.num_channels,\n            out_channels=self.embed_dim,\n            kernel_size=self.patch_size,\n            stride=self.patch_size,\n            padding=\"valid\",\n        )\n\n        self.num_patches = (self.image_size // self.patch_size) ** 2\n        self.num_positions = self.num_patches\n        self.position_embedding = nn.Embedding(self.num_positions, self.embed_dim)\n        self.register_buffer(\"position_ids\", torch.arange(self.num_positions).expand((1, -1)), persistent=False)\n\n    def forward(self, pixel_values: torch.FloatTensor) -> torch.Tensor:\n        patch_embeds = self.patch_embedding(pixel_values)  # shape = [*, width, grid, grid]\n        embeddings = patch_embeds.flatten(2).transpose(1, 2)\n\n        embeddings = embeddings + self.position_embedding(self.position_ids)\n        return embeddings\n\n\nclass SigLipAttention(nn.Module):\n    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n\n    # Copied from transformers.models.clip.modeling_clip.CLIPAttention.__init__\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.embed_dim = config.hidden_size\n        self.num_heads = config.num_attention_heads\n        self.head_dim = self.embed_dim // self.num_heads\n        if self.head_dim * self.num_heads != self.embed_dim:\n            raise ValueError(f\"embed_dim must be divisible by num_heads (got `embed_dim`: {self.embed_dim} and `num_heads`:\" f\" {self.num_heads}).\")\n        self.scale = self.head_dim**-0.5\n        self.dropout = config.attention_dropout\n\n        self.k_proj = nn.Linear(self.embed_dim, self.embed_dim)\n        self.v_proj = nn.Linear(self.embed_dim, self.embed_dim)\n        self.q_proj = nn.Linear(self.embed_dim, self.embed_dim)\n        self.out_proj = nn.Linear(self.embed_dim, self.embed_dim)\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        output_attentions: Optional[bool] = False,\n    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n        \"\"\"Input shape: Batch x Time x Channel\"\"\"\n\n        batch_size, q_len, _ = hidden_states.size()\n\n        query_states = self.q_proj(hidden_states)\n        key_states = self.k_proj(hidden_states)\n        value_states = self.v_proj(hidden_states)\n\n        query_states = query_states.view(batch_size, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n        key_states = key_states.view(batch_size, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n        value_states = value_states.view(batch_size, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n\n        k_v_seq_len = key_states.shape[-2]\n        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) * self.scale\n\n        if attn_weights.size() != (batch_size, self.num_heads, q_len, k_v_seq_len):\n            raise ValueError(f\"Attention weights should be of size {(batch_size, self.num_heads, q_len, k_v_seq_len)}, but is\" f\" {attn_weights.size()}\")\n\n        if attention_mask is not None:\n            if attention_mask.size() != (batch_size, 1, q_len, k_v_seq_len):\n                raise ValueError(f\"Attention mask should be of size {(batch_size, 1, q_len, k_v_seq_len)}, but is {attention_mask.size()}\")\n            attn_weights = attn_weights + attention_mask\n\n        # upcast attention to fp32\n        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n        attn_weights = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n        attn_output = torch.matmul(attn_weights, value_states)\n\n        if attn_output.size() != (batch_size, self.num_heads, q_len, self.head_dim):\n            raise ValueError(f\"`attn_output` should be of size {(batch_size, self.num_heads, q_len, self.head_dim)}, but is\" f\" {attn_output.size()}\")\n\n        attn_output = attn_output.transpose(1, 2).contiguous()\n        attn_output = attn_output.reshape(batch_size, q_len, self.embed_dim)\n\n        attn_output = self.out_proj(attn_output)\n\n        return attn_output, attn_weights\n\n\n# Copied from transformers.models.clip.modeling_clip.CLIPMLP with CLIP->SigLip\nclass SigLipMLP(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.activation_fn = ACT2FN[config.hidden_act]\n        self.fc1 = nn.Linear(config.hidden_size, config.intermediate_size)\n        self.fc2 = nn.Linear(config.intermediate_size, config.hidden_size)\n\n    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n        hidden_states = self.fc1(hidden_states)\n        hidden_states = self.activation_fn(hidden_states)\n        hidden_states = self.fc2(hidden_states)\n        return hidden_states\n\n\n# Copied from transformers.models.clip.modeling_clip.CLIPEncoderLayer with CLIP->SigLip\nclass SigLipEncoderLayer(nn.Module):\n    def __init__(self, config: SigLipVisionConfig):\n        super().__init__()\n        self.embed_dim = config.hidden_size\n        self.self_attn = SigLipAttention(config)\n        self.layer_norm1 = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_eps)\n        self.mlp = SigLipMLP(config)\n        self.layer_norm2 = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_eps)\n\n    # Ignore copy\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: torch.Tensor,\n        output_attentions: Optional[bool] = False,\n    ) -> Tuple[torch.FloatTensor]:\n        \"\"\"\n        Args:\n            hidden_states (`torch.FloatTensor`):\n                Input to the layer of shape `(batch, seq_len, embed_dim)`.\n            attention_mask (`torch.FloatTensor`):\n                Attention mask of shape `(batch, 1, q_len, k_v_seq_len)` where padding elements are indicated by very large negative values.\n            output_attentions (`bool`, *optional*, defaults to `False`):\n                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                returned tensors for more detail.\n        \"\"\"\n        residual = hidden_states\n\n        hidden_states = self.layer_norm1(hidden_states)\n        hidden_states, attn_weights = self.self_attn(\n            hidden_states=hidden_states,\n            attention_mask=attention_mask,\n            output_attentions=output_attentions,\n        )\n        hidden_states = residual + hidden_states\n\n        residual = hidden_states\n        hidden_states = self.layer_norm2(hidden_states)\n        hidden_states = self.mlp(hidden_states)\n        hidden_states = residual + hidden_states\n\n        outputs = (hidden_states,)\n\n        if output_attentions:\n            outputs += (attn_weights,)\n\n        return outputs\n\n\nclass SigLipPreTrainedModel(PreTrainedModel):\n    \"\"\"\n    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n    models.\n    \"\"\"\n\n    config_class = SigLipVisionConfig\n    base_model_prefix = \"siglip\"\n    supports_gradient_checkpointing = True\n\n    def _init_weights(self, module):\n        \"\"\"Initialize the weights\"\"\"\n        pass\n\n\n# Copied from transformers.models.clip.modeling_clip.CLIPEncoder with CLIP->SigLip\nclass SigLipEncoder(nn.Module):\n    \"\"\"\n    Transformer encoder consisting of `config.num_hidden_layers` self attention layers. Each layer is a\n    [`SigLipEncoderLayer`].\n\n    Args:\n        config: SigLipVisionConfig\n    \"\"\"\n\n    def __init__(self, config: SigLipVisionConfig):\n        super().__init__()\n        self.config = config\n        self.layers = nn.ModuleList([SigLipEncoderLayer(config) for _ in range(config.num_hidden_layers)])\n        self.gradient_checkpointing = False\n\n    # Ignore copy\n    def forward(\n        self,\n        inputs_embeds,\n        attention_mask: Optional[torch.Tensor] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ) -> Union[Tuple, BaseModelOutput]:\n        r\"\"\"\n        Args:\n            inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n                Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation.\n                This is useful if you want more control over how to convert `input_ids` indices into associated vectors\n                than the model's internal embedding lookup matrix.\n            attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n                Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n\n                - 1 for tokens that are **not masked**,\n                - 0 for tokens that are **masked**.\n\n                [What are attention masks?](../glossary#attention-mask)\n            output_attentions (`bool`, *optional*):\n                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                returned tensors for more detail.\n            output_hidden_states (`bool`, *optional*):\n                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n                for more detail.\n            return_dict (`bool`, *optional*):\n                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n        \"\"\"\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        encoder_states = () if output_hidden_states else None\n        all_attentions = () if output_attentions else None\n\n        hidden_states = inputs_embeds\n        for encoder_layer in self.layers:\n            if output_hidden_states:\n                encoder_states = encoder_states + (hidden_states,)\n            if self.gradient_checkpointing and self.training:\n                layer_outputs = self._gradient_checkpointing_func(\n                    encoder_layer.__call__,\n                    hidden_states,\n                    attention_mask,\n                    output_attentions,\n                )\n            else:\n                layer_outputs = encoder_layer(\n                    hidden_states,\n                    attention_mask,\n                    output_attentions=output_attentions,\n                )\n\n            hidden_states = layer_outputs[0]\n\n            if output_attentions:\n                all_attentions = all_attentions + (layer_outputs[1],)\n\n        if output_hidden_states:\n            encoder_states = encoder_states + (hidden_states,)\n\n        if not return_dict:\n            return tuple(v for v in [hidden_states, encoder_states, all_attentions] if v is not None)\n        return BaseModelOutput(last_hidden_state=hidden_states, hidden_states=encoder_states, attentions=all_attentions)\n\n\nclass SigLipVisionTransformer(nn.Module):\n    def __init__(self, config: SigLipVisionConfig):\n        super().__init__()\n        self.config = config\n        embed_dim = config.hidden_size\n\n        self.embeddings = SigLipVisionEmbeddings(config)\n        self.encoder = SigLipEncoder(config)\n        self.post_layernorm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps)\n        self.head = SigLipMultiheadAttentionPoolingHead(config)\n\n    def forward(\n        self,\n        pixel_values,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ) -> Union[Tuple, BaseModelOutputWithPooling]:\n        r\"\"\"\n        Returns:\n\n        \"\"\"\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        hidden_states = self.embeddings(pixel_values)\n\n        encoder_outputs = self.encoder(\n            inputs_embeds=hidden_states,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        last_hidden_state = encoder_outputs[0]\n        last_hidden_state = self.post_layernorm(last_hidden_state)\n\n        pooled_output = self.head(last_hidden_state)\n\n        if not return_dict:\n            return (last_hidden_state, pooled_output) + encoder_outputs[1:]\n\n        return BaseModelOutputWithPooling(\n            last_hidden_state=last_hidden_state,\n            pooler_output=pooled_output,\n            hidden_states=encoder_outputs.hidden_states,\n            attentions=encoder_outputs.attentions,\n        )\n\n\nclass SigLipMultiheadAttentionPoolingHead(nn.Module):\n    \"\"\"Multihead Attention Pooling.\"\"\"\n\n    def __init__(self, config: SigLipVisionConfig):\n        super().__init__()\n\n        self.probe = nn.Parameter(torch.randn(1, 1, config.hidden_size))\n        self.attention = torch.nn.MultiheadAttention(config.hidden_size, config.num_attention_heads, batch_first=True)\n        self.layernorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n        self.mlp = SigLipMLP(config)\n\n    def forward(self, hidden_state):\n        batch_size = hidden_state.shape[0]\n        probe = self.probe.repeat(batch_size, 1, 1)\n\n        hidden_state = self.attention(probe, hidden_state, hidden_state)[0]\n\n        residual = hidden_state\n        hidden_state = self.layernorm(hidden_state)\n        hidden_state = residual + self.mlp(hidden_state)\n\n        return hidden_state[:, 0]\n\n\nclass SigLipVisionModel(SigLipPreTrainedModel):\n    config_class = SigLipVisionConfig\n    main_input_name = \"pixel_values\"\n    _no_split_modules = [\"SigLipEncoderLayer\"]\n\n    def __init__(self, config: SigLipVisionConfig):\n        super().__init__(config)\n\n        self.vision_model = SigLipVisionTransformer(config)\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_input_embeddings(self) -> nn.Module:\n        return self.vision_model.embeddings.patch_embedding\n\n    def forward(\n        self,\n        pixel_values,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ) -> Union[Tuple, BaseModelOutputWithPooling]:\n        r\"\"\"\n        Returns:\n\n        Examples:\n\n        ```python\n        >>> from PIL import Image\n        >>> import requests\n        >>> from transformers import AutoProcessor, SigLipVisionModel\n\n        >>> model = SigLipVisionModel.from_pretrained(\"google/siglip-base-patch16-224\")\n        >>> processor = AutoProcessor.from_pretrained(\"google/siglip-base-patch16-224\")\n\n        >>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n        >>> image = Image.open(requests.get(url, stream=True).raw)\n\n        >>> inputs = processor(images=image, return_tensors=\"pt\")\n\n        >>> outputs = model(**inputs)\n        >>> last_hidden_state = outputs.last_hidden_state\n        >>> pooled_output = outputs.pooler_output  # pooled features\n        ```\"\"\"\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        return self.vision_model(\n            pixel_values=pixel_values,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n\nclass SigLipVisionTower(nn.Module):\n    def __init__(self, vision_tower, vision_tower_cfg, delay_load=False):\n        super().__init__()\n\n        self.is_loaded = False\n\n        self.config = SigLipVisionConfig()\n\n        self.vision_tower_name = vision_tower\n\n        self.image_processor = SigLipImageProcessor()\n\n        if not delay_load:\n            rank0_print(f\"Loading vision tower: {vision_tower}\")\n            self.load_model()\n        elif getattr(vision_tower_cfg, \"unfreeze_mm_vision_tower\", False):\n            # TODO: better detector is needed.\n            rank0_print(f\"The checkpoint seems to contain `vision_tower` weights: `unfreeze_mm_vision_tower`: True.\")\n            self.load_model()\n        elif hasattr(vision_tower_cfg, \"mm_tunable_parts\") and \"mm_vision_tower\" in vision_tower_cfg.mm_tunable_parts:\n            rank0_print(f\"The checkpoint seems to contain `vision_tower` weights: `mm_tunable_parts` contains `mm_vision_tower`.\")\n            self.load_model()\n        else:\n            self.cfg_only = self.config\n\n    def load_model(self, device_map=None):\n        if self.is_loaded:\n            rank0_print(\"{} is already loaded, `load_model` called again, skipping.\".format(self.vision_tower_name))\n            return\n\n        self.vision_tower = SigLipVisionModel.from_pretrained(self.vision_tower_name, device_map=device_map)\n\n        del self.vision_tower.vision_model.encoder.layers[-1:]\n        self.vision_tower.vision_model.head = nn.Identity()\n        self.vision_tower.requires_grad_(False)\n\n        self.is_loaded = True\n\n    def forward(self, images):\n        if type(images) is list:\n            image_features = []\n            for image in images:\n                image_forward_out = self.vision_tower(image.to(device=self.device, dtype=self.dtype).unsqueeze(0), output_hidden_states=True)\n                image_feature = image_forward_out.hidden_states[-1].to(image.dtype)\n                assert image_features.shape[-2] == 729\n                image_features.append(image_feature)\n        else:\n            image_forward_outs = self.vision_tower(images.to(device=self.device, dtype=self.dtype), output_hidden_states=True)\n            image_features = image_forward_outs.hidden_states[-1].to(images.dtype)\n            assert image_features.shape[-2] == 729\n\n        return image_features\n\n    @property\n    def dummy_feature(self):\n        return torch.zeros(1, self.hidden_size, device=self.device, dtype=self.dtype)\n\n    @property\n    def dtype(self):\n        for p in self.vision_tower.parameters():\n            return p.dtype\n\n    @property\n    def device(self):\n        for p in self.vision_tower.parameters():\n            return p.device\n\n    @property\n    def hidden_size(self):\n        return self.config.hidden_size\n\n    @property\n    def num_patches(self):\n        return (self.config.image_size // self.config.patch_size) ** 2\n\n    @property\n    def num_patches_per_side(self):\n        return self.config.image_size // self.config.patch_size\n        # return self.model_config[\"vision_cfg\"][\"image_size\"] // self.model_config[\"vision_cfg\"][\"patch_size\"]\n\n    @property\n    def image_size(self):\n        return self.config.image_size\n"}
