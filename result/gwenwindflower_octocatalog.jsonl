{"repo_info": {"repo_name": "octocatalog", "repo_owner": "gwenwindflower", "repo_url": "https://github.com/gwenwindflower/octocatalog"}}
{"type": "source_file", "path": "el.py", "content": "import argparse\nimport os\nfrom datetime import datetime, timedelta\n\nimport duckdb\nimport requests\nfrom halo import Halo\nfrom tqdm import tqdm\n\ncolumns = \"\"\"\n            columns={\n                'id': 'VARCHAR',\n                'type': 'VARCHAR',\n                'actor': 'STRUCT(\n                    id VARCHAR,\n                    login VARCHAR,\n                    display_login VARCHAR,\n                    gravatar_id VARCHAR,\n                    url VARCHAR,\n                    avatar_url VARCHAR\n                )',\n                'repo': 'STRUCT(id VARCHAR, name VARCHAR, url VARCHAR)',\n                'payload': 'JSON',\n                'public': 'BOOLEAN',\n                'created_at': 'TIMESTAMP',\n                'org': 'STRUCT(\n                    id VARCHAR,\n                    login VARCHAR,\n                    gravatar_id VARCHAR,\n                    url VARCHAR,\n                    avatar_url VARCHAR\n                )'\n            }\n        \"\"\"\n\n\ndef validate_date(datetime_str):\n    try:\n        input_datetime = datetime.strptime(datetime_str, \"%Y-%m-%d-%H\")\n        current_datetime = datetime.utcnow()\n        if input_datetime < datetime(2015, 1, 1) or input_datetime > current_datetime:\n            raise argparse.ArgumentTypeError(\n                f\"Datetime {datetime_str} is outside the acceptable range.\"\n            )\n        return input_datetime\n    except ValueError:\n        raise argparse.ArgumentTypeError(\n            f\"Invalid datetime format: {datetime_str}. \\\n            Please use the format YYYY-MM-DD-HH.\"\n        )\n\n\ndef download_data(active_datetime):\n    url_datetime = datetime.strftime(active_datetime, \"%Y-%m-%d-%-H\")\n    url = f\"https://data.gharchive.org/{url_datetime}.json.gz\"\n\n    file_path = f\"./data/{url_datetime}.json.gz\"\n\n    if not os.path.exists(file_path):\n        response = requests.get(url, stream=True)\n        total_size_in_bytes = int(response.headers.get(\"content-length\", 0))\n        progress_bar = tqdm(\n            total=total_size_in_bytes,\n            unit=\"kB\",\n            unit_scale=True,\n            leave=False,\n            desc=f\"💾 Downloading data from Github Archive for {active_datetime}...\",\n        )\n\n        if response.status_code == 200:\n            with open(file_path, \"wb\") as output_file:\n                for chunk in response.iter_content(chunk_size=1024):\n                    output_file.write(chunk)\n                    progress_bar.update(len(chunk))\n            progress_bar.close()\n        else:\n            print(\n                f\"💩 Crap! {url_datetime} returned status code {response.status_code}.\"\n            )\n    else:\n        print(f\"🎉 Hooray! {url_datetime} already exists. Skipping download.\")\n\n\ndef extract_data(start_datetime, end_datetime):\n    total_hours = int((end_datetime - start_datetime).total_seconds() / 3600)\n    progress_bar = tqdm(total=total_hours)\n\n    active_datetime = start_datetime\n\n    while active_datetime <= end_datetime:\n        download_data(active_datetime)\n        if args.incremental and args.load:\n            load_data(incremental=True)\n            os.remove(\n                f\"data/{datetime.strftime(active_datetime, '%Y-%m-%d-%-H')}.json.gz\"\n            )\n        active_datetime += timedelta(hours=1)\n        progress_bar.update(1)\n\n    progress_bar.close()\n\n\ndef load_data(incremental=False):\n    if args.check:\n        data_path = \"data/testing.json\"\n    else:\n        data_path = \"data/*.json.gz\"\n\n    if args.prod:\n        spinner_text = \"🦆☁️  Loading data into MotherDuck...\"\n        connection = \"md:octocatalog\"\n    else:\n        spinner_text = \"🦆💾 Loading data into DuckDB...\"\n        connection = \"./reports/octocatalog.db\"\n\n    if incremental:\n        table_create = \"create table if not exists raw.github_events as \"\n    else:\n        table_create = \"create or replace table raw.github_events as \"\n\n    spinner = Halo(text=spinner_text, spinner=\"dots\")\n    spinner.start()\n\n    con = duckdb.connect(database=connection, read_only=False)\n    con.execute(\n        \"create schema if not exists raw;\"\n        + table_create\n        + \"select * from read_ndjson(\"\n        + \"'\"\n        + data_path\n        + \"'\"\n        + \",\"\n        + columns\n        + \");\"\n    )\n    con.close()\n\n    if args.prod:\n        spinner.succeed(\"🦆☁️ Loading data into MotherDuck... Done!\")\n    else:\n        spinner.succeed(\"🦆💾 Loading data into DuckDB... Done!\")\n\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\n    \"start_datetime\",\n    help=\"The start date of the range\",\n    default=datetime.strftime(datetime.utcnow() - timedelta(hours=1), \"%Y-%m-%d-%H\"),\n    nargs=\"?\",\n    type=validate_date,\n)\nparser.add_argument(\n    \"end_datetime\",\n    help=\"The end date of the range (inclusive)\",\n    default=datetime.strftime(datetime.utcnow(), \"%Y-%m-%d-%H\"),\n    nargs=\"?\",\n    type=validate_date,\n)\nparser.add_argument(\n    \"-e\",\n    \"--extract\",\n    help=\"Just pull data from the GitHub Archive don't load it into DuckDB\",\n    default=False,\n    action=\"store_true\",\n)\nparser.add_argument(\n    \"-l\",\n    \"--load\",\n    help=\"Load data already existing from the data directory into DuckDB\",\n    default=False,\n    action=\"store_true\",\n)\nparser.add_argument(\n    \"-p\",\n    \"--prod\",\n    help=\"Run in production mode connected to MotherDuck\",\n    default=False,\n    action=\"store_true\",\n)\nparser.add_argument(\n    \"-i\",\n    \"--incremental\",\n    help=\"Run in incremental load mode, only works with a production target\",\n    default=False,\n    action=\"store_true\",\n)\nparser.add_argument(\n    \"-c\",\n    \"--check\",\n    help=\"Run in CI mode using data in data-test directory\",\n    default=False,\n    action=\"store_true\",\n)\nargs = parser.parse_args()\n\nif args.extract:\n    extract_data(args.start_datetime, args.end_datetime)\n\nif args.load and not args.incremental and not args.extract:\n    load_data()\n"}
{"type": "source_file", "path": "el-modal.py", "content": "import modal\n\nstub = modal.Stub(\n    image=modal.Image.debian_slim().pip_install([\"boto3\", \"requests\"]),\n)\n\n\n@stub.function(secret=modal.Secret.from_name(\"aws\"))\ndef scrape_data(active_datetime):\n    from datetime import datetime\n\n    import boto3\n    import requests\n\n    bucket_name = \"github-archive-ddb\"\n    s3 = boto3.client(\"s3\")\n\n    file_path = f\"{datetime.strftime(active_datetime, '%Y-%m-%d-%-H')}.json.gz\"\n    url = f\"https://data.gharchive.org/{file_path}\"\n\n    response = requests.get(url)\n\n    if response.status_code == 200:\n        file_content = response.content\n        s3.put_object(Bucket=bucket_name, Key=file_path, Body=file_content)\n    else:\n        print(f\"💩 Crap! {file_path} returned status code {response.status_code}.\")\n\n\n@stub.function()\ndef bucket_data(start, end):\n    from datetime import datetime, timedelta\n\n    start_datetime = datetime.strptime(start, \"%Y-%m-%d-%H\")\n    end_datetime = datetime.strptime(end, \"%Y-%m-%d-%H\")\n    active_datetime = start_datetime\n\n    while active_datetime <= end_datetime:\n        scrape_data.remote(active_datetime)\n        active_datetime += timedelta(hours=1)\n\n\n@stub.local_entrypoint()\n# TODO: validate args\ndef main(start, end):\n    bucket_data.remote(start, end)\n"}
