{"repo_info": {"repo_name": "fides", "repo_owner": "ethyca", "repo_url": "https://github.com/ethyca/fides"}}
{"type": "test_file", "path": "noxfiles/test_docker_nox.py", "content": "from docker_nox import generate_buildx_command\n\n\nclass TestGenerateBuildxCommand:\n    def test_single_tag(self) -> None:\n        actual_result = generate_buildx_command(\n            image_tags=[\"foo\"],\n            docker_build_target=\"prod\",\n            dockerfile_path=\".\",\n        )\n        expected_result = (\n            \"docker\",\n            \"buildx\",\n            \"build\",\n            \"--push\",\n            \"--target=prod\",\n            \"--platform\",\n            \"linux/amd64,linux/arm64\",\n            \".\",\n            \"--tag\",\n            \"foo\",\n        )\n        assert actual_result == expected_result\n\n    def test_multiplte_tags(self) -> None:\n        actual_result = generate_buildx_command(\n            image_tags=[\"foo\", \"bar\"],\n            docker_build_target=\"prod\",\n            dockerfile_path=\".\",\n        )\n        expected_result = (\n            \"docker\",\n            \"buildx\",\n            \"build\",\n            \"--push\",\n            \"--target=prod\",\n            \"--platform\",\n            \"linux/amd64,linux/arm64\",\n            \".\",\n            \"--tag\",\n            \"foo\",\n            \"--tag\",\n            \"bar\",\n        )\n        assert actual_result == expected_result\n\n    def test_different_path(self) -> None:\n        actual_result = generate_buildx_command(\n            image_tags=[\"foo\", \"bar\"],\n            docker_build_target=\"prod\",\n            dockerfile_path=\"other_path\",\n        )\n        expected_result = (\n            \"docker\",\n            \"buildx\",\n            \"build\",\n            \"--push\",\n            \"--target=prod\",\n            \"--platform\",\n            \"linux/amd64,linux/arm64\",\n            \"other_path\",\n            \"--tag\",\n            \"foo\",\n            \"--tag\",\n            \"bar\",\n        )\n        assert actual_result == expected_result\n"}
{"type": "test_file", "path": "noxfiles/test_git_nox.py", "content": "from unittest import mock\n\nimport pytest\nfrom git import Repo\n\nfrom git_nox import generate_tag\n\n\nclass TestGitNox:\n    \"\"\"\n    Tests for git nox commands and/or utilities\n    \"\"\"\n\n    @pytest.fixture(scope=\"session\")\n    def repo(self):\n        repo = Repo()\n        git_session = repo.git()\n        git_session.fetch(\"--force\", \"--tags\")\n        return repo\n\n    @mock.patch(\"nox.Session.log\")\n    @pytest.mark.parametrize(\n        \"current_branch,tags,expected_tag\",\n        [\n            (\"main\", [\"2.9.0\"], \"2.9.1b0\"),\n            (\"main\", [\"2.9.1\", \"2.9.0\"], \"2.9.2b0\"),\n            (\n                \"main\",\n                [\"2.9.0\", \"2.9.1\"],\n                \"2.9.2b0\",\n            ),  # releases _could_ be \"out of order\" chronological order, for e.g. hotfixes\n            (\n                \"main\",\n                [\"2.9.0\", \"2.10.0\"],\n                \"2.10.1b0\",\n            ),  # out of order releases with version >= 10 to ensure numerical sort\n            (\n                \"main\",\n                [\"2.18.0\", \"2.17.0rc1\", \"2.19.1rc0\", \"2.19.0rc0\"],\n                \"2.19.2b0\",\n            ),  # beta tags should be an increment ahead of rc tag version, if it is ahead of our newest release\n            (\n                \"main\",\n                [\"2.18.0\", \"2.17.0rc1\", \"2.16.0rc0\"],\n                \"2.18.1b0\",\n            ),  # beta tags should ignore rc tags if they are not ahead of latest release\n            (\n                \"main\",\n                [\"2.9.0\", \"2.17.0rc1\", \"2.16.0rc0\"],\n                \"2.17.1b0\",\n            ),  # ensure rc version check is robust with numerical sort\n            (\n                \"main\",\n                [\"2.9.0\", \"2.19.1rc0\", \"2.19.2b1\", \"2.16.0rc0\"],\n                \"2.19.2b2\",\n            ),  # ensure we still increment well when using rc version check\n            (\n                \"some-test-feature-branch\",\n                [\"2.18.0\", \"2.17.0rc1\", \"2.19.0rc0\"],\n                \"2.18.1a0\",\n            ),  # alpha tags should ignore rc tags when determining next increment\n            (\"some-test-feature-branch\", [\"2.9.1\"], \"2.9.2a0\"),\n            (\"some-test-feature-branch\", [\"2.9.1\", \"2.9.0\"], \"2.9.2a0\"),\n            (\n                \"some-test-feature-branch\",\n                [\"2.9.3a0\", \"2.9.2\"],\n                \"2.9.3a1\",\n            ),  # our repo happens to already have a 2.9.3a0 tag, let's use it for testing here\n            (\n                \"some-other-feature\",\n                [\"2.14.1a0\", \"2.14.1a1\", \"2.14.0\"],\n                \"2.14.1a2\",\n            ),  # unsorted tags\n            (\n                \"some-other-feature\",\n                [\"2.14.1a9\", \"2.14.1a10\", \"2.14.0\"],\n                \"2.14.1a11\",\n            ),  # unsorted tags with current increment = 10 to ensure we're doing a numeric sort\n            (\"release-2.9.0\", [], \"2.9.0rc0\"),\n            (\"release-2.9.0\", [\"2.9.0rc1\"], \"2.9.0rc2\"),\n            (\n                \"release-2.9.0\",\n                [\"2.9.1\"],\n                \"2.9.0rc0\",\n            ),  # even with a release further ahead, the RC tag generation relies on the release branch name\n            # release branches MUST adhere to `release-n.n.n` convention\n            # if not, they will be treated as any other \"feature\" branch\n            (\"release/2.9.0\", [\"2.9.0\"], \"2.9.1a0\"),\n            (\"release_2.9.0\", [\"2.9.0\"], \"2.9.1a0\"),\n        ],\n    )\n    def test_generate_tag(\n        self,\n        mock_session,\n        current_branch,\n        tags,\n        expected_tag,\n        repo,\n    ) -> None:\n        \"\"\"\n        Test generate_tag function based on a given `current_branch` and set of existing repo tags\n\n        No tag is actually applied to the repo! The generated tag string is just evaluated within the test runtime\n        \"\"\"\n\n        # get the real tags from the repo because it's hard to instantiate tags from \"scratch\"\n\n        all_tags = [repo.tags[tag] for tag in tags]\n\n        # evaluate whether we generate the expected tag\n        tag = generate_tag(mock_session, current_branch, all_tags)\n        assert tag == expected_tag\n"}
{"type": "test_file", "path": "src/fides/api/schemas/connection_configuration/enums/test_status.py", "content": "from enum import Enum\nfrom typing import Optional\n\n\nclass TestStatus(Enum):\n    passed = \"passed\"\n    failed = \"failed\"\n    untested = \"untested\"\n\n    def str_to_bool(self) -> Optional[bool]:\n        \"\"\"Translates query param string to optional/bool value\n        for filtering ConnectionConfig.last_test_succeeded field\"\"\"\n        if self == self.passed:\n            return True\n        if self == self.failed:\n            return False\n        return None\n"}
{"type": "test_file", "path": "tests/__init__.py", "content": ""}
{"type": "test_file", "path": "tests/api/__init__.py", "content": ""}
{"type": "test_file", "path": "tests/api/models/privacy_request/test_webhook.py", "content": "import pytest\n\nfrom fides.api.models.privacy_request.webhook import (\n    generate_request_callback_pre_approval_jwe,\n    generate_request_callback_resume_jwe,\n    generate_request_task_callback_jwe,\n)\nfrom fides.api.oauth.jwt import decrypt_jwe\nfrom fides.config import CONFIG\n\n\n@pytest.mark.parametrize(\n    \"jwe_generator\",\n    [\n        generate_request_callback_pre_approval_jwe,\n        generate_request_task_callback_jwe,\n        generate_request_callback_resume_jwe,\n    ],\n    ids=[\n        \"pre_approval_jwe\",\n        \"task_callback_jwe\",\n        \"resume_callback_jwe\",\n    ],\n)\ndef test_generate_valid_jwe(jwe_generator, policy_pre_execution_webhooks):\n    for webhook in policy_pre_execution_webhooks:\n        jwe = jwe_generator(webhook)\n        assert isinstance(jwe, str)\n        assert jwe\n        assert len(jwe.split(\".\")) == 5\n\n\n@pytest.mark.parametrize(\n    \"jwe_generator\",\n    [\n        generate_request_callback_pre_approval_jwe,\n        generate_request_task_callback_jwe,\n        generate_request_callback_resume_jwe,\n    ],\n    ids=[\n        \"pre_approval_jwe\",\n        \"task_callback_jwe\",\n        \"resume_callback_jwe\",\n    ],\n)\ndef test_generate_jwe_round_trip(jwe_generator, policy_pre_execution_webhooks):\n    for webhook in policy_pre_execution_webhooks:\n        jwe = jwe_generator(webhook)\n        decrypted_payload = decrypt_jwe(jwe, CONFIG.security.app_encryption_key)\n        assert decrypted_payload  # Ensure the payload is not empty\n        assert isinstance(decrypted_payload, str)  # Ensure the payload is a string\n        assert (\n            \"webhook_id\" in decrypted_payload or \"request_task_id\" in decrypted_payload\n        )  # Verify expected fields in the payload\n        assert \"scopes\" in decrypted_payload\n        assert \"iat\" in decrypted_payload\n\n\n@pytest.mark.parametrize(\n    \"jwe_generator\",\n    [\n        generate_request_callback_pre_approval_jwe,\n        generate_request_task_callback_jwe,\n        generate_request_callback_resume_jwe,\n    ],\n    ids=[\n        \"pre_approval_jwe\",\n        \"task_callback_jwe\",\n        \"resume_callback_jwe\",\n    ],\n)\ndef test_generate_jwe_invalid_input(jwe_generator):\n    with pytest.raises(AttributeError):\n        jwe_generator(None)\n\n    with pytest.raises(AttributeError):\n        jwe_generator({})\n\n\n@pytest.mark.parametrize(\n    \"jwe_generator\",\n    [\n        generate_request_callback_pre_approval_jwe,\n        generate_request_task_callback_jwe,\n        generate_request_callback_resume_jwe,\n    ],\n    ids=[\n        \"pre_approval_jwe\",\n        \"task_callback_jwe\",\n        \"resume_callback_jwe\",\n    ],\n)\ndef test_generate_jwe_replay_protection(jwe_generator, policy_pre_execution_webhooks):\n    for webhook in policy_pre_execution_webhooks:\n        jwe1 = jwe_generator(webhook)\n        jwe2 = jwe_generator(webhook)\n        assert jwe1 != jwe2  # Ensure the JWEs are different\n"}
{"type": "test_file", "path": "tests/api/v1/endpoints/__init__.py", "content": ""}
{"type": "test_file", "path": "tests/api/v1/endpoints/test_dataset_endpoints.py", "content": "from typing import Generator\n\nimport pytest\nfrom starlette.status import (\n    HTTP_201_CREATED,\n    HTTP_401_UNAUTHORIZED,\n    HTTP_403_FORBIDDEN,\n    HTTP_422_UNPROCESSABLE_ENTITY,\n)\nfrom starlette.testclient import TestClient\n\nfrom fides.api.models.sql_models import Dataset as CtlDataset\nfrom fides.common.api.scope_registry import CTL_DATASET_CREATE, CTL_DATASET_READ\nfrom fides.common.api.v1.urn_registry import DATASETS, V1_URL_PREFIX\nfrom tests.conftest import generate_role_header_for_user\n\n\nclass TestCreateDataset:\n    @pytest.fixture(scope=\"function\")\n    def url(self) -> str:\n        return V1_URL_PREFIX + DATASETS\n\n    @pytest.fixture(scope=\"function\")\n    def dataset_payload(self) -> dict:\n        return {\n            \"fides_key\": \"test_dataset\",\n            \"name\": \"Test Dataset\",\n            \"description\": \"A test dataset\",\n            \"collections\": [],\n        }\n\n    @pytest.fixture(scope=\"function\")\n    def existing_dataset(\n        self, db, dataset_payload\n    ) -> Generator[CtlDataset, None, None]:\n        \"\"\"Create a dataset in the database\"\"\"\n        dataset = CtlDataset.create_from_dataset_dict(db, dataset_payload)\n        yield dataset\n        dataset.delete(db)\n\n    def test_create_dataset_not_authenticated(\n        self, api_client: TestClient, url: str, dataset_payload: dict\n    ) -> None:\n        \"\"\"Test that creating a dataset without authentication returns a 401 error\"\"\"\n        response = api_client.post(url, json=dataset_payload)\n        assert response.status_code == HTTP_401_UNAUTHORIZED\n\n    def test_create_dataset_wrong_scope(\n        self,\n        api_client: TestClient,\n        generate_auth_header,\n        url: str,\n        dataset_payload: dict,\n    ) -> None:\n        \"\"\"Test that creating a dataset with wrong scope returns a 403 error\"\"\"\n        auth_header = generate_auth_header(scopes=[CTL_DATASET_READ])\n        response = api_client.post(url, headers=auth_header, json=dataset_payload)\n        assert response.status_code == HTTP_403_FORBIDDEN\n\n    @pytest.mark.parametrize(\n        \"role,expected_status\",\n        [\n            (\"owner_user\", HTTP_201_CREATED),\n            (\"contributor_user\", HTTP_201_CREATED),\n            (\"viewer_and_approver_user\", HTTP_403_FORBIDDEN),\n            (\"viewer_user\", HTTP_403_FORBIDDEN),\n            (\"approver_user\", HTTP_403_FORBIDDEN),\n        ],\n    )\n    def test_create_dataset_with_role(\n        self,\n        api_client: TestClient,\n        url: str,\n        dataset_payload: dict,\n        db,\n        request,\n        role: str,\n        expected_status: int,\n    ) -> None:\n        \"\"\"Test dataset creation with different user roles\"\"\"\n        # Use a different dataset key for each test to avoid conflicts\n        modified_payload = dataset_payload.copy()\n        modified_payload[\"fides_key\"] = f\"{dataset_payload['fides_key']}_{role}\"\n        modified_payload[\"name\"] = f\"{dataset_payload['name']} {role}\"\n\n        user_role = request.getfixturevalue(role)\n        auth_header = generate_role_header_for_user(\n            user_role, roles=user_role.permissions.roles\n        )\n\n        response = api_client.post(url, headers=auth_header, json=modified_payload)\n        assert response.status_code == expected_status\n\n        # Clean up if dataset was created\n        if expected_status == HTTP_201_CREATED:\n            dataset = (\n                db.query(CtlDataset)\n                .filter(CtlDataset.fides_key == modified_payload[\"fides_key\"])\n                .first()\n            )\n            if dataset:\n                dataset.delete(db)\n\n    @pytest.mark.usefixtures(\"existing_dataset\")\n    def test_create_dataset_key_already_exists(\n        self,\n        api_client: TestClient,\n        generate_auth_header,\n        url: str,\n        dataset_payload: dict,\n    ) -> None:\n        \"\"\"Test that creating a dataset with a key that already exists returns a 422 error\"\"\"\n        auth_header = generate_auth_header(scopes=[CTL_DATASET_CREATE])\n\n        # Try to create a dataset with the same key but different name\n        payload = dataset_payload.copy()\n        payload[\"name\"] = \"Different Name\"\n\n        response = api_client.post(url, headers=auth_header, json=payload)\n\n        assert response.status_code == HTTP_422_UNPROCESSABLE_ENTITY\n        assert (\n            response.json()[\"detail\"]\n            == 'Dataset with fides_key \"test_dataset\" already exists.'\n        )\n\n    @pytest.mark.usefixtures(\"existing_dataset\")\n    def test_create_dataset_duplicate_name(\n        self,\n        api_client: TestClient,\n        generate_auth_header,\n        url: str,\n        dataset_payload: dict,\n    ) -> None:\n        \"\"\"Test that creating a dataset with duplicate names is allowed\"\"\"\n        auth_header = generate_auth_header(scopes=[CTL_DATASET_CREATE])\n\n        # Try to create a dataset with the same name but different key\n        payload = dataset_payload.copy()\n        payload[\"fides_key\"] = \"different_key\"\n\n        response = api_client.post(url, headers=auth_header, json=payload)\n\n        assert response.status_code == HTTP_201_CREATED\n\n    def test_create_dataset(\n        self, api_client: TestClient, generate_auth_header, url: str\n    ) -> None:\n        \"\"\"Test that creating a dataset successfully returns a 201 status code\"\"\"\n        auth_header = generate_auth_header(scopes=[CTL_DATASET_CREATE])\n\n        payload = {\n            \"fides_key\": \"new_test_dataset\",\n            \"name\": \"New Test Dataset\",\n            \"description\": \"A new test dataset\",\n            \"collections\": [],\n        }\n\n        response = api_client.post(url, headers=auth_header, json=payload)\n        assert response.status_code == HTTP_201_CREATED\n"}
{"type": "test_file", "path": "tests/conftest.py", "content": "import asyncio\nimport json\nimport os\nimport time\nfrom datetime import datetime\nfrom pathlib import Path\nfrom typing import Callable\nfrom uuid import uuid4\n\nimport boto3\nimport pytest\nimport requests\nimport yaml\nfrom fastapi import Query\nfrom fastapi.testclient import TestClient\nfrom fideslang import DEFAULT_TAXONOMY, models\nfrom fideslang.models import System as SystemSchema\nfrom httpx import AsyncClient\nfrom loguru import logger\nfrom moto import mock_aws\nfrom sqlalchemy.engine.base import Engine\nfrom sqlalchemy.exc import IntegrityError\nfrom sqlalchemy.ext.asyncio import AsyncSession, create_async_engine\nfrom sqlalchemy.orm import sessionmaker\n\nfrom fides.api.common_exceptions import PrivacyRequestExit\nfrom fides.api.cryptography.schemas.jwt import (\n    JWE_ISSUED_AT,\n    JWE_PAYLOAD_CLIENT_ID,\n    JWE_PAYLOAD_ROLES,\n    JWE_PAYLOAD_SCOPES,\n    JWE_PAYLOAD_SYSTEMS,\n)\nfrom fides.api.db.ctl_session import sync_engine\nfrom fides.api.db.system import create_system\nfrom fides.api.main import app\nfrom fides.api.models.privacy_request import (\n    EXITED_EXECUTION_LOG_STATUSES,\n    RequestTask,\n    generate_request_callback_pre_approval_jwe,\n    generate_request_callback_resume_jwe,\n)\nfrom fides.api.models.sql_models import Cookies, DataUse, PrivacyDeclaration\nfrom fides.api.oauth.jwt import generate_jwe\nfrom fides.api.oauth.roles import APPROVER, CONTRIBUTOR, OWNER, VIEWER_AND_APPROVER\nfrom fides.api.schemas.messaging.messaging import MessagingServiceType\nfrom fides.api.task.graph_runners import access_runner, consent_runner, erasure_runner\nfrom fides.api.tasks import celery_app\nfrom fides.api.tasks.scheduled.scheduler import async_scheduler, scheduler\nfrom fides.api.util.cache import get_cache\nfrom fides.api.util.collection_util import Row\nfrom fides.common.api.scope_registry import SCOPE_REGISTRY\nfrom fides.config import get_config\nfrom fides.config.config_proxy import ConfigProxy\nfrom tests.fixtures.application_fixtures import *\nfrom tests.fixtures.bigquery_fixtures import *\nfrom tests.fixtures.datahub_fixtures import *\nfrom tests.fixtures.detection_discovery_fixtures import *\nfrom tests.fixtures.dynamodb_fixtures import *\nfrom tests.fixtures.email_fixtures import *\nfrom tests.fixtures.fides_connector_example_fixtures import *\nfrom tests.fixtures.google_cloud_sql_mysql_fixtures import *\nfrom tests.fixtures.google_cloud_sql_postgres_fixtures import *\nfrom tests.fixtures.integration_fixtures import *\nfrom tests.fixtures.manual_fixtures import *\nfrom tests.fixtures.manual_webhook_fixtures import *\nfrom tests.fixtures.mariadb_fixtures import *\nfrom tests.fixtures.messaging_fixtures import *\nfrom tests.fixtures.mongodb_fixtures import *\nfrom tests.fixtures.mssql_fixtures import *\nfrom tests.fixtures.mysql_fixtures import *\nfrom tests.fixtures.postgres_fixtures import *\nfrom tests.fixtures.rds_mysql_fixtures import *\nfrom tests.fixtures.rds_postgres_fixtures import *\nfrom tests.fixtures.redshift_fixtures import *\nfrom tests.fixtures.saas import *\nfrom tests.fixtures.saas_erasure_order_fixtures import *\nfrom tests.fixtures.saas_example_fixtures import *\nfrom tests.fixtures.scylladb_fixtures import *\nfrom tests.fixtures.snowflake_fixtures import *\nfrom tests.fixtures.timescale_fixtures import *\n\nROOT_PATH = Path().absolute()\nCONFIG = get_config()\nTEST_CONFIG_PATH = \"tests/ctl/test_config.toml\"\nTEST_INVALID_CONFIG_PATH = \"tests/ctl/test_invalid_config.toml\"\nTEST_DEPRECATED_CONFIG_PATH = \"tests/ctl/test_deprecated_config.toml\"\n\n\n@pytest.fixture\ndef s3_client(storage_config):\n    with mock_aws():\n        session = boto3.Session(\n            aws_access_key_id=\"fake_access_key\",\n            aws_secret_access_key=\"fake_secret_key\",\n            region_name=\"us-east-1\",\n        )\n        s3 = session.client(\"s3\")\n        s3.create_bucket(Bucket=storage_config.details[StorageDetails.BUCKET.value])\n        yield s3\n\n\n@pytest.fixture(scope=\"session\")\ndef db(api_client, config):\n    \"\"\"Return a connection to the test DB\"\"\"\n    # Create the test DB engine\n    assert config.test_mode\n    assert requests.post != api_client.post\n    engine = get_db_engine(\n        database_uri=config.database.sqlalchemy_test_database_uri,\n    )\n\n    create_citext_extension(engine)\n\n    if not scheduler.running:\n        scheduler.start()\n    if not async_scheduler.running:\n        async_scheduler.start()\n\n    SessionLocal = get_db_session(config, engine=engine)\n    the_session = SessionLocal()\n    # Setup above...\n\n    yield the_session\n    # Teardown below...\n    the_session.close()\n    engine.dispose()\n\n\n@pytest.fixture(scope=\"session\")\ndef test_client():\n    \"\"\"Starlette test client fixture. Easier to use mocks with when testing out API calls\"\"\"\n    with TestClient(app) as test_client:\n        yield test_client\n\n\n@pytest.fixture(scope=\"session\")\n@pytest.mark.asyncio\nasync def async_session(test_client):\n    assert CONFIG.test_mode\n    assert requests.post == test_client.post\n\n    create_citext_extension(sync_engine)\n\n    async_engine = create_async_engine(\n        CONFIG.database.async_database_uri,\n        echo=False,\n    )\n\n    session_maker = sessionmaker(\n        async_engine, class_=AsyncSession, expire_on_commit=False\n    )\n\n    async with session_maker() as session:\n        yield session\n        session.close()\n        async_engine.dispose()\n\n\n# TODO: THIS IS A HACKY WORKAROUND.\n# This is specific for this test: test_get_resource_with_custom_field\n# this was added to account for weird error that only happens during a\n# long testing session. Something causes a config/schema change with\n# the DB. Giving the test a dedicated session fixes the issue and\n# matches how runtime works.\n# It does look like there MAY be a small bug that is unlikely to ever\n# occur during runtime. What surfaced the \"benign\" failure is the\n# `connection_configs` relationship on the `System` model. We are\n# unsure of which upstream test causes the error.\n# https://github.com/MagicStack/asyncpg/blob/2f20bae772d71122e64f424cc4124e2ebdd46a58/asyncpg/exceptions/_base.py#L120-L124\n# <class 'asyncpg.exceptions.InvalidCachedStatementError'>: cached statement plan is invalid due to a database schema or configuration change (SQLAlchemy asyncpg dialect will now invalidate all prepared caches in response to this exception)\n@pytest.fixture(scope=\"function\")\n@pytest.mark.asyncio\nasync def async_session_temp(test_client):\n    assert CONFIG.test_mode\n\n    create_citext_extension(sync_engine)\n\n    async_engine = create_async_engine(\n        CONFIG.database.async_database_uri,\n        echo=False,\n    )\n\n    session_maker = sessionmaker(\n        async_engine, class_=AsyncSession, expire_on_commit=False\n    )\n\n    async with session_maker() as session:\n        yield session\n        session.close()\n        async_engine.dispose()\n\n\n@pytest.fixture(scope=\"session\")\ndef api_client():\n    \"\"\"Return a client used to make API requests\"\"\"\n\n    with TestClient(app) as c:\n        yield c\n\n\n@pytest.fixture(scope=\"session\")\nasync def async_api_client():\n    \"\"\"Return an async client used to make API requests\"\"\"\n    async with AsyncClient(\n        app=app, base_url=\"http://0.0.0.0:8080\", follow_redirects=True\n    ) as client:\n        yield client\n\n\n@pytest.fixture(scope=\"session\", autouse=True)\ndef event_loop():\n    try:\n        loop = asyncio.get_running_loop()\n    except RuntimeError:\n        loop = asyncio.new_event_loop()\n    yield loop\n    loop.close()\n\n\n@pytest.fixture(scope=\"session\")\ndef config():\n    CONFIG.test_mode = True\n    yield CONFIG\n\n\n@pytest.fixture(scope=\"function\")\ndef enable_tcf(config):\n    assert config.test_mode\n    config.consent.tcf_enabled = True\n    yield config\n    config.consent.tcf_enabled = False\n\n\n@pytest.fixture(scope=\"function\")\ndef enable_celery_worker(config):\n    \"\"\"This doesn't actually spin up a worker container\"\"\"\n    celery_app.conf[\"task_always_eager\"] = False\n    yield config\n    celery_app.conf[\"task_always_eager\"] = True\n\n\n@pytest.fixture(scope=\"function\")\ndef enable_ac(config):\n    assert config.test_mode\n    config.consent.ac_enabled = True\n    yield config\n    config.consent.ac_enabled = False\n\n\n@pytest.fixture(scope=\"function\")\ndef enable_override_vendor_purposes(config, db):\n    assert config.test_mode\n    config.consent.override_vendor_purposes = True\n    ApplicationConfig.create_or_update(\n        db,\n        data={\"config_set\": {\"consent\": {\"override_vendor_purposes\": True}}},\n    )\n    yield config\n    config.consent.override_vendor_purposes = False\n    ApplicationConfig.create_or_update(\n        db,\n        data={\"config_set\": {\"consent\": {\"override_vendor_purposes\": False}}},\n    )\n\n\n@pytest.fixture(scope=\"function\")\ndef enable_override_vendor_purposes_api_set(db):\n    \"\"\"Enable override vendor purposes via api_set setting, not via traditional app config\"\"\"\n    ApplicationConfig.create_or_update(\n        db,\n        data={\"api_set\": {\"consent\": {\"override_vendor_purposes\": True}}},\n    )\n    yield\n    # reset back to false on teardown\n    ApplicationConfig.create_or_update(\n        db,\n        data={\"api_set\": {\"consent\": {\"override_vendor_purposes\": False}}},\n    )\n\n\n@pytest.fixture\ndef loguru_caplog(caplog):\n    handler_id = logger.add(caplog.handler, format=\"{message} | {extra}\")\n    yield caplog\n    logger.remove(handler_id)\n\n\ndef create_citext_extension(engine: Engine) -> None:\n    with engine.connect() as con:\n        con.execute(\"CREATE EXTENSION IF NOT EXISTS citext;\")\n\n\n@pytest.fixture\ndef fides_toml_path():\n    yield ROOT_PATH / \".fides\" / \"fides.toml\"\n\n\n@pytest.fixture\ndef oauth_client(db):\n    \"\"\"Return a client for authentication purposes.\"\"\"\n    client = ClientDetail(\n        hashed_secret=\"thisisatest\",\n        salt=\"thisisstillatest\",\n        scopes=SCOPE_REGISTRY,\n    )\n    db.add(client)\n    db.commit()\n    db.refresh(client)\n    yield client\n\n\n@pytest.fixture\ndef oauth_root_client(db):\n    \"\"\"Return the configured root client (never persisted)\"\"\"\n    return ClientDetail.get(\n        db,\n        object_id=CONFIG.security.oauth_root_client_id,\n        config=CONFIG,\n        scopes=SCOPE_REGISTRY,\n    )\n\n\n@pytest.fixture\ndef application_user(db, oauth_client):\n    unique_username = f\"user-{uuid4()}\"\n    user = FidesUser.create(\n        db=db,\n        data={\n            \"username\": unique_username,\n            \"password\": \"test_password\",\n            \"email_address\": f\"{unique_username}@ethyca.com\",\n            \"first_name\": \"Test\",\n            \"last_name\": \"User\",\n        },\n    )\n    oauth_client.user_id = user.id\n    oauth_client.save(db=db)\n    yield user\n\n\n@pytest.fixture\ndef user(db):\n    try:\n        user = FidesUser.create(\n            db=db,\n            data={\n                \"username\": \"test_fidesops_user\",\n                \"password\": \"TESTdcnG@wzJeu0&%3Qe2fGo7\",\n                \"email_address\": \"fides.user@ethyca.com\",\n            },\n        )\n        permission = FidesUserPermissions.create(\n            db=db, data={\"user_id\": user.id, \"roles\": [APPROVER]}\n        )\n    except IntegrityError:\n        user = db.query(FidesUser).filter_by(username=\"test_fidesops_user\").first()\n        permission = db.query(FidesUserPermissions).filter_by(user_id=user.id).first()\n    client = ClientDetail(\n        hashed_secret=\"thisisatest\",\n        salt=\"thisisstillatest\",\n        roles=[APPROVER],\n        scopes=[],\n        user_id=user.id,\n    )\n\n    db.add(client)\n    db.commit()\n    db.refresh(client)\n    yield user\n    try:\n        client.delete(db)\n    except ObjectDeletedError:\n        pass\n\n\n@pytest.fixture\ndef auth_header(request, oauth_client, config):\n    client_id = oauth_client.id\n\n    payload = {\n        JWE_PAYLOAD_SCOPES: request.param,\n        JWE_PAYLOAD_CLIENT_ID: client_id,\n        JWE_ISSUED_AT: datetime.now().isoformat(),\n    }\n    jwe = generate_jwe(json.dumps(payload), config.security.app_encryption_key)\n\n    return {\"Authorization\": \"Bearer \" + jwe}\n\n\n@pytest.fixture(autouse=True)\ndef clear_get_config_cache() -> None:\n    get_config.cache_clear()\n\n\n@pytest.fixture(scope=\"session\")\ndef test_config_path():\n    yield TEST_CONFIG_PATH\n\n\n@pytest.fixture(scope=\"session\")\ndef test_deprecated_config_path():\n    yield TEST_DEPRECATED_CONFIG_PATH\n\n\n@pytest.fixture(scope=\"session\")\ndef test_invalid_config_path():\n    \"\"\"\n    This config file contains url/connection strings that are invalid.\n\n    This ensures that the CLI isn't calling out to those resources\n    directly during certain tests.\n    \"\"\"\n    yield TEST_INVALID_CONFIG_PATH\n\n\n@pytest.fixture(scope=\"session\")\ndef test_config(test_config_path: str):\n    yield get_config(test_config_path)\n\n\n@pytest.fixture\ndef test_config_dev_mode_disabled():\n    original_value = CONFIG.dev_mode\n    CONFIG.dev_mode = False\n    yield CONFIG\n    CONFIG.dev_mode = original_value\n\n\n@pytest.fixture\ndef resources_dict():\n    \"\"\"\n    Yields a resource containing sample representations of different\n    Fides resources.\n    \"\"\"\n    resources_dict = {\n        \"data_category\": models.DataCategory(\n            organization_fides_key=\"1\",\n            fides_key=\"user.custom\",\n            parent_key=\"user\",\n            name=\"Custom Data Category\",\n            description=\"Custom Data Category\",\n        ),\n        \"dataset\": models.Dataset(\n            organization_fides_key=\"1\",\n            fides_key=\"test_sample_db_dataset\",\n            name=\"Sample DB Dataset\",\n            description=\"This is a Sample Database Dataset\",\n            collections=[\n                models.DatasetCollection(\n                    name=\"user\",\n                    fields=[\n                        models.DatasetField(\n                            name=\"Food_Preference\",\n                            description=\"User's favorite food\",\n                            path=\"some.path\",\n                        ),\n                        models.DatasetField(\n                            name=\"First_Name\",\n                            description=\"A First Name Field\",\n                            path=\"another.path\",\n                            data_categories=[\"user.name\"],\n                        ),\n                        models.DatasetField(\n                            name=\"Email\",\n                            description=\"User's Email\",\n                            path=\"another.another.path\",\n                            data_categories=[\"user.contact.email\"],\n                        ),\n                        models.DatasetField(\n                            name=\"address\",\n                            description=\"example top level field for nesting\",\n                            path=\"table.address\",\n                            data_categories=[\"user.contact.address\"],\n                            fields=[\n                                models.DatasetField(\n                                    name=\"city\",\n                                    description=\"example city field\",\n                                    path=\"table.address.city\",\n                                    data_categories=[\"user.contact.address.city\"],\n                                ),\n                                models.DatasetField(\n                                    name=\"state\",\n                                    description=\"example state field\",\n                                    path=\"table.address.state\",\n                                    data_categories=[\"user.contact.address.state\"],\n                                ),\n                            ],\n                        ),\n                    ],\n                )\n            ],\n        ),\n        \"data_subject\": models.DataSubject(\n            organization_fides_key=\"1\",\n            fides_key=\"custom_subject\",\n            name=\"Custom Data Subject\",\n            description=\"Custom Data Subject\",\n        ),\n        \"data_use\": models.DataUse(\n            organization_fides_key=\"1\",\n            fides_key=\"custom_data_use\",\n            name=\"Custom Data Use\",\n            description=\"Custom Data Use\",\n        ),\n        \"evaluation\": models.Evaluation(\n            fides_key=\"test_evaluation\", status=\"PASS\", details=[\"foo\"], message=\"bar\"\n        ),\n        \"organization\": models.Organization(\n            fides_key=\"test_organization\",\n            name=\"Test Organization\",\n            description=\"Test Organization\",\n        ),\n        \"policy\": models.Policy(\n            organization_fides_key=\"1\",\n            fides_key=\"test_policy\",\n            name=\"Test Policy\",\n            version=\"1.3\",\n            description=\"Test Policy\",\n            rules=[],\n        ),\n        \"policy_rule\": models.PolicyRule(\n            name=\"Test Policy\",\n            data_categories=models.PrivacyRule(matches=\"NONE\", values=[]),\n            data_uses=models.PrivacyRule(matches=\"NONE\", values=[\"essential.service\"]),\n            data_subjects=models.PrivacyRule(matches=\"ANY\", values=[]),\n        ),\n        \"system\": models.System(\n            organization_fides_key=\"1\",\n            fides_key=\"test_system\",\n            system_type=\"SYSTEM\",\n            name=\"Test System\",\n            description=\"Test Policy\",\n            cookies=[],\n            privacy_declarations=[\n                models.PrivacyDeclaration(\n                    name=\"declaration-name\",\n                    data_categories=[],\n                    data_use=\"essential\",\n                    data_subjects=[],\n                    dataset_references=[],\n                    cookies=[],\n                )\n            ],\n        ),\n    }\n    yield resources_dict\n\n\n@pytest.fixture\ndef test_manifests():\n    test_manifests = {\n        \"manifest_1\": {\n            \"dataset\": [\n                {\n                    \"name\": \"Test Dataset 1\",\n                    \"organization_fides_key\": 1,\n                    \"datasetType\": {},\n                    \"datasetLocation\": \"somedb:3306\",\n                    \"description\": \"Test Dataset 1\",\n                    \"fides_key\": \"some_dataset\",\n                    \"datasetTables\": [],\n                }\n            ],\n            \"system\": [\n                {\n                    \"name\": \"Test System 1\",\n                    \"organization_fides_key\": 1,\n                    \"systemType\": \"mysql\",\n                    \"description\": \"Test System 1\",\n                    \"fides_key\": \"some_system\",\n                }\n            ],\n        },\n        \"manifest_2\": {\n            \"dataset\": [\n                {\n                    \"name\": \"Test Dataset 2\",\n                    \"description\": \"Test Dataset 2\",\n                    \"organization_fides_key\": 1,\n                    \"datasetType\": {},\n                    \"datasetLocation\": \"somedb:3306\",\n                    \"fides_key\": \"another_dataset\",\n                    \"datasetTables\": [],\n                }\n            ],\n            \"system\": [\n                {\n                    \"name\": \"Test System 2\",\n                    \"organization_fides_key\": 1,\n                    \"systemType\": \"mysql\",\n                    \"description\": \"Test System 2\",\n                    \"fides_key\": \"another_system\",\n                }\n            ],\n        },\n    }\n    yield test_manifests\n\n\n@pytest.fixture\ndef populated_manifest_dir(test_manifests, tmp_path):\n    manifest_dir = f\"{tmp_path}/populated_manifest\"\n    os.mkdir(manifest_dir)\n    for manifest in test_manifests.keys():\n        with open(f\"{manifest_dir}/{manifest}.yml\", \"w\") as manifest_file:\n            yaml.dump(test_manifests[manifest], manifest_file)\n    return manifest_dir\n\n\n@pytest.fixture\ndef populated_nested_manifest_dir(test_manifests, tmp_path):\n    manifest_dir = f\"{tmp_path}/populated_nested_manifest\"\n    os.mkdir(manifest_dir)\n    for manifest in test_manifests.keys():\n        nested_manifest_dir = f\"{manifest_dir}/{manifest}\"\n        os.mkdir(nested_manifest_dir)\n        with open(f\"{nested_manifest_dir}/{manifest}.yml\", \"w\") as manifest_file:\n            yaml.dump(test_manifests[manifest], manifest_file)\n    return manifest_dir\n\n\n@pytest.fixture(scope=\"session\")\ndef cache():\n    yield get_cache()\n\n\n@pytest.fixture\ndef root_auth_header(oauth_root_client):\n    \"\"\"Return an auth header for the root client\"\"\"\n    payload = {\n        JWE_PAYLOAD_SCOPES: oauth_root_client.scopes,\n        JWE_PAYLOAD_CLIENT_ID: oauth_root_client.id,\n        JWE_ISSUED_AT: datetime.now().isoformat(),\n    }\n    jwe = generate_jwe(json.dumps(payload), CONFIG.security.app_encryption_key)\n    return {\"Authorization\": \"Bearer \" + jwe}\n\n\ndef generate_auth_header_for_user(user, scopes):\n    payload = {\n        JWE_PAYLOAD_SCOPES: scopes,\n        JWE_PAYLOAD_CLIENT_ID: user.client.id,\n        JWE_ISSUED_AT: datetime.now().isoformat(),\n    }\n    jwe = generate_jwe(json.dumps(payload), CONFIG.security.app_encryption_key)\n    return {\"Authorization\": \"Bearer \" + jwe}\n\n\n@pytest.fixture\ndef generate_auth_header(oauth_client):\n    return _generate_auth_header(oauth_client, CONFIG.security.app_encryption_key)\n\n\n@pytest.fixture\ndef generate_auth_header_ctl_config(oauth_client):\n    return _generate_auth_header(oauth_client, CONFIG.security.app_encryption_key)\n\n\ndef _generate_auth_header(oauth_client, app_encryption_key):\n    client_id = oauth_client.id\n\n    def _build_jwt(scopes):\n        payload = {\n            JWE_PAYLOAD_SCOPES: scopes,\n            JWE_PAYLOAD_CLIENT_ID: client_id,\n            JWE_ISSUED_AT: datetime.now().isoformat(),\n        }\n        jwe = generate_jwe(json.dumps(payload), app_encryption_key)\n        return {\"Authorization\": \"Bearer \" + jwe}\n\n    return _build_jwt\n\n\n@pytest.fixture\ndef generate_policy_webhook_auth_header():\n    def _build_jwt(webhook):\n        jwe = generate_request_callback_resume_jwe(webhook)\n        return {\"Authorization\": \"Bearer \" + jwe}\n\n    return _build_jwt\n\n\n@pytest.fixture\ndef generate_pre_approval_webhook_auth_header():\n    def _build_jwt(webhook):\n        jwe = generate_request_callback_pre_approval_jwe(webhook)\n        return {\"Authorization\": \"Bearer \" + jwe}\n\n    return _build_jwt\n\n\n@pytest.fixture(scope=\"session\")\ndef integration_config():\n    yield load_toml(\"tests/ops/integration_test_config.toml\")\n\n\n@pytest.fixture(scope=\"session\")\ndef celery_config():\n    return {\"task_always_eager\": False}\n\n\n@pytest.fixture(autouse=True, scope=\"session\")\ndef celery_enable_logging():\n    \"\"\"Turns on celery output logs.\"\"\"\n    return True\n\n\n@pytest.fixture(autouse=True, scope=\"session\")\ndef celery_use_virtual_worker(celery_session_worker):\n    \"\"\"\n    This is a catch-all fixture that forces all of our\n    tests to use a virtual celery worker if a registered\n    task is executed within the scope of the test.\n    \"\"\"\n    yield celery_session_worker\n\n\n@pytest.fixture(scope=\"session\")\ndef run_privacy_request_task(celery_session_app):\n    \"\"\"\n    This fixture is the version of the run_privacy_request task that is\n    registered to the `celery_app` fixture which uses the virtualised `celery_worker`\n    \"\"\"\n    yield celery_session_app.tasks[\n        \"fides.api.service.privacy_request.request_runner_service.run_privacy_request\"\n    ]\n\n\nclass DSRThreeTestRunnerTimedOut(Exception):\n    \"\"\"DSR 3.0 Test Runner Timed Out\"\"\"\n\n\ndef wait_for_tasks_to_complete(\n    db: Session, pr: PrivacyRequest, action_type: ActionType\n):\n    \"\"\"Testing Helper for DSR 3.0 - repeatedly checks to see if all Request Tasks\n    have exited so bogged down test doesn't hang\"\"\"\n\n    def all_tasks_have_run(tasks: Query) -> bool:\n        return all(tsk.status in EXITED_EXECUTION_LOG_STATUSES for tsk in tasks)\n\n    db.commit()\n    counter = 0\n    while not all_tasks_have_run(\n        (\n            db.query(RequestTask).filter(\n                RequestTask.privacy_request_id == pr.id,\n                RequestTask.action_type == action_type,\n            )\n        )\n    ):\n        time.sleep(1)\n        counter += 1\n        if counter == 5:\n            raise DSRThreeTestRunnerTimedOut()\n\n\ndef access_runner_tester(\n    privacy_request: PrivacyRequest,\n    policy: Policy,\n    graph: DatasetGraph,\n    connection_configs: List[ConnectionConfig],\n    identity: Dict[str, Any],\n    session: Session,\n):\n    \"\"\"\n    Function for testing the access request for either DSR 2.0 and DSR 3.0\n    \"\"\"\n    try:\n        return access_runner(\n            privacy_request,\n            policy,\n            graph,\n            connection_configs,\n            identity,\n            session,\n            privacy_request_proceed=False,  # This allows the DSR 3.0 Access Runner to be tested in isolation, to just test running the access graph without queuing the privacy request\n        )\n    except PrivacyRequestExit:\n        # DSR 3.0 intentionally raises a PrivacyRequestExit status while it waits for\n        # RequestTasks to finish\n        wait_for_tasks_to_complete(session, privacy_request, ActionType.access)\n        return privacy_request.get_raw_access_results()\n\n\ndef erasure_runner_tester(\n    privacy_request: PrivacyRequest,\n    policy: Policy,\n    graph: DatasetGraph,\n    connection_configs: List[ConnectionConfig],\n    identity: Dict[str, Any],\n    access_request_data: Dict[str, List[Row]],\n    session: Session,\n):\n    \"\"\"\n    Function for testing the erasure runner for either DSR 2.0 and DSR 3.0\n    \"\"\"\n    try:\n        return erasure_runner(\n            privacy_request,\n            policy,\n            graph,\n            connection_configs,\n            identity,\n            access_request_data,\n            session,\n            privacy_request_proceed=False,  # This allows the DSR 3.0 Erasure Runner to be tested in isolation\n        )\n    except PrivacyRequestExit:\n        # DSR 3.0 intentionally raises a PrivacyRequestExit status while it waits\n        # for RequestTasks to finish\n        wait_for_tasks_to_complete(session, privacy_request, ActionType.erasure)\n        return privacy_request.get_raw_masking_counts()\n\n\ndef consent_runner_tester(\n    privacy_request: PrivacyRequest,\n    policy: Policy,\n    graph: DatasetGraph,\n    connection_configs: List[ConnectionConfig],\n    identity: Dict[str, Any],\n    session: Session,\n):\n    \"\"\"\n    Function for testing the consent request for either DSR 2.0 and DSR 3.0\n    \"\"\"\n    try:\n        return consent_runner(\n            privacy_request,\n            policy,\n            graph,\n            connection_configs,\n            identity,\n            session,\n            privacy_request_proceed=False,  # This allows the DSR 3.0 Consent Runner to be tested in isolation, to just test running the consent graph without queuing the privacy request\n        )\n    except PrivacyRequestExit:\n        # DSR 3.0 intentionally raises a PrivacyRequestExit status while it waits for\n        # RequestTasks to finish\n        wait_for_tasks_to_complete(session, privacy_request, ActionType.consent)\n        return privacy_request.get_consent_results()\n\n\n@pytest.fixture(autouse=True, scope=\"session\")\ndef analytics_opt_out():\n    \"\"\"Disable sending analytics when running tests.\"\"\"\n    original_value = CONFIG.user.analytics_opt_out\n    CONFIG.user.analytics_opt_out = True\n    yield\n    CONFIG.user.analytics_opt_out = original_value\n\n\n@pytest.fixture\ndef automatically_approved(db):\n    \"\"\"Do not require manual request approval\"\"\"\n    original_value = CONFIG.execution.require_manual_request_approval\n    CONFIG.execution.require_manual_request_approval = False\n    ApplicationConfig.update_config_set(db, CONFIG)\n    yield\n    CONFIG.execution.require_manual_request_approval = original_value\n    ApplicationConfig.update_config_set(db, CONFIG)\n\n\n@pytest.fixture\ndef require_manual_request_approval(db):\n    \"\"\"Require manual request approval\"\"\"\n    original_value = CONFIG.execution.require_manual_request_approval\n    CONFIG.execution.require_manual_request_approval = True\n    ApplicationConfig.update_config_set(db, CONFIG)\n    yield\n    CONFIG.execution.require_manual_request_approval = original_value\n    ApplicationConfig.update_config_set(db, CONFIG)\n\n\n@pytest.fixture\ndef subject_identity_verification_required(db):\n    \"\"\"Enable identity verification.\"\"\"\n    original_value = CONFIG.execution.subject_identity_verification_required\n    CONFIG.execution.subject_identity_verification_required = True\n    ApplicationConfig.update_config_set(db, CONFIG)\n    yield\n    CONFIG.execution.subject_identity_verification_required = original_value\n    ApplicationConfig.update_config_set(db, CONFIG)\n\n\n@pytest.fixture(autouse=True, scope=\"function\")\ndef subject_identity_verification_not_required(db):\n    \"\"\"Disable identity verification for most tests unless overridden\"\"\"\n    original_value = CONFIG.execution.subject_identity_verification_required\n    CONFIG.execution.subject_identity_verification_required = False\n    ApplicationConfig.update_config_set(db, CONFIG)\n    db.commit()\n    yield\n    CONFIG.execution.subject_identity_verification_required = original_value\n    ApplicationConfig.update_config_set(db, CONFIG)\n    db.commit()\n\n\n@pytest.fixture(scope=\"function\")\ndef disable_consent_identity_verification(db):\n    \"\"\"Fixture to set disable_consent_identity_verification for tests\"\"\"\n    original_value = CONFIG.execution.disable_consent_identity_verification\n    CONFIG.execution.disable_consent_identity_verification = True\n    ApplicationConfig.update_config_set(db, CONFIG)\n    yield\n    CONFIG.execution.disable_consent_identity_verification = original_value\n    ApplicationConfig.update_config_set(db, CONFIG)\n\n\n@pytest.fixture(autouse=True, scope=\"function\")\ndef privacy_request_complete_email_notification_disabled(db):\n    \"\"\"Disable request completion email for most tests unless overridden\"\"\"\n    original_value = CONFIG.notifications.send_request_completion_notification\n    CONFIG.notifications.send_request_completion_notification = False\n    ApplicationConfig.update_config_set(db, CONFIG)\n    db.commit()\n    yield\n    CONFIG.notifications.send_request_completion_notification = original_value\n    ApplicationConfig.update_config_set(db, CONFIG)\n    db.commit()\n\n\n@pytest.fixture(autouse=True, scope=\"function\")\ndef privacy_request_receipt_notification_disabled(db):\n    \"\"\"Disable request receipt notification for most tests unless overridden\"\"\"\n    original_value = CONFIG.notifications.send_request_receipt_notification\n    CONFIG.notifications.send_request_receipt_notification = False\n    ApplicationConfig.update_config_set(db, CONFIG)\n    db.commit()\n    yield\n    CONFIG.notifications.send_request_receipt_notification = original_value\n    ApplicationConfig.update_config_set(db, CONFIG)\n    db.commit()\n\n\n@pytest.fixture(autouse=True, scope=\"function\")\ndef privacy_request_review_notification_disabled(db):\n    \"\"\"Disable request review notification for most tests unless overridden\"\"\"\n    original_value = CONFIG.notifications.send_request_review_notification\n    ApplicationConfig.update_config_set(db, CONFIG)\n    db.commit()\n    CONFIG.notifications.send_request_review_notification = False\n    yield\n    CONFIG.notifications.send_request_review_notification = original_value\n    ApplicationConfig.update_config_set(db, CONFIG)\n    db.commit()\n\n\n@pytest.fixture(scope=\"function\", autouse=True)\ndef set_notification_service_type_mailgun(db):\n    \"\"\"Set default notification service type\"\"\"\n    original_value = CONFIG.notifications.notification_service_type\n    CONFIG.notifications.notification_service_type = MessagingServiceType.mailgun.value\n    ApplicationConfig.update_config_set(db, CONFIG)\n    db.commit()\n    yield\n    CONFIG.notifications.notification_service_type = original_value\n    ApplicationConfig.update_config_set(db, CONFIG)\n    db.commit()\n\n\n@pytest.fixture(scope=\"function\")\ndef set_notification_service_type_to_none(db):\n    \"\"\"Overrides autouse fixture to remove default notification service type\"\"\"\n    original_value = CONFIG.notifications.notification_service_type\n    CONFIG.notifications.notification_service_type = None\n    ApplicationConfig.update_config_set(db, CONFIG)\n    yield\n    CONFIG.notifications.notification_service_type = original_value\n    ApplicationConfig.update_config_set(db, CONFIG)\n\n\n@pytest.fixture(scope=\"function\")\ndef set_notification_service_type_to_twilio_email(db):\n    \"\"\"Overrides autouse fixture to set notification service type to twilio email\"\"\"\n    original_value = CONFIG.notifications.notification_service_type\n    CONFIG.notifications.notification_service_type = (\n        MessagingServiceType.twilio_email.value\n    )\n    ApplicationConfig.update_config_set(db, CONFIG)\n    yield\n    CONFIG.notifications.notification_service_type = original_value\n    ApplicationConfig.update_config_set(db, CONFIG)\n\n\n@pytest.fixture(scope=\"function\")\ndef set_notification_service_type_to_twilio_text(db):\n    \"\"\"Overrides autouse fixture to set notification service type to twilio text\"\"\"\n    original_value = CONFIG.notifications.notification_service_type\n    CONFIG.notifications.notification_service_type = (\n        MessagingServiceType.twilio_text.value\n    )\n    ApplicationConfig.update_config_set(db, CONFIG)\n    yield\n    CONFIG.notifications.notification_service_type = original_value\n    ApplicationConfig.update_config_set(db, CONFIG)\n\n\n@pytest.fixture(scope=\"function\")\ndef set_property_specific_messaging_enabled(db):\n    \"\"\"Overrides autouse fixture to enable property specific messaging\"\"\"\n    original_value = CONFIG.notifications.enable_property_specific_messaging\n    CONFIG.notifications.enable_property_specific_messaging = True\n    ApplicationConfig.update_config_set(db, CONFIG)\n    yield\n    CONFIG.notifications.enable_property_specific_messaging = original_value\n    ApplicationConfig.update_config_set(db, CONFIG)\n\n\n@pytest.fixture(autouse=True, scope=\"function\")\ndef set_property_specific_messaging_disabled(db):\n    \"\"\"Disable property specific messaging for all tests unless overridden\"\"\"\n    original_value = CONFIG.notifications.enable_property_specific_messaging\n    CONFIG.notifications.enable_property_specific_messaging = False\n    ApplicationConfig.update_config_set(db, CONFIG)\n    yield\n    CONFIG.notifications.enable_property_specific_messaging = original_value\n    ApplicationConfig.update_config_set(db, CONFIG)\n\n\n@pytest.fixture(scope=\"session\")\ndef config_proxy(db):\n    return ConfigProxy(db)\n\n\n@pytest.fixture(scope=\"function\")\ndef oauth_role_client(db: Session) -> Generator:\n    \"\"\"Return a client that has all roles for authentication purposes\n    This is not a typical state but this client will then work with any\n    roles a token is given\n    \"\"\"\n    client = ClientDetail(\n        hashed_secret=\"thisisatest\",\n        salt=\"thisisstillatest\",\n        roles=[OWNER, APPROVER, VIEWER, VIEWER_AND_APPROVER, CONTRIBUTOR],\n    )  # Intentionally adding all roles here so the client will always\n    # have a role that matches a role on a token for testing\n    db.add(client)\n    db.commit()\n    db.refresh(client)\n    yield client\n\n\ndef generate_role_header_for_user(user, roles) -> Dict[str, str]:\n    payload = {\n        JWE_PAYLOAD_ROLES: roles,\n        JWE_PAYLOAD_CLIENT_ID: user.client.id,\n        JWE_ISSUED_AT: datetime.now().isoformat(),\n    }\n    jwe = generate_jwe(json.dumps(payload), CONFIG.security.app_encryption_key)\n    return {\"Authorization\": \"Bearer \" + jwe}\n\n\n@pytest.fixture(scope=\"function\")\ndef generate_role_header(oauth_role_client) -> Callable[[Any], Dict[str, str]]:\n    return _generate_auth_role_header(\n        oauth_role_client, CONFIG.security.app_encryption_key\n    )\n\n\ndef _generate_auth_role_header(\n    oauth_role_client, app_encryption_key\n) -> Callable[[Any], Dict[str, str]]:\n    client_id = oauth_role_client.id\n\n    def _build_jwt(roles: List[str]) -> Dict[str, str]:\n        payload = {\n            JWE_PAYLOAD_ROLES: roles,\n            JWE_PAYLOAD_CLIENT_ID: client_id,\n            JWE_ISSUED_AT: datetime.now().isoformat(),\n        }\n        jwe = generate_jwe(json.dumps(payload), app_encryption_key)\n        return {\"Authorization\": \"Bearer \" + jwe}\n\n    return _build_jwt\n\n\n@pytest.fixture(scope=\"function\")\ndef oauth_system_client(db: Session, system) -> Generator:\n    \"\"\"Return a client that has system for authentication purposes\"\"\"\n    client = ClientDetail(\n        hashed_secret=\"thisisatest\",\n        salt=\"thisisstillatest\",\n        systems=[system.id],\n    )  # Intentionally adding all roles here so the client will always\n    # have a role that matches a role on a token for testing\n    db.add(client)\n    db.commit()\n    db.refresh(client)\n    yield client\n\n\n@pytest.fixture(scope=\"function\")\ndef generate_system_manager_header(\n    oauth_system_client,\n) -> Callable[[Any], Dict[str, str]]:\n    return _generate_system_manager_header(\n        oauth_system_client, CONFIG.security.app_encryption_key\n    )\n\n\ndef _generate_system_manager_header(\n    oauth_system_client, app_encryption_key\n) -> Callable[[Any], Dict[str, str]]:\n    client_id = oauth_system_client.id\n\n    def _build_jwt(systems: List[str]) -> Dict[str, str]:\n        payload = {\n            JWE_PAYLOAD_ROLES: [],\n            JWE_PAYLOAD_CLIENT_ID: client_id,\n            JWE_ISSUED_AT: datetime.now().isoformat(),\n            JWE_PAYLOAD_SYSTEMS: systems,\n        }\n        jwe = generate_jwe(json.dumps(payload), app_encryption_key)\n        return {\"Authorization\": \"Bearer \" + jwe}\n\n    return _build_jwt\n\n\n@pytest.fixture\ndef owner_client(db):\n    \"\"\"Return a client with an \"owner\" role for authentication purposes.\"\"\"\n    client = ClientDetail(\n        hashed_secret=\"thisisatest\", salt=\"thisisstillatest\", scopes=[], roles=[OWNER]\n    )\n    db.add(client)\n    db.commit()\n    db.refresh(client)\n    yield client\n    client.delete(db)\n\n\n@pytest.fixture\ndef viewer_client(db):\n    \"\"\"Return a client with a \"viewer\" role for authentication purposes.\"\"\"\n    client = ClientDetail(\n        hashed_secret=\"thisisatest\", salt=\"thisisstillatest\", scopes=[], roles=[VIEWER]\n    )\n    db.add(client)\n    db.commit()\n    db.refresh(client)\n    yield client\n    client.delete(db)\n\n\n@pytest.fixture\ndef owner_user(db):\n    user = FidesUser.create(\n        db=db,\n        data={\n            \"username\": \"test_fides_owner_user\",\n            \"password\": \"TESTdcnG@wzJeu0&%3Qe2fGo7\",\n            \"email_address\": \"owner.user@ethyca.com\",\n        },\n    )\n    client = ClientDetail(\n        hashed_secret=\"thisisatest\",\n        salt=\"thisisstillatest\",\n        scopes=[],\n        roles=[OWNER],\n        user_id=user.id,\n    )\n\n    FidesUserPermissions.create(db=db, data={\"user_id\": user.id, \"roles\": [OWNER]})\n\n    db.add(client)\n    db.commit()\n    db.refresh(client)\n    yield user\n    user.delete(db)\n\n\n@pytest.fixture\ndef approver_user(db):\n    user = FidesUser.create(\n        db=db,\n        data={\n            \"username\": \"test_fides_viewer_user\",\n            \"password\": \"TESTdcnG@wzJeu0&%3Qe2fGo7\",\n            \"email_address\": \"approver.user@ethyca.com\",\n        },\n    )\n    client = ClientDetail(\n        hashed_secret=\"thisisatest\",\n        salt=\"thisisstillatest\",\n        scopes=[],\n        roles=[APPROVER],\n        user_id=user.id,\n    )\n\n    FidesUserPermissions.create(db=db, data={\"user_id\": user.id, \"roles\": [APPROVER]})\n\n    db.add(client)\n    db.commit()\n    db.refresh(client)\n    yield user\n    user.delete(db)\n\n\n@pytest.fixture\ndef viewer_user(db):\n    user = FidesUser.create(\n        db=db,\n        data={\n            \"username\": \"test_fides_viewer_user\",\n            \"password\": \"TESTdcnG@wzJeu0&%3Qe2fGo7\",\n            \"email_address\": \"viewer2.user@ethyca.com\",\n        },\n    )\n    client = ClientDetail(\n        hashed_secret=\"thisisatest\",\n        salt=\"thisisstillatest\",\n        roles=[VIEWER],\n        user_id=user.id,\n    )\n\n    FidesUserPermissions.create(db=db, data={\"user_id\": user.id, \"roles\": [VIEWER]})\n\n    db.add(client)\n    db.commit()\n    db.refresh(client)\n    yield user\n    user.delete(db)\n\n\n@pytest.fixture\ndef contributor_user(db):\n    user = FidesUser.create(\n        db=db,\n        data={\n            \"username\": \"test_fides_contributor_user\",\n            \"password\": \"TESTdcnG@wzJeu0&%3Qe2fGo7\",\n            \"email_address\": \"contributor.user@ethyca.com\",\n        },\n    )\n    client = ClientDetail(\n        hashed_secret=\"thisisatest\",\n        salt=\"thisisstillatest\",\n        scopes=[],\n        roles=[CONTRIBUTOR],\n        user_id=user.id,\n    )\n\n    FidesUserPermissions.create(\n        db=db, data={\"user_id\": user.id, \"roles\": [CONTRIBUTOR]}\n    )\n\n    db.add(client)\n    db.commit()\n    db.refresh(client)\n    yield user\n    user.delete(db)\n\n\n@pytest.fixture\ndef viewer_and_approver_user(db):\n    user = FidesUser.create(\n        db=db,\n        data={\n            \"username\": \"test_fides_viewer_and_approver_user\",\n            \"password\": \"TESTdcnG@wzJeu0&%3Qe2fGo7\",\n            \"email_address\": \"viewerapprover.user@ethyca.com\",\n        },\n    )\n    client = ClientDetail(\n        hashed_secret=\"thisisatest\",\n        salt=\"thisisstillatest\",\n        scopes=[],\n        roles=[VIEWER_AND_APPROVER],\n        user_id=user.id,\n    )\n\n    FidesUserPermissions.create(\n        db=db, data={\"user_id\": user.id, \"roles\": [VIEWER_AND_APPROVER]}\n    )\n\n    db.add(client)\n    db.commit()\n    db.refresh(client)\n    yield user\n    user.delete(db)\n\n\n@pytest.fixture(scope=\"function\")\ndef system(db: Session) -> System:\n    system = System.create(\n        db=db,\n        data={\n            \"fides_key\": f\"system_key-f{uuid4()}\",\n            \"name\": f\"system-{uuid4()}\",\n            \"description\": \"fixture-made-system\",\n            \"organization_fides_key\": \"default_organization\",\n            \"system_type\": \"Service\",\n        },\n    )\n\n    privacy_declaration = PrivacyDeclaration.create(\n        db=db,\n        data={\n            \"name\": \"Collect data for marketing\",\n            \"system_id\": system.id,\n            \"data_categories\": [\"user.device.cookie_id\"],\n            \"data_use\": \"marketing.advertising\",\n            \"data_subjects\": [\"customer\"],\n            \"dataset_references\": None,\n            \"egress\": None,\n            \"ingress\": None,\n        },\n    )\n\n    Cookies.create(\n        db=db,\n        data={\n            \"name\": \"test_cookie\",\n            \"path\": \"/\",\n            \"privacy_declaration_id\": privacy_declaration.id,\n            \"system_id\": system.id,\n        },\n        check_name=False,\n    )\n\n    db.refresh(system)\n    return system\n\n\n@pytest.fixture()\n@pytest.mark.asyncio\nasync def system_async(async_session):\n    \"\"\"Creates a system for testing with an async session, to be used in async tests\"\"\"\n    resource = SystemSchema(\n        fides_key=str(uuid4()),\n        organization_fides_key=\"default_organization\",\n        name=\"test_system_1\",\n        system_type=\"test\",\n        privacy_declarations=[],\n    )\n\n    system = await create_system(\n        resource, async_session, CONFIG.security.oauth_root_client_id\n    )\n    return system\n\n\n@pytest.fixture(scope=\"function\")\ndef system_hidden(db: Session) -> Generator[System, None, None]:\n    system = System.create(\n        db=db,\n        data={\n            \"fides_key\": f\"system_key-f{uuid4()}\",\n            \"name\": f\"system-{uuid4()}\",\n            \"description\": \"fixture-made-system set as hidden\",\n            \"organization_fides_key\": \"default_organization\",\n            \"system_type\": \"Service\",\n            \"hidden\": True,\n        },\n    )\n\n    db.refresh(system)\n    yield system\n    db.delete(system)\n\n\n@pytest.fixture(scope=\"function\")\ndef system_with_cleanup(db: Session) -> Generator[System, None, None]:\n    system = System.create(\n        db=db,\n        data={\n            \"fides_key\": f\"system_key-f{uuid4()}\",\n            \"name\": f\"system-{uuid4()}\",\n            \"description\": \"fixture-made-system\",\n            \"organization_fides_key\": \"default_organization\",\n            \"system_type\": \"Service\",\n        },\n    )\n\n    privacy_declaration = PrivacyDeclaration.create(\n        db=db,\n        data={\n            \"name\": \"Collect data for marketing\",\n            \"system_id\": system.id,\n            \"data_categories\": [\"user.device.cookie_id\"],\n            \"data_use\": \"marketing.advertising\",\n            \"data_subjects\": [\"customer\"],\n            \"dataset_references\": None,\n            \"egress\": None,\n            \"ingress\": None,\n        },\n    )\n\n    Cookies.create(\n        db=db,\n        data={\n            \"name\": \"test_cookie\",\n            \"path\": \"/\",\n            \"privacy_declaration_id\": privacy_declaration.id,\n            \"system_id\": system.id,\n        },\n        check_name=False,\n    )\n\n    ConnectionConfig.create(\n        db=db,\n        data={\n            \"system_id\": system.id,\n            \"connection_type\": \"bigquery\",\n            \"name\": \"test_connection\",\n            \"secrets\": {\"password\": \"test_password\"},\n            \"access\": \"write\",\n        },\n    )\n\n    db.refresh(system)\n    yield system\n    db.delete(system)\n\n\n@pytest.fixture(scope=\"function\")\ndef system_with_dataset_references(db: Session) -> System:\n    ctl_dataset = CtlDataset.create_from_dataset_dict(\n        db, {\"fides_key\": f\"dataset_key-f{uuid4()}\", \"collections\": []}\n    )\n    system = System.create(\n        db=db,\n        data={\n            \"fides_key\": f\"system_key-f{uuid4()}\",\n            \"name\": f\"system-{uuid4()}\",\n            \"description\": \"fixture-made-system\",\n            \"organization_fides_key\": \"default_organization\",\n            \"system_type\": \"Service\",\n            \"dataset_references\": [ctl_dataset.fides_key],\n        },\n    )\n\n    return system\n\n\n@pytest.fixture(scope=\"function\")\ndef system_with_undeclared_data_categories(db: Session) -> System:\n    ctl_dataset = CtlDataset.create_from_dataset_dict(\n        db,\n        {\n            \"fides_key\": f\"dataset_key-f{uuid4()}\",\n            \"collections\": [\n                {\n                    \"name\": \"customer\",\n                    \"fields\": [\n                        {\n                            \"name\": \"email\",\n                            \"data_categories\": [\"user.contact.email\"],\n                        },\n                        {\"name\": \"first_name\"},\n                    ],\n                }\n            ],\n        },\n    )\n    system = System.create(\n        db=db,\n        data={\n            \"fides_key\": f\"system_key-f{uuid4()}\",\n            \"name\": f\"system-{uuid4()}\",\n            \"description\": \"fixture-made-system\",\n            \"organization_fides_key\": \"default_organization\",\n            \"system_type\": \"Service\",\n            \"dataset_references\": [ctl_dataset.fides_key],\n        },\n    )\n\n    return system\n\n\n@pytest.fixture(scope=\"function\")\ndef system_with_a_single_dataset_reference(db: Session) -> System:\n    first_dataset = CtlDataset.create_from_dataset_dict(\n        db,\n        {\n            \"fides_key\": f\"dataset_key-f{uuid4()}\",\n            \"collections\": [\n                {\n                    \"name\": \"loyalty\",\n                    \"fields\": [\n                        {\n                            \"name\": \"id\",\n                            \"data_categories\": [\"user.unique_id\"],\n                        },\n                    ],\n                }\n            ],\n        },\n    )\n    second_dataset = CtlDataset.create_from_dataset_dict(\n        db,\n        {\n            \"fides_key\": f\"dataset_key-f{uuid4()}\",\n            \"collections\": [\n                {\n                    \"name\": \"customer\",\n                    \"fields\": [\n                        {\n                            \"name\": \"shipping_info\",\n                            \"fields\": [\n                                {\n                                    \"name\": \"street\",\n                                    \"data_categories\": [\"user.contact.address.street\"],\n                                }\n                            ],\n                        },\n                        {\n                            \"name\": \"first_name\",\n                            \"data_categories\": [\"user.name.first\"],\n                        },\n                    ],\n                },\n                {\n                    \"name\": \"activity\",\n                    \"fields\": [\n                        {\n                            \"name\": \"last_login\",\n                            \"data_categories\": [\"user.behavior\"],\n                        },\n                    ],\n                },\n            ],\n        },\n    )\n    system = System.create(\n        db=db,\n        data={\n            \"fides_key\": f\"system_key-f{uuid4()}\",\n            \"name\": f\"system-{uuid4()}\",\n            \"description\": \"fixture-made-system\",\n            \"organization_fides_key\": \"default_organization\",\n            \"system_type\": \"Service\",\n            \"dataset_references\": [first_dataset.fides_key, second_dataset.fides_key],\n        },\n    )\n\n    return system\n\n\n@pytest.fixture(scope=\"function\")\ndef privacy_declaration_with_single_dataset_reference(\n    db: Session,\n) -> PrivacyDeclaration:\n    ctl_dataset = CtlDataset.create_from_dataset_dict(\n        db,\n        {\n            \"fides_key\": f\"dataset_key-f{uuid4()}\",\n            \"collections\": [\n                {\n                    \"name\": \"customer\",\n                    \"fields\": [\n                        {\n                            \"name\": \"email\",\n                            \"data_categories\": [\"user.contact.email\"],\n                        },\n                        {\"name\": \"first_name\"},\n                    ],\n                }\n            ],\n        },\n    )\n    system = System.create(\n        db=db,\n        data={\n            \"fides_key\": f\"system_key-f{uuid4()}\",\n            \"name\": f\"system-{uuid4()}\",\n            \"description\": \"fixture-made-system\",\n            \"organization_fides_key\": \"default_organization\",\n            \"system_type\": \"Service\",\n        },\n    )\n\n    privacy_declaration = PrivacyDeclaration.create(\n        db=db,\n        data={\n            \"name\": \"Collect data for third party sharing\",\n            \"system_id\": system.id,\n            \"data_categories\": [\"user.device.cookie_id\"],\n            \"data_use\": \"third_party_sharing\",\n            \"data_subjects\": [\"customer\"],\n            \"dataset_references\": [ctl_dataset.fides_key],\n            \"egress\": None,\n            \"ingress\": None,\n        },\n    )\n\n    return privacy_declaration\n\n\n@pytest.fixture(scope=\"function\")\ndef privacy_declaration_with_multiple_dataset_references(\n    db: Session,\n) -> PrivacyDeclaration:\n    first_dataset = CtlDataset.create_from_dataset_dict(\n        db,\n        {\n            \"fides_key\": f\"dataset_key-f{uuid4()}\",\n            \"collections\": [\n                {\n                    \"name\": \"loyalty\",\n                    \"fields\": [\n                        {\n                            \"name\": \"id\",\n                            \"data_categories\": [\"user.unique_id\"],\n                        },\n                    ],\n                }\n            ],\n        },\n    )\n    second_dataset = CtlDataset.create_from_dataset_dict(\n        db,\n        {\n            \"fides_key\": f\"dataset_key-f{uuid4()}\",\n            \"collections\": [\n                {\n                    \"name\": \"customer\",\n                    \"fields\": [\n                        {\n                            \"name\": \"shipping_info\",\n                            \"fields\": [\n                                {\n                                    \"name\": \"street\",\n                                    \"data_categories\": [\"user.contact.address.street\"],\n                                }\n                            ],\n                        },\n                        {\n                            \"name\": \"first_name\",\n                            \"data_categories\": [\"user.name.first\"],\n                        },\n                    ],\n                },\n                {\n                    \"name\": \"activity\",\n                    \"fields\": [\n                        {\n                            \"name\": \"last_login\",\n                            \"data_categories\": [\"user.behavior\"],\n                        },\n                    ],\n                },\n            ],\n        },\n    )\n    system = System.create(\n        db=db,\n        data={\n            \"fides_key\": f\"system_key-f{uuid4()}\",\n            \"name\": f\"system-{uuid4()}\",\n            \"description\": \"fixture-made-system\",\n            \"organization_fides_key\": \"default_organization\",\n            \"system_type\": \"Service\",\n        },\n    )\n\n    privacy_declaration = PrivacyDeclaration.create(\n        db=db,\n        data={\n            \"name\": \"Collect data for third party sharing\",\n            \"system_id\": system.id,\n            \"data_categories\": [\"user.device.cookie_id\"],\n            \"data_use\": \"third_party_sharing\",\n            \"data_subjects\": [\"customer\"],\n            \"dataset_references\": [first_dataset.fides_key, second_dataset.fides_key],\n            \"egress\": None,\n            \"ingress\": None,\n        },\n    )\n\n    return privacy_declaration\n\n\n@pytest.fixture(scope=\"function\")\ndef system_multiple_decs(db: Session, system: System) -> Generator[System, None, None]:\n    \"\"\"\n    Add an additional PrivacyDeclaration onto the base System to test scenarios with\n    multiple PrivacyDeclarations on a given system\n    \"\"\"\n    PrivacyDeclaration.create(\n        db=db,\n        data={\n            \"name\": \"Collect data for third party sharing\",\n            \"system_id\": system.id,\n            \"data_categories\": [\"user.device.cookie_id\"],\n            \"data_use\": \"third_party_sharing\",\n            \"data_subjects\": [\"customer\"],\n            \"dataset_references\": None,\n            \"egress\": None,\n            \"ingress\": None,\n        },\n    )\n\n    db.refresh(system)\n    yield system\n\n\n@pytest.fixture(scope=\"function\")\ndef system_third_party_sharing(db: Session) -> Generator[System, None, None]:\n    system_third_party_sharing = System.create(\n        db=db,\n        data={\n            \"fides_key\": f\"system_third_party_sharing-f{uuid4()}\",\n            \"name\": f\"system-{uuid4()}\",\n            \"description\": \"fixture-made-system\",\n            \"organization_fides_key\": \"default_organization\",\n            \"system_type\": \"Service\",\n        },\n    )\n\n    PrivacyDeclaration.create(\n        db=db,\n        data={\n            \"name\": \"Collect data for third party sharing\",\n            \"system_id\": system_third_party_sharing.id,\n            \"data_categories\": [\"user.device.cookie_id\"],\n            \"data_use\": \"third_party_sharing\",\n            \"data_subjects\": [\"customer\"],\n            \"dataset_references\": None,\n            \"egress\": None,\n            \"ingress\": None,\n        },\n    )\n    db.refresh(system_third_party_sharing)\n    yield system_third_party_sharing\n    db.delete(system_third_party_sharing)\n\n\n@pytest.fixture(scope=\"function\")\ndef system_provide_service(db: Session) -> System:\n    system_provide_service = System.create(\n        db=db,\n        data={\n            \"fides_key\": f\"system_key-f{uuid4()}\",\n            \"name\": f\"system-{uuid4()}\",\n            \"description\": \"fixture-made-system\",\n            \"organization_fides_key\": \"default_organization\",\n            \"system_type\": \"Service\",\n        },\n    )\n\n    PrivacyDeclaration.create(\n        db=db,\n        data={\n            \"name\": \"The source service, system, or product being provided to the user\",\n            \"system_id\": system_provide_service.id,\n            \"data_categories\": [\"user.device.cookie_id\"],\n            \"data_use\": \"essential.service\",\n            \"data_subjects\": [\"customer\"],\n            \"dataset_references\": None,\n            \"egress\": None,\n            \"ingress\": None,\n        },\n    )\n    db.refresh(system_provide_service)\n    return system_provide_service\n\n\n@pytest.fixture(scope=\"function\")\ndef system_provide_service_operations_support_optimization(db: Session) -> System:\n    system_provide_service_operations_support_optimization = System.create(\n        db=db,\n        data={\n            \"fides_key\": f\"system_key-f{uuid4()}\",\n            \"name\": f\"system-{uuid4()}\",\n            \"description\": \"fixture-made-system\",\n            \"organization_fides_key\": \"default_organization\",\n            \"system_type\": \"Service\",\n        },\n    )\n\n    PrivacyDeclaration.create(\n        db=db,\n        data={\n            \"name\": \"Optimize and improve support operations in order to provide the service\",\n            \"system_id\": system_provide_service_operations_support_optimization.id,\n            \"data_categories\": [\"user.device.cookie_id\"],\n            \"data_use\": \"essential.service.operations.improve\",\n            \"data_subjects\": [\"customer\"],\n            \"dataset_references\": None,\n            \"egress\": None,\n            \"ingress\": None,\n        },\n    )\n    db.refresh(system_provide_service_operations_support_optimization)\n    return system_provide_service_operations_support_optimization\n\n\n@pytest.fixture\ndef system_manager_client(db, system):\n    \"\"\"Return a client assigned to a system for authentication purposes.\"\"\"\n    client = ClientDetail(\n        hashed_secret=\"thisisatest\",\n        salt=\"thisisstillatest\",\n        roles=[],\n        systems=[system.id],\n    )\n    db.add(client)\n    db.commit()\n    db.refresh(client)\n    yield client\n    client.delete(db)\n\n\n@pytest.fixture\ndef connection_client(db, connection_config):\n    \"\"\"Return a client assigned to a connection for authentication purposes.\"\"\"\n    client = ClientDetail(\n        hashed_secret=\"thisisatest\",\n        salt=\"thisisstillatest\",\n        roles=[],\n        systems=[],\n        connections=[connection_config.id],\n    )\n    db.add(client)\n    db.commit()\n    db.refresh(client)\n    yield client\n    client.delete(db)\n\n\n@pytest.fixture(scope=\"function\", autouse=True)\ndef load_default_data_uses(db):\n    for data_use in DEFAULT_TAXONOMY.data_use:\n        # weirdly, only in some test scenarios, we already have the default taxonomy\n        # loaded, in which case the create will throw an error. so we first check existence.\n        if DataUse.get_by(db, field=\"name\", value=data_use.name) is None:\n            DataUse.create(db=db, data=data_use.model_dump(mode=\"json\"))\n\n\n@pytest.fixture\ndef owner_auth_header(owner_user):\n    return generate_role_header_for_user(owner_user, owner_user.client.roles)\n\n\n@pytest.fixture\ndef contributor_auth_header(contributor_user):\n    return generate_role_header_for_user(\n        contributor_user, contributor_user.client.roles\n    )\n\n\n@pytest.fixture\ndef viewer_auth_header(viewer_user):\n    return generate_role_header_for_user(viewer_user, viewer_user.client.roles)\n\n\n@pytest.fixture\ndef approver_auth_header(approver_user):\n    return generate_role_header_for_user(approver_user, approver_user.client.roles)\n\n\n@pytest.fixture\ndef viewer_and_approver_auth_header(viewer_and_approver_user):\n    return generate_role_header_for_user(\n        viewer_and_approver_user, viewer_and_approver_user.client.roles\n    )\n"}
{"type": "test_file", "path": "tests/ctl/__init__.py", "content": ""}
{"type": "test_file", "path": "tests/ctl/api/__init__.py", "content": ""}
{"type": "test_file", "path": "tests/ctl/api/test_admin.py", "content": "# pylint: disable=missing-docstring, redefined-outer-name\nimport pytest\nfrom starlette.testclient import TestClient\n\nfrom fides.api.util.endpoint_utils import API_PREFIX\nfrom fides.config import FidesConfig\n\n\n@pytest.mark.skip(\"Troubleshooting\")\ndef test_db_reset_dev_mode_enabled(\n    test_config: FidesConfig,\n    test_client: TestClient,\n) -> None:\n    assert test_config.dev_mode\n    response = test_client.post(\n        test_config.cli.server_url + API_PREFIX + \"/admin/db/reset/\",\n        headers=test_config.user.auth_header,\n    )\n    assert response.status_code == 200\n    assert response.json() == {\n        \"data\": {\"message\": \"Fides database action performed successfully: reset\"}\n    }\n\n\n@pytest.mark.skip(\"Troubleshooting\")\ndef test_db_reset_dev_mode_disabled(\n    test_config: FidesConfig,\n    test_config_dev_mode_disabled: FidesConfig,  # temporarily switches off config.dev_mode\n    test_client: TestClient,\n) -> None:\n    error_message = (\n        \"Resetting the application database outside of dev_mode is not supported.\"\n    )\n    response = test_client.post(\n        test_config.cli.server_url + API_PREFIX + \"/admin/db/reset/\",\n        headers=test_config.user.auth_header,\n    )\n\n    assert response.status_code == 501\n    assert response.json()[\"detail\"] == error_message\n"}
{"type": "test_file", "path": "tests/ctl/api/test_generate.py", "content": "# pylint: disable=missing-docstring, redefined-outer-name\nfrom base64 import b64decode\nfrom json import dumps, loads\nfrom os import getenv\n\nimport pytest\nfrom starlette.testclient import TestClient\n\nfrom fides.api.api.v1.endpoints.generate import GenerateResponse\nfrom fides.api.util.endpoint_utils import API_PREFIX\nfrom fides.config import FidesConfig\n\nEXTERNAL_CONFIG_BODY = {\n    \"aws\": {\n        \"region_name\": getenv(\"AWS_DEFAULT_REGION\", \"\"),\n        \"aws_access_key_id\": getenv(\"AWS_ACCESS_KEY_ID\", \"\"),\n        \"aws_secret_access_key\": getenv(\"AWS_SECRET_ACCESS_KEY\", \"\"),\n    },\n    \"bigquery\": {\n        \"dataset\": \"fidesopstest\",\n        \"keyfile_creds\": loads(\n            b64decode(getenv(\"BIGQUERY_CONFIG\", \"e30=\").encode(\"utf-8\")).decode(\"utf-8\")\n        ),\n    },\n    \"db\": {\n        \"connection_string\": \"postgresql+psycopg2://postgres:postgres@postgres-test:5432/postgres_example?\"\n    },\n    \"okta\": {\n        \"orgUrl\": \"https://dev-78908748.okta.com\",\n        \"token\": getenv(\"OKTA_CLIENT_TOKEN\", \"\"),\n    },\n    \"dynamodb\": {\n        \"region_name\": getenv(\"DYNAMODB_REGION\", \"\"),\n        \"aws_access_key_id\": getenv(\"DYNAMODB_ACCESS_KEY_ID\", \"\"),\n        \"aws_secret_access_key\": getenv(\"DYNAMODB_ACCESS_KEY\", \"\"),\n    },\n}\n\nEXTERNAL_FAILURE_CONFIG_BODY = {\n    \"aws\": {\n        \"region_name\": getenv(\"AWS_DEFAULT_REGION\", \"\"),\n        \"aws_access_key_id\": \"ILLEGAL_ACCESS_KEY_ID\",\n        \"aws_secret_access_key\": \"ILLEGAL_SECRET_ACCESS_KEY_ID\",\n    },\n    \"bigquery\": {\n        \"dataset\": \"fidesopstest\",\n        \"keyfile_creds\": loads(\n            b64decode(getenv(\"BIGQUERY_CONFIG\", \"e30=\").encode(\"utf-8\")).decode(\"utf-8\")\n        ),\n    },\n    \"db\": {\n        \"connection_string\": \"postgresql+psycopg2://postgres:postgres@postgres-test:5432/INVALID_DB\"\n    },\n    \"okta\": {\n        \"orgUrl\": \"https://dev-78908748.okta.com\",\n        \"token\": \"INVALID_TOKEN\",\n    },\n    \"dynamodb\": {\n        \"region_name\": getenv(\"DYNAMODB_REGION\", \"\"),\n        \"aws_access_key_id\": \"ILLEGAL_ACCESS_KEY_ID\",\n        \"aws_secret_access_key\": \"ILLEGAL_SECRET_ACCESS_KEY_ID\",\n    },\n}\nEXTERNAL_FAILURE_CONFIG_BODY[\"bigquery\"][\"keyfile_creds\"][\n    \"project_id\"\n] = \"INVALID_PROJECT_ID\"\n\nEXPECTED_FAILURE_MESSAGES = {\n    \"aws\": \"The security token included in the request is invalid.\",\n    \"okta\": \"Invalid token provided\",\n    \"db\": 'FATAL:  database \"INVALID_DB\" does not exist\\n\\n(Background on this error at: https://sqlalche.me/e/14/e3q8)',\n    \"bigquery\": \"Invalid project ID 'INVALID_PROJECT_ID'. Project IDs must contain 6-63 lowercase letters, digits, or dashes. Some project IDs also include domain name separated by a colon. IDs must start with a letter and may not end with a dash.\",\n    \"dynamodb\": \"The security token included in the request is invalid.\",\n}\n\n\n@pytest.mark.external\n@pytest.mark.parametrize(\n    \"generate_type, generate_target\",\n    [\n        (\"systems\", \"aws\"),\n        (\"systems\", \"okta\"),\n        (\"datasets\", \"db\"),\n        (\"datasets\", \"bigquery\"),\n        (\"datasets\", \"dynamodb\"),\n    ],\n)\ndef test_generate(\n    test_config: FidesConfig,\n    generate_type: str,\n    generate_target: str,\n    test_client: TestClient,\n) -> None:\n    data = {\n        \"organization_key\": \"default_organization\",\n        \"generate\": {\n            \"config\": EXTERNAL_CONFIG_BODY[generate_target],\n            \"target\": generate_target,\n            \"type\": generate_type,\n        },\n    }\n\n    response = test_client.post(\n        test_config.cli.server_url + API_PREFIX + \"/generate/\",\n        headers=test_config.user.auth_header,\n        data=dumps(data),\n    )\n    generate_response = GenerateResponse.parse_raw(response.text)\n    assert len(generate_response.generate_results) > 0\n    assert response.status_code == 200\n\n\n@pytest.mark.external\n@pytest.mark.parametrize(\n    \"generate_type, generate_target\",\n    [\n        (\"systems\", \"aws\"),\n        (\"systems\", \"okta\"),\n        (\"datasets\", \"db\"),\n        (\"datasets\", \"bigquery\"),\n        (\"datasets\", \"dynamodb\"),\n    ],\n)\ndef test_generate_failure(\n    test_config: FidesConfig,\n    generate_type: str,\n    generate_target: str,\n    test_client: TestClient,\n) -> None:\n    data = {\n        \"organization_key\": \"default_organization\",\n        \"generate\": {\n            \"config\": EXTERNAL_FAILURE_CONFIG_BODY[generate_target],\n            \"target\": generate_target,\n            \"type\": generate_type,\n        },\n    }\n\n    response = test_client.post(\n        test_config.cli.server_url + API_PREFIX + \"/generate/\",\n        headers=test_config.user.auth_header,\n        data=dumps(data),\n    )\n\n    actual_failure_message = loads(response.text)[\"detail\"]\n    print(actual_failure_message)\n    assert EXPECTED_FAILURE_MESSAGES[generate_target] in actual_failure_message\n"}
{"type": "test_file", "path": "tests/ctl/api/test_ui.py", "content": "# pylint: disable=missing-docstring, redefined-outer-name\nimport re\nfrom pathlib import Path\nfrom typing import Dict\nfrom unittest import mock\nfrom unittest.mock import Mock\n\nimport pytest\nimport requests\nfrom starlette.testclient import TestClient\n\nfrom fides.api.ui import generate_route_file_map, match_route, path_is_in_ui_directory\n\n# Path segments of temporary files whose routes are tested.\nSTATIC_FILES = (\n    \"index.html\",\n    \"404.html\",\n    \"dataset.html\",\n    \"dataset/new.html\",\n    \"dataset/[id].html\",\n    \"nested/[...slug].html\",\n    \"multimatch/[first].html\",\n)\n\n\n@pytest.fixture(scope=\"session\")\ndef tmp_static(tmp_path_factory: pytest.TempPathFactory) -> Path:\n    return tmp_path_factory.mktemp(\"static\")\n\n\n@pytest.fixture(scope=\"session\")\ndef route_file_map(tmp_static: Path) -> Dict[re.Pattern, Path]:\n    # Generate the temporary files to test against.\n    for static_segment in STATIC_FILES:\n        static_path = tmp_static / static_segment\n        static_path.parent.mkdir(parents=True, exist_ok=True)\n        static_path.touch()\n\n    return generate_route_file_map(str(tmp_static))\n\n\n@pytest.mark.unit\ndef test_generate_route_file_map(route_file_map: Dict[re.Pattern, Path]) -> None:\n    # Ensure all paths in the map are real files.\n    for path in route_file_map.values():\n        assert path.exists()\n\n\n@pytest.mark.unit\n@pytest.mark.parametrize(\n    \"route,expected\",\n    [\n        (\"index\", \"index.html\"),\n        (\"dataset\", \"dataset.html\"),\n        (\"dataset/\", \"dataset.html\"),\n        (\"dataset/new\", \"dataset/new.html\"),\n        (\"dataset/1234\", \"dataset/[id].html\"),\n        (\"dataset/G00d-Times_R011/\", \"dataset/[id].html\"),\n        (\"nested/you/me/and\", \"nested/[...slug].html\"),\n        (\"nested/the_devil/makes/3/\", \"nested/[...slug].html\"),\n        (\"multimatch/one\", \"multimatch/[first].html\"),\n    ],\n)\ndef test_match_route(\n    tmp_static: Path, route_file_map: Dict[re.Pattern, Path], route: str, expected: str\n) -> None:\n    # Test example routes.\n    assert match_route(route_file_map, route) == tmp_static / expected\n\n\n@pytest.mark.unit\n@mock.patch(\"fides.api.ui.get_path_to_admin_ui_file\")\n@pytest.mark.parametrize(\n    \"route, expected\",\n    [\n        (\"index.html\", True),\n        (\"//etc/passwd\", False),\n        (\"dataset/new.html\", True),\n        (\"//fides/example.env\", False),\n    ],\n)\ndef test_path_is_in_ui_directory(\n    mock_get_path_to_admin_ui_file: Mock, tmp_static: Path, route: str, expected: bool\n):\n    \"\"\"Test various paths for if they are in the UI directory\"\"\"\n    mock_get_path_to_admin_ui_file.return_value = tmp_static\n    assert path_is_in_ui_directory(tmp_static / Path(route)) == expected\n\n\n@pytest.mark.integration\n@pytest.mark.parametrize(\"route, expected\", [(\"/\", 200), (\"//etc/passwd\", 404)])\ndef test_check_file_within_ui_directory(\n    test_client: TestClient, route: str, expected: int\n):\n    \"\"\"Test attempts at retrieving files outside the UI directory\"\"\"\n    # We use localhost:8080 here because otherwise TestClient will strip out\n    # the leading `//` for malicious paths\n    res = test_client.get(f\"http://localhost:8080{route}\")\n    assert res.status_code == expected\n"}
{"type": "test_file", "path": "tests/ctl/api/test_seed.py", "content": "import io\nimport os\nfrom textwrap import dedent\nfrom typing import Generator\nfrom unittest.mock import patch\n\nimport pytest\nimport yaml\nfrom fideslang.default_taxonomy import DEFAULT_TAXONOMY\nfrom fideslang.models import DataCategory, Organization\nfrom sqlalchemy.ext.asyncio import AsyncSession\nfrom sqlalchemy.future import select\n\nfrom fides.api.db import samples, seed\nfrom fides.api.models.connectionconfig import ConnectionConfig\nfrom fides.api.models.datasetconfig import DatasetConfig\nfrom fides.api.models.fides_user import FidesUser\nfrom fides.api.models.policy import ActionType, DrpAction, Policy, Rule, RuleTarget\nfrom fides.api.models.sql_models import Dataset, PolicyCtl, System\nfrom fides.api.util.data_category import filter_data_categories\nfrom fides.config import CONFIG, FidesConfig\nfrom fides.core import api as _api\n\n\n@pytest.fixture(scope=\"function\", name=\"data_category\")\ndef fixture_data_category(test_config: FidesConfig) -> Generator:\n    \"\"\"\n    Fixture that yields a data category and then deletes it for each test run.\n    \"\"\"\n    fides_key = \"foo\"\n    yield DataCategory(fides_key=fides_key, parent_key=None)\n\n    _api.delete(\n        url=test_config.cli.server_url,\n        resource_type=\"data_category\",\n        resource_id=fides_key,\n        headers=CONFIG.user.auth_header,\n    )\n\n\n@pytest.fixture\ndef parent_server_config():\n    original_username = CONFIG.security.parent_server_username\n    original_password = CONFIG.security.parent_server_password\n    CONFIG.security.parent_server_username = \"test_user\"\n    CONFIG.security.parent_server_password = \"Atestpassword1!\"\n    yield\n    CONFIG.security.parent_server_username = original_username\n    CONFIG.security.parent_server_password = original_password\n\n\n@pytest.fixture\ndef parent_server_config_none():\n    original_username = CONFIG.security.parent_server_username\n    original_password = CONFIG.security.parent_server_password\n    CONFIG.security.parent_server_username = None\n    CONFIG.security.parent_server_password = None\n    yield\n    CONFIG.security.parent_server_username = original_username\n    CONFIG.security.parent_server_password = original_password\n\n\n@pytest.fixture\ndef parent_server_config_username_only():\n    original_username = CONFIG.security.parent_server_username\n    original_password = CONFIG.security.parent_server_password\n    CONFIG.security.parent_server_username = \"test_user\"\n    CONFIG.security.parent_server_password = None\n    yield\n    CONFIG.security.parent_server_username = original_username\n    CONFIG.security.parent_server_password = original_password\n\n\n@pytest.fixture\ndef parent_server_config_password_only():\n    original_username = CONFIG.security.parent_server_username\n    original_password = CONFIG.security.parent_server_password\n    CONFIG.security.parent_server_username = None\n    CONFIG.security.parent_server_password = \"Atestpassword1!\"\n    yield\n    CONFIG.security.parent_server_username = original_username\n    CONFIG.security.parent_server_password = original_password\n\n\n@pytest.mark.unit\nclass TestFilterDataCategories:\n    @pytest.mark.skip(\"this times out on CI\")\n    def test_filter_data_categories_excluded(self) -> None:\n        \"\"\"Test that the filter method works as intended\"\"\"\n        excluded_data_categories = [\n            \"user.financial\",\n            \"user.payment\",\n            \"user.authorization\",\n        ]\n        all_data_categories = [\n            \"user.name\",\n            \"user.test\",\n            # These should be excluded\n            \"user.payment\",\n            \"user.payment.financial_account_number\",\n            \"user.authorization.credentials\",\n            \"user.authorization.biometric\",\n            \"user.financial.bank_account\",\n            \"user.financial\",\n        ]\n        expected_result = [\n            \"user.name\",\n            \"user.test\",\n        ]\n        assert filter_data_categories(\n            all_data_categories, excluded_data_categories\n        ) == sorted(expected_result)\n\n    def test_filter_data_categories_no_third_level(self) -> None:\n        \"\"\"Test that the filter method works as intended\"\"\"\n        excluded_data_categories = [\n            \"user.financial\",\n            \"user.payment\",\n            \"user.authorization\",\n        ]\n        all_data_categories = [\n            \"user.name\",\n            \"user.test\",\n            # These should be excluded\n            \"user.payment\",\n            \"user.payment.financial_account_number\",\n            \"user.authorization.credentials\",\n            \"user.authorization.biometric\",\n            \"user.financial.bank_account\",\n            \"user.financial\",\n        ]\n        expected_result = [\n            \"user.name\",\n            \"user.test\",\n        ]\n        assert filter_data_categories(\n            all_data_categories, excluded_data_categories\n        ) == sorted(expected_result)\n\n    def test_filter_data_categories_no_top_level(self) -> None:\n        \"\"\"Test that the filter method works as intended\"\"\"\n        all_data_categories = [\n            \"user\",\n            \"user.name\",\n            \"user.test\",\n        ]\n        expected_result = [\n            \"user.name\",\n            \"user.test\",\n        ]\n        assert filter_data_categories(all_data_categories, []) == expected_result\n\n    def test_filter_data_categories_empty_excluded(self) -> None:\n        \"\"\"Test that the filter method works as intended\"\"\"\n        all_data_categories = [\n            \"user.name\",\n            \"user.payment\",\n            \"user.authorization\",\n            \"user.financial\",\n        ]\n        assert filter_data_categories(all_data_categories, []) == sorted(\n            all_data_categories\n        )\n\n    def test_filter_data_categories_no_exclusions(self) -> None:\n        \"\"\"Test that the filter method works as intended\"\"\"\n        excluded_data_categories = [\"user.payment\"]\n        all_data_categories = [\n            \"user.name\",\n            \"user.authorization\",\n            \"user.financial\",\n        ]\n        assert filter_data_categories(\n            all_data_categories, excluded_data_categories\n        ) == sorted(all_data_categories)\n\n    def test_filter_data_categories_only_return_users(self) -> None:\n        \"\"\"Test that the filter method works as intended\"\"\"\n        all_data_categories = [\n            \"user.name\",\n            \"user.authorization\",\n            \"user.financial\",\n            # These are excluded\n            \"nonuser.foo\",\n            \"anotheruser.foo\",\n        ]\n        expected_categories = [\n            \"user.name\",\n            \"user.authorization\",\n            \"user.financial\",\n        ]\n        assert filter_data_categories(all_data_categories, []) == sorted(\n            expected_categories\n        )\n\n\n@pytest.mark.integration\nclass TestLoadDefaultTaxonomy:\n    \"\"\"Tests related to load_default_taxonomy\"\"\"\n\n    async def test_add_to_default_taxonomy(\n        self,\n        monkeypatch: pytest.MonkeyPatch,\n        test_config: FidesConfig,\n        data_category: DataCategory,\n        async_session: AsyncSession,\n    ) -> None:\n        \"\"\"Should be able to add to the existing default taxonomy\"\"\"\n        result = _api.get(\n            test_config.cli.server_url,\n            \"data_category\",\n            data_category.fides_key,\n            headers=CONFIG.user.auth_header,\n        )\n        assert result.status_code == 404\n\n        updated_default_taxonomy = DEFAULT_TAXONOMY.model_copy()\n        updated_default_taxonomy.data_category.append(data_category)\n\n        monkeypatch.setattr(seed, \"DEFAULT_TAXONOMY\", updated_default_taxonomy)\n        await seed.load_default_resources(async_session)\n\n        result = _api.get(\n            test_config.cli.server_url,\n            \"data_category\",\n            data_category.fides_key,\n            headers=CONFIG.user.auth_header,\n        )\n        assert result.status_code == 200\n\n    async def test_does_not_override_user_changes(\n        self, test_config: FidesConfig, async_session: AsyncSession\n    ) -> None:\n        \"\"\"\n        Loading the default taxonomy should not override user changes\n        to their default taxonomy\n        \"\"\"\n        default_category = DEFAULT_TAXONOMY.data_category[0].model_copy()\n        new_description = \"foo description\"\n        default_category.description = new_description\n        result = _api.update(\n            test_config.cli.server_url,\n            \"data_category\",\n            json_resource=default_category.json(),\n            headers=CONFIG.user.auth_header,\n        )\n        assert result.status_code == 200\n\n        await seed.load_default_resources(async_session)\n        result = _api.get(\n            test_config.cli.server_url,\n            \"data_category\",\n            default_category.fides_key,\n            headers=CONFIG.user.auth_header,\n        )\n        assert result.json()[\"description\"] == new_description\n\n    async def test_does_not_remove_user_added_taxonomies(\n        self,\n        test_config: FidesConfig,\n        data_category: DataCategory,\n        async_session: AsyncSession,\n    ) -> None:\n        \"\"\"\n        Loading the default taxonomy should not delete user additions\n        to their default taxonomy\n        \"\"\"\n        _api.create(\n            test_config.cli.server_url,\n            \"data_category\",\n            json_resource=data_category.json(),\n            headers=CONFIG.user.auth_header,\n        )\n\n        await seed.load_default_resources(async_session)\n\n        result = _api.get(\n            test_config.cli.server_url,\n            \"data_category\",\n            data_category.fides_key,\n            headers=CONFIG.user.auth_header,\n        )\n        assert result.status_code == 200\n\n\n@pytest.mark.usefixtures(\"parent_server_config\")\ndef test_create_or_update_parent_user(db):\n    seed.create_or_update_parent_user()\n    user = FidesUser.get_by(\n        db, field=\"username\", value=CONFIG.security.parent_server_username\n    )\n\n    assert user is not None\n    user.delete(db)\n\n\n@pytest.mark.usefixtures(\"parent_server_config\")\ndef test_create_or_update_parent_user_called_twice(db):\n    \"\"\"\n    Ensure seed method can be called twice with same parent user config,\n    since this is effectively what happens on server restart.\n    \"\"\"\n    seed.create_or_update_parent_user()\n    user = FidesUser.get_by(\n        db, field=\"username\", value=CONFIG.security.parent_server_username\n    )\n\n    assert user is not None\n\n    seed.create_or_update_parent_user()\n    user = FidesUser.get_by(\n        db, field=\"username\", value=CONFIG.security.parent_server_username\n    )\n\n    assert user is not None\n    user.delete(db)\n\n\n@pytest.mark.usefixtures(\"parent_server_config\")\ndef test_create_or_update_parent_user_change_password(db):\n    user = FidesUser.create(\n        db=db,\n        data={\n            \"username\": CONFIG.security.parent_server_username,\n            \"password\": \"Somepassword1!\",\n        },\n    )\n\n    seed.create_or_update_parent_user()\n    db.refresh(user)\n\n    assert user.password_reset_at is not None\n    assert user.credentials_valid(CONFIG.security.parent_server_password) is True\n    user.delete(db)\n\n\n@pytest.mark.usefixtures(\"parent_server_config_none\")\ndef test_create_or_update_parent_user_no_settings(db):\n    seed.create_or_update_parent_user()\n    user = FidesUser.all(db)\n\n    assert user == []\n\n\n@pytest.mark.usefixtures(\"parent_server_config_username_only\")\ndef test_create_or_update_parent_user_username_only():\n    with pytest.raises(ValueError):\n        seed.create_or_update_parent_user()\n\n\n@pytest.mark.usefixtures(\"parent_server_config_password_only\")\ndef test_create_or_update_parent_user_password_only():\n    with pytest.raises(ValueError):\n        seed.create_or_update_parent_user()\n\n\nasync def test_load_default_dsr_policies(\n    db,\n):\n    # seed the default dsr policies and its artifacts\n    seed.load_default_dsr_policies()\n\n    # run some basic checks on its artifacts to make sure they're populated as we expect\n    access_policy: Policy = Policy.get_by(\n        db, field=\"key\", value=seed.DEFAULT_ACCESS_POLICY\n    )\n    assert access_policy.name == \"Default Access Policy\"\n    assert len(access_policy.rules) == 1\n\n    access_rule: Rule = access_policy.rules[0]\n    assert access_rule.key == seed.DEFAULT_ACCESS_POLICY_RULE\n    assert access_rule.name == \"Default Access Rule\"\n    assert access_rule.action_type == ActionType.access.value\n\n    assert len(access_rule.targets) >= 1\n\n    erasure_policy: Policy = Policy.get_by(\n        db, field=\"key\", value=seed.DEFAULT_ERASURE_POLICY\n    )\n    assert erasure_policy.name == \"Default Erasure Policy\"\n    assert len(erasure_policy.rules) == 1\n\n    erasure_rule: Rule = erasure_policy.rules[0]\n    assert erasure_rule.key == seed.DEFAULT_ERASURE_POLICY_RULE\n    assert erasure_rule.name == \"Default Erasure Rule\"\n    assert erasure_rule.action_type == ActionType.erasure.value\n\n    # make some manual changes to the artifacts to test that they will not\n    # be overwritten when we re-run the seed function\n    Policy.create_or_update(\n        db,\n        data={\n            \"name\": \"-- changed policy name --\",\n            \"key\": seed.DEFAULT_ACCESS_POLICY,\n            \"execution_timeframe\": 45,\n            \"drp_action\": DrpAction.access.value,\n        },\n    )\n\n    Rule.create_or_update(\n        db=db,\n        data={\n            \"action_type\": ActionType.access.value,\n            \"name\": \"-- changed access rule name --\",\n            \"key\": seed.DEFAULT_ACCESS_POLICY_RULE,\n            \"policy_id\": access_policy.id,\n        },\n    )\n\n    num_rule_targets = len(access_rule.targets)\n    rule_target: RuleTarget = access_rule.targets[0]\n    rule_target.delete(db)\n\n    assert RuleTarget.get_by(db, field=\"key\", value=rule_target.key) is None\n    db.refresh(access_rule)\n    assert len(access_rule.targets) == num_rule_targets - 1\n\n    # now test that re-running `load_default_dsr_policies()` does not\n    # overwrite any of the manual changed that have been made to the artifacts\n\n    seed.load_default_dsr_policies()\n\n    access_policy: Policy = Policy.get_by(\n        db, field=\"key\", value=seed.DEFAULT_ACCESS_POLICY\n    )\n    db.refresh(access_policy)\n    assert access_policy.name == \"-- changed policy name --\"\n    assert len(access_policy.rules) == 1\n\n    access_rule: Rule = access_policy.rules[0]\n    db.refresh(access_policy)\n    assert access_rule.key == seed.DEFAULT_ACCESS_POLICY_RULE\n    assert access_rule.name == \"-- changed access rule name --\"\n    assert access_rule.action_type == ActionType.access.value\n\n    assert len(access_rule.targets) == num_rule_targets - 1\n\n\nasync def test_load_organizations(loguru_caplog, async_session, monkeypatch):\n    updated_default_taxonomy = DEFAULT_TAXONOMY.model_copy()\n    current_orgs = len(updated_default_taxonomy.organization)\n    updated_default_taxonomy.organization.append(\n        Organization(fides_key=\"new_organization\")\n    )\n\n    monkeypatch.setattr(seed, \"DEFAULT_TAXONOMY\", updated_default_taxonomy)\n    await seed.load_default_organization(async_session)\n\n    assert \"INSERTED 1\" in loguru_caplog.text\n    assert f\"SKIPPED {current_orgs}\" in loguru_caplog.text\n\n\n@pytest.mark.integration\nclass TestLoadSamples:\n    \"\"\"Tests related to load_samples\"\"\"\n\n    SAMPLE_ENV_VARS = {\n        # Include test secrets for Postgres, Mongo, and Stripe, only\n        \"FIDES_DEPLOY__CONNECTORS__POSTGRES__HOST\": \"test-var-expansion\",\n        \"FIDES_DEPLOY__CONNECTORS__POSTGRES__PORT\": \"9090\",\n        \"FIDES_DEPLOY__CONNECTORS__POSTGRES__DBNAME\": \"test-var-db\",\n        \"FIDES_DEPLOY__CONNECTORS__POSTGRES__USERNAME\": \"test-var-user\",\n        \"FIDES_DEPLOY__CONNECTORS__POSTGRES__PASSWORD\": \"&anchor!-test-password\",\n        \"FIDES_DEPLOY__CONNECTORS__POSTGRES__SSH_REQUIRED\": \"false\",\n        \"FIDES_DEPLOY__CONNECTORS__STRIPE__DOMAIN\": \"test-stripe-domain\",\n        \"FIDES_DEPLOY__CONNECTORS__STRIPE__API_KEY\": \"test-stripe-api-key\",\n        \"FIDES_DEPLOY__CONNECTORS__MONGO_HOST\": \"test-var-expansion\",\n        \"FIDES_DEPLOY__CONNECTORS__MONGO_PORT\": \"9090\",\n        \"FIDES_DEPLOY__CONNECTORS__MONGO_DEFAULTAUTHDB\": \"test-var-db\",\n        \"FIDES_DEPLOY__CONNECTORS__MONGO_USERNAME\": \"test-var-user\",\n        \"FIDES_DEPLOY__CONNECTORS__MONGO_PASSWORD\": \"&anchor!-test-password\",\n    }\n\n    @patch.dict(os.environ, SAMPLE_ENV_VARS, clear=True)\n    async def test_load_samples(\n        self,\n        async_session: AsyncSession,\n    ) -> None:\n        \"\"\"\n        Test that we can load the sample resources, connections, and upsert those\n        into the database. See the other tests in this class for more detailed\n        assertions - this one just ensures the e2e result is what we expect: a\n        database full of sample data!\n        \"\"\"\n\n        # Load the sample resources & connections\n        await seed.load_samples(async_session)\n\n        async with async_session.begin():\n            # Check the results are as expected!\n            systems = (await async_session.execute(select(System))).scalars().all()\n            datasets = (await async_session.execute(select(Dataset))).scalars().all()\n            policies = (await async_session.execute(select(PolicyCtl))).scalars().all()\n            connections = (\n                (await async_session.execute(select(ConnectionConfig))).scalars().all()\n            )\n            dataset_configs = (\n                (await async_session.execute(select(DatasetConfig))).scalars().all()\n            )\n            assert len(systems) == 6\n            assert len(datasets) == 5\n            assert len(policies) == 1\n            assert len(connections) == 4\n            assert len(dataset_configs) == 4\n\n            assert sorted([e.fides_key for e in systems]) == [\n                \"cookie_house\",\n                \"cookie_house_custom_request_fields_database\",\n                \"cookie_house_customer_database\",\n                \"cookie_house_loyalty_database\",\n                \"cookie_house_marketing_system\",\n                \"cookie_house_postgresql_database\",\n            ]\n            assert sorted([e.fides_key for e in datasets]) == [\n                \"mongo_test\",\n                \"postgres_example_custom_request_field_dataset\",\n                \"postgres_example_test_dataset\",\n                \"postgres_example_test_extended_dataset\",\n                \"stripe_connector\",\n            ]\n            assert sorted([e.fides_key for e in policies]) == [\"sample_policy\"]\n\n            # NOTE: Only the connections configured by SAMPLE_ENV_VARS above are\n            # expected to exist; the others defined in the sample_connections.yml\n            # will be ignored since they are missing secrets!\n            assert sorted([e.key for e in connections]) == [\n                \"cookie_house_custom_request_fields_database\",\n                \"cookie_house_customer_database_mongodb\",\n                \"cookie_house_postgresql_database\",\n                \"stripe_connector\",\n            ]\n            assert sorted([e.fides_key for e in dataset_configs]) == [\n                \"mongo_test\",\n                \"postgres_example_custom_request_field_dataset\",\n                \"postgres_example_test_dataset\",\n                \"stripe_connector\",\n            ]\n\n    async def test_load_sample_resources(self):\n        \"\"\"\n        Ensure that the resource files in the sample project are all\n        successfully parsed by the load_sample_resources_from_project()\n        function. This makes sure we don't make some changes to the sample\n        project files that aren't automatically loaded into the database via\n        this function.\n\n        NOTE: If you've found this test, that probably means you were making\n        some changes to the code and this failed unexpectedly. Maybe you removed\n        a field, changed a default, or wanted to edit some sample data?\n\n        To fix the test, you just need to ensure the code has all the logic it\n        needs to parse everything from this directory:\n        - src/fides/data/sample_project/sample_resources/*.yml\n\n        See src/fides/api/database/samples.py for details.\n\n        Sorry for the trouble, but we want to ensure there isn't a subtle bug\n        sneaking into our sample project code!\n        \"\"\"\n        error_message = (\n            \"Unexpected error loading sample resources; did you make changes to the sample project? \"\n            \"See tests/ctl/api/test_seed.py for details.\"\n        )\n        try:\n            samples.load_sample_resources_from_project()\n            assert True\n        except Exception as exc:\n            print(exc)\n            assert False, error_message\n\n    @patch.dict(os.environ, SAMPLE_ENV_VARS, clear=True)\n    async def test_load_sample_connections(self):\n        \"\"\"\n        Ensure that the sample connections file in the sample project can be\n        parsed and loaded by the load_sample_connections_from_project() function.\n        This makes sure we don't make some changes to the sample project files\n        that aren't automatically loaded into the database via this function.\n\n        NOTE: If you've found this test, that probably means you were making\n        some changes to the code and this failed unexpectedly. Maybe you removed\n        a field, changed a default, or wanted to edit some sample data?\n\n        To fix the test, you just need to ensure the code has all the logic it\n        needs to parse everything from this directory:\n        - src/fides/data/sample_project/sample_connections/*.yml\n\n        See src/fides/api/database/samples.py for details.\n\n        Sorry for the trouble, but we want to ensure there isn't a subtle bug\n        sneaking into our sample project code!\n        \"\"\"\n        error_message = (\n            \"Unexpected error loading sample connections; did you make changes to the sample project? \"\n            \"See tests/ctl/api/test_seed.py for details.\"\n        )\n        connections = []\n        try:\n            connections = samples.load_sample_connections_from_project()\n        except Exception as exc:\n            print(exc)\n            assert False, error_message\n\n        # Assert that only the connections with all their secrets are returned\n        assert len(connections) == 4\n        assert sorted([e.key for e in connections]) == [\n            \"cookie_house_custom_request_fields_database\",\n            \"cookie_house_customer_database_mongodb\",\n            \"cookie_house_postgresql_database\",\n            \"stripe_connector\",\n        ]\n\n        # Assert that variable expansion worked as expected\n        postgres = [e for e in connections if e.connection_type == \"postgres\"][\n            0\n        ].model_dump(mode=\"json\")\n        assert postgres[\"secrets\"][\"host\"] == \"test-var-expansion\"\n        assert postgres[\"secrets\"][\"port\"] == \"9090\"\n        assert postgres[\"secrets\"][\"password\"] == \"&anchor!-test-password\"\n\n    @patch.dict(\n        os.environ,\n        {\n            \"TEST_VAR_1\": \"var-1\",\n            \"TEST_VAR_2\": \"var-2\",\n        },\n        clear=True,\n    )\n    async def test_load_sample_yaml_file(self):\n        \"\"\"\n        Test that we can safely load, parse, and perform variable expansion on\n        a sample project file.\n        \"\"\"\n        sample_str = dedent(\n            \"\"\"\\\n            connection:\n              - key: test_connection\n                name: Test Connector $TEST_VAR_1\n                connection_type: postgres\n                access: write\n                secrets:\n                  host: test-host\n                  port: 9001\n                  dbname: $TEST_VAR_2\n                  username: user-${TEST_VAR_2}\n                  password: ${TEST_VAR_1}-${TEST_VAR_2}\n        \"\"\"\n        )\n        sample_file = io.StringIO(sample_str)\n\n        sample_dict = samples.load_sample_yaml_file(sample_file)\n        assert list(sample_dict.keys()) == [\"connection\"]\n        sample_connection = sample_dict[\"connection\"][0]\n        assert sample_connection[\"key\"] == \"test_connection\"\n        assert sample_connection[\"name\"] == \"Test Connector var-1\"\n        assert sample_connection[\"connection_type\"] == \"postgres\"\n        assert sample_connection[\"access\"] == \"write\"\n        assert sample_connection[\"secrets\"][\"host\"] == \"test-host\"\n        assert sample_connection[\"secrets\"][\"port\"] == 9001\n        assert sample_connection[\"secrets\"][\"dbname\"] == \"var-2\"\n        assert sample_connection[\"secrets\"][\"username\"] == \"user-var-2\"\n        assert sample_connection[\"secrets\"][\"password\"] == \"var-1-var-2\"\n\n    @patch.dict(\n        os.environ,\n        {\n            \"TEST_PASSWORD\": \"&anchor!'quote'!@#$%^&*\",\n        },\n        clear=True,\n    )\n    async def test_load_sample_yaml_with_special_chars(self):\n        \"\"\"Test that YAML parsing requires proper quoting for environment variables with special characters\"\"\"\n        # Test safe usage with quotes\n        safe_yaml = dedent(\n            \"\"\"\\\n            connection:\n              - key: test_connection\n                name: Test Connection\n                connection_type: postgres\n                access: write\n                secrets:\n                  password: \"$TEST_PASSWORD\"\n            \"\"\"\n        )\n        sample_file = io.StringIO(safe_yaml)\n        sample_dict = samples.load_sample_yaml_file(sample_file)\n        assert (\n            sample_dict[\"connection\"][0][\"secrets\"][\"password\"]\n            == \"&anchor!'quote'!@#$%^&*\"\n        )\n\n        # Test unsafe usage without quotes - should raise YAML parsing error\n        unsafe_yaml = dedent(\n            \"\"\"\\\n            connection:\n              - key: test_connection\n                name: Test Connection\n                connection_type: postgres\n                access: write\n                secrets:\n                  password: $TEST_PASSWORD\n            \"\"\"\n        )\n        sample_file = io.StringIO(unsafe_yaml)\n        with pytest.raises(yaml.scanner.ScannerError):\n            samples.load_sample_yaml_file(sample_file)\n"}
{"type": "test_file", "path": "tests/ctl/api/test_validate.py", "content": "# pylint: disable=missing-docstring, redefined-outer-name\nfrom base64 import b64decode\nfrom json import dumps, loads\nfrom os import getenv\n\nimport pytest\nfrom starlette.testclient import TestClient\n\nfrom fides.api.api.v1.endpoints.validate import ValidateResponse\nfrom fides.api.util.endpoint_utils import API_PREFIX\nfrom fides.config import FidesConfig\n\nEXTERNAL_CONFIG_BODY = {\n    \"aws\": {\n        \"region_name\": getenv(\"AWS_DEFAULT_REGION\", \"\"),\n        \"aws_access_key_id\": getenv(\"AWS_ACCESS_KEY_ID\", \"\"),\n        \"aws_secret_access_key\": getenv(\"AWS_SECRET_ACCESS_KEY\", \"\"),\n    },\n    \"bigquery\": {\n        \"dataset\": \"fidesopstest\",\n        \"keyfile_creds\": loads(\n            b64decode(getenv(\"BIGQUERY_CONFIG\", \"e30=\").encode(\"utf-8\")).decode(\"utf-8\")\n        ),\n    },\n    \"okta\": {\n        \"orgUrl\": \"https://dev-78908748.okta.com\",\n        \"token\": getenv(\"OKTA_CLIENT_TOKEN\", \"\"),\n    },\n}\n\n\n@pytest.mark.external\n@pytest.mark.parametrize(\"validate_target\", [\"aws\", \"okta\", \"bigquery\"])\ndef test_validate_success(\n    test_config: FidesConfig,\n    validate_target: str,\n    test_client: TestClient,\n) -> None:\n    data = {\n        \"config\": EXTERNAL_CONFIG_BODY[validate_target],\n        \"target\": validate_target,\n    }\n\n    response = test_client.post(\n        test_config.cli.server_url + API_PREFIX + \"/validate/\",\n        headers=test_config.user.auth_header,\n        data=dumps(data),\n    )\n\n    validate_response = ValidateResponse.parse_raw(response.text)\n    assert validate_response.status == \"success\"\n    assert validate_response.message == \"Config validation succeeded\"\n    assert response.status_code == 200\n\n\nEXTERNAL_FAILURE_CONFIG_BODY = {\n    \"aws\": {\n        \"region_name\": getenv(\"AWS_DEFAULT_REGION\", \"\"),\n        \"aws_access_key_id\": \"ILLEGAL_ACCESS_KEY_ID\",\n        \"aws_secret_access_key\": \"ILLEGAL_SECRET_ACCESS_KEY_ID\",\n    },\n    \"okta\": {\n        \"orgUrl\": \"https://dev-78908748.okta.com\",\n        \"token\": \"INVALID_TOKEN\",\n    },\n    \"bigquery\": {\n        \"dataset\": \"fidesopstest\",\n        \"keyfile_creds\": loads(\n            b64decode(getenv(\"BIGQUERY_CONFIG\", \"e30=\").encode(\"utf-8\")).decode(\"utf-8\")\n        ),\n    },\n}\n\nEXTERNAL_FAILURE_CONFIG_BODY[\"bigquery\"][\"keyfile_creds\"][\n    \"project_id\"\n] = \"INVALID_PROJECT_ID\"\n\nEXPECTED_FAILURE_MESSAGES = {\n    \"aws\": \"Authentication failed validating config. The security token included in the request is invalid.\",\n    \"okta\": \"Authentication failed validating config. Invalid token provided\",\n    \"bigquery\": \"Unexpected failure validating config. Invalid project ID 'INVALID_PROJECT_ID'. Project IDs must contain 6-63 lowercase letters, digits, or dashes. Some project IDs also include domain name separated by a colon. IDs must start with a letter and may not end with a dash.\",\n}\n\n\n@pytest.mark.external\n@pytest.mark.parametrize(\"validate_target\", [\"aws\", \"okta\", \"bigquery\"])\ndef test_validate_failure(\n    test_config: FidesConfig,\n    validate_target: str,\n    test_client: TestClient,\n) -> None:\n    data = {\n        \"config\": EXTERNAL_FAILURE_CONFIG_BODY[validate_target],\n        \"target\": validate_target,\n    }\n\n    response = test_client.post(\n        test_config.cli.server_url + API_PREFIX + \"/validate/\",\n        headers=test_config.user.auth_header,\n        data=dumps(data),\n    )\n\n    validate_response = ValidateResponse.parse_raw(response.text)\n    assert validate_response.status == \"failure\"\n    assert validate_response.message == EXPECTED_FAILURE_MESSAGES[validate_target]\n    assert response.status_code == 200\n"}
{"type": "test_file", "path": "tests/ctl/cli/test_cli.py", "content": "# pylint: disable=missing-docstring, redefined-outer-name\nimport os\nfrom base64 import b64decode\nfrom json import dump, loads\nfrom typing import Generator\n\nimport pytest\nimport yaml\nfrom click.testing import CliRunner\nfrom git.repo import Repo\nfrom py._path.local import LocalPath\n\nfrom fides.api.oauth.roles import OWNER, VIEWER\nfrom fides.cli import cli\nfrom fides.common.api.scope_registry import SCOPE_REGISTRY\nfrom fides.config import CONFIG\nfrom fides.core.user import get_systems_managed_by_user, get_user_permissions\nfrom fides.core.utils import get_auth_header, read_credentials_file\n\nOKTA_URL = \"https://dev-78908748.okta.com\"\n\n\ndef git_reset(change_dir: str) -> None:\n    \"\"\"This fixture is used to reset the repo files to HEAD.\"\"\"\n\n    git_session = Repo().git()\n    git_session.checkout(\"HEAD\", change_dir)\n\n\n@pytest.fixture()\ndef test_cli_runner() -> Generator:\n    runner = CliRunner()\n    yield runner\n\n\n@pytest.mark.integration\ndef test_init(test_cli_runner: CliRunner) -> None:\n    result = test_cli_runner.invoke(\n        cli, [\"init\"], env={\"FIDES__USER__ANALYTICS_OPT_OUT\": \"true\"}\n    )\n    print(result.output)\n    assert result.exit_code == 0\n\n\n@pytest.mark.integration\ndef test_init_opt_in(test_cli_runner: CliRunner) -> None:\n    result = test_cli_runner.invoke(\n        cli,\n        [\"init\", \"--opt-in\"],\n    )\n    print(result.output)\n    assert result.exit_code == 0\n\n\n@pytest.mark.unit\ndef test_local_flag_invalid_command(test_cli_runner: CliRunner) -> None:\n    result = test_cli_runner.invoke(\n        cli,\n        [\"--local\", \"push\"],\n    )\n    print(result.output)\n    assert result.exit_code == 1\n\n\n@pytest.mark.unit\ndef test_commands_print_help_text_even_on_invalid(\n    test_config_path: str, test_cli_runner: CliRunner, credentials_path: str\n) -> None:\n\n    # the context needs to have a placeholder URL since these tests are testing for behavior when the server is invalid/shutdown\n    result = test_cli_runner.invoke(\n        cli,\n        [\"-f\", test_config_path, \"user\", \"permissions\"],\n        env={\"FIDES_CREDENTIALS_PATH\": \"/root/notarealfile.credentials\"},\n    )\n    assert result.exit_code == 1\n\n    result = test_cli_runner.invoke(\n        cli,\n        [\"-f\", test_config_path, \"user\", \"permissions\", \"--help\"],\n        env={\"FIDES_CREDENTIALS_PATH\": \"/root/notarealfile.credentials\"},\n    )\n    print(f\"results output: {result.output}\")\n    assert result.exit_code == 0\n    assert \"Usage: fides user permissions [OPTIONS]\" in result.output\n\n\n@pytest.mark.unit\ndef test_cli_version(test_cli_runner: CliRunner) -> None:\n    result = test_cli_runner.invoke(cli, [\"--version\"])\n    import fides\n\n    assert f\"fides, version {fides.__version__}\" in result.output\n    assert result.exit_code == 0\n\n\nclass TestView:\n    @pytest.mark.unit\n    def test_view_config(self, test_cli_runner: CliRunner) -> None:\n        result = test_cli_runner.invoke(\n            cli, [\"view\", \"config\"], env={\"FIDES__USER__ANALYTICS_OPT_OUT\": \"true\"}\n        )\n        print(result.output)\n        assert result.exit_code == 0\n\n    @pytest.mark.unit\n    def test_view_credentials(self, test_cli_runner: CliRunner) -> None:\n        result = test_cli_runner.invoke(\n            cli, [\"view\", \"credentials\"], env={\"FIDES__USER__ANALYTICS_OPT_OUT\": \"true\"}\n        )\n        print(result.output)\n        assert result.exit_code == 0\n\n\n@pytest.mark.unit\ndef test_webserver() -> None:\n    \"\"\"\n    This is specifically meant to catch when the webserver command breaks,\n    without spinning up an additional instance.\n    \"\"\"\n    from fides.api.main import start_webserver  # pylint: disable=unused-import\n\n    assert True\n\n\n@pytest.mark.unit\ndef test_worker() -> None:\n    \"\"\"\n    This is specifically meant to catch when the worker command breaks,\n    without spinning up an additional instance.\n    \"\"\"\n    from fides.api.worker import start_worker  # pylint: disable=unused-import\n\n    assert True\n\n\n@pytest.mark.unit\ndef test_parse(test_config_path: str, test_cli_runner: CliRunner) -> None:\n    result = test_cli_runner.invoke(\n        cli, [\"-f\", test_config_path, \"parse\", \"demo_resources/\"]\n    )\n    print(result.output)\n    assert result.exit_code == 0\n\n\nclass TestDB:\n    @pytest.mark.skip(\n        \"This test is timing out only in CI: Safe-Tests (3.10.16, ctl-not-external)\"\n    )\n    @pytest.mark.integration\n    def test_reset_db(self, test_config_path: str, test_cli_runner: CliRunner) -> None:\n        result = test_cli_runner.invoke(\n            cli, [\"-f\", test_config_path, \"db\", \"reset\", \"-y\"]\n        )\n        print(result.output)\n        assert result.exit_code == 0, result.output\n\n    @pytest.mark.integration\n    def test_init_db(self, test_config_path: str, test_cli_runner: CliRunner) -> None:\n        result = test_cli_runner.invoke(cli, [\"-f\", test_config_path, \"db\", \"init\"])\n        print(result.output)\n        assert result.exit_code == 0\n\n    @pytest.mark.integration\n    def test_upgrade_db(\n        self, test_config_path: str, test_cli_runner: CliRunner\n    ) -> None:\n        result = test_cli_runner.invoke(cli, [\"-f\", test_config_path, \"db\", \"upgrade\"])\n        print(result.output)\n        assert result.exit_code == 0\n\n\nclass TestPush:\n    @pytest.mark.integration\n    def test_push(self, test_config_path: str, test_cli_runner: CliRunner) -> None:\n        result = test_cli_runner.invoke(\n            cli, [\"-f\", test_config_path, \"push\", \"demo_resources/\"]\n        )\n        print(result.output)\n        assert result.exit_code == 0\n\n    @pytest.mark.integration\n    def test_dry_push(self, test_config_path: str, test_cli_runner: CliRunner) -> None:\n        result = test_cli_runner.invoke(\n            cli, [\"-f\", test_config_path, \"push\", \"--dry\", \"demo_resources/\"]\n        )\n        print(result.output)\n        assert result.exit_code == 0\n\n    @pytest.mark.integration\n    def test_diff_push(self, test_config_path: str, test_cli_runner: CliRunner) -> None:\n        result = test_cli_runner.invoke(\n            cli, [\"-f\", test_config_path, \"push\", \"--diff\", \"demo_resources/\"]\n        )\n        print(result.output)\n        assert result.exit_code == 0\n\n    @pytest.mark.integration\n    def test_dry_diff_push(\n        self, test_config_path: str, test_cli_runner: CliRunner\n    ) -> None:\n        result = test_cli_runner.invoke(\n            cli, [\"-f\", test_config_path, \"push\", \"--dry\", \"--diff\", \"demo_resources/\"]\n        )\n        print(result.output)\n        assert result.exit_code == 0\n\n\n@pytest.mark.integration\nclass TestPull:\n    def test_pull(\n        self,\n        test_config_path: str,\n        test_cli_runner: CliRunner,\n    ) -> None:\n        \"\"\"\n        Due to the fact that this command checks the real git status, a pytest\n        tmp_dir can't be used. Consequently a real directory must be tested against\n        and then reset.\n        \"\"\"\n        test_dir = \".fides/\"\n        result = test_cli_runner.invoke(cli, [\"-f\", test_config_path, \"pull\", test_dir])\n        git_reset(test_dir)\n        print(result.output)\n        assert result.exit_code == 0\n\n    def test_pull_all(\n        self,\n        test_config_path: str,\n        test_cli_runner: CliRunner,\n    ) -> None:\n        \"\"\"\n        Due to the fact that this command checks the real git status, a pytest\n        tmp_dir can't be used. Consequently a real directory must be tested against\n        and then reset.\n        \"\"\"\n        test_dir = \".fides/\"\n        test_file = \".fides/test_resources.yml\"\n        result = test_cli_runner.invoke(\n            cli,\n            [\n                \"-f\",\n                test_config_path,\n                \"pull\",\n                test_dir,\n                \"-a\",\n                \".fides/test_resources.yml\",\n            ],\n        )\n        git_reset(test_dir)\n        os.remove(test_file)\n        print(result.output)\n        assert result.exit_code == 0\n\n    def test_pull_one_resource(\n        self,\n        test_config_path: str,\n        test_cli_runner: CliRunner,\n    ) -> None:\n        \"\"\"\n        Pull only one dataset into an empty dir and check if the file is created.\n        \"\"\"\n        test_dir = \".fides/\"\n        result = test_cli_runner.invoke(\n            cli, [\"-f\", test_config_path, \"pull\", \"data_category\", \"system\"]\n        )\n        git_reset(test_dir)\n        print(result.output)\n        assert result.exit_code == 0\n        assert \"not found\" not in result.output\n\n\n@pytest.mark.integration\nclass TestAnnotate:\n\n    def test_annotate(\n        self,\n        test_config_path: str,\n        test_cli_runner: CliRunner,\n    ) -> None:\n        \"\"\"\n        Test annotating dataset allowing you to interactively annotate the dataset with data categories\n        \"\"\"\n        with open(\n            \"tests/ctl/data/dataset_missing_categories.yml\", \"r\"\n        ) as current_dataset_yml:\n            dataset_yml = yaml.safe_load(current_dataset_yml)\n            # Confirm starting state, that the first field has no data categories\n            assert (\n                \"data_categories\"\n                not in dataset_yml[\"dataset\"][0][\"collections\"][0][\"fields\"][0]\n            )\n\n        result = test_cli_runner.invoke(\n            cli,\n            [\n                \"-f\",\n                test_config_path,\n                \"annotate\",\n                \"dataset\",\n                \"tests/ctl/data/dataset_missing_categories.yml\",\n            ],\n            input=\"user\\n\",\n        )\n        print(result.output)\n        with open(\"tests/ctl/data/dataset_missing_categories.yml\", \"r\") as dataset_yml:\n            # Helps assert that the data category was output correctly\n            dataset_yml = yaml.safe_load(dataset_yml)\n            assert dataset_yml[\"dataset\"][0][\"collections\"][0][\"fields\"][0][\n                \"data_categories\"\n            ] == [\"user\"]\n\n            # Now remove the data category that was written by annotate dataset\n            del dataset_yml[\"dataset\"][0][\"collections\"][0][\"fields\"][0][\n                \"data_categories\"\n            ]\n\n        with open(\n            \"tests/ctl/data/dataset_missing_categories.yml\", \"w\"\n        ) as current_dataset_yml:\n            # Restore the original contents to the file\n            yaml.safe_dump(dataset_yml, current_dataset_yml)\n\n        assert result.exit_code == 0\n        print(result.output)\n\n    def test_regression_annotate_dataset(\n        self,\n        test_config_path: str,\n        test_cli_runner: CliRunner,\n    ):\n        test_cli_runner.invoke(\n            cli,\n            [\n                \"-f\",\n                test_config_path,\n                \"annotate\",\n                \"dataset\",\n                \"tests/ctl/data/failing_direction.yml\",\n            ],\n            input=\"user\\n\",\n        )\n        with open(\"tests/ctl/data/failing_direction.yml\", \"r\") as dataset_yml:\n            try:\n                dataset_yml = yaml.safe_load(dataset_yml)\n            except yaml.constructor.ConstructorError:\n                assert False, \"The yaml file is not valid\"\n\n\n@pytest.mark.integration\ndef test_audit(test_config_path: str, test_cli_runner: CliRunner) -> None:\n    result = test_cli_runner.invoke(cli, [\"-f\", test_config_path, \"evaluate\", \"-a\"])\n    print(result.output)\n    assert result.exit_code == 0\n\n\n@pytest.mark.integration\nclass TestCRUD:\n    def test_get(self, test_config_path: str, test_cli_runner: CliRunner) -> None:\n        result = test_cli_runner.invoke(\n            cli,\n            [\"-f\", test_config_path, \"get\", \"data_category\", \"user\"],\n        )\n        print(result.output)\n        assert result.exit_code == 0\n\n    def test_delete(self, test_config_path: str, test_cli_runner: CliRunner) -> None:\n        result = test_cli_runner.invoke(\n            cli,\n            [\"-f\", test_config_path, \"delete\", \"system\", \"demo_marketing_system\"],\n        )\n        print(result.output)\n        assert result.exit_code == 0\n\n    def test_ls(self, test_config_path: str, test_cli_runner: CliRunner) -> None:\n        result = test_cli_runner.invoke(cli, [\"-f\", test_config_path, \"ls\", \"system\"])\n        print(result.output)\n        assert result.exit_code == 0\n\n    def test_ls_verbose(\n        self, test_config_path: str, test_cli_runner: CliRunner\n    ) -> None:\n        result = test_cli_runner.invoke(\n            cli, [\"-f\", test_config_path, \"ls\", \"system\", \"--verbose\"]\n        )\n        print(result.output)\n        assert result.exit_code == 0\n\n    def test_ls_no_resources_found(\n        self, test_config_path: str, test_cli_runner: CliRunner\n    ) -> None:\n        \"\"\"This test only works because we don't have any system resources by default.\"\"\"\n        result = test_cli_runner.invoke(cli, [\"-f\", test_config_path, \"ls\", \"system\"])\n        print(result.output)\n        assert result.exit_code == 0\n\n\nclass TestEvaluate:\n    @pytest.mark.integration\n    def test_evaluate_with_declaration_pass(\n        self, test_config_path: str, test_cli_runner: CliRunner\n    ) -> None:\n        result = test_cli_runner.invoke(\n            cli,\n            [\n                \"-f\",\n                test_config_path,\n                \"evaluate\",\n                \"tests/ctl/data/passing_declaration_taxonomy.yml\",\n            ],\n        )\n        print(result.output)\n        assert result.exit_code == 0\n\n    @pytest.mark.integration\n    def test_evaluate_demo_resources_pass(\n        self, test_config_path: str, test_cli_runner: CliRunner\n    ) -> None:\n        result = test_cli_runner.invoke(\n            cli,\n            [\"-f\", test_config_path, \"evaluate\", \"demo_resources/\"],\n        )\n        print(result.output)\n        assert result.exit_code == 0\n\n    @pytest.mark.integration\n    def test_local_evaluate(\n        self, test_invalid_config_path: str, test_cli_runner: CliRunner\n    ) -> None:\n        result = test_cli_runner.invoke(\n            cli,\n            [\n                \"--local\",\n                \"-f\",\n                test_invalid_config_path,\n                \"evaluate\",\n                \"tests/ctl/data/passing_declaration_taxonomy.yml\",\n            ],\n        )\n        print(result.output)\n        assert result.exit_code == 0\n\n    @pytest.mark.integration\n    def test_local_evaluate_demo_resources(\n        self, test_invalid_config_path: str, test_cli_runner: CliRunner\n    ) -> None:\n        result = test_cli_runner.invoke(\n            cli,\n            [\n                \"--local\",\n                \"-f\",\n                test_invalid_config_path,\n                \"evaluate\",\n                \"demo_resources/\",\n            ],\n        )\n        print(result.output)\n        assert result.exit_code == 0\n\n    @pytest.mark.integration\n    def test_evaluate_with_key_pass(\n        self, test_config_path: str, test_cli_runner: CliRunner\n    ) -> None:\n        result = test_cli_runner.invoke(\n            cli,\n            [\n                \"-f\",\n                test_config_path,\n                \"evaluate\",\n                \"-k\",\n                \"primary_privacy_policy\",\n                \"tests/ctl/data/passing_declaration_taxonomy.yml\",\n            ],\n        )\n        print(result.output)\n        assert result.exit_code == 0\n\n    @pytest.mark.integration\n    def test_evaluate_with_declaration_failed(\n        self, test_config_path: str, test_cli_runner: CliRunner\n    ) -> None:\n        result = test_cli_runner.invoke(\n            cli,\n            [\n                \"-f\",\n                test_config_path,\n                \"evaluate\",\n                \"tests/ctl/data/failing_declaration_taxonomy.yml\",\n            ],\n        )\n        print(result.output)\n        assert result.exit_code == 1\n\n    @pytest.mark.integration\n    def test_evaluate_with_dataset_failed(\n        self, test_config_path: str, test_cli_runner: CliRunner\n    ) -> None:\n        result = test_cli_runner.invoke(\n            cli,\n            [\n                \"-f\",\n                test_config_path,\n                \"evaluate\",\n                \"tests/ctl/data/failing_dataset_taxonomy.yml\",\n            ],\n        )\n        print(result.output)\n        assert result.exit_code == 1\n\n    @pytest.mark.integration\n    def test_evaluate_with_dataset_field_failed(\n        self, test_config_path: str, test_cli_runner: CliRunner\n    ) -> None:\n        result = test_cli_runner.invoke(\n            cli,\n            [\n                \"-f\",\n                test_config_path,\n                \"evaluate\",\n                \"tests/ctl/data/failing_dataset_collection_taxonomy.yml\",\n            ],\n        )\n        print(result.output)\n        assert result.exit_code == 1\n\n    @pytest.mark.integration\n    def test_evaluate_with_dataset_collection_failed(\n        self, test_config_path: str, test_cli_runner: CliRunner\n    ) -> None:\n        result = test_cli_runner.invoke(\n            cli,\n            [\n                \"-f\",\n                test_config_path,\n                \"evaluate\",\n                \"tests/ctl/data/failing_dataset_field_taxonomy.yml\",\n            ],\n        )\n        print(result.output)\n        assert result.exit_code == 1\n\n    @pytest.mark.integration\n    def test_evaluate_nested_field_fails(\n        self, test_config_path: str, test_cli_runner: CliRunner\n    ) -> None:\n        \"\"\"\n        Tests a taxonomy that is rigged to fail only due to\n        one of the nested fields violating the policy. Test\n        will fail if the nested field is not discovered.\n        \"\"\"\n        result = test_cli_runner.invoke(\n            cli,\n            [\n                \"-f\",\n                test_config_path,\n                \"evaluate\",\n                \"tests/ctl/data/failing_nested_dataset.yml\",\n            ],\n        )\n        print(result.output)\n        assert result.exit_code == 1\n\n\nclass TestScan:\n    @pytest.mark.integration\n    def test_scan_dataset_db_input_connection_string(\n        self, test_config_path: str, test_cli_runner: CliRunner\n    ) -> None:\n        result = test_cli_runner.invoke(\n            cli,\n            [\n                \"-f\",\n                test_config_path,\n                \"scan\",\n                \"dataset\",\n                \"db\",\n                \"--connection-string\",\n                \"postgresql+psycopg2://postgres:fides@fides-db:5432/fides_test\",\n                \"--coverage-threshold\",\n                \"0\",\n            ],\n        )\n        print(result.output)\n        assert result.exit_code == 0\n\n    @pytest.mark.integration\n    def test_scan_dataset_db_input_credentials_id(\n        self, test_config_path: str, test_cli_runner: CliRunner\n    ) -> None:\n        result = test_cli_runner.invoke(\n            cli,\n            [\n                \"-f\",\n                test_config_path,\n                \"scan\",\n                \"dataset\",\n                \"db\",\n                \"--credentials-id\",\n                \"postgres_1\",\n                \"--coverage-threshold\",\n                \"0\",\n            ],\n        )\n        print(result.output)\n        assert result.exit_code == 0\n\n    @pytest.mark.integration\n    def test_scan_dataset_db_local_flag(\n        self, test_config_path: str, test_cli_runner: CliRunner\n    ) -> None:\n        result = test_cli_runner.invoke(\n            cli,\n            [\n                \"-f\",\n                test_config_path,\n                \"--local\",\n                \"scan\",\n                \"dataset\",\n                \"db\",\n                \"--credentials-id\",\n                \"postgres_1\",\n                \"--coverage-threshold\",\n                \"0\",\n            ],\n        )\n        print(result.output)\n        assert result.exit_code == 0\n\n    @pytest.mark.external\n    def test_scan_system_aws_environment_credentials(\n        self, test_config_path: str, test_cli_runner: CliRunner\n    ) -> None:\n        result = test_cli_runner.invoke(\n            cli,\n            [\n                \"-f\",\n                test_config_path,\n                \"scan\",\n                \"system\",\n                \"aws\",\n                \"--coverage-threshold\",\n                \"0\",\n            ],\n        )\n        print(result.output)\n        assert result.exit_code == 0\n\n    @pytest.mark.external\n    def test_scan_system_aws_input_credential_options(\n        self, test_config_path: str, test_cli_runner: CliRunner\n    ) -> None:\n        result = test_cli_runner.invoke(\n            cli,\n            [\n                \"-f\",\n                test_config_path,\n                \"scan\",\n                \"system\",\n                \"aws\",\n                \"--coverage-threshold\",\n                \"0\",\n                \"--access_key_id\",\n                os.environ[\"AWS_ACCESS_KEY_ID\"],\n                \"--secret_access_key\",\n                os.environ[\"AWS_SECRET_ACCESS_KEY\"],\n                \"--region\",\n                os.environ[\"AWS_DEFAULT_REGION\"],\n            ],\n        )\n        print(result.output)\n        assert result.exit_code == 0\n\n    @pytest.mark.external\n    def test_scan_system_aws_input_credentials_id(\n        self, test_config_path: str, test_cli_runner: CliRunner\n    ) -> None:\n        os.environ[\"FIDES__CREDENTIALS__AWS_1__AWS_ACCESS_KEY_ID\"] = os.environ[\n            \"AWS_ACCESS_KEY_ID\"\n        ]\n        os.environ[\"FIDES__CREDENTIALS__AWS_1__AWS_SECRET_ACCESS_KEY\"] = os.environ[\n            \"AWS_SECRET_ACCESS_KEY\"\n        ]\n\n        result = test_cli_runner.invoke(\n            cli,\n            [\n                \"-f\",\n                test_config_path,\n                \"scan\",\n                \"system\",\n                \"aws\",\n                \"--coverage-threshold\",\n                \"0\",\n                \"--credentials-id\",\n                \"aws_1\",\n            ],\n        )\n        print(result.output)\n        assert result.exit_code == 0\n\n    @pytest.mark.external\n    def test_scan_system_okta_input_credential_options(\n        self, test_config_path: str, test_cli_runner: CliRunner\n    ) -> None:\n        token = os.environ[\"OKTA_CLIENT_TOKEN\"]\n        result = test_cli_runner.invoke(\n            cli,\n            [\n                \"-f\",\n                test_config_path,\n                \"scan\",\n                \"system\",\n                \"okta\",\n                \"--org-url\",\n                OKTA_URL,\n                \"--token\",\n                token,\n                \"--coverage-threshold\",\n                \"0\",\n            ],\n        )\n        print(result.output)\n        assert result.exit_code == 0\n\n    @pytest.mark.external\n    def test_scan_system_okta_input_credentials_id(\n        self,\n        test_config_path: str,\n        test_cli_runner: CliRunner,\n    ) -> None:\n        os.environ[\"FIDES__CREDENTIALS__OKTA_1__TOKEN\"] = os.environ[\n            \"OKTA_CLIENT_TOKEN\"\n        ]\n        result = test_cli_runner.invoke(\n            cli,\n            [\n                \"-f\",\n                test_config_path,\n                \"scan\",\n                \"system\",\n                \"okta\",\n                \"--credentials-id\",\n                \"okta_1\",\n                \"--coverage-threshold\",\n                \"0\",\n            ],\n        )\n        print(result.output)\n        assert result.exit_code == 0\n\n    @pytest.mark.external\n    def test_scan_system_okta_environment_credentials(\n        self,\n        test_config_path: str,\n        test_cli_runner: CliRunner,\n    ) -> None:\n        os.environ[\"OKTA_CLIENT_ORGURL\"] = OKTA_URL\n        result = test_cli_runner.invoke(\n            cli,\n            [\n                \"-f\",\n                test_config_path,\n                \"scan\",\n                \"system\",\n                \"okta\",\n                \"--coverage-threshold\",\n                \"0\",\n            ],\n        )\n        print(result.output)\n        assert result.exit_code == 0\n\n\nclass TestGenerate:\n    @pytest.mark.integration\n    def test_generate_dataset_db_with_connection_string(\n        self,\n        test_config_path: str,\n        test_cli_runner: CliRunner,\n        tmpdir: LocalPath,\n    ) -> None:\n        tmp_file = tmpdir.join(\"dataset.yml\")\n        result = test_cli_runner.invoke(\n            cli,\n            [\n                \"-f\",\n                test_config_path,\n                \"generate\",\n                \"dataset\",\n                \"db\",\n                f\"{tmp_file}\",\n                \"--connection-string\",\n                \"postgresql+psycopg2://postgres:fides@fides-db:5432/fides_test\",\n            ],\n        )\n        print(result.output)\n        assert result.exit_code == 0\n\n        with open(tmp_file, \"r\") as dataset_yml:\n            # Helps assert that the file was output correctly, namely, fides_keys were serialized as strings\n            # and not a FidesKey python object\n            dataset = yaml.safe_load(dataset_yml).get(\"dataset\", [])\n            assert isinstance(dataset[0][\"fides_key\"], str)\n\n    @pytest.mark.integration\n    def test_generate_dataset_db_with_credentials_id(\n        self,\n        test_config_path: str,\n        test_cli_runner: CliRunner,\n        tmpdir: LocalPath,\n    ) -> None:\n        tmp_file = tmpdir.join(\"dataset.yml\")\n        result = test_cli_runner.invoke(\n            cli,\n            [\n                \"-f\",\n                test_config_path,\n                \"generate\",\n                \"dataset\",\n                \"db\",\n                f\"{tmp_file}\",\n                \"--credentials-id\",\n                \"postgres_1\",\n            ],\n        )\n        print(result.output)\n        assert result.exit_code == 0\n\n    @pytest.mark.external\n    def test_generate_system_aws_input_credential_options(\n        self,\n        test_config_path: str,\n        test_cli_runner: CliRunner,\n        tmpdir: LocalPath,\n    ) -> None:\n        tmp_file = tmpdir.join(\"system.yml\")\n        result = test_cli_runner.invoke(\n            cli,\n            [\n                \"-f\",\n                test_config_path,\n                \"generate\",\n                \"system\",\n                \"aws\",\n                f\"{tmp_file}\",\n                \"--access_key_id\",\n                os.environ[\"AWS_ACCESS_KEY_ID\"],\n                \"--secret_access_key\",\n                os.environ[\"AWS_SECRET_ACCESS_KEY\"],\n                \"--region\",\n                os.environ[\"AWS_DEFAULT_REGION\"],\n            ],\n        )\n        print(result.output)\n        assert result.exit_code == 0\n\n    @pytest.mark.external\n    def test_generate_system_aws_environment_credentials(\n        self,\n        test_config_path: str,\n        test_cli_runner: CliRunner,\n        tmpdir: LocalPath,\n    ) -> None:\n        tmp_file = tmpdir.join(\"system.yml\")\n        result = test_cli_runner.invoke(\n            cli,\n            [\"-f\", test_config_path, \"generate\", \"system\", \"aws\", f\"{tmp_file}\"],\n        )\n        print(result.output)\n        assert result.exit_code == 0\n\n    @pytest.mark.external\n    def test_generate_system_aws_input_credentials_id(\n        self,\n        test_config_path: str,\n        test_cli_runner: CliRunner,\n        tmpdir: LocalPath,\n    ) -> None:\n        os.environ[\"FIDES__CREDENTIALS__AWS_1__AWS_ACCESS_KEY_ID\"] = os.environ[\n            \"AWS_ACCESS_KEY_ID\"\n        ]\n        os.environ[\"FIDES__CREDENTIALS__AWS_1__AWS_SECRET_ACCESS_KEY\"] = os.environ[\n            \"AWS_SECRET_ACCESS_KEY\"\n        ]\n        tmp_file = tmpdir.join(\"system.yml\")\n        result = test_cli_runner.invoke(\n            cli,\n            [\n                \"-f\",\n                test_config_path,\n                \"generate\",\n                \"system\",\n                \"aws\",\n                f\"{tmp_file}\",\n                \"--credentials-id\",\n                \"aws_1\",\n            ],\n        )\n        print(result.output)\n        assert result.exit_code == 0\n\n    @pytest.mark.external\n    def test_generate_system_okta_input_credential_options(\n        self,\n        test_config_path: str,\n        test_cli_runner: CliRunner,\n        tmpdir: LocalPath,\n    ) -> None:\n        tmp_file = tmpdir.join(\"system.yml\")\n        token = os.environ[\"OKTA_CLIENT_TOKEN\"]\n        result = test_cli_runner.invoke(\n            cli,\n            [\n                \"-f\",\n                test_config_path,\n                \"generate\",\n                \"system\",\n                \"okta\",\n                f\"{tmp_file}\",\n                \"--org-url\",\n                OKTA_URL,\n                \"--token\",\n                token,\n            ],\n        )\n        print(result.output)\n        assert result.exit_code == 0\n\n    @pytest.mark.external\n    def test_generate_system_okta_environment_credentials(\n        test_config_path: str,\n        test_cli_runner: CliRunner,\n        tmpdir: LocalPath,\n    ) -> None:\n        tmp_file = tmpdir.join(\"system.yml\")\n        os.environ[\"OKTA_CLIENT_ORGURL\"] = OKTA_URL\n        result = test_cli_runner.invoke(\n            cli,\n            [\"-f\", test_config_path, \"generate\", \"system\", \"okta\", f\"{tmp_file}\"],\n        )\n        print(result.output)\n        assert result.exit_code == 0\n\n    @pytest.mark.external\n    def test_generate_system_okta_input_credentials_id(\n        self,\n        test_config_path: str,\n        test_cli_runner: CliRunner,\n        tmpdir: LocalPath,\n    ) -> None:\n        tmp_file = tmpdir.join(\"system.yml\")\n        os.environ[\"FIDES__CREDENTIALS__OKTA_1__TOKEN\"] = os.environ[\n            \"OKTA_CLIENT_TOKEN\"\n        ]\n        result = test_cli_runner.invoke(\n            cli,\n            [\n                \"-f\",\n                test_config_path,\n                \"generate\",\n                \"system\",\n                \"okta\",\n                f\"{tmp_file}\",\n                \"--credentials-id\",\n                \"okta_1\",\n            ],\n        )\n        print(result.output)\n        assert result.exit_code == 0\n\n    @pytest.mark.external\n    def test_generate_dataset_bigquery_credentials_id(\n        self,\n        test_config_path: str,\n        test_cli_runner: CliRunner,\n        tmpdir: LocalPath,\n    ) -> None:\n        tmp_output_file = tmpdir.join(\"dataset.yml\")\n        config_data = os.getenv(\"BIGQUERY_CONFIG\", \"e30=\")\n        config_data_decoded = loads(\n            b64decode(config_data.encode(\"utf-8\")).decode(\"utf-8\")\n        )\n        os.environ[\"FIDES__CREDENTIALS__BIGQUERY_1__PROJECT_ID\"] = config_data_decoded[\n            \"project_id\"\n        ]\n        os.environ[\"FIDES__CREDENTIALS__BIGQUERY_1__PRIVATE_KEY_ID\"] = (\n            config_data_decoded[\"private_key_id\"]\n        )\n        os.environ[\"FIDES__CREDENTIALS__BIGQUERY_1__PRIVATE_KEY\"] = config_data_decoded[\n            \"private_key\"\n        ]\n        os.environ[\"FIDES__CREDENTIALS__BIGQUERY_1__CLIENT_EMAIL\"] = (\n            config_data_decoded[\"client_email\"]\n        )\n        os.environ[\"FIDES__CREDENTIALS__BIGQUERY_1__CLIENT_ID\"] = config_data_decoded[\n            \"client_id\"\n        ]\n        os.environ[\"FIDES__CREDENTIALS__BIGQUERY_1__CLIENT_X509_CERT_URL\"] = (\n            config_data_decoded[\"client_x509_cert_url\"]\n        )\n        dataset_name = \"fidesopstest\"\n        result = test_cli_runner.invoke(\n            cli,\n            [\n                \"-f\",\n                test_config_path,\n                \"generate\",\n                \"dataset\",\n                \"gcp\",\n                \"bigquery\",\n                dataset_name,\n                f\"{tmp_output_file}\",\n                \"--credentials-id\",\n                \"bigquery_1\",\n            ],\n        )\n        print(result.output)\n        assert result.exit_code == 0\n\n    @pytest.mark.external\n    def test_generate_dataset_bigquery_keyfile_path(\n        self,\n        test_config_path: str,\n        test_cli_runner: CliRunner,\n        tmpdir: LocalPath,\n    ) -> None:\n        tmp_output_file = tmpdir.join(\"dataset.yml\")\n        tmp_keyfile = tmpdir.join(\"bigquery.json\")\n        config_data = os.getenv(\"BIGQUERY_CONFIG\", \"e30=\")\n        config_data_decoded = loads(\n            b64decode(config_data.encode(\"utf-8\")).decode(\"utf-8\")\n        )\n        with open(tmp_keyfile, \"w\", encoding=\"utf-8\") as keyfile:\n            dump(config_data_decoded, keyfile)\n        dataset_name = \"fidesopstest\"\n        result = test_cli_runner.invoke(\n            cli,\n            [\n                \"-f\",\n                test_config_path,\n                \"generate\",\n                \"dataset\",\n                \"gcp\",\n                \"bigquery\",\n                dataset_name,\n                f\"{tmp_output_file}\",\n                \"--keyfile-path\",\n                f\"{tmp_keyfile}\",\n            ],\n        )\n        print(result.output)\n        assert result.exit_code == 0\n\n\n@pytest.fixture(scope=\"class\")\ndef credentials_path(tmp_path_factory) -> str:\n    credentials_dir = tmp_path_factory.mktemp(\"credentials\")\n    credentials_path = credentials_dir / \".fides_credentials\"\n    return str(credentials_path)\n\n\n@pytest.mark.integration\nclass TestUser:\n    \"\"\"\n    Test the \"user\" command group.\n\n    Most tests rely on previous tests.\n    \"\"\"\n\n    def test_user_login_provide_credentials(\n        self, test_config_path: str, test_cli_runner: CliRunner, credentials_path: str\n    ) -> None:\n        \"\"\"Test logging in as a user with a provided username and password.\"\"\"\n        print(credentials_path)\n        result = test_cli_runner.invoke(\n            cli,\n            [\n                \"-f\",\n                test_config_path,\n                \"user\",\n                \"login\",\n                \"-u\",\n                \"root_user\",\n                \"-p\",\n                \"Testpassword1!\",\n            ],\n            env={\"FIDES_CREDENTIALS_PATH\": credentials_path},\n        )\n        print(result.output)\n        assert result.exit_code == 0\n\n    def test_user_login_env_var_failed(\n        self, test_config_path: str, test_cli_runner: CliRunner, credentials_path: str\n    ) -> None:\n        \"\"\"\n        Test logging in as a user with a provided username and password\n        provided via env vars, but the username is invalid.\n        \"\"\"\n        print(credentials_path)\n        result = test_cli_runner.invoke(\n            cli,\n            [\n                \"-f\",\n                test_config_path,\n                \"user\",\n                \"login\",\n            ],\n            env={\n                \"FIDES_CREDENTIALS_PATH\": credentials_path,\n                \"FIDES__USER__USERNAME\": \"fakeuser\",\n                \"FIDES__USER__PASSWORD\": \"Testpassword1!\",\n            },\n        )\n        print(result.output)\n        assert result.exit_code == 1\n\n    def test_user_login_env_var_password(\n        self, test_config_path: str, test_cli_runner: CliRunner, credentials_path: str\n    ) -> None:\n        \"\"\"\n        Test logging in as a user with a provided username but password\n        provided via env vars.\n        \"\"\"\n        print(credentials_path)\n        result = test_cli_runner.invoke(\n            cli,\n            [\"-f\", test_config_path, \"user\", \"login\", \"-u\", \"root_user\"],\n            env={\n                \"FIDES_CREDENTIALS_PATH\": credentials_path,\n                \"FIDES__USER__PASSWORD\": \"Testpassword1!\",\n            },\n        )\n        print(result.output)\n        assert result.exit_code == 0\n\n    def test_user_login_env_var_credentials(\n        self, test_config_path: str, test_cli_runner: CliRunner, credentials_path: str\n    ) -> None:\n        \"\"\"\n        Test logging in as a user with a provided username and password\n        provided via env vars.\n        \"\"\"\n        print(credentials_path)\n        result = test_cli_runner.invoke(\n            cli,\n            [\n                \"-f\",\n                test_config_path,\n                \"user\",\n                \"login\",\n            ],\n            env={\n                \"FIDES_CREDENTIALS_PATH\": credentials_path,\n                \"FIDES__USER__USERNAME\": \"root_user\",\n                \"FIDES__USER__PASSWORD\": \"Testpassword1!\",\n            },\n        )\n        print(result.output)\n        assert result.exit_code == 0\n\n    def test_user_create(\n        self, test_config_path: str, test_cli_runner: CliRunner, credentials_path: str\n    ) -> None:\n        \"\"\"Test creating a user with the current credentials.\"\"\"\n        print(credentials_path)\n        result = test_cli_runner.invoke(\n            cli,\n            [\n                \"-f\",\n                test_config_path,\n                \"user\",\n                \"create\",\n                \"newuser\",\n                \"Newpassword1!\",\n                \"test@ethyca.com\",\n            ],\n            env={\"FIDES_CREDENTIALS_PATH\": credentials_path},\n        )\n        print(result.output)\n        assert result.exit_code == 0\n\n        test_cli_runner.invoke(\n            cli,\n            [\n                \"-f\",\n                test_config_path,\n                \"user\",\n                \"login\",\n                \"-u\",\n                \"newuser\",\n                \"-p\",\n                \"Newpassword1!\",\n            ],\n            env={\"FIDES_CREDENTIALS_PATH\": credentials_path},\n        )\n\n        credentials = read_credentials_file(credentials_path)\n        total_scopes, roles = get_user_permissions(\n            credentials.user_id, get_auth_header(), CONFIG.cli.server_url\n        )\n        assert set(total_scopes) == set(SCOPE_REGISTRY)\n        assert roles == [OWNER]\n\n    def test_user_permissions_valid(\n        self, test_config_path: str, test_cli_runner: CliRunner, credentials_path: str\n    ) -> None:\n        \"\"\"Test getting user permissions for the current user.\"\"\"\n        print(credentials_path)\n        result = test_cli_runner.invoke(\n            cli,\n            [\"-f\", test_config_path, \"user\", \"permissions\"],\n            env={\"FIDES_CREDENTIALS_PATH\": credentials_path},\n        )\n        print(result.output)\n        assert result.exit_code == 0\n\n    def test_get_self_user_permissions(\n        self, test_config_path, test_cli_runner, credentials_path\n    ) -> None:\n        \"\"\"Test getting user permissions\"\"\"\n        test_cli_runner.invoke(\n            cli,\n            [\n                \"-f\",\n                test_config_path,\n                \"user\",\n                \"login\",\n                \"-u\",\n                \"root_user\",\n                \"-p\",\n                \"Testpassword1!\",\n            ],\n            env={\"FIDES_CREDENTIALS_PATH\": credentials_path},\n        )\n        total_scopes, roles = get_user_permissions(\n            CONFIG.security.oauth_root_client_id,\n            get_auth_header(),\n            CONFIG.cli.server_url,\n        )\n        assert set(total_scopes) == set(SCOPE_REGISTRY)\n        assert roles == [OWNER]\n\n    @pytest.mark.unit\n    def test_get_self_user_systems(\n        self, test_config_path, test_cli_runner, credentials_path\n    ) -> None:\n        \"\"\"Test getting user permissions\"\"\"\n        test_cli_runner.invoke(\n            cli,\n            [\n                \"-f\",\n                test_config_path,\n                \"user\",\n                \"login\",\n                \"-u\",\n                \"root_user\",\n                \"-p\",\n                \"Testpassword1!\",\n            ],\n            env={\"FIDES_CREDENTIALS_PATH\": credentials_path},\n        )\n        systems = get_systems_managed_by_user(\n            CONFIG.security.oauth_root_client_id,\n            get_auth_header(),\n            CONFIG.cli.server_url,\n        )\n        assert systems == []\n\n    def test_get_other_user_perms_and_systems(\n        self, test_config_path, test_cli_runner, credentials_path, system_manager\n    ) -> None:\n        \"\"\"Test getting another user's permissions and systems\"\"\"\n        test_cli_runner.invoke(\n            cli,\n            [\n                \"-f\",\n                test_config_path,\n                \"user\",\n                \"login\",\n                \"-u\",\n                \"root_user\",\n                \"-p\",\n                \"Testpassword1!\",\n            ],\n            env={\"FIDES_CREDENTIALS_PATH\": credentials_path},\n        )\n        total_scopes, roles = get_user_permissions(\n            system_manager.id,\n            get_auth_header(),\n            CONFIG.cli.server_url,\n        )\n        assert roles == [VIEWER]\n\n        systems = get_systems_managed_by_user(\n            system_manager.id,\n            get_auth_header(),\n            CONFIG.cli.server_url,\n        )\n        assert systems == [system_manager.systems[0].fides_key]\n\n    def test_user_permissions_not_found(\n        self, test_config_path: str, test_cli_runner: CliRunner, credentials_path: str\n    ) -> None:\n        \"\"\"Test getting user permissions but the credentials file doesn't exit.\"\"\"\n        print(credentials_path)\n        result = test_cli_runner.invoke(\n            cli,\n            [\"-f\", test_config_path, \"user\", \"permissions\"],\n            env={\"FIDES_CREDENTIALS_PATH\": \"/root/notarealfile.credentials\"},\n        )\n        print(result.output)\n        assert result.exit_code == 1\n"}
{"type": "test_file", "path": "tests/ctl/api/test_worker.py", "content": "from unittest.mock import patch\n\nimport pytest\n\nfrom fides.api.worker import start_worker\n\n\n@patch(\"fides.api.worker.celery_app.worker_main\")\nclass TestStartWorker:\n    \"\"\"\n    Unit tests for the start_worker function. Does not test the actual worker functionality,\n    since we mock the worker_main method.\n    \"\"\"\n\n    def test_cannot_provide_both_queues_and_exclude_queues(self, worker_main_mock):\n        with pytest.raises(AssertionError):\n            start_worker(queues=\"fidesops.messaging\", exclude_queues=\"fides.dsr\")\n\n        worker_main_mock.assert_not_called()\n\n    def test_start_worker_unknown_queue(self, worker_main_mock):\n        with pytest.raises(ValueError):\n            start_worker(queues=\"fidesops.messaging,unknown_queue\")\n\n        worker_main_mock.assert_not_called()\n\n    @pytest.mark.parametrize(\n        \"queues, exclude_queues, expected_queues\",\n        [\n            (\n                None,\n                None,\n                \"fides,fidesops.messaging,fides.privacy_preferences,fides.dsr\",\n            ),\n            (\"fides.dsr\", None, \"fides.dsr\"),\n            (None, \"fides.dsr,fides.privacy_preferences\", \"fides,fidesops.messaging\"),\n            (\"fides,fides.dsr\", None, \"fides,fides.dsr\"),\n            (None, \"fides,fides.dsr\", \"fidesops.messaging,fides.privacy_preferences\"),\n        ],\n    )\n    def test_start_worker_with_arguments(\n        self,\n        worker_main_mock,\n        queues,\n        exclude_queues,\n        expected_queues,\n    ):\n        start_worker(queues=queues, exclude_queues=exclude_queues)\n\n        worker_main_mock.assert_called_once_with(\n            argv=[\n                \"--quiet\",\n                \"worker\",\n                \"--loglevel=info\",\n                \"--concurrency=2\",\n                f\"--queues={expected_queues}\",\n            ]\n        )\n"}
{"type": "test_file", "path": "tests/ctl/cli/test_deploy.py", "content": "from unittest import mock\n\nimport pytest\n\nfrom fides.core import deploy\n\n\n@pytest.mark.unit\nclass TestDeploy:\n    def test_convert_semver_to_list(self):\n        assert deploy.convert_semver_to_list(\"0.1.0\") == [0, 1, 0]\n\n    @pytest.mark.parametrize(\n        \"semver_1, semver_2, expected\",\n        [\n            ([2, 10, 3], [2, 10, 2], True),\n            ([3, 1, 3], [2, 10, 2], True),\n            ([2, 10, 2], [2, 10, 2], True),\n            ([1, 1, 3], [2, 10, 2], False),\n            ([1, 10, 1], [1, 1, 0], True),\n            ([1, 1, 0], [1, 10, 1], False),\n            ([1, 1, 1], [1, 1, 0], True),\n            ([1, 1, 0], [1, 1, 1], False),\n        ],\n    )\n    def test_compare_semvers(self, semver_1, semver_2, expected):\n        assert deploy.compare_semvers(semver_1, semver_2) is expected\n\n    @mock.patch(\"fides.core.deploy.sys\")\n    def test_check_virtualenv(self, mock_sys):\n        # Emulate non-virtual environment\n        mock_sys.prefix = (\n            \"/usr/local/opt/python@3.9/Frameworks/Python.framework/Versions/3.9\"\n        )\n        mock_sys.base_prefix = (\n            \"/usr/local/opt/python@3.9/Frameworks/Python.framework/Versions/3.9\"\n        )\n        assert deploy.check_virtualenv() == False\n\n        # Emulate virtual environment\n        mock_sys.prefix = \"/Users/fidesuser/fides/venv\"\n        mock_sys.base_prefix = (\n            \"/usr/local/opt/python@3.9/Frameworks/Python.framework/Versions/3.9\"\n        )\n        assert deploy.check_virtualenv() == True\n\n\n@pytest.mark.unit\nclass TestCheckDockerVersion:\n    def test_docker_version_exception(self):\n        with pytest.raises(SystemExit):\n            result = deploy.check_docker_version(\"30.1.24\")\n            assert result\n"}
{"type": "test_file", "path": "tests/ctl/cli/test_utils.py", "content": "# pylint: disable=missing-docstring, redefined-outer-name\nimport os\nfrom unittest.mock import patch\n\nimport click\nimport pytest\n\nimport fides.cli.utils as utils\nfrom fides.config import FidesConfig\nfrom tests.ctl.conftest import orig_requests_get\n\n\n@pytest.mark.unit\ndef test_check_server_bad_ping(test_client, monkeypatch) -> None:\n    \"\"\"Check for an exception if the server isn't up.\"\"\"\n    import requests\n\n    monkeypatch.setattr(requests, \"get\", orig_requests_get)\n    with pytest.raises(SystemExit):\n        utils.check_server(\"foo\", \"http://fake_address:8080\")\n    monkeypatch.setattr(requests, \"get\", test_client.get)\n\n\n@pytest.mark.unit\n@pytest.mark.parametrize(\n    \"server_version, cli_version, expected_result\",\n    [\n        (\"1.6.0+7.ge953df5\", \"1.6.0+7.ge953df5\", True),\n        (\"1.6.0+7.ge953df5\", \"1.6.0+9.ge953df5\", False),\n        (\n            \"1.6.0+7.ge953df5\",\n            \"1.6.0+7.ge953df5.dirty\",\n            True,\n        ),\n        (\n            \"1.6.0+7.ge953df5.dirty\",\n            \"1.6.0+7.ge953df5\",\n            True,\n        ),\n    ],\n)\ndef test_check_server_version_comparisons(\n    server_version: str,\n    cli_version: str,\n    expected_result: str,\n) -> None:\n    \"\"\"Check that version comparison works.\"\"\"\n    actual_result = utils.compare_application_versions(server_version, cli_version)\n    assert expected_result == actual_result\n\n\n@pytest.mark.unit\nclass TestHandleDatabaseCredentialsOptions:\n    def test_neither_option_supplied_raises(\n        self,\n        test_config: FidesConfig,\n    ) -> None:\n        \"Check for an exception if neither option is supplied.\"\n        with pytest.raises(click.UsageError):\n            input_connection_string = \"\"\n            input_credentials_id = \"\"\n            utils.handle_database_credentials_options(\n                fides_config=test_config,\n                connection_string=input_connection_string,\n                credentials_id=input_credentials_id,\n            )\n\n    def test_both_options_supplied_raises(\n        self,\n        test_config: FidesConfig,\n    ) -> None:\n        \"Check for an exception if both options are supplied.\"\n        with pytest.raises(click.UsageError):\n            input_connection_string = \"my_connection_string\"\n            input_credentials_id = \"postgres_1\"\n            utils.handle_database_credentials_options(\n                fides_config=test_config,\n                connection_string=input_connection_string,\n                credentials_id=input_credentials_id,\n            )\n\n    def test_config_does_not_exist_raises(\n        self,\n        test_config: FidesConfig,\n    ) -> None:\n        \"Check for an exception if credentials dont exist\"\n        with pytest.raises(click.UsageError):\n            input_connection_string = \"\"\n            input_credentials_id = \"UNKNOWN\"\n            utils.handle_database_credentials_options(\n                fides_config=test_config,\n                connection_string=input_connection_string,\n                credentials_id=input_credentials_id,\n            )\n\n    def test_returns_input_connection_string(\n        self,\n        test_config: FidesConfig,\n    ) -> None:\n        \"Checks if expected connection string is returned from input\"\n        input_connection_string = \"my_connection_string\"\n        input_credentials_id = \"\"\n        connection_string = utils.handle_database_credentials_options(\n            fides_config=test_config,\n            connection_string=input_connection_string,\n            credentials_id=input_credentials_id,\n        )\n        assert connection_string == input_connection_string\n\n    def test_returns_config_connection_string(\n        self,\n        test_config: FidesConfig,\n    ) -> None:\n        \"Checks if expected connection string is returned from config\"\n        input_connection_string = \"\"\n        input_credentials_id = \"postgres_1\"\n        connection_string = utils.handle_database_credentials_options(\n            fides_config=test_config,\n            connection_string=input_connection_string,\n            credentials_id=input_credentials_id,\n        )\n        assert (\n            connection_string\n            == \"postgresql+psycopg2://postgres:fides@fides-db:5432/fides_test\"\n        )\n\n\ndef test_handle_okta_credentials_options_both_raises(\n    test_config: FidesConfig,\n) -> None:\n    \"Check for an exception if both credentials options are supplied.\"\n    with pytest.raises(click.UsageError):\n        input_org_url = \"hello.com\"\n        input_token = \"abcd12345\"\n        input_credentials_id = \"okta_1\"\n        utils.handle_okta_credentials_options(\n            fides_config=test_config,\n            token=input_token,\n            org_url=input_org_url,\n            credentials_id=input_credentials_id,\n        )\n\n\n@pytest.mark.unit\nclass TestHandleOktaCredentialsOptions:\n    def test_config_dne_raises(\n        self,\n        test_config: FidesConfig,\n    ) -> None:\n        \"Check for an exception if credentials dont exist\"\n        with pytest.raises(click.UsageError):\n            input_org_url = \"\"\n            input_token = \"\"\n            input_credentials_id = \"UNKNOWN\"\n            utils.handle_okta_credentials_options(\n                fides_config=test_config,\n                token=input_token,\n                org_url=input_org_url,\n                credentials_id=input_credentials_id,\n            )\n\n    def test_returns_config_dict(\n        self,\n        test_config: FidesConfig,\n    ) -> None:\n        \"Check for an exception if credentials dont exist\"\n        input_org_url = \"\"\n        input_token = \"\"\n        input_credentials_id = \"okta_1\"\n        okta_config = utils.handle_okta_credentials_options(\n            fides_config=test_config,\n            token=input_token,\n            org_url=input_org_url,\n            credentials_id=input_credentials_id,\n        )\n        assert okta_config.model_dump() == {\n            \"orgUrl\": \"https://dev-78908748.okta.com\",\n            \"token\": \"redacted_override_in_tests\",\n        }\n\n    def test_returns_input_dict(\n        self,\n        test_config: FidesConfig,\n    ) -> None:\n        \"Check for an exception if credentials dont exist\"\n        input_org_url = \"hello.com\"\n        input_token = \"abcd12345\"\n        input_credentials_id = \"\"\n        okta_config = utils.handle_okta_credentials_options(\n            fides_config=test_config,\n            token=input_token,\n            org_url=input_org_url,\n            credentials_id=input_credentials_id,\n        )\n        assert okta_config.model_dump() == {\n            \"orgUrl\": input_org_url,\n            \"token\": input_token,\n        }\n\n\n@pytest.mark.unit\nclass TestHandleAWSCredentialsOptions:\n    def test_both_raises(\n        self,\n        test_config: FidesConfig,\n    ) -> None:\n        \"Check for an exception if both credentials options are supplied.\"\n        with pytest.raises(click.UsageError):\n            input_access_key = \"access_key\"\n            input_access_key_id = \"access_key_id\"\n            input_region = \"us-east-1\"\n            input_credentials_id = \"aws_1\"\n            utils.handle_aws_credentials_options(\n                fides_config=test_config,\n                access_key_id=input_access_key_id,\n                secret_access_key=input_access_key,\n                session_token=None,\n                region=input_region,\n                credentials_id=input_credentials_id,\n            )\n\n    def test_config_dne_raises(\n        self,\n        test_config: FidesConfig,\n    ) -> None:\n        \"Check for an exception if credentials dont exist\"\n        with pytest.raises(click.UsageError):\n            input_access_key = \"\"\n            input_access_key_id = \"\"\n            input_region = \"\"\n            input_credentials_id = \"UNKNOWN\"\n            utils.handle_aws_credentials_options(\n                fides_config=test_config,\n                access_key_id=input_access_key_id,\n                secret_access_key=input_access_key,\n                session_token=None,\n                region=input_region,\n                credentials_id=input_credentials_id,\n            )\n\n    def test_returns_config_dict(\n        self,\n        test_config: FidesConfig,\n    ) -> None:\n        \"Verify credentials specified in config dict\"\n        input_access_key = \"\"\n        input_access_key_id = \"\"\n        input_region = \"\"\n        input_credentials_id = \"aws_1\"\n        aws_config = utils.handle_aws_credentials_options(\n            fides_config=test_config,\n            access_key_id=input_access_key_id,\n            secret_access_key=input_access_key,\n            session_token=None,\n            region=input_region,\n            credentials_id=input_credentials_id,\n        )\n\n        assert aws_config.model_dump(mode=\"json\") == {\n            \"aws_access_key_id\": \"redacted_id_override_in_tests\",\n            \"aws_secret_access_key\": \"redacted_override_in_tests\",\n            \"region_name\": \"us-east-1\",\n            \"aws_session_token\": None,\n        }\n\n    def test_returns_input_dict(\n        self,\n        test_config: FidesConfig,\n    ) -> None:\n        \"Verify credentials specified directly as an input args\"\n        input_access_key = \"access_key\"\n        input_access_key_id = \"access_key_id\"\n        input_region = \"us-east-1\"\n        input_credentials_id = \"\"\n        aws_config = utils.handle_aws_credentials_options(\n            fides_config=test_config,\n            access_key_id=input_access_key_id,\n            secret_access_key=input_access_key,\n            session_token=None,\n            region=input_region,\n            credentials_id=input_credentials_id,\n        )\n        assert aws_config.model_dump(mode=\"json\") == {\n            \"aws_access_key_id\": input_access_key_id,\n            \"aws_secret_access_key\": input_access_key,\n            \"region_name\": input_region,\n            \"aws_session_token\": None,\n        }\n\n    def test_session_token_temporary_credentials(\n        self,\n        test_config: FidesConfig,\n    ) -> None:\n        \"Verify AWS temporary credential support, i.e. passing a session token\"\n        input_access_key = \"access_key\"\n        input_access_key_id = \"access_key_id\"\n        input_region = \"us-east-1\"\n        session_token = \"session_token\"\n        input_credentials_id = \"\"\n        aws_config = utils.handle_aws_credentials_options(\n            fides_config=test_config,\n            access_key_id=input_access_key_id,\n            secret_access_key=input_access_key,\n            session_token=session_token,\n            region=input_region,\n            credentials_id=input_credentials_id,\n        )\n        assert aws_config.model_dump(mode=\"json\") == {\n            \"aws_access_key_id\": input_access_key_id,\n            \"aws_secret_access_key\": input_access_key,\n            \"region_name\": input_region,\n            \"aws_session_token\": session_token,\n        }\n\n\n@pytest.mark.unit\nclass TestHandleBigQueryCredentialsOptions:\n    def test_multiple_config_options_raises(\n        self,\n        test_config: FidesConfig,\n    ) -> None:\n        with pytest.raises(click.UsageError):\n            utils.handle_bigquery_config_options(\n                fides_config=test_config,\n                dataset=\"dataset\",\n                keyfile_path=\"path/to/keyfile.json\",\n                credentials_id=\"bigquery_1\",\n            )\n\n    def test_missing_credential_id(\n        self,\n        test_config: FidesConfig,\n    ) -> None:\n        with pytest.raises(click.UsageError):\n            utils.handle_bigquery_config_options(\n                fides_config=test_config,\n                dataset=\"dataset\",\n                keyfile_path=\"\",\n                credentials_id=\"UNKNOWN\",\n            )\n"}
{"type": "test_file", "path": "tests/ctl/cli/__init__.py", "content": ""}
{"type": "test_file", "path": "tests/ctl/conftest.py", "content": "\"\"\"This file is only for the database fixture. For all other fixtures add them to the\ntests/conftest.py file.\n\"\"\"\n\nimport pytest\nimport requests\nfrom fideslang import DEFAULT_TAXONOMY\nfrom pytest import MonkeyPatch\nfrom sqlalchemy.exc import IntegrityError\n\nfrom fides.api.db.base_class import Base\nfrom fides.api.db.ctl_session import sync_engine, sync_session\nfrom fides.api.models.sql_models import DataUse\nfrom fides.core import api\nfrom tests.conftest import create_citext_extension\n\norig_requests_get = requests.get\norig_requests_post = requests.post\norig_requests_put = requests.put\norig_requests_patch = requests.patch\norig_requests_delete = requests.delete\n\n\n@pytest.fixture(scope=\"session\")\ndef monkeysession():\n    \"\"\"\n    Monkeypatch at the session level instead of the function level.\n    Automatically undoes the monkeypatching when the session finishes.\n    \"\"\"\n    mpatch = MonkeyPatch()\n    yield mpatch\n    mpatch.undo()\n\n\n@pytest.fixture(autouse=True, scope=\"session\")\ndef monkeypatch_requests(test_client, monkeysession) -> None:\n    \"\"\"\n    Some places within the application, for example `fides.core.api`, use the `requests`\n    library to interact with the webserver. This fixture patches those `requests` calls\n    so that all of those tests instead interact with the test instance.\n    \"\"\"\n    monkeysession.setattr(requests, \"get\", test_client.get)\n    monkeysession.setattr(requests, \"post\", test_client.post)\n    monkeysession.setattr(requests, \"put\", test_client.put)\n    monkeysession.setattr(requests, \"patch\", test_client.patch)\n    monkeysession.setattr(requests, \"delete\", test_client.delete)\n\n\n@pytest.fixture(scope=\"session\", autouse=True)\n@pytest.mark.usefixtures(\"monkeypatch_requests\")\ndef setup_ctl_db(test_config, test_client, config):\n    \"Sets up the database for testing.\"\n    assert config.test_mode\n    assert (\n        requests.post == test_client.post\n    )  # Sanity check to make sure monkeypatch_requests fixture has run\n    yield api.db_action(\n        server_url=test_config.cli.server_url,\n        headers=config.user.auth_header,\n        action=\"reset\",\n    )\n\n\n@pytest.fixture(scope=\"session\")\ndef db():\n    create_citext_extension(sync_engine)\n\n    session = sync_session()\n\n    yield session\n    session.close()\n\n\n@pytest.fixture(scope=\"function\", autouse=True)\ndef load_default_data_uses(db):\n    for data_use in DEFAULT_TAXONOMY.data_use:\n        # Default data uses are cleared and not automatically reloaded by `clear_db_tables` fixture.\n        # Here we make sure our default data uses are always available for our tests,\n        # if they're not present already.\n        if DataUse.get_by(db, field=\"name\", value=data_use.name) is None:\n            DataUse.create(db=db, data=data_use.model_dump(mode=\"json\"))\n"}
{"type": "test_file", "path": "tests/ctl/connectors/__init__.py", "content": ""}
{"type": "test_file", "path": "tests/ctl/connectors/test_aws.py", "content": "# pylint: disable=missing-docstring, redefined-outer-name\nimport os\nfrom typing import Generator\n\nimport pytest\nfrom fideslang.models import System, SystemMetadata\nfrom py._path.local import LocalPath\n\nimport fides.connectors.aws as aws_connector\nfrom fides.config import FidesConfig\nfrom fides.connectors.models import AWSConfig\n\n\n@pytest.fixture()\ndef redshift_describe_clusters() -> Generator:\n    describe_clusters = {\n        \"Clusters\": [\n            {\n                \"ClusterIdentifier\": \"redshift-cluster-1\",\n                \"Endpoint\": {\n                    \"Address\": \"redshift-cluster-1.c2angfh5kpo4.us-east-1.redshift.amazonaws.com\",\n                    \"Port\": 5439,\n                },\n                \"ClusterNamespaceArn\": \"arn:aws:redshift:us-east-1:910934740016:namespace:057d5b0e-7eaa-4012-909c-3957c7149176\",\n            },\n            {\n                \"ClusterIdentifier\": \"redshift-cluster-2\",\n                \"Endpoint\": {\n                    \"Address\": \"redshift-cluster-2.c2angfh5kpo4.us-east-1.redshift.amazonaws.com\",\n                    \"Port\": 5439,\n                },\n                \"ClusterNamespaceArn\": \"arn:aws:redshift:us-east-1:910934740016:namespace:057d5b0e-7eaa-4012-909c-3957c7149177\",\n            },\n        ]\n    }\n    yield describe_clusters\n\n\n@pytest.fixture()\ndef redshift_systems() -> Generator:\n    redshift_systems = [\n        System(\n            fides_key=\"redshift-cluster-1\",\n            organization_fides_key=\"default_organization\",\n            name=\"redshift-cluster-1\",\n            description=\"Fides Generated Description for Redshift Cluster: redshift-cluster-1\",\n            meta={},\n            fidesctl_meta=SystemMetadata(\n                endpoint_address=\"redshift-cluster-1.c2angfh5kpo4.us-east-1.redshift.amazonaws.com\",\n                endpoint_port=\"5439\",\n                resource_id=\"arn:aws:redshift:us-east-1:910934740016:namespace:057d5b0e-7eaa-4012-909c-3957c7149176\",\n            ),\n            system_type=\"redshift_cluster\",\n            privacy_declarations=[],\n        ),\n        System(\n            fides_key=\"redshift-cluster-2\",\n            organization_fides_key=\"default_organization\",\n            name=\"redshift-cluster-2\",\n            description=\"Fides Generated Description for Redshift Cluster: redshift-cluster-2\",\n            meta={},\n            fidesctl_meta=SystemMetadata(\n                endpoint_address=\"redshift-cluster-2.c2angfh5kpo4.us-east-1.redshift.amazonaws.com\",\n                endpoint_port=\"5439\",\n                resource_id=\"arn:aws:redshift:us-east-1:910934740016:namespace:057d5b0e-7eaa-4012-909c-3957c7149177\",\n            ),\n            system_type=\"redshift_cluster\",\n            privacy_declarations=[],\n        ),\n    ]\n    yield redshift_systems\n\n\ndef get_test_aws_config() -> AWSConfig:\n    return AWSConfig(\n        region_name=os.environ[\"AWS_DEFAULT_REGION\"],\n        aws_secret_access_key=os.environ[\"AWS_SECRET_ACCESS_KEY\"],\n        aws_access_key_id=os.environ[\"AWS_ACCESS_KEY_ID\"],\n    )\n\n\ndef get_test_aws_config_temporary_credentials() -> AWSConfig:\n    # first get an STS client with our permanent credentials\n    client = aws_connector.get_aws_client(\n        service=\"sts\", aws_config=get_test_aws_config()\n    )\n    # then use the STS client to get temporary credentials\n    temporary_credentials = client.get_session_token()[\"Credentials\"]\n    # return an AWS config with the temporary credentials\n    return AWSConfig(\n        region_name=os.environ[\"AWS_DEFAULT_REGION\"],\n        aws_access_key_id=temporary_credentials[\"AccessKeyId\"],\n        aws_secret_access_key=temporary_credentials[\"SecretAccessKey\"],\n        aws_session_token=temporary_credentials[\"SessionToken\"],\n    )\n\n\n@pytest.fixture()\ndef rds_systems() -> Generator:\n    rds_systems = [\n        System(\n            fides_key=\"database-2\",\n            organization_fides_key=\"default_organization\",\n            name=\"database-2\",\n            description=\"Fides Generated Description for RDS Cluster: database-2\",\n            meta={},\n            fidesctl_meta=SystemMetadata(\n                endpoint_address=\"database-2.cluster-ckrdpkkb4ukm.us-east-1.rds.amazonaws.com\",\n                endpoint_port=\"3306\",\n                resource_id=\"arn:aws:rds:us-east-1:910934740016:cluster:database-2\",\n            ),\n            system_type=\"rds_cluster\",\n            privacy_declarations=[],\n        ),\n        System(\n            fides_key=\"database-1\",\n            organization_fides_key=\"default_organization\",\n            name=\"database-1\",\n            description=\"Fides Generated Description for RDS Instance: database-1\",\n            meta={},\n            fidesctl_meta=SystemMetadata(\n                endpoint_address=\"database-1.ckrdpkkb4ukm.us-east-1.rds.amazonaws.com\",\n                endpoint_port=\"3306\",\n                resource_id=\"arn:aws:rds:us-east-1:910934740016:db:database-1\",\n            ),\n            system_type=\"rds_instance\",\n            privacy_declarations=[],\n        ),\n    ]\n    yield rds_systems\n\n\n@pytest.fixture()\ndef rds_describe_clusters() -> Generator:\n    describe_clusters = {\n        \"DBClusters\": [\n            {\n                \"DBClusterIdentifier\": \"database-2\",\n                \"Endpoint\": \"database-2.cluster-ckrdpkkb4ukm.us-east-1.rds.amazonaws.com\",\n                \"Port\": 3306,\n                \"DBClusterArn\": \"arn:aws:rds:us-east-1:910934740016:cluster:database-2\",\n            },\n        ]\n    }\n    yield describe_clusters\n\n\n@pytest.fixture()\ndef rds_describe_instances() -> Generator:\n    describe_instances = {\n        \"DBInstances\": [\n            {\n                \"DBInstanceIdentifier\": \"database-1\",\n                \"Endpoint\": {\n                    \"Address\": \"database-1.ckrdpkkb4ukm.us-east-1.rds.amazonaws.com\",\n                    \"Port\": 3306,\n                },\n                \"DBInstanceArn\": \"arn:aws:rds:us-east-1:910934740016:db:database-1\",\n            },\n        ]\n    }\n    yield describe_instances\n\n\n# Unit\n@pytest.mark.unit\ndef test_transform_redshift_systems(\n    redshift_describe_clusters: Generator, redshift_systems: Generator\n) -> None:\n    actual_result = aws_connector.create_redshift_systems(\n        describe_clusters=redshift_describe_clusters,\n        organization_key=\"default_organization\",\n    )\n    assert actual_result == redshift_systems\n\n\n@pytest.mark.unit\ndef test_transform_rds_systems(\n    rds_describe_clusters: Generator,\n    rds_describe_instances: Generator,\n    rds_systems: Generator,\n) -> None:\n    actual_result = aws_connector.create_rds_systems(\n        describe_clusters=rds_describe_clusters,\n        describe_instances=rds_describe_instances,\n        organization_key=\"default_organization\",\n    )\n    assert actual_result == rds_systems\n\n\n# Integration\n@pytest.mark.external\ndef test_describe_redshift_clusters(\n    tmpdir: LocalPath, test_config: FidesConfig\n) -> None:\n    client = aws_connector.get_aws_client(\n        service=\"redshift\",\n        aws_config=get_test_aws_config(),\n    )\n    actual_result = aws_connector.describe_redshift_clusters(client=client)\n    assert actual_result\n\n\n@pytest.mark.external\ndef test_describe_redshift_clusters_temporary_credentials(\n    tmpdir: LocalPath, test_config: FidesConfig\n) -> None:\n    \"\"\"\n    Test temporary credential (session token) auth mechanism.\n\n    The test is covering the auth mechanism.\n    We could use any operation to test this, it should work the same as permanent AWS credentials.\n    \"\"\"\n    client = aws_connector.get_aws_client(\n        service=\"redshift\",\n        aws_config=get_test_aws_config_temporary_credentials(),\n    )\n    actual_result = aws_connector.describe_redshift_clusters(client=client)\n    assert actual_result\n\n\n@pytest.mark.external\ndef test_describe_rds_instances(tmpdir: LocalPath, test_config: FidesConfig) -> None:\n    client = aws_connector.get_aws_client(\n        service=\"rds\",\n        aws_config=get_test_aws_config(),\n    )\n    actual_result = aws_connector.describe_rds_instances(client=client)\n    assert actual_result\n\n\n@pytest.mark.external\ndef test_describe_rds_clusters(tmpdir: LocalPath, test_config: FidesConfig) -> None:\n    client = aws_connector.get_aws_client(\n        service=\"rds\",\n        aws_config=get_test_aws_config(),\n    )\n    actual_result = aws_connector.describe_rds_clusters(client=client)\n    assert actual_result\n"}
{"type": "test_file", "path": "tests/ctl/connectors/test_okta.py", "content": "# pylint: disable=missing-docstring, redefined-outer-name\nimport os\nfrom typing import Generator\n\nimport pytest\nfrom fideslang.models import System, SystemMetadata\nfrom okta.models import Application as OktaApplication\nfrom py._path.local import LocalPath\n\nimport fides.connectors.okta as okta_connector\nfrom fides.config import FidesConfig\nfrom fides.connectors.models import OktaConfig\n\n\n@pytest.fixture()\ndef okta_list_applications() -> Generator:\n    okta_applications = [\n        OktaApplication(\n            config={\n                \"id\": \"okta_id_1\",\n                \"name\": \"okta_id_1\",\n                \"label\": \"okta_label_1\",\n                \"status\": \"ACTIVE\",\n            }\n        ),\n        OktaApplication(\n            config={\n                \"id\": \"okta_id_2\",\n                \"name\": \"okta_id_2\",\n                \"label\": \"okta_label_2\",\n                \"status\": \"ACTIVE\",\n            }\n        ),\n    ]\n    yield okta_applications\n\n\n@pytest.fixture()\ndef okta_list_applications_with_inactive() -> Generator:\n    okta_applications = [\n        OktaApplication(\n            config={\n                \"id\": \"okta_id_1\",\n                \"name\": \"okta_id_1\",\n                \"label\": \"okta_label_1\",\n                \"status\": \"ACTIVE\",\n            }\n        ),\n        OktaApplication(\n            config={\n                \"id\": \"okta_id_2\",\n                \"name\": \"okta_id_2\",\n                \"label\": \"okta_label_2\",\n                \"status\": \"INACTIVE\",\n            }\n        ),\n    ]\n    yield okta_applications\n\n\n# Unit\n@pytest.mark.unit\ndef test_create_okta_systems(okta_list_applications: Generator) -> None:\n    expected_result = [\n        System(\n            fides_key=\"okta_id_1\",\n            name=\"okta_id_1\",\n            fidesctl_meta=SystemMetadata(\n                resource_id=\"okta_id_1\",\n            ),\n            organization_fides_key=\"default_organization\",\n            description=\"Fides Generated Description for Okta Application: okta_label_1\",\n            system_type=\"okta_application\",\n            privacy_declarations=[],\n        ),\n        System(\n            fides_key=\"okta_id_2\",\n            name=\"okta_id_2\",\n            fidesctl_meta=SystemMetadata(\n                resource_id=\"okta_id_2\",\n            ),\n            organization_fides_key=\"default_organization\",\n            description=\"Fides Generated Description for Okta Application: okta_label_2\",\n            system_type=\"okta_application\",\n            privacy_declarations=[],\n        ),\n    ]\n    okta_systems = okta_connector.create_okta_systems(\n        okta_applications=okta_list_applications,\n        organization_key=\"default_organization\",\n    )\n    assert okta_systems == expected_result\n\n\n@pytest.mark.unit\ndef test_create_okta_datasets_filters_inactive(\n    okta_list_applications_with_inactive: Generator,\n) -> None:\n    expected_result = [\n        System(\n            fides_key=\"okta_id_1\",\n            name=\"okta_id_1\",\n            fidesctl_meta=SystemMetadata(\n                resource_id=\"okta_id_1\",\n            ),\n            description=\"Fides Generated Description for Okta Application: okta_label_1\",\n            system_type=\"okta_application\",\n            organization_fides_key=\"default_organization\",\n            privacy_declarations=[],\n        ),\n    ]\n    okta_systems = okta_connector.create_okta_systems(\n        okta_applications=okta_list_applications_with_inactive,\n        organization_key=\"default_organization\",\n    )\n    assert okta_systems == expected_result\n\n\n# Integration\n@pytest.mark.external\ndef test_list_okta_applications(tmpdir: LocalPath, test_config: FidesConfig) -> None:\n    client = okta_connector.get_okta_client(\n        OktaConfig(\n            orgUrl=\"https://dev-78908748.okta.com\",\n            token=os.environ[\"OKTA_CLIENT_TOKEN\"],\n        )\n    )\n    actual_result = okta_connector.list_okta_applications(okta_client=client)\n    assert actual_result\n"}
{"type": "test_file", "path": "tests/ctl/core/__init__.py", "content": ""}
{"type": "test_file", "path": "tests/ctl/core/config/test_config.py", "content": "# pylint: disable=missing-docstring, redefined-outer-name\nimport os\nfrom unittest.mock import patch\n\nimport pytest\nfrom pydantic import ValidationError\n\nfrom fides.api.db.database import get_alembic_config\nfrom fides.config import check_required_webserver_config_values, get_config\nfrom fides.config.database_settings import DatabaseSettings\nfrom fides.config.redis_settings import RedisSettings\nfrom fides.config.security_settings import SecuritySettings\n\nREQUIRED_ENV_VARS = {\n    \"FIDES__SECURITY__APP_ENCRYPTION_KEY\": \"OLMkv91j8DHiDAULnK5Lxx3kSCov30b3\",\n    \"FIDES__SECURITY__OAUTH_ROOT_CLIENT_ID\": \"fidesadmin\",\n    \"FIDES__SECURITY__OAUTH_ROOT_CLIENT_SECRET\": \"fidesadminsecret\",\n    \"FIDES__SECURITY__DRP_JWT_SECRET\": \"secret\",\n}\n\n\n@pytest.mark.unit\ndef test_get_config_verbose() -> None:\n    \"\"\"Simple test to check the 'verbose' code path.\"\"\"\n    config = get_config(verbose=True, config_path_override=\"fakefiledoesntexist.toml\")\n    assert config\n\n\n@pytest.mark.unit\nclass TestSecurityEnv:\n    def test_security_invalid(self):\n        \"\"\"\n        Test that an exception is raised when an invalid Enum value is used.\n        \"\"\"\n        with pytest.raises(ValueError):\n            SecuritySettings(env=\"invalid\")\n\n\n# Unit\n@patch.dict(\n    os.environ,\n    REQUIRED_ENV_VARS,\n    clear=True,\n)\n@pytest.mark.unit\ndef test_get_config(test_config_path: str) -> None:\n    \"\"\"Test that the actual config matches what the function returns.\"\"\"\n    config = get_config(test_config_path)\n    assert config.database.user == \"postgres\"\n    assert config.cli.server_url == \"http://fides:8080\"\n    assert (\n        config.credentials[\"postgres_1\"][\"connection_string\"]\n        == \"postgresql+psycopg2://postgres:fides@fides-db:5432/fides_test\"\n    )\n\n\n@patch.dict(\n    os.environ,\n    {\n        \"FIDES__CONFIG_PATH\": \"tests/ctl/test_default_config.toml\",\n    },\n    clear=True,\n)\n@pytest.mark.unit\ndef test_get_config_default() -> None:\n    \"\"\"Check that get_config loads default values when given an empty TOML.\"\"\"\n    config = get_config()\n    assert config.database.api_engine_pool_size == 50\n    assert config.security.env == \"prod\"\n    assert config.security.app_encryption_key == \"\"\n    assert config.logging.level == \"INFO\"\n\n\n@patch.dict(\n    os.environ,\n    REQUIRED_ENV_VARS,\n    clear=True,\n)\n@pytest.mark.unit\ndef test_get_deprecated_api_config_from_file(test_deprecated_config_path: str) -> None:\n    \"\"\"\n    Test that the deprecated API config values get written as database values.\n    \"\"\"\n    config = get_config(test_deprecated_config_path)\n    assert config.database.user == \"postgres_deprecated\"\n    assert config.database.password == \"fidesctl_deprecated\"\n    assert config.database.port == \"5431\"\n    assert config.database.db == \"fidesctl_deprecated\"\n    assert config.database.test_db == \"fidesctl_test_deprecated\"\n\n\n@patch.dict(\n    os.environ,\n    {\n        \"FIDES__API__DATABASE_HOST\": \"test_host\",\n        \"FIDES__API__DATABASE_NAME\": \"test_db_name\",\n        \"FIDES__API__DATABASE_PASSWORD\": \"test_password\",\n        \"FIDES__API__DATABASE_PORT\": \"1234\",\n        \"FIDES__API__DATABASE_TEST_DATABASE_NAME\": \"test_test_db_name\",\n        \"FIDES__API__DATABASE_USER\": \"phil_rules\",\n        **REQUIRED_ENV_VARS,\n    },\n    clear=True,\n)\n@pytest.mark.unit\ndef test_get_deprecated_api_config_from_env(test_config_path: str) -> None:\n    \"\"\"\n    Test that the deprecated API config values get written as database values,\n    when set via ENV variables.\n    \"\"\"\n\n    config = get_config(test_config_path)\n    assert config.database.db == \"test_db_name\"\n    assert config.database.password == \"test_password\"\n    assert config.database.port == \"1234\"\n    assert config.database.server == \"test_host\"\n    assert config.database.test_db == \"test_test_db_name\"\n    assert config.database.user == \"phil_rules\"\n\n\n@patch.dict(\n    os.environ,\n    {\"FIDES__CONFIG_PATH\": \"\", **REQUIRED_ENV_VARS},\n    clear=True,\n)\n@pytest.mark.unit\ndef test_get_config_cache() -> None:\n    \"Test lru cache hits.\"\n\n    config = get_config()\n    cache_info = get_config.cache_info()\n    assert config.user.encryption_key == \"test_encryption_key\"\n    assert cache_info.hits == 0\n    assert cache_info.misses == 1\n\n    config = get_config()\n    cache_info = get_config.cache_info()\n    assert config.user.encryption_key == \"test_encryption_key\"\n    assert cache_info.hits == 1\n    assert cache_info.misses == 1\n\n    config = get_config(\"tests/ctl/test_config.toml\")\n    cache_info = get_config.cache_info()\n    assert config.user.encryption_key == \"test_encryption_key\"\n    assert cache_info.hits == 1\n    assert cache_info.misses == 2\n\n\n@patch.dict(\n    os.environ,\n    {\n        \"FIDES__USER__ENCRYPTION_KEY\": \"test_key_one\",\n        \"FIDES__CLI__SERVER_HOST\": \"test\",\n        \"FIDES__CLI__SERVER_PORT\": \"8080\",\n        \"FIDES__ADMIN_UI__URL\": \"http://localhost:3000/\",\n        \"FIDES__CREDENTIALS__POSTGRES_1__CONNECTION_STRING\": \"postgresql+psycopg2://fides:env_variable.com:5439/fidesctl_test\",\n        **REQUIRED_ENV_VARS,\n    },\n    clear=True,\n)\n@pytest.mark.unit\ndef test_config_from_env_vars() -> None:\n    \"\"\"Test building a config from env vars.\"\"\"\n    config = get_config()\n\n    assert config.user.encryption_key == \"test_key_one\"\n    assert (\n        config.cli.server_url == \"http://test:8080\"\n    )  # No trailing slash because this is constructed from components\n    assert config.admin_ui.url == \"http://localhost:3000\"  # Trailing slash is removed\n    assert (\n        config.credentials[\"postgres_1\"][\"connection_string\"]\n        == \"postgresql+psycopg2://fides:env_variable.com:5439/fidesctl_test\"\n    )\n\n\n@patch.dict(\n    os.environ,\n    {},\n    clear=True,\n)\n@pytest.mark.unit\ndef test_database_url_test_mode_disabled() -> None:\n    os.environ[\"FIDES__TEST_MODE\"] = \"False\"\n    database_settings = DatabaseSettings(\n        user=\"postgres\",\n        password=\"fides\",\n        server=\"fides-db\",\n        port=\"5432\",\n        db=\"database\",\n        test_db=\"test_database\",\n    )\n    assert (\n        database_settings.async_database_uri\n        == \"postgresql+asyncpg://postgres:fides@fides-db:5432/database\"\n    )\n\n\n@pytest.mark.unit\ndef test_password_escaped_by_database_settings_validation() -> None:\n    database_settings = DatabaseSettings(\n        user=\"postgres\",\n        password=\"fidesp@ssword\",\n        server=\"fides-db\",\n        port=\"5432\",\n        db=\"database\",\n        test_db=\"test_database\",\n    )\n    assert (\n        database_settings.async_database_uri\n        == \"postgresql+asyncpg://postgres:fidesp%40ssword@fides-db:5432/test_database\"\n    )\n\n    assert (\n        database_settings.sync_database_uri\n        == \"postgresql+psycopg2://postgres:fidesp%40ssword@fides-db:5432/test_database\"\n    )\n\n    assert (\n        database_settings.sqlalchemy_database_uri\n        == \"postgresql://postgres:fidesp%40ssword@fides-db:5432/database\"\n    )\n\n    assert (\n        database_settings.sqlalchemy_test_database_uri\n        == \"postgresql://postgres:fidesp%40ssword@fides-db:5432/test_database\"\n    )\n\n\ndef test_get_alembic_config_with_special_char_in_database_url():\n    database_url = (\n        \"postgresql+psycopg2://postgres:fidesp%40ssword@fides-db:5432/test_database\"\n    )\n    # this would fail with - ValueError: invalid interpolation syntax\n    # if not handled\n    get_alembic_config(database_url)\n\n\n@patch.dict(\n    os.environ,\n    {\n        \"FIDES__CONFIG_PATH\": \"src/fides/data/sample_project/fides.toml\",\n    },\n    clear=True,\n)\ndef test_config_from_path() -> None:\n    \"\"\"Test reading config using the FIDES__CONFIG_PATH option.\"\"\"\n    config = get_config()\n    print(os.environ)\n    assert config.admin_ui.enabled is True\n    assert config.execution.require_manual_request_approval is True\n\n\n@patch.dict(\n    os.environ,\n    {\n        \"FIDES__DATABASE__SERVER\": \"envserver\",\n        \"FIDES__DATABASE__PARAMS\": '{\"sslmode\": \"verify-full\", \"sslrootcert\": \"/etc/ssl/private/myca.crt\"}',\n        \"FIDES__REDIS__HOST\": \"envhost\",\n        **REQUIRED_ENV_VARS,\n    },\n    clear=True,\n)\ndef test_overriding_config_from_env_vars() -> None:\n    \"\"\"Test overriding config using ENV vars.\"\"\"\n    config = get_config()\n    assert config.database.server == \"envserver\"\n    assert config.redis.host == \"envhost\"\n    assert config.security.app_encryption_key == \"OLMkv91j8DHiDAULnK5Lxx3kSCov30b3\"\n    assert config.database.params == {\n        \"sslmode\": \"verify-full\",\n        \"sslrootcert\": \"/etc/ssl/private/myca.crt\",\n    }\n\n\ndef test_config_app_encryption_key_validation() -> None:\n    \"\"\"Test APP_ENCRYPTION_KEY is validated to be exactly 32 characters.\"\"\"\n    app_encryption_key = \"atestencryptionkeythatisvalidlen\"\n\n    with patch.dict(\n        os.environ,\n        {\n            **REQUIRED_ENV_VARS,\n            \"FIDES__SECURITY__APP_ENCRYPTION_KEY\": app_encryption_key,\n        },\n        clear=True,\n    ):\n        config = get_config()\n        assert config.security.app_encryption_key == app_encryption_key\n\n\n@pytest.mark.parametrize(\n    \"app_encryption_key\",\n    [\"tooshortkey\", \"muchmuchmuchmuchmuchmuchmuchmuchtoolongkey\"],\n)\ndef test_config_app_encryption_key_validation_length_error(app_encryption_key) -> None:\n    \"\"\"Test APP_ENCRYPTION_KEY is validated to be exactly 32 characters.\"\"\"\n    with patch.dict(\n        os.environ,\n        {\n            **REQUIRED_ENV_VARS,\n            \"FIDES__SECURITY__APP_ENCRYPTION_KEY\": app_encryption_key,\n        },\n        clear=True,\n    ):\n        with pytest.raises(ValidationError) as err:\n            get_config()\n        assert \"must be exactly 32 characters\" in str(err.value)\n\n\n@pytest.mark.parametrize(\n    \"log_level,expected_log_level\",\n    [\n        (\"DEBUG\", \"DEBUG\"),\n        (\"debug\", \"DEBUG\"),\n        (\"INFO\", \"INFO\"),\n        (\"WARNING\", \"WARNING\"),\n        (\"ERROR\", \"ERROR\"),\n        (\"CRITICAL\", \"CRITICAL\"),\n    ],\n)\ndef test_config_log_level(log_level, expected_log_level):\n    \"\"\"Test overriding the log level using ENV vars.\"\"\"\n    with patch.dict(\n        os.environ,\n        {\n            \"FIDES__LOGGING__LEVEL\": log_level,\n            **REQUIRED_ENV_VARS,\n        },\n        clear=True,\n    ):\n        config = get_config()\n        assert config.logging.level == expected_log_level\n\n\ndef test_config_log_level_invalid():\n    with patch.dict(\n        os.environ,\n        {\n            \"FIDES__LOGGING__LEVEL\": \"INVALID\",\n            **REQUIRED_ENV_VARS,\n        },\n        clear=True,\n    ):\n        with pytest.raises(ValidationError) as err:\n            get_config()\n        assert \"Invalid LOG_LEVEL\" in str(err.value)\n\n\nclass TestBuildingDatabaseValues:\n    def test_validating_included_sqlalchemy_database_uri(self) -> None:\n        \"\"\"\n        Test that injecting a pre-configured database uri is\n        correctly used as opposed to building a new one.\n        \"\"\"\n        incorrect_value = \"incorrecthost\"\n        correct_value = \"correcthosthost\"\n        database_settings = DatabaseSettings(\n            server=incorrect_value,\n            sqlalchemy_database_uri=f\"postgresql://postgres:fides@{correct_value}:5432/fides\",\n        )\n        assert incorrect_value not in database_settings.sqlalchemy_database_uri\n        assert correct_value in database_settings.sqlalchemy_database_uri\n\n    def test_validating_included_sqlalchemy_test_database_uri(self) -> None:\n        \"\"\"\n        Test that injecting a pre-configured test database uri is\n        correctly used as opposed to building a new one.\n        \"\"\"\n        incorrect_value = \"incorrecthost\"\n        correct_value = \"correcthosthost\"\n        database_settings = DatabaseSettings(\n            server=incorrect_value,\n            sqlalchemy_test_database_uri=f\"postgresql://postgres:fides@{correct_value}:5432/fides\",\n        )\n        assert incorrect_value not in database_settings.sqlalchemy_test_database_uri\n        assert correct_value in database_settings.sqlalchemy_test_database_uri\n\n    def test_validating_included_sync_database_uri(self) -> None:\n        \"\"\"\n        Test that injecting a pre-configured database uri is\n        correctly used as opposed to building a new one.\n        \"\"\"\n        incorrect_value = \"incorrecthost\"\n        correct_value = \"correcthosthost\"\n        database_settings = DatabaseSettings(\n            server=incorrect_value,\n            sync_database_uri=f\"postgresql://postgres:fides@{correct_value}:5432/fides\",\n        )\n        assert incorrect_value not in database_settings.sync_database_uri\n        assert correct_value in database_settings.sync_database_uri\n\n    def test_validating_included_async_database_uri(self) -> None:\n        \"\"\"\n        Test that injecting a pre-configured database uri is\n        correctly used as opposed to building a new one.\n        \"\"\"\n        incorrect_value = \"incorrecthost\"\n        correct_value = \"correcthosthost\"\n        database_settings = DatabaseSettings(\n            server=incorrect_value,\n            async_database_uri=f\"postgresql://postgres:fides@{correct_value}:5432/fides\",\n        )\n        assert incorrect_value not in database_settings.async_database_uri\n        assert correct_value in database_settings.async_database_uri\n\n    def test_builds_with_params(self) -> None:\n        \"\"\"\n        Test that when params are passed, they are correctly\n        encoded as query parameters on the resulting database uris\n        \"\"\"\n        os.environ[\"FIDES__TEST_MODE\"] = \"False\"\n        database_settings = DatabaseSettings(\n            user=\"postgres\",\n            password=\"fides\",\n            server=\"fides-db\",\n            port=\"5432\",\n            db=\"database\",\n            params={\n                \"sslmode\": \"verify-full\",\n                \"sslrootcert\": \"/etc/ssl/private/myca.crt\",\n            },\n        )\n        assert (\n            database_settings.async_database_uri\n            == \"postgresql+asyncpg://postgres:fides@fides-db:5432/database?ssl=verify-full\"\n            # Q: But why! Where did the sslrootcert parameter go?\n            # A: asyncpg cannot accept it, and an ssl context must be\n            #    passed to the create_async_engine function.\n            # Q: But wait! `ssl` is a different name than what we\n            #    passed in the parameters!\n            # A: That was more of a statement, but Jeopardy rules\n            #    aside, asyncpg has a different set of names\n            #    for these extremely standardized parameter names...\n        )\n        assert (\n            database_settings.sync_database_uri\n            == \"postgresql+psycopg2://postgres:fides@fides-db:5432/database?sslmode=verify-full&sslrootcert=/etc/ssl/private/myca.crt\"\n        )\n\n\n@pytest.mark.unit\ndef test_check_required_webserver_config_values_success(test_config_path: str) -> None:\n    config = get_config(test_config_path)\n    assert check_required_webserver_config_values(config=config) is None\n\n\n@patch.dict(\n    os.environ,\n    {\n        \"FIDES__CONFIG_PATH\": \"tests/ctl/test_default_config.toml\",\n    },\n    clear=True,\n)\n@pytest.mark.unit\ndef test_check_required_webserver_config_values_error(capfd) -> None:\n    config = get_config()\n    assert config.security.app_encryption_key is \"\"\n\n    with pytest.raises(SystemExit):\n        check_required_webserver_config_values(config=config)\n\n    out, _ = capfd.readouterr()\n    assert \"app_encryption_key\" in out\n    assert \"oauth_root_client_id\" in out\n    assert \"oauth_root_client_secret\" in out\n\n\n@patch.dict(\n    os.environ,\n    {\n        \"FIDES__CONFIG_PATH\": \"src/fides/data/sample_project/fides.toml\",\n    },\n    clear=True,\n)\ndef test_check_required_webserver_config_values_success_from_path() -> None:\n    config = get_config()\n    assert check_required_webserver_config_values(config=config) is None\n\n\nclass TestBuildingRedisURLs:\n    def test_generic(self) -> None:\n        redis_settings = RedisSettings()\n        assert redis_settings.connection_url == \"redis://:testpassword@redis:6379/\"\n\n    def test_configured(self) -> None:\n        redis_settings = RedisSettings(\n            db_index=1, host=\"myredis\", port=\"6380\", password=\"supersecret\"\n        )\n        assert redis_settings.connection_url == \"redis://:supersecret@myredis:6380/1\"\n\n    def test_tls(self) -> None:\n        redis_settings = RedisSettings(ssl=True, ssl_cert_reqs=\"none\")\n        assert (\n            redis_settings.connection_url\n            == \"rediss://:testpassword@redis:6379/?ssl_cert_reqs=none\"\n        )\n\n    def test_tls_custom_ca(self) -> None:\n        redis_settings = RedisSettings(ssl=True, ssl_ca_certs=\"/path/to/my/cert.crt\")\n        assert (\n            redis_settings.connection_url\n            == \"rediss://:testpassword@redis:6379/?ssl_cert_reqs=required&ssl_ca_certs=/path/to/my/cert.crt\"\n        )\n\n\n@patch.dict(\n    os.environ,\n    {\n        \"FIDES__CONSENT__TCF_ENABLED\": \"true\",\n        \"FIDES__CONSENT__AC_ENABLED\": \"true\",\n    },\n    clear=True,\n)\n@pytest.mark.unit\ndef test_tcf_and_ac_mode() -> None:\n    \"\"\"Test that AC mode cannot be true without TCF mode on\"\"\"\n    config = get_config()\n    assert config.consent.tcf_enabled\n    assert config.consent.ac_enabled\n\n\n@patch.dict(\n    os.environ,\n    {\n        \"FIDES__CONSENT__TCF_ENABLED\": \"false\",\n        \"FIDES__CONSENT__AC_ENABLED\": \"true\",\n    },\n    clear=True,\n)\n@pytest.mark.unit\ndef test_get_config_ac_mode_without_tc_mode() -> None:\n    \"\"\"Test that AC mode cannot be true without TCF mode on\"\"\"\n    with pytest.raises(ValidationError) as exc:\n        get_config()\n\n    assert (\n        exc.value.errors()[0][\"msg\"]\n        == \"Value error, AC cannot be enabled unless TCF mode is also enabled.\"\n    )\n"}
{"type": "test_file", "path": "tests/ctl/core/config/test_config_helpers.py", "content": "from unittest.mock import patch\n\nimport pytest\nimport toml\n\nfrom fides.config import helpers\n\n\n@pytest.mark.unit\nclass TestConfigHelpers:\n    def test_load_file_not_found(self):\n        with pytest.raises(FileNotFoundError):\n            helpers.load_file([\"bad.toml\"])\n\n    def test_get_config_from_file(self, tmp_path):\n        file = tmp_path / \"fides.toml\"\n        with open(file, \"w\") as f:\n            toml.dump(\n                {\n                    \"section 1\": {\"sub 1\": \"value 1\"},\n                    \"section 2\": {\"sub 2\": \"value 2\", \"sub 3\": \"value 3\"},\n                },\n                f,\n            )\n\n        assert helpers.get_config_from_file(file, \"section 2\", \"sub 2\")\n\n    @pytest.mark.parametrize(\n        \"section, option\", [(\"good\", \"something\"), (\"bad\", \"something\")]\n    )\n    def test_get_config_from_file_none(self, section, option, tmp_path):\n        file = tmp_path / \"fides.toml\"\n        with open(file, \"w\") as f:\n            toml.dump({section: {option: \"value\"}}, f)\n\n        assert helpers.get_config_from_file(file, \"bad\", \"missing\") is None\n"}
{"type": "test_file", "path": "tests/ctl/core/config/test_create.py", "content": "import os\n\nimport pytest\nimport toml\nfrom py._path.local import LocalPath\n\nfrom fides.config import FidesConfig\nfrom fides.config.create import (\n    build_field_documentation,\n    create_and_update_config_file,\n    create_config_file,\n    validate_generated_config,\n)\n\n\n@pytest.mark.unit\ndef test_create_and_update_config_file_opt_in(\n    tmpdir: LocalPath, test_config: FidesConfig\n) -> None:\n    \"\"\"Test that config creation works when opting-in to analytics.\"\"\"\n\n    create_and_update_config_file(\n        config=test_config, fides_directory_location=str(tmpdir), opt_in=True\n    )\n    assert True\n\n\n@pytest.mark.unit\nclass TestValidateGeneratedConfig:\n    def test_valid_config(self) -> None:\n        \"\"\"Test that a minimal, but still valid config, can be built.\"\"\"\n        config_docs = toml.dumps({\"database\": {\"server_port\": \"1234\"}})\n        validate_generated_config(config_docs)\n        assert True\n\n    def test_invalid_toml(self) -> None:\n        \"\"\"Test that a config with invalid toml throws an error.\"\"\"\n        with pytest.raises(ValueError):\n            config_docs = \"[database]\\nsom_key = # Empty value, will cause error\"\n            validate_generated_config(config_docs)\n\n    def test_includes_todo(self) -> None:\n        \"\"\"Test that a valid config that contains '# TODO' is invalid.\"\"\"\n        with pytest.raises(ValueError):\n            config_docs = toml.dumps({\"database\": {\"server_port\": \"# TODO\"}})\n            validate_generated_config(config_docs)\n        assert True\n\n\n@pytest.fixture()\ndef remove_fides_dir(tmp_path) -> None:\n    try:\n        os.remove(tmp_path / \".fides/fides.toml\")\n        os.rmdir(tmp_path / \".fides\")\n    except FileNotFoundError:\n        pass\n    except NotADirectoryError:\n        pass\n\n\n@pytest.mark.unit\nclass TestCreateConfigFile:\n    def test_create_config_file(\n        self, config, tmp_path, capfd, remove_fides_dir\n    ) -> None:\n        config_path = create_config_file(config, tmp_path)\n\n        fides_directory = tmp_path / \".fides\"\n        fides_file_path = fides_directory / \"fides.toml\"\n        out, _ = capfd.readouterr()\n\n        assert f\"Created a '{fides_directory}' directory\" in out\n        assert f\"Exported configuration file to: {fides_file_path}\" in out\n        assert config_path == str(fides_file_path)\n\n    def test_create_config_file_dir_exists(\n        self, config, tmp_path, capfd, remove_fides_dir\n    ) -> None:\n        fides_directory = tmp_path / \".fides\"\n        fides_directory.mkdir()\n        fides_file_path = fides_directory / \"fides.toml\"\n\n        config_path = create_config_file(config, tmp_path)\n\n        out, _ = capfd.readouterr()\n\n        assert f\"Directory '{fides_directory}' already exists\" in out\n        assert f\"Exported configuration file to: {fides_file_path}\" in out\n        assert config_path == str(fides_file_path)\n\n    def test_create_config_file_exists(\n        self, config, tmp_path, capfd, remove_fides_dir\n    ) -> None:\n        fides_directory = tmp_path / \".fides\"\n        fides_directory.mkdir()\n        fides_file_path = fides_directory / \"fides.toml\"\n\n        with open(fides_file_path, \"w\", encoding=\"utf-8\") as f:\n            toml.dump(config.model_dump(mode=\"json\"), f)\n\n        config_path = create_config_file(config, tmp_path)\n\n        out, _ = capfd.readouterr()\n\n        assert f\"Directory '{fides_directory}' already exists\" in out\n        assert f\"Configuration file already exists: {fides_file_path}\" in out\n        assert config_path == str(fides_file_path)\n"}
{"type": "test_file", "path": "tests/ctl/core/config/test_credentials_settings.py", "content": "# pylint: disable=missing-docstring, redefined-outer-name\nimport os\nfrom typing import Dict\nfrom unittest.mock import patch\n\nimport pytest\n\nfrom fides.config.credentials_settings import merge_credentials_environment\n\n\n@pytest.mark.unit\nclass TestMergeCredentialsEnvironment:\n    @patch.dict(\n        os.environ,\n        {\n            \"FIDES__CREDENTIALS__POSTGRES_1__CONNECTION_STRING\": \"postgresql+psycopg2://fides:env_variable.com:5439/fidesctl_test\",\n            \"FIDES__CREDENTIALS__AWS_ACCOUNT_1__REGION\": \"us-east-1\",\n            \"FIDES__CREDENTIALS__AWS_ACCOUNT_1__ACCESS_KEY_ID\": \"ACCESS_KEY_ID_1\",\n            \"FIDES__CREDENTIALS__AWS_ACCOUNT_1__ACCESS_KEY\": \"ACCESS_KEY_1\",\n        },\n        clear=True,\n    )\n    def test_merge_credentials(self) -> None:\n        \"Test merging environment variables into a settings dict.\"\n        credentials_dict: Dict = dict()\n        merge_credentials_environment(credentials_dict)\n\n        assert credentials_dict == {\n            \"postgres_1\": {\n                \"connection_string\": \"postgresql+psycopg2://fides:env_variable.com:5439/fidesctl_test\"\n            },\n            \"aws_account_1\": {\n                \"region\": \"us-east-1\",\n                \"access_key_id\": \"ACCESS_KEY_ID_1\",\n                \"access_key\": \"ACCESS_KEY_1\",\n            },\n        }\n\n    @patch.dict(\n        os.environ,\n        {\n            \"FIDES__CREDENTIALS__POSTGRES_1__CONNECTION_STRING\": \"postgresql+psycopg2://fides:env_variable.com:5439/fidesctl_test\",\n            \"FIDES__CREDENTIALS__AWS_ACCOUNT_1__ACCESS_KEY_ID\": \"ACCESS_KEY_ID_1\",\n            \"FIDES__CREDENTIALS__AWS_ACCOUNT_1__ACCESS_KEY\": \"ACCESS_KEY_1\",\n        },\n        clear=True,\n    )\n    def test_mixed_configs(self) -> None:\n        \"Test that merging environment variables works with mixed configs\"\n        credentials_dict = {\"aws_account_1\": {\"region\": \"us-east-1\"}}\n        merge_credentials_environment(credentials_dict)\n\n        assert credentials_dict == {\n            \"postgres_1\": {\n                \"connection_string\": \"postgresql+psycopg2://fides:env_variable.com:5439/fidesctl_test\"\n            },\n            \"aws_account_1\": {\n                \"region\": \"us-east-1\",\n                \"access_key_id\": \"ACCESS_KEY_ID_1\",\n                \"access_key\": \"ACCESS_KEY_1\",\n            },\n        }\n\n    @patch.dict(\n        os.environ,\n        {\n            \"FIDES__CREDENTIALS__AWS_ACCOUNT_1__ACCESS_KEY_ID\": \"ACCESS_KEY_ID_OVERRIDE\",\n            \"FIDES__CREDENTIALS__AWS_ACCOUNT_1__ACCESS_KEY\": \"ACCESS_KEY_OVERRIDE\",\n        },\n        clear=True,\n    )\n    def test_environment_override(self) -> None:\n        \"Test merging environment variable works as an override.\"\n        credentials_dict = {\n            \"aws_account_1\": {\n                \"region\": \"us-east-1\",\n                \"access_key_id\": \"ACCESS_KEY_ID_1\",\n                \"access_key\": \"ACCESS_KEY_1\",\n            }\n        }\n        merge_credentials_environment(credentials_dict)\n\n        assert credentials_dict == {\n            \"aws_account_1\": {\n                \"region\": \"us-east-1\",\n                \"access_key_id\": \"ACCESS_KEY_ID_OVERRIDE\",\n                \"access_key\": \"ACCESS_KEY_OVERRIDE\",\n            }\n        }\n"}
{"type": "test_file", "path": "tests/ctl/core/config/test_execution_settings.py", "content": "import pytest\n\nfrom fides.config.execution_settings import ExecutionSettings\n\n\n@pytest.mark.unit\nclass TestExecutionSettings:\n    \"\"\"\n    The disable_consent_identity_verification should be dynamic and based on subject_verification_required but only\n    if a value for disable_consent_identity_verification is not provided. In order for this fallback to work as\n    expected we don't derive the fallback value in the ExecutionSettings but in the ExecutionSettingsProxy.\n    These tests just make sure that we leave disable_consent_identity_verification as is.\n    \"\"\"\n\n    @pytest.mark.parametrize(\n        \"subject_verification_required, disable_consent_verification, expected\",\n        [\n            (True, None, None),\n            (None, None, None),\n            (None, True, True),\n            (True, False, False),\n            (False, None, None),\n        ],\n    )\n    def test_default_value_for_disable_consent_identity_verification(\n        self, subject_verification_required, disable_consent_verification, expected\n    ):\n        kwargs = {}\n        if subject_verification_required is not None:\n            kwargs[\"subject_identity_verification_required\"] = (\n                subject_verification_required\n            )\n        if disable_consent_verification is not None:\n            kwargs[\"disable_consent_identity_verification\"] = (\n                disable_consent_verification\n            )\n\n        settings = ExecutionSettings(**kwargs)\n        assert settings.disable_consent_identity_verification is expected\n"}
{"type": "test_file", "path": "tests/ctl/core/config/test_security_settings.py", "content": "import pytest\n\nfrom fides.config.security_settings import SecuritySettings\n\n\n@pytest.mark.unit\nclass TestSecuritySettings:\n    def test_validate_encryption_key_length_default_value(self):\n        settings = SecuritySettings()\n        assert settings.app_encryption_key == \"\"\n\n    def test_validate_encryption_key_invalid_length_error(self):\n        with pytest.raises(ValueError):\n            SecuritySettings(app_encryption_key=\"bad\")\n\n    def test_validate_encryption_key_valid(self):\n        key = \"aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\"\n        settings = SecuritySettings(app_encryption_key=key)\n        assert settings.app_encryption_key == key\n\n    def test_validate_assemble_cors_origins_invalid_url(self):\n        with pytest.raises(ValueError) as err:\n            SecuritySettings(cors_origins=\"123\")\n\n        assert \"Input should be a valid URL\" in str(err.value)\n\n    def test_validate_assemble_cors_origins_invalid_type(self):\n        with pytest.raises(ValueError):\n            SecuritySettings(cors_origins=123)\n\n    def test_validate_assemble_cors_origins_string_of_urls(self):\n        urls = [\"http://localhost.com\", \"http://test.com\"]\n        settings = SecuritySettings(cors_origins=\", \".join(urls))\n\n        assert settings.cors_origins == urls\n\n    def test_validate_cors_origins_0_0_0_0_is_allowed(self):\n        \"\"\"\n        Test that `0.0.0.0` is allowed as an origin value.\n\n        `0.0.0.0` had been rejected in the past, but there's no reason for\n        us to disallow it, even if it's a non-standard (but valid!) origin value.\n        \"\"\"\n        urls = [\"http://localhost.com\", \"http://0.0.0.0:8000\"]\n        settings = SecuritySettings(cors_origins=\", \".join(urls))\n\n        assert settings.cors_origins == urls\n\n    def test_validate_cors_origins_asterisk_wildcard_is_disallowed(self):\n        \"\"\"\n        Test that `*` is NOT allowed as a special origin value.\n\n        `*` is NOT allowed as an origin because it presents a security risk,\n        even though it's a valid origin. we allow non-owners to edit their own origins\n        but we don't want them to be able to set a wildcard origin.\n        `.*` _can_ be set via the cors_origin_regex setting.\n        \"\"\"\n        urls = [\"*\"]\n        with pytest.raises(ValueError):\n            SecuritySettings(cors_origins=\", \".join(urls))\n\n    def test_validate_cors_origins_urls_with_paths(self):\n        with pytest.raises(ValueError) as e:\n            SecuritySettings(cors_origins=[\"http://test.com/longerpath\"])\n\n        assert \"URL origin values cannot contain a path.\" in str(e)\n\n        with pytest.raises(ValueError) as e:\n            SecuritySettings(cors_origins=[\"http://test.com/123/456\"])\n\n        assert \"URL origin values cannot contain a path.\" in str(e)\n\n        # If there is a trailing slash, it is now stripped off\n        settings = SecuritySettings(cors_origins=[\"http://test.com/\"])\n        assert settings.cors_origins == [\"http://test.com\"]\n\n    def test_assemble_root_access_token_none(self):\n        settings = SecuritySettings(oauth_root_client_secret=\"\")\n\n        assert settings.oauth_root_client_secret_hash is None\n\n    def test_validate_request_rate_limit_invalid_format(self):\n        with pytest.raises(ValueError):\n            SecuritySettings(request_rate_limit=\"invalid\")\n\n    def test_security_settings_env_default_to_prod(self):\n        settings = SecuritySettings()\n        assert settings.env == \"prod\"\n"}
{"type": "test_file", "path": "tests/ctl/core/config/test_utils.py", "content": "# pylint: disable=missing-docstring, redefined-outer-name\nimport os\nfrom types import SimpleNamespace\nfrom typing import Generator\n\nimport pytest\nfrom py._path.local import LocalPath\nfrom toml import dump, load\n\nfrom fides.config import FidesConfig\nfrom fides.config.cli_settings import CLISettings\nfrom fides.config.helpers import update_config_file\nfrom fides.config.utils import replace_config_value\n\n\n@pytest.fixture\ndef test_change_config() -> Generator:\n    \"\"\"Create a dictionary to be used as an example config file\"\"\"\n\n    yield {\"cli\": {\"analytics_id\": \"initial_id\"}}\n\n\n@pytest.mark.unit\ndef test_replace_config_value(tmpdir: LocalPath) -> None:\n    config_dir = tmpdir / \".fides\"\n    os.mkdir(config_dir)\n    config_path = config_dir / \"fides.toml\"\n\n    expected_result = \"# test_value = true\"\n    test_file = \"# test_value = false\"\n\n    with open(config_path, \"w\") as config_file:\n        config_file.write(test_file)\n\n    replace_config_value(str(tmpdir), \"test_value\", \"false\", \"true\")\n\n    with open(config_path, \"r\") as config_file:\n        actual_result = config_file.read()\n\n    print(actual_result)\n    assert actual_result == expected_result\n\n\n@pytest.mark.unit\ndef test_update_config_file_new_value(\n    test_change_config: FidesConfig, tmpdir: LocalPath\n) -> None:\n    \"\"\"\n    Create an example config.toml and validate both updating an\n    existing config setting and adding a new section and setting.\n    \"\"\"\n\n    config_path = os.path.join(tmpdir, \"test_writing_config.toml\")\n\n    with open(config_path, \"w\") as config_file:\n        dump(test_change_config, config_file)\n\n    config_updates = {\n        \"cli\": {\"analytics_id\": \"updated_id\"},\n        \"user\": {\"analytics_opt_out\": True},\n    }\n\n    update_config_file(config_updates, config_path)\n\n    updated_config = load(config_path)\n\n    assert updated_config[\"cli\"] is not None, \"updated_config.cli should exist\"\n    assert (\n        updated_config[\"cli\"][\"analytics_id\"] == \"updated_id\"\n    ), \"updated_config.cli.analytics_id should be 'updated_id'\"\n    assert updated_config[\"user\"] is not None, \"updated_config.user should exist\"\n    assert updated_config[\"user\"][\n        \"analytics_opt_out\"\n    ], \"updated_config.user.analytics_opt_out should be True\"\n\n\n@pytest.mark.unit\ndef test_cli_settings_get_server_url() -> None:\n    \"\"\"Test the get_server_url method of CLISettings\"\"\"\n\n    # no path\n    validation_info = SimpleNamespace(\n        data={\n            \"server_host\": \"test_host\",\n            \"server_protocol\": \"http\",\n            \"server_port\": \"8080\",\n        }\n    )\n    server_url = CLISettings.get_server_url(info=validation_info, value=\"\")\n    assert server_url == \"http://test_host:8080\"\n\n    # specifying a path\n    validation_info.data[\"server_host\"] = \"test_host/api/v1\"\n    server_url = CLISettings.get_server_url(info=validation_info, value=\"\")\n    assert server_url == \"http://test_host:8080\"\n"}
{"type": "test_file", "path": "tests/ctl/core/test_api_helpers.py", "content": "# pylint: disable=missing-docstring, redefined-outer-name\nimport uuid\nfrom typing import Dict, Generator, List\n\nimport pytest\nfrom fideslang import model_list\nfrom fideslang.models import FidesModel\n\nfrom fides.config import FidesConfig\nfrom fides.core import api as _api\nfrom fides.core import api_helpers as _api_helpers\nfrom tests.ctl.types import FixtureRequest\n\nRESOURCE_CREATION_COUNT = 5\n# These resources have tricky validation so the fides_key replacement doesn't work\nEXCLUDED_RESOURCE_TYPES = \"data_category\", \"data_use\"\nPARAM_MODEL_LIST = [\n    model for model in model_list if model not in EXCLUDED_RESOURCE_TYPES\n]\n\n\n# Fixtures\n@pytest.fixture\ndef created_resources(\n    test_config: FidesConfig, resources_dict: Dict, request: FixtureRequest\n) -> Generator:\n    \"\"\"\n    Fixture that creates and tears down a set of resources for each test run.\n    Only creates resources for a given type based on test parameter\n    \"\"\"\n    created_keys = []\n    resource_type = request.param\n    for _ in range(RESOURCE_CREATION_COUNT):\n        base_resource = resources_dict[resource_type].model_copy()\n        uuid_suffix = str(uuid.uuid4())[:6]\n        base_resource.fides_key = \"{}_{}\".format(base_resource.fides_key, uuid_suffix)\n        if hasattr(base_resource, \"name\"):\n            base_resource.name = \"{} {}\".format(base_resource.name, uuid_suffix)\n        _api.create(\n            url=test_config.cli.server_url,\n            resource_type=resource_type,\n            json_resource=base_resource.json(exclude_none=True),\n            headers=test_config.user.auth_header,\n        )\n        created_keys.append(base_resource.fides_key)\n\n    # Wait for test to finish before cleaning up resources\n    yield resource_type, created_keys\n\n    for created_key in created_keys:\n        _api.delete(\n            url=test_config.cli.server_url,\n            resource_type=resource_type,\n            resource_id=created_key,\n            headers=test_config.user.auth_header,\n        )\n\n\ndef delete_resource_type(test_config: FidesConfig, resource_type: str) -> None:\n    \"\"\"Deletes all of the resources of a certain type.\"\"\"\n    url = test_config.cli.server_url\n    fides_keys = [\n        resource[\"fides_key\"]\n        for resource in _api.ls(\n            url,\n            resource_type,\n            headers=test_config.user.auth_header,\n        ).json()\n    ]\n    for fides_key in fides_keys:\n        _api.delete(\n            url,\n            resource_type,\n            fides_key,\n            test_config.user.auth_header,\n        )\n\n\n@pytest.mark.integration\nclass TestGetServerResource:\n    @pytest.mark.parametrize(\n        \"created_resources\", PARAM_MODEL_LIST, indirect=[\"created_resources\"]\n    )\n    def test_get_server_resource_found_resource(\n        self, test_config: FidesConfig, created_resources: List\n    ) -> None:\n        \"\"\"\n        Tests that an existing resource is returned by helper\n        \"\"\"\n        resource_type = created_resources[0]\n        resource_key = created_resources[1][0]\n        result: Dict = _api_helpers.get_server_resource(\n            url=test_config.cli.server_url,\n            resource_type=resource_type,\n            resource_key=resource_key,\n            headers=test_config.user.auth_header,\n        )\n        assert result.get(\"fides_key\") == resource_key\n\n    @pytest.mark.parametrize(\"resource_type\", PARAM_MODEL_LIST)\n    def test_get_server_resource_missing_resource(\n        self, test_config: FidesConfig, resource_type: str\n    ) -> None:\n        \"\"\"\n        Tests that a missing resource returns None\n        \"\"\"\n        resource_key = str(uuid.uuid4())\n        result = _api_helpers.get_server_resource(\n            url=test_config.cli.server_url,\n            resource_type=resource_type,\n            resource_key=resource_key,\n            headers=test_config.user.auth_header,\n        )\n        assert not result\n\n\n@pytest.mark.integration\nclass TestGetServerResources:\n    @pytest.mark.integration\n    @pytest.mark.parametrize(\n        \"created_resources\", PARAM_MODEL_LIST, indirect=[\"created_resources\"]\n    )\n    def test_get_server_resources_found_resources(\n        self, test_config: FidesConfig, created_resources: List\n    ) -> None:\n        \"\"\"\n        Tests that existing resources are returned by helper\n        \"\"\"\n        resource_type = created_resources[0]\n        resource_keys = created_resources[1]\n        result: List[FidesModel] = _api_helpers.get_server_resources(\n            url=test_config.cli.server_url,\n            resource_type=resource_type,\n            existing_keys=resource_keys,\n            headers=test_config.user.auth_header,\n        )\n        print(result)\n        assert set(resource_keys) == set(resource.fides_key for resource in result)\n\n    @pytest.mark.parametrize(\"resource_type\", PARAM_MODEL_LIST)\n    def test_get_server_resources_missing_resources(\n        self, test_config: FidesConfig, resource_type: str\n    ) -> None:\n        \"\"\"\n        Tests that a missing resource returns an empty list\n        \"\"\"\n        resource_keys = [str(uuid.uuid4())]\n        result: List[FidesModel] = _api_helpers.get_server_resources(\n            url=test_config.cli.server_url,\n            resource_type=resource_type,\n            existing_keys=resource_keys,\n            headers=test_config.user.auth_header,\n        )\n        assert result == []\n\n\n@pytest.mark.integration\nclass TestListServerResources:\n    def test_list_server_resources_passing(self, test_config: FidesConfig) -> None:\n        resource_type = \"data_category\"\n        result = _api_helpers.list_server_resources(\n            url=test_config.cli.server_url,\n            resource_type=resource_type,\n            headers=test_config.user.auth_header,\n            exclude_keys=[],\n        )\n        assert result\n\n    def test_list_server_resources_none(self, test_config: FidesConfig) -> None:\n        resource_type = \"system\"\n        delete_resource_type(test_config, resource_type)\n        result: List[FidesModel] = _api_helpers.list_server_resources(\n            url=test_config.cli.server_url,\n            resource_type=resource_type,\n            headers=test_config.user.auth_header,\n            exclude_keys=[],\n        )\n        assert result == []\n"}
{"type": "test_file", "path": "tests/ctl/core/test_apply.py", "content": "# pylint: disable=missing-docstring, redefined-outer-name\n\"\"\"Unit tests for the Commands module.\"\"\"\nfrom typing import Dict, Generator, List\n\nimport fideslang as models\nimport pytest\n\nfrom fides.core.push import get_orphan_datasets, sort_create_update\n\n\n# Helpers\n@pytest.fixture()\ndef server_resource_list() -> Generator:\n    yield [\n        {\"fides_key\": \"testKey\", \"id\": 1, \"name\": \"Some resource\"},\n        {\"fides_key\": \"anotherTestKey\", \"id\": 2, \"name\": \"Another resource\"},\n    ]\n\n\n@pytest.fixture()\ndef server_resource_key_pairs() -> Generator:\n    yield {\"testKey\": 1, \"anotherTestKey\": 2}\n\n\n@pytest.fixture()\ndef system_with_dataset_reference() -> Generator:\n    yield [\n        models.System(\n            organization_fides_key=\"1\",\n            fides_key=\"test_system\",\n            system_type=\"test\",\n            privacy_declarations=[\n                models.PrivacyDeclaration(\n                    name=\"test_privacy_declaration\",\n                    data_categories=[],\n                    data_use=\"test_data_use\",\n                    data_subjects=[],\n                    dataset_references=[\"test_dataset\"],\n                ),\n            ],\n        ),\n    ]\n\n\n# Unit\n@pytest.mark.unit\ndef test_sort_create_update_create() -> None:\n    resource_1 = models.DataCategory(\n        organization_fides_key=\"1\",\n        fides_key=\"some_resource\",\n        name=\"Test resource 1\",\n        description=\"Test Description\",\n    )\n    resource_2 = models.DataCategory(\n        organization_fides_key=\"1\",\n        fides_key=\"another_system\",\n        name=\"Test System 2\",\n        description=\"Test Description\",\n    )\n    expected_create_result = [resource_2]\n    manifest_resource_list = [resource_2]\n    server_resource_list = [resource_1]\n\n    (\n        create_result,\n        update_result,\n    ) = sort_create_update(manifest_resource_list, server_resource_list)\n    assert create_result == expected_create_result\n    assert update_result == []\n\n\n@pytest.mark.unit\ndef test_sort_create_update_update() -> None:\n    resource_1 = models.DataCategory(\n        id=1,\n        organization_fides_key=\"1\",\n        fides_key=\"some_resource\",\n        name=\"Test resource 1\",\n        description=\"Test Description\",\n    )\n    resource_2 = models.DataCategory(\n        organization_fides_key=\"1\",\n        fides_key=\"some_resource\",\n        name=\"Test System 2\",\n        description=\"Test Description\",\n    )\n    expected_update_result = [resource_2]\n    manifest_resource_list = [resource_2]\n    server_resource_list = [resource_1]\n\n    (\n        create_result,\n        update_result,\n    ) = sort_create_update(manifest_resource_list, server_resource_list)\n    assert [] == create_result\n    assert expected_update_result == update_result\n\n\n@pytest.mark.parametrize(\n    \"taxonomies, expected_length\",\n    [\n        (\n            {\n                \"dataset\": [\n                    models.Dataset(\n                        organization_fides_key=\"1\",\n                        fides_key=\"test_dataset\",\n                        collections=[],\n                    ),\n                ],\n            },\n            0,\n        ),\n        (\n            {\n                \"dataset\": [\n                    models.Dataset(\n                        organization_fides_key=\"1\",\n                        fides_key=\"test_dataset_unused\",\n                        collections=[],\n                    ),\n                ],\n            },\n            1,\n        ),\n        (\n            {\n                \"dataset\": [\n                    models.Dataset(\n                        organization_fides_key=\"1\",\n                        fides_key=\"test_dataset\",\n                        collections=[],\n                    ),\n                    models.Dataset(\n                        organization_fides_key=\"1\",\n                        fides_key=\"test_dataset_unused\",\n                        collections=[],\n                    ),\n                ],\n            },\n            1,\n        ),\n        (\n            {\n                \"dataset\": [\n                    models.Dataset(\n                        organization_fides_key=\"1\",\n                        fides_key=\"test_dataset\",\n                        collections=[],\n                    ),\n                    models.Dataset(\n                        organization_fides_key=\"1\",\n                        fides_key=\"test_dataset_unused\",\n                        collections=[],\n                    ),\n                    models.Dataset(\n                        organization_fides_key=\"1\",\n                        fides_key=\"test_dataset_also_unused\",\n                        collections=[],\n                    ),\n                ],\n            },\n            2,\n        ),\n    ],\n)\n@pytest.mark.unit\ndef test_validate_dataset_usage(\n    taxonomies: Dict, expected_length: int, system_with_dataset_reference: List\n) -> None:\n    \"\"\"\n    Tests different scenarios for referenced datasets\n    \"\"\"\n    taxonomies[\"system\"] = system_with_dataset_reference\n    taxonomy = models.Taxonomy.model_validate(taxonomies)\n    missing_datasets = get_orphan_datasets(taxonomy)\n    assert len(missing_datasets) == expected_length\n"}
{"type": "test_file", "path": "tests/ctl/core/test_audit.py", "content": "# pylint: disable=missing-docstring, redefined-outer-name\nfrom typing import Generator, List\n\nimport pytest\nfrom fideslang.models import (\n    ContactDetails,\n    DataSubject,\n    DataSubjectRights,\n    DataUse,\n    Organization,\n)\n\nfrom fides.core import audit\n\n\n@pytest.fixture\ndef test_default_organization() -> Generator:\n    yield [Organization(fides_key=\"default_organization\")]\n\n\n@pytest.fixture\ndef test_rich_organization() -> Generator:\n    test_contact = ContactDetails(name=\"test\")\n    yield [\n        Organization(\n            fides_key=\"default_organization\",\n            controller=test_contact,\n            data_protection_officer=test_contact,\n            representative=test_contact,\n            security_policy=\"http://sample.org/\",\n        )\n    ]\n\n\n@pytest.fixture\ndef test_basic_data_subject() -> Generator:\n    yield [DataSubject(fides_key=\"test_data_subject\")]\n\n\n@pytest.fixture\ndef test_rich_data_subject() -> Generator:\n    data_subject_rights = DataSubjectRights(strategy=\"ALL\", values=None)\n    yield [\n        DataSubject(\n            fides_key=\"test_data_subject\",\n            rights=data_subject_rights,\n            automated_decisions_or_profiling=False,\n        )\n    ]\n\n\n@pytest.mark.unit\ndef test_basic_organization_fails_audit(\n    test_default_organization: List[Organization],\n) -> None:\n    audit_findings = audit.audit_organization_attributes(test_default_organization[0])\n    assert audit_findings > 0\n\n\n@pytest.mark.unit\ndef test_rich_organization_passes_audit(\n    test_rich_organization: List[Organization],\n) -> None:\n    audit_findings = audit.audit_organization_attributes(test_rich_organization[0])\n    assert audit_findings == 0\n\n\n@pytest.mark.unit\ndef test_basic_data_subject_fails_audit(\n    test_basic_data_subject: List[DataSubject],\n) -> None:\n    audit_findings = audit.audit_data_subject_attributes(\n        test_basic_data_subject[0], \"test_system_name\"\n    )\n    assert audit_findings > 0\n\n\n@pytest.mark.unit\ndef test_rich_data_subject_passes_audit(\n    test_rich_data_subject: List[DataSubject],\n) -> None:\n    audit_findings = audit.audit_data_subject_attributes(\n        test_rich_data_subject[0], \"test_system_name\"\n    )\n    assert audit_findings == 0\n"}
{"type": "test_file", "path": "tests/ctl/core/test_custom_field_models.py", "content": "# pylint: disable=missing-docstring, redefined-outer-name\n\nimport pytest\nfrom sqlalchemy.exc import IntegrityError\n\nfrom fides.api.common_exceptions import KeyOrNameAlreadyExists\nfrom fides.api.models.sql_models import CustomFieldDefinition\n\n\n@pytest.fixture(autouse=True)\ndef clear_custom_metadata_resources(db):\n    \"\"\"\n    Fixture run on each test to clear custom field DB tables,\n    to ensure each test runs with a clean slate.\n    \"\"\"\n\n    def delete_data(tables):\n        for table in tables:\n            table.query(db).delete()\n        db.commit()\n\n    tables = [\n        CustomFieldDefinition,\n    ]\n\n    delete_data(tables)\n\n    yield\n    delete_data(tables)\n\n\ndef test_custom_field_definition_duplicate_name_rejected_create(db):\n    \"\"\"Assert case-insensitive unique checks on name/resource type upon creation\"\"\"\n    definition1 = CustomFieldDefinition.create(\n        db=db,\n        data={\n            \"name\": \"test1\",\n            \"description\": \"test\",\n            \"field_type\": \"string\",\n            \"resource_type\": \"system\",\n            \"field_definition\": \"string\",\n        },\n    )\n    with pytest.raises(KeyOrNameAlreadyExists):\n        CustomFieldDefinition.create(\n            db=db,\n            data={\n                \"name\": \"Test1\",\n                \"description\": \"test\",\n                \"field_type\": \"string\",\n                \"resource_type\": \"system\",\n                \"field_definition\": \"string\",\n            },\n        )\n\n    # assert the second record didn't get created\n    assert len(CustomFieldDefinition.all(db)) == 1\n\n    # with a different resource type, we should allow creation\n    definition2 = CustomFieldDefinition.create(\n        db=db,\n        data={\n            \"name\": \"Test1\",\n            \"description\": \"test\",\n            \"field_type\": \"string\",\n            \"resource_type\": \"privacy_declaration\",\n            \"field_definition\": \"string\",\n        },\n    )\n\n    # assert we've got two different records created successfully\n    assert len(CustomFieldDefinition.all(db)) == 2\n    assert definition1.id != definition2.id\n\n\ndef test_custom_field_definition_duplicate_name_different_resource_type_accepted(db):\n    \"\"\"Assert that we can create custom field definition with same name on a different resource type\"\"\"\n    definition1 = CustomFieldDefinition.create(\n        db=db,\n        data={\n            \"name\": \"test1\",\n            \"description\": \"test\",\n            \"field_type\": \"string\",\n            \"resource_type\": \"system\",\n            \"field_definition\": \"string\",\n        },\n    )\n\n    definition2 = CustomFieldDefinition.create(\n        db=db,\n        data={\n            \"name\": \"test1\",\n            \"description\": \"test\",\n            \"field_type\": \"string\",\n            \"resource_type\": \"privacy_declaration\",  # different resource type, so this should be allowed\n            \"field_definition\": \"string\",\n        },\n    )\n\n    # assert we've got two different records created successfully\n    assert len(CustomFieldDefinition.all(db)) == 2\n    assert definition1.id != definition2.id\n\n\ndef test_custom_field_definition_duplicate_name_rejected_update(db):\n    \"\"\"Assert case-insensitive unique checks on name/resource type upon update\"\"\"\n\n    definition1 = CustomFieldDefinition.create(\n        db=db,\n        data={\n            \"name\": \"test1\",\n            \"description\": \"test\",\n            \"field_type\": \"string\",\n            \"resource_type\": \"system\",\n            \"field_definition\": \"string\",\n        },\n    )\n\n    definition2 = CustomFieldDefinition.create(\n        db=db,\n        data={\n            \"name\": \"Test 1\",  # space in name should allow creation, considered unique name\n            \"description\": \"test\",\n            \"field_type\": \"string\",\n            \"resource_type\": \"system\",\n            \"field_definition\": \"string\",\n        },\n    )\n\n    # assert we've got two different records created successfully\n    assert len(CustomFieldDefinition.all(db)) == 2\n    assert definition1.id != definition2.id\n\n    with pytest.raises(KeyOrNameAlreadyExists) as e:\n        definition2 = definition2.update(\n            db=db,\n            data={\n                \"name\": \"Test1\",  # if we try to update name to remove space, we should hit uniqueness error\n            },\n        )\n\n    # assert update did not go through\n    db.refresh(definition2)\n    assert definition2.name == \"Test 1\"\n"}
{"type": "test_file", "path": "tests/ctl/core/test_dataset.py", "content": "# pylint: disable=missing-docstring, redefined-outer-name\nimport os\nfrom typing import Dict, Generator, List\nfrom urllib.parse import quote_plus\nfrom uuid import uuid4\n\nimport pymssql\nimport pytest\nimport sqlalchemy\nfrom fideslang.manifests import write_manifest\nfrom fideslang.models import Dataset, DatasetCollection, DatasetField\nfrom py._path.local import LocalPath\nfrom sqlalchemy.orm import Session\n\nfrom fides.api.db.crud import get_resource\nfrom fides.api.models.connectionconfig import (\n    AccessLevel,\n    ConnectionConfig,\n    ConnectionType,\n)\nfrom fides.api.models.datasetconfig import DatasetConfig\nfrom fides.api.models.sql_models import Dataset as CtlDataset\nfrom fides.config import FidesConfig\nfrom fides.core import api\nfrom fides.core import dataset as _dataset\n\n\ndef create_server_datasets(test_config: FidesConfig, datasets: List[Dataset]) -> None:\n    for dataset in datasets:\n        api.delete(\n            url=test_config.cli.server_url,\n            resource_type=\"dataset\",\n            resource_id=dataset.fides_key,\n            headers=test_config.user.auth_header,\n        )\n        api.create(\n            url=test_config.cli.server_url,\n            resource_type=\"dataset\",\n            json_resource=dataset.json(exclude_none=True),\n            headers=test_config.user.auth_header,\n        )\n\n\ndef set_field_data_categories(datasets: List[Dataset], category: str) -> None:\n    for dataset in datasets:\n        for collection in dataset.collections:\n            for field in collection.fields:\n                field.data_categories.append(category)\n\n\n# Unit\n@pytest.mark.unit\ndef test_create_db_datasets() -> None:\n    test_resource = {\"ds\": {\"foo\": [\"1\", \"2\"], \"bar\": [\"4\", \"5\"]}}\n    expected_result = [\n        Dataset(\n            name=\"ds\",\n            fides_key=\"ds\",\n            data_categories=[],\n            description=\"Fides Generated Description for Schema: ds\",\n            collections=[\n                DatasetCollection(\n                    name=\"foo\",\n                    description=\"Fides Generated Description for Table: foo\",\n                    data_categories=[],\n                    fields=[\n                        DatasetField(\n                            name=\"1\",\n                            description=\"Fides Generated Description for Column: 1\",\n                            data_categories=[],\n                        ),\n                        DatasetField(\n                            name=\"2\",\n                            description=\"Fides Generated Description for Column: 2\",\n                            data_categories=[],\n                        ),\n                    ],\n                ),\n                DatasetCollection(\n                    name=\"bar\",\n                    description=\"Fides Generated Description for Table: bar\",\n                    data_categories=[],\n                    fields=[\n                        DatasetField(\n                            name=\"4\",\n                            description=\"Fides Generated Description for Column: 4\",\n                            data_categories=[],\n                        ),\n                        DatasetField(\n                            name=\"5\",\n                            description=\"Fides Generated Description for Column: 5\",\n                            data_categories=[],\n                        ),\n                    ],\n                ),\n            ],\n        )\n    ]\n    actual_result = _dataset.create_db_datasets(test_resource)\n    assert actual_result == expected_result\n\n\n@pytest.mark.unit\ndef test_find_uncategorized_dataset_fields_all_categorized() -> None:\n    test_resource = {\"foo\": [\"1\", \"2\"], \"bar\": [\"4\", \"5\"]}\n    test_resource_dataset = _dataset.create_db_dataset(\"ds\", test_resource)\n    dataset = Dataset(\n        name=\"ds\",\n        fides_key=\"ds\",\n        collections=[\n            DatasetCollection(\n                name=\"foo\",\n                fields=[\n                    DatasetField(\n                        name=\"1\",\n                        data_categories=[\"category_1\"],\n                    ),\n                    DatasetField(\n                        name=\"2\",\n                        data_categories=[\"category_1\"],\n                    ),\n                ],\n            ),\n            DatasetCollection(\n                name=\"bar\",\n                fields=[\n                    DatasetField(\n                        name=\"4\",\n                        data_categories=[\"category_1\"],\n                    ),\n                    DatasetField(name=\"5\", data_categories=[\"category_1\"]),\n                ],\n            ),\n        ],\n    )\n    (\n        uncategorized_keys,\n        total_field_count,\n    ) = _dataset.find_uncategorized_dataset_fields(\n        existing_dataset=dataset, source_dataset=test_resource_dataset\n    )\n    assert not uncategorized_keys\n    assert total_field_count == 4\n\n\n@pytest.fixture(scope=\"function\")\ndef connection_config(\n    db: Session,\n) -> Generator:\n    connection_config = ConnectionConfig.create(\n        db=db,\n        data={\n            \"name\": str(uuid4()),\n            \"key\": \"my_postgres_db_1\",\n            \"connection_type\": ConnectionType.postgres,\n            \"access\": AccessLevel.write,\n            \"disabled\": False,\n            \"description\": \"Primary postgres connection\",\n        },\n    )\n    yield connection_config\n    connection_config.delete(db)\n\n\n@pytest.mark.unit\nasync def test_upsert_db_datasets(\n    test_config: FidesConfig, db: Session, connection_config, async_session\n) -> None:\n    \"\"\"\n    Upsert a CTL Dataset, link this to a DatasetConfig and then upsert that CTL Dataset again.\n\n    The id of the CTL Dataset cannot change on upsert, as the DatasetConfig has a FK to this resource.\n    \"\"\"\n\n    dataset = Dataset(\n        name=\"ds1\",\n        fides_key=\"ds\",\n        data_categories=[],\n        description=\"Fides Generated Description for Schema: ds\",\n        collections=[\n            DatasetCollection(\n                name=\"foo\",\n                description=\"Fides Generated Description for Table: foo\",\n                data_categories=[],\n                fields=[\n                    DatasetField(\n                        name=\"1\",\n                        description=\"Fides Generated Description for Column: 1\",\n                        data_categories=[],\n                    ),\n                    DatasetField(\n                        name=\"2\",\n                        description=\"Fides Generated Description for Column: 2\",\n                        data_categories=[],\n                    ),\n                ],\n            ),\n            DatasetCollection(\n                name=\"bar\",\n                description=\"Fides Generated Description for Table: bar\",\n                data_categories=[],\n                fields=[\n                    DatasetField(\n                        name=\"4\",\n                        description=\"Fides Generated Description for Column: 4\",\n                        data_categories=[],\n                    ),\n                    DatasetField(\n                        name=\"5\",\n                        description=\"Fides Generated Description for Column: 5\",\n                        data_categories=[],\n                    ),\n                ],\n            ),\n        ],\n    )\n\n    resp = api.upsert(\n        url=test_config.cli.server_url,\n        resource_type=\"dataset\",\n        resources=[dataset.model_dump(exclude_none=True)],\n        headers=test_config.user.auth_header,\n    )\n    assert resp.status_code == 201\n    assert resp.json()[\"inserted\"] == 1\n\n    ds: CtlDataset = await get_resource(CtlDataset, \"ds\", async_session)\n\n    # Create a DatasetConfig that links to the created CTL Dataset\n    dataset_config = DatasetConfig.create(\n        db=db,\n        data={\n            \"connection_config_id\": connection_config.id,\n            \"fides_key\": \"new_fides_key\",\n            \"ctl_dataset_id\": ds.id,\n        },\n    )\n\n    ctl_dataset_id = ds.id\n    assert dataset_config.ctl_dataset_id == ctl_dataset_id\n\n    # Do another upsert of the CTL Dataset to update the name\n    dataset.name = \"new name\"\n    resp = api.upsert(\n        url=test_config.cli.server_url,\n        resource_type=\"dataset\",\n        resources=[dataset.model_dump(exclude_none=True)],\n        headers=test_config.user.auth_header,\n    )\n    assert resp.status_code == 200\n    assert resp.json()[\"inserted\"] == 0\n    assert resp.json()[\"updated\"] == 1\n\n    db.refresh(dataset_config)\n    assert dataset_config.ctl_dataset.name == \"new name\"\n    assert dataset_config.ctl_dataset.id == ctl_dataset_id, \"Id unchanged with upsert\"\n\n\n@pytest.mark.unit\ndef test_find_uncategorized_dataset_fields_uncategorized_fields() -> None:\n    test_resource = {\"foo\": [\"1\", \"2\"]}\n    test_resource_dataset = _dataset.create_db_dataset(\"ds\", test_resource)\n    existing_dataset = Dataset(\n        name=\"ds\",\n        fides_key=\"ds\",\n        data_categories=[\"category_1\"],\n        collections=[\n            DatasetCollection(\n                name=\"foo\",\n                data_categories=[\"category_1\"],\n                fields=[\n                    DatasetField(\n                        name=\"1\",\n                        data_categories=[\"category_1\"],\n                    ),\n                    DatasetField(name=\"2\"),\n                ],\n            )\n        ],\n    )\n    (\n        uncategorized_keys,\n        total_field_count,\n    ) = _dataset.find_uncategorized_dataset_fields(\n        existing_dataset=existing_dataset, source_dataset=test_resource_dataset\n    )\n    assert set(uncategorized_keys) == {\"ds.foo.2\"}\n    assert total_field_count == 2\n\n\n@pytest.mark.unit\ndef test_find_uncategorized_dataset_fields_missing_field() -> None:\n    test_resource = {\"bar\": [\"4\", \"5\"]}\n    test_resource_dataset = _dataset.create_db_dataset(\"ds\", test_resource)\n    existing_dataset = Dataset(\n        name=\"ds\",\n        fides_key=\"ds\",\n        collections=[\n            DatasetCollection(\n                name=\"bar\",\n                fields=[\n                    DatasetField(\n                        name=\"4\",\n                        data_categories=[\"category_1\"],\n                    )\n                ],\n            ),\n        ],\n    )\n    (\n        uncategorized_keys,\n        total_field_count,\n    ) = _dataset.find_uncategorized_dataset_fields(\n        existing_dataset=existing_dataset, source_dataset=test_resource_dataset\n    )\n    assert set(uncategorized_keys) == {\"ds.bar.5\"}\n    assert total_field_count == 2\n\n\n@pytest.mark.unit\ndef test_find_uncategorized_dataset_fields_missing_collection() -> None:\n    test_resource = {\"foo\": [\"1\", \"2\"], \"bar\": [\"4\", \"5\"]}\n    test_resource_dataset = _dataset.create_db_dataset(\"ds\", test_resource)\n    existing_dataset = Dataset(\n        name=\"ds\",\n        fides_key=\"ds\",\n        collections=[\n            DatasetCollection(\n                name=\"bar\",\n                fields=[\n                    DatasetField(\n                        name=\"4\",\n                        data_categories=[\"category_1\"],\n                    ),\n                    DatasetField(\n                        name=\"5\",\n                        data_categories=[\"category_1\"],\n                    ),\n                ],\n            ),\n        ],\n    )\n    (\n        uncategorized_keys,\n        total_field_count,\n    ) = _dataset.find_uncategorized_dataset_fields(\n        existing_dataset=existing_dataset, source_dataset=test_resource_dataset\n    )\n    assert set(uncategorized_keys) == {\"ds.foo.1\", \"ds.foo.2\"}\n    assert total_field_count == 4\n\n\n@pytest.mark.unit\ndef test_unsupported_dialect_error() -> None:\n    test_url = \"foo+psycopg2://fidesdb:fidesdb@fidesdb:5432/fidesdb\"\n    with pytest.raises(Exception):\n        _dataset.generate_dataset_db(test_url, \"test_file.yml\", False)\n\n\n@pytest.mark.unit\ndef test_field_data_categories(db) -> None:\n    \"\"\"\n    Verify that field_data_categories works for fields with and without data categories.\n    \"\"\"\n\n    ctl_dataset = CtlDataset.create_from_dataset_dict(\n        db,\n        {\n            \"fides_key\": f\"dataset_key-f{uuid4()}\",\n            \"collections\": [\n                {\n                    \"name\": \"customer\",\n                    \"fields\": [\n                        {\n                            \"name\": \"email\",\n                            \"data_categories\": [\"user.contact.email\"],\n                        },\n                        {\"name\": \"first_name\"},\n                    ],\n                }\n            ],\n        },\n    )\n    assert ctl_dataset.field_data_categories\n\n\n@pytest.mark.unit\ndef test_namespace_meta(db) -> None:\n    ctl_dataset = CtlDataset.create_from_dataset_dict(\n        db,\n        {\n            \"fides_key\": f\"dataset_key-f{uuid4()}\",\n            \"fides_meta\": {\"namespace\": {\"dataset_id\": \"public\"}},\n            \"collections\": [],\n        },\n    )\n    assert ctl_dataset.fides_meta == {\n        \"resource_id\": None,\n        \"after\": None,\n        \"namespace\": {\"dataset_id\": \"public\"},\n    }\n\n\n# Generate Dataset Database Integration Tests\n\n# These URLs are for the databases in the docker-compose.integration-tests.yml file\nPOSTGRES_URL = (\n    \"postgresql+psycopg2://postgres:postgres@postgres-test:5432/postgres_example?\"\n)\n\nMYSQL_URL = \"mysql+pymysql://mysql_user:mysql_pw@mysql-test:3306/mysql_example\"\n\nMSSQL_URL_TEMPLATE = \"mssql+pymssql://sa:SQLserver1@sqlserver-test:1433/{}\"\nMSSQL_URL = MSSQL_URL_TEMPLATE.format(\"sqlserver_example\")\nMASTER_MSSQL_URL = MSSQL_URL_TEMPLATE.format(\"master\")\n\n# External databases require credentials passed through environment variables\nSNOWFLAKE_URL_TEMPLATE = \"snowflake://FIDESCTL:{}@ZOA73785/FIDESCTL_TEST\"\nSNOWFLAKE_URL = SNOWFLAKE_URL_TEMPLATE.format(\n    quote_plus(os.getenv(\"SNOWFLAKE_FIDESCTL_PASSWORD\", \"\"))\n)\n\nREDSHIFT_URL_TEMPLATE = \"redshift+psycopg2://fidesctl:{}@redshift-cluster-1.cohs2e5eq2e4.us-east-1.redshift.amazonaws.com:5439/fidesctl_test\"\nREDSHIFT_URL = REDSHIFT_URL_TEMPLATE.format(\n    quote_plus(os.getenv(\"REDSHIFT_FIDESCTL_PASSWORD\", \"\"))\n)\n\n\nTEST_DATABASE_PARAMETERS = {\n    \"postgresql\": {\n        \"url\": POSTGRES_URL,\n        \"setup_url\": POSTGRES_URL,\n        \"init_script_path\": \"tests/ctl/data/example_sql/postgres_example.sql\",\n        \"is_external\": False,\n        \"expected_collection\": {\n            \"public\": {\n                \"visit\": [\"email\", \"last_visit\"],\n                \"login\": [\"id\", \"customer_id\", \"time\"],\n            }\n        },\n    },\n    \"mysql\": {\n        \"url\": MYSQL_URL,\n        \"setup_url\": MYSQL_URL,\n        \"init_script_path\": \"tests/ctl/data/example_sql/mysql_example.sql\",\n        \"is_external\": False,\n        \"expected_collection\": {\n            \"mysql_example\": {\n                \"visit\": [\"email\", \"last_visit\"],\n                \"login\": [\"id\", \"customer_id\", \"time\"],\n            }\n        },\n    },\n    \"mssql\": {\n        \"url\": MSSQL_URL,\n        \"setup_url\": MASTER_MSSQL_URL,\n        \"server\": \"sqlserver-test:1433\",\n        \"username\": \"sa\",\n        \"password\": \"SQLserver1\",\n        \"init_script_path\": \"tests/ctl/data/example_sql/sqlserver_example.sql\",\n        \"is_external\": False,\n        \"expected_collection\": {\n            \"dbo\": {\n                \"visit\": [\"email\", \"last_visit\"],\n                \"login\": [\"id\", \"customer_id\", \"time\"],\n            }\n        },\n    },\n    \"snowflake\": {\n        \"url\": SNOWFLAKE_URL,\n        \"setup_url\": SNOWFLAKE_URL,\n        \"init_script_path\": \"tests/ctl/data/example_sql/snowflake_example.sql\",\n        \"is_external\": True,\n        \"expected_collection\": {\n            \"PUBLIC\": {\n                \"VISIT\": [\"EMAIL\", \"LAST_VISIT\"],\n                \"LOGIN\": [\"ID\", \"CUSTOMER_ID\", \"TIME\"],\n            }\n        },\n    },\n    \"redshift\": {\n        \"url\": REDSHIFT_URL,\n        \"setup_url\": REDSHIFT_URL,\n        \"init_script_path\": \"tests/ctl/data/example_sql/redshift_example.sql\",\n        \"is_external\": True,\n        \"expected_collection\": {\n            \"public\": {\n                \"visit\": [\"email\", \"last_visit\"],\n                \"login\": [\"id\", \"customer_id\", \"time\"],\n            }\n        },\n    },\n}\n\n\n@pytest.mark.external\n@pytest.mark.parametrize(\"database_type\", TEST_DATABASE_PARAMETERS.keys())\nclass TestDatabase:\n    @pytest.fixture(scope=\"class\", autouse=True)\n    def database_setup(self) -> Generator:\n        \"\"\"\n        Set up the Database for testing.\n\n        The query file must have each query on a separate line.\n        \"\"\"\n        for database_parameters in TEST_DATABASE_PARAMETERS.values():\n            engine = sqlalchemy.create_engine(database_parameters.get(\"setup_url\"))\n            with open(database_parameters.get(\"init_script_path\"), \"r\") as query_file:  # type: ignore\n                queries = [\n                    query for query in query_file.read().splitlines() if query != \"\"\n                ]\n\n            try:\n                if \"pymssql\" not in database_parameters.get(\"setup_url\", \"\"):\n                    for query in queries:\n                        engine.execute(sqlalchemy.sql.text(query))\n                else:\n                    with pymssql.connect(\n                        database_parameters[\"server\"],\n                        database_parameters[\"username\"],\n                        database_parameters[\"password\"],\n                        autocommit=True,\n                    ) as connection:\n                        for query in queries:\n                            with connection.cursor() as cursor:\n                                cursor.execute(query)\n            except:\n                print(f\"> FAILED DB SETUP: {database_parameters.get('setup_url')}\")\n                # We don't want to error all tests if a single setup fails\n                pass\n        yield\n\n    def test_get_db_tables(self, request: Dict, database_type: str) -> None:\n        database_parameters = TEST_DATABASE_PARAMETERS[database_type]\n        engine = sqlalchemy.create_engine(database_parameters.get(\"url\"))\n        actual_result = _dataset.get_db_schemas(engine=engine)\n        assert actual_result == database_parameters.get(\"expected_collection\")\n\n    def test_generate_dataset(self, tmpdir: LocalPath, database_type: str) -> None:\n        database_parameters = TEST_DATABASE_PARAMETERS[database_type]\n        actual_result = _dataset.generate_dataset_db(\n            database_parameters.get(\"url\"), f\"{tmpdir}/test_file.yml\", False\n        )\n        assert actual_result\n\n    def test_generate_dataset_passes_(\n        self, test_config: FidesConfig, database_type: str\n    ) -> None:\n        database_parameters = TEST_DATABASE_PARAMETERS[database_type]\n        datasets: List[Dataset] = _dataset.create_db_datasets(\n            database_parameters.get(\"expected_collection\")\n        )\n        set_field_data_categories(datasets, \"system.operations\")\n        create_server_datasets(test_config, datasets)\n        _dataset.scan_dataset_db(\n            connection_string=database_parameters.get(\"url\"),\n            manifest_dir=\"\",\n            coverage_threshold=100,\n            url=test_config.cli.server_url,\n            headers=test_config.user.auth_header,\n        )\n\n    def test_generate_dataset_coverage_failure(\n        self, test_config: FidesConfig, database_type: str\n    ) -> None:\n        database_parameters = TEST_DATABASE_PARAMETERS[database_type]\n        datasets: List[Dataset] = _dataset.create_db_datasets(\n            database_parameters.get(\"expected_collection\")\n        )\n        create_server_datasets(test_config, datasets)\n        with pytest.raises(SystemExit):\n            _dataset.scan_dataset_db(\n                connection_string=database_parameters.get(\"url\"),\n                manifest_dir=\"\",\n                coverage_threshold=100,\n                url=test_config.cli.server_url,\n                headers=test_config.user.auth_header,\n            )\n\n    def test_dataset_coverage_manifest_passes(\n        self, test_config: FidesConfig, tmpdir: LocalPath, database_type: str\n    ) -> None:\n        database_parameters = TEST_DATABASE_PARAMETERS[database_type]\n        datasets: List[Dataset] = _dataset.create_db_datasets(\n            database_parameters.get(\"expected_collection\")\n        )\n        set_field_data_categories(datasets, \"system.operations\")\n\n        file_name = tmpdir.join(\"dataset.yml\")\n        write_manifest(\n            file_name, [i.model_dump(mode=\"json\") for i in datasets], \"dataset\"\n        )\n\n        create_server_datasets(test_config, datasets)\n        _dataset.scan_dataset_db(\n            connection_string=database_parameters.get(\"url\"),\n            manifest_dir=f\"{tmpdir}\",\n            coverage_threshold=100,\n            url=test_config.cli.server_url,\n            headers=test_config.user.auth_header,\n        )\n"}
{"type": "test_file", "path": "tests/ctl/core/test_evaluate.py", "content": "# pylint: disable=missing-docstring, redefined-outer-name\nfrom typing import List\nfrom unittest.mock import MagicMock, patch\n\nimport pytest\nfrom fideslang.models import (\n    DataCategory,\n    Dataset,\n    DatasetCollection,\n    DataSubject,\n    DataUse,\n    MatchesEnum,\n    Policy,\n    PolicyRule,\n    PrivacyDeclaration,\n    System,\n    Taxonomy,\n)\n\nfrom fides.config import FidesConfig\nfrom fides.core import evaluate\n\n\n# Helpers\n@pytest.fixture()\ndef evaluation_key_validation_basic_taxonomy() -> Taxonomy:\n    yield Taxonomy(\n        data_subject=[\n            DataSubject(fides_key=\"data_subject_1\"),\n            DataSubject(fides_key=\"data_subject_2\"),\n        ],\n        data_category=[\n            DataCategory(fides_key=\"data_category_1\"),\n            DataCategory(fides_key=\"data_category_2\"),\n        ],\n        data_use=[DataUse(fides_key=\"data_use_1\"), DataUse(fides_key=\"data_use_2\")],\n    )\n\n\n@pytest.fixture()\ndef evaluation_hierarchical_key_basic_taxonomy() -> Taxonomy:\n    yield Taxonomy(\n        data_category=[\n            DataCategory(\n                fides_key=\"data_category\",\n            ),\n            DataCategory(\n                fides_key=\"data_category.parent\",\n                parent_key=\"data_category\",\n            ),\n            DataCategory(\n                fides_key=\"data_category.parent.child\",\n                parent_key=\"data_category.parent\",\n            ),\n        ]\n    )\n\n\ndef create_policy_rule_with_keys(\n    data_categories: List[str],\n    data_uses: List[str],\n    data_subjects: List[str],\n) -> PolicyRule:\n    return PolicyRule(\n        name=\"policy_rule_1\",\n        data_categories={\n            \"values\": data_categories,\n            \"matches\": MatchesEnum.ANY,\n        },\n        data_uses={\n            \"values\": data_uses,\n            \"matches\": MatchesEnum.ANY,\n        },\n        data_subjects={\n            \"values\": data_subjects,\n            \"matches\": MatchesEnum.ANY,\n        },\n    )\n\n\n@pytest.mark.integration\ndef test_get_all_server_policies(test_config: FidesConfig) -> None:\n    result = evaluate.get_all_server_policies(\n        url=test_config.cli.server_url, headers=test_config.user.auth_header\n    )\n    assert len(result) > 0\n\n\n@pytest.mark.integration\ndef test_populate_referenced_keys_recursively(test_config: FidesConfig) -> None:\n    \"\"\"\n    Test that populate_referenced_keys works recursively. It should be able to\n    find the keys in the declaration and also populate any keys which those reference.\n    For instance, a category would reference a new resource in its parent_key but it would\n    not be known until that category was populated first.\n    \"\"\"\n    result_taxonomy = evaluate.populate_referenced_keys(\n        taxonomy=Taxonomy(\n            system=[\n                System(\n                    fides_key=\"test_system\",\n                    system_type=\"test\",\n                    privacy_declarations=[\n                        PrivacyDeclaration(\n                            name=\"privacy_declaration_1\",\n                            data_categories=[\"user.contact.email\"],\n                            data_use=\"essential.service\",\n                            data_subjects=[\"customer\"],\n                        )\n                    ],\n                )\n            ],\n        ),\n        url=test_config.cli.server_url,\n        headers=test_config.user.auth_header,\n        last_keys=[],\n    )\n\n    populated_categories = [\n        category.fides_key for category in result_taxonomy.data_category\n    ]\n    assert sorted(populated_categories) == sorted(\n        [\"user.contact.email\", \"user.contact\", \"user\"]\n    )\n\n    populated_data_uses = [data_use.fides_key for data_use in result_taxonomy.data_use]\n    assert sorted(populated_data_uses) == sorted([\"essential.service\", \"essential\"])\n\n    populated_subjects = [\n        data_subject.fides_key for data_subject in result_taxonomy.data_subject\n    ]\n    assert sorted(populated_subjects) == sorted([\"customer\"])\n\n\n@pytest.mark.integration\ndef test_populate_referenced_keys_fails_missing_keys(\n    test_config: FidesConfig,\n) -> None:\n    \"\"\"\n    Test that populate_referenced_keys will fail if missing keys\n    are referenced in taxonomy\n    \"\"\"\n    with pytest.raises(SystemExit):\n        evaluate.populate_referenced_keys(\n            taxonomy=Taxonomy(\n                system=[\n                    System(\n                        fides_key=\"test_system\",\n                        system_type=\"test\",\n                        privacy_declarations=[\n                            PrivacyDeclaration(\n                                name=\"privacy_declaration_1\",\n                                data_categories=[\"missing.category\"],\n                                data_use=\"essential.service\",\n                                data_subjects=[\"customer\"],\n                            )\n                        ],\n                    )\n                ],\n            ),\n            url=test_config.cli.server_url,\n            headers=test_config.user.auth_header,\n            last_keys=[],\n        )\n\n\n@pytest.mark.integration\ndef test_hydrate_missing_resources(test_config: FidesConfig) -> None:\n    dehydrated_taxonomy = Taxonomy(\n        data_category=[\n            DataCategory(\n                name=\"test_dc\",\n                fides_key=\"key_1.test_dc\",\n                description=\"test description\",\n                parent_key=\"key_1\",\n            ),\n        ],\n        system=[\n            System.model_construct(\n                name=\"test_dc\",\n                fides_key=\"test_dc\",\n                description=\"test description\",\n                egress=[{\"fides_key\": \"key_3\"}, {\"fides_key\": \"key_4\"}],\n                system_type=\"test\",\n                privacy_declarations=None,\n            )\n        ],\n    )\n    actual_hydrated_taxonomy = evaluate.hydrate_missing_resources(\n        url=test_config.cli.server_url,\n        headers=test_config.user.auth_header,\n        dehydrated_taxonomy=dehydrated_taxonomy,\n        missing_resource_keys={\n            \"user.authorization.credentials\",\n            \"user\",\n        },\n    )\n    assert len(actual_hydrated_taxonomy.data_category) == 3\n\n\n@pytest.mark.unit\ndef test_get_evaluation_policies_with_key_found_local() -> None:\n    \"\"\"\n    Test that when a fides key is supplied the local policy is returned when found\n    \"\"\"\n    server_policy = Policy(fides_key=\"fides_key_1\", rules=[])\n    local_policy_1 = Policy(fides_key=\"fides_key_1\", rules=[])\n    local_policy_2 = Policy(fides_key=\"fides_key_2\", rules=[])\n    get_server_resource_mock = MagicMock(return_value=server_policy)\n    with patch(\"fides.core.evaluate.get_server_resource\", get_server_resource_mock):\n        policies = evaluate.get_evaluation_policies(\n            local_policies=[local_policy_1, local_policy_2],\n            evaluate_fides_key=\"fides_key_1\",\n            url=\"url\",\n            headers={},\n        )\n\n    assert len(policies) == 1\n    assert policies[0] is local_policy_1\n\n\n@pytest.mark.unit\ndef test_get_evaluation_policies_with_key_found_remote() -> None:\n    \"\"\"\n    Test that when a fides key is supplied and not found locally, it will be\n    fetched from the server\n    \"\"\"\n    server_policy = Policy(fides_key=\"fides_key_1\", rules=[])\n    local_policy = Policy(fides_key=\"fides_key_2\", rules=[])\n    get_server_resource_mock = MagicMock(return_value=server_policy)\n    with patch(\"fides.core.evaluate.get_server_resource\", get_server_resource_mock):\n        policies = evaluate.get_evaluation_policies(\n            local_policies=[local_policy],\n            evaluate_fides_key=\"fides_key_1\",\n            url=\"url\",\n            headers={},\n        )\n\n    assert len(policies) == 1\n    assert policies[0] is server_policy\n    get_server_resource_mock.assert_called_with(\n        url=\"url\", resource_type=\"policy\", resource_key=\"fides_key_1\", headers={}\n    )\n\n\n@pytest.mark.unit\ndef test_get_evaluation_policies_with_no_key(test_config: FidesConfig) -> None:\n    \"\"\"\n    Test that when no fides key is supplied all local and server policies are\n    returned.\n    \"\"\"\n    server_policy_1 = Policy(fides_key=\"fides_key_1\", rules=[])\n    server_policy_2 = Policy(fides_key=\"fides_key_2\", rules=[])\n    local_policy_1 = Policy(fides_key=\"fides_key_3\", rules=[])\n    local_policy_2 = Policy(fides_key=\"fides_key_4\", rules=[])\n    get_all_server_policies_mock = MagicMock(\n        return_value=[server_policy_1, server_policy_2]\n    )\n    with patch(\n        \"fides.core.evaluate.get_all_server_policies\",\n        get_all_server_policies_mock,\n    ):\n        policies = evaluate.get_evaluation_policies(\n            local_policies=[local_policy_1, local_policy_2],\n            evaluate_fides_key=\"\",\n            url=\"url\",\n            headers={},\n        )\n\n    assert len(policies) == 4\n    get_all_server_policies_mock.assert_called_with(\n        url=\"url\", headers={}, exclude=[\"fides_key_3\", \"fides_key_4\"]\n    )\n\n\n@pytest.mark.unit\ndef test_validate_policies_exist_throws_with_empty() -> None:\n    with pytest.raises(SystemExit):\n        evaluate.validate_policies_exist(policies=[], evaluate_fides_key=\"fides_key\")\n\n\n@pytest.mark.unit\ndef test_validate_policies_exist_with_policies() -> None:\n    evaluate.validate_policies_exist(\n        policies=[Policy(fides_key=\"fides_key_1\", rules=[])],\n        evaluate_fides_key=\"fides_key\",\n    )\n\n\n@pytest.mark.unit\ndef test_compare_rule_to_declaration_any_true() -> None:\n    result = evaluate.compare_rule_to_declaration(\n        rule_types=[\"key_1\"],\n        declaration_type_hierarchies=[[\"key_2\"], [\"key_1\"]],\n        rule_match=\"ANY\",\n    )\n    assert {\"key_1\"} == result\n\n\n@pytest.mark.unit\ndef test_compare_rule_to_declaration_any_true_hierarchical() -> None:\n    result = evaluate.compare_rule_to_declaration(\n        rule_types=[\"key_1_parent\"],\n        declaration_type_hierarchies=[[\"key_2\"], [\"key_1\", \"key_1_parent\"]],\n        rule_match=\"ANY\",\n    )\n    assert {\"key_1\"} == result\n\n\n@pytest.mark.unit\ndef test_compare_rule_to_declaration_any_false() -> None:\n    result = evaluate.compare_rule_to_declaration(\n        rule_types=[\"key_1\"],\n        declaration_type_hierarchies=[[\"key_2\"], [\"key_3\"]],\n        rule_match=\"ANY\",\n    )\n    assert not result\n\n\n@pytest.mark.unit\ndef test_compare_rule_to_declaration_any_false_hierarchical() -> None:\n    result = evaluate.compare_rule_to_declaration(\n        rule_types=[\"key_1\"],\n        declaration_type_hierarchies=[[\"key_2\", \"key_2_parent\"], [\"key_3\"]],\n        rule_match=\"ANY\",\n    )\n    assert not result\n\n\n@pytest.mark.unit\ndef test_compare_rule_to_declaration_all_true() -> None:\n    result = evaluate.compare_rule_to_declaration(\n        rule_types=[\"key_1\", \"key_3\"],\n        declaration_type_hierarchies=[[\"key_3\"], [\"key_1\"]],\n        rule_match=\"ALL\",\n    )\n    assert {\"key_3\", \"key_1\"} == result\n\n\n@pytest.mark.unit\ndef test_compare_rule_to_declaration_all_true_hierarchical() -> None:\n    result = evaluate.compare_rule_to_declaration(\n        rule_types=[\"key_1_parent\", \"key_3_parent\"],\n        declaration_type_hierarchies=[\n            [\"key_3\", \"key_3_parent\"],\n            [\"key_1\", \"key_1_parent\"],\n        ],\n        rule_match=\"ALL\",\n    )\n    assert {\"key_3\", \"key_1\"} == result\n\n\n@pytest.mark.unit\ndef test_compare_rule_to_declaration_all_false() -> None:\n    result = evaluate.compare_rule_to_declaration(\n        rule_types=[\"key_1\", \"key_3\"],\n        declaration_type_hierarchies=[[\"key_2\"], [\"key_1\"]],\n        rule_match=\"ALL\",\n    )\n    assert not result\n\n\n@pytest.mark.unit\ndef test_compare_rule_to_declaration_all_false_hierarchical() -> None:\n    result = evaluate.compare_rule_to_declaration(\n        rule_types=[\"key_1\", \"key_1_parent\", \"key_3\"],\n        declaration_type_hierarchies=[[\"key_2\"], [\"key_1\", \"key_1_parent\"]],\n        rule_match=\"ALL\",\n    )\n    assert not result\n\n\n@pytest.mark.unit\ndef test_compare_rule_to_declaration_none_true() -> None:\n    result = evaluate.compare_rule_to_declaration(\n        rule_types=[\"key_1\"],\n        declaration_type_hierarchies=[[\"key_2\"], [\"key_3\"]],\n        rule_match=\"NONE\",\n    )\n    assert {\"key_2\", \"key_3\"} == result\n\n\n@pytest.mark.unit\ndef test_compare_rule_to_declaration_none_true_hierarchical() -> None:\n    result = evaluate.compare_rule_to_declaration(\n        rule_types=[\"key_1\"],\n        declaration_type_hierarchies=[[\"key_2\", \"key_2_parent\"], [\"key_3\"]],\n        rule_match=\"NONE\",\n    )\n    assert {\"key_2\", \"key_3\"} == result\n\n\n@pytest.mark.unit\ndef test_compare_rule_to_declaration_none_false() -> None:\n    result = evaluate.compare_rule_to_declaration(\n        rule_types=[\"key_1\"],\n        declaration_type_hierarchies=[[\"key_2\"], [\"key_3\"], [\"key_1\"]],\n        rule_match=\"NONE\",\n    )\n    assert not result\n\n\n@pytest.mark.unit\ndef test_compare_rule_to_declaration_none_false_hierarchical() -> None:\n    result = evaluate.compare_rule_to_declaration(\n        rule_types=[\"key_1_parent\"],\n        declaration_type_hierarchies=[[\"key_2\"], [\"key_3\"], [\"key_1\", \"key_1_parent\"]],\n        rule_match=\"NONE\",\n    )\n    assert not result\n\n\n@pytest.mark.unit\ndef test_compare_rule_to_declaration_other_true() -> None:\n    result = evaluate.compare_rule_to_declaration(\n        rule_types=[\"key_1\"],\n        declaration_type_hierarchies=[[\"key_2\"], [\"key_1\"]],\n        rule_match=\"OTHER\",\n    )\n    assert {\"key_2\"} == result\n\n\n@pytest.mark.unit\ndef test_compare_rule_to_declaration_other_true_hierarchical() -> None:\n    result = evaluate.compare_rule_to_declaration(\n        rule_types=[\"key_1_parent\"],\n        declaration_type_hierarchies=[[\"key_2\"], [\"key_1\", \"key_1_parent\"]],\n        rule_match=\"OTHER\",\n    )\n    assert {\"key_2\"} == result\n\n\n@pytest.mark.unit\ndef test_compare_rule_to_declaration_other_false() -> None:\n    result = evaluate.compare_rule_to_declaration(\n        rule_types=[\"key_1\", \"key_3\"],\n        declaration_type_hierarchies=[[\"key_1\"], [\"key_3\"]],\n        rule_match=\"OTHER\",\n    )\n    assert not result\n\n\n@pytest.mark.unit\ndef test_compare_rule_to_declaration_other_false_hierarchical() -> None:\n    result = evaluate.compare_rule_to_declaration(\n        rule_types=[\"key_1\", \"key_3_parent\"],\n        declaration_type_hierarchies=[[\"key_1\"], [\"key_3\", \"key_3_parent\"]],\n        rule_match=\"OTHER\",\n    )\n    assert not result\n\n\n@pytest.mark.unit\ndef test_get_dataset_by_fides_key_exists() -> None:\n    dataset_1 = Dataset(\n        fides_key=\"dataset_1\", collections=[DatasetCollection(name=\"\", fields=[])]\n    )\n    dataset_2 = Dataset(\n        fides_key=\"dataset_2\", collections=[DatasetCollection(name=\"\", fields=[])]\n    )\n    result = evaluate.get_dataset_by_fides_key(\n        taxonomy=Taxonomy(dataset=[dataset_1, dataset_2]), fides_key=\"dataset_1\"\n    )\n    assert result == dataset_1\n\n\n@pytest.mark.unit\ndef test_get_dataset_by_fides_key_does_not_exist() -> None:\n    dataset1 = Dataset(\n        fides_key=\"dataset_1\", collections=[DatasetCollection(name=\"\", fields=[])]\n    )\n    dataset_2 = Dataset(\n        fides_key=\"dataset_2\", collections=[DatasetCollection(name=\"\", fields=[])]\n    )\n    result = evaluate.get_dataset_by_fides_key(\n        taxonomy=Taxonomy(dataset=[dataset1, dataset_2]), fides_key=\"dataset_3\"\n    )\n    assert not result\n\n\n@pytest.mark.unit\ndef test_get_fides_key_parent_hierarchy_child(\n    evaluation_hierarchical_key_basic_taxonomy: Taxonomy,\n) -> None:\n    result = evaluate.get_fides_key_parent_hierarchy(\n        taxonomy=evaluation_hierarchical_key_basic_taxonomy,\n        fides_key=\"data_category.parent.child\",\n    )\n    assert result == [\n        \"data_category.parent.child\",\n        \"data_category.parent\",\n        \"data_category\",\n    ]\n\n\n@pytest.mark.unit\ndef test_get_fides_key_parent_hierarchy_parent(\n    evaluation_hierarchical_key_basic_taxonomy: Taxonomy,\n) -> None:\n    result = evaluate.get_fides_key_parent_hierarchy(\n        taxonomy=evaluation_hierarchical_key_basic_taxonomy,\n        fides_key=\"data_category.parent\",\n    )\n    assert result == [\"data_category.parent\", \"data_category\"]\n\n\n@pytest.mark.unit\ndef test_get_fides_key_parent_hierarchy_top_level(\n    evaluation_hierarchical_key_basic_taxonomy: Taxonomy,\n) -> None:\n    result = evaluate.get_fides_key_parent_hierarchy(\n        taxonomy=evaluation_hierarchical_key_basic_taxonomy, fides_key=\"data_category\"\n    )\n    assert result == [\"data_category\"]\n\n\n@pytest.mark.unit\ndef test_get_fides_key_parent_hierarchy_missing_key(\n    evaluation_hierarchical_key_basic_taxonomy: Taxonomy,\n) -> None:\n    with pytest.raises(SystemExit):\n        evaluate.get_fides_key_parent_hierarchy(\n            taxonomy=evaluation_hierarchical_key_basic_taxonomy,\n            fides_key=\"data_category.invalid\",\n        )\n\n\n@pytest.mark.unit\ndef test_get_fides_key_parent_hierarchy_missing_parent() -> None:\n    with pytest.raises(SystemExit):\n        evaluate.get_fides_key_parent_hierarchy(\n            taxonomy=Taxonomy(\n                data_category=[\n                    DataCategory(\n                        fides_key=\"data_category.parent\",\n                        parent_key=\"data_category\",\n                    ),\n                ]\n            ),\n            fides_key=\"data_category.parent\",\n        )\n\n\n@pytest.mark.unit\ndef test_failed_evaluation_error_message(\n    test_config: FidesConfig, capsys: pytest.CaptureFixture\n) -> None:\n    \"\"\"\n    Check that the returned error message matches what is expected.\n\n    Due to fides_keys being randomized here, we want to check that\n    the violations specifically are in the output.\n    \"\"\"\n    string_cleaner = lambda x: x.replace(\"\\n\", \"\").replace(\"\\t\", \"\").replace(\" \", \"\")\n    expected_error_message = string_cleaner(\n        \"\"\"\n  'message': '',\n  'status': 'FAIL',\n  'violations': [ { 'detail': 'Declaration (Share Political Opinions) of '\n                              'system (customer_data_sharing_system) failed '\n                              'rule (reject_targeted_marketing) from policy '\n                              '(primary_privacy_policy). Violated usage of '\n                              'data categories '\n                              '(user.demographic.political_opinion) for data'\n                              'uses (marketing.advertising.third_party) and'\n                              'subjects (customer)',\n                    'violating_attributes': { 'data_categories': [ 'user.demographic.political_opinion'],\n                                              'data_subjects': ['customer'],\n                                              'data_uses': [ 'marketing.advertising.third_party']}}]}\n                                              \"\"\"\n    )\n    with pytest.raises(SystemExit):\n        evaluate.evaluate(\n            url=test_config.cli.server_url,\n            manifests_dir=\"tests/ctl/data/failing_declaration_taxonomy.yml\",\n            headers=test_config.user.auth_header,\n            local=True,\n        )\n    captured_out = string_cleaner(capsys.readouterr().out)\n    print(f\"Expected output:\\n{expected_error_message}\")\n    print(f\"Captured output:\\n{captured_out}\")\n    assert captured_out.endswith(expected_error_message)\n\n\n@pytest.mark.unit\nclass TestMergeTaxonomies:\n    def test_no_key_conflicts(self) -> None:\n        taxonomy_1 = Taxonomy(data_subject=[DataSubject(fides_key=\"foo\", name=\"bar\")])\n        taxonomy_2 = Taxonomy(data_subject=[DataSubject(fides_key=\"bar\", name=\"baz\")])\n        taxonomy_3 = Taxonomy(\n            data_subject=[\n                DataSubject(fides_key=\"foo\", name=\"bar\"),\n                DataSubject(fides_key=\"bar\", name=\"baz\"),\n            ]\n        )\n        assert evaluate.merge_taxonomies(taxonomy_1, taxonomy_2) == taxonomy_3\n\n    def test_key_conflicts(self) -> None:\n        taxonomy_1 = Taxonomy(data_subject=[DataSubject(fides_key=\"foo\", name=\"bar\")])\n        taxonomy_2 = Taxonomy(data_subject=[DataSubject(fides_key=\"foo\", name=\"baz\")])\n        taxonomy_3 = Taxonomy(\n            data_subject=[\n                DataSubject(fides_key=\"foo\", name=\"bar\"),\n            ]\n        )\n        assert evaluate.merge_taxonomies(taxonomy_1, taxonomy_2) == taxonomy_3\n\n    def test_no_same_resources(self) -> None:\n        taxonomy_1 = Taxonomy(data_subject=[DataSubject(fides_key=\"foo\", name=\"bar\")])\n        taxonomy_2 = Taxonomy(data_category=[DataCategory(fides_key=\"foo\", name=\"bar\")])\n        taxonomy_3 = Taxonomy(\n            data_subject=[\n                DataSubject(fides_key=\"foo\", name=\"bar\"),\n            ],\n            data_category=[\n                DataCategory(fides_key=\"foo\", name=\"bar\"),\n            ],\n        )\n        assert evaluate.merge_taxonomies(taxonomy_1, taxonomy_2) == taxonomy_3\n"}
{"type": "test_file", "path": "tests/ctl/core/test_filters.py", "content": "# pylint: disable=missing-docstring, redefined-outer-name\nfrom typing import Generator\n\nimport pytest\nfrom fideslang.models import (\n    Organization,\n    OrganizationMetadata,\n    ResourceFilter,\n    System,\n    SystemMetadata,\n)\n\nfrom fides.core import filters as _filters\n\n\n@pytest.fixture()\ndef filter_systems(\n    system_with_arn1: Generator, system_with_arn2: Generator\n) -> Generator:\n    systems = [system_with_arn1, system_with_arn2]\n    yield systems\n\n\n@pytest.fixture()\ndef system_with_arn1() -> Generator:\n    system = System(\n        fides_key=\"database-2\",\n        organization_fides_key=\"default_organization\",\n        name=\"database-2\",\n        fidesctl_meta=SystemMetadata(\n            resource_id=\"arn:aws:rds:us-east-1:910934740016:cluster:database-2\",\n        ),\n        system_type=\"rds_cluster\",\n        privacy_declarations=[],\n    )\n    yield system\n\n\n@pytest.fixture()\ndef system_with_arn2() -> Generator:\n    system = System(\n        fides_key=\"database-3\",\n        organization_fides_key=\"default_organization\",\n        name=\"database-3\",\n        fidesctl_meta=SystemMetadata(\n            resource_id=\"arn:aws:rds:us-east-1:910934740016:cluster:database-3\",\n        ),\n        system_type=\"rds_cluster\",\n        privacy_declarations=[],\n    )\n    yield system\n\n\n@pytest.fixture()\ndef system_with_no_arn() -> Generator:\n    system = System(\n        fides_key=\"database-2\",\n        organization_fides_key=\"default_organization\",\n        name=\"database-2\",\n        system_type=\"rds_cluster\",\n        privacy_declarations=[],\n    )\n    yield system\n\n\n@pytest.fixture()\ndef organization_with_filter() -> Generator:\n    system = Organization(\n        fides_key=\"organization_with_filter\",\n        name=\"organization_with_filter\",\n        fidesctl_meta=OrganizationMetadata(\n            resource_filters=[\n                ResourceFilter(\n                    type=\"ignore_resource_arn\",\n                    value=\"arn:aws:rds:us-east-1:910934740016:cluster:database-2\",\n                ),\n            ]\n        ),\n    )\n    yield system\n\n\n@pytest.fixture()\ndef organization_with_no_filter() -> Generator:\n    system = Organization(\n        fides_key=\"organization_with_no_filter\",\n        name=\"organization_with_no_filter\",\n    )\n    yield system\n\n\n@pytest.mark.unit\ndef test_get_system_arn(system_with_arn1: Generator) -> None:\n    actual_result = _filters.get_system_arn(system=system_with_arn1)\n    assert actual_result == \"arn:aws:rds:us-east-1:910934740016:cluster:database-2\"\n\n\n@pytest.mark.unit\ndef test_get_system_arn_no_arn(system_with_no_arn: Generator) -> None:\n    actual_result = _filters.get_system_arn(system=system_with_no_arn)\n    assert not actual_result\n\n\n@pytest.mark.unit\ndef test_is_arn_filter_match_exact_match() -> None:\n    arn = \"arn:aws:rds:us-east-1:910934740016:cluster:database-2\"\n    actual_result = _filters.is_arn_filter_match(arn=arn, filter_arn=arn)\n    assert actual_result\n\n\n@pytest.mark.unit\ndef test_is_arn_filter_match_wildcard_match() -> None:\n    arn = \"arn:aws:rds:us-east-1:910934740016:cluster:database-2\"\n    filter_arn = \"arn:aws:rds:us-east-1:910934740016:cluster:\"\n    actual_result = _filters.is_arn_filter_match(arn=arn, filter_arn=filter_arn)\n    assert actual_result\n\n\n@pytest.mark.unit\ndef test_is_arn_filter_match_mismatch() -> None:\n    arn = \"arn:aws:rds:us-east-1:910934740016:cluster:database-2\"\n    filter_arn = \"arn:aws:rds:us-east-1:12345678:cluster:database-2\"\n    actual_result = _filters.is_arn_filter_match(arn=arn, filter_arn=filter_arn)\n    assert not actual_result\n\n\n@pytest.mark.unit\ndef test_ignore_resource_arn(\n    filter_systems: Generator, system_with_arn2: Generator\n) -> None:\n    filter_value = \"arn:aws:rds:us-east-1:910934740016:cluster:database-2\"\n    actual_result = _filters.ignore_resource_arn(\n        systems=filter_systems, filter_value=filter_value\n    )\n    assert len(actual_result) == 1\n    assert actual_result[0] == system_with_arn2\n\n\n@pytest.mark.unit\ndef test_ignore_resource_arn_missing_arn(system_with_no_arn: Generator) -> None:\n    filter_value = \"arn:aws:rds:us-east-1:910934740016:cluster:database-2\"\n    actual_result = _filters.ignore_resource_arn(\n        systems=[system_with_no_arn], filter_value=filter_value\n    )\n    assert len(actual_result) == 1\n    assert actual_result[0] == system_with_no_arn\n\n\n@pytest.mark.unit\ndef test_filter_aws_systems_arn_filter(\n    filter_systems: Generator,\n    system_with_arn2: Generator,\n    organization_with_filter: Generator,\n) -> None:\n    actual_result = _filters.filter_aws_systems(\n        systems=filter_systems, organization=organization_with_filter\n    )\n    assert len(actual_result) == 1\n    assert actual_result[0] == system_with_arn2\n\n\n@pytest.mark.unit\ndef test_filter_aws_systems_no_filter(\n    filter_systems: Generator, organization_with_no_filter: Generator\n) -> None:\n    actual_result = _filters.filter_aws_systems(\n        systems=filter_systems, organization=organization_with_no_filter\n    )\n    assert actual_result == filter_systems\n"}
{"type": "test_file", "path": "tests/ctl/core/test_privacy_declaration.py", "content": "from typing import Dict, Set\n\nimport pytest\n\nfrom fides.api.models.sql_models import PrivacyDeclaration\nfrom fides.api.util.data_category import get_data_categories_map\n\n\nclass TestPrivacyDeclaration:\n\n    @pytest.fixture(scope=\"function\")\n    def data_categories_map(self, db) -> Dict[str, Set[str]]:\n        return get_data_categories_map(db)\n\n    def test_privacy_declaration_dataset_data_categories(\n        self,\n        privacy_declaration_with_multiple_dataset_references: PrivacyDeclaration,\n        data_categories_map,\n    ):\n        assert set(\n            privacy_declaration_with_multiple_dataset_references.dataset_data_categories(\n                data_categories_map\n            )\n        ) == {\n            \"user.behavior\",\n            \"user.name.first\",\n            \"user.unique_id\",\n            \"user.contact.address.street\",\n        }\n\n    def test_privacy_declaration_undeclared_data_categories(\n        self,\n        privacy_declaration_with_single_dataset_reference: PrivacyDeclaration,\n        data_categories_map,\n    ):\n        assert privacy_declaration_with_single_dataset_reference.undeclared_data_categories(\n            data_categories_map\n        ) == {\n            \"user.contact.email\"\n        }\n\n    def test_privacy_declaration_data_category_defined_on_sibling(\n        self,\n        db,\n        privacy_declaration_with_single_dataset_reference: PrivacyDeclaration,\n        data_categories_map,\n    ):\n        system = privacy_declaration_with_single_dataset_reference.system\n\n        # Create a new privacy declaration with the data category we're searching for\n        PrivacyDeclaration.create(\n            db=db,\n            data={\n                \"name\": \"Collect data for third party sharing\",\n                \"system_id\": system.id,\n                \"data_categories\": [\"user.contact.email\"],\n                \"data_use\": \"third_party_sharing\",\n                \"data_subjects\": [\"customer\"],\n                \"dataset_references\": None,\n                \"egress\": None,\n                \"ingress\": None,\n            },\n        )\n\n        db.refresh(system)\n\n        # Check that the original privacy declaration doesn't have any undeclared data categories\n        # because we also search sibling privacy declarations for the data category\n        assert (\n            privacy_declaration_with_single_dataset_reference.undeclared_data_categories(\n                data_categories_map\n            )\n            == set()\n        )\n"}
{"type": "test_file", "path": "tests/ctl/core/test_pull.py", "content": "import pytest\nfrom git.repo import Repo\n\nfrom fides.config import FidesConfig\nfrom fides.core.pull import pull_existing_resources\n\n\ndef git_reset(change_dir: str) -> None:\n    \"\"\"This fixture is used to reset the repo files to HEAD.\"\"\"\n\n    git_session = Repo().git()\n    git_session.checkout(\"HEAD\", change_dir)\n\n\n@pytest.mark.unit\ndef test_pull_existing_resources(test_config: FidesConfig) -> None:\n    \"\"\"Placeholder test.\"\"\"\n    test_dir = \".fides/\"\n    existing_keys = pull_existing_resources(\n        test_dir, test_config.cli.server_url, test_config.user.auth_header\n    )\n    git_reset(test_dir)\n    assert len(existing_keys) > 1\n"}
{"type": "test_file", "path": "tests/ctl/core/test_system.py", "content": "# pylint: disable=missing-docstring, redefined-outer-name\nimport os\nfrom typing import Generator, List\nfrom uuid import uuid4\n\nimport pytest\nfrom fideslang.models import PrivacyDeclaration as PrivacyDeclarationSchema\nfrom fideslang.models import System, SystemMetadata\nfrom py._path.local import LocalPath\nfrom sqlalchemy import delete\nfrom sqlalchemy.exc import InvalidRequestError\n\nfrom fides.api.db.system import create_system, upsert_cookies\nfrom fides.api.models.sql_models import Cookies, PrivacyDeclaration\nfrom fides.api.models.sql_models import System as sql_System\nfrom fides.api.util.data_category import get_data_categories_map\nfrom fides.config import FidesConfig\nfrom fides.connectors.models import OktaConfig\nfrom fides.core import api\nfrom fides.core import system as _system\n\n\ndef create_server_systems(test_config: FidesConfig, systems: List[System]) -> None:\n    for system in systems:\n        api.create(\n            url=test_config.cli.server_url,\n            resource_type=\"system\",\n            json_resource=system.json(exclude_none=True),\n            headers=test_config.user.auth_header,\n        )\n\n\ndef delete_server_systems(test_config: FidesConfig, systems: List[System]) -> None:\n    for system in systems:\n        api.delete(\n            url=test_config.cli.server_url,\n            resource_type=\"system\",\n            resource_id=system.fides_key,\n            headers=test_config.user.auth_header,\n        )\n\n\ndef test_get_system_data_uses(db, system) -> None:\n    assert sql_System.get_data_uses([system]) == {\"marketing\", \"marketing.advertising\"}\n\n    system.privacy_declarations[0].update(\n        db=db, data={\"data_use\": \"marketing.advertising.first_party\"}\n    )\n\n    assert sql_System.get_data_uses([system]) == {\n        \"marketing\",\n        \"marketing.advertising\",\n        \"marketing.advertising.first_party\",\n    }\n    assert sql_System.get_data_uses([system], include_parents=False) == {\n        \"marketing.advertising.first_party\"\n    }\n\n    system.privacy_declarations[0].delete(db)\n    db.refresh(system)\n    assert sql_System.get_data_uses([system]) == set()\n\n\ndef test_system_dataset_data_categories(\n    db,\n    system_with_a_single_dataset_reference: System,\n) -> None:\n    assert set(\n        system_with_a_single_dataset_reference.dataset_data_categories(\n            get_data_categories_map(db)\n        )\n    ) == {\n        \"user.behavior\",\n        \"user.contact.address.street\",\n        \"user.name.first\",\n        \"user.unique_id\",\n    }\n\n\ndef test_system_undeclared_data_categories(\n    db, system_with_undeclared_data_categories: System\n) -> None:\n    assert system_with_undeclared_data_categories.undeclared_data_categories(\n        get_data_categories_map(db)\n    ) == {\"user.contact.email\"}\n\n\n@pytest.fixture(scope=\"function\")\ndef create_test_server_systems(\n    test_config: FidesConfig, redshift_systems: List[System]\n) -> Generator:\n    systems = redshift_systems\n    delete_server_systems(test_config, systems)\n    create_server_systems(test_config, systems)\n    yield\n    delete_server_systems(test_config, systems)\n\n\n@pytest.fixture(scope=\"function\")\ndef create_external_server_systems(test_config: FidesConfig) -> Generator:\n    systems = (\n        _system.generate_redshift_systems(\n            organization_key=\"default_organization\",\n            aws_config={},\n        )\n        + _system.generate_rds_systems(\n            organization_key=\"default_organization\",\n            aws_config={},\n        )\n        + _system.generate_resource_tagging_systems(\n            organization_key=\"default_organization\",\n            aws_config={},\n        )\n    )\n    delete_server_systems(test_config, systems)\n    create_server_systems(test_config, systems)\n    yield\n    delete_server_systems(test_config, systems)\n\n\n@pytest.fixture()\ndef redshift_describe_clusters() -> Generator:\n    describe_clusters = {\n        \"Clusters\": [\n            {\n                \"ClusterIdentifier\": \"redshift-cluster-1\",\n                \"Endpoint\": {\n                    \"Address\": \"redshift-cluster-1.cue8hjdl1kb1.us-east-1.redshift.amazonaws.com\",\n                    \"Port\": 5439,\n                },\n                \"ClusterNamespaceArn\": \"arn:aws:redshift:us-east-1:469973866127:namespace:5eb1f195-7815-4c62-9140-e062dd98da83\",\n            },\n            {\n                \"ClusterIdentifier\": \"redshift-cluster-2\",\n                \"Endpoint\": {\n                    \"Address\": \"redshift-cluster-2.cue8hjdl1kb1.us-east-1.redshift.amazonaws.com\",\n                    \"Port\": 5439,\n                },\n                \"ClusterNamespaceArn\": \"arn:aws:redshift:us-east-1:469973866127:namespace:06ba7fe3-8cb3-4e1c-b2c6-cc2f2415a979\",\n            },\n        ]\n    }\n    yield describe_clusters\n\n\n@pytest.fixture()\ndef redshift_systems() -> Generator:\n    redshift_systems = [\n        System.model_construct(\n            fides_key=\"redshift-cluster-1\",\n            organization_fides_key=\"default_organization\",\n            name=\"redshift-cluster-1\",\n            description=\"Fides Generated Description for Redshift Cluster: redshift-cluster-1\",\n            fidesctl_meta=SystemMetadata(\n                endpoint_address=\"redshift-cluster-1.cue8hjdl1kb1.us-east-1.redshift.amazonaws.com\",\n                endpoint_port=\"5439\",\n                resource_id=\"arn:aws:redshift:us-east-1:469973866127:namespace:5eb1f195-7815-4c62-9140-e062dd98da83\",\n            ),\n            system_type=\"redshift_cluster\",\n            privacy_declarations=[],\n        ),\n        System.model_construct(\n            fides_key=\"redshift-cluster-2\",\n            organization_fides_key=\"default_organization\",\n            name=\"redshift-cluster-2\",\n            description=\"Fides Generated Description for Redshift Cluster: redshift-cluster-2\",\n            fidesctl_meta=SystemMetadata(\n                endpoint_address=\"redshift-cluster-2.cue8hjdl1kb1.us-east-1.redshift.amazonaws.com\",\n                endpoint_port=\"5439\",\n                resource_id=\"arn:aws:redshift:us-east-1:469973866127:namespace:06ba7fe3-8cb3-4e1c-b2c6-cc2f2415a979\",\n            ),\n            system_type=\"redshift_cluster\",\n            privacy_declarations=[],\n        ),\n    ]\n    yield redshift_systems\n\n\n@pytest.fixture()\ndef rds_systems() -> Generator:\n    rds_systems = [\n        System(\n            fides_key=\"database-2\",\n            organization_fides_key=\"default_organization\",\n            name=\"database-2\",\n            description=\"Fides Generated Description for RDS Cluster: database-2\",\n            fidesctl_meta=SystemMetadata(\n                endpoint_address=\"database-2.cluster-ckrdpkkb4ukm.us-east-1.rds.amazonaws.com\",\n                endpoint_port=\"3306\",\n                resource_id=\"arn:aws:rds:us-east-1:910934740016:cluster:database-2\",\n            ),\n            system_type=\"rds_cluster\",\n            privacy_declarations=[],\n        ),\n        System(\n            fides_key=\"database-1\",\n            organization_fides_key=\"default_organization\",\n            name=\"database-1\",\n            description=\"Fides Generated Description for RDS Instance: database-1\",\n            fidesctl_meta=SystemMetadata(\n                endpoint_address=\"database-1.ckrdpkkb4ukm.us-east-1.rds.amazonaws.com\",\n                endpoint_port=3306,  # This is converted to a string via model_config = ConfigDict(coerce_numbers_to_str=True)\n                resource_id=\"arn:aws:rds:us-east-1:910934740016:db:database-1\",\n            ),\n            system_type=\"rds_instance\",\n            privacy_declarations=[],\n        ),\n    ]\n    yield rds_systems\n\n\n@pytest.fixture()\ndef rds_describe_clusters() -> Generator:\n    describe_clusters = {\n        \"DBClusters\": [\n            {\n                \"DBClusterIdentifier\": \"database-2\",\n                \"Endpoint\": \"database-2.cluster-cjh1qplnnv3b.us-east-1.rds.amazonaws.com\",\n                \"Port\": 3306,\n                \"DBClusterArn\": \"arn:aws:rds:us-east-1:469973866127:cluster:database-2\",\n            },\n        ]\n    }\n    yield describe_clusters\n\n\n@pytest.fixture()\ndef rds_describe_instances() -> Generator:\n    describe_instances = {\n        \"DBInstances\": [\n            {\n                \"DBInstanceIdentifier\": \"database-1\",\n                \"Endpoint\": {\n                    \"Address\": \"database-1.cjh1qplnnv3b.us-east-1.rds.amazonaws.com\",\n                    \"Port\": 3306,\n                },\n                \"DBInstanceArn\": \"arn:aws:rds:us-east-1:469973866127:db:database-1\",\n            },\n        ]\n    }\n    yield describe_instances\n\n\n@pytest.mark.integration\ndef test_get_all_server_systems(\n    test_config: FidesConfig, create_test_server_systems: Generator\n) -> None:\n    actual_result = _system.get_all_server_systems(\n        url=test_config.cli.server_url,\n        headers=test_config.user.auth_header,\n        exclude_systems=[],\n    )\n    assert actual_result\n\n\nclass TestSystemAWS:\n    @pytest.mark.unit\n    def test_get_system_resource_ids(self, redshift_systems: List[System]) -> None:\n        expected_result = [\n            \"arn:aws:redshift:us-east-1:469973866127:namespace:5eb1f195-7815-4c62-9140-e062dd98da83\",\n            \"arn:aws:redshift:us-east-1:469973866127:namespace:06ba7fe3-8cb3-4e1c-b2c6-cc2f2415a979\",\n        ]\n        actual_result = _system.get_system_resource_ids(redshift_systems)\n        assert actual_result == expected_result\n\n    @pytest.mark.unit\n    def test_find_missing_systems(\n        self, redshift_systems: List[System], rds_systems: List[System]\n    ) -> None:\n        source_systems = rds_systems + redshift_systems\n        existing_systems = redshift_systems\n        actual_result = _system.find_missing_systems(\n            source_systems=source_systems, existing_systems=existing_systems\n        )\n        assert actual_result == rds_systems\n\n    @pytest.mark.external\n    def test_scan_system_aws_passes(\n        self, test_config: FidesConfig, create_external_server_systems: Generator\n    ) -> None:\n        _system.scan_system_aws(\n            coverage_threshold=100,\n            manifest_dir=\"\",\n            organization_key=\"default_organization\",\n            aws_config=None,\n            url=test_config.cli.server_url,\n            headers=test_config.user.auth_header,\n        )\n\n    @pytest.mark.external\n    def test_generate_system_aws(\n        self, tmpdir: LocalPath, test_config: FidesConfig\n    ) -> None:\n        actual_result = _system.generate_system_aws(\n            file_name=f\"{tmpdir}/test_file.yml\",\n            include_null=False,\n            organization_key=\"default_organization\",\n            aws_config=None,\n            url=test_config.cli.server_url,\n            headers=test_config.user.auth_header,\n        )\n        assert actual_result\n\n\nOKTA_ORG_URL = \"https://dev-78908748.okta.com\"\n\n\nclass TestSystemOkta:\n    @pytest.mark.external\n    def test_generate_system_okta(\n        self, tmpdir: LocalPath, test_config: FidesConfig\n    ) -> None:\n        actual_result = _system.generate_system_okta(\n            file_name=f\"{tmpdir}/test_file.yml\",\n            include_null=False,\n            organization_key=\"default_organization\",\n            okta_config=OktaConfig(\n                orgUrl=OKTA_ORG_URL,\n                token=os.environ[\"OKTA_CLIENT_TOKEN\"],\n            ),\n            url=test_config.cli.server_url,\n            headers=test_config.user.auth_header,\n        )\n        assert actual_result\n\n    @pytest.mark.external\n    def test_scan_system_okta_success(\n        self, tmpdir: LocalPath, test_config: FidesConfig\n    ) -> None:\n        file_name = f\"{tmpdir}/test_file.yml\"\n        _system.generate_system_okta(\n            file_name=file_name,\n            include_null=False,\n            organization_key=\"default_organization\",\n            okta_config=OktaConfig(\n                orgUrl=OKTA_ORG_URL,\n                token=os.environ[\"OKTA_CLIENT_TOKEN\"],\n            ),\n            url=test_config.cli.server_url,\n            headers=test_config.user.auth_header,\n        )\n        _system.scan_system_okta(\n            manifest_dir=file_name,\n            okta_config=OktaConfig(\n                orgUrl=OKTA_ORG_URL,\n                token=os.environ[\"OKTA_CLIENT_TOKEN\"],\n            ),\n            organization_key=\"default_organization\",\n            coverage_threshold=100,\n            url=test_config.cli.server_url,\n            headers=test_config.user.auth_header,\n        )\n        assert True\n\n    @pytest.mark.external\n    def test_scan_system_okta_fail(\n        self, tmpdir: LocalPath, test_config: FidesConfig\n    ) -> None:\n        with pytest.raises(SystemExit):\n            _system.scan_system_okta(\n                manifest_dir=\"\",\n                okta_config=OktaConfig(\n                    orgUrl=OKTA_ORG_URL,\n                    token=os.environ[\"OKTA_CLIENT_TOKEN\"],\n                ),\n                coverage_threshold=100,\n                organization_key=\"default_organization\",\n                url=test_config.cli.server_url,\n                headers=test_config.user.auth_header,\n            )\n\n\nclass TestPrivacyDeclarationPurpose:\n    def test_privacy_declaration_purpose(self):\n        pd = PrivacyDeclaration(\n            name=\"declaration-name\",\n            data_categories=[],\n            data_use=\"analytics.reporting.campaign_insights\",\n            data_subjects=[],\n            dataset_references=[],\n            ingress=None,\n            egress=None,\n        )\n\n        assert pd.purpose == 9\n\n    def test_privacy_declaration_special_purpose(self):\n        \"\"\"Special purposes are not returned under the purpose hybrid property\"\"\"\n        pd = PrivacyDeclaration(\n            name=\"declaration-name\",\n            data_categories=[],\n            data_use=\"essential.service.security\",\n            data_subjects=[],\n            dataset_references=[],\n            ingress=None,\n            egress=None,\n        )\n\n        assert pd.purpose is None\n\n    def test_privacy_declaration_non_tcf_data_use(self):\n        pd = PrivacyDeclaration(\n            name=\"declaration-name\",\n            data_categories=[],\n            data_use=\"essential\",\n            data_subjects=[],\n            dataset_references=[],\n            ingress=None,\n            egress=None,\n        )\n\n        assert pd.purpose is None\n\n\nclass TestUpsertCookies:\n    @pytest.fixture()\n    async def test_cookie_system(\n        self,\n        async_session_temp,\n        generate_auth_header,\n        test_config,\n        generate_role_header,\n        db,\n    ):\n        \"\"\"Existing system has no cookies at System level but one at Privacy Declaration level\"\"\"\n        resource = System(\n            fides_key=str(uuid4()),\n            organization_fides_key=\"default_organization\",\n            name=\"test_system_1\",\n            system_type=\"test\",\n            privacy_declarations=[\n                PrivacyDeclarationSchema(\n                    name=\"declaration-name\",\n                    data_categories=[],\n                    data_use=\"essential\",\n                    data_subjects=[],\n                    dataset_references=[],\n                ),\n                PrivacyDeclarationSchema(\n                    name=\"declaration-name-2\",\n                    data_categories=[],\n                    data_use=\"functional.service.improve\",\n                    data_subjects=[],\n                    dataset_references=[],\n                ),\n            ],\n        )\n\n        system = await create_system(resource, async_session_temp)\n\n        Cookies.create(\n            db=db,\n            data={\n                \"name\": \"strawberry\",\n                \"path\": \"/\",\n                \"privacy_declaration_id\": sorted(\n                    system.privacy_declarations, key=lambda x: x.name\n                )[1].id,\n            },\n            check_name=False,\n        )\n        await async_session_temp.refresh(system)\n        yield system\n        delete(sql_System).where(sql_System.id == system.id)\n\n    async def test_new_system_cookies(self, test_cookie_system, async_session_temp):\n        \"\"\"Test adding a cookie at the system level\"\"\"\n        new_cookies = [{\"name\": \"carrots\"}]\n\n        await upsert_cookies(\n            async_session_temp,\n            new_cookies,\n            privacy_declaration=None,\n            system=test_cookie_system,\n        )\n\n        await async_session_temp.refresh(test_cookie_system)\n        assert len(test_cookie_system.cookies) == 1\n\n        new_cookie = test_cookie_system.cookies[0]\n        assert new_cookie.created_at is not None\n        assert new_cookie.updated_at is not None\n        assert new_cookie.name == \"carrots\"\n        assert new_cookie.system_id == test_cookie_system.id\n        assert new_cookie.privacy_declaration_id is None\n\n    async def test_new_privacy_declaration_cookies(\n        self, test_cookie_system, async_session_temp\n    ):\n        \"\"\"Test adding a new cookie to a privacy declaration.  The other privacy declaration on the\n        system already has a cookie.\"\"\"\n\n        new_cookies = [{\"name\": \"apple\"}]\n        privacy_declaration = sorted(\n            test_cookie_system.privacy_declarations, key=lambda x: x.name\n        )[0]\n        other_decl = sorted(\n            test_cookie_system.privacy_declarations, key=lambda x: x.name\n        )[1]\n\n        await upsert_cookies(\n            async_session_temp,\n            new_cookies,\n            privacy_declaration,\n            system=None,\n        )\n        await async_session_temp.refresh(test_cookie_system)\n        assert len(test_cookie_system.cookies) == 0\n\n        assert len(privacy_declaration.cookies) == 1\n        assert privacy_declaration.cookies[0].name == \"apple\"\n\n        new_cookie = privacy_declaration.cookies[0]\n        assert new_cookie.created_at is not None\n        assert new_cookie.updated_at is not None\n        assert new_cookie.name == \"apple\"\n        assert new_cookie.system_id is None\n        assert new_cookie.privacy_declaration_id == privacy_declaration.id\n\n        existing_cookie = other_decl.cookies[0]\n        assert existing_cookie.created_at is not None\n        assert existing_cookie.updated_at is not None\n        assert existing_cookie.name == \"strawberry\"\n        assert new_cookie.system_id is None\n        assert new_cookie.privacy_declaration_id == privacy_declaration.id\n\n    async def test_no_change_to_privacy_declaration_cookies(\n        self, test_cookie_system, async_session_temp\n    ):\n        \"\"\"Test specified cookies already exist on given privacy declaration, so no change required\"\"\"\n        new_cookies = [{\"name\": \"strawberry\"}]\n        privacy_declaration = sorted(\n            test_cookie_system.privacy_declarations, key=lambda x: x.name\n        )[1]\n        existing_cookie = privacy_declaration.cookies[0]\n        assert existing_cookie.name == \"strawberry\"\n\n        await upsert_cookies(\n            async_session_temp,\n            new_cookies,\n            privacy_declaration,\n            system=None,\n        )\n        await async_session_temp.refresh(test_cookie_system)\n        assert len(test_cookie_system.cookies) == 0\n\n        assert len(privacy_declaration.cookies) == 1\n        assert privacy_declaration.cookies[0].name == \"strawberry\"\n\n        new_cookie = privacy_declaration.cookies[0]\n        assert new_cookie.created_at is not None\n        assert new_cookie.updated_at is not None\n        assert new_cookie.name == \"strawberry\"\n        assert new_cookie.system_id is None\n        assert new_cookie.privacy_declaration_id == privacy_declaration.id\n\n    async def test_update_and_delete_system_cookies(\n        self, test_cookie_system, async_session_temp\n    ):\n        \"\"\"Test updating and deleting cookies on System. Cookies are updated to match request, so this\n        involves adding, updating, and deleting Cookies to match.\"\"\"\n        new_cookies = [{\"name\": \"carrots\", \"path\": \"first_path/\"}]\n\n        # Let's add cookie first\n        await upsert_cookies(\n            async_session_temp,\n            new_cookies,\n            privacy_declaration=None,\n            system=test_cookie_system,\n        )\n\n        await async_session_temp.refresh(test_cookie_system)\n        assert len(test_cookie_system.cookies) == 1\n\n        new_cookie = test_cookie_system.cookies[0]\n        assert new_cookie.name == \"carrots\"\n        assert new_cookie.path == \"first_path/\"\n\n        # Let's update cookie now\n        await upsert_cookies(\n            async_session_temp,\n            [{\"name\": \"carrots\", \"path\": \"second_path/\"}],\n            privacy_declaration=None,\n            system=test_cookie_system,\n        )\n\n        await async_session_temp.refresh(test_cookie_system)\n        assert len(test_cookie_system.cookies) == 1\n\n        updated_cookie = test_cookie_system.cookies[0]\n        assert updated_cookie.name == \"carrots\"\n        assert updated_cookie.path == \"second_path/\"\n        assert new_cookie.id == updated_cookie.id  # They're the same resource\n\n        # Let's delete this cookie\n        await upsert_cookies(\n            async_session_temp,\n            [],\n            privacy_declaration=None,\n            system=test_cookie_system,\n        )\n\n        await async_session_temp.refresh(test_cookie_system)\n\n        with pytest.raises(InvalidRequestError):\n            # This cookie was deleted altogether\n            await async_session_temp.refresh(updated_cookie)\n\n        assert len(test_cookie_system.cookies) == 0\n\n    async def test_update_privacy_declaration_cookies(\n        self, test_cookie_system, async_session_temp\n    ):\n        \"\"\"Test cookie exists but path has changed\"\"\"\n\n        new_cookies = [{\"name\": \"strawberry\", \"path\": \"/\"}]\n        privacy_declaration = sorted(\n            test_cookie_system.privacy_declarations, key=lambda x: x.name\n        )[1]\n        existing_cookie = privacy_declaration.cookies[0]\n        assert existing_cookie.name == \"strawberry\"\n\n        await upsert_cookies(\n            async_session_temp,\n            new_cookies,\n            privacy_declaration,\n            system=None,\n        )\n        await async_session_temp.refresh(test_cookie_system)\n        assert len(test_cookie_system.cookies) == 0\n\n        assert len(privacy_declaration.cookies) == 1\n        updated_cookie = privacy_declaration.cookies[0]\n\n        assert updated_cookie.name == \"strawberry\"\n        assert updated_cookie.path == \"/\"\n        assert updated_cookie.created_at is not None\n        assert updated_cookie.updated_at is not None\n        assert updated_cookie.system_id is None\n        assert updated_cookie.privacy_declaration_id == privacy_declaration.id\n\n    async def test_remove_cookies_from_privacy_declaration(\n        self, test_cookie_system, async_session_temp\n    ):\n        \"\"\"Test cookie list is missing a cookie currently on the privacy declaration so add the new\n        cookie and we remove the existing one\"\"\"\n\n        new_cookies = [{\"name\": \"apple\"}]\n        privacy_declaration = sorted(\n            test_cookie_system.privacy_declarations, key=lambda x: x.name\n        )[1]\n        existing_cookie = privacy_declaration.cookies[0]\n        assert existing_cookie.name == \"strawberry\"\n\n        await upsert_cookies(\n            async_session_temp,\n            new_cookies,\n            privacy_declaration,\n            system=None,\n        )\n        await async_session_temp.refresh(test_cookie_system)\n        assert len(test_cookie_system.cookies) == 0\n\n        assert len(privacy_declaration.cookies) == 1\n        assert privacy_declaration.cookies[0].name == \"apple\"\n\n        new_cookie = privacy_declaration.cookies[0]\n        assert new_cookie.created_at is not None\n        assert new_cookie.updated_at is not None\n        assert new_cookie.name == \"apple\"\n        assert new_cookie.system_id is None\n        assert new_cookie.privacy_declaration_id == privacy_declaration.id\n\n    async def test_delete_privacy_declaration(\n        self, test_cookie_system, async_session_temp\n    ):\n        \"\"\"Test if a privacy declaration is deleted, its cookie is still linked to the system\"\"\"\n\n        privacy_declaration = sorted(\n            test_cookie_system.privacy_declarations, key=lambda x: x.name\n        )[1]\n        existing_cookie = privacy_declaration.cookies[0]\n\n        assert existing_cookie.privacy_declaration_id == privacy_declaration.id\n        assert existing_cookie.system_id is None\n\n        stmt = delete(PrivacyDeclaration).where(\n            PrivacyDeclaration.id == privacy_declaration.id\n        )\n        await async_session_temp.execute(stmt)\n\n        with pytest.raises(InvalidRequestError):\n            # This cookie was deleted altogether\n            await async_session_temp.refresh(existing_cookie)\n\n    async def test_either_system_or_privacy_declaration_required(\n        self, test_cookie_system, async_session_temp\n    ):\n        \"\"\"Test if a privacy declaration is deleted, its cookie is still linked to the system\"\"\"\n        with pytest.raises(Exception):\n            await upsert_cookies(\n                async_session_temp,\n                [{\"name\": \"cookie_name\"}],\n                privacy_declaration=None,\n                system=None,\n            )\n\n    async def test_only_system_or_declaration_not_both(\n        self, test_cookie_system, async_session_temp\n    ):\n        \"\"\"Test if a privacy declaration is deleted, its cookie is still linked to the system\"\"\"\n        with pytest.raises(Exception):\n            await upsert_cookies(\n                async_session_temp,\n                [{\"name\": \"cookie_name\"}],\n                privacy_declaration=test_cookie_system.privacy_declarations[0],\n                system=test_cookie_system,\n            )\n"}
{"type": "test_file", "path": "tests/ctl/core/test_system_compass_sync.py", "content": "from fides.api.models.system_compass_sync import SystemCompassSync\n\n\nclass TestSystemGVLSync:\n    def test_system_compass_sync(self, db, system):\n        start_sync = SystemCompassSync.start_system_sync(db)\n        assert start_sync.id\n        assert start_sync.created_at\n        assert start_sync.updated_at\n        assert start_sync.sync_started_at\n        assert not start_sync.sync_completed_at\n        assert not start_sync.updated_systems\n\n        start_sync.finish_system_sync(db, updated_systems=[system.id])\n\n        db.refresh(start_sync)\n\n        assert start_sync.sync_completed_at\n        assert start_sync.sync_completed_at > start_sync.sync_started_at\n        assert start_sync.updated_systems == [system.id]\n"}
{"type": "test_file", "path": "tests/ctl/core/test_system_history.py", "content": "from uuid import uuid4\n\nimport pytest\nfrom fideslang.models import DataFlow as DataFlowSchema\nfrom fideslang.models import PrivacyDeclaration as PrivacyDeclarationSchema\nfrom fideslang.models import System as SystemSchema\nfrom sqlalchemy import delete\n\nfrom fides.api.db.system import create_system, update_system\nfrom fides.api.models.sql_models import System\nfrom fides.api.models.system_history import SystemHistory\nfrom fides.config import get_config\n\nCONFIG = get_config()\n\n\nclass TestSystemHistory:\n    \"\"\"Verifies the correct number of system history entries are created per system update.\"\"\"\n\n    @pytest.fixture()\n    async def system(self, async_session_temp):\n        resource = SystemSchema(\n            fides_key=str(uuid4()),\n            organization_fides_key=\"default_organization\",\n            name=\"test_system_1\",\n            system_type=\"test\",\n            privacy_declarations=[],\n        )\n\n        system = await create_system(\n            resource, async_session_temp, CONFIG.security.oauth_root_client_id\n        )\n        yield system\n        delete(System).where(System.id == system.id)\n\n    async def test_system_information_changes(\n        self, db, async_session_temp, system: System\n    ):\n        system_schema = SystemSchema.model_validate(system)\n        system_schema.description = \"Test system\"\n        _, updated = await update_system(\n            system_schema, async_session_temp, CONFIG.security.oauth_root_client_id\n        )\n        assert updated\n\n        system_histories = SystemHistory.filter(\n            db=db, conditions=(SystemHistory.system_id == system.id)\n        ).all()\n        assert len(system_histories) == 1\n        assert system_histories[0].edited_by == CONFIG.security.root_username\n\n    async def test_privacy_declaration_changes(\n        self, db, async_session_temp, system: System\n    ):\n        system_schema = SystemSchema.model_validate(system)\n        system_schema.privacy_declarations = (\n            PrivacyDeclarationSchema(\n                name=\"declaration-name\",\n                data_categories=[],\n                data_use=\"essential\",\n                data_subjects=[],\n                dataset_references=[],\n            ),\n        )\n        _, updated = await update_system(\n            system_schema, async_session_temp, CONFIG.security.oauth_root_client_id\n        )\n        assert updated\n\n        system_histories = SystemHistory.filter(\n            db=db, conditions=(SystemHistory.system_id == system.id)\n        ).all()\n        assert len(system_histories) == 1\n        assert system_histories[0].edited_by == CONFIG.security.root_username\n\n    async def test_ingress_egress_changes(self, db, async_session_temp, system: System):\n        system_schema = SystemSchema.model_validate(system)\n        system_schema.ingress = [DataFlowSchema(fides_key=\"upstream\", type=\"system\")]\n        system_schema.egress = [DataFlowSchema(fides_key=\"user\", type=\"user\")]\n        _, updated = await update_system(\n            system_schema, async_session_temp, CONFIG.security.oauth_root_client_id\n        )\n        assert updated\n\n        system_histories = SystemHistory.filter(\n            db=db, conditions=(SystemHistory.system_id == system.id)\n        ).all()\n        assert len(system_histories) == 1\n\n    async def test_multiple_changes(self, db, async_session_temp, system: System):\n        system_schema = SystemSchema.model_validate(system)\n        system_schema.description = \"Test system\"\n        system_schema.privacy_declarations = (\n            PrivacyDeclarationSchema(\n                name=\"declaration-name\",\n                data_categories=[],\n                data_use=\"essential\",\n                data_subjects=[],\n                dataset_references=[],\n            ),\n        )\n        system_schema.ingress = [DataFlowSchema(fides_key=\"upstream\", type=\"system\")]\n        system_schema.egress = [DataFlowSchema(fides_key=\"user\", type=\"user\")]\n        _, updated = await update_system(\n            system_schema, async_session_temp, CONFIG.security.oauth_root_client_id\n        )\n        assert updated\n\n        system_histories = SystemHistory.filter(\n            db=db, conditions=(SystemHistory.system_id == system.id)\n        ).all()\n        assert len(system_histories) == 3\n        for system_history in system_histories:\n            assert system_history.edited_by == CONFIG.security.root_username\n\n    async def test_no_changes(self, db, async_session_temp, system: System):\n        system_schema = SystemSchema.model_validate(system)\n        _, updated = await update_system(\n            system_schema, async_session_temp, CONFIG.security.oauth_root_client_id\n        )\n        assert not updated\n\n        assert (\n            SystemHistory.filter(\n                db=db, conditions=(SystemHistory.system_id == system.id)\n            ).count()\n            == 0\n        )\n\n    async def test_automatic_system_update(\n        self, db, async_session_temp, system: System\n    ):\n        \"\"\"If user id doesn't map to a user in the db or the root user, we just return the original user string\"\"\"\n        system_schema = SystemSchema.model_validate(system)\n        system_schema.description = \"Test system\"\n        updated_system, updated = await update_system(\n            system_schema, async_session_temp, \"automatic_system_update\"\n        )\n        assert updated\n\n        system_histories = SystemHistory.filter(\n            db=db, conditions=(SystemHistory.system_id == system.id)\n        ).all()\n\n        assert system_histories[0].edited_by == \"automatic_system_update\"\n"}
{"type": "test_file", "path": "tests/ctl/core/test_taxonomy_models.py", "content": "from typing import Generator\n\nimport pytest\nfrom sqlalchemy.exc import IntegrityError\nfrom sqlalchemy.orm import Session\n\nfrom fides.api.models.sql_models import DataUse\n\n\nclass TestHierarchicalTaxonomy:\n\n    @pytest.fixture(scope=\"function\")\n    def child_data_use_b(self, db: Session) -> Generator:\n        payload_b = {\n            \"name\": \"Data Use B\",\n            \"fides_key\": \"data_use_a.data_use_b\",\n            \"parent_key\": \"data_use_a\",\n            \"active\": True,\n            \"is_default\": False,\n            \"description\": \"Data Use B\",\n        }\n        data_use_b = DataUse.create(db=db, data=payload_b)\n\n        yield data_use_b\n\n        data_use_b.delete(db)\n\n    @pytest.fixture(scope=\"function\")\n    def child_data_use_c(self, db: Session) -> Generator:\n        payload_c = {\n            \"name\": \"Data Use C\",\n            \"fides_key\": \"data_use_a.data_use_c\",\n            \"parent_key\": \"data_use_a\",\n            \"active\": True,\n            \"is_default\": False,\n            \"description\": \"Data Use C\",\n        }\n        data_use_c = DataUse.create(db=db, data=payload_c)\n\n        yield data_use_c\n\n        data_use_c.delete(db)\n\n    @pytest.fixture(scope=\"function\")\n    def parent_data_use_a(self, db) -> Generator:\n\n        payload_a = {\n            \"name\": \"Data Use A\",\n            \"fides_key\": \"data_use_a\",\n            \"active\": True,\n            \"is_default\": False,\n            \"description\": \"Data Use A\",\n        }\n\n        data_use_a = DataUse.create(db=db, data=payload_a)\n\n        yield data_use_a\n\n        data_use_a.delete(db)\n\n    def test_create_with_invalid_parent_key(self, db):\n        with pytest.raises(IntegrityError) as exc:\n            payload = {\n                \"name\": \"Data Use Testing\",\n                \"fides_key\": \"data_use_testing\",\n                \"parent_key\": \"invalid\",\n                \"active\": True,\n                \"is_default\": False,\n                \"description\": \"Data Use Test\",\n            }\n\n            DataUse.create(db=db, data=payload)\n            assert \"violates foreign key\" in str(exc)\n\n    def test_update_with_invalid_parent_key(\n        self, db, parent_data_use_a, child_data_use_b, child_data_use_c\n    ):\n        with pytest.raises(IntegrityError) as exc:\n            payload = {\n                \"name\": \"Data Use update\",\n                \"fides_key\": child_data_use_b.fides_key,\n                \"parent_key\": \"invalid\",\n                \"description\": \"Data Use Update\",\n            }\n\n            child_data_use_b.update(db=db, data=payload)\n        assert \"violates foreign key\" in str(exc)\n\n    def test_create_with_parent_key(\n        self, db, parent_data_use_a, child_data_use_b, child_data_use_c\n    ):\n        new_key = \"new_data_use_with_b_parent\"\n        payload = {\n            \"name\": \"New data use b parent\",\n            \"fides_key\": new_key,\n            \"parent_key\": child_data_use_b.fides_key,\n            \"active\": True,\n            \"is_default\": False,\n            \"description\": \"Something\",\n        }\n\n        data_use = DataUse.create(db=db, data=payload)\n        assert data_use.fides_key == new_key\n\n        # clean up\n        db.delete(data_use)\n        db.commit()\n\n    def test_update_with_parent_key(\n        self, db, parent_data_use_a, child_data_use_b, child_data_use_c\n    ):\n        \"\"\"\n        Tree: A----B\n               \\\n                ----C\n        \"\"\"\n        payload = {\n            \"name\": \"Data Use Test Update With Parent\",\n            \"fides_key\": child_data_use_b.fides_key,\n            \"parent_key\": child_data_use_b.parent_key,\n            \"active\": True,\n            \"is_default\": False,\n            \"description\": \"updating this\",\n        }\n\n        child_data_use_b.update(db=db, data=payload)\n        db.commit()\n        assert child_data_use_b.description == \"updating this\"\n\n    def test_update_no_parent_key(\n        self, db, parent_data_use_a, child_data_use_b, child_data_use_c\n    ):\n        \"\"\"\n        Tree: A----B\n               \\\n                ----C\n        \"\"\"\n        payload = {\n            \"name\": \"Data Use Test No Parent Key\",\n            \"fides_key\": child_data_use_b.fides_key,\n            \"active\": True,\n            \"is_default\": False,\n            \"description\": \"updating this\",\n        }\n\n        child_data_use_b.update(db=db, data=payload)\n        db.commit()\n        assert child_data_use_b.description == \"updating this\"\n\n    @pytest.mark.skip(\n        reason=\"We never update the parent key from the FE, but we should evaluate what we want to do here\"\n    )\n    def test_update_new_parent_key(\n        self, db, parent_data_use_a, child_data_use_b, child_data_use_c\n    ):\n        pass\n\n    def test_delete_child_data_use(self, db, parent_data_use_a, child_data_use_b):\n        \"\"\"\n        Tree: A----B\n               \\\n                ----C\n        \"\"\"\n        # Manually create data use c so that we can delete it safely outside the fixture\n        payload_c = {\n            \"name\": \"Data Use C\",\n            \"fides_key\": \"data_use_a.data_use_c\",\n            \"parent_key\": \"data_use_a\",\n            \"active\": True,\n            \"is_default\": False,\n            \"description\": \"Data Use C\",\n        }\n        child_data_use_c = DataUse.create(db=db, data=payload_c)\n        assert len(parent_data_use_a.children) == 2\n        assert child_data_use_b.parent.fides_key == parent_data_use_a.fides_key\n        assert child_data_use_c.parent.fides_key == parent_data_use_a.fides_key\n\n        child_data_use_c.delete(db)\n\n        # verify data use A is removed from the parent\n        db.refresh(parent_data_use_a)\n        assert len(parent_data_use_a.children) == 1\n        assert parent_data_use_a.children[0].fides_key == child_data_use_b.fides_key\n\n    def test_cannot_delete_parent_notice_with_children(\n        self, db, parent_data_use_a, child_data_use_c\n    ):\n        \"\"\"\n        Tree: A----C\n        \"\"\"\n        assert len(parent_data_use_a.children) == 1\n        assert child_data_use_c.parent.fides_key == parent_data_use_a.fides_key\n\n        with pytest.raises(IntegrityError) as exc:\n            parent_data_use_a.delete(db)\n            assert \"violates foreign key\" in str(exc)\n\n        assert DataUse.get_by(db, field=\"fides_key\", value=parent_data_use_a.fides_key)\n        assert DataUse.get_by(db, field=\"fides_key\", value=child_data_use_c.fides_key)\n"}
{"type": "test_file", "path": "tests/ctl/core/test_user.py", "content": "from os import environ\n\nimport pytest\n\nfrom fides.core.user import (\n    Credentials,\n    get_auth_header,\n    get_credentials_path,\n    read_credentials_file,\n)\n\n\n@pytest.mark.unit\nclass TestCredentials:\n    \"\"\"\n    Test the Credentials object.\n    \"\"\"\n\n    def test_valid_credentials(self):\n        credentials = Credentials(\n            username=\"test\",\n            password=\"password\",\n            user_id=\"some_id\",\n            access_token=\"some_token\",\n        )\n        assert credentials.username == \"test\"\n        assert credentials.user_id == \"some_id\"\n        assert credentials.access_token == \"some_token\"\n\n\ndef test_get_credentials_path() -> None:\n    \"\"\"Test that a custom path for the credentials file works as expected.\"\"\"\n    expected_path = \"test_credentials\"\n    environ[\"FIDES_CREDENTIALS_PATH\"] = expected_path\n    actual_path = get_credentials_path()\n    assert expected_path == actual_path\n\n\ndef test_read_credentials_file_not_found() -> None:\n    with pytest.raises(FileNotFoundError):\n        credentials = read_credentials_file(credentials_path=\"notarealfile\")\n        print(credentials)\n\n\ndef test_get_auth_header_file_not_found():\n    \"\"\"\n    Verify that a SystemExit is raised when a credentials file\n    can't be found.\n\n    Additionally, use the `verbose` flag to make sure that code\n    path is also covered.\n    \"\"\"\n    environ[\"FIDES_CREDENTIALS_PATH\"] = \"thisfiledoesnotexist\"\n    with pytest.raises(SystemExit):\n        header = get_auth_header(verbose=True)\n        print(header)\n"}
{"type": "test_file", "path": "tests/ctl/core/test_utils.py", "content": "# pylint: disable=missing-docstring, redefined-outer-name\nimport os\nfrom pathlib import PosixPath\nfrom typing import Generator\n\nimport pytest\nimport requests\nfrom fideslang.models import DatasetCollection, DatasetField\n\nfrom fides.common import utils as common_utils\nfrom fides.config import get_config\nfrom fides.core import utils as core_utils\n\n\n@pytest.fixture()\ndef test_nested_collection_fields() -> Generator:\n    nested_collection_fields = DatasetCollection(\n        name=\"test_collection\",\n        fields=[\n            DatasetField(\n                name=\"top_level_field_1\",\n            ),\n            DatasetField(\n                name=\"top_level_field_2\",\n                fields=[\n                    DatasetField(\n                        name=\"first_nested_level\",\n                        fields=[\n                            DatasetField(\n                                name=\"second_nested_level\",\n                                fields=[DatasetField(name=\"third_nested_level\")],\n                            )\n                        ],\n                    )\n                ],\n            ),\n        ],\n    )\n\n    yield nested_collection_fields\n\n\n@pytest.mark.unit\ndef test_get_db_engine(test_config_path) -> None:\n    conn_str = get_config(test_config_path).database.sync_database_uri\n    engine = core_utils.get_db_engine(conn_str)\n    assert str(engine.url) == conn_str\n\n\n@pytest.mark.unit\ndef test_nested_fields_unpacked(\n    test_nested_collection_fields: DatasetCollection,\n) -> None:\n    \"\"\"\n    Tests unpacking fields from a data collection results in the\n    correct number of fields being returned to be evaluated.\n    \"\"\"\n    collection = test_nested_collection_fields\n    collected_field_names = []\n    for field in core_utils.get_all_level_fields(\n        collection.model_dump(mode=\"json\")[\"fields\"]\n    ):\n        collected_field_names.append(field[\"name\"])\n    assert len(collected_field_names) == 5\n\n\n@pytest.mark.unit\ndef test_get_manifest_list(tmp_path: PosixPath) -> None:\n    \"\"\"Test that the correct number of yml files are returned.\"\"\"\n    test_dir = tmp_path / \"test\"\n    test_dir.mkdir()\n    test_files = [\"foo.yml\", \"foo.yaml\"]\n\n    for file in test_files:\n        test_file = test_dir / file\n        print(test_file)\n        test_file.write_text(\"content\")\n\n    manifest_list = core_utils.get_manifest_list(str(test_dir))\n    assert len(manifest_list) == 2\n\n\n@pytest.mark.parametrize(\n    \"fides_key, sanitized_fides_key\",\n    [(\"foo\", \"foo\"), (\"@foo#\", \"_foo_\"), (\":_foo)bar!123$\", \"__foo_bar_123_\")],\n)\ndef test_sanitize_fides_key(fides_key: str, sanitized_fides_key: str) -> None:\n    assert sanitized_fides_key == core_utils.sanitize_fides_key(fides_key)\n\n\n@pytest.mark.unit\n@pytest.mark.parametrize(\n    \"fides_key, sanitized_fides_key\",\n    [(\"foo\", \"foo\"), (\"@foo#\", \"_foo_\"), (\":_foo)bar!123$\", \"__foo_bar_123_\")],\n)\ndef test_check_fides_key(fides_key: str, sanitized_fides_key: str) -> None:\n    assert sanitized_fides_key == core_utils.check_fides_key(fides_key)\n\n\n@pytest.mark.unit\nclass TestGitIsDirty:\n    \"\"\"\n    These tests can't use the standard pytest tmpdir\n    because the files need to be within the git repo\n    to be properly tested.\n\n    They will therefore also break if the real dir\n    used for testing is deleted.\n    \"\"\"\n\n    def test_not_dirty(self) -> None:\n        assert not core_utils.git_is_dirty(\"tests/ctl/data/example_sql/\")\n\n    def test_new_file_is_dirty(self) -> None:\n        test_file = \"tests/ctl/data/example_sql/new_file.txt\"\n        with open(test_file, \"w\") as file:\n            file.write(\"test file\")\n        assert core_utils.git_is_dirty()\n        os.remove(test_file)\n\n\n@pytest.mark.unit\ndef test_repeatable_unique_key() -> None:\n    expected_unique_fides_key = \"test_dataset_87ccd73621\"\n    unique_fides_key = core_utils.generate_unique_fides_key(\n        \"test_dataset\", \"test_host\", \"test_name\"\n    )\n    assert unique_fides_key == expected_unique_fides_key\n\n\n@pytest.mark.integration\nclass TestCheckResponseAuth:\n    def test_check_response_auth_sys_exit(self) -> None:\n        \"\"\"\n        Verify that a SystemExit is raised when expected.\n\n        Note that this must be an endpoint that requires\n        authentication as it is looking for 401/403!\n        \"\"\"\n        response = requests.get(\"/api/v1/cryptography/encryption/key\")\n        with pytest.raises(SystemExit):\n            common_utils.check_response_auth(response)\n\n    def test_check_response_auth_ok(self) -> None:\n        \"\"\"Verify that a response object is returned if no auth errors.\"\"\"\n        response = requests.get(\"/health\")\n        assert common_utils.check_response_auth(response)\n"}
{"type": "test_file", "path": "tests/ctl/database/test_crud.py", "content": "from json import dumps\nfrom typing import Generator, List\nfrom uuid import uuid4\n\nimport pytest\nfrom fastapi import HTTPException\nfrom sqlalchemy.exc import SQLAlchemyError\nfrom sqlalchemy.ext.asyncio import AsyncSession\n\nfrom fides.api.db.crud import (\n    create_resource,\n    delete_resource,\n    get_custom_fields_filtered,\n    get_resource_with_custom_fields,\n    list_resource,\n)\nfrom fides.api.models import sql_models\nfrom fides.api.util.errors import QueryError\nfrom fides.config import FidesConfig\nfrom fides.core import api as _api\nfrom tests.ctl.types import FixtureRequest\n\n\n@pytest.fixture(name=\"created_resources\")\ndef fixture_created_resources(\n    test_config: FidesConfig, request: FixtureRequest\n) -> Generator:\n    \"\"\"\n    Fixture that creates and tears down a set of resources for each test run.\n    Only creates resources for a given type based on test parameter\n    \"\"\"\n    created_keys = []\n    resource_type = request.param\n    to_create = [\"foo\", \"foo.bar\", \"foo.bar.baz\", \"foo.boo\"]\n    for key in to_create:\n        parent_key = \".\".join(key.split(\".\")[:-1]) if \".\" in key else None\n\n        _api.create(\n            url=test_config.cli.server_url,\n            resource_type=resource_type,\n            json_resource=dumps({\"fides_key\": key, \"parent_key\": parent_key}),\n            headers=test_config.user.auth_header,\n        )\n        created_keys.append(key)\n\n    # Wait for test to finish before cleaning up resources\n    yield resource_type, created_keys\n\n    for created_key in created_keys:\n        _api.delete(\n            url=test_config.cli.server_url,\n            resource_type=resource_type,\n            resource_id=created_key,\n            headers=test_config.user.auth_header,\n        )\n\n\n@pytest.mark.integration\n@pytest.mark.parametrize(\n    \"created_resources\",\n    [\"data_category\", \"data_use\"],\n    indirect=[\"created_resources\"],\n)\nasync def test_cascade_delete_taxonomy_children(\n    created_resources: List, async_session: AsyncSession\n) -> None:\n    \"\"\"Deleting a parent taxonomy should delete all of its children too\"\"\"\n    resource_type, keys = created_resources\n    sql_model = sql_models.sql_model_map[resource_type]\n    await delete_resource(sql_model, keys[0], async_session)\n    resources = await list_resource(sql_model, async_session)\n    remaining_keys = {resource.fides_key for resource in resources}\n    assert len(set(keys).intersection(remaining_keys)) == 0\n\n\n@pytest.mark.integration\nasync def test_delete_fails_linked_resources(\n    linked_dataset,\n    async_session: AsyncSession,\n) -> None:\n    \"\"\"Deleting a resource should fail if a linked resource (without cascade delete relationship) still exists.\"\"\"\n    with pytest.raises(HTTPException) as e:\n        await delete_resource(\n            sql_models.Dataset, linked_dataset.fides_key, async_session\n        )\n\n    assert \"try deleting related resources first\" in str(e)\n\n\n@pytest.fixture(scope=\"function\")\ndef custom_field_definition_data_use(db):\n    custom_field_definition_data = {\n        \"name\": \"custom_field_def_string_data_use_1\",\n        \"field_type\": \"string[]\",\n        \"resource_type\": \"data_use\",\n        \"field_definition\": \"string\",\n    }\n    custom_field_definition = sql_models.CustomFieldDefinition.create(\n        db=db, data=custom_field_definition_data\n    )\n    yield custom_field_definition\n    db.delete(custom_field_definition)\n\n\n@pytest.fixture(scope=\"function\")\ndef custom_fields_data_use(\n    db,\n    custom_field_definition_data_use,\n):\n    field_1 = sql_models.CustomField.create(\n        db=db,\n        data={\n            \"resource_type\": custom_field_definition_data_use.resource_type,\n            \"resource_id\": \"advertising\",\n            \"custom_field_definition_id\": custom_field_definition_data_use.id,\n            \"value\": [\"Test value 1\"],\n        },\n    )\n\n    field_2 = sql_models.CustomField.create(\n        db=db,\n        data={\n            \"resource_type\": custom_field_definition_data_use.resource_type,\n            \"resource_id\": \"third_party_sharing\",\n            \"custom_field_definition_id\": custom_field_definition_data_use.id,\n            \"value\": [\"Test value 2\"],\n        },\n    )\n\n    yield field_1, field_2\n    db.delete(field_1)\n    db.delete(field_2)\n\n\n@pytest.fixture(scope=\"function\")\ndef custom_field_definition_system(db):\n    custom_field_definition_data = {\n        \"name\": \"custom_field_def_string_system_1\",\n        \"field_type\": \"string[]\",\n        \"resource_type\": \"system\",\n        \"field_definition\": \"string\",\n    }\n    custom_field_definition = sql_models.CustomFieldDefinition.create(\n        db=db, data=custom_field_definition_data\n    )\n    yield custom_field_definition\n    db.delete(custom_field_definition)\n\n\n@pytest.fixture(scope=\"function\")\ndef custom_field_definition_system_2(db):\n    custom_field_definition_data = {\n        \"name\": \"custom_field_def_string_system_2\",\n        \"field_type\": \"string\",\n        \"resource_type\": \"system\",\n        \"field_definition\": \"string\",\n    }\n    custom_field_definition = sql_models.CustomFieldDefinition.create(\n        db=db, data=custom_field_definition_data\n    )\n    yield custom_field_definition\n    db.delete(custom_field_definition)\n\n\n@pytest.fixture(scope=\"function\")\ndef custom_field_definition_system_disabled(db):\n    \"\"\"A disabled custom field on system, to ensure its data is filtered out properly\"\"\"\n    custom_field_definition_data = {\n        \"name\": \"disabled custom field\",\n        \"field_type\": \"string\",\n        \"resource_type\": \"system\",\n        \"field_definition\": \"string\",\n        \"active\": False,\n    }\n    custom_field_definition = sql_models.CustomFieldDefinition.create(\n        db=db, data=custom_field_definition_data\n    )\n    yield custom_field_definition\n    db.delete(custom_field_definition)\n\n\n@pytest.fixture(scope=\"function\")\ndef custom_fields_system(\n    db,\n    system,\n    system_third_party_sharing,\n    custom_field_definition_system,\n    custom_field_definition_system_2,\n    custom_field_definition_system_disabled,\n):\n    field_1 = sql_models.CustomField.create(\n        db=db,\n        data={\n            \"resource_type\": custom_field_definition_system.resource_type,\n            \"resource_id\": system.fides_key,\n            \"custom_field_definition_id\": custom_field_definition_system.id,\n            \"value\": [\"Test value 1\"],\n        },\n    )\n\n    field_2 = sql_models.CustomField.create(\n        db=db,\n        data={\n            \"resource_type\": custom_field_definition_system.resource_type,\n            \"resource_id\": system.fides_key,\n            \"custom_field_definition_id\": custom_field_definition_system.id,\n            \"value\": [\"Test value 2\"],\n        },\n    )\n\n    field_3 = sql_models.CustomField.create(\n        db=db,\n        data={\n            \"resource_type\": custom_field_definition_system_2.resource_type,\n            \"resource_id\": system.fides_key,\n            \"custom_field_definition_id\": custom_field_definition_system_2.id,\n            \"value\": [\"Test value 3\"],\n        },\n    )\n\n    field_4 = sql_models.CustomField.create(\n        db=db,\n        data={\n            \"resource_type\": custom_field_definition_system_2.resource_type,\n            \"resource_id\": system_third_party_sharing.fides_key,\n            \"custom_field_definition_id\": custom_field_definition_system_2.id,\n            \"value\": [\"Test value 4\"],\n        },\n    )\n    field_5 = sql_models.CustomField.create(\n        db=db,\n        data={\n            \"resource_type\": custom_field_definition_system_disabled.resource_type,\n            \"resource_id\": system_third_party_sharing.fides_key,\n            \"custom_field_definition_id\": custom_field_definition_system_disabled.id,\n            \"value\": [\"Disabled value, should be filtered out!\"],\n        },\n    )\n\n    yield (field_1, field_2, field_3, field_4, field_5)\n    db.delete(field_1)\n    db.delete(field_2)\n    db.delete(field_3)\n    db.delete(field_4)\n    db.delete(field_5)\n\n\nasync def test_get_custom_fields_filtered(\n    db,\n    async_session,\n    system,\n    system_third_party_sharing,\n    custom_fields_system,\n    custom_fields_data_use,\n):\n    # we should get all results because we've specified both systems\n    filtered_fields = await get_custom_fields_filtered(\n        async_session,\n        {\n            sql_models.ResourceTypes.system: [\n                system.fides_key,\n                system_third_party_sharing.fides_key,\n            ],\n            sql_models.ResourceTypes.data_use: [\n                custom_fields_data_use[0].resource_id,\n                custom_fields_data_use[1].resource_id,\n            ],\n        },\n    )\n    for field in custom_fields_system:\n        cfd = sql_models.CustomFieldDefinition.get_by_key_or_id(\n            db=db, data={\"id\": field.custom_field_definition_id}\n        )\n        if cfd.active:  # only active fields should be in our result\n            assert any(\n                [\n                    (\n                        field.resource_id == f[\"resource_id\"]\n                        and field.value == f[\"value\"]\n                        and cfd.name == f[\"name\"]\n                        and cfd.field_type == f[\"field_type\"]\n                    )\n                    for f in filtered_fields\n                ]\n            )\n        else:  # inactive fields should NOT be in our result\n            assert not any(\n                [\n                    (\n                        field.resource_id == f[\"resource_id\"]\n                        and field.value == f[\"value\"]\n                        and cfd.name == f[\"name\"]\n                        and cfd.field_type == f[\"field_type\"]\n                    )\n                    for f in filtered_fields\n                ]\n            )\n\n    for field in custom_fields_data_use:\n        cfd = sql_models.CustomFieldDefinition.get_by_key_or_id(\n            db=db, data={\"id\": field.custom_field_definition_id}\n        )\n        assert any(\n            [\n                (\n                    field.resource_id == f[\"resource_id\"]\n                    and field.value == f[\"value\"]\n                    and cfd.name == f[\"name\"]\n                    and cfd.field_type == f[\"field_type\"]\n                )\n                for f in filtered_fields\n            ]\n        )\n\n    # we should get only field 4 because we've specified only its system\n    filtered_fields = await get_custom_fields_filtered(\n        async_session,\n        {sql_models.ResourceTypes.system: [system_third_party_sharing.fides_key]},\n    )\n    assert len(filtered_fields) == 1\n    assert (\n        filtered_fields[0][\"resource_id\"] == custom_fields_system[3].resource_id\n        and filtered_fields[0][\"value\"] == custom_fields_system[3].value\n    )\n\n    # we should get only first 3 fields because we've specified only their system\n    filtered_fields = await get_custom_fields_filtered(\n        async_session, {sql_models.ResourceTypes.system: [system.fides_key]}\n    )\n    assert len(filtered_fields) == 3\n    for i in range(0, 3):\n        field = custom_fields_system[i]\n        cfd = sql_models.CustomFieldDefinition.get_by_key_or_id(\n            db=db, data={\"id\": field.custom_field_definition_id}\n        )\n        assert any(\n            [\n                (\n                    field.resource_id == f[\"resource_id\"]\n                    and field.value == f[\"value\"]\n                    and cfd.name == f[\"name\"]\n                    and cfd.field_type == f[\"field_type\"]\n                )\n                for f in filtered_fields\n            ]\n        )\n\n    # we should get only a single field for each type because of our filtering\n    filtered_fields = await get_custom_fields_filtered(\n        async_session,\n        {\n            sql_models.ResourceTypes.system: [\n                system_third_party_sharing.fides_key,\n            ],\n            sql_models.ResourceTypes.data_use: [\n                custom_fields_data_use[1].resource_id,\n            ],\n        },\n    )\n\n    assert len(filtered_fields) == 2\n    # only system field 4 because we've specified only its system\n    assert any(\n        (\n            custom_fields_system[3].resource_id == f[\"resource_id\"]\n            and custom_fields_system[3].value == f[\"value\"]\n            and cfd.field_type == f[\"field_type\"]\n        )\n        for f in filtered_fields\n    )\n    # only data_use field 2 because we've specified only its data_use\n    assert any(\n        (\n            custom_fields_data_use[1].resource_id == f[\"resource_id\"]\n            and custom_fields_data_use[1].value == f[\"value\"]\n        )\n        for f in filtered_fields\n    )\n\n\nasync def test_get_resource_with_custom_field(db, async_session_temp):\n    system_data = {\n        \"name\": \"my system\",\n        \"system_type\": \"test\",\n        \"fides_key\": str(uuid4()),\n    }\n\n    system = await create_resource(sql_models.System, system_data, async_session_temp)\n\n    custom_definition_data = {\n        \"name\": \"test1\",\n        \"description\": \"test\",\n        \"field_type\": \"string\",\n        \"resource_type\": \"system\",\n        \"field_definition\": \"string\",\n    }\n\n    custom_field_definition = sql_models.CustomFieldDefinition.create(\n        db=db, data=custom_definition_data\n    )\n\n    sql_models.CustomField.create(\n        db=db,\n        data={\n            \"resource_type\": custom_field_definition.resource_type,\n            \"resource_id\": system.fides_key,\n            \"custom_field_definition_id\": custom_field_definition.id,\n            \"value\": [\"Test value 1\"],\n        },\n    )\n\n    sql_models.CustomField.create(\n        db=db,\n        data={\n            \"resource_type\": custom_field_definition.resource_type,\n            \"resource_id\": system.fides_key,\n            \"custom_field_definition_id\": custom_field_definition.id,\n            \"value\": [\"Test value 2\"],\n        },\n    )\n\n    result = await get_resource_with_custom_fields(\n        sql_models.System, system.fides_key, async_session_temp\n    )\n\n    assert result[\"name\"] == system.name\n    assert custom_field_definition.name in result\n    assert result[custom_field_definition.name] == \"Test value 1, Test value 2\"\n\n\nasync def test_get_resource_with_custom_field_no_custom_field(async_session_temp):\n    system_data = {\n        \"name\": \"my system\",\n        \"system_type\": \"test\",\n        \"fides_key\": str(uuid4()),\n    }\n\n    system = await create_resource(sql_models.System, system_data, async_session_temp)\n    result = await get_resource_with_custom_fields(\n        sql_models.System, system.fides_key, async_session_temp\n    )\n\n    assert result[\"name\"] == system.name\n\n\nasync def test_get_resource_with_custom_field_error(async_session, monkeypatch):\n    async def mock_execute(*args, **kwargs):\n        raise SQLAlchemyError\n\n    monkeypatch.setattr(\"fides.api.db.crud.AsyncSession.execute\", mock_execute)\n    with pytest.raises(QueryError):\n        await get_resource_with_custom_fields(sql_models.System, \"ABC\", async_session)\n"}
{"type": "test_file", "path": "tests/ctl/core/test_api.py", "content": "# pylint: disable=missing-docstring, redefined-outer-name\n\"\"\"Integration tests for the API module.\"\"\"\nimport json\nimport typing\nfrom datetime import datetime, timedelta, timezone\nfrom json import loads\nfrom typing import Dict, List, Tuple\nfrom uuid import uuid4\n\nimport pytest\nimport requests\nfrom fideslang import DEFAULT_TAXONOMY, model_list, models, parse\nfrom fideslang.models import PrivacyDeclaration as PrivacyDeclarationSchema\nfrom fideslang.models import System as SystemSchema\nfrom pytest import MonkeyPatch\nfrom sqlalchemy import text\nfrom sqlalchemy.orm import Session\nfrom starlette.status import (\n    HTTP_200_OK,\n    HTTP_201_CREATED,\n    HTTP_400_BAD_REQUEST,\n    HTTP_401_UNAUTHORIZED,\n    HTTP_403_FORBIDDEN,\n    HTTP_404_NOT_FOUND,\n    HTTP_409_CONFLICT,\n    HTTP_422_UNPROCESSABLE_ENTITY,\n)\nfrom starlette.testclient import TestClient\n\nfrom fides.api.api.v1.endpoints import health\nfrom fides.api.db.crud import get_resource\nfrom fides.api.db.system import create_system\nfrom fides.api.models.connectionconfig import ConnectionConfig\nfrom fides.api.models.datasetconfig import DatasetConfig\nfrom fides.api.models.sql_models import DataCategory as DataCategoryModel\nfrom fides.api.models.sql_models import Dataset\nfrom fides.api.models.sql_models import DataSubject as DataSubjectModel\nfrom fides.api.models.sql_models import DataUse as DataUseModel\nfrom fides.api.models.sql_models import PrivacyDeclaration, System\nfrom fides.api.models.system_history import SystemHistory\nfrom fides.api.models.tcf_purpose_overrides import TCFPurposeOverride\nfrom fides.api.oauth.roles import OWNER, VIEWER\nfrom fides.api.schemas.system import PrivacyDeclarationResponse, SystemResponse\nfrom fides.api.schemas.taxonomy_extensions import DataCategory, DataSubject, DataUse\nfrom fides.api.util.endpoint_utils import API_PREFIX, CLI_SCOPE_PREFIX_MAPPING\nfrom fides.common.api.scope_registry import (\n    CREATE,\n    DELETE,\n    POLICY_CREATE_OR_UPDATE,\n    PRIVACY_REQUEST_CREATE,\n    PRIVACY_REQUEST_DELETE,\n    PRIVACY_REQUEST_READ,\n    READ,\n    SYSTEM_CREATE,\n    SYSTEM_DELETE,\n    SYSTEM_READ,\n    SYSTEM_UPDATE,\n    UPDATE,\n)\nfrom fides.common.api.v1.urn_registry import V1_URL_PREFIX\nfrom fides.config import FidesConfig, get_config\nfrom fides.core import api as _api\n\nCONFIG = get_config()\n\nTAXONOMY_ENDPOINTS = [\"data_category\", \"data_subject\", \"data_use\"]\nTAXONOMY_EXTENSIONS = {\n    \"data_category\": DataCategory,\n    \"data_subject\": DataSubject,\n    \"data_use\": DataUse,\n}\n\n\n# Helper Functions\ndef get_existing_key(test_config: FidesConfig, resource_type: str) -> int:\n    \"\"\"Get an ID that is known to exist.\"\"\"\n    return _api.ls(\n        test_config.cli.server_url, resource_type, test_config.user.auth_header\n    ).json()[-1][\"fides_key\"]\n\n\n@pytest.fixture(scope=\"function\", name=\"inactive_data_category\")\ndef fixture_inactive_data_category(db: Session) -> typing.Generator:\n    \"\"\"\n    Fixture that yields an inactive data category and then deletes it for each test run.\n    \"\"\"\n    fides_key = \"inactive_data_category\"\n    data_category = DataCategoryModel.create(\n        db=db,\n        data={\n            \"fides_key\": fides_key,\n            \"name\": \"Inactive Category\",\n            \"active\": False,\n        },\n    )\n\n    yield data_category\n\n    data_category.delete(db)\n\n\n@pytest.fixture(scope=\"function\", name=\"inactive_data_use\")\ndef fixture_inactive_data_use(db: Session) -> typing.Generator:\n    \"\"\"\n    Fixture that yields an inactive data use and then deletes it for each test run.\n    \"\"\"\n    fides_key = \"inactive_data_use\"\n    data_use = DataUseModel.create(\n        db=db,\n        data={\n            \"fides_key\": fides_key,\n            \"name\": \"Inactive Use\",\n            \"active\": False,\n        },\n    )\n\n    yield data_use\n\n    data_use.delete(db)\n\n\n@pytest.fixture(scope=\"function\", name=\"inactive_data_subject\")\ndef fixture_inactive_data_subject(db: Session) -> typing.Generator:\n    \"\"\"\n    Fixture that yields an inactive data subject and then deletes it for each test run.\n    \"\"\"\n    fides_key = \"inactive_data_subject\"\n    data_subject = DataSubjectModel.create(\n        db=db,\n        data={\n            \"fides_key\": fides_key,\n            \"name\": \"Inactive Subject\",\n            \"active\": False,\n        },\n    )\n\n    yield data_subject\n\n    data_subject.delete(db)\n\n\n# Unit Tests\n@pytest.mark.unit\ndef test_generate_resource_urls_no_id(test_config: FidesConfig) -> None:\n    \"\"\"\n    Test that the URL generator works as intended.\n    \"\"\"\n    server_url = test_config.cli.server_url\n    expected_url = f\"{server_url}{API_PREFIX}/test/\"\n    result_url = _api.generate_resource_url(url=server_url, resource_type=\"test\")\n    assert expected_url == result_url\n\n\n@pytest.mark.unit\ndef test_generate_resource_urls_with_id(test_config: FidesConfig) -> None:\n    \"\"\"\n    Test that the URL generator works as intended.\n    \"\"\"\n    server_url = test_config.cli.server_url\n    expected_url = f\"{server_url}{API_PREFIX}/test/1\"\n    result_url = _api.generate_resource_url(\n        url=server_url,\n        resource_type=\"test\",\n        resource_id=\"1\",\n    )\n    assert expected_url == result_url\n\n\n@pytest.mark.integration\nclass TestCrud:\n    @pytest.mark.parametrize(\"endpoint\", model_list)\n    def test_api_create(\n        self,\n        generate_auth_header,\n        test_config: FidesConfig,\n        resources_dict: Dict,\n        endpoint: str,\n    ) -> None:\n        manifest = resources_dict[endpoint]\n        print(manifest.json(exclude_none=True))\n        token_scopes: List[str] = [f\"{CLI_SCOPE_PREFIX_MAPPING[endpoint]}:{CREATE}\"]\n        auth_header = generate_auth_header(scopes=token_scopes)\n        result = _api.create(\n            url=test_config.cli.server_url,\n            resource_type=endpoint,\n            json_resource=manifest.json(exclude_none=True),\n            headers=auth_header,\n        )\n        print(result.text)\n        assert result.status_code == 201\n\n    @pytest.mark.parametrize(\"endpoint\", model_list)\n    def test_api_create_wrong_scope(\n        self,\n        generate_auth_header,\n        test_config: FidesConfig,\n        resources_dict: Dict,\n        endpoint: str,\n    ) -> None:\n        manifest = resources_dict[endpoint]\n        token_scopes: List[str] = [PRIVACY_REQUEST_CREATE]\n        auth_header = generate_auth_header(scopes=token_scopes)\n        result = _api.create(\n            url=test_config.cli.server_url,\n            resource_type=endpoint,\n            json_resource=manifest.json(exclude_none=True),\n            headers=auth_header,\n        )\n        assert result.status_code == 403\n\n    async def test_create_dataset_data_categories_validated(\n        self, test_config: FidesConfig, resources_dict: Dict\n    ):\n        endpoint = \"dataset\"\n        manifest: Dataset = resources_dict[endpoint]\n        manifest.collections[0].data_categories = [\"bad_category\"]\n\n        result = _api.create(\n            url=test_config.cli.server_url,\n            headers=test_config.user.auth_header,\n            json_resource=manifest.json(exclude_none=True),\n            resource_type=endpoint,\n        )\n        assert result.status_code == 422\n        assert (\n            result.json()[\"detail\"][0][\"msg\"]\n            == \"Value error, The data category bad_category is not supported.\"\n        )\n\n    @pytest.mark.parametrize(\"endpoint\", model_list)\n    def test_api_ls(\n        self, test_config: FidesConfig, endpoint: str, generate_auth_header\n    ) -> None:\n        token_scopes: List[str] = [f\"{CLI_SCOPE_PREFIX_MAPPING[endpoint]}:{READ}\"]\n        auth_header = generate_auth_header(scopes=token_scopes)\n\n        result = _api.ls(\n            url=test_config.cli.server_url,\n            resource_type=endpoint,\n            headers=auth_header,\n        )\n        print(result.text)\n        assert result.status_code == 200\n\n    @pytest.mark.parametrize(\"endpoint\", model_list)\n    def test_api_ls_wrong_scope(\n        self, test_config: FidesConfig, endpoint: str, generate_auth_header\n    ) -> None:\n        token_scopes: List[str] = [PRIVACY_REQUEST_READ]\n        auth_header = generate_auth_header(scopes=token_scopes)\n\n        result = _api.ls(\n            url=test_config.cli.server_url,\n            resource_type=endpoint,\n            headers=auth_header,\n        )\n        assert result.status_code == 403\n\n    @pytest.mark.parametrize(\"endpoint\", model_list)\n    def test_api_get(\n        self, test_config: FidesConfig, endpoint: str, generate_auth_header\n    ) -> None:\n        token_scopes: List[str] = [f\"{CLI_SCOPE_PREFIX_MAPPING[endpoint]}:{READ}\"]\n        auth_header = generate_auth_header(scopes=token_scopes)\n\n        existing_id = get_existing_key(test_config, endpoint)\n        result = _api.get(\n            url=test_config.cli.server_url,\n            headers=auth_header,\n            resource_type=endpoint,\n            resource_id=existing_id,\n        )\n        print(result.text)\n        assert result.status_code == 200\n\n    @pytest.mark.parametrize(\"endpoint\", model_list)\n    def test_api_get_wrong_scope(\n        self, test_config: FidesConfig, endpoint: str, generate_auth_header\n    ) -> None:\n        token_scopes: List[str] = [PRIVACY_REQUEST_READ]\n        auth_header = generate_auth_header(scopes=token_scopes)\n\n        existing_id = get_existing_key(test_config, endpoint)\n        result = _api.get(\n            url=test_config.cli.server_url,\n            headers=auth_header,\n            resource_type=endpoint,\n            resource_id=existing_id,\n        )\n        assert result.status_code == 403\n\n    @pytest.mark.parametrize(\"endpoint\", model_list)\n    def test_sent_is_received(\n        self, test_config: FidesConfig, resources_dict: Dict, endpoint: str\n    ) -> None:\n        \"\"\"\n        Confirm that the resource and values that we send are the\n        same as the resource that the server returns.\n        \"\"\"\n        manifest = resources_dict[endpoint]\n        resource_key = manifest.fides_key if endpoint != \"user\" else manifest.userName\n\n        print(manifest.json(exclude_none=True))\n        result = _api.get(\n            url=test_config.cli.server_url,\n            headers=test_config.user.auth_header,\n            resource_type=endpoint,\n            resource_id=resource_key,\n        )\n        print(result.text)\n        assert result.status_code == 200\n        parsed_result = parse.parse_dict(endpoint, result.json())\n\n        assert parsed_result == manifest\n\n    @pytest.mark.parametrize(\"endpoint\", model_list)\n    def test_api_update(\n        self,\n        test_config: FidesConfig,\n        resources_dict: Dict,\n        endpoint: str,\n        generate_auth_header,\n    ) -> None:\n        token_scopes: List[str] = [f\"{CLI_SCOPE_PREFIX_MAPPING[endpoint]}:{UPDATE}\"]\n        auth_header = generate_auth_header(scopes=token_scopes)\n        manifest = resources_dict[endpoint]\n        result = _api.update(\n            url=test_config.cli.server_url,\n            headers=auth_header,\n            resource_type=endpoint,\n            json_resource=manifest.json(exclude_none=True),\n        )\n        print(result.text)\n        assert result.status_code == 200\n\n    @pytest.mark.parametrize(\"endpoint\", model_list)\n    def test_api_update_wrong_scope(\n        self,\n        test_config: FidesConfig,\n        resources_dict: Dict,\n        endpoint: str,\n        generate_auth_header,\n    ) -> None:\n        token_scopes: List[str] = [POLICY_CREATE_OR_UPDATE]\n        auth_header = generate_auth_header(scopes=token_scopes)\n        manifest = resources_dict[endpoint]\n        result = _api.update(\n            url=test_config.cli.server_url,\n            headers=auth_header,\n            resource_type=endpoint,\n            json_resource=manifest.json(exclude_none=True),\n        )\n        assert result.status_code == 403\n\n    async def test_update_dataset_data_categories_validated(\n        self, test_config: FidesConfig, resources_dict: Dict\n    ):\n        endpoint = \"dataset\"\n        manifest: Dataset = resources_dict[endpoint]\n        manifest.collections[0].data_categories = [\"bad_category\"]\n\n        result = _api.update(\n            url=test_config.cli.server_url,\n            headers=test_config.user.auth_header,\n            resource_type=endpoint,\n            json_resource=manifest.json(exclude_none=True),\n        )\n        assert result.status_code == 422\n        assert (\n            result.json()[\"detail\"][0][\"msg\"]\n            == \"Value error, The data category bad_category is not supported.\"\n        )\n\n    @pytest.mark.parametrize(\"endpoint\", model_list)\n    def test_api_upsert(\n        self,\n        test_config: FidesConfig,\n        resources_dict: Dict,\n        endpoint: str,\n        generate_auth_header,\n    ) -> None:\n        token_scopes: List[str] = [\n            f\"{CLI_SCOPE_PREFIX_MAPPING[endpoint]}:{UPDATE}\",\n            f\"{CLI_SCOPE_PREFIX_MAPPING[endpoint]}:{CREATE}\",\n        ]\n        auth_header = generate_auth_header(scopes=token_scopes)\n\n        manifest = resources_dict[endpoint]\n        result = _api.upsert(\n            url=test_config.cli.server_url,\n            headers=auth_header,\n            resource_type=endpoint,\n            resources=[loads(manifest.json())],\n        )\n        assert result.status_code == 200\n\n    @pytest.mark.parametrize(\"endpoint\", model_list)\n    def test_api_upsert_wrong_scope(\n        self,\n        test_config: FidesConfig,\n        resources_dict: Dict,\n        endpoint: str,\n        generate_auth_header,\n    ) -> None:\n        token_scopes: List[str] = [\n            f\"{CLI_SCOPE_PREFIX_MAPPING[endpoint]}:{CREATE}\",\n        ]  # Needs both create AND update\n        auth_header = generate_auth_header(scopes=token_scopes)\n\n        manifest = resources_dict[endpoint]\n        result = _api.upsert(\n            url=test_config.cli.server_url,\n            headers=auth_header,\n            resource_type=endpoint,\n            resources=[loads(manifest.json())],\n        )\n        assert result.status_code == 403\n\n    async def test_upsert_validates_resources_against_pydantic_model(\n        self, test_config: FidesConfig, resources_dict: Dict, async_session\n    ):\n        endpoint = \"dataset\"\n        manifest: Dataset = resources_dict[endpoint]\n        dict_manifest = manifest.model_dump(mode=\"json\")\n        del dict_manifest[\"organization_fides_key\"]\n\n        result = _api.upsert(\n            url=test_config.cli.server_url,\n            headers=test_config.user.auth_header,\n            resource_type=endpoint,\n            resources=[dict_manifest],\n        )\n        assert result.status_code == 200\n\n        resource = await get_resource(Dataset, manifest.fides_key, async_session)\n        assert resource.organization_fides_key == \"default_organization\"\n\n    async def test_upsert_dataset_data_categories_validated(\n        self, test_config: FidesConfig, resources_dict: Dict\n    ):\n        endpoint = \"dataset\"\n        manifest: Dataset = resources_dict[endpoint]\n        dict_manifest = manifest.model_dump(mode=\"json\")\n        dict_manifest[\"collections\"][0][\"data_categories\"] = [\"bad_category\"]\n\n        result = _api.upsert(\n            url=test_config.cli.server_url,\n            headers=test_config.user.auth_header,\n            resource_type=endpoint,\n            resources=[dict_manifest],\n        )\n        assert result.status_code == 422\n        assert (\n            result.json()[\"detail\"][0][\"msg\"]\n            == \"Value error, The data category bad_category is not supported.\"\n        )\n\n    @pytest.mark.parametrize(\"endpoint\", model_list)\n    def test_api_delete_wrong_scope(\n        self,\n        test_config: FidesConfig,\n        resources_dict: Dict,\n        endpoint: str,\n        generate_auth_header,\n    ) -> None:\n        token_scopes: List[str] = [PRIVACY_REQUEST_DELETE]\n        auth_header = generate_auth_header(scopes=token_scopes)\n\n        manifest = resources_dict[endpoint]\n        resource_key = manifest.fides_key if endpoint != \"user\" else manifest.userName\n\n        result = _api.delete(\n            url=test_config.cli.server_url,\n            resource_type=endpoint,\n            resource_id=resource_key,\n            headers=auth_header,\n        )\n        assert result.status_code == 403\n\n    @pytest.mark.parametrize(\"endpoint\", model_list)\n    def test_api_delete(\n        self,\n        test_config: FidesConfig,\n        resources_dict: Dict,\n        endpoint: str,\n        generate_auth_header,\n    ) -> None:\n        token_scopes: List[str] = [f\"{CLI_SCOPE_PREFIX_MAPPING[endpoint]}:{DELETE}\"]\n        auth_header = generate_auth_header(scopes=token_scopes)\n\n        manifest = resources_dict[endpoint]\n        resource_key = manifest.fides_key if endpoint != \"user\" else manifest.userName\n\n        result = _api.delete(\n            url=test_config.cli.server_url,\n            resource_type=endpoint,\n            resource_id=resource_key,\n            headers=auth_header,\n        )\n        print(result.text)\n        assert result.status_code == 200\n        resp = result.json()\n        assert resp[\"message\"] == \"resource deleted\"\n        assert resp[\"resource\"][\"fides_key\"] == manifest.fides_key\n\n\n@pytest.mark.unit\nclass TestSystemCreate:\n    @pytest.fixture(scope=\"function\", autouse=True)\n    def remove_all_systems(self, db) -> None:\n        \"\"\"Remove any systems (and privacy declarations) before test execution for clean state\"\"\"\n        for privacy_declaration in PrivacyDeclaration.all(db):\n            privacy_declaration.delete(db)\n        for system in System.all(db):\n            system.delete(db)\n\n    @pytest.fixture(scope=\"function\")\n    def system_create_request_body(self) -> SystemSchema:\n        return SystemSchema(\n            organization_fides_key=\"1\",\n            fides_key=\"system_fides_key\",\n            system_type=\"SYSTEM\",\n            name=\"Test System\",\n            description=\"A Test System\",\n            vendor_id=\"test_vendor\",\n            dataset_references=[\"another_system_reference\"],\n            processes_personal_data=True,\n            exempt_from_privacy_regulations=False,\n            reason_for_exemption=None,\n            uses_profiling=True,\n            legal_basis_for_profiling=[\"Authorised by law\", \"Contract\"],\n            does_international_transfers=True,\n            legal_basis_for_transfers=[\"Adequacy Decision\", \"BCRs\"],\n            requires_data_protection_assessments=True,\n            dpa_location=\"https://www.example.com/dpa\",\n            dpa_progress=\"pending\",\n            privacy_policy=\"https://www.example.com/privacy_policy\",\n            legal_name=\"Sunshine Corporation\",\n            legal_address=\"35925 Test Lane, Test Town, TX 24924\",\n            responsibility=[\"Processor\"],\n            dpo=\"John Doe, CIPT\",\n            joint_controller_info=\"Jane Doe\",\n            data_security_practices=\"We encrypt all your data in transit and at rest\",\n            cookie_max_age_seconds=\"31536000\",\n            uses_cookies=True,\n            cookie_refresh=True,\n            uses_non_cookie_access=True,\n            legitimate_interest_disclosure_url=\"http://www.example.com/legitimate_interest_disclosure\",\n            meta={},\n            privacy_declarations=[\n                models.PrivacyDeclaration(\n                    name=\"declaration-name\",\n                    data_categories=[],\n                    data_use=\"essential\",\n                    data_subjects=[],\n                    dataset_references=[\"another_system_reference\"],\n                    features=[\"Link different devices\"],\n                    legal_basis_for_processing=\"Public interest\",\n                    impact_assessment_location=\"https://www.example.com/impact_assessment_location\",\n                    retention_period=\"3-5 years\",\n                    processes_special_category_data=True,\n                    special_category_legal_basis=\"Reasons of substantial public interest (with a basis in law)\",\n                    data_shared_with_third_parties=True,\n                    third_parties=\"Third Party Marketing Dept.\",\n                    shared_categories=[\"user\"],\n                    cookies=[\n                        {\n                            \"name\": \"essential_cookie\",\n                            \"path\": \"/\",\n                            \"domain\": \"example.com\",\n                        }\n                    ],\n                ),\n                models.PrivacyDeclaration(\n                    name=\"declaration-name-2\",\n                    data_categories=[],\n                    data_use=\"marketing.advertising\",\n                    data_subjects=[],\n                    dataset_references=[],\n                ),\n            ],\n        )\n\n    def test_system_create_not_authenticated(\n        self,\n        test_config,\n        system_create_request_body,\n        db,\n    ):\n        result = _api.create(\n            url=test_config.cli.server_url,\n            headers={},\n            resource_type=\"system\",\n            json_resource=system_create_request_body.json(exclude_none=True),\n        )\n\n        assert result.status_code == HTTP_401_UNAUTHORIZED\n        assert not System.all(db)  # ensure our system wasn't created\n\n    def test_system_create_no_direct_scope(\n        self,\n        test_config,\n        generate_auth_header,\n        system_create_request_body,\n        db,\n    ):\n        auth_header = generate_auth_header(scopes=[POLICY_CREATE_OR_UPDATE])\n\n        result = _api.create(\n            url=test_config.cli.server_url,\n            headers=auth_header,\n            resource_type=\"system\",\n            json_resource=system_create_request_body.json(exclude_none=True),\n        )\n        assert result.status_code == HTTP_403_FORBIDDEN\n\n        assert not System.all(db)  # ensure our system wasn't created\n\n    def test_system_create_no_encompassing_role(\n        self,\n        test_config,\n        system_create_request_body,\n        db,\n        generate_role_header,\n    ):\n        auth_header = generate_role_header(roles=[VIEWER])\n        result = _api.create(\n            url=test_config.cli.server_url,\n            headers=auth_header,\n            resource_type=\"system\",\n            json_resource=system_create_request_body.json(exclude_none=True),\n        )\n        assert result.status_code == HTTP_403_FORBIDDEN\n\n        assert not System.all(db)  # ensure our system wasn't created\n\n    def test_system_create_system_already_exists(\n        self,\n        test_config,\n        system,\n        system_create_request_body,\n        db,\n        generate_auth_header,\n    ):\n        system_create_request_body.fides_key = system.fides_key\n        auth_header = generate_auth_header(scopes=[SYSTEM_CREATE])\n        result = _api.create(\n            url=test_config.cli.server_url,\n            headers=auth_header,\n            resource_type=\"system\",\n            json_resource=system_create_request_body.json(exclude_none=True),\n        )\n        assert result.status_code == HTTP_409_CONFLICT\n\n        assert (\n            len(System.all(db)) == 1\n        )  # ensure our system wasn't created, still only one system\n\n    def test_system_create_invalid_data_category(\n        self, test_config, db, generate_auth_header\n    ):\n        auth_header = generate_auth_header(scopes=[SYSTEM_CREATE])\n\n        result = _api.create(\n            url=test_config.cli.server_url,\n            headers=auth_header,\n            resource_type=\"system\",\n            json_resource=json.dumps(\n                {\n                    \"fides_key\": \"system_key\",\n                    \"system_type\": \"system\",\n                    \"privacy_declarations\": [\n                        {\n                            \"fides_key\": \"test\",\n                            \"data_categories\": [\n                                \"user.name.first\",\n                                \"user.name.nickname\",\n                                \"user.name.last\",\n                            ],  # Nickname does not exist\n                            \"data_use\": \"marketing\",\n                        }\n                    ],\n                }\n            ),\n        )\n\n        assert result.status_code == HTTP_400_BAD_REQUEST\n        assert result.json() == {\n            \"detail\": \"Invalid privacy declaration referencing unknown DataCategory user.name.nickname\"\n        }\n\n        assert not System.all(db)  # ensure our system wasn't created\n\n    def test_system_create_invalid_data_subject(\n        self, test_config, db, generate_auth_header\n    ):\n        auth_header = generate_auth_header(scopes=[SYSTEM_CREATE])\n\n        result = _api.create(\n            url=test_config.cli.server_url,\n            headers=auth_header,\n            resource_type=\"system\",\n            json_resource=json.dumps(\n                {\n                    \"fides_key\": \"system_key\",\n                    \"system_type\": \"system\",\n                    \"privacy_declarations\": [\n                        {\n                            \"fides_key\": \"test\",\n                            \"data_categories\": [\"user.name.first\", \"user.name.last\"],\n                            \"data_use\": \"marketing\",\n                            \"data_subjects\": [\n                                \"employee\",\n                                \"intern\",\n                            ],  # Intern does not exist\n                        }\n                    ],\n                }\n            ),\n        )\n\n        assert result.status_code == HTTP_400_BAD_REQUEST\n        assert result.json() == {\n            \"detail\": \"Invalid privacy declaration referencing unknown DataSubject intern\"\n        }\n\n        assert not System.all(db)  # ensure our system wasn't created\n\n    def test_system_create_invalid_data_use(\n        self, test_config, db, generate_auth_header\n    ):\n        auth_header = generate_auth_header(scopes=[SYSTEM_CREATE])\n\n        result = _api.create(\n            url=test_config.cli.server_url,\n            headers=auth_header,\n            resource_type=\"system\",\n            json_resource=json.dumps(\n                {\n                    \"fides_key\": \"system_key\",\n                    \"system_type\": \"system\",\n                    \"privacy_declarations\": [\n                        {\n                            \"fides_key\": \"test\",\n                            \"data_categories\": [\"user.name.first\", \"user.name.last\"],\n                            \"data_use\": \"marketing.measure\",  # Invalid data use\n                        }\n                    ],\n                }\n            ),\n        )\n\n        assert result.status_code == HTTP_400_BAD_REQUEST\n        assert result.json() == {\n            \"detail\": \"Invalid privacy declaration referencing unknown DataUse marketing.measure\"\n        }\n\n        assert not System.all(db)  # ensure our system wasn't created\n\n    def test_system_create_inactive_data_category(\n        self, test_config, inactive_data_category, db, generate_auth_header\n    ):\n        auth_header = generate_auth_header(scopes=[SYSTEM_CREATE])\n\n        result = _api.create(\n            url=test_config.cli.server_url,\n            headers=auth_header,\n            resource_type=\"system\",\n            json_resource=json.dumps(\n                {\n                    \"fides_key\": \"system_key\",\n                    \"system_type\": \"system\",\n                    \"privacy_declarations\": [\n                        {\n                            \"fides_key\": \"test\",\n                            \"data_categories\": [\n                                \"user.name.first\",\n                                \"user.name.last\",\n                                inactive_data_category.fides_key,\n                            ],\n                            \"data_use\": \"marketing\",\n                        }\n                    ],\n                }\n            ),\n        )\n\n        assert result.status_code == HTTP_400_BAD_REQUEST\n        assert result.json() == {\n            \"detail\": f\"Invalid privacy declaration referencing inactive DataCategory {inactive_data_category.fides_key}\"\n        }\n\n        assert not System.all(db)  # ensure our system wasn't created\n\n    def test_system_create_inactive_data_use(\n        self, test_config, inactive_data_use, db, generate_auth_header\n    ):\n        auth_header = generate_auth_header(scopes=[SYSTEM_CREATE])\n\n        result = _api.create(\n            url=test_config.cli.server_url,\n            headers=auth_header,\n            resource_type=\"system\",\n            json_resource=json.dumps(\n                {\n                    \"fides_key\": \"system_key\",\n                    \"system_type\": \"system\",\n                    \"privacy_declarations\": [\n                        {\n                            \"fides_key\": \"test\",\n                            \"data_categories\": [\n                                \"user.name.first\",\n                                \"user.name.last\",\n                            ],\n                            \"data_use\": inactive_data_use.fides_key,\n                        }\n                    ],\n                }\n            ),\n        )\n\n        assert result.status_code == HTTP_400_BAD_REQUEST\n        assert result.json() == {\n            \"detail\": f\"Invalid privacy declaration referencing inactive DataUse {inactive_data_use.fides_key}\"\n        }\n\n        assert not System.all(db)  # ensure our system wasn't created\n\n    def test_system_create_inactive_data_subject(\n        self, test_config, inactive_data_subject, db, generate_auth_header\n    ):\n        auth_header = generate_auth_header(scopes=[SYSTEM_CREATE])\n\n        result = _api.create(\n            url=test_config.cli.server_url,\n            headers=auth_header,\n            resource_type=\"system\",\n            json_resource=json.dumps(\n                {\n                    \"fides_key\": \"system_key\",\n                    \"system_type\": \"system\",\n                    \"privacy_declarations\": [\n                        {\n                            \"fides_key\": \"test\",\n                            \"data_categories\": [\n                                \"user.name.first\",\n                                \"user.name.last\",\n                            ],\n                            \"data_use\": \"marketing\",\n                            \"data_subjects\": [inactive_data_subject.fides_key],\n                        }\n                    ],\n                }\n            ),\n        )\n\n        assert result.status_code == HTTP_400_BAD_REQUEST\n        assert result.json() == {\n            \"detail\": f\"Invalid privacy declaration referencing inactive DataSubject {inactive_data_subject.fides_key}\"\n        }\n\n        assert not System.all(db)  # ensure our system wasn't created\n\n    async def test_system_create(\n        self, generate_auth_header, db, test_config, system_create_request_body\n    ):\n        \"\"\"Ensure system create works for base case, which includes 2 privacy declarations\"\"\"\n        auth_header = generate_auth_header(scopes=[SYSTEM_CREATE])\n\n        result = _api.create(\n            url=test_config.cli.server_url,\n            headers=auth_header,\n            resource_type=\"system\",\n            json_resource=system_create_request_body.json(exclude_none=True),\n        )\n\n        assert result.status_code == HTTP_201_CREATED\n        json_results = result.json()\n        assert json_results[\"cookies\"] == []  # No cookies at System level\n        assert json_results[\"privacy_declarations\"][0][\"cookies\"] == [\n            {\"name\": \"essential_cookie\", \"path\": \"/\", \"domain\": \"example.com\"}\n        ]\n        assert json_results[\"privacy_declarations\"][1][\"cookies\"] == []\n        assert json_results[\"data_stewards\"] == []\n\n        systems = System.all(db)\n        assert len(systems) == 1\n        system = systems[0]\n\n        for field in SystemResponse.model_fields:\n            system_val = getattr(system, field)\n            if isinstance(system_val, typing.Hashable) and not isinstance(\n                system_val, datetime\n            ):\n                assert system_val == json_results[field]\n        assert len(json_results[\"privacy_declarations\"]) == 2\n        assert json_results[\"created_at\"]\n\n        for i, decl in enumerate(system.privacy_declarations):\n            for field in PrivacyDeclarationResponse.model_fields:\n                decl_val = getattr(decl, field)\n                if isinstance(decl_val, typing.Hashable):\n                    assert decl_val == json_results[\"privacy_declarations\"][i][field]\n\n        assert len(system.privacy_declarations) == 2\n\n        assert system.name == \"Test System\"\n        assert system.vendor_id == \"test_vendor\"\n        assert system.dataset_references == [\"another_system_reference\"]\n        assert system.processes_personal_data is True\n        assert system.exempt_from_privacy_regulations is False\n        assert system.reason_for_exemption is None\n        assert system.uses_profiling is True\n        assert system.legal_basis_for_profiling == [\"Authorised by law\", \"Contract\"]\n        assert system.does_international_transfers is True\n        assert system.legal_basis_for_transfers == [\"Adequacy Decision\", \"BCRs\"]\n        assert system.requires_data_protection_assessments is True\n        assert system.dpa_location == \"https://www.example.com/dpa\"\n        assert system.dpa_progress == \"pending\"\n        assert system.privacy_policy == \"https://www.example.com/privacy_policy\"\n        assert system.legal_name == \"Sunshine Corporation\"\n        assert system.legal_address == \"35925 Test Lane, Test Town, TX 24924\"\n        assert system.responsibility == [\"Processor\"]\n        assert system.dpo == \"John Doe, CIPT\"\n        assert system.joint_controller_info == \"Jane Doe\"\n        assert (\n            system.data_security_practices\n            == \"We encrypt all your data in transit and at rest\"\n        )\n        assert system.cookie_max_age_seconds == 31536000\n        assert system.uses_cookies is True\n        assert system.cookie_refresh is True\n        assert system.uses_non_cookie_access is True\n        assert (\n            system.legitimate_interest_disclosure_url\n            == \"http://www.example.com/legitimate_interest_disclosure\"\n        )\n        assert system.data_stewards == []\n        assert [cookie.name for cookie in systems[0].cookies] == []\n        assert [\n            cookie.name for cookie in systems[0].privacy_declarations[0].cookies\n        ] == [\"essential_cookie\"]\n        assert systems[0].privacy_declarations[1].cookies == []\n\n        privacy_decl = system.privacy_declarations[0]\n        assert privacy_decl.name == \"declaration-name\"\n        assert privacy_decl.dataset_references == [\"another_system_reference\"]\n        assert privacy_decl.features == [\"Link different devices\"]\n        assert privacy_decl.legal_basis_for_processing == \"Public interest\"\n        assert (\n            await privacy_decl.get_purpose_legal_basis_override() == \"Public interest\"\n        )\n        assert (\n            privacy_decl.impact_assessment_location\n            == \"https://www.example.com/impact_assessment_location\"\n        )\n        assert privacy_decl.retention_period == \"3-5 years\"\n        assert privacy_decl.processes_special_category_data is True\n        assert (\n            privacy_decl.special_category_legal_basis\n            == \"Reasons of substantial public interest (with a basis in law)\"\n        )\n        assert privacy_decl.data_shared_with_third_parties is True\n        assert privacy_decl.third_parties == \"Third Party Marketing Dept.\"\n        assert privacy_decl.shared_categories == [\"user\"]\n        assert privacy_decl.flexible_legal_basis_for_processing is True\n\n    async def test_system_create_minimal_request_body(\n        self, generate_auth_header, db, test_config, system_create_request_body\n    ):\n        \"\"\"Assert system default fields are what is expected when a very minimal system request is sent\"\"\"\n        auth_header = generate_auth_header(scopes=[SYSTEM_CREATE])\n\n        result = _api.create(\n            url=test_config.cli.server_url,\n            headers=auth_header,\n            resource_type=\"system\",\n            json_resource=json.dumps(\n                {\n                    \"fides_key\": \"system_key\",\n                    \"system_type\": \"system\",\n                    \"privacy_declarations\": [\n                        {\n                            \"fides_key\": \"test\",\n                            \"data_categories\": [\"user\"],\n                            \"data_use\": \"marketing\",\n                        }\n                    ],\n                }\n            ),\n        )\n        assert result.status_code == HTTP_201_CREATED\n        systems = System.all(db)\n        assert len(systems) == 1\n        system = systems[0]\n\n        expected_none = [\n            \"connection_configs\",\n            \"data_security_practices\",\n            \"description\",\n            \"dpa_location\",\n            \"dpa_progress\",\n            \"dpo\",\n            \"egress\",\n            \"fidesctl_meta\",\n            \"ingress\",\n            \"joint_controller_info\",\n            \"legal_address\",\n            \"legal_name\",\n            \"meta\",\n            \"name\",\n            \"privacy_policy\",\n            \"reason_for_exemption\",\n            \"tags\",\n            \"vendor_id\",\n        ]\n        for field in expected_none:\n            assert getattr(system, field) is None\n\n        assert system.processes_personal_data is True\n\n        expected_false = [\n            \"does_international_transfers\",\n            \"exempt_from_privacy_regulations\",\n            \"requires_data_protection_assessments\",\n            \"uses_profiling\",\n        ]\n\n        for field in expected_false:\n            assert getattr(system, field) is False\n\n        expected_empty_list = [\n            \"cookies\",\n            \"dataset_references\",\n            \"data_stewards\",\n            \"legal_basis_for_profiling\",\n            \"legal_basis_for_transfers\",\n            \"responsibility\",\n        ]\n        for field in expected_empty_list:\n            assert getattr(system, field) == []\n\n        privacy_declaration = system.privacy_declarations[0]\n\n        expected_none_privacy_declaration_fields = [\n            \"dataset_references\",\n            \"egress\",\n            \"impact_assessment_location\",\n            \"ingress\",\n            \"legal_basis_for_processing\",\n            \"name\",\n            \"retention_period\",\n            \"special_category_legal_basis\",\n            \"third_parties\",\n        ]\n        for field in expected_none_privacy_declaration_fields:\n            assert getattr(privacy_declaration, field) is None\n\n        expected_false_pd_fields = [\n            \"data_shared_with_third_parties\",\n            \"processes_special_category_data\",\n        ]\n        for field in expected_false_pd_fields:\n            assert getattr(privacy_declaration, field) is False\n\n        expected_empty_list_pd_fields = [\n            \"cookies\",\n            \"data_subjects\",\n            \"features\",\n            \"shared_categories\",\n        ]\n        for field in expected_empty_list_pd_fields:\n            assert getattr(privacy_declaration, field) == []\n\n        assert privacy_declaration.system_id == system.id\n        assert privacy_declaration.data_use == \"marketing\"\n        assert privacy_declaration.data_categories == [\"user\"]\n\n    async def test_system_create_custom_metadata_saas_config(\n        self,\n        generate_auth_header,\n        db,\n        test_config,\n        system_create_request_body: SystemSchema,\n    ):\n        \"\"\"Ensure system create works with custom metadata, including tested objects\"\"\"\n        auth_header = generate_auth_header(scopes=[SYSTEM_CREATE])\n        system_create_request_body.meta = {\n            \"saas_config\": {\n                \"type\": \"stripe\",\n                \"icon\": \"test\",\n            }\n        }\n        result = _api.create(\n            url=test_config.cli.server_url,\n            headers=auth_header,\n            resource_type=\"system\",\n            json_resource=system_create_request_body.json(exclude_none=True),\n        )\n\n        assert result.status_code == HTTP_201_CREATED\n        assert result.json()[\"name\"] == \"Test System\"\n        assert len(result.json()[\"privacy_declarations\"]) == 2\n\n        assert result.json()[\"meta\"] == {\n            \"saas_config\": {\n                \"type\": \"stripe\",\n                \"icon\": \"test\",\n            }\n        }\n\n        systems = System.all(db)\n        assert len(systems) == 1\n        assert systems[0].name == \"Test System\"\n        assert len(systems[0].privacy_declarations) == 2\n        assert systems[0].meta == {\n            \"saas_config\": {\n                \"type\": \"stripe\",\n                \"icon\": \"test\",\n            }\n        }\n\n        # and assert we can retrieve the custom metadata property via API (`GET`)\n        auth_header = generate_auth_header(scopes=[SYSTEM_READ])\n        get_response = _api.get(\n            url=test_config.cli.server_url,\n            headers=auth_header,\n            resource_type=\"system\",\n            resource_id=systems[0].fides_key,\n        )\n        assert get_response.json()[\"meta\"] == {\n            \"saas_config\": {\n                \"type\": \"stripe\",\n                \"icon\": \"test\",\n            }\n        }\n\n    def test_system_create_has_role_that_can_update_all_systems(\n        self,\n        test_config,\n        system_create_request_body,\n        db,\n        generate_role_header,\n    ):\n        \"\"\"Ensure system create works for owner role, which has necessary scope\"\"\"\n        auth_header = generate_role_header(roles=[OWNER])\n        result = _api.create(\n            url=test_config.cli.server_url,\n            headers=auth_header,\n            resource_type=\"system\",\n            json_resource=system_create_request_body.json(exclude_none=True),\n        )\n\n        assert result.status_code == HTTP_201_CREATED\n        assert result.json()[\"name\"] == \"Test System\"\n        assert len(result.json()[\"privacy_declarations\"]) == 2\n\n        systems = System.all(db)\n        assert len(systems) == 1\n        assert systems[0].name == \"Test System\"\n        assert len(systems[0].privacy_declarations) == 2\n\n    async def test_system_create_no_privacy_declarations(\n        self, generate_auth_header, db, test_config, system_create_request_body\n    ):\n        \"\"\"Ensure system create works even with no privacy declarations passed\"\"\"\n        system_create_request_body.privacy_declarations = []\n        auth_header = generate_auth_header(scopes=[SYSTEM_CREATE])\n\n        result = _api.create(\n            url=test_config.cli.server_url,\n            headers=auth_header,\n            resource_type=\"system\",\n            json_resource=system_create_request_body.json(exclude_none=True),\n        )\n\n        assert result.status_code == HTTP_201_CREATED\n        assert result.json()[\"name\"] == \"Test System\"\n        assert len(result.json()[\"privacy_declarations\"]) == 0\n\n        systems = System.all(db)\n        assert len(systems) == 1\n        assert systems[0].name == \"Test System\"\n        assert len(systems[0].privacy_declarations) == 0\n\n    async def test_system_create_invalid_privacy_declarations(\n        self, generate_auth_header, db, test_config, system_create_request_body\n    ):\n        \"\"\"Ensure system create errors with invalid privacy declarations\"\"\"\n        system_create_request_body.privacy_declarations[1].data_use = None\n        auth_header = generate_auth_header(scopes=[SYSTEM_CREATE])\n\n        result = _api.create(\n            url=test_config.cli.server_url,\n            headers=auth_header,\n            resource_type=\"system\",\n            json_resource=system_create_request_body.json(exclude_none=True),\n        )\n\n        assert result.status_code == HTTP_422_UNPROCESSABLE_ENTITY\n        assert len(System.all(db)) == 0  # ensure our system wasn't created\n        assert (\n            len(PrivacyDeclaration.all(db)) == 0\n        )  # ensure neither of our declarations were created\n\n        system_create_request_body.privacy_declarations[1].data_use = \"invalid_data_use\"\n        auth_header = generate_auth_header(scopes=[SYSTEM_CREATE])\n\n        result = _api.create(\n            url=test_config.cli.server_url,\n            headers=auth_header,\n            resource_type=\"system\",\n            json_resource=system_create_request_body.json(exclude_none=True),\n        )\n\n        assert result.status_code == HTTP_400_BAD_REQUEST\n        assert len(System.all(db)) == 0  # ensure our system wasn't created\n        assert (\n            len(PrivacyDeclaration.all(db)) == 0\n        )  # ensure neither of our declarations were created\n\n    async def test_system_create_invalid_legal_basis_for_profiling(\n        self, generate_auth_header, test_config, system_create_request_body\n    ):\n        system_create_request_body.legal_basis_for_profiling = [\"bad_basis\"]\n        auth_header = generate_auth_header(scopes=[SYSTEM_CREATE])\n\n        result = _api.create(\n            url=test_config.cli.server_url,\n            headers=auth_header,\n            resource_type=\"system\",\n            json_resource=system_create_request_body.json(exclude_none=True),\n        )\n\n        assert result.status_code == HTTP_422_UNPROCESSABLE_ENTITY\n        assert result.json()[\"detail\"][0][\"loc\"] == [\n            \"body\",\n            \"legal_basis_for_profiling\",\n            0,\n        ]\n\n\n@pytest.mark.unit\nclass TestSystemGet:\n    def test_data_stewards_included_in_response(\n        self, test_config, system, system_manager\n    ):\n        result = _api.get(\n            url=test_config.cli.server_url,\n            headers=test_config.user.auth_header,\n            resource_type=\"system\",\n            resource_id=system.fides_key,\n        )\n        assert result.status_code == 200\n        assert result.json()[\"fides_key\"] == system.fides_key\n\n        data_stewards = result.json()[\"data_stewards\"]\n        assert len(data_stewards) == 1\n        steward = data_stewards[0]\n\n        assert steward[\"id\"] == system_manager.id\n        assert steward[\"username\"] == system_manager.username\n        assert \"first_name\" in steward\n        assert \"last_name\" in steward\n\n    def test_system_privacy_declarations_are_sorted(self, test_config, system, db):\n        \"\"\"Test system Privacy Declarations are returned in alphabetical order by name.\"\"\"\n        data = {\n            \"data_use\": \"essential\",\n            \"name\": \"Another Declaration Name\",\n            \"system_id\": system.id,\n            \"data_subjects\": [],\n            \"data_categories\": [],\n        }\n        new_pd = PrivacyDeclaration.create(db, data=data)\n\n        result = _api.get(\n            url=test_config.cli.server_url,\n            headers=test_config.user.auth_header,\n            resource_type=\"system\",\n            resource_id=system.fides_key,\n        )\n        assert result.status_code == 200\n\n        privacy_declarations = result.json()[\"privacy_declarations\"]\n        assert len(privacy_declarations) == 2\n        assert privacy_declarations[0][\"name\"] == \"Another Declaration Name\"\n        assert privacy_declarations[1][\"name\"] == \"Collect data for marketing\"\n\n\n@pytest.mark.unit\nclass TestSystemList:\n    @pytest.fixture(scope=\"function\", autouse=True)\n    def remove_all_systems(self, db) -> None:\n        \"\"\"Remove any systems (and privacy declarations) before test execution for clean state\"\"\"\n        for privacy_declaration in PrivacyDeclaration.all(db):\n            privacy_declaration.delete(db)\n        for system in System.all(db):\n            system.delete(db)\n\n    def test_list_no_pagination(self, test_config, system_with_cleanup):\n        result = _api.ls(\n            url=test_config.cli.server_url,\n            headers=test_config.user.auth_header,\n            resource_type=\"system\",\n        )\n\n        assert result.status_code == 200\n        result_json = result.json()\n\n        assert len(result_json) == 1\n        assert result_json[0][\"fides_key\"] == system_with_cleanup.fides_key\n\n    def test_list_with_pagination(\n        self,\n        test_config,\n        system_with_cleanup,\n        tcf_system,\n    ):\n        result = _api.ls(\n            url=test_config.cli.server_url,\n            headers=test_config.user.auth_header,\n            resource_type=\"system\",\n            query_params={\n                \"page\": 1,\n                \"size\": 5,\n            },\n        )\n\n        assert result.status_code == 200\n        result_json = result.json()\n\n        assert result_json[\"page\"] == 1\n        assert result_json[\"size\"] == 5\n        assert result_json[\"total\"] == 2\n        assert len(result_json[\"items\"]) == 2\n\n        sorted_items = sorted(result_json[\"items\"], key=lambda x: x[\"fides_key\"])\n        assert sorted_items[0][\"fides_key\"] == system_with_cleanup.fides_key\n        assert sorted_items[1][\"fides_key\"] == tcf_system.fides_key\n\n    def test_list_with_pagination_default_page(\n        self,\n        test_config,\n        system_with_cleanup,\n        tcf_system,\n    ):\n        # We don't pass in the page but we pass in the size,\n        # so we should get a paginated response with the default page number (1)\n        result = _api.ls(\n            url=test_config.cli.server_url,\n            headers=test_config.user.auth_header,\n            resource_type=\"system\",\n            query_params={\n                \"size\": 5,\n            },\n        )\n\n        assert result.status_code == 200\n        result_json = result.json()\n\n        assert result_json[\"page\"] == 1\n        assert result_json[\"size\"] == 5\n        assert result_json[\"total\"] == 2\n        assert len(result_json[\"items\"]) == 2\n\n        sorted_items = sorted(result_json[\"items\"], key=lambda x: x[\"fides_key\"])\n        assert sorted_items[0][\"fides_key\"] == system_with_cleanup.fides_key\n        assert sorted_items[1][\"fides_key\"] == tcf_system.fides_key\n\n    def test_list_with_pagination_default_size(\n        self,\n        test_config,\n        system_with_cleanup,\n        tcf_system,\n    ):\n        # We don't pass in the size but we pass in the page,\n        # so we should get a paginated response with the default size (50)\n        result = _api.ls(\n            url=test_config.cli.server_url,\n            headers=test_config.user.auth_header,\n            resource_type=\"system\",\n            query_params={\n                \"page\": 1,\n            },\n        )\n\n        assert result.status_code == 200\n        result_json = result.json()\n\n        assert result_json[\"page\"] == 1\n        assert result_json[\"size\"] == 50\n        assert result_json[\"total\"] == 2\n        assert len(result_json[\"items\"]) == 2\n\n        sorted_items = sorted(result_json[\"items\"], key=lambda x: x[\"fides_key\"])\n        assert sorted_items[0][\"fides_key\"] == system_with_cleanup.fides_key\n        assert sorted_items[1][\"fides_key\"] == tcf_system.fides_key\n\n    def test_list_with_pagination_and_search(\n        self,\n        test_config,\n        system_with_cleanup,\n        tcf_system,\n        system_third_party_sharing,\n    ):\n        result = _api.ls(\n            url=test_config.cli.server_url,\n            headers=test_config.user.auth_header,\n            resource_type=\"system\",\n            query_params={\"page\": 1, \"size\": 5, \"search\": \"tcf\"},\n        )\n\n        assert result.status_code == 200\n        result_json = result.json()\n        assert result_json[\"total\"] == 1\n        assert len(result_json[\"items\"]) == 1\n        assert result_json[\"items\"][0][\"fides_key\"] == tcf_system.fides_key\n\n    def test_list_with_pagination_and_data_uses_filter(\n        self,\n        test_config,\n        system_multiple_decs,\n        tcf_system,\n        system_third_party_sharing,\n    ):\n        result = _api.ls(\n            url=test_config.cli.server_url,\n            headers=test_config.user.auth_header,\n            resource_type=\"system\",\n            query_params={\n                \"page\": 1,\n                \"size\": 5,\n                \"data_uses\": [\"third_party_sharing\"],\n            },\n        )\n\n        assert result.status_code == 200\n        result_json = result.json()\n        assert result_json[\"total\"] == 2\n        assert len(result_json[\"items\"]) == 2\n\n        sorted_items = sorted(result_json[\"items\"], key=lambda x: x[\"fides_key\"])\n\n        assert sorted_items[0][\"fides_key\"] == system_multiple_decs.fides_key\n        assert sorted_items[1][\"fides_key\"] == system_third_party_sharing.fides_key\n\n    def test_list_with_pagination_and_multiple_data_uses_filter(\n        self,\n        test_config,\n        system_multiple_decs,\n        tcf_system,\n        system_third_party_sharing,\n    ):\n        result = _api.ls(\n            url=test_config.cli.server_url,\n            headers=test_config.user.auth_header,\n            resource_type=\"system\",\n            query_params={\n                \"page\": 1,\n                \"size\": 5,\n                \"data_uses\": [\"third_party_sharing\", \"essential.fraud_detection\"],\n            },\n        )\n\n        assert result.status_code == 200\n        result_json = result.json()\n        assert result_json[\"total\"] == 3\n        assert len(result_json[\"items\"]) == 3\n\n        sorted_items = sorted(result_json[\"items\"], key=lambda x: x[\"fides_key\"])\n        assert sorted_items[0][\"fides_key\"] == system_multiple_decs.fides_key\n        assert sorted_items[1][\"fides_key\"] == system_third_party_sharing.fides_key\n        assert sorted_items[2][\"fides_key\"] == tcf_system.fides_key\n\n    def test_list_with_pagination_and_data_categories_filter(\n        self,\n        test_config,\n        system_with_cleanup,\n        tcf_system,\n        system_third_party_sharing,\n        system_with_no_uses,\n    ):\n        result = _api.ls(\n            url=test_config.cli.server_url,\n            headers=test_config.user.auth_header,\n            resource_type=\"system\",\n            query_params={\n                \"page\": 1,\n                \"size\": 5,\n                \"data_categories\": [\"user.device.cookie_id\"],\n            },\n        )\n\n        assert result.status_code == 200\n        result_json = result.json()\n        assert result_json[\"total\"] == 3\n        assert len(result_json[\"items\"]) == 3\n\n        sorted_items = sorted(result_json[\"items\"], key=lambda x: x[\"fides_key\"])\n        assert sorted_items[0][\"fides_key\"] == system_with_cleanup.fides_key\n        assert sorted_items[1][\"fides_key\"] == system_third_party_sharing.fides_key\n        assert sorted_items[2][\"fides_key\"] == tcf_system.fides_key\n\n    def test_list_with_pagination_and_data_subjects_filter(\n        self,\n        test_config,\n        system_with_cleanup,\n        tcf_system,\n        system_third_party_sharing,\n        system_with_no_uses,\n    ):\n        result = _api.ls(\n            url=test_config.cli.server_url,\n            headers=test_config.user.auth_header,\n            resource_type=\"system\",\n            query_params={\n                \"page\": 1,\n                \"size\": 5,\n                \"data_subjects\": [\"customer\"],\n            },\n        )\n\n        assert result.status_code == 200\n        result_json = result.json()\n        assert result_json[\"total\"] == 3\n        assert len(result_json[\"items\"]) == 3\n\n        sorted_items = sorted(result_json[\"items\"], key=lambda x: x[\"fides_key\"])\n        assert sorted_items[0][\"fides_key\"] == system_with_cleanup.fides_key\n        assert sorted_items[1][\"fides_key\"] == system_third_party_sharing.fides_key\n        assert sorted_items[2][\"fides_key\"] == tcf_system.fides_key\n\n    def test_list_with_pagination_and_multiple_filters(\n        self,\n        test_config,\n        system_with_cleanup,\n        tcf_system,\n        system_third_party_sharing,\n        system_with_no_uses,\n    ):\n        result = _api.ls(\n            url=test_config.cli.server_url,\n            headers=test_config.user.auth_header,\n            resource_type=\"system\",\n            query_params={\n                \"page\": 1,\n                \"size\": 5,\n                # TCF System has different privacy declarations, that together have all these fields\n                \"data_uses\": [\"essential.fraud_detection\"],\n                \"data_subjects\": [\"customer\"],\n                \"data_categories\": [\"user\"],\n            },\n        )\n\n        assert result.status_code == 200\n        result_json = result.json()\n        assert result_json[\"total\"] == 1\n        assert len(result_json[\"items\"]) == 1\n\n        assert result_json[\"items\"][0][\"fides_key\"] == tcf_system.fides_key\n\n    @pytest.mark.skip(\"Until we re-visit filter implementation\")\n    def test_list_with_pagination_and_multiple_filters_2(\n        self,\n        test_config,\n        system_with_cleanup,\n        tcf_system,\n        system_third_party_sharing,\n        system_with_no_uses,\n        db,\n    ):\n\n        db.que\n        result = _api.ls(\n            url=test_config.cli.server_url,\n            headers=test_config.user.auth_header,\n            resource_type=\"system\",\n            query_params={\n                \"page\": 1,\n                \"size\": 5,\n                # TCF system has a single privacy declaration with all these fields\n                \"data_uses\": [\"essential.fraud_detection\"],\n                \"data_subjects\": [\"customer\"],\n                \"data_categories\": [\"user.device.cookie_id\"],\n            },\n        )\n\n        assert result.status_code == 200\n        result_json = result.json()\n        assert result_json[\"total\"] == 1\n        assert len(result_json[\"items\"]) == 1\n\n        assert result_json[\"items\"][0][\"fides_key\"] == tcf_system.fides_key\n\n    @pytest.mark.parametrize(\n        \"vendor_deleted_date, expected_systems_count, show_deleted\",\n        [\n            (datetime.now() - timedelta(days=1), 1, True),\n            (datetime.now() - timedelta(days=1), 0, False),\n            (datetime.now() + timedelta(days=1), 1, False),\n            (None, 1, False),\n        ],\n    )\n    def test_vendor_deleted_systems(\n        self,\n        db,\n        test_config,\n        system_with_cleanup,\n        vendor_deleted_date,\n        expected_systems_count,\n        show_deleted,\n    ):\n\n        system_with_cleanup.vendor_deleted_date = vendor_deleted_date\n        db.commit()\n\n        result = _api.ls(\n            url=test_config.cli.server_url,\n            headers=test_config.user.auth_header,\n            resource_type=\"system\",\n            query_params={\"show_deleted\": show_deleted, \"size\": 50},\n        )\n\n        assert result.status_code == 200\n        result_json = result.json()\n\n        assert len(result_json[\"items\"]) == expected_systems_count\n\n\n@pytest.mark.unit\nclass TestSystemUpdate:\n    updated_system_name = \"Updated System Name\"\n\n    @pytest.fixture(scope=\"function\", autouse=True)\n    def remove_all_systems(self, db) -> None:\n        \"\"\"Remove any systems (and privacy declarations) before test execution for clean state\"\"\"\n        for privacy_declaration in PrivacyDeclaration.all(db):\n            privacy_declaration.delete(db)\n        for system in System.all(db):\n            system.delete(db)\n\n    @pytest.fixture(scope=\"function\")\n    def system_update_request_body(self, system) -> SystemSchema:\n        return SystemSchema(\n            organization_fides_key=\"1\",\n            fides_key=system.fides_key,\n            system_type=\"SYSTEM\",\n            name=self.updated_system_name,\n            vendor_deleted_date=datetime(2022, 5, 22),\n            description=\"Test Policy\",\n            privacy_declarations=[\n                models.PrivacyDeclaration(\n                    name=\"declaration-name\",\n                    data_categories=[],\n                    data_use=\"essential\",\n                    data_subjects=[],\n                    dataset_references=[],\n                    ingress=None,\n                    egress=None,\n                )\n            ],\n        )\n\n    @pytest.fixture(scope=\"function\")\n    def system_update_request_body_with_system_cookies(self, system) -> SystemSchema:\n        return SystemSchema(\n            organization_fides_key=\"1\",\n            fides_key=system.fides_key,\n            system_type=\"SYSTEM\",\n            name=self.updated_system_name,\n            description=\"Test Policy\",\n            cookies=[\n                {\"name\": \"my_system_cookie\", \"domain\": \"example.com\"},\n                {\"name\": \"my_other_system_cookie\"},\n            ],\n            privacy_declarations=[\n                models.PrivacyDeclaration(\n                    name=\"declaration-name\",\n                    data_categories=[],\n                    data_use=\"essential\",\n                    data_subjects=[],\n                    dataset_references=[],\n                    ingress=None,\n                    egress=None,\n                )\n            ],\n        )\n\n    @pytest.fixture(scope=\"function\")\n    def system_update_request_body_with_privacy_declaration_cookies(\n        self, system\n    ) -> SystemSchema:\n        return SystemSchema(\n            organization_fides_key=\"1\",\n            fides_key=system.fides_key,\n            system_type=\"SYSTEM\",\n            name=self.updated_system_name,\n            description=\"Test Policy\",\n            privacy_declarations=[\n                models.PrivacyDeclaration(\n                    name=\"declaration-name\",\n                    data_categories=[],\n                    data_use=\"essential\",\n                    data_subjects=[],\n                    dataset_references=[],\n                    cookies=[\n                        {\"name\": \"my_cookie\", \"domain\": \"example.com\"},\n                        {\"name\": \"my_other_cookie\"},\n                    ],\n                )\n            ],\n        )\n\n    @pytest.fixture(scope=\"function\")\n    def system_update_request_body_with_new_dictionary_fields(\n        self, system\n    ) -> SystemSchema:\n        return SystemSchema(\n            organization_fides_key=\"1\",\n            fides_key=system.fides_key,\n            system_type=\"SYSTEM\",\n            name=self.updated_system_name,\n            description=\"Test Policy\",\n            vendor_id=\"test_vendor\",\n            dataset_references=[\"another_system_reference\"],\n            processes_personal_data=True,\n            exempt_from_privacy_regulations=False,\n            reason_for_exemption=None,\n            uses_profiling=True,\n            legal_basis_for_profiling=[\"Authorised by law\", \"Contract\"],\n            does_international_transfers=True,\n            legal_basis_for_transfers=[\n                \"Adequacy Decision\",\n                \"BCRs\",\n                \"Supplementary Measures\",\n                \"Unknown legal basis\",\n            ],\n            requires_data_protection_assessments=True,\n            dpa_location=\"https://www.example.com/dpa\",\n            dpa_progress=\"pending\",\n            privacy_policy=\"https://www.example.com/privacy_policy\",\n            legal_name=\"Sunshine Corporation\",\n            legal_address=\"35925 Test Lane, Test Town, TX 24924\",\n            responsibility=[\"Processor\"],\n            dpo=\"John Doe, CIPT\",\n            joint_controller_info=\"Jane Doe\",\n            data_security_practices=\"We encrypt all your data in transit and at rest\",\n            privacy_declarations=[\n                models.PrivacyDeclaration(\n                    name=\"declaration-name\",\n                    data_categories=[],\n                    data_use=\"essential\",\n                    data_subjects=[],\n                    dataset_references=[\"another_system_reference\"],\n                    features=[\"Link different devices\"],\n                    legal_basis_for_processing=\"Public interest\",\n                    impact_assessment_location=\"https://www.example.com/impact_assessment_location\",\n                    retention_period=\"3-5 years\",\n                    processes_special_category_data=True,\n                    special_category_legal_basis=\"Reasons of substantial public interest (with a basis in law)\",\n                    data_shared_with_third_parties=True,\n                    third_parties=\"Third Party Marketing Dept.\",\n                    shared_categories=[\"user\"],\n                    cookies=[\n                        {\n                            \"name\": \"essential_cookie\",\n                            \"path\": \"/\",\n                            \"domain\": \"example.com\",\n                        }\n                    ],\n                )\n            ],\n        )\n\n    def test_system_update_not_authenticated(\n        self, test_config, system_update_request_body\n    ):\n        result = _api.update(\n            url=test_config.cli.server_url,\n            headers={},\n            resource_type=\"system\",\n            json_resource=system_update_request_body.json(exclude_none=True),\n        )\n        assert result.status_code == HTTP_401_UNAUTHORIZED\n\n    def test_system_update_no_direct_scope(\n        self,\n        test_config,\n        generate_auth_header,\n        system_update_request_body,\n        db,\n        system,\n    ):\n        auth_header = generate_auth_header(scopes=[POLICY_CREATE_OR_UPDATE])\n\n        result = _api.update(\n            url=test_config.cli.server_url,\n            headers=auth_header,\n            resource_type=\"system\",\n            json_resource=system_update_request_body.json(exclude_none=True),\n        )\n        assert result.status_code == HTTP_403_FORBIDDEN\n\n        db.refresh(system)\n        assert system.name != self.updated_system_name\n\n    async def test_system_update_has_direct_scope(\n        self, generate_auth_header, system, db, test_config, system_update_request_body\n    ):\n        assert system.name != self.updated_system_name\n        auth_header = generate_auth_header(scopes=[SYSTEM_UPDATE])\n\n        result = _api.update(\n            url=test_config.cli.server_url,\n            headers=auth_header,\n            resource_type=\"system\",\n            json_resource=system_update_request_body.json(exclude_none=True),\n        )\n\n        assert result.status_code == HTTP_200_OK\n        assert result.json()[\"name\"] == self.updated_system_name\n        assert len(result.json()[\"privacy_declarations\"]) == 1\n        assert (\n            result.json()[\"privacy_declarations\"][0][\"data_use\"]\n            == system_update_request_body.privacy_declarations[0].data_use\n        )\n\n        db.refresh(system)\n        assert system.name == self.updated_system_name\n        assert len(system.privacy_declarations) == 1\n        assert (\n            system.privacy_declarations[0].data_use\n            == system_update_request_body.privacy_declarations[0].data_use\n        )\n\n    def test_system_update_no_encompassing_role(\n        self,\n        test_config,\n        system_update_request_body,\n        system,\n        db,\n        generate_role_header,\n    ):\n        auth_header = generate_role_header(roles=[VIEWER])\n        result = _api.update(\n            url=test_config.cli.server_url,\n            headers=auth_header,\n            resource_type=\"system\",\n            json_resource=system_update_request_body.json(exclude_none=True),\n        )\n        assert result.status_code == HTTP_403_FORBIDDEN\n\n        db.refresh(system)\n        assert system.name != self.updated_system_name\n\n    def test_system_update_has_role_that_can_update_all_systems(\n        self,\n        test_config,\n        system_update_request_body,\n        system,\n        db,\n        generate_role_header,\n    ):\n        assert system.name != self.updated_system_name\n        auth_header = generate_role_header(roles=[OWNER])\n        result = _api.update(\n            url=test_config.cli.server_url,\n            headers=auth_header,\n            resource_type=\"system\",\n            json_resource=system_update_request_body.json(exclude_none=True),\n        )\n        assert result.status_code == HTTP_200_OK\n        assert result.json()[\"name\"] == self.updated_system_name\n\n        db.refresh(system)\n        assert system.name == self.updated_system_name\n\n    def test_system_update_as_system_manager(\n        self,\n        test_config,\n        system_update_request_body,\n        system,\n        db,\n        generate_system_manager_header,\n    ):\n        assert system.name != self.updated_system_name\n\n        auth_header = generate_system_manager_header([system.id])\n        result = _api.update(\n            url=test_config.cli.server_url,\n            headers=auth_header,\n            resource_type=\"system\",\n            json_resource=system_update_request_body.json(exclude_none=True),\n        )\n        assert result.status_code == HTTP_200_OK\n        assert result.json()[\"name\"] == self.updated_system_name\n\n        db.refresh(system)\n        assert system.name == self.updated_system_name\n        assert system.vendor_deleted_date == datetime(2022, 5, 22, tzinfo=timezone.utc)\n\n    def test_system_update_as_system_manager_403_if_not_found(\n        self,\n        test_config,\n        system_update_request_body,\n        system,\n        generate_system_manager_header,\n    ):\n        auth_header = generate_system_manager_header([system.id])\n        system_update_request_body.fides_key = \"system-does-not-exist\"\n        result = _api.update(\n            url=test_config.cli.server_url,\n            headers=auth_header,\n            resource_type=\"system\",\n            json_resource=system_update_request_body.json(exclude_none=True),\n        )\n        assert result.status_code == HTTP_403_FORBIDDEN\n\n    def test_system_update_as_owner_404_if_not_found(\n        self,\n        test_config,\n        system_update_request_body,\n        generate_role_header,\n    ):\n        auth_header = generate_role_header(roles=[OWNER])\n        system_update_request_body.fides_key = \"system-does-not-exist\"\n        result = _api.update(\n            url=test_config.cli.server_url,\n            headers=auth_header,\n            resource_type=\"system\",\n            json_resource=system_update_request_body.json(exclude_none=True),\n        )\n        assert result.status_code == HTTP_404_NOT_FOUND\n\n    def test_system_update_privacy_declaration_invalid_data_use(\n        self,\n        system,\n        test_config,\n        system_update_request_body,\n        generate_role_header,\n        db,\n    ):\n        auth_header = generate_role_header(roles=[OWNER])\n        system_update_request_body.privacy_declarations[0].data_use = \"invalid_data_use\"\n        result = _api.update(\n            url=test_config.cli.server_url,\n            headers=auth_header,\n            resource_type=\"system\",\n            json_resource=system_update_request_body.json(exclude_none=True),\n        )\n        assert result.status_code == HTTP_400_BAD_REQUEST\n        assert result.json() == {\n            \"detail\": \"Invalid privacy declaration referencing unknown DataUse invalid_data_use\"\n        }\n\n        # assert the system's privacy declaration has not been updated\n        db.refresh(system)\n        assert system.privacy_declarations[0].data_use == \"marketing.advertising\"\n\n    def test_system_update_privacy_declaration_invalid_data_category(\n        self,\n        system,\n        test_config,\n        system_update_request_body,\n        generate_role_header,\n        db,\n    ):\n        auth_header = generate_role_header(roles=[OWNER])\n        system_update_request_body.privacy_declarations[0].data_categories = [\n            \"invalid_data_category\"\n        ]\n        result = _api.update(\n            url=test_config.cli.server_url,\n            headers=auth_header,\n            resource_type=\"system\",\n            json_resource=system_update_request_body.json(exclude_none=True),\n        )\n        assert result.status_code == HTTP_400_BAD_REQUEST\n        assert result.json() == {\n            \"detail\": \"Invalid privacy declaration referencing unknown DataCategory invalid_data_category\"\n        }\n        # assert the system's privacy declaration has not been updated\n        db.refresh(system)\n        assert system.privacy_declarations[0].data_categories == [\n            \"user.device.cookie_id\"\n        ]\n\n    def test_system_update_privacy_declaration_invalid_data_subject(\n        self,\n        system,\n        test_config,\n        system_update_request_body,\n        generate_role_header,\n        db,\n    ):\n        auth_header = generate_role_header(roles=[OWNER])\n        system_update_request_body.privacy_declarations[0].data_subjects = [\n            \"invalid_data_subject\"\n        ]\n        result = _api.update(\n            url=test_config.cli.server_url,\n            headers=auth_header,\n            resource_type=\"system\",\n            json_resource=system_update_request_body.json(exclude_none=True),\n        )\n        assert result.status_code == HTTP_400_BAD_REQUEST\n        assert result.json() == {\n            \"detail\": \"Invalid privacy declaration referencing unknown DataSubject invalid_data_subject\"\n        }\n        # assert the system's privacy declaration has not been updated\n        db.refresh(system)\n        assert system.privacy_declarations[0].data_subjects == [\"customer\"]\n\n    def test_system_update_privacy_declaration_inactive_data_use(\n        self,\n        system,\n        test_config,\n        system_update_request_body,\n        inactive_data_use,\n        generate_role_header,\n        db,\n    ):\n        auth_header = generate_role_header(roles=[OWNER])\n        system_update_request_body.privacy_declarations[0].data_use = (\n            inactive_data_use.fides_key\n        )\n        result = _api.update(\n            url=test_config.cli.server_url,\n            headers=auth_header,\n            resource_type=\"system\",\n            json_resource=system_update_request_body.json(exclude_none=True),\n        )\n        assert result.status_code == HTTP_400_BAD_REQUEST\n        assert result.json() == {\n            \"detail\": f\"Invalid privacy declaration referencing inactive DataUse {inactive_data_use.fides_key}\"\n        }\n        # assert the system's privacy declaration has not been updated\n        db.refresh(system)\n        assert system.privacy_declarations[0].data_use == \"marketing.advertising\"\n\n    def test_system_update_privacy_declaration_inactive_data_category(\n        self,\n        system,\n        test_config,\n        system_update_request_body,\n        inactive_data_category,\n        generate_role_header,\n        db,\n    ):\n        auth_header = generate_role_header(roles=[OWNER])\n        system_update_request_body.privacy_declarations[0].data_categories = [\n            inactive_data_category.fides_key\n        ]\n        result = _api.update(\n            url=test_config.cli.server_url,\n            headers=auth_header,\n            resource_type=\"system\",\n            json_resource=system_update_request_body.json(exclude_none=True),\n        )\n        assert result.status_code == HTTP_400_BAD_REQUEST\n        assert result.json() == {\n            \"detail\": f\"Invalid privacy declaration referencing inactive DataCategory {inactive_data_category.fides_key}\"\n        }\n        # assert the system's privacy declaration has not been updated\n        db.refresh(system)\n        assert system.privacy_declarations[0].data_categories == [\n            \"user.device.cookie_id\"\n        ]\n\n    def test_system_update_privacy_declaration_inactive_data_subject(\n        self,\n        system,\n        test_config,\n        system_update_request_body,\n        inactive_data_subject,\n        generate_role_header,\n        db,\n    ):\n        auth_header = generate_role_header(roles=[OWNER])\n        system_update_request_body.privacy_declarations[0].data_subjects = [\n            inactive_data_subject.fides_key\n        ]\n        result = _api.update(\n            url=test_config.cli.server_url,\n            headers=auth_header,\n            resource_type=\"system\",\n            json_resource=system_update_request_body.json(exclude_none=True),\n        )\n        assert result.status_code == HTTP_400_BAD_REQUEST\n        assert result.json() == {\n            \"detail\": f\"Invalid privacy declaration referencing inactive DataSubject {inactive_data_subject.fides_key}\"\n        }\n        # assert the system's privacy declaration has not been updated\n        db.refresh(system)\n        assert system.privacy_declarations[0].data_subjects == [\"customer\"]\n\n    def test_system_update_privacy_declaration_invalid_duplicate(\n        self,\n        system,\n        test_config,\n        system_update_request_body,\n        generate_role_header,\n        db,\n    ):\n        auth_header = generate_role_header(roles=[OWNER])\n\n        # test that 'exact' duplicate fails (data_use and name match)\n        system_update_request_body.privacy_declarations.append(\n            models.PrivacyDeclaration(\n                name=\"declaration-name\",  # same as initial PrivacyDeclaration\n                data_categories=[\"user.payment\"],  # other fields can differ\n                data_use=\"essential\",  # same as initial PrivacyDeclaration\n                data_subjects=[\"anonymous_user\"],  # other fields can differ\n                dataset_references=[],\n            )\n        )\n        result = _api.update(\n            url=test_config.cli.server_url,\n            headers=auth_header,\n            resource_type=\"system\",\n            json_resource=system_update_request_body.json(exclude_none=True),\n        )\n        assert result.status_code == HTTP_400_BAD_REQUEST\n        # assert the system's privacy declaration has not been updated\n        db.refresh(system)\n        assert system.privacy_declarations[0].data_use == \"marketing.advertising\"\n\n        # test that duplicate with no name on either declaration fails\n        system_update_request_body.privacy_declarations = []\n        system_update_request_body.privacy_declarations.append(\n            models.PrivacyDeclaration(\n                name=\"\",  # no name specified\n                data_categories=[\"user.payment\"],\n                data_use=\"essential\",  # identical data use\n                data_subjects=[\"anonymous_user\"],  # other fields can differ\n                dataset_references=[],\n            )\n        )\n        system_update_request_body.privacy_declarations.append(\n            models.PrivacyDeclaration(\n                name=\"\",  # no name specified\n                data_categories=[\"user.payment\"],\n                data_use=\"essential\",  # identicial data use\n                data_subjects=[\"anonymous_user\"],\n                dataset_references=[],\n            )\n        )\n        result = _api.update(\n            url=test_config.cli.server_url,\n            headers=auth_header,\n            resource_type=\"system\",\n            json_resource=system_update_request_body.json(exclude_none=True),\n        )\n        assert result.status_code == HTTP_400_BAD_REQUEST\n        # assert the system's privacy declaration has not been updated\n        db.refresh(system)\n        assert system.privacy_declarations[0].data_use == \"marketing.advertising\"\n\n        # test that duplicate data_use with no name on one declaration succeeds\n        system_update_request_body.privacy_declarations = []\n        system_update_request_body.privacy_declarations.append(\n            models.PrivacyDeclaration(\n                name=\"\",  # no name specified\n                data_categories=[\"user.payment\"],\n                data_use=\"essential\",  # identical data use\n                data_subjects=[\"anonymous_user\"],\n                dataset_references=[],\n            )\n        )\n        system_update_request_body.privacy_declarations.append(\n            models.PrivacyDeclaration(\n                name=\"new declaration\",  # this name distinguishes the declaration from the above\n                data_categories=[\"user.payment\"],\n                data_use=\"essential\",  # identicial data use\n                data_subjects=[\"anonymous_user\"],\n                dataset_references=[],\n            )\n        )\n        result = _api.update(\n            url=test_config.cli.server_url,\n            headers=auth_header,\n            resource_type=\"system\",\n            json_resource=system_update_request_body.json(exclude_none=True),\n        )\n        assert result.status_code == HTTP_200_OK\n        # assert the system's privacy declarations have been updated\n        db.refresh(system)\n        # both declarations should have 'provide' data_use since the update was allowed\n        assert system.privacy_declarations[0].data_use == \"essential\"\n        assert system.privacy_declarations[1].data_use == \"essential\"\n\n        # test that duplicate data_use with differeing names on declarations succeeds\n        system_update_request_body.privacy_declarations = []\n        system_update_request_body.privacy_declarations.append(\n            models.PrivacyDeclaration(\n                name=\"new declaration 1\",  # specify a unique name here\n                data_categories=[\"user.payment\"],\n                data_use=\"marketing.advertising\",  # identical data use\n                data_subjects=[\"anonymous_user\"],\n                dataset_references=[],\n            )\n        )\n        system_update_request_body.privacy_declarations.append(\n            models.PrivacyDeclaration(\n                name=\"new declaration 2\",  # this name distinguishes the declaration from the above\n                data_categories=[\"user.payment\"],\n                data_use=\"marketing.advertising\",  # identicial data use\n                data_subjects=[\"anonymous_user\"],\n                dataset_references=[],\n            )\n        )\n        result = _api.update(\n            url=test_config.cli.server_url,\n            headers=auth_header,\n            resource_type=\"system\",\n            json_resource=system_update_request_body.json(exclude_none=True),\n        )\n        assert result.status_code == HTTP_200_OK\n        # assert the system's privacy declarations have been updated\n        db.refresh(system)\n        # both declarations should have 'advertising' data_use since the update was allowed\n        assert system.privacy_declarations[0].data_use == \"marketing.advertising\"\n        assert system.privacy_declarations[1].data_use == \"marketing.advertising\"\n\n        # test that differeing data_use with same names on declarations succeeds\n        system_update_request_body.privacy_declarations = []\n        system_update_request_body.privacy_declarations.append(\n            models.PrivacyDeclaration(\n                name=\"new declaration 1\",  # identical name\n                data_categories=[\"user.payment\"],\n                data_use=\"marketing.advertising\",  # differing data use\n                data_subjects=[\"anonymous_user\"],\n                dataset_references=[],\n            )\n        )\n        system_update_request_body.privacy_declarations.append(\n            models.PrivacyDeclaration(\n                name=\"new declaration 1\",  # identical name\n                data_categories=[\"user.payment\"],\n                data_use=\"essential\",  # differing data use\n                data_subjects=[\"anonymous_user\"],\n                dataset_references=[],\n            )\n        )\n        result = _api.update(\n            url=test_config.cli.server_url,\n            headers=auth_header,\n            resource_type=\"system\",\n            json_resource=system_update_request_body.json(exclude_none=True),\n        )\n        assert result.status_code == HTTP_200_OK\n        # assert the system's privacy declarations have been updated\n        db.refresh(system)\n        # should be one declaration with advertising, one with provide\n        assert (\n            system.privacy_declarations[0].data_use == \"marketing.advertising\"\n            and system.privacy_declarations[1].data_use == \"essential\"\n        ) or (\n            system.privacy_declarations[1].data_use == \"marketing.advertising\"\n            and system.privacy_declarations[0].data_use == \"essential\"\n        )\n        assert (\n            system.privacy_declarations[0].name == \"new declaration 1\"\n            and system.privacy_declarations[1].name == \"new declaration 1\"\n        )\n\n    def test_system_update_dictionary_fields(\n        self,\n        test_config,\n        system_update_request_body_with_new_dictionary_fields,\n        system,\n        db,\n        generate_system_manager_header,\n    ):\n        assert system.name != self.updated_system_name\n\n        auth_header = generate_system_manager_header([system.id])\n        result = _api.update(\n            url=test_config.cli.server_url,\n            headers=auth_header,\n            resource_type=\"system\",\n            json_resource=system_update_request_body_with_new_dictionary_fields.json(\n                exclude_none=True\n            ),\n        )\n\n        assert result.status_code == HTTP_200_OK\n\n        db.refresh(system)\n\n        assert system.name == self.updated_system_name\n        assert system.vendor_id == \"test_vendor\"\n        assert system.dataset_references == [\"another_system_reference\"]\n        assert system.processes_personal_data is True\n        assert system.exempt_from_privacy_regulations is False\n        assert system.reason_for_exemption is None\n        assert system.uses_profiling is True\n        assert system.legal_basis_for_profiling == [\"Authorised by law\", \"Contract\"]\n        assert system.does_international_transfers is True\n        assert system.legal_basis_for_transfers == [\n            \"Adequacy Decision\",\n            \"BCRs\",\n            \"Supplementary Measures\",\n            \"Unknown legal basis\",\n        ]\n        assert system.requires_data_protection_assessments is True\n        assert system.dpa_location == \"https://www.example.com/dpa\"\n        assert system.dpa_progress == \"pending\"\n        assert system.privacy_policy == \"https://www.example.com/privacy_policy\"\n        assert system.legal_name == \"Sunshine Corporation\"\n        assert system.legal_address == \"35925 Test Lane, Test Town, TX 24924\"\n        assert system.responsibility == [\"Processor\"]\n        assert system.dpo == \"John Doe, CIPT\"\n        assert system.joint_controller_info == \"Jane Doe\"\n        assert (\n            system.data_security_practices\n            == \"We encrypt all your data in transit and at rest\"\n        )\n        assert system.data_stewards == []\n\n        privacy_decl = system.privacy_declarations[0]\n        assert privacy_decl.name == \"declaration-name\"\n        assert privacy_decl.dataset_references == [\"another_system_reference\"]\n        assert privacy_decl.features == [\"Link different devices\"]\n        assert privacy_decl.legal_basis_for_processing == \"Public interest\"\n        assert (\n            privacy_decl.impact_assessment_location\n            == \"https://www.example.com/impact_assessment_location\"\n        )\n        assert privacy_decl.retention_period == \"3-5 years\"\n        assert privacy_decl.processes_special_category_data is True\n        assert (\n            privacy_decl.special_category_legal_basis\n            == \"Reasons of substantial public interest (with a basis in law)\"\n        )\n        assert privacy_decl.data_shared_with_third_parties is True\n        assert privacy_decl.third_parties == \"Third Party Marketing Dept.\"\n        assert privacy_decl.shared_categories == [\"user\"]\n\n        json_results = result.json()\n        for field in SystemResponse.model_fields:\n            system_val = getattr(system, field)\n            if isinstance(system_val, typing.Hashable) and not isinstance(\n                system_val, datetime\n            ):\n                assert system_val == json_results[field]\n        assert len(json_results[\"privacy_declarations\"]) == 1\n        assert json_results[\"data_stewards\"] == []\n        assert json_results[\"created_at\"]\n\n        for i, decl in enumerate(system.privacy_declarations):\n            for field in PrivacyDeclarationResponse.model_fields:\n                decl_val = getattr(decl, field, None)\n                if hasattr(decl, field) and isinstance(decl_val, typing.Hashable):\n                    assert decl_val == json_results[\"privacy_declarations\"][i][field]\n\n    def test_system_update_system_cookies(\n        self,\n        test_config,\n        system_update_request_body_with_system_cookies,\n        system,\n        db,\n        generate_system_manager_header,\n    ):\n        assert system.name != self.updated_system_name\n        assert len(system.cookies) == 1\n\n        auth_header = generate_system_manager_header([system.id])\n        result = _api.update(\n            url=test_config.cli.server_url,\n            headers=auth_header,\n            resource_type=\"system\",\n            json_resource=system_update_request_body_with_system_cookies.json(\n                exclude_none=True\n            ),\n        )\n        assert result.status_code == HTTP_200_OK\n        assert result.json()[\"name\"] == self.updated_system_name\n        # System level cookies removed\n        assert result.json()[\"cookies\"] == [\n            {\"name\": \"my_system_cookie\", \"domain\": \"example.com\", \"path\": None},\n            {\"name\": \"my_other_system_cookie\", \"domain\": None, \"path\": None},\n        ]\n\n        # Privacy declaration cookies added\n        assert result.json()[\"privacy_declarations\"][0][\"cookies\"] == []\n\n        db.refresh(system)\n        assert system.name == self.updated_system_name\n        assert len(system.cookies) == 2\n        assert len(system.privacy_declarations[0].cookies) == 0\n\n        system_history = (\n            db.query(SystemHistory).filter(SystemHistory.system_id == system.id).first()\n        )\n        cookie_history = system_history.after[\"cookies\"]\n        assert {cookie[\"name\"] for cookie in cookie_history} == {\n            \"my_system_cookie\",\n            \"my_other_system_cookie\",\n        }\n\n    def test_system_update_privacy_declaration_cookies(\n        self,\n        test_config,\n        system_update_request_body_with_privacy_declaration_cookies,\n        system,\n        db,\n        generate_system_manager_header,\n    ):\n        assert system.name != self.updated_system_name\n        assert len(system.cookies) == 1\n\n        auth_header = generate_system_manager_header([system.id])\n        result = _api.update(\n            url=test_config.cli.server_url,\n            headers=auth_header,\n            resource_type=\"system\",\n            json_resource=system_update_request_body_with_privacy_declaration_cookies.json(\n                exclude_none=True\n            ),\n        )\n        assert result.status_code == HTTP_200_OK\n        assert result.json()[\"name\"] == self.updated_system_name\n        # System level cookies removed\n        assert result.json()[\"cookies\"] == []\n        # Privacy declaration cookies added\n        assert sorted(\n            result.json()[\"privacy_declarations\"][0][\"cookies\"], key=lambda r: r[\"name\"]\n        ) == sorted(\n            [\n                {\"name\": \"my_cookie\", \"path\": None, \"domain\": \"example.com\"},\n                {\"name\": \"my_other_cookie\", \"path\": None, \"domain\": None},\n            ],\n            key=lambda r: r[\"name\"],\n        )\n\n        db.refresh(system)\n        assert system.name == self.updated_system_name\n        assert (\n            len(system.cookies)\n            == 0  # System cookies were deleted because they weren't in the request\n        )  # Two from the current privacy declaration\n        assert len(system.privacy_declarations[0].cookies) == 2\n\n    @pytest.mark.parametrize(\n        \"update_declarations\",\n        [\n            (\n                [  # add a privacy declaration distinct from existing declaration\n                    models.PrivacyDeclaration(\n                        name=\"declaration-name\",\n                        data_categories=[],\n                        data_use=\"essential\",\n                        data_subjects=[],\n                        dataset_references=[],\n                        cookies=[],\n                        egress=None,\n                        ingress=None,\n                    )\n                ]\n            ),\n            (\n                # add 2 privacy declarations distinct from existing declaration\n                [\n                    models.PrivacyDeclaration(\n                        name=\"declaration-name\",\n                        data_categories=[],\n                        data_use=\"essential\",\n                        data_subjects=[],\n                        dataset_references=[],\n                        cookies=[],\n                        egress=None,\n                        ingress=None,\n                    ),\n                    models.PrivacyDeclaration(\n                        name=\"declaration-name-2\",\n                        data_categories=[],\n                        data_use=\"third_party_sharing\",\n                        data_subjects=[],\n                        dataset_references=[],\n                        cookies=[],\n                        egress=None,\n                        ingress=None,\n                    ),\n                ]\n            ),\n            (\n                # add 2 privacy declarations, one the same data use and name as existing\n                [\n                    models.PrivacyDeclaration(\n                        name=\"declaration-name\",\n                        data_categories=[],\n                        data_use=\"third_party_sharing\",\n                        data_subjects=[],\n                        dataset_references=[],\n                        cookies=[],\n                        ingress=None,\n                        egress=None,\n                    ),\n                    models.PrivacyDeclaration(\n                        name=\"Collect data for marketing\",\n                        data_categories=[],\n                        data_use=\"marketing.advertising\",\n                        data_subjects=[],\n                        dataset_references=[],\n                        cookies=[],\n                        ingress=None,\n                        egress=None,\n                    ),\n                ]\n            ),\n            (\n                # add 2 privacy declarations, one the same data use and name as existing, other same data use\n                [\n                    models.PrivacyDeclaration(\n                        name=\"Collect data for marketing\",\n                        data_categories=[],\n                        data_use=\"marketing.advertising\",\n                        data_subjects=[],\n                        dataset_references=[],\n                        cookies=[],\n                        egress=None,\n                        ingress=None,\n                    ),\n                    models.PrivacyDeclaration(\n                        name=\"declaration-name-2\",\n                        data_categories=[],\n                        data_use=\"marketing.advertising\",\n                        data_subjects=[],\n                        dataset_references=[],\n                        cookies=[],\n                        egress=None,\n                        ingress=None,\n                    ),\n                ]\n            ),\n            (\n                # specify no declarations, declarations should be cleared off the system\n                []\n            ),\n        ],\n    )\n    def test_system_update_updates_declarations(\n        self,\n        db,\n        test_config,\n        system,\n        generate_auth_header,\n        system_update_request_body,\n        update_declarations,\n    ):\n        \"\"\"\n        Test to assert that our `PUT` endpoint acts in a fully declarative manner, putting the DB state of the\n        system's privacy requests in *exactly* the same state as specified on the request payload.\n\n        This is executed against various different sets of input privacy declarations to ensure it works\n        in a variety of scenarios\n        \"\"\"\n\n        auth_header = generate_auth_header(scopes=[SYSTEM_UPDATE])\n        system_update_request_body.privacy_declarations = update_declarations\n        result = _api.update(\n            url=test_config.cli.server_url,\n            headers=auth_header,\n            resource_type=\"system\",\n            json_resource=system_update_request_body.json(exclude_none=True),\n        )\n\n        # assert the declarations in our responses match those in our requests\n        response_decs: List[dict] = result.json()[\"privacy_declarations\"]\n\n        assert len(response_decs) == len(\n            update_declarations\n        ), \"Response declaration count doesn't match the number sent!\"\n        for response_dec in response_decs:\n            assert (\n                \"id\" in response_dec.keys()\n            ), \"No 'id' field in the response declaration!\"\n\n            parsed_response_declaration = models.PrivacyDeclaration.model_validate(\n                response_dec\n            )\n            assert (\n                parsed_response_declaration in update_declarations\n            ), \"The response declaration '{}' doesn't match anything in the request declarations!\".format(\n                parsed_response_declaration.name\n            )\n\n        # do the same for the declarations in our db record\n        system = System.all(db)[0]\n        db.refresh(system)\n        db_decs = [\n            models.PrivacyDeclaration.model_validate(db_dec)\n            for db_dec in system.privacy_declarations\n        ]\n\n        for update_dec in update_declarations:\n            db_decs.remove(update_dec)\n        # and assert we don't have any extra response declarations\n        assert len(db_decs) == 0\n\n    @pytest.mark.parametrize(\n        \"update_declarations\",\n        [\n            (\n                [  # Check 1: update a dec matching one existing dec\n                    models.PrivacyDeclaration(\n                        name=\"Collect data for marketing\",\n                        data_categories=[],\n                        data_use=\"marketing.advertising\",\n                        data_subjects=[],\n                        dataset_references=[],\n                    )\n                ]\n            ),\n            (\n                [  # Check 2: add a new single dec with same data use\n                    models.PrivacyDeclaration(\n                        name=\"declaration-name-1\",\n                        data_categories=[],\n                        data_use=\"marketing.advertising\",\n                        data_subjects=[],\n                        dataset_references=[],\n                    )\n                ]\n            ),\n            (\n                [  # Check 3: add a new single dec with same data use, no name\n                    models.PrivacyDeclaration(\n                        name=\"\",\n                        data_categories=[],\n                        data_use=\"marketing.advertising\",\n                        data_subjects=[],\n                        dataset_references=[],\n                    )\n                ]\n            ),\n            (\n                # Check 4: update 2 privacy declarations both matching existing decs\n                [\n                    models.PrivacyDeclaration(\n                        name=\"Collect data for marketing\",\n                        data_categories=[],\n                        data_use=\"marketing.advertising\",\n                        data_subjects=[],\n                        dataset_references=[],\n                    ),\n                    models.PrivacyDeclaration(\n                        name=\"Collect data for third party sharing\",\n                        data_categories=[],\n                        data_use=\"third_party_sharing\",\n                        data_subjects=[],\n                        dataset_references=[],\n                    ),\n                ]\n            ),\n            (\n                # Check 5: update 2 privacy declarations, one with matching name and data use, other only data use\n                [\n                    models.PrivacyDeclaration(\n                        name=\"Collect data for marketing\",\n                        data_categories=[],\n                        data_use=\"marketing.advertising\",\n                        data_subjects=[],\n                        dataset_references=[],\n                    ),\n                    models.PrivacyDeclaration(\n                        name=\"declaration-name-2\",\n                        data_categories=[],\n                        data_use=\"third_party_sharing\",\n                        data_subjects=[],\n                        dataset_references=[],\n                    ),\n                ]\n            ),\n            (\n                # Check 6: update 2 privacy declarations, one with matching name and data use, other only data use but same data use\n                [\n                    models.PrivacyDeclaration(\n                        name=\"Collect data for marketing\",\n                        data_categories=[],\n                        data_use=\"marketing.advertising\",\n                        data_subjects=[],\n                        dataset_references=[],\n                    ),\n                    models.PrivacyDeclaration(\n                        name=\"declaration-name-2\",\n                        data_categories=[],\n                        data_use=\"marketing.advertising\",\n                        data_subjects=[],\n                        dataset_references=[],\n                    ),\n                ]\n            ),\n            (\n                # Check 7: update 2 privacy declarations, one with only matching data use, other totally new\n                [\n                    models.PrivacyDeclaration(\n                        name=\"declaration-name-1\",\n                        data_categories=[],\n                        data_use=\"marketing.advertising\",\n                        data_subjects=[],\n                        dataset_references=[],\n                    ),\n                    models.PrivacyDeclaration(\n                        name=\"declaration-name-2\",\n                        data_categories=[],\n                        data_use=\"essential\",\n                        data_subjects=[],\n                        dataset_references=[],\n                    ),\n                ]\n            ),\n            (\n                # Check 8: add 2 new privacy declarations\n                [\n                    models.PrivacyDeclaration(\n                        name=\"declaration-name\",\n                        data_categories=[],\n                        data_use=\"essential\",\n                        data_subjects=[],\n                        dataset_references=[],\n                    ),\n                    models.PrivacyDeclaration(\n                        name=\"declaration-name-2\",\n                        data_categories=[],\n                        data_use=\"functional\",\n                        data_subjects=[],\n                        dataset_references=[],\n                    ),\n                ]\n            ),\n            (\n                # Check 9: add 2 new privacy declarations, same data uses as existing decs but no names\n                [\n                    models.PrivacyDeclaration(\n                        name=\"\",\n                        data_categories=[],\n                        data_use=\"marketing.advertising\",\n                        data_subjects=[],\n                        dataset_references=[],\n                    ),\n                    models.PrivacyDeclaration(\n                        name=\"\",\n                        data_categories=[],\n                        data_use=\"third_party_sharing\",\n                        data_subjects=[],\n                        dataset_references=[],\n                    ),\n                ]\n            ),\n            (\n                # Check 10: specify no declarations, declarations should be cleared off the system\n                []\n            ),\n        ],\n    )\n    def test_system_update_manages_declaration_records(\n        self,\n        db,\n        test_config,\n        system_multiple_decs: System,\n        generate_auth_header,\n        system_update_request_body,\n        update_declarations,\n    ):\n        \"\"\"\n        Test to assert that existing privacy declaration records stay constant when necessary\n        \"\"\"\n        old_db_decs = [\n            PrivacyDeclarationResponse.model_validate(dec)\n            for dec in system_multiple_decs.privacy_declarations\n        ]\n        old_decs_updated = [\n            old_db_dec\n            for old_db_dec in old_db_decs\n            if any(\n                (\n                    old_db_dec.name == update_declaration.name\n                    and old_db_dec.data_use == update_declaration.data_use\n                )\n                for update_declaration in update_declarations\n            )\n        ]\n        auth_header = generate_auth_header(scopes=[SYSTEM_UPDATE])\n        system_update_request_body.privacy_declarations = update_declarations\n        _api.update(\n            url=test_config.cli.server_url,\n            headers=auth_header,\n            resource_type=\"system\",\n            json_resource=system_update_request_body.json(exclude_none=True),\n        )\n\n        db.refresh(system_multiple_decs)\n        updated_decs: List[PrivacyDeclaration] = (\n            system_multiple_decs.privacy_declarations.copy()\n        )\n\n        for old_dec_updated in old_decs_updated:\n            updated_dec = next(\n                updated_dec\n                for updated_dec in updated_decs\n                if updated_dec.name == old_dec_updated.name\n                and updated_dec.data_use == old_dec_updated.data_use\n            )\n            # assert that the updated dec in the DB kept the same ID\n            assert updated_dec.id == old_dec_updated.id\n\n            # remove from our lists to check since we've confirmed ID stayed constant\n            updated_decs.remove(updated_dec)\n            old_db_decs.remove(old_dec_updated)\n\n        # our old db decs that were _not_ updated should no longer be in the db\n        for old_db_dec in old_db_decs:\n            assert not any(\n                old_db_dec.id == updated_dec.id for updated_dec in updated_decs\n            )\n\n        # and just verify that we have same number of privacy declarations in db as specified in the update request\n        assert len(PrivacyDeclaration.all(db)) == len(update_declarations)\n\n\n@pytest.mark.unit\nclass TestSystemDelete:\n    @pytest.fixture(scope=\"function\")\n    def url(self, system) -> str:\n        return V1_URL_PREFIX + f\"/system/{system.fides_key}\"\n\n    def test_system_delete_not_authenticated(self, test_config, system):\n        result = _api.delete(\n            url=test_config.cli.server_url,\n            resource_type=\"system\",\n            resource_id=system.fides_key,\n            headers={},\n        )\n        assert result.status_code == HTTP_401_UNAUTHORIZED\n\n    def test_system_delete_no_direct_scope(\n        self,\n        test_config,\n        url,\n        system,\n        generate_auth_header,\n    ):\n        auth_header = generate_auth_header(scopes=[SYSTEM_UPDATE])\n        result = _api.delete(\n            url=test_config.cli.server_url,\n            resource_type=\"system\",\n            resource_id=system.fides_key,\n            headers=auth_header,\n        )\n\n        assert result.status_code == HTTP_403_FORBIDDEN\n\n    def test_system_delete_has_direct_scope(\n        self, test_config, generate_auth_header, system\n    ):\n        auth_header = generate_auth_header(scopes=[SYSTEM_DELETE])\n        result = _api.delete(\n            url=test_config.cli.server_url,\n            resource_type=\"system\",\n            resource_id=system.fides_key,\n            headers=auth_header,\n        )\n\n        assert result.status_code == HTTP_200_OK\n        assert result.json()[\"message\"] == \"resource deleted\"\n        assert result.json()[\"resource\"][\"fides_key\"] == system.fides_key\n\n    def test_system_delete_no_encompassing_role(\n        self, test_config, generate_role_header, system\n    ):\n        auth_header = generate_role_header(roles=[VIEWER])\n        result = _api.delete(\n            url=test_config.cli.server_url,\n            resource_type=\"system\",\n            resource_id=system.fides_key,\n            headers=auth_header,\n        )\n        assert result.status_code == HTTP_403_FORBIDDEN\n\n    def test_system_delete_has_role_that_can_delete_systems(\n        self,\n        test_config,\n        system,\n        generate_role_header,\n    ):\n        auth_header = generate_role_header(roles=[OWNER])\n        result = _api.delete(\n            url=test_config.cli.server_url,\n            resource_type=\"system\",\n            resource_id=system.fides_key,\n            headers=auth_header,\n        )\n        assert result.status_code == HTTP_200_OK\n        assert result.json()[\"message\"] == \"resource deleted\"\n        assert result.json()[\"resource\"][\"fides_key\"] == system.fides_key\n\n    def test_system_delete_as_system_manager(\n        self,\n        test_config,\n        system,\n        generate_system_manager_header,\n    ):\n        auth_header = generate_system_manager_header([system.id])\n        result = _api.delete(\n            url=test_config.cli.server_url,\n            resource_type=\"system\",\n            resource_id=system.fides_key,\n            headers=auth_header,\n        )\n        assert result.status_code == HTTP_403_FORBIDDEN\n\n    def test_delete_system_deletes_connection_config_and_dataset(\n        self,\n        test_config,\n        db,\n        system,\n        generate_auth_header,\n        dataset_config: DatasetConfig,\n    ) -> None:\n        \"\"\"\n        Ensure that deleting the system also deletes any associated\n        ConnectionConfig and DatasetConfig records\n        \"\"\"\n        auth_header = generate_auth_header(scopes=[SYSTEM_DELETE])\n\n        connection_config = dataset_config.connection_config\n        connection_config.system_id = (\n            system.id\n        )  # tie the connectionconfig to the system we will delete\n        connection_config.save(db)\n        # the keys are cached before the delete\n        connection_config_key = connection_config.key\n        dataset_config_key = dataset_config.fides_key\n\n        # delete the system via API\n        result = _api.delete(\n            url=test_config.cli.server_url,\n            resource_type=\"system\",\n            resource_id=system.fides_key,\n            headers=auth_header,\n        )\n        assert result.status_code == HTTP_200_OK\n\n        # ensure our system itself was deleted\n        assert db.query(System).filter_by(fides_key=system.fides_key).first() is None\n        # ensure our associated ConnectionConfig was deleted\n        assert (\n            db.query(ConnectionConfig).filter_by(key=connection_config_key).first()\n            is None\n        )\n        # and ensure our associated DatasetConfig was deleted\n        assert (\n            db.query(DatasetConfig).filter_by(fides_key=dataset_config_key).first()\n            is None\n        )\n\n    def test_owner_role_gets_404_if_system_not_found(\n        self,\n        test_config,\n        generate_role_header,\n    ):\n        auth_header = generate_role_header(roles=[OWNER])\n        result = _api.delete(\n            url=test_config.cli.server_url,\n            resource_type=\"system\",\n            resource_id=\"bad_fides_key\",\n            headers=auth_header,\n        )\n        assert result.status_code == HTTP_404_NOT_FOUND\n\n    def test_system_manager_gets_403_if_system_not_found(\n        self, test_config, system, generate_system_manager_header\n    ):\n        auth_header = generate_system_manager_header([system.id])\n        result = _api.delete(\n            url=test_config.cli.server_url,\n            resource_type=\"system\",\n            resource_id=\"bad_fides_key\",\n            headers=auth_header,\n        )\n        assert result.status_code == HTTP_403_FORBIDDEN\n\n\n@pytest.mark.integration\nclass TestDefaultTaxonomyCrud:\n    @pytest.mark.parametrize(\"endpoint\", TAXONOMY_ENDPOINTS)\n    def test_api_cannot_delete_default(\n        self, test_config: FidesConfig, endpoint: str\n    ) -> None:\n        resource = getattr(DEFAULT_TAXONOMY, endpoint)[0]\n\n        result = _api.delete(\n            url=test_config.cli.server_url,\n            resource_type=endpoint,\n            resource_id=resource.fides_key,\n            headers=test_config.user.auth_header,\n        )\n        assert result.status_code == 403\n        assert (\n            \"cannot modify 'is_default' field on an existing resource\"\n            in result.json()[\"detail\"][\"error\"]\n        )\n\n    @pytest.mark.parametrize(\"endpoint\", TAXONOMY_ENDPOINTS)\n    def test_api_can_update_default(\n        self, test_config: FidesConfig, endpoint: str\n    ) -> None:\n        \"\"\"Should be able to update as long as `is_default` is not changing\"\"\"\n        resource = getattr(DEFAULT_TAXONOMY, endpoint)[0]\n        json_resource = resource.json(exclude_none=True)\n\n        result = _api.update(\n            url=test_config.cli.server_url,\n            headers=test_config.user.auth_header,\n            resource_type=endpoint,\n            json_resource=json_resource,\n        )\n        assert result.status_code == 200\n\n    @pytest.mark.parametrize(\"endpoint\", TAXONOMY_ENDPOINTS)\n    def test_api_can_upsert_default(\n        self, test_config: FidesConfig, endpoint: str\n    ) -> None:\n        \"\"\"Should be able to upsert as long as `is_default` is not changing\"\"\"\n        resources = [\n            r.model_dump(mode=\"json\") for r in getattr(DEFAULT_TAXONOMY, endpoint)[0:2]\n        ]\n        result = _api.upsert(\n            url=test_config.cli.server_url,\n            headers=test_config.user.auth_header,\n            resource_type=endpoint,\n            resources=resources,\n        )\n        assert result.status_code == 200\n\n    @pytest.mark.parametrize(\"endpoint\", TAXONOMY_ENDPOINTS)\n    def test_api_cannot_create_default_taxonomy(\n        self, test_config: FidesConfig, resources_dict: Dict, endpoint: str\n    ) -> None:\n        manifest = resources_dict[endpoint]\n\n        #  Set fields for default labels\n        manifest.is_default = True\n        manifest.version_added = \"2.0.0\"\n\n        result = _api.create(\n            url=test_config.cli.server_url,\n            resource_type=endpoint,\n            json_resource=manifest.json(exclude_none=True),\n            headers=test_config.user.auth_header,\n        )\n        assert result.status_code == 403\n        assert (\n            \"cannot create a resource where 'is_default' is true\"\n            in result.json()[\"detail\"][\"error\"]\n        )\n\n        _api.delete(\n            url=test_config.cli.server_url,\n            resource_type=endpoint,\n            resource_id=manifest.fides_key,\n            headers=test_config.user.auth_header,\n        )\n\n    @pytest.mark.parametrize(\"endpoint\", TAXONOMY_ENDPOINTS)\n    def test_api_cannot_upsert_default_taxonomy(\n        self, test_config: FidesConfig, resources_dict: Dict, endpoint: str\n    ) -> None:\n        manifest = resources_dict[endpoint]\n\n        #  Set fields for default labels\n        manifest.is_default = True\n        manifest.version_added = \"2.0.0\"\n\n        result = _api.upsert(\n            url=test_config.cli.server_url,\n            headers=test_config.user.auth_header,\n            resource_type=endpoint,\n            resources=[manifest.model_dump(mode=\"json\")],\n        )\n        assert result.status_code == 403\n        assert (\n            \"cannot create a resource where 'is_default' is true\"\n            in result.json()[\"detail\"][\"error\"]\n        )\n\n        _api.delete(\n            url=test_config.cli.server_url,\n            resource_type=endpoint,\n            resource_id=manifest.fides_key,\n            headers=test_config.user.auth_header,\n        )\n\n    @pytest.mark.parametrize(\"endpoint\", TAXONOMY_ENDPOINTS)\n    def test_api_cannot_update_is_default(\n        self, test_config: FidesConfig, resources_dict: Dict, endpoint: str\n    ) -> None:\n        manifest = resources_dict[endpoint]\n        _api.create(\n            url=test_config.cli.server_url,\n            resource_type=endpoint,\n            json_resource=manifest.json(exclude_none=True),\n            headers=test_config.user.auth_header,\n        )\n\n        #  Set fields for default labels\n        manifest.is_default = True\n        manifest.version_added = \"2.0.0\"\n\n        result = _api.update(\n            url=test_config.cli.server_url,\n            headers=test_config.user.auth_header,\n            resource_type=endpoint,\n            json_resource=manifest.json(exclude_none=True),\n        )\n        assert result.status_code == 403\n        assert (\n            \"cannot modify 'is_default' field on an existing resource\"\n            in result.json()[\"detail\"][\"error\"]\n        )\n\n    @pytest.mark.parametrize(\"endpoint\", TAXONOMY_ENDPOINTS)\n    def test_api_cannot_upsert_is_default(\n        self, test_config: FidesConfig, resources_dict: Dict, endpoint: str\n    ) -> None:\n        manifest = resources_dict[endpoint]\n        second_item = manifest.model_copy()\n\n        #  Set fields for default labels\n        manifest.is_default = True\n        manifest.version_added = \"2.0.0\"\n\n        second_item.is_default = False\n\n        _api.create(\n            url=test_config.cli.server_url,\n            resource_type=endpoint,\n            json_resource=second_item.json(exclude_none=True),\n            headers=test_config.user.auth_header,\n        )\n\n        result = _api.upsert(\n            url=test_config.cli.server_url,\n            headers=test_config.user.auth_header,\n            resource_type=endpoint,\n            resources=[\n                manifest.model_dump(mode=\"json\"),\n                second_item.model_dump(mode=\"json\"),\n            ],\n        )\n        assert result.status_code == 403\n        assert (\n            \"cannot modify 'is_default' field on an existing resource\"\n            in result.json()[\"detail\"][\"error\"]\n        )\n\n        _api.delete(\n            url=test_config.cli.server_url,\n            resource_type=endpoint,\n            resource_id=manifest.fides_key,\n            headers=test_config.user.auth_header,\n        )\n        _api.delete(\n            url=test_config.cli.server_url,\n            resource_type=endpoint,\n            resource_id=second_item.fides_key,\n            headers=test_config.user.auth_header,\n        )\n\n\n@pytest.mark.integration\nclass TestCrudActiveProperty:\n    \"\"\"\n    Ensure `active` property is exposed properly via CRUD endpoints.\n    Specific tests for this property since it's a fides-specific\n    extension to the underlying fideslang taxonomy models.\n    \"\"\"\n\n    @pytest.mark.parametrize(\"endpoint\", TAXONOMY_ENDPOINTS)\n    def test_api_can_update_active_on_default(\n        self, test_config: FidesConfig, endpoint: str\n    ) -> None:\n        \"\"\"Ensure we can toggle `active` property on default taxonomy elements\"\"\"\n        # Use the third element to avoid deactivating top-level items, which deactivates\n        # all their descendants and we'd need to manually re-activate each one.\n        resource = getattr(DEFAULT_TAXONOMY, endpoint)[2]\n        resource = TAXONOMY_EXTENSIONS[endpoint](\n            **resource.model_dump(mode=\"json\")\n        )  # cast resource to extended model\n        resource.active = False\n        json_resource = resource.json(exclude_none=True)\n        result = _api.update(\n            url=test_config.cli.server_url,\n            headers=test_config.user.auth_header,\n            resource_type=endpoint,\n            json_resource=json_resource,\n        )\n        assert result.status_code == 200\n        assert result.json()[\"active\"] is False\n\n        result = _api.get(\n            url=test_config.cli.server_url,\n            headers=test_config.user.auth_header,\n            resource_type=endpoint,\n            resource_id=resource.fides_key,\n        )\n        assert result.json()[\"active\"] is False\n\n        resource.active = True\n        json_resource = resource.json(exclude_none=True)\n        result = _api.update(\n            url=test_config.cli.server_url,\n            headers=test_config.user.auth_header,\n            resource_type=endpoint,\n            json_resource=json_resource,\n        )\n        assert result.status_code == 200\n        assert result.json()[\"active\"] is True\n\n        result = _api.get(\n            url=test_config.cli.server_url,\n            headers=test_config.user.auth_header,\n            resource_type=endpoint,\n            resource_id=resource.fides_key,\n        )\n        assert result.json()[\"active\"] is True\n\n    @pytest.mark.parametrize(\"endpoint\", TAXONOMY_ENDPOINTS)\n    def test_api_can_create_with_active_property(\n        self,\n        test_config: FidesConfig,\n        endpoint: str,\n        generate_auth_header,\n    ) -> None:\n        \"\"\"Ensure we can create taxonomy elements with `active` property set\"\"\"\n        # get a default taxonomy element as a sample resource\n        resource = getattr(DEFAULT_TAXONOMY, endpoint)[0]\n        resource = TAXONOMY_EXTENSIONS[endpoint](\n            **resource.model_dump(mode=\"json\")\n        )  # cast resource to extended model\n        resource.fides_key = resource.fides_key + \"_test_create_active_false\"\n        resource.name = resource.name + \"_test_create_active_false\"\n        resource.is_default = False\n        resource.version_added = None\n        resource.active = False\n        json_resource = resource.json(exclude_none=True)\n        token_scopes: List[str] = [f\"{CLI_SCOPE_PREFIX_MAPPING[endpoint]}:{CREATE}\"]\n        auth_header = generate_auth_header(scopes=token_scopes)\n        result = _api.create(\n            url=test_config.cli.server_url,\n            headers=auth_header,\n            resource_type=endpoint,\n            json_resource=json_resource,\n        )\n        assert result.status_code == 201\n        assert result.json()[\"active\"] is False\n\n        result = _api.get(\n            url=test_config.cli.server_url,\n            headers=test_config.user.auth_header,\n            resource_type=endpoint,\n            resource_id=resource.fides_key,\n        )\n        assert result.json()[\"active\"] is False\n\n        resource.fides_key = resource.fides_key + \"_test_create_active_true\"\n        resource.name = resource.name + \"_test_create_active_true\"\n        resource.is_default = False\n        resource.active = True\n        json_resource = resource.json(exclude_none=True)\n        token_scopes: List[str] = [f\"{CLI_SCOPE_PREFIX_MAPPING[endpoint]}:{CREATE}\"]\n        auth_header = generate_auth_header(scopes=token_scopes)\n        result = _api.create(\n            url=test_config.cli.server_url,\n            headers=auth_header,\n            resource_type=endpoint,\n            json_resource=json_resource,\n        )\n        assert result.status_code == 201\n        assert result.json()[\"active\"] is True\n\n        result = _api.get(\n            url=test_config.cli.server_url,\n            headers=test_config.user.auth_header,\n            resource_type=endpoint,\n            resource_id=resource.fides_key,\n        )\n        assert result.json()[\"active\"] is True\n\n\n@pytest.mark.integration\ndef test_static_sink(test_config: FidesConfig) -> None:\n    \"\"\"Make sure we are hosting something at / and not getting a 404\"\"\"\n    response = requests.get(f\"{test_config.cli.server_url}\")\n    assert response.status_code == 200\n\n\n@pytest.mark.integration\ndef test_404_on_api_routes(test_config: FidesConfig) -> None:\n    \"\"\"Should get a 404 on routes that start with API_PREFIX but do not exist\"\"\"\n    response = requests.get(\n        f\"{test_config.cli.server_url}{API_PREFIX}/path/that/does/not/exist\"\n    )\n    assert response.status_code == 404\n\n\n# Integration Tests\n@pytest.mark.integration\nclass TestHealthchecks:\n    @pytest.mark.parametrize(\n        \"database_health, expected_status_code\",\n        [(\"healthy\", 200), (\"unhealthy\", 503), (\"needs migration\", 503)],\n    )\n    def test_database_healthcheck(\n        self,\n        test_config: FidesConfig,\n        database_health: str,\n        expected_status_code: int,\n        monkeypatch: MonkeyPatch,\n        test_client: TestClient,\n    ) -> None:\n        \"\"\"Test the database health checks.\"\"\"\n\n        def mock_get_db_health(url: str, db) -> Tuple[str, str]:\n            return (\n                database_health,\n                \"9e83545ed9b6\",\n            )  # just an arbitrary revision # for testing\n\n        monkeypatch.setattr(health, \"get_db_health\", mock_get_db_health)\n        response = test_client.get(test_config.cli.server_url + \"/health/database\")\n        assert (\n            response.status_code == expected_status_code\n        ), f\"Request failed: {response.text}\"\n\n    def test_server_healthcheck(\n        self,\n        test_config: FidesConfig,\n        test_client: TestClient,\n    ) -> None:\n        \"\"\"Test the server healthcheck.\"\"\"\n        response = test_client.get(test_config.cli.server_url + \"/health\")\n        assert response.status_code == 200\n\n    def test_worker_healthcheck(\n        self,\n        test_config: FidesConfig,\n        test_client: TestClient,\n    ) -> None:\n        \"\"\"Test the server healthcheck.\"\"\"\n        response = test_client.get(test_config.cli.server_url + \"/health/workers\")\n        assert response.status_code == 200\n        assert response.json() == {\n            \"workers_enabled\": False,\n            \"workers\": [],\n            \"queue_counts\": {},\n        }\n\n    @pytest.mark.usefixtures(\"enable_celery_worker\")\n    def test_worker_health_check_with_workers_enabled(self, test_config, test_client):\n        response = test_client.get(test_config.cli.server_url + \"/health/workers\")\n        assert response.status_code == 200\n        # Workers not actually running in pytest though\n        assert response.json() == {\n            \"workers_enabled\": True,\n            \"workers\": [],\n            \"queue_counts\": {\n                \"fides.dsr\": 0,\n                \"fidesops.messaging\": 0,\n                \"fides.privacy_preferences\": 0,\n                \"fides\": 0,\n            },\n        }\n\n\n@pytest.mark.integration\n@pytest.mark.parametrize(\"endpoint_name\", [f\"{API_PREFIX}/organization\", \"/health\"])\ndef test_trailing_slash(test_config: FidesConfig, endpoint_name: str) -> None:\n    \"\"\"URLs both with and without a trailing slash should resolve and not 404\"\"\"\n    url = f\"{test_config.cli.server_url}{endpoint_name}\"\n    response = requests.get(url, headers=CONFIG.user.auth_header)\n    assert response.status_code == 200\n    response = requests.get(f\"{url}/\", headers=CONFIG.user.auth_header)\n    assert response.status_code == 200\n\n\nclass TestPrivacyDeclarationGetPurposeLegalBasisOverride:\n    async def test_privacy_declaration_enable_override_is_false(\n        self, async_session_temp\n    ):\n        \"\"\"Enable override is false so overridden legal basis is going to default\n        to the defined legal basis\"\"\"\n        resource = SystemSchema(\n            fides_key=str(uuid4()),\n            organization_fides_key=\"default_organization\",\n            name=f\"test_system_1_{uuid4()}\",\n            system_type=\"test\",\n            privacy_declarations=[\n                PrivacyDeclarationSchema(\n                    name=\"Collect data for content performance\",\n                    data_use=\"analytics.reporting.campaign_insights\",\n                    legal_basis_for_processing=\"Consent\",\n                    data_categories=[\"user\"],\n                )\n            ],\n        )\n\n        system = await create_system(\n            resource, async_session_temp, CONFIG.security.oauth_root_client_id\n        )\n        pd = system.privacy_declarations[0]\n\n        assert pd.purpose == 9\n        assert await pd.get_purpose_legal_basis_override() == \"Consent\"\n\n    @pytest.mark.usefixtures(\n        \"enable_override_vendor_purposes\",\n    )\n    async def test_enable_override_is_true_but_no_matching_purpose(\n        self, async_session_temp, db\n    ):\n        \"\"\"Privacy Declaration has Special Purpose not Purpose, so no overrides applicable\"\"\"\n        resource = SystemSchema(\n            fides_key=str(uuid4()),\n            organization_fides_key=\"default_organization\",\n            name=f\"test_system_1_{uuid4()}\",\n            system_type=\"test\",\n            privacy_declarations=[\n                PrivacyDeclarationSchema(\n                    name=\"Collect data for content performance\",\n                    data_use=\"essential.fraud_detection\",\n                    legal_basis_for_processing=\"Consent\",\n                    data_categories=[\"user\"],\n                )\n            ],\n        )\n\n        system = await create_system(\n            resource, async_session_temp, CONFIG.security.oauth_root_client_id\n        )\n        pd = system.privacy_declarations[0]\n\n        assert pd.purpose is None\n        assert await pd.get_purpose_legal_basis_override() == \"Consent\"\n\n    @pytest.mark.usefixtures(\n        \"enable_override_vendor_purposes\",\n    )\n    async def test_enable_override_is_true_but_purpose_is_excluded(\n        self, async_session_temp, db\n    ):\n        \"\"\"Purpose is overridden as excluded, so legal basis returns as None, to match\n        class-wide override\"\"\"\n        resource = SystemSchema(\n            fides_key=str(uuid4()),\n            organization_fides_key=\"default_organization\",\n            name=f\"test_system_1_{uuid4()}\",\n            system_type=\"test\",\n            privacy_declarations=[\n                PrivacyDeclarationSchema(\n                    name=\"Collect data for content performance\",\n                    data_use=\"personalize.content.profiling\",\n                    legal_basis_for_processing=\"Consent\",\n                    data_categories=[\"user\"],\n                )\n            ],\n        )\n\n        system = await create_system(\n            resource, async_session_temp, CONFIG.security.oauth_root_client_id\n        )\n        pd = system.privacy_declarations[0]\n\n        constraint = TCFPurposeOverride.create(\n            db,\n            data={\n                \"purpose\": 5,\n                \"is_included\": False,\n            },\n        )\n\n        assert pd.purpose == 5\n        assert await pd.get_purpose_legal_basis_override() is None\n\n        constraint.delete(db)\n\n    @pytest.mark.usefixtures(\n        \"enable_override_vendor_purposes\",\n    )\n    async def test_purpose_is_excluded_even_with_inflexible_legal_basis(\n        self, async_session_temp, db\n    ):\n        \"\"\"Purpose is overridden as excluded, so even if legal basis is not flexible,\n        legal basis returns as None, to match class-wide override.\"\"\"\n        resource = SystemSchema(\n            fides_key=str(uuid4()),\n            organization_fides_key=\"default_organization\",\n            name=f\"test_system_1_{uuid4()}\",\n            system_type=\"test\",\n            privacy_declarations=[\n                PrivacyDeclarationSchema(\n                    name=\"Collect data for content performance\",\n                    data_use=\"personalize.content.profiling\",\n                    legal_basis_for_processing=\"Consent\",\n                    flexible_legal_basis_for_processing=False,\n                    data_categories=[\"user\"],\n                )\n            ],\n        )\n\n        system = await create_system(\n            resource, async_session_temp, CONFIG.security.oauth_root_client_id\n        )\n        pd = system.privacy_declarations[0]\n\n        constraint = TCFPurposeOverride.create(\n            db,\n            data={\n                \"purpose\": 5,\n                \"is_included\": False,\n            },\n        )\n\n        assert pd.purpose == 5\n        assert await pd.get_purpose_legal_basis_override() is None\n\n        constraint.delete(db)\n\n    @pytest.mark.usefixtures(\n        \"enable_override_vendor_purposes\",\n    )\n    async def test_legal_basis_is_inflexible(self, async_session_temp, db):\n        \"\"\"Purpose is overridden but we can't apply because the legal basis is specified as inflexible\"\"\"\n        resource = SystemSchema(\n            fides_key=str(uuid4()),\n            organization_fides_key=\"default_organization\",\n            name=f\"test_system_1_{uuid4()}\",\n            system_type=\"test\",\n            privacy_declarations=[\n                PrivacyDeclarationSchema(\n                    name=\"Collect data for content performance\",\n                    data_use=\"personalize.content.profiling\",\n                    legal_basis_for_processing=\"Consent\",\n                    flexible_legal_basis_for_processing=False,\n                    data_categories=[\"user\"],\n                )\n            ],\n        )\n\n        system = await create_system(\n            resource, async_session_temp, CONFIG.security.oauth_root_client_id\n        )\n        pd = system.privacy_declarations[0]\n\n        constraint = TCFPurposeOverride.create(\n            db,\n            data={\n                \"purpose\": 5,\n                \"is_included\": True,\n                \"required_legal_basis\": \"Legitimate interests\",\n            },\n        )\n\n        assert pd.purpose == 5\n        assert await pd.get_purpose_legal_basis_override() == \"Consent\"\n\n        constraint.delete(db)\n\n    @pytest.mark.usefixtures(\n        \"enable_override_vendor_purposes\",\n    )\n    async def test_publisher_override_defined_but_no_required_legal_basis_specified(\n        self, db, async_session_temp\n    ):\n        \"\"\"Purpose override *object* is defined, but no legal basis override\"\"\"\n        resource = SystemSchema(\n            fides_key=str(uuid4()),\n            organization_fides_key=\"default_organization\",\n            name=f\"test_system_1_{uuid4()}\",\n            system_type=\"test\",\n            privacy_declarations=[\n                PrivacyDeclarationSchema(\n                    name=\"Collect data for content performance\",\n                    data_use=\"analytics.reporting.campaign_insights\",\n                    legal_basis_for_processing=\"Consent\",\n                    data_categories=[\"user\"],\n                )\n            ],\n        )\n\n        system = await create_system(\n            resource, async_session_temp, CONFIG.security.oauth_root_client_id\n        )\n        pd = system.privacy_declarations[0]\n\n        constraint = TCFPurposeOverride.create(\n            db,\n            data={\n                \"purpose\": 9,\n                \"is_included\": True,\n            },\n        )\n\n        assert pd.purpose == 9\n        assert await pd.get_purpose_legal_basis_override() == \"Consent\"\n\n        constraint.delete(db)\n\n    @pytest.mark.usefixtures(\n        \"enable_override_vendor_purposes\",\n    )\n    async def test_publisher_override_defined_with_required_legal_basis_specified(\n        self, async_session_temp, db\n    ):\n        \"\"\"Purpose override specified along with the requirements to apply that override\"\"\"\n        resource = SystemSchema(\n            fides_key=str(uuid4()),\n            organization_fides_key=\"default_organization\",\n            name=f\"test_system_1_{uuid4()}\",\n            system_type=\"test\",\n            privacy_declarations=[\n                PrivacyDeclarationSchema(\n                    name=\"Collect data for content performance\",\n                    data_use=\"functional.service.improve\",\n                    legal_basis_for_processing=\"Consent\",\n                    data_categories=[\"user\"],\n                )\n            ],\n        )\n\n        system = await create_system(\n            resource, async_session_temp, CONFIG.security.oauth_root_client_id\n        )\n        override = TCFPurposeOverride.create(\n            db,\n            data={\n                \"purpose\": 10,\n                \"is_included\": True,\n                \"required_legal_basis\": \"Legitimate interests\",\n            },\n        )\n        pd = system.privacy_declarations[0]\n\n        assert pd.purpose == 10\n        assert await pd.get_purpose_legal_basis_override() == \"Legitimate interests\"\n\n        override.delete(db)\n"}
{"type": "source_file", "path": "noxfile.py", "content": "\"\"\"\nThis file aggregates nox commands for various development tasks.\n\"\"\"\n\nimport platform\nimport shutil\nimport sys\nimport webbrowser\nfrom os.path import isfile\nfrom subprocess import PIPE, CalledProcessError, run\nfrom typing import List\n\nimport nox\n\nsys.path.append(\"noxfiles\")\n# pylint: disable=unused-wildcard-import, wildcard-import, wrong-import-position\nfrom ci_nox import *\nfrom dev_nox import *\nfrom docker_nox import *\nfrom docs_nox import *\nfrom git_nox import *\nfrom utils_nox import *\n\n# pylint: enable=unused-wildcard-import, wildcard-import, wrong-import-position\n\nREQUIRED_DOCKER_VERSION = \"20.10.17\"\nREQUIRED_PYTHON_VERSIONS = [\"3.9\", \"3.10\"]\n\nnox.options.sessions = [\"open_docs\"]\n\n# This is important for caching pip installs\nnox.options.reuse_existing_virtualenvs = True\n\n\n@nox.session()\ndef open_docs(session: nox.Session) -> None:\n    \"\"\"Open the webpage for the developer/contribution docs.\"\"\"\n    dev_url = \"http://localhost:8000/fides/development/developing_fides/\"\n    prod_url = \"https://ethyca.github.io/fides/dev/development/developing_fides/\"\n\n    if \"dev\" in session.posargs:\n        webbrowser.open(dev_url)\n    else:\n        webbrowser.open(prod_url)\n\n\n@nox.session()\ndef usage(session: nox.Session) -> None:\n    \"\"\"\n    Prints the documentation for a nox session provided as a posarg.\n\n    Example:\n        - 'nox -s usage -- <session>'\n    \"\"\"\n\n    if not session.posargs:\n        session.error(\"Please provide a session name, such as `clean`\")\n\n    session_target = session.posargs[0]\n\n    if not session_target in globals():\n        session.error(\n            \"Sorry, this isn't a valid nox session.\\nTry `nox -l` for a list of session names\"\n        )\n\n    session_object = globals()[session_target]\n    separator = \"-\" * 40\n    session.log(separator)\n\n    name_str = f\"Command: '{session_object.__name__}' \"\n    session.log(name_str)\n    session.log(separator)\n\n    session.log(f\"Module Location: '{session_object.__module__}'\")\n    session.log(separator)\n\n    # This cleaning step helps the docstring properly align left in the terminal\n    cleaned_docstring = session_object.__doc__.lstrip().rstrip().replace(\"    \", \"\")\n    session.log(\"Docstring:\\n\" + cleaned_docstring)\n    session.log(separator)\n\n\ndef check_for_env_file() -> None:\n    \"\"\"Create a .env file if none exists.\"\"\"\n    env_file_example = \"example.env\"\n    env_file = \".env\"\n    if not isfile(env_file):\n        print(\n            f\"Creating env file for local testing & development from {env_file_example}: {env_file}\"\n        )\n        shutil.copy(env_file_example, env_file)\n\n\ndef convert_semver_to_list(semver: str) -> List[int]:\n    \"\"\"\n    Convert a standard semver string to a list of ints\n\n    '2.10.7' -> [2,10,7]\n    \"\"\"\n    return [int(x) for x in semver.split(\".\")]\n\n\ndef compare_semvers(version_a: List[int], version_b: List[int]) -> bool:\n    \"\"\"\n    Determine which semver-style list of integers is larger.\n\n    [2,10,3], [2,10,2] -> True\n    [3,1,3], [2,10,2] -> True\n    [2,10,2], [2,10,2] -> True\n    [1,1,3], [2,10,2] -> False\n    \"\"\"\n\n    major, minor, patch = [0, 1, 2]\n    # Compare Major Versions\n    if version_a[major] > version_b[major]:\n        return True\n    if version_a[major] < version_b[major]:\n        return False\n\n    # If Major Versions Match, Compare Minor Versions\n    if version_a[minor] > version_b[minor]:\n        return True\n    if version_a[minor] < version_b[minor]:\n        return False\n\n    # If Both Major & Minor Versions Match, Compare Patch Versions\n    if version_a[patch] < version_b[patch]:\n        return False\n\n    return True\n\n\ndef check_docker_version() -> bool:\n    \"\"\"Verify the Docker version.\"\"\"\n    try:\n        raw = run(\"docker --version\", stdout=PIPE, check=True, shell=True)\n    except CalledProcessError:\n        raise SystemExit(\"Error: Command 'docker' is not available.\")\n\n    parsed = raw.stdout.decode(\"utf-8\").rstrip(\"\\n\")\n    # We need to handle multiple possible version formats here\n    # Docker version 20.10.17, build 100c701\n    # Docker version 20.10.18+azure-1, build b40c2f6\n    docker_version = parsed.split(\"version \")[-1].split(\",\")[0].split(\"+\")[0]\n    print(parsed)\n\n    split_docker_version = convert_semver_to_list(docker_version)\n    split_required_docker_version = convert_semver_to_list(REQUIRED_DOCKER_VERSION)\n\n    if len(split_docker_version) != 3:\n        raise SystemExit(\n            \"Error: Docker version format is invalid, expecting semver format. Please upgrade to a more recent version and try again\"\n        )\n\n    version_is_valid = compare_semvers(\n        split_docker_version, split_required_docker_version\n    )\n    if not version_is_valid:\n        raise SystemExit(\n            f\"Error: Your Docker version ({docker_version}) is not compatible, please update to at least version {REQUIRED_DOCKER_VERSION}!\"\n        )\n    return version_is_valid\n\n\ndef check_python_version() -> bool:\n    \"\"\"Verify the Python version.\"\"\"\n    python_version = platform.python_version()\n    print(platform.platform())\n    print(f\"Python version {python_version}\")\n\n    python_major_minor_version = \".\".join(platform.python_version_tuple()[0:2])\n    if python_major_minor_version not in REQUIRED_PYTHON_VERSIONS:\n        raise SystemExit(\n            f\"Error: Your Python version ({python_version}) is not compatible, please install one of the following versions: {REQUIRED_PYTHON_VERSIONS}!\"\n        )\n\n\n# Run startup checks\ncheck_docker_version()\ncheck_python_version()\ncheck_for_env_file()\n"}
{"type": "source_file", "path": "noxfiles/ci_nox.py", "content": "\"\"\"Contains the nox sessions used during CI checks.\"\"\"\n\nfrom functools import partial\nfrom typing import Callable, Dict\n\nimport nox\nfrom nox.command import CommandFailed\n\nfrom constants_nox import (\n    CONTAINER_NAME,\n    IMAGE_NAME,\n    LOGIN,\n    RUN_NO_DEPS,\n    START_APP,\n    WITH_TEST_CONFIG,\n)\nfrom setup_tests_nox import pytest_ctl, pytest_lib, pytest_nox, pytest_ops\nfrom utils_nox import install_requirements\n\n\n###################\n## Static Checks ##\n###################\n@nox.session()\ndef static_checks(session: nox.Session) -> None:\n    \"\"\"Run the static checks only.\"\"\"\n    session.notify(\"black(fix)\")\n    session.notify(\"isort(fix)\")\n    session.notify(\"xenon\")\n    session.notify(\"mypy\")\n    session.notify(\"pylint\")\n\n\n@nox.session()\n@nox.parametrize(\n    \"mode\",\n    [\n        nox.param(\"check\", id=\"check\"),\n        nox.param(\"fix\", id=\"fix\"),\n    ],\n)\ndef black(session: nox.Session, mode: str) -> None:\n    \"\"\"Run the 'black' style linter.\"\"\"\n    install_requirements(session)\n    command = (\"black\", \"src\", \"tests\", \"noxfiles\", \"scripts\", \"noxfile.py\")\n    if session.posargs:\n        command = (\"black\", *session.posargs)\n    if mode == \"check\":\n        command = (*command, \"--check\")\n    session.run(*command)\n\n\n@nox.session()\n@nox.parametrize(\n    \"mode\",\n    [\n        nox.param(\"check\", id=\"check\"),\n        nox.param(\"fix\", id=\"fix\"),\n    ],\n)\ndef isort(session: nox.Session, mode: str) -> None:\n    \"\"\"Run the 'isort' import linter.\"\"\"\n    install_requirements(session)\n    command = (\"isort\", \"src\", \"tests\", \"noxfiles\", \"scripts\", \"noxfile.py\")\n    if session.posargs:\n        command = (\"isort\", *session.posargs)\n    if mode == \"check\":\n        command = (*command, \"--check\")\n    session.run(*command)\n\n\n@nox.session()\ndef mypy(session: nox.Session) -> None:\n    \"\"\"Run the 'mypy' static type checker.\"\"\"\n    install_requirements(session)\n    command = \"mypy\"\n    session.run(command)\n\n\n@nox.session()\ndef pylint(session: nox.Session) -> None:\n    \"\"\"Run the 'pylint' code linter.\"\"\"\n    install_requirements(session)\n    command = (\"pylint\", \"src\", \"noxfiles\", \"noxfile.py\", \"--jobs\", \"0\")\n    session.run(*command)\n\n\n@nox.session()\ndef xenon(session: nox.Session) -> None:\n    \"\"\"Run 'xenon' code complexity monitoring.\"\"\"\n    install_requirements(session)\n    command = (\n        \"xenon\",\n        \"noxfiles\",\n        \"src\",\n        \"tests\",\n        \"scripts\",\n        \"--max-absolute=B\",\n        \"--max-modules=B\",\n        \"--max-average=A\",\n        \"--ignore=data,docs\",\n        \"--exclude=src/fides/_version.py\",\n    )\n    session.run(*command, success_codes=[0, 1])\n    session.warn(\n        \"Note: This command was malformed so it's been failing to report complexity issues.\"\n    )\n    session.warn(\n        \"Intentionally suppressing the error status code for now to slowly work through the issues.\"\n    )\n\n\n##################\n## Fides Checks ##\n##################\n@nox.session()\ndef check_install(session: nox.Session) -> None:\n    \"\"\"\n    Check that fides installs and works correctly.\n\n    This is also a good sanity check for correct syntax.\n    \"\"\"\n    session.install(\".\")\n\n    REQUIRED_ENV_VARS = {\n        \"FIDES__SECURITY__APP_ENCRYPTION_KEY\": \"OLMkv91j8DHiDAULnK5Lxx3kSCov30b3\",\n        \"FIDES__SECURITY__OAUTH_ROOT_CLIENT_ID\": \"fidesadmin\",\n        \"FIDES__SECURITY__OAUTH_ROOT_CLIENT_SECRET\": \"fidesadminsecret\",\n        \"FIDES__SECURITY__DRP_JWT_SECRET\": \"secret\",\n    }\n\n    run_command = (\"fides\", \"--version\")\n    session.run(*run_command, env=REQUIRED_ENV_VARS)\n\n    run_command = (\"python\", \"-c\", \"from fides.api.main import start_webserver\")\n    session.run(*run_command, env=REQUIRED_ENV_VARS)\n\n\n@nox.session()\ndef check_fides_annotations(session: nox.Session) -> None:\n    \"\"\"Run a fides evaluation.\"\"\"\n    run_command = (*RUN_NO_DEPS, \"fides\", \"--local\", *(WITH_TEST_CONFIG), \"evaluate\")\n    session.run(*run_command, external=True)\n\n\n@nox.session()\ndef fides_db_scan(session: nox.Session) -> None:\n    \"\"\"Scan the fides application database to check for dataset discrepancies.\"\"\"\n    session.notify(\"teardown\")\n    session.run(*START_APP, external=True)\n    scan_command = (\n        \"docker\",\n        \"container\",\n        \"exec\",\n        CONTAINER_NAME,\n        \"fides\",\n        \"scan\",\n        \"dataset\",\n        \"db\",\n        \"--credentials-id\",\n        \"app_postgres\",\n    )\n    session.run(*LOGIN, external=True)\n    session.run(*scan_command, external=True)\n\n\n@nox.session()\ndef check_container_startup(session: nox.Session) -> None:\n    \"\"\"\n    Start the containers in `wait` mode. If container startup fails, show logs.\n    \"\"\"\n    throw_error = False\n    start_command = (\n        \"docker\",\n        \"compose\",\n        \"up\",\n        \"--wait\",\n        IMAGE_NAME,\n    )\n    healthcheck_logs_command = (\n        \"docker\",\n        \"inspect\",\n        \"--format\",\n        '\"{{json .State.Health }}\"',\n        IMAGE_NAME,\n    )\n    startup_logs_command = (\n        \"docker\",\n        \"logs\",\n        \"--tail\",\n        \"50\",\n        IMAGE_NAME,\n    )\n    try:\n        session.run(*start_command, external=True)\n    except CommandFailed:\n        throw_error = True\n\n    # We want to see the logs regardless of pass/failure, just in case\n    log_dashes = \"*\" * 20\n    session.log(f\"{log_dashes} Healthcheck Logs {log_dashes}\")\n    session.run(*healthcheck_logs_command, external=True)\n    session.log(f\"{log_dashes} Startup Logs {log_dashes}\")\n    session.run(*startup_logs_command, external=True)\n\n    if throw_error:\n        session.error(\"Container startup failed\")\n\n\n@nox.session()\ndef minimal_config_startup(session: nox.Session) -> None:\n    \"\"\"\n    Check that the server can start successfully with a minimal\n    configuration set through environment vairables.\n    \"\"\"\n    session.notify(\"teardown\")\n    compose_file = \"docker/docker-compose.minimal-config.yml\"\n    start_command = (\n        \"docker\",\n        \"compose\",\n        \"-f\",\n        compose_file,\n        \"up\",\n        \"--wait\",\n        IMAGE_NAME,\n    )\n    session.run(*start_command, external=True)\n\n\n#################\n## Performance ##\n#################\n@nox.session()\ndef performance_tests(session: nox.Session) -> None:\n    \"\"\"Compose the various performance checks into a single uber-test.\"\"\"\n    session.notify(\"teardown\")\n    session.run(*START_APP, external=True, silent=True)\n    samples = 2\n    for i in range(samples):\n        session.log(f\"Sample {i + 1} of {samples}\")\n        load_tests(session)\n        docker_stats(session)\n\n\n@nox.session()\ndef docker_stats(session: nox.Session) -> None:\n    \"\"\"\n    Use the builtin `docker stats` command to show resource usage.\n\n    Run this _last_ to get a better worst-case scenario\n    \"\"\"\n    session.run(\n        \"docker\",\n        \"stats\",\n        \"--no-stream\",\n        \"--format\",\n        \"table {{.Name}}\\t{{.CPUPerc}}\\t{{.MemUsage}}\",\n        external=True,\n    )\n\n\n@nox.session()\ndef load_tests(session: nox.Session) -> None:\n    \"\"\"\n    Load test the application.\n\n    Requires a Rust/Cargo installation and then `cargo install drill`\n\n    https://github.com/fcsonline/drill\n    \"\"\"\n    session.run(\n        \"drill\", \"-b\", \"noxfiles/drill.yml\", \"--quiet\", \"--stats\", external=True\n    )\n\n\n############\n## Pytest ##\n############\nTEST_GROUPS = [\n    nox.param(\"ctl-unit\", id=\"ctl-unit\"),\n    nox.param(\"ctl-not-external\", id=\"ctl-not-external\"),\n    nox.param(\"ctl-integration\", id=\"ctl-integration\"),\n    nox.param(\"ctl-external\", id=\"ctl-external\"),\n    nox.param(\"ops-unit\", id=\"ops-unit\"),\n    nox.param(\"ops-unit-api\", id=\"ops-unit-api\"),\n    nox.param(\"ops-unit-non-api\", id=\"ops-unit-non-api\"),\n    nox.param(\"ops-integration\", id=\"ops-integration\"),\n    nox.param(\"ops-external-datastores\", id=\"ops-external-datastores\"),\n    nox.param(\"ops-saas\", id=\"ops-saas\"),\n    nox.param(\"lib\", id=\"lib\"),\n    nox.param(\"nox\", id=\"nox\"),\n]\n\nTEST_MATRIX: Dict[str, Callable] = {\n    \"ctl-unit\": partial(pytest_ctl, mark=\"unit\"),\n    \"ctl-not-external\": partial(pytest_ctl, mark=\"not external\"),\n    \"ctl-integration\": partial(pytest_ctl, mark=\"integration\"),\n    \"ctl-external\": partial(pytest_ctl, mark=\"external\"),\n    \"ops-unit\": partial(pytest_ops, mark=\"unit\"),\n    \"ops-unit-api\": partial(pytest_ops, mark=\"unit\", subset_dir=\"api\"),\n    \"ops-unit-non-api\": partial(pytest_ops, mark=\"unit\", subset_dir=\"non-api\"),\n    \"ops-integration\": partial(pytest_ops, mark=\"integration\"),\n    \"ops-external-datastores\": partial(pytest_ops, mark=\"external_datastores\"),\n    \"ops-saas\": partial(pytest_ops, mark=\"saas\"),\n    \"lib\": pytest_lib,\n    \"nox\": pytest_nox,\n}\n\n\ndef validate_test_matrix(session: nox.Session) -> None:\n    \"\"\"\n    Validates that all test groups are represented in the test matrix.\n    \"\"\"\n    test_group_ids = sorted([str(param) for param in TEST_GROUPS])\n    test_matrix_keys = sorted(TEST_MATRIX.keys())\n\n    if not test_group_ids == test_matrix_keys:\n        session.error(\"TEST_GROUPS and TEST_MATRIX do not match\")\n\n\n@nox.session()\ndef collect_tests(session: nox.Session) -> None:\n    \"\"\"\n    Collect all pytests as a validity check.\n\n    Good to run as a sanity check that there aren't any obvious syntax\n    errors within the test code.\n    \"\"\"\n    session.install(\".\")\n    install_requirements(session, True)\n    command = (\"pytest\", \"tests/\", \"--collect-only\")\n    session.run(*command)\n\n\n@nox.session()\n@nox.parametrize(\n    \"test_group\",\n    TEST_GROUPS,\n)\ndef pytest(session: nox.Session, test_group: str) -> None:\n    \"\"\"\n    Runs Pytests.\n\n    As new TEST_GROUPS are added, the TEST_MATRIX must also be updated.\n    \"\"\"\n    session.notify(\"teardown\")\n\n    validate_test_matrix(session)\n    coverage_arg = \"--cov-report=xml\"\n    TEST_MATRIX[test_group](session=session, coverage_arg=coverage_arg)\n\n\n@nox.session()\n@nox.parametrize(\n    \"dist\",\n    [\n        nox.param(\"sdist\", id=\"source\"),\n        nox.param(\"bdist_wheel\", id=\"wheel\"),\n    ],\n)\ndef python_build(session: nox.Session, dist: str) -> None:\n    \"Build the Python distribution.\"\n    session.run(\n        *RUN_NO_DEPS,\n        \"python\",\n        \"setup.py\",\n        dist,\n        external=True,\n    )\n"}
{"type": "source_file", "path": "noxfiles/constants_nox.py", "content": "\"\"\"Define constants to be used across the noxfiles.\"\"\"\n\nfrom os import getcwd, getenv\n\n# Files\nCOMPOSE_FILE = \"docker-compose.yml\"\nINTEGRATION_COMPOSE_FILE = \"./docker-compose.integration-tests.yml\"\nINTEGRATION_POSTGRES_COMPOSE_FILE = \"./docker/docker-compose.integration-postgres.yml\"\nREMOTE_DEBUG_COMPOSE_FILE = \"docker-compose.remote-debug.yml\"\nSAMPLE_PROJECT_COMPOSE_FILE = \"./src/fides/data/sample_project/docker-compose.yml\"\nWITH_TEST_CONFIG = (\"-f\", \"tests/ctl/test_config.toml\")\n\nCOMPOSE_FILE_LIST = {\n    COMPOSE_FILE,\n    SAMPLE_PROJECT_COMPOSE_FILE,\n    INTEGRATION_COMPOSE_FILE,\n    \"docker/docker-compose.integration-mariadb.yml\",\n    \"docker/docker-compose.integration-mongodb.yml\",\n    \"docker/docker-compose.integration-mysql.yml\",\n    \"docker/docker-compose.integration-postgres.yml\",\n    \"docker/docker-compose.integration-mssql.yml\",\n}\n\n# Image Names & Tags\nREGISTRY = \"ethyca\"\nIMAGE_NAME = \"fides\"\nCONTAINER_NAME = \"fides\"\nCOMPOSE_SERVICE_NAME = \"fides\"\n\n# Image Names & Tags\nREGISTRY = \"ethyca\"\nIMAGE_NAME = \"fides\"\nIMAGE = f\"{REGISTRY}/{IMAGE_NAME}\"\nIMAGE_LOCAL = f\"{IMAGE}:local\"\nIMAGE_LOCAL_UI = f\"{IMAGE}:local-ui\"\nIMAGE_LOCAL_PC = f\"{IMAGE}:local-pc\"\n\n# Image names for the secondary apps\nPRIVACY_CENTER_IMAGE = f\"{REGISTRY}/fides-privacy-center\"\nSAMPLE_APP_IMAGE = f\"{REGISTRY}/fides-sample-app\"\n\n# Constant tag suffixes for published images\nDEV_TAG_SUFFIX = \"dev\"\nPRERELEASE_TAG_SUFFIX = \"prerelease\"\nRC_TAG_SUFFIX = \"rc\"\nLATEST_TAG_SUFFIX = \"latest\"\n\n# Image names for 3rd party apps\nCYPRESS_IMAGE = \"cypress/included:12.8.1\"\n\n# Helpful paths\nCWD = getcwd()\n\n# Disable TTY to preserve output within Github Actions logs\n# CI env variable is always set to true in Github Actions\n# The else statement is required due to the way commands are structured and is arbitrary.\nCI_ARGS = \"-T\" if getenv(\"CI\") else \"--user=fidesuser\"\nCI_ARGS_EXEC = \"-t\" if not getenv(\"CI\") else \"--user=fidesuser\"\n\n# If FIDES__CLI__ANALYTICS_ID is set in the local environment, use its value as the analytics_id\nANALYTICS_ID_OVERRIDE = (\"-e\", \"FIDES__CLI__ANALYTICS_ID\")\nANALYTICS_OPT_OUT = (\"-e\", \"ANALYTICS_OPT_OUT\")\n\n# Reusable Commands\nLOGIN = (\n    \"docker\",\n    \"exec\",\n    CONTAINER_NAME,\n    \"fides\",\n    \"user\",\n    \"login\",\n    \"-u\",\n    \"root_user\",\n    \"-p\",\n    \"Testpassword1!\",\n)\nEXEC = (\n    \"docker\",\n    \"exec\",\n    *ANALYTICS_OPT_OUT,\n    *ANALYTICS_ID_OVERRIDE,\n    CI_ARGS_EXEC,\n    CONTAINER_NAME,\n)\nEXEC_IT = (\n    \"docker\",\n    \"exec\",\n    \"-it\",\n    *ANALYTICS_OPT_OUT,\n    *ANALYTICS_ID_OVERRIDE,\n    CI_ARGS_EXEC,\n    CONTAINER_NAME,\n)\nRUN_NO_DEPS = (\n    \"docker\",\n    \"compose\",\n    \"run\",\n    \"--no-deps\",\n    \"--rm\",\n    *ANALYTICS_ID_OVERRIDE,\n    CI_ARGS,\n    IMAGE_NAME,\n)\nSTART_APP = (\"docker\", \"compose\", \"up\", \"--wait\", COMPOSE_SERVICE_NAME)\nSTART_APP_EXTERNAL = (\n    \"docker\",\n    \"compose\",\n    \"-f\",\n    COMPOSE_FILE,\n    \"-f\",\n    INTEGRATION_COMPOSE_FILE,\n    \"up\",\n    \"--wait\",\n    COMPOSE_SERVICE_NAME,\n)\nSTART_APP_REMOTE_DEBUG = (\n    \"docker\",\n    \"compose\",\n    \"-f\",\n    COMPOSE_FILE,\n    \"-f\",\n    REMOTE_DEBUG_COMPOSE_FILE,\n    \"up\",\n    COMPOSE_SERVICE_NAME,\n)\nSTART_APP_WITH_EXTERNAL_POSTGRES = (\n    \"docker\",\n    \"compose\",\n    \"-f\",\n    COMPOSE_FILE,\n    \"-f\",\n    INTEGRATION_POSTGRES_COMPOSE_FILE,\n    \"up\",\n    \"--wait\",\n    \"fides\",\n    \"postgres_example\",\n)\nRUN_CYPRESS_TESTS = (\n    \"docker\",\n    \"run\",\n    \"-t\",\n    \"--network=host\",\n    \"-v\",\n    f\"{CWD}/clients/cypress-e2e:/cypress-e2e\",\n    \"-w\",\n    \"/cypress-e2e\",\n    \"--entrypoint=\",\n    \"-e\",\n    \"CYPRESS_VIDEO=false\",\n    CYPRESS_IMAGE,\n    \"/bin/bash\",\n    \"-c\",\n    \"npm install && cypress run\",\n)\n"}
{"type": "source_file", "path": "noxfiles/dev_nox.py", "content": "\"\"\"Contains the nox sessions for running development environments.\"\"\"\n\nimport time\nfrom pathlib import Path\nfrom typing import Literal\n\nfrom nox import Session, param, parametrize\nfrom nox import session as nox_session\nfrom nox.command import CommandFailed\n\nfrom constants_nox import (\n    COMPOSE_SERVICE_NAME,\n    EXEC_IT,\n    RUN_CYPRESS_TESTS,\n    START_APP,\n    START_APP_REMOTE_DEBUG,\n)\nfrom docker_nox import build\nfrom run_infrastructure import ALL_DATASTORES, run_infrastructure\nfrom utils_nox import install_requirements, teardown\n\n\n@nox_session()\ndef shell(session: Session) -> None:\n    \"\"\"\n    Open a shell in an already-running Fides webserver container.\n\n    If the container is not running, the command will fail.\n    \"\"\"\n    shell_command = (*EXEC_IT, \"/bin/bash\")\n    try:\n        session.run(*shell_command, external=True)\n    except CommandFailed:\n        session.error(\n            \"Could not connect to the webserver container. Please confirm it is running and try again.\"\n        )\n\n\n# pylint: disable=too-many-branches\n@nox_session()\ndef dev(session: Session) -> None:\n    \"\"\"\n    Spin up the Fides webserver in development mode.\n    Includes the Postgres database and Redis cache.\n\n    Use positional arguments to run other services like the privacy center & admin UI.\n    Positional arguments can be combined and in any order.\n\n    Positional Arguments:\n        - shell = Open a shell on the Fides webserver\n        - ui = Build and run the Admin UI\n        - pc = Build and run the Privacy Center\n        - remote_debug = Run with remote debugging enabled (see docker-compose.remote-debug.yml)\n        - worker = Run a Fides worker\n        - flower = Run Flower monitoring dashboard for Celery\n        - child = Run a Fides child node\n        - <datastore(s)> = Run a test datastore (e.g. 'mssql', 'mongodb')\n\n    Parameters:\n        N/A\n    \"\"\"\n\n    build(session, \"dev\")\n    session.notify(\"teardown\")\n\n    workers = [\"worker\", \"worker-privacy-preferences\", \"worker-dsr\"]\n\n    for worker in workers:\n        if worker in session.posargs:\n            session.run(\"docker\", \"compose\", \"up\", \"--wait\", worker, external=True)\n\n    if \"flower\" in session.posargs:\n        # Only start Flower if worker is also enabled\n        if any(worker in session.posargs for worker in workers):\n            session.run(\"docker\", \"compose\", \"up\", \"-d\", \"flower\", external=True)\n        else:\n            session.error(\n                \"Flower requires the worker service. Please add 'worker' to your arguments.\"\n            )\n\n    datastores = [\n        datastore for datastore in session.posargs if datastore in ALL_DATASTORES\n    ] or None\n\n    if \"child\" in session.posargs:\n        session.run(\n            \"docker\",\n            \"compose\",\n            \"-f\",\n            \"docker-compose.child-env.yml\",\n            \"up\",\n            \"-d\",\n            external=True,\n        )\n\n    if \"ui\" in session.posargs:\n        build(session, \"admin_ui\")\n        session.run(\"docker\", \"compose\", \"up\", \"-d\", \"fides-ui\", external=True)\n\n    if \"pc\" in session.posargs:\n        build(session, \"privacy_center\")\n        session.run(\"docker\", \"compose\", \"up\", \"-d\", \"fides-pc\", external=True)\n\n    open_shell = \"shell\" in session.posargs\n    remote_debug = \"remote_debug\" in session.posargs\n    if not datastores:\n        if open_shell:\n            session.run(*START_APP, external=True)\n            session.log(\"~~Remember to login with `fides user login`!~~\")\n            session.run(*EXEC_IT, \"/bin/bash\", external=True)\n        else:\n            if remote_debug:\n                session.run(*START_APP_REMOTE_DEBUG, external=True)\n            else:\n                session.run(\n                    \"docker\", \"compose\", \"up\", COMPOSE_SERVICE_NAME, external=True\n                )\n    else:\n        # Run the webserver with additional datastores\n        run_infrastructure(\n            open_shell=open_shell,\n            run_application=True,\n            datastores=datastores,\n            remote_debug=remote_debug,\n        )\n\n\n@nox_session()\ndef cypress_tests(session: Session) -> None:\n    \"\"\"\n    End-to-end Cypress tests designed to be run as part of the 'e2e_test' session.\n    \"\"\"\n    session.log(\"Running Cypress tests...\")\n    session.run(*RUN_CYPRESS_TESTS, external=True)\n\n\n@nox_session()\ndef e2e_test(session: Session) -> None:\n    \"\"\"\n    Spins up the fides_env session and runs Cypress E2E tests against it.\n    \"\"\"\n    session.log(\"Running end-to-end tests...\")\n    session.notify(\"fides_env(test)\", posargs=[\"keep_alive\"])\n    session.notify(\"cypress_tests\")\n    session.notify(\"teardown\")\n\n\n@nox_session()\n@parametrize(\n    \"fides_image\",\n    [\n        param(\"dev\", id=\"dev\"),\n        param(\"test\", id=\"test\"),\n    ],\n)\ndef fides_env(session: Session, fides_image: Literal[\"test\", \"dev\"] = \"test\") -> None:\n    \"\"\"\n    Spins up a full fides environment seeded with data.\n\n    Params:\n        dev = Spins up a full fides application with a dev-style docker container.\n              This includes hot-reloading and no pre-baked UI.\n\n        test = Spins up a full fides application with a production-style docker\n               container. This includes the UI being pre-built as static files.\n\n    Posargs:\n        keep_alive = does not automatically call teardown after the session\n    \"\"\"\n    keep_alive = \"keep_alive\" in session.posargs\n    if fides_image == \"dev\":\n        session.error(\n            \"'fides_env(dev)' is not currently implemented! Use 'nox -s dev' to run the server in dev mode. \"\n            \"Currently unclear how to (cleanly) mount the source code into the running container...\"\n        )\n\n    # Record timestamps along the way, so we can generate a build-time report\n    timestamps = []\n    timestamps.append({\"time\": time.monotonic(), \"label\": \"Start\"})\n\n    session.log(\"Tearing down existing containers & volumes...\")\n    try:\n        teardown(session, volumes=True)\n    except CommandFailed:\n        session.error(\"Failed to cleanly teardown. Please try again!\")\n    timestamps.append({\"time\": time.monotonic(), \"label\": \"Docker Teardown\"})\n\n    session.log(\"Building production images with 'build(test)'...\")\n    build(session, \"test\")\n    timestamps.append({\"time\": time.monotonic(), \"label\": \"Docker Build\"})\n\n    session.log(\"Installing ethyca-fides locally...\")\n    install_requirements(session)\n    session.install(\"-e\", \".\", \"--no-deps\")\n    session.run(\"fides\", \"--version\")\n    timestamps.append({\"time\": time.monotonic(), \"label\": \"pip install\"})\n\n    # Configure the args for 'fides deploy up' for testing\n    env_file_path = Path(__file__, \"../../.env\").resolve()\n    fides_deploy_args = [\n        \"--no-pull\",\n        \"--no-init\",\n        \"--env-file\",\n        str(env_file_path),\n    ]\n\n    session.log(\"Deploying test environment with 'fides deploy up'...\")\n    session.log(\n        f\"NOTE: Customize your local Fides configuration via ENV file here: {env_file_path}\"\n    )\n    session.run(\n        \"fides\",\n        \"deploy\",\n        \"up\",\n        *fides_deploy_args,\n    )\n    timestamps.append({\"time\": time.monotonic(), \"label\": \"fides deploy\"})\n\n    # Log a quick build-time report to help troubleshoot slow builds\n    session.log(\"[fides_env]: Ready! Build time report:\")\n    session.log(f\"{'Step':5} | {'Label':20} | Time\")\n    session.log(\"------+----------------------+------\")\n    for index, value in enumerate(timestamps):\n        if index == 0:\n            continue\n        session.log(\n            f\"{index:5} | {value['label']:20} | {value['time'] - timestamps[index-1]['time']:.2f}s\"\n        )\n    session.log(\n        f\"      | {'Total':20} | {timestamps[-1]['time'] - timestamps[0]['time']:.2f}s\"\n    )\n    session.log(\"------+----------------------+------\\n\")\n\n    # Start a shell session unless 'keep_alive' is provided as a posarg\n    if not keep_alive:\n        session.log(\"Opening Fides CLI shell... (press CTRL+D to exit)\")\n        session.run(*EXEC_IT, \"/bin/bash\", external=True, success_codes=[0, 1])\n        session.run(\"fides\", \"deploy\", \"down\")\n\n\n@nox_session()\ndef quickstart(session: Session) -> None:\n    \"\"\"Run the quickstart tutorial.\"\"\"\n    build(session, \"dev\")\n    build(session, \"privacy_center\")\n    build(session, \"admin_ui\")\n    session.notify(\"teardown\")\n    run_infrastructure(datastores=[\"mongodb\", \"postgres\"], run_quickstart=True)\n\n\n@nox_session()\n@parametrize(\n    \"action\",\n    [\n        param(\"dry\", id=\"dry\"),\n        param(\"live\", id=\"live\"),\n    ],\n)\ndef delete_old_test_pypi_packages(session: Session, action: str) -> None:\n    \"\"\"\n    Delete old (specifically, >1 year old) packages from the test pypi repository.\n    \"\"\"\n    session.install(\"pypi-cleanup\")\n\n    if action == \"dry\":\n        session.run(\n            \"pypi-cleanup\",\n            \"-u\",\n            \"fides-ethyca\",\n            \"-p\",\n            \"ethyca-fides\",\n            \"-t\",\n            \"https://test.pypi.org\",\n            \"-d\",\n            \"365\",\n            \"-y\",\n        )\n    elif action == \"live\":\n        session.run(\n            \"pypi-cleanup\",\n            \"-u\",\n            \"fides-ethyca\",\n            \"-p\",\n            \"ethyca-fides\",\n            \"-t\",\n            \"https://test.pypi.org\",\n            \"-d\",\n            \"365\",\n            \"-y\",\n            \"--do-it\",\n        )\n"}
{"type": "source_file", "path": "noxfiles/docker_nox.py", "content": "\"\"\"Contains the nox sessions for docker-related tasks.\"\"\"\n\nfrom typing import Callable, Dict, List, Optional, Tuple\n\nimport nox\n\nfrom constants_nox import (\n    DEV_TAG_SUFFIX,\n    IMAGE,\n    IMAGE_LOCAL,\n    IMAGE_LOCAL_PC,\n    IMAGE_LOCAL_UI,\n    PRERELEASE_TAG_SUFFIX,\n    PRIVACY_CENTER_IMAGE,\n    RC_TAG_SUFFIX,\n    SAMPLE_APP_IMAGE,\n)\nfrom git_nox import get_current_tag, recognized_tag\n\nDOCKER_PLATFORMS = \"linux/amd64,linux/arm64\"\n\n\ndef verify_git_tag(session: nox.Session) -> Optional[str]:\n    \"\"\"\n    Get the git tag for HEAD and validate it before using it.\n    Return `None` if no valid git tag is found on HEAD\n    \"\"\"\n    existing_commit_tag = get_current_tag(existing=True)\n    if existing_commit_tag is None:\n        session.log(\n            \"Did not find an existing git tag on the current commit, not using git-tag image tag\"\n        )\n        return None\n\n    if not recognized_tag(existing_commit_tag):\n        session.log(\n            f\"Existing git tag {existing_commit_tag} is not a recognized tag, not using git-tag image tag\"\n        )\n        return None\n\n    session.log(\n        f\"Found git tag {existing_commit_tag} on the current commit, pushing corresponding git-tag image tags!\"\n    )\n    return existing_commit_tag\n\n\ndef generate_buildx_command(\n    image_tags: List[str],\n    docker_build_target: str,\n    dockerfile_path: str = \".\",\n) -> Tuple[str, ...]:\n    \"\"\"\n    Generate the command for building and publishing an image.\n\n    See tests for example usage in `test_docker_nox.py`\n    \"\"\"\n    buildx_command: Tuple[str, ...] = (\n        \"docker\",\n        \"buildx\",\n        \"build\",\n        \"--push\",\n        f\"--target={docker_build_target}\",\n        \"--platform\",\n        DOCKER_PLATFORMS,\n        dockerfile_path,\n    )\n\n    for tag in image_tags:\n        buildx_command += (\"--tag\", tag)\n\n    return buildx_command\n\n\ndef get_current_image() -> str:\n    \"\"\"Returns the current image tag\"\"\"\n    return f\"{IMAGE}:{get_current_tag()}\"\n\n\n@nox.session()\n@nox.parametrize(\n    \"image\",\n    [\n        nox.param(\"admin_ui\", id=\"admin-ui\"),\n        nox.param(\"dev\", id=\"dev\"),\n        nox.param(\"privacy_center\", id=\"privacy-center\"),\n        nox.param(\"prod\", id=\"prod\"),\n        nox.param(\"test\", id=\"test\"),\n    ],\n)\ndef build(session: nox.Session, image: str, machine_type: str = \"\") -> None:\n    \"\"\"\n    Build various Docker images.\n\n    Params:\n        admin-ui = Build the Next.js Admin UI application.\n        dev = Build the fides webserver/CLI, tagged as `local` and with an editable pip install of Fides.\n        privacy-center = Build the Next.js Privacy Center application.\n        prod = Build the fides webserver/CLI and tag it as the current application version.\n        test = Build the fides webserver/CLI the same as `prod`, but tag it as `local`.\n    \"\"\"\n\n    # This check needs to be here so it has access to the session to throw an error\n    if image == \"prod\":\n        try:\n            import git  # pylint: disable=unused-import\n        except ModuleNotFoundError:\n            session.error(\n                \"Building the prod image requires the GitPython module! Please run 'pip install gitpython' and try again\"\n            )\n\n    # The lambdas are a workaround to lazily evaluate get_current_image\n    # This allows the dev deployment to run without requirements\n    build_matrix = {\n        \"prod\": {\"tag\": get_current_image, \"target\": \"prod\"},\n        \"test\": {\"tag\": lambda: IMAGE_LOCAL, \"target\": \"prod\"},\n        \"dev\": {\"tag\": lambda: IMAGE_LOCAL, \"target\": \"dev\"},\n        \"admin_ui\": {\"tag\": lambda: IMAGE_LOCAL_UI, \"target\": \"frontend\"},\n        \"privacy_center\": {\"tag\": lambda: IMAGE_LOCAL_PC, \"target\": \"frontend\"},\n    }\n\n    # When building for release, there are additional images that need\n    # to get built. These images are outside of the primary `ethyca/fides`\n    # image so some additional logic is required.\n    if image in (\"test\", \"prod\"):\n        tag_name = None\n\n        if image == \"prod\":\n            tag_name = get_current_tag()\n        if image == \"test\":\n            tag_name = \"local\"\n        session.log(\"Building extra images:\")\n        privacy_center_image_tag = f\"{PRIVACY_CENTER_IMAGE}:{tag_name}\"\n        session.log(f\"  - {privacy_center_image_tag}\")\n        session.run(\n            \"docker\",\n            \"build\",\n            \"--target=prod_pc\",\n            \"--tag\",\n            privacy_center_image_tag,\n            \".\",\n            external=True,\n        )\n        sample_app_image_tag = f\"{SAMPLE_APP_IMAGE}:{tag_name}\"\n        session.log(f\"  - {sample_app_image_tag}\")\n        session.run(\n            \"docker\",\n            \"build\",\n            \"clients/sample-app\",\n            \"--tag\",\n            sample_app_image_tag,\n            external=True,\n        )\n\n    # Build the main ethyca/fides image\n    target = build_matrix[image][\"target\"]\n    tag = build_matrix[image][\"tag\"]\n    build_command = (\n        \"docker\",\n        \"build\",\n        f\"--target={target}\",\n        \"--tag\",\n        tag(),\n        \".\",\n    )\n    if \"nocache\" in session.posargs:\n        build_command = (*build_command, \"--no-cache\")\n\n    session.run(*build_command, external=True)\n\n\n@nox.session()\n@nox.parametrize(\n    \"tag\",\n    [\n        nox.param(\"prod\", id=\"prod\"),\n        nox.param(\"dev\", id=\"dev\"),\n        nox.param(\"rc\", id=\"rc\"),\n        nox.param(\"prerelease\", id=\"prerelease\"),\n    ],\n)\n@nox.parametrize(\n    \"app\",\n    [\n        nox.param(\"fides\", id=\"fides\"),\n        nox.param(\"privacy_center\", id=\"privacy_center\"),\n        nox.param(\"sample_app\", id=\"sample_app\"),\n    ],\n)\ndef push(session: nox.Session, tag: str, app: str) -> None:\n    \"\"\"\n    Push the main image & extra apps to DockerHub:\n      - ethyca/fides\n      - ethyca/fides-privacy-center\n      - ethyca/fides-sample-app\n\n    Params:\n\n    prod - Tags images with the current version of the application, and constant `latest` tag\n    dev - Tags images with `dev`\n    prerelease - Tags images with `prerelease` - used for alpha and beta tags\n    rc - Tags images with `rc` - used for rc tags\n\n    Posargs:\n    git_tag - Additionally tags images with the git tag of the current commit, if it exists\n\n    Note:\n    Due to how `buildx` works, all platform images need to be build in a\n    single `buildx` command. Otherwise it will cause the images in\n    Dockerhub to be overwritten.\n\n    Example Calls:\n    nox -s \"push(fides, prod)\"\n    nox -s \"push(sample_app, prerelease) -- git_tag\"\n    \"\"\"\n\n    # Create the buildx builder\n    session.run(\n        \"docker\",\n        \"buildx\",\n        \"create\",\n        \"--name\",\n        \"fides_builder\",\n        \"--bootstrap\",\n        \"--use\",\n        external=True,\n        success_codes=[0, 1],  # Will fail if it already exists, but this is fine\n    )\n\n    # Use lambdas to force lazy evaluation\n    param_tag_map: Dict[str, Callable] = {\n        \"dev\": lambda: [DEV_TAG_SUFFIX],\n        \"prerelease\": lambda: [PRERELEASE_TAG_SUFFIX],\n        \"rc\": lambda: [RC_TAG_SUFFIX],\n        \"prod\": lambda: [get_current_tag(), \"latest\"],\n    }\n\n    app_info_map = {\n        \"fides\": {\"image\": IMAGE, \"target\": \"prod\", \"path\": \".\"},\n        \"privacy_center\": {\n            \"image\": PRIVACY_CENTER_IMAGE,\n            \"target\": \"prod_pc\",\n            \"path\": \".\",\n        },\n        \"sample_app\": {\n            \"image\": SAMPLE_APP_IMAGE,\n            \"target\": \"prod\",\n            \"path\": \"clients/sample-app\",\n        },\n    }\n    app_info: Dict[str, str] = app_info_map[app]\n\n    # Get the list of Tupled commands to run\n    tag_suffixes: List[str] = param_tag_map[tag]()\n    # add a git tag based tag suffix, if requested\n    if \"git_tag\" in session.posargs:\n        # no-op if no git tag is found\n        if git_tag_suffix := verify_git_tag(session):\n            tag_suffixes.append(git_tag_suffix)\n\n    full_tags: List[str] = [\n        f\"{app_info['image']}:{tag_suffix}\" for tag_suffix in tag_suffixes\n    ]\n\n    # Parallel build the various images\n\n    buildx_command: Tuple[str, ...] = generate_buildx_command(\n        image_tags=full_tags,\n        docker_build_target=app_info[\"target\"],\n        dockerfile_path=app_info[\"path\"],\n    )\n\n    session.run(*buildx_command, external=True)\n"}
{"type": "source_file", "path": "noxfiles/docs_nox.py", "content": "\"\"\"Contains the nox sessions for developing docs.\"\"\"\n\nimport nox\n\nfrom constants_nox import CI_ARGS\n\n\n@nox.session()\ndef generate_docs(session: nox.Session) -> None:\n    \"\"\"Check that the autogenerated docs build succeeds.\"\"\"\n    session.install(\".\")\n    session.run(\"python\", \"scripts/generate_docs.py\")\n\n\n@nox.session()\ndef docs_serve(session: nox.Session) -> None:\n    \"\"\"Serve the docs.\"\"\"\n    generate_docs(session)\n    session.notify(\"teardown\")\n    session.run(\"docker\", \"compose\", \"build\", \"docs\", external=True)\n    run_shell = (\n        \"docker\",\n        \"compose\",\n        \"run\",\n        \"--rm\",\n        \"--service-ports\",\n        CI_ARGS,\n        \"docs\",\n        \"/bin/bash\",\n        \"-c\",\n        \"mkdocs serve --dev-addr=0.0.0.0:8000\",\n    )\n    session.run(*run_shell, external=True)\n\n\n@nox.session()\ndef docs_check(session: nox.Session) -> None:\n    \"\"\"Check that the docs can build.\"\"\"\n    generate_docs(session)\n    session.notify(\"teardown\")\n    session.run(\"docker\", \"compose\", \"build\", \"docs\", external=True)\n    run_shell = (\n        \"docker\",\n        \"compose\",\n        \"run\",\n        \"--rm\",\n        \"--service-ports\",\n        CI_ARGS,\n        \"docs\",\n        \"/bin/bash\",\n        \"-c\",\n        \"pip install -e /fides && mkdocs build\",\n    )\n    session.run(*run_shell, external=True)\n"}
{"type": "source_file", "path": "noxfiles/git_nox.py", "content": "\"\"\"Contains the nox sessions utilities for git-related tasks\"\"\"\n\nimport re\nfrom enum import Enum\nfrom typing import List, Optional\n\nimport nox\nfrom packaging.version import Version\n\nRELEASE_BRANCH_REGEX = r\"release-(([0-9]+\\.)+[0-9]+)\"\nRELEASE_TAG_REGEX = r\"(([0-9]+\\.)+[0-9]+)\"\nVERSION_TAG_REGEX = r\"{version}{tag_type}([0-9]+)\"\nRC_TAG_REGEX = r\"(([0-9]+\\.)+[0-9]+)rc([0-9]+)\"\nGENERIC_TAG_REGEX = r\"{tag_type}([0-9]+)$\"\n\nINITIAL_TAG_INCREMENT = 0\nTAG_INCREMENT = 1\n\n\nclass TagType(Enum):\n    \"\"\"\n    The 'types' of git tags used for different types of branches in our repo\n    \"\"\"\n\n    RC = \"rc\"  # used for release branches\n    ALPHA = \"a\"  # used for feature branches\n    BETA = \"b\"  # used for `main` branch\n\n\ndef get_all_tags(repo) -> List[str]:\n    \"\"\"\n    Returns a list of all tags in the repo, sorted by committed date, latest first\n    \"\"\"\n\n    git_session = repo.git()\n    git_session.fetch(\"--force\", \"--tags\")\n    return sorted(repo.tags, key=lambda t: t.commit.committed_datetime, reverse=True)\n\n\ndef get_current_tag(\n    existing: bool = False, repo=None, all_tags: List = []\n) -> Optional[str]:\n    \"\"\"\n    Get the current git tag.\n    If `existing` is true, this tag must already exist.\n    Otherwise, a tag is generated via `git describe --tags --dirty --always`,\n    which includes \"dirty\" tags if the working tree has local modifications.\n    \"\"\"\n    if repo is None:\n        from git.repo import Repo\n\n        repo = Repo()\n    if existing:  # checks for an existing tag on current commit\n        if not all_tags:\n            all_tags = get_all_tags(repo)\n        return next(\n            (tag.name for tag in all_tags if tag.commit == repo.head.commit),\n            None,\n        )\n    git_session = repo.git()\n    git_session.fetch(\"--force\", \"--tags\")\n    current_tag = git_session.describe(\"--tags\", \"--dirty\", \"--always\")\n    return current_tag\n\n\n@nox.session()\n@nox.parametrize(\n    \"action\",\n    [\n        nox.param(\"dry\", id=\"dry\"),\n        nox.param(\"push\", id=\"push\"),\n    ],\n)\ndef tag(session: nox.Session, action: str) -> None:\n    \"\"\"\n    Generates and optionally applies and pushes a git tag for the current HEAD commit.\n\n    Programmatically generates new tags based on the currently checked out\n    branch and existing tags in the repo.\n\n    Positional Arguments:\n        N/A\n\n    Parameters:\n        - tag(dry) = Show the tag that would be applied.\n        - tag(push) = Tag the current commit and push it. NOTE: This will trigger a new CI job to publish the tag.\n    \"\"\"\n    # pip3.10 install GitPython\n    from git.repo import Repo\n\n    repo = Repo()\n    all_tags = get_all_tags(repo)\n\n    # generate a tag based on the current repo state\n    generated_tag = generate_tag(session, repo.active_branch.name, all_tags)\n\n    if action == \"dry\":\n        session.log(f\"Dry-run -- would generate tag: {generated_tag}\")\n\n    elif action == \"push\":\n        repo.create_tag(generated_tag)\n        session.log(f\"Pushing tag {generated_tag} to remote (origin)\")\n        repo.remotes.origin.push(generated_tag)\n\n    else:\n        session.error(f\"Invalid action: {action}\")\n\n\ndef next_release_increment(\n    session: nox.Session, all_tags: List, treat_rc_as_release: bool = False\n) -> Version:\n    \"\"\"Helper to generate the next release 'increment' based on latest release tag found\n\n    If `treat_rc_as_release` is `True`, also treat `rc` tags as releases\n    \"\"\"\n\n    releases = sorted(  # sorted by Version - what we want!\n        (\n            Version(tag.name)\n            for tag in all_tags\n            if re.fullmatch(RELEASE_TAG_REGEX, tag.name)\n        ),\n        reverse=True,\n    )\n    latest_release = releases[0] if releases else None\n    if not latest_release:  # this would be bad...\n        session.error(\"Could not identify the latest release!\")\n\n    if treat_rc_as_release:\n        rc_releases = sorted(  # sorted by Version - what we want!\n            (\n                Version(tag.name)\n                for tag in all_tags\n                if re.fullmatch(RC_TAG_REGEX, tag.name)\n            ),\n            reverse=True,\n        )\n        latest_rc_release = rc_releases[0] if rc_releases else None\n        if latest_rc_release and latest_rc_release > latest_release:\n            latest_release = latest_rc_release\n\n    return Version(\n        f\"{latest_release.major}.{latest_release.minor}.{latest_release.micro + 1}\"\n    )\n\n\ndef recognized_tag(tag_to_check: str) -> bool:\n    \"\"\"Utility function to check whether the provided tag matches one of our recognized tag patterns\"\"\"\n    for tag_type in TagType:\n        pattern = GENERIC_TAG_REGEX.format(tag_type=tag_type.value)\n        if re.search(pattern, tag_to_check):\n            return True\n    return False\n\n\ndef increment_tag(\n    session: nox.Session,\n    all_tags: List,\n    version_number: str,\n    tag_type: TagType,\n):\n    \"\"\"\n    Utility method to generate the \"next\" appropriate tag on the given version number\n    with the given tag type.\n    \"\"\"\n    version_branch_tag_pattern = VERSION_TAG_REGEX.format(\n        version=version_number, tag_type=tag_type.value\n    )\n\n    # find our latest existing tag for this version/type\n    sorted_tag_matches = sorted(\n        (\n            re.fullmatch(version_branch_tag_pattern, tag.name)\n            for tag in all_tags\n            if re.fullmatch(version_branch_tag_pattern, tag.name)\n        ),\n        key=lambda match: int(\n            match.group(1)\n        ),  # numeric (not alphabetical) sort of the tag increment\n        reverse=True,\n    )\n\n    latest_tag_match = sorted_tag_matches[0] if sorted_tag_matches else None\n    if (\n        latest_tag_match\n    ):  # if we have an existing tag for this version/type, increment it\n        session.log(\n            f\"Found existing {tag_type.name.lower()} tag {latest_tag_match.group(0)}, incrementing it\"\n        )\n        tag_increment = (\n            int(latest_tag_match.group(1)) + TAG_INCREMENT\n        )  # increment the tag\n        return f\"{version_number}{tag_type.value}{tag_increment}\"\n        # return the full {version_number}{tag_type}{increment}\n\n    # we don't have an existing tag for this version/type, so start at our initial increment\n    return f\"{version_number}{tag_type.value}{INITIAL_TAG_INCREMENT}\"\n\n\ndef generate_tag(session: nox.Session, branch_name: str, all_tags: List) -> str:\n    \"\"\"Generate a tag used for package deployment\"\"\"\n    session.log(\n        f\"Fetching current tags and branch to generate a new tag, branch = '{branch_name}'\"\n    )\n\n    if release_branch_match := re.fullmatch(\n        RELEASE_BRANCH_REGEX, branch_name\n    ):  # release branch\n        session.log(\n            f\"Current branch '{branch_name}' matched release branch, determined release '{release_branch_match.group(1)}'. Generating a new {TagType.RC.name.lower()} tag for this release\"\n        )\n        return increment_tag(\n            session, all_tags, release_branch_match.group(1), TagType.RC\n        )\n\n    if branch_name == \"main\":  # main\n        # we consider `rc` tags as releases when generating beta tags while on `main``\n        # this keeps our tags based off `main` _ahead_ of our `rc` tags\n        next_release = next_release_increment(\n            session, all_tags, treat_rc_as_release=True\n        )\n        session.log(\n            f\"Current branch '{branch_name}' matched main branch. Generating a new {TagType.BETA.name.lower()} tag\"\n        )\n        return increment_tag(session, all_tags, next_release, TagType.BETA)\n\n    next_release = next_release_increment(session, all_tags)\n    # feature branch, if we've made it here\n    if \"release\" in branch_name:\n        session.warn(\n            f\"WARNING: Current branch '{branch_name}' does not follow release branch format ('release-n.n.n') and will be tagged as a feature branch.\"\n        )\n        session.warn(\"WARNING: Did you mean to name your branch differently?\")\n    session.log(\n        f\"Current branch '{branch_name}' matched feature branch. Generating a new {TagType.ALPHA.name.lower()} tag\"\n    )\n    return increment_tag(session, all_tags, next_release, TagType.ALPHA)\n"}
{"type": "source_file", "path": "noxfiles/run_infrastructure.py", "content": "\"\"\"\nThis file invokes a command used to setup infrastructure for use in testing Fidesops\nand related workflows.\n\"\"\"\n\n# pylint: disable=inconsistent-return-statements\nimport argparse\nimport subprocess\nimport sys\nfrom typing import List\n\nfrom constants_nox import COMPOSE_SERVICE_NAME\n\nDOCKER_WAIT = 5\nDOCKERFILE_DATASTORES = [\n    \"mssql\",\n    \"postgres\",\n    \"mysql\",\n    \"mongodb\",\n    \"mariadb\",\n    \"timescale\",\n    \"scylladb\",\n]\nEXTERNAL_DATASTORE_CONFIG = {\n    \"snowflake\": [\n        \"SNOWFLAKE_TEST_ACCOUNT_IDENTIFIER\",\n        \"SNOWFLAKE_TEST_USER_LOGIN_NAME\",\n        \"SNOWFLAKE_TEST_PASSWORD\",\n        \"SNOWFLAKE_TEST_PRIVATE_KEY\",\n        \"SNOWFLAKE_TEST_PRIVATE_KEY_PASSPHRASE\",\n        \"SNOWFLAKE_TEST_WAREHOUSE_NAME\",\n        \"SNOWFLAKE_TEST_DATABASE_NAME\",\n        \"SNOWFLAKE_TEST_SCHEMA_NAME\",\n    ],\n    \"redshift\": [\n        \"REDSHIFT_TEST_HOST\",\n        \"REDSHIFT_TEST_PORT\",\n        \"REDSHIFT_TEST_USER\",\n        \"REDSHIFT_TEST_PASSWORD\",\n        \"REDSHIFT_TEST_DATABASE\",\n        \"REDSHIFT_TEST_DB_SCHEMA\",\n    ],\n    \"bigquery\": [\"BIGQUERY_KEYFILE_CREDS\", \"BIGQUERY_DATASET\"],\n    \"dynamodb\": [\n        \"DYNAMODB_REGION\",\n        \"DYNAMODB_ACCESS_KEY_ID\",\n        \"DYNAMODB_ACCESS_KEY\",\n    ],\n    \"google_cloud_sql_mysql\": [\n        \"GOOGLE_CLOUD_SQL_MYSQL_DB_IAM_USER\",\n        \"GOOGLE_CLOUD_SQL_MYSQL_INSTANCE_CONNECTION_NAME\",\n        \"GOOGLE_CLOUD_SQL_MYSQL_DATABASE_NAME\",\n        \"GOOGLE_CLOUD_SQL_MYSQL_KEYFILE_CREDS\",\n    ],\n    \"google_cloud_sql_postgres\": [\n        \"GOOGLE_CLOUD_SQL_POSTGRES_DB_IAM_USER\",\n        \"GOOGLE_CLOUD_SQL_POSTGRES_INSTANCE_CONNECTION_NAME\",\n        \"GOOGLE_CLOUD_SQL_POSTGRES_DATABASE_NAME\",\n        \"GOOGLE_CLOUD_SQL_POSTGRES_DATABASE_SCHEMA_NAME\",\n        \"GOOGLE_CLOUD_SQL_POSTGRES_KEYFILE_CREDS\",\n    ],\n    \"rds_mysql\": [\n        \"RDS_MYSQL_AWS_ACCESS_KEY_ID\",\n        \"RDS_MYSQL_AWS_SECRET_ACCESS_KEY\",\n        \"RDS_MYSQL_DB_USERNAME\",\n        \"RDS_MYSQL_DB_INSTANCE\",\n        \"RDS_MYSQL_DB_NAME\",\n        \"RDS_MYSQL_REGION\",\n    ],\n    \"rds_postgres\": [\n        \"RDS_POSTGRES_AWS_ACCESS_KEY_ID\",\n        \"RDS_POSTGRES_AWS_SECRET_ACCESS_KEY\",\n        \"RDS_POSTGRES_DB_USERNAME\",\n        \"RDS_POSTGRES_REGION\",\n    ],\n}\nEXTERNAL_DATASTORES = list(EXTERNAL_DATASTORE_CONFIG.keys())\nALL_DATASTORES = DOCKERFILE_DATASTORES + EXTERNAL_DATASTORES\nOPS_TEST_DIR = \"tests/ops/\"\nAPI_TEST_DIRS = [f\"{OPS_TEST_DIR}api/\", \"tests/api/v1/endpoints/\"]\n\n\ndef run_infrastructure(\n    datastores: List[str] = [],  # Which infra should we create? If empty, we create all\n    open_shell: bool = False,  # Should we open a bash shell?\n    pytest_path: str = \"\",  # Which subset of tests should we run?\n    run_application: bool = False,  # Should we run the Fides webserver?\n    run_quickstart: bool = False,  # Should we run the quickstart command?\n    run_tests: bool = False,  # Should we run the tests after creating the infra?\n    run_create_test_data: bool = False,  # Should we run the create_test_data command?\n    analytics_opt_out: bool = False,  # Should we opt out of analytics?\n    remote_debug: bool = False,  # Should remote debugging be enabled?\n) -> None:\n    \"\"\"\n    - Create a Docker Compose file path for all datastores specified in `datastores`.\n    - Defaults to creating infrastructure for all datastores in `DOCKERFILE_DATASTORES` if none\n    are provided.\n    - Optionally runs integration tests against those datastores from the container identified\n    with `COMPOSE_SERVICE_NAME`.\n    \"\"\"\n\n    if len(datastores) == 0:\n        _run_cmd_or_err(\n            'echo \"no datastores specified, configuring infrastructure for all datastores\"'\n        )\n        datastores = DOCKERFILE_DATASTORES + EXTERNAL_DATASTORES\n    else:\n        _run_cmd_or_err(f'echo \"datastores specified: {\", \".join(datastores)}\"')\n\n    # De-duplicate datastores\n    datastores = list(set(datastores))\n    docker_datastores = [\n        f\"{datastore}_example\"\n        for datastore in datastores\n        if datastore in DOCKERFILE_DATASTORES\n    ]\n\n    _run_cmd_or_err(f'echo \"Docker datastores {docker_datastores}\"')\n\n    # Configure docker compose path\n    path: str = get_path_for_datastores(datastores, remote_debug)\n\n    _run_cmd_or_err(\n        f\"docker compose {path} up --wait {COMPOSE_SERVICE_NAME} {' '.join(docker_datastores)}\"\n    )\n\n    seed_initial_data(\n        datastores,\n        path,\n        service_name=COMPOSE_SERVICE_NAME,\n    )\n\n    if open_shell:\n        return _open_shell(path, COMPOSE_SERVICE_NAME)\n\n    if run_application:\n        return _run_application(path, COMPOSE_SERVICE_NAME)\n\n    if run_quickstart:\n        return _run_quickstart(path, COMPOSE_SERVICE_NAME)\n\n    if run_tests:\n        # Now run the tests\n        return _run_tests(\n            datastores,\n            docker_compose_path=path,\n            pytest_path=pytest_path,\n            analytics_opt_out=analytics_opt_out,\n        )\n\n    if run_create_test_data:\n        return _run_create_test_data(path, COMPOSE_SERVICE_NAME)\n\n\ndef seed_initial_data(\n    datastores: List[str],\n    path: str,\n    service_name: str,\n) -> None:\n    \"\"\"\n    Seed the datastores with initial data as defined in the file at `setup_path`\n    \"\"\"\n    _run_cmd_or_err('echo \"Seeding initial data for all datastores...\"')\n    for datastore in datastores:\n        if datastore in DOCKERFILE_DATASTORES:\n            setup_path = (\n                f\"{OPS_TEST_DIR}integration_tests/setup_scripts/{datastore}_setup.py\"\n            )\n            _run_cmd_or_err(\n                f'echo \"Attempting to create schema and seed initial data for {datastore} from {setup_path}...\"'\n            )\n            _run_cmd_or_err(\n                f'docker compose {path} run {service_name} python {setup_path} || echo \"no custom setup logic found for {datastore}, skipping\"'\n            )\n\n\ndef get_path_for_datastores(datastores: List[str], remote_debug: bool) -> str:\n    \"\"\"\n    Returns the docker compose file paths for the specified datastores\n    \"\"\"\n    path: str = \"-f docker-compose.yml\"\n    if remote_debug:\n        path += \" -f docker-compose.remote-debug.yml\"\n    for datastore in datastores:\n        _run_cmd_or_err(f'echo \"configuring infrastructure for {datastore}\"')\n        if datastore in DOCKERFILE_DATASTORES:\n            # We only need to locate the docker-compose file if the datastore runs in Docker\n            path += f\" -f docker/docker-compose.integration-{datastore}.yml\"\n        elif datastore not in EXTERNAL_DATASTORES:\n            # If the specified datastore is not known to us\n            _run_cmd_or_err(f'echo \"Datastore {datastore} is currently not supported\"')\n\n    return path\n\n\ndef _run_cmd_or_err(cmd: str) -> None:\n    \"\"\"\n    Runs a command in the bash prompt and throws an error if the command was not successful\n    \"\"\"\n    with subprocess.Popen(cmd, shell=True) as result:\n        if result.wait() > 0:\n            raise Exception(f\"Error executing command: {cmd}\")\n\n\ndef _run_quickstart(\n    path: str,\n    service_name: str,\n) -> None:\n    \"\"\"\n    Invokes the Fidesops command line quickstart\n    \"\"\"\n    _run_cmd_or_err('echo \"Running the quickstart...\"')\n    _run_cmd_or_err(f\"docker compose {path} up -d\")\n    _run_cmd_or_err(f\"docker compose run {service_name} python scripts/quickstart.py\")\n\n\ndef _run_create_test_data(\n    path: str,\n    service_name: str,\n) -> None:\n    \"\"\"\n    Invokes the Fidesops create_user_and_client command\n    \"\"\"\n    _run_cmd_or_err('echo \"Running create test data...\"')\n    _run_cmd_or_err(f\"docker compose {path} up -d\")\n    _run_cmd_or_err(\n        f\"docker compose run {service_name} python scripts/create_test_data.py\"\n    )\n\n\ndef _open_shell(\n    path: str,\n    service_name: str,\n) -> None:\n    \"\"\"\n    Opens a bash shell on the container at `service_name`\n    \"\"\"\n    _run_cmd_or_err(f'echo \"Opening bash shell on {service_name}\"')\n    _run_cmd_or_err(f\"docker compose {path} run {service_name} /bin/bash\")\n\n\ndef _run_application(docker_compose_path: str, service_name: str) -> None:\n    \"\"\"\n    Runs the application at `docker_compose_path` without detaching it from the shell\n    \"\"\"\n    _run_cmd_or_err('echo \"Running application\"')\n    _run_cmd_or_err(f\"docker compose {docker_compose_path} up {service_name}\")\n\n\ndef _run_tests(\n    datastores: List[str],\n    docker_compose_path: str,\n    pytest_path: str = \"\",\n    analytics_opt_out: bool = False,\n) -> None:\n    \"\"\"\n    Runs unit tests against the specified datastores\n    \"\"\"\n    if pytest_path is None:\n        pytest_path = \"\"\n\n    path_includes_markers = \"-m\" in pytest_path\n    pytest_markers: str = \"\"\n    if not path_includes_markers:\n        # If the path manually specified already uses markers, don't add more here\n        if set(datastores) == set(DOCKERFILE_DATASTORES + EXTERNAL_DATASTORES):\n            # If all datastores have been specified use the generic `integration` flag\n            pytest_markers += \"integration\"\n        else:\n            # Otherwise only include the datastores provided\n            for datastore in datastores:\n                if len(pytest_markers) == 0:\n                    pytest_markers += f\"integration_{datastore}\"\n                else:\n                    pytest_markers += f\" or integration_{datastore}\"\n\n    environment_variables = \"\"\n    for datastore in EXTERNAL_DATASTORES:\n        if datastore in datastores:\n            for env_var in EXTERNAL_DATASTORE_CONFIG[datastore]:\n                environment_variables += f\"-e {env_var} \"\n\n    if analytics_opt_out:\n        environment_variables += \"-e ANALYTICS_OPT_OUT\"\n\n    pytest_path += f' -m \"{pytest_markers}\"'\n\n    _run_cmd_or_err(\n        f'echo \"running pytest for conditions: {pytest_path} with environment variables: {environment_variables}\"'\n    )\n    coverage_arg = \"--cov-report=xml\"\n    _run_cmd_or_err(\n        f\"docker compose {docker_compose_path} run {environment_variables} {COMPOSE_SERVICE_NAME} pytest {coverage_arg} {pytest_path}\"\n    )\n\n    # Now tear down the infrastructure\n    _run_cmd_or_err(f\"docker compose {docker_compose_path} down --remove-orphans\")\n    _run_cmd_or_err('echo \"fin.\"')\n\n\nif __name__ == \"__main__\":\n    if sys.version_info.major < 3:\n        raise Exception(\"Python3 is required to configure Fidesops.\")\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"-d\",\n        \"--datastores\",\n        action=\"extend\",\n        nargs=\"*\",\n        type=str,\n    )\n    parser.add_argument(\n        \"-t\",\n        \"--run_tests\",\n        action=\"store_true\",\n    )\n    parser.add_argument(\n        \"-p\",\n        \"--pytest_path\",\n    )\n    parser.add_argument(\n        \"-s\",\n        \"--open_shell\",\n        action=\"store_true\",\n    )\n    parser.add_argument(\n        \"-r\",\n        \"--run_application\",\n        action=\"store_true\",\n    )\n    parser.add_argument(\n        \"-q\",\n        \"--run_quickstart\",\n        action=\"store_true\",\n    )\n\n    parser.add_argument(\n        \"-a\",\n        \"--analytics_opt_out\",\n        action=\"store_true\",\n    )\n\n    config_args = parser.parse_args()\n\n    run_infrastructure(\n        datastores=config_args.datastores,\n        open_shell=config_args.open_shell,\n        pytest_path=config_args.pytest_path,\n        run_application=config_args.run_application,\n        run_quickstart=config_args.run_quickstart,\n        run_tests=config_args.run_tests,\n        analytics_opt_out=config_args.analytics_opt_out,\n    )\n"}
{"type": "source_file", "path": "noxfiles/utils_nox.py", "content": "\"\"\"Contains various utility-related nox sessions.\"\"\"\n\nfrom pathlib import Path\n\nimport nox\n\nfrom constants_nox import COMPOSE_FILE_LIST\nfrom run_infrastructure import run_infrastructure\n\n\n@nox.session()\ndef seed_test_data(session: nox.Session) -> None:\n    \"\"\"Seed test data in the Postgres application database.\"\"\"\n    run_infrastructure(datastores=[\"postgres\"], run_create_test_data=True)\n\n\n@nox.session()\ndef clean(session: nox.Session) -> None:\n    \"\"\"\n    Clean up docker containers, remove orphans, remove volumes\n    and prune images related to this project.\n    \"\"\"\n    teardown(session, volumes=True, images=True)\n    session.run(\"docker\", \"system\", \"prune\", \"--force\", \"--all\", external=True)\n    print(\"Clean Complete!\")\n\n\n@nox.session()\ndef teardown(session: nox.Session, volumes: bool = False, images: bool = False) -> None:\n    \"\"\"Tear down all docker environments.\"\"\"\n    for compose_file in COMPOSE_FILE_LIST:\n        teardown_command = (\n            \"docker\",\n            \"compose\",\n            \"-f\",\n            compose_file,\n            \"down\",\n            \"--remove-orphans\",\n        )\n\n        if volumes or \"volumes\" in session.posargs:\n            teardown_command = (*teardown_command, \"--volumes\")\n\n        if images:\n            teardown_command = (*teardown_command, \"--rmi\", \"all\")\n\n        try:\n            session.run(*teardown_command, external=True)\n        except nox.command.CommandFailed:\n            session.warn(f\"Teardown failed: '{teardown_command}'\")\n\n    session.log(\"Teardown complete\")\n\n\ndef install_requirements(session: nox.Session, include_optional: bool = False) -> None:\n    session.install(\"-r\", \"requirements.txt\")\n    session.install(\"-r\", \"dev-requirements.txt\")\n    if include_optional:\n        session.install(\"-r\", \"optional-requirements.txt\")\n\n\n@nox.session()\ndef init_saas_connector(session: nox.Session) -> None:\n    connector_name = session.posargs[0].replace(\" \", \"\")\n    connector_id = \"_\".join(session.posargs[0].lower().split(\" \"))\n    variable_map = {\"connector_name\": connector_name, \"connector_id\": connector_id}\n\n    # create empty config and dataset files\n    try:\n        Path(f\"data/saas/config/{variable_map['connector_id']}_config.yml\").touch(\n            exist_ok=False\n        )\n        Path(f\"data/saas/dataset/{variable_map['connector_id']}_dataset.yml\").touch(\n            exist_ok=False\n        )\n    except Exception:\n        session.error(\n            f\"Files for {session.posargs[0]} already exist, skipping initialization\"\n        )\n\n    # location of Jinja templates\n    from jinja2 import Environment, FileSystemLoader\n\n    environment = Environment(\n        loader=FileSystemLoader(\"data/saas/saas_connector_scaffolding/\")\n    )\n\n    # render fixtures file\n    fixtures_template = environment.get_template(\"new_fixtures.jinja\")\n    filename = f\"tests/fixtures/saas/{variable_map['connector_id']}_fixtures.py\"\n    contents = fixtures_template.render(variable_map)\n    try:\n        with open(filename, mode=\"x\", encoding=\"utf-8\") as fixtures:\n            fixtures.write(contents)\n            fixtures.close()\n    except FileExistsError:\n        session.error(\n            f\"Files for {session.posargs[0]} already exist, skipping initialization\"\n        )\n\n    # render tests file\n    test_template = environment.get_template(\"test_new_task.jinja\")\n    filename = (\n        f\"tests/ops/integration_tests/saas/test_{variable_map['connector_id']}_task.py\"\n    )\n    contents = test_template.render(variable_map)\n    try:\n        with open(filename, mode=\"x\", encoding=\"utf-8\") as tests:\n            tests.write(contents)\n            tests.close()\n    except FileExistsError:\n        session.error(\n            f\"Files for {session.posargs[0]} already exist, skipping initialization\"\n        )\n"}
{"type": "source_file", "path": "scripts/generate_docs.py", "content": "\"\"\"\nThis module handles generating documentation from code.\n\"\"\"\n\nimport json\nimport sys\n\nfrom fides.api.main import app\nfrom fides.config import CONFIG\nfrom fides.config.create import generate_config_docs\n\n\ndef generate_openapi(outfile_dir: str) -> str:\n    \"Write out an openapi.json file for the API.\"\n\n    outfile_name = \"api/openapi.json\"\n    outfile_path = f\"{outfile_dir}/{outfile_name}\"\n    print(f\"Generating OpenAPI JSON from the API and writing to '{outfile_path}'...\")\n    with open(outfile_path, \"w\") as outfile:\n        json.dump(app.openapi(), outfile, indent=2)\n        print(f\"Exported OpenAPI JSON from the API to '{outfile_path}'\")\n    return outfile_path\n\n\nif __name__ == \"__main__\":\n    default_outfile_dir = \"docs/fides/docs\"\n    try:\n        outfile_dir = sys.argv[1]\n    except IndexError:\n        outfile_dir = default_outfile_dir\n\n    generate_openapi(outfile_dir)\n    generate_config_docs(\n        config=CONFIG, outfile_path=\"docs/fides/docs/config/fides.toml\"\n    )\n"}
{"type": "source_file", "path": "scripts/verify_index_creation.py", "content": "import json\nfrom typing import Dict, List\n\nfrom sqlalchemy import inspect, text\nfrom sqlalchemy.engine.reflection import Inspector\nfrom sqlalchemy.orm import Session\n\nfrom fides.api.db.session import get_db_session\nfrom fides.config import get_config\n\nCONFIG = get_config()\n\n\ndef check_index_exists(inspector: Inspector, table_name: str, index_name: str) -> bool:\n    indexes = inspector.get_indexes(table_name)\n    return any(index[\"name\"] == index_name for index in indexes)\n\n\ndef get_index_progress(db: Session, table_name: str, index_name: str) -> str:\n    progress_query = text(\n        \"\"\"\n        SELECT\n            c.phase AS progress\n        FROM\n            pg_stat_progress_create_index c\n            JOIN pg_index i ON c.index_relid = i.indexrelid\n        WHERE\n            i.indrelid::regclass::text = :table_name\n            AND i.indexrelid::regclass::text = :index_name\n        \"\"\"\n    )\n    result = db.execute(\n        progress_query, {\"table_name\": table_name, \"index_name\": index_name}\n    ).fetchone()\n    return result[0] if result else \"complete\"\n\n\ndef get_index_info(\n    db: Session, table_index_map: Dict[str, List[str]]\n) -> Dict[str, str]:\n    inspector: Inspector = inspect(db.bind)\n    index_info: Dict[str, str] = {}\n    for table_name, index_names in table_index_map.items():\n        for index_name in index_names:\n            exists = check_index_exists(inspector, table_name, index_name)\n            progress = (\n                get_index_progress(db, table_name, index_name)\n                if exists\n                else \"not found\"\n            )\n            index_info[index_name] = progress\n    return index_info\n\n\nif __name__ == \"__main__\":\n    SessionLocal = get_db_session(CONFIG)\n    db: Session = SessionLocal()\n\n    table_index_map: Dict[str, List[str]] = {\n        \"privacypreferencehistory\": [\"ix_privacypreferencehistory_property_id\"],\n        \"servednoticehistory\": [\"ix_servednoticehistory_property_id\"],\n    }\n\n    index_info: Dict[str, str] = get_index_info(db, table_index_map)\n    print(json.dumps(index_info))\n"}
{"type": "source_file", "path": "src/fides/api/alembic/migrations/versions/1d2f04ac19f2_add_page_column_to_asset_table.py", "content": "\"\"\"\n- add page column to asset table and\n- change `Browser Request` values to `Browser request` StagedResource.resource_type column\n- change `Browser Request` values to `Browser request` Asset.asset_type column\n\nRevision ID: 1d2f04ac19f2\nRevises: 7c3fbee90c78\nCreate Date: 2025-03-17 19:36:43.016383\n\n\"\"\"\n\nimport sqlalchemy as sa\nfrom alembic import op\nfrom sqlalchemy.dialects import postgresql\n\n# revision identifiers, used by Alembic.\nrevision = \"1d2f04ac19f2\"\ndown_revision = \"7c3fbee90c78\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade():\n    # adds page column to asset table\n    op.add_column(\n        \"asset\",\n        sa.Column(\"page\", sa.ARRAY(sa.String()), server_default=\"{}\", nullable=False),\n    )\n\n    # changes `Browser Request` values to `Browser request` in the two tables where it shows up`\n    op.execute(\n        \"UPDATE stagedresource SET resource_type = 'Browser request' WHERE resource_type = 'Browser Request'\"\n    )\n    op.execute(\n        \"UPDATE asset SET asset_type = 'Browser request' WHERE asset_type = 'Browser Request'\"\n    )\n\n\ndef downgrade():\n    # drops page column from asset table\n    op.drop_column(\"asset\", \"page\")\n\n    # changes `Browser request` values back to `Browser Request` in the two tables where it shows up\n    op.execute(\n        \"UPDATE stagedresource SET resource_type = 'Browser Request' WHERE resource_type = 'Browser request'\"\n    )\n    op.execute(\n        \"UPDATE asset SET asset_type = 'Browser Request' WHERE asset_type = 'Browser request'\"\n    )\n"}
{"type": "source_file", "path": "src/fides/api/alembic/migrations/versions/021d288d0ce3_add_consent_request.py", "content": "\"\"\"Add consent request\n\nRevision ID: 021d288d0ce3\nRevises: a0e6feb5bdc8\nCreate Date: 2022-09-17 01:26:43.187484\n\n\"\"\"\n\nimport sqlalchemy as sa\nfrom alembic import op\n\n# revision identifiers, used by Alembic.\nrevision = \"021d288d0ce3\"\ndown_revision = \"a0e6feb5bdc8\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade():\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.create_table(\n        \"consentrequest\",\n        sa.Column(\"id\", sa.String(length=255), nullable=False),\n        sa.Column(\n            \"created_at\",\n            sa.DateTime(timezone=True),\n            server_default=sa.text(\"now()\"),\n            nullable=True,\n        ),\n        sa.Column(\n            \"updated_at\",\n            sa.DateTime(timezone=True),\n            server_default=sa.text(\"now()\"),\n            nullable=True,\n        ),\n        sa.Column(\"provided_identity_id\", sa.String(), nullable=False),\n        sa.ForeignKeyConstraint(\n            [\"provided_identity_id\"],\n            [\"providedidentity.id\"],\n        ),\n        sa.PrimaryKeyConstraint(\"id\"),\n    )\n    op.create_index(\n        op.f(\"ix_consentrequest_id\"), \"consentrequest\", [\"id\"], unique=False\n    )\n    op.alter_column(\n        \"consent\", \"provided_identity_id\", existing_type=sa.VARCHAR(), nullable=False\n    )\n    # ### end Alembic commands ###\n\n\ndef downgrade():\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.alter_column(\n        \"consent\", \"provided_identity_id\", existing_type=sa.VARCHAR(), nullable=True\n    )\n    op.drop_index(op.f(\"ix_consentrequest_id\"), table_name=\"consentrequest\")\n    op.drop_table(\"consentrequest\")\n    # ### end Alembic commands ###\n"}
{"type": "source_file", "path": "scripts/quickstart.py", "content": "\"\"\"\nA five-step fidesops quickstart\n\"\"\"\n\nimport json\nimport os\nimport time\nfrom datetime import datetime\nfrom os.path import exists\nfrom time import sleep\nfrom typing import Optional\n\nimport requests\nimport yaml\nfrom loguru import logger\n\nfrom fides.api.models.connectionconfig import ConnectionType\nfrom fides.api.models.policy import ActionType\nfrom fides.common.api.scope_registry import SCOPE_REGISTRY\nfrom fides.common.api.v1 import urn_registry as ops_urls\nfrom fides.config import get_config\n\nCONFIG = get_config()\n\n\ndef get_access_token(client_id: str, client_secret: str) -> str:\n    \"\"\"\n    Authorize with fidesops via OAuth.\n    Returns a valid access token if successful, or throws an error otherwise.\n    See http://localhost:8000/api#operations-OAuth-acquire_access_token_api_v1_oauth_token_post\n    \"\"\"\n    data = {\n        \"grant_type\": \"client_credentials\",\n        \"client_id\": client_id,\n        \"client_secret\": client_secret,\n    }\n    url = f\"{FIDESOPS_V1_API_URL}{ops_urls.TOKEN}\"\n    response = requests.post(url, data=data)\n\n    if response.ok:\n        token = (response.json())[\"access_token\"]\n        if token:\n            logger.info(f\"Completed fidesops oauth login via {url}\")\n            return token\n\n    raise RuntimeError(\n        f\"fidesops oauth login failed! response.status_code={response.status_code}, response.json()={response.json()}\"\n    )\n\n\ndef create_oauth_client():\n    \"\"\"\n    Create a new OAuth client in fidesops.\n    Returns the response JSON if successful, or throws an error otherwise.\n    See http://localhost:8000/api#operations-OAuth-acquire_access_token_api_v1_oauth_token_post\n    See http://localhost:8000/api#operations-OAuth-acquire_access_token_api_v1_oauth_token_post\n    \"\"\"\n    url = f\"{FIDESOPS_V1_API_URL}{ops_urls.CLIENT}\"\n    response = requests.post(\n        url,\n        headers=root_oauth_header,\n        json=SCOPE_REGISTRY,\n    )\n\n    if response.ok:\n        created_client = response.json()\n        if created_client[\"client_id\"] and created_client[\"client_secret\"]:\n            logger.info(f\"Created fidesops oauth client via {url}\")\n            return created_client\n\n    raise RuntimeError(\n        f\"fidesops oauth client creation failed! response.status_code={response.status_code}, response.json()={response.json()}\"\n    )\n\n\ndef create_connection(key: str, connection_type: ConnectionType):\n    \"\"\"\n    Create a connection in fidesops for your PostgreSQL database\n    Returns the response JSON if successful, or throws an error otherwise.\n    See http://localhost:8000/api#operations-Connections-put_connections_api_v1_connection_put\n    \"\"\"\n    connection_create_data = [\n        {\n            \"name\": key,\n            \"key\": key,\n            \"connection_type\": connection_type.value,\n            \"access\": \"write\",\n        },\n    ]\n    url = f\"{FIDESOPS_V1_API_URL}{ops_urls.CONNECTIONS}\"\n    response = requests.patch(\n        url,\n        headers=oauth_header,\n        json=connection_create_data,\n    )\n\n    if response.ok:\n        connections = (response.json())[\"succeeded\"]\n        if len(connections) > 0:\n            logger.info(\n                f\"Created fidesops {connection_type.value} connection with key={key} via {url}\"\n            )\n            return response.json()\n\n    raise RuntimeError(\n        f\"fidesops connection creation failed! response.status_code={response.status_code}, response.json()={response.json()}\"\n    )\n\n\ndef configure_postgres_connection(\n    key: str, host: str, port: int, dbname: str, username: str, password: str\n):\n    \"\"\"\n    Configure the connection with the given `key` in fidesops with your PostgreSQL database credentials.\n    Returns the response JSON if successful, or throws an error otherwise.\n    See http://localhost:8000/api#operations-Connections-put_connection_config_secrets_api_v1_connection__connection_key__secret_put\n    \"\"\"\n    connection_secrets_data = {\n        \"host\": host,\n        \"port\": port,\n        \"dbname\": dbname,\n        \"username\": username,\n        \"password\": password,\n    }\n    connection_secrets_path = ops_urls.CONNECTION_SECRETS.format(connection_key=key)\n    url = f\"{FIDESOPS_V1_API_URL}{connection_secrets_path}\"\n    response = requests.put(\n        url,\n        headers=oauth_header,\n        json=connection_secrets_data,\n    )\n\n    if response.ok:\n        if (response.json())[\"test_status\"] != \"failed\":\n            logger.info(\n                f\"Configured fidesops postgres connection secrets for via {url}\"\n            )\n            return response.json()\n\n    raise RuntimeError(\n        f\"fidesops connection configuration failed! response.status_code={response.status_code}, response.json()={response.json()}\"\n    )\n\n\ndef configure_mongo_connection(\n    key: str, host: str, port: int, dbname: str, username: str, password: str\n):\n    \"\"\"\n    Configure the connection with the given `key` in fidesops with your PostgreSQL database credentials.\n    Returns the response JSON if successful, or throws an error otherwise.\n    See http://localhost:8000/api#operations-Connections-put_connection_config_secrets_api_v1_connection__connection_key__secret_put\n    \"\"\"\n    connection_secrets_data = {\n        \"host\": host,\n        \"port\": port,\n        \"defaultauthdb\": dbname,\n        \"username\": username,\n        \"password\": password,\n    }\n    connection_secrets_path = ops_urls.CONNECTION_SECRETS.format(connection_key=key)\n    url = f\"{FIDESOPS_V1_API_URL}{connection_secrets_path}\"\n    response = requests.put(\n        url,\n        headers=oauth_header,\n        json=connection_secrets_data,\n    )\n\n    if response.ok:\n        if (response.json())[\"test_status\"] != \"failed\":\n            logger.info(f\"Configured fidesops mongo connection secrets via {url}\")\n            return response.json()\n\n    raise RuntimeError(\n        f\"fidesops connection configuration failed! response.status_code={response.status_code}, response.json()={response.json()}\"\n    )\n\n\ndef validate_dataset(connection_key: str, yaml_path: str):\n    \"\"\"\n    Validate a dataset in fidesops given a YAML manifest file.\n    Requires the `connection_key` for the connection, and `yaml_path`\n    that is a local filepath to a .yml dataset Fides manifest file.\n    Returns the response JSON if successful, or throws an error otherwise.\n    See http://localhost:8000/api#operations-Datasets-validate_dataset_api_v1_connection__connection_key__validate_dataset_put\n    \"\"\"\n\n    with open(yaml_path, \"r\") as file:\n        dataset = yaml.safe_load(file).get(\"dataset\", [])[0]\n\n    validate_dataset_data = dataset\n    dataset_validate_path = ops_urls.DATASET_VALIDATE.format(\n        connection_key=connection_key\n    )\n    url = f\"{FIDESOPS_V1_API_URL}{dataset_validate_path}\"\n    response = requests.put(\n        url,\n        headers=oauth_header,\n        json=validate_dataset_data,\n    )\n\n    if response.ok:\n        traversal_details = (response.json())[\"traversal_details\"]\n        if traversal_details[\"is_traversable\"]:\n            logger.info(f\"Validated fidesops dataset via {url}\")\n            return response.json()\n        else:\n            raise RuntimeError(\n                f\"fidesops dataset is not traversable! traversal_details={traversal_details}\"\n            )\n    print(vars(response))\n    raise RuntimeError(\n        f\"fidesops dataset creation failed! response.status_code={response.status_code}, response.json()={response.json()}\"\n    )\n\n\ndef create_dataset(connection_key: str, yaml_path: str):\n    \"\"\"\n    Create a dataset and a datasetconfig in fides given a YAML manifest file.\n    Requires the `connection_key` for the PostgreSQL connection, and `yaml_path`\n    that is a local filepath to a .yml dataset Fides manifest file.\n    Returns the response JSON if successful, or throws an error otherwise.\n    See http://localhost:8000/api#operations-Datasets-put_datasets_api_v1_connection__connection_key__dataset_put\n    \"\"\"\n\n    with open(yaml_path, \"r\") as file:\n        dataset = yaml.safe_load(file).get(\"dataset\", [])[0]\n\n    # Create ctl_dataset resource first\n    dataset_create_data = [dataset]\n    dataset_path = \"/dataset/upsert\"\n    url = f\"{FIDESOPS_V1_API_URL}{dataset_path}\"\n    response = requests.post(\n        url,\n        headers=oauth_header,\n        json=dataset_create_data,\n    )\n\n    if response.ok:\n        json_data = response.json()\n        logger.info(\"{} via {}\", json_data[\"message\"], url)\n    else:\n        raise RuntimeError(\n            f\"fides dataset creation failed! response.status_code={response.status_code}, response.json()={response.json()}\"\n        )\n\n    # Now link that ctl_dataset to the DatasetConfig\n    dataset_config_path = ops_urls.DATASET_CONFIGS.format(connection_key=connection_key)\n    url = f\"{FIDESOPS_V1_API_URL}{dataset_config_path}\"\n\n    fides_key = dataset[\"fides_key\"]\n    response = requests.patch(\n        url,\n        headers=oauth_header,\n        json=[{\"fides_key\": fides_key, \"ctl_dataset_fides_key\": fides_key}],\n    )\n\n    if response.ok:\n        datasets = (response.json())[\"succeeded\"]\n        if len(datasets) > 0:\n            logger.info(f\"Created fidesops dataset config via {url}\")\n            return response.json()\n\n    raise RuntimeError(\n        f\"fidesops dataset creation failed! response.status_code={response.status_code}, response.json()={response.json()}\"\n    )\n\n\ndef create_local_storage(key: str, file_format: str):\n    \"\"\"\n    Create a storage config in fidesops to write to a local file.\n    Returns the response JSON if successful, or throws an error otherwise.\n    See http://localhost:8000/api#operations-Storage-put_config_api_v1_storage_config_put\n    \"\"\"\n    storage_create_data = [\n        {\n            \"name\": key,\n            \"key\": key,\n            \"type\": \"local\",\n            \"format\": file_format,\n            \"details\": {\n                \"naming\": \"request_id\",\n            },\n        },\n    ]\n    url = f\"{FIDESOPS_V1_API_URL}{ops_urls.STORAGE_CONFIG}\"\n    response = requests.patch(\n        url,\n        headers=oauth_header,\n        json=storage_create_data,\n    )\n\n    if response.ok:\n        storage = (response.json())[\"succeeded\"]\n        if len(storage) > 0:\n            logger.info(f\"Created fidesops storage with key={key} via {url}\")\n            return response.json()\n\n    raise RuntimeError(\n        f\"fidesops storage creation failed! response.status_code={response.status_code}, response.json()={response.json()}\"\n    )\n\n\ndef create_dsr_policy(key: str):\n    \"\"\"\n    Create a request policy in fidesops with the given key.\n    Returns the response JSON if successful, or throws an error otherwise.\n    See http://localhost:8000/api#operations-Policy-create_or_update_policies_api_v1_policy_put\n    \"\"\"\n\n    policy_create_data = [\n        {\n            \"name\": key,\n            \"key\": key,\n        },\n    ]\n    url = f\"{FIDESOPS_V1_API_URL}{ops_urls.POLICY_LIST}\"\n    response = requests.patch(\n        url,\n        headers=oauth_header,\n        json=policy_create_data,\n    )\n\n    if response.ok:\n        policies = (response.json())[\"succeeded\"]\n        if len(policies) > 0:\n            logger.info(\"Created fidesops policy with key={} via {}\", key, url)\n            return response.json()\n\n    raise RuntimeError(\n        f\"fidesops policy creation failed! response.status_code={response.status_code}, response.json()={response.json()}\"\n    )\n\n\ndef delete_policy_rule(policy_key: str, key: str):\n    \"\"\"\n    Deletes a policy rule with the given key.\n    Returns the response JSON.\n    See http://localhost:8000/api#operations-Policy-delete_rule_api_v1_policy__policy_key__rule__rule_key__delete\n    \"\"\"\n    rule_path = ops_urls.RULE_DETAIL.format(policy_key=policy_key, rule_key=key)\n    return requests.delete(f\"{FIDESOPS_V1_API_URL}{rule_path}\", headers=oauth_header)\n\n\ndef create_dsr_policy_rule(\n    policy_key: str,\n    key: str,\n    action_type: ActionType,\n    storage_destination_key: Optional[str] = None,\n):\n    \"\"\"\n    Create a policy rule to return matched data in an access request to the given storage destination.\n    Returns the response JSON if successful, or throws an error otherwise.\n    See http://localhost:8000/api#operations-Policy-create_or_update_rules_api_v1_policy__policy_key__rule_put\n    \"\"\"\n\n    rule_create_data = {\n        \"name\": key,\n        \"key\": key,\n        \"action_type\": action_type.value,\n    }\n\n    if action_type == ActionType.access:\n        rule_create_data[\"storage_destination_key\"] = storage_destination_key\n    elif action_type == ActionType.erasure:\n        # Only null masking supported currently\n        rule_create_data[\"masking_strategy\"] = {\n            \"strategy\": \"null_rewrite\",\n            \"configuration\": {},\n        }\n\n    rule_path = ops_urls.RULE_LIST.format(policy_key=policy_key)\n    url = f\"{FIDESOPS_V1_API_URL}{rule_path}\"\n    response = requests.patch(\n        url,\n        headers=oauth_header,\n        json=[rule_create_data],\n    )\n\n    if response.ok:\n        rules = (response.json())[\"succeeded\"]\n        if len(rules) > 0:\n            logger.info(f\"Created fidesops policy rule via {url}\")\n            return response.json()\n\n    raise RuntimeError(\n        f\"fidesops policy rule creation failed! response.status_code={response.status_code}, response.json()={response.json()}\"\n    )\n\n\ndef create_dsr_policy_rule_target(policy_key: str, rule_key: str, data_cat: str):\n    \"\"\"\n    Create a policy rule target that matches the given data_category.\n    Returns the response JSON if successful, or throws an error otherwise.\n    See http://localhost:8000/api#operations-Policy-create_or_update_rule_targets_api_v1_policy__policy_key__rule__rule_key__target_put\n    \"\"\"\n\n    target_create_data = [\n        {\n            \"data_category\": data_cat,\n        },\n    ]\n    rule_target_path = ops_urls.RULE_TARGET_LIST.format(\n        policy_key=policy_key, rule_key=rule_key\n    )\n    url = f\"{FIDESOPS_V1_API_URL}{rule_target_path}\"\n    response = requests.patch(\n        url,\n        headers=oauth_header,\n        json=target_create_data,\n    )\n\n    if response.ok:\n        targets = (response.json())[\"succeeded\"]\n        if len(targets) > 0:\n            logger.info(\n                f\"Created fidesops policy rule target for '{data_cat}' via {url}\"\n            )\n            return response.json()\n\n    raise RuntimeError(\n        f\"fidesops policy rule target creation failed! response.status_code={response.status_code}, response.json()={response.json()}\"\n    )\n\n\ndef create_privacy_request(user_email: str, policy_key: str):\n    \"\"\"\n    Create a privacy request that is executed against the given request policy.\n    Returns the response JSON if successful, or throws an error otherwise.\n    See http://localhost:8000/api#operations-Privacy_Requests-create_privacy_request_api_v1_privacy_request_post\n    \"\"\"\n\n    privacy_request_data = [\n        {\n            \"requested_at\": str(datetime.utcnow()),\n            \"policy_key\": policy_key,\n            \"identity\": {\"email\": user_email},\n        },\n    ]\n    url = f\"{FIDESOPS_V1_API_URL}{ops_urls.PRIVACY_REQUESTS}\"\n    response = requests.post(\n        url,\n        json=privacy_request_data,\n    )\n\n    if response.ok:\n        created_privacy_requests = (response.json())[\"succeeded\"]\n        if len(created_privacy_requests) > 0:\n            logger.info(f\"Created fidesops privacy request for email={email} via {url}\")\n            return response.json()\n\n    raise RuntimeError(\n        f\"fidesops privacy request creation failed! response.status_code={response.status_code}, response.json()={response.json()}\"\n    )\n\n\ndef print_results(request_id: str) -> None:\n    \"\"\"\n    Check to see if a result JSON for the given privacy request exists, and\n    print it to the console if so.\n    \"\"\"\n    results_path = f\"fides_uploads/{request_id}.json\"\n\n    count = 0\n    max_allowed_waiting = 10\n    while not os.path.exists(results_path) and count < max_allowed_waiting:\n        logger.info(\"Waiting for privacy request results...\")\n        time.sleep(5)\n        count += 1  # Only loop through a reasonable number of times\n\n    if exists(results_path):\n        logger.info(\n            f\"Successfully read fidesops privacy request results from {results_path}:\"\n        )\n        with open(results_path, \"r\") as file:\n            results_json = json.loads(file.read())\n            print(json.dumps(results_json, indent=4))\n    else:\n        raise RuntimeError(\n            f\"fidesops privacy request results not found at results_path={results_path}\"\n        )\n\n\nif __name__ == \"__main__\":\n    print(\n        \"-------------------------------------------------------------------------------------\"\n    )\n    print(\n        \"\"\"\n           \n                     \n                     \n    \"\"\"\n    )\n\n    # NOTE: In a real application, these secrets and config values would be provided\n    # via ENV vars or similar, but we've inlined everything here for simplicity\n    FIDESOPS_URL = \"http://fides:8080\"\n    FIDESOPS_V1_API_URL = f\"{FIDESOPS_URL}{ops_urls.V1_URL_PREFIX}\"\n    ROOT_CLIENT_ID = \"fidesadmin\"\n    ROOT_CLIENT_SECRET = \"fidesadminsecret\"\n\n    POSTGRES_SERVER = \"host.docker.internal\"\n    POSTGRES_USER = \"postgres\"\n    POSTGRES_PASSWORD = \"postgres\"\n    POSTGRES_PORT = 6432\n    POSTGRES_DB_NAME = \"postgres_example\"\n\n    MONGO_SERVER = \"mongodb_example\"\n    MONGO_USER = \"mongo_user\"\n    MONGO_PASSWORD = \"mongo_pass\"\n    MONGO_PORT = 27017\n    MONGO_DB = \"mongo_test\"\n\n    print(\n        \"-------------------------------------------------------------------------------------\"\n    )\n    print(\"Setting up the fidesops environment with the following test configuration:\")\n    print(f\"  FIDESOPS_URL = {FIDESOPS_URL}\")\n    print(f\"  FIDESOPS_V1_API_URL = {FIDESOPS_V1_API_URL}\")\n    print(f\"  ROOT_CLIENT_ID = {ROOT_CLIENT_ID}\")\n    print(f\"  ROOT_CLIENT_SECRET = {ROOT_CLIENT_SECRET}\")\n    print(f\"  POSTGRES_SERVER = {POSTGRES_SERVER}\")\n    print(f\"  POSTGRES_USER = {POSTGRES_USER}\")\n    print(f\"  POSTGRES_PASSWORD = {POSTGRES_PASSWORD}\")\n    print(f\"  POSTGRES_PORT = {POSTGRES_PORT}\")\n    print(f\"  POSTGRES_DB_NAME = {POSTGRES_DB_NAME}\")\n    print(f\"  MONGO_SERVER = {MONGO_SERVER}\")\n    print(f\"  MONGO_USER = {MONGO_USER}\")\n    print(f\"  MONGO_PASSWORD = {MONGO_PASSWORD}\")\n    print(f\"  MONGO_PORT = {MONGO_PORT}\")\n    print(f\"  MONGO_DB = {MONGO_DB}\")\n\n    print(\n        \"-------------------------------------------------------------------------------------\"\n    )\n    print(\n        \"\"\"\n      \n               ...  Set up basic configuration\n          \n    \"\"\"\n    )\n    print(\n        \"-------------------------------------------------------------------------------------\"\n    )\n\n    # Create a new OAuth client to use for your app\n    print(\"Press [enter] to create an Oauth Token...\")\n    input()\n\n    root_token = get_access_token(\n        client_id=CONFIG.security.oauth_root_client_id,\n        client_secret=CONFIG.security.oauth_root_client_secret,\n    )\n    root_oauth_header = {\"Authorization\": f\"Bearer {root_token}\"}\n    client = create_oauth_client()\n    access_token = get_access_token(\n        client_id=client[\"client_id\"], client_secret=client[\"client_secret\"]\n    )\n    # In scope for all the methods below to use\n    oauth_header = {\"Authorization\": f\"Bearer {access_token}\"}\n\n    # Connect to your PostgreSQL database\n    print(\n        \"-------------------------------------------------------------------------------------\"\n    )\n    print(\"Press [enter] to connect fidesops to your test PostgreSQL database...\")\n    input()\n\n    create_connection(\n        key=\"test_application_postgres_db\", connection_type=ConnectionType.postgres\n    )\n    configure_postgres_connection(\n        key=\"test_application_postgres_db\",\n        host=POSTGRES_SERVER,\n        port=POSTGRES_PORT,\n        dbname=POSTGRES_DB_NAME,\n        username=POSTGRES_USER,\n        password=POSTGRES_PASSWORD,\n    )\n\n    # Connect to your Mongo database\n    print(\n        \"-------------------------------------------------------------------------------------\"\n    )\n    print(\"Press [enter] to connect fidesops to your test Mongo database...\")\n    input()\n\n    create_connection(\n        key=\"test_application_mongo_db\", connection_type=ConnectionType.mongodb\n    )\n    sleep(5)\n    configure_mongo_connection(\n        key=\"test_application_mongo_db\",\n        host=MONGO_SERVER,\n        port=MONGO_PORT,\n        dbname=MONGO_DB,\n        username=MONGO_USER,\n        password=MONGO_PASSWORD,\n    )\n\n    # Upload the dataset YAML for your PostgreSQL schema\n    print(\n        \"-------------------------------------------------------------------------------------\"\n    )\n    print(\n        \"Press [enter] to define the data categories and relationships in your Postgres tables...\"\n    )\n    input()\n\n    validate_dataset(\n        connection_key=\"test_application_postgres_db\",\n        yaml_path=\"data/dataset/postgres_example_test_dataset.yml\",\n    )\n    postgres_dataset = create_dataset(\n        connection_key=\"test_application_postgres_db\",\n        yaml_path=\"data/dataset/postgres_example_test_dataset.yml\",\n    )\n\n    # Upload the dataset YAML for your MongoDB schema\n    print(\n        \"-------------------------------------------------------------------------------------\"\n    )\n    print(\n        \"Press [enter] to define the data categories and relationships in your Mongo collections...\"\n    )\n    input()\n\n    mongo_dataset = create_dataset(\n        connection_key=\"test_application_mongo_db\",\n        yaml_path=\"data/dataset/mongo_example_test_dataset.yml\",\n    )\n\n    # Configure a storage config to upload the results\n    print(\n        \"-------------------------------------------------------------------------------------\"\n    )\n    print(\n        \"Press [enter] to configure a storage destination to upload your final results (just local for now)...\"\n    )\n    input()\n\n    create_local_storage(\n        key=\"example_storage\",\n        file_format=\"json\",\n    )\n\n    # Create a policy that returns all user identifiable contact data\n    print(\n        \"-------------------------------------------------------------------------------------\"\n    )\n    print(\n        \"\"\"\n       \n              ...  Create an access policy rule\n            \n    \"\"\"\n    )\n    print(\n        \"-------------------------------------------------------------------------------------\"\n    )\n    data_category = \"user\"\n    print(\n        f\"Press [enter] to create a Policy Rule that accesses information with the data category '{data_category}':\"\n    )\n    input()\n\n    create_dsr_policy(\n        key=\"example_request_policy\",\n    )\n    # Delete any existing policy rule so we can reconfigure it based on input\n    delete_policy_rule(\n        policy_key=\"example_request_policy\",\n        key=\"access_user_data\",\n    )\n    create_dsr_policy_rule(\n        policy_key=\"example_request_policy\",\n        key=\"access_user_data\",\n        action_type=ActionType.access,\n        storage_destination_key=\"example_storage\",\n    )\n    create_dsr_policy_rule_target(\n        policy_key=\"example_request_policy\",\n        rule_key=\"access_user_data\",\n        data_cat=data_category,\n    )\n\n    print(\n        \"-------------------------------------------------------------------------------------\"\n    )\n    print(\n        \"\"\"\n       \n                ...  Run an access privacy request\n             \n    \"\"\"\n    )\n\n    # Execute a privacy request\n    print(\n        \"-------------------------------------------------------------------------------------\"\n    )\n    email = \"jane@example.com\"\n    print(\n        f\"Press [enter] to run an access request for {email} with Policy `example_request_policy`:\"\n    )\n    input()\n    print(\"Please wait...\")\n    privacy_requests = create_privacy_request(\n        user_email=email,\n        policy_key=\"example_request_policy\",\n    )\n    privacy_request_id = privacy_requests[\"succeeded\"][0][\"id\"]\n    print_results(request_id=privacy_request_id)\n\n    sleep(2)\n\n    print(\n        \"-------------------------------------------------------------------------------------\"\n    )\n    print(\n        \"\"\"\n       \n               ...  Create an erasure policy rule\n            \n    \"\"\"\n    )\n    print(\n        \"-------------------------------------------------------------------------------------\"\n    )\n\n    # Create a policy that erases all user data\n    print(\n        f\"Press [enter] to create a Policy Rule describing how to erase information with the data category `{data_category}`:\"\n    )\n    input()\n\n    create_dsr_policy(\n        key=\"example_erasure_policy\",\n    )\n    # Delete any existing policy rule so we can reconfigure it based on input\n    delete_policy_rule(\n        policy_key=\"example_erasure_policy\",\n        key=\"erase_user_data\",\n    )\n    create_dsr_policy_rule(\n        policy_key=\"example_erasure_policy\",\n        key=\"erase_user_data\",\n        action_type=ActionType.erasure,\n    )\n    create_dsr_policy_rule_target(\n        policy_key=\"example_erasure_policy\",\n        rule_key=\"erase_user_data\",\n        data_cat=data_category,\n    )\n\n    print(\n        \"-------------------------------------------------------------------------------------\"\n    )\n    print(\n        \"\"\"\n        \n               ...  Issue an erasure privacy request and verify\n              \n    \"\"\"\n    )\n    print(\n        \"-------------------------------------------------------------------------------------\"\n    )\n\n    # Execute a privacy request for jane@example.com\n    email = \"jane@example.com\"\n    print(\n        f\"Press [enter] to issue an erasure request for email {email}: with policy `example_erasure_policy`\"\n    )\n    input()\n    print(\"Please wait...\")\n    privacy_requests = create_privacy_request(\n        user_email=email,\n        policy_key=\"example_erasure_policy\",\n    )\n    erasure_privacy_request_id = privacy_requests[\"succeeded\"][0][\"id\"]\n\n    print(\n        f\"Press [enter] to issue a follow-up access request to confirm removal of user data for {email}:\"\n    )\n    input()\n    print(\"Please wait...\")\n    privacy_requests = create_privacy_request(\n        user_email=email,\n        policy_key=\"example_request_policy\",\n    )\n    print(\n        \"-------------------------------------------------------------------------------------\"\n    )\n    access_result_id = privacy_requests[\"succeeded\"][0][\"id\"]\n    print_results(request_id=access_result_id)\n    print(f\"Jane's data has been removed for data category `{data_category}`.\")\n    exit(0)\n"}
{"type": "source_file", "path": "scripts/mssql_discover.py", "content": "# This file is not committed to the repo, please create secrets.py with the required\n# variables in the same dir as this file before running this script\nfrom secrets import DB, IP, PASS, PORT, USER\n\nimport sqlalchemy\n\nMASTER_MSSQL_URL = f\"mssql+pymssql://{USER}:{PASS}@{IP}:{PORT}/{DB}\"\n\n\nSUPPORTED_DATA_TYPES = set(\n    [\n        # char types\n        \"varchar\",\n        \"nvarchar\",\n        \"char\",\n        \"nchar\",\n        \"ntext\",\n        \"text\",\n        # numeric types\n        \"int\",\n        \"bigint\",\n        \"smallint\",\n        \"tinyint\",\n        \"money\",\n        \"float\",\n        \"decimal\",\n        # date types\n        \"date\",\n        \"datetime\",\n        \"datetime2\",\n        \"smalldatetime\",\n        # other types\n        \"bit\",\n    ]\n)\n\n\ndef mssql_discover():\n    \"\"\"\n    Select all databases from the instance\n    Select the schema data for each data base\n    Check if there are any fields in the schema that Fidesops does not yet support\n    \"\"\"\n    engine = sqlalchemy.create_engine(MASTER_MSSQL_URL)\n    all_dbs = engine.execute(\"SELECT name FROM sys.databases;\").all()\n    all_columns = []\n    flagged_columns = []\n    flagged_datatypes = set()\n    for db_name in all_dbs:\n        db_name = db_name[0]\n        try:\n            columns = engine.execute(\n                f\"SELECT TABLE_NAME, COLUMN_NAME, DATA_TYPE FROM {db_name}.INFORMATION_SCHEMA.COLUMNS;\"\n            ).all()\n        except Exception:\n            continue\n\n        all_columns.extend(columns)\n        for table, column, data_type in columns:\n            if data_type not in SUPPORTED_DATA_TYPES:\n                flagged_datatypes.add(data_type)\n                flagged_columns.append(f\"{db_name}.{table}.{column}: {data_type}\")\n\n    print(f\"{len(all_columns)} columns found\")\n    print(f\"{len(flagged_columns)} columns flagged\")\n    print(f\"Flagged datatypes:\")\n    print(\",\\n\".join(flagged_datatypes))\n    print(f\"Flagged columns:\")\n    print(\",\\n\".join(flagged_columns))\n\n\nif __name__ == \"__main__\":\n    mssql_discover()\n"}
{"type": "source_file", "path": "src/fides/api/alembic/migrations/versions/14acee6f5459_translation_data.py", "content": "\"\"\"migrate translation data - notices and experiences\n\nRevision ID: 14acee6f5459\nRevises: 0c65325843bd\nCreate Date: 2024-01-09 21:17:13.115020\n\n\"\"\"\n\nimport csv\nimport uuid\nfrom datetime import datetime\nfrom enum import Enum\nfrom typing import Optional\n\nfrom alembic import op\nfrom loguru import logger\nfrom sqlalchemy import text\nfrom sqlalchemy.engine import ResultProxy\n\nfrom fides.api.alembic.migrations.helpers.database_functions import generate_record_id\n\n# revision identifiers, used by Alembic.\n\nrevision = \"14acee6f5459\"\ndown_revision = \"0c65325843bd\"\nbranch_labels = None\ndepends_on = None\n\n\nEXPERIENCE_CONFIG_DEFAULTS = [\n    {\n        \"id\": \"pri-76b8-cc52-11ee-9eaa-8ef97a-modal-config\",\n        \"name\": \"US Modal\",\n        \"regions\": [\"us_ca\", \"us_co\", \"us_ct\", \"us_ut\", \"us_va\"],\n        \"component\": \"modal\",\n        \"allow_language_selection\": False,\n        \"auto_detect_language\": True,\n        \"dismissable\": True,\n        \"disabled\": True,\n        \"privacy_notice_keys\": [\"data_sales_and_sharing\"],\n        \"origin\": \"pri-76b8-cc52-11ee-9eaa-8ef97a-modal\",\n    },\n    {\n        \"id\": \"pri-27d4-cc53-11ee-9eaa-8ef97a04-pri-config\",\n        \"name\": \"US Privacy Center\",\n        \"regions\": [\"us_ca\", \"us_co\", \"us_ct\", \"us_ut\", \"us_va\"],\n        \"component\": \"privacy_center\",\n        \"allow_language_selection\": False,\n        \"auto_detect_language\": True,\n        \"dismissable\": True,\n        \"disabled\": True,\n        \"privacy_notice_keys\": [\"data_sales_and_sharing\"],\n        \"origin\": \"pri-27d4-cc53-11ee-9eaa-8ef97a04-pri\",\n    },\n    {\n        \"id\": \"pri-c8ff-78d6-4a02-850f-2c0ban-modal-config\",\n        \"name\": \"EEA Banner + Modal\",\n        \"regions\": [\n            \"be\",\n            \"bg\",\n            \"cz\",\n            \"dk\",\n            \"de\",\n            \"ee\",\n            \"ie\",\n            \"gr\",\n            \"es\",\n            \"fr\",\n            \"hr\",\n            \"it\",\n            \"cy\",\n            \"lv\",\n            \"lt\",\n            \"lu\",\n            \"hu\",\n            \"mt\",\n            \"nl\",\n            \"at\",\n            \"pl\",\n            \"pt\",\n            \"ro\",\n            \"si\",\n            \"sk\",\n            \"fi\",\n            \"se\",\n            \"gb\",\n            \"is\",\n            \"no\",\n            \"li\",\n        ],\n        \"component\": \"banner_and_modal\",\n        \"allow_language_selection\": False,\n        \"auto_detect_language\": True,\n        \"dismissable\": True,\n        \"disabled\": True,\n        \"privacy_notice_keys\": [\"essential\", \"functional\", \"analytics\", \"marketing\"],\n        \"origin\": \"pri-c8ff-78d6-4a02-850f-2c0ban-modal\",\n    },\n    {\n        \"id\": \"pri-694e-02bd-4afe-81b3-2a2ban-modal-config\",\n        \"name\": \"Canada Banner + Modal\",\n        \"regions\": [\n            \"ca\",\n        ],\n        \"component\": \"banner_and_modal\",\n        \"allow_language_selection\": False,\n        \"auto_detect_language\": True,\n        \"dismissable\": True,\n        \"disabled\": True,\n        \"privacy_notice_keys\": [\"marketing\"],\n        \"origin\": \"pri-694e-02bd-4afe-81b3-2a2ban-modal\",\n    },\n    {\n        \"id\": \"pri-d9ed6-25e7-4953-8977-772ban-modal-config\",\n        \"name\": \"Quebec Banner + Modal\",\n        \"regions\": [\n            \"ca_qc\",\n        ],\n        \"component\": \"banner_and_modal\",\n        \"allow_language_selection\": False,\n        \"auto_detect_language\": True,\n        \"dismissable\": True,\n        \"disabled\": True,\n        \"privacy_notice_keys\": [\"essential\", \"analytics\", \"marketing\"],\n        \"origin\": \"pri-d9ed6-25e7-4953-8977-772ban-modal\",\n    },\n    {\n        \"id\": \"d7c3ce0a-a3b3-43ff-bf6f-3d8-tcf-over-config\",\n        \"name\": \"TCF\",\n        \"allow_language_selection\": False,\n        \"auto_detect_language\": True,\n        \"dismissable\": True,\n        \"disabled\": True,\n        \"notices\": [],\n        \"regions\": [\n            \"be\",\n            \"bg\",\n            \"cz\",\n            \"dk\",\n            \"de\",\n            \"ee\",\n            \"ie\",\n            \"gr\",\n            \"es\",\n            \"fr\",\n            \"hr\",\n            \"it\",\n            \"cy\",\n            \"lv\",\n            \"lt\",\n            \"lu\",\n            \"hu\",\n            \"mt\",\n            \"nl\",\n            \"at\",\n            \"pl\",\n            \"pt\",\n            \"ro\",\n            \"si\",\n            \"sk\",\n            \"fi\",\n            \"se\",\n            \"gb\",\n            \"is\",\n            \"no\",\n            \"li\",\n            \"eea\",\n        ],\n        \"component\": \"tcf_overlay\",\n        \"privacy_notice_keys\": [],\n        \"origin\": \"d7c3ce0a-a3b3-43ff-bf6f-3d8-tcf-over\",\n    },\n]\n\n# the known existing (legacy) experience config IDs.\n# we will only attempt to migrate config data from these experience configs\nKNOWN_EXISTING_EXPERIENCE_CONFIGS = {\n    \"pri-7ae3-f06b-4096-970f-0bbbdef-over\",\n    \"pri-097a-d00d-40b6-a08f-f8e50def-pri\",\n    \"a4974670-abad-471f-9084-2cb-tcf-over\",\n}\n\n\n# this ties a DB record ID to our logical identifier of the type of OOB experience.\n# the DB record ID is based on what we've defined in the associated config yml.\nclass DefaultExperienceConfigTypes(Enum):\n    EEA_BANNER_AND_MODAL = \"pri-c8ff-78d6-4a02-850f-2c0ban-modal-config\"\n    EEA_TCF_OVERLAY = \"d7c3ce0a-a3b3-43ff-bf6f-3d8-tcf-over-config\"\n    US_MODAL = \"pri-76b8-cc52-11ee-9eaa-8ef97a-modal-config\"\n    US_PRIVACY_CENTER = \"pri-27d4-cc53-11ee-9eaa-8ef97a04-pri-config\"\n    CANADA_BANNER_MODAL = \"pri-694e-02bd-4afe-81b3-2a2ban-modal-config\"\n    QUEBEC_BANNER_MODAL = \"pri-d9ed6-25e7-4953-8977-772ban-modal-config\"\n\n\nclass ComponentType(Enum):\n    PrivacyCenter = \"privacy_center\"\n    Overlay = \"overlay\"\n    TCFOverlay = \"tcf_overlay\"\n\n\n# our new OOB EEA (EU) experience configs, keyed by their component type\nEEA_MAPPING = {\n    ComponentType.Overlay: DefaultExperienceConfigTypes.EEA_BANNER_AND_MODAL,\n    ComponentType.TCFOverlay: DefaultExperienceConfigTypes.EEA_TCF_OVERLAY,\n}\n\n# our new OOB US experience configs, keyed by their component type\nUS_MAPPING = {\n    ComponentType.PrivacyCenter: DefaultExperienceConfigTypes.US_PRIVACY_CENTER,\n    ComponentType.Overlay: DefaultExperienceConfigTypes.US_MODAL,\n}\n\n# our new OOB Canada experience configs, keyed by their component type\nCANADA_MAPPING = {\n    ComponentType.Overlay: DefaultExperienceConfigTypes.CANADA_BANNER_MODAL,\n}\n\nQUEBEC_MAPPING = {\n    ComponentType.Overlay: DefaultExperienceConfigTypes.QUEBEC_BANNER_MODAL,\n}\n\n# this map contains the biggest _assumption_ about the migration -\n# these are the notices that will be associated with our new OOB experience configs.\n# any existing notices besides these will remain \"orphaned\" post-migration.\n# additionally, the regions that (pre-migration) are associated with these notices\n# will be used to infer the regions associated with the post-migration experience\n# configs to which they map.\nNOTICES_TO_EXPERIENCE_CONFIG = {\n    \"marketing\": [EEA_MAPPING, CANADA_MAPPING, QUEBEC_MAPPING],\n    \"functional\": [EEA_MAPPING],\n    \"essential\": [EEA_MAPPING, QUEBEC_MAPPING],\n    \"analytics\": [EEA_MAPPING, QUEBEC_MAPPING],\n    \"data_sales_and_sharing\": [US_MAPPING],\n}\n\n\n# as part of this upgrade, we're removing the following GB sub-regions as region options across the app\nGB_REGIONS_TO_REMOVE = {\n    \"gb_eng\",\n    \"gb_sct\",\n    \"gb_wls\",\n    \"gb_nir\",\n}\n# the GB sub-regions are replaced by the `gb` region\nGB_REGIONS_TO_ADD = {\n    \"gb\",\n}\n\n# Canadian regions are ignored in the migration of existing data, since we create a net-new OOB experience config for Canada\nCA_REGIONS_TO_IGNORE = {\n    \"ca_qc\",\n    \"ca_nt\",\n    \"ca_nb\",\n    \"ca_nu\",\n    \"ca_bc\",\n    \"ca_mb\",\n    \"ca_ns\",\n    \"ca_pe\",\n    \"ca_sk\",\n    \"ca_yt\",\n    \"ca_on\",\n    \"ca_nl\",\n    \"ca_ab\",\n    \"ca\",\n}\n\n\ndef dump_table_to_csv(bind, table_name):\n    csv_file_path = f'./multilang_migration_backup_{table_name}_{datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")}.csv'\n    query = text(f\"SELECT * FROM {table_name}\")\n    result = bind.execute(query)\n\n    # Get column names from the result set\n    column_names = result.keys()\n\n    try:\n        # Write results to CSV\n        with open(csv_file_path, \"w\") as csv_file:\n            csv_writer = csv.writer(csv_file)\n\n            # Write header row\n            csv_writer.writerow(column_names)\n\n            # Write data rows\n            for row in result:\n                csv_writer.writerow(row)\n    except PermissionError:\n        logger.exception(\n            f\"Permission error writing migration backup file {csv_file_path}\"\n        )\n\n\ndef load_default_experience_configs():\n    \"\"\"\n    Loads default experience config definitions from yml file.\n\n    Returns two dicts in a tuple:\n        - first dict holds \"new\" experience config records that will be used to reconcile existing data\n        - second dict holds the \"raw\" experience config records based on the definitions on disk. these will be used as a reference for e.g. template creation.\n    \"\"\"\n    _raw_experience_config_map = (\n        {}\n    )  # this will just hold config templates exactly as they are in the 'oob' file\n    _reconciled_experience_config_map = (\n        {}\n    )  # will hold experience configs 'reconciled' between existing data and 'oob' template data\n\n    for experience_config in EXPERIENCE_CONFIG_DEFAULTS:\n        experience_config_type = DefaultExperienceConfigTypes(experience_config[\"id\"])\n        _raw_experience_config_map[experience_config_type] = experience_config.copy()\n\n        # the reconciled experience config will have its regions populated by existing notices that are\n        # associated with this experience config. so in most cases, we start \"fresh\" with an empty set.\n        # the TCF experience and Canada/Quebec banner modal experiences are exceptions:\n        # their regions are not derived from existing notices (TCF is not linked to notices; Canada experience config is net-new),\n        # so we hardcode their regions to the config defaults.\n        if experience_config_type not in (\n            DefaultExperienceConfigTypes.EEA_TCF_OVERLAY,\n            DefaultExperienceConfigTypes.CANADA_BANNER_MODAL,\n            DefaultExperienceConfigTypes.QUEBEC_BANNER_MODAL,\n        ):\n            experience_config[\"regions\"] = set()\n        else:\n            experience_config[\"regions\"] = set(experience_config[\"regions\"])\n\n        experience_config[\"privacy_notices\"] = set()\n        experience_config[\"needs_migration\"] = (\n            False  # indicator whether the record requires migration, or we can default to the template values\n        )\n        _reconciled_experience_config_map[experience_config_type] = experience_config\n        experience_config[\"needs_migration\"] = False\n    return _reconciled_experience_config_map, _raw_experience_config_map\n\n\n(\n    reconciled_experience_config_map,\n    raw_experience_config_map,\n) = load_default_experience_configs()\n\n\ndef backup_existing_data_csv(bind):\n    \"\"\"\n    Perform a backup (dump to csv) of the data in our tables that will be impacted by migration.\n\n    Because this data migration is risky, it is recommended to perform a comprehensive DB backup/snapshot beforehand!\n    The \"automated\" dump to CSV here is an additional safety measure.\n    \"\"\"\n    for table_name in [\n        \"privacyexperience\",\n        \"privacyexperienceconfig\",\n        \"privacyexperienceconfighistory\",\n        \"privacynotice\",\n        \"privacynoticehistory\",\n    ]:\n        try:\n            dump_table_to_csv(bind, table_name)\n        except Exception:\n            # permission error is handled within the function, but just to be extra safe\n            # let's make sure our migration proceeds if ANY error is thrown from the CSV dump,\n            # since this is just a nice-to-have.\n            logger.exception(f\"Error backing up table {table_name} to disk\")\n\n\ndef remove_existing_experience_data(bind):\n    \"\"\"\n    We remove any existing PrivacyExperience and PrivacyExperienceConfig records,\n    to allow for a \"clean-slate\" in creation of _new_ PrivacyExperienceConfigs based on the\n    \"out of the box\" templates.\n\n    This means that any existing user-edited data on PrivacyExperienceConfig records will need to be\n    _manually_ re-added to the post-migration state. It will not be migrated as part of the automatic migration.\n    \"\"\"\n\n    def delete_experiences(bind):\n        \"\"\"\n        Delete all PrivacyExperience records.\n\n        These records will be effectively re-created as a result of the \"out of the box\" creation\n        of 'new-style' PrivacyExperienceConfig records post-migration.\n        \"\"\"\n        bind.execute(\n            text(\n                \"\"\"\n                DELETE FROM privacyexperience;\n                \"\"\"\n            )\n        )\n\n    def delete_experience_configs(bind):\n        \"\"\"\n        Delete all PrivacyExperienceConfig records.\n\n        These records will be effectively re-created via the server's \"out of the box\" creation\n        of the updated PrivacyExperienceConfig records, post-migration.\n        \"\"\"\n        bind.execute(\n            text(\n                \"\"\"\n                DELETE FROM privacyexperienceconfig;\n                \"\"\"\n            )\n        )\n\n    delete_experiences(bind)\n    delete_experience_configs(bind)\n\n\ndef is_canada_or_quebec_experience(\n    config_type: Optional[DefaultExperienceConfigTypes],\n) -> bool:\n    \"\"\"Helper to determine whether the target experience config type is a canada or quebec experience\"\"\"\n    return config_type in {\n        DefaultExperienceConfigTypes.CANADA_BANNER_MODAL,\n        DefaultExperienceConfigTypes.QUEBEC_BANNER_MODAL,\n    }\n\n\ndef determine_needed_experience_configs(bind):\n    \"\"\"\n    Our data model changes necessitate the creation of new Experience Configs. Now that \"where\" a Notice is served\n    is being moved from the \"Notice\" constructs to \"Experiences\", existing Experience Configs are not sufficient\n    to capture the full range of configuration on current Notices.\n    \"\"\"\n\n    notice_query = \"\"\"select id, name, regions, disabled, displayed_in_overlay, displayed_in_privacy_center, notice_key from privacynotice;\"\"\"\n    experience_config_query = \"\"\"SELECT id, component, disabled, accept_button_label, description, privacy_preferences_link_label, privacy_policy_link_label, privacy_policy_url, save_button_label, title, banner_description, banner_title, reject_button_label, acknowledge_button_label from privacyexperienceconfig;\"\"\"\n\n    existing_notices: ResultProxy = bind.execute(text(notice_query))\n    for existing_notice in existing_notices:\n        notice_key = existing_notice[\"notice_key\"]\n        component_mappings = NOTICES_TO_EXPERIENCE_CONFIG.get(notice_key, [])\n        if component_mappings:\n            for component_mapping in component_mappings:\n                if is_canada_or_quebec_experience(\n                    component_mapping.get(ComponentType.Overlay)\n                ):\n                    # If this is a Canadian/Quebec Banner and Modal, skip updating regions, and just use OOB\n                    regions_to_add = set()\n                else:\n                    regions_to_add = set(existing_notice[\"regions\"]).copy()\n\n                    # remove GB subregions and replace with top-level `gb` region!\n                    if not GB_REGIONS_TO_REMOVE.isdisjoint(\n                        regions_to_add\n                    ):  # if we have GB regions to remove\n                        regions_to_add.difference_update(\n                            GB_REGIONS_TO_REMOVE\n                        )  # remove the GB regions\n                        regions_to_add.update(GB_REGIONS_TO_ADD)  # add the GB region\n\n                    # ignore any canada regions in migration - they are not migrated,\n                    # but instead handled by a net-new OOB experience\n                    regions_to_add.difference_update(CA_REGIONS_TO_IGNORE)\n\n                if existing_notice[\"displayed_in_overlay\"]:\n                    experience_config = component_mapping.get(ComponentType.Overlay)\n                    if experience_config:\n                        reconciled_experience_config_map[experience_config][\n                            \"regions\"\n                        ].update(regions_to_add)\n                        reconciled_experience_config_map[experience_config][\n                            \"privacy_notices\"\n                        ].add(existing_notice[\"id\"])\n                        if not existing_notice[\n                            \"disabled\"\n                        ] and not is_canada_or_quebec_experience(experience_config):\n                            # Disabled by default, but if not a Canada/Quebec experience and any notices are enabled,\n                            # let's enable this Experience Config as the safer default\n                            reconciled_experience_config_map[experience_config][\n                                \"disabled\"\n                            ] = False\n                        reconciled_experience_config_map[experience_config][\n                            \"needs_migration\"\n                        ] = True\n\n                if existing_notice[\"displayed_in_privacy_center\"]:\n                    experience_config = component_mapping.get(\n                        ComponentType.PrivacyCenter\n                    )\n                    if experience_config:\n                        reconciled_experience_config_map[experience_config][\n                            \"regions\"\n                        ].update(regions_to_add)\n                        reconciled_experience_config_map[experience_config][\n                            \"privacy_notices\"\n                        ].add(existing_notice[\"id\"])\n                        if not existing_notice[\n                            \"disabled\"\n                        ] and not is_canada_or_quebec_experience(experience_config):\n                            # Disabled by default, but if not a Canada/Quebec experience and any notices are enabled,\n                            # let's enable this Experience Config as the safer default\n                            reconciled_experience_config_map[experience_config][\n                                \"disabled\"\n                            ] = False\n\n                        reconciled_experience_config_map[experience_config][\n                            \"needs_migration\"\n                        ] = True\n\n    existing_experience_configs: ResultProxy = bind.execute(\n        text(experience_config_query)\n    )\n    has_existing_experience_configs = False\n    for eec in existing_experience_configs:\n        # only migrate experience config data if its a known config\n        if eec[\"id\"] in KNOWN_EXISTING_EXPERIENCE_CONFIGS:\n            has_existing_experience_configs = True\n            component = ComponentType(eec[\"component\"])\n            configs = []\n\n            # because we've given experiences a location dimension in this migration,\n            # an existing experience effectively maps to two \"new\" default experiences -\n            # one that is surfaced in the US (and is associated with \"US\" notices), and\n            # one that is surfaced in the EU (and is associated with the \"EU\" notices).\n            # because of this, we'll copy over existing experience text/attributes to _both_\n            # of these corresponding new experience configs.\n            if eea_config_id := EEA_MAPPING.get(component):\n                configs.append(reconciled_experience_config_map.get(eea_config_id))\n            if us_config_id := US_MAPPING.get(component):\n                configs.append(reconciled_experience_config_map.get(us_config_id))\n            if canada_config_id := CANADA_MAPPING.get(component):\n                configs.append(reconciled_experience_config_map.get(canada_config_id))\n            if quebec_config_id := QUEBEC_MAPPING.get(component):\n                configs.append(reconciled_experience_config_map.get(quebec_config_id))\n            for config in configs:\n                if config:\n                    config[\"accept_button_label\"] = eec[\"accept_button_label\"]\n                    config[\"acknowledge_button_label\"] = eec[\"acknowledge_button_label\"]\n                    config[\"banner_description\"] = eec[\"banner_description\"]\n                    config[\"banner_title\"] = eec[\"banner_title\"]\n                    config[\"description\"] = eec[\"description\"]\n                    config[\"privacy_policy_link_label\"] = eec[\n                        \"privacy_policy_link_label\"\n                    ]\n                    config[\"privacy_policy_url\"] = eec[\"privacy_policy_url\"]\n                    config[\"privacy_preferences_link_label\"] = eec[\n                        \"privacy_preferences_link_label\"\n                    ]\n                    config[\"reject_button_label\"] = eec[\"reject_button_label\"]\n                    config[\"save_button_label\"] = eec[\"save_button_label\"]\n                    config[\"title\"] = eec[\"title\"]\n                    config[\"needs_migration\"] = True\n\n                    if eec[\"component\"] == \"tcf_overlay\":\n                        # TCF doesn't have notices so enable the new TCF overlay if its existing overlay is enabled\n                        config[\"disabled\"] = eec[\"disabled\"]\n\n    if has_existing_experience_configs:\n        return reconciled_experience_config_map\n\n    # if no existing experience configs are found, don't return the map, because no experience config migration is needed.\n    # instead, we'll rely on the OOB loading of experience configs on server startup.\n    # basically, this is allows us to bypass the data migration if we're not actually migrating a db that's been in use!\n    return {}\n\n\ndef migrate_experiences(bind):\n    \"\"\"\n    Migrates data from any existing ExperienceConfig records to the new, translation-based data model, and reconciles\n    the old ExperienceConfig records with new, OOB ExperienceConfig records.\n\n    - First the necessary existing ExperienceConfig and PrivacyNotice data is retrieved and stored in memory\n    - It is then, in memory, reconciled with our new OOB ExperienceConfig records, which are loaded from a YAML definition on disk\n    - Then, we remove all existing Experience and ExperienceConfig records to start from a 'blank slate'\n    - Then, based on the reconciled existing ExperienceConfig data, we create new ExperienceConfig and Experience records\n    \"\"\"\n\n    def create_new_experience_config_templates(experience_configs):\n        for experience_config in experience_configs:\n            # for config template record:\n            # - `id` is the `origin` field from the config template record loaded from disk\n            create_experience_config_template_query = text(\n                \"\"\"\n                INSERT INTO experienceconfigtemplate (id, component, name, disabled, allow_language_selection, dismissable, privacy_notice_keys, regions, auto_detect_language)\n                VALUES (:id, :component, :name, :disabled, :allow_language_selection, :dismissable, :privacy_notice_keys, :regions, :auto_detect_language)\n                \"\"\"\n            )\n\n            bind.execute(\n                create_experience_config_template_query,\n                {\n                    **experience_config,\n                    \"id\": experience_config[\n                        \"origin\"\n                    ],  # origin of the config record is the template ID\n                },\n            )\n\n    def create_new_experience_config(experience_config):\n        \"\"\"\n        Creates a new experience config based on the provided definition. Also creates corresponding history record.\n\n        \"\"\"\n        # for config record:\n        # - `id` is the `id` grabbed from our config template record\n        # - `origin` is the `origin` grabbed from the config template\n        create_experience_config_query = text(\n            \"\"\"\n            INSERT INTO privacyexperienceconfig (id, version, origin, component, name, disabled, is_default, allow_language_selection, dismissable, auto_detect_language, accept_button_label, description, privacy_preferences_link_label, privacy_policy_link_label, privacy_policy_url, save_button_label, title, banner_description, banner_title, reject_button_label, acknowledge_button_label)\n            VALUES (:id,  :version, :origin, :component, :name, :disabled, :is_default, :allow_language_selection, :dismissable, :auto_detect_language, :accept_button_label, :description, :privacy_preferences_link_label, :privacy_policy_link_label, :privacy_policy_url, :save_button_label, :title, :banner_description, :banner_title, :reject_button_label, :acknowledge_button_label)\n            \"\"\"\n        )\n\n        bind.execute(\n            create_experience_config_query,\n            {\n                **experience_config,\n                \"version\": 1,\n                \"is_default\": True,\n            },\n        )\n\n        # for history record:\n        # - `id` is a generated UUID\n        # - `experience_config_id` is the `id` grabbed from our config template record on disk\n        # - `origin` is the `origin` grabbed from the config template record on disk\n        create_experience_config_history = text(\n            \"\"\"\n             INSERT INTO privacyexperienceconfighistory (id, experience_config_id, origin, name, disabled, version, component, is_default, allow_language_selection, auto_detect_language, dismissable, accept_button_label, description, privacy_preferences_link_label, privacy_policy_link_label, privacy_policy_url, save_button_label, title, banner_description, banner_title, reject_button_label, acknowledge_button_label, language)\n             VALUES (:history_id, :id, :origin, :name, :disabled, :version, :component, :is_default, :allow_language_selection, :auto_detect_language, :dismissable, :accept_button_label, :description, :privacy_preferences_link_label, :privacy_policy_link_label, :privacy_policy_url, :save_button_label, :title, :banner_description, :banner_title, :reject_button_label, :acknowledge_button_label, :language)\n            \"\"\"\n        )\n\n        bind.execute(\n            create_experience_config_history,\n            {\n                **experience_config,\n                \"history_id\": generate_record_id(\"pri\"),\n                \"version\": 1,\n                \"is_default\": True,\n                \"language\": \"en\",\n            },\n        )\n\n    def create_applicable_experiences(experience_config):\n        \"\"\"\n        Creates all the applicable experience records based on the provided experience config definition.\n\n        i.e., looks at all regions on the provided experience config definition, and creates a corresponding\n        experience record for each region.\n        \"\"\"\n        new_experience = text(\n            \"\"\"\n            INSERT INTO privacyexperience (id, experience_config_id, region)\n            VALUES(:experience_id, :experience_config_id, :region)\n            \"\"\"\n        )\n        for region in experience_config[\"regions\"]:\n            if region is not None:\n                bind.execute(\n                    new_experience,\n                    {\n                        \"experience_id\": generate_record_id(\"pri\"),\n                        \"experience_config_id\": experience_config[\"id\"],\n                        \"region\": region,\n                    },\n                )\n\n    def link_notices_to_experiences(experience_config):\n        \"\"\"\n        Creates experience notice records based on the provided experience config definition.\n\n        i.e., looks at all `privacy_notices` linked to the experience config, and creates a corresponding\n        experience notice record for each linked notice.\n        \"\"\"\n        link_notice_query = text(\n            \"\"\"\n                INSERT INTO experiencenotices (id, notice_id, experience_config_id)\n                VALUES (:record_id, :notice_id, :experience_config_id)\n                \"\"\"\n        )\n        for notice_id in experience_config[\"privacy_notices\"]:\n            bind.execute(\n                link_notice_query,\n                {\n                    \"record_id\": generate_record_id(\"exp\"),\n                    \"notice_id\": notice_id,\n                    \"experience_config_id\": experience_config[\"id\"],\n                },\n            )\n\n    def create_experience_translations(bind):\n        \"\"\"\n        Creates new experience translation records for each privacy experience config record in the db.\n\n        We simply take the text that's been temporarily saved on the config record directly and move it\n        over to the translation record. The translation is assumed to be english.\n\n        This requires the privacy experience config records to have been created before this is invoked!\n\n        \"\"\"\n        get_experience_config_ids = text(\n            \"\"\"SELECT id FROM privacyexperienceconfig;\n         \"\"\"\n        )\n\n        # for translation record:\n        # - `id` is a generated UUID\n        # - `experience_config_id` is the `id` column grabbed from the experience config query result\n        # - language is assumed to be english!\n        create_translation_query = text(\n            \"\"\"\n            INSERT INTO experiencetranslation (id, experience_config_id, created_at, updated_at, language, is_default, accept_button_label, reject_button_label, acknowledge_button_label, banner_description, banner_title, description, title,  privacy_preferences_link_label, privacy_policy_link_label, privacy_policy_url, save_button_label)\n            SELECT :record_id, id, created_at, updated_at, :language, :is_default, accept_button_label, reject_button_label, acknowledge_button_label, banner_description, banner_title, description, title,  privacy_preferences_link_label, privacy_policy_link_label, privacy_policy_url, save_button_label\n            FROM privacyexperienceconfig\n            WHERE id = :experience_config_id\n            \"\"\"\n        )\n        for res in bind.execute(get_experience_config_ids):\n            # Create a translation for each Experience\n            bind.execute(\n                create_translation_query,\n                {\n                    \"record_id\": generate_record_id(\"exp\"),\n                    \"language\": \"en\",\n                    \"experience_config_id\": res[\"id\"],\n                    \"is_default\": True,\n                },\n            )\n\n        # Repoint the Experience Config History to the Experience Translation\n        link_config_to_translation_id = text(\n            \"\"\"\n            UPDATE privacyexperienceconfighistory\n            SET translation_id = experiencetranslation.id\n            FROM experiencetranslation\n            WHERE experiencetranslation.experience_config_id = privacyexperienceconfighistory.experience_config_id;\n        \"\"\"\n        )\n\n        bind.execute(link_config_to_translation_id)\n\n    # first extract existing experience (config) data needed for migration\n    experience_configs = determine_needed_experience_configs(bind)\n\n    # before wiping data, let's perform a csv backup to disk just to be extra safe\n    backup_existing_data_csv(bind)\n\n    # wipe the existing experience + experience config data to start from a blank slate\n    remove_existing_experience_data(bind)\n\n    # if we don't have experience configs, we can bypass all experience config data migration\n    if experience_configs:\n        # first create experience config templates based on raw experience config definitions\n        create_new_experience_config_templates(raw_experience_config_map.values())\n\n        # create new experience + experience configs based on old experience config data\n        # these have been reconciled with the new OOB experience config records\n        for experience_config_type in DefaultExperienceConfigTypes:\n            logger.info(f\"Migrating experience config {experience_config_type.name}\")\n            reconciled_experience_config = experience_configs.get(\n                experience_config_type, {}\n            )\n            raw_experience_config = raw_experience_config_map.get(\n                experience_config_type, {}\n            )\n\n            if not reconciled_experience_config and not raw_experience_config:\n                logger.info(\n                    f\"No reconciled or raw experience config found for {experience_config_type.name}\"\n                )\n                continue\n\n            # revert to the \"raw\" config if we haven't flagged a config as needing migration\n            # in practice, we should always have a reconciled record here that's flagged for migration,\n            # but this covers our bases in case anyone has e.g. deleted a previous OOB notice.\n            if reconciled_experience_config.get(\"needs_migration\", False):\n                logger.info(\n                    f\"Migrating reconciled experience config {reconciled_experience_config['id']}\"\n                )\n                experience_config_type = reconciled_experience_config\n            else:\n                logger.info(\n                    f\"Creating raw experience config {raw_experience_config['id']}\"\n                )\n                experience_config_type = raw_experience_config\n\n            create_new_experience_config(experience_config_type)\n            create_applicable_experiences(experience_config_type)\n            link_notices_to_experiences(experience_config_type)\n\n        # experience translations are then created based on the new experience config records\n        create_experience_translations(bind)\n\n\ndef migrate_notices(bind):\n    \"\"\"\n    Migrates existing PrivacyNotice records to the new, translation-based data model.\n\n    Notice keys are not impacted. Most existing notice data is moved onto a linked NoticeTranslation record,\n    which is assumed to be english-language as a default. If the existing notice text/content is _not_ english-language,\n    then users will need to *manually* adjust the language on the created translation record.\n\n    The existing PrivacyNoticeHistory records are adjusted to point to the associated NoticeTranslation record,\n    as is expected in the new data model.\n    \"\"\"\n\n    get_notices_query = text(\n        \"\"\"SELECT id from privacynotice;\n         \"\"\"\n    )\n\n    create_notice_translation_query = text(\n        \"\"\"\n        INSERT INTO noticetranslation (id, privacy_notice_id, language, title, description, created_at, updated_at)\n        SELECT :record_id, :original_notice_id, :language, name, description, created_at, updated_at\n        FROM privacynotice\n        WHERE id = :original_notice_id\n        \"\"\"\n    )\n    for res in bind.execute(get_notices_query):\n        # Create a Notice Translation for each notice, setting to english as a default\n        bind.execute(\n            create_notice_translation_query,\n            {\n                \"record_id\": generate_record_id(\"pri\"),\n                \"language\": \"en\",\n                \"original_notice_id\": res[\"id\"],\n            },\n        )\n\n    # Add a FK from the Notice History back to the Notice Translation\n    link_notice_history_to_translation_id = text(\n        \"\"\"\n        UPDATE privacynoticehistory\n        SET translation_id = noticetranslation.id, title = privacynoticehistory.name\n        FROM noticetranslation\n        WHERE noticetranslation.privacy_notice_id = privacynoticehistory.privacy_notice_id;\n        \"\"\"\n    )\n\n    bind.execute(link_notice_history_to_translation_id)\n\n    # Queries the latest historical version for each notice\n    new_notice_histories_to_create_query = text(\n        \"\"\"\n        select privacy_notice_id, max(version) from privacynoticehistory group by privacy_notice_id;\n        \"\"\"\n    )\n    # Create a new privacy notice history record that bumps the version and clears regions and displayed_in* fields and sets language\n    for res in bind.execute(new_notice_histories_to_create_query):\n        create_privacy_notice_history_query = text(\n            \"\"\"\n            INSERT INTO privacynoticehistory (id, name, description, origin, consent_mechanism, data_uses, version, disabled, enforcement_level, has_gpc_flag, internal_description, notice_key, gpp_field_mapping, framework, language, title, translation_id, privacy_notice_id)\n            SELECT :record_id, name, description, origin, consent_mechanism, data_uses, :new_version, disabled, enforcement_level, has_gpc_flag, internal_description, notice_key, gpp_field_mapping, framework, :language, title, translation_id, privacy_notice_id\n            FROM privacynoticehistory\n            WHERE version = :current_version AND\n            privacy_notice_id = :privacy_notice_id\n            ORDER BY created_at DESC LIMIT 1\n        \"\"\"\n        )\n        bind.execute(\n            create_privacy_notice_history_query,\n            {\n                \"record_id\": generate_record_id(\"pri\"),\n                \"language\": \"en\",\n                \"new_version\": res[\"max\"] + 1,\n                \"current_version\": res[\"max\"],\n                \"privacy_notice_id\": res[\"privacy_notice_id\"],\n            },\n        )\n\n\ndef downward_migrate_notices(bind):\n    \"\"\"\n    Back-migration of PrivacyNotice data.\n\n    Moves data in existing NoticeTranslation records back to the associated PrivacyNotice records,\n    to match the old data model.\n    # NOTE: this won't work _well_ if multiple translations are present. Should we even attempt it?\n\n    The existing PrivacyNoticeHistory records are adjusted to point back to the associated PrivacyNotice records,\n    rather than the NoticeTranslation records, as is expected in the old data model.\n\n    \"\"\"\n    noticetranslation_data_to_notice_query = text(\n        \"\"\"\n        UPDATE privacynotice\n        SET name = noticetranslation.title, description = noticetranslation.description\n        FROM noticetranslation\n        WHERE noticetranslation.privacy_notice_id = privacynotice.id\n        \"\"\"\n    )\n    bind.execute(noticetranslation_data_to_notice_query)\n\n    # Update the FK from the Notice History back to the Notice record\n    link_notice_history_to_notice = text(\n        \"\"\"\n        UPDATE privacynoticehistory\n        SET privacy_notice_id = noticetranslation.privacy_notice_id\n        FROM noticetranslation\n        WHERE noticetranslation.id = privacynoticehistory.translation_id;\n    \"\"\"\n    )\n\n    bind.execute(link_notice_history_to_notice)\n\n\ndef upgrade():\n    logger.info(\n        f\"Starting Consent Multitranslation Data Migration (#2) {datetime.now()}\"\n    )\n\n    bind = op.get_bind()\n\n    migrate_experiences(bind)\n\n    migrate_notices(bind)\n\n\ndef downgrade():\n    # Downgrade will _not_ attempt to recreate PrivacyExperienceConfig recrords,\n    # as that should be handled by the (old) server's OOB loading behavior\n    bind = op.get_bind()\n\n    downward_migrate_notices(bind)\n\n    bind.execute(\n        text(\n            \"\"\"\n            DELETE FROM experiencetranslation;\n            \"\"\"\n        )\n    )\n    remove_existing_experience_data(\n        bind\n    )  # remove existing experience data to ensure it won't prevent startup on downgraded versions!\n"}
{"type": "source_file", "path": "src/fides/api/alembic/migrations/versions/144d9b85c712_update_consent_mechanism.py", "content": "\"\"\"update consent mechanism\n\nRevision ID: 144d9b85c712\nRevises: 8188ddbf8cae\nCreate Date: 2023-04-12 14:57:07.532660\n\n\"\"\"\n\nfrom alembic import op\n\n# revision identifiers, used by Alembic.\nrevision = \"144d9b85c712\"\ndown_revision = \"8188ddbf8cae\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade():\n    \"\"\"\n    Extra steps added here to avoid the error that enums have to be committed before they can be used.  This\n    avoids having to commit in the middle of this migration and lets the entire thing be completed in a single transaction\n    \"\"\"\n    op.execute(\"alter type consentmechanism rename to consentmechanismold\")\n\n    op.execute(\n        \"create type consentmechanism as enum ('opt_in', 'opt_out', 'notice_only', 'necessary');\"\n    )\n    op.execute(\n        \"create type consentmechanismnew as enum ('opt_in', 'opt_out', 'notice_only');\"\n    )\n    op.execute(\n        (\n            \"alter table privacynoticehistory alter column consent_mechanism type consentmechanism using \"\n            \"consent_mechanism::text::consentmechanism\"\n        )\n    )\n    op.execute(\n        (\n            \"alter table privacynotice alter column consent_mechanism type consentmechanism using \"\n            \"consent_mechanism::text::consentmechanism\"\n        )\n    )\n    op.execute(\n        (\n            \"update privacynoticehistory set consent_mechanism = 'notice_only' where consent_mechanism = 'necessary';\"\n        )\n    )\n    op.execute(\n        (\n            \"update privacynotice set consent_mechanism = 'notice_only' where consent_mechanism = 'necessary';\"\n        )\n    )\n    op.execute(\n        (\n            \"alter table privacynoticehistory alter column consent_mechanism type consentmechanismnew using \"\n            \"consent_mechanism::text::consentmechanismnew\"\n        )\n    )\n    op.execute(\n        (\n            \"alter table privacynotice alter column consent_mechanism type consentmechanismnew using \"\n            \"consent_mechanism::text::consentmechanismnew\"\n        )\n    )\n    op.execute((\"drop type consentmechanismold;\"))\n    op.execute((\"drop type consentmechanism;\"))\n    op.execute((\"alter type consentmechanismnew rename to consentmechanism;\"))\n\n\ndef downgrade():\n    op.execute(\"alter type consentmechanism rename to consentmechanismold\")\n\n    op.execute(\n        \"create type consentmechanism as enum ('opt_in', 'opt_out', 'notice_only', 'necessary');\"\n    )\n    op.execute(\n        \"create type consentmechanismnew as enum ('opt_in', 'opt_out', 'necessary');\"\n    )\n    op.execute(\n        (\n            \"alter table privacynoticehistory alter column consent_mechanism type consentmechanism using \"\n            \"consent_mechanism::text::consentmechanism\"\n        )\n    )\n    op.execute(\n        (\n            \"alter table privacynotice alter column consent_mechanism type consentmechanism using \"\n            \"consent_mechanism::text::consentmechanism\"\n        )\n    )\n    op.execute(\n        (\n            \"update privacynoticehistory set consent_mechanism = 'necessary' where consent_mechanism = 'notice_only';\"\n        )\n    )\n    op.execute(\n        (\n            \"update privacynotice set consent_mechanism = 'necessary' where consent_mechanism = 'notice_only';\"\n        )\n    )\n    op.execute(\n        (\n            \"alter table privacynoticehistory alter column consent_mechanism type consentmechanismnew using \"\n            \"consent_mechanism::text::consentmechanismnew\"\n        )\n    )\n    op.execute(\n        (\n            \"alter table privacynotice alter column consent_mechanism type consentmechanismnew using \"\n            \"consent_mechanism::text::consentmechanismnew\"\n        )\n    )\n    op.execute((\"drop type consentmechanismold;\"))\n    op.execute((\"drop type consentmechanism;\"))\n    op.execute((\"alter type consentmechanismnew rename to consentmechanism;\"))\n"}
{"type": "source_file", "path": "src/fides/__init__.py", "content": "\"\"\"The root module for the Fides package.\"\"\"\n\nfrom ._version import get_versions\n\n__version__ = get_versions()[\"version\"]\ndel get_versions\n"}
{"type": "source_file", "path": "src/fides/api/alembic/migrations/versions/093bb28a8270_add_plus_system_history_table.py", "content": "\"\"\"Add plus_system_history table\n\nRevision ID: 093bb28a8270\nRevises: 3038667ba898\nCreate Date: 2023-08-18 23:48:22.934916\n\n\"\"\"\n\nimport sqlalchemy as sa\nfrom alembic import op\nfrom sqlalchemy.dialects import postgresql\n\n# revision identifiers, used by Alembic.\nrevision = \"093bb28a8270\"\ndown_revision = \"3038667ba898\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade():\n    op.create_table(\n        \"plus_system_history\",\n        sa.Column(\"id\", sa.String(length=255), nullable=False),\n        sa.Column(\n            \"created_at\",\n            sa.DateTime(timezone=True),\n            server_default=sa.text(\"now()\"),\n            nullable=True,\n        ),\n        sa.Column(\n            \"updated_at\",\n            sa.DateTime(timezone=True),\n            server_default=sa.text(\"now()\"),\n            nullable=True,\n        ),\n        sa.Column(\"user_id\", sa.String(), nullable=True),\n        sa.Column(\"system_id\", sa.String(), nullable=False),\n        sa.Column(\"before\", postgresql.JSONB(astext_type=sa.Text()), nullable=False),\n        sa.Column(\"after\", postgresql.JSONB(astext_type=sa.Text()), nullable=False),\n        sa.ForeignKeyConstraint([\"system_id\"], [\"ctl_systems.id\"], ondelete=\"CASCADE\"),\n        sa.PrimaryKeyConstraint(\"id\"),\n    )\n    op.create_index(\n        op.f(\"ix_plus_system_history_id\"), \"plus_system_history\", [\"id\"], unique=False\n    )\n    op.create_index(\n        \"idx_plus_system_history_created_at_system_id\",\n        \"plus_system_history\",\n        [\"created_at\", \"system_id\"],\n    )\n    op.add_column(\n        \"ctl_systems\",\n        sa.Column(\"user_id\", sa.String, nullable=True),\n    )\n\n\ndef downgrade():\n    op.drop_column(\"ctl_systems\", \"user_id\")\n    op.drop_index(\n        op.f(\"idx_plus_system_history_created_at_system_id\"),\n        table_name=\"plus_system_history\",\n    )\n    op.drop_index(op.f(\"ix_plus_system_history_id\"), table_name=\"plus_system_history\")\n    op.drop_table(\"plus_system_history\")\n"}
{"type": "source_file", "path": "src/fides/api/__init__.py", "content": ""}
{"type": "source_file", "path": "src/fides/_version.py", "content": "# This file helps to compute a version number in source trees obtained from\n# git-archive tarball (such as those provided by githubs download-from-tag\n# feature). Distribution tarballs (built by setup.py sdist) and build\n# directories (produced by setup.py build) will contain a much shorter file\n# that just contains the computed version number.\n\n# This file is released into the public domain. Generated by\n# versioneer-0.19 (https://github.com/python-versioneer/python-versioneer)\n\n# pylint: skip-file\n\n\"\"\"Git implementation of _version.py.\"\"\"\n\nimport errno\nimport os\nimport re\nimport subprocess\nimport sys\n\n\ndef get_keywords():\n    \"\"\"Get the keywords needed to look up the version information.\"\"\"\n    # these strings will be replaced by git during git-archive.\n    # setup.py/versioneer.py will grep for the variable names, so they must\n    # each be defined on a line of their own. _version.py will just call\n    # get_keywords().\n    git_refnames = \"$Format:%d$\"\n    git_full = \"$Format:%H$\"\n    git_date = \"$Format:%ci$\"\n    keywords = {\"refnames\": git_refnames, \"full\": git_full, \"date\": git_date}\n    return keywords\n\n\nclass VersioneerConfig:\n    \"\"\"Container for Versioneer configuration parameters.\"\"\"\n\n\ndef get_config():\n    \"\"\"Create, populate and return the VersioneerConfig() object.\"\"\"\n    # these strings are filled in when 'setup.py versioneer' creates\n    # _version.py\n    cfg = VersioneerConfig()\n    cfg.VCS = \"git\"\n    cfg.style = \"pep440-pre\"\n    cfg.tag_prefix = \"\"\n    cfg.parentdir_prefix = \"\"\n    cfg.versionfile_source = \"src/fides/_version.py\"\n    cfg.verbose = False\n    return cfg\n\n\nclass NotThisMethod(Exception):\n    \"\"\"Exception raised if a method is not valid for the current scenario.\"\"\"\n\n\nLONG_VERSION_PY = {}\nHANDLERS = {}\n\n\ndef register_vcs_handler(vcs, method):  # decorator\n    \"\"\"Create decorator to mark a method as the handler of a VCS.\"\"\"\n\n    def decorate(f):\n        \"\"\"Store f in HANDLERS[vcs][method].\"\"\"\n        if vcs not in HANDLERS:\n            HANDLERS[vcs] = {}\n        HANDLERS[vcs][method] = f\n        return f\n\n    return decorate\n\n\ndef run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    \"\"\"Call the given command(s).\"\"\"\n    assert isinstance(commands, list)\n    p = None\n    for c in commands:\n        try:\n            dispcmd = str([c] + args)\n            # remember shell=False, so use git.cmd on windows, not just git\n            p = subprocess.Popen(\n                [c] + args,\n                cwd=cwd,\n                env=env,\n                stdout=subprocess.PIPE,\n                stderr=(subprocess.PIPE if hide_stderr else None),\n            )\n            break\n        except EnvironmentError:\n            e = sys.exc_info()[1]\n            if e.errno == errno.ENOENT:\n                continue\n            if verbose:\n                print(\"unable to run %s\" % dispcmd)\n                print(e)\n            return None, None\n    else:\n        if verbose:\n            print(\n                \"unable to find command, tried %s\" % (commands),\n            )\n        return None, None\n    stdout = p.communicate()[0].strip().decode()\n    if p.returncode != 0:\n        if verbose:\n            print(\"unable to run %s (error)\" % dispcmd)\n            print(\"stdout was %s\" % stdout)\n        return None, p.returncode\n    return stdout, p.returncode\n\n\ndef versions_from_parentdir(parentdir_prefix, root, verbose):\n    \"\"\"Try to determine the version from the parent directory name.\n\n    Source tarballs conventionally unpack into a directory that includes both\n    the project name and a version string. We will also support searching up\n    two directory levels for an appropriately named parent directory\n    \"\"\"\n    rootdirs = []\n\n    for i in range(3):\n        dirname = os.path.basename(root)\n        if dirname.startswith(parentdir_prefix):\n            return {\n                \"version\": dirname[len(parentdir_prefix) :],\n                \"full-revisionid\": None,\n                \"dirty\": False,\n                \"error\": None,\n                \"date\": None,\n            }\n        else:\n            rootdirs.append(root)\n            root = os.path.dirname(root)  # up a level\n\n    if verbose:\n        print(\n            \"Tried directories %s but none started with prefix %s\"\n            % (str(rootdirs), parentdir_prefix)\n        )\n    raise NotThisMethod(\"rootdir doesn't start with parentdir_prefix\")\n\n\n@register_vcs_handler(\"git\", \"get_keywords\")\ndef git_get_keywords(versionfile_abs):\n    \"\"\"Extract version information from the given file.\"\"\"\n    # the code embedded in _version.py can just fetch the value of these\n    # keywords. When used from setup.py, we don't want to import _version.py,\n    # so we do it with a regexp instead. This function is not used from\n    # _version.py.\n    keywords = {}\n    try:\n        f = open(versionfile_abs, \"r\")\n        for line in f.readlines():\n            if line.strip().startswith(\"git_refnames =\"):\n                mo = re.search(r'=\\s*\"(.*)\"', line)\n                if mo:\n                    keywords[\"refnames\"] = mo.group(1)\n            if line.strip().startswith(\"git_full =\"):\n                mo = re.search(r'=\\s*\"(.*)\"', line)\n                if mo:\n                    keywords[\"full\"] = mo.group(1)\n            if line.strip().startswith(\"git_date =\"):\n                mo = re.search(r'=\\s*\"(.*)\"', line)\n                if mo:\n                    keywords[\"date\"] = mo.group(1)\n        f.close()\n    except EnvironmentError:\n        pass\n    return keywords\n\n\n@register_vcs_handler(\"git\", \"keywords\")\ndef git_versions_from_keywords(keywords, tag_prefix, verbose):\n    \"\"\"Get version information from git keywords.\"\"\"\n    if not keywords:\n        raise NotThisMethod(\"no keywords at all, weird\")\n    date = keywords.get(\"date\")\n    if date is not None:\n        # Use only the last line.  Previous lines may contain GPG signature\n        # information.\n        date = date.splitlines()[-1]\n\n        # git-2.2.0 added \"%cI\", which expands to an ISO-8601 -compliant\n        # datestamp. However we prefer \"%ci\" (which expands to an \"ISO-8601\n        # -like\" string, which we must then edit to make compliant), because\n        # it's been around since git-1.5.3, and it's too difficult to\n        # discover which version we're using, or to work around using an\n        # older one.\n        date = date.strip().replace(\" \", \"T\", 1).replace(\" \", \"\", 1)\n    refnames = keywords[\"refnames\"].strip()\n    if refnames.startswith(\"$Format\"):\n        if verbose:\n            print(\"keywords are unexpanded, not using\")\n        raise NotThisMethod(\"unexpanded keywords, not a git-archive tarball\")\n    refs = set([r.strip() for r in refnames.strip(\"()\").split(\",\")])\n    # starting in git-1.8.3, tags are listed as \"tag: foo-1.0\" instead of\n    # just \"foo-1.0\". If we see a \"tag: \" prefix, prefer those.\n    TAG = \"tag: \"\n    tags = set([r[len(TAG) :] for r in refs if r.startswith(TAG)])\n    if not tags:\n        # Either we're using git < 1.8.3, or there really are no tags. We use\n        # a heuristic: assume all version tags have a digit. The old git %d\n        # expansion behaves like git log --decorate=short and strips out the\n        # refs/heads/ and refs/tags/ prefixes that would let us distinguish\n        # between branches and tags. By ignoring refnames without digits, we\n        # filter out many common branch names like \"release\" and\n        # \"stabilization\", as well as \"HEAD\" and \"master\".\n        tags = set([r for r in refs if re.search(r\"\\d\", r)])\n        if verbose:\n            print(\"discarding '%s', no digits\" % \",\".join(refs - tags))\n    if verbose:\n        print(\"likely tags: %s\" % \",\".join(sorted(tags)))\n    for ref in sorted(tags):\n        # sorting will prefer e.g. \"2.0\" over \"2.0rc1\"\n        if ref.startswith(tag_prefix):\n            r = ref[len(tag_prefix) :]\n            if verbose:\n                print(\"picking %s\" % r)\n            return {\n                \"version\": r,\n                \"full-revisionid\": keywords[\"full\"].strip(),\n                \"dirty\": False,\n                \"error\": None,\n                \"date\": date,\n            }\n    # no suitable tags, so version is \"0+unknown\", but full hex is still there\n    if verbose:\n        print(\"no suitable tags, using unknown + full revision id\")\n    return {\n        \"version\": \"0+unknown\",\n        \"full-revisionid\": keywords[\"full\"].strip(),\n        \"dirty\": False,\n        \"error\": \"no suitable tags\",\n        \"date\": None,\n    }\n\n\n@register_vcs_handler(\"git\", \"pieces_from_vcs\")\ndef git_pieces_from_vcs(tag_prefix, root, verbose, run_command=run_command):\n    \"\"\"Get version from 'git describe' in the root of the source tree.\n\n    This only gets called if the git-archive 'subst' keywords were *not*\n    expanded, and _version.py hasn't already been rewritten with a short\n    version string, meaning we're inside a checked out source tree.\n    \"\"\"\n    GITS = [\"git\"]\n    if sys.platform == \"win32\":\n        GITS = [\"git.cmd\", \"git.exe\"]\n\n    out, rc = run_command(GITS, [\"rev-parse\", \"--git-dir\"], cwd=root, hide_stderr=True)\n    if rc != 0:\n        if verbose:\n            print(\"Directory %s not under git control\" % root)\n        raise NotThisMethod(\"'git rev-parse --git-dir' returned error\")\n\n    # if there is a tag matching tag_prefix, this yields TAG-NUM-gHEX[-dirty]\n    # if there isn't one, this yields HEX[-dirty] (no NUM)\n    describe_out, rc = run_command(\n        GITS,\n        [\n            \"describe\",\n            \"--tags\",\n            \"--dirty\",\n            \"--always\",\n            \"--long\",\n            \"--match\",\n            \"%s*\" % tag_prefix,\n        ],\n        cwd=root,\n    )\n    # --long was added in git-1.5.5\n    if describe_out is None:\n        raise NotThisMethod(\"'git describe' failed\")\n    describe_out = describe_out.strip()\n    full_out, rc = run_command(GITS, [\"rev-parse\", \"HEAD\"], cwd=root)\n    if full_out is None:\n        raise NotThisMethod(\"'git rev-parse' failed\")\n    full_out = full_out.strip()\n\n    pieces = {}\n    pieces[\"long\"] = full_out\n    pieces[\"short\"] = full_out[:7]  # maybe improved later\n    pieces[\"error\"] = None\n\n    # parse describe_out. It will be like TAG-NUM-gHEX[-dirty] or HEX[-dirty]\n    # TAG might have hyphens.\n    git_describe = describe_out\n\n    # look for -dirty suffix\n    dirty = git_describe.endswith(\"-dirty\")\n    pieces[\"dirty\"] = dirty\n    if dirty:\n        git_describe = git_describe[: git_describe.rindex(\"-dirty\")]\n\n    # now we have TAG-NUM-gHEX or HEX\n\n    if \"-\" in git_describe:\n        # TAG-NUM-gHEX\n        mo = re.search(r\"^(.+)-(\\d+)-g([0-9a-f]+)$\", git_describe)\n        if not mo:\n            # unparseable. Maybe git-describe is misbehaving?\n            pieces[\"error\"] = \"unable to parse git-describe output: '%s'\" % describe_out\n            return pieces\n\n        # tag\n        full_tag = mo.group(1)\n        if not full_tag.startswith(tag_prefix):\n            if verbose:\n                fmt = \"tag '%s' doesn't start with prefix '%s'\"\n                print(fmt % (full_tag, tag_prefix))\n            pieces[\"error\"] = \"tag '%s' doesn't start with prefix '%s'\" % (\n                full_tag,\n                tag_prefix,\n            )\n            return pieces\n        pieces[\"closest-tag\"] = full_tag[len(tag_prefix) :]\n\n        # distance: number of commits since tag\n        pieces[\"distance\"] = int(mo.group(2))\n\n        # commit: short hex revision ID\n        pieces[\"short\"] = mo.group(3)\n\n    else:\n        # HEX: no tags\n        pieces[\"closest-tag\"] = None\n        count_out, rc = run_command(GITS, [\"rev-list\", \"HEAD\", \"--count\"], cwd=root)\n        pieces[\"distance\"] = int(count_out)  # total number of commits\n\n    # commit date: see ISO-8601 comment in git_versions_from_keywords()\n    date = run_command(GITS, [\"show\", \"-s\", \"--format=%ci\", \"HEAD\"], cwd=root)[\n        0\n    ].strip()\n    # Use only the last line.  Previous lines may contain GPG signature\n    # information.\n    date = date.splitlines()[-1]\n    pieces[\"date\"] = date.strip().replace(\" \", \"T\", 1).replace(\" \", \"\", 1)\n\n    return pieces\n\n\ndef plus_or_dot(pieces):\n    \"\"\"Return a + if we don't already have one, else return a .\"\"\"\n    if \"+\" in pieces.get(\"closest-tag\", \"\"):\n        return \".\"\n    return \"+\"\n\n\ndef render_pep440(pieces):\n    \"\"\"Build up version string, with post-release \"local version identifier\".\n\n    Our goal: TAG[+DISTANCE.gHEX[.dirty]] . Note that if you\n    get a tagged build and then dirty it, you'll get TAG+0.gHEX.dirty\n\n    Exceptions:\n    1: no tags. git_describe was just HEX. 0+untagged.DISTANCE.gHEX[.dirty]\n    \"\"\"\n    if pieces[\"closest-tag\"]:\n        rendered = pieces[\"closest-tag\"]\n        if pieces[\"distance\"] or pieces[\"dirty\"]:\n            rendered += plus_or_dot(pieces)\n            rendered += \"%d.g%s\" % (pieces[\"distance\"], pieces[\"short\"])\n            if pieces[\"dirty\"]:\n                rendered += \".dirty\"\n    else:\n        # exception #1\n        rendered = \"0+untagged.%d.g%s\" % (pieces[\"distance\"], pieces[\"short\"])\n        if pieces[\"dirty\"]:\n            rendered += \".dirty\"\n    return rendered\n\n\ndef render_pep440_pre(pieces):\n    \"\"\"TAG[.post0.devDISTANCE] -- No -dirty.\n\n    Exceptions:\n    1: no tags. 0.post0.devDISTANCE\n    \"\"\"\n    if pieces[\"closest-tag\"]:\n        rendered = pieces[\"closest-tag\"]\n        if pieces[\"distance\"]:\n            rendered += \".post0.dev%d\" % pieces[\"distance\"]\n    else:\n        # exception #1\n        rendered = \"0.post0.dev%d\" % pieces[\"distance\"]\n    return rendered\n\n\ndef render_pep440_post(pieces):\n    \"\"\"TAG[.postDISTANCE[.dev0]+gHEX] .\n\n    The \".dev0\" means dirty. Note that .dev0 sorts backwards\n    (a dirty tree will appear \"older\" than the corresponding clean one),\n    but you shouldn't be releasing software with -dirty anyways.\n\n    Exceptions:\n    1: no tags. 0.postDISTANCE[.dev0]\n    \"\"\"\n    if pieces[\"closest-tag\"]:\n        rendered = pieces[\"closest-tag\"]\n        if pieces[\"distance\"] or pieces[\"dirty\"]:\n            rendered += \".post%d\" % pieces[\"distance\"]\n            if pieces[\"dirty\"]:\n                rendered += \".dev0\"\n            rendered += plus_or_dot(pieces)\n            rendered += \"g%s\" % pieces[\"short\"]\n    else:\n        # exception #1\n        rendered = \"0.post%d\" % pieces[\"distance\"]\n        if pieces[\"dirty\"]:\n            rendered += \".dev0\"\n        rendered += \"+g%s\" % pieces[\"short\"]\n    return rendered\n\n\ndef render_pep440_old(pieces):\n    \"\"\"TAG[.postDISTANCE[.dev0]] .\n\n    The \".dev0\" means dirty.\n\n    Exceptions:\n    1: no tags. 0.postDISTANCE[.dev0]\n    \"\"\"\n    if pieces[\"closest-tag\"]:\n        rendered = pieces[\"closest-tag\"]\n        if pieces[\"distance\"] or pieces[\"dirty\"]:\n            rendered += \".post%d\" % pieces[\"distance\"]\n            if pieces[\"dirty\"]:\n                rendered += \".dev0\"\n    else:\n        # exception #1\n        rendered = \"0.post%d\" % pieces[\"distance\"]\n        if pieces[\"dirty\"]:\n            rendered += \".dev0\"\n    return rendered\n\n\ndef render_git_describe(pieces):\n    \"\"\"TAG[-DISTANCE-gHEX][-dirty].\n\n    Like 'git describe --tags --dirty --always'.\n\n    Exceptions:\n    1: no tags. HEX[-dirty]  (note: no 'g' prefix)\n    \"\"\"\n    if pieces[\"closest-tag\"]:\n        rendered = pieces[\"closest-tag\"]\n        if pieces[\"distance\"]:\n            rendered += \"-%d-g%s\" % (pieces[\"distance\"], pieces[\"short\"])\n    else:\n        # exception #1\n        rendered = pieces[\"short\"]\n    if pieces[\"dirty\"]:\n        rendered += \"-dirty\"\n    return rendered\n\n\ndef render_git_describe_long(pieces):\n    \"\"\"TAG-DISTANCE-gHEX[-dirty].\n\n    Like 'git describe --tags --dirty --always -long'.\n    The distance/hash is unconditional.\n\n    Exceptions:\n    1: no tags. HEX[-dirty]  (note: no 'g' prefix)\n    \"\"\"\n    if pieces[\"closest-tag\"]:\n        rendered = pieces[\"closest-tag\"]\n        rendered += \"-%d-g%s\" % (pieces[\"distance\"], pieces[\"short\"])\n    else:\n        # exception #1\n        rendered = pieces[\"short\"]\n    if pieces[\"dirty\"]:\n        rendered += \"-dirty\"\n    return rendered\n\n\ndef render(pieces, style):\n    \"\"\"Render the given version pieces into the requested style.\"\"\"\n    if pieces[\"error\"]:\n        return {\n            \"version\": \"unknown\",\n            \"full-revisionid\": pieces.get(\"long\"),\n            \"dirty\": None,\n            \"error\": pieces[\"error\"],\n            \"date\": None,\n        }\n\n    if not style or style == \"default\":\n        style = \"pep440\"  # the default\n\n    if style == \"pep440\":\n        rendered = render_pep440(pieces)\n    elif style == \"pep440-pre\":\n        rendered = render_pep440_pre(pieces)\n    elif style == \"pep440-post\":\n        rendered = render_pep440_post(pieces)\n    elif style == \"pep440-old\":\n        rendered = render_pep440_old(pieces)\n    elif style == \"git-describe\":\n        rendered = render_git_describe(pieces)\n    elif style == \"git-describe-long\":\n        rendered = render_git_describe_long(pieces)\n    else:\n        raise ValueError(\"unknown style '%s'\" % style)\n\n    return {\n        \"version\": rendered,\n        \"full-revisionid\": pieces[\"long\"],\n        \"dirty\": pieces[\"dirty\"],\n        \"error\": None,\n        \"date\": pieces.get(\"date\"),\n    }\n\n\ndef get_versions():\n    \"\"\"Get version information or return default if unable to do so.\"\"\"\n    # I am in _version.py, which lives at ROOT/VERSIONFILE_SOURCE. If we have\n    # __file__, we can work backwards from there to the root. Some\n    # py2exe/bbfreeze/non-CPython implementations don't do __file__, in which\n    # case we can only use expanded keywords.\n\n    cfg = get_config()\n    verbose = cfg.verbose\n\n    try:\n        return git_versions_from_keywords(get_keywords(), cfg.tag_prefix, verbose)\n    except NotThisMethod:\n        pass\n\n    try:\n        root = os.path.realpath(__file__)\n        # versionfile_source is the relative path from the top of the source\n        # tree (where the .git directory might live) to this file. Invert\n        # this to find the root from __file__.\n        for i in cfg.versionfile_source.split(\"/\"):\n            root = os.path.dirname(root)\n    except NameError:\n        return {\n            \"version\": \"0+unknown\",\n            \"full-revisionid\": None,\n            \"dirty\": None,\n            \"error\": \"unable to find root of source tree\",\n            \"date\": None,\n        }\n\n    try:\n        pieces = git_pieces_from_vcs(cfg.tag_prefix, root, verbose)\n        return render(pieces, cfg.style)\n    except NotThisMethod:\n        pass\n\n    try:\n        if cfg.parentdir_prefix:\n            return versions_from_parentdir(cfg.parentdir_prefix, root, verbose)\n    except NotThisMethod:\n        pass\n\n    return {\n        \"version\": \"0+unknown\",\n        \"full-revisionid\": None,\n        \"dirty\": None,\n        \"error\": \"unable to compute version\",\n        \"date\": None,\n    }\n"}
{"type": "source_file", "path": "src/fides/api/alembic/__init__.py", "content": ""}
{"type": "source_file", "path": "src/fides/api/alembic/migrations/versions/10c6b7709be3_adding_foreign_key_on_fides_key_for_.py", "content": "\"\"\"adding foreign key on fides key for taxonomy\n\nRevision ID: 10c6b7709be3\nRevises: b63ecb007556\nCreate Date: 2024-12-17 14:54:02.325442\n\n\"\"\"\n\nimport sqlalchemy as sa\nfrom alembic import op\n\n# revision identifiers, used by Alembic.\nrevision = \"10c6b7709be3\"\ndown_revision = \"b63ecb007556\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade():\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.create_foreign_key(\n        \"ctl_data_categories_parent_key_fkey\",\n        \"ctl_data_categories\",\n        \"ctl_data_categories\",\n        [\"parent_key\"],\n        [\"fides_key\"],\n        ondelete=\"RESTRICT\",\n    )\n    op.create_foreign_key(\n        \"ctl_data_uses_parent_key_fkey\",\n        \"ctl_data_uses\",\n        \"ctl_data_uses\",\n        [\"parent_key\"],\n        [\"fides_key\"],\n        ondelete=\"RESTRICT\",\n    )\n    # ### end Alembic commands ###\n\n\ndef downgrade():\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_constraint(\n        \"ctl_data_categories_parent_key_fkey\", \"ctl_data_categories\", type_=\"foreignkey\"\n    )\n    op.drop_constraint(\n        \"ctl_data_uses_parent_key_fkey\", \"ctl_data_uses\", type_=\"foreignkey\"\n    )\n    # ### end Alembic commands ###\n"}
{"type": "source_file", "path": "src/fides/api/alembic/migrations/versions/1088e8353890_update_privacy_request_relationships.py", "content": "\"\"\"update privacy request relationships\n\nRevision ID: 1088e8353890\nRevises: d9237a0c0d5a\nCreate Date: 2024-12-26 22:38:37.905571\n\n\"\"\"\n\nfrom alembic import op\n\n# revision identifiers, used by Alembic.\nrevision = \"1088e8353890\"\ndown_revision = \"d9237a0c0d5a\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade():\n    op.drop_constraint(\n        \"custom_privacy_request_field_privacy_request_id_fkey\",\n        \"custom_privacy_request_field\",\n        type_=\"foreignkey\",\n    )\n    op.create_foreign_key(\n        \"custom_privacy_request_field_privacy_request_id_fkey\",\n        \"custom_privacy_request_field\",\n        \"privacyrequest\",\n        [\"privacy_request_id\"],\n        [\"id\"],\n        onupdate=\"CASCADE\",\n        ondelete=\"CASCADE\",\n    )\n    op.drop_constraint(\n        \"providedidentity_privacy_request_id_fkey\",\n        \"providedidentity\",\n        type_=\"foreignkey\",\n    )\n    op.create_foreign_key(\n        \"providedidentity_privacy_request_id_fkey\",\n        \"providedidentity\",\n        \"privacyrequest\",\n        [\"privacy_request_id\"],\n        [\"id\"],\n        onupdate=\"CASCADE\",\n        ondelete=\"CASCADE\",\n    )\n    op.drop_constraint(\n        \"privacyrequesterror_privacy_request_id_fkey\",\n        \"privacyrequesterror\",\n        type_=\"foreignkey\",\n    )\n    op.create_foreign_key(\n        \"privacyrequesterror_privacy_request_id_fkey\",\n        \"privacyrequesterror\",\n        \"privacyrequest\",\n        [\"privacy_request_id\"],\n        [\"id\"],\n        onupdate=\"CASCADE\",\n        ondelete=\"CASCADE\",\n    )\n\n\ndef downgrade():\n    op.drop_constraint(\n        \"providedidentity_privacy_request_id_fkey\",\n        \"providedidentity\",\n        type_=\"foreignkey\",\n    )\n    op.create_foreign_key(\n        \"providedidentity_privacy_request_id_fkey\",\n        \"providedidentity\",\n        \"privacyrequest\",\n        [\"privacy_request_id\"],\n        [\"id\"],\n    )\n    op.drop_constraint(\n        \"custom_privacy_request_field_privacy_request_id_fkey\",\n        \"custom_privacy_request_field\",\n        type_=\"foreignkey\",\n    )\n    op.create_foreign_key(\n        \"custom_privacy_request_field_privacy_request_id_fkey\",\n        \"custom_privacy_request_field\",\n        \"privacyrequest\",\n        [\"privacy_request_id\"],\n        [\"id\"],\n    )\n    op.drop_constraint(\n        \"privacyrequesterror_privacy_request_id_fkey\",\n        \"privacyrequesterror\",\n        type_=\"foreignkey\",\n    )\n    op.create_foreign_key(\n        \"privacyrequesterror_privacy_request_id_fkey\",\n        \"privacyrequesterror\",\n        \"privacyrequest\",\n        [\"privacy_request_id\"],\n        [\"id\"],\n    )\n"}
{"type": "source_file", "path": "setup.py", "content": "import pathlib\nfrom typing import List\n\nfrom setuptools import find_packages, setup\n\nimport versioneer\n\nhere = pathlib.Path(__file__).parent.resolve()\nlong_description = open(\"README.md\", encoding=\"utf-8\").read()\n\n##################\n## Requirements ##\n##################\n\ninstall_requires = open(\"requirements.txt\", encoding=\"utf-8\").read().strip().split(\"\\n\")\ndev_requires = open(\"dev-requirements.txt\", encoding=\"utf-8\").read().strip().split(\"\\n\")\noptional_requires = (\n    open(\"optional-requirements.txt\", encoding=\"utf-8\").read().strip().split(\"\\n\")\n)\n\n\ndef optional_requirements(\n    dependency_names: List[str], requires: List[str] = optional_requires\n) -> List[str]:\n    \"\"\"\n    Matches the provided dependency names to lines in `optional-requirements.txt`,\n    and returns the full dependency string for each one.\n    Prevents the need to store version numbers in two places.\n    \"\"\"\n\n    requirements: List[str] = []\n\n    for dependency in dependency_names:\n        for optional_dependency in requires:\n            if optional_dependency.startswith(dependency):\n                requirements.append(optional_dependency)\n                break\n\n    if len(requirements) == len(dependency_names):\n        return requirements\n\n    raise ModuleNotFoundError\n\n\n# Human-Readable Extras\n# Versions are read from corresponding lines in `optional-requirements.txt`\nextras = {\n    \"mssql\": optional_requirements([\"pymssql\"], optional_requires),\n}\n\nextras[\"all\"] = sum([value for value in extras.values()], [])\n\n###################\n## Package Setup ##\n###################\nsetup(\n    name=\"ethyca_fides\",\n    version=versioneer.get_version(),\n    cmdclass=versioneer.get_cmdclass(),\n    description=\"Open-source ecosystem for data privacy as code.\",\n    long_description=long_description,\n    long_description_content_type=\"text/markdown\",\n    url=\"https://github.com/ethyca/fides\",\n    entry_points={\"console_scripts\": [\"fides=fides.cli:cli\"]},\n    python_requires=\">=3.9, <4\",\n    package_dir={\"\": \"src\"},\n    packages=find_packages(where=\"src\"),\n    include_package_data=True,\n    author=\"Ethyca, Inc.\",\n    author_email=\"fidesteam@ethyca.com\",\n    license=\"Apache License 2.0\",\n    install_requires=install_requires,\n    dev_requires=dev_requires,\n    extras_require=extras,\n    classifiers=[\n        \"License :: OSI Approved :: Apache Software License\",\n        \"Programming Language :: Python :: 3 :: Only\",\n        \"Programming Language :: Python :: 3.9\",\n        \"Programming Language :: Python :: 3.10\",\n        \"Topic :: Software Development :: Libraries\",\n    ],\n)\n"}
{"type": "source_file", "path": "src/fides/api/alembic/migrations/versions/1152c1717849_fix_monitorexecution_started_default.py", "content": "\"\"\"fix monitorexecution.started default\n\nRevision ID: 1152c1717849\nRevises: 3c58001ad310\nCreate Date: 2025-03-10 16:04:57.104689\n\n\"\"\"\n\nimport sqlalchemy as sa\nfrom alembic import op\nfrom sqlalchemy.dialects import postgresql\n\n# revision identifiers, used by Alembic.\nrevision = \"1152c1717849\"\ndown_revision = \"3c58001ad310\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade():\n    # Set the new server default\n    # And set the datetime to be timezone aware\n    op.alter_column(\n        \"monitorexecution\",\n        \"started\",\n        existing_type=sa.DateTime(),\n        type_=sa.DateTime(timezone=True),\n        server_default=sa.text(\"now()\"),\n    )\n\n    # Set the datetime to be timezone aware\n    op.alter_column(\n        \"monitorexecution\",\n        \"completed\",\n        existing_type=sa.DateTime(),\n        type_=sa.DateTime(timezone=True),\n    )\n\n\ndef downgrade():\n    # Remove the server default\n    # And set the datetime to not be timezone aware\n    op.alter_column(\n        \"monitorexecution\",\n        \"started\",\n        server_default=None,\n        existing_type=sa.DateTime(timezone=True),\n        type_=sa.DateTime(),\n    )\n\n    # Set the datetime to not be timezone aware\n    op.alter_column(\n        \"monitorexecution\",\n        \"completed\",\n        existing_type=sa.DateTime(timezone=True),\n        type_=sa.DateTime(),\n    )\n"}
{"type": "source_file", "path": "src/fides/api/alembic/migrations/versions/021166731846_add_asset_table.py", "content": "\"\"\"add asset table\n\nRevision ID: 021166731846\nRevises: 58f8edd66b69\nCreate Date: 2025-01-22 22:14:35.548869\n\n\"\"\"\n\nimport sqlalchemy as sa\nfrom alembic import op\nfrom sqlalchemy.dialects import postgresql\n\n# revision identifiers, used by Alembic.\nrevision = \"021166731846\"\ndown_revision = \"58f8edd66b69\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade():\n\n    op.create_table(\n        \"asset\",\n        sa.Column(\"id\", sa.String(length=255), nullable=False),\n        sa.Column(\n            \"created_at\",\n            sa.DateTime(timezone=True),\n            server_default=sa.text(\"now()\"),\n            nullable=True,\n        ),\n        sa.Column(\n            \"updated_at\",\n            sa.DateTime(timezone=True),\n            server_default=sa.text(\"now()\"),\n            nullable=True,\n        ),\n        sa.Column(\"name\", sa.String(), nullable=False),\n        sa.Column(\"asset_type\", sa.String(), nullable=False),\n        sa.Column(\"domain\", sa.String(), nullable=True),\n        sa.Column(\n            \"parent\",\n            postgresql.ARRAY(sa.String()),\n            server_default=\"{}\",\n            nullable=False,\n        ),\n        sa.Column(\"parent_domain\", sa.String(), nullable=True),\n        sa.Column(\n            \"locations\",\n            postgresql.ARRAY(sa.String()),\n            server_default=\"{}\",\n            nullable=False,\n        ),\n        sa.Column(\"with_consent\", sa.BOOLEAN(), nullable=False),\n        sa.Column(\n            \"data_uses\",\n            postgresql.ARRAY(sa.String()),\n            server_default=\"{}\",\n            nullable=False,\n        ),\n        sa.Column(\n            \"meta\",\n            postgresql.JSONB(astext_type=sa.Text()),\n            server_default=\"{}\",\n            nullable=False,\n        ),\n        sa.Column(\"base_url\", sa.String(), nullable=True),\n        sa.Column(\"system_id\", sa.String(), nullable=True),\n        sa.ForeignKeyConstraint([\"system_id\"], [\"ctl_systems.id\"], ondelete=\"CASCADE\"),\n        sa.PrimaryKeyConstraint(\"id\"),\n    )\n    op.create_index(op.f(\"ix_asset_asset_type\"), \"asset\", [\"asset_type\"], unique=False)\n    op.create_index(op.f(\"ix_asset_domain\"), \"asset\", [\"domain\"], unique=False)\n    op.create_index(op.f(\"ix_asset_id\"), \"asset\", [\"id\"], unique=False)\n    op.create_index(op.f(\"ix_asset_name\"), \"asset\", [\"name\"], unique=False)\n    op.create_index(op.f(\"ix_asset_system_id\"), \"asset\", [\"system_id\"], unique=False)\n\n    op.create_index(\n        op.f(\"ix_asset_name_asset_type_domain_base_url_system_id\"),\n        \"asset\",\n        [\n            \"name\",\n            \"asset_type\",\n            \"domain\",\n            sa.text(\"coalesce(md5(base_url), 'NULL')\"),\n            \"system_id\",\n        ],\n        unique=True,\n    )\n\n    # ### end Alembic commands ###\n\n\ndef downgrade():\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_index(op.f(\"ix_asset_system_id\"), table_name=\"asset\")\n    op.drop_index(op.f(\"ix_asset_name\"), table_name=\"asset\")\n    op.drop_index(op.f(\"ix_asset_id\"), table_name=\"asset\")\n    op.drop_index(op.f(\"ix_asset_domain\"), table_name=\"asset\")\n    op.drop_index(op.f(\"ix_asset_asset_type\"), table_name=\"asset\")\n    op.drop_index(\n        op.f(\"ix_asset_name_asset_type_domain_base_url_system_id\"), table_name=\"asset\"\n    )\n    op.drop_table(\"asset\")\n    # ### end Alembic commands ###\n"}
{"type": "source_file", "path": "src/fides/api/alembic/migrations/versions/1739aa4a4ab7_add_security_policy_link_and_erasure_.py", "content": "\"\"\"add security policy link and erasure time limits\n\nRevision ID: 1739aa4a4ab7\nRevises: 4c3693c289d0\nCreate Date: 2022-01-28 20:41:20.065432\n\n\"\"\"\n\nimport sqlalchemy as sa\nfrom alembic import op\n\n# revision identifiers, used by Alembic.\nrevision = \"1739aa4a4ab7\"\ndown_revision = \"4c3693c289d0\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade():\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.add_column(\n        \"organizations\", sa.Column(\"security_policy\", sa.String(), nullable=True)\n    )\n    op.add_column(\"datasets\", sa.Column(\"retention\", sa.String(), nullable=True))\n    # ### end Alembic commands ###\n\n\ndef downgrade():\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_column(\"organizations\", \"security_policy\")\n    op.drop_column(\"datasets\", \"retention\")\n    # ### end Alembic commands ###\n"}
{"type": "source_file", "path": "src/fides/api/alembic/migrations/versions/192f23f4c968_add_fides_cloud_table.py", "content": "\"\"\"add fides cloud table\n\nRevision ID: 192f23f4c968\nRevises: 093bb28a8270\nCreate Date: 2023-09-01 17:28:22.099543\n\n\"\"\"\n\nimport sqlalchemy as sa\nfrom alembic import op\nfrom sqlalchemy.dialects import postgresql\n\n# revision identifiers, used by Alembic.\nrevision = \"192f23f4c968\"\ndown_revision = \"093bb28a8270\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade():\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.create_table(\n        \"fidescloud\",\n        sa.Column(\"id\", sa.String(length=255), nullable=False),\n        sa.Column(\n            \"created_at\",\n            sa.DateTime(timezone=True),\n            server_default=sa.text(\"now()\"),\n            nullable=True,\n        ),\n        sa.Column(\n            \"updated_at\",\n            sa.DateTime(timezone=True),\n            server_default=sa.text(\"now()\"),\n            nullable=True,\n        ),\n        sa.Column(\"config\", postgresql.JSONB(astext_type=sa.Text()), nullable=True),\n        sa.PrimaryKeyConstraint(\"id\"),\n    )\n    op.create_index(op.f(\"ix_fidescloud_id\"), \"fidescloud\", [\"id\"], unique=False)\n    # ### end Alembic commands ###\n\n\ndef downgrade():\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_index(op.f(\"ix_fidescloud_id\"), table_name=\"fidescloud\")\n    op.drop_table(\"fidescloud\")\n    # ### end Alembic commands ###\n"}
{"type": "source_file", "path": "src/fides/api/alembic/migrations/versions/1ea164cee8bc_fideslang_2_data_migrations.py", "content": "\"\"\"fideslang 2 data migrations\n\nRevision ID: 1ea164cee8bc\nRevises: 708a780b01ba\nCreate Date: 2023-09-06 04:09:06.212144\n\n\"\"\"\n\nfrom typing import Dict\n\nfrom alembic import op\nfrom loguru import logger\nfrom sqlalchemy import text\nfrom sqlalchemy.engine import Connection\n\nfrom fides.api.alembic.migrations.helpers.fideslang_migration_functions import (\n    update_consent,\n    update_ctl_policies,\n    update_data_label_tables,\n    update_datasets_data_categories,\n    update_privacy_declarations,\n    update_privacy_notices,\n    update_rule_targets,\n    update_system_ingress_egress_data_categories,\n)\n\n# revision identifiers, used by Alembic.\nrevision = \"1ea164cee8bc\"\ndown_revision = \"708a780b01ba\"\nbranch_labels = None\ndepends_on = None\n\n###############\n## Data Uses ##\n###############\n\"\"\"\nThe `key` is the old value, the `value` is the new value\nThese are ordered specifically so that string replacement works on both parent and child items\n\"\"\"\ndata_use_upgrades: Dict[str, str] = {\n    \"essential.service.operations.support.optimization\": \"essential.service.operations.improve\",\n    \"improve.system\": \"functional.service.improve\",\n    \"improve\": \"functional\",\n}\ndata_use_downgrades: Dict[str, str] = {\n    value: key for key, value in data_use_upgrades.items()\n}\n#####################\n## Data Categories ##\n#####################\n\"\"\"\nThe `key` is the old value, the `value` is the new value\nThese are ordered specifically so that string replacement works on both parent and child items\n\"\"\"\ndata_category_upgrades: Dict[str, str] = {\n    \"user.financial.account_number\": \"user.financial.bank_account\",\n    \"user.credentials\": \"user.authorization.credentials\",\n    \"user.browsing_history\": \"user.behavior.browsing_history\",\n    \"user.media_consumption\": \"user.behavior.media_consumption\",\n    \"user.search_history\": \"user.behavior.search_history\",\n    \"user.organization\": \"user.contact.organization\",\n    \"user.non_specific_age\": \"user.demographic.age_range\",\n    \"user.date_of_birth\": \"user.demographic.date_of_birth\",\n    \"user.gender\": \"user.demographic.gender\",\n    \"user.political_opinion\": \"user.demographic.political_opinion\",\n    \"user.profiling\": \"user.demographic.profile\",\n    \"user.race\": \"user.demographic.race_ethnicity\",\n    \"user.religious_belief\": \"user.demographic.religious_belief\",\n    \"user.sexual_orientation\": \"user.demographic.sexual_orientation\",\n    \"user.genetic\": \"user.health_and_medical.genetic\",\n    \"user.observed\": \"user.behavior\",\n}\ndata_category_downgrades: Dict[str, str] = {\n    value: key for key, value in data_category_upgrades.items()\n}\n\n\ndef upgrade() -> None:\n    \"\"\"\n    Given that our advice is to turn off auto-migrations and make a db copy,\n    there is no \"downgrade\" version of this. It also wouldn't be feasible given\n    it would require an older version of fideslang.\n    \"\"\"\n    bind: Connection = op.get_bind()\n\n    logger.info(\"Removing old default data categories and data uses\")\n    bind.execute(text(\"DELETE FROM ctl_data_uses WHERE is_default = TRUE;\"))\n    bind.execute(text(\"DELETE FROM ctl_data_categories WHERE is_default = TRUE;\"))\n\n    logger.info(\"Upgrading Privacy Declarations for Fideslang 2.0\")\n    update_privacy_declarations(bind, data_use_upgrades, data_category_upgrades)\n\n    logger.info(\"Upgrading Policy Rules for Fideslang 2.0\")\n    update_ctl_policies(bind, data_use_upgrades, data_category_upgrades)\n\n    logger.info(\"Upgrading Data Categories in Datasets\")\n    update_datasets_data_categories(bind, data_category_upgrades)\n\n    logger.info(\"Upgrading Data Categories in System egress/ingress\")\n    update_system_ingress_egress_data_categories(bind, data_category_upgrades)\n\n    logger.info(\"Updating Privacy Notices\")\n    update_privacy_notices(bind, data_use_upgrades)\n\n    logger.info(\"Updating Consent\")\n    update_consent(bind, data_use_upgrades)\n\n    logger.info(\"Updating Rule Targets\")\n    update_rule_targets(bind, data_category_upgrades)\n\n    logger.info(\"Upgrading Taxonomy Items for Fideslang 2.0\")\n    update_data_label_tables(bind, data_use_upgrades, \"ctl_data_uses\")\n    update_data_label_tables(bind, data_category_upgrades, \"ctl_data_categories\")\n\n\ndef downgrade() -> None:\n    \"\"\"\n    This migration does not support downgrades.\n    \"\"\"\n    logger.info(\n        \"Data migrations from Fideslang 2.0 to Fideslang 1.0 are not supported.\"\n    )\n"}
{"type": "source_file", "path": "src/fides/api/alembic/migrations/versions/150f234ef1de_add_fidesctl_meta_to_organization_object.py", "content": "\"\"\"add fidesctl_meta to organization object\n\nRevision ID: 150f234ef1de\nRevises: edcd28ede1f7\nCreate Date: 2022-03-07 20:14:00.530408\n\n\"\"\"\n\nimport sqlalchemy as sa\nfrom alembic import op\n\n# revision identifiers, used by Alembic.\nrevision = \"150f234ef1de\"\ndown_revision = \"edcd28ede1f7\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade():\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.add_column(\"organizations\", sa.Column(\"fidesctl_meta\", sa.JSON(), nullable=True))\n    # ### end Alembic commands ###\n\n\ndef downgrade():\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_column(\"organizations\", \"fidesctl_meta\")\n    # ### end Alembic commands ###\n"}
{"type": "source_file", "path": "src/fides/api/alembic/migrations/versions/1af6950f4625_update_flexible_legal_basis_default.py", "content": "\"\"\"update_flexible_legal_basis_default\n\nRevision ID: 1af6950f4625\nRevises: 3cc39a6ce32b\nCreate Date: 2023-11-15 22:15:38.688530\n\n\"\"\"\n\nimport sqlalchemy as sa\nfrom alembic import op\n\n# revision identifiers, used by Alembic.\nfrom sqlalchemy import text\nfrom sqlalchemy.engine import Connection, ResultProxy\n\nrevision = \"1af6950f4625\"\ndown_revision = \"3cc39a6ce32b\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade():\n    bind: Connection = op.get_bind()\n\n    bind.execute(\n        text(\n            \"\"\"\n            UPDATE privacydeclaration \n            SET flexible_legal_basis_for_processing = false \n            WHERE flexible_legal_basis_for_processing IS NULL;\n            \"\"\"\n        )\n    )\n\n    op.alter_column(\n        \"privacydeclaration\",\n        \"flexible_legal_basis_for_processing\",\n        existing_type=sa.BOOLEAN(),\n        server_default=\"t\",\n        nullable=False,\n    )\n\n\ndef downgrade():\n    op.alter_column(\n        \"privacydeclaration\",\n        \"flexible_legal_basis_for_processing\",\n        existing_type=sa.BOOLEAN(),\n        nullable=True,\n        server_default=None,\n    )\n"}
{"type": "source_file", "path": "src/fides/api/alembic/migrations/versions/07014ff34eb2_add_mariadb.py", "content": "\"\"\"add mariadb\n\nRevision ID: 07014ff34eb2\nRevises: f3841942d90c\nCreate Date: 2022-01-27 19:18:11.899734\n\n\"\"\"\n\nimport sqlalchemy as sa\nfrom alembic import op\n\n# revision identifiers, used by Alembic.\nrevision = \"07014ff34eb2\"\ndown_revision = \"f3841942d90c\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade():\n    op.execute(\"alter type connectiontype rename to connectiontype_old\")\n    op.execute(\n        \"create type connectiontype as enum('postgres', 'mongodb', 'mysql', 'https', 'snowflake', 'redshift', 'mssql', 'mariadb')\"\n    )\n    op.execute(\n        (\n            \"alter table connectionconfig alter column connection_type type connectiontype using \"\n            \"connection_type::text::connectiontype\"\n        )\n    )\n    op.execute(\"drop type connectiontype_old\")\n\n\ndef downgrade():\n    op.execute(\"delete from connectionconfig where connection_type in ('mariadb')\")\n    op.execute(\"alter type connectiontype rename to connectiontype_old\")\n    op.execute(\n        \"create type connectiontype as enum('postgres', 'mongodb', 'mysql', 'https', 'snowflake', 'redshift', 'mssql')\"\n    )\n    op.execute(\n        (\n            \"alter table connectionconfig alter column connection_type type connectiontype using \"\n            \"connection_type::text::connectiontype\"\n        )\n    )\n    op.execute(\"drop type connectiontype_old\")\n"}
{"type": "source_file", "path": "src/fides/api/alembic/migrations/versions/0c65325843bd_add_location_groups_column.py", "content": "\"\"\"add_location_groups_column\n\nRevision ID: 0c65325843bd\nRevises: a1e23b70f2b2\nCreate Date: 2024-02-21 19:56:39.228072\n\n\"\"\"\n\nimport sqlalchemy as sa\nfrom alembic import op\n\n# revision identifiers, used by Alembic.\nrevision = \"0c65325843bd\"\ndown_revision = \"a1e23b70f2b2\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade():\n    op.add_column(\n        \"location_regulation_selections\",\n        sa.Column(\n            \"selected_location_groups\",\n            sa.ARRAY(sa.String()),\n            server_default=\"{}\",\n            nullable=False,\n        ),\n    )\n\n\ndef downgrade():\n    op.drop_column(\"location_regulation_selections\", \"selected_location_groups\")\n"}
{"type": "source_file", "path": "src/fides/api/alembic/migrations/versions/1332815dcd71_purpose_header_to_translations.py", "content": "\"\"\"purpose_header_to_translations\n\nAdds new purpose_header field to Experience Translation table and\nPrivacy Experience Config History table\n\nRevision ID: 1332815dcd71\nRevises: 9cad5a5c438c\nCreate Date: 2024-08-28 21:05:41.933572\n\n\"\"\"\n\nimport sqlalchemy as sa\nfrom alembic import op\n\n# revision identifiers, used by Alembic.\nrevision = \"1332815dcd71\"\ndown_revision = \"9cad5a5c438c\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade():\n    op.add_column(\n        \"experiencetranslation\", sa.Column(\"purpose_header\", sa.String(), nullable=True)\n    )\n    op.add_column(\n        \"privacyexperienceconfighistory\",\n        sa.Column(\"purpose_header\", sa.String(), nullable=True),\n    )\n\n\ndef downgrade():\n    op.drop_column(\"privacyexperienceconfighistory\", \"purpose_header\")\n    op.drop_column(\"experiencetranslation\", \"purpose_header\")\n"}
{"type": "source_file", "path": "src/fides/api/alembic/migrations/versions/1dfc5a2d30e7_add_saas_config_to_connection_config.py", "content": "\"\"\"add saas config to connection config\n\nRevision ID: 1dfc5a2d30e7\nRevises: e55a51b354e3\nCreate Date: 2022-02-09 23:27:24.742938\n\n\"\"\"\n\nimport sqlalchemy as sa\nfrom alembic import op\nfrom sqlalchemy.dialects import postgresql\n\n# revision identifiers, used by Alembic.\nrevision = \"1dfc5a2d30e7\"\ndown_revision = \"e55a51b354e3\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade():\n    op.add_column(\n        \"connectionconfig\",\n        sa.Column(\n            \"saas_config\", postgresql.JSONB(astext_type=sa.Text()), nullable=True\n        ),\n    )\n\n    op.execute(\"alter type connectiontype rename to connectiontype_old\")\n    op.execute(\n        \"create type connectiontype as enum('postgres', 'mongodb', 'mysql', 'https', 'snowflake', 'redshift', 'mssql', 'mariadb', 'saas')\"\n    )\n    op.execute(\n        (\n            \"alter table connectionconfig alter column connection_type type connectiontype using \"\n            \"connection_type::text::connectiontype\"\n        )\n    )\n    op.execute(\"drop type connectiontype_old\")\n\n\ndef downgrade():\n    op.drop_column(\"connectionconfig\", \"saas_config\")\n    op.execute(\"delete from connectionconfig where connection_type in ('saas')\")\n    op.execute(\"alter type connectiontype rename to connectiontype_old\")\n    op.execute(\n        \"create type connectiontype as enum('postgres', 'mongodb', 'mysql', 'https', 'snowflake', 'redshift', 'mssql', 'mariadb')\"\n    )\n    op.execute(\n        (\n            \"alter table connectionconfig alter column connection_type type connectiontype using \"\n            \"connection_type::text::connectiontype\"\n        )\n    )\n    op.execute(\"drop type connectiontype_old\")\n"}
{"type": "source_file", "path": "src/fides/api/alembic/migrations/versions/0debabbb9c6a_add_property_config_and_css.py", "content": "\"\"\"add property config and css\n\nRevision ID: 0debabbb9c6a\nRevises: 4fba24045fec\nCreate Date: 2024-05-06 16:45:45.958888\n\n\"\"\"\n\nimport sqlalchemy as sa\nfrom alembic import op\nfrom sqlalchemy.dialects import postgresql\n\n# revision identifiers, used by Alembic.\nrevision = \"0debabbb9c6a\"\ndown_revision = \"4fba24045fec\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade():\n    op.add_column(\n        \"plus_property\",\n        sa.Column(\n            \"privacy_center_config\",\n            postgresql.JSONB(astext_type=sa.Text()),\n            nullable=True,\n        ),\n    )\n    op.add_column(\"plus_property\", sa.Column(\"stylesheet\", sa.Text(), nullable=True))\n    op.add_column(\n        \"plus_property\",\n        sa.Column(\"paths\", sa.ARRAY(sa.String()), server_default=\"{}\", nullable=False),\n    )\n\n\ndef downgrade():\n    op.drop_column(\"plus_property\", \"paths\")\n    op.drop_column(\"plus_property\", \"stylesheet\")\n    op.drop_column(\"plus_property\", \"privacy_center_config\")\n"}
{"type": "source_file", "path": "src/fides/api/alembic/migrations/versions/1f61c765cd1c_merge_alembic_heads.py", "content": "\"\"\"Merge alembic heads\n\nRevision ID: 1f61c765cd1c\nRevises: 8f84fad4e00b, b72541d79f10\nCreate Date: 2022-12-02 17:59:08.490577\n\n\"\"\"\n\nimport sqlalchemy as sa\nfrom alembic import op\n\n# revision identifiers, used by Alembic.\nrevision = \"1f61c765cd1c\"\ndown_revision = (\"8f84fad4e00b\", \"b72541d79f10\")\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade():\n    pass\n\n\ndef downgrade():\n    pass\n"}
{"type": "source_file", "path": "src/fides/api/alembic/migrations/versions/15a3e7483249_make_customfielddefinition_uniqueness_.py", "content": "\"\"\"make customfielddefinition uniqueness case insensitive\n\nRevision ID: 15a3e7483249\nRevises: e92da354691e\nCreate Date: 2023-05-02 15:03:56.256982\n\n\"\"\"\n\nimport sqlalchemy as sa\nfrom alembic import op\nfrom sqlalchemy.exc import IntegrityError\n\n# revision identifiers, used by Alembic.\nrevision = \"15a3e7483249\"\ndown_revision = \"e92da354691e\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade():\n    try:\n        op.execute(\n            \"\"\" CREATE UNIQUE INDEX ix_plus_custom_field_definition_unique_lowername_resourcetype\n                ON plus_custom_field_definition\n                (lower(name), resource_type)\n            \"\"\"\n        )\n    except IntegrityError as exc:\n        raise Exception(\n            f\"Fides attempted to create new custom field definition unique index but got error: {exc}. \"\n            f\"Adjust custom field names to avoid case-insensitive name overlaps for the same resource type.\"\n        )\n\n    # remove unnecessary index of unused field\n    op.drop_index(\n        \"ix_plus_custom_field_definition_field_definition\",\n        table_name=\"plus_custom_field_definition\",\n    )\n\n\ndef downgrade():\n    op.drop_index(\n        op.f(\"ix_plus_custom_field_definition_unique_lowername_resourcetype\"),\n        table_name=\"plus_custom_field_definition\",\n    )\n\n    # re-add unnecessray index of unused field in downgrade, for consistency\n    op.create_index(\n        op.f(\"ix_plus_custom_field_definition_field_definition\"),\n        \"plus_custom_field_definition\",\n        [\"field_definition\"],\n        unique=False,\n    )\n"}
{"type": "source_file", "path": "src/fides/api/alembic/migrations/versions/155fd8e51d9d_add_fideslib_models.py", "content": "\"\"\"Add fideslib models\n\nRevision ID: 155fd8e51d9d\nRevises: be432bd23596\nCreate Date: 2022-07-09 01:13:23.440193\n\n\"\"\"\n\nimport sqlalchemy as sa\nfrom alembic import op\n\n# revision identifiers, used by Alembic.\nrevision = \"155fd8e51d9d\"\ndown_revision = \"be432bd23596\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade():\n    # ### commands auto generated by Alembic - please adjust! ###\n\n    # data_categories\n    op.execute(\"DROP SEQUENCE data_categories_id_seq CASCADE\")\n    op.execute(\n        \"ALTER TABLE data_categories DROP CONSTRAINT data_categories_pkey CASCADE\"\n    )\n    op.alter_column(\n        table_name=\"data_categories\",\n        column_name=\"id\",\n        existing_type=sa.Integer,\n        type_=sa.String(255),\n        nullable=False,\n    )\n    op.create_primary_key(\n        table_name=\"data_categories\",\n        constraint_name=\"data_categories_pkey\",\n        columns=[\"id\"],\n    )\n\n    # data_qualifiers\n    op.execute(\"DROP SEQUENCE data_qualifiers_id_seq CASCADE\")\n    op.execute(\n        \"ALTER TABLE data_qualifiers DROP CONSTRAINT data_qualifiers_pkey CASCADE\"\n    )\n    op.alter_column(\n        table_name=\"data_qualifiers\",\n        column_name=\"id\",\n        existing_type=sa.Integer,\n        type_=sa.String(255),\n        nullable=False,\n    )\n    op.create_primary_key(\n        table_name=\"data_qualifiers\",\n        constraint_name=\"data_qualifiers_pkey\",\n        columns=[\"id\"],\n    )\n\n    # data_subjects\n    op.execute(\"DROP SEQUENCE data_subjects_id_seq CASCADE\")\n    op.execute(\"ALTER TABLE data_subjects DROP CONSTRAINT data_subjects_pkey CASCADE\")\n    op.alter_column(\n        table_name=\"data_subjects\",\n        column_name=\"id\",\n        existing_type=sa.Integer,\n        type_=sa.String(255),\n        nullable=False,\n    )\n    op.create_primary_key(\n        table_name=\"data_subjects\",\n        constraint_name=\"data_subjects_pkey\",\n        columns=[\"id\"],\n    )\n\n    # data_uses\n    op.execute(\"DROP SEQUENCE data_uses_id_seq CASCADE\")\n    op.execute(\"ALTER TABLE data_uses DROP CONSTRAINT data_uses_pkey CASCADE\")\n    op.alter_column(\n        table_name=\"data_uses\",\n        column_name=\"id\",\n        existing_type=sa.Integer,\n        type_=sa.String(255),\n        nullable=False,\n    )\n    op.create_primary_key(\n        table_name=\"data_uses\", constraint_name=\"data_uses_pkey\", columns=[\"id\"]\n    )\n\n    # datasets\n    op.execute(\"DROP SEQUENCE datasets_id_seq CASCADE\")\n    op.execute(\"ALTER TABLE datasets DROP CONSTRAINT datasets_pkey CASCADE\")\n    op.alter_column(\n        table_name=\"datasets\",\n        column_name=\"id\",\n        existing_type=sa.Integer,\n        type_=sa.String(255),\n        nullable=False,\n    )\n    op.create_primary_key(\n        table_name=\"datasets\",\n        constraint_name=\"data_sets_pkey\",\n        columns=[\"id\"],\n    )\n\n    # evaluations\n    op.add_column(\n        \"evaluations\",\n        sa.Column(\n            \"created_at\",\n            sa.DateTime(timezone=True),\n            server_default=sa.text(\"now()\"),\n            nullable=True,\n        ),\n    )\n    op.add_column(\n        \"evaluations\",\n        sa.Column(\n            \"updated_at\",\n            sa.DateTime(timezone=True),\n            server_default=sa.text(\"now()\"),\n            nullable=True,\n        ),\n    )\n    op.execute(\"DROP SEQUENCE evaluations_id_seq CASCADE\")\n    op.execute(\"ALTER TABLE evaluations DROP CONSTRAINT evaluations_pkey CASCADE\")\n    op.alter_column(\n        table_name=\"evaluations\",\n        column_name=\"id\",\n        existing_type=sa.Integer,\n        type_=sa.String(255),\n        nullable=False,\n    )\n    op.create_primary_key(\n        table_name=\"evaluations\",\n        constraint_name=\"evaluations_pkey\",\n        columns=[\"id\"],\n    )\n\n    # organizations\n    op.execute(\"DROP SEQUENCE organizations_id_seq CASCADE\")\n    op.execute(\"ALTER TABLE organizations DROP CONSTRAINT organizations_pkey CASCADE\")\n    op.alter_column(\n        table_name=\"organizations\",\n        column_name=\"id\",\n        existing_type=sa.Integer,\n        type_=sa.String(255),\n        nullable=False,\n    )\n    op.create_primary_key(\n        table_name=\"organizations\", constraint_name=\"organizations_pkey\", columns=[\"id\"]\n    )\n\n    # policies\n    op.execute(\"DROP SEQUENCE policies_id_seq CASCADE\")\n    op.execute(\"ALTER TABLE policies DROP CONSTRAINT policies_pkey CASCADE\")\n    op.alter_column(\n        table_name=\"policies\",\n        column_name=\"id\",\n        existing_type=sa.Integer,\n        type_=sa.String(255),\n        nullable=False,\n    )\n    op.create_primary_key(\n        table_name=\"policies\", constraint_name=\"policies_pkey\", columns=[\"id\"]\n    )\n\n    # registries\n    op.execute(\"DROP SEQUENCE registries_id_seq CASCADE\")\n    op.execute(\"ALTER TABLE registries DROP CONSTRAINT registries_pkey CASCADE\")\n    op.alter_column(\n        table_name=\"registries\",\n        column_name=\"id\",\n        existing_type=sa.Integer,\n        type_=sa.String(255),\n        nullable=False,\n    )\n    op.create_primary_key(\n        table_name=\"registries\", constraint_name=\"registries_pkey\", columns=[\"id\"]\n    )\n\n    # systems\n    op.execute(\"DROP SEQUENCE systems_id_seq CASCADE\")\n    op.execute(\"ALTER TABLE systems DROP CONSTRAINT systems_pkey CASCADE\")\n    op.alter_column(\n        table_name=\"systems\",\n        column_name=\"id\",\n        existing_type=sa.Integer,\n        type_=sa.String(255),\n        nullable=False,\n    )\n    op.create_primary_key(\n        table_name=\"systems\", constraint_name=\"systems_pkey\", columns=[\"id\"]\n    )\n    # ### end Alembic commands ###\n\n\ndef downgrade():\n    # ### commands auto generated by Alembic - please adjust! ###\n\n    # systems\n    op.execute(\"ALTER TABLE systems DROP CONSTRAINT systems_pkey CASCADE\")\n    op.execute(\"CREATE SEQUENCE systems_id_seq\")\n    op.alter_column(\n        table_name=\"systems\",\n        column_name=\"id\",\n        existing_type=sa.String(255),\n        type_=sa.Integer,\n        nullable=False,\n    )\n    op.create_primary_key(\n        table_name=\"systems\", constraint_name=\"systems_pkey\", columns=[\"id\"]\n    )\n\n    # registries\n    op.execute(\"ALTER TABLE registries DROP CONSTRAINT registries_pkey CASCADE\")\n    op.execute(\"CREATE SEQUENCE registries_id_seq\")\n    op.alter_column(\n        table_name=\"registries\",\n        column_name=\"id\",\n        existing_type=sa.String(255),\n        type_=sa.Integer,\n        nullable=False,\n    )\n    op.create_primary_key(\n        table_name=\"registries\", constraint_name=\"registries_pkey\", columns=[\"id\"]\n    )\n\n    # policies\n\n    op.execute(\"ALTER TABLE policies DROP CONSTRAINT policies_pkey CASCADE\")\n    op.execute(\"CREATE SEQUENCE policies_id_seq\")\n    op.alter_column(\n        table_name=\"policies\",\n        column_name=\"id\",\n        existing_type=sa.String(255),\n        type_=sa.Integer,\n        nullable=False,\n    )\n    op.create_primary_key(\n        table_name=\"policies\", constraint_name=\"policies_pkey\", columns=[\"id\"]\n    )\n\n    # organizations\n    op.execute(\"ALTER TABLE organizations DROP CONSTRAINT organizations_pkey CASCADE\")\n    op.execute(\"CREATE SEQUENCE policies_id_seq\")\n    op.alter_column(\n        table_name=\"organizations\",\n        column_name=\"id\",\n        existing_type=sa.String(255),\n        type_=sa.Integer,\n        nullable=False,\n    )\n    op.create_primary_key(\n        table_name=\"organizations\", constraint_name=\"organizations_pkey\", columns=[\"id\"]\n    )\n\n    # evaluations\n    op.execute(\"ALTER TABLE evaluations DROP CONSTRAINT evaluations_pkey CASCADE\")\n    op.execute(\"CREATE SEQUENCE policies_id_seq\")\n    op.alter_column(\n        table_name=\"evaluations\",\n        column_name=\"id\",\n        existing_type=sa.String(255),\n        type_=sa.Integer,\n        nullable=False,\n    )\n    op.create_primary_key(\n        table_name=\"evaluations\", constraint_name=\"evaluations_pkey\", columns=[\"id\"]\n    )\n    op.drop_column(\"evaluations\", \"updated_at\")\n    op.drop_column(\"evaluations\", \"created_at\")\n\n    # datasets\n    op.execute(\"ALTER TABLE datasets DROP CONSTRAINT datasets_pkey CASCADE\")\n    op.execute(\"CREATE SEQUENCE datasets_id_seq\")\n    op.alter_column(\n        table_name=\"datasets\",\n        column_name=\"id\",\n        existing_type=sa.String(255),\n        type_=sa.Integer,\n        nullable=False,\n    )\n    op.create_primary_key(\n        table_name=\"datasets\", constraint_name=\"datasets_pkey\", columns=[\"id\"]\n    )\n\n    # data_uses\n    op.execute(\"ALTER TABLE data_uses DROP CONSTRAINT data_uses_pkey CASCADE\")\n    op.execute(\"CREATE SEQUENCE data_uses_id_seq\")\n    op.alter_column(\n        table_name=\"data_uses\",\n        column_name=\"id\",\n        existing_type=sa.String(255),\n        type_=sa.Integer,\n        nullable=False,\n    )\n    op.create_primary_key(\n        table_name=\"data_uses\", constraint_name=\"data_uses_pkey\", columns=[\"id\"]\n    )\n\n    # data_subjects\n    op.execute(\"ALTER TABLE data_subjects DROP CONSTRAINT data_subjects_pkey CASCADE\")\n    op.execute(\"CREATE SEQUENCE data_uses_id_seq\")\n    op.alter_column(\n        table_name=\"data_subjects\",\n        column_name=\"id\",\n        existing_type=sa.String(255),\n        type_=sa.Integer,\n        nullable=False,\n    )\n    op.create_primary_key(\n        table_name=\"data_subjects\", constraint_name=\"data_subjects_pkey\", columns=[\"id\"]\n    )\n\n    # data_qualifiers\n    op.execute(\n        \"ALTER TABLE data_qualifiers DROP CONSTRAINT data_qualifiers_pkey CASCADE\"\n    )\n    op.execute(\"CREATE SEQUENCE data_uses_id_seq\")\n    op.alter_column(\n        table_name=\"data_qualifiers\",\n        column_name=\"id\",\n        existing_type=sa.String(255),\n        type_=sa.Integer,\n        nullable=False,\n    )\n    op.create_primary_key(\n        table_name=\"data_qualifiers\",\n        constraint_name=\"data_qualifiers_pkey\",\n        columns=[\"id\"],\n    )\n\n    # data_categories\n    op.execute(\n        \"ALTER TABLE data_categories DROP CONSTRAINT data_categories_pkey CASCADE\"\n    )\n    op.execute(\"CREATE SEQUENCE data_categories_id_seq\")\n    op.alter_column(\n        table_name=\"data_categories\",\n        column_name=\"id\",\n        existing_type=sa.String(255),\n        type_=sa.Integer,\n        nullable=False,\n    )\n    op.create_primary_key(\n        table_name=\"data_categories\",\n        constraint_name=\"data_categories_pkey\",\n        columns=[\"id\"],\n    )\n\n    op.drop_index(op.f(\"ix_fidesuserpermissions_id\"), table_name=\"fidesuserpermissions\")\n    op.drop_table(\"fidesuserpermissions\")\n    op.drop_index(op.f(\"ix_client_id\"), table_name=\"client\")\n    op.drop_index(op.f(\"ix_client_fides_key\"), table_name=\"client\")\n    op.drop_table(\"client\")\n    op.drop_index(op.f(\"ix_fidesuser_username\"), table_name=\"fidesuser\")\n    op.drop_index(op.f(\"ix_fidesuser_id\"), table_name=\"fidesuser\")\n    op.drop_table(\"fidesuser\")\n    op.drop_index(op.f(\"ix_auditlog_user_id\"), table_name=\"auditlog\")\n    op.drop_index(op.f(\"ix_auditlog_privacy_request_id\"), table_name=\"auditlog\")\n    op.drop_index(op.f(\"ix_auditlog_id\"), table_name=\"auditlog\")\n    op.drop_index(op.f(\"ix_auditlog_action\"), table_name=\"auditlog\")\n    op.drop_table(\"auditlog\")\n    # ### end Alembic commands ###\n"}
{"type": "source_file", "path": "src/fides/api/alembic/migrations/versions/179f2bb623ae_update_table_for_twilio.py", "content": "\"\"\"Update table for twilio\n\nRevision ID: 179f2bb623ae\nRevises: 8f1a19465239\nCreate Date: 2022-10-31 18:19:26.845723\n\n\"\"\"\n\nimport sqlalchemy as sa\nimport sqlalchemy_utils\nfrom alembic import op\nfrom sqlalchemy.dialects import postgresql\n\n# revision identifiers, used by Alembic.\nrevision = \"179f2bb623ae\"\ndown_revision = \"8f1a19465239\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade():\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.create_table(\n        \"messagingconfig\",\n        sa.Column(\"id\", sa.String(length=255), nullable=False),\n        sa.Column(\n            \"created_at\",\n            sa.DateTime(timezone=True),\n            server_default=sa.text(\"now()\"),\n            nullable=True,\n        ),\n        sa.Column(\n            \"updated_at\",\n            sa.DateTime(timezone=True),\n            server_default=sa.text(\"now()\"),\n            nullable=True,\n        ),\n        sa.Column(\"key\", sa.String(), nullable=False),\n        sa.Column(\"name\", sa.String(), nullable=True),\n        sa.Column(\n            \"service_type\",\n            sa.Enum(\n                \"MAILGUN\", \"TWILIO_TEXT\", \"TWILIO_EMAIL\", name=\"messagingservicetype\"\n            ),\n            nullable=False,\n        ),\n        sa.Column(\"details\", postgresql.JSONB(astext_type=sa.Text()), nullable=True),\n        sa.Column(\n            \"secrets\",\n            sqlalchemy_utils.types.encrypted.encrypted_type.StringEncryptedType(),\n            nullable=True,\n        ),\n        sa.PrimaryKeyConstraint(\"id\"),\n    )\n    op.create_index(\n        op.f(\"ix_messagingconfig_id\"), \"messagingconfig\", [\"id\"], unique=False\n    )\n    op.create_index(\n        op.f(\"ix_messagingconfig_key\"), \"messagingconfig\", [\"key\"], unique=True\n    )\n    op.create_index(\n        op.f(\"ix_messagingconfig_name\"), \"messagingconfig\", [\"name\"], unique=True\n    )\n    op.create_index(\n        op.f(\"ix_messagingconfig_service_type\"),\n        \"messagingconfig\",\n        [\"service_type\"],\n        unique=True,\n    )\n    # ### end Alembic commands ###\n\n\ndef downgrade():\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.create_table(\n        \"emailconfig\",\n        sa.Column(\"id\", sa.VARCHAR(length=255), autoincrement=False, nullable=False),\n        sa.Column(\n            \"created_at\",\n            postgresql.TIMESTAMP(timezone=True),\n            server_default=sa.text(\"now()\"),\n            autoincrement=False,\n            nullable=True,\n        ),\n        sa.Column(\n            \"updated_at\",\n            postgresql.TIMESTAMP(timezone=True),\n            server_default=sa.text(\"now()\"),\n            autoincrement=False,\n            nullable=True,\n        ),\n        sa.Column(\"key\", sa.VARCHAR(), autoincrement=False, nullable=False),\n        sa.Column(\"name\", sa.VARCHAR(), autoincrement=False, nullable=True),\n        sa.Column(\n            \"service_type\",\n            postgresql.ENUM(\"MAILGUN\", name=\"emailservicetype\"),\n            autoincrement=False,\n            nullable=False,\n        ),\n        sa.Column(\n            \"details\",\n            postgresql.JSONB(astext_type=sa.Text()),\n            autoincrement=False,\n            nullable=False,\n        ),\n        sa.Column(\"secrets\", sa.VARCHAR(), autoincrement=False, nullable=True),\n        sa.PrimaryKeyConstraint(\"id\", name=\"emailconfig_pkey\"),\n    )\n    op.create_index(\n        \"ix_emailconfig_service_type\", \"emailconfig\", [\"service_type\"], unique=False\n    )\n    op.create_index(\"ix_emailconfig_name\", \"emailconfig\", [\"name\"], unique=False)\n    op.create_index(\"ix_emailconfig_key\", \"emailconfig\", [\"key\"], unique=False)\n    op.create_index(\"ix_emailconfig_id\", \"emailconfig\", [\"id\"], unique=False)\n    op.drop_index(op.f(\"ix_messagingconfig_service_type\"), table_name=\"messagingconfig\")\n    op.drop_index(op.f(\"ix_messagingconfig_name\"), table_name=\"messagingconfig\")\n    op.drop_index(op.f(\"ix_messagingconfig_key\"), table_name=\"messagingconfig\")\n    op.drop_index(op.f(\"ix_messagingconfig_id\"), table_name=\"messagingconfig\")\n    op.drop_table(\"messagingconfig\")\n    # ### end Alembic commands ###\n"}
{"type": "source_file", "path": "src/fides/api/alembic/migrations/versions/1c8dfd6a1bc6_add_azure_as_a_sso_provider.py", "content": "\"\"\"Add azure as a SSO provider\n\nRevision ID: 1c8dfd6a1bc6\nRevises: 4ebe0766021b\nCreate Date: 2024-10-21 18:04:29.130138\n\n\"\"\"\n\nfrom alembic import op\n\n# revision identifiers, used by Alembic.\nrevision = \"1c8dfd6a1bc6\"\ndown_revision = \"4ebe0766021b\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade():\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.execute(\"alter type providerenum add value 'azure'\")\n    # ### end Alembic commands ###\n\n\ndef downgrade():\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.execute(\"delete from openid_provider where provider in ('azure')\")\n    op.execute(\"alter type providerenum rename to providerenum_old\")\n    op.execute(\"create type providerenum as enum('google','okta','custom')\")\n    op.execute(\n        (\n            \"alter table openid_provider alter column provider type providerenum using \"\n            \"openid_provider::text::providerenum;\"\n        )\n    )\n    op.execute(\"drop type providerenum_old\")\n    # ### end Alembic commands ###\n"}
{"type": "source_file", "path": "src/fides/api/alembic/migrations/versions/1ff88b7bd579_add_authorization_request.py", "content": "\"\"\"add authorization request\n\nRevision ID: 1ff88b7bd579\nRevises: 3a7c5fb119c9\nCreate Date: 2022-05-25 04:09:22.149110\n\n\"\"\"\n\nimport sqlalchemy as sa\nfrom alembic import op\n\n# revision identifiers, used by Alembic.\nrevision = \"1ff88b7bd579\"\ndown_revision = \"3a7c5fb119c9\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade():\n    op.create_table(\n        \"authenticationrequest\",\n        sa.Column(\"id\", sa.String(length=255), nullable=False),\n        sa.Column(\n            \"created_at\",\n            sa.DateTime(timezone=True),\n            server_default=sa.text(\"now()\"),\n            nullable=True,\n        ),\n        sa.Column(\n            \"updated_at\",\n            sa.DateTime(timezone=True),\n            server_default=sa.text(\"now()\"),\n            nullable=True,\n        ),\n        sa.Column(\"connection_key\", sa.String(), nullable=False),\n        sa.Column(\"state\", sa.String(), nullable=False),\n        sa.PrimaryKeyConstraint(\"id\"),\n        sa.UniqueConstraint(\"connection_key\"),\n    )\n    op.create_index(\n        op.f(\"ix_authenticationrequest_id\"),\n        \"authenticationrequest\",\n        [\"id\"],\n        unique=False,\n    )\n    op.create_index(\n        op.f(\"ix_authenticationrequest_state\"),\n        \"authenticationrequest\",\n        [\"state\"],\n        unique=True,\n    )\n\n\ndef downgrade():\n    op.drop_index(\n        op.f(\"ix_authenticationrequest_state\"), table_name=\"authenticationrequest\"\n    )\n    op.drop_index(\n        op.f(\"ix_authenticationrequest_id\"), table_name=\"authenticationrequest\"\n    )\n    op.drop_table(\"authenticationrequest\")\n"}
{"type": "source_file", "path": "src/fides/api/alembic/migrations/helpers/fideslang_migration_functions.py", "content": "import json\nfrom typing import Dict, List, Optional\n\nfrom loguru import logger\nfrom sqlalchemy import text\nfrom sqlalchemy.engine import Connection, ResultProxy\nfrom sqlalchemy.sql.elements import TextClause\n\nfrom fides.api.alembic.migrations.helpers.database_functions import generate_record_id\nfrom fides.api.db.seed import DEFAULT_ACCESS_POLICY_RULE, DEFAULT_ERASURE_POLICY_RULE\nfrom fides.api.schemas.policy import ActionType\nfrom fides.api.util.text import to_snake_case\n\n\ndef _replace_matching_data_label(\n    data_label: str, data_label_map: Dict[str, str]\n) -> str:\n    \"\"\"\n    Helper function to do string replacement for updated fides_keys.\n    \"\"\"\n    for old, new in data_label_map.items():\n        if data_label and data_label.startswith(old):\n            return data_label.replace(old, new)\n\n    return data_label\n\n\ndef update_privacy_declarations(\n    bind: Connection, data_use_map: Dict[str, str], data_category_map: Dict[str, str]\n) -> None:\n    \"\"\"\n    Upgrade or downgrade Privacy Declarations for Fideslang 2.0\n\n    This updates:\n    - data uses\n    - data categories\n    - shared categories\n    \"\"\"\n    existing_privacy_declarations: ResultProxy = bind.execute(\n        text(\n            \"SELECT id, data_use, data_categories, shared_categories FROM privacydeclaration;\"\n        )\n    )\n    for row in existing_privacy_declarations:\n        data_use: str = _replace_matching_data_label(row[\"data_use\"], data_use_map)\n        data_categories: List[str] = [\n            _replace_matching_data_label(data_category, data_category_map)\n            for data_category in row[\"data_categories\"]\n        ]\n        shared_categories: List[str] = [\n            _replace_matching_data_label(data_category, data_category_map)\n            for data_category in row[\"shared_categories\"]\n        ]\n\n        update_query: TextClause = text(\n            \"UPDATE privacydeclaration SET data_use = :updated_use, data_categories = :updated_categories, shared_categories = :updated_shared WHERE id= :declaration_id\"\n        )\n        bind.execute(\n            update_query,\n            {\n                \"declaration_id\": row[\"id\"],\n                \"updated_use\": data_use,\n                \"updated_categories\": data_categories,\n                \"updated_shared\": shared_categories,\n            },\n        )\n\n\ndef update_ctl_policies(\n    bind: Connection, data_use_map: Dict[str, str], data_category_map: Dict[str, str]\n) -> None:\n    \"\"\"\n    Upgrade or downgrade Policy Rules for Fideslang 2.0\n\n    This updates:\n    - data uses\n    - data categories\n    \"\"\"\n    existing_ctl_policies: ResultProxy = bind.execute(\n        text(\"SELECT id, rules FROM ctl_policies;\")\n    )\n\n    for row in existing_ctl_policies:\n        rules: List[Dict] = row[\"rules\"]\n\n        for i, rule in enumerate(rules or []):\n            data_uses: List = rule.get(\"data_uses\", {}).get(\"values\", [])\n            rules[i][\"data_uses\"][\"values\"] = [\n                _replace_matching_data_label(use, data_use_map) for use in data_uses\n            ]\n\n            data_categories: List = rule.get(\"data_categories\", {}).get(\"values\", [])\n            rules[i][\"data_categories\"][\"values\"] = [\n                _replace_matching_data_label(category, data_category_map)\n                for category in data_categories\n            ]\n\n        update_data_use_query: TextClause = text(\n            \"UPDATE ctl_policies SET rules = :updated_rules WHERE id= :policy_id\"\n        )\n        bind.execute(\n            update_data_use_query,\n            {\"policy_id\": row[\"id\"], \"updated_rules\": json.dumps(rules)},\n        )\n\n\ndef update_data_label_tables(\n    bind: Connection, update_map: Dict[str, str], table_name: str\n) -> None:\n    \"\"\"\n    Upgrade or downgrade Data Labels for Fideslang 2.0\n    \"\"\"\n    existing_labels: ResultProxy = bind.execute(\n        text(f\"SELECT fides_key, parent_key FROM {table_name};\")\n    )\n    for row in existing_labels:\n        old_key = row[\"fides_key\"]\n        new_key = _replace_matching_data_label(old_key, update_map)\n\n        old_parent = row[\"parent_key\"]\n        new_parent = _replace_matching_data_label(old_parent, update_map)\n\n        update_query: TextClause = text(\n            f\"UPDATE {table_name} SET fides_key = :updated_key, parent_key = :updated_parent WHERE fides_key = :old_key\"\n        )\n        bind.execute(\n            update_query,\n            {\n                \"updated_key\": new_key,\n                \"old_key\": old_key,\n                \"updated_parent\": new_parent,\n            },\n        )\n\n\ndef update_rule_targets(bind: Connection, data_label_map: Dict[str, str]) -> None:\n    \"\"\"Upgrade ruletargets to use the new data categories.\"\"\"\n\n    existing_rule_targets: ResultProxy = bind.execute(\n        text(\"SELECT id, data_category FROM ruletarget;\")\n    )\n\n    for row in existing_rule_targets:\n        data_category = row[\"data_category\"]\n\n        if not data_category:\n            continue\n\n        updated_category: str = _replace_matching_data_label(\n            data_category, data_label_map\n        )\n\n        update_data_category_query: TextClause = text(\n            \"UPDATE ruletarget SET data_category = :updated_category WHERE id= :target_id\"\n        )\n        bind.execute(\n            update_data_category_query,\n            {\"target_id\": row[\"id\"], \"updated_category\": updated_category},\n        )\n\n\ndef update_datasets_data_categories(\n    bind: Connection, data_label_map: Dict[str, str]\n) -> None:\n    \"\"\"Upgrade the datasets and their collections/fields in the database to use the new data categories.\"\"\"\n\n    # Get all datasets out of the database\n    existing_datasets: ResultProxy = bind.execute(\n        text(\"SELECT id, data_categories, collections FROM ctl_datasets;\")\n    )\n\n    for row in existing_datasets:\n        # Update data categories at the top level\n        dataset_data_categories: Optional[List[str]] = row[\"data_categories\"]\n\n        if dataset_data_categories:\n            updated_categories: List[str] = [\n                _replace_matching_data_label(category, data_label_map)\n                for category in dataset_data_categories\n            ]\n\n            update_label_query: TextClause = text(\n                \"UPDATE ctl_datasets SET data_categories = :updated_labels WHERE id= :dataset_id\"\n            )\n            bind.execute(\n                update_label_query,\n                {\"dataset_id\": row[\"id\"], \"updated_labels\": updated_categories},\n            )\n\n        # Update the collections objects\n        collections: str = json.dumps(row[\"collections\"])\n        if collections:\n            for key, value in data_label_map.items():\n                collections = collections.replace(key, value)\n\n            update_collections_query: TextClause = text(\n                \"UPDATE ctl_datasets SET collections = :updated_collections WHERE id= :dataset_id\"\n            )\n            bind.execute(\n                update_collections_query,\n                {\"dataset_id\": row[\"id\"], \"updated_collections\": collections},\n            )\n\n\ndef update_system_ingress_egress_data_categories(\n    bind: Connection, data_label_map: Dict[str, str]\n) -> None:\n    \"\"\"Upgrade or downgrade data categories on system DataFlow objects (egress/ingress)\"\"\"\n    existing_systems: ResultProxy = bind.execute(\n        text(\"SELECT id, egress, ingress FROM ctl_systems;\")\n    )\n\n    for row in existing_systems:\n        ingress = row[\"ingress\"]\n        egress = row[\"egress\"]\n\n        # Do a blunt find/replace\n        if ingress:\n            for item in ingress:\n                if item[\"data_categories\"]:\n                    item[\"data_categories\"] = [\n                        _replace_matching_data_label(category, data_label_map)\n                        for category in item[\"data_categories\"]\n                    ]\n\n            update_ingress_query: TextClause = text(\n                \"UPDATE ctl_systems SET ingress = :updated_ingress WHERE id= :system_id\"\n            )\n            bind.execute(\n                update_ingress_query,\n                {\"system_id\": row[\"id\"], \"updated_ingress\": json.dumps(ingress)},\n            )\n\n        if egress:\n            for item in egress:\n                if item[\"data_categories\"]:\n                    item[\"data_categories\"] = [\n                        _replace_matching_data_label(category, data_label_map)\n                        for category in item[\"data_categories\"]\n                    ]\n\n            update_egress_query: TextClause = text(\n                \"UPDATE ctl_systems SET egress = :updated_egress WHERE id= :system_id\"\n            )\n            bind.execute(\n                update_egress_query,\n                {\"system_id\": row[\"id\"], \"updated_egress\": json.dumps(egress)},\n            )\n\n\ndef update_privacy_notices(bind: Connection, data_use_map: Dict[str, str]) -> None:\n    \"\"\"\n    Update the Privacy Notice Models.\n\n    This includes the following models:\n    - PrivacyNotice\n    - PrivacyNoticeHistory\n    - PrivacyNoticeTemplate\n    \"\"\"\n    privacy_notice_tables = [\n        \"privacynotice\",\n        \"privacynoticetemplate\",\n        \"privacynoticehistory\",\n    ]\n    for table in privacy_notice_tables:\n        existing_notices: ResultProxy = bind.execute(\n            text(f\"SELECT id, data_uses FROM {table};\")\n        )\n\n        for row in existing_notices:\n            data_uses = row[\"data_uses\"]\n\n            # Do a blunt find/replace\n            updated_data_uses = [\n                _replace_matching_data_label(use, data_use_map) for use in data_uses\n            ]\n\n            update_query: TextClause = text(\n                f\"UPDATE {table} SET data_uses= :updated_uses WHERE id= :notice_id\"\n            )\n            bind.execute(\n                update_query,\n                {\"notice_id\": row[\"id\"], \"updated_uses\": updated_data_uses},\n            )\n\n\ndef update_consent(bind: Connection, data_use_map: Dict[str, str]) -> None:\n    \"\"\"\n    Update Consent objects in the database.\n    \"\"\"\n\n    # Update the Consent table\n    existing_consents: ResultProxy = bind.execute(\n        text(\"SELECT provided_identity_id, data_use FROM consent;\")\n    )\n\n    for row in existing_consents:\n        updated_use: str = _replace_matching_data_label(row[\"data_use\"], data_use_map)\n\n        update_label_query: TextClause = text(\n            \"UPDATE consent SET data_use= :updated_label WHERE provided_identity_id= :key AND data_use = :old_use\"\n        )\n        bind.execute(\n            update_label_query,\n            {\n                \"key\": row[\"provided_identity_id\"],\n                \"old_use\": row[\"data_use\"],\n                \"updated_label\": updated_use,\n            },\n        )\n\n    # Update the Privacy Request Table\n    existing_privacy_requests: ResultProxy = bind.execute(\n        text(\"select id, consent_preferences from privacyrequest;\")\n    )\n\n    for row in existing_privacy_requests:\n        preferences: List[Dict] = row[\"consent_preferences\"]\n\n        if preferences:\n            for index, preference in enumerate(preferences):\n                preferences[index][\"data_use\"] = _replace_matching_data_label(\n                    data_label=preference[\"data_use\"], data_label_map=data_use_map\n                )\n\n        update_pr_query: TextClause = text(\n            \"UPDATE privacyrequest SET consent_preferences= :updated_preferences WHERE id= :id\"\n        )\n        bind.execute(\n            update_pr_query,\n            {\"id\": row[\"id\"], \"updated_preferences\": json.dumps(preferences)},\n        )\n\n    # Update the Consent Request Table\n    existing_consent_requests: ResultProxy = bind.execute(\n        text(\"select id, preferences from consentrequest;\")\n    )\n\n    for row in existing_consent_requests:\n        preferences: List[Dict] = row[\"preferences\"]\n\n        if preferences:\n            for index, preference in enumerate(preferences):\n                preferences[index][\"data_use\"] = _replace_matching_data_label(\n                    data_label=preference[\"data_use\"], data_label_map=data_use_map\n                )\n\n        update_cr_query: TextClause = text(\n            \"UPDATE consentrequest SET preferences= :updated_preferences WHERE id= :id\"\n        )\n        bind.execute(\n            update_cr_query,\n            {\"id\": row[\"id\"], \"updated_preferences\": json.dumps(preferences)},\n        )\n\n\ndef remove_conflicting_rule_targets(bind: Connection):\n    \"\"\"\n    Iterates through all of the erasure policies and removes level 3 data categories in favor of level 2 data categories.\n\n    For example: user.demographic is preserved over user.demographic.*\n\n    This is needed because RuleTarget.create() validates all sibling rule targets to prevent invalid masking scenarios.\n    \"\"\"\n    erasure_rules: ResultProxy = bind.execute(\n        text(\"SELECT id, key FROM rule WHERE action_type = :action_type\"),\n        {\"action_type\": ActionType.erasure.value},\n    )\n\n    for rule in erasure_rules:\n        all_categories_query: ResultProxy = bind.execute(\n            text(\"SELECT data_category FROM ruletarget WHERE rule_id = :rule_id\"),\n            {\"rule_id\": rule.id},\n        )\n        all_categories = {row.data_category for row in all_categories_query}\n\n        rule_targets = bind.execute(\n            text(\"SELECT id, data_category FROM ruletarget WHERE rule_id = :rule_id\"),\n            {\"rule_id\": rule.id},\n        )\n\n        rule_targets_to_remove = []\n        for target in rule_targets:\n            parts = target.data_category.split(\".\")\n            if len(parts) == 3:\n                parent_category = f\"{parts[0]}.{parts[1]}\"\n                if parent_category in all_categories:\n                    rule_targets_to_remove.append(target)\n                    logger.info(\n                        f\"Marking conflicting rule target {target.data_category} for removal from rule {rule.key}\"\n                    )\n\n        if rule_targets_to_remove:\n            target_ids = [target.id for target in rule_targets_to_remove]\n            bind.execute(\n                text(\"DELETE FROM ruletarget WHERE id IN :target_ids\"),\n                {\"target_ids\": tuple(target_ids)},\n            )\n            logger.info(f\"Removed {len(target_ids)} conflicting rule targets\")\n\n\ndef update_default_dsr_policies(bind: Connection) -> None:\n    \"\"\"\n    Updates the default policies with new data categories using manual insertion.\n    \"\"\"\n\n    new_data_categories = [\n        \"user.behavior\",\n        \"user.content\",\n        \"user.privacy_preferences\",\n    ]\n\n    rules: ResultProxy = bind.execute(\n        text(\n            \"SELECT id, key FROM rule WHERE key IN (:access_policy, :erasure_policy);\"\n        ),\n        {\n            \"access_policy\": DEFAULT_ACCESS_POLICY_RULE,\n            \"erasure_policy\": DEFAULT_ERASURE_POLICY_RULE,\n        },\n    )\n\n    if rules.rowcount == 0:\n        logger.info(\"No default policies were found to update\")\n        return\n\n    updates_made = False\n    for rule in rules:\n        for data_category in new_data_categories:\n            compound_key = to_snake_case(f\"{rule.id}_{data_category}\")\n\n            # check if the rule target already exists\n            existing_target: ResultProxy = bind.execute(\n                text(\n                    \"SELECT 1 FROM ruletarget WHERE rule_id = :rule_id AND data_category = :data_category\"\n                ),\n                {\"rule_id\": rule.id, \"data_category\": data_category},\n            ).first()\n\n            if existing_target is None:\n                # Insert rule targets directly into the database to bypass validation checks.\n                # Invalid entries are removed in remove_conflicting_rule_targets\n                bind.execute(\n                    text(\n                        \"INSERT INTO ruletarget (id, name, key, data_category, rule_id) VALUES (:id, :name, :key, :data_category, :rule_id)\"\n                    ),\n                    {\n                        \"id\": generate_record_id(\"rul\"),\n                        \"name\": f\"{rule.id}-{data_category}\",\n                        \"key\": compound_key,\n                        \"data_category\": data_category,\n                        \"rule_id\": rule.id,\n                    },\n                )\n                logger.info(\n                    f\"Inserted new rule target: {data_category} for rule {rule.key}\"\n                )\n                updates_made = True\n            else:\n                logger.info(\n                    f\"Rule target already exists: {data_category} for rule {rule.key}\"\n                )\n\n    if updates_made:\n        logger.info(\"The default policies have been updated with new data categories\")\n    else:\n        logger.info(\"No updates were necessary for the default policies\")\n"}
{"type": "source_file", "path": "src/fides/api/alembic/migrations/__init__.py", "content": ""}
{"type": "source_file", "path": "src/fides/api/alembic/migrations/helpers/custom_report_migration_functions.py", "content": "import json\n\nimport sqlalchemy as sa\nfrom alembic import op\nfrom sqlalchemy.engine import Connection\n\n\ndef requires_upgrade_migration(report):\n    config = report.config\n    column_map = config.get(\"column_map\", {})\n\n    for column in column_map.values():\n        if isinstance(column, str):\n            return True\n    return False\n\n\ndef requires_downgrade_migration(report):\n    config = report.config\n    column_map = config.get(\"column_map\", {})\n\n    for column in column_map.values():\n        if isinstance(column, dict):\n            return True\n    return False\n\n\ndef upgrade_custom_reports(connection: Connection):\n    reports = connection.execute(sa.text(\"SELECT id, config FROM plus_custom_report\"))\n\n    for report in reports:\n        config = report.config\n\n        if not requires_upgrade_migration(report):\n            continue\n\n        column_map = config.get(\"column_map\")\n        table_state = config.get(\"table_state\", {})\n        column_visibility = table_state.get(\"columnVisibility\", {})\n\n        new_column_map = {\n            key: {\"label\": value, \"enabled\": column_visibility.get(key, True)}\n            for key, value in column_map.items()\n        }\n\n        config[\"column_map\"] = new_column_map\n\n        if \"columnVisibility\" in table_state:\n            del table_state[\"columnVisibility\"]\n\n        connection.execute(\n            sa.text(\"UPDATE plus_custom_report SET config = :config WHERE id = :id\"),\n            config=json.dumps(config),\n            id=report.id,\n        )\n\n\ndef downgrade_custom_reports(connection: Connection):\n    reports = connection.execute(sa.text(\"SELECT id, config FROM plus_custom_report\"))\n    for report in reports:\n        config = report.config\n\n        if not requires_downgrade_migration(report):\n            continue\n\n        column_map = config.get(\"column_map\")\n        table_state = config.get(\"table_state\", {})\n        column_visibility = {key: value[\"enabled\"] for key, value in column_map.items()}\n\n        new_column_map = {key: value[\"label\"] for key, value in column_map.items()}\n\n        config[\"column_map\"] = new_column_map\n        table_state[\"columnVisibility\"] = column_visibility\n\n        connection.execute(\n            sa.text(\"UPDATE plus_custom_report SET config = :config WHERE id = :id\"),\n            config=json.dumps(config),\n            id=report.id,\n        )\n"}
{"type": "source_file", "path": "src/fides/api/alembic/migrations/env.py", "content": "from logging.config import fileConfig\n\nfrom alembic import context\nfrom sqlalchemy import engine_from_config, pool\n\nfrom fides.api.util.logger import setup as setup_fidesapi_logger\nfrom fides.config import CONFIG\n\n# this is the Alembic Config object, which provides\n# access to the values within the .ini file in use.\nalembic_config = context.config\nfides_config = CONFIG\n\n# Interpret the config file for Python logging.\n# This line sets up loggers basically.\nfileConfig(alembic_config.config_file_name)\nsetup_fidesapi_logger(CONFIG)\n\n# add your model's MetaData object here\n# for 'autogenerate' support\n# from myapp import mymodel\n# target_metadata = mymodel.Base.metadata\nfrom fides.api.models.sql_models import Base\n\ntarget_metadata = Base.metadata\n\n# other values from the config, defined by the needs of env.py,\n# can be acquired:\n# my_important_option = config.get_main_option(\"my_important_option\")\n# ... etc.\n\n\ndef run_migrations_offline():\n    \"\"\"Run migrations in 'offline' mode.\n\n    This configures the context with just a URL\n    and not an Engine, though an Engine is acceptable\n    here as well.  By skipping the Engine creation\n    we don't even need a DBAPI to be available.\n\n    Calls to context.execute() here emit the given string to the\n    script output.\n\n    \"\"\"\n    url = fides_config.database.sync_database_uri\n    context.configure(\n        url=url,\n        target_metadata=target_metadata,\n        literal_binds=True,\n        dialect_opts={\"paramstyle\": \"named\"},\n    )\n\n    with context.begin_transaction():\n        context.run_migrations()\n\n\ndef run_migrations_online():\n    \"\"\"Run migrations in 'online' mode.\n\n    In this scenario we need to create an Engine\n    and associate a connection with the context.\n\n    \"\"\"\n    configuration = alembic_config.get_section(alembic_config.config_ini_section)\n    configuration[\"sqlalchemy.url\"] = fides_config.database.sync_database_uri\n    connectable = engine_from_config(\n        configuration,\n        prefix=\"sqlalchemy.\",\n        poolclass=pool.NullPool,\n    )\n\n    with connectable.connect() as connection:\n        context.configure(connection=connection, target_metadata=target_metadata)\n\n        with context.begin_transaction():\n            context.run_migrations()\n\n\nif context.is_offline_mode():\n    run_migrations_offline()\nelse:\n    run_migrations_online()\n"}
{"type": "source_file", "path": "src/fides/api/alembic/migrations/versions/0210948a8147_initial.py", "content": "\"\"\"initial\n\nRevision ID: 0210948a8147\nRevises: \nCreate Date: 2021-10-29 11:51:19.265141\n\n\"\"\"\n\nimport sqlalchemy as sa\nimport sqlalchemy_utils\nfrom alembic import op\nfrom sqlalchemy.dialects import postgresql\n\n# revision identifiers, used by Alembic.\nrevision = \"0210948a8147\"\ndown_revision = None\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade():\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.create_table(\n        \"client\",\n        sa.Column(\"id\", sa.String(length=255), nullable=False),\n        sa.Column(\n            \"created_at\",\n            sa.DateTime(timezone=True),\n            server_default=sa.text(\"now()\"),\n            nullable=True,\n        ),\n        sa.Column(\n            \"updated_at\",\n            sa.DateTime(timezone=True),\n            server_default=sa.text(\"now()\"),\n            nullable=True,\n        ),\n        sa.Column(\"hashed_secret\", sa.String(), nullable=False),\n        sa.Column(\"salt\", sa.String(), nullable=False),\n        sa.Column(\"scopes\", sa.ARRAY(sa.String()), nullable=False),\n        sa.PrimaryKeyConstraint(\"id\"),\n    )\n    op.create_index(op.f(\"ix_client_id\"), \"client\", [\"id\"], unique=False)\n    op.create_table(\n        \"connectionconfig\",\n        sa.Column(\"id\", sa.String(length=255), nullable=False),\n        sa.Column(\n            \"created_at\",\n            sa.DateTime(timezone=True),\n            server_default=sa.text(\"now()\"),\n            nullable=True,\n        ),\n        sa.Column(\n            \"updated_at\",\n            sa.DateTime(timezone=True),\n            server_default=sa.text(\"now()\"),\n            nullable=True,\n        ),\n        sa.Column(\"name\", sa.String(), nullable=False),\n        sa.Column(\"key\", sa.String(), nullable=False),\n        sa.Column(\n            \"connection_type\",\n            sa.Enum(\"postgres\", \"mongodb\", \"mysql\", \"https\", name=\"connectiontype\"),\n            nullable=False,\n        ),\n        sa.Column(\n            \"access\", sa.Enum(\"read\", \"write\", name=\"accesslevel\"), nullable=False\n        ),\n        sa.Column(\n            \"secrets\",\n            sqlalchemy_utils.types.encrypted.encrypted_type.StringEncryptedType(),\n            nullable=True,\n        ),\n        sa.Column(\"last_test_timestamp\", sa.DateTime(timezone=True), nullable=True),\n        sa.Column(\"last_test_succeeded\", sa.Boolean(), nullable=True),\n        sa.PrimaryKeyConstraint(\"id\"),\n    )\n    op.create_index(\n        op.f(\"ix_connectionconfig_id\"), \"connectionconfig\", [\"id\"], unique=False\n    )\n    op.create_index(\n        op.f(\"ix_connectionconfig_key\"), \"connectionconfig\", [\"key\"], unique=True\n    )\n    op.create_index(\n        op.f(\"ix_connectionconfig_name\"), \"connectionconfig\", [\"name\"], unique=True\n    )\n    op.create_table(\n        \"executionlog\",\n        sa.Column(\"id\", sa.String(length=255), nullable=False),\n        sa.Column(\n            \"created_at\",\n            sa.DateTime(timezone=True),\n            server_default=sa.text(\"now()\"),\n            nullable=True,\n        ),\n        sa.Column(\n            \"updated_at\",\n            sa.DateTime(timezone=True),\n            server_default=sa.text(\"now()\"),\n            nullable=True,\n        ),\n        sa.Column(\"dataset_name\", sa.String(), nullable=True),\n        sa.Column(\"collection_name\", sa.String(), nullable=True),\n        sa.Column(\n            \"fields_affected\", postgresql.JSONB(astext_type=sa.Text()), nullable=True\n        ),\n        sa.Column(\"message\", sa.String(), nullable=True),\n        sa.Column(\n            \"action_type\",\n            sa.Enum(\"access\", \"consent\", \"erasure\", \"update\", name=\"actiontype\"),\n            nullable=False,\n        ),\n        sa.Column(\n            \"status\",\n            sa.Enum(\n                \"in_processing\",\n                \"pending\",\n                \"complete\",\n                \"error\",\n                \"retrying\",\n                name=\"executionlogstatus\",\n            ),\n            nullable=False,\n        ),\n        sa.Column(\"privacy_request_id\", sa.String(), nullable=False),\n        sa.PrimaryKeyConstraint(\"id\"),\n    )\n    op.create_index(\n        op.f(\"ix_executionlog_action_type\"),\n        \"executionlog\",\n        [\"action_type\"],\n        unique=False,\n    )\n    op.create_index(\n        op.f(\"ix_executionlog_collection_name\"),\n        \"executionlog\",\n        [\"collection_name\"],\n        unique=False,\n    )\n    op.create_index(\n        op.f(\"ix_executionlog_dataset_name\"),\n        \"executionlog\",\n        [\"dataset_name\"],\n        unique=False,\n    )\n    op.create_index(op.f(\"ix_executionlog_id\"), \"executionlog\", [\"id\"], unique=False)\n    op.create_index(\n        op.f(\"ix_executionlog_privacy_request_id\"),\n        \"executionlog\",\n        [\"privacy_request_id\"],\n        unique=False,\n    )\n    op.create_index(\n        op.f(\"ix_executionlog_status\"), \"executionlog\", [\"status\"], unique=False\n    )\n    op.create_table(\n        \"storageconfig\",\n        sa.Column(\"id\", sa.String(length=255), nullable=False),\n        sa.Column(\n            \"created_at\",\n            sa.DateTime(timezone=True),\n            server_default=sa.text(\"now()\"),\n            nullable=True,\n        ),\n        sa.Column(\n            \"updated_at\",\n            sa.DateTime(timezone=True),\n            server_default=sa.text(\"now()\"),\n            nullable=True,\n        ),\n        sa.Column(\"name\", sa.String(), nullable=True),\n        sa.Column(\n            \"type\",\n            sa.Enum(\n                \"s3\",\n                \"gcs\",\n                \"transcend\",\n                \"onetrust\",\n                \"ethyca\",\n                \"local\",\n                name=\"storagetype\",\n            ),\n            nullable=False,\n        ),\n        sa.Column(\"details\", postgresql.JSONB(astext_type=sa.Text()), nullable=False),\n        sa.Column(\"key\", sa.String(), nullable=False),\n        sa.Column(\n            \"secrets\",\n            sqlalchemy_utils.types.encrypted.encrypted_type.StringEncryptedType(),\n            nullable=True,\n        ),\n        sa.Column(\n            \"format\",\n            sa.Enum(\"json\", \"csv\", name=\"responseformat\"),\n            server_default=\"json\",\n            nullable=False,\n        ),\n        sa.PrimaryKeyConstraint(\"id\"),\n    )\n    op.create_index(op.f(\"ix_storageconfig_id\"), \"storageconfig\", [\"id\"], unique=False)\n    op.create_index(op.f(\"ix_storageconfig_key\"), \"storageconfig\", [\"key\"], unique=True)\n    op.create_index(\n        op.f(\"ix_storageconfig_name\"), \"storageconfig\", [\"name\"], unique=True\n    )\n    op.create_index(\n        op.f(\"ix_storageconfig_type\"), \"storageconfig\", [\"type\"], unique=False\n    )\n    op.create_table(\n        \"datasetconfig\",\n        sa.Column(\"id\", sa.String(length=255), nullable=False),\n        sa.Column(\n            \"created_at\",\n            sa.DateTime(timezone=True),\n            server_default=sa.text(\"now()\"),\n            nullable=True,\n        ),\n        sa.Column(\n            \"updated_at\",\n            sa.DateTime(timezone=True),\n            server_default=sa.text(\"now()\"),\n            nullable=True,\n        ),\n        sa.Column(\"connection_config_id\", sa.String(), nullable=False),\n        sa.Column(\"fides_key\", sa.String(), nullable=False),\n        sa.Column(\"dataset\", postgresql.JSONB(astext_type=sa.Text()), nullable=False),\n        sa.ForeignKeyConstraint(\n            [\"connection_config_id\"],\n            [\"connectionconfig.id\"],\n        ),\n        sa.PrimaryKeyConstraint(\"id\"),\n    )\n    op.create_index(\n        op.f(\"ix_datasetconfig_fides_key\"), \"datasetconfig\", [\"fides_key\"], unique=True\n    )\n    op.create_index(op.f(\"ix_datasetconfig_id\"), \"datasetconfig\", [\"id\"], unique=False)\n    op.create_table(\n        \"policy\",\n        sa.Column(\"id\", sa.String(length=255), nullable=False),\n        sa.Column(\n            \"created_at\",\n            sa.DateTime(timezone=True),\n            server_default=sa.text(\"now()\"),\n            nullable=True,\n        ),\n        sa.Column(\n            \"updated_at\",\n            sa.DateTime(timezone=True),\n            server_default=sa.text(\"now()\"),\n            nullable=True,\n        ),\n        sa.Column(\"name\", sa.String(), nullable=False),\n        sa.Column(\"key\", sa.String(), nullable=False),\n        sa.Column(\"client_id\", sa.String(), nullable=False),\n        sa.ForeignKeyConstraint(\n            [\"client_id\"],\n            [\"client.id\"],\n        ),\n        sa.PrimaryKeyConstraint(\"id\"),\n        sa.UniqueConstraint(\"name\"),\n    )\n    op.create_index(op.f(\"ix_policy_id\"), \"policy\", [\"id\"], unique=False)\n    op.create_index(op.f(\"ix_policy_key\"), \"policy\", [\"key\"], unique=True)\n    op.create_table(\n        \"privacyrequest\",\n        sa.Column(\"id\", sa.String(length=255), nullable=False),\n        sa.Column(\n            \"created_at\",\n            sa.DateTime(timezone=True),\n            server_default=sa.text(\"now()\"),\n            nullable=True,\n        ),\n        sa.Column(\n            \"updated_at\",\n            sa.DateTime(timezone=True),\n            server_default=sa.text(\"now()\"),\n            nullable=True,\n        ),\n        sa.Column(\"external_id\", sa.String(), nullable=True),\n        sa.Column(\"started_processing_at\", sa.DateTime(timezone=True), nullable=True),\n        sa.Column(\"finished_processing_at\", sa.DateTime(timezone=True), nullable=True),\n        sa.Column(\"requested_at\", sa.DateTime(timezone=True), nullable=True),\n        sa.Column(\n            \"status\",\n            sa.Enum(\n                \"in_processing\",\n                \"pending\",\n                \"complete\",\n                \"error\",\n                name=\"privacyrequeststatus\",\n            ),\n            nullable=False,\n        ),\n        sa.Column(\"client_id\", sa.String(), nullable=True),\n        sa.Column(\"origin\", sa.String(), nullable=True),\n        sa.Column(\"policy_id\", sa.String(), nullable=True),\n        sa.ForeignKeyConstraint(\n            [\"client_id\"],\n            [\"client.id\"],\n        ),\n        sa.ForeignKeyConstraint(\n            [\"policy_id\"],\n            [\"policy.id\"],\n        ),\n        sa.PrimaryKeyConstraint(\"id\"),\n    )\n    op.create_index(\n        op.f(\"ix_privacyrequest_external_id\"),\n        \"privacyrequest\",\n        [\"external_id\"],\n        unique=False,\n    )\n    op.create_index(\n        op.f(\"ix_privacyrequest_id\"), \"privacyrequest\", [\"id\"], unique=False\n    )\n    op.create_index(\n        op.f(\"ix_privacyrequest_status\"), \"privacyrequest\", [\"status\"], unique=False\n    )\n    op.create_table(\n        \"rule\",\n        sa.Column(\"id\", sa.String(length=255), nullable=False),\n        sa.Column(\n            \"created_at\",\n            sa.DateTime(timezone=True),\n            server_default=sa.text(\"now()\"),\n            nullable=True,\n        ),\n        sa.Column(\n            \"updated_at\",\n            sa.DateTime(timezone=True),\n            server_default=sa.text(\"now()\"),\n            nullable=True,\n        ),\n        sa.Column(\"name\", sa.String(), nullable=False),\n        sa.Column(\"key\", sa.String(), nullable=False),\n        sa.Column(\"policy_id\", sa.String(), nullable=False),\n        sa.Column(\n            \"action_type\",\n            sa.Enum(\"access\", \"consent\", \"erasure\", \"update\", name=\"actiontype\"),\n            nullable=False,\n        ),\n        sa.Column(\n            \"masking_strategy\",\n            sqlalchemy_utils.types.encrypted.encrypted_type.StringEncryptedType(),\n            nullable=True,\n        ),\n        sa.Column(\"storage_destination_id\", sa.String(), nullable=True),\n        sa.Column(\"client_id\", sa.String(), nullable=True),\n        sa.ForeignKeyConstraint(\n            [\"client_id\"],\n            [\"client.id\"],\n        ),\n        sa.ForeignKeyConstraint(\n            [\"policy_id\"],\n            [\"policy.id\"],\n        ),\n        sa.ForeignKeyConstraint(\n            [\"storage_destination_id\"],\n            [\"storageconfig.id\"],\n        ),\n        sa.PrimaryKeyConstraint(\"id\"),\n        sa.UniqueConstraint(\"name\"),\n    )\n    op.create_index(op.f(\"ix_rule_id\"), \"rule\", [\"id\"], unique=False)\n    op.create_index(op.f(\"ix_rule_key\"), \"rule\", [\"key\"], unique=True)\n    op.create_table(\n        \"ruletarget\",\n        sa.Column(\"id\", sa.String(length=255), nullable=False),\n        sa.Column(\n            \"created_at\",\n            sa.DateTime(timezone=True),\n            server_default=sa.text(\"now()\"),\n            nullable=True,\n        ),\n        sa.Column(\n            \"updated_at\",\n            sa.DateTime(timezone=True),\n            server_default=sa.text(\"now()\"),\n            nullable=True,\n        ),\n        sa.Column(\"name\", sa.String(), nullable=False),\n        sa.Column(\"key\", sa.String(), nullable=False),\n        sa.Column(\"data_category\", sa.String(), nullable=False),\n        sa.Column(\"rule_id\", sa.String(), nullable=False),\n        sa.Column(\"client_id\", sa.String(), nullable=True),\n        sa.ForeignKeyConstraint(\n            [\"client_id\"],\n            [\"client.id\"],\n        ),\n        sa.ForeignKeyConstraint(\n            [\"rule_id\"],\n            [\"rule.id\"],\n        ),\n        sa.PrimaryKeyConstraint(\"id\"),\n        sa.UniqueConstraint(\"name\"),\n        sa.UniqueConstraint(\n            \"rule_id\", \"data_category\", name=\"_rule_id_data_category_uc\"\n        ),\n    )\n    op.create_index(op.f(\"ix_ruletarget_id\"), \"ruletarget\", [\"id\"], unique=False)\n    op.create_index(op.f(\"ix_ruletarget_key\"), \"ruletarget\", [\"key\"], unique=True)\n    # ### end Alembic commands ###\n\n\ndef downgrade():\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_index(op.f(\"ix_ruletarget_key\"), table_name=\"ruletarget\")\n    op.drop_index(op.f(\"ix_ruletarget_id\"), table_name=\"ruletarget\")\n    op.drop_table(\"ruletarget\")\n    op.drop_index(op.f(\"ix_rule_key\"), table_name=\"rule\")\n    op.drop_index(op.f(\"ix_rule_id\"), table_name=\"rule\")\n    op.drop_table(\"rule\")\n    op.drop_index(op.f(\"ix_privacyrequest_status\"), table_name=\"privacyrequest\")\n    op.drop_index(op.f(\"ix_privacyrequest_id\"), table_name=\"privacyrequest\")\n    op.drop_index(op.f(\"ix_privacyrequest_external_id\"), table_name=\"privacyrequest\")\n    op.drop_table(\"privacyrequest\")\n    op.drop_index(op.f(\"ix_policy_key\"), table_name=\"policy\")\n    op.drop_index(op.f(\"ix_policy_id\"), table_name=\"policy\")\n    op.drop_table(\"policy\")\n    op.drop_index(op.f(\"ix_datasetconfig_id\"), table_name=\"datasetconfig\")\n    op.drop_index(op.f(\"ix_datasetconfig_fides_key\"), table_name=\"datasetconfig\")\n    op.drop_table(\"datasetconfig\")\n    op.drop_index(op.f(\"ix_storageconfig_type\"), table_name=\"storageconfig\")\n    op.drop_index(op.f(\"ix_storageconfig_name\"), table_name=\"storageconfig\")\n    op.drop_index(op.f(\"ix_storageconfig_key\"), table_name=\"storageconfig\")\n    op.drop_index(op.f(\"ix_storageconfig_id\"), table_name=\"storageconfig\")\n    op.drop_table(\"storageconfig\")\n    op.drop_index(op.f(\"ix_executionlog_status\"), table_name=\"executionlog\")\n    op.drop_index(op.f(\"ix_executionlog_privacy_request_id\"), table_name=\"executionlog\")\n    op.drop_index(op.f(\"ix_executionlog_id\"), table_name=\"executionlog\")\n    op.drop_index(op.f(\"ix_executionlog_dataset_name\"), table_name=\"executionlog\")\n    op.drop_index(op.f(\"ix_executionlog_collection_name\"), table_name=\"executionlog\")\n    op.drop_index(op.f(\"ix_executionlog_action_type\"), table_name=\"executionlog\")\n    op.drop_table(\"executionlog\")\n    op.drop_index(op.f(\"ix_connectionconfig_name\"), table_name=\"connectionconfig\")\n    op.drop_index(op.f(\"ix_connectionconfig_key\"), table_name=\"connectionconfig\")\n    op.drop_index(op.f(\"ix_connectionconfig_id\"), table_name=\"connectionconfig\")\n    op.drop_table(\"connectionconfig\")\n    op.drop_index(op.f(\"ix_client_id\"), table_name=\"client\")\n    op.drop_table(\"client\")\n    # ### end Alembic commands ###\n"}
{"type": "source_file", "path": "src/fides/api/alembic/migrations/helpers/database_functions.py", "content": "import uuid\n\n\ndef generate_record_id(prefix):\n    \"\"\"Generates an ID that can be used for a database table row ID.\"\"\"\n    return prefix + \"_\" + str(uuid.uuid4())\n"}
{"type": "source_file", "path": "src/fides/api/alembic/migrations/helpers/__init__.py", "content": ""}
