{"repo_info": {"repo_name": "EditGuard", "repo_owner": "xuanyuzhang21", "repo_url": "https://github.com/xuanyuzhang21/EditGuard"}}
{"type": "test_file", "path": "code/data/test_dataset_td.py", "content": "import os\nimport os.path as osp\nimport torch\nimport torch.utils.data as data\nimport data.util as util\n\nimport random\nimport numpy as np\nfrom PIL import Image\n\nclass imageTestDataset(data.Dataset):\n\n    def __init__(self, opt):\n        super(imageTestDataset, self).__init__()\n        self.opt = opt\n        self.half_N_frames = opt['N_frames'] // 2\n        self.data_path = opt['data_path']\n        self.bit_path = opt['bit_path']\n        self.txt_path = self.opt['txt_path']\n        self.num_image = self.opt['num_image']\n        with open(self.txt_path) as f:\n            self.list_image = f.readlines()\n        self.list_image = [line.strip('\\n') for line in self.list_image]\n        # self.list_image = sorted(self.list_image)\n        l = len(self.list_image) // (self.num_image + 1)\n        self.image_list_gt = self.list_image\n        self.image_list_bit = self.list_image\n\n\n    def __getitem__(self, index):\n        path_GT = self.image_list_gt[index]\n        \n        img_GT = util.read_img(None, osp.join(self.data_path, path_GT))\n        img_GT = img_GT[:, :, [2, 1, 0]]\n        img_GT = torch.from_numpy(np.ascontiguousarray(np.transpose(img_GT, (2, 0, 1)))).float().unsqueeze(0)\n        img_GT = torch.nn.functional.interpolate(img_GT, size=(512, 512), mode='nearest', align_corners=None)\n\n        T, C, W, H = img_GT.shape\n        list_h = []\n        R = 0\n        G = 0\n        B = 255\n        image = Image.new('RGB', (W, H), (R, G, B))\n        result = np.array(image) / 255.\n        expanded_matrix = np.expand_dims(result, axis=0) \n        expanded_matrix = np.repeat(expanded_matrix, T, axis=0)\n        imgs_LQ = torch.from_numpy(np.ascontiguousarray(expanded_matrix)).float()\n        imgs_LQ = imgs_LQ.permute(0, 3, 1, 2)\n\n\n        imgs_LQ = torch.nn.functional.interpolate(imgs_LQ, size=(W, H), mode='nearest', align_corners=None)\n\n        list_h.append(imgs_LQ)\n\n        list_h = torch.stack(list_h, dim=0)\n\n        return {\n                'LQ': list_h,\n                'GT': img_GT\n            }\n    \n    def __len__(self):\n        return len(self.image_list_gt)  \n"}
{"type": "test_file", "path": "code/test_gradio.py", "content": "import sys\n\nimport os\nimport math\nimport argparse\nimport random\nimport logging\n\nimport torch\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\nfrom data.data_sampler import DistIterSampler\n\nimport options.options as option\nfrom utils import util\nfrom data.util import read_img \nfrom data import create_dataloader, create_dataset\nfrom models import create_model\nimport numpy as np\nfrom PIL import Image\nfrom diffusers import StableDiffusionInpaintPipeline\n\n\ndef init_dist(backend='nccl', **kwargs):\n    ''' initialization for distributed training'''\n    # if mp.get_start_method(allow_none=True) is None:\n    if mp.get_start_method(allow_none=True) != 'spawn':\n        mp.set_start_method('spawn')\n    rank = int(os.environ['RANK'])\n    num_gpus = torch.cuda.device_count()\n    torch.cuda.set_device(rank % num_gpus)\n    dist.init_process_group(backend=backend, **kwargs)\n\n\ndef load_image(image, message = None):\n    # img_GT = read_img(None, image_path)\n    img_GT = image / 255\n    # print(img_GT)\n    img_GT = img_GT[:, :, [2, 1, 0]]\n    img_GT = torch.from_numpy(np.ascontiguousarray(np.transpose(img_GT, (2, 0, 1)))).float().unsqueeze(0)\n    img_GT = torch.nn.functional.interpolate(img_GT, size=(512, 512), mode='nearest', align_corners=None)\n    img_GT = img_GT.unsqueeze(0)\n\n    _, T, C, W, H = img_GT.shape\n    list_h = []\n    R = 0\n    G = 0\n    B = 255\n    image = Image.new('RGB', (W, H), (R, G, B))\n    result = np.array(image) / 255.\n    expanded_matrix = np.expand_dims(result, axis=0) \n    expanded_matrix = np.repeat(expanded_matrix, T, axis=0)\n    imgs_LQ = torch.from_numpy(np.ascontiguousarray(expanded_matrix)).float()\n    imgs_LQ = imgs_LQ.permute(0, 3, 1, 2)\n    imgs_LQ = torch.nn.functional.interpolate(imgs_LQ, size=(W, H), mode='nearest', align_corners=None)\n    imgs_LQ = imgs_LQ.unsqueeze(0)\n\n    list_h.append(imgs_LQ)\n\n    list_h = torch.stack(list_h, dim=0)\n\n    return {\n            'LQ': list_h,\n            'GT': img_GT,\n            'MES': message\n        }\n\n\ndef image_editing(image_numpy, mask_image, prompt):\n\n    pipe = StableDiffusionInpaintPipeline.from_pretrained(\n            \"stabilityai/stable-diffusion-2-inpainting\",\n            torch_dtype=torch.float16,\n        ).to(\"cuda\")\n    \n    pil_image = Image.fromarray(image_numpy)\n    print(mask_image.shape)\n    print(\"maskmin\", mask_image.min(), \"maskmax\", mask_image.max())\n    mask_image = Image.fromarray(mask_image.astype(np.uint8)).convert(\"L\")\n    image_init = pil_image.convert(\"RGB\").resize((512, 512))\n    \n    h, w = mask_image.size\n    \n    image_inpaint = pipe(prompt=prompt, image=image_init, mask_image=mask_image, height=w, width=h).images[0]\n    image_inpaint = np.array(image_inpaint) / 255.\n    image = np.array(image_init) / 255.\n    mask_image = np.array(mask_image)\n    mask_image = np.stack([mask_image] * 3, axis=-1) / 255.\n    mask_image = mask_image.astype(np.uint8)\n    image_fuse = image * (1 - mask_image) + image_inpaint * mask_image\n\n    return image_fuse"}
{"type": "source_file", "path": "code/utils/face_detection.py", "content": "import cv2\nimport dlib\nimport numpy as np\n\n## Face detection\ndef face_detection(img,upsample_times=1):\n    # Ask the detector to find the bounding boxes of each face. The 1 in the\n    # second argument indicates that we should upsample the image 1 time. This\n    # will make everything bigger and allow us to detect more faces.\n    detector = dlib.get_frontal_face_detector()\n    faces = detector(img, upsample_times)\n\n    return faces\n\nPREDICTOR_PATH = '/userhome/NewIBSN/IBSN_SepMark/shape_predictor_68_face_landmarks.dat'\npredictor = dlib.shape_predictor(PREDICTOR_PATH)\n## Face and points detection\ndef face_points_detection(img, bbox:dlib.rectangle):\n    # Get the landmarks/parts for the face in box d.\n    shape = predictor(img, bbox)\n\n    # loop over the 68 facial landmarks and convert them\n    # to a 2-tuple of (x, y)-coordinates\n    coords = np.asarray(list([p.x, p.y] for p in shape.parts()), dtype=int)\n\n    # return the array of (x, y)-coordinates\n    return coords\n\ndef select_face(im, r=10, choose=True):\n    faces = face_detection(im)\n\n    if len(faces) == 0:\n        return None, None, None\n\n    if len(faces) == 1 or not choose:\n        idx = np.argmax([(face.right() - face.left()) * (face.bottom() - face.top()) for face in faces])\n        bbox = faces[idx]\n    else:\n        bbox = []\n\n        def click_on_face(event, x, y, flags, params):\n            if event != cv2.EVENT_LBUTTONDOWN:\n                return\n\n            for face in faces:\n                if face.left() < x < face.right() and face.top() < y < face.bottom():\n                    bbox.append(face)\n                    break\n\n        im_copy = im.copy()\n        for face in faces:\n            # draw the face bounding box\n            cv2.rectangle(im_copy, (face.left(), face.top()), (face.right(), face.bottom()), (0, 0, 255), 1)\n        cv2.imshow('Click the Face:', im_copy)\n        cv2.setMouseCallback('Click the Face:', click_on_face)\n        while len(bbox) == 0:\n            cv2.waitKey(1)\n        cv2.destroyAllWindows()\n        bbox = bbox[0]\n\n    points = np.asarray(face_points_detection(im, bbox))\n\n    im_w, im_h = im.shape[:2]\n    left, top = np.min(points, 0)\n    right, bottom = np.max(points, 0)\n\n    x, y = max(0, left - r), max(0, top - r)\n    w, h = min(right + r, im_h) - x, min(bottom + r, im_w) - y\n\n    return points - np.asarray([[x, y]]), (x, y, w, h), im[y:y + h, x:x + w]\n\n\ndef select_all_faces(im, r=10):\n    faces = face_detection(im)\n\n    if len(faces) == 0:\n        return None\n\n    faceBoxes = {k : {\"points\" : None,\n                      \"shape\" : None,\n                      \"face\" : None} for k in range(len(faces))}\n    for i, bbox in enumerate(faces):\n        points = np.asarray(face_points_detection(im, bbox))\n\n        im_w, im_h = im.shape[:2]\n        left, top = np.min(points, 0)\n        right, bottom = np.max(points, 0)\n\n        x, y = max(0, left - r), max(0, top - r)\n        w, h = min(right + r, im_h) - x, min(bottom + r, im_w) - y\n        faceBoxes[i][\"points\"] = points - np.asarray([[x, y]])\n        faceBoxes[i][\"shape\"] = (x, y, w, h)\n        faceBoxes[i][\"face\"] = im[y:y + h, x:x + w]\n\n    return faceBoxes\n"}
{"type": "source_file", "path": "code/utils/decompression.py", "content": "# Standard libraries\nimport itertools\nimport numpy as np\n# PyTorch\nimport torch\nimport torch.nn as nn\n# Local\nfrom . import JPEG_utils as utils\n\n\nclass y_dequantize(nn.Module):\n    \"\"\" Dequantize Y channel\n    Inputs:\n        image(tensor): batch x height x width\n        factor(float): compression factor\n    Outputs:\n        image(tensor): batch x height x width\n    \"\"\"\n    def __init__(self, factor=1):\n        super(y_dequantize, self).__init__()\n        self.y_table = utils.y_table\n        self.factor = factor\n\n    def forward(self, image):\n        return image * (self.y_table * self.factor)\n\n\nclass c_dequantize(nn.Module):\n    \"\"\" Dequantize CbCr channel\n    Inputs:\n        image(tensor): batch x height x width\n        factor(float): compression factor\n    Outputs:\n        image(tensor): batch x height x width\n    \"\"\"\n    def __init__(self, factor=1):\n        super(c_dequantize, self).__init__()\n        self.factor = factor\n        self.c_table = utils.c_table\n\n    def forward(self, image):\n        return image * (self.c_table * self.factor)\n\n\nclass idct_8x8(nn.Module):\n    \"\"\" Inverse discrete Cosine Transformation\n    Input:\n        dcp(tensor): batch x height x width\n    Output:\n        image(tensor): batch x height x width\n    \"\"\"\n    def __init__(self):\n        super(idct_8x8, self).__init__()\n        alpha = np.array([1. / np.sqrt(2)] + [1] * 7)\n        self.alpha = nn.Parameter(torch.from_numpy(np.outer(alpha, alpha)).float())\n        tensor = np.zeros((8, 8, 8, 8), dtype=np.float32)\n        for x, y, u, v in itertools.product(range(8), repeat=4):\n            tensor[x, y, u, v] = np.cos((2 * u + 1) * x * np.pi / 16) * np.cos(\n                (2 * v + 1) * y * np.pi / 16)\n        self.tensor = nn.Parameter(torch.from_numpy(tensor).float())\n\n    def forward(self, image):\n        \n        image = image * self.alpha\n        result = 0.25 * torch.tensordot(image, self.tensor, dims=2) + 128\n        result.view(image.shape)\n        return result\n\n\nclass block_merging(nn.Module):\n    \"\"\" Merge pathces into image\n    Inputs:\n        patches(tensor) batch x height*width/64, height x width\n        height(int)\n        width(int)\n    Output:\n        image(tensor): batch x height x width\n    \"\"\"\n    def __init__(self):\n        super(block_merging, self).__init__()\n        \n    def forward(self, patches, height, width):\n        k = 8\n        batch_size = patches.shape[0]\n        # print(patches.shape) # (1,1024,8,8) \n        image_reshaped = patches.view(batch_size, height//k, width//k, k, k)\n        image_transposed = image_reshaped.permute(0, 1, 3, 2, 4)\n        return image_transposed.contiguous().view(batch_size, height, width)\n\n\nclass chroma_upsampling(nn.Module):\n    \"\"\" Upsample chroma layers\n    Input: \n        y(tensor): y channel image\n        cb(tensor): cb channel\n        cr(tensor): cr channel\n    Ouput:\n        image(tensor): batch x height x width x 3\n    \"\"\"\n    def __init__(self):\n        super(chroma_upsampling, self).__init__()\n    \n    def forward(self, y, cb, cr):\n        def repeat(x, k=2):\n            height, width = x.shape[1:3]\n            x = x.unsqueeze(-1)\n            x = x.repeat(1, 1, k, k)\n            x = x.view(-1, height * k, width * k)\n            return x\n\n        cb = repeat(cb)\n        cr = repeat(cr)\n        \n        return torch.cat([y.unsqueeze(3), cb.unsqueeze(3), cr.unsqueeze(3)], dim=3)\n\n\nclass ycbcr_to_rgb_jpeg(nn.Module):\n    \"\"\" Converts YCbCr image to RGB JPEG\n    Input:\n        image(tensor): batch x height x width x 3\n    Outpput:\n        result(tensor): batch x 3 x height x width\n    \"\"\"\n    def __init__(self):\n        super(ycbcr_to_rgb_jpeg, self).__init__()\n\n        matrix = np.array(\n            [[1., 0., 1.402], [1, -0.344136, -0.714136], [1, 1.772, 0]],\n            dtype=np.float32).T\n        self.shift = nn.Parameter(torch.tensor([0, -128., -128.]))\n        self.matrix = nn.Parameter(torch.from_numpy(matrix))\n\n    def forward(self, image):\n        result = torch.tensordot(image + self.shift, self.matrix, dims=1)\n        #result = torch.from_numpy(result)\n        result.view(image.shape)\n        return result.permute(0, 3, 1, 2)\n\n\nclass decompress_jpeg(nn.Module):\n    \"\"\" Full JPEG decompression algortihm\n    Input:\n        compressed(dict(tensor)): batch x h*w/64 x 8 x 8\n        rounding(function): rounding function to use\n        factor(float): Compression factor\n    Ouput:\n        image(tensor): batch x 3 x height x width\n    \"\"\"\n    # def __init__(self, height, width, rounding=torch.round, factor=1):\n    def __init__(self, rounding=torch.round, factor=1):\n        super(decompress_jpeg, self).__init__()\n        self.c_dequantize = c_dequantize(factor=factor)\n        self.y_dequantize = y_dequantize(factor=factor)\n        self.idct = idct_8x8()\n        self.merging = block_merging()\n        # comment this line if no subsampling\n        self.chroma = chroma_upsampling()\n        self.colors = ycbcr_to_rgb_jpeg()\n        \n        # self.height, self.width = height, width\n        \n    def forward(self, y, cb, cr, height, width):\n        components = {'y': y, 'cb': cb, 'cr': cr}\n        # height = y.shape[0]\n        # width = y.shape[1]\n        self.height = height\n        self.width = width\n        for k in components.keys():\n            if k in ('cb', 'cr'):\n                comp = self.c_dequantize(components[k])\n                # comment this line if no subsampling\n                height, width = int(self.height/2), int(self.width/2)\n                # height, width = int(self.height), int(self.width)\n                \n            else:\n                comp = self.y_dequantize(components[k]) \n                # comment this line if no subsampling \n                height, width = self.height, self.width \n            comp = self.idct(comp) \n            components[k] = self.merging(comp, height, width) \n            # \n        # comment this line if no subsampling \n        image = self.chroma(components['y'], components['cb'], components['cr']) \n        # image = torch.cat([components['y'].unsqueeze(3), components['cb'].unsqueeze(3), components['cr'].unsqueeze(3)], dim=3) \n        image = self.colors(image)\n\n        image = torch.min(255*torch.ones_like(image),\n                          torch.max(torch.zeros_like(image), image))\n        return image/255\n\n"}
{"type": "source_file", "path": "code/data/__init__.py", "content": "'''create dataset and dataloader'''\nimport logging\nimport torch\nimport torch.utils.data\n\ndef create_dataloader(dataset, dataset_opt, opt=None, sampler=None):\n    phase = dataset_opt['phase']\n    if phase == 'train':\n        if opt['dist']:\n            world_size = torch.distributed.get_world_size()\n            num_workers = dataset_opt['n_workers']\n            assert dataset_opt['batch_size'] % world_size == 0\n            batch_size = dataset_opt['batch_size'] // world_size\n            shuffle = False\n        else:\n            num_workers = dataset_opt['n_workers'] * len(opt['gpu_ids'])\n            batch_size = dataset_opt['batch_size']\n            shuffle = True\n        return torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=shuffle,\n                                           num_workers=num_workers, sampler=sampler, drop_last=True,\n                                           pin_memory=False)\n    else:\n        return torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=False, num_workers=1,\n                                           pin_memory=True)\n\n\ndef create_dataset(dataset_opt):\n    mode = dataset_opt['mode']\n    if mode == 'test':\n        from data.coco_test_dataset import imageTestDataset as D\n    elif mode == 'train':\n        from data.coco_dataset import CoCoDataset as D\n    elif mode == 'td':\n        from data.test_dataset_td import imageTestDataset as D\n    else:\n        raise NotImplementedError('Dataset [{:s}] is not recognized.'.format(mode))\n    print(mode)\n    dataset = D(dataset_opt)\n\n    logger = logging.getLogger('base')\n    logger.info('Dataset [{:s} - {:s}] is created.'.format(dataset.__class__.__name__,\n                                                           dataset_opt['name']))\n    return dataset\n"}
{"type": "source_file", "path": "code/data/coco_dataset.py", "content": "'''\nVimeo90K dataset\nsupport reading images from lmdb, image folder and memcached\n'''\nimport logging\nimport os\nimport os.path as osp\nimport pickle\nimport random\n\nimport cv2\nimport lmdb\nimport numpy as np\nimport torch\nimport torch.utils.data as data\n\nimport data.util as util\n\ntry:\n    import mc\nexcept ImportError:\n    pass\nlogger = logging.getLogger('base')\n\nclass CoCoDataset(data.Dataset):\n    def __init__(self, opt):\n        super(CoCoDataset, self).__init__()\n        self.opt = opt\n        # get train indexes\n        self.data_path = self.opt['data_path']\n        self.txt_path = self.opt['txt_path']\n        with open(self.txt_path) as f:\n            self.list_image = f.readlines()\n        self.list_image = [line.strip('\\n') for line in self.list_image]\n        # temporal augmentation\n        self.interval_list = opt['interval_list']\n        self.random_reverse = opt['random_reverse']\n        logger.info('Temporal augmentation interval list: [{}], with random reverse is {}.'.format(\n            ','.join(str(x) for x in opt['interval_list']), self.random_reverse))\n        self.data_type = self.opt['data_type']\n        random.shuffle(self.list_image)\n        self.LR_input = True\n        self.num_image = self.opt['num_image']\n\n    def _ensure_memcached(self):\n        if self.mclient is None:\n            # specify the config files\n            server_list_config_file = None\n            client_config_file = None\n            self.mclient = mc.MemcachedClient.GetInstance(server_list_config_file,\n                                                          client_config_file)\n\n    def __getitem__(self, index):\n        GT_size = self.opt['GT_size']\n        image_name = self.list_image[index]\n        path_frame = os.path.join(self.data_path, image_name)\n        img_GT = util.read_img(None, osp.join(path_frame, path_frame))\n        index_h = random.randint(0, len(self.list_image) - 1)\n\n        # random crop\n        H, W, C = img_GT.shape\n        rnd_h = random.randint(0, max(0, H - GT_size))\n        rnd_w = random.randint(0, max(0, W - GT_size))\n        img_frames = img_GT[rnd_h:rnd_h + GT_size, rnd_w:rnd_w + GT_size, :]\n        # BGR to RGB, HWC to CHW, numpy to tensor\n        img_frames = img_frames[:, :, [2, 1, 0]]\n        img_frames = torch.from_numpy(np.ascontiguousarray(np.transpose(img_frames, (2, 0, 1)))).float().unsqueeze(0)\n\n        # process h_list\n        if index_h % 100 == 0:\n            path_frame_h = \"../dataset/locwatermark/blue.png\"\n        else:\n            image_name_h = self.list_image[index_h]\n            path_frame_h = os.path.join(self.data_path, image_name_h)\n        \n        frame_h = util.read_img(None, osp.join(path_frame_h, path_frame_h))\n        H1, W1, C1 = frame_h.shape\n        rnd_h = random.randint(0, max(0, H1 - GT_size))\n        rnd_w = random.randint(0, max(0, W1 - GT_size))\n        img_frames_h = frame_h[rnd_h:rnd_h + GT_size, rnd_w:rnd_w + GT_size, :]\n        img_frames_h = img_frames_h[:, :, [2, 1, 0]]\n        img_frames_h = torch.from_numpy(np.ascontiguousarray(np.transpose(img_frames_h, (2, 0, 1)))).float().unsqueeze(0)\n\n        img_frames_h = torch.nn.functional.interpolate(img_frames_h, size=(512, 512), mode='nearest', align_corners=None).unsqueeze(0)\n        img_frames = torch.nn.functional.interpolate(img_frames, size=(512, 512), mode='nearest', align_corners=None)\n\n        return {'GT': img_frames, 'LQ': img_frames_h}\n\n    def __len__(self):\n        return len(self.list_image)"}
{"type": "source_file", "path": "code/maskextract.py", "content": "import os\nfrom PIL import Image\nimport numpy as np\nimport argparse\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--threshold', default=0.2, type=float, help='Path to option YMAL file.')\n    args = parser.parse_args()\n\n    input_folder = 'results/test_age-set'\n    output_folder = 'results/mask'\n\n    for filename in os.listdir(input_folder):\n        if filename.endswith('_0_0_LRGT.png'):\n            digits = filename.split('_')[0]\n            if digits.isdigit():\n                digits = int(digits)\n                \n                if digits >= 0 and digits <= 1000:\n\n                    input_path_LRGT = os.path.join(input_folder, filename)\n                    input_path_SR_h = os.path.join(input_folder, filename).replace('LRGT', 'SR_h')\n\n                    image_LRGT = Image.open(input_path_LRGT).convert(\"RGB\")\n                    image_SR_h = Image.open(input_path_SR_h).convert(\"RGB\")\n\n                    w, h = image_SR_h.size\n                    image_LRGT = image_LRGT.resize((w, h))\n                    \n                    array_LRGT = np.array(image_LRGT) / 255.\n                    array_SR_h = np.array(image_SR_h) / 255.\n                    \n                    residual = np.abs(array_LRGT - array_SR_h)\n\n                    threshold = args.threshold\n                    mask = np.where(residual > threshold, 1, 0)\n                    \n                    os.makedirs(output_folder, exist_ok=True)\n                    \n                    output_path = os.path.join(output_folder, str(digits+1).zfill(4)+'.png')\n\n                    mask = np.sum(mask, axis=2)\n\n                    mask_image = Image.fromarray((mask * 255).astype(np.uint8))\n                    mask_image.save(output_path)"}
{"type": "source_file", "path": "code/utils/JPEG.py", "content": "\n\nimport torch\nimport torch.nn as nn\n\nfrom .JPEG_utils import diff_round, quality_to_factor, Quantization\nfrom .compression import compress_jpeg\nfrom .decompression import decompress_jpeg\n\n\nclass DiffJPEG(nn.Module):    \n    def __init__(self, differentiable=True, quality=75):\n        ''' Initialize the DiffJPEG layer\n        Inputs:\n            height(int): Original image height\n            width(int): Original image width\n            differentiable(bool): If true uses custom differentiable\n                rounding function, if false uses standrard torch.round\n            quality(float): Quality factor for jpeg compression scheme. \n        '''\n        super(DiffJPEG, self).__init__()\n        if differentiable:\n            rounding = diff_round\n            # rounding = Quantization()\n        else:\n            rounding = torch.round\n        factor = quality_to_factor(quality)\n        self.compress = compress_jpeg(rounding=rounding, factor=factor)\n        # self.decompress = decompress_jpeg(height, width, rounding=rounding,\n        #                                   factor=factor)\n        self.decompress = decompress_jpeg(rounding=rounding, factor=factor)\n\n    def forward(self, x):\n        '''\n        '''\n        org_height = x.shape[2]\n        org_width = x.shape[3]\n        y, cb, cr = self.compress(x)\n\n        recovered = self.decompress(y, cb, cr, org_height, org_width)\n        return recovered\n\n\n"}
{"type": "source_file", "path": "code/app.py", "content": "import gradio as gr\nimport numpy as np\nimport torch\nfrom PIL import Image, ImageDraw\nimport requests\nfrom copy import deepcopy\nimport cv2\nfrom test_gradio import load_image, image_editing\n\nimport options.options as option\nfrom utils.JPEG import DiffJPEG\nfrom scipy.io.wavfile import read as wav_read\nfrom scipy.io import wavfile\n\nimport os\nimport math\nimport argparse\nimport random\nimport logging\n\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\nfrom data.data_sampler import DistIterSampler\n\nfrom utils import util\nfrom data.util import read_img \nfrom data import create_dataloader, create_dataset\nfrom models import create_model as create_model_editguard\nfrom diffusers import StableDiffusionInpaintPipeline\n\nimport base64\nimport gradio as gr\n\nfrom diffusers import StableDiffusionInpaintPipeline\nfrom scipy.ndimage import zoom\n\nimport matplotlib.pyplot as plt\n\ndef img_to_base64(filepath):\n    with open(filepath, \"rb\") as img_file:\n        return base64.b64encode(img_file.read()).decode()\n\nlogo_base64 = img_to_base64(\"../logo.png\")\n\nhtml_content = f\"\"\"\n<div style='display: flex; align-items: center; justify-content: center; padding: 20px;'>\n    <img src='data:image/png;base64,{logo_base64}' alt='Logo' style='height: 50px; margin-right: 20px;'>\n    <strong><font size='8'>EditGuard<font></strong>\n</div>\n\"\"\"\n\n# Examples\nexamples = [\n    [\"../dataset/examples/0011.png\"],\n    [\"../dataset/examples/0012.png\"],\n    [\"../dataset/examples/0003.png\"],\n    [\"../dataset/examples/0004.png\"],\n    [\"../dataset/examples/0005.png\"],\n    [\"../dataset/examples/0006.png\"],\n    [\"../dataset/examples/0007.png\"],\n    [\"../dataset/examples/0008.png\"],\n    [\"../dataset/examples/0009.png\"],\n    [\"../dataset/examples/0010.png\"],\n    [\"../dataset/examples/0002.png\"],\n]\n\ndefault_example = examples[0]\n\ndef hiding(image_input, bit_input, model):\n\n    message = np.array([int(bit_input[i:i+1]) for i in range(0, len(bit_input), 1)])\n    message = message - 0.5\n    val_data = load_image(image_input, message)\n    model.feed_data(val_data)\n    container = model.image_hiding()\n\n    from PIL import Image\n    image = Image.fromarray(container)\n    return container, container\n\ndef rand(num_bits=64):\n    random_str = ''.join([str(random.randint(0, 1)) for _ in range(num_bits)])\n    return random_str\n\ndef ImageEdit(img, prompt, model_index):\n    image, mask = img[\"image\"], np.float32(img[\"mask\"])\n\n    received_image = image_editing(image, mask, prompt)\n    return received_image, received_image, received_image\n\n\ndef imgae_model_select(ckp_index=0):\n    # options\n    opt = option.parse(\"options/test_editguard.yml\", is_train=True)\n    # distributed training settings\n    opt['dist'] = False\n    rank = -1\n    print('Disabled distributed training.')\n\n    # loading resume state if exists\n    if opt['path'].get('resume_state', None):\n        # distributed resuming: all load into default GPU\n        device_id = torch.cuda.current_device()\n        resume_state = torch.load(opt['path']['resume_state'],\n                                    map_location=lambda storage, loc: storage.cuda(device_id))\n        option.check_resume(opt, resume_state['iter'])  # check resume options\n    else:\n        resume_state = None\n\n    # convert to NoneDict, which returns None for missing keys\n    opt = option.dict_to_nonedict(opt)\n    torch.backends.cudnn.benchmark = True\n    # create model\n\n    model = create_model_editguard(opt)\n\n    if ckp_index == 0:\n        model_pth = '../checkpoints/clean.pth'\n    print(model_pth)\n    model.load_test(model_pth)\n    return model\n\n\n\ndef Gaussian_image_degradation(image, NL):\n    image = torch.from_numpy(np.transpose(image, (2, 0, 1)))\n    image = image.unsqueeze(0)\n    NL = NL / 255.0\n    noise = np.random.normal(0, NL, image.shape)\n    torchnoise = torch.from_numpy(noise).float()\n    y_forw = image + torchnoise\n    y_forw = torch.clamp(y_forw, 0, 1)\n    y_forw = y_forw.permute(0, 2, 3, 1)\n    y_forw = y_forw.cpu().detach().numpy().squeeze()\n\n    y_forw = (y_forw * 255.0).astype(np.uint8)\n    return y_forw, y_forw\n\n\n\ndef JPEG_image_degradation(image, NL):\n    image = image.astype(np.float32)\n    image = torch.from_numpy(np.transpose(image, (2, 0, 1)))\n    image = image.unsqueeze(0)\n    JPEG = DiffJPEG(differentiable=True, quality=int(NL))\n    y_forw = JPEG(image)\n    y_forw = y_forw.permute(0, 2, 3, 1)\n    y_forw = y_forw.cpu().detach().numpy().squeeze()\n    y_forw = (y_forw * 255.0).astype(np.uint8)\n\n    return y_forw, y_forw\n\n\ndef revealing(image_edited, input_bit, model_list, model):\n\n    if model_list==0:\n        number = 0.2\n    else:\n        number = 0.2\n\n    container_data = load_image(image_edited) ## load tampered images\n    model.feed_data(container_data)\n    mask, remesg = model.image_recovery(number)\n    mask = Image.fromarray(mask.astype(np.uint8))\n    remesg = remesg.cpu().numpy()[0]\n    remesg = ''.join([str(int(x)) for x in remesg])\n    bit_acc = calculate_similarity_percentage(input_bit, remesg)\n    return mask, remesg, bit_acc\n\n\n\ndef calculate_similarity_percentage(str1, str2):\n\n    if len(str1) == 0:\n        return \"原始版权水印未知\"\n    elif len(str1) != len(str2):\n        return \"输入输出水印长度不同\"\n    total_length = len(str1)\n    same_count = sum(1 for x, y in zip(str1, str2) if x == y)\n    similarity_percentage = (same_count / total_length) * 100\n    return f\"{similarity_percentage}%\"\n\n\n\n# Description\ntitle = \"<center><strong><font size='8'>EditGuard<font></strong></center>\"\n\ncss = \"h1 { text-align: center } .about { text-align: justify; padding-left: 10%; padding-right: 10%; }\"\n\nwith gr.Blocks(css=css, title=\"EditGuard\") as demo:\n    gr.HTML(html_content)\n    model = gr.State(value = None)\n    save_h = gr.State(value = None)\n    save_w = gr.State(value = None)\n    sam_global_points = gr.State([])\n    sam_global_point_label = gr.State([])\n    sam_original_image = gr.State(value=None)\n    sam_mask = gr.State(value=None)\n\n    with gr.Tabs():\n        with gr.TabItem('多功能取证水印'):\n\n            DESCRIPTION = \"\"\"\n            ## 使用方法：\n            - 上传图像和版权水印（64位比特序列），点击\"嵌入水印\"按钮，生成带水印的图像。\n            - 涂抹要编辑的区域，并使用Inpainting算法编辑图像。\n            - 点击\"提取\"按钮检测篡改区域并输出版权水印。\"\"\"\n            \n            gr.Markdown(DESCRIPTION)\n            save_inpainted_image = gr.State(value=None)\n            with gr.Column():\n                with gr.Row():\n                    model_list = gr.Dropdown(label=\"选择模型\", choices=[\"模型1\"], type = 'index')\n                    clear_button = gr.Button(\"清除全部\")\n                with gr.Box():\n                    gr.Markdown(\"# 1. 嵌入水印\")\n                    with gr.Row():\n                        with gr.Column():\n                            image_input = gr.Image(source='upload', label=\"原始图片\", interactive=True, type=\"numpy\", value=default_example[0])\n                            with gr.Row():\n                                bit_input = gr.Textbox(label=\"输入版权水印（64位比特序列）\", placeholder=\"在这里输入...\")\n                                rand_bit = gr.Button(\"🎲 随机生成版权水印\")\n                            hiding_button = gr.Button(\"嵌入水印\")\n                        with gr.Column():\n                            image_watermark = gr.Image(source=\"upload\", label=\"带有水印的图片\", interactive=True, type=\"numpy\")\n\n\n                with gr.Box():\n                    gr.Markdown(\"# 2. 篡改图片\")\n                    with gr.Row():\n                        with gr.Column():\n                            image_edit = gr.Image(source='upload',tool=\"sketch\", label=\"选取篡改区域\", interactive=True, type=\"numpy\")\n                            inpainting_model_list = gr.Dropdown(label=\"选择篡改模型\", choices=[\"模型1：SD_inpainting\"], type = 'index')\n                            text_prompt = gr.Textbox(label=\"篡改提示词\")\n                            inpainting_button = gr.Button(\"篡改图片\")\n                        with gr.Column():\n                            image_edited = gr.Image(source=\"upload\", label=\"篡改结果\", interactive=True, type=\"numpy\")\n                \n\n                with gr.Box():\n                    gr.Markdown(\"# 3. 提取水印&篡改区域\")\n                    with gr.Row():\n                        with gr.Column():\n                            image_edited_1 = gr.Image(source=\"upload\", label=\"待提取图片\", interactive=True, type=\"numpy\")\n                            \n                            revealing_button = gr.Button(\"提取\")\n                        with gr.Column():\n                            edit_mask = gr.Image(source='upload', label=\"编辑区域蒙版预测\", interactive=True, type=\"numpy\")\n                            bit_output = gr.Textbox(label=\"版权水印预测\")\n                            acc_output = gr.Textbox(label=\"水印预测准确率\")\n                \n                gr.Examples(\n                            examples=examples,\n                            inputs=[image_input],\n                        )\n\n\n                model_list.change(\n                    imgae_model_select, inputs = [model_list], outputs=[model]\n                    )\n                hiding_button.click(\n                    hiding, inputs=[image_input, bit_input, model], outputs=[image_watermark, image_edit]\n                    )\n                rand_bit.click(\n                    rand, inputs=[], outputs=[bit_input]\n                    )\n\n\n                inpainting_button.click(\n                    ImageEdit, inputs = [image_edit, text_prompt, inpainting_model_list], outputs=[image_edited, image_edited_1, save_inpainted_image]\n                    )\n\n                revealing_button.click(\n                    revealing, inputs=[image_edited_1, bit_input, model_list, model], outputs=[edit_mask, bit_output, acc_output]\n                    )\n\ndemo.launch(server_name=\"0.0.0.0\", server_port=2002, share=True, favicon_path='../logo.png')"}
{"type": "source_file", "path": "code/models/modules/module_util.py", "content": "import torch\nimport torch.nn as nn\nimport torch.nn.init as init\nimport torch.nn.functional as F\n\n\ndef initialize_weights(net_l, scale=1):\n    if not isinstance(net_l, list):\n        net_l = [net_l]\n    for net in net_l:\n        for m in net.modules():\n            if isinstance(m, nn.Conv2d):\n                init.kaiming_normal_(m.weight, a=0, mode='fan_in')\n                m.weight.data *= scale  # for residual block\n                if m.bias is not None:\n                    m.bias.data.zero_()\n            elif isinstance(m, nn.Linear):\n                init.kaiming_normal_(m.weight, a=0, mode='fan_in')\n                m.weight.data *= scale\n                if m.bias is not None:\n                    m.bias.data.zero_()\n            elif isinstance(m, nn.BatchNorm2d):\n                init.constant_(m.weight, 1)\n                init.constant_(m.bias.data, 0.0)\n\n\ndef initialize_weights_xavier(net_l, scale=1):\n    if not isinstance(net_l, list):\n        net_l = [net_l]\n    for net in net_l:\n        for m in net.modules():\n            if isinstance(m, nn.Conv2d):\n                init.xavier_normal_(m.weight)\n                m.weight.data *= scale  # for residual block\n                if m.bias is not None:\n                    m.bias.data.zero_()\n            elif isinstance(m, nn.Linear):\n                init.xavier_normal_(m.weight)\n                m.weight.data *= scale\n                if m.bias is not None:\n                    m.bias.data.zero_()\n            elif isinstance(m, nn.BatchNorm2d):\n                init.constant_(m.weight, 1)\n                init.constant_(m.bias.data, 0.0)\n\n\ndef make_layer(block, n_layers):\n    layers = []\n    for _ in range(n_layers):\n        layers.append(block())\n    return nn.Sequential(*layers)\n\n\nclass ResidualBlock_noBN(nn.Module):\n    '''Residual block w/o BN\n    ---Conv-ReLU-Conv-+-\n     |________________|\n    '''\n\n    def __init__(self, nf=64):\n        super(ResidualBlock_noBN, self).__init__()\n        self.conv1 = nn.Conv2d(nf, nf, 3, 1, 1, bias=True)\n        self.conv2 = nn.Conv2d(nf, nf, 3, 1, 1, bias=True)\n\n        # initialization\n        initialize_weights([self.conv1, self.conv2], 0.1)\n\n    def forward(self, x):\n        identity = x\n        out = F.relu(self.conv1(x), inplace=True)\n        out = self.conv2(out)\n        return identity + out\n\n\ndef flow_warp(x, flow, interp_mode='bilinear', padding_mode='zeros'):\n    \"\"\"Warp an image or feature map with optical flow\n    Args:\n        x (Tensor): size (N, C, H, W)\n        flow (Tensor): size (N, H, W, 2), normal value\n        interp_mode (str): 'nearest' or 'bilinear'\n        padding_mode (str): 'zeros' or 'border' or 'reflection'\n\n    Returns:\n        Tensor: warped image or feature map\n    \"\"\"\n    assert x.size()[-2:] == flow.size()[1:3]\n    B, C, H, W = x.size()\n    # mesh grid\n    grid_y, grid_x = torch.meshgrid(torch.arange(0, H), torch.arange(0, W))\n    grid = torch.stack((grid_x, grid_y), 2).float()  # W(x), H(y), 2\n    grid.requires_grad = False\n    grid = grid.type_as(x)\n    vgrid = grid + flow\n    # scale grid to [-1,1]\n    vgrid_x = 2.0 * vgrid[:, :, :, 0] / max(W - 1, 1) - 1.0\n    vgrid_y = 2.0 * vgrid[:, :, :, 1] / max(H - 1, 1) - 1.0\n    vgrid_scaled = torch.stack((vgrid_x, vgrid_y), dim=3)\n    output = F.grid_sample(x, vgrid_scaled, mode=interp_mode, padding_mode=padding_mode)\n    return output\n"}
{"type": "source_file", "path": "code/models/IBSN.py", "content": "import logging\nfrom collections import OrderedDict\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn.parallel import DataParallel, DistributedDataParallel\n\nimport models.networks as networks\nimport models.lr_scheduler as lr_scheduler\nfrom .base_model import BaseModel\nfrom models.modules.loss import ReconstructionLoss, ReconstructionMsgLoss\nfrom models.modules.Quantization import Quantization\nfrom .modules.common import DWT,IWT\nfrom utils.jpegtest import JpegTest\nfrom utils.JPEG import DiffJPEG\nimport utils.util as util\n\n\nimport numpy as np\nimport random\nimport cv2\nimport time\n\nlogger = logging.getLogger('base')\ndwt=DWT()\niwt=IWT()\n\nfrom diffusers import StableDiffusionInpaintPipeline\nfrom diffusers import StableDiffusionControlNetInpaintPipeline, ControlNetModel, DDIMScheduler\nfrom diffusers import StableDiffusionXLInpaintPipeline\nfrom diffusers.utils import load_image\nfrom diffusers import RePaintPipeline, RePaintScheduler\n\nclass Model_VSN(BaseModel):\n    def __init__(self, opt):\n        super(Model_VSN, self).__init__(opt)\n\n        if opt['dist']:\n            self.rank = torch.distributed.get_rank()\n        else:\n            self.rank = -1  # non dist training\n\n        self.gop = opt['gop']\n        train_opt = opt['train']\n        test_opt = opt['test']\n        self.opt = opt\n        self.train_opt = train_opt\n        self.test_opt = test_opt\n        self.opt_net = opt['network_G']\n        self.center = self.gop // 2\n        self.num_image = opt['num_image']\n        self.mode = opt[\"mode\"]\n        self.idxx = 0\n\n        self.netG = networks.define_G_v2(opt).to(self.device)\n        if opt['dist']:\n            self.netG = DistributedDataParallel(self.netG, device_ids=[torch.cuda.current_device()])\n        else:\n            self.netG = DataParallel(self.netG)\n        # print network\n        self.print_network()\n        self.load()\n\n        self.Quantization = Quantization()\n\n        if not self.opt['hide']:\n            file_path = \"bit_sequence.txt\"\n\n            data_list = []\n\n            with open(file_path, \"r\") as file:\n                for line in file:\n                    data = [int(bit) for bit in line.strip()]\n                    data_list.append(data)\n            \n            self.msg_list = data_list\n\n        if self.opt['sdinpaint']:\n            self.pipe = StableDiffusionInpaintPipeline.from_pretrained(\n                \"stabilityai/stable-diffusion-2-inpainting\",\n                torch_dtype=torch.float16,\n            ).to(\"cuda\")\n        \n        if self.opt['controlnetinpaint']:\n            controlnet = ControlNetModel.from_pretrained(\n                \"lllyasviel/control_v11p_sd15_inpaint\", torch_dtype=torch.float32\n            ).to(\"cuda\")\n            self.pipe_control = StableDiffusionControlNetInpaintPipeline.from_pretrained(\n                \"runwayml/stable-diffusion-v1-5\", controlnet=controlnet, torch_dtype=torch.float32\n            ).to(\"cuda\")\n        \n        if self.opt['sdxl']:\n            self.pipe_sdxl = StableDiffusionXLInpaintPipeline.from_pretrained(\n                \"diffusers/stable-diffusion-xl-1.0-inpainting-0.1\",\n                torch_dtype=torch.float16,\n                variant=\"fp16\",\n                use_safetensors=True,\n            ).to(\"cuda\")\n        \n        if self.opt['repaint']:\n            self.scheduler = RePaintScheduler.from_pretrained(\"google/ddpm-ema-celebahq-256\")\n            self.pipe_repaint = RePaintPipeline.from_pretrained(\"google/ddpm-ema-celebahq-256\", scheduler=self.scheduler)\n            self.pipe_repaint = self.pipe_repaint.to(\"cuda\")\n\n        if self.is_train:\n            self.netG.train()\n\n            # loss\n            self.Reconstruction_forw = ReconstructionLoss(losstype=self.train_opt['pixel_criterion_forw'])\n            self.Reconstruction_back = ReconstructionLoss(losstype=self.train_opt['pixel_criterion_back'])\n            self.Reconstruction_center = ReconstructionLoss(losstype=\"center\")\n            self.Reconstruction_msg = ReconstructionMsgLoss(losstype=self.opt['losstype'])\n\n            # optimizers\n            wd_G = train_opt['weight_decay_G'] if train_opt['weight_decay_G'] else 0\n            optim_params = []\n\n            if self.mode == \"image\":\n                for k, v in self.netG.named_parameters():\n                    if (k.startswith('module.irn') or k.startswith('module.pm')) and v.requires_grad: \n                        optim_params.append(v)\n                    else:\n                        if self.rank <= 0:\n                            logger.warning('Params [{:s}] will not optimize.'.format(k))\n\n            elif self.mode == \"bit\":\n                for k, v in self.netG.named_parameters():\n                    if (k.startswith('module.bitencoder') or k.startswith('module.bitdecoder')) and v.requires_grad:\n                        optim_params.append(v)\n                    else:\n                        if self.rank <= 0:\n                            logger.warning('Params [{:s}] will not optimize.'.format(k))\n\n\n            self.optimizer_G = torch.optim.Adam(optim_params, lr=train_opt['lr_G'],\n                                                weight_decay=wd_G,\n                                                betas=(train_opt['beta1'], train_opt['beta2']))\n            self.optimizers.append(self.optimizer_G)\n\n            # schedulers\n            if train_opt['lr_scheme'] == 'MultiStepLR':\n                for optimizer in self.optimizers:\n                    self.schedulers.append(\n                        lr_scheduler.MultiStepLR_Restart(optimizer, train_opt['lr_steps'],\n                                                         restarts=train_opt['restarts'],\n                                                         weights=train_opt['restart_weights'],\n                                                         gamma=train_opt['lr_gamma'],\n                                                         clear_state=train_opt['clear_state']))\n            elif train_opt['lr_scheme'] == 'CosineAnnealingLR_Restart':\n                for optimizer in self.optimizers:\n                    self.schedulers.append(\n                        lr_scheduler.CosineAnnealingLR_Restart(\n                            optimizer, train_opt['T_period'], eta_min=train_opt['eta_min'],\n                            restarts=train_opt['restarts'], weights=train_opt['restart_weights']))\n            else:\n                raise NotImplementedError('MultiStepLR learning rate scheme is enough.')\n\n            self.log_dict = OrderedDict()\n\n    def feed_data(self, data):\n        self.ref_L = data['LQ'].to(self.device)  \n        self.real_H = data['GT'].to(self.device)\n        # self.mes = data['MES']\n\n    def init_hidden_state(self, z):\n        b, c, h, w = z.shape\n        h_t = []\n        c_t = []\n        for _ in range(self.opt_net['block_num_rbm']):\n            h_t.append(torch.zeros([b, c, h, w]).cuda())\n            c_t.append(torch.zeros([b, c, h, w]).cuda())\n        memory = torch.zeros([b, c, h, w]).cuda()\n\n        return h_t, c_t, memory\n\n    def loss_forward(self, out, y):\n        l_forw_fit = self.train_opt['lambda_fit_forw'] * self.Reconstruction_forw(out, y)\n        return l_forw_fit\n\n    def loss_back_rec(self, out, x):\n        l_back_rec = self.train_opt['lambda_rec_back'] * self.Reconstruction_back(out, x)\n        return l_back_rec\n    \n    def loss_back_rec_mul(self, out, x):\n        l_back_rec = self.train_opt['lambda_rec_back'] * self.Reconstruction_back(out, x)\n        return l_back_rec\n\n    def optimize_parameters(self, current_step):\n        self.optimizer_G.zero_grad()\n      \n        b, n, t, c, h, w = self.ref_L.shape\n        center = t // 2\n        intval = self.gop // 2\n\n        message = torch.Tensor(np.random.choice([-0.5, 0.5], (self.ref_L.shape[0], self.opt['message_length']))).to(self.device)\n\n        add_noise = self.opt['addnoise']\n        add_jpeg = self.opt['addjpeg']\n        add_possion = self.opt['addpossion']\n        add_sdinpaint = self.opt['sdinpaint']\n        degrade_shuffle = self.opt['degrade_shuffle']\n\n        self.host = self.real_H[:, center - intval:center + intval + 1]\n        self.secret = self.ref_L[:, :, center - intval:center + intval + 1]\n        self.output, container = self.netG(x=dwt(self.host.reshape(b, -1, h, w)), x_h=dwt(self.secret[:,0].reshape(b, -1, h, w)), message=message)\n\n        Gt_ref = self.real_H[:, center - intval:center + intval + 1].detach()\n\n        y_forw = container\n\n        l_forw_fit = self.loss_forward(y_forw, self.host[:,0])\n\n\n        if degrade_shuffle:\n            import random\n            choice = random.randint(0, 2)\n            \n            if choice == 0:\n                NL = float((np.random.randint(1, 16))/255)\n                noise = np.random.normal(0, NL, y_forw.shape)\n                torchnoise = torch.from_numpy(noise).cuda().float()\n                y_forw = y_forw + torchnoise\n\n            elif choice == 1:\n                NL = int(np.random.randint(70,95))\n                self.DiffJPEG = DiffJPEG(differentiable=True, quality=int(NL)).cuda()\n                y_forw = self.DiffJPEG(y_forw)\n            \n            elif choice == 2:\n                vals = 10**4\n                if random.random() < 0.5:\n                    noisy_img_tensor = torch.poisson(y_forw * vals) / vals\n                else:\n                    img_gray_tensor = torch.mean(y_forw, dim=0, keepdim=True)\n                    noisy_gray_tensor = torch.poisson(img_gray_tensor * vals) / vals\n                    noisy_img_tensor = y_forw + (noisy_gray_tensor - img_gray_tensor)\n\n                y_forw = torch.clamp(noisy_img_tensor, 0, 1)\n\n        else:\n\n            if add_noise:\n                NL = float((np.random.randint(1,16))/255)\n                noise = np.random.normal(0, NL, y_forw.shape)\n                torchnoise = torch.from_numpy(noise).cuda().float()\n                y_forw = y_forw + torchnoise\n\n            elif add_jpeg:\n                NL = int(np.random.randint(70,95))\n                self.DiffJPEG = DiffJPEG(differentiable=True, quality=int(NL)).cuda()\n                y_forw = self.DiffJPEG(y_forw)\n\n            elif add_possion:\n                vals = 10**4\n                if random.random() < 0.5:\n                    noisy_img_tensor = torch.poisson(y_forw * vals) / vals\n                else:\n                    img_gray_tensor = torch.mean(y_forw, dim=0, keepdim=True)\n                    noisy_gray_tensor = torch.poisson(img_gray_tensor * vals) / vals\n                    noisy_img_tensor = y_forw + (noisy_gray_tensor - img_gray_tensor)\n\n                y_forw = torch.clamp(noisy_img_tensor, 0, 1)\n\n        y = self.Quantization(y_forw)\n        all_zero = torch.zeros(message.shape).to(self.device)\n\n        if self.mode == \"image\":\n            out_x, out_x_h, out_z, recmessage = self.netG(x=y, message=all_zero, rev=True)\n            out_x = iwt(out_x)\n            out_x_h = [iwt(out_x_h_i) for out_x_h_i in out_x_h]\n\n            l_back_rec = self.loss_back_rec(out_x, self.host[:,0])\n            out_x_h = torch.stack(out_x_h, dim=1)\n\n            l_center_x = self.loss_back_rec(out_x_h[:, 0], self.secret[:,0].reshape(b, -1, h, w))\n\n            recmessage = torch.clamp(recmessage, -0.5, 0.5)\n\n            l_msg = self.Reconstruction_msg(message, recmessage)\n\n            loss = l_forw_fit*2 + l_back_rec + l_center_x*4\n\n            loss.backward()\n\n            if self.train_opt['lambda_center'] != 0:\n                self.log_dict['l_center_x'] = l_center_x.item()\n\n            # set log\n            self.log_dict['l_back_rec'] = l_back_rec.item()\n            self.log_dict['l_forw_fit'] = l_forw_fit.item()\n            self.log_dict['l_msg'] = l_msg.item()\n            \n            self.log_dict['l_h'] = (l_center_x*10).item()\n\n            # gradient clipping\n            if self.train_opt['gradient_clipping']:\n                nn.utils.clip_grad_norm_(self.netG.parameters(), self.train_opt['gradient_clipping'])\n\n            self.optimizer_G.step()\n\n        elif self.mode == \"bit\":\n            recmessage = self.netG(x=y, message=all_zero, rev=True)\n\n            recmessage = torch.clamp(recmessage, -0.5, 0.5)\n\n            l_msg = self.Reconstruction_msg(message, recmessage)\n            \n            lambda_msg = self.train_opt['lambda_msg']\n\n            loss = l_msg * lambda_msg + l_forw_fit\n\n            loss.backward()\n\n            # set log\n            self.log_dict['l_forw_fit'] = l_forw_fit.item()\n            self.log_dict['l_msg'] = l_msg.item()\n\n            # gradient clipping\n            if self.train_opt['gradient_clipping']:\n                nn.utils.clip_grad_norm_(self.netG.parameters(), self.train_opt['gradient_clipping'])\n\n            self.optimizer_G.step()\n\n    def test(self, image_id):\n        self.netG.eval()\n        add_noise = self.opt['addnoise']\n        add_jpeg = self.opt['addjpeg']\n        add_possion = self.opt['addpossion']\n        add_sdinpaint = self.opt['sdinpaint']\n        add_controlnet = self.opt['controlnetinpaint']\n        add_sdxl = self.opt['sdxl']\n        add_repaint = self.opt['repaint']\n        degrade_shuffle = self.opt['degrade_shuffle']\n\n        with torch.no_grad():\n            forw_L = []\n            forw_L_h = []\n            fake_H = []\n            fake_H_h = []\n            pred_z = []\n            recmsglist = []\n            msglist = []\n            b, t, c, h, w = self.real_H.shape\n            center = t // 2\n            intval = self.gop // 2\n            b, n, t, c, h, w = self.ref_L.shape\n            id=0\n            # forward downscaling\n            self.host = self.real_H[:, center - intval+id:center + intval + 1+id]\n            self.secret = self.ref_L[:, :, center - intval+id:center + intval + 1+id]\n            self.secret = [dwt(self.secret[:,i].reshape(b, -1, h, w)) for i in range(n)]\n\n            messagenp = np.random.choice([-0.5, 0.5], (self.ref_L.shape[0], self.opt['message_length']))\n\n            message = torch.Tensor(messagenp).to(self.device)\n\n            if self.opt['bitrecord']:\n                mymsg = message.clone()\n\n                mymsg[mymsg>0] = 1\n                mymsg[mymsg<0] = 0\n                mymsg = mymsg.squeeze(0).to(torch.int)\n\n                bit_list = mymsg.tolist()\n\n                bit_string = ''.join(map(str, bit_list))\n\n                file_name = \"bit_sequence.txt\"\n\n                with open(file_name, \"a\") as file:\n                    file.write(bit_string + \"\\n\")\n\n            if self.opt['hide']:\n                self.output, container = self.netG(x=dwt(self.host.reshape(b, -1, h, w)), x_h=self.secret, message=message)\n                y_forw = container\n            else:\n                \n                message = torch.tensor(self.msg_list[image_id]).unsqueeze(0).cuda()\n                self.output = self.host\n                y_forw = self.output.squeeze(1)\n\n            if add_sdinpaint:\n                import random\n                from PIL import Image\n                prompt = \"\"\n\n                b, _, _, _ = y_forw.shape\n                \n                image_batch = y_forw.permute(0, 2, 3, 1).detach().cpu().numpy()\n                forw_list = []\n\n                for j in range(b):\n                    i = image_id + 1\n                    masksrc = \"../dataset/valAGE-Set-Mask/\"\n                    mask_image = Image.open(masksrc + str(i).zfill(4) + \".png\").convert(\"L\")\n                    mask_image = mask_image.resize((512, 512))\n                    h, w = mask_image.size\n                    \n                    image = image_batch[j, :, :, :]\n                    image_init = Image.fromarray((image * 255).astype(np.uint8), mode = \"RGB\")\n                    image_inpaint = self.pipe(prompt=prompt, image=image_init, mask_image=mask_image, height=w, width=h).images[0]\n                    image_inpaint = np.array(image_inpaint) / 255.\n                    mask_image = np.array(mask_image)\n                    mask_image = np.stack([mask_image] * 3, axis=-1) / 255.\n                    mask_image = mask_image.astype(np.uint8)\n                    image_fuse = image * (1 - mask_image) + image_inpaint * mask_image\n                    forw_list.append(torch.from_numpy(image_fuse).permute(2, 0, 1))\n                \n                y_forw = torch.stack(forw_list, dim=0).float().cuda()\n\n            if add_controlnet:\n                from diffusers.utils import load_image\n                from PIL import Image\n\n                b, _, _, _ = y_forw.shape\n                forw_list = []\n                \n                image_batch = y_forw.permute(0, 2, 3, 1).detach().cpu().numpy()\n                generator = torch.Generator(device=\"cuda\").manual_seed(1)\n\n                for j in range(b):\n                    i = image_id + 1\n                    mask_path = \"../dataset/valAGE-Set-Mask/\" + str(i).zfill(4) + \".png\"\n                    mask_image = load_image(mask_path)\n                    mask_image = mask_image.resize((512, 512))\n                    image_init = image_batch[j, :, :, :]\n                    image_init1 = Image.fromarray((image_init * 255).astype(np.uint8), mode = \"RGB\")\n                    image_mask = np.array(mask_image.convert(\"L\")).astype(np.float32) / 255.0\n\n                    assert image_init.shape[0:1] == image_mask.shape[0:1], \"image and image_mask must have the same image size\"\n                    image_init[image_mask > 0.5] = -1.0  # set as masked pixel\n                    image = np.expand_dims(image_init, 0).transpose(0, 3, 1, 2)\n                    control_image = torch.from_numpy(image)\n\n                    # generate image\n                    image_inpaint = self.pipe_control(\n                        \"\",\n                        num_inference_steps=20,\n                        generator=generator,\n                        eta=1.0,\n                        image=image_init1,\n                        mask_image=image_mask,\n                        control_image=control_image,\n                    ).images[0]\n                    \n                    image_inpaint = np.array(image_inpaint) / 255.\n                    image_mask = np.stack([image_mask] * 3, axis=-1)\n                    image_mask = image_mask.astype(np.uint8)\n                    image_fuse = image_init * (1 - image_mask) + image_inpaint * image_mask\n                    forw_list.append(torch.from_numpy(image_fuse).permute(2, 0, 1))\n\n                y_forw = torch.stack(forw_list, dim=0).float().cuda()\n            \n            if add_sdxl:\n                import random\n                from PIL import Image\n                from diffusers.utils import load_image\n                prompt = \"\"\n\n                b, _, _, _ = y_forw.shape\n                \n                image_batch = y_forw.permute(0, 2, 3, 1).detach().cpu().numpy()\n                forw_list = []\n\n                for j in range(b):\n                    i = image_id + 1\n                    masksrc = \"../dataset/valAGE-Set-Mask/\"\n                    mask_image = load_image(masksrc + str(i).zfill(4) + \".png\").convert(\"RGB\")\n                    mask_image = mask_image.resize((512, 512))\n                    h, w = mask_image.size\n                    \n                    image = image_batch[j, :, :, :]\n                    image_init = Image.fromarray((image * 255).astype(np.uint8), mode = \"RGB\")\n                    image_inpaint = self.pipe_sdxl(\n                        prompt=prompt, image=image_init, mask_image=mask_image, num_inference_steps=50, strength=0.80, target_size=(512, 512)\n                    ).images[0]\n                    image_inpaint = image_inpaint.resize((512, 512))\n                    image_inpaint = np.array(image_inpaint) / 255.\n                    mask_image = np.array(mask_image) / 255.\n                    mask_image = mask_image.astype(np.uint8)\n                    image_fuse = image * (1 - mask_image) + image_inpaint * mask_image\n                    forw_list.append(torch.from_numpy(image_fuse).permute(2, 0, 1))\n                \n                y_forw = torch.stack(forw_list, dim=0).float().cuda()\n\n            \n            if add_repaint:\n                from PIL import Image\n                \n                b, _, _, _ = y_forw.shape\n                \n                image_batch = y_forw.permute(0, 2, 3, 1).detach().cpu().numpy()\n                forw_list = []\n\n                generator = torch.Generator(device=\"cuda\").manual_seed(0)\n                for j in range(b):\n                    i = image_id + 1\n                    masksrc = \"../dataset/valAGE-Set-Mask/\" + str(i).zfill(4) + \".png\"\n                    mask_image = Image.open(masksrc).convert(\"RGB\")\n                    mask_image = mask_image.resize((256, 256))\n                    mask_image = Image.fromarray(255 - np.array(mask_image))\n                    image = image_batch[j, :, :, :]\n                    original_image = Image.fromarray((image * 255).astype(np.uint8), mode = \"RGB\")\n                    original_image = original_image.resize((256, 256))\n                    output = self.pipe_repaint(\n                        image=original_image,\n                        mask_image=mask_image,\n                        num_inference_steps=150,\n                        eta=0.0,\n                        jump_length=10,\n                        jump_n_sample=10,\n                        generator=generator,\n                    )\n                    image_inpaint = output.images[0]\n                    image_inpaint = image_inpaint.resize((512, 512))\n                    image_inpaint = np.array(image_inpaint) / 255.\n                    mask_image = mask_image.resize((512, 512))\n                    mask_image = np.array(mask_image) / 255.\n                    mask_image = mask_image.astype(np.uint8)\n                    image_fuse = image * mask_image + image_inpaint * (1 - mask_image)\n                    forw_list.append(torch.from_numpy(image_fuse).permute(2, 0, 1))\n                \n                y_forw = torch.stack(forw_list, dim=0).float().cuda()\n\n            if degrade_shuffle:\n                import random\n                choice = random.randint(0, 2)\n                \n                if choice == 0:\n                    NL = float((np.random.randint(1,5))/255)\n                    noise = np.random.normal(0, NL, y_forw.shape)\n                    torchnoise = torch.from_numpy(noise).cuda().float()\n                    y_forw = y_forw + torchnoise\n\n                elif choice == 1:\n                    NL = 90\n                    self.DiffJPEG = DiffJPEG(differentiable=True, quality=int(NL)).cuda()\n                    y_forw = self.DiffJPEG(y_forw)\n                \n                elif choice == 2:\n                    vals = 10**4\n                    if random.random() < 0.5:\n                        noisy_img_tensor = torch.poisson(y_forw * vals) / vals\n                    else:\n                        img_gray_tensor = torch.mean(y_forw, dim=0, keepdim=True)\n                        noisy_gray_tensor = torch.poisson(img_gray_tensor * vals) / vals\n                        noisy_img_tensor = y_forw + (noisy_gray_tensor - img_gray_tensor)\n\n                    y_forw = torch.clamp(noisy_img_tensor, 0, 1)\n\n            else:\n\n                if add_noise:\n                    NL = self.opt['noisesigma'] / 255.0\n                    noise = np.random.normal(0, NL, y_forw.shape)\n                    torchnoise = torch.from_numpy(noise).cuda().float()\n                    y_forw = y_forw + torchnoise\n\n                elif add_jpeg:\n                    Q = self.opt['jpegfactor']\n                    self.DiffJPEG = DiffJPEG(differentiable=True, quality=int(Q)).cuda()\n                    y_forw = self.DiffJPEG(y_forw)\n\n                elif add_possion:\n                    vals = 10**4\n                    if random.random() < 0.5:\n                        noisy_img_tensor = torch.poisson(y_forw * vals) / vals\n                    else:\n                        img_gray_tensor = torch.mean(y_forw, dim=0, keepdim=True)\n                        noisy_gray_tensor = torch.poisson(img_gray_tensor * vals) / vals\n                        noisy_img_tensor = y_forw + (noisy_gray_tensor - img_gray_tensor)\n\n                    y_forw = torch.clamp(noisy_img_tensor, 0, 1)\n\n            # backward upscaling\n            if self.opt['hide']:\n                y = self.Quantization(y_forw)\n            else:\n                y = y_forw\n\n            if self.mode == \"image\":\n                out_x, out_x_h, out_z, recmessage = self.netG(x=y, rev=True)\n                out_x = iwt(out_x)\n\n                out_x_h = [iwt(out_x_h_i) for out_x_h_i in out_x_h]\n                out_x = out_x.reshape(-1, self.gop, 3, h, w)\n                out_x_h = torch.stack(out_x_h, dim=1)\n                out_x_h = out_x_h.reshape(-1, 1, self.gop, 3, h, w)\n\n                forw_L.append(y_forw)\n                fake_H.append(out_x[:, self.gop//2])\n                fake_H_h.append(out_x_h[:,:, self.gop//2])\n                recmsglist.append(recmessage)\n                msglist.append(message)\n            \n            elif self.mode == \"bit\":\n                recmessage = self.netG(x=y, rev=True)\n                forw_L.append(y_forw)\n                recmsglist.append(recmessage)\n                msglist.append(message)\n\n        if self.mode == \"image\":\n            self.fake_H = torch.clamp(torch.stack(fake_H, dim=1),0,1)\n            self.fake_H_h = torch.clamp(torch.stack(fake_H_h, dim=2),0,1)\n\n        self.forw_L = torch.clamp(torch.stack(forw_L, dim=1),0,1)\n        remesg = torch.clamp(torch.stack(recmsglist, dim=0),-0.5,0.5)\n\n        if self.opt['hide']:\n            mesg = torch.clamp(torch.stack(msglist, dim=0),-0.5,0.5)\n        else:\n            mesg = torch.stack(msglist, dim=0)\n\n        self.recmessage = remesg.clone()\n        self.recmessage[remesg > 0] = 1\n        self.recmessage[remesg <= 0] = 0\n\n        self.message = mesg.clone()\n        self.message[mesg > 0] = 1\n        self.message[mesg <= 0] = 0\n\n        self.netG.train()\n\n\n    def image_hiding(self, ):\n        self.netG.eval()\n        with torch.no_grad():\n            b, t, c, h, w = self.real_H.shape\n            center = t // 2\n            intval = self.gop // 2\n            b, n, t, c, h, w = self.ref_L.shape\n            id=0\n            # forward downscaling\n            self.host = self.real_H[:, center - intval+id:center + intval + 1+id]\n            self.secret = self.ref_L[:, :, center - intval+id:center + intval + 1+id]\n            self.secret = [dwt(self.secret[:,i].reshape(b, -1, h, w)) for i in range(n)]\n\n            message = torch.Tensor(self.mes).to(self.device)\n\n            self.output, container = self.netG(x=dwt(self.host.reshape(b, -1, h, w)), x_h=self.secret, message=message)\n            y_forw = container\n\n            result = torch.clamp(y_forw,0,1)\n\n            lr_img = util.tensor2img(result)\n\n            return lr_img\n\n    def image_recovery(self, number):\n        self.netG.eval()\n        with torch.no_grad():\n            b, t, c, h, w = self.real_H.shape\n            center = t // 2\n            intval = self.gop // 2\n            b, n, t, c, h, w = self.ref_L.shape\n            id=0\n            # forward downscaling\n            self.host = self.real_H[:, center - intval+id:center + intval + 1+id]\n            self.secret = self.ref_L[:, :, center - intval+id:center + intval + 1+id]\n            template = self.secret.reshape(b, -1, h, w)\n            self.secret = [dwt(self.secret[:,i].reshape(b, -1, h, w)) for i in range(n)]\n\n            self.output = self.host\n            y_forw = self.output.squeeze(1)\n\n            y = self.Quantization(y_forw)\n\n            out_x, out_x_h, out_z, recmessage = self.netG(x=y, rev=True)\n            out_x = iwt(out_x)\n\n            out_x_h = [iwt(out_x_h_i) for out_x_h_i in out_x_h]\n            out_x = out_x.reshape(-1, self.gop, 3, h, w)\n            out_x_h = torch.stack(out_x_h, dim=1)\n            out_x_h = out_x_h.reshape(-1, 1, self.gop, 3, h, w)\n\n            rec_loc = out_x_h[:,:, self.gop//2]\n            # from PIL import Image\n            # tmp = util.tensor2img(rec_loc)\n            # save\n            residual = torch.abs(template - rec_loc)\n            binary_residual = (residual > number).float()\n            residual = util.tensor2img(binary_residual)\n            mask = np.sum(residual, axis=2)\n            # print(mask)\n\n            remesg = torch.clamp(recmessage,-0.5,0.5)\n            remesg[remesg > 0] = 1\n            remesg[remesg <= 0] = 0\n\n            return mask, remesg\n        \n    def get_current_log(self):\n        return self.log_dict\n\n    def get_current_visuals(self):\n        b, n, t, c, h, w = self.ref_L.shape\n        center = t // 2\n        intval = self.gop // 2\n        out_dict = OrderedDict()\n        LR_ref = self.ref_L[:, :, center - intval:center + intval + 1].detach()[0].float().cpu()\n        LR_ref = torch.chunk(LR_ref, self.num_image, dim=0)\n        out_dict['LR_ref'] = [image.squeeze(0) for image in LR_ref]\n        \n        if self.mode == \"image\":\n            out_dict['SR'] = self.fake_H.detach()[0].float().cpu()\n            SR_h = self.fake_H_h.detach()[0].float().cpu()\n            SR_h = torch.chunk(SR_h, self.num_image, dim=0)\n            out_dict['SR_h'] = [image.squeeze(0) for image in SR_h]\n        \n        out_dict['LR'] = self.forw_L.detach()[0].float().cpu()\n        out_dict['GT'] = self.real_H[:, center - intval:center + intval + 1].detach()[0].float().cpu()\n        out_dict['message'] = self.message\n        out_dict['recmessage'] = self.recmessage\n\n        return out_dict\n\n    def print_network(self):\n        s, n = self.get_network_description(self.netG)\n        if isinstance(self.netG, nn.DataParallel) or isinstance(self.netG, DistributedDataParallel):\n            net_struc_str = '{} - {}'.format(self.netG.__class__.__name__,\n                                             self.netG.module.__class__.__name__)\n        else:\n            net_struc_str = '{}'.format(self.netG.__class__.__name__)\n        if self.rank <= 0:\n            logger.info('Network G structure: {}, with parameters: {:,d}'.format(net_struc_str, n))\n            logger.info(s)\n\n    def load(self):\n        load_path_G = self.opt['path']['pretrain_model_G']\n        if load_path_G is not None:\n            logger.info('Loading model for G [{:s}] ...'.format(load_path_G))\n            self.load_network(load_path_G, self.netG, self.opt['path']['strict_load'])\n    \n    def load_test(self,load_path_G):\n        self.load_network(load_path_G, self.netG, self.opt['path']['strict_load'])\n\n    def save(self, iter_label):\n        self.save_network(self.netG, 'G', iter_label)\n"}
{"type": "source_file", "path": "code/data/data_sampler.py", "content": "\"\"\"\nModified from torch.utils.data.distributed.DistributedSampler\nSupport enlarging the dataset for *iter-oriented* training, for saving time when restart the\ndataloader after each epoch\n\"\"\"\nimport math\nimport torch\nfrom torch.utils.data.sampler import Sampler\nimport torch.distributed as dist\n\n\nclass DistIterSampler(Sampler):\n    \"\"\"Sampler that restricts data loading to a subset of the dataset.\n\n    It is especially useful in conjunction with\n    :class:`torch.nn.parallel.DistributedDataParallel`. In such case, each\n    process can pass a DistributedSampler instance as a DataLoader sampler,\n    and load a subset of the original dataset that is exclusive to it.\n\n    .. note::\n        Dataset is assumed to be of constant size.\n\n    Arguments:\n        dataset: Dataset used for sampling.\n        num_replicas (optional): Number of processes participating in\n            distributed training.\n        rank (optional): Rank of the current process within num_replicas.\n    \"\"\"\n\n    def __init__(self, dataset, num_replicas=None, rank=None, ratio=100):\n        if num_replicas is None:\n            if not dist.is_available():\n                raise RuntimeError(\"Requires distributed package to be available\")\n            num_replicas = dist.get_world_size()\n        if rank is None:\n            if not dist.is_available():\n                raise RuntimeError(\"Requires distributed package to be available\")\n            rank = dist.get_rank()\n        self.dataset = dataset\n        self.num_replicas = num_replicas\n        self.rank = rank\n        self.epoch = 0\n        self.num_samples = int(math.ceil(len(self.dataset) * ratio / self.num_replicas))\n        self.total_size = self.num_samples * self.num_replicas\n\n    def __iter__(self):\n        # deterministically shuffle based on epoch\n        g = torch.Generator()\n        g.manual_seed(self.epoch)\n        indices = torch.randperm(self.total_size, generator=g).tolist()\n\n        dsize = len(self.dataset)\n        indices = [v % dsize for v in indices]\n\n        # subsample\n        indices = indices[self.rank:self.total_size:self.num_replicas]\n        assert len(indices) == self.num_samples\n\n        return iter(indices)\n\n    def __len__(self):\n        return self.num_samples\n\n    def set_epoch(self, epoch):\n        self.epoch = epoch\n"}
{"type": "source_file", "path": "code/models/bitnetwork/Encoder_U.py", "content": "from . import *\n\nclass DW_Encoder(nn.Module):\n\n    def __init__(self, message_length, blocks=2, channels=64, attention=None):\n        super(DW_Encoder, self).__init__()\n\n        self.conv1 = ConvBlock(3, 16, blocks=blocks)\n        self.down1 = Down(16, 32, blocks=blocks)\n        self.down2 = Down(32, 64, blocks=blocks)\n        self.down3 = Down(64, 128, blocks=blocks)\n\n        self.down4 = Down(128, 256, blocks=blocks)\n\n        self.up3 = UP(256, 128)\n        self.linear3 = nn.Linear(message_length, message_length * message_length)\n        self.Conv_message3 = ConvBlock(1, channels, blocks=blocks)\n        self.att3 = ResBlock(128 * 2 + channels, 128, blocks=blocks, attention=attention)\n\n        self.up2 = UP(128, 64)\n        self.linear2 = nn.Linear(message_length, message_length * message_length)\n        self.Conv_message2 = ConvBlock(1, channels, blocks=blocks)\n        self.att2 = ResBlock(64 * 2 + channels, 64, blocks=blocks, attention=attention)\n\n        self.up1 = UP(64, 32)\n        self.linear1 = nn.Linear(message_length, message_length * message_length)\n        self.Conv_message1 = ConvBlock(1, channels, blocks=blocks)\n        self.att1 = ResBlock(32 * 2 + channels, 32, blocks=blocks, attention=attention)\n\n        self.up0 = UP(32, 16)\n        self.linear0 = nn.Linear(message_length, message_length * message_length)\n        self.Conv_message0 = ConvBlock(1, channels, blocks=blocks)\n        self.att0 = ResBlock(16 * 2 + channels, 16, blocks=blocks, attention=attention)\n\n        self.Conv_1x1 = nn.Conv2d(16 + 3, 3, kernel_size=1, stride=1, padding=0)\n\n        self.message_length = message_length\n\n        self.transform = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n        ])\n\n\n    def forward(self, x, watermark):\n        d0 = self.conv1(x)\n        d1 = self.down1(d0)\n        d2 = self.down2(d1)\n        d3 = self.down3(d2)\n\n        d4 = self.down4(d3)\n\n        u3 = self.up3(d4)\n        expanded_message = self.linear3(watermark)\n        expanded_message = expanded_message.view(-1, 1, self.message_length, self.message_length)\n        expanded_message = F.interpolate(expanded_message, size=(d3.shape[2], d3.shape[3]),\n                                                           mode='nearest')\n        expanded_message = self.Conv_message3(expanded_message)\n        u3 = torch.cat((d3, u3, expanded_message), dim=1)\n        u3 = self.att3(u3)\n\n        u2 = self.up2(u3)\n        expanded_message = self.linear2(watermark)\n        expanded_message = expanded_message.view(-1, 1, self.message_length, self.message_length)\n        expanded_message = F.interpolate(expanded_message, size=(d2.shape[2], d2.shape[3]),\n                                                           mode='nearest')\n        expanded_message = self.Conv_message2(expanded_message)\n        u2 = torch.cat((d2, u2, expanded_message), dim=1)\n        u2 = self.att2(u2)\n\n        u1 = self.up1(u2)\n        expanded_message = self.linear1(watermark)\n        expanded_message = expanded_message.view(-1, 1, self.message_length, self.message_length)\n        expanded_message = F.interpolate(expanded_message, size=(d1.shape[2], d1.shape[3]),\n                                                           mode='nearest')\n        expanded_message = self.Conv_message1(expanded_message)\n        u1 = torch.cat((d1, u1, expanded_message), dim=1)\n        u1 = self.att1(u1)\n\n        u0 = self.up0(u1)\n        expanded_message = self.linear0(watermark)\n        expanded_message = expanded_message.view(-1, 1, self.message_length, self.message_length)\n        expanded_message = F.interpolate(expanded_message, size=(d0.shape[2], d0.shape[3]),\n                                                           mode='nearest')\n        expanded_message = self.Conv_message0(expanded_message)\n        u0 = torch.cat((d0, u0, expanded_message), dim=1)\n        u0 = self.att0(u0)\n\n        image = self.Conv_1x1(torch.cat((x, u0), dim=1))\n\n        forward_image = image.clone().detach()\n        '''read_image = torch.zeros_like(forward_image)\n\n        for index in range(forward_image.shape[0]):\n            single_image = ((forward_image[index].clamp(-1, 1).permute(1, 2, 0) + 1) / 2 * 255).add(0.5).clamp(0, 255).to('cpu', torch.uint8).numpy()\n            im = Image.fromarray(single_image)\n            read = np.array(im, dtype=np.uint8)\n            read_image[index] = self.transform(read).unsqueeze(0).to(image.device)\n\n        gap = read_image - forward_image'''\n        gap = forward_image.clamp(0, 1) - forward_image\n\n        return image + gap\n\n\nclass Down(nn.Module):\n    def __init__(self, in_channels, out_channels, blocks):\n        super(Down, self).__init__()\n        self.layer = torch.nn.Sequential(\n            ConvBlock(in_channels, in_channels, stride=2),\n            ConvBlock(in_channels, out_channels, blocks=blocks)\n        )\n\n    def forward(self, x):\n        return self.layer(x)\n\n\nclass UP(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(UP, self).__init__()\n        self.conv = ConvBlock(in_channels, out_channels)\n\n    def forward(self, x):\n        x = F.interpolate(x, scale_factor=2, mode='nearest')\n        return self.conv(x)\n"}
{"type": "source_file", "path": "code/models/modules/Quantization.py", "content": "import torch\nimport torch.nn as nn\n\nclass Quant(torch.autograd.Function):\n\n    @staticmethod\n    def forward(ctx, input):\n        input = torch.clamp(input, 0, 1)\n        output = (input * 255.).round() / 255.\n        return output\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        return grad_output\n\nclass Quantization(nn.Module):\n    def __init__(self):\n        super(Quantization, self).__init__()\n\n    def forward(self, input):\n        return Quant.apply(input)\n"}
{"type": "source_file", "path": "code/models/modules/Inv_arch.py", "content": "import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom .module_util import initialize_weights_xavier\nfrom torch.nn import init\nfrom .common import DWT,IWT\nimport cv2\nfrom basicsr.archs.arch_util import flow_warp\nfrom models.modules.Subnet_constructor import subnet\nimport numpy as np\n\nfrom pdb import set_trace as stx\nimport numbers\n\nfrom einops import rearrange\nfrom models.bitnetwork.Encoder_U import DW_Encoder\nfrom models.bitnetwork.Decoder_U import DW_Decoder\n\n\n## Layer Norm\ndef to_3d(x):\n    return rearrange(x, 'b c h w -> b (h w) c')\n\n\ndef to_4d(x, h, w):\n    return rearrange(x, 'b (h w) c -> b c h w', h=h, w=w)\n\n\nclass BiasFree_LayerNorm(nn.Module):\n    def __init__(self, normalized_shape):\n        super(BiasFree_LayerNorm, self).__init__()\n        if isinstance(normalized_shape, numbers.Integral):\n            normalized_shape = (normalized_shape,)\n        normalized_shape = torch.Size(normalized_shape)\n\n        assert len(normalized_shape) == 1\n\n        self.weight = nn.Parameter(torch.ones(normalized_shape))\n        self.normalized_shape = normalized_shape\n\n    def forward(self, x):\n        sigma = x.var(-1, keepdim=True, unbiased=False)\n        return x / torch.sqrt(sigma + 1e-5) * self.weight\n\n\nclass WithBias_LayerNorm(nn.Module):\n    def __init__(self, normalized_shape):\n        super(WithBias_LayerNorm, self).__init__()\n        if isinstance(normalized_shape, numbers.Integral):\n            normalized_shape = (normalized_shape,)\n        normalized_shape = torch.Size(normalized_shape)\n\n        assert len(normalized_shape) == 1\n\n        self.weight = nn.Parameter(torch.ones(normalized_shape))\n        self.bias = nn.Parameter(torch.zeros(normalized_shape))\n        self.normalized_shape = normalized_shape\n\n    def forward(self, x):\n        mu = x.mean(-1, keepdim=True)\n        sigma = x.var(-1, keepdim=True, unbiased=False)\n        return (x - mu) / torch.sqrt(sigma + 1e-5) * self.weight + self.bias\n\n\nclass LayerNorm(nn.Module):\n    def __init__(self, dim, LayerNorm_type):\n        super(LayerNorm, self).__init__()\n        if LayerNorm_type == 'BiasFree':\n            self.body = BiasFree_LayerNorm(dim)\n        else:\n            self.body = WithBias_LayerNorm(dim)\n\n    def forward(self, x):\n        h, w = x.shape[-2:]\n        return to_4d(self.body(to_3d(x)), h, w)\n\n\n##########################################################################\n## Gated-Dconv Feed-Forward Network (GDFN)\nclass FeedForward(nn.Module):\n    def __init__(self, dim, ffn_expansion_factor, bias):\n        super(FeedForward, self).__init__()\n\n        hidden_features = int(dim * ffn_expansion_factor)\n\n        self.project_in = nn.Conv2d(dim, hidden_features * 2, kernel_size=1, bias=bias)\n\n        self.dwconv = nn.Conv2d(hidden_features * 2, hidden_features * 2, kernel_size=3, stride=1, padding=1,\n                                groups=hidden_features * 2, bias=bias)\n\n        self.project_out = nn.Conv2d(hidden_features, dim, kernel_size=1, bias=bias)\n\n    def forward(self, x):\n        x = self.project_in(x)\n        x1, x2 = self.dwconv(x).chunk(2, dim=1)\n        x = F.gelu(x1) * x2\n        x = self.project_out(x)\n        return x\n\n\n##########################################################################\n## Multi-DConv Head Transposed Self-Attention (MDTA)\nclass Attention(nn.Module):\n    def __init__(self, dim, num_heads, bias):\n        super(Attention, self).__init__()\n        self.num_heads = num_heads\n        self.temperature = nn.Parameter(torch.ones(num_heads, 1, 1))\n\n        self.qkv = nn.Conv2d(dim, dim * 3, kernel_size=1, bias=bias)\n        self.qkv_dwconv = nn.Conv2d(dim * 3, dim * 3, kernel_size=3, stride=1, padding=1, groups=dim * 3, bias=bias)\n        self.project_out = nn.Conv2d(dim, dim, kernel_size=1, bias=bias)\n\n    def forward(self, x):\n        b, c, h, w = x.shape\n\n        qkv = self.qkv_dwconv(self.qkv(x))\n        q, k, v = qkv.chunk(3, dim=1)\n\n        q = rearrange(q, 'b (head c) h w -> b head c (h w)', head=self.num_heads)\n        k = rearrange(k, 'b (head c) h w -> b head c (h w)', head=self.num_heads)\n        v = rearrange(v, 'b (head c) h w -> b head c (h w)', head=self.num_heads)\n\n        q = torch.nn.functional.normalize(q, dim=-1)\n        k = torch.nn.functional.normalize(k, dim=-1)\n\n        attn = (q @ k.transpose(-2, -1)) * self.temperature\n        attn = attn.softmax(dim=-1)\n\n        out = (attn @ v)\n\n        out = rearrange(out, 'b head c (h w) -> b (head c) h w', head=self.num_heads, h=h, w=w)\n\n        out = self.project_out(out)\n        return out\n\n\n##########################################################################\nclass TransformerBlock(nn.Module):\n    def __init__(self, dim, num_heads=4, ffn_expansion_factor=4, bias=False, LayerNorm_type=\"withbias\"):\n        super(TransformerBlock, self).__init__()\n\n        self.norm1 = LayerNorm(dim, LayerNorm_type)\n        self.attn = Attention(dim, num_heads, bias)\n        self.norm2 = LayerNorm(dim, LayerNorm_type)\n        self.ffn = FeedForward(dim, ffn_expansion_factor, bias)\n\n    def forward(self, x):\n        x = x + self.attn(self.norm1(x))\n        x = x + self.ffn(self.norm2(x))\n\n        return x\n\ndwt=DWT()\niwt=IWT()\n\nclass LayerNormFunction(torch.autograd.Function):\n\n    @staticmethod\n    def forward(ctx, x, weight, bias, eps):\n        ctx.eps = eps\n        N, C, H, W = x.size()\n        mu = x.mean(1, keepdim=True)\n        var = (x - mu).pow(2).mean(1, keepdim=True)\n        y = (x - mu) / (var + eps).sqrt()\n        ctx.save_for_backward(y, var, weight)\n        y = weight.view(1, C, 1, 1) * y + bias.view(1, C, 1, 1)\n        return y\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        eps = ctx.eps\n\n        N, C, H, W = grad_output.size()\n        y, var, weight = ctx.saved_variables\n        g = grad_output * weight.view(1, C, 1, 1)\n        mean_g = g.mean(dim=1, keepdim=True)\n\n        mean_gy = (g * y).mean(dim=1, keepdim=True)\n        gx = 1. / torch.sqrt(var + eps) * (g - y * mean_gy - mean_g)\n        return gx, (grad_output * y).sum(dim=3).sum(dim=2).sum(dim=0), grad_output.sum(dim=3).sum(dim=2).sum(\n            dim=0), None\n\nclass LayerNorm2d(nn.Module):\n\n    def __init__(self, channels, eps=1e-6):\n        super(LayerNorm2d, self).__init__()\n        self.register_parameter('weight', nn.Parameter(torch.ones(channels)))\n        self.register_parameter('bias', nn.Parameter(torch.zeros(channels)))\n        self.eps = eps\n\n    def forward(self, x):\n        return LayerNormFunction.apply(x, self.weight, self.bias, self.eps)\n\nclass SimpleGate(nn.Module):\n    def forward(self, x):\n        x1, x2 = x.chunk(2, dim=1)\n        return x1 * x2\n\nclass NAFBlock(nn.Module):\n    def __init__(self, c, DW_Expand=2, FFN_Expand=2, drop_out_rate=0.):\n        super().__init__()\n        dw_channel = c * DW_Expand\n        self.conv1 = nn.Conv2d(in_channels=c, out_channels=dw_channel, kernel_size=1, padding=0, stride=1, groups=1, bias=True)\n        self.conv2 = nn.Conv2d(in_channels=dw_channel, out_channels=dw_channel, kernel_size=3, padding=1, stride=1, groups=dw_channel,\n                               bias=True)\n        self.conv3 = nn.Conv2d(in_channels=dw_channel // 2, out_channels=c, kernel_size=1, padding=0, stride=1, groups=1, bias=True)\n        \n        # Simplified Channel Attention\n        self.sca = nn.Sequential(\n            nn.AdaptiveAvgPool2d(1),\n            nn.Conv2d(in_channels=dw_channel // 2, out_channels=dw_channel // 2, kernel_size=1, padding=0, stride=1,\n                      groups=1, bias=True),\n        )\n\n        # SimpleGate\n        self.sg = SimpleGate()\n\n        ffn_channel = FFN_Expand * c\n        self.conv4 = nn.Conv2d(in_channels=c, out_channels=ffn_channel, kernel_size=1, padding=0, stride=1, groups=1, bias=True)\n        self.conv5 = nn.Conv2d(in_channels=ffn_channel // 2, out_channels=c, kernel_size=1, padding=0, stride=1, groups=1, bias=True)\n\n        self.norm1 = LayerNorm2d(c)\n        self.norm2 = LayerNorm2d(c)\n\n        self.dropout1 = nn.Dropout(drop_out_rate) if drop_out_rate > 0. else nn.Identity()\n        self.dropout2 = nn.Dropout(drop_out_rate) if drop_out_rate > 0. else nn.Identity()\n\n        self.beta = nn.Parameter(torch.zeros((1, c, 1, 1)), requires_grad=True)\n        self.gamma = nn.Parameter(torch.zeros((1, c, 1, 1)), requires_grad=True)\n\n    def forward(self, inp):\n        x = inp\n\n        x = self.norm1(x)\n\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = self.sg(x)\n        x = x * self.sca(x)\n        x = self.conv3(x)\n\n        x = self.dropout1(x)\n\n        y = inp + x * self.beta\n\n        x = self.conv4(self.norm2(y))\n        x = self.sg(x)\n        x = self.conv5(x)\n\n        x = self.dropout2(x)\n\n        return y + x * self.gamma\n\ndef thops_mean(tensor, dim=None, keepdim=False):\n    if dim is None:\n        # mean all dim\n        return torch.mean(tensor)\n    else:\n        if isinstance(dim, int):\n            dim = [dim]\n        dim = sorted(dim)\n        for d in dim:\n            tensor = tensor.mean(dim=d, keepdim=True)\n        if not keepdim:\n            for i, d in enumerate(dim):\n                tensor.squeeze_(d-i)\n        return tensor\n\n\nclass ResidualBlockNoBN(nn.Module):\n    def __init__(self, nf=64, model='MIMO-VRN'):\n        super(ResidualBlockNoBN, self).__init__()\n        self.conv1 = nn.Conv2d(nf, nf, 3, 1, 1, bias=True)\n        self.conv2 = nn.Conv2d(nf, nf, 3, 1, 1, bias=True)\n        # honestly, there's no significant difference between ReLU and leaky ReLU in terms of performance here\n        # but this is how we trained the model in the first place and what we reported in the paper\n        if model == 'LSTM-VRN':\n            self.relu = nn.ReLU(inplace=True)\n        elif model == 'MIMO-VRN':\n            self.relu = nn.LeakyReLU(negative_slope=0.2, inplace=True)\n\n        # initialization\n        initialize_weights_xavier([self.conv1, self.conv2], 0.1)\n\n    def forward(self, x):\n        identity = x\n        out = self.relu(self.conv1(x))\n        out = self.conv2(out)\n        return identity + out\n\n\nclass InvBlock(nn.Module):\n    def __init__(self, subnet_constructor, subnet_constructor_v2, channel_num_ho, channel_num_hi, groups, clamp=1.):\n        super(InvBlock, self).__init__()\n        self.split_len1 = channel_num_ho  # channel_split_num\n        self.split_len2 = channel_num_hi  # channel_num - channel_split_num\n        self.clamp = clamp\n\n        self.F = subnet_constructor_v2(self.split_len2, self.split_len1, groups=groups)\n        self.NF = NAFBlock(self.split_len2)\n        if groups == 1: \n            self.G = subnet_constructor(self.split_len1, self.split_len2, groups=groups)\n            self.NG = NAFBlock(self.split_len1)\n            self.H = subnet_constructor(self.split_len1, self.split_len2, groups=groups)\n            self.NH = NAFBlock(self.split_len1)\n        else:\n            self.G = subnet_constructor(self.split_len1, self.split_len2)\n            self.NG = NAFBlock(self.split_len1)\n            self.H = subnet_constructor(self.split_len1, self.split_len2)\n            self.NH = NAFBlock(self.split_len1)\n\n    def forward(self, x1, x2, rev=False):\n        if not rev:\n            y1 = x1 + self.NF(self.F(x2))\n            self.s = self.clamp * (torch.sigmoid(self.NH(self.H(y1))) * 2 - 1)\n            y2 = [x2i.mul(torch.exp(self.s)) + self.NG(self.G(y1)) for x2i in x2]\n        else:\n            self.s = self.clamp * (torch.sigmoid(self.NH(self.H(x1))) * 2 - 1)\n            y2 = [(x2i - self.NG(self.G(x1))).div(torch.exp(self.s)) for x2i in x2]\n            y1 = x1 - self.NF(self.F(y2))\n\n        return y1, y2  # torch.cat((y1, y2), 1)\n\n    def jacobian(self, x, rev=False):\n        if not rev:\n            jac = torch.sum(self.s)\n        else:\n            jac = -torch.sum(self.s)\n\n        return jac / x.shape[0]\n\nclass InvNN(nn.Module):\n    def __init__(self, channel_in_ho=3, channel_in_hi=3, subnet_constructor=None, subnet_constructor_v2=None, block_num=[], down_num=2, groups=None):\n        super(InvNN, self).__init__()\n        operations = []\n\n        current_channel_ho = channel_in_ho\n        current_channel_hi = channel_in_hi\n        for i in range(down_num):\n            for j in range(block_num[i]):\n                b = InvBlock(subnet_constructor, subnet_constructor_v2, current_channel_ho, current_channel_hi, groups=groups)\n                operations.append(b)\n\n        self.operations = nn.ModuleList(operations)\n\n    def forward(self, x, x_h, rev=False, cal_jacobian=False):\n        # \t\tout = x\n        jacobian = 0\n\n        if not rev:\n            for op in self.operations:\n                x, x_h = op.forward(x, x_h, rev)\n                if cal_jacobian:\n                    jacobian += op.jacobian(x, rev)\n        else:\n            for op in reversed(self.operations):\n                x, x_h = op.forward(x, x_h, rev)\n                if cal_jacobian:\n                    jacobian += op.jacobian(x, rev)\n\n        if cal_jacobian:\n            return x, x_h, jacobian\n        else:\n            return x, x_h\n\nclass PredictiveModuleMIMO(nn.Module):\n    def __init__(self, channel_in, nf, block_num_rbm=8, block_num_trans=4):\n        super(PredictiveModuleMIMO, self).__init__()\n        self.conv_in = nn.Conv2d(channel_in, nf, 3, 1, 1, bias=True)\n        res_block = []\n        trans_block = []\n        for i in range(block_num_rbm):\n            res_block.append(ResidualBlockNoBN(nf))\n        for j in range(block_num_trans):\n            trans_block.append(TransformerBlock(nf))\n\n        self.res_block = nn.Sequential(*res_block)\n        self.transformer_block = nn.Sequential(*trans_block)\n\n    def forward(self, x):\n        x = self.conv_in(x)\n        x = self.res_block(x)\n        res = self.transformer_block(x) + x\n\n        return res\n\nclass ConvRelu(nn.Module):\n    def __init__(self, channels_in, channels_out, stride=1, init_zero=False):\n        super(ConvRelu, self).__init__()\n        self.init_zero = init_zero\n        if self.init_zero:\n            self.layers = nn.Conv2d(channels_in, channels_out, 3, stride, padding=1)\n\n        else:\n            self.layers = nn.Sequential(\n                nn.Conv2d(channels_in, channels_out, 3, stride, padding=1),\n                nn.LeakyReLU(inplace=True)\n            )\n\n    def forward(self, x):\n        return self.layers(x)\n    \nclass PredictiveModuleBit(nn.Module):\n    def __init__(self, channel_in, nf, block_num_rbm=4, block_num_trans=2):\n        super(PredictiveModuleBit, self).__init__()\n        self.conv_in = nn.Conv2d(channel_in, nf, 3, 1, 1, bias=True)\n        res_block = []\n        trans_block = []\n        for i in range(block_num_rbm):\n            res_block.append(ResidualBlockNoBN(nf))\n        for j in range(block_num_trans):\n            trans_block.append(TransformerBlock(nf))\n        \n        blocks = 4\n        layers = [ConvRelu(nf, 1, 2)]\n        for _ in range(blocks - 1):\n            layer = ConvRelu(1, 1, 2)\n            layers.append(layer)\n        self.layers = nn.Sequential(*layers)\n\n        self.res_block = nn.Sequential(*res_block)\n        self.transformer_block = nn.Sequential(*trans_block)\n\n    def forward(self, x):\n        x = self.conv_in(x)\n        x = self.res_block(x)\n        res = self.transformer_block(x) + x\n        res = self.layers(res)\n\n        return res\n\n\n##---------- Prompt Gen Module -----------------------\nclass PromptGenBlock(nn.Module):\n    def __init__(self,prompt_dim=12,prompt_len=3,prompt_size = 36,lin_dim = 12):\n        super(PromptGenBlock,self).__init__()\n        self.prompt_param = nn.Parameter(torch.rand(1,prompt_len,prompt_dim,prompt_size,prompt_size))\n        self.linear_layer = nn.Linear(lin_dim,prompt_len)\n        self.conv3x3 = nn.Conv2d(prompt_dim,prompt_dim,kernel_size=3,stride=1,padding=1,bias=False)\n        \n\n    def forward(self,x):\n        B,C,H,W = x.shape\n        emb = x.mean(dim=(-2,-1))\n        prompt_weights = F.softmax(self.linear_layer(emb),dim=1)\n        prompt = prompt_weights.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1) * self.prompt_param.unsqueeze(0).repeat(B,1,1,1,1,1).squeeze(1)\n        prompt = torch.sum(prompt,dim=1)\n        prompt = F.interpolate(prompt,(H,W),mode=\"bilinear\")\n        prompt = self.conv3x3(prompt)\n\n        return prompt\n\nclass PredictiveModuleMIMO_prompt(nn.Module):\n    def __init__(self, channel_in, nf, prompt_len=3, block_num_rbm=8, block_num_trans=4):\n        super(PredictiveModuleMIMO_prompt, self).__init__()\n        self.conv_in = nn.Conv2d(channel_in, nf, 3, 1, 1, bias=True)\n        res_block = []\n        trans_block = []\n        for i in range(block_num_rbm):\n            res_block.append(ResidualBlockNoBN(nf))\n        for j in range(block_num_trans):\n            trans_block.append(TransformerBlock(nf))\n\n        self.res_block = nn.Sequential(*res_block)\n        self.transformer_block = nn.Sequential(*trans_block)\n        self.prompt = PromptGenBlock(prompt_dim=nf,prompt_len=prompt_len,prompt_size = 36,lin_dim = nf)\n        self.fuse = nn.Conv2d(nf * 2, nf, 3, 1, 1, bias=True)\n\n    def forward(self, x):\n        x = self.conv_in(x)\n        x = self.res_block(x)\n        res = self.transformer_block(x) + x\n        prompt = self.prompt(res)\n\n        result = self.fuse(torch.cat([res, prompt], dim=1))\n\n        return result\n\ndef gauss_noise(shape):\n    noise = torch.zeros(shape).cuda()\n    for i in range(noise.shape[0]):\n        noise[i] = torch.randn(noise[i].shape).cuda()\n\n    return noise\n\ndef gauss_noise_mul(shape):\n    noise = torch.randn(shape).cuda()\n\n    return noise\n\nclass PredictiveModuleBit_prompt(nn.Module):\n    def __init__(self, channel_in, nf, prompt_length, block_num_rbm=4, block_num_trans=2):\n        super(PredictiveModuleBit_prompt, self).__init__()\n        self.conv_in = nn.Conv2d(channel_in, nf, 3, 1, 1, bias=True)\n        res_block = []\n        trans_block = []\n        for i in range(block_num_rbm):\n            res_block.append(ResidualBlockNoBN(nf))\n        for j in range(block_num_trans):\n            trans_block.append(TransformerBlock(nf))\n        \n        blocks = 4\n        layers = [ConvRelu(nf, 1, 2)]\n        for _ in range(blocks - 1):\n            layer = ConvRelu(1, 1, 2)\n            layers.append(layer)\n        self.layers = nn.Sequential(*layers)\n\n        self.res_block = nn.Sequential(*res_block)\n        self.transformer_block = nn.Sequential(*trans_block)\n        self.prompt = PromptGenBlock(prompt_dim=nf,prompt_len=prompt_length,prompt_size = 36,lin_dim = nf)\n        self.fuse = nn.Conv2d(nf * 2, nf, 3, 1, 1, bias=True)\n\n    def forward(self, x):\n        x = self.conv_in(x)\n        x = self.res_block(x)\n        res = self.transformer_block(x) + x\n        prompt = self.prompt(res)\n        res = self.fuse(torch.cat([res, prompt], dim=1))\n        res = self.layers(res)\n\n        return res\n\nclass VSN(nn.Module):\n    def __init__(self, opt, subnet_constructor=None, subnet_constructor_v2=None, down_num=2):\n        super(VSN, self).__init__()\n        self.model = opt['model']\n        self.mode = opt['mode']\n        opt_net = opt['network_G']\n        self.num_image = opt['num_image']\n        self.gop = opt['gop']\n        self.channel_in = opt_net['in_nc'] * self.gop\n        self.channel_out = opt_net['out_nc'] * self.gop\n        self.channel_in_hi = opt_net['in_nc'] * self.gop\n        self.channel_in_ho = opt_net['in_nc'] * self.gop\n        self.message_len = opt['message_length']\n\n        self.block_num = opt_net['block_num']\n        self.block_num_rbm = opt_net['block_num_rbm']\n        self.block_num_trans = opt_net['block_num_trans']\n        self.nf = self.channel_in_hi \n        \n        self.bitencoder = DW_Encoder(self.message_len, attention = \"se\")\n        self.bitdecoder = DW_Decoder(self.message_len, attention = \"se\")\n        self.irn = InvNN(self.channel_in_ho, self.channel_in_hi, subnet_constructor, subnet_constructor_v2, self.block_num, down_num, groups=self.num_image)\n\n        if opt['prompt']:\n            self.pm = PredictiveModuleMIMO_prompt(self.channel_in_ho, self.nf* self.num_image, opt['prompt_len'], block_num_rbm=self.block_num_rbm, block_num_trans=self.block_num_trans)\n        else:\n            self.pm = PredictiveModuleMIMO(self.channel_in_ho, self.nf* self.num_image, opt['prompt_len'], block_num_rbm=self.block_num_rbm, block_num_trans=self.block_num_trans)\n            self.BitPM = PredictiveModuleBit(3, 4, block_num_rbm=4, block_num_trans=2)\n\n\n    def forward(self, x, x_h=None, message=None, rev=False, hs=[], direction='f'):\n        if not rev:\n            if self.mode == \"image\":\n                out_y, out_y_h = self.irn(x, x_h, rev)\n                out_y = iwt(out_y)\n                encoded_image = self.bitencoder(out_y, message)          \n                return out_y, encoded_image\n            \n            elif self.mode == \"bit\":\n                out_y = iwt(x)\n                encoded_image = self.bitencoder(out_y, message)            \n                return out_y, encoded_image\n\n        else:\n            if self.mode == \"image\":\n                recmessage = self.bitdecoder(x)\n\n                x = dwt(x)\n                out_z = self.pm(x).unsqueeze(1)\n                out_z_new = out_z.view(-1, self.num_image, self.channel_in, x.shape[-2], x.shape[-1])\n                out_z_new = [out_z_new[:,i] for i in range(self.num_image)]\n                out_x, out_x_h = self.irn(x, out_z_new, rev)\n\n                return out_x, out_x_h, out_z, recmessage\n            \n            elif self.mode == \"bit\":\n                recmessage = self.bitdecoder(x)\n                return recmessage\n\n"}
{"type": "source_file", "path": "code/models/bitnetwork/Random_Noise.py", "content": "from . import *\nfrom .noise_layers import *\n\n\nclass Random_Noise(nn.Module):\n\n    def __init__(self, layers, len_layers_R, len_layers_F):\n        super(Random_Noise, self).__init__()\n        for i in range(len(layers)):\n            layers[i] = eval(layers[i])\n        self.noise = nn.Sequential(*layers)\n        self.len_layers_R = len_layers_R\n        self.len_layers_F = len_layers_F\n        print(self.noise)\n        self.transform = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n        ])\n\n    def forward(self, image_cover_mask):\n        image, cover_image, mask = image_cover_mask[0], image_cover_mask[1], image_cover_mask[2]\n        forward_image = image.clone().detach()\n        forward_cover_image = cover_image.clone().detach()\n        forward_mask = mask.clone().detach()\n        noised_image_C = torch.zeros_like(forward_image)\n        noised_image_R = torch.zeros_like(forward_image)\n        noised_image_F = torch.zeros_like(forward_image)\n\n        for index in range(forward_image.shape[0]):\n            random_noise_layer_C = np.random.choice(self.noise, 1)[0]\n            random_noise_layer_R = np.random.choice(self.noise[0:self.len_layers_R], 1)[0]\n            random_noise_layer_F = np.random.choice(self.noise[self.len_layers_R:self.len_layers_R + self.len_layers_F], 1)[0]\n            noised_image_C[index] = random_noise_layer_C([forward_image[index].clone().unsqueeze(0), forward_cover_image[index].clone().unsqueeze(0), forward_mask[index].clone().unsqueeze(0)])\n            noised_image_R[index] = random_noise_layer_R([forward_image[index].clone().unsqueeze(0), forward_cover_image[index].clone().unsqueeze(0), forward_mask[index].clone().unsqueeze(0)])\n            noised_image_F[index] = random_noise_layer_F([forward_image[index].clone().unsqueeze(0), forward_cover_image[index].clone().unsqueeze(0), forward_mask[index].clone().unsqueeze(0)])\n\n            '''single_image = ((noised_image_C[index].clamp(-1, 1).permute(1, 2, 0) + 1) / 2 * 255).add(0.5).clamp(0, 255).to('cpu', torch.uint8).numpy()\n            im = Image.fromarray(single_image)\n            read = np.array(im, dtype=np.uint8)\n            noised_image_C[index] = self.transform(read).unsqueeze(0).to(image.device)\n\n            single_image = ((noised_image_R[index].clamp(-1, 1).permute(1, 2, 0) + 1) / 2 * 255).add(0.5).clamp(0, 255).to('cpu', torch.uint8).numpy()\n            im = Image.fromarray(single_image)\n            read = np.array(im, dtype=np.uint8)\n            noised_image_R[index] = self.transform(read).unsqueeze(0).to(image.device)\n\n            single_image = ((noised_image_F[index].clamp(-1, 1).permute(1, 2, 0) + 1) / 2 * 255).add(0.5).clamp(0, 255).to('cpu', torch.uint8).numpy()\n            im = Image.fromarray(single_image)\n            read = np.array(im, dtype=np.uint8)\n            noised_image_F[index] = self.transform(read).unsqueeze(0).to(image.device)\n\n        noised_image_gap_C = noised_image_C - forward_image\n        noised_image_gap_R = noised_image_R - forward_image\n        noised_image_gap_F = noised_image_F - forward_image'''\n        noised_image_gap_C = noised_image_C.clamp(-1, 1) - forward_image\n        noised_image_gap_R = noised_image_R.clamp(-1, 1) - forward_image\n        noised_image_gap_F = noised_image_F.clamp(-1, 1) - forward_image\n\n        return image + noised_image_gap_C, image + noised_image_gap_R, image + noised_image_gap_F\n"}
{"type": "source_file", "path": "code/options/__init__.py", "content": ""}
{"type": "source_file", "path": "code/models/base_model.py", "content": "import os\nfrom collections import OrderedDict\nimport torch\nimport torch.nn as nn\nfrom torch.nn.parallel import DistributedDataParallel\n\n\nclass BaseModel():\n    def __init__(self, opt):\n        self.opt = opt\n        self.device = torch.device('cuda' if opt['gpu_ids'] is not None else 'cpu')\n        self.is_train = opt['is_train']\n        self.schedulers = []\n        self.optimizers = []\n\n    def feed_data(self, data):\n        pass\n\n    def optimize_parameters(self):\n        pass\n\n    def get_current_visuals(self):\n        pass\n\n    def get_current_losses(self):\n        pass\n\n    def print_network(self):\n        pass\n\n    def save(self, label):\n        pass\n\n    def load(self):\n        pass\n\n    def _set_lr(self, lr_groups_l):\n        ''' set learning rate for warmup,\n        lr_groups_l: list for lr_groups. each for a optimizer'''\n        for optimizer, lr_groups in zip(self.optimizers, lr_groups_l):\n            for param_group, lr in zip(optimizer.param_groups, lr_groups):\n                param_group['lr'] = lr\n\n    def _get_init_lr(self):\n        # get the initial lr, which is set by the scheduler\n        init_lr_groups_l = []\n        for optimizer in self.optimizers:\n            init_lr_groups_l.append([v['initial_lr'] for v in optimizer.param_groups])\n        return init_lr_groups_l\n\n    def update_learning_rate(self, cur_iter, warmup_iter=-1):\n        for scheduler in self.schedulers:\n            scheduler.step()\n        #### set up warm up learning rate\n        if cur_iter < warmup_iter:\n            # get initial lr for each group\n            init_lr_g_l = self._get_init_lr()\n            # modify warming-up learning rates\n            warm_up_lr_l = []\n            for init_lr_g in init_lr_g_l:\n                warm_up_lr_l.append([v / warmup_iter * cur_iter for v in init_lr_g])\n            # set learning rate\n            self._set_lr(warm_up_lr_l)\n\n    def get_current_learning_rate(self):\n        # return self.schedulers[0].get_lr()[0]\n        return self.optimizers[0].param_groups[0]['lr']\n\n    def get_network_description(self, network):\n        '''Get the string and total parameters of the network'''\n        if isinstance(network, nn.DataParallel) or isinstance(network, DistributedDataParallel):\n            network = network.module\n        s = str(network)\n        n = sum(map(lambda x: x.numel(), network.parameters()))\n        return s, n\n\n    def save_network(self, network, network_label, iter_label):\n        save_filename = '{}_{}.pth'.format(iter_label, network_label)\n        save_path = os.path.join(self.opt['path']['models'], save_filename)\n        if isinstance(network, nn.DataParallel) or isinstance(network, DistributedDataParallel):\n            network = network.module\n        state_dict = network.state_dict()\n        for key, param in state_dict.items():\n            state_dict[key] = param.cpu()\n        torch.save(state_dict, save_path)\n\n    def load_network(self, load_path, network, strict=True):\n        if isinstance(network, nn.DataParallel) or isinstance(network, DistributedDataParallel):\n            network = network.module\n        load_net = torch.load(load_path)\n        load_net_clean = OrderedDict()  # remove unnecessary 'module.'\n        for k, v in load_net.items():\n            if k.startswith('module.'):\n                load_net_clean[k[7:]] = v\n            else:\n                load_net_clean[k] = v\n        network.load_state_dict(load_net_clean, strict=strict)\n\n    def save_training_state(self, epoch, iter_step):\n        '''Saves training state during training, which will be used for resuming'''\n        state = {'epoch': epoch, 'iter': iter_step, 'schedulers': [], 'optimizers': []}\n        for s in self.schedulers:\n            state['schedulers'].append(s.state_dict())\n        for o in self.optimizers:\n            state['optimizers'].append(o.state_dict())\n        save_filename = '{}.state'.format(iter_step)\n        save_path = os.path.join(self.opt['path']['training_state'], save_filename)\n        torch.save(state, save_path)\n\n    def resume_training(self, resume_state):\n        '''Resume the optimizers and schedulers for training'''\n        resume_optimizers = resume_state['optimizers']\n        resume_schedulers = resume_state['schedulers']\n        assert len(resume_optimizers) == len(self.optimizers), 'Wrong lengths of optimizers'\n        assert len(resume_schedulers) == len(self.schedulers), 'Wrong lengths of schedulers'\n        for i, o in enumerate(resume_optimizers):\n            self.optimizers[i].load_state_dict(o)\n        for i, s in enumerate(resume_schedulers):\n            self.schedulers[i].load_state_dict(s)\n"}
{"type": "source_file", "path": "code/models/modules/loss.py", "content": "import torch\nimport torch.nn as nn\nimport numpy as np\n\n\nclass ReconstructionLoss(nn.Module):\n    def __init__(self, losstype='l2', eps=1e-6):\n        super(ReconstructionLoss, self).__init__()\n        self.losstype = losstype\n        self.eps = eps\n\n    def forward(self, x, target):\n        if self.losstype == 'l2':\n            return torch.mean(torch.sum((x - target) ** 2, (1, 2, 3)))\n        elif self.losstype == 'l1':\n            diff = x - target\n            return torch.mean(torch.sum(torch.sqrt(diff * diff + self.eps), (1, 2, 3)))\n        elif self.losstype == 'center':\n            return torch.sum((x - target) ** 2, (1, 2, 3))\n\n        else:\n            print(\"reconstruction loss type error!\")\n            return 0\n\n\n# Define GAN loss: [vanilla | lsgan | wgan-gp]\nclass GANLoss(nn.Module):\n    def __init__(self, gan_type, real_label_val=1.0, fake_label_val=0.0):\n        super(GANLoss, self).__init__()\n        self.gan_type = gan_type.lower()\n        self.real_label_val = real_label_val\n        self.fake_label_val = fake_label_val\n\n        if self.gan_type == 'gan' or self.gan_type == 'ragan':\n            self.loss = nn.BCEWithLogitsLoss()\n        elif self.gan_type == 'lsgan':\n            self.loss = nn.MSELoss()\n        elif self.gan_type == 'wgan-gp':\n\n            def wgan_loss(input, target):\n                # target is boolean\n                return -1 * input.mean() if target else input.mean()\n\n            self.loss = wgan_loss\n        else:\n            raise NotImplementedError('GAN type [{:s}] is not found'.format(self.gan_type))\n\n    def get_target_label(self, input, target_is_real):\n        if self.gan_type == 'wgan-gp':\n            return target_is_real\n        if target_is_real:\n            return torch.empty_like(input).fill_(self.real_label_val)\n        else:\n            return torch.empty_like(input).fill_(self.fake_label_val)\n\n    def forward(self, input, target_is_real):\n        target_label = self.get_target_label(input, target_is_real)\n        loss = self.loss(input, target_label)\n        return loss\n\n\nclass GradientPenaltyLoss(nn.Module):\n    def __init__(self, device=torch.device('cpu')):\n        super(GradientPenaltyLoss, self).__init__()\n        self.register_buffer('grad_outputs', torch.Tensor())\n        self.grad_outputs = self.grad_outputs.to(device)\n\n    def get_grad_outputs(self, input):\n        if self.grad_outputs.size() != input.size():\n            self.grad_outputs.resize_(input.size()).fill_(1.0)\n        return self.grad_outputs\n\n    def forward(self, interp, interp_crit):\n        grad_outputs = self.get_grad_outputs(interp_crit)\n        grad_interp = torch.autograd.grad(outputs=interp_crit, inputs=interp,\n                                          grad_outputs=grad_outputs, create_graph=True,\n                                          retain_graph=True, only_inputs=True)[0]\n        grad_interp = grad_interp.view(grad_interp.size(0), -1)\n        grad_interp_norm = grad_interp.norm(2, dim=1)\n\n        loss = ((grad_interp_norm - 1) ** 2).mean()\n        return loss\n    \n\nclass ReconstructionMsgLoss(nn.Module):\n    def __init__(self, losstype='mse'):\n        super(ReconstructionMsgLoss, self).__init__()\n        self.losstype = losstype\n        self.mse_loss = nn.MSELoss()\n        self.bce_loss = nn.BCELoss()\n        self.bce_logits_loss = nn.BCEWithLogitsLoss()\n\n    def forward(self, messages, decoded_messages): \n        if self.losstype == 'mse':\n            return self.mse_loss(messages, decoded_messages)\n        elif self.losstype == 'bce':\n            return self.bce_loss(messages, decoded_messages)\n        elif self.losstype == 'bce_logits':\n            return self.bce_logits_loss(messages, decoded_messages)\n        else:\n            print(\"ReconstructionMsgLoss loss type error!\")\n            return 0\n"}
{"type": "source_file", "path": "code/utils/compression.py", "content": "# Standard libraries\nimport itertools\nimport numpy as np\n# PyTorch\nimport torch\nimport torch.nn as nn\n# Local\nfrom . import JPEG_utils\n\n\nclass rgb_to_ycbcr_jpeg(nn.Module):\n    \"\"\" Converts RGB image to YCbCr\n\n    \"\"\"\n    def __init__(self):\n        super(rgb_to_ycbcr_jpeg, self).__init__()\n        matrix = np.array(\n            [[0.299, 0.587, 0.114], [-0.168736, -0.331264, 0.5],\n             [0.5, -0.418688, -0.081312]], dtype=np.float32).T\n        self.shift = nn.Parameter(torch.tensor([0., 128., 128.]))\n        #\n        self.matrix = nn.Parameter(torch.from_numpy(matrix))\n\n    def forward(self, image):\n        image = image.permute(0, 2, 3, 1)\n        result = torch.tensordot(image, self.matrix, dims=1) + self.shift\n    #    result = torch.from_numpy(result)\n        result.view(image.shape)\n        return result\n\n\n\nclass chroma_subsampling(nn.Module):\n    \"\"\" Chroma subsampling on CbCv channels\n    Input:\n        image(tensor): batch x height x width x 3\n    Output:\n        y(tensor): batch x height x width\n        cb(tensor): batch x height/2 x width/2\n        cr(tensor): batch x height/2 x width/2\n    \"\"\"\n    def __init__(self):\n        super(chroma_subsampling, self).__init__()\n\n    def forward(self, image):\n        image_2 = image.permute(0, 3, 1, 2).clone()\n        avg_pool = nn.AvgPool2d(kernel_size=2, stride=(2, 2),\n                                count_include_pad=False)\n        cb = avg_pool(image_2[:, 1, :, :].unsqueeze(1))\n        cr = avg_pool(image_2[:, 2, :, :].unsqueeze(1))\n        cb = cb.permute(0, 2, 3, 1)\n        cr = cr.permute(0, 2, 3, 1)\n        return image[:, :, :, 0], cb.squeeze(3), cr.squeeze(3)\n        \n\nclass block_splitting(nn.Module):\n    \"\"\" Splitting image into patches\n    Input:\n        image(tensor): batch x height x width\n    Output: \n        patch(tensor):  batch x h*w/64 x h x w\n    \"\"\"\n    def __init__(self):\n        super(block_splitting, self).__init__()\n        self.k = 8\n\n    def forward(self, image):\n        height, width = image.shape[1:3]\n        # print(height, width)\n        batch_size = image.shape[0]\n        # print(image.shape)\n        image_reshaped = image.view(batch_size, height // self.k, self.k, -1, self.k)\n        image_transposed = image_reshaped.permute(0, 1, 3, 2, 4)\n        return image_transposed.contiguous().view(batch_size, -1, self.k, self.k)\n    \n\nclass dct_8x8(nn.Module):\n    \"\"\" Discrete Cosine Transformation\n    Input:\n        image(tensor): batch x height x width\n    Output:\n        dcp(tensor): batch x height x width\n    \"\"\"\n    def __init__(self):\n        super(dct_8x8, self).__init__()\n        tensor = np.zeros((8, 8, 8, 8), dtype=np.float32)\n        for x, y, u, v in itertools.product(range(8), repeat=4):\n            tensor[x, y, u, v] = np.cos((2 * x + 1) * u * np.pi / 16) * np.cos(\n                (2 * y + 1) * v * np.pi / 16)\n        alpha = np.array([1. / np.sqrt(2)] + [1] * 7)\n        #\n        self.tensor =  nn.Parameter(torch.from_numpy(tensor).float())\n        self.scale = nn.Parameter(torch.from_numpy(np.outer(alpha, alpha) * 0.25).float() )\n        \n    def forward(self, image):\n        image = image - 128\n        result = self.scale * torch.tensordot(image, self.tensor, dims=2)\n        result.view(image.shape)\n        return result\n\n\nclass y_quantize(nn.Module):\n    \"\"\" JPEG Quantization for Y channel\n    Input:\n        image(tensor): batch x height x width\n        rounding(function): rounding function to use\n        factor(float): Degree of compression\n    Output:\n        image(tensor): batch x height x width\n    \"\"\"\n    def __init__(self, rounding, factor=1):\n        super(y_quantize, self).__init__()\n        self.rounding = rounding\n        self.factor = factor\n        self.y_table = JPEG_utils.y_table\n\n    def forward(self, image):\n        image = image.float() / (self.y_table * self.factor)\n        image = self.rounding(image)\n        return image\n\n\nclass c_quantize(nn.Module):\n    \"\"\" JPEG Quantization for CrCb channels\n    Input:\n        image(tensor): batch x height x width\n        rounding(function): rounding function to use\n        factor(float): Degree of compression\n    Output:\n        image(tensor): batch x height x width\n    \"\"\"\n    def __init__(self, rounding, factor=1):\n        super(c_quantize, self).__init__()\n        self.rounding = rounding\n        self.factor = factor\n        self.c_table = JPEG_utils.c_table\n\n    def forward(self, image):\n        image = image.float() / (self.c_table * self.factor)\n        image = self.rounding(image)\n        return image\n\n\nclass compress_jpeg(nn.Module):\n    \"\"\" Full JPEG compression algortihm\n    Input:\n        imgs(tensor): batch x 3 x height x width \n        rounding(function): rounding function to use\n        factor(float): Compression factor\n    Ouput:\n        compressed(dict(tensor)): batch x h*w/64 x 8 x 8\n    \"\"\"\n    def __init__(self, rounding=torch.round, factor=1):\n        super(compress_jpeg, self).__init__()\n        self.l1 = nn.Sequential(\n            rgb_to_ycbcr_jpeg(),\n            # comment this line if no subsampling \n            chroma_subsampling()\n        )\n        self.l2 = nn.Sequential(\n            block_splitting(),\n            dct_8x8()\n        )\n        self.c_quantize = c_quantize(rounding=rounding, factor=factor)\n        self.y_quantize = y_quantize(rounding=rounding, factor=factor)\n        \n    def forward(self, image):\n        y, cb, cr = self.l1(image*255) # modify \n\n        # y, cb, cr = result[:,:,:,0], result[:,:,:,1], result[:,:,:,2]\n        components = {'y': y, 'cb': cb, 'cr': cr}\n        for k in components.keys():\n            comp = self.l2(components[k])\n            # print(comp.shape)\n            if k in ('cb', 'cr'):\n                comp = self.c_quantize(comp)\n            else:\n                comp = self.y_quantize(comp)\n\n            components[k] = comp\n\n        return components['y'], components['cb'], components['cr']"}
{"type": "source_file", "path": "code/models/networks.py", "content": "import logging\nimport math\n\nfrom models.modules.Inv_arch import *\nfrom models.modules.Subnet_constructor import subnet\n\nlogger = logging.getLogger('base')\n\n####################\n# define network\n####################\ndef define_G_v2(opt):\n\topt_net = opt['network_G']\n\twhich_model = opt_net['which_model_G']\n\tsubnet_type = which_model['subnet_type']\n\topt_datasets = opt['datasets']\n\tdown_num = int(math.log(opt_net['scale'], 2))\n\tif opt['num_image'] == 1:\n\t\tnetG = VSN(opt, subnet(subnet_type, 'xavier'), subnet(subnet_type, 'xavier'), down_num)\n\telse:\n\t\tnetG = VSN(opt, subnet(subnet_type, 'xavier'), subnet(subnet_type, 'xavier_v2'), down_num)\n\n\treturn netG\n"}
{"type": "source_file", "path": "code/models/modules/__init__.py", "content": ""}
{"type": "source_file", "path": "code/utils/__init__.py", "content": ""}
{"type": "source_file", "path": "code/models/modules/common.py", "content": "import math\n\nimport torch\nimport torch.nn as nn\n\ndef dwt_init3d(x):\n\n    x01 = x[:, :, :, 0::2, :] / 2\n    x02 = x[:, :, :, 1::2, :] / 2\n    x1 = x01[:, :, :, :, 0::2]\n    x2 = x02[:, :, :, :, 0::2]\n    x3 = x01[:, :, :, :, 1::2]\n    x4 = x02[:, :, :, :, 1::2]\n    x_LL = x1 + x2 + x3 + x4\n    x_HL = -x1 - x2 + x3 + x4\n    x_LH = -x1 + x2 - x3 + x4\n    x_HH = x1 - x2 - x3 + x4\n\n    return torch.cat((x_LL, x_HL, x_LH, x_HH), 1)\n\ndef dwt_init(x):\n\n    x01 = x[:, :, 0::2, :] / 2\n    x02 = x[:, :, 1::2, :] / 2\n    x1 = x01[:, :, :, 0::2]\n    x2 = x02[:, :, :, 0::2]\n    x3 = x01[:, :, :, 1::2]\n    x4 = x02[:, :, :, 1::2]\n    x_LL = x1 + x2 + x3 + x4\n    x_HL = -x1 - x2 + x3 + x4\n    x_LH = -x1 + x2 - x3 + x4\n    x_HH = x1 - x2 - x3 + x4\n\n    return torch.cat((x_LL, x_HL, x_LH, x_HH), 1)\n\ndef iwt_init(x):\n    r = 2\n    in_batch, in_channel, in_height, in_width = x.size()\n    #print([in_batch, in_channel, in_height, in_width])\n    out_batch, out_channel, out_height, out_width = in_batch, int(\n        in_channel / (r ** 2)), r * in_height, r * in_width\n    x1 = x[:, 0:out_channel, :, :] / 2\n    x2 = x[:, out_channel:out_channel * 2, :, :] / 2\n    x3 = x[:, out_channel * 2:out_channel * 3, :, :] / 2\n    x4 = x[:, out_channel * 3:out_channel * 4, :, :] / 2\n\n\n    h = torch.zeros([out_batch, out_channel, out_height, out_width]).float().cuda()\n\n    h[:, :, 0::2, 0::2] = x1 - x2 - x3 + x4\n    h[:, :, 1::2, 0::2] = x1 - x2 + x3 - x4\n    h[:, :, 0::2, 1::2] = x1 + x2 - x3 - x4\n    h[:, :, 1::2, 1::2] = x1 + x2 + x3 + x4\n\n    return h\n\nclass DWT(nn.Module):\n    def __init__(self):\n        super(DWT, self).__init__()\n        self.requires_grad = False\n\n    def forward(self, x):\n        return dwt_init(x)\n\nclass DWT3d(nn.Module):\n    def __init__(self):\n        super(DWT3d, self).__init__()\n        self.requires_grad = False\n\n    def forward(self, x):\n        return dwt_init3d(x)\n\nclass IWT(nn.Module):\n    def __init__(self):\n        super(IWT, self).__init__()\n        self.requires_grad = False\n\n    def forward(self, x):\n        return iwt_init(x)"}
{"type": "source_file", "path": "code/models/modules/Subnet_constructor.py", "content": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport models.modules.module_util as mutil\nfrom basicsr.archs.arch_util import flow_warp, ResidualBlockNoBN\nfrom models.modules.module_util import initialize_weights_xavier\n\nclass DenseBlock(nn.Module):\n    def __init__(self, channel_in, channel_out, init='xavier', gc=32, bias=True):\n        super(DenseBlock, self).__init__()\n        self.conv1 = nn.Conv2d(channel_in, gc, 3, 1, 1, bias=bias)\n        self.conv2 = nn.Conv2d(channel_in + gc, gc, 3, 1, 1, bias=bias)\n        self.conv3 = nn.Conv2d(channel_in + 2 * gc, gc, 3, 1, 1, bias=bias)\n        self.conv4 = nn.Conv2d(channel_in + 3 * gc, gc, 3, 1, 1, bias=bias)\n        self.conv5 = nn.Conv2d(channel_in + 4 * gc, channel_out, 3, 1, 1, bias=bias)\n        self.lrelu = nn.LeakyReLU(negative_slope=0.2, inplace=True)\n        self.H = None\n\n        if init == 'xavier':\n            mutil.initialize_weights_xavier([self.conv1, self.conv2, self.conv3, self.conv4], 0.1)\n        else:\n            mutil.initialize_weights([self.conv1, self.conv2, self.conv3, self.conv4], 0.1)\n        mutil.initialize_weights(self.conv5, 0)\n\n    def forward(self, x):\n        if isinstance(x, list):\n            x = x[0]\n        x1 = self.lrelu(self.conv1(x))\n        x2 = self.lrelu(self.conv2(torch.cat((x, x1), 1)))\n        x3 = self.lrelu(self.conv3(torch.cat((x, x1, x2), 1)))\n        x4 = self.lrelu(self.conv4(torch.cat((x, x1, x2, x3), 1)))\n        x5 = self.conv5(torch.cat((x, x1, x2, x3, x4), 1))\n\n        return x5\n\nclass DenseBlock_v2(nn.Module):\n    def __init__(self, channel_in, channel_out, groups, init='xavier', gc=32, bias=True):\n        super(DenseBlock_v2, self).__init__()\n        self.conv1 = nn.Conv2d(channel_in, gc, 3, 1, 1, bias=bias)\n        self.conv2 = nn.Conv2d(channel_in + gc, gc, 3, 1, 1, bias=bias)\n        self.conv3 = nn.Conv2d(channel_in + 2 * gc, gc, 3, 1, 1, bias=bias)\n        self.conv4 = nn.Conv2d(channel_in + 3 * gc, gc, 3, 1, 1, bias=bias)\n        self.conv5 = nn.Conv2d(channel_in + 4 * gc, channel_out, 3, 1, 1, bias=bias)\n        self.conv_final = nn.Conv2d(channel_out*groups, channel_out, 3, 1, 1, bias=bias)\n        self.lrelu = nn.LeakyReLU(negative_slope=0.2, inplace=True)\n\n        if init == 'xavier':\n            mutil.initialize_weights_xavier([self.conv1, self.conv2, self.conv3, self.conv4, self.conv5], 0.1)\n        else:\n            mutil.initialize_weights([self.conv1, self.conv2, self.conv3, self.conv4, self.conv5], 0.1)\n        mutil.initialize_weights(self.conv_final, 0)\n\n    def forward(self, x):\n        res = []\n        for xi in x:\n            x1 = self.lrelu(self.conv1(xi))\n            x2 = self.lrelu(self.conv2(torch.cat((xi, x1), 1)))\n            x3 = self.lrelu(self.conv3(torch.cat((xi, x1, x2), 1)))\n            x4 = self.lrelu(self.conv4(torch.cat((xi, x1, x2, x3), 1)))\n            x5 = self.lrelu(self.conv5(torch.cat((xi, x1, x2, x3, x4), 1)))\n            res.append(x5)\n        res = torch.cat(res, dim=1)\n        res = self.conv_final(res)\n\n        return res\n\ndef subnet(net_structure, init='xavier'):\n    def constructor(channel_in, channel_out, groups=None):\n        if net_structure == 'DBNet':\n            if init == 'xavier':\n                return DenseBlock(channel_in, channel_out, init)\n            elif init == 'xavier_v2':\n                return DenseBlock_v2(channel_in, channel_out, groups, 'xavier')\n            else:\n                return DenseBlock(channel_in, channel_out)\n        else:\n            return None\n\n    return constructor\n"}
{"type": "source_file", "path": "code/models/lr_scheduler.py", "content": "import math\nfrom collections import Counter\nfrom collections import defaultdict\nimport torch\nfrom torch.optim.lr_scheduler import _LRScheduler\n\n\nclass MultiStepLR_Restart(_LRScheduler):\n    def __init__(self, optimizer, milestones, restarts=None, weights=None, gamma=0.1,\n                 clear_state=False, last_epoch=-1):\n        self.milestones = Counter(milestones)\n        self.gamma = gamma\n        self.clear_state = clear_state\n        self.restarts = restarts if restarts else [0]\n        self.restart_weights = weights if weights else [1]\n        assert len(self.restarts) == len(\n            self.restart_weights), 'restarts and their weights do not match.'\n        super(MultiStepLR_Restart, self).__init__(optimizer, last_epoch)\n\n    def get_lr(self):\n        if self.last_epoch in self.restarts:\n            if self.clear_state:\n                self.optimizer.state = defaultdict(dict)\n            weight = self.restart_weights[self.restarts.index(self.last_epoch)]\n            return [group['initial_lr'] * weight for group in self.optimizer.param_groups]\n        if self.last_epoch not in self.milestones:\n            return [group['lr'] for group in self.optimizer.param_groups]\n        return [\n            group['lr'] * self.gamma**self.milestones[self.last_epoch]\n            for group in self.optimizer.param_groups\n        ]\n\n\nclass CosineAnnealingLR_Restart(_LRScheduler):\n    def __init__(self, optimizer, T_period, restarts=None, weights=None, eta_min=0, last_epoch=-1):\n        self.T_period = T_period\n        self.T_max = self.T_period[0]  # current T period\n        self.eta_min = eta_min\n        self.restarts = restarts if restarts else [0]\n        self.restart_weights = weights if weights else [1]\n        self.last_restart = 0\n        assert len(self.restarts) == len(\n            self.restart_weights), 'restarts and their weights do not match.'\n        super(CosineAnnealingLR_Restart, self).__init__(optimizer, last_epoch)\n\n    def get_lr(self):\n        if self.last_epoch == 0:\n            return self.base_lrs\n        elif self.last_epoch in self.restarts:\n            self.last_restart = self.last_epoch\n            self.T_max = self.T_period[self.restarts.index(self.last_epoch) + 1]\n            weight = self.restart_weights[self.restarts.index(self.last_epoch)]\n            return [group['initial_lr'] * weight for group in self.optimizer.param_groups]\n        elif (self.last_epoch - self.last_restart - 1 - self.T_max) % (2 * self.T_max) == 0:\n            return [\n                group['lr'] + (base_lr - self.eta_min) * (1 - math.cos(math.pi / self.T_max)) / 2\n                for base_lr, group in zip(self.base_lrs, self.optimizer.param_groups)\n            ]\n        return [(1 + math.cos(math.pi * (self.last_epoch - self.last_restart) / self.T_max)) /\n                (1 + math.cos(math.pi * ((self.last_epoch - self.last_restart) - 1) / self.T_max)) *\n                (group['lr'] - self.eta_min) + self.eta_min\n                for group in self.optimizer.param_groups]\n\n\nif __name__ == \"__main__\":\n    optimizer = torch.optim.Adam([torch.zeros(3, 64, 3, 3)], lr=2e-4, weight_decay=0,\n                                 betas=(0.9, 0.99))\n    ##############################\n    # MultiStepLR_Restart\n    ##############################\n    ## Original\n    lr_steps = [200000, 400000, 600000, 800000]\n    restarts = None\n    restart_weights = None\n\n    ## two\n    lr_steps = [100000, 200000, 300000, 400000, 490000, 600000, 700000, 800000, 900000, 990000]\n    restarts = [500000]\n    restart_weights = [1]\n\n    ## four\n    lr_steps = [\n        50000, 100000, 150000, 200000, 240000, 300000, 350000, 400000, 450000, 490000, 550000,\n        600000, 650000, 700000, 740000, 800000, 850000, 900000, 950000, 990000\n    ]\n    restarts = [250000, 500000, 750000]\n    restart_weights = [1, 1, 1]\n\n    scheduler = MultiStepLR_Restart(optimizer, lr_steps, restarts, restart_weights, gamma=0.5,\n                                    clear_state=False)\n\n    ##############################\n    # Cosine Annealing Restart\n    ##############################\n    ## two\n    T_period = [500000, 500000]\n    restarts = [500000]\n    restart_weights = [1]\n\n    ## four\n    T_period = [250000, 250000, 250000, 250000]\n    restarts = [250000, 500000, 750000]\n    restart_weights = [1, 1, 1]\n\n    scheduler = CosineAnnealingLR_Restart(optimizer, T_period, eta_min=1e-7, restarts=restarts,\n                                          weights=restart_weights)\n\n    ##############################\n    # Draw figure\n    ##############################\n    N_iter = 1000000\n    lr_l = list(range(N_iter))\n    for i in range(N_iter):\n        scheduler.step()\n        current_lr = optimizer.param_groups[0]['lr']\n        lr_l[i] = current_lr\n\n    import matplotlib as mpl\n    from matplotlib import pyplot as plt\n    import matplotlib.ticker as mtick\n    mpl.style.use('default')\n    import seaborn\n    seaborn.set(style='whitegrid')\n    seaborn.set_context('paper')\n\n    plt.figure(1)\n    plt.subplot(111)\n    plt.ticklabel_format(style='sci', axis='x', scilimits=(0, 0))\n    plt.title('Title', fontsize=16, color='k')\n    plt.plot(list(range(N_iter)), lr_l, linewidth=1.5, label='learning rate scheme')\n    legend = plt.legend(loc='upper right', shadow=False)\n    ax = plt.gca()\n    labels = ax.get_xticks().tolist()\n    for k, v in enumerate(labels):\n        labels[k] = str(int(v / 1000)) + 'K'\n    ax.set_xticklabels(labels)\n    ax.yaxis.set_major_formatter(mtick.FormatStrFormatter('%.1e'))\n\n    ax.set_ylabel('Learning rate')\n    ax.set_xlabel('Iteration')\n    fig = plt.gcf()\n    plt.show()\n"}
{"type": "source_file", "path": "code/models/__init__.py", "content": "import logging\nlogger = logging.getLogger('base')\n\ndef create_model(opt):\n    model = opt['model']\n    frame_num = opt['gop']\n    from .IBSN import Model_VSN as M\n\n    m = M(opt)\n    logger.info('Model [{:s}] is created.'.format(m.__class__.__name__))\n    return m"}
{"type": "source_file", "path": "code/data/util.py", "content": "import os\nimport math\nimport pickle\nimport random\nimport numpy as np\nimport glob\nimport torch\nimport cv2\n\n####################\n# Files & IO\n####################\n\n###################### get image path list ######################\nIMG_EXTENSIONS = ['.jpg', '.JPG', '.jpeg', '.JPEG', '.png', '.PNG', '.ppm', '.PPM', '.bmp', '.BMP']\n\n\ndef is_image_file(filename):\n    return any(filename.endswith(extension) for extension in IMG_EXTENSIONS)\n\n\ndef _get_paths_from_images(path):\n    '''get image path list from image folder'''\n    assert os.path.isdir(path), '{:s} is not a valid directory'.format(path)\n    images = []\n    for dirpath, _, fnames in sorted(os.walk(path)):\n        for fname in sorted(fnames):\n            if is_image_file(fname):\n                img_path = os.path.join(dirpath, fname)\n                images.append(img_path)\n    assert images, '{:s} has no valid image file'.format(path)\n    return images\n\n\ndef _get_paths_from_lmdb(dataroot):\n    '''get image path list from lmdb meta info'''\n    meta_info = pickle.load(open(os.path.join(dataroot, 'meta_info.pkl'), 'rb'))\n    paths = meta_info['keys']\n    sizes = meta_info['resolution']\n    if len(sizes) == 1:\n        sizes = sizes * len(paths)\n    return paths, sizes\n\n\ndef get_image_paths(data_type, dataroot):\n    '''get image path list\n    support lmdb or image files'''\n    paths, sizes = None, None\n    if dataroot is not None:\n        if data_type == 'lmdb':\n            paths, sizes = _get_paths_from_lmdb(dataroot)\n        elif data_type == 'img':\n            paths = sorted(_get_paths_from_images(dataroot))\n        else:\n            raise NotImplementedError('data_type [{:s}] is not recognized.'.format(data_type))\n    return paths, sizes\n\n\ndef glob_file_list(root):\n    return sorted(glob.glob(os.path.join(root, '*')))\n\n\n###################### read images ######################\ndef _read_img_lmdb(env, key, size):\n    '''read image from lmdb with key (w/ and w/o fixed size)\n    size: (C, H, W) tuple'''\n    with env.begin(write=False) as txn:\n        buf = txn.get(key.encode('ascii'))\n    img_flat = np.frombuffer(buf, dtype=np.uint8)\n    C, H, W = size\n    img = img_flat.reshape(H, W, C)\n    return img\n\n\ndef read_img(env, path, size=None):\n    '''read image by cv2 or from lmdb\n    return: Numpy float32, HWC, BGR, [0,1]'''\n    if env is None:  # img\n#         print(path)\n        #img = cv2.imread(path, cv2.IMREAD_UNCHANGED)\n        img = cv2.imread(path, cv2.IMREAD_COLOR)\n    else:\n        img = _read_img_lmdb(env, path, size)\n#     print(img.shape)\n#     if img is None:\n    # print(path)\n#     print(img.shape)\n    img = img.astype(np.float32) / 255.\n    if img.ndim == 2:\n        img = np.expand_dims(img, axis=2)\n    # some images have 4 channels\n    if img.shape[2] > 3:\n        img = img[:, :, :3]\n    return img\n\n\ndef read_img_seq(path):\n    \"\"\"Read a sequence of images from a given folder path\n    Args:\n        path (list/str): list of image paths/image folder path\n\n    Returns:\n        imgs (Tensor): size (T, C, H, W), RGB, [0, 1]\n    \"\"\"\n    if type(path) is list:\n        img_path_l = path\n    else:\n        img_path_l = sorted(glob.glob(os.path.join(path, '*.png')))\n#     print(path)\n#     print(path,img_path_l)\n    img_l = [read_img(None, v) for v in img_path_l]\n    # stack to Torch tensor\n    imgs = np.stack(img_l, axis=0)\n    imgs = imgs[:, :, :, [2, 1, 0]]\n    imgs = torch.from_numpy(np.ascontiguousarray(np.transpose(imgs, (0, 3, 1, 2)))).float()\n    return imgs\n\n\ndef index_generation(crt_i, max_n, N, padding='reflection'):\n    \"\"\"Generate an index list for reading N frames from a sequence of images\n    Args:\n        crt_i (int): current center index\n        max_n (int): max number of the sequence of images (calculated from 1)\n        N (int): reading N frames\n        padding (str): padding mode, one of replicate | reflection | new_info | circle\n            Example: crt_i = 0, N = 5\n            replicate: [0, 0, 0, 1, 2]\n            reflection: [2, 1, 0, 1, 2]\n            new_info: [4, 3, 0, 1, 2]\n            circle: [3, 4, 0, 1, 2]\n\n    Returns:\n        return_l (list [int]): a list of indexes\n    \"\"\"\n    max_n = max_n - 1\n    n_pad = N // 2\n    return_l = []\n\n    for i in range(crt_i - n_pad, crt_i + n_pad + 1):\n        if i < 0:\n            if padding == 'replicate':\n                add_idx = 0\n            elif padding == 'reflection':\n                add_idx = -i\n            elif padding == 'new_info':\n                add_idx = (crt_i + n_pad) + (-i)\n            elif padding == 'circle':\n                add_idx = N + i\n            else:\n                raise ValueError('Wrong padding mode')\n        elif i > max_n:\n            if padding == 'replicate':\n                add_idx = max_n\n            elif padding == 'reflection':\n                add_idx = max_n * 2 - i\n            elif padding == 'new_info':\n                add_idx = (crt_i - n_pad) - (i - max_n)\n            elif padding == 'circle':\n                add_idx = i - N\n            else:\n                raise ValueError('Wrong padding mode')\n        else:\n            add_idx = i\n        return_l.append(add_idx)\n    return return_l\n\n\n####################\n# image processing\n# process on numpy image\n####################\n\n\ndef augment(img_list, hflip=True, rot=True):\n    # horizontal flip OR rotate\n    hflip = hflip and random.random() < 0.5\n    vflip = rot and random.random() < 0.5\n    rot90 = rot and random.random() < 0.5\n\n    def _augment(img):\n        if hflip:\n            img = img[:, ::-1, :]\n        if vflip:\n            img = img[::-1, :, :]\n        if rot90:\n            img = img.transpose(1, 0, 2)\n        return img\n\n    return [_augment(img) for img in img_list]\n\n\ndef augment_flow(img_list, flow_list, hflip=True, rot=True):\n    # horizontal flip OR rotate\n    hflip = hflip and random.random() < 0.5\n    vflip = rot and random.random() < 0.5\n    rot90 = rot and random.random() < 0.5\n\n    def _augment(img):\n        if hflip:\n            img = img[:, ::-1, :]\n        if vflip:\n            img = img[::-1, :, :]\n        if rot90:\n            img = img.transpose(1, 0, 2)\n        return img\n\n    def _augment_flow(flow):\n        if hflip:\n            flow = flow[:, ::-1, :]\n            flow[:, :, 0] *= -1\n        if vflip:\n            flow = flow[::-1, :, :]\n            flow[:, :, 1] *= -1\n        if rot90:\n            flow = flow.transpose(1, 0, 2)\n            flow = flow[:, :, [1, 0]]\n        return flow\n\n    rlt_img_list = [_augment(img) for img in img_list]\n    rlt_flow_list = [_augment_flow(flow) for flow in flow_list]\n\n    return rlt_img_list, rlt_flow_list\n\n\ndef channel_convert(in_c, tar_type, img_list):\n    # conversion among BGR, gray and y\n    if in_c == 3 and tar_type == 'gray':  # BGR to gray\n        gray_list = [cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) for img in img_list]\n        return [np.expand_dims(img, axis=2) for img in gray_list]\n    elif in_c == 3 and tar_type == 'y':  # BGR to y\n        y_list = [bgr2ycbcr(img, only_y=True) for img in img_list]\n        return [np.expand_dims(img, axis=2) for img in y_list]\n    elif in_c == 1 and tar_type == 'RGB':  # gray/y to BGR\n        return [cv2.cvtColor(img, cv2.COLOR_GRAY2BGR) for img in img_list]\n    else:\n        return img_list\n\n\ndef rgb2ycbcr(img, only_y=True):\n    '''same as matlab rgb2ycbcr\n    only_y: only return Y channel\n    Input:\n        uint8, [0, 255]\n        float, [0, 1]\n    '''\n    in_img_type = img.dtype\n    img.astype(np.float32)\n    if in_img_type != np.uint8:\n        img *= 255.\n    # convert\n    if only_y:\n        rlt = np.dot(img, [65.481, 128.553, 24.966]) / 255.0 + 16.0\n    else:\n        rlt = np.matmul(img, [[65.481, -37.797, 112.0], [128.553, -74.203, -93.786],\n                              [24.966, 112.0, -18.214]]) / 255.0 + [16, 128, 128]\n    if in_img_type == np.uint8:\n        rlt = rlt.round()\n    else:\n        rlt /= 255.\n    return rlt.astype(in_img_type)\n\n\ndef bgr2ycbcr(img, only_y=True):\n    '''bgr version of rgb2ycbcr\n    only_y: only return Y channel\n    Input:\n        uint8, [0, 255]\n        float, [0, 1]\n    '''\n    in_img_type = img.dtype\n    img.astype(np.float32)\n    if in_img_type != np.uint8:\n        img *= 255.\n    # convert\n    if only_y:\n        rlt = np.dot(img, [24.966, 128.553, 65.481]) / 255.0 + 16.0\n    else:\n        rlt = np.matmul(img, [[24.966, 112.0, -18.214], [128.553, -74.203, -93.786],\n                              [65.481, -37.797, 112.0]]) / 255.0 + [16, 128, 128]\n    if in_img_type == np.uint8:\n        rlt = rlt.round()\n    else:\n        rlt /= 255.\n    return rlt.astype(in_img_type)\n\n\ndef ycbcr2rgb(img):\n    '''same as matlab ycbcr2rgb\n    Input:\n        uint8, [0, 255]\n        float, [0, 1]\n    '''\n    in_img_type = img.dtype\n    img.astype(np.float32)\n    if in_img_type != np.uint8:\n        img *= 255.\n    # convert\n    rlt = np.matmul(img, [[0.00456621, 0.00456621, 0.00456621], [0, -0.00153632, 0.00791071],\n                          [0.00625893, -0.00318811, 0]]) * 255.0 + [-222.921, 135.576, -276.836]\n    if in_img_type == np.uint8:\n        rlt = rlt.round()\n    else:\n        rlt /= 255.\n    return rlt.astype(in_img_type)\n\n\ndef modcrop(img_in, scale):\n    # img_in: Numpy, HWC or HW\n    img = np.copy(img_in)\n    if img.ndim == 2:\n        H, W = img.shape\n        H_r, W_r = H % scale, W % scale\n        img = img[:H - H_r, :W - W_r]\n    elif img.ndim == 3:\n        H, W, C = img.shape\n        H_r, W_r = H % scale, W % scale\n        img = img[:H - H_r, :W - W_r, :]\n    else:\n        raise ValueError('Wrong img ndim: [{:d}].'.format(img.ndim))\n    return img\n\n\n####################\n# Functions\n####################\n\n\n# matlab 'imresize' function, now only support 'bicubic'\ndef cubic(x):\n    absx = torch.abs(x)\n    absx2 = absx**2\n    absx3 = absx**3\n    return (1.5 * absx3 - 2.5 * absx2 + 1) * (\n        (absx <= 1).type_as(absx)) + (-0.5 * absx3 + 2.5 * absx2 - 4 * absx + 2) * ((\n            (absx > 1) * (absx <= 2)).type_as(absx))\n\n\ndef calculate_weights_indices(in_length, out_length, scale, kernel, kernel_width, antialiasing):\n    if (scale < 1) and (antialiasing):\n        # Use a modified kernel to simultaneously interpolate and antialias- larger kernel width\n        kernel_width = kernel_width / scale\n\n    # Output-space coordinates\n    x = torch.linspace(1, out_length, out_length)\n\n    # Input-space coordinates. Calculate the inverse mapping such that 0.5\n    # in output space maps to 0.5 in input space, and 0.5+scale in output\n    # space maps to 1.5 in input space.\n    u = x / scale + 0.5 * (1 - 1 / scale)\n\n    # What is the left-most pixel that can be involved in the computation?\n    left = torch.floor(u - kernel_width / 2)\n\n    # What is the maximum number of pixels that can be involved in the\n    # computation?  Note: it's OK to use an extra pixel here; if the\n    # corresponding weights are all zero, it will be eliminated at the end\n    # of this function.\n    P = math.ceil(kernel_width) + 2\n\n    # The indices of the input pixels involved in computing the k-th output\n    # pixel are in row k of the indices matrix.\n    indices = left.view(out_length, 1).expand(out_length, P) + torch.linspace(0, P - 1, P).view(\n        1, P).expand(out_length, P)\n\n    # The weights used to compute the k-th output pixel are in row k of the\n    # weights matrix.\n    distance_to_center = u.view(out_length, 1).expand(out_length, P) - indices\n    # apply cubic kernel\n    if (scale < 1) and (antialiasing):\n        weights = scale * cubic(distance_to_center * scale)\n    else:\n        weights = cubic(distance_to_center)\n    # Normalize the weights matrix so that each row sums to 1.\n    weights_sum = torch.sum(weights, 1).view(out_length, 1)\n    weights = weights / weights_sum.expand(out_length, P)\n\n    # If a column in weights is all zero, get rid of it. only consider the first and last column.\n    weights_zero_tmp = torch.sum((weights == 0), 0)\n    if not math.isclose(weights_zero_tmp[0], 0, rel_tol=1e-6):\n        indices = indices.narrow(1, 1, P - 2)\n        weights = weights.narrow(1, 1, P - 2)\n    if not math.isclose(weights_zero_tmp[-1], 0, rel_tol=1e-6):\n        indices = indices.narrow(1, 0, P - 2)\n        weights = weights.narrow(1, 0, P - 2)\n    weights = weights.contiguous()\n    indices = indices.contiguous()\n    sym_len_s = -indices.min() + 1\n    sym_len_e = indices.max() - in_length\n    indices = indices + sym_len_s - 1\n    return weights, indices, int(sym_len_s), int(sym_len_e)\n\n\ndef imresize(img, scale, antialiasing=True):\n    # Now the scale should be the same for H and W\n    # input: img: CHW RGB [0,1]\n    # output: CHW RGB [0,1] w/o round\n\n    in_C, in_H, in_W = img.size()\n    _, out_H, out_W = in_C, math.ceil(in_H * scale), math.ceil(in_W * scale)\n    kernel_width = 4\n    kernel = 'cubic'\n\n    # Return the desired dimension order for performing the resize.  The\n    # strategy is to perform the resize first along the dimension with the\n    # smallest scale factor.\n    # Now we do not support this.\n\n    # get weights and indices\n    weights_H, indices_H, sym_len_Hs, sym_len_He = calculate_weights_indices(\n        in_H, out_H, scale, kernel, kernel_width, antialiasing)\n    weights_W, indices_W, sym_len_Ws, sym_len_We = calculate_weights_indices(\n        in_W, out_W, scale, kernel, kernel_width, antialiasing)\n    # process H dimension\n    # symmetric copying\n    img_aug = torch.FloatTensor(in_C, in_H + sym_len_Hs + sym_len_He, in_W)\n    img_aug.narrow(1, sym_len_Hs, in_H).copy_(img)\n\n    sym_patch = img[:, :sym_len_Hs, :]\n    inv_idx = torch.arange(sym_patch.size(1) - 1, -1, -1).long()\n    sym_patch_inv = sym_patch.index_select(1, inv_idx)\n    img_aug.narrow(1, 0, sym_len_Hs).copy_(sym_patch_inv)\n\n    sym_patch = img[:, -sym_len_He:, :]\n    inv_idx = torch.arange(sym_patch.size(1) - 1, -1, -1).long()\n    sym_patch_inv = sym_patch.index_select(1, inv_idx)\n    img_aug.narrow(1, sym_len_Hs + in_H, sym_len_He).copy_(sym_patch_inv)\n\n    out_1 = torch.FloatTensor(in_C, out_H, in_W)\n    kernel_width = weights_H.size(1)\n    for i in range(out_H):\n        idx = int(indices_H[i][0])\n        out_1[0, i, :] = img_aug[0, idx:idx + kernel_width, :].transpose(0, 1).mv(weights_H[i])\n        out_1[1, i, :] = img_aug[1, idx:idx + kernel_width, :].transpose(0, 1).mv(weights_H[i])\n        out_1[2, i, :] = img_aug[2, idx:idx + kernel_width, :].transpose(0, 1).mv(weights_H[i])\n\n    # process W dimension\n    # symmetric copying\n    out_1_aug = torch.FloatTensor(in_C, out_H, in_W + sym_len_Ws + sym_len_We)\n    out_1_aug.narrow(2, sym_len_Ws, in_W).copy_(out_1)\n\n    sym_patch = out_1[:, :, :sym_len_Ws]\n    inv_idx = torch.arange(sym_patch.size(2) - 1, -1, -1).long()\n    sym_patch_inv = sym_patch.index_select(2, inv_idx)\n    out_1_aug.narrow(2, 0, sym_len_Ws).copy_(sym_patch_inv)\n\n    sym_patch = out_1[:, :, -sym_len_We:]\n    inv_idx = torch.arange(sym_patch.size(2) - 1, -1, -1).long()\n    sym_patch_inv = sym_patch.index_select(2, inv_idx)\n    out_1_aug.narrow(2, sym_len_Ws + in_W, sym_len_We).copy_(sym_patch_inv)\n\n    out_2 = torch.FloatTensor(in_C, out_H, out_W)\n    kernel_width = weights_W.size(1)\n    for i in range(out_W):\n        idx = int(indices_W[i][0])\n        out_2[0, :, i] = out_1_aug[0, :, idx:idx + kernel_width].mv(weights_W[i])\n        out_2[1, :, i] = out_1_aug[1, :, idx:idx + kernel_width].mv(weights_W[i])\n        out_2[2, :, i] = out_1_aug[2, :, idx:idx + kernel_width].mv(weights_W[i])\n\n    return out_2\n\n\ndef imresize_np(img, scale, antialiasing=True):\n    # Now the scale should be the same for H and W\n    # input: img: Numpy, HWC BGR [0,1]\n    # output: HWC BGR [0,1] w/o round\n    img = torch.from_numpy(img)\n\n    in_H, in_W, in_C = img.size()\n    _, out_H, out_W = in_C, math.ceil(in_H * scale), math.ceil(in_W * scale)\n    kernel_width = 4\n    kernel = 'cubic'\n\n    # Return the desired dimension order for performing the resize.  The\n    # strategy is to perform the resize first along the dimension with the\n    # smallest scale factor.\n    # Now we do not support this.\n\n    # get weights and indices\n    weights_H, indices_H, sym_len_Hs, sym_len_He = calculate_weights_indices(\n        in_H, out_H, scale, kernel, kernel_width, antialiasing)\n    weights_W, indices_W, sym_len_Ws, sym_len_We = calculate_weights_indices(\n        in_W, out_W, scale, kernel, kernel_width, antialiasing)\n    # process H dimension\n    # symmetric copying\n    img_aug = torch.FloatTensor(in_H + sym_len_Hs + sym_len_He, in_W, in_C)\n    img_aug.narrow(0, sym_len_Hs, in_H).copy_(img)\n\n    sym_patch = img[:sym_len_Hs, :, :]\n    inv_idx = torch.arange(sym_patch.size(0) - 1, -1, -1).long()\n    sym_patch_inv = sym_patch.index_select(0, inv_idx)\n    img_aug.narrow(0, 0, sym_len_Hs).copy_(sym_patch_inv)\n\n    sym_patch = img[-sym_len_He:, :, :]\n    inv_idx = torch.arange(sym_patch.size(0) - 1, -1, -1).long()\n    sym_patch_inv = sym_patch.index_select(0, inv_idx)\n    img_aug.narrow(0, sym_len_Hs + in_H, sym_len_He).copy_(sym_patch_inv)\n\n    out_1 = torch.FloatTensor(out_H, in_W, in_C)\n    kernel_width = weights_H.size(1)\n    for i in range(out_H):\n        idx = int(indices_H[i][0])\n        out_1[i, :, 0] = img_aug[idx:idx + kernel_width, :, 0].transpose(0, 1).mv(weights_H[i])\n        out_1[i, :, 1] = img_aug[idx:idx + kernel_width, :, 1].transpose(0, 1).mv(weights_H[i])\n        out_1[i, :, 2] = img_aug[idx:idx + kernel_width, :, 2].transpose(0, 1).mv(weights_H[i])\n\n    # process W dimension\n    # symmetric copying\n    out_1_aug = torch.FloatTensor(out_H, in_W + sym_len_Ws + sym_len_We, in_C)\n    out_1_aug.narrow(1, sym_len_Ws, in_W).copy_(out_1)\n\n    sym_patch = out_1[:, :sym_len_Ws, :]\n    inv_idx = torch.arange(sym_patch.size(1) - 1, -1, -1).long()\n    sym_patch_inv = sym_patch.index_select(1, inv_idx)\n    out_1_aug.narrow(1, 0, sym_len_Ws).copy_(sym_patch_inv)\n\n    sym_patch = out_1[:, -sym_len_We:, :]\n    inv_idx = torch.arange(sym_patch.size(1) - 1, -1, -1).long()\n    sym_patch_inv = sym_patch.index_select(1, inv_idx)\n    out_1_aug.narrow(1, sym_len_Ws + in_W, sym_len_We).copy_(sym_patch_inv)\n\n    out_2 = torch.FloatTensor(out_H, out_W, in_C)\n    kernel_width = weights_W.size(1)\n    for i in range(out_W):\n        idx = int(indices_W[i][0])\n        out_2[:, i, 0] = out_1_aug[:, idx:idx + kernel_width, 0].mv(weights_W[i])\n        out_2[:, i, 1] = out_1_aug[:, idx:idx + kernel_width, 1].mv(weights_W[i])\n        out_2[:, i, 2] = out_1_aug[:, idx:idx + kernel_width, 2].mv(weights_W[i])\n\n    return out_2.numpy()\n\n\nif __name__ == '__main__':\n    # test imresize function\n    # read images\n    img = cv2.imread('test.png')\n    img = img * 1.0 / 255\n    img = torch.from_numpy(np.transpose(img[:, :, [2, 1, 0]], (2, 0, 1))).float()\n    # imresize\n    scale = 1 / 4\n    import time\n    total_time = 0\n    for i in range(10):\n        start_time = time.time()\n        rlt = imresize(img, scale, antialiasing=True)\n        use_time = time.time() - start_time\n        total_time += use_time\n    print('average time: {}'.format(total_time / 10))\n\n    import torchvision.utils\n    torchvision.utils.save_image((rlt * 255).round() / 255, 'rlt.png', nrow=1, padding=0,\n                                 normalize=False)\n"}
{"type": "source_file", "path": "code/utils/face_swap.py", "content": "#! /usr/bin/env python\nimport cv2\nimport numpy as np\nimport scipy.spatial as spatial\nimport logging\n\n\n## 3D Transform\ndef bilinear_interpolate(img, coords):\n    \"\"\" Interpolates over every image channel\n    http://en.wikipedia.org/wiki/Bilinear_interpolation\n    :param img: max 3 channel image\n    :param coords: 2 x _m_ array. 1st row = xcoords, 2nd row = ycoords\n    :returns: array of interpolated pixels with same shape as coords\n    \"\"\"\n    int_coords = np.int32(coords)\n    x0, y0 = int_coords\n    dx, dy = coords - int_coords\n\n    # 4 Neighour pixels\n    q11 = img[y0, x0]\n    q21 = img[y0, x0 + 1]\n    q12 = img[y0 + 1, x0]\n    q22 = img[y0 + 1, x0 + 1]\n\n    btm = q21.T * dx + q11.T * (1 - dx)\n    top = q22.T * dx + q12.T * (1 - dx)\n    inter_pixel = top * dy + btm * (1 - dy)\n\n    return inter_pixel.T\n\ndef grid_coordinates(points):\n    \"\"\" x,y grid coordinates within the ROI of supplied points\n    :param points: points to generate grid coordinates\n    :returns: array of (x, y) coordinates\n    \"\"\"\n    xmin = np.min(points[:, 0])\n    xmax = np.max(points[:, 0]) + 1\n    ymin = np.min(points[:, 1])\n    ymax = np.max(points[:, 1]) + 1\n\n    return np.asarray([(x, y) for y in range(ymin, ymax)\n                       for x in range(xmin, xmax)], np.uint32)\n\n\ndef process_warp(src_img, result_img, tri_affines, dst_points, delaunay):\n    \"\"\"\n    Warp each triangle from the src_image only within the\n    ROI of the destination image (points in dst_points).\n    \"\"\"\n    roi_coords = grid_coordinates(dst_points)\n    # indices to vertices. -1 if pixel is not in any triangle\n    roi_tri_indices = delaunay.find_simplex(roi_coords)\n\n    for simplex_index in range(len(delaunay.simplices)):\n        coords = roi_coords[roi_tri_indices == simplex_index]\n        num_coords = len(coords)\n        out_coords = np.dot(tri_affines[simplex_index],\n                            np.vstack((coords.T, np.ones(num_coords))))\n        x, y = coords.T\n        result_img[y, x] = bilinear_interpolate(src_img, out_coords)\n\n    return None\n\n\ndef triangular_affine_matrices(vertices, src_points, dst_points):\n    \"\"\"\n    Calculate the affine transformation matrix for each\n    triangle (x,y) vertex from dst_points to src_points\n    :param vertices: array of triplet indices to corners of triangle\n    :param src_points: array of [x, y] points to landmarks for source image\n    :param dst_points: array of [x, y] points to landmarks for destination image\n    :returns: 2 x 3 affine matrix transformation for a triangle\n    \"\"\"\n    ones = [1, 1, 1]\n    for tri_indices in vertices:\n        src_tri = np.vstack((src_points[tri_indices, :].T, ones))\n        dst_tri = np.vstack((dst_points[tri_indices, :].T, ones))\n        mat = np.dot(src_tri, np.linalg.inv(dst_tri))[:2, :]\n        yield mat\n\n\ndef warp_image_3d(src_img, src_points, dst_points, dst_shape, dtype=np.uint8):\n    rows, cols = dst_shape[:2]\n    result_img = np.zeros((rows, cols, 3), dtype=dtype)\n\n    delaunay = spatial.Delaunay(dst_points)\n    tri_affines = np.asarray(list(triangular_affine_matrices(\n        delaunay.simplices, src_points, dst_points)))\n\n    process_warp(src_img, result_img, tri_affines, dst_points, delaunay)\n\n    return result_img\n\n\n## 2D Transform\ndef transformation_from_points(points1, points2):\n    points1 = points1.astype(np.float64)\n    points2 = points2.astype(np.float64)\n\n    c1 = np.mean(points1, axis=0)\n    c2 = np.mean(points2, axis=0)\n    points1 -= c1\n    points2 -= c2\n\n    s1 = np.std(points1)\n    s2 = np.std(points2)\n    points1 /= s1\n    points2 /= s2\n\n    U, S, Vt = np.linalg.svd(np.dot(points1.T, points2))\n    R = (np.dot(U, Vt)).T\n\n    return np.vstack([np.hstack([s2 / s1 * R,\n                                (c2.T - np.dot(s2 / s1 * R, c1.T))[:, np.newaxis]]),\n                      np.array([[0., 0., 1.]])])\n\n\ndef warp_image_2d(im, M, dshape):\n    output_im = np.zeros(dshape, dtype=im.dtype)\n    cv2.warpAffine(im,\n                   M[:2],\n                   (dshape[1], dshape[0]),\n                   dst=output_im,\n                   borderMode=cv2.BORDER_TRANSPARENT,\n                   flags=cv2.WARP_INVERSE_MAP)\n\n    return output_im\n\n\n## Generate Mask\ndef mask_from_points(size, points,erode_flag=1):\n    radius = 10  # kernel size\n    kernel = np.ones((radius, radius), np.uint8)\n\n    mask = np.zeros(size, np.uint8)\n    cv2.fillConvexPoly(mask, cv2.convexHull(points), 255)\n    if erode_flag:\n        mask = cv2.erode(mask, kernel,iterations=1)\n\n    return mask\n\n\n## Color Correction\ndef correct_colours(im1, im2, landmarks1):\n    COLOUR_CORRECT_BLUR_FRAC = 0.75\n    LEFT_EYE_POINTS = list(range(42, 48))\n    RIGHT_EYE_POINTS = list(range(36, 42))\n\n    blur_amount = COLOUR_CORRECT_BLUR_FRAC * np.linalg.norm(\n                              np.mean(landmarks1[LEFT_EYE_POINTS], axis=0) -\n                              np.mean(landmarks1[RIGHT_EYE_POINTS], axis=0))\n    blur_amount = int(blur_amount)\n    if blur_amount % 2 == 0:\n        blur_amount += 1\n    im1_blur = cv2.GaussianBlur(im1, (blur_amount, blur_amount), 0)\n    im2_blur = cv2.GaussianBlur(im2, (blur_amount, blur_amount), 0)\n\n    # Avoid divide-by-zero errors.\n    im2_blur = im2_blur.astype(int)\n    im2_blur += 128*(im2_blur <= 1)\n\n    result = im2.astype(np.float64) * im1_blur.astype(np.float64) / im2_blur.astype(np.float64)\n    result = np.clip(result, 0, 255).astype(np.uint8)\n\n    return result\n\n\n## Copy-and-paste\ndef apply_mask(img, mask):\n    \"\"\" Apply mask to supplied image\n    :param img: max 3 channel image\n    :param mask: [0-255] values in mask\n    :returns: new image with mask applied\n    \"\"\"\n    masked_img=cv2.bitwise_and(img,img,mask=mask)\n\n    return masked_img\n\n\n## Alpha blending\ndef alpha_feathering(src_img, dest_img, img_mask, blur_radius=15):\n    mask = cv2.blur(img_mask, (blur_radius, blur_radius))\n    mask = mask / 255.0\n\n    result_img = np.empty(src_img.shape, np.uint8)\n    for i in range(3):\n        result_img[..., i] = src_img[..., i] * mask + dest_img[..., i] * (1-mask)\n\n    return result_img\n\n\ndef check_points(img,points):\n    # Todo: I just consider one situation.\n    if points[8,1]>img.shape[0]:\n        logging.error(\"Jaw part out of image\")\n    else:\n        return True\n    return False\n\n\ndef face_swap(src_face, dst_face, src_points, dst_points, dst_shape, dst_img, end=48):\n    h, w = dst_face.shape[:2]\n\n    correct_color = False\n    warp_2d = True\n\n    ## 3d warp\n    warped_src_face = warp_image_3d(src_face, src_points[:end], dst_points[:end], (h, w))\n    ## Mask for blending\n    mask = mask_from_points((h, w), dst_points)\n    mask_src = np.mean(warped_src_face, axis=2) > 0\n    mask = np.asarray(mask * mask_src, dtype=np.uint8)\n    ## Correct color\n    if correct_color:\n        warped_src_face = apply_mask(warped_src_face, mask)\n        dst_face_masked = apply_mask(dst_face, mask)\n        warped_src_face = correct_colours(dst_face_masked, warped_src_face, dst_points)\n    ## 2d warp\n    if warp_2d:\n        unwarped_src_face = warp_image_3d(warped_src_face, dst_points[:end], src_points[:end], src_face.shape[:2])\n        warped_src_face = warp_image_2d(unwarped_src_face, transformation_from_points(dst_points, src_points),\n                                        (h, w, 3))\n\n        mask = mask_from_points((h, w), dst_points)\n        mask_src = np.mean(warped_src_face, axis=2) > 0\n        mask = np.asarray(mask * mask_src, dtype=np.uint8)\n\n    ## Shrink the mask\n    kernel = np.ones((10, 10), np.uint8)\n    mask = cv2.erode(mask, kernel, iterations=1)\n    cv2.imwrite(\"/userhome/NewIBSN/IBSN_SepMark/mask.png\", mask)\n    ##Poisson Blending\n    r = cv2.boundingRect(mask)\n    center = ((r[0] + int(r[2] / 2), r[1] + int(r[3] / 2)))\n    output = cv2.seamlessClone(warped_src_face, dst_face, mask, center, cv2.NORMAL_CLONE)\n\n    x, y, w, h = dst_shape\n    dst_img_cp = dst_img.copy()\n    dst_img_cp[y:y + h, x:x + w] = output\n\n    return dst_img_cp\n"}
{"type": "source_file", "path": "code/utils/JPEG_utils.py", "content": "# Standard libraries\nimport numpy as np\n# PyTorch\nimport torch\nimport torch.nn as nn\nimport math\n\ny_table = np.array(\n    [[16, 11, 10, 16, 24, 40, 51, 61], [12, 12, 14, 19, 26, 58, 60,\n                                        55], [14, 13, 16, 24, 40, 57, 69, 56],\n     [14, 17, 22, 29, 51, 87, 80, 62], [18, 22, 37, 56, 68, 109, 103,\n                                        77], [24, 35, 55, 64, 81, 104, 113, 92],\n     [49, 64, 78, 87, 103, 121, 120, 101], [72, 92, 95, 98, 112, 100, 103, 99]],\n    dtype=np.float32).T\n\ny_table = nn.Parameter(torch.from_numpy(y_table))\n#\nc_table = np.empty((8, 8), dtype=np.float32)\nc_table.fill(99)\nc_table[:4, :4] = np.array([[17, 18, 24, 47], [18, 21, 26, 66],\n                            [24, 26, 56, 99], [47, 66, 99, 99]]).T\nc_table = nn.Parameter(torch.from_numpy(c_table))\n\n\ndef diff_round_back(x):\n    \"\"\" Differentiable rounding function\n    Input:\n        x(tensor)\n    Output:\n        x(tensor)\n    \"\"\"\n    return torch.round(x) + (x - torch.round(x))**3\n\n\n\ndef diff_round(input_tensor):\n    test = 0\n    for n in range(1, 10):\n        test += math.pow(-1, n+1) / n * torch.sin(2 * math.pi * n * input_tensor)\n    final_tensor = input_tensor - 1 / math.pi * test\n    return final_tensor\n\n\nclass Quant(torch.autograd.Function):\n\n    @staticmethod\n    def forward(ctx, input):\n        input = torch.clamp(input, 0, 1)\n        output = (input * 255.).round() / 255.\n        return output\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        return grad_output\n\nclass Quantization(nn.Module):\n    def __init__(self):\n        super(Quantization, self).__init__()\n\n    def forward(self, input):\n        return Quant.apply(input)\n\n\ndef quality_to_factor(quality):\n    \"\"\" Calculate factor corresponding to quality\n    Input:\n        quality(float): Quality for jpeg compression\n    Output:\n        factor(float): Compression factor\n    \"\"\"\n    if quality < 50:\n        quality = 5000. / quality\n    else:\n        quality = 200. - quality*2\n    return quality / 100."}
{"type": "source_file", "path": "code/models/bitnetwork/Decoder_U.py", "content": "from . import *\n\n\nclass DW_Decoder(nn.Module):\n\n    def __init__(self, message_length, blocks=2, channels=64, attention=None):\n        super(DW_Decoder, self).__init__()\n\n        self.conv1 = ConvBlock(3, 16, blocks=blocks)\n        self.down1 = Down(16, 32, blocks=blocks)\n        self.down2 = Down(32, 64, blocks=blocks)\n        self.down3 = Down(64, 128, blocks=blocks)\n\n        self.down4 = Down(128, 256, blocks=blocks)\n\n        self.up3 = UP(256, 128)\n        self.att3 = ResBlock(128 * 2, 128, blocks=blocks, attention=attention)\n\n        self.up2 = UP(128, 64)\n        self.att2 = ResBlock(64 * 2, 64, blocks=blocks, attention=attention)\n\n        self.up1 = UP(64, 32)\n        self.att1 = ResBlock(32 * 2, 32, blocks=blocks, attention=attention)\n\n        self.up0 = UP(32, 16)\n        self.att0 = ResBlock(16 * 2, 16, blocks=blocks, attention=attention)\n\n        self.Conv_1x1 = nn.Conv2d(16, 1, kernel_size=1, stride=1, padding=0, bias=False)\n\n        self.message_layer = nn.Linear(message_length * message_length, message_length)\n        self.message_length = message_length\n\n\n    def forward(self, x):\n        d0 = self.conv1(x)\n        d1 = self.down1(d0)\n        d2 = self.down2(d1)\n        d3 = self.down3(d2)\n\n        d4 = self.down4(d3)\n\n        u3 = self.up3(d4)\n        u3 = torch.cat((d3, u3), dim=1)\n        u3 = self.att3(u3)\n\n        u2 = self.up2(u3)\n        u2 = torch.cat((d2, u2), dim=1)\n        u2 = self.att2(u2)\n\n        u1 = self.up1(u2)\n        u1 = torch.cat((d1, u1), dim=1)\n        u1 = self.att1(u1)\n\n        u0 = self.up0(u1)\n        u0 = torch.cat((d0, u0), dim=1)\n        u0 = self.att0(u0)\n\n        residual = self.Conv_1x1(u0)\n\n        message = F.interpolate(residual, size=(self.message_length, self.message_length),\n                                                           mode='nearest')\n        message = message.view(message.shape[0], -1)\n        message = self.message_layer(message)\n\n        return message\n\n\nclass Down(nn.Module):\n    def __init__(self, in_channels, out_channels, blocks):\n        super(Down, self).__init__()\n        self.layer = torch.nn.Sequential(\n            ConvBlock(in_channels, in_channels, stride=2),\n            ConvBlock(in_channels, out_channels, blocks=blocks)\n        )\n\n    def forward(self, x):\n        return self.layer(x)\n\n\nclass UP(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(UP, self).__init__()\n        self.conv = ConvBlock(in_channels, out_channels)\n\n    def forward(self, x):\n        x = F.interpolate(x, scale_factor=2, mode='nearest')\n        return self.conv(x)\n"}
{"type": "source_file", "path": "code/models/bitnetwork/ConvBlock.py", "content": "import torch.nn as nn\n\n\nclass ConvINRelu(nn.Module):\n\t\"\"\"\n\tA sequence of Convolution, Instance Normalization, and ReLU activation\n\t\"\"\"\n\n\tdef __init__(self, channels_in, channels_out, stride):\n\t\tsuper(ConvINRelu, self).__init__()\n\n\t\tself.layers = nn.Sequential(\n\t\t\tnn.Conv2d(channels_in, channels_out, 3, stride, padding=1),\n\t\t\tnn.InstanceNorm2d(channels_out),\n\t\t\tnn.ReLU(inplace=True)\n\t\t)\n\n\tdef forward(self, x):\n\t\treturn self.layers(x)\n\n\nclass ConvBlock(nn.Module):\n\t'''\n\tNetwork that composed by layers of ConvINRelu\n\t'''\n\n\tdef __init__(self, in_channels, out_channels, blocks=1, stride=1):\n\t\tsuper(ConvBlock, self).__init__()\n\n\t\tlayers = [ConvINRelu(in_channels, out_channels, stride)] if blocks != 0 else []\n\t\tfor _ in range(blocks - 1):\n\t\t\tlayer = ConvINRelu(out_channels, out_channels, 1)\n\t\t\tlayers.append(layer)\n\n\t\tself.layers = nn.Sequential(*layers)\n\n\tdef forward(self, x):\n\t\treturn self.layers(x)\n"}
{"type": "source_file", "path": "code/train.py", "content": "import os\nimport math\nimport argparse\nimport random\nimport logging\n\nimport torch\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\nfrom data.data_sampler import DistIterSampler\n\nimport options.options as option\nfrom utils import util\nfrom data import create_dataloader, create_dataset\nfrom models import create_model\n\n\ndef init_dist(backend='nccl', **kwargs):\n    ''' initialization for distributed training'''\n    # if mp.get_start_method(allow_none=True) is None:\n    if mp.get_start_method(allow_none=True) != 'spawn':\n        mp.set_start_method('spawn')\n    rank = int(os.environ['RANK'])\n    num_gpus = torch.cuda.device_count()\n    torch.cuda.set_device(rank % num_gpus)\n    dist.init_process_group(backend=backend, **kwargs)\n\ndef cal_pnsr(sr_img, gt_img):\n    # calculate PSNR\n    gt_img = gt_img / 255.\n    sr_img = sr_img / 255.\n    psnr = util.calculate_psnr(sr_img * 255, gt_img * 255)\n\n    return psnr\n\ndef main():\n    # options\n    parser = argparse.ArgumentParser()\n    parser.add_argument('-opt', type=str, help='Path to option YMAL file.')  # config 文件\n    parser.add_argument('--launcher', choices=['none', 'pytorch'], default='none',\n                        help='job launcher')\n    parser.add_argument('--local_rank', type=int, default=0)\n    args = parser.parse_args()\n    opt = option.parse(args.opt, is_train=True)\n\n    # distributed training settings\n    if args.launcher == 'none':  # disabled distributed training\n        opt['dist'] = False\n        rank = -1\n        print('Disabled distributed training.')\n    else:\n        opt['dist'] = True\n        init_dist()\n        world_size = torch.distributed.get_world_size()\n        rank = torch.distributed.get_rank()\n\n    # loading resume state if exists\n    if opt['path'].get('resume_state', None):\n        # distributed resuming: all load into default GPU\n        device_id = torch.cuda.current_device()\n        resume_state = torch.load(opt['path']['resume_state'],\n                                  map_location=lambda storage, loc: storage.cuda(device_id))\n        # resume_state = torch.load(opt['path']['resume_state'],\n        #                           map_location=lambda storage, loc: storage.cuda(device_id), strict=False)\n        option.check_resume(opt, resume_state['iter'])  # check resume options\n    else:\n        resume_state = None\n\n    # mkdir and loggers\n    if rank <= 0:  # normal training (rank -1) OR distributed training (rank 0)\n        if resume_state is None:\n            util.mkdir_and_rename(\n                opt['path']['experiments_root'])  # rename experiment folder if exists\n            util.mkdirs((path for key, path in opt['path'].items() if not key == 'experiments_root'\n                         and 'pretrain_model' not in key and 'resume' not in key))\n\n        # config loggers. Before it, the log will not work\n        util.setup_logger('base', opt['path']['log'], 'train_' + opt['name'], level=logging.INFO,\n                          screen=True, tofile=True)\n        util.setup_logger('val', opt['path']['log'], 'val_' + opt['name'], level=logging.INFO,\n                          screen=True, tofile=True)\n        logger = logging.getLogger('base')\n        logger.info(option.dict2str(opt))\n        # tensorboard logger\n        if opt['use_tb_logger'] and 'debug' not in opt['name']:\n            version = float(torch.__version__[0:3])\n            if version >= 1.1:  # PyTorch 1.1\n                from torch.utils.tensorboard import SummaryWriter\n            else:\n                logger.info(\n                    'You are using PyTorch {}. Tensorboard will use [tensorboardX]'.format(version))\n                from tensorboardX import SummaryWriter\n            tb_logger = SummaryWriter(log_dir='../tb_logger/' + opt['name'])\n    else:\n        util.setup_logger('base', opt['path']['log'], 'train', level=logging.INFO, screen=True)\n        logger = logging.getLogger('base')\n\n    # convert to NoneDict, which returns None for missing keys\n    opt = option.dict_to_nonedict(opt)\n\n    # random seed\n    seed = opt['train']['manual_seed']\n    if seed is None:\n        seed = random.randint(1, 10000)\n    if rank <= 0:\n        logger.info('Random seed: {}'.format(seed))\n    util.set_random_seed(seed)\n\n    torch.backends.cudnn.benchmark = True\n    # torch.backends.cudnn.deterministic = True\n\n    #### create train and val dataloader\n    dataset_ratio = 200  # enlarge the size of each epoch\n    for phase, dataset_opt in opt['datasets'].items():\n        if phase == 'train':\n            train_set = create_dataset(dataset_opt)\n            train_size = int(math.ceil(len(train_set) / dataset_opt['batch_size']))\n            total_iters = int(opt['train']['niter'])\n            total_epochs = int(math.ceil(total_iters / train_size))\n            if opt['dist']:\n                train_sampler = DistIterSampler(train_set, world_size, rank, dataset_ratio)\n                total_epochs = int(math.ceil(total_iters / (train_size * dataset_ratio)))\n            else:\n                train_sampler = None\n            train_loader = create_dataloader(train_set, dataset_opt, opt, train_sampler)\n            if rank <= 0:\n                logger.info('Number of train images: {:,d}, iters: {:,d}'.format(\n                    len(train_set), train_size))\n                logger.info('Total epochs needed: {:d} for iters {:,d}'.format(\n                    total_epochs, total_iters))\n        elif phase == 'val':\n            val_set = create_dataset(dataset_opt)\n            val_loader = create_dataloader(val_set, dataset_opt, opt, None)\n            if rank <= 0:\n                logger.info('Number of val images in [{:s}]: {:d}'.format(\n                    dataset_opt['name'], len(val_set)))\n        else:\n            raise NotImplementedError('Phase [{:s}] is not recognized.'.format(phase))\n    assert train_loader is not None\n\n    # create model\n    model = create_model(opt)\n    # resume training\n    if resume_state:\n        logger.info('Resuming training from epoch: {}, iter: {}.'.format(\n            resume_state['epoch'], resume_state['iter']))\n\n        start_epoch = resume_state['epoch']\n        current_step = resume_state['iter']\n        model.resume_training(resume_state)  # handle optimizers and schedulers\n    else:\n        current_step = 0\n        start_epoch = 0\n\n    # training\n    logger.info('Start training from epoch: {:d}, iter: {:d}'.format(start_epoch, current_step))\n    for epoch in range(start_epoch, total_epochs + 1):\n        if opt['dist']:\n            train_sampler.set_epoch(epoch)\n        for _, train_data in enumerate(train_loader):\n            current_step += 1\n            if current_step > total_iters:\n                break\n            # training\n            model.feed_data(train_data)\n            model.optimize_parameters(current_step)\n\n            # update learning rate\n            model.update_learning_rate(current_step, warmup_iter=opt['train']['warmup_iter'])\n\n            # log\n            if current_step % opt['logger']['print_freq'] == 0:\n                logs = model.get_current_log()\n                message = '<epoch:{:3d}, iter:{:8,d}, lr:{:.3e}> '.format(\n                    epoch, current_step, model.get_current_learning_rate())\n                for k, v in logs.items():\n                    message += '{:s}: {:.4e} '.format(k, v)\n                    # tensorboard logger\n                    if opt['use_tb_logger'] and 'debug' not in opt['name']:\n                        if rank <= 0:\n                            tb_logger.add_scalar(k, v, current_step)\n                if rank <= 0:\n                    logger.info(message)\n\n            # validation\n            if current_step % opt['train']['val_freq'] == 0 and rank <= 0:\n                avg_psnr = 0.0\n                avg_psnr_h = [0.0]*opt['num_image']\n                avg_psnr_lr = 0.0\n                avg_biterr = 0.0\n                idx = 0\n                for image_id, val_data in enumerate(val_loader):\n                    img_dir = os.path.join(opt['path']['val_images'])\n                    util.mkdir(img_dir)\n\n                    model.feed_data(val_data)\n                    model.test(image_id)\n\n                    visuals = model.get_current_visuals()\n\n                    t_step = visuals['SR'].shape[0]\n                    idx += t_step\n                    n = len(visuals['SR_h'])\n\n                    avg_biterr += util.decoded_message_error_rate_batch(visuals['recmessage'][0], visuals['message'][0])\n\n                    for i in range(t_step):\n\n                        sr_img = util.tensor2img(visuals['SR'][i])  # uint8\n                        sr_img_h = []\n                        for j in range(n):\n                            sr_img_h.append(util.tensor2img(visuals['SR_h'][j][i]))  # uint8\n                        gt_img = util.tensor2img(visuals['GT'][i])  # uint8\n                        lr_img = util.tensor2img(visuals['LR'][i])\n                        lrgt_img = []\n                        for j in range(n):\n                            lrgt_img.append(util.tensor2img(visuals['LR_ref'][j][i]))\n\n                        # Save SR images for reference\n                        save_img_path = os.path.join(img_dir,'{:d}_{:d}_{:s}.png'.format(image_id, i, 'SR'))\n                        util.save_img(sr_img, save_img_path)\n\n                        for j in range(n):\n                            save_img_path = os.path.join(img_dir,'{:d}_{:d}_{:d}_{:s}.png'.format(image_id, i, j, 'SR_h'))\n                            util.save_img(sr_img_h[j], save_img_path)\n\n                        save_img_path = os.path.join(img_dir,'{:d}_{:d}_{:s}.png'.format(image_id, i, 'GT'))\n                        util.save_img(gt_img, save_img_path)\n\n                        save_img_path = os.path.join(img_dir,'{:d}_{:d}_{:s}.png'.format(image_id, i, 'LR'))\n                        util.save_img(lr_img, save_img_path)\n\n                        for j in range(n):\n                            save_img_path = os.path.join(img_dir,'{:d}_{:d}_{:d}_{:s}.png'.format(image_id, i, j, 'LRGT'))\n                            util.save_img(lrgt_img[j], save_img_path)\n\n                        psnr = cal_pnsr(sr_img, gt_img)\n                        psnr_h = []\n                        for j in range(n):\n                            psnr_h.append(cal_pnsr(sr_img_h[j], lrgt_img[j]))\n                        psnr_lr = cal_pnsr(lr_img, gt_img)\n\n                        avg_psnr += psnr\n                        for j in range(n):\n                            avg_psnr_h[j] += psnr_h[j]\n                        avg_psnr_lr += psnr_lr\n\n                avg_psnr = avg_psnr / idx\n                avg_psnr_h = [psnr / idx for psnr in avg_psnr_h]\n                avg_psnr_lr = avg_psnr_lr / idx\n                avg_biterr = avg_biterr / idx\n\n                # log\n                res_psnr_h = ''\n                for p in avg_psnr_h:\n                    res_psnr_h+=('_{:.4e}'.format(p))\n\n                logger.info('# Validation # PSNR_Cover: {:.4e}, PSNR_Secret: {:s}, PSNR_Stego: {:.4e}, Bit_acc: {: .4e}'.format(avg_psnr, res_psnr_h, avg_psnr_lr, avg_biterr))\n                logger_val = logging.getLogger('val')  # validation logger\n                logger_val.info('<epoch:{:3d}, iter:{:8,d}> PSNR_Cover: {:.4e}, PSNR_Secret: {:s}, PSNR_Stego: {:.4e}, Bit_acc: {: .4e}'.format(\n                    epoch, current_step, avg_psnr, res_psnr_h, avg_psnr_lr, avg_biterr))\n                # tensorboard logger\n                if opt['use_tb_logger'] and 'debug' not in opt['name']:\n                    tb_logger.add_scalar('psnr', avg_psnr, current_step)\n\n            # save models and training states\n            if current_step % opt['logger']['save_checkpoint_freq'] == 0:\n                if rank <= 0:\n                    logger.info('Saving models and training states.')\n                    model.save(current_step)\n                    model.save_training_state(epoch, current_step)\n\n    if rank <= 0:\n        logger.info('Saving the final model.')\n        model.save('latest')\n        logger.info('End of training.')\n\n\nif __name__ == '__main__':\n    main()\n"}
{"type": "source_file", "path": "code/models/discrim.py", "content": "from torch import nn as nn\nfrom torch.nn import functional as F\nfrom torch.nn.utils import spectral_norm\n\n\nclass UNetDiscriminatorSN(nn.Module):\n    \"\"\"Defines a U-Net discriminator with spectral normalization (SN)\n\n    It is used in Real-ESRGAN: Training Real-World Blind Super-Resolution with Pure Synthetic Data.\n\n    Arg:\n        num_in_ch (int): Channel number of inputs. Default: 3.\n        num_feat (int): Channel number of base intermediate features. Default: 64.\n        skip_connection (bool): Whether to use skip connections between U-Net. Default: True.\n    \"\"\"\n\n    def __init__(self, num_in_ch, num_feat=64, skip_connection=True):\n        super(UNetDiscriminatorSN, self).__init__()\n        self.skip_connection = skip_connection\n        norm = spectral_norm\n        # the first convolution\n        self.conv0 = nn.Conv2d(num_in_ch, num_feat, kernel_size=3, stride=1, padding=1)\n        # downsample\n        self.conv1 = norm(nn.Conv2d(num_feat, num_feat * 2, 4, 2, 1, bias=False))\n        self.conv2 = norm(nn.Conv2d(num_feat * 2, num_feat * 4, 4, 2, 1, bias=False))\n        self.conv3 = norm(nn.Conv2d(num_feat * 4, num_feat * 8, 4, 2, 1, bias=False))\n        # upsample\n        self.conv4 = norm(nn.Conv2d(num_feat * 8, num_feat * 4, 3, 1, 1, bias=False))\n        self.conv5 = norm(nn.Conv2d(num_feat * 4, num_feat * 2, 3, 1, 1, bias=False))\n        self.conv6 = norm(nn.Conv2d(num_feat * 2, num_feat, 3, 1, 1, bias=False))\n        # extra convolutions\n        self.conv7 = norm(nn.Conv2d(num_feat, num_feat, 3, 1, 1, bias=False))\n        self.conv8 = norm(nn.Conv2d(num_feat, num_feat, 3, 1, 1, bias=False))\n        self.conv9 = nn.Conv2d(num_feat, 1, 3, 1, 1)\n\n    def forward(self, x):\n        # downsample\n        x0 = F.leaky_relu(self.conv0(x), negative_slope=0.2, inplace=True)\n        x1 = F.leaky_relu(self.conv1(x0), negative_slope=0.2, inplace=True)\n        x2 = F.leaky_relu(self.conv2(x1), negative_slope=0.2, inplace=True)\n        x3 = F.leaky_relu(self.conv3(x2), negative_slope=0.2, inplace=True)\n\n        # upsample\n        x3 = F.interpolate(x3, scale_factor=2, mode='bilinear', align_corners=False)\n        x4 = F.leaky_relu(self.conv4(x3), negative_slope=0.2, inplace=True)\n\n        if self.skip_connection:\n            x4 = x4 + x2\n        x4 = F.interpolate(x4, scale_factor=2, mode='bilinear', align_corners=False)\n        x5 = F.leaky_relu(self.conv5(x4), negative_slope=0.2, inplace=True)\n\n        if self.skip_connection:\n            x5 = x5 + x1\n        x5 = F.interpolate(x5, scale_factor=2, mode='bilinear', align_corners=False)\n        x6 = F.leaky_relu(self.conv6(x5), negative_slope=0.2, inplace=True)\n\n        if self.skip_connection:\n            x6 = x6 + x0\n\n        # extra convolutions\n        out = F.leaky_relu(self.conv7(x6), negative_slope=0.2, inplace=True)\n        out = F.leaky_relu(self.conv8(out), negative_slope=0.2, inplace=True)\n        out = self.conv9(out)\n\n        return out\n    \n\nclass GANLoss(nn.Module):\n    \"\"\"Define GAN loss.\n\n    Args:\n        gan_type (str): Support 'vanilla', 'lsgan', 'wgan', 'hinge'.\n        real_label_val (float): The value for real label. Default: 1.0.\n        fake_label_val (float): The value for fake label. Default: 0.0.\n        loss_weight (float): Loss weight. Default: 1.0.\n            Note that loss_weight is only for generators; and it is always 1.0\n            for discriminators.\n    \"\"\"\n\n    def __init__(self, gan_type, real_label_val=1.0, fake_label_val=0.0, loss_weight=1.0):\n        super(GANLoss, self).__init__()\n        self.gan_type = gan_type\n        self.loss_weight = loss_weight\n        self.real_label_val = real_label_val\n        self.fake_label_val = fake_label_val\n\n        if self.gan_type == 'vanilla':\n            self.loss = nn.BCEWithLogitsLoss()\n        elif self.gan_type == 'lsgan':\n            self.loss = nn.MSELoss()\n        elif self.gan_type == 'wgan':\n            self.loss = self._wgan_loss\n        elif self.gan_type == 'wgan_softplus':\n            self.loss = self._wgan_softplus_loss\n        elif self.gan_type == 'hinge':\n            self.loss = nn.ReLU()\n        else:\n            raise NotImplementedError(f'GAN type {self.gan_type} is not implemented.')\n\n    def _wgan_loss(self, input, target):\n        \"\"\"wgan loss.\n\n        Args:\n            input (Tensor): Input tensor.\n            target (bool): Target label.\n\n        Returns:\n            Tensor: wgan loss.\n        \"\"\"\n        return -input.mean() if target else input.mean()\n\n    def _wgan_softplus_loss(self, input, target):\n        \"\"\"wgan loss with soft plus. softplus is a smooth approximation to the\n        ReLU function.\n\n        In StyleGAN2, it is called:\n            Logistic loss for discriminator;\n            Non-saturating loss for generator.\n\n        Args:\n            input (Tensor): Input tensor.\n            target (bool): Target label.\n\n        Returns:\n            Tensor: wgan loss.\n        \"\"\"\n        return F.softplus(-input).mean() if target else F.softplus(input).mean()\n\n    def get_target_label(self, input, target_is_real):\n        \"\"\"Get target label.\n\n        Args:\n            input (Tensor): Input tensor.\n            target_is_real (bool): Whether the target is real or fake.\n\n        Returns:\n            (bool | Tensor): Target tensor. Return bool for wgan, otherwise,\n                return Tensor.\n        \"\"\"\n\n        if self.gan_type in ['wgan', 'wgan_softplus']:\n            return target_is_real\n        target_val = (self.real_label_val if target_is_real else self.fake_label_val)\n        return input.new_ones(input.size()) * target_val\n\n    def forward(self, input, target_is_real, is_disc=False):\n        \"\"\"\n        Args:\n            input (Tensor): The input for the loss module, i.e., the network\n                prediction.\n            target_is_real (bool): Whether the targe is real or fake.\n            is_disc (bool): Whether the loss for discriminators or not.\n                Default: False.\n\n        Returns:\n            Tensor: GAN loss value.\n        \"\"\"\n        target_label = self.get_target_label(input, target_is_real)\n        if self.gan_type == 'hinge':\n            if is_disc:  # for discriminators in hinge-gan\n                input = -input if target_is_real else input\n                loss = self.loss(1 + input).mean()\n            else:  # for generators in hinge-gan\n                loss = -input.mean()\n        else:  # other gan types\n            loss = self.loss(input, target_label)\n\n        # loss_weight is always 1.0 for discriminators\n        return loss if is_disc else loss * self.loss_weight"}
{"type": "source_file", "path": "code/models/bitnetwork/__init__.py", "content": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\n# import kornia.losses\nfrom PIL import Image\nfrom torchvision import transforms\nfrom .ResBlock import *\nfrom .ConvBlock import *"}
{"type": "source_file", "path": "code/models/bitnetwork/DW_EncoderDecoder.py", "content": "from . import *\nfrom .Encoder_U import DW_Encoder\nfrom .Decoder_U import DW_Decoder\nfrom .Noise import Noise\nfrom .Random_Noise import Random_Noise\n\n\nclass DW_EncoderDecoder(nn.Module):\n\t'''\n\tA Sequential of Encoder_MP-Noise-Decoder\n\t'''\n\n\tdef __init__(self, message_length, noise_layers_R, noise_layers_F, attention_encoder, attention_decoder):\n\t\tsuper(DW_EncoderDecoder, self).__init__()\n\t\tself.encoder = DW_Encoder(message_length, attention = attention_encoder)\n\t\tself.noise = Random_Noise(noise_layers_R + noise_layers_F, len(noise_layers_R), len(noise_layers_F))\n\t\tself.decoder_C = DW_Decoder(message_length, attention = attention_decoder)\n\t\tself.decoder_RF = DW_Decoder(message_length, attention = attention_decoder)\n\n\n\tdef forward(self, image, message, mask):\n\t\tencoded_image = self.encoder(image, message)\n\t\tnoised_image_C, noised_image_R, noised_image_F = self.noise([encoded_image, image, mask])\n\t\tdecoded_message_C = self.decoder_C(noised_image_C)\n\t\tdecoded_message_R = self.decoder_RF(noised_image_R)\n\t\tdecoded_message_F = self.decoder_RF(noised_image_F)\n\t\treturn encoded_image, noised_image_C, decoded_message_C, decoded_message_R, decoded_message_F\n\n"}
{"type": "source_file", "path": "code/models/bitnetwork/Dual_Mark.py", "content": "from .DW_EncoderDecoder import *\nfrom .Patch_Discriminator import Patch_Discriminator\nimport torch\nimport kornia.losses\nimport lpips\n\n\nclass Network:\n\n\tdef __init__(self, message_length, noise_layers_R, noise_layers_F, device, batch_size, lr, beta1, attention_encoder, attention_decoder, weight):\n\t\t# device\n\t\tself.device = device\n\n\t\t# loss function\n\t\tself.criterion_MSE = nn.MSELoss().to(device)\n\t\tself.criterion_LPIPS = lpips.LPIPS().to(device)\n\n\t\t# weight of encoder-decoder loss\n\t\tself.encoder_weight = weight[0]\n\t\tself.decoder_weight_C = weight[1]\n\t\tself.decoder_weight_R = weight[2]\n\t\tself.decoder_weight_F = weight[3]\n\t\tself.discriminator_weight = weight[4]\n\n\t\t# network\n\t\tself.encoder_decoder = DW_EncoderDecoder(message_length, noise_layers_R, noise_layers_F, attention_encoder, attention_decoder).to(device)\n\t\tself.discriminator = Patch_Discriminator().to(device)\n\n\t\tself.encoder_decoder = torch.nn.DataParallel(self.encoder_decoder)\n\t\tself.discriminator = torch.nn.DataParallel(self.discriminator)\n\n\t\t# mark \"cover\" as 1, \"encoded\" as -1\n\t\tself.label_cover = 1.0\n\t\tself.label_encoded = - 1.0\n\n\t\tfor p in self.encoder_decoder.module.noise.parameters():\n\t\t\tp.requires_grad = False\n\n\t\t# optimizer\n\t\tself.opt_encoder_decoder = torch.optim.Adam(\n\t\t\tfilter(lambda p: p.requires_grad, self.encoder_decoder.parameters()), lr=lr, betas=(beta1, 0.999))\n\t\tself.opt_discriminator = torch.optim.Adam(self.discriminator.parameters(), lr=lr, betas=(beta1, 0.999))\n\n\n\tdef train(self, images: torch.Tensor, messages: torch.Tensor, masks: torch.Tensor):\n\t\tself.encoder_decoder.train()\n\t\tself.discriminator.train()\n\n\t\twith torch.enable_grad():\n\t\t\t# use device to compute\n\t\t\timages, messages, masks = images.to(self.device), messages.to(self.device), masks.to(self.device)\n\t\t\tencoded_images, noised_images, decoded_messages_C, decoded_messages_R, decoded_messages_F = self.encoder_decoder(images, messages, masks)\n\n\t\t\t'''\n\t\t\ttrain discriminator\n\t\t\t'''\n\t\t\tfor p in self.discriminator.parameters():\n\t\t\t\tp.requires_grad = True\n\n\t\t\tself.opt_discriminator.zero_grad()\n\n\t\t\t# RAW : target label for image should be \"cover\"(1)\n\t\t\td_label_cover = self.discriminator(images)\n\t\t\t#d_cover_loss = self.criterion_MSE(d_label_cover, torch.ones_like(d_label_cover))\n\t\t\t#d_cover_loss.backward()\n\n\t\t\t# GAN : target label for encoded image should be \"encoded\"(0)\n\t\t\td_label_encoded = self.discriminator(encoded_images.detach())\n\t\t\t#d_encoded_loss = self.criterion_MSE(d_label_encoded, torch.zeros_like(d_label_encoded))\n\t\t\t#d_encoded_loss.backward()\n\n\t\t\td_loss = self.criterion_MSE(d_label_cover - torch.mean(d_label_encoded), self.label_cover * torch.ones_like(d_label_cover)) +\\\n\t\t\t         self.criterion_MSE(d_label_encoded - torch.mean(d_label_cover), self.label_encoded * torch.ones_like(d_label_encoded))\n\t\t\td_loss.backward()\n\n\t\t\tself.opt_discriminator.step()\n\n\t\t\t'''\n\t\t\ttrain encoder and decoder\n\t\t\t'''\n\t\t\t# Make it a tiny bit faster\n\t\t\tfor p in self.discriminator.parameters():\n\t\t\t\tp.requires_grad = False\n\n\t\t\tself.opt_encoder_decoder.zero_grad()\n\n\t\t\t# GAN : target label for encoded image should be \"cover\"(0)\n\t\t\tg_label_cover = self.discriminator(images)\n\t\t\tg_label_encoded = self.discriminator(encoded_images)\n\t\t\tg_loss_on_discriminator = self.criterion_MSE(g_label_cover - torch.mean(g_label_encoded), self.label_encoded * torch.ones_like(g_label_cover)) +\\\n\t\t\t\t\t\t\t\t\t  self.criterion_MSE(g_label_encoded - torch.mean(g_label_cover), self.label_cover * torch.ones_like(g_label_encoded))\n\n\t\t\t# RAW : the encoded image should be similar to cover image\n\t\t\tg_loss_on_encoder_MSE = self.criterion_MSE(encoded_images, images)\n\t\t\tg_loss_on_encoder_LPIPS = torch.mean(self.criterion_LPIPS(encoded_images, images))\n\n\t\t\t# RESULT : the decoded message should be similar to the raw message /Dual\n\t\t\tg_loss_on_decoder_C = self.criterion_MSE(decoded_messages_C, messages)\n\t\t\tg_loss_on_decoder_R = self.criterion_MSE(decoded_messages_R, messages)\n\t\t\tg_loss_on_decoder_F = self.criterion_MSE(decoded_messages_F, torch.zeros_like(messages))\n\n\t\t\t# full loss\n\t\t\tg_loss = self.discriminator_weight * g_loss_on_discriminator + self.encoder_weight * g_loss_on_encoder_MSE +\\\n\t\t\t\t\t self.decoder_weight_C * g_loss_on_decoder_C + self.decoder_weight_R * g_loss_on_decoder_R + self.decoder_weight_F * g_loss_on_decoder_F\n\n\t\t\tg_loss.backward()\n\t\t\tself.opt_encoder_decoder.step()\n\n\t\t\t# psnr\n\t\t\tpsnr = - kornia.losses.psnr_loss(encoded_images.detach(), images, 2)\n\n\t\t\t# ssim\n\t\t\tssim = 1 - 2 * kornia.losses.ssim_loss(encoded_images.detach(), images, window_size=11, reduction=\"mean\")\n\n\t\t'''\n\t\tdecoded message error rate /Dual\n\t\t'''\n\t\terror_rate_C = self.decoded_message_error_rate_batch(messages, decoded_messages_C)\n\t\terror_rate_R = self.decoded_message_error_rate_batch(messages, decoded_messages_R)\n\t\terror_rate_F = self.decoded_message_error_rate_batch(messages, decoded_messages_F)\n\n\t\tresult = {\n\t\t\t\"g_loss\": g_loss,\n\t\t\t\"error_rate_C\": error_rate_C,\n\t\t\t\"error_rate_R\": error_rate_R,\n\t\t\t\"error_rate_F\": error_rate_F,\n\t\t\t\"psnr\": psnr,\n\t\t\t\"ssim\": ssim,\n\t\t\t\"g_loss_on_discriminator\": g_loss_on_discriminator,\n\t\t\t\"g_loss_on_encoder_MSE\": g_loss_on_encoder_MSE,\n\t\t\t\"g_loss_on_encoder_LPIPS\": g_loss_on_encoder_LPIPS,\n\t\t\t\"g_loss_on_decoder_C\": g_loss_on_decoder_C,\n\t\t\t\"g_loss_on_decoder_R\": g_loss_on_decoder_R,\n\t\t\t\"g_loss_on_decoder_F\": g_loss_on_decoder_F,\n\t\t\t\"d_loss\": d_loss\n\t\t}\n\t\treturn result\n\n\n\tdef validation(self, images: torch.Tensor, messages: torch.Tensor, masks: torch.Tensor):\n\t\tself.encoder_decoder.eval()\n\t\tself.encoder_decoder.module.noise.train()\n\t\tself.discriminator.eval()\n\n\t\twith torch.no_grad():\n\t\t\t# use device to compute\n\t\t\timages, messages, masks = images.to(self.device), messages.to(self.device), masks.to(self.device)\n\t\t\tencoded_images, noised_images, decoded_messages_C, decoded_messages_R, decoded_messages_F = self.encoder_decoder(images, messages, masks)\n\n\t\t\t'''\n\t\t\tvalidate discriminator\n\t\t\t'''\n\t\t\t# RAW : target label for image should be \"cover\"(1)\n\t\t\td_label_cover = self.discriminator(images)\n\t\t\t#d_cover_loss = self.criterion_MSE(d_label_cover, torch.ones_like(d_label_cover))\n\n\t\t\t# GAN : target label for encoded image should be \"encoded\"(0)\n\t\t\td_label_encoded = self.discriminator(encoded_images.detach())\n\t\t\t#d_encoded_loss = self.criterion_MSE(d_label_encoded, torch.zeros_like(d_label_encoded))\n\n\t\t\td_loss = self.criterion_MSE(d_label_cover - torch.mean(d_label_encoded), self.label_cover * torch.ones_like(d_label_cover)) +\\\n\t\t\t         self.criterion_MSE(d_label_encoded - torch.mean(d_label_cover), self.label_encoded * torch.ones_like(d_label_encoded))\n\n\t\t\t'''\n\t\t\tvalidate encoder and decoder\n\t\t\t'''\n\n\t\t\t# GAN : target label for encoded image should be \"cover\"(0)\n\t\t\tg_label_cover = self.discriminator(images)\n\t\t\tg_label_encoded = self.discriminator(encoded_images)\n\t\t\tg_loss_on_discriminator = self.criterion_MSE(g_label_cover - torch.mean(g_label_encoded), self.label_encoded * torch.ones_like(g_label_cover)) +\\\n\t\t\t\t\t\t\t\t\t  self.criterion_MSE(g_label_encoded - torch.mean(g_label_cover), self.label_cover * torch.ones_like(g_label_encoded))\n\n\t\t\t# RAW : the encoded image should be similar to cover image\n\t\t\tg_loss_on_encoder_MSE = self.criterion_MSE(encoded_images, images)\n\t\t\tg_loss_on_encoder_LPIPS = torch.mean(self.criterion_LPIPS(encoded_images, images))\n\n\t\t\t# RESULT : the decoded message should be similar to the raw message /Dual\n\t\t\tg_loss_on_decoder_C = self.criterion_MSE(decoded_messages_C, messages)\n\t\t\tg_loss_on_decoder_R = self.criterion_MSE(decoded_messages_R, messages)\n\t\t\tg_loss_on_decoder_F = self.criterion_MSE(decoded_messages_F, torch.zeros_like(messages))\n\n\t\t\t# full loss\n\t\t\t# unstable g_loss_on_discriminator is not used during validation\n\n\t\t\tg_loss = 0 * g_loss_on_discriminator + self.encoder_weight * g_loss_on_encoder_LPIPS +\\\n\t\t\t\t\t self.decoder_weight_C * g_loss_on_decoder_C + self.decoder_weight_R * g_loss_on_decoder_R + self.decoder_weight_F * g_loss_on_decoder_F\n\n\n\t\t\t# psnr\n\t\t\tpsnr = - kornia.losses.psnr_loss(encoded_images.detach(), images, 2)\n\n\t\t\t# ssim\n\t\t\tssim = 1 - 2 * kornia.losses.ssim_loss(encoded_images.detach(), images, window_size=11, reduction=\"mean\")\n\n\t\t'''\n\t\tdecoded message error rate /Dual\n\t\t'''\n\t\terror_rate_C = self.decoded_message_error_rate_batch(messages, decoded_messages_C)\n\t\terror_rate_R = self.decoded_message_error_rate_batch(messages, decoded_messages_R)\n\t\terror_rate_F = self.decoded_message_error_rate_batch(messages, decoded_messages_F)\n\n\t\tresult = {\n\t\t\t\"g_loss\": g_loss,\n\t\t\t\"error_rate_C\": error_rate_C,\n\t\t\t\"error_rate_R\": error_rate_R,\n\t\t\t\"error_rate_F\": error_rate_F,\n\t\t\t\"psnr\": psnr,\n\t\t\t\"ssim\": ssim,\n\t\t\t\"g_loss_on_discriminator\": g_loss_on_discriminator,\n\t\t\t\"g_loss_on_encoder_MSE\": g_loss_on_encoder_MSE,\n\t\t\t\"g_loss_on_encoder_LPIPS\": g_loss_on_encoder_LPIPS,\n\t\t\t\"g_loss_on_decoder_C\": g_loss_on_decoder_C,\n\t\t\t\"g_loss_on_decoder_R\": g_loss_on_decoder_R,\n\t\t\t\"g_loss_on_decoder_F\": g_loss_on_decoder_F,\n\t\t\t\"d_loss\": d_loss\n\t\t}\n\n\t\treturn result, (images, encoded_images, noised_images)\n\n\tdef decoded_message_error_rate(self, message, decoded_message):\n\t\tlength = message.shape[0]\n\n\t\tmessage = message.gt(0)\n\t\tdecoded_message = decoded_message.gt(0)\n\t\terror_rate = float(sum(message != decoded_message)) / length\n\t\treturn error_rate\n\n\tdef decoded_message_error_rate_batch(self, messages, decoded_messages):\n\t\terror_rate = 0.0\n\t\tbatch_size = len(messages)\n\t\tfor i in range(batch_size):\n\t\t\terror_rate += self.decoded_message_error_rate(messages[i], decoded_messages[i])\n\t\terror_rate /= batch_size\n\t\treturn error_rate\n\n\tdef save_model(self, path_encoder_decoder: str, path_discriminator: str):\n\t\ttorch.save(self.encoder_decoder.module.state_dict(), path_encoder_decoder)\n\t\ttorch.save(self.discriminator.module.state_dict(), path_discriminator)\n\n\tdef load_model(self, path_encoder_decoder: str, path_discriminator: str):\n\t\tself.load_model_ed(path_encoder_decoder)\n\t\tself.load_model_dis(path_discriminator)\n\n\tdef load_model_ed(self, path_encoder_decoder: str):\n\t\tself.encoder_decoder.module.load_state_dict(torch.load(path_encoder_decoder), strict=False)\n\n\tdef load_model_dis(self, path_discriminator: str):\n\t\tself.discriminator.module.load_state_dict(torch.load(path_discriminator))\n"}
{"type": "source_file", "path": "code/options/options.py", "content": "import os\nimport os.path as osp\nimport logging\nimport yaml\nfrom utils.util import OrderedYaml\nLoader, Dumper = OrderedYaml()\n\n\ndef parse(opt_path, is_train=True):\n    with open(opt_path, mode='r') as f:\n        opt = yaml.load(f, Loader=Loader)\n    # export CUDA_VISIBLE_DEVICES\n    gpu_list = ','.join(str(x) for x in opt['gpu_ids'])\n    os.environ['CUDA_VISIBLE_DEVICES'] = gpu_list\n    print('export CUDA_VISIBLE_DEVICES=' + gpu_list)\n\n    opt['is_train'] = is_train\n    if opt['distortion'] == 'sr':\n        scale = opt['scale']\n\n    # datasets\n    for phase, dataset in opt['datasets'].items():\n        phase = phase.split('_')[0]\n        dataset['phase'] = phase\n        if opt['distortion'] == 'sr':\n            dataset['scale'] = scale\n        is_lmdb = False\n        if dataset.get('dataroot_GT', None) is not None:\n            dataset['dataroot_GT'] = osp.expanduser(dataset['dataroot_GT'])\n            if dataset['dataroot_GT'].endswith('lmdb'):\n                is_lmdb = True\n        # if dataset.get('dataroot_GT_bg', None) is not None:\n        #     dataset['dataroot_GT_bg'] = osp.expanduser(dataset['dataroot_GT_bg'])\n        if dataset.get('dataroot_LQ', None) is not None:\n            dataset['dataroot_LQ'] = osp.expanduser(dataset['dataroot_LQ'])\n            if dataset['dataroot_LQ'].endswith('lmdb'):\n                is_lmdb = True\n        dataset['data_type'] = 'lmdb' if is_lmdb else 'img'\n        if dataset['mode'].endswith('mc'):  # for memcached\n            dataset['data_type'] = 'mc'\n            dataset['mode'] = dataset['mode'].replace('_mc', '')\n\n    # path\n    for key, path in opt['path'].items():\n        if path and key in opt['path'] and key != 'strict_load':\n            opt['path'][key] = osp.expanduser(path)\n    opt['path']['root'] = osp.abspath(osp.join(__file__, osp.pardir, osp.pardir, osp.pardir))\n    if is_train:\n        experiments_root = osp.join(opt['path']['root'], 'experiments', opt['name'])\n        opt['path']['experiments_root'] = experiments_root\n        opt['path']['models'] = osp.join(experiments_root, 'models')\n        opt['path']['training_state'] = osp.join(experiments_root, 'training_state')\n        opt['path']['log'] = experiments_root\n        opt['path']['val_images'] = osp.join(experiments_root, 'val_images')\n\n        # change some options for debug mode\n        if 'debug' in opt['name']:\n            opt['train']['val_freq'] = 8\n            opt['logger']['print_freq'] = 1\n            opt['logger']['save_checkpoint_freq'] = 8\n    else:  # test\n        results_root = osp.join(opt['path']['root'], 'results', opt['name'])\n        opt['path']['results_root'] = results_root\n        opt['path']['log'] = results_root\n\n    # network\n    if opt['distortion'] == 'sr':\n        opt['network_G']['scale'] = scale\n\n    return opt\n\n\ndef dict2str(opt, indent_l=1):\n    '''dict to string for logger'''\n    msg = ''\n    for k, v in opt.items():\n        if isinstance(v, dict):\n            msg += ' ' * (indent_l * 2) + k + ':[\\n'\n            msg += dict2str(v, indent_l + 1)\n            msg += ' ' * (indent_l * 2) + ']\\n'\n        else:\n            msg += ' ' * (indent_l * 2) + k + ': ' + str(v) + '\\n'\n    return msg\n\n\nclass NoneDict(dict):\n    def __missing__(self, key):\n        return None\n\n\n# convert to NoneDict, which return None for missing key.\ndef dict_to_nonedict(opt):\n    if isinstance(opt, dict):\n        new_opt = dict()\n        for key, sub_opt in opt.items():\n            new_opt[key] = dict_to_nonedict(sub_opt)\n        return NoneDict(**new_opt)\n    elif isinstance(opt, list):\n        return [dict_to_nonedict(sub_opt) for sub_opt in opt]\n    else:\n        return opt\n\n\ndef check_resume(opt, resume_iter):\n    '''Check resume states and pretrain_model paths'''\n    logger = logging.getLogger('base')\n    if opt['path']['resume_state']:\n        if opt['path'].get('pretrain_model_G', None) is not None or opt['path'].get(\n                'pretrain_model_D', None) is not None:\n            logger.warning('pretrain_model path will be ignored when resuming training.')\n\n        opt['path']['pretrain_model_G'] = osp.join(opt['path']['models'],\n                                                   '{}_G.pth'.format(resume_iter))\n        logger.info('Set [pretrain_model_G] to ' + opt['path']['pretrain_model_G'])\n"}
{"type": "source_file", "path": "code/train_bit.py", "content": "import os\nimport math\nimport argparse\nimport random\nimport logging\n\nimport torch\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\nfrom data.data_sampler import DistIterSampler\n\nimport options.options as option\nfrom utils import util\nfrom data import create_dataloader, create_dataset\nfrom models import create_model\n\n\ndef init_dist(backend='nccl', **kwargs):\n    ''' initialization for distributed training'''\n    # if mp.get_start_method(allow_none=True) is None:\n    if mp.get_start_method(allow_none=True) != 'spawn':\n        mp.set_start_method('spawn')\n    rank = int(os.environ['RANK'])\n    num_gpus = torch.cuda.device_count()\n    torch.cuda.set_device(rank % num_gpus)\n    dist.init_process_group(backend=backend, **kwargs)\n\ndef cal_pnsr(sr_img, gt_img):\n    # calculate PSNR\n    gt_img = gt_img / 255.\n    sr_img = sr_img / 255.\n    psnr = util.calculate_psnr(sr_img * 255, gt_img * 255)\n\n    return psnr\n\ndef main():\n    # options\n    parser = argparse.ArgumentParser()\n    parser.add_argument('-opt', type=str, help='Path to option YMAL file.')  # config 文件\n    parser.add_argument('--launcher', choices=['none', 'pytorch'], default='none',\n                        help='job launcher')\n    parser.add_argument('--local_rank', type=int, default=0)\n    args = parser.parse_args()\n    opt = option.parse(args.opt, is_train=True)\n\n    # distributed training settings\n    if args.launcher == 'none':  # disabled distributed training\n        opt['dist'] = False\n        rank = -1\n        print('Disabled distributed training.')\n    else:\n        opt['dist'] = True\n        init_dist()\n        world_size = torch.distributed.get_world_size()\n        rank = torch.distributed.get_rank()\n\n    # loading resume state if exists\n    if opt['path'].get('resume_state', None):\n        # distributed resuming: all load into default GPU\n        device_id = torch.cuda.current_device()\n        resume_state = torch.load(opt['path']['resume_state'],\n                                  map_location=lambda storage, loc: storage.cuda(device_id))\n        # resume_state = torch.load(opt['path']['resume_state'],\n        #                           map_location=lambda storage, loc: storage.cuda(device_id), strict=False)\n        option.check_resume(opt, resume_state['iter'])  # check resume options\n    else:\n        resume_state = None\n\n    # mkdir and loggers\n    if rank <= 0:  # normal training (rank -1) OR distributed training (rank 0)\n        if resume_state is None:\n            util.mkdir_and_rename(\n                opt['path']['experiments_root'])  # rename experiment folder if exists\n            util.mkdirs((path for key, path in opt['path'].items() if not key == 'experiments_root'\n                         and 'pretrain_model' not in key and 'resume' not in key))\n\n        # config loggers. Before it, the log will not work\n        util.setup_logger('base', opt['path']['log'], 'train_' + opt['name'], level=logging.INFO,\n                          screen=True, tofile=True)\n        util.setup_logger('val', opt['path']['log'], 'val_' + opt['name'], level=logging.INFO,\n                          screen=True, tofile=True)\n        logger = logging.getLogger('base')\n        logger.info(option.dict2str(opt))\n        # tensorboard logger\n        if opt['use_tb_logger'] and 'debug' not in opt['name']:\n            version = float(torch.__version__[0:3])\n            if version >= 1.1:  # PyTorch 1.1\n                from torch.utils.tensorboard import SummaryWriter\n            else:\n                logger.info(\n                    'You are using PyTorch {}. Tensorboard will use [tensorboardX]'.format(version))\n                from tensorboardX import SummaryWriter\n            tb_logger = SummaryWriter(log_dir='../tb_logger/' + opt['name'])\n    else:\n        util.setup_logger('base', opt['path']['log'], 'train', level=logging.INFO, screen=True)\n        logger = logging.getLogger('base')\n\n    # convert to NoneDict, which returns None for missing keys\n    opt = option.dict_to_nonedict(opt)\n\n    # random seed\n    seed = opt['train']['manual_seed']\n    if seed is None:\n        seed = random.randint(1, 10000)\n    if rank <= 0:\n        logger.info('Random seed: {}'.format(seed))\n    util.set_random_seed(seed)\n\n    torch.backends.cudnn.benchmark = True\n    # torch.backends.cudnn.deterministic = True\n\n    #### create train and val dataloader\n    dataset_ratio = 200  # enlarge the size of each epoch\n    for phase, dataset_opt in opt['datasets'].items():\n        if phase == 'train':\n            train_set = create_dataset(dataset_opt)\n            train_size = int(math.ceil(len(train_set) / dataset_opt['batch_size']))\n            total_iters = int(opt['train']['niter'])\n            total_epochs = int(math.ceil(total_iters / train_size))\n            if opt['dist']:\n                train_sampler = DistIterSampler(train_set, world_size, rank, dataset_ratio)\n                total_epochs = int(math.ceil(total_iters / (train_size * dataset_ratio)))\n            else:\n                train_sampler = None\n            train_loader = create_dataloader(train_set, dataset_opt, opt, train_sampler)\n            if rank <= 0:\n                logger.info('Number of train images: {:,d}, iters: {:,d}'.format(\n                    len(train_set), train_size))\n                logger.info('Total epochs needed: {:d} for iters {:,d}'.format(\n                    total_epochs, total_iters))\n        elif phase == 'val':\n            val_set = create_dataset(dataset_opt)\n            val_loader = create_dataloader(val_set, dataset_opt, opt, None)\n            if rank <= 0:\n                logger.info('Number of val images in [{:s}]: {:d}'.format(\n                    dataset_opt['name'], len(val_set)))\n        else:\n            raise NotImplementedError('Phase [{:s}] is not recognized.'.format(phase))\n    assert train_loader is not None\n\n    # create model\n    model = create_model(opt)\n    # resume training\n    if resume_state:\n        logger.info('Resuming training from epoch: {}, iter: {}.'.format(\n            resume_state['epoch'], resume_state['iter']))\n\n        start_epoch = resume_state['epoch']\n        current_step = resume_state['iter']\n        model.resume_training(resume_state)  # handle optimizers and schedulers\n    else:\n        current_step = 0\n        start_epoch = 0\n\n    # training\n    logger.info('Start training from epoch: {:d}, iter: {:d}'.format(start_epoch, current_step))\n    for epoch in range(start_epoch, total_epochs + 1):\n        if opt['dist']:\n            train_sampler.set_epoch(epoch)\n        for _, train_data in enumerate(train_loader):\n            current_step += 1\n            if current_step > total_iters:\n                break\n            # training\n            model.feed_data(train_data)\n            model.optimize_parameters(current_step)\n\n            # update learning rate\n            model.update_learning_rate(current_step, warmup_iter=opt['train']['warmup_iter'])\n\n            # log\n            if current_step % opt['logger']['print_freq'] == 0:\n                logs = model.get_current_log()\n                message = '<epoch:{:3d}, iter:{:8,d}, lr:{:.3e}> '.format(\n                    epoch, current_step, model.get_current_learning_rate())\n                for k, v in logs.items():\n                    message += '{:s}: {:.4e} '.format(k, v)\n                    # tensorboard logger\n                    if opt['use_tb_logger'] and 'debug' not in opt['name']:\n                        if rank <= 0:\n                            tb_logger.add_scalar(k, v, current_step)\n                if rank <= 0:\n                    logger.info(message)\n\n            # validation\n            if current_step % opt['train']['val_freq'] == 0 and rank <= 0:\n                avg_psnr = 0.0\n                avg_psnr_h = [0.0]*opt['num_image']\n                avg_psnr_lr = 0.0\n                avg_biterr = 0.0\n                idx = 0\n                for image_id, val_data in enumerate(val_loader):\n                    img_dir = os.path.join(opt['path']['val_images'])\n                    util.mkdir(img_dir)\n\n                    model.feed_data(val_data)\n                    model.test(image_id)\n\n                    visuals = model.get_current_visuals()\n\n                    t_step = visuals['recmessage'].shape[0]\n                    idx += t_step\n                    n = 1\n                    # print(visuals['message'].shape)\n                    avg_biterr += util.decoded_message_error_rate_batch(visuals['recmessage'][0], visuals['message'][0])\n                    print(util.decoded_message_error_rate_batch(visuals['recmessage'][0], visuals['message'][0]))\n\n                    for i in range(t_step):\n\n                        gt_img = util.tensor2img(visuals['GT'][i])  # uint8\n                        lr_img = util.tensor2img(visuals['LR'][i])\n\n                        save_img_path = os.path.join(img_dir,'{:d}_{:d}_{:s}.png'.format(image_id, i, 'GT'))\n                        util.save_img(gt_img, save_img_path)\n\n                        save_img_path = os.path.join(img_dir,'{:d}_{:d}_{:s}.png'.format(image_id, i, 'LR'))\n                        util.save_img(lr_img, save_img_path)\n                        psnr_lr = cal_pnsr(lr_img, gt_img)\n                        avg_psnr_lr += psnr_lr\n\n                avg_psnr_lr = avg_psnr_lr / idx\n                avg_biterr = avg_biterr / idx\n\n                logger.info('# Validation # PSNR_Stego: {:.4e}, Bit_acc: {: .4e}'.format(avg_psnr_lr, avg_biterr))\n                logger_val = logging.getLogger('val')  # validation logger\n                logger_val.info('<epoch:{:3d}, iter:{:8,d}> PSNR_Stego: {:.4e}, Bit_acc: {: .4e}'.format(\n                    epoch, current_step, avg_psnr_lr, avg_biterr))\n                # tensorboard logger\n                if opt['use_tb_logger'] and 'debug' not in opt['name']:\n                    tb_logger.add_scalar('psnr', avg_psnr, current_step)\n\n            # save models and training states\n            if current_step % opt['logger']['save_checkpoint_freq'] == 0:\n                if rank <= 0:\n                    logger.info('Saving models and training states.')\n                    model.save(current_step)\n                    model.save_training_state(epoch, current_step)\n\n    if rank <= 0:\n        logger.info('Saving the final model.')\n        model.save('latest')\n        logger.info('End of training.')\n\n\nif __name__ == '__main__':\n    main()\n"}
{"type": "source_file", "path": "code/models/bitnetwork/ResBlock.py", "content": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass SEAttention(nn.Module):\n    def __init__(self, in_channels, out_channels, reduction=8):\n        super(SEAttention, self).__init__()\n        self.se = nn.Sequential(\n            nn.AdaptiveAvgPool2d((1, 1)),\n            nn.Conv2d(in_channels=in_channels, out_channels=out_channels // reduction, kernel_size=1, bias=False),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(in_channels=out_channels // reduction, out_channels=out_channels, kernel_size=1, bias=False),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        x = self.se(x) * x\n        return x\n\n\nclass ChannelAttention(nn.Module):\n    def __init__(self, in_channels, out_channels, reduction=8):\n        super(ChannelAttention, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n        self.max_pool = nn.AdaptiveMaxPool2d((1, 1))\n\n        self.fc = nn.Sequential(nn.Conv2d(in_channels=in_channels, out_channels=out_channels // reduction, kernel_size=1, bias=False),\n                                nn.ReLU(inplace=True),\n                                nn.Conv2d(in_channels=out_channels // reduction, out_channels=out_channels, kernel_size=1, bias=False))\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        avg_out = self.fc(self.avg_pool(x))\n        max_out = self.fc(self.max_pool(x))\n        out = avg_out + max_out\n        return self.sigmoid(out)\n\n\nclass SpatialAttention(nn.Module):\n    def __init__(self, kernel_size=7):\n        super(SpatialAttention, self).__init__()\n\n        self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=kernel_size // 2, bias=False)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        avg_out = torch.mean(x, dim=1, keepdim=True)\n        max_out, _ = torch.max(x, dim=1, keepdim=True)\n        x = torch.cat([avg_out, max_out], dim=1)\n        x = self.conv1(x)\n        return self.sigmoid(x)\n\n\nclass CBAMAttention(nn.Module):\n    def __init__(self, in_channels, out_channels, reduction=8):\n        super(CBAMAttention, self).__init__()\n        self.ca = ChannelAttention(in_channels=in_channels, out_channels=out_channels, reduction=reduction)\n        self.sa = SpatialAttention()\n\n    def forward(self, x):\n        x = self.ca(x) * x\n        x = self.sa(x) * x\n        return x\n\n\nclass h_sigmoid(nn.Module):\n    def __init__(self, inplace=True):\n        super(h_sigmoid, self).__init__()\n        self.relu = nn.ReLU6(inplace=inplace)\n\n    def forward(self, x):\n        return self.relu(x + 3) / 6\n\n\nclass h_swish(nn.Module):\n    def __init__(self, inplace=True):\n        super(h_swish, self).__init__()\n        self.sigmoid = h_sigmoid(inplace=inplace)\n\n    def forward(self, x):\n        return x * self.sigmoid(x)\n\n\nclass CoordAttention(nn.Module):\n    def __init__(self, in_channels, out_channels, reduction=8):\n        super(CoordAttention, self).__init__()\n        self.pool_w, self.pool_h = nn.AdaptiveAvgPool2d((1, None)), nn.AdaptiveAvgPool2d((None, 1))\n        temp_c = max(8, in_channels // reduction)\n        self.conv1 = nn.Conv2d(in_channels, temp_c, kernel_size=1, stride=1, padding=0)\n\n        self.bn1 = nn.InstanceNorm2d(temp_c)\n        self.act1 = h_swish() # nn.SiLU() # nn.Hardswish() # nn.SiLU()\n\n        self.conv2 = nn.Conv2d(temp_c, out_channels, kernel_size=1, stride=1, padding=0)\n        self.conv3 = nn.Conv2d(temp_c, out_channels, kernel_size=1, stride=1, padding=0)\n\n    def forward(self, x):\n        short = x\n        n, c, H, W = x.shape\n        x_h, x_w = self.pool_h(x), self.pool_w(x).permute(0, 1, 3, 2)\n        x_cat = torch.cat([x_h, x_w], dim=2)\n        out = self.act1(self.bn1(self.conv1(x_cat)))\n        x_h, x_w = torch.split(out, [H, W], dim=2)\n        x_w = x_w.permute(0, 1, 3, 2)\n        out_h = torch.sigmoid(self.conv2(x_h))\n        out_w = torch.sigmoid(self.conv3(x_w))\n        return short * out_w * out_h\n\n\nclass BasicBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, reduction, stride, attention=None):\n        super(BasicBlock, self).__init__()\n\n        self.change = None\n        if (in_channels != out_channels or stride != 1):\n            self.change = nn.Sequential(\n                nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=1, padding=0,\n                          stride=stride, bias=False),\n                nn.InstanceNorm2d(out_channels)\n            )\n\n        self.left = nn.Sequential(\n            nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, padding=1,\n                      stride=stride, bias=False),\n            nn.InstanceNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=3, padding=1, bias=False),\n            nn.InstanceNorm2d(out_channels)\n        )\n\n        if attention == 'se':\n            print('SEAttention')\n            self.attention = SEAttention(in_channels=out_channels, out_channels=out_channels, reduction=reduction)\n        elif attention == 'cbam':\n            print('CBAMAttention')\n            self.attention = CBAMAttention(in_channels=out_channels, out_channels=out_channels, reduction=reduction)\n        elif attention == 'coord':\n            print('CoordAttention')\n            self.attention = CoordAttention(in_channels=out_channels, out_channels=out_channels, reduction=reduction)\n        else:\n            print('None Attention')\n            self.attention = nn.Identity()\n\n    def forward(self, x):\n        identity = x\n        x = self.left(x)\n        x = self.attention(x)\n\n        if self.change is not None:\n            identity = self.change(identity)\n\n        x += identity\n        x = F.relu(x)\n        return x\n\n\nclass BottleneckBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, reduction, stride, attention=None):\n        super(BottleneckBlock, self).__init__()\n\n        self.change = None\n        if (in_channels != out_channels or stride != 1):\n            self.change = nn.Sequential(\n                nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=1, padding=0,\n                          stride=stride, bias=False),\n                nn.InstanceNorm2d(out_channels)\n            )\n\n        self.left = nn.Sequential(\n            nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=1,\n                      stride=stride, padding=0, bias=False),\n            nn.InstanceNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=3, padding=1, bias=False),\n            nn.InstanceNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=1, padding=0, bias=False),\n            nn.InstanceNorm2d(out_channels)\n        )\n\n        if attention == 'se':\n            print('SEAttention')\n            self.attention = SEAttention(in_channels=out_channels, out_channels=out_channels, reduction=reduction)\n        elif attention == 'cbam':\n            print('CBAMAttention')\n            self.attention = CBAMAttention(in_channels=out_channels, out_channels=out_channels, reduction=reduction)\n        elif attention == 'coord':\n            print('CoordAttention')\n            self.attention = CoordAttention(in_channels=out_channels, out_channels=out_channels, reduction=reduction)\n        else:\n            print('None Attention')\n            self.attention = nn.Identity()\n\n    def forward(self, x):\n        identity = x\n        x = self.left(x)\n        x = self.attention(x)\n\n        if self.change is not None:\n            identity = self.change(identity)\n\n        x += identity\n        x = F.relu(x)\n        return x\n\n\nclass ResBlock(nn.Module):\n\n    def __init__(self, in_channels, out_channels, blocks=1, block_type=\"BottleneckBlock\", reduction=8, stride=1, attention=None):\n        super(ResBlock, self).__init__()\n\n        layers = [eval(block_type)(in_channels, out_channels, reduction, stride, attention=attention)] if blocks != 0 else []\n        for _ in range(blocks - 1):\n            layer = eval(block_type)(out_channels, out_channels, reduction, 1, attention=attention)\n            layers.append(layer)\n\n        self.layers = nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.layers(x)\n\n"}
{"type": "source_file", "path": "code/utils/util.py", "content": "import os\nimport sys\nimport time\nimport math\nfrom datetime import datetime\nimport random\nimport logging\nfrom collections import OrderedDict\nimport numpy as np\nimport cv2\nimport torch\nfrom torchvision.utils import make_grid\nfrom shutil import get_terminal_size\n\nimport yaml\ntry:\n    from yaml import CLoader as Loader, CDumper as Dumper\nexcept ImportError:\n    from yaml import Loader, Dumper\n\n\ndef OrderedYaml():\n    '''yaml orderedDict support'''\n    _mapping_tag = yaml.resolver.BaseResolver.DEFAULT_MAPPING_TAG\n\n    def dict_representer(dumper, data):\n        return dumper.represent_dict(data.items())\n\n    def dict_constructor(loader, node):\n        return OrderedDict(loader.construct_pairs(node))\n\n    Dumper.add_representer(OrderedDict, dict_representer)\n    Loader.add_constructor(_mapping_tag, dict_constructor)\n    return Loader, Dumper\n\n\n####################\n# miscellaneous\n####################\n\n\ndef get_timestamp():\n    return datetime.now().strftime('%y%m%d-%H%M%S')\n\n\ndef mkdir(path):\n    if not os.path.exists(path):\n        os.makedirs(path)\n\n\ndef mkdirs(paths):\n    if isinstance(paths, str):\n        mkdir(paths)\n    else:\n        for path in paths:\n            mkdir(path)\n\n\ndef mkdir_and_rename(path):\n#     print(path)\n#     exit(0)\n    if os.path.exists(path):\n        new_name = path + '_archived_' + get_timestamp()\n        print('Path already exists. Rename it to [{:s}]'.format(new_name))\n        logger = logging.getLogger('base')\n        logger.info('Path already exists. Rename it to [{:s}]'.format(new_name))\n#         path = new_name\n        os.rename(path, new_name)\n    os.makedirs(path)\n#     return path\n\n\ndef set_random_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n\ndef setup_logger(logger_name, root, phase, level=logging.INFO, screen=False, tofile=False):\n    '''set up logger'''\n    lg = logging.getLogger(logger_name)\n    formatter = logging.Formatter('%(asctime)s.%(msecs)03d - %(levelname)s: %(message)s',\n                                  datefmt='%y-%m-%d %H:%M:%S')\n    lg.setLevel(level)\n    if tofile:\n        log_file = os.path.join(root, phase + '_{}.log'.format(get_timestamp()))\n        fh = logging.FileHandler(log_file, mode='w')\n        fh.setFormatter(formatter)\n        lg.addHandler(fh)\n    if screen:\n        sh = logging.StreamHandler()\n        sh.setFormatter(formatter)\n        lg.addHandler(sh)\n\n\n####################\n# image convert\n####################\n\n\ndef tensor2img(tensor, out_type=np.uint8, min_max=(0, 1)):\n    '''\n    Converts a torch Tensor into an image Numpy array\n    Input: 4D(B,(3/1),H,W), 3D(C,H,W), or 2D(H,W), any range, RGB channel order\n    Output: 3D(H,W,C) or 2D(H,W), [0,255], np.uint8 (default)\n    '''\n    tensor = tensor.squeeze().float().cpu().clamp_(*min_max)  # clamp\n    tensor = (tensor - min_max[0]) / (min_max[1] - min_max[0])  # to range [0,1]\n    n_dim = tensor.dim()\n    if n_dim == 4:\n        n_img = len(tensor)\n        img_np = make_grid(tensor, nrow=int(math.sqrt(n_img)), normalize=False).numpy()\n        img_np = np.transpose(img_np[[2, 1, 0], :, :], (1, 2, 0))  # HWC, BGR\n    elif n_dim == 3:\n        img_np = tensor.numpy()\n        img_np = np.transpose(img_np[[2, 1, 0], :, :], (1, 2, 0))  # HWC, BGR\n    elif n_dim == 2:\n        img_np = tensor.numpy()\n    else:\n        raise TypeError(\n            'Only support 4D, 3D and 2D tensor. But received with dimension: {:d}'.format(n_dim))\n    if out_type == np.uint8:\n        img_np = (img_np * 255.0).round()\n        # Important. Unlike matlab, numpy.unit8() WILL NOT round by default.\n    return img_np.astype(out_type)\n\n\ndef save_img(img, img_path, mode='RGB'):\n    cv2.imwrite(img_path, img)\n\n\n####################\n# metric\n####################\n\n\ndef calculate_psnr(img1, img2):\n    # img1 and img2 have range [0, 255]\n    img1 = img1.astype(np.float64)\n    img2 = img2.astype(np.float64)\n    mse = np.mean((img1 - img2)**2)\n    if mse == 0:\n        return float('inf')\n    return 20 * math.log10(255.0 / math.sqrt(mse))\n\n\ndef ssim(img1, img2):\n    C1 = (0.01 * 255)**2\n    C2 = (0.03 * 255)**2\n\n    img1 = img1.astype(np.float64)\n    img2 = img2.astype(np.float64)\n    kernel = cv2.getGaussianKernel(11, 1.5)\n    window = np.outer(kernel, kernel.transpose())\n\n    mu1 = cv2.filter2D(img1, -1, window)[5:-5, 5:-5]  # valid\n    mu2 = cv2.filter2D(img2, -1, window)[5:-5, 5:-5]\n    mu1_sq = mu1**2\n    mu2_sq = mu2**2\n    mu1_mu2 = mu1 * mu2\n    sigma1_sq = cv2.filter2D(img1**2, -1, window)[5:-5, 5:-5] - mu1_sq\n    sigma2_sq = cv2.filter2D(img2**2, -1, window)[5:-5, 5:-5] - mu2_sq\n    sigma12 = cv2.filter2D(img1 * img2, -1, window)[5:-5, 5:-5] - mu1_mu2\n\n    ssim_map = ((2 * mu1_mu2 + C1) * (2 * sigma12 + C2)) / ((mu1_sq + mu2_sq + C1) *\n                                                            (sigma1_sq + sigma2_sq + C2))\n    return ssim_map.mean()\n\n\ndef calculate_ssim(img1, img2):\n    '''calculate SSIM\n    the same outputs as MATLAB's\n    img1, img2: [0, 255]\n    '''\n    if not img1.shape == img2.shape:\n        raise ValueError('Input images must have the same dimensions.')\n    if img1.ndim == 2:\n        return ssim(img1, img2)\n    elif img1.ndim == 3:\n        if img1.shape[2] == 3:\n            ssims = []\n            for i in range(3):\n                ssims.append(ssim(img1, img2))\n            return np.array(ssims).mean()\n        elif img1.shape[2] == 1:\n            return ssim(np.squeeze(img1), np.squeeze(img2))\n    else:\n        raise ValueError('Wrong input image dimensions.')\n\n\nclass ProgressBar(object):\n    '''A progress bar which can print the progress\n    modified from https://github.com/hellock/cvbase/blob/master/cvbase/progress.py\n    '''\n\n    def __init__(self, task_num=0, bar_width=50, start=True):\n        self.task_num = task_num\n        max_bar_width = self._get_max_bar_width()\n        self.bar_width = (bar_width if bar_width <= max_bar_width else max_bar_width)\n        self.completed = 0\n        if start:\n            self.start()\n\n    def _get_max_bar_width(self):\n        terminal_width, _ = get_terminal_size()\n        max_bar_width = min(int(terminal_width * 0.6), terminal_width - 50)\n        if max_bar_width < 10:\n            print('terminal width is too small ({}), please consider widen the terminal for better '\n                  'progressbar visualization'.format(terminal_width))\n            max_bar_width = 10\n        return max_bar_width\n\n    def start(self):\n        if self.task_num > 0:\n            sys.stdout.write('[{}] 0/{}, elapsed: 0s, ETA:\\n{}\\n'.format(\n                ' ' * self.bar_width, self.task_num, 'Start...'))\n        else:\n            sys.stdout.write('completed: 0, elapsed: 0s')\n        sys.stdout.flush()\n        self.start_time = time.time()\n\n    def update(self, msg='In progress...'):\n        self.completed += 1\n        elapsed = time.time() - self.start_time\n        fps = self.completed / elapsed\n        if self.task_num > 0:\n            percentage = self.completed / float(self.task_num)\n            eta = int(elapsed * (1 - percentage) / percentage + 0.5)\n            mark_width = int(self.bar_width * percentage)\n            bar_chars = '>' * mark_width + '-' * (self.bar_width - mark_width)\n            sys.stdout.write('\\033[2F')  # cursor up 2 lines\n            sys.stdout.write('\\033[J')  # clean the output (remove extra chars since last display)\n            sys.stdout.write('[{}] {}/{}, {:.1f} task/s, elapsed: {}s, ETA: {:5}s\\n{}\\n'.format(\n                bar_chars, self.completed, self.task_num, fps, int(elapsed + 0.5), eta, msg))\n        else:\n            sys.stdout.write('completed: {}, elapsed: {}s, {:.1f} tasks/s'.format(\n                self.completed, int(elapsed + 0.5), fps))\n        sys.stdout.flush()\n\ndef bitWise_accurary(msg_fake, message):\n    #\n    if msg_fake == None:\n        return None, None\n    else:\n        DecodedMsg_rounded = msg_fake.detach().cpu().numpy().round().clip(0, 1)\n        \n        diff = DecodedMsg_rounded - message.detach().cpu().numpy().round().clip(0, 1)\n        count = np.sum(np.abs(diff))\n        b, l = msg_fake.shape\n        \n        accuracy = (1 - count / (b * l)) \n        BitWise_AvgErr = count / (b * l)\n        \n        return accuracy * 100, BitWise_AvgErr\n\ndef decoded_message_error_rate(message, decoded_message):\n    message = message.view(message.shape[0], -1).squeeze()\n    length = message.shape[0]\n    message = message.gt(0)\n    decoded_message = decoded_message.gt(0)\n    error_rate = float(sum(message != decoded_message)) / length\n    return error_rate\n\ndef decoded_message_error_rate_batch(messages, decoded_messages):\n    error_rate = 0.0\n    batch_size = len(messages)\n    for i in range(batch_size):\n        error_rate += decoded_message_error_rate(messages[i], decoded_messages[i])\n    error_rate /= batch_size\n    return error_rate\n"}
