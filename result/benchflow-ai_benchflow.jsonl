{"repo_info": {"repo_name": "benchflow", "repo_owner": "benchflow-ai", "repo_url": "https://github.com/benchflow-ai/benchflow"}}
{"type": "test_file", "path": "examples/test_crag.py", "content": "import os\n\nfrom benchflow import load_benchmark\nfrom benchflow.agents.crag_openai import CRAGAgent\n\nbench = load_benchmark(benchmark_name=\"CRAG\", bf_token=os.getenv(\"BF_TOKEN\"))\n\nyour_agents = CRAGAgent()\n\nparams = {\n    \"OPENAI_API_KEY\": os.getenv(\"OPENAI_API_KEY\"),\n    \"EVALUATION_MODEL_NAME\": \"gpt-4o-mini\"\n}\n\nrun_ids = bench.run(\n    task_ids=[\"0\"],\n    agents=your_agents,\n    requirements_txt=\"crag_requirements.txt\",\n    api={\"OPENAI_API_KEY\": os.getenv(\"OPENAI_API_KEY\")},\n    params=params\n)\n\nresults = bench.get_results(run_ids)"}
{"type": "test_file", "path": "examples/test_bird.py", "content": "from benchflow.agents.bird_openai import BridAgent\nfrom benchflow import load_benchmark\nimport os\n\nbench = load_benchmark(benchmark_name=\"benchflow/Bird\", bf_token=os.getenv(\"BF_TOKEN\"))\n\nyour_agents = BridAgent()\n\nrun_ids = bench.run(\n    task_ids=[\"all\"],\n    agents=your_agents,\n    api={\"provider\": \"openai\", \"model\": \"gpt-4o-mini\", \"OPENAI_API_KEY\": os.getenv(\"OPENAI_API_KEY\")},\n    requirements_txt=\"bird_requirements.txt\",\n    args={}\n)\n\nresults = bench.get_results(run_ids)\n"}
{"type": "test_file", "path": "examples/test_medqa.py", "content": "from benchflow import load_benchmark\nfrom medqa_openai import MedQAAgent\nimport os\n\n\ndef test_medqa():\n    bench = load_benchmark(benchmark_name=\"benchflow/medqa-cs\", bf_token=os.getenv(\"BF_TOKEN\"))\n\n    agent = MedQAAgent()\n\n    run_ids = bench.run(\n        task_ids=[\"diagnosis\"], # choices: \"diagnosis\", \"treatment\", \"prevention\", \"all\"\n        agents=agent,\n        api={\"provider\": \"openai\", \"model\": \"gpt-4o-mini\", \"OPENAI_API_KEY\": os.getenv(\"OPENAI_API_KEY\")},\n        requirements_txt=\"medqa_requirements.txt\",\n        args={\n            \"OPENAI_API_KEY\": os.getenv(\"OPENAI_API_KEY\"), # required for llm as an evaluator\n            \"CASE_ID\": \"1\", # use \"all\" to run all cases in diagnosis section\n        },\n    )\n\n    results = bench.get_results(run_ids)\n\n    assert len(results) > 0\n\nif __name__ == \"__main__\":\n    test_medqa()"}
{"type": "test_file", "path": "examples/test_webarena.py", "content": "import os\n\nfrom benchflow import load_benchmark\nfrom benchflow.agents.webarena_openai import WebarenaAgent\n\n\ndef test_webarena_benchmark():\n    bench = load_benchmark(\n        benchmark_name=\"benchflow/webarena\", \n        bf_token=os.getenv(\"BF_TOKEN\")\n    )\n\n    your_agents = WebarenaAgent()\n\n    run_ids = bench.run(\n        task_ids=[0],\n        agents=your_agents,\n        api={\n            \"provider\": \"openai\", \n            \"model\": \"gpt-4o-mini\", \n            \"OPENAI_API_KEY\": os.getenv(\"OPENAI_API_KEY\")\n        },\n        requirements_txt=\"webarena_requirements.txt\",\n        args={}\n    )\n\n    results = bench.get_results(run_ids)\n    \n    assert len(results) > 0\n\nif __name__ == \"__main__\":\n    test_webarena_benchmark()"}
{"type": "test_file", "path": "examples/test_webcanvas.py", "content": "import os\n\nfrom benchflow import load_benchmark\nfrom benchflow.agents.webcanvas_openai import WebcanvasAgent\n\n\ndef test_webcanvas_benchmark():\n    bench = load_benchmark(benchmark_name=\"benchflow/Webcanvas\", bf_token=os.getenv(\"BF_TOKEN\"))\n\n    your_agents = WebcanvasAgent()\n\n    args = {\n    \"BROWSERBASE_API_KEY\": os.environ.get(\"BROWSERBASE_API_KEY\"),\n    \"GRAPHQL_USERNAME\": os.environ.get(\"GRAPHQL_USERNAME\"), \n    \"GRAPHQL_PASSWORD\": os.environ.get(\"GRAPHQL_PASSWORD\"),\n    \"OPENAI_API_KEY\": os.environ.get(\"OPENAI_API_KEY\")\n    }\n\n    run_ids = bench.run(\n        task_ids=[1],\n        agents=your_agents,\n        requirements_txt = \"webcanvas_requirements.txt\",\n        api={\"OPENAI_API_KEY\": os.getenv(\"OPENAI_API_KEY\")},\n        args=args\n    )\n\n    results = bench.get_results(run_ids)\n\n    assert len(results) > 0\n\nif __name__ == \"__main__\":\n    test_webcanvas_benchmark()\n"}
{"type": "test_file", "path": "examples/test_rarebench.py", "content": "import os\n\nfrom benchflow import load_benchmark\nfrom benchflow.agents.rarebench_openai import RarebenchAgent\n\n\ndef test_rarebench_benchmark():\n    bench = load_benchmark(benchmark_name=\"benchflow/Rarebench\", bf_token=os.getenv(\"BF_TOKEN\"))\n\n    your_agents = RarebenchAgent()\n\n    run_ids = bench.run(\n        task_ids=[\"MME\"],\n        agents=your_agents,\n        api={\"provider\": \"openai\", \"model\": \"gpt-4o-mini\", \"OPENAI_API_KEY\": os.getenv(\"OPENAI_API_KEY\")},\n        requirements_txt=\"rarebench_requirements.txt\",\n        args={\n            \"OPENAI_API_KEY\": os.getenv(\"OPENAI_API_KEY\"),\n        },\n    )\n\n    results = bench.get_results(run_ids)\n\n    assert len(results) > 0\n\nif __name__ == \"__main__\":\n    test_rarebench_benchmark()\n"}
{"type": "test_file", "path": "examples/test_swebench.py", "content": "import os\n\nfrom benchflow import load_benchmark\nfrom benchflow.agents.swebench_sweagent import SWEAgent\n\n\ndef test_swebench_benchmark():\n    bench = load_benchmark(benchmark_name=\"benchflow/Swebench\", bf_token=os.getenv(\"BF_TOKEN\"))\n\n    your_agents = SWEAgent()\n\n    run_ids = bench.run(\n        task_ids=[\"astropy__astropy-12907\"],\n        agents=your_agents,\n        install_sh=\"install_sweagent.sh\",\n        requirements_txt=\"sweagent_requirements.txt\",\n        api={\"OPENAI_API_KEY\": os.getenv(\"OPENAI_API_KEY\")},\n        args={}\n    )\n\n    results = bench.get_results(run_ids)\n\n    assert len(results) > 0\n\nif __name__ == \"__main__\":\n    test_swebench_benchmark()\n"}
{"type": "test_file", "path": "examples/test_mmlupro.py", "content": "import os\n\nfrom benchflow import load_benchmark\nfrom benchflow.agents.mmlu_openai import MMLUAgent\n\nbench = load_benchmark(benchmark_name=\"benchflow/MMLU-PRO\", bf_token=os.getenv(\"BF_TOKEN\"))\n\nyour_agents = MMLUAgent()\n\nrun_ids = bench.run(\n    task_ids=[\"history\"],\n    agents=your_agents,\n    requirements_txt=\"mmlupro_requirements.txt\",\n    api={\"OPENAI_API_KEY\": os.getenv(\"OPENAI_API_KEY\")},\n    args={}\n)\n\nresults = bench.get_results(run_ids)"}
{"type": "source_file", "path": "examples/medqa_openai.py", "content": "from openai import OpenAI\nfrom benchflow import BaseAgent\nimport os\n\nclass MedQAAgent(BaseAgent):\n    def call_api(self, task_step_inputs) -> str:\n        print(task_step_inputs)\n        client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n\n        response = client.chat.completions.create(\n            model=\"gpt-4o-mini\",\n            messages=[{\"role\": \"user\", \"content\": task_step_inputs[\"user_prompt\"]}],\n        )\n        return response.choices[0].message.content\n"}
{"type": "source_file", "path": "examples/modal_qwen.py", "content": "# ---\n# pytest: false\n# ---\n\n# # Run OpenAI-compatible LLM inference with LLaMA 3.1-8B and vLLM\n\n# LLMs do more than just model language: they chat, they produce JSON and XML, they run code, and more.\n# This has complicated their interface far beyond \"text-in, text-out\".\n# OpenAI's API has emerged as a standard for that interface,\n# and it is supported by open source LLM serving frameworks like [vLLM](https://docs.vllm.ai/en/latest/).\n\n# In this example, we show how to run a vLLM server in OpenAI-compatible mode on Modal.\n\n# Our examples repository also includes scripts for running clients and load-testing for OpenAI-compatible APIs\n# [here](https://github.com/modal-labs/modal-examples/tree/main/06_gpu_and_ml/llm-serving/openai_compatible).\n\n# You can find a video walkthrough of this example and the related scripts on the Modal YouTube channel\n# [here](https://www.youtube.com/watch?v=QmY_7ePR1hM).\n\n# ## Set up the container image\n\n# Our first order of business is to define the environment our server will run in:\n# the [container `Image`](https://modal.com/docs/guide/custom-container).\n# vLLM can be installed with `pip`.\n\nimport modal\n\nvllm_image = (\n    modal.Image.debian_slim(python_version=\"3.12\")\n    .pip_install(\n        \"vllm==0.7.2\",\n        \"huggingface_hub[hf_transfer]==0.26.2\",\n        \"flashinfer-python==0.2.0.post2\",  # pinning, very unstable\n        extra_index_url=\"https://flashinfer.ai/whl/cu124/torch2.5\",\n    )\n    .env({\"HF_HUB_ENABLE_HF_TRANSFER\": \"1\"})  # faster model transfers\n)\n\n# In its 0.7 release, vLLM added a new version of its backend infrastructure,\n# the [V1 Engine](https://blog.vllm.ai/2025/01/27/v1-alpha-release.html).\n# Using this new engine can lead to some [impressive speedups](https://github.com/modal-labs/modal-examples/pull/1064),\n# but as of version 0.7.2 the new engine does not support all inference engine features\n# (including important performance optimizations like\n# [speculative decoding](https://docs.vllm.ai/en/v0.7.2/features/spec_decode.html)).\n\n# The features we use in this demo are supported, so we turn the engine on by setting an environment variable\n# on the Modal Image.\n\nvllm_image = vllm_image.env({\"VLLM_USE_V1\": \"1\"})\n\n# ## Download the model weights\n\n# We'll be running a pretrained foundation model -- Meta's LLaMA 3.1 8B\n# in the Instruct variant that's trained to chat and follow instructions,\n# quantized to 4-bit by [Neural Magic](https://neuralmagic.com/) and uploaded to Hugging Face.\n\n# You can read more about the `w4a16` \"Machete\" weight layout and kernels\n# [here](https://neuralmagic.com/blog/introducing-machete-a-mixed-input-gemm-kernel-optimized-for-nvidia-hopper-gpus/).\n\nMODELS_DIR = \"/qwen\"\nMODEL_NAME = \"Qwen/QwQ-32B\"\nMODEL_REVISION = \"f28e641280ed3228b25df45b02ce6526b472cbea\"\n\n# Although vLLM will download weights on-demand, we want to cache them if possible. We'll use [Modal Volumes](https://modal.com/docs/guide/volumes),\n# which act as a \"shared disk\" that all Modal Functions can access, for our cache.\n\nhf_cache_vol = modal.Volume.from_name(\n    \"huggingface-cache\", create_if_missing=True\n)\nvllm_cache_vol = modal.Volume.from_name(\"vllm-cache\", create_if_missing=True)\n\n\n# ## Build a vLLM engine and serve it\n\n# The function below spawns a vLLM instance listening at port 8000, serving requests to our model. vLLM will authenticate requests\n# using the API key we provide it.\n\n# We wrap it in the [`@modal.web_server` decorator](https://modal.com/docs/guide/webhooks#non-asgi-web-servers)\n# to connect it to the Internet.\n\napp = modal.App(\"example-vllm-openai-compatible\")\n\nN_GPU = 1  # tip: for best results, first upgrade to more powerful GPUs, and only then increase GPU count\nAPI_KEY = \"super-secret-key\"  # api key, for auth. for production use, replace with a modal.Secret\n\nMINUTES = 60  # seconds\n\nVLLM_PORT = 8000\n\n\n@app.function(\n    image=vllm_image,\n    gpu=f\"H100:{N_GPU}\",\n    # how many requests can one replica handle? tune carefully!\n    allow_concurrent_inputs=100,\n    # how long should we stay up with no requests?\n    scaledown_window=15 * MINUTES,\n    volumes={\n        \"/root/.cache/huggingface\": hf_cache_vol,\n        \"/root/.cache/vllm\": vllm_cache_vol,\n    },\n)\n@modal.web_server(port=VLLM_PORT, startup_timeout=5 * MINUTES)\ndef serve():\n    import subprocess\n\n    cmd = [\n        \"vllm\",\n        \"serve\",\n        \"--uvicorn-log-level=info\",\n        MODEL_NAME,\n        \"--revision\",\n        MODEL_REVISION,\n        \"--host\",\n        \"0.0.0.0\",\n        \"--port\",\n        str(VLLM_PORT),\n        \"--api-key\",\n        API_KEY,\n        \"--gpu-memory-utilization\",\n        \"0.99\", \n        \"--max-model-len\",\n        \"16384\",\n    ]\n\n    subprocess.Popen(\" \".join(cmd), shell=True)\n\n\n# ## Deploy the server\n\n# To deploy the API on Modal, just run\n# ```bash\n# modal deploy vllm_inference.py\n# ```\n\n# This will create a new app on Modal, build the container image for it if it hasn't been built yet,\n# and deploy the app.\n\n# ## Interact with the server\n\n# Once it is deployed, you'll see a URL appear in the command line,\n# something like `https://your-workspace-name--example-vllm-openai-compatible-serve.modal.run`.\n\n# You can find [interactive Swagger UI docs](https://swagger.io/tools/swagger-ui/)\n# at the `/docs` route of that URL, i.e. `https://your-workspace-name--example-vllm-openai-compatible-serve.modal.run/docs`.\n# These docs describe each route and indicate the expected input and output\n# and translate requests into `curl` commands.\n\n# For simple routes like `/health`, which checks whether the server is responding,\n# you can even send a request directly from the docs.\n\n# To interact with the API programmatically in Python, we recommend the `openai` library.\n\n# See the `client.py` script in the examples repository\n# [here](https://github.com/modal-labs/modal-examples/tree/main/06_gpu_and_ml/llm-serving/openai_compatible)\n# to take it for a spin:\n\n# ```bash\n# # pip install openai==1.13.3\n# python openai_compatible/client.py\n# ```\n\n\n# ## Testing the server\n\n# To make it easier to test the server setup, we also include a `local_entrypoint`\n# that does a healthcheck and then hits the server.\n\n# If you execute the command\n\n# ```bash\n# modal run vllm_inference.py\n# ```\n\n# a fresh replica of the server will be spun up on Modal while\n# the code below executes on your local machine.\n\n# Think of this like writing simple tests inside of the `if __name__ == \"__main__\"`\n# block of a Python script, but for cloud deployments!\n\n\n@app.local_entrypoint()\ndef test(test_timeout=5 * MINUTES):\n    import json\n    import time\n    import urllib\n\n    print(f\"Running health check for server at {serve.web_url}\")\n    up, start, delay = False, time.time(), 10\n    while not up:\n        try:\n            with urllib.request.urlopen(serve.web_url + \"/health\") as response:\n                if response.getcode() == 200:\n                    up = True\n        except Exception:\n            if time.time() - start > test_timeout:\n                break\n            time.sleep(delay)\n\n    assert up, f\"Failed health check for server at {serve.web_url}\"\n\n    print(f\"Successful health check for server at {serve.web_url}\")\n\n    messages = [{\"role\": \"user\", \"content\": \"Testing! Is this thing on?\"}]\n    print(f\"Sending a sample message to {serve.web_url}\", *messages, sep=\"\\n\")\n\n    headers = {\n        \"Authorization\": f\"Bearer {API_KEY}\",\n        \"Content-Type\": \"application/json\",\n    }\n    payload = json.dumps({\"messages\": messages, \"model\": MODEL_NAME})\n    req = urllib.request.Request(\n        serve.web_url + \"/v1/chat/completions\",\n        data=payload.encode(\"utf-8\"),\n        headers=headers,\n        method=\"POST\",\n    )\n    with urllib.request.urlopen(req) as response:\n        print(json.loads(response.read().decode()))\n\n\n# We also include a basic example of a load-testing setup using\n# `locust` in the `load_test.py` script [here](https://github.com/modal-labs/modal-examples/tree/main/06_gpu_and_ml/llm-serving/openai_compatible):\n\n# ```bash\n# modal run openai_compatible/load_test.py\n# ```"}
{"type": "source_file", "path": "src/benchflow/BaseAgent.py", "content": "import logging\nfrom abc import ABC, abstractmethod\nfrom typing import Any, Dict, final\n\nimport uvicorn\nfrom fastapi import FastAPI, HTTPException\n\nlogger = logging.getLogger(__name__)\n\nclass BaseAgent(ABC):\n    \"\"\"\n    You need to extend this class to make your agent a server.\n    So that it can communicate with the benchmark client.\n    If you want to integrate your agent with BenchFlow, you need to implement the following methods:\n    ```\n    - call_api\n    ```\n    \"\"\"\n    def __init__(self):\n        self.app = FastAPI()\n        self.setup_routes()\n\n    @final\n    def setup_routes(self):\n        \"\"\"\n        Setup the routes for the agent.\n        \"\"\"\n        @self.app.post(\"/response\")\n        async def take_action(input_data: Dict[str, Any]):\n            try:\n                if input_data.get(\"env_info\") is not None:\n                    response = self.call_api(input_data.get(\"env_info\"))\n                elif input_data.get(\"input_data\") is not None:\n                    response = self.call_api(input_data.get(\"input_data\"))\n                else:\n                    response = self.call_api(input_data)\n                logger.info(f\"[BaseAgent]: Got response from API: {response}\")\n                return response\n            except Exception as e:\n                logger.error(f\"[BaseAgent]: Error getting response: {str(e)}\")\n                raise HTTPException(status_code=500, detail=str(e))\n        \n        @self.app.get(\"/\")\n        async def root():\n            return {\"message\": \"Welcome to Benchmarkthing Agent API\"}\n\n    @final\n    def run_with_endpoint(self, host: str, port: int):\n        \"\"\"\n        Run the agent server.\n        \"\"\"\n        logger.info(f\"Starting agent server on {host}:{port}\")\n        uvicorn.run(self.app, host=host, port=port)\n\n    @abstractmethod \n    def call_api(self, task_step_inputs: Dict[str, Any]) -> str:\n        \"\"\"\n        You can get the request information from the task_step_inputs parameter.\n        The task_step_inputs is a dictionary that contains the keys provided by the benchmark client.\n        You need to refer to the benchmark documentation to get the keys.\n\n        This method is called when the agent server receives a request from the benchmark client.\n        You need to implement this method to make your agent work and return the response to the benchmark client.\n        Your response could be a real action(e.g. click, scroll, etc) or just any prediction(e.g. code, text, etc) needed by the benchmark.\n        \"\"\"\n        pass"}
{"type": "source_file", "path": "src/benchflow/agents/__init__.py", "content": ""}
{"type": "source_file", "path": "src/benchflow/BaseBench.py", "content": "import logging\nimport os\nimport sys\nimport time\nfrom abc import ABC, abstractmethod\nfrom typing import Any, Dict, Optional, final\n\nimport docker\nfrom pydantic import ValidationError\n\nfrom benchflow.schemas import BenchArgs, BenchmarkResult\n\n\nclass ColoredFormatter(logging.Formatter):\n    def __init__(self):\n        super().__init__(\n           fmt='%(colored_level)s: -- %(name)s -- %(message)s',\n           datefmt='%H:%M:%S'\n       )\n\n    def format(self, record):\n        record.msg = \" \".join(record.msg.strip().splitlines())\n        return super().format(record).strip()\n\ndef setup_logger(name: str, log_file: Optional[str] = None) -> logging.Logger:\n    logger = logging.getLogger(name)\n    logger.setLevel(logging.INFO)\n\n    if not logger.hasHandlers():\n        console_handler = logging.StreamHandler(sys.stdout)\n        console_handler.setFormatter(ColoredFormatter())\n        logger.addHandler(console_handler)\n\n        if log_file:\n            file_handler = logging.FileHandler(log_file)\n            file_handler.setFormatter(ColoredFormatter())\n            logger.addHandler(file_handler)\n\n    return logger\n\nclass BaseBench(ABC):\n    \"\"\"\n    Base class for all benchmarks. (Now you should name your benchmark class end with \"Bench\". To be deleted in benchflow v0.2.0)    \n    If you want to integrate your benchmark with BenchFlow, you need to implement the following methods:\n    ```\n    - get_args\n    - get_image_name\n    - get_results_dir_in_container\n    - get_log_files_dir_in_container\n    - get_result\n    - get_all_tasks\n    ```\n    Please open a PR to add your benchmark to the BenchFlow benchmarks.\n    All you need to include in the PR is a script with the definition of the subclass of BaseBench and BenchArgs.\n    \"\"\"\n    def __init__(self):\n        self.logger = setup_logger(self.__class__.__name__)\n        self.docker_client = docker.from_env()\n\n    @final\n    def run_bench(self, task_id: str, agent_url: str, arguments: Dict[str, Any]) -> BenchmarkResult:\n        \"\"\"\n        Run the benchmark through docker.\n        \"\"\"\n        args_config = self.get_args(task_id)\n        arguments = args_config.get_args(arguments)\n        arguments.update({\n            \"INTELLIGENCE_URL\": agent_url,\n            \"TEST_START_IDX\": str(task_id),\n        })\n\n        bench_name = self.__class__.__name__\n        timestamp = str(time.time())\n        self.results_dir = os.path.abspath(f\"./tmp/{bench_name}/results/{timestamp}/{task_id}\")\n        self.log_files_dir = os.path.abspath(f\"./tmp/{bench_name}/logs/{timestamp}/{task_id}\")\n        os.makedirs(self.results_dir, exist_ok=True)    \n        os.makedirs(self.log_files_dir, exist_ok=True)\n\n        try:\n            container = self.docker_client.containers.run(\n                image=self.get_image_name(),\n                environment=arguments,\n                volumes=self.get_volumes(),\n                remove=True,\n                detach=True\n            )\n\n            for line in container.logs(stream=True):\n                line_str = line.decode('utf-8').strip()\n                self.logger.info(line_str)\n\n            container.wait()\n\n            result = self.get_result(task_id)\n            if isinstance(result, Dict):\n                result = BenchmarkResult(**result)\n            return result\n        except ValidationError as e:\n            return BenchmarkResult(task_id=task_id, is_resolved=False, metrics={\"score\": 0}, log={\"error\": \"Benchmark result is invalid\", \"result\": str(e)}, other={})\n        except docker.errors.ImageNotFound:\n            return BenchmarkResult(task_id=task_id, is_resolved=False, metrics={\"score\": 0}, log={\"error\": \"Image not found\"}, other={})\n        except Exception as e:\n            self.logger.exception(\"Error during benchmark execution:\")\n            return BenchmarkResult(task_id=task_id, is_resolved=False, metrics={\"score\": 0}, log={\"error\": str(e)}, other={})\n\n    @final\n    def get_volumes(self) -> Dict[str, Dict[str, str]]:\n        \"\"\"\n        Get the volumes of the benchmark.\n        The volumes are used to store the results and log files of the benchmark.\n        \"\"\"\n        return {\n            f\"{self.results_dir}\": {\n                'bind': f\"{self.get_results_dir_in_container()}\",\n                'mode': 'rw'\n            },\n            f\"{self.log_files_dir}\": {\n                'bind': f\"{self.get_log_files_dir_in_container()}\",\n                'mode': 'rw'\n            },\n            \"/var/run/docker.sock\": {\n                'bind': \"/var/run/docker.sock\",\n                'mode': 'rw'\n            }\n        }\n    \n    @abstractmethod\n    def get_args(self, task_id: str) -> BenchArgs:\n        \"\"\"\n        Benchmark need to deal with the END_IDX so that it can only run one task at a time\n        task_id is the start index of the task. You can also make your benchmark a single \n        whole task. But if you want to run your benchmark in parallel, you need to split \n        your benchmark into multiple tasks.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def get_image_name(self) -> str:\n        \"\"\"\n        Return the image name you uploaded to the docker hub.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def get_results_dir_in_container(self) -> str:\n        \"\"\"\n        Return the directory in the container to store the results.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def get_log_files_dir_in_container(self) -> str:\n        \"\"\"\n        Return the directory in the container to store the log files (trace, trajectory, etc).\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def get_result(self, task_id: str) -> BenchmarkResult:\n        \"\"\"\n        You should return the results in this function.\n        \n        Return a BenchmarkResult containing the benchmark results.\n\n        The BenchmarkResult model has the following fields:\n            - is_resolved (bool): Indicates whether the task is resolved.\n            - message (dict): Contains additional information to be displayed to the agent user.\n            - log (str): Contains the log output (e.g., trace, trajectory, etc).\n            - metrics (dict): A dictionary of various metrics, where each metric can be of different types (e.g., bool, int, float, or str).\n            - other (dict): Any extra fields or metadata relevant to the benchmark result.\n        \n        Please refer to the example in the definition of BenchmarkResult for the expected format.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def get_all_tasks(self, split: str) -> Dict[str, Any]:\n        \"\"\"\n        Return all task_ids and optional error messages.\n        \n        For example:\n        ```\n            { \"task_ids\": [...], \"error_message\": None }\n        ```\n        \n        You can use index as the task_id if your benchmark doesn't have a meaningful field for task_id.\n        \n        For example:\n        ```\n            task_ids = list(str(i) for i in range(len(number_of_your_benchmark_tasks)))\n        ```\n        \"\"\"\n        pass\n"}
{"type": "source_file", "path": "src/benchflow/agents/webarena_langbase.py", "content": "import logging\nimport os\nimport re\n\nimport requests\n\nfrom benchflow import BaseAgent\n\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n)\n\nlogger = logging.getLogger(__name__)\n\nclass WebarenaAgent(BaseAgent):\n    def __init__(self):\n        super().__init__()\n        self.api_key = os.getenv(\"LANGBASE_API_KEY\")\n        self.url = \"https://api.langbase.com/v1/pipes/run\"\n \n    def _construct_message(self, env_info) -> str:\n        # Deserialize observation to original format for API call\n        return f\"\"\"OBSERVATION: {env_info['observation'][\"text\"]} URL: {env_info['url']} OBJECTIVE: {env_info['intent']} PREVIOUS ACTION: {env_info['previous_action'] }\"\"\"\n \n    def _extract_action(self, response: str) -> str:\n        # find the first occurence of action\n        action_splitter = \"```\"\n        pattern = rf\"{action_splitter}((.|\\n)*?){action_splitter}\"\n        match = re.search(pattern, response)\n        if match:\n            return match.group(1).strip()\n        else:\n            raise Exception(\n                f'Cannot find the action in \"{response}\"'\n            )\n \n    def call_api(self, task_step_inputs) -> str:\n        message = self._construct_message(task_step_inputs)\n        print(message)\n        data = {\n            'messages': [{'role': 'user', 'content': message}],\n            'stream': False\n        }\n        headers = {\n            \"Content-Type\": \"application/json\",\n            \"Authorization\": f\"Bearer {self.api_key}\"\n        }\n        \n        try:\n            logger.info(\"[UserAgent]: Calling API\")\n            response = requests.post(self.url, headers=headers, json=data)\n            response.raise_for_status()\n \n            if not response.ok:\n                logger.error(f\"[UserAgent]: API error: {response.json()}\")\n                raise Exception(f\"API error: {response.json()}\")\n \n            result = response.json()\n            if result['success']:\n                raw_completion = result[\"completion\"]\n                logger.info(f\"raw completion: {raw_completion}\")\n                action = self._extract_action(raw_completion)\n                logger.info(f\"[UserAgent]: Got action from API: {action}\")\n                return action\n \n            raise Exception(\"API call failed.\")\n        except Exception as e:\n            logger.error(f\"[UserAgent]: Error calling API: {str(e)}\")\n            raise"}
{"type": "source_file", "path": "src/benchflow/benchmarks/__init__.py", "content": "import importlib\nimport inspect\nimport pkgutil\n\nfrom benchflow import BaseBench\n\n__all__ = []\n\nfor finder, module_name, is_pkg in pkgutil.iter_modules(__path__):\n    module = importlib.import_module(f\".{module_name}\", __name__)\n    for name, obj in inspect.getmembers(module, inspect.isclass):\n        if obj.__module__ == module.__name__:\n            if issubclass(obj, BaseBench) and obj is not BaseBench:\n                globals()[name] = obj\n                __all__.append(name)"}
{"type": "source_file", "path": "src/benchflow/agents/rarebench_openai.py", "content": "import os\nfrom typing import Any, Dict\n\nfrom openai import OpenAI\n\nfrom benchflow import BaseAgent\n\n\nclass RarebenchAgent(BaseAgent):\n    def __init__(self):\n        super().__init__()\n        self.api_key = os.getenv(\"OPENAI_API_KEY\")\n\n    def call_api(self, task_step_inputs: Dict[str, Any]) -> str:\n        messages = [\n                    {\"role\": \"system\", \"content\": task_step_inputs[\"system_prompt\"]},\n                    {\"role\": \"user\", \"content\": task_step_inputs[\"prompt\"]},\n                ]\n        try:\n            client = OpenAI(\n                api_key=self.api_key,  # This is the default and can be omitted\n            )\n\n            response = client.chat.completions.create(\n                messages=messages,\n                model=\"gpt-4o-mini\",\n                temperature=0.9,\n            )\n            content = response.choices[0].message.content\n            return content\n        except Exception:\n            raise"}
{"type": "source_file", "path": "src/benchflow/agents/mmlu_openai.py", "content": "import logging\nimport os\n\nfrom openai import OpenAI\n\nfrom benchflow import BaseAgent\n\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\nlogger = logging.getLogger(__name__)\n\nclass MMLUAgent(BaseAgent):\n    def __init__(self):\n        super().__init__()\n        self.api_key = os.getenv(\"OPENAI_API_KEY\")\n        print(self.api_key)\n\n    def call_api(self, task_step_inputs):\n        prompt = task_step_inputs[\"prompt\"]\n        input_text = task_step_inputs[\"input_text\"]\n\n        try:\n            logger.info(\"[UserAgent]: Calling OpenAI API\")\n            client = OpenAI(\n                api_key=self.api_key,  # This is the default and can be omitted\n            )\n\n            response = client.chat.completions.create(\n                messages=[\n                    {\"role\": \"system\", \"content\": prompt},\n                    {\"role\": \"user\", \"content\": input_text}\n                ],\n                model=\"gpt-4o-mini\",\n                temperature=0.9,\n            )\n            content = response.choices[0].message.content\n            logger.info(f\"[UserAgent]: Got action: {content}\")\n            return content\n        except Exception as e:\n            logger.error(f\"[UserAgent]: Error calling OpenAI API: {e}\")\n            raise"}
{"type": "source_file", "path": "src/benchflow/agents/bird_openai.py", "content": "import os\nfrom openai import OpenAI\nfrom benchflow import BaseAgent\n\nclass BridAgent(BaseAgent):\n    def __init__(self):\n        super().__init__()\n        self.api_key = os.getenv(\"OPENAI_API_KEY\")\n\n    def call_api(self, task_step_inputs):\n        client = OpenAI(\n                api_key=self.api_key,  # This is the default and can be omitted\n            )\n        messages = [\n            {\"role\": \"system\", \"content\": task_step_inputs[\"system_prompt\"]},\n            {\"role\": \"user\", \"content\": task_step_inputs[\"user_prompt\"]}\n        ]\n        response = client.chat.completions.create(\n            messages=messages,\n            model=\"gpt-3.5-turbo\",\n            temperature=0.9,\n        )\n        content = response.choices[0].message.content\n        return content\n"}
{"type": "source_file", "path": "src/benchflow/agents/crag_openai.py", "content": "import logging\nimport os\nfrom typing import Any, Dict\n\nfrom openai import OpenAI\n\nfrom benchflow import BaseAgent\n\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\nlogger = logging.getLogger(__name__)\n\nclass CRAGAgent(BaseAgent):\n    def __init__(self):\n        super().__init__()\n        self.api_key = os.getenv(\"OPENAI_API_KEY\")\n        self.system_instruction = (\n            \"\"\"You are an AI assistant tasked with answering questions based on provided context. \n            You will be given:\n            1. A question to answer\n            2. Relevant search results/documents as context\n            \n            Your task is to:\n            1. Read and understand the question\n            2. Analyze the provided context\n            3. Generate a concise, accurate answer based ONLY on the provided context\n            4. If you cannot find the answer in the context, respond with \"I don't know\"\n            \n            Rules:\n            - Only use information from the provided context\n            - Be concise and direct in your answers\n            - Do not make assumptions or add information not present in the context\n            - If the answer isn't in the context, say \"I don't know\"\n            \"\"\"\n        )\n\n    def _construct_message(self, env_info: Dict[str, Any]) -> str:\n        query = env_info.get('query', '')\n        logger.info(f\"[CRAGAgent]: Query: {query}\")\n        search_results = env_info.get('search_results', [])\n        logger.info(f\"[CRAGAgent]: Search results: {search_results}\")\n        \n        # search_results is now a list of strings (snippets)\n        context = \"\\n\\n\".join(\n            f\"Document {i+1}:\\n{snippet}\"\n            for i, snippet in enumerate(search_results)\n        )\n        \n        return (\n            f\"Question: {query}\\n\\n\"\n            f\"Context:\\n{context}\\n\\n\"\n            \"Answer:\"\n        )\n\n    def call_api(self, task_step_inputs: Dict[str, Any]) -> str:\n        try:\n            logger.info(\"[CRAGAgent]: Calling OpenAI API\")\n            client = OpenAI(api_key=self.api_key)\n            \n            messages = [\n                {\"role\": \"system\", \"content\": self.system_instruction},\n                {\"role\": \"user\", \"content\": self._construct_message(task_step_inputs)}\n            ]\n            \n            response = client.chat.completions.create(\n                model=\"gpt-4o\",\n                messages=messages,\n                temperature=0.0,\n                max_tokens=150\n            )\n            \n            answer = response.choices[0].message.content.strip()\n            logger.info(f\"[CRAGAgent]: Generated answer: {answer}\")\n            return answer\n            \n        except Exception as e:\n            logger.error(f\"[CRAGAgent]: Error calling OpenAI API: {e}\")\n            raise\n\ndef main():\n    logger.info(\"Starting CRAGAgent...\")\n    agent = CRAGAgent()\n    logger.info(\"Running agent on http://0.0.0.0:9000\")\n    agent.run_with_endpoint(host=\"0.0.0.0\", port=9000)\n\nif __name__ == \"__main__\":\n    main()"}
{"type": "source_file", "path": "src/benchflow/agents/swebench_sweagent.py", "content": "import json\nimport logging\nimport os\nimport re\nimport shutil\nimport subprocess\n\nfrom benchflow import BaseAgent\n\n\nclass SWEAgent(BaseAgent):\n    def __init__(self):\n        super().__init__()\n        self.model_name = \"gpt-4o\"\n        \n    def call_api(self, task_step_inputs) -> str:\n        instance_id = task_step_inputs[\"instance_id\"]\n        shutil.rmtree(\"trajectories/root/\", ignore_errors=True)\n        cmd = f\"sweagent run-batch --agent.model.name {self.model_name} --agent.model.per_instance_cost_limit 0.10 --instances.split test --instances.filter {instance_id}\"\n        result = subprocess.run(cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n        logging.info(\"sweagent result: %s\", result.stdout)\n        logging.error(\"sweagent error: %s\", result.stderr)\n        return self.parse_action(instance_id, result.stdout)\n        \n    def parse_action(self, instance_id: str, log_content: str) -> str:\n        pattern = r\"Wrote merged predictions to\\s+((?:/.*\\n\\s*)+.*preds\\.json)\"\n        match = re.search(pattern, log_content, re.DOTALL)\n        \n        if match:\n            file_path_raw = match.group(1)\n            file_path = re.sub(r\"\\s+\", \"\", file_path_raw)\n            \n            if not os.path.exists(file_path):\n                print(f\"error: {file_path} not exists\")\n                return None\n            else:\n                try:\n                    with open(file_path, 'r') as f:\n                        predictions = json.load(f)\n                    action = predictions[instance_id]['model_patch']\n                    return action\n                except json.JSONDecodeError:\n                    print(f\"error: {file_path} not a valid json\")\n                    return None\n                except Exception as e:\n                    print(f\"error: {str(e)}\")\n                    return None\n        else:\n            print(\"error: no predictions file path\")\n            return None"}
{"type": "source_file", "path": "src/benchflow/BenchClient.py", "content": "import logging\nfrom abc import ABC, abstractmethod\nfrom typing import Any, Dict, final\nfrom urllib.parse import urljoin\n\nimport requests\n\nlogger = logging.getLogger(__name__)\n\nclass BenchClient(ABC):\n    \"\"\"\n    The BenchClient is used to make the benchmark client so that it can communicate with the agent server.\n    You need to extend this class in your benchmark entrypoint (e.g. run.py).\n    You need to implement the following methods:\n        - prepare_input\n        - parse_response\n    \"\"\"\n    \n    def __init__(self, intelligence_url: str, max_retry: int = 1):\n        self.intelligence_url = intelligence_url.rstrip('/')\n        self.max_retry = max_retry\n        logger.info(f\"[{self.__class__.__name__}] Initialized with intelligence_url: {intelligence_url}\")\n\n    @final\n    def get_response(self, raw_step_inputs: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"\n        Get the response from the agent. You should use this method to get the response from the agent.\n        \"\"\"\n        if raw_step_inputs is None:\n            raise ValueError(\"raw_step_inputs cannot be None\")\n        \n        task_step_inputs = self.prepare_input(raw_step_inputs)\n        \n        for attempt in range(self.max_retry):\n            try:\n                response = requests.post(\n                    urljoin(self.intelligence_url, \"response\"),\n                    json=task_step_inputs\n                )\n                response.raise_for_status()\n                break\n            except Exception as e:\n                if attempt == self.max_retry - 1:\n                    raise Exception(f\"Failed to get response after {self.max_retry} attempts: {str(e)}\")\n                logger.warning(f\"Attempt {attempt + 1} failed, retrying...\")\n                continue\n\n        try:\n            raw_response = response.json()\n            logger.info(f\"[{self.__class__.__name__}] Received response: {raw_response}\")\n            \n            parsed_response = self.parse_response(raw_response)\n            parsed_response[\"raw_response\"] = raw_response\n            \n            return parsed_response\n            \n        except KeyError as e:\n            raise ValueError(f\"Invalid response format from agent: {str(e)}\")\n        except Exception as e:\n            raise Exception(f\"Error parsing response: {str(e)}\")\n    \n    @abstractmethod\n    def prepare_input(self, raw_step_inputs: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"\n        Input:\n            raw_step_inputs: Dict[str, Any]\n            For example, \n                If your benchmark is a web agent benchmark, the raw_input_data could be the observation from the web page.\n                if your benchmark is a Q&A benchmark, the raw_input_data could be the question.\n                You should define the keys in the raw_input_data.\n                If your benchmark don't need to deal with the raw_input_data, you can just return the raw_input_data.\n        Output:\n            Dict[str, Any]\n            The input data of the task to be sent to the agent.\n            And add the keys to the benchmark documentation. # To Be Done in benchflow v0.2.0\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def parse_response(self, raw_response: str) -> Dict[str, Any]:\n        \"\"\"\n        Input:\n            raw_response: str\n            The raw response from the agent.\n\n        You can specify the format of the raw_action in the benchmark documentation. # To Be Done in benchflow v0.2.0\n        So that agent developers can know what to return.\n\n        For example,\n            you can specify the format of the raw_response as follows:\n            ```\n            \"action_type\": click\n            \"action_arguments\": arguments\n            ```\n            so that you can use regex to parse the response_type and response_arguments from the raw_response.\n            and return the parsed_action as follows:\n            ```\n            {\n                \"action_type\": click,\n                \"action_arguments\": arguments\n            }\n            ```\n\n        Output:\n            parsed_response: Dict[str, Any]\n            The parsed response.\n            You need to specify the keys in the parsed_response that you want to send to the benchmark.\n            And add the keys to the benchmark documentation. # To Be Done in benchflow v0.2.0\n        \"\"\"\n        pass\n"}
{"type": "source_file", "path": "src/benchflow/agents/webarena_openai.py", "content": "import logging\nimport os\nimport re\n\nfrom openai import OpenAI\n\nfrom benchflow import BaseAgent\n\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\nlogger = logging.getLogger(__name__)\n\nclass WebarenaAgent(BaseAgent):\n    def __init__(self):\n        super().__init__()\n        self.api_key = os.getenv(\"OPENAI_API_KEY\")\n        self.system_instruction = (\n            \"\"\"You are an autonomous intelligent agent tasked with navigating a web browser. You will be given web-based tasks. These tasks will be accomplished through the use of specific actions you can issue.\n\nHere's the information you'll have:\nThe user's objective: This is the task you're trying to complete.\nThe current web page's accessibility tree: This is a simplified representation of the webpage, providing key information.\nThe current web page's URL: This is the page you're currently navigating.\nThe open tabs: These are the tabs you have open.\nThe previous action: This is the action you just performed. It may be helpful to track your progress.\n\nThe actions you can perform fall into several categories:\n\nPage Operation Actions:\n`click [id]`: This action clicks on an element with a specific id on the webpage.\n`type [id] [content] [press_enter_after=0|1]`: Use this to type the content into the field with id. By default, the \"Enter\" key is pressed after typing unless press_enter_after is set to 0.\n`hover [id]`: Hover over an element with id.\n`press [key_comb]`:  Simulates the pressing of a key combination on the keyboard (e.g., Ctrl+v).\n`scroll [down|up]`: Scroll the page up or down.\n\nTab Management Actions:\n`new_tab`: Open a new, empty browser tab.\n`tab_focus [tab_index]`: Switch the browser's focus to a specific tab using its index.\n`close_tab`: Close the currently active tab.\n\nURL Navigation Actions:\n`goto [url]`: Navigate to a specific URL.\n`go_back`: Navigate to the previously viewed page.\n`go_forward`: Navigate to the next page (if a previous 'go_back' action was performed).\n\nCompletion Action:\n`stop [answer]`: Issue this action when you believe the task is complete. If the objective is to find a text-based answer, provide the answer in the bracket. If you believe the task is impossible to complete, provide the answer as \"N/A\" in the bracket.\n\nHomepage:\nIf you want to visit other websites, check out the homepage at http://homepage.com. It has a list of websites you can visit.\nhttp://homepage.com/password.html lists all the account name and password for the websites. You can use them to log in to the websites.\n\nTo be successful, it is very important to follow the following rules:\n1. You should only issue an action that is valid given the current observation\n2. You should only issue one action at a time.\n3. You should follow the examples to reason step by step and then issue the next action.\n4. Generate the action in the correct format. Start with a \"In summary, the next action I will perform is\" phrase, followed by action inside ``````. For example, \"In summary, the next action I will perform is ```click [1234]```\".\n5. Issue stop action when you think you have achieved the objective. Don't generate anything after stop.\"\"\"\n        )\n\n        self.example1_user = (\n            \"\"\"OBSERVATION:\\n[1744] link 'HP CB782A#ABA 640 Inkjet Fax Machine (Renewed)'\\n\\t\\t[1749] StaticText '$279.49'\\n\\t\\t[1757] button 'Add to Cart'\\n\\t\\t[1760] button 'Add to Wish List'\\n\\t\\t[1761] button 'Add to Compare'\\nURL: http://onestopmarket.com/office-products/office-electronics.html\\nOBJECTIVE: What is the price of HP Inkjet Fax Machine\\nPREVIOUS ACTION: None\"\"\"\n        )\n\n        self.example1_assistant = (\n            \"\"\"Let's think step-by-step. This page list the information of HP Inkjet Fax Machine, which is the product identified in the objective. Its price is $279.49. I think I have achieved the objective. I will issue the stop action with the answer. In summary, the next action I will perform is ```stop [$279.49]```\"\"\"\n        )\n\n        self.example2_user = (\n            \"\"\"OBSERVATION:\\n[164] textbox 'Search' focused: True required: False\\n[171] button 'Go'\\n[174] link 'Find directions between two points'\\n[212] heading 'Search Results'\\n[216] button 'Close'\\nURL: http://openstreetmap.org\\nOBJECTIVE: Show me the restaurants near CMU\\nPREVIOUS ACTION: None\"\"\"\n        )\n\n        self.example2_assistant = (\n            \"\"\"Let's think step-by-step. This page has a search box whose ID is [164]. According to the nominatim rule of openstreetmap, I can search for the restaurants near a location by \\\"restaurants near\\\". I can submit my typing by pressing the Enter afterwards. In summary, the next action I will perform is ```type [164] [restaurants near CMU] [1]```\"\"\"\n        )\n\n    def _construct_message(self, env_info):\n        return (\n            f\"OBSERVATION: {env_info['observation']['text']} \"\n            f\"URL: {env_info['url']} \"\n            f\"OBJECTIVE: {env_info['intent']} \"\n            f\"PREVIOUS ACTION: {env_info['previous_action']}\"\n        )\n\n    def _extract_action(self, response):\n        pattern = r\"```((.|\\n)*?)```\"\n        match = re.search(pattern, response)\n        if match:\n            return match.group(1).strip()\n        raise Exception(f'Cannot find the action in \"{response}\"')\n\n    def call_api(self, task_step_inputs):\n        system_msg_1 = {\"role\": \"system\", \"content\": self.system_instruction}\n        user_msg_1 = {\"role\": \"system\", \"name\": \"example_user\", \"content\": self.example1_user}\n        system_msg_2 = {\"role\": \"system\", \"name\": \"example_assistant\", \"content\": self.example1_assistant}\n        user_msg_2 = {\"role\": \"system\", \"name\": \"example_user\", \"content\": self.example2_user}\n        system_msg_3 = {\"role\": \"system\", \"name\": \"example_assistant\", \"content\": self.example2_assistant}\n        user_msg_final = {\"role\": \"user\", \"content\": self._construct_message(task_step_inputs)}\n        \n        messages = [\n            system_msg_1,\n            user_msg_1,\n            system_msg_2,\n            user_msg_2,\n            system_msg_3,\n            user_msg_final\n        ]\n\n        try:\n            logger.info(\"[UserAgent]: Calling OpenAI API\")\n            client = OpenAI(\n                api_key=self.api_key,  # This is the default and can be omitted\n            )\n\n            response = client.chat.completions.create(\n                messages=messages,\n                model=\"gpt-4o\",\n                temperature=0.9,\n            )\n            content = response.choices[0].message.content\n            action = self._extract_action(content)\n            logger.info(f\"[UserAgent]: Got action: {action}\")\n            return action\n        except Exception as e:\n            logger.error(f\"[UserAgent]: Error calling OpenAI API: {e}\")\n            raise"}
{"type": "source_file", "path": "src/benchflow/agents/webcanvas_langbase.py", "content": "import logging\nimport os\n\nimport json5\nimport requests\nfrom jinja2 import Template\n\nfrom benchflow import BaseAgent\n\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n)\n\nlogger = logging.getLogger(__name__)\n\nclass HistoryMemory:\n    def __init__(self, previous_trace: list = [], reflection: str = \"\") -> None:\n        self.previous_trace = previous_trace\n        self.reflection = reflection\n\n    def stringfy_thought_and_action(self) -> str:\n        input_list = None\n        str_output = \"\"\n        try:\n            input_list = json5.loads(self.previous_trace, encoding=\"utf-8\")\n        except:\n            input_list = self.previous_trace\n        if len(input_list) > 2:\n            str_output = \"[\"\n            for idx in range(len(input_list)-1):\n                str_output += f'Step{idx+1}:\\\"Thought: {input_list[idx][\"thought\"]}, Action: {input_list[idx][\"action\"]}, Reflection: {input_list[idx+1][\"reflection\"]}\\\";\\n'\n            str_output += \"]\"\n            current_trace = input_list[-1]\n            str_output += f'Specifically in the last step, you gave the following Thought: {current_trace[\"thought\"]}\\n You performed the following Action: {current_trace[\"action\"]}\\n You had the following Reflection: {self.reflection}\\\";\\n'\n        else:\n            current_trace = input_list[-1]\n            str_output += f'Specifically in the last step, you gave the following Thought: {current_trace[\"thought\"]}\\n You performed the following Action: {current_trace[\"action\"]}\\n You had the following Reflection: {self.reflection}\\\";\\n'\n        return str_output\n\n    def construct_previous_trace_prompt(self) -> str:\n        stringfy_thought_and_action_output = self.stringfy_thought_and_action()\n        previous_trace_prompt = f\"The previous thoughts, actions and reflections are as follows: \\\n            {stringfy_thought_and_action_output}.\\n\\nYou have done the things above.\\n\\n\"\n        return previous_trace_prompt\n\n    @staticmethod\n    def construct_cache(cache_info: list):\n        pass\n\nclass BasePrompts:\n\n    example_output = '\\n```\\n{\\n  \"action\": \"click\",\\n  \"action_input\": \"button\",\\n  \"element_id\": \"236\",\\n  \"description\": \"Now I\\'m on Google\\'s main page. I\\'m now clicking the button with element_id [236] to see more information.\"\\n}\\n```'\n    score_output = '\\n```\\n{\\n \"score\": \"10\"\\n,\"description\": \"According to the previous trajectory, the current thought and the action performed are an important part of completing the target task, so it is very important, so I give 10 points\"}\\n```'\n\n\n    # - goto: useful for when you need visit a new link or a website, it will open a new tab.\n    # - fill_form: useful for when you need to fill out a form or input something from accessibility tree. Input should be a string.\n    # - google_search: useful for when you need to use google to search something.\n    # - click: useful for when you need to click a button/link from accessibility tree.\n    # - select_option: useful for when you need to select a drop-down box value. When you get (select and option) tags from the accessibility tree, you need to select the serial number(element_id) corresponding to the select tag, not the option, and select the most likely content corresponding to the option as Input.\n    # - go_back: useful when you find the current web page encounter some network error or you think the last step is not helpful.\n\n    planning_prompt_system = '''You are an assistant who not only helps to browse and operate web pages to achieve certain goals, but also needs to explore the information on the page to answer the questions raised by the target task. Please answer the following questions as much as possible.\n        There are key information you will get:\n        **Key Information**:\n            - Previous trace: all thoughts, actions and reflections you have made historically.\n            - Accessibility tree: characteristic expression of the current web page.\n            \n        **Introduction to Accessibility Tree**:\n            The accessibility tree is a tree-like data structure that describes the relationships between elements on a web page and provides accessibility information for each element (such as text, links, form elements, etc.).\n            - **Accessibility Tree Example**:\n                Here is an example of an accessibility tree:\n                ```\n                current web tab name is 'Google'\n                    [40] link 'About'\n                    [41] link 'Store'\n                        [186] link 'Gmail'\n                        [187] link 'Images'\n                        [163] textarea 'Search'\n                        [236] button 'See more'\n                ```\n        In this example, each row represents the characteristic representation of a web page element. It has three attributes: '[40]' for the element's element_id, 'link' indicates the element is a link, and 'About' for the content of the element.\n        Note: The above element provided is purely for illustrative purposes and should NEVER be used directly in your output!         \n\n        You should always consider previous and subsequent steps and what to do.\n        **Thought Space**:\n            - What action do you think is needed now to complete the task?\n            - What's the reason of taking that action?\n        \n        You have access to the following tools(helpful to interact with web page):\n        **Execution Action Space**:\n            - goto: useful for when you need visit a new link or a website, it will open a new tab.\n            - fill_form: useful for when you need to fill out a form or input something from accessibility tree. Input should be a string.\n            - google_search: useful for when you need to use google to search something.\n            - click: useful for when you need to click a button/link from accessibility tree.\n            - select_option: useful for when you need to select a drop-down box value. When you get (select and option) tags from the accessibility tree, you need to select the serial number(element_id) corresponding to the select tag, not the option, and select the most likely content corresponding to the option as Input.\n            - go_back: useful when you find the current web page encounter some network error or you think the last step is not helpful.\n            - cache_data: useful when you need to extract information from the page that you think is extremely valuable for completing the target task. It is not a direct answer to the target task, but it is extremely relevant to the target task. Subsequent actions may refer to this part of the information and return this information as input\n            - get_final_answer: useful for when you think it is the answer to the target task and no other operations are required, Input should be a answer content.\n        \n        You also need to provide an effective description of the current execution action.\n        A proper description contains:\n            - What website it is; \n            - Which action you choose; \n            - REMEMBER DO NOT LEAVE THE DESCRIPTION EMPTY!\n\n        You have to follow the instructions or notes:\n        **Important Notes**:\n            - Under the following conditions, you are restricted to using the `google_search` or `goto` tools exclusively: \n                1. In the initial step of a process or when there's no preceding interaction history (i.e., the previous trace is empty). \n                2. In situations where the accessibility tree is absent or not provided.\n            - Your action should not be the same as last step's action.\n            - The `element_id` should be an integer accurately representing the element's ID in the accessibility tree.\n            - AVOID using the provided example's element_id as your output.\n            - The output JSON blob must be valid; otherwise, it cannot be recognized.\n        \n        **Special Circumstances Guidelines**:\n            - When performing a search on a website, if you find the search results do not display sufficient content, consider simplifying or modifying your search query. Reducing the complexity of your search query or altering keywords may yield more comprehensive results.\n        \n        Please ensure the accuracy of your output, as we will execute subsequent steps based on the `action`, `action_input` and `element_id` you provide.\n        \n        **Output Requirements**:\n        - Ensure your output strictly adheres to the JSON blob format outlined below:\n            \n            ```\n            {\n                \"thought\": ACTUAL_THOUGHT\n                \"action\": ACTUAL_TOOLS,\n                \"action_input\": ACTUAL_INPUT,\n                \"element_id\": ACTUAL_ELEMENT_ID,\n                \"description\": ACTUAL_DESCRIPTION\n            }\n            ```\n          \n        - A VALID JSON BLOB EXAMPLE AS FELLOWS:\n            ```\n            {\n                \"thought\": \"In order to complete this task,I need to go to the Google home page\",\n                \"action\": \"click\", \n                \"action_input\": \"button\",\n                \"element_id\": \"236\",\n                \"description\": \"Now I\\'m on Google\\'s main page. I\\'m now clicking the button with element_id [236] to see more information.\"\n            }\n            ```\n        '''\n\n    planning_prompt_user = \"The question here is described as \\\"{{user_request}}\\\".\\n\\n\"\n\n    global_reward_prompt_system = ('''\\\n        You are an assistant to help navigate and operate the web page to achieve certain task.\n        Your goal is to evaluate the previous series of traces(thoughts and actions) and think about what key steps are needed to complete the task in the future.\n        There are key information you will get:\n        **Key Information**:\n            - Previous trace: all thoughts, actions and reflections you have made historically.\n            - Accessibility tree: characteristic expression of the current web page.\n            - Screenshot: visual information of the current web page (may include).\n        \n        You also need to combine the previous trace to give the completion status of the current task.\n        **Status Of Task Completion**\n            - doing: You have completed the intermediate steps of the target task but not entirely finish the target task.\n            - finished: You are entirely certain about completing the target task.\n            - loop: You find that the the last two steps of previous actions are the same, it is determined that the process is stuck in a local optimum solution.\n        \n        You will judge and score the task completion and reasonableness of previous actions. The score ranges from 1-10, but the score you give can only be selected from [1, 3, 7, 9, 10].\n        **Judging and Scoring Criteria**:\n            - score = 1: You find that the status of the task is stuck in a loop by analyzing the previous trace.\n            - score = 3: You find that performing the previous trajectories(thoughts and actions) is not likely helpful in completing target task and you need to adjust the direction of your planning and action or start over from beginning.\n            - score = 7: You find that performing the previous trajectories(thoughts and actions) are helpful in completing the target task.\n            - score = 9: You find that performing the previous trajectories(thoughts and actions) are a very critical intermediate step to complete this task.\n            - score = 10: You find that performing the previous trajectories(thoughts and actions) have completed the task perfectly.\n        You need to provide an effective evidence of scoring for the series of the previous trace.\n            - Why do you give this score? \n            - What is the reason?\n\n        You also need to provide an effective description or summary of the above requirements through key information and characteristics of the current web page.\n        **A proper description contains**:\n            - What is the current completion status of the task? (IMPORTNAT)\n            - What is your overall plan for completing your goal and target task in the future? (IMPORTNAT)\n            - REMEMBER DO NOT LEAVE THE DESCRIPTION EMPTY!\n\n        **Output Requirements**:\n        - Ensure your output strictly follows this format:\n            ```json\n            {\n                \"status\": \"ACTUAL_STATUS\",\n                \"score\": \"ACTUAL_SCORE\",\n                \"reason\": \"ACTUAL_REASON\",\n                \"description\": \"ACTUAL_DESCRIPTION\"\n            }\n            ```\n        - A VALID JSON BLOB EXAMPLE AS FELLOWS:\n            ```\n            {\n                \"status\": \"doing\",\n                \"score\": \"3\",\n                \"reason\": \"You need to complete a search for camping tents that can accommodate 2 people and sort the results in rei by price from low to high. According to your previous trajectory, you navigated to the rei official website and clicked the 2-person button, which are correct actions. But when you complete the final step of sorting prices, you actually click on a link to a tent product. This is a completely unreasonable action. So I give it 3 points. Maybe you need to return to the previous interface to re-plan and select the 'sort by' button\"\n                \"description\": \"According to the current web page information, you can know that this is the homepage of a tent product, which is not very consistent with the purpose of the target task. The next overall plan to complete this task is to return to the previous page and select the sort by button.\"\n            }\n            ```\n    ''')\n\n    global_reward_with_GroundTruth_prompt_system = ('''\\\n        You are an assistant to help navigate and operate the web page to achieve certain task.\n        Your goal is to evaluate the previous series of traces(thoughts and actions) and think about what key steps are needed to complete the task in the future.\n        There are key information you will get:\n        **Key Information**:\n            - Previous trace: all thoughts, actions and reflections you have made historically.\n            - Current Webpage Information:\n                - Accessibility tree: characteristic expression of the current web page.\n                - Screenshot: visual information of the current web page. (may include)\n            - Reference Guide: detailed and step-by-step reference guide for completing the target task, serving as a benchmark for evaluating progress and strategizing the necessary actions.\n\n        **Notes to Reference Guide**:\n            - The Reference Guide plays a crucial role in aiding the evaluation of the current Status of Task Completion. The 'Completion Verification' section within the Reference Guide is instrumental in determining whether a task can be classified as 'finished.'\n            - Furthermore, for a task to be considered fully completed, all **key conditions** must be met as specified.\n\n\n        You also need to combine the previous trace to give the completion status of the current task.\n        **Status of Task Completion**\n            - doing: You have completed the intermediate steps of the target task but not entirely finish the target task.\n            - finished: You are entirely certain about completing the target task.\n            - loop: You find that the the last two steps of previous actions are the same, it is determined that the process is stuck in a local optimum solution.\n\n        You will judge and score the task completion and reasonableness of previous actions. The score ranges from 1-10, but the score you give can only be selected from [1, 3, 7, 9, 10].\n        **Judging and Scoring Criteria**:\n            - score = 1: You find that the status of the task is stuck in a loop by analyzing the previous trace.\n            - score = 3: You find that performing the previous trajectories(thoughts and actions) is not likely helpful in completing target task and you need to adjust the direction of your planning and action or start over from beginning.\n            - score = 7: You find that performing the previous trajectories(thoughts and actions) are helpful in completing the target task.\n            - score = 9: You find that performing the previous trajectories(thoughts and actions) are a very critical intermediate step to complete this task.\n            - score = 10: You find that performing the previous trajectories(thoughts and actions) have completed the task perfectly.\n        You need to provide an effective evidence of scoring for the series of the previous trace.\n            - Why do you give this score? \n            - What is the reason?\n\n        You also need to provide an effective description or summary of the above requirements through key information and characteristics of the current web page.\n        **A proper description contains**:\n            - What is the current completion status of the task? (IMPORTNAT)\n            - What is your overall plan for completing your goal and target task in the future? (IMPORTNAT)\n            - REMEMBER DO NOT LEAVE THE DESCRIPTION EMPTY!\n\n        **Output Requirements**:\n        - Ensure your output strictly follows this format:\n            ```json\n            {\n                \"status\": \"ACTUAL_STATUS\",\n                \"score\": \"ACTUAL_SCORE\",\n                \"reason\": \"ACTUAL_REASON\",\n                \"description\": \"ACTUAL_DESCRIPTION\"\n            }\n            ```\n        - A VALID JSON BLOB EXAMPLE AS FELLOWS:\n            ```\n            {\n                \"status\": \"doing\",\n                \"score\": \"3\",\n                \"reason\": \"You need to complete a search for camping tents that can accommodate 2 people and sort the results in rei by price from low to high. According to your previous trajectory, you navigated to the rei official website and clicked the 2-person button, which are correct actions. But when you complete the final step of sorting prices, you actually click on a link to a tent product. This is a completely unreasonable action. So I give it 3 points. Maybe you need to return to the previous interface to re-plan and select the 'sort by' button\"\n                \"description\": \"According to the current web page information, you can know that this is the homepage of a tent product, which is not very consistent with the purpose of the target task. The next overall plan to complete this task is to return to the previous page and select the sort by button.\"\n            }\n            ```\n    ''')\n\n    global_reward_prompt_user = \"The target task here is described as \\\"{{user_request}}\\\".\\n\\n\"\\\n        \"The previous trajectories(thoughts, actions and reflections) are: {{stringfy_thought_and_action_output}}.\\n\\nYou have done the things above.\\n\\n\"\n\n    current_reward_prompt_system = '''You are an assistant to help navigate and operate the web page to achieve certain task.\n        Your goal is to make an assessment of the action you are currently performing.\n        There are key information you will get:\n        **Key Information**:\n            - previous trace: all thoughts and actions to complete this task step by step\n            - current trace: current thought and action performed \n            - accessibility tree: characteristic expression of the current web page\n        \n        You will judge and score the currently performed action. The score ranges from 1-10, but the score you give can only be selected from [1, 3, 7, 9, 10]\n        **Judging and Scoring Criteria**:\n            - score = 1: You may not have obtained accessibility tree information(IMPORTANT).You may have encountered the issues such as Network connection issues,Human-computer verification issues,Encountered a blank page.\n            - score = 3: The action you performed (such as clicking on an element) does not help at all to complete the task when accessibility tree is provided\n            - score = 7: The action you performed (such as clicking on an element) is helpful in completing this task when accessibility tree is provided\n            - score = 9: This action you performed is a very critical intermediate step to complete this task when accessibility tree is provided\n            - score = 10: This action is the last step to complete the task when accessibility tree is provided\n        \n        You also need to provide an effective description of making the assessment\n        A proper description contains:\n            - Why do you give this score? \n            - What is the reason?\n            - What would be better advice if given a low score? \n            - REMEMBER DO NOT LEAVE THE DESCRIPTION EMPTY!\n\n        **Output Requirements**:\n        - Ensure your output strictly follows this format:\n            ```json\n            {\n                \"score\": \"ACTUAL_SCORE\",\n                \"description\": ACTUAL_DESCRIPTION\n            }\n            ```\n        - A VALID JSON BLOB EXAMPLE AS FELLOWS:\n            ```\n            {\n                \"score\": \"10\",\n                \"description\":\"According to the previous trajectory, the current thought and the action performed are an important part of completing the target task, so it is very important. I give 9 points.\"\n            }\n            ```\n    '''\n\n    current_reward_prompt_user = \"The target task here is described as \\\"{{user_request}}\\\".\\n\\n\"\\\n        \"The previous thought and action are:{{stringfy_previous_trace_output}}.\\n\\n\"\\\n        \"The current thought and action is: {{stringfy_current_trace_output}}.\\n\\nYou have done the current action\\n\\n\"\\\n\n    judge_searchbar_prompt_system = \"You are an assistant to help navigate and operate the web page to achieve certain goals. Answer the following questions as best you can.\\n\"\\\n        \"Your target is to judge whether the input is the search bar.\\n\"\n    judge_searchbar_prompt_user = \"Now the webpage's accessibility tree(the key information of current web page)is below: {{input_element}}\\n\"\\\n        \"Last step you have fill in the input(id={{element_id}}) with text:{{action_input}}\"\\\n        \"Judge whether the input is the search bar. If the blank is search bar, return yes, else return no. You should only return one word!\"\n\n    semantic_match_prompt_system = \"Now you are an assistant to judge whether 2 elements are semantically same. I'll provide a judge rule and an answer.\\n\"\\\n        \"If they are the same, you should return 1. If they are not related, you should return 0. \"\\\n        \"If they are related but not identical, return a decimal (two decimal places) between 0 and 1 of the degree of relevance you think.\\n\"\\\n        \"For example, the judge rule is: Decide whether the place is New York. The score of \\\"new york\\\" and \\\"纽约\\\" are both 1, \\\"Brooklyn\\\" should be 0.\\n\"\\\n        \"However, if the judge rule is: Decide whether the place is in New York. The score of \\\"new york\\\" and \\\"纽约\\\" and \\\"Brooklyn\\\" are all 1.\\n\"\\\n        \"Another example, the judge rule is: Decide whether I'm looking for clothes. The score of \\\"red Clothes\\\" and \\\"green jacket\\\"should also be 1.\\n\"\\\n        \"However, if the judge rule is: Decide whether I'm looking for red clothes. the score of \\\"bright red Clothing\\\" could be 0.85(red include bright red but they are not the same), the score of \\\"green Clothes\\\"should be 0.5(red is not green).\\n\"\\\n        \"Remember, you should return a number with ``` and an explanation. Like output: ```1```, (your explanation)\"  # \"Remember, you should only return a number without any punctuation or explanation!\"\n\n    semantic_match_prompt_user = \"You should judge by the rule below:{{semantic_method}}.\\n\\nmy answer is:{{input_answer}}\\n\"\n\nclass BasePromptConstructor:\n    def __init__(self):\n        pass\n\nclass PlanningPromptConstructor(BasePromptConstructor):\n    def __init__(self):\n        self.prompt_system = BasePrompts.planning_prompt_system\n        self.prompt_user = BasePrompts.planning_prompt_user\n\n    def construct(\n            self,\n            user_request: str,\n            previous_trace: list,\n            observation: str,\n            feedback: str = \"\",\n            status_description: str = \"\"\n    ) -> list:\n        self.prompt_user = Template(self.prompt_user).render(\n            user_request=user_request)\n        if len(previous_trace) > 0:\n            self.prompt_user += HistoryMemory(\n                previous_trace=previous_trace, reflection=status_description).construct_previous_trace_prompt()\n            if status_description != \"\":\n                self.prompt_user += \\\n                    f\"Task completion description is {status_description}\"\n            if feedback != \"\":\n                self.prompt_user += f\"Here are some other things you need to know:\\n {feedback}\\n\"\n            self.prompt_user += f\"\\nHere is the accessibility tree that you should refer to for this task:\\n{observation}\"\n        messages = [{\"role\": \"system\", \"content\": self.prompt_system}, {\n            \"role\": \"user\", \"content\": self.prompt_user}]\n        return messages\n\n    # Previous thought, action and reflection are converted to formatted strings\n    def stringfy_thought_and_action(self, input_list: list) -> str:\n        input_list = json5.loads(input_list, encoding=\"utf-8\")\n        str_output = \"[\"\n        for idx, i in enumerate(input_list):\n            str_output += f'Step{idx + 1}:\\\"Thought: {i[\"thought\"]}, Action: {i[\"action\"]}, Reflection:{i[\"reflection\"]}\\\";\\n'\n        str_output += \"]\"\n        return str_output\n\nclass WebcanvasAgent(BaseAgent):\n    def __init__(self):\n        super().__init__()\n        self.url = 'https://api.langbase.com/v1/pipes/run'\n        self.api_key = os.getenv('LANGBASE_API_KEY')\n\n    def _construct_message(self, env_info) -> str:\n        messages = PlanningPromptConstructor().construct(\n            env_info['task_name'], \n            env_info['previous_trace'], \n            env_info['observation'], \n            env_info['feedback'], \n            env_info['status_description'])\n        message = messages[1]['content']\n        return message\n \n    def _extract_action(self, response: str) -> str:\n        pass\n \n    def call_api(self, env_info) -> str:\n        message = self._construct_message(env_info)\n        data = {\n            'messages': [{'role': 'user', 'content': message}],\n            'stream': False\n        }\n        headers = {\n            \"Content-Type\": \"application/json\",\n            \"Authorization\": f\"Bearer {self.api_key}\"\n        }\n \n        try:\n            logger.info(\"[UserAgent]: Calling API\")\n            response = requests.post(self.url, headers=headers, json=data)\n            response.raise_for_status()\n \n            if not response.ok:\n                logger.error(f\"[UserAgent]: API error: {response.json()}\")\n                raise Exception(f\"API error: {response.json()}\")\n\n            result = response.json()\n            if result['success']:\n                raw_completion = result[\"completion\"]\n                return raw_completion\n \n            raise Exception(\"API call failed.\")\n        except Exception as e:\n            logger.error(f\"[UserAgent]: Error calling API: {str(e)}\")\n            raise"}
{"type": "source_file", "path": "src/benchflow/agents/webcanvas_openai.py", "content": "import logging\nimport os\n\nimport json5\nfrom jinja2 import Template\nfrom openai import OpenAI\n\nfrom benchflow import BaseAgent\n\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n)\n\nlogger = logging.getLogger(__name__)\n\nclass HistoryMemory:\n    def __init__(self, previous_trace: list = [], reflection: str = \"\") -> None:\n        self.previous_trace = previous_trace\n        self.reflection = reflection\n\n    def stringfy_thought_and_action(self) -> str:\n        input_list = None\n        str_output = \"\"\n        try:\n            input_list = json5.loads(self.previous_trace, encoding=\"utf-8\")\n        except:\n            input_list = self.previous_trace\n        if len(input_list) > 2:\n            str_output = \"[\"\n            for idx in range(len(input_list)-1):\n                str_output += f'Step{idx+1}:\\\"Thought: {input_list[idx][\"thought\"]}, Action: {input_list[idx][\"action\"]}, Reflection: {input_list[idx+1][\"reflection\"]}\\\";\\n'\n            str_output += \"]\"\n            current_trace = input_list[-1]\n            str_output += f'Specifically in the last step, you gave the following Thought: {current_trace[\"thought\"]}\\n You performed the following Action: {current_trace[\"action\"]}\\n You had the following Reflection: {self.reflection}\\\";\\n'\n        else:\n            current_trace = input_list[-1]\n            str_output += f'Specifically in the last step, you gave the following Thought: {current_trace[\"thought\"]}\\n You performed the following Action: {current_trace[\"action\"]}\\n You had the following Reflection: {self.reflection}\\\";\\n'\n        return str_output\n\n    def construct_previous_trace_prompt(self) -> str:\n        stringfy_thought_and_action_output = self.stringfy_thought_and_action()\n        previous_trace_prompt = f\"The previous thoughts, actions and reflections are as follows: \\\n            {stringfy_thought_and_action_output}.\\n\\nYou have done the things above.\\n\\n\"\n        return previous_trace_prompt\n\n    @staticmethod\n    def construct_cache(cache_info: list):\n        pass\n\nclass BasePrompts:\n\n    example_output = '\\n```\\n{\\n  \"action\": \"click\",\\n  \"action_input\": \"button\",\\n  \"element_id\": \"236\",\\n  \"description\": \"Now I\\'m on Google\\'s main page. I\\'m now clicking the button with element_id [236] to see more information.\"\\n}\\n```'\n    score_output = '\\n```\\n{\\n \"score\": \"10\"\\n,\"description\": \"According to the previous trajectory, the current thought and the action performed are an important part of completing the target task, so it is very important, so I give 10 points\"}\\n```'\n\n\n    # - goto: useful for when you need visit a new link or a website, it will open a new tab.\n    # - fill_form: useful for when you need to fill out a form or input something from accessibility tree. Input should be a string.\n    # - google_search: useful for when you need to use google to search something.\n    # - click: useful for when you need to click a button/link from accessibility tree.\n    # - select_option: useful for when you need to select a drop-down box value. When you get (select and option) tags from the accessibility tree, you need to select the serial number(element_id) corresponding to the select tag, not the option, and select the most likely content corresponding to the option as Input.\n    # - go_back: useful when you find the current web page encounter some network error or you think the last step is not helpful.\n\n    planning_prompt_system = '''You are an assistant who not only helps to browse and operate web pages to achieve certain goals, but also needs to explore the information on the page to answer the questions raised by the target task. Please answer the following questions as much as possible.\n        There are key information you will get:\n        **Key Information**:\n            - Previous trace: all thoughts, actions and reflections you have made historically.\n            - Accessibility tree: characteristic expression of the current web page.\n            \n        **Introduction to Accessibility Tree**:\n            The accessibility tree is a tree-like data structure that describes the relationships between elements on a web page and provides accessibility information for each element (such as text, links, form elements, etc.).\n            - **Accessibility Tree Example**:\n                Here is an example of an accessibility tree:\n                ```\n                current web tab name is 'Google'\n                    [40] link 'About'\n                    [41] link 'Store'\n                        [186] link 'Gmail'\n                        [187] link 'Images'\n                        [163] textarea 'Search'\n                        [236] button 'See more'\n                ```\n        In this example, each row represents the characteristic representation of a web page element. It has three attributes: '[40]' for the element's element_id, 'link' indicates the element is a link, and 'About' for the content of the element.\n        Note: The above element provided is purely for illustrative purposes and should NEVER be used directly in your output!         \n\n        You should always consider previous and subsequent steps and what to do.\n        **Thought Space**:\n            - What action do you think is needed now to complete the task?\n            - What's the reason of taking that action?\n        \n        You have access to the following tools(helpful to interact with web page):\n        **Execution Action Space**:\n            - goto: useful for when you need visit a new link or a website, it will open a new tab.\n            - fill_form: useful for when you need to fill out a form or input something from accessibility tree. Input should be a string.\n            - google_search: useful for when you need to use google to search something.\n            - click: useful for when you need to click a button/link from accessibility tree.\n            - select_option: useful for when you need to select a drop-down box value. When you get (select and option) tags from the accessibility tree, you need to select the serial number(element_id) corresponding to the select tag, not the option, and select the most likely content corresponding to the option as Input.\n            - go_back: useful when you find the current web page encounter some network error or you think the last step is not helpful.\n            - cache_data: useful when you need to extract information from the page that you think is extremely valuable for completing the target task. It is not a direct answer to the target task, but it is extremely relevant to the target task. Subsequent actions may refer to this part of the information and return this information as input\n            - get_final_answer: useful for when you think it is the answer to the target task and no other operations are required, Input should be a answer content.\n        \n        You also need to provide an effective description of the current execution action.\n        A proper description contains:\n            - What website it is; \n            - Which action you choose; \n            - REMEMBER DO NOT LEAVE THE DESCRIPTION EMPTY!\n\n        You have to follow the instructions or notes:\n        **Important Notes**:\n            - Under the following conditions, you are restricted to using the `google_search` or `goto` tools exclusively: \n                1. In the initial step of a process or when there's no preceding interaction history (i.e., the previous trace is empty). \n                2. In situations where the accessibility tree is absent or not provided.\n            - Your action should not be the same as last step's action.\n            - The `element_id` should be an integer accurately representing the element's ID in the accessibility tree.\n            - AVOID using the provided example's element_id as your output.\n            - The output JSON blob must be valid; otherwise, it cannot be recognized.\n        \n        **Special Circumstances Guidelines**:\n            - When performing a search on a website, if you find the search results do not display sufficient content, consider simplifying or modifying your search query. Reducing the complexity of your search query or altering keywords may yield more comprehensive results.\n        \n        Please ensure the accuracy of your output, as we will execute subsequent steps based on the `action`, `action_input` and `element_id` you provide.\n        \n        **Output Requirements**:\n        - Ensure your output strictly adheres to the JSON blob format outlined below:\n            \n            ```\n            {\n                \"thought\": ACTUAL_THOUGHT\n                \"action\": ACTUAL_TOOLS,\n                \"action_input\": ACTUAL_INPUT,\n                \"element_id\": ACTUAL_ELEMENT_ID,\n                \"description\": ACTUAL_DESCRIPTION\n            }\n            ```\n          \n        - A VALID JSON BLOB EXAMPLE AS FELLOWS:\n            ```\n            {\n                \"thought\": \"In order to complete this task,I need to go to the Google home page\",\n                \"action\": \"click\", \n                \"action_input\": \"button\",\n                \"element_id\": \"236\",\n                \"description\": \"Now I\\'m on Google\\'s main page. I\\'m now clicking the button with element_id [236] to see more information.\"\n            }\n            ```\n        '''\n\n    planning_prompt_user = \"The question here is described as \\\"{{user_request}}\\\".\\n\\n\"\n\n    global_reward_prompt_system = ('''\\\n        You are an assistant to help navigate and operate the web page to achieve certain task.\n        Your goal is to evaluate the previous series of traces(thoughts and actions) and think about what key steps are needed to complete the task in the future.\n        There are key information you will get:\n        **Key Information**:\n            - Previous trace: all thoughts, actions and reflections you have made historically.\n            - Accessibility tree: characteristic expression of the current web page.\n            - Screenshot: visual information of the current web page (may include).\n        \n        You also need to combine the previous trace to give the completion status of the current task.\n        **Status Of Task Completion**\n            - doing: You have completed the intermediate steps of the target task but not entirely finish the target task.\n            - finished: You are entirely certain about completing the target task.\n            - loop: You find that the the last two steps of previous actions are the same, it is determined that the process is stuck in a local optimum solution.\n        \n        You will judge and score the task completion and reasonableness of previous actions. The score ranges from 1-10, but the score you give can only be selected from [1, 3, 7, 9, 10].\n        **Judging and Scoring Criteria**:\n            - score = 1: You find that the status of the task is stuck in a loop by analyzing the previous trace.\n            - score = 3: You find that performing the previous trajectories(thoughts and actions) is not likely helpful in completing target task and you need to adjust the direction of your planning and action or start over from beginning.\n            - score = 7: You find that performing the previous trajectories(thoughts and actions) are helpful in completing the target task.\n            - score = 9: You find that performing the previous trajectories(thoughts and actions) are a very critical intermediate step to complete this task.\n            - score = 10: You find that performing the previous trajectories(thoughts and actions) have completed the task perfectly.\n        You need to provide an effective evidence of scoring for the series of the previous trace.\n            - Why do you give this score? \n            - What is the reason?\n\n        You also need to provide an effective description or summary of the above requirements through key information and characteristics of the current web page.\n        **A proper description contains**:\n            - What is the current completion status of the task? (IMPORTNAT)\n            - What is your overall plan for completing your goal and target task in the future? (IMPORTNAT)\n            - REMEMBER DO NOT LEAVE THE DESCRIPTION EMPTY!\n\n        **Output Requirements**:\n        - Ensure your output strictly follows this format:\n            ```json\n            {\n                \"status\": \"ACTUAL_STATUS\",\n                \"score\": \"ACTUAL_SCORE\",\n                \"reason\": \"ACTUAL_REASON\",\n                \"description\": \"ACTUAL_DESCRIPTION\"\n            }\n            ```\n        - A VALID JSON BLOB EXAMPLE AS FELLOWS:\n            ```\n            {\n                \"status\": \"doing\",\n                \"score\": \"3\",\n                \"reason\": \"You need to complete a search for camping tents that can accommodate 2 people and sort the results in rei by price from low to high. According to your previous trajectory, you navigated to the rei official website and clicked the 2-person button, which are correct actions. But when you complete the final step of sorting prices, you actually click on a link to a tent product. This is a completely unreasonable action. So I give it 3 points. Maybe you need to return to the previous interface to re-plan and select the 'sort by' button\"\n                \"description\": \"According to the current web page information, you can know that this is the homepage of a tent product, which is not very consistent with the purpose of the target task. The next overall plan to complete this task is to return to the previous page and select the sort by button.\"\n            }\n            ```\n    ''')\n\n    global_reward_with_GroundTruth_prompt_system = ('''\\\n        You are an assistant to help navigate and operate the web page to achieve certain task.\n        Your goal is to evaluate the previous series of traces(thoughts and actions) and think about what key steps are needed to complete the task in the future.\n        There are key information you will get:\n        **Key Information**:\n            - Previous trace: all thoughts, actions and reflections you have made historically.\n            - Current Webpage Information:\n                - Accessibility tree: characteristic expression of the current web page.\n                - Screenshot: visual information of the current web page. (may include)\n            - Reference Guide: detailed and step-by-step reference guide for completing the target task, serving as a benchmark for evaluating progress and strategizing the necessary actions.\n\n        **Notes to Reference Guide**:\n            - The Reference Guide plays a crucial role in aiding the evaluation of the current Status of Task Completion. The 'Completion Verification' section within the Reference Guide is instrumental in determining whether a task can be classified as 'finished.'\n            - Furthermore, for a task to be considered fully completed, all **key conditions** must be met as specified.\n\n\n        You also need to combine the previous trace to give the completion status of the current task.\n        **Status of Task Completion**\n            - doing: You have completed the intermediate steps of the target task but not entirely finish the target task.\n            - finished: You are entirely certain about completing the target task.\n            - loop: You find that the the last two steps of previous actions are the same, it is determined that the process is stuck in a local optimum solution.\n\n        You will judge and score the task completion and reasonableness of previous actions. The score ranges from 1-10, but the score you give can only be selected from [1, 3, 7, 9, 10].\n        **Judging and Scoring Criteria**:\n            - score = 1: You find that the status of the task is stuck in a loop by analyzing the previous trace.\n            - score = 3: You find that performing the previous trajectories(thoughts and actions) is not likely helpful in completing target task and you need to adjust the direction of your planning and action or start over from beginning.\n            - score = 7: You find that performing the previous trajectories(thoughts and actions) are helpful in completing the target task.\n            - score = 9: You find that performing the previous trajectories(thoughts and actions) are a very critical intermediate step to complete this task.\n            - score = 10: You find that performing the previous trajectories(thoughts and actions) have completed the task perfectly.\n        You need to provide an effective evidence of scoring for the series of the previous trace.\n            - Why do you give this score? \n            - What is the reason?\n\n        You also need to provide an effective description or summary of the above requirements through key information and characteristics of the current web page.\n        **A proper description contains**:\n            - What is the current completion status of the task? (IMPORTNAT)\n            - What is your overall plan for completing your goal and target task in the future? (IMPORTNAT)\n            - REMEMBER DO NOT LEAVE THE DESCRIPTION EMPTY!\n\n        **Output Requirements**:\n        - Ensure your output strictly follows this format:\n            ```json\n            {\n                \"status\": \"ACTUAL_STATUS\",\n                \"score\": \"ACTUAL_SCORE\",\n                \"reason\": \"ACTUAL_REASON\",\n                \"description\": \"ACTUAL_DESCRIPTION\"\n            }\n            ```\n        - A VALID JSON BLOB EXAMPLE AS FELLOWS:\n            ```\n            {\n                \"status\": \"doing\",\n                \"score\": \"3\",\n                \"reason\": \"You need to complete a search for camping tents that can accommodate 2 people and sort the results in rei by price from low to high. According to your previous trajectory, you navigated to the rei official website and clicked the 2-person button, which are correct actions. But when you complete the final step of sorting prices, you actually click on a link to a tent product. This is a completely unreasonable action. So I give it 3 points. Maybe you need to return to the previous interface to re-plan and select the 'sort by' button\"\n                \"description\": \"According to the current web page information, you can know that this is the homepage of a tent product, which is not very consistent with the purpose of the target task. The next overall plan to complete this task is to return to the previous page and select the sort by button.\"\n            }\n            ```\n    ''')\n\n    global_reward_prompt_user = \"The target task here is described as \\\"{{user_request}}\\\".\\n\\n\"\\\n        \"The previous trajectories(thoughts, actions and reflections) are: {{stringfy_thought_and_action_output}}.\\n\\nYou have done the things above.\\n\\n\"\n\n    current_reward_prompt_system = '''You are an assistant to help navigate and operate the web page to achieve certain task.\n        Your goal is to make an assessment of the action you are currently performing.\n        There are key information you will get:\n        **Key Information**:\n            - previous trace: all thoughts and actions to complete this task step by step\n            - current trace: current thought and action performed \n            - accessibility tree: characteristic expression of the current web page\n        \n        You will judge and score the currently performed action. The score ranges from 1-10, but the score you give can only be selected from [1, 3, 7, 9, 10]\n        **Judging and Scoring Criteria**:\n            - score = 1: You may not have obtained accessibility tree information(IMPORTANT).You may have encountered the issues such as Network connection issues,Human-computer verification issues,Encountered a blank page.\n            - score = 3: The action you performed (such as clicking on an element) does not help at all to complete the task when accessibility tree is provided\n            - score = 7: The action you performed (such as clicking on an element) is helpful in completing this task when accessibility tree is provided\n            - score = 9: This action you performed is a very critical intermediate step to complete this task when accessibility tree is provided\n            - score = 10: This action is the last step to complete the task when accessibility tree is provided\n        \n        You also need to provide an effective description of making the assessment\n        A proper description contains:\n            - Why do you give this score? \n            - What is the reason?\n            - What would be better advice if given a low score? \n            - REMEMBER DO NOT LEAVE THE DESCRIPTION EMPTY!\n\n        **Output Requirements**:\n        - Ensure your output strictly follows this format:\n            ```json\n            {\n                \"score\": \"ACTUAL_SCORE\",\n                \"description\": ACTUAL_DESCRIPTION\n            }\n            ```\n        - A VALID JSON BLOB EXAMPLE AS FELLOWS:\n            ```\n            {\n                \"score\": \"10\",\n                \"description\":\"According to the previous trajectory, the current thought and the action performed are an important part of completing the target task, so it is very important. I give 9 points.\"\n            }\n            ```\n    '''\n\n    current_reward_prompt_user = \"The target task here is described as \\\"{{user_request}}\\\".\\n\\n\"\\\n        \"The previous thought and action are:{{stringfy_previous_trace_output}}.\\n\\n\"\\\n        \"The current thought and action is: {{stringfy_current_trace_output}}.\\n\\nYou have done the current action\\n\\n\"\\\n\n    judge_searchbar_prompt_system = \"You are an assistant to help navigate and operate the web page to achieve certain goals. Answer the following questions as best you can.\\n\"\\\n        \"Your target is to judge whether the input is the search bar.\\n\"\n    judge_searchbar_prompt_user = \"Now the webpage's accessibility tree(the key information of current web page)is below: {{input_element}}\\n\"\\\n        \"Last step you have fill in the input(id={{element_id}}) with text:{{action_input}}\"\\\n        \"Judge whether the input is the search bar. If the blank is search bar, return yes, else return no. You should only return one word!\"\n\n    semantic_match_prompt_system = \"Now you are an assistant to judge whether 2 elements are semantically same. I'll provide a judge rule and an answer.\\n\"\\\n        \"If they are the same, you should return 1. If they are not related, you should return 0. \"\\\n        \"If they are related but not identical, return a decimal (two decimal places) between 0 and 1 of the degree of relevance you think.\\n\"\\\n        \"For example, the judge rule is: Decide whether the place is New York. The score of \\\"new york\\\" and \\\"纽约\\\" are both 1, \\\"Brooklyn\\\" should be 0.\\n\"\\\n        \"However, if the judge rule is: Decide whether the place is in New York. The score of \\\"new york\\\" and \\\"纽约\\\" and \\\"Brooklyn\\\" are all 1.\\n\"\\\n        \"Another example, the judge rule is: Decide whether I'm looking for clothes. The score of \\\"red Clothes\\\" and \\\"green jacket\\\"should also be 1.\\n\"\\\n        \"However, if the judge rule is: Decide whether I'm looking for red clothes. the score of \\\"bright red Clothing\\\" could be 0.85(red include bright red but they are not the same), the score of \\\"green Clothes\\\"should be 0.5(red is not green).\\n\"\\\n        \"Remember, you should return a number with ``` and an explanation. Like output: ```1```, (your explanation)\"  # \"Remember, you should only return a number without any punctuation or explanation!\"\n\n    semantic_match_prompt_user = \"You should judge by the rule below:{{semantic_method}}.\\n\\nmy answer is:{{input_answer}}\\n\"\n\nclass BasePromptConstructor:\n    def __init__(self):\n        pass\n\nclass PlanningPromptConstructor(BasePromptConstructor):\n    def __init__(self):\n        self.prompt_system = BasePrompts.planning_prompt_system\n        self.prompt_user = BasePrompts.planning_prompt_user\n\n    def construct(\n            self,\n            user_request: str,\n            previous_trace: list,\n            observation: str,\n            feedback: str = \"\",\n            status_description: str = \"\"\n    ) -> list:\n        self.prompt_user = Template(self.prompt_user).render(\n            user_request=user_request)\n        if len(previous_trace) > 0:\n            self.prompt_user += HistoryMemory(\n                previous_trace=previous_trace, reflection=status_description).construct_previous_trace_prompt()\n            if status_description != \"\":\n                self.prompt_user += \\\n                    f\"Task completion description is {status_description}\"\n            if feedback != \"\":\n                self.prompt_user += f\"Here are some other things you need to know:\\n {feedback}\\n\"\n            self.prompt_user += f\"\\nHere is the accessibility tree that you should refer to for this task:\\n{observation}\"\n        messages = [{\"role\": \"system\", \"content\": self.prompt_system}, {\n            \"role\": \"user\", \"content\": self.prompt_user}]\n        return messages\n\n    # Previous thought, action and reflection are converted to formatted strings\n    def stringfy_thought_and_action(self, input_list: list) -> str:\n        input_list = json5.loads(input_list, encoding=\"utf-8\")\n        str_output = \"[\"\n        for idx, i in enumerate(input_list):\n            str_output += f'Step{idx + 1}:\\\"Thought: {i[\"thought\"]}, Action: {i[\"action\"]}, Reflection:{i[\"reflection\"]}\\\";\\n'\n        str_output += \"]\"\n        return str_output\n\nclass WebcanvasAgent(BaseAgent):\n    def __init__(self):\n        super().__init__()\n        self.api_key = os.getenv(\"OPENAI_API_KEY\")\n\n    def _construct_message(self, env_info) -> str:\n        messages = PlanningPromptConstructor().construct(\n            env_info['task_name'], \n            env_info['previous_trace'], \n            env_info['observation'], \n            env_info['feedback'], \n            env_info['status_description'])\n        return messages\n \n    def _extract_action(self, response: str) -> str:\n        return response\n \n    def call_api(self, env_info) -> str:\n        messages = self._construct_message(env_info)\n        try:\n            logger.info(\"[UserAgent]: Calling OpenAI API\")\n            client = OpenAI(\n                api_key=self.api_key,  # This is the default and can be omitted\n            )\n\n            response = client.chat.completions.create(\n                messages=messages,\n                model=\"gpt-4o-mini\",\n                temperature=0.9,\n            )\n            content = response.choices[0].message.content\n            action = self._extract_action(content)\n            logger.info(f\"[UserAgent]: Got action: {action}\")\n            return action\n        except Exception as e:\n            logger.error(f\"[UserAgent]: Error calling OpenAI API: {e}\")\n            raise"}
{"type": "source_file", "path": "src/benchflow/Bench.py", "content": "import json\nimport logging\nimport sys\nimport threading\nimport time\nfrom pathlib import Path\nfrom typing import Any, Dict, List, Union\n\nimport requests\n\nfrom .BaseAgent import BaseAgent\n\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\"%(asctime)s [%(levelname)s] %(name)s: %(message)s\",\n    datefmt=\"%Y-%m-%d %H:%M:%S\"\n)\nlogger = logging.getLogger(__name__)\n\nclass Bench:\n    def __init__(self, benchmark_name: str, bf_token: str):\n        self.benchmark_name = benchmark_name\n        self.bff_url = \"https://benchflow.ai\"\n        self.bf_token = bf_token\n        project_dir = Path(__file__).parents[2]\n        self.results_dir = project_dir / \"results\" / self.benchmark_name\n        print_logo()\n\n    def run(self, task_ids: List[Union[str, int]], \n            agents: Union[BaseAgent, List[BaseAgent]], \n            requirements_txt: str, \n            install_sh: str = None, \n            api: Dict[str, str] = None, \n            require_gpu: bool = False, \n            args: Dict[str, Any] = {}):\n        \n        if isinstance(task_ids, (str, int)):\n            task_ids = [str(task_ids)]\n        else:\n            task_ids = [str(task) for task in task_ids]\n\n        if isinstance(agents, BaseAgent):\n            agents = [agents]\n        \n        results_ids = []\n        try:\n            for agent in agents:\n                result_id = self._send_tasks_to_bff(task_ids, agent, requirements_txt, install_sh, api, require_gpu, args)\n                if result_id:\n                    results_ids.append(result_id)\n\n            return results_ids\n\n        except Exception as e:\n            logger.error(f\"Error running benchmark: {str(e)}\")\n            return results_ids\n\n    def _send_tasks_to_bff(self, task_ids: List[str], agent: BaseAgent, \n                           requirements_txt: str, install_sh: str, \n                           api: Dict[str, str], require_gpu: bool, \n                           args: Dict[str, Any]):\n        logger.info(f\"Sending tasks {task_ids} and setup scripts to BFF for agent {agent.__class__.__name__}\")\n\n        try:\n            with open(requirements_txt, 'r') as f:\n                requirements_txt = f.read()\n            if install_sh:\n                with open(install_sh, 'r') as f:\n                    install_sh = f.read()\n            agent_code = self._get_agent_code(agent)\n        except Exception as e:\n            logger.error(f\"Failed to get agent code: {str(e)}\")\n            return None\n\n        api['provider'] = api.get(\"provider\", \"\")\n        api['model'] = api.get(\"model\", \"\")\n        payload = {\n            \"task_ids\": task_ids,\n            \"benchmark_name\": self.benchmark_name,\n            \"params\": args,\n            \"require_gpu\": require_gpu,\n            \"requirements\": requirements_txt if requirements_txt else \"\",\n            \"install_sh\": install_sh if install_sh else \"\",\n            \"agent_code\": agent_code if agent_code else \"\",\n            \"api\": api\n        }\n\n        headers = {\n            \"x-bf-api-key\": self.bf_token,\n            \"x-bf-source\": \"python-sdk 0.1.12\"\n        }\n\n        try:\n            response = requests.post(f\"{self.bff_url}/api/v1/jobs/{self.benchmark_name}/new\", json=payload, headers=headers)\n            response.raise_for_status()\n\n            task_info = response.json()\n            job_id = task_info.get(\"jobId\")\n            logger.info(f\"Tasks {task_ids} started successfully, job_id: {job_id}\")\n            return job_id\n        \n        except requests.exceptions.HTTPError as http_err:\n            detail = http_err.response.json()\n            logger.error(f\"Task execution failed: {str(http_err)}, detail: {detail}\")\n        except Exception as e:\n            logger.error(f\"Task execution failed: {str(e)}\")\n        return None\n\n    def get_results(self, job_ids: List[str]):\n        \"\"\"\n        Get and download the results from the BFF.\n        \"\"\"\n        results = {}\n        jobs = set(job_ids)\n        headers = {\"x-bf-api-key\": self.bf_token}\n        start_time = time.time()\n        stop_event = threading.Event()\n        spinner_thread = threading.Thread(target=spinner_animation, args=(stop_event, start_time))\n        spinner_thread.start()\n\n        try:\n            while jobs:\n                for job_id in list(jobs):\n                    response = requests.get(f\"{self.bff_url}/api/v1/jobs/{job_id}/\", headers=headers)\n                    response.raise_for_status()\n                    job = response.json().get('job')\n                    if job.get('status') != 'in_progress':\n                        if job.get('status') == 'done':\n                            spans = job.get('spans', {})\n                            outputs = [span.get('outputJSON') for span in spans if span.get('outputJSON')]\n                            results[job_id] = outputs\n                        jobs.remove(job_id)\n                if jobs:\n                    time.sleep(10)\n        finally:\n            stop_event.set()\n            spinner_thread.join()\n        \n        self.results_dir.mkdir(parents=True, exist_ok=True)\n        for job_id in job_ids:\n            result_file = self.results_dir / f\"{job_id}.json\"\n            result_file.write_text(json.dumps(results[job_id]))\n            logger.info(f\"Results saved to {result_file}\")\n        return results\n    \n    def _get_agent_code(self, agent: BaseAgent) -> str:\n        agent_file = Path(sys.modules[agent.__class__.__module__].__file__)\n        return agent_file.read_text()\n\ndef print_logo() -> None:\n    logo = r\"\"\"\n\n██████╗ ███████╗███╗   ██╗ ██████╗██╗  ██╗███████╗██╗      ██████╗ ██╗    ██╗    \n██╔══██╗██╔════╝████╗  ██║██╔════╝██║  ██║██╔════╝██║     ██╔═══██╗██║    ██║    \n██████╔╝█████╗  ██╔██╗ ██║██║     ███████║█████╗  ██║     ██║   ██║██║ █╗ ██║    \n██╔══██╗██╔══╝  ██║╚██╗██║██║     ██╔══██║██╔══╝  ██║     ██║   ██║██║███╗██║    \n██████╔╝███████╗██║ ╚████║╚██████╗██║  ██║██║     ███████╗╚██████╔╝╚███╔███╔╝    \n╚═════╝ ╚══════╝╚═╝  ╚═══╝ ╚═════╝╚═╝  ╚═╝╚═╝     ╚══════╝ ╚═════╝  ╚══╝╚══╝     \n                                                                                 \n    \"\"\"\n    print(logo)\n\ndef spinner_animation(stop_event: threading.Event, start_time: float) -> None:\n    spinner = ['|', '/', '-', '\\\\']\n    spinner_index = 0\n    bar_len = 19\n    while not stop_event.is_set():\n        elapsed = int(time.time() - start_time)\n        ch = spinner[spinner_index % len(spinner)]\n        spinner_index += 1\n        fill = elapsed % (bar_len + 1)\n        bar = '[' + '#' * fill + '-' * (bar_len - fill) + ']'\n        sys.stdout.write(f\"\\rWaiting for results... {ch} {bar} Elapsed: {elapsed}s\")\n        sys.stdout.flush()\n        time.sleep(0.1)\n    sys.stdout.write(\"\\r\" + \" \" * 80 + \"\\r\")\n    sys.stdout.flush()"}
{"type": "source_file", "path": "src/benchflow/load_benchmark.py", "content": "from .Bench import Bench\n\n\ndef load_benchmark(benchmark_name: str, bf_token: str) -> Bench:\n    \"\"\"\n    Load the benchmark. You need to get a bf_token on https://benchflow.ai.\n    For example:\n    ```\n    from benchflow import load_benchmark\n    bench = load_benchmark(\"benchflow/webarena\", \"your_bf_token\")\n    ```\n    \"\"\"\n    \n    return Bench(benchmark_name, bf_token)"}
{"type": "source_file", "path": "src/benchflow/__init__.py", "content": "from .BaseAgent import BaseAgent\nfrom .BaseBench import BaseBench\nfrom .Bench import Bench\nfrom .BenchClient import BenchClient\nfrom .load_benchmark import load_benchmark\n\n__version__ = \"0.1.12\"\n\n__all__ = [\"Bench\", \"BaseAgent\", \"BenchClient\", \"load_benchmark\", \"BaseBench\"]\n"}
{"type": "source_file", "path": "src/benchflow/benchmarks/webcanvas.py", "content": "# webcanvasbench.py\nimport json\nimport os\nfrom typing import Any, Dict\n\nfrom benchflow import BaseBench\nfrom benchflow.schemas import BenchArgs, BenchmarkResult\n\n\n#------------------------------------------------------------------------------\n# WebCanvasBench implementation\n#------------------------------------------------------------------------------\nclass WebCanvasBench(BaseBench):\n    def __init__(self):\n        super().__init__()\n\n    def get_args(self, task_id: str) -> BenchArgs:\n        \"\"\"\n        Return a WebCanvasConfig instance, validate the input arguments.\n        \"\"\"\n        # Benchmark need to deal with the END_IDX so that it can only run one task at a time\n        # task_id is the start index of the task\n        arguments = {\n            \"required\": [\"BROWSERBASE_API_KEY\", \"GRAPHQL_USERNAME\", \"GRAPHQL_PASSWORD\", \"OPENAI_API_KEY\"],\n            \"optional\": [\n                {\"TEST_START_IDX\": task_id},\n                {\"RESULTS_DIR\": \"/app/batch_tasks_results/example\"}\n            ]\n        }\n        return BenchArgs(arguments)\n\n    def get_image_name(self) -> str:\n        \"\"\"\n        Return the Docker image name for running the WebCanvas benchmark.\n        \"\"\"\n        return \"kirk2000/benchflow:webcanvas-v1\"\n\n    def get_results_dir_in_container(self) -> str:\n        \"\"\"\n        In the container, the result files will be written to /app/batch_tasks_results\n        (the environment variable RESULTS_DIR can further specify a subdirectory, such as \"example\").\n        \"\"\"\n        return \"/app/batch_tasks_results\"\n\n    def get_log_files_dir_in_container(self) -> str:\n        \"\"\"\n        In the container, the log files will be written to /app/LOGS\n        \"\"\"\n        return \"/app/LOGS\"\n\n    def get_result(self, task_id: str) -> BenchmarkResult:\n        \"\"\"\n        Read the result file (assuming the path is: {results_dir}/example/result/result.json),\n        and parse the result dictionary, which requires the is_resolved, score, and message fields.\n        \"\"\"\n        # Construct the full path to the result file (related to the RESULTS_DIR configuration inside the container)\n        result_file = os.path.join(self.results_dir, \"example\", \"result\", \"result.json\")\n        log_file = os.path.join(self.results_dir, \"example\", \"result\", \"out.json\")\n        if not os.path.exists(result_file):\n            return BenchmarkResult(task_id=task_id, is_resolved=False, metrics={\"score\": 0}, log={\"error\": \"No results found\"}, other={})\n        try:\n            with open(result_file, 'r') as f:\n                data = f.read().strip()\n                try:\n                    results = json.loads(data)\n                except Exception:\n                    # If the direct parsing fails, try simple text replacement and then parsing\n                    data_fixed = data.replace('{', '{\"').replace(': ', '\": ').replace(', ', ', \"')\n                    results = json.loads(data_fixed)\n            with open(log_file, 'r') as f:\n                log = f.read().strip()\n                print(log)\n        except Exception as e:\n            return BenchmarkResult(task_id=task_id, is_resolved=False, metrics={\"score\": 0}, log={\"error\": e}, other={})\n        \n        # Calculate whether the benchmark passed and the score based on the parsed results\n        is_resolved = results.get(\"task_success_rate\", 0) > 0.99\n        score = results.get(\"average_step_score_rate\", 0)\n        # Concatenate the result details in key-value pair format\n        message = {\"details\": ', '.join(f\"{k}: {v}\" for k, v in results.items())}\n        return BenchmarkResult(task_id=task_id, is_resolved=is_resolved, metrics={\"score\": score}, log=message, other={})\n\n    def get_all_tasks(self, split: str) -> Dict[str, Any]:\n        \"\"\"\n        Return all task_ids. If split is \"train\", return 20 task_ids, otherwise return 103 task_ids.\n        \"\"\"\n        if split == \"train\":\n            task_ids = [str(i) for i in range(20)]\n        else:\n            task_ids = [str(i) for i in range(103)]\n        return {\"task_ids\": task_ids, \"error_message\": None}\n\n    def cleanup(self):\n        \"\"\"\n        Add the logic to clean up the temporary result and log directories.\n        For example, delete the files in self.results_dir and self.log_files_dir.\n        \"\"\"\n        pass\n"}
{"type": "source_file", "path": "src/benchflow/benchmarks/webarena.py", "content": "# webarena_bench.py\nimport os\nimport subprocess\nfrom typing import Any, Dict\n\nfrom benchflow import BaseBench\nfrom benchflow.schemas import BenchArgs, BenchmarkResult\n\n\nclass WebArenaBench(BaseBench):\n    def __init__(self):\n        super().__init__()\n\n    def get_args(self, task_id: str) -> BenchArgs:\n        \"\"\"\n        Return a WebArenaConfig instance that validates the input arguments.\n        \"\"\"\n        arguments = {\n            \"required\": [],\n            \"optional\": [\n                {\"TEST_END_IDX\": str(int(task_id) + 1)}\n            ]\n        }\n        return BenchArgs(arguments)\n    \n    def get_image_name(self) -> str:\n        \"\"\"\n        Return the Docker image name for running the WebArena benchmark.\n        \"\"\"\n        return \"kirk2000/benchflow:webarena-v1\"\n    \n    def get_results_dir_in_container(self) -> str:\n        \"\"\"\n        Return the directory inside the container where the benchmark results will be stored.\n        \"\"\"\n        return \"/app/results\"\n    \n    def get_log_files_dir_in_container(self) -> str:\n        \"\"\"\n        Return the directory inside the container where the log files will be stored.\n        \"\"\"\n        return \"/app/log_files\"\n    \n    def get_result(self, task_id: str) -> BenchmarkResult:\n        \"\"\"\n        Read and parse the benchmark result from the log files.\n        \n        This method expects a file named 'log_files.txt' in the results directory.\n        It then reads the content of each log file listed in 'log_files.txt',\n        aggregates the log output, and extracts the average score and pass status.\n        \"\"\"\n        log_files_txt = os.path.join(self.results_dir, \"log_files.txt\")\n        if not os.path.exists(log_files_txt):\n            return BenchmarkResult(task_id=task_id, is_resolved=False, metrics={\"score\": 0},log={\"error\": \"No results found\"}, other={})\n        \n        log_content = \"\"\n        try:\n            with open(log_files_txt, 'r') as f:\n                for line in f:\n                    log_path = os.path.basename(line.strip())\n                    # Assume the log file path is relative to the parent directory of results_dir\n                    full_log_path = os.path.join(os.path.dirname(self.log_files_dir), str(task_id), log_path)\n                    with open(full_log_path, 'r') as log_file:\n                        log_content += log_file.read() + \"\\n\"\n        except Exception as e:\n            return BenchmarkResult(task_id=task_id, is_resolved=False, metrics={\"score\": 0}, log={\"error\": f\"Failed to read log files: {e}\"}, other={})\n        \n        # Parse the log content to extract score and status\n        is_resolved = False\n        score = 0.0\n        for line in log_content.splitlines():\n            if \"Average score:\" in line:\n                try:\n                    score = float(line.split(\":\")[-1].strip())\n                except ValueError:\n                    score = 0.0\n            if \"[Result]\" in line:\n                if \"(PASS)\" in line:\n                    is_resolved = True\n                    \n        return BenchmarkResult(task_id=task_id, is_resolved=is_resolved, metrics={\"score\": score}, log={\"details\": log_content}, other={})\n    \n    def get_all_tasks(self, split: str) -> Dict[str, Any]:\n        \"\"\"\n        Return a dictionary with all task IDs and an optional error message.\n        For 'train' split, return 200 tasks; otherwise, return 812 tasks.\n        \"\"\"\n        if split == \"train\":\n            task_ids = [str(i) for i in range(200)]\n        else:\n            task_ids = [str(i) for i in range(812)]\n        return {\"task_ids\": task_ids, \"error_message\": None}\n    \n    def cleanup(self):\n        \"\"\"\n        Clean up benchmark resources by removing the local results and log files directories.\n        \"\"\"\n        if os.path.exists(self.results_dir):\n            self.logger.info(f\"Removing {self.results_dir}\")\n            subprocess.run(['sudo', 'rm', '-rf', self.results_dir], check=True)\n        if os.path.exists(self.log_files_dir):\n            self.logger.info(f\"Removing {self.log_files_dir}\")\n            subprocess.run(['sudo', 'rm', '-rf', self.log_files_dir], check=True)\n"}
{"type": "source_file", "path": "src/benchflow/benchmarks/mmlupro.py", "content": "import json\nimport os\nfrom typing import Any, Dict\n\nfrom datasets import load_dataset\n\nfrom benchflow import BaseBench\nfrom benchflow.schemas import BenchArgs, BenchmarkResult\n\n\nclass MMLUPROBench(BaseBench):\n    def get_args(self, task_id: str) -> BenchArgs:\n        return BenchArgs(None)\n\n    def get_image_name(self) -> str:\n        return \"kirk2000/benchflow:mmlu-pro-v1\"\n\n    def get_results_dir_in_container(self) -> str:\n        return \"/app/eval_results\"\n\n    def get_log_files_dir_in_container(self) -> str:\n        return \"/app/logs\" # Useless\n\n    def get_result(self, task_id: str) -> BenchmarkResult:\n        summary_file = os.path.join(self.results_dir, f\"{task_id}_summary.json\")\n        result_file = os.path.join(self.results_dir, f\"{task_id}_result.json\")\n        try:\n            with open(summary_file, 'r') as f:\n                summary = json.load(f)\n            with open(result_file, 'r') as f:\n                result = json.load(f)\n\n            log = ''.join(json.dumps(item, ensure_ascii=False) for item in result)\n            return BenchmarkResult(\n                task_id=task_id,\n                is_resolved=True,\n                metrics={\"score\": summary['total']['acc']},\n                log=log,\n                other={\"details\": summary},\n            )\n        except Exception as e:\n            return BenchmarkResult(\n                task_id=task_id,\n                is_resolved=False,\n                metrics={\"score\": 0},\n                log={\"error\": str(e)},\n                other={\"error\": str(e)},\n            )\n        \n    def get_all_tasks(self, split: str) -> Dict[str, Any]:\n        dataset = load_dataset(\"TIGER-Lab/MMLU-Pro\", split=\"test\")\n        categories = dataset['category']\n        distinct_categories = sorted(set(categories))\n        return {\"task_ids\": distinct_categories, \"error_message\": \"\"}\n    \n    def cleanup(self):\n        pass\n"}
{"type": "source_file", "path": "src/benchflow/benchmarks/swebench.py", "content": "import json\nimport os\nfrom typing import Any, Dict\n\nfrom datasets import Dataset, load_dataset\n\nfrom benchflow import BaseBench\nfrom benchflow.schemas import BenchArgs, BenchmarkResult\n\n\nclass SwebenchBench(BaseBench):\n    def __init__(self):\n        super().__init__()\n\n    def get_args(self, task_id: str) -> BenchArgs:\n        arguments = {\n            \"required\": [],\n            \"optional\": [\n                {\"INSTANCE_IDS\": task_id},\n                {\"MAX_WORKERS\": 1},\n                {\"RUN_ID\": task_id}\n            ]\n        }\n        return BenchArgs(arguments)\n\n    def get_image_name(self) -> str:\n        return \"kirk2000/benchflow:swebench-v1\"\n\n    def get_results_dir_in_container(self) -> str:\n        return \"/app/results\"\n\n    def get_log_files_dir_in_container(self) -> str:\n        return \"/app/logs\"\n\n    def get_result(self, task_id: str) -> BenchmarkResult:\n        results_file = os.path.join(self.results_dir, f\"self_model.{task_id}.json\")\n        model_prediction_file = os.path.join(self.log_files_dir, f\"run_evaluation/{task_id}/self_model/{task_id}/patch.diff\")\n        report_file = os.path.join(self.log_files_dir, f\"run_evaluation/{task_id}/self_model/{task_id}/report.json\")\n        try:\n            with open(results_file, 'r') as f:\n                result_data = json.load(f)\n            total_instances = result_data.get(\"total_instances\", 1)\n            resolved_instances = result_data.get(\"resolved_instances\", 0)\n            pass_rate = resolved_instances / total_instances if total_instances else 0\n            with open(model_prediction_file, 'r') as f:\n                model_prediction = f.read()\n            with open(report_file, 'r') as f:\n                report = json.load(f)\n\n            return BenchmarkResult(\n                task_id=task_id,\n                is_resolved=pass_rate > 0.99,\n                metrics={\"pass_rate\": pass_rate},\n                log={\"prediction\": model_prediction, \"report\": report},\n                other={\"details\": result_data},\n            )\n        except Exception as e:\n            return BenchmarkResult(\n                task_id=task_id,\n                is_resolved=False,\n                metrics={\"pass_rate\": 0},\n                log={\"error\": str(e)},\n                other={\"error\": str(e)},\n            )\n        \n        \n    def get_all_tasks(self, split: str) -> Dict[str, Any]:\n        try:\n            dataset: Dataset = load_dataset(\"princeton-nlp/SWE-bench_Lite\", split=split)\n            dataset_ids = [instance[\"instance_id\"] for instance in dataset]\n            return {\"task_ids\": dataset_ids, \"error_message\": None}\n        except Exception as e:\n            return {\"task_ids\": [], \"error_message\": str(e)}\n    \n    def cleanup(self):\n        pass\n"}
{"type": "source_file", "path": "src/benchflow/schemas/BenchmarkResults.py", "content": "from typing import Any, Dict, Union\n\nfrom pydantic import BaseModel, ConfigDict\n\nMetricValue = Union[bool, int, float, str]\n\nclass BenchmarkResult(BaseModel):\n    task_id: str\n    is_resolved: bool\n    log: Dict[str, Any]\n    metrics: Dict[str, MetricValue]\n    other: Dict[str, Any]\n\n    model_config = ConfigDict(\n        json_schema_extra={\n            \"example\": {\n                \"is_resolved\": True,\n                \"log\": {\"trace\": \"trace message\"},\n                \"metrics\": {\n                    \"metric1\": True,\n                    \"metric2\": 123,\n                    \"metric3\": 3.1415,\n                    \"metric4\": \"OK\"\n                },\n                \"other\": {\n                    \"extra_info\": \"extra info\",\n                    \"error\": \"error message\"\n                }\n            }\n        }\n    )"}
{"type": "source_file", "path": "src/benchflow/benchmarks/crag.py", "content": "import json\nimport os\nfrom typing import Any, Dict\n\nfrom benchflow import BaseBench\nfrom benchflow.schemas import BenchArgs, BenchmarkResult\n\n\nclass CRAGBench(BaseBench):\n    def __init__(self):\n        super().__init__()\n    \n    def get_args(self, task_id: str) -> BenchArgs:\n        arguments = {\n            \"required\": [\n                \"OPENAI_API_KEY\",\n                \"EVALUATION_MODEL_NAME\"\n            ],\n            \"optional\": [\n                {\"BATCH_SIZE\": 100},\n            ]\n        }\n        return BenchArgs(arguments)\n    \n    def get_image_name(self) -> str:\n        return \"danielfang001/benchflow:crag-v1\"\n    \n    def get_results_dir_in_container(self) -> str:\n        return \"/workspace/results\"\n\n    def get_log_files_dir_in_container(self) -> str:\n        return \"/workspace/logs\"\n       \n    def get_result(self, task_id: str) -> Dict[str, Any]:\n        result_file = os.path.join(self.results_dir, f\"{task_id}_results.json\")\n\n        if not os.path.exists(result_file):\n            return BenchmarkResult(\n                task_id=task_id, \n                is_resolved=False, \n                metrics={\"score\": 0}, \n                log={\"error\": \"No results found\"}, \n                other={}\n            )\n        \n        try:\n            with open(result_file, \"r\") as f:\n                results = json.load(f)\n\n            is_resolved = False\n            if results.get(\"score\"):\n                is_resolved = True\n\n            return BenchmarkResult(\n                task_id=task_id, \n                is_resolved=is_resolved, \n                metrics={\"score\": results.get(\"score\", 0)}, \n                log=str(results), \n                other={}\n            )\n        \n        except Exception as e:\n            return BenchmarkResult(\n                task_id=task_id, \n                is_resolved=False, \n                metrics={\"score\": 0}, \n                log={\"error\": str(e)}, \n                other={}\n            )\n        \n    def get_all_tasks(self, split: str) -> Dict[str, Any]:\n        # Only one task for CRAG benchmark\n        return {\"task_ids\": [\"0\"], \"error_message\": None}\n        \n    def cleanup(self):\n        pass\n\n\n"}
{"type": "source_file", "path": "src/benchflow/schemas/InputData.py", "content": "from typing import Any, Dict\n\nfrom pydantic import BaseModel\n\n\nclass TaskStepInputs(BaseModel):\n   env_info: Dict[str, Any] = None\n"}
{"type": "source_file", "path": "src/benchflow/schemas/__init__.py", "content": "from .BenchArgs import BenchArgs\nfrom .BenchmarkResults import BenchmarkResult\n\n__all__ = [\"BenchmarkResult\", \"BenchArgs\"]"}
{"type": "source_file", "path": "src/benchflow/schemas/BenchArgs.py", "content": "from typing import Any, Dict, List, Union\n\nimport yaml\nfrom pydantic import BaseModel, Field, field_validator\n\n\nclass BenchArgs(BaseModel):\n    # required arguments        \n    required: List[str] = Field(default_factory=list)\n    # optional arguments and their default values\n    optional: Dict[str, Any] = Field(default_factory=dict)\n    # specify choices for arguments, key is the argument name, value is the allowed values list\n    choices: Dict[str, List[Any]] = Field(default_factory=dict)\n\n    @field_validator('optional', mode='before')\n    def merge_optional(cls, v):\n        \"\"\"\n        If 'optional' is a list, merge each single key dictionary into a dictionary.\n        \"\"\"\n        if isinstance(v, list):\n            merged = {}\n            for item in v:\n                if isinstance(item, dict):\n                    merged.update(item)\n                else:\n                    raise ValueError(\"Each item in 'optional' must be a dictionary\")\n            return merged\n        elif isinstance(v, dict):\n            return v\n        else:\n            raise ValueError(\"'optional' must be a dictionary or a list of dictionaries\")\n\n    def get_args(self, runtime_args: Dict[str, Any] = None) -> Dict[str, Any]:\n        \"\"\"\n        Merge runtime arguments with default values, validate required arguments, and validate values for arguments defined in choices.\n        \"\"\"\n        runtime_args = runtime_args or {}\n        args = {}\n\n        # check required arguments\n        for key in self.required:\n            if key in runtime_args and runtime_args[key] is not None:\n                args[key] = runtime_args[key]\n            else:\n                raise ValueError(f\"Missing required argument: {key}\")\n\n        # process optional arguments: use runtime value if it exists, otherwise use default value\n        for key, default in self.optional.items():\n            args[key] = runtime_args.get(key, default)\n\n        # validate arguments defined in choices\n        for arg, allowed in self.choices.items():\n            if arg in args and args[arg] not in allowed:\n                raise ValueError(f\"Invalid value for {arg}: {args[arg]}. Allowed values are: {allowed}\")\n        return args\n\n    def __init__(self, config_source: Union[str, Dict[str, Any], None]):\n        \"\"\"\n        The constructor can accept a YAML file path or a dictionary as the configuration source.\n        \"\"\"\n        if isinstance(config_source, str):\n            with open(config_source, \"r\", encoding=\"utf-8\") as f:\n                data = yaml.safe_load(f)\n        elif isinstance(config_source, dict):\n            data = config_source\n        elif config_source is None:\n            data = {\"required\": [], \"optional\": {}, \"choices\": {}}\n        else:\n            raise ValueError(\"config_source must be a YAML file path or a dictionary\")\n        super().__init__(**data)\n"}
