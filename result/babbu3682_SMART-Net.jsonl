{"repo_info": {"repo_name": "SMART-Net", "repo_owner": "babbu3682", "repo_url": "https://github.com/babbu3682/SMART-Net"}}
{"type": "source_file", "path": "arch/our_network.py", "content": "import torch \nimport torch.nn as nn\nimport torch.nn.functional as F\nimport timm\nfrom timm.layers.conv2d_same import Conv2dSame\nfrom timm.layers.norm import LayerNorm2d\n# from timm.models.layers.conv2d_same import Conv2dSame\n# from timm.models.layers.norm import LayerNorm2d\n\nfrom typing import Any, Iterator, Mapping\nfrom itertools import chain\n\nimport monai\nimport types\nfrom itertools import islice\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, pad_sequence\nfrom monai.inferers import SliceInferer\nfrom transformers import BertModel, BertConfig\nfrom monai.networks.nets.vit import ViT\nfrom einops import rearrange\nfrom einops.layers.torch import Rearrange\n\n# from arch.deeplab import ASPP, SeparableConv3d\n# from module.Non_Local_block import NONLocalBlock3D\n# from sliding_window import sliding_window_inference_cls_output, sliding_window_inference_seg_output\n\n\ndef initialize_head(module):\n    for m in module.modules():\n        if isinstance(m, (nn.Linear, nn.Conv2d)):\n            nn.init.xavier_uniform_(m.weight)\n            if m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n\ndef initialize_decoder(module):\n    for m in module.modules():\n        if isinstance(m, nn.Conv2d):\n            nn.init.kaiming_uniform_(m.weight, mode=\"fan_in\", nonlinearity=\"relu\")\n            if m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.BatchNorm2d):\n            nn.init.constant_(m.weight, 1)\n            nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.Linear):\n            nn.init.xavier_uniform_(m.weight)\n            if m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n\n\n# Modules\nclass Conv2dReLU(nn.Sequential):\n    def __init__(self, in_channels, out_channels, kernel_size, padding=0, stride=1, use_batchnorm=True):\n        conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, bias=not (use_batchnorm))\n        relu = nn.ReLU(inplace=True)\n        if use_batchnorm:\n            bn = nn.BatchNorm2d(out_channels)\n        else:\n            bn = nn.Identity()\n        super(Conv2dReLU, self).__init__(conv, bn, relu)\n\nclass SCSEModule(nn.Module):\n    def __init__(self, in_channels, reduction=16):\n        super().__init__()\n        self.cSE = nn.Sequential(\n            nn.AdaptiveAvgPool2d(1),\n            nn.Conv2d(in_channels, in_channels // reduction, 1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(in_channels // reduction, in_channels, 1),\n            nn.Sigmoid(),\n        )\n        self.sSE = nn.Sequential(nn.Conv2d(in_channels, 1, 1), nn.Sigmoid())\n\n    def forward(self, x):\n        return x * self.cSE(x) + x * self.sSE(x)\n\nclass Attention(nn.Module):\n    def __init__(self, name, **params):\n        super().__init__()\n        if name is None:\n            self.attention = nn.Identity(**params)\n        elif name == \"scse\":\n            self.attention = SCSEModule(**params)\n        else:\n            raise ValueError(\"Attention {} is not implemented\".format(name))\n\n    def forward(self, x):\n        return self.attention(x)\n\nclass Flatten(nn.Module):\n    def forward(self, x):\n        return x.view(x.shape[0], -1)\n\nclass SEG_DecoderBlock(nn.Module):\n    def __init__(self, in_channels, skip_channels, out_channels, use_batchnorm=True, attention_type=None, last=False):\n        super().__init__()\n        self.attention1 = nn.Identity() if last else Attention(attention_type, in_channels=in_channels+skip_channels)\n        self.conv1      = Conv2dReLU(in_channels+skip_channels, out_channels, kernel_size=3, padding=1, use_batchnorm=use_batchnorm)\n        self.conv2      = Conv2dReLU(out_channels, out_channels, kernel_size=3, padding=1, use_batchnorm=use_batchnorm)\n        self.attention2 = Attention(attention_type, in_channels=out_channels)\n\n    def forward(self, x, skip=None):\n        x = F.interpolate(x, scale_factor=2, mode=\"nearest\")\n        if skip is not None:\n            x = torch.cat([x, skip], dim=1)\n            x = self.attention1(x)\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = self.attention2(x)\n        return x\n\nclass UpsampleBlock(nn.Module):\n    def __init__(self, scale, input_channels, output_channels, ksize=1):\n        super(UpsampleBlock, self).__init__()\n        self.upsample = nn.Sequential(\n            nn.Conv2d(input_channels, output_channels*(scale**2), kernel_size=1, stride=1, padding=ksize//2),\n            nn.PixelShuffle(upscale_factor=scale)\n        )\n\n    def forward(self, input):\n        return self.upsample(input)\n\nclass REC_Skip_DecoderBlock(nn.Module):\n    def __init__(self, in_channels, skip_channels, out_channels, use_batchnorm=True, attention_type=None, last=False):\n        super().__init__()\n        self.upsample   = UpsampleBlock(scale=2, input_channels=in_channels, output_channels=in_channels)\n        self.attention1 = nn.Identity() if last else Attention(attention_type, in_channels=in_channels+skip_channels)\n        self.conv1      = Conv2dReLU(in_channels+skip_channels, out_channels, kernel_size=3, padding=1, use_batchnorm=use_batchnorm)\n        self.conv2      = Conv2dReLU(out_channels, out_channels, kernel_size=3, padding=1, use_batchnorm=use_batchnorm)\n        self.attention2 = Attention(attention_type, in_channels=out_channels)\n\n    def forward(self, x, skip=None):\n        x = self.upsample(x)\n        if skip is not None:\n            x = torch.cat([x, skip], dim=1)\n            x = self.attention1(x)\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = self.attention2(x)\n        return x\n\nclass REC_DecoderBlock(nn.Module):\n    def __init__(self, in_channels, skip_channels, out_channels, use_batchnorm=True, attention_type=None, last=False):\n        super().__init__()\n        self.upsample   = UpsampleBlock(scale=2, input_channels=in_channels, output_channels=in_channels)\n        self.attention1 = nn.Identity() if last else Attention(attention_type, in_channels=in_channels)\n        self.conv1      = Conv2dReLU(in_channels, out_channels, kernel_size=3, padding=1, use_batchnorm=use_batchnorm)\n        self.conv2      = Conv2dReLU(out_channels, out_channels, kernel_size=3, padding=1, use_batchnorm=use_batchnorm)\n        self.attention2 = Attention(attention_type, in_channels=out_channels)\n\n    def forward(self, x, skip=None):\n        x = self.upsample(x)\n        x = self.attention1(x)\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = self.attention2(x)\n        return x\n\nclass Window_Conv2D(nn.Module):    \n    '''\n    Ref: https://arxiv.org/pdf/1812.00572.pdf\n    HU summary  \n          [HU threshold]                           [weight / bias]\n    softtissue = W:350 L:40 (-135 ~ 215)         W:11.70275 B:-2.53996\n    liver      = W:150 L:30 (-45  ~ 105)         W:27.30748 B:-6.52676\n\n    softtissue =  Adapt                          W:4.00000 B:-2.47143\n    liver      =  Adapt                          W:9.33333 B:-6.36667\n    '''      \n\n    def __init__(self, mode, in_channels=1, w_channels=2):\n        super(Window_Conv2D, self).__init__()\n        self.w_channels = w_channels\n        self.conv_layer = nn.Conv2d(in_channels=in_channels, out_channels=w_channels, kernel_size=1, stride=1, padding=0, bias=True)\n        \n        if mode == \"relu\":\n            self.act_layer = self.upbound_relu\n        elif mode == \"sigmoid\":\n            self.act_layer = self.upbound_sigmoid\n        else:\n            raise Exception()\n        \n        # Initialize by xavier_uniform_\n        self.init_weight()\n        \n    def upbound_relu(self, x):\n        return torch.minimum(torch.maximum(x, torch.tensor(0)), torch.tensor(1.0))\n\n    def upbound_sigmoid(self, x):\n        return 1.0 * torch.sigmoid(x)\n                    \n    def init_weight(self):\n        print(\"inintializing...!\")\n        # Range [-1 ~ 1]\n        # softtissue\n        self.conv_layer.weight.data[0, :, :, :] = 5.850000000000001\n        self.conv_layer.bias.data[0]            = 3.310000000000001\n        # liver\n        self.conv_layer.weight.data[1, :, :, :] = 13.650000000000007\n        self.conv_layer.bias.data[1]            = 7.123333333333336\n    \n    def cal_w_b(self, min, max):\n        min = (((min + 1024)/4095) - 0.5) / 0.5\n        max = (((max + 1024)/4095) - 0.5) / 0.5\n        \n        width = max - min\n        level = (max + min) / 2.0\n\n        weight = 1/width\n        bias = -(1/width) * (level - width/2)\n        \n        return weight, bias\n\n    def forward(self, x):\n        windowed_x = self.conv_layer(x)\n        windowed_x = self.act_layer(windowed_x)\n        return torch.cat([x, windowed_x], dim=1)\n    \n    def inference(self, x):\n        self.eval()\n        with torch.no_grad():\n            windowed_x = self.conv_layer(x)\n            windowed_x = self.act_layer(windowed_x)\n        return torch.cat([x, windowed_x], dim=1)   \n\nclass GeneralizedMeanPooling(nn.Module):\n    # Ref: Fine-tuning CNN Image Retrieval with No Human Annotation (https://arxiv.org/pdf/1711.02512)\n    def __init__(self, p=3, eps=1e-6):\n        super(GeneralizedMeanPooling, self).__init__()\n        self.p   = nn.Parameter(torch.ones(1)*p)\n        self.eps = eps\n\n    def gem(self, x, p=3, eps=1e-6):\n        return F.adaptive_avg_pool2d(input=x.clamp(min=eps).pow(p), output_size=1).pow(1./p)\n\n    def forward(self, x):\n        return self.gem(x, p=self.p, eps=self.eps).flatten(start_dim=1, end_dim=-1)  # (B, C)\n\nclass MaxAvgPooling(nn.Module):\n    # We have to adjust the segmentation pred depending on classification pred\n    # ResNet50 uses four 2x2 maxpools and 1 global avgpool to extract classification pred. that is the same as 16x16 maxpool and 16x16 avgpool\n    def __init__(self):\n        super(MaxAvgPooling, self).__init__()\n        self.maxpool = torch.nn.MaxPool2d(kernel_size=16, stride=16, padding=0)\n        self.avgpool = torch.nn.AvgPool2d(kernel_size=16, stride=16, padding=0)\n\n    def forward(self, x):\n        return self.avgpool(self.maxpool(x)).flatten(start_dim=1, end_dim=-1)  # (B, C)\n\nclass ViT_Decoder(ViT):\n    def __init__(self, *args, **kwargs):\n        super(ViT_Decoder, self).__init__(*args, **kwargs)\n\n    def forward(self, x):\n        x = self.patch_embedding(x)\n        if hasattr(self, \"cls_token\"):\n            cls_token = self.cls_token.expand(x.shape[0], -1, -1)\n            x = torch.cat((cls_token, x), dim=1)\n        for blk in self.blocks:\n            x = blk(x)\n        x = self.norm(x)\n        return x\n\n\n\n\n\n\n# 2D: encoder with MTL\nclass SMART_Net_2D(nn.Module):\n    def __init__(self, backbone='resnet50', use_skip=True, pool_type='gem', use_consist=True, roi_size=256):\n        super(SMART_Net_2D, self).__init__()\n        \n        self.backbone    = backbone\n        self.use_skip    = use_skip\n        self.pool_type   = pool_type\n        self.use_consist = use_consist\n        \n        if backbone == 'resnet-50':\n            self.encoder = monai.networks.nets.ResNetFeatures(model_name='resnet50', pretrained=False, spatial_dims=2, in_channels=1)\n            self.output_channels = [2048, 1024, 512, 256, 64]\n        elif backbone == 'efficientnet-b7':\n            self.encoder = monai.networks.nets.EfficientNetBNFeatures(model_name=\"efficientnet-b7\", pretrained=True, spatial_dims=2, in_channels=1)\n            self.output_channels = [640, 224, 80, 48, 32]\n        elif backbone == 'maxvit-xlarge':\n            self.encoder = timm.create_model('maxvit_xlarge_tf_512.in21k_ft_in1k', pretrained=True, features_only=True, img_size=roi_size)\n            self.encoder.stem.conv1 = Conv2dSame(1, 192, kernel_size=(3, 3), stride=(2, 2))\n            self.output_channels = [1536, 768, 384, 192, 192]\n        elif backbone == 'maxvit-small':\n            self.encoder = timm.create_model('maxvit_small_tf_512.in1k', pretrained=True, features_only=True, img_size=roi_size)\n            self.encoder.stem.conv1 = Conv2dSame(1, 64, kernel_size=(3, 3), stride=(2, 2))\n            self.output_channels = [768, 384, 192, 96, 64]            \n\n        # CLS Decoder\n        if backbone == 'resnet50' or backbone == 'efficientnet-b7':\n            # Pooling -> Flatten -> FC -> ReLU -> Drop -> FC\n            self.cls_decoder_block1 = nn.AdaptiveAvgPool2d(output_size=1)\n            self.cls_decoder_block2 = Flatten()\n            self.cls_decoder_block3 = nn.Linear(in_features=self.output_channels[0], out_features=self.output_channels[0]//2, bias=True)\n            self.cls_decoder_block4 = nn.ReLU()\n            self.cls_decoder_block5 = nn.Dropout(p=0.3, inplace=False)\n        \n        else:\n            # Pooling -> Norm -> Flatten -> FC -> Tanh -> Drop -> FC\n            self.cls_decoder_block1 = nn.Sequential(nn.AdaptiveAvgPool2d(output_size=1), LayerNorm2d(num_channels=self.output_channels[0], eps=1e-05, affine=True))\n            self.cls_decoder_block2 = Flatten()\n            self.cls_decoder_block3 = nn.Linear(in_features=self.output_channels[0], out_features=self.output_channels[0]//2, bias=True)\n            self.cls_decoder_block4 = nn.Tanh()\n            self.cls_decoder_block5 = nn.Dropout(p=0.3, inplace=False)            \n\n        # SEG Decoder\n        self.seg_decoder_block1 = SEG_DecoderBlock(in_channels=self.output_channels[0], skip_channels=self.output_channels[1], out_channels=256, use_batchnorm=True, attention_type='scse')\n        self.seg_decoder_block2 = SEG_DecoderBlock(in_channels=256,                     skip_channels=self.output_channels[2], out_channels=128, use_batchnorm=True, attention_type='scse')\n        self.seg_decoder_block3 = SEG_DecoderBlock(in_channels=128,                     skip_channels=self.output_channels[3], out_channels=64,  use_batchnorm=True, attention_type='scse')\n        self.seg_decoder_block4 = SEG_DecoderBlock(in_channels=64,                      skip_channels=self.output_channels[4], out_channels=32,  use_batchnorm=True, attention_type='scse')\n        self.seg_decoder_block5 = SEG_DecoderBlock(in_channels=32,                      skip_channels=0,                       out_channels=16,  use_batchnorm=True, attention_type='scse', last=True)\n\n        # REC Decoder\n        Selected_REC_DecoderBlock = REC_Skip_DecoderBlock if use_skip else REC_DecoderBlock\n        self.rec_decoder_block1   = Selected_REC_DecoderBlock(in_channels=self.output_channels[0], skip_channels=self.output_channels[1], out_channels=256, use_batchnorm=True, attention_type='scse')\n        self.rec_decoder_block2   = Selected_REC_DecoderBlock(in_channels=256,                     skip_channels=self.output_channels[2], out_channels=128, use_batchnorm=True, attention_type='scse')\n        self.rec_decoder_block3   = Selected_REC_DecoderBlock(in_channels=128,                     skip_channels=self.output_channels[3], out_channels=64,  use_batchnorm=True, attention_type='scse')\n        self.rec_decoder_block4   = Selected_REC_DecoderBlock(in_channels=64,                      skip_channels=self.output_channels[4], out_channels=32,  use_batchnorm=True, attention_type='scse')\n        self.rec_decoder_block5   = Selected_REC_DecoderBlock(in_channels=32,                      skip_channels=0,                       out_channels=16,  use_batchnorm=True, attention_type='scse', last=True)\n\n        # Head\n        self.cls_head = nn.Linear(in_features=self.output_channels[0]//2, out_features=1, bias=True)\n        self.seg_head = nn.Conv2d(in_channels=16,   out_channels=1, kernel_size=3, padding=1)\n        self.rec_head = nn.Conv2d(in_channels=16,   out_channels=1, kernel_size=3, padding=1)\n\n        # For consistency loss\n        if pool_type == 'gem':\n            self.pool_for_consist = GeneralizedMeanPooling(p=3, eps=1e-6) # GeM pooling\n        else:\n            self.pool_for_consist = MaxAvgPooling()\n            \n        # Init\n        self.initialize()\n\n    def initialize(self):\n        initialize_decoder(self.cls_decoder_block1)\n        initialize_decoder(self.cls_decoder_block2)\n        initialize_decoder(self.cls_decoder_block3)\n        initialize_decoder(self.cls_decoder_block4)\n        initialize_decoder(self.cls_decoder_block5)               \n        initialize_decoder(self.seg_decoder_block1)\n        initialize_decoder(self.seg_decoder_block2)\n        initialize_decoder(self.seg_decoder_block3)\n        initialize_decoder(self.seg_decoder_block4)\n        initialize_decoder(self.seg_decoder_block5) \n        initialize_decoder(self.rec_decoder_block1)\n        initialize_decoder(self.rec_decoder_block2)\n        initialize_decoder(self.rec_decoder_block3)\n        initialize_decoder(self.rec_decoder_block4)\n        initialize_decoder(self.rec_decoder_block5)     \n        initialize_head(self.cls_head)\n        initialize_head(self.seg_head)\n        initialize_head(self.rec_head)\n \n    def forward(self, x):\n        # encoder\n        skip4, skip3, skip2, skip1, x = self.encoder(x)\n\n        # cls decoder\n        cls = self.cls_decoder_block1(x)\n        cls = self.cls_decoder_block2(cls)\n        cls = self.cls_decoder_block3(cls)\n        cls = self.cls_decoder_block4(cls)\n        cls = self.cls_decoder_block5(cls)\n\n        # seg decoder\n        seg = self.seg_decoder_block1(x,   skip1)\n        seg = self.seg_decoder_block2(seg, skip2)\n        seg = self.seg_decoder_block3(seg, skip3)\n        seg = self.seg_decoder_block4(seg, skip4)        \n        seg = self.seg_decoder_block5(seg)\n\n        # rec decoder\n        rec = self.rec_decoder_block1(x,   skip1)\n        rec = self.rec_decoder_block2(rec, skip2)\n        rec = self.rec_decoder_block3(rec, skip3)\n        rec = self.rec_decoder_block4(rec, skip4)\n        rec = self.rec_decoder_block5(rec)\n\n        # head\n        cls = self.cls_head(cls)\n        seg = self.seg_head(seg)\n        rec = self.rec_head(rec)\n        \n        if self.use_consist:\n            return cls, seg, rec, self.pool_for_consist(seg)\n        else:\n            return cls, seg, rec\n    \n\n\n\n\n\n# 3D-CLS: 3D operator w/ 2D encoder\nclass SMART_Net_3D_CLS(nn.Module):\n    def __init__(self, transfer_pretrained=None, use_pretrained_encoder=True, freeze_encoder=True, roi_size=512, sw_batch_size=64, spatial_dim=0, backbone='maxvit-xlarge', use_skip=True, pool_type='gem', operator_3d='lstm'):\n        super(SMART_Net_3D_CLS, self).__init__()\n        self.slice_inferer = SliceInferer(roi_size=(roi_size, roi_size), sw_batch_size=sw_batch_size, spatial_dim=spatial_dim, device=torch.device(\"cuda\"))\n        self.operator_3d = operator_3d\n        self.freeze_encoder = freeze_encoder\n\n        # 2D Encoder\n        self.model_2d = SMART_Net_2D(backbone=backbone, use_skip=use_skip, pool_type=pool_type)\n        self.feat_dim = self.model_2d.output_channels[0]\n        self.encoder  = self.model_2d.encoder\n\n        # 3D operator\n        if operator_3d == 'lstm':\n            # FC -> ReLU -> Drop -> FC\n            self.LSTM        = nn.LSTM(input_size=self.feat_dim, hidden_size=self.feat_dim, num_layers=3, batch_first=True, bidirectional=True)\n            self.linear_lstm = nn.Linear(self.feat_dim*2, self.feat_dim, True)\n            self.relu_lstm   = nn.ReLU()\n        elif operator_3d == 'bert':\n            # FC -> Tanh -> Drop -> FC\n            self.config = BertConfig.from_pretrained(\"bert-base-uncased\")\n            self.config.hidden_size=self.feat_dim             # hidden_size를 self.feat_dim으로 설정\n            self.config.num_hidden_layers=12                  # BERT base에서 사용하는 레이어 수\n            self.config.num_attention_heads=8                 # 640 hidden size에 맞게 8 헤드 사용\n            self.config.intermediate_size=self.feat_dim*4     # FFN 레이어의 크기 (보통 hidden_size * 4)\n            self.BERT = BertModel(config=self.config)\n            self.cls_token = nn.Parameter(torch.zeros(1, 1, self.feat_dim)) # BERT 안에서 pos_embed 더해줌.\n\n        # Head\n        self.head = nn.Sequential(nn.Dropout(0.3), nn.Linear(self.feat_dim, 1, True))\n\n        # Init\n        if (transfer_pretrained is not None) and (use_pretrained_encoder):\n            self.load_pretrained_encoder(transfer_pretrained)\n\n    def load_pretrained_encoder(self, transfer_pretrained):\n        print(\"Load State Dict...!\")\n        check_layer = list(self.encoder.state_dict().keys())[0]\n        print(\"Original weight = \", self.encoder.state_dict()[check_layer][0].mean())\n        checkpoint = torch.load(transfer_pretrained, map_location=torch.device('cpu'))['model_state_dict'] # 초반 EfficientNet + Focal + Tversky\n        checkpoint = {k.replace('module.', ''): v for k, v in checkpoint.items()}\n        checkpoint = {k.replace('encoder.', ''): v for k, v in checkpoint.items() if 'encoder.' in k}\n        self.encoder.load_state_dict(checkpoint)\n        print(\"Updated weight = \", self.encoder.state_dict()[check_layer][0].mean())\n\n    def feat_cls_extract(self, x):\n        if self.freeze_encoder:\n            self.encoder.eval()\n            with torch.no_grad():\n                last_feat = self.encoder.forward(x)[-1]\n                last_feat = F.adaptive_avg_pool2d(last_feat, output_size=1)\n        else:\n            last_feat = self.encoder.forward(x)[-1]\n            last_feat = F.adaptive_avg_pool2d(last_feat, output_size=1)\n        return last_feat # (B, C, 1, 1)\n\n    def forward(self, x, x_lens):\n        # CNN feature extraction\n        sequenced_feat = self.slice_inferer(x, self.feat_cls_extract) # output: [B, C, Depth, 1, 1]\n        sequenced_feat = sequenced_feat.flatten(start_dim=2) # output: [B, C, Depth]\n        sequenced_feat = sequenced_feat.permute(0, 2, 1)     # output: [B, Depth, C]\n\n        # Squential representation\n        if self.operator_3d == 'lstm':\n            self.LSTM.flatten_parameters()  # For Multi GPU  \n            x_packed = pack_padded_sequence(sequenced_feat, x_lens.cpu(), batch_first=True, enforce_sorted=False)  # x_len이 cpu int64로 들어가야함!!!\n            RNN_out, (h_n, h_c) = self.LSTM(x_packed, None)    # input shape must be [batch, seq, feat_dim]\n            fc_output = torch.cat([h_n[-2, :, :], h_n[-1, :, :]], dim=1) # Due to the Bi-directional\n            fc_output = self.linear_lstm(fc_output)\n            fc_output = self.relu_lstm(fc_output)\n        elif self.operator_3d == 'bert':\n            cls_tokens     = self.cls_token.expand(sequenced_feat.shape[0], -1, -1)\n            sequenced_feat = torch.cat((cls_tokens, sequenced_feat), dim=1)  # cls token 추가\n            attention_mask = torch.ones(sequenced_feat.shape[:2], dtype=torch.long).to(sequenced_feat.device) # x_lens을 이용하여 attention_mask 생성\n            for i, x_len in enumerate(x_lens):\n                attention_mask[i, x_len:] = 0 # 여기 디버깅 필요.\n            fc_output = self.BERT(inputs_embeds=sequenced_feat, attention_mask=attention_mask).pooler_output # inputs_embeds shape must be (batch_size, sequence_length, hidden_size)\n\n        # HEAD\n        fc_output = self.head(fc_output)\n    \n        return fc_output     \n\n\n    # def forward(self, x, x_lens):\n    #     B, C, D, H, W = x.shape\n    #     slice_feat_list = []\n    #     for i in range(D):\n    #         slice_feat = self.encoder(x[:, :, i, :, :])[-1]\n    #         slice_feat = F.adaptive_avg_pool2d(slice_feat, output_size=1)\n    #         # slice_feat = self.feat_cls_extract(x[:, :, i, :, :])\n    #         slice_feat_list.append(slice_feat)\n        \n    #     stacked_slice_feat = torch.stack(slice_feat_list, dim=2) # output: [B, C, Depth, 1, 1]\n    #     sequenced_feat = stacked_slice_feat.flatten(start_dim=2)    # output: [B, C, Depth]\n    #     sequenced_feat = sequenced_feat.permute(0, 2, 1)            # output: [B, Depth, C]\n\n    #     # CNN feature extraction\n    #     # sequenced_feat = self.slice_inferer(x, self.feat_cls_extract) # output: [B, C, Depth, 1, 1]\n    #     # sequenced_feat = sequenced_feat.flatten(start_dim=2) # output: [B, C, Depth]\n    #     # sequenced_feat = sequenced_feat.permute(0, 2, 1)     # output: [B, Depth, C]\n\n    #     # Squential representation\n    #     if self.operator_3d == 'lstm':\n    #         self.LSTM.flatten_parameters()  # For Multi GPU  \n    #         x_packed = pack_padded_sequence(sequenced_feat, x_lens.cpu(), batch_first=True, enforce_sorted=False)  # x_len이 cpu int64로 들어가야함!!!\n    #         RNN_out, (h_n, h_c) = self.LSTM(x_packed, None)    # input shape must be [batch, seq, feat_dim]\n    #         fc_output = torch.cat([h_n[-2, :, :], h_n[-1, :, :]], dim=1) # Due to the Bi-directional\n    #         fc_output = self.linear_lstm(fc_output)\n    #         fc_output = self.relu_lstm(fc_output)\n    #     elif self.operator_3d == 'bert':\n    #         cls_tokens     = self.cls_token.expand(sequenced_feat.shape[0], -1, -1)\n    #         sequenced_feat = torch.cat((cls_tokens, sequenced_feat), dim=1)  # cls token 추가\n    #         attention_mask = torch.ones(sequenced_feat.shape[:2], dtype=torch.long).to(sequenced_feat.device) # x_lens을 이용하여 attention_mask 생성\n    #         for i, x_len in enumerate(x_lens):\n    #             attention_mask[i, x_len:] = 0 # 여기 디버깅 필요.\n    #         fc_output = self.BERT(inputs_embeds=sequenced_feat, attention_mask=attention_mask).pooler_output # inputs_embeds shape must be (batch_size, sequence_length, hidden_size)\n\n    #     # HEAD\n    #     fc_output = self.head(fc_output)\n    \n    #     return fc_output    \n    \n\n# 3D-SEG: 3D operator w/ 2D encoder\nclass SMART_Net_3D_SEG(nn.Module):\n    def __init__(self, transfer_pretrained=None, use_pretrained_encoder=True, use_pretrained_decoder=False, freeze_encoder=True, freeze_decoder=False, roi_size=512, sw_batch_size=64, spatial_dim=0, backbone='maxvit-xlarge', use_skip=True, pool_type='gem', operator_3d='3d_cnn'):\n        super(SMART_Net_3D_SEG, self).__init__()\n        self.roi_size = roi_size\n        self.slice_inferer = SliceInferer(roi_size=(roi_size, roi_size), sw_batch_size=sw_batch_size, spatial_dim=spatial_dim, device=torch.device(\"cuda\"))\n        self.operator_3d = operator_3d\n        self.freeze_encoder = freeze_encoder\n        self.freeze_decoder = freeze_decoder\n\n        # 2D Encoder\n        self.model_2d = SMART_Net_2D(backbone=backbone, use_skip=use_skip, pool_type=pool_type)\n        self.feat_dim = self.model_2d.output_channels\n        self.encoder  = self.model_2d.encoder\n\n        # 2D SEG Decoder\n        self.seg_decoder1 = SEG_DecoderBlock(in_channels=self.feat_dim[0], skip_channels=self.feat_dim[1], out_channels=256, use_batchnorm=True, attention_type='scse')\n        self.seg_decoder2 = SEG_DecoderBlock(in_channels=256,              skip_channels=self.feat_dim[2], out_channels=128, use_batchnorm=True, attention_type='scse')\n        self.seg_decoder3 = SEG_DecoderBlock(in_channels=128,              skip_channels=self.feat_dim[3], out_channels=64,  use_batchnorm=True, attention_type='scse')\n        self.seg_decoder4 = SEG_DecoderBlock(in_channels=64,               skip_channels=self.feat_dim[4], out_channels=32,  use_batchnorm=True, attention_type='scse')\n        self.seg_decoder5 = SEG_DecoderBlock(in_channels=32,               skip_channels=0,                out_channels=16,  use_batchnorm=True, attention_type='scse', last=True)\n\n        # 3D operator\n        if self.operator_3d == '3d_cnn':\n            self.conv1 = nn.Conv3d(in_channels=16, out_channels=16, kernel_size=3, stride=1, padding=1, bias=True)\n            self.bn1   = nn.BatchNorm3d(16)\n            self.relu1 = nn.ReLU()\n            self.conv2 = nn.Conv3d(in_channels=16, out_channels=16, kernel_size=3, stride=1, padding=1, bias=True)\n            self.bn2   = nn.BatchNorm3d(16)\n            self.relu2 = nn.ReLU()\n        elif self.operator_3d == '3d_vit':\n            self.img_size    = (64, roi_size, roi_size)\n            self.patch_size  = (4, roi_size//16, roi_size//16)\n            self.feat_size   = (self.img_size[0]//self.patch_size[0], self.img_size[1]//self.patch_size[1], self.img_size[2]//self.patch_size[2])\n            self.in_channels = 16\n            self.vit = ViT_Decoder(in_channels=self.in_channels,\n                                   hidden_size=768,\n                                   img_size=self.img_size,\n                                   patch_size=self.patch_size,\n                                   pos_embed=\"perceptron\",\n                                   spatial_dims=3,\n                                   classification=True)\n            self.vit_decoder = nn.Linear(768, self.patch_size[0] * self.patch_size[1] * self.patch_size[2] * self.in_channels, bias=True)  # patch_size[0]*patch_size[1]*patch_size[2]*in_channels\n        \n        # Head\n        self.head = nn.Conv3d(in_channels=16, out_channels=1, kernel_size=3, stride=1, padding=1, bias=True)\n\n        # Init\n        if (transfer_pretrained is not None) and (use_pretrained_encoder):\n            self.load_pretrained_encoder(transfer_pretrained)\n        \n        if (transfer_pretrained is not None) and (use_pretrained_decoder):\n            self.load_pretrained_decoder(transfer_pretrained)\n\n    def load_pretrained_encoder(self, transfer_pretrained):\n        print(\"Load State Dict...!\")\n        check_layer = list(self.encoder.state_dict().keys())[0]\n        print(\"Original weight = \", self.encoder.state_dict()[check_layer][0].mean())\n        checkpoint = torch.load(transfer_pretrained, map_location=torch.device('cpu'))['model_state_dict'] # 초반 EfficientNet + Focal + Tversky\n        checkpoint = {k.replace('module.', ''): v for k, v in checkpoint.items()}\n        checkpoint = {k.replace('encoder.', ''): v for k, v in checkpoint.items() if 'encoder.' in k}\n        self.encoder.load_state_dict(checkpoint)\n        print(\"Updated weight  = \", self.encoder.state_dict()[check_layer][0].mean())\n\n    def load_pretrained_decoder(self, transfer_pretrained):\n        print(\"Load State Dict...!\")\n        check_layer = list(self.seg_decoder1.state_dict().keys())[0]\n        print(\"Original weight =\", self.seg_decoder1.state_dict()[check_layer][0].mean())\n        checkpoint = torch.load(transfer_pretrained, map_location=torch.device('cpu'))['model_state_dict']\n        checkpoint = {k.replace('module.', ''): v for k, v in checkpoint.items()}\n        def load_decoder_block(block, block_num):\n            block_dict = {k.replace(f'seg_decoder_block{block_num}.', ''): v for k, v in checkpoint.items() if f'seg_decoder_block{block_num}.' in k}\n            block.load_state_dict(block_dict)\n        for i in range(1, 6):\n            load_decoder_block(getattr(self, f'seg_decoder{i}'), i)\n        print(\"Updated weight  =\", self.seg_decoder1.state_dict()[check_layer][0].mean())\n\n    def feat_seg_extract(self, x):\n        if self.freeze_encoder:\n            self.encoder.eval()\n            with torch.no_grad():\n                skip4, skip3, skip2, skip1, x = self.encoder.forward(x)\n        else:\n            skip4, skip3, skip2, skip1, x = self.encoder.forward(x)\n        \n        if self.freeze_decoder:\n            self.seg_decoder1.eval()\n            self.seg_decoder2.eval()\n            self.seg_decoder3.eval()\n            self.seg_decoder4.eval()\n            self.seg_decoder5.eval()\n            with torch.no_grad():            \n                seg = self.seg_decoder1(x,   skip1)\n                seg = self.seg_decoder2(seg, skip2)\n                seg = self.seg_decoder3(seg, skip3)\n                seg = self.seg_decoder4(seg, skip4)        \n                seg = self.seg_decoder5(seg)\n        else:\n            seg = self.seg_decoder1(x,   skip1)\n            seg = self.seg_decoder2(seg, skip2)\n            seg = self.seg_decoder3(seg, skip3)\n            seg = self.seg_decoder4(seg, skip4)        \n            seg = self.seg_decoder5(seg)\n        \n        return seg\n\n\n    def forward(self, x):\n        # cnn feature extraction\n        stacked_feat = self.slice_inferer(x, self.feat_seg_extract) # output shape = [B, C(=16), D, H, W]\n        \n        # Squential representation\n        if self.operator_3d == '3d_cnn':\n            output = self.conv1(stacked_feat)  #  input = (B, C, D, H, W)\n            output = self.bn1(output)\n            output = self.relu1(output)\n            output = self.conv2(output)\n            output = self.bn2(output)\n            output = self.relu2(output)\n        elif self.operator_3d == '3d_vit':\n            output = self.vit(stacked_feat)    # output = b (d h w) (p1 p2 p3 c)\n            output = self.vit_decoder(output)\n            output = output[:, 1:, :]   # remove cls token\n            # (B, L, patch_size0*patch_size1*patch_size2*in_channels) -> (B, d, h, w, patch_size0, patch_size1, patch_size2, in_channels)\n            output = output.view(output.shape[0], self.feat_size[0], self.feat_size[1], self.feat_size[2], self.patch_size[0], self.patch_size[1], self.patch_size[2], self.in_channels)\n            output = output.permute(0, 7, 1, 4, 2, 5, 3, 6).contiguous()\n            output = output.view(output.shape[0], self.in_channels, self.feat_size[0]*self.patch_size[0], self.feat_size[1]*self.patch_size[1], self.feat_size[2]*self.patch_size[2])\n\n        # Head\n        output = self.head(output)\n        return output\n\n\n\n\n\n\n# 3D - 331\nclass SCSEModule_3D(nn.Module):\n    def __init__(self, in_channels, reduction=16):\n        super().__init__()\n        self.cSE = nn.Sequential(\n            nn.AdaptiveAvgPool3d(1),\n            nn.Conv3d(in_channels, in_channels // reduction, 1),\n            nn.ReLU(inplace=True),\n            nn.Conv3d(in_channels // reduction, in_channels, 1),\n            nn.Sigmoid(),\n        )\n        self.sSE = nn.Sequential(nn.Conv3d(in_channels, 1, 1), nn.Sigmoid())\n\n    def forward(self, x):\n        return x * self.cSE(x) + x * self.sSE(x)\n\nclass Attention_3D(nn.Module):\n    def __init__(self, name, **params):\n        super().__init__()\n        if name is None:\n            self.attention = nn.Identity(**params)\n        elif name == \"scse\":\n            self.attention = SCSEModule_3D(**params)\n        else:\n            raise ValueError(\"Attention {} is not implemented\".format(name))\n\n    def forward(self, x):\n        return self.attention(x)\n\nclass Conv3dReLU(nn.Sequential):\n    def __init__(self, in_channels, out_channels, kernel_size, padding=0, stride=1, use_batchnorm=True):\n        conv = nn.Conv3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, bias=not (use_batchnorm))\n        relu = nn.ReLU(inplace=True)\n        if use_batchnorm:\n            bn = nn.BatchNorm3d(out_channels)\n        else:\n            bn = nn.Identity()\n        super(Conv3dReLU, self).__init__(conv, bn, relu)\n\nclass SEG_DecoderBlock_3D_331(nn.Module):\n    def __init__(self, in_channels, skip_channels, out_channels, use_batchnorm=True, attention_type=None, last=False):\n        super().__init__()\n        self.attention1 = nn.Identity() if last else Attention_3D(attention_type, in_channels=in_channels+skip_channels)\n        self.conv1      = Conv3dReLU(in_channels+skip_channels, out_channels, kernel_size=(3, 3, 1), padding=(1, 1, 0), use_batchnorm=use_batchnorm)\n        self.conv2      = Conv3dReLU(out_channels, out_channels, kernel_size=(3, 3, 1), padding=(1, 1, 0), use_batchnorm=use_batchnorm)        \n        self.attention2 = Attention_3D(attention_type, in_channels=out_channels)\n\n    def forward(self, x, skip=None):\n        x = F.interpolate(x, scale_factor=(2, 2, 1), mode=\"nearest\")\n        if skip is not None:\n            x = torch.cat([x, skip], dim=1)\n            x = self.attention1(x)\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = self.attention2(x)\n        return x\n\nclass PixelShuffle3d_331(nn.Module):\n    def __init__(self, scale):\n        super().__init__()\n        self.scale = scale\n\n    def forward(self, input):\n        batch_size, channels, in_height, in_width, in_depth = input.size()\n        nOut = channels // self.scale ** 2\n\n        out_depth  = in_depth\n        out_height = in_height * self.scale\n        out_width  = in_width  * self.scale\n\n        input_view = input.contiguous().view(batch_size, nOut, self.scale, self.scale, 1, in_height, in_width, in_depth)\n\n        output = input_view.permute(0, 1, 5, 2, 6, 3, 7, 4).contiguous()\n\n        return output.view(batch_size, nOut, out_height, out_width, out_depth)\n\nclass UpsampleBlock_3D_331(nn.Module):\n    def __init__(self, scale, input_channels, output_channels, ksize=1):\n        super(UpsampleBlock_3D_331, self).__init__()\n        self.upsample = nn.Sequential(\n            nn.Conv3d(input_channels, output_channels*(scale**2), kernel_size=1, stride=1, padding=ksize//2),\n            PixelShuffle3d_331(scale=scale)\n        )\n\n    def forward(self, input):\n        return self.upsample(input)\n\nclass REC_Skip_DecoderBlock_3D_331(nn.Module):\n    def __init__(self, in_channels, skip_channels, out_channels, use_batchnorm=True, attention_type=None, last=False):\n        super().__init__()\n        self.upsample   = UpsampleBlock_3D_331(scale=2, input_channels=in_channels, output_channels=in_channels)\n        self.attention1 = nn.Identity() if last else Attention_3D(attention_type, in_channels=in_channels+skip_channels)\n        self.conv1      = Conv3dReLU(in_channels+skip_channels, out_channels, kernel_size=(3, 3, 1), padding=(1, 1, 0), use_batchnorm=use_batchnorm)\n        self.conv2      = Conv3dReLU(out_channels, out_channels, kernel_size=(3, 3, 1), padding=(1, 1, 0), use_batchnorm=use_batchnorm)        \n        self.attention2 = Attention_3D(attention_type, in_channels=out_channels)\n\n    def forward(self, x, skip=None):\n        x = self.upsample(x)\n        if skip is not None:\n            x = torch.cat([x, skip], dim=1)\n            x = self.attention1(x)\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = self.attention2(x)\n        return x\n\nclass EfficientNetB5_UNet_MTL_CLS_SEG_REC_25D_331_CLS(nn.Module):\n    def __init__(self):\n        super(EfficientNetB5_UNet_MTL_CLS_SEG_REC_25D_331_CLS, self).__init__()\n\n        # Encoder\n        self.encoder = monai.networks.nets.EfficientNetBNFeatures(\"efficientnet-b5\", spatial_dims=3, in_channels=1, num_classes=1)\n\n        # CLS Decoder (ref: https://github.com/Project-MONAI/MONAI/blob/a86c0e01555727bd848275c7eabc5b9dc26da73d/monai/networks/nets/efficientnet.py#L229)\n        self.cls_decoder_block1 = nn.Conv3d(512, 2048, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n        self.cls_decoder_block2 = nn.BatchNorm3d(2048, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n        self.cls_decoder_block3 = monai.networks.blocks.activation.MemoryEfficientSwish()\n        self.cls_decoder_block4 = nn.AdaptiveAvgPool3d(output_size=1)\n        self.cls_decoder_block5 = Flatten()\n        self.cls_decoder_block6 = nn.Dropout(p=0.5, inplace=False)\n\n        # SEG Decoder\n        self.seg_decoder_block1 = SEG_DecoderBlock_3D_331(in_channels=512, skip_channels=176, out_channels=256, use_batchnorm=True, attention_type='scse')\n        self.seg_decoder_block2 = SEG_DecoderBlock_3D_331(in_channels=256, skip_channels=64,  out_channels=128, use_batchnorm=True, attention_type='scse')\n        self.seg_decoder_block3 = SEG_DecoderBlock_3D_331(in_channels=128, skip_channels=40,  out_channels=64,  use_batchnorm=True, attention_type='scse')\n        self.seg_decoder_block4 = SEG_DecoderBlock_3D_331(in_channels=64,  skip_channels=24,  out_channels=32,  use_batchnorm=True, attention_type='scse')\n        self.seg_decoder_block5 = SEG_DecoderBlock_3D_331(in_channels=32,  skip_channels=0,   out_channels=16,  use_batchnorm=True, attention_type='scse', last=True)\n        \n        # REC Decoder\n        self.rec_decoder_block1 = REC_Skip_DecoderBlock_3D_331(in_channels=512, skip_channels=176, out_channels=256, use_batchnorm=True, attention_type='scse')\n        self.rec_decoder_block2 = REC_Skip_DecoderBlock_3D_331(in_channels=256, skip_channels=64,  out_channels=128, use_batchnorm=True, attention_type='scse')\n        self.rec_decoder_block3 = REC_Skip_DecoderBlock_3D_331(in_channels=128, skip_channels=40,  out_channels=64,  use_batchnorm=True, attention_type='scse')\n        self.rec_decoder_block4 = REC_Skip_DecoderBlock_3D_331(in_channels=64,  skip_channels=24,  out_channels=32,  use_batchnorm=True, attention_type='scse')\n        self.rec_decoder_block5 = REC_Skip_DecoderBlock_3D_331(in_channels=32,  skip_channels=0,   out_channels=16,  use_batchnorm=True, attention_type='scse', last=True)\n\n        # Head\n        self.cls_head = nn.Linear(in_features=2048, out_features=3, bias=True)\n        self.seg_head = nn.Conv3d(in_channels=16,   out_channels=1, kernel_size=(3, 3, 1), padding=(1, 1, 0))\n        self.rec_head = nn.Conv3d(in_channels=16,   out_channels=1, kernel_size=(3, 3, 1), padding=(1, 1, 0))\n        \n        # Init\n        self.modify_kernel_size(self.encoder)\n\n    def modify_kernel_size(self, module):\n        for child_name, child in module.named_children():\n            if isinstance(child, nn.Conv3d) and child.kernel_size == (3, 3, 3) and child.stride == (2, 2, 2):\n                if child.groups is not None:\n                    new = nn.Conv3d(child.in_channels, child.out_channels, kernel_size=(3, 3, 1), stride=(2, 2, 1), groups=child.groups, bias=child.bias is not None)\n                    setattr(module, child_name, new)\n                else:  \n                    new = nn.Conv3d(child.in_channels, child.out_channels, kernel_size=(3, 3, 1), stride=(2, 2, 1), bias=child.bias is not None)\n                    setattr(module, child_name, new)\n\n            if isinstance(child, nn.Conv3d) and child.kernel_size == (5, 5, 5) and child.stride == (2, 2, 2):\n                if child.groups is not None:\n                    new = nn.Conv3d(child.in_channels, child.out_channels, kernel_size=(5, 5, 1), stride=(2, 2, 1), groups=child.groups, bias=child.bias is not None)\n                    setattr(module, child_name, new)\n                else:  \n                    new = nn.Conv3d(child.in_channels, child.out_channels, kernel_size=(5, 5, 1), stride=(2, 2, 1), bias=child.bias is not None)\n                    setattr(module, child_name, new)\n\n            if isinstance(child, nn.Conv3d) and child.kernel_size == (3, 3, 3) and child.stride == (1, 1, 1):\n                if child.groups is not None:\n                    new = nn.Conv3d(child.in_channels, child.out_channels, kernel_size=(3, 3, 1), stride=(1, 1, 1), groups=child.groups, bias=child.bias is not None)\n                    setattr(module, child_name, new)\n                else:  \n                    new = nn.Conv3d(child.in_channels, child.out_channels, kernel_size=(3, 3, 1), stride=(1, 1, 1), bias=child.bias is not None)\n                    setattr(module, child_name, new)\n\n            if isinstance(child, nn.Conv3d) and child.kernel_size == (5, 5, 5) and child.stride == (1, 1, 1):\n                if child.groups is not None:\n                    new = nn.Conv3d(child.in_channels, child.out_channels, kernel_size=(5, 5, 1), stride=(1, 1, 1), groups=child.groups, bias=child.bias is not None)\n                    setattr(module, child_name, new)\n                else:  \n                    new = nn.Conv3d(child.in_channels, child.out_channels, kernel_size=(5, 5, 1), stride=(1, 1, 1), bias=child.bias is not None)\n                    setattr(module, child_name, new)\n\n            if isinstance(child, nn.ConstantPad3d) and child.padding == (1, 2, 1, 2, 1, 2):\n                new = nn.ConstantPad3d(padding=(0, 0)+child.padding[2:], value=0.0)\n                setattr(module, child_name, new)\n\n            if isinstance(child, nn.ConstantPad3d) and child.padding == (0, 1, 0, 1, 0, 1):\n                new = nn.ConstantPad3d(padding=(0, 0)+child.padding[2:], value=0.0)\n                setattr(module, child_name, new)\n\n            if isinstance(child, nn.ConstantPad3d) and child.padding == (1, 1, 1, 1, 1, 1):\n                new = nn.ConstantPad3d(padding=(0, 0)+child.padding[2:], value=0.0)\n                setattr(module, child_name, new)                \n\n            if isinstance(child, nn.ConstantPad3d) and child.padding == (2, 2, 2, 2, 2, 2):\n                new = nn.ConstantPad3d(padding=(0, 0)+child.padding[2:], value=0.0)\n                setattr(module, child_name, new)\n\n            # 재귀적으로 모든 하위 레이어를 검사합니다.\n            self.modify_kernel_size(child)\n\n    def forward(self, x):\n        # encoder\n        skip4, skip3, skip2, skip1, x = self.encoder(x)\n\n        # cls decoder\n        cls = self.cls_decoder_block1(x)\n        cls = self.cls_decoder_block2(cls)\n        cls = self.cls_decoder_block3(cls)\n        cls = self.cls_decoder_block4(cls)\n        cls = self.cls_decoder_block5(cls)\n        cls = self.cls_decoder_block6(cls)\n\n        # seg decoder\n        seg = self.seg_decoder_block1(x,   skip1)\n        seg = self.seg_decoder_block2(seg, skip2)\n        seg = self.seg_decoder_block3(seg, skip3)\n        seg = self.seg_decoder_block4(seg, skip4)        \n        seg = self.seg_decoder_block5(seg)\n\n        # rec decoder\n        rec = self.rec_decoder_block1(x,   skip1)\n        rec = self.rec_decoder_block2(rec, skip2)\n        rec = self.rec_decoder_block3(rec, skip3)\n        rec = self.rec_decoder_block4(rec, skip4)\n        rec = self.rec_decoder_block5(rec)\n\n        # head\n        cls = self.cls_head(cls)\n        seg = self.seg_head(seg)\n        rec = self.rec_head(rec)\n        \n        return cls, seg, rec"}
{"type": "source_file", "path": "losses.py", "content": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport random\nimport numpy as np\nimport functools\n\ndef binary_dice_loss(y_true, y_pred, smooth=0.0, eps=1e-7, return_score=False):\n    bs = y_true.size(0)\n    y_true = y_true.view(bs, 1, -1)\n    y_pred = y_pred.view(bs, 1, -1)\n\n    intersection = torch.sum(y_true * y_pred, dim=(0, 2))\n    cardinality  = torch.sum(y_true + y_pred, dim=(0, 2))\n\n    dice_score = (2. * intersection + smooth) / (cardinality + smooth).clamp_min(eps)\n    dice_loss  = 1 - dice_score\n    if return_score:\n        return dice_score.mean()\n    else:\n        return dice_loss.mean()\n\ndef binary_focal_loss(y_pred, y_true, gamma=2.0, alpha=0.25):\n    # alpha: Weight constant that penalize model for FNs (False Negatives)\n    logpt  = F.binary_cross_entropy(y_pred, y_true, reduction=\"none\")\n    focal_term = (1.0 - torch.exp(-logpt)).pow(gamma)\n    loss = focal_term * logpt\n    loss *= alpha*y_true + (1-alpha)*(1-y_true)\n    return loss.mean()\n\ndef binary_tversky_loss(y_pred, y_true, alpha=0.5, beta=0.5, smooth=0.0, eps=1e-7):\n    # With alpha == beta == 0.5, this loss becomes equal DiceLoss.\n    # alpha: Weight constant that penalize model for FPs (False Positives)\n    # beta: Weight constant that penalize model for FNs (False Negatives)    \n    bs = y_true.size(0)\n    y_true = y_true.view(bs, 1, -1)\n    y_pred = y_pred.view(bs, 1, -1)\n\n    intersection = torch.sum(y_true * y_pred, dim=(0, 2))\n    fp = torch.sum(y_pred * (1.0 - y_true), dim=(0, 2))\n    fn = torch.sum((1 - y_pred) * y_true, dim=(0, 2))\n\n    tversky_score = (intersection + smooth) / (intersection + alpha * fp + beta * fn + smooth).clamp_min(eps)\n    tversky_loss  = 1 - tversky_score\n    return tversky_loss.mean()\n\n\n\nclass MTL_Loss(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.loss_cls     = F.binary_cross_entropy\n        self.loss_seg     = binary_dice_loss\n        self.loss_rec     = F.l1_loss\n        self.loss_consist = F.mse_loss\n\n    def forward(self, pred_cls, pred_seg, pred_rec, label, mask, image, pooled_seg=None):\n        assert pred_cls.size() == label.size(), f\"{pred_cls.size()} != {label.size()}\"\n        assert pred_seg.size() == mask.size(),  f\"{pred_seg.size()} != {mask.size()}\"\n        assert pred_rec.size() == image.size(), f\"{pred_rec.size()} != {image.size()}\"\n\n        cls_loss  = self.loss_cls(input=pred_cls, target=label)\n        seg_loss  = self.loss_seg(y_pred=pred_seg, y_true=mask)\n        rec_loss  = self.loss_rec(input=pred_rec, target=image)\n\n        total_loss = cls_loss + seg_loss + rec_loss\n        loss_dict  = {\"total_loss\": total_loss.item(), \"cls_loss\": cls_loss.item(), \"seg_loss\": seg_loss.item(), \"rec_loss\": rec_loss.item()}\n\n        if pooled_seg is not None:\n            total_loss += self.loss_consist(pred_cls, pooled_seg)\n            loss_dict[\"consist_loss\"] = self.loss_consist(pred_cls, pooled_seg).item()\n            return total_loss, loss_dict\n        else:\n            return total_loss, loss_dict\n\n\n# 3D - 2D transfer\nclass CLS_Loss(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.loss_cls = F.binary_cross_entropy\n        # self.loss_cls_binary_focal = functools.partial(binary_focal_loss, gamma=0.0, alpha=0.7)\n\n    def forward(self, pred_cls, label):\n        # print(logit_cls.shape, logit_seg.shape, logit_det.shape, logit_rec.shape, logit_idx.shape, label.shape, mask.shape, bbox.shape, image.shape, idx.shape)\n        assert pred_cls.size() == label.size(), f\"{pred_cls.size()} != {label.size()}\"\n        \n        cls_loss = self.loss_cls(input=pred_cls, target=label)\n        # cls_binary_focal_loss  = self.loss_cls_binary_focal(y_pred=pred_cls, y_true=label)\n\n        # return cls_bce_loss + cls_binary_focal_loss, {'cls_bce_loss': cls_bce_loss, 'cls_binary_focal_loss': cls_binary_focal_loss}\n        total_loss = cls_loss\n        loss_dict  = {\"total_loss\": total_loss.item(), \"cls_bce_loss\": cls_loss.item()}\n        return total_loss, loss_dict\n    \nclass SEG_Loss(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.loss_seg_dice = binary_dice_loss\n        self.loss_seg_bce  = F.binary_cross_entropy\n\n    def forward(self, pred_seg, mask):\n        assert pred_seg.size() == mask.size(), f\"{pred_seg.size()} != {mask.size()}\"\n        seg_dice_loss = self.loss_seg_dice(y_pred=pred_seg, y_true=mask)\n        seg_bce_loss  = self.loss_seg_bce(input=pred_seg, target=mask)\n\n        return seg_dice_loss + seg_bce_loss, {\"seg_dice_loss\": seg_dice_loss, \"seg_bce_loss\": seg_bce_loss}\n\n\n\n\ndef get_loss(name):\n    # 2D\n    if name == 'MTL_Loss':\n        return MTL_Loss()                   \n    \n    # elif name == 'MTL_CLS_SEG_REC_Loss_2':\n    #     return MTL_CLS_SEG_REC_Loss_2()                       \n\n    # 3D - 2D transfer\n    elif name == 'CLS_Loss':\n        return CLS_Loss()    \n\n    elif name == 'SEG_Loss':\n        return SEG_Loss()    \n\n    else:\n        raise NotImplementedError"}
{"type": "source_file", "path": "dataset.py", "content": "import re\nimport glob\nimport cv2\nimport functools\nimport torch\nimport numpy as np\nimport skimage\nimport random\nimport SimpleITK as sitk\nimport pydicom\nimport albumentations as A\nimport pandas as pd\n\nfrom albumentations.pytorch import ToTensorV2\nfrom albumentations.core.transforms_interface import ImageOnlyTransform, DualTransform\n\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import Dataset as BaseDataset\nfrom typing import Dict, Optional, Sequence, Tuple, Union, Callable\n\nfrom monai.transforms import Resize\n\nimport warnings\nwarnings.filterwarnings(action='ignore') \n\n# functions\ndef list_sort_nicely(l):\n    def convert(text): return int(text) if text.isdigit() else text\n    def alphanum_key(key): return [convert(c) for c in re.split('([0-9]+)', key)]\n    return sorted(l, key=alphanum_key)\n\ndef change_to_uint8(image, **kwargs):\n    return skimage.util.img_as_ubyte(image)\n\ndef change_to_float32(image, **kwargs):\n    return skimage.util.img_as_float32(image)\n\ndef squeeze_transpose(image, **kwargs):\n    return image.squeeze(3).transpose(1, 2, 0)\n\ndef crop(img: np.ndarray, x_min: int, y_min: int, z_min: int, x_max: int, y_max: int, z_max: int):\n    channel, height, width, depth = img.shape\n    if x_max <= x_min or y_max <= y_min or z_max <= z_min:\n        raise ValueError(\n            \"We should have x_min < x_max and y_min < y_max. But we got\"\n            \" (x_min = {x_min}, y_min = {y_min}, z_min = {z_min}, x_max = {x_max}, y_max = {y_max}, z_max = {z_max})\".format(\n                x_min=x_min, x_max=x_max, y_min=y_min, y_max=y_max, z_min=z_min, z_max=z_max\n            )\n        )\n\n    if x_min < 0 or x_max > width or y_min < 0 or y_max > height or z_min < 0 or z_max > depth:\n        raise ValueError(\n            \"Values for crop should be non negative and equal or smaller than image sizes \\n\"\n            \"(x_min = {x_min}, y_min = {y_min}, z_min = {z_min}, x_max = {x_max}, y_max = {y_max}, z_max = {z_max}) \\n\"\n            \"height = {height}, width = {width}, depth = {depth})\".format(\n                x_min=x_min, x_max=x_max, y_min=y_min, y_max=y_max, z_min=z_min, z_max=z_max, height=height, width=width, depth=depth\n            )\n        )\n\n    return img[:, y_min:y_max, x_min:x_max, z_min:z_max] # image is C, H, W, D\n\ndef windowing(image, window_center=40, window_width=80, **kwargs):\n    lower_bound = window_center - window_width/2\n    upper_bound = window_center + window_width/2\n    image = (np.clip(image, lower_bound, upper_bound) - lower_bound) / window_width\n    return image.astype(np.float32)\n\ndef fixed_clahe(image, **kwargs):\n    clahe_mat = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n    if len(image.shape) == 2 or image.shape[2] == 1:\n        image = clahe_mat.apply(image)\n    else:\n        image = cv2.cvtColor(image, cv2.COLOR_RGB2LAB)\n        image[:, :, 0] = clahe_mat.apply(image[:, :, 0])\n        image = cv2.cvtColor(image, cv2.COLOR_LAB2RGB)\n    return image\n\ndef minmax_normalize(image, option=False, **kwargs):\n    if len(np.unique(image)) != 1:  # Sometimes it cause the nan inputs...\n        image -= image.min()\n        image /= image.max() \n\n    if option:\n        image = (image - 0.5) / 0.5  # Range -1.0 ~ 1.0   @ We do not use -1~1 range becuase there is no Tanh act.\n\n    return image.astype('float32')\n\ndef get_array_pydicom(path):\n    ds = pydicom.dcmread(path)\n    # Correct DICOM pixel_array if PixelRepresentation == 1.\n    pixel_array = ds.pixel_array\n    if ds.PixelRepresentation == 1:\n        bit_shift = ds.BitsAllocated - ds.BitsStored\n        dtype = pixel_array.dtype \n        pixel_array = (pixel_array << bit_shift).astype(dtype) >>  bit_shift\n    \n    intercept = float(ds.RescaleIntercept)\n    slope     = float(ds.RescaleSlope)\n\n    pixel_array = (pixel_array * slope) + intercept\n    pixel_array = np.array(pixel_array, dtype=np.float32)\n    return pixel_array\n\ndef extract_3D(x):\n    dcm_list = list_sort_nicely(glob.glob(x + '/*.dcm'))\n    # data_3d  = np.concatenate([sitk.GetArrayFromImage(sitk.ReadImage(dcm)) for dcm in dcm_list], axis=0)\n    data_3d  = np.stack([get_array_pydicom(dcm) for dcm in dcm_list], axis=-1) # H, W, D\n    return data_3d\n\nclass DualTransform_V2(DualTransform):\n    @property\n    def targets(self) -> Dict[str, Callable]:\n        return {\n            \"image\": self.apply,\n            \"mask\": self.apply_to_mask,\n            \"masks\": self.apply_to_masks,\n            \"bboxes\": self.apply_to_bboxes,\n            \"keypoints\": self.apply_to_keypoints,\n            \"idx\": self.apply_to_idx,\n            \"pad_loc\": self.apply_to_loc,\n        }    \n    def apply_to_idx(self, idx, **params):\n        return idx\n\n    def apply_to_loc(self, pad_loc, **params):\n        return pad_loc\n\nclass SamplingSlice_3D_Pos_Crop(DualTransform_V2):\n    # reference: https://github.com/ZFTurbo/volumentations, Crop_Non_Empty_Mask_If_Exists\n    def __init__(self, ignore_values=None, ignore_channels=None, always_apply=False, p=1.0):\n        super(SamplingSlice_3D_Pos_Crop, self).__init__(always_apply, p)\n\n        if ignore_values is not None and not isinstance(ignore_values, list):\n            raise ValueError(\"Expected `ignore_values` of type `list`, got `{}`\".format(type(ignore_values)))\n        if ignore_channels is not None and not isinstance(ignore_channels, list):\n            raise ValueError(\"Expected `ignore_channels` of type `list`, got `{}`\".format(type(ignore_channels)))\n\n        self.depth  = 1\n        self.width  = 512\n        self.height = 512\n\n        self.ignore_values   = ignore_values\n        self.ignore_channels = ignore_channels\n\n    def apply(self, img, x_min=0, x_max=0, y_min=0, y_max=0, z_min=0, z_max=0, **params):\n        return crop(img, x_min, y_min, z_min, x_max, y_max, z_max)\n\n    def apply_to_idx(self, idx, x_min=0, x_max=0, y_min=0, y_max=0, z_min=0, z_max=0, **params):\n        return z_min\n\n    def _preprocess_mask(self, mask):\n        channel, mask_height, mask_width, mask_depth = mask.shape\n\n        if self.ignore_values is not None:\n            ignore_values_np = np.array(self.ignore_values)\n            mask = np.where(np.isin(mask, ignore_values_np), 0, mask)\n\n        if mask.ndim == 4 and self.ignore_channels is not None:\n            target_channels = np.array([ch for ch in range(mask.shape[0]) if ch not in self.ignore_channels])\n            mask = np.take(mask, target_channels, axis=0)\n\n        if self.height > mask_height or self.width > mask_width or self.depth > mask_depth:\n            raise ValueError(\"Crop size ({},{},{}) is larger than image ({},{},{})\".format(self.height, self.width, self.depth, mask_height, mask_width, mask_depth))\n\n        return mask\n\n    def update_params(self, params, **kwargs):\n        super().update_params(params, **kwargs)\n\n        if \"mask\" in kwargs:\n            mask = self._preprocess_mask(kwargs[\"mask\"])\n        elif \"masks\" in kwargs and len(kwargs[\"masks\"]):\n            masks = kwargs[\"masks\"]\n            mask = self._preprocess_mask(np.copy(masks[0]))  # need copy as we perform in-place mod afterwards\n            for m in masks[1:]:\n                mask |= self._preprocess_mask(m)\n        else:\n            raise RuntimeError(\"Can not find mask for CropNonEmptyMaskIfExists\")\n\n        channel, mask_height, mask_width, mask_depth = mask.shape\n\n        if mask.any():\n            mask = mask.sum(axis=0) if mask.ndim == 4 else mask\n            non_zero_yxz = np.argwhere(mask)\n            y, x, z = random.choice(non_zero_yxz)\n            x_min = x - random.randint(0, self.width  - 1)\n            y_min = y - random.randint(0, self.height - 1)\n            z_min = z - random.randint(0, self.depth  - 1)\n            x_min = np.clip(x_min, 0, mask_width  - self.width)\n            y_min = np.clip(y_min, 0, mask_height - self.height)\n            z_min = np.clip(z_min, 0, mask_depth  - self.depth)\n        else:\n            x_min = random.randint(0, mask_width  - self.width)\n            y_min = random.randint(0, mask_height - self.height)\n            z_min = random.randint(0, mask_depth  - self.depth)\n\n        x_max = x_min + self.width\n        y_max = y_min + self.height\n        z_max = z_min + self.depth\n\n        params.update({\"x_min\": x_min, \"x_max\": x_max, \"y_min\": y_min, \"y_max\": y_max, \"z_min\": z_min, \"z_max\": z_max})\n        return params\n\n    def get_transform_init_args_names(self):\n        return (\"ignore_values\", \"ignore_channels\")\n\nclass ResizeTransform(DualTransform_V2):\n    def __init__(self, height, width, always_apply=False, p=1):\n        super(ResizeTransform, self).__init__(always_apply, p)\n        self.height = height\n        self.width  = width\n\n    def apply(self, img, **params):\n        img = cv2.resize(img, (self.height, self.width), interpolation=cv2.INTER_LINEAR)\n        if len(img.shape) == 2:\n            img = img[..., np.newaxis]\n        return img\n\n    def apply_to_mask(self, img, **params):\n        img = cv2.resize(img, (self.height, self.width), interpolation=cv2.INTER_NEAREST)\n        if len(img.shape) == 2:\n            img = img[..., np.newaxis]\n        return img\n\n    def get_transform_init_args_names(self):\n        return (\"height\", \"width\")\n\nclass WindowingTransform(ImageOnlyTransform):\n    def __init__(self, always_apply=False, p=1.0):\n        super(WindowingTransform, self).__init__(always_apply, p)\n\n    def apply(self, image, **params):\n        return windowing(image)\n                  \n    def get_transform_init_args_names(self):\n        return ()\n    \nclass ChangeToUint8(ImageOnlyTransform):\n    def __init__(self, always_apply=False, p=1.0):\n        super(ChangeToUint8, self).__init__(always_apply, p)\n\n    def apply(self, image, **params):\n        return change_to_uint8(image)        \n                  \n    def get_transform_init_args_names(self):\n        return ()\n    \nclass ChangeToFloat32(ImageOnlyTransform):\n    def __init__(self, always_apply=False, p=1.0):\n        super(ChangeToFloat32, self).__init__(always_apply, p)\n\n    def apply(self, image, **params):\n        return change_to_float32(image)            \n              \n    def get_transform_init_args_names(self):\n        return ()\n    \nclass FixedClahe(ImageOnlyTransform):\n    def __init__(self, always_apply=False, p=1.0):\n        super(FixedClahe, self).__init__(always_apply, p)\n\n    def apply(self, image, **params):\n        return fixed_clahe(image)  \n              \n    def get_transform_init_args_names(self):\n        return ()\n    \nclass MinmaxNormalize(ImageOnlyTransform):\n    def __init__(self, always_apply=False, p=1.0):\n        super(MinmaxNormalize, self).__init__(always_apply, p)\n\n    def apply(self, image, **params):\n        return minmax_normalize(image)   \n             \n    def get_transform_init_args_names(self):\n        return ()\n\n# waste\n# Lambdad(keys=[\"label\"], func=functools.partial(resize_keep_depths, size=256, mode='label'))\n\ndef resize_keep_depths(x, size, mode):\n    depths = x.shape[-1]  # (C, H, W, D)\n    if mode == 'image':\n        x = Resize(spatial_size=(size, size, depths), mode='trilinear', align_corners=True)(x)\n    else :\n        x = Resize(spatial_size=(size, size, depths), mode='nearest', align_corners=None)(x)\n    return x\n\ndef clahe_keep_depths(image, clipLimit, tileGridSize):\n    image = skimage.util.img_as_ubyte(image.squeeze(0))\n    \n    assert image.dtype == np.uint8\n    assert len(image.shape) == 3  # 2d --> (H, W, 1) / 3d --> (H, W, D)\n\n    clahe_mat   = cv2.createCLAHE(clipLimit=clipLimit, tileGridSize=tileGridSize)\n    stacked_img = np.stack([clahe_mat.apply(image[..., i]) for i in range(image.shape[-1])], axis=-1)\n    \n    stacked_img = skimage.util.img_as_float32(stacked_img)        \n\n    return np.expand_dims(stacked_img, axis=0)\n\ndef delete_too_small_noise_keep_depths(mask, min_size, connectivity):\n    mask  = skimage.morphology.remove_small_objects(mask.squeeze(0).astype('bool'), min_size=min_size, connectivity=connectivity) # for noise reduction\n    \n    return np.expand_dims(mask, axis=0).astype('float32')\n    \ndef clahe_norm(image):\n    image = skimage.util.img_as_float32(image)\n    \n    if len(np.unique(image)) != 1:  # Sometimes it cause the nan inputs...\n        image -= image.min()\n        image /= image.max() \n\n    return image\n\nclass FixedCLAHE(ImageOnlyTransform):\n    def __init__(self, clip_limit=2.0, tile_grid_size=(8, 8), always_apply=False, p=0.5):\n        super(FixedCLAHE, self).__init__(always_apply, p)\n        self.clip_limit = clip_limit\n        self.tile_grid_size = tile_grid_size\n\n    def apply(self, image, **params):\n        clahe_mat = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n        if len(image.shape) == 2 or image.shape[2] == 1:\n            image = clahe_mat.apply(image)\n        else:\n            image = cv2.cvtColor(image, cv2.COLOR_RGB2LAB)\n            image[:, :, 0] = clahe_mat.apply(image[:, :, 0])\n            image = cv2.cvtColor(image, cv2.COLOR_LAB2RGB)\n        return image\n\n    def get_transform_init_args_names(self):\n        return (\"clip_limit\", \"tile_grid_size\")\n\n\n\n\n# collate_fn --> attention map 해야함....\ndef CLS_pad_collate_fn_LSTM(batch, roi_size=256):\n    paths  = []\n    depths = []\n    labels = []\n    \n    # len(batch) == batch_size\n    for sample in batch:\n        paths.append(sample[0])\n        labels.append(sample[2])\n        depths.append(sample[3])\n\n    labels = torch.stack(labels)\n    depths = torch.IntTensor(depths)\n\n    stack_padded_image = torch.zeros( (len(batch), 1, depths.max(), roi_size, roi_size) )    \n    for i, sample in enumerate(batch):\n        ori_c, ori_z, ori_x, ori_y = sample[1].shape\n        stack_padded_image[i, :ori_c, :ori_z, :ori_x, :ori_y] = sample[1]\n\n    return paths, stack_padded_image, labels, depths\n\ndef CLS_pad_collate_fn_TR(batch, roi_size=256, max_length=64):\n    paths  = []\n    depths = []\n    labels = []\n\n    for sample in batch:    \n        paths.append(sample[0])\n        labels.append(sample[2])\n        depths.append(sample[3])\n\n    labels = torch.stack(labels)\n    depths = torch.IntTensor(depths)\n\n    stack_padded_image = torch.zeros( (len(batch), 1, max_length, roi_size, roi_size) )    \n    for i, sample in enumerate(batch):\n        ori_c, ori_z, ori_x, ori_y = sample[1].shape\n        stack_padded_image[i, :ori_c, :ori_z, :ori_x, :ori_y] = sample[1]\n\n    return paths, stack_padded_image, labels, depths\n\ndef SEG_pad_collate_fn_PAD(batch, roi_size=256, max_length=64):\n    paths  = []\n    depths = []\n    for sample in batch:    \n        paths.append(sample[0])\n        depths.append(sample[3])\n\n    depths = torch.IntTensor(depths)\n\n    stack_padded_image = torch.zeros( (len(batch), 1, max_length, roi_size, roi_size) )\n    stack_padded_mask  = torch.zeros( (len(batch), 1, max_length, roi_size, roi_size) )\n    for i, sample in enumerate(batch):\n        ori_c, ori_z, ori_x, ori_y = sample[1].shape\n        stack_padded_image[i, :ori_c, :ori_z, :ori_x, :ori_y] = sample[1]\n        stack_padded_mask[i, :ori_c, :ori_z, :ori_x, :ori_y]  = sample[2]\n\n    return paths, stack_padded_image, stack_padded_mask, depths\n\ndef TEST_2D_pad_collate_fn(batch, roi_size=256, max_length=64):\n    paths  = []\n    depths = []\n    # len(batch) == batch_size\n    for sample in batch:\n        paths.append(sample[0])\n        depths.append(sample[4])\n\n    depths = torch.IntTensor(depths)\n    \n    stack_padded_image = torch.zeros( (len(batch), 1, max_length, roi_size, roi_size) )\n    stack_padded_mask  = torch.zeros( (len(batch), 1, max_length, roi_size, roi_size) )\n    stacl_padded_label = torch.zeros( (len(batch), max_length) )\n\n    for i, sample in enumerate(batch):\n        ori_c, ori_z, ori_x, ori_y = sample[1].shape\n        stack_padded_image[i, :ori_c, :ori_z, :ori_x, :ori_y] = sample[1]\n        stack_padded_mask[i, :ori_c, :ori_z, :ori_x, :ori_y]  = sample[3]\n        stacl_padded_label[i, :ori_z] = sample[2]\n\n    return paths, stack_padded_image, stacl_padded_label, stack_padded_mask, depths\n\n\ndef get_transforms_2D(mode=\"train\", roi_size=256):\n    if mode == \"train\":\n        transform_list = [\n            SamplingSlice_3D_Pos_Crop(ignore_values=None, ignore_channels=None, always_apply=False, p=1.0),\n            A.Lambda(image=squeeze_transpose, mask=squeeze_transpose), # H, W, C\n            A.Lambda(image=windowing),\n            A.Lambda(image=change_to_uint8, always_apply=True),\n            A.CLAHE(clip_limit=2.0, tile_grid_size=(8, 8), always_apply=True),\n            A.Lambda(image=change_to_float32, always_apply=True),\n            A.Lambda(image=minmax_normalize, always_apply=True),            \n            ]\n\n        if roi_size != 512:\n                transform_list.append(ResizeTransform(height=roi_size, width=roi_size, always_apply=True, p=1.0))\n\n        transform_list.extend([\n            # augmentation\n            A.RandomRotate90(p=0.5),\n            A.HorizontalFlip(p=0.5),\n            A.VerticalFlip(p=0.5),\n            A.ShiftScaleRotate(scale_limit=0.15, rotate_limit=15, shift_limit=0.1, border_mode=0, p=1.0),\n            A.OneOf([\n                A.RandomBrightnessContrast(brightness_limit=0.05, contrast_limit=0.05, p=1.0),\n                A.RandomBrightness(limit=0.05, p=1.0),\n                A.RandomContrast(limit=0.05, p=1.0),\n                ], p=0.5),\n            A.OneOf([\n                A.GaussianBlur(blur_limit=3, sigma_limit=0, p=1.0),\n                A.MedianBlur(blur_limit=3, p=1.0),\n                A.MotionBlur(blur_limit=3, p=1.0),\n                ], p=0.5),\n            A.GaussNoise(var_limit=(0.00001, 0.00005), mean=0, per_channel=True, p=0.5),\n            A.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05, p=0.8),    # https://arxiv.org/pdf/2212.04690.pdf            \n            \n            # normalization\n            A.Normalize(mean=0.5, std=0.5, max_pixel_value=1.0, p=1.0),\n            ToTensorV2(transpose_mask=True)\n        ])    \n        return A.Compose(transform_list, additional_targets={'image2': 'image'})\n    \n    elif mode == \"valid\":\n        transform_list = [\n            SamplingSlice_3D_Pos_Crop(ignore_values=None, ignore_channels=None, always_apply=False, p=1.0),\n            A.Lambda(image=squeeze_transpose, mask=squeeze_transpose), # H, W, C\n            A.Lambda(image=windowing),\n            A.Lambda(image=change_to_uint8, always_apply=True),\n            A.Lambda(image=fixed_clahe, always_apply=True),\n            A.Lambda(image=change_to_float32, always_apply=True),\n            A.Lambda(image=minmax_normalize, always_apply=True),      \n            ]\n        \n        if roi_size != 512:\n                transform_list.append(ResizeTransform(height=256, width=256, always_apply=True, p=1.0))\n\n        transform_list.extend([\n            # normalization\n            A.Normalize(mean=0.5, std=0.5, max_pixel_value=1.0, p=1.0),\n            ToTensorV2(transpose_mask=True)\n        ])\n        return A.Compose(transform_list, additional_targets={'image2': 'image'})\n\n    elif mode == \"test\":\n        transform_list = [\n            WindowingTransform(always_apply=True, p=1.0),\n            ChangeToUint8(always_apply=True, p=1.0),\n            FixedClahe(always_apply=True, p=1.0),\n            ChangeToFloat32(always_apply=True, p=1.0),\n            MinmaxNormalize(always_apply=True, p=1.0),\n            ]\n        \n        if roi_size != 512:\n            transform_list.append(ResizeTransform(height=roi_size, width=roi_size, always_apply=True, p=1.0))\n\n        transform_list.extend([\n            # normalization\n            A.Normalize(mean=0.5, std=0.5, max_pixel_value=1.0, p=1.0),\n            ToTensorV2(transpose_mask=True)\n        ])\n        return A.ReplayCompose(transform_list, additional_targets={'image2':'image'})\n\ndef get_transforms_3D_2Dtransfer(mode=\"train\", roi_size=256):\n    if mode == \"train\":\n        transform_list = [\n            WindowingTransform(always_apply=True, p=1.0),\n            ChangeToUint8(always_apply=True, p=1.0),\n            A.CLAHE(clip_limit=2.0, tile_grid_size=(8, 8), always_apply=True), # random slice\n            ChangeToFloat32(always_apply=True, p=1.0),\n            MinmaxNormalize(always_apply=True, p=1.0),\n            ]            \n        \n        if roi_size != 512:\n                transform_list.append(ResizeTransform(height=roi_size, width=roi_size, always_apply=True, p=1.0))\n\n        transform_list.extend([\n            # augmentation\n            A.RandomRotate90(p=0.5),\n            A.HorizontalFlip(p=0.5),\n            A.VerticalFlip(p=0.5),\n            A.ShiftScaleRotate(scale_limit=0.15, rotate_limit=15, shift_limit=0.1, border_mode=0, p=1.0),\n            A.OneOf([\n                A.RandomBrightnessContrast(brightness_limit=0.05, contrast_limit=0.05, p=1.0),\n                A.RandomBrightness(limit=0.05, p=1.0),\n                A.RandomContrast(limit=0.05, p=1.0),\n                ], p=0.5),\n            A.OneOf([\n                A.GaussianBlur(blur_limit=3, sigma_limit=0, p=1.0),\n                A.MedianBlur(blur_limit=3, p=1.0),\n                A.MotionBlur(blur_limit=3, p=1.0),\n                ], p=0.5),\n            A.GaussNoise(var_limit=(0.00001, 0.00005), mean=0, per_channel=True, p=0.5),\n            A.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05, p=0.8),    # https://arxiv.org/pdf/2212.04690.pdf            \n            \n            # normalization\n            A.Normalize(mean=0.5, std=0.5, max_pixel_value=1.0, p=1.0),\n            ToTensorV2(transpose_mask=True)\n        ])\n    \n    elif mode == \"valid\" or mode == \"test\":\n        transform_list = [\n            WindowingTransform(always_apply=True, p=1.0),\n            ChangeToUint8(always_apply=True, p=1.0),\n            FixedClahe(always_apply=True, p=1.0),\n            ChangeToFloat32(always_apply=True, p=1.0),\n            MinmaxNormalize(always_apply=True, p=1.0),\n            ]\n        \n        if roi_size != 512:\n            transform_list.append(ResizeTransform(height=roi_size, width=roi_size, always_apply=True, p=1.0))\n\n        transform_list.extend([\n            # normalization\n            A.Normalize(mean=0.5, std=0.5, max_pixel_value=1.0, p=1.0),\n            ToTensorV2(transpose_mask=True)\n        ])\n\n    return A.ReplayCompose(transform_list, additional_targets={'image2':'image'})\n\n\n\n\n\n\n# 2D\nclass HEMO_2D_MTL_Dataset(BaseDataset):\n    def __init__(self, mode=\"train\", roi_size=256):\n        self.root = '/workspace/1.Hemorrhage/SMART-Net/datasets'\n        self.mode = mode\n        if mode == 'train':\n            self.image_list = list_sort_nicely(glob.glob('/workspace/1.Hemorrhage/SMART-Net/dataset/Train_nii/*_img.nii.gz'))  + list_sort_nicely(glob.glob('/workspace/1.Hemorrhage/SMART-Net/dataset/Test_nii/Coreline_1350/NIFTI_IMG/*.nii.gz')) + list_sort_nicely(glob.glob('/workspace/1.Hemorrhage/SMART-Net/dataset/Test_nii/Asan_internal/*_img.nii.gz'))\n            self.mask_list  = list_sort_nicely(glob.glob('/workspace/1.Hemorrhage/SMART-Net/dataset/Train_nii/*_mask.nii.gz')) + list_sort_nicely(glob.glob('/workspace/1.Hemorrhage/SMART-Net/dataset/Test_nii/Coreline_1350/NIFTI_SEGGT/*.nii.gz')) + list_sort_nicely(glob.glob('/workspace/1.Hemorrhage/SMART-Net/dataset/Test_nii/Asan_internal/*_mask.nii.gz'))\n        else:\n            self.image_list = list_sort_nicely(glob.glob('/workspace/1.Hemorrhage/SMART-Net/dataset/Valid_nii/*_img.nii.gz'))\n            self.mask_list  = list_sort_nicely(glob.glob('/workspace/1.Hemorrhage/SMART-Net/dataset/Valid_nii/*_mask.nii.gz'))\n        \n        self.transforms = get_transforms_2D(mode=mode, roi_size=roi_size)\n\n        self.image_list = self.image_list\n        self.mask_list = self.mask_list\n    def __len__(self):\n        return len(self.image_list)\n\n    def __getitem__(self, index):\n        max_attempts = 100\n        for _ in range(max_attempts):\n            try:\n                # image\n                image = sitk.ReadImage(self.image_list[index])\n                image = sitk.GetArrayFromImage(image).transpose(1, 2, 0)  # H, W, D\n                image = np.expand_dims(image, axis=0) # C, H, W, D\n\n                # mask\n                mask = sitk.ReadImage(self.mask_list[index])\n                mask = sitk.GetArrayFromImage(mask).transpose(1, 2, 0)  # H, W, D\n                mask = np.expand_dims(mask, axis=0) # C, H, W, D\n        \n                # C, H, W, D = image.shape\n\n                # augmentation\n                sample = self.transforms(image=image, mask=mask, idx='slice_idx', pad_loc=('pad_top', 'pad_bottom', 'pad_left', 'pad_right'))\n                image  = sample['image']\n                mask   = sample['mask']\n                \n                # BG=0, B = 1, M = 2\n                if mask.sum().item() > 0:\n                    label = torch.tensor([1.0])\n                else :\n                    label = torch.tensor([0.0])\n\n                # image [B, 1, 512, 512], mask [B, 1, 512, 512], label [B, 1]\n                return self.image_list[index], image.float(), label.float(), mask.float()\n\n            except Exception as e:\n                print(f\"Error in __getitem__ at path {self.image_list[index]}: {e}\")\n                index = random.randint(0, len(self.image_list) - 1)\n\n# 3D - 2D transfer\nclass HEMO_3D_CLS_Dataset_2Dtransfer(BaseDataset):\n    def __init__(self, mode=\"train\", roi_size=256):\n        self.root = '/workspace/1.Hemorrhage/SMART-Net/SMART-Net/datasets'\n        self.roi_size = roi_size\n        self.mode = mode\n        if mode == 'train':\n            self.image_list = list_sort_nicely(glob.glob('/workspace/1.Hemorrhage/SMART-Net/dataset/Train_nii/*_img.nii.gz'))  + list_sort_nicely(glob.glob('/workspace/1.Hemorrhage/SMART-Net/dataset/Test_nii/Coreline_1350/NIFTI_IMG/*.nii.gz')) + list_sort_nicely(glob.glob('/workspace/1.Hemorrhage/SMART-Net/dataset/Test_nii/Asan_internal/*_img.nii.gz'))\n            self.mask_list  = list_sort_nicely(glob.glob('/workspace/1.Hemorrhage/SMART-Net/dataset/Train_nii/*_mask.nii.gz')) + list_sort_nicely(glob.glob('/workspace/1.Hemorrhage/SMART-Net/dataset/Test_nii/Coreline_1350/NIFTI_SEGGT/*.nii.gz')) + list_sort_nicely(glob.glob('/workspace/1.Hemorrhage/SMART-Net/dataset/Test_nii/Asan_internal/*_mask.nii.gz'))\n        else:\n            self.image_list = list_sort_nicely(glob.glob('/workspace/1.Hemorrhage/SMART-Net/dataset/Valid_nii/*_img.nii.gz'))\n            self.mask_list  = list_sort_nicely(glob.glob('/workspace/1.Hemorrhage/SMART-Net/dataset/Valid_nii/*_mask.nii.gz'))\n        \n        self.transforms = get_transforms_3D_2Dtransfer(mode=mode, roi_size=roi_size)\n        \n        self.image_list = self.image_list\n        self.mask_list = self.mask_list\n    def __len__(self):\n        return len(self.image_list)\n\n    def __getitem__(self, index):\n        max_attempts = 100\n        for _ in range(max_attempts):\n            try:        \n                # image\n                image = sitk.ReadImage(self.image_list[index])\n                image = sitk.GetArrayFromImage(image)  # D, H, W\n\n                # mask\n                mask = sitk.ReadImage(self.mask_list[index])\n                mask = sitk.GetArrayFromImage(mask)  # D, H, W\n        \n                D, H, W = image.shape\n\n                # augmentation\n                augmented_images  = torch.empty((1, D, self.roi_size, self.roi_size), dtype=torch.float32)\n                first_image_slice = image[0, :, :]\n                sample = self.transforms(image=first_image_slice)\n                augmented_images[:, 0, :, :] = sample['image']\n                replay = sample['replay']\n                for d in range(1, D):\n                    slice_image = image[d, :, :]\n                    sample = A.ReplayCompose.replay(replay, image=slice_image)\n                    augmented_images[:, d, :, :] = sample[\"image\"]\n                \n                # BG=0, B = 1, M = 2\n                if mask.sum().item() > 0:\n                    label = torch.tensor([1.0])\n                else :\n                    label = torch.tensor([0.0])\n\n                # image [B, 1, 512, 512], label [B, 1]\n                return self.image_list[index], augmented_images.float(), label.float(), D\n\n            except Exception as e:\n                print(f\"Error in __getitem__ at path {self.image_list[index]}: {e}\")\n                index = random.randint(0, len(self.image_list) - 1)\n\nclass HEMO_3D_SEG_Dataset_2Dtransfer(BaseDataset):\n    def __init__(self, mode=\"train\", roi_size=256):\n        self.root = '/workspace/1.Hemorrhage/SMART-Net/SMART-Net/datasets'\n        self.roi_size = roi_size\n        self.mode = mode\n        if mode == 'train':\n            self.image_list = list_sort_nicely(glob.glob('/workspace/1.Hemorrhage/SMART-Net/dataset/Train_nii/*_img.nii.gz'))  + list_sort_nicely(glob.glob('/workspace/1.Hemorrhage/SMART-Net/dataset/Test_nii/Coreline_1350/NIFTI_IMG/*.nii.gz')) + list_sort_nicely(glob.glob('/workspace/1.Hemorrhage/SMART-Net/dataset/Test_nii/Asan_internal/*_img.nii.gz'))\n            self.mask_list  = list_sort_nicely(glob.glob('/workspace/1.Hemorrhage/SMART-Net/dataset/Train_nii/*_mask.nii.gz')) + list_sort_nicely(glob.glob('/workspace/1.Hemorrhage/SMART-Net/dataset/Test_nii/Coreline_1350/NIFTI_SEGGT/*.nii.gz')) + list_sort_nicely(glob.glob('/workspace/1.Hemorrhage/SMART-Net/dataset/Test_nii/Asan_internal/*_mask.nii.gz'))\n        else:\n            self.image_list = list_sort_nicely(glob.glob('/workspace/1.Hemorrhage/SMART-Net/dataset/Valid_nii/*_img.nii.gz'))\n            self.mask_list  = list_sort_nicely(glob.glob('/workspace/1.Hemorrhage/SMART-Net/dataset/Valid_nii/*_mask.nii.gz'))\n        \n        self.transforms = get_transforms_3D_2Dtransfer(mode=mode, roi_size=roi_size)\n        \n        self.image_list = self.image_list\n        self.mask_list = self.mask_list\n    def __len__(self):\n        return len(self.image_list)\n\n    def __getitem__(self, index):\n        max_attempts = 100\n        for _ in range(max_attempts):\n            try:        \n                # image\n                image = sitk.ReadImage(self.image_list[index])\n                image = sitk.GetArrayFromImage(image)  # D, H, W\n\n                # mask\n                mask = sitk.ReadImage(self.mask_list[index])\n                mask = sitk.GetArrayFromImage(mask)  # D, H, W\n        \n                D, H, W = image.shape\n\n                # augmentation\n                augmented_images  = torch.empty((1, D, self.roi_size, self.roi_size), dtype=torch.float32)\n                augmented_masks   = torch.empty((1, D, self.roi_size, self.roi_size), dtype=torch.float32)\n                first_image_slice = image[0, :, :]\n                first_mask_slice  = mask[0, :, :]\n                sample = self.transforms(image=first_image_slice, mask=first_mask_slice)\n                augmented_images[:, 0, :, :] = sample['image']\n                augmented_masks[:, 0, :, :]  = sample['mask']\n                replay = sample['replay']\n                for d in range(1, D):\n                    slice_image = image[d, :, :]\n                    slice_mask  = mask[d, :, :]\n                    sample = A.ReplayCompose.replay(replay, image=slice_image, mask=slice_mask)\n                    augmented_images[:, d, :, :] = sample[\"image\"]\n                    augmented_masks[:, d, :, :]  = sample[\"mask\"]\n\n                # image [B, 1, 512, 512], mask [B, 1, 512, 512]\n                return self.image_list[index], augmented_images.float(), augmented_masks.float(), D\n\n            except Exception as e:\n                print(f\"Error in __getitem__ at path {self.image_list[index]}: {e}\")\n                index = random.randint(0, len(self.image_list) - 1)\n\n\n# 3D - 331\nclass TDSC_REAL3D_MTL_CLS_SEG_REC_Dataset_CLS(BaseDataset):\n    def __init__(self, mode=\"train\"):\n        self.root = '/workspace/0.Challenge/MICCAI2023_TDSC/dataset'\n        self.mode = mode\n        if mode == 'train':\n            self.image_list = list_sort_nicely(glob.glob('/workspace/0.Challenge/MICCAI2023_TDSC/dataset/DATA/train/*.nrrd'))\n            self.mask_list  = list_sort_nicely(glob.glob('/workspace/0.Challenge/MICCAI2023_TDSC/dataset/MASK/train/*.nrrd'))\n        else:\n            self.image_list = list_sort_nicely(glob.glob('/workspace/0.Challenge/MICCAI2023_TDSC/dataset/DATA/valid/*.nrrd'))\n            self.mask_list  = list_sort_nicely(glob.glob('/workspace/0.Challenge/MICCAI2023_TDSC/dataset/MASK/valid/*.nrrd'))\n        \n        self.transforms = get_transforms_slice_3D(mode=mode)\n\n    def __len__(self):\n        return len(self.image_list)\n\n    def __getitem__(self, index):\n        label_name = self.label[self.label['data_path'] == self.image_list[index].split('/')[-1]]['label'].values[0]\n\n        # image\n        image = sitk.ReadImage(self.image_list[index])\n        image = sitk.GetArrayFromImage(image).transpose(1, 2, 0)  # H, W, D\n\n        # mask\n        mask  = sitk.ReadImage(self.mask_list[index])\n        mask  = sitk.GetArrayFromImage(mask).transpose(1, 2, 0)  # H, W, D\n\n        # Resize\n        H, W, D = image.shape\n\n        image = zoom(input=image, zoom=(0.5, 0.5, 0.5), order=1) # nearest\n        mask  = zoom(input=mask,  zoom=(0.5, 0.5, 0.5), order=0) # Bilinear\n\n        if self.mode == 'train':\n            # preprocessing\n            image = np.expand_dims(image, axis=0)  # C, H, W, D\n            mask  = np.expand_dims(mask,  axis=0)  # C, H, W, D\n            \n            sample = A.OneOf([\n                Crop_3D_Pos(height=256, width=256, depth=32, ignore_values=None, ignore_channels=None, always_apply=False, p=0.80),\n                Crop_3D_Neg(height=256, width=256, depth=32, ignore_values=None, ignore_channels=None, always_apply=False, p=0.20)\n                ], p=1.0)(image=image, mask=mask)\n\n            image = sample['image'].squeeze(0) # H, W, D\n            mask  = sample['mask'].squeeze(0)\n    \n            # augmentation\n            augmented_images = torch.empty((1, 256, 256, 32), dtype=torch.float32)\n            augmented_masks  = torch.empty((1, 256, 256, 32), dtype=torch.float32)\n            first_image_slice = image[:, :, 0]\n            first_mask_slice  = mask[:, :, 0]\n            sample = self.transforms(image=first_image_slice, mask=first_mask_slice, pad_loc=('pad_top', 'pad_bottom', 'pad_left', 'pad_right'))\n            augmented_images[:, :, :, 0] = sample['image']\n            augmented_masks[:, :, :, 0]  = sample['mask']\n            replay = sample['replay']\n            for d in range(1, 32):\n                slice_image = image[:, :, d]\n                slice_mask  = mask[:, :, d]\n                sample = A.ReplayCompose.replay(replay, image=slice_image, mask=slice_mask)\n                augmented_images[:, :, :, d] = sample[\"image\"]\n                augmented_masks[:, :, :, d]  = sample[\"mask\"]\n        \n        else:\n            # preprocessing\n            image = np.expand_dims(image, axis=0)  # C, H, W, D\n            mask  = np.expand_dims(mask,  axis=0)  # C, H, W, D\n            \n            # augmentation\n            augmented_images = torch.empty(image.shape, dtype=torch.float32)\n            augmented_masks  = torch.empty(mask.shape, dtype=torch.float32)\n            first_image_slice = image[0, :, :, 0]\n            first_mask_slice  = mask[0, :, :, 0]\n            sample = self.transforms(image=first_image_slice, mask=first_mask_slice, pad_loc=('pad_top', 'pad_bottom', 'pad_left', 'pad_right'))\n            augmented_images[:, :, :, 0] = sample['image']\n            augmented_masks[:, :, :, 0]  = sample['mask']\n            replay = sample['replay']\n            for d in range(1, image.shape[-1]):\n                slice_image = image[0, :, :, d]\n                slice_mask  = mask[0, :, :, d]\n                sample = A.ReplayCompose.replay(replay, image=slice_image, mask=slice_mask)\n                augmented_images[:, :, :, d] = sample[\"image\"]\n                augmented_masks[:, :, :, d]  = sample[\"mask\"]\n\n        # label\n        # BG = 0, B = 1, M = 2\n        if mask.sum().item() > 0:\n            if label_name == 'B':\n                label = torch.tensor(1).long()\n            else: \n                label = torch.tensor(2).long()\n        else :\n            label = torch.tensor(0).long()\n\n        return augmented_images.float(), augmented_masks.float(), label.long()\n\n\n# Define the DataLoader\ndef get_dataloader(name, mode, batch_size, num_workers, roi_size, operator_3d):\n    # 2D\n    if name == 'coreline_dataset':\n        if mode == 'train':            \n            train_dataset = HEMO_2D_MTL_Dataset(mode='train', roi_size=roi_size)\n            print(\"Train [Total] number = \", len(train_dataset))\n            data_loader   = DataLoader(train_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=True)\n    \n        elif mode == 'valid':\n            valid_dataset = HEMO_2D_MTL_Dataset(mode='valid', roi_size=roi_size)\n            print(\"Valid [Total] number = \", len(valid_dataset))\n            data_loader   = DataLoader(valid_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=False)\n\n    # 3D - 2D transfer\n    elif name == 'coreline_dataset-3d_cls-2d_transfer':\n        if mode == 'train':            \n            train_dataset = HEMO_3D_CLS_Dataset_2Dtransfer(mode='train', roi_size=roi_size)\n            print(\"Train [Total] number = \", len(train_dataset))\n            data_loader   = DataLoader(train_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=True, \n                                       collate_fn=functools.partial(CLS_pad_collate_fn_LSTM, roi_size=roi_size) if operator_3d=='lstm' else functools.partial(CLS_pad_collate_fn_TR, roi_size=roi_size))\n    \n        elif mode == 'valid':\n            valid_dataset = HEMO_3D_CLS_Dataset_2Dtransfer(mode='valid', roi_size=roi_size)\n            print(\"Valid [Total] number = \", len(valid_dataset))\n            data_loader   = DataLoader(valid_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=False, \n                                       collate_fn=functools.partial(CLS_pad_collate_fn_LSTM, roi_size=roi_size) if operator_3d=='lstm' else functools.partial(CLS_pad_collate_fn_TR, roi_size=roi_size))\n\n    elif name == 'coreline_dataset-3d_seg-2d_transfer':\n        if mode == 'train':            \n            train_dataset = HEMO_3D_SEG_Dataset_2Dtransfer(mode='train', roi_size=roi_size)\n            print(\"Train [Total] number = \", len(train_dataset))\n            data_loader   = DataLoader(train_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=True, \n                                       collate_fn=functools.partial(SEG_pad_collate_fn_PAD, roi_size=roi_size))\n    \n        elif mode == 'valid':\n            valid_dataset = HEMO_3D_SEG_Dataset_2Dtransfer(mode='valid', roi_size=roi_size)\n            print(\"Valid [Total] number = \", len(valid_dataset))\n            data_loader   = DataLoader(valid_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=False, \n                                       collate_fn=functools.partial(SEG_pad_collate_fn_PAD, roi_size=roi_size))\n            \n    return data_loader\n\n\n\n################# TEST ##################################\n# 2D\nclass TEST_HEMO_2D_MTL_Dataset(BaseDataset):\n    def __init__(self, roi_size=256):\n        self.root = '/workspace/1.Hemorrhage/SMART-Net/SMART-Net/datasets'\n        self.roi_size   = roi_size\n        self.image_list = list_sort_nicely(glob.glob('/workspace/1.Hemorrhage/SMART-Net/dataset/Valid_nii/*_img.nii.gz'))\n        self.mask_list  = list_sort_nicely(glob.glob('/workspace/1.Hemorrhage/SMART-Net/dataset/Valid_nii/*_mask.nii.gz'))\n        self.transforms = get_transforms_2D(mode='test', roi_size=roi_size)\n    \n    def __len__(self):\n        return len(self.image_list)\n\n    def __getitem__(self, index):\n        max_attempts = 100\n        for _ in range(max_attempts):\n            try:        \n                # image\n                image = sitk.ReadImage(self.image_list[index])\n                image = sitk.GetArrayFromImage(image)  # D, H, W\n\n                # mask\n                mask = sitk.ReadImage(self.mask_list[index])\n                mask = sitk.GetArrayFromImage(mask)  # D, H, W\n        \n                D, H, W = image.shape\n\n                # augmentation\n                augmented_images  = torch.empty((1, D, self.roi_size, self.roi_size), dtype=torch.float32)\n                augmented_masks   = torch.empty((1, D, self.roi_size, self.roi_size), dtype=torch.float32)\n                first_image_slice = image[0, :, :]\n                first_mask_slice  = mask[0, :, :]\n                sample = self.transforms(image=first_image_slice, mask=first_mask_slice)\n                augmented_images[:, 0, :, :] = sample['image']\n                augmented_masks[:, 0, :, :]  = sample['mask']\n                replay = sample['replay']\n                for d in range(1, D):\n                    slice_image = image[d, :, :]\n                    slice_mask  = mask[d, :, :]\n                    sample = A.ReplayCompose.replay(replay, image=slice_image, mask=slice_mask)\n                    augmented_images[:, d, :, :] = sample[\"image\"]\n                    augmented_masks[:, d, :, :]  = sample[\"mask\"]\n\n                # BG=0, B = 1, M = 2\n                label = augmented_masks.flatten(2).any(dim=2).float().squeeze(0)  # [D]\n\n                # image [B, 1, 512, 512], mask [B, 1, 512, 512]\n                return self.image_list[index], augmented_images.float(), label.float(), augmented_masks.float(), D\n\n            except Exception as e:\n                print(f\"Error in __getitem__ at path {self.image_list[index]}: {e}\")\n                index = random.randint(0, len(self.image_list) - 1)\n\nclass TEST_HEMO_2D_MTL_Dataset_Demo(BaseDataset):\n    def __init__(self, roi_size=256):\n        self.root = '/workspace/1.Hemorrhage/SMART-Net/datasets'\n        self.image_list = list_sort_nicely(glob.glob('/workspace/1.Hemorrhage/SMART-Net/dataset/Coreline_TEST_dcm/*/*'))\n        self.label_list = pd.read_csv('/workspace/1.Hemorrhage/SMART-Net/dataset/Coreline_1795_TEST_Label.csv')['Label'].values\n        self.transforms = get_transforms_2D(mode='test', roi_size=roi_size)\n        self.image_list = self.image_list\n        \n    def __len__(self):\n        return len(self.image_list)\n\n    def __getitem__(self, index):\n        max_attempts = 100\n        for _ in range(max_attempts):\n            try:\n                # image\n                image = extract_3D(self.image_list[index]).transpose(2, 0, 1) # D, H, W\n\n                D, H, W = image.shape\n\n                # augmentation\n                augmented_images  = torch.empty((1, D, self.roi_size, self.roi_size), dtype=torch.float32)\n                first_image_slice = image[0, :, :]\n                sample = self.transforms(image=first_image_slice)\n                augmented_images[:, 0, :, :] = sample['image']\n                replay = sample['replay']\n                for d in range(1, D):\n                    slice_image = image[d, :, :]\n                    sample = A.ReplayCompose.replay(replay, image=slice_image)\n                    augmented_images[:, d, :, :] = sample[\"image\"]\n\n                # label                \n                label = torch.tensor([self.label_list[index]])\n\n                # image [B, 1, 512, 512], label [B, 1]\n                return self.image_list[index], augmented_images.float(), label.float()\n\n            except Exception as e:\n                print(f\"Error in __getitem__ at path {self.image_list[index]}: {e}\")\n                index = random.randint(0, len(self.image_list) - 1)\n\n# 3D - 2D transfer\nclass TEST_HEMO_3D_CLS_Dataset_2Dtransfer(BaseDataset):\n    def __init__(self, roi_size=256):\n        self.root = '/workspace/1.Hemorrhage/SMART-Net/datasets'\n        self.roi_size   = roi_size\n        self.image_list = list_sort_nicely(glob.glob('/workspace/1.Hemorrhage/SMART-Net/dataset/Coreline_TEST_dcm/*/*'))\n        self.label_list = pd.read_csv('/workspace/1.Hemorrhage/SMART-Net/dataset/Coreline_1795_TEST_Label.csv')['Label'].values\n        self.transforms = get_transforms_3D_2Dtransfer(mode='test', roi_size=roi_size)\n\n    def __len__(self):\n        return len(self.image_list)\n\n    def __getitem__(self, index):\n        max_attempts = 100\n        for _ in range(max_attempts):\n            try:        \n                # image\n                image = extract_3D(self.image_list[index]).transpose(2, 0, 1) # D, H, W\n        \n                D, H, W = image.shape\n\n                # augmentation\n                augmented_images  = torch.empty((1, D, self.roi_size, self.roi_size), dtype=torch.float32)\n                first_image_slice = image[0, :, :]\n                sample = self.transforms(image=first_image_slice)\n                augmented_images[:, 0, :, :] = sample['image']\n                replay = sample['replay']\n                for d in range(1, D):\n                    slice_image = image[d, :, :]\n                    sample = A.ReplayCompose.replay(replay, image=slice_image)\n                    augmented_images[:, d, :, :] = sample[\"image\"]\n                \n                # label                \n                label = torch.tensor([self.label_list[index]])\n\n                # image [B, 1, 512, 512], label [B, 1]\n                return self.image_list[index], augmented_images.float(), label.float(), D\n\n            except Exception as e:\n                print(f\"Error in __getitem__ at path {self.image_list[index]}: {e}\")\n                index = random.randint(0, len(self.image_list) - 1)\n\nclass TEST_HEMO_3D_SEG_Dataset_2Dtransfer(BaseDataset):\n    def __init__(self, mode=\"train\", roi_size=256):\n        self.root = '/workspace/1.Hemorrhage/SMART-Net/SMART-Net/datasets'\n        self.roi_size = roi_size\n        self.mode = mode\n        if mode == 'train':\n            self.image_list = list_sort_nicely(glob.glob('/workspace/1.Hemorrhage/SMART-Net/dataset/Train_nii/*_img.nii.gz'))  + list_sort_nicely(glob.glob('/workspace/1.Hemorrhage/SMART-Net/dataset/Test_nii/Coreline_1350/NIFTI_IMG/*.nii.gz')) + list_sort_nicely(glob.glob('/workspace/1.Hemorrhage/SMART-Net/dataset/Test_nii/Asan_internal/*_img.nii.gz'))\n            self.mask_list  = list_sort_nicely(glob.glob('/workspace/1.Hemorrhage/SMART-Net/dataset/Train_nii/*_mask.nii.gz')) + list_sort_nicely(glob.glob('/workspace/1.Hemorrhage/SMART-Net/dataset/Test_nii/Coreline_1350/NIFTI_SEGGT/*.nii.gz')) + list_sort_nicely(glob.glob('/workspace/1.Hemorrhage/SMART-Net/dataset/Test_nii/Asan_internal/*_mask.nii.gz'))\n        else:\n            self.image_list = list_sort_nicely(glob.glob('/workspace/1.Hemorrhage/SMART-Net/dataset/Valid_nii/*_img.nii.gz'))\n            self.mask_list  = list_sort_nicely(glob.glob('/workspace/1.Hemorrhage/SMART-Net/dataset/Valid_nii/*_mask.nii.gz'))\n        \n        self.transforms = get_transforms_3D_2Dtransfer(mode=mode, roi_size=roi_size)\n        \n        self.image_list = self.image_list\n        self.mask_list = self.mask_list\n    def __len__(self):\n        return len(self.image_list)\n\n    def __getitem__(self, index):\n        max_attempts = 100\n        for _ in range(max_attempts):\n            try:        \n                # image\n                image = sitk.ReadImage(self.image_list[index])\n                image = sitk.GetArrayFromImage(image)  # D, H, W\n\n                # mask\n                mask = sitk.ReadImage(self.mask_list[index])\n                mask = sitk.GetArrayFromImage(mask)  # D, H, W\n        \n                D, H, W = image.shape\n\n                # augmentation\n                augmented_images  = torch.empty((1, D, self.roi_size, self.roi_size), dtype=torch.float32)\n                augmented_masks   = torch.empty((1, D, self.roi_size, self.roi_size), dtype=torch.float32)\n                first_image_slice = image[0, :, :]\n                first_mask_slice  = mask[0, :, :]\n                sample = self.transforms(image=first_image_slice, mask=first_mask_slice)\n                augmented_images[:, 0, :, :] = sample['image']\n                augmented_masks[:, 0, :, :]  = sample['mask']\n                replay = sample['replay']\n                for d in range(1, D):\n                    slice_image = image[d, :, :]\n                    slice_mask  = mask[d, :, :]\n                    sample = A.ReplayCompose.replay(replay, image=slice_image, mask=slice_mask)\n                    augmented_images[:, d, :, :] = sample[\"image\"]\n                    augmented_masks[:, d, :, :]  = sample[\"mask\"]\n\n                # image [B, 1, 512, 512], mask [B, 1, 512, 512]\n                return self.image_list[index], augmented_images.float(), augmented_masks.float(), D\n\n            except Exception as e:\n                print(f\"Error in __getitem__ at path {self.image_list[index]}: {e}\")\n                index = random.randint(0, len(self.image_list) - 1)\n\n\n\ndef get_dataloader_test(name, batch_size, num_workers, roi_size, operator_3d):\n    # 2D\n    if name == 'coreline_dataset_test':            \n        test_dataset = TEST_HEMO_2D_MTL_Dataset(roi_size=roi_size)\n        print(\"TEST [Total] number = \", len(test_dataset))\n        data_loader   = DataLoader(test_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=False, collate_fn=functools.partial(TEST_2D_pad_collate_fn, roi_size=roi_size))\n    # 3D - 2D transfer\n    elif name == 'coreline_dataset_test_3d_cls': \n        test_dataset = TEST_HEMO_3D_CLS_Dataset_2Dtransfer(roi_size=roi_size)\n        print(\"TEST [Total] number = \", len(test_dataset))\n        data_loader   = DataLoader(test_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=False, \n                                    collate_fn=functools.partial(CLS_pad_collate_fn_LSTM, roi_size=roi_size) if operator_3d=='lstm' else functools.partial(CLS_pad_collate_fn_TR, roi_size=roi_size))\n\n    elif name == 'coreline_dataset_test_3d_seg':\n        test_dataset = HEMO_3D_SEG_Dataset_2Dtransfer(mode='test', roi_size=roi_size)\n        print(\"TEST [Total] number = \", len(test_dataset))\n        data_loader   = DataLoader(test_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=False, \n                                    collate_fn=functools.partial(SEG_pad_collate_fn_PAD, roi_size=roi_size))\n        \n    return data_loader"}
{"type": "source_file", "path": "module/ACM_block.py", "content": "import torch\nimport torch.nn as nn\n\n\nclass ACM(nn.Module):\n    # def __init__(self, in_channels, num_heads=32, orthogonal_loss=True):\n    def __init__(self, in_channels, num_heads=8, orthogonal_loss=True):\n        super(ACM, self).__init__()\n\n        assert in_channels % num_heads == 0\n\n        self.in_channels = in_channels\n        self.num_heads = num_heads\n\n        self.add_mod = AttendModule(self.in_channels, num_heads=num_heads)\n        self.sub_mod = AttendModule(self.in_channels, num_heads=num_heads)\n        self.mul_mod = ModulateModule(channel=self.in_channels, num_groups=num_heads, compressions=2)\n\n        self.orthogonal_loss = orthogonal_loss\n\n        self.init_parameters()\n\n    def init_parameters(self):\n        if self.add_mod is not None:\n            self.add_mod.init_parameters()\n        if self.sub_mod is not None:\n            self.sub_mod.init_parameters()\n        if self.mul_mod is not None:\n            self.mul_mod.init_parameters()\n\n    def forward(self, x):\n\n        mu = x.mean([2, 3], keepdim=True)\n        x_mu = x - mu\n\n        # creates multipying feature\n        mul_feature = self.mul_mod(mu)  # P\n\n        # creates add or sub feature\n        add_feature = self.add_mod(x_mu)  # K\n\n        # creates add or sub feature\n        sub_feature = self.sub_mod(x_mu)  # Q\n\n        y = (x + add_feature - sub_feature) * mul_feature\n\n        if self.orthogonal_loss:\n            dp = torch.mean(add_feature * sub_feature, dim=1, keepdim=True)\n            return y, dp\n        else:\n            return y\n\n\nclass AttendModule(nn.Module):\n\n    def __init__(self, in_channels, num_heads=4):\n        super(AttendModule, self).__init__()\n\n        self.num_heads = int(num_heads)\n        self.in_channels = in_channels\n        self.num_c_per_head = self.in_channels // self.num_heads\n        assert self.in_channels % self.num_heads == 0\n\n        self.map_gen = nn.Sequential(\n            nn.Conv2d(in_channels, num_heads, kernel_size=1, stride=1, padding=0, bias=True, groups=num_heads)\n        )\n\n        self.normalize = nn.Softmax(dim=2)\n        self.return_weight = False\n\n    def init_parameters(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n                nn.init.constant_(m.bias, 0.0)\n\n    def batch_weighted_avg(self, xhats, weights):\n\n        b, c, h, w = xhats.shape\n        # xhat reshape\n        xhats_reshape = xhats.view(b * self.num_heads, self.num_c_per_head, h, w)\n        xhats_reshape = xhats_reshape.view(b * self.num_heads, self.num_c_per_head, h * w)\n\n        # weight reshape\n        weights_reshape = weights.view(b * self.num_heads, 1, h, w)\n        weights_reshape = weights_reshape.view(b * self.num_heads, 1, h * w)\n\n        weights_normalized = self.normalize(weights_reshape)\n        weights_normalized = weights_normalized.transpose(1, 2)\n\n        mus = torch.bmm(xhats_reshape, weights_normalized)\n        mus = mus.view(b, self.num_heads * self.num_c_per_head, 1, 1)\n\n        return mus, weights_normalized\n\n    def forward(self, x):\n\n        b, c, h, w = x.shape\n\n        weights = self.map_gen(x)\n\n        mus, weights_normalized = self.batch_weighted_avg(x, weights)\n\n        if self.return_weight:\n            weights_normalized = weights_normalized.view(b, self.num_heads, h * w, 1)\n            weights_normalized = weights_normalized.squeeze(-1)\n\n            weights_normalized = weights_normalized.view(b, self.num_heads, h, w)\n            weights_splitted = torch.split(weights_normalized, 1, 1)\n            return mus, weights_splitted\n\n        return mus\n\n\nclass ModulateModule(nn.Module):\n\n    def __init__(self, channel, num_groups=32, compressions=2):\n        super(ModulateModule, self).__init__()\n        self.feature_gen = nn.Sequential(\n            nn.Conv2d(channel, channel//compressions, kernel_size=1, stride=1, padding=0, bias=True, groups=num_groups),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(channel//compressions, channel, kernel_size=1, stride=1, padding=0, bias=True, groups=num_groups),\n            nn.Sigmoid()\n        )\n\n    def init_parameters(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n                nn.init.constant_(m.bias, 0.0)\n\n    def forward(self, x):\n        y = self.feature_gen(x)\n        return y\n\n\nif __name__ == '__main__':\n\n    x1 = torch.randn(256 * 20 * 20 * 5).view(5, 256, 20, 20).float()\n    acm = ACM(num_heads=32, in_channels=256, orthogonal_loss=True)\n    acm.init_parameters()\n    y, dp = acm(x1)\n    print(y.shape)\n    print(dp.shape)\n\n    # ACM without orthogonal loss\n    acm = ACM(num_heads=32, in_channels=256, orthogonal_loss=False)\n    acm.init_parameters()\n    y = acm(x1)\n    print(y.shape)\n"}
{"type": "source_file", "path": "metrics.py", "content": "from sklearn.metrics import accuracy_score, recall_score, f1_score\nimport numpy as np\nimport torch\n\n\ndef accuracy(y_true, y_pred):\n    return accuracy_score(y_true, y_pred)\n\ndef sensitivity(y_true, y_pred):\n    return recall_score(y_true, y_pred)\n\ndef f1_metric(y_true, y_pred):\n    return f1_score(y_true, y_pred)\n\ndef specificity(y_true, y_pred):\n    true_negative = np.sum((y_true == 0) & (y_pred == 0))\n    false_positive = np.sum((y_true == 0) & (y_pred == 1))\n    return true_negative / (true_negative + false_positive)\n\ndef binary_dice_score(y_true, y_pred, smooth=0.0, eps=1e-7):\n    bs = y_true.size(0)\n    y_true = y_true.view(bs, 1, -1)\n    y_pred = y_pred.view(bs, 1, -1)\n\n    intersection = torch.sum(y_true * y_pred, dim=(0, 2))\n    cardinality  = torch.sum(y_true + y_pred, dim=(0, 2))\n\n    dice_score = (2. * intersection + smooth) / (cardinality + smooth).clamp_min(eps)\n    return dice_score.mean()\n\ndef fp_rate_score(y_true, y_pred):\n    false_positive = ((y_true == 0) & (y_pred == 1)).sum()\n    true_negative  = ((y_true == 0) & (y_pred == 0)).sum()\n    fp_count       = false_positive.float()\n    max_fp_count   = (false_positive + true_negative).float()\n    numerator      = torch.log1p(fp_count)           # log(1 + fp_count)\n    denominator    = torch.log1p(max_fp_count)       # log(1 + max_fp_count)\n    log_scaled_fp  = numerator / denominator\n    return (1 - log_scaled_fp).mean()\n\n\ndef binary_dice_score_np(y_true, y_pred, smooth=0.0, eps=1e-7):\n    bs = y_true.shape[0]\n    y_true = y_true.reshape(bs, 1, -1)\n    y_pred = y_pred.reshape(bs, 1, -1)\n\n    intersection = np.sum(y_true * y_pred, axis=(0, 2))\n    cardinality  = np.sum(y_true + y_pred, axis=(0, 2))\n\n    dice_score = (2. * intersection + smooth) / np.clip(cardinality + smooth, eps, None)\n    return dice_score.mean()\n\ndef fp_rate_score_np(y_true, y_pred):\n    false_positive = np.sum((y_true == 0) & (y_pred == 1))\n    true_negative  = np.sum((y_true == 0) & (y_pred == 0))\n    \n    fp_count = false_positive.astype(np.float32)\n    max_fp_count = (false_positive + true_negative).astype(np.float32)\n    \n    numerator = np.log1p(fp_count)           # log(1 + fp_count)\n    denominator = np.log1p(max_fp_count)     # log(1 + max_fp_count)\n    \n    log_scaled_fp = numerator / denominator\n    return (1 - log_scaled_fp).mean()"}
{"type": "source_file", "path": "inference.py", "content": "import os\nfrom pathlib import Path\nimport argparse\nimport datetime\nimport numpy as np\nimport time\nimport torch\nimport json\nimport random\n# import functools\n\nimport utils\nfrom create_model import create_model\nfrom create_datasets.prepare_datasets import build_test_dataset\nfrom engine import *\nfrom losses import Uptask_Loss, Downtask_Loss\n\n\ndef str2bool(v):\n    if v.lower() in ('yes', 'true', 't', 'y', '1'):\n        return True\n    elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n        return False\n    else:\n        raise argparse.ArgumentTypeError('Boolean value expected.')\n\ndef get_args_parser():\n    parser = argparse.ArgumentParser('SMART-Net Framework Train and Test script', add_help=False)\n\n    # Dataset parameters\n    parser.add_argument('--data-folder-dir', default=\"/workspace/sunggu/1.Hemorrhage/SMART-Net/datasets/samples\", type=str, help='dataset folder dirname')    \n    parser.add_argument('--test-dataset-name', default=\"Custom\", type=str, help='test dataset name')    \n    parser.add_argument('--slice-wise-manner', type=str2bool, default=\"True\", help='stacking slices like slice-wise manner for patient-level testing for Upstream')\n    \n    # Model parameters\n    parser.add_argument('--model-name', default='SMART_Net', type=str, help='model name')\n    parser.add_argument('--backbone', default='efficientnet-b7', type=str, help='backbone name')\n    \n    # DataLoader setting\n    parser.add_argument('--num-workers', default=10, type=int)\n    parser.add_argument('--pin-mem',    action='store_true', default=False, help='Pin CPU memory in DataLoader for more efficient (sometimes) transfer to GPU.')\n    \n    # Setting Upstream, Downstream task\n    parser.add_argument('--training-stream', default='Upstream', choices=['Upstream', 'Downstream'], type=str, help='training stream')  \n\n    # DataParrel or Single GPU train\n    parser.add_argument('--multi-gpu-mode',       default='DataParallel', choices=['DataParallel', 'Single'], type=str, help='multi-gpu-mode')          \n    parser.add_argument('--device',               default='cuda', help='device to use for training / testing')\n    parser.add_argument('--cuda-device-order',    default='PCI_BUS_ID', type=str, help='cuda_device_order')\n    parser.add_argument('--cuda-visible-devices', default='0', type=str, help='cuda_visible_devices')\n\n    # Continue Training\n    parser.add_argument('--resume', default='',  help='resume from checkpoint')  # '' = None\n\n    # Validation setting\n    parser.add_argument('--print-freq', default=10, type=int, metavar='N', help='print frequency (default: 10)')\n\n    # Prediction and Save setting\n    parser.add_argument('--output-dir', default='', help='path where to save, empty for no saving')\n\n    return parser\n\n\n# Fix random seeds for reproducibility\nrandom_seed = 42\ntorch.manual_seed(random_seed)\ntorch.cuda.manual_seed(random_seed)\ntorch.cuda.manual_seed_all(random_seed) \ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\nnp.random.seed(random_seed)\nrandom.seed(random_seed)\n\ndef main(args):\n           \n    utils.print_args_test(args)\n    device = torch.device(args.device)\n\n    print(\"Loading dataset ....\")\n    dataset_test, collate_fn_test = build_test_dataset(args=args)\n    \n    data_loader_test = torch.utils.data.DataLoader(dataset_test, batch_size=1, num_workers=args.num_workers, shuffle=False, pin_memory=args.pin_mem, drop_last=False, collate_fn=collate_fn_test)\n\n\n    # Select Loss\n    if args.training_stream == 'Upstream':\n        criterion = Uptask_Loss(name=args.model_name)\n    else :\n        criterion = Downtask_Loss(name=args.model_name)\n\n\n    # Select Model\n    print(f\"Creating model  : {args.model_name}\")\n    model = create_model(stream=args.training_stream, name=args.model_name, backbone=args.backbone)\n    print(model)\n\n\n    # Resume\n    if args.resume:\n        print(\"Loading... Resume\")\n        checkpoint = torch.load(args.resume, map_location='cpu')\n        model.load_state_dict(checkpoint['model_state_dict'])        \n\n        try:\n            log_path = os.path.dirname(args.resume)+'/log.txt'\n            lines    = open(log_path,'r').readlines()\n            val_loss_list = []\n            for l in lines:\n                exec('log_dict='+l.replace('NaN', '0'))\n                val_loss_list.append(log_dict['valid_loss'])\n            print(\"Epoch: \", np.argmin(val_loss_list), \" Minimum Val Loss ==> \", np.min(val_loss_list))\n        except:\n            pass\n\n\n\n    # Multi GPU\n    if args.multi_gpu_mode == 'DataParallel':\n        model = torch.nn.DataParallel(model)\n        model.to(device)\n    elif args.multi_gpu_mode == 'Single':\n        model.to(device)\n    else :\n        raise Exception('Error...! args.multi_gpu_mode')    \n\n\n    start_time = time.time()\n\n\n    # TEST\n    if args.training_stream == 'Upstream':\n\n        if args.model_name == 'Up_SMART_Net':\n            if args.slice_wise_manner:\n                test_stats = test_Up_SMART_Net(model, criterion, data_loader_test, device, args.print_freq, 1)\n            else :\n                test_stats = test_Up_SMART_Net_Patient_Level(model, criterion, data_loader_test, device, args.print_freq, 1)\n\n        else : \n            raise KeyError(\"Wrong model name `{}`\".format(args.model_name))     \n\n    elif args.training_stream == 'Downstream':\n\n        if args.model_name == 'Down_SMART_Net_CLS':\n            infer_Down_SMART_Net_CLS(model, data_loader_test, device, args.print_freq, args.output_dir)\n        elif args.model_name == 'Down_SMART_Net_SEG':\n            infer_Down_SMART_Net_SEG(model, data_loader_test, device, args.print_freq, args.output_dir)\n        else :\n            raise KeyError(\"Wrong model name `{}`\".format(args.model_name))     \n    \n    else :\n        raise KeyError(\"Wrong training stream `{}`\".format(args.training_stream))        \n\n\n    # Finish\n    total_time = time.time() - start_time\n    total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n    print('TEST time {}'.format(total_time_str))\n\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser('SMART-Net Framework training and evaluation script', parents=[get_args_parser()])\n    args = parser.parse_args()\n\n    if args.output_dir:\n        Path(args.output_dir).mkdir(parents=True, exist_ok=True)\n        \n    os.environ[\"CUDA_DEVICE_ORDER\"]     =  args.cuda_device_order\n    os.environ[\"CUDA_VISIBLE_DEVICES\"]  =  args.cuda_visible_devices        \n    \n    main(args)\n"}
{"type": "source_file", "path": "models.py", "content": "import torch \nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom arch.our_network import *\n\ndef get_model(args):\n    \n    # 2D: encoder with MTL\n    if args.model == 'SMART-Net-2D':\n        model = SMART_Net_2D(backbone=args.backbone, use_skip=args.use_skip, pool_type=args.pool_type, use_consist=args.use_consist, roi_size=args.roi_size)\n    \n    # 3D-CLS: 3D operator w/ 2D encoder\n    elif args.model == 'SMART-Net-3D-CLS':\n        model = SMART_Net_3D_CLS(transfer_pretrained=args.transfer_pretrained, \n                                 use_pretrained_encoder=args.use_pretrained_encoder,\n                                 freeze_encoder=args.freeze_encoder, \n                                 roi_size=args.roi_size, \n                                 sw_batch_size=args.sw_batch_size, \n                                 spatial_dim=0, \n                                 backbone=args.backbone, \n                                 use_skip=args.use_skip, \n                                 pool_type=args.pool_type, \n                                 operator_3d=args.operator_3d)\n\n    # 3D-SEG: 3D operator w/ 2D encoder\n    elif args.model == 'SMART-Net-3D-SEG':\n        model = SMART_Net_3D_SEG(transfer_pretrained=args.transfer_pretrained, \n                                 use_pretrained_encoder=args.use_pretrained_encoder,\n                                 use_pretrained_decoder=args.use_pretrained_decoder,\n                                 freeze_encoder=args.freeze_encoder,\n                                 freeze_decoder=args.freeze_decoder,\n                                 roi_size=args.roi_size, \n                                 sw_batch_size=args.sw_batch_size, \n                                 spatial_dim=0, \n                                 backbone=args.backbone, \n                                 use_skip=args.use_skip, \n                                 pool_type=args.pool_type, \n                                 operator_3d=args.operator_3d)\n        \n\n    # print number of learnable parameters\n    n_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    print('Number of Learnable Params:', n_parameters)   \n\n    return model\n"}
{"type": "source_file", "path": "engine.py", "content": "import os\nimport math\nimport utils\nimport numpy as np\nimport torch\nimport cv2\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nfrom metrics import f1_metric, accuracy, sensitivity, specificity, binary_dice_score, fp_rate_score, binary_dice_score_np, fp_rate_score_np\nfrom sklearn.metrics import roc_auc_score\n# from monai.utils import ImageMetaKey as Key\n# from monai.inferers import sliding_window_inference\nfrom accelerate.utils import gather_object\nfrom utils import check_2d_data_device, check_3d_cls_data_device, check_3d_seg_data_device, collect_dict_all_processor, save_image_and_prediction\nfrom tqdm import tqdm\n\n\n# Setting...!\nfn_denorm  = lambda x: (x * 0.5) + 0.5\nfn_tonumpy = lambda x: x.detach().cpu().numpy()\n\nactivation = {}\ndef get_activation(name):\n    def hook(model, input, output):\n        activation[name] = output.detach()\n    return hook\n\nfeatures   = {}\ndef get_features(name):\n    def hook(model, input, output):\n        features[name] = output.detach()\n    return hook\n\ndef freeze_params(model: torch.nn.Module):\n    \"\"\"Set requires_grad=False for each of model.parameters()\"\"\"\n    for par in model.parameters():\n        par.requires_grad = False\n\ndef unfreeze_params(model: torch.nn.Module):\n    \"\"\"Set requires_grad=True for each of model.parameters()\"\"\"\n    for par in model.parameters():\n        par.requires_grad = True\n\ndef freeze_or_unfreeze_block(model, layer_name, stage_idxs=None, freeze=True):\n    \"\"\"\n    Helper function to freeze or unfreeze blocks based on the model type and the specific stage indices.\n    \"\"\"\n    if hasattr(model, 'module'):\n        encoder = model.module.encoder\n    else:\n        encoder = model.encoder\n    \n    if stage_idxs:\n        blocks = encoder._blocks[stage_idxs[0]:stage_idxs[1]]\n    else:\n        blocks = getattr(encoder, layer_name)\n\n    if freeze:\n        freeze_params(blocks)\n        print(f\"Freeze {layer_name} ...!\")\n    else:\n        unfreeze_params(blocks)\n        print(f\"Unfreeze {layer_name} ...!\")\n\ndef handle_efficientnet_freezing(model, epoch):\n    \"\"\"\n    EfficientNet freezing/unfreezing logic based on the current epoch.\n    \"\"\"\n    if epoch <= 100:\n        freeze_or_unfreeze_block(model, 'encoder', freeze=True)\n    elif 101 <= epoch < 111:\n        freeze_or_unfreeze_block(model, 'encoder', freeze=True)\n        freeze_or_unfreeze_block(model, 'blocks_stage_3', stage_idxs=[model.encoder._stage_idxs[2], None], freeze=False)\n    elif 111 <= epoch < 121:\n        freeze_or_unfreeze_block(model, 'encoder', freeze=True)\n        freeze_or_unfreeze_block(model, 'blocks_stage_2', stage_idxs=[model.encoder._stage_idxs[1], model.encoder._stage_idxs[2]], freeze=False)\n    elif 121 <= epoch < 131:\n        freeze_or_unfreeze_block(model, 'encoder', freeze=True)\n        freeze_or_unfreeze_block(model, 'blocks_stage_1', stage_idxs=[model.encoder._stage_idxs[0], model.encoder._stage_idxs[1]], freeze=False)\n    elif 131 <= epoch < 141:\n        freeze_or_unfreeze_block(model, 'encoder', freeze=True)\n        freeze_or_unfreeze_block(model, 'blocks_stage_0', stage_idxs=[None, model.encoder._stage_idxs[0]], freeze=False)\n    else:\n        freeze_or_unfreeze_block(model, 'encoder', freeze=False)\n\ndef handle_resnet_freezing(model, epoch):\n    \"\"\"\n    ResNet freezing/unfreezing logic based on the current epoch.\n    \"\"\"\n    if epoch <= 100:\n        freeze_or_unfreeze_block(model, 'encoder', freeze=True)\n    elif 101 <= epoch < 111:\n        freeze_or_unfreeze_block(model, 'layer4', freeze=False)\n    elif 111 <= epoch < 121:\n        freeze_or_unfreeze_block(model, 'layer3', freeze=False)\n    elif 121 <= epoch < 131:\n        freeze_or_unfreeze_block(model, 'layer2', freeze=False)\n    elif 131 <= epoch < 141:\n        freeze_or_unfreeze_block(model, 'layer1', freeze=False)\n    else:\n        freeze_or_unfreeze_block(model, 'encoder', freeze=False)\n\ndef apply_gradual_unfreezing(model, epoch, gradual_unfreeze=True, model_type='efficientnet'):\n    \"\"\"\n    Function to apply gradual unfreezing based on the model type and epoch.\n    \"\"\"\n    if gradual_unfreeze:\n        if model_type == 'efficientnet':\n            handle_efficientnet_freezing(model, epoch)\n        elif model_type == 'resnet':\n            handle_resnet_freezing(model, epoch)\n    else:\n        print(\"Freeze encoder ...!\")\n        freeze_or_unfreeze_block(model, 'encoder', freeze=True)\n\ndef activation_map(x):\n    # print(\"mean 0 == \", x.shape)                                # x = (B, 2048, 16, 16, D)\n    mean = torch.mean(x, dim=1, keepdim=True)                     # x = (B, 1, H, W ,D)\n    mean = torch.sigmoid(mean).squeeze().detach().cpu().numpy()   # x = (H, W, D)\n    mean = np.stack([ cv2.resize(mean[..., i], (256, 256), interpolation=cv2.INTER_CUBIC) for i in range(mean.shape[-1]) ], axis=-1)\n    mean -= mean.min()\n    mean /= mean.max()\n    return torch.tensor(mean).unsqueeze(0)\n\n########################################################\n# Uptask Task\ndef train_smartnet_2d(train_loader, model, criterion, optimizer, device, epoch, use_consist, multi_gpu, accelerator):\n    model.train()\n    metric_logger  = utils.AverageMeter()\n    epoch_iterator = tqdm(train_loader, desc=\"Training (X / X Steps) (loss=X.X)\", dynamic_ncols=True, total=len(train_loader), disable=(not accelerator.is_local_main_process))\n    \n    for step, batch_data in enumerate(epoch_iterator):\n        \n        path, image, label, mask = batch_data\n        image, label, mask = check_2d_data_device(multi_gpu, device, image, label, mask)\n\n        with accelerator.autocast():\n            if use_consist:\n                pred_cls, pred_seg, pred_rec, pooled_seg = model(image) # pred.shape [20, 1]\n                pred_cls = pred_cls.sigmoid()\n                pred_seg = pred_seg.sigmoid()\n                loss, loss_dict = criterion(pred_cls=pred_cls, pred_seg=pred_seg, pred_rec=pred_rec, label=label, mask=mask, image=image, pooled_seg=pooled_seg)\n            else:\n                pred_cls, pred_seg, pred_rec = model(image)\n                pred_cls = pred_cls.sigmoid()\n                pred_seg = pred_seg.sigmoid()\n                loss, loss_dict = criterion(pred_cls=pred_cls, pred_seg=pred_seg, pred_rec=pred_rec, label=label, mask=mask, image=image)\n\n        loss_value = loss.item()\n        optimizer.zero_grad()\n        if multi_gpu != 'DDP':\n            loss.backward()\n        else:\n            accelerator.backward(loss)\n        optimizer.step()\n\n        if not math.isfinite(loss_value):\n            print(\"Loss is {}, stopping training\".format(loss_value))\n        \n        all_loss_dict  = gather_object([loss_dict])\n        all_loss_dict  = collect_dict_all_processor(all_loss_dict, average=True)\n\n        if accelerator.is_main_process:\n            metric_logger.update(key='train_loss', value=loss_dict['total_loss'], n=image.shape[0]*accelerator.num_processes)\n            for key in loss_dict:\n                if key.startswith('cls_'):\n                    metric_logger.update(key='train_'+key, value=loss_dict[key], n=image.shape[0]*accelerator.num_processes)\n                elif key.startswith('seg_'):\n                    metric_logger.update(key='train_'+key, value=loss_dict[key], n=image.shape[0]*accelerator.num_processes)\n                elif key.startswith('rec_'):\n                    metric_logger.update(key='train_'+key, value=loss_dict[key], n=image.shape[0]*accelerator.num_processes)\n\n        epoch_iterator.set_description(\"Training: Epochs %d (%d / %d Steps), (train_loss=%2.5f)\" % (epoch, step, len(train_loader), loss_value))\n\n    return {k: round(v, 7) for k, v in metric_logger.average().items()}\n\n@torch.no_grad()\ndef valid_smartnet_2d(valid_loader, model, device, epoch, save_dir, use_consist, multi_gpu, accelerator):\n    model.eval()\n    metric_logger  = utils.AverageMeter()\n    epoch_iterator = tqdm(valid_loader, desc=\"Validating (X / X Steps) (loss=X.X)\", dynamic_ncols=True, total=len(valid_loader), disable=(not accelerator.is_local_main_process))\n    path_list      = []\n    pred_prob_list = []\n    gt_binary_list = []\n    dice_list      = []\n    fp_list        = []\n    mae_list       = []\n\n    for step, batch_data in enumerate(epoch_iterator):\n        \n        path, image, label, mask = batch_data\n        image, label, mask = check_2d_data_device(multi_gpu, device, image, label, mask)\n\n        # model.encoder.layer4.register_forward_hook(get_activation('Activation Map')) # for Activation Map\n        # act_list.append(Activation_Map(activation['Activation Map']))\n        \n        with accelerator.autocast():\n            if use_consist:\n                pred_cls, pred_seg, pred_rec, pooled_seg = model(image)\n            else:\n                pred_cls, pred_seg, pred_rec = model(image)\n\n        pred_cls = pred_cls.sigmoid()\n        pred_seg = pred_seg.sigmoid()\n\n        epoch_iterator.set_description(\"Validating: Epochs %d (%d / %d Steps)\" % (epoch, step, len(valid_loader)))\n        \n        path_list.extend(path)\n        pred_prob_list.append(fn_tonumpy(pred_cls))\n        gt_binary_list.append(fn_tonumpy(label))\n\n        # Metrics SEG\n        non_zero_mask    = mask.view(mask.shape[0], -1).any(dim=1)\n        indices_non_zero = torch.where(non_zero_mask)[0]\n        indices_zero     = torch.where(~non_zero_mask)[0]\n        mask_non_zero, mask_zero         = mask[indices_non_zero], mask[indices_zero]\n        pred_seg_non_zero, pred_seg_zero = pred_seg[indices_non_zero], pred_seg[indices_zero]\n        dice_score = binary_dice_score(pred_seg_non_zero.round(), mask_non_zero).item() if indices_non_zero.numel() > 0 else -1.0\n        fp_score   = fp_rate_score(pred_seg_zero.round(), mask_zero).item() if indices_zero.numel() > 0 else -1.0\n        dice_list.append(dice_score)\n        fp_list.append(fp_score)\n\n        # Metrics REC\n        mae = torch.nn.functional.l1_loss(input=pred_rec, target=image).item()\n        mae_list.append(mae)\n\n    # Summary\n    pred_prob_list = gather_object(pred_prob_list)\n    gt_binary_list = gather_object(gt_binary_list)\n    all_dice_score = gather_object(dice_list)\n    all_fp_score   = gather_object(fp_list)\n    all_mae        = gather_object(mae_list)\n\n    if accelerator.is_main_process:\n        # Metric CLS\n        pred_prob_list  = np.concatenate(pred_prob_list, axis=0).squeeze() # (B,)\n        gt_binary_list  = np.concatenate(gt_binary_list, axis=0).squeeze() # (B,)        \n        auc  = roc_auc_score(y_true=gt_binary_list, y_score=pred_prob_list)\n        f1   = f1_metric(y_true=gt_binary_list, y_pred=pred_prob_list.round())\n        acc  = accuracy(y_true=gt_binary_list, y_pred=pred_prob_list.round())\n        sen  = sensitivity(y_true=gt_binary_list, y_pred=pred_prob_list.round())\n        spe  = specificity(y_true=gt_binary_list, y_pred=pred_prob_list.round())\n        \n        metric_logger.update(key='valid_cls_auc', value=auc, n=1)\n        metric_logger.update(key='valid_cls_f1',  value=f1,  n=1)\n        metric_logger.update(key='valid_cls_acc', value=acc, n=1)\n        metric_logger.update(key='valid_cls_sen', value=sen, n=1)\n        metric_logger.update(key='valid_cls_spe', value=spe, n=1)\n\n        # Metrics SEG & REC\n        all_dice_score   = np.array(all_dice_score)\n        all_fp_score     = np.array(all_fp_score)        \n        valid_dice_score = all_dice_score[all_dice_score != -1]\n        valid_fp_score   = all_fp_score[all_fp_score != -1]\n\n        metric_logger.update(key='valid_seg_dice', value=np.mean(valid_dice_score), n=1)\n        metric_logger.update(key='valid_seg_fp',   value=np.mean(valid_fp_score),   n=1)\n        metric_logger.update(key='valid_rec_mae',  value=np.mean(all_mae),          n=1)\n        \n        # SAVE PNG\n        if indices_non_zero.numel() > 0:\n            save_image_and_prediction(image, mask, pred_seg, pred_rec, save_dir, epoch, index=indices_non_zero[0])\n        else:\n            save_image_and_prediction(image, mask, pred_seg, pred_rec, save_dir, epoch)\n\n    return {k: round(v, 7) for k, v in metric_logger.average().items()}\n\n@torch.no_grad()\ndef test_smartnet_2d(test_loader, model, device, epoch, save_dir, use_consist):\n    model.eval()\n    metric_logger  = utils.AverageMeter()\n    epoch_iterator = tqdm(test_loader, desc=\"Testing (X / X Steps) (loss=X.X)\", dynamic_ncols=True, total=len(test_loader))\n    dice_list      = []\n    fp_list        = []\n    pred_prob_list = []\n    gt_binary_list = []\n\n    for step, batch_data in enumerate(epoch_iterator):\n        \n        path, image, label, mask, depth = batch_data\n        image = image.to(device).float()\n        label = label.to(device).float()\n        mask  = mask.to(device).float()        \n\n        # model.encoder.layer4.register_forward_hook(get_activation('Activation Map')) # for Activation Map\n        # act_list.append(Activation_Map(activation['Activation Map']))        \n        \n        B, C, D, H, W = image.shape\n        # print(\"label == \", label.shape) # label ==  torch.Size([10, 64])\n\n        image = image.permute(2, 0, 1, 3, 4)      # (B, C, D, H, W) --> (D, B, C, H, W)\n        mask  = mask.permute(2, 0, 1, 3, 4)       # (B, C, D, H, W) --> (D, B, C, H, W)\n\n        image = torch.reshape(image, [D*B, C, H, W])      # (D, B, C, H, W) --> (D*B, C, H, W)\n        mask  = torch.reshape(mask, [D*B, C, H, W])       # (D, B, C, H, W) --> (D*B, C, H, W)\n        label = torch.reshape(label, [D*B, 1])            # (D, B, 1) --> (D*B, 1)\n\n        for i in range(image.shape[0]):\n            target_image = image[i].unsqueeze(0)\n            target_mask  = mask[i].unsqueeze(0)\n            target_label = label[i].unsqueeze(0)\n\n            if use_consist:\n                pred_cls, pred_seg, pred_rec, pooled_seg = model(target_image)\n            else:\n                pred_cls, pred_seg, pred_rec = model(target_image)\n\n            pred_seg = pred_seg.sigmoid()\n            pred_cls = pred_cls.sigmoid()\n\n            epoch_iterator.set_description(\"Testing: Epochs %d (%d / %d Steps)\" % (epoch, step, len(test_loader)))\n\n            pred_prob_list.append(fn_tonumpy(pred_cls))\n            gt_binary_list.append(fn_tonumpy(target_label))\n\n            # Metrics SEG\n            non_zero_mask    = target_mask.view(target_mask.shape[0], -1).any(dim=1)\n            indices_non_zero = torch.where(non_zero_mask)[0]\n            indices_zero     = torch.where(~non_zero_mask)[0]\n            mask_non_zero, mask_zero         = target_mask[indices_non_zero], target_mask[indices_zero]\n            pred_seg_non_zero, pred_seg_zero = pred_seg[indices_non_zero], pred_seg[indices_zero]\n            dice_score = binary_dice_score(pred_seg_non_zero.round(), mask_non_zero).item() if indices_non_zero.numel() > 0 else -1.0\n            fp_score   = fp_rate_score(pred_seg_zero.round(), mask_zero).item() if indices_zero.numel() > 0 else -1.0\n            dice_list.append(dice_score)\n            fp_list.append(fp_score)\n\n    # Metric CLS\n    pred_prob_list  = np.concatenate(pred_prob_list, axis=0).squeeze() # (B,)\n    gt_binary_list  = np.concatenate(gt_binary_list, axis=0).squeeze() # (B,)\n    auc  = roc_auc_score(y_true=gt_binary_list, y_score=pred_prob_list)\n    f1   = f1_metric(y_true=gt_binary_list, y_pred=pred_prob_list.round())\n    acc  = accuracy(y_true=gt_binary_list, y_pred=pred_prob_list.round())\n    sen  = sensitivity(y_true=gt_binary_list, y_pred=pred_prob_list.round())\n    spe  = specificity(y_true=gt_binary_list, y_pred=pred_prob_list.round())\n    \n    metric_logger.update(key='test_cls_auc', value=auc, n=1)\n    metric_logger.update(key='test_cls_f1',  value=f1,  n=1)\n    metric_logger.update(key='test_cls_acc', value=acc, n=1)\n    metric_logger.update(key='test_cls_sen', value=sen, n=1)\n    metric_logger.update(key='test_cls_spe', value=spe, n=1)\n\n    # Metrics SEG\n    dice_list        = np.array(dice_list)\n    fp_list          = np.array(fp_list)\n    valid_dice_score = dice_list[dice_list != -1]\n    valid_fp_score   = fp_list[fp_list != -1]\n\n    metric_logger.update(key='test_seg_dice', value=np.mean(valid_dice_score), n=1)\n    metric_logger.update(key='test_seg_fp',   value=np.mean(valid_fp_score),   n=1)\n\n    return {k: round(v, 7) for k, v in metric_logger.average().items()}\n\n\n########################################################\n# Down Task\ndef train_smartnet_3d_2dtransfer_CLS(train_loader, model, criterion, optimizer, device, epoch, multi_gpu, accelerator):\n    model.train()\n    metric_logger  = utils.AverageMeter()\n    epoch_iterator = tqdm(train_loader, desc=\"Training (X / X Steps) (loss=X.X)\", dynamic_ncols=True, total=len(train_loader), disable=(not accelerator.is_local_main_process))\n    \n    # apply_gradual_unfreezing(model, epoch, gradual_unfreeze=True, model_type='efficientnet')\n\n    for step, batch_data in enumerate(epoch_iterator):\n        \n        path, image, label, depth = batch_data\n        image, label = check_3d_cls_data_device(multi_gpu, device, image, label)\n        \n        with accelerator.autocast():\n            pred_cls = model(image, depth)\n            \n            pred_cls = pred_cls.sigmoid()\n            loss, loss_dict = criterion(pred_cls, label)\n\n        loss_value = loss.item()\n        optimizer.zero_grad()\n        if multi_gpu != 'DDP':\n            loss.backward()\n        else:\n            accelerator.backward(loss)\n        optimizer.step()\n\n        if not math.isfinite(loss_value):\n            print(\"Loss is {}, stopping training\".format(loss_value))\n        \n        all_loss_dict  = gather_object([loss_dict])\n        all_loss_dict  = collect_dict_all_processor(all_loss_dict, average=True)\n\n        if accelerator.is_main_process:\n            metric_logger.update(key='train_loss', value=loss_dict['total_loss'], n=image.shape[0]*accelerator.num_processes)\n            for key in loss_dict:\n                if key.startswith('cls_'):\n                    metric_logger.update(key='train_'+key, value=loss_dict[key], n=image.shape[0]*accelerator.num_processes)\n\n        epoch_iterator.set_description(\"Training: Epochs %d (%d / %d Steps), (train_loss=%2.5f)\" % (epoch, step, len(train_loader), loss_value))\n\n    return {k: round(v, 7) for k, v in metric_logger.average().items()}\n\n@torch.no_grad()\ndef valid_smartnet_3d_2dtransfer_CLS(valid_loader, model, device, epoch, save_dir, multi_gpu, accelerator):\n    model.eval()\n    metric_logger  = utils.AverageMeter()\n    epoch_iterator = tqdm(valid_loader, desc=\"Validating (X / X Steps) (loss=X.X)\", dynamic_ncols=True, total=len(valid_loader), disable=(not accelerator.is_local_main_process))\n    path_list      = []\n    pred_prob_list = []\n    gt_binary_list = []\n    \n    for step, batch_data in enumerate(epoch_iterator):\n        \n        path, image, label, depth = batch_data\n        image, label = check_3d_cls_data_device(multi_gpu, device, image, label)\n\n        # model.fc.register_forward_hook(get_features('feat')) # for Representation\n        # feat_list.append(features['feat'].detach().cpu().numpy())\n        \n        with accelerator.autocast():\n            pred_cls = model(image, depth)\n            pred_cls = pred_cls.sigmoid()\n\n        epoch_iterator.set_description(\"Validating: Epochs %d (%d / %d Steps)\" % (epoch, step, len(valid_loader)))\n        \n        path_list.extend(path)\n        pred_prob_list.append(fn_tonumpy(pred_cls))\n        gt_binary_list.append(fn_tonumpy(label))\n\n    # Summary\n    pred_prob_list = gather_object(pred_prob_list)\n    gt_binary_list = gather_object(gt_binary_list)\n    \n    if accelerator.is_main_process:\n        pred_prob_list  = np.concatenate(pred_prob_list, axis=0).squeeze() # (B,)\n        gt_binary_list  = np.concatenate(gt_binary_list, axis=0).squeeze() # (B,)\n\n        # Metric CLS\n        auc  = roc_auc_score(y_true=gt_binary_list, y_score=pred_prob_list)\n        f1   = f1_metric(y_true=gt_binary_list, y_pred=pred_prob_list.round())\n        acc  = accuracy(y_true=gt_binary_list, y_pred=pred_prob_list.round())\n        sen  = sensitivity(y_true=gt_binary_list, y_pred=pred_prob_list.round())\n        spe  = specificity(y_true=gt_binary_list, y_pred=pred_prob_list.round())\n\n        metric_logger.update(key='valid_cls_auc', value=auc, n=1)\n        metric_logger.update(key='valid_cls_f1',  value=f1,  n=1)\n        metric_logger.update(key='valid_cls_acc', value=acc, n=1)\n        metric_logger.update(key='valid_cls_sen', value=sen, n=1)\n        metric_logger.update(key='valid_cls_spe', value=spe, n=1)\n\n        # CAM ?\n\n    return {k: round(v, 7) for k, v in metric_logger.average().items()}\n\n@torch.no_grad()\ndef test_smartnet_3d_2dtransfer_CLS(test_loader, model, device, epoch, save_dir):\n    model.eval()\n    metric_logger  = utils.AverageMeter()\n    epoch_iterator = tqdm(test_loader, desc=\"Testing (X / X Steps) (loss=X.X)\", dynamic_ncols=True, total=len(test_loader))\n    path_list      = []\n    pred_prob_list = []\n    gt_binary_list = []\n\n    for step, batch_data in enumerate(epoch_iterator):\n        \n        path, image, label, depth = batch_data\n        image = image.to(device).float()\n        label = label.to(device).float()\n\n        pred_cls = model(image, depth)\n        pred_cls = pred_cls.sigmoid()\n\n        epoch_iterator.set_description(\"Testing: Epochs %d (%d / %d Steps)\" % (epoch, step, len(test_loader)))\n        \n        path_list.extend(path)\n        pred_prob_list.append(fn_tonumpy(pred_cls))\n        gt_binary_list.append(fn_tonumpy(label))\n\n    # Metric CLS\n    pred_prob_list  = np.concatenate(pred_prob_list, axis=0).squeeze() # (B,)\n    gt_binary_list  = np.concatenate(gt_binary_list, axis=0).squeeze() # (B,)\n\n    auc  = roc_auc_score(y_true=gt_binary_list, y_score=pred_prob_list)\n    f1   = f1_metric(y_true=gt_binary_list, y_pred=pred_prob_list.round())\n    acc  = accuracy(y_true=gt_binary_list, y_pred=pred_prob_list.round())\n    sen  = sensitivity(y_true=gt_binary_list, y_pred=pred_prob_list.round())\n    spe  = specificity(y_true=gt_binary_list, y_pred=pred_prob_list.round())\n    \n    metric_logger.update(key='test_cls_auc', value=auc, n=1)\n    metric_logger.update(key='test_cls_f1',  value=f1,  n=1)\n    metric_logger.update(key='test_cls_acc', value=acc, n=1)\n    metric_logger.update(key='test_cls_sen', value=sen, n=1)\n    metric_logger.update(key='test_cls_spe', value=spe, n=1)\n\n    # DataFrame\n    df = pd.DataFrame()\n    df['Patient_id'] = path_list\n    df['Prob']       = pred_prob_list\n    df['Label']      = gt_binary_list\n    df['Decision']   = pred_prob_list.round()\n    df.to_csv(save_dir+'/test_cls_result_epoch_'+str(epoch)+'.csv')\n\n    return {k: round(v, 7) for k, v in metric_logger.average().items()}\n\n\n# Down Task\ndef train_smartnet_3d_2dtransfer_SEG(train_loader, model, criterion, optimizer, device, epoch):\n    model.train()\n    metric_logger  = utils.AverageMeter()\n    epoch_iterator = tqdm(train_loader, desc=\"Training (X / X Steps) (loss=X.X)\", dynamic_ncols=True, total=len(train_loader))\n    \n    for step, batch_data in enumerate(epoch_iterator):\n        \n        path, image, mask, depth = batch_data\n\n        image = image.to(device)\n        mask  = mask.to(device)\n\n        pred_seg = model(image)\n\n        # act\n        pred_seg = pred_seg.sigmoid()\n\n        loss, loss_dict = criterion(pred_seg, mask)\n        loss_value = loss.item()\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        if not math.isfinite(loss_value):\n            print(\"Loss is {}, stopping training\".format(loss_value))\n        \n        metric_logger.update(key='train_loss', value=loss_value, n=image.shape[0])\n        # metric_logger.update(key='train_cls_bce_loss', value=loss_dict['cls_bce_loss'].item(), n=image.shape[0])\n\n        epoch_iterator.set_description(\"Training: Epochs %d (%d / %d Steps), (train_loss=%2.5f)\" % (epoch, step, len(train_loader), loss_value))\n\n    return {k: round(v, 7) for k, v in metric_logger.average().items()}\n\n@torch.no_grad()\ndef valid_smartnet_3d_2dtransfer_SEG(valid_loader, model, device, epoch, save_dir):\n    model.eval()\n    metric_logger  = utils.AverageMeter()\n    epoch_iterator = tqdm(valid_loader, desc=\"Validating (X / X Steps) (loss=X.X)\", dynamic_ncols=True, total=len(valid_loader))\n    \n    for step, batch_data in enumerate(epoch_iterator):\n        \n        path, image, mask, depth = batch_data\n\n        image = image.to(device)\n        mask  = mask.to(device)\n\n        pred_seg = model(image)\n\n        # act\n        pred_seg = pred_seg.sigmoid()\n\n        epoch_iterator.set_description(\"Validating: Epochs %d (%d / %d Steps)\" % (epoch, step, len(valid_loader)))\n        \n        # # Metrics SEG\n        # if mask.any():\n        #     dice = binary_dice_loss(y_pred=pred_seg.round(), y_true=mask, smooth=0.0, return_score=True)    # pred_seg must be round() !!  \n        #     metric_logger.update(key='valid_dice', value=dice.item(), n=mask.shape[0])\n\n    return {k: round(v, 7) for k, v in metric_logger.average().items()}\n\n@torch.no_grad()\ndef test_smartnet_3d_2dtransfer_SEG(test_loader, model, device, save_dir):\n    model.eval()\n    metric_logger  = utils.AverageMeter()\n    epoch_iterator = tqdm(test_loader, desc=\"Validating (X / X Steps) (loss=X.X)\", dynamic_ncols=True, total=len(test_loader))\n\n    pred_prob_list = []\n    gt_binary_list = []\n    patient_id_list = []\n    type_id_list = []\n\n    for step, batch_data in enumerate(epoch_iterator):\n        \n        image, mask, _, patient_id, type_id = batch_data\n\n        image = image.to(device)\n        label = mask.to(device)\n\n        pred_cls = model(image, depth)\n\n        # act\n        pred_cls = pred_cls.sigmoid()\n\n        epoch_iterator.set_description(\"TEST: (%d / %d Steps)\" % (step, len(test_loader)))\n        \n        pred_prob_list.append(fn_tonumpy(pred_cls))\n        gt_binary_list.append(fn_tonumpy(label))\n        patient_id_list.append(patient_id)\n        type_id_list.append(type_id)\n\n    pred_prob_list  = np.concatenate(pred_prob_list, axis=0).squeeze() # (B,)\n    gt_binary_list  = np.concatenate(gt_binary_list, axis=0).squeeze() # (B,)\n    patient_id_list = np.concatenate(patient_id_list, axis=0).squeeze() # (B,)\n    type_id_list    = np.concatenate(type_id_list, axis=0).squeeze() # (B,)\n\n    # Metric CLS\n    auc            = roc_auc_score(y_true=gt_binary_list, y_score=pred_prob_list)\n    f1             = f1_metric(y_true=gt_binary_list, y_pred=pred_prob_list.round())\n    acc            = accuracy(y_true=gt_binary_list, y_pred=pred_prob_list.round())\n    sen            = sensitivity(y_true=gt_binary_list, y_pred=pred_prob_list.round())\n    spe            = specificity(y_true=gt_binary_list, y_pred=pred_prob_list.round())\n\n    metric_logger.update(key='valid_auc', value=auc, n=1)\n    metric_logger.update(key='valid_f1',  value=f1,  n=1)\n    metric_logger.update(key='valid_acc', value=acc, n=1)\n    metric_logger.update(key='valid_sen', value=sen, n=1)\n    metric_logger.update(key='valid_spe', value=spe, n=1)\n\n    # DataFrame\n    df = pd.DataFrame()\n    df['Patient_id'] = patient_id_list\n    df['Type_id']    = type_id_list\n    df['Prob']       = pred_prob_list\n    df['Label']      = gt_binary_list\n    df['Decision']   = pred_prob_list.round()\n    df.to_csv(save_dir+'/pred_results.csv')\n\n    return {k: round(v, 7) for k, v in metric_logger.average().items()}\n\n\n\n\n\n\n\n\n\n# ########################################################\n# ## Inference code \n# from monai.transforms import SaveImage\n# from monai.transforms import Resize, Flip, Rotate90\n# from monai.utils import ensure_tuple\n# from typing import Dict, Optional, Union\n# import logging\n# import traceback\n\n\n#     # SMAET-Net\n# @torch.no_grad()\n# def infer_Up_SMART_Net(model, data_loader, device, print_freq, save_dir):\n#     # 2d slice-wise based evaluate...! \n#     model.eval()\n#     metric_logger = utils.MetricLogger(delimiter=\"  \", n=1)\n#     header = 'TEST:'\n    \n#     save_dict = dict()\n#     img_path_list = []\n#     img_list = []\n#     cls_list = []\n#     seg_list = []\n#     rec_list = []\n#     act_list = []\n\n#     for batch_data in metric_logger.log_every(data_loader, print_freq, header):\n        \n#         inputs  = batch_data[\"image\"].squeeze(4).to(device)      # (B, C, H, W, 1) ---> (B, C, H, W)\n\n#         model.encoder.layer4.register_forward_hook(get_activation('Activation Map')) # for Activation Map\n\n#         cls_pred, seg_pred, rec_pred = model(inputs)\n\n#         # post-processing\n#         cls_pred = torch.sigmoid(cls_pred)\n#         seg_pred = torch.sigmoid(seg_pred)\n\n#         img_path_list.append(batch_data[\"image_path\"][0])\n#         img_list.append(inputs.detach().cpu().squeeze())\n#         cls_list.append(cls_pred.detach().cpu().squeeze())\n#         seg_list.append(seg_pred.detach().cpu().squeeze())\n#         rec_list.append(rec_pred.detach().cpu().squeeze())\n#         act_list.append(Activation_Map(activation['Activation Map']))\n\n\n#     save_dict['img_path_list']  = img_path_list\n#     save_dict['img_list']       = img_list\n#     save_dict['cls_pred']       = cls_list\n#     save_dict['seg_pred']       = seg_list\n#     save_dict['rec_pred']       = rec_list\n#     save_dict['activation_map'] = act_list\n#     np.savez(save_dir + '/result.npz', result=save_dict) \n\n#     # CLS\n# @torch.no_grad()\n# def infer_Down_SMART_Net_CLS(model, data_loader, device, print_freq, save_dir):\n#     model.eval()\n#     metric_logger = utils.MetricLogger(delimiter=\"  \", n=1)\n#     header = 'TEST:'\n\n#     save_dict = dict()\n#     img_path_list = []\n#     img_list  = []\n#     cls_list  = []\n#     feat_list = []\n\n\n#     for batch_data in metric_logger.log_every(data_loader, print_freq, header):\n        \n        \n#         inputs  = batch_data[\"image\"].to(device)                                                        # (B, C, H, W, D)\n#         depths  = batch_data[\"depths\"]                                                                  #  ---> (B, 1) Fix bug, change cpu()\n#         paths   = batch_data[\"image_path\"][0]\n\n\n#         model.fc.register_forward_hook(get_features('feat')) # for Representation\n        \n#         cls_pred = model(inputs, depths)\n\n#         # Post-processing\n#         cls_pred = torch.sigmoid(cls_pred)\n\n#         img_path_list.append(paths)\n#         img_list.append(inputs.detach().cpu().numpy().squeeze())\n#         cls_list.append(cls_pred.detach().cpu().numpy().squeeze())\n#         feat_list.append(features['feat'].detach().cpu().numpy().squeeze())\n\n\n#     save_dict['img_path_list']  = img_path_list\n#     save_dict['img_list']       = img_list\n#     save_dict['cls_pred']       = cls_list\n#     save_dict['feat']           = feat_list\n#     np.savez(save_dir + '/result.npz', result=save_dict) \n\n\n#     # SEG\n# @torch.no_grad()\n# def infer_Down_SMART_Net_SEG(model, data_loader, device, print_freq, save_dir):"}
{"type": "source_file", "path": "module/sunggu_module.py", "content": "import torch.nn as nn\r\nimport torch\r\n\r\nclass TimeDistributed(nn.Module):\r\n    def __init__(self, module, batch_first=False):\r\n        super(TimeDistributed, self).__init__()\r\n        self.module = module\r\n        self.batch_first = batch_first\r\n\r\n    def forward(self, x):\r\n\r\n        if len(x.size()) <= 2:\r\n            return self.module(x)\r\n    \r\n#         print(\"0# =\", x.shape)   0# = torch.Size([5, 1024, 2, 40, 40])\r\n        # Squash samples and timesteps into a single axis\r\n        batch, channel, time, height, width = x.shape\r\n        \r\n        x_reshape = x.contiguous().view(batch*time, channel, height, width)  # (samples * timesteps, input_size)\r\n#         print(\"2# =\", x_reshape.shape)  torch.Size([10, 1024, 40, 40])\r\n\r\n        y = self.module(x_reshape)\r\n    \r\n#         print(\"1\", y.shape) [10, 512, 40, 40])\r\n#         print(\"self.module.out_channels\", self.module.out_channels) 512\r\n\r\n        # We have to reshape Y\r\n        if self.batch_first:\r\n            y = y.contiguous().view(batch, self.module.out_channels, time, height, width)  # (samples, timesteps, output_size)\r\n        else:\r\n            y = y.view(-1, x.size(1), y.size(-1))  # (timesteps, samples, output_size)\r\n            \r\n        return y \r\n    \r\n\r\nclass ConvLSTMCell(nn.Module):\r\n    def __init__(self, input_dim, hidden_dim, kernel_size, bias):\r\n        super(ConvLSTMCell, self).__init__()\r\n\r\n        self.input_dim = input_dim\r\n        self.hidden_dim = hidden_dim\r\n\r\n        self.kernel_size = kernel_size\r\n        self.padding = kernel_size[0] // 2, kernel_size[1] // 2\r\n        self.bias = bias\r\n\r\n        self.conv = nn.Conv2d(in_channels=self.input_dim + self.hidden_dim,\r\n                              out_channels=4 * self.hidden_dim,\r\n                              kernel_size=self.kernel_size,\r\n                              padding=self.padding,\r\n                              bias=self.bias)\r\n\r\n    def forward(self, input_tensor, cur_state):\r\n        h_cur, c_cur = cur_state\r\n\r\n        combined = torch.cat([input_tensor, h_cur], dim=1)  # concatenate along channel axis\r\n\r\n        combined_conv = self.conv(combined)\r\n        cc_i, cc_f, cc_o, cc_g = torch.split(combined_conv, self.hidden_dim, dim=1)\r\n        i = torch.sigmoid(cc_i)\r\n        f = torch.sigmoid(cc_f)\r\n        o = torch.sigmoid(cc_o)\r\n        g = torch.tanh(cc_g)\r\n\r\n        c_next = f * c_cur + i * g\r\n        h_next = o * torch.tanh(c_next)\r\n\r\n        return h_next, c_next\r\n\r\n    def init_hidden(self, batch_size, image_size):\r\n        height, width = image_size\r\n        return (torch.zeros(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device),\r\n                torch.zeros(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device))\r\n\r\n\r\nclass ConvLSTM2d(nn.Module):\r\n    def __init__(self, input_dim, hidden_dim, kernel_size, num_layers,\r\n                 batch_first=False, bias=True, return_all_layers=False):\r\n        super(ConvLSTM2d, self).__init__()\r\n\r\n        self._check_kernel_size_consistency(kernel_size)\r\n\r\n        # Make sure that both `kernel_size` and `hidden_dim` are lists having len == num_layers\r\n        kernel_size = self._extend_for_multilayer(kernel_size, num_layers)\r\n        hidden_dim = self._extend_for_multilayer(hidden_dim, num_layers)\r\n        if not len(kernel_size) == len(hidden_dim) == num_layers:\r\n            raise ValueError('Inconsistent list length.')\r\n\r\n        self.input_dim = input_dim\r\n        self.hidden_dim = hidden_dim\r\n        self.kernel_size = kernel_size\r\n        self.num_layers = num_layers\r\n        self.batch_first = batch_first\r\n        self.bias = bias\r\n        self.return_all_layers = return_all_layers\r\n\r\n        cell_list = []\r\n        for i in range(0, self.num_layers):\r\n            cur_input_dim = self.input_dim if i == 0 else self.hidden_dim[i - 1]\r\n\r\n            cell_list.append(ConvLSTMCell(input_dim=cur_input_dim,\r\n                                          hidden_dim=self.hidden_dim[i],\r\n                                          kernel_size=self.kernel_size[i],\r\n                                          bias=self.bias))\r\n\r\n        self.cell_list = nn.ModuleList(cell_list)\r\n\r\n    def forward(self, input_tensor, hidden_state=None):\r\n        if not self.batch_first:\r\n            # (t, b, c, h, w) -> (b, t, c, h, w)  \r\n            input_tensor = input_tensor.permute(1, 0, 2, 3, 4)\r\n        \r\n#         print(\"input_tensor.shape\", input_tensor.shape)\r\n        # (b, c, t, h, w) -> (b, t, c, h, w)  \r\n        input_tensor = input_tensor.permute(0, 2, 1, 3, 4)\r\n        b, _, _, h, w = input_tensor.size()\r\n\r\n        # Implement stateful ConvLSTM\r\n        if hidden_state is not None:\r\n            raise NotImplementedError()\r\n        else:\r\n            # Since the init is done in forward. Can send image size here\r\n            hidden_state = self._init_hidden(batch_size=b,\r\n                                             image_size=(h, w))\r\n        layer_output_list = []\r\n        last_state_list = []\r\n\r\n        seq_len = input_tensor.size(1)\r\n        cur_layer_input = input_tensor\r\n\r\n        for layer_idx in range(self.num_layers):\r\n\r\n            h, c = hidden_state[layer_idx]\r\n            output_inner = []\r\n            for t in range(seq_len):\r\n                h, c = self.cell_list[layer_idx](input_tensor=cur_layer_input[:, t, :, :, :],\r\n                                                 cur_state=[h, c])\r\n                output_inner.append(h)\r\n\r\n            layer_output = torch.stack(output_inner, dim=1)\r\n            cur_layer_input = layer_output\r\n\r\n            layer_output_list.append(layer_output)\r\n            last_state_list.append([h, c])\r\n\r\n        if not self.return_all_layers:\r\n            # (b, t, c, h, w) -> (b, c, t, h, w)\r\n            layer_output_list = layer_output_list[-1].permute(0, 2, 1, 3, 4)\r\n            last_state_list = last_state_list[-1]\r\n\r\n        return layer_output_list, last_state_list\r\n\r\n    def _init_hidden(self, batch_size, image_size):\r\n        init_states = []\r\n        for i in range(self.num_layers):\r\n            init_states.append(self.cell_list[i].init_hidden(batch_size, image_size))\r\n        return init_states\r\n\r\n    @staticmethod\r\n    def _check_kernel_size_consistency(kernel_size):\r\n        if not (isinstance(kernel_size, tuple) or\r\n                (isinstance(kernel_size, list) and all([isinstance(elem, tuple) for elem in kernel_size]))):\r\n            raise ValueError('`kernel_size` must be tuple or list of tuples')\r\n\r\n    @staticmethod\r\n    def _extend_for_multilayer(param, num_layers):\r\n        if not isinstance(param, list):\r\n            param = [param] * num_layers\r\n        return param\r\n\r\n\r\n\r\n\r\n\r\n# ########################################################################################################################\r\n# ##################### 모델 ##########################\r\n# ########################################################################################################################\r\n# class Flatten(torch.nn.Module):\r\n#     def forward(self, x):\r\n#         return x.view(x.shape[0], -1)\r\n\r\n# class conv_block(nn.Module):\r\n#     def __init__(self, ch_in, ch_out):\r\n#         super(conv_block,self).__init__()\r\n#         self.convlstm1 = ConvLSTM2d(input_dim=ch_in, hidden_dim=ch_out, kernel_size=(3, 3), num_layers=1, batch_first=True, bias=True, return_all_layers=False)\r\n#         self.norm1 = nn.BatchNorm3d(ch_out)\r\n#         self.relu1 = nn.ReLU(inplace=True)\r\n        \r\n#         self.convlstm2 = ConvLSTM2d(input_dim=ch_out, hidden_dim=ch_out, kernel_size=(3, 3), num_layers=1, batch_first=True, bias=True, return_all_layers=False)\r\n#         self.norm2 = nn.BatchNorm3d(ch_out)\r\n#         self.relu2 = nn.ReLU(inplace=True)\r\n    \r\n#     def forward(self,x):\r\n#         x = self.convlstm1(x)\r\n#         x = self.norm1(x[0])\r\n#         x = self.relu1(x)\r\n#         # x = self.convlstm2(x)\r\n#         # x = self.norm2(x[0])\r\n#         # x = self.relu2(x)\r\n#         return x\r\n    \r\n# class up_conv(nn.Module):\r\n#     def __init__(self, ch_in, ch_out):\r\n#         super(up_conv,self).__init__()\r\n#         self.up = nn.Upsample(scale_factor=2, mode='trilinear', align_corners=False)\r\n#         self.timedistributed = TimeDistributed(module=nn.Conv2d(in_channels=ch_in, out_channels=ch_out, kernel_size=3, stride=1, padding=1, bias=True), batch_first=True)\r\n#         self.norm = nn.BatchNorm3d(ch_out)\r\n#         self.relu = nn.ReLU(inplace=True)\r\n        \r\n\r\n#     def forward(self,x):\r\n#         x = self.up(x)\r\n# #         print(\"check1\",x.shape) check1 torch.Size([5, 1024, 2, 40, 40])\r\n#         x = self.timedistributed(x)\r\n# #         print(\"check2\",x.shape) torch.Size([5, 512, 2, 40, 40])\r\n#         x = self.norm(x)\r\n#         x = self.relu(x)\r\n#         return x\r\n    \r\n    \r\n# class U_Net(nn.Module):\r\n#     def __init__(self, img_ch=1, output_ch=1):\r\n#         super(U_Net,self).__init__()\r\n        \r\n#         self.Maxpool = nn.MaxPool3d(kernel_size=2, stride=2)\r\n\r\n#         self.Conv1 = conv_block(ch_in=img_ch, ch_out=64)\r\n#         self.Conv2 = conv_block(ch_in=64, ch_out=128)\r\n#         self.Conv3 = conv_block(ch_in=128, ch_out=256)\r\n#         self.Conv4 = conv_block(ch_in=256, ch_out=512)\r\n#         self.Conv5 = conv_block(ch_in=512, ch_out=1024)\r\n\r\n#         self.Up5 = up_conv(ch_in=1024, ch_out=512)\r\n#         self.Up_conv5 = conv_block(ch_in=1024, ch_out=512)\r\n\r\n#         self.Up4 = up_conv(ch_in=512,ch_out=256)\r\n#         self.Up_conv4 = conv_block(ch_in=512, ch_out=256)\r\n        \r\n#         self.Up3 = up_conv(ch_in=256,ch_out=128)\r\n#         self.Up_conv3 = conv_block(ch_in=256, ch_out=128)\r\n        \r\n#         self.Up2 = up_conv(ch_in=128,ch_out=64)\r\n#         self.Up_conv2 = conv_block(ch_in=128, ch_out=64)\r\n\r\n#         self.Conv_1x1 = nn.Conv3d(64, output_ch, kernel_size=1, stride=1, padding=0)\r\n        \r\n#         self.linear = nn.Sequential(nn.AdaptiveAvgPool2d(1), Flatten(), nn.Linear(1024, 1, bias=True))\r\n\r\n#     def forward(self, x):\r\n#         # encoding path\r\n# #         print(\"x = \" ,x.shape) torch.Size([5, 1, 16, 320, 320])\r\n#         x1 = self.Conv1(x)\r\n# #         print(\"1 = \" ,x1.shape) 1 =  torch.Size([5, 64, 16, 320, 320])\r\n#         x2 = self.Maxpool(x1)\r\n# #         print(\"2 = \" ,x2.shape) 2 =  torch.Size([5, 64, 8, 160, 160])\r\n#         x2 = self.Conv2(x2)\r\n# #         print(\"3 = \" ,x2.shape) 3 =  torch.Size([5, 128, 8, 160, 160])\r\n#         x3 = self.Maxpool(x2)\r\n# #         print(\"4 = \" ,x3.shape) 4 =  torch.Size([5, 128, 4, 80, 80])\r\n#         x3 = self.Conv3(x3)\r\n# #         print(\"5 = \" ,x3.shape) 5 =  torch.Size([5, 256, 4, 80, 80])\r\n\r\n#         x4 = self.Maxpool(x3)\r\n# #         print(\"6 = \" ,x4.shape) 6 =  torch.Size([5, 256, 2, 40, 40])\r\n#         x4 = self.Conv4(x4)\r\n# #         print(\"7 = \" ,x4.shape) 7 =  torch.Size([5, 512, 2, 40, 40])\r\n\r\n#         x5 = self.Maxpool(x4)\r\n# #         print(\"8 = \" ,x5.shape) 8 =  torch.Size([5, 512, 1, 20, 20])\r\n#         x5 = self.Conv5(x5)\r\n#         # print(\"9 = \", x5.shape) # 9 =  torch.Size([5, 1024, 1, 20, 20])\r\n        \r\n#         # Aux 붙이기\r\n#         cls = self.linear(x5.squeeze())\r\n# #         print(\"10 = \" ,cls.shape)  10 =  torch.Size([5, 1])\r\n\r\n#         # decoding + concat path\r\n#         d5 = self.Up5(x5)\r\n# #         print(\"11 = \" ,d5.shape) 11 =  torch.Size([5, 512, 2, 40, 40])\r\n#         d5 = torch.cat((x4,d5),dim=1)\r\n# #         print(\"12 = \" ,d5.shape)  torch.Size([5, 1024, 2, 40, 40])\r\n#         d5 = self.Up_conv5(d5)\r\n# #         print(\"13 = \" ,d5.shape) torch.Size([5, 512, 2, 40, 40])\r\n        \r\n#         d4 = self.Up4(d5)\r\n# #         print(\"14 = \" ,d4.shape)  torch.Size([5, 256, 4, 80, 80])\r\n#         d4 = torch.cat((x3,d4),dim=1)\r\n# #         print(\"15 = \" ,d4.shape)   torch.Size([5, 512, 4, 80, 80])\r\n#         d4 = self.Up_conv4(d4)\r\n# #         print(\"16 = \" ,d4.shape) torch.Size([5, 256, 4, 80, 80])\r\n\r\n#         d3 = self.Up3(d4)\r\n# #         print(\"17 = \" ,d3.shape)   torch.Size([5, 128, 8, 160, 160])\r\n#         d3 = torch.cat((x2,d3),dim=1)\r\n# #         print(\"18 = \" ,d3.shape)     torch.Size([5, 256, 8, 160, 160])\r\n#         d3 = self.Up_conv3(d3)\r\n# #         print(\"19 = \" ,d3.shape)    torch.Size([5, 128, 8, 160, 160])\r\n\r\n#         d2 = self.Up2(d3)\r\n# #         print(\"20 = \" ,d2.shape)     torch.Size([5, 64, 16, 320, 320])\r\n#         d2 = torch.cat((x1,d2),dim=1)\r\n# #         print(\"21 = \" ,d2.shape)      torch.Size([5, 128, 16, 320, 320])\r\n#         d2 = self.Up_conv2(d2)\r\n# #         print(\"22 = \" ,d2.shape)      torch.Size([5, 64, 16, 320, 320])\r\n\r\n#         d1 = self.Conv_1x1(d2)\r\n# #         print(\"1 = \" ,d1.shape)       torch.Size([5, 1, 16, 320, 320])\r\n\r\n#         return d1, cls\r\n\r\n\r\n########################################################################################################################\r\n##################### 모델 2##########################\r\n########################################################################################################################\r\nclass Flatten(torch.nn.Module):\r\n    def forward(self, x):\r\n        return x.view(x.shape[0], -1)\r\n\r\nclass conv_block(nn.Module):\r\n    def __init__(self, ch_in, ch_out):\r\n        super(conv_block,self).__init__()\r\n        self.timedistributed1 = TimeDistributed(module=nn.Conv2d(in_channels=ch_in, out_channels=ch_out, kernel_size=3, stride=1, padding=1, bias=True), batch_first=True)\r\n        self.norm1 = nn.BatchNorm3d(ch_out)\r\n        self.relu1 = nn.ReLU(inplace=True)\r\n        \r\n        self.timedistributed2 = TimeDistributed(module=nn.Conv2d(in_channels=ch_out, out_channels=ch_out, kernel_size=3, stride=1, padding=1, bias=True), batch_first=True)\r\n        self.norm2 = nn.BatchNorm3d(ch_out)\r\n        self.relu2 = nn.ReLU(inplace=True)\r\n    \r\n    def forward(self,x):\r\n        x = self.timedistributed1(x)\r\n        x = self.norm1(x)\r\n        x = self.relu1(x)\r\n        x = self.timedistributed2(x)\r\n        x = self.norm2(x)\r\n        x = self.relu2(x)\r\n        return x\r\n    \r\nclass up_conv(nn.Module):\r\n    def __init__(self, ch_in, ch_out):\r\n        super(up_conv,self).__init__()\r\n        self.up = nn.Upsample(scale_factor=2, mode='trilinear', align_corners=False)\r\n        self.convlstm = ConvLSTM2d(input_dim=ch_in, hidden_dim=ch_out, kernel_size=(3, 3), num_layers=1, batch_first=True, bias=True, return_all_layers=False)\r\n        self.norm = nn.BatchNorm3d(ch_out)\r\n        self.relu = nn.ReLU(inplace=True)\r\n        \r\n        \r\n    def forward(self,x):\r\n        x = self.up(x)\r\n#         print(\"check1\",x.shape) check1 torch.Size([5, 1024, 2, 40, 40])\r\n        x = self.convlstm(x)\r\n#         print(\"check2\",x.shape) torch.Size([5, 512, 2, 40, 40])\r\n        x = self.norm(x[0])\r\n        x = self.relu(x)\r\n        return x\r\n    \r\n    \r\nclass U_Net(nn.Module):\r\n    def __init__(self, img_ch=1, output_ch=1):\r\n        super(U_Net,self).__init__()\r\n        \r\n        self.Maxpool = nn.MaxPool3d(kernel_size=2, stride=2)\r\n\r\n        self.Conv1 = conv_block(ch_in=img_ch, ch_out=64)\r\n        self.Conv2 = conv_block(ch_in=64, ch_out=128)\r\n        self.Conv3 = conv_block(ch_in=128, ch_out=256)\r\n        self.Conv4 = conv_block(ch_in=256, ch_out=512)\r\n        self.Conv5 = conv_block(ch_in=512, ch_out=1024)\r\n\r\n        self.Up5 = up_conv(ch_in=1024, ch_out=512)\r\n        self.Up_conv5 = conv_block(ch_in=1024, ch_out=512)\r\n\r\n        self.Up4 = up_conv(ch_in=512,ch_out=256)\r\n        self.Up_conv4 = conv_block(ch_in=512, ch_out=256)\r\n        \r\n        self.Up3 = up_conv(ch_in=256,ch_out=128)\r\n        self.Up_conv3 = conv_block(ch_in=256, ch_out=128)\r\n        \r\n        self.Up2 = up_conv(ch_in=128,ch_out=64)\r\n        self.Up_conv2 = conv_block(ch_in=128, ch_out=64)\r\n\r\n        self.Conv_1x1 = nn.Conv3d(64, output_ch, kernel_size=1, stride=1, padding=0)\r\n        \r\n        self.linear = nn.Sequential(nn.AdaptiveAvgPool2d(1), Flatten(), nn.Linear(1024, 1, bias=True))\r\n\r\n    def forward(self, x):\r\n        # encoding path\r\n#         print(\"x = \" ,x.shape) torch.Size([5, 1, 16, 320, 320])\r\n        x1 = self.Conv1(x)\r\n#         print(\"1 = \" ,x1.shape) 1 =  torch.Size([5, 64, 16, 320, 320])\r\n        x2 = self.Maxpool(x1)\r\n#         print(\"2 = \" ,x2.shape) 2 =  torch.Size([5, 64, 8, 160, 160])\r\n        x2 = self.Conv2(x2)\r\n#         print(\"3 = \" ,x2.shape) 3 =  torch.Size([5, 128, 8, 160, 160])\r\n        x3 = self.Maxpool(x2)\r\n#         print(\"4 = \" ,x3.shape) 4 =  torch.Size([5, 128, 4, 80, 80])\r\n        x3 = self.Conv3(x3)\r\n#         print(\"5 = \" ,x3.shape) 5 =  torch.Size([5, 256, 4, 80, 80])\r\n\r\n        x4 = self.Maxpool(x3)\r\n#         print(\"6 = \" ,x4.shape) 6 =  torch.Size([5, 256, 2, 40, 40])\r\n        x4 = self.Conv4(x4)\r\n#         print(\"7 = \" ,x4.shape) 7 =  torch.Size([5, 512, 2, 40, 40])\r\n\r\n        x5 = self.Maxpool(x4)\r\n#         print(\"8 = \" ,x5.shape) 8 =  torch.Size([5, 512, 1, 20, 20])\r\n        x5 = self.Conv5(x5)\r\n        # print(\"9 = \", x5.shape) # 9 =  torch.Size([5, 1024, 1, 20, 20])\r\n        \r\n        # Aux 붙이기\r\n        cls = self.linear(x5.squeeze())\r\n#         print(\"10 = \" ,cls.shape)  10 =  torch.Size([5, 1])\r\n\r\n        # decoding + concat path\r\n        d5 = self.Up5(x5)\r\n#         print(\"11 = \" ,d5.shape) 11 =  torch.Size([5, 512, 2, 40, 40])\r\n        d5 = torch.cat((x4,d5),dim=1)\r\n#         print(\"12 = \" ,d5.shape)  torch.Size([5, 1024, 2, 40, 40])\r\n        d5 = self.Up_conv5(d5)\r\n#         print(\"13 = \" ,d5.shape) torch.Size([5, 512, 2, 40, 40])\r\n        \r\n        d4 = self.Up4(d5)\r\n#         print(\"14 = \" ,d4.shape)  torch.Size([5, 256, 4, 80, 80])\r\n        d4 = torch.cat((x3,d4),dim=1)\r\n#         print(\"15 = \" ,d4.shape)   torch.Size([5, 512, 4, 80, 80])\r\n        d4 = self.Up_conv4(d4)\r\n#         print(\"16 = \" ,d4.shape) torch.Size([5, 256, 4, 80, 80])\r\n\r\n        d3 = self.Up3(d4)\r\n#         print(\"17 = \" ,d3.shape)   torch.Size([5, 128, 8, 160, 160])\r\n        d3 = torch.cat((x2,d3),dim=1)\r\n#         print(\"18 = \" ,d3.shape)     torch.Size([5, 256, 8, 160, 160])\r\n        d3 = self.Up_conv3(d3)\r\n#         print(\"19 = \" ,d3.shape)    torch.Size([5, 128, 8, 160, 160])\r\n\r\n        d2 = self.Up2(d3)\r\n#         print(\"20 = \" ,d2.shape)     torch.Size([5, 64, 16, 320, 320])\r\n        d2 = torch.cat((x1,d2),dim=1)\r\n#         print(\"21 = \" ,d2.shape)      torch.Size([5, 128, 16, 320, 320])\r\n        d2 = self.Up_conv2(d2)\r\n#         print(\"22 = \" ,d2.shape)      torch.Size([5, 64, 16, 320, 320])\r\n\r\n        d1 = self.Conv_1x1(d2)\r\n#         print(\"1 = \" ,d1.shape)       torch.Size([5, 1, 16, 320, 320])\r\n\r\n        return d1, cls\r\n\r\n\r\n\r\n\r\n"}
{"type": "source_file", "path": "module/unet_model.py", "content": "import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom functools import partial\nimport torch.nn.functional as F\nfrom typing import Callable, Union\n\n## 네트워크 구축하기\nclass UNet(nn.Module):\n    def __init__(self, input_nc=1, output_nc=1):\n        super(UNet, self).__init__()\n\n        def CBR2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=True):\n            layers = []\n            layers += [nn.Conv2d(in_channels=in_channels, out_channels=out_channels,\n                                 kernel_size=kernel_size, stride=stride, padding=padding,\n                                 bias=bias)]\n            layers += [nn.BatchNorm2d(num_features=out_channels)]\n            layers += [nn.ReLU()]\n\n            cbr = nn.Sequential(*layers)\n\n            return cbr\n\n        # Contracting path\n        self.enc1_1 = CBR2d(in_channels=input_nc, out_channels=64)\n        self.enc1_2 = CBR2d(in_channels=64, out_channels=64)\n\n        self.pool1 = nn.MaxPool2d(kernel_size=2)\n\n        self.enc2_1 = CBR2d(in_channels=64, out_channels=128)\n        self.enc2_2 = CBR2d(in_channels=128, out_channels=128)\n\n        self.pool2 = nn.MaxPool2d(kernel_size=2)\n\n        self.enc3_1 = CBR2d(in_channels=128, out_channels=256)\n        self.enc3_2 = CBR2d(in_channels=256, out_channels=256)\n\n        self.pool3 = nn.MaxPool2d(kernel_size=2)\n\n        self.enc4_1 = CBR2d(in_channels=256, out_channels=512)\n        self.enc4_2 = CBR2d(in_channels=512, out_channels=512)\n\n        self.pool4 = nn.MaxPool2d(kernel_size=2)\n\n        self.enc5_1 = CBR2d(in_channels=512, out_channels=1024)\n\n        # Expansive path\n        self.dec5_1 = CBR2d(in_channels=1024, out_channels=512)\n\n        self.unpool4 = nn.ConvTranspose2d(in_channels=512, out_channels=512,\n                                          kernel_size=2, stride=2, padding=0, bias=True)\n\n        self.dec4_2 = CBR2d(in_channels=2 * 512, out_channels=512)\n        self.dec4_1 = CBR2d(in_channels=512, out_channels=256)\n\n        self.unpool3 = nn.ConvTranspose2d(in_channels=256, out_channels=256,\n                                          kernel_size=2, stride=2, padding=0, bias=True)\n\n        self.dec3_2 = CBR2d(in_channels=2 * 256, out_channels=256)\n        self.dec3_1 = CBR2d(in_channels=256, out_channels=128)\n\n        self.unpool2 = nn.ConvTranspose2d(in_channels=128, out_channels=128,\n                                          kernel_size=2, stride=2, padding=0, bias=True)\n\n        self.dec2_2 = CBR2d(in_channels=2 * 128, out_channels=128)\n        self.dec2_1 = CBR2d(in_channels=128, out_channels=64)\n\n        self.unpool1 = nn.ConvTranspose2d(in_channels=64, out_channels=64,\n                                          kernel_size=2, stride=2, padding=0, bias=True)\n\n        self.dec1_2 = CBR2d(in_channels=2 * 64, out_channels=64)\n        self.dec1_1 = CBR2d(in_channels=64, out_channels=64)\n\n        self.fc = nn.Conv2d(in_channels=64, out_channels=output_nc, kernel_size=1, stride=1, padding=0, bias=True)\n\n    def forward(self, x):\n        enc1_1 = self.enc1_1(x)\n        enc1_2 = self.enc1_2(enc1_1)\n        pool1 = self.pool1(enc1_2)\n\n        enc2_1 = self.enc2_1(pool1)\n        enc2_2 = self.enc2_2(enc2_1)\n        pool2 = self.pool2(enc2_2)\n\n        enc3_1 = self.enc3_1(pool2)\n        enc3_2 = self.enc3_2(enc3_1)\n        pool3 = self.pool3(enc3_2)\n\n        enc4_1 = self.enc4_1(pool3)\n        enc4_2 = self.enc4_2(enc4_1)\n        pool4 = self.pool4(enc4_2)\n\n        enc5_1 = self.enc5_1(pool4)\n\n        dec5_1 = self.dec5_1(enc5_1)\n\n        unpool4 = self.unpool4(dec5_1)\n        cat4 = torch.cat((unpool4, enc4_2), dim=1)\n        dec4_2 = self.dec4_2(cat4)\n        dec4_1 = self.dec4_1(dec4_2)\n\n        unpool3 = self.unpool3(dec4_1)\n        cat3 = torch.cat((unpool3, enc3_2), dim=1)\n        dec3_2 = self.dec3_2(cat3)\n        dec3_1 = self.dec3_1(dec3_2)\n\n        unpool2 = self.unpool2(dec3_1)\n        cat2 = torch.cat((unpool2, enc2_2), dim=1)\n        dec2_2 = self.dec2_2(cat2)\n        dec2_1 = self.dec2_1(dec2_2)\n\n        unpool1 = self.unpool1(dec2_1)\n        cat1 = torch.cat((unpool1, enc1_2), dim=1)\n        dec1_2 = self.dec1_2(cat1)\n        dec1_1 = self.dec1_1(dec1_2)\n\n        x = self.fc(dec1_1)\n\n        return x\n############################################################## 3D UNET ##########################################################################\n## 네트워크 구축하기\nclass UNet_Encoder_3D(nn.Module):\n    def __init__(self, input_nc=1, output_nc=1):\n        super(UNet_Encoder_3D, self).__init__()\n\n        def CBR3d(in_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=True):\n            layers = []\n            layers += [nn.Conv3d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=bias)]\n            layers += [nn.BatchNorm3d(num_features=out_channels)]\n            layers += [nn.ReLU()]\n            cbr = nn.Sequential(*layers)\n            return cbr\n\n        # Contracting path\n        self.enc1_1 = CBR3d(in_channels=input_nc, out_channels=64)\n        self.enc1_2 = CBR3d(in_channels=64, out_channels=64)\n        self.pool1  = nn.MaxPool3d(kernel_size=2)\n\n        self.enc2_1 = CBR3d(in_channels=64, out_channels=128)\n        self.enc2_2 = CBR3d(in_channels=128, out_channels=128)\n        self.pool2  = nn.MaxPool3d(kernel_size=2)\n\n        self.enc3_1 = CBR3d(in_channels=128, out_channels=256)\n        self.enc3_2 = CBR3d(in_channels=256, out_channels=256)\n        self.pool3  = nn.MaxPool3d(kernel_size=2)\n\n        self.enc4_1 = CBR3d(in_channels=256, out_channels=512)\n        self.enc4_2 = CBR3d(in_channels=512, out_channels=512)\n        self.pool4  = nn.MaxPool3d(kernel_size=2)\n\n        self.enc5_1 = CBR3d(in_channels=512, out_channels=1024)\n\n\n    def forward(self, x):\n        enc1_1 = self.enc1_1(x)\n        enc1_2 = self.enc1_2(enc1_1)\n        pool1 = self.pool1(enc1_2)\n\n        enc2_1 = self.enc2_1(pool1)\n        enc2_2 = self.enc2_2(enc2_1)\n        pool2 = self.pool2(enc2_2)\n\n        enc3_1 = self.enc3_1(pool2)\n        enc3_2 = self.enc3_2(enc3_1)\n        pool3 = self.pool3(enc3_2)\n\n        enc4_1 = self.enc4_1(pool3)\n        enc4_2 = self.enc4_2(enc4_1)\n        pool4 = self.pool4(enc4_2)\n\n        enc5_1 = self.enc5_1(pool4)\n\n        return enc5_1, enc4_2, enc3_2, enc2_2, enc1_2\n\n\n\n## 네트워크 구축하기\nclass UNet_Decoder_3D(nn.Module):\n    def __init__(self):\n        super(UNet_Decoder_3D, self).__init__()\n\n        def CBR3d(in_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=True):\n            layers = []\n            layers += [nn.Conv3d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=bias)]\n            layers += [nn.BatchNorm3d(num_features=out_channels)]\n            layers += [nn.ReLU()]\n            cbr = nn.Sequential(*layers)\n            return cbr\n\n\n        # Expansive path\n        self.dec5_1 = CBR3d(in_channels=1024, out_channels=512)\n\n        self.unpool4 = nn.ConvTranspose3d(in_channels=512, out_channels=512, kernel_size=2, stride=2, padding=0, bias=True)\n\n        self.dec4_2 = CBR3d(in_channels=2 * 512, out_channels=512)\n        self.dec4_1 = CBR3d(in_channels=512, out_channels=256)\n\n        self.unpool3 = nn.ConvTranspose3d(in_channels=256, out_channels=256, kernel_size=2, stride=2, padding=0, bias=True)\n\n        self.dec3_2 = CBR3d(in_channels=2 * 256, out_channels=256)\n        self.dec3_1 = CBR3d(in_channels=256, out_channels=128)\n\n        self.unpool2 = nn.ConvTranspose3d(in_channels=128, out_channels=128, kernel_size=2, stride=2, padding=0, bias=True)\n\n        self.dec2_2 = CBR3d(in_channels=2 * 128, out_channels=128)\n        self.dec2_1 = CBR3d(in_channels=128, out_channels=64)\n\n        self.unpool1 = nn.ConvTranspose3d(in_channels=64, out_channels=64, kernel_size=2, stride=2, padding=0, bias=True)\n\n        self.dec1_2 = CBR3d(in_channels=2 * 64, out_channels=64)\n        self.dec1_1 = CBR3d(in_channels=64, out_channels=64)\n\n\n    def forward(self, enc5_1, enc4_2, enc3_2, enc2_2, enc1_2):\n\n        dec5_1 = self.dec5_1(enc5_1)\n\n        unpool4 = self.unpool4(dec5_1)\n        cat4 = torch.cat((unpool4, enc4_2), dim=1)\n        dec4_2 = self.dec4_2(cat4)\n        dec4_1 = self.dec4_1(dec4_2)\n\n        unpool3 = self.unpool3(dec4_1)\n        cat3 = torch.cat((unpool3, enc3_2), dim=1)\n        dec3_2 = self.dec3_2(cat3)\n        dec3_1 = self.dec3_1(dec3_2)\n\n        unpool2 = self.unpool2(dec3_1)\n        cat2 = torch.cat((unpool2, enc2_2), dim=1)\n        dec2_2 = self.dec2_2(cat2)\n        dec2_1 = self.dec2_1(dec2_2)\n\n        unpool1 = self.unpool1(dec2_1)\n        cat1 = torch.cat((unpool1, enc1_2), dim=1)\n        dec1_2 = self.dec1_2(cat1)\n        dec1_1 = self.dec1_1(dec1_2)\n\n        return dec1_1\n\n\n\n########################################################################### 2D Unet #########################################################################################\n## 네트워크 구축하기\nclass UNet_Encoder(nn.Module):\n    def __init__(self, input_nc=1, output_nc=1):\n        super(UNet_Encoder, self).__init__()\n\n        def CBR3d(in_channels, out_channels, kernel_size=(3, 3, 1), stride=1, padding=(1, 1, 0), bias=True):\n            layers = []\n            layers += [nn.Conv3d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=bias)]\n            layers += [nn.BatchNorm3d(num_features=out_channels)]\n            layers += [nn.ReLU()]\n            cbr = nn.Sequential(*layers)\n            return cbr\n\n        # Contracting path\n        self.enc1_1 = CBR3d(in_channels=input_nc, out_channels=64)\n        self.enc1_2 = CBR3d(in_channels=64, out_channels=64)\n        self.pool1 = nn.MaxPool3d(kernel_size=(2, 2, 1))\n\n        self.enc2_1 = CBR3d(in_channels=64, out_channels=128)\n        self.enc2_2 = CBR3d(in_channels=128, out_channels=128)\n        self.pool2 = nn.MaxPool3d(kernel_size=(2, 2, 1))\n\n        self.enc3_1 = CBR3d(in_channels=128, out_channels=256)\n        self.enc3_2 = CBR3d(in_channels=256, out_channels=256)\n        self.pool3 = nn.MaxPool3d(kernel_size=(2, 2, 1))\n\n        self.enc4_1 = CBR3d(in_channels=256, out_channels=512)\n        self.enc4_2 = CBR3d(in_channels=512, out_channels=512)\n        self.pool4 = nn.MaxPool3d(kernel_size=(2, 2, 1))\n\n        self.enc5_1 = CBR3d(in_channels=512, out_channels=1024)\n\n\n    def forward(self, x):\n        enc1_1 = self.enc1_1(x)\n        enc1_2 = self.enc1_2(enc1_1)\n        pool1 = self.pool1(enc1_2)\n\n        enc2_1 = self.enc2_1(pool1)\n        enc2_2 = self.enc2_2(enc2_1)\n        pool2 = self.pool2(enc2_2)\n\n        enc3_1 = self.enc3_1(pool2)\n        enc3_2 = self.enc3_2(enc3_1)\n        pool3 = self.pool3(enc3_2)\n\n        enc4_1 = self.enc4_1(pool3)\n        enc4_2 = self.enc4_2(enc4_1)\n        pool4 = self.pool4(enc4_2)\n\n        enc5_1 = self.enc5_1(pool4)\n\n        return enc5_1, enc4_2, enc3_2, enc2_2, enc1_2\n\n\n\n## 네트워크 구축하기\nclass UNet_Decoder(nn.Module):\n    def __init__(self, input_nc=1, output_nc=1):\n        super(UNet_Decoder, self).__init__()\n\n        def CBR3d(in_channels, out_channels, kernel_size=(3, 3, 1), stride=1, padding=(1, 1, 0), bias=True):\n            layers = []\n            layers += [nn.Conv3d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=bias)]\n            layers += [nn.BatchNorm3d(num_features=out_channels)]\n            layers += [nn.ReLU()]\n            cbr = nn.Sequential(*layers)\n            return cbr\n\n\n        # Expansive path\n        self.dec5_1 = CBR3d(in_channels=1024, out_channels=512)\n\n        self.unpool4 = nn.ConvTranspose3d(in_channels=512, out_channels=512, kernel_size=(2, 2, 1), stride=(2, 2, 1), padding=0, bias=True)\n\n        self.dec4_2 = CBR3d(in_channels=2 * 512, out_channels=512)\n        self.dec4_1 = CBR3d(in_channels=512, out_channels=256)\n\n        self.unpool3 = nn.ConvTranspose3d(in_channels=256, out_channels=256, kernel_size=(2, 2, 1), stride=(2, 2, 1), padding=0, bias=True)\n\n        self.dec3_2 = CBR3d(in_channels=2 * 256, out_channels=256)\n        self.dec3_1 = CBR3d(in_channels=256, out_channels=128)\n\n        self.unpool2 = nn.ConvTranspose3d(in_channels=128, out_channels=128, kernel_size=(2, 2, 1), stride=(2, 2, 1), padding=0, bias=True)\n\n        self.dec2_2 = CBR3d(in_channels=2 * 128, out_channels=128)\n        self.dec2_1 = CBR3d(in_channels=128, out_channels=64)\n\n        self.unpool1 = nn.ConvTranspose3d(in_channels=64, out_channels=64, kernel_size=(2, 2, 1), stride=(2, 2, 1), padding=0, bias=True)\n\n        self.dec1_2 = CBR3d(in_channels=2 * 64, out_channels=64)\n        self.dec1_1 = CBR3d(in_channels=64, out_channels=64)\n\n\n    def forward(self, enc5_1, enc4_2, enc3_2, enc2_2, enc1_2):\n\n        dec5_1 = self.dec5_1(enc5_1)\n\n        unpool4 = self.unpool4(dec5_1)\n        cat4 = torch.cat((unpool4, enc4_2), dim=1)\n        dec4_2 = self.dec4_2(cat4)\n        dec4_1 = self.dec4_1(dec4_2)\n\n        unpool3 = self.unpool3(dec4_1)\n        cat3 = torch.cat((unpool3, enc3_2), dim=1)\n        dec3_2 = self.dec3_2(cat3)\n        dec3_1 = self.dec3_1(dec3_2)\n\n        unpool2 = self.unpool2(dec3_1)\n        cat2 = torch.cat((unpool2, enc2_2), dim=1)\n        dec2_2 = self.dec2_2(cat2)\n        dec2_1 = self.dec2_1(dec2_2)\n\n        unpool1 = self.unpool1(dec2_1)\n        cat1 = torch.cat((unpool1, enc1_2), dim=1)\n        dec1_2 = self.dec1_2(cat1)\n        dec1_1 = self.dec1_1(dec1_2)\n\n        return dec1_1\n\n########################################### SEG HEAD ############################################\n\nclass Seg_Head(nn.Module):\n    def __init__(self, input_nc=1, output_nc=1):\n        super(Seg_Head, self).__init__()\n\n        # Expansive path\n        self.conv1 = nn.Conv3d(in_channels=64, out_channels=64, kernel_size=(3, 3, 1), stride=1, padding=(1, 1, 0), bias=True)\n        self.bn1   = nn.BatchNorm3d(64)\n        self.relu1 = nn.ReLU()\n\n        self.conv2 = nn.Conv3d(in_channels=64, out_channels=64, kernel_size=(3, 3, 1), stride=1, padding=(1, 1, 0), bias=True)\n        self.bn2   = nn.BatchNorm3d(64)\n        self.relu2 = nn.ReLU()\n\n        self.last  = nn.Conv3d(in_channels=64, out_channels=output_nc, kernel_size=(3, 3, 1), stride=1, padding=(1, 1, 0), bias=True)\n\n    def forward(self, x_skip):\n\n        x = self.conv1(x_skip)\n        x = self.bn1(x)\n        x = self.relu1(x)\n        \n        x = self.conv2(x)\n        x = self.bn2(x)\n        x = self.relu2(x)\n\n        x = self.last(x)\n\n        return x\n\n\n\nclass Seg_Head_Conv3d(nn.Module):\n    def __init__(self, input_nc=1, output_nc=1):\n        super(Seg_Head_Conv3d, self).__init__()\n\n        # Expansive path\n        self.conv1 = nn.Conv3d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1, bias=True)\n        self.bn1   = nn.BatchNorm3d(64)\n        self.relu1 = nn.ReLU()\n\n        self.conv2 = nn.Conv3d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1, bias=True)\n        self.bn2   = nn.BatchNorm3d(64)\n        self.relu2 = nn.ReLU()\n\n        self.last  = nn.Conv3d(in_channels=64, out_channels=output_nc, kernel_size=3, stride=1, padding=1, bias=True)\n\n    def forward(self, x_skip):\n\n        x = self.conv1(x_skip)\n        x = self.bn1(x)\n        x = self.relu1(x)\n        \n        x = self.conv2(x)\n        x = self.bn2(x)\n        x = self.relu2(x)\n\n        x = self.last(x)\n\n        return x\n\n\n\n\n\n\n\n########### CLS HEAD ####################\n\n\nclass PyramidPooling(nn.Module):\n\n    def __init__(self, pooling: Callable, levels: int = 1):\n        super().__init__()\n        self.levels = levels\n        self.pooling = pooling\n\n    def forward(self, x):\n        assert x.dim() > 2\n        shape = np.array(x.shape[2:], dtype=int)\n        batch_size = x.shape[0]\n        pyramid = []\n\n        for level in range(self.levels):\n            # adaptive pooling\n            level = 2 ** level\n            stride = np.floor(shape / level)\n            kernel_size = shape - (level - 1) * stride\n            stride, kernel_size = tuple(map(int, stride)), tuple(map(int, kernel_size))\n            temp = self.pooling(x, kernel_size=kernel_size, stride=stride)\n            # print(\"!!! = \", temp.shape)   # torch.Size([2, 1024, 1, 1, 1])\n            pyramid.append(temp.view(batch_size, -1))\n\n        return torch.cat(pyramid, dim=-1)\n\n    @staticmethod\n    def get_multiplier(levels, ndim):\n        return (2 ** (ndim * levels) - 1) // (2 ** ndim - 1)\n\nclass Cls_Head(nn.Module):\n    def __init__(self, levels=4):\n        super(Cls_Head, self).__init__()\n\n        # Expansive path\n        self.pyramid_pool  = PyramidPooling(partial(F.max_pool3d, ceil_mode=True), levels=levels)\n        self.dropout = nn.Dropout()\n        # self.linear  = nn.Linear(PyramidPooling.get_multiplier(levels=4, ndim=3) * 8, 1024)\n        self.linear  = nn.Linear(PyramidPooling.get_multiplier(levels=levels, ndim=3)*1024, 1024)\n        self.relu    = nn.ReLU()\n        self.dropout = nn.Dropout()\n        self.last    = nn.Linear(1024, 1)  # shape (N, 1)\n\n    def forward(self, x):\n\n        x = self.pyramid_pool(x)\n        x = self.dropout(x)\n        x = self.linear(x)\n        x = self.relu(x)\n        x = self.dropout(x)\n        x = self.last(x)\n\n        return x\n"}
{"type": "source_file", "path": "module/Skip_Attention_block.py", "content": "import torch.nn as nn\r\n\r\nclass SkipAttentionBlock(nn.Module):\r\n    def __init__(self, F_g, F_l, F_int):\r\n        super(SkipAttentionBlock, self).__init__()\r\n        self.W_g = nn.Sequential(\r\n            nn.Conv2d(F_g, F_int, kernel_size=1, stride=1, padding=0, bias=True),\r\n            nn.BatchNorm2d(F_int)\r\n            )\r\n        \r\n        self.W_x = nn.Sequential(\r\n            nn.Conv2d(F_l, F_int, kernel_size=1, stride=1, padding=0, bias=True),\r\n            nn.BatchNorm2d(F_int)\r\n        )\r\n\r\n        self.psi = nn.Sequential(\r\n            nn.Conv2d(F_int, 1, kernel_size=1, stride=1, padding=0, bias=True),\r\n            nn.BatchNorm2d(1),\r\n            nn.Sigmoid()\r\n        )\r\n        \r\n        self.relu = nn.ReLU(inplace=True)\r\n        \r\n    def forward(self, g, x):\r\n        # x = skip connection feature, g = down feature\r\n\r\n        g1 = self.W_g(g)\r\n        x1 = self.W_x(x)\r\n        psi = self.relu(g1+x1)\r\n        psi = self.psi(psi)\r\n        return x * psi"}
{"type": "source_file", "path": "module/ConvLSTM_block.py", "content": "import torch.nn.functional as F\nimport torch.nn as nn\nimport torch\n\nclass ConvLSTMCell(nn.Module):\n\n    def __init__(self, input_dim, hidden_dim, kernel_size, bias):\n        \"\"\"\n        Initialize ConvLSTM cell.\n        Parameters\n        ----------\n        input_dim: int\n            Number of channels of input tensor.\n        hidden_dim: int\n            Number of channels of hidden state.\n        kernel_size: (int, int)\n            Size of the convolutional kernel.\n        bias: bool\n            Whether or not to add the bias.\n        \"\"\"\n\n        super(ConvLSTMCell, self).__init__()\n\n        self.input_dim = input_dim\n        self.hidden_dim = hidden_dim\n\n        self.kernel_size = kernel_size\n        self.padding = kernel_size[0] // 2, kernel_size[1] // 2\n        self.bias = bias\n\n        self.conv = nn.Conv2d(in_channels  = self.input_dim + self.hidden_dim,\n                              out_channels = 4*self.hidden_dim, # 4를 곱하고 4개로 찢음\n                              kernel_size  = self.kernel_size,\n                              padding      = self.padding,\n                              bias         = self.bias)\n\n    def forward(self, input_tensor, cur_state):\n        h_cur, c_cur = cur_state\n\n        # h_cur 이전의 값, input_tensor 지금의 값\n        combined = torch.cat([input_tensor, h_cur], dim=1)  # concatenate along channel axis\n\n        combined_conv = self.conv(combined)\n        cc_i, cc_f, cc_o, cc_g = torch.split(combined_conv, self.hidden_dim, dim=1)\n        # gates\n        i = torch.sigmoid(cc_i)\n        f = torch.sigmoid(cc_f)\n        o = torch.sigmoid(cc_o)\n\n        # Cell 생성 gate\n        g = torch.tanh(cc_g)\n\n        c_next = f * c_cur + i * g\n        h_next = o * torch.tanh(c_next)\n\n        return h_next, c_next\n\n    def init_hidden(self, batch_size, image_size):\n        height, width = image_size\n        return (torch.zeros(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device),   # B, C, H, W 형태의 zero\n                torch.zeros(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device))\n\n\nclass ConvLSTM(nn.Module):\n\n    \"\"\"\n    Parameters:\n        input_dim: Number of channels in input\n        hidden_dim: Number of hidden channels\n        kernel_size: Size of kernel in convolutions\n        num_layers: Number of LSTM layers stacked on each other\n        batch_first: Whether or not dimension 0 is the batch or not\n        bias: Bias or no bias in Convolution\n        return_all_layers: Return the list of computations for all layers\n        Note: Will do same padding.\n    Input:\n        A tensor of size B, T, C, H, W or T, B, C, H, W\n    Output:\n        A tuple of two lists of length num_layers (or length 1 if return_all_layers is False).\n            0 - layer_output_list is the list of lists of length T of each output\n            1 - last_state_list is the list of last states\n                    each element of the list is a tuple (h, c) for hidden state and memory\n    Example:\n        >> x = torch.rand((32, 10, 64, 128, 128))\n        >> convlstm = ConvLSTM(64, 16, 3, 1, True, True, False)\n        >> _, last_states = convlstm(x)\n        >> h = last_states[0][0]  # 0 for layer index, 0 for h index\n    \"\"\"\n\n    def __init__(self, input_dim, hidden_dim, kernel_size, num_layers,\n                 batch_first=False, bias=True, return_all_layers=False):\n        super(ConvLSTM, self).__init__()\n\n        self._check_kernel_size_consistency(kernel_size)\n\n        # Make sure that both `kernel_size` and `hidden_dim` are lists having len == num_layers\n        kernel_size = self._extend_for_multilayer(kernel_size, num_layers)\n        hidden_dim = self._extend_for_multilayer(hidden_dim, num_layers)\n        if not len(kernel_size) == len(hidden_dim) == num_layers:\n            raise ValueError('Inconsistent list length.')\n\n        self.input_dim = input_dim\n        self.hidden_dim = hidden_dim\n        self.kernel_size = kernel_size\n        self.num_layers = num_layers\n        self.batch_first = batch_first\n        self.bias = bias\n        self.return_all_layers = return_all_layers\n\n        cell_list = []\n        for i in range(0, self.num_layers):\n            cur_input_dim = self.input_dim if i == 0 else self.hidden_dim[i - 1]\n\n            cell_list.append(ConvLSTMCell(input_dim=cur_input_dim,\n                                          hidden_dim=self.hidden_dim[i],\n                                          kernel_size=self.kernel_size[i],\n                                          bias=self.bias))\n\n        self.cell_list = nn.ModuleList(cell_list)\n\n    def forward(self, input_tensor, hidden_state=None):\n        \"\"\"\n        Parameters\n        ----------\n        input_tensor: todo\n            5-D Tensor either of shape (t, b, c, h, w) or (b, t, c, h, w)\n        hidden_state: todo\n            None. todo implement stateful\n        Returns\n        -------\n        last_state_list, layer_output\n        \"\"\"\n        if not self.batch_first:\n            # (t, b, c, h, w) -> (b, t, c, h, w)\n            input_tensor = input_tensor.permute(1, 0, 2, 3, 4)\n\n        b, _, _, h, w = input_tensor.size()\n\n        # Implement stateful ConvLSTM\n        if hidden_state is not None:\n            raise NotImplementedError()\n        else:\n            # Since the init is done in forward. Can send image size here\n            hidden_state = self._init_hidden(batch_size=b, image_size=(h, w))\n\n        layer_output_list = []\n        last_state_list = []\n\n        seq_len = input_tensor.size(1)\n        cur_layer_input = input_tensor\n\n        # 이놈이 복병 ㅎㅎㅎ;;;;\n        for layer_idx in range(self.num_layers):\n\n            h, c = hidden_state[layer_idx]\n            output_inner = []\n            for t in range(seq_len):\n                h, c = self.cell_list[layer_idx](input_tensor=cur_layer_input[:, t, :, :, :], cur_state=[h, c])\n                output_inner.append(h)\n\n            layer_output = torch.stack(output_inner, dim=1)\n            cur_layer_input = layer_output\n\n            layer_output_list.append(layer_output)\n            last_state_list.append([h, c])\n\n        if not self.return_all_layers:\n            layer_output_list = layer_output_list[-1]\n            last_state_list = last_state_list[-1]\n\n        return layer_output_list, last_state_list\n\n    def _init_hidden(self, batch_size, image_size):\n        init_states = []\n        for i in range(self.num_layers):\n            init_states.append(self.cell_list[i].init_hidden(batch_size, image_size))\n        return init_states\n\n    @staticmethod\n    def _check_kernel_size_consistency(kernel_size):\n        if not (isinstance(kernel_size, tuple) or\n                (isinstance(kernel_size, list) and all([isinstance(elem, tuple) for elem in kernel_size]))):\n            raise ValueError('`kernel_size` must be tuple or list of tuples')\n\n    @staticmethod\n    def _extend_for_multilayer(param, num_layers):\n        if not isinstance(param, list):\n            param = [param] * num_layers\n        return param\n\n#############################################################################################################\n# Custom CRNN\n\nclass Real_ConvLSTMCell(nn.Module):\n    def __init__(self, input_dim, hidden_dim, kernel_size, stride, padding, cnn_dropout, rnn_dropout, bias=True):\n        super(Real_ConvLSTMCell, self).__init__()\n\n        self.input_dim = input_dim\n        self.hidden_dim = hidden_dim\n        self.kernel_size = kernel_size\n        self.stride = stride\n        self.padding = padding\n        self.bias = bias\n        \n        self.input_conv = nn.Conv2d(in_channels = self.input_dim, \n                                out_channels = 4*self.hidden_dim,\n                                kernel_size = self.kernel_size,\n                                stride = self.stride,\n                                padding = self.padding,\n                                bias = self.bias)\n\n        self.rnn_conv = nn.Conv2d(in_channels = self.hidden_dim, \n                                out_channels = 4*self.hidden_dim, \n                                kernel_size = self.kernel_size,\n                                stride = 1, # reference tensorflow\n                                padding = self.padding)\n            \n        self.cnn_dropout = nn.Dropout(cnn_dropout, inplace=False)\n        self.rnn_dropout = nn.Dropout(rnn_dropout, inplace=False)\n        \n    \n    def forward(self, input_tensor, cur_state):\n        h_cur, c_cur = cur_state\n\n        # CNN\n        x = self.cnn_dropout(input_tensor)\n        x_conv = self.input_conv(x)\n        # separate i, f, c, o\n        x_i, x_f, x_c, x_o = torch.split(x_conv, self.hidden_dim, dim=1)\n        \n        # LSTM\n        h = self.rnn_dropout(h_cur)\n        h_conv = self.rnn_conv(h)\n        \n        # separate i, f, c, o\n        h_i, h_f, h_c, h_o = torch.split(h_conv, self.hidden_dim, dim=1)\n        \n        # Gate\n        f = torch.sigmoid((x_f + h_f))\n        i = torch.sigmoid((x_i + h_i))\n        o = torch.sigmoid(x_o + h_o)\n\n        c_next = f * c_cur + i * torch.tanh(x_c + h_c)\n        h_next = o * torch.tanh(c_next)\n        \n        \n        return h_next, c_next\n\n    def init_hidden(self, batch_size, image_size):\n        height, width = image_size\n        return (torch.zeros(batch_size, self.hidden_dim, height, width, device=self.input_conv.weight.device),\n                torch.zeros(batch_size, self.hidden_dim, height, width, device=self.input_conv.weight.device))\n\n\nclass Real_ConvLSTM(nn.Module):\n    def __init__(self, \n                input_dim, \n                hidden_dim, \n                kernel_size, \n                stride, \n                padding, \n                bias=False,\n                cnn_dropout=0.2, \n                rnn_dropout=0.2, \n                bidirectional=False):\n        super(Real_ConvLSTM, self).__init__()\n\n        # self.bidirectional = bidirectional\n        self.bidirectional = False\n        self.LSTM_cell = Real_ConvLSTMCell(input_dim = input_dim,\n                                        hidden_dim = hidden_dim,\n                                        kernel_size = kernel_size,\n                                        stride = stride,\n                                        padding = padding,\n                                        cnn_dropout = cnn_dropout,\n                                        rnn_dropout = rnn_dropout,\n                                        bias = bias)\n        if (self.bidirectional==True):\n            self.inverse_LSTM_cell = Real_ConvLSTMCell(input_dim = input_dim,\n                                                    hidden_dim = hidden_dim,\n                                                    kernel_size = kernel_size,\n                                                    stride = stride,\n                                                    padding = padding,\n                                                    cnn_dropout = cnn_dropout,\n                                                    rnn_dropout = rnn_dropout,\n                                                    bias = bias)\n\n    \n    def forward(self, input_tensor, hidden_state=None):\n        # input_tensor shape = (B, C, D, H, W) \n        b, _, seq_len, h, w = input_tensor.size()\n\n        # Implement stateful ConvLSTM\n        if hidden_state is not None:\n            raise NotImplementedError() # \"아직 구현하지 않은 부분입니다\"\n        else:\n            # Since the init is done in forward. Can send image size here\n            hidden_state = self._init_hidden(batch_size=b, image_size=(h, w))\n            \n            if self.bidirectional is True:\n                hidden_state_inverse = self._init_hidden(batch_size=b, image_size=(h, w))\n\n        ## LSTM forward direction\n        input_fw = input_tensor\n        h, c = hidden_state\n\n        # if stride 2\n        if (self.LSTM_cell.input_conv.stride[0] == 2):\n            h = F.max_pool3d(input=h, kernel_size=(1,3,3), stride=(1,2,2), padding=(0,1,1))\n            c = F.max_pool3d(input=c, kernel_size=(1,3,3), stride=(1,2,2), padding=(0,1,1))\n\n        output_inner = []\n        for t in range(seq_len):\n            h, c = self.LSTM_cell(input_tensor=input_fw[:, :, t, :, :], cur_state=[h, c])\n            output_inner.append(h)\n\n        output_inner = torch.stack(output_inner, dim=2)\n        \n        layer_output = output_inner\n        last_state = [h, c]\n        \n        ## LSTM inverse direction\n        if self.bidirectional is True:\n            input_inverse = input_tensor.flip(dims=[2])\n            h_inverse, c_inverse = hidden_state_inverse\n\n            # if stride 2\n            if (self.inverse_LSTM_cell.input_conv.stride[0] == 2):\n                h_inverse = F.max_pool3d(input=h_inverse, kernel_size=(1,3,3), stride=(1,2,2), padding=(0,1,1))\n                c_inverse = F.max_pool3d(input=c_inverse, kernel_size=(1,3,3), stride=(1,2,2), padding=(0,1,1))\n\n            output_inverse = []\n            for t in range(seq_len):\n                h_inverse, c_inverse = self.inverse_LSTM_cell(input_tensor=input_inverse[:, :, t, :, :], cur_state=[h_inverse, c_inverse])\n                output_inverse.append(h_inverse)\n\n            output_inverse.reverse() # 뒤집혀 있기에 다시 뒤집어야 한다..\n            output_inverse = torch.stack(output_inverse, dim=2)\n            \n            layer_output = torch.cat((output_inner, output_inverse), dim=1)\n            last_state_inverse = [h_inverse, c_inverse]\n        \n        # return layer_output, last_state, last_state_inverse if self.bidirectional is True else None\n        return layer_output\n\n    def _init_hidden(self, batch_size, image_size):\n        init_states = self.LSTM_cell.init_hidden(batch_size, image_size)\n        return init_states\n\n\n\n################################################################# BD LSTM #########################################################################\n# inverse 사용 할꺼면 input 뒤집고 output도 다시 한번 뒤집어서 Concat 해야 한다!!\n\nclass BD_ConvLSTM(nn.Module):\n    # Constructor\n    def __init__(self, input_channels=16, hidden_channels=16, kernel_size=3, stride=1, padding=1, bias=False, cnn_dropout=0.2, rnn_dropout=0.2, num_classes=2, bidirectional=True):\n        super(BD_ConvLSTM, self).__init__()\n        self.BD_convlstm_1 = Real_ConvLSTM(input_dim=input_channels, hidden_dim=hidden_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=bias, \n                                        cnn_dropout=cnn_dropout, rnn_dropout=rnn_dropout, bidirectional=bidirectional)\n        self.bn_1 = nn.InstanceNorm3d(num_features=hidden_channels)\n\n        self.BD_convlstm_2 = Real_ConvLSTM(input_dim=2*hidden_channels, hidden_dim=hidden_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=bias, \n                                        cnn_dropout=cnn_dropout, rnn_dropout=rnn_dropout, bidirectional=bidirectional)\n        self.bn_2 = nn.InstanceNorm3d(num_features=hidden_channels)\n      \n        # self.reverse_net = ConvLSTM(input_dim=input_channels, hidden_dim=hidden_channels, kernel_size=(kernel_size, kernel_size), num_layers=num_layers, bias=bias, batch_first=True, return_all_layers=False)\n        self.conv = nn.Conv3d( 2*hidden_channels, num_classes, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1) )\n\n    \n    def forward(self, x):\n        y_out = self.BD_convlstm_1(x)[0]\n        y_out = self.bn_1(y_out)\n\n        y_out = self.BD_convlstm_2(y_out)[0]\n        y_out = self.bn_2(y_out)\n\n        y_out = self.conv(y_out)\n\n        return y_out\n\n\nclass BD_LSTM(nn.Module):\n    # Constructor\n    def __init__(self, input_channels=384, hidden_channels=384, num_classes=1, bias=True, num_layers=2, dropout_p=0.4, bidirectional=True):\n        super(BD_LSTM, self).__init__()\n        self.pool = nn.AdaptiveAvgPool3d(output_size=(None, 1, 1)) # nn.AdaptiveMaxPool3d(output_size=(None, 1, 1))\n        \n        self.BD_lstm = nn.LSTM(input_size=input_channels, hidden_size=hidden_channels, num_layers=num_layers, bias=bias, batch_first=True, dropout=dropout_p, bidirectional=bidirectional)\n        # self.reverse_net = nn.LSTM(input_size=input_channels, hidden_size=hidden_channels, num_layers=num_layers, bias=bias, batch_first=True) # input 형태 (batch, seq, feature)\n\n        self.fc1 = nn.Linear( 2*hidden_channels, hidden_channels, bias=True )\n        # self.bn1 = nn.BatchNorm1d(hidden_channels, momentum=0.01)\n        # self.bn1 = nn.InstanceNorm1d(hidden_channels, momentum=0.01)\n        self.bn1 = nn.GroupNorm(num_groups=32, num_channels=hidden_channels)\n        self.relu1 = nn.ReLU(inplace=False)\n\n        self.fc2 = nn.Linear( hidden_channels, hidden_channels//2, bias=True )\n        # self.bn2 = nn.BatchNorm1d(hidden_channels//2, momentum=0.01)\n        # self.bn2 = nn.InstanceNorm1d(hidden_channels//2, momentum=0.01)\n        self.bn2 = nn.GroupNorm(num_groups=32, num_channels=hidden_channels//2)\n        self.relu2 = nn.ReLU(inplace=False)\n\n        self.dropout = nn.Dropout(p=dropout_p, inplace=False)\n        self.linear = nn.Linear( hidden_channels//2, num_classes, bias=True )\n        self.activation = nn.Sigmoid()\n\n\n    def forward(self, x):\n        x = self.pool(x)\n        x = x.view(x.shape[0], x.shape[1], -1) # (B, C, D)\n\n        # Bottle neck input은 (B, C, D) ---> LSTM input 형태 (B, Seq, Feature) 로 바꿔야 함\n        x = x.permute(0, 2, 1).contiguous()  \n\n        self.BD_lstm.flatten_parameters()  # nn.DataParallel\n        # self.reverse_net.flatten_parameters()  # nn.DataParallel\n\n        y_cat = self.BD_lstm(x)[0]\n        # y_reverse_out = self.reverse_net(x.flip(dims=[1]))[0]\n\n        # y_cat = torch.cat((y_forward_out[:, -1, :], y_reverse_out[:, -1, :]), dim=1)\n\n        y = self.relu1(self.bn1(self.fc1(y_cat[:, -1, :])))\n        y = self.relu2(self.bn2(self.fc2(y)))\n        y = self.dropout(y)\n        y = self.linear(y)\n        y = self.activation(y)\n        \n        return y"}
{"type": "source_file", "path": "module/rcnn_functions.py", "content": "import os\r\nimport numpy as np\r\nfrom PIL import Image\r\nfrom torch.utils import data\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nimport torchvision.models as models\r\nimport torchvision.transforms as transforms\r\nfrom tqdm import tqdm\r\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, pad_sequence\r\n\r\n## ------------------- label conversion tools ------------------ ##\r\ndef labels2cat(label_encoder, list):\r\n    return label_encoder.transform(list)\r\n\r\ndef labels2onehot(OneHotEncoder, label_encoder, list):\r\n    return OneHotEncoder.transform(label_encoder.transform(list).reshape(-1, 1)).toarray()\r\n\r\ndef onehot2labels(label_encoder, y_onehot):\r\n    return label_encoder.inverse_transform(np.where(y_onehot == 1)[1]).tolist()\r\n\r\ndef cat2labels(label_encoder, y_cat):\r\n    return label_encoder.inverse_transform(y_cat).tolist()\r\n\r\n\r\n## ---------------------- Dataloaders ---------------------- ##\r\n# for 3DCNN\r\nclass Dataset_3DCNN(data.Dataset):\r\n    \"Characterizes a dataset for PyTorch\"\r\n    def __init__(self, data_path, folders, labels, frames, transform=None):\r\n        \"Initialization\"\r\n        self.data_path = data_path\r\n        self.labels = labels\r\n        self.folders = folders\r\n        self.transform = transform\r\n        self.frames = frames\r\n\r\n    def __len__(self):\r\n        \"Denotes the total number of samples\"\r\n        return len(self.folders)\r\n\r\n    def read_images(self, path, selected_folder, use_transform):\r\n        X = []\r\n        for i in self.frames:\r\n            image = Image.open(os.path.join(path, selected_folder, 'frame{:06d}.jpg'.format(i))).convert('L')\r\n\r\n            if use_transform is not None:\r\n                image = use_transform(image)\r\n\r\n            X.append(image.squeeze_(0))\r\n        X = torch.stack(X, dim=0)\r\n\r\n        return X\r\n\r\n    def __getitem__(self, index):\r\n        \"Generates one sample of data\"\r\n        # Select sample\r\n        folder = self.folders[index]\r\n\r\n        # Load data\r\n        X = self.read_images(self.data_path, folder, self.transform).unsqueeze_(0)  # (input) spatial images\r\n        y = torch.LongTensor([self.labels[index]])                             # (labels) LongTensor are for int64 instead of FloatTensor\r\n\r\n        # print(X.shape)\r\n        return X, y\r\n\r\n\r\n# for CRNN\r\nclass Dataset_CRNN(data.Dataset):\r\n    \"Characterizes a dataset for PyTorch\"\r\n    def __init__(self, data_path, folders, labels, frames, transform=None):\r\n        \"Initialization\"\r\n        self.data_path = data_path\r\n        self.labels = labels\r\n        self.folders = folders\r\n        self.transform = transform\r\n        self.frames = frames\r\n\r\n    def __len__(self):\r\n        \"Denotes the total number of samples\"\r\n        return len(self.folders)\r\n\r\n    def read_images(self, path, selected_folder, use_transform):\r\n        X = []\r\n        for i in self.frames:\r\n            image = Image.open(os.path.join(path, selected_folder, 'frame{:06d}.jpg'.format(i)))\r\n\r\n            if use_transform is not None:\r\n                image = use_transform(image)\r\n\r\n            X.append(image)\r\n        X = torch.stack(X, dim=0)\r\n\r\n        return X\r\n\r\n    def __getitem__(self, index):\r\n        \"Generates one sample of data\"\r\n        # Select sample\r\n        folder = self.folders[index]\r\n\r\n        # Load data\r\n        X = self.read_images(self.data_path, folder, self.transform)     # (input) spatial images\r\n        y = torch.LongTensor([self.labels[index]])                  # (labels) LongTensor are for int64 instead of FloatTensor\r\n\r\n        # print(X.shape)\r\n        return X, y\r\n\r\n## ---------------------- end of Dataloaders ---------------------- ##\r\n\r\n\r\n\r\n## -------------------- (reload) model prediction ---------------------- ##\r\ndef Conv3d_final_prediction(model, device, loader):\r\n    model.eval()\r\n\r\n    all_y_pred = []\r\n    with torch.no_grad():\r\n        for batch_idx, (X, y) in enumerate(tqdm(loader)):\r\n            # distribute data to device\r\n            X = X.to(device)\r\n            output = model(X)\r\n            y_pred = output.max(1, keepdim=True)[1]  # location of max log-probability as prediction\r\n            all_y_pred.extend(y_pred.cpu().data.squeeze().numpy().tolist())\r\n\r\n    return all_y_pred\r\n\r\n\r\ndef CRNN_final_prediction(model, device, loader):\r\n    cnn_encoder, rnn_decoder = model\r\n    cnn_encoder.eval()\r\n    rnn_decoder.eval()\r\n\r\n    all_y_pred = []\r\n    with torch.no_grad():\r\n        for batch_idx, (X, y) in enumerate(tqdm(loader)):\r\n            # distribute data to device\r\n            X = X.to(device)\r\n            output = rnn_decoder(cnn_encoder(X))\r\n            y_pred = output.max(1, keepdim=True)[1]  # location of max log-probability as prediction\r\n            all_y_pred.extend(y_pred.cpu().data.squeeze().numpy().tolist())\r\n\r\n    return all_y_pred\r\n\r\n## -------------------- end of model prediction ---------------------- ##\r\n\r\n\r\n\r\n## ------------------------ 3D CNN module ---------------------- ##\r\ndef conv3D_output_size(img_size, padding, kernel_size, stride):\r\n    # compute output shape of conv3D\r\n    outshape = (np.floor((img_size[0] + 2 * padding[0] - (kernel_size[0] - 1) - 1) / stride[0] + 1).astype(int),\r\n                np.floor((img_size[1] + 2 * padding[1] - (kernel_size[1] - 1) - 1) / stride[1] + 1).astype(int),\r\n                np.floor((img_size[2] + 2 * padding[2] - (kernel_size[2] - 1) - 1) / stride[2] + 1).astype(int))\r\n    return outshape\r\n\r\nclass CNN3D(nn.Module):\r\n    def __init__(self, t_dim=120, img_x=90, img_y=120, drop_p=0.2, fc_hidden1=256, fc_hidden2=128, num_classes=50):\r\n        super(CNN3D, self).__init__()\r\n\r\n        # set video dimension\r\n        self.t_dim = t_dim\r\n        self.img_x = img_x\r\n        self.img_y = img_y\r\n        # fully connected layer hidden nodes\r\n        self.fc_hidden1, self.fc_hidden2 = fc_hidden1, fc_hidden2\r\n        self.drop_p = drop_p\r\n        self.num_classes = num_classes\r\n        self.ch1, self.ch2 = 32, 48\r\n        self.k1, self.k2 = (5, 5, 5), (3, 3, 3)  # 3d kernel size\r\n        self.s1, self.s2 = (2, 2, 2), (2, 2, 2)  # 3d strides\r\n        self.pd1, self.pd2 = (0, 0, 0), (0, 0, 0)  # 3d padding\r\n\r\n        # compute conv1 & conv2 output shape\r\n        self.conv1_outshape = conv3D_output_size((self.t_dim, self.img_x, self.img_y), self.pd1, self.k1, self.s1)\r\n        self.conv2_outshape = conv3D_output_size(self.conv1_outshape, self.pd2, self.k2, self.s2)\r\n\r\n        self.conv1 = nn.Conv3d(in_channels=1, out_channels=self.ch1, kernel_size=self.k1, stride=self.s1,\r\n                               padding=self.pd1)\r\n        self.bn1 = nn.BatchNorm3d(self.ch1)\r\n        self.conv2 = nn.Conv3d(in_channels=self.ch1, out_channels=self.ch2, kernel_size=self.k2, stride=self.s2,\r\n                               padding=self.pd2)\r\n        self.bn2 = nn.BatchNorm3d(self.ch2)\r\n        self.relu = nn.ReLU(inplace=True)\r\n        self.drop = nn.Dropout3d(self.drop_p)\r\n        self.pool = nn.MaxPool3d(2)\r\n        self.fc1 = nn.Linear(self.ch2 * self.conv2_outshape[0] * self.conv2_outshape[1] * self.conv2_outshape[2],\r\n                             self.fc_hidden1)  # fully connected hidden layer\r\n        self.fc2 = nn.Linear(self.fc_hidden1, self.fc_hidden2)\r\n        self.fc3 = nn.Linear(self.fc_hidden2, self.num_classes)  # fully connected layer, output = multi-classes\r\n\r\n    def forward(self, x_3d):\r\n        # Conv 1\r\n        x = self.conv1(x_3d)\r\n        x = self.bn1(x)\r\n        x = self.relu(x)\r\n        x = self.drop(x)\r\n        # Conv 2\r\n        x = self.conv2(x)\r\n        x = self.bn2(x)\r\n        x = self.relu(x)\r\n        x = self.drop(x)\r\n        # FC 1 and 2\r\n        x = x.view(x.size(0), -1)\r\n        x = F.relu(self.fc1(x))\r\n        x = F.relu(self.fc2(x))\r\n        x = F.dropout(x, p=self.drop_p, training=self.training)\r\n        x = self.fc3(x)\r\n\r\n        return x\r\n\r\n## --------------------- end of 3D CNN module ---------------- ##\r\n\r\n\r\n\r\n## ------------------------ CRNN module ---------------------- ##\r\n\r\ndef conv2D_output_size(img_size, padding, kernel_size, stride):\r\n    # compute output shape of conv2D\r\n    outshape = (np.floor((img_size[0] + 2 * padding[0] - (kernel_size[0] - 1) - 1) / stride[0] + 1).astype(int),\r\n                np.floor((img_size[1] + 2 * padding[1] - (kernel_size[1] - 1) - 1) / stride[1] + 1).astype(int))\r\n    return outshape\r\n\r\n\r\n# 2D CNN encoder train from scratch (no transfer learning)\r\nclass EncoderCNN(nn.Module):\r\n    def __init__(self, img_x=90, img_y=120, fc_hidden1=512, fc_hidden2=512, drop_p=0.3, CNN_embed_dim=300):\r\n        super(EncoderCNN, self).__init__()\r\n\r\n        self.img_x = img_x\r\n        self.img_y = img_y\r\n        self.CNN_embed_dim = CNN_embed_dim\r\n\r\n        # CNN architechtures\r\n        self.ch1, self.ch2, self.ch3, self.ch4 = 32, 64, 128, 256\r\n        self.k1, self.k2, self.k3, self.k4 = (5, 5), (3, 3), (3, 3), (3, 3)      # 2d kernal size\r\n        self.s1, self.s2, self.s3, self.s4 = (2, 2), (2, 2), (2, 2), (2, 2)      # 2d strides\r\n        self.pd1, self.pd2, self.pd3, self.pd4 = (0, 0), (0, 0), (0, 0), (0, 0)  # 2d padding\r\n\r\n        # conv2D output shapes\r\n        self.conv1_outshape = conv2D_output_size((self.img_x, self.img_y), self.pd1, self.k1, self.s1)  # Conv1 output shape\r\n        self.conv2_outshape = conv2D_output_size(self.conv1_outshape, self.pd2, self.k2, self.s2)\r\n        self.conv3_outshape = conv2D_output_size(self.conv2_outshape, self.pd3, self.k3, self.s3)\r\n        self.conv4_outshape = conv2D_output_size(self.conv3_outshape, self.pd4, self.k4, self.s4)\r\n\r\n        # fully connected layer hidden nodes\r\n        self.fc_hidden1, self.fc_hidden2 = fc_hidden1, fc_hidden2\r\n        self.drop_p = drop_p\r\n\r\n        self.conv1 = nn.Sequential(\r\n            nn.Conv2d(in_channels=3, out_channels=self.ch1, kernel_size=self.k1, stride=self.s1, padding=self.pd1),\r\n            nn.BatchNorm2d(self.ch1, momentum=0.01),\r\n            nn.ReLU(inplace=True),                      \r\n            # nn.MaxPool2d(kernel_size=2),\r\n        )\r\n        self.conv2 = nn.Sequential(\r\n            nn.Conv2d(in_channels=self.ch1, out_channels=self.ch2, kernel_size=self.k2, stride=self.s2, padding=self.pd2),\r\n            nn.BatchNorm2d(self.ch2, momentum=0.01),\r\n            nn.ReLU(inplace=True),\r\n            # nn.MaxPool2d(kernel_size=2),\r\n        )\r\n\r\n        self.conv3 = nn.Sequential(\r\n            nn.Conv2d(in_channels=self.ch2, out_channels=self.ch3, kernel_size=self.k3, stride=self.s3, padding=self.pd3),\r\n            nn.BatchNorm2d(self.ch3, momentum=0.01),\r\n            nn.ReLU(inplace=True),\r\n            # nn.MaxPool2d(kernel_size=2),\r\n        )\r\n\r\n        self.conv4 = nn.Sequential(\r\n            nn.Conv2d(in_channels=self.ch3, out_channels=self.ch4, kernel_size=self.k4, stride=self.s4, padding=self.pd4),\r\n            nn.BatchNorm2d(self.ch4, momentum=0.01),\r\n            nn.ReLU(inplace=True),\r\n            # nn.MaxPool2d(kernel_size=2),\r\n        )\r\n\r\n        self.drop = nn.Dropout2d(self.drop_p)\r\n        self.pool = nn.MaxPool2d(2)\r\n        self.fc1 = nn.Linear(self.ch4 * self.conv4_outshape[0] * self.conv4_outshape[1], self.fc_hidden1)   # fully connected layer, output k classes\r\n        self.fc2 = nn.Linear(self.fc_hidden1, self.fc_hidden2)\r\n        self.fc3 = nn.Linear(self.fc_hidden2, self.CNN_embed_dim)   # output = CNN embedding latent variables\r\n\r\n    def forward(self, x_3d):\r\n        cnn_embed_seq = []\r\n        for t in range(x_3d.size(1)):\r\n            # CNNs\r\n            x = self.conv1(x_3d[:, t, :, :, :])\r\n            x = self.conv2(x)\r\n            x = self.conv3(x)\r\n            x = self.conv4(x)\r\n            x = x.view(x.size(0), -1)           # flatten the output of conv\r\n\r\n            # FC layers\r\n            x = F.relu(self.fc1(x))\r\n            # x = F.dropout(x, p=self.drop_p, training=self.training)\r\n            x = F.relu(self.fc2(x))\r\n            x = F.dropout(x, p=self.drop_p, training=self.training)\r\n            x = self.fc3(x)\r\n            cnn_embed_seq.append(x)\r\n\r\n        # swap time and sample dim such that (sample dim, time dim, CNN latent dim)\r\n        cnn_embed_seq = torch.stack(cnn_embed_seq, dim=0).transpose_(0, 1)\r\n        # cnn_embed_seq: shape=(batch, time_step, input_size)\r\n\r\n        return cnn_embed_seq\r\n\r\n\r\n# 2D CNN encoder using ResNet-152 pretrained\r\nclass ResCNNEncoder(nn.Module):\r\n    def __init__(self, fc_hidden1=512, fc_hidden2=512, drop_p=0.3, CNN_embed_dim=300):\r\n        \"\"\"Load the pretrained ResNet-152 and replace top fc layer.\"\"\"\r\n        super(ResCNNEncoder, self).__init__()\r\n\r\n        self.fc_hidden1, self.fc_hidden2 = fc_hidden1, fc_hidden2\r\n        self.drop_p = drop_p\r\n\r\n        resnet = models.resnet152(pretrained=True)\r\n        modules = list(resnet.children())[:-1]      # delete the last fc layer.  \r\n        self.resnet = nn.Sequential(*modules)\r\n        self.fc1 = nn.Linear(resnet.fc.in_features, fc_hidden1)\r\n        self.bn1 = nn.BatchNorm1d(fc_hidden1, momentum=0.01)\r\n        self.fc2 = nn.Linear(fc_hidden1, fc_hidden2)\r\n        self.bn2 = nn.BatchNorm1d(fc_hidden2, momentum=0.01)\r\n        self.fc3 = nn.Linear(fc_hidden2, CNN_embed_dim)\r\n        \r\n    def forward(self, x_3d):\r\n        cnn_embed_seq = []\r\n        for t in range(x_3d.size(1)):\r\n            # ResNet CNN\r\n            with torch.no_grad():\r\n                x = self.resnet(x_3d[:, t, :, :, :])  # ResNet\r\n                x = x.view(x.size(0), -1)             # flatten output of conv\r\n\r\n            # FC layers\r\n            x = self.bn1(self.fc1(x))\r\n            x = F.relu(x)\r\n            x = self.bn2(self.fc2(x))\r\n            x = F.relu(x)\r\n            x = F.dropout(x, p=self.drop_p, training=self.training)\r\n            x = self.fc3(x)\r\n\r\n            cnn_embed_seq.append(x)\r\n\r\n        # swap time and sample dim such that (sample dim, time dim, CNN latent dim)\r\n        cnn_embed_seq = torch.stack(cnn_embed_seq, dim=0).transpose_(0, 1)\r\n        # cnn_embed_seq: shape=(batch, time_step, input_size)\r\n\r\n        return cnn_embed_seq\r\n\r\n\r\n\r\nclass EfficientEncoder(nn.Module):\r\n    def __init__(self, model, fc_hidden1=1024, fc_hidden2=768, drop_p=0.4, CNN_embed_dim=512):\r\n        \"\"\"Load the pretrained ResNet-152 and replace top fc layer.\"\"\"\r\n        super(EfficientEncoder, self).__init__()\r\n\r\n        self.fc_hidden1, self.fc_hidden2 = fc_hidden1, fc_hidden2\r\n        self.drop_p = drop_p\r\n        self.efficientnet = model\r\n        \r\n        # modules = list(model.children())[:-1]      # delete the last fc layer.  \r\n        # self.efficientnet = nn.Sequential(*modules)\r\n\r\n        self.fc1 = nn.Linear(640, fc_hidden1)\r\n        self.bn1 = nn.BatchNorm1d(fc_hidden1, momentum=0.01)\r\n        self.fc2 = nn.Linear(fc_hidden1, fc_hidden2)\r\n        self.bn2 = nn.BatchNorm1d(fc_hidden2, momentum=0.01)\r\n        self.fc3 = nn.Linear(fc_hidden2, CNN_embed_dim)\r\n        \r\n    def forward(self, x_3d):\r\n        cnn_embed_seq = []\r\n        seg_list = []\r\n        for t in range(x_3d.size(1)):\r\n\r\n            with torch.no_grad():\r\n                self.efficientnet.eval()\r\n                seg, cls_x = self.efficientnet.predict(x_3d[:, t, :, :, :]) \r\n            \r\n            # FC layers\r\n            x = self.bn1(self.fc1(cls_x))\r\n            x = F.relu(x)\r\n            x = self.bn2(self.fc2(x))\r\n            x = F.relu(x)\r\n            x = F.dropout(x, p=self.drop_p, training=self.training)\r\n            x = self.fc3(x)\r\n            \r\n            cnn_embed_seq.append(x)\r\n            seg_list.append(seg)\r\n\r\n        # print(\"img = \", x_3d[:, t, :, :, :].shape, \"값 = \", torch.unique(x_3d[:, t, :, :, :]))\r\n        # print(\"shape = \", cnn_embed_seq[-1].shape, \"슬라이스 맨 마지막 값은 = \", torch.unique(cnn_embed_seq[-1]))\r\n\r\n        return torch.stack(cnn_embed_seq, dim=0).transpose_(0, 1), torch.stack(seg_list, dim=0).transpose_(0, 1)  # (16, 34, 512) (batch, 길이, Eembed)  , torch.Size([배치, 32, 1, 512, 512])\r\n\r\n\r\nclass DecoderRNN(nn.Module):\r\n    def __init__(self, CNN_embed_dim=512, h_RNN_layers=3, h_RNN=512, h_FC_dim=256, drop_p=0.4, num_classes=1):\r\n        super(DecoderRNN, self).__init__()\r\n\r\n        self.RNN_input_size = CNN_embed_dim\r\n        self.h_RNN_layers = h_RNN_layers   # RNN hidden layers\r\n        self.h_RNN = h_RNN                 # RNN hidden nodes\r\n        self.h_FC_dim = h_FC_dim\r\n        self.drop_p = drop_p\r\n        self.num_classes = num_classes\r\n\r\n        self.LSTM = nn.LSTM(\r\n            input_size=self.RNN_input_size,\r\n            hidden_size=self.h_RNN,        \r\n            num_layers=h_RNN_layers,       \r\n            batch_first=True,       # input & output will has batch size as 1s dimension. e.g. (batch, time_step, input_size)\r\n        )\r\n\r\n        self.fc1 = nn.Linear(self.h_RNN, self.h_FC_dim)\r\n        self.fc2 = nn.Linear(self.h_FC_dim, self.num_classes)\r\n        self.activation = nn.Sigmoid()\r\n\r\n    def forward(self, x_RNN):\r\n        self.LSTM.flatten_parameters()\r\n        RNN_out, (h_n, h_c) = self.LSTM(x_RNN, None)  # batch, time_step, embed_size\r\n\r\n        # print(\"RNN_out\", RNN_out.shape) # torch.Size([25, 50, 512])\r\n        # print(\"RNN_out[:, -1, :]\", RNN_out[:, -1, :].shape)  # torch.Size([25, 512])\r\n\r\n        # FC layers\r\n        x = self.fc1(RNN_out[:, -1, :])   # choose RNN_out at the last time step\r\n        x = F.relu(x)\r\n        x_feature = F.dropout(x, p=self.drop_p, training=self.training)  # Classifier 마지막 Feature layer\r\n        \r\n        x = self.fc2(x_feature)\r\n        x = self.activation(x)\r\n\r\n        return x, x_feature\r\n\r\n\r\n\r\nclass DecoderRNN_uncert(nn.Module):\r\n    def __init__(self, CNN_embed_dim=512, h_RNN_layers=3, h_RNN=512, h_FC_dim=256, drop_p=0.4, num_classes=1):\r\n        super(DecoderRNN_uncert, self).__init__()\r\n\r\n        self.RNN_input_size = CNN_embed_dim\r\n        self.h_RNN_layers = h_RNN_layers   # RNN hidden layers\r\n        self.h_RNN = h_RNN                 # RNN hidden nodes\r\n        self.h_FC_dim = h_FC_dim\r\n        self.drop_p = drop_p\r\n        self.num_classes = num_classes\r\n\r\n        self.LSTM = nn.LSTM(\r\n            input_size=self.RNN_input_size,\r\n            hidden_size=self.h_RNN,        \r\n            num_layers=h_RNN_layers,       \r\n            batch_first=True,       # input & output will has batch size as 1s dimension. e.g. (batch, time_step, input_size)\r\n        )\r\n\r\n        self.fc1 = nn.Linear(self.h_RNN, self.h_FC_dim)\r\n        self.fc2 = nn.Linear(self.h_FC_dim, self.num_classes*2)\r\n\r\n\r\n\r\n\r\n\r\n    def forward(self, x_RNN):\r\n        self.LSTM.flatten_parameters()\r\n        RNN_out, (h_n, h_c) = self.LSTM(x_RNN, None)  # batch, time_step, embed_size\r\n\r\n        # print(\"RNN_out\", RNN_out.shape) # torch.Size([25, 50, 512])\r\n        # print(\"RNN_out[:, -1, :]\", RNN_out[:, -1, :].shape)  # torch.Size([25, 512])\r\n\r\n        # FC layers\r\n        x = self.fc1(RNN_out[:, -1, :])   # choose RNN_out at the last time step\r\n        x = F.relu(x)\r\n        x = F.dropout(x, p=self.drop_p, training=self.training)  # 여기 수정했음\r\n        logit = self.fc2(x)\r\n        mu, sigma = logit.split(self.num_classes, 1)  # split\r\n    \r\n        return mu, sigma\r\n\r\n\r\n\r\n\r\nclass EfficientEncoder_PAD(nn.Module):\r\n    def __init__(self, model, fc_hidden1=512, fc_hidden2=512, drop_p=0.3, CNN_embed_dim=300):\r\n        \"\"\"Load the pretrained ResNet-152 and replace top fc layer.\"\"\"\r\n        super(EfficientEncoder_PAD, self).__init__()\r\n\r\n        self.fc_hidden1, self.fc_hidden2 = fc_hidden1, fc_hidden2\r\n        self.drop_p = drop_p\r\n        self.efficientnet = model\r\n        \r\n        # modules = list(model.children())[:-1]      # delete the last fc layer.  \r\n        # self.efficientnet = nn.Sequential(*modules)\r\n\r\n        self.fc1 = nn.Linear(640, fc_hidden1)\r\n        self.bn1 = nn.BatchNorm1d(fc_hidden1, momentum=0.01)\r\n        self.fc2 = nn.Linear(fc_hidden1, fc_hidden2)\r\n        self.bn2 = nn.BatchNorm1d(fc_hidden2, momentum=0.01)\r\n        self.fc3 = nn.Linear(fc_hidden2, CNN_embed_dim)\r\n        \r\n    def forward(self, x_3d):\r\n        cnn_embed_seq = []\r\n        seg_list = []\r\n        for t in range(x_3d.size(1)):\r\n            if ( (x_3d[:, t, :, :, :] != 0).any() ):\r\n                with torch.no_grad():\r\n                    self.efficientnet.eval()\r\n                    seg, cls_x = self.efficientnet.predict(x_3d[:, t, :, :, :]) \r\n                    # check.append(cls_x)\r\n                    # x = cls_x.view(cls_x.size(0), -1)             # flatten output of conv\r\n                \r\n                # FC layers\r\n                x = self.bn1(self.fc1(cls_x))\r\n                x = F.relu(x)\r\n                x = self.bn2(self.fc2(x))\r\n                x = F.relu(x)\r\n                x = F.dropout(x, p=self.drop_p, training=self.training)\r\n                x = self.fc3(x)\r\n    \r\n                cnn_embed_seq.append(x)\r\n                seg_list.append(seg)\r\n\r\n            else :\r\n                zero = torch.zeros([x_3d.size(0), 512], dtype=torch.float32).to('cuda')\r\n                # print(\"zero dtype = \", zero.dtype, \" zero.shape= \", zero.shape, \"값\", zero)\r\n                cnn_embed_seq.append(zero)\r\n                seg_list.append(seg)\r\n                \r\n        return torch.stack(cnn_embed_seq, dim=0).transpose_(0, 1), torch.stack(seg_list, dim=0).transpose_(0, 1)  # (16, 34, 512) (batch, 길이, Eembed)  \r\n\r\n\r\nclass DecoderRNN_PAD(nn.Module):\r\n    def __init__(self, CNN_embed_dim=300, h_RNN_layers=3, h_RNN=256, h_FC_dim=128, drop_p=0.3, num_classes=50):\r\n        super(DecoderRNN_PAD, self).__init__()\r\n\r\n        self.RNN_input_size = CNN_embed_dim\r\n        self.h_RNN_layers = h_RNN_layers   # RNN hidden layers\r\n        self.h_RNN = h_RNN                 # RNN hidden nodes\r\n        self.h_FC_dim = h_FC_dim\r\n        self.drop_p = drop_p\r\n        self.num_classes = num_classes\r\n\r\n        self.LSTM = nn.LSTM(\r\n            input_size=self.RNN_input_size,\r\n            hidden_size=self.h_RNN,        \r\n            num_layers=h_RNN_layers,       \r\n            batch_first=True,       # input & output will has batch size as 1s dimension. e.g. (batch, time_step, input_size)\r\n        )\r\n\r\n        self.fc1 = nn.Linear(self.h_RNN, self.h_FC_dim)\r\n        self.fc2 = nn.Linear(self.h_FC_dim, self.num_classes)\r\n        self.activation = nn.Sigmoid()\r\n\r\n    def forward(self, x_RNN, x_lens):\r\n        self.LSTM.flatten_parameters()\r\n        total_length = x_RNN.size(1)  # get the max sequence length\r\n        x_packed = pack_padded_sequence(x_RNN, x_lens, batch_first=True, enforce_sorted=False)\r\n\r\n        # print(\"packed data\", x_packed[0])  # packed data\r\n        # print(\"batch_sizes\", x_packed[1])  # batch_sizes\r\n\r\n        output_packed, (h_n, h_c) = self.LSTM(x_packed, None)  # batch, time_step, embed_size\r\n        \r\n        # print(h_n.shape)\r\n        # print(h_n[-1].shape)\r\n\r\n        # output_padded, output_lengths = pad_packed_sequence(output_packed, batch_first=True, total_length=total_length) # 다시 (N, M, E(or H)) 로 변형\r\n        # output_padded, output_lengths = pad_packed_sequence(output_packed, batch_first=True) \r\n\r\n        # print(\"output_padded.shape\", output_padded.shape) [30, 40, 512]\r\n        # print(\"output_padded\", output_padded)\r\n        # print(\"output_padded\", output_padded[0])\r\n        \r\n\r\n        # # 여기 파트가 [:, -1, :] 부분의 마지막 히든 value를 가져온다...! 중요!!\r\n        # extract_last_hidden_list = []\r\n        # for idx, value in enumerate(output_lengths):\r\n        #     extract_last_hidden_list.append( output_padded[idx, :, :][value-1] )\r\n        # last_rnn_output = torch.stack(extract_last_hidden_list, dim=0) \r\n\r\n        # print(\"last_rnn_output.shape = \", last_rnn_output.shape)\r\n        # print(\"last_rnn_output = \", last_rnn_output[0][:10])\r\n        \r\n        # FC layers\r\n        # x = self.fc1(output_padded[:, -1, :])   # choose RNN_out at the last time step\r\n        # x = self.fc1(last_rnn_output)\r\n\r\n        x = self.fc1(h_n[-1])   \r\n        # x = self.fc1(output_packed[:, -1, :])\r\n        x = F.relu(x)\r\n        x_feature = F.dropout(x, p=self.drop_p, training=self.training)  # 여기 수정했음\r\n        x = self.fc2(x_feature)\r\n        x = self.activation(x)\r\n\r\n        return x, x_feature\r\n\r\n\r\n'''\r\n## ---------------------- end of CRNN module ---------------------- ##\r\n\r\nclass EfficientEncoderFromScratch(nn.Module):\r\n    def __init__(self, model, fc_hidden1=512, fc_hidden2=512, drop_p=0.3, CNN_embed_dim=300):\r\n        \"\"\"Load the pretrained ResNet-152 and replace top fc layer.\"\"\"\r\n        super(EfficientEncoderFromScratch, self).__init__()\r\n\r\n        self.fc_hidden1, self.fc_hidden2 = fc_hidden1, fc_hidden2\r\n        self.drop_p = drop_p\r\n\r\n        self.efficientnet = model\r\n        \r\n        # modules = list(model.children())[:-1]      # delete the last fc layer.  \r\n        # self.efficientnet = nn.Sequential(*modules)\r\n\r\n        self.fc1 = nn.Linear(640, fc_hidden1)\r\n        self.bn1 = nn.BatchNorm1d(fc_hidden1, momentum=0.01)\r\n        self.fc2 = nn.Linear(fc_hidden1, fc_hidden2)\r\n        self.bn2 = nn.BatchNorm1d(fc_hidden2, momentum=0.01)\r\n        self.fc3 = nn.Linear(fc_hidden2, CNN_embed_dim)\r\n        \r\n    def forward(self, x_3d):\r\n        cnn_embed_seq = []\r\n        for t in range(x_3d.size(1)):\r\n\r\n            seg, cls_x = self.efficientnet(x_3d[:, t, :, :, :])  # ResNet\r\n            # FC layers\r\n            x = self.bn1(self.fc1(cls_x))\r\n            x = F.relu(x)\r\n            x = self.bn2(self.fc2(x))\r\n            x = F.relu(x)\r\n            x = F.dropout(x, p=self.drop_p, training=self.training)\r\n            x = self.fc3(x)\r\n\r\n            cnn_embed_seq.append(x)\r\n\r\n        # swap time and sample dim such that (sample dim, time dim, CNN latent dim)\r\n        cnn_embed_seq = torch.stack(cnn_embed_seq, dim=0).transpose_(0, 1)\r\n        # cnn_embed_seq: shape=(batch, time_step, input_size)\r\n\r\n        return cnn_embed_seq\r\n\r\n\r\nclass EfficientEncoder_OnlySeg(nn.Module):\r\n    def __init__(self, model, fc_hidden1=512, fc_hidden2=512, fc_hidden3=512, drop_p=0.3, CNN_embed_dim=300):\r\n        \"\"\"Load the pretrained ResNet-152 and replace top fc layer.\"\"\"\r\n        super(EfficientEncoder_OnlySeg, self).__init__()\r\n\r\n        self.fc_hidden1, self.fc_hidden2, self.fc_hidden3 = fc_hidden1, fc_hidden2, fc_hidden3\r\n        self.drop_p = drop_p\r\n\r\n        self.efficientnet = model\r\n        \r\n        # modules = list(model.children())[:-1]      # delete the last fc layer.  \r\n        # self.efficientnet = nn.Sequential(*modules)\r\n\r\n        self.pool = nn.AdaptiveAvgPool2d(1)\r\n        self.dropout = nn.Dropout(p=0.5, inplace=True)\r\n        self.flatten = nn.Flatten()\r\n\r\n        self.fc1 = nn.Linear(2560, fc_hidden1)\r\n        self.bn1 = nn.BatchNorm1d(fc_hidden1, momentum=0.01)\r\n\r\n        self.fc2 = nn.Linear(fc_hidden1, fc_hidden2)\r\n        self.bn2 = nn.BatchNorm1d(fc_hidden2, momentum=0.01)\r\n\r\n        self.fc3 = nn.Linear(fc_hidden2, fc_hidden3)\r\n        self.bn3 = nn.BatchNorm1d(fc_hidden3, momentum=0.01)\r\n\r\n        self.fc4 = nn.Linear(fc_hidden3, CNN_embed_dim)\r\n        \r\n    def forward(self, x_3d):\r\n        cnn_embed_seq = []\r\n        for t in range(x_3d.size(1)):\r\n            # ResNet CNN\r\n            with torch.no_grad():\r\n                self.efficientnet.eval()\r\n                cls_x = self.efficientnet.encoder.extract_features(x_3d[:, t, :, :, :])  # ResNet\r\n                # print(\"1 = \", cls_x.shape)\r\n                cls_x = self.pool(cls_x)\r\n                # print(\"2 = \", cls_x.shape)\r\n                cls_x = self.flatten(cls_x)\r\n                # cls_x = cls_x.view(cls_x.size(0), -1)             # flatten output of conv\r\n                # print(\"3 = \", cls_x.shape)\r\n                cls_x = self.dropout(cls_x)\r\n\r\n                # print(\"cls_x.shape\", cls_x.shape) # [25, 2560, 16, 16]\r\n                # cls_x = cls_x.view(cls_x.size(0), -1)             # flatten output of conv\r\n\r\n            # FC layers\r\n            x = self.bn1(self.fc1(cls_x))\r\n            x = F.relu(x)\r\n            x = self.bn2(self.fc2(x))\r\n            x = F.relu(x)\r\n            x = self.bn3(self.fc3(x))\r\n            x = F.relu(x)            \r\n            x = F.dropout(x, p=self.drop_p, training=self.training)\r\n            x = self.fc4(x)\r\n\r\n            cnn_embed_seq.append(x)\r\n\r\n        # swap time and sample dim such that (sample dim, time dim, CNN latent dim)\r\n        cnn_embed_seq = torch.stack(cnn_embed_seq, dim=0).transpose_(0, 1)\r\n        # cnn_embed_seq: shape=(batch, time_step, input_size)\r\n\r\n        return cnn_embed_seq\r\n\r\n'''"}
{"type": "source_file", "path": "module/Non_Local_block.py", "content": "import torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\n\nclass _NonLocalBlockND(nn.Module):\n    def __init__(self, in_channels, inter_channels=None, dimension=3, sub_sample=True, bn_layer=True):\n        super(_NonLocalBlockND, self).__init__()\n\n        assert dimension in [1, 2, 3]\n\n        self.dimension = dimension\n        self.sub_sample = sub_sample\n\n        self.in_channels = in_channels\n        self.inter_channels = inter_channels\n\n        if self.inter_channels is None:\n            self.inter_channels = in_channels // 2\n            if self.inter_channels == 0:\n                self.inter_channels = 1\n\n        if dimension == 3:\n            conv_nd = nn.Conv3d\n            max_pool_layer = nn.MaxPool3d(kernel_size=(1, 2, 2))\n            bn = nn.BatchNorm3d\n        elif dimension == 2:\n            conv_nd = nn.Conv2d\n            max_pool_layer = nn.MaxPool2d(kernel_size=(2, 2))\n            bn = nn.BatchNorm2d\n        else:\n            conv_nd = nn.Conv1d\n            max_pool_layer = nn.MaxPool1d(kernel_size=(2))\n            bn = nn.BatchNorm1d\n\n        self.g = conv_nd(in_channels=self.in_channels, out_channels=self.inter_channels,\n                         kernel_size=1, stride=1, padding=0)\n\n        if bn_layer:\n            self.W = nn.Sequential(\n                conv_nd(in_channels=self.inter_channels, out_channels=self.in_channels,\n                        kernel_size=1, stride=1, padding=0),\n                bn(self.in_channels)\n            )\n            nn.init.constant_(self.W[1].weight, 0)\n            nn.init.constant_(self.W[1].bias, 0)\n        else:\n            self.W = conv_nd(in_channels=self.inter_channels, out_channels=self.in_channels,\n                             kernel_size=1, stride=1, padding=0)\n            nn.init.constant_(self.W.weight, 0)\n            nn.init.constant_(self.W.bias, 0)\n\n        self.theta = conv_nd(in_channels=self.in_channels, out_channels=self.inter_channels,\n                             kernel_size=1, stride=1, padding=0)\n\n        self.phi = conv_nd(in_channels=self.in_channels, out_channels=self.inter_channels,\n                           kernel_size=1, stride=1, padding=0)\n\n        if sub_sample:\n            self.g = nn.Sequential(self.g, max_pool_layer)\n            self.phi = nn.Sequential(self.phi, max_pool_layer)\n\n    def forward(self, x, return_nl_map=False):\n        \"\"\"\n        :param x: (b, c, t, h, w)\n        :param return_nl_map: if True return z, nl_map, else only return z.\n        :return:\n        \"\"\"\n\n        batch_size = x.size(0)\n\n        g_x = self.g(x).view(batch_size, self.inter_channels, -1)\n        g_x = g_x.permute(0, 2, 1)\n\n        theta_x = self.theta(x).view(batch_size, self.inter_channels, -1)\n        theta_x = theta_x.permute(0, 2, 1)\n        phi_x = self.phi(x).view(batch_size, self.inter_channels, -1)\n        f = torch.matmul(theta_x, phi_x)\n        N = f.size(-1)\n        f_div_C = f / N\n\n        y = torch.matmul(f_div_C, g_x)\n        y = y.permute(0, 2, 1).contiguous()\n        y = y.view(batch_size, self.inter_channels, *x.size()[2:])\n        W_y = self.W(y)\n        z = W_y + x\n\n        if return_nl_map:\n            return z, f_div_C\n        return z\n\n\nclass NONLocalBlock1D(_NonLocalBlockND):\n    def __init__(self, in_channels, inter_channels=None, sub_sample=True, bn_layer=True):\n        super(NONLocalBlock1D, self).__init__(in_channels,\n                                              inter_channels=inter_channels,\n                                              dimension=1, sub_sample=sub_sample,\n                                              bn_layer=bn_layer)\n\n\nclass NONLocalBlock2D(_NonLocalBlockND):\n    def __init__(self, in_channels, inter_channels=None, sub_sample=True, bn_layer=True):\n        super(NONLocalBlock2D, self).__init__(in_channels,\n                                              inter_channels=inter_channels,\n                                              dimension=2, sub_sample=sub_sample,\n                                              bn_layer=bn_layer)\n\n\nclass NONLocalBlock3D(_NonLocalBlockND):\n    def __init__(self, in_channels, inter_channels=None, sub_sample=True, bn_layer=True):\n        super(NONLocalBlock3D, self).__init__(in_channels,\n                                              inter_channels=inter_channels,\n                                              dimension=3, sub_sample=sub_sample,\n                                              bn_layer=bn_layer)\n\n\nif __name__ == '__main__':\n    import torch\n\n    for (sub_sample_, bn_layer_) in [(True, True), (False, False), (True, False), (False, True)]:\n        img = torch.zeros(2, 3, 20)\n        net = NONLocalBlock1D(3, sub_sample=sub_sample_, bn_layer=bn_layer_)\n        out = net(img)\n        print(out.size())\n\n        img = torch.zeros(2, 3, 20, 20)\n        net = NONLocalBlock2D(3, sub_sample=sub_sample_, bn_layer=bn_layer_)\n        out = net(img)\n        print(out.size())\n\n        img = torch.randn(2, 3, 8, 20, 20)\n        net = NONLocalBlock3D(3, sub_sample=sub_sample_, bn_layer=bn_layer_)\n        out = net(img)\n        print(out.size())\n\n\n"}
{"type": "source_file", "path": "train.py", "content": "import os\nimport argparse\nimport datetime\nimport numpy as np\nimport time\nimport torch\nimport json\nimport random\nfrom torch.utils.tensorboard import SummaryWriter\nfrom collections import defaultdict\n\nimport utils\nfrom dataset import get_dataloader\nfrom models import get_model\nfrom schedulers import get_scheduler\nfrom losses import get_loss\nfrom optimizers import get_optimizer\nfrom engine import *\nfrom accelerate.utils import gather_object\nfrom accelerate import Accelerator\nfrom accelerate import DistributedDataParallelKwargs\n\n\ndef get_args_parser():\n    parser = argparse.ArgumentParser('SMART-Net Framework Train script', add_help=False)\n\n    # Dataset parameters\n    parser.add_argument('--dataset',           default='ldctiqa',  type=str, help='Name of the dataset to be used for training and validation (e.g., \"ldctiqa\")')    \n    parser.add_argument('--train-batch-size',  default=72, type=int, help='Batch size for training data')\n    parser.add_argument('--valid-batch-size',  default=72, type=int, help='Batch size for validation data')\n    parser.add_argument('--train-num-workers', default=10, type=int, help='Number of workers for loading training data')\n    parser.add_argument('--valid-num-workers', default=10, type=int, help='Number of workers for loading validation data')\n    \n    # Model parameters\n    parser.add_argument('--model',                     default='Unet',  type=str, help='Model architecture to be used (e.g., \"Unet\")')    \n    parser.add_argument('--transfer-pretrained',       default=None,    type=str, help='Path to a pre-trained model for transfer learning')    \n    parser.add_argument('--use-pretrained-encoder',    default=True,    type=bool, help='Whether to use a pre-trained encoder (True or False)')    \n    parser.add_argument('--use-pretrained-decoder',    default=True,    type=bool, help='Whether to use a pre-trained decoder (True or False)')    \n    parser.add_argument('--freeze-encoder',            default=True,    type=bool, help='Whether to freeze encoder layers during training (True or False)')    \n    parser.add_argument('--freeze-decoder',            default=True,    type=bool, help='Whether to freeze decoder layers during training (True or False)')    \n    parser.add_argument('--roi_size',                  default=512,     type=int, help='Region of interest size for input images (e.g., 512)')    \n    parser.add_argument('--sw_batch_size',             default=32,      type=int, help='Sliding window batch size for model inference')    \n    parser.add_argument('--backbone',                  default='resnet-50',  type=str, choices=['resnet-50', 'efficientnet-b7', 'maxvit-small', 'maxvit-xlarge'], help='Backbone model for feature extraction (e.g., \"resnet-50\")')    \n    parser.add_argument('--use_skip',                  default=True,    type=bool, help='Whether to use skip connections in the model (True or False)')\n    parser.add_argument('--use_consist',               default=True,    type=bool, help='Whether to apply consistency regularization (True or False)')\n    parser.add_argument('--pool_type',                 default='gem',   type=str, help='Type of pooling to use in the model (e.g., \"gem\" for generalized mean pooling)')\n    parser.add_argument('--operator_3d',               default='lstm',  type=str, choices=['lstm', 'bert', '3d_cnn', '3d_vit'], help='3D operator to be used in the model (e.g., \"LSTM\")')\n\n    # Loss parameters\n    parser.add_argument('--loss',             default='dice_loss',  type=str, help='Loss function to be used during training (e.g., \"dice_loss\")')\n\n    # Training parameters - Optimizer, LR, Scheduler, Epoch\n    parser.add_argument('--optimizer',        default='adamw', type=str, metavar='OPTIMIZER', help='Optimizer for training (e.g., \"adamw\")')\n    parser.add_argument('--scheduler',        default='poly_lr', type=str, metavar='scheduler', help='Learning rate scheduler (e.g., \"poly_lr\" for polynomial learning rate)')\n    parser.add_argument('--epochs',           default=1000, type=int, help='Number of epochs for training (e.g., 1000 for upstream training)')\n    parser.add_argument('--lr',               default=5e-4, type=float, metavar='LR', help='Initial learning rate (default: 5e-4)')\n    parser.add_argument('--min-lr',           default=1e-5, type=float, metavar='LR', help='Minimum learning rate for the scheduler (default: 1e-5)')\n    parser.add_argument('--warmup-epochs',    default=10, type=int, metavar='N', help='Number of warmup epochs for learning rate')\n\n    # Continue Training (Resume)\n    parser.add_argument('--from-pretrained',  default='',  help='Path to pre-trained model checkpoint')\n    parser.add_argument('--resume',           default='',  help='Resume training from a checkpoint')\n\n    # DataParallel or Single GPU train\n    parser.add_argument('--multi-gpu',        default='DataParallel', choices=['DDP', 'DataParallel', 'Single'], type=str, help='Mode for multi-GPU training (e.g., \"DataParallel\")')          \n    parser.add_argument('--device',           default='cuda', help='Device to be used for training and testing (e.g., \"cuda\" for GPU)')\n\n    # Save setting\n    parser.add_argument('--save-dir',         default='', help='Directory where prediction outputs (e.g., PNG files) will be saved')\n    parser.add_argument('--time',             default='', help='for log')\n    parser.add_argument('--memo',             default='', help='Additional notes or comments for the script')\n\n    return parser\n\n\n\n# fix random seeds for reproducibility\ndef seed_everything(seed=42):\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed) \n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    np.random.seed(seed)\n    random.seed(seed)\n\n\n# MAIN\ndef main(args):\n    seed_everything()\n\n    ddp_kwargs = DistributedDataParallelKwargs(find_unused_parameters=True)\n\n    start_epoch = 0\n    \n    # Dataset\n    train_loader = get_dataloader(name=args.dataset, mode='train', batch_size=args.train_batch_size, num_workers=args.train_num_workers, roi_size=args.roi_size, operator_3d=args.operator_3d)\n    valid_loader = get_dataloader(name=args.dataset, mode='valid', batch_size=args.valid_batch_size, num_workers=args.valid_num_workers, roi_size=args.roi_size, operator_3d=args.operator_3d)\n\n    # Model\n    model = get_model(args)\n    \n    # Pretrained\n    if args.from_pretrained:\n        print(\"Loading... Pretrained\")\n        checkpoint = torch.load(args.from_pretrained)\n        model.load_state_dict(checkpoint['model_state_dict'])\n\n    # Optimizer & LR Schedule & Loss\n    optimizer = get_optimizer(name=args.optimizer, model=model, lr=args.lr)\n    scheduler = get_scheduler(name=args.scheduler, optimizer=optimizer, warm_up_epoch=10, start_decay_epoch=args.epochs/10, total_epoch=args.epochs, min_lr=1e-6)\n    criterion = get_loss(name=args.loss)\n\n    # Resume\n    if args.resume:\n        print(\"Loading... Resume\")\n        start_epoch, model, optimizer, scheduler = utils.load_checkpoint(model, optimizer, scheduler, filename=args.resume)\n\n    # Multi-GPU & CUDA\n    if args.multi_gpu == 'DataParallel':\n        device = torch.device(args.device)\n        model  = torch.nn.DataParallel(model)\n        model  = model.to(device)\n    elif args.multi_gpu == 'DDP':\n        device      = None\n        accelerator = Accelerator(kwargs_handlers=[ddp_kwargs])\n        # accelerator.init_trackers(\"SMART-Net\")\n        train_loader, valid_loader, model, optimizer = accelerator.prepare(train_loader, \n                                                                           valid_loader, \n                                                                           model, \n                                                                           optimizer)\n    elif args.multi_gpu == 'Single':\n        model = model.to(device)\n\n    # Tensorboard\n    tensorboard = SummaryWriter(args.save_dir + '/runs')\n    \n    if accelerator.is_main_process and args.multi_gpu == 'DDP':\n        print(torch.__version__)\n        print(torch.backends.cudnn.version())    \n        utils.print_args(args)\n        print(f\"Start training for {args.epochs} epochs\")\n\n    # Whole Loop Train & Valid \n    start_time = time.time()\n    for epoch in range(start_epoch, args.epochs):\n\n        # 2D\n        if args.model == 'SMART-Net-2D':\n            accelerator.wait_for_everyone()\n            train_stats = train_smartnet_2d(train_loader, model, criterion, optimizer, device, epoch, args.use_consist, args.multi_gpu, accelerator)\n            if accelerator.is_main_process and args.multi_gpu == 'DDP':\n                print(\"Averaged train_stats: \", train_stats)\n                for key, value in train_stats.items():\n                    tensorboard.add_scalar(f'Train Stats/{key}', value, epoch)\n            accelerator.wait_for_everyone()\n            valid_stats = valid_smartnet_2d(valid_loader, model, device, epoch, args.save_dir, args.use_consist, args.multi_gpu, accelerator)\n            if accelerator.is_main_process and args.multi_gpu == 'DDP':\n                print(\"Averaged valid_stats: \", valid_stats)\n                for key, value in valid_stats.items():\n                    tensorboard.add_scalar(f'Valid Stats/{key}', value, epoch)\n        \n        # 3D - 2D transfer\n        elif args.model == 'SMART-Net-3D-CLS':\n            train_stats = train_smartnet_3d_2dtransfer_CLS(train_loader, model, criterion, optimizer, device, epoch, args.multi_gpu, accelerator)\n            if accelerator.is_main_process and args.multi_gpu == 'DDP':\n                print(\"Averaged train_stats: \", train_stats)\n                for key, value in train_stats.items():\n                    tensorboard.add_scalar(f'Train Stats/{key}', value, epoch)\n            valid_stats = valid_smartnet_3d_2dtransfer_CLS(valid_loader, model, device, epoch, args.save_dir, args.multi_gpu, accelerator)\n            if accelerator.is_main_process and args.multi_gpu == 'DDP':\n                print(\"Averaged valid_stats: \", valid_stats)\n                for key, value in valid_stats.items():\n                    tensorboard.add_scalar(f'Valid Stats/{key}', value, epoch)\n\n        elif args.model == 'SMART-Net-3D-SEG':\n            train_stats = train_smartnet_3d_2dtransfer_SEG(train_loader, model, criterion, optimizer, device, epoch, args.multi_gpu, accelerator)\n            print(\"Averaged train_stats: \", train_stats)\n            for key, value in train_stats.items():\n                tensorboard.add_scalar(f'Train Stats/{key}', value, epoch)            \n            valid_stats = valid_smartnet_3d_2dtransfer_SEG(valid_loader, model, device, epoch, args.save_dir, args.multi_gpu, accelerator)\n            print(\"Averaged valid_stats: \", valid_stats)\n            for key, value in valid_stats.items():\n                tensorboard.add_scalar(f'Valid Stats/{key}', value, epoch)                \n\n        # LR update\n        scheduler.step()\n\n        # Save checkpoint\n        if args.multi_gpu == 'Single':\n            model_state_dict = model.state_dict()\n            torch.save({\n                'epoch': epoch,\n                'model_state_dict': model_state_dict,\n                'optimizer': optimizer.state_dict(),\n                'scheduler': scheduler.state_dict(),\n            }, args.save_dir + '/weights/epoch_' + str(epoch) + '_checkpoint.pth')\n        \n        elif args.multi_gpu == 'DataParallel':\n            model_state_dict = model.module.state_dict()\n            torch.save({\n                'epoch': epoch,\n                'model_state_dict': model_state_dict,\n                'optimizer': optimizer.state_dict(),\n                'scheduler': scheduler.state_dict(),\n            }, args.save_dir + '/weights/epoch_' + str(epoch) + '_checkpoint.pth')\n        \n        elif args.multi_gpu == 'DDP':\n            accelerator.wait_for_everyone()\n            accelerator.save({\n                'epoch': epoch,\n                'model_state_dict': accelerator.unwrap_model(model).state_dict(),\n                'optimizer': optimizer.state_dict(),\n                'scheduler': scheduler.state_dict(),\n            }, args.save_dir + '/weights/epoch_' + str(epoch) + '_checkpoint.pth')\n\n        else:\n            raise ValueError(f\"Invalid multi_gpu mode: {args.multi_gpu}\")\n\n        if accelerator.is_main_process and args.multi_gpu == 'DDP':\n            # Log text\n            log_stats = {**{f'{k}': v for k, v in train_stats.items()}, \n                        **{f'{k}': v for k, v in valid_stats.items()}, \n                        'epoch': epoch,\n                        'lr': optimizer.param_groups[0]['lr']}\n\n            with open(args.save_dir + \"/logs/log_\" + args.time + \".txt\", \"a\") as f:\n                f.write(json.dumps(log_stats) + \"\\n\")\n        \n    # Finish\n    tensorboard.close()\n    total_time_str = str(datetime.timedelta(seconds=int(time.time()-start_time)))\n    print('Training time {}'.format(total_time_str))\n    accelerator.end_training()\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser('train script', parents=[get_args_parser()])\n    args = parser.parse_args()\n\n\n    # Make folder if not exist\n    os.makedirs(args.save_dir, exist_ok=True)\n    os.makedirs(args.save_dir + \"/args\", exist_ok=True)\n    os.makedirs(args.save_dir + \"/weights\", exist_ok=True)\n    os.makedirs(args.save_dir + \"/predictions\", exist_ok=True)\n    os.makedirs(args.save_dir + \"/runs\", exist_ok=True)\n    os.makedirs(args.save_dir + \"/logs\", exist_ok=True)\n\n    # Save args to json\n    the_time = datetime.datetime.now().strftime(\"%y%m%d_%H%M\")\n    if not os.path.isfile(args.save_dir + \"/args/args_\" + the_time + \".json\"):\n        with open(args.save_dir + \"/args/args_\" + the_time + \".json\", \"w\") as f:\n            json.dump(args.__dict__, f, indent=2)\n\n    args.time = the_time\n       \n    main(args)"}
{"type": "source_file", "path": "utils.py", "content": "\"\"\"\nMisc functions, including distributed helpers.\nMostly copy-paste from torchvision references.\n\"\"\"\nfrom collections import defaultdict, OrderedDict\nimport os\nimport torch\n\n\n# Please make a code for AverageMeter. All indicators and losses are stored in dictionary form. Track a series of values and provide access to smoothed values over a window or the global series average.\nfrom collections import defaultdict\nimport matplotlib.pyplot as plt\n\n\n# Setting...!\nfn_denorm  = lambda x: (x * 0.5) + 0.5\nfn_tonumpy = lambda x: x.detach().cpu().numpy()\n\n\nclass AverageMeter:\n    def __init__(self, **kwargs):\n        self.reset()\n\n    def reset(self):\n        self.data = defaultdict(lambda: {'sum': 0, 'count': 0})\n\n    def update(self, key, value, n):\n        self.data[key]['sum']   += value * n\n        self.data[key]['count'] += n\n    \n    def average(self):\n        return {k: v['sum'] / v['count'] for k, v in self.data.items()}\n\n\n# Check the resume point\ndef load_checkpoint(model, optimizer, scheduler, filename='checkpoint.pth'):\n    start_epoch = 0\n    best_loss   = 1000\n\n    if os.path.isfile(filename):\n        checkpoint  = torch.load(filename)\n        start_epoch = checkpoint['epoch'] + 1\n        \n        model.load_state_dict(checkpoint['model_state_dict'])\n        optimizer.load_state_dict(checkpoint['optimizer'])\n        scheduler.load_state_dict(checkpoint['scheduler'])\n        print(\"=> loaded checkpoint '{}' (epoch {})\".format(filename, checkpoint['epoch']))\n\n    else:\n        print(\"=> no checkpoint found at '{}'\".format(filename))\n\n    return start_epoch, model, optimizer, scheduler\n\n\ndef fix_optimizer(optimizer):\n    # Optimizer Error fix...!\n    for state in optimizer.state.values():\n        for k, v in state.items():\n            if torch.is_tensor(v):\n                state[k] = v.cuda()\n\ndef str2bool(value):\n    value = value.lower()\n    if value in ['true', '1', 'yes', 'y', 'on']:\n        return True\n    elif value in ['false', '0', 'no', 'n', 'off']:\n        return False\n    else:\n        raise ValueError(f\"Invalid boolean value: {value}\")\n\ndef check_checkpoint_if_wrapper(model_state_dict):\n    if list(model_state_dict.keys())[0].startswith('module'):\n        return OrderedDict({k.replace('module.', ''): v for k, v in model_state_dict.items()}) # 'module.' 제거\n    else:\n        return model_state_dict\n\ndef check_2d_data_device(multi_gpu, device, image, label, mask):\n    if multi_gpu == 'DDP':\n        image = image\n        label = label\n        mask  = mask\n    else:\n        image = image.to(device).float()\n        label = label.to(device).float()\n        mask  = mask.to(device).float()\n    return image, label, mask\n\ndef check_3d_cls_data_device(multi_gpu, device, image, label):\n    if multi_gpu == 'DDP':\n        image = image\n        label = label\n    else:\n        image = image.to(device).float()\n        label = label.to(device).float()\n    return image, label\n\ndef check_3d_seg_data_device(multi_gpu, device, image, mask):\n    if multi_gpu == 'DDP':\n        image = image\n        mask  = mask\n    else:\n        image = image.to(device).float()\n        mask  = mask.to(device).float()\n    return image, mask\n\ndef collect_dict_all_processor(processor, average=True):\n    merged_dict = defaultdict(list)\n    for i in processor:\n        for k, v in i.items():\n            merged_dict[k].append(v)\n    # Average\n    if average:\n        for k, v in merged_dict.items():\n            merged_dict[k] = sum(v) / len(v)\n    return merged_dict\n\ndef save_image_and_prediction(image, mask, pred_seg, pred_rec, save_dir, epoch, index=0):\n    # SAVE SEG\n    image_png = fn_tonumpy(fn_denorm(mask[index]))  # B, C, H, W\n    plt.imsave(f'{save_dir}/predictions/epoch_{epoch}_mask.png', image_png[0], cmap='gray')\n\n    pred_png = fn_tonumpy(fn_denorm(pred_seg[index])).round()\n    plt.imsave(f'{save_dir}/predictions/epoch_{epoch}_pred_seg.png', pred_png[0], cmap='gray')\n\n    # SAVE REC\n    image_png = fn_tonumpy(fn_denorm(image[index]))  # B, C, H, W\n    plt.imsave(f'{save_dir}/predictions/epoch_{epoch}_image.png', image_png[0], cmap='gray')\n\n    pred_png = fn_tonumpy(fn_denorm(pred_rec[index]))\n    plt.imsave(f'{save_dir}/predictions/epoch_{epoch}_pred_rec.png', pred_png[0], cmap='gray')\n\n\ndef print_args(args):\n    print('***********************************************')\n    print('Dataset Name:   ', args.dataset)\n    print('---------- Model --------------')\n    print('Model Name:     ', args.model)\n    print('Resume From:    ', args.resume)\n    print('Save To:        ', args.save_dir)\n    print('Available CPUs: ', os.cpu_count())\n    print('---------- Loss ---------------')\n    print('Loss Name:      ', args.loss)\n    print('---------- Optimizer ----------')\n    print('Optimizer Name: ', args.optimizer)\n    print('Learning Rate:  ', args.lr)\n    print('Scheduler Name: ', args.scheduler)\n    print('Train Batchsize:      ', args.train_batch_size)\n    print('Valid Batchsize:      ', args.valid_batch_size)\n    print('Total Epoch:    ', args.epochs)\n    \n\ndef print_args_test(args):\n    print('***********************************************')\n    print('Dataset Name:   ', args.dataset)\n    print('---------- Model --------------')\n    print('Model Name:     ', args.model)\n    print('Resume From:    ', args.resume)\n    print('Save To:        ', args.save_dir)\n    print('Available CPUs: ', os.cpu_count())\n"}
{"type": "source_file", "path": "optimizers.py", "content": "'''\nDeclares the Resnet-50 model using either the torchvision or the huggingface package. Note that input is one-channel.\n'''\n\nimport torch\n\ndef get_optimizer(name, model, lr=1e-4):\n    if name == 'adam':\n        optimizer = torch.optim.Adam(params=model.parameters(), lr=lr, betas=(0.9, 0.999), eps=1e-08, weight_decay=5e-4, amsgrad=False)\n\n    elif name == 'adamw':\n        optimizer = torch.optim.AdamW(params=model.parameters(), lr=lr, betas=(0.9, 0.999), eps=1e-08, weight_decay=5e-4, amsgrad=False)\n\n    else :\n        raise KeyError(\"Wrong optim name `{}`\".format(name))        \n\n    return optimizer"}
{"type": "source_file", "path": "module/min_norm_solvers.py", "content": "import numpy as np\nimport torch\n\n\n# This code is from\n# Multi-Task Learning as Multi-Objective Optimization\n# Ozan Sener, Vladlen Koltun\n# Neural Information Processing Systems (NeurIPS) 2018\n# https://github.com/intel-isl/MultiObjectiveOptimization\nclass MinNormSolver:\n    MAX_ITER = 250\n    STOP_CRIT = 1e-5\n\n    @staticmethod\n    def _min_norm_element_from2(v1v1, v1v2, v2v2):\n        \"\"\"\n        Analytical solution for min_{c} |cx_1 + (1-c)x_2|_2^2\n        d is the distance (objective) optimzed\n        v1v1 = <x1,x1>\n        v1v2 = <x1,x2>\n        v2v2 = <x2,x2>\n        \"\"\"\n        if v1v2 >= v1v1:\n            # Case: Fig 1, third column\n            gamma = 0.999\n            cost = v1v1\n            return gamma, cost\n        if v1v2 >= v2v2:\n            # Case: Fig 1, first column\n            gamma = 0.001\n            cost = v2v2\n            return gamma, cost\n        # Case: Fig 1, second column\n        gamma = -1.0 * ((v1v2 - v2v2) / (v1v1 + v2v2 - 2 * v1v2))\n        cost = v2v2 + gamma * (v1v2 - v2v2)\n        return gamma, cost\n\n    @staticmethod\n    def _min_norm_2d(vecs, dps):\n        \"\"\"\n        Find the minimum norm solution as combination of two points\n        This is correct only in 2D\n        ie. min_c |\\sum c_i x_i|_2^2 st. \\sum c_i = 1 , 1 >= c_1 >= 0 for all i, c_i + c_j = 1.0 for some i, j\n        \"\"\"\n        dmin = 1e8\n        for i in range(len(vecs)):\n            for j in range(i + 1, len(vecs)):\n                if (i, j) not in dps:\n                    dps[(i, j)] = 0.0\n                    for k in range(len(vecs[i])):\n                        dps[(i, j)] += torch.dot(\n                            vecs[i][k], vecs[j][k]\n                        ).item()  # torch.dot(vecs[i][k], vecs[j][k]).dataset[0]\n                    dps[(j, i)] = dps[(i, j)]\n                if (i, i) not in dps:\n                    dps[(i, i)] = 0.0\n                    for k in range(len(vecs[i])):\n                        dps[(i, i)] += torch.dot(\n                            vecs[i][k], vecs[i][k]\n                        ).item()  # torch.dot(vecs[i][k], vecs[i][k]).dataset[0]\n                if (j, j) not in dps:\n                    dps[(j, j)] = 0.0\n                    for k in range(len(vecs[i])):\n                        dps[(j, j)] += torch.dot(\n                            vecs[j][k], vecs[j][k]\n                        ).item()  # torch.dot(vecs[j][k], vecs[j][k]).dataset[0]\n                c, d = MinNormSolver._min_norm_element_from2(\n                    dps[(i, i)], dps[(i, j)], dps[(j, j)]\n                )\n                if d < dmin:\n                    dmin = d\n                    sol = [(i, j), c, d]\n        return sol, dps\n\n    @staticmethod\n    def _projection2simplex(y):\n        \"\"\"\n        Given y, it solves argmin_z |y-z|_2 st \\sum z = 1 , 1 >= z_i >= 0 for all i\n        \"\"\"\n        m = len(y)\n        sorted_y = np.flip(np.sort(y), axis=0)\n        tmpsum = 0.0\n        tmax_f = (np.sum(y) - 1.0) / m\n        for i in range(m - 1):\n            tmpsum += sorted_y[i]\n            tmax = (tmpsum - 1) / (i + 1.0)\n            if tmax > sorted_y[i + 1]:\n                tmax_f = tmax\n                break\n        return np.maximum(y - tmax_f, np.zeros(y.shape))\n\n    @staticmethod\n    def _next_point(cur_val, grad, n):\n        proj_grad = grad - (np.sum(grad) / n)\n        tm1 = -1.0 * cur_val[proj_grad < 0] / proj_grad[proj_grad < 0]\n        tm2 = (1.0 - cur_val[proj_grad > 0]) / (proj_grad[proj_grad > 0])\n\n        skippers = np.sum(tm1 < 1e-7) + np.sum(tm2 < 1e-7)\n        t = 1\n        if len(tm1[tm1 > 1e-7]) > 0:\n            t = np.min(tm1[tm1 > 1e-7])\n        if len(tm2[tm2 > 1e-7]) > 0:\n            t = min(t, np.min(tm2[tm2 > 1e-7]))\n\n        next_point = proj_grad * t + cur_val\n        next_point = MinNormSolver._projection2simplex(next_point)\n        return next_point\n\n    @staticmethod\n    def find_min_norm_element(vecs):\n        \"\"\"\n        Given a list of vectors (vecs), this method finds the minimum norm element in the convex hull\n        as min |u|_2 st. u = \\sum c_i vecs[i] and \\sum c_i = 1.\n        It is quite geometric, and the main idea is the fact that if d_{ij} = min |u|_2 st u = c x_i + (1-c) x_j; the solution lies in (0, d_{i,j})\n        Hence, we find the best 2-task solution, and then run the projected gradient descent until convergence\n        \"\"\"\n        # Solution lying at the combination of two points\n        dps = {}\n        init_sol, dps = MinNormSolver._min_norm_2d(vecs, dps)\n\n        n = len(vecs)\n        sol_vec = np.zeros(n)\n        sol_vec[init_sol[0][0]] = init_sol[1]\n        sol_vec[init_sol[0][1]] = 1 - init_sol[1]\n\n        if n < 3:\n            # This is optimal for n=2, so return the solution\n            return sol_vec, init_sol[2]\n\n        iter_count = 0\n\n        grad_mat = np.zeros((n, n))\n        for i in range(n):\n            for j in range(n):\n                grad_mat[i, j] = dps[(i, j)]\n\n        while iter_count < MinNormSolver.MAX_ITER:\n            grad_dir = -1.0 * np.dot(grad_mat, sol_vec)\n            new_point = MinNormSolver._next_point(sol_vec, grad_dir, n)\n            # Re-compute the inner products for line search\n            v1v1 = 0.0\n            v1v2 = 0.0\n            v2v2 = 0.0\n            for i in range(n):\n                for j in range(n):\n                    v1v1 += sol_vec[i] * sol_vec[j] * dps[(i, j)]\n                    v1v2 += sol_vec[i] * new_point[j] * dps[(i, j)]\n                    v2v2 += new_point[i] * new_point[j] * dps[(i, j)]\n            nc, nd = MinNormSolver._min_norm_element_from2(v1v1, v1v2, v2v2)\n            new_sol_vec = nc * sol_vec + (1 - nc) * new_point\n            change = new_sol_vec - sol_vec\n            if np.sum(np.abs(change)) < MinNormSolver.STOP_CRIT:\n                return sol_vec, nd\n            sol_vec = new_sol_vec\n\n    @staticmethod\n    def find_min_norm_element_FW(vecs):\n        \"\"\"\n        Given a list of vectors (vecs), this method finds the minimum norm element in the convex hull\n        as min |u|_2 st. u = \\sum c_i vecs[i] and \\sum c_i = 1.\n        It is quite geometric, and the main idea is the fact that if d_{ij} = min |u|_2 st u = c x_i + (1-c) x_j; the solution lies in (0, d_{i,j})\n        Hence, we find the best 2-task solution, and then run the Frank Wolfe until convergence\n        \"\"\"\n        # Solution lying at the combination of two points\n        dps = {}\n        init_sol, dps = MinNormSolver._min_norm_2d(vecs, dps)\n\n        n = len(vecs)\n        sol_vec = np.zeros(n)\n        sol_vec[init_sol[0][0]] = init_sol[1]\n        sol_vec[init_sol[0][1]] = 1 - init_sol[1]\n\n        if n < 3:\n            # This is optimal for n=2, so return the solution\n            return sol_vec, init_sol[2]\n\n        iter_count = 0\n\n        grad_mat = np.zeros((n, n))\n        for i in range(n):\n            for j in range(n):\n                grad_mat[i, j] = dps[(i, j)]\n\n        while iter_count < MinNormSolver.MAX_ITER:\n            t_iter = np.argmin(np.dot(grad_mat, sol_vec))\n\n            v1v1 = np.dot(sol_vec, np.dot(grad_mat, sol_vec))\n            v1v2 = np.dot(sol_vec, grad_mat[:, t_iter])\n            v2v2 = grad_mat[t_iter, t_iter]\n\n            nc, nd = MinNormSolver._min_norm_element_from2(v1v1, v1v2, v2v2)\n            new_sol_vec = nc * sol_vec\n            new_sol_vec[t_iter] += 1 - nc\n\n            change = new_sol_vec - sol_vec\n            if np.sum(np.abs(change)) < MinNormSolver.STOP_CRIT:\n                return sol_vec, nd\n            sol_vec = new_sol_vec\n\n\ndef gradient_normalizers(grads, losses, normalization_type):\n    gn = {}\n    if normalization_type == \"norm\":\n        for t in grads:\n            gn[t] = np.sqrt(np.sum([gr.pow(2).sum().data[0] for gr in grads[t]]))\n    elif normalization_type == \"loss\":\n        for t in grads:\n            gn[t] = losses[t]\n    elif normalization_type == \"loss+\":\n        for t in grads:\n            gn[t] = losses[t] * np.sqrt(\n                np.sum([gr.pow(2).sum().data[0] for gr in grads[t]])\n            )\n    elif normalization_type == \"none\":\n        for t in grads:\n            gn[t] = 1.0\n    else:\n        print(\"ERROR: Invalid Normalization Type\")\n    return gn\n"}
{"type": "source_file", "path": "schedulers.py", "content": "'''\nDeclares the Simple Optimizer & Scheduler for training.\n'''\n\nimport torch\nimport functools\n\ndef poly_learning_rate(epoch, warm_up_epoch, start_decay_epoch, total_epoch, min_lr):\n    # Linear Warmup\n    if (epoch < warm_up_epoch):\n        return max(0, epoch / warm_up_epoch)\n    else :\n        lr = 1.0 - max(0, epoch - start_decay_epoch) /(float(total_epoch) - start_decay_epoch)\n\n        if lr <= min_lr:\n            lr = min_lr\n\n    return lr\n\n\ndef get_scheduler(name, optimizer, warm_up_epoch=10, start_decay_epoch=1000/10, total_epoch=1000, min_lr=1e-6):\n    if name == 'poly_lr':\n        lr_scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=functools.partial(poly_learning_rate, warm_up_epoch=warm_up_epoch, start_decay_epoch=start_decay_epoch, total_epoch=total_epoch, min_lr=min_lr))\n\n    elif name == 'ReduceLROnPlateau':\n        lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)\n\n    elif name == 'CosineAnnealingLR':\n        lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=total_epoch)\n\n    else :\n        raise KeyError(\"Wrong scheduler name `{}`\".format(name))        \n\n\n    return lr_scheduler"}
