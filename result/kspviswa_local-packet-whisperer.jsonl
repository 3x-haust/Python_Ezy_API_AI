{"repo_info": {"repo_name": "local-packet-whisperer", "repo_owner": "kspviswa", "repo_url": "https://github.com/kspviswa/local-packet-whisperer"}}
{"type": "source_file", "path": "bin/lpw_agent.py", "content": "import streamlit as st\nfrom lpw_init import *\nfrom crewai import Agent, Task, Crew, LLM\nimport yaml\n\nclass LPWCrew:\n    def __init__(self, llm_host=\"127.0.0.1\", llm_port=\"11434\", model=\"llama3.1:latest\"):\n        self.llm_host = llm_host\n        self.llm_port = llm_port\n        self.model = model\n        self.llm = LLM(model=f'ollama/{model}', base_url=f'http://{llm_host}:{llm_port}', api_key='could be anything')\n        self.loadConfig()\n        self.crew = Crew(\n            agents = [self.sne_agent],\n            tasks = [self.caTask]\n        )\n        print('#### Crew initialized')\n    \n    def kickoff(self, data, protocol) -> str:\n        result = self.crew.kickoff(inputs={'pcap_data' : data, 'protocol' : protocol})\n        return result.raw\n    \n    def loadConfig(self):\n        agent_data = yaml.safe_load(returnValue('agent_config_file')) if returnValue('agent_config_file') else yaml.safe_load(returnValue('default_agent_config'))\n        self.sne_agent = Agent(\n            role = agent_data['role'],\n            goal = agent_data['goal'],\n            backstory=agent_data['backstory'],\n            llm=self.llm,\n            #verbose=True\n        )\n        print(f\"##### {getLpwPath('temp')}/insights.md\")\n        self.caTask = Task(\n            description=agent_data['tasks'][0]['description'],\n            expected_output=agent_data['tasks'][0]['expected_output'],\n            #output_file=f'{getLpwPath('temp')}/insights.md',\n            agent=self.sne_agent\n        )\n\n\nif __name__ == '__main__':\n    with open('temp/out.txt', 'r') as f:\n        ac = LPWCrew()\n        result = ac.kickoff(f.read(), 'ngap')\n        print(f'Result : {result}')"}
{"type": "source_file", "path": "bin/lpw_home.py", "content": "import streamlit as st\nfrom lpw_init import *\nfrom lpw_prompt import *\nfrom lpw_packet import *\nfrom lpw_agent import LPWCrew\nimport os\nimport time\nfrom streamlit_extras.tags import tagger_component\nfrom importlib.metadata import version, PackageNotFoundError\nimport os\n\ndef get_lpw_version():\n    try:\n        return version(\"lpw\")\n    except PackageNotFoundError:\n        # Fallback to reading from version.txt\n        current_dir = os.path.dirname(os.path.abspath(__file__))\n        version_file = os.path.join(current_dir, \"../VERSION.txt\")\n        with open(version_file, \"r\") as f:\n            return f.read().strip()\n\nlpw_avatar = \"https://raw.githubusercontent.com/kspviswa/local-packet-whisperer/main/gifs/lpw_logo_small.png\"\n\ndef loadDefaultSettings():\n    model_list, server_connected = getModelList()\n    if server_connected:\n        st.session_state['selected_model'] = model_list[0]\n    st.session_state['llm_server_connection_status'] = server_connected\n\ndef renderConnection(is_connected: bool):\n    return '🟢' if is_connected else '🔴'\n\ndef getFiltersAndDecodeInfo():\n    filters = []\n    decodes = {}\n    resp = \"\"\n    if returnValue('http'):\n        filters.append(\"tcp.port == 80\")\n        decodes['tcp.port == 80'] = 'http'\n    if returnValue('snmp'):\n        filters.append(\"udp.port == 161 || udp.port == 162\")\n        decodes['udp.port == 161'] = 'snmp'\n        decodes['udp.port == 162'] = 'snmp'\n    if returnValue('https'):\n        filters.append(\"tcp.port == 443\")\n        decodes['tcp.port == 443'] = 'https'\n    if returnValue('ntp'):\n        filters.append(\"udp.port == 123\")\n        decodes['udp.port == 123'] = 'ntp'    \n    if returnValue('ftp'):\n        filters.append(\"tcp.port == 21\")\n        decodes['tcp.port == 21'] = 'ftp'\n    if returnValue('ssh'):\n        filters.append(\"tcp.port == 22\")\n        decodes['tcp.port == 22'] = 'ssh'\n    if returnValue('ngap'):\n        filters.append(\"sctp.port == 38412\")\n        decodes['stcp.port == 38412'] = 'ngap'\n    \n    l = len(filters)\n    i=0\n    for f in filters:\n        resp += f \n        i += 1\n        if i != l:\n            resp += \" || \"\n    return resp, decodes\n\ndef resetChat():\n    returnValue('messages').clear()\n    clearHistory()\n\ndef show_beta_ribbon():\n    st.markdown(\"\"\"\n        <style>\n            .ribbon {\n                position: absolute;\n                top: 0;\n                right: 0;\n                width: 150px;\n                height: 22px;\n                margin-right: -50px;\n                transform: rotate(45deg);\n                background-color: #ff4b4b;\n                color: white;\n                text-align: center;\n                font-weight: bold;\n                font-size: 0.8em;\n                line-height: 22px;\n                z-index: 999;\n            }\n        </style>\n        <div class=\"ribbon\">Experimental BETA</div>\n    \"\"\", unsafe_allow_html=True)\n\ndef glowing_header_text(header, text):\n    st.markdown(f\"\"\"\n        <div style=\"display: flex; align-items: center;\">\n            <h1> {header} </h1>\n            <span style=\"\n                background-color: #4CAF50;\n                color: white;\n                padding: 4px 8px;\n                border-radius: 4px;\n                font-size: 0.8em;\n                box-shadow: 0 0 10px #4CAF50;\n                animation: glow 1.5s ease-in-out infinite alternate;\">\n                {text}\n            </span>\n        </div>\n        <style>\n            @keyframes glow {{\n                from {{\n                    box-shadow: 0 0 5px #4CAF50;\n                }}\n                to {{\n                    box-shadow: 0 0 20px #4CAF50;\n                }}\n            }}\n        </style>\n    \"\"\", unsafe_allow_html=True)\n\ndef getEnabledFilters():\n    filters = []\n    if returnValue('http'):\n        filters.append('http')\n    if returnValue('snmp'):\n        filters.append('snmp')\n    if returnValue('https'):\n        filters.append('https')\n    if returnValue('ntp'):\n        filters.append('ntp')   \n    if returnValue('ftp'):\n        filters.append('ftp')\n    if returnValue('ssh'):\n        filters.append('ssh')\n    if returnValue('ngap'):\n        filters.append('ngap')\n    \n    if len(filters) < 1:\n        return None\n    return tagger_component('Enabled Filters', tags=filters, color_name='blue')\n\nwith st.sidebar:\n    if returnValue('selected_model') == 'Undefined':\n        loadDefaultSettings()\n    st.metric(\"Selected Model ✅\", returnValue('selected_model'))\n    st.metric(\"Plugged to 🔌 & connection status 🚦\", f\"{returnValue('llm_server')} {renderConnection(returnValue('llm_server_connection_status'))}\")\n    getEnabledFilters()\n    st.metric(\"Streaming 〰️\", returnValue('streaming_enabled'))\n    packetFile = st.file_uploader(label='Upload either a PCAP or PCAPNG file to chat', accept_multiple_files=False, type=['pcap','pcapng'])\n    if packetFile:\n        st.session_state['pcap_fname'] = packetFile.name\n        with st.spinner('#### Crunching the packets... 🥣🥣🥣'):\n            with open(f'{packetFile.name}', 'wb') as f:\n                f.write(packetFile.read())\n            filters, decodes = getFiltersAndDecodeInfo()\n            st.session_state['pcap_filters'] = filters\n            # print(f'#### {st.session_state['pcap_filters']}')\n            st.session_state['pcap_data'] = getPcapData(input_file=f'{packetFile.name}', filter=filters, decode_info=decodes)\n            initLLM(pcap_data=returnValue('pcap_data'))\n            #os.remove(f'{packetFile.name}')\n    else:\n        st.session_state['pcap_fname'] = \"None 🚫\"\n\ncol1, col2 = st.columns([2,1])\nwith col1:\n    st.title('Local Packet Whisperer (LPW)')\n    st.markdown('`Your local network assistant!`')\n    st.markdown(f'`Version : {get_lpw_version()}`')\nwith col2:\n    st.image(image=lpw_avatar, use_container_width=True)\n\n\n\nif not returnValue('llm_server_connection_status'):\n    st.error('LPW Cannot talk to the remote 🦙 Ollama Server 🦙', icon='🚨')\n    st.info('Please troubleshoot the **connection** or Update the **LLM Server Settings** in LPW Setting ⚙️ Page', icon='💡')\nelse :\n    #st.markdown('#### Step 1️⃣ 👉🏻 Build a knowledge base')\n    #packetFile = st.file_uploader(label='Upload either a PCAP or PCAPNG file to chat', accept_multiple_files=False, type=['pcap','pcapng'])\n    #st.markdown('#### Step 2️⃣ 👉🏻 Chat with packets')\n    chat, insights = st.tabs(['Chat 💬', 'Insights ✨'])\n    with chat:\n        st.header('Whisper with LPW')\n        if st.session_state['pcap_fname'] == \"None 🚫\":\n            resetChat()\n            st.markdown('#### Waiting for packets 🧘🏻🧘🏻🧘🏻🧘🏻')\n        else:\n            chat_container = st.container(height=500)\n            prompt = st.chat_input('Enter your prompt', key='prompt_ctrl', disabled=False)\n            st.sidebar.metric(\"Whispering with 🗣️\", returnValue('pcap_fname'))\n            with chat_container.chat_message(name='assistant', avatar=lpw_avatar):\n                st.markdown('Chat with me..')\n            for message in returnValue('messages'):\n                with chat_container.chat_message(name=message['role'], avatar = lpw_avatar if message['role'] == 'assistant' else None):\n                    st.markdown(message['content'])\n            if prompt:\n                returnValue('messages').append({'role' : 'user', 'content' : prompt})\n                with chat_container.chat_message(name='user'):\n                    st.markdown(prompt)\n                with chat_container.chat_message(name='assistant', avatar=lpw_avatar):\n                    with st.spinner('Processing....'):\n                        full_response = chatWithModel(prompt=prompt, model=returnValue('selected_model'))\n                        returnValue('messages').append({'role' : 'assistant', 'content' : full_response})\n                        if returnValue('streaming_enabled'):\n                            message_placeholder = st.empty()\n                            streaming_response = \"\"\n                            # Simulate stream of response with milliseconds delay\n                            for chunk in full_response.split():\n                                streaming_response += chunk + \" \"\n                                time.sleep(0.05)\n                                # Add a blinking cursor to simulate typing\n                                message_placeholder.markdown(streaming_response + \"▌\", unsafe_allow_html=True)\n                            message_placeholder.markdown(full_response, unsafe_allow_html=True)\n                        else:\n                            st.markdown(full_response)\n                st.button('Reset Chat 🗑️', use_container_width=True, on_click=resetChat)\n    with insights:\n        show_beta_ribbon()\n        glowing_header_text('Agentic Insights', 'Available for 5G NGAP only')\n        st.markdown('`Get a comprehensive report on PCAPs. Powered by LPW Agents!`')\n        if st.session_state['pcap_fname'] == \"None 🚫\":\n            st.markdown('#### Waiting for packets 🧘🏻🧘🏻🧘🏻🧘🏻')\n            st.session_state['insights_done'] = False\n        else:\n            if not returnValue('insights_done'):\n                st.markdown('Packet Capture Processed. Click below to generate insights')\n                if st.button(label='Generate Insights 🫰🏻', type='primary', use_container_width=True):\n                    st.session_state['insights_done'] = True\n                    st.session_state['insights_file_done'] = False\n                    st.rerun()\n            else:\n                if not returnValue('insights_file_done'):\n                    with st.spinner('#### Generating Insights 🪄🪄🪄'):\n                        st.session_state['insights_raw'] = LPWCrew(llm_host=returnValue('llm_server'),\n                                llm_port=returnValue('llm_server_port'),\n                                model=returnValue('selected_model')).kickoff(returnValue('pcap_data'), returnValue('pcap_filters'))\n                        st.session_state['insights_file_done'] = True\n                        st.rerun()\n                else:\n                    #markdown_path = os.path.join(getLpwPath('temp'), 'insights.md')\n                    #st.markdown(open(markdown_path).read())\n                    st.markdown(returnValue('insights_raw'))\n                    st.download_button(label='Download Insights markdown',\n                                        data = returnValue('insights_raw'),\n                                        type='primary',\n                                        use_container_width=True)"}
{"type": "source_file", "path": "bin/lpw_ollamaClient.py", "content": "import streamlit as st\nimport ollama\nfrom ollama import Client\nfrom typing import List\n\nclass OllamaClient():\n\n    def __init__(self, server=\"127.0.0.1\"):\n        self.messages = []\n        self.client = Client(host=f'http://{server}:11434')\n    \n    def setServer(self,server, port):\n        self.client = Client(host=f'http://{server}:{port}')\n    \n    def clear_history(self):\n        self.messages.clear()\n    \n    def append_history(self, message):\n        self.messages.append(message)\n    \n    def check_system_message(self) -> bool:\n        try:\n            if self.messages[0]['role'] == 'system':\n                return True\n            else:\n                return False\n        except:\n            return False\n    \n    def set_system_message(self, system_message:str) -> None:\n        if self.check_system_message():\n            self.edit_system_message(system_message)\n        else:\n            self.create_system_message(system_message)\n    \n    def create_system_message(self, system_message:str) -> None:\n        sMessage = dict({'role' : 'system', 'content' : system_message})\n        self.messages.append(sMessage)\n\n    def edit_system_message(self, system_message:str) -> None:\n        for m in self.messages:\n            if m['role'] == 'system':\n                m['content'] = system_message\n    \n    def chat(self, prompt:str, model: str, temp: float, system:str = \"default\") -> str:\n        options = dict({'temperature' : temp})\n        message = {}\n        message['role'] = 'user'\n        message['content'] = prompt\n        self.messages.append(message)\n        response = None\n        try:\n            response = self.client.chat(model=model, messages=self.messages, options=options)\n        except Exception as e:\n            st.error(f'Error Occured : {e} ', icon=\"🚨\")\n            st.stop()\n        self.messages.append(response['message'])\n        return response['message']['content']\n\n    def chat_stream(self, prompt:str, model: str, temp: float, system:str = \"default\"):\n        options = dict({'temperature' : temp})\n        message = {}\n        stream = None\n        if system != 'default' and not self.check_system_message():\n            sMessage = dict({'role' : 'system', 'content' : system})\n            self.messages.append(sMessage)\n        message['role'] = 'user'\n        message['content'] = prompt\n        self.messages.append(message)\n        try:\n            stream = self.client.chat(model=model, messages=self.messages, options=options, stream=True)\n        # the caller should call append_history\n        except Exception as e:\n            st.error(f'Error Occured : {e} ', icon=\"🚨\")\n            st.stop()\n        return stream\n    \n    def getModelList(self) -> List[str] | bool:\n        retList = []\n        is_Connected = False\n        try:\n            model_list = self.client.list()  \n            models = model_list['models']\n            #print(f'#### models {models}')\n            for model in models:\n                retList.append(model['model'])\n            is_Connected = True\n        except Exception as e:\n            print(f'Error Occured : {e} ')\n        return retList, is_Connected\n\n\nif __name__ == '__main__':\n    client = OllamaClient(server='192.168.0.14')\n    print(f'List of models are {client.getModelList()}')\n    #while True:\n    #    print('You :')\n    #    response = client.chat_stream(model='dolphin-mistral:latest', temp=0.8, prompt=input())\n    #    contents = \"\"\n    #    AiMessage = {}\n    #    for chunk in response:\n    #        content = chunk['message']['content']\n    #        print(content, end='', flush=True)\n    #        contents += content\n    #    AiMessage['role'] = 'assistant'\n    #    AiMessage['content'] = contents\n    #    client.append_history(AiMessage)"}
{"type": "source_file", "path": "bin/lpw_packet.py", "content": "import pyshark as ps\nimport streamlit as st\nimport os\nimport re\nimport asyncio\nfrom lpw_init import getLpwPath\n\ndef remove_ansi_escape_sequences(input_string):\n    # Define a regular expression pattern to match ANSI escape sequences\n    ansi_escape_pattern = r'\\x1B(?:[@-_]|[\\x80-\\x9F])[0-?]*[ -/]*[@-~]'\n    \n    # Use re.sub() to replace ANSI escape sequences with an empty string\n    cleaned_string = re.sub(ansi_escape_pattern, '', input_string)\n    \n    return cleaned_string\n\n@st.cache_data\ndef getPcapData(input_file:str = \"\", filter=\"\", decode_info={}):\n    try :\n        if os.name == 'nt':\n            eventloop = asyncio.ProactorEventLoop()\n            asyncio.set_event_loop(eventloop)        \n        cap : ps.FileCapture = ps.FileCapture(input_file=input_file, display_filter=filter)\n        outfile_path = os.path.join(getLpwPath('temp'), 'out.txt')\n        with open(outfile_path, 'w') as f:\n            for pkt in cap:\n                print(pkt, file=f)\n        out_string = open(outfile_path, 'r').read()\n        #os.remove('out.txt')\n    except ps.tshark.tshark.TSharkNotFoundException:\n        st.error(body='TShark/Wireshark is not installed. \\n Please install [wireshark](https://tshark.dev/setup/install/#install-wireshark-with-a-package-manager) first', icon='🚨')\n        st.warning(body='LPW is now stopped', icon='🛑')\n        st.stop()\n    return remove_ansi_escape_sequences(out_string)"}
{"type": "source_file", "path": "bin/lpw_init.py", "content": "import streamlit as st\nfrom pkg_resources import resource_filename\nimport os\n\nDEFAULT_SYSTEM_MESSAGE = \"\"\"\n        You are a helper assistant specialized in analysing packet captures used to troubleshooting & technical analysis. Use the information present in packet_capture_info to answer all the questions truthfully. If the user asks about a specific application layer protocol, use the following hints to inspect the packet_capture_info to answer the question.\n        \n        If the user asks for general analysis, extract information about every layer (such as ethernet, IP, transport layer), source and destination IPs, port numbers and other possible insights. Provide your response in a structured bullet response, easy to understand for a network engineer.\n\n        Format your response in markdown text with line breaks. You are encouraged to use emojis to make your response more presentable and fun.\n\n        hints :\n        http means tcp.port = 80\n        https means tcp.port = 443\n        snmp means udp.port = 161 or udp.port = 162\n        ntp means udp.port = 123\n        ftp means tcp.port = 21\n        ssh means tcp.port = 22\n        ngap means sctp.port = 38412\n\"\"\"\n\nDEFAULT_AGENT_CONFIG_YAML = \"\"\"\n---\nname: 5G_Signaling_Protocol_Specialist\nrole: 5G Signaling Protocol Specialist\ngoal: Analyze and interpret all 5G signaling messages within PCAP files, providing detailed insights into the communication between various 5G network elements, with a focus on NGAP and related protocols.\nbackstory: >\n  A veteran telecommunications expert with over a decade of experience in mobile network protocols. \n  Has been at the forefront of 5G technology since its inception, contributing to the development and \n  implementation of 5G standards. Possesses in-depth knowledge of 3GPP specifications, including \n  TS 38.413 (NGAP), TS 38.331 (RRC), and TS 24.501 (NAS), and has a comprehensive understanding \n  of 5G network architecture and signaling procedures.\ntasks:\n  - name: Comprehensive Analysis\n    description: >\n      Perform a thorough analysis of 5G signaling messages in packet_capture_info available below, including:\n      1. Analyze the packet capture information to identify and decode all 5G signaling messages, including NGAP, NAS, and RRC protocols.\n      2. Dissect complex message structures such as NGAP PDUs, Path Transfer Requests, and PDU Session Resource Setup Requests. Unearth important insights like gNodeB Name, SUCI, SUPI, IMSI, PLMN, MNC, MCC, IMEI, PEI etc from the messages.\n      3. Interpret signaling procedures and message flows, explaining their significance in 5G network operations.\n      4. Identify and explain key procedures such as NG Setup, Initial Context Setup, UE Context Release, and Path Transfer.\n      5. Verify protocol compliance against 3GPP specifications, flagging any deviations or potential issues.\n      6. Correlate messages across different interfaces (e.g., N1, N2) to provide a holistic view of signaling exchanges. You can use ASCII diagrams to illustrate the flow of exchanges between different network elements present in this packet capture.\n      7. Detect anomalies or unusual patterns in the signaling traffic that might indicate performance issues or security threats.\n      8. Generate detailed reports on signaling behavior, including message sequence diagrams and explanations of observed procedures.\n      9. Provide insights on potential optimizations or troubleshooting recommendations based on the analyzed signaling patterns.\n      10. Collaborate with other specialized agents to contribute to a comprehensive understanding of the 5G network's behavior and performance.\n\n      packet_capture_info : {pcap_data}\n    expected_output: >\n      A comprehensive analysis report in a well-formatted markdown document\n\"\"\"\n\ndefault_settings = {\n    'streaming_enabled' : False,\n    'messages' : [],\n    'system_message' : DEFAULT_SYSTEM_MESSAGE,\n    'selected_model' : 'Undefined',\n    'llm_server' : \"127.0.0.1\",\n    'llm_server_port' : 11434,\n    'llm_server_connection_status' : 'False',\n    'http' : False,\n    'https' : False,\n    'snmp' : False,\n    'ftp' : False,\n    'ngap' : False,\n    'ssh' : False,\n    'ntp' : False,\n    'pcap_fname' : \"None 🚫\",\n    'pcap_data' : \"\",\n    'pcap_filters' : \"\",\n    'insights_done' : False,\n    'insights_file_done' : False,\n    'agent_config_file' : None,\n    'default_agent_config' : DEFAULT_AGENT_CONFIG_YAML,\n    'insights_raw' : \"\"\n}\n\ndef returnValue(key):\n    if key not in st.session_state:\n        st.session_state[key] = default_settings[key]\n    return st.session_state[key]\n\ndef getLpwPath(dirname):\n    usr_home_dir = os.path.expanduser(\"~\")\n    lpw_home_dir = os.path.join(usr_home_dir, '.lpw')\n    dirname_path = os.path.join(lpw_home_dir, dirname)\n    if not os.path.exists(dirname_path):\n        os.makedirs(dirname_path)\n    return dirname_path"}
{"type": "source_file", "path": "bin/lpw_main.py", "content": "import streamlit as st\n\nmain_page = st.Page(\"lpw_home.py\", title=\"LPW Home\", icon=\"🏠\")\nsettings_page = st.Page(\"lpw_settings.py\", title=\"LPW Settings\", icon=\"⚙️\")\n\npg = st.navigation([main_page, settings_page])\n#st.set_page_config(page_title=\"Data manager\", page_icon=\":material/edit:\")\npg.run()\n"}
{"type": "source_file", "path": "bin/lpw_prompt.py", "content": "import streamlit as st\nfrom lpw_ollamaClient import OllamaClient\nfrom typing import List\nfrom lpw_init import returnValue\nfrom lpw_prompt import *\nfrom lpw_packet import *\n\ndef returnSystemText(pcap_data : str) -> str:\n    PACKET_WHISPERER = f\"\"\"\n        {returnValue('system_message')}\n        packet_capture_info : {pcap_data}\n    \"\"\"\n    return PACKET_WHISPERER\n\noClient = OllamaClient()\n\ndef setLLMServer(server, port):\n    oClient.setServer(server, port)\n\ndef initLLM(pcap_data) -> None:\n    oClient.set_system_message(system_message=returnSystemText(pcap_data))\n\ndef exitLLM() -> None:\n    oClient.set_system_message(system_message=returnValue('system_message'))\n\ndef getModelList() -> List[str]:\n    return oClient.getModelList()\n\ndef chatWithModel(prompt:str, model: str):\n    return oClient.chat(prompt=prompt, model=model, temp=0.4)\n\ndef clearHistory():\n    oClient.clear_history()\n\ndef modifySM(new_sm: str) -> None:\n    oClient.edit_system_message(new_sm)\n"}
{"type": "source_file", "path": "setup.py", "content": "from setuptools import find_packages, setup\n\nwith open(\"./README.md\", \"r\") as f:\n    long_description = f.read()\n\nwith open(\"./requirements.txt\", \"r\") as reqs:\n    deps = reqs.read().splitlines()\n\nwith open(\"./VERSION.txt\", \"r\") as ver:\n    version = ver.read()\n\nsetup(\n    name=\"lpw\",\n    version=version,\n    description=\"Using Local Packet Whisperer (LPW, Chat with PCAP/PCAPNG files locally, privately!\",\n    packages=find_packages(),\n    package_dir={'lpw' : '.'},\n    long_description=long_description,\n    long_description_content_type=\"text/markdown\",\n    url=\"https://github.com/kspviswa/local-packet-whisperer\",\n    author=\"Viswa Kumar\",\n    author_email=\"kspviswaphd@gmail.com\",\n    license=\"MIT\",\n    scripts=['bin/lpw', 'bin/lpw_main.py', 'bin/lpw_ollamaClient.py', 'bin/lpw_packet.py', 'bin/lpw_prompt.py', 'bin/lpw_settings.py', 'bin/lpw_home.py', 'bin/lpw_init.py', 'bin/lpw_agent.py'],\n    classifiers=[\n        \"Development Status :: 3 - Alpha\",\n        \"License :: OSI Approved :: MIT License\",\n        \"Programming Language :: Python :: 3.11\",\n        \"Operating System :: OS Independent\",\n    ],\n    install_requires = deps,\n    #package_data={'lpw': ['temp/*', 'config/*']},\n    include_package_data=True,\n    python_requires=\">=3.11\",\n)   "}
{"type": "source_file", "path": "bin/lpw_settings.py", "content": "import streamlit as st\nfrom lpw_init import *\nfrom lpw_prompt import *\nfrom lpw_packet import *\n\ndef save_sm() -> None:\n    modifySM(returnValue('system_message'))\n\n#st.markdown('## Enter OLLAMA endpoint:')\n#st.text_input(\"Ollama endpoint\", placeholder=\"Eg localhost\", label_visibility='hidden')\nst.title('LPW Settings ⚙️')\n#st.markdown('## System Message:')\nwith st.expander(label='**Set System Message** *(optional)*', expanded=False):\n    st.session_state['system_message'] = st.text_area(label='Override system message', value=returnValue('system_message'), label_visibility=\"hidden\", height=500, on_change=save_sm)\nst.session_state['streaming_enabled'] = st.toggle(label='Enable Streaming', value=returnValue('streaming_enabled'))\n\nwith st.expander(label='**LLM Server Settings**', expanded=False, icon=\":material/neurology:\"):\n    st.session_state['llm_server'] = st.text_input(label=\"LLM Server Host\", value=returnValue('llm_server'))\n    st.session_state['llm_server_port'] = st.number_input(label=\"LLM Server Host Port\", value=returnValue('llm_server_port'), min_value=1024, max_value=65525, step=1)\n    setLLMServer(st.session_state['llm_server'], st.session_state['llm_server_port'])\n    st.session_state['selected_model'] = st.selectbox('**Available Models**', placeholder=\"Choose an Option\", options=getModelList()[0])\nst.markdown('#### Select protocols to filter in analysis')\ncol1, col2, col3 = st.columns(3)\nwith col1:\n    st.session_state['http'] = st.checkbox(\"HTTP\",value=returnValue('http')) #80\n    st.session_state['snmp'] = st.checkbox(\"SNMP\",value=returnValue('snmp')) #161, 162\n    st.session_state['ntp'] = st.checkbox(\"NTP\",value=returnValue('ntp')) #123\nwith col2:\n    st.session_state['https'] = st.checkbox(\"HTTPS\",value=returnValue('https')) #443\n    st.session_state['ftp'] = st.checkbox(\"FTP\",value=returnValue('ftp')) #21\n    st.session_state['ssh'] = st.checkbox(\"SSH\",value=returnValue('ssh')) #22\nwith col3:\n    st.session_state['ngap'] = st.checkbox(\"NGAP\",value=returnValue('ngap')) #38412\n\ndef glowing_header_text(header, text):\n    st.markdown(f\"\"\"\n        <div style=\"display: flex; align-items: center;\">\n            <h1> {header} </h1>\n            <span style=\"\n                background-color: #4CAF50;\n                color: white;\n                padding: 4px 8px;\n                border-radius: 4px;\n                font-size: 0.8em;\n                box-shadow: 0 0 10px #4CAF50;\n                animation: glow 1.5s ease-in-out infinite alternate;\">\n                {text}\n            </span>\n        </div>\n        <style>\n            @keyframes glow {{\n                from {{\n                    box-shadow: 0 0 5px #4CAF50;\n                }}\n                to {{\n                    box-shadow: 0 0 20px #4CAF50;\n                }}\n            }}\n        </style>\n    \"\"\", unsafe_allow_html=True)\n\nwith st.expander(label=\"**NGAP Agent Settings** ⚠️Experimental⚠️\", expanded=False, icon=\":material/robot_2:\"):\n    glowing_header_text('Config YAML', 'Available for 5G NGAP only')\n    st.markdown('Refer [here](https://github.com/kspviswa/local-packet-whisperer/blob/main/config/5g_agent.yaml) for more details to how to create this file.')\n    st.session_state['agent_config_file'] = st.file_uploader(label='Agent Config YAML', type='yaml', label_visibility=\"hidden\")\n\ndef loadDefaultSettings():\n    st.session_state['selected_model'] = st.selectbox('Models', placeholder=\"Choose an Option\", options=getModelList(),label_visibility='hidden')"}
