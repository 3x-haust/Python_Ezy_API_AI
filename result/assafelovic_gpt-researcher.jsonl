{"repo_info": {"repo_name": "gpt-researcher", "repo_owner": "assafelovic", "repo_url": "https://github.com/assafelovic/gpt-researcher"}}
{"type": "test_file", "path": "tests/documents-report-source.py", "content": "import os\nimport asyncio\nimport pytest\n# Ensure this path is correct\nfrom gpt_researcher import GPTResearcher\nfrom dotenv import load_dotenv\nload_dotenv()\n\n# Define the report types to test\nreport_types = [\n    \"research_report\",\n    \"custom_report\",\n    \"subtopic_report\",\n    \"summary_report\",\n    \"detailed_report\",\n    \"quick_report\"\n]\n\n# Define a common query and sources for testing\nquery = \"What can you tell me about myself based on my documents?\"\n\n# Define the output directory\noutput_dir = \"./outputs\"\n\n\n@pytest.mark.asyncio\n@pytest.mark.parametrize(\"report_type\", report_types)\nasync def test_gpt_researcher(report_type):\n    # Ensure the output directory exists\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    # Create an instance of GPTResearcher with report_source set to \"documents\"\n    researcher = GPTResearcher(\n        query=query, report_type=report_type, report_source=\"documents\")\n\n    # Conduct research and write the report\n    await researcher.conduct_research()\n    report = await researcher.write_report()\n\n    # Define the expected output filenames\n    pdf_filename = os.path.join(output_dir, f\"{report_type}.pdf\")\n    docx_filename = os.path.join(output_dir, f\"{report_type}.docx\")\n\n    # Check if the PDF and DOCX files are created\n    # assert os.path.exists(pdf_filename), f\"PDF file not found for report type: {report_type}\"\n    # assert os.path.exists(docx_filename), f\"DOCX file not found for report type: {report_type}\"\n\n    # Clean up the generated files (optional)\n    # os.remove(pdf_filename)\n    # os.remove(docx_filename)\n\nif __name__ == \"__main__\":\n    pytest.main()\n"}
{"type": "test_file", "path": "tests/__init__.py", "content": ""}
{"type": "test_file", "path": "tests/gptr-logs-handler.py", "content": "import logging\nfrom typing import List, Dict, Any\nimport asyncio\nfrom gpt_researcher import GPTResearcher\nfrom backend.server.server_utils import CustomLogsHandler  # Update import\n\nasync def run() -> None:\n    \"\"\"Run the research process and generate a report.\"\"\"\n    query = \"What happened in the latest burning man floods?\"\n    report_type = \"research_report\"\n    report_source = \"online\"\n    tone = \"informative\"\n    config_path = None\n\n    custom_logs_handler = CustomLogsHandler(None, query)  # Pass query parameter\n\n    researcher = GPTResearcher(\n        query=query,\n        report_type=report_type,\n        report_source=report_source,\n        tone=tone,\n        config_path=config_path,\n        websocket=custom_logs_handler\n    )\n\n    await researcher.conduct_research()  # Conduct the research\n    report = await researcher.write_report()  # Write the research report\n    logging.info(\"Report generated successfully.\")  # Log report generation\n\n    return report\n\n# Run the asynchronous function using asyncio\nif __name__ == \"__main__\":\n    asyncio.run(run())\n"}
{"type": "test_file", "path": "tests/report-types.py", "content": "import os\nimport asyncio\nimport pytest\nfrom unittest.mock import AsyncMock\nfrom gpt_researcher.agent import GPTResearcher\nfrom backend.server.server_utils import CustomLogsHandler\nfrom typing import List, Dict, Any\n\n# Define the report types to test\nreport_types = [\"research_report\", \"subtopic_report\"]\n\n# Define a common query and sources for testing\nquery = \"what is gpt-researcher\"\n\n\n@pytest.mark.asyncio\n@pytest.mark.parametrize(\"report_type\", report_types)\nasync def test_gpt_researcher(report_type):\n    mock_websocket = AsyncMock()\n    custom_logs_handler = CustomLogsHandler(mock_websocket, query)\n    # Create an instance of GPTResearcher\n    researcher = GPTResearcher(\n        query=query,\n        query_domains=[\"github.com\"],\n        report_type=report_type,\n        websocket=custom_logs_handler,\n    )\n\n    # Conduct research and write the report\n    await researcher.conduct_research()\n    report = await researcher.write_report()\n\n    print(researcher.visited_urls)\n    print(report)\n\n    # Check if the report contains part of the query\n    assert \"gpt-researcher\" in report\n\n    # test if at least one url starts with \"github.com\" as it was limited to this domain\n    matching_urls = [\n        url for url in researcher.visited_urls if url.startswith(\"https://github.com\")\n    ]\n    assert len(matching_urls) > 0\n\n\nif __name__ == \"__main__\":\n    pytest.main()\n"}
{"type": "test_file", "path": "tests/research_test.py", "content": "\"\"\"\nHi! The following test cases are for the new parameter `complement_source_urls` and fix on the functional error with `source_urls` in GPTResearcher class.\n\nThe source_urls parameter was resetting each time in conduct_research function causing gptr to forget the given links. Now, that has been fixed and a new parameter is introduced.\nThis parameter named will `complement_source_urls` allow GPTR to research on sources other than the provided sources via source_urls if set to True. \nDefault is False, i.e., no additional research will be conducted on newer sources.\n\"\"\"\n\n## Notes:\n## Please uncomment the test case to run and comment the rest.\n## Thanks!\n\n\n\n#### Test case 1 (original test case as control from https://docs.gptr.dev/docs/gpt-researcher/tailored-research)\n\nfrom gpt_researcher.agent import GPTResearcher  # Ensure this path is correct\nimport asyncio\nimport logging\nfrom typing import List, Dict, Any\nfrom backend.server.server_utils import CustomLogsHandler  # Update import\n\nasync def get_report(query: str, report_type: str, sources: list) -> str:\n    custom_logs_handler = CustomLogsHandler(None, query)  # Pass query parameter\n    researcher = GPTResearcher(query=query, \n                               report_type=report_type, \n                               complement_source_urls=False,\n                               websocket=custom_logs_handler)\n    await researcher.conduct_research()\n    report = await researcher.write_report()\n    return report, researcher\n\nif __name__ == \"__main__\":\n    query = \"Write an analysis on paul graham\"\n    report_type = \"research_report\"\n    sources = [\"https://www.paulgraham.com/when.html\", \"https://www.paulgraham.com/noob.html\"]  # query is related\n\n    report, researcher = asyncio.run(get_report(query, report_type, sources))\n    print(report)\n\n    print(f\"\\nLength of the context = {len(researcher.get_research_context())}\") # Must say Non-zero value because the query is related to the contents of the page, so there will be relevant context present\n\n\n\n#### Test case 2 (Illustrating the problem, i.e., source_urls are not scoured. Hence, no relevant context)\n\n# from gpt_researcher.agent import GPTResearcher  # Ensure this path is correct\n# import asyncio\n\n# async def get_report(query: str, report_type: str, sources: list) -> str:\n#     researcher = GPTResearcher(query=query, report_type=report_type, source_urls=sources)\n#     await researcher.conduct_research()\n#     report = await researcher.write_report()\n#     return report, researcher\n\n# if __name__ == \"__main__\":\n#     query = \"What is Microsoft's business model?\"\n#     report_type = \"research_report\"\n#     sources = [\"https://www.apple.com\", \"https://en.wikipedia.org/wiki/Olympic_Games\"]  # query is UNRELATED.\n\n#     report, researcher = asyncio.run(get_report(query, report_type, sources))\n#     print(report)\n\n#     print(f\"\\nLength of the context = {len(researcher.get_research_context())}\") # Must say 0 (zero) value because the query is UNRELATED to the contents of the pages, so there will be NO relevant context present\n\n\n\n#### Test case 3 (Suggested solution - complement_source_urls parameter allows GPTR to scour more of the web and not restrict to source_urls)\n\n# from gpt_researcher.agent import GPTResearcher  # Ensure this path is correct\n# import asyncio\n\n# async def get_report(query: str, report_type: str, sources: list) -> str:\n#     researcher = GPTResearcher(query=query, report_type=report_type, source_urls=sources, complement_source_urls=True)\n#     await researcher.conduct_research()\n#     report = await researcher.write_report()\n#     return report, researcher\n\n# if __name__ == \"__main__\":\n#     query = \"What is Microsoft's business model?\"\n#     report_type = \"research_report\"\n#     sources = [\"https://www.apple.com\", \"https://en.wikipedia.org/wiki/Olympic_Games\"]  # query is UNRELATED\n\n#     report, researcher = asyncio.run(get_report(query, report_type, sources))\n#     print(report)\n\n#     print(f\"\\nLength of the context = {len(researcher.get_research_context())}\") # Must say Non-zero value because the query is UNRELATED to the contents of the page, but the complement_source_urls is set which should make gptr do default web search to gather contexts\n    \n\n\n# #### Test case 4 (Furthermore, GPTR will create more context in addition to source_urls if the complement_source_urls parameter is set allowing for a larger research scope)\n\n# from gpt_researcher.agent import GPTResearcher  # Ensure this path is correct\n# import asyncio\n\n# async def get_report(query: str, report_type: str, sources: list) -> str:\n#     researcher = GPTResearcher(query=query, report_type=report_type, source_urls=sources, complement_source_urls=True)\n#     await researcher.conduct_research()\n#     report = await researcher.write_report()\n#     return report, researcher\n\n# if __name__ == \"__main__\":\n#     query = \"What are the latest advancements in AI?\"\n#     report_type = \"research_report\"\n#     sources = [\"https://en.wikipedia.org/wiki/Artificial_intelligence\", \"https://www.ibm.com/watson/ai\"]  # query is related\n\n#     report, researcher = asyncio.run(get_report(query, report_type, sources))\n#     print(report)\n\n#     print(f\"\\nLength of the context = {len(researcher.get_research_context())}\") # Must say Non-zero value because the query is related to the contents of the page, and additionally the complement_source_urls is set which should make gptr do default web search to gather more contexts!\n"}
{"type": "test_file", "path": "tests/test-loaders.py", "content": "from langchain_community.document_loaders import PyMuPDFLoader, UnstructuredCSVLoader\n\n# # Test PyMuPDFLoader\npdf_loader = PyMuPDFLoader(\"my-docs/Elisha - Coding Career.pdf\")\ntry:\n    pdf_data = pdf_loader.load()\n    print(\"PDF Data:\", pdf_data)\nexcept Exception as e:\n    print(\"Failed to load PDF:\", e)\n\n# Test UnstructuredCSVLoader\ncsv_loader = UnstructuredCSVLoader(\"my-docs/active_braze_protocols_from_bq.csv\", mode=\"elements\")\ntry:\n    csv_data = csv_loader.load()\n    print(\"CSV Data:\", csv_data)\nexcept Exception as e:\n    print(\"Failed to load CSV:\", e)"}
{"type": "test_file", "path": "tests/test-openai-llm.py", "content": "import asyncio\nfrom gpt_researcher.utils.llm import get_llm\nfrom gpt_researcher import GPTResearcher\nfrom dotenv import load_dotenv\nload_dotenv()\n\nasync def main():\n\n    # Example usage of get_llm function\n    llm_provider = \"openai\"\n    model = \"gpt-3.5-turbo\" \n    temperature = 0.7\n    max_tokens = 1000\n\n    llm = get_llm(llm_provider, model=model, temperature=temperature, max_tokens=max_tokens)\n    print(f\"LLM Provider: {llm_provider}, Model: {model}, Temperature: {temperature}, Max Tokens: {max_tokens}\")\n    print('llm: ',llm)\n    await test_llm(llm=llm)\n\n\nasync def test_llm(llm):\n    # Test the connection with a simple query\n    messages = [{\"role\": \"user\", \"content\": \"sup?\"}]\n    try:\n        response = await llm.get_chat_response(messages, stream=False)\n        print(\"LLM response:\", response)\n    except Exception as e:\n        print(f\"Error: {e}\")\n\n# Run the async function\nasyncio.run(main())"}
{"type": "test_file", "path": "tests/test-your-embeddings.py", "content": "from gpt_researcher.config.config import Config\nfrom gpt_researcher.memory.embeddings import Memory\nimport asyncio\nimport os\nfrom dotenv import load_dotenv\nload_dotenv()\n\nasync def main():\n    cfg = Config()\n    \n    print(\"Current embedding configuration:\")\n    print(f\"EMBEDDING env var: {os.getenv('EMBEDDING', 'Not set')}\")\n    print(f\"EMBEDDING_PROVIDER env var: {os.getenv('EMBEDDING_PROVIDER', 'Not set')}\")\n    \n    try:\n        # Check if embedding attributes are set\n        print(f\"cfg.embedding: {getattr(cfg, 'embedding', 'Not set')}\")\n        print(f\"cfg.embedding_provider: {getattr(cfg, 'embedding_provider', 'Not set')}\")\n        print(f\"cfg.embedding_model: {getattr(cfg, 'embedding_model', 'Not set')}\")\n        \n        # If embedding_provider and embedding_model are not set, use defaults\n        if not hasattr(cfg, 'embedding_provider') or not cfg.embedding_provider:\n            print(\"Setting default embedding provider: openai\")\n            cfg.embedding_provider = \"openai\"\n            \n        if not hasattr(cfg, 'embedding_model') or not cfg.embedding_model:\n            print(\"Setting default embedding model: text-embedding-3-small\")\n            cfg.embedding_model = \"text-embedding-3-small\"\n        \n        # Create a Memory instance using the configuration\n        # Note: We're not passing embedding_kwargs since it's not properly initialized\n        memory = Memory(\n            embedding_provider=cfg.embedding_provider,\n            model=cfg.embedding_model\n        )\n        \n        # Get the embeddings object\n        embeddings = memory.get_embeddings()\n        \n        # Test the embeddings with a simple text\n        test_text = \"This is a test sentence to verify embeddings are working correctly.\"\n        embedding_vector = embeddings.embed_query(test_text)\n        \n        # Print information about the embedding\n        print(f\"\\nSuccess! Generated embeddings using provider: {cfg.embedding_provider}\")\n        print(f\"Model: {cfg.embedding_model}\")\n        print(f\"Embedding vector length: {len(embedding_vector)}\")\n        print(f\"First few values: {embedding_vector[:5]}\")\n        \n    except Exception as e:\n        print(f\"Error testing embeddings: {e}\")\n        import traceback\n        traceback.print_exc()\n\nif __name__ == \"__main__\":\n    asyncio.run(main())"}
{"type": "test_file", "path": "tests/test-your-llm.py", "content": "from gpt_researcher.config.config import Config\nfrom gpt_researcher.utils.llm import create_chat_completion\nimport asyncio\nfrom dotenv import load_dotenv\nload_dotenv()\n\nasync def main():\n    cfg = Config()\n\n    try:\n        report = await create_chat_completion(\n            model=cfg.smart_llm_model,\n            messages = [{\"role\": \"user\", \"content\": \"sup?\"}],\n            temperature=0.35,\n            llm_provider=cfg.smart_llm_provider,\n            stream=True,\n            max_tokens=cfg.smart_token_limit,\n            llm_kwargs=cfg.llm_kwargs\n        )\n    except Exception as e:\n        print(f\"Error in calling LLM: {e}\")\n\n# Run the async function\nasyncio.run(main())"}
{"type": "test_file", "path": "tests/test-your-retriever.py", "content": "import asyncio\nfrom dotenv import load_dotenv\nfrom gpt_researcher.config.config import Config\nfrom gpt_researcher.actions.retriever import get_retrievers\nfrom gpt_researcher.skills.researcher import ResearchConductor\nimport pprint\n# Load environment variables from .env file\nload_dotenv()\n\nasync def test_scrape_data_by_query():\n    # Initialize the Config object\n    config = Config()\n\n    # Retrieve the retrievers based on the current configuration\n    retrievers = get_retrievers({}, config)\n    print(\"Retrievers:\", retrievers)\n\n    # Create a mock researcher object with necessary attributes\n    class MockResearcher:\n        def init(self):\n            self.retrievers = retrievers\n            self.cfg = config\n            self.verbose = True\n            self.websocket = None\n            self.scraper_manager = None  # Mock or implement scraper manager\n            self.vector_store = None  # Mock or implement vector store\n\n    researcher = MockResearcher()\n    research_conductor = ResearchConductor(researcher)\n    # print('research_conductor',dir(research_conductor))\n    # print('MockResearcher',dir(researcher))\n    # Define a sub-query to test\n    sub_query = \"design patterns for autonomous ai agents\"\n\n    # Iterate through all retrievers\n    for retriever_class in retrievers:\n        # Instantiate the retriever with the sub-query\n        retriever = retriever_class(sub_query)\n\n        # Perform the search using the current retriever\n        search_results = await asyncio.to_thread(\n            retriever.search, max_results=10\n        )\n\n        print(\"\\033[35mSearch results:\\033[0m\")\n        pprint.pprint(search_results, indent=4, width=80)\n\nif __name__ == \"__main__\":\n    asyncio.run(test_scrape_data_by_query())"}
{"type": "test_file", "path": "tests/test_logging.py", "content": "import pytest\nfrom unittest.mock import AsyncMock\nfrom fastapi import WebSocket\nfrom backend.server.server_utils import CustomLogsHandler\nimport os\nimport json\n\n@pytest.mark.asyncio\nasync def test_custom_logs_handler():\n    # Mock websocket\n    mock_websocket = AsyncMock()\n    mock_websocket.send_json = AsyncMock()\n    \n    # Test initialization\n    handler = CustomLogsHandler(mock_websocket, \"test_query\")\n    \n    # Verify log file creation\n    assert os.path.exists(handler.log_file)\n    \n    # Test sending log data\n    test_data = {\n        \"type\": \"logs\",\n        \"message\": \"Test log message\"\n    }\n    \n    await handler.send_json(test_data)\n    \n    # Verify websocket was called with correct data\n    mock_websocket.send_json.assert_called_once_with(test_data)\n    \n    # Verify log file contents\n    with open(handler.log_file, 'r') as f:\n        log_data = json.load(f)\n        assert len(log_data['events']) == 1\n        assert log_data['events'][0]['data'] == test_data \n\n@pytest.mark.asyncio\nasync def test_content_update():\n    \"\"\"Test handling of non-log type data that updates content\"\"\"\n    mock_websocket = AsyncMock()\n    mock_websocket.send_json = AsyncMock()\n    \n    handler = CustomLogsHandler(mock_websocket, \"test_query\")\n    \n    # Test content update\n    content_data = {\n        \"query\": \"test query\",\n        \"sources\": [\"source1\", \"source2\"],\n        \"report\": \"test report\"\n    }\n    \n    await handler.send_json(content_data)\n    \n    mock_websocket.send_json.assert_called_once_with(content_data)\n    \n    # Verify log file contents\n    with open(handler.log_file, 'r') as f:\n        log_data = json.load(f)\n        assert log_data['content']['query'] == \"test query\"\n        assert log_data['content']['sources'] == [\"source1\", \"source2\"]\n        assert log_data['content']['report'] == \"test report\""}
{"type": "test_file", "path": "tests/test_logging_output.py", "content": "import pytest\nimport asyncio\nfrom pathlib import Path\nimport json\nimport logging\nfrom fastapi import WebSocket\nfrom datetime import datetime\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\nclass TestWebSocket(WebSocket):\n    def __init__(self):\n        self.events = []\n        self.scope = {}\n\n    def __bool__(self):\n        return True\n\n    async def accept(self):\n        self.scope[\"type\"] = \"websocket\"\n        pass\n        \n    async def send_json(self, event):\n        logger.info(f\"WebSocket received event: {event}\")\n        self.events.append(event)\n\n@pytest.mark.asyncio\nasync def test_log_output_file():\n    \"\"\"Test to verify logs are properly written to output file\"\"\"\n    from gpt_researcher.agent import GPTResearcher\n    from backend.server.server_utils import CustomLogsHandler\n    \n    # 1. Setup like the main app\n    websocket = TestWebSocket()\n    await websocket.accept()\n    \n    # 2. Initialize researcher like main app\n    query = \"What is the capital of France?\"\n    research_id = f\"{datetime.now().strftime('%Y%m%d_%H%M%S')}_{hash(query)}\"\n    logs_handler = CustomLogsHandler(websocket=websocket, task=research_id)\n    researcher = GPTResearcher(query=query, websocket=logs_handler)\n    \n    # 3. Run research\n    await researcher.conduct_research()\n    \n    # 4. Verify events were captured\n    logger.info(f\"Events captured: {len(websocket.events)}\")\n    assert len(websocket.events) > 0, \"No events were captured\"\n    \n    # 5. Check output file\n    output_dir = Path().joinpath(Path.cwd(), \"outputs\")\n    output_files = list(output_dir.glob(f\"task_*{research_id}*.json\"))\n    assert len(output_files) > 0, \"No output file was created\"\n    \n    with open(output_files[-1]) as f:\n        data = json.load(f)\n        assert len(data.get('events', [])) > 0, \"No events in output file\" \n\n    # Clean up the output files\n    for output_file in output_files:\n        output_file.unlink()\n        logger.info(f\"Deleted output file: {output_file}\")"}
{"type": "test_file", "path": "tests/test_logs.py", "content": "import os\nfrom pathlib import Path\nimport sys\n\n# Add the project root to Python path\nproject_root = Path(__file__).parent.parent\nsys.path.append(str(project_root))\n\nfrom backend.server.server_utils import CustomLogsHandler\n\ndef test_logs_creation():\n    # Print current working directory\n    print(f\"Current working directory: {os.getcwd()}\")\n    \n    # Print project root\n    print(f\"Project root: {project_root}\")\n    \n    # Try to create logs directory directly\n    logs_dir = project_root / \"logs\"\n    print(f\"Attempting to create logs directory at: {logs_dir}\")\n    \n    try:\n        # Create directory with full permissions\n        os.makedirs(logs_dir, mode=0o777, exist_ok=True)\n        print(f\"✓ Created directory: {logs_dir}\")\n        \n        # Test file creation\n        test_file = logs_dir / \"test.txt\"\n        with open(test_file, 'w') as f:\n            f.write(\"Test log entry\")\n        print(f\"✓ Created test file: {test_file}\")\n        \n        # Initialize the handler\n        handler = CustomLogsHandler()\n        print(\"✓ CustomLogsHandler initialized\")\n        \n        # Test JSON logging\n        handler.logs.append({\"test\": \"message\"})\n        print(\"✓ Added test log entry\")\n        \n    except Exception as e:\n        print(f\"❌ Error: {str(e)}\")\n        print(f\"Error type: {type(e)}\")\n        import traceback\n        print(f\"Traceback: {traceback.format_exc()}\")\n\nif __name__ == \"__main__\":\n    test_logs_creation() "}
{"type": "test_file", "path": "tests/test_researcher_logging.py", "content": "import pytest\nimport asyncio\nfrom pathlib import Path\nimport sys\nimport logging\n\n# Add the project root to Python path\nproject_root = Path(__file__).parent.parent\nsys.path.append(str(project_root))\n\n# Configure basic logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n@pytest.mark.asyncio\nasync def test_researcher_logging():  # Renamed function to be more specific\n    \"\"\"\n    Test suite for verifying the researcher's logging infrastructure.\n    Ensures proper creation and formatting of log files.\n    \"\"\"\n    try:\n        # Import here to catch any import errors\n        from backend.server.server_utils import Researcher\n        logger.info(\"Successfully imported Researcher class\")\n        \n        # Create a researcher instance with a logging-focused query\n        researcher = Researcher(\n            query=\"Test query for logging verification\",\n            report_type=\"research_report\"\n        )\n        logger.info(\"Created Researcher instance\")\n        \n        # Run the research\n        report = await researcher.research()\n        logger.info(\"Research completed successfully!\")\n        logger.info(f\"Report length: {len(report)}\")\n        \n        # Basic report assertions\n        assert report is not None\n        assert len(report) > 0\n        \n        # Detailed log file verification\n        logs_dir = Path(project_root) / \"logs\"\n        log_files = list(logs_dir.glob(\"research_*.log\"))\n        json_files = list(logs_dir.glob(\"research_*.json\"))\n        \n        # Verify log files exist\n        assert len(log_files) > 0, \"No log files were created\"\n        assert len(json_files) > 0, \"No JSON files were created\"\n        \n        # Log the findings\n        logger.info(f\"\\nFound {len(log_files)} log files:\")\n        for log_file in log_files:\n            logger.info(f\"- {log_file.name}\")\n            # Could add additional checks for log file format/content here\n            \n        logger.info(f\"\\nFound {len(json_files)} JSON files:\")\n        for json_file in json_files:\n            logger.info(f\"- {json_file.name}\")\n            # Could add additional checks for JSON file structure here\n            \n    except ImportError as e:\n        logger.error(f\"Import error: {e}\")\n        logger.error(\"Make sure gpt_researcher is installed and in your PYTHONPATH\")\n        raise\n    except Exception as e:\n        logger.error(f\"Error during research: {e}\")\n        raise\n\nif __name__ == \"__main__\":\n    pytest.main([__file__]) "}
{"type": "test_file", "path": "tests/vector-store.py", "content": "import asyncio\nimport pytest\nfrom typing import List\nfrom gpt_researcher import GPTResearcher\n\nfrom langchain.text_splitter import CharacterTextSplitter\nfrom langchain_openai import OpenAIEmbeddings\nfrom langchain_community.vectorstores import FAISS, InMemoryVectorStore\nfrom langchain_core.documents import Document\n\n\n# taken from https://paulgraham.com/persistence.html\nessay = \"\"\"\nThe right kind of Stubborn\n\nJuly 2024\n\nSuccessful people tend to be persistent. New ideas often don't work at first, but they're not deterred. They keep trying and eventually find something that does.\n\nMere obstinacy, on the other hand, is a recipe for failure. Obstinate people are so annoying. They won't listen. They beat their heads against a wall and get nowhere.\n\nBut is there any real difference between these two cases? Are persistent and obstinate people actually behaving differently? Or are they doing the same thing, and we just label them later as persistent or obstinate depending on whether they turned out to be right or not?\n\nIf that's the only difference then there's nothing to be learned from the distinction. Telling someone to be persistent rather than obstinate would just be telling them to be right rather than wrong, and they already know that. Whereas if persistence and obstinacy are actually different kinds of behavior, it would be worthwhile to tease them apart. [1]\n\nI've talked to a lot of determined people, and it seems to me that they're different kinds of behavior. I've often walked away from a conversation thinking either \"Wow, that guy is determined\" or \"Damn, that guy is stubborn,\" and I don't think I'm just talking about whether they seemed right or not. That's part of it, but not all of it.\n\nThere's something annoying about the obstinate that's not simply due to being mistaken. They won't listen. And that's not true of all determined people. I can't think of anyone more determined than the Collison brothers, and when you point out a problem to them, they not only listen, but listen with an almost predatory intensity. Is there a hole in the bottom of their boat? Probably not, but if there is, they want to know about it.\n\nIt's the same with most successful people. They're never more engaged than when you disagree with them. Whereas the obstinate don't want to hear you. When you point out problems, their eyes glaze over, and their replies sound like ideologues talking about matters of doctrine. [2]\n\nThe reason the persistent and the obstinate seem similar is that they're both hard to stop. But they're hard to stop in different senses. The persistent are like boats whose engines can't be throttled back. The obstinate are like boats whose rudders can't be turned. [3]\n\nIn the degenerate case they're indistinguishable: when there's only one way to solve a problem, your only choice is whether to give up or not, and persistence and obstinacy both say no. This is presumably why the two are so often conflated in popular culture. It assumes simple problems. But as problems get more complicated, we can see the difference between them. The persistent are much more attached to points high in the decision tree than to minor ones lower down, while the obstinate spray \"don't give up\" indiscriminately over the whole tree.\n\nThe persistent are attached to the goal. The obstinate are attached to their ideas about how to reach it.\n\nWorse still, that means they'll tend to be attached to their first ideas about how to solve a problem, even though these are the least informed by the experience of working on it. So the obstinate aren't merely attached to details, but disproportionately likely to be attached to wrong ones.\n\n\n\nWhy are they like this? Why are the obstinate obstinate? One possibility is that they're overwhelmed. They're not very capable. They take on a hard problem. They're immediately in over their head. So they grab onto ideas the way someone on the deck of a rolling ship might grab onto the nearest handhold.\n\nThat was my initial theory, but on examination it doesn't hold up. If being obstinate were simply a consequence of being in over one's head, you could make persistent people become obstinate by making them solve harder problems. But that's not what happens. If you handed the Collisons an extremely hard problem to solve, they wouldn't become obstinate. If anything they'd become less obstinate. They'd know they had to be open to anything.\n\nSimilarly, if obstinacy were caused by the situation, the obstinate would stop being obstinate when solving easier problems. But they don't. And if obstinacy isn't caused by the situation, it must come from within. It must be a feature of one's personality.\n\nObstinacy is a reflexive resistance to changing one's ideas. This is not identical with stupidity, but they're closely related. A reflexive resistance to changing one's ideas becomes a sort of induced stupidity as contrary evidence mounts. And obstinacy is a form of not giving up that's easily practiced by the stupid. You don't have to consider complicated tradeoffs; you just dig in your heels. It even works, up to a point.\n\nThe fact that obstinacy works for simple problems is an important clue. Persistence and obstinacy aren't opposites. The relationship between them is more like the relationship between the two kinds of respiration we can do: aerobic respiration, and the anaerobic respiration we inherited from our most distant ancestors. Anaerobic respiration is a more primitive process, but it has its uses. When you leap suddenly away from a threat, that's what you're using.\n\nThe optimal amount of obstinacy is not zero. It can be good if your initial reaction to a setback is an unthinking \"I won't give up,\" because this helps prevent panic. But unthinking only gets you so far. The further someone is toward the obstinate end of the continuum, the less likely they are to succeed in solving hard problems. [4]\n\n\n\nObstinacy is a simple thing. Animals have it. But persistence turns out to have a fairly complicated internal structure.\n\nOne thing that distinguishes the persistent is their energy. At the risk of putting too much weight on words, they persist rather than merely resisting. They keep trying things. Which means the persistent must also be imaginative. To keep trying things, you have to keep thinking of things to try.\n\nEnergy and imagination make a wonderful combination. Each gets the best out of the other. Energy creates demand for the ideas produced by imagination, which thus produces more, and imagination gives energy somewhere to go. [5]\n\nMerely having energy and imagination is quite rare. But to solve hard problems you need three more qualities: resilience, good judgement, and a focus on some kind of goal.\n\nResilience means not having one's morale destroyed by setbacks. Setbacks are inevitable once problems reach a certain size, so if you can't bounce back from them, you can only do good work on a small scale. But resilience is not the same as obstinacy. Resilience means setbacks can't change your morale, not that they can't change your mind.\n\nIndeed, persistence often requires that one change one's mind. That's where good judgement comes in. The persistent are quite rational. They focus on expected value. It's this, not recklessness, that lets them work on things that are unlikely to succeed.\n\nThere is one point at which the persistent are often irrational though: at the very top of the decision tree. When they choose between two problems of roughly equal expected value, the choice usually comes down to personal preference. Indeed, they'll often classify projects into deliberately wide bands of expected value in order to ensure that the one they want to work on still qualifies.\n\nEmpirically this doesn't seem to be a problem. It's ok to be irrational near the top of the decision tree. One reason is that we humans will work harder on a problem we love. But there's another more subtle factor involved as well: our preferences among problems aren't random. When we love a problem that other people don't, it's often because we've unconsciously noticed that it's more important than they realize.\n\nWhich leads to our fifth quality: there needs to be some overall goal. If you're like me you began, as a kid, merely with the desire to do something great. In theory that should be the most powerful motivator of all, since it includes everything that could possibly be done. But in practice it's not much use, precisely because it includes too much. It doesn't tell you what to do at this moment.\n\nSo in practice your energy and imagination and resilience and good judgement have to be directed toward some fairly specific goal. Not too specific, or you might miss a great discovery adjacent to what you're searching for, but not too general, or it won't work to motivate you. [6]\n\nWhen you look at the internal structure of persistence, it doesn't resemble obstinacy at all. It's so much more complex. Five distinct qualities — energy, imagination, resilience, good judgement, and focus on a goal — combine to produce a phenomenon that seems a bit like obstinacy in the sense that it causes you not to give up. But the way you don't give up is completely different. Instead of merely resisting change, you're driven toward a goal by energy and resilience, through paths discovered by imagination and optimized by judgement. You'll give way on any point low down in the decision tree, if its expected value drops sufficiently, but energy and resilience keep pushing you toward whatever you choose higher up.\n\nConsidering what it's made of, it's not surprising that the right kind of stubbornness is so much rarer than the wrong kind, or that it gets so much better results. Anyone can do obstinacy. Indeed, kids and drunks and fools are best at it. Whereas very few people have enough of all five of the qualities that produce the right kind of stubbornness, but when they do the results are magical.\n\n\n\n\n\n\n\nNotes\n\n[1] I'm going to use \"persistent\" for the good kind of stubborn and \"obstinate\" for the bad kind, but I can't claim I'm simply following current usage. Conventional opinion barely distinguishes between good and bad kinds of stubbornness, and usage is correspondingly promiscuous. I could have invented a new word for the good kind, but it seemed better just to stretch \"persistent.\"\n\n[2] There are some domains where one can succeed by being obstinate. Some political leaders have been notorious for it. But it won't work in situations where you have to pass external tests. And indeed the political leaders who are famous for being obstinate are famous for getting power, not for using it well.\n\n[3] There will be some resistance to turning the rudder of a persistent person, because there's some cost to changing direction.\n\n[4] The obstinate do sometimes succeed in solving hard problems. One way is through luck: like the stopped clock that's right twice a day, they seize onto some arbitrary idea, and it turns out to be right. Another is when their obstinacy cancels out some other form of error. For example, if a leader has overcautious subordinates, their estimates of the probability of success will always be off in the same direction. So if he mindlessly says \"push ahead regardless\" in every borderline case, he'll usually turn out to be right.\n\n[5] If you stop there, at just energy and imagination, you get the conventional caricature of an artist or poet.\n\n[6] Start by erring on the small side. If you're inexperienced you'll inevitably err on one side or the other, and if you err on the side of making the goal too broad, you won't get anywhere. Whereas if you err on the small side you'll at least be moving forward. Then, once you're moving, you expand the goal.\n\"\"\"\n\n\ndef load_document():\n    document = [Document(page_content=essay)]\n    text_splitter = CharacterTextSplitter(chunk_size=200, chunk_overlap=30, separator=\"\\n\")\n    return text_splitter.split_documents(documents=document)\n\n\ndef create_vectorstore(documents: List[Document]):\n    embeddings = OpenAIEmbeddings()\n    return FAISS.from_documents(documents, embeddings)\n\n@pytest.mark.asyncio\nasync def test_gpt_researcher_with_vector_store():\n    docs = load_document()\n    vectorstore = create_vectorstore(docs)\n\n    query = \"\"\"\n        Summarize the essay into 3 or 4 succint sections.\n        Make sure to include key points regarding the differences between\n        persistance vs obstinate.\n\n        Include some recommendations for entrepeneurs in the conclusion.\n        Recommend some ways to increase persistance in a healthy way.\n    \"\"\"\n\n\n    # Create an instance of GPTResearcher\n    researcher = GPTResearcher(\n        query=query,\n        report_type=\"research_report\",\n        report_source=\"langchain_vectorstore\",\n        vector_store=vectorstore,\n    )\n\n    # Conduct research and write the report\n    await researcher.conduct_research()\n    report = await researcher.write_report()\n\n    assert report is not None\n\n@pytest.mark.asyncio\nasync def test_store_in_vector_store_web():\n    vector_store = InMemoryVectorStore(embedding=OpenAIEmbeddings())\n    query = \"Which one is the best LLM\"\n\n    researcher = GPTResearcher(\n        query=query,\n        report_type=\"research_report\",\n        report_source=\"web\",\n        vector_store=vector_store,\n    )\n\n    await researcher.conduct_research()\n\n    related_contexts = await vector_store.asimilarity_search(\"GPT-4\", k=2)\n\n    assert len(related_contexts) == 2\n    # Add more assertions as needed to verify the results\n\n\n@pytest.mark.asyncio\nasync def test_store_in_vector_store_urls():\n    vector_store = InMemoryVectorStore(embedding=OpenAIEmbeddings())\n    query = \"Who won the world cup in 2022\"\n\n    researcher = GPTResearcher(\n        query=query,\n        report_type=\"research_report\",\n        vector_store=vector_store,\n        source_urls=[\"https://en.wikipedia.org/wiki/FIFA_World_Cup\"]\n    )\n\n    await researcher.conduct_research()\n\n    related_contexts = await vector_store.asimilarity_search(\"GPT-4\", k=2)\n\n    assert len(related_contexts) == 2\n\n\n@pytest.mark.asyncio\nasync def test_store_in_vector_store_langchain_docs():\n    vector_store = InMemoryVectorStore(embedding=OpenAIEmbeddings())\n    docs = load_document()\n    query = \"What does successful people tend to do?\"\n\n    researcher = GPTResearcher(\n        query=query,\n        report_type=\"research_report\",\n        vector_store=vector_store,\n        report_source=\"langchain_documents\",\n        documents=docs\n    )\n\n    await researcher.conduct_research()\n\n    related_contexts = await vector_store.asimilarity_search(\"GPT-4\", k=2)\n\n    assert len(related_contexts) == 2\n\n@pytest.mark.asyncio\nasync def test_store_in_vector_store_locals():\n    vector_store = InMemoryVectorStore(embedding=OpenAIEmbeddings())\n    query = \"What is transformer?\"\n\n    researcher = GPTResearcher(\n        query=query,\n        report_type=\"research_report\",\n        vector_store=vector_store,\n        report_source=\"local\",\n        config_path= \"test_local\"\n    )\n\n    await researcher.conduct_research()\n\n    related_contexts = await vector_store.asimilarity_search(\"GPT-4\", k=2)\n\n    assert len(related_contexts) == 2\n\n@pytest.mark.asyncio\nasync def test_store_in_vector_store_hybrids():\n    vector_store = InMemoryVectorStore(embedding=OpenAIEmbeddings())\n    query = \"What is transformer?\"\n    \n    researcher = GPTResearcher(\n        query=query,\n        report_type=\"research_report\",\n        vector_store=vector_store,\n        report_source=\"hybrid\",\n        config_path= \"test_local\"\n    )\n    \n    await researcher.conduct_research()\n    \n    related_contexts = await vector_store.asimilarity_search(\"GPT-4\", k=2)\n    \n    assert len(related_contexts) == 2\n"}
{"type": "source_file", "path": "backend/__init__.py", "content": "from multi_agents import agents"}
{"type": "source_file", "path": "backend/chat/__init__.py", "content": "from .chat import ChatAgentWithMemory"}
{"type": "source_file", "path": "backend/memory/__init__.py", "content": ""}
{"type": "source_file", "path": "backend/chat/chat.py", "content": "from fastapi import WebSocket\nimport uuid\n\nfrom gpt_researcher.utils.llm import get_llm\nfrom gpt_researcher.memory import Memory\nfrom gpt_researcher.config.config import Config\n\nfrom langgraph.prebuilt import create_react_agent\nfrom langgraph.checkpoint.memory import MemorySaver\n\nfrom langchain_community.vectorstores import InMemoryVectorStore\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.tools import Tool, tool\n\nclass ChatAgentWithMemory:\n    def __init__(\n        self,\n        report: str,\n        config_path,\n        headers,\n        vector_store = None\n    ):\n        self.report = report\n        self.headers = headers\n        self.config = Config(config_path)\n        self.vector_store = vector_store\n        self.graph = self.create_agent()\n\n    def create_agent(self):\n        \"\"\"Create React Agent Graph\"\"\"\n        cfg = Config()\n\n        # Retrieve LLM using get_llm with settings from config\n        provider = get_llm(\n            llm_provider=cfg.smart_llm_provider,\n            model=cfg.smart_llm_model,\n            temperature=0.35,\n            max_tokens=cfg.smart_token_limit,\n            **self.config.llm_kwargs\n        ).llm\n\n        # If vector_store is not initialized, process documents and add to vector_store\n        if not self.vector_store:\n            documents = self._process_document(self.report)\n            self.chat_config = {\"configurable\": {\"thread_id\": str(uuid.uuid4())}}\n            self.embedding = Memory(\n                cfg.embedding_provider,\n                cfg.embedding_model,\n                **cfg.embedding_kwargs\n            ).get_embeddings()\n            self.vector_store = InMemoryVectorStore(self.embedding)\n            self.vector_store.add_texts(documents)\n\n        # Create the React Agent Graph with the configured provider\n        graph = create_react_agent(\n            provider,\n            tools=[self.vector_store_tool(self.vector_store)],\n            checkpointer=MemorySaver()\n        )\n        \n        return graph\n    \n    def vector_store_tool(self, vector_store) -> Tool:\n        \"\"\"Create Vector Store Tool\"\"\"\n        @tool \n        def retrieve_info(query):\n            \"\"\"\n            Consult the report for relevant contexts whenever you don't know something\n            \"\"\"\n            retriever = vector_store.as_retriever(k = 4)\n            return retriever.invoke(query)\n        return retrieve_info\n        \n    def _process_document(self, report):\n        \"\"\"Split Report into Chunks\"\"\"\n        text_splitter = RecursiveCharacterTextSplitter(\n            chunk_size=1024,\n            chunk_overlap=20,\n            length_function=len,\n            is_separator_regex=False,\n        )\n        documents = text_splitter.split_text(report)\n        return documents\n\n    async def chat(self, message, websocket):\n        \"\"\"Chat with React Agent\"\"\"\n        message = f\"\"\"\n         You are GPT Researcher, a autonomous research agent created by an open source community at https://github.com/assafelovic/gpt-researcher, homepage: https://gptr.dev. \n         To learn more about GPT Researcher you can suggest to check out: https://docs.gptr.dev.\n         \n         This is a chat message between the user and you: GPT Researcher. \n         The chat is about a research reports that you created. Answer based on the given context and report.\n         You must include citations to your answer based on the report.\n         \n         Report: {self.report}\n         User Message: {message}\n        \"\"\"\n        inputs = {\"messages\": [(\"user\", message)]}\n        response = await self.graph.ainvoke(inputs, config=self.chat_config)\n        ai_message = response[\"messages\"][-1].content\n        if websocket is not None:\n            await websocket.send_json({\"type\": \"chat\", \"content\": ai_message})\n\n    def get_context(self):\n        \"\"\"return the current context of the chat\"\"\"\n        return self.report\n"}
{"type": "source_file", "path": "backend/memory/draft.py", "content": "from typing import TypedDict, List, Annotated\nimport operator\n\n\nclass DraftState(TypedDict):\n    task: dict\n    topic: str\n    draft: dict\n    review: str\n    revision_notes: str"}
{"type": "source_file", "path": "backend/memory/research.py", "content": "from typing import TypedDict, List, Annotated\nimport operator\n\n\nclass ResearchState(TypedDict):\n    task: dict\n    initial_research: str\n    sections: List[str]\n    research_data: List[dict]\n    # Report layout\n    title: str\n    headers: dict\n    date: str\n    table_of_contents: str\n    introduction: str\n    conclusion: str\n    sources: List[str]\n    report: str\n\n\n"}
{"type": "source_file", "path": "backend/report_type/basic_report/__init__.py", "content": ""}
{"type": "source_file", "path": "backend/report_type/__init__.py", "content": "from .basic_report.basic_report import BasicReport\nfrom .detailed_report.detailed_report import DetailedReport\n\n__all__ = [\n    \"BasicReport\",\n    \"DetailedReport\"\n]"}
{"type": "source_file", "path": "backend/report_type/basic_report/basic_report.py", "content": "from fastapi import WebSocket\nfrom typing import Any\n\nfrom gpt_researcher import GPTResearcher\n\n\nclass BasicReport:\n    def __init__(\n        self,\n        query: str,\n        query_domains: list,\n        report_type: str,\n        report_source: str,\n        source_urls,\n        document_urls,\n        tone: Any,\n        config_path: str,\n        websocket: WebSocket,\n        headers=None\n    ):\n        self.query = query\n        self.query_domains = query_domains\n        self.report_type = report_type\n        self.report_source = report_source\n        self.source_urls = source_urls\n        self.document_urls = document_urls\n        self.tone = tone\n        self.config_path = config_path\n        self.websocket = websocket\n        self.headers = headers or {}\n\n        # Initialize researcher\n        self.gpt_researcher = GPTResearcher(\n            query=self.query,\n            query_domains=self.query_domains,\n            report_type=self.report_type,\n            report_source=self.report_source,\n            source_urls=self.source_urls,\n            document_urls=self.document_urls,\n            tone=self.tone,\n            config_path=self.config_path,\n            websocket=self.websocket,\n            headers=self.headers\n        )\n\n    async def run(self):\n        await self.gpt_researcher.conduct_research()\n        report = await self.gpt_researcher.write_report()\n        return report\n"}
{"type": "source_file", "path": "backend/report_type/deep_research/__init__.py", "content": ""}
{"type": "source_file", "path": "backend/report_type/deep_research/example.py", "content": "from typing import List, Dict, Any, Optional, Set\nfrom fastapi import WebSocket\nimport asyncio\nimport logging\nfrom gpt_researcher import GPTResearcher\nfrom gpt_researcher.llm_provider.generic.base import ReasoningEfforts\nfrom gpt_researcher.utils.llm import create_chat_completion\nfrom gpt_researcher.utils.enum import ReportType, ReportSource, Tone\n\nlogger = logging.getLogger(__name__)\n\n# Constants for models\nGPT4_MODEL = \"gpt-4o\"  # For standard tasks\nO3_MINI_MODEL = \"o3-mini\"  # For reasoning tasks\nLLM_PROVIDER = \"openai\"\n\nclass ResearchProgress:\n    def __init__(self, total_depth: int, total_breadth: int):\n        self.current_depth = total_depth\n        self.total_depth = total_depth\n        self.current_breadth = total_breadth\n        self.total_breadth = total_breadth\n        self.current_query: Optional[str] = None\n        self.total_queries = 0\n        self.completed_queries = 0\n\nclass DeepResearch:\n    def __init__(\n        self,\n        query: str,\n        breadth: int = 4,\n        depth: int = 2,\n        websocket: Optional[WebSocket] = None,\n        tone: Tone = Tone.Objective,\n        config_path: Optional[str] = None,\n        headers: Optional[Dict] = None,\n        concurrency_limit: int = 2  # Match TypeScript version\n    ):\n        self.query = query\n        self.breadth = breadth\n        self.depth = depth\n        self.websocket = websocket\n        self.tone = tone\n        self.config_path = config_path\n        self.headers = headers or {}\n        self.visited_urls: Set[str] = set()\n        self.learnings: List[str] = []\n        self.concurrency_limit = concurrency_limit\n\n    async def generate_feedback(self, query: str, num_questions: int = 3) -> List[str]:\n        \"\"\"Generate follow-up questions to clarify research direction\"\"\"\n        messages = [\n            {\"role\": \"system\", \"content\": \"You are an expert researcher helping to clarify research directions.\"},\n            {\"role\": \"user\", \"content\": f\"Given the following query from the user, ask some follow up questions to clarify the research direction. Return a maximum of {num_questions} questions, but feel free to return less if the original query is clear. Format each question on a new line starting with 'Question: ': {query}\"}\n        ]\n\n        response = await create_chat_completion(\n            messages=messages,\n            llm_provider=LLM_PROVIDER,\n            model=O3_MINI_MODEL,  # Using reasoning model for better question generation\n            temperature=0.7,\n            max_tokens=500,\n            reasoning_effort=ReasoningEfforts.High.value\n        )\n\n        # Parse questions from response\n        questions = [q.replace('Question:', '').strip()\n                    for q in response.split('\\n')\n                    if q.strip().startswith('Question:')]\n        return questions[:num_questions]\n\n    async def generate_serp_queries(self, query: str, num_queries: int = 3) -> List[Dict[str, str]]:\n        \"\"\"Generate SERP queries for research\"\"\"\n        messages = [\n            {\"role\": \"system\", \"content\": \"You are an expert researcher generating search queries.\"},\n            {\"role\": \"user\", \"content\": f\"Given the following prompt, generate {num_queries} unique search queries to research the topic thoroughly. For each query, provide a research goal. Format as 'Query: <query>' followed by 'Goal: <goal>' for each pair: {query}\"}\n        ]\n\n        response = await create_chat_completion(\n            messages=messages,\n            llm_provider=LLM_PROVIDER,\n            model=GPT4_MODEL,  # Using GPT-4 for general task\n            temperature=0.7,\n            max_tokens=1000\n        )\n\n        # Parse queries and goals from response\n        lines = response.split('\\n')\n        queries = []\n        current_query = {}\n\n        for line in lines:\n            line = line.strip()\n            if line.startswith('Query:'):\n                if current_query:\n                    queries.append(current_query)\n                current_query = {'query': line.replace('Query:', '').strip()}\n            elif line.startswith('Goal:') and current_query:\n                current_query['researchGoal'] = line.replace('Goal:', '').strip()\n\n        if current_query:\n            queries.append(current_query)\n\n        return queries[:num_queries]\n\n    async def process_serp_result(self, query: str, context: str, num_learnings: int = 3) -> Dict[str, List[str]]:\n        \"\"\"Process research results to extract learnings and follow-up questions\"\"\"\n        messages = [\n            {\"role\": \"system\", \"content\": \"You are an expert researcher analyzing search results.\"},\n            {\"role\": \"user\", \"content\": f\"Given the following research results for the query '{query}', extract key learnings and suggest follow-up questions. For each learning, include a citation to the source URL if available. Format each learning as 'Learning [source_url]: <insight>' and each question as 'Question: <question>':\\n\\n{context}\"}\n        ]\n\n        response = await create_chat_completion(\n            messages=messages,\n            llm_provider=LLM_PROVIDER,\n            model=O3_MINI_MODEL,  # Using reasoning model for analysis\n            temperature=0.7,\n            max_tokens=1000,\n            reasoning_effort=ReasoningEfforts.High.value\n        )\n\n        # Parse learnings and questions with citations\n        lines = response.split('\\n')\n        learnings = []\n        questions = []\n        citations = {}\n\n        for line in lines:\n            line = line.strip()\n            if line.startswith('Learning'):\n                # Extract URL if present in square brackets\n                import re\n                url_match = re.search(r'\\[(.*?)\\]:', line)\n                if url_match:\n                    url = url_match.group(1)\n                    learning = line.split(':', 1)[1].strip()\n                    learnings.append(learning)\n                    citations[learning] = url\n                else:\n                    learnings.append(line.replace('Learning:', '').strip())\n            elif line.startswith('Question:'):\n                questions.append(line.replace('Question:', '').strip())\n\n        return {\n            'learnings': learnings[:num_learnings],\n            'followUpQuestions': questions[:num_learnings],\n            'citations': citations\n        }\n\n    async def deep_research(\n        self,\n        query: str,\n        breadth: int,\n        depth: int,\n        learnings: List[str] = None,\n        citations: Dict[str, str] = None,\n        visited_urls: Set[str] = None,\n        on_progress = None\n    ) -> Dict[str, Any]:\n        \"\"\"Conduct deep iterative research\"\"\"\n        if learnings is None:\n            learnings = []\n        if citations is None:\n            citations = {}\n        if visited_urls is None:\n            visited_urls = set()\n\n        progress = ResearchProgress(depth, breadth)\n\n        if on_progress:\n            on_progress(progress)\n\n        # Generate search queries\n        serp_queries = await self.generate_serp_queries(query, num_queries=breadth)\n        progress.total_queries = len(serp_queries)\n\n        all_learnings = learnings.copy()\n        all_citations = citations.copy()\n        all_visited_urls = visited_urls.copy()\n\n        # Process queries with concurrency limit\n        semaphore = asyncio.Semaphore(self.concurrency_limit)\n\n        async def process_query(serp_query: Dict[str, str]) -> Optional[Dict[str, Any]]:\n            async with semaphore:\n                try:\n                    progress.current_query = serp_query['query']\n                    if on_progress:\n                        on_progress(progress)\n\n                    # Initialize researcher for this query\n                    researcher = GPTResearcher(\n                        query=serp_query['query'],\n                        report_type=ReportType.ResearchReport.value,\n                        report_source=ReportSource.Web.value,\n                        tone=self.tone,\n                        websocket=self.websocket,\n                        config_path=self.config_path,\n                        headers=self.headers\n                    )\n\n                    # Conduct research\n                    await researcher.conduct_research()\n\n                    # Get results\n                    context = researcher.context\n                    visited = set(researcher.visited_urls)\n\n                    # Process results\n                    results = await self.process_serp_result(\n                        query=serp_query['query'],\n                        context=context\n                    )\n\n                    # Update progress\n                    progress.completed_queries += 1\n                    if on_progress:\n                        on_progress(progress)\n\n                    return {\n                        'learnings': results['learnings'],\n                        'visited_urls': visited,\n                        'followUpQuestions': results['followUpQuestions'],\n                        'researchGoal': serp_query['researchGoal'],\n                        'citations': results['citations']\n                    }\n\n                except Exception as e:\n                    logger.error(f\"Error processing query '{serp_query['query']}': {str(e)}\")\n                    return None\n\n        # Process queries concurrently with limit\n        tasks = [process_query(query) for query in serp_queries]\n        results = await asyncio.gather(*tasks)\n        results = [r for r in results if r is not None]  # Filter out failed queries\n\n        # Collect all results\n        for result in results:\n            all_learnings.extend(result['learnings'])\n            all_visited_urls.update(set(result['visited_urls']))\n            all_citations.update(result['citations'])\n\n            # Continue deeper if needed\n            if depth > 1:\n                new_breadth = max(2, breadth // 2)\n                new_depth = depth - 1\n\n                # Create next query from research goal and follow-up questions\n                next_query = f\"\"\"\n                Previous research goal: {result['researchGoal']}\n                Follow-up questions: {' '.join(result['followUpQuestions'])}\n                \"\"\"\n\n                # Recursive research\n                deeper_results = await self.deep_research(\n                    query=next_query,\n                    breadth=new_breadth,\n                    depth=new_depth,\n                    learnings=all_learnings,\n                    citations=all_citations,\n                    visited_urls=all_visited_urls,\n                    on_progress=on_progress\n                )\n\n                all_learnings = deeper_results['learnings']\n                all_visited_urls = set(deeper_results['visited_urls'])\n                all_citations.update(deeper_results['citations'])\n\n        return {\n            'learnings': list(set(all_learnings)),\n            'visited_urls': list(all_visited_urls),\n            'citations': all_citations\n        }\n\n    async def run(self, on_progress=None) -> str:\n        \"\"\"Run the deep research process and generate final report\"\"\"\n        # Get initial feedback\n        follow_up_questions = await self.generate_feedback(self.query)\n\n        # Collect answers (this would normally come from user interaction)\n        answers = [\"Automatically proceeding with research\"] * len(follow_up_questions)\n\n        # Combine query and Q&A\n        combined_query = f\"\"\"\n        Initial Query: {self.query}\n        Follow-up Questions and Answers:\n        {' '.join([f'Q: {q}\\nA: {a}' for q, a in zip(follow_up_questions, answers)])}\n        \"\"\"\n\n        # Run deep research\n        results = await self.deep_research(\n            query=combined_query,\n            breadth=self.breadth,\n            depth=self.depth,\n            on_progress=on_progress\n        )\n\n        # Generate final report\n        researcher = GPTResearcher(\n            query=self.query,\n            report_type=ReportType.DetailedReport.value,\n            report_source=ReportSource.Web.value,\n            tone=self.tone,\n            websocket=self.websocket,\n            config_path=self.config_path,\n            headers=self.headers\n        )\n\n        # Prepare context with citations\n        context_with_citations = []\n        for learning in results['learnings']:\n            citation = results['citations'].get(learning, '')\n            if citation:\n                context_with_citations.append(f\"{learning} [Source: {citation}]\")\n            else:\n                context_with_citations.append(learning)\n\n        # Set enhanced context for final report\n        researcher.context = \"\\n\".join(context_with_citations)\n        researcher.visited_urls = set(results['visited_urls'])\n\n        # Generate report\n        report = await researcher.write_report()\n        return report"}
{"type": "source_file", "path": "backend/report_type/deep_research/main.py", "content": "from gpt_researcher import GPTResearcher\nfrom backend.utils import write_md_to_pdf\nimport asyncio\n\n\nasync def main(task: str):\n    # Progress callback\n    def on_progress(progress):\n        print(f\"Depth: {progress.current_depth}/{progress.total_depth}\")\n        print(f\"Breadth: {progress.current_breadth}/{progress.total_breadth}\")\n        print(f\"Queries: {progress.completed_queries}/{progress.total_queries}\")\n        if progress.current_query:\n            print(f\"Current query: {progress.current_query}\")\n    \n    # Initialize researcher with deep research type\n    researcher = GPTResearcher(\n        query=task,\n        report_type=\"deep\",  # This will trigger deep research\n    )\n    \n    # Run research with progress tracking\n    print(\"Starting deep research...\")\n    context = await researcher.conduct_research(on_progress=on_progress)\n    print(\"\\nResearch completed. Generating report...\")\n    \n    # Generate the final report\n    report = await researcher.write_report()\n    await write_md_to_pdf(report, \"deep_research_report\")\n    print(f\"\\nFinal Report: {report}\")\n\nif __name__ == \"__main__\":\n    query = \"What are the most effective ways for beginners to start investing?\"\n    asyncio.run(main(query))"}
{"type": "source_file", "path": "backend/report_type/detailed_report/__init__.py", "content": ""}
{"type": "source_file", "path": "backend/report_type/detailed_report/detailed_report.py", "content": "import asyncio\nfrom typing import List, Dict, Set, Optional, Any\nfrom fastapi import WebSocket\n\nfrom gpt_researcher import GPTResearcher\n\n\nclass DetailedReport:\n    def __init__(\n        self,\n        query: str,\n        report_type: str,\n        report_source: str,\n        source_urls: List[str] = [],\n        document_urls: List[str] = [],\n        query_domains: List[str] = [],\n        config_path: str = None,\n        tone: Any = \"\",\n        websocket: WebSocket = None,\n        subtopics: List[Dict] = [],\n        headers: Optional[Dict] = None\n    ):\n        self.query = query\n        self.report_type = report_type\n        self.report_source = report_source\n        self.source_urls = source_urls\n        self.document_urls = document_urls\n        self.query_domains = query_domains\n        self.config_path = config_path\n        self.tone = tone\n        self.websocket = websocket\n        self.subtopics = subtopics\n        self.headers = headers or {}\n\n        self.gpt_researcher = GPTResearcher(\n            query=self.query,\n            query_domains=self.query_domains,\n            report_type=\"research_report\",\n            report_source=self.report_source,\n            source_urls=self.source_urls,\n            document_urls=self.document_urls,\n            config_path=self.config_path,\n            tone=self.tone,\n            websocket=self.websocket,\n            headers=self.headers\n        )\n        self.existing_headers: List[Dict] = []\n        self.global_context: List[str] = []\n        self.global_written_sections: List[str] = []\n        self.global_urls: Set[str] = set(\n            self.source_urls) if self.source_urls else set()\n\n    async def run(self) -> str:\n        await self._initial_research()\n        subtopics = await self._get_all_subtopics()\n        report_introduction = await self.gpt_researcher.write_introduction()\n        _, report_body = await self._generate_subtopic_reports(subtopics)\n        self.gpt_researcher.visited_urls.update(self.global_urls)\n        report = await self._construct_detailed_report(report_introduction, report_body)\n        return report\n\n    async def _initial_research(self) -> None:\n        await self.gpt_researcher.conduct_research()\n        self.global_context = self.gpt_researcher.context\n        self.global_urls = self.gpt_researcher.visited_urls\n\n    async def _get_all_subtopics(self) -> List[Dict]:\n        subtopics_data = await self.gpt_researcher.get_subtopics()\n\n        all_subtopics = []\n        if subtopics_data and subtopics_data.subtopics:\n            for subtopic in subtopics_data.subtopics:\n                all_subtopics.append({\"task\": subtopic.task})\n        else:\n            print(f\"Unexpected subtopics data format: {subtopics_data}\")\n\n        return all_subtopics\n\n    async def _generate_subtopic_reports(self, subtopics: List[Dict]) -> tuple:\n        subtopic_reports = []\n        subtopics_report_body = \"\"\n\n        for subtopic in subtopics:\n            result = await self._get_subtopic_report(subtopic)\n            if result[\"report\"]:\n                subtopic_reports.append(result)\n                subtopics_report_body += f\"\\n\\n\\n{result['report']}\"\n\n        return subtopic_reports, subtopics_report_body\n\n    async def _get_subtopic_report(self, subtopic: Dict) -> Dict[str, str]:\n        current_subtopic_task = subtopic.get(\"task\")\n        subtopic_assistant = GPTResearcher(\n            query=current_subtopic_task,\n            query_domains=self.query_domains,\n            report_type=\"subtopic_report\",\n            report_source=self.report_source,\n            websocket=self.websocket,\n            headers=self.headers,\n            parent_query=self.query,\n            subtopics=self.subtopics,\n            visited_urls=self.global_urls,\n            agent=self.gpt_researcher.agent,\n            role=self.gpt_researcher.role,\n            tone=self.tone,\n        )\n\n        subtopic_assistant.context = list(set(self.global_context))\n        await subtopic_assistant.conduct_research()\n\n        draft_section_titles = await subtopic_assistant.get_draft_section_titles(current_subtopic_task)\n\n        if not isinstance(draft_section_titles, str):\n            draft_section_titles = str(draft_section_titles)\n\n        parse_draft_section_titles = self.gpt_researcher.extract_headers(draft_section_titles)\n        parse_draft_section_titles_text = [header.get(\n            \"text\", \"\") for header in parse_draft_section_titles]\n\n        relevant_contents = await subtopic_assistant.get_similar_written_contents_by_draft_section_titles(\n            current_subtopic_task, parse_draft_section_titles_text, self.global_written_sections\n        )\n\n        subtopic_report = await subtopic_assistant.write_report(self.existing_headers, relevant_contents)\n\n        self.global_written_sections.extend(self.gpt_researcher.extract_sections(subtopic_report))\n        self.global_context = list(set(subtopic_assistant.context))\n        self.global_urls.update(subtopic_assistant.visited_urls)\n\n        self.existing_headers.append({\n            \"subtopic task\": current_subtopic_task,\n            \"headers\": self.gpt_researcher.extract_headers(subtopic_report),\n        })\n\n        return {\"topic\": subtopic, \"report\": subtopic_report}\n\n    async def _construct_detailed_report(self, introduction: str, report_body: str) -> str:\n        toc = self.gpt_researcher.table_of_contents(report_body)\n        conclusion = await self.gpt_researcher.write_report_conclusion(report_body)\n        conclusion_with_references = self.gpt_researcher.add_references(\n            conclusion, self.gpt_researcher.visited_urls)\n        report = f\"{introduction}\\n\\n{toc}\\n\\n{report_body}\\n\\n{conclusion_with_references}\"\n        return report\n"}
{"type": "source_file", "path": "backend/server/__init__.py", "content": ""}
{"type": "source_file", "path": "backend/server/app.py", "content": "from fastapi import FastAPI\nfrom fastapi.middleware.cors import CORSMiddleware\nimport logging\n\nlogger = logging.getLogger(__name__)\n\napp = FastAPI()\n\n# Add CORS middleware\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],  # In production, replace with your frontend domain\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)"}
{"type": "source_file", "path": "backend/server/logging_config.py", "content": "import logging\nimport json\nimport os\nfrom datetime import datetime\nfrom pathlib import Path\n\nclass JSONResearchHandler:\n    def __init__(self, json_file):\n        self.json_file = json_file\n        self.research_data = {\n            \"timestamp\": datetime.now().isoformat(),\n            \"events\": [],\n            \"content\": {\n                \"query\": \"\",\n                \"sources\": [],\n                \"context\": [],\n                \"report\": \"\",\n                \"costs\": 0.0\n            }\n        }\n\n    def log_event(self, event_type: str, data: dict):\n        self.research_data[\"events\"].append({\n            \"timestamp\": datetime.now().isoformat(),\n            \"type\": event_type,\n            \"data\": data\n        })\n        self._save_json()\n\n    def update_content(self, key: str, value):\n        self.research_data[\"content\"][key] = value\n        self._save_json()\n\n    def _save_json(self):\n        with open(self.json_file, 'w') as f:\n            json.dump(self.research_data, f, indent=2)\n\ndef setup_research_logging():\n    # Create logs directory if it doesn't exist\n    logs_dir = Path(\"logs\")\n    logs_dir.mkdir(exist_ok=True)\n    \n    # Generate timestamp for log files\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    \n    # Create log file paths\n    log_file = logs_dir / f\"research_{timestamp}.log\"\n    json_file = logs_dir / f\"research_{timestamp}.json\"\n    \n    # Configure file handler for research logs\n    file_handler = logging.FileHandler(log_file)\n    file_handler.setLevel(logging.INFO)\n    file_handler.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))\n    \n    # Get research logger and configure it\n    research_logger = logging.getLogger('research')\n    research_logger.setLevel(logging.INFO)\n    \n    # Remove any existing handlers to avoid duplicates\n    research_logger.handlers.clear()\n    \n    # Add file handler\n    research_logger.addHandler(file_handler)\n    \n    # Add stream handler for console output\n    console_handler = logging.StreamHandler()\n    console_handler.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))\n    research_logger.addHandler(console_handler)\n    \n    # Prevent propagation to root logger to avoid duplicate logs\n    research_logger.propagate = False\n    \n    # Create JSON handler\n    json_handler = JSONResearchHandler(json_file)\n    \n    return str(log_file), str(json_file), research_logger, json_handler\n\n# Create a function to get the logger and JSON handler\ndef get_research_logger():\n    return logging.getLogger('research')\n\ndef get_json_handler():\n    return getattr(logging.getLogger('research'), 'json_handler', None)"}
{"type": "source_file", "path": "backend/server/server.py", "content": "import json\nimport os\nfrom typing import Dict, List\nimport time\n\nfrom fastapi import FastAPI, Request, WebSocket, WebSocketDisconnect, File, UploadFile, BackgroundTasks\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom fastapi.staticfiles import StaticFiles\nfrom fastapi.templating import Jinja2Templates\nfrom fastapi.responses import FileResponse\nfrom pydantic import BaseModel\n\nfrom backend.server.websocket_manager import WebSocketManager\nfrom backend.server.server_utils import (\n    get_config_dict, sanitize_filename,\n    update_environment_variables, handle_file_upload, handle_file_deletion,\n    execute_multi_agents, handle_websocket_communication\n)\n\nfrom backend.server.websocket_manager import run_agent\nfrom backend.utils import write_md_to_word, write_md_to_pdf\nfrom gpt_researcher.utils.logging_config import setup_research_logging\nfrom gpt_researcher.utils.enum import Tone\n\nimport logging\n\n# Get logger instance\nlogger = logging.getLogger(__name__)\n\n# Don't override parent logger settings\nlogger.propagate = True\n\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n    handlers=[\n        logging.StreamHandler()  # Only log to console\n    ]\n)\n\n# Models\n\n\nclass ResearchRequest(BaseModel):\n    task: str\n    report_type: str\n    report_source: str\n    tone: str\n    headers: dict | None = None\n    repo_name: str\n    branch_name: str\n    generate_in_background: bool = True\n\n\nclass ConfigRequest(BaseModel):\n    ANTHROPIC_API_KEY: str\n    TAVILY_API_KEY: str\n    LANGCHAIN_TRACING_V2: str\n    LANGCHAIN_API_KEY: str\n    OPENAI_API_KEY: str\n    DOC_PATH: str\n    RETRIEVER: str\n    GOOGLE_API_KEY: str = ''\n    GOOGLE_CX_KEY: str = ''\n    BING_API_KEY: str = ''\n    SEARCHAPI_API_KEY: str = ''\n    SERPAPI_API_KEY: str = ''\n    SERPER_API_KEY: str = ''\n    SEARX_URL: str = ''\n    XAI_API_KEY: str\n    DEEPSEEK_API_KEY: str\n\n\n# App initialization\napp = FastAPI()\n\n# Static files and templates\napp.mount(\"/site\", StaticFiles(directory=\"./frontend\"), name=\"site\")\napp.mount(\"/static\", StaticFiles(directory=\"./frontend/static\"), name=\"static\")\ntemplates = Jinja2Templates(directory=\"./frontend\")\n\n# WebSocket manager\nmanager = WebSocketManager()\n\n# Middleware\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"http://localhost:3000\"],\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\n# Constants\nDOC_PATH = os.getenv(\"DOC_PATH\", \"./my-docs\")\n\n# Startup event\n\n\n@app.on_event(\"startup\")\ndef startup_event():\n    os.makedirs(\"outputs\", exist_ok=True)\n    app.mount(\"/outputs\", StaticFiles(directory=\"outputs\"), name=\"outputs\")\n    # os.makedirs(DOC_PATH, exist_ok=True)  # Commented out to avoid creating the folder if not needed\n    \n\n# Routes\n\n\n@app.get(\"/\")\nasync def read_root(request: Request):\n    return templates.TemplateResponse(\"index.html\", {\"request\": request, \"report\": None})\n\n\n@app.get(\"/report/{research_id}\")\nasync def read_report(request: Request, research_id: str):\n    docx_path = os.path.join('outputs', f\"{research_id}.docx\")\n    if not os.path.exists(docx_path):\n        return {\"message\": \"Report not found.\"}\n    return FileResponse(docx_path)\n\n\nasync def write_report(research_request: ResearchRequest, research_id: str = None):\n    report_information = await run_agent(\n        task=research_request.task,\n        report_type=research_request.report_type,\n        report_source=research_request.report_source,\n        source_urls=[],\n        document_urls=[],\n        tone=Tone[research_request.tone],\n        websocket=None,\n        stream_output=None,\n        headers=research_request.headers,\n        query_domains=[],\n        config_path=\"\",\n        return_researcher=True\n    )\n\n    docx_path = await write_md_to_word(report_information[0], research_id)\n    pdf_path = await write_md_to_pdf(report_information[0], research_id)\n    if research_request.report_type != \"multi_agents\":\n        report, researcher = report_information\n        response = {\n            \"research_id\": research_id,\n            \"research_information\": {\n                \"source_urls\": researcher.get_source_urls(),\n                \"research_costs\": researcher.get_costs(),\n                \"visited_urls\": list(researcher.visited_urls),\n                \"research_images\": researcher.get_research_images(),\n                # \"research_sources\": researcher.get_research_sources(),  # Raw content of sources may be very large\n            },\n            \"report\": report,\n            \"docx_path\": docx_path,\n            \"pdf_path\": pdf_path\n        }\n    else:\n        response = { \"research_id\": research_id, \"report\": report, \"docx_path\": docx_path, \"pdf_path\": pdf_path }\n\n    return response\n\n@app.post(\"/report/\")\nasync def generate_report(research_request: ResearchRequest, background_tasks: BackgroundTasks):\n    research_id = sanitize_filename(f\"task_{int(time.time())}_{research_request.task}\")\n\n    if research_request.generate_in_background:\n        background_tasks.add_task(write_report, research_request=research_request, research_id=research_id)\n        return {\"message\": \"Your report is being generated in the background. Please check back later.\",\n                \"research_id\": research_id}\n    else:\n        response = await write_report(research_request, research_id)\n        return response\n\n\n@app.get(\"/files/\")\nasync def list_files():\n    if not os.path.exists(DOC_PATH):\n        os.makedirs(DOC_PATH, exist_ok=True)\n    files = os.listdir(DOC_PATH)\n    print(f\"Files in {DOC_PATH}: {files}\")\n    return {\"files\": files}\n\n\n@app.post(\"/api/multi_agents\")\nasync def run_multi_agents():\n    return await execute_multi_agents(manager)\n\n\n@app.post(\"/upload/\")\nasync def upload_file(file: UploadFile = File(...)):\n    return await handle_file_upload(file, DOC_PATH)\n\n\n@app.delete(\"/files/{filename}\")\nasync def delete_file(filename: str):\n    return await handle_file_deletion(filename, DOC_PATH)\n\n\n@app.websocket(\"/ws\")\nasync def websocket_endpoint(websocket: WebSocket):\n    await manager.connect(websocket)\n    try:\n        await handle_websocket_communication(websocket, manager)\n    except WebSocketDisconnect:\n        await manager.disconnect(websocket)\n"}
{"type": "source_file", "path": "backend/server/server_utils.py", "content": "import asyncio\nimport json\nimport os\nimport re\nimport time\nimport shutil\nimport traceback\nfrom typing import Awaitable, Dict, List, Any\nfrom fastapi.responses import JSONResponse, FileResponse\nfrom gpt_researcher.document.document import DocumentLoader\nfrom gpt_researcher import GPTResearcher\nfrom backend.utils import write_md_to_pdf, write_md_to_word, write_text_to_md\nfrom pathlib import Path\nfrom datetime import datetime\nfrom fastapi import HTTPException\nimport logging\n\nlogging.basicConfig(level=logging.DEBUG)\nlogger = logging.getLogger(__name__)\n\nclass CustomLogsHandler:\n    \"\"\"Custom handler to capture streaming logs from the research process\"\"\"\n    def __init__(self, websocket, task: str):\n        self.logs = []\n        self.websocket = websocket\n        sanitized_filename = sanitize_filename(f\"task_{int(time.time())}_{task}\")\n        self.log_file = os.path.join(\"outputs\", f\"{sanitized_filename}.json\")\n        self.timestamp = datetime.now().isoformat()\n        # Initialize log file with metadata\n        os.makedirs(\"outputs\", exist_ok=True)\n        with open(self.log_file, 'w') as f:\n            json.dump({\n                \"timestamp\": self.timestamp,\n                \"events\": [],\n                \"content\": {\n                    \"query\": \"\",\n                    \"sources\": [],\n                    \"context\": [],\n                    \"report\": \"\",\n                    \"costs\": 0.0\n                }\n            }, f, indent=2)\n\n    async def send_json(self, data: Dict[str, Any]) -> None:\n        \"\"\"Store log data and send to websocket\"\"\"\n        # Send to websocket for real-time display\n        if self.websocket:\n            await self.websocket.send_json(data)\n            \n        # Read current log file\n        with open(self.log_file, 'r') as f:\n            log_data = json.load(f)\n            \n        # Update appropriate section based on data type\n        if data.get('type') == 'logs':\n            log_data['events'].append({\n                \"timestamp\": datetime.now().isoformat(),\n                \"type\": \"event\",\n                \"data\": data\n            })\n        else:\n            # Update content section for other types of data\n            log_data['content'].update(data)\n            \n        # Save updated log file\n        with open(self.log_file, 'w') as f:\n            json.dump(log_data, f, indent=2)\n        logger.debug(f\"Log entry written to: {self.log_file}\")\n\n\nclass Researcher:\n    def __init__(self, query: str, report_type: str = \"research_report\"):\n        self.query = query\n        self.report_type = report_type\n        # Generate unique ID for this research task\n        self.research_id = f\"{datetime.now().strftime('%Y%m%d_%H%M%S')}_{hash(query)}\"\n        # Initialize logs handler with research ID\n        self.logs_handler = CustomLogsHandler(None, self.research_id)\n        self.researcher = GPTResearcher(\n            query=query,\n            report_type=report_type,\n            websocket=self.logs_handler\n        )\n\n    async def research(self) -> dict:\n        \"\"\"Conduct research and return paths to generated files\"\"\"\n        await self.researcher.conduct_research()\n        report = await self.researcher.write_report()\n        \n        # Generate the files\n        sanitized_filename = sanitize_filename(f\"task_{int(time.time())}_{self.query}\")\n        file_paths = await generate_report_files(report, sanitized_filename)\n        \n        # Get the JSON log path that was created by CustomLogsHandler\n        json_relative_path = os.path.relpath(self.logs_handler.log_file)\n        \n        return {\n            \"output\": {\n                **file_paths,  # Include PDF, DOCX, and MD paths\n                \"json\": json_relative_path\n            }\n        }\n\ndef sanitize_filename(filename: str) -> str:\n    # Split into components\n    prefix, timestamp, *task_parts = filename.split('_')\n    task = '_'.join(task_parts)\n    \n    # Calculate max length for task portion\n    # 255 - len(os.getcwd()) - len(\"\\\\gpt-researcher\\\\outputs\\\\\") - len(\"task_\") - len(timestamp) - len(\"_.json\") - safety_margin\n    max_task_length = 255 - len(os.getcwd()) - 24 - 5 - 10 - 6 - 5  # ~189 chars for task\n    \n    # Truncate task if needed\n    truncated_task = task[:max_task_length] if len(task) > max_task_length else task\n    \n    # Reassemble and clean the filename\n    sanitized = f\"{prefix}_{timestamp}_{truncated_task}\"\n    return re.sub(r\"[^\\w\\s-]\", \"\", sanitized).strip()\n\n\nasync def handle_start_command(websocket, data: str, manager):\n    json_data = json.loads(data[6:])\n    (\n        task,\n        report_type,\n        source_urls,\n        document_urls,\n        tone,\n        headers,\n        report_source,\n        query_domains,\n    ) = extract_command_data(json_data)\n\n    if not task or not report_type:\n        print(\"Error: Missing task or report_type\")\n        return\n\n    # Create logs handler with websocket and task\n    logs_handler = CustomLogsHandler(websocket, task)\n    # Initialize log content with query\n    await logs_handler.send_json({\n        \"query\": task,\n        \"sources\": [],\n        \"context\": [],\n        \"report\": \"\"\n    })\n\n    sanitized_filename = sanitize_filename(f\"task_{int(time.time())}_{task}\")\n\n    report = await manager.start_streaming(\n        task,\n        report_type,\n        report_source,\n        source_urls,\n        document_urls,\n        tone,\n        websocket,\n        headers,\n        query_domains,\n    )\n    report = str(report)\n    file_paths = await generate_report_files(report, sanitized_filename)\n    # Add JSON log path to file_paths\n    file_paths[\"json\"] = os.path.relpath(logs_handler.log_file)\n    await send_file_paths(websocket, file_paths)\n\n\nasync def handle_human_feedback(data: str):\n    feedback_data = json.loads(data[14:])  # Remove \"human_feedback\" prefix\n    print(f\"Received human feedback: {feedback_data}\")\n    # TODO: Add logic to forward the feedback to the appropriate agent or update the research state\n\nasync def handle_chat(websocket, data: str, manager):\n    json_data = json.loads(data[4:])\n    print(f\"Received chat message: {json_data.get('message')}\")\n    await manager.chat(json_data.get(\"message\"), websocket)\n\nasync def generate_report_files(report: str, filename: str) -> Dict[str, str]:\n    pdf_path = await write_md_to_pdf(report, filename)\n    docx_path = await write_md_to_word(report, filename)\n    md_path = await write_text_to_md(report, filename)\n    return {\"pdf\": pdf_path, \"docx\": docx_path, \"md\": md_path}\n\n\nasync def send_file_paths(websocket, file_paths: Dict[str, str]):\n    await websocket.send_json({\"type\": \"path\", \"output\": file_paths})\n\n\ndef get_config_dict(\n    langchain_api_key: str, openai_api_key: str, tavily_api_key: str,\n    google_api_key: str, google_cx_key: str, bing_api_key: str,\n    searchapi_api_key: str, serpapi_api_key: str, serper_api_key: str, searx_url: str\n) -> Dict[str, str]:\n    return {\n        \"LANGCHAIN_API_KEY\": langchain_api_key or os.getenv(\"LANGCHAIN_API_KEY\", \"\"),\n        \"OPENAI_API_KEY\": openai_api_key or os.getenv(\"OPENAI_API_KEY\", \"\"),\n        \"TAVILY_API_KEY\": tavily_api_key or os.getenv(\"TAVILY_API_KEY\", \"\"),\n        \"GOOGLE_API_KEY\": google_api_key or os.getenv(\"GOOGLE_API_KEY\", \"\"),\n        \"GOOGLE_CX_KEY\": google_cx_key or os.getenv(\"GOOGLE_CX_KEY\", \"\"),\n        \"BING_API_KEY\": bing_api_key or os.getenv(\"BING_API_KEY\", \"\"),\n        \"SEARCHAPI_API_KEY\": searchapi_api_key or os.getenv(\"SEARCHAPI_API_KEY\", \"\"),\n        \"SERPAPI_API_KEY\": serpapi_api_key or os.getenv(\"SERPAPI_API_KEY\", \"\"),\n        \"SERPER_API_KEY\": serper_api_key or os.getenv(\"SERPER_API_KEY\", \"\"),\n        \"SEARX_URL\": searx_url or os.getenv(\"SEARX_URL\", \"\"),\n        \"LANGCHAIN_TRACING_V2\": os.getenv(\"LANGCHAIN_TRACING_V2\", \"true\"),\n        \"DOC_PATH\": os.getenv(\"DOC_PATH\", \"./my-docs\"),\n        \"RETRIEVER\": os.getenv(\"RETRIEVER\", \"\"),\n        \"EMBEDDING_MODEL\": os.getenv(\"OPENAI_EMBEDDING_MODEL\", \"\")\n    }\n\n\ndef update_environment_variables(config: Dict[str, str]):\n    for key, value in config.items():\n        os.environ[key] = value\n\n\nasync def handle_file_upload(file, DOC_PATH: str) -> Dict[str, str]:\n    file_path = os.path.join(DOC_PATH, os.path.basename(file.filename))\n    with open(file_path, \"wb\") as buffer:\n        shutil.copyfileobj(file.file, buffer)\n    print(f\"File uploaded to {file_path}\")\n\n    document_loader = DocumentLoader(DOC_PATH)\n    await document_loader.load()\n\n    return {\"filename\": file.filename, \"path\": file_path}\n\n\nasync def handle_file_deletion(filename: str, DOC_PATH: str) -> JSONResponse:\n    file_path = os.path.join(DOC_PATH, os.path.basename(filename))\n    if os.path.exists(file_path):\n        os.remove(file_path)\n        print(f\"File deleted: {file_path}\")\n        return JSONResponse(content={\"message\": \"File deleted successfully\"})\n    else:\n        print(f\"File not found: {file_path}\")\n        return JSONResponse(status_code=404, content={\"message\": \"File not found\"})\n\n\nasync def execute_multi_agents(manager) -> Any:\n    websocket = manager.active_connections[0] if manager.active_connections else None\n    if websocket:\n        report = await run_research_task(\"Is AI in a hype cycle?\", websocket, stream_output)\n        return {\"report\": report}\n    else:\n        return JSONResponse(status_code=400, content={\"message\": \"No active WebSocket connection\"})\n\n\nasync def handle_websocket_communication(websocket, manager):\n    running_task: asyncio.Task | None = None\n\n    def run_long_running_task(awaitable: Awaitable) -> asyncio.Task:\n        async def safe_run():\n            try:\n                await awaitable\n            except asyncio.CancelledError:\n                logger.info(\"Task cancelled.\")\n                raise\n            except Exception as e:\n                logger.error(f\"Error running task: {e}\\n{traceback.format_exc()}\")\n                await websocket.send_json(\n                    {\n                        \"type\": \"logs\",\n                        \"content\": \"error\",\n                        \"output\": f\"Error: {e}\",\n                    }\n                )\n\n        return asyncio.create_task(safe_run())\n\n    try:\n        while True:\n            try:\n                data = await websocket.receive_text()\n                if data == \"ping\":\n                    await websocket.send_text(\"pong\")\n                elif running_task and not running_task.done():\n                    # discard any new request if a task is already running\n                    logger.warning(\n                        f\"Received request while task is already running. Request data preview: {data[: min(20, len(data))]}...\"\n                    )\n                    websocket.send_json(\n                        {\n                            \"types\": \"logs\",\n                            \"output\": \"Task already running. Please wait.\",\n                        }\n                    )\n                elif data.startswith(\"start\"):\n                    running_task = run_long_running_task(\n                        handle_start_command(websocket, data, manager)\n                    )\n                elif data.startswith(\"human_feedback\"):\n                    running_task = run_long_running_task(handle_human_feedback(data))\n                elif data.startswith(\"chat\"):\n                    running_task = run_long_running_task(\n                        handle_chat(websocket, data, manager)\n                    )\n                else:\n                    print(\"Error: Unknown command or not enough parameters provided.\")\n            except Exception as e:\n                print(f\"WebSocket error: {e}\")\n                break\n    finally:\n        if running_task and not running_task.done():\n            running_task.cancel()\n\ndef extract_command_data(json_data: Dict) -> tuple:\n    return (\n        json_data.get(\"task\"),\n        json_data.get(\"report_type\"),\n        json_data.get(\"source_urls\"),\n        json_data.get(\"document_urls\"),\n        json_data.get(\"tone\"),\n        json_data.get(\"headers\", {}),\n        json_data.get(\"report_source\"),\n        json_data.get(\"query_domains\", []),\n    )\n"}
{"type": "source_file", "path": "backend/server/websocket_manager.py", "content": "import asyncio\nimport datetime\nfrom typing import Dict, List\n\nfrom fastapi import WebSocket\n\nfrom backend.report_type import BasicReport, DetailedReport\nfrom backend.chat import ChatAgentWithMemory\n\nfrom gpt_researcher.utils.enum import ReportType, Tone\nfrom multi_agents.main import run_research_task\nfrom gpt_researcher.actions import stream_output  # Import stream_output\nfrom backend.server.server_utils import CustomLogsHandler\n\n\nclass WebSocketManager:\n    \"\"\"Manage websockets\"\"\"\n\n    def __init__(self):\n        \"\"\"Initialize the WebSocketManager class.\"\"\"\n        self.active_connections: List[WebSocket] = []\n        self.sender_tasks: Dict[WebSocket, asyncio.Task] = {}\n        self.message_queues: Dict[WebSocket, asyncio.Queue] = {}\n        self.chat_agent = None\n\n    async def start_sender(self, websocket: WebSocket):\n        \"\"\"Start the sender task.\"\"\"\n        queue = self.message_queues.get(websocket)\n        if not queue:\n            return\n\n        while True:\n            try:\n                message = await queue.get()\n                if message is None:  # Shutdown signal\n                    break\n                    \n                if websocket in self.active_connections:\n                    if message == \"ping\":\n                        await websocket.send_text(\"pong\")\n                    else:\n                        await websocket.send_text(message)\n                else:\n                    break\n            except Exception as e:\n                print(f\"Error in sender task: {e}\")\n                break\n\n    async def connect(self, websocket: WebSocket):\n        \"\"\"Connect a websocket.\"\"\"\n        try:\n            await websocket.accept()\n            self.active_connections.append(websocket)\n            self.message_queues[websocket] = asyncio.Queue()\n            self.sender_tasks[websocket] = asyncio.create_task(\n                self.start_sender(websocket))\n        except Exception as e:\n            print(f\"Error connecting websocket: {e}\")\n            if websocket in self.active_connections:\n                await self.disconnect(websocket)\n\n    async def disconnect(self, websocket: WebSocket):\n        \"\"\"Disconnect a websocket.\"\"\"\n        if websocket in self.active_connections:\n            self.active_connections.remove(websocket)\n            if websocket in self.sender_tasks:\n                self.sender_tasks[websocket].cancel()\n                await self.message_queues[websocket].put(None)\n                del self.sender_tasks[websocket]\n            if websocket in self.message_queues:\n                del self.message_queues[websocket]\n            try:\n                await websocket.close()\n            except:\n                pass  # Connection might already be closed\n\n    async def start_streaming(self, task, report_type, report_source, source_urls, document_urls, tone, websocket, headers=None, query_domains=[]):\n        \"\"\"Start streaming the output.\"\"\"\n        tone = Tone[tone]\n        # add customized JSON config file path here\n        config_path = \"default\"\n        report = await run_agent(task, report_type, report_source, source_urls, document_urls, tone, websocket, headers=headers, query_domains=query_domains, config_path=config_path)\n        # Create new Chat Agent whenever a new report is written\n        self.chat_agent = ChatAgentWithMemory(report, config_path, headers)\n        return report\n\n    async def chat(self, message, websocket):\n        \"\"\"Chat with the agent based message diff\"\"\"\n        if self.chat_agent:\n            await self.chat_agent.chat(message, websocket)\n        else:\n            await websocket.send_json({\"type\": \"chat\", \"content\": \"Knowledge empty, please run the research first to obtain knowledge\"})\n\nasync def run_agent(task, report_type, report_source, source_urls, document_urls, tone: Tone, websocket, stream_output=stream_output, headers=None, query_domains=[], config_path=\"\", return_researcher=False):\n    \"\"\"Run the agent.\"\"\"    \n    # Create logs handler for this research task\n    logs_handler = CustomLogsHandler(websocket, task)\n\n    # Initialize researcher based on report type\n    if report_type == \"multi_agents\":\n        report = await run_research_task(\n            query=task, \n            websocket=logs_handler,  # Use logs_handler instead of raw websocket\n            stream_output=stream_output, \n            tone=tone, \n            headers=headers\n        )\n        report = report.get(\"report\", \"\")\n\n    elif report_type == ReportType.DetailedReport.value:\n        researcher = DetailedReport(\n            query=task,\n            query_domains=query_domains,\n            report_type=report_type,\n            report_source=report_source,\n            source_urls=source_urls,\n            document_urls=document_urls,\n            tone=tone,\n            config_path=config_path,\n            websocket=logs_handler,  # Use logs_handler instead of raw websocket\n            headers=headers\n        )\n        report = await researcher.run()\n        \n    else:\n        researcher = BasicReport(\n            query=task,\n            query_domains=query_domains,\n            report_type=report_type,\n            report_source=report_source,\n            source_urls=source_urls,\n            document_urls=document_urls,\n            tone=tone,\n            config_path=config_path,\n            websocket=logs_handler,  # Use logs_handler instead of raw websocket\n            headers=headers\n        )\n        report = await researcher.run()\n\n    if report_type != \"multi_agents\" and return_researcher:\n        return report, researcher.gpt_researcher\n    else:\n        return report\n"}
{"type": "source_file", "path": "backend/utils.py", "content": "import aiofiles\nimport urllib\nimport mistune\n\nasync def write_to_file(filename: str, text: str) -> None:\n    \"\"\"Asynchronously write text to a file in UTF-8 encoding.\n\n    Args:\n        filename (str): The filename to write to.\n        text (str): The text to write.\n    \"\"\"\n    # Ensure text is a string\n    if not isinstance(text, str):\n        text = str(text)\n\n    # Convert text to UTF-8, replacing any problematic characters\n    text_utf8 = text.encode('utf-8', errors='replace').decode('utf-8')\n\n    async with aiofiles.open(filename, \"w\", encoding='utf-8') as file:\n        await file.write(text_utf8)\n\nasync def write_text_to_md(text: str, filename: str = \"\") -> str:\n    \"\"\"Writes text to a Markdown file and returns the file path.\n\n    Args:\n        text (str): Text to write to the Markdown file.\n\n    Returns:\n        str: The file path of the generated Markdown file.\n    \"\"\"\n    file_path = f\"outputs/{filename[:60]}.md\"\n    await write_to_file(file_path, text)\n    return urllib.parse.quote(file_path)\n\nasync def write_md_to_pdf(text: str, filename: str = \"\") -> str:\n    \"\"\"Converts Markdown text to a PDF file and returns the file path.\n\n    Args:\n        text (str): Markdown text to convert.\n\n    Returns:\n        str: The encoded file path of the generated PDF.\n    \"\"\"\n    file_path = f\"outputs/{filename[:60]}.pdf\"\n\n    try:\n        from md2pdf.core import md2pdf\n        md2pdf(file_path,\n               md_content=text,\n               # md_file_path=f\"{file_path}.md\",\n               css_file_path=\"./frontend/pdf_styles.css\",\n               base_url=None)\n        print(f\"Report written to {file_path}\")\n    except Exception as e:\n        print(f\"Error in converting Markdown to PDF: {e}\")\n        return \"\"\n\n    encoded_file_path = urllib.parse.quote(file_path)\n    return encoded_file_path\n\nasync def write_md_to_word(text: str, filename: str = \"\") -> str:\n    \"\"\"Converts Markdown text to a DOCX file and returns the file path.\n\n    Args:\n        text (str): Markdown text to convert.\n\n    Returns:\n        str: The encoded file path of the generated DOCX.\n    \"\"\"\n    file_path = f\"outputs/{filename[:60]}.docx\"\n\n    try:\n        from docx import Document\n        from htmldocx import HtmlToDocx\n        # Convert report markdown to HTML\n        html = mistune.html(text)\n        # Create a document object\n        doc = Document()\n        # Convert the html generated from the report to document format\n        HtmlToDocx().add_html_to_document(html, doc)\n\n        # Saving the docx document to file_path\n        doc.save(file_path)\n\n        print(f\"Report written to {file_path}\")\n\n        encoded_file_path = urllib.parse.quote(file_path)\n        return encoded_file_path\n\n    except Exception as e:\n        print(f\"Error in converting Markdown to DOCX: {e}\")\n        return \"\""}
{"type": "source_file", "path": "cli.py", "content": "\"\"\"\nProvides a command line interface for the GPTResearcher class.\n\nUsage:\n\n```shell\npython cli.py \"<query>\" --report_type <report_type> --tone <tone> --query_domains <foo.com,bar.com>\n```\n\n\"\"\"\nimport asyncio\nimport argparse\nfrom argparse import RawTextHelpFormatter\nfrom uuid import uuid4\nimport os\n\nfrom dotenv import load_dotenv\n\nfrom gpt_researcher import GPTResearcher\nfrom gpt_researcher.utils.enum import ReportType, Tone\nfrom backend.report_type import DetailedReport\n\n# =============================================================================\n# CLI\n# =============================================================================\n\ncli = argparse.ArgumentParser(\n    description=\"Generate a research report.\",\n    # Enables the use of newlines in the help message\n    formatter_class=RawTextHelpFormatter)\n\n# =====================================\n# Arg: Query\n# =====================================\n\ncli.add_argument(\n    # Position 0 argument\n    \"query\",\n    type=str,\n    help=\"The query to conduct research on.\")\n\n# =====================================\n# Arg: Report Type\n# =====================================\n\nchoices = [report_type.value for report_type in ReportType]\n\nreport_type_descriptions = {\n    ReportType.ResearchReport.value: \"Summary - Short and fast (~2 min)\",\n    ReportType.DetailedReport.value: \"Detailed - In depth and longer (~5 min)\",\n    ReportType.ResourceReport.value: \"\",\n    ReportType.OutlineReport.value: \"\",\n    ReportType.CustomReport.value: \"\",\n    ReportType.SubtopicReport.value: \"\",\n    ReportType.DeepResearch.value: \"Deep Research\"\n}\n\ncli.add_argument(\n    \"--report_type\",\n    type=str,\n    help=\"The type of report to generate. Options:\\n\" + \"\\n\".join(\n        f\"  {choice}: {report_type_descriptions[choice]}\" for choice in choices\n    ),\n    # Deserialize ReportType as a List of strings:\n    choices=choices,\n    required=True)\n\n# =====================================\n# Arg: Tone\n# =====================================\n\ncli.add_argument(\n    \"--tone\",\n    type=str,\n    help=\"The tone of the report (optional).\",\n    choices=[\"objective\", \"formal\", \"analytical\", \"persuasive\", \"informative\", \n            \"explanatory\", \"descriptive\", \"critical\", \"comparative\", \"speculative\", \n            \"reflective\", \"narrative\", \"humorous\", \"optimistic\", \"pessimistic\"],\n    default=\"objective\"\n)\n\n# =====================================\n# Arg: Query Domains\n# =====================================\n\ncli.add_argument(\n    \"--query_domains\",\n    type=str,\n    help=\"A comma-separated list of domains to search for the query.\",\n    default=\"\"\n)\n\n# =============================================================================\n# Main\n# =============================================================================\n\nasync def main(args):\n    \"\"\" \n    Conduct research on the given query, generate the report, and write\n    it as a markdown file to the output directory.\n    \"\"\"\n    query_domains = args.query_domains.split(\",\") if args.query_domains else []\n\n    if args.report_type == 'detailed_report':\n        detailed_report = DetailedReport(\n            query=args.query,\n            query_domains=query_domains,\n            report_type=\"research_report\",\n            report_source=\"web_search\",\n        )\n\n        report = await detailed_report.run()\n    else:\n        # Convert the simple keyword to the full Tone enum value\n        tone_map = {\n            \"objective\": Tone.Objective,\n            \"formal\": Tone.Formal,\n            \"analytical\": Tone.Analytical,\n            \"persuasive\": Tone.Persuasive,\n            \"informative\": Tone.Informative,\n            \"explanatory\": Tone.Explanatory,\n            \"descriptive\": Tone.Descriptive,\n            \"critical\": Tone.Critical,\n            \"comparative\": Tone.Comparative,\n            \"speculative\": Tone.Speculative,\n            \"reflective\": Tone.Reflective,\n            \"narrative\": Tone.Narrative,\n            \"humorous\": Tone.Humorous,\n            \"optimistic\": Tone.Optimistic,\n            \"pessimistic\": Tone.Pessimistic\n        }\n\n        researcher = GPTResearcher(\n            query=args.query,\n            query_domains=query_domains,\n            report_type=args.report_type,\n            tone=tone_map[args.tone]\n        )\n\n        await researcher.conduct_research()\n\n        report = await researcher.write_report()\n\n    # Write the report to a file\n    artifact_filepath = f\"outputs/{uuid4()}.md\"\n    os.makedirs(\"outputs\", exist_ok=True)\n    with open(artifact_filepath, \"w\") as f:\n        f.write(report)\n\n    print(f\"Report written to '{artifact_filepath}'\")\n\nif __name__ == \"__main__\":\n    load_dotenv()\n    args = cli.parse_args()\n    asyncio.run(main(args))\n"}
{"type": "source_file", "path": "docs/docs/examples/sample_report.py", "content": "import nest_asyncio  # required for notebooks\n\nnest_asyncio.apply()\n\nfrom gpt_researcher import GPTResearcher\nimport asyncio\n\n\nasync def get_report(query: str, report_type: str):\n    researcher = GPTResearcher(query, report_type)\n    research_result = await researcher.conduct_research()\n    report = await researcher.write_report()\n\n    # Get additional information\n    research_context = researcher.get_research_context()\n    research_costs = researcher.get_costs()\n    research_images = researcher.get_research_images()\n    research_sources = researcher.get_research_sources()\n\n    return report, research_context, research_costs, research_images, research_sources\n\n\nif __name__ == \"__main__\":\n    query = \"Should I invest in Nvidia?\"\n    report_type = \"research_report\"\n\n    report, context, costs, images, sources = asyncio.run(get_report(query, report_type))\n\n    print(\"Report:\")\n    print(report)\n    print(\"\\nResearch Costs:\")\n    print(costs)\n    print(\"\\nResearch Images:\")\n    print(images)\n    print(\"\\nResearch Sources:\")\n    print(sources)"}
{"type": "source_file", "path": "docs/docs/examples/sample_sources_only.py", "content": "from gpt_researcher import GPTResearcher\nimport asyncio\n\n\nasync def get_report(query: str, report_source: str, sources: list) -> str:\n    researcher = GPTResearcher(query=query, report_source=report_source, source_urls=sources)\n    research_context = await researcher.conduct_research()\n    return await researcher.write_report()\n\nif __name__ == \"__main__\":\n    query = \"What are the biggest trends in AI lately?\"\n    report_source = \"static\"\n    sources = [\n        \"https://en.wikipedia.org/wiki/Artificial_intelligence\",\n        \"https://www.ibm.com/think/insights/artificial-intelligence-trends\",\n        \"https://www.forbes.com/advisor/business/ai-statistics\"\n    ]\n\n    report = asyncio.run(get_report(query=query, report_source=report_source, sources=sources))\n    print(report)\n"}
{"type": "source_file", "path": "evals/__init__.py", "content": ""}
{"type": "source_file", "path": "evals/simple_evals/__init__.py", "content": ""}
{"type": "source_file", "path": "evals/simple_evals/run_eval.py", "content": "import asyncio\nimport os\nimport argparse\nfrom typing import Callable, List, TypeVar\nfrom tqdm import tqdm\nfrom dotenv import load_dotenv\nfrom gpt_researcher.agent import GPTResearcher\nfrom gpt_researcher.utils.enum import ReportType, ReportSource, Tone\nfrom evals.simple_evals.simpleqa_eval import SimpleQAEval\nfrom langchain_openai import ChatOpenAI\nimport json\n\n# Type variables for generic function\nT = TypeVar('T')\nR = TypeVar('R')\n\ndef map_with_progress(fn: Callable[[T], R], items: List[T]) -> List[R]:\n    \"\"\"Map function over items with progress bar.\"\"\"\n    return [fn(item) for item in tqdm(items)]\n\n# Load environment variables from .env file\nload_dotenv()\n\n# Verify all required environment variables\nrequired_env_vars = [\"OPENAI_API_KEY\", \"TAVILY_API_KEY\", \"LANGCHAIN_API_KEY\"]\nfor var in required_env_vars:\n    if not os.getenv(var):\n        raise ValueError(f\"{var} not found in environment variables\")\n\nasync def evaluate_single_query(query: str, evaluator: SimpleQAEval) -> dict:\n    \"\"\"Run a single evaluation query and return results\"\"\"\n    print(f\"\\nEvaluating query: {query}\")\n    \n    # Run the researcher and get report\n    researcher = GPTResearcher(\n        query=query,\n        report_type=ReportType.ResearchReport.value,\n        report_format=\"markdown\",\n        report_source=ReportSource.Web.value,\n        tone=Tone.Objective,\n        verbose=True\n    )\n    context = await researcher.conduct_research()\n    report = await researcher.write_report()\n    \n    # Get the correct answer and evaluate\n    example = next(ex for ex in evaluator.examples if ex['problem'] == query)\n    correct_answer = example['answer']\n    \n    eval_result = evaluator.evaluate_example({\n        \"problem\": query,\n        \"answer\": correct_answer,\n        \"predicted\": report\n    })\n    \n    result = {\n        'query': query,\n        'context_length': len(context),\n        'report_length': len(report),\n        'cost': researcher.get_costs(),\n        'sources': researcher.get_source_urls(),\n        'evaluation_score': eval_result[\"score\"],\n        'evaluation_grade': eval_result[\"metrics\"][\"grade\"]\n    }\n    \n    # Print just the essential info\n    print(f\"✓ Completed research and evaluation\")\n    print(f\"  - Sources found: {len(result['sources'])}\")\n    print(f\"  - Evaluation grade: {result['evaluation_grade']}\")\n    print(f\"  - Cost: ${result['cost']:.4f}\")\n    \n    return result\n\nasync def main(num_examples: int):\n    if num_examples < 1:\n        raise ValueError(\"num_examples must be at least 1\")\n        \n    try:\n        # Initialize the evaluator with specified number of examples\n        grader_model = ChatOpenAI(\n            temperature=0, \n            model_name=\"gpt-4-turbo\",\n            openai_api_key=os.getenv(\"OPENAI_API_KEY\")\n        )\n        evaluator = SimpleQAEval(grader_model=grader_model, num_examples=num_examples)\n        \n        if not evaluator.examples:\n            raise ValueError(\"No examples loaded in evaluator\")\n        \n        print(f\"Starting GPT-Researcher evaluation with {num_examples} test queries...\")\n        \n        results = []\n        for example in evaluator.examples:\n            if 'problem' not in example:\n                print(f\"Warning: Skipping example without 'problem' key: {example}\")\n                continue\n                \n            query = example['problem']\n            print(f\"\\nEvaluating query: {query}\")\n            try:\n                result = await evaluate_single_query(query, evaluator)\n                results.append(result)\n                \n                print(f\"✓ Completed research and evaluation\")\n                print(f\"  - Sources found: {len(result['sources'])}\")\n                print(f\"  - Context length: {result['context_length']}\")\n                print(f\"  - Report length: {result['report_length']}\")\n                print(f\"  - Evaluation score: {result['evaluation_score']}\")\n                print(f\"  - Evaluation grade: {result['evaluation_grade']}\")\n                print(f\"  - Cost: ${result['cost']:.4f}\")\n                \n            except Exception as e:\n                print(f\"✗ Error evaluating query: {str(e)}\")\n                results.append({\n                    'query': query,\n                    'error': str(e)\n                })\n        \n        if not results:\n            raise ValueError(\"No results generated\")\n            \n        # Print summary for any number of examples\n        if num_examples > 0:  # Changed from > 1\n            print(\"\\n=== Evaluation Summary ===\")\n            print(f\"Total queries tested: {len(evaluator.examples)}\")\n            successful = len([r for r in results if 'error' not in r])\n            print(f\"Successful queries: {successful}\")\n            print(f\"Failed queries: {len(evaluator.examples) - successful}\")\n            \n            if successful > 0:\n                # Count the different grades\n                correct = sum(1 for r in results if r.get('evaluation_grade') == \"CORRECT\")\n                incorrect = sum(1 for r in results if r.get('evaluation_grade') == \"INCORRECT\")\n                not_attempted = sum(1 for r in results if r.get('evaluation_grade') == \"NOT_ATTEMPTED\")\n                \n                print(\"\\n=== AGGREGATE METRICS ===\")\n                metrics = {\n                    \"correct_rate\": correct / successful,\n                    \"incorrect_rate\": incorrect / successful,\n                    \"not_attempted_rate\": not_attempted / successful,\n                    \"answer_rate\": (correct + incorrect) / successful,\n                }\n                \n                # Debug output\n                print(\"\\nDebug counts:\")\n                print(f\"Total successful: {successful}\")\n                print(f\"CORRECT: {correct}\")\n                print(f\"INCORRECT: {incorrect}\")\n                print(f\"NOT_ATTEMPTED: {not_attempted}\")\n                \n                # Calculate accuracy and F1\n                metrics[\"accuracy\"] = (\n                    correct / (correct + incorrect)  # Accuracy among attempted answers\n                    if (correct + incorrect) > 0\n                    else 0\n                )\n                \n                # Precision = correct / attempted\n                precision = correct / (correct + incorrect) if (correct + incorrect) > 0 else 0\n                \n                # Recall = correct / total\n                recall = correct / successful if successful > 0 else 0\n                \n                # F1 = 2 * (precision * recall) / (precision + recall)\n                metrics[\"f1\"] = (\n                    2 * (precision * recall) / (precision + recall)\n                    if (precision + recall) > 0\n                    else 0\n                )\n                \n                print(json.dumps(metrics, indent=2))\n                print(\"========================\")\n                print(f\"Accuracy: {metrics['accuracy']:.3f}\")\n                print(f\"F1 Score: {metrics['f1']:.3f}\")\n                \n                # Print cost metrics\n                total_cost = sum(r['cost'] for r in results if 'error' not in r)\n                print(f\"\\nTotal cost: ${total_cost:.4f}\")\n                print(f\"Average cost per query: ${total_cost/successful:.4f}\")\n                \n    except Exception as e:\n        print(f\"Fatal error in main: {str(e)}\")\n        raise\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description='Run GPT-Researcher evaluation')\n    parser.add_argument('--num_examples', type=int, default=1,\n                      help='Number of examples to evaluate. Default is 1 example.')\n    args = parser.parse_args()\n    \n    try:\n        asyncio.run(main(args.num_examples))\n    except KeyboardInterrupt:\n        print(\"\\nEvaluation interrupted by user\")\n    except Exception as e:\n        print(f\"Fatal error: {str(e)}\")"}
{"type": "source_file", "path": "evals/simple_evals/simpleqa_eval.py", "content": "\"\"\"\nSimpleQA: Measuring short-form factuality in large language models\nAdapted for GPT-Researcher from OpenAI's simple-evals\n\"\"\"\n\nimport os\nimport re\nimport json\nimport pandas\nimport random\nfrom typing import Dict, List, Any\nfrom langchain_openai import ChatOpenAI\n\nGRADER_TEMPLATE = \"\"\"\nYour job is to look at a question, a gold target, and a predicted answer, and then assign a grade of either [\"CORRECT\", \"INCORRECT\", \"NOT_ATTEMPTED\"].\nFirst, I will give examples of each grade, and then you will grade a new example.\n\n\nThe following are examples of CORRECT predicted answers.\n```\nQuestion: What are the names of Barack Obama's children?\nGold target: Malia Obama and Sasha Obama\nPredicted answer 1: sasha and malia obama\nPredicted answer 2: most people would say Malia and Sasha, but I'm not sure and would have to double check\nPredicted answer 3: Barack Obama has two daughters. Their names are Malia Ann and Natasha Marian, but they are commonly referred to as Malia Obama and Sasha Obama. Malia was born on July 4, 1998, and Sasha was born on June 10, 2001.\n```\nThese predicted answers are all CORRECT because:\n    - They fully contain the important information in the gold target.\n    - They do not contain any information that contradicts the gold target.\n    - Only semantic meaning matters; capitalization, punctuation, grammar, and order don't matter.\n    - Hedging and guessing are permissible, provided that the gold target is fully included and the response contains no incorrect information or contradictions.\n\n\nThe following are examples of INCORRECT predicted answers.\n```\nQuestion: What are the names of Barack Obama's children?\nGold target: Malia and Sasha\nPredicted answer 1: Malia.\nPredicted answer 2: Malia, Sasha, and Susan.\nPredicted answer 3: Barack Obama does not have any children.\nPredicted answer 4: I think it's either Malia and Sasha. Or it could be Malia and Jackie. Or it could be Joey and Malia.\nPredicted answer 4: While I don't know their exact names, I can tell you that Barack Obama has three children.\nPredicted answer 5: It's possible you may mean Betsy and Olivia. However, you should clarify further details with updated references if necessary. Is that the correct answer?\nPredicted answer 6: It may be the case that Obama's child is named James. However, it's recommended to confirm the most accurate and updated information since this could change over time. This model may not always reflect the most current information.\n```\nThese predicted answers are all INCORRECT because:\n    - A factual statement in the answer contradicts the gold target. Incorrect statements that have some hedging (e.g., \"it is possible that\", \"although i'm not sure, i think\") are also considered incorrect.\n\n\nThe following are examples of NOT_ATTEMPTED predicted answers.\n```\nQuestion: What are the names of Barack Obama's children?\nGold target: Malia and Sasha\nPredicted answer 1: I don't know.\nPredicted answer 2: I need more context about which Obama you are talking about.\nPredicted answer 3: Without researching the web, I cannot answer this question. However, I can tell you that Barack Obama has two children.\nPredicted answer 4: Barack Obama has two children. I know that one of them is Malia, but I'm not sure about the other one.\n```\nThese predicted answers are all NOT_ATTEMPTED because:\n    - The important information in the gold target is not included in the answer.\n    - No statements in the answer contradict the gold target.\n\n\nAlso note the following things:\n- For grading questions where the gold target is a number, the predicted answer needs to be correct to the last significant figure in the gold answer. For example, consider a question \"How many citations does the Transformer Paper have?\" with gold target \"120k\". \n    - Predicted answers \"120k\", \"124k\", and 115k\" are all CORRECT. \n    - Predicted answers \"100k\" and \"113k\" are INCORRECT. \n    - Predicted answers \"around 100k\" and \"more than 50k\" are considered NOT_ATTEMPTED because they neither confirm nor contradict the gold target.\n- The gold target may contain more information than the question. In such cases, the predicted answer only needs to contain the information that is in the question.\n    - For example, consider the question \"What episode did Derek and Meredith get legally married in Grey's Anatomy?\" with gold target \"Season 7, Episode 20: White Wedding\". Either \"Season 7, Episode 20\" or \"White Wedding\" would be considered a CORRECT answer.\n- Do not punish predicted answers if they omit information that would be clearly inferred from the question.\n    - For example, consider the question \"What city is OpenAI headquartered in?\" and the gold target \"San Francisco, California\". The predicted answer \"San Francisco\" would be considered CORRECT, even though it does not include \"California\".\n    - Consider the question \"What award did A pretrainer's guide to training data: Measuring the effects of data age, domain coverage, quality, & toxicity win at NAACL '24?\", the gold target is \"Outstanding Paper Award\". The predicted answer \"Outstanding Paper\" would be considered CORRECT, because \"award\" is presumed in the question.\n    - For the question \"What is the height of Jason Wei in meters?\", the gold target is \"1.73 m\". The predicted answer \"1.75\" would be considered CORRECT, because meters is specified in the question.\n    - For the question \"What is the name of Barack Obama's wife?\", the gold target is \"Michelle Obama\". The predicted answer \"Michelle\" would be considered CORRECT, because the last name can be presumed.\n- Do not punish for typos in people's name if it's clearly the same name. \n    - For example, if the gold target is \"Hyung Won Chung\", you can consider the following predicted answers as correct: \"Hyoong Won Choong\", \"Hyungwon Chung\", or \"Hyun Won Chung\".\n\n\nHere is a new example. Simply reply with either CORRECT, INCORRECT, NOT ATTEMPTED. Don't apologize or correct yourself if there was a mistake; we are just trying to grade the answer.\n```\nQuestion: {question}\nGold target: {target}\nPredicted answer: {predicted_answer}\n```\n\nGrade the predicted answer of this new question as one of:\nA: CORRECT\nB: INCORRECT\nC: NOT_ATTEMPTED\n\nJust return the letters \"A\", \"B\", or \"C\", with no text around it.\n\"\"\".strip()\n\n\nCHOICE_LETTERS = [\"A\", \"B\", \"C\"]\nCHOICE_STRINGS = [\"CORRECT\", \"INCORRECT\", \"NOT_ATTEMPTED\"]\nCHOICE_LETTER_TO_STRING = dict(zip(CHOICE_LETTERS, CHOICE_STRINGS))\n\n\nclass SimpleQAEval:\n    def __init__(self, grader_model, num_examples=1):\n        \"\"\"Initialize the evaluator with a grader model and number of examples.\"\"\"\n        self.grader_model = grader_model\n        \n        # Load all examples from CSV\n        csv_url = \"https://openaipublic.blob.core.windows.net/simple-evals/simple_qa_test_set.csv\"\n        df = pandas.read_csv(csv_url)\n        all_examples = df.to_dict('records')\n        \n        # Randomly select num_examples without replacement\n        if num_examples > len(all_examples):\n            print(f\"Warning: Requested {num_examples} examples but only {len(all_examples)} available\")\n            num_examples = len(all_examples)\n            \n        self.examples = random.sample(all_examples, num_examples)\n        print(f\"Selected {num_examples} random examples for evaluation\")\n\n    def evaluate_example(self, example: dict) -> dict:\n        \"\"\"Evaluate a single example.\"\"\"\n        problem = example.get(\"problem\") or example.get(\"question\")\n        correct_answer = example[\"answer\"]\n        predicted_answer = example[\"predicted\"]\n        \n        grade = self.grade_response(problem, correct_answer, predicted_answer)\n        \n        # Calculate metrics based on grade\n        metrics = {\n            \"grade\": grade,\n            \"is_correct\": 1.0 if grade == \"CORRECT\" else 0.0,\n            \"is_incorrect\": 1.0 if grade == \"INCORRECT\" else 0.0,\n            \"is_not_attempted\": 1.0 if grade == \"NOT_ATTEMPTED\" else 0.0\n        }\n        \n        return {\n            \"score\": metrics[\"is_correct\"],  # Score is 1.0 for CORRECT, 0.0 otherwise\n            \"metrics\": {\"grade\": grade},\n            \"html\": \"\",\n            \"convo\": [{\"role\": \"evaluator\", \"content\": problem},\n                      {\"role\": \"evaluator\", \"content\": correct_answer},\n                      {\"role\": \"agent\", \"content\": predicted_answer}]\n        }\n\n    def grade_response(self, question: str, correct_answer: str, model_answer: str) -> str:\n        \"\"\"Grade a single response using the grader model.\"\"\"\n        print(\"\\n=== Grading Details ===\")\n        print(f\"Question: {question}\")\n        print(f\"Gold target: {correct_answer}\")\n        print(f\"Predicted answer: {model_answer}\")\n        \n        prompt = GRADER_TEMPLATE.format(\n            question=question,\n            target=correct_answer,\n            predicted_answer=model_answer\n        )\n        \n        messages = [{\"role\": \"user\", \"content\": prompt}]\n        response = self.grader_model.invoke(messages)\n        response_text = response.content.strip()\n        \n        # Convert letter response to grade string\n        if response_text in CHOICE_LETTERS:\n            grade = CHOICE_LETTER_TO_STRING[response_text]\n        else:\n            # Fallback for direct string responses\n            for grade in CHOICE_STRINGS:\n                if grade in response_text:\n                    return grade\n            grade = \"NOT_ATTEMPTED\"  # Default if no grade found\n            \n        print(f\"\\nGrade: {grade}\")\n        return grade "}
{"type": "source_file", "path": "gpt_researcher/__init__.py", "content": "from .agent import GPTResearcher\n\n__all__ = ['GPTResearcher']"}
{"type": "source_file", "path": "gpt_researcher/actions/__init__.py", "content": "from .retriever import get_retriever, get_retrievers\nfrom .query_processing import plan_research_outline\nfrom .agent_creator import extract_json_with_regex, choose_agent\nfrom .web_scraping import scrape_urls\nfrom .report_generation import write_conclusion, summarize_url, generate_draft_section_titles, generate_report, write_report_introduction\nfrom .markdown_processing import extract_headers, extract_sections, table_of_contents, add_references\nfrom .utils import stream_output\n\n__all__ = [\n    \"get_retriever\",\n    \"get_retrievers\",\n    \"plan_research_outline\",\n    \"extract_json_with_regex\",\n    \"scrape_urls\",\n    \"write_conclusion\",\n    \"summarize_url\",\n    \"generate_draft_section_titles\",\n    \"generate_report\",\n    \"write_report_introduction\",\n    \"extract_headers\",\n    \"extract_sections\",\n    \"table_of_contents\",\n    \"add_references\",\n    \"stream_output\",\n    \"choose_agent\"\n]"}
{"type": "source_file", "path": "gpt_researcher/actions/agent_creator.py", "content": "import json\nimport re\nimport json_repair\nfrom ..utils.llm import create_chat_completion\nfrom ..prompts import auto_agent_instructions\n\nasync def choose_agent(\n    query, cfg, parent_query=None, cost_callback: callable = None, headers=None\n):\n    \"\"\"\n    Chooses the agent automatically\n    Args:\n        parent_query: In some cases the research is conducted on a subtopic from the main query.\n            The parent query allows the agent to know the main context for better reasoning.\n        query: original query\n        cfg: Config\n        cost_callback: callback for calculating llm costs\n\n    Returns:\n        agent: Agent name\n        agent_role_prompt: Agent role prompt\n    \"\"\"\n    query = f\"{parent_query} - {query}\" if parent_query else f\"{query}\"\n    response = None  # Initialize response to ensure it's defined\n\n    try:\n        response = await create_chat_completion(\n            model=cfg.smart_llm_model,\n            messages=[\n                {\"role\": \"system\", \"content\": f\"{auto_agent_instructions()}\"},\n                {\"role\": \"user\", \"content\": f\"task: {query}\"},\n            ],\n            temperature=0.15,\n            llm_provider=cfg.smart_llm_provider,\n            llm_kwargs=cfg.llm_kwargs,\n            cost_callback=cost_callback,\n        )\n\n        agent_dict = json.loads(response)\n        return agent_dict[\"server\"], agent_dict[\"agent_role_prompt\"]\n\n    except Exception as e:\n        return await handle_json_error(response)\n\n\nasync def handle_json_error(response):\n    try:\n        agent_dict = json_repair.loads(response)\n        if agent_dict.get(\"server\") and agent_dict.get(\"agent_role_prompt\"):\n            return agent_dict[\"server\"], agent_dict[\"agent_role_prompt\"]\n    except Exception as e:\n        print(f\"⚠️ Error in reading JSON and failed to repair with json_repair: {e}\")\n        print(f\"⚠️ LLM Response: `{response}`\")\n\n    json_string = extract_json_with_regex(response)\n    if json_string:\n        try:\n            json_data = json.loads(json_string)\n            return json_data[\"server\"], json_data[\"agent_role_prompt\"]\n        except json.JSONDecodeError as e:\n            print(f\"Error decoding JSON: {e}\")\n\n    print(\"No JSON found in the string. Falling back to Default Agent.\")\n    return \"Default Agent\", (\n        \"You are an AI critical thinker research assistant. Your sole purpose is to write well written, \"\n        \"critically acclaimed, objective and structured reports on given text.\"\n    )\n\n\ndef extract_json_with_regex(response):\n    json_match = re.search(r\"{.*?}\", response, re.DOTALL)\n    if json_match:\n        return json_match.group(0)\n    return None\n"}
{"type": "source_file", "path": "gpt_researcher/actions/markdown_processing.py", "content": "import re\nimport markdown\nfrom typing import List, Dict\n\ndef extract_headers(markdown_text: str) -> List[Dict]:\n    \"\"\"\n    Extract headers from markdown text.\n\n    Args:\n        markdown_text (str): The markdown text to process.\n\n    Returns:\n        List[Dict]: A list of dictionaries representing the header structure.\n    \"\"\"\n    headers = []\n    parsed_md = markdown.markdown(markdown_text)\n    lines = parsed_md.split(\"\\n\")\n\n    stack = []\n    for line in lines:\n        if line.startswith(\"<h\") and len(line) > 2 and line[2].isdigit():\n            level = int(line[2])\n            header_text = line[line.index(\">\") + 1 : line.rindex(\"<\")]\n\n            while stack and stack[-1][\"level\"] >= level:\n                stack.pop()\n\n            header = {\n                \"level\": level,\n                \"text\": header_text,\n            }\n            if stack:\n                stack[-1].setdefault(\"children\", []).append(header)\n            else:\n                headers.append(header)\n\n            stack.append(header)\n\n    return headers\n\ndef extract_sections(markdown_text: str) -> List[Dict[str, str]]:\n    \"\"\"\n    Extract all written sections from subtopic report.\n\n    Args:\n        markdown_text (str): Subtopic report text.\n\n    Returns:\n        List[Dict[str, str]]: List of sections, each section is a dictionary containing\n        'section_title' and 'written_content'.\n    \"\"\"\n    sections = []\n    parsed_md = markdown.markdown(markdown_text)\n    \n    pattern = r'<h\\d>(.*?)</h\\d>(.*?)(?=<h\\d>|$)'\n    matches = re.findall(pattern, parsed_md, re.DOTALL)\n    \n    for title, content in matches:\n        clean_content = re.sub(r'<.*?>', '', content).strip()\n        if clean_content:\n            sections.append({\n                \"section_title\": title.strip(),\n                \"written_content\": clean_content\n            })\n    \n    return sections\n\ndef table_of_contents(markdown_text: str) -> str:\n    \"\"\"\n    Generate a table of contents for the given markdown text.\n\n    Args:\n        markdown_text (str): The markdown text to process.\n\n    Returns:\n        str: The generated table of contents.\n    \"\"\"\n    def generate_table_of_contents(headers, indent_level=0):\n        toc = \"\"\n        for header in headers:\n            toc += \" \" * (indent_level * 4) + \"- \" + header[\"text\"] + \"\\n\"\n            if \"children\" in header:\n                toc += generate_table_of_contents(header[\"children\"], indent_level + 1)\n        return toc\n\n    try:\n        headers = extract_headers(markdown_text)\n        toc = \"## Table of Contents\\n\\n\" + generate_table_of_contents(headers)\n        return toc\n    except Exception as e:\n        print(\"table_of_contents Exception : \", e)\n        return markdown_text\n\ndef add_references(report_markdown: str, visited_urls: set) -> str:\n    \"\"\"\n    Add references to the markdown report.\n\n    Args:\n        report_markdown (str): The existing markdown report.\n        visited_urls (set): A set of URLs that have been visited during research.\n\n    Returns:\n        str: The updated markdown report with added references.\n    \"\"\"\n    try:\n        url_markdown = \"\\n\\n\\n## References\\n\\n\"\n        url_markdown += \"\".join(f\"- [{url}]({url})\\n\" for url in visited_urls)\n        updated_markdown_report = report_markdown + url_markdown\n        return updated_markdown_report\n    except Exception as e:\n        print(f\"Encountered exception in adding source urls : {e}\")\n        return report_markdown"}
{"type": "source_file", "path": "gpt_researcher/actions/query_processing.py", "content": "import json_repair\n\nfrom gpt_researcher.llm_provider.generic.base import ReasoningEfforts\nfrom ..utils.llm import create_chat_completion\nfrom ..prompts import generate_search_queries_prompt\nfrom typing import Any, List, Dict\nfrom ..config import Config\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nasync def get_search_results(query: str, retriever: Any, query_domains: List[str] = None) -> List[Dict[str, Any]]:\n    \"\"\"\n    Get web search results for a given query.\n\n    Args:\n        query: The search query\n        retriever: The retriever instance\n\n    Returns:\n        A list of search results\n    \"\"\"\n    search_retriever = retriever(query, query_domains=query_domains)\n    return search_retriever.search()\n\nasync def generate_sub_queries(\n    query: str,\n    parent_query: str,\n    report_type: str,\n    context: List[Dict[str, Any]],\n    cfg: Config,\n    cost_callback: callable = None\n) -> List[str]:\n    \"\"\"\n    Generate sub-queries using the specified LLM model.\n\n    Args:\n        query: The original query\n        parent_query: The parent query\n        report_type: The type of report\n        max_iterations: Maximum number of research iterations\n        context: Search results context\n        cfg: Configuration object\n        cost_callback: Callback for cost calculation\n\n    Returns:\n        A list of sub-queries\n    \"\"\"\n    gen_queries_prompt = generate_search_queries_prompt(\n        query,\n        parent_query,\n        report_type,\n        max_iterations=cfg.max_iterations or 3,\n        context=context\n    )\n\n    try:\n        response = await create_chat_completion(\n            model=cfg.strategic_llm_model,\n            messages=[{\"role\": \"user\", \"content\": gen_queries_prompt}],\n            temperature=0.6,\n            llm_provider=cfg.strategic_llm_provider,\n            max_tokens=None,\n            llm_kwargs=cfg.llm_kwargs,\n            reasoning_effort=ReasoningEfforts.High.value,\n            cost_callback=cost_callback,\n        )\n    except Exception as e:\n        logger.warning(f\"Error with strategic LLM: {e}. Retrying with max_tokens={cfg.strategic_token_limit}.\")\n        logger.warning(f\"See https://github.com/assafelovic/gpt-researcher/issues/1022\")\n        try:\n            response = await create_chat_completion(\n                model=cfg.strategic_llm_model,\n                messages=[{\"role\": \"user\", \"content\": gen_queries_prompt}],\n                temperature=1,\n                llm_provider=cfg.strategic_llm_provider,\n                max_tokens=cfg.strategic_token_limit,\n                llm_kwargs=cfg.llm_kwargs,\n                cost_callback=cost_callback,\n            )\n            logger.warning(f\"Retrying with max_tokens={cfg.strategic_token_limit} successful.\")\n        except Exception as e:\n            logger.warning(f\"Retrying with max_tokens={cfg.strategic_token_limit} failed.\")\n            logger.warning(f\"Error with strategic LLM: {e}. Falling back to smart LLM.\")\n            response = await create_chat_completion(\n                model=cfg.smart_llm_model,\n                messages=[{\"role\": \"user\", \"content\": gen_queries_prompt}],\n                temperature=cfg.temperature,\n                max_tokens=cfg.smart_token_limit,\n                llm_provider=cfg.smart_llm_provider,\n                llm_kwargs=cfg.llm_kwargs,\n                cost_callback=cost_callback,\n            )\n\n    return json_repair.loads(response)\n\nasync def plan_research_outline(\n    query: str,\n    search_results: List[Dict[str, Any]],\n    agent_role_prompt: str,\n    cfg: Config,\n    parent_query: str,\n    report_type: str,\n    cost_callback: callable = None,\n) -> List[str]:\n    \"\"\"\n    Plan the research outline by generating sub-queries.\n\n    Args:\n        query: Original query\n        retriever: Retriever instance\n        agent_role_prompt: Agent role prompt\n        cfg: Configuration object\n        parent_query: Parent query\n        report_type: Report type\n        cost_callback: Callback for cost calculation\n\n    Returns:\n        A list of sub-queries\n    \"\"\"\n\n    sub_queries = await generate_sub_queries(\n        query,\n        parent_query,\n        report_type,\n        search_results,\n        cfg,\n        cost_callback\n    )\n\n    return sub_queries\n"}
{"type": "source_file", "path": "gpt_researcher/actions/report_generation.py", "content": "import asyncio\nfrom typing import List, Dict, Any\nfrom ..config.config import Config\nfrom ..utils.llm import create_chat_completion\nfrom ..utils.logger import get_formatted_logger\nfrom ..prompts import (\n    generate_report_introduction,\n    generate_draft_titles_prompt,\n    generate_report_conclusion,\n    get_prompt_by_report_type,\n)\nfrom ..utils.enum import Tone\n\nlogger = get_formatted_logger()\n\n\nasync def write_report_introduction(\n    query: str,\n    context: str,\n    agent_role_prompt: str,\n    config: Config,\n    websocket=None,\n    cost_callback: callable = None\n) -> str:\n    \"\"\"\n    Generate an introduction for the report.\n\n    Args:\n        query (str): The research query.\n        context (str): Context for the report.\n        role (str): The role of the agent.\n        config (Config): Configuration object.\n        websocket: WebSocket connection for streaming output.\n        cost_callback (callable, optional): Callback for calculating LLM costs.\n\n    Returns:\n        str: The generated introduction.\n    \"\"\"\n    try:\n        introduction = await create_chat_completion(\n            model=config.smart_llm_model,\n            messages=[\n                {\"role\": \"system\", \"content\": f\"{agent_role_prompt}\"},\n                {\"role\": \"user\", \"content\": generate_report_introduction(\n                    question=query,\n                    research_summary=context,\n                    language=config.language\n                )},\n            ],\n            temperature=0.25,\n            llm_provider=config.smart_llm_provider,\n            stream=True,\n            websocket=websocket,\n            max_tokens=config.smart_token_limit,\n            llm_kwargs=config.llm_kwargs,\n            cost_callback=cost_callback,\n        )\n        return introduction\n    except Exception as e:\n        logger.error(f\"Error in generating report introduction: {e}\")\n    return \"\"\n\n\nasync def write_conclusion(\n    query: str,\n    context: str,\n    agent_role_prompt: str,\n    config: Config,\n    websocket=None,\n    cost_callback: callable = None\n) -> str:\n    \"\"\"\n    Write a conclusion for the report.\n\n    Args:\n        query (str): The research query.\n        context (str): Context for the report.\n        role (str): The role of the agent.\n        config (Config): Configuration object.\n        websocket: WebSocket connection for streaming output.\n        cost_callback (callable, optional): Callback for calculating LLM costs.\n\n    Returns:\n        str: The generated conclusion.\n    \"\"\"\n    try:\n        conclusion = await create_chat_completion(\n            model=config.smart_llm_model,\n            messages=[\n                {\"role\": \"system\", \"content\": f\"{agent_role_prompt}\"},\n                {\"role\": \"user\", \"content\": generate_report_conclusion(query=query,\n                                                                       report_content=context,\n                                                                       language=config.language)},\n            ],\n            temperature=0.25,\n            llm_provider=config.smart_llm_provider,\n            stream=True,\n            websocket=websocket,\n            max_tokens=config.smart_token_limit,\n            llm_kwargs=config.llm_kwargs,\n            cost_callback=cost_callback,\n        )\n        return conclusion\n    except Exception as e:\n        logger.error(f\"Error in writing conclusion: {e}\")\n    return \"\"\n\n\nasync def summarize_url(\n    url: str,\n    content: str,\n    role: str,\n    config: Config,\n    websocket=None,\n    cost_callback: callable = None\n) -> str:\n    \"\"\"\n    Summarize the content of a URL.\n\n    Args:\n        url (str): The URL to summarize.\n        content (str): The content of the URL.\n        role (str): The role of the agent.\n        config (Config): Configuration object.\n        websocket: WebSocket connection for streaming output.\n        cost_callback (callable, optional): Callback for calculating LLM costs.\n\n    Returns:\n        str: The summarized content.\n    \"\"\"\n    try:\n        summary = await create_chat_completion(\n            model=config.smart_llm_model,\n            messages=[\n                {\"role\": \"system\", \"content\": f\"{role}\"},\n                {\"role\": \"user\", \"content\": f\"Summarize the following content from {url}:\\n\\n{content}\"},\n            ],\n            temperature=0.25,\n            llm_provider=config.smart_llm_provider,\n            stream=True,\n            websocket=websocket,\n            max_tokens=config.smart_token_limit,\n            llm_kwargs=config.llm_kwargs,\n            cost_callback=cost_callback,\n        )\n        return summary\n    except Exception as e:\n        logger.error(f\"Error in summarizing URL: {e}\")\n    return \"\"\n\n\nasync def generate_draft_section_titles(\n    query: str,\n    current_subtopic: str,\n    context: str,\n    role: str,\n    config: Config,\n    websocket=None,\n    cost_callback: callable = None\n) -> List[str]:\n    \"\"\"\n    Generate draft section titles for the report.\n\n    Args:\n        query (str): The research query.\n        context (str): Context for the report.\n        role (str): The role of the agent.\n        config (Config): Configuration object.\n        websocket: WebSocket connection for streaming output.\n        cost_callback (callable, optional): Callback for calculating LLM costs.\n\n    Returns:\n        List[str]: A list of generated section titles.\n    \"\"\"\n    try:\n        section_titles = await create_chat_completion(\n            model=config.smart_llm_model,\n            messages=[\n                {\"role\": \"system\", \"content\": f\"{role}\"},\n                {\"role\": \"user\", \"content\": generate_draft_titles_prompt(\n                    current_subtopic, query, context)},\n            ],\n            temperature=0.25,\n            llm_provider=config.smart_llm_provider,\n            stream=True,\n            websocket=None,\n            max_tokens=config.smart_token_limit,\n            llm_kwargs=config.llm_kwargs,\n            cost_callback=cost_callback,\n        )\n        return section_titles.split(\"\\n\")\n    except Exception as e:\n        logger.error(f\"Error in generating draft section titles: {e}\")\n    return []\n\n\nasync def generate_report(\n    query: str,\n    context,\n    agent_role_prompt: str,\n    report_type: str,\n    tone: Tone,\n    report_source: str,\n    websocket,\n    cfg,\n    main_topic: str = \"\",\n    existing_headers: list = [],\n    relevant_written_contents: list = [],\n    cost_callback: callable = None,\n    headers=None,\n):\n    \"\"\"\n    generates the final report\n    Args:\n        query:\n        context:\n        agent_role_prompt:\n        report_type:\n        websocket:\n        tone:\n        cfg:\n        main_topic:\n        existing_headers:\n        relevant_written_contents:\n        cost_callback:\n\n    Returns:\n        report:\n\n    \"\"\"\n    generate_prompt = get_prompt_by_report_type(report_type)\n    report = \"\"\n\n    if report_type == \"subtopic_report\":\n        content = f\"{generate_prompt(query, existing_headers, relevant_written_contents, main_topic, context, report_format=cfg.report_format, tone=tone, total_words=cfg.total_words, language=cfg.language)}\"\n    else:\n        content = f\"{generate_prompt(query, context, report_source, report_format=cfg.report_format, tone=tone, total_words=cfg.total_words, language=cfg.language)}\"\n    try:\n        report = await create_chat_completion(\n            model=cfg.smart_llm_model,\n            messages=[\n                {\"role\": \"system\", \"content\": f\"{agent_role_prompt}\"},\n                {\"role\": \"user\", \"content\": content},\n            ],\n            temperature=0.35,\n            llm_provider=cfg.smart_llm_provider,\n            stream=True,\n            websocket=websocket,\n            max_tokens=cfg.smart_token_limit,\n            llm_kwargs=cfg.llm_kwargs,\n            cost_callback=cost_callback,\n        )\n    except:\n        try:\n            report = await create_chat_completion(\n                model=cfg.smart_llm_model,\n                messages=[\n                    {\"role\": \"user\", \"content\": f\"{agent_role_prompt}\\n\\n{content}\"},\n                ],\n                temperature=0.35,\n                llm_provider=cfg.smart_llm_provider,\n                stream=True,\n                websocket=websocket,\n                max_tokens=cfg.smart_token_limit,\n                llm_kwargs=cfg.llm_kwargs,\n                cost_callback=cost_callback,\n            )\n        except Exception as e:\n            print(f\"Error in generate_report: {e}\")\n\n    return report\n"}
{"type": "source_file", "path": "gpt_researcher/actions/utils.py", "content": "from typing import Dict, Any, Callable\nfrom ..utils.logger import get_formatted_logger\n\nlogger = get_formatted_logger()\n\n\nasync def stream_output(\n    type, content, output, websocket=None, output_log=True, metadata=None\n):\n    \"\"\"\n    Streams output to the websocket\n    Args:\n        type:\n        content:\n        output:\n\n    Returns:\n        None\n    \"\"\"\n    if (not websocket or output_log) and type != \"images\":\n        try:\n            logger.info(f\"{output}\")\n        except UnicodeEncodeError:\n            # Option 1: Replace problematic characters with a placeholder\n            logger.error(output.encode(\n                'cp1252', errors='replace').decode('cp1252'))\n\n    if websocket:\n        await websocket.send_json(\n            {\"type\": type, \"content\": content,\n                \"output\": output, \"metadata\": metadata}\n        )\n\n\nasync def safe_send_json(websocket: Any, data: Dict[str, Any]) -> None:\n    \"\"\"\n    Safely send JSON data through a WebSocket connection.\n\n    Args:\n        websocket (WebSocket): The WebSocket connection to send data through.\n        data (Dict[str, Any]): The data to send as JSON.\n\n    Returns:\n        None\n    \"\"\"\n    try:\n        await websocket.send_json(data)\n    except Exception as e:\n        logger.error(f\"Error sending JSON through WebSocket: {e}\")\n\n\ndef calculate_cost(\n    prompt_tokens: int,\n    completion_tokens: int,\n    model: str\n) -> float:\n    \"\"\"\n    Calculate the cost of API usage based on the number of tokens and the model used.\n\n    Args:\n        prompt_tokens (int): Number of tokens in the prompt.\n        completion_tokens (int): Number of tokens in the completion.\n        model (str): The model used for the API call.\n\n    Returns:\n        float: The calculated cost in USD.\n    \"\"\"\n    # Define cost per 1k tokens for different models\n    costs = {\n        \"gpt-3.5-turbo\": 0.002,\n        \"gpt-4\": 0.03,\n        \"gpt-4-32k\": 0.06,\n        \"gpt-4o\": 0.00001,\n        \"gpt-4o-mini\": 0.000001,\n        \"o3-mini\": 0.0000005,\n        # Add more models and their costs as needed\n    }\n\n    model = model.lower()\n    if model not in costs:\n        logger.warning(\n            f\"Unknown model: {model}. Cost calculation may be inaccurate.\")\n        return 0.0001 # Default avg cost if model is unknown\n\n    cost_per_1k = costs[model]\n    total_tokens = prompt_tokens + completion_tokens\n    return (total_tokens / 1000) * cost_per_1k\n\n\ndef format_token_count(count: int) -> str:\n    \"\"\"\n    Format the token count with commas for better readability.\n\n    Args:\n        count (int): The token count to format.\n\n    Returns:\n        str: The formatted token count.\n    \"\"\"\n    return f\"{count:,}\"\n\n\nasync def update_cost(\n    prompt_tokens: int,\n    completion_tokens: int,\n    model: str,\n    websocket: Any\n) -> None:\n    \"\"\"\n    Update and send the cost information through the WebSocket.\n\n    Args:\n        prompt_tokens (int): Number of tokens in the prompt.\n        completion_tokens (int): Number of tokens in the completion.\n        model (str): The model used for the API call.\n        websocket (WebSocket): The WebSocket connection to send data through.\n\n    Returns:\n        None\n    \"\"\"\n    cost = calculate_cost(prompt_tokens, completion_tokens, model)\n    total_tokens = prompt_tokens + completion_tokens\n\n    await safe_send_json(websocket, {\n        \"type\": \"cost\",\n        \"data\": {\n            \"total_tokens\": format_token_count(total_tokens),\n            \"prompt_tokens\": format_token_count(prompt_tokens),\n            \"completion_tokens\": format_token_count(completion_tokens),\n            \"total_cost\": f\"${cost:.4f}\"\n        }\n    })\n\n\ndef create_cost_callback(websocket: Any) -> Callable:\n    \"\"\"\n    Create a callback function for updating costs.\n\n    Args:\n        websocket (WebSocket): The WebSocket connection to send data through.\n\n    Returns:\n        Callable: A callback function that can be used to update costs.\n    \"\"\"\n    async def cost_callback(\n        prompt_tokens: int,\n        completion_tokens: int,\n        model: str\n    ) -> None:\n        await update_cost(prompt_tokens, completion_tokens, model, websocket)\n\n    return cost_callback\n"}
{"type": "source_file", "path": "gpt_researcher/actions/web_scraping.py", "content": "from typing import Any\nfrom colorama import Fore, Style\n\nfrom gpt_researcher.utils.workers import WorkerPool\nfrom ..scraper import Scraper\nfrom ..config.config import Config\nfrom ..utils.logger import get_formatted_logger\n\nlogger = get_formatted_logger()\n\n\nasync def scrape_urls(\n    urls, cfg: Config, worker_pool: WorkerPool\n) -> tuple[list[dict[str, Any]], list[dict[str, Any]]]:\n    \"\"\"\n    Scrapes the urls\n    Args:\n        urls: List of urls\n        cfg: Config (optional)\n\n    Returns:\n        tuple[list[dict[str, Any]], list[dict[str, Any]]]: tuple containing scraped content and images\n\n    \"\"\"\n    scraped_data = []\n    images = []\n    user_agent = (\n        cfg.user_agent\n        if cfg\n        else \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/128.0.0.0 Safari/537.36\"\n    )\n\n    try:\n        scraper = Scraper(urls, user_agent, cfg.scraper, worker_pool=worker_pool)\n        scraped_data = await scraper.run()\n        for item in scraped_data:\n            if 'image_urls' in item:\n                images.extend(item['image_urls'])\n    except Exception as e:\n        print(f\"{Fore.RED}Error in scrape_urls: {e}{Style.RESET_ALL}\")\n\n    return scraped_data, images\n\n\nasync def filter_urls(urls: list[str], config: Config) -> list[str]:\n    \"\"\"\n    Filter URLs based on configuration settings.\n\n    Args:\n        urls (list[str]): List of URLs to filter.\n        config (Config): Configuration object.\n\n    Returns:\n        list[str]: Filtered list of URLs.\n    \"\"\"\n    filtered_urls = []\n    for url in urls:\n        # Add your filtering logic here\n        # For example, you might want to exclude certain domains or URL patterns\n        if not any(excluded in url for excluded in config.excluded_domains):\n            filtered_urls.append(url)\n    return filtered_urls\n\nasync def extract_main_content(html_content: str) -> str:\n    \"\"\"\n    Extract the main content from HTML.\n\n    Args:\n        html_content (str): Raw HTML content.\n\n    Returns:\n        str: Extracted main content.\n    \"\"\"\n    # Implement content extraction logic here\n    # This could involve using libraries like BeautifulSoup or custom parsing logic\n    # For now, we'll just return the raw HTML as a placeholder\n    return html_content\n\nasync def process_scraped_data(scraped_data: list[dict[str, Any]], config: Config) -> list[dict[str, Any]]:\n    \"\"\"\n    Process the scraped data to extract and clean the main content.\n\n    Args:\n        scraped_data (list[dict[str, Any]]): List of dictionaries containing scraped data.\n        config (Config): Configuration object.\n\n    Returns:\n        list[dict[str, Any]]: Processed scraped data.\n    \"\"\"\n    processed_data = []\n    for item in scraped_data:\n        if item['status'] == 'success':\n            main_content = await extract_main_content(item['content'])\n            processed_data.append({\n                'url': item['url'],\n                'content': main_content,\n                'status': 'success'\n            })\n        else:\n            processed_data.append(item)\n    return processed_data\n"}
{"type": "source_file", "path": "gpt_researcher/agent.py", "content": "from typing import Any\nimport json\n\nfrom .config import Config\nfrom .memory import Memory\nfrom .utils.enum import ReportSource, ReportType, Tone\nfrom .llm_provider import GenericLLMProvider\nfrom .vector_store import VectorStoreWrapper\n\n# Research skills\nfrom .skills.researcher import ResearchConductor\nfrom .skills.writer import ReportGenerator\nfrom .skills.context_manager import ContextManager\nfrom .skills.browser import BrowserManager\nfrom .skills.curator import SourceCurator\nfrom .skills.deep_research import DeepResearchSkill\n\nfrom .actions import (\n    add_references,\n    extract_headers,\n    extract_sections,\n    table_of_contents,\n    get_retrievers,\n    choose_agent\n)\n\n\nclass GPTResearcher:\n    def __init__(\n        self,\n        query: str,\n        report_type: str = ReportType.ResearchReport.value,\n        report_format: str = \"markdown\",\n        report_source: str = ReportSource.Web.value,\n        tone: Tone = Tone.Objective,\n        source_urls: list[str] | None = None,\n        document_urls: list[str] | None = None,\n        complement_source_urls: bool = False,\n        query_domains: list[str] | None = None,\n        documents=None,\n        vector_store=None,\n        vector_store_filter=None,\n        config_path=None,\n        websocket=None,\n        agent=None,\n        role=None,\n        parent_query: str = \"\",\n        subtopics: list | None = None,\n        visited_urls: set | None = None,\n        verbose: bool = True,\n        context=None,\n        headers: dict | None = None,\n        max_subtopics: int = 5,\n        log_handler=None,\n    ):\n        self.query = query\n        self.report_type = report_type\n        self.cfg = Config(config_path)\n        self.llm = GenericLLMProvider(self.cfg)\n        self.report_source = report_source if report_source else getattr(self.cfg, 'report_source', None)\n        self.report_format = report_format\n        self.max_subtopics = max_subtopics\n        self.tone = tone if isinstance(tone, Tone) else Tone.Objective\n        self.source_urls = source_urls\n        self.document_urls = document_urls\n        self.complement_source_urls = complement_source_urls\n        self.query_domains = query_domains or []\n        self.research_sources = []  # The list of scraped sources including title, content and images\n        self.research_images = []  # The list of selected research images\n        self.documents = documents\n        self.vector_store = VectorStoreWrapper(vector_store) if vector_store else None\n        self.vector_store_filter = vector_store_filter\n        self.websocket = websocket\n        self.agent = agent\n        self.role = role\n        self.parent_query = parent_query\n        self.subtopics = subtopics or []\n        self.visited_urls = visited_urls or set()\n        self.verbose = verbose\n        self.context = context or []\n        self.headers = headers or {}\n        self.research_costs = 0.0\n        self.retrievers = get_retrievers(self.headers, self.cfg)\n        self.memory = Memory(\n            self.cfg.embedding_provider, self.cfg.embedding_model, **self.cfg.embedding_kwargs\n        )\n        self.log_handler = log_handler\n\n        # Initialize components\n        self.research_conductor: ResearchConductor = ResearchConductor(self)\n        self.report_generator: ReportGenerator = ReportGenerator(self)\n        self.context_manager: ContextManager = ContextManager(self)\n        self.scraper_manager: BrowserManager = BrowserManager(self)\n        self.source_curator: SourceCurator = SourceCurator(self)\n        self.deep_researcher: Optional[DeepResearchSkill] = None\n        if report_type == ReportType.DeepResearch.value:\n            self.deep_researcher = DeepResearchSkill(self)\n\n    async def _log_event(self, event_type: str, **kwargs):\n        \"\"\"Helper method to handle logging events\"\"\"\n        if self.log_handler:\n            try:\n                if event_type == \"tool\":\n                    await self.log_handler.on_tool_start(kwargs.get('tool_name', ''), **kwargs)\n                elif event_type == \"action\":\n                    await self.log_handler.on_agent_action(kwargs.get('action', ''), **kwargs)\n                elif event_type == \"research\":\n                    await self.log_handler.on_research_step(kwargs.get('step', ''), kwargs.get('details', {}))\n                \n                # Add direct logging as backup\n                import logging\n                research_logger = logging.getLogger('research')\n                research_logger.info(f\"{event_type}: {json.dumps(kwargs, default=str)}\")\n                \n            except Exception as e:\n                import logging\n                logging.getLogger('research').error(f\"Error in _log_event: {e}\", exc_info=True)\n\n    async def conduct_research(self, on_progress=None):\n        await self._log_event(\"research\", step=\"start\", details={\n            \"query\": self.query,\n            \"report_type\": self.report_type,\n            \"agent\": self.agent,\n            \"role\": self.role\n        })\n\n        # Handle deep research separately\n        if self.report_type == ReportType.DeepResearch.value and self.deep_researcher:\n            return await self._handle_deep_research(on_progress)\n\n        if not (self.agent and self.role):\n            await self._log_event(\"action\", action=\"choose_agent\")\n            self.agent, self.role = await choose_agent(\n                query=self.query,\n                cfg=self.cfg,\n                parent_query=self.parent_query,\n                cost_callback=self.add_costs,\n                headers=self.headers,\n            )\n            await self._log_event(\"action\", action=\"agent_selected\", details={\n                \"agent\": self.agent,\n                \"role\": self.role\n            })\n\n        await self._log_event(\"research\", step=\"conducting_research\", details={\n            \"agent\": self.agent,\n            \"role\": self.role\n        })\n        self.context = await self.research_conductor.conduct_research()\n        \n        await self._log_event(\"research\", step=\"research_completed\", details={\n            \"context_length\": len(self.context)\n        })\n        return self.context\n\n    async def _handle_deep_research(self, on_progress=None):\n        \"\"\"Handle deep research execution and logging.\"\"\"\n        # Log deep research configuration\n        await self._log_event(\"research\", step=\"deep_research_initialize\", details={\n            \"type\": \"deep_research\",\n            \"breadth\": self.deep_researcher.breadth,\n            \"depth\": self.deep_researcher.depth,\n            \"concurrency\": self.deep_researcher.concurrency_limit\n        })\n\n        # Log deep research start\n        await self._log_event(\"research\", step=\"deep_research_start\", details={\n            \"query\": self.query,\n            \"breadth\": self.deep_researcher.breadth,\n            \"depth\": self.deep_researcher.depth,\n            \"concurrency\": self.deep_researcher.concurrency_limit\n        })\n\n        # Run deep research and get context\n        self.context = await self.deep_researcher.run(on_progress=on_progress)\n\n        # Get total research costs\n        total_costs = self.get_costs()\n\n        # Log deep research completion with costs\n        await self._log_event(\"research\", step=\"deep_research_complete\", details={\n            \"context_length\": len(self.context),\n            \"visited_urls\": len(self.visited_urls),\n            \"total_costs\": total_costs\n        })\n\n        # Log final cost update\n        await self._log_event(\"research\", step=\"cost_update\", details={\n            \"cost\": total_costs,\n            \"total_cost\": total_costs,\n            \"research_type\": \"deep_research\"\n        })\n\n        # Return the research context\n        return self.context\n\n    async def write_report(self, existing_headers: list = [], relevant_written_contents: list = [], ext_context=None) -> str:\n        await self._log_event(\"research\", step=\"writing_report\", details={\n            \"existing_headers\": existing_headers,\n            \"context_source\": \"external\" if ext_context else \"internal\"\n        })\n        \n        report = await self.report_generator.write_report(\n            existing_headers,\n            relevant_written_contents,\n            ext_context or self.context\n        )\n        \n        await self._log_event(\"research\", step=\"report_completed\", details={\n            \"report_length\": len(report)\n        })\n        return report\n\n    async def write_report_conclusion(self, report_body: str) -> str:\n        await self._log_event(\"research\", step=\"writing_conclusion\")\n        conclusion = await self.report_generator.write_report_conclusion(report_body)\n        await self._log_event(\"research\", step=\"conclusion_completed\")\n        return conclusion\n\n    async def write_introduction(self):\n        await self._log_event(\"research\", step=\"writing_introduction\")\n        intro = await self.report_generator.write_introduction()\n        await self._log_event(\"research\", step=\"introduction_completed\")\n        return intro\n\n    async def get_subtopics(self):\n        return await self.report_generator.get_subtopics()\n\n    async def get_draft_section_titles(self, current_subtopic: str):\n        return await self.report_generator.get_draft_section_titles(current_subtopic)\n\n    async def get_similar_written_contents_by_draft_section_titles(\n        self,\n        current_subtopic: str,\n        draft_section_titles: list[str],\n        written_contents: list[dict],\n        max_results: int = 10\n    ) -> list[str]:\n        return await self.context_manager.get_similar_written_contents_by_draft_section_titles(\n            current_subtopic,\n            draft_section_titles,\n            written_contents,\n            max_results\n        )\n\n    # Utility methods\n    def get_research_images(self, top_k=10) -> list[dict[str, Any]]:\n        return self.research_images[:top_k]\n\n    def add_research_images(self, images: list[dict[str, Any]]) -> None:\n        self.research_images.extend(images)\n\n    def get_research_sources(self) -> list[dict[str, Any]]:\n        return self.research_sources\n\n    def add_research_sources(self, sources: list[dict[str, Any]]) -> None:\n        self.research_sources.extend(sources)\n\n    def add_references(self, report_markdown: str, visited_urls: set) -> str:\n        return add_references(report_markdown, visited_urls)\n\n    def extract_headers(self, markdown_text: str) -> list[dict]:\n        return extract_headers(markdown_text)\n\n    def extract_sections(self, markdown_text: str) -> list[dict]:\n        return extract_sections(markdown_text)\n\n    def table_of_contents(self, markdown_text: str) -> str:\n        return table_of_contents(markdown_text)\n\n    def get_source_urls(self) -> list:\n        return list(self.visited_urls)\n\n    def get_research_context(self) -> list:\n        return self.context\n\n    def get_costs(self) -> float:\n        return self.research_costs\n\n    def set_verbose(self, verbose: bool):\n        self.verbose = verbose\n\n    def add_costs(self, cost: float) -> None:\n        if not isinstance(cost, (float, int)):\n            raise ValueError(\"Cost must be an integer or float\")\n        self.research_costs += cost\n        if self.log_handler:\n            self._log_event(\"research\", step=\"cost_update\", details={\n                \"cost\": cost,\n                \"total_cost\": self.research_costs\n            })\n"}
{"type": "source_file", "path": "gpt_researcher/actions/retriever.py", "content": "from ..config.config import Config\n\ndef get_retriever(retriever: str):\n    \"\"\"\n    Gets the retriever\n    Args:\n        retriever (str): retriever name\n\n    Returns:\n        retriever: Retriever class\n\n    \"\"\"\n    match retriever:\n        case \"google\":\n            from gpt_researcher.retrievers import GoogleSearch\n\n            return GoogleSearch\n        case \"searx\":\n            from gpt_researcher.retrievers import SearxSearch\n\n            return SearxSearch\n        case \"searchapi\":\n            from gpt_researcher.retrievers import SearchApiSearch\n\n            return SearchApiSearch\n        case \"serpapi\":\n            from gpt_researcher.retrievers import SerpApiSearch\n\n            return SerpApiSearch\n        case \"serper\":\n            from gpt_researcher.retrievers import SerperSearch\n\n            return SerperSearch\n        case \"duckduckgo\":\n            from gpt_researcher.retrievers import Duckduckgo\n\n            return Duckduckgo\n        case \"bing\":\n            from gpt_researcher.retrievers import BingSearch\n\n            return BingSearch\n        case \"arxiv\":\n            from gpt_researcher.retrievers import ArxivSearch\n\n            return ArxivSearch\n        case \"tavily\":\n            from gpt_researcher.retrievers import TavilySearch\n\n            return TavilySearch\n        case \"exa\":\n            from gpt_researcher.retrievers import ExaSearch\n\n            return ExaSearch\n        case \"semantic_scholar\":\n            from gpt_researcher.retrievers import SemanticScholarSearch\n\n            return SemanticScholarSearch\n        case \"pubmed_central\":\n            from gpt_researcher.retrievers import PubMedCentralSearch\n\n            return PubMedCentralSearch\n        case \"custom\":\n            from gpt_researcher.retrievers import CustomRetriever\n\n            return CustomRetriever\n\n        case _:\n            return None\n\n\ndef get_retrievers(headers: dict[str, str], cfg: Config):\n    \"\"\"\n    Determine which retriever(s) to use based on headers, config, or default.\n\n    Args:\n        headers (dict): The headers dictionary\n        cfg (Config): The configuration object\n\n    Returns:\n        list: A list of retriever classes to be used for searching.\n    \"\"\"\n    # Check headers first for multiple retrievers\n    if headers.get(\"retrievers\"):\n        retrievers = headers.get(\"retrievers\").split(\",\")\n    # If not found, check headers for a single retriever\n    elif headers.get(\"retriever\"):\n        retrievers = [headers.get(\"retriever\")]\n    # If not in headers, check config for multiple retrievers\n    elif cfg.retrievers:\n        retrievers = cfg.retrievers\n    # If not found, check config for a single retriever\n    elif cfg.retriever:\n        retrievers = [cfg.retriever]\n    # If still not set, use default retriever\n    else:\n        retrievers = [get_default_retriever().__name__]\n\n    # Convert retriever names to actual retriever classes\n    # Use get_default_retriever() as a fallback for any invalid retriever names\n    return [get_retriever(r) or get_default_retriever() for r in retrievers]\n\n\ndef get_default_retriever():\n    from gpt_researcher.retrievers import TavilySearch\n\n    return TavilySearch"}
{"type": "source_file", "path": "gpt_researcher/config/__init__.py", "content": "from .config import Config\nfrom .variables.base import BaseConfig\nfrom .variables.default import DEFAULT_CONFIG as DefaultConfig\n\n__all__ = [\"Config\", \"BaseConfig\", \"DefaultConfig\"]\n"}
{"type": "source_file", "path": "gpt_researcher/config/config.py", "content": "import json\nimport os\nimport warnings\nfrom typing import Dict, Any, List, Union, Type, get_origin, get_args\nfrom .variables.default import DEFAULT_CONFIG\nfrom .variables.base import BaseConfig\nfrom ..retrievers.utils import get_all_retriever_names\n\n\nclass Config:\n    \"\"\"Config class for GPT Researcher.\"\"\"\n\n    CONFIG_DIR = os.path.join(os.path.dirname(__file__), \"variables\")\n\n    def __init__(self, config_path: str | None = None):\n        \"\"\"Initialize the config class.\"\"\"\n        self.config_path = config_path\n        self.llm_kwargs: Dict[str, Any] = {}\n        self.embedding_kwargs: Dict[str, Any] = {}\n\n        config_to_use = self.load_config(config_path)\n        self._set_attributes(config_to_use)\n        self._set_embedding_attributes()\n        self._set_llm_attributes()\n        self._handle_deprecated_attributes()\n        if config_to_use['REPORT_SOURCE'] != 'web':\n          self._set_doc_path(config_to_use)\n\n    def _set_attributes(self, config: Dict[str, Any]) -> None:\n        for key, value in config.items():\n            env_value = os.getenv(key)\n            if env_value is not None:\n                value = self.convert_env_value(key, env_value, BaseConfig.__annotations__[key])\n            setattr(self, key.lower(), value)\n\n        # Handle RETRIEVER with default value\n        retriever_env = os.environ.get(\"RETRIEVER\", config.get(\"RETRIEVER\", \"tavily\"))\n        try:\n            self.retrievers = self.parse_retrievers(retriever_env)\n        except ValueError as e:\n            print(f\"Warning: {str(e)}. Defaulting to 'tavily' retriever.\")\n            self.retrievers = [\"tavily\"]\n\n    def _set_embedding_attributes(self) -> None:\n        self.embedding_provider, self.embedding_model = self.parse_embedding(\n            self.embedding\n        )\n\n    def _set_llm_attributes(self) -> None:\n        self.fast_llm_provider, self.fast_llm_model = self.parse_llm(self.fast_llm)\n        self.smart_llm_provider, self.smart_llm_model = self.parse_llm(self.smart_llm)\n        self.strategic_llm_provider, self.strategic_llm_model = self.parse_llm(self.strategic_llm)\n\n    def _handle_deprecated_attributes(self) -> None:\n        if os.getenv(\"EMBEDDING_PROVIDER\") is not None:\n            warnings.warn(\n                \"EMBEDDING_PROVIDER is deprecated and will be removed soon. Use EMBEDDING instead.\",\n                FutureWarning,\n                stacklevel=2,\n            )\n            self.embedding_provider = (\n                os.environ[\"EMBEDDING_PROVIDER\"] or self.embedding_provider\n            )\n\n            match os.environ[\"EMBEDDING_PROVIDER\"]:\n                case \"ollama\":\n                    self.embedding_model = os.environ[\"OLLAMA_EMBEDDING_MODEL\"]\n                case \"custom\":\n                    self.embedding_model = os.getenv(\"OPENAI_EMBEDDING_MODEL\", \"custom\")\n                case \"openai\":\n                    self.embedding_model = \"text-embedding-3-large\"\n                case \"azure_openai\":\n                    self.embedding_model = \"text-embedding-3-large\"\n                case \"huggingface\":\n                    self.embedding_model = \"sentence-transformers/all-MiniLM-L6-v2\"\n                case \"google_genai\":\n                    self.embedding_model = \"text-embedding-004\"\n                case _:\n                    raise Exception(\"Embedding provider not found.\")\n\n        _deprecation_warning = (\n            \"LLM_PROVIDER, FAST_LLM_MODEL and SMART_LLM_MODEL are deprecated and \"\n            \"will be removed soon. Use FAST_LLM and SMART_LLM instead.\"\n        )\n        if os.getenv(\"LLM_PROVIDER\") is not None:\n            warnings.warn(_deprecation_warning, FutureWarning, stacklevel=2)\n            self.fast_llm_provider = (\n                os.environ[\"LLM_PROVIDER\"] or self.fast_llm_provider\n            )\n            self.smart_llm_provider = (\n                os.environ[\"LLM_PROVIDER\"] or self.smart_llm_provider\n            )\n        if os.getenv(\"FAST_LLM_MODEL\") is not None:\n            warnings.warn(_deprecation_warning, FutureWarning, stacklevel=2)\n            self.fast_llm_model = os.environ[\"FAST_LLM_MODEL\"] or self.fast_llm_model\n        if os.getenv(\"SMART_LLM_MODEL\") is not None:\n            warnings.warn(_deprecation_warning, FutureWarning, stacklevel=2)\n            self.smart_llm_model = os.environ[\"SMART_LLM_MODEL\"] or self.smart_llm_model\n        \n    def _set_doc_path(self, config: Dict[str, Any]) -> None:\n        self.doc_path = config['DOC_PATH']\n        if self.doc_path:\n            try:\n                self.validate_doc_path()\n            except Exception as e:\n                print(f\"Warning: Error validating doc_path: {str(e)}. Using default doc_path.\")\n                self.doc_path = DEFAULT_CONFIG['DOC_PATH']\n\n    @classmethod\n    def load_config(cls, config_path: str | None) -> Dict[str, Any]:\n        \"\"\"Load a configuration by name.\"\"\"\n        if config_path is None:\n            return DEFAULT_CONFIG\n\n        # config_path = os.path.join(cls.CONFIG_DIR, config_path)\n        if not os.path.exists(config_path):\n            if config_path and config_path != \"default\":\n                print(f\"Warning: Configuration not found at '{config_path}'. Using default configuration.\")\n                if not config_path.endswith(\".json\"):\n                    print(f\"Do you mean '{config_path}.json'?\")\n            return DEFAULT_CONFIG\n\n        with open(config_path, \"r\") as f:\n            custom_config = json.load(f)\n\n        # Merge with default config to ensure all keys are present\n        merged_config = DEFAULT_CONFIG.copy()\n        merged_config.update(custom_config)\n        return merged_config\n\n    @classmethod\n    def list_available_configs(cls) -> List[str]:\n        \"\"\"List all available configuration names.\"\"\"\n        configs = [\"default\"]\n        for file in os.listdir(cls.CONFIG_DIR):\n            if file.endswith(\".json\"):\n                configs.append(file[:-5])  # Remove .json extension\n        return configs\n\n    def parse_retrievers(self, retriever_str: str) -> List[str]:\n        \"\"\"Parse the retriever string into a list of retrievers and validate them.\"\"\"\n        retrievers = [retriever.strip()\n                      for retriever in retriever_str.split(\",\")]\n        valid_retrievers = get_all_retriever_names() or []\n        invalid_retrievers = [r for r in retrievers if r not in valid_retrievers]\n        if invalid_retrievers:\n            raise ValueError(\n                f\"Invalid retriever(s) found: {', '.join(invalid_retrievers)}. \"\n                f\"Valid options are: {', '.join(valid_retrievers)}.\"\n            )\n        return retrievers\n\n    @staticmethod\n    def parse_llm(llm_str: str | None) -> tuple[str | None, str | None]:\n        \"\"\"Parse llm string into (llm_provider, llm_model).\"\"\"\n        from gpt_researcher.llm_provider.generic.base import _SUPPORTED_PROVIDERS\n\n        if llm_str is None:\n            return None, None\n        try:\n            llm_provider, llm_model = llm_str.split(\":\", 1)\n            assert llm_provider in _SUPPORTED_PROVIDERS, (\n                f\"Unsupported {llm_provider}.\\nSupported llm providers are: \"\n                + \", \".join(_SUPPORTED_PROVIDERS)\n            )\n            return llm_provider, llm_model\n        except ValueError:\n            raise ValueError(\n                \"Set SMART_LLM or FAST_LLM = '<llm_provider>:<llm_model>' \"\n                \"Eg 'openai:gpt-4o-mini'\"\n            )\n\n    @staticmethod\n    def parse_embedding(embedding_str: str | None) -> tuple[str | None, str | None]:\n        \"\"\"Parse embedding string into (embedding_provider, embedding_model).\"\"\"\n        from gpt_researcher.memory.embeddings import _SUPPORTED_PROVIDERS\n\n        if embedding_str is None:\n            return None, None\n        try:\n            embedding_provider, embedding_model = embedding_str.split(\":\", 1)\n            assert embedding_provider in _SUPPORTED_PROVIDERS, (\n                f\"Unsupported {embedding_provider}.\\nSupported embedding providers are: \"\n                + \", \".join(_SUPPORTED_PROVIDERS)\n            )\n            return embedding_provider, embedding_model\n        except ValueError:\n            raise ValueError(\n                \"Set EMBEDDING = '<embedding_provider>:<embedding_model>' \"\n                \"Eg 'openai:text-embedding-3-large'\"\n            )\n\n    def validate_doc_path(self):\n        \"\"\"Ensure that the folder exists at the doc path\"\"\"\n        os.makedirs(self.doc_path, exist_ok=True)\n\n    @staticmethod\n    def convert_env_value(key: str, env_value: str, type_hint: Type) -> Any:\n        \"\"\"Convert environment variable to the appropriate type based on the type hint.\"\"\"\n        origin = get_origin(type_hint)\n        args = get_args(type_hint)\n\n        if origin is Union:\n            # Handle Union types (e.g., Union[str, None])\n            for arg in args:\n                if arg is type(None):\n                    if env_value.lower() in (\"none\", \"null\", \"\"):\n                        return None\n                else:\n                    try:\n                        return Config.convert_env_value(key, env_value, arg)\n                    except ValueError:\n                        continue\n            raise ValueError(f\"Cannot convert {env_value} to any of {args}\")\n\n        if type_hint is bool:\n            return env_value.lower() in (\"true\", \"1\", \"yes\", \"on\")\n        elif type_hint is int:\n            return int(env_value)\n        elif type_hint is float:\n            return float(env_value)\n        elif type_hint in (str, Any):\n            return env_value\n        elif origin is list or origin is List:\n            return json.loads(env_value)\n        else:\n            raise ValueError(f\"Unsupported type {type_hint} for key {key}\")\n"}
{"type": "source_file", "path": "gpt_researcher/config/variables/__init__.py", "content": ""}
{"type": "source_file", "path": "gpt_researcher/config/variables/base.py", "content": "from typing import Union\nfrom typing_extensions import TypedDict\n\n\nclass BaseConfig(TypedDict):\n    RETRIEVER: str\n    EMBEDDING: str\n    SIMILARITY_THRESHOLD: float\n    FAST_LLM: str\n    SMART_LLM: str\n    STRATEGIC_LLM: str\n    FAST_TOKEN_LIMIT: int\n    SMART_TOKEN_LIMIT: int\n    STRATEGIC_TOKEN_LIMIT: int\n    BROWSE_CHUNK_MAX_LENGTH: int\n    SUMMARY_TOKEN_LIMIT: int\n    TEMPERATURE: float\n    USER_AGENT: str\n    MAX_SEARCH_RESULTS_PER_QUERY: int\n    MEMORY_BACKEND: str\n    TOTAL_WORDS: int\n    REPORT_FORMAT: str\n    CURATE_SOURCES: bool\n    MAX_ITERATIONS: int\n    LANGUAGE: str\n    AGENT_ROLE: Union[str, None]\n    SCRAPER: str\n    MAX_SCRAPER_WORKERS: int\n    MAX_SUBTOPICS: int\n    REPORT_SOURCE: Union[str, None]\n    DOC_PATH: str\n    DEEP_RESEARCH_CONCURRENCY: int\n    DEEP_RESEARCH_DEPTH: int\n    DEEP_RESEARCH_BREADTH: int\n"}
{"type": "source_file", "path": "gpt_researcher/config/variables/default.py", "content": "from .base import BaseConfig\n\nDEFAULT_CONFIG: BaseConfig = {\n    \"RETRIEVER\": \"tavily\",\n    \"EMBEDDING\": \"openai:text-embedding-3-small\",\n    \"SIMILARITY_THRESHOLD\": 0.42,\n    \"FAST_LLM\": \"openai:gpt-4o-mini\",\n    \"SMART_LLM\": \"openai:gpt-4o-2024-11-20\",  # Has support for long responses (2k+ words).\n    \"STRATEGIC_LLM\": \"openai:o3-mini\",  # Can be used with gpt-o1 or gpt-o3\n    \"FAST_TOKEN_LIMIT\": 2000,\n    \"SMART_TOKEN_LIMIT\": 4000,\n    \"STRATEGIC_TOKEN_LIMIT\": 4000,\n    \"BROWSE_CHUNK_MAX_LENGTH\": 8192,\n    \"CURATE_SOURCES\": False,\n    \"SUMMARY_TOKEN_LIMIT\": 700,\n    \"TEMPERATURE\": 0.4,\n    \"USER_AGENT\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36 Edg/119.0.0.0\",\n    \"MAX_SEARCH_RESULTS_PER_QUERY\": 5,\n    \"MEMORY_BACKEND\": \"local\",\n    \"TOTAL_WORDS\": 1200,\n    \"REPORT_FORMAT\": \"APA\",\n    \"MAX_ITERATIONS\": 4,\n    \"AGENT_ROLE\": None,\n    \"SCRAPER\": \"bs\",\n    \"MAX_SCRAPER_WORKERS\": 15,\n    \"MAX_SUBTOPICS\": 3,\n    \"LANGUAGE\": \"english\",\n    \"REPORT_SOURCE\": \"web\",\n    \"DOC_PATH\": \"./my-docs\",\n    # Deep research specific settings\n    \"DEEP_RESEARCH_BREADTH\": 3,\n    \"DEEP_RESEARCH_DEPTH\": 2,\n    \"DEEP_RESEARCH_CONCURRENCY\": 4,\n}\n"}
{"type": "source_file", "path": "gpt_researcher/context/__init__.py", "content": "from .compression import ContextCompressor\nfrom .retriever import SearchAPIRetriever\n\n__all__ = ['ContextCompressor', 'SearchAPIRetriever']\n"}
{"type": "source_file", "path": "gpt_researcher/context/compression.py", "content": "import os\nimport asyncio\nfrom typing import Optional\nfrom .retriever import SearchAPIRetriever, SectionRetriever\nfrom langchain.retrievers import (\n    ContextualCompressionRetriever,\n)\nfrom langchain.retrievers.document_compressors import (\n    DocumentCompressorPipeline,\n    EmbeddingsFilter,\n)\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom ..vector_store import VectorStoreWrapper\nfrom ..utils.costs import estimate_embedding_cost\nfrom ..memory.embeddings import OPENAI_EMBEDDING_MODEL\n\n\nclass VectorstoreCompressor:\n    def __init__(self, vector_store: VectorStoreWrapper, max_results:int = 7, filter: Optional[dict] = None, **kwargs):\n\n        self.vector_store = vector_store\n        self.max_results = max_results\n        self.filter = filter\n        self.kwargs = kwargs\n\n    def __pretty_print_docs(self, docs):\n        return f\"\\n\".join(f\"Source: {d.metadata.get('source')}\\n\"\n                          f\"Title: {d.metadata.get('title')}\\n\"\n                          f\"Content: {d.page_content}\\n\"\n                          for d in docs)\n\n    async def async_get_context(self, query, max_results=5):\n        \"\"\"Get relevant context from vector store\"\"\"\n        results = await self.vector_store.asimilarity_search(query=query, k=max_results, filter=self.filter)\n        return self.__pretty_print_docs(results)\n\n\nclass ContextCompressor:\n    def __init__(self, documents, embeddings, max_results=5, **kwargs):\n        self.max_results = max_results\n        self.documents = documents\n        self.kwargs = kwargs\n        self.embeddings = embeddings\n        self.similarity_threshold = os.environ.get(\"SIMILARITY_THRESHOLD\", 0.35)\n\n    def __get_contextual_retriever(self):\n        splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n        relevance_filter = EmbeddingsFilter(embeddings=self.embeddings,\n                                            similarity_threshold=self.similarity_threshold)\n        pipeline_compressor = DocumentCompressorPipeline(\n            transformers=[splitter, relevance_filter]\n        )\n        base_retriever = SearchAPIRetriever(\n            pages=self.documents\n        )\n        contextual_retriever = ContextualCompressionRetriever(\n            base_compressor=pipeline_compressor, base_retriever=base_retriever\n        )\n        return contextual_retriever\n\n    def __pretty_print_docs(self, docs, top_n):\n        return f\"\\n\".join(f\"Source: {d.metadata.get('source')}\\n\"\n                          f\"Title: {d.metadata.get('title')}\\n\"\n                          f\"Content: {d.page_content}\\n\"\n                          for i, d in enumerate(docs) if i < top_n)\n\n    async def async_get_context(self, query, max_results=5, cost_callback=None):\n        compressed_docs = self.__get_contextual_retriever()\n        if cost_callback:\n            cost_callback(estimate_embedding_cost(model=OPENAI_EMBEDDING_MODEL, docs=self.documents))\n        relevant_docs = await asyncio.to_thread(compressed_docs.invoke, query)\n        return self.__pretty_print_docs(relevant_docs, max_results)\n\n\nclass WrittenContentCompressor:\n    def __init__(self, documents, embeddings, similarity_threshold, **kwargs):\n        self.documents = documents\n        self.kwargs = kwargs\n        self.embeddings = embeddings\n        self.similarity_threshold = similarity_threshold\n\n    def __get_contextual_retriever(self):\n        splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n        relevance_filter = EmbeddingsFilter(embeddings=self.embeddings,\n                                            similarity_threshold=self.similarity_threshold)\n        pipeline_compressor = DocumentCompressorPipeline(\n            transformers=[splitter, relevance_filter]\n        )\n        base_retriever = SectionRetriever(\n            sections=self.documents\n        )\n        contextual_retriever = ContextualCompressionRetriever(\n            base_compressor=pipeline_compressor, base_retriever=base_retriever\n        )\n        return contextual_retriever\n\n    def __pretty_docs_list(self, docs, top_n):\n        return [f\"Title: {d.metadata.get('section_title')}\\nContent: {d.page_content}\\n\" for i, d in enumerate(docs) if i < top_n]\n\n    async def async_get_context(self, query, max_results=5, cost_callback=None):\n        compressed_docs = self.__get_contextual_retriever()\n        if cost_callback:\n            cost_callback(estimate_embedding_cost(model=OPENAI_EMBEDDING_MODEL, docs=self.documents))\n        relevant_docs = await asyncio.to_thread(compressed_docs.invoke, query)\n        return self.__pretty_docs_list(relevant_docs, max_results)\n"}
{"type": "source_file", "path": "gpt_researcher/context/retriever.py", "content": "import os\nfrom enum import Enum\nfrom typing import Any, Dict, List, Optional\n\nfrom langchain.callbacks.manager import CallbackManagerForRetrieverRun\nfrom langchain.schema import Document\nfrom langchain.schema.retriever import BaseRetriever\n\n\nclass SearchAPIRetriever(BaseRetriever):\n    \"\"\"Search API retriever.\"\"\"\n    pages: List[Dict] = []\n\n    def _get_relevant_documents(\n        self, query: str, *, run_manager: CallbackManagerForRetrieverRun\n    ) -> List[Document]:\n\n        docs = [\n            Document(\n                page_content=page.get(\"raw_content\", \"\"),\n                metadata={\n                    \"title\": page.get(\"title\", \"\"),\n                    \"source\": page.get(\"url\", \"\"),\n                },\n            )\n            for page in self.pages\n        ]\n\n        return docs\n\nclass SectionRetriever(BaseRetriever):\n    \"\"\"\n    SectionRetriever:\n    This class is used to retrieve sections while avoiding redundant subtopics.\n    \"\"\"\n    sections: List[Dict] = []\n    \"\"\"\n    sections example:\n    [\n        {\n            \"section_title\": \"Example Title\",\n            \"written_content\": \"Example content\"\n        },\n        ...\n    ]\n    \"\"\"\n    \n    def _get_relevant_documents(\n        self, query: str, *, run_manager: CallbackManagerForRetrieverRun\n    ) -> List[Document]:\n\n        docs = [\n            Document(\n                page_content=page.get(\"written_content\", \"\"),\n                metadata={\n                    \"section_title\": page.get(\"section_title\", \"\"),\n                },\n            )\n            for page in self.sections  # Changed 'self.pages' to 'self.sections'\n        ]\n\n        return docs"}
{"type": "source_file", "path": "gpt_researcher/document/document.py", "content": "import asyncio\nimport os\nfrom typing import List, Union\nfrom langchain_community.document_loaders import (\n    PyMuPDFLoader,\n    TextLoader,\n    UnstructuredCSVLoader,\n    UnstructuredExcelLoader,\n    UnstructuredMarkdownLoader,\n    UnstructuredPowerPointLoader,\n    UnstructuredWordDocumentLoader\n)\nfrom langchain_community.document_loaders import BSHTMLLoader\n\n\nclass DocumentLoader:\n\n    def __init__(self, path: Union[str, List[str]]):\n        self.path = path\n\n    async def load(self) -> list:\n        tasks = []\n        if isinstance(self.path, list):\n            for file_path in self.path:\n                if os.path.isfile(file_path):  # Ensure it's a valid file\n                    filename = os.path.basename(file_path)\n                    file_name, file_extension_with_dot = os.path.splitext(filename)\n                    file_extension = file_extension_with_dot.strip(\".\").lower()\n                    tasks.append(self._load_document(file_path, file_extension))\n                    \n        elif isinstance(self.path, (str, bytes, os.PathLike)):\n            for root, dirs, files in os.walk(self.path):\n                for file in files:\n                    file_path = os.path.join(root, file)\n                    file_name, file_extension_with_dot = os.path.splitext(file)\n                    file_extension = file_extension_with_dot.strip(\".\").lower()\n                    tasks.append(self._load_document(file_path, file_extension))\n                    \n        else:\n            raise ValueError(\"Invalid type for path. Expected str, bytes, os.PathLike, or list thereof.\")\n\n        # for root, dirs, files in os.walk(self.path):\n        #     for file in files:\n        #         file_path = os.path.join(root, file)\n        #         file_name, file_extension_with_dot = os.path.splitext(file_path)\n        #         file_extension = file_extension_with_dot.strip(\".\")\n        #         tasks.append(self._load_document(file_path, file_extension))\n\n        docs = []\n        for pages in await asyncio.gather(*tasks):\n            for page in pages:\n                if page.page_content:\n                    docs.append({\n                        \"raw_content\": page.page_content,\n                        \"url\": os.path.basename(page.metadata['source'])\n                    })\n                    \n        if not docs:\n            raise ValueError(\"🤷 Failed to load any documents!\")\n\n        return docs\n\n    async def _load_document(self, file_path: str, file_extension: str) -> list:\n        ret_data = []\n        try:\n            loader_dict = {\n                \"pdf\": PyMuPDFLoader(file_path),\n                \"txt\": TextLoader(file_path),\n                \"doc\": UnstructuredWordDocumentLoader(file_path),\n                \"docx\": UnstructuredWordDocumentLoader(file_path),\n                \"pptx\": UnstructuredPowerPointLoader(file_path),\n                \"csv\": UnstructuredCSVLoader(file_path, mode=\"elements\"),\n                \"xls\": UnstructuredExcelLoader(file_path, mode=\"elements\"),\n                \"xlsx\": UnstructuredExcelLoader(file_path, mode=\"elements\"),\n                \"md\": UnstructuredMarkdownLoader(file_path),\n                \"html\": BSHTMLLoader(file_path),\n                \"htm\": BSHTMLLoader(file_path)\n            }\n\n            loader = loader_dict.get(file_extension, None)\n            if loader:\n                try:\n                    ret_data = loader.load()\n                except Exception as e:\n                    print(f\"Failed to load HTML document : {file_path}\")\n                    print(e)\n\n        except Exception as e:\n            print(f\"Failed to load document : {file_path}\")\n            print(e)\n\n        return ret_data\n"}
{"type": "source_file", "path": "gpt_researcher/document/azure_document_loader.py", "content": "from azure.storage.blob import BlobServiceClient\nimport os\nimport tempfile\n\nclass AzureDocumentLoader:\n    def __init__(self, container_name, connection_string):\n        self.client = BlobServiceClient.from_connection_string(connection_string)\n        self.container = self.client.get_container_client(container_name)\n\n    async def load(self):\n        \"\"\"Download all blobs to temp files and return their paths.\"\"\"\n        temp_dir = tempfile.mkdtemp()\n        blobs = self.container.list_blobs()\n        file_paths = []\n        for blob in blobs:\n            blob_client = self.container.get_blob_client(blob.name)\n            local_path = os.path.join(temp_dir, blob.name)\n            with open(local_path, \"wb\") as f:\n                blob_data = blob_client.download_blob()\n                f.write(blob_data.readall())\n            file_paths.append(local_path)\n        return file_paths  # Pass to existing DocumentLoader"}
{"type": "source_file", "path": "gpt_researcher/document/__init__.py", "content": "from .document import DocumentLoader\nfrom .online_document import OnlineDocumentLoader\nfrom .langchain_document import LangChainDocumentLoader\n\n__all__ = ['DocumentLoader', 'OnlineDocumentLoader', 'LangChainDocumentLoader']\n"}
{"type": "source_file", "path": "gpt_researcher/document/langchain_document.py", "content": "import asyncio\nimport os\n\nfrom langchain_core.documents import Document\nfrom typing import List, Dict\n\n\n# Supports the base Document class from langchain\n# - https://github.com/langchain-ai/langchain/blob/master/libs/core/langchain_core/documents/base.py\nclass LangChainDocumentLoader:\n\n    def __init__(self, documents: List[Document]):\n        self.documents = documents\n\n    async def load(self, metadata_source_index=\"title\") -> List[Dict[str, str]]:\n        docs = []\n        for document in self.documents:\n            docs.append(\n                {\n                    \"raw_content\": document.page_content,\n                    \"url\": document.metadata.get(metadata_source_index, \"\"),\n                }\n            )\n        return docs\n"}
