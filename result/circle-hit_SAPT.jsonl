{"repo_info": {"repo_name": "SAPT", "repo_owner": "circle-hit", "repo_url": "https://github.com/circle-hit/SAPT"}}
{"type": "test_file", "path": "pseudo_data/src/rouge/test_util.py", "content": "# coding=utf-8\n# Copyright 2022 The Google Research Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Test utils for ROUGE.\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\n\n_TESTDATA_PREFIX = os.path.join(os.path.dirname(__file__), \"testdata\")\n\nTARGETS_FILE = os.path.join(_TESTDATA_PREFIX, \"target.txt\")\n\nPREDICTIONS_FILE = os.path.join(_TESTDATA_PREFIX, \"prediction.txt\")\n\nLARGE_TARGETS_FILE = os.path.join(_TESTDATA_PREFIX, \"target_large.txt\")\n\nLARGE_PREDICTIONS_FILE = os.path.join(_TESTDATA_PREFIX, \"prediction_large.txt\")\n\nDELIMITED_FILE = os.path.join(_TESTDATA_PREFIX, \"delimited.txt\")\n\nPYROUGE_DIR = os.path.join(_TESTDATA_PREFIX, \"pyrouge_files\")\n\n\ndef get_text(fname):\n  with open(fname) as f:\n    return f.read()\n"}
{"type": "test_file", "path": "src/rouge/test_util.py", "content": "# coding=utf-8\n# Copyright 2022 The Google Research Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Test utils for ROUGE.\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\n\n_TESTDATA_PREFIX = os.path.join(os.path.dirname(__file__), \"testdata\")\n\nTARGETS_FILE = os.path.join(_TESTDATA_PREFIX, \"target.txt\")\n\nPREDICTIONS_FILE = os.path.join(_TESTDATA_PREFIX, \"prediction.txt\")\n\nLARGE_TARGETS_FILE = os.path.join(_TESTDATA_PREFIX, \"target_large.txt\")\n\nLARGE_PREDICTIONS_FILE = os.path.join(_TESTDATA_PREFIX, \"prediction_large.txt\")\n\nDELIMITED_FILE = os.path.join(_TESTDATA_PREFIX, \"delimited.txt\")\n\nPYROUGE_DIR = os.path.join(_TESTDATA_PREFIX, \"pyrouge_files\")\n\n\ndef get_text(fname):\n  with open(fname) as f:\n    return f.read()\n"}
{"type": "source_file", "path": "data/generate_labels.py", "content": "\"\"\"\ninstruction 中任务的标签集合作为选项提供，故每个任务应当提供label文件\n\"\"\"\n\nimport os\nimport json\n\n\ndef label_collect(task_path, task_collect_fun, filter_path=None, max_labels=30):\n    \"\"\"\n    在任务路径下，为每个数据集，生成一个labels.json\n    labels.json 包含内容为所有标签的list，以NER为例：\n            ['person', 'location', 'organization']\n\n    Args:\n        task_path: task files path which contains datasets dir\n        task_collect_fun: task specific label collect function\n        filter_path: filter datasets with too much labels\n\n    Returns:\n\n    \"\"\"\n    filter_ds_dirs = []\n\n    for dirpath, dirnames, filenames in os.walk(task_path):\n        for dirname in dirnames:\n            ds_dir = os.path.join(dirpath, dirname)\n            labels = []\n\n            # 假设全部包含train、dev、test数据集\n            for ds_type in ['train', 'dev', 'test']:\n                ds_type_file = os.path.join(ds_dir, ds_type+'.json')\n                ds_type_labels = task_collect_fun(ds_type_file)\n                labels += ds_type_labels\n\n            labels = list(set(labels))\n            out_file = os.path.join(ds_dir, 'labels.json')\n            json.dump(labels, open(out_file, 'w+', encoding='utf-8'), ensure_ascii=False)\n            print('Finish out {} labels to {}!'.format(len(labels), out_file))\n\n            if len(labels) >= max_labels:\n                filter_dir = os.path.join(filter_path, dirname)\n                filter_ds_dirs.append([ds_dir, filter_dir])\n\n    # filter datasets by mv them to another dirs\n    for ds_dir, filter_dir in filter_ds_dirs:\n        cmd = \"mv {} {}\".format(ds_dir, filter_dir)\n        os.system(cmd)\n        print('Move {} to {} cz too much labels!'.format(ds_dir, filter_dir))\n\n\ndef NER_label_collect(file_path):\n    \"\"\"\n    按照输入文件路径，收集标签，按照list返回。\n    输入文件为一个样例json列表，返回为标签集合\n    Args:\n        file_path: dataset file name\n\n    Returns:\n        [label1, label2]\n    \"\"\"\n    fi = open(file_path, 'r+', encoding='utf-8')\n    samples = json.load(fi)\n    labels = []\n\n    for sample in samples:\n        for entity in sample['entities']:\n            if entity['type'] not in labels:\n                labels.append(entity['type'])\n\n    return labels\n\n\nif __name__ == \"__main__\":\n    NER_path = '/root/InstructUIE/data/NER_processed'\n    filter_path = '/root/InstructUIE/data/NER_filter'\n    label_collect(NER_path, NER_label_collect, filter_path)\n"}
{"type": "source_file", "path": "gen_script_long_t5.py", "content": "import json\nimport random\n\ndef load_json(path):\n    with open(path, 'r', encoding='utf-8') as f:\n        data = json.load(f)\n    return data\n\ndef write_json(path, data):\n    with open(path, 'w', encoding='utf-8') as f:\n        json.dump(data, f, ensure_ascii=False,indent=2)\n\ndef load_jsonline(path):\n    with open(path, 'r', encoding='utf-8') as f:\n        result=[]\n        for line_s in f:\n            line=json.loads(line_s)\n            result.append(line)\n    return result\n\ndef write_jsonline(path, data):\n    with open(path, 'w', encoding='utf-8') as f:\n        for line in data:\n            line_s=json.dumps(line, ensure_ascii=False)\n            f.write(line_s)\n            f.write('\\n')\n\norder_idx = 4\n\nif order_idx == 4:\n    all_tasks=[\n        \"yelp\",\n        \"amazon\",\n        \"mnli\",\n        \"cb\",\n        \"copa\",\n        \"qqp\",\n        \"rte\",\n        \"imdb\",\n        \"sst2\",\n        \"dbpedia\",\n        \"agnews\",\n        \"yahoo\",\n        \"multirc\",\n        \"boolq\",\n        \"wic\"\n    ] # Order 4\nelse:\n    all_tasks = [\"mnli\",\n                 \"cb\",\n                 \"wic\",\n                 \"copa\",\n                 \"qqp\",\n                 \"boolq\",\n                 \"rte\",\n                 \"imdb\",\n                 \"yelp\",\n                 \"amazon\",\n                 \"sst2\",\n                 \"dbpedia\",\n                 \"agnews\",\n                 \"multirc\",\n                 \"yahoo\"] # Order 5\n\ndataset_list = all_tasks\ntask_order = ','.join(all_tasks)\n\nconfig_template={\n    \"Long_Sequence\": [\n    ],\n}\n\nimport os\nimport pathlib\nimport numpy as np\nfrom copy import deepcopy\n\nrun_name = f\"your_job_name\"\nlora_r = 8\nlora_alpha = 32\nlora_dropout = 0.\nkl_ratio = 0.1\nattn_temperature = 1\nlearning_rate = 3e-4\nreplay_after_n_epoch = 0\nnum_train_epochs = 10\n\n ############# Dataset ##############\nhistory_config=[]\nfor one_data_name in dataset_list:\n\n ############# Config ##############\n    pathlib.Path(f'./configs/{run_name}_configs/{one_data_name}').mkdir(parents=True, exist_ok=True)\n\n    config={\n        \"sampling strategy\": \"full\",\n        \"dataset name\": f\"{one_data_name}\"\n    } \n    history_config.append(config)\n\n    dev_config=deepcopy(config_template)\n    dev_config['Long_Sequence'].append(config)\n    write_json(f'./configs/{run_name}_configs/{one_data_name}/dev_tasks.json', dev_config)\n    \n    train_config=deepcopy(config_template)\n    train_config['Long_Sequence'].append(config)\n    write_json(f'./configs/{run_name}_configs/{one_data_name}/train_tasks.json', train_config)\n\n    test_config=deepcopy(config_template)\n    test_config['Long_Sequence'].extend(history_config)\n    write_json(f'./configs/{run_name}_configs/{one_data_name}/test_tasks.json', test_config)\n\n\n############# Bash ##############\n\nsh_str=rf'''#!/bin/bash\n#SBATCH -J cl                           \n#SBATCH -o cl-%j.out                       \n#SBATCH -p compute \n#SBATCH -N 1                           \n#SBATCH -t 20:00:00   \n#SBATCH --mem 128G \n#SBATCH --gres=gpu:a100-sxm4-80gb:1  \n\nexport CUDA_DEVICE_ORDER=\"PCI_BUS_ID\"\n\nport=$(shuf -i25000-30000 -n1)  \n\npython src/run_t5.py \\\n   --do_train \\\n   --do_predict \\\n   --predict_with_generate \\\n   --model_name_or_path your_t5_model_path \\\n   --data_dir CL_Benchmark \\\n   --task_order {task_order} \\\n   --task_config_dir configs/{run_name}_configs/{dataset_list[0]} \\\n   --output_dir logs_and_outputs/{run_name}/outputs/1-{dataset_list[0]} \\\n   --per_device_train_batch_size 16 \\\n   --per_device_eval_batch_size 128 \\\n   --gradient_accumulation_steps 2 \\\n   --learning_rate {learning_rate} \\\n   --num_train_epochs {num_train_epochs} \\\n   --bf16 \\\n   --run_name {run_name} \\\n   --max_source_length 512 \\\n   --max_target_length 50 \\\n   --generation_max_length 50 \\\n   --add_task_name False \\\n   --add_dataset_name False \\\n   --overwrite_output_dir \\\n   --overwrite_cache \\\n   --lr_scheduler_type constant \\\n   --warmup_steps 0 \\\n   --logging_strategy steps \\\n   --logging_steps 10 \\\n   --metric_for_best_model eval_exact_match \\\n   --evaluation_strategy steps \\\n   --save_strategy steps \\\n   --save_total_limit 1 \\\n   --load_best_model_at_end \\\n   --lora_r {lora_r} \\\n   --lora_alpha {lora_alpha} \\\n   --lora_dropout {lora_dropout} \\\n   --add_instruction_replay \\\n   --run_single \\\n   --data_replay_freq -1 \\\n   --replay_after_n_epoch 0 \\\n\nrm -rf logs_and_outputs/{run_name}/outputs/1-{dataset_list[0]}/checkpoint*\n\nsleep 5\n'''\n\nprevious_lora_path_list = []\nfor idx in range(len(dataset_list)-1):\n\n    previous_lora_path_list.append(f\"logs_and_outputs/{run_name}/outputs/{idx+1}-{dataset_list[idx]}/saved_weights\")\n    previous_lora_path = ','.join(previous_lora_path_list)\n\n    sh_str+=rf'''\n\npython src/run_t5.py \\\n   --do_train \\\n   --do_predict \\\n   --predict_with_generate \\\n   --model_name_or_path your_t5_model_path \\\n   --load_checkpoint_from logs_and_outputs/{run_name}/outputs/{idx+1}-{dataset_list[idx]}/saved_weights/trans_input.pt \\\n   --previous_lora_path {previous_lora_path} \\\n   --previous_prompt_key_path logs_and_outputs/{run_name}/outputs/{idx+1}-{dataset_list[idx]}/saved_weights/prompts_keys_till_now.pt \\\n   --data_dir CL_Benchmark \\\n   --task_order {task_order} \\\n   --gen_data_dir generated_data/lora_gen_long_t5 \\\n   --task_config_dir configs/{run_name}_configs/{dataset_list[idx+1]} \\\n   --output_dir logs_and_outputs/{run_name}/outputs/{idx+2}-{dataset_list[idx+1]} \\\n   --per_device_train_batch_size 16 \\\n   --per_device_eval_batch_size 128 \\\n   --gradient_accumulation_steps 2 \\\n   --learning_rate {learning_rate} \\\n   --num_train_epochs {num_train_epochs}\\\n   --bf16 \\\n   --run_name {run_name} \\\n   --max_source_length 512 \\\n   --max_target_length 50 \\\n   --generation_max_length 50 \\\n   --add_task_name False \\\n   --add_dataset_name False \\\n   --overwrite_output_dir \\\n   --overwrite_cache \\\n   --lr_scheduler_type constant \\\n   --warmup_steps 0 \\\n   --logging_strategy steps \\\n   --logging_steps 10 \\\n   --metric_for_best_model eval_exact_match_for_{dataset_list[idx+1]} \\\n   --evaluation_strategy steps \\\n   --save_strategy steps \\\n   --save_total_limit 1 \\\n   --load_best_model_at_end \\\n   --lora_r {lora_r} \\\n   --lora_alpha {lora_alpha} \\\n   --lora_dropout {lora_dropout} \\\n   --data_replay_freq 1 \\\n   --kl_ratio {kl_ratio} \\\n   --attn_temperature {attn_temperature}\n\nrm -rf logs_and_outputs/{run_name}/outputs/{idx+2}-{dataset_list[idx+1]}/checkpoint*\n   \nsleep 5\n'''\n\nsh_str+=rf'''\npython score.py {run_name} single_train_results_path\n'''\n    \nwith open(f'{run_name}.sh', 'w') as f:\n    f.write(sh_str)"}
{"type": "source_file", "path": "gen_script_superni_llama.py", "content": "import json\ndef load_json(path):\n    with open(path, 'r', encoding='utf-8') as f:\n        data = json.load(f)\n    return data\n\ndef write_json(path, data):\n    with open(path, 'w', encoding='utf-8') as f:\n        json.dump(data, f, ensure_ascii=False,indent=2)\n\ndef load_jsonline(path):\n    with open(path, 'r', encoding='utf-8') as f:\n        result=[]\n        for line_s in f:\n            line=json.loads(line_s)\n            result.append(line)\n    return result\n\ndef write_jsonline(path, data):\n    with open(path, 'w', encoding='utf-8') as f:\n        for line in data:\n            line_s=json.dumps(line, ensure_ascii=False)\n            f.write(line_s)\n            f.write('\\n')\n\norder_idx = 1\n\nif order_idx == 1:\n    all_tasks = [\"task1572_samsum_summary\",\n                 \"task363_sst2_polarity_classification\",\n                 \"task1290_xsum_summarization\",\n                 \"task181_outcome_extraction\",\n                 \"task002_quoref_answer_generation\",\n                 \"task1510_evalution_relation_extraction\",\n                 \"task639_multi_woz_user_utterance_generation\",\n                 \"task1729_personachat_generate_next\",\n                 \"task073_commonsenseqa_answer_generation\",\n                 \"task1590_diplomacy_text_generation\",\n                 \"task748_glucose_reverse_cause_event_detection\",\n                 \"task511_reddit_tifu_long_text_summarization\",\n                 \"task591_sciq_answer_generation\",\n                 \"task1687_sentiment140_classification\",\n                 \"task875_emotion_classification\"] # Order 1\nelse:\n    all_tasks = ['task748_glucose_reverse_cause_event_detection',\n                 'task073_commonsenseqa_answer_generation',\n                 'task1590_diplomacy_text_generation',\n                 'task639_multi_woz_user_utterance_generation',\n                 'task1572_samsum_summary',\n                 'task1687_sentiment140_classification',\n                 'task591_sciq_answer_generation',\n                 'task363_sst2_polarity_classification',\n                 'task1510_evalution_relation_extraction',\n                 'task1729_personachat_generate_next',\n                 'task181_outcome_extraction',\n                 'task511_reddit_tifu_long_text_summarization',\n                 'task002_quoref_answer_generation',\n                 'task1290_xsum_summarization',\n                 'task875_emotion_classification'] # Order 2\n    \ndataset_list = all_tasks\ntask_order = ','.join(all_tasks)\n\nconfig_template={\n    \"SuperNI\": [\n    ],\n}\n\nimport os\nimport pathlib\nimport numpy as np\nfrom copy import deepcopy\n\nlora_r = 4\nlora_alpha = 32\nlora_dropout = 0.\nkl_ratio = 1\nattn_temperature = 1\nlearning_rate = 5e-5\nnum_train_epochs = 50\nattn_lr = 0.\ntrans_hidden_dim = 100\nreplay_after_n_epoch = 0\n\nrun_name = f\"your_job_name\"\n\nhistory_config=[]\nfor one_data_name in dataset_list:\n\n    pathlib.Path(f'./configs/{run_name}_configs/{one_data_name}').mkdir(parents=True, exist_ok=True)\n\n    config={\n        \"sampling strategy\": \"full\",\n        \"dataset name\": f\"{one_data_name}\"\n    } \n    history_config.append(config)\n\n    dev_config=deepcopy(config_template)\n    dev_config['SuperNI'].append(config)\n    write_json(f'./configs/{run_name}_configs/{one_data_name}/dev_tasks.json', dev_config)\n    \n    train_config=deepcopy(config_template)\n    train_config['SuperNI'].append(config)\n    write_json(f'./configs/{run_name}_configs/{one_data_name}/train_tasks.json', train_config)\n\n    test_config=deepcopy(config_template)\n    test_config['SuperNI'].extend(history_config)\n    write_json(f'./configs/{run_name}_configs/{one_data_name}/test_tasks.json', test_config)\n\nsh_str=rf'''#!/bin/bash\n#SBATCH -J cl                           \n#SBATCH -o cl-%j.out                       \n#SBATCH -p compute \n#SBATCH -N 1                           \n#SBATCH -t 20:00:00   \n#SBATCH --mem 128G \n#SBATCH --gres=gpu:a100-sxm4-80gb:1  \n\nexport CUDA_DEVICE_ORDER=\"PCI_BUS_ID\"\n\nport=$(shuf -i25000-30000 -n1)  \n\ndeepspeed --num_gpus=4 src/run_llama.py \\\n   --do_train \\\n   --do_predict \\\n   --predict_with_generate \\\n   --model_name_or_path your_llama_model_path \\\n   --data_dir CL_Benchmark \\\n   --task_order {task_order} \\\n   --task_config_dir configs/{run_name}_configs/{dataset_list[0]} \\\n   --output_dir logs_and_outputs/{run_name}/outputs/1-{dataset_list[0]} \\\n   --per_device_train_batch_size 2 \\\n   --per_device_eval_batch_size 32 \\\n   --gradient_accumulation_steps 4 \\\n   --learning_rate {learning_rate} \\\n   --attn_lr {attn_lr} \\\n   --num_train_epochs {num_train_epochs} \\\n   --bf16 \\\n   --deepspeed configs/ds_configs/stage2.config \\\n   --run_name {run_name} \\\n   --max_source_length 1024 \\\n   --max_target_length 50 \\\n   --generation_max_length 50 \\\n   --add_task_name False \\\n   --add_dataset_name False \\\n   --overwrite_output_dir \\\n   --overwrite_cache \\\n   --lr_scheduler_type constant \\\n   --warmup_steps 0 \\\n   --logging_strategy steps \\\n   --logging_steps 10 \\\n   --metric_for_best_model eval_rougeL \\\n   --evaluation_strategy steps \\\n   --save_strategy steps \\\n   --save_total_limit 1 \\\n   --lora_r {lora_r} \\\n   --lora_alpha {lora_alpha} \\\n   --lora_dropout {lora_dropout} \\\n   --load_best_model_at_end \\\n   --data_replay_freq -1 \\\n   --replay_after_n_epoch 0 \\\n   --kl_ratio {kl_ratio} \\\n   --attn_temperature {attn_temperature} \\\n   --trans_hidden_dim {trans_hidden_dim} \\\n\nrm -rf logs_and_outputs/{run_name}/outputs/1-{dataset_list[0]}/checkpoint*\n   \nsleep 5\n'''\n\nprevious_lora_path_list = []\nfor idx in range(len(dataset_list)-1):\n\n    previous_lora_path_list.append(f\"logs_and_outputs/{run_name}/outputs/{idx+1}-{dataset_list[idx]}/saved_weights\")\n    previous_lora_path = ','.join(previous_lora_path_list)\n    sh_str+=rf'''\n\ndeepspeed --num_gpus=4 src/run_llama.py \\\n   --do_train \\\n   --do_predict \\\n   --predict_with_generate \\\n   --model_name_or_path your_llama_model_path \\\n   --load_checkpoint_from logs_and_outputs/{run_name}/outputs/{idx+1}-{dataset_list[idx]}/saved_weights/trans_input.pt \\\n   --previous_lora_path {previous_lora_path} \\\n   --previous_prompt_key_path logs_and_outputs/{run_name}/outputs/{idx+1}-{dataset_list[idx]}/saved_weights/prompts_keys_till_now.pt \\\n   --data_dir CL_Benchmark \\\n   --task_order {task_order} \\\n   --gen_data_dir generated_data/lora_gen_superni_llama \\\n   --task_config_dir configs/{run_name}_configs/{dataset_list[idx+1]} \\\n   --output_dir logs_and_outputs/{run_name}/outputs/{idx+2}-{dataset_list[idx+1]} \\\n   --per_device_train_batch_size 2 \\\n   --per_device_eval_batch_size 32 \\\n   --gradient_accumulation_steps 4 \\\n   --learning_rate {learning_rate} \\\n   --attn_lr {attn_lr} \\\n   --num_train_epochs {num_train_epochs} \\\n   --bf16 \\\n   --deepspeed configs/ds_configs/stage2.config \\\n   --run_name {run_name} \\\n   --max_source_length 1024 \\\n   --max_target_length 50 \\\n   --generation_max_length 50 \\\n   --add_task_name False \\\n   --add_dataset_name False \\\n   --overwrite_output_dir \\\n   --overwrite_cache \\\n   --lr_scheduler_type constant \\\n   --warmup_steps 0 \\\n   --logging_strategy steps \\\n   --logging_steps 10 \\\n   --metric_for_best_model eval_rougeL_for_{dataset_list[idx+1]} \\\n   --evaluation_strategy steps \\\n   --save_strategy steps \\\n   --save_total_limit 1 \\\n   --load_best_model_at_end \\\n   --lora_r {lora_r} \\\n   --lora_alpha {lora_alpha} \\\n   --lora_dropout {lora_dropout} \\\n   --add_instruction_replay \\\n   --data_replay_freq 1 \\\n   --replay_after_n_epoch {replay_after_n_epoch} \\\n   --kl_ratio {kl_ratio} \\\n   --attn_temperature {attn_temperature} \\\n   --trans_hidden_dim {trans_hidden_dim} \\\n\nrm -rf logs_and_outputs/{run_name}/outputs/{idx+2}-{dataset_list[idx+1]}/checkpoint*\n\nsleep 5\n'''\n    \nsh_str+=rf'''\npython score.py {run_name} single_train_results_path\n'''\n    \nwith open(f'{run_name}.sh', 'w') as f:\n    f.write(sh_str)"}
{"type": "source_file", "path": "gen_script_superni_t5.py", "content": "import json\ndef load_json(path):\n    with open(path, 'r', encoding='utf-8') as f:\n        data = json.load(f)\n    return data\n\ndef write_json(path, data):\n    with open(path, 'w', encoding='utf-8') as f:\n        json.dump(data, f, ensure_ascii=False,indent=2)\n\ndef load_jsonline(path):\n    with open(path, 'r', encoding='utf-8') as f:\n        result=[]\n        for line_s in f:\n            line=json.loads(line_s)\n            result.append(line)\n    return result\n\ndef write_jsonline(path, data):\n    with open(path, 'w', encoding='utf-8') as f:\n        for line in data:\n            line_s=json.dumps(line, ensure_ascii=False)\n            f.write(line_s)\n            f.write('\\n')\n\norder_idx = 1\n\nif order_idx == 1:\n    all_tasks = [\"task1572_samsum_summary\",\n                 \"task363_sst2_polarity_classification\",\n                 \"task1290_xsum_summarization\",\n                 \"task181_outcome_extraction\",\n                 \"task002_quoref_answer_generation\",\n                 \"task1510_evalution_relation_extraction\",\n                 \"task639_multi_woz_user_utterance_generation\",\n                 \"task1729_personachat_generate_next\",\n                 \"task073_commonsenseqa_answer_generation\",\n                 \"task1590_diplomacy_text_generation\",\n                 \"task748_glucose_reverse_cause_event_detection\",\n                 \"task511_reddit_tifu_long_text_summarization\",\n                 \"task591_sciq_answer_generation\",\n                 \"task1687_sentiment140_classification\",\n                 \"task875_emotion_classification\"] # Order 1\nelse:\n    all_tasks = ['task748_glucose_reverse_cause_event_detection',\n                 'task073_commonsenseqa_answer_generation',\n                 'task1590_diplomacy_text_generation',\n                 'task639_multi_woz_user_utterance_generation',\n                 'task1572_samsum_summary',\n                 'task1687_sentiment140_classification',\n                 'task591_sciq_answer_generation',\n                 'task363_sst2_polarity_classification',\n                 'task1510_evalution_relation_extraction',\n                 'task1729_personachat_generate_next',\n                 'task181_outcome_extraction',\n                 'task511_reddit_tifu_long_text_summarization',\n                 'task002_quoref_answer_generation',\n                 'task1290_xsum_summarization',\n                 'task875_emotion_classification'] # Order 2\n\ndataset_list = all_tasks\ntask_order = ','.join(all_tasks)\n\nconfig_template={\n    \"SuperNI\": [\n    ],\n}\n\nimport os\nimport pathlib\nimport numpy as np\nfrom copy import deepcopy\n\nrun_name = f\"your_job_name\"\nlora_r = 4\nlora_alpha = 32\nlora_dropout = 0.\nkl_ratio = 0.5\nattn_temperature = 1\nlearning_rate = 3e-4\n\nhistory_config=[]\nfor one_data_name in dataset_list:\n\n    pathlib.Path(f'./configs/{run_name}_configs/{one_data_name}').mkdir(parents=True, exist_ok=True)\n\n    config={\n        \"sampling strategy\": \"full\",\n        \"dataset name\": f\"{one_data_name}\"\n    } \n    history_config.append(config)\n\n    dev_config=deepcopy(config_template)\n    dev_config['SuperNI'].append(config)\n    write_json(f'./configs/{run_name}_configs/{one_data_name}/dev_tasks.json', dev_config)\n    \n    train_config=deepcopy(config_template)\n    train_config['SuperNI'].append(config)\n    write_json(f'./configs/{run_name}_configs/{one_data_name}/train_tasks.json', train_config)\n\n    test_config=deepcopy(config_template)\n    test_config['SuperNI'].extend(history_config)\n    write_json(f'./configs/{run_name}_configs/{one_data_name}/test_tasks.json', test_config)\n\nsh_str=rf'''#!/bin/bash\n#SBATCH -J cl                           \n#SBATCH -o cl-%j.out                       \n#SBATCH -p compute \n#SBATCH -N 1                           \n#SBATCH -t 20:00:00   \n#SBATCH --mem 128G \n#SBATCH --gres=gpu:a100-sxm4-80gb:1  \n\nexport CUDA_DEVICE_ORDER=\"PCI_BUS_ID\"\n\nport=$(shuf -i25000-30000 -n1)  \n\nCUDA_VISIBLE_DEVICES=0 python src/run_t5.py \\\n   --do_train \\\n   --do_predict \\\n   --predict_with_generate \\\n   --model_name_or_path your_t5_model_path \\\n   --data_dir CL_Benchmark \\\n   --task_order {task_order} \\\n   --task_config_dir configs/{run_name}_configs/{dataset_list[0]} \\\n   --output_dir logs_and_outputs/{run_name}/outputs/1-{dataset_list[0]} \\\n   --per_device_train_batch_size 8 \\\n   --per_device_eval_batch_size 128 \\\n   --gradient_accumulation_steps 4 \\\n   --learning_rate {learning_rate} \\\n   --num_train_epochs 100 \\\n   --bf16 \\\n   --run_name {run_name} \\\n   --max_source_length 512 \\\n   --max_target_length 50 \\\n   --generation_max_length 50 \\\n   --add_task_name False \\\n   --add_dataset_name False \\\n   --overwrite_output_dir \\\n   --overwrite_cache \\\n   --lr_scheduler_type constant \\\n   --warmup_steps 0 \\\n   --logging_strategy steps \\\n   --logging_steps 10 \\\n   --metric_for_best_model eval_rougeL \\\n   --evaluation_strategy steps \\\n   --save_strategy steps \\\n   --save_total_limit 1 \\\n   --lora_r {lora_r} \\\n   --lora_alpha {lora_alpha} \\\n   --lora_dropout {lora_dropout} \\\n   --load_best_model_at_end \\\n   --data_replay_freq -1 \\\n   --replay_after_n_epoch 0 \\\n   --kl_ratio {kl_ratio} \\\n   --attn_temperature {attn_temperature}\n'''\n\nprevious_lora_path_list = []\nfor idx in range(len(dataset_list)-1):\n\n    previous_lora_path_list.append(f\"logs_and_outputs/{run_name}/outputs/{idx+1}-{dataset_list[idx]}/saved_weights\")\n    previous_lora_path = ','.join(previous_lora_path_list)\n    sh_str+=rf'''\n\nCUDA_VISIBLE_DEVICES=0 python src/run_t5.py \\\n   --do_train \\\n   --do_predict \\\n   --predict_with_generate \\\n   --model_name_or_path your_t5_model_path \\\n   --load_checkpoint_from logs_and_outputs/{run_name}/outputs/{idx+1}-{dataset_list[idx]}/saved_weights/trans_input.pt \\\n   --previous_lora_path {previous_lora_path} \\\n   --previous_prompt_key_path logs_and_outputs/{run_name}/outputs/{idx+1}-{dataset_list[idx]}/saved_weights/prompts_keys_till_now.pt \\\n   --data_dir CL_Benchmark \\\n   --task_order {task_order} \\\n   --gen_data_dir generated_data/lora_gen_superni_t5 \\\n   --task_config_dir configs/{run_name}_configs/{dataset_list[idx+1]} \\\n   --output_dir logs_and_outputs/{run_name}/outputs/{idx+2}-{dataset_list[idx+1]} \\\n   --per_device_train_batch_size 8 \\\n   --per_device_eval_batch_size 128 \\\n   --gradient_accumulation_steps 4 \\\n   --learning_rate {learning_rate} \\\n   --num_train_epochs 100 \\\n   --bf16 \\\n   --run_name {run_name} \\\n   --max_source_length 512 \\\n   --max_target_length 50 \\\n   --generation_max_length 50 \\\n   --add_task_name False \\\n   --add_dataset_name False \\\n   --overwrite_output_dir \\\n   --overwrite_cache \\\n   --lr_scheduler_type constant \\\n   --warmup_steps 0 \\\n   --logging_strategy steps \\\n   --logging_steps 10 \\\n   --metric_for_best_model eval_rougeL_for_{dataset_list[idx+1]} \\\n   --evaluation_strategy steps \\\n   --save_strategy steps \\\n   --save_total_limit 1 \\\n   --load_best_model_at_end \\\n   --lora_r {lora_r} \\\n   --lora_alpha {lora_alpha} \\\n   --lora_dropout {lora_dropout} \\\n   --add_instruction_replay \\\n   --data_replay_freq 1 \\\n   --replay_after_n_epoch 0 \\\n   --kl_ratio {kl_ratio} \\\n   --attn_temperature {attn_temperature}\n'''\n    \nsh_str+=rf'''\npython score.py {run_name} single_train_results_path\n'''\n    \nwith open(f'{run_name}.sh', 'w') as f:\n    f.write(sh_str)\n"}
{"type": "source_file", "path": "pseudo_data/CL_Benchmark/remove_oup.py", "content": "from glob import glob \nimport json\nimport random\nimport pathlib\ndef load_json(path):\n    with open(path, 'r', encoding='utf-8') as f:\n        data = json.load(f)\n    return data\n\ndef write_json(path, data):\n    with open(path, 'w', encoding='utf-8') as f:\n        json.dump(data, f, ensure_ascii=False,indent=2)\n\nfor path in glob('./Ours_CL/*'):\n    for split in ['train', 'test']:\n        data=load_json(f'{path}/{split}.json')\n        if '__ans__' in data['Positive Examples'][0]['output']:\n            for one in data['Instances']:\n                one['output']=one['output'].split('__ans__')[0]\n            for one in data['Positive Examples']:\n                one['output']=one['output'].split('__ans__')[0]\n            for one in data['Negative Examples']:\n                one['output']=one['output'].split('__ans__')[0]\n        write_json(f'{path}/{split}.json',data)\n"}
{"type": "source_file", "path": "pseudo_data/after_inference_transfer_lookback_format.py", "content": "import json\nimport random\nimport pathlib\ndef load_json(path):\n    with open(path, 'r', encoding='utf-8') as f:\n        data = json.load(f)\n    return data\n\ndef write_json(path, data):\n    with open(path, 'w', encoding='utf-8') as f:\n        json.dump(data, f, ensure_ascii=False,indent=2)\n\ndef load_jsonline(path):\n    with open(path, 'r', encoding='utf-8') as f:\n        result=[]\n        for line_s in f:\n            line=json.loads(line_s)\n            result.append(line)\n    return result\n\ndef write_jsonline(path, data):\n    with open(path, 'w', encoding='utf-8') as f:\n        for line in data:\n            line_s=json.dumps(line, ensure_ascii=False)\n            f.write(line_s)\n            f.write('\\n')\n\n\ndataset_list=[\n    # \"task181_outcome_extraction\",\n    # \"task591_sciq_answer_generation\",\n    # \"task1729_personachat_generate_next\",\n    # \"task1510_evalution_relation_extraction\",\n    # \"task748_glucose_reverse_cause_event_detection\",\n    # \"task002_quoref_answer_generation\",\n    # \"task1687_sentiment140_classification\",\n    # \"task511_reddit_tifu_long_text_summarization\",\n    # \"task875_emotion_classification\",\n\n    # \"task1590_diplomacy_text_generation\",\n    # \"task1572_samsum_summary\",\n    # \"task639_multi_woz_user_utterance_generation\",\n    # \"task1290_xsum_summarization\",\n\n    \"task073_commonsenseqa_answer_generation\",\n    \"task363_sst2_polarity_classification\",\n]\n\n\nfor one_name in dataset_list:\n    print(one_name)\n    origial_data=load_json(f'./CL_Benchmark/Ours_CL/lookback_{one_name}/train.json')   \n    origial_len=len(origial_data['Instances'])\n    ckpt_name='lr_0001_topk_20'\n    lookback_data=load_json(f'./logs_and_outputs/Ours_CL/{ckpt_name}/lookback_{one_name}/lookback_inference_result.json')\n    instruction=set()\n    instences=[]\n    positive=[]\n    for one_item in lookback_data['Instances']:\n        text=one_item['input']\n        if '__inp__' in text and '__ans__' in text:\n            temp=text.split('__inp__')\n            one_instru=temp[0].strip()\n            if '__ans__' in one_instru:\n                continue\n            one_input,one_output=temp[1].split('__ans__')\n            instruction.add(one_instru)\n            instences.append({\n                'input':one_input.strip(),\n                'output':one_output.strip(),\n            })\n            positive.append({\n                'input':one_input.strip(),\n                'output':one_output.strip(),\n            })\n        elif '__inp__' in text:\n            temp=text.split('__inp__')\n            one_instru=temp[0].strip()\n            one_input=temp[1].strip()\n            instruction.add(one_instru)\n            instences.append({\n                'input':one_input,\n                'output':'',\n            })\n        else:\n            continue\n\n    if one_name in ['task1590_diplomacy_text_generation', 'task1572_samsum_summary']:\n        for one_item in lookback_data['Instances']:\n            original_instru_len=len(origial_data['Definition'][0])\n            text=one_item['input']\n            if '__inp__' not in text and '__ans__' not in text:\n                one_instru=text[:original_instru_len]\n                one_input=text[original_instru_len:]\n                instruction.add(one_instru)\n                instences.append({\n                    'input':one_input,\n                    'output':'',\n                })\n                positive.append({\n                    'input':one_input,\n                    'output':'',\n                })\n            \n    if len(positive)>3:\n        positive=random.sample(positive, 3)\n    origial_data['Definition']=list(instruction)\n    origial_data['Positive Examples']=positive\n    origial_data['Negative Examples']=[]\n    origial_data['Instances']=instences\n\n    print(f'{int(len(instences)*100/origial_len)}%')\n    print(len(instences))\n    output_root_path=f'./generate_lookback/generate_instruction_input_{ckpt_name.replace(\"outputs_\", \"\")}'\n    pathlib.Path(f'{output_root_path}/{one_name}/').mkdir(parents=True, exist_ok=True)\n    write_json(f'{output_root_path}/{one_name}/train.json', origial_data)  \n    write_json(f'{output_root_path}/{one_name}/test.json', origial_data)  \n    write_json(f'{output_root_path}/{one_name}/dev.json', origial_data)  \n    \n\n\n    output_root_path=f'./generate_lookback/generate_instruction_input_instances_0.02_{ckpt_name.replace(\"outputs_\", \"\")}'\n    pathlib.Path(f'{output_root_path}/{one_name}/').mkdir(parents=True, exist_ok=True)\n    origial_data['Definition']=random.sample(origial_data['Definition'], 1)\n    target_len=int(origial_len*0.02)\n    if target_len==0:\n        target_len=1\n    if len(origial_data['Instances'])>target_len:\n        origial_data['Instances']=random.sample(origial_data['Instances'], target_len)\n    write_json(f'{output_root_path}/{one_name}/train.json', origial_data)  \n    write_json(f'{output_root_path}/{one_name}/test.json', origial_data)  \n    write_json(f'{output_root_path}/{one_name}/dev.json', origial_data)  \n\n"}
{"type": "source_file", "path": "pseudo_data/sample_instances.py", "content": "import json\nimport random\nimport pathlib\nimport os\ndef load_json(path):\n    with open(path, 'r', encoding='utf-8') as f:\n        data = json.load(f)\n    return data\n\ndef write_json(path, data):\n    with open(path, 'w', encoding='utf-8') as f:\n        json.dump(data, f, ensure_ascii=False,indent=2)\n\ndef load_jsonline(path):\n    with open(path, 'r', encoding='utf-8') as f:\n        result=[]\n        for line_s in f:\n            line=json.loads(line_s)\n            result.append(line)\n    return result\n\ndef write_jsonline(path, data):\n    with open(path, 'w', encoding='utf-8') as f:\n        for line in data:\n            line_s=json.dumps(line, ensure_ascii=False)\n            f.write(line_s)\n            f.write('\\n')\n\n\ndataset_list=[\n    \"task181_outcome_extraction\",\n    \"task591_sciq_answer_generation\",\n    \"task1729_personachat_generate_next\",\n    \"task1510_evalution_relation_extraction\",\n    \"task748_glucose_reverse_cause_event_detection\",\n    \"task002_quoref_answer_generation\",\n    \"task1687_sentiment140_classification\",\n    \"task511_reddit_tifu_long_text_summarization\",\n    \"task875_emotion_classification\",\n\n    \"task1590_diplomacy_text_generation\",\n    \"task1572_samsum_summary\",\n    \"task639_multi_woz_user_utterance_generation\",\n    \"task1290_xsum_summarization\",\n\n    \"task073_commonsenseqa_answer_generation\",\n    \"task363_sst2_polarity_classification\",\n]\n# model='t5_xl'\nmodel='t5_large'\n\nfor one_name in dataset_list:\n    ckpt_name='lr_0001_topk_20'\n\n\n    for rate in [0.05, 0.1, 0.5, 1]:\n        origial_data=load_json(f'./generate_lookback/{model}_generate_instruction_input_lr_0001_topk_20/{one_name}/train.json')\n        origial_len=len(origial_data['Instances'])\n\n        output_root_path=f'./generate_lookback/{model}_generate_instruction_input_instances_{rate}_{ckpt_name}'\n        pathlib.Path(f'{output_root_path}/{one_name}/').mkdir(parents=True, exist_ok=True)\n        origial_data['Definition']=random.sample(origial_data['Definition'], 1)\n        target_len=int(origial_len*rate)\n        if target_len==0:\n            target_len=1\n        if len(origial_data['Instances'])>target_len:\n            origial_data['Instances']=random.sample(origial_data['Instances'], target_len)\n        write_json(f'{output_root_path}/{one_name}/train.json', origial_data)  \n        write_json(f'{output_root_path}/{one_name}/test.json', origial_data)  \n        write_json(f'{output_root_path}/{one_name}/dev.json', origial_data)  \n\n"}
{"type": "source_file", "path": "pseudo_data/src/__init__.py", "content": ""}
{"type": "source_file", "path": "pseudo_data/src/compute_metrics.py", "content": "import string\nimport json\nimport os\nimport argparse\nimport logging\n\nfrom rouge import rouge_scorer\nfrom transformers import AutoTokenizer\n\n\nlogger = logging.getLogger(__name__)\nCURRENT_DIR = os.path.dirname(__file__)\nGPT2TOKENIZER = os.path.join(CURRENT_DIR, \"../data/gpt2tokenizer\")\n\n\nclass GPTTokenizer:\n    gpt_tokenizer = AutoTokenizer.from_pretrained(GPT2TOKENIZER, max_length=1e5)\n\n    def tokenize(self, s):\n        tokens = self.gpt_tokenizer.tokenize(s)\n        # GPT2 uses Byte-level BPE, which will include space as part of the word. \n        # But for the first word of a sentence, there is no space before it. \n        # So, we remove all the added spaces (\"Ġ\"). \n        tokens = [t.lstrip(\"Ġ\") for t in tokens]\n        return tokens\n\n\nxlingual_tokenizer = GPTTokenizer()\n\n\n# adapted the flowing from Squad v1.1 evaluation, without removing the articles.\ndef normalize_answer(s):\n    \"\"\"Lower text and remove punctuation, and extra whitespace.\"\"\"\n\n    def white_space_fix(text):\n        return ' '.join(text.split())\n\n    def remove_punc(text):\n        exclude = set(string.punctuation)\n        return ''.join(ch for ch in text if ch not in exclude)\n\n    def lower(text):\n        return text.lower()\n\n    return white_space_fix(remove_punc(lower(s)))\n\n\ndef exact_match_score(prediction, ground_truth, xlingual=False):\n    return (normalize_answer(prediction) == normalize_answer(ground_truth))\n\n\ndef rouge1_score(prediction, ground_truth, xlingual=False):\n    if xlingual:\n        scorer = rouge_scorer.RougeScorer(['rouge1'], tokenizer=xlingual_tokenizer)\n    else:\n        scorer = rouge_scorer.RougeScorer(['rouge1'], use_stemmer=True)\n    scores = scorer.score(prediction=prediction, target=ground_truth)\n    return scores[\"rouge1\"].fmeasure\n\n\ndef rougeL_score(prediction, ground_truth, xlingual=False):\n    if xlingual:\n        scorer = rouge_scorer.RougeScorer(['rougeL'], tokenizer=xlingual_tokenizer) \n    else:\n        scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n    scores = scorer.score(prediction=prediction, target=ground_truth)\n    return scores[\"rougeL\"].fmeasure\n\n\ndef metric_max_over_ground_truths(metric_fn, prediction, ground_truths, xlingual=False):\n    scores_for_ground_truths = []\n    for ground_truth in ground_truths:\n        score = metric_fn(prediction, ground_truth, xlingual=xlingual)\n        scores_for_ground_truths.append(score)\n    return max(scores_for_ground_truths)\n\n\ndef compute_metrics(predictions, references, xlingual=False):\n    assert len(predictions) == len(references), f\"# of predictions {len(predictions)} doesn't match # of references {len(references)}.\"\n    exact_match, rouge1, rougeL = 0, 0, 0\n    for pred, gold in zip(predictions, references):\n        gold = [gold]\n        exact_match += metric_max_over_ground_truths(\n            exact_match_score, prediction=pred, ground_truths=gold, xlingual=xlingual\n        )\n        rouge1 += metric_max_over_ground_truths(\n            rouge1_score, prediction=pred, ground_truths=gold, xlingual=xlingual\n        )\n        rougeL += metric_max_over_ground_truths(\n            rougeL_score, prediction=pred, ground_truths=gold, xlingual=xlingual\n        )\n    exact_match = 100.0 * exact_match / len(references)\n    rouge1 = 100.0 * rouge1 / len(references)\n    rougeL = 100.0 * rougeL / len(references)\n    metrics = {\"exact_match\": exact_match, \"rouge1\": rouge1, \"rougeL\": rougeL}\n    metrics = {k: round(v, 4) for k, v in metrics.items()}\n    return metrics\n\n\ndef compute_grouped_metrics(predictions, references, groups, xlingual=False):\n    assert len(predictions) == len(references) == len(groups)\n\n    examples_by_group = {}\n    for pred, gold, group in zip(predictions, references, groups):\n        if group not in examples_by_group:\n            examples_by_group[group] = []\n        examples_by_group[group].append((pred, gold))\n    \n    results = {}\n    for group, group_examples in examples_by_group.items():\n        task_predictions, task_references = zip(*group_examples)\n        group_metrics = compute_metrics(task_predictions, task_references, xlingual=xlingual)\n        for metric, value in group_metrics.items():\n            results[f\"{metric}_for_{group}\"] = value\n    return results\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--predictions\", required=True, help=\"Path to predictions file.\")\n    parser.add_argument(\"--track\", choices=[\"default\", \"xlingual\"], default=\"default\", \n        help=\"default track or xlingual track. For xlingual, we need to use a different tokenizer.\"\n    )\n    parser.add_argument(\"--compute_per_category_metrics\", action=\"store_true\", help=\"Compute metrics on every evaluation category.\")\n    parser.add_argument(\"--compute_per_task_metrics\", action=\"store_true\", help=\"Compute metrics on every evaluation task.\")\n    return parser.parse_args()\n\n\nif __name__ == \"__main__\":\n    args = parse_args()\n    with open(args.predictions) as fin:\n        examples = [json.loads(l) for l in fin]\n\n    predictions = [e[\"prediction\"] for e in examples]\n    references = [e[\"instance\"][\"output\"] for e in examples]\n    tasks = []\n    for e in examples:\n        if e[\"task\"] == \"task121_atomic_question_rewriting\":\n            e[\"task\"] = \"task121_zest_question_rewriting\"\n        tasks.append(e[\"Task\"])\n\n    results = compute_metrics(predictions, references, xlingual=args.track == \"xlingual\")\n    print(\"======== Overall Metrics ========\")\n    print(\"all_rougeL\", results[\"rougeL\"])\n    print(\"all_EM\", results[\"exact_match\"])\n    print()\n    \n    category_metrics = [\n        (\"Textual Entailment\", \"exact_match\"),\n        (\"Cause Effect Classification\", \"exact_match\"),\n        (\"Coreference Resolution\", \"exact_match\"),\n        (\"Dialogue Act Recognition\", \"exact_match\"),\n        (\"Answerability Classification\", \"exact_match\"),\n        (\"Word Analogy\", \"exact_match\"),\n        (\"Overlap Extraction\", \"rougeL\"),\n        (\"Keyword Tagging\", \"rougeL\"),\n        (\"Question Rewriting\", \"rougeL\"),\n        (\"Title Generation\", \"rougeL\"),\n        (\"Data to Text\", \"rougeL\"),\n        (\"Grammar Error Correction\", \"rougeL\"),\n    ]\n    category_metrics = {\"_\".join(category.lower().split()): metric for category, metric in category_metrics}\n\n    if args.compute_per_category_metrics:\n        print(\"======== Metrics per category ========\")\n        task_category = {}\n        for task in set(tasks):\n            with open(os.path.join(\"./data/tasks/\", task+\".json\")) as fin:\n                task_data = json.load(fin)\n                task_category[task] = \"_\".join(task_data[\"Categories\"][0].lower().split())\n        categories = [task_category[e[\"Task\"]] for e in examples] \n        results.update(compute_grouped_metrics(predictions, references, categories, xlingual=args.track==\"xlingual\"))\n        \n        for category, metric in category_metrics.items():\n            # category = \"_\".join(category.lower().split())\n            if f\"{metric}_for_{category}\" in results:\n                print(f\"{metric}_for_{category}\", results[f\"{metric}_for_{category}\"])\n        print()\n            \n    if args.compute_per_task_metrics:\n        print(\"======== Metrics per task ========\")\n        results_by_task = compute_grouped_metrics(predictions, references, tasks, xlingual=args.track==\"xlingual\")\n        for task in sorted(list(set(tasks))):\n            category = task_category[task]\n            metric = category_metrics[category]\n            print(task, results_by_task[f\"{metric}_for_{task}\"])\n        print()"}
{"type": "source_file", "path": "pseudo_data/src/peft_1/__init__.py", "content": "# flake8: noqa\n# There's no way to ignore \"F401 '...' imported but unused\" warnings in this\n# module, but to preserve other warnings. So, don't check this module at all.\n\n# coding=utf-8\n# Copyright 2023-present the HuggingFace Inc. team.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n__version__ = \"0.3.0\"\n\nfrom .mapping import MODEL_TYPE_TO_PEFT_MODEL_MAPPING, PEFT_TYPE_TO_CONFIG_MAPPING, get_peft_config, get_peft_model\nfrom .peft_model import (\n    PeftModel,\n    PeftModelForCausalLM,\n    PeftModelForSeq2SeqLM,\n    PeftModelForSequenceClassification,\n    PeftModelForTokenClassification,\n)\nfrom .tuners import (\n    AdaptionPromptConfig,\n    AdaptionPromptModel,\n    LoraConfig,\n    LoraModel,\n    AdaLoraConfig,\n    AdaLoraModel,\n    PrefixEncoder,\n    PrefixTuningConfig,\n    PromptEmbedding,\n    PromptEncoder,\n    PromptEncoderConfig,\n    PromptEncoderReparameterizationType,\n    PromptTuningConfig,\n    PromptTuningInit,\n)\nfrom .utils import (\n    TRANSFORMERS_MODELS_TO_PREFIX_TUNING_POSTPROCESS_MAPPING,\n    PeftConfig,\n    PeftType,\n    PromptLearningConfig,\n    TaskType,\n    bloom_model_postprocess_past_key_value,\n    get_peft_model_state_dict,\n    prepare_model_for_int8_training,\n    set_peft_model_state_dict,\n    shift_tokens_right,\n)\n"}
{"type": "source_file", "path": "pseudo_data/src/peft_1/tuners/__init__.py", "content": "# flake8: noqa\n# There's no way to ignore \"F401 '...' imported but unused\" warnings in this\n# module, but to preserve other warnings. So, don't check this module at all\n\n# coding=utf-8\n# Copyright 2023-present the HuggingFace Inc. team.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom .adaption_prompt import AdaptionPromptConfig, AdaptionPromptModel\nfrom .lora import LoraConfig, LoraModel\nfrom .adalora import AdaLoraConfig, AdaLoraModel\nfrom .p_tuning import PromptEncoder, PromptEncoderConfig, PromptEncoderReparameterizationType\nfrom .prefix_tuning import PrefixEncoder, PrefixTuningConfig\nfrom .prompt_tuning import PromptEmbedding, PromptTuningConfig, PromptTuningInit\n"}
{"type": "source_file", "path": "pseudo_data/src/peft_1/tuners/adalora.py", "content": "import importlib\nimport re\nimport warnings\nfrom dataclasses import dataclass, field\nfrom typing import Optional\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom transformers.pytorch_utils import Conv1D\n\nfrom ..utils import (\n    TRANSFORMERS_MODELS_TO_ADALORA_TARGET_MODULES_MAPPING,\n    PeftType,\n    _freeze_adapter,\n    _get_submodules,\n    transpose,\n)\nfrom .lora import (\n    LoraConfig,\n    LoraLayer,\n    LoraModel,\n    mark_only_lora_as_trainable,\n)\n\n\ndef is_bnb_available():\n    return importlib.util.find_spec(\"bitsandbytes\") is not None\n\n\nif is_bnb_available():\n    import bitsandbytes as bnb\n\n\n@dataclass\nclass AdaLoraConfig(LoraConfig):\n    \"\"\"\n    This is the configuration class to store the configuration of a [`~peft.AdaLora`].\n\n    Args:\n        target_r (`int`): The target average rank of incremental matrix.\n        init_r (`int`): The initial rank for each incremental matrix.\n        tinit (`int`): The steps of initial fine-tuning warmup.\n        tfinal (`int`): The step of final fine-tuning.\n        deltaT (`int`): The time internval between two budget allocations.\n        beta1 (`float`): The hyperparameter of EMA for sensitivity smoothing.\n        beta2 (`float`): The hyperparameter of EMA for undertainty quantification.\n        orth_reg_weight (`float`): The coefficient of orthogonal regularization.\n        total_step (`int`): The total training steps that should be specified before training.\n        rank_pattern (`list`): The allocated rank for each weight matrix by RankAllocator.\n    \"\"\"\n\n    target_r: int = field(default=8, metadata={\"help\": \"Target Lora matrix dimension.\"})\n    init_r: int = field(default=12, metadata={\"help\": \"Intial Lora matrix dimension.\"})\n    tinit: int = field(default=0, metadata={\"help\": \"The steps of initial warmup.\"})\n    tfinal: int = field(default=0, metadata={\"help\": \"The steps of final warmup.\"})\n    deltaT: int = field(default=1, metadata={\"help\": \"Step interval of rank allocation.\"})\n    beta1: float = field(default=0.85, metadata={\"help\": \"Hyperparameter of EMA.\"})\n    beta2: float = field(default=0.85, metadata={\"help\": \"Hyperparameter of EMA.\"})\n    orth_reg_weight: float = field(default=0.5, metadata={\"help\": \"The orthogonal regularization coefficient.\"})\n    total_step: Optional[int] = field(default=None, metadata={\"help\": \"The total training steps.\"})\n    rank_pattern: Optional[dict] = field(default=None, metadata={\"help\": \"The saved rank pattern.\"})\n\n    def __post_init__(self):\n        self.peft_type = PeftType.ADALORA\n\n\nclass AdaLoraModel(LoraModel):\n    \"\"\"\n    Creates AdaLoRA (Adaptive LoRA) model from a pretrained transformers model. Paper:\n    https://openreview.net/pdf?id=lq62uWRJjiY\n\n    Args:\n        model ([`transformers.PreTrainedModel`]): The model to be adapted.\n        config ([`AdaLoraConfig`]): The configuration of the AdaLora model.\n\n    Returns:\n        `torch.nn.Module`: The AdaLora model.\n\n    Example::\n\n        >>> from transformers import AutoModelForSeq2SeqLM, LoraConfig >>> from peft import AdaLoraModel, AdaLoraConfig\n        >>> config = AdaLoraConfig(\n                peft_type=\"ADALORA\", task_type=\"SEQ_2_SEQ_LM\", r=8, lora_alpha=32, target_modules=[\"q\", \"v\"],\n                lora_dropout=0.01,\n            )\n        >>> model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\") >>> model = AdaLoraModel(config, model)\n\n    **Attributes**:\n        - **model** ([`transformers.PreTrainedModel`]) -- The model to be adapted.\n        - **peft_config** ([`AdaLoraConfig`]): The configuration of the AdaLora model.\n    \"\"\"\n\n    def __init__(self, model, config, adapter_name):\n        nn.Module.__init__(self)\n        self.model = model\n        self.peft_config = config\n        self.add_adapter(adapter_name, self.peft_config[adapter_name])\n\n    def add_adapter(self, adapter_name, config=None):\n        if config is not None:\n            model_config = self.model.config.to_dict() if hasattr(self.model.config, \"to_dict\") else self.model.config\n            config = self._prepare_adalora_config(config, model_config)\n            self.peft_config[adapter_name] = config\n        self._find_and_replace(adapter_name)\n        if len(self.peft_config) > 1 and self.peft_config[adapter_name].bias != \"none\":\n            raise ValueError(\n                \"AdaLoraModel supports only 1 adapter with bias. When using multiple adapters, set bias to 'none' for all adapters.\"\n            )\n        traininable_mode_counter = 0\n        for config in self.peft_config.values():\n            if not config.inference_mode:\n                traininable_mode_counter += 1\n\n        if traininable_mode_counter > 1:\n            raise ValueError(\n                \"AdaLoraModel supports only 1 trainable adapter. \"\n                \"When using multiple adapters, set inference_mode to True for all adapters except the one you want to train.\"\n            )\n\n        mark_only_lora_as_trainable(self.model, self.peft_config[adapter_name].bias)\n        if self.peft_config[adapter_name].inference_mode:\n            _freeze_adapter(self.model, adapter_name)\n        else:\n            self.trainable_adapter_name = adapter_name\n            self.rankallocator = RankAllocator(self.model, self.peft_config[adapter_name], self.trainable_adapter_name)\n\n    def _find_and_replace(self, adapter_name):\n        lora_config = self.peft_config[adapter_name]\n        loaded_in_8bit = getattr(self.model, \"is_loaded_in_8bit\", False)\n        if loaded_in_8bit and not is_bnb_available():\n            raise ImportError(\n                \"To use Lora with 8-bit quantization, please install the `bitsandbytes` package. \"\n                \"You can install it with `pip install bitsandbytes`.\"\n            )\n        is_target_modules_in_base_model = False\n        kwargs = {\n            \"r\": lora_config.init_r,\n            \"lora_alpha\": lora_config.lora_alpha,\n            \"lora_dropout\": lora_config.lora_dropout,\n            \"fan_in_fan_out\": lora_config.fan_in_fan_out,\n            \"init_lora_weights\": lora_config.init_lora_weights,\n        }\n        key_list = [key for key, _ in self.model.named_modules()]\n        for key in key_list:\n            if isinstance(lora_config.target_modules, str):\n                target_module_found = re.fullmatch(lora_config.target_modules, key)\n            else:\n                target_module_found = any(key.endswith(target_key) for target_key in lora_config.target_modules)\n            if target_module_found:\n                if not is_target_modules_in_base_model:\n                    is_target_modules_in_base_model = True\n                parent, target, target_name = _get_submodules(self.model, key)\n                bias = target.bias is not None\n                if isinstance(target, LoraLayer):\n                    target.update_layer(\n                        adapter_name,\n                        lora_config.init_r,\n                        lora_config.lora_alpha,\n                        lora_config.lora_dropout,\n                        lora_config.init_lora_weights,\n                    )\n                else:\n                    if loaded_in_8bit and isinstance(target, bnb.nn.Linear8bitLt):\n                        kwargs.update(\n                            {\n                                \"has_fp16_weights\": target.state.has_fp16_weights,\n                                \"memory_efficient_backward\": target.state.memory_efficient_backward,\n                                \"threshold\": target.state.threshold,\n                                \"index\": target.index,\n                            }\n                        )\n                        new_module = SVDLinear8bitLt(\n                            adapter_name, target.in_features, target.out_features, bias=bias, **kwargs\n                        )\n                    else:\n                        if isinstance(target, torch.nn.Linear):\n                            in_features, out_features = target.in_features, target.out_features\n                            if kwargs[\"fan_in_fan_out\"]:\n                                warnings.warn(\n                                    \"fan_in_fan_out is set to True but the target module is `torch.nn.Linear`. \"\n                                    \"Setting fan_in_fan_out to False.\"\n                                )\n                                kwargs[\"fan_in_fan_out\"] = lora_config.fan_in_fan_out = False\n                        elif isinstance(target, Conv1D):\n                            in_features, out_features = (\n                                target.weight.ds_shape if hasattr(target.weight, \"ds_shape\") else target.weight.shape\n                            )\n                            if not kwargs[\"fan_in_fan_out\"]:\n                                warnings.warn(\n                                    \"fan_in_fan_out is set to False but the target module is `Conv1D`. \"\n                                    \"Setting fan_in_fan_out to True.\"\n                                )\n                                kwargs[\"fan_in_fan_out\"] = lora_config.fan_in_fan_out = True\n                        else:\n                            raise ValueError(\n                                f\"Target module {target} is not supported. \"\n                                f\"Currently, only `torch.nn.Linear` and `Conv1D` are supported.\"\n                            )\n                        new_module = SVDLinear(adapter_name, in_features, out_features, bias=bias, **kwargs)\n\n                    self._replace_module(parent, target_name, new_module, target)\n        if not is_target_modules_in_base_model:\n            raise ValueError(\n                f\"Target modules {lora_config.target_modules} not found in the base model. \"\n                f\"Please check the target modules and try again.\"\n            )\n\n    def __getattr__(self, name: str):\n        \"\"\"Forward missing attributes to the wrapped module.\"\"\"\n        try:\n            return super().__getattr__(name)  # defer to nn.Module's logic\n        except AttributeError:\n            return getattr(self.model, name)\n\n    def forward(self, *args, **kwargs):\n        outputs = self.model.forward(*args, **kwargs)\n\n        # Calculate the orthogonal regularization\n        orth_reg_weight = self.peft_config[self.trainable_adapter_name].orth_reg_weight\n        assert orth_reg_weight > 0\n\n        if hasattr(outputs, \"loss\"):\n            regu_loss = 0\n            num_param = 0\n            for n, p in self.model.named_parameters():\n                if (\"lora_A\" in n or \"lora_B\" in n) and self.trainable_adapter_name in n:\n                    para_cov = p @ p.T if \"lora_A\" in n else p.T @ p\n                    I = torch.eye(*para_cov.size(), out=torch.empty_like(para_cov))\n                    I.requires_grad = False\n                    num_param += 1\n                    regu_loss += torch.norm(para_cov - I, p=\"fro\")\n            regu_loss = regu_loss / num_param\n            outputs.loss += orth_reg_weight * regu_loss\n        return outputs\n\n    def resize_modules_by_rank_pattern(self, rank_pattern, adapter_name):\n        lora_config = self.peft_config[adapter_name]\n        for name, rank_idx in rank_pattern.items():\n            if isinstance(rank_idx, list):\n                rank = sum(rank_idx)\n            elif isinstance(rank_idx, torch.Tensor):\n                rank_idx = rank_idx.view(-1)\n                rank = rank_idx.sum().item()\n            else:\n                raise ValueError(\"Unexcepted type of rank_idx\")\n            key = \".\".join(name.split(\".\")[0:-2]) if adapter_name in name else \".\".join(name.split(\".\")[0:-1])\n            _, target, _ = _get_submodules(self.model, key)\n            lora_E_weights = target.lora_E[adapter_name][rank_idx]\n            lora_A_weights = target.lora_A[adapter_name][rank_idx]\n            lora_B_weights = target.lora_B[adapter_name][:, rank_idx]\n            ranknum = target.ranknum[adapter_name]\n            target.update_layer(\n                adapter_name,\n                rank,\n                lora_config.lora_alpha,\n                lora_config.lora_dropout,\n                lora_config.init_lora_weights,\n            )\n            with torch.no_grad():\n                if rank > 0:\n                    target.lora_E[adapter_name].copy_(lora_E_weights)\n                    target.lora_A[adapter_name].copy_(lora_A_weights)\n                    target.lora_B[adapter_name].copy_(lora_B_weights)\n                    # The scaling is exactly as the previous\n                    target.ranknum[adapter_name].copy_(ranknum)\n\n    def resize_state_dict_by_rank_pattern(self, rank_pattern, state_dict, adapter_name):\n        for name, rank_idx in rank_pattern.items():\n            rank = sum(rank_idx)\n            prefix = \".\".join(name.split(\".\")[0:-2]) if adapter_name in name else \".\".join(name.split(\".\")[0:-1])\n            for layer in [\"lora_E\", \"lora_A\", \"lora_B\"]:\n                key = f\"base_model.model.{prefix}.{layer}.{adapter_name}\"\n                if layer != \"lora_B\":\n                    state_dict[key] = (\n                        state_dict[key][rank_idx] if rank != state_dict[key].shape[0] else state_dict[key]\n                    )\n                else:\n                    state_dict[key] = (\n                        state_dict[key][:, rank_idx] if rank != state_dict[key].shape[1] else state_dict[key]\n                    )\n        return state_dict\n\n    def update_and_allocate(self, global_step):\n        lora_config = self.peft_config[self.trainable_adapter_name]\n        # Update the importance score and allocate the budget\n        if global_step < lora_config.total_step - lora_config.tfinal:\n            _, rank_pattern = self.rankallocator.update_and_allocate(self.model, global_step)\n            if rank_pattern:\n                lora_config.rank_pattern = rank_pattern\n        # Finalize the budget allocation\n        elif global_step == lora_config.total_step - lora_config.tfinal:\n            _, rank_pattern = self.rankallocator.update_and_allocate(self.model, global_step, force_mask=True)\n            # for some reason, this freezes the trainable parameters and nothing gets updates\n            # self.resize_modules_by_rank_pattern(rank_pattern, self.trainable_adapter_name)\n            lora_config.rank_pattern = rank_pattern\n            self.rankallocator.reset_ipt()\n        # Currently using inefficient way to mask the unimportant weights using the rank pattern\n        #  due to problem mentioned above\n        elif global_step > lora_config.total_step - lora_config.tfinal:\n            self.rankallocator.mask_using_rank_pattern(self.model, lora_config.rank_pattern)\n        # Pass the function and do forward propagation\n        else:\n            return None\n\n    @staticmethod\n    def _prepare_adalora_config(peft_config, model_config):\n        if peft_config.target_modules is None:\n            if model_config[\"model_type\"] not in TRANSFORMERS_MODELS_TO_ADALORA_TARGET_MODULES_MAPPING:\n                raise ValueError(\"Please specify `target_modules` in `peft_config`\")\n            peft_config.target_modules = TRANSFORMERS_MODELS_TO_ADALORA_TARGET_MODULES_MAPPING[\n                model_config[\"model_type\"]\n            ]\n        if peft_config.inference_mode:\n            peft_config.merge_weights = True\n        return peft_config\n\n\nclass AdaLoraLayer(LoraLayer):\n    def __init__(\n        self,\n        in_features: int,\n        out_features: int,\n    ):\n        super().__init__(in_features, out_features)\n        self.lora_E = nn.ParameterDict({})\n        self.lora_A = nn.ParameterDict({})\n        self.lora_B = nn.ParameterDict({})\n        self.ranknum = nn.ParameterDict({})\n\n    def update_layer(self, adapter_name, r, lora_alpha, lora_dropout, init_lora_weights):\n        self.r[adapter_name] = r\n        self.lora_alpha[adapter_name] = lora_alpha\n        if lora_dropout > 0.0:\n            lora_dropout_layer = nn.Dropout(p=lora_dropout)\n        else:\n\n            def lora_dropout_layer(x):\n                return x\n\n        self.lora_dropout.update(nn.ModuleDict({adapter_name: lora_dropout_layer}))\n        # Actual trainable parameters\n        # Right singular vectors\n        self.lora_A.update(nn.ParameterDict({adapter_name: nn.Parameter(torch.zeros(r, self.in_features))}))\n        # Singular values\n        self.lora_E.update(nn.ParameterDict({adapter_name: nn.Parameter(torch.zeros(r, 1))}))\n        # Left singular vectors\n        self.lora_B.update(nn.ParameterDict({adapter_name: nn.Parameter(torch.zeros(self.out_features, r))}))\n        # The current rank\n        self.ranknum.update(nn.ParameterDict({adapter_name: nn.Parameter(torch.zeros(1), requires_grad=False)}))\n        self.ranknum[adapter_name].data.fill_(float(r))\n        self.ranknum[adapter_name].requires_grad = False\n        self.scaling[adapter_name] = lora_alpha if lora_alpha > 0 else float(r)\n        if init_lora_weights:\n            self.reset_lora_parameters(adapter_name)\n        self.to(self.weight.device)\n\n    def reset_lora_parameters(self, adapter_name):\n        if adapter_name in self.lora_A.keys():\n            nn.init.zeros_(self.lora_E[adapter_name])\n            nn.init.normal_(self.lora_A[adapter_name], mean=0.0, std=0.02)\n            nn.init.normal_(self.lora_B[adapter_name], mean=0.0, std=0.02)\n\n\nclass SVDLinear(nn.Linear, AdaLoraLayer):\n    # SVD-based adaptation by a dense layer\n    def __init__(\n        self,\n        adapter_name: str,\n        in_features: int,\n        out_features: int,\n        r: int = 0,\n        lora_alpha: int = 1,\n        lora_dropout: float = 0.0,\n        fan_in_fan_out: bool = False,\n        **kwargs,\n    ):\n        init_lora_weights = kwargs.pop(\"init_lora_weights\", True)\n        nn.Linear.__init__(self, in_features, out_features, **kwargs)\n        AdaLoraLayer.__init__(self, in_features=in_features, out_features=out_features)\n        # Freezing the pre-trained weight matrix\n        self.weight.requires_grad = False\n\n        self.fan_in_fan_out = fan_in_fan_out\n        if fan_in_fan_out:\n            self.weight.data = self.weight.data.T\n\n        nn.Linear.reset_parameters(self)\n        self.update_layer(adapter_name, r, lora_alpha, lora_dropout, init_lora_weights)\n        self.active_adapter = adapter_name\n\n    def merge(self):\n        if self.active_adapter not in self.lora_A.keys():\n            return\n        if self.merged:\n            warnings.warn(\"Already merged. Nothing to do.\")\n            return\n        if self.r[self.active_adapter] > 0:\n            self.weight.data += (\n                transpose(\n                    self.lora_B[self.active_adapter]\n                    @ (self.lora_A[self.active_adapter] * self.lora_E[self.active_adapter]),\n                    self.fan_in_fan_out,\n                )\n                * self.scaling[self.active_adapter]\n                / (self.ranknum[self.active_adapter] + 1e-5)\n            )\n            self.merged = True\n\n    def unmerge(self):\n        if self.active_adapter not in self.lora_A.keys():\n            return\n        if not self.merged:\n            warnings.warn(\"Already unmerged. Nothing to do.\")\n            return\n        if self.r[self.active_adapter] > 0:\n            self.weight.data -= (\n                transpose(\n                    self.lora_B[self.active_adapter]\n                    @ (self.lora_A[self.active_adapter] * self.lora_E[self.active_adapter])\n                )\n                * self.scaling[self.active_adapter]\n                / (self.ranknum[self.active_adapter] + 1e-5)\n            )\n            self.merged = False\n\n    def forward(self, x: torch.Tensor):\n        if self.active_adapter not in self.lora_A.keys():\n            return F.linear(x, transpose(self.weight, self.fan_in_fan_out), bias=self.bias)\n        if self.disable_adapters:\n            if self.r[self.active_adapter] > 0 and self.merged:\n                self.unmerge()\n            result = F.linear(x, transpose(self.weight, self.fan_in_fan_out), bias=self.bias)\n        elif self.r[self.active_adapter] > 0 and not self.merged:\n            result = F.linear(x, transpose(self.weight, self.fan_in_fan_out), bias=self.bias)\n            result += (\n                (\n                    self.lora_dropout[self.active_adapter](x)\n                    @ (self.lora_A[self.active_adapter] * self.lora_E[self.active_adapter]).T\n                    @ self.lora_B[self.active_adapter].T\n                )\n                * self.scaling[self.active_adapter]\n                / (self.ranknum[self.active_adapter] + 1e-5)\n            )\n        else:\n            result = F.linear(x, transpose(self.weight, self.fan_in_fan_out), bias=self.bias)\n        return result\n\n\nif is_bnb_available():\n\n    class SVDLinear8bitLt(bnb.nn.Linear8bitLt, AdaLoraLayer):\n        # Low-rank matrix for SVD-based adaptation\n        def __init__(\n            self,\n            adapter_name,\n            in_features,\n            out_features,\n            r: int = 0,\n            lora_alpha: int = 1,\n            lora_dropout: float = 0.0,\n            **kwargs,\n        ):\n            bnb.nn.Linear8bitLt.__init__(\n                self,\n                in_features,\n                out_features,\n                bias=kwargs.get(\"bias\", True),\n                has_fp16_weights=kwargs.get(\"has_fp16_weights\", True),\n                memory_efficient_backward=kwargs.get(\"memory_efficient_backward\", False),\n                threshold=kwargs.get(\"threshold\", 0.0),\n                index=kwargs.get(\"index\", None),\n            )\n            AdaLoraLayer.__init__(self, in_features=in_features, out_features=out_features)\n            # Freezing the pre-trained weight matrix\n            self.weight.requires_grad = False\n\n            init_lora_weights = kwargs.pop(\"init_lora_weights\", True)\n            self.update_layer(adapter_name, r, lora_alpha, lora_dropout, init_lora_weights)\n            self.active_adapter = adapter_name\n\n        def forward(self, x: torch.Tensor):\n            result = super().forward(x)\n\n            if self.disable_adapters or self.active_adapter not in self.lora_A.keys():\n                return result\n            elif self.r[self.active_adapter] > 0:\n                if not torch.is_autocast_enabled():\n                    expected_dtype = result.dtype\n\n                    if x.dtype != torch.float32:\n                        x = x.float()\n                    output = (\n                        (\n                            self.lora_dropout[self.active_adapter](x)\n                            @ (self.lora_A[self.active_adapter] * self.lora_E[self.active_adapter]).T\n                            @ self.lora_B[self.active_adapter].T\n                        ).to(expected_dtype)\n                        * self.scaling[self.active_adapter]\n                        / (self.ranknum[self.active_adapter] + 1e-5)\n                    )\n                else:\n                    output = (\n                        (\n                            self.lora_dropout[self.active_adapter](x)\n                            @ (self.lora_A[self.active_adapter] * self.lora_E[self.active_adapter]).T\n                            @ self.lora_B[self.active_adapter].T\n                        )\n                        * self.scaling[self.active_adapter]\n                        / (self.ranknum[self.active_adapter] + 1e-5)\n                    )\n                result += output\n            return result\n\n\nclass RankAllocator(object):\n    \"\"\"\n    The RankAllocator for AdaLoraModel. Paper: https://openreview.net/pdf?id=lq62uWRJjiY\n\n    Args:\n        config ([`AdaLoraConfig`]): The configuration of the AdaLora model.\n        model: the model that we apply AdaLoRA to.\n\n    \"\"\"\n\n    def __init__(self, model, peft_config, adapter_name):\n        self.peft_config = peft_config\n        self.adapter_name = adapter_name\n        self.beta1 = peft_config.beta1\n        self.beta2 = peft_config.beta2\n        assert self.beta1 > 0 and self.beta1 < 1\n        assert self.beta2 > 0 and self.beta2 < 1\n\n        self.reset_ipt()\n        self._set_budget_scheduler(model)\n\n    def set_total_step(self, total_step):\n        self.peft_config.total_step = total_step\n\n    def reset_ipt(self):\n        self.ipt = {}\n        self.exp_avg_ipt = {}\n        self.exp_avg_unc = {}\n\n    def _set_budget_scheduler(self, model):\n        self.init_bgt = 0\n        self.name_set = set()\n        for n, p in model.named_parameters():\n            if f\"lora_A.{self.adapter_name}\" in n:\n                self.init_bgt += p.size(0)\n                self.name_set.add(n.replace(\"lora_A\", \"%s\"))\n        self.name_set = sorted(self.name_set)\n        # The total final rank budget\n        self.target_bgt = self.peft_config.target_r * len(self.name_set)\n\n    def budget_schedule(self, step: int):\n        tinit = self.peft_config.tinit\n        tfinal = self.peft_config.tfinal\n        total_step = self.peft_config.total_step\n        # Initial warmup\n        if step <= tinit:\n            budget = self.init_bgt\n            mask_ind = False\n        # Final fine-tuning\n        elif step > total_step - tfinal:\n            budget = self.target_bgt\n            mask_ind = True\n        else:\n            # Budget decreasing with a cubic scheduler\n            mul_coeff = 1 - (step - tinit) / (total_step - tfinal - tinit)\n            budget = int((self.init_bgt - self.target_bgt) * (mul_coeff**3) + self.target_bgt)\n            mask_ind = True if step % self.peft_config.deltaT == 0 else False\n        return budget, mask_ind\n\n    def update_ipt(self, model):\n        # Update the sensitivity and uncertainty for every weight\n        for n, p in model.named_parameters():\n            if \"lora_\" in n and self.adapter_name in n:\n                if n not in self.ipt:\n                    self.ipt[n] = torch.zeros_like(p)\n                    self.exp_avg_ipt[n] = torch.zeros_like(p)\n                    self.exp_avg_unc[n] = torch.zeros_like(p)\n                with torch.no_grad():\n                    self.ipt[n] = (p * p.grad).abs().detach()\n                    # Sensitivity smoothing\n                    self.exp_avg_ipt[n] = self.beta1 * self.exp_avg_ipt[n] + (1 - self.beta1) * self.ipt[n]\n                    # Uncertainty quantification\n                    self.exp_avg_unc[n] = (\n                        self.beta2 * self.exp_avg_unc[n] + (1 - self.beta2) * (self.ipt[n] - self.exp_avg_ipt[n]).abs()\n                    )\n\n    def _element_score(self, n):\n        return self.exp_avg_ipt[n] * self.exp_avg_unc[n]\n\n    def _combine_ipt(self, ipt_E, ipt_AB):\n        ipt_AB = ipt_AB.sum(dim=1, keepdim=False)\n        sum_ipt = ipt_E.view(-1) + ipt_AB.view(-1)\n        return sum_ipt\n\n    def mask_to_budget(self, model, budget):\n        value_ipt = {}\n        vector_ipt = {}\n        triplet_ipt = {}\n        # Get the importance score for A, E, B\n        for n, p in model.named_parameters():\n            if f\"lora_A.{self.adapter_name}\" in n:\n                entry_ipt = self._element_score(n)\n                comb_ipt = torch.mean(entry_ipt, dim=1, keepdim=True)\n                name_m = n.replace(\"lora_A\", \"%s\")\n                if name_m not in vector_ipt:\n                    vector_ipt[name_m] = [comb_ipt]\n                else:\n                    vector_ipt[name_m].append(comb_ipt)\n            if f\"lora_B.{self.adapter_name}\" in n:\n                entry_ipt = self._element_score(n)\n                comb_ipt = torch.mean(entry_ipt, dim=0, keepdim=False).view(-1, 1)\n                name_m = n.replace(\"lora_B\", \"%s\")\n                if name_m not in vector_ipt:\n                    vector_ipt[name_m] = [comb_ipt]\n                else:\n                    vector_ipt[name_m].append(comb_ipt)\n            if f\"lora_E.{self.adapter_name}\" in n:\n                entry_ipt = self._element_score(n)\n                name_m = n.replace(\"lora_E\", \"%s\")\n                value_ipt[name_m] = entry_ipt\n\n        all_score = []\n        # Calculate the score for each triplet\n        for name_m in vector_ipt:\n            ipt_E = value_ipt[name_m]\n            ipt_AB = torch.cat(vector_ipt[name_m], dim=1)\n            sum_ipt = self._combine_ipt(ipt_E, ipt_AB)\n            name_E = name_m % \"lora_E\"\n            triplet_ipt[name_E] = sum_ipt.view(-1, 1)\n            all_score.append(sum_ipt.view(-1))\n\n        # Get the threshold by ranking ipt\n        mask_threshold = torch.kthvalue(\n            torch.cat(all_score),\n            k=self.init_bgt - budget,\n        )[0].item()\n\n        rank_pattern = {}\n        # Mask the unimportant triplets\n        with torch.no_grad():\n            for n, p in model.named_parameters():\n                if f\"lora_E.{self.adapter_name}\" in n:\n                    p.masked_fill_(triplet_ipt[n] <= mask_threshold, 0.0)\n                    rank_pattern[n] = (~(triplet_ipt[n] <= mask_threshold)).view(-1).tolist()\n        return rank_pattern\n\n    def update_and_allocate(self, model, global_step, force_mask=False):\n        # # Update the importance score and allocate the budget\n        if global_step < self.peft_config.total_step - self.peft_config.tfinal:\n            self.update_ipt(model)\n        budget, mask_ind = self.budget_schedule(global_step)\n        # Allocate the budget according to importance scores\n        if mask_ind or force_mask:\n            rank_pattern = self.mask_to_budget(model, budget)\n        else:\n            rank_pattern = None\n        return budget, rank_pattern\n\n    def mask_using_rank_pattern(self, model, rank_pattern):\n        # Mask the unimportant triplets\n        is_adapter_name_truncated = False\n        if self.adapter_name not in next(iter(rank_pattern.keys())):\n            is_adapter_name_truncated = True\n\n        with torch.no_grad():\n            for n, p in model.named_parameters():\n                if f\"lora_E.{self.adapter_name}\" in n:\n                    key = n if not is_adapter_name_truncated else n.replace(f\".{self.adapter_name}\", \"\")\n                    mask = torch.Tensor(rank_pattern[key]).unsqueeze(-1).to(p.device)\n                    p.masked_fill_(~mask.bool(), 0.0)\n"}
{"type": "source_file", "path": "pseudo_data/src/peft_1/tuners/adaption_prompt.py", "content": "# coding=utf-8\n# Copyright 2023-present the HuggingFace Inc. team.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport math\nfrom collections import namedtuple\nfrom dataclasses import dataclass, field\nfrom typing import Dict, List\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom peft.utils.config import PeftConfig, PeftType\nfrom peft.utils.other import _freeze_adapter, _get_submodules\n\n\ndef llama_rotate_half(x: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    Rotate half the hidden dims of the input.\n\n    This function was duplicated verbatim from:\n    https://github.com/huggingface/transformers/blob/1de8ce9ee1191ba761a593ac15d9ccbf5851bfc5/src/transformers/models/llama/modeling_llama.py#L126\n\n    This was done to eliminate the Llama transformers implementation as a dependency of this file. Note that some other\n    functions were also adapted from the transformers implementation but were modified.\n    \"\"\"\n    x1 = x[..., : x.shape[-1] // 2]\n    x2 = x[..., x.shape[-1] // 2 :]\n    return torch.cat((-x2, x1), dim=-1)\n\n\ndef llama_apply_rotary_pos_emb(q, cos, sin, position_ids):\n    \"\"\"\n    Apply rotary position embedding to query states in the Llama model.\n\n    This function was adapted from:\n    https://github.com/huggingface/transformers/blob/1de8ce9ee1191ba761a593ac15d9ccbf5851bfc5/src/transformers/models/llama/modeling_llama.py#L133\n\n    It was modified to remove unnecessary processing of key states.\n    \"\"\"\n    gather_indices = position_ids[:, None, :, None]  # [bs, 1, seq_len, 1]\n    gather_indices = gather_indices.repeat(1, cos.shape[1], 1, cos.shape[3])\n    cos = torch.gather(cos.repeat(gather_indices.shape[0], 1, 1, 1), 2, gather_indices)\n    sin = torch.gather(sin.repeat(gather_indices.shape[0], 1, 1, 1), 2, gather_indices)\n    q_embed = (q * cos) + (llama_rotate_half(q) * sin)\n    return q_embed\n\n\ndef llama_compute_query_states(model: nn.Module, **kwargs) -> torch.Tensor:\n    \"\"\"\n    Compute query states for Llama models specifically.\n\n    They need to be recomputed as the forward() method of the original LlamaModel in the transformers library does not\n    return them. See the related discussion in the PR: https://github.com/huggingface/peft/pull/268\n    \"\"\"\n    hidden_states = kwargs.get(\"hidden_states\")\n    position_ids = kwargs.get(\"position_ids\")\n    past_key_value = kwargs.get(\"past_key_value\")\n    bsz, q_len, _ = hidden_states.size()\n    query_states = model.q_proj(hidden_states).view(bsz, q_len, model.num_heads, model.head_dim).transpose(1, 2)\n    value_states = model.v_proj(hidden_states).view(bsz, q_len, model.num_heads, model.head_dim).transpose(1, 2)\n\n    seq_len = q_len\n    if past_key_value is not None:\n        seq_len += past_key_value[0].shape[-2]\n    cos, sin = model.rotary_emb(value_states, seq_len=seq_len)\n\n    return llama_apply_rotary_pos_emb(query_states, cos, sin, position_ids)\n\n\n# Contains the config that is specific to a transformers model type.\nModelTypeConfig = namedtuple(\n    \"ModelTypeConfig\", [\"compute_query_states\", \"target_modules\", \"k_proj_layer\", \"v_proj_layer\", \"o_proj_layer\"]\n)\n# Mapping of transformers model types to their specific configuration.\nTRANSFORMERS_MODEL_CONFIG = {\n    \"llama\": ModelTypeConfig(\n        compute_query_states=llama_compute_query_states,\n        target_modules=\"self_attn\",\n        k_proj_layer=\"k_proj\",\n        v_proj_layer=\"v_proj\",\n        o_proj_layer=\"o_proj\",\n    ),\n}\n\n\ndef is_adaption_prompt_trainable(params: str) -> bool:\n    \"\"\"Return True if module is trainable under adaption prompt fine-tuning.\"\"\"\n    return params.split(\".\")[-1].startswith(\"adaption_\")\n\n\n@dataclass\nclass AdaptionPromptConfig(PeftConfig):\n    \"\"\"Stores the configuration of an [`AdaptionPromptModel`].\"\"\"\n\n    target_modules: str = field(\n        default=None, metadata={\"help\": \"Name of the attention submodules to insert adaption prompts into.\"}\n    )\n    adapter_len: int = field(default=None, metadata={\"help\": \"Number of adapter tokens to insert\"})\n    adapter_layers: int = field(default=None, metadata={\"help\": \"Number of adapter layers (from the top)\"})\n\n    def __post_init__(self):\n        self.peft_type = PeftType.ADAPTION_PROMPT\n\n\ndef prepare_config(\n    peft_config: AdaptionPromptConfig,\n    model,\n) -> AdaptionPromptConfig:\n    \"\"\"Prepare the config based on the llama model type.\"\"\"\n    if model.config.model_type not in TRANSFORMERS_MODEL_CONFIG:\n        raise ValueError(\"Unsupported model type for adaption prompt: '{model.config.model_type}'.\")\n\n    model_config = TRANSFORMERS_MODEL_CONFIG[model.config.model_type]\n\n    if peft_config.target_modules is None:\n        peft_config.target_modules = model_config.target_modules\n\n    return peft_config\n\n\nclass AdaptionPromptModel(nn.Module):\n    \"\"\"\n    Implements adaption prompts as described in https://arxiv.org/pdf/2303.16199.pdf.\n\n    The top L attention modules are replaced with AdaptedAttention modules that wrap the original ones, but insert\n    trainable prompts with gates (for zero init).\n\n    Notes on the multi-adapter pattern:\n    - We store the states of different adapters by keeping a dictionary of AdaptedAttention modules indexed by adapter\n      name.\n    - Every time we switch adapters, we remove the modules of the currently active adapter from the model, store them\n      in the dictionary, and replace them with the modules of the new adapter.\n    - To avoid duplicated and potentially inconsistent state, the currently active adapter is always removed from the\n      dictionary.\n    - Disabling the adapter would also result in the modules being removed from the model.\n    \"\"\"\n\n    def __init__(self, model, configs: Dict, adapter_name: str):\n        super().__init__()\n        self.model = model\n        # Store adapter configs by name.\n        self._configs: Dict[str, AdaptionPromptConfig] = {}\n        # Store lists of the parents of the affected attention modules by adapter name.\n        # We keep references to the parents so we can swap the adapters in-and-out of the model.\n        self._parents: Dict[str, List[nn.Module]] = {}\n        # Store lists of cached AdaptedAttention modules by name.\n        self._cached_adapters: Dict[str, List] = {}\n        # The name of the currently active adapter.\n        self._active_adapter = None\n        # Whether the adapter is enabled.\n        self._enabled = True\n        self.forward = self.model.forward\n        self.add_adapter(adapter_name, configs[adapter_name])\n        self._mark_only_adaption_prompts_as_trainable()\n\n    def add_adapter(self, adapter_name: str, config: AdaptionPromptConfig) -> None:\n        \"\"\"Add an adapter with the given name and config.\"\"\"\n        config = prepare_config(config, self.model)\n        if adapter_name in self._configs:\n            raise ValueError(f\"Adapter with name '{adapter_name}' already exists.\")\n\n        parents = []\n        for name, _ in self.model.named_modules():\n            if name.endswith(config.target_modules):\n                par, _, _ = _get_submodules(self.model, name)\n                parents.append(par)\n        if len(parents) < config.adapter_layers:\n            raise ValueError(\n                f\"Config specifies more adapter layers '{config.adapter_layers}'\"\n                f\" than the model has '{len(parents)}'.\"\n            )\n        # Note that if the target modules are not in Sequential, ModuleList, or\n        # some other PyTorch ordered container, the behavior is undefined as we\n        # assume here that the order of the modules is the same as the order of\n        # the transformer decoder layers.\n        parents = parents[-config.adapter_layers :]\n        self._parents[adapter_name] = parents\n\n        # It is only None during initialization.\n        # If it is disabled, we don't have to remove the modules.\n        if self._active_adapter is not None and self._enabled:\n            self._remove_adapted_attentions(self._active_adapter)\n        self._active_adapter = adapter_name\n        self._configs[adapter_name] = config\n        self._create_adapted_attentions(config, parents)\n        if not self._enabled:\n            self._remove_adapted_attentions(self._active_adapter)\n\n        if config.inference_mode:\n            _freeze_adapter(self.model, adapter_name)\n\n    def set_adapter(self, adapter_name: str) -> None:\n        \"\"\"Set the model to use the adapter with the given name.\"\"\"\n        if self._active_adapter == adapter_name:\n            return\n        if adapter_name not in self._configs:\n            raise ValueError(f\"Adapter with name '{adapter_name}' does not exist.\")\n\n        if self._enabled:\n            self._remove_adapted_attentions(self._active_adapter)\n            self._set_adapted_attentions(adapter_name)\n\n        self._active_adapter = adapter_name\n\n    def enable_adapter_layers(self):\n        \"\"\"Enable adapter layers by swapping in cached AdaptedAttention modules.\"\"\"\n        self._enabled = True\n        self._set_adapted_attentions(self._active_adapter)\n\n    def disable_adapter_layers(self):\n        \"\"\"Disable adapter layers by swapping out AdaptedAttention modules.\"\"\"\n        self._enabled = False\n        self._remove_adapted_attentions(self._active_adapter)\n\n    def _create_adapted_attentions(self, config: AdaptionPromptConfig, parents: List[nn.Module]) -> None:\n        \"\"\"Wrap LlamaAttention modules with newly created AdaptedAttention modules.\"\"\"\n        for par in parents:\n            attn = AdaptedAttention(\n                model_type=self.model.config.model_type,\n                adapter_len=config.adapter_len,\n                model=getattr(par, config.target_modules),\n            )\n            setattr(par, config.target_modules, attn)\n\n    def _set_adapted_attentions(self, adapter_name: str) -> None:\n        \"\"\"Replace LlamaAttention modules with cached AdaptedAttention modules.\"\"\"\n        cached = self._cached_adapters[adapter_name]\n        del self._cached_adapters[adapter_name]\n        config = self._configs[adapter_name]\n        for i, par in enumerate(self._parents[adapter_name]):\n            setattr(par, config.target_modules, cached[i])\n\n    def _remove_adapted_attentions(self, adapter_name: str) -> None:\n        \"\"\"Remove AdaptedAttention modules from the model and store them in the cache.\"\"\"\n        config = self._configs[adapter_name]\n        adapted_attentions = []\n        for par in self._parents[adapter_name]:\n            attn = getattr(par, config.target_modules)\n            adapted_attentions.append(attn)\n            setattr(par, config.target_modules, attn.model)\n        self._cached_adapters[adapter_name] = adapted_attentions\n\n    def _mark_only_adaption_prompts_as_trainable(self) -> None:\n        \"\"\"Freeze all parameters of the model except the adaption prompts.\"\"\"\n        for n, p in self.model.named_parameters():\n            if not is_adaption_prompt_trainable(n):\n                p.requires_grad = False\n\n    def __getattr__(self, name: str):\n        \"\"\"Forward missing attributes to the wrapped module.\"\"\"\n        try:\n            return super().__getattr__(name)  # defer to nn.Module's logic\n        except AttributeError:\n            # This is necessary as e.g. causal models have various methods that we\n            # don't want to re-implement here.\n            return getattr(self.model, name)\n\n\nclass AdaptedAttention(nn.Module):\n    \"\"\"This module wraps a LLamaAttention module and injects adaption prompts.\"\"\"\n\n    def __init__(self, model_type: str, adapter_len: int, model):\n        \"\"\"\n        Initialize object.\n\n        Args:\n            model_type: The transformer model type. This is used to retrieve the right method to\n                compute query states.\n            adapter_len: The length of the adaption prompt to insert.\n            model: The original transformer attention module that is being wrapped.\n        \"\"\"\n        assert not isinstance(model, AdaptedAttention)\n        super().__init__()\n        self.model_type = model_type\n        self.model = model\n        self.adapter_len = adapter_len\n        # Assume all parameters of the attention model we are wrapping are on the same device.\n        device = next(model.parameters()).device\n        # Don't think this was specified in the paper, but we follow the official repo which used an Embedding\n        # which initializes the tokens with standard normal values.\n        # https://github.com/ZrrSkywalker/LLaMA-Adapter/blob/41c3546fe1997ab8a65809dc8d8f9252b19d9faf/llama/model.py#L234\n        # (bsz, adapter_len, hidden_size)\n        self.adaption_prompt = nn.Parameter(\n            torch.empty(1, adapter_len, self.model.hidden_size, device=device).normal_()\n        )\n        # Initialize the gate to 0 as this is \"zero-init\".\n        self.adaption_gate = nn.Parameter(torch.zeros(1, device=device))\n\n    def forward(self, **kwargs):\n        \"\"\"\n        Forward pass for the adapter which wraps the original LlamaAttention module.\n\n        \"Official\" paper implementation:\n        https://github.com/ZrrSkywalker/LLaMA-Adapter/blob/41c3546fe1997ab8a65809dc8d8f9252b19d9faf/llama/model.py#L141\n\n        Args:\n            kwargs: See the original LlamaAttention module.\n        \"\"\"\n        if kwargs.get(\"output_attention\", False):\n            raise NotImplementedError(\"output_attention is not currently supported.\")\n\n        output, _, past_key_value = self.model(**kwargs)\n        bsz = output.shape[0]\n        q_len = output.shape[1]\n        embed_dim = output.shape[2]\n        k_proj_layer = TRANSFORMERS_MODEL_CONFIG[self.model_type].k_proj_layer\n        v_proj_layer = TRANSFORMERS_MODEL_CONFIG[self.model_type].v_proj_layer\n        o_proj_layer = TRANSFORMERS_MODEL_CONFIG[self.model_type].o_proj_layer\n\n        if k_proj_layer == v_proj_layer:\n            _, key, value = getattr(self.model, k_proj_layer)(self.adaption_prompt).split(embed_dim, dim=2)\n        else:\n            key = getattr(self.model, k_proj_layer)(self.adaption_prompt)\n            value = getattr(self.model, v_proj_layer)(self.adaption_prompt)\n        # (bsz, num_heads, adapter_len, head_dim)\n        adapter_k = (\n            key.view(1, self.adapter_len, self.model.num_heads, self.model.head_dim)\n            .repeat(bsz, 1, 1, 1)\n            .transpose(1, 2)\n        )\n        # (bsz, num_heads, adapter_len, head_dim)\n        adapter_v = (\n            value.view(1, self.adapter_len, self.model.num_heads, self.model.head_dim)\n            .repeat(bsz, 1, 1, 1)\n            .transpose(1, 2)\n        )\n\n        # Recompute query states.\n        compute_query_states = TRANSFORMERS_MODEL_CONFIG[self.model_type].compute_query_states\n        # (bsz, num_heads, q_len, head_dim)\n        query_states = compute_query_states(model=self.model, **kwargs)\n\n        # (bsz, num_heads, q_len, adapter_len)\n        scores = torch.matmul(query_states, adapter_k.transpose(2, 3)) / math.sqrt(self.model.head_dim)\n        # Upcast attention to fp32\n        # (bsz, num_heads, q_len, adapter_len)\n        scores = self.adaption_gate * F.softmax(scores, dim=-1, dtype=torch.float32).to(query_states.dtype)\n        # (bsz, q_len, num_heads * head_dim)\n        adapter_output = torch.matmul(scores, adapter_v).transpose(1, 2).reshape(bsz, q_len, -1)\n        # (bsz, q_len, hidden_size)\n        if o_proj_layer is not None:\n            adapter_output = getattr(self.model, o_proj_layer)(adapter_output)\n\n        # Add adaption prompt output to original output.\n        output = output + adapter_output\n        return output, None, past_key_value\n"}
{"type": "source_file", "path": "pseudo_data/src/peft_1/tuners/lora.py", "content": "# coding=utf-8\n# Copyright 2023-present the HuggingFace Inc. team.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport math\nimport re\nimport warnings\nfrom dataclasses import asdict, dataclass, field\nfrom enum import Enum\nfrom typing import List, Optional, Union\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom transformers.pytorch_utils import Conv1D\n\nfrom ..import_utils import is_bnb_available\nfrom ..utils import (\n    TRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING,\n    ModulesToSaveWrapper,\n    PeftConfig,\n    PeftType,\n    _freeze_adapter,\n    _get_submodules,\n    transpose,\n)\n\n\nif is_bnb_available():\n    import bitsandbytes as bnb\n\n\n@dataclass\nclass LoraConfig(PeftConfig):\n    \"\"\"\n    This is the configuration class to store the configuration of a [`LoraModel`].\n\n    Args:\n        r (`int`): Lora attention dimension.\n        target_modules (`Union[List[str],str]`): The names of the modules to apply Lora to.\n        lora_alpha (`float`): The alpha parameter for Lora scaling.\n        lora_dropout (`float`): The dropout probability for Lora layers.\n        fan_in_fan_out (`bool`): Set this to True if the layer to replace stores weight like (fan_in, fan_out).\n        For example, gpt-2 uses `Conv1D` which stores weights like (fan_in, fan_out) and hence this should be set to `True`.:\n        bias (`str`): Bias type for Lora. Can be 'none', 'all' or 'lora_only'\n        modules_to_save (`List[str]`):List of modules apart from LoRA layers to be set as trainable\n            and saved in the final checkpoint.\n    \"\"\"\n\n    r: int = field(default=8, metadata={\"help\": \"Lora attention dimension\"})\n    target_modules: Optional[Union[List[str], str]] = field(\n        default=None,\n        metadata={\n            \"help\": \"List of module names or regex expression of the module names to replace with Lora.\"\n            \"For example, ['q', 'v'] or '.*decoder.*(SelfAttention|EncDecAttention).*(q|v)$' \"\n        },\n    )\n    lora_alpha: int = field(default=None, metadata={\"help\": \"Lora alpha\"})\n    lora_dropout: float = field(default=None, metadata={\"help\": \"Lora dropout\"})\n    fan_in_fan_out: bool = field(\n        default=False,\n        metadata={\"help\": \"Set this to True if the layer to replace stores weight like (fan_in, fan_out)\"},\n    )\n    bias: str = field(default=\"none\", metadata={\"help\": \"Bias type for Lora. Can be 'none', 'all' or 'lora_only'\"})\n    modules_to_save: Optional[List[str]] = field(\n        default=None,\n        metadata={\n            \"help\": \"List of modules apart from LoRA layers to be set as trainable and saved in the final checkpoint. \"\n            \"For example, in Sequence Classification or Token Classification tasks, \"\n            \"the final layer `classifier/score` are randomly initialized and as such need to be trainable and saved.\"\n        },\n    )\n    init_lora_weights: bool = field(\n        default=True,\n        metadata={\"help\": \"Whether to initialize the weights of the Lora layers.\"},\n    )\n    r_sum: int = field(default=0) # modified. This argument represents the dim of the previous LoRA parameters. \n    save_loranew: bool = field(default=False) # modified. This arguments represents whether modules named of 'loranew_A/B' are saved independently, rather than being combined with \"lora_A/B\".  \n\n    def __post_init__(self):\n        self.peft_type = PeftType.LORA\n\n\nclass LoraModel(torch.nn.Module):\n    \"\"\"\n    Creates Low Rank Adapter (Lora) model from a pretrained transformers model.\n\n    Args:\n        model ([`~transformers.PreTrainedModel`]): The model to be adapted.\n        config ([`LoraConfig`]): The configuration of the Lora model.\n\n    Returns:\n        `torch.nn.Module`: The Lora model.\n\n    Example:\n\n        ```py\n        >>> from transformers import AutoModelForSeq2SeqLM, LoraConfig\n        >>> from peft import LoraModel, LoraConfig\n\n        >>> config = LoraConfig(\n        ...     peft_type=\"LORA\",\n        ...     task_type=\"SEQ_2_SEQ_LM\",\n        ...     r=8,\n        ...     lora_alpha=32,\n        ...     target_modules=[\"q\", \"v\"],\n        ...     lora_dropout=0.01,\n        ... )\n\n        >>> model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\")\n        >>> lora_model = LoraModel(config, model)\n        ```\n\n        ```py\n        >>> import transformers\n        >>> from peft import LoraConfig, PeftModel, get_peft_model, prepare_model_for_int8_training\n\n        >>> target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"out_proj\", \"fc_in\", \"fc_out\", \"wte\"]\n        >>> config = LoraConfig(\n        ...     r=4, lora_alpha=16, target_modules=target_modules, lora_dropout=0.1, bias=\"none\", task_type=\"CAUSAL_LM\"\n        ... )\n\n        >>> model = transformers.GPTJForCausalLM.from_pretrained(\n        ...     \"kakaobrain/kogpt\",\n        ...     revision=\"KoGPT6B-ryan1.5b-float16\",  # or float32 version: revision=KoGPT6B-ryan1.5b\n        ...     pad_token_id=tokenizer.eos_token_id,\n        ...     use_cache=False,\n        ...     device_map={\"\": rank},\n        ...     torch_dtype=torch.float16,\n        ...     load_in_8bit=True,\n        ... )\n        >>> model = prepare_model_for_int8_training(model)\n        >>> lora_model = get_peft_model(model, config)\n        ```\n\n    **Attributes**:\n        - **model** ([`~transformers.PreTrainedModel`]) -- The model to be adapted.\n        - **peft_config** ([`LoraConfig`]): The configuration of the Lora model.\n    \"\"\"\n\n    def __init__(self, model, config, adapter_name):\n        super().__init__()\n        self.model = model\n        self.forward = self.model.forward\n        self.peft_config = config\n        self.add_adapter(adapter_name, self.peft_config[adapter_name])\n\n    def add_adapter(self, adapter_name, config=None):\n        if config is not None:\n            model_config = self.model.config.to_dict() if hasattr(self.model.config, \"to_dict\") else self.model.config\n            config = self._prepare_lora_config(config, model_config)\n            self.peft_config[adapter_name] = config\n        self._find_and_replace(adapter_name)\n        if len(self.peft_config) > 1 and self.peft_config[adapter_name].bias != \"none\":\n            raise ValueError(\n                \"LoraModel supports only 1 adapter with bias. When using multiple adapters, set bias to 'none' for all adapters.\"\n            )\n        mark_only_lora_as_trainable(self.model, self.peft_config[adapter_name].bias)\n        if self.peft_config[adapter_name].inference_mode:\n            _freeze_adapter(self.model, adapter_name)\n\n    def _find_and_replace(self, adapter_name):\n        lora_config = self.peft_config[adapter_name]\n        loaded_in_8bit = getattr(self.model, \"is_loaded_in_8bit\", False)\n        if loaded_in_8bit and not is_bnb_available():\n            raise ImportError(\n                \"To use Lora with 8-bit quantization, please install the `bitsandbytes` package. \"\n                \"You can install it with `pip install bitsandbytes`.\"\n            )\n        is_target_modules_in_base_model = False\n        kwargs = {\n            \"r\": lora_config.r,\n            \"lora_alpha\": lora_config.lora_alpha,\n            \"lora_dropout\": lora_config.lora_dropout,\n            \"fan_in_fan_out\": lora_config.fan_in_fan_out,\n            \"init_lora_weights\": lora_config.init_lora_weights,\n        }\n        key_list = [key for key, _ in self.model.named_modules()]\n        for key in key_list:\n            if isinstance(lora_config.target_modules, str):\n                target_module_found = re.fullmatch(lora_config.target_modules, key)\n            else:\n                target_module_found = any(key.endswith(target_key) for target_key in lora_config.target_modules)\n            if target_module_found:\n                if not is_target_modules_in_base_model:\n                    is_target_modules_in_base_model = True\n                parent, target, target_name = _get_submodules(self.model, key)\n                if hasattr(target, \"bias\"):\n                    bias = target.bias is not None\n\n                if isinstance(target, LoraLayer):\n                    target.update_layer(\n                        adapter_name,\n                        lora_config.r,\n                        lora_config.lora_alpha,\n                        lora_config.lora_dropout,\n                        lora_config.init_lora_weights,\n                    )\n                else:\n                    if loaded_in_8bit and isinstance(target, bnb.nn.Linear8bitLt):\n                        eightbit_kwargs = kwargs.copy()\n                        eightbit_kwargs.update(\n                            {\n                                \"has_fp16_weights\": target.state.has_fp16_weights,\n                                \"memory_efficient_backward\": target.state.memory_efficient_backward,\n                                \"threshold\": target.state.threshold,\n                                \"index\": target.index,\n                            }\n                        )\n                        new_module = Linear8bitLt(\n                            adapter_name, target.in_features, target.out_features, bias=bias, **eightbit_kwargs\n                        )\n                    elif isinstance(target, torch.nn.Embedding):\n                        embedding_kwargs = kwargs.copy()\n                        embedding_kwargs.pop(\"fan_in_fan_out\", None)\n                        in_features, out_features = target.num_embeddings, target.embedding_dim\n                        new_module = Embedding(adapter_name, in_features, out_features, **embedding_kwargs)\n                    else:\n                        if isinstance(target, torch.nn.Linear):\n                            in_features, out_features = target.in_features, target.out_features\n                            if kwargs[\"fan_in_fan_out\"]:\n                                warnings.warn(\n                                    \"fan_in_fan_out is set to True but the target module is `torch.nn.Linear`. \"\n                                    \"Setting fan_in_fan_out to False.\"\n                                )\n                                kwargs[\"fan_in_fan_out\"] = lora_config.fan_in_fan_out = False\n                        elif isinstance(target, Conv1D):\n                            in_features, out_features = (\n                                target.weight.ds_shape if hasattr(target.weight, \"ds_shape\") else target.weight.shape\n                            )\n                            if not kwargs[\"fan_in_fan_out\"]:\n                                warnings.warn(\n                                    \"fan_in_fan_out is set to False but the target module is `Conv1D`. \"\n                                    \"Setting fan_in_fan_out to True.\"\n                                )\n                                kwargs[\"fan_in_fan_out\"] = lora_config.fan_in_fan_out = True\n                        else:\n                            raise ValueError(\n                                f\"Target module {target} is not supported. \"\n                                f\"Currently, only `torch.nn.Linear` and `Conv1D` are supported.\"\n                            )\n                        new_module = Linear(adapter_name, in_features, out_features, bias=bias, r_sum=lora_config.r_sum, **kwargs) # modified\n\n                    self._replace_module(parent, target_name, new_module, target)\n        if not is_target_modules_in_base_model:\n            raise ValueError(\n                f\"Target modules {lora_config.target_modules} not found in the base model. \"\n                f\"Please check the target modules and try again.\"\n            )\n\n    def _replace_module(self, parent_module, child_name, new_module, old_module):\n        setattr(parent_module, child_name, new_module)\n        new_module.weight = old_module.weight\n        if hasattr(old_module, \"bias\"):\n            if old_module.bias is not None:\n                new_module.bias = old_module.bias\n\n        if getattr(old_module, \"state\", None) is not None:\n            new_module.state = old_module.state\n            new_module.to(old_module.weight.device)\n\n        # dispatch to correct device\n        for name, module in new_module.named_modules():\n            if \"lora_\" in name:\n                module.to(old_module.weight.device)\n\n    def __getattr__(self, name: str):\n        \"\"\"Forward missing attributes to the wrapped module.\"\"\"\n        try:\n            return super().__getattr__(name)  # defer to nn.Module's logic\n        except AttributeError:\n            return getattr(self.model, name)\n\n    def get_peft_config_as_dict(self, inference: bool = False):\n        config_dict = {}\n        for key, value in self.peft_config.items():\n            config = {k: v.value if isinstance(v, Enum) else v for k, v in asdict(value).items()}\n            if inference:\n                config[\"inference_mode\"] = True\n        config_dict[key] = config\n        return config\n\n    def _set_adapter_layers(self, enabled=True):\n        for module in self.model.modules():\n            if isinstance(module, LoraLayer):\n                module.disable_adapters = False if enabled else True\n\n    def enable_adapter_layers(self):\n        self._set_adapter_layers(enabled=True)\n\n    def disable_adapter_layers(self):\n        self._set_adapter_layers(enabled=False)\n\n    def set_adapter(self, adapter_name):\n        for module in self.model.modules():\n            if isinstance(module, LoraLayer):\n                if module.merged:\n                    warnings.warn(\"Adapter cannot be set when the model is merged. Unmerging the model first.\")\n                    module.unmerge()\n                module.active_adapter = adapter_name\n\n    def merge_adapter(self):\n        for module in self.model.modules():\n            if isinstance(module, LoraLayer):\n                module.merge()\n\n    def unmerge_adapter(self):\n        for module in self.model.modules():\n            if isinstance(module, LoraLayer):\n                module.unmerge()\n\n    @staticmethod\n    def _prepare_lora_config(peft_config, model_config):\n        if peft_config.target_modules is None:\n            if model_config[\"model_type\"] not in TRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING:\n                raise ValueError(\"Please specify `target_modules` in `peft_config`\")\n            peft_config.target_modules = TRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING[model_config[\"model_type\"]]\n        if peft_config.inference_mode:\n            peft_config.merge_weights = True\n        return peft_config\n\n    def merge_and_unload(self):\n        r\"\"\"\n        This method merges the LoRa layers into the base model. This is needed if someone wants to use the base model\n        as a standalone model.\n        \"\"\"\n        if getattr(self.config, \"model_type\", None) == \"gpt2\":\n            raise ValueError(\"GPT2 models are not supported for merging LORA layers\")\n\n        if getattr(self.model, \"is_loaded_in_8bit\", False):\n            raise ValueError(\"Cannot merge LORA layers when the model is loaded in 8-bit mode\")\n\n        key_list = [key for key, _ in self.model.named_modules() if \"lora\" not in key]\n        for key in key_list:\n            try:\n                parent, target, target_name = _get_submodules(self.model, key)\n            except AttributeError:\n                continue\n            if isinstance(target, LoraLayer):\n                bias = target.bias is not None\n                new_module = torch.nn.Linear(target.in_features, target.out_features, bias=bias)\n                target.merge()\n                self._replace_module(parent, target_name, new_module, target)\n\n            # save any additional trainable modules part of `modules_to_save`\n            if isinstance(target, ModulesToSaveWrapper):\n                setattr(parent, target_name, target.modules_to_save[target.active_adapter])\n\n        return self.model\n\n    def add_weighted_adapter(self, adapters, weights, adapter_name):\n        if len({self.peft_config[adapter].r for adapter in adapters}) != 1:\n            raise ValueError(\"All adapters must have the same r value\")\n        self.peft_config[adapter_name] = self.peft_config[adapters[0]]\n        self.peft_config[adapter_name].lora_alpha = self.peft_config[adapters[0]].r\n        self._find_and_replace(adapter_name)\n        mark_only_lora_as_trainable(self.model, self.peft_config[adapter_name].bias)\n        _freeze_adapter(self.model, adapter_name)\n        key_list = [key for key, _ in self.model.named_modules() if \"lora\" not in key]\n        for key in key_list:\n            _, target, _ = _get_submodules(self.model, key)\n            if isinstance(target, LoraLayer):\n                if adapter_name in target.lora_A:\n                    target.lora_A[adapter_name].weight.data = target.lora_A[adapter_name].weight.data * 0.0\n                    target.lora_B[adapter_name].weight.data = target.lora_B[adapter_name].weight.data * 0.0\n                    for adapter, weight in zip(adapters, weights):\n                        if adapter not in target.lora_A:\n                            continue\n                        target.lora_A[adapter_name].weight.data += (\n                            target.lora_A[adapter].weight.data * weight * target.scaling[adapter]\n                        )\n                        target.lora_B[adapter_name].weight.data += target.lora_B[adapter].weight.data * weight\n\n                elif adapter_name in target.lora_embedding_A:\n                    target.lora_embedding_A[adapter_name].data = target.lora_embedding_A[adapter_name].data * 0.0\n                    target.lora_embedding_B[adapter_name].data = target.lora_embedding_B[adapter_name].data * 0.0\n                    for adapter, weight in zip(adapters, weights):\n                        if adapter not in target.lora_embedding_A:\n                            continue\n                        target.lora_embedding_A[adapter_name].data += (\n                            target.lora_embedding_A[adapter].data * weight * target.scaling[adapter]\n                        )\n                        target.lora_embedding_B[adapter_name].data += target.lora_embedding_B[adapter].data * weight\n\n\n# Below code is based on https://github.com/microsoft/LoRA/blob/main/loralib/layers.py\n# and modified to work with PyTorch FSDP\n\n\n#  ------------------------------------------------------------------------------------------\n#  Copyright (c) Microsoft Corporation. All rights reserved.\n#  Licensed under the MIT License (MIT). See LICENSE in the repo root for license information.\n#  ------------------------------------------------------------------------------------------\n\n\n# had to adapt it for `lora_only` to work\ndef mark_only_lora_as_trainable(model: nn.Module, bias: str = \"none\") -> None:\n    for n, p in model.named_parameters():\n        if \"lora_\" not in n:\n            p.requires_grad = False\n    if bias == \"none\":\n        return\n    elif bias == \"all\":\n        for n, p in model.named_parameters():\n            if \"bias\" in n:\n                p.requires_grad = True\n    elif bias == \"lora_only\":\n        for m in model.modules():\n            if isinstance(m, LoraLayer) and hasattr(m, \"bias\") and m.bias is not None:\n                m.bias.requires_grad = True\n    else:\n        raise NotImplementedError\n\n\nclass LoraLayer:\n    def __init__(\n        self,\n        in_features: int,\n        out_features: int,\n    ):\n        self.r = {}\n        self.lora_alpha = {}\n        self.scaling = {}\n        self.lora_dropout = nn.ModuleDict({})\n        self.lora_A = nn.ModuleDict({})\n        self.lora_B = nn.ModuleDict({})\n        # self.lorapre_A = nn.ModuleDict({}) # modified\n        # self.lorapre_B = nn.ModuleDict({}) # modified\n        self.loranew_A = nn.ModuleDict({}) # modified\n        self.loranew_B = nn.ModuleDict({}) # modified\n        # For Embedding layer\n        self.lora_embedding_A = nn.ParameterDict({})\n        self.lora_embedding_B = nn.ParameterDict({})\n        # Mark the weight as unmerged\n        self.merged = False\n        self.disable_adapters = False\n        self.in_features = in_features\n        self.out_features = out_features\n\n    def update_layer(self, adapter_name, r, lora_alpha, lora_dropout, init_lora_weights, r_sum): # modified \n        self.r[adapter_name] = r\n        self.lora_alpha[adapter_name] = lora_alpha\n        if lora_dropout > 0.0:\n            lora_dropout_layer = nn.Dropout(p=lora_dropout)\n        else:\n            lora_dropout_layer = nn.Identity()\n\n        self.lora_dropout.update(nn.ModuleDict({adapter_name: lora_dropout_layer}))\n        # Actual trainable parameters\n        if r > 0:\n            self.loranew_A.update(nn.ModuleDict({adapter_name: nn.Linear(self.in_features, r, bias=False)})) # modified\n            self.loranew_B.update(nn.ModuleDict({adapter_name: nn.Linear(r, self.out_features, bias=False)})) # modified\n            self.lora_A.update(nn.ModuleDict({adapter_name: nn.Linear(self.in_features, r_sum, bias=False)})) # modified\n            self.lora_B.update(nn.ModuleDict({adapter_name: nn.Linear(r_sum, self.out_features, bias=False)})) # modified\n            # self.lorapre_A.update(nn.ModuleDict({adapter_name: nn.Linear(self.in_features, r_sum, bias=False)})) # modified\n            # self.lorapre_B.update(nn.ModuleDict({adapter_name: nn.Linear(r_sum, self.out_features, bias=False)})) # modified\n            self.scaling[adapter_name] = lora_alpha / r\n        if init_lora_weights:\n            self.reset_lora_parameters(adapter_name)\n        self.to(self.weight.device)\n\n    def update_layer_embedding(self, adapter_name, r, lora_alpha, lora_dropout, init_lora_weights):\n        self.r[adapter_name] = r\n        self.lora_alpha[adapter_name] = lora_alpha\n        if lora_dropout > 0.0:\n            lora_dropout_layer = nn.Dropout(p=lora_dropout)\n        else:\n            lora_dropout_layer = nn.Identity()\n\n        self.lora_dropout.update(nn.ModuleDict({adapter_name: lora_dropout_layer}))\n        # Actual trainable parameters\n        if r > 0:\n            self.lora_embedding_A.update(\n                nn.ParameterDict({adapter_name: nn.Parameter(self.weight.new_zeros((r, self.in_features)))})\n            )\n            self.lora_embedding_B.update(\n                nn.ParameterDict({adapter_name: nn.Parameter(self.weight.new_zeros((self.out_features, r)))})\n            )\n            self.scaling[adapter_name] = lora_alpha / r\n        if init_lora_weights:\n            self.reset_lora_parameters(adapter_name)\n        self.to(self.weight.device)\n\n    def reset_lora_parameters(self, adapter_name):\n\n        # modified\n        if adapter_name in self.lora_A.keys(): \n            # initialize A and B to zero\n            nn.init.zeros_(self.lora_A[adapter_name].weight)\n            nn.init.zeros_(self.lora_B[adapter_name].weight)\n\n        if adapter_name in self.lora_embedding_A.keys():\n            # initialize a the same way as the default for nn.linear and b to zero\n            nn.init.zeros_(self.lora_embedding_A[adapter_name])\n            nn.init.normal_(self.lora_embedding_B[adapter_name])\n\n        # modified\n        if adapter_name in self.loranew_A.keys(): \n            nn.init.kaiming_uniform_(self.loranew_A[adapter_name].weight, a=math.sqrt(5))\n            nn.init.zeros_(self.loranew_B[adapter_name].weight)\n\n        # # modified\n        # if adapter_name in self.lorapre_A.keys(): \n        #     nn.init.zeros_(self.lorapre_A[adapter_name].weight)\n        #     nn.init.zeros_(self.lorapre_B[adapter_name].weight)\n\n\nclass Linear(nn.Linear, LoraLayer):\n    # Lora implemented in a dense layer\n    def __init__(\n        self,\n        adapter_name: str,\n        in_features: int,\n        out_features: int,\n        r: int = 0,\n        lora_alpha: int = 1,\n        lora_dropout: float = 0.0,\n        fan_in_fan_out: bool = False,  # Set this to True if the layer to replace stores weight like (fan_in, fan_out)\n        r_sum: int = 0, # modified\n        **kwargs,\n    ):\n        init_lora_weights = kwargs.pop(\"init_lora_weights\", True)\n\n        nn.Linear.__init__(self, in_features, out_features, **kwargs)\n        LoraLayer.__init__(self, in_features=in_features, out_features=out_features)\n        # Freezing the pre-trained weight matrix\n        self.weight.requires_grad = False\n\n        self.fan_in_fan_out = fan_in_fan_out\n        if fan_in_fan_out:\n            self.weight.data = self.weight.data.T\n\n        nn.Linear.reset_parameters(self)\n        self.update_layer(adapter_name, r, lora_alpha, lora_dropout, init_lora_weights, r_sum) # modified\n        self.active_adapter = adapter_name\n\n    def merge(self):\n        if self.active_adapter not in self.lora_A.keys():\n            return\n        if self.merged:\n            warnings.warn(\"Already merged. Nothing to do.\")\n            return\n        if self.r[self.active_adapter] > 0:\n            self.weight.data += (\n                transpose(\n                    self.lora_B[self.active_adapter].weight @ self.lora_A[self.active_adapter].weight,\n                    self.fan_in_fan_out,\n                )\n                * self.scaling[self.active_adapter]\n            )\n            self.merged = True\n\n    def unmerge(self):\n        if self.active_adapter not in self.lora_A.keys():\n            return\n        if not self.merged:\n            warnings.warn(\"Already unmerged. Nothing to do.\")\n            return\n        if self.r[self.active_adapter] > 0:\n            self.weight.data -= (\n                transpose(\n                    self.lora_B[self.active_adapter].weight @ self.lora_A[self.active_adapter].weight,\n                    self.fan_in_fan_out,\n                )\n                * self.scaling[self.active_adapter]\n            )\n            self.merged = False\n\n    def forward(self, x: torch.Tensor):\n        previous_dtype = x.dtype\n\n        if self.active_adapter not in self.lora_A.keys():\n            return F.linear(x, transpose(self.weight, self.fan_in_fan_out), bias=self.bias)\n        if self.disable_adapters:\n            if self.r[self.active_adapter] > 0 and self.merged:\n                self.unmerge()\n            result = F.linear(x, transpose(self.weight, self.fan_in_fan_out), bias=self.bias)\n        elif self.r[self.active_adapter] > 0 and not self.merged:\n            result = F.linear(x, transpose(self.weight, self.fan_in_fan_out), bias=self.bias)\n\n            x = x.to(self.lora_A[self.active_adapter].weight.dtype)\n            x = self.lora_dropout[self.active_adapter](x)\n\n            result += (\n                self.lora_B[self.active_adapter](\n                    self.lora_A[self.active_adapter](x)\n                )\n                * self.scaling[self.active_adapter]\n            )\n\n            # modified\n            result += (\n                self.loranew_B[self.active_adapter](\n                    self.loranew_A[self.active_adapter](x)\n                )\n                * self.scaling[self.active_adapter] \n            )\n            \n        else:\n            result = F.linear(x, transpose(self.weight, self.fan_in_fan_out), bias=self.bias)\n\n        result = result.to(previous_dtype)\n\n        return result\n\n\nclass Embedding(nn.Embedding, LoraLayer):\n    # LoRA implemented in a Embedding layer\n    def __init__(\n        self,\n        adapter_name: str,\n        num_embeddings: int,\n        embedding_dim: int,\n        r: int = 0,\n        lora_alpha: int = 1,\n        lora_dropout: float = 0.0,\n        **kwargs,\n    ):\n        init_lora_weights = kwargs.pop(\"init_lora_weights\", True)\n\n        nn.Embedding.__init__(self, num_embeddings, embedding_dim, **kwargs)\n        LoraLayer.__init__(self, in_features=num_embeddings, out_features=embedding_dim)\n\n        self.weight.requires_grad = False\n\n        nn.Embedding.reset_parameters(self)\n        self.update_layer_embedding(adapter_name, r, lora_alpha, lora_dropout, init_lora_weights)\n        self.active_adapter = adapter_name\n\n    def unmerge(self, mode: bool = True):\n        if not self.merged:\n            warnings.warn(\"Already unmerged. Nothing to do.\")\n            return\n        if self.r[self.active_adapter] > 0:\n            self.weight.data -= (\n                transpose(\n                    self.lora_embedding_B[self.active_adapter] @ self.lora_embedding_A[self.active_adapter], True\n                )\n                * self.scaling[self.active_adapter]\n            )\n            self.merged = False\n\n    def merge(self):\n        if self.merged:\n            warnings.warn(\"Already merged. Nothing to do.\")\n            return\n        if self.r[self.active_adapter] > 0:\n            self.weight.data += (\n                transpose(\n                    self.lora_embedding_B[self.active_adapter] @ self.lora_embedding_A[self.active_adapter], True\n                )\n                * self.scaling[self.active_adapter]\n            )\n            self.merged = True\n\n    def forward(self, x: torch.Tensor):\n        if self.disable_adapters:\n            if self.r[self.active.adapter] > 0 and self.merged:\n                self.weight.data -= (\n                    transpose(\n                        self.lora_embedding_B[self.active_adapter].weight\n                        @ self.lora_embedding_A[self.active_adapter].weight,\n                        True,\n                    )\n                    * self.scaling[self.active_adapter]\n                )\n                self.merged = False\n            return nn.Embedding.forward(self, x)\n\n        elif self.r[self.active_adapter] > 0 and not self.merged:\n            result = nn.Embedding.forward(self, x)\n            if self.r[self.active_adapter] > 0:\n                after_A = F.embedding(\n                    x,\n                    self.lora_embedding_A[self.active_adapter].T,\n                    self.padding_idx,\n                    self.max_norm,\n                    self.norm_type,\n                    self.scale_grad_by_freq,\n                    self.sparse,\n                )\n                result += (after_A @ self.lora_embedding_B[self.active_adapter].T) * self.scaling[self.active_adapter]\n            return result\n        else:\n            return nn.Embedding.forward(self, x)\n\n\nif is_bnb_available():\n\n    class Linear8bitLt(bnb.nn.Linear8bitLt, LoraLayer):\n        # Lora implemented in a dense layer\n        def __init__(\n            self,\n            adapter_name,\n            in_features,\n            out_features,\n            r: int = 0,\n            lora_alpha: int = 1,\n            lora_dropout: float = 0.0,\n            **kwargs,\n        ):\n            bnb.nn.Linear8bitLt.__init__(\n                self,\n                in_features,\n                out_features,\n                bias=kwargs.get(\"bias\", True),\n                has_fp16_weights=kwargs.get(\"has_fp16_weights\", True),\n                memory_efficient_backward=kwargs.get(\"memory_efficient_backward\", False),\n                threshold=kwargs.get(\"threshold\", 0.0),\n                index=kwargs.get(\"index\", None),\n            )\n            LoraLayer.__init__(self, in_features=in_features, out_features=out_features)\n\n            # Freezing the pre-trained weight matrix\n            self.weight.requires_grad = False\n\n            init_lora_weights = kwargs.pop(\"init_lora_weights\", True)\n            self.update_layer(adapter_name, r, lora_alpha, lora_dropout, init_lora_weights)\n            self.active_adapter = adapter_name\n\n        def forward(self, x: torch.Tensor):\n            result = super().forward(x)\n\n            if self.disable_adapters or self.active_adapter not in self.lora_A.keys():\n                return result\n            elif self.r[self.active_adapter] > 0:\n                if not torch.is_autocast_enabled():\n                    expected_dtype = result.dtype\n\n                    if x.dtype != torch.float32:\n                        x = x.float()\n                    output = (\n                        self.lora_B[self.active_adapter](\n                            self.lora_A[self.active_adapter](self.lora_dropout[self.active_adapter](x))\n                        ).to(expected_dtype)\n                        * self.scaling[self.active_adapter]\n                    )\n                else:\n                    output = (\n                        self.lora_B[self.active_adapter](\n                            self.lora_A[self.active_adapter](self.lora_dropout[self.active_adapter](x))\n                        )\n                        * self.scaling[self.active_adapter]\n                    )\n                result += output\n            return result\n"}
{"type": "source_file", "path": "pseudo_data/src/peft_1/tuners/p_tuning.py", "content": "# coding=utf-8\n# Copyright 2023-present the HuggingFace Inc. team.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport enum\nimport warnings\nfrom dataclasses import dataclass, field\nfrom typing import Union\n\nimport torch\n\nfrom ..utils import PeftType, PromptLearningConfig\n\n\nclass PromptEncoderReparameterizationType(str, enum.Enum):\n    MLP = \"MLP\"\n    LSTM = \"LSTM\"\n\n\n@dataclass\nclass PromptEncoderConfig(PromptLearningConfig):\n    \"\"\"\n    This is the configuration class to store the configuration of a [`PromptEncoder`].\n\n    Args:\n        encoder_reparameterization_type (Union[[`PromptEncoderReparameterizationType`], `str`]):\n            The type of reparameterization to use.\n        encoder_hidden_size (`int`): The hidden size of the prompt encoder.\n        encoder_num_layers (`int`): The number of layers of the prompt encoder.\n        encoder_dropout (`float`): The dropout probability of the prompt encoder.\n    \"\"\"\n\n    encoder_reparameterization_type: Union[str, PromptEncoderReparameterizationType] = field(\n        default=PromptEncoderReparameterizationType.MLP,\n        metadata={\"help\": \"How to reparameterize the prompt encoder\"},\n    )\n    encoder_hidden_size: int = field(\n        default=None,\n        metadata={\"help\": \"The hidden size of the prompt encoder\"},\n    )\n    encoder_num_layers: int = field(\n        default=2,\n        metadata={\"help\": \"The number of layers of the prompt encoder\"},\n    )\n    encoder_dropout: float = field(\n        default=0.0,\n        metadata={\"help\": \"The dropout of the prompt encoder\"},\n    )\n\n    def __post_init__(self):\n        self.peft_type = PeftType.P_TUNING\n\n\n# Based on https://github.com/NVIDIA/NeMo/blob/main/nemo/collections/nlp/modules/common/prompt_encoder.py\n# with some refactor\nclass PromptEncoder(torch.nn.Module):\n    \"\"\"\n    The prompt encoder network that is used to generate the virtual token embeddings for p-tuning.\n\n    Args:\n        config ([`PromptEncoderConfig`]): The configuration of the prompt encoder.\n\n    Example:\n\n    ```py\n    >>> from peft import PromptEncoder, PromptEncoderConfig\n\n    >>> config = PromptEncoderConfig(\n    ...     peft_type=\"P_TUNING\",\n    ...     task_type=\"SEQ_2_SEQ_LM\",\n    ...     num_virtual_tokens=20,\n    ...     token_dim=768,\n    ...     num_transformer_submodules=1,\n    ...     num_attention_heads=12,\n    ...     num_layers=12,\n    ...     encoder_reparameterization_type=\"MLP\",\n    ...     encoder_hidden_size=768,\n    ... )\n\n    >>> prompt_encoder = PromptEncoder(config)\n    ```\n\n    **Attributes**:\n        - **embedding** (`torch.nn.Embedding`) -- The embedding layer of the prompt encoder.\n        - **mlp_head** (`torch.nn.Sequential`) -- The MLP head of the prompt encoder if `inference_mode=False`.\n        - **lstm_head** (`torch.nn.LSTM`) -- The LSTM head of the prompt encoder if `inference_mode=False` and\n        `encoder_reparameterization_type=\"LSTM\"`.\n        - **token_dim** (`int`) -- The hidden embedding dimension of the base transformer model.\n        - **input_size** (`int`) -- The input size of the prompt encoder.\n        - **output_size** (`int`) -- The output size of the prompt encoder.\n        - **hidden_size** (`int`) -- The hidden size of the prompt encoder.\n        - **total_virtual_tokens** (`int`): The total number of virtual tokens of the\n        prompt encoder.\n        - **encoder_type** (Union[[`PromptEncoderReparameterizationType`], `str`]): The encoder type of the prompt\n          encoder.\n\n\n    Input shape: (`batch_size`, `total_virtual_tokens`)\n\n    Output shape: (`batch_size`, `total_virtual_tokens`, `token_dim`)\n    \"\"\"\n\n    def __init__(self, config):\n        super().__init__()\n        self.token_dim = config.token_dim\n        self.input_size = self.token_dim\n        self.output_size = self.token_dim\n        self.hidden_size = config.encoder_hidden_size\n        self.total_virtual_tokens = config.num_virtual_tokens * config.num_transformer_submodules\n        self.encoder_type = config.encoder_reparameterization_type\n\n        # embedding\n        self.embedding = torch.nn.Embedding(self.total_virtual_tokens, self.token_dim)\n        if not config.inference_mode:\n            if self.encoder_type == PromptEncoderReparameterizationType.LSTM:\n                lstm_dropout = config.encoder_dropout\n                num_layers = config.encoder_num_layers\n                # LSTM\n                self.lstm_head = torch.nn.LSTM(\n                    input_size=self.input_size,\n                    hidden_size=self.hidden_size,\n                    num_layers=num_layers,\n                    dropout=lstm_dropout,\n                    bidirectional=True,\n                    batch_first=True,\n                )\n\n                self.mlp_head = torch.nn.Sequential(\n                    torch.nn.Linear(self.hidden_size * 2, self.hidden_size * 2),\n                    torch.nn.ReLU(),\n                    torch.nn.Linear(self.hidden_size * 2, self.output_size),\n                )\n\n            elif self.encoder_type == PromptEncoderReparameterizationType.MLP:\n                warnings.warn(\n                    f\"for {self.encoder_type}, the `encoder_num_layers` is ignored. Exactly 2 MLP layers are used.\"\n                )\n                layers = [\n                    torch.nn.Linear(self.input_size, self.hidden_size),\n                    torch.nn.ReLU(),\n                    torch.nn.Linear(self.hidden_size, self.hidden_size),\n                    torch.nn.ReLU(),\n                    torch.nn.Linear(self.hidden_size, self.output_size),\n                ]\n                self.mlp_head = torch.nn.Sequential(*layers)\n\n            else:\n                raise ValueError(\"Prompt encoder type not recognized. Please use one of MLP (recommended) or LSTM.\")\n\n    def forward(self, indices):\n        input_embeds = self.embedding(indices)\n        if self.encoder_type == PromptEncoderReparameterizationType.LSTM:\n            output_embeds = self.mlp_head(self.lstm_head(input_embeds)[0])\n        elif self.encoder_type == PromptEncoderReparameterizationType.MLP:\n            output_embeds = self.mlp_head(input_embeds)\n        else:\n            raise ValueError(\"Prompt encoder type not recognized. Please use one of MLP (recommended) or LSTM.\")\n\n        return output_embeds\n"}
{"type": "source_file", "path": "pseudo_data/src/peft_1/tuners/prefix_tuning.py", "content": "# coding=utf-8\n# Copyright 2023-present the HuggingFace Inc. team.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nfrom dataclasses import dataclass, field\n\nimport torch\n\nfrom ..utils import PeftType, PromptLearningConfig\n\n\n@dataclass\nclass PrefixTuningConfig(PromptLearningConfig):\n    \"\"\"\n    This is the configuration class to store the configuration of a [`PrefixEncoder`].\n\n    Args:\n        encoder_hidden_size (`int`): The hidden size of the prompt encoder.\n        prefix_projection (`bool`): Whether to project the prefix embeddings.\n    \"\"\"\n\n    encoder_hidden_size: int = field(\n        default=None,\n        metadata={\"help\": \"The hidden size of the encoder\"},\n    )\n    prefix_projection: bool = field(\n        default=False,\n        metadata={\"help\": \"Whether to project the prefix tokens\"},\n    )\n\n    def __post_init__(self):\n        self.peft_type = PeftType.PREFIX_TUNING\n\n\n# Based on https://github.com/THUDM/P-tuning-v2/blob/main/model/prefix_encoder.py\n# with some refactor\nclass PrefixEncoder(torch.nn.Module):\n    r\"\"\"\n    The `torch.nn` model to encode the prefix.\n\n    Args:\n        config ([`PrefixTuningConfig`]): The configuration of the prefix encoder.\n\n    Example:\n\n    ```py\n    >>> from peft import PrefixEncoder, PrefixTuningConfig\n\n    >>> config = PrefixTuningConfig(\n    ...     peft_type=\"PREFIX_TUNING\",\n    ...     task_type=\"SEQ_2_SEQ_LM\",\n    ...     num_virtual_tokens=20,\n    ...     token_dim=768,\n    ...     num_transformer_submodules=1,\n    ...     num_attention_heads=12,\n    ...     num_layers=12,\n    ...     encoder_hidden_size=768,\n    ... )\n    >>> prefix_encoder = PrefixEncoder(config)\n    ```\n\n    **Attributes**:\n        - **embedding** (`torch.nn.Embedding`) -- The embedding layer of the prefix encoder.\n        - **transform** (`torch.nn.Sequential`) -- The two-layer MLP to transform the prefix embeddings if\n          `prefix_projection` is `True`.\n        - **prefix_projection** (`bool`) -- Whether to project the prefix embeddings.\n\n    Input shape: (`batch_size`, `num_virtual_tokens`)\n\n    Output shape: (`batch_size`, `num_virtual_tokens`, `2*layers*hidden`)\n    \"\"\"\n\n    def __init__(self, config):\n        super().__init__()\n        self.prefix_projection = config.prefix_projection\n        token_dim = config.token_dim\n        num_layers = config.num_layers\n        encoder_hidden_size = config.encoder_hidden_size\n        num_virtual_tokens = config.num_virtual_tokens\n        if self.prefix_projection and not config.inference_mode:\n            # Use a two-layer MLP to encode the prefix\n            self.embedding = torch.nn.Embedding(num_virtual_tokens, token_dim)\n            self.transform = torch.nn.Sequential(\n                torch.nn.Linear(token_dim, encoder_hidden_size),\n                torch.nn.Tanh(),\n                torch.nn.Linear(encoder_hidden_size, num_layers * 2 * token_dim),\n            )\n        else:\n            self.embedding = torch.nn.Embedding(num_virtual_tokens, num_layers * 2 * token_dim)\n\n    def forward(self, prefix: torch.Tensor):\n        if self.prefix_projection:\n            prefix_tokens = self.embedding(prefix)\n            past_key_values = self.transform(prefix_tokens)\n        else:\n            past_key_values = self.embedding(prefix)\n        return past_key_values\n"}
{"type": "source_file", "path": "pseudo_data/src/peft_1/tuners/prompt_tuning.py", "content": "# coding=utf-8\n# Copyright 2023-present the HuggingFace Inc. team.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport enum\nimport math\nfrom dataclasses import dataclass, field\nfrom typing import Optional, Union\n\nimport torch\n\nfrom ..utils import PeftType, PromptLearningConfig\n\n\nclass PromptTuningInit(str, enum.Enum):\n    TEXT = \"TEXT\"\n    RANDOM = \"RANDOM\"\n\n\n@dataclass\nclass PromptTuningConfig(PromptLearningConfig):\n    \"\"\"\n    This is the configuration class to store the configuration of a [`PromptEmbedding`].\n\n    Args:\n        prompt_tuning_init (Union[[`PromptTuningInit`], `str`]): The initialization of the prompt embedding.\n        prompt_tuning_init_text (`str`, *optional*):\n            The text to initialize the prompt embedding. Only used if `prompt_tuning_init` is `TEXT`.\n        tokenizer_name_or_path (`str`, *optional*):\n            The name or path of the tokenizer. Only used if `prompt_tuning_init` is `TEXT`.\n    \"\"\"\n\n    prompt_tuning_init: Union[PromptTuningInit, str] = field(\n        default=PromptTuningInit.RANDOM,\n        metadata={\"help\": \"How to initialize the prompt tuning parameters\"},\n    )\n    prompt_tuning_init_text: Optional[str] = field(\n        default=None,\n        metadata={\n            \"help\": \"The text to use for prompt tuning initialization. Only used if prompt_tuning_init is `TEXT`\"\n        },\n    )\n    tokenizer_name_or_path: Optional[str] = field(\n        default=None,\n        metadata={\n            \"help\": \"The tokenizer to use for prompt tuning initialization. Only used if prompt_tuning_init is `TEXT`\"\n        },\n    )\n\n    def __post_init__(self):\n        self.peft_type = PeftType.PROMPT_TUNING\n\n\nclass PromptEmbedding(torch.nn.Module):\n    \"\"\"\n    The model to encode virtual tokens into prompt embeddings.\n\n    Args:\n        config ([`PromptTuningConfig`]): The configuration of the prompt embedding.\n        word_embeddings (`torch.nn.Module`): The word embeddings of the base transformer model.\n\n    **Attributes**:\n        - **embedding** (`torch.nn.Embedding`) -- The embedding layer of the prompt embedding.\n\n    Example:\n\n    ```py\n    >>> from peft import PromptEmbedding, PromptTuningConfig\n\n    >>> config = PromptTuningConfig(\n    ...     peft_type=\"PROMPT_TUNING\",\n    ...     task_type=\"SEQ_2_SEQ_LM\",\n    ...     num_virtual_tokens=20,\n    ...     token_dim=768,\n    ...     num_transformer_submodules=1,\n    ...     num_attention_heads=12,\n    ...     num_layers=12,\n    ...     prompt_tuning_init=\"TEXT\",\n    ...     prompt_tuning_init_text=\"Predict if sentiment of this review is positive, negative or neutral\",\n    ...     tokenizer_name_or_path=\"t5-base\",\n    ... )\n\n    >>> # t5_model.shared is the word embeddings of the base model\n    >>> prompt_embedding = PromptEmbedding(config, t5_model.shared)\n    ```\n\n    Input Shape: (`batch_size`, `total_virtual_tokens`)\n\n    Output Shape: (`batch_size`, `total_virtual_tokens`, `token_dim`)\n    \"\"\"\n\n    def __init__(self, config, word_embeddings):\n        super().__init__()\n\n        total_virtual_tokens = config.num_virtual_tokens * config.num_transformer_submodules\n        self.embedding = torch.nn.Embedding(total_virtual_tokens, config.token_dim)\n        if config.prompt_tuning_init == PromptTuningInit.TEXT:\n            from transformers import AutoTokenizer\n\n            tokenizer = AutoTokenizer.from_pretrained(config.tokenizer_name_or_path)\n            init_text = config.prompt_tuning_init_text\n            init_token_ids = tokenizer(init_text)[\"input_ids\"]\n            # Trim or iterate until num_text_tokens matches total_virtual_tokens\n            num_text_tokens = len(init_token_ids)\n            if num_text_tokens > total_virtual_tokens:\n                init_token_ids = init_token_ids[:total_virtual_tokens]\n            elif num_text_tokens < total_virtual_tokens:\n                num_reps = math.ceil(total_virtual_tokens / num_text_tokens)\n                init_token_ids = init_token_ids * num_reps\n            init_token_ids = init_token_ids[:total_virtual_tokens]\n\n            word_embedding_weights = word_embeddings(torch.LongTensor(init_token_ids)).detach().clone()\n            word_embedding_weights = word_embedding_weights.to(torch.float32)\n            self.embedding.weight = torch.nn.Parameter(word_embedding_weights)\n\n    def forward(self, indices):\n        # Just get embeddings\n        prompt_embeddings = self.embedding(indices)\n        return prompt_embeddings\n"}
{"type": "source_file", "path": "pseudo_data/src/peft_1/mapping.py", "content": "# coding=utf-8\n# Copyright 2023-present the HuggingFace Inc. team.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom .peft_model import (\n    PeftModel,\n    PeftModelForCausalLM,\n    PeftModelForSeq2SeqLM,\n    PeftModelForSequenceClassification,\n    PeftModelForTokenClassification,\n)\nfrom .tuners import (\n    AdaLoraConfig,\n    AdaptionPromptConfig,\n    LoraConfig,\n    PrefixTuningConfig,\n    PromptEncoderConfig,\n    PromptTuningConfig,\n)\nfrom .utils import PromptLearningConfig\n\n\nMODEL_TYPE_TO_PEFT_MODEL_MAPPING = {\n    \"SEQ_CLS\": PeftModelForSequenceClassification,\n    \"SEQ_2_SEQ_LM\": PeftModelForSeq2SeqLM,\n    \"CAUSAL_LM\": PeftModelForCausalLM,\n    \"TOKEN_CLS\": PeftModelForTokenClassification,\n}\n\nPEFT_TYPE_TO_CONFIG_MAPPING = {\n    \"ADAPTION_PROMPT\": AdaptionPromptConfig,\n    \"PROMPT_TUNING\": PromptTuningConfig,\n    \"PREFIX_TUNING\": PrefixTuningConfig,\n    \"P_TUNING\": PromptEncoderConfig,\n    \"LORA\": LoraConfig,\n    \"ADALORA\": AdaLoraConfig,\n}\n\n\ndef get_peft_config(config_dict):\n    \"\"\"\n    Returns a Peft config object from a dictionary.\n\n    Args:\n        config_dict (`Dict[str, Any]`): Dictionary containing the configuration parameters.\n    \"\"\"\n\n    return PEFT_TYPE_TO_CONFIG_MAPPING[config_dict[\"peft_type\"]](**config_dict)\n\n\ndef _prepare_prompt_learning_config(peft_config, model_config):\n    if peft_config.num_layers is None:\n        if \"num_hidden_layers\" in model_config:\n            num_layers = model_config[\"num_hidden_layers\"]\n        elif \"num_layers\" in model_config:\n            num_layers = model_config[\"num_layers\"]\n        elif \"n_layer\" in model_config:\n            num_layers = model_config[\"n_layer\"]\n        else:\n            raise ValueError(\"Please specify `num_layers` in `peft_config`\")\n        peft_config.num_layers = num_layers\n\n    if peft_config.token_dim is None:\n        if \"hidden_size\" in model_config:\n            token_dim = model_config[\"hidden_size\"]\n        elif \"n_embd\" in model_config:\n            token_dim = model_config[\"n_embd\"]\n        elif \"d_model\" in model_config:\n            token_dim = model_config[\"d_model\"]\n        else:\n            raise ValueError(\"Please specify `token_dim` in `peft_config`\")\n        peft_config.token_dim = token_dim\n\n    if peft_config.num_attention_heads is None:\n        if \"num_attention_heads\" in model_config:\n            num_attention_heads = model_config[\"num_attention_heads\"]\n        elif \"n_head\" in model_config:\n            num_attention_heads = model_config[\"n_head\"]\n        elif \"num_heads\" in model_config:\n            num_attention_heads = model_config[\"num_heads\"]\n        elif \"encoder_attention_heads\" in model_config:\n            num_attention_heads = model_config[\"encoder_attention_heads\"]\n        else:\n            raise ValueError(\"Please specify `num_attention_heads` in `peft_config`\")\n        peft_config.num_attention_heads = num_attention_heads\n\n    if getattr(peft_config, \"encoder_hidden_size\", None) is None:\n        setattr(peft_config, \"encoder_hidden_size\", token_dim)\n\n    return peft_config\n\n\ndef get_peft_model(model, peft_config):\n    \"\"\"\n    Returns a Peft model object from a model and a config.\n\n    Args:\n        model ([`transformers.PreTrainedModel`]): Model to be wrapped.\n        peft_config ([`PeftConfig`]): Configuration object containing the parameters of the Peft model.\n    \"\"\"\n    model_config = model.config.to_dict() if hasattr(model.config, \"to_dict\") else model.config\n    peft_config.base_model_name_or_path = model.__dict__.get(\"name_or_path\", None)\n    if peft_config.task_type not in MODEL_TYPE_TO_PEFT_MODEL_MAPPING.keys() and not isinstance(\n        peft_config, PromptLearningConfig\n    ):\n        return PeftModel(model, peft_config)\n    if isinstance(peft_config, PromptLearningConfig):\n        peft_config = _prepare_prompt_learning_config(peft_config, model_config)\n    return MODEL_TYPE_TO_PEFT_MODEL_MAPPING[peft_config.task_type](model, peft_config)\n"}
{"type": "source_file", "path": "pseudo_data/src/peft_1/import_utils.py", "content": "# coding=utf-8\n# Copyright 2023-present the HuggingFace Inc. team.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport importlib\n\n\ndef is_bnb_available():\n    return importlib.util.find_spec(\"bitsandbytes\") is not None\n"}
{"type": "source_file", "path": "pseudo_data/src/model/llama.py", "content": "import warnings\nfrom typing import Optional, Tuple\nimport torch.nn.functional as F\nimport torch\nfrom torch.nn import CrossEntropyLoss\nfrom transformers import add_start_docstrings\nfrom transformers.modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast, SequenceClassifierOutputWithPast\nfrom transformers.models.llama.modeling_llama import LlamaForCausalLM\nfrom typing import List, Optional, Tuple, Union\n\n\nclass LlamaForCausalLM_with_lossmask(LlamaForCausalLM):\n\n\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_values: Optional[List[torch.FloatTensor]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        labels: Optional[torch.LongTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n        loss_mask: Optional[torch.Tensor] = None,\n\n    ) -> Union[Tuple, CausalLMOutputWithPast]:\n        r\"\"\"\n        Args:\n            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n\n        Returns:\n\n        Example:\n\n        ```python\n        >>> from transformers import AutoTokenizer, LlamaForCausalLM\n\n        >>> model = LlamaForCausalLM.from_pretrained(PATH_TO_CONVERTED_WEIGHTS)\n        >>> tokenizer = AutoTokenizer.from_pretrained(PATH_TO_CONVERTED_TOKENIZER)\n\n        >>> prompt = \"Hey, are you consciours? Can you talk to me?\"\n        >>> inputs = tokenizer(prompt, return_tensors=\"pt\")\n\n        >>> # Generate\n        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)\n        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n        \"Hey, are you consciours? Can you talk to me?\\nI'm not consciours, but I can talk to you.\"\n        ```\"\"\"\n\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n        outputs = self.model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            past_key_values=past_key_values,\n            inputs_embeds=inputs_embeds,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        hidden_states = outputs[0]\n        logits = self.lm_head(hidden_states)\n\n        # loss = None\n        # if labels is not None:\n        #     # Shift so that tokens < n predict n\n        #     shift_logits = logits[..., :-1, :].contiguous()\n        #     shift_labels = labels[..., 1:].contiguous()\n        #     # Flatten the tokens\n        #     loss_fct = CrossEntropyLoss()\n        #     shift_logits = shift_logits.view(-1, self.config.vocab_size)\n        #     shift_labels = shift_labels.view(-1)\n        #     # Enable model parallelism\n        #     shift_labels = shift_labels.to(shift_logits.device)\n        #     loss = loss_fct(shift_logits, shift_labels)\n\n        loss = None\n        if labels is not None:\n            # Shift so that tokens < n predict n\n            shift_logits = logits[..., :-1, :].contiguous()\n            shift_labels = labels[..., 1:].contiguous()\n            batch_size, seq_length, vocab_size = shift_logits.shape\n            # Flatten the tokens\n            loss = F.cross_entropy(shift_logits.view(batch_size * seq_length, vocab_size), shift_labels.view(batch_size * seq_length),reduction='none')\n            if loss_mask != None:\n                loss = loss * loss_mask[..., :-1].contiguous().view(-1)\n            loss = loss.sum()/loss_mask.sum()\n\n        if not return_dict:\n            output = (logits,) + outputs[1:]\n            return (loss,) + output if loss is not None else output\n\n        return CausalLMOutputWithPast(\n            loss=loss,\n            logits=logits,\n            past_key_values=outputs.past_key_values,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )\n"}
{"type": "source_file", "path": "pseudo_data/src/peft_1/peft_model.py", "content": "# coding=utf-8\n# Copyright 2023-present the HuggingFace Inc. team.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport inspect\nimport os\nimport warnings\nfrom contextlib import contextmanager\n\nimport torch\nfrom accelerate import dispatch_model, infer_auto_device_map\nfrom accelerate.hooks import AlignDevicesHook, add_hook_to_module, remove_hook_from_submodules\nfrom accelerate.utils import get_balanced_memory\nfrom huggingface_hub import hf_hub_download\nfrom torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\nfrom transformers import PreTrainedModel\nfrom transformers.modeling_outputs import SequenceClassifierOutput, TokenClassifierOutput\nfrom transformers.utils import PushToHubMixin\n\nfrom .tuners import (\n    AdaLoraModel,\n    AdaptionPromptModel,\n    LoraModel,\n    PrefixEncoder,\n    PromptEmbedding,\n    PromptEncoder,\n)\nfrom .utils import (\n    TRANSFORMERS_MODELS_TO_PREFIX_TUNING_POSTPROCESS_MAPPING,\n    WEIGHTS_NAME,\n    PeftConfig,\n    PeftType,\n    PromptLearningConfig,\n    TaskType,\n    _set_adapter,\n    _set_trainable,\n    get_peft_model_state_dict,\n    set_peft_model_state_dict,\n    shift_tokens_right,\n)\n\n\nPEFT_TYPE_TO_MODEL_MAPPING = {\n    PeftType.LORA: LoraModel,\n    PeftType.PROMPT_TUNING: PromptEmbedding,\n    PeftType.P_TUNING: PromptEncoder,\n    PeftType.PREFIX_TUNING: PrefixEncoder,\n    PeftType.ADALORA: AdaLoraModel,\n    PeftType.ADAPTION_PROMPT: AdaptionPromptModel,\n}\n\n\nclass PeftModel(PushToHubMixin, torch.nn.Module):\n    \"\"\"\n    Base model encompassing various Peft methods.\n\n    Args:\n        model ([`~transformers.PreTrainedModel`]): The base transformer model used for Peft.\n        peft_config ([`PeftConfig`]): The configuration of the Peft model.\n\n\n    **Attributes**:\n        - **base_model** ([`~transformers.PreTrainedModel`]) -- The base transformer model used for Peft.\n        - **peft_config** ([`PeftConfig`]) -- The configuration of the Peft model.\n        - **modules_to_save** (`list` of `str`) -- The list of sub-module names to save when\n        saving the model.\n        - **prompt_encoder** ([`PromptEncoder`]) -- The prompt encoder used for Peft if\n        using [`PromptLearningConfig`].\n        - **prompt_tokens** (`torch.Tensor`) -- The virtual prompt tokens used for Peft if\n        using [`PromptLearningConfig`].\n        - **transformer_backbone_name** (`str`) -- The name of the transformer\n        backbone in the base model if using [`PromptLearningConfig`].\n        - **word_embeddings** (`torch.nn.Embedding`) -- The word embeddings of the transformer backbone\n        in the base model if using [`PromptLearningConfig`].\n    \"\"\"\n\n    def __init__(self, model, peft_config: PeftConfig, adapter_name=\"default\"):\n        super().__init__()\n        self.base_model = model\n        self.config = self.base_model.config\n        self.modules_to_save = None\n        self.peft_config = {}\n        self.active_adapter = adapter_name\n        self.peft_type = peft_config.peft_type\n        self.base_model_torch_dtype = getattr(model, \"dtype\", None)\n        if not isinstance(peft_config, PromptLearningConfig):\n            self.peft_config[adapter_name] = peft_config\n            self.base_model = PEFT_TYPE_TO_MODEL_MAPPING[peft_config.peft_type](\n                self.base_model, self.peft_config, adapter_name\n            )\n            self.set_additional_trainable_modules(peft_config, adapter_name)\n        else:\n            self.add_adapter(adapter_name, peft_config)\n\n    def save_pretrained(self, save_directory, **kwargs):\n        r\"\"\"\n        This function saves the adapter model and the adapter configuration files to a directory, so that it can be\n        reloaded using the [`LoraModel.from_pretrained`] class method, and also used by the [`LoraModel.push_to_hub`]\n        method.\n\n        Args:\n            save_directory (`str`):\n                Directory where the adapter model and configuration files will be saved (will be created if it does not\n                exist).\n            kwargs (additional keyword arguments, *optional*):\n                Additional keyword arguments passed along to the `push_to_hub` method.\n        \"\"\"\n        if os.path.isfile(save_directory):\n            raise ValueError(f\"Provided path ({save_directory}) should be a directory, not a file\")\n        os.makedirs(save_directory, exist_ok=True)\n\n        for adapter_name, peft_config in self.peft_config.items():\n            # save only the trainable weights\n            output_state_dict = get_peft_model_state_dict(\n                self, state_dict=kwargs.get(\"state_dict\", None), adapter_name=adapter_name\n            )\n            output_dir = os.path.join(save_directory, adapter_name) if adapter_name != \"default\" else save_directory\n            os.makedirs(output_dir, exist_ok=True)\n            torch.save(output_state_dict, os.path.join(output_dir, WEIGHTS_NAME))\n\n            # save the config and change the inference mode to `True`\n            if peft_config.base_model_name_or_path is None:\n                peft_config.base_model_name_or_path = (\n                    self.base_model.__dict__.get(\"name_or_path\", None)\n                    if isinstance(peft_config, PromptLearningConfig)\n                    else self.base_model.model.__dict__.get(\"name_or_path\", None)\n                )\n            inference_mode = peft_config.inference_mode\n            peft_config.inference_mode = True\n            peft_config.save_pretrained(output_dir)\n            peft_config.inference_mode = inference_mode\n\n    @classmethod\n    def from_pretrained(cls, model, model_id, adapter_name=\"default\", is_trainable=False, **kwargs):\n        r\"\"\"\n        Instantiate a [`LoraModel`] from a pretrained Lora configuration and weights.\n\n        Args:\n            model ([`~transformers.PreTrainedModel`]):\n                The model to be adapted. The model should be initialized with the\n                [`~transformers.PreTrainedModel.from_pretrained`] method from the 🤗 Transformers library.\n            model_id (`str` or `os.PathLike`):\n                The name of the Lora configuration to use. Can be either:\n                    - A string, the `model id` of a Lora configuration hosted inside a model repo on the Hugging Face\n                      Hub.\n                    - A path to a directory containing a Lora configuration file saved using the `save_pretrained`\n                      method (`./my_lora_config_directory/`).\n        \"\"\"\n        from .mapping import MODEL_TYPE_TO_PEFT_MODEL_MAPPING, PEFT_TYPE_TO_CONFIG_MAPPING\n\n        # load the config\n        config = PEFT_TYPE_TO_CONFIG_MAPPING[\n            PeftConfig.from_pretrained(model_id, subfolder=kwargs.get(\"subfolder\", None)).peft_type\n        ].from_pretrained(model_id, subfolder=kwargs.get(\"subfolder\", None))\n\n        if (getattr(model, \"hf_device_map\", None) is not None) and len(\n            set(model.hf_device_map.values()).intersection({\"cpu\", \"disk\"})\n        ) > 0:\n            remove_hook_from_submodules(model)\n\n        if isinstance(config, PromptLearningConfig) and is_trainable:\n            raise ValueError(\"Cannot set a prompt learning adapter to trainable when loading pretrained adapter.\")\n        else:\n            config.inference_mode = not is_trainable\n\n        if config.task_type not in MODEL_TYPE_TO_PEFT_MODEL_MAPPING.keys():\n            model = cls(model, config, adapter_name)\n        else:\n            model = MODEL_TYPE_TO_PEFT_MODEL_MAPPING[config.task_type](model, config, adapter_name)\n        model.load_adapter(model_id, adapter_name, **kwargs)\n        return model\n\n    def _setup_prompt_encoder(self, adapter_name):\n        config = self.peft_config[adapter_name]\n        self.prompt_encoder = torch.nn.ModuleDict({})\n        self.prompt_tokens = {}\n        transformer_backbone = None\n        for name, module in self.base_model.named_children():\n            for param in module.parameters():\n                param.requires_grad = False\n            if isinstance(module, PreTrainedModel):\n                # Make sure to freeze Tranformers model\n                if transformer_backbone is None:\n                    transformer_backbone = module\n                    self.transformer_backbone_name = name\n\n        if config.num_transformer_submodules is None:\n            config.num_transformer_submodules = 2 if config.task_type == TaskType.SEQ_2_SEQ_LM else 1\n\n        for named_param, value in list(transformer_backbone.named_parameters()):\n            if value.shape[0] == self.base_model.config.vocab_size:\n                self.word_embeddings = transformer_backbone.get_submodule(named_param.replace(\".weight\", \"\"))\n                break\n\n        if config.peft_type == PeftType.PROMPT_TUNING:\n            prompt_encoder = PromptEmbedding(config, self.word_embeddings)\n        elif config.peft_type == PeftType.P_TUNING:\n            prompt_encoder = PromptEncoder(config)\n        elif config.peft_type == PeftType.PREFIX_TUNING:\n            prompt_encoder = PrefixEncoder(config)\n        else:\n            raise ValueError(\"Not supported\")\n        self.prompt_encoder.update(torch.nn.ModuleDict({adapter_name: prompt_encoder}))\n        self.prompt_tokens[adapter_name] = torch.arange(\n            config.num_virtual_tokens * config.num_transformer_submodules\n        ).long()\n\n    def get_prompt_embedding_to_save(self, adapter_name):\n        \"\"\"\n        Returns the prompt embedding to save when saving the model. Only applicable when `peft_config.peft_type !=\n        PeftType.LORA`.\n        \"\"\"\n        prompt_tokens = self.prompt_tokens[adapter_name].unsqueeze(0).expand(1, -1).to(self.device)\n        if self.peft_config[adapter_name].peft_type == PeftType.PREFIX_TUNING:\n            prompt_tokens = prompt_tokens[:, : self.peft_config[adapter_name].num_virtual_tokens]\n        prompt_embeddings = self.prompt_encoder[adapter_name](prompt_tokens)\n        return prompt_embeddings[0].detach().cpu()\n\n    def get_prompt(self, batch_size):\n        \"\"\"\n        Returns the virtual prompts to use for Peft. Only applicable when `peft_config.peft_type != PeftType.LORA`.\n        \"\"\"\n        peft_config = self.active_peft_config\n        prompt_encoder = self.prompt_encoder[self.active_adapter]\n        prompt_tokens = self.prompt_tokens[self.active_adapter].unsqueeze(0).expand(batch_size, -1).to(self.device)\n        if peft_config.peft_type == PeftType.PREFIX_TUNING:\n            prompt_tokens = prompt_tokens[:, : peft_config.num_virtual_tokens]\n            if peft_config.inference_mode:\n                past_key_values = prompt_encoder.embedding.weight.repeat(batch_size, 1, 1)\n            else:\n                past_key_values = prompt_encoder(prompt_tokens)\n            past_key_values = past_key_values.view(\n                batch_size,\n                peft_config.num_virtual_tokens,\n                peft_config.num_layers * 2,\n                peft_config.num_attention_heads,\n                peft_config.token_dim // peft_config.num_attention_heads,\n            )\n            if peft_config.num_transformer_submodules == 2:\n                past_key_values = torch.cat([past_key_values, past_key_values], dim=2)\n            past_key_values = past_key_values.permute([2, 0, 3, 1, 4]).split(\n                peft_config.num_transformer_submodules * 2\n            )\n            if TRANSFORMERS_MODELS_TO_PREFIX_TUNING_POSTPROCESS_MAPPING.get(self.config.model_type, None) is not None:\n                post_process_fn = TRANSFORMERS_MODELS_TO_PREFIX_TUNING_POSTPROCESS_MAPPING[self.config.model_type]\n                past_key_values = post_process_fn(past_key_values)\n            return past_key_values\n        else:\n            if peft_config.inference_mode:\n                prompts = prompt_encoder.embedding.weight.repeat(batch_size, 1, 1)\n            else:\n                prompts = prompt_encoder(prompt_tokens)\n            return prompts\n\n    def print_trainable_parameters(self):\n        \"\"\"\n        Prints the number of trainable parameters in the model.\n        \"\"\"\n        trainable_params = 0\n        all_param = 0\n        for _, param in self.named_parameters():\n            num_params = param.numel()\n            # if using DS Zero 3 and the weights are initialized empty\n            if num_params == 0 and hasattr(param, \"ds_numel\"):\n                num_params = param.ds_numel\n\n            all_param += num_params\n            if param.requires_grad:\n                trainable_params += num_params\n        print(\n            f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n        )\n\n    def __getattr__(self, name: str):\n        \"\"\"Forward missing attributes to the wrapped module.\"\"\"\n        try:\n            return super().__getattr__(name)  # defer to nn.Module's logic\n        except AttributeError:\n            return getattr(self.base_model, name)\n\n    def forward(self, *args, **kwargs):\n        \"\"\"\n        Forward pass of the model.\n        \"\"\"\n        return self.get_base_model()(*args, **kwargs)\n\n    @contextmanager\n    def disable_adapter(self):\n        \"\"\"\n        Disables the adapter module.\n        \"\"\"\n        try:\n            if isinstance(self.peft_config, PromptLearningConfig):\n                old_forward = self.forward\n                self.forward = self.base_model.forward\n            else:\n                self.base_model.disable_adapter_layers()\n            yield\n        finally:\n            if isinstance(self.peft_config, PromptLearningConfig):\n                self.forward = old_forward\n            else:\n                self.base_model.enable_adapter_layers()\n\n    def get_base_model(self):\n        \"\"\"\n        Returns the base model.\n        \"\"\"\n        return self.base_model if isinstance(self.active_peft_config, PromptLearningConfig) else self.base_model.model\n\n    def add_adapter(self, adapter_name, peft_config):\n        if peft_config.peft_type != self.peft_type:\n            raise ValueError(\n                f\"Cannot combine adapters with different peft types. \"\n                f\"Found {self.peft_type} and {peft_config.peft_type}.\"\n            )\n        self.peft_config[adapter_name] = peft_config\n        if isinstance(peft_config, PromptLearningConfig):\n            self._setup_prompt_encoder(adapter_name)\n        else:\n            self.base_model.add_adapter(adapter_name, peft_config)\n\n        self.set_additional_trainable_modules(peft_config, adapter_name)\n\n    def set_additional_trainable_modules(self, peft_config, adapter_name):\n        if getattr(peft_config, \"modules_to_save\", None) is not None:\n            if self.modules_to_save is None:\n                self.modules_to_save = set(peft_config.modules_to_save)\n            else:\n                self.modules_to_save.update(peft_config.modules_to_save)\n            _set_trainable(self, adapter_name)\n\n    def load_adapter(self, model_id, adapter_name, is_trainable=False, **kwargs):\n        from .mapping import PEFT_TYPE_TO_CONFIG_MAPPING\n\n        if adapter_name not in self.peft_config:\n            # load the config\n            peft_config = PEFT_TYPE_TO_CONFIG_MAPPING[\n                PeftConfig.from_pretrained(model_id, subfolder=kwargs.get(\"subfolder\", None)).peft_type\n            ].from_pretrained(model_id, subfolder=kwargs.get(\"subfolder\", None))\n            if isinstance(peft_config, PromptLearningConfig) and is_trainable:\n                raise ValueError(\"Cannot set a prompt learning adapter to trainable when loading pretrained adapter.\")\n            else:\n                peft_config.inference_mode = not is_trainable\n            self.add_adapter(adapter_name, peft_config)\n\n        # load weights if any\n        path = os.path.join(model_id, kwargs[\"subfolder\"]) if kwargs.get(\"subfolder\", None) is not None else model_id\n\n        if os.path.exists(os.path.join(path, WEIGHTS_NAME)):\n            filename = os.path.join(path, WEIGHTS_NAME)\n        else:\n            try:\n                filename = hf_hub_download(model_id, WEIGHTS_NAME, subfolder=kwargs.get(\"subfolder\", None))\n            except:  # noqa\n                raise ValueError(\n                    f\"Can't find weights for {model_id} in {model_id} or in the Hugging Face Hub. \"\n                    f\"Please check that the file {WEIGHTS_NAME} is present at {model_id}.\"\n                )\n\n        adapters_weights = torch.load(\n            filename, map_location=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        )\n        # load the weights into the model\n        set_peft_model_state_dict(self, adapters_weights, adapter_name=adapter_name)\n        if (\n            (getattr(self, \"hf_device_map\", None) is not None)\n            and (len(set(self.hf_device_map.values()).intersection({\"cpu\", \"disk\"})) > 0)\n            and len(self.peft_config) == 1\n        ):\n            device_map = kwargs.get(\"device_map\", \"auto\")\n            max_memory = kwargs.get(\"max_memory\", None)\n            offload_dir = kwargs.get(\"offload_folder\", None)\n            offload_index = kwargs.get(\"offload_index\", None)\n\n            dispatch_model_kwargs = {}\n            # Safety checker for previous `accelerate` versions\n            # `offload_index` was introduced in https://github.com/huggingface/accelerate/pull/873/\n            if \"offload_index\" in inspect.signature(dispatch_model).parameters:\n                dispatch_model_kwargs[\"offload_index\"] = offload_index\n\n            no_split_module_classes = self._no_split_modules\n\n            if device_map != \"sequential\":\n                max_memory = get_balanced_memory(\n                    self,\n                    max_memory=max_memory,\n                    no_split_module_classes=no_split_module_classes,\n                    low_zero=(device_map == \"balanced_low_0\"),\n                )\n            if isinstance(device_map, str):\n                device_map = infer_auto_device_map(\n                    self, max_memory=max_memory, no_split_module_classes=no_split_module_classes\n                )\n            dispatch_model(\n                self,\n                device_map=device_map,\n                offload_dir=offload_dir,\n                **dispatch_model_kwargs,\n            )\n            hook = AlignDevicesHook(io_same_device=True)\n            if isinstance(self.peft_config[adapter_name], PromptLearningConfig):\n                remove_hook_from_submodules(self.prompt_encoder)\n            add_hook_to_module(self.get_base_model(), hook)\n\n        # Set model in evaluation mode to deactivate Dropout modules by default\n        self.eval()\n\n    def set_adapter(self, adapter_name):\n        \"\"\"\n        Sets the active adapter.\n        \"\"\"\n        if adapter_name not in self.peft_config:\n            raise ValueError(f\"Adapter {adapter_name} not found.\")\n        self.active_adapter = adapter_name\n        if not isinstance(self.peft_config[adapter_name], PromptLearningConfig):\n            self.base_model.set_adapter(adapter_name)\n        _set_adapter(self, adapter_name)\n\n    @property\n    def active_peft_config(self):\n        return self.peft_config[self.active_adapter]\n\n\nclass PeftModelForSequenceClassification(PeftModel):\n    \"\"\"\n    Peft model for sequence classification tasks.\n\n    Args:\n        model ([`~transformers.PreTrainedModel`]): Base transformer model.\n        peft_config ([`PeftConfig`]): Peft config.\n\n    **Attributes**:\n        - **config** ([`~transformers.PretrainedConfig`]) -- The configuration object of the base model.\n        - **cls_layer_name** (`str`) -- The name of the classification layer.\n\n    Example:\n\n        ```py\n        >>> from transformers import AutoModelForSequenceClassification\n        >>> from peft import PeftModelForSequenceClassification, get_peft_config\n\n        >>> config = {\n        ...     \"peft_type\": \"PREFIX_TUNING\",\n        ...     \"task_type\": \"SEQ_CLS\",\n        ...     \"inference_mode\": False,\n        ...     \"num_virtual_tokens\": 20,\n        ...     \"token_dim\": 768,\n        ...     \"num_transformer_submodules\": 1,\n        ...     \"num_attention_heads\": 12,\n        ...     \"num_layers\": 12,\n        ...     \"encoder_hidden_size\": 768,\n        ...     \"prefix_projection\": False,\n        ...     \"postprocess_past_key_value_function\": None,\n        ... }\n\n        >>> peft_config = get_peft_config(config)\n        >>> model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\")\n        >>> peft_model = PeftModelForSequenceClassification(model, peft_config)\n        >>> peft_model.print_trainable_parameters()\n        trainable params: 370178 || all params: 108680450 || trainable%: 0.3406113979101117\n        ```\n    \"\"\"\n\n    def __init__(self, model, peft_config: PeftConfig, adapter_name=\"default\"):\n        super().__init__(model, peft_config, adapter_name)\n        if self.modules_to_save is None:\n            self.modules_to_save = {\"classifier\", \"score\"}\n        else:\n            self.modules_to_save.update({\"classifier\", \"score\"})\n\n        for name, _ in self.base_model.named_children():\n            if any(module_name in name for module_name in self.modules_to_save):\n                self.cls_layer_name = name\n                break\n\n        # to make sure classifier layer is trainable\n        _set_trainable(self, adapter_name)\n\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,\n        inputs_embeds=None,\n        labels=None,\n        output_attentions=None,\n        output_hidden_states=None,\n        return_dict=None,\n        **kwargs,\n    ):\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n        peft_config = self.active_peft_config\n        if not isinstance(peft_config, PromptLearningConfig):\n            return self.base_model(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                inputs_embeds=inputs_embeds,\n                labels=labels,\n                output_attentions=output_attentions,\n                output_hidden_states=output_hidden_states,\n                return_dict=return_dict,\n                **kwargs,\n            )\n\n        batch_size = input_ids.shape[0]\n        if attention_mask is not None:\n            # concat prompt attention mask\n            prefix_attention_mask = torch.ones(batch_size, peft_config.num_virtual_tokens).to(self.device)\n            attention_mask = torch.cat((prefix_attention_mask, attention_mask), dim=1)\n        if kwargs.get(\"position_ids\", None) is not None:\n            warnings.warn(\"Position ids are not supported for parameter efficient tuning. Ignoring position ids.\")\n            kwargs[\"position_ids\"] = None\n        kwargs.update(\n            {\n                \"attention_mask\": attention_mask,\n                \"labels\": labels,\n                \"output_attentions\": output_attentions,\n                \"output_hidden_states\": output_hidden_states,\n                \"return_dict\": return_dict,\n            }\n        )\n\n        if peft_config.peft_type == PeftType.PREFIX_TUNING:\n            return self._prefix_tuning_forward(input_ids=input_ids, **kwargs)\n        else:\n            if kwargs.get(\"token_type_ids\", None) is not None:\n                kwargs[\"token_type_ids\"] = torch.cat(\n                    (\n                        torch.zeros(batch_size, peft_config.num_virtual_tokens).to(self.device),\n                        kwargs[\"token_type_ids\"],\n                    ),\n                    dim=1,\n                ).long()\n            if inputs_embeds is None:\n                inputs_embeds = self.word_embeddings(input_ids)\n            prompts = self.get_prompt(batch_size=batch_size)\n            prompts = prompts.to(inputs_embeds.dtype)\n            inputs_embeds = torch.cat((prompts, inputs_embeds), dim=1)\n            return self.base_model(inputs_embeds=inputs_embeds, **kwargs)\n\n    def _prefix_tuning_forward(\n        self,\n        input_ids=None,\n        attention_mask=None,\n        inputs_embeds=None,\n        labels=None,\n        output_attentions=None,\n        output_hidden_states=None,\n        return_dict=None,\n        **kwargs,\n    ):\n        batch_size = input_ids.shape[0]\n        past_key_values = self.get_prompt(batch_size)\n        fwd_params = list(inspect.signature(self.base_model.forward).parameters.keys())\n        kwargs.update(\n            {\n                \"input_ids\": input_ids,\n                \"attention_mask\": attention_mask,\n                \"inputs_embeds\": inputs_embeds,\n                \"output_attentions\": output_attentions,\n                \"output_hidden_states\": output_hidden_states,\n                \"return_dict\": return_dict,\n                \"past_key_values\": past_key_values,\n            }\n        )\n        if \"past_key_values\" in fwd_params:\n            return self.base_model(labels=labels, **kwargs)\n        else:\n            transformer_backbone_name = self.base_model.get_submodule(self.transformer_backbone_name)\n            fwd_params = list(inspect.signature(transformer_backbone_name.forward).parameters.keys())\n            if \"past_key_values\" not in fwd_params:\n                raise ValueError(\"Model does not support past key values which are required for prefix tuning.\")\n            outputs = transformer_backbone_name(**kwargs)\n            pooled_output = outputs[1] if len(outputs) > 1 else outputs[0]\n            if \"dropout\" in [name for name, _ in list(self.base_model.named_children())]:\n                pooled_output = self.base_model.dropout(pooled_output)\n            logits = self.base_model.get_submodule(self.cls_layer_name)(pooled_output)\n\n            loss = None\n            if labels is not None:\n                if self.config.problem_type is None:\n                    if self.base_model.num_labels == 1:\n                        self.config.problem_type = \"regression\"\n                    elif self.base_model.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n                        self.config.problem_type = \"single_label_classification\"\n                    else:\n                        self.config.problem_type = \"multi_label_classification\"\n\n                if self.config.problem_type == \"regression\":\n                    loss_fct = MSELoss()\n                    if self.base_model.num_labels == 1:\n                        loss = loss_fct(logits.squeeze(), labels.squeeze())\n                    else:\n                        loss = loss_fct(logits, labels)\n                elif self.config.problem_type == \"single_label_classification\":\n                    loss_fct = CrossEntropyLoss()\n                    loss = loss_fct(logits.view(-1, self.base_model.num_labels), labels.view(-1))\n                elif self.config.problem_type == \"multi_label_classification\":\n                    loss_fct = BCEWithLogitsLoss()\n                    loss = loss_fct(logits, labels)\n            if not return_dict:\n                output = (logits,) + outputs[2:]\n                return ((loss,) + output) if loss is not None else output\n\n            return SequenceClassifierOutput(\n                loss=loss,\n                logits=logits,\n                hidden_states=outputs.hidden_states,\n                attentions=outputs.attentions,\n            )\n\n\nclass PeftModelForCausalLM(PeftModel):\n    \"\"\"\n    Peft model for causal language modeling.\n\n    Args:\n        model ([`~transformers.PreTrainedModel`]): Base transformer model.\n        peft_config ([`PeftConfig`]): Peft config.\n\n\n    Example:\n\n        ```py\n        >>> from transformers import AutoModelForCausalLM\n        >>> from peft import PeftModelForCausalLM, get_peft_config\n\n        >>> config = {\n        ...     \"peft_type\": \"PREFIX_TUNING\",\n        ...     \"task_type\": \"CAUSAL_LM\",\n        ...     \"inference_mode\": False,\n        ...     \"num_virtual_tokens\": 20,\n        ...     \"token_dim\": 1280,\n        ...     \"num_transformer_submodules\": 1,\n        ...     \"num_attention_heads\": 20,\n        ...     \"num_layers\": 36,\n        ...     \"encoder_hidden_size\": 1280,\n        ...     \"prefix_projection\": False,\n        ...     \"postprocess_past_key_value_function\": None,\n        ... }\n\n        >>> peft_config = get_peft_config(config)\n        >>> model = AutoModelForCausalLM.from_pretrained(\"gpt2-large\")\n        >>> peft_model = PeftModelForCausalLM(model, peft_config)\n        >>> peft_model.print_trainable_parameters()\n        trainable params: 1843200 || all params: 775873280 || trainable%: 0.23756456724479544\n        ```\n    \"\"\"\n\n    def __init__(self, model, peft_config: PeftConfig, adapter_name=\"default\"):\n        super().__init__(model, peft_config, adapter_name)\n        self.base_model_prepare_inputs_for_generation = self.base_model.prepare_inputs_for_generation\n\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,\n        inputs_embeds=None,\n        labels=None,\n        output_attentions=None,\n        output_hidden_states=None,\n        return_dict=None,\n        **kwargs,\n    ):\n        peft_config = self.active_peft_config\n        if not isinstance(peft_config, PromptLearningConfig):\n            return self.base_model(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                inputs_embeds=inputs_embeds,\n                labels=labels,\n                output_attentions=output_attentions,\n                output_hidden_states=output_hidden_states,\n                return_dict=return_dict,\n                **kwargs,\n            )\n\n        batch_size = input_ids.shape[0]\n        if attention_mask is not None:\n            # concat prompt attention mask\n            prefix_attention_mask = torch.ones(batch_size, peft_config.num_virtual_tokens).to(self.device)\n            attention_mask = torch.cat((prefix_attention_mask, attention_mask), dim=1)\n\n        if kwargs.get(\"position_ids\", None) is not None:\n            warnings.warn(\"Position ids are not supported for parameter efficient tuning. Ignoring position ids.\")\n            kwargs[\"position_ids\"] = None\n        if kwargs.get(\"token_type_ids\", None) is not None:\n            warnings.warn(\"Token type ids are not supported for parameter efficient tuning. Ignoring token type ids\")\n            kwargs[\"token_type_ids\"] = None\n        kwargs.update(\n            {\n                \"attention_mask\": attention_mask,\n                \"labels\": labels,\n                \"output_attentions\": output_attentions,\n                \"output_hidden_states\": output_hidden_states,\n                \"return_dict\": return_dict,\n            }\n        )\n\n        if peft_config.peft_type == PeftType.PREFIX_TUNING:\n            past_key_values = self.get_prompt(batch_size)\n            return self.base_model(input_ids=input_ids, past_key_values=past_key_values, **kwargs)\n        else:\n            if inputs_embeds is None:\n                inputs_embeds = self.word_embeddings(input_ids)\n            # concat prompt labels\n            if labels is not None:\n                prefix_labels = torch.full((batch_size, peft_config.num_virtual_tokens), -100).to(self.device)\n                kwargs[\"labels\"] = torch.cat((prefix_labels, labels), dim=1)\n            prompts = self.get_prompt(batch_size=batch_size)\n            prompts = prompts.to(inputs_embeds.dtype)\n            inputs_embeds = torch.cat((prompts, inputs_embeds), dim=1)\n            return self.base_model(inputs_embeds=inputs_embeds, **kwargs)\n\n    def generate(self, **kwargs):\n        peft_config = self.active_peft_config\n        self.base_model.prepare_inputs_for_generation = self.prepare_inputs_for_generation\n        try:\n            if not isinstance(peft_config, PromptLearningConfig):\n                outputs = self.base_model.generate(**kwargs)\n            else:\n                if \"input_ids\" not in kwargs:\n                    raise ValueError(\"input_ids must be provided for Peft model generation\")\n                # For gpt2 models, we construct postion_ids on the fly by using attention mask, and position ids need to match input_shape.\n                # for prefix tuning, input shape is determined using `input_ids`. Thus we should not expand 'attention_mask' here\n                # for prompt tuning input_ids is not passed but a concatenated input_embeds is passed. Thus attention_mask needs to be of same size of num_virtual_tokens + input_ids\n                if kwargs.get(\"attention_mask\", None) is not None and peft_config.peft_type in [\n                    PeftType.PROMPT_TUNING,\n                    PeftType.P_TUNING,\n                ]:\n                    # concat prompt attention mask\n                    prefix_attention_mask = torch.ones(\n                        kwargs[\"input_ids\"].shape[0], peft_config.num_virtual_tokens\n                    ).to(kwargs[\"input_ids\"].device)\n                    kwargs[\"attention_mask\"] = torch.cat((prefix_attention_mask, kwargs[\"attention_mask\"]), dim=1)\n\n                if kwargs.get(\"position_ids\", None) is not None:\n                    warnings.warn(\n                        \"Position ids are not supported for parameter efficient tuning. Ignoring position ids.\"\n                    )\n                    kwargs[\"position_ids\"] = None\n                if kwargs.get(\"token_type_ids\", None) is not None:\n                    warnings.warn(\n                        \"Token type ids are not supported for parameter efficient tuning. Ignoring token type ids\"\n                    )\n                    kwargs[\"token_type_ids\"] = None\n\n                outputs = self.base_model.generate(**kwargs)\n        except:\n            self.base_model.prepare_inputs_for_generation = self.base_model_prepare_inputs_for_generation\n            raise\n        else:\n            self.base_model.prepare_inputs_for_generation = self.base_model_prepare_inputs_for_generation\n            return outputs\n\n    def prepare_inputs_for_generation(self, *args, **kwargs):\n        peft_config = self.active_peft_config\n        model_kwargs = self.base_model_prepare_inputs_for_generation(*args, **kwargs)\n        if isinstance(peft_config, PromptLearningConfig):\n            if peft_config.peft_type == PeftType.PREFIX_TUNING:\n                prefix_attention_mask = torch.ones(\n                    model_kwargs[\"input_ids\"].shape[0], peft_config.num_virtual_tokens\n                ).to(model_kwargs[\"input_ids\"].device)\n                model_kwargs[\"attention_mask\"] = torch.cat(\n                    (prefix_attention_mask, model_kwargs[\"attention_mask\"]), dim=1\n                )\n\n            if model_kwargs[\"past_key_values\"] is None and peft_config.peft_type == PeftType.PREFIX_TUNING:\n                past_key_values = self.get_prompt(batch_size=model_kwargs[\"input_ids\"].shape[0])\n\n                if self.base_model_torch_dtype is not None:\n                    # handle the case for Bloom where it outputs tuple of tuples\n                    if isinstance(past_key_values[0], tuple):\n                        past_key_values = tuple(\n                            tuple(\n                                past_key_value.to(self.base_model_torch_dtype)\n                                for past_key_value in past_key_value_tuple\n                            )\n                            for past_key_value_tuple in past_key_values\n                        )\n                    else:\n                        past_key_values = tuple(\n                            past_key_value.to(self.base_model_torch_dtype) for past_key_value in past_key_values\n                        )\n\n                model_kwargs[\"past_key_values\"] = past_key_values\n            else:\n                if model_kwargs[\"past_key_values\"] is None:\n                    inputs_embeds = self.word_embeddings(model_kwargs[\"input_ids\"])\n                    prompts = self.get_prompt(batch_size=model_kwargs[\"input_ids\"].shape[0])\n                    prompts = prompts.to(inputs_embeds.dtype)\n                    model_kwargs[\"inputs_embeds\"] = torch.cat((prompts, inputs_embeds), dim=1)\n                    model_kwargs[\"input_ids\"] = None\n\n        return model_kwargs\n\n\nclass PeftModelForSeq2SeqLM(PeftModel):\n    \"\"\"\n    Peft model for sequence-to-sequence language modeling.\n\n    Args:\n        model ([`~transformers.PreTrainedModel`]): Base transformer model.\n        peft_config ([`PeftConfig`]): Peft config.\n\n\n    Example:\n\n        ```py\n        >>> from transformers import AutoModelForSeq2SeqLM\n        >>> from peft import PeftModelForSeq2SeqLM, get_peft_config\n\n        >>> config = {\n        ...     \"peft_type\": \"LORA\",\n        ...     \"task_type\": \"SEQ_2_SEQ_LM\",\n        ...     \"inference_mode\": False,\n        ...     \"r\": 8,\n        ...     \"target_modules\": [\"q\", \"v\"],\n        ...     \"lora_alpha\": 32,\n        ...     \"lora_dropout\": 0.1,\n        ...     \"merge_weights\": False,\n        ...     \"fan_in_fan_out\": False,\n        ...     \"enable_lora\": None,\n        ...     \"bias\": \"none\",\n        ... }\n\n        >>> peft_config = get_peft_config(config)\n        >>> model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\")\n        >>> peft_model = PeftModelForSeq2SeqLM(model, peft_config)\n        >>> peft_model.print_trainable_parameters()\n        trainable params: 884736 || all params: 223843584 || trainable%: 0.3952474242013566\n        ```\n    \"\"\"\n\n    def __init__(self, model, peft_config: PeftConfig, adapter_name=\"default\"):\n        super().__init__(model, peft_config, adapter_name)\n        self.base_model_prepare_inputs_for_generation = self.base_model.prepare_inputs_for_generation\n        self.base_model_prepare_encoder_decoder_kwargs_for_generation = (\n            self.base_model._prepare_encoder_decoder_kwargs_for_generation\n        )\n\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,\n        inputs_embeds=None,\n        decoder_input_ids=None,\n        decoder_attention_mask=None,\n        decoder_inputs_embeds=None,\n        labels=None,\n        output_attentions=None,\n        output_hidden_states=None,\n        return_dict=None,\n        **kwargs,\n    ):\n        peft_config = self.active_peft_config\n        if not isinstance(peft_config, PromptLearningConfig):\n            return self.base_model(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                inputs_embeds=inputs_embeds,\n                decoder_input_ids=decoder_input_ids,\n                decoder_attention_mask=decoder_attention_mask,\n                decoder_inputs_embeds=decoder_inputs_embeds,\n                labels=labels,\n                output_attentions=output_attentions,\n                output_hidden_states=output_hidden_states,\n                return_dict=return_dict,\n                **kwargs,\n            )\n\n        batch_size = input_ids.shape[0]\n        if decoder_attention_mask is not None:\n            # concat prompt attention mask\n            prefix_attention_mask = torch.ones(batch_size, peft_config.num_virtual_tokens).to(self.device)\n            decoder_attention_mask = torch.cat((prefix_attention_mask, decoder_attention_mask), dim=1)\n\n        if kwargs.get(\"position_ids\", None) is not None:\n            warnings.warn(\"Position ids are not supported for parameter efficient tuning. Ignoring position ids.\")\n            kwargs[\"position_ids\"] = None\n        if kwargs.get(\"token_type_ids\", None) is not None:\n            warnings.warn(\"Token type ids are not supported for parameter efficient tuning. Ignoring token type ids\")\n            kwargs[\"token_type_ids\"] = None\n        kwargs.update(\n            {\n                \"attention_mask\": attention_mask,\n                \"decoder_attention_mask\": decoder_attention_mask,\n                \"labels\": labels,\n                \"output_attentions\": output_attentions,\n                \"output_hidden_states\": output_hidden_states,\n                \"return_dict\": return_dict,\n            }\n        )\n\n        if peft_config.peft_type == PeftType.PREFIX_TUNING:\n            past_key_values = self.get_prompt(batch_size)\n            return self.base_model(\n                input_ids=input_ids, decoder_input_ids=decoder_input_ids, past_key_values=past_key_values, **kwargs\n            )\n        else:\n            if inputs_embeds is None:\n                inputs_embeds = self.word_embeddings(input_ids)\n            if decoder_inputs_embeds is None and decoder_input_ids is None:\n                decoder_input_ids = shift_tokens_right(\n                    labels, self.config.pad_token_id, self.config.decoder_start_token_id\n                )\n                decoder_inputs_embeds = self.word_embeddings(decoder_input_ids)\n\n            if attention_mask is not None:\n                # concat prompt attention mask\n                prefix_attention_mask = torch.ones(batch_size, peft_config.num_virtual_tokens).to(self.device)\n                kwargs[\"attention_mask\"] = torch.cat((prefix_attention_mask, attention_mask), dim=1)\n            # concat prompt labels\n            if labels is not None:\n                if peft_config.num_transformer_submodules == 1:\n                    kwargs[\"labels\"] = labels\n                elif peft_config.num_transformer_submodules == 2:\n                    prefix_labels = torch.full((batch_size, peft_config.num_virtual_tokens), -100).to(self.device)\n                    kwargs[\"labels\"] = torch.cat((prefix_labels, labels), dim=1)\n            prompts = self.get_prompt(batch_size=batch_size)\n            prompts = prompts.to(inputs_embeds.dtype)\n            inputs_embeds = torch.cat((prompts[:, : peft_config.num_virtual_tokens], inputs_embeds), dim=1)\n            if peft_config.num_transformer_submodules == 1:\n                return self.base_model(inputs_embeds=inputs_embeds, **kwargs)\n            elif peft_config.num_transformer_submodules == 2:\n                decoder_inputs_embeds = torch.cat(\n                    (prompts[:, peft_config.num_virtual_tokens :], decoder_inputs_embeds), dim=1\n                )\n                return self.base_model(\n                    inputs_embeds=inputs_embeds, decoder_inputs_embeds=decoder_inputs_embeds, **kwargs\n                )\n\n    def generate(self, **kwargs):\n        peft_config = self.active_peft_config\n        self.base_model.prepare_inputs_for_generation = self.prepare_inputs_for_generation\n        self.base_model._prepare_encoder_decoder_kwargs_for_generation = (\n            self._prepare_encoder_decoder_kwargs_for_generation\n        )\n        try:\n            if not isinstance(peft_config, PromptLearningConfig):\n                outputs = self.base_model.generate(**kwargs)\n            else:\n                if \"input_ids\" not in kwargs:\n                    raise ValueError(\"input_ids must be provided for Peft model generation\")\n                if kwargs.get(\"position_ids\", None) is not None:\n                    warnings.warn(\n                        \"Position ids are not supported for parameter efficient tuning. Ignoring position ids.\"\n                    )\n                    kwargs[\"position_ids\"] = None\n                if kwargs.get(\"token_type_ids\", None) is not None:\n                    warnings.warn(\n                        \"Token type ids are not supported for parameter efficient tuning. Ignoring token type ids\"\n                    )\n                    kwargs[\"token_type_ids\"] = None\n\n                if peft_config.peft_type == PeftType.PREFIX_TUNING:\n                    outputs = self.base_model.generate(**kwargs)\n                else:\n                    raise NotImplementedError\n        except:\n            self.base_model.prepare_inputs_for_generation = self.base_model_prepare_inputs_for_generation\n            self.base_model._prepare_encoder_decoder_kwargs_for_generation = (\n                self.base_model_prepare_encoder_decoder_kwargs_for_generation\n            )\n            raise\n        else:\n            self.base_model.prepare_inputs_for_generation = self.base_model_prepare_inputs_for_generation\n            self.base_model._prepare_encoder_decoder_kwargs_for_generation = (\n                self.base_model_prepare_encoder_decoder_kwargs_for_generation\n            )\n            return outputs\n\n    def prepare_inputs_for_generation(self, *args, **kwargs):\n        peft_config = self.active_peft_config\n        model_kwargs = self.base_model_prepare_inputs_for_generation(*args, **kwargs)\n        if model_kwargs[\"past_key_values\"] is None and peft_config.peft_type == PeftType.PREFIX_TUNING:\n            batch_size = model_kwargs[\"decoder_input_ids\"].shape[0]\n            past_key_values = self.get_prompt(batch_size)\n            if self.base_model_torch_dtype is not None:\n                # handle the case for Bloom where it outputs tuple of tuples\n                if isinstance(past_key_values[0], tuple):\n                    past_key_values = tuple(\n                        tuple(\n                            past_key_value.to(self.base_model_torch_dtype) for past_key_value in past_key_value_tuple\n                        )\n                        for past_key_value_tuple in past_key_values\n                    )\n                else:\n                    past_key_values = tuple(\n                        past_key_value.to(self.base_model_torch_dtype) for past_key_value in past_key_values\n                    )\n            model_kwargs[\"past_key_values\"] = past_key_values\n\n        return model_kwargs\n\n\nclass PeftModelForTokenClassification(PeftModel):\n    \"\"\"\n    Peft model for token classification tasks.\n\n    Args:\n        model ([`~transformers.PreTrainedModel`]): Base transformer model.\n        peft_config ([`PeftConfig`]): Peft config.\n\n    **Attributes**:\n        - **config** ([`~transformers.PretrainedConfig`]) -- The configuration object of the base model.\n        - **cls_layer_name** (`str`) -- The name of the classification layer.\n\n    Example:\n\n        ```py\n        >>> from transformers import AutoModelForSequenceClassification\n        >>> from peft import PeftModelForTokenClassification, get_peft_config\n\n        >>> config = {\n        ...     \"peft_type\": \"PREFIX_TUNING\",\n        ...     \"task_type\": \"TOKEN_CLS\",\n        ...     \"inference_mode\": False,\n        ...     \"num_virtual_tokens\": 20,\n        ...     \"token_dim\": 768,\n        ...     \"num_transformer_submodules\": 1,\n        ...     \"num_attention_heads\": 12,\n        ...     \"num_layers\": 12,\n        ...     \"encoder_hidden_size\": 768,\n        ...     \"prefix_projection\": False,\n        ...     \"postprocess_past_key_value_function\": None,\n        ... }\n\n        >>> peft_config = get_peft_config(config)\n        >>> model = AutoModelForTokenClassification.from_pretrained(\"bert-base-cased\")\n        >>> peft_model = PeftModelForTokenClassification(model, peft_config)\n        >>> peft_model.print_trainable_parameters()\n        trainable params: 370178 || all params: 108680450 || trainable%: 0.3406113979101117\n        ```\n    \"\"\"\n\n    def __init__(self, model, peft_config: PeftConfig = None, adapter_name=\"default\"):\n        super().__init__(model, peft_config, adapter_name)\n        if self.modules_to_save is None:\n            self.modules_to_save = {\"classifier\", \"score\"}\n        else:\n            self.modules_to_save.update({\"classifier\", \"score\"})\n\n        for name, _ in self.base_model.named_children():\n            if any(module_name in name for module_name in self.modules_to_save):\n                self.cls_layer_name = name\n                break\n\n        # to make sure classifier layer is trainable\n        _set_trainable(self, adapter_name)\n\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,\n        inputs_embeds=None,\n        labels=None,\n        output_attentions=None,\n        output_hidden_states=None,\n        return_dict=None,\n        **kwargs,\n    ):\n        peft_config = self.active_peft_config\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        if not isinstance(peft_config, PromptLearningConfig):\n            return self.base_model(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                inputs_embeds=inputs_embeds,\n                labels=labels,\n                output_attentions=output_attentions,\n                output_hidden_states=output_hidden_states,\n                return_dict=return_dict,\n                **kwargs,\n            )\n\n        batch_size = input_ids.shape[0]\n        if attention_mask is not None:\n            # concat prompt attention mask\n            prefix_attention_mask = torch.ones(batch_size, peft_config.num_virtual_tokens).to(self.device)\n            attention_mask = torch.cat((prefix_attention_mask, attention_mask), dim=1)\n        if kwargs.get(\"position_ids\", None) is not None:\n            warnings.warn(\"Position ids are not supported for parameter efficient tuning. Ignoring position ids.\")\n            kwargs[\"position_ids\"] = None\n        kwargs.update(\n            {\n                \"attention_mask\": attention_mask,\n                \"labels\": labels,\n                \"output_attentions\": output_attentions,\n                \"output_hidden_states\": output_hidden_states,\n                \"return_dict\": return_dict,\n            }\n        )\n\n        if peft_config.peft_type == PeftType.PREFIX_TUNING:\n            return self._prefix_tuning_forward(input_ids=input_ids, **kwargs)\n        else:\n            if kwargs.get(\"token_type_ids\", None) is not None:\n                kwargs[\"token_type_ids\"] = torch.cat(\n                    (\n                        torch.zeros(batch_size, peft_config.num_virtual_tokens).to(self.device),\n                        kwargs[\"token_type_ids\"],\n                    ),\n                    dim=1,\n                ).long()\n            if inputs_embeds is None:\n                inputs_embeds = self.word_embeddings(input_ids)\n            prompts = self.get_prompt(batch_size=batch_size)\n            prompts = prompts.to(inputs_embeds.dtype)\n            inputs_embeds = torch.cat((prompts, inputs_embeds), dim=1)\n            return self.base_model(inputs_embeds=inputs_embeds, **kwargs)\n\n    def _prefix_tuning_forward(\n        self,\n        input_ids=None,\n        attention_mask=None,\n        inputs_embeds=None,\n        labels=None,\n        output_attentions=None,\n        output_hidden_states=None,\n        return_dict=None,\n        **kwargs,\n    ):\n        batch_size = input_ids.shape[0]\n        past_key_values = self.get_prompt(batch_size)\n        fwd_params = list(inspect.signature(self.base_model.forward).parameters.keys())\n        kwargs.update(\n            {\n                \"input_ids\": input_ids,\n                \"attention_mask\": attention_mask,\n                \"inputs_embeds\": inputs_embeds,\n                \"output_attentions\": output_attentions,\n                \"output_hidden_states\": output_hidden_states,\n                \"return_dict\": return_dict,\n                \"past_key_values\": past_key_values,\n            }\n        )\n        if \"past_key_values\" in fwd_params:\n            return self.base_model(labels=labels, **kwargs)\n        else:\n            transformer_backbone_name = self.base_model.get_submodule(self.transformer_backbone_name)\n            fwd_params = list(inspect.signature(transformer_backbone_name.forward).parameters.keys())\n            if \"past_key_values\" not in fwd_params:\n                raise ValueError(\"Model does not support past key values which are required for prefix tuning.\")\n            outputs = transformer_backbone_name(**kwargs)\n            sequence_output = outputs[0]\n            if \"dropout\" in [name for name, _ in list(self.base_model.named_children())]:\n                sequence_output = self.base_model.dropout(sequence_output)\n            logits = self.base_model.get_submodule(self.cls_layer_name)(sequence_output)\n\n            loss = None\n            loss = None\n            if labels is not None:\n                loss_fct = CrossEntropyLoss()\n                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n\n            if not return_dict:\n                output = (logits,) + outputs[2:]\n                return ((loss,) + output) if loss is not None else output\n\n            return TokenClassifierOutput(\n                loss=loss,\n                logits=logits,\n                hidden_states=outputs.hidden_states,\n                attentions=outputs.attentions,\n            )\n"}
{"type": "source_file", "path": "gen_script_long_llama.py", "content": "import json\ndef load_json(path):\n    with open(path, 'r', encoding='utf-8') as f:\n        data = json.load(f)\n    return data\n\ndef write_json(path, data):\n    with open(path, 'w', encoding='utf-8') as f:\n        json.dump(data, f, ensure_ascii=False,indent=2)\n\ndef load_jsonline(path):\n    with open(path, 'r', encoding='utf-8') as f:\n        result=[]\n        for line_s in f:\n            line=json.loads(line_s)\n            result.append(line)\n    return result\n\ndef write_jsonline(path, data):\n    with open(path, 'w', encoding='utf-8') as f:\n        for line in data:\n            line_s=json.dumps(line, ensure_ascii=False)\n            f.write(line_s)\n            f.write('\\n')\n\norder_idx = 3\n\nif order_idx == 4:\n    all_tasks=[\n        \"yelp\",\n        \"amazon\",\n        \"mnli\",\n        \"cb\",\n        \"copa\",\n        \"qqp\",\n        \"rte\",\n        \"imdb\",\n        \"sst2\",\n        \"dbpedia\",\n        \"agnews\",\n        \"yahoo\",\n        \"multirc\",\n        \"boolq\",\n        \"wic\"\n    ] # Order 4\nelse:\n    all_tasks = [\"mnli\",\n                 \"cb\",\n                 \"wic\",\n                 \"copa\",\n                 \"qqp\",\n                 \"boolq\",\n                 \"rte\",\n                 \"imdb\",\n                 \"yelp\",\n                 \"amazon\",\n                 \"sst2\",\n                 \"dbpedia\",\n                 \"agnews\",\n                 \"multirc\",\n                 \"yahoo\"] # Order 3\n\ndataset_list = all_tasks\ntask_order = ','.join(all_tasks)\n\nconfig_template={\n    \"Long_Sequence\": [\n    ],\n}\n\nimport os\nimport pathlib\nimport numpy as np\nfrom copy import deepcopy\n\nlora_r = 4\nlora_alpha = 32\nlora_dropout = 0.\nkl_ratio = 2\nattn_temperature = 1\nlearning_rate = 5e-5\nnum_train_epochs = 20\nattn_lr = 0.\nreplay_after_n_epoch = 0\n\nrun_name = f\"your_job_name\"\n\nhistory_config=[]\nfor one_data_name in dataset_list:\n\n    pathlib.Path(f'./configs/{run_name}_configs/{one_data_name}').mkdir(parents=True, exist_ok=True)\n\n    config={\n        \"sampling strategy\": \"full\",\n        \"dataset name\": f\"{one_data_name}\"\n    } \n    history_config.append(config)\n\n    dev_config=deepcopy(config_template)\n    dev_config['Long_Sequence'].append(config)\n    write_json(f'./configs/{run_name}_configs/{one_data_name}/dev_tasks.json', dev_config)\n    \n    train_config=deepcopy(config_template)\n    train_config['Long_Sequence'].append(config)\n    write_json(f'./configs/{run_name}_configs/{one_data_name}/train_tasks.json', train_config)\n\n    test_config=deepcopy(config_template)\n    test_config['Long_Sequence'].extend(history_config)\n    write_json(f'./configs/{run_name}_configs/{one_data_name}/test_tasks.json', test_config)\n\nsh_str=rf'''#!/bin/bash\n#SBATCH -J cl                           \n#SBATCH -o cl-%j.out                       \n#SBATCH -p compute \n#SBATCH -N 1                           \n#SBATCH -t 20:00:00   \n#SBATCH --mem 128G \n#SBATCH --gres=gpu:a100-sxm4-80gb:1  \n\nexport CUDA_DEVICE_ORDER=\"PCI_BUS_ID\"\n\nport=$(shuf -i25000-30000 -n1)  \n\ndeepspeed --num_gpus=4 src/run.py \\\n   --do_train \\\n   --do_predict \\\n   --predict_with_generate \\\n   --model_name_or_path your_llama_model_path \\\n   --data_dir CL_Benchmark \\\n   --task_order {task_order} \\\n   --task_config_dir configs/{run_name}_configs/{dataset_list[0]} \\\n   --output_dir logs_and_outputs/{run_name}/outputs/1-{dataset_list[0]} \\\n   --per_device_train_batch_size 2 \\\n   --per_device_eval_batch_size 32 \\\n   --gradient_accumulation_steps 4 \\\n   --learning_rate {learning_rate} \\\n   --attn_lr {attn_lr} \\\n   --num_train_epochs {num_train_epochs} \\\n   --bf16 \\\n   --deepspeed configs/ds_configs/stage2.config \\\n   --run_name {run_name} \\\n   --max_source_length 1024 \\\n   --max_target_length 50 \\\n   --generation_max_length 50 \\\n   --add_task_name False \\\n   --add_dataset_name False \\\n   --overwrite_output_dir \\\n   --overwrite_cache \\\n   --lr_scheduler_type constant \\\n   --warmup_steps 0 \\\n   --logging_strategy steps \\\n   --logging_steps 10 \\\n   --metric_for_best_model eval_exact_match \\\n   --evaluation_strategy steps \\\n   --save_strategy steps \\\n   --save_total_limit 1 \\\n   --lora_r {lora_r} \\\n   --lora_alpha {lora_alpha} \\\n   --lora_dropout {lora_dropout} \\\n   --load_best_model_at_end \\\n   --data_replay_freq -1 \\\n   --replay_after_n_epoch 0 \\\n   --kl_ratio {kl_ratio} \\\n   --attn_temperature {attn_temperature} \\\n\nrm -rf logs_and_outputs/{run_name}/outputs/1-{dataset_list[0]}/checkpoint*\n   \nsleep 5\n'''\n\nprevious_lora_path_list = []\nfor idx in range(len(dataset_list)-1):\n\n    previous_lora_path_list.append(f\"logs_and_outputs/{run_name}/outputs/{idx+1}-{dataset_list[idx]}/saved_weights\")\n    previous_lora_path = ','.join(previous_lora_path_list)\n    \n    if dataset_list[idx+1] in [\"cb\", \"copa\", \"boolq\", \"imdb\", \"dbpedia\", \"multirc\"]:\n        if dataset_list[idx+1] == \"cb\":\n            max_steps = 100\n        elif dataset_list[idx+1] == \"copa\":\n            max_steps = 200\n        elif dataset_list[idx+1] == \"boolq\":\n            max_steps = 500\n        elif dataset_list[idx+1] == \"imdb\":\n            max_steps = 250\n        elif dataset_list[idx+1] == \"dbpedia\":\n            max_steps = 200\n        else:\n            max_steps = 500\n        \n        sh_str+=rf'''\n\ndeepspeed --num_gpus=4 src/run.py \\\n   --do_train \\\n   --do_predict \\\n   --predict_with_generate \\\n   --model_name_or_path /your_llama_model_path \\\n   --load_checkpoint_from logs_and_outputs/{run_name}/outputs/{idx+1}-{dataset_list[idx]}/saved_weights/trans_input.pt \\\n   --previous_lora_path {previous_lora_path} \\\n   --previous_prompt_key_path logs_and_outputs/{run_name}/outputs/{idx+1}-{dataset_list[idx]}/saved_weights/prompts_keys_till_now.pt \\\n   --data_dir CL_Benchmark \\\n   --task_order {task_order} \\\n   --gen_data_dir generated_data/lora_gen_15datasets_t5_xl \\\n   --task_config_dir configs/{run_name}_configs/{dataset_list[idx+1]} \\\n   --output_dir logs_and_outputs/{run_name}/outputs/{idx+2}-{dataset_list[idx+1]} \\\n   --per_device_train_batch_size 2 \\\n   --per_device_eval_batch_size 32 \\\n   --gradient_accumulation_steps 4 \\\n   --learning_rate {learning_rate} \\\n   --attn_lr {attn_lr} \\\n   --max_steps {max_steps} \\\n   --bf16 \\\n   --deepspeed configs/ds_configs/stage2.config \\\n   --run_name {run_name} \\\n   --max_source_length 1024 \\\n   --max_target_length 50 \\\n   --generation_max_length 50 \\\n   --add_task_name False \\\n   --add_dataset_name False \\\n   --overwrite_output_dir \\\n   --overwrite_cache \\\n   --lr_scheduler_type constant \\\n   --warmup_steps 0 \\\n   --logging_strategy steps \\\n   --logging_steps 10 \\\n   --metric_for_best_model eval_exact_match_for_{dataset_list[idx+1]} \\\n   --evaluation_strategy steps \\\n   --save_strategy steps \\\n   --save_total_limit 1 \\\n   --load_best_model_at_end \\\n   --lora_r {lora_r} \\\n   --lora_alpha {lora_alpha} \\\n   --lora_dropout {lora_dropout} \\\n   --data_replay_freq -1 \\\n   --replay_after_n_epoch {replay_after_n_epoch} \\\n   --kl_ratio {kl_ratio} \\\n   --attn_temperature {attn_temperature} \\\n\nrm -rf logs_and_outputs/{run_name}/outputs/{idx+2}-{dataset_list[idx+1]}/checkpoint*\n\nsleep 5\n'''\n    else:\n        sh_str+=rf'''\n\ndeepspeed --num_gpus=4 src/run_llama.py \\\n   --do_train \\\n   --do_predict \\\n   --predict_with_generate \\\n   --model_name_or_path your_llama_model_path \\\n   --load_checkpoint_from logs_and_outputs/{run_name}/outputs/{idx+1}-{dataset_list[idx]}/saved_weights/trans_input.pt \\\n   --previous_lora_path {previous_lora_path} \\\n   --previous_prompt_key_path logs_and_outputs/{run_name}/outputs/{idx+1}-{dataset_list[idx]}/saved_weights/prompts_keys_till_now.pt \\\n   --data_dir CL_Benchmark \\\n   --task_order {task_order} \\\n   --gen_data_dir generated_data/lora_gen_long_llama \\\n   --task_config_dir configs/{run_name}_configs/{dataset_list[idx+1]} \\\n   --output_dir logs_and_outputs/{run_name}/outputs/{idx+2}-{dataset_list[idx+1]} \\\n   --per_device_train_batch_size 2 \\\n   --per_device_eval_batch_size 32 \\\n   --gradient_accumulation_steps 4 \\\n   --learning_rate {learning_rate} \\\n   --attn_lr {attn_lr} \\\n   --num_train_epochs {num_train_epochs} \\\n   --bf16 \\\n   --deepspeed configs/ds_configs/stage2.config \\\n   --run_name {run_name} \\\n   --max_source_length 1024 \\\n   --max_target_length 50 \\\n   --generation_max_length 50 \\\n   --add_task_name False \\\n   --add_dataset_name False \\\n   --overwrite_output_dir \\\n   --overwrite_cache \\\n   --lr_scheduler_type constant \\\n   --warmup_steps 0 \\\n   --logging_strategy steps \\\n   --logging_steps 10 \\\n   --metric_for_best_model eval_exact_match_for_{dataset_list[idx+1]} \\\n   --evaluation_strategy steps \\\n   --save_strategy steps \\\n   --save_total_limit 1 \\\n   --load_best_model_at_end \\\n   --lora_r {lora_r} \\\n   --lora_alpha {lora_alpha} \\\n   --lora_dropout {lora_dropout} \\\n   --data_replay_freq 1 \\\n   --replay_after_n_epoch {replay_after_n_epoch} \\\n   --kl_ratio {kl_ratio} \\\n   --attn_temperature {attn_temperature} \\\n\nrm -rf logs_and_outputs/{run_name}/outputs/{idx+2}-{dataset_list[idx+1]}/checkpoint*\n\nsleep 5\n'''\n    \nsh_str+=rf'''\npython score.py {run_name} single_train_results_path\n'''\n    \nwith open(f'{run_name}.sh', 'w') as f:\n    f.write(sh_str)"}
{"type": "source_file", "path": "pseudo_data/src/evaluation/evaluator.py", "content": "# Auth: Xia Han \n# Date: 2023/03/09\n\nimport json\nimport re\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport random\n\nclass MetricBase:\n    def __init__(self):\n        raise NotImplementedError()\n    def update(self, y_truth, y_pred):\n        raise NotImplementedError()\n    def get_metric(self):\n        raise NotImplementedError()\n    def get_last(self):\n        raise NotImplementedError()\n\nclass MetricAcc(MetricBase):\n    def __init__(self):\n        self.scores = []\n    def update(self, y_truth: str, y_pred: str):\n        if y_truth == y_pred:\n            self.scores.append(1)\n        else:\n            self.scores.append(0)\n    def get_metric(self):\n        if len(self.scores) == 0:\n            return 0\n        else:\n            return sum(self.scores) / len(self.scores)\n    def get_last(self):\n        return self.scores[-1]\n            \nclass MetricF1(MetricBase):\n    def __init__(self):\n        self.sum_TP = 0\n        self.sum_FN = 0\n        self.sum_FP = 0\n        self.last_TP = None\n        self.last_FN = None\n        self.last_FP = None\n    def update(self, y_truth: set, y_pred: set):\n        # TP: 在truth中存在，且在pred中存在\n        # FN: 在truth中存在，但在pred中不存在\n        # FP: 在truth中不存在，但在pred中存在\n        self.last_TP = len(y_truth & y_pred)\n        self.last_FN = len(y_truth - y_pred)\n        self.last_FP = len(y_pred - y_truth)\n        self.sum_TP += self.last_TP\n        self.sum_FN += self.last_FN\n        self.sum_FP += self.last_FP\n    def get_metric(self):\n        # TP + FN 可能为0\n        # TP + FP 可能为0\n        TP = self.sum_TP\n        FN = self.sum_FN\n        FP = self.sum_FP\n        if TP + FN == 0:\n            recall = 0\n        else:\n            recall = TP / (TP + FN)\n        if TP + FP == 0:\n            precision = 0\n        else:\n            precision = TP / (TP + FP)\n        if recall + precision == 0:\n            f1 = 0\n        else:\n            f1 = 2 * recall * precision / (recall + precision)\n        self.recall = recall\n        self.precision = precision\n        return f1\n    def get_detail(self):\n        if not hasattr(self, 'recall'):\n            f1 = self.get_metric()\n        return f1, self.recall, self.precision\n    def get_last(self):\n        return self.last_TP, self.last_FN, self.last_FP\n\nclass MetricF1NA(MetricF1):\n    \"对于RE中关系类型为NA的特殊处理\"\n    def update(self, y_truth: set, y_pred: set):\n        self.last_TP = 0\n        self.last_FN = 0\n        self.last_FP = 0\n        for truth in y_truth:\n            if ',na,' in truth:\n                pattern = re.escape(truth).replace(',na,', ',(.+),')    # 因为在evaluator._extract的时候就全部变为了小写，所以是na而非NA\n                pattern = re.compile(pattern)\n                pred_fail = False\n                for pred in y_pred:\n                    match = pattern.match(pred)\n                    if match is not None and match.group(1) != 'na':     # truth: (A,NA,B); pred:(A,notNA,B)\n                        pred_fail = True\n                        break\n                if not pred_fail:       # 只有当预测中没有给出错误的明确肯定时才加TP\n                    self.last_TP += 1    # 至于FP会在后面统计，这里就不用计算了，否则会造成重复统计\n            else:\n                if truth in y_pred:\n                    self.last_TP += 1\n                else:\n                    self.last_FN += 1\n        for pred in y_pred:\n            if ',na,' in pred:\n                pattern = re.escape(pred).replace(',na,', ',(.+),')\n                pattern = re.compile(pattern)\n                pred_fail = False\n                for truth in y_truth:\n                    match = pattern.match(truth)\n                    if match is not None and match.group(1) != 'na':    # pred: (A,NA,B); truth:(A,notNA,B)\n                        pred_fail = True\n                        break\n                if pred_fail:\n                    self.last_FP += 1\n                else:\n                    self.last_TP += 0    # 这里不太确定，对于pred给出的(A,NA,B)，如果truth中不包含(A,*,B)，是否应该算作TP? 我姑且认为是不算的，预测中给出(A,NA,B)的话，对了不加分，错了要扣分。\n            else:\n                if pred not in y_truth:\n                    self.last_FP += 1\n        self.sum_TP += self.last_TP\n        self.sum_FN += self.last_FN\n        self.sum_FP += self.last_FP\n\nclass AuditBase:\n    def __init__(self, record_limit=16):\n        # record_limit: maximum size of record, `-1` for infinite, `0` for no record\n        self.record_limit = record_limit\n        self.cnt = 0\n        self.record = []\n    def _check(self, last) -> bool:\n        # must be overrided\n        # return whether be recorded or not\n        raise NotImplementedError()\n    def _add_record(self, new_record):\n        self.cnt += 1\n        if self.record_limit < 0 or len(self.record) < self.record_limit:\n            # record limit check\n            self.record.append(new_record)\n        elif os.environ.get('RANDOM_RECORD')=='1':\n            # 流式均匀采样问题\n            if random.randint(1,self.cnt) <= self.record_limit:\n                idx = random.randint(0,len(self.record)-1)\n                self.record[idx] = new_record\n    def update(self, last):\n        if self._check(last):\n            new_record = {\n                'json_data': last['json_data'],\n                'predict': last['predict'],\n                'y_truth': last['y_truth'],\n                'y_pred': last['y_pred']\n            }\n            new_record = self._to_json_object(new_record)\n            self._add_record(new_record)\n    @staticmethod\n    def _to_json_object(obj):\n        if isinstance(obj, str) or isinstance(obj, int) or isinstance(obj, float):\n            return obj\n        if isinstance(obj, tuple) or isinstance(obj, list) or isinstance(obj, set):\n            return [AuditBase._to_json_object(x) for x in obj]\n        if isinstance(obj, dict):\n            return {AuditBase._to_json_object(k): AuditBase._to_json_object(v) for k, v in obj.items()}\n        else:\n            raise NotImplementedError()\n    def get_cnt(self):\n        return self.cnt\n    def get_record(self):\n        return self.record\n    def get_report(self):\n        return {\n            'count': self.cnt,\n            'record': self.record\n        }\n    def get_name(self):\n        # 默认为类名，如果想要定制名字的话请考虑重载此方法\n        return self.__class__.__name__\n\nclass AuditVoid(AuditBase):\n    \"检测空输出\"\n    def _check(self, last) -> bool:\n        return last['predict'].strip() == ''\n\nclass AuditLong(AuditBase):\n    \"检测过长的输出\"\n    def _check(self, last) -> bool:\n        return len(last['predict']) >= 512     # 长度上限根据需要自行修改\n\nclass AuditInsane(AuditBase):\n    \"检测胡言乱语\"\n    def _check(self, last) -> bool:\n        return last['predict'].strip().lower() not in {'na', 'no relation', 'none', '[]', ''} and len(last['y_pred']) == 0    # 说了点什么，但又什么有用的都没说\n\nclass AuditBothEmpty(AuditBase):\n    \"检测Label和predict都为空的条目\"\n    def _check(self, last) -> bool:\n        return len(last['y_truth']) == 0 and len(last['y_pred']) == 0\n\nclass AuditLabelEmptyOnly(AuditBase):\n    \"检测label为空，但predict不为空\"\n    def _check(self, last) -> bool:\n        return len(last['y_truth']) == 0 and len(last['y_pred']) != 0\n\nclass AuditPredEmptyOnly(AuditBase):\n    \"检测predict为空，label不为空\"\n    def _check(self, last) -> bool:\n        return len(last['y_truth']) != 0 and len(last['y_pred']) == 0\n    \nclass AuditNA(AuditBase):\n    \"检测包含类型为NA的输出，目前只用于RE\"\n    def _check(self, last) -> bool:\n        for i in last['y_pred']:    # assert isinstance(i, str)\n            if ',na,' in i:\n                return True\n        return False\n\nclass AuditInvalid(AuditBase):\n    \"检测包含非法标签类型的输出，目前只用于RE和NER\"\n    def _check(self, last) -> bool:\n        valid_labels = EvaluatorBase._resolve_option(last['json_data']['Instance']['instruction'])\n        if len(valid_labels) == 0:\n            # 如果是没有提供option，则忽略该审计项\n            return False\n        valid_labels = set(valid_labels)\n\n        for pred in last['y_pred']:\n            pred = pred.split(':')\n            if len(pred) >= 2:\n                label = pred[0]\n                if label not in valid_labels:\n                    return True\n        return False\n\nclass AuditFidelity(AuditBase):\n    \"检测不来源于句子的实体，目前只用于RE和NER\"\n    def _check(self, last) -> bool:\n        for item in last['y_pred']:\n            item = item.split(':')       #   这里对于实体或标签本身就包含逗号的情况不好处理，\n            if len(item) < 2:\n                continue\n            ents = item[-1].split(',')\n            for ent in ents:\n                if EvaluatorBase._format(ent) not in EvaluatorBase._format(last['json_data']['Instance']['sentence']):\n                    return True\n            return False\n\nclass AuditGoldenlabelFault(AuditBase):\n    \"golden label中的三元组有空缺，目前只用于RE\"\n    def _check(self, last) -> bool:\n        for item in last['y_truth']:\n            cnt = 0\n            if len(item.split(':')) < 2:\n                continue\n            for i in item.split(':')[-1].split(','):\n                i = i.strip()\n                if i != '':\n                    cnt += 1\n            if cnt <= 1:\n                return True\n        return False\n\nclass AuditRepeat(AuditBase):\n    \"检测复读机\"\n    def _check(self, last) -> bool:\n        pattern = r'(\\w{5,})\\1{2,}'  # 匹配连续出现三次及以上的长度大于5的子串\n        match = re.search(pattern, last['predict'])\n        return match is not None\n\nclass AuditRetard(AuditBase):\n    \"检测二者都非空前提下的错误\"\n    def _check(self, last) -> bool:\n        last_metric = last['metric']\n        if hasattr(last_metric, 'last_TP'):\n            if len(last['y_pred']) != 0 and len(last['y_truth']) != 0:\n                return last_metric.last_TP == 0\n        if hasattr(last_metric, 'scores'):\n            return last_metric.scores[-1] == 0\n        return False\n        \nclass AuditWhatever(AuditBase):\n    \"无差别逮捕\"\n    def _check(self, last) -> bool:\n        return True\n    \nclass AuditConfuseMatrix(AuditBase):\n    \"\"\"\n    1.检测自相矛盾，比如一个对于同一个实体有两个不同的标签\n        例如：[(texas, person), (texas, place)]\n    2.同时维护和导出混淆矩阵，只用于NER和RE\n    \"\"\"\n    # 这个子类的第二个功能背离了Audit系列的初衷，或许放到Metric系列会更好\n    # 但是后者的方案会需要扩大Metric能获得的信息范围，框架需要大改。\n    # 或许所有的代码都是这样一步一步变成屎山的吧。。\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.options = None\n        self.options2idx = None\n        self.matrix = None\n        self.dataset_name = None\n    def _check(self, last) -> bool:\n        raise NotImplementedError()\n    @staticmethod\n    def _resolve(s):\n        # 'A,B,C' --> 'A,C', 'B'\n        # 'A,B' --> 'A', 'B'\n        # 'A,B,C,D' --> None\n        # 此处假定s已经经过了标准格式化\n        s = [i.strip() for i in s.split(',')]\n        if len(s) == 2:\n            return s[0], s[1]\n        elif len(s) == 3:\n            return '%s,%s'%(s[0],s[2]), s[1]\n        else:\n            return None\n    def update(self, last):\n        if self.dataset_name is None:\n            self.dataset_name = last['json_data']['Dataset']\n            self.options = EvaluatorBase._resolve_option(last['json_data']['Instance']['instruction'])\n            if 'na' not in self.options:\n                self.options.append('na')\n            self.options2idx = dict()\n            for idx, option in enumerate(self.options):\n                self.options2idx[option] = idx\n            N = len(self.options)\n            self.matrix = np.zeros((N, N), dtype=np.int16)\n        truth = dict()\n        pred = dict()\n        is_conflict = False\n        for item in last['y_truth']:\n            res = self._resolve(item)\n            if res is not None:\n                if res[0] in truth:\n                    is_conflict = True\n                truth[res[0]] = res[1]\n        for item in last['y_pred']:\n            res = self._resolve(item)\n            if res is not None:\n                if res[0] in pred:\n                    is_conflict = True\n                pred[res[0]] = res[1]\n        for k in truth:\n            if truth[k] not in self.options2idx:\n                continue    # 可能是因为出现了意料之外的格式解析异常\n            idx_truth = self.options2idx[truth[k]]\n            if k in pred:\n                if pred[k] in self.options2idx:\n                    idx_pred = self.options2idx[pred[k]]\n                    self.matrix[idx_truth][idx_pred] += 1\n            else:\n                idx_pred = self.options2idx['na']\n                self.matrix[idx_truth][idx_pred] += 1\n        for k in pred:\n            if pred[k] not in self.options2idx:\n                continue\n            idx_pred = self.options2idx[pred[k]]\n            if k not in truth:\n                idx_truth = self.options2idx['na']\n                self.matrix[idx_truth][idx_pred] += 1\n\n        if is_conflict:\n            new_record = {\n                'json_data': last['json_data'],\n                'predict': last['predict'],\n                'y_truth': list(last['y_truth']),\n                'y_pred': list(last['y_pred'])\n            }\n            new_record = self._to_json_object(new_record)\n            self._add_record(new_record)\n\n    def get_report(self):\n        if os.environ.get('EXPORT_IMG') == '1':\n            root = 'img'    # 虽然硬编码是坏行为，但这是生成只写的临时数据，该数据只会被用户阅读，不会被程序读取。\n            if not os.path.exists(root):\n                os.mkdir(root)\n            fpath = os.path.join(root, '%s.png'%self.dataset_name)\n            if True:\n                # 大部分时候并不关心主对角线和na上的元素，mask掉减少视觉干扰\n                matrix = ((1-np.eye(self.matrix.shape[0])) * self.matrix).astype(np.int16)    \n                na = self.options2idx['na']\n                matrix[na,:]=0\n                matrix[:,na]=0\n            else:\n                matrix = self.matrix\n            self._plot_matrix(matrix, self.options, fpath, title=self.dataset_name)\n        return super().get_report()\n    @staticmethod\n    def _plot_matrix(A, labels, fpath, title=None, min_size = 50):\n        N = len(labels)\n        figsize = N * min_size / 100 + 1, N * min_size / 100 + 1\n        fig, ax = plt.subplots(figsize=figsize)\n\n        im = ax.imshow(A, cmap='viridis')\n\n        ax.set_xticks(np.arange(N))\n        ax.set_yticks(np.arange(N))\n        ax.set_xticklabels(labels)\n        ax.set_yticklabels(labels)\n\n        plt.setp(ax.get_xticklabels(), rotation=45, ha='right', rotation_mode='anchor')\n\n        for i in range(N):\n            for j in range(N):\n                text = ax.text(j, i, A[i, j], ha='center', va='center', color='w')\n        \n        if title is not None:\n            ax.set_title(title)\n        fig.tight_layout()\n        plt.savefig(fpath)\n        plt.close()\n    \nclass EvaluatorBase:\n    def __init__(self):\n        self.last = dict()\n        self._init_audit()\n        self._init_metric()\n    \n    def _init_metric(self):\n        # must be overrided to init self.metric\n        self.metric = MetricBase()\n\n    def _init_audit(self):\n        # override if necessary\n        # 如果需要添加其他审计项目或者自定义实例化的话请考虑重载此方法\n        self.audit = [\n            AuditVoid(),\n            AuditBothEmpty(),\n            AuditLabelEmptyOnly(),\n            AuditPredEmptyOnly(),\n            AuditLong(),\n            AuditInsane(),\n            AuditRepeat(),\n            AuditRetard(),\n            AuditWhatever()\n        ]\n    \n    def _update_audit(self):\n        # override if necessary\n        for audit in self.audit:\n            audit.update(self.last)\n\n    def _extract(self, json_data, predict: str):\n        # must be overrided\n        # return: y_truth, y_pred\n        raise NotImplementedError()\n\n    def add(self, json_data, predict):\n        \"\"\"\n        可以输入多条数据，也可以输入一条数据。\n        当输入单条数据时，json_data应该为json-like object(即json.load解析出来的)，predict应该为单个字符串。\n        当输入为多条数据时，json_data和predict都应该为列表。\n        \"\"\"\n        assert isinstance(json_data, list) == isinstance(predict, list)\n\n        if isinstance(json_data, list) and isinstance(predict, list):\n            for i, j in zip(json_data, predict):\n                self.add(i, j)\n                return\n        \n        # add single case\n        y_truth, y_pred = self._extract(json_data, predict)\n        self.metric.update(y_truth, y_pred)\n\n        # audit\n        # last中存储需要提交审计的所有可能会用到的信息\n        self.last['json_data'] = json_data\n        self.last['predict'] = predict\n        self.last['y_truth'] = y_truth\n        self.last['y_pred'] = y_pred\n        self.last['metric'] = self.metric\n\n        self._update_audit()\n    \n    def get_metric(self) -> float:\n        return self.metric.get_metric()\n\n    def get_audit_report(self):\n        '获取所有审计项结果报告，返回一个json-like object'\n        return {\n            a.get_name() : a.get_report()\n            for a in self.audit\n        }\n    def dump_audit_report(self, fpath):\n        with open(fpath, 'w', encoding='utf-8') as f:\n            json.dump(self.get_audit_report(), f, indent=4, ensure_ascii=False)\n    \n    @staticmethod\n    def _resolve_option(s):\n        \"s: instruction\"\n        option_parts = re.findall('Option:(.+?)\\n', s)\n        if len(option_parts) <= 0:\n            return []\n        option_part = option_parts[0]\n        ans = [EvaluatorBase._format(x) for x in option_part.split(',')]\n        return ans\n\n    @staticmethod\n    def _remove_redundant_space(s):\n        # '   a  b  \\t  c  \\n' --> 'a b c'\n        #'  kjc,  jns , ((  : ()  )  ( . )( ln  kc  a,,  ' --> 'kjc,jns,((:())(.)(ln kc a,,'\n        s = ' '.join(s.split())     # 多个空白字符变为单个空格\n        s = re.sub(r\"\\s*(,|:|\\(|\\)|\\.|_|;|'|-)\\s*\", r'\\1', s)   #去除特殊符号旁的空白字符\n        return s\n    \n    @staticmethod\n    def _format(s):\n        \"集大成的格式规范化，集中解决各种格式的疑难杂症\"\n        s = EvaluatorBase._remove_redundant_space(s)\n        s = s.lower()\n        s = s.replace('{','').replace('}','')\n        s = re.sub(',+', ',', s)\n        s = re.sub('\\.+', '.', s)\n        s = re.sub(';+', ';', s)\n        s = s.replace('’', \"'\")\n        s = s.replace('location', 'located')\n        return s\n    \n    @staticmethod\n    def _re_item(s):\n        # '   A,B,C),   (D,EF),  ,,(GH ' --> ['A,B,C', 'D,EF', 'GH']\n        # ' A,B,C)  ' --> ['A,B,C']\n        # 因为有时模型的输出会缺少开头的左括号或者结尾的右括号\n        # 该正则表达式不捕获括号，只捕获中间的内容\n        # Deprecated\n        return re.findall(r'(?:^|\\()([^\\(\\)]+?)(?:$|\\))', s.strip())\n    \n    @staticmethod\n    def _resolve_brackets(s):\n        # 将最上层的配对括号内的内容抽取出来，以字符串列表的形式返回，抛弃括号外的内容。\n        # 此函数容忍句子开头缺失的一个左括号和句子结尾缺失的一个右括号（但不会同时容忍）\n        # 'a(b)(c(d))(' --> ['b', 'c(d)']\n        ans = []\n        level = 0\n        last_lb_idx = None\n        for idx, char in enumerate(s):\n            if char == '(':\n                if level == 0:\n                    last_lb_idx = idx\n                level += 1\n            elif char == ')':\n                if last_lb_idx is None and len(ans) == 0 and 0 != idx:\n                    ans.append(s[0 : idx])\n                if level == 1 and last_lb_idx+1 != idx:\n                    ans.append(s[last_lb_idx+1 : idx])\n                if level >= 1:\n                    level -= 1\n        if level == 1 and last_lb_idx+1 != len(s):\n            ans.append(s[last_lb_idx+1:])\n        return ans\n    \n    @staticmethod\n    def _resolve_comma(s):\n        # 将句子按逗号分割，但是括号内的逗号不算，分割出来的空字符串忽略\n        # 'a,(b,c),,d,' --> ['a', '(b,c)', 'd']\n        ans = []\n        level = 0\n        last_comma = -1\n        for idx, char in enumerate(s):\n            if char == '(':\n                level += 1\n            elif char == ')':\n                level -= 1\n            elif char == ',' and level == 0 and last_comma + 1 != idx:\n                ans.append(s[last_comma+1 : idx])\n                last_comma = idx\n        if last_comma+1 != len(s):\n            ans.append(s[last_comma+1:])\n        return ans\n\nclass EvaluatorNER(EvaluatorBase):\n    def _init_metric(self):\n        self.metric = MetricF1()\n\n    def _init_audit(self):\n        super()._init_audit()    \n        self.audit += [\n            AuditInvalid(),\n            AuditFidelity(),\n            AuditConfuseMatrix()\n        ]\n    def _extract(self, json_data, predict):\n        # person: a; person: b; org: c\n        entity_truth = set()\n        for ent in self._format(json_data['Instance']['ground_truth']).split(';'):\n            ent = self._format(ent)\n            entity_truth.add(ent)\n        \n        entity_pred = set()\n        for ent in self._format(predict).split(';'):\n            # 部分地名可能会包含逗号，因此这里不检查逗号个数\n            ent = self._format(ent)\n            entity_pred.add(ent)\n        return entity_truth, entity_pred\n\nclass EvaluatorRE(EvaluatorBase):\n    def _init_metric(self):\n        self.metric = MetricF1NA()  # 对NA类关系的特殊处理\n\n    def _init_audit(self):\n        super()._init_audit()    \n        self.audit += [\n            AuditInvalid(),\n            AuditFidelity(),\n            AuditGoldenlabelFault(),\n            AuditConfuseMatrix(),\n            AuditNA()\n        ]\n\n    def _extract(self, json_data, predict):\n        y_truth = set()\n        # pattern = r'(?:head entity:)(.+?)(?:\\s*,\\s*)+(?:tail entity:)(.+?)(?:\\s*,\\s*)+(?:relation:)(.+?)(?:\\s*,\\s*)*$'\n        for rel in self._format(json_data['Instance']['ground_truth']).split(';'):   # FIXME:字段名可能有变\n            # type为'no_relation'或'NA'的关系现在不忽略，下同\n            # elem = re.findall(pattern, rel)\n            # if len(elem) == 0:\n            #     continue\n            # elem = ','.join(self._format(i) for i in elem[0])\n            # elem = self._format(elem)\n            elem = self._format(rel)\n            if ':' not in elem:\n                continue\n            y_truth.add(elem)\n\n        y_pred = set()\n        # 如果模型输出'no relation'或'[]'，则认为其预测的关系集合为空集，但这里并不需要做特殊判别\n        for rel in self._format(predict).split(';'):\n            # 因为字段中可能本身就存在逗号，此处不再进行数量校验\n            # elem = re.findall(pattern, rel)\n            # if len(elem) == 0:\n            #     continue\n            # elem = ','.join(self._format(i) for i in elem[0])\n            # elem = self._format(elem)       # 没有一个format解决不了的问题，如果有，就多加几个\n            elem = self._format(rel)\n            if ':' not in elem:\n                continue\n            y_pred.add(elem)\n        return y_truth, y_pred\n\nclass EvaluatorMRC(EvaluatorBase):\n    def _init_metric(self):\n        self.metric = MetricF1()\n    def _extract(self, json_data, predict):\n        truth = self._remove_redundant_space(json_data['answer_text'])\n        pred = self._remove_redundant_space(predict)\n        return truth.lower(), pred.lower()\n\n\nclass EvaluatorSM(EvaluatorBase):\n    def _init_metric(self):\n        self.metric = MetricAcc()\n    def _extract(self, json_data, predict):\n        y_truth = self._remove_redundant_space(json_data['ground_truth'])\n        y_pred = self._remove_redundant_space(predict)\n        trans_dict = {\n            '是': 'Yes',\n            '否': 'No',\n            'yes': 'Yes',\n            'no': 'No'\n        }\n        if y_truth in trans_dict:\n            y_truth = trans_dict[y_truth]\n        if y_pred in trans_dict:\n            y_pred = trans_dict[y_pred]\n        return y_truth.lower(), y_pred.lower()\n\nclass EvaluatorEvent(EvaluatorBase):\n    def _init_metric(self):\n        self.metric = MetricAcc()\n    def _extract(self, json_data, predict):\n        y_truth = set()\n        for event in self._resolve_brackets(json_data['Instance']['ground_truth']):   # FIXME:字段名可能有变\n            event = self._format(event)\n            event = event.replace('arguments:', '')\n            event_elements = self._resolve_comma(event)  # 因为后面会排序，所以每个pair的规整化需要提前进行\n            \n            event_string = ','.join(sorted(event_elements)) # 'a:b,c:d'\n            y_truth.add(event_string)\n        \n        y_pred = set()\n        for event in self._resolve_brackets(predict):\n            event = self._format(event)\n            event = event.replace('arguments:', '')\n            event_elements = self._resolve_comma(event)  # 因为后面会排序，所以每个pair的规整化需要提前进行\n            \n            event_string = ','.join(sorted(event_elements)) # 'a:b,c:d'\n            y_pred.add(event_string)\n        return y_truth, y_pred\n\nclass EvaluatorEET(EvaluatorBase):\n    def _init_metric(self):\n        self.metric = MetricAcc()\n    def _extract(self, json_data, predict: str):\n        y_truth = json_data['Instance']['ground_truth']\n        y_truth = self._format(y_truth)\n\n        y_pred = self._format(predict)\n        return y_truth, y_pred\n\nclass EvaluatorEEA(EvaluatorBase):\n    def _init_metric(self):\n        self.metric = MetricF1()\n    def _extract(self, json_data, predict: str):\n        y_truth = set()\n        for item in json_data['Instance']['ground_truth'].split(';'):\n            if ':' not in item:\n                continue\n            y_truth.add(self._format(item))\n        \n        y_pred = set()\n        for item in self._format(predict).split(';'):\n            if ':' not in item:\n                continue\n            y_pred.add(self._format(item))\n        \n        return y_truth, y_pred\n\n# 因为后来的实际格式与最初表格中的不同，因此下列测试可能无法通过，仅作为使用示例\nif __name__ == '__main__':\n    eval_ner = EvaluatorNER()\n    eval_re = EvaluatorRE()\n    eval_event = EvaluatorEvent()\n    eval_mrc = EvaluatorMRC()\n    eval_sm = EvaluatorSM()\n    def test(evaluator:EvaluatorBase, json_str, predict):\n        json_data = json.loads(json_str)\n        evaluator.add(json_data, predict)\n        print(evaluator.get_metric())\n        print(evaluator.get_report())\n    \n    test(\n        eval_ner,\n        \"\"\"\n        [{\n            \"sentence\": \"我在复旦大学上学，想去墨脱喝奶茶。\",\n            \"entities\": [\n                {\n                    \"name\": \"复旦大学\",\n                    \"type\": \"organization\",\n                    \"pos\": [\n                        2,\n                        6\n                    ]\n                },\n                {\n                    \"name\": \"墨脱\",\n                    \"type\": \"location\",\n                    \"pos\": [\n                        11,\n                        13\n                    ]\n                }\n            ]\n        }]\n        \"\"\",\n        [\"organization: 复旦大学, location: 墨脱\"]\n    )\n    print('Expected Result: 1.0')\n\n    test(\n        eval_re,\n        \"\"\"\n        [{\n            \"sentence\": \"我在复旦大学上学，想去墨脱喝奶茶。\",\n            \"relations\": [\n                {\n                    \"head\": \n                            {\"name\": \"复旦大学\",\n                                \"type\": \"organization\",\n                                \"pos\": [\n                                    2,\n                                    6\n                                ]\n                            },\n                    \"type\":\n                            \"no_relation\",\n                    \"tail\":\n                            {\"name\": \"墨脱\",\n                            \"type\": \"location\",\n                            \"pos\": [\n                                11,\n                                13\n                                ]\n                            }\n                }\n            ]\n        },\n        {\n            \"sentence\": \"小明在上海的复旦大学上学。\",\n            \"relations\": [\n                {\n                    \"head\": \n                            {\"name\": \"复旦大学\",\n                                \"type\": \"organization\",\n                                \"pos\": [\n                                    6,\n                                    10\n                                ]\n                            },\n                    \"type\":\n                            \"locate in\",\n                    \"tail\":\n                            {\"name\": \"上海\",\n                            \"type\": \"location\",\n                            \"pos\": [\n                                3,\n                                5\n                                ]\n                            }\n                },\n                {\n                    \"head\": \n                            {\"name\": \"小明\",\n                                \"type\": \"person\",\n                                \"pos\": [\n                                    0,\n                                    2\n                                ]\n                            },\n                    \"type\":\n                            \"belong to\",\n                    \"tail\":\n                            {\"name\": \"复旦大学\",\n                            \"type\": \"organization\",\n                            \"pos\": [\n                                6,\n                                10\n                                ]\n                            }\n                }\n            ]\n        }\n        ]\n        \"\"\",\n        ['no relation', '(复旦大学, locate  in, 上海), (李田所,belong to, 复旦大学)']\n    )\n    print('Expected Result: 0.5')\n\n    test(\n        eval_event,\n        \"\"\"\n        [{\n            \"sentence\": \"雀巢裁员4000人：时代抛弃你时，连招呼都不会打！\",\n            \"events\": [\n                {\n                    \"trigger\": \"裁员\",\n                    \"type\": \"组织关系-裁员\", \n                    \"pos\":[\n                        2,\n                        3\n                    ],\n                    \"arguments\": [\n                        {\n                            \"name\": \"雀巢\",\n                            \"role\": \"裁员方\", \n                            \"pos\":[\n                                0,\n                                2\n                            ]\n                        }, \n                        {\n                            \"name\": \"4000人\",\n                            \"role\": \"裁员人数\", \n                            \"pos\":[\n                                4,\n                                9\n                            ]\n                        }\n                    ]\n                }\n            ]\n        }]\n        \"\"\",\n        [\"(event type : 组织关系-裁员, trigger: 裁员, 裁员方: 雀巢, 裁员人数: 4000人), (event type : 组织关系-裁员, trigger: 裁员, 裁员方: 雀巢, 裁员人数: 3000人)\"]\n    )\n    print('Expected Result: 2/3')\n    test(\n        eval_mrc,\n        \"\"\"\n        [{\n            \"paragraph\": \"Another approach to brain function is to examine the consequences of damage to specific brain areas. Even though it is protected by the skull and meninges, surrounded by cerebrospinal fluid, and isolated from the bloodstream by the blood\\u2013brain barrier, the delicate nature of the brain makes it vulnerable to numerous diseases and several types of damage. In humans, the effects of strokes and other types of brain damage have been a key source of information about brain function. Because there is no ability to experimentally control the nature of the damage, however, this information is often difficult to interpret. In animal studies, most commonly involving rats, it is possible to use electrodes or locally injected chemicals to produce precise patterns of damage and then examine the consequences for behavior.\",\n            \"question\": \"What sare the benifts of the blood brain barrir?\",\n            \"answer_start\": 195,\n            \"answer_text\": \"isolated from the bloodstream\"\n        }]\n        \"\"\",\n        [\"isolated from the bloodstream\"]\n    )\n    print('Expected Result: 1.0')\n    test(\n        eval_sm,\n        \"\"\"\n        [{\n            \"sen1\": \"A man with a hard hat is dancing.\",\n            \"sen2\": \"A man wearing a hard hat is dancing.\",\n            \"label\": \"Yes\"\n        },\n        {\n            \"sen1\": \"蚂蚁借呗等额还款可以换成先息后本吗\",\n            \"sen2\": \"借呗有先息到期还本吗\",\n            \"label\": \"否\"\n        }\n        ]\n        \"\"\",\n        [\"是\", \"no\"]\n    )\n    print('Expected Result: 1.0')"}
{"type": "source_file", "path": "pseudo_data/src/peft_1/utils/adapters_utils.py", "content": "# coding=utf-8\n# Copyright 2023-present the HuggingFace Inc. team.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nWEIGHTS_NAME = \"adapter_model.bin\"\nCONFIG_NAME = \"adapter_config.json\"\n\n# TODO: add automapping and superclass here?\n"}
{"type": "source_file", "path": "pseudo_data/src/peft_1/utils/__init__.py", "content": "# flake8: noqa\n# There's no way to ignore \"F401 '...' imported but unused\" warnings in this\n# module, but to preserve other warnings. So, don't check this module at all\n\n# coding=utf-8\n# Copyright 2023-present the HuggingFace Inc. team.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom .config import PeftConfig, PeftType, PromptLearningConfig, TaskType\nfrom .other import (\n    TRANSFORMERS_MODELS_TO_PREFIX_TUNING_POSTPROCESS_MAPPING,\n    TRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING,\n    TRANSFORMERS_MODELS_TO_ADALORA_TARGET_MODULES_MAPPING,\n    CONFIG_NAME,\n    WEIGHTS_NAME,\n    _set_trainable,\n    bloom_model_postprocess_past_key_value,\n    prepare_model_for_int8_training,\n    shift_tokens_right,\n    transpose,\n    _get_submodules,\n    _set_adapter,\n    _freeze_adapter,\n    ModulesToSaveWrapper,\n)\nfrom .save_and_load import get_peft_model_state_dict, set_peft_model_state_dict\n"}
{"type": "source_file", "path": "pseudo_data/src/rouge/setup.py", "content": "# coding=utf-8\n# Copyright 2022 The Google Research Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport setuptools\n\nwith open(\"README.md\", \"r\") as fh:\n    long_description = fh.read()\n\nsetuptools.setup(\n    name=\"rouge_score\",\n    version=\"0.0.4\",\n    author=\"Google LLC\",\n    author_email=\"no-reply@google.com\",\n    description=\"Pure python implementation of ROUGE-1.5.5.\",\n    long_description=long_description,\n    long_description_content_type=\"text/markdown\",\n    url=\"https://github.com/google-research/google-research/tree/master/rouge\",\n    packages=[\"rouge_score\"],\n    package_dir={\"rouge_score\": \"\"},\n    classifiers=[\n        \"Programming Language :: Python :: 3\",\n        \"License :: OSI Approved :: Apache Software License\",\n        \"Operating System :: OS Independent\",\n    ],\n    install_requires=[\n        \"absl-py\",\n        \"nltk\",\n        \"numpy\",\n        \"six>=1.14.0\",\n    ],\n    python_requires=\">=2.7\",\n)\n"}
{"type": "source_file", "path": "pseudo_data/src/rouge/__init__.py", "content": "# coding=utf-8\n# Copyright 2022 The Google Research Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\n"}
{"type": "source_file", "path": "pseudo_data/src/rouge/tokenizers.py", "content": "# coding=utf-8\n# Copyright 2022 The Google Research Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Library containing Tokenizer definitions.\n\nThe RougeScorer class can be instantiated with the tokenizers defined here. New\ntokenizers can be defined by creating a subclass of the Tokenizer abstract class\nand overriding the tokenize() method.\n\"\"\"\nimport abc\nfrom nltk.stem import porter\nfrom rouge import tokenize\n\n\nclass Tokenizer(abc.ABC):\n  \"\"\"Abstract base class for a tokenizer.\n\n  Subclasses of Tokenizer must implement the tokenize() method.\n  \"\"\"\n\n  @abc.abstractmethod\n  def tokenize(self, text):\n    raise NotImplementedError(\"Tokenizer must override tokenize() method\")\n\n\nclass DefaultTokenizer(Tokenizer):\n  \"\"\"Default tokenizer which tokenizes on whitespace.\"\"\"\n\n  def __init__(self, use_stemmer=False):\n    \"\"\"Constructor for DefaultTokenizer.\n\n    Args:\n      use_stemmer: boolean, indicating whether Porter stemmer should be used to\n      strip word suffixes to improve matching.\n    \"\"\"\n    self._stemmer = porter.PorterStemmer() if use_stemmer else None\n\n  def tokenize(self, text):\n    return tokenize.tokenize(text, self._stemmer)\n"}
{"type": "source_file", "path": "pseudo_data/src/peft_1/utils/config.py", "content": "# coding=utf-8\n# Copyright 2023-present the HuggingFace Inc. team.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport enum\nimport json\nimport os\nfrom dataclasses import asdict, dataclass, field\nfrom typing import Optional, Union\n\nfrom huggingface_hub import hf_hub_download\nfrom transformers.utils import PushToHubMixin\n\nfrom .other import CONFIG_NAME\n\n\nclass PeftType(str, enum.Enum):\n    PROMPT_TUNING = \"PROMPT_TUNING\"\n    P_TUNING = \"P_TUNING\"\n    PREFIX_TUNING = \"PREFIX_TUNING\"\n    LORA = \"LORA\"\n    ADALORA = \"ADALORA\"\n    ADAPTION_PROMPT = \"ADAPTION_PROMPT\"\n\n\nclass TaskType(str, enum.Enum):\n    SEQ_CLS = \"SEQ_CLS\"\n    SEQ_2_SEQ_LM = \"SEQ_2_SEQ_LM\"\n    CAUSAL_LM = \"CAUSAL_LM\"\n    TOKEN_CLS = \"TOKEN_CLS\"\n\n\n@dataclass\nclass PeftConfigMixin(PushToHubMixin):\n    r\"\"\"\n    This is the base configuration class for PEFT adapter models. It contains all the methods that are common to all\n    PEFT adapter models. This class inherits from [`~transformers.utils.PushToHubMixin`] which contains the methods to\n    push your model to the Hub. The method `save_pretrained` will save the configuration of your adapter model in a\n    directory. The method `from_pretrained` will load the configuration of your adapter model from a directory.\n\n    Args:\n        peft_type (Union[[`~peft.utils.config.PeftType`], `str`]): The type of Peft method to use.\n    \"\"\"\n    peft_type: Optional[PeftType] = field(default=None, metadata={\"help\": \"The type of PEFT model.\"})\n\n    @property\n    def __dict__(self):\n        return asdict(self)\n\n    def to_dict(self):\n        return self.__dict__\n\n    def save_pretrained(self, save_directory, **kwargs):\n        r\"\"\"\n        This method saves the configuration of your adapter model in a directory.\n\n        Args:\n            save_directory (`str`):\n                The directory where the configuration will be saved.\n            kwargs (additional keyword arguments, *optional*):\n                Additional keyword arguments passed along to the [`~transformers.utils.PushToHubMixin.push_to_hub`]\n                method.\n        \"\"\"\n        if os.path.isfile(save_directory):\n            raise AssertionError(f\"Provided path ({save_directory}) should be a directory, not a file\")\n\n        os.makedirs(save_directory, exist_ok=True)\n\n        output_dict = self.__dict__\n        output_path = os.path.join(save_directory, CONFIG_NAME)\n\n        # save it\n        with open(output_path, \"w\") as writer:\n            writer.write(json.dumps(output_dict, indent=2, sort_keys=True))\n\n    @classmethod\n    def from_pretrained(cls, pretrained_model_name_or_path, subfolder=None, **kwargs):\n        r\"\"\"\n        This method loads the configuration of your adapter model from a directory.\n\n        Args:\n            pretrained_model_name_or_path (`str`):\n                The directory or the Hub repository id where the configuration is saved.\n            kwargs (additional keyword arguments, *optional*):\n                Additional keyword arguments passed along to the child class initialization.\n        \"\"\"\n        path = (\n            os.path.join(pretrained_model_name_or_path, subfolder)\n            if subfolder is not None\n            else pretrained_model_name_or_path\n        )\n        if os.path.isfile(os.path.join(path, CONFIG_NAME)):\n            config_file = os.path.join(path, CONFIG_NAME)\n        else:\n            try:\n                config_file = hf_hub_download(pretrained_model_name_or_path, CONFIG_NAME, subfolder=subfolder)\n            except Exception:\n                raise ValueError(f\"Can't find '{CONFIG_NAME}' at '{pretrained_model_name_or_path}'\")\n\n        loaded_attributes = cls.from_json_file(config_file)\n\n        config = cls(**kwargs)\n\n        for key, value in loaded_attributes.items():\n            if hasattr(config, key):\n                setattr(config, key, value)\n\n        return config\n\n    @classmethod\n    def from_json_file(cls, path_json_file, **kwargs):\n        r\"\"\"\n        Loads a configuration file from a json file.\n\n        Args:\n            path_json_file (`str`):\n                The path to the json file.\n        \"\"\"\n        with open(path_json_file, \"r\") as file:\n            json_object = json.load(file)\n\n        return json_object\n\n\n@dataclass\nclass PeftConfig(PeftConfigMixin):\n    \"\"\"\n    This is the base configuration class to store the configuration of a [`PeftModel`].\n\n    Args:\n        peft_type (Union[[`~peft.utils.config.PeftType`], `str`]): The type of Peft method to use.\n        task_type (Union[[`~peft.utils.config.TaskType`], `str`]): The type of task to perform.\n        inference_mode (`bool`, defaults to `False`): Whether to use the Peft model in inference mode.\n    \"\"\"\n\n    base_model_name_or_path: str = field(default=None, metadata={\"help\": \"The name of the base model to use.\"})\n    peft_type: Union[str, PeftType] = field(default=None, metadata={\"help\": \"Peft type\"})\n    task_type: Union[str, TaskType] = field(default=None, metadata={\"help\": \"Task type\"})\n    inference_mode: bool = field(default=False, metadata={\"help\": \"Whether to use inference mode\"})\n\n@dataclass\nclass PromptLearningConfig(PeftConfig):\n    \"\"\"\n    This is the base configuration class to store the configuration of [`PrefixTuning`], [`PromptEncoder`], or\n    [`PromptTuning`].\n\n    Args:\n        num_virtual_tokens (`int`): The number of virtual tokens to use.\n        token_dim (`int`): The hidden embedding dimension of the base transformer model.\n        num_transformer_submodules (`int`): The number of transformer submodules in the base transformer model.\n        num_attention_heads (`int`): The number of attention heads in the base transformer model.\n        num_layers (`int`): The number of layers in the base transformer model.\n    \"\"\"\n\n    num_virtual_tokens: int = field(default=None, metadata={\"help\": \"Number of virtual tokens\"})\n    token_dim: int = field(\n        default=None, metadata={\"help\": \"The hidden embedding dimension of the base transformer model\"}\n    )\n    num_transformer_submodules: Optional[int] = field(\n        default=None, metadata={\"help\": \"Number of transformer submodules\"}\n    )\n    num_attention_heads: Optional[int] = field(default=None, metadata={\"help\": \"Number of attention heads\"})\n    num_layers: Optional[int] = field(default=None, metadata={\"help\": \"Number of transformer layers\"})\n"}
{"type": "source_file", "path": "pseudo_data/src/rouge/tokenize.py", "content": "# coding=utf-8\n# Copyright 2022 The Google Research Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"A library for tokenizing text.\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport re\nimport six\n\n\n# Pre-compile regexes that are use often\nNON_ALPHANUM_PATTERN = r\"[^a-z0-9]+\"\nNON_ALPHANUM_RE = re.compile(NON_ALPHANUM_PATTERN)\nSPACES_PATTERN = r\"\\s+\"\nSPACES_RE = re.compile(SPACES_PATTERN)\nVALID_TOKEN_PATTERN = r\"^[a-z0-9]+$\"\nVALID_TOKEN_RE = re.compile(VALID_TOKEN_PATTERN)\n\n\ndef tokenize(text, stemmer):\n  \"\"\"Tokenize input text into a list of tokens.\n\n  This approach aims to replicate the approach taken by Chin-Yew Lin in\n  the original ROUGE implementation.\n\n  Args:\n    text: A text blob to tokenize.\n    stemmer: An optional stemmer.\n\n  Returns:\n    A list of string tokens extracted from input text.\n  \"\"\"\n\n  # Convert everything to lowercase.\n  text = text.lower()\n  # Replace any non-alpha-numeric characters with spaces.\n  text = NON_ALPHANUM_RE.sub(\" \", six.ensure_str(text))\n\n  tokens = SPACES_RE.split(text)\n  if stemmer:\n    # Only stem words more than 3 characters long.\n    tokens = [six.ensure_str(stemmer.stem(x)) if len(x) > 3 else x\n              for x in tokens]\n\n  # One final check to drop any empty or invalid tokens.\n  tokens = [x for x in tokens if VALID_TOKEN_RE.match(x)]\n\n  return tokens\n"}
{"type": "source_file", "path": "pseudo_data/src/rouge/create_pyrouge_files.py", "content": "# coding=utf-8\n# Copyright 2022 The Google Research Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"For creating files from {target,prediction}.txt that can be processed\nby pyrouge to compare with scores in scoring_test.py.\n\n  create_pyrouge_files -- --testdata_dir=`pwd`/testdata\n\n  # testConfidenceIntervalsAgainstRouge155WithStemming result\n  pyrouge_evaluate_plain_text_files \\\n      -s /tmp/lkj -sfp \"prediction.(.*).txt\" \\\n      -m /tmp/lkj -mfp target.#ID#.txt\n\n  pyrouge_evaluate_plain_text_files \\\n      -s /tmp/lkj -sfp \"prediction_multi.(.*).txt\" \\\n      -m /tmp/lkj -mfp target_multi.#ID#.txt\n\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\n\nfrom absl import app\nfrom absl import flags\n\nFLAGS = flags.FLAGS\n\nflags.DEFINE_string('testdata_dir', '', 'testdata path')\nflags.DEFINE_string('output',  '/tmp/lkj', 'testdata path')\n\n\ndef main(argv):\n  if len(argv) > 1:\n    raise app.UsageError('Too many command-line arguments.')\n\n  # One line per target\n  with open(os.path.join(FLAGS.testdata_dir, 'target_large.txt')) as f:\n    targets = f.readlines()\n  with open(os.path.join(FLAGS.testdata_dir, 'prediction_large.txt')) as f:\n    predictions = f.readlines()\n\n  def write_files(prefix, items):\n    for i, t in enumerate(items):\n      out = '%s.%d.txt' % (prefix, i)\n      with open(os.path.join(FLAGS.output, out), 'w') as f:\n        f.write(t)\n  write_files('target', targets)\n  write_files('prediction', predictions)\n\n  # Delete this block\n  def write_files2(prefix, items):\n    index = 0\n    f = None\n    for i, t in enumerate(items):\n      # Write 4 lines per file\n      if i % 4 == 0:\n        if f:\n          f.close()\n        f = open(\n            os.path.join(FLAGS.output, '%s.%d.txt' % (prefix, index)),\n            'w')\n        index += 1\n      f.write(t)\n    f.close()\n  write_files2('target_multi', targets)\n  write_files2('prediction_multi', predictions)\n\n\nif __name__ == '__main__':\n  app.run(main)\n"}
{"type": "source_file", "path": "src/__init__.py", "content": ""}
{"type": "source_file", "path": "pseudo_data/src/rouge/io.py", "content": "# coding=utf-8\n# Copyright 2022 The Google Research Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Library for reading/writing input and score files.\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport glob\n\nfrom absl import logging\nimport six\nfrom six.moves import zip\nfrom six.moves import zip_longest\n\n\n\ndef compute_scores_and_write_to_csv(target_filepattern,\n                                    prediction_filepattern,\n                                    output_filename,\n                                    scorer,\n                                    aggregator,\n                                    delimiter=\"\\n\"):\n  \"\"\"Runs aggregate score calculations and outputs results to a CSV file.\n\n  Args:\n    target_filepattern: Pattern for files containing target text.\n    prediction_filepattern: Pattern for files containing prediction text.\n    output_filename: Name of file to write results to.\n    scorer: A BaseScorer object to compute scores.\n    aggregator: An aggregator to aggregate scores. If None, outputs are\n      per-example scores.\n    delimiter: Record delimiter.\n  \"\"\"\n\n  target_filenames = _glob(target_filepattern)\n  prediction_filenames = _glob(prediction_filepattern)\n  if (len(target_filenames) < 1 or\n      len(target_filenames) != len(prediction_filenames)):\n    raise ValueError(\"Must have equal and positive number of target and \"\n                     \"prediction files. Found: %d target files (%s),\"\n                     \" %d prediction files (%s).\" %\n                     (len(target_filenames), target_filepattern,\n                      len(prediction_filenames), prediction_filepattern))\n\n  scores = _compute_scores(target_filenames, prediction_filenames, scorer,\n                           delimiter)\n  if aggregator:\n    for score in scores:\n      aggregator.add_scores(score)\n    _write_aggregates_to_csv(output_filename, aggregator.aggregate())\n  else:\n    _write_scores_to_csv(output_filename, scores)\n\n\ndef _glob(filepattern):\n  return glob.glob(filepattern)  # pylint: disable=unreachable\n\n\ndef _open(filepattern, mode=\"r\"):\n  return open(filepattern, mode)  # pylint: disable=unreachable\n\n\ndef _record_gen(filename, delimiter):\n  \"\"\"Opens file and yields records separated by delimiter.\"\"\"\n  with _open(filename) as f:\n    records = f.read().split(six.ensure_str(delimiter))\n  if records[-1]:\n    # Need a final delimiter at end of file to be able to detect an empty last\n    # record.\n    logging.warn(\"Expected delimiter at end of file\")\n  else:\n    records = records[:-1]\n  for record in records:\n    yield record\n\n\ndef _compute_scores(target_filenames, prediction_filenames, scorer, delimiter):\n  \"\"\"Computes aggregates scores across the given target and prediction files.\n\n  Args:\n    target_filenames: List of filenames from which to read target lines.\n    prediction_filenames: List of filenames from which to read prediction lines.\n    scorer: A BaseScorer object to compute scores.\n    delimiter: string delimiter between each record in input files\n\n  Returns:\n    A list of dicts mapping score_type to Score objects.\n  Raises:\n    ValueError: If invalid targets or predictions are provided.\n  \"\"\"\n\n  scores = []\n  for target_filename, prediction_filename in zip(\n      sorted(target_filenames), sorted(prediction_filenames)):\n    logging.info(\"Reading targets from %s.\", target_filename)\n    logging.info(\"Reading predictions from %s.\", prediction_filename)\n    targets = _record_gen(target_filename, delimiter)\n    preds = _record_gen(prediction_filename, delimiter)\n    for target_rec, prediction_rec in zip_longest(targets, preds):\n      if target_rec is None or prediction_rec is None:\n        raise ValueError(\"Must have equal number of lines across target and \"\n                         \"prediction files. Mismatch between files: %s, %s.\" %\n                         (target_filename, prediction_filename))\n      scores.append(scorer.score(target_rec, prediction_rec))\n\n  return scores\n\n\ndef _write_aggregates_to_csv(output_filename, aggregates):\n  \"\"\"Writes aggregate scores to an output CSV file.\n\n  Output file is a comma separated where each line has the format:\n    score_type-(P|R|F),low_ci,mean,high_ci\n\n  P/R/F indicates whether the score is a precision, recall or f-measure.\n\n  Args:\n    output_filename: Name of file to write results to.\n    aggregates: A dict mapping each score_type to a AggregateScore object.\n  \"\"\"\n\n  logging.info(\"Writing results to %s.\", output_filename)\n  with _open(output_filename, \"w\") as output_file:\n    output_file.write(\"score_type,low,mid,high\\n\")\n    for score_type, aggregate in sorted(aggregates.items()):\n      output_file.write(\"%s-R,%f,%f,%f\\n\" %\n                        (score_type, aggregate.low.recall, aggregate.mid.recall,\n                         aggregate.high.recall))\n      output_file.write(\"%s-P,%f,%f,%f\\n\" %\n                        (score_type, aggregate.low.precision,\n                         aggregate.mid.precision, aggregate.high.precision))\n      output_file.write(\"%s-F,%f,%f,%f\\n\" %\n                        (score_type, aggregate.low.fmeasure,\n                         aggregate.mid.fmeasure, aggregate.high.fmeasure))\n  logging.info(\"Finished writing results.\")\n\n\ndef _write_scores_to_csv(output_filename, scores):\n  \"\"\"Writes scores for each individual example to an output CSV file.\n\n  Output file is a comma separated where each line has the format:\n    id,score1,score2,score3,...\n\n  The header row indicates the type of each score column.\n\n  Args:\n    output_filename: Name of file to write results to.\n    scores: A list of dicts mapping each score_type to a Score object.\n  \"\"\"\n\n  if len(scores) < 1:\n    logging.warn(\"No scores to write\")\n    return\n  rouge_types = sorted(scores[0].keys())\n\n  logging.info(\"Writing results to %s.\", output_filename)\n  with _open(output_filename, \"w\") as out_file:\n    out_file.write(\"id\")\n    for rouge_type in rouge_types:\n      out_file.write(\",{t}-P,{t}-R,{t}-F\".format(t=rouge_type))\n    out_file.write(\"\\n\")\n    for i, result in enumerate(scores):\n      out_file.write(\"%d\" % i)\n      for rouge_type in rouge_types:\n        out_file.write(\",%f,%f,%f\" %\n                       (result[rouge_type].precision, result[rouge_type].recall,\n                        result[rouge_type].fmeasure))\n      out_file.write(\"\\n\")\n  logging.info(\"Finished writing results.\")\n"}
{"type": "source_file", "path": "pseudo_data/src/peft_1/utils/save_and_load.py", "content": "# coding=utf-8\n# Copyright 2023-present the HuggingFace Inc. team.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom .config import PeftType, PromptLearningConfig\nimport torch\n\n\ndef get_peft_model_state_dict(model, state_dict=None, adapter_name=\"default\"):\n    \"\"\"\n    Get the state dict of the Peft model.\n\n    Args:\n        model ([`PeftModel`]): The Peft model. When using torch.nn.DistributedDataParallel, DeepSpeed or FSDP,\n        the model should be the underlying model/unwrapped model (i.e. model.module).\n        state_dict (`dict`, *optional*, defaults to `None`):\n            The state dict of the model. If not provided, the state dict of the model\n        will be used.\n    \"\"\"\n    config = model.peft_config[adapter_name]\n    if state_dict is None:\n        state_dict = model.state_dict()\n        \n        # modified\n        if config.save_loranew == False:\n            flag = 1 # this is a switch represents whether 'r_sum' is written to the config file\n            for k in state_dict:\n                if \"lora_A\" in k:\n                    for k_ in state_dict:\n                        if \"loranew_A\" in k_ and k.split(\"lora_A\")[0] == k_.split(\"loranew_A\")[0]:\n                            state_dict[k] = torch.cat((state_dict[k], state_dict[k_]), dim=0) # [r_sum + r, r]\n                            if flag == 1:\n                                config.r_sum = state_dict[k].shape[0] \n                                flag = 0\n                            break # target modules have been matched\n                elif \"lora_B\" in k:\n                    for k_ in state_dict:\n                        if \"loranew_B\" in k_ and k.split(\"lora_B\")[0] == k_.split(\"loranew_B\")[0]:\n                            state_dict[k] = torch.cat((state_dict[k], state_dict[k_]), dim=1) # [r, r_sum + r]\n                            break # target modules have been matched\n\n                \n    if config.peft_type in (PeftType.LORA, PeftType.ADALORA):\n        # to_return = lora_state_dict(model, bias=model.peft_config.bias)\n        # adapted from `https://github.com/microsoft/LoRA/blob/main/loralib/utils.py`\n        # to be used directly with the state dict which is necessary when using DeepSpeed or FSDP\n        bias = config.bias\n\n        # modified\n        if bias == \"none\":\n            if config.save_loranew: \n                to_return = {k: state_dict[k] for k in state_dict if \"lora_\" in k or \"loranew_\" in k} # modified\n            else:\n                to_return = {k: state_dict[k] for k in state_dict if \"lora_\" in k}\n\n        elif bias == \"all\":\n            to_return = {k: state_dict[k] for k in state_dict if \"lora_\" in k or \"bias\" in k}\n        elif bias == \"lora_only\":\n            to_return = {}\n            for k in state_dict:\n                if \"lora_\" in k:\n                    to_return[k] = state_dict[k]\n                    bias_name = k.split(\"lora_\")[0] + \"bias\"\n                    if bias_name in state_dict:\n                        to_return[bias_name] = state_dict[bias_name]\n        else:\n            raise NotImplementedError\n\n        # modified\n        to_return = {k: v for k, v in to_return.items() if ((\"lora_\" in k and adapter_name in k) or (\"bias\" in k) or (\"loranew_\" in k))}\n        \n        if config.peft_type == PeftType.ADALORA:\n            rank_pattern = config.rank_pattern\n            if rank_pattern is not None:\n                rank_pattern = {k.replace(f\".{adapter_name}\", \"\"): v for k, v in rank_pattern.items()}\n                config.rank_pattern = rank_pattern\n                to_return = model.resize_state_dict_by_rank_pattern(rank_pattern, to_return, adapter_name)\n\n    elif config.peft_type == PeftType.ADAPTION_PROMPT:\n        to_return = {k: state_dict[k] for k in state_dict if k.split(\".\")[-1].startswith(\"adaption_\")}\n    elif isinstance(config, PromptLearningConfig):\n        to_return = {}\n        if config.inference_mode:\n            prompt_embeddings = model.prompt_encoder[adapter_name].embedding.weight\n        else:\n            prompt_embeddings = model.get_prompt_embedding_to_save(adapter_name)\n        to_return[\"prompt_embeddings\"] = prompt_embeddings\n    else:\n        raise NotImplementedError\n    if model.modules_to_save is not None:\n        for key, value in state_dict.items():\n            if any(f\"{module_name}.modules_to_save.{adapter_name}\" in key for module_name in model.modules_to_save):\n                to_return[key.replace(\"modules_to_save.\", \"\")] = value\n\n    to_return = {k.replace(f\".{adapter_name}\", \"\"): v for k, v in to_return.items()}\n    return to_return\n\n\ndef set_peft_model_state_dict(model, peft_model_state_dict, adapter_name=\"default\"):\n    \"\"\"\n    Set the state dict of the Peft model.\n\n    Args:\n        model ([`PeftModel`]): The Peft model.\n        peft_model_state_dict (`dict`): The state dict of the Peft model.\n    \"\"\"\n    config = model.peft_config[adapter_name]\n    state_dict = {}\n    if model.modules_to_save is not None:\n        for key, value in peft_model_state_dict.items():\n            if any(module_name in key for module_name in model.modules_to_save):\n                for module_name in model.modules_to_save:\n                    if module_name in key:\n                        key = key.replace(module_name, f\"{module_name}.modules_to_save.{adapter_name}\")\n                        break\n            state_dict[key] = value\n    else:\n        state_dict = peft_model_state_dict\n\n    if config.peft_type in (PeftType.LORA, PeftType.ADALORA):\n        peft_model_state_dict = {}\n        for k, v in state_dict.items():\n            if \"lora_\" in k:\n                suffix = k.split(\"lora_\")[1]\n                if \".\" in suffix:\n                    suffix_to_replace = \".\".join(suffix.split(\".\")[1:])\n                    k = k.replace(suffix_to_replace, f\"{adapter_name}.{suffix_to_replace}\")\n                else:\n                    k = f\"{k}.{adapter_name}\"\n                peft_model_state_dict[k] = v\n            \n            # modified\n            elif \"loranew_\" in k: \n                suffix = k.split(\"loranew_\")[1]\n                if \".\" in suffix:\n                    suffix_to_replace = \".\".join(suffix.split(\".\")[1:])\n                    k = k.replace(suffix_to_replace, f\"{adapter_name}.{suffix_to_replace}\")\n                else:\n                    k = f\"{k}.{adapter_name}\"\n                peft_model_state_dict[k] = v\n                \n            else:\n                peft_model_state_dict[k] = v\n        if config.peft_type == PeftType.ADALORA:\n            rank_pattern = config.rank_pattern\n            if rank_pattern is not None:\n                model.resize_modules_by_rank_pattern(rank_pattern, adapter_name)\n    elif isinstance(config, PromptLearningConfig) or config.peft_type == PeftType.ADAPTION_PROMPT:\n        peft_model_state_dict = state_dict\n    else:\n        raise NotImplementedError\n\n    model.load_state_dict(peft_model_state_dict, strict=False)\n    if isinstance(config, PromptLearningConfig):\n        model.prompt_encoder[adapter_name].embedding.load_state_dict(\n            {\"weight\": peft_model_state_dict[\"prompt_embeddings\"]}, strict=True\n        )\n"}
{"type": "source_file", "path": "pseudo_data/src/uie_trainer_lora.py", "content": "from typing import Callable, Dict, List, Optional, Tuple, Union\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset\nfrom transformers import GenerationConfig\nfrom transformers.data.data_collator import DataCollator\nfrom transformers.modeling_utils import PreTrainedModel\nfrom transformers.tokenization_utils_base import PreTrainedTokenizerBase\nfrom transformers.trainer_seq2seq import Seq2SeqTrainer\nfrom transformers.trainer import *\nfrom transformers.trainer_callback import TrainerCallback\nfrom transformers.trainer_utils import EvalPrediction\nfrom transformers.training_args import TrainingArguments\n\nfrom uie_collator import SUPPORTED_DECODER_MODELS, check_model\nfrom uie_dataset_lora import ANSWER_PREFIX\n\n\ndef skip_instructions(model, predictions_ids, tokenizer, ignore_idx=-100):\n    predictions_ids = np.where(predictions_ids == ignore_idx, tokenizer.pad_token_id, predictions_ids)\n\n    predictions = tokenizer.batch_decode(\n        predictions_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True\n    )\n\n    final_predictions = []\n    if check_model(model.config._name_or_path, SUPPORTED_DECODER_MODELS):\n        for pred in predictions:\n\n            if ANSWER_PREFIX in pred:\n                splits = pred.split(ANSWER_PREFIX)\n                final_predictions.append(splits[-1].strip())\n            else:\n                final_predictions.append('')\n    else:\n        final_predictions = predictions\n\n    return final_predictions\n\n\nclass DenserEvalCallback(TrainerCallback):\n\n    def on_step_end(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs):\n\n        log_eval_steps = [1, 50, 100, 200]\n\n        # Log\n        if args.logging_strategy == IntervalStrategy.STEPS and state.global_step in log_eval_steps:\n            control.should_log = True\n\n        # Evaluate\n        if args.evaluation_strategy == IntervalStrategy.STEPS and state.global_step in log_eval_steps:\n            control.should_evaluate = True\n\n        # Save\n        # if args.save_strategy\n\n        return control\n\n\nclass UIETrainer(Seq2SeqTrainer):\n\n    def __init__(self, *args_list, **kwags_dict):\n        super().__init__(*args_list, **kwags_dict)\n        self.save_index=0\n\n    def training_step(self, model: nn.Module, inputs: Dict[str, Union[torch.Tensor, Any]]) -> torch.Tensor:\n        \"\"\"\n        Perform a training step on a batch of inputs.\n\n        Subclass and override to inject custom behavior.\n\n        Args:\n            model (`nn.Module`):\n                The model to train.\n            inputs (`Dict[str, Union[torch.Tensor, Any]]`):\n                The inputs and targets of the model.\n\n                The dictionary will be unpacked before being fed to the model. Most models expect the targets under the\n                argument `labels`. Check your model's documentation for all accepted arguments.\n\n        Return:\n            `torch.Tensor`: The tensor with training loss on this batch.\n        \"\"\"\n        model.train()\n        inputs = self._prepare_inputs(inputs)\n\n        if is_sagemaker_mp_enabled():\n            loss_mb = smp_forward_backward(model, inputs, self.args.gradient_accumulation_steps)\n            return loss_mb.reduce_mean().detach().to(self.args.device)\n\n        with self.compute_loss_context_manager():\n            loss = self.compute_loss(model, inputs)\n\n        if self.args.n_gpu > 1:\n            loss = loss.mean()  # mean() to average on multi-gpu parallel training\n\n        if self.args.gradient_accumulation_steps > 1 and not self.deepspeed:\n            # deepspeed handles loss scaling by gradient_accumulation_steps in its `backward`\n            loss = loss / self.args.gradient_accumulation_steps\n\n        # ########################### Regularization ##########################\n        # orthogonal_loss = 0.\n        # for name, param in self.model.named_parameters():\n        #     if \"lora_A\" in name:\n        #         for name_, param_ in self.model.named_parameters():\n        #             if \"loranew_A\" in name_ and name.split(\"lora_A\")[0] == name_.split(\"loranew_A\")[0]:\n        #                 orthogonal_loss += torch.abs(torch.mm(param, param_.T)).sum() # [r * dim] * [dim * r]\n        #                 break # target modules have been matched\n\n        # # l2-normalization for loranew_A/B\n        # l2_loss = 0.\n        # for name, param in self.model.named_parameters():\n        #     if \"loranew_\" in name:\n        #         l2_loss += torch.norm(param, p=2)\n\n        # lamda_1 = self.args.lamda_1\n        # lamda_2 = self.args.lamda_2\n        # print(orthogonal_loss)\n        # print(l2_loss)\n        # logger.info(f\"orthogonal_loss: {orthogonal_loss.item()}; l2_loss: {l2_loss.item()}; accuracy_loss: {loss.item()}; λ1: {lamda_1}; λ2: {lamda_2}\")\n        # loss = loss + orthogonal_loss * lamda_1 + l2_loss * lamda_2\n        # ######################################################################\n        \n        if self.do_grad_scaling:\n            self.scaler.scale(loss).backward()\n        elif self.use_apex:\n            with amp.scale_loss(loss, self.optimizer) as scaled_loss:\n                scaled_loss.backward()\n        elif self.deepspeed:\n            # loss gets scaled under gradient_accumulation_steps in deepspeed\n            self.deepspeed.backward(loss)\n        else:\n            loss.requires_grad=True\n            loss.backward()\n\n        return loss.detach()\n\n\n    def evaluation_loop(\n        self,\n        dataloader: DataLoader,\n        description: str,\n        prediction_loss_only: Optional[bool] = None,\n        ignore_keys: Optional[List[str]] = None,\n        metric_key_prefix: str = \"eval\",\n    ) -> EvalLoopOutput:\n        \"\"\"\n        Prediction/evaluation loop, shared by `Trainer.evaluate()` and `Trainer.predict()`.\n\n        Works both with or without labels.\n        \"\"\"\n        args = self.args\n\n        prediction_loss_only = prediction_loss_only if prediction_loss_only is not None else args.prediction_loss_only\n\n        # if eval is called w/o train init deepspeed here\n        if args.deepspeed and not self.deepspeed:\n\n            # XXX: eval doesn't have `resume_from_checkpoint` arg but we should be able to do eval\n            # from the checkpoint eventually\n            deepspeed_engine, _, _ = deepspeed_init(\n                self, num_training_steps=0, resume_from_checkpoint=None, # inference=True\n            )\n            self.model = deepspeed_engine.module\n            self.model_wrapped = deepspeed_engine\n            self.deepspeed = deepspeed_engine\n\n        model = self._wrap_model(self.model, training=False)\n\n        # if full fp16 or bf16 eval is wanted and this ``evaluation`` or ``predict`` isn't called\n        # while ``train`` is running, cast it to the right dtype first and then put on device\n        if not self.is_in_train:\n            if args.fp16_full_eval:\n                model = model.to(dtype=torch.float16, device=args.device)\n            elif args.bf16_full_eval:\n                model = model.to(dtype=torch.bfloat16, device=args.device)\n\n        batch_size = dataloader.batch_size\n\n        logger.info(f\"***** Running {description} *****\")\n        if has_length(dataloader.dataset):\n            logger.info(f\"  Num examples = {self.num_examples(dataloader)}\")\n        else:\n            logger.info(\"  Num examples: Unknown\")\n        logger.info(f\"  Batch size = {batch_size}\")\n\n        model.eval()\n\n        self.callback_handler.eval_dataloader = dataloader\n        # Do this before wrapping.\n        eval_dataset = dataloader.dataset\n\n        if args.past_index >= 0:\n            self._past = None\n\n        # Initialize containers\n        # losses/preds/labels on GPU/TPU (accumulated for eval_accumulation_steps)\n        losses_host = None\n        preds_host = None\n        labels_host = None\n        # losses/preds/labels on CPU (final containers)\n        all_losses = None\n        all_preds = None\n        all_labels = None\n        # Will be useful when we have an iterable dataset so don't know its length.\n\n        observed_num_examples = 0\n        # Main evaluation loop\n        for step, inputs in enumerate(dataloader):\n            # Update the observed num examples\n            observed_batch_size = find_batch_size(inputs)\n            if observed_batch_size is not None:\n                observed_num_examples += observed_batch_size\n                # For batch samplers, batch_size is not known by the dataloader in advance.\n                if batch_size is None:\n                    batch_size = observed_batch_size\n\n            # Prediction step\n            loss, logits, labels = self.prediction_step(model, inputs, prediction_loss_only, ignore_keys=ignore_keys)\n\n            # Update containers on host\n            if loss is not None:\n                losses = self._nested_gather(loss.repeat(batch_size))\n                losses_host = losses if losses_host is None else torch.cat((losses_host, losses), dim=0)\n            if labels is not None:\n                labels = self._pad_across_processes(labels)\n                labels = self._nested_gather(labels)\n                labels_host = labels if labels_host is None else nested_concat(labels_host, labels, padding_index=-100)\n            if logits is not None:\n                logits = self._pad_across_processes(logits)\n                logits = self._nested_gather(logits)\n                if self.preprocess_logits_for_metrics is not None:\n                    logits = self.preprocess_logits_for_metrics(logits, labels)\n                preds_host = logits if preds_host is None else nested_concat(preds_host, logits, padding_index=-100)\n            self.control = self.callback_handler.on_prediction_step(args, self.state, self.control)\n\n            # Gather all tensors and put them back on the CPU if we have done enough accumulation steps.\n            if args.eval_accumulation_steps is not None and (step + 1) % args.eval_accumulation_steps == 0:\n                if losses_host is not None:\n                    losses = nested_numpify(losses_host)\n                    all_losses = losses if all_losses is None else np.concatenate((all_losses, losses), axis=0)\n                if preds_host is not None:\n                    logits = nested_numpify(preds_host)\n                    all_preds = logits if all_preds is None else nested_concat(all_preds, logits, padding_index=-100)\n                if labels_host is not None:\n                    labels = nested_numpify(labels_host)\n                    all_labels = (\n                        labels if all_labels is None else nested_concat(all_labels, labels, padding_index=-100)\n                    )\n\n                # Set back to None to begin a new accumulation\n                losses_host, preds_host, labels_host = None, None, None\n\n        if args.past_index and hasattr(self, \"_past\"):\n            # Clean the state at the end of the evaluation loop\n            delattr(self, \"_past\")\n\n        # Gather all remaining tensors and put them back on the CPU\n        if losses_host is not None:\n            losses = nested_numpify(losses_host)\n            all_losses = losses if all_losses is None else np.concatenate((all_losses, losses), axis=0)\n        if preds_host is not None:\n            logits = nested_numpify(preds_host)\n            all_preds = logits if all_preds is None else nested_concat(all_preds, logits, padding_index=-100)\n        if labels_host is not None:\n            labels = nested_numpify(labels_host)\n            all_labels = labels if all_labels is None else nested_concat(all_labels, labels, padding_index=-100)\n\n        # Number of samples\n        if has_length(eval_dataset):\n            num_samples = len(eval_dataset)\n        # The instance check is weird and does not actually check for the type, but whether the dataset has the right\n        # methods. Therefore we need to make sure it also has the attribute.\n        elif isinstance(eval_dataset, IterableDatasetShard) and hasattr(eval_dataset, \"num_examples\"):\n            num_samples = eval_dataset.num_examples\n        else:\n            num_samples = observed_num_examples\n\n        # Number of losses has been rounded to a multiple of batch_size and in a distributed training, the number of\n        # samplers has been rounded to a multiple of batch_size, so we truncate.\n        if all_losses is not None:\n            all_losses = all_losses[:num_samples]\n        if all_preds is not None:\n            all_preds = nested_truncate(all_preds, num_samples)\n        if all_labels is not None:\n            all_labels = nested_truncate(all_labels, num_samples)\n\n        # Metrics!\n        if self.compute_metrics is not None and all_preds is not None and all_labels is not None:\n            metrics = self.compute_metrics(dataset=eval_dataset, preds=all_preds, save_prefix=metric_key_prefix)\n        else:\n            metrics = {}\n\n        metrics[\"global_step\"] = self.state.global_step\n\n        # To be JSON-serializable, we need to remove numpy types or zero-d tensors\n        metrics = denumpify_detensorize(metrics)\n\n        if all_losses is not None:\n            metrics[f\"{metric_key_prefix}_loss\"] = all_losses.mean().item()\n\n        # Prefix all keys with metric_key_prefix + '_'\n        for key in list(metrics.keys()):\n            if not key.startswith(f\"{metric_key_prefix}_\"):\n                metrics[f\"{metric_key_prefix}_{key}\"] = metrics.pop(key)\n\n        return EvalLoopOutput(predictions=all_preds, label_ids=all_labels, metrics=metrics, num_samples=num_samples)\n\n\n    def prediction_step(\n        self,\n        model: nn.Module,\n        inputs: Dict[str, Union[torch.Tensor, Any]],\n        prediction_loss_only: bool,\n        ignore_keys: Optional[List[str]] = None,\n    ) -> Tuple[Optional[float], Optional[torch.Tensor], Optional[torch.Tensor]]:\n        \"\"\"\n        Perform an evaluation step on `model` using `inputs`.\n\n        Subclass and override to inject custom behavior.\n\n        Args:\n            model (`nn.Module`):\n                The model to evaluate.\n            inputs (`Dict[str, Union[torch.Tensor, Any]]`):\n                The inputs and targets of the model.\n\n                The dictionary will be unpacked before being fed to the model. Most models expect the targets under the\n                argument `labels`. Check your model's documentation for all accepted arguments.\n            prediction_loss_only (`bool`):\n                Whether or not to return the loss only.\n\n        Return:\n            Tuple[Optional[float], Optional[torch.Tensor], Optional[torch.Tensor]]: A tuple with the loss, logits and\n            labels (each being optional).\n        \"\"\"\n\n        if not self.args.predict_with_generate or prediction_loss_only:\n            return super().prediction_step(\n                model, inputs, prediction_loss_only=prediction_loss_only, ignore_keys=ignore_keys\n            )\n\n        has_labels = \"labels\" in inputs\n        inputs = self._prepare_inputs(inputs)\n\n        # XXX: adapt synced_gpus for fairscale as well\n        gen_kwargs = self._gen_kwargs\n        gen_kwargs[\"synced_gpus\"] = False\n\n        if \"attention_mask\" in inputs:\n            gen_kwargs[\"attention_mask\"] = inputs.get(\"attention_mask\", None)\n\n        generation_config = GenerationConfig(**gen_kwargs)\n\n        # prepare generation inputs\n        # some encoder-decoder models can have varying encder's and thus\n        # varying model input names\n        if hasattr(self.model, \"encoder\") and self.model.encoder.main_input_name != self.model.main_input_name:\n            generation_inputs = inputs[self.model.encoder.main_input_name]\n        else:\n            generation_inputs = inputs[self.model.main_input_name]\n\n        generated_tokens = self.model.generate(\n            input_ids=generation_inputs, \n            generation_config=generation_config\n        )\n\n        bs, source_len = inputs['input_ids'].shape\n        # in case the batch is shorter than max length, the output should be padded\n        if check_model(self.model.config._name_or_path, SUPPORTED_DECODER_MODELS):\n            max_length = source_len + gen_kwargs[\"max_new_tokens\"]\n        else:\n            max_length = gen_kwargs[\"max_new_tokens\"]\n\n        if generated_tokens.shape[-1] < max_length:\n            generated_tokens = self._pad_tensors_to_max_len(generated_tokens, max_length)\n\n        with torch.no_grad():\n            if has_labels:\n                with self.autocast_smart_context_manager():\n                    outputs = model(**inputs)\n                if self.label_smoother is not None:\n                    loss = self.label_smoother(outputs, inputs[\"labels\"]).mean().detach()\n                else:\n                    loss = (outputs[\"loss\"] if isinstance(outputs, dict) else outputs[0]).mean().detach()\n            else:\n                loss = None\n\n        if self.args.prediction_loss_only:\n            return (loss, None, None)\n\n        if has_labels:\n            labels = inputs[\"labels\"]\n            if labels.shape[-1] < gen_kwargs[\"max_new_tokens\"]:\n                labels = self._pad_tensors_to_max_len(labels, gen_kwargs[\"max_new_tokens\"])\n        else:\n            labels = None\n\n        return (loss, generated_tokens, labels)\n\n    \n    def save_model(self, output_dir: Optional[str] = None, _internal_call: bool = False):\n        # super().save_model(output_dir, _internal_call)\n        while os.path.exists(f'{self.args.output_dir}/ckpt_{self.save_index}'):\n            self.save_index+=1\n        self.model.save_pretrained(f'{self.args.output_dir}/ckpt_{self.save_index}')  \n\n"}
{"type": "source_file", "path": "src/assets.py", "content": "from torch import nn\nimport torch\nfrom typing import Dict\n\ntask_config = {\n    \"task1590_diplomacy_text_generation\": \"configs/SuperNI/task1590_diplomacy_text_generation\",\n    \"task181_outcome_extraction\": \"configs/SuperNI/task181_outcome_extraction\",\n    \"task591_sciq_answer_generation\": \"configs/SuperNI/task591_sciq_answer_generation\",\n    \"task1729_personachat_generate_next\": \"configs/SuperNI/task1729_personachat_generate_next\",\n    \"task1572_samsum_summary\": \"configs/SuperNI/task1572_samsum_summary\",\n    \"task1510_evalution_relation_extraction\": \"configs/SuperNI/task1510_evalution_relation_extraction\",\n    \"task748_glucose_reverse_cause_event_detection\": \"configs/SuperNI/task748_glucose_reverse_cause_event_detection\",\n    \"task002_quoref_answer_generation\": \"configs/SuperNI/task002_quoref_answer_generation\",\n    \"task1687_sentiment140_classification\": \"configs/SuperNI/task1687_sentiment140_classification\",\n    \"task511_reddit_tifu_long_text_summarization\": \"configs/SuperNI/task511_reddit_tifu_long_text_summarization\",\n    \"task875_emotion_classification\": \"configs/SuperNI/task511_reddit_tifu_long_text_summarization\",\n    \"task639_multi_woz_user_utterance_generation\": \"configs/SuperNI/task639_multi_woz_user_utterance_generation\",\n    \"task1290_xsum_summarization\": \"configs/SuperNI/task1290_xsum_summarization\",\n    \"task073_commonsenseqa_answer_generation\": \"configs/SuperNI/task073_commonsenseqa_answer_generation\",\n    \"task363_sst2_polarity_classification\": \"configs/SuperNI/task363_sst2_polarity_classification\",\n    \"dbpedia\": \"configs/Long_Sequence/dbpedia\",\n    \"amazon\": \"configs/Long_Sequence/amazon\",\n    \"agnews\": \"configs/Long_Sequence/agnews\",\n    \"yahoo\": \"configs/Long_Sequence/yahoo\",\n    \"yelp\": \"configs/Long_Sequence/yelp\",\n    \"copa\": \"configs/Long_Sequence/copa\",\n    \"mnli\": \"configs/Long_Sequence/mnli\",\n    \"cb\": \"configs/Long_Sequence/cb\",\n    \"imdb\": \"configs/Long_Sequence/imdb\",\n    \"multirc\": \"configs/Long_Sequence/multirc\",\n    \"sst2\": \"configs/Long_Sequence/sst2\",\n    \"boolq\": \"configs/Long_Sequence/boolq\",\n    \"rte\": \"configs/Long_Sequence/rte\",\n    \"wic\": \"configs/Long_Sequence/wic\",\n    \"qqp\": \"configs/Long_Sequence/qqp\",\n}\n\ndef lora_state_dict_A(model: nn.Module, bias: str = 'none', task_name=None) -> Dict[str, torch.Tensor]:\n    my_state_dict = model.state_dict()\n    if bias == 'none':\n        return {k: my_state_dict[k] for k in my_state_dict if 'lora_A' in k}\n    elif bias == 'all':\n        return {k: my_state_dict[k] for k in my_state_dict if 'lora_A' in k or 'bias' in k}\n    elif bias == 'lora_only':\n        to_return = {}\n        for k in my_state_dict:\n            if 'lora_' in k:\n                to_return[k] = my_state_dict[k]\n                bias_name = k.split('lora_A')[0]+'bias'\n                if bias_name in my_state_dict:\n                    to_return[bias_name] = my_state_dict[bias_name]\n        return to_return\n    else:\n        raise NotImplementedError\n\ndef lora_state_dict_B(model: nn.Module, bias: str = 'none', task_name=None) -> Dict[str, torch.Tensor]:\n    my_state_dict = model.state_dict()\n    if bias == 'none':\n        return {k: my_state_dict[k] for k in my_state_dict if 'lora_B' in k}\n    elif bias == 'all':\n        return {k: my_state_dict[k] for k in my_state_dict if 'lora_B' in k or 'bias' in k}\n    elif bias == 'lora_only':\n        to_return = {}\n        for k in my_state_dict:\n            if 'lora_' in k:\n                to_return[k] = my_state_dict[k]\n                bias_name = k.split('lora_B')[0]+'bias'\n                if bias_name in my_state_dict:\n                    to_return[bias_name] = my_state_dict[bias_name]\n        return to_return\n    else:\n        raise NotImplementedError"}
{"type": "source_file", "path": "score.py", "content": "import json\nimport os\nimport sys\n\ndef load_json(path):\n    with open(path, 'r', encoding='utf-8') as f:\n        data = json.load(f)\n    return data\n\ndef write_json(path, data):\n    with open(path, 'w', encoding='utf-8') as f:\n        json.dump(data, f, ensure_ascii=False,indent=2)\n\ndef cal_continue_learning_metrics(scores_array, individual_scores):\n    task_num=len(scores_array)\n\n    Cl=sum(scores_array[-1])/task_num\n\n    fgt_list=[]\n    for t_idx in range(task_num-1):\n        history=[line[t_idx] for line in scores_array[:-1]]\n        history_best=max(history)\n        fgt_list.append(history_best-scores_array[-1][t_idx])\n    Fgt=sum(fgt_list)/len(fgt_list)\n\n    Fwt=sum([scores_array[i][i] for i in range(task_num)])/task_num - 50.94\n\n    Bwt=sum([scores_array[-1][i] - scores_array[i][i] for i in range(task_num)])/task_num\n    \n    return {\n        'Cl':Cl,\n        'Fgt':Fgt,\n        'Fwt':Fwt,\n        'Bwt':Bwt,\n    }\n\nrun_name = sys.argv[1]\n\nsingle_path = sys.argv[2]\n\nwith open(f\"logs_and_outputs/{run_name}/outputs/task_order.txt\", 'r') as f:\n    data_list = f.readlines()\ndata_list = data_list[0].split(',')\n\ntask_num=len(data_list)\n\nresult_root_path=f'logs_and_outputs/{run_name}/outputs'\nsingle_root_path=f'logs_and_outputs/{single_path}/outputs'\n\nscores=[]\nfor i in range(len(data_list)):\n    score_line=[]\n    print(data_list[i])\n    inference_result=load_json(f'{result_root_path}/{i+1}-{data_list[i]}/all_results.json')\n    for j in range(i+1):\n        score=inference_result[f'predict_eval_rougeL_for_{data_list[j]}'] #  \"predict_exact_match_for_\" for Long Sequence\n        score_line.append(score)\n    score_line.extend([0]*(task_num-i-1))\n    scores.append(score_line)\n\nwith open(os.path.join(single_root_path, \"task_order.txt\"), 'r') as f:\n    single_task_order = f.readlines()\n    single_task_list = single_task_order[0].split(',')\nindividual_scores=[]\n\nfor i in range(task_num):\n    inference_result=load_json(f'{single_root_path}/{i+1}-{single_task_list[i]}/all_results.json')\n    score=inference_result[f'predict_eval_rougeL_for_{single_task_list[i]}']\n    individual_scores.append(score)\n\ncl_scores=cal_continue_learning_metrics(scores, individual_scores)\nprint(json.dumps(cl_scores,indent=2))\n\nfrom tabulate import tabulate\ntitle=list(range(task_num))\nprint(tabulate([individual_scores], headers=title, tablefmt='fancy_grid'))\nwith open(os.path.join(\"results\", run_name + '.txt'), 'w') as f:\n    f.write(str(cl_scores))\n    f.write('\\n')\n\n    f.write(tabulate([individual_scores], headers=title, tablefmt='fancy_grid'))\n    f.write('\\n')\n\n\n    title=['']+title\n    scores_line=[[i]+line for i,line in enumerate(scores)]\n    print(tabulate(scores_line, headers=title, tablefmt='fancy_grid'))\n\n    f.write(tabulate(scores_line, headers=title, tablefmt='fancy_grid'))\n\n"}
{"type": "source_file", "path": "pseudo_data/src/run_uie_lora.py", "content": "#!/usr/bin/env python\n# coding=utf-8\n# Copyright 2021 The HuggingFace Team. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nFine-tuning the library models for sequence to sequence.\n\"\"\"\n# You can also adapt this script on your own sequence to sequence task. Pointers for this are left as comments.\n\nimport logging\nimport os\nimport sys\nimport json\nimport time\nimport torch\nfrom dataclasses import dataclass, field\nfrom typing import Optional\n\nimport datasets\nimport nltk  # Here to have a nice missing dependency error message early on\nimport numpy as np\nfrom datasets import load_dataset\n\nimport transformers\nfrom filelock import FileLock\nfrom transformers import (\n    AutoConfig,\n    AutoModel,\n    AutoModelForSeq2SeqLM,\n    AutoModelForCausalLM,  # add\n    AutoTokenizer,\n    HfArgumentParser,\n    Seq2SeqTrainingArguments,\n    set_seed, )\nfrom transformers.file_utils import is_offline_mode\nfrom transformers.trainer_utils import get_last_checkpoint\nfrom peft import get_peft_config, get_peft_model, LoraConfig, PromptTuningInit, TaskType, PeftModel, PeftConfig # add\n\nfrom uie_collator import DataCollatorForUIE\nfrom uie_dataset_lora import gen_cache_path\n\nfrom uie_trainer_lora import UIETrainer, DenserEvalCallback, skip_instructions\nfrom compute_metrics import compute_metrics, compute_grouped_metrics\nfrom model.llama import LlamaForCausalLM_with_lossmask\n\n# off wandb\nos.environ['WANDB_DISABLED'] = \"True\"\n# os.environ['CUDA_VISIBLE_DEVICES'] = '0'\nlogger = logging.getLogger(__name__)\nCURRENT_DIR = os.path.dirname(__file__)\n\ntry:\n    nltk.data.find(\"tokenizers/punkt\")\nexcept (LookupError, OSError):\n    if is_offline_mode():\n        raise LookupError(\n            \"Offline mode: run this script without TRANSFORMERS_OFFLINE first to download nltk data files\"\n        )\n    with FileLock(\".lock\") as lock:\n        nltk.download(\"punkt\", quiet=True)\n\n\n@dataclass\nclass ModelArguments:\n    \"\"\"\n    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.\n    \"\"\"\n\n    model_name_or_path: str = field(\n        metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n    )\n    config_name: Optional[str] = field(\n        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n    )\n    tokenizer_name: Optional[str] = field(\n        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n    )\n    cache_dir: Optional[str] = field(\n        default=None,\n        metadata={\"help\": \"Where to store the pretrained models downloaded from huggingface.co\"},\n    )\n    use_fast_tokenizer: bool = field(\n        default=True,\n        metadata={\"help\": \"Whether to use one of the fast tokenizer (backed by the tokenizers library) or not.\"},\n    )\n    model_revision: str = field(\n        default=\"main\",\n        metadata={\"help\": \"The specific model version to use (can be a branch name, tag name or commit id).\"},\n    )\n    use_auth_token: bool = field(\n        default=False,\n        metadata={\n            \"help\": \"Will use the token generated when running `transformers-cli login` (necessary to use this script \"\n                    \"with private models).\"\n        },\n    )\n    resize_position_embeddings: Optional[bool] = field(\n        default=None,\n        metadata={\n            \"help\": \"Whether to automatically resize the position embeddings if `max_source_length` exceeds \"\n                    \"the model's position embeddings.\"\n        },\n    )\n    # added for AutoCL\n    lora_dim: Optional[int] = field(\n        default=8,\n        metadata={\n            \"help\": \"Intrinsic dimension of the latent space.\"\n        },\n    )\n    num_virtual_tokens: Optional[int] = field(\n        default=300,\n        metadata={\n            \"help\": \"Intrinsic dimension of the latent space.\"\n        },\n    )\n\n\n@dataclass\nclass DataTrainingArguments:\n    \"\"\"\n    Arguments pertaining to what data we are going to input our model for training and eval.\n    \"\"\"\n    lang: str = field(default=None, metadata={\"help\": \"Language id for multilingual model.\"})\n    data_dir: str = field(\n        default=None, metadata={\"help\": \"The directory for saving the UIE train/dev/test splits.\"}\n    )\n    task_config_dir: str = field(\n        default=None, metadata={\"help\": \"The json file for config training and testing tasks\"}\n    )\n    instruction_file: str = field(\n        default=None, metadata={\"help\": \"The instruction file for different tasks.\"}\n    )\n    instruction_strategy: Optional[str] = field(\n        default='single', metadata={\n            \"help\": \"How many different instructions to use? Support 'single' and 'multiple' mode.\"\n        }\n    )\n    overwrite_cache: bool = field(\n        default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n    )\n    input_record_file: str = field(\n        default=None, metadata={\"help\": \"file to record model input\"}\n    )\n    preprocessing_num_workers: Optional[int] = field(\n        default=None,\n        metadata={\"help\": \"The number of processes to use for the preprocessing.\"},\n    )\n    max_source_length: Optional[int] = field(\n        default=512,\n        metadata={\n            \"help\": \"The maximum total input sequence length after tokenization. Sequences longer \"\n                    \"than this will be truncated, sequences shorter will be padded.\"\n        },\n    )\n    # for decoder model, it means max_new_tokens\n    max_target_length: Optional[int] = field(\n        default=50,\n        metadata={\n            \"help\": \"The maximum total sequence length for target text after tokenization. Sequences longer \"\n                    \"than this will be truncated, sequences shorter will be padded.\"\n        },\n    )\n    repetition_penalty: Optional[float] = field(\n        default=1.0,\n        metadata={\n            \"help\": \"Penalty for repeat tokens in decode stage.\"\n        },\n    )\n    num_beams: Optional[int] = field(\n        default=1,\n        metadata={\n            \"help\": \"Number of beams to use for evaluation. This argument will be passed to ``model.generate``, \"\n                    \"which is used during ``evaluate`` and ``predict``.\"\n        },\n    )\n    max_num_instances_per_task: int = field(\n        default=10000, metadata={\"help\": \"The maximum number of instances we will consider for each training task.\"}\n    )\n    max_num_instances_per_eval_task: int = field(\n        default=200,\n        metadata={\"help\": \"The maximum number of instances we will consider for each validation/test task.\"}\n    )\n    max_train_samples: Optional[int] = field(\n        default=None,\n        metadata={\n            \"help\": \"For debugging purposes or quicker training, truncate the number of training examples to this \"\n                    \"value if set.\"\n        },\n    )\n    max_eval_samples: Optional[int] = field(\n        default=None,\n        metadata={\n            \"help\": \"For debugging purposes or quicker training, truncate the number of evaluation examples to this \"\n                    \"value if set.\"\n        },\n    )\n    max_predict_samples: Optional[int] = field(\n        default=None,\n        metadata={\n            \"help\": \"For debugging purposes or quicker training, truncate the number of prediction examples to this \"\n                    \"value if set.\"\n        },\n    )\n    num_examples: Optional[int] = field(\n        default=0,\n        metadata={\"help\": \"number of in-context positive examples.\"}\n    )\n    ignore_pad_token_for_loss: bool = field(\n        default=True,\n        metadata={\n            \"help\": \"Whether to ignore the tokens corresponding to padded labels in the loss computation or not.\"\n        },\n    )\n    add_task_name: Optional[bool] = field(\n        default=False,\n        metadata={\"help\": \"whether to preappend task name before the task input.\"}\n    )\n    add_dataset_name: Optional[bool] = field(\n        default=False,\n        metadata={\"help\": \"whether to preappend dataset name before the task input.\"}\n    )\n\n\n@dataclass\nclass UIETrainingArguments(Seq2SeqTrainingArguments):\n    gradient_checkpointing: Optional[bool] = field(\n        default=False,\n        metadata={\"help\": \"Whether to use computing time to gain more memory\"}\n    )\n    denser_evaluation: Optional[bool] = field(\n        default=False,\n        metadata={\"help\": \"If specifid, the model will do more evaluation at the beginning of training.\"}\n    )\n    remove_unused_columns: Optional[bool] = field(\n        default=False,\n        metadata={\"help\": \"If specifid, the model will do more evaluation at the beginning of training.\"}\n    )\n    top_k: Optional[int] = field(\n        default=10,\n        metadata={\"help\": \"topk\"}\n    )\n    do_demo: bool = field(default=False, metadata={\"help\": \"Whether to run the model as a demo in the terminal.\"})\n    lamda_1: float = field(default = 0.5)\n    lamda_2: float = field(default = 0)\n\n\ndef main():\n    # See all possible arguments in src/transformers/training_args.py\n    # or by passing the --help flag to this script.\n    # We now keep distinct sets of args, for a cleaner separation of concerns.\n\n    parser = HfArgumentParser((ModelArguments, DataTrainingArguments, UIETrainingArguments))\n    if len(sys.argv) == 2 and sys.argv[1].endswith(\".json\"):\n        # If we pass only one argument to the script and it's the path to a json file,\n        # let's parse it to get our arguments.\n        model_args, data_args, training_args = parser.parse_json_file(json_file=os.path.abspath(sys.argv[1]))\n    else:\n        model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n\n\n    # Setup logging\n    logging.basicConfig(\n        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n        datefmt=\"%m/%d/%Y %H:%M:%S\",\n        handlers=[logging.StreamHandler(sys.stdout)],\n    )\n    log_level = training_args.get_process_log_level()\n    logger.setLevel(log_level)\n    datasets.utils.logging.set_verbosity(log_level)\n    transformers.utils.logging.set_verbosity(log_level)\n    transformers.utils.logging.enable_default_handler()\n    transformers.utils.logging.enable_explicit_format()\n\n    # Log on each process the small summary:\n    logger.warning(\n        f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}\"\n        + f\"distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}\"\n    )\n    logger.info(f\"Training/evaluation parameters {training_args}\")\n\n    # Detecting last checkpoint.\n    last_checkpoint = None\n    if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:\n        last_checkpoint = get_last_checkpoint(training_args.output_dir)\n        if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:\n            raise ValueError(\n                f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"\n                \"Use --overwrite_output_dir to overcome.\"\n            )\n        elif last_checkpoint is not None and training_args.resume_from_checkpoint is None:\n            logger.info(\n                f\"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change \"\n                \"the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\"\n            )\n\n    # Set seed before initializing model.\n    set_seed(training_args.seed)\n    data_cache_dir = gen_cache_path(training_args.output_dir, data_args)\n\n    # Get the UIE dataset\n    raw_datasets = load_dataset(\n        os.path.join(CURRENT_DIR, \"uie_dataset_lora_ours.py\"),\n        data_dir=data_args.data_dir,\n        task_config_dir=data_args.task_config_dir,\n        instruction_file=data_args.instruction_file,\n        instruction_strategy=data_args.instruction_strategy,\n        cache_dir=data_cache_dir,  # for debug, change dataset size, otherwise open it\n        max_num_instances_per_task=data_args.max_num_instances_per_task,\n        max_num_instances_per_eval_task=data_args.max_num_instances_per_eval_task,\n        num_examples=data_args.num_examples\n    )\n    raw_datasets.cleanup_cache_files()\n\n    # Load pretrained model and tokenizer\n    #\n    # Distributed training:\n    # The .from_pretrained methods guarantee that only one local process can concurrently\n    # download model & vocab.\n    if 'adapter' in model_args.model_name_or_path: # load lora-config\n        config = PeftConfig.from_pretrained(model_args.model_name_or_path)\n        if 'llama' in model_args.model_name_or_path.lower():\n            tokenizer = transformers.LlamaTokenizer.from_pretrained(config.base_model_name_or_path)\n            config.bos_token_id = 1\n            config.eos_token_id = 2\n            config.pad_token_id = 1\n            tokenizer.bos_token_id = 1\n            tokenizer.eos_token_id = 2\n            tokenizer.pad_token_id = 1\n        else:\n            tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n    elif 'llama' in model_args.model_name_or_path.lower():\n        config = AutoConfig.from_pretrained(\n            model_args.model_name_or_path,\n            cache_dir=model_args.cache_dir,\n            revision=model_args.model_revision,\n            use_auth_token=True if model_args.use_auth_token else None,\n        )\n        config.bos_token_id = 1\n        config.eos_token_id = 2\n        config.pad_token_id = 1\n        tokenizer = transformers.LlamaTokenizer.from_pretrained(\n            model_args.model_name_or_path,\n            cache_dir = model_args.cache_dir,\n            use_fast = model_args.use_fast_tokenizer,\n            revision = model_args.model_revision,\n            use_auth_token = True if model_args.use_auth_token else None,\n        )\n        tokenizer.bos_token_id = 1\n        tokenizer.eos_token_id = 2\n        tokenizer.pad_token_id = 1\n    else: # load original config\n        config = AutoConfig.from_pretrained(\n            model_args.config_name if model_args.config_name else model_args.model_name_or_path,\n            cache_dir=model_args.cache_dir,\n            revision=model_args.model_revision,\n            use_auth_token=True if model_args.use_auth_token else None,\n        )\n        tokenizer = AutoTokenizer.from_pretrained(\n            model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,\n            cache_dir=model_args.cache_dir,\n            use_fast=model_args.use_fast_tokenizer,\n            revision=model_args.model_revision,\n            use_auth_token=True if model_args.use_auth_token else None,\n        )\n\n    if 'llama' in model_args.model_name_or_path.lower():  # add llama\n        model_class = LlamaForCausalLM_with_lossmask\n        tokenizer.padding_side = 'left'\n    else: \n        model_class = AutoModelForSeq2SeqLM\n\n    if 'adapter' in model_args.model_name_or_path: # add lora-adapter to the original model\n        model = model_class.from_pretrained(config.base_model_name_or_path)\n        model = PeftModel.from_pretrained(model, model_args.model_name_or_path)\n    elif 'llama' in model_args.model_name_or_path.lower():\n        model = model_class.from_pretrained(\n            model_args.model_name_or_path,\n            from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n            config=config,\n            cache_dir=model_args.cache_dir,\n            revision=model_args.model_revision,\n            use_auth_token=True if model_args.use_auth_token else None\n        )\n        peft_config = LoraConfig(\n            task_type=TaskType.CAUSAL_LM, inference_mode=False, r=model_args.lora_dim, lora_alpha=32, lora_dropout=0.1\n        )\n        model = get_peft_model(model, peft_config)\n    else:\n        model = model_class.from_pretrained(\n            model_args.model_name_or_path,\n            from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n            config=config,\n            cache_dir=model_args.cache_dir,\n            revision=model_args.model_revision,\n            use_auth_token=True if model_args.use_auth_token else None,\n        )\n        peft_config = LoraConfig(\n            task_type=TaskType.SEQ_2_SEQ_LM, inference_mode=False, r=model_args.lora_dim, lora_alpha=32, lora_dropout=0.1\n        )\n        model = get_peft_model(model, peft_config)\n    for n,p in model.named_parameters():\n        print(n, p.requires_grad)\n    print(model.print_trainable_parameters())\n    gen_token = \"__gen__\"\n    inp_token = '__inp__'\n    ans_token = '__ans__'\n    tokenizer.add_tokens([gen_token, inp_token, ans_token])\n    model.resize_token_embeddings(len(tokenizer))\n\n    if 'llama' in model_args.model_name_or_path.lower():\n        model.generation_config.bos_token_id = 1\n        model.generation_config.eos_token_id = 2\n        model.generation_config.pad_token_id = 1\n        \n    # fix lora_A/B (bases of previous LoRA parameters, loaded in \"load_adapter\"[peft_momdel.py])\n    # fine-tune loranew_A/B (initialized in \"update_layer\"[lora.py])\n    # optional: lora_A/B is trainable but should not move too far from lorapre_A/B\n    # (constrained in \"training_step\"[uie_trainer_lora.py])\n    for name, param in model.named_parameters():\n        if name.find(\"loranew_\") != -1:\n            param.requires_grad = True\n        elif name.find(\"lora_\") != -1:\n            param.requires_grad = False\n        # this module should always be frozen because we change the vocabulary\n        elif name.find(\"shared\") != -1:\n            param.requires_grad = False\n\n    if (\n            hasattr(model.config, \"max_position_embeddings\")\n            and model.config.max_position_embeddings < data_args.max_source_length\n    ):\n        if model_args.resize_position_embeddings is None:\n            logger.warning(\n                f\"Increasing the model's number of position embedding vectors from {model.config.max_position_embeddings} \"\n                f\"to {data_args.max_source_length}.\"\n            )\n            model.resize_position_embeddings(data_args.max_source_length)\n        elif model_args.resize_position_embeddings:\n            model.resize_position_embeddings(data_args.max_source_length)\n        else:\n            raise ValueError(\n                f\"`--max_source_length` is set to {data_args.max_source_length}, but the model only has {model.config.max_position_embeddings}\"\n                f\" position encodings. Consider either reducing `--max_source_length` to {model.config.max_position_embeddings} or to automatically \"\n                \"resize the model's position encodings by passing `--resize_position_embeddings`.\"\n            )\n\n    if training_args.do_train:\n        if \"train\" not in raw_datasets:\n            raise ValueError(\"--do_train requires a train dataset\")\n        train_dataset = raw_datasets[\"train\"]\n        if data_args.max_train_samples is not None:\n            train_dataset = train_dataset.select(range(data_args.max_train_samples))\n\n    if training_args.do_eval:\n        if \"validation\" not in raw_datasets:\n            raise ValueError(\"--do_eval requires a validation dataset\")\n        eval_dataset = raw_datasets[\"validation\"]\n        if data_args.max_eval_samples is not None:\n            eval_dataset = eval_dataset.select(range(data_args.max_eval_samples))\n\n    if training_args.do_predict:\n        if \"test\" not in raw_datasets:\n            raise ValueError(\"--do_predict requires a test dataset\")\n        predict_dataset = raw_datasets[\"test\"]\n        if data_args.max_predict_samples is not None:\n            predict_dataset = predict_dataset.select(range(data_args.max_predict_samples))\n\n    # Data collator\n    label_pad_token_id = -100 if data_args.ignore_pad_token_for_loss else tokenizer.pad_token_id\n    data_collator = DataCollatorForUIE(\n        tokenizer,\n        model=model,\n        padding=\"longest\",\n        max_source_length=data_args.max_source_length,\n        max_target_length=data_args.max_target_length,\n        label_pad_token_id=label_pad_token_id,\n        pad_to_multiple_of=8 if training_args.fp16 else None,\n        add_task_name=data_args.add_task_name,\n        add_dataset_name=data_args.add_dataset_name,\n        num_examples=data_args.num_examples,\n        input_record_file=data_args.input_record_file\n    )\n    # we don't want to remove unused columns because we will prepare each batch during training,\n    # and some of the information will also be used in evaluation.\n    # training_args.remove_unused_columns = False\n\n    # Metric\n    def compute_rouge_metrics(dataset, preds, save_prefix=None):\n        decoded_preds = skip_instructions(model, preds, tokenizer)\n        references = [e[\"Instance\"][\"label\"] for e in dataset]\n        result = compute_metrics(predictions=decoded_preds, references=references)\n        result_per_task = compute_grouped_metrics(predictions=decoded_preds, references=references,\n                                                  groups=dataset[\"Task\"])\n        result.update(result_per_task)\n        categories = dataset[\"Dataset\"]\n        result_per_category = compute_grouped_metrics(predictions=decoded_preds, references=references,\n                                                      groups=categories)\n        result.update(result_per_category)\n        prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n        result[\"gen_len\"] = np.mean(prediction_lens)\n        result = {k: round(v, 4) for k, v in result.items()}\n        if save_prefix is not None:\n            with open(os.path.join(training_args.output_dir, f\"{save_prefix}_eval_predictions.jsonl\"), \"w\") as fout:\n                for example, pred in zip(dataset, decoded_preds):\n                    fout.write(json.dumps({\n                        \"Task\": example[\"Task\"],\n                        \"Dataset\": example[\"Dataset\"],\n                        \"Instance\": example[\"Instance\"],\n                        \"Prediction\": pred\n                    }) + \"\\n\")\n        return result\n\n    print(f\"-----Gradient checkpointing: {training_args.gradient_checkpointing} -----\")\n    if training_args.gradient_checkpointing:\n        model.gradient_checkpointing_enable()\n\n    trainer = UIETrainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset if training_args.do_train else None,\n        eval_dataset=eval_dataset if training_args.do_eval else None,\n        tokenizer=tokenizer,\n        data_collator=data_collator,\n        compute_metrics=compute_rouge_metrics,\n        callbacks=[DenserEvalCallback] if training_args.denser_evaluation else None\n    )\n\n    all_metrics = {\"run_name\": training_args.run_name}\n\n    # Training\n    if training_args.do_train:\n        checkpoint = None\n        if training_args.resume_from_checkpoint is not None:\n            checkpoint = training_args.resume_from_checkpoint\n        elif last_checkpoint is not None:\n            checkpoint = last_checkpoint\n        train_result = trainer.train(resume_from_checkpoint=checkpoint)\n\n        peft_model_id = training_args.output_dir + \"/adapter\"\n        trainer.model.save_pretrained(peft_model_id)  \n        tokenizer.save_pretrained(peft_model_id)\n\n        metrics = train_result.metrics\n        max_train_samples = (\n            data_args.max_train_samples if data_args.max_train_samples is not None else len(train_dataset)\n        )\n        metrics[\"train_samples\"] = min(max_train_samples, len(train_dataset))\n\n        trainer.log_metrics(\"train\", metrics)\n        trainer.save_metrics(\"train\", metrics)\n        trainer.save_state()\n        logger.info(f\"Metrics {metrics}\")\n        all_metrics.update(metrics)\n    # print('==================')\n    # print(trainer.state.log_history)\n    # trainer_loss=[one['loss'] for one in trainer.state.log_history[:int(training_args.num_train_epochs)]]\n    # import numpy as np\n    # best_epoch_idx=np.argmin(trainer_loss)\n    # best_ckpt=f'{training_args.output_dir}/ckpt_{best_epoch_idx}'\n    # print(best_ckpt)\n\n    # model.base_model.cpu()\n    # model=PeftModel.from_pretrained(model.base_model, best_ckpt).cuda()\n    \n    # Evaluation\n    results = {}\n    # in case the batch is shorter than max length, the output should be padded\n    max_new_tokens = (\n        training_args.generation_max_length\n        if training_args.generation_max_length is not None\n        else data_args.max_target_length\n    )\n\n    num_beams = data_args.num_beams if data_args.num_beams is not None else training_args.generation_num_beams\n    repetition_penalty = data_args.repetition_penalty\n\n    if training_args.do_predict:\n        logger.info(\"*** Prediction ***\")\n        logger.info(\"*** Loading CheckPoint ***\")\n\n        if data_args.max_predict_samples is not None:\n            predict_dataset = predict_dataset.select(range(data_args.max_predict_samples))\n\n        predict_results = trainer.predict(\n            predict_dataset,\n            metric_key_prefix=\"predict\",\n            pad_token_id=tokenizer.pad_token_id,\n            num_beams=1,\n            max_new_tokens=max_new_tokens,\n            use_cache=True,\n            repetition_penalty=2.5,\n            length_penalty=1.0,\n            early_stopping=True,\n            do_sample=True,\n            top_k=training_args.top_k\n        )\n        metrics = predict_results.metrics\n        max_predict_samples = (\n            data_args.max_predict_samples if data_args.max_predict_samples is not None else len(predict_dataset)\n        )\n        metrics[\"predict_samples\"] = min(max_predict_samples, len(predict_dataset))\n\n        trainer.log(metrics)\n        trainer.log_metrics(\"predict\", metrics)\n        trainer.save_metrics(\"predict\", metrics)\n        all_metrics.update(metrics)\n\n    from pathlib import PurePath\n    with open(f'{training_args.output_dir}/predict_eval_predictions.jsonl' ,'r') as f:\n        data=f.readlines()\n        instances=[{\n            'input': json.loads(one)['Prediction'],\n            'output': ''\n        } for one in data]\n\n    original_data={\n        \"Definition\":[],\n        \"Positive Examples\": [],\n        \"Negative Examples\": [],\n        \"Instances\": instances,\n    }\n    with open(f'{training_args.output_dir}/lookback_inference_result.json' ,'w') as f:\n        json.dump(original_data, f, indent=2)\n\n    from glob import glob\n    for one_path in glob(f'{training_args.output_dir}/checkpoint-*'):\n        os.system(f'rm -r {one_path}')\n\n    return results\n\n\nif __name__ == \"__main__\":\n    main()\n\n"}
{"type": "source_file", "path": "pseudo_data/src/run_dataset.py", "content": "#!/usr/bin/env python\n# coding=utf-8\n# Copyright 2021 The HuggingFace Team. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nFine-tuning the library models for sequence to sequence.\n\"\"\"\n# You can also adapt this script on your own sequence to sequence task. Pointers for this are left as comments.\n\nimport logging\nimport os\nimport sys\nimport json\nimport time\nfrom dataclasses import dataclass, field\nfrom typing import Optional\n\nimport datasets\nimport nltk  # Here to have a nice missing dependency error message early on\nimport numpy as np\nfrom datasets import load_dataset\n\nimport transformers\nfrom filelock import FileLock\nfrom transformers import (\n    AutoConfig,\n    AutoModel,\n    AutoModelForSeq2SeqLM,\n    AutoModelForCausalLM,  # add\n    AutoTokenizer,\n    HfArgumentParser,\n    Seq2SeqTrainingArguments,\n    set_seed, )\nfrom transformers.file_utils import is_offline_mode\nfrom transformers.trainer_utils import get_last_checkpoint\nfrom peft import get_peft_config, get_peft_model, LoraConfig, TaskType, PeftModel, PeftConfig # add\n\nfrom uie_collator import DataCollatorForUIE\nfrom uie_dataset_lora import gen_cache_path\n\nfrom uie_trainer_lora import UIETrainer, DenserEvalCallback, skip_instructions\nfrom compute_metrics import compute_metrics, compute_grouped_metrics\nfrom model.llama import LlamaForCausalLM_with_lossmask\n\n# off wandb\nos.environ['WANDB_DISABLED'] = \"True\"\n# os.environ['CUDA_VISIBLE_DEVICES'] = '0'\nlogger = logging.getLogger(__name__)\nCURRENT_DIR = os.path.dirname(__file__)\n\ntry:\n    nltk.data.find(\"tokenizers/punkt\")\nexcept (LookupError, OSError):\n    if is_offline_mode():\n        raise LookupError(\n            \"Offline mode: run this script without TRANSFORMERS_OFFLINE first to download nltk data files\"\n        )\n    with FileLock(\".lock\") as lock:\n        nltk.download(\"punkt\", quiet=True)\n\n\n@dataclass\nclass ModelArguments:\n    \"\"\"\n    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.\n    \"\"\"\n\n    model_name_or_path: str = field(\n        metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n    )\n    config_name: Optional[str] = field(\n        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n    )\n    tokenizer_name: Optional[str] = field(\n        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n    )\n    cache_dir: Optional[str] = field(\n        default=None,\n        metadata={\"help\": \"Where to store the pretrained models downloaded from huggingface.co\"},\n    )\n    use_fast_tokenizer: bool = field(\n        default=True,\n        metadata={\"help\": \"Whether to use one of the fast tokenizer (backed by the tokenizers library) or not.\"},\n    )\n    model_revision: str = field(\n        default=\"main\",\n        metadata={\"help\": \"The specific model version to use (can be a branch name, tag name or commit id).\"},\n    )\n    use_auth_token: bool = field(\n        default=False,\n        metadata={\n            \"help\": \"Will use the token generated when running `transformers-cli login` (necessary to use this script \"\n                    \"with private models).\"\n        },\n    )\n    resize_position_embeddings: Optional[bool] = field(\n        default=None,\n        metadata={\n            \"help\": \"Whether to automatically resize the position embeddings if `max_source_length` exceeds \"\n                    \"the model's position embeddings.\"\n        },\n    )\n    # added for AutoCL\n    lora_dim: Optional[int] = field(\n        default=8,\n        metadata={\n            \"help\": \"Intrinsic dimension of the latent space.\"\n        },\n    )\n\n\n@dataclass\nclass DataTrainingArguments:\n    \"\"\"\n    Arguments pertaining to what data we are going to input our model for training and eval.\n    \"\"\"\n    lang: str = field(default=None, metadata={\"help\": \"Language id for multilingual model.\"})\n    data_dir: str = field(\n        default=None, metadata={\"help\": \"The directory for saving the UIE train/dev/test splits.\"}\n    )\n    task_config_dir: str = field(\n        default=None, metadata={\"help\": \"The json file for config training and testing tasks\"}\n    )\n    instruction_file: str = field(\n        default=None, metadata={\"help\": \"The instruction file for different tasks.\"}\n    )\n    instruction_strategy: Optional[str] = field(\n        default='single', metadata={\n            \"help\": \"How many different instructions to use? Support 'single' and 'multiple' mode.\"\n        }\n    )\n    overwrite_cache: bool = field(\n        default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n    )\n    input_record_file: str = field(\n        default=None, metadata={\"help\": \"file to record model input\"}\n    )\n    preprocessing_num_workers: Optional[int] = field(\n        default=None,\n        metadata={\"help\": \"The number of processes to use for the preprocessing.\"},\n    )\n    max_source_length: Optional[int] = field(\n        default=512,\n        metadata={\n            \"help\": \"The maximum total input sequence length after tokenization. Sequences longer \"\n                    \"than this will be truncated, sequences shorter will be padded.\"\n        },\n    )\n    # for decoder model, it means max_new_tokens\n    max_target_length: Optional[int] = field(\n        default=50,\n        metadata={\n            \"help\": \"The maximum total sequence length for target text after tokenization. Sequences longer \"\n                    \"than this will be truncated, sequences shorter will be padded.\"\n        },\n    )\n    repetition_penalty: Optional[float] = field(\n        default=1.0,\n        metadata={\n            \"help\": \"Penalty for repeat tokens in decode stage.\"\n        },\n    )\n    num_beams: Optional[int] = field(\n        default=1,\n        metadata={\n            \"help\": \"Number of beams to use for evaluation. This argument will be passed to ``model.generate``, \"\n                    \"which is used during ``evaluate`` and ``predict``.\"\n        },\n    )\n    max_num_instances_per_task: int = field(\n        default=10000, metadata={\"help\": \"The maximum number of instances we will consider for each training task.\"}\n    )\n    max_num_instances_per_eval_task: int = field(\n        default=200,\n        metadata={\"help\": \"The maximum number of instances we will consider for each validation/test task.\"}\n    )\n    max_train_samples: Optional[int] = field(\n        default=None,\n        metadata={\n            \"help\": \"For debugging purposes or quicker training, truncate the number of training examples to this \"\n                    \"value if set.\"\n        },\n    )\n    max_eval_samples: Optional[int] = field(\n        default=None,\n        metadata={\n            \"help\": \"For debugging purposes or quicker training, truncate the number of evaluation examples to this \"\n                    \"value if set.\"\n        },\n    )\n    max_predict_samples: Optional[int] = field(\n        default=None,\n        metadata={\n            \"help\": \"For debugging purposes or quicker training, truncate the number of prediction examples to this \"\n                    \"value if set.\"\n        },\n    )\n    num_examples: Optional[int] = field(\n        default=0,\n        metadata={\"help\": \"number of in-context positive examples.\"}\n    )\n    ignore_pad_token_for_loss: bool = field(\n        default=True,\n        metadata={\n            \"help\": \"Whether to ignore the tokens corresponding to padded labels in the loss computation or not.\"\n        },\n    )\n    add_task_name: Optional[bool] = field(\n        default=False,\n        metadata={\"help\": \"whether to preappend task name before the task input.\"}\n    )\n    add_dataset_name: Optional[bool] = field(\n        default=False,\n        metadata={\"help\": \"whether to preappend dataset name before the task input.\"}\n    )\n\n\n@dataclass\nclass UIETrainingArguments(Seq2SeqTrainingArguments):\n    gradient_checkpointing: Optional[bool] = field(\n        default=False,\n        metadata={\"help\": \"Whether to use computing time to gain more memory\"}\n    )\n    denser_evaluation: Optional[bool] = field(\n        default=False,\n        metadata={\"help\": \"If specifid, the model will do more evaluation at the beginning of training.\"}\n    )\n    do_demo: bool = field(default=False, metadata={\"help\": \"Whether to run the model as a demo in the terminal.\"})\n    lamda_1: float = field(default = 0.5)\n    lamda_2: float = field(default = 0)\n\n\ndef main():\n    # See all possible arguments in src/transformers/training_args.py\n    # or by passing the --help flag to this script.\n    # We now keep distinct sets of args, for a cleaner separation of concerns.\n\n    parser = HfArgumentParser((ModelArguments, DataTrainingArguments, UIETrainingArguments))\n    if len(sys.argv) == 2 and sys.argv[1].endswith(\".json\"):\n        # If we pass only one argument to the script and it's the path to a json file,\n        # let's parse it to get our arguments.\n        model_args, data_args, training_args = parser.parse_json_file(json_file=os.path.abspath(sys.argv[1]))\n    else:\n        model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n\n    # Setup logging\n    logging.basicConfig(\n        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n        datefmt=\"%m/%d/%Y %H:%M:%S\",\n        handlers=[logging.StreamHandler(sys.stdout)],\n    )\n    log_level = training_args.get_process_log_level()\n    logger.setLevel(log_level)\n    datasets.utils.logging.set_verbosity(log_level)\n    transformers.utils.logging.set_verbosity(log_level)\n    transformers.utils.logging.enable_default_handler()\n    transformers.utils.logging.enable_explicit_format()\n\n    # Log on each process the small summary:\n    logger.warning(\n        f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}\"\n        + f\"distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}\"\n    )\n    logger.info(f\"Training/evaluation parameters {training_args}\")\n\n    # Detecting last checkpoint.\n    last_checkpoint = None\n    if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:\n        last_checkpoint = get_last_checkpoint(training_args.output_dir)\n        if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:\n            raise ValueError(\n                f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"\n                \"Use --overwrite_output_dir to overcome.\"\n            )\n        elif last_checkpoint is not None and training_args.resume_from_checkpoint is None:\n            logger.info(\n                f\"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change \"\n                \"the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\"\n            )\n\n    # Set seed before initializing model.\n    set_seed(training_args.seed)\n    data_cache_dir = gen_cache_path(training_args.output_dir, data_args)\n\n    # Get the UIE dataset\n    raw_datasets = load_dataset(\n        os.path.join(CURRENT_DIR, \"uie_dataset_lora.py\"),\n        data_dir=data_args.data_dir,\n        task_config_dir=data_args.task_config_dir,\n        instruction_file=data_args.instruction_file,\n        instruction_strategy=data_args.instruction_strategy,\n        # cache_dir=data_cache_dir,  # for debug, change dataset size, otherwise open it\n        max_num_instances_per_task=data_args.max_num_instances_per_task,\n        max_num_instances_per_eval_task=data_args.max_num_instances_per_eval_task,\n        num_examples=data_args.num_examples\n    )\n    raw_datasets.cleanup_cache_files()\n\n    # Load pretrained model and tokenizer\n    #\n    # Distributed training:\n    # The .from_pretrained methods guarantee that only one local process can concurrently\n    # download model & vocab.\n    if 'adapter' in model_args.model_name_or_path: # load lora-config\n        config = PeftConfig.from_pretrained(model_args.model_name_or_path)\n        if 'llama' in model_args.model_name_or_path.lower():\n            tokenizer = transformers.LlamaTokenizer.from_pretrained(config.base_model_name_or_path)\n            config.bos_token_id = 1\n            config.eos_token_id = 2\n            config.pad_token_id = 1\n            tokenizer.bos_token_id = 1\n            tokenizer.eos_token_id = 2\n            tokenizer.pad_token_id = 1\n        else:\n            tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n    elif 'llama' in model_args.model_name_or_path.lower():\n        config = AutoConfig.from_pretrained(\n            model_args.model_name_or_path,\n            cache_dir=model_args.cache_dir,\n            revision=model_args.model_revision,\n            use_auth_token=True if model_args.use_auth_token else None,\n        )\n        config.bos_token_id = 1\n        config.eos_token_id = 2\n        config.pad_token_id = 1\n        tokenizer = transformers.LlamaTokenizer.from_pretrained(\n            model_args.model_name_or_path,\n            cache_dir = model_args.cache_dir,\n            use_fast = model_args.use_fast_tokenizer,\n            revision = model_args.model_revision,\n            use_auth_token = True if model_args.use_auth_token else None,\n        )\n        tokenizer.bos_token_id = 1\n        tokenizer.eos_token_id = 2\n        tokenizer.pad_token_id = 1\n    else: # load original config\n        config = AutoConfig.from_pretrained(\n            model_args.config_name if model_args.config_name else model_args.model_name_or_path,\n            cache_dir=model_args.cache_dir,\n            revision=model_args.model_revision,\n            use_auth_token=True if model_args.use_auth_token else None,\n        )\n        tokenizer = AutoTokenizer.from_pretrained(\n            model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,\n            cache_dir=model_args.cache_dir,\n            use_fast=model_args.use_fast_tokenizer,\n            revision=model_args.model_revision,\n            use_auth_token=True if model_args.use_auth_token else None,\n        )\n\n    if 'llama' in model_args.model_name_or_path.lower():  # add llama\n        model_class = LlamaForCausalLM_with_lossmask\n        tokenizer.padding_side = 'left'\n    else: \n        model_class = AutoModelForSeq2SeqLM\n\n    if 'adapter' in model_args.model_name_or_path: # add lora-adapter to the original model\n        model = model_class.from_pretrained(config.base_model_name_or_path)\n        model = PeftModel.from_pretrained(model, model_args.model_name_or_path)\n    elif 'llama' in model_args.model_name_or_path.lower():\n        model = model_class.from_pretrained(\n            model_args.model_name_or_path,\n            from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n            config=config,\n            cache_dir=model_args.cache_dir,\n            revision=model_args.model_revision,\n            use_auth_token=True if model_args.use_auth_token else None\n        )\n        peft_config = LoraConfig(\n            task_type=TaskType.CAUSAL_LM, inference_mode=False, r=model_args.lora_dim, lora_alpha=32, lora_dropout=0.1\n        )\n        model = get_peft_model(model, peft_config)\n    else:\n        model = model_class.from_pretrained(\n            model_args.model_name_or_path,\n            from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n            config=config,\n            cache_dir=model_args.cache_dir,\n            revision=model_args.model_revision,\n            use_auth_token=True if model_args.use_auth_token else None,\n        )\n        peft_config = LoraConfig(\n            task_type=TaskType.SEQ_2_SEQ_LM, inference_mode=False, r=model_args.lora_dim, lora_alpha=32, lora_dropout=0.1\n        )\n        model = get_peft_model(model, peft_config)\n\n    model.resize_token_embeddings(len(tokenizer))\n\n    if 'llama' in model_args.model_name_or_path.lower():\n        model.generation_config.bos_token_id = 1\n        model.generation_config.eos_token_id = 2\n        model.generation_config.pad_token_id = 1\n        \n    # fix lora_A/B (bases of previous LoRA parameters, loaded in \"load_adapter\"[peft_momdel.py])\n    # fine-tune loranew_A/B (initialized in \"update_layer\"[lora.py])\n    # optional: lora_A/B is trainable but should not move too far from lorapre_A/B\n    # (constrained in \"training_step\"[uie_trainer_lora.py])\n    for name, param in model.named_parameters():\n        if name.find(\"loranew_\") != -1:\n            param.requires_grad = True\n        elif name.find(\"lora_\") != -1:\n            param.requires_grad = False\n        # this module should always be frozen because we change the vocabulary\n        elif name.find(\"shared\") != -1:\n            param.requires_grad = False\n\n    if (\n            hasattr(model.config, \"max_position_embeddings\")\n            and model.config.max_position_embeddings < data_args.max_source_length\n    ):\n        if model_args.resize_position_embeddings is None:\n            logger.warning(\n                f\"Increasing the model's number of position embedding vectors from {model.config.max_position_embeddings} \"\n                f\"to {data_args.max_source_length}.\"\n            )\n            model.resize_position_embeddings(data_args.max_source_length)\n        elif model_args.resize_position_embeddings:\n            model.resize_position_embeddings(data_args.max_source_length)\n        else:\n            raise ValueError(\n                f\"`--max_source_length` is set to {data_args.max_source_length}, but the model only has {model.config.max_position_embeddings}\"\n                f\" position encodings. Consider either reducing `--max_source_length` to {model.config.max_position_embeddings} or to automatically \"\n                \"resize the model's position encodings by passing `--resize_position_embeddings`.\"\n            )\n\n    if training_args.do_train:\n        if \"train\" not in raw_datasets:\n            raise ValueError(\"--do_train requires a train dataset\")\n        train_dataset = raw_datasets[\"train\"]\n        if data_args.max_train_samples is not None:\n            train_dataset = train_dataset.select(range(data_args.max_train_samples))\n\n    if training_args.do_eval:\n        if \"validation\" not in raw_datasets:\n            raise ValueError(\"--do_eval requires a validation dataset\")\n        eval_dataset = raw_datasets[\"validation\"]\n        if data_args.max_eval_samples is not None:\n            eval_dataset = eval_dataset.select(range(data_args.max_eval_samples))\n\n    if training_args.do_predict:\n        if \"test\" not in raw_datasets:\n            raise ValueError(\"--do_predict requires a test dataset\")\n        predict_dataset = raw_datasets[\"test\"]\n        if data_args.max_predict_samples is not None:\n            predict_dataset = predict_dataset.select(range(data_args.max_predict_samples))\n\n    # Data collator\n    label_pad_token_id = -100 if data_args.ignore_pad_token_for_loss else tokenizer.pad_token_id\n    data_collator = DataCollatorForUIE(\n        tokenizer,\n        model=model,\n        padding=\"longest\",\n        max_source_length=data_args.max_source_length,\n        max_target_length=data_args.max_target_length,\n        label_pad_token_id=label_pad_token_id,\n        pad_to_multiple_of=8 if training_args.fp16 else None,\n        add_task_name=data_args.add_task_name,\n        add_dataset_name=data_args.add_dataset_name,\n        num_examples=data_args.num_examples,\n        input_record_file=data_args.input_record_file\n    )\n    # we don't want to remove unused columns because we will prepare each batch during training,\n    # and some of the information will also be used in evaluation.\n    training_args.remove_unused_columns = False\n\n    # Metric\n    def compute_rouge_metrics(dataset, preds, save_prefix=None):\n        decoded_preds = skip_instructions(model, preds, tokenizer)\n        references = [e[\"Instance\"][\"label\"] for e in dataset]\n        result = compute_metrics(predictions=decoded_preds, references=references)\n        result_per_task = compute_grouped_metrics(predictions=decoded_preds, references=references,\n                                                  groups=dataset[\"Task\"])\n        result.update(result_per_task)\n        categories = dataset[\"Dataset\"]\n        result_per_category = compute_grouped_metrics(predictions=decoded_preds, references=references,\n                                                      groups=categories)\n        result.update(result_per_category)\n        prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n        result[\"gen_len\"] = np.mean(prediction_lens)\n        result = {k: round(v, 4) for k, v in result.items()}\n        if save_prefix is not None:\n            with open(os.path.join(training_args.output_dir, f\"{save_prefix}_eval_predictions.jsonl\"), \"w\") as fout:\n                for example, pred in zip(dataset, decoded_preds):\n                    fout.write(json.dumps({\n                        \"Task\": example[\"Task\"],\n                        \"Dataset\": example[\"Dataset\"],\n                        \"Instance\": example[\"Instance\"],\n                        \"Prediction\": pred\n                    }) + \"\\n\")\n        return result\n\n    print(f\"-----Gradient checkpointing: {training_args.gradient_checkpointing} -----\")\n    if training_args.gradient_checkpointing:\n        model.gradient_checkpointing_enable()\n\n    trainer = UIETrainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset if training_args.do_train else None,\n        eval_dataset=eval_dataset if training_args.do_eval else None,\n        tokenizer=tokenizer,\n        data_collator=data_collator,\n        compute_metrics=compute_rouge_metrics,\n        callbacks=[DenserEvalCallback] if training_args.denser_evaluation else None\n    )\n\n    all_metrics = {\"run_name\": training_args.run_name}\n\n    # Training\n    if training_args.do_train:\n        checkpoint = None\n        if training_args.resume_from_checkpoint is not None:\n            checkpoint = training_args.resume_from_checkpoint\n        elif last_checkpoint is not None:\n            checkpoint = last_checkpoint\n        train_result = trainer.train(resume_from_checkpoint=checkpoint)\n\n        peft_model_id = training_args.output_dir + \"/adapter\"\n        trainer.model.save_pretrained(peft_model_id)  \n        tokenizer.save_pretrained(peft_model_id)\n\n        metrics = train_result.metrics\n        max_train_samples = (\n            data_args.max_train_samples if data_args.max_train_samples is not None else len(train_dataset)\n        )\n        metrics[\"train_samples\"] = min(max_train_samples, len(train_dataset))\n\n        trainer.log_metrics(\"train\", metrics)\n        trainer.save_metrics(\"train\", metrics)\n        trainer.save_state()\n        logger.info(f\"Metrics {metrics}\")\n        all_metrics.update(metrics)\n\n    # Evaluation\n    results = {}\n    # in case the batch is shorter than max length, the output should be padded\n    max_new_tokens = (\n        training_args.generation_max_length\n        if training_args.generation_max_length is not None\n        else data_args.max_target_length\n    )\n\n    num_beams = data_args.num_beams if data_args.num_beams is not None else training_args.generation_num_beams\n    repetition_penalty = data_args.repetition_penalty\n\n    if training_args.do_predict:\n        logger.info(\"*** Prediction ***\")\n        logger.info(\"*** Loading CheckPoint ***\")\n\n        if data_args.max_predict_samples is not None:\n            predict_dataset = predict_dataset.select(range(data_args.max_predict_samples))\n\n        predict_results = trainer.predict(\n            predict_dataset,\n            metric_key_prefix=\"predict\",\n            max_new_tokens=max_new_tokens,\n            num_beams=num_beams,\n            repetition_penalty=repetition_penalty,\n            pad_token_id=tokenizer.pad_token_id\n        )\n        metrics = predict_results.metrics\n        max_predict_samples = (\n            data_args.max_predict_samples if data_args.max_predict_samples is not None else len(predict_dataset)\n        )\n        metrics[\"predict_samples\"] = min(max_predict_samples, len(predict_dataset))\n\n        trainer.log(metrics)\n        trainer.log_metrics(\"predict\", metrics)\n        trainer.save_metrics(\"predict\", metrics)\n        all_metrics.update(metrics)\n\n    return results\n\n\nif __name__ == \"__main__\":\n    main()\n"}
{"type": "source_file", "path": "pseudo_data/transfer_generate_sh.py", "content": "import json\ndef load_json(path):\n    with open(path, 'r', encoding='utf-8') as f:\n        data = json.load(f)\n    return data\n\ndef write_json(path, data):\n    with open(path, 'w', encoding='utf-8') as f:\n        json.dump(data, f, ensure_ascii=False,indent=2)\n\ndef load_jsonline(path):\n    with open(path, 'r', encoding='utf-8') as f:\n        result=[]\n        for line_s in f:\n            line=json.loads(line_s)\n            result.append(line)\n    return result\n\ndef write_jsonline(path, data):\n    with open(path, 'w', encoding='utf-8') as f:\n        for line in data:\n            line_s=json.dumps(line, ensure_ascii=False)\n            f.write(line_s)\n            f.write('\\n')\n\n\ndataset_list=[\n    \"task181_outcome_extraction\",\n    \"task591_sciq_answer_generation\",\n    \"task1729_personachat_generate_next\",\n    \"task1510_evalution_relation_extraction\",\n    \"task748_glucose_reverse_cause_event_detection\",\n    \"task002_quoref_answer_generation\",\n    \"task1687_sentiment140_classification\",\n    \"task511_reddit_tifu_long_text_summarization\",\n    \"task875_emotion_classification\",\n    \"task639_multi_woz_user_utterance_generation\",\n    \"task1290_xsum_summarization\",\n    \"task1590_diplomacy_text_generation\",\n    \"task1572_samsum_summary\",\n    \"task073_commonsenseqa_answer_generation\",\n    \"task363_sst2_polarity_classification\",\n]\n\nconfig_template={\n    \"Ours_CL\": [\n    ],\n}\n\nsh_str=rf'''#!/bin/bash\n#SBATCH -J cl                           \n#SBATCH -o cl-%j.out                       \n#SBATCH -p compute \n#SBATCH -N 1                           \n#SBATCH -t 5:00:00   \n#SBATCH --mem 64G \n#SBATCH --gres=gpu:nvidia_a100_80gb_pcie:1        \n\n\nexport CUDA_DEVICE_ORDER=\"PCI_BUS_ID\"\n\nport=$(shuf -i25000-30000 -n1)  \n\nlr=0.001\ntopk=20\n'''\noutput_name=r'outputs_lr_0001_topk_${topk}'\n\nfor idx in range(len(dataset_list)):\n    sh_str+=rf'''\n\nCUDA_VISIBLE_DEVICES=0 deepspeed --master_port $port src/run_uie_lora.py \\\n   --do_train \\\n   --do_predict \\\n   --predict_with_generate \\\n   --model_name_or_path ~/workplace/A_pretrain_models/t5_large \\\n   --data_dir CL_Benchmark \\\n   --task_config_dir configs/Ours_CL_configs/lookback_{dataset_list[idx]} \\\n   --instruction_file configs/instruction_config.json \\\n   --instruction_strategy single \\\n   --output_dir logs_and_outputs/Ours_CL/{output_name}/lookback_{dataset_list[idx]} \\\n   --per_device_train_batch_size 16 \\\n   --per_device_eval_batch_size 32 \\\n   --gradient_accumulation_steps 1 \\\n   --learning_rate $lr \\\n   --max_steps  5000 \\\n   --deepspeed configs/ds_configs/stage2.config \\\n   --run_name Ours_CL_round1 \\\n   --max_source_length 5 \\\n   --max_target_length 512 \\\n   --generation_max_length 512 \\\n   --add_task_name False \\\n   --add_dataset_name False \\\n   --overwrite_output_dir \\\n   --overwrite_cache \\\n   --lr_scheduler_type constant \\\n   --warmup_steps 0 \\\n   --logging_strategy epoch \\\n   --save_strategy epoch \\\n   --top_k $topk  \\\n'''\n    \nwith open(f'./scripts/Ours_CL_lookback.sh', 'w') as f:\n    f.write(sh_str)"}
{"type": "source_file", "path": "pseudo_data/src/uie_dataset_lora.py", "content": "# coding=utf-8\n# Copyright 2020 The TensorFlow Datasets Authors and the HuggingFace Datasets Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n\"\"\"CL_Benchmark Dataset.\"\"\"\n\nimport json\nimport os\nimport random\nimport datasets\nfrom hashlib import md5\n\nlogger = datasets.logging.get_logger(__name__)\nTASK_CONFIG_FILES = {\"train\": \"train_tasks.json\", \"dev\": \"dev_tasks.json\", \"test\": \"test_tasks.json\"}\nINSTRUCTION_STRATEGIES = ['single', 'multiple']\nANSWER_PREFIX = \"Answer:\"\nSINGLE_QUOTES_SUBSTITUTE = \"#$%#\"\nAUX_PROB = 0.3\n\n\ndef gen_cache_path(cache_dir, data_args):\n    hash_str = data_args.data_dir + data_args.task_config_dir + \\\n               data_args.instruction_file + data_args.instruction_strategy + \\\n               str(data_args.max_num_instances_per_task) + str(data_args.max_num_instances_per_eval_task)\n    hash_obj = md5(hash_str.encode(\"utf-8\"))\n    hash_id = hash_obj.hexdigest()\n    cache_path = os.path.join(cache_dir, str(hash_id))\n\n    return cache_path\n\n\ndef check_path(path):\n    if not path or not os.path.exists(path):\n        raise ValueError('{} is not valid, please check the input path!'.format(path))\n\n\ndef save_ds(instances, file_name):\n    with open(file_name, \"w+\", encoding='utf-8') as fi:\n        json.dump(instances, fi, ensure_ascii=False, indent=2)\n\n\nclass UIEConfig(datasets.BuilderConfig):\n    \"\"\"\n    Config dataset load procedure.\n\n    Args:\n        data_dir: task data dir, which contains the corresponding dataset dirs\n        prompt_path: prompt json file, which saves task and its prompts map\n        task_file: task config file, save training and testing split config, and sampling strategies.\n         Support two sampling strategies: 'random' indicates random sampling, while 'full' means to return all samples.\n        max_num_instances_per_task: max training sample size of each task\n        max_num_instances_per_eval_task: max eval sample size of each task\n    \"\"\"\n\n    def __init__(\n            self,\n            *args,\n            data_dir=None,\n            instruction_file=None,\n            instruction_strategy=None,\n            task_config_dir=None,\n            num_examples=None,\n            max_num_instances_per_task=None,\n            max_num_instances_per_eval_task=None,\n            over_sampling=None,\n            **kwargs\n    ):\n        super().__init__(*args, **kwargs)\n        self.data_dir = data_dir\n        self.num_examples = num_examples\n        self.over_sampling = over_sampling\n        self.instructions = self._parse_instruction(instruction_file)\n        self.task_configs = self._parse_task_config(task_config_dir)\n        self.instruction_strategy = instruction_strategy\n        self.max_num_instances_per_task = max_num_instances_per_task\n        self.max_num_instances_per_eval_task = max_num_instances_per_eval_task\n\n\n    def _parse_instruction(self, instruction_file):\n        \"\"\"\n        Instruction example:\n        {\n          \"RE\": [\n            {\"instruction_type\": \"zero-shot\", \"instruction\": \"Given a phrase that describes the relationship between\n            two words, extract the words and the lexical relationship between them.\n            The output format should be :[(word1, relation, word2)]. \\n\"},\n          ],\n          \"NER\": [\n            {\"instruction_type\": \"zero-shot\", \"instruction\": \"Please list all entity words in the text that\n            fit the category.Output format is [(word1, type1), (word2, type2))]. \\n\"},\n          ],\n          \"EE\": [\n            {\"instruction_type\": \"zero-shot\", \"instruction\": \"Extract the event information in the text\n            and return them in the event list. \\n\"}\n          ]\n        }\n        \"\"\"\n        if not instruction_file:\n            return None\n        instructions = {\"zero-shot\": {}, \"few-shot\": {}}\n\n        with open(instruction_file, 'r+') as f:\n            origin_instructions = json.load(f)\n\n        for task in origin_instructions:\n            for task_instruction in origin_instructions[task]:\n                instruct_type = task_instruction[\"instruction_type\"]\n                if instruct_type == \"zero-shot\":\n                    instructions['zero-shot'][task] = instructions['zero-shot'].get(task, [])\n                    instructions['zero-shot'][task].append(task_instruction[\"instruction\"])\n                elif instruct_type == \"few-shot\":\n                    instructions['few-shot'][task] = instructions['few-shot'].get(task, [])\n                    instructions['few-shot'][task].append(task_instruction[\"instruction\"])\n                else:\n                    raise ValueError(\"Invalid instruction type {}, please check your instruction file {}\"\n                                     .format(instruct_type, instruction_file))\n        return instructions\n\n\n    def _parse_task_config(self, task_config_dir):\n        \"\"\"\n        Task config file example:\n            {\n              \"SC\": [\n                {\"sampling strategy\": \"random\", \"dataset name\": \"amazon_review_full\"}\n              ],\n              \"TC\": [\n                {\"sampling strategy\": \"full\", \"dataset name\": \"ag_news\"}\n              ]\n            }\n        \"\"\"\n        if not task_config_dir:\n            return None\n\n        task_configs = {}\n        for task, file_name in TASK_CONFIG_FILES.items():\n            task_config_file = os.path.join(task_config_dir, file_name)\n\n            if not os.path.exists(task_config_file):\n                raise ValueError('Please check {} config, {} not exists!'.format(task, task_config_file))\n\n            with open(task_config_file, 'r+') as f:\n                task_configs[task] = json.loads(f.read())\n\n        return task_configs\n\n\n# TODO, few-shot, 需要 load 的时候就将值存好，放在 \"Examples\" 里面\nclass UIEInstructions(datasets.GeneratorBasedBuilder):\n    \"\"\"InstructUIE Dataset.\"\"\"\n\n    VERSION = datasets.Version(\"2.0.0\")\n    BUILDER_CONFIG_CLASS = UIEConfig\n    BUILDER_CONFIGS = [\n        UIEConfig(name=\"default\", description=\"Default config for NaturalInstructions\")\n    ]\n    DEFAULT_CONFIG_NAME = \"default\"\n\n    def _info(self):\n        return datasets.DatasetInfo(\n            features=datasets.Features(\n                {\n                    \"Task\": datasets.Value(\"string\"),\n                    \"Dataset\": datasets.Value(\"string\"),\n                    \"subset\": datasets.Value(\"string\"),\n                    \"Samples\": [{\n                        \"id\": datasets.Value(\"string\"),\n                        \"sentence\": datasets.Value(\"string\"),\n                        \"label\": datasets.Value(\"string\"),\n                        \"ground_truth\": datasets.Value(\"string\")\n                    }],\n                    \"Instance\": {\n                        \"id\": datasets.Value(\"string\"),\n                        \"sentence\": datasets.Value(\"string\"),\n                        \"label\": datasets.Value(\"string\"),\n                        \"instruction\": datasets.Value(\"string\"),\n                        \"ground_truth\": datasets.Value(\"string\")\n                    }\n                }\n            ),\n            supervised_keys=None\n        )\n\n\n    def _split_generators(self, dl_manager):\n        \"\"\"Returns SplitGenerators.\"\"\"\n        if self.config.data_dir is None or self.config.task_configs is None:\n            logger.error(\"Please provide right input: data_dir or task_config_dir!\")\n\n        # split dir save datasets\n        # task config to specify train,dev,test\n        split_dir = self.config.data_dir\n        task_configs = self.config.task_configs\n\n        return [\n            datasets.SplitGenerator(\n                name=datasets.Split.TRAIN,\n                gen_kwargs={\n                    \"path\": split_dir,\n                    \"task_config\": task_configs['train'],\n                    \"max_num_instances_per_task\": self.config.max_num_instances_per_task,\n                    \"subset\": \"train\"\n                }),\n            datasets.SplitGenerator(\n                name=datasets.Split.VALIDATION,\n                gen_kwargs={\n                    \"path\": split_dir,\n                    \"task_config\": task_configs['dev'],\n                    \"max_num_instances_per_task\": self.config.max_num_instances_per_eval_task,\n                    \"subset\": \"dev\"\n                }),\n            datasets.SplitGenerator(\n                name=datasets.Split.TEST,\n                gen_kwargs={\n                    \"path\": split_dir,\n                    \"task_config\": task_configs['test'],\n                    \"max_num_instances_per_task\": None,  # default load total test samples to test\n                    \"subset\": \"test\"\n                }),\n        ]\n\n\n    def _load_dataset(self, dataset_path, labels_path):\n        with open(dataset_path, encoding=\"utf-8\") as task_f:\n            s = task_f.read()\n            instances = json.loads(s)\n        with open(labels_path, encoding=\"utf-8\") as labels_f:\n            labels = json.load(labels_f)\n\n        return instances, labels\n\n\n    def _get_instruction(self, task):\n        assert self.config.instruction_strategy in INSTRUCTION_STRATEGIES\n        if self.config.num_examples is not None and self.config.num_examples > 0:\n            task_instructions = self.config.instructions['few-shot'][task]\n        else:\n            task_instructions = self.config.instructions['zero-shot'][task]\n        if self.config.instruction_strategy == \"single\":\n            return task_instructions[0]\n        else:\n            return random.choice(task_instructions)\n\n\n    def _sampling_dataset(self, instances, sampling_strategy, max_num_instances):\n        if sampling_strategy == 'random' and max_num_instances is not None and max_num_instances >= 0:\n            instances = instances[:max_num_instances]\n        if max_num_instances!=None and self.config.over_sampling and len(instances) < max_num_instances:\n            origin_instances = instances.copy()\n            while len(instances) < max_num_instances:\n                instances.append(random.choice(origin_instances))\n\n        return instances\n\n\n    def load_SC_dataset(self, dataset_path, labels_path, dataset_name, sampling_strategy, max_num_instances, subset):\n        # sentiment classification\n        # you should first modify the original dataset to the standard format (json):\n        # {\"label\": xxx, \"sentence\": \"Title\" + xxx + \"\\nText: \" + xxx + \"\\n\"}\n        \n        instances, labels = self._load_dataset(dataset_path, labels_path)\n\n        sample_template = {\"Task\": \"SC\", \"Dataset\": dataset_name, \"Samples\": [], \"subset\": subset}\n\n        labels_str = ', '.join(labels)\n        instances = self._sampling_dataset(instances, sampling_strategy, max_num_instances)\n\n        for idx, instance in enumerate(instances):\n            example = sample_template.copy()\n            instruction = self._get_instruction('SC')\n            instruction += \"Option: \" + labels_str + \" \\n\" + \"{0}\" + \"\\nAnswer:\" # value of \"sentence\" will be filled in {0}\n            label = instance['label']\n\n            example[\"Instance\"] = {\n                \"id\": str(idx),\n                \"sentence\": instance['sentence'],\n                \"label\": label,\n                \"ground_truth\": label,\n                \"instruction\": instruction\n            }\n\n            yield example\n\n\n    def load_TC_dataset(self, dataset_path, labels_path, dataset_name, sampling_strategy, max_num_instances, subset):\n        # text classification\n\n        instances, labels = self._load_dataset(dataset_path, labels_path)\n\n        sample_template = {\"Task\": \"TC\", \"Dataset\": dataset_name, \"Samples\": [], \"subset\": subset}\n\n        labels_str = ', '.join(labels)\n        instances = self._sampling_dataset(instances, sampling_strategy, max_num_instances)\n\n        for idx, instance in enumerate(instances):\n            example = sample_template.copy()\n            instruction = self._get_instruction('TC')\n            instruction += \"Option: \" + labels_str + \" \\n\" + \"{0}\" + \"\\nAnswer:\"\n            label = instance['label']\n\n            example[\"Instance\"] = {\n                \"id\": str(idx),\n                \"sentence\": instance['sentence'],\n                \"label\": label,\n                \"ground_truth\": label,\n                \"instruction\": instruction\n            }\n\n            yield example\n\n\n    def load_NLI_dataset(self, dataset_path, labels_path, dataset_name, sampling_strategy, max_num_instances, subset):\n\n        instances, labels = self._load_dataset(dataset_path, labels_path)\n\n        sample_template = {\"Task\": \"NLI\", \"Dataset\": dataset_name, \"Samples\": [], \"subset\": subset}\n\n        labels_str = ', '.join(labels)\n        instances = self._sampling_dataset(instances, sampling_strategy, max_num_instances)\n\n        for idx, instance in enumerate(instances):\n            example = sample_template.copy()\n            instruction = self._get_instruction('NLI')\n            instruction += \"Option: \" + labels_str + \" \\n\" + \"{0}\" + \"\\nAnswer:\"\n            label = instance['label']\n\n            example[\"Instance\"] = {\n                \"id\": str(idx),\n                \"sentence\": instance['sentence'],\n                \"label\": label,\n                \"ground_truth\": label,\n                \"instruction\": instruction\n            }\n\n            yield example\n\n\n    def load_QQP_dataset(self, dataset_path, labels_path, dataset_name, sampling_strategy, max_num_instances, subset):\n\n        instances, labels = self._load_dataset(dataset_path, labels_path)\n\n        sample_template = {\"Task\": \"QQP\", \"Dataset\": dataset_name, \"Samples\": [], \"subset\": subset}\n\n        labels_str = ', '.join(labels)\n        instances = self._sampling_dataset(instances, sampling_strategy, max_num_instances)\n\n        for idx, instance in enumerate(instances):\n            example = sample_template.copy()\n            instruction = self._get_instruction('QQP')\n            instruction += \"Option: \" + labels_str + \" \\n\" + \"{0}\" + \"\\nAnswer:\"\n            label = instance['label']\n\n            example[\"Instance\"] = {\n                \"id\": str(idx),\n                \"sentence\": instance['sentence'],\n                \"label\": label,\n                \"ground_truth\": label,\n                \"instruction\": instruction\n            }\n\n            yield example\n    \n\n    def load_BoolQA_dataset(self, dataset_path, labels_path, dataset_name, sampling_strategy, max_num_instances, subset):\n\n        instances, labels = self._load_dataset(dataset_path, labels_path)\n\n        sample_template = {\"Task\": \"BoolQA\", \"Dataset\": dataset_name, \"Samples\": [], \"subset\": subset}\n\n        labels_str = ', '.join(labels)\n        instances = self._sampling_dataset(instances, sampling_strategy, max_num_instances)\n\n        for idx, instance in enumerate(instances):\n            example = sample_template.copy()\n            instruction = self._get_instruction('BoolQA')\n            instruction += \"Option: \" + labels_str + \" \\n\" + \"{0}\" + \"\\nAnswer:\"\n            label = instance['label']\n\n            example[\"Instance\"] = {\n                \"id\": str(idx),\n                \"sentence\": instance['sentence'],\n                \"label\": label,\n                \"ground_truth\": label,\n                \"instruction\": instruction\n            }\n\n            yield example\n            \n\n    def load_COPA_dataset(self, dataset_path, labels_path, dataset_name, sampling_strategy, max_num_instances, subset):\n\n        instances, labels = self._load_dataset(dataset_path, labels_path)\n\n        sample_template = {\"Task\": \"COPA\", \"Dataset\": dataset_name, \"Samples\": [], \"subset\": subset}\n\n        labels_str = ', '.join(labels)\n        instances = self._sampling_dataset(instances, sampling_strategy, max_num_instances)\n\n        for idx, instance in enumerate(instances):\n            example = sample_template.copy()\n            instruction = \"{0}\" + \"\\nAnswer:\"\n            label = instance['label']\n\n            example[\"Instance\"] = {\n                \"id\": str(idx),\n                \"sentence\": instance['sentence'],\n                \"label\": label,\n                \"ground_truth\": label,\n                \"instruction\": instruction\n            }\n\n            yield example\n\n\n    def load_MultiRC_dataset(self, dataset_path, labels_path, dataset_name, sampling_strategy, max_num_instances, subset):\n\n        instances, labels = self._load_dataset(dataset_path, labels_path)\n\n        sample_template = {\"Task\": \"MultiRC\", \"Dataset\": dataset_name, \"Samples\": [], \"subset\": subset}\n\n        labels_str = ', '.join(labels)\n        instances = self._sampling_dataset(instances, sampling_strategy, max_num_instances)\n\n        for idx, instance in enumerate(instances):\n            example = sample_template.copy()\n            instruction = self._get_instruction('MultiRC')\n            instruction += \"Option: \" + labels_str + \" \\n\" + \"{0}\" + \"\\nAnswer:\"\n            label = instance['label']\n\n            example[\"Instance\"] = {\n                \"id\": str(idx),\n                \"sentence\": instance['sentence'],\n                \"label\": label,\n                \"ground_truth\": label,\n                \"instruction\": instruction\n            }\n\n            yield example\n\n\n    def load_WiC_dataset(self, dataset_path, labels_path, dataset_name, sampling_strategy, max_num_instances, subset):\n\n        instances, labels = self._load_dataset(dataset_path, labels_path)\n\n        sample_template = {\"Task\": \"WiC\", \"Dataset\": dataset_name, \"Samples\": [], \"subset\": subset}\n\n        labels_str = ', '.join(labels)\n        instances = self._sampling_dataset(instances, sampling_strategy, max_num_instances)\n\n        for idx, instance in enumerate(instances):\n            example = sample_template.copy()\n            instruction = self._get_instruction('WiC')\n            instruction += \"Option: \" + labels_str + \" \\n\" + \"{0}\" + \"\\nAnswer:\"\n            label = instance['label']\n\n            example[\"Instance\"] = {\n                \"id\": str(idx),\n                \"sentence\": instance['sentence'],\n                \"label\": label,\n                \"ground_truth\": label,\n                \"instruction\": instruction\n            }\n\n            yield example\n\n\n    def _generate_examples(self, path=None, task_config=None, max_num_instances_per_task=None, subset=None):\n        \"\"\"Yields examples.\"\"\"\n        logger.info(f\"Generating tasks from = {path}\")\n\n        for task in task_config:\n            if task == \"SC\":\n                load_func = self.load_SC_dataset\n            elif task == 'TC':\n                load_func = self.load_TC_dataset\n            elif task == 'NLI':\n                load_func = self.load_NLI_dataset\n            elif task == 'QQP':\n                load_func = self.load_QQP_dataset\n            elif task == 'BoolQA':\n                load_func = self.load_BoolQA_dataset\n            elif task == 'COPA':\n                load_func = self.load_COPA_dataset\n            elif task == 'MultiRC':\n                load_func = self.load_MultiRC_dataset\n            elif task == 'WiC':\n                load_func = self.load_WiC_dataset\n            else:\n                raise ValueError(\"Unsupport {} task, plz check {} task config!\".format(task, subset))\n\n            # load dataset\n            for dataset in task_config[task]:\n                ds_name = dataset[\"dataset name\"]\n                sampling_strategy = dataset.get(\"sampling strategy\", \"random\")\n                ds_path = os.path.join(path, task, ds_name, subset + '.json')\n                labels_path = os.path.join(path, task, ds_name, 'labels.json')\n                assert os.path.exists(ds_path)\n                assert os.path.exists(labels_path)\n\n                idx = -1\n                instances = []\n                for sample in load_func(ds_path, labels_path, ds_name, sampling_strategy, max_num_instances_per_task,\n                                        subset):\n                    idx += 1\n                    instances.append(sample)\n                    yield f\"{task}##{ds_path}##{idx}\", sample\n"}
{"type": "source_file", "path": "pseudo_data/src/rouge/scoring.py", "content": "# coding=utf-8\n# Copyright 2022 The Google Research Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Library for scoring and evaluation of text samples.\n\nAggregation functions use bootstrap resampling to compute confidence intervals\nas per the original ROUGE perl implementation.\n\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport abc\nimport collections\nfrom typing import Dict\n\nimport numpy as np\nimport six\nfrom six.moves import range\n\n\nclass Score(\n    collections.namedtuple(\"Score\", [\"precision\", \"recall\", \"fmeasure\"])):\n  \"\"\"Tuple containing precision, recall, and f-measure values.\"\"\"\n\n\nclass BaseScorer(object, metaclass=abc.ABCMeta):\n  \"\"\"Base class for Scorer objects.\"\"\"\n\n  @abc.abstractmethod\n  def score(self, target, prediction):\n    \"\"\"Calculates score between the target and prediction.\n\n    Args:\n      target: Text containing the target (ground truth) text.\n      prediction: Text containing the predicted text.\n\n    Returns:\n      A dict mapping each score_type (string) to Score object.\n    \"\"\"\n\n\nclass AggregateScore(\n    collections.namedtuple(\"AggregateScore\", [\"low\", \"mid\", \"high\"])):\n  \"\"\"Tuple containing confidence intervals for scores.\"\"\"\n\n\nclass BootstrapAggregator(object):\n  \"\"\"Aggregates scores to provide confidence intervals.\n\n  Sample usage:\n    scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'])\n    aggregator = Aggregator()\n    aggregator.add_scores(scorer.score(\"one two three\", \"one two\"))\n    aggregator.add_scores(scorer.score(\"one two five six\", \"seven eight\"))\n    result = aggregator.aggregate()\n    print result\n    {'rougeL': AggregateScore(\n         low=Score(precision=0.0, recall=0.0, fmeasure=0.0),\n         mid=Score(precision=0.5, recall=0.33, fmeasure=0.40),\n         high=Score(precision=1.0, recall=0.66, fmeasure=0.80)),\n     'rouge1': AggregateScore(\n         low=Score(precision=0.0, recall=0.0, fmeasure=0.0),\n         mid=Score(precision=0.5, recall=0.33, fmeasure=0.40),\n         high=Score(precision=1.0, recall=0.66, fmeasure=0.80))}\n  \"\"\"\n\n  def __init__(self, confidence_interval=0.95, n_samples=1000):\n    \"\"\"Initializes a BootstrapAggregator object.\n\n    Args:\n      confidence_interval: Confidence interval to compute on the mean as a\n        decimal.\n      n_samples: Number of samples to use for bootstrap resampling.\n\n    Raises:\n      ValueError: If invalid argument is given.\n    \"\"\"\n\n    if confidence_interval < 0 or confidence_interval > 1:\n      raise ValueError(\"confidence_interval must be in range [0, 1]\")\n    if n_samples <= 0:\n      raise ValueError(\"n_samples must be positive\")\n\n    self._n_samples = n_samples\n    self._confidence_interval = confidence_interval\n    self._scores = collections.defaultdict(list)\n\n  def add_scores(self, scores):\n    \"\"\"Adds a sample for future aggregation.\n\n    Args:\n      scores: Dict mapping score_type strings to a namedtuple object/class\n        representing a score.\n    \"\"\"\n\n    for score_type, score in six.iteritems(scores):\n      self._scores[score_type].append(score)\n\n  def aggregate(self):\n    \"\"\"Aggregates scores previously added using add_scores.\n\n    Returns:\n      A dict mapping score_type to AggregateScore objects.\n    \"\"\"\n\n    result = {}\n    for score_type, scores in six.iteritems(self._scores):\n      # Stack scores into a 2-d matrix of (sample, measure).\n      score_matrix = np.vstack(tuple(scores))\n      # Percentiles are returned as (interval, measure).\n      percentiles = self._bootstrap_resample(score_matrix)\n      # Extract the three intervals (low, mid, high).\n      intervals = tuple(\n          (scores[0].__class__(*percentiles[j, :]) for j in range(3)))\n      result[score_type] = AggregateScore(\n          low=intervals[0], mid=intervals[1], high=intervals[2])\n    return result\n\n  def _bootstrap_resample(self, matrix):\n    \"\"\"Performs bootstrap resampling on a matrix of scores.\n\n    Args:\n      matrix: A 2-d matrix of (sample, measure).\n\n    Returns:\n      A 2-d matrix of (bounds, measure). There are three bounds: low (row 0),\n      mid (row 1) and high (row 2). Mid is always the mean, while low and high\n      bounds are specified by self._confidence_interval (which defaults to 0.95\n      meaning it will return the 2.5th and 97.5th percentiles for a 95%\n      confidence interval on the mean).\n    \"\"\"\n\n    # Matrix of (bootstrap sample, measure).\n    sample_mean = np.zeros((self._n_samples, matrix.shape[1]))\n    for i in range(self._n_samples):\n      sample_idx = np.random.choice(\n          np.arange(matrix.shape[0]), size=matrix.shape[0])\n      sample = matrix[sample_idx, :]\n      sample_mean[i, :] = np.mean(sample, axis=0)\n\n    # Take percentiles on the estimate of the mean using bootstrap samples.\n    # Final result is a (bounds, measure) matrix.\n    percentile_delta = (1 - self._confidence_interval) / 2\n    q = 100 * np.array([percentile_delta, 0.5, 1 - percentile_delta])\n    return np.percentile(sample_mean, q, axis=0)\n\n\ndef fmeasure(precision, recall):\n  \"\"\"Computes f-measure given precision and recall values.\"\"\"\n\n  if precision + recall > 0:\n    return 2 * precision * recall / (precision + recall)\n  else:\n    return 0.0\n"}
{"type": "source_file", "path": "pseudo_data/src/rouge/rouge_scorer.py", "content": "# coding=utf-8\n# Copyright 2022 The Google Research Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Computes rouge scores between two text blobs.\n\nImplementation replicates the functionality in the original ROUGE package. See:\n\nLin, Chin-Yew. ROUGE: a Package for Automatic Evaluation of Summaries. In\nProceedings of the Workshop on Text Summarization Branches Out (WAS 2004),\nBarcelona, Spain, July 25 - 26, 2004.\n\nDefault options are equivalent to running:\nROUGE-1.5.5.pl -e data -n 2 -a settings.xml\n\nOr with use_stemmer=True:\nROUGE-1.5.5.pl -m -e data -n 2 -a settings.xml\n\nIn these examples settings.xml lists input files and formats.\n\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport re\n\nfrom absl import logging\nimport nltk\nimport six\nfrom six.moves import map\nfrom six.moves import range\nfrom rouge import scoring\nfrom rouge import tokenizers\n\n\nclass RougeScorer(scoring.BaseScorer):\n  \"\"\"Calculate rouges scores between two blobs of text.\n\n  Sample usage:\n    scorer = RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n    scores = scorer.score('The quick brown fox jumps over the lazy dog',\n                          'The quick brown dog jumps on the log.')\n  \"\"\"\n\n  def __init__(self, rouge_types, use_stemmer=False, split_summaries=False,\n               tokenizer=None):\n    \"\"\"Initializes a new RougeScorer.\n\n    Valid rouge types that can be computed are:\n      rougen (e.g. rouge1, rouge2): n-gram based scoring.\n      rougeL: Longest common subsequence based scoring.\n\n    Args:\n      rouge_types: A list of rouge types to calculate.\n      use_stemmer: Bool indicating whether Porter stemmer should be used to\n        strip word suffixes to improve matching. This arg is used in the\n        DefaultTokenizer, but other tokenizers might or might not choose to\n        use this.\n      split_summaries: whether to add newlines between sentences for rougeLsum\n      tokenizer: Tokenizer object which has a tokenize() method.\n    Returns:\n      A dict mapping rouge types to Score tuples.\n    \"\"\"\n\n    self.rouge_types = rouge_types\n    if tokenizer:\n      self._tokenizer = tokenizer\n    else:\n      self._tokenizer = tokenizers.DefaultTokenizer(use_stemmer)\n      logging.info(\"Using default tokenizer.\")\n\n    self._split_summaries = split_summaries\n\n  def score(self, target, prediction):\n    \"\"\"Calculates rouge scores between the target and prediction.\n\n    Args:\n      target: Text containing the target (ground truth) text.\n      prediction: Text containing the predicted text.\n    Returns:\n      A dict mapping each rouge type to a Score object.\n    Raises:\n      ValueError: If an invalid rouge type is encountered.\n    \"\"\"\n\n    # Pre-compute target tokens and prediction tokens for use by different\n    # types, except if only \"rougeLsum\" is requested.\n    if len(self.rouge_types) == 1 and self.rouge_types[0] == \"rougeLsum\":\n      target_tokens = None\n      prediction_tokens = None\n    else:\n      target_tokens = self._tokenizer.tokenize(target)\n      prediction_tokens = self._tokenizer.tokenize(prediction)\n    result = {}\n\n    for rouge_type in self.rouge_types:\n      if rouge_type == \"rougeL\":\n        # Rouge from longest common subsequences.\n        scores = _score_lcs(target_tokens, prediction_tokens)\n      elif rouge_type == \"rougeLsum\":\n        # Note: Does not support multi-line text.\n        def get_sents(text):\n          if self._split_summaries:\n            sents = nltk.sent_tokenize(text)\n          else:\n            # Assume sentences are separated by newline.\n            sents = six.ensure_str(text).split(\"\\n\")\n          sents = [x for x in sents if len(x)]\n          return sents\n\n        target_tokens_list = [\n            self._tokenizer.tokenize(s) for s in get_sents(target)]\n        prediction_tokens_list = [\n            self._tokenizer.tokenize(s) for s in get_sents(prediction)]\n\n        scores = _summary_level_lcs(target_tokens_list,\n                                    prediction_tokens_list)\n      elif re.match(r\"rouge[0-9]$\", six.ensure_str(rouge_type)):\n        # Rouge from n-grams.\n        n = int(rouge_type[5:])\n        if n <= 0:\n          raise ValueError(\"rougen requires positive n: %s\" % rouge_type)\n        target_ngrams = _create_ngrams(target_tokens, n)\n        prediction_ngrams = _create_ngrams(prediction_tokens, n)\n        scores = _score_ngrams(target_ngrams, prediction_ngrams)\n      else:\n        raise ValueError(\"Invalid rouge type: %s\" % rouge_type)\n      result[rouge_type] = scores\n\n    return result\n\n\ndef _create_ngrams(tokens, n):\n  \"\"\"Creates ngrams from the given list of tokens.\n\n  Args:\n    tokens: A list of tokens from which ngrams are created.\n    n: Number of tokens to use, e.g. 2 for bigrams.\n  Returns:\n    A dictionary mapping each bigram to the number of occurrences.\n  \"\"\"\n\n  ngrams = collections.Counter()\n  for ngram in (tuple(tokens[i:i + n]) for i in range(len(tokens) - n + 1)):\n    ngrams[ngram] += 1\n  return ngrams\n\n\ndef _score_lcs(target_tokens, prediction_tokens):\n  \"\"\"Computes LCS (Longest Common Subsequence) rouge scores.\n\n  Args:\n    target_tokens: Tokens from the target text.\n    prediction_tokens: Tokens from the predicted text.\n  Returns:\n    A Score object containing computed scores.\n  \"\"\"\n\n  if not target_tokens or not prediction_tokens:\n    return scoring.Score(precision=0, recall=0, fmeasure=0)\n\n  # Compute length of LCS from the bottom up in a table (DP appproach).\n  lcs_table = _lcs_table(target_tokens, prediction_tokens)\n  lcs_length = lcs_table[-1][-1]\n\n  precision = lcs_length / len(prediction_tokens)\n  recall = lcs_length / len(target_tokens)\n  fmeasure = scoring.fmeasure(precision, recall)\n\n  return scoring.Score(precision=precision, recall=recall, fmeasure=fmeasure)\n\n\ndef _lcs_table(ref, can):\n  \"\"\"Create 2-d LCS score table.\"\"\"\n  rows = len(ref)\n  cols = len(can)\n  lcs_table = [[0] * (cols + 1) for _ in range(rows + 1)]\n  for i in range(1, rows + 1):\n    for j in range(1, cols + 1):\n      if ref[i - 1] == can[j - 1]:\n        lcs_table[i][j] = lcs_table[i - 1][j - 1] + 1\n      else:\n        lcs_table[i][j] = max(lcs_table[i - 1][j], lcs_table[i][j - 1])\n  return lcs_table\n\n\ndef _backtrack_norec(t, ref, can):\n  \"\"\"Read out LCS.\"\"\"\n  i = len(ref)\n  j = len(can)\n  lcs = []\n  while i > 0 and j > 0:\n    if ref[i - 1] == can[j - 1]:\n      lcs.insert(0, i-1)\n      i -= 1\n      j -= 1\n    elif t[i][j - 1] > t[i - 1][j]:\n      j -= 1\n    else:\n      i -= 1\n  return lcs\n\n\ndef _summary_level_lcs(ref_sent, can_sent):\n  \"\"\"ROUGE: Summary-level LCS, section 3.2 in ROUGE paper.\n\n  Args:\n    ref_sent: list of tokenized reference sentences\n    can_sent: list of tokenized candidate sentences\n\n  Returns:\n    summary level ROUGE score\n  \"\"\"\n  if not ref_sent or not can_sent:\n    return scoring.Score(precision=0, recall=0, fmeasure=0)\n\n  m = sum(map(len, ref_sent))\n  n = sum(map(len, can_sent))\n  if not n or not m:\n    return scoring.Score(precision=0, recall=0, fmeasure=0)\n\n  # get token counts to prevent double counting\n  token_cnts_r = collections.Counter()\n  token_cnts_c = collections.Counter()\n  for s in ref_sent:\n    # s is a list of tokens\n    token_cnts_r.update(s)\n  for s in can_sent:\n    token_cnts_c.update(s)\n\n  hits = 0\n  for r in ref_sent:\n    lcs = _union_lcs(r, can_sent)\n    # Prevent double-counting:\n    # The paper describes just computing hits += len(_union_lcs()),\n    # but the implementation prevents double counting. We also\n    # implement this as in version 1.5.5.\n    for t in lcs:\n      if token_cnts_c[t] > 0 and token_cnts_r[t] > 0:\n        hits += 1\n        token_cnts_c[t] -= 1\n        token_cnts_r[t] -= 1\n\n  recall = hits / m\n  precision = hits / n\n  fmeasure = scoring.fmeasure(precision, recall)\n  return scoring.Score(precision=precision, recall=recall, fmeasure=fmeasure)\n\n\ndef _union_lcs(ref, c_list):\n  \"\"\"Find union LCS between a ref sentence and list of candidate sentences.\n\n  Args:\n    ref: list of tokens\n    c_list: list of list of indices for LCS into reference summary\n\n  Returns:\n    List of tokens in ref representing union LCS.\n  \"\"\"\n  lcs_list = [lcs_ind(ref, c) for c in c_list]\n  return [ref[i] for i in _find_union(lcs_list)]\n\n\ndef _find_union(lcs_list):\n  \"\"\"Finds union LCS given a list of LCS.\"\"\"\n  return sorted(list(set().union(*lcs_list)))\n\n\ndef lcs_ind(ref, can):\n  \"\"\"Returns one of the longest lcs.\"\"\"\n  t = _lcs_table(ref, can)\n  return _backtrack_norec(t, ref, can)\n\n\ndef _score_ngrams(target_ngrams, prediction_ngrams):\n  \"\"\"Compute n-gram based rouge scores.\n\n  Args:\n    target_ngrams: A Counter object mapping each ngram to number of\n      occurrences for the target text.\n    prediction_ngrams: A Counter object mapping each ngram to number of\n      occurrences for the prediction text.\n  Returns:\n    A Score object containing computed scores.\n  \"\"\"\n\n  intersection_ngrams_count = 0\n  for ngram in six.iterkeys(target_ngrams):\n    intersection_ngrams_count += min(target_ngrams[ngram],\n                                     prediction_ngrams[ngram])\n  target_ngrams_count = sum(target_ngrams.values())\n  prediction_ngrams_count = sum(prediction_ngrams.values())\n\n  precision = intersection_ngrams_count / max(prediction_ngrams_count, 1)\n  recall = intersection_ngrams_count / max(target_ngrams_count, 1)\n  fmeasure = scoring.fmeasure(precision, recall)\n\n  return scoring.Score(precision=precision, recall=recall, fmeasure=fmeasure)\n"}
{"type": "source_file", "path": "pseudo_data/src/uie_dataset_lora_ours.py", "content": "# coding=utf-8\n# Copyright 2020 The TensorFlow Datasets Authors and the HuggingFace Datasets Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n\"\"\"CL_Benchmark Dataset.\"\"\"\n\nimport json\nimport os\nimport random\nimport datasets\nfrom hashlib import md5\n\nlogger = datasets.logging.get_logger(__name__)\nTASK_CONFIG_FILES = {\"train\": \"train_tasks.json\", \"dev\": \"dev_tasks.json\", \"test\": \"test_tasks.json\"}\nINSTRUCTION_STRATEGIES = ['single', 'multiple']\nANSWER_PREFIX = \"Answer:\"\nSINGLE_QUOTES_SUBSTITUTE = \"#$%#\"\nAUX_PROB = 0.3\n\n\ndef gen_cache_path(cache_dir, data_args):\n    hash_str = data_args.data_dir + data_args.task_config_dir + \\\n               data_args.instruction_file + data_args.instruction_strategy + \\\n               str(data_args.max_num_instances_per_task) + str(data_args.max_num_instances_per_eval_task)\n    hash_obj = md5(hash_str.encode(\"utf-8\"))\n    hash_id = hash_obj.hexdigest()\n    cache_path = os.path.join(cache_dir, str(hash_id))\n\n    return cache_path\n\n\ndef check_path(path):\n    if not path or not os.path.exists(path):\n        raise ValueError('{} is not valid, please check the input path!'.format(path))\n\n\ndef save_ds(instances, file_name):\n    with open(file_name, \"w+\", encoding='utf-8') as fi:\n        json.dump(instances, fi, ensure_ascii=False, indent=2)\n\n\nclass UIEConfig(datasets.BuilderConfig):\n    \"\"\"\n    Config dataset load procedure.\n\n    Args:\n        data_dir: task data dir, which contains the corresponding dataset dirs\n        prompt_path: prompt json file, which saves task and its prompts map\n        task_file: task config file, save training and testing split config, and sampling strategies.\n         Support two sampling strategies: 'random' indicates random sampling, while 'full' means to return all samples.\n        max_num_instances_per_task: max training sample size of each task\n        max_num_instances_per_eval_task: max eval sample size of each task\n    \"\"\"\n\n    def __init__(\n            self,\n            *args,\n            data_dir=None,\n            instruction_file=None,\n            instruction_strategy=None,\n            task_config_dir=None,\n            num_examples=None,\n            max_num_instances_per_task=None,\n            max_num_instances_per_eval_task=None,\n            over_sampling=None,\n            **kwargs\n    ):\n        super().__init__(*args, **kwargs)\n        self.data_dir = data_dir\n        self.num_examples = num_examples\n        self.over_sampling = over_sampling\n        self.instructions = self._parse_instruction(instruction_file)\n        self.task_configs = self._parse_task_config(task_config_dir)\n        self.instruction_strategy = instruction_strategy\n        self.max_num_instances_per_task = max_num_instances_per_task\n        self.max_num_instances_per_eval_task = max_num_instances_per_eval_task\n\n\n    def _parse_instruction(self, instruction_file):\n        \"\"\"\n        Instruction example:\n        {\n          \"RE\": [\n            {\"instruction_type\": \"zero-shot\", \"instruction\": \"Given a phrase that describes the relationship between\n            two words, extract the words and the lexical relationship between them.\n            The output format should be :[(word1, relation, word2)]. \\n\"},\n          ],\n          \"NER\": [\n            {\"instruction_type\": \"zero-shot\", \"instruction\": \"Please list all entity words in the text that\n            fit the category.Output format is [(word1, type1), (word2, type2))]. \\n\"},\n          ],\n          \"EE\": [\n            {\"instruction_type\": \"zero-shot\", \"instruction\": \"Extract the event information in the text\n            and return them in the event list. \\n\"}\n          ]\n        }\n        \"\"\"\n        if not instruction_file:\n            return None\n        instructions = {\"zero-shot\": {}, \"few-shot\": {}}\n\n        with open(instruction_file, 'r+') as f:\n            origin_instructions = json.load(f)\n\n        for task in origin_instructions:\n            for task_instruction in origin_instructions[task]:\n                instruct_type = task_instruction[\"instruction_type\"]\n                if instruct_type == \"zero-shot\":\n                    instructions['zero-shot'][task] = instructions['zero-shot'].get(task, [])\n                    instructions['zero-shot'][task].append(task_instruction[\"instruction\"])\n                elif instruct_type == \"few-shot\":\n                    instructions['few-shot'][task] = instructions['few-shot'].get(task, [])\n                    instructions['few-shot'][task].append(task_instruction[\"instruction\"])\n                else:\n                    raise ValueError(\"Invalid instruction type {}, please check your instruction file {}\"\n                                     .format(instruct_type, instruction_file))\n        return instructions\n\n\n    def _parse_task_config(self, task_config_dir):\n        \"\"\"\n        Task config file example:\n            {\n              \"SC\": [\n                {\"sampling strategy\": \"random\", \"dataset name\": \"amazon_review_full\"}\n              ],\n              \"TC\": [\n                {\"sampling strategy\": \"full\", \"dataset name\": \"ag_news\"}\n              ]\n            }\n        \"\"\"\n        if not task_config_dir:\n            return None\n\n        task_configs = {}\n        for task, file_name in TASK_CONFIG_FILES.items():\n            task_config_file = os.path.join(task_config_dir, file_name)\n\n            if not os.path.exists(task_config_file):\n                raise ValueError('Please check {} config, {} not exists!'.format(task, task_config_file))\n\n            with open(task_config_file, 'r+') as f:\n                task_configs[task] = json.loads(f.read())\n\n        return task_configs\n\n\n# TODO, few-shot, 需要 load 的时候就将值存好，放在 \"Examples\" 里面\nclass UIEInstructions(datasets.GeneratorBasedBuilder):\n    \"\"\"InstructUIE Dataset.\"\"\"\n\n    VERSION = datasets.Version(\"2.0.0\")\n    BUILDER_CONFIG_CLASS = UIEConfig\n    BUILDER_CONFIGS = [\n        UIEConfig(name=\"default\", description=\"Default config for NaturalInstructions\")\n    ]\n    DEFAULT_CONFIG_NAME = \"default\"\n\n    def _info(self):\n        return datasets.DatasetInfo(\n            features=datasets.Features(\n                {\n                    \"Task\": datasets.Value(\"string\"),\n                    \"Dataset\": datasets.Value(\"string\"),\n                    \"subset\": datasets.Value(\"string\"),\n                    \"Samples\": [{\n                        \"id\": datasets.Value(\"string\"),\n                        \"sentence\": datasets.Value(\"string\"),\n                        \"label\": datasets.Value(\"string\"),\n                        \"ground_truth\": datasets.Value(\"string\")\n                    }],\n                    \"Instance\": {\n                        \"id\": datasets.Value(\"string\"),\n                        \"sentence\": datasets.Value(\"string\"),\n                        \"label\": datasets.Value(\"string\"),\n                        \"instruction\": datasets.Value(\"string\"),\n                        \"ground_truth\": datasets.Value(\"string\")\n                    }\n                }\n            ),\n            supervised_keys=None\n        )\n\n\n    def _split_generators(self, dl_manager):\n        \"\"\"Returns SplitGenerators.\"\"\"\n        if self.config.data_dir is None or self.config.task_configs is None:\n            logger.error(\"Please provide right input: data_dir or task_config_dir!\")\n\n        # split dir save datasets\n        # task config to specify train,dev,test\n        split_dir = self.config.data_dir\n        task_configs = self.config.task_configs\n\n        return [\n            datasets.SplitGenerator(\n                name=datasets.Split.TRAIN,\n                gen_kwargs={\n                    \"path\": split_dir,\n                    \"task_config\": task_configs['train'],\n                    \"max_num_instances_per_task\": self.config.max_num_instances_per_task,\n                    \"subset\": \"train\"\n                }),\n            datasets.SplitGenerator(\n                name=datasets.Split.VALIDATION,\n                gen_kwargs={\n                    \"path\": split_dir,\n                    \"task_config\": task_configs['dev'],\n                    \"max_num_instances_per_task\": self.config.max_num_instances_per_eval_task,\n                    \"subset\": \"dev\"\n                }),\n            datasets.SplitGenerator(\n                name=datasets.Split.TEST,\n                gen_kwargs={\n                    \"path\": split_dir,\n                    \"task_config\": task_configs['test'],\n                    \"max_num_instances_per_task\": None,  # default load total test samples to test\n                    \"subset\": \"test\"\n                }),\n        ]\n\n\n    def _load_dataset(self, dataset_path):\n        with open(dataset_path, encoding=\"utf-8\") as task_f:\n            s = task_f.read()\n            instances = json.loads(s)\n        \n        return instances\n\n\n    def _get_instruction(self, task):\n        assert self.config.instruction_strategy in INSTRUCTION_STRATEGIES\n        if self.config.num_examples is not None and self.config.num_examples > 0:\n            task_instructions = self.config.instructions['few-shot'][task]\n        else:\n            task_instructions = self.config.instructions['zero-shot'][task]\n        if self.config.instruction_strategy == \"single\":\n            return task_instructions[0]\n        else:\n            return random.choice(task_instructions)\n\n\n    def _sampling_dataset(self, instances, sampling_strategy, max_num_instances):\n        if sampling_strategy == 'random' and max_num_instances is not None and max_num_instances >= 0:\n            instances = instances[:max_num_instances]\n        if max_num_instances!=None and self.config.over_sampling and len(instances) < max_num_instances:\n            origin_instances = instances.copy()\n            while len(instances) < max_num_instances:\n                instances.append(random.choice(origin_instances))\n\n        return instances\n    \n\n    def load_Ours_CL_dataset(self, dataset_path, labels_path, dataset_name, sampling_strategy, max_num_instances, subset):\n\n        data = self._load_dataset(dataset_path)\n\n        sample_template = {\"Task\": \"CL\", \"Dataset\": dataset_name, \"Samples\": [], \"subset\": subset}\n\n        for idx, instance in enumerate(data['Instances']):\n            example = sample_template.copy()\n\n            instruction = \"{0}\"\n            if isinstance(instance[\"output\"], list):\n                label=instance[\"output\"][random.randint(0, len(instance[\"output\"])-1)]\n            else:\n                label=instance[\"output\"]\n\n            example[\"Instance\"] = {\n                \"id\": str(idx),\n                \"sentence\": instance['input'],\n                \"label\": label,\n                \"ground_truth\": label,\n                \"instruction\": instruction\n            }\n\n            yield example\n\n\n    def _generate_examples(self, path=None, task_config=None, max_num_instances_per_task=None, subset=None):\n        \"\"\"Yields examples.\"\"\"\n        logger.info(f\"Generating tasks from = {path}\")\n\n        for task in task_config:\n            if task == 'Ours_CL':\n                load_func = self.load_Ours_CL_dataset\n            else:\n                raise ValueError(\"Unsupport {} task, plz check {} task config!\".format(task, subset))\n\n            # load dataset\n            for dataset in task_config[task]:\n                ds_name = dataset[\"dataset name\"]\n                sampling_strategy = dataset.get(\"sampling strategy\", \"random\")\n                ds_path = os.path.join(path, task, ds_name, subset + '.json')\n                labels_path = None\n                assert os.path.exists(ds_path)\n\n                idx = -1\n                instances = []\n                for sample in load_func(ds_path, labels_path, ds_name, sampling_strategy, max_num_instances_per_task,\n                                        subset):\n                    idx += 1\n                    instances.append(sample)\n                    yield f\"{task}##{ds_path}##{idx}\", sample\n"}
{"type": "source_file", "path": "pseudo_data/src/peft_1/utils/other.py", "content": "# coding=utf-8\n# Copyright 2023-present the HuggingFace Inc. team.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport copy\n\nimport torch\n\n\n# needed for prefix-tuning of bloom model\ndef bloom_model_postprocess_past_key_value(past_key_values):\n    past_key_values = torch.cat(past_key_values)\n    total_layers, batch_size, num_attention_heads, num_virtual_tokens, head_dim = past_key_values.shape\n    keys = past_key_values[: total_layers // 2]\n    keys = keys.transpose(2, 3).reshape(\n        total_layers // 2, batch_size * num_attention_heads, head_dim, num_virtual_tokens\n    )\n    values = past_key_values[total_layers // 2 :]\n    values = values.reshape(total_layers // 2, batch_size * num_attention_heads, num_virtual_tokens, head_dim)\n\n    return tuple(zip(keys, values))\n\n\ndef prepare_model_for_int8_training(model, use_gradient_checkpointing=True):\n    r\"\"\"\n    This method wraps the entire protocol for preparing a model before running a training. This includes:\n        1- Cast the layernorm in fp32 2- making output embedding layer require grads 3- Add the upcasting of the lm\n        head to fp32\n\n    Args:\n        model, (`transformers.PreTrainedModel`):\n            The loaded model from `transformers`\n    \"\"\"\n    loaded_in_8bit = getattr(model, \"is_loaded_in_8bit\", False)\n\n    for name, param in model.named_parameters():\n        # freeze base model's layers\n        param.requires_grad = False\n\n    # cast all non INT8 parameters to fp32\n    for param in model.parameters():\n        if (param.dtype == torch.float16) or (param.dtype == torch.bfloat16):\n            param.data = param.data.to(torch.float32)\n\n    if loaded_in_8bit and use_gradient_checkpointing:\n        # For backward compatibility\n        if hasattr(model, \"enable_input_require_grads\"):\n            model.enable_input_require_grads()\n        else:\n\n            def make_inputs_require_grad(module, input, output):\n                output.requires_grad_(True)\n\n            model.get_input_embeddings().register_forward_hook(make_inputs_require_grad)\n\n        # enable gradient checkpointing for memory efficiency\n        model.gradient_checkpointing_enable()\n\n    return model\n\n\n# copied from transformers.models.bart.modeling_bart\ndef shift_tokens_right(input_ids: torch.Tensor, pad_token_id: int, decoder_start_token_id: int):\n    \"\"\"\n    Shift input ids one token to the right.\n\n    Args:\n        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`): input ids\n        pad_token_id (`int`): The id of the `padding` token.\n        decoder_start_token_id (`int`): The id of the `start` token.\n    \"\"\"\n    shifted_input_ids = input_ids.new_zeros(input_ids.shape)\n    shifted_input_ids[:, 1:] = input_ids[:, :-1].clone()\n    shifted_input_ids[:, 0] = decoder_start_token_id\n\n    if pad_token_id is None:\n        raise ValueError(\"self.model.config.pad_token_id has to be defined.\")\n    # replace possible -100 values in labels by `pad_token_id`\n    shifted_input_ids.masked_fill_(shifted_input_ids == -100, pad_token_id)\n\n    return shifted_input_ids\n\n\nclass ModulesToSaveWrapper(torch.nn.Module):\n    def __init__(self, module_to_save, adapter_name):\n        super().__init__()\n        self.original_module = module_to_save\n        self.modules_to_save = torch.nn.ModuleDict({})\n        self.update(adapter_name)\n        self.active_adapter = adapter_name\n\n    def update(self, adapter_name):\n        self.modules_to_save.update(torch.nn.ModuleDict({adapter_name: copy.deepcopy(self.original_module)}))\n\n    def forward(self, *args, **kwargs):\n        if self.active_adapter not in self.modules_to_save:\n            return self.original_module(*args, **kwargs)\n        return self.modules_to_save[self.active_adapter](*args, **kwargs)\n\n\ndef _get_submodules(model, key):\n    parent = model.get_submodule(\".\".join(key.split(\".\")[:-1]))\n    target_name = key.split(\".\")[-1]\n    target = model.get_submodule(key)\n    return parent, target, target_name\n\n\ndef _freeze_adapter(model, adapter_name):\n    for n, p in model.named_parameters():\n        if adapter_name in n:\n            p.requires_grad = False\n\n\ndef _set_trainable(model, adapter_name):\n    key_list = [key for key, _ in model.named_modules()]\n    for key in key_list:\n        target_module_found = any(key.endswith(target_key) for target_key in model.modules_to_save)\n        if target_module_found:\n            parent, target, target_name = _get_submodules(model, key)\n            if isinstance(target, ModulesToSaveWrapper):\n                target.update(adapter_name)\n            else:\n                for param in target.parameters():\n                    param.requires_grad = True\n                setattr(parent, target_name, ModulesToSaveWrapper(target, adapter_name))\n\n\ndef _set_adapter(model, adapter_name):\n    for module in model.modules():\n        if isinstance(module, ModulesToSaveWrapper):\n            module.active_adapter = adapter_name\n\n\ndef fsdp_auto_wrap_policy(model):\n    import functools\n    import os\n\n    from accelerate import FullyShardedDataParallelPlugin\n    from torch.distributed.fsdp.wrap import _or_policy, lambda_auto_wrap_policy, transformer_auto_wrap_policy\n\n    from ..tuners import PrefixEncoder, PromptEmbedding, PromptEncoder\n\n    def lambda_policy_fn(module):\n        if (\n            len(list(module.named_children())) == 0\n            and getattr(module, \"weight\", None) is not None\n            and module.weight.requires_grad\n        ):\n            return True\n        return False\n\n    lambda_policy = functools.partial(lambda_auto_wrap_policy, lambda_fn=lambda_policy_fn)\n    transformer_wrap_policy = functools.partial(\n        transformer_auto_wrap_policy,\n        transformer_layer_cls=(\n            PrefixEncoder,\n            PromptEncoder,\n            PromptEmbedding,\n            FullyShardedDataParallelPlugin.get_module_class_from_name(\n                model, os.environ.get(\"FSDP_TRANSFORMER_CLS_TO_WRAP\", \"\")\n            ),\n        ),\n    )\n\n    auto_wrap_policy = functools.partial(_or_policy, policies=[lambda_policy, transformer_wrap_policy])\n    return auto_wrap_policy\n\n\ndef transpose(weight, fan_in_fan_out):\n    return weight.T if fan_in_fan_out else weight\n\n\nTRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING = {\n    \"t5\": [\"q\", \"v\"],\n    \"mt5\": [\"q\", \"v\"],\n    \"bart\": [\"q_proj\", \"v_proj\"],\n    \"gpt2\": [\"c_attn\"],\n    \"bloom\": [\"query_key_value\"],\n    \"blip-2\": [\"q\", \"v\", \"q_proj\", \"v_proj\"],\n    \"opt\": [\"q_proj\", \"v_proj\"],\n    \"gptj\": [\"q_proj\", \"v_proj\"],\n    \"gpt_neox\": [\"query_key_value\"],\n    \"gpt_neo\": [\"q_proj\", \"v_proj\"],\n    \"bert\": [\"query\", \"value\"],\n    \"roberta\": [\"query\", \"value\"],\n    \"xlm-roberta\": [\"query\", \"value\"],\n    \"electra\": [\"query\", \"value\"],\n    \"deberta-v2\": [\"query_proj\", \"value_proj\"],\n    \"deberta\": [\"in_proj\"],\n    \"layoutlm\": [\"query\", \"value\"],\n    \"llama\": [\"q_proj\", \"v_proj\"],\n    \"chatglm\": [\"query_key_value\"],\n}\n\nTRANSFORMERS_MODELS_TO_ADALORA_TARGET_MODULES_MAPPING = {\n    \"t5\": [\"q\", \"k\", \"v\", \"o\", \"wi\", \"wo\"],\n    \"mt5\": [\"q\", \"k\", \"v\", \"o\", \"wi_0\", \"wi_1\", \"wo\"],\n    \"bart\": [\"q_proj\", \"k_proj\", \"v_proj\", \"out_proj\", \"fc1\", \"fc2\"],\n    # \"gpt2\": [\"c_attn\"],\n    # \"bloom\": [\"query_key_value\"],\n    \"opt\": [\"q_proj\", \"k_proj\", \"v_proj\", \"out_proj\", \"fc1\", \"fc2\"],\n    # \"gptj\": [\"q_proj\", \"v_proj\"],\n    # \"gpt_neox\": [\"query_key_value\"],\n    # \"gpt_neo\": [\"q_proj\", \"v_proj\"],\n    # \"bert\": [\"query\", \"value\"],\n    \"roberta\": [\"query\", \"key\", \"value\", \"dense\"],\n    # \"xlm-roberta\": [\"query\", \"value\"],\n    # \"electra\": [\"query\", \"value\"],\n    \"deberta-v2\": [\"query_proj\", \"key_proj\", \"value_proj\", \"dense\"],\n    # \"deberta\": [\"in_proj\"],\n    # \"layoutlm\": [\"query\", \"value\"],\n}\n\nTRANSFORMERS_MODELS_TO_PREFIX_TUNING_POSTPROCESS_MAPPING = {\n    \"bloom\": bloom_model_postprocess_past_key_value,\n}\n\nWEIGHTS_NAME = \"adapter_model.bin\"\nCONFIG_NAME = \"adapter_config.json\"\n"}
{"type": "source_file", "path": "pseudo_data/src/uie_collator.py", "content": "import logging\r\n\r\nimport torch\r\nfrom transformers.data.data_collator import *\r\n\r\n\r\nlogger = logging.getLogger(__name__)\r\n\r\nSUPPORTED_DECODER_MODELS = ['codegen', 'bloomz', 'gpt-neox', 'llama']\r\nSUPPORTED_SEQ2SEQ_MODELS = ['t5', 'flan-t5']\r\n\r\n\r\ndef check_model(model_name, supported_models):\r\n    for sup_model in supported_models:\r\n        if sup_model.lower() in model_name.lower():\r\n            return True\r\n\r\n    return False\r\n\r\n\r\n@dataclass\r\nclass DataCollatorForUIE:\r\n    tokenizer: PreTrainedTokenizerBase\r\n    model: Optional[Any] = None\r\n    padding: Union[bool, str, PaddingStrategy] = True\r\n    max_source_length: Optional[int] = None\r\n    max_target_length: Optional[int] = None\r\n    pad_to_multiple_of: Optional[int] = None\r\n    label_pad_token_id: int = -100\r\n    return_tensors: str = \"pt\"\r\n    add_task_name: bool = False\r\n    add_dataset_name: bool = False\r\n    common_dataset_name: str = None\r\n    text_only: bool = False\r\n    num_examples: int = 0\r\n    input_record_file: str = None\r\n\r\n    def __call__(self, batch, return_tensors=None):\r\n        if return_tensors is None:\r\n            return_tensors = self.return_tensors\r\n\r\n        model_name = self.model.config._name_or_path\r\n        # print(model_name)\r\n        if check_model(model_name, SUPPORTED_DECODER_MODELS):\r\n            model_inputs = self.decoder_call(batch, return_tensors)\r\n        elif check_model(model_name, SUPPORTED_SEQ2SEQ_MODELS):\r\n            model_inputs = self.seq2seq_call(batch, return_tensors)\r\n        else:\r\n            raise ValueError('Unsupport model {}!'.format(model_name))\r\n\r\n        return model_inputs\r\n\r\n    def get_instruction(self, instance):\r\n        # \"instructions \\n options \\n {0} \\n Answer: \"\r\n        instruction = instance['Instance'][\"instruction\"]\r\n        content = instance['Instance']['sentence']\r\n\r\n        # add task/ds prefix\r\n        prefix = ''\r\n        if self.add_task_name:\r\n            prefix += \"Task:\" + instance['Task'] + '\\n'\r\n        if self.add_dataset_name:\r\n            ds_name = self.common_dataset_name if self.common_dataset_name else instance['Dataset']\r\n            prefix = prefix + \"Dataset:\"\r\n            prefix = prefix + ds_name + '\\n' if prefix else instance['Dataset'] + '\\n'\r\n        if prefix:\r\n            instruction = prefix + instruction\r\n\r\n        # TODO, support few shot\r\n        # add few shot samples\r\n        samples = ''\r\n        if len(instance['Samples']) > 0:\r\n            raise Exception('Few shot is coming soon...')\r\n        if samples:\r\n            content = samples + content\r\n        # TODO, fix bug\r\n        try:\r\n            instruction = instruction.format(content)\r\n        finally:\r\n            return instruction\r\n\r\n\r\n    def seq2seq_call(self, batch, return_tensors):\r\n        sources = []\r\n        labels = []\r\n\r\n        for instance in batch:\r\n            label = instance['Instance']['label']\r\n            labels.append(label)\r\n            instruction = self.get_instruction(instance)\r\n\r\n            source = instruction\r\n            tokenized_source = self.tokenizer(source)[\"input_ids\"]\r\n            if len(tokenized_source) <= self.max_source_length:\r\n                sources.append(source)\r\n            else:\r\n                sources.append(self.tokenizer.decode(tokenized_source[:self.max_source_length], skip_special_tokens=True))\r\n\r\n        # TODO, support online demo\r\n        if self.text_only:\r\n            model_inputs = {\"inputs\": sources, \"labels\": labels}\r\n        else:\r\n            model_inputs = self.tokenizer(\r\n                sources,\r\n                max_length=self.max_source_length,\r\n                padding=self.padding,\r\n                return_tensors=return_tensors,\r\n                truncation=True,\r\n                pad_to_multiple_of=self.pad_to_multiple_of\r\n            )\r\n            with self.tokenizer.as_target_tokenizer():\r\n                labels = self.tokenizer(\r\n                    labels,\r\n                    max_length=self.max_target_length,\r\n                    padding=self.padding,\r\n                    return_tensors=return_tensors,\r\n                    truncation=True,\r\n                    pad_to_multiple_of=self.pad_to_multiple_of\r\n                )\r\n            label_mask = labels[\"attention_mask\"].bool()\r\n            model_inputs[\"labels\"] = labels[\"input_ids\"].masked_fill(~label_mask, self.label_pad_token_id)\r\n\r\n            # prepare decoder_input_ids\r\n            if self.model is not None:\r\n                decoder_input_ids = self.model.prepare_decoder_input_ids_from_labels(labels=model_inputs[\"labels\"])\r\n                model_inputs[\"decoder_input_ids\"] = decoder_input_ids\r\n\r\n            self._save_samples(model_inputs, sources, labels)\r\n\r\n        return model_inputs\r\n\r\n    def decoder_call(self, batch, return_tensors):\r\n        self.tokenizer.padding_side = 'left'\r\n        sources = []\r\n        label_lens = []\r\n        labels = []\r\n        max_len = -1\r\n        if batch[0]['subset'] == \"train\":\r\n            limit_input_len = self.max_source_length + self.max_target_length\r\n        else:\r\n            limit_input_len = self.max_source_length\r\n\r\n        for instance in batch:\r\n            label = instance['Instance']['label']\r\n            labels.append(label)\r\n            instruction = self.get_instruction(instance)\r\n\r\n            # add bos and eos\r\n            task_input = self.tokenizer.bos_token + instruction\r\n            label = label + self.tokenizer.eos_token\r\n\r\n            tokenized_input = self.tokenizer(task_input)[\"input_ids\"]\r\n            tokenized_label = self.tokenizer(label)[\"input_ids\"]\r\n\r\n            # (input) for inference, (input + label) for training\r\n            if instance['subset'] in ['dev', 'test']:\r\n                label_lens.append(0)\r\n                if len(tokenized_input) <= limit_input_len:\r\n                    max_len = max(len(tokenized_input), max_len)\r\n                    sources.append(task_input)\r\n                else:\r\n                    max_len = limit_input_len\r\n                    input_wo_label = self.tokenizer.decode(\r\n                        tokenized_input[: limit_input_len],\r\n                        skip_special_tokens=False\r\n                    )\r\n                    sources.append(input_wo_label)\r\n            else:\r\n                if len(tokenized_input) + len(tokenized_label) <= limit_input_len:\r\n                    max_len = max(len(tokenized_input) + len(tokenized_label), max_len)\r\n                    label_lens.append(len(tokenized_label))\r\n                    sources.append(task_input + label)\r\n                else:\r\n                    max_len = self.max_source_length\r\n                    input_w_label = self.tokenizer.decode(\r\n                        (tokenized_input + tokenized_label)[: limit_input_len],\r\n                        skip_special_tokens=False\r\n                    )\r\n                    sources.append(input_w_label)\r\n                    label_lens.append(max(0, limit_input_len - len(tokenized_input)))\r\n\r\n        # TODO, support online demo\r\n        if self.text_only:\r\n            model_inputs = {\"inputs\": sources, 'labels': labels}\r\n        else:\r\n            model_inputs = self.tokenizer(\r\n                sources,\r\n                max_length=self.max_source_length,\r\n                padding=self.padding,\r\n                return_tensors=return_tensors,\r\n                truncation=True,\r\n                pad_to_multiple_of=self.pad_to_multiple_of\r\n            )\r\n\r\n            label_mask = model_inputs[\"attention_mask\"].bool()\r\n            model_inputs[\"labels\"] = model_inputs['input_ids'].masked_fill(~label_mask, self.label_pad_token_id)\r\n\r\n            # loss mask\r\n            max_len = min(max_len, limit_input_len)\r\n            loss_mask = torch.ones((label_mask.shape))\r\n            for k, label_len in enumerate(label_lens):\r\n                loss_mask[k, : max_len - label_len - 1] = 0\r\n            model_inputs['loss_mask'] = loss_mask.masked_fill(~label_mask, 0)\r\n\r\n            self._save_samples(model_inputs, sources, labels)\r\n\r\n        return model_inputs\r\n\r\n    def _save_samples(self, model_inputs, sources, labels):\r\n        if not self.input_record_file:\r\n            return\r\n\r\n        loss_label = []\r\n        if hasattr(model_inputs, 'loss_mask'):\r\n            for loss, id in zip(model_inputs.loss_mask, model_inputs.input_ids):\r\n                loss_label.append(self.tokenizer.decode((loss * id).view(-1).int()))\r\n\r\n            with open(self.input_record_file, 'a+', encoding='utf-8') as f:\r\n                for text, label, mask_label in zip(sources, labels, loss_label):\r\n                    f.write(text+'\\n')\r\n                    f.write(label + '\\n')\r\n                    f.write(mask_label+'\\n\\n')\r\n        else:\r\n            with open(self.input_record_file, 'a+', encoding='utf-8') as f:\r\n                for text, label in zip(sources, labels['input_ids']):\r\n                    f.write(text + '\\n')\r\n                    f.write(self.tokenizer.decode(label, clean_up_tokenization_spaces=False) + '\\n')"}
{"type": "source_file", "path": "src/cl_dataset.py", "content": "# coding=utf-8\n# Copyright 2020 The TensorFlow Datasets Authors and the HuggingFace Datasets Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n\"\"\"CL_Benchmark Dataset.\"\"\"\n\nimport json\nimport os\nimport random\nimport datasets\nfrom hashlib import md5\n\nlogger = datasets.logging.get_logger(__name__)\nTASK_CONFIG_FILES = {\"train\": \"train_tasks.json\", \"dev\": \"dev_tasks.json\", \"test\": \"test_tasks.json\"}\nINSTRUCTION_STRATEGIES = ['single', 'multiple']\nANSWER_PREFIX = \"Output:\"\nSINGLE_QUOTES_SUBSTITUTE = \"#$%#\"\nAUX_PROB = 0.3\n\n\ndef gen_cache_path(cache_dir, data_args):\n    hash_str = data_args.data_dir + data_args.task_config_dir + \\\n               str(data_args.max_num_instances_per_task) + str(data_args.max_num_instances_per_eval_task)\n    hash_obj = md5(hash_str.encode(\"utf-8\"))\n    hash_id = hash_obj.hexdigest()\n    cache_path = os.path.join(cache_dir, str(hash_id))\n\n    return cache_path\n\n\ndef check_path(path):\n    if not path or not os.path.exists(path):\n        raise ValueError('{} is not valid, please check the input path!'.format(path))\n\n\ndef save_ds(instances, file_name):\n    with open(file_name, \"w+\", encoding='utf-8') as fi:\n        json.dump(instances, fi, ensure_ascii=False, indent=2)\n\n\nclass CLConfig(datasets.BuilderConfig):\n    \"\"\"\n    Config dataset load procedure.\n\n    Args:\n        data_dir: task data dir, which contains the corresponding dataset dirs\n        prompt_path: prompt json file, which saves task and its prompts map\n        task_file: task config file, save training and testing split config, and sampling strategies.\n         Support two sampling strategies: 'random' indicates random sampling, while 'full' means to return all samples.\n        max_num_instances_per_task: max training sample size of each task\n        max_num_instances_per_eval_task: max eval sample size of each task\n    \"\"\"\n\n    def __init__(\n            self,\n            *args,\n            data_dir=None,\n            task_config_dir=None,\n            num_examples=None,\n            max_num_instances_per_task=None,\n            max_num_instances_per_eval_task=None,\n            over_sampling=None,\n            **kwargs\n    ):\n        super().__init__(*args, **kwargs)\n        self.data_dir = data_dir\n        self.num_examples = num_examples\n        self.over_sampling = over_sampling\n        self.task_configs = self._parse_task_config(task_config_dir)\n        self.max_num_instances_per_task = max_num_instances_per_task\n        self.max_num_instances_per_eval_task = max_num_instances_per_eval_task\n\n    def _parse_task_config(self, task_config_dir):\n        if not task_config_dir:\n            return None\n\n        task_configs = {}\n        for task, file_name in TASK_CONFIG_FILES.items():\n            task_config_file = os.path.join(task_config_dir, file_name)\n\n            if not os.path.exists(task_config_file):\n                raise ValueError('Please check {} config, {} not exists!'.format(task, task_config_file))\n\n            with open(task_config_file, 'r+') as f:\n                task_configs[task] = json.loads(f.read())\n\n        return task_configs\n\n\nclass CLInstructions(datasets.GeneratorBasedBuilder):\n    \"\"\"CL Dataset.\"\"\"\n\n    VERSION = datasets.Version(\"2.0.0\")\n    BUILDER_CONFIG_CLASS = CLConfig\n    BUILDER_CONFIGS = [\n        CLConfig(name=\"default\", description=\"Default config for NaturalInstructions\")\n    ]\n    DEFAULT_CONFIG_NAME = \"default\"\n\n    def _info(self):\n        return datasets.DatasetInfo(\n            features=datasets.Features(\n                {\n                    \"Task\": datasets.Value(\"string\"),\n                    \"Dataset\": datasets.Value(\"string\"),\n                    \"subset\": datasets.Value(\"string\"),\n                    \"Samples\": [{\n                        \"id\": datasets.Value(\"string\"),\n                        \"sentence\": datasets.Value(\"string\"),\n                        \"label\": datasets.Value(\"string\"),\n                        \"ground_truth\": datasets.Value(\"string\")\n                    }],\n                    \"Instance\": {\n                        \"id\": datasets.Value(\"string\"),\n                        \"sentence\": datasets.Value(\"string\"),\n                        \"label\": datasets.Value(\"string\"),\n                        \"instruction\": datasets.Value(\"string\"),\n                        \"ground_truth\": datasets.Value(\"string\")\n                    }\n                }\n            ),\n            supervised_keys=None\n        )\n\n\n    def _split_generators(self, dl_manager):\n        \"\"\"Returns SplitGenerators.\"\"\"\n        if self.config.data_dir is None or self.config.task_configs is None:\n            logger.error(\"Please provide right input: data_dir or task_config_dir!\")\n\n        # split dir save datasets\n        # task config to specify train,dev,test\n        split_dir = self.config.data_dir\n        task_configs = self.config.task_configs\n\n        return [\n            datasets.SplitGenerator(\n                name=datasets.Split.TRAIN,\n                gen_kwargs={\n                    \"path\": split_dir,\n                    \"task_config\": task_configs['train'],\n                    \"max_num_instances_per_task\": self.config.max_num_instances_per_task,\n                    \"subset\": \"train\"\n                }),\n            datasets.SplitGenerator(\n                name=datasets.Split.VALIDATION,\n                gen_kwargs={\n                    \"path\": split_dir,\n                    \"task_config\": task_configs['dev'],\n                    \"max_num_instances_per_task\": self.config.max_num_instances_per_eval_task,\n                    \"subset\": \"dev\"\n                }),\n            datasets.SplitGenerator(\n                name=datasets.Split.TEST,\n                gen_kwargs={\n                    \"path\": split_dir,\n                    \"task_config\": task_configs['test'],\n                    \"max_num_instances_per_task\": None,  # default load total test samples to test\n                    \"subset\": \"test\"\n                }),\n        ]\n\n\n    def _load_dataset(self, dataset_path):\n        with open(dataset_path, encoding=\"utf-8\") as task_f:\n            s = task_f.read()\n            instances = json.loads(s)\n        \n        return instances\n    \n    def load_LongSeq_dataset(self, dataset_path, dataset_name, sampling_strategy, max_num_instances, subset):\n\n        data = self._load_dataset(dataset_path)\n        print(list(data.keys()))\n        input_mode='zeroshot'\n        definition = \"\"\n        if len(data[\"Definition\"]) > 0:\n            if input_mode=='fewshot' or input_mode=='zeroshot':\n                if isinstance(data[\"Definition\"], list):\n                    definition = data[\"Definition\"][0].strip() # TODO: should we use <Definition>?\n                else:\n                    definition = data[\"Definition\"].strip()\n                definition += \"\\n\"\n\n        sample_template = {\"Task\": \"CL\", \"Dataset\": dataset_name, \"Samples\": [], \"subset\": subset}\n\n        for idx, instance in enumerate(data['Instances']):\n            example = sample_template.copy()\n            instruction = \"\"\n            # add the input first.\n            instruction += \"{0}\"\n            instruction += \"\\n\"\n            instruction += \"Output: \"\n            pos_examples = []\n            if input_mode=='fewshot':\n                for idx, pos_example in enumerate(data[\"Positive Examples\"][:1]):\n                    pos_example_str = f\"Positive Example {idx+1} -\\n\"\n                    pos_example_str += f\"Input: {pos_example['input'].strip()}\"\n                    pos_example_str += \"\\n\"\n                    pos_example_str += f\"Output: {pos_example['output'].strip()}\"\n                    pos_example_str += \"\\n\" \n                    pos_examples.append(pos_example_str)\n\n            instruction = definition + \"\".join(pos_examples) + instruction\n\n            # print('-------------------')\n            # print(instruction)\n            # print('-------------------')\n\n            if isinstance(instance[\"output\"], list):\n                label=instance[\"output\"][random.randint(0, len(instance[\"output\"])-1)]\n            else:\n                label=instance[\"output\"]\n\n            example[\"Instance\"] = {\n                \"id\": str(idx),\n                \"sentence\": instance['input'],\n                \"label\": label,\n                \"ground_truth\": label,\n                \"instruction\": instruction\n            }\n\n            yield example\n\n    def load_SuperNI_dataset(self, dataset_path, labels_path, dataset_name, sampling_strategy, max_num_instances, subset):\n\n        data = self._load_dataset(dataset_path)\n        print(list(data.keys()))\n        input_mode='zeroshot'\n        definition = \"\"\n        if input_mode=='fewshot' or input_mode=='zeroshot':\n            if isinstance(data[\"Definition\"], list):\n                definition = \"Definition: \" + data[\"Definition\"][0].strip() # TODO: should we use <Definition>?\n            else:\n                definition = \"Definition: \" + data[\"Definition\"].strip()\n            definition += \"\\n\\n\"\n\n        sample_template = {\"Task\": \"CL\", \"Dataset\": dataset_name, \"Samples\": [], \"subset\": subset}\n\n        for idx, instance in enumerate(data['Instances']):\n            example = sample_template.copy()\n            instruction = \"\"\n            # add the input first.\n            if input_mode=='fewshot' or input_mode=='zeroshot':\n                instruction += \"Now complete the following example -\\n\"\n            instruction += \"Input: {0}\"\n            instruction += \"\\n\"\n            instruction += \"Output: \"\n            pos_examples = []\n            if input_mode=='fewshot':\n                for idx, pos_example in enumerate(data[\"Positive Examples\"][:1]):\n                    pos_example_str = f\"Positive Example {idx+1} -\\n\"\n                    pos_example_str += f\"Input: {pos_example['input'].strip()}\"\n                    pos_example_str += \"\\n\"\n                    pos_example_str += f\"Output: {pos_example['output'].strip()}\"\n                    pos_example_str += \"\\n\" \n                    pos_examples.append(pos_example_str)\n\n            instruction = definition + \"\".join(pos_examples) + instruction\n\n            # print('-------------------')\n            # print(instruction)\n            # print('-------------------')\n\n            if isinstance(instance[\"output\"], list):\n                label=instance[\"output\"][random.randint(0, len(instance[\"output\"])-1)]\n            else:\n                label=instance[\"output\"]\n\n            example[\"Instance\"] = {\n                \"id\": str(idx),\n                \"sentence\": instance['input'],\n                \"label\": label,\n                \"ground_truth\": label,\n                \"instruction\": instruction\n            }\n\n            yield example\n\n\n    def _generate_examples(self, path=None, task_config=None, max_num_instances_per_task=None, subset=None):\n        \"\"\"Yields examples.\"\"\"\n        logger.info(f\"Generating tasks from = {path}\")\n\n        for task in task_config:\n            if task == 'SuperNI':\n                load_func = self.load_SuperNI_dataset\n            elif task == \"Long_Sequence\":\n                load_func = self.load_LongSeq_dataset\n            else:\n                raise ValueError(\"Unsupport {} task, plz check {} task config!\".format(task, subset))\n\n            # load dataset\n            for dataset in task_config[task]:\n                ds_name = dataset[\"dataset name\"]\n                sampling_strategy = dataset.get(\"sampling strategy\", \"random\")\n                ds_path = os.path.join(path, task, ds_name, subset + '.json')\n                print(ds_path)\n                labels_path = None\n                assert os.path.exists(ds_path)\n\n                idx = -1\n                instances = []\n                for sample in load_func(ds_path, labels_path, ds_name, sampling_strategy, max_num_instances_per_task,\n                                        subset):\n                    idx += 1\n                    instances.append(sample)\n                    yield f\"{task}##{ds_path}##{idx}\", sample\n"}
{"type": "source_file", "path": "src/cl_collator.py", "content": "import logging\r\n\r\nimport torch\r\nfrom transformers.data.data_collator import *\r\n\r\n\r\nlogger = logging.getLogger(__name__)\r\n\r\nSUPPORTED_DECODER_MODELS = ['llama']\r\nSUPPORTED_SEQ2SEQ_MODELS = ['t5']\r\n\r\ndef check_model(model_name, supported_models):\r\n    for sup_model in supported_models:\r\n        if sup_model.lower() in model_name.lower():\r\n            return True\r\n\r\n    return False\r\n\r\ndef replace_sublist(lst, sublist, replacement):\r\n    n = len(lst)\r\n    m = len(sublist)\r\n    \r\n    for i in range(n - m + 1):\r\n        if lst[i:i+m] == sublist:\r\n            return lst[:i] + replacement + lst[i+m:]\r\n    \r\n    return lst\r\n\r\n@dataclass\r\nclass DataCollator:\r\n    tokenizer: PreTrainedTokenizerBase\r\n    model: Optional[Any] = None\r\n    padding: Union[bool, str, PaddingStrategy] = True\r\n    max_source_length: Optional[int] = None\r\n    max_target_length: Optional[int] = None\r\n    pad_to_multiple_of: Optional[int] = None\r\n    label_pad_token_id: int = -100\r\n    return_tensors: str = \"pt\"\r\n    add_task_name: bool = False\r\n    add_dataset_name: bool = False\r\n    add_instruction_replay: bool = True\r\n    common_dataset_name: str = None\r\n    text_only: bool = False\r\n    num_examples: int = 0\r\n    input_record_file: str = None\r\n\r\n    def __call__(self, batch, return_tensors=None):\r\n        if return_tensors is None:\r\n            return_tensors = self.return_tensors\r\n\r\n        model_name = self.model.config._name_or_path\r\n        # print(model_name)\r\n        if check_model(model_name, SUPPORTED_DECODER_MODELS):\r\n            model_inputs = self.decoder_call(batch, return_tensors)\r\n        elif check_model(model_name, SUPPORTED_SEQ2SEQ_MODELS):\r\n            model_inputs = self.seq2seq_call(batch, return_tensors)\r\n        else:\r\n            raise ValueError('Unsupport model {}!'.format(model_name))\r\n\r\n        return model_inputs\r\n\r\n    def get_instruction(self, instance):\r\n        # \"instructions \\n options \\n {0} \\n Answer: \"\r\n        instruction = instance['Instance'][\"instruction\"]\r\n        content = instance['Instance']['sentence']\r\n\r\n        # add task/ds prefix\r\n        prefix = ''\r\n        if self.add_task_name:\r\n            prefix += \"Task:\" + instance['Task'] + '\\n'\r\n        if self.add_dataset_name:\r\n            ds_name = self.common_dataset_name if self.common_dataset_name else instance['Dataset']\r\n            prefix = prefix + \"Dataset:\"\r\n            prefix = prefix + ds_name + '\\n' if prefix else instance['Dataset'] + '\\n'\r\n        if prefix:\r\n            instruction = prefix + instruction\r\n\r\n        # TODO, support few shot\r\n        # add few shot samples\r\n        samples = ''\r\n        if len(instance['Samples']) > 0:\r\n            raise Exception('Few shot is coming soon...')\r\n        if samples:\r\n            content = samples + content\r\n        # TODO, fix bug\r\n        if self.add_instruction_replay:\r\n            try:\r\n                instruction = instruction.format(content)\r\n            finally:\r\n                return instruction\r\n        else:\r\n            return instruction\r\n\r\n\r\n    def seq2seq_call(self, batch, return_tensors):\r\n        sources = []\r\n        labels = []\r\n\r\n        for instance in batch:\r\n            label = instance['Instance']['label']\r\n            labels.append(label)\r\n            instruction = self.get_instruction(instance)\r\n\r\n            source = instruction\r\n            tokenized_source = self.tokenizer(source)[\"input_ids\"]\r\n            if len(tokenized_source) <= self.max_source_length:\r\n                sources.append(source)\r\n            else:\r\n                sources.append(self.tokenizer.decode(tokenized_source[:self.max_source_length], skip_special_tokens=True))\r\n\r\n        # TODO, support online demo\r\n        if self.text_only:\r\n            model_inputs = {\"inputs\": sources, \"labels\": labels}\r\n        else:\r\n            model_inputs = self.tokenizer(\r\n                sources,\r\n                max_length=self.max_source_length,\r\n                padding=self.padding,\r\n                return_tensors=return_tensors,\r\n                truncation=True,\r\n                pad_to_multiple_of=self.pad_to_multiple_of\r\n            )\r\n            with self.tokenizer.as_target_tokenizer():\r\n                labels = self.tokenizer(\r\n                    labels,\r\n                    max_length=self.max_target_length,\r\n                    padding=self.padding,\r\n                    return_tensors=return_tensors,\r\n                    truncation=True,\r\n                    pad_to_multiple_of=self.pad_to_multiple_of\r\n                )\r\n            label_mask = labels[\"attention_mask\"].bool()\r\n            model_inputs[\"labels\"] = labels[\"input_ids\"].masked_fill(~label_mask, self.label_pad_token_id)\r\n\r\n            # prepare decoder_input_ids\r\n            if self.model is not None:\r\n                decoder_input_ids = self.model.prepare_decoder_input_ids_from_labels(labels=model_inputs[\"labels\"])\r\n                model_inputs[\"decoder_input_ids\"] = decoder_input_ids\r\n\r\n            self._save_samples(model_inputs, sources, labels)\r\n\r\n        return model_inputs\r\n\r\n    def decoder_call(self, batch, return_tensors):\r\n        self.tokenizer.padding_side = 'left'\r\n        input_ids= []\r\n        attention_mask= []\r\n        input_ids_wo_label = []\r\n        labels= []\r\n\r\n        for instance in batch:\r\n            label = instance['Instance']['label']\r\n            instruction = self.get_instruction(instance)\r\n\r\n            # add bos and eos\r\n            task_input = instruction\r\n            label = label + self.tokenizer.eos_token\r\n\r\n            tokenized_input = self.tokenizer(task_input, add_special_tokens=False)[\"input_ids\"]\r\n            if len(tokenized_input)>self.max_source_length:\r\n                tokenized_input=tokenized_input[:self.max_source_length]\r\n\r\n            tokenized_label = self.tokenizer(label, add_special_tokens=False)[\"input_ids\"]\r\n            if len(tokenized_label)>self.max_target_length:\r\n                tokenized_label=tokenized_label[:self.max_target_length]\r\n\r\n            # (input) for inference, (input + label) for training\r\n            if instance['subset'] in ['dev', 'test']:\r\n                input_ids.append(tokenized_input)\r\n                input_ids_wo_label.append(tokenized_input)\r\n                labels.append([self.label_pad_token_id]*len(tokenized_input))\r\n            else:\r\n                input_ids.append(tokenized_input+tokenized_label)\r\n                input_ids_wo_label.append(tokenized_input)\r\n                labels.append([self.label_pad_token_id]*len(tokenized_input)+tokenized_label)\r\n        \r\n        inputs_length=[len(i) for i in input_ids]\r\n        inputs_length_wo_label = [len(i) for i in input_ids_wo_label]\r\n\r\n        max_length=max(inputs_length)\r\n        max_length_wo_label=max(inputs_length_wo_label)\r\n        for i,(l,l_wo) in enumerate(zip(inputs_length, inputs_length_wo_label)):\r\n            input_ids[i]=[self.tokenizer.pad_token_id]*(max_length-l) + input_ids[i]\r\n            labels[i]=[self.label_pad_token_id]*(max_length-l) + labels[i]\r\n            input_ids_wo_label[i] = [self.tokenizer.pad_token_id]*(max_length_wo_label-l_wo) + input_ids_wo_label[i]\r\n            attention_mask.append([0]*(max_length-l) + [1]*l)\r\n\r\n        input_ids=torch.tensor(input_ids)\r\n        attention_mask=torch.tensor(attention_mask)\r\n        labels=torch.tensor(labels)\r\n        input_ids_wo_label=torch.tensor(input_ids_wo_label)\r\n        model_inputs={\r\n            'input_ids': input_ids,\r\n            'attention_mask': attention_mask,\r\n            'labels': labels,\r\n            'input_ids_wo_label': input_ids_wo_label,\r\n        }\r\n        return model_inputs\r\n\r\n    def _save_samples(self, model_inputs, sources, labels):\r\n        if not self.input_record_file:\r\n            return\r\n\r\n        loss_label = []\r\n        if hasattr(model_inputs, 'loss_mask'):\r\n            for loss, id in zip(model_inputs.loss_mask, model_inputs.input_ids):\r\n                loss_label.append(self.tokenizer.decode((loss * id).view(-1).int()))\r\n\r\n            with open(self.input_record_file, 'a+', encoding='utf-8') as f:\r\n                for text, label, mask_label in zip(sources, labels, loss_label):\r\n                    f.write(text+'\\n')\r\n                    f.write(label + '\\n')\r\n                    f.write(mask_label+'\\n\\n')\r\n        else:\r\n            with open(self.input_record_file, 'a+', encoding='utf-8') as f:\r\n                for text, label in zip(sources, labels['input_ids']):\r\n                    f.write(text + '\\n')\r\n                    f.write(self.tokenizer.decode(label, clean_up_tokenization_spaces=False) + '\\n')"}
{"type": "source_file", "path": "pseudo_data/src/rouge/rouge.py", "content": "# coding=utf-8\n# Copyright 2022 The Google Research Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nr\"\"\"Main routine to calculate ROUGE scores across text files.\n\nDesigned to replicate scores computed by the ROUGE perl implementation as\nclosely as possible.\n\nOutput is a text file in CSV format.\n\nSample usage:\n\nrouge ---rouge_types=rouge1,rouge2,rougeL \\\n    --target_filepattern=*.targets \\\n    --prediction_fliepattern=*.decodes \\\n    --output_filename=scores.csv \\\n    --use_stemmer\n\nWhich is equivalent to calling the perl ROUGE script as:\n\nROUGE-1.5.5.pl -m -e ./data -n 2 -a /tmp/rouge/settings.xml\n\nWhere settings.xml provides target and decode text.\n\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl import app\nfrom absl import flags\nfrom rouge import io\nfrom rouge import rouge_scorer\nfrom rouge import scoring\n\nflags.DEFINE_string(\"target_filepattern\", None,\n                    \"Files containing target text.\")\nflags.DEFINE_string(\"prediction_filepattern\", None,\n                    \"Files containing prediction text.\")\nflags.DEFINE_string(\"output_filename\", None,\n                    \"File in which to write calculated ROUGE scores as a CSV.\")\nflags.DEFINE_string(\"delimiter\", \"\\n\",\n                    \"Record delimiter  in files.\")\nflags.DEFINE_list(\"rouge_types\", [\"rouge1\", \"rouge2\", \"rougeL\"],\n                  \"List of ROUGE types to calculate.\")\nflags.DEFINE_boolean(\"use_stemmer\", False,\n                     \"Whether to use Porter stemmer to remove common suffixes.\")\nflags.DEFINE_boolean(\"aggregate\", True,\n                     \"Write aggregates if this is set to True\")\nflags.DEFINE_boolean(\"split_summaries\", False,\n                     (\"Whether to split references and candidates into\"\n                      \" sentences before computing RougeLsum.\"))\n\nFLAGS = flags.FLAGS\n\n\ndef main(argv):\n  if len(argv) > 1:\n    raise app.UsageError(\"Too many command-line arguments.\")\n  scorer = rouge_scorer.RougeScorer(\n      FLAGS.rouge_types,\n      use_stemmer=FLAGS.use_stemmer,\n      split_summaries=FLAGS.split_summaries)\n  aggregator = scoring.BootstrapAggregator() if FLAGS.aggregate else None\n  io.compute_scores_and_write_to_csv(\n      FLAGS.target_filepattern,\n      FLAGS.prediction_filepattern,\n      FLAGS.output_filename,\n      scorer,\n      aggregator,\n      delimiter=FLAGS.delimiter)\n\n\nif __name__ == \"__main__\":\n  flags.mark_flag_as_required(\"target_filepattern\")\n  flags.mark_flag_as_required(\"prediction_filepattern\")\n  flags.mark_flag_as_required(\"output_filename\")\n  app.run(main)\n"}
{"type": "source_file", "path": "src/cl_trainer.py", "content": "import torch\nfrom transformers import GenerationConfig\nfrom transformers.trainer_seq2seq import Seq2SeqTrainer\nfrom transformers.trainer import *\nfrom transformers.trainer_callback import TrainerCallback\nimport numpy as np\n\nfrom cl_collator import SUPPORTED_DECODER_MODELS, check_model\nfrom cl_dataset import ANSWER_PREFIX\n\n\ndef skip_instructions(model, predictions_ids, tokenizer, ignore_idx=-100):\n    predictions_ids = np.where(predictions_ids == ignore_idx, tokenizer.pad_token_id, predictions_ids)\n\n    predictions = tokenizer.batch_decode(\n        predictions_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True\n    )\n\n    final_predictions = []\n    if check_model(model.config._name_or_path, SUPPORTED_DECODER_MODELS):\n        for pred in predictions:\n            if ANSWER_PREFIX in pred:\n                splits = pred.split(ANSWER_PREFIX)\n                final_predictions.append(splits[-1].strip())\n            else:\n                final_predictions.append('')\n    else:\n        final_predictions = predictions\n\n    return final_predictions\n\ndef create_memory_replay_generators(task, task_list, replay_data_dict, split='train_mem'): # creating previous tasks memory buffers\n    print('Creating generators for previous tasks ...')\n    tasks_to_generators = {}\n    curr_task_num = task_list.index(task)\n    for idx in np.arange(curr_task_num):\n        prev_task = task_list[idx]\n        tasks_to_generators[prev_task] = iter(replay_data_dict[prev_task])\n    return tasks_to_generators\n\nclass DenserEvalCallback(TrainerCallback):\n\n    def on_step_end(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs):\n\n        log_eval_steps = [1, 50, 100, 200]\n\n        # Log\n        if args.logging_strategy == IntervalStrategy.STEPS and state.global_step in log_eval_steps:\n            control.should_log = True\n\n        # Evaluate\n        if args.evaluation_strategy == IntervalStrategy.STEPS and state.global_step in log_eval_steps:\n            control.should_evaluate = True\n\n        # Save\n        # if args.save_strategy\n\n        return control\n\n\nclass Trainer(Seq2SeqTrainer):\n\n    def __init__(self, model, args, train_dataset, cur_task_id, task_order, data_collator_replay=None, replay_dataset_dict=None, replay_label_dict=None, eval_dataset=None, tokenizer=None, data_collator=None, compute_metrics=None, callbacks=None):\n        super().__init__(model=model, args=args, train_dataset=train_dataset, eval_dataset=eval_dataset, tokenizer=tokenizer, data_collator=data_collator, compute_metrics=compute_metrics, callbacks=callbacks)\n\n        self.data_collator_replay = data_collator_replay\n        self.replay_dataset_dict = replay_dataset_dict\n        self.replay_label_dict = replay_label_dict\n        self.task_order = task_order\n        self.cur_task_id = cur_task_id\n\n        if self.args.data_replay_freq != -1:\n            seed = self.args.data_seed if self.args.data_seed is not None else self.args.seed\n            self.replay_dataloader_dict = {}\n            generator = torch.Generator()\n            generator.manual_seed(seed)\n            if replay_dataset_dict is not None:\n                for dataset_name, dataset in self.replay_dataset_dict.items():\n                    train_sampler = RandomSampler(dataset, generator=generator)\n                    self.replay_dataloader_dict[dataset_name] = DataLoader(\n                        dataset,\n                        batch_size=self._train_batch_size,\n                        sampler=train_sampler,\n                        collate_fn=self.data_collator_replay,\n                        drop_last=self.args.dataloader_drop_last,\n                        num_workers=self.args.dataloader_num_workers,\n                        pin_memory=False,\n                        worker_init_fn=seed_worker)\n            self.replay_iterator_dict = create_memory_replay_generators(task_order[cur_task_id], task_order, self.replay_dataloader_dict)\n\n    def training_step(self, model: nn.Module, inputs: Dict[str, Union[torch.Tensor, Any]]) -> torch.Tensor:\n        \"\"\"\n        Perform a training step on a batch of inputs.\n\n        Subclass and override to inject custom behavior.\n\n        Args:\n            model (`nn.Module`):\n                The model to train.\n            inputs (`Dict[str, Union[torch.Tensor, Any]]`):\n                The inputs and targets of the model.\n\n                The dictionary will be unpacked before being fed to the model. Most models expect the targets under the\n                argument `labels`. Check your model's documentation for all accepted arguments.\n\n        Return:\n            `torch.Tensor`: The tensor with training loss on this batch.\n        \"\"\"\n        model.train()\n        \n        inputs = self._prepare_inputs(inputs)\n\n        if is_sagemaker_mp_enabled():\n            loss_mb = smp_forward_backward(model, inputs, self.args.gradient_accumulation_steps)\n            return loss_mb.reduce_mean().detach().to(self.args.device)\n        \n        with self.compute_loss_context_manager():\n            loss = self.compute_loss(model, inputs)\n\n        if self.args.n_gpu > 1:\n            loss = loss.mean()  # mean() to average on multi-gpu parallel training\n\n        if self.args.gradient_accumulation_steps > 1 and not self.is_deepspeed_enabled:\n            # deepspeed handles loss scaling by gradient_accumulation_steps in its `backward`\n            loss = loss / self.args.gradient_accumulation_steps\n        \n        if self.do_grad_scaling:\n            self.scaler.scale(loss).backward()\n        elif self.use_apex:\n            with amp.scale_loss(loss, self.optimizer) as scaled_loss:\n                scaled_loss.backward()\n        elif self.is_deepspeed_enabled:\n            # loss gets scaled under gradient_accumulation_steps in deepspeed\n            self.accelerator.backward(loss)\n        else:\n            loss.backward()\n        \n        if self.state.global_step > self.args.replay_after_n_epoch*self.args.step_per_epoch and self.args.data_replay_freq != -1 and self.state.global_step % self.args.data_replay_freq == 0:\n            for item in self.replay_iterator_dict.keys():\n                generator_mem1 = self.replay_iterator_dict[item]\n                try:\n                    # Samples the batch\n                    b = next(generator_mem1)\n                except StopIteration:\n                    generator_mem1 = iter(self.replay_dataloader_dict[item])\n\n                    self.replay_iterator_dict[item] = generator_mem1\n                    b = next(generator_mem1)\n\n                replay_task_id = self.task_order.index(item)\n                b[\"replay_labels\"] = self.replay_label_dict[self.task_order[replay_task_id]]\n                \n                replay_inputs = self._prepare_inputs(b)\n                with self.compute_loss_context_manager():\n                    kl_loss = self.args.kl_ratio * self.model.memory_replay(replay_inputs[\"input_ids\"], replay_inputs[\"replay_labels\"])\n\n                if self.args.n_gpu > 1:\n                    kl_loss = kl_loss.mean()  # mean() to average on multi-gpu parallel trainin\n        \n                if self.do_grad_scaling:\n                    self.scaler.scale(kl_loss).backward()\n                elif self.use_apex:\n                    with amp.scale_loss(kl_loss, self.optimizer) as scaled_loss:\n                        scaled_loss.backward()\n                elif self.is_deepspeed_enabled:\n                    self.accelerator.backward(kl_loss)\n                else:\n                    kl_loss.backward()\n\n        return loss.detach()\n    \n    def create_optimizer_and_scheduler(self, num_training_steps: int):\n        \"\"\"\n        Setup the optimizer and the learning rate scheduler.\n\n        We provide a reasonable default that works well. If you want to use something else, you can pass a tuple in the\n        Trainer's init through `optimizers`, or subclass and override this method (or `create_optimizer` and/or\n        `create_scheduler`) in a subclass.\n        \"\"\"\n        self.create_optimizer()\n        if IS_SAGEMAKER_MP_POST_1_10 and smp.state.cfg.fp16:\n            # If smp >= 1.10 and fp16 is enabled, we unwrap the optimizer\n            optimizer = self.optimizer.optimizer\n        else:\n            optimizer = self.optimizer\n        self.create_scheduler(num_training_steps=num_training_steps, optimizer=optimizer)\n\n    def create_optimizer(self):\n        \"\"\"\n        Setup the optimizer.\n\n        We provide a reasonable default that works well. If you want to use something else, you can pass a tuple in the\n        Trainer's init through `optimizers`, or subclass and override this method in a subclass.\n        \"\"\"\n        opt_model = self.model_wrapped if is_sagemaker_mp_enabled() else self.model\n        \n        if self.optimizer is None:\n            if self.args.attn_lr == 0:\n                print(\"Using Same Learning Rate for All Modules\")\n                decay_parameters = get_parameter_names(opt_model, ALL_LAYERNORM_LAYERS)\n                decay_parameters = [name for name in decay_parameters if \"bias\" not in name]\n                optimizer_grouped_parameters = [\n                    {\n                        \"params\": [\n                            p for n, p in opt_model.named_parameters() if (n in decay_parameters and p.requires_grad)\n                        ],\n                        \"weight_decay\": self.args.weight_decay,\n                    },\n                    {\n                        \"params\": [\n                            p for n, p in opt_model.named_parameters() if (n not in decay_parameters and p.requires_grad)\n                        ],\n                        \"weight_decay\": 0.0,\n                    },\n                ]\n            else:\n                print(\"Using Different Learning Rates for Different Modules\")\n                decay_parameters = get_parameter_names(opt_model, ALL_LAYERNORM_LAYERS)\n                decay_parameters = [name for name in decay_parameters if \"bias\" not in name]\n                \n                param_no_decay = [p for n, p in opt_model.named_parameters() if n not in decay_parameters and p.requires_grad]\n                \n                resett_param_with_decay = [p for n, p in opt_model.named_parameters() if \"trans_input\" in n and n in decay_parameters and p.requires_grad]\n                other_param_with_decay = [p for n, p in opt_model.named_parameters() if \"trans_input\" not in n and n in decay_parameters and p.requires_grad]\n                optimizer_grouped_parameters = [\n                    {\n                        \"params\": other_param_with_decay,\n                        \"weight_decay\": self.args.weight_decay,\n                        \"lr\": self.args.learning_rate\n                    },\n                    {\n                        \"params\": resett_param_with_decay,\n                        \"weight_decay\": self.args.weight_decay,\n                        \"lr\": self.args.attn_lr\n                    },\n                    {\n                        \"params\": param_no_decay,\n                        \"weight_decay\": 0.0,\n                        \"lr\": self.args.learning_rate\n                    },\n                ]\n\n            optimizer_cls, optimizer_kwargs = Trainer.get_optimizer_cls_and_kwargs(self.args)\n\n            if self.sharded_ddp == ShardedDDPOption.SIMPLE:\n                self.optimizer = OSS(\n                    params=optimizer_grouped_parameters,\n                    optim=optimizer_cls,\n                    **optimizer_kwargs,\n                )\n            else:\n                self.optimizer = optimizer_cls(optimizer_grouped_parameters, **optimizer_kwargs)\n                if optimizer_cls.__name__ == \"Adam8bit\":\n                    import bitsandbytes\n\n                    manager = bitsandbytes.optim.GlobalOptimManager.get_instance()\n\n                    skipped = 0\n                    for module in opt_model.modules():\n                        if isinstance(module, nn.Embedding):\n                            skipped += sum({p.data_ptr(): p.numel() for p in module.parameters()}.values())\n                            logger.info(f\"skipped {module}: {skipped/2**20}M params\")\n                            manager.register_module_override(module, \"weight\", {\"optim_bits\": 32})\n                            logger.debug(f\"bitsandbytes: will optimize {module} in fp32\")\n                    logger.info(f\"skipped: {skipped/2**20}M params\")\n\n        if is_sagemaker_mp_enabled():\n            self.optimizer = smp.DistributedOptimizer(self.optimizer)\n\n        return self.optimizer\n\n    def evaluation_loop(\n        self,\n        dataloader: DataLoader,\n        description: str,\n        prediction_loss_only: Optional[bool] = None,\n        ignore_keys: Optional[List[str]] = None,\n        metric_key_prefix: str = \"eval\",\n    ) -> EvalLoopOutput:\n        \"\"\"\n        Prediction/evaluation loop, shared by `Trainer.evaluate()` and `Trainer.predict()`.\n\n        Works both with or without labels.\n        \"\"\"\n        args = self.args\n\n        prediction_loss_only = prediction_loss_only if prediction_loss_only is not None else args.prediction_loss_only\n\n        # if eval is called w/o train init deepspeed here\n        if args.deepspeed and not self.is_deepspeed_enabled:\n\n            # XXX: eval doesn't have `resume_from_checkpoint` arg but we should be able to do eval\n            # from the checkpoint eventually\n            deepspeed_engine, _, _ = deepspeed_init(\n                self, num_training_steps=0, resume_from_checkpoint=None, # inference=True\n            )\n            self.model = deepspeed_engine.module\n            self.model_wrapped = deepspeed_engine\n            self.deepspeed = deepspeed_engine\n\n        model = self._wrap_model(self.model, training=False)\n\n        # if full fp16 or bf16 eval is wanted and this ``evaluation`` or ``predict`` isn't called\n        # while ``train`` is running, cast it to the right dtype first and then put on device\n        if not self.is_in_train:\n            if args.fp16_full_eval:\n                model = model.to(dtype=torch.float16, device=args.device)\n            elif args.bf16_full_eval:\n                model = model.to(dtype=torch.bfloat16, device=args.device)\n\n        batch_size = dataloader.batch_size\n\n        logger.info(f\"***** Running {description} *****\")\n        if has_length(dataloader.dataset):\n            logger.info(f\"  Num examples = {self.num_examples(dataloader)}\")\n        else:\n            logger.info(\"  Num examples: Unknown\")\n        logger.info(f\"  Batch size = {batch_size}\")\n\n        model.eval()\n\n        self.callback_handler.eval_dataloader = dataloader\n        # Do this before wrapping.\n        eval_dataset = dataloader.dataset\n\n        if args.past_index >= 0:\n            self._past = None\n\n        # Initialize containers\n        # losses/preds/labels on GPU/TPU (accumulated for eval_accumulation_steps)\n        losses_host = None\n        preds_host = None\n        labels_host = None\n        # losses/preds/labels on CPU (final containers)\n        all_losses = None\n        all_preds = None\n        all_labels = None\n        # Will be useful when we have an iterable dataset so don't know its length.\n\n        observed_num_examples = 0\n        # Main evaluation loop\n        for step, inputs in enumerate(dataloader):\n            # Update the observed num examples\n            observed_batch_size = find_batch_size(inputs)\n            if observed_batch_size is not None:\n                observed_num_examples += observed_batch_size\n                # For batch samplers, batch_size is not known by the dataloader in advance.\n                if batch_size is None:\n                    batch_size = observed_batch_size\n\n            # Prediction step\n            loss, logits, labels = self.prediction_step(model, inputs, prediction_loss_only, ignore_keys=ignore_keys)\n\n            # Update containers on host\n            if loss is not None:\n                losses = self._nested_gather(loss.repeat(batch_size))\n                losses_host = losses if losses_host is None else torch.cat((losses_host, losses), dim=0)\n            if labels is not None:\n                labels = self._pad_across_processes(labels)\n                labels = self._nested_gather(labels)\n                labels_host = labels if labels_host is None else nested_concat(labels_host, labels, padding_index=-100)\n            if logits is not None:\n                logits = self._pad_across_processes(logits)\n                logits = self._nested_gather(logits)\n                if self.preprocess_logits_for_metrics is not None:\n                    logits = self.preprocess_logits_for_metrics(logits, labels)\n                preds_host = logits if preds_host is None else nested_concat(preds_host, logits, padding_index=-100)\n            self.control = self.callback_handler.on_prediction_step(args, self.state, self.control)\n\n            # Gather all tensors and put them back on the CPU if we have done enough accumulation steps.\n            if args.eval_accumulation_steps is not None and (step + 1) % args.eval_accumulation_steps == 0:\n                if losses_host is not None:\n                    losses = nested_numpify(losses_host)\n                    all_losses = losses if all_losses is None else np.concatenate((all_losses, losses), axis=0)\n                if preds_host is not None:\n                    logits = nested_numpify(preds_host)\n                    all_preds = logits if all_preds is None else nested_concat(all_preds, logits, padding_index=-100)\n                if labels_host is not None:\n                    labels = nested_numpify(labels_host)\n                    all_labels = (\n                        labels if all_labels is None else nested_concat(all_labels, labels, padding_index=-100)\n                    )\n\n                # Set back to None to begin a new accumulation\n                losses_host, preds_host, labels_host = None, None, None\n\n        if args.past_index and hasattr(self, \"_past\"):\n            # Clean the state at the end of the evaluation loop\n            delattr(self, \"_past\")\n\n        # Gather all remaining tensors and put them back on the CPU\n        if losses_host is not None:\n            losses = nested_numpify(losses_host)\n            all_losses = losses if all_losses is None else np.concatenate((all_losses, losses), axis=0)\n        if preds_host is not None:\n            logits = nested_numpify(preds_host)\n            all_preds = logits if all_preds is None else nested_concat(all_preds, logits, padding_index=-100)\n        if labels_host is not None:\n            labels = nested_numpify(labels_host)\n            all_labels = labels if all_labels is None else nested_concat(all_labels, labels, padding_index=-100)\n\n        # Number of samples\n        if has_length(eval_dataset):\n            num_samples = len(eval_dataset)\n        # The instance check is weird and does not actually check for the type, but whether the dataset has the right\n        # methods. Therefore we need to make sure it also has the attribute.\n        elif isinstance(eval_dataset, IterableDatasetShard) and hasattr(eval_dataset, \"num_examples\"):\n            num_samples = eval_dataset.num_examples\n        else:\n            num_samples = observed_num_examples\n\n        # Number of losses has been rounded to a multiple of batch_size and in a distributed training, the number of\n        # samplers has been rounded to a multiple of batch_size, so we truncate.\n        if all_losses is not None:\n            all_losses = all_losses[:num_samples]\n        if all_preds is not None:\n            all_preds = nested_truncate(all_preds, num_samples)\n        if all_labels is not None:\n            all_labels = nested_truncate(all_labels, num_samples)\n\n        # Metrics!\n        if self.compute_metrics is not None and all_preds is not None and all_labels is not None:\n            metrics = self.compute_metrics(dataset=eval_dataset, preds=all_preds, save_prefix=metric_key_prefix)\n        else:\n            metrics = {}\n\n        metrics[\"global_step\"] = self.state.global_step\n\n        # To be JSON-serializable, we need to remove numpy types or zero-d tensors\n        metrics = denumpify_detensorize(metrics)\n\n        if all_losses is not None:\n            metrics[f\"{metric_key_prefix}_loss\"] = all_losses.mean().item()\n\n        # Prefix all keys with metric_key_prefix + '_'\n        for key in list(metrics.keys()):\n            if not key.startswith(f\"{metric_key_prefix}_\"):\n                metrics[f\"{metric_key_prefix}_{key}\"] = metrics.pop(key)\n\n        return EvalLoopOutput(predictions=all_preds, label_ids=all_labels, metrics=metrics, num_samples=num_samples)\n\n\n    def prediction_step(\n        self,\n        model: nn.Module,\n        inputs: Dict[str, Union[torch.Tensor, Any]],\n        prediction_loss_only: bool,\n        ignore_keys: Optional[List[str]] = None,\n    ) -> Tuple[Optional[float], Optional[torch.Tensor], Optional[torch.Tensor]]:\n        \"\"\"\n        Perform an evaluation step on `model` using `inputs`.\n\n        Subclass and override to inject custom behavior.\n\n        Args:\n            model (`nn.Module`):\n                The model to evaluate.\n            inputs (`Dict[str, Union[torch.Tensor, Any]]`):\n                The inputs and targets of the model.\n\n                The dictionary will be unpacked before being fed to the model. Most models expect the targets under the\n                argument `labels`. Check your model's documentation for all accepted arguments.\n            prediction_loss_only (`bool`):\n                Whether or not to return the loss only.\n\n        Return:\n            Tuple[Optional[float], Optional[torch.Tensor], Optional[torch.Tensor]]: A tuple with the loss, logits and\n            labels (each being optional).\n        \"\"\"\n\n        if not self.args.predict_with_generate or prediction_loss_only:\n            return super().prediction_step(\n                model, inputs, prediction_loss_only=prediction_loss_only, ignore_keys=ignore_keys\n            )\n\n        has_labels = \"labels\" in inputs\n        inputs = self._prepare_inputs(inputs)\n\n        # XXX: adapt synced_gpus for fairscale as well\n        # gen_kwargs = self._gen_kwargs\n        if hasattr(self.model, \"encoder\") and self.model.encoder.main_input_name != self.model.main_input_name:\n            # T5 generation config\n            gen_kwargs = {\n                \"max_new_tokens\": 50,\n                \"num_beams\": 1,\n                \"repetition_penalty\": 1.0,\n                \"decoder_start_token_id\": 0,\n                \"eos_token_id\": 1,\n                \"pad_token_id\": 0,\n            }\n            gen_kwargs[\"synced_gpus\"] = False\n        else:\n            if inputs.get(\"input_ids_wo_label\", None) is not None:\n                # LLaMA-2 generation config\n                gen_kwargs = {\n                    \"bos_token_id\": 1,\n                    \"max_new_tokens\": 50,\n                    \"num_beams\": 1,\n                    \"temperature\": 1.0,\n                    \"repetition_penalty\": 1.0,\n                    \"eos_token_id\": 2,\n                    \"pad_token_id\": 1,\n                }\n            else:\n                # T5 generation config\n                gen_kwargs = {\n                    \"max_new_tokens\": 50,\n                    \"num_beams\": 1,\n                    \"repetition_penalty\": 1.0,\n                    \"decoder_start_token_id\": 0,\n                    \"eos_token_id\": 1,\n                    \"pad_token_id\": 0,\n                }\n                \n            gen_kwargs[\"synced_gpus\"] = False\n\n        if \"attention_mask\" in inputs:\n            gen_kwargs[\"attention_mask\"] = inputs.get(\"attention_mask\", None)\n\n        generation_config = GenerationConfig(**gen_kwargs)\n\n        # prepare generation inputs\n        # some encoder-decoder models can have varying encder's and thus\n        # varying model input names\n        if hasattr(self.model, \"encoder\") and self.model.encoder.main_input_name != self.model.main_input_name:\n            generation_inputs = inputs[self.model.encoder.main_input_name]\n            \n            generated_tokens = self.model.generate(\n                input_ids=generation_inputs, \n                generation_config=generation_config,\n            )\n        else:\n            generation_inputs = inputs[self.model.main_input_name]\n\n            if inputs.get(\"input_ids_wo_label\", None) is not None:\n                generated_tokens = self.model.generate(\n                    input_ids=generation_inputs,\n                    input_ids_wo_label=inputs[\"input_ids_wo_label\"],\n                    generation_config=generation_config,\n                )\n            \n            else:\n                generated_tokens = self.model.generate(\n                    input_ids=generation_inputs,\n                    generation_config=generation_config,\n                )\n\n        bs, source_len = inputs['input_ids'].shape\n        # in case the batch is shorter than max length, the output should be padded\n        if check_model(self.model.config._name_or_path, SUPPORTED_DECODER_MODELS):\n            max_length = source_len + gen_kwargs[\"max_new_tokens\"]\n        else:\n            max_length = gen_kwargs[\"max_new_tokens\"]\n\n        if generated_tokens.shape[-1] < max_length:\n            generated_tokens = self._pad_tensors_to_max_len(generated_tokens, max_length)\n\n        with torch.no_grad():\n            if has_labels:\n                with self.autocast_smart_context_manager():\n                    outputs = model(**inputs)\n                if self.label_smoother is not None:\n                    loss = self.label_smoother(outputs, inputs[\"labels\"]).mean().detach()\n                else:\n                    loss = (outputs[\"loss\"] if isinstance(outputs, dict) else outputs[0]).mean().detach()\n            else:\n                loss = None\n\n        if self.args.prediction_loss_only:\n            return (loss, None, None)\n\n        if has_labels:\n            labels = inputs[\"labels\"]\n            if labels.shape[-1] < gen_kwargs[\"max_new_tokens\"]:\n                labels = self._pad_tensors_to_max_len(labels, gen_kwargs[\"max_new_tokens\"])\n        else:\n            labels = None\n\n        return (loss, generated_tokens, labels)\n"}
