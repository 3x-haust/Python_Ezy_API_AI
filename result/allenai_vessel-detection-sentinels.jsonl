{"repo_info": {"repo_name": "vessel-detection-sentinels", "repo_owner": "allenai", "repo_url": "https://github.com/allenai/vessel-detection-sentinels"}}
{"type": "test_file", "path": "tests/__init__.py", "content": ""}
{"type": "test_file", "path": "tests/test_geom.py", "content": "import unittest\nimport math\n\nfrom src.utils import geom\n\n\nclass TestDegreesLonPerMeter(unittest.TestCase):\n\n    def test_zero_latitude(self):\n        self.assertAlmostEqual(geom.degrees_lon_per_meter(0), geom.LON_RANGE/(2*math.pi*geom.R_EARTH_M))\n\n    def test_negative_latitude(self):\n        lat = -math.pi/4  # -45 degrees in radians\n        self.assertAlmostEqual(geom.degrees_lon_per_meter(lat), geom.degrees_lon_per_meter(-lat))\n"}
{"type": "source_file", "path": "src/data/image.py", "content": "import datetime\nimport glob\nimport json\nimport logging\nimport multiprocessing\nimport os\nimport sys\nimport typing as t\n\nimport numpy as np\nimport skimage.io\nimport torch\n\nfrom src.data.retrieve import RetrieveImage\nfrom src.data.warp import warp\n\n# Configure logger\nlogger = logging.getLogger(\"prepare_scenes\")\nlogger.setLevel(logging.DEBUG)\nformatter = logging.Formatter(\"%(asctime)s (%(levelname)s): %(message)s\")\nstream_handler = logging.StreamHandler(sys.stdout)\nstream_handler.setLevel(logging.DEBUG)\nstream_handler.setFormatter(formatter)\nlogger.addHandler(stream_handler)\n\nSENTINEL_1_REQUIRED_CHANNEL_TYPES = [\"vh\", \"vv\"]\n\n# Map Sentinel-2 channels according to DB naming convention\n# to raw imagery path naming convention, and channel count.\n# Unlike Sentinel-1, actual channels used by model are specified\n# via config that can be altered at runtime.\nSENTINEL_2_CHANNEL_MAPPING = {\"tci\": {\"path_abbrev\": \"TCI\", \"count\": 3}, \"b08\": {\"path_abbrev\": \"B08\", \"count\": 1},\n                              \"b11\": {\"path_abbrev\": \"B11\", \"count\": 1}, \"b12\": {\"path_abbrev\": \"B12\", \"count\": 1}}\nSUPPORTED_IMAGERY_CATALOGS = [\"sentinel1\", \"sentinel2\"]\n\n\nclass InvalidDataError(Exception):\n    pass\n\n\nclass InvalidConfigError(Exception):\n    pass\n\n\nclass Channels(object):\n    def __init__(self, channels):\n        \"\"\"\n        Given a (JSON-decoded) list of fChannel, creates a Channels object.\n        \"\"\"\n        self.channels = channels\n\n    def __len__(self):\n        return len(self.channels)\n\n    def __getitem__(self, idx):\n        return self.channels[idx]\n\n    def count(self):\n        return sum([channel[\"Count\"] for channel in self.channels])\n\n    def flatten(self):\n        \"\"\"\n        For example, [tci, b5] -> ['tci-0', 'tci-1', 'tci-2', 'b5']\n        \"\"\"\n        flat_list = []\n        for channel in self.channels:\n            if channel[\"Count\"] > 1:\n                for i in range(channel[\"Count\"]):\n                    flat_list.append(\"{}-{}\".format(channel[\"Name\"], i))\n            else:\n                flat_list.append(channel[\"Name\"])\n        return flat_list\n\n    def with_ranges(self):\n        l = []\n        cur_idx = 0\n        for channel in self.channels:\n            l.append((channel, (cur_idx, cur_idx + channel[\"Count\"])))\n            cur_idx += channel[\"Count\"]\n        return l\n\n\ndef warp_func(job: list):\n    \"\"\"Perform a single image warp job.\n\n    Parameters\n    ----------\n    job: list\n        List specifying job params.\n\n    Returns\n    -------\n    : ImageInfo\n        Class containing warped image metadata.\n    \"\"\"\n    retrieve_image, scene_id, channel_name, src_path, dst_path = job\n    return warp(retrieve_image, src_path, dst_path, projection=\"epsg:3857\")\n\n\ndef prepare_scenes(\n    raw_path: str,\n    scratch_path: str,\n    scene_id: str,\n    historical1: t.Optional[str],\n    historical2: t.Optional[str],\n    catalog: str,\n    cat_path: str,\n    base_path: str,\n    device: torch.device,\n    detector_model_dir: str,\n    postprocess_model_dir: str\n) -> None:\n    \"\"\"Extract and warp scenes, then save numpy array with scene info.\n\n    Parameters\n    ----------\n    raw_path: str\n        Path to directory containing raw inference image and (optionally) historical overlaps.\n\n    scratch_path: str\n        Path to directory where intermediate files can be populated.\n\n    scene_id: str\n        File name of raw inference image target in raw_path dir.\n\n    historical1: Optional[str]\n        File name of raw inference image overlap-1 in raw_path dir.\n\n    historical2: Optional[str]\n        File name of raw inference image overlap-1 in raw_path dir.\n\n    catalog: str\n        Imagery catalog. For now, only Sentinel-1 (\"sentinel1\") and\n        Sentinel-2 (\"sentinel2\") are covered.\n\n    cat_path: str\n        Path to pre-processed numpy array containing preprocessed, concatenated overlaps.\n\n    base_path: str\n        Path to preprocessed copy of original inference target tif image.\n\n    device: torch.device\n        Device to use to prepare scenes.\n\n    detector_model_dir: str\n        Path to dir containing json model config and weights.\n\n    postprocess_model_dir: str\n        Path to dir containing json attribute predictor model config and weights.\n\n    Returns\n    -------\n    : None\n\n    \"\"\"\n    # Load detector and postprocessor config\n    with open(os.path.join(detector_model_dir, \"cfg.json\"), \"r\") as f:\n        detector_cfg = json.load(f)\n    with open(os.path.join(postprocess_model_dir, \"cfg.json\"), \"r\") as f:\n        postprocess_cfg = json.load(f)\n\n    # Verify channel requirements align (ignoring overlap channels),\n    # since the models will share the same pre-processed base imagery\n    postprocess_channels = set(ch[\"Name\"] for ch in postprocess_cfg[\"Channels\"] if \"overlap\" not in ch[\"Name\"])\n    detector_channels = set(ch[\"Name\"] for ch in detector_cfg[\"Channels\"] if \"overlap\" not in ch[\"Name\"])\n    if postprocess_channels != detector_channels:\n        raise InvalidConfigError(\"Detector and postprocessor models are required to use the\"\n                                 f\"same underlying channels.\\n You passed\"\n                                 f\" detector_channels={detector_channels}\\n\"\n                                 f\"postprocessor_channels={postprocess_channels}\")\n\n    # Warp the scenes, in parallel.\n    # Each job is (retrieve_image, scene_id, channel_name, src_path, dst_path)\n    retrieve_images = []\n    jobs = []\n    scene_ids = [scene_id]\n    if historical1:\n        scene_ids.append(historical1)\n    if historical2:\n        scene_ids.append(historical2)\n    for scene_id in scene_ids:\n        scene_channels = []\n        if catalog == \"sentinel1\":\n            measurement_path = os.path.join(raw_path, scene_id, \"measurement\")\n            fnames = {\n                fname.split(\"-\")[3]: fname for fname in os.listdir(measurement_path)\n            }\n            if all(key in fnames for key in SENTINEL_1_REQUIRED_CHANNEL_TYPES):\n                scene_channels.append(\n                    {\n                        \"Name\": \"vh\",\n                        \"Path\": os.path.join(measurement_path, fnames[\"vh\"]),\n                        \"Count\": 1,\n                    }\n                )\n                scene_channels.append(\n                    {\n                        \"Name\": \"vv\",\n                        \"Path\": os.path.join(measurement_path, fnames[\"vv\"]),\n                        \"Count\": 1,\n                    }\n                )\n            else:\n                raise InvalidDataError(\n                    f\"Raw Sentinel-1 data must contain polarization channels={SENTINEL_1_REQUIRED_CHANNEL_TYPES}.\\n\"\n                    f\"Found: {fnames}\"\n                )\n        elif catalog == \"sentinel2\":\n            # Channels of interest for model, as specified in model cfg\n            # Length rules out overlap channels from cfg, which are handled separately here\n            sentinel_2_cois = [x[\"Name\"] for x in detector_cfg[\"Channels\"] if len(x[\"Name\"]) == 3]\n            sentinel_2_coi_map = dict((k, SENTINEL_2_CHANNEL_MAPPING[k])\n                                      for k in sentinel_2_cois if k in SENTINEL_2_CHANNEL_MAPPING)\n            for channel, val in sentinel_2_coi_map.items():\n                path_abbrev = val[\"path_abbrev\"]\n                count = val[\"count\"]\n                path_pattern = os.path.join(raw_path, scene_id, f\"GRANULE/*/IMG_DATA/*_{path_abbrev}.jp2\")\n                paths = glob.glob(path_pattern)\n                if len(paths) == 1:\n                    path = paths[0]\n                    scene_channels.append(\n                        {\n                            \"Name\": channel,\n                            \"Path\": path,\n                            \"Count\": count\n                        }\n                    )\n                else:\n                    raise InvalidDataError(\n                        f\"Raw Sentinel-2 data must be of L1C product type, and contain channel={channel}.\\n\"\n                        f\"Did not find a unique path using the pattern: {path_pattern}\"\n                    )\n\n        else:\n            raise ValueError(\n                f\"You specified imagery catalog={catalog}.\\n\"\n                f\"The only supported catalogs are: {SUPPORTED_IMAGERY_CATALOGS}\"\n            )\n\n        retrieve_image = RetrieveImage(\n            uuid=\"x\",\n            name=scene_id,\n            time=datetime.datetime.now(),\n            format=\"x\",\n            channels=scene_channels,\n            pixel_size=10,\n        )\n        retrieve_image.job_ids = []\n\n        for ch in scene_channels:\n            retrieve_image.job_ids.append(len(jobs))\n\n            if len(jobs) == 0:\n                dst_path = base_path\n            else:\n                dst_path = os.path.join(\n                    scratch_path, scene_id + \"_\" + ch[\"Name\"] + \".tif\"\n                )\n\n            jobs.append(\n                [\n                    retrieve_image,\n                    scene_id,\n                    ch[\"Name\"],\n                    ch[\"Path\"],\n                    dst_path,\n                ]\n            )\n\n        retrieve_images.append(retrieve_image)\n\n    p = multiprocessing.Pool(8)\n    image_infos = p.map(warp_func, jobs)\n    p.close()\n\n    first_info = None\n    ims = []\n    for retrieve_image in retrieve_images:\n        overlap_offset = (0, 0)\n        for ch_idx, ch in enumerate(retrieve_image.channels):\n            job_id = retrieve_image.job_ids[ch_idx]\n            job = jobs[job_id]\n            image_info = image_infos[job_id]\n            _, _, _, _, tmp_path = job\n\n            im = skimage.io.imread(tmp_path)\n            im = np.clip(im, 0, 255).astype(np.uint8)\n            if len(im.shape) == 2:\n                im = im[None, :, :]\n            else:\n                im = im.transpose(2, 0, 1)\n            im = torch.as_tensor(im)\n\n            if not first_info:\n                first_info = image_info\n                ims.append(im)\n                continue\n\n            # Align later images with the first one.\n            left = image_info.column - first_info.column\n            top = image_info.row - first_info.row\n\n            # Automatically fix mis-alignment in the georeference metadata.\n            # We re-align by maximizing the dot product between the overlapping image with small offsets, and the base image.\n            if \"vh\" in ch[\"Name\"] or \"tci\" in ch[\"Name\"]:\n                base_padded = ims[0][0, :, :]\n                other_padded = im[0, :, :]\n                if top < 0:\n                    other_padded = other_padded[-top:, :]\n                else:\n                    base_padded = base_padded[top:, :]\n                if left < 0:\n                    other_padded = other_padded[:, -left:]\n                else:\n                    base_padded = base_padded[:, left:]\n\n                if other_padded.shape[0] > base_padded.shape[0]:\n                    other_padded = other_padded[0: base_padded.shape[0], :]\n                else:\n                    base_padded = base_padded[0: other_padded.shape[0], :]\n                if other_padded.shape[1] > base_padded.shape[1]:\n                    other_padded = other_padded[:, 0: base_padded.shape[1]]\n                else:\n                    base_padded = base_padded[:, 0: other_padded.shape[1]]\n\n                # Try re-alignments up to 32 pixels off from the georeference metadata.\n                max_offset_orig = 32\n                # Test dot products at 1/4 the original resolution.\n                realign_scale = 4\n\n                base_padded = (\n                    base_padded[::realign_scale, ::realign_scale].float().to(device)\n                )\n                other_padded = (\n                    other_padded[::realign_scale, ::realign_scale].float().to(device)\n                )\n\n                max_offset = max_offset_orig // realign_scale\n                best_offset = None\n                best_score = None\n                for top_offset in range(-max_offset, max_offset):\n                    for left_offset in range(-max_offset, max_offset):\n                        # Crop the base by a constant amount, i.e., the maximum offset.\n                        # Then crop the other image by anywhere between 0 and 2*max_offset on the first side, and the opposite on the other side.\n                        cur_base = base_padded[\n                            max_offset:-max_offset, max_offset:-max_offset\n                        ]\n                        cur_other = other_padded[\n                            max_offset\n                            - top_offset: other_padded.shape[0]\n                            - (max_offset + top_offset),\n                            max_offset\n                            - left_offset: other_padded.shape[1]\n                            - (max_offset + left_offset),\n                        ]\n                        score = torch.mean(cur_base * cur_other)\n                        if best_score is None or score > best_score:\n                            best_offset = (left_offset, top_offset)\n                            best_score = score\n\n                overlap_offset = (\n                    realign_scale * best_offset[0],\n                    realign_scale * best_offset[1],\n                )\n                logger.info(\n                    \"computed best offset for {}: {}\".format(\n                        retrieve_image.name, overlap_offset\n                    )\n                )\n\n            left += overlap_offset[0]\n            top += overlap_offset[1]\n\n            if top < 0:\n                im = im[:, -top:, :]\n            else:\n                im = torch.nn.functional.pad(im, (0, 0, top, 0))\n            if left < 0:\n                im = im[:, :, -left:]\n            else:\n                im = torch.nn.functional.pad(im, (left, 0, 0, 0))\n\n            # Crop to size if needed.\n            if im.shape[1] > first_info.height:\n                im = im[:, 0: first_info.height, :]\n            elif im.shape[1] < first_info.height:\n                im = torch.nn.functional.pad(\n                    im, (0, 0, 0, first_info.height - im.shape[1])\n                )\n            if im.shape[2] > first_info.width:\n                im = im[:, :, 0: first_info.width]\n            elif im.shape[2] < first_info.width:\n                im = torch.nn.functional.pad(\n                    im, (0, first_info.width - im.shape[2], 0, 0)\n                )\n\n            ims.append(im)\n\n    im = torch.cat(ims, dim=0)\n    logger.debug(f\"Writing numpy array at {cat_path}\")\n    np.save(cat_path, im.numpy())\n\n    return None\n"}
{"type": "source_file", "path": "evaluate.py", "content": "import argparse\nimport json\nimport logging\nimport os\nimport sqlite3\nimport sys\n\nfrom src.data.dataset import Dataset\nfrom src.data.image import Channels\nfrom src.data.transforms import get_transform\nfrom src.inference.pipeline import load_model\nfrom src.training.evaluate import evaluate, get_evaluator\nfrom src.training.utils import collate_fn\nfrom src.utils.db import dict_factory, get_dataset, get_image, get_labels, get_windows\n\nimport torch\n\nnum_loader_workers = 4\n\n# Configure logger\nlogger = logging.getLogger(\"evaluate\")\nlogger.setLevel(logging.INFO)\nformatter = logging.Formatter(\"%(asctime)s (%(name)s) (%(levelname)s): %(message)s\")\nstream_handler = logging.StreamHandler(sys.stdout)\nstream_handler.setLevel(logging.INFO)\nstream_handler.setFormatter(formatter)\nlogger.addHandler(stream_handler)\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\"Training script.\")\n\n    # General\n    parser.add_argument(\n        \"--model_dir\",\n        help=\"Path to model and validation config json.\",\n        default=\"./data/inference/frcnn_cmp2/3dff445\",\n    )\n    parser.add_argument(\n        \"--validation_data_dir\",\n        help=\"Path to validation data directory containing preprocess folder.\",\n        default=\"./data\",\n    )\n    parser.add_argument(\n        \"--metadata_path\",\n        help=\"Path to sqlite database containing metadata\",\n        default=\"./data/metadata.sqlite3\",\n    )\n\n    args = parser.parse_args()\n    return args\n\n\ndef main(\n    model_dir: str, validation_data_dir: str, metadata_path: str\n) -> None:\n    \"\"\"Run a validation pass on specified model.\n\n    Parameters\n    ----------\n    model_dir: str\n        Path to dir containing model weights and configuration json.\n\n    validation_data_dir: str\n        Path to directory containing validation data.\n\n    metadata_path: str\n        Path to metadata sqlite file.\n\n    Returns\n    -------\n    : None\n    \"\"\"\n    with open(os.path.join(model_dir, \"cfg.json\")) as f:\n        cfg = json.load(f)\n\n    logger.info(\"Reading validation metadata.\")\n\n    # Instantiate DB conn\n    db_path = os.path.abspath(metadata_path)\n    conn = sqlite3.connect(db_path)\n    conn.row_factory = dict_factory\n\n    # Get dataset specified in cfg from db\n    dataset_id = cfg[\"DatasetID\"]\n    dataset = get_dataset(conn, dataset_id)\n\n    # Read test splits specified in config\n    val_splits = cfg[\"Options\"][\"TestSplits\"]\n\n    # Get windows associated with val splits\n    windows = []\n    for split in val_splits:\n        windows.extend(get_windows(conn, dataset_id, split=split))\n    windows_with_labels = []\n\n    # Populate labels associated with each window from db\n    for window in windows:\n        if window[\"hidden\"]:\n            continue\n        image = get_image(conn, window[\"image_id\"])\n        labels = get_labels(conn, window[\"id\"])\n        updated_window = window\n        updated_window[\"image\"] = image\n        updated_window[\"labels\"] = labels\n        windows_with_labels.append(updated_window)\n\n    conn.close()\n\n    model_cfg = cfg\n    options = model_cfg[\"Options\"]\n    channels = Channels(model_cfg[\"Channels\"])\n    task = dataset[\"task\"]\n    model_cfg[\"Data\"] = {}\n    if dataset.get(\"task\"):\n        model_cfg[\"Data\"][\"task\"] = dataset[\"task\"]\n    if dataset.get(\"categories\"):\n        model_cfg[\"Data\"][\"categories\"] = dataset[\"categories\"]\n    batch_size = options.get(\"BatchSize\", 4)\n    chip_size = options.get(\"ChipSize\", 0)\n    image_size = options.get(\"ImageSize\", 0)\n    half_enabled = options.get(\"Half\", True)\n    val_transforms = get_transform(cfg, options, options.get(\"ValTransforms\", []))\n\n    val_data = Dataset(\n        dataset=dataset,\n        windows=windows,\n        channels=channels,\n        splits=val_splits,\n        transforms=val_transforms,\n        image_size=image_size,\n        chip_size=chip_size,\n        valid=True,\n        preprocess_dir=os.path.join(validation_data_dir, \"preprocess\"),\n    )\n\n    device = torch.device(\"cuda\")\n\n    val_sampler = torch.utils.data.SequentialSampler(val_data)\n\n    val_loader = torch.utils.data.DataLoader(\n        val_data,\n        batch_size=batch_size,\n        sampler=val_sampler,\n        num_workers=num_loader_workers,\n        collate_fn=collate_fn,\n    )\n\n    # instantiate model from weights and config\n    model = load_model(\n        model_dir,\n        example=val_data[0],\n        device=device,\n    )\n\n    model.to(device)\n    model.eval()\n    evaluator = get_evaluator(task, options)\n    val_loss, _ = evaluate(\n        model,\n        device,\n        val_loader,\n        half_enabled=half_enabled,\n        evaluator=evaluator,\n    )\n    val_scores = evaluator.score()\n    val_score = val_scores[\"score\"]\n    logger.info(f\"Validation score {val_score}.\")\n    logger.info(f\"Validation loss {val_loss}.\")\n    if task == \"point\":\n        # Log full val set confusion matrix\n        evaluator.log_metrics(\"class0\", logger, use_wandb=False)\n    if task == \"custom\":\n        # Log full val set MAEs by attribute\n        evaluator.log_metrics(logger, use_wandb=False)\n\n    return None\n\n\nif __name__ == \"__main__\":\n    args = parse_args()\n    args_dict = vars(args)\n    main(**args_dict)\n"}
{"type": "source_file", "path": "src/data/dataset.py", "content": "import json\nimport random\n\nimport numpy as np\nimport skimage.io\nimport torch\nimport torchvision\n\nfrom src.data.tiles import load_window\n\n# Weight given to positive fishing activity class instances\n# relative to negative fishing activity class instances\n# TODO: Pass or compute at runtime.\nNEGATIVE_VESSEL_TYPE_FACTOR = 32\n\n# Heuristic based on qualitative PCA performance.\nPSEUDO_HEADING_THRESHOLD_M = 100\n\n# Functions to convert labels at a window into a target dict.\n\n\ndef detect_preprocess(\n    task: str,\n    window: dict,\n    target_width: int,\n    target_height: int,\n    col_factor: int,\n    row_factor: int,\n) -> dict:\n    \"\"\"Construct target dict for detection task from window labels.\n\n    Parameters\n    ----------\n    task: str\n        Task label (currently point is supported).\n\n    window: dict\n        Window as in sqlite metadata database.\n\n    target_width:\n\n    target_height:\n\n    col_factor: int\n\n    row_factor: int\n\n    Returns\n    -------\n    : dict\n        Target dictionary with which to train a Faster RCNN model.\n    \"\"\"\n    centers = []\n    boxes = []\n    class_labels = []\n\n    for label in window[\"labels\"]:\n        # Throwout labels produced to facilitate annotation work\n        if label[\"properties\"] and \"OnKey\" in label[\"properties\"]:\n            continue\n\n        if task == \"point\":\n            column = int((label[\"column\"] - window[\"column\"]) * col_factor)\n            row = int((label[\"row\"] - window[\"row\"]) * row_factor)\n\n            xmin = column - 20\n            xmax = column + 20\n            ymin = row - 20\n            ymax = row + 20\n        else:\n            xmin = label[\"column\"] - window[\"column\"]\n            xmax = xmin + label[\"width\"]\n            ymin = label[\"row\"] - window[\"row\"]\n            ymax = ymin + label[\"height\"]\n            column = (xmin + xmax) // 2\n            row = (ymin + ymax) // 2\n\n        centers.append([column, row])\n        boxes.append([xmin, ymin, xmax, ymax])\n        class_labels.append(0)\n\n    if len(boxes) == 0:\n        centers = torch.zeros((0, 2), dtype=torch.float32)\n        boxes = torch.zeros((0, 4), dtype=torch.float32)\n        class_labels = torch.zeros((0,), dtype=torch.int64)\n    else:\n        centers = torch.as_tensor(centers, dtype=torch.float32)\n        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n        class_labels = torch.as_tensor(class_labels, dtype=torch.int64)\n\n    area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n\n    # Create target dictionary in expected format for Faster R-CNN\n    return {\n        \"centers\": centers,\n        \"boxes\": boxes,\n        \"labels\": class_labels,\n        \"area\": area,\n        \"iscrowd\": torch.zeros((len(boxes),), dtype=torch.int64),\n    }\n\n\ndef pixel_regression_preprocess(\n    task: str,\n    window: dict,\n    target_width: int,\n    target_height: int,\n    col_factor: int,\n    row_factor: int,\n) -> dict:\n    \"\"\"Construct target dict for pixel level regression task from window labels.\n\n    Parameters\n    ----------\n    task: str\n        Task label.\n\n    window: dict\n        Window as in sqlite metadata database.\n\n    target_width: int\n\n    target_height: int\n\n    col_factor: int\n\n    row_factor: int\n\n    Returns\n    -------\n    : dict\n        Target dictionary with which to train a pixel level regression model.\n    \"\"\"\n    target_img = np.zeros((target_height, target_width), dtype=np.float32)\n\n    labels = window[\"labels\"]\n    labels.sort(key=lambda label: label[\"value\"])\n\n    for label in labels:\n        poly_coords = json.loads(label[\"extent\"])\n        col_coords = [\n            (coord[0] - window[\"column\"]) * col_factor for coord in poly_coords\n        ]\n        row_coords = [(coord[1] - window[\"row\"]) * row_factor for coord in poly_coords]\n        rr, cc = skimage.draw.polygon(row_coords, col_coords, shape=target_img.shape)\n        target_img[rr, cc] = label[\"value\"]\n\n    return {\"target\": torch.as_tensor(target_img)}\n\n\ndef regression_preprocess(\n    task: str,\n    window: dict,\n    target_width: int,\n    target_height: int,\n    col_factor: int,\n    row_factor: int,\n):\n    \"\"\"Construct target dict for general regression task from window labels.\n\n    Parameters\n    ----------\n    task: str\n        Task label.\n\n    window: dict\n        Window as in sqlite metadata database.\n\n    target_width: int\n\n    target_height: int\n\n    col_factor: int\n\n    row_factor: int\n\n    Returns\n    -------\n    : dict\n        Target dictionary with which to train a regression model.\n    \"\"\"\n    if len(window[\"labels\"]) > 0:\n        label = window[\"labels\"][0]\n        value = label[\"value\"]\n    else:\n        value = 0\n\n    return {\"target\": torch.tensor(value, dtype=torch.float32)}\n\n\ndef custom_preprocess(\n    task: str,\n    window: dict,\n    target_width: int,\n    target_height: int,\n    col_factor: int,\n    row_factor: int,\n) -> dict:\n    \"\"\"Construct target dict for custom attribute prediction task from window labels.\n\n    Parameters\n    ----------\n    task: str\n        Task label.\n\n    window: dict\n        Window as in sqlite metadata database.\n\n    target_width: int\n\n    target_height: int\n\n    col_factor: int\n\n    row_factor: int\n\n    Returns\n    -------\n    : dict\n        Target dictionary with which to train a custom attribute prediction model.\n    \"\"\"\n    # One label per window in this context\n    label = window[\"labels\"][0]\n    properties = json.loads(label[\"properties\"])\n    target = torch.zeros((21,), dtype=torch.float32)\n    target[0] = properties[\"Length\"] / 100\n    target[1] = properties[\"Width\"] / 100\n\n    if properties[\"Length\"] > PSEUDO_HEADING_THRESHOLD_M and properties.get(\"PseudoHeading\", None):\n        heading_bucket = properties[\"PseudoHeading\"] * 16 // 360\n    else:\n        heading_bucket = properties[\"Heading\"] * 16 // 360\n\n    buckets = 16\n    target[2 + heading_bucket] = 8 / 14\n    # Give weight to adjacent heading buckets\n    # TODO: Only assign this weight if label is near bin boundary\n    for offset in [-1, 1]:\n        cur = (heading_bucket + 16 + offset) % buckets\n        target[2 + cur] = 1 / 14\n\n    # Give equal weight to opposite heading bucket, as pseudo labels are\n    # based on undirected alignment.\n    opp_bucket = int((heading_bucket + buckets/2) % buckets)\n    target[2 + opp_bucket] = 2 / 14\n    for offset in [-1, 1]:\n        cur = (opp_bucket + offset) % buckets\n        target[2 + cur] = 1 / 14\n\n    target[18] = properties[\"Speed\"]\n    vessel_type = properties[\"ShipAndCargoType\"]\n    fishing = vessel_type == 30\n    # not_available = vessel_type == 0\n    if fishing:\n        target[19] = 0.0\n        target[20] = 1.0\n    # elif not_available:\n    #     target[19] = 0.5\n    #     target[20] = 0.5\n    else:\n        target[19] = 1.0\n        target[20] = 0.0\n\n    return {\"target\": target}\n\n\n# Map from task type to preprocess function.\nlabel_preprocessors = {\n    \"point\": detect_preprocess,\n    \"box\": detect_preprocess,\n    \"pixel_regression\": pixel_regression_preprocess,\n    \"regression\": regression_preprocess,\n    \"custom\": custom_preprocess,\n}\n\n\nclass Dataset(object):\n    \"\"\"\n    Pytorch dataset for working with Sentinel-1 data.\n    \"\"\"\n\n    def __init__(\n        self,\n        dataset,\n        windows,\n        channels,\n        splits=None,\n        transforms=None,\n        image_size=None,\n        chip_size=None,\n        valid=False,\n        preprocess_dir=\"../data/preprocess\",\n    ):\n\n        self.dataset = dataset\n        self.channels = channels\n        self.transforms = transforms\n        self.image_size = image_size\n        self.chip_size = chip_size\n        self.valid = valid\n        self.preprocess_dir = preprocess_dir\n\n        self.windows = windows\n        if splits:\n            splits = set(splits)\n            self.windows = [\n                window for window in self.windows if window[\"split\"] in splits\n            ]\n\n    def __len__(self):\n        return len(self.windows)\n\n    def __getitem__(self, idx):\n        window = self.windows[idx]\n\n        if self.chip_size:\n            if window[\"Width\"] < self.chip_size:\n                pick_col = 0\n            elif self.valid:\n                pick_col = window[\"Width\"] // 2 - self.chip_size // 2\n            else:\n                pick_col = random.randint(0, window[\"width\"] - self.chip_size)\n\n            if window[\"height\"] < self.chip_size:\n                pick_row = 0\n            elif self.valid:\n                pick_row = window[\"height\"] // 2 - self.chip_size // 2\n            else:\n                pick_row = random.randint(0, window[\"height\"] - self.chip_size)\n\n            window = {\n                \"ID\": window[\"id\"],\n                \"Image\": window[\"image\"],\n                \"Labels\": window[\"labels\"],\n                \"Width\": self.chip_size,\n                \"Height\": self.chip_size,\n                \"Column\": window[\"column\"] + pick_col,\n                \"Row\": window[\"row\"] + pick_row,\n            }\n\n        if self.image_size:\n            target_width, target_height = self.image_size, self.image_size\n            col_factor = target_width / window[\"width\"]\n            row_factor = target_height / window[\"height\"]\n        else:\n            target_width = window[\"width\"]\n            target_height = window[\"height\"]\n            col_factor = 1\n            row_factor = 1\n\n        data = torch.zeros(\n            (self.channels.count(), target_height, target_width), dtype=torch.uint8\n        )\n\n        for channel, rng in self.channels.with_ranges():\n            im = load_window(\n                window[\"image\"][\"uuid\"],\n                channel,\n                window[\"column\"],\n                window[\"row\"],\n                window[\"width\"],\n                window[\"height\"],\n                preprocess_dir=self.preprocess_dir,\n            )\n            im = torch.as_tensor(im)\n\n            if channel[\"Count\"] > 1:\n                im = im.permute(2, 0, 1)\n\n            if im.shape[0] != target_height or im.shape[1] != target_width:\n                im = torchvision.transforms.functional.resize(\n                    im.unsqueeze(0), [target_height, target_width]\n                ).squeeze(0)\n\n            data[rng[0]: rng[1], :, :] = im\n\n        task = self.dataset[\"task\"]\n\n        target = {\n            \"chip\": (window[\"image\"][\"name\"], window[\"column\"], window[\"row\"]),\n            \"window_id\": torch.tensor(window[\"id\"]),\n            \"image_id\": torch.tensor(idx),\n        }\n\n        preprocess_func = label_preprocessors[task]\n        target.update(\n            preprocess_func(\n                task, window, target_width, target_height, col_factor, row_factor\n            )\n        )\n\n        if self.transforms is not None:\n            data, target = self.transforms(data, target)\n\n        return data, target\n\n    def get_bg_balanced_sampler(self, background_frac=1.0):\n        \"\"\"\n        Returns a torch.utils.data.Sampler that samples foreground/background at 1:1 ratio.\n        Background is defined as having no labels.\n        \"\"\"\n        bg_indices = []\n        fg_indices = []\n        for i, window in enumerate(self.windows):\n            if len(window[\"labels\"]) >= 1:\n                fg_indices.append(i)\n            else:\n                bg_indices.append(i)\n\n        bg_weight = background_frac * (len(fg_indices) / len(bg_indices))\n        weights = [0.0] * len(self.windows)\n        for i in fg_indices:\n            weights[i] = 1.0\n        for i in bg_indices:\n            weights[i] = bg_weight\n\n        print(\n            \"using bg_balanced_sampler with {} bg chips, {} fg chips (bg_weight={})\".format(\n                len(bg_indices), len(fg_indices), bg_weight\n            )\n        )\n        return torch.utils.data.WeightedRandomSampler(weights, len(self.windows))\n"}
{"type": "source_file", "path": "src/__init__.py", "content": ""}
{"type": "source_file", "path": "src/inference/__init__.py", "content": ""}
{"type": "source_file", "path": "src/data/__init__.py", "content": ""}
{"type": "source_file", "path": "src/data/retrieve.py", "content": "import json\nimport os\nimport os.path\nimport shutil\nimport typing as t\n\nfrom src.data.preprocess import preprocess\nfrom src.data.warp import get_image_info, warp\nfrom src.utils.parallel import starmap\n\n\nclass RetrieveImage(object):\n    \"\"\"Container for image metadata.\"\"\"\n\n    def __init__(self, uuid, name, time, channels, format=\"geotiff\", pixel_size=None):\n        self.uuid = uuid\n        self.name = name\n        self.time = time\n        self.channels = channels\n        self.format = format\n        self.pixel_size = pixel_size\n\n\ndef warp_and_print_one(\n    image: RetrieveImage,\n    tmp_dir: str,\n    channel: dict,\n    dst_dir: str = \"../data/images/\",\n    roi: dict = None,\n) -> dict:\n    \"\"\"Warp and single RetrieveImage instance.\n\n    Parameters\n    ----------\n    image: RetrieveImage\n\n    tmp_dir: str\n        Directory in which intermediate warp artifacts will get stored.\n        Intended use is to use a temporary directory.\n\n    channel: dict\n\n    dst_dir: str\n        Path where warped images will get written.\n\n    roi: dict\n        Dict representation of ROI object as in sqlite metadata database.\n\n    Returns\n    -------\n    image_data: dict\n        Dictionary describing the warped image metadata.\n    \"\"\"\n    dst_path = os.path.join(dst_dir, image.uuid)\n    os.makedirs(dst_path, exist_ok=True)\n\n    projection = None\n    if roi and roi[\"Projection\"]:\n        projection = roi[\"Projection\"]\n\n    src_fname = channel[\"Path\"]\n\n    # Determine where to write layer.\n    # Projected images don't need to be stored since we can transform coordinates based on the projection details.\n    # We also decide whether to warp the image or just copy it.\n    if projection:\n        layer_fname = os.path.join(\n            tmp_dir, \"{}_{}.tif\".format(image.uuid, channel[\"Name\"])\n        )\n        image_info = warp(image, src_fname, layer_fname, projection=projection)\n    else:\n        layer_fname = os.path.join(dst_path, \"{}.tif\".format(channel[\"Name\"]))\n        if image.format == \"geotiff\":\n            shutil.move(src_fname, layer_fname)\n            image_info = get_image_info(layer_fname)\n        else:\n            image_info = warp(image, src_fname, layer_fname)\n\n    image_data = {\n        \"UUID\": image.uuid,\n        \"Name\": image.name,\n        \"Format\": \"geotiff\",\n        \"Channels\": image.channels,\n        \"Width\": image_info.width,\n        \"Height\": image_info.height,\n        \"Bounds\": image_info.bounds,\n        \"Time\": image.time.isoformat(),\n        \"Projection\": image_info.projection,\n        \"Column\": image_info.column,\n        \"Row\": image_info.row,\n        \"Zoom\": image_info.zoom,\n    }\n\n    preprocess(image_data, layer_fname, channel, roi=roi)\n\n    if projection:\n        # We don't need the layer, delete it immediately.\n        os.remove(layer_fname)\n\n    return image_data\n\n\ndef warp_and_print(\n    images: t.List[RetrieveImage], tmp_dir: str, roi: dict = None, workers: int = 0\n) -> None:\n    \"\"\"Warp a list of RetrieveImage instances.\n\n\n    Given a list of RetrieveImage:\n    (1) Copy/warp the image file to data/images/ if needed (i.e., if no projection is set and we need the image for coordinate conversion)\n    (2) Pre-process the image into tiles.\n    (3) Print out image JSON so caller can add it to database.\n\n    Parameters\n    ----------\n    images: list[RetrieveImage]\n        List of RetrieveImage objects to warp.\n\n    tmp_dir: str\n        Directory in which intermediate warp artifacts will get stored.\n        Intended use is to use a temporary directory.\n\n    roi: dict\n        Dict representation of ROI object as in sqlite metadata database.\n\n    workers: int\n        Number of workers to use to perform jobs in parallel.\n\n    Returns\n    -------\n    : None\n    \"\"\"\n\n    # Get list of jobs. Each job is to warp/preprocess one channel of one image.\n    jobs = []\n    for image in images:\n        for channel in image.channels:\n            jobs.append((image, tmp_dir, channel, roi))\n\n    image_datas = starmap(warp_and_print_one, [job for job in jobs], workers=workers)\n\n    # image_datas includes one dict per channel of each image.\n    # Here, we print an arbitrary dict per image.\n    seen_uuids = set()\n    for image_data in image_datas:\n        if image_data[\"UUID\"] in seen_uuids:\n            continue\n        seen_uuids.add(image_data[\"UUID\"])\n        print(\"imagejson:\" + json.dumps(image_data), flush=True)\n"}
{"type": "source_file", "path": "src/data/transforms/augment.py", "content": "import math\nimport random\n\nimport torch\nimport torchvision\n\n\nclass CropFlip(object):\n    def __init__(self, model_cfg, options, my_cfg):\n        self.my_cfg = my_cfg\n        self.task = model_cfg[\"Data\"][\"task\"]\n\n    def __call__(self, data, targets):\n        flip_horizontal = (\n            self.my_cfg.get(\"HorizontalFlip\", False) and random.random() < 0.5\n        )\n        flip_vertical = self.my_cfg.get(\"VerticalFlip\", False) and random.random() < 0.5\n\n        if \"CropTo\" in self.my_cfg:\n            crop_width = self.my_cfg[\"CropTo\"]\n            crop_height = self.my_cfg[\"CropTo\"]\n        else:\n            crop_width = data.shape[2]\n            crop_height = data.shape[1]\n\n        crop_left = random.randint(0, data.shape[2] - crop_width)\n        crop_top = random.randint(0, data.shape[1] - crop_height)\n\n        def crop_and_flip(im):\n            if len(im.shape) == 3:\n                im = im[\n                    :,\n                    crop_top: crop_top + crop_height,\n                    crop_left: crop_left + crop_width,\n                ]\n                if flip_horizontal:\n                    im = torch.flip(im, dims=[2])\n                if flip_vertical:\n                    im = torch.flip(im, dims=[1])\n                return im\n            elif len(im.shape) == 2:\n                im = im[crop_top: crop_top + crop_height, crop_left: crop_left + crop_width]\n                if flip_horizontal:\n                    im = torch.flip(im, dims=[1])\n                if flip_vertical:\n                    im = torch.flip(im, dims=[0])\n                return im\n\n        data = crop_and_flip(data)\n\n        if self.task in [\"segmentation\", \"pixel_regression\"]:\n            targets[\"target\"] = crop_and_flip(targets[\"target\"])\n\n        elif self.task in [\"point\", \"box\"]:\n            valid_indices = (\n                (targets[\"centers\"][:, 0] > crop_left)\n                & (targets[\"centers\"][:, 0] < crop_left + crop_width)\n                & (targets[\"centers\"][:, 1] > crop_top)\n                & (targets[\"centers\"][:, 1] < crop_top + crop_height)\n            )\n\n            targets[\"centers\"] = targets[\"centers\"][valid_indices, :].contiguous()\n            targets[\"boxes\"] = targets[\"boxes\"][valid_indices, :].contiguous()\n            targets[\"labels\"] = targets[\"labels\"][valid_indices].contiguous()\n\n            targets[\"centers\"][:, 0] -= crop_left\n            targets[\"centers\"][:, 1] -= crop_top\n            targets[\"boxes\"][:, 0] -= crop_left\n            targets[\"boxes\"][:, 1] -= crop_top\n            targets[\"boxes\"][:, 2] -= crop_left\n            targets[\"boxes\"][:, 3] -= crop_top\n\n            if flip_horizontal:\n                targets[\"centers\"][:, 0] = crop_width - targets[\"centers\"][:, 0]\n                targets[\"boxes\"] = torch.stack(\n                    [\n                        crop_width - targets[\"boxes\"][:, 2],\n                        targets[\"boxes\"][:, 1],\n                        crop_width - targets[\"boxes\"][:, 0],\n                        targets[\"boxes\"][:, 3],\n                    ],\n                    dim=1,\n                )\n            if flip_vertical:\n                targets[\"centers\"][:, 1] = data.shape[1] - targets[\"centers\"][:, 1]\n                targets[\"boxes\"] = torch.stack(\n                    [\n                        targets[\"boxes\"][:, 0],\n                        crop_height - targets[\"boxes\"][:, 3],\n                        targets[\"boxes\"][:, 2],\n                        crop_height - targets[\"boxes\"][:, 1],\n                    ],\n                    dim=1,\n                )\n\n        return data, targets\n\n\nclass Rotate(object):\n    def __init__(self, info):\n        pass\n\n    def __call__(self, image, targets):\n        angle_deg = random.randint(0, 359)\n        angle_rad = angle_deg * math.pi / 180\n        image = torchvision.transforms.functional.rotate(image, angle_deg)\n\n        if len(targets[\"boxes\"]) == 0:\n            return image, targets\n\n        im_center = (image.shape[2] // 2, image.shape[1] // 2)\n        # Subtract center.\n        centers = torch.stack(\n            [\n                targets[\"centers\"][:, 0] - im_center[0],\n                targets[\"centers\"][:, 1] - im_center[1],\n            ],\n            dim=1,\n        )\n        # Rotate around origin.\n        centers = torch.stack(\n            [\n                math.sin(angle_rad) * centers[:, 1]\n                + math.cos(angle_rad) * centers[:, 0],\n                math.cos(angle_rad) * centers[:, 1]\n                - math.sin(angle_rad) * centers[:, 0],\n            ],\n            dim=1,\n        )\n        # Add back the center.\n        centers = torch.stack(\n            [\n                centers[:, 0] + im_center[0],\n                centers[:, 1] + im_center[1],\n            ],\n            dim=1,\n        )\n        # Prune ones outside image window.\n        valid_indices = (\n            (centers[:, 0] > 0)\n            & (centers[:, 0] < image.shape[2])\n            & (centers[:, 1] > 0)\n            & (centers[:, 1] < image.shape[1])\n        )\n        centers = centers[valid_indices, :].contiguous()\n        targets[\"centers\"] = centers\n\n        # Update boxes too, by keeping dimensions the same.\n        boxes = targets[\"boxes\"][valid_indices, :]\n        x_sides = (boxes[:, 2] - boxes[:, 0]) / 2\n        y_sides = (boxes[:, 3] - boxes[:, 1]) / 2\n        targets[\"boxes\"] = torch.stack(\n            [\n                centers[:, 0] - x_sides,\n                centers[:, 1] - y_sides,\n                centers[:, 0] + x_sides,\n                centers[:, 1] + y_sides,\n            ],\n            dim=1,\n        )\n\n        targets[\"labels\"] = targets[\"labels\"][valid_indices].contiguous()\n\n        return image, targets\n\n\nclass Noise(object):\n    def __init__(self, info):\n        pass\n\n    def __call__(self, image, targets):\n        image = image + 0.1 * torch.randn(image.shape)\n        image = torch.clip(image, min=0, max=1)\n        return image, targets\n\n\nclass Jitter(object):\n    def __init__(self, info):\n        pass\n\n    def __call__(self, image, targets):\n        jitter = 0.4 * (torch.rand(image.shape[0]) - 0.5)\n        image = image + jitter[:, None, None]\n        image = torch.clip(image, min=0, max=1)\n        return image, targets\n\n\nclass Jitter2(object):\n    def __init__(self, info):\n        pass\n\n    def __call__(self, image, targets):\n        jitter = 0.1 * (torch.rand(image.shape[0]) - 0.5)\n        image = image + jitter[:, None, None]\n        image = torch.clip(image, min=0, max=1)\n        return image, targets\n\n\nclass Bucket(object):\n    def __init__(self, info):\n        pass\n\n    def __call__(self, image, targets):\n        for channel_idx in [0, 1]:\n            buckets = torch.tensor(\n                [(i + 1) / 10 + (random.random() - 0.5) / 10 for i in range(9)],\n                device=image.device,\n            )\n            image[channel_idx, :, :] = (\n                torch.bucketize(image[channel_idx, :, :], buckets).float() / 10\n            )\n        return image, targets\n\n\nclass HideOverlapping(object):\n    def __init__(self, info):\n        self.overlap_indexes = []\n        for i, channel in enumerate(info[\"Channels\"].flatten()):\n            if \"overlap\" in channel:\n                self.overlap_indexes.append(i)\n\n    def __call__(self, image, targets):\n        hide = random.random() < 0.2\n        if not hide:\n            return image, targets\n\n        if random.random() < 0.5:\n            color = 0\n        else:\n            color = 255\n\n        for i in self.overlap_indexes:\n            image[i, :, :] = color\n\n        return image, targets\n"}
{"type": "source_file", "path": "scripts/create_symlinks.py", "content": "import argparse\nimport os\n\n\ndef create_links(data_dir: str) -> None:\n    \"\"\"Create symbolic links for overlaps in Sentinel-1 training data.\n\n    Parameters\n    ----------\n    data_dir: str\n        Absolute path to local directory containing: 1) Preprocessed Sentinel-1 image\n        folders, and 2) A file `overlap_links.txt` describing one symlink per line,\n        in the format '(data_dir relative path to symlink source) (data_dir relative path to symlink target)'.\n\n    \"\"\"\n    link_file = os.path.join(data_dir, 'overlap_links.txt')\n    with open(link_file, 'r') as f:\n        tot = sum(1 for line in f)\n        f.seek(0)\n        created = 0\n        skipped = 0\n        log_freq = 100000\n        for line in f:\n            line = line.strip()\n            src, tgt = line.split(\" \")\n\n            # Get base paths\n            src = \"/\".join(src.split(\"/\")[1:])\n            tgt = \"/\".join(tgt.split(\"/\")[2:])\n\n            # Get absolute dir path\n            src_dir = \"/\".join(src.split(\"/\")[:-1])\n            src_dir = os.path.join(data_dir, src_dir)\n\n            # Make dirs as necessary\n            os.makedirs(src_dir, exist_ok=True)\n\n            # Get absolute src, tgt\n            tgt = os.path.join(data_dir, tgt)\n            src = os.path.join(data_dir, src)\n            try:\n                os.symlink(tgt, src)\n                if created % log_freq == 0:\n                    print(f\"Created {created}/{tot} links.\")\n                created += 1\n            except FileExistsError:\n                print(f\"Skipping existing link: {src}\")\n                skipped += 1\n                pass\n            except Exception as e:\n                print(f\"Unexpected exception: {e}\")\n        print(f\"Created {created} symlinks.\")\n        print(f\"Skipped {skipped} existing symlinks.\")\n\n    return None\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--data-dir', type=str, default=\"./preprocess\",\n                        help='Absolute path to preprocess directory.')\n    args = parser.parse_args()\n    args_dict = vars(args)\n    create_links(**args_dict)\n"}
{"type": "source_file", "path": "src/downstream/__init__.py", "content": ""}
{"type": "source_file", "path": "src/inference/pipeline.py", "content": "import glob\nimport json\nimport logging\nimport math\nimport os\nimport time\nimport typing as t\n\nfrom functools import partial\n\nimport numpy as np\nimport pandas as pd\nimport PIL\nimport pyproj\nimport skimage.io\nimport torch\nimport torch.utils.data\nfrom osgeo import gdal\n\nfrom src.data.image import Channels, SUPPORTED_IMAGERY_CATALOGS\nfrom src.models import models\nfrom src.utils.filter import filter_out_locs\n\nPIL.Image.MAX_IMAGE_PIXELS = None\n\nlogger = logging.getLogger(\"inference\")\n\n\nclass GridIndex(object):\n    \"\"\"Implements a grid index for spatial data.\n\n    The index supports inserting points or rectangles, and efficiently searching by bounding box.\n    \"\"\"\n\n    def __init__(self, size):\n        self.size = size\n        self.grid = {}\n\n    # Insert a point with data.\n    def insert(self, p, data):\n        self.insert_rect([p[0], p[1], p[0], p[1]], data)\n\n    # Insert a data with rectangle bounds.\n    def insert_rect(self, rect, data):\n        def f(cell):\n            if cell not in self.grid:\n                self.grid[cell] = []\n            self.grid[cell].append(data)\n\n        self.each_cell(rect, f)\n\n    def each_cell(self, rect, f):\n        for i in range(rect[0] // self.size, rect[2] // self.size + 1):\n            for j in range(rect[1] // self.size, rect[3] // self.size + 1):\n                f((i, j))\n\n    def search(self, rect):\n        matches = set()\n\n        def f(cell):\n            if cell not in self.grid:\n                return\n            for data in self.grid[cell]:\n                matches.add(data)\n\n        self.each_cell(rect, f)\n        return matches\n\n\ndef save_detection_crops(\n        img: np.ndarray, label: t.NamedTuple, output_dir: str, detect_id: str, catalog: str = \"sentinel1\",\n        out_crop_size: int = 128, transformer: gdal.Transformer = None) -> t.Tuple[np.ndarray, t.List[t.Tuple[float, float]]]:\n    \"\"\"Save detection crops of interest for a given imagery catalog.\n\n    Parameters\n    ----------\n    img: np.ndarray\n        Full preprocessed image (base, no historical concats).\n\n    label: NamedTuple\n        Namedtuple with at least preprocess_row and preprocess_column attrs\n        specifying row and column in img at which a detection label was made.\n\n    output_dir: str\n        Directory in which output crops will be saved.\n\n    detect_id: str\n        String identifying detection.\n\n    catalog: str\n        String identifying imagery collection. Currently \"sentinel1\" and\n        \"sentinel2\" are supported.\n\n    out_crop_size: int\n        Size of output crop around center of detection.\n\n    transformer: gdal.Transformer\n        Transformer specifying source and targer coordinate reference system to use to\n        record output crop coordinates (e.g. lat/lon).\n\n    Returns\n    -------\n    crop: np.ndarray\n        A crop with all channels from the preprocessed image.\n\n    corner_lat_lons: list[tuple(float)]\n        List of coordinate tuples (lat, lon) of image corners. Ordered as\n        upper left, upper right, lower right, lower left, viewed from\n        above with north up.\n    \"\"\"\n    row, col = label.preprocess_row, label.preprocess_column\n\n    row = np.clip(row, out_crop_size // 2, img.shape[1] - out_crop_size // 2)\n    col = np.clip(col, out_crop_size // 2, img.shape[2] - out_crop_size // 2)\n\n    crop_start_row = row - out_crop_size // 2\n    crop_end_row = row + out_crop_size // 2\n    crop_start_col = col - out_crop_size // 2\n    crop_end_col = col + out_crop_size // 2\n\n    crop = img[\n        :,\n        crop_start_row: crop_end_row,\n        crop_start_col: crop_end_col,\n    ].numpy()\n\n    # Crop subchannels of interest\n    crop_sois = {}\n    if catalog == \"sentinel1\":\n        crop_sois[\"vh\"] = crop[0, :, :]\n        crop_sois[\"vv\"] = crop[1, :, :]\n    elif catalog == \"sentinel2\":\n        crop_sois[\"tci\"] = crop[0:3, :, :].transpose(1, 2, 0)\n    else:\n        raise ValueError(\n            f\"You specified imagery catalog={catalog}.\\n\"\n            f\"The only supported catalogs are: {SUPPORTED_IMAGERY_CATALOGS}\"\n        )\n\n    for key, crop_soi in crop_sois.items():\n        skimage.io.imsave(\n            os.path.join(output_dir, f\"{detect_id}_{key}.png\"),\n            crop_soi,\n            check_contrast=False,\n        )\n\n    # Get corner coordinates of crops\n    corner_lat_lons = []\n    corner_cols_and_rows = [\n        (crop_start_col, crop_start_row),\n        (crop_end_col, crop_start_row),\n        (crop_end_col, crop_end_row),\n        (crop_start_col, crop_end_row)]\n    for corner in corner_cols_and_rows:\n        success, point = transformer.TransformPoint(0, float(corner[0]), float(corner[1]), 0)\n        if success != 1:\n            raise Exception(\"transform error\")\n        longitude, latitude = point[0], point[1]\n        corner_lat_lons.append((latitude, longitude))\n\n    return crop, corner_lat_lons\n\n\ndef nms(pred: pd.DataFrame, distance_thresh: int = 10) -> pd.DataFrame:\n    \"\"\"Prune detections that are redundant due to a nearby higher-scoring detection.\n\n    Parameters\n    ----------\n    pred: pd.DataFrame\n        Dataframe containing detections, from detect.py.\n\n    distance_threshold: int\n        If two detections are closer this threshold, only keep detection\n        with a higher score.\n\n    Returns\n    -------\n    : pd.DataFrame\n        Dataframe of detections filtered via NMS.\n    \"\"\"\n    # Create table index so we can refer to rows by unique index.\n    pred.reset_index()\n\n    elim_inds = set()\n\n    # Create grid index.\n    grid_index = GridIndex(max(64, distance_thresh))\n    for index, row in enumerate(pred.itertuples()):\n        grid_index.insert((row.preprocess_row, row.preprocess_column), index)\n\n    # Remove points that are close to a higher-confidence, not already-eliminated point.\n    for index, row in enumerate(pred.itertuples()):\n        if row.score == 1:\n            continue\n        rect = [\n            row.preprocess_row - distance_thresh,\n            row.preprocess_column - distance_thresh,\n            row.preprocess_row + distance_thresh,\n            row.preprocess_column + distance_thresh,\n        ]\n        for other_index in grid_index.search(rect):\n            other = pred.loc[other_index]\n            if other.score < row.score or (\n                other.score == row.score and other_index <= index\n            ):\n                continue\n            if other_index in elim_inds:\n                continue\n\n            dx = other.preprocess_column - row.preprocess_column\n            dy = other.preprocess_row - row.preprocess_row\n            distance = math.sqrt(dx * dx + dy * dy)\n            if distance > distance_thresh:\n                continue\n\n            elim_inds.add(index)\n            break\n\n    logger.info(\n        \"NMS: retained {} of {} detections.\".format(\n            len(pred) - len(elim_inds), len(pred)\n        )\n    )\n\n    return pred.drop(list(elim_inds))\n\n\ndef load_model(model_dir: str, example: list, device: torch.device) -> torch.nn.Module:\n    \"\"\"Load a model from a dir containing config specifying arch, and weights.\n\n    Parameters\n    ----------\n    model_dir: str\n        Directory containing model weights .pth file, and config specifying model\n        architechture.\n\n    example: list\n        An example input to the model, consisting of two elements. First is\n        a torch tensor encoding an image, second is an (optionally None) label.\n        TODO: Check second elt is described correctly.\n\n    device: torch.device\n        A device on which model should be loaded.\n\n    Returns\n    -------\n    model: torch.nn.Module\n        Loaded model class.\n    \"\"\"\n    with open(os.path.join(model_dir, \"cfg.json\"), \"r\") as f:\n        model_cfg = json.load(f)\n\n    data_cfg = model_cfg[\"Data\"]\n\n    options = model_cfg[\"Options\"]\n\n    channels = Channels(model_cfg[\"Channels\"])\n    model_name = model_cfg[\"Architecture\"]\n\n    model_cls = models[model_name]\n    model = model_cls(\n        {\n            \"Channels\": channels,\n            \"Device\": device,\n            \"Model\": model_cfg,\n            \"Options\": options,\n            \"Data\": data_cfg,\n            \"Example\": example,\n        }\n    )\n\n    model.load_state_dict(\n        torch.load(os.path.join(model_dir, \"best.pth\"), map_location=device)\n    )\n    model.to(device)\n\n    model.eval()\n\n    return model\n\n\ndef apply_model(\n    detector_dir: str,\n    img: np.ndarray,\n    window_size: int = 1024,\n    padding: int = 0,\n    overlap: int = 0,\n    threshold: float = 0.5,\n    transformer: gdal.Transformer = None,\n    nms_thresh: float = None,\n    postprocess_model_dir: str = None,\n    out_path: str = None,\n    catalog: str = 'sentinel1',\n    device: torch.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n) -> pd.DataFrame:\n    \"\"\"Apply a model on a large image, by running it on sliding windows along both axes of the image.\n\n\n    This function currently assumes the task is point detection or the custom attribute prediction task.\n\n    Parameters\n    ----------\n    detector_dir: str\n        Path to dir containing json model config and weights.\n\n    img: np.ndarray\n        3D numpy array (channel, row, col). Must be uint8.\n\n    window_size: int\n        Size of windows on which to apply the model.\n\n    padding: int\n\n    overlap: int\n\n    threshold: float\n        Object detection confidence threshold.\n\n    transformer: gdal.Transformer\n        Transformer specifying source and targer coordinate reference system to use to\n        record output prediction coordinates (e.g. lat/lon).\n\n    nms_thresh: float\n        Distance threshold to use for NMS.\n\n    postprocess_model_dir: str\n        Path to dir containing json attribute predictor model config and weights.\n\n    out_path: str\n        Path to output directory in which model results will be written.\n\n    device: torch.device\n        Device on which model will be applied.\n\n    Returns\n    -------\n    pred: pd.DataFrame\n        Dataframe containing prediction results.\n    \"\"\"\n    model = load_model(\n        detector_dir,\n        example=[img[:, 0:window_size, 0:window_size].float() / 255, None],\n        device=device,\n    )\n    postprocess_model = load_model(\n        postprocess_model_dir,\n        example=[img[:, 0:128, 0:128].float() / 255, None],\n        device=device,\n    )\n    outputs = []\n\n    with torch.no_grad():\n        # Loop over windows.\n        row_offsets = (\n            [0]\n            + list(\n                range(\n                    window_size - 2 * padding,\n                    img.shape[1] - window_size,\n                    window_size - 2 * padding,\n                )\n            )\n            + [img.shape[1] - window_size]\n        )\n        col_offsets = (\n            [0]\n            + list(\n                range(\n                    window_size - 2 * padding,\n                    img.shape[2] - window_size,\n                    window_size - 2 * padding,\n                )\n            )\n            + [img.shape[2] - window_size]\n        )\n\n        start_time = time.time()\n        for row_offset in row_offsets:\n            logger.info(\n                f\"Processing row offset {row_offset}/{row_offsets[-1]}. {(time.time()-start_time):.1f} seconds elapsed.\"\n            )\n            for col_offset in col_offsets:\n                crop = img[\n                    :,\n                    row_offset: row_offset + window_size,\n                    col_offset: col_offset + window_size,\n                ]\n                crop = crop.to(device).float() / 255\n\n                output = model([crop])[0][0]\n\n                # Only keep output detections that are within bounds based\n                # on window size, padding, and overlap.\n                keep_bounds = [\n                    padding,\n                    padding,\n                    window_size - padding,\n                    window_size - padding,\n                ]\n                if col_offset == 0:\n                    keep_bounds[0] = 0\n                if row_offset == 0:\n                    keep_bounds[1] = 0\n                if col_offset >= img.shape[2] - window_size:\n                    keep_bounds[2] = window_size\n                if row_offset >= img.shape[1] - window_size:\n                    keep_bounds[3] = window_size\n\n                keep_bounds[0] -= overlap\n                keep_bounds[1] -= overlap\n                keep_bounds[2] += overlap\n                keep_bounds[3] += overlap\n\n                for pred_idx, box in enumerate(output[\"boxes\"].tolist()):\n                    score = output[\"scores\"][pred_idx].item()\n\n                    if score < threshold:\n                        continue\n\n                    crop_column = (box[0] + box[2]) / 2\n                    crop_row = (box[1] + box[3]) / 2\n\n                    if crop_column < keep_bounds[0] or crop_column > keep_bounds[2]:\n                        continue\n                    if crop_row < keep_bounds[1] or crop_row > keep_bounds[3]:\n                        continue\n\n                    column = col_offset + int(crop_column)\n                    row = row_offset + int(crop_row)\n\n                    success, point = transformer.TransformPoint(0, float(column), float(row), 0)\n                    if success != 1:\n                        raise Exception(\"transform error\")\n                    longitude, latitude = point[0], point[1]\n\n                    outputs.append(\n                        [\n                            row,\n                            column,\n                            longitude,\n                            latitude,\n                            score,\n                        ]\n                    )\n\n        pred = pd.DataFrame(\n            data=[output + [0] * 20 for output in outputs],\n            columns=[\n                \"preprocess_row\",\n                \"preprocess_column\",\n                \"lon\",\n                \"lat\",\n                \"score\",\n                \"vessel_length_m\",\n                \"vessel_width_m\",\n            ]\n            + [\"heading_bucket_{}\".format(i) for i in range(16)]\n            + [\"vessel_speed_k\", \"is_fishing_vessel\"],\n        )\n        pred = pred.astype({\"preprocess_row\": \"int64\", \"preprocess_column\": \"int64\"})\n        logger.info(\"{} detections found\".format(len(pred)))\n\n        if nms_thresh is not None:\n            pred = nms(pred, distance_thresh=nms_thresh)\n\n        # Post-processing.\n        bs = 32\n        crop_size = 120\n        pred = pred.reset_index(drop=True)\n        for x in range(0, len(pred), bs):\n            batch_df = pred.iloc[x: min((x + bs), len(pred))]\n\n            crops, indices = [], []\n            for idx, b in enumerate(batch_df.itertuples()):\n                indices.append(idx)\n                row, col = b.preprocess_row, b.preprocess_column\n\n                row = np.clip(row, crop_size // 2, img.shape[1] - crop_size // 2)\n                col = np.clip(col, crop_size // 2, img.shape[2] - crop_size // 2)\n                if catalog == \"sentinel1\":\n                    crop = img[\n                        0:2,\n                        row - crop_size // 2: row + crop_size // 2,\n                        col - crop_size // 2: col + crop_size // 2,\n                    ]\n                elif catalog == \"sentinel2\":\n                    crop = img[\n                        0:postprocess_model.num_channels,\n                        row - crop_size // 2: row + crop_size // 2,\n                        col - crop_size // 2: col + crop_size // 2,\n                    ]\n                else:\n                    crop = img[\n                        :,\n                        row - crop_size // 2: row + crop_size // 2,\n                        col - crop_size // 2: col + crop_size // 2,\n                    ]\n                crop = crop.to(device).float() / 255\n                crops.append(crop)\n\n            outputs = postprocess_model(crops)[0].cpu()\n\n            for i in range(len(indices)):\n                index = x + i\n                pred.loc[index, \"vessel_length_m\"] = 100 * outputs[i, 0].item()\n                pred.loc[index, \"vessel_width_m\"] = 100 * outputs[i, 1].item()\n                heading_probs = torch.nn.functional.softmax(outputs[i, 2:18], dim=0)\n                for j in range(16):\n                    pred.loc[index, \"heading_bucket_{}\".format(j)] = heading_probs[\n                        j\n                    ].item()\n                pred.loc[index, \"vessel_speed_k\"] = outputs[i, 18].item()\n                pred.loc[index, \"is_fishing_vessel\"] = round(\n                    torch.nn.functional.softmax(outputs[i, 19:21], dim=0)[1].item(), 15\n                )\n\n    return pred\n\n\ndef get_approximate_pixel_size(img: np.ndarray, corner_lat_lons: t.List[t.Tuple[float, float]]) -> t.Tuple[float, float]:\n    \"\"\"Return approximate pixel size given an image (as numpy array), and extremal lat/lons.\n\n    Computes:\n\n    1/2 * [(total width in meters top row) / num_pixels wide + (total width in meters bottom row) / num_pixels wide]\n\n    and\n\n    1/2 * [(total height in meters left col) / num_pixels tall + (total height in meters right col) / num_pixels tall]\n\n    Parameters\n    ----------\n    img: np.ndarray\n        Input numpy array encoding image, shape (C, H, W).\n\n    corner_lat_lons: list\n        List of coordinate tuples (lat, lon) of image corners. Ordered as\n        upper left, upper right, lower right, lower left, viewed from\n        above with north up. Assumed that upper corners share fixed latitude,\n        lower corners share fixed latitude, left corners share fixed longitude,\n        right corners share fixed longitude.\n\n    Returns\n    -------\n    approx_pixel_height: float\n        Approximate pixel length in image.\n\n    approx_pixel_width: float\n        Approximate pixel width in image.\n    \"\"\"\n    # Image spatial shape\n    n_rows = int(img.shape[1])\n    n_cols = int(img.shape[2])\n\n    # Corner coords\n    ul, ur, lr, ll = corner_lat_lons\n\n    geodesic = pyproj.Geod(ellps=\"WGS84\")\n\n    _, _, top_width_m = geodesic.inv(ul[1], ul[0], ur[1], ur[0])\n    _, _, bottom_width_m = geodesic.inv(ll[1], ll[0], lr[1], lr[0])\n\n    approx_pixel_width = .5 * ((top_width_m // n_cols) + (bottom_width_m // n_cols))\n    _, _, left_height_m = geodesic.inv(ul[1], ul[0], ll[1], ll[0])\n    _, _, right_height_m = geodesic.inv(ur[1], ur[0], lr[1], lr[0])\n\n    approx_pixel_height = .5 * ((left_height_m // n_rows) + (right_height_m // n_rows))\n\n    return approx_pixel_height, approx_pixel_width\n\n\ndef detect_vessels(\n    detector_model_dir: str,\n    postprocess_model_dir: str,\n    raw_path: str,\n    scene_id: str,\n    cat_path: str,\n    base_path: str,\n    output_dir: str,\n    window_size: int,\n    padding: int,\n    overlap: int,\n    conf: float,\n    nms_thresh: float,\n    save_crops: bool,\n    device: torch.device,\n    catalog: str,\n    avoid: t.Optional[str]\n) -> None:\n    \"\"\"Detect vessels in specified image using specified model.\n\n    Parameters\n    ----------\n    detector_model_dir: str\n        Path to dir containing json model config and weights.\n\n    postprocess_model_dir: str\n        Path to dir containing json attribute predictor model config and weights.\n\n    raw_path: str\n        Path to dir containing images on which inference will be performed.\n\n    scene_id: str\n        The directory name of a (decompressed) Sentinel-1 scene in the raw_path\n        directory on which inference will be performed.\n\n        E.g. S1B_IW_GRDH_1SDV_20211130T025211_20211130T025236_029811_038EEF_D350.SAFE\n\n    cat_path: str\n        The path to an intermediate numpy array containing a preprocessed target\n        consisting of concatenated inference target and optional historical imagery.\n        See detect.py for an example of usage.\n\n    base_path: str\n        The path to a preprocessed copy of the inference target geotiff file.\n        See detect.py for an example of usage.\n\n    output_dir: str\n        Path to output directory in which model results will be written.\n\n    window_size: int\n        Size of windows on which to apply the model.\n\n    padding: int\n\n    overlap: int\n\n    conf: float\n        Object detection confidence threshold.\n\n    nms_thresh: float\n        Distance threshold to use for NMS.\n\n    save_crops: bool\n        If True, crops surrounding point detections will be saved to output dir.\n\n    device: torch.device\n        Device on which model will be applied.\n\n    catalog: str\n        Imagery catalog. Currently supported: \"sentinel1\", \"sentinel2\".\n\n    avoid: Optional[str]\n        If not None, a path to a csv file containing columns lon, lat, width_m.\n        Every row should have lon and lat specified, and optionally width_m specified.\n        Locations specified will be used to filter, using a default extent (or width_m if specified),\n        any detections that overlap. Could be used to filter out e.g. known fixed infrastructure.\n    \"\"\"\n    # Isolate original file name to write with detections\n    suffix = \"_cat.npy\"\n    if cat_path.endswith(suffix):\n        filename = cat_path.split(\"/\")[-1][: -len(suffix)]\n    else:\n        filename = \"scene\"\n\n    # For greyscale input, fix image to have three dimensions.\n    if cat_path.endswith(\".npy\"):\n        img = np.load(cat_path)\n    else:\n        img = skimage.io.imread(cat_path)\n        if len(img.shape) == 2:\n            img = img[None, :, :]\n        else:\n            img = img.transpose(2, 0, 1)\n    img = torch.as_tensor(img)\n\n    layer = gdal.Open(base_path)\n    transformer = gdal.Transformer(layer, None, [\"DST_SRS=WGS84\"])\n\n    pred = apply_model(\n        detector_model_dir,\n        img,\n        window_size=window_size,\n        padding=padding,\n        overlap=overlap,\n        threshold=conf,\n        transformer=transformer,\n        nms_thresh=nms_thresh,\n        postprocess_model_dir=postprocess_model_dir,\n        out_path=output_dir,\n        catalog=catalog,\n        device=device,\n    )\n\n    # Add pixel coordinates of detections w.r.t. original image\n    # TODO: Modularize this, and make it robust to different sources, etc.\n    def get_input_pixel_coords(out_col, out_row, transformer):\n        success, point = transformer.TransformPoint(\n            0, float(out_col), float(out_row), 0\n        )\n        input_col, input_row, _ = point\n        return int(input_col), int(input_row)\n\n    if catalog == \"sentinel1\":\n        measurement_path = os.path.join(raw_path, scene_id, \"measurement\")\n        source_paths = os.listdir(measurement_path)\n        input_path = os.path.join(measurement_path, source_paths[0])\n    elif catalog == \"sentinel2\":\n        base_channel = \"TCI\"\n        raw_match = f\"GRANULE/*/IMG_DATA/*_{base_channel}.jp2\"\n        path_pattern = os.path.join(raw_path, scene_id, raw_match)\n        paths = glob.glob(path_pattern)\n        input_path = paths[0]\n\n    else:\n        raise ValueError(\n            f\"You specified imagery catalog={catalog}.\\n\"\n            f\"The only supported catalogs are: {SUPPORTED_IMAGERY_CATALOGS}\"\n        )\n\n    output_raster = gdal.Open(base_path)\n    input_raster = gdal.Open(input_path)\n    transformer = gdal.Transformer(output_raster, input_raster, [])\n    get_input_coords = partial(get_input_pixel_coords, transformer=transformer)\n    del output_raster, input_raster\n    if len(pred) > 0:\n        pred[[\"column\", \"row\"]] = pred.apply(\n            lambda row: get_input_coords(row.preprocess_column, row.preprocess_row),\n            axis=1,\n            result_type=\"expand\",\n        )\n\n    pred = pred.reset_index(drop=True)\n\n    # Construct scene and detection ids, crops if requested\n    transformer = gdal.Transformer(layer, None, [\"DST_SRS=WGS84\"])\n    detect_ids = [None] * len(pred)\n    scene_ids = [None] * len(pred)\n    for index, label in enumerate(pred.itertuples()):\n        scene_id = filename\n        scene_ids[index] = scene_id\n        detect_id = \"{}_{}\".format(filename, index)\n        detect_ids[index] = detect_id\n\n        if save_crops:\n            transformer = gdal.Transformer(layer, None, [\"DST_SRS=WGS84\"])\n            crop, corner_lat_lons = save_detection_crops(\n                img, label, output_dir, detect_id, catalog=catalog, out_crop_size=128, transformer=transformer)\n\n            # CW rotation angle necessary to rotate vertical line in image to align with North\n            pred[\"orientation\"] = 0  # by virtue of crops coming from web mercator aligned image.\n\n            # TODO: This approach assumes all crops associated with a detection have the same pixel\n            # resolution, which need not always be the case.\n            # Avoid storing crop metadata on the detection object directly.\n            _, pixel_width = get_approximate_pixel_size(crop, corner_lat_lons)\n            pred[\"meters_per_pixel\"] = pixel_width\n\n    # Insert scene/detect ids in csv\n    pred.insert(len(pred.columns), \"detect_id\", detect_ids)\n    pred.insert(len(pred.columns), \"scene_id\", scene_ids)\n\n    # Filter out undesirable locations\n    if avoid:\n        logger.info(f\"Filtering detections based on locs in {avoid}.\")\n        num_unfiltered = len(pred)\n        pred = filter_out_locs(pred, loc_path=avoid)\n        logger.info(f\"Retained {len(pred)} of {num_unfiltered} detections.\")\n\n    pred.to_csv(os.path.join(output_dir, \"predictions.csv\"), index=False)\n\n    return None\n"}
{"type": "source_file", "path": "src/models/__init__.py", "content": "from . import custom as custom\nfrom . import resnet as resnet\nfrom . import unet as unet\nfrom . import frcnn_cmp2 as frcnn_cmp2\nfrom . import frcnn as frcnn\nmodels = {}\n\nmodels[\"frcnn\"] = frcnn.FasterRCNNModel\n\nmodels[\"frcnn_cmp2\"] = frcnn_cmp2.FasterRCNNModel\n\nmodels[\"unet\"] = unet.Model\n\nmodels[\"resnet\"] = resnet.Model\n\nmodels[\"custom\"] = custom.Model\nmodels[\"custom_separate_heads\"] = custom.SeparateHeadAttrModel\n"}
{"type": "source_file", "path": "src/models/frcnn.py", "content": "from collections import OrderedDict\n\nimport torch\nimport torchvision\nfrom torchvision.models.detection.backbone_utils import resnet_fpn_backbone\nfrom torchvision.models.detection.faster_rcnn import FasterRCNN, FastRCNNPredictor\n\n\nclass FasterRCNNModel(torch.nn.Module):\n    \"\"\"\n    Baseline model class for xView3 reference implementation.\n    Wraps torchvision faster-rcnn, updates initial layer to handle\n    man arbitrary number of input channels.\n    \"\"\"\n\n    def __init__(self, info):\n        super(FasterRCNNModel, self).__init__()\n\n        options = info[\"Options\"]\n        image_size = info[\"Example\"][0].shape[1]\n        num_classes = len(info[\"Data\"][\"categories\"])\n        num_channels = info[\"Channels\"].count()\n\n        backbone = options.get(\"Backbone\", \"resnet50\")\n        pretrained = options.get(\"Pretrained\", True)\n        pretrained_backbone = options.get(\"Pretrained-Backbone\", True)\n        trainable_backbone_layers = options.get(\"Trainable-Backbone-Layers\", 5)\n        use_noop_transform = options.get(\"NoopTransform\", True)\n\n        # Load in a backbone, with capability to be pretrained on COCO\n        if backbone == \"resnet50\":\n            box_detections_per_img = max(\n                100, 100 * image_size * image_size // 800 // 800\n            )\n            rpn_pre_nms_top_n_train = max(\n                2000, 2000 * image_size * image_size // 800 // 800\n            )\n            rpn_post_nms_top_n_train = max(\n                2000, 2000 * image_size * image_size // 800 // 800\n            )\n            rpn_pre_nms_top_n_test = max(\n                1000, 1000 * image_size * image_size // 800 // 800\n            )\n            rpn_post_nms_top_n_test = max(\n                1000, 1000 * image_size * image_size // 800 // 800\n            )\n\n            self.faster_rcnn = torchvision.models.detection.fasterrcnn_resnet50_fpn(\n                pretrained=pretrained,\n                min_size=image_size,\n                max_size=image_size,\n                pretrained_backbone=pretrained_backbone,\n                trainable_backbone_layers=trainable_backbone_layers,\n                box_detections_per_img=box_detections_per_img,\n                rpn_pre_nms_top_n_train=rpn_pre_nms_top_n_train,\n                rpn_post_nms_top_n_train=rpn_post_nms_top_n_train,\n                rpn_pre_nms_top_n_test=rpn_pre_nms_top_n_test,\n                rpn_post_nms_top_n_test=rpn_post_nms_top_n_test,\n            )\n        else:\n            backbone = resnet_fpn_backbone(\n                backbone_name=backbone,\n                pretrained=pretrained_backbone,\n                trainable_layers=trainable_backbone_layers,\n            )\n            model = FasterRCNN(\n                backbone,\n                num_classes + 1,\n                min_size=image_size,\n                max_size=image_size,\n            )\n\n        # replace the classifier with a new one for user-defined num_classes\n        in_features = self.faster_rcnn.roi_heads.box_predictor.cls_score.in_features\n        self.faster_rcnn.roi_heads.box_predictor = FastRCNNPredictor(\n            in_features, num_classes + 1\n        )\n\n        if use_noop_transform:\n            self.faster_rcnn.transform = NoopTransform()\n\n        print(f\"Using {num_channels} channels for input layer...\")\n        self.num_channels = num_channels\n        if num_channels != 3:\n            # Adjusting initial layer to handle arbitrary number of inputchannels\n            self.faster_rcnn.backbone.body.conv1 = torch.nn.Conv2d(\n                num_channels,\n                self.faster_rcnn.backbone.body.conv1.out_channels,\n                kernel_size=7,\n                stride=2,\n                padding=3,\n                bias=False,\n            )\n\n    def forward(self, images, raw_targets=None):\n        device = images[0].device\n\n        # Fix targets: if there are no labels, then for some reason we need to set one label.\n        # If there are labels, we need to increment it by one, since 0 is reserved for background.\n        targets = None\n        if raw_targets:\n            targets = []\n            for target in raw_targets:\n                target = dict(target)\n                if len(target[\"boxes\"]) == 0:\n                    target[\"labels\"] = torch.zeros(\n                        (1,), device=device, dtype=torch.int64\n                    )\n                else:\n                    target[\"labels\"] = target[\"labels\"] + 1\n                targets.append(target)\n\n        images, targets = self.faster_rcnn.transform(images, targets)\n        features = self.faster_rcnn.backbone(images.tensors)\n        if isinstance(features, torch.Tensor):\n            features = OrderedDict([(\"0\", features)])\n        proposals, proposal_losses = self.faster_rcnn.rpn(images, features, targets)\n        detections, detector_losses = self.faster_rcnn.roi_heads(\n            features, proposals, images.image_sizes, targets\n        )\n\n        losses = {\"base\": torch.tensor(0, device=device, dtype=torch.float32)}\n        losses.update(proposal_losses)\n        losses.update(detector_losses)\n\n        # Fix detections: need to decrement class label.\n        for output in detections:\n            output[\"labels\"] = output[\"labels\"] - 1\n\n        loss = sum(x for x in losses.values())\n        return detections, loss\n\n\nclass NoopTransform(torch.nn.Module):\n    def __init__(self):\n        super(NoopTransform, self).__init__()\n\n        self.transform = (\n            torchvision.models.detection.transform.GeneralizedRCNNTransform(\n                min_size=800,\n                max_size=800,\n                image_mean=[],\n                image_std=[],\n            )\n        )\n\n    def forward(self, images, targets):\n        images = self.transform.batch_images(images, size_divisible=32)\n        image_sizes = [(image.shape[1], image.shape[2]) for image in images]\n        image_list = torchvision.models.detection.image_list.ImageList(\n            images, image_sizes\n        )\n        return image_list, targets\n\n    def postprocess(self, detections, image_sizes, orig_sizes):\n        return detections\n"}
{"type": "source_file", "path": "src/main.py", "content": "\"\"\"VIIRS Vessel Detection Service\n\"\"\"\nfrom __future__ import annotations\n\nimport logging\nimport logging.config\nimport os\nfrom datetime import datetime\nfrom tempfile import TemporaryDirectory\nfrom time import perf_counter\nfrom typing import Optional\n\nimport torch\nimport uvicorn\nimport yaml\nfrom fastapi import FastAPI, Response\nfrom pydantic import BaseModel\n\nfrom data.image import prepare_scenes\nfrom inference.pipeline import detect_vessels\n\napp = FastAPI()\n\nlogger = logging.getLogger(__name__)\n\n\nCONFIG_PATH = os.path.join(\n    os.path.dirname(os.path.realpath(__file__)), \"src\", \"config\", \"config.yml\"\n)\n\nHOST = \"0.0.0.0\"  # nosec B104\nPORT = os.getenv(\"SVD_PORT\", default=5557)\n\nMODEL_VERSION = datetime.today()  # concatenate with git hash\n\n\nwith open(CONFIG_PATH, \"r\") as file:\n    config = yaml.safe_load(file)[\"main\"]\n\n\nclass SVDResponse(BaseModel):\n    \"\"\"Response object for vessel detections\"\"\"\n\n    status: str\n\n\nclass SVDRequest(BaseModel):\n    \"\"\"Request object for vessel detections\"\"\"\n\n    scene_id: str  # S2A_MSIL1C_20230108T060231_N0509_R091_T42RUN_20230108T062956.SAFE\n    output_dir: str\n    raw_path: str\n    force_cpu: Optional[bool] = False\n    historical1: Optional[str] = None\n    historical2: Optional[str] = None\n    gcp_bucket: Optional[str] = None\n    window_size: Optional[int] = 2048\n    padding: Optional[int] = 400\n    overlap: Optional[int] = 20\n    avoid: Optional[bool] = False\n    nms_thresh: Optional[float] = 10\n    conf: Optional[float] = 0.9\n    save_crops: Optional[bool] = True\n    detector_batch_size: int = 4\n    postprocessor_batch_size: int = 32\n    debug_mode: Optional[bool] = False\n    remove_clouds: Optional[bool] = False\n\n\n@app.on_event(\"startup\")\nasync def sentinel_init() -> None:\n    \"\"\"Sentinel Vessel Service Initialization\"\"\"\n    logger.info(\"Loading model\")\n\n\nasync def load_sentinel1_model() -> dict:\n    global current_model\n    current_model = \"sentinel1\"\n    load_sentinel1_model()\n\n\nasync def load_sentinel2_model() -> dict:\n    global current_model\n    current_model = \"sentinel2\"\n    load_sentinel2_model()\n\n\n@app.post(\"/detections\", response_model=SVDResponse)\nasync def get_detections(info: SVDRequest, response: Response) -> SVDResponse:\n    \"\"\"Returns vessel detections Response object for a given Request object\"\"\"\n    start = perf_counter()\n    scene_id = info.scene_id\n    raw_path = info.raw_path\n    output = info.output_dir\n\n    with TemporaryDirectory() as tmpdir:\n        if not os.path.exists(output):\n            logger.debug(f\"Creating output dir at {output}\")\n            os.makedirs(output)\n\n        scratch_path = tmpdir\n        device = torch.device(\n            \"cuda\" if torch.cuda.is_available() and not info.force_cpu else \"cpu\"\n        )\n\n        cat_path = os.path.join(scratch_path, scene_id + \"_cat.npy\")\n        base_path = os.path.join(scratch_path, scene_id + \"_base.tif\")\n        catalog = \"sentinel\" + scene_id[1]  # the second char contains a 1 or 2\n\n        detector_model_dir = config[f\"{catalog}_detector\"]\n        postprocess_model_dir = config[f\"{catalog}_postprocessor\"]\n\n        if not os.path.exists(cat_path) or not os.path.exists(base_path):\n            logger.info(\"Preprocessing raw scenes.\")\n            img_array = prepare_scenes(\n                raw_path,\n                scratch_path,\n                scene_id,\n                info.historical1,\n                info.historical2,\n                catalog,\n                cat_path,\n                base_path,\n                device,\n                detector_model_dir,\n                postprocess_model_dir,\n            )\n\n        # Run inference.\n        detect_vessels(\n            detector_model_dir,\n            postprocess_model_dir,\n            raw_path,\n            scene_id,\n            img_array,\n            base_path,\n            output,\n            info.window_size,\n            info.padding,\n            info.overlap,\n            info.conf,\n            info.nms_thresh,\n            info.save_crops,\n            device,\n            catalog,\n            info.avoid,\n            info.remove_clouds,\n            info.detector_batch_size,\n            info.postprocessor_batch_size,\n            debug_mode=info.debug_mode,\n        )\n\n        status = str(200)\n\n    elapsed_time = perf_counter() - start\n    logger.info(f\"SVD {elapsed_time=}, found detections)\")\n\n    return SVDResponse(\n        status=status,\n    )\n\n\nif __name__ == \"__main__\":\n    uvicorn.run(\"main:app\", host=HOST, port=PORT, proxy_headers=True)\n"}
{"type": "source_file", "path": "example/s1_request.py", "content": "\"\"\" Use this script to inference the API with locally stored data\n\n\n\"\"\"\nimport json\nimport os\nimport time\n\nimport requests\n\nPORT = os.getenv(\"SVD_PORT\", default=5557)\nSVD_ENDPOINT = f\"http://localhost:{PORT}/detections\"\nSAMPLE_INPUT_DIR = \"/home/vessel_detection/raw_data/\"\nSAMPLE_OUTPUT_DIR = \"/home/vessel_detection/s1_example/output\"\nSCENE_ID = \"S1A_IW_GRDH_1SDV_20221002T205126_20221002T205156_045268_056950_0664.SAFE\"\nHISTORICAL_1 = (\n    \"S1A_IW_GRDH_1SDV_20221014T205126_20221014T205155_045443_056F2D_861B.SAFE\"\n)\nHISTORICAL_2 = (\n    \"S1A_IW_GRDH_1SDV_20221026T205127_20221026T205156_045618_05745F_B918.SAFE\"\n)\nTIMEOUT_SECONDS = 600\n\n\ndef sample_request() -> None:\n    \"\"\"Sample request for files stored locally\"\"\"\n    start = time.time()\n\n    REQUEST_BODY = {\n        \"raw_path\": SAMPLE_INPUT_DIR,\n        \"output_dir\": SAMPLE_OUTPUT_DIR,\n        \"scene_id\": SCENE_ID,\n        \"historical_1\": HISTORICAL_1,\n        \"historical_2\": HISTORICAL_2,\n    }\n\n    response = requests.post(SVD_ENDPOINT, json=REQUEST_BODY, timeout=TIMEOUT_SECONDS)\n    output_filename = os.path.join(\n        os.path.dirname(os.path.realpath(__file__)), \"sample_s1_response.json\"\n    )\n    if response.ok:\n        with open(output_filename, \"w\") as outfile:\n            json.dump(response.json(), outfile)\n    end = time.time()\n    print(f\"elapsed time: {end-start}\")\n\n\nif __name__ == \"__main__\":\n    sample_request()\n"}
{"type": "source_file", "path": "src/data/tiles.py", "content": "import os.path\n\nimport numpy\nimport skimage.io\n\n\ndef load_window(\n    image_uuid: str,\n    channel: dict,\n    column: int,\n    row: int,\n    width: int,\n    height: int,\n    chip_size: int = 512,\n    preprocess_dir=\"../data/preprocess\",\n):\n    \"\"\"Stitch together an image from multiple tiles.\n\n    Parameters\n    ----------\n    image_uuid: str\n        UUID for image to load.\n\n    channel: dict\n\n    column: int\n\n    row: int\n\n    width: int\n\n    height: int\n\n    chip_size: int\n\n    preprocess_dir: str\n        Directory housing preprocessed images (warped images, broken into tiles).\n\n    Returns\n    -------\n    im: numpy.ndarray\n        Total image window.\n    \"\"\"\n    # Initialize array for output.\n    # We need different cases for single-channel image and multi-channel image.\n    if channel[\"Count\"] == 1:\n        im = numpy.zeros((height, width), dtype=numpy.uint8)\n    else:\n        im = numpy.zeros((height, width, channel[\"Count\"]), dtype=numpy.uint8)\n\n    # Load tiles one at a time.\n    start_tile = (column // chip_size, row // chip_size)\n    end_tile = ((column + width - 1) // chip_size, (row + height - 1) // chip_size)\n    for i in range(start_tile[0], end_tile[0] + 1):\n        for j in range(start_tile[1], end_tile[1] + 1):\n            fname = os.path.join(\n                preprocess_dir,\n                \"{}/{}/{}_{}.png\".format(image_uuid, channel[\"Name\"], i, j),\n            )\n            if not os.path.exists(fname):\n                continue\n\n            cur_im = skimage.io.imread(fname)\n            cur_col_off = chip_size * i\n            cur_row_off = chip_size * j\n\n            src_col_offset = max(column - cur_col_off, 0)\n            src_row_offset = max(row - cur_row_off, 0)\n            dst_col_offset = max(cur_col_off - column, 0)\n            dst_row_offset = max(cur_row_off - row, 0)\n            col_overlap = min(cur_im.shape[1] - src_col_offset, width - dst_col_offset)\n            row_overlap = min(cur_im.shape[0] - src_row_offset, height - dst_row_offset)\n            im[\n                dst_row_offset: dst_row_offset + row_overlap,\n                dst_col_offset: dst_col_offset + col_overlap,\n            ] = cur_im[\n                src_row_offset: src_row_offset + row_overlap,\n                src_col_offset: src_col_offset + col_overlap,\n            ]\n    return im\n"}
{"type": "source_file", "path": "example/s2_request.py", "content": "\"\"\" Use this script to inference the API with locally stored data\n\n\"\"\"\nimport json\nimport os\nimport time\n\nimport requests\n\nPORT = os.getenv(\"SVD_PORT\", default=5557)\nSVD_ENDPOINT = f\"http://localhost:{PORT}/detections\"\nSAMPLE_INPUT_DIR = \"/home/vessel_detection/raw_data/\"\nSAMPLE_OUTPUT_DIR = \"/home/vessel_detection/s2_example/output\"\nSCENE_ID = \"S2A_MSIL1C_20221125T112411_N0400_R037_T30VXK_20221125T132446.SAFE\"\nTIMEOUT_SECONDS = 600\nDEBUG_MODE = True\nREMOVE_CLOUDS = True\n\n\ndef sample_request() -> None:\n    \"\"\"Sample request for files stored locally\"\"\"\n    start = time.time()\n\n    REQUEST_BODY = {\n        \"raw_path\": SAMPLE_INPUT_DIR,\n        \"output_dir\": SAMPLE_OUTPUT_DIR,\n        \"scene_id\": SCENE_ID,\n        \"debug_mode\": DEBUG_MODE,\n        \"remove_clouds\": REMOVE_CLOUDS,\n    }\n\n    response = requests.post(SVD_ENDPOINT, json=REQUEST_BODY, timeout=TIMEOUT_SECONDS)\n    output_filename = os.path.join(\n        os.path.dirname(os.path.realpath(__file__)), \"sample_s2_response.json\"\n    )\n    if response.ok:\n        with open(output_filename, \"w\") as outfile:\n            json.dump(response.json(), outfile)\n    end = time.time()\n    print(f\"elapsed time: {end-start}\")\n\n\nif __name__ == \"__main__\":\n    sample_request()\n"}
{"type": "source_file", "path": "src/data/transforms/__init__.py", "content": "class Compose(object):\n    def __init__(self, transforms):\n        self.transforms = transforms\n\n    def __call__(self, image, targets):\n        for transform in self.transforms:\n            image, targets = transform(image, targets)\n        return image, targets\n\n\ndef get_transform(model_cfg, options, transforms_cfg):\n    from . import augment as siv_transforms\n\n    if len(transforms_cfg) == 0:\n        return None\n\n    transforms = []\n    for transform_cfg in transforms_cfg:\n        transform_cls = getattr(siv_transforms, transform_cfg[\"Name\"])\n        transform = transform_cls(model_cfg, options, transform_cfg)\n        transforms.append(transform)\n    return Compose(transforms)\n"}
{"type": "source_file", "path": "src/models/resnet.py", "content": "import torch\nimport torchvision\n\n\nclass Model(torch.nn.Module):\n    def __init__(self, info):\n        super(Model, self).__init__()\n\n        self.task = info[\"Data\"][\"task\"]\n        num_channels = info[\"Channels\"].count()\n\n        options = info[\"Options\"]\n        resnet_mode = options.get(\"Mode\", \"resnet50\")\n\n        if self.task == \"regression\":\n            self.num_classes = 1\n        else:\n            self.num_classes = len(info[\"Data\"][\"categories\"])\n\n        resnet_fn = None\n        if resnet_mode == \"resnet18\":\n            resnet_fn = torchvision.models.resnet.resnet18\n        elif resnet_mode == \"resnet34\":\n            resnet_fn = torchvision.models.resnet.resnet34\n        elif resnet_mode == \"resnet50\":\n            resnet_fn = torchvision.models.resnet.resnet50\n        elif resnet_mode == \"resnet101\":\n            resnet_fn = torchvision.models.resnet.resnet101\n        elif resnet_mode == \"resnet152\":\n            resnet_fn = torchvision.models.resnet.resnet152\n\n        self.resnet = resnet_fn(\n            pretrained=True,\n        )\n        self.resnet.conv1 = torch.nn.Conv2d(\n            num_channels,\n            self.resnet.conv1.out_channels,\n            kernel_size=7,\n            stride=2,\n            padding=3,\n            bias=False,\n        )\n\n        # We could pass num_classes to resnet_fn, but then it doesn't work with loading pre-trained model.\n        # So instead we override the fully-connected layer here.\n        self.resnet.fc = torch.nn.Linear(self.resnet.fc.in_features, self.num_classes)\n\n    def forward(self, images, targets=None):\n        images = torch.stack(images, dim=0)\n        output = self.resnet(images)  # batch x self.num_classes\n\n        if self.task == \"regression\":\n            loss = None\n            if targets:\n                targets = torch.stack([target[\"target\"] for target in targets], dim=0)\n                loss = torch.mean(torch.square(output[:, 0] - targets))\n\n            return output[:, 0], loss\n        elif self.task == \"classification\":\n            loss = None\n            if targets:\n                targets = torch.stack([target[\"target\"] for target in targets], dim=0)\n                loss = torch.nn.functional.cross_entropy(input=output, target=targets)\n\n            return output, loss\n"}
{"type": "source_file", "path": "src/training/__init__.py", "content": ""}
{"type": "source_file", "path": "src/models/unet.py", "content": "import torch\n\n\nclass Model(torch.nn.Module):\n    def __init__(self, info):\n        super(Model, self).__init__()\n\n        num_channels = info[\"Channels\"].count()\n\n        def down_layer(in_channels, out_channels):\n            return torch.nn.Sequential(\n                torch.nn.Conv2d(in_channels, out_channels, 4, stride=2, padding=1),\n                torch.nn.ReLU(inplace=True),\n                torch.nn.Conv2d(out_channels, out_channels, 3, padding=1),\n                torch.nn.BatchNorm2d(out_channels, eps=1e-3, momentum=0.03),\n                torch.nn.ReLU(inplace=True),\n            )\n\n        def up_layer(in_channels, out_channels):\n            return torch.nn.Sequential(\n                torch.nn.Conv2d(in_channels, in_channels // 2, 3, padding=1),\n                torch.nn.ReLU(inplace=True),\n                torch.nn.BatchNorm2d(in_channels // 2, eps=1e-3, momentum=0.03),\n                torch.nn.ConvTranspose2d(\n                    in_channels // 2, out_channels, 4, stride=2, padding=1\n                ),\n                torch.nn.ReLU(inplace=True),\n            )\n\n        self.layers = torch.nn.Sequential(\n            down_layer(num_channels, 32),  # 1/2\n            down_layer(32, 64),  # 1/4\n            down_layer(64, 128),  # 1/8\n            down_layer(128, 256),  # 1/16\n            up_layer(256, 128),  # 1/8\n            up_layer(128, 64),  # 1/4\n            up_layer(64, 32),  # 1/2\n            up_layer(32, 32),  # 1,\n            torch.nn.Conv2d(32, 1, 3, padding=1),\n        )\n\n    def forward(self, images, targets=None):\n        images = torch.stack(images, dim=0)\n        output = self.layers(images)[:, 0, :, :]\n\n        loss = None\n        if targets:\n            targets = torch.stack([target[\"target\"] for target in targets], dim=0)\n            loss = torch.mean(torch.square(output - targets))\n\n        return output, loss\n"}
{"type": "source_file", "path": "src/models/custom.py", "content": "import torch\nimport torchvision\n\nfrom src.data.dataset import NEGATIVE_VESSEL_TYPE_FACTOR\n\n\nclass Model(torch.nn.Module):\n    def __init__(self, info):\n        super(Model, self).__init__()\n\n        num_channels = info[\"Channels\"].count()\n        self.num_channels = num_channels\n        self.resnet = torchvision.models.resnet.resnet50(\n            weights=torchvision.models.ResNet50_Weights.IMAGENET1K_V1\n        )\n        self.resnet.conv1 = torch.nn.Conv2d(\n            num_channels,\n            self.resnet.conv1.out_channels,\n            kernel_size=7,\n            stride=2,\n            padding=3,\n            bias=False,\n        )\n        self.resnet.fc = torch.nn.Sequential(\n            torch.nn.Linear(self.resnet.fc.in_features, 256),\n            torch.nn.ReLU(),\n            torch.nn.Linear(\n                256, 1 + 1 + 16 + 1 + 2\n            ),  # loss, width, heading classes, speed, is_fishing_vessel classes\n        )\n\n    def forward(self, images, targets=None):\n        images = torch.stack(images, dim=0)\n        output = self.resnet(images)  # batch x self.num_classes\n\n        loss = None\n        if targets:\n            targets = torch.stack([target[\"target\"] for target in targets], dim=0)\n            loss_length = torch.mean(torch.abs(output[:, 0] - targets[:, 0]))\n            loss_width = torch.mean(torch.abs(output[:, 1] - targets[:, 1]))\n            loss_heading = torch.nn.functional.cross_entropy(\n                input=output[:, 2:18], target=targets[:, 2:18]\n            )\n            loss_speed = torch.mean(torch.abs(output[:, 18] - targets[:, 18]))\n            loss_type = torch.nn.functional.cross_entropy(\n                input=output[:, 19:21],\n                target=targets[:, 19:21],\n                weight=torch.tensor([1, NEGATIVE_VESSEL_TYPE_FACTOR]).to(\n                    targets.device\n                ),\n            )\n            loss = loss_length + loss_width + loss_speed + loss_type + loss_heading\n\n        return output, loss\n\n\nclass SeparateHeadAttrModel(torch.nn.Module):\n    def __init__(self, info):\n        super().__init__()\n\n        num_channels = info[\"Channels\"].count()\n        self.num_channels = num_channels\n\n        def down_layer(in_channels, out_channels, stride=2):\n            return torch.nn.Sequential(\n                torch.nn.Conv2d(in_channels, out_channels, 4, stride=stride, padding=1),\n                torch.nn.BatchNorm2d(out_channels, eps=1e-3, momentum=0.03),\n                torch.nn.ReLU(inplace=True),\n            )\n\n        self.layer1 = down_layer(num_channels, 32, stride=1)\n        self.layer2 = down_layer(32, 64)\n        self.layer3 = down_layer(64, 128)\n        self.layer4 = down_layer(128, 256)\n        self.layer5 = down_layer(256, 512)\n        self.layer6 = down_layer(512, 512)\n\n        self.pred_length = torch.nn.Conv2d(512, 1, 4, stride=2, padding=1)\n        self.pred_width = torch.nn.Conv2d(512, 1, 4, stride=2, padding=1)\n        self.pred_heading = torch.nn.Conv2d(512, 16, 4, stride=2, padding=1)\n        self.pred_speed = torch.nn.Conv2d(512, 1, 4, stride=2, padding=1)\n        self.pred_activity_type = torch.nn.Conv2d(512, 2, 4, stride=2, padding=1)\n\n    def forward(self, images, targets=None):\n        images = torch.stack(images, dim=0)\n        device = images.device\n        layer1 = self.layer1(images)\n        layer2 = self.layer2(layer1)\n        layer3 = self.layer3(layer2)\n        layer4 = self.layer4(layer3)\n        layer5 = self.layer5(layer4)\n        layer6 = self.layer6(layer5)\n\n        lengths = self.pred_length(layer6)[:, :1, 0, 0]\n        widths = self.pred_width(layer6)[:, :1, 0, 0]\n        heading_bucket_preds = self.pred_heading(layer6)[:, :, 0, 0]\n        speeds = self.pred_speed(layer6)[:, :1, 0, 0]\n        activity_type = self.pred_activity_type(layer6)[:, :, 0, 0]\n\n        output = torch.cat([lengths, widths, heading_bucket_preds, speeds, activity_type], dim=-1)\n\n        loss = None\n        if targets:\n            targets = torch.stack([target[\"target\"] for target in targets], dim=0)\n\n            def get_normalized_pe(labels, preds):\n                \"\"\"Get a normalized percent error for labels which ought to be > 0.\n                \"\"\"\n                valid_indices = labels >= 0\n                valid_labels = labels[valid_indices]\n                valid_preds = preds[valid_indices]\n                if len(valid_labels) > 0:\n                    loss = torch.div(\n                        torch.abs(valid_labels - valid_preds),\n                        valid_labels).mean()\n                else:\n                    loss = torch.zeros((1,), dtype=torch.float32, device=device)\n                return loss\n\n            # Length\n            length_labels = targets[:, 0]\n            loss_length = get_normalized_pe(length_labels, lengths)\n\n            # Width\n            width_labels = targets[:, 1]\n            loss_width = get_normalized_pe(width_labels, widths)\n\n            # Speed\n            speed_labels = targets[:, 18]\n            loss_speed = torch.mean(torch.abs(speeds - speed_labels))\n\n            # Heading\n            heading_labels = targets[:, 2:18]\n            loss_heading = torch.nn.functional.cross_entropy(heading_bucket_preds, heading_labels)\n\n            # Activity type\n            activity_labels = targets[:, 19:21]\n            loss_activity_type = torch.nn.functional.cross_entropy(activity_type, activity_labels, weight=torch.tensor([\n                                                                   1, NEGATIVE_VESSEL_TYPE_FACTOR]).to(targets.device))\n\n            loss = loss_length + loss_width + loss_speed + loss_heading + loss_activity_type\n\n        return output, loss\n"}
{"type": "source_file", "path": "src/data/warp.py", "content": "import math\nimport subprocess\nimport typing as t\n\nfrom osgeo import gdal\n\nweb_mercator_m = 2 * math.pi * 6378137\n\n\nclass ImageInfo(object):\n    \"\"\"Container for image metadata.\"\"\"\n\n    def __init__(self, width, height, bounds, column, row, zoom, projection=None):\n        self.width = width\n        self.height = height\n        self.bounds = bounds\n        self.column = column\n        self.row = row\n        self.zoom = zoom\n        self.projection = projection\n\n\ndef warp(\n    image, in_path: str, out_path: str, projection: t.Optional[str] = None\n) -> ImageInfo:\n    \"\"\"Warp a raw image to the specified projection.\n\n    Parameters\n    ----------\n    image: src.data.retrieve.RetrieveImage\n        Image class to warp.\n\n    in_path: str\n        Path to input raster geotiff.\n\n    out_path: str\n        Path to output raster geotiff.\n\n    projection: Optional[str]\n        Desired output projection (e.g. 'epsg:3857' for pseudo-mercator).\n        If None or '', the files are converted without warping.\n\n    Returns\n    -------\n     : ImageInfo\n        Class containing warped image information.\n    \"\"\"\n\n    def get_pixel_size():\n        \"\"\"Returns pixel size provided in metadata, or if unavailable, computes\n        pixel size from the input raster.\n        \"\"\"\n        if image.pixel_size:\n            return image.pixel_size\n\n        raster = gdal.Open(in_path)\n        geo_transform = raster.GetGeoTransform()\n        pixel_size_x = geo_transform[1]\n        pixel_size_y = -geo_transform[5]\n        return min(pixel_size_x, pixel_size_y)\n\n    if not projection:\n        stdout = subprocess.check_output([\"gdalwarp\", in_path, out_path, \"-overwrite\"])\n        return get_image_info(out_path)\n    elif projection == \"epsg:3857\":\n        # Determine desired output resolution.\n        # We scale up to the zoom level just above the native resolution.\n        # This takes up more space than needed, but ensures we don't \"lose\" any resolution.\n        in_pixel_size = get_pixel_size()\n        out_pixel_size = None\n        out_zoom = None\n\n        for zoom in range(20):\n            zoom_pixel_size = web_mercator_m / 512 / (2**zoom)\n            out_pixel_size = zoom_pixel_size\n            out_zoom = zoom\n            if out_pixel_size < in_pixel_size * 1.1:\n                break\n\n        # Warp the input image.\n        stdout = subprocess.check_output(\n            [\n                \"gdalwarp\",\n                \"-r\",\n                \"bilinear\",\n                \"-t_srs\",\n                projection,\n                \"-tr\",\n                str(out_pixel_size),\n                str(out_pixel_size),\n                in_path,\n                out_path,\n                \"-overwrite\",\n            ]\n        )\n        # logger.debug(stdout)\n\n        raster = gdal.Open(out_path)\n        geo_transform = raster.GetGeoTransform()\n        offset_x = geo_transform[0] + web_mercator_m / 2\n        offset_y = web_mercator_m - (geo_transform[3] + web_mercator_m / 2)\n        offset_x /= out_pixel_size\n        offset_y /= out_pixel_size\n\n        return ImageInfo(\n            width=raster.RasterXSize,\n            height=raster.RasterYSize,\n            bounds=get_wgs84_bounds(raster),\n            column=int(offset_x),\n            row=int(offset_y),\n            zoom=out_zoom,\n            projection=projection,\n        )\n\n    else:\n        raise Exception(\"unknown projection {}\".format(projection))\n\n\ndef get_image_info(fname: str) -> ImageInfo:\n    \"\"\"Get image information, assuming no projection.\n\n    Parameters\n    ----------\n    fname: str\n        Path to image.\n\n    Returns\n    -------\n    : ImageInfo\n        Image metadata.\n    \"\"\"\n    raster = gdal.Open(fname)\n    return ImageInfo(\n        width=raster.RasterXSize,\n        height=raster.RasterYSize,\n        bounds=get_wgs84_bounds(raster),\n        column=0,\n        row=0,\n        zoom=0,\n    )\n\n\ndef get_wgs84_bounds(raster) -> dict:\n    \"\"\"Get WGS84 bounding box given gdal raster.\n\n    Parameters\n    ----------\n    raster: gdal.raster\n        Gdal image raster.\n\n    Returns\n    -------\n    : dict\n        Dictionary containing minimal and maximal lat/lon coords of raster.\n    \"\"\"\n    transformer = gdal.Transformer(raster, None, [\"DST_SRS=WGS84\"])\n    _, p1 = transformer.TransformPoint(0, 0, 0, 0)\n    _, p2 = transformer.TransformPoint(0, 0, raster.RasterYSize, 0)\n    _, p3 = transformer.TransformPoint(0, raster.RasterXSize, 0, 0)\n    _, p4 = transformer.TransformPoint(0, raster.RasterXSize, raster.RasterYSize, 0)\n    points = [p1, p2, p3, p4]\n    return {\n        \"Min\": {\n            \"Lon\": min([p[0] for p in points]),\n            \"Lat\": min([p[1] for p in points]),\n        },\n        \"Max\": {\n            \"Lon\": max([p[0] for p in points]),\n            \"Lat\": max([p[1] for p in points]),\n        },\n    }\n"}
{"type": "source_file", "path": "src/utils/download/__init__.py", "content": ""}
{"type": "source_file", "path": "download_imagery.py", "content": "import argparse\nimport logging\nimport os\nimport sqlite3\nimport sys\nimport tempfile\nimport typing as t\n\nfrom src.utils.db import dict_factory, get_image, get_windows\nfrom src.utils.download.sentinel1 import retrieve, warp_and_print\n\n# Configure logger\nlogger = logging.getLogger(\"downloader\")\nlogger.setLevel(logging.INFO)\nformatter = logging.Formatter(\"%(asctime)s (%(name)s) (%(levelname)s): %(message)s\")\nstream_handler = logging.StreamHandler(sys.stdout)\nstream_handler.setLevel(logging.INFO)\nstream_handler.setFormatter(formatter)\nlogger.addHandler(stream_handler)\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\"Raw imagery download script.\")\n\n    # General\n    parser.add_argument(\n        \"--save_dir\",\n        help=\"Path to dir where data will be saved.\",\n        default=\"./training_data\",\n    )\n    parser.add_argument(\n        \"--db_path\",\n        help=\"Path to sqlite DB with labels\",\n        default=\"./data/metadata.sqlite3\",\n    )\n\n    args = parser.parse_args()\n    return args\n\n\ndef get_corners(im: dict) -> t.Tuple[float, float, float, float]:\n    \"\"\"Get min and max corner coords of image.\n\n    Parameters\n    ----------\n    im: dict\n        Dictionary w/ column, row, width and height keys.\n\n    Returns\n    -------\n    x_min: float\n        Minimal x coordinate.\n\n    y_min: float\n        Minimal y coordinate.\n\n    x_max: float\n        Maximal x coordinate.\n\n    y_max: float\n        Maximal y coordinate.\n    \"\"\"\n    x_min, y_min = im[\"column\"], im[\"row\"]\n    x_max, y_max = im[\"column\"] + im[\"width\"], im[\"row\"] + im[\"height\"]\n    return x_min, y_min, x_max, y_max\n\n\ndef has_intersection(im1: dict, im2: dict) -> bool:\n    \"\"\"Check whether two image records have nonempty intersection.\n\n    Parameters\n    ----------\n    im1: dict\n        Dictionary for one image w/ column, row, width and height keys.\n\n    im2: dict\n        Dictionary for second image w/ column, row, width and height keys.\n\n    Returns\n    -------\n    : bool\n        True if the images have nonempty intersection.\n    \"\"\"\n    x1_min, y1_min, x1_max, y1_max = get_corners(im1)\n    x2_min, y2_min, x2_max, y2_max = get_corners(im2)\n\n    return x2_min < x1_max and x1_min < x2_max and y2_min < y1_min and y1_min < y2_max\n\n\ndef produce_overlaps(\n    images: t.List[dict], overlap_options: t.List[dict], save_dir: str\n) -> None:\n    \"\"\"Produce overlaps for preprocessed images, if not already populated.\n\n    Parameters\n    ----------\n    images: list[dict]\n        List of image dictionaries, where image dictionaries correspond to image schema in metadata sqlite.\n\n    overlap_options: list[dict]\n        List of image dictionaries, where image dictionaries correspond to image schema in metadata sqlite.\n\n    save_dir: str\n        Directory in which preprocess data lives.\n\n    Returns\n    -------\n    : None\n    \"\"\"\n    # Produce overlaps for preprocessed images, if not already populated.\n    targets = images\n    chip_size = 512\n    count = 2\n    channels = [\"vv\", \"vh\"]\n    for channel in channels:\n        for target in targets:\n            # Filter options for those which are not the image\n            # in question, but overlap the image in question.\n            filtered_overlap_options = []\n            for option in overlap_options:\n                if option[\"id\"] == target[\"id\"]:\n                    continue\n                if not has_intersection(target, option):\n                    continue\n                filtered_overlap_options.append(option)\n\n            if len(filtered_overlap_options) > 0:\n                # Account Tiles\n                # Produces available_images, a map from (column, row) specifying a\n                # tile, to corresponding list of image indices that contain\n                # that tile.\n                available_images = {}\n                for idx, image in enumerate(filtered_overlap_options):\n                    hits = {}\n                    # get available tiles for image, channels\n                    files = os.listdir(\n                        os.path.join(save_dir, f\"preprocess/{image['uuid']}/{channel}\")\n                    )\n                    for file in files:\n                        if \".png\" not in file:\n                            continue\n                        parts = file.split(\".png\")[0].split(\"_\")\n                        column = int(parts[0])\n                        row = int(parts[1])\n                        hits[(column, row)] = hits.get((column, row), 0) + 1\n                    for key, count in hits.items():\n                        available_images[key] = available_images.get(key, [])\n                        available_images[key].append(idx)\n                # Get bounds of current image, in terms of chips\n                min_tile = (target[\"column\"] // chip_size, target[\"row\"] // chip_size)\n                max_tile = (\n                    (target[\"column\"] + target[\"width\"] - 1) // chip_size,\n                    (target[\"row\"] + target[\"height\"] - 1) // chip_size,\n                )\n\n                # Create overlap channels\n                logger.info(\n                    f\"Creating overlap {channel} channels for image w/ uuid={target['uuid']}.\"\n                )\n                for count_idx in range(count):\n                    syms = 0\n                    channel_name = f\"{channel}_overlap{count_idx}\"\n                    logger.info(f\"Creating overlap channel: {channel_name}.\")\n                    if \"overlap\" in channel_name and channel in channel_name:\n                        if not os.path.isdir(\n                            os.path.join(\n                                save_dir, f\"preprocess/{target['uuid']}/{channel_name}\"\n                            )\n                        ):\n                            os.mkdir(\n                                os.path.join(\n                                    save_dir,\n                                    f\"preprocess/{target['uuid']}/{channel_name}\",\n                                )\n                            )\n                            x = min_tile[0]\n                            y = min_tile[1]\n                            while x < max_tile[0]:\n                                while y < max_tile[1]:\n                                    cur_options = available_images.get((x, y), [])\n                                    if len(cur_options) < count_idx + 1:\n                                        y += 1\n                                        continue\n                                    option = filtered_overlap_options[\n                                        cur_options[count_idx]\n                                    ]\n                                    src = os.path.join(\n                                        save_dir,\n                                        \"preprocess\",\n                                        option[\"uuid\"],\n                                        channel,\n                                        f\"{x}_{y}.png\",\n                                    )\n                                    dst = os.path.join(\n                                        save_dir,\n                                        \"preprocess\",\n                                        target[\"uuid\"],\n                                        channel_name,\n                                        f\"{x}_{y}.png\",\n                                    )\n                                    os.symlink(src, dst)\n                                    syms += 1\n                                    y += 1\n                                x += 1\n                            logger.info(f\"Created {syms} symlinks.\")\n                        else:\n                            logger.info(\n                                f\"Overlap channel {channel_name} already exists.\"\n                            )\n                    count_idx += 1\n\n    return None\n\n\ndef main(save_dir: str, db_path: str) -> None:\n    \"\"\"Run imagery download script for training data.\n\n    Parameters\n    ----------\n    save_dir: str\n        Path to directory in which downloaded data will be stored.\n\n    db_path: str\n        Path to metadata sqlite file defining images to be downloaded.\n\n    Returns\n    -------\n    : None\n    \"\"\"\n\n    # Create training data dir\n    if not os.path.exists(save_dir):\n        # logger.debug(f\"Creating data dir at {save_dir}\")\n        os.makedirs(save_dir, exist_ok=True)\n\n    # Instantiate DB conn\n    conn = sqlite3.connect(db_path)\n    conn.row_factory = dict_factory\n\n    # Get fixed list of windows corresponding to splits/datasets we care about\n    dataset_id = 1\n    splits = [\n        \"jan-march-may-2022-point-train\",\n        \"jan-march-may-2022-point-val\",\n        \"jun-july-aug-2022-point-train\",\n        \"jun-july-aug-2022-point-val\",\n        \"apr-2022-point-train\",\n        \"apr-2022-point-val\",\n        \"nov-2021-point-train\",  # In long term storage on Copernicus Hub at time of writing\n        \"nov-2021-point-val\",  # In long term storage on Copernicus Hub at time of writing\n        \"jun-2020-point-train\",  # In long term storage on Copernicus Hub at time of writing\n        \"jun-2020-point-val\",  # In long term storage on Copernicus Hub at time of writing\n    ]\n    windows = []\n    for split in splits:\n        windows.extend(get_windows(conn, dataset_id, split=split))\n    logger.info(f\"Collecting images corresponding to {len(windows)} windows.\")\n\n    # Get fixed list of corresponding images, and their uuids\n    image_ids = set([win[\"image_id\"] for win in windows])\n    images = [get_image(conn, id) for id in image_ids]\n    logger.info(f\"Found {len(images)} such images to synchronize to local.\\n\")\n\n    # Loop over targets in batches of fixed size, and download and preprocess\n    def batcher(lst, batch_size):\n        return (lst[pos: pos + batch_size] for pos in range(0, len(lst), batch_size))\n\n    batch_size = 8\n    workers = 4\n    synced = 0\n    for tgt_batch in batcher(images, batch_size=batch_size):\n        logger.info(f\"Currently syncing {len(tgt_batch)} images...\")\n        with tempfile.TemporaryDirectory() as tmp_dir:\n            retrieved_images = retrieve(tgt_batch, tmp_dir, save_dir, workers=workers)\n            warp_and_print(\n                retrieved_images, tmp_dir, save_dir, roi=None, workers=workers\n            )\n        logger.info(\"...done\\n\")\n        synced += len(tgt_batch)\n        logger.info(f\"{synced}/{len(images)} total images synced.\")\n\n    produce_overlaps(images, images, save_dir)\n\n    return None\n\n\nif __name__ == \"__main__\":\n    args = parse_args()\n    args_dict = vars(args)\n    main(**args_dict)\n"}
{"type": "source_file", "path": "src/utils/cloud_mask.py", "content": "import os\n\nimport cv2\nimport numpy as np\nimport rasterio\nfrom s2cloudless import S2PixelCloudDetector\n\n\nclass S2CloudMask:\n    def __init__(self, directory_path):\n        self.img_array = self.get_img_data_as_array(directory_path)\n\n    def resize_image(self, input_image: np.array, output_size: tuple):\n        \"\"\"Resize a numpy array image to a specific size using nearest neighbor interpolation.\n        Args:\n            input_image (np.array): The input image represented as a numpy array.\n            output_size (tuple): The desired output size as a tuple (width, height).\n        Returns:\n            np.array: The resized image as a numpy array.\n        \"\"\"\n        resized_image = cv2.resize(\n            input_image, output_size, interpolation=cv2.INTER_NEAREST\n        )\n        return resized_image\n\n    def get_img_data_as_array(self, directory_path):\n        \"\"\"\n        Convert Sentinel-2 image bands in the nested IMG_DATA directory of a SAFE folder to a stacked numpy array and a list of channel names.\n        Parameters:\n        - directory_path: string, path to the .SAFE directory of the Sentinel-2 image.\n        Returns:\n        - Stacked numpy array of shape (height, width, num_bands) and a list of channel names.\n        \"\"\"\n\n        # Locate the nested IMG_DATA directory\n        img_data_path = None\n        print(directory_path)\n        for subdir, _, _ in os.walk(directory_path):\n            if os.path.basename(subdir) == \"IMG_DATA\":\n                img_data_path = subdir\n\n                break\n        print(img_data_path)\n        if not img_data_path:\n            raise ValueError(\n                f\"IMG_DATA directory not found in the provided path: {directory_path}\"\n            )\n\n            # List all .jp2 files in the found IMG_DATA directory and its subdirectories\n        jp2_files = []\n        for subdir, _, files in os.walk(img_data_path):\n            for file in files:\n                if file.endswith(\".jp2\"):\n                    jp2_files.append(os.path.join(subdir, file))\n\n        print(jp2_files)\n        # Convert each .jp2 file to numpy array and store in a list\n        channel_dict = {}\n        channel_names = []\n        for jp2_file in jp2_files:\n            with rasterio.open(jp2_file) as src:\n                band_name = (\n                    os.path.basename(jp2_file).split(\"_\")[2].split(\".\")[0]\n                )  # Assuming the band name is in the filename like \"Txxxxx_B0x.jp2\"\n                channel_names.append(band_name)\n                if band_name.startswith(\"TCI\"):\n                    tci_dims = src.read(1).shape\n                channel_dict[band_name] = src.read(1)\n        resized_arrays = []\n        for band in [\n            \"B01\",\n            \"B02\",\n            \"B03\",\n            \"B04\",\n            \"B05\",\n            \"B06\",\n            \"B07\",\n            \"B08\",\n            \"B8A\",\n            \"B09\",\n            \"B10\",\n            \"B11\",\n            \"B12\",\n        ]:\n            resized_arrays.append(self.resize_image(channel_dict[band], tci_dims))\n        del channel_dict\n        # Stack the arrays along the third dimension (channels)\n        stacked_array = np.stack(resized_arrays, axis=-1)\n        # img is (10980, 10980, 13)\n        return stacked_array\n\n    def preprocess_data(self, raw_data):\n        \"\"\"_summary_\n        Parameters\n        ----------\n        raw_data : _type_\n            _description_\n        Returns\n        -------\n        _type_\n        \"\"\"\n        normalized_data = (raw_data - 1000) / 10000\n        normalized_data = normalized_data.astype(\"float32\")\n        return normalized_data\n\n    def get_cloud_mask(\n        self,\n        x_coord: int,\n        y_coord: int,\n        threshold: float = 0.4,\n        average_over_pix: int = 22,\n        dilation_size: int = 11,\n        all_bands: bool = True,\n    ):\n        \"\"\"gets cloud mask from sentinel-2 scene\n        Parameters\n        ----------\n        scene_dir_path : str\n        threshold : float, optional\n        average_over_pix : int, optional\n        dilation_size : int, optional\n        all_bands : bool, optional\n        Returns\n        -------\n        _type_\n        \"\"\"\n\n        data = self.img_array[x_coord - 1 : x_coord + 1, y_coord - 1 : y_coord + 1, :]\n        normalized_data = self.preprocess_data(data)\n\n        cloud_detector = S2PixelCloudDetector(\n            threshold=threshold,\n            average_over=average_over_pix,\n            dilation_size=dilation_size,\n            all_bands=True,\n        )\n        cloud_prob = cloud_detector.get_cloud_probability_maps(\n            normalized_data[np.newaxis, ...]\n        )\n        return cloud_prob\n"}
{"type": "source_file", "path": "src/training/metric.py", "content": "import typing as t\n\nimport numpy as np\nfrom scipy.optimize import linear_sum_assignment\nfrom scipy.spatial import distance_matrix\n\n\ndef compute_loc_performance(\n    gt_array: np.ndarray, pred_array: np.ndarray, distance_tolerance: int = 20\n) -> t.Tuple[float, float, float]:\n    \"\"\"Compute confusion matrix components for gt/prediction locations.\"\"\"\n    # distance_matrix below doesn't work when preds is empty, so handle that first\n    if len(pred_array) == 0:\n        return [], [], gt_array.tolist()\n\n    # Building distance matrix using Euclidean distance pixel space\n    # multiplied by the UTM resolution (10 m per pixel)\n    dist_mat = distance_matrix(pred_array, gt_array, p=2)\n    dist_mat[dist_mat > distance_tolerance] = 99999\n\n    # Using Hungarian matching algorithm to assign lowest-cost gt-pred pairs\n    rows, cols = linear_sum_assignment(dist_mat)\n\n    tp_inds = [\n        {\"pred_idx\": rows[ii], \"gt_idx\": cols[ii]}\n        for ii in range(len(rows))\n        if dist_mat[rows[ii], cols[ii]] < distance_tolerance\n    ]\n    tp = [\n        {\n            \"pred\": pred_array[a[\"pred_idx\"]].tolist(),\n            \"gt\": gt_array[a[\"gt_idx\"]].tolist(),\n        }\n        for a in tp_inds\n    ]\n    tp_inds_pred = set([a[\"pred_idx\"] for a in tp_inds])\n    tp_inds_gt = set([a[\"gt_idx\"] for a in tp_inds])\n    fp = [\n        pred_array[i].tolist() for i in range(len(pred_array)) if i not in tp_inds_pred\n    ]\n    fn = [gt_array[i].tolist() for i in range(len(gt_array)) if i not in tp_inds_gt]\n\n    return tp, fp, fn\n\n\ndef score(\n    gt: dict, pred: dict, distance_tolerance: int = 20\n) -> t.Tuple[float, float, float, dict]:\n    \"\"\"Compute confusion matrix based metrics for ground truth/preds.\"\"\"\n    tp, fp, fn = [], [], []\n\n    for scene_id in gt.keys():\n        cur_tp, cur_fp, cur_fn = compute_loc_performance(\n            gt[scene_id], pred[scene_id], distance_tolerance=distance_tolerance\n        )\n        tp += [{\"scene_id\": scene_id, \"pred\": a[\"pred\"], \"gt\": a[\"gt\"]} for a in cur_tp]\n        fp += [{\"scene_id\": scene_id, \"point\": a} for a in cur_fp]\n        fn += [{\"scene_id\": scene_id, \"point\": a} for a in cur_fn]\n\n    if len(tp) == 0:\n        return 0, 0, 0, None\n\n    precision = len(tp) / (len(tp) + len(fp))\n    recall = len(tp) / (len(tp) + len(fn))\n    fscore = 2 * precision * recall / (precision + recall)\n    return precision, recall, fscore, {\"tp\": tp, \"fp\": fp, \"fn\": fn}\n"}
{"type": "source_file", "path": "src/utils/__init__.py", "content": ""}
{"type": "source_file", "path": "src/training/evaluate.py", "content": "import logging\n\nimport numpy as np\nimport scipy.optimize\nimport torch\nimport wandb\nfrom sklearn.metrics import confusion_matrix\n\nfrom src.data.dataset import NEGATIVE_VESSEL_TYPE_FACTOR\nfrom src.training.utils import compute_f1\n\n\nclass Evaluator:\n    def __init__(self, options, tuner_data=None):\n        pass\n\n    def update(self, targets: list, outputs: list, loss: float):\n        \"\"\"\n        Update Evaluator given the provided targets and outputs.\n        Does not return anything.\n        \"\"\"\n        pass\n\n    def score(self) -> dict:\n        \"\"\"\n        Returns a dict from str -> float of computed scores.\n        \"\"\"\n        pass\n\n\nclass Processor:\n    def __init__(self, options, windows, tuner_data=None):\n        pass\n\n    def process(self, images: list, targets: list, outputs: list):\n        \"\"\"\n        Returns a list of siv.Label representing the outputs.\n        \"\"\"\n        pass\n\n\nclass Tuner:\n    def __init__(self, options):\n        pass\n\n    def update(self, targets: list, outputs: list, loss: float):\n        \"\"\"\n        Update Tuner given the provided targets and outputs.\n        Does not return anything.\n        \"\"\"\n        pass\n\n    def tune(self) -> dict:\n        \"\"\"\n        Returns a dict, usually from str -> float, of tuner_data.\n        \"\"\"\n        pass\n\n\nclass LossEvaluator(Evaluator):\n    def __init__(self, options, tuner_data=None):\n        self.losses = []\n\n    def update(self, targets: list, outputs: list, loss: float):\n        self.losses.append(loss)\n\n    def score(self) -> dict:\n        return {\"score\": -np.mean(self.losses)}\n\n\ndef accuracy(preds, gt):\n    \"\"\"Compute ratio of correct predictions to total predictions\n    for classification task, given 1-d torch tensor of predicted classes,\n    and 1-d torch tensor of ground truth classes.\n    \"\"\"\n    correct = (preds == gt).sum()\n    total = len(preds)\n    if total > 0:\n        return correct / total\n    else:\n        return 0\n\n\nclass AttributeEvaluator(LossEvaluator):\n    def __init__(self, options, tuner_data=None):\n        super().__init__(options, tuner_data)\n        self._empty_labels()\n        self.thresholds = [\n            0.01,\n            0.02,\n            0.05,\n            0.1,\n            0.2,\n            0.3,\n            0.4,\n            0.5,\n            0.6,\n            0.7,\n            0.8,\n            0.9,\n            0.95,\n            0.98,\n            0.99,\n        ]\n\n    def _empty_labels(self):\n        self.losses = []\n        self.outputs = []\n        self.length_outputs = []\n        self.length_targets = []\n        self.width_outputs = []\n        self.width_targets = []\n        self.heading_outputs = []\n        self.heading_targets = []\n        self.speed_outputs = []\n        self.speed_targets = []\n        self.type_outputs = []\n        self.type_targets = []\n\n    def update(self, targets: list, outputs: list, loss: float):\n        self.losses.append(loss)\n        targets = torch.stack([target[\"target\"] for target in targets], dim=0)\n        self.length_outputs.append(outputs[:, 0])\n        self.length_targets.append(targets[:, 0])\n        self.width_outputs.append(outputs[:, 1])\n        self.width_targets.append(targets[:, 1])\n        self.heading_outputs.append(outputs[:, 2:18])\n        self.heading_targets.append(targets[:, 2:18])\n        self.speed_outputs.append(outputs[:, 18])\n        self.speed_targets.append(targets[:, 18])\n        self.type_outputs.append(outputs[:, 19:21])\n        self.type_targets.append(targets[:, 19:21])\n\n    def score(self) -> dict:\n        return {\"score\": -np.mean(self.losses)}\n\n    def log_metrics(self, logger: logging.Logger, use_wandb=True) -> None:\n        \"\"\"Get MAE and cross entropy metrics for length, width and heading.\"\"\"\n        length_outputs = torch.cat(self.length_outputs, dim=0)\n        length_targets = torch.cat(self.length_targets, dim=0)\n        width_outputs = torch.cat(self.width_outputs, dim=0)\n        width_targets = torch.cat(self.width_targets, dim=0)\n        heading_outputs = torch.cat(self.heading_outputs, dim=0)\n        heading_targets = torch.cat(self.heading_targets, dim=0)\n        speed_outputs = torch.cat(self.speed_outputs, dim=0)\n        speed_targets = torch.cat(self.speed_targets, dim=0)\n        type_outputs = torch.cat(self.type_outputs, dim=0)\n        type_targets = torch.cat(self.type_targets, dim=0)\n        width_mae = torch.mean(torch.abs(width_outputs - width_targets))\n        length_mae = torch.mean(torch.abs(length_outputs - length_targets))\n        speed_mae = torch.mean(torch.abs(speed_outputs - speed_targets))\n\n        # Cross entropies\n        heading_ce = torch.nn.functional.cross_entropy(\n            input=heading_outputs, target=heading_targets\n        )\n        type_ce = torch.nn.functional.cross_entropy(\n            input=type_outputs,\n            target=type_targets,\n            weight=torch.tensor([1, NEGATIVE_VESSEL_TYPE_FACTOR]).to(\n                type_outputs.device\n            ),\n        )\n\n        # Accuracy stats\n        heading_preds = heading_outputs.argmax(dim=1)\n        heading_gt = heading_targets.argmax(dim=1)\n        heading_acc = accuracy(heading_preds, heading_gt)\n\n        # Activity type preds conf mat based metrics\n        best_f1_score = None\n        best_threshold = None\n        type_f1s = []\n        type_precisions = []\n        type_recalls = []\n        tns = []\n        fps = []\n        fns = []\n        tps = []\n        normalized_type_outputs = torch.nn.functional.softmax(type_outputs, dim=1)\n\n        for _, threshold in enumerate(self.thresholds):\n            type_preds = torch.where(normalized_type_outputs[:, 1] > threshold, 1, 0)\n            type_gt = type_targets.argmax(dim=1)\n            type_acc = accuracy(type_preds, type_gt)\n\n            # Full confusion matrix\n            tn, fp, fn, tp = confusion_matrix(type_gt.cpu(), type_preds.cpu()).ravel()\n            tns.append(tn)\n            fps.append(fp)\n            fns.append(fn)\n            tps.append(tp)\n            f1, precision, recall = compute_f1(tp, fp, fn)\n            type_f1s.append(f1)\n            type_precisions.append(precision)\n            type_recalls.append(recall)\n\n            if best_f1_score is None or f1 > best_f1_score:\n                best_f1_score = f1\n                best_threshold = threshold\n                best_threshold_idx = self.thresholds.index(best_threshold)\n                best_type_acc = type_acc\n\n        best_precision = type_precisions[best_threshold_idx]\n        best_recall = type_recalls[best_threshold_idx]\n\n        # Full confusion matrix for heading\n        # heading_conf_mat = confusion_matrix(heading_gt.cpu(), heading_preds.cpu())\n\n        # Log metrics to stdout\n        logger.info(\"New best model save. Reporting val metrics.\")\n        logger.info(f\"Length MAE: {length_mae.item()}\")\n        logger.info(f\"Width MAE: {width_mae.item()}\")\n        logger.info(f\"Speed MAE: {speed_mae.item()}\")\n        logger.info(f\"Heading CE: {heading_ce.item()}\")\n        logger.info(f\"Heading Accuracy: {heading_acc}\")\n        logger.info(f\"Type CE: {type_ce.item()}\")\n        logger.info(f\"Type Accuracy (@best): {best_type_acc}\")\n        logger.info(\n            f\"Type (tn, fp, fn, tp) (@best)={tns[best_threshold_idx],fps[best_threshold_idx],fns[best_threshold_idx],tps[best_threshold_idx]}\"\n        )\n        logger.info(f\"Type Thresholds: {self.thresholds}\")\n        logger.info(f\"Type True negatives: {tns}\")\n        logger.info(f\"Type True positives: {tps}\")\n        logger.info(f\"Type False positives: {fps}\")\n        logger.info(f\"Type False negatives: {fns}\")\n\n        # WANDB Log fishing type confusion\n        columns = [\"Threshold\", \"TP\", \"FP\", \"FN\", \"TN\", \"Recall\", \"Precision\", \"F1\"]\n        data = [\n            [\n                thresh,\n                tps[idx],\n                fps[idx],\n                fns[idx],\n                tns[idx],\n                type_recalls[idx],\n                type_precisions[idx],\n                type_f1s[idx],\n            ]\n            for idx, thresh in enumerate(self.thresholds)\n        ]\n        if use_wandb:\n            fishing_type_table = wandb.Table(columns=columns, data=data)\n            wandb.log({\"Vessel-Type Is-Fishing Metrics\": fishing_type_table})\n\n        # WANDB Log heading preds confusion matrix\n        class_names = [f\"cls_{i}\" for i in range(16)]\n        if use_wandb:\n            wandb.log(\n                {\n                    \"conf_mat\": wandb.plot.confusion_matrix(\n                        probs=None,\n                        y_true=heading_gt.cpu().tolist(),\n                        preds=heading_preds.cpu().tolist(),\n                        class_names=class_names,\n                    )\n                }\n            )\n\n            wandb.log(\n                {\n                    \"Val Length MAE\": length_mae.item(),\n                    \"Val Width MAE\": width_mae.item(),\n                    \"Val Speed MAE\": speed_mae.item(),\n                    \"Val Heading CE\": heading_ce.item(),\n                    \"Val Heading Accuracy\": heading_acc,\n                    \"Val Type CEs\": type_ce.item(),\n                    \"Val Type Accuracy (@best)\": best_type_acc,\n                    \"Val Type F1 (@best)\": best_f1_score,\n                    \"Val Type Recall (@best)\": best_recall,\n                    \"Val Type Precision (@best)\": best_precision,\n                }\n            )\n        return None\n\n\nclass DetectPerClassF1Evaluator(Evaluator, Tuner):\n    \"\"\"\n    Evaluates F1 scores and tunes confidence thresholds for box and point detection tasks.\n    Compares each class independently.\n    \"\"\"\n\n    def __init__(self, options, tuner_data=None):\n        num_classes = 1  # TODO\n        if tuner_data:\n            self.thresholds = []\n            for cls_idx in range(num_classes):\n                threshold = tuner_data[\"class{}\".format(cls_idx)]\n                self.thresholds.append([threshold])\n        else:\n            self.thresholds = [\n                [\n                    0.01,\n                    0.02,\n                    0.05,\n                    0.1,\n                    0.2,\n                    0.3,\n                    0.4,\n                    0.5,\n                    0.6,\n                    0.7,\n                    0.8,\n                    0.9,\n                    0.95,\n                    0.98,\n                    0.99,\n                ]\n                for _ in range(num_classes)\n            ]\n        self._zero_confusion_matrix()\n\n    def _zero_confusion_matrix(self):\n        self.true_positives = [\n            [0] * len(self.thresholds[i]) for i in range(len(self.thresholds))\n        ]\n        self.false_positives = [\n            [0] * len(self.thresholds[i]) for i in range(len(self.thresholds))\n        ]\n        self.false_negatives = [\n            [0] * len(self.thresholds[i]) for i in range(len(self.thresholds))\n        ]\n\n    def update(self, targets: list, outputs: list, loss: float):\n        for img_idx in range(len(targets)):\n            for cls_idx, cls_thresholds in enumerate(self.thresholds):\n                gt_valid = targets[img_idx][\"labels\"] == cls_idx\n                gt_boxes = targets[img_idx][\"boxes\"].cpu().numpy()\n\n                for threshold_idx, threshold in enumerate(cls_thresholds):\n                    pred_valid = (outputs[img_idx][\"scores\"] >= threshold) & (\n                        outputs[img_idx][\"labels\"] == cls_idx\n                    )\n                    pred_boxes = outputs[img_idx][\"boxes\"][pred_valid, :].cpu().numpy()\n\n                    if len(gt_boxes) == 0:\n                        self.false_positives[cls_idx][threshold_idx] += len(pred_boxes)\n                        continue\n                    elif len(pred_boxes) == 0:\n                        self.false_negatives[cls_idx][threshold_idx] += len(gt_boxes)\n                        continue\n\n                    # Create binary association matrix of overlapping boxes.\n                    gt_tiled = gt_boxes[:, None, :].repeat(\n                        repeats=len(pred_boxes), axis=1\n                    )\n                    pred_tiled = pred_boxes[None, :, :].repeat(\n                        repeats=len(gt_boxes), axis=0\n                    )\n                    assoc = (\n                        (gt_tiled[:, :, 0] <= pred_tiled[:, :, 2])\n                        & (gt_tiled[:, :, 1] <= pred_tiled[:, :, 3])\n                        & (gt_tiled[:, :, 2] >= pred_tiled[:, :, 0])\n                        & (gt_tiled[:, :, 3] >= pred_tiled[:, :, 1])\n                    )\n\n                    # Optimize the assignment on the association matrix.\n                    rows, cols = scipy.optimize.linear_sum_assignment(\n                        1 - assoc.astype(\"float32\")\n                    )\n                    tp = len(\n                        [ii for ii in range(len(rows)) if assoc[rows[ii], cols[ii]]]\n                    )\n                    fp = len(pred_boxes) - tp\n                    fn = len(gt_boxes) - tp\n                    self.true_positives[cls_idx][threshold_idx] += tp\n                    self.false_positives[cls_idx][threshold_idx] += fp\n                    self.false_negatives[cls_idx][threshold_idx] += fn\n\n    def _score(self):\n        best_scores = {}\n        best_thresholds = {}\n\n        for cls_idx, cls_thresholds in enumerate(self.thresholds):\n            best_score = None\n            best_threshold = None\n\n            for threshold_idx, threshold in enumerate(cls_thresholds):\n                tp = self.true_positives[cls_idx][threshold_idx]\n                fp = self.false_positives[cls_idx][threshold_idx]\n                fn = self.false_negatives[cls_idx][threshold_idx]\n\n                f1, _, _ = compute_f1(tp, fp, fn)\n\n                if best_score is None or f1 > best_score:\n                    best_score = f1\n                    best_threshold = threshold\n\n            best_scores[\"class{}\".format(cls_idx)] = best_score\n            best_thresholds[\"class{}\".format(cls_idx)] = best_threshold\n\n        best_scores[\"score\"] = np.mean(list(best_scores.values()))\n        return best_scores, best_thresholds\n\n    def score(self):\n        scores, _ = self._score()\n        return scores\n\n    def tune(self):\n        _, tuner_data = self._score()\n        return tuner_data\n\n    def log_metrics(\n        self, class_name: str, logger: logging.Logger, use_wandb=True\n    ) -> None:\n        \"\"\"Get point/box detection metrics for specified class_name (at best threshold).\"\"\"\n        best_thresholds = self.tune()\n        classes = list(best_thresholds.keys())\n        class_idx = classes.index(class_name)\n        logger.info(f'New best model save. Getting metrics for class \"{class_name}\"')\n\n        thresholds = self.thresholds[class_idx]\n        best_threshold = self.tune()[class_name]\n        best_threshold_idx = thresholds.index(best_threshold)\n\n        tps = self.true_positives[class_idx]\n        fps = self.false_positives[class_idx]\n        fns = self.false_negatives[class_idx]\n        logger.info(f\"Thresholds: {thresholds}\")\n        logger.info(f\"True positives: {tps}\")\n        logger.info(f\"False positives: {fps}\")\n        logger.info(f\"False negatives: {fns}\")\n\n        precisions = []\n        recalls = []\n        f1s = []\n\n        for (tp, fp, fn) in zip(tps, fps, fns):\n            f1, precision, recall = compute_f1(tp, fp, fn)\n            precisions.append(precision)\n            recalls.append(recall)\n            f1s.append(f1)\n        logger.info(f\"F1s: {f1s}\")\n        logger.info(f\"Precisions: {precisions}\")\n        logger.info(f\"Recalls: {recalls}\")\n\n        logger.info(f\"Best F1 occurs at threshold {best_threshold}\")\n        logger.info(\n            f\"At best threshold:\\nPrecision: {precisions[best_threshold_idx]} \\nRecall: {recalls[best_threshold_idx]} \\nF1: {f1s[best_threshold_idx]}\\n\"\n        )\n        if use_wandb:\n            wandb.log(\n                {\n                    \"Val Precision (@best)\": precisions[best_threshold_idx],\n                    \"Val Recall (@best)\": recalls[best_threshold_idx],\n                    \"val F1 (@best)\": f1s[best_threshold_idx],\n                }\n            )\n\n        columns = [\"Threshold\", \"TP\", \"FP\", \"FN\", \"Recall\", \"Precision\", \"F1\"]\n        data = [\n            [\n                thresh,\n                tps[idx],\n                fps[idx],\n                fns[idx],\n                recalls[idx],\n                precisions[idx],\n                f1s[idx],\n            ]\n            for idx, thresh in enumerate(thresholds)\n        ]\n        if use_wandb:\n            detection_metrics_table = wandb.Table(columns=columns, data=data)\n            wandb.log({\"Detection Metrics\": detection_metrics_table})\n        return None\n\n\nclass MSEEvaluator(Evaluator):\n    def __init__(self, options, tuner_data=None):\n        self.scores = []\n\n    def update(self, targets: list, outputs: list, loss: float):\n        for img_idx in range(len(targets)):\n            score = torch.square(targets[img_idx][\"target\"] - outputs[img_idx]).mean()\n            self.scores.append(score.item())\n\n    def score(self) -> dict:\n        if self.scores:\n            return {\"score\": -np.mean(self.scores)}\n        else:\n            return 0\n\n\nclass DetectProcessor(Processor):\n    def __init__(self, task: str, options: dict, windows: list, tuner_data=None):\n        self.task = task\n        self.windows = windows\n        num_classes = 1\n        if tuner_data:\n            self.thresholds = tuner_data\n        else:\n            self.thresholds = [0] * num_classes\n\n    def process(self, images: list, targets: list, outputs: list):\n        labels = []\n\n        for img_idx, output in enumerate(outputs):\n            window_idx = targets[img_idx][\"image_id\"].item()\n            window = self.windows[window_idx]\n\n            for pred_idx, box in enumerate(output[\"boxes\"].tolist()):\n                score = output[\"scores\"][pred_idx].item()\n                cls_idx = output[\"labels\"][pred_idx].item()\n\n                if score < self.thresholds[cls_idx]:\n                    continue\n\n                d = {\n                    \"WindowID\": window[\"ID\"],\n                    \"Score\": score,\n                    \"CategoryID\": cls_idx,\n                }\n\n                if self.task == \"point\":\n                    column = (box[0] + box[2]) / 2\n                    row = (box[1] + box[3]) / 2\n                    column *= window[\"Width\"] / images[img_idx].shape[2]\n                    row *= window[\"Height\"] / images[img_idx].shape[1]\n\n                    d[\"Column\"] = window[\"Column\"] + int(column)\n                    d[\"Row\"] = window[\"Row\"] + int(row)\n\n                elif self.task == \"box\":\n                    box[0] *= window[\"Width\"] / images[img_idx].shape[2]\n                    box[1] *= window[\"Height\"] / images[img_idx].shape[1]\n                    box[2] *= window[\"Width\"] / images[img_idx].shape[2]\n                    box[3] *= window[\"Height\"] / images[img_idx].shape[1]\n\n                    d[\"Column\"] = window[\"Column\"] + int(box[0])\n                    d[\"Row\"] = window[\"Row\"] + int(box[1])\n                    d[\"Width\"] = int(box[2])\n                    d[\"Height\"] = int(box[3])\n\n                labels.append(d)\n\n        return labels\n\n\nclass ValueProcessor(Processor):\n    def __init__(self, options: dict, windows: list, tuner_data=None):\n        self.windows = windows\n\n    def process(self, images: list, targets: list, outputs: list):\n        labels = []\n\n        for img_idx, output in enumerate(outputs):\n            window_idx = targets[img_idx][\"image_id\"].item()\n            window = self.windows[window_idx]\n\n            labels.append(\n                {\n                    \"WindowID\": window[\"ID\"],\n                    \"Value\": output.item(),\n                }\n            )\n\n        return labels\n\n\ndef get_evaluator(task, options, tuner_data=None):\n    name = options.get(\"Evaluator\", \"accuracy\")\n\n    if name == \"loss\":\n        if task == \"custom\":\n            return AttributeEvaluator(options, tuner_data=tuner_data)\n        else:\n            return LossEvaluator(options, tuner_data=tuner_data)\n    if task in [\"point\", \"box\"]:\n        if name == \"per_class_f1\":\n            return DetectPerClassF1Evaluator(options, tuner_data=tuner_data)\n    elif task in [\"regression\", \"segmentation\"]:\n        if name == \"mse\":\n            return MSEEvaluator(options, tuner_data=tuner_data)\n\n    return None\n\n\ndef get_tuner(task, options):\n    name = options.get(\"Tuner\", \"f1\")\n\n    if task in [\"point\", \"box\"]:\n        if name == \"per_class_f1\":\n            return DetectPerClassF1Evaluator(options)\n\n    return None\n\n\ndef get_processor(task, options, windows, tuner_data=None):\n\n    if task in [\"point\", \"box\"]:\n        return DetectProcessor(task, options, windows, tuner_data=tuner_data)\n    elif task == \"regression\":\n        return ValueProcessor(options, windows, tuner_data=tuner_data)\n\n    return None\n\n\ndef evaluate(\n    model,\n    device,\n    loader,\n    half_enabled=False,\n    tuner_data=None,\n    evaluator=None,\n    processor=None,\n):\n    eval_losses = []\n    processed_outputs = []\n\n    if isinstance(evaluator, DetectPerClassF1Evaluator):\n        evaluator._zero_confusion_matrix()\n    if isinstance(evaluator, AttributeEvaluator):\n        evaluator._empty_labels()\n\n    with torch.no_grad():\n        for images, targets in loader:\n            images = list(image.to(device).float() / 255 for image in images)\n            gpu_targets = [\n                {\n                    k: v.to(device)\n                    for k, v in t.items()\n                    if not isinstance(v, str) and not isinstance(v, tuple)\n                }\n                for t in targets\n            ]\n\n            with torch.autocast(device.type, enabled=half_enabled):\n                outputs, loss = model(images, gpu_targets)\n\n            eval_losses.append(loss.item())\n\n            if evaluator:\n                evaluator.update(gpu_targets, outputs, loss.item())\n            if processor:\n                processed_outputs.extend(\n                    processor.process(images, gpu_targets, outputs)\n                )\n\n    return np.mean(eval_losses), processed_outputs\n"}
{"type": "source_file", "path": "src/utils/db.py", "content": "def dict_factory(cursor, row):\n    \"\"\"Converts rows returned from sqlite queries to dicts.\"\"\"\n    d = {}\n    for idx, col in enumerate(cursor.description):\n        d[col[0]] = row[idx]\n    return d\n\n\ndef get_dataset(conn, id):\n    query = f\"SELECT d.id, d.collection_id, d.name, d.task, d.categories FROM datasets AS d WHERE d.id = {id}\"\n    cur = conn.cursor()\n    row = cur.execute(query).fetchone()\n    return row\n\n\ndef get_windows(conn, dataset_id, split=None):\n    query = f\"SELECT w.id, w.dataset_id, w.image_id, w.row, w.column, w.height, w.width, w.hidden, w.split FROM windows AS w WHERE w.dataset_id = {dataset_id}\"\n    if split:\n        query += f' AND w.split = \"{split}\"'\n    cur = conn.cursor()\n    rows = cur.execute(query).fetchall()\n    return rows\n\n\ndef get_image(conn, image_id):\n    query = f\"SELECT ims.id, ims.uuid, ims.name, ims.format, ims.channels, ims.width, ims.height, ims.preprocessed, ims.hidden, ims.bounds, ims.time, ims.projection, ims.column, ims.row, ims.zoom FROM images AS ims WHERE ims.id = {image_id}\"\n    cur = conn.cursor()\n    row = cur.execute(query).fetchone()\n    return row\n\n\ndef get_labels(conn, window_id):\n    query = f\"SELECT l.id, l.window_id, l.row, l.column, l.height, l.width, l.extent, l.value, l.properties FROM labels AS l WHERE l.window_id = {window_id}\"\n    cur = conn.cursor()\n    rows = cur.execute(query).fetchall()\n    return rows\n\n\ndef get_dataset_labels(conn, dataset_id, splits=[]):\n    split_params = \"\"\n    if len(splits) > 0:\n        split_params += \" AND\"\n        for idx, split in enumerate(splits):\n            split_params += f\" split='{split}'\"\n            if idx < (len(splits) - 1):\n                split_params += \" OR\"\n\n    query = f\"SELECT l.id, l.window_id, l.row, l.column, l.height, l.width, l.extent, l.value, l.properties FROM labels AS l WHERE l.window_id in (SELECT id from windows where dataset_id={dataset_id}{split_params})\"\n    cur = conn.cursor()\n    rows = cur.execute(query).fetchall()\n    return rows\n"}
{"type": "source_file", "path": "src/training/utils.py", "content": "import logging\nimport typing as t\n\nimport mapcalc\nimport numpy as np\nimport torch\nfrom google.api_core.exceptions import GoogleAPIError\nfrom google.cloud.storage import Client\n\nfrom src.training.metric import score\n\n\ndef collate_fn(batch) -> tuple:\n    return tuple(zip(*batch))\n\n\ndef warmup_lr_scheduler(\n    optimizer: torch.optim.Optimizer, warmup_iters: int, warmup_factor: float\n) -> torch.optim.lr_scheduler.LambdaLR:\n    \"\"\"Return a warmup scheduler for a given optimizer.\"\"\"\n\n    def f(x):\n        if x >= warmup_iters:\n            return 1\n        alpha = float(x) / warmup_iters\n        return warmup_factor * (1 - alpha) + alpha\n\n    return torch.optim.lr_scheduler.LambdaLR(optimizer, f)\n\n\ndef get_map(target: dict, output: dict) -> float:\n    \"\"\"Compute mean average precision of gt labels and predicted boxes.\"\"\"\n    ground_truth = {\n        \"boxes\": target[\"boxes\"].tolist(),\n        \"labels\": target[\"labels\"].tolist(),\n    }\n    result_dict = {\n        \"boxes\": output[\"boxes\"].tolist(),\n        \"labels\": output[\"labels\"].tolist(),\n        \"scores\": output[\"scores\"].tolist(),\n    }\n    return mapcalc.calculate_map(ground_truth, result_dict, 0.5)\n\n\ndef get_score(gt_raw: dict, pred: dict) -> float:\n    \"\"\"Compute best fscore across thresholds ground truth bboxes and associated predictions.\"\"\"\n    gt = {}\n    for scene_id, boxes in gt_raw.items():\n        if len(boxes) > 0:\n            gt[scene_id] = np.array(\n                [((box[0] + box[2]) / 2, (box[1] + box[3]) / 2) for box in boxes],\n                dtype=np.float32,\n            )\n        else:\n            gt[scene_id] = np.zeros((0, 2), dtype=np.float32)\n\n    best_fscore = 0\n\n    for threshold in [0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95]:\n        cur_pred = {}\n        for scene_id, (boxes, scores) in pred.items():\n            if len(boxes) > 0:\n                cur_pred[scene_id] = np.array(\n                    [\n                        ((box[0] + box[2]) / 2, (box[1] + box[3]) / 2)\n                        for i, box in enumerate(boxes)\n                        if scores[i] >= threshold\n                    ],\n                    dtype=np.float32,\n                )\n            else:\n                cur_pred[scene_id] = np.zeros((0, 2), dtype=np.float32)\n\n        precision, recall, fscore, _ = score(gt, cur_pred)\n        best_fscore = max(best_fscore, fscore)\n\n    return best_fscore\n\n\ndef compute_f1(tp: float, fp: float, fn: float, eps: float = 0.01) -> t.Tuple[float]:\n    \"\"\"Compute F1, precision and recall from true positives, false positives, false negatives.\n\n    Parameters\n    ----------\n    tp: int\n        Number of true positive instances.\n\n    fp: int\n        Number of false positive instances.\n\n    fn: int\n        Number of false negative instances.\n\n    eps: float\n        Lower bound on (precision + recall) necessary to compute F1.\n\n    Returns\n    -------\n    f1: float\n        F1 score.\n\n    precision: float\n        Precision score.\n\n    recall: float\n        Recall score.\n    \"\"\"\n    if tp + fp == 0:\n        precision = 0\n    else:\n        precision = tp / (tp + fp)\n\n    if tp + fn == 0:\n        recall = 0\n    else:\n        recall = tp / (tp + fn)\n\n    if precision + recall < eps:\n        f1 = 0\n    else:\n        f1 = 2 * precision * recall / (precision + recall)\n\n    return (f1, precision, recall)\n\n\ndef upload_file_to_object_storage(\n    client: Client,\n    bucket: str,\n    local_path: str,\n    cloud_path: str,\n    logger: t.Optional[logging.Logger] = None,\n) -> None:\n    \"\"\"Upload a local file to GCS bucket.\n\n    Parameters\n    ----------\n    client: google.cloud.storage.Client\n        Cloud storage client.\n\n\n    bucket: str\n        GCS bucket name.\n\n    local_path: str\n        Relative local path to file to upload.\n\n    cloud_path: str\n        Absolute object name to be written to in GCS.\n\n    logger: logging.Logger\n        Optional logger to which to log exceptions.\n\n    Returns\n    -------\n    : None\n    \"\"\"\n    try:\n        bucket = client.get_bucket(bucket)\n        target = bucket.blob(cloud_path)\n        target.upload_from_filename(local_path)\n    except GoogleAPIError as e:\n\n        if logger:\n            logger.error(\n                f\"Failed to upload file at {local_path} to {bucket}/{cloud_path}:\\n\\n {e}.\"\n            )\n        else:\n            print(f\"Failed to upload file at {local_path} to {bucket}/{cloud_path}.\")\n    return None\n"}
{"type": "source_file", "path": "src/utils/download/sentinel1.py", "content": "import datetime\nimport logging\nimport os\nimport os.path\nimport shutil\nimport subprocess\n\nfrom sentinelsat import SentinelAPI\n\nfrom src.data.preprocess import preprocess\nfrom src.data.warp import get_image_info, warp\nfrom src.utils.parallel import starmap\n\nlogger = logging.getLogger(\"downloader\")\n\n\ndef retrieve(images, tmp_dir, download_dir, workers=0):\n    \"\"\"Retrieve images from ESA API.\"\"\"\n    api = SentinelAPI(\n        os.environ.get(\"COPERNICUS_USERNAME\"),\n        os.environ.get(\"COPERNICUS_PASSWORD\"),\n        \"https://scihub.copernicus.eu/dhus\",\n        timeout=60,\n    )\n\n    # Filter for those images which still need to be preprocessed.\n    preprocess_dir = os.path.join(download_dir, \"preprocess\")\n    filtered_images = [\n        image\n        for image in images\n        if not os.path.isdir(os.path.join(preprocess_dir, image[\"uuid\"]))\n    ]\n    logger.info(f\"{len(images) - len(filtered_images)}/{len(images)} already processed\")\n\n    # For unprocessed targets, download compressed images that have yet to be downloaded\n    needed_filenames = [\n        image[\"name\"]\n        for image in filtered_images\n        if not os.path.exists(\n            os.path.join(\n                download_dir, \"image_cache\", image[\"name\"].replace(\".SAFE\", \".zip\")\n            )\n        )\n    ]\n    needed_product_ids = [list(api.query(filename=fn))[0] for fn in needed_filenames]\n    api.download_all(\n        needed_product_ids,\n        directory_path=os.path.join(download_dir, \"image_cache\"),\n        n_concurrent_dl=4,\n        max_attempts=4,\n    )\n\n    # Pass image metadata to be inflated and preprocessed\n    image_paths = {\n        image[\"uuid\"]: os.path.join(\n            download_dir, \"image_cache\", image[\"name\"].replace(\".SAFE\", \".zip\")\n        )\n        for image in filtered_images\n    }\n    return retrieve_paths(image_paths, tmp_dir, workers=workers)\n\n\ndef unzip(fname, tmp_dir):\n    \"\"\"Unzip an archive to a specified directory.\"\"\"\n    subprocess.call([\"unzip\", fname, \"-d\", tmp_dir])\n\n\ndef retrieve_paths(image_paths, tmp_dir, workers=0):\n    \"\"\"Decompresses and collects metadata associated with image paths.\"\"\"\n    # Unzip.\n    starmap(\n        unzip, [(fname, tmp_dir) for fname in image_paths.values()], workers=workers\n    )\n\n    wanted_channels = set([\"vh\", \"vv\"])\n\n    # Add images.\n    images = []\n    for uuid, path in image_paths.items():\n        scene_id = os.path.basename(path).replace(\".zip\", \".SAFE\")\n        if not scene_id.endswith(\".SAFE\"):\n            continue\n\n        # Identify which channels are available.\n        src_path = os.path.join(tmp_dir, scene_id)\n        measurement_path = os.path.join(src_path, \"measurement\")\n        if not os.path.exists(measurement_path):\n            logger.error(\"scene {} missing measurement path\".format(scene_id))\n            continue\n\n        channels = []\n        for fname in os.listdir(measurement_path):\n            channel_name = fname.split(\"-\")[3]\n            if wanted_channels and channel_name not in wanted_channels:\n                continue\n\n            channels.append(\n                {\n                    \"Name\": channel_name,\n                    \"Path\": os.path.join(measurement_path, fname),\n                    \"Count\": 1,\n                }\n            )\n\n        # Extract timestamp from scene ID. We use the first time that appears in it.\n        # Should appear like \"..._20211101T181712_...\"\n        parts = scene_id.split(\"_\")\n        ts = None\n        for part in parts:\n            if len(part) != 15 or not part[0:8].isdigit():\n                continue\n\n            ts = datetime.datetime(\n                year=int(part[0:4]),\n                month=int(part[4:6]),\n                day=int(part[6:8]),\n                hour=int(part[9:11]),\n                minute=int(part[11:13]),\n                second=int(part[13:15]),\n                tzinfo=datetime.timezone.utc,\n            )\n            break\n\n        images.append(\n            RetrieveImage(\n                uuid=uuid,\n                name=scene_id,\n                time=ts,\n                format=\"geotiff\",\n                channels=channels,\n                pixel_size=10,\n            )\n        )\n\n    return images\n\n\nclass RetrieveImage(object):\n    \"\"\"Metadata container for an image retrieved from ESA api.\"\"\"\n\n    def __init__(self, uuid, name, time, channels, format=\"geotiff\", pixel_size=None):\n        self.uuid = uuid\n        self.name = name\n        self.time = time\n        self.channels = channels\n        self.format = format\n        self.pixel_size = pixel_size\n\n\ndef warp_and_print_one(image, tmp_dir, channel, download_dir, roi=None):\n    \"\"\" \"\"\"\n    dst_path = os.path.join(download_dir, \"images\", image.uuid)\n    os.makedirs(dst_path, exist_ok=True)\n\n    projection = \"epsg:3857\"\n    src_fname = channel[\"Path\"]\n\n    # Determine where to write layer.\n    # Projected images don't need to be stored since we can transform coordinates based on the projection details.\n    # We also decide whether to warp the image or just copy it.\n    if projection:\n        layer_fname = os.path.join(\n            tmp_dir, \"{}_{}.tif\".format(image.uuid, channel[\"Name\"])\n        )\n        image_info = warp(image, src_fname, layer_fname, projection=projection)\n    else:\n        layer_fname = os.path.join(dst_path, \"{}.tif\".format(channel[\"Name\"]))\n        if image.format == \"geotiff\":\n            shutil.move(src_fname, layer_fname)\n            image_info = get_image_info(layer_fname)\n        else:\n            image_info = warp(image, src_fname, layer_fname)\n\n    image_data = {\n        \"UUID\": image.uuid,\n        \"Name\": image.name,\n        \"Format\": \"geotiff\",\n        \"Channels\": image.channels,\n        \"Width\": image_info.width,\n        \"Height\": image_info.height,\n        \"Bounds\": image_info.bounds,\n        \"Time\": image.time.isoformat(),\n        \"Projection\": image_info.projection,\n        \"Column\": image_info.column,\n        \"Row\": image_info.row,\n        \"Zoom\": image_info.zoom,\n    }\n\n    preprocess(image_data, layer_fname, channel, roi=roi, dest_dir=download_dir)\n\n    if projection:\n        # We don't need the layer, delete it immediately.\n        os.remove(layer_fname)\n\n    return image_data\n\n\ndef warp_and_print(images, tmp_dir, download_dir, roi=None, workers=0):\n    \"\"\"\n    Given a list of RetrieveImage:\n    (1) Copy/warp the image file to data/images/ if needed (i.e., if no projection is set and we need the image for coordinate conversion)\n    (2) Pre-process the image into tiles.\n    (3) Print out image JSON so caller can add it to database.\n    \"\"\"\n\n    # Get list of jobs. Each job is to warp/preprocess one channel of one image.\n    jobs = []\n    for image in images:\n        for channel in image.channels:\n            jobs.append((image, tmp_dir, channel, download_dir, roi))\n\n    image_datas = starmap(warp_and_print_one, [job for job in jobs], workers=workers)\n\n    # image_datas includes one dict per channel of each image.\n    # Here, we print an arbitrary dict per image.\n    seen_uuids = set()\n    for image_data in image_datas:\n        if image_data[\"UUID\"] in seen_uuids:\n            continue\n        seen_uuids.add(image_data[\"UUID\"])\n        logger.info(f\"Finished preprocessing Image with UUID={image_data['UUID']}\")\n"}
{"type": "source_file", "path": "src/utils/geom.py", "content": "\nimport math\n\nimport typing as t\n\nR_EARTH_M = 6371000\nLAT_RANGE = 180\nLON_RANGE = 360\nDEGREES_LAT_PER_METER = LAT_RANGE / (math.pi * R_EARTH_M)\n\n\ndef degrees_lon_per_meter(lat: float) -> float:\n    \"\"\"Approximate degrees of longitude per meter at specified latitude.\n\n    Note: This is an infinitesimal approximation, and breaks down near poles.\n\n    Parameters\n    ----------\n    lat: float\n        Latitude.\n\n    Returns\n    -------\n    dlpm: float\n        Degrees longitude per meter.\n    \"\"\"\n    lat_circumference = 2 * math.pi * R_EARTH_M * math.cos(lat)\n    return LON_RANGE / lat_circumference\n\n\ndef extremal_bounds(\n    center_lon: float, center_lat: float, width_m: float\n) -> t.Tuple[float, float, float, float]:\n    \"\"\"Calculate minimal and maximal lon/lat coordinates given a center coordinate and a desired square width.\n\n    Parameters\n    ----------\n    center_lon: float\n        Longitude for center of object.\n\n    center_lat: float\n        Latitude for center of object.\n\n    width_m: float\n        Width of object in meters.\n\n    Returns\n    -------\n    min_lon: float\n        Minimum longitude of object square.\n\n    min_lat: float\n        Minimum latitude of object square.\n\n    max_lon: flaot\n        Maximum longitude of object square.\n\n    max_lat:\n        Minimum latitude of object square.\n    \"\"\"\n    extend_by = width_m / 2\n    dlonpm = degrees_lon_per_meter(center_lat)\n\n    min_lat, min_lon = center_lat - (DEGREES_LAT_PER_METER * extend_by), center_lon - (\n        dlonpm * extend_by\n    )\n    max_lat, max_lon = center_lat + (DEGREES_LAT_PER_METER * extend_by), center_lon + (\n        dlonpm * extend_by\n    )\n\n    return min_lon, min_lat, max_lon, max_lat\n"}
{"type": "source_file", "path": "src/training/ema.py", "content": "from collections import OrderedDict\nfrom copy import deepcopy\nfrom sys import stderr\n\nimport torch\n\nfrom torch import nn\n\n\nclass EMA(nn.Module):\n    def __init__(self, model: nn.Module, decay: float):\n        super().__init__()\n        self.decay = decay\n\n        self.model = model\n        print(\"preparing EMA with decay={}\".format(decay))\n        self.shadow = deepcopy(self.model)\n\n        for param in self.shadow.parameters():\n            param.detach_()\n\n    @torch.no_grad()\n    def update(self, epoch):\n        if not self.training:\n            print(\n                \"EMA update should only be called during training\",\n                file=stderr,\n                flush=True,\n            )\n            return\n\n        model_params = OrderedDict(self.model.named_parameters())\n        shadow_params = OrderedDict(self.shadow.named_parameters())\n\n        # check if both model contains the same set of keys\n        assert model_params.keys() == shadow_params.keys()\n\n        if epoch <= 5:\n            decay = 0.99\n        else:\n            decay = self.decay\n\n        for name, param in model_params.items():\n            # see https://www.tensorflow.org/api_docs/python/tf/train/ExponentialMovingAverage\n            # shadow_variable -= (1 - decay) * (shadow_variable - variable)\n            shadow_params[name].sub_((1.0 - decay) * (shadow_params[name] - param))\n\n        model_buffers = OrderedDict(self.model.named_buffers())\n        shadow_buffers = OrderedDict(self.shadow.named_buffers())\n\n        # check if both model contains the same set of keys\n        assert model_buffers.keys() == shadow_buffers.keys()\n\n        for name, buffer in model_buffers.items():\n            # buffers are copied\n            shadow_buffers[name].copy_(buffer)\n\n    def forward(self, *input, **kwargs):\n        if self.training:\n            return self.model(*input, **kwargs)\n        else:\n            return self.shadow(*input, **kwargs)\n"}
{"type": "source_file", "path": "src/models/frcnn_cmp2.py", "content": "from collections import OrderedDict\n\nimport torch\nimport torchvision\nfrom torchvision.models import resnet\nfrom torchvision.models._utils import IntermediateLayerGetter\nfrom torchvision.models.detection.faster_rcnn import FasterRCNN, FastRCNNPredictor\nfrom torchvision.ops.feature_pyramid_network import (\n    FeaturePyramidNetwork,\n    LastLevelMaxPool,\n)\n\nfrom .frcnn import NoopTransform\n\nPRETRAINED_MODELS = {\n    \"tiny\": {\n        \"constructor\": torchvision.models.swin_v2_t,\n        \"weights\": torchvision.models.Swin_V2_T_Weights.IMAGENET1K_V1,\n    },\n    \"small\": {\n        \"constructor\": torchvision.models.swin_v2_s,\n        \"weights\": torchvision.models.Swin_V2_S_Weights.IMAGENET1K_V1,\n    },\n    \"base\": {\n        \"constructor\": torchvision.models.swin_v2_b,\n        \"weights\": torchvision.models.Swin_V2_B_Weights.IMAGENET1K_V1,\n    },\n}\n\n\nINTERMEDIATE_SPECIFICATIONS = {\n    \"tiny\": {\n        \"four_features\": {\n            \"feature_slices\": [slice(0, 2), slice(2, 4), slice(4, 6), slice(6, 8)],\n            \"feature_dims\": [96 * 2, 192 * 2, 384 * 2, 768 * 2],\n        },\n        \"single_feature\": {\"feature_slices\": [slice(0, 6)], \"feature_dims\": [384 * 2]},\n    },\n    \"small\": {\n        \"four_features\": {\n            \"feature_slices\": [slice(0, 2), slice(2, 4), slice(4, 6), slice(6, 8)],\n            \"feature_dims\": [96 * 2, 192 * 2, 384 * 2, 768 * 2],\n        },\n        \"single_feature\": {\"feature_slices\": [slice(0, 6)], \"feature_dims\": [384 * 2]},\n    },\n    \"base\": {\n        \"four_features\": {\n            \"feature_slices\": [slice(0, 2), slice(2, 4), slice(4, 6), slice(6, 8)],\n            \"feature_dims\": [128 * 2, 256 * 2, 512 * 2, 1024 * 2],\n        },\n        \"single_feature\": {\"feature_slices\": [slice(0, 6)], \"feature_dims\": [384 * 2]},\n    },\n}\n\n\nclass SimpleBackbone(torch.nn.Module):\n    def __init__(self, num_channels):\n        super(SimpleBackbone, self).__init__()\n\n        def down_layer(in_channels, out_channels):\n            return torch.nn.Sequential(\n                torch.nn.Conv2d(in_channels, out_channels, 4, stride=2, padding=1),\n                torch.nn.ReLU(inplace=True),\n                torch.nn.Conv2d(out_channels, out_channels, 3, padding=1),\n                torch.nn.BatchNorm2d(out_channels, eps=1e-3, momentum=0.03),\n                torch.nn.ReLU(inplace=True),\n            )\n\n        self.down1 = down_layer(num_channels, 32)  # -> 400x400\n        self.down2 = down_layer(32, 64)  # -> 200x200\n        self.down3 = down_layer(64, 128)  # -> 100x100\n        self.down4 = down_layer(128, 256)  # -> 50x50\n        self.down5 = down_layer(256, 512)  # -> 25x25\n        self.features = torch.nn.Sequential(\n            torch.nn.Conv2d(512, 512, 3, padding=1),\n            torch.nn.ReLU(inplace=True),\n            torch.nn.Conv2d(512, 512, 3, padding=1),\n            torch.nn.ReLU(inplace=True),\n            torch.nn.Conv2d(512, 512, 3, padding=1),\n        )\n\n    def forward(self, x):\n        down1 = self.down1(x)\n        down2 = self.down2(down1)\n        down3 = self.down3(down2)\n        down4 = self.down4(down3)\n        down5 = self.down5(down4)\n        features = self.features(down5)\n        return {\n            \"0\": down2,\n            \"1\": down3,\n            \"2\": down4,\n            \"3\": features,\n        }\n\n\nclass MyBackbone(torch.nn.Module):\n    def __init__(self, aggregate_op=\"sum\", encoder_backbone=\"simple\", group_channels=3):\n        super(MyBackbone, self).__init__()\n        self.aggregate_op = aggregate_op\n        self.group_channels = group_channels\n\n        if encoder_backbone == \"simple\":\n            self.backbone = SimpleBackbone(self.group_channels)\n            encoder_channels = [128, 256, 512, 1024]\n        elif encoder_backbone == \"resnet50\":\n            returned_layers = [1, 2, 3, 4]\n            return_layers = {f\"layer{k}\": str(v) for v, k in enumerate(returned_layers)}\n            self.backbone = IntermediateLayerGetter(\n                resnet.resnet50(\n                    weights=torchvision.models.ResNet50_Weights.IMAGENET1K_V1\n                ),\n                return_layers=return_layers,\n            )\n            if self.group_channels != 3:\n                self.backbone.conv1 = torch.nn.Conv2d(\n                    self.group_channels,\n                    self.backbone.conv1.out_channels,\n                    kernel_size=7,\n                    stride=2,\n                    padding=3,\n                    bias=False,\n                )\n            encoder_channels = [512, 1024, 2048, 4096]\n        elif encoder_backbone == \"resnet101\":\n            returned_layers = [1, 2, 3, 4]\n            return_layers = {f\"layer{k}\": str(v) for v, k in enumerate(returned_layers)}\n            self.backbone = IntermediateLayerGetter(\n                resnet.resnet101(\n                    weights=torchvision.models.ResNet101_Weights.IMAGENET1K_V1\n                ),\n                return_layers=return_layers,\n            )\n            if self.group_channels != 3:\n                self.backbone.conv1 = torch.nn.Conv2d(\n                    self.group_channels,\n                    self.backbone.conv1.out_channels,\n                    kernel_size=7,\n                    stride=2,\n                    padding=3,\n                    bias=False,\n                )\n            encoder_channels = [512, 1024, 2048, 4096]\n        else:\n            raise Exception(\"bad encoder backbone {}\".format(encoder_backbone))\n\n        self.out_channels = 256\n        self.fpn = FeaturePyramidNetwork(\n            in_channels_list=encoder_channels,\n            out_channels=self.out_channels,\n            extra_blocks=LastLevelMaxPool(),\n        )\n\n    def forward(self, x):\n        base_im_features = self.backbone(x[:, 0: self.group_channels, :, :])\n\n        # Geat features for overlaps\n        overlap_dict = {}\n        for i in range(self.group_channels, x.shape[1], self.group_channels):\n            overlap_features = self.backbone(x[:, i: i + self.group_channels, :, :])\n            # Add feature contribution for current overlap\n            for k, v in overlap_features.items():\n                if k not in overlap_dict:\n                    overlap_dict[k] = v\n                else:\n                    if self.aggregate_op == \"sum\":\n                        overlap_dict[k] = overlap_dict[k] + v\n                    elif self.aggregate_op == \"max\":\n                        overlap_dict[k] = torch.maximum(overlap_dict[k], v)\n                    else:\n                        raise Exception(\n                            \"Unknown aggregate op: {}\".format(self.aggregate_op)\n                        )\n        out_dict = {}\n        if len(overlap_dict.keys()) == len(base_im_features.keys()):\n            for k, v in base_im_features.items():\n                out_dict[k] = torch.cat([base_im_features[k], overlap_dict[k]], dim=1)\n        else:\n            for k, v in base_im_features.items():\n                blank_history = torch.zeros(\n                    size=base_im_features[k].shape,\n                    dtype=base_im_features[k].dtype,\n                    device=base_im_features[k].device,\n                )\n                out_dict[k] = torch.cat([base_im_features[k], blank_history], dim=1)\n\n        return self.fpn(out_dict)\n\n\nclass SwinTransformerIntermediateLayerModel(torch.nn.Module):\n    def __init__(self, variant=\"tiny\", feature_cfg=\"four_features\", in_channels=3):\n        super().__init__()\n\n        if variant not in [\"tiny\", \"small\", \"base\"]:\n            raise ValueError(\n                \"This class only supports 'tiny', 'small' and 'base' swin transformer v2 variants.\"\n            )\n\n        # Isolate models to produce feature maps\n        self.pretrained_model_cfg = PRETRAINED_MODELS[variant]\n        self.pretrained_model = self.pretrained_model_cfg[\"constructor\"](\n            weights=self.pretrained_model_cfg[\"weights\"]\n        )\n\n        # Default torch impl. has 3-channel input conv\n        if in_channels != 3:\n            conv0 = self.pretrained_model.features[0][0]\n            self.pretrained_model.features[0][0] = torch.nn.Conv2d(\n                in_channels,\n                conv0.out_channels,\n                kernel_size=conv0.kernel_size,\n                stride=conv0.stride,\n                padding=conv0.padding\n            )\n        self.intermediate_feature_cfg = INTERMEDIATE_SPECIFICATIONS[variant][\n            feature_cfg\n        ]\n        pretrained_model_slices = self.intermediate_feature_cfg[\"feature_slices\"]\n        self.intermediate_models = [\n            self.pretrained_model.features[s] for s in pretrained_model_slices\n        ]\n        self.feature_dims = self.intermediate_feature_cfg[\"feature_dims\"]\n\n    def forward(self, x):\n        \"\"\"Return OrderedDict of intermediate features from specified Swin Transformer variant.\n\n        Features returned with shape [n_samples, n_channels, height, width]\n        \"\"\"\n        out_dict = OrderedDict()\n        for idx, model in enumerate(self.intermediate_models):\n            out_dict[f\"{idx}\"] = x = model(x)\n\n        for k, v in out_dict.items():\n            out_dict[k] = v.permute(0, 3, 1, 2)\n\n        return out_dict\n\n\nclass SwinTransformerRCNNBackbone(torch.nn.Module):\n    def __init__(\n        self, aggregate_op=\"sum\", use_fpn=False, variant=\"small\", group_channels=3\n    ):\n        super().__init__()\n        self.aggregate_op = aggregate_op\n        self.group_channels = group_channels\n        self.use_fpn = use_fpn\n\n        if variant not in [\"tiny\", \"small\", \"base\"]:\n            raise ValueError(\n                \"This class only supports 'tiny', 'small' and 'base' swin transformer v2 variants.\"\n            )\n\n        self.out_channels = 256\n\n        # Prepare dict of intermediate layer outputs for consumption by FPN.\n        if self.use_fpn:\n            self.backbone = SwinTransformerIntermediateLayerModel(\n                variant=variant, feature_cfg=\"four_features\", in_channels=group_channels\n            )\n            feature_dims = self.backbone.feature_dims\n\n            self.fpn = FeaturePyramidNetwork(\n                in_channels_list=feature_dims,\n                out_channels=self.out_channels,\n                extra_blocks=LastLevelMaxPool(),\n            )\n\n        # Output a single feature vector for consumption downstream\n        else:\n            self.backbone = SwinTransformerIntermediateLayerModel(\n                variant=variant, feature_cfg=\"single_feature\", in_channels=group_channels\n            )\n\n    def forward(self, x):\n\n        base_im_features = self.backbone(x[:, 0: self.group_channels, :, :])\n        # Get features for overlaps\n        overlap_dict = {}\n        for i in range(self.group_channels, x.shape[1], self.group_channels):\n            overlap_features = self.backbone(x[:, i: i + self.group_channels, :, :])\n            # Add feature contribution for current overlap\n            for k, v in overlap_features.items():\n                if k not in overlap_dict:\n                    overlap_dict[k] = v\n                else:\n                    if self.aggregate_op == \"sum\":\n                        overlap_dict[k] = overlap_dict[k] + v\n                    elif self.aggregate_op == \"max\":\n                        overlap_dict[k] = torch.maximum(overlap_dict[k], v)\n                    else:\n                        raise Exception(\n                            \"Unknown aggregate op: {}\".format(self.aggregate_op)\n                        )\n        out_dict = {}\n        if len(overlap_dict.keys()) == len(base_im_features.keys()):\n            for k, v in base_im_features.items():\n                out_dict[k] = torch.cat([base_im_features[k], overlap_dict[k]], dim=1)\n        else:\n            for k, v in base_im_features.items():\n                blank_history = torch.zeros(\n                    size=base_im_features[k].shape,\n                    dtype=base_im_features[k].dtype,\n                    device=base_im_features[k].device,\n                )\n                out_dict[k] = torch.cat([base_im_features[k], blank_history], dim=1)\n        if self.use_fpn:\n            return self.fpn(out_dict)\n\n        else:\n            return out_dict\n\n\nclass FasterRCNNModel(torch.nn.Module):\n    def __init__(self, info):\n        super(FasterRCNNModel, self).__init__()\n\n        options = info[\"Options\"]\n        image_size = info[\"Example\"][0].shape[1]\n        num_classes = len(info[\"Data\"][\"categories\"])\n\n        use_noop_transform = options.get(\"NoopTransform\", True)\n        aggregate_op = options.get(\"AggregateOp\", \"sum\")\n        encoder_backbone = options.get(\"EncoderBackbone\", \"simple\")\n        encoder_backbone_variant = options.get(\"EncoderBackboneVariant\", \"tiny\")\n        encoder_backbone_use_fpn = options.get(\"EncoderBackboneUseFPN\", \"true\")\n        # Number of channels per layer.\n        # This is used to distinguish channels of current image from those of overlapping images.\n        group_channels = options.get(\"GroupChannels\", 2)\n\n        # We have max 86 points per 800x800 chip.\n        # So here, in case we're using larger image sizes, determine if we need to increase some parameters.\n        box_detections_per_img = max(100, 100 * image_size * image_size // 800 // 800)\n        rpn_pre_nms_top_n_train = max(\n            2000, 2000 * image_size * image_size // 800 // 800\n        )\n        rpn_post_nms_top_n_train = max(\n            2000, 2000 * image_size * image_size // 800 // 800\n        )\n        rpn_pre_nms_top_n_test = max(1000, 1000 * image_size * image_size // 800 // 800)\n        rpn_post_nms_top_n_test = max(\n            1000, 1000 * image_size * image_size // 800 // 800\n        )\n\n        if encoder_backbone == \"swintransformer\":\n            use_fpn = encoder_backbone_use_fpn == \"true\"\n            self.backbone = SwinTransformerRCNNBackbone(\n                use_fpn=use_fpn,\n                group_channels=group_channels,\n                aggregate_op=aggregate_op,\n                variant=encoder_backbone_variant,\n            )\n\n        else:\n            self.backbone = MyBackbone(\n                aggregate_op=aggregate_op,\n                encoder_backbone=encoder_backbone,\n                group_channels=group_channels,\n            )\n        self.faster_rcnn = FasterRCNN(\n            self.backbone,\n            num_classes + 1,\n            min_size=image_size,\n            max_size=image_size,\n            box_detections_per_img=box_detections_per_img,\n            rpn_pre_nms_top_n_train=rpn_pre_nms_top_n_train,\n            rpn_post_nms_top_n_train=rpn_post_nms_top_n_train,\n            rpn_pre_nms_top_n_test=rpn_pre_nms_top_n_test,\n            rpn_post_nms_top_n_test=rpn_post_nms_top_n_test,\n        )\n\n        # replace the classifier with a new one for user-defined num_classes\n        in_features = self.faster_rcnn.roi_heads.box_predictor.cls_score.in_features\n        self.faster_rcnn.roi_heads.box_predictor = FastRCNNPredictor(\n            in_features, num_classes + 1\n        )\n\n        if use_noop_transform:\n            self.faster_rcnn.transform = NoopTransform()\n\n    def forward(self, images, raw_targets=None):\n        device = images[0].device\n\n        # Fix targets: if there are no labels, then for some reason we need to set one label.\n        # If there are labels, we need to increment it by one, since 0 is reserved for background.\n        targets = None\n        if raw_targets:\n            targets = []\n            for target in raw_targets:\n                target = dict(target)\n                if len(target[\"boxes\"]) == 0:\n                    target[\"labels\"] = torch.zeros(\n                        (1,), device=device, dtype=torch.int64\n                    )\n                else:\n                    target[\"labels\"] = target[\"labels\"] + 1\n                targets.append(target)\n\n        images, targets = self.faster_rcnn.transform(images, targets)\n        features = self.faster_rcnn.backbone(images.tensors)\n        if isinstance(features, torch.Tensor):\n            features = OrderedDict([(\"0\", features)])\n        proposals, proposal_losses = self.faster_rcnn.rpn(images, features, targets)\n        detections, detector_losses = self.faster_rcnn.roi_heads(\n            features, proposals, images.image_sizes, targets\n        )\n\n        losses = {\"base\": torch.tensor(0, device=device, dtype=torch.float32)}\n        losses.update(proposal_losses)\n        losses.update(detector_losses)\n\n        # Fix detections: need to decrement class label.\n        for output in detections:\n            output[\"labels\"] = output[\"labels\"] - 1\n\n        loss = sum(x for x in losses.values())\n        return detections, loss\n"}
{"type": "source_file", "path": "src/utils/misc.py", "content": "import glob\nimport os\nimport typing as t\n\n\ndef delete_scratch(file_names: t.List[str], scratch_dir_path: str) -> None:\n    \"\"\"Delete files in specified directory, and subsequently directory itself if empty.\n\n    Parameters\n    ----------\n    file_names: list[str]\n        List of filenames in scratch directory.\n\n    scratch_dir_path: str\n        Path to scratch directory.\n\n    Returns\n    -------\n    : None\n    \"\"\"\n    for filename in glob.glob(file_names):\n        os.remove(filename)\n    try:\n        os.rmdir(scratch_dir_path)\n    except OSError:\n        pass\n    return None\n"}
{"type": "source_file", "path": "train.py", "content": "import argparse\nimport json\nimport logging\nimport os\nimport sqlite3\nimport sys\n\nfrom src.training.train import train_loop\nfrom src.utils.db import dict_factory, get_dataset, get_image, get_labels, get_windows\n\n# Configure logger\nlogger = logging.getLogger(\"training\")\nlogger.setLevel(logging.INFO)\nformatter = logging.Formatter(\"%(asctime)s (%(name)s) (%(levelname)s): %(message)s\")\nstream_handler = logging.StreamHandler(sys.stdout)\nstream_handler.setLevel(logging.INFO)\nstream_handler.setFormatter(formatter)\nlogger.addHandler(stream_handler)\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\"Training script.\")\n\n    # General\n    parser.add_argument(\n        \"--config_path\",\n        help=\"Path to model and training config json.\",\n        default=\"./data/cfg/train_sentinel1_detector.json\",\n    )\n    parser.add_argument(\n        \"--training_data_dir\",\n        help=\"Path to training data directory containing preprocess folder.\",\n        default=\"./data\",\n    )\n    parser.add_argument(\n        \"--metadata_path\",\n        help=\"Path to sqlite database containing metadata\",\n        default=\"./data/metadata.sqlite3\",\n    )\n    parser.add_argument(\n        \"--save_dir\",\n        help=\"Path to train artifact output directory.\",\n        default=\"./data/output\",\n    )\n\n    args = parser.parse_args()\n    return args\n\n\ndef main(\n    config_path: str, training_data_dir: str, metadata_path: str, save_dir: str\n) -> None:\n    \"\"\"Run a training loop for a model.\n\n    Parameters\n    ----------\n    config_path: str\n        Path to configuration json for model one wants to train,\n        and data to train on.\n\n    training_data_dir: str\n        Path to directory containing training data.\n\n    metadata_path: str\n        Path to metadata sqlite file.\n\n    save_dir: str\n        Path to directory in which to save train artifacts.\n        If nonexistent, will be created.\n\n    Returns\n    -------\n    : None\n    \"\"\"\n    with open(config_path) as f:\n        cfg = json.load(f)\n\n    logger.info(\"Reading training metadata.\")\n\n    # Instantiate DB conn\n    db_path = os.path.abspath(metadata_path)\n    conn = sqlite3.connect(db_path)\n    conn.row_factory = dict_factory\n\n    # Get dataset specified in cfg from db\n    dataset_id = cfg[\"DatasetID\"]\n    dataset = get_dataset(conn, dataset_id)\n\n    # Get corresponding windows from db\n    windows = get_windows(conn, dataset_id)\n    windows_with_labels = []\n\n    # Populate labels associated with each window from db\n    for window in windows:\n        if window[\"hidden\"]:\n            continue\n        image = get_image(conn, window[\"image_id\"])\n        labels = get_labels(conn, window[\"id\"])\n        updated_window = window\n        updated_window[\"image\"] = image\n        updated_window[\"labels\"] = labels\n        windows_with_labels.append(updated_window)\n\n    conn.close()\n\n    # Train and validation loop\n    train_loop(cfg, dataset, windows_with_labels, save_dir, training_data_dir)\n    return None\n\n\nif __name__ == \"__main__\":\n    args = parse_args()\n    args_dict = vars(args)\n    main(**args_dict)\n"}
{"type": "source_file", "path": "src/training/train.py", "content": "import datetime\nimport json\nimport logging\nimport math\nimport os\nimport sys\nimport time\nimport typing as t\n\nimport numpy as np\nimport torch\nimport torch.cuda.amp\nimport torch.utils.data\nimport wandb\n\nfrom src.data.dataset import Dataset\nfrom src.data.image import Channels\nfrom src.data.transforms import get_transform\nfrom src.models import models\nfrom src.training.ema import EMA\nfrom src.training.evaluate import evaluate, get_evaluator\nfrom src.training.utils import collate_fn\n\nnum_loader_workers = 4\n\n# Acquire logger\nlogger = logging.getLogger(\"training\")\n\n\ndef train_loop(\n    model_cfg: dict,\n    dataset: dict,\n    windows: t.List[dict],\n    save_dir: str,\n    training_data_dir: str,\n) -> None:\n    \"\"\"Run training loop for detection or attribute prediction model.\n\n    Parameters\n    ----------\n    model_cfg: dict\n        Dictionary of model config.\n\n    dataset: dict\n        Dictionary specifying dataset from sqlite DB.\n\n    windows: list[dict]\n        List of window dictionaries corresponding to all sqlite records\n        from relevant database. Will get filtered by model_cfg.\n        TODO: Only retrieve relevant windows to begin with, rather\n        than getting all and filtering.\n\n    save_dir: str\n        Local directoy in which trained artifacts will be saved.\n\n    training_data_dir: str\n        Local directory in which training data (preprocess folder) lives.\n\n    Returns\n    -------\n    : None\n    \"\"\"\n    options = model_cfg[\"Options\"]\n\n    channels = Channels(model_cfg[\"Channels\"])\n    task = dataset[\"task\"]\n    model_cfg[\"Data\"] = {}\n    if dataset.get(\"task\"):\n        model_cfg[\"Data\"][\"task\"] = dataset[\"task\"]\n    if dataset.get(\"categories\"):\n        model_cfg[\"Data\"][\"categories\"] = dataset[\"categories\"]\n    model_name = model_cfg[\"Architecture\"]\n\n    run_tag = os.environ.get(\"RUN_TAG\", \"test_run\")\n    os.environ[\"WANDB_DIR\"] = save_dir\n    os.makedirs(save_dir, exist_ok=True)\n    experiment_name = \"-\".join(\n        [task, run_tag, datetime.datetime.now().strftime(\"%B-%d-%Y-%I-%M-%p\")]\n    )\n    run = wandb.init(\n        save_code=False,\n        name=experiment_name,\n        mode=os.environ.get(\"WANDB_MODE\", \"offline\"),\n    )\n\n    train_splits = options.get(\"TrainSplits\", [\"train\"])\n    val_splits = options.get(\"ValSplits\", [\"valid\"])\n    batch_size = options.get(\"BatchSize\", 4)\n    effective_batch_size = options.get(\"EffectiveBatchSize\", batch_size)\n    num_epochs = options.get(\"NumberEpochs\", 10)\n    chip_size = options.get(\"ChipSize\", 0)\n    image_size = options.get(\"ImageSize\", 0)\n\n    half_enabled = options.get(\"Half\", True)\n    summary_frequency = options.get(\"SummaryFrequency\", 8192)\n    restore_path = options.get(\"RestorePath\", None)\n    ema_factor = options.get(\"EMA\", 0)\n    save_path = os.path.join(save_dir, str(model_cfg[\"Name\"]), run_tag)\n    os.makedirs(save_path, exist_ok=True)\n\n    # Prepare transforms.\n    train_transforms = get_transform(\n        model_cfg, options, options.get(\"TrainTransforms\", [])\n    )\n    val_transforms = get_transform(model_cfg, options, options.get(\"ValTransforms\", []))\n\n    train_data = Dataset(\n        dataset=dataset,\n        windows=windows,\n        channels=channels,\n        splits=train_splits,\n        transforms=train_transforms,\n        image_size=image_size,\n        chip_size=chip_size,\n        preprocess_dir=os.path.join(training_data_dir, \"preprocess\"),\n    )\n\n    val_data = Dataset(\n        dataset=dataset,\n        windows=windows,\n        channels=channels,\n        splits=val_splits,\n        transforms=val_transforms,\n        image_size=image_size,\n        chip_size=chip_size,\n        valid=True,\n        preprocess_dir=os.path.join(training_data_dir, \"preprocess\"),\n    )\n\n    # Export model and training config\n    logger.info(f\"Writing training artifact outputs to {save_path}.\")\n    with open(os.path.join(save_path, \"cfg.json\"), \"w\") as f:\n        f.write(json.dumps(model_cfg))\n\n    logger.info(\n        \"Loaded {} train image references, and {} validation image references from DB.\".format(\n            len(train_data), len(val_data)\n        )\n    )\n    device = torch.device(\"cuda\")\n\n    train_sampler_cfg = options.get(\"TrainSampler\", {\"Name\": \"random\"})\n    if train_sampler_cfg[\"Name\"] == \"random\":\n        train_sampler = torch.utils.data.RandomSampler(train_data)\n    elif train_sampler_cfg[\"Name\"] == \"bg_balanced\":\n        train_sampler = train_data.get_bg_balanced_sampler()\n    else:\n        raise Exception(\"invalid sampler config {}\".format(train_sampler_cfg))\n\n    val_sampler = torch.utils.data.SequentialSampler(val_data)\n\n    train_loader = torch.utils.data.DataLoader(\n        train_data,\n        batch_size=batch_size,\n        sampler=train_sampler,\n        num_workers=num_loader_workers,\n        collate_fn=collate_fn,\n    )\n\n    val_loader = torch.utils.data.DataLoader(\n        val_data,\n        batch_size=batch_size,\n        sampler=val_sampler,\n        num_workers=num_loader_workers,\n        collate_fn=collate_fn,\n    )\n\n    # instantiate model with a number of classes\n    model_cls = models[model_name]\n    example = train_data[0]\n    logger.debug(\"The shape of training samples is {}.\".format(example[0].shape))\n    model = model_cls(\n        {\n            \"Channels\": channels,\n            \"Device\": device,\n            \"Model\": model_cfg,\n            \"Options\": options,\n            \"Data\": model_cfg[\"Data\"],\n            \"Example\": example,\n        }\n    )\n\n    # Restore saved model if requested.\n    if restore_path:\n        logger.info(f\"Restoring model from {restore_path}\")\n        state_dict = torch.load(restore_path)\n        model.load_state_dict(state_dict)\n\n    if ema_factor:\n        logger.info(\"creating EMA model\")\n        model = EMA(model, decay=ema_factor)\n\n    # move model to the correct device\n    model.to(device)\n\n    # construct an optimizer\n    optimizer_config = options.get(\"Optimizer\", {})\n    optimizer_name = optimizer_config.get(\"Name\", \"sgd\")\n    initial_lr = optimizer_config.get(\"InitialLR\", 0.001)\n    params = [p for p in model.parameters() if p.requires_grad]\n    if optimizer_name == \"adam\":\n        optimizer = torch.optim.Adam(params, lr=initial_lr)\n    elif optimizer_name == \"sgd\":\n        optimizer = torch.optim.SGD(params, lr=initial_lr)\n    else:\n        raise Exception(\"unknown optimizer name {}\".format(optimizer_name))\n\n    lr_scheduler = None\n\n    if \"Scheduler\" in options:\n        scheduler_config = options[\"Scheduler\"]\n        if scheduler_config[\"Name\"] == \"plateau\":\n            lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n                optimizer,\n                mode=\"min\",\n                factor=scheduler_config.get(\"Factor\", 0.1),\n                patience=scheduler_config.get(\"Patience\", 2),\n                min_lr=scheduler_config.get(\"MinLR\", 1e-5),\n                cooldown=scheduler_config.get(\"Cooldown\", 5),\n            )\n        else:\n            raise Exception(\"invalid scheduler config {}\".format(scheduler_config))\n\n    warmup_iters = 0\n    warmup_lr_scheduler = None\n\n    scaler = torch.cuda.amp.GradScaler(enabled=half_enabled)\n    best_score = None\n\n    # TensorBoard logging\n    cur_iterations = 0\n    summary_iters = summary_frequency // batch_size\n    summary_epoch = 0\n    summary_prev_time = time.time()\n    train_losses = []\n\n    if effective_batch_size:\n        accumulate_freq = effective_batch_size // batch_size\n    else:\n        accumulate_freq = 1\n\n    logger.info(f\"Beginning training for {num_epochs} epochs.\")\n    model.train()\n    for epoch in range(num_epochs):\n        logger.info(\"Starting epoch {}\".format(epoch))\n\n        model.train()\n        optimizer.zero_grad()\n\n        for images, targets in train_loader:\n            cur_iterations += 1\n\n            images = [image.to(device).float() / 255 for image in images]\n            targets = [\n                {\n                    k: v.to(device)\n                    for k, v in t.items()\n                    if not isinstance(v, str) and not isinstance(v, tuple)\n                }\n                for t in targets\n            ]\n\n            with torch.cuda.amp.autocast(enabled=half_enabled):\n                _, loss = model(images, targets)\n\n            loss_value = loss.item()\n            if not math.isfinite(loss_value):\n                logger.info(\"Loss is {}, stopping training\".format(loss_value))\n                sys.exit(1)\n\n            scaler.scale(loss).backward()\n\n            if cur_iterations == 1 or cur_iterations % accumulate_freq == 0:\n                scaler.step(optimizer)\n                scaler.update()\n                optimizer.zero_grad()\n\n                if ema_factor:\n                    model.update(summary_epoch)\n\n            train_losses.append(loss_value)\n\n            if warmup_lr_scheduler:\n                warmup_lr_scheduler.step()\n                if cur_iterations > warmup_iters + 1:\n                    logger.info(\"removing warmup_lr_scheduler\")\n                    warmup_lr_scheduler = None\n\n            if cur_iterations % summary_iters == 0:\n                train_loss = np.mean(train_losses)\n\n                eval_time = time.time()\n                model.eval()\n                evaluator = get_evaluator(task, options)\n                val_loss, _ = evaluate(\n                    model,\n                    device,\n                    val_loader,\n                    half_enabled=half_enabled,\n                    evaluator=evaluator,\n                )\n                val_scores = evaluator.score()\n                model.train()\n\n                val_score = val_scores[\"score\"]\n\n                if task == \"point\":\n                    # Note: due to current implementation, constant 0 val-loss is expected for\n                    # point detection task.\n                    wandb.log({\"train_loss\": train_loss, \"val_score\": val_score})\n                    logger.info(\n                        \"summary_epoch {}: train_loss={} val_score={} best_val_score={} elapsed={} lr={}\".format(\n                            summary_epoch,\n                            train_loss,\n                            val_score,\n                            best_score,\n                            int(eval_time - summary_prev_time),\n                            optimizer.param_groups[0][\"lr\"],\n                        )\n                    )\n                else:\n                    wandb.log({\"train_loss\": train_loss, \"val_loss\": val_loss})\n                    logger.info(\n                        \"summary_epoch {}: train_loss={} val_loss = {} val_score={} best_val_score={} elapsed={} lr={}\".format(\n                            summary_epoch,\n                            train_loss,\n                            val_loss,\n                            val_score,\n                            best_score,\n                            int(eval_time - summary_prev_time),\n                            optimizer.param_groups[0][\"lr\"],\n                        )\n                    )\n\n                del train_losses[:]\n                summary_epoch += 1\n                summary_prev_time = time.time()\n\n                # update the learning rate\n                if lr_scheduler and warmup_lr_scheduler is None:\n                    lr_scheduler.step(train_loss)\n\n                # Model saving.\n                if ema_factor:\n                    state_dict = model.shadow.state_dict()\n                else:\n                    state_dict = model.state_dict()\n                torch.save(state_dict, os.path.join(save_path, \"last.pth\"))\n\n                if best_score is None or val_score > best_score:\n                    torch.save(state_dict, os.path.join(save_path, \"best.pth\"))\n                    best_score = val_score\n                    if task == \"point\":\n                        # Log full val set confusion matrix\n                        evaluator.log_metrics(\"class0\", logger)\n                    if task == \"custom\":\n                        # Log full val set MAEs by attribute\n                        evaluator.log_metrics(logger)\n    run.finish()\n    return None\n"}
{"type": "source_file", "path": "src/utils/parallel.py", "content": "import multiprocessing\n\nfrom typing import Callable\n\n\ndef map(fn: Callable, tasks: list, workers: int) -> list:\n    if workers >= 2:\n        p = multiprocessing.Pool(workers)\n        output = p.map(fn, tasks)\n        p.close()\n    else:\n        output = [fn(task) for task in tasks]\n\n    return output\n\n\ndef starmap(fn: Callable, tasks: list, workers: int) -> list:\n    if workers >= 2:\n        p = multiprocessing.Pool(workers)\n        output = p.starmap(fn, tasks)\n        p.close()\n    else:\n        output = [fn(*task) for task in tasks]\n\n    return output\n"}
{"type": "source_file", "path": "src/data/preprocess.py", "content": "import os\nimport os.path\n\nimport numpy as np\nimport skimage.exposure\nimport skimage.io\n\nchip_size = 512\n\n\ndef pad_chip(image_data: dict, im: np.ndarray) -> np.ndarray:\n    \"\"\"Pad an image chip.\n\n    Parameters\n    ----------\n    image_data: dict\n        Image metadata.\n\n    im: np.ndarray\n        Raw image data as numpy array.\n\n    Returns\n    -------\n     : np.ndarray\n        Padded image.\n    \"\"\"\n    start_tile = [\n        image_data[\"Column\"] // chip_size,\n        image_data[\"Row\"] // chip_size,\n    ]\n    end_tile = [\n        (image_data[\"Column\"] + im.shape[1]) // chip_size,\n        (image_data[\"Row\"] + im.shape[0]) // chip_size,\n    ]\n    col_padding = (\n        image_data[\"Column\"] - start_tile[0] * chip_size,\n        (end_tile[0] + 1) * chip_size - (image_data[\"Column\"] + im.shape[1]),\n    )\n    row_padding = (\n        image_data[\"Row\"] - start_tile[1] * chip_size,\n        (end_tile[1] + 1) * chip_size - (image_data[\"Row\"] + im.shape[0]),\n    )\n    padding = [row_padding, col_padding]\n    while len(padding) < len(im.shape):\n        padding += [(0, 0)]\n    return np.pad(im, padding), start_tile\n\n\ndef preprocess(\n    image_data: dict,\n    fname: str,\n    channel: dict,\n    roi: dict = None,\n    dest_dir: str = \"../data\",\n) -> None:\n    \"\"\"Preprocess a Sentinel-1, Sentinel-2 or Landsat8 image.\n\n    Performs the following operations on the raw imagery:\n\n    Sentinel-1:\n    (1) Cast to uint8.\n    (2) Pad.\n    (3) Crop out tiles.\n    (4) Save all tiles associated with an image in preprocess output dir.\n\n    Parameters\n    ----------\n    image_data: dict\n        Metadata dict for raw image file to preprocess.\n\n    fname: str\n        Path to raw image file to preprocess.\n\n    channel: dict\n\n    roi: dict\n        Dict representation of ROI object as in sqlite metadata database.\n\n    dest_dir: str\n        Path to directory in which preprocessed data will get stored.\n\n    Returns\n    -------\n    : None\n    \"\"\"\n    out_dir = os.path.join(dest_dir, \"preprocess\")\n    im = skimage.io.imread(fname)\n    out_path = os.path.join(out_dir, image_data[\"UUID\"])\n    channel_name = channel[\"Name\"]\n    os.makedirs(os.path.join(out_path, channel_name), exist_ok=True)\n\n    # Normalize the image to 0-255.\n    if roi and roi[\"CatalogName\"] == \"sentinel2\":\n        if channel_name[0] == \"b\" and len(channel_name) == 3:\n            im = im // 4\n    elif roi and roi[\"CatalogName\"] == \"landsat8\":\n        # Landsat-8 seems to need per-band normalization to get good outputs.\n        im = skimage.exposure.equalize_hist(im)\n        band_min, band_max = im.min(), im.max()\n        im = 255 * (im - band_min) / (band_max - band_min)\n    elif roi and roi[\"CatalogName\"] == \"naip\":\n        # Get rid of the NIR channel if it exists, we just want RGB images for now (for consistency).\n        im = im[:, :, 0:3]\n\n    im = np.clip(im, 0, 255).astype(\"uint8\")\n\n    im, start_tile = pad_chip(image_data, im)\n\n    for i in range(0, im.shape[1] // chip_size):\n        for j in range(0, im.shape[0] // chip_size):\n            crop = im[\n                j * chip_size : (j + 1) * chip_size, i * chip_size : (i + 1) * chip_size\n            ]\n            if crop.max() == 0:\n                continue\n            skimage.io.imsave(\n                os.path.join(\n                    out_path,\n                    channel_name,\n                    \"{}_{}.png\".format(start_tile[0] + i, start_tile[1] + j),\n                ),\n                crop,\n                check_contrast=False,\n            )\n"}
{"type": "source_file", "path": "src/downstream/correlation_utils.py", "content": "import glob\nimport os\nimport typing as t\nfrom datetime import datetime\n\nimport pandas as pd\nfrom osgeo import gdal, osr\n\nfrom src.data.image import InvalidDataError\n\n\ndef get_extent(ds: gdal.Dataset) -> t.Tuple[t.Tuple[float]]:\n    \"\"\"Return list of corner coordinates from a gdal Dataset.\n\n    Return the corner coordinates of the minimal rectangle \n    circumscribing a gdal dataset. Corner coordinates\n    are returned with the ordering\n    (xmin, ymax), (xmax, ymax), (xmax, ymin), (xmin, ymin).\n\n    Parameters\n    ----------\n    ds: gdal.Dataset\n        Input gdal dataset\n\n    Returns\n    -------\n    : tuple[tuple[float]]\n        Corner coordinates of the minimal circumscribed\n        rectangle surrounding the input dataset.\n    \"\"\"\n    xmin, xpixel, _, ymax, _, ypixel = ds.GetGeoTransform()\n    width, height = ds.RasterXSize, ds.RasterYSize\n    xmax = xmin + width * xpixel\n    ymin = ymax + height * ypixel\n\n    return (xmin, ymax), (xmax, ymax), (xmax, ymin), (xmin, ymin)\n\n\ndef reproject_coords(coords: t.List[t.List[float]],\n                     src_srs: osr.SpatialReference, tgt_srs: osr.SpatialReference) -> t.List[t.List[float]]:\n    \"\"\"Reproject a list of coordinates from one coordinate system to another.\n\n    Parameters\n    ----------\n    coords: list[list[float]]\n        Original coordinates in source coordinate system.\n\n    src_srs: osr.SpatialReference\n        A source SpatialReference object specifying coordinate system.\n\n    tgt_srs: osr.SpatialReference\n        A target SpatialReference object specifying coordinate system. \n\n    Returns\n    -------\n    trans_coords: list[list[float]]\n        Transformed coordinates in target coordinate system.\n    \"\"\"\n    trans_coords = []\n    transform = osr.CoordinateTransformation(src_srs, tgt_srs)\n    for x, y in coords:\n        x, y, z = transform.TransformPoint(x, y)\n        trans_coords.append([x, y])\n    return trans_coords\n\n\ndef get_frame_coords(raster_path: str, catalog: str) -> t.List[t.List[float]]:\n    \"\"\"Get list of (lon, lat) frame boundary coordinates.\n\n    Returned list specifies a _closed_ polygon (i.e. starting\n    coordinates are required to be the same as ending coordinates).\n\n    Parameters\n    ----------\n    raster_path: str\n        Path to a raw satellite image raster.\n\n    catalog: str\n        Specifies the satellite catalog from which the\n        passed scene_dir is drawn. Only currently supported\n        values are \"sentinel1\" and \"sentinel2\".\n\n    Returns\n    -------\n    coordinates: list[list[float]]\n        List of (lon, lat) frame boundary coordinates.\n    \"\"\"\n    ds = gdal.Open(raster_path)\n\n    if catalog == \"sentinel2\":\n        ext = get_extent(ds)\n        src_srs = osr.SpatialReference()\n        src_srs.ImportFromWkt(ds.GetProjection())\n        tgt_srs = osr.SpatialReference()\n        tgt_srs.ImportFromEPSG(4326)\n        geo_ext = reproject_coords(ext, src_srs, tgt_srs)\n        geo_ext_lon_first = [[p[1], p[0]] for p in geo_ext]\n\n    if catalog == \"sentinel1\":\n        transformer = gdal.Transformer(ds, None, [\"DST_SRS=WGS84\"])\n        _, p1 = transformer.TransformPoint(0, 0, 0, 0)\n        _, p2 = transformer.TransformPoint(0, 0, ds.RasterYSize, 0)\n        _, p3 = transformer.TransformPoint(0, ds.RasterXSize, 0, 0)\n        _, p4 = transformer.TransformPoint(0, ds.RasterXSize, ds.RasterYSize, 0)\n        geo_ext_lon_first = [p1, p2, p3, p4]\n        geo_ext_lon_first = [p[0:2] for p in geo_ext_lon_first]\n\n    # Close loop:\n    geo_ext_lon_first.append(geo_ext_lon_first[0])\n    coordinates = geo_ext_lon_first\n\n    return coordinates\n\n\ndef get_raster_path_base(scene_dir_path: str, catalog: str) -> str:\n    \"\"\"Retreives path to one sample raster from a satellite scene dir.\n\n    Parameters\n    ----------\n    scene_dir_path: str\n        Path to a satellite scene dir (Sentinel-1 or Sentinel-2).\n\n    catalog: str\n        Specifies the satellite catalog from which the\n        passed scene_dir is drawn. Only currently supported\n        values are \"sentinel1\" and \"sentinel2\".\n\n    Returns\n    -------\n    path: str\n        Path to a raster file within the specified satellite scene\n        dir.\n    \"\"\"\n    if catalog == \"sentinel2\":\n        coi_abbrev = \"TCI\"\n        path_pattern = os.path.join(scene_dir_path, f\"GRANULE/*/IMG_DATA/*_{coi_abbrev}.jp2\")\n        paths = glob.glob(path_pattern)\n        if len(paths) == 1:\n            path = paths[0]\n        else:\n            raise InvalidDataError(\n                f\"Raw Sentinel-2 data must be of L1C product type, and contain channel={coi_abbrev}.\\n\"\n                f\"Did not find a unique path using the pattern: {path_pattern}\"\n            )\n    if catalog == \"sentinel1\":\n        channel = \"vh\"\n        measurement_path = os.path.join(scene_dir_path, \"measurement\")\n        fnames = {\n            fname.split(\"-\")[3]: fname for fname in os.listdir(measurement_path)\n        }\n        path = os.path.join(measurement_path, fnames[channel])\n        if not os.path.exists(path):\n            raise InvalidDataError(\n                f\"Raw Sentinel-1 data does not contain required polarization channel={channel}.\\n\"\n            )\n    return path\n\n\ndef construct_scene(scene_dir_path: str, catalog: str) -> dict:\n    \"\"\"Constructs a diu-correlation-api compatible scene dict.\n\n    Given a path to a Sentinel-1 or Sentinel-2 scene, this\n    constructs a dictionary of scene metadata that can be\n    used with the diu-correlation-api service.\n\n    Parameters\n    ----------\n    scene_dir_path: str\n        Path to an uncompressed raw satellite product dir.\n\n        E.g. a Sentinel-1 SAFE dir.\n\n    catalog: str\n        Specification of satellite frame type.\n\n        Currently supported catalogs are \"sentinel1\" and \"sentinel2\".\n        For \"sentinel1\", only Level-1 GRD products are supported.\n        For \"sentinel2\", only MSI L1C products are supported.\n\n\n    Returns\n    -------\n    scene_obj: dict\n        A diu-correlation-api compatible dictionary encoding\n        the satellite scene metadata.\n    \"\"\"\n    raster_path = get_raster_path_base(scene_dir_path, catalog)\n    frame_coordinates = get_frame_coords(raster_path, catalog)\n    if catalog == \"sentinel2\":\n        time_loc = 2\n    if catalog == \"sentinel1\":\n        time_loc = 4\n    frame_timestamp = os.path.basename(scene_dir_path).split(\"_\")[time_loc]\n    frame_timestamp = datetime.strptime(frame_timestamp, \"%Y%m%dT%H%M%S\")\n    frame_timestamp = frame_timestamp.isoformat()\n\n    scene_obj = {\"ts\": frame_timestamp, \"polygon_points\": frame_coordinates}\n\n    return scene_obj\n\n\ndef format_detections(detections_path: str, scene_obj: dict) -> t.List[dict]:\n    \"\"\"Constructs diu-correlation-api compatible detection object from csv.\n\n    Given a path to a Sentinel-1 or Sentinel-2 model output csv, this\n    constructs a dictionary of detection metadata that can be\n    used with the diu-correlation-api service.\n\n    Parameters\n    ----------\n    detections_path: str\n        Path to a a predictions csv.\n\n    scene_obj: str\n        A diu-correlation-api compatible dictionary encoding\n        the satellite scene metadata.\n\n\n    Returns\n    -------\n    detection_arr: list[dict]\n        A diu-correlation-api compatible dictionary encoding detections\n        from a scene.\n    \"\"\"\n    detection_df = pd.read_csv(detections_path)\n\n    detection_arr = detection_df.apply(\n        lambda x: {\"id\": x.detect_id, \"ts\": scene_obj[\"ts\"],\n                   \"lon\": x.lon, \"lat\": x.lat}, axis=1).tolist()\n\n    return detection_arr\n"}
{"type": "source_file", "path": "src/utils/filter.py", "content": "from functools import partial\n\nimport numpy as np\nimport pandas as pd\n\nfrom src.utils.geom import extremal_bounds\n\n\ndef filter_detection(row: pd.Series, loc_df: str, default_width_m: float = 100) -> bool:\n    \"\"\"Return True if a row should be filtered due to overlap with loc_df.\n\n    Parameters\n    ----------\n    row: pandas.Series\n        Row of a pandas series with lat and lon columns, whose rows are detections.\n\n    loc_df: pandas.DataFrame\n        Dataframe with columns \"lon\", \"lat\" and \"width_m\" specifying\n        locations of undesired detection centers, and the (assumed to be square)\n        extent of the undesired object.\n\n    default_width_m: float\n        A default width to assign to locations in loc_df if the width_m field is empty.\n\n\n    Returns\n    -------\n    : bool\n        True if detection should be filtered due to overlap.\n    \"\"\"\n\n    detect_lon = row.lon\n    detect_lat = row.lat\n\n    for _, r in loc_df.iterrows():\n        lon = r.lon\n        lat = r.lat\n        width_m = r.width_m\n        if np.isnan(width_m):\n            width_m = default_width_m\n\n        min_lon, min_lat, max_lon, max_lat = extremal_bounds(lon, lat, width_m)\n\n        if (\n            (detect_lon >= min_lon)\n            and (detect_lon <= max_lon)\n            and (detect_lat >= min_lat)\n            and (detect_lat <= max_lat)\n        ):\n            return True\n\n    return False\n\n\ndef filter_out_locs(\n    df: pd.DataFrame, loc_path: str, default_width_m: float = 100\n) -> pd.DataFrame:\n    \"\"\"Filter out rows in df overlapping locations specified in loc_path.\n\n    Parameters\n    ----------\n    df: pandas.DataFrame\n        Dataframe of predictions containing, at the least, columns\n        named \"lon\" and \"lat\".\n\n    loc_path: str\n        Path to a csv with columns \"lon\", \"lat\" and \"width_m\" specifying\n        locations of undesired detection centers, and the (assumed to be square)\n        extent of the undesired object.\n\n    Returns\n    -------\n    filtered_df: pandas.DatadFrame\n        A sub-dataframe of df with the rows corresponding to detections\n        overlapping the undesired object extents removed.\n    \"\"\"\n    loc_df = pd.read_csv(loc_path)\n\n    remove_detection = partial(\n        filter_detection, loc_df=loc_df, default_width_m=default_width_m\n    )\n\n    filtered_df = df[~df.apply(remove_detection, axis=1)]\n\n    return filtered_df\n"}
