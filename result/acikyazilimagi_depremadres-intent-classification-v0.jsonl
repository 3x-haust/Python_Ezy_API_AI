{"repo_info": {"repo_name": "depremadres-intent-classification-v0", "repo_owner": "acikyazilimagi", "repo_url": "https://github.com/acikyazilimagi/depremadres-intent-classification-v0"}}
{"type": "test_file", "path": "tests/__init__.py", "content": ""}
{"type": "test_file", "path": "tests/ml/__init__.py", "content": ""}
{"type": "test_file", "path": "tests/ml/classifiers/test_models.py", "content": "from unittest import TestCase\nfrom unittest.mock import Mock, patch\n\nfrom src.ml_modules.bert_classifier import BertClassifier\nfrom src.ml_modules.rule_based_clustering import RuleBasedClassifier\n\n\nclass TestClassifiers(TestCase):\n\n    @classmethod\n    def setUpClass(cls):\n        cls.classifiers = [BertClassifier(), RuleBasedClassifier()]\n\n    def test_classify_empty_case(self):\n        with patch('src.ml_modules.bert_classifier.requests.post') as mock_post:\n            mock_post.return_value = Mock()\n            mock_post.return_value.json.return_value = [\n                [\n                    {\n                        \"label\": \"foo\",\n                        \"score\": 0.0,\n                    },\n                    {\n                        \"label\": \"bar\",\n                        \"score\": 0.0,\n                    },\n                ]\n            ]\n            mock_post.return_value.status_code = 200\n\n            for classifier in self.classifiers:\n                result = classifier.classify(\"\")\n                assert isinstance(result, list)\n                assert len(result) == 0\n\n    def test_classify_base(self):\n        with patch('src.ml_modules.bert_classifier.requests.post') as mock_post:\n            mock_post.return_value = Mock()\n            mock_post.return_value.json.return_value = [\n                [\n                    {\n                        \"label\": \"foo\",\n                        \"score\": 0.7,\n                    },\n                    {\n                        \"label\": \"bar\",\n                        \"score\": 0.4,\n                    },\n                ]\n            ]\n            mock_post.return_value.status_code = 200\n\n            for classifier in self.classifiers:\n                result = classifier.classify(\n                    \"Enkaz altındayım lütfen yardım edin\")\n                assert isinstance(result, list)\n                assert len(result) > 0\n                for el in result:\n                    assert isinstance(el, str)\n"}
{"type": "test_file", "path": "tests/ml/classifiers/__init__.py", "content": ""}
{"type": "source_file", "path": "src/data_modules/process_csv_turkish_data.py", "content": "import re\n\nimport pandas as pd\n# from unicodedata import normalize\nfrom unidecode import unidecode\n\n\ndef get_data(file_name):\n    df = pd.read_csv(file_name, header=None)\n    return df\n\n# write pd dataframe to csv\n\n\ndef write_to_csv(df, file_name):\n    df.to_csv(file_name, index=False)\n\n\ndef check_regex(full_text):\n    full_text = re.sub(r'[^a-z\\s]+', '', full_text, flags=re.IGNORECASE)\n    return full_text\n\n\ndef remove_diacritics(text):\n    # define the mapping from diacritic characters to non-diacritic characters\n    mapping = {\n        '\\u00c7': 'C', '\\u00e7': 'c',\n        '\\u011e': 'G', '\\u011f': 'g',\n        '\\u0130': 'I', '\\u0131': 'i',\n        '\\u015e': 'S', '\\u015f': 's',\n        '\\u00d6': 'O', '\\u00f6': 'o',\n        '\\u00dc': 'U', '\\u00fc': 'u',\n        '\\u0152': 'OE', '\\u0153': 'oe',\n        '\\u0049': 'I', '\\u0131': 'i',\n    }\n\n    # replace each diacritic character with its non-diacritic counterpart\n    text = ''.join(mapping.get(c, c) for c in text)\n\n    return text\n\n\nif __name__ == \"__main__\":\n    df = get_data(\"deprem_convert_csv/v10.csv\")\n    # df.drop(index=df.index[1], inplace=True) # drop first row\n    print(df.head())\n    for i in df.columns:\n        df[i] = df[i].apply(lambda x: unidecode(x) if type(x) == str else x)\n    print(df.head())\n    write_to_csv(df, \"deprem_converted_csv/v10.csv\")\n"}
{"type": "source_file", "path": "src/eval_modules/__init__.py", "content": ""}
{"type": "source_file", "path": "src/data_modules/url_redirect.py", "content": "from typing import List, Union\n\nimport requests\nfrom requests import Response\n\n\ndef chase_redirects(url: Union[List, str]) -> List[str]:\n    _urls = []\n\n    def _chase(url: str):\n        resp: Response = requests.head(url)\n        if 300 < resp.status_code < 400:\n            return resp.headers[\"location\"]\n\n    if isinstance(url, list):\n        for each in url:\n            _urls.append(_chase(each))\n\n    if isinstance(url, str):\n        _urls.append(_chase(url))\n\n    return _urls\n\n\n# url = [\"https://t.co/xRhOZeNJLe\", \"https://t.co/Dy33xpoxAV\"]\n# ret = chase_redirects(url)\n# print(ret)\n"}
{"type": "source_file", "path": "src/eval_modules/eval.py", "content": "\"\"\"Eval.\n\nUsing the eval.csv file, evaluate the performance of the classifier for each intent.\n\nUsage:\n    python3 eval_modules.eval\n\n\"\"\"\n\nimport argparse\n\nimport pandas as pd\n\nfrom src.ml_modules.bert_classifier import BertClassifier\nfrom src.ml_modules.rule_based_clustering import (RuleBasedClassifier,\n                                                  preprocess_tweet)\n\n# Define command line arguments to control which classifiers to run.\nparser = argparse.ArgumentParser()\n\n# CSV file containing the eval data.\nparser.add_argument('--eval_file', type=str, default=\"../data/eval.csv\")\n\n# Number of entries to use for evaluation. 0 means use all entries.\nparser.add_argument('--max_num_entries', type=int, default=0)\n\nparser.add_argument('--run_rule_based_classifier',\n                    action=argparse.BooleanOptionalAction, default=True)\nparser.add_argument('--run_bert_classifier',\n                    action=argparse.BooleanOptionalAction, default=False)\nargs = parser.parse_args()\n\n\nclass ClassificationEval(object):\n    def __init__(self, eval_frame, classifier_instance):\n        self.classifier = classifier_instance\n        self.eval_frame = eval_frame\n\n    def __eval_fn(self, arg):\n        def err():\n            print(f\"No funciton named 'classify()' found in {self.classifier}\")\n        func = getattr(self.classifier, 'classify', err)\n        # TODO find a more robust way of getting at most one item and/or handling multiclass eval.\n        return func(arg)\n\n    def __all_intents_fn(self):\n        def err():\n            print(\n                f\"No funciton named 'all_intents()' found in {self.classifier}\")\n        func = getattr(self.classifier, 'all_intents', err)\n        return func()\n\n    def __prep_eval_frame(self, df):\n        # Fill NaNs with empty strings to include no intent tweets.\n        df = df.fillna(\"\")\n        df = df[df['label'].notna()]\n        df = df[df['tweet_text'].notna()]\n        df['tweet_text'] = df['tweet_text'].apply(preprocess_tweet)\n        # Only needed columps\n        df = df[['tweet_text', 'label']]\n        # One hot encode the labels.\n        for intent in self.__all_intents_fn():\n            df[f'{intent}_golden'] = df['label'] == intent\n        return df\n\n    def __prep_classification_frame(self, df):\n        \"\"\"Only using tweet_text, return a one hot encoded prediciton frame\"\"\"\n        all_intents = self.__all_intents_fn()\n        # Returns a set.\n        df['predicted'] = df['tweet_text'].apply(self.__eval_fn)\n\n        # Create a one hot encoded frame.\n        for intent in all_intents:\n            df[f'{intent}_pred'] = df['predicted'].apply(lambda x: intent in x)\n\n        del df['predicted']\n        return df\n\n    def eval(self):\n        df = self.__prep_eval_frame(self.eval_frame)\n        df = self.__prep_classification_frame(df)\n\n        df['predicted'] = df['tweet_text'].apply(self.__eval_fn)\n        for intent in self.__all_intents_fn():\n            # Calculate false positives.\n            df[f'{intent}_fp'] = df.apply(\n                lambda x: not x[f'{intent}_golden'] and x[f'{intent}_pred'], axis=1)\n            # Calculate false negatives.\n            df[f'{intent}_fn'] = df.apply(\n                lambda x: x[f'{intent}_golden'] and not x[f'{intent}_pred'], axis=1)\n            # Calculate true positives.\n            df[f'{intent}_tp'] = df.apply(\n                lambda x: x[f'{intent}_golden'] and x[f'{intent}_pred'], axis=1)\n\n        # Calcualte metrics for each intent.\n        for intent in self.__all_intents_fn():\n            print(f\"Intent: {intent}\")\n            print(\"==================================\")\n            # Calculate precision.\n            precision = df[f'{intent}_tp'].sum(\n            ) / (df[f'{intent}_tp'].sum() + df[f'{intent}_fp'].sum())\n            print(f\"{intent} Precision: {precision:.2f}\")\n            # Calculate recall.\n            recall = df[f'{intent}_tp'].sum(\n            ) / (df[f'{intent}_tp'].sum() + df[f'{intent}_fn'].sum())\n            print(f\"{intent} Recall: {recall:.2f}\")\n            # Calculate F1 score.\n            f1 = 2 * (precision * recall) / (precision + recall)\n            print(f\"{intent} F1: {f1:.2f}\")\n            print(\"\")\n\n        return df\n\n\nif __name__ == '__main__':\n    eval_frame = pd.read_csv(args.eval_file)\n    if args.max_num_entries:\n        eval_frame = eval_frame[args.max_num_entries]\n\n    if args.run_rule_based_classifier:\n        eval = ClassificationEval(eval_frame, RuleBasedClassifier())\n        eval.eval()\n\n    if args.run_bert_classifier:\n        eval = ClassificationEval(eval_frame, BertClassifier())\n        eval.eval()\n"}
{"type": "source_file", "path": "src/main.py", "content": "\"\"\"\nCommand line verison of the intent classification app.\n\nUsage:\n    python app_main.py --run_rule_based_classifier --run_bert_classifier -text='Yardim'\n\"\"\"\nimport argparse\n\n# ML modules\nfrom src.ml_modules.rule_based_clustering import RuleBasedClassifier\nfrom src.ml_modules.bert_classifier import BertClassifier\n\n# Define command line arguments to control which classifiers to run.\nparser = argparse.ArgumentParser()\nparser.add_argument('--text', type=str)\nparser.add_argument('--run_rule_based_classifier',\n                    action=argparse.BooleanOptionalAction, default=True)\nparser.add_argument('--run_bert_classifier',\n                    action=argparse.BooleanOptionalAction, default=True)\nargs = parser.parse_args()\n\n# Initialize classifiers\nrule_based_classifier = None\nif args.run_rule_based_classifier:\n    rule_based_classifier = RuleBasedClassifier()\n\nbert_classifier = None\nif args.run_bert_classifier:\n    bert_classifier = BertClassifier()\n\n\ndef run_classifiers(text):\n    intents = []\n\n    if args.run_rule_based_classifier:\n        assert rule_based_classifier\n        intents.extend(rule_based_classifier.classify(text))\n\n    if args.run_bert_classifier:\n        assert bert_classifier\n        intents.extend(bert_classifier.classify(text))\n\n    # Remove duplicates.\n    intents = list(set(intents))\n    return intents\n\n\nif __name__ == '__main__':\n    intents = run_classifiers(args.text)\n    print(intents)\n"}
{"type": "source_file", "path": "src/ml_modules/__init__.py", "content": ""}
{"type": "source_file", "path": "src/ml_modules/base_classifier.py", "content": "from abc import ABC, abstractmethod\nfrom typing import List\n\n\nclass BaseClassifier(ABC):\n\n    @abstractmethod\n    def classify(self, text: str) -> List[str]:\n        raise NotImplementedError\n\n    @abstractmethod\n    def all_intents(self) -> List[str]:\n        raise NotImplementedError\n"}
{"type": "source_file", "path": "src/ml_modules/gpt3_classifier.py", "content": "from typing import List\n\nfrom src.ml_modules.base_classifier import BaseClassifier\n\n\nclass GPT3Classifier(BaseClassifier):\n\n    def classify(self, text: str) -> List[str]:\n        raise NotImplementedError\n\n    def all_intents(self) -> List[str]:\n        raise NotImplementedError\n"}
{"type": "source_file", "path": "src/data_modules/pg_ops.py", "content": "import os\nfrom typing import Any, List, Tuple\n\nimport psycopg2\nfrom dotenv import load_dotenv\n\n\ndef get_data(conn: psycopg2.connection, table_name: str, column_names: List[str],\n             condition: str) -> List[Tuple]:\n    \"\"\" Get data from the table\n\n    Args:\n        conn: PostgreSQL connection object\n        table_name: Table name to get data\n        column_names: Name of the columns to select\n        condition: WHERE condition\n\n    Returns:\n        Query results\n    \"\"\"\n    cur = conn.cursor()\n    query = \"SELECT {} FROM {} WHERE {}\".format(', '.join(column_names), table_name, condition)\n    # query for filtering data with multiple claueses\n    # query = \"SELECT {} FROM {} WHERE is_done = True AND intent_result = ''\".format(', '.join(column_names), table_name) # noqa\n    cur.execute(query)\n    return cur.fetchall()\n\n\ndef update_data(conn: psycopg2.connection, table_name: str, column_name: str, new_value: Any,\n                condition: str) -> None:\n    \"\"\" Update data in the table\n\n    Args:\n        conn: PostgreSQL connection object\n        table_name: Table name to update data\n        column_name: Name of the column to update\n        new_value: New value for the column\n        condition: WHERE condition\n    \"\"\"\n\n    cur = conn.cursor()\n    query = \"UPDATE {} SET {} = '{}' WHERE {}\".format(table_name, column_name, new_value, condition)\n    cur.execute(query)\n    conn.commit()\n\n\ndef connect_to_db():\n    \"\"\" Connect to the database\n\n    Returns:\n        PostgreSQL connection object\n    \"\"\"\n    # Load environment variables\n    load_dotenv(\".env\")\n\n    host = os.getenv('HOST')\n    database = os.getenv('DATABASE')\n    user = os.getenv('USERNAME')\n    password = os.getenv('PASSWORD')\n\n    # Connect to the database\n    conn = psycopg2.connect(\n        host=host,\n        database=database,\n        user=user,\n        password=password\n    )\n    return conn\n"}
{"type": "source_file", "path": "src/app_main.py", "content": "﻿import argparse\nfrom typing import List\n\nimport uvicorn\nfrom aiokafka import AIOKafkaConsumer\nfrom fastapi import FastAPI, HTTPException\nfrom pydantic import BaseModel\n\n# ML modules\nfrom src.ml_modules.bert_classifier import BertClassifier\nfrom src.ml_modules.rule_based_clustering import RuleBasedClassifier\n\n# import ml_modules.run_zsc as zsc\n\n# Define command line arguments to control which classifiers to run.\nparser = argparse.ArgumentParser()\nparser.add_argument('--run_rule_based_classifier',\n                    action=argparse.BooleanOptionalAction, default=True)\nparser.add_argument('--run_bert_classifier',\n                    action=argparse.BooleanOptionalAction, default=True)\nargs = parser.parse_args()\n\n# Initialize classifiers\nrule_based_classifier = None\nif args.run_rule_based_classifier:\n    rule_based_classifier = RuleBasedClassifier()\n\nbert_classifier = None\nif args.run_bert_classifier:\n    bert_classifier = BertClassifier()\n\n# Initialize fastapi.\napp = FastAPI()\n\n\n# Data models.\nclass Request(BaseModel):\n    text: str\n\n\nclass Response(BaseModel):\n    intents: List[str]\n\n\n@app.post(\"/get_intents/\")\nasync def Get_Intent(item: Request) -> Response:\n    if not item.text:\n        raise HTTPException(status_code=400, detail=\"Bad request, no text\")\n\n    try:\n        intents = []\n\n        if args.run_rule_based_classifier:\n            assert rule_based_classifier\n            intents.extend(rule_based_classifier.classify(item.text))\n\n        if args.run_bert_classifier:\n            assert bert_classifier\n            intents.extend(bert_classifier.classify(item.text))\n\n        # Remove duplicates.\n        intents = list(set(intents))\n        return intents\n\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n\n\nasync def start_kafka_consumer(app):\n    consumer = AIOKafkaConsumer(\n        \"your_topic\",\n        bootstrap_servers=\"localhost:9092\",\n        group_id=\"your_group_id\",\n        auto_offset_reset=\"earliest\"\n    )\n\n    # Start consuming\n    await consumer.start()\n\n    try:\n        # Poll for new messages\n        async for msg in consumer:\n            print(f\"Consumed message: {msg.value}\")\n    finally:\n        # Close the consumer\n        await consumer.stop()\n\n\nif __name__ == '__main__':\n    # app.add_event_handler(\"startup\", start_kafka_consumer)\n    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n"}
{"type": "source_file", "path": "src/ml_modules/intent_config/__init__.py", "content": ""}
{"type": "source_file", "path": "src/__init__.py", "content": ""}
{"type": "source_file", "path": "src/data_modules/__init__.py", "content": ""}
{"type": "source_file", "path": "src/data_modules/get_all_data.py", "content": "import csv\n\nimport pg_ops\n\nconn = pg_ops.connect_to_db()\n\ndata = pg_ops.get_data(conn, 'tweets_depremaddress', [\n                       'id', 'full_text', 'tweet_id', 'geo_link'], '1=1')\n\n# function to write data to csv file\n\n\ndef write_to_csv(data, filename):\n    with open(filename, 'w', newline='') as csvfile:\n        # creating a csv writer object\n        csvwriter = csv.writer(csvfile)\n        # writing the fields\n        csvwriter.writerow(['id', 'full_text', 'tweet_id', 'geo_loc'])\n        # writing the data rows\n        csvwriter.writerows(data)\n\n# write_to_csv(data, 'data.csv')\n"}
{"type": "source_file", "path": "src/ml_modules/bert_classifier.py", "content": "import logging\nimport os\nfrom typing import List\n\nimport requests\nfrom dotenv import load_dotenv\n\nfrom src.ml_modules.base_classifier import BaseClassifier\n\nload_dotenv(\".env\")\n\nAPI_TOKEN = os.getenv(\"HF_HUB_TOKEN\")\nMODEL_NAME = \"deprem-ml/multilabel_earthquake_tweet_intent_bert_base_turkish_cased\"\n\n# Add logging.\nlogging.basicConfig(level=logging.INFO)\n\n\nclass BertClassifier(BaseClassifier):\n    \"\"\"\n    BERT based classifier to uses huggingface inference API to classify tweets.\n\n    Example usage:\n    >>> classifier = BertClassifier()\n    >>> classifier.classify(\"Kırıkkale'de deprem oldu.\")\n    [\"KURTARMA\"]\n    \"\"\"\n\n    def __init__(self, classification_threshold=0.5):\n        # The score threshold to deem a label as positive.\n        self.classification_threshold = classification_threshold\n\n        self.headers = {\"Authorization\": f\"Bearer {API_TOKEN}\"}\n        self.api_url = f\"https://api-inference.huggingface.co/models/{MODEL_NAME}\"\n\n    def __query(self, text):\n        response = requests.post(\n            self.api_url, headers=self.headers, json={\"inputs\": text})\n\n        # raise HTTPException if status code != 200\n        response.raise_for_status()\n        return response.json()\n\n    def all_intents(self):\n        \"\"\"\n        Returns list of all possible intents this classifier can classify.\n        \"\"\"\n        return [\"ALAKASIZ\", \"KURTARMA\", \"BARINMA\", \"SAGLIK\", \"LOJISTIK\", \"SU\",\n                \"YAGMA\", \"YEMEK\", \"GIYSI\", \"ELEKTRONIK\"]\n\n    @property\n    def none_label(self):\n        return \"ALAKASIZ\"\n\n    def classify(self, text: str) -> List[str]:\n        \"\"\"\n        Check if the given text contains any of the keywords of any intent.\n        Args:\n            text: The text to check.\n\n        Returns:\n            List of labels of the tweet, if any, ordered by score.\n        \"\"\"\n        response = self.__query(text)\n        logging.info(f\"Response: {response}\")\n        labels = []\n\n        if response:\n            # Labels are returned as a list of lists.\n            labels_and_scores = response[0]\n            labels = [l_and_s[\"label\"].upper()\n                      for l_and_s in labels_and_scores\n                      if l_and_s[\"score\"] > self.classification_threshold\n                      ]\n\n        # if ALAKASIZ and other category survive after threshold filter,\n        # remove ALAKASIZ\n        if self.none_label in labels and len(labels) > 1:\n            labels.pop(self.none_label)\n\n        # Don't return set, as it will lose ordering.\n        return labels\n\n\n# If name main\nif __name__ == \"__main__\":\n    text = \"Kırıkkale'de deprem oldu.\"\n    labels = BertClassifier().classify(text)\n    print(labels)\n"}
{"type": "source_file", "path": "src/ml_modules/rule_based_clustering.py", "content": "import os\nimport re\nfrom typing import Dict, List, Optional, Set, Tuple, Union\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom unidecode import unidecode\nfrom src.ml_modules.base_classifier import BaseClassifier\n\n\n# Directory of this file.\nCUR_DIR = os.path.dirname(os.path.realpath(__file__))\n\n# Directory that has <intent>.txt config files.\nINTENT_CONFIG_DIR = os.path.join(CUR_DIR, \"intent_config\")\n\n\nclass RuleBasedClassifier(BaseClassifier):\n    \"\"\"\n    Rule based classifier that uses regex patterns to classify tweets.\n\n    It will read all the .txt files in the intent_config directory and use the\n    file name as the intent name and the file contents as the keywords.\n\n    The keywords are used to compile regex patterns that are used to classify\n    the tweets.\n\n    Example usage:\n    >>> classifier = RuleBasedClassifier()\n    >>> classifier.classify(\"Yardim edin\")\n    {\"KURTARMA\"}\n    \"\"\"\n\n    def __init__(self, intent_config_dir=INTENT_CONFIG_DIR):\n        self.intent_to_keywords = {}  # Will be loaded below.\n        self.intent_to_patterns = {}  # Will be loaded below.\n        self.__load_intent_configs(intent_config_dir)\n        self.__compile_keywords_to_patterns()\n\n    def __load_intent_configs(self, intent_config_dir):\n        configs = [f for f in os.listdir(\n            intent_config_dir) if f.endswith(\".txt\")]\n        self.intent_to_config = {os.path.splitext(c)[0]: c for c in configs}\n        for intent, config in self.intent_to_config.items():\n            with open(os.path.join(intent_config_dir, config), \"r\") as f:\n                self.intent_to_keywords[intent] = f.read().splitlines()\n\n    def __compile_keywords_to_patterns(self):\n        for intent, keywords in self.intent_to_keywords.items():\n            self.intent_to_patterns[intent] = [re.compile(\n                f\"(^|\\\\W){k}($|\\\\W)\", re.IGNORECASE) for k in keywords]\n\n    def all_intents(self):\n        \"\"\"\n        Returns list of all possible intents this classifier can classify.\n        \"\"\"\n        return self.intent_to_patterns.keys()\n\n    def classify(self, text: str) -> List[str]:\n        \"\"\"\n        Check if the given text contains any of the keywords of any intent.\n        Args:\n            text: The text to check.\n\n        Returns:\n            Set of labels of the tweet, if any.\n        \"\"\"\n        intents = []\n        for intent, patterns in self.intent_to_patterns.items():\n            for pattern in patterns:\n                if re.search(pattern, text):\n                    intents.append(intent)\n                    break  # No need to check other patterns for this intent.\n        return list(set(intents))\n\n\ndef get_data(file_name):\n    df = pd.read_csv(file_name)\n    return df\n\n\ndef preprocess_tweet(text: str) -> str:\n    \"\"\"\n    Preprocess the given text before inference.\n\n    Right now only converts diacritics to ascii versions (turkish letters).\n    Args:\n        text: The text to normalize.\n\n    Returns:\n        Normalized text.\n    \"\"\"\n    return unidecode(text)\n\n\ndef process_tweet(classifier, tweet: Tuple, plot_data: Dict) -> Tuple[Optional[Set[str]], Dict]:\n    \"\"\"\n    Process the given tweet.\n    Check if the tweet contains any of the keywords using rules.\n    If it does, update the plot data and assign labels to the tweet\n\n    Args:\n        tweet: The tweet to process. tweet[1] -> full_text\n        plot_data: The plot data to update.\n\n    Returns:\n        The labels of the tweet. If the tweet does not contain any of the keywords, return None.\n    \"\"\"\n\n    # normalize text to english characters\n    tweet_normalized = preprocess_tweet(tweet[1])  # tweet[1] -> full_text\n\n    # check if tweet contains any of the keywords\n    labels = classifier.classify(tweet_normalized)\n    if not labels:\n        return None, plot_data\n\n    plot_data = update_plot_data(plot_data, labels)\n    return labels, plot_data\n\n\ndef process_tweet_stream(df):\n    classifier = RuleBasedClassifier()\n    plot_data = {}\n    db_ready_data_list = []\n    for _, row in df.iterrows():\n        db_ready_data_list.append(classifier, process_tweet(row, plot_data))\n    return db_ready_data_list, plot_data\n\n\ndef update_plot_data(plot_data: Dict, labels: Union[Set[str], List[str]]) -> Dict:\n    \"\"\"\n    Increment the count of the given labels in the plot data.\n    Args:\n        plot_data: The dictionary that holds INTENT - COUNT pairs.\n        labels: The labels to increment the count of.\n\n    Returns:\n        The updated plot data.\n    \"\"\"\n    for label in labels:\n        if label in plot_data:\n            plot_data[label] += 1\n        else:\n            plot_data[label] = 1\n    return plot_data\n\n\ndef draw_plot(plot_data: Dict):\n    \"\"\" Draw the plot with the given plot data.\n       It draws label count of the tweets.\n\n    Args:\n        plot_data: The plot data to draw the plot with.\n\n    Returns:\n        None\n    \"\"\"\n    plt.bar(plot_data.keys(), plot_data.values())\n    plt.xlabel(\"Cluster Label\")\n    plt.ylabel(\"Tweet Count\")\n    plt.title(\"Tweet Count per Cluster Label\")\n    plt.show()\n\n\nif __name__ == '__main__':\n    data = get_data('sample_data.csv')\n    processed_data, plot_data = process_tweet_stream(data)\n    draw_plot(plot_data)\n"}
{"type": "source_file", "path": "src/ml_modules/run_zsc.py", "content": "import json\nimport os\nimport time\nfrom typing import Dict, Optional\n\nimport requests\nfrom dotenv import load_dotenv\n\nload_dotenv(\".env\")\n\nAPI_TOKEN = os.getenv(\"HF_HUB_TOKEN\")\n\nheaders = {\"Authorization\": f\"Bearer {API_TOKEN}\"}\n\nAPI_URL = \"https://api-inference.huggingface.co/models/emrecan/convbert-base-turkish-mc4-cased-allnli_tr\"  # noqa\n\n\ndef query(payload: Dict) -> Optional[Dict]:\n    data = json.dumps(payload)\n    response = requests.request(\"POST\", API_URL, headers=headers, data=data)\n\n    # Check status code\n    if response.status_code != 200:\n        print(\n            f\"Query: {payload} failed to run by returning code of {response.status_code}. Response: {response.text} \") # noqa\n        print(\"Trying again in 10 seconds...\")\n        # Wait 10 seconds and try again\n        time.sleep(10)\n        response = requests.request(\"POST\", API_URL, headers=headers, data=data)\n        if response.status_code != 200:\n            return None\n\n    return response.json()\n\n\ndef batch_query(data, candidate_labels):\n    \"\"\"\n    List ya da text'leri iceren herhangi bir iterable alir.\n\n    Parameter\n    ---------\n    data : Iterable\n        Text'leri iceren iterable.\n    candidate_labels : List\n        Siniflandirilmak istenen topic'ler.\n\n    Returns\n    -------\n    outputs\n        JSON output listesi: JSON'un key'ler sequence (asil input), labels (tahmin edilen siniflar)\n        ve scores (siniflarin kac olasilikla tahmin edildigi)\n    # TODO: olasiliklara gore fallback mekanizmasi yazilacak.\n    \"\"\"\n    outputs = []\n    if not candidate_labels:\n        candidate_labels = [\"battaniye\", \"yemek\", \"göçük\"]\n    for tweet in data:\n        outputs.append(query(\n            {\n                \"inputs\": tweet,\n                \"parameters\": {\"candidate_labels\": candidate_labels},\n            }))\n    return outputs\n"}
