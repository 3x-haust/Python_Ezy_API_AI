{"repo_info": {"repo_name": "Ollama-Workbench", "repo_owner": "marc-shade", "repo_url": "https://github.com/marc-shade/Ollama-Workbench"}}
{"type": "test_file", "path": "backup_files/test_torch_mps.py", "content": "#!/usr/bin/env python3\n\"\"\"\nTest script to verify PyTorch with MPS support on Apple Silicon Macs.\n\nThis script performs a simple tensor operation using the MPS device\nto verify that PyTorch is correctly configured to use Metal Performance Shaders\nfor hardware acceleration on Apple Silicon.\n\"\"\"\n\nimport os\nimport sys\nimport time\nimport platform\n\ndef print_header(text):\n    \"\"\"Print a formatted header.\"\"\"\n    print(f\"\\n{'=' * 60}\")\n    print(f\"  {text}\")\n    print(f\"{'=' * 60}\\n\")\n\ndef print_section(text):\n    \"\"\"Print a section header.\"\"\"\n    print(f\"\\n--- {text} ---\")\n\ndef main():\n    print_header(\"PyTorch MPS Test for Apple Silicon\")\n    \n    # Check if running on Apple Silicon\n    if platform.system() != \"Darwin\" or platform.machine() != \"arm64\":\n        print(\"❌ Not running on Apple Silicon Mac.\")\n        print(f\"   System: {platform.system()}\")\n        print(f\"   Machine: {platform.machine()}\")\n        return False\n    \n    print(\"✅ Running on Apple Silicon Mac.\")\n    \n    # Try importing torch\n    print_section(\"Checking PyTorch installation\")\n    try:\n        import torch\n        print(f\"✅ PyTorch version: {torch.__version__}\")\n    except ImportError:\n        print(\"❌ Failed to import torch. Please install PyTorch first.\")\n        return False\n    \n    # Check for MPS availability\n    print_section(\"Checking MPS availability\")\n    \n    # Different versions of PyTorch have different ways to check MPS\n    mps_available = False\n    \n    # Try the standard way (PyTorch 2.0+)\n    if hasattr(torch, 'backends') and hasattr(torch.backends, 'mps'):\n        try:\n            if torch.backends.mps.is_available():\n                mps_available = True\n        except AttributeError:\n            pass\n    \n    # Try the older way or direct check\n    if not mps_available and hasattr(torch, 'mps'):\n        try:\n            # Some versions use torch.mps.is_available()\n            if hasattr(torch.mps, 'is_available') and torch.mps.is_available():\n                mps_available = True\n            # Some versions just check if the module exists\n            else:\n                mps_available = True\n        except (AttributeError, ImportError):\n            pass\n            \n    # Try creating a tensor on MPS as final check\n    if not mps_available:\n        try:\n            device = torch.device(\"mps\")\n            x = torch.ones(1, device=device)\n            mps_available = True\n        except (RuntimeError, ValueError):\n            pass\n    \n    if not mps_available:\n        print(\"❌ MPS is not available on this system.\")\n        return False\n    \n    print(\"✅ MPS is available!\")\n    \n    # Run a simple test with CPU vs. MPS\n    print_section(\"Running performance comparison test\")\n    \n    # Create test tensors\n    size = 2000\n    print(f\"Creating {size}x{size} matrices for multiplication test...\")\n    \n    # Test on CPU\n    start_time = time.time()\n    a_cpu = torch.randn(size, size)\n    b_cpu = torch.randn(size, size)\n    print(\"Running matrix multiplication on CPU...\", end=\"\", flush=True)\n    c_cpu = torch.matmul(a_cpu, b_cpu)\n    cpu_time = time.time() - start_time\n    print(f\" Done in {cpu_time:.4f} seconds\")\n    \n    # Test on MPS if available\n    try:\n        start_time = time.time()\n        mps_device = torch.device(\"mps\")\n        a_mps = torch.randn(size, size, device=mps_device)\n        b_mps = torch.randn(size, size, device=mps_device)\n        print(\"Running matrix multiplication on MPS...\", end=\"\", flush=True)\n        c_mps = torch.matmul(a_mps, b_mps)\n        # Force synchronization to get accurate timing\n        torch.mps.synchronize()\n        mps_time = time.time() - start_time\n        print(f\" Done in {mps_time:.4f} seconds\")\n        \n        # Compare results for correctness (allow for some floating point differences)\n        print_section(\"Verifying results\")\n        c_mps_cpu = c_mps.to(\"cpu\")\n        max_diff = torch.max(torch.abs(c_cpu - c_mps_cpu)).item()\n        print(f\"Maximum difference between CPU and MPS results: {max_diff:.6f}\")\n        \n        if max_diff < 1e-3:\n            print(\"✅ Results match within tolerance!\")\n        else:\n            print(\"⚠️ Results have significant differences.\")\n        \n        # Show speedup\n        print_section(\"Performance Results\")\n        if mps_time < cpu_time:\n            speedup = cpu_time / mps_time\n            print(f\"✅ MPS is {speedup:.2f}x faster than CPU!\")\n        else:\n            slowdown = mps_time / cpu_time\n            print(f\"⚠️ MPS is {slowdown:.2f}x slower than CPU. This is unusual for matrix operations.\")\n    \n    except Exception as e:\n        print(f\"\\n❌ Error running MPS test: {str(e)}\")\n        return False\n    \n    print_header(\"Test Summary\")\n    print(\"✅ PyTorch is correctly installed with MPS support\")\n    print(\"✅ MPS acceleration is working on this Apple Silicon Mac\")\n    print(\"\\nYour setup is optimized for machine learning on Apple Silicon!\")\n    \n    return True\n\nif __name__ == \"__main__\":\n    success = main()\n    sys.exit(0 if success else 1)"}
{"type": "test_file", "path": "test_openai.py", "content": "#!/usr/bin/env python3\n\"\"\"\nTest script to verify openai import\n\"\"\"\n\nimport sys\nimport subprocess\nimport importlib.util\n\nprint(\"Python version:\", sys.version)\nprint(\"\\n=== Testing openai import ===\")\n\n# Try to import openai\ntry:\n    import openai\n    print(\"✅ Successfully imported openai\")\n    print(\"openai version:\", getattr(openai, \"__version__\", \"unknown\"))\n    print(\"openai path:\", openai.__file__)\nexcept ImportError as e:\n    print(\"❌ Failed to import openai:\", e)\n\n# Check what's installed with pip\nprint(\"\\n=== Checking pip installation ===\")\ntry:\n    result = subprocess.run(\n        [sys.executable, \"-m\", \"pip\", \"show\", \"openai\"],\n        capture_output=True,\n        text=True,\n        check=False\n    )\n    if result.returncode == 0:\n        print(\"✅ openai is installed via pip:\")\n        for line in result.stdout.splitlines():\n            if line.startswith(\"Name:\") or line.startswith(\"Version:\") or line.startswith(\"Location:\"):\n                print(\"  \", line)\n    else:\n        print(\"❌ openai is NOT installed via pip\")\nexcept Exception as e:\n    print(\"Error checking openai installation:\", e)\n\nprint(\"\\n=== Test complete ===\")"}
{"type": "test_file", "path": "test_torch_mps.py", "content": "#!/usr/bin/env python3\n\"\"\"\nTest script to verify PyTorch with MPS support on Apple Silicon Macs.\n\nThis script performs a simple tensor operation using the MPS device\nto verify that PyTorch is correctly configured to use Metal Performance Shaders\nfor hardware acceleration on Apple Silicon.\n\"\"\"\n\nimport os\nimport sys\nimport time\nimport platform\n\ndef print_header(text):\n    \"\"\"Print a formatted header.\"\"\"\n    print(f\"\\n{'=' * 60}\")\n    print(f\"  {text}\")\n    print(f\"{'=' * 60}\\n\")\n\ndef print_section(text):\n    \"\"\"Print a section header.\"\"\"\n    print(f\"\\n--- {text} ---\")\n\ndef main():\n    print_header(\"PyTorch MPS Test for Apple Silicon\")\n    \n    # Check if running on Apple Silicon\n    if platform.system() != \"Darwin\" or platform.machine() != \"arm64\":\n        print(\"❌ Not running on Apple Silicon Mac.\")\n        print(f\"   System: {platform.system()}\")\n        print(f\"   Machine: {platform.machine()}\")\n        return False\n    \n    print(\"✅ Running on Apple Silicon Mac.\")\n    \n    # Try importing torch\n    print_section(\"Checking PyTorch installation\")\n    try:\n        import torch\n        print(f\"✅ PyTorch version: {torch.__version__}\")\n    except ImportError:\n        print(\"❌ Failed to import torch. Please install PyTorch first.\")\n        return False\n    \n    # Check for MPS availability\n    print_section(\"Checking MPS availability\")\n    \n    # Different versions of PyTorch have different ways to check MPS\n    mps_available = False\n    \n    # Try the standard way (PyTorch 2.0+)\n    if hasattr(torch, 'backends') and hasattr(torch.backends, 'mps'):\n        try:\n            if torch.backends.mps.is_available():\n                mps_available = True\n        except AttributeError:\n            pass\n    \n    # Try the older way or direct check\n    if not mps_available and hasattr(torch, 'mps'):\n        try:\n            # Some versions use torch.mps.is_available()\n            if hasattr(torch.mps, 'is_available') and torch.mps.is_available():\n                mps_available = True\n            # Some versions just check if the module exists\n            else:\n                mps_available = True\n        except (AttributeError, ImportError):\n            pass\n            \n    # Try creating a tensor on MPS as final check\n    if not mps_available:\n        try:\n            device = torch.device(\"mps\")\n            x = torch.ones(1, device=device)\n            mps_available = True\n        except (RuntimeError, ValueError):\n            pass\n    \n    if not mps_available:\n        print(\"❌ MPS is not available on this system.\")\n        return False\n    \n    print(\"✅ MPS is available!\")\n    \n    # Run a simple test with CPU vs. MPS\n    print_section(\"Running performance comparison test\")\n    \n    # Create test tensors\n    size = 2000\n    print(f\"Creating {size}x{size} matrices for multiplication test...\")\n    \n    # Test on CPU\n    start_time = time.time()\n    a_cpu = torch.randn(size, size)\n    b_cpu = torch.randn(size, size)\n    print(\"Running matrix multiplication on CPU...\", end=\"\", flush=True)\n    c_cpu = torch.matmul(a_cpu, b_cpu)\n    cpu_time = time.time() - start_time\n    print(f\" Done in {cpu_time:.4f} seconds\")\n    \n    # Test on MPS if available\n    try:\n        start_time = time.time()\n        mps_device = torch.device(\"mps\")\n        a_mps = torch.randn(size, size, device=mps_device)\n        b_mps = torch.randn(size, size, device=mps_device)\n        print(\"Running matrix multiplication on MPS...\", end=\"\", flush=True)\n        c_mps = torch.matmul(a_mps, b_mps)\n        # Force synchronization to get accurate timing\n        torch.mps.synchronize()\n        mps_time = time.time() - start_time\n        print(f\" Done in {mps_time:.4f} seconds\")\n        \n        # Compare results for correctness (allow for some floating point differences)\n        print_section(\"Verifying results\")\n        c_mps_cpu = c_mps.to(\"cpu\")\n        max_diff = torch.max(torch.abs(c_cpu - c_mps_cpu)).item()\n        print(f\"Maximum difference between CPU and MPS results: {max_diff:.6f}\")\n        \n        if max_diff < 1e-3:\n            print(\"✅ Results match within tolerance!\")\n        else:\n            print(\"⚠️ Results have significant differences.\")\n        \n        # Show speedup\n        print_section(\"Performance Results\")\n        if mps_time < cpu_time:\n            speedup = cpu_time / mps_time\n            print(f\"✅ MPS is {speedup:.2f}x faster than CPU!\")\n        else:\n            slowdown = mps_time / cpu_time\n            print(f\"⚠️ MPS is {slowdown:.2f}x slower than CPU. This is unusual for matrix operations.\")\n    \n    except Exception as e:\n        print(f\"\\n❌ Error running MPS test: {str(e)}\")\n        return False\n    \n    print_header(\"Test Summary\")\n    print(\"✅ PyTorch is correctly installed with MPS support\")\n    print(\"✅ MPS acceleration is working on this Apple Silicon Mac\")\n    print(\"\\nYour setup is optimized for machine learning on Apple Silicon!\")\n    \n    return True\n\nif __name__ == \"__main__\":\n    success = main()\n    sys.exit(0 if success else 1)"}
{"type": "test_file", "path": "test_enhanced_corpus.py", "content": "#!/usr/bin/env python3\n\"\"\"\nTest script to verify PyPDF2 import in enhanced_corpus.py\n\"\"\"\n\nimport sys\nimport importlib.util\nimport os\n\nprint(\"Python version:\", sys.version)\nprint(\"\\n=== Testing enhanced_corpus.py imports ===\")\n\n# Try to import PyPDF2 directly\ntry:\n    import PyPDF2\n    print(\"✅ Successfully imported PyPDF2 (uppercase)\")\n    print(\"PyPDF2 version:\", getattr(PyPDF2, \"__version__\", \"unknown\"))\n    print(\"PyPDF2 path:\", PyPDF2.__file__)\nexcept ImportError as e:\n    print(\"❌ Failed to import PyPDF2 (uppercase):\", e)\n\n# Try to import enhanced_corpus\ntry:\n    import enhanced_corpus\n    print(\"✅ Successfully imported enhanced_corpus\")\n    \n    # Test DocumentProcessor class\n    try:\n        processor = enhanced_corpus.DocumentProcessor()\n        print(\"✅ Successfully created DocumentProcessor instance\")\n    except Exception as e:\n        print(\"❌ Failed to create DocumentProcessor instance:\", e)\n    \n    # Test OllamaEmbedder class\n    try:\n        embedder = enhanced_corpus.OllamaEmbedder()\n        print(\"✅ Successfully created OllamaEmbedder instance\")\n    except Exception as e:\n        print(\"❌ Failed to create OllamaEmbedder instance:\", e)\n    \n    # Test GraphRAGCorpus class\n    try:\n        corpus = enhanced_corpus.GraphRAGCorpus(\"test_corpus\", embedder)\n        print(\"✅ Successfully created GraphRAGCorpus instance\")\n    except Exception as e:\n        print(\"❌ Failed to create GraphRAGCorpus instance:\", e)\n        \nexcept ImportError as e:\n    print(\"❌ Failed to import enhanced_corpus:\", e)\n    \n    # Try to trace the import error\n    print(\"\\n=== Tracing import error ===\")\n    try:\n        spec = importlib.util.find_spec(\"enhanced_corpus\")\n        if spec is None:\n            print(\"❌ enhanced_corpus.py not found in Python path\")\n        else:\n            print(\"✅ enhanced_corpus.py found at:\", spec.origin)\n            \n            # Try to load the module manually\n            try:\n                module = importlib.util.module_from_spec(spec)\n                spec.loader.exec_module(module)\n                print(\"✅ Successfully loaded enhanced_corpus module manually\")\n            except Exception as e:\n                print(\"❌ Failed to load enhanced_corpus module manually:\", e)\n                \n                # Try to trace the specific import that's failing\n                with open(spec.origin, 'r') as f:\n                    lines = f.readlines()\n                \n                print(\"\\nImport statements in enhanced_corpus.py:\")\n                for i, line in enumerate(lines):\n                    if line.strip().startswith('import ') or line.strip().startswith('from '):\n                        print(f\"{i+1}: {line.strip()}\")\n                        \n                        # Try to import each module\n                        module_name = line.strip().split()[1].split('.')[0]\n                        if module_name != 'enhanced_corpus':\n                            try:\n                                importlib.import_module(module_name)\n                                print(f\"  ✅ Successfully imported {module_name}\")\n                            except ImportError as e:\n                                print(f\"  ❌ Failed to import {module_name}: {e}\")\n    except Exception as e:\n        print(\"❌ Error while tracing import:\", e)\n\nprint(\"\\n=== Test complete ===\")"}
{"type": "test_file", "path": "test_all_deps.py", "content": "#!/usr/bin/env python3\n\"\"\"\nComprehensive test script to verify all critical dependencies\n\"\"\"\n\nimport sys\nimport subprocess\nimport importlib.util\n\nprint(\"Python version:\", sys.version)\nprint(\"\\n=== Testing Critical Dependencies ===\")\n\n# List of critical dependencies to check\ncritical_deps = [\n    \"openai\",\n    \"PyPDF2\",\n    \"streamlit_extras\",\n    \"streamlit_javascript\",\n    \"streamlit_option_menu\",\n    \"ollama\",\n    \"langchain\",\n    \"langchain_community\",\n    \"flask\",\n    \"psutil\",\n    \"groq\",\n    \"mistralai\",\n    \"matplotlib\",\n    \"tiktoken\",\n    \"sklearn\",\n    \"numpy\",\n    \"pandas\",\n    \"chromadb\",\n    \"pdfkit\",\n    \"requests\",\n    \"bs4\",\n    \"selenium\",\n    \"webdriver_manager\"\n]\n\n# Try to import each dependency\nfor dep in critical_deps:\n    try:\n        module = importlib.import_module(dep)\n        print(f\"✅ Successfully imported {dep}\")\n        version = getattr(module, \"__version__\", \"unknown\")\n        path = getattr(module, \"__file__\", \"unknown\")\n        print(f\"   Version: {version}\")\n        print(f\"   Path: {path}\")\n    except ImportError as e:\n        print(f\"❌ Failed to import {dep}: {e}\")\n\n# Check what's installed with pip\nprint(\"\\n=== Checking pip installation ===\")\nfor package in [\n    \"openai\",\n    \"PyPDF2\",\n    \"streamlit-extras\",\n    \"streamlit-javascript\",\n    \"streamlit-option-menu\",\n    \"ollama\",\n    \"langchain\",\n    \"langchain-community\",\n    \"flask\",\n    \"psutil\",\n    \"groq\",\n    \"mistralai\",\n    \"matplotlib\",\n    \"tiktoken\",\n    \"scikit-learn\",\n    \"numpy\",\n    \"pandas\",\n    \"chromadb\",\n    \"pdfkit\",\n    \"requests\",\n    \"beautifulsoup4\",\n    \"bs4\",\n    \"selenium\",\n    \"webdriver-manager\"\n]:\n    try:\n        result = subprocess.run(\n            [sys.executable, \"-m\", \"pip\", \"show\", package],\n            capture_output=True,\n            text=True,\n            check=False\n        )\n        if result.returncode == 0:\n            print(f\"✅ {package} is installed via pip:\")\n            for line in result.stdout.splitlines():\n                if line.startswith(\"Name:\") or line.startswith(\"Version:\"):\n                    print(\"  \", line)\n        else:\n            print(f\"❌ {package} is NOT installed via pip\")\n    except Exception as e:\n        print(f\"Error checking {package} installation:\", e)\n\n# Check if spaCy language model is installed\nprint(\"\\n=== Checking spaCy language model ===\")\ntry:\n    import spacy\n    try:\n        nlp = spacy.load(\"en_core_web_sm\")\n        print(f\"✅ Successfully loaded spaCy language model 'en_core_web_sm'\")\n        print(f\"   Version: {getattr(nlp.meta, 'version', 'unknown')}\")\n    except Exception as e:\n        print(f\"❌ Failed to load spaCy language model 'en_core_web_sm': {e}\")\n        print(\"   Try installing it with: python -m spacy download en_core_web_sm\")\nexcept ImportError:\n    print(\"❌ spaCy not installed, skipping language model check\")\n\nprint(\"\\n=== Test complete ===\")"}
{"type": "test_file", "path": "test_pdfkit.py", "content": "#!/usr/bin/env python3\n\"\"\"\nTest script to verify pdfkit import\n\"\"\"\n\nimport sys\nimport subprocess\nimport importlib.util\n\nprint(\"Python version:\", sys.version)\nprint(\"\\n=== Testing pdfkit import ===\")\n\n# Try to import pdfkit\ntry:\n    import pdfkit\n    print(\"✅ Successfully imported pdfkit\")\n    print(\"pdfkit version:\", getattr(pdfkit, \"__version__\", \"unknown\"))\n    print(\"pdfkit path:\", pdfkit.__file__)\n    \n    # Check if wkhtmltopdf is installed (required by pdfkit)\n    try:\n        path = pdfkit.configuration().wkhtmltopdf\n        if path:\n            print(f\"✅ wkhtmltopdf found at: {path}\")\n            # Try to get version\n            try:\n                version_output = subprocess.check_output([path, '--version'], stderr=subprocess.STDOUT).decode('utf-8').strip()\n                print(f\"wkhtmltopdf version: {version_output}\")\n            except subprocess.CalledProcessError as e:\n                print(f\"❌ Error getting wkhtmltopdf version: {e}\")\n        else:\n            print(\"❌ wkhtmltopdf path not found in configuration\")\n    except Exception as e:\n        print(f\"❌ Error checking wkhtmltopdf: {e}\")\n        \nexcept ImportError as e:\n    print(\"❌ Failed to import pdfkit:\", e)\n\n# Check what's installed with pip\nprint(\"\\n=== Checking pip installation ===\")\ntry:\n    result = subprocess.run(\n        [sys.executable, \"-m\", \"pip\", \"show\", \"pdfkit\"],\n        capture_output=True,\n        text=True,\n        check=False\n    )\n    if result.returncode == 0:\n        print(\"✅ pdfkit is installed via pip:\")\n        for line in result.stdout.splitlines():\n            if line.startswith(\"Name:\") or line.startswith(\"Version:\") or line.startswith(\"Location:\"):\n                print(\"  \", line)\n    else:\n        print(\"❌ pdfkit is NOT installed via pip\")\nexcept Exception as e:\n    print(\"Error checking pdfkit installation:\", e)\n\nprint(\"\\n=== Test complete ===\")"}
{"type": "test_file", "path": "test_import.py", "content": "import sys\nimport subprocess\nimport importlib.util\n\nprint(\"Python version:\", sys.version)\nprint(\"\\n=== Testing PyPDF2 import ===\")\n\n# Try to import PyPDF2 (uppercase)\ntry:\n    import PyPDF2\n    print(\"✅ Successfully imported PyPDF2 (uppercase)\")\n    print(\"PyPDF2 version:\", getattr(PyPDF2, \"__version__\", \"unknown\"))\n    print(\"PyPDF2 path:\", PyPDF2.__file__)\nexcept ImportError as e:\n    print(\"❌ Failed to import PyPDF2 (uppercase):\", e)\n\n# Try to import pypdf2 (lowercase)\ntry:\n    import pypdf2\n    print(\"✅ Successfully imported pypdf2 (lowercase)\")\n    print(\"pypdf2 version:\", getattr(pypdf2, \"__version__\", \"unknown\"))\n    print(\"pypdf2 path:\", pypdf2.__file__)\nexcept ImportError as e:\n    print(\"❌ Failed to import pypdf2 (lowercase):\", e)\n\n# Check what's installed with pip\nprint(\"\\n=== Checking pip installation ===\")\ntry:\n    result = subprocess.run(\n        [sys.executable, \"-m\", \"pip\", \"show\", \"PyPDF2\"],\n        capture_output=True,\n        text=True,\n        check=False\n    )\n    if result.returncode == 0:\n        print(\"✅ PyPDF2 (uppercase) is installed via pip:\")\n        for line in result.stdout.splitlines():\n            if line.startswith(\"Name:\") or line.startswith(\"Version:\") or line.startswith(\"Location:\"):\n                print(\"  \", line)\n    else:\n        print(\"❌ PyPDF2 (uppercase) is NOT installed via pip\")\nexcept Exception as e:\n    print(\"Error checking PyPDF2 installation:\", e)\n\ntry:\n    result = subprocess.run(\n        [sys.executable, \"-m\", \"pip\", \"show\", \"pypdf2\"],\n        capture_output=True,\n        text=True,\n        check=False\n    )\n    if result.returncode == 0:\n        print(\"✅ pypdf2 (lowercase) is installed via pip:\")\n        for line in result.stdout.splitlines():\n            if line.startswith(\"Name:\") or line.startswith(\"Version:\") or line.startswith(\"Location:\"):\n                print(\"  \", line)\n    else:\n        print(\"❌ pypdf2 (lowercase) is NOT installed via pip\")\nexcept Exception as e:\n    print(\"Error checking pypdf2 installation:\", e)"}
{"type": "test_file", "path": "test_autogen_compat.py", "content": "#!/usr/bin/env python3\n\"\"\"\nTest script to verify the autogen compatibility layer works correctly.\n\"\"\"\n\nimport sys\nimport os\n\ndef test_autogen_import():\n    \"\"\"Test importing autogen modules through the compatibility layer.\"\"\"\n    try:\n        print(\"Attempting to import autogen...\")\n        import autogen\n        print(f\"✓ Successfully imported autogen (actual package: {autogen.__name__})\")\n        \n        # Try to import some common submodules\n        print(\"\\nTesting submodule imports:\")\n        \n        try:\n            from autogen import ConversableAgent\n            print(f\"✓ Successfully imported ConversableAgent\")\n        except ImportError as e:\n            print(f\"✗ Failed to import ConversableAgent: {e}\")\n        \n        try:\n            from autogen import UserProxyAgent\n            print(f\"✓ Successfully imported UserProxyAgent\")\n        except ImportError as e:\n            print(f\"✗ Failed to import UserProxyAgent: {e}\")\n        \n        try:\n            from autogen.oai import openai_utils\n            print(f\"✓ Successfully imported autogen.oai.openai_utils\")\n        except ImportError as e:\n            print(f\"✗ Failed to import autogen.oai.openai_utils: {e}\")\n        \n        # Print version information\n        if hasattr(autogen, '__version__'):\n            print(f\"\\nAutogen version: {autogen.__version__}\")\n        else:\n            print(\"\\nAutogen version information not available\")\n        \n        return True\n    except ImportError as e:\n        print(f\"✗ Failed to import autogen: {e}\")\n        return False\n\ndef check_environment():\n    \"\"\"Check if the environment is set up correctly.\"\"\"\n    print(\"Environment Information:\")\n    print(f\"Python version: {sys.version}\")\n    print(f\"PYTHONPATH: {os.environ.get('PYTHONPATH', 'Not set')}\")\n    \n    # Check if pyautogen is installed\n    try:\n        import pyautogen\n        print(f\"✓ pyautogen is installed (version: {pyautogen.__version__ if hasattr(pyautogen, '__version__') else 'unknown'})\")\n    except ImportError:\n        print(\"✗ pyautogen is not installed\")\n    \n    # Check if the compatibility layer directory is in the path\n    current_dir = os.path.dirname(os.path.abspath(__file__))\n    if current_dir in sys.path or '' in sys.path:\n        print(\"✓ Current directory is in sys.path\")\n    else:\n        print(\"✗ Current directory is not in sys.path\")\n    \n    # Check if the compatibility layer exists\n    compat_path = os.path.join(current_dir, 'autogen_compat', '__init__.py')\n    if os.path.exists(compat_path):\n        print(f\"✓ Compatibility layer found at {compat_path}\")\n    else:\n        print(f\"✗ Compatibility layer not found at {compat_path}\")\n\nif __name__ == \"__main__\":\n    print(\"=\" * 60)\n    print(\"AUTOGEN COMPATIBILITY LAYER TEST\")\n    print(\"=\" * 60)\n    \n    check_environment()\n    \n    print(\"\\n\" + \"=\" * 60)\n    print(\"TESTING AUTOGEN IMPORT\")\n    print(\"=\" * 60)\n    \n    success = test_autogen_import()\n    \n    print(\"\\n\" + \"=\" * 60)\n    print(\"TEST RESULT: \" + (\"SUCCESS\" if success else \"FAILURE\"))\n    print(\"=\" * 60)\n    \n    if not success:\n        print(\"\\nTo fix this issue:\")\n        print(\"1. Make sure pyautogen is installed: pip install pyautogen>=0.2.0\")\n        print(\"2. Run the script with the compatibility layer: ./use_autogen_compat.sh python test_autogen_compat.py\")\n    \n    sys.exit(0 if success else 1)"}
{"type": "test_file", "path": "test_app_imports.py", "content": "#!/usr/bin/env python3\n\"\"\"\nTest script to verify all imports in the application\n\"\"\"\n\nimport sys\nimport importlib.util\nimport os\n\nprint(\"Python version:\", sys.version)\nprint(\"\\n=== Testing Application Imports ===\")\n\n# List of files to check\nfiles_to_check = [\n    \"main.py\",\n    \"ui_elements.py\",\n    \"chat_interface.py\",\n    \"brainstorm.py\",\n    \"enhanced_corpus.py\"\n]\n\n# Function to extract imports from a file\ndef extract_imports(file_path):\n    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n        content = f.read()\n    \n    # Simple regex to match import statements\n    import_lines = []\n    for line in content.split('\\n'):\n        line = line.strip()\n        if line.startswith('import ') or line.startswith('from '):\n            # Skip comments\n            if '#' in line:\n                line = line[:line.index('#')].strip()\n            import_lines.append(line)\n    \n    return import_lines\n\n# Function to extract module names from import statements\ndef extract_modules(import_lines):\n    modules = set()\n    for line in import_lines:\n        if line.startswith('import '):\n            # Handle multiple imports on one line (import x, y, z)\n            parts = line[7:].split(',')\n            for part in parts:\n                # Handle \"import x as y\"\n                module = part.strip().split(' as ')[0].strip()\n                modules.add(module)\n        elif line.startswith('from '):\n            # Handle \"from x import y\"\n            module = line[5:].split(' import ')[0].strip()\n            modules.add(module)\n    \n    return modules\n\n# Process each file\nall_modules = set()\nfor file_path in files_to_check:\n    if os.path.exists(file_path):\n        print(f\"\\nExtracting imports from {file_path}:\")\n        import_lines = extract_imports(file_path)\n        modules = extract_modules(import_lines)\n        \n        print(f\"Found {len(modules)} modules in {file_path}:\")\n        for module in sorted(modules):\n            print(f\"  - {module}\")\n        \n        all_modules.update(modules)\n    else:\n        print(f\"File not found: {file_path}\")\n\n# Try to import each module\nprint(\"\\n=== Testing imports for all modules ===\")\nsuccessful_imports = 0\nfailed_imports = 0\n\nfor module in sorted(all_modules):\n    # Skip relative imports and built-in modules\n    if module.startswith('.') or module in sys.builtin_module_names:\n        continue\n    \n    # Get the top-level module name\n    top_module = module.split('.')[0]\n    \n    try:\n        importlib.import_module(top_module)\n        print(f\"✅ Successfully imported {top_module}\")\n        successful_imports += 1\n    except ImportError as e:\n        print(f\"❌ Failed to import {top_module}: {e}\")\n        failed_imports += 1\n\nprint(f\"\\nImport test complete: {successful_imports} successful, {failed_imports} failed\")"}
{"type": "test_file", "path": "test_humanize.py", "content": "#!/usr/bin/env python3\n\"\"\"\nTest script to check if the humanize package is installed and can be imported.\n\"\"\"\n\nimport sys\nimport subprocess\n\nprint(\"Python version:\", sys.version)\nprint(\"\\n=== Testing humanize package ===\")\n\n# Try to import humanize\ntry:\n    import humanize\n    print(f\"✅ Successfully imported humanize\")\n    print(f\"   Version: {getattr(humanize, '__version__', 'unknown')}\")\n    print(f\"   Path: {getattr(humanize, '__file__', 'unknown')}\")\n    \n    # Test some functions\n    print(\"\\n=== Testing humanize functions ===\")\n    print(f\"naturalsize(1024): {humanize.naturalsize(1024)}\")\n    print(f\"naturaldelta(3600): {humanize.naturaldelta(3600)}\")\nexcept ImportError as e:\n    print(f\"❌ Failed to import humanize: {e}\")\n\n# Check what's installed with pip\nprint(\"\\n=== Checking pip installation ===\")\ntry:\n    result = subprocess.run(\n        [sys.executable, \"-m\", \"pip\", \"show\", \"humanize\"],\n        capture_output=True,\n        text=True,\n        check=False\n    )\n    if result.returncode == 0:\n        print(f\"✅ humanize is installed via pip:\")\n        for line in result.stdout.splitlines():\n            if line.startswith(\"Name:\") or line.startswith(\"Version:\") or line.startswith(\"Location:\"):\n                print(\"  \", line)\n    else:\n        print(f\"❌ humanize is NOT installed via pip\")\n        \n        # Try to install it\n        print(\"\\n=== Attempting to install humanize ===\")\n        install_result = subprocess.run(\n            [sys.executable, \"-m\", \"pip\", \"install\", \"humanize\"],\n            capture_output=True,\n            text=True,\n            check=False\n        )\n        if install_result.returncode == 0:\n            print(f\"✅ Successfully installed humanize\")\n            print(install_result.stdout)\n        else:\n            print(f\"❌ Failed to install humanize\")\n            print(install_result.stderr)\nexcept Exception as e:\n    print(f\"Error checking humanize installation:\", e)\n\nprint(\"\\n=== Test complete ===\")"}
{"type": "test_file", "path": "test_streamlit_deps.py", "content": "#!/usr/bin/env python3\n\"\"\"\nTest script to verify streamlit dependencies\n\"\"\"\n\nimport sys\nimport subprocess\nimport importlib.util\n\nprint(\"Python version:\", sys.version)\nprint(\"\\n=== Testing streamlit dependencies ===\")\n\n# Try to import streamlit_extras\ntry:\n    import streamlit_extras\n    print(\"✅ Successfully imported streamlit_extras\")\n    print(\"streamlit_extras version:\", getattr(streamlit_extras, \"__version__\", \"unknown\"))\n    print(\"streamlit_extras path:\", streamlit_extras.__file__)\nexcept ImportError as e:\n    print(\"❌ Failed to import streamlit_extras:\", e)\n\n# Try to import streamlit_javascript\ntry:\n    import streamlit_javascript\n    print(\"✅ Successfully imported streamlit_javascript\")\n    print(\"streamlit_javascript version:\", getattr(streamlit_javascript, \"__version__\", \"unknown\"))\n    print(\"streamlit_javascript path:\", streamlit_javascript.__file__)\nexcept ImportError as e:\n    print(\"❌ Failed to import streamlit_javascript:\", e)\n\n# Check what's installed with pip\nprint(\"\\n=== Checking pip installation ===\")\nfor package in [\"streamlit-extras\", \"streamlit-javascript\"]:\n    try:\n        result = subprocess.run(\n            [sys.executable, \"-m\", \"pip\", \"show\", package],\n            capture_output=True,\n            text=True,\n            check=False\n        )\n        if result.returncode == 0:\n            print(f\"✅ {package} is installed via pip:\")\n            for line in result.stdout.splitlines():\n                if line.startswith(\"Name:\") or line.startswith(\"Version:\") or line.startswith(\"Location:\"):\n                    print(\"  \", line)\n        else:\n            print(f\"❌ {package} is NOT installed via pip\")\n    except Exception as e:\n        print(f\"Error checking {package} installation:\", e)\n\nprint(\"\\n=== Test complete ===\")"}
{"type": "source_file", "path": "backup_files/brainstorm.py", "content": "# brainstorm.py\nimport os\nimport json\nimport streamlit as st\nfrom autogen import ConversableAgent, UserProxyAgent, GroupChat, GroupChatManager\nfrom autogen.agentchat.contrib.capabilities.teachability import Teachability\nfrom ollama_utils import get_available_models\nimport markdown\nfrom prompts import get_agent_prompt, get_metacognitive_prompt, get_voice_prompt, get_identity_prompt\nfrom info_brainstorm import display_info_brainstorm\n\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\nSETTINGS_FILE = \"brainstorm_agents_settings.json\"\nWORKFLOWS_DIR = \"brainstorm_workflows\"\n\n# List of animal emojis\nANIMAL_EMOJIS = [\n    \"🐶\", \"🐱\", \"🐭\", \"🐹\", \"🐰\", \"🦊\", \"🐻\", \"🐼\", \"🐨\", \"🐯\", \"🦁\", \"🐮\", \"🐷\", \"🐸\", \"🐵\", \"🐔\", \"🐧\", \"🐦\", \"🐤\",\n    \"🦆\", \"🦅\", \"🦉\", \"🦇\", \"🐺\", \"🐗\", \"🐴\", \"🦄\", \"🐝\", \"🐛\", \"🦋\", \"🐌\", \"🐞\", \"🐜\", \"🦟\", \"🦗\", \"🕷\", \"🦂\", \"🐢\",\n    \"🐍\", \"🦎\", \"🦖\", \"🦕\", \"🐙\", \"🦑\", \"🦐\", \"🦞\", \"🦀\", \"🐡\", \"🐠\", \"🐟\", \"🐬\", \"🐳\", \"🐋\", \"🦈\", \"🐊\", \"🐅\", \"🐆\",\n    \"🦓\", \"🦍\", \"🦧\", \"🐘\", \"🦛\", \"🦏\", \"🐪\", \"🐫\", \"🦒\", \"🦘\", \"🐃\", \"🐂\", \"🐄\", \"🐎\", \"🐖\", \"🐏\", \"🐑\", \"🦙\", \"🐐\",\n    \"🦌\", \"🐕\", \"🐩\", \"🦮\", \"🐕‍🦺\", \"🐈\", \"🐈‍⬛\", \"🐓\", \"🦃\", \"🦚\", \"🦜\", \"🦢\", \"🦩\", \"🕊\", \"🐇\", \"🦝\", \"🦨\", \"🦡\", \"🦦\",\n    \"🦥\", \"🐁\", \"🐀\", \"🐿\", \"🦔\", \"🐾\", \"🐉\", \"🐲\", \"🤖\", \"🧚\"\n]\n\ntry:\n    from files_management import get_corpus_options\nexcept ImportError:\n    def get_corpus_options(): return []\n\nclass CustomConversableAgent(ConversableAgent):\n    def __init__(self, name, llm_config, agent_type, identity, metacognitive_type, voice_type, corpus, temperature, max_tokens, presence_penalty, frequency_penalty, db_path, *args, **kwargs):\n        super().__init__(name=name, llm_config=llm_config, *args, **kwargs)\n        self.agent_type = agent_type\n        self.identity = identity\n        self.metacognitive_type = metacognitive_type\n        self.voice_type = voice_type\n        self.corpus = corpus\n        self.temperature = temperature\n        self.max_tokens = max_tokens\n        self.presence_penalty = presence_penalty\n        self.frequency_penalty = frequency_penalty\n        self.teachability = Teachability(path_to_db_dir=db_path)\n        self.teachability.add_to_agent(self)\n\n    def generate_reply(self, messages, sender, config):\n        context = \"\\n\".join([f\"{m['role']} ({m.get('name', 'Unknown')}): {m['content']}\" for m in messages])\n        prompt = f\"\"\"\n        You are a {self.agent_type} agent named {self.name}.\n        Identity: {self.identity}\n        Metacognitive Type: {self.metacognitive_type}\n        Voice Type: {self.voice_type}\n        Corpus: {self.corpus}\n\n        Full conversation context:\n        {context}\n\n        Your task is to respond to the latest message, considering the full context above. Remember your role and respond accordingly:\n        \"\"\"\n        return super().generate_reply(messages=[{\"role\": \"user\", \"content\": prompt}], sender=sender, config=config)\n\ndef create_agent(settings):\n    llm_config = {\n        \"config_list\": [\n            {\n                \"model\": settings[\"model\"],\n                \"api_key\": settings[\"api_key\"],\n                \"base_url\": settings[\"base_url\"]\n            }\n        ],\n        \"timeout\": 120\n    }\n    return CustomConversableAgent(\n        name=f\"{settings['emoji']} {settings['name']}\",\n        llm_config=llm_config,\n        agent_type=settings[\"agent_type\"],\n        identity=settings[\"identity\"],\n        metacognitive_type=settings[\"metacognitive_type\"],\n        voice_type=settings[\"voice_type\"],\n        corpus=settings[\"corpus\"],\n        temperature=settings[\"temperature\"],\n        max_tokens=settings[\"max_tokens\"],\n        presence_penalty=settings[\"presence_penalty\"],\n        frequency_penalty=settings[\"frequency_penalty\"],\n        db_path=settings[\"db_path\"]\n    )\n\ndef load_agent_settings():\n    if os.path.exists(SETTINGS_FILE):\n        with open(SETTINGS_FILE, 'r') as f:\n            return json.load(f)\n    return {\"agents\": []}\n\ndef save_agent_settings(settings):\n    with open(SETTINGS_FILE, 'w') as f:\n        json.dump(settings, f, indent=2)\n\ndef get_available_workflows():\n    if not os.path.exists(WORKFLOWS_DIR):\n        os.makedirs(WORKFLOWS_DIR)\n    return [f[:-5] for f in os.listdir(WORKFLOWS_DIR) if f.endswith('.json')]\n\ndef save_workflow(workflow_name, agent_sequence):\n    if not os.path.exists(WORKFLOWS_DIR):\n        os.makedirs(WORKFLOWS_DIR)\n    workflow_path = os.path.join(WORKFLOWS_DIR, f\"{workflow_name}.json\")\n    with open(workflow_path, 'w') as f:\n        json.dump(agent_sequence, f, indent=2)\n    st.success(f\"Workflow '{workflow_name}' saved successfully!\")\n    print(f\"Saved workflow: {workflow_name}\")\n    print(f\"Agent sequence: {agent_sequence}\")\n\ndef load_workflow(workflow_name):\n    workflow_path = os.path.join(WORKFLOWS_DIR, f\"{workflow_name}.json\")\n    if os.path.exists(workflow_path):\n        with open(workflow_path, 'r') as f:\n            agent_sequence = json.load(f)\n        print(f\"Loaded workflow: {workflow_name}\")\n        print(f\"Agent sequence: {agent_sequence}\")\n        return agent_sequence\n    return None\n\ndef edit_agent_settings(agent_settings):\n    st.subheader(f\"Edit Agent: {agent_settings['name']}\")\n    \n    col1, col2, col3, col4 = st.columns(4)\n    \n    with col1:\n        agent_settings['name'] = st.text_input(\"Agent Name\", agent_settings['name'], key=f\"{agent_settings['name']}_name\")\n        agent_settings['emoji'] = st.selectbox(\"Emoji\", ANIMAL_EMOJIS, index=ANIMAL_EMOJIS.index(agent_settings.get('emoji', '🐶')), key=f\"{agent_settings['name']}_emoji\")\n        agent_settings['model'] = st.selectbox(\"Model\", get_available_models(), \n                                               index=get_available_models().index(agent_settings['model']),\n                                               key=f\"{agent_settings['name']}_model\")\n        voice_options = [\"None\"] + list(get_voice_prompt().keys())\n        agent_settings['voice_type'] = st.selectbox(\n            \"Voice Type\",\n            voice_options,\n            index=voice_options.index(agent_settings['voice_type']) if agent_settings['voice_type'] in voice_options else 0,\n            key=f\"{agent_settings['name']}_voice_type\"\n        )\n    \n    with col2:\n        agent_type_options = [\"None\"] + list(get_agent_prompt().keys())\n        agent_settings['agent_type'] = st.selectbox(\n            \"Agent Type\",\n            agent_type_options,\n            index=agent_type_options.index(agent_settings['agent_type']) if agent_settings['agent_type'] in agent_type_options else 0,\n            key=f\"{agent_settings['name']}_agent_type\"\n        )\n        identity_options = [\"None\"] + list(get_identity_prompt().keys())\n        agent_settings['identity'] = st.selectbox(\n            \"Identity\",\n            identity_options,\n            index=identity_options.index(agent_settings['identity']) if agent_settings['identity'] in identity_options else 0,\n            key=f\"{agent_settings['name']}_identity\"\n        )\n        metacognitive_options = [\"None\"] + list(get_metacognitive_prompt().keys())\n        agent_settings['metacognitive_type'] = st.selectbox(\n            \"Metacognitive Type\",\n            metacognitive_options,\n            index=metacognitive_options.index(agent_settings['metacognitive_type']) if agent_settings['metacognitive_type'] in metacognitive_options else 0,\n            key=f\"{agent_settings['name']}_metacognitive_type\"\n        )\n        corpus_options = [\"None\"] + get_corpus_options()\n        agent_settings['corpus'] = st.selectbox(\n            \"Corpus\",\n            corpus_options,\n            index=corpus_options.index(agent_settings['corpus']) if agent_settings['corpus'] in corpus_options else 0,\n            key=f\"{agent_settings['name']}_corpus\"\n        )\n    \n    with col3:\n        agent_settings['base_url'] = st.text_input(\"Base URL\", agent_settings['base_url'], key=f\"{agent_settings['name']}_base_url\")\n        agent_settings['api_key'] = st.text_input(\"API Key\", agent_settings['api_key'], key=f\"{agent_settings['name']}_api_key\")\n        agent_settings['db_path'] = st.text_input(\"Database Path\", agent_settings['db_path'], key=f\"{agent_settings['name']}_db_path\")\n    \n    with col4:\n        agent_settings['temperature'] = st.slider(\"Temperature\", 0.0, 1.0, agent_settings['temperature'], key=f\"{agent_settings['name']}_temperature\")\n        agent_settings['max_tokens'] = st.slider(\"Max Tokens\", 100, 32000, agent_settings['max_tokens'], key=f\"{agent_settings['name']}_max_tokens\")\n        agent_settings['presence_penalty'] = st.slider(\"Presence Penalty\", -2.0, 2.0, agent_settings['presence_penalty'], key=f\"{agent_settings['name']}_presence_penalty\")\n        agent_settings['frequency_penalty'] = st.slider(\"Frequency Penalty\", -2.0, 2.0, agent_settings['frequency_penalty'], key=f\"{agent_settings['name']}_frequency_penalty\")\n\n    return agent_settings\n\ndef manage_agents():\n    st.subheader(\"Manage Agents\")\n    settings = load_agent_settings()\n    \n    for i, agent in enumerate(settings[\"agents\"]):\n        with st.expander(f\"{agent.get('emoji', '🐶')} {agent['name']}\"):\n            updated_agent = edit_agent_settings(agent)\n            settings[\"agents\"][i] = updated_agent\n            if st.button(f\"Remove {agent['name']}\", key=f\"remove_{agent['name']}\"):\n                settings[\"agents\"].pop(i)\n                save_agent_settings(settings)\n                st.success(f\"Agent {agent['name']} removed.\")\n                st.rerun()\n    \n    with st.expander(\"➕ Add New Agent\"):\n        new_agent = {\n            \"name\": \"\",\n            \"emoji\": \"🐶\",\n            \"model\": get_available_models()[0],\n            \"api_key\": \"ollama\",\n            \"base_url\": \"http://localhost:11434/v1\",\n            \"agent_type\": \"None\",\n            \"identity\": \"None\",\n            \"metacognitive_type\": \"None\",\n            \"voice_type\": \"None\",\n            \"corpus\": \"None\",\n            \"temperature\": 0.7,\n            \"max_tokens\": 4000,\n            \"presence_penalty\": 0.0,\n            \"frequency_penalty\": 0.0,\n            \"db_path\": \"./tmp/new_agent_db\"\n        }\n        \n        new_agent = edit_agent_settings(new_agent)\n        \n        if st.button(\"Add Agent\"):\n            if new_agent[\"name\"]:\n                new_agent['db_path'] = f\"./tmp/{new_agent['name'].lower().replace(' ', '_')}_db\"\n                settings[\"agents\"].append(new_agent)\n                save_agent_settings(settings)\n                st.success(f\"Agent {new_agent['name']} added successfully!\")\n                st.rerun()\n            else:\n                st.error(\"Please provide a name for the new agent.\")\n    \n    save_agent_settings(settings)\n\ndef brainstorm_session():\n    settings = load_agent_settings()\n    agents = [create_agent(agent_settings) for agent_settings in settings[\"agents\"]]\n\n    if 'group_chat' not in st.session_state:\n        user = UserProxyAgent(\"👨‍💼 User\", human_input_mode=\"NEVER\")\n        st.session_state.group_chat = GroupChat(\n            agents=[*agents, user],\n            messages=[],\n            speaker_selection_method=\"manual\"\n        )\n        st.session_state.group_chat_manager = GroupChatManager(groupchat=st.session_state.group_chat)\n\n    # Workflow management\n    st.subheader(\"Workflow Management\")\n    available_workflows = get_available_workflows()\n    selected_workflow = st.selectbox(\"Load Workflow\", [\"\"] + available_workflows)\n    workflow_name = st.text_input(\"Workflow Name\")\n\n    if selected_workflow and selected_workflow != st.session_state.get('last_loaded_workflow'):\n        agent_sequence = load_workflow(selected_workflow)\n        if agent_sequence:\n            st.session_state.agent_sequence = agent_sequence\n            st.session_state.last_loaded_workflow = selected_workflow\n            st.success(f\"Workflow '{selected_workflow}' loaded successfully!\")\n            st.rerun()\n\n    # Agent sequence setup\n    st.subheader(\"Agent Response Sequence\")\n    if 'agent_sequence' not in st.session_state:\n        st.session_state.agent_sequence = []\n\n    agent_names = [f\"{agent.name}\" for agent in agents]\n    agent_names.insert(0, \"\")  # Add empty option at the beginning\n\n    # Number of agents in the workflow\n    num_agents = len(st.session_state.agent_sequence)\n    col1, col2 = st.columns([1, 1])\n    with col1:\n        st.write(\"Number of Agents:\")\n    with col2:\n        num_agents = st.number_input(\n            \"Number of Agents in Workflow\",\n            min_value=0,\n            value=num_agents,\n            step=1,\n            label_visibility=\"collapsed\"\n        )\n\n    # Display agent selection dropdowns\n    for i in range(num_agents):\n        current_agent = st.session_state.agent_sequence[i] if i < len(st.session_state.agent_sequence) else \"\"\n        agent = st.selectbox(\n            f\"Agent {i+1}\", \n            agent_names, \n            index=agent_names.index(current_agent) if current_agent in agent_names else 0,\n            key=f\"agent_{i}\"\n        )\n        if i < len(st.session_state.agent_sequence):\n            st.session_state.agent_sequence[i] = agent\n        else:\n            st.session_state.agent_sequence.append(agent)\n\n    if st.button(\"Save Workflow\"):\n        if workflow_name:\n            current_sequence = [agent for agent in st.session_state.agent_sequence if agent]\n            save_workflow(workflow_name, current_sequence)\n        else:\n            st.error(\"Please enter a workflow name before saving.\")\n\n    # User input\n    user_message = st.text_input(\"Enter your message:\")\n\n    if st.button(\"Send\"):\n        if user_message and any(st.session_state.agent_sequence):\n            # Add user message to the group chat\n            st.session_state.group_chat.messages.append({\"role\": \"user\", \"name\": \"User\", \"content\": user_message})\n            \n            # Generate responses from the sequence of agents\n            for agent_name in st.session_state.agent_sequence:\n                if agent_name:  # Skip empty selections\n                    with st.spinner(f\"{agent_name} is thinking...\"):\n                        selected_agent_obj = next((agent for agent in agents if agent.name == agent_name), None)\n                        if selected_agent_obj:\n                            response = selected_agent_obj.generate_reply(st.session_state.group_chat.messages, sender=st.session_state.group_chat_manager, config=None)\n                            # Add agent's response to the group chat\n                            st.session_state.group_chat.messages.append({\"role\": \"assistant\", \"name\": agent_name, \"content\": response})\n                        else:\n                            st.error(f\"Agent '{agent_name}' not found.\")\n\n    # Display conversation history with formatting\n    st.subheader(\"Conversation History\")\n    for message in st.session_state.group_chat.messages:\n        with st.chat_message(message['role']):\n            st.markdown(f\"**{message['name']}**\")\n            formatted_content = markdown.markdown(message['content'])\n            st.markdown(formatted_content, unsafe_allow_html=True)\n\n    # Display info about Brainstorm feature\n    display_info_brainstorm()\n\ndef brainstorm_interface():\n    st.title(\"🧠 Brainstorm\")\n    \n    tab1, tab2 = st.tabs([\"Brainstorm Session\", \"Manage Agents\"])\n    \n    with tab1:\n        brainstorm_session()\n    \n    with tab2:\n        manage_agents()\n\nif __name__ == \"__main__\":\n    brainstorm_interface()"}
{"type": "source_file", "path": "backup_files/validate_installation.py", "content": "#!/usr/bin/env python3\n\"\"\"\nValidation script for Ollama Workbench installation.\nThis script verifies that all required components are properly installed and configured.\n\"\"\"\n\nimport importlib\nimport os\nimport platform\nimport subprocess\nimport sys\nfrom pathlib import Path\n\n# ANSI colors for terminal output\nCOLORS = {\n    \"CYAN\": \"\\033[0;36m\",  \n    \"GREEN\": \"\\033[0;32m\",  \n    \"YELLOW\": \"\\033[1;33m\", \n    \"RED\": \"\\033[0;31m\",    \n    \"NC\": \"\\033[0m\",        # No Color\n}\n\ndef print_header(text):\n    \"\"\"Print a formatted header.\"\"\"\n    print(f\"\\n{COLORS['CYAN']}{'=' * 60}{COLORS['NC']}\")\n    print(f\"{COLORS['CYAN']}  {text}{COLORS['NC']}\")\n    print(f\"{COLORS['CYAN']}{'=' * 60}{COLORS['NC']}\\n\")\n\ndef print_success(text):\n    \"\"\"Print a success message.\"\"\"\n    print(f\"{COLORS['GREEN']}✓{COLORS['NC']} {text}\")\n\ndef print_warning(text):\n    \"\"\"Print a warning message.\"\"\"\n    print(f\"{COLORS['YELLOW']}!{COLORS['NC']} {text}\")\n\ndef print_error(text):\n    \"\"\"Print an error message.\"\"\"\n    print(f\"{COLORS['RED']}✗{COLORS['NC']} {text}\")\n\ndef check_import(module_name, package_name=None):\n    \"\"\"Check if a module can be imported.\"\"\"\n    try:\n        module = importlib.import_module(module_name)\n        if hasattr(module, '__version__'):\n            print_success(f\"{module_name} v{module.__version__} is installed\")\n            return True, module.__version__\n        else:\n            print_success(f\"{module_name} is installed (version unknown)\")\n            return True, None\n    except ImportError:\n        suggested_package = package_name or module_name\n        print_error(f\"{module_name} could not be imported\")\n        print_warning(f\"Try installing it with: pip install {suggested_package}\")\n        return False, None\n\ndef check_python_version():\n    \"\"\"Check if Python version is compatible.\"\"\"\n    python_version = platform.python_version()\n    python_version_tuple = tuple(map(int, python_version.split('.')))\n    \n    if python_version_tuple >= (3, 11):\n        print_success(f\"Python {python_version} detected (✓ Compatible)\")\n        return True\n    else:\n        print_error(f\"Python {python_version} detected (✗ Version 3.11+ recommended)\")\n        print_warning(\"Some components may not work properly with older Python versions\")\n        return False\n\ndef check_platform():\n    \"\"\"Check if running on Apple Silicon and report platform details.\"\"\"\n    system = platform.system()\n    machine = platform.machine()\n    processor = platform.processor()\n    \n    if system == \"Darwin\" and machine == \"arm64\":\n        print_success(f\"Detected Apple Silicon Mac ({processor})\")\n        return True, \"Apple Silicon\"\n    elif system == \"Darwin\" and (machine == \"x86_64\" or machine == \"i386\"):\n        print_warning(f\"Detected Intel Mac ({processor})\")\n        print_warning(\"This setup is optimized for Apple Silicon, but should work on Intel Macs\")\n        return True, \"Intel Mac\"\n    elif system == \"Linux\":\n        print_warning(f\"Detected Linux ({machine})\")\n        print_warning(\"This setup is optimized for Apple Silicon, but should work on Linux\")\n        return True, \"Linux\"\n    elif system == \"Windows\":\n        print_warning(f\"Detected Windows ({machine})\")\n        print_warning(\"This setup is optimized for Apple Silicon, but should work on Windows with adjustments\")\n        return True, \"Windows\"\n    else:\n        print_warning(f\"Unknown platform: {system} {machine}\")\n        return False, f\"{system} {machine}\"\n\ndef check_torch_mps():\n    \"\"\"Check if PyTorch with MPS support is installed.\"\"\"\n    try:\n        import torch\n        print_success(f\"PyTorch v{torch.__version__} is installed\")\n        \n        # Different versions of PyTorch have different ways to check MPS\n        mps_available = False\n        \n        # Try the standard way (PyTorch 2.0+)\n        if hasattr(torch, 'backends') and hasattr(torch.backends, 'mps'):\n            try:\n                if torch.backends.mps.is_available():\n                    mps_available = True\n            except AttributeError:\n                pass\n        \n        # Try the older way or direct check\n        if not mps_available and hasattr(torch, 'mps'):\n            try:\n                # Some versions use torch.mps.is_available()\n                if hasattr(torch.mps, 'is_available') and torch.mps.is_available():\n                    mps_available = True\n                # Some versions just check if the module exists\n                else:\n                    mps_available = True\n            except (AttributeError, ImportError):\n                pass\n                \n        # Try creating a tensor on MPS as final check\n        if not mps_available:\n            try:\n                device = torch.device(\"mps\")\n                x = torch.ones(1, device=device)\n                mps_available = True\n            except (RuntimeError, ValueError):\n                pass\n                \n        if mps_available:\n            print_success(\"MPS (Metal Performance Shaders) is available\")\n            return True\n        else:\n            print_warning(\"MPS is supported but not available on this system\")\n            print_warning(\"This might be due to hardware limitations or configuration issues\")\n            return False\n    except ImportError:\n        print_error(\"PyTorch could not be imported\")\n        print_warning(\"Try installing it with: pip install torch torchvision\")\n        return False\n\ndef check_ollama_server():\n    \"\"\"Check if Ollama server is running.\"\"\"\n    try:\n        import requests\n        try:\n            response = requests.get(\"http://localhost:11434/api/tags\", timeout=5)\n            if response.status_code == 200:\n                print_success(\"Ollama server is running\")\n                \n                # Check available models\n                models = response.json().get('models', [])\n                if models:\n                    model_names = [model.get('name', 'unknown') for model in models]\n                    print_success(f\"Available models: {', '.join(model_names)}\")\n                else:\n                    print_warning(\"No models found. You might need to pull models.\")\n                    print_warning(\"Try: ollama pull llama3\")\n                \n                return True\n            else:\n                print_error(f\"Ollama server returned status code: {response.status_code}\")\n                return False\n        except requests.RequestException:\n            print_error(\"Ollama server is not running or not accessible\")\n            print_warning(\"Start the server with: ollama serve\")\n            return False\n    except ImportError:\n        print_error(\"requests library could not be imported\")\n        print_warning(\"Try installing it with: pip install requests\")\n        return False\n\ndef main():\n    \"\"\"Run validation checks.\"\"\"\n    print_header(\"Ollama Workbench Validation\")\n    \n    success_count = 0\n    warning_count = 0\n    error_count = 0\n    \n    # Check platform\n    platform_ok, platform_type = check_platform()\n    if platform_ok:\n        success_count += 1\n    else:\n        warning_count += 1\n    \n    # Check Python version\n    if check_python_version():\n        success_count += 1\n    else:\n        warning_count += 1\n    \n    # Check Poetry installation\n    poetry_path = None\n    for path in [\n        os.path.expanduser(\"~/.local/bin/poetry\"),\n        \"/usr/local/bin/poetry\",\n        \"/opt/homebrew/bin/poetry\"\n    ]:\n        if os.path.exists(path):\n            poetry_path = path\n            break\n    \n    if poetry_path:\n        print_success(f\"Poetry found at {poetry_path}\")\n        try:\n            result = subprocess.run([poetry_path, \"--version\"], \n                                    capture_output=True, text=True, check=True)\n            print_success(f\"Poetry version: {result.stdout.strip()}\")\n            success_count += 1\n        except (subprocess.CalledProcessError, FileNotFoundError):\n            print_warning(\"Poetry found but could not determine version\")\n            warning_count += 1\n    else:\n        print_error(\"Poetry not found in common locations\")\n        print_warning(\"Install Poetry with: curl -sSL https://install.python-poetry.org | python3 -\")\n        error_count += 1\n    \n    # Check PyTorch with MPS (for Apple Silicon)\n    if platform_type == \"Apple Silicon\":\n        if check_torch_mps():\n            success_count += 1\n        else:\n            error_count += 1\n    else:\n        # Check regular PyTorch\n        success, version = check_import(\"torch\")\n        if success:\n            success_count += 1\n        else:\n            error_count += 1\n    \n    # Check key dependencies\n    dependencies = [\n        (\"streamlit\", None),\n        (\"transformers\", None),\n        (\"sentence_transformers\", \"sentence-transformers\"),\n        (\"langchain\", None),\n        (\"langchain_community\", \"langchain-community\"),\n    ]\n    \n    for module, package in dependencies:\n        success, _ = check_import(module, package)\n        if success:\n            success_count += 1\n        else:\n            error_count += 1\n    \n    # Check Ollama server\n    if check_ollama_server():\n        success_count += 1\n    else:\n        warning_count += 1\n    \n    # Print summary\n    print_header(\"Validation Summary\")\n    print(f\"Successes: {success_count}\")\n    print(f\"Warnings: {warning_count}\")\n    print(f\"Errors: {error_count}\")\n    \n    if error_count > 0:\n        print_error(\"Some critical components are missing or misconfigured\")\n        print_warning(\"Please resolve the issues above and run this script again\")\n        return False\n    elif warning_count > 0:\n        print_warning(\"Setup completed with some warnings\")\n        print_warning(\"You may experience some limitations, but the core functionality should work\")\n        return True\n    else:\n        print_success(\"All validation checks passed!\")\n        print_success(\"Ollama Workbench is ready to use\")\n        return True\n\nif __name__ == \"__main__\":\n    success = main()\n    sys.exit(0 if success else 1)"}
{"type": "source_file", "path": "build.py", "content": "import ast\nimport json\nimport os\nimport re\nimport shutil\nimport subprocess\nimport sys\nimport tempfile\nfrom datetime import datetime\nfrom pathlib import Path\nfrom typing import Any, Dict, List, Tuple, Union\n\nimport ollama\nimport pytest\nfrom duckduckgo_search import DDGS\nfrom googleapiclient.discovery import build\nfrom ollama import Client\nfrom openai import ChatCompletion, OpenAI\nfrom rich.console import Console\nfrom streamlit import session_state as st_ss\n\nfrom openai_utils import *\nfrom groq_utils import *\nfrom ollama_utils import *\nfrom external_providers import *\n\nAPI_KEYS_FILE = \"api_keys.json\"\nSETTINGS_FILE = \"build_settings.json\"\n\n# Initialize the Rich Console\nconsole = Console()\n\n# Initialize the Ollama client\nclient = Client(host=\"http://localhost:11434\")\n\ndef load_json_file(filepath: str) -> dict:\n    \"\"\"Loads JSON data from a file if it exists.\"\"\"\n    if os.path.exists(filepath):\n        with open(filepath, \"r\") as f:\n            return json.load(f)\n    return {}\n\ndef save_json_file(filepath: str, data: dict) -> None:\n    \"\"\"Saves JSON data to a file.\"\"\"\n    with open(filepath, \"w\") as f:\n        json.dump(data, f, indent=4)\n\ndef load_api_keys() -> dict:\n    \"\"\"Loads API keys from the JSON file.\"\"\"\n    return load_json_file(API_KEYS_FILE)\n\ndef save_api_keys(api_keys: dict) -> None:\n    \"\"\"Saves API keys to the JSON file.\"\"\"\n    save_json_file(API_KEYS_FILE, api_keys)\n\ndef load_settings() -> dict:\n    \"\"\"Loads settings from the JSON file.\"\"\"\n    return load_json_file(SETTINGS_FILE)\n\ndef save_settings(settings: dict) -> None:\n    \"\"\"Saves settings to the JSON file.\"\"\"\n    save_json_file(SETTINGS_FILE, settings)\n\ndef set_openai_api_key(api_key: str) -> None:\n    \"\"\"Sets the OpenAI API key.\"\"\"\n    api_keys = load_api_keys()\n    api_keys['openai_api_key'] = api_key\n    save_api_keys(api_keys)\n    console.print(\"[bold green]OpenAI API key has been set.[/bold green]\")\n\ndef call_openai_api(\n    model: str,\n    messages: List[Dict[str, Any]],\n    temperature: float = 0.7,\n    max_tokens: int = 1000,\n    frequency_penalty: float = 0.0,\n    presence_penalty: float = 0.0,\n    stream: bool = False,\n    openai_api_key: str = None\n) -> Any:\n    \"\"\"Wrapper function to call the OpenAI Chat API with a unified interface.\"\"\"\n    try:\n        temperature = float(temperature)\n        max_tokens = int(max_tokens)\n        frequency_penalty = float(frequency_penalty)\n        presence_penalty = float(presence_penalty)\n    except ValueError as ve:\n        raise ValueError(f\"Invalid value for one of the numerical parameters: {ve}\")\n\n    if not openai_api_key or not isinstance(openai_api_key, str):\n        api_keys = load_api_keys()\n        openai_api_key = api_keys.get('openai_api_key')\n        if not openai_api_key or not isinstance(openai_api_key, str):\n            raise ValueError(\"Invalid or missing OpenAI API key.\")\n\n    openai.api_key = openai_api_key\n\n    try:\n        response = openai.ChatCompletion.create(\n            model=model,\n            messages=messages,\n            temperature=temperature,\n            max_tokens=max_tokens,\n            frequency_penalty=frequency_penalty,\n            presence_penalty=presence_penalty,\n            stream=stream\n        )\n\n        if stream:\n            return response  # Return the stream object\n        else:\n            return response.choices[0].message['content'].strip()\n\n    except json.JSONDecodeError as json_err:\n        raise ValueError(f\"Failed to parse response as JSON: {json_err}\")\n    except Exception as e:\n        console.print(f\"[bold red]Error calling OpenAI API:[/bold red] {e}\")\n        return \"Error occurred while calling OpenAI API\"\n\ndef is_groq_model(model_name: str) -> bool:\n    \"\"\"Determines if a given model name is a Groq model.\"\"\"\n    api_keys = load_api_keys()\n    available_groq_models = get_available_groq_models(api_keys)\n    return model_name in available_groq_models\n\ndef perform_search(\n    query: str,\n    search_method: str,\n    api_keys: Dict[str, str],\n    num_results: int = 5\n) -> List[Dict[str, str]]:\n    \"\"\"Performs a web search using the specified method.\"\"\"\n    if search_method == \"duckduckgo\":\n        with DDGS() as ddgs:\n            results = list(ddgs.text(query, max_results=num_results))\n        return [{\"title\": result[\"title\"], \"url\": result[\"href\"]} for result in results]\n\n    elif search_method == \"google\":\n        if \"google_api_key\" not in api_keys or \"google_cse_id\" not in api_keys:\n            console.print(\"[bold red]Error: Google API Key or CSE ID not provided.[/bold red]\")\n            return []\n        service = build(\"customsearch\", \"v1\", developerKey=api_keys[\"google_api_key\"])\n        res = service.cse().list(q=query, cx=api_keys[\"google_cse_id\"], num=num_results).execute()\n        return [{\"title\": item[\"title\"], \"url\": item[\"link\"]} for item in res.get(\"items\", [])]\n\n    elif search_method == \"serpapi\":\n        if \"serpapi_api_key\" not in api_keys:\n            console.print(\"[bold red]Error: SerpAPI Key not provided.[/bold red]\")\n            return []\n        try:\n            params = {\n                \"engine\": \"google\",\n                \"q\": query,\n                \"api_key\": api_keys[\"serpapi_api_key\"],\n                \"num\": num_results,\n            }\n            search = GoogleSearch(params)\n            results = search.get_dict()\n            organic_results = results.get(\"organic_results\", [])\n            return [{\"title\": result[\"title\"], \"url\": result[\"link\"]} for result in organic_results]\n        except Exception as e:\n            console.print(f\"Error in SerpApi search: {str(e)}\")\n            return []\n\n    else:\n        console.print(f\"[bold red]Unsupported search method: {search_method}[/bold red]\")\n        return []\n\ndef create_agent_context(\n    project_state: dict,\n    current_task: str\n) -> dict:\n    \"\"\"Creates the context for agents based on the project state.\"\"\"\n    return {\n        \"project_state\": {\n            \"status\": project_state[\"status\"],\n            \"current_step\": project_state[\"current_step\"],\n            \"iterations\": project_state[\"iterations\"],\n            \"max_iterations\": project_state[\"max_iterations\"],\n        },\n        \"current_task\": current_task,\n        \"previous_tasks\": project_state.get(\"previous_tasks\", []),\n    }\n\ndef manager_agent_task(\n    context: Dict[str, Any],\n    model: str,\n    temperature: float,\n    max_tokens: int,\n    openai_api_key=None\n) -> Tuple[Dict[str, Any], Any]:\n    \"\"\"Handles the manager agent task based on the context.\"\"\"\n    prompt = f\"\"\"\n    You are the manager agent in an Agile software development process. \n    Current project state: {json.dumps(context['project_state'], indent=2)}\n    Current task: {context['current_task']}\n\n    Based on the current state, please:\n    1. Analyze the current project step.\n    2. Update the work plan for the next sprint.\n    3. Prioritize tasks for the coding agents.\n    4. If repository files haven't been created yet, make this the top priority.\n\n    Respond with a JSON object containing:\n    1. \"analysis\": Your analysis of the current state.\n    2. \"work_plan\": The updated work plan for the next sprint.\n    3. \"priorities\": A list of prioritized tasks.\n    4. \"instructions\": Clear instructions for the coding agents.\n    5. \"create_files\": true if repository files should be created, false otherwise.\n\n    Your response must be a valid JSON object.\n    \"\"\"\n\n    try:\n        temperature = float(temperature)\n        max_tokens = int(max_tokens)\n\n        if model.startswith(\"gpt-\"):\n            client = OpenAI(api_key=openai_api_key)\n            response = client.chat.completions.create(\n                model=model,\n                messages=[{\"role\": \"user\", \"content\": prompt}],\n                temperature=temperature,\n                max_tokens=max_tokens,\n                response_format={\"type\": \"json_object\"}\n            )\n            response_content = response.choices[0].message.content\n        elif model in GROQ_MODELS:\n            response_content = call_groq_api(model, prompt, temperature, max_tokens, groq_api_key=openai_api_key)\n        else:\n            # Assume it's an Ollama model\n            response_content, _, _, _ = call_ollama_endpoint(\n                model=model,\n                prompt=prompt,\n                temperature=temperature,\n                max_tokens=max_tokens\n            )\n\n        parsed_response = json.loads(response_content)\n        return parsed_response, None\n\n    except json.JSONDecodeError as e:\n        error_msg = f\"Error parsing JSON response from Manager Agent: {e}\"\n        console.print(error_msg)\n        console.print(f\"Raw response that caused the error: {response_content}\")\n        return {\n            \"analysis\": \"Error parsing response. Please review the raw output.\",\n            \"work_plan\": \"Unable to generate work plan due to parsing error.\",\n            \"priorities\": [\"Review and fix JSON parsing issues\"],\n            \"instructions\": \"Please review the raw output and manually extract relevant information.\",\n        }, None\n    except Exception as e:\n        error_msg = f\"An error occurred during the manager agent task: {str(e)}\"\n        console.print(error_msg)\n        return {}, None\n\ndef create_repository_files(project_dir: Path, refined_output: str) -> Dict[str, str]:\n    \"\"\"\n    Creates repository files based on the refined output.\n    Returns a dictionary of filenames and their contents.\n    \"\"\"\n    created_files = {}\n    \n    # Extract folder structure\n    folder_structure = parse_folder_structure(refined_output)\n    if folder_structure:\n        create_folder_structure(project_dir, folder_structure)\n    \n    # Extract and create code files\n    code_blocks = extract_code_blocks(refined_output)\n    for filename, code in code_blocks.items():\n        file_path = project_dir / filename\n        file_path.parent.mkdir(parents=True, exist_ok=True)\n        file_path.write_text(code, encoding=\"utf-8\")\n        created_files[str(file_path.relative_to(project_dir))] = code\n    \n    return created_files\n\ndef coding_agent_task(\n    prompt: str,\n    model: str,\n    previous_tasks=None,\n    use_search=False,\n    continuation=False,\n    temperature: float = 0.2,\n    max_tokens: int = 8000,\n    search_results=None,\n    groq_api_key=None,\n    openai_api_key=None\n) -> Dict[str, Any]:\n    \"\"\"Handles the coding agent task based on the provided prompt and context.\"\"\"\n    \n    if previous_tasks is None:\n        previous_tasks = []\n\n    previous_tasks_str = \"\\n\".join(\n        [\n            f\"Task: {task.get('task', '')}\\nResult: {task.get('result', '')}\"\n            for task in previous_tasks\n            if isinstance(task, dict)\n        ]\n    )\n\n    continuation_prompt = (\n        \"Continuing from the previous answer, please complete the response.\"\n    )\n    previous_tasks_summary = (\n        f\"Previous Sub-agent tasks:\\n{previous_tasks_str}\"\n        if previous_tasks_str\n        else \"No previous sub-agent tasks.\"\n    )\n    if continuation:\n        prompt = continuation_prompt\n\n    full_prompt = f\"{previous_tasks_summary}\\n\\n{prompt}\"\n    if use_search and search_results:\n        full_prompt += f\"\\n\\nSearch Results:\\n{json.dumps(search_results, indent=2)}\"\n\n    if not full_prompt.strip():\n        raise ValueError(\"Prompt cannot be empty\")\n\n    full_prompt += \"\\n\\nRespond with a JSON object containing your implementation details and any necessary explanations. Your response must be a valid JSON object.\"\n\n    messages = [{\"role\": \"user\", \"content\": full_prompt}]\n\n    try:\n        temperature = float(temperature)\n        max_tokens = int(max_tokens)\n\n        if model.startswith(\"gpt-\"):\n            client = OpenAI(api_key=openai_api_key)\n            response = client.chat.completions.create(\n                model=model,\n                messages=messages,\n                temperature=temperature,\n                max_tokens=max_tokens,\n                response_format={\"type\": \"json_object\"}\n            )\n            response_content = response.choices[0].message.content\n        elif model in GROQ_MODELS:\n            response_content = call_groq_api(model, full_prompt, temperature, max_tokens, groq_api_key=groq_api_key)\n        else:\n            # Assume it's an Ollama model\n            response_content, _, _, _ = call_ollama_endpoint(\n                model=model,\n                prompt=full_prompt,\n                temperature=temperature,\n                max_tokens=max_tokens\n            )\n\n        parsed_response = json.loads(response_content)\n        return parsed_response\n\n    except json.JSONDecodeError as e:\n        error_msg = f\"Error parsing JSON response from Coding Agent: {e}\"\n        console.print(error_msg)\n        console.print(f\"Raw response that caused the error: {response_content}\")\n        return {\"error\": error_msg, \"raw_response\": response_content}\n    except Exception as e:\n        error_msg = f\"An error occurred during the coding agent task: {str(e)}\"\n        console.print(error_msg)\n        return {\"error\": error_msg}\n\ndef refine_task(\n    objective: str,\n    model: str,\n    sub_task_results: List[str],\n    filename: str,\n    projectname: str,\n    continuation=False,\n    temperature: float = 0.2,\n    max_tokens: int = 8000,\n    groq_api_key=None,\n    openai_api_key=None\n) -> str:\n    \"\"\"Handles the task refinement process for the final output.\"\"\"\n    console.print(\"\\nCalling Refiner to provide the refined final output for your objective:\")\n    messages = [\n        {\n            \"role\": \"user\",\n            \"content\": \"Objective: \"\n            + objective\n            + \"\\n\\nSub-task results:\\n\"\n            + \"\\n\".join(sub_task_results)\n            + \"\\n\\nPlease review and refine the sub-task results into a cohesive final output. Add any missing information or details as needed. When working on code projects, ONLY AND ONLY IF THE PROJECT IS CLEARLY A CODING ONE please provide the following:\\n1. Project Name: Create a concise and appropriate project name that fits the project based on what it's creating. The project name should be no more than 20 characters long.\\n2. Folder Structure: Provide the folder structure as a valid JSON object, where each key represents a folder or file, and nested keys represent subfolders. Use null values for files. Ensure the JSON is properly formatted without any syntax errors. Please make sure all keys are enclosed in double quotes, and ensure objects are correctly encapsulated with braces, separating items with commas as necessary.\\nWrap the JSON object in <folder_structure> tags.\\n3. Code Files: For each code file, include ONLY the file name NEVER EVER USE THE FILE PATH OR ANY OTHER FORMATTING YOU ONLY USE THE FOLLOWING format 'Filename: <filename>' followed by the code block enclosed in triple backticks, with the language identifier after the opening backticks, like this:\\n\\nFilename: <filename>\\n```python\\n<code>\\n```\",\n        }\n    ]\n\n    try:\n        if isinstance(model, str) and model.startswith(\"gpt-\"):\n            response_text = call_openai_api(model, messages, temperature, max_tokens, openai_api_key=openai_api_key)\n        elif isinstance(model, str) and is_groq_model(model):\n            response_text = call_groq_api(model, messages[0][\"content\"], temperature, max_tokens, groq_api_key=groq_api_key)\n        else:\n            response_text = call_ollama_endpoint(\n                model=model,\n                prompt=messages[0][\"content\"],\n                temperature=temperature,\n                max_tokens=max_tokens\n            )\n\n        # Ensure response_text is a string\n        if isinstance(response_text, tuple):\n            response_text = response_text[0]\n        \n        if not isinstance(response_text, str):\n            response_text = str(response_text)\n\n        if len(response_text) >= 8000 and not continuation:\n            console.print(\"Warning: Output may be truncated. Attempting to continue the response.\")\n            continuation_response_text = refine_task(\n                objective,\n                model,\n                sub_task_results + [response_text],\n                filename,\n                projectname,\n                continuation=True,\n                temperature=temperature,\n                max_tokens=max_tokens,\n                groq_api_key=groq_api_key,\n                openai_api_key=openai_api_key,\n            )\n            response_text += \"\\n\" + continuation_response_text\n\n        return response_text\n\n    except Exception as e:\n        error_msg = f\"An error occurred during the refine task: {str(e)}\"\n        console.print(error_msg)\n        return error_msg\n\ndef parse_folder_structure(structure_string: str) -> dict:\n    \"\"\"Parses the folder structure from the response string.\"\"\"\n    structure_string = re.sub(r\"\\s+\", \" \", structure_string)\n    match = re.search(r\"<folder_structure>(.*?)</folder_structure>\", structure_string)\n    if not match:\n        return None\n\n    json_string = match.group(1)\n\n    try:\n        structure = json.loads(json_string)\n        return structure\n    except json.JSONDecodeError as e:\n        console.print(f\"[bold red]Error parsing JSON:[/bold red] {e}\")\n        console.print(f\"[bold red]Invalid JSON string:[/bold red] {json_string}\")\n        return None\n\ndef extract_code_blocks(refined_output: str) -> Dict[str, str]:\n    \"\"\"Extracts code blocks from the refined output.\"\"\"\n    code_blocks = {}\n    pattern = r\"Filename: (.*?)\\n```(.*?)\\n```\"\n    matches = re.finditer(pattern, refined_output, re.DOTALL)\n    for match in matches:\n        filename = match.group(1).strip()\n        code = match.group(2).strip()\n        code_blocks[filename] = code\n    return code_blocks\n\ndef save_file(content: str, filename: str, project_dir: str) -> None:\n    \"\"\"Saves content to a file within the specified project directory.\"\"\"\n    try:\n        file_path = Path(project_dir) / \"code\" / filename\n        file_path.parent.mkdir(parents=True, exist_ok=True)\n        file_path.write_text(content, encoding=\"utf-8\")\n        console.print(f\"[bold green]Saved file: {file_path}[/bold green]\")\n    except Exception as e:\n        console.print(f\"[bold red]Error saving file '{filename}': {str(e)}[/bold red]\")\n\ndef dump_repository(repo_path: str) -> Dict[str, str]:\n    \"\"\"Dumps the contents of a repository to a dictionary.\"\"\"\n    repo_contents = {}\n    for root, _, files in os.walk(repo_path):\n        for file in files:\n            if file.endswith((\".py\", \".json\", \".md\", \".txt\", \".yml\", \".yaml\")):\n                file_path = os.path.join(root, file)\n                with open(file_path, \"r\", encoding=\"utf-8\") as f:\n                    try:\n                        content = f.read()\n                        relative_path = os.path.relpath(file_path, repo_path)\n                        repo_contents[relative_path] = content\n                    except UnicodeDecodeError:\n                        console.print(f\"[bold yellow]Skipping binary file: {file_path}[/bold yellow]\")\n    return repo_contents\n\ndef generate_test_cases(code_analysis: Dict[str, List[str]]) -> str:\n    \"\"\"Generates test cases based on the analyzed code.\"\"\"\n    test_cases = []\n    for func in code_analysis[\"functions\"]:\n        test_cases.append(\n            f\"\"\"\ndef test_{func}():\n    # TODO: Implement test for {func}\n    assert True\n\"\"\"\n        )\n    for cls in code_analysis[\"classes\"]:\n        test_cases.append(\n            f\"\"\"\nclass Test{cls}:\n    def test_init(self):\n        # TODO: Implement test for {cls} initialization\n        assert True\n\n    def test_methods(self):\n        # TODO: Implement tests for {cls} methods\n        assert True\n\"\"\"\n        )\n    return \"\\n\".join(test_cases)\n\ndef run_tests(project_dir: str) -> Dict[str, Any]:\n    \"\"\"Runs tests in the project directory and returns the results.\"\"\"\n    test_dir = os.path.join(project_dir, \"tests\")\n    if not os.path.exists(test_dir):\n        os.makedirs(test_dir)\n\n    for root, _, files in os.walk(project_dir):\n        for file in files:\n            if file.endswith(\".py\") and not file.startswith(\"test_\"):\n                file_path = os.path.join(root, file)\n                with open(file_path, \"r\") as f:\n                    code = f.read()\n                code_analysis = analyze_code(code)\n                test_cases = generate_test_cases(code_analysis)\n                test_file_path = os.path.join(test_dir, f\"test_{file}\")\n                with open(test_file_path, \"w\") as f:\n                    f.write(\n                        f\"import sys\\nimport os\\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\\n\"\n                    )\n                    f.write(f\"from {file[:-3]} import *\\n\\n\")\n                    f.write(test_cases)\n\n    try:\n        pytest_result = subprocess.run(\n            [\"pytest\", \"-v\", test_dir], capture_output=True, text=True\n        )\n\n        return {\n            \"pytest_output\": pytest_result.stdout,\n            \"passed\": pytest_result.stdout.count(\"PASSED\"),\n            \"failed\": pytest_result.stdout.count(\"FAILED\"),\n            \"errors\": pytest_result.stdout.count(\"ERROR\"),\n            \"warnings\": pytest_result.stdout.count(\"WARN\"),\n        }\n    except Exception as e:\n        console.print(f\"[bold red]An error occurred during testing: {str(e)}[/bold red]\")\n        return {}\n\ndef generate_readme(\n    project_name: str,\n    user_request: str,\n    project_type: str,\n    refined_output: str,\n    refiner_model: str\n) -> str:\n    \"\"\"Generates a README.md file for the project.\"\"\"\n    readme_prompt = f\"\"\"\n    Create a comprehensive README.md file for the following project:\n\n    Project Name: {project_name}\n    Project Type: {project_type}\n    User Request: {user_request}\n\n    Include the following sections:\n    1. Project Title and Description\n    2. Installation Instructions\n    3. Usage Guide\n    4. Features\n    5. Dependencies\n    6. Contributing Guidelines\n    7. License Information (Assume MIT license)\n    8. Contact/Support Information (Use generic placeholder information)\n\n    Use appropriate Markdown formatting to make the README visually appealing and easy to read.\n    Base the content on the following refined output from the project generation:\n\n    {refined_output}\n    \"\"\"\n\n    messages = [{\"role\": \"user\", \"content\": readme_prompt}]\n    try:\n        api_keys = load_api_keys()\n        if refiner_model.startswith(\"gpt-\"):\n            readme_content = call_openai_api(\n                model=refiner_model,\n                messages=messages,\n                temperature=0.7,\n                max_tokens=2000,\n                openai_api_key=api_keys.get(\"openai_api_key\")\n            )\n        elif is_groq_model(refiner_model):\n            readme_content = call_groq_api(\n                model=refiner_model,\n                prompt=messages[0][\"content\"],\n                temperature=0.7,\n                max_tokens=2000,\n                groq_api_key=api_keys.get(\"groq_api_key\")\n            )\n        else:\n            readme_content = call_ollama_endpoint(\n                model=refiner_model,\n                prompt=messages[0][\"content\"],\n                temperature=0.7,\n                max_tokens=2000\n            )\n\n        # Ensure readme_content is a string\n        if isinstance(readme_content, tuple):\n            readme_content = readme_content[0]\n        \n        if not isinstance(readme_content, str):\n            readme_content = str(readme_content)\n\n        return readme_content\n    except Exception as e:\n        error_msg = f\"An error occurred during README generation: {str(e)}\"\n        console.print(error_msg)\n        return f\"# README\\n\\nError generating README: {error_msg}\\n\\nPlease check the API connection and try again.\"\n    \ndef execute_code(code: str, project_type: str) -> str:\n    \"\"\"Executes the generated code based on the project type.\"\"\"\n    if project_type == \"Command-line Tool\":\n        with tempfile.NamedTemporaryFile(mode=\"w+\", delete=False, suffix=\".py\") as temp_file:\n            temp_file.write(code)\n            temp_file_path = temp_file.name\n        try:\n            result = subprocess.run(\n                [sys.executable, temp_file_path], capture_output=True, text=True\n            )\n            return result.stdout\n        except Exception as e:\n            return f\"Error executing command-line tool: {str(e)}\"\n        finally:\n            os.remove(temp_file_path)\n\n    elif project_type == \"Streamlit App\":\n        with tempfile.NamedTemporaryFile(mode=\"w+\", delete=False, suffix=\".py\") as temp_file:\n            temp_file.write(code)\n            temp_file_path = temp_file.name\n        try:\n            subprocess.Popen(\n                [sys.executable, \"-m\", \"streamlit\", \"run\", temp_file_path]\n            )\n            return \"Streamlit app launched in a new window.\"\n        except Exception as e:\n            return f\"Error launching Streamlit app: {str(e)}\"\n        finally:\n            os.remove(temp_file_path)\n\n    elif project_type == \"API\":\n        return \"API execution not yet implemented.\"\n\n    elif project_type == \"Data Analysis Script\":\n        with tempfile.NamedTemporaryFile(mode=\"w+\", delete=False, suffix=\".py\") as temp_file:\n            temp_file.write(code)\n            temp_file_path = temp_file.name\n        try:\n            result = subprocess.run(\n                [sys.executable, temp_file_path], capture_output=True, text=True\n            )\n            return result.stdout\n        except Exception as e:\n            return f\"Error executing data analysis script: {str(e)}\"\n        finally:\n            os.remove(temp_file_path)\n\n    else:\n        return \"Unsupported project type for execution.\"\n\ndef build_project(user_request: str, repo_contents: dict, project_type: str) -> Dict[str, Any]:\n    \"\"\"\n    Implements the core build logic without running tests.\n\n    Args:\n        user_request (str): The user's project request.\n        repo_contents (dict): The contents of the repository.\n        project_type (str): The type of the project.\n\n    Returns:\n        dict: A dictionary containing the build results.\n    \"\"\"\n    try:\n        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        project_dir = Path(\"generated_projects\") / f\"project_{timestamp}\"\n        project_dir.mkdir(parents=True, exist_ok=True)\n\n        return {\"success\": True, \"project_dir\": project_dir}\n    except Exception as e:\n        return {\"success\": False, \"error\": str(e)}\n\ndef build_interface() -> None:\n    \"\"\"The main interface for the build system.\"\"\"\n    os.environ[\"USER_AGENT\"] = \"BuildAgent/1.0\"\n\n    st.title(\"🔨 Build: Autonomous Multi-Agent Software Development System\")\n\n    if \"project_state\" not in st.session_state:\n        st.session_state.project_state = {\n            \"status\": \"Not Started\",\n            \"current_step\": \"\",\n            \"iterations\": 0,\n            \"max_iterations\": 3,\n            \"code\": \"\",\n            \"documentation\": \"\",\n            \"test_results\": {},\n            \"quality_review\": \"\",\n            \"project_dir\": \"\",\n            \"errors\": [],\n            \"warnings\": [],\n            \"agent_logs\": [],\n            \"progress\": 0,\n            \"previous_tasks\": [],\n        }\n\n    if \"settings\" not in st.session_state:\n        st.session_state.settings = load_settings()\n\n    if \"api_keys\" not in st.session_state:\n        st.session_state.api_keys = load_api_keys()\n\n    all_models = get_all_models()\n    available_groq_models = get_available_groq_models(st.session_state.api_keys)\n    all_models = [\n        model for model in all_models if model not in GROQ_MODELS or model in available_groq_models\n    ]\n\n    progress_bar = st.progress(0)\n    status_text = st.empty()\n\n    # Sidebar\n    with st.sidebar:\n        with st.expander(\"🤖 Model Selection\"):\n            def get_valid_model(model_key, default_index=0):\n                saved_model = st.session_state.settings.get(model_key)\n                try:\n                    index = all_models.index(saved_model)\n                except ValueError:\n                    index = default_index\n                return st.selectbox(\n                    f\"{model_key.replace('_', ' ').title()}\", all_models, index=index\n                )\n\n            # Manager settings\n            st.session_state.settings[\"manager_model\"] = get_valid_model(\"manager_model\")\n            st.session_state.settings[\"manager_temperature\"] = st.slider(\n                \"Manager Temperature\",\n                min_value=0.0,\n                max_value=1.0,\n                value=float(st.session_state.settings.get(\"manager_temperature\", 0.2)),\n                step=0.1,\n            )\n            st.session_state.settings[\"manager_max_tokens\"] = st.slider(\n                \"Manager Max Tokens\",\n                min_value=1000,\n                max_value=128000,\n                value=int(st.session_state.settings.get(\"manager_max_tokens\", 8000)),\n                step=1000,\n            )\n\n            # Subagent settings\n            st.session_state.settings[\"subagent_model\"] = get_valid_model(\"subagent_model\")\n            st.session_state.settings[\"subagent_temperature\"] = st.slider(\n                \"Subagent Temperature\",\n                min_value=0.0,\n                max_value=1.0,\n                value=float(st.session_state.settings.get(\"subagent_temperature\", 0.2)),\n                step=0.1,\n            )\n            st.session_state.settings[\"subagent_max_tokens\"] = st.slider(\n                \"Subagent Max Tokens\",\n                min_value=1000,\n                max_value=128000,\n                value=int(st.session_state.settings.get(\"subagent_max_tokens\", 8000)),\n                step=1000,\n            )\n\n            # Refiner settings\n            st.session_state.settings[\"refiner_model\"] = get_valid_model(\"refiner_model\")\n            st.session_state.settings[\"refiner_temperature\"] = st.slider(\n                \"Refiner Temperature\",\n                min_value=0.0,\n                max_value=1.0,\n                value=float(st.session_state.settings.get(\"refiner_temperature\", 0.2)),\n                step=0.1,\n            )\n            st.session_state.settings[\"refiner_max_tokens\"] = st.slider(\n                \"Refiner Max Tokens\",\n                min_value=1000,\n                max_value=128000,\n                value=int(st.session_state.settings.get(\"refiner_max_tokens\", 8000)),\n                step=1000,\n            )\n\n        if st.button(\"💾 Save Settings\"):\n            save_settings(st.session_state.settings)\n            st.success(\"Settings saved successfully!\")\n\n    input_method = st.radio(\n        \"Choose input method:\", [\"Enter Project Request\", \"Provide Repository Directory\"]\n    )\n\n    if input_method == \"Enter Project Request\":\n        user_request = st.text_area(\n            \"Enter your project request:\", height=100, key=\"user_request\"\n        )\n        project_type = st.selectbox(\n            \"Select Project Type\",\n            [\"Command-line Tool\", \"Streamlit App\", \"API\", \"Data Analysis Script\"],\n            key=\"project_type\",\n        )\n        repo_contents = None\n    else:\n        repo_dir = st.text_input(\"Enter the path to your repository directory:\")\n        if repo_dir and os.path.isdir(repo_dir):\n            repo_contents = dump_repository(repo_dir)\n            st.success(\n                f\"Repository loaded successfully. Found {len(repo_contents)} files.\"\n            )\n        else:\n            repo_contents = None\n            if repo_dir:\n                st.error(\"Invalid directory path. Please enter a valid directory.\")\n        user_request = None\n        project_type = None\n\n    use_search = st.checkbox(\"Use search functionality\")\n    if use_search:\n        search_method = st.selectbox(\n            \"Select search method\", [\"duckduckgo\", \"google\", \"serpapi\"]\n        )\n\n        if search_method == \"google\":\n            st.session_state.api_keys[\"google_api_key\"] = st.text_input(\n                \"Google API Key\",\n                value=st.session_state.api_keys.get(\"google_api_key\", \"\"),\n                type=\"password\",\n            )\n            st.session_state.api_keys[\"google_cse_id\"] = st.text_input(\n                \"Google Custom Search Engine ID\",\n                value=st.session_state.api_keys.get(\"google_cse_id\", \"\"),\n                type=\"password\",\n            )\n        elif search_method == \"serpapi\":\n            st.session_state.api_keys[\"serpapi_api_key\"] = st.text_input(\n                \"SerpApi API Key\",\n                value=st.session_state.api_keys.get(\"serpapi_api_key\", \"\"),\n                type=\"password\",\n            )\n\n        num_results = st.number_input(\n            \"Number of search results\", min_value=1, max_value=20, value=5\n        )\n\n        save_api_keys(st.session_state.api_keys)\n    else:\n        search_method = None\n        num_results = None\n\n    (tab1, tab2, tab3) = st.tabs(\n        [\"Workflow\", \"Documentation\", \"Test Results\"]\n    )\n\n    if st.button(\"🚀 Build Project\"):\n        if user_request or repo_contents:\n            st.info(f\"Starting new project build\")\n            if user_request:\n                st.info(f\"User Request: {user_request}\")\n                st.info(f\"Project Type: {project_type}\")\n            else:\n                st.info(f\"Repository loaded with {len(repo_contents)} files\")\n\n            st.session_state.project_state[\"status\"] = \"In Progress\"\n            st.session_state.project_state[\"agent_logs\"] = []\n            st.session_state.project_state[\"progress\"] = 0\n            st.session_state.project_state[\"iterations\"] = 0\n            st.session_state.project_state[\"errors\"].clear()\n\n            # Create project directory\n            sanitized_objective = re.sub(r\"\\W+\", \"_\", user_request if user_request else \"repository_improvement\")\n            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n            project_dir = Path(\"generated_projects\") / f\"{sanitized_objective}_{timestamp}\"\n            project_dir.mkdir(parents=True, exist_ok=True)\n            st.success(f\"Created project folder: {project_dir}\")\n\n            search_results = None\n            if use_search:\n                search_query = user_request or \"Repository improvement techniques\"\n                search_results = perform_search(\n                    search_query, search_method, st.session_state.api_keys, num_results\n                )\n                st.json(search_results)\n\n            task_exchanges = []\n            worker_tasks = []\n            refined_output = \"\"\n\n            while (\n                st.session_state.project_state[\"iterations\"] < st.session_state.project_state[\"max_iterations\"]\n            ):\n                manager_response, _ = manager_agent_task(\n                    create_agent_context(\n                        st.session_state.project_state,\n                        user_request or \"Analyze and improve the provided repository\",\n                    ),\n                    model=st.session_state.settings[\"manager_model\"],\n                    temperature=float(st.session_state.settings[\"manager_temperature\"]),\n                    max_tokens=int(st.session_state.settings[\"manager_max_tokens\"]),\n                    openai_api_key=st.session_state.api_keys.get(\"openai_api_key\"),\n                )\n\n                st.subheader(f\"Manager Response - Iteration {st.session_state.project_state['iterations'] + 1}\")\n                st.json(manager_response)\n\n                if manager_response.get(\"create_files\", False):\n                    st.subheader(\"Creating Repository Files\")\n                    created_files = create_repository_files(project_dir, refined_output)\n                    st.success(f\"Created {len(created_files)} repository files\")\n                    for filename, content in created_files.items():\n                        st.text(f\"Created file: {filename}\")\n                        st.code(content, language=\"python\")\n                else:\n                    sub_agent_response = coding_agent_task(\n                        manager_response[\"instructions\"],\n                        model=st.session_state.settings[\"subagent_model\"],\n                        previous_tasks=worker_tasks,\n                        use_search=use_search,\n                        temperature=float(st.session_state.settings[\"subagent_temperature\"]),\n                        max_tokens=int(st.session_state.settings[\"subagent_max_tokens\"]),\n                        search_results=search_results,\n                        openai_api_key=st.session_state.api_keys.get(\"openai_api_key\"),\n                    )\n\n                    st.subheader(f\"Sub-agent Response - Iteration {st.session_state.project_state['iterations'] + 1}\")\n                    st.json(sub_agent_response)\n\n                    worker_tasks.append({\"task\": manager_response[\"instructions\"], \"result\": sub_agent_response})\n                    task_exchanges.append((manager_response[\"instructions\"], sub_agent_response))\n\n                st.session_state.project_state[\"iterations\"] += 1\n                st.session_state.project_state[\"progress\"] = min(\n                    st.session_state.project_state[\"iterations\"]\n                    / st.session_state.project_state[\"max_iterations\"],\n                    1.0\n                )\n                progress_bar.progress(st.session_state.project_state[\"progress\"])\n                status_text.text(\n                    f\"Iteration {st.session_state.project_state['iterations']} of {st.session_state.project_state['max_iterations']}\"\n                )\n\n            # Refiner task\n            st.subheader(\"Refiner Task\")\n            refined_output = refine_task(\n                user_request or \"Analyze and improve the provided repository\",\n                model=st.session_state.settings[\"refiner_model\"],\n                sub_task_results=[json.dumps(result) if isinstance(result, dict) else str(result) for _, result in task_exchanges],\n                filename=timestamp,\n                projectname=sanitized_objective,\n                temperature=float(st.session_state.settings[\"refiner_temperature\"]),\n                max_tokens=int(st.session_state.settings[\"refiner_max_tokens\"]),\n                openai_api_key=st.session_state.api_keys.get(\"openai_api_key\"),\n            )\n\n            if isinstance(refined_output, tuple):\n                refined_output = refined_output[0]\n\n            st.text_area(\"Refined Output:\", refined_output, height=400)\n\n            project_name_match = re.search(r\"Project Name: (.*)\", refined_output)\n            project_name = (\n                project_name_match.group(1).strip()\n                if project_name_match\n                else sanitized_objective\n            )\n\n            folder_structure = parse_folder_structure(refined_output)\n            code_blocks = extract_code_blocks(refined_output)\n\n            project_dir = Path(\"generated_projects\") / f\"{project_name}_{timestamp}\"\n            project_dir.mkdir(parents=True, exist_ok=True)\n            st.success(f\"Created project folder: {project_dir}\")\n\n            # Create folder structure\n            if folder_structure:\n                create_folder_structure(project_dir, folder_structure)\n                st.success(\"Created folder structure\")\n\n            # Create code files\n            if code_blocks:\n                for filename, code in code_blocks.items():\n                    file_path = project_dir / filename\n                    file_path.parent.mkdir(parents=True, exist_ok=True)\n                    file_path.write_text(code, encoding=\"utf-8\")\n                    st.success(f\"Created file: {file_path}\")\n\n            # Generate README.md\n            readme_content = generate_readme(\n                project_name,\n                user_request,\n                project_type,\n                refined_output,\n                st.session_state.settings[\"refiner_model\"],\n            )\n\n            # Ensure readme_content is a string\n            if isinstance(readme_content, tuple):\n                readme_content = readme_content[0]\n\n            if not isinstance(readme_content, str):\n                readme_content = str(readme_content)\n\n            readme_path = project_dir / \"README.md\"\n            readme_path.write_text(readme_content, encoding=\"utf-8\")\n            st.success(f\"Created README.md: {readme_path}\")\n\n            st.session_state.project_state[\"documentation\"] = readme_content\n\n            # Option to run tests separately\n            if st.button(\"🔍 Run Tests\"):\n                test_results = run_tests(project_dir)\n                st.session_state.project_state[\"test_results\"] = test_results\n\n                if (\n                    test_results.get(\"failed\", 0) > 0\n                    or test_results.get(\"errors\", 0) > 0\n                ):\n                    st.warning(\"Tests failed or errors encountered. Review the results and consider additional iterations.\")\n                else:\n                    st.success(\"All tests passed successfully!\")\n\n            st.session_state.project_state[\"status\"] = \"Completed\"\n            st.session_state.project_state[\"progress\"] = 1.0\n            progress_bar.progress(1.0)\n            st.success(\"Project build completed successfully!\")\n\n            # Display list of generated files\n            st.subheader(\"Generated Files\")\n            for root, dirs, files in os.walk(project_dir):\n                for file in files:\n                    file_path = Path(root) / file\n                    relative_path = file_path.relative_to(project_dir)\n                    st.markdown(f\"[{relative_path}]({file_path})\")\n\n        else:\n            st.error(\"Please enter a valid project request or repository directory.\")\n\n    with tab1:\n        st.subheader(\"Workflow\")\n        for i, (task, response) in enumerate(st.session_state.project_state.get(\"agent_logs\", [])):\n            st.subheader(f\"Iteration {i+1}\")\n            st.text_area(f\"Task {i+1}\", task, height=100)\n            st.text_area(f\"Response {i+1}\", json.dumps(response, indent=2), height=200)\n\n    with tab2:\n        st.subheader(\"Documentation\")\n        st.text_area(\n            \"Generated Documentation:\", st.session_state.project_state[\"documentation\"], height=400\n        )\n\n    with tab3:\n        st.subheader(\"Test Results\")\n        st.text_area(\n            \"Test Results:\",\n            st.session_state.project_state[\"test_results\"].get(\"pytest_output\", \"\"),\n            height=400,\n        )\n\n    # Display any errors\n    if st.session_state.project_state[\"errors\"]:\n        st.header(\"Errors\")\n        for error in st.session_state.project_state[\"errors\"]:\n            st.error(error)\n\n# Add this function to create the folder structure\ndef create_folder_structure(base_path: Path, structure: dict) -> None:\n    for name, content in structure.items():\n        path = base_path / name\n        if content is None:\n            path.touch()\n        elif isinstance(content, dict):\n            path.mkdir(parents=True, exist_ok=True)\n            create_folder_structure(path, content)\n\ndef main():\n    try:\n        build_interface()\n    except Exception as e:\n        console.print(f\"[bold red]An unexpected error occurred: {str(e)}[/bold red]\")\n        raise e\n\nif __name__ == \"__main__\":\n    main()\n"}
{"type": "source_file", "path": "brainstorm.py", "content": "# brainstorm.py\nimport os\nimport openai\nimport json\nimport streamlit as st\nimport subprocess\ntry:\n    from autogen import ConversableAgent, UserProxyAgent, GroupChat, GroupChatManager\nexcept ImportError:\n    print(\"Warning: autogen package not found, using fallback implementation\")\n    # Fallback implementation for autogen\n    class ConversableAgent:\n        def __init__(self, name, llm_config=None, human_input_mode=None, **kwargs):\n            self.name = name\n            self.llm_config = llm_config\n            print(f\"Warning: Using fallback ConversableAgent with name: {name}\")\n            \n        def generate_reply(self, messages, sender, config=None):\n            print(f\"Warning: Using fallback generate_reply for {self.name}\")\n            return f\"This is a fallback response from {self.name}. The autogen package is not installed.\"\n    \n    class UserProxyAgent(ConversableAgent):\n        def __init__(self, name, human_input_mode=None, code_execution_config=None, **kwargs):\n            super().__init__(name=name, **kwargs)\n            self.human_input_mode = human_input_mode\n            self.code_execution_config = code_execution_config\n    \n    class GroupChat:\n        def __init__(self, agents, messages=None, speaker_selection_method=None):\n            self.agents = agents\n            self.messages = messages or []\n            self.speaker_selection_method = speaker_selection_method\n    \n    class GroupChatManager:\n        def __init__(self, groupchat):\n            self.groupchat = groupchat\ntry:\n    from autogen.agentchat.contrib.capabilities.teachability import Teachability\nexcept ImportError:\n    print(\"Warning: autogen.agentchat.contrib.capabilities.teachability package not found, using fallback implementation\")\n    # Fallback implementation for Teachability\n    class Teachability:\n        def __init__(self, path_to_db_dir=None):\n            self.path_to_db_dir = path_to_db_dir\n            print(f\"Warning: Using fallback Teachability with path: {path_to_db_dir}\")\n            \n        def add_to_agent(self, agent):\n            print(f\"Warning: Using fallback add_to_agent for {agent.name}\")\n            pass\nfrom ollama_utils import get_available_models, get_all_models\nimport markdown\nfrom prompts import get_agent_prompt, get_metacognitive_prompt, get_voice_prompt, get_identity_prompt\nfrom info_brainstorm import display_info_brainstorm\nfrom chat_interface import chat_interface\nfrom ollama_utils import load_api_keys\nfrom groq_utils import GROQ_MODELS\nfrom openai_utils import OPENAI_MODELS\n\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\nSETTINGS_FILE = \"brainstorm_agents_settings.json\"\nWORKFLOWS_DIR = \"brainstorm_workflows\"\nAPI_KEYS_FILE = \"api_keys.json\"\n\n# List of animal emojis\nANIMAL_EMOJIS = [\n    \"🐶\", \"🐱\", \"🐭\", \"🐹\", \"🐰\", \"🦊\", \"🐻\", \"🐼\", \"🐨\", \"🐯\", \"🦁\", \"🐮\", \"🐷\", \"🐸\", \"🐵\", \"🐔\", \"🐧\", \"🐦\", \"🐤\",\n    \"🦆\", \"🦅\", \"🦉\", \"🦇\", \"🐺\", \"🐗\", \"🐴\", \"🦄\", \"🐝\", \"🐛\", \"🦋\", \"🐌\", \"🐞\", \"🐜\", \"🦟\", \"🦗\", \"🕷\", \"🦂\", \"🐢\",\n    \"🐍\", \"🦎\", \"🦖\", \"🦕\", \"🐙\", \"🦑\", \"🦐\", \"🦞\", \"🦀\", \"🐡\", \"🐠\", \"🐟\", \"🐬\", \"🐳\", \"🐋\", \"🦈\", \"🐊\", \"🐅\", \"🐆\",\n    \"🦓\", \"🦍\", \"🦧\", \"🐘\", \"🦛\", \"🦏\", \"🐪\", \"🐫\", \"🦒\", \"🦘\", \"🐃\", \"🐂\", \"🐄\", \"🐎\", \"🐖\", \"🐏\", \"🐑\", \"🦙\", \"🐐\",\n    \"🦌\", \"🐕\", \"🐩\", \"🦮\", \"🐕‍🦺\", \"🐈\", \"🐈‍⬛\", \"🐓\", \"🦃\", \"🦚\", \"🦜\", \"🦢\", \"🦩\", \"🕊\", \"🐇\", \"🦝\", \"🦨\", \"🦡\", \"🦦\",\n    \"🦥\", \"🐁\", \"🐀\", \"🐿\", \"🦔\", \"🐾\", \"🐉\", \"🐲\", \"🤖\", \"🧚\"\n]\n\ntry:\n    from files_management import get_corpus_options\nexcept ImportError:\n    def get_corpus_options(): return []\n\nclass CustomConversableAgent(ConversableAgent):\n    def __init__(self, name, llm_config, agent_type, identity, metacognitive_type, voice_type, corpus, temperature, max_tokens, presence_penalty, frequency_penalty, db_path, *args, **kwargs):\n        if 'api_key' in llm_config:\n            os.environ[\"OPENAI_API_KEY\"] = llm_config['api_key']\n        \n        super().__init__(name=name, llm_config=llm_config, *args, **kwargs)\n        self.agent_type = agent_type\n        self.identity = identity\n        self.metacognitive_type = metacognitive_type\n        self.voice_type = voice_type\n        self.corpus = corpus\n        self.temperature = temperature\n        self.max_tokens = max_tokens\n        self.presence_penalty = presence_penalty\n        self.frequency_penalty = frequency_penalty\n        self.teachability = Teachability(path_to_db_dir=db_path)\n        self.teachability.add_to_agent(self)\n\n    def generate_reply(self, messages, sender, config):\n        context = \"\\n\".join([f\"{m['role']} ({m.get('name', 'Unknown')}): {m['content']}\" for m in messages])\n        prompt = f\"\"\"\n        You are a {self.agent_type} agent named {self.name}.\n        Identity: {self.identity}\n        Metacognitive Type: {self.metacognitive_type}\n        Voice Type: {self.voice_type}\n        Corpus: {self.corpus}\n\n        Full conversation context:\n        {context}\n\n        Your task is to respond to the latest message, considering the full context above. Remember your role and respond accordingly:\n        \"\"\"\n        return super().generate_reply(messages=[{\"role\": \"user\", \"content\": prompt}], sender=sender, config=config)\n\ndef brainstorm_session():\n    st.header(\"💡 Brainstorming Session\")\n    \n    st.write(\"Welcome to the brainstorming session! Here you can discuss ideas, share thoughts, and collaborate in real-time.\")\n    \n    chat_interface()\n\ndef create_agent(settings):\n    api_keys = load_api_keys()  # Load API keys\n    print(f\"Creating agent with model: {settings['model']}\")\n    print(f\"API keys loaded: {api_keys}\")\n    \n    if settings['model'] in OPENAI_MODELS:\n        print(\"Using OpenAI model\")\n        llm_config = {\n            \"request_timeout\": 120,\n            \"api_key\": api_keys.get(\"openai_api_key\"),\n            \"model\": settings['model'],\n        }\n        # Set the OpenAI API key in the environment variable\n        os.environ[\"OPENAI_API_KEY\"] = api_keys.get(\"openai_api_key\", \"\")\n    elif settings['model'] in GROQ_MODELS:\n        print(\"Using Groq model\")\n        llm_config = {\n            \"request_timeout\": 120,\n            \"api_key\": api_keys.get(\"groq_api_key\"),\n            \"model\": settings['model'],\n        }\n    else:\n        print(\"Using Ollama model\")\n        llm_config = {\n            \"request_timeout\": 120,\n            \"api_base\": \"http://localhost:11434/v1\",\n            \"api_type\": \"open_ai\",\n            \"model\": settings[\"model\"],\n        }\n    \n    print(f\"LLM config: {llm_config}\")\n    \n    return CustomConversableAgent(\n        name=f\"{settings['emoji']} {settings['name']}\",\n        llm_config=llm_config,\n        agent_type=settings[\"agent_type\"],\n        identity=settings[\"identity\"],\n        metacognitive_type=settings[\"metacognitive_type\"],\n        voice_type=settings[\"voice_type\"],\n        corpus=settings[\"corpus\"],\n        temperature=settings[\"temperature\"],\n        max_tokens=settings[\"max_tokens\"],\n        presence_penalty=settings[\"presence_penalty\"],\n        frequency_penalty=settings[\"frequency_penalty\"],\n        db_path=settings[\"db_path\"]\n    )\n    \ndef load_agent_settings():\n    if os.path.exists(SETTINGS_FILE):\n        with open(SETTINGS_FILE, 'r') as f:\n            return json.load(f)\n    return {\"agents\": []}\n\ndef save_agent_settings(settings):\n    with open(SETTINGS_FILE, 'w') as f:\n        json.dump(settings, f, indent=2)\n\ndef get_available_workflows():\n    if not os.path.exists(WORKFLOWS_DIR):\n        os.makedirs(WORKFLOWS_DIR)\n    return [f[:-5] for f in os.listdir(WORKFLOWS_DIR) if f.endswith('.json')]\n\ndef save_workflow(workflow_name, agent_sequence):\n    if not os.path.exists(WORKFLOWS_DIR):\n        os.makedirs(WORKFLOWS_DIR)\n    workflow_path = os.path.join(WORKFLOWS_DIR, f\"{workflow_name}.json\")\n    with open(workflow_path, 'w') as f:\n        json.dump(agent_sequence, f, indent=2)\n    st.success(f\"Workflow '{workflow_name}' saved successfully!\")\n    print(f\"Saved workflow: {workflow_name}\")\n    print(f\"Agent sequence: {agent_sequence}\")\n\ndef load_workflow(workflow_name):\n    workflow_path = os.path.join(WORKFLOWS_DIR, f\"{workflow_name}.json\")\n    if os.path.exists(workflow_path):\n        with open(workflow_path, 'r') as f:\n            agent_sequence = json.load(f)\n        print(f\"Loaded workflow: {workflow_name}\")\n        print(f\"Agent sequence: {agent_sequence}\")\n        return agent_sequence\n    return None\n\ndef edit_agent_settings(agent_settings):\n    st.subheader(f\"Edit Agent: {agent_settings['name']}\")\n    \n    col1, col2, col3 = st.columns(3)\n    \n    with col1:\n        agent_settings['name'] = st.text_input(\"Agent Name\", agent_settings['name'], key=f\"{agent_settings['name']}_name\")\n        agent_settings['emoji'] = st.selectbox(\"Emoji\", ANIMAL_EMOJIS, index=ANIMAL_EMOJIS.index(agent_settings.get('emoji', '🐶')), key=f\"{agent_settings['name']}_emoji\")\n        agent_settings['model'] = st.selectbox(\"Model\", get_all_models(), \n                                               index=get_all_models().index(agent_settings['model']) if agent_settings['model'] in get_all_models() else 0,\n                                               key=f\"{agent_settings['name']}_model\")\n        voice_options = [\"None\"] + list(get_voice_prompt().keys())\n        agent_settings['voice_type'] = st.selectbox(\n            \"Voice Type\",\n            voice_options,\n            index=voice_options.index(agent_settings['voice_type']) if agent_settings['voice_type'] in voice_options else 0,\n            key=f\"{agent_settings['name']}_voice_type\"\n        )\n    \n    with col2:\n        agent_type_options = [\"None\"] + list(get_agent_prompt().keys())\n        agent_settings['agent_type'] = st.selectbox(\n            \"Agent Type\",\n            agent_type_options,\n            index=agent_type_options.index(agent_settings['agent_type']) if agent_settings['agent_type'] in agent_type_options else 0,\n            key=f\"{agent_settings['name']}_agent_type\"\n        )\n        identity_options = [\"None\"] + list(get_identity_prompt().keys())\n        agent_settings['identity'] = st.selectbox(\n            \"Identity\",\n            identity_options,\n            index=identity_options.index(agent_settings['identity']) if agent_settings['identity'] in identity_options else 0,\n            key=f\"{agent_settings['name']}_identity\"\n        )\n        metacognitive_options = [\"None\"] + list(get_metacognitive_prompt().keys())\n        agent_settings['metacognitive_type'] = st.selectbox(\n            \"Metacognitive Type\",\n            metacognitive_options,\n            index=metacognitive_options.index(agent_settings['metacognitive_type']) if agent_settings['metacognitive_type'] in metacognitive_options else 0,\n            key=f\"{agent_settings['name']}_metacognitive_type\"\n        )\n        corpus_options = [\"None\"] + get_corpus_options()\n        agent_settings['corpus'] = st.selectbox(\n            \"Corpus\",\n            corpus_options,\n            index=corpus_options.index(agent_settings['corpus']) if agent_settings['corpus'] in corpus_options else 0,\n            key=f\"{agent_settings['name']}_corpus\"\n        )\n    \n    with col3:\n        agent_settings['temperature'] = st.slider(\"Temperature\", 0.0, 1.0, agent_settings['temperature'], key=f\"{agent_settings['name']}_temperature\")\n        agent_settings['max_tokens'] = st.slider(\"Max Tokens\", min_value=1000, max_value=128000, value=agent_settings['max_tokens'], step=1000, key=f\"{agent_settings['name']}_max_tokens\")\n        agent_settings['presence_penalty'] = st.slider(\"Presence Penalty\", -2.0, 2.0, agent_settings['presence_penalty'], key=f\"{agent_settings['name']}_presence_penalty\")\n        agent_settings['frequency_penalty'] = st.slider(\"Frequency Penalty\", -2.0, 2.0, agent_settings['frequency_penalty'], key=f\"{agent_settings['name']}_frequency_penalty\")\n        agent_settings['db_path'] = st.text_input(\"Database Path\", agent_settings['db_path'], key=f\"{agent_settings['name']}_db_path\")\n\n    return agent_settings\n\ndef manage_agents():\n    st.subheader(\"Manage Agents\")\n    settings = load_agent_settings()\n    \n    for i, agent in enumerate(settings[\"agents\"]):\n        with st.expander(f\"{agent.get('emoji', '🐶')} {agent['name']}\"):\n            updated_agent = edit_agent_settings(agent)\n            settings[\"agents\"][i] = updated_agent\n            if st.button(f\"Remove {agent['name']}\", key=f\"remove_{agent['name']}\"):\n                settings[\"agents\"].pop(i)\n                save_agent_settings(settings)\n                st.success(f\"Agent {agent['name']} removed.\")\n                st.rerun()\n    \n    with st.expander(\"➕ Add New Agent\"):\n        new_agent = {\n            \"name\": \"\",\n            \"emoji\": \"🐶\",\n            \"model\": get_all_models()[0],\n            \"agent_type\": \"None\",\n            \"identity\": \"None\",\n            \"metacognitive_type\": \"None\",\n            \"voice_type\": \"None\",\n            \"corpus\": \"None\",\n            \"temperature\": 0.7,\n            \"max_tokens\": 4000,\n            \"presence_penalty\": 0.0,\n            \"frequency_penalty\": 0.0,\n            \"db_path\": \"./tmp/new_agent_db\"\n        }\n        \n        new_agent = edit_agent_settings(new_agent)\n        \n        if st.button(\"Add Agent\"):\n            if new_agent[\"name\"]:\n                new_agent['db_path'] = f\"./tmp/{new_agent['name'].lower().replace(' ', '_')}_db\"\n                settings[\"agents\"].append(new_agent)\n                save_agent_settings(settings)\n                st.success(f\"Agent {new_agent['name']} added successfully!\")\n                st.rerun()\n            else:\n                st.error(\"Please provide a name for the new agent.\")\n    \n    save_agent_settings(settings)\n\ndef brainstorm_session(use_docker):\n    settings = load_agent_settings()\n    api_keys = load_api_keys()\n    os.environ[\"OPENAI_API_KEY\"] = api_keys.get(\"openai_api_key\", \"\")\n    openai.api_key = api_keys.get(\"openai_api_key\", \"\")\n    print(f\"OpenAI API Key: {os.environ.get('OPENAI_API_KEY')}\")\n    agents = [create_agent(agent_settings) for agent_settings in settings[\"agents\"]]\n\n    if 'group_chat' not in st.session_state:\n        user = UserProxyAgent(\n            \"👨‍💼 User\",\n            human_input_mode=\"NEVER\",\n            code_execution_config={\"use_docker\": use_docker}\n        )\n        st.session_state.group_chat = GroupChat(\n            agents=[*agents, user],\n            messages=[],\n            speaker_selection_method=\"manual\"\n        )\n        st.session_state.group_chat_manager = GroupChatManager(groupchat=st.session_state.group_chat)\n\n    # Workflow management\n    available_workflows = get_available_workflows()\n    col1, col2, col3 = st.columns([10, 1, 10], vertical_alignment=\"bottom\")\n    with col1:\n        workflow_name = st.text_input(\"Add a New Workflow\")\n    with col2:\n        st.write(\"OR\")\n    with col3:\n        selected_workflow = st.selectbox(\"Load an Existing Workflow\", [\"\"] + available_workflows)\n\n    if selected_workflow and selected_workflow != st.session_state.get('last_loaded_workflow'):\n        agent_sequence = load_workflow(selected_workflow)\n        if agent_sequence:\n            st.session_state.agent_sequence = agent_sequence\n            st.session_state.last_loaded_workflow = selected_workflow\n            st.success(f\"Workflow '{selected_workflow}' loaded successfully!\")\n            st.rerun()\n\n    # Agent sequence setup\n    st.subheader(\"🦊 Agent Response Sequence\")\n    if 'agent_sequence' not in st.session_state:\n        st.session_state.agent_sequence = []\n\n    agent_names = [f\"{agent.name}\" for agent in agents]\n    agent_names.insert(0, \"\")  # Add empty option at the beginning\n\n    # Number of agents in the workflow\n    num_agents = len(st.session_state.agent_sequence)\n    col1, col2 = st.columns([1, 5])\n    with col1:\n        st.write(\"Number of Agents:\")\n    with col2:\n        num_agents = st.number_input(\n            \"Number of Agents in Workflow\",\n            min_value=0,\n            value=num_agents,\n            step=1,\n            label_visibility=\"collapsed\"\n        )\n\n    # Display agent selection dropdowns\n    for i in range(num_agents):\n        current_agent = st.session_state.agent_sequence[i] if i < len(st.session_state.agent_sequence) else \"\"\n        agent = st.selectbox(\n            f\"Agent {i+1}\",\n            agent_names,\n            index=agent_names.index(current_agent) if current_agent in agent_names else 0,\n            key=f\"agent_{i}\"\n        )\n        if i < len(st.session_state.agent_sequence):\n            st.session_state.agent_sequence[i] = agent\n        else:\n            st.session_state.agent_sequence.append(agent)\n\n    if st.button(\"💾 Save Workflow\"):\n        if workflow_name:\n            current_sequence = [agent for agent in st.session_state.agent_sequence if agent]\n            save_workflow(workflow_name, current_sequence)\n        else:\n            st.error(\"Please enter a workflow name before saving.\")\n\n    # User input\n    st.subheader(\"🧑‍⚕️ User Input\")\n    user_message = st.text_input(\"Enter your question or topic for the Brainstorm session:\")\n\n    if st.button(\"👥 Send Input to Agents\"):\n        if user_message and any(st.session_state.agent_sequence):\n            # Add user message to the group chat\n            st.session_state.group_chat.messages.append({\"role\": \"user\", \"name\": \"User\", \"content\": user_message})\n\n            # Generate responses from the sequence of agents\n            for agent_name in st.session_state.agent_sequence:\n                if agent_name:  # Skip empty selections\n                    with st.spinner(f\"{agent_name} is thinking...\"):\n                        selected_agent_obj = next((agent for agent in agents if agent.name == agent_name), None)\n                        if selected_agent_obj:\n                            response = selected_agent_obj.generate_reply(st.session_state.group_chat.messages, sender=st.session_state.group_chat_manager, config=None)\n                            # Add agent's response to the group chat\n                            st.session_state.group_chat.messages.append({\"role\": \"assistant\", \"name\": agent_name, \"content\": response})\n                        else:\n                            st.error(f\"Agent '{agent_name}' not found.\")\n\n    # Display conversation history with formatting\n    st.subheader(\"📜 Conversation History\")\n    for message in st.session_state.group_chat.messages:\n        with st.chat_message(message['role']):\n            st.markdown(f\"**{message['name']}**\")\n            formatted_content = markdown.markdown(message['content'])\n            st.markdown(formatted_content, unsafe_allow_html=True)\n\n    # Display info about Brainstorm feature\n    display_info_brainstorm()\n\ndef brainstorm_interface():\n    st.title(\"🧠 Brainstorm\")\n\n    # Docker usage option\n    use_docker = st.checkbox(\"Use Docker for code execution\", value=False)\n\n    if use_docker:\n        # Check if Docker is installed and running\n        if not is_docker_running():\n            st.warning(\"🟠 Docker is not running. Please start Docker or install it if not already installed.\")\n            st.info(\"\"\"\n            ❓ To install Docker:\n            1. Visit https://www.docker.com/products/docker-desktop\n            2. Download and install Docker Desktop for your operating system\n            3. After installation, start Docker Desktop\n            4. Once Docker is running, refresh this page\n            \"\"\")\n            st.stop()\n    else:\n        os.environ[\"AUTOGEN_USE_DOCKER\"] = \"0\"\n\n    tab1, tab2 = st.tabs([\"💡 Brainstorm Session\", \"🦊 Manage Agents\"])\n\n    with tab1:\n        brainstorm_session(use_docker)\n\n    with tab2:\n        manage_agents()\n\ndef is_docker_running():\n    try:\n        subprocess.run([\"docker\", \"info\"], check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n        return True\n    except subprocess.CalledProcessError:\n        return False\n    except FileNotFoundError:\n        return False\n\nif __name__ == \"__main__\":\n    brainstorm_interface()"}
{"type": "source_file", "path": "contextual_response.py", "content": "# contextual_response.py\nimport streamlit as st\nimport pandas as pd\nimport time\nfrom ollama_utils import get_available_models, call_ollama_endpoint, check_json_handling, check_function_calling\n\ndef contextual_response_test():\n    st.header(\"💬 Contextual Response Test by Model\")\n\n    available_models = get_available_models()\n\n    if \"selected_model\" not in st.session_state:\n        st.session_state.selected_model = available_models[0] if available_models else None\n\n    selectbox_key = \"contextual_test_model_selector\"\n\n    if selectbox_key in st.session_state:\n        st.session_state.selected_model = st.session_state[selectbox_key]\n\n    selected_model = st.selectbox(\n        \"Select the model you want to test:\", \n        available_models, \n        key=selectbox_key,\n        index=available_models.index(st.session_state.selected_model) if st.session_state.selected_model in available_models else 0\n    )\n\n    prompts = st.text_area(\"Enter the prompts (one per line):\", value=\"Hi, how are you?\\nWhat's your name?\\nTell me a joke.\")\n\n    col1, col2, col3, col4 = st.columns(4)\n    with col1:\n        temperature = st.slider(\"Temperature\", min_value=0.0, max_value=1.0, value=0.5, step=0.1)\n    with col2:\n        max_tokens = st.slider(\"Max Tokens\", min_value=100, max_value=32000, value=4000, step=100)\n    with col3:\n        presence_penalty = st.slider(\"Presence Penalty\", min_value=-2.0, max_value=2.0, value=0.0, step=0.1)\n    with col4:\n        frequency_penalty = st.slider(\"Frequency Penalty\", min_value=-2.0, max_value=2.0, value=0.0, step=0.1)\n\n    if st.button(\"Start Contextual Test\", key=\"start_contextual_test\"):\n        prompt_list = [p.strip() for p in prompts.split(\"\\n\")]\n        context = []\n        times = []\n        tokens_per_second_list = []\n        for i, prompt in enumerate(prompt_list):\n            start_time = time.time()\n            result, context, eval_count, eval_duration = call_ollama_endpoint(\n                selected_model,\n                prompt=prompt,\n                temperature=temperature,\n                max_tokens=max_tokens,\n                presence_penalty=presence_penalty,\n                frequency_penalty=frequency_penalty,\n                context=context,\n            )\n            end_time = time.time()\n            elapsed_time = end_time - start_time\n            times.append(elapsed_time)\n            tokens_per_second = eval_count / (eval_duration / (10**9)) if eval_count and eval_duration else 0\n            tokens_per_second_list.append(tokens_per_second)\n            st.subheader(f\"Prompt {i+1}: {prompt} (Time taken: {elapsed_time:.2f} seconds, Tokens/second: {tokens_per_second:.2f}):\")\n            st.write(f\"Response: {result}\")\n\n        data = {\"Prompt\": prompt_list, \"Time (seconds)\": times, \"Tokens/second\": tokens_per_second_list}\n        df = pd.DataFrame(data)\n\n        st.bar_chart(df, x=\"Prompt\", y=[\"Time (seconds)\", \"Tokens/second\"], color=[\"#4CAF50\", \"#FFC107\"])\n\n        st.write(\"📦 JSON Handling Capability: \", \"✅\" if check_json_handling(selected_model, temperature, max_tokens, presence_penalty, frequency_penalty) else \"❌\")\n        st.write(\"⚙️ Function Calling Capability: \", \"✅\" if check_function_calling(selected_model, temperature, max_tokens, presence_penalty, frequency_penalty) else \"❌\")\n"}
{"type": "source_file", "path": "chroma_client.py", "content": "# chroma_client.py\n\nimport os\nimport chromadb\nfrom chromadb.config import Settings\n\ndef get_chroma_client(corpus_name):\n    corpus_path = os.path.join(\"corpus\", corpus_name)\n    if not os.path.exists(corpus_path):\n        os.makedirs(corpus_path)\n\n    # Configure the Chroma client with the new API structure\n    settings = Settings(\n        chroma_api_impl=\"rest\",  # Can also be \"sqlite\" or \"duckdb+parquet\" based on your setup\n        chroma_server_host=\"localhost\",  # Replace with your Chroma server host if remote\n        chroma_server_port=\"8000\",  # Replace with the appropriate port\n        persist_directory=corpus_path  # Directory to store the persistent data\n    )\n    return chromadb.Client(settings=settings)\n\ndef sanitize_collection_name(name):\n    sanitized_name = name.replace(\" \", \"_\")\n    sanitized_name = ''.join(c for c in sanitized_name if c.isalnum() or c in ['_', '-'])\n    return sanitized_name\n"}
{"type": "source_file", "path": "db_init.py", "content": "\"\"\"\nDatabase initialization script for Ollama Workbench.\nCreates and populates the necessary database tables.\n\"\"\"\nimport sqlite3\nimport json\nimport os\n\ndef init_db():\n    \"\"\"Initialize the database with required tables.\"\"\"\n    conn = sqlite3.connect('ollama_models.db')\n    cursor = conn.cursor()\n\n    # Create models table if it doesn't exist\n    cursor.execute('''\n    CREATE TABLE IF NOT EXISTS models (\n        model_name TEXT PRIMARY KEY,\n        description TEXT,\n        capabilities TEXT,\n        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n    )\n    ''')\n\n    # Default model descriptions\n    default_models = {\n        \"llama2\": {\n            \"description\": \"Meta's Llama 2 model, optimized for chat and general text generation\",\n            \"capabilities\": \"General text generation, chat, code completion, reasoning\"\n        },\n        \"mistral\": {\n            \"description\": \"Mistral AI's powerful language model\",\n            \"capabilities\": \"Chat, text generation, analysis, code completion\"\n        },\n        \"codellama\": {\n            \"description\": \"Specialized variant of Llama 2 focused on code generation\",\n            \"capabilities\": \"Code completion, code explanation, debugging, technical documentation\"\n        },\n        \"neural-chat\": {\n            \"description\": \"Intel's optimized chat model\",\n            \"capabilities\": \"Conversational AI, text generation, task assistance\"\n        },\n        \"starling-lm\": {\n            \"description\": \"Berkeley's instruction-tuned language model\",\n            \"capabilities\": \"Chat, instruction following, reasoning, analysis\"\n        },\n        \"dolphin-phi\": {\n            \"description\": \"Microsoft's Phi-2 model fine-tuned for chat\",\n            \"capabilities\": \"Chat, reasoning, code generation, task completion\"\n        }\n    }\n\n    # Insert default models\n    for model_name, info in default_models.items():\n        cursor.execute('''\n        INSERT OR REPLACE INTO models (model_name, description, capabilities)\n        VALUES (?, ?, ?)\n        ''', (model_name, info[\"description\"], info[\"capabilities\"]))\n\n    conn.commit()\n    conn.close()\n\nif __name__ == \"__main__\":\n    init_db()\n"}
{"type": "source_file", "path": "check_dependencies.py", "content": "#!/usr/bin/env python3\n\"\"\"\nDependency checker for Ollama Workbench.\nThis script scans Python files for imports and checks if they're installed.\n\"\"\"\n\nimport os\nimport sys\nimport re\nimport importlib.util\nimport subprocess\nfrom collections import defaultdict\n\n# ANSI colors for terminal output\nGREEN = '\\033[0;32m'\nYELLOW = '\\033[1;33m'\nRED = '\\033[0;31m'\nNC = '\\033[0m'  # No Color\n\ndef print_colored(color, message):\n    \"\"\"Print a colored message.\"\"\"\n    print(f\"{color}{message}{NC}\")\n\ndef find_python_files(directory):\n    \"\"\"Find all Python files in the directory.\"\"\"\n    python_files = []\n    for root, _, files in os.walk(directory):\n        for file in files:\n            if file.endswith('.py'):\n                python_files.append(os.path.join(root, file))\n    return python_files\n\ndef extract_imports(file_path):\n    \"\"\"Extract import statements from a Python file.\"\"\"\n    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n        content = f.read()\n    \n    # Regular expressions to match different import patterns\n    import_patterns = [\n        r'^import\\s+([\\w\\.]+)',  # import module\n        r'^from\\s+([\\w\\.]+)\\s+import',  # from module import ...\n        r'^\\s+import\\s+([\\w\\.]+)',  # indented import module\n        r'^\\s+from\\s+([\\w\\.]+)\\s+import',  # indented from module import ...\n    ]\n    \n    imports = set()\n    for pattern in import_patterns:\n        matches = re.findall(pattern, content, re.MULTILINE)\n        imports.update(matches)\n    \n    # Extract the top-level module name from each import\n    top_level_modules = set()\n    for imp in imports:\n        top_level = imp.split('.')[0]\n        if top_level not in ['__future__', 'os', 'sys', 'io', 're', 'json', 'time', 'datetime', 'math', 'random', 'collections', 'itertools', 'functools', 'typing', 'pathlib', 'subprocess', 'tempfile', 'shutil', 'glob', 'argparse', 'logging', 'threading', 'multiprocessing', 'concurrent', 'urllib', 'http', 'socket', 'ssl', 'email', 'smtplib', 'ftplib', 'telnetlib', 'uuid', 'hashlib', 'hmac', 'base64', 'pickle', 'csv', 'xml', 'html', 'unittest', 'doctest', 'pdb', 'traceback', 'warnings', 'contextlib', 'abc', 'ast', 'inspect', 'importlib', 'zipfile', 'tarfile', 'gzip', 'bz2', 'lzma', 'zlib', 'struct', 'codecs', 'unicodedata', 'stringprep', 'readline', 'rlcompleter', 'stat', 'filecmp', 'fnmatch', 'linecache', 'tokenize', 'tabnanny', 'pyclbr', 'py_compile', 'compileall', 'dis', 'pickletools', 'distutils', 'ensurepip', 'venv', 'zipapp', 'platform', 'errno', 'ctypes', 'site', 'code', 'codeop', 'timeit', 'trace', 'tracemalloc', 'gc', 'inspect', 'site', 'user', 'builtins', 'copy', 'pprint', 'reprlib', 'enum', 'numbers', 'cmath', 'decimal', 'fractions', 'statistics', 'array', 'dataclasses', 'heapq', 'bisect', 'weakref', 'types', 'copy_reg', 'copyreg', 'operator', 'reprlib', 'keyword', 'parser', 'symbol', 'token', 'keyword', 'tokenize', 'tabnanny', 'pyclbr', 'py_compile', 'compileall', 'dis', 'pickletools', 'formatter', 'msilib', 'msvcrt', 'winreg', 'winsound', 'posix', 'pwd', 'spwd', 'grp', 'crypt', 'termios', 'tty', 'pty', 'fcntl', 'pipes', 'resource', 'nis', 'syslog', 'optparse', 'imp']:\n            top_level_modules.add(top_level)\n    \n    return top_level_modules\n\ndef check_package_installed(package_name):\n    \"\"\"Check if a package is installed.\"\"\"\n    spec = importlib.util.find_spec(package_name)\n    return spec is not None\n\ndef get_package_version(package_name):\n    \"\"\"Get the installed version of a package.\"\"\"\n    try:\n        result = subprocess.run(\n            [sys.executable, \"-m\", \"pip\", \"show\", package_name],\n            capture_output=True,\n            text=True,\n            check=False\n        )\n        if result.returncode == 0:\n            for line in result.stdout.splitlines():\n                if line.startswith(\"Version:\"):\n                    return line.split(\":\", 1)[1].strip()\n        return None\n    except Exception:\n        return None\n\ndef map_import_to_package(import_name):\n    \"\"\"Map import name to package name.\"\"\"\n    # Common mappings where import name differs from package name\n    mappings = {\n        'PIL': 'pillow',\n        'bs4': 'beautifulsoup4',\n        'sklearn': 'scikit-learn',\n        'cv2': 'opencv-python',\n        'streamlit_option_menu': 'streamlit-option-menu',\n        'streamlit_extras': 'streamlit-extras',\n        'streamlit_flow': 'streamlit-flow',\n        'streamlit_javascript': 'streamlit-javascript',\n        'PyPDF2': 'PyPDF2',  # Ensure case-sensitive mapping\n        'openai': 'openai',  # Ensure openai is mapped correctly\n        'langchain_community': 'langchain-community',  # Handle underscore to hyphen conversion\n        'pdfkit': 'pdfkit',  # Ensure pdfkit is mapped correctly\n        'requests': 'requests',  # Ensure requests is mapped correctly\n        'selenium': 'selenium',  # Ensure selenium is mapped correctly\n        'webdriver_manager': 'webdriver-manager',  # Handle underscore to hyphen conversion\n    }\n    return mappings.get(import_name, import_name)\n\ndef suggest_installation_command(package_name):\n    \"\"\"Suggest installation command for a package.\"\"\"\n    return f\"pip install {package_name}\"\n\ndef main():\n    \"\"\"Main function.\"\"\"\n    print_colored(GREEN, \"=== Ollama Workbench Dependency Checker ===\")\n    \n    # Find all Python files in the current directory\n    python_files = find_python_files('.')\n    print_colored(GREEN, f\"Found {len(python_files)} Python files\")\n    \n    # Extract imports from each file\n    file_imports = {}\n    all_imports = set()\n    for file_path in python_files:\n        imports = extract_imports(file_path)\n        file_imports[file_path] = imports\n        all_imports.update(imports)\n    \n    print_colored(GREEN, f\"Found {len(all_imports)} unique imports\")\n    \n    # Check if each import is installed\n    installed = []\n    missing = []\n    for import_name in sorted(all_imports):\n        package_name = map_import_to_package(import_name)\n        is_installed = check_package_installed(import_name)\n        version = get_package_version(package_name) if is_installed else None\n        \n        if is_installed:\n            installed.append((import_name, version))\n        else:\n            missing.append(import_name)\n    \n    # Print results\n    print_colored(GREEN, \"\\n=== Installed Packages ===\")\n    for import_name, version in installed:\n        version_str = f\"v{version}\" if version else \"unknown version\"\n        print(f\"✓ {import_name} ({version_str})\")\n    \n    print_colored(YELLOW if missing else GREEN, f\"\\n=== Missing Packages ({len(missing)}) ===\")\n    if missing:\n        for import_name in missing:\n            package_name = map_import_to_package(import_name)\n            print(f\"✗ {import_name}\")\n            print(f\"  Suggested: {suggest_installation_command(package_name)}\")\n    else:\n        print(\"No missing packages found!\")\n    \n    # Print imports by file\n    print_colored(GREEN, \"\\n=== Imports by File ===\")\n    for file_path, imports in file_imports.items():\n        if imports:\n            missing_in_file = [imp for imp in imports if imp in missing]\n            status = f\"({len(missing_in_file)} missing)\" if missing_in_file else \"(all installed)\"\n            print(f\"{file_path} {status}\")\n            if missing_in_file:\n                for imp in missing_in_file:\n                    print(f\"  ✗ {imp}\")\n    \n    # Print summary\n    print_colored(GREEN, \"\\n=== Summary ===\")\n    print(f\"Total imports: {len(all_imports)}\")\n    print(f\"Installed: {len(installed)}\")\n    print(f\"Missing: {len(missing)}\")\n    \n    if missing:\n        print_colored(YELLOW, \"\\n=== Installation Commands ===\")\n        print(\"Run the following commands to install all missing packages:\")\n        for import_name in missing:\n            package_name = map_import_to_package(import_name)\n            print(suggest_installation_command(package_name))\n    \n    return len(missing) == 0\n\nif __name__ == \"__main__\":\n    success = main()\n    sys.exit(0 if success else 1)"}
{"type": "source_file", "path": "build_manager.py", "content": "# build_manager.py\nimport json\nimport os\nimport queue\nimport csv\nimport threading\nimport logging\nfrom agents import Agent\nfrom ollama_utils import call_ollama_endpoint\nfrom projects import Task, load_tasks\nimport ollama\nfrom concurrent.futures import ThreadPoolExecutor\nimport re\nfrom functools import reduce\nimport streamlit as st\nfrom datetime import datetime, timedelta\n\n# Set up logging\nlogging.basicConfig(filename='build_manager.log', level=logging.ERROR,\n                    format='%(asctime)s - %(levelname)s - %(message)s')\n\nclass BuildManager(Agent):\n    def __init__(self, message_queue: queue.Queue, update_task_callback, agents_dir: str = \"agents\", workflows_dir: str = \"workflows\"):\n        super().__init__(name=\"Build Manager\", capabilities=[\"workflow_management\"], prompts={}, model=\"mistral:7b-instruct-v0.2-q8_0\")\n        self.message_queue = message_queue\n        self.agents_dir = agents_dir\n        self.workflows_dir = workflows_dir\n        self.update_task_callback = update_task_callback\n        self.cancel_flag = False  # Flag for cancellation\n\n        # Create the agents and workflows directories if they don't exist\n        os.makedirs(self.agents_dir, exist_ok=True)\n        os.makedirs(self.workflows_dir, exist_ok=True)\n\n        try:\n            self.load_agents()\n            self.load_workflows()\n        except Exception as e:\n            print(f\"Critical error during initialization: {e}\")\n            logging.error(f\"Critical error during initialization: {e}\")\n            self.cancel_workflow()\n            self.update_task_callback(0, \"Failed\", f\"Critical Error: {e}\")\n            return\n\n    def load_agents(self):\n        self.agents = {}\n        for filename in os.listdir(self.agents_dir):\n            if filename.endswith(\".json\"):\n                filepath = os.path.join(self.agents_dir, filename)\n                try:\n                    agent = Agent.from_json(filepath)\n                    self.agents[agent.name] = agent\n                except Exception as e:\n                    print(f\"Error loading agent from {filepath}: {e}\")\n                    logging.error(f\"Error loading agent from {filepath}: {e}\")\n                    raise\n\n    def load_workflows(self):\n        self.workflows = {}\n        for filename in os.listdir(self.workflows_dir):\n            if filename.endswith(\".json\"):\n                filepath = os.path.join(self.workflows_dir, filename)\n                with open(filepath, 'r') as f:\n                    try:\n                        workflow_data = json.load(f)\n                        self.workflows[workflow_data[\"name\"]] = workflow_data\n                    except Exception as e:\n                        print(f\"Error loading workflow from {filepath}: {e}\")\n                        logging.error(f\"Error loading workflow from {filepath}: {e}\")\n                        raise\n\n    def process_request(self, user_request: str):\n        try:\n            # 1. Interpret User Request (Using an LLM for classification)\n            classification_prompt = f\"\"\"\n            Classify the following user request into one of these categories:\n            - data_analysis\n            - code_generation\n            - content_creation\n            - other\n\n            User Request: {user_request}\n\n            Classification: \n            \"\"\"\n            response = ollama.generate(model=\"mistral:7b-instruct-v0.2-q8_0\", prompt=classification_prompt)\n            task_category = response['response'].strip().lower()\n\n            if task_category == \"data_analysis\":\n                self.execute_workflow(\"Data Analysis Workflow\", user_request)\n            elif task_category == \"code_generation\":\n                self.execute_workflow(\"Code Generation Workflow\", user_request)\n            # ... handle other categories ...\n            else:\n                print(f\"Error: Unable to classify user request: {user_request}\")\n                logging.error(f\"Error: Unable to classify user request: {user_request}\")\n                self.cancel_workflow()\n                self.update_task_callback(0, \"Failed\", f\"Error: Unable to classify user request\")\n\n        except Exception as e:\n            print(f\"Critical error in process_request: {e}\")\n            logging.error(f\"Critical error in process_request: {e}\")\n            self.cancel_workflow()\n            self.update_task_callback(0, \"Failed\", f\"Critical Error: {e}\")\n\n    def execute_workflow(self, workflow_name: str, user_request: str, project_name=\"My Project\"):\n        workflow = self.workflows.get(workflow_name)\n        if not workflow:\n            print(f\"Error: Workflow '{workflow_name}' not found.\")\n            return\n\n        # Create initial task data from user request\n        task_data = {\"user_request\": user_request}\n\n        # Create agents and assign them to tasks\n        for step_index, step in enumerate(workflow[\"steps\"]):\n            agent_name = step[\"agent\"]\n\n            # Check if agent already exists\n            agent = self.agents.get(agent_name)\n            if not agent:\n                # Create a new agent\n                agent = Agent(name=agent_name, capabilities=[], prompts={}, model=\"mistral:7b-instruct-v0.2-q8_0\")\n                self.agents[agent_name] = agent\n\n            # Handle user input before creating the Task object\n            user_input_config = step.get(\"user_input\")\n            if user_input_config:\n                input_type = user_input_config[\"type\"]\n                prompt = user_input_config[\"prompt\"]\n\n                if input_type == \"file_path\":\n                    file_path = st.text_input(prompt, key=f\"user_input_{step_index}\")\n                    if file_path:\n                        task_data[\"file_path\"] = file_path  # Add file_path to task_data\n                    else:\n                        st.warning(\"Please provide a file path.\")\n                        return  # Stop workflow execution if no file path is provided\n\n                elif input_type == \"options\":\n                    options = user_input_config.get(\"options\", [])\n                    selected_option = st.selectbox(prompt, options, key=f\"user_input_{step_index}\")\n                    if selected_option:\n                        task_data[\"selected_option\"] = selected_option  # Add selected_option to task_data\n                    else:\n                        st.warning(\"Please select an option.\")\n                        return  # Stop workflow execution if no option is selected\n\n                elif input_type == \"confirmation\":\n                    if not st.button(prompt, key=f\"user_input_{step_index}\"):\n                        st.warning(\"Task skipped due to unconfirmed user input.\")\n                        continue  # Skip to the next step if not confirmed\n\n            # Prepare task description and inputs (now with user input in task_data)\n            task_description = step[\"task_description\"].format(**task_data)\n            task_inputs = {input_key: task_data.get(input_key) for input_key in step[\"inputs\"]}\n\n            # Create Task object\n            task = Task(name=agent.name, description=task_description, **task_inputs)\n\n            # Create a threading event for this task\n            task_event = threading.Event()\n            task_event.set()  # Start in running state\n\n            # Send task to agent\n            task_message = {\n                \"task\": task,\n                \"agent\": agent,\n                \"step_index\": step_index,\n                \"project_name\": project_name,\n                \"task_event\": task_event\n            }\n            self.message_queue.put(task_message)\n            print(f\"Task {task.name} added to message queue.\")\n\n        # Create initial task entries in session state\n        st.session_state.bm_tasks = [\n            {\"task_id\": task.task_id, \"name\": step[\"agent\"], \"status\": \"Pending\", \"result\": None, \"project_name\": workflow_name}\n            for step, task in zip(workflow[\"steps\"], self.message_queue.queue)\n        ]\n\n        # Get workflow start time\n        start_time = st.session_state.get(\"workflow_start_time\")\n        if not start_time:\n            start_time = datetime.now()\n            st.session_state.workflow_start_time = start_time\n\n    def run(self):\n        # Initialize session state for tasks\n        if \"bm_tasks\" not in st.session_state:\n            st.session_state.bm_tasks = []\n\n        # Process all tasks in the queue\n        while not self.message_queue.empty() and not self.cancel_flag:\n            task_message = self.message_queue.get()\n            print(f\"Task {task_message['task'].name} retrieved from message queue.\")\n            self.execute_task(task_message)\n\n        if not self.cancel_flag:\n            print(\"Workflow completed!\")\n            self.reset_workflow()\n\n    def execute_task(self, task_message):\n        \"\"\"Executes a task in the main thread.\"\"\"\n        agent = task_message[\"agent\"]\n        task = task_message[\"task\"]\n        step_index = task_message[\"step_index\"]\n        project_name = task_message[\"project_name\"]\n        task_event = task_message[\"task_event\"]\n\n        # Task cancellation flag\n        task_canceled = False\n\n        try:\n            # Update task status to \"In Progress\"\n            self.update_task_callback(step_index, \"In Progress\", None)\n\n            # Generate prompt using the task description\n            prompt_template = agent.prompts.get(\"generate_code\")\n            if prompt_template:\n                prompt = prompt_template.format(task_description=task.description)\n            else:\n                prompt = task.description\n\n            # Execute the task using the agent's model\n            response, _, _, _ = call_ollama_endpoint(agent.model, prompt=prompt)\n\n            # Process agent response and update Task object\n            if agent.name == \"Data Extractor\":\n                try:\n                    # Assuming the response is a CSV string\n                    reader = csv.reader(response.splitlines())\n                    extracted_data = list(reader)\n                    task.result = extracted_data\n                    task.completed = True\n                except Exception as e:\n                    print(f\"Error extracting data: {e}\")\n                    task.result = f\"Error: {e}\"\n                    task.completed = False\n\n            elif agent.name == \"Data Analyst\":\n                try:\n                    analysis_results = json.loads(response)\n                    task.result = analysis_results\n                    task.completed = True\n                except json.JSONDecodeError:\n                    print(f\"Error: Invalid JSON response from Data Analyst: {response}\")\n                    task.result = f\"Error: Invalid JSON response\"\n                    task.completed = False\n\n            elif agent.name == \"Report Writer\":\n                task.result = response\n                task.completed = True\n\n            # Wait for task event (for pause/resume)\n            task_event.wait()\n\n            # Check if task was canceled\n            if task_canceled:\n                print(f\"Task {task.name} canceled.\")\n                self.update_task_callback(step_index, \"Canceled\", None)\n                return\n\n            # Update task status to \"Completed\"\n            self.update_task_callback(step_index, \"Completed\", task.result)\n\n        except Exception as e:\n            print(f\"Error executing task: {e}\")\n            logging.error(f\"Error executing task: {task.name} - {e}\", exc_info=True)\n            self.update_task_callback(step_index, \"Failed\", f\"Error: {e}\")\n\n        # Handle pause/resume/cancel commands\n        command = st.session_state.get(\"command\")\n        task_name = st.session_state.get(\"task_name\")\n        if task_name and task.name == task_name:\n            if command == \"pauseTask\":\n                task_event.clear()  # Pause the task\n                self.update_task_callback(step_index, \"Paused\", None)\n            elif command == \"resumeTask\":\n                task_event.set()  # Resume the task\n                self.update_task_callback(step_index, \"In Progress\", None)\n            elif command == \"cancelTask\":\n                task_canceled = True  # Set the cancellation flag\n                task_event.set()  # Release the task if it's waiting\n\n    def cancel_workflow(self):\n        self.cancel_flag = True\n        # Clear the message queue\n        while not self.message_queue.empty():\n            self.message_queue.get()\n\n        # Reset parallel task counts\n        self.parallel_task_counts = {}\n\n    def reset_workflow(self):\n        \"\"\"Resets the workflow state.\"\"\"\n        self.cancel_flag = False\n        # Clear the message queue\n        while not self.message_queue.empty():\n            self.message_queue.get()\n\n        # Reset parallel task counts\n        self.parallel_task_counts = {}\n\n        # Reset session state\n        st.session_state.bm_tasks = []\n        st.session_state.workflow_start_time = None\n\n    def cancel_task(self, task_id: str):\n        \"\"\"\n        Cancels a task and all its dependent tasks.\n\n        Args:\n            task_id: The ID of the task to cancel.\n        \"\"\"\n        # Find the task index in the session state\n        task_index = next((i for i, t in enumerate(st.session_state.bm_tasks) if t[\"task_id\"] == task_id), None)\n        if task_index is None:\n            print(f\"Error: Task with ID {task_id} not found.\")\n            return\n\n        # Cancel the task\n        self.cancel_task_by_index(task_index)\n\n        # Recursively cancel dependent tasks\n        workflow_name = st.session_state.bm_tasks[0].get(\"project_name\", \"Data Analysis Workflow\")  # Get workflow name from task data\n        workflow = self.workflows.get(workflow_name)\n        if workflow:\n            for i, step in enumerate(workflow[\"steps\"]):\n                if step.get(\"depends_on\") == task_index:\n                    self.cancel_task_by_index(i)\n\n    def cancel_task_by_index(self, task_index: int):\n        \"\"\"\n        Cancels a task by its index in the session state.\n\n        Args:\n            task_index: The index of the task to cancel.\n        \"\"\"\n        task = st.session_state.bm_tasks[task_index]\n        task[\"status\"] = \"Canceled\"\n\n        # Get the task_event from the task_message queue\n        for message in self.message_queue.queue:\n            if message[\"task\"].task_id == task[\"task_id\"]:\n                task_event = message.get(\"task_event\")\n                if task_event:\n                    task_event.set()  # Signal the task to stop\n                break\n\n        # Call the agent's cancel_task method\n        agent = self.agents.get(task[\"name\"])\n        if agent:\n            agent.cancel_task(task)\n\n    def pause_task(self, task_id: str):\n        \"\"\"Pauses a specific task.\"\"\"\n        task_index = next((i for i, t in enumerate(st.session_state.bm_tasks) if t[\"task_id\"] == task_id), None)\n        if task_index is not None:\n            for message in self.message_queue.queue:\n                if message[\"task\"].task_id == task_id:\n                    task_event = message.get(\"task_event\")\n                    if task_event:\n                        task_event.clear()  # Pause the task\n                        self.update_task_callback(task_index, \"Paused\", None)\n                        print(f\"Task {task_id} paused.\")\n                    break\n\n    def resume_task(self, task_id: str):\n        \"\"\"Resumes a specific task.\"\"\"\n        task_index = next((i for i, t in enumerate(st.session_state.bm_tasks) if t[\"task_id\"] == task_id), None)\n        if task_index is not None:\n            for message in self.message_queue.queue:\n                if message[\"task\"].task_id == task_id:\n                    task_event = message.get(\"task_event\")\n                    if task_event:\n                        task_event.set()  # Resume the task\n                        self.update_task_callback(task_index, \"In Progress\", None)\n                        print(f\"Task {task_id} resumed.\")\n                    break\n\n    def generate_workflow(self, user_request: str):\n        \"\"\"\n        Generates a workflow template based on a user request.\n\n        Args:\n            user_request: The user's request for the workflow.\n\n        Returns:\n            The generated workflow template as a JSON string, or None if generation or validation failed.\n        \"\"\"\n        # 1. Generate Workflow Template using LLM\n        generation_prompt = f\"\"\"\n        Generate a valid JSON workflow template to fulfill the following user request:\n\n        User Request: {user_request}\n\n        Available Agents:\n        - Data Extractor: Extracts data from CSV files.\n        - Data Analyst: Analyzes data and calculates statistics.\n        - Report Writer: Generates reports from analysis results.\n        - Data Visualizer: Creates visualizations from data.\n\n        Workflow Template Structure:\n        {{\n          \"name\": \"Workflow Name\",\n          \"description\": \"Workflow Description\",\n          \"steps\": [\n            {{\n              \"agent\": \"Agent Name\",\n              \"task_description\": \"Task Description\",\n              \"inputs\": [\"Input Variables\"],\n              \"outputs\": [\"Output Variables\"],\n              \"depends_on\": null or Step Index,\n              \"parallel_group\": null or Group Name,\n              \"user_input\": {{\n                \"type\": \"Input Type\",\n                \"prompt\": \"User Input Prompt\",\n                \"options\": [\"Option 1\", \"Option 2\"]  // If applicable\n              }},\n              \"condition\": \"Condition Expression\"  // Evaluate to True or False\n            }}\n          ]\n        }}\n\n        Example:\n        {{\n          \"name\": \"Data Analysis Workflow\",\n          \"description\": \"A simple workflow to extract data from a CSV file, analyze it, and generate a report.\",\n          \"steps\": [\n            {{\n              \"agent\": \"Data Extractor\",\n              \"task_description\": \"Extract data from the CSV file.\",\n              \"inputs\": [\"csv_file_path\"],\n              \"outputs\": [\"extracted_data\"],\n              \"depends_on\": null,\n              \"parallel_group\": null,\n              \"user_input\": {{\n                \"type\": \"file_path\",\n                \"prompt\": \"Please provide the path to the CSV file:\"\n              }},\n              \"condition\": null\n            }},\n            {{\n              \"agent\": \"Data Analyst\",\n              \"task_description\": \"Analyze the extracted data.\",\n              \"inputs\": [\"extracted_data\"],\n              \"outputs\": [\"analysis_results\"],\n              \"depends_on\": 0,\n              \"parallel_group\": null,\n              \"user_input\": null,\n              \"condition\": null\n            }},\n            {{\n              \"agent\": \"Report Writer\",\n              \"task_description\": \"Generate a report from the analysis results.\",\n              \"inputs\": [\"analysis_results\"],\n              \"outputs\": [\"report\"],\n              \"depends_on\": 1,\n              \"parallel_group\": null,\n              \"user_input\": null,\n              \"condition\": null\n            }}\n          ]\n        }}\n        \"\"\"\n        response = ollama.generate(model=\"mistral:7b-instruct-v0.2-q8_0\", prompt=generation_prompt)\n        generated_workflow = response['response']\n\n        # 2. Preprocess generated workflow (remove extra characters and whitespace)\n        generated_workflow = generated_workflow.strip()  # Remove leading/trailing whitespace\n        generated_workflow = generated_workflow.replace(\"'\", '\"')  # Replace single quotes with double quotes\n        generated_workflow = re.sub(r\"[^\\w\\s{}\\[\\]\\\":,.-]+\", \"\", generated_workflow)  # Remove invalid characters\n\n        # 3. Ensure Enclosing Braces\n        if not generated_workflow.startswith(\"{\"):\n            generated_workflow = \"{\" + generated_workflow\n        if not generated_workflow.endswith(\"}\"):\n            generated_workflow = generated_workflow + \"}\"\n\n        # 4. Ensure property names are enclosed in double quotes\n        generated_workflow = re.sub(r\"(\\s*)'(\\w+)'(\\s*):\", r'\\1\"\\2\"\\3:', generated_workflow)\n\n        # 5. Add Missing Commas (More Robust)\n        generated_workflow = re.sub(r\"([}\\]])(\\s*)(?=[{\\[\\\"a-zA-Z])\", r\"\\1,\\2\", generated_workflow)\n\n        # 6. Load JSON and Post-process to ensure 'condition' is an array and 'depends_on' is valid\n        try:\n            workflow_data = json.loads(generated_workflow)  # Load JSON here\n            num_steps = len(workflow_data[\"steps\"])\n            for step_index, step in enumerate(workflow_data[\"steps\"]):  # Include step_index in the loop\n                if \"condition\" in step and not isinstance(step[\"condition\"], list):\n                    if step[\"condition\"] is None:\n                        step[\"condition\"] = []\n                    else:\n                        # Create a condition object with \"expression\" and a default \"next_step\"\n                        step[\"condition\"] = [{\"expression\": step[\"condition\"], \"next_step\": step_index + 1}]\n\n                # Validate depends_on field\n                depends_on = step.get(\"depends_on\")\n                if depends_on is not None and (not isinstance(depends_on, int) or depends_on < 0 or depends_on >= num_steps):\n                    step[\"depends_on\"] = None  # Reset depends_on to None if invalid\n\n                # Validate condition field (ensure each condition has a valid next_step)\n                if \"condition\" in step and isinstance(step[\"condition\"], list) and len(step[\"condition\"]) > 0:  # Check if condition is a non-empty list\n                    for condition in step[\"condition\"]:\n                        if \"next_step\" not in condition or (not isinstance(condition[\"next_step\"], int) or condition[\"next_step\"] < 0 or condition[\"next_step\"] >= num_steps):\n                            condition[\"next_step\"] = step_index + 1  # Set a default next_step if invalid\n\n            generated_workflow = json.dumps(workflow_data)  # Update generated_workflow after post-processing\n        except json.JSONDecodeError as e:\n            print(f\"Error loading or post-processing JSON: {e}\")\n            return None\n\n        # 7. Validate Workflow Template\n        is_valid, error_message = self.validate_workflow_template(generated_workflow)\n        if is_valid:\n            print(\"Generated workflow template is valid.\")\n            return generated_workflow\n        else:\n            print(f\"Generated workflow template is invalid: {error_message}\")\n            return None\n\n    def validate_workflow_template(self, workflow_json: str) -> (bool, str):\n        \"\"\"\n        Validates a workflow template in JSON format.\n\n        Args:\n            workflow_json: The workflow template as a JSON string.\n\n        Returns:\n            A tuple containing:\n                - True if the template is valid, False otherwise.\n                - An error message if the template is invalid, an empty string otherwise.\n        \"\"\"\n        try:\n            workflow_data = json.loads(workflow_json)\n        except json.JSONDecodeError as e:\n            return False, f\"Invalid JSON: {e}\"\n\n        # Check for required fields\n        required_fields = [\"name\", \"description\", \"steps\"]\n        for field in required_fields:\n            if field not in workflow_data:\n                return False, f\"Missing required field: '{field}'\"\n\n        # Check steps array\n        steps = workflow_data[\"steps\"]\n        if not isinstance(steps, list):\n            return False, \"The 'steps' field must be an array.\"\n\n        # Check individual steps\n        for i, step in enumerate(steps):\n            if not isinstance(step, dict):\n                return False, f\"Step {i+1} is not a valid object.\"\n\n            # Check required step fields\n            required_step_fields = [\"agent\", \"task_description\", \"inputs\", \"outputs\"]\n            for field in required_step_fields:\n                if field not in step:\n                    return False, f\"Step {i+1} is missing required field: '{field}'\"\n\n            # Check data types of step fields\n            if not isinstance(step[\"agent\"], str):\n                return False, f\"Step {i+1}: 'agent' must be a string.\"\n            if not isinstance(step[\"task_description\"], str):\n                return False, f\"Step {i+1}: 'task_description' must be a string.\"\n            if not isinstance(step[\"inputs\"], list):\n                return False, f\"Step {i+1}: 'inputs' must be an array.\"\n            if not isinstance(step[\"outputs\"], list):\n                return False, f\"Step {i+1}: 'outputs' must be an array.\"\n\n            # Check depends_on field\n            depends_on = step.get(\"depends_on\")\n            if depends_on is not None and (not isinstance(depends_on, int) or depends_on < 0 or depends_on >= len(steps)):\n                return False, f\"Step {i+1}: 'depends_on' must be a valid step index.\"\n\n            # Check parallel_group field (optional)\n            parallel_group = step.get(\"parallel_group\")\n            if parallel_group is not None and not isinstance(parallel_group, str):\n                return False, f\"Step {i+1}: 'parallel_group' must be a string.\"\n\n            # Check user_input field (optional)\n            user_input = step.get(\"user_input\")\n            if user_input is not None:\n                if not isinstance(user_input, dict):\n                    return False, f\"Step {i+1}: 'user_input' must be an object.\"\n                if \"type\" not in user_input or not isinstance(user_input[\"type\"], str):\n                    return False, f\"Step {i+1}: 'user_input' must have a 'type' field (string).\"\n\n            # Check condition field (ensure it's an array)\n            conditions = step.get(\"condition\")\n            if conditions is not None and not isinstance(conditions, list):\n                return False, f\"Step {i+1}: 'condition' must be an array.\"\n\n            # Check individual conditions\n            if isinstance(conditions, list):\n                for j, cond in enumerate(conditions):\n                    if not isinstance(cond, dict):\n                        return False, f\"Step {i+1}, Condition {j+1}: Each condition must be an object.\"\n                    if \"expression\" not in cond or not isinstance(cond[\"expression\"], str):\n                        return False, f\"Step {i+1}, Condition {j+1}: Each condition must have an 'expression' field (string).\"\n                    if \"next_step\" not in cond or (not isinstance(cond[\"next_step\"], int) or cond[\"next_step\"] < 0 or cond[\"next_step\"] >= len(steps)):\n                        return False, f\"Step {i+1}, Condition {j+1}: Each condition must have a 'next_step' field (valid step index).\"\n\n        return True, \"\""}
{"type": "source_file", "path": "backup_files/chat_interface.py", "content": "# chat_interface.py\nimport streamlit as st\nimport os\nimport json\nfrom datetime import datetime\nimport re\nimport ollama\nfrom ollama_utils import get_available_models\nfrom prompts import get_agent_prompt, get_metacognitive_prompt, get_voice_prompt\nimport tiktoken\nfrom files_management import files_tab  # Import from files_management.py\n# Removed dependency on streamlit_extras.bottom_container\n\ndef chat_interface():\n    st.header(\"💬 Chat\")\n\n    # Initialize session state variables\n    if \"chat_history\" not in st.session_state:\n        st.session_state.chat_history = []\n    if \"workspace_items\" not in st.session_state:\n        st.session_state.workspace_items = []\n    if \"agent_type\" not in st.session_state:\n        st.session_state.agent_type = \"None\"\n    if \"metacognitive_type\" not in st.session_state:\n        st.session_state.metacognitive_type = \"None\"\n    if \"voice_type\" not in st.session_state:\n        st.session_state.voice_type = \"None\"\n    if \"selected_model\" not in st.session_state:\n        available_models = get_available_models()\n        st.session_state.selected_model = available_models[0] if available_models else None\n    if \"total_tokens\" not in st.session_state:\n        st.session_state.total_tokens = 0\n\n    # Create tabs for Chat, Workspace, and Files\n    chat_tab, workspace_tab, files_tab_ui = st.tabs([\"Chat\", \"Workspace\", \"Files\"])\n\n    with chat_tab:\n        # Settings (Collapsible, open by default)\n        with st.expander(\"⚙️ Settings\", expanded=True):\n            col1, col2, col3, col4, col5 = st.columns(5)\n            with col1:\n                available_models = get_available_models()\n                selected_model = st.selectbox(\n                    \"📦 Select a Model:\",\n                    available_models,\n                    key=\"selected_model\",\n                    index=available_models.index(st.session_state.selected_model) if st.session_state.selected_model in available_models else 0\n                )\n            with col2:\n                agent_types = [\"None\"] + list(get_agent_prompt().keys())\n                agent_type = st.selectbox(\"🧑‍🔧 Select Agent Type:\", agent_types, key=\"agent_type\")\n            with col3:\n                metacognitive_types = [\"None\"] + list(get_metacognitive_prompt().keys())\n                metacognitive_type = st.selectbox(\"🧠 Select Metacognitive Type:\", metacognitive_types, key=\"metacognitive_type\")\n            with col4:\n                voice_types = [\"None\"] + list(get_voice_prompt().keys())\n                voice_type = st.selectbox(\"🗣️ Select Voice Type:\", voice_types, key=\"voice_type\")\n            with col5:\n                corpus_folder = \"corpus\"\n                if not os.path.exists(corpus_folder):\n                    os.makedirs(corpus_folder)\n                corpus_options = [\"None\"] + [f for f in os.listdir(corpus_folder) if os.path.isdir(os.path.join(corpus_folder, f))]\n                selected_corpus = st.selectbox(\"📚 Select Corpus:\", corpus_options, key=\"selected_corpus\")\n\n        # Advanced Settings (Collapsible, collapsed by default)\n        with st.expander(\"🛠️ Advanced Settings\", expanded=False):\n            col1, col2, col3, col4 = st.columns(4)\n            with col1:\n                temperature = st.slider(\"🌡️ Temperature\", min_value=0.0, max_value=1.0, value=0.5, step=0.1, key=\"temperature_slider_chat\")\n            with col2:\n                max_tokens = st.slider(\"📊 Max Tokens\", min_value=100, max_value=32000, value=4000, step=100, key=\"max_tokens_slider_chat\")\n            with col3:\n                presence_penalty = st.slider(\"🚫 Presence Penalty\", min_value=-2.0, max_value=2.0, value=0.0, step=0.1, key=\"presence_penalty_slider_chat\")\n            with col4:\n                frequency_penalty = st.slider(\"🔁 Frequency Penalty\", min_value=-2.0, max_value=2.0, value=0.0, step=0.1, key=\"frequency_penalty_slider_chat\")\n\n        # Display total token count\n        st.write(f\"Total Token Count: {st.session_state.total_tokens}\")\n\n    # Display chat history\n    for message in st.session_state.chat_history:\n        with st.chat_message(message[\"role\"]):\n            if message[\"role\"] == \"assistant\":\n                code_blocks = extract_code_blocks(message[\"content\"])\n                for code_block in code_blocks:\n                    st.code(code_block)\n                non_code_parts = re.split(r'```[\\s\\S]*?```', message[\"content\"])\n                for part in non_code_parts:\n                    st.markdown(part.strip())\n            else:\n                st.markdown(message[\"content\"])\n\n    # Create placeholders for user input and assistant's response\n    user_input_placeholder = st.empty()\n    response_placeholder = st.empty()\n\n    # Chat input using standard Streamlit approach\n    prompt = st.chat_input(\"🧐 What is up my person❔\", key=\"chat_input\")\n\n    # Process the user input and generate response outside the bottom container\n    if prompt:\n        # Display user input\n        with user_input_placeholder.container():\n            with st.chat_message(\"user\"):\n                st.markdown(prompt)\n\n        st.session_state.chat_history.append({\"role\": \"user\", \"content\": prompt})\n\n        # Update token count for user input\n        st.session_state.total_tokens += count_tokens(prompt)\n\n        # Generate and display the assistant's response\n        full_response = \"\"\n\n        # Combine agent type, metacognitive type, and voice type prompts\n        combined_prompt = \"\"\n        if agent_type != \"None\":\n            combined_prompt += get_agent_prompt()[agent_type] + \"\\n\\n\"\n        if metacognitive_type != \"None\":\n            combined_prompt += get_metacognitive_prompt()[metacognitive_type] + \"\\n\\n\"\n        if voice_type != \"None\":\n            combined_prompt += get_voice_prompt()[voice_type] + \"\\n\\n\"\n\n        # Include chat history and corpus context\n        chat_history = \"\\n\".join([f\"{msg['role']}: {msg['content']}\" for msg in st.session_state.chat_history])\n        if selected_corpus != \"None\":\n            corpus_context = get_corpus_context_from_db(corpus_folder, selected_corpus, prompt)\n            final_prompt = f\"{combined_prompt}Conversation History:\\n{chat_history}\\n\\nContext: {corpus_context}\\n\\nUser: {prompt}\\n\\n{combined_prompt}\"\n        else:\n            final_prompt = f\"{combined_prompt}Conversation History:\\n{chat_history}\\n\\nUser: {prompt}\\n\\n{combined_prompt}\"\n\n        # Update token count for the entire prompt\n        st.session_state.total_tokens += count_tokens(final_prompt)\n\n        with response_placeholder.container():\n            with st.chat_message(\"assistant\"):\n                message_placeholder = st.empty()\n                for response_chunk in ollama.generate(st.session_state.selected_model, final_prompt, stream=True):\n                    full_response += response_chunk[\"response\"]\n                    message_placeholder.markdown(full_response + \"▌\")\n                    # Update token count for each chunk of the response\n                    st.session_state.total_tokens += count_tokens(response_chunk[\"response\"])\n                message_placeholder.markdown(full_response)\n\n        st.session_state.chat_history.append({\"role\": \"assistant\", \"content\": full_response})\n\n        # Update the total token count display\n        st.write(f\"Total Token Count: {st.session_state.total_tokens}\")\n\n        # Automatically detect and save code to workspace\n        code_blocks = extract_code_blocks(full_response)\n        for code_block in code_blocks:\n            st.session_state.workspace_items.append({\n                \"type\": \"code\",\n                \"content\": code_block,\n                \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n            })\n        if code_blocks:\n            st.success(f\"{len(code_blocks)} code block(s) automatically saved to Workspace\")\n\n    # Save chat and workspace button (outside the bottom container)\n    if st.button(\"📥 Save Chat and Workspace\"):\n        save_chat_and_workspace()\n\n    # Load/Rename/Delete chat and workspace\n    manage_saved_chats()\n\n    # Workspace tab\n    with workspace_tab:\n        st.subheader(\"📜 Workspace\")\n        \n        # Display workspace items\n        for index, item in enumerate(st.session_state.workspace_items):\n            with st.expander(f\"Item {index + 1} - {item['timestamp']}\"):\n                if item['type'] == 'code':\n                    st.code(item['content'])\n                else:\n                    st.write(item['content'])\n                if st.button(f\"Remove Item {index + 1}\"):\n                    st.session_state.workspace_items.pop(index)\n                    st.rerun()\n\n        # Option to add a new workspace item manually\n        new_item = st.text_area(\"Add a new item to the workspace:\", key=\"new_workspace_item\")\n        if st.button(\"Add to Workspace\"):\n            if new_item:\n                st.session_state.workspace_items.append({\n                    \"type\": \"text\",\n                    \"content\": new_item,\n                    \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n                })\n                st.success(\"New item added to Workspace\")\n                st.rerun()\n\n    # Files tab\n    with files_tab_ui:\n        files_tab()\n\ndef count_tokens(text):\n    encoding = tiktoken.get_encoding(\"cl100k_base\")\n    return len(encoding.encode(text))\n\ndef extract_code_blocks(text):\n    code_blocks = re.findall(r'```[\\s\\S]*?```', text)\n    return [block.strip('`').strip() for block in code_blocks]\n\ndef get_corpus_context_from_db(corpus_folder, corpus_name, query):\n    from langchain_community.embeddings import OllamaEmbeddings\n    from langchain_community.vectorstores import Chroma\n    corpus_path = os.path.join(corpus_folder, corpus_name)\n    embeddings = OllamaEmbeddings()\n    db = Chroma(persist_directory=corpus_path, embedding_function=embeddings)\n    results = db.similarity_search(query, k=3)\n    return \"\\n\".join([doc.page_content for doc in results])\n\ndef save_chat_and_workspace():\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    default_filename = f\"chat_and_workspace_{timestamp}\"\n    chat_name = st.text_input(\"Enter a name for the save:\", value=default_filename, key=\"save_chat_name\")\n    if chat_name:\n        save_data = {\n            \"chat_history\": st.session_state.chat_history,\n            \"workspace_items\": st.session_state.workspace_items,\n            \"total_tokens\": st.session_state.total_tokens\n        }\n        sessions_folder = \"sessions\"\n        if not os.path.exists(sessions_folder):\n            os.makedirs(sessions_folder)\n        file_path = os.path.join(sessions_folder, chat_name + \".json\")\n        with open(file_path, \"w\") as f:\n            json.dump(save_data, f)\n        st.success(f\"Chat and Workspace saved to {chat_name}\")\n\ndef manage_saved_chats():\n    st.sidebar.subheader(\"Saved Chats and Workspaces\")\n    sessions_folder = \"sessions\"\n    if not os.path.exists(sessions_folder):\n        os.makedirs(sessions_folder)\n    saved_files = [f for f in os.listdir(sessions_folder) if f.endswith(\".json\")]\n\n    if \"rename_file\" not in st.session_state:\n        st.session_state.rename_file = None\n\n    for file in saved_files:\n        col1, col2, col3 = st.sidebar.columns([3, 1, 1])\n        with col1:\n            file_name = os.path.splitext(file)[0]\n            if st.button(file_name):\n                load_chat_and_workspace(os.path.join(sessions_folder, file))\n        with col2:\n            if st.button(\"✏️\", key=f\"rename_{file}\"):\n                st.session_state.rename_file = file\n                st.rerun()\n        with col3:\n            if st.button(\"🗑️\", key=f\"delete_{file}\"):\n                delete_chat_and_workspace(os.path.join(sessions_folder, file))\n\n    if st.session_state.rename_file:\n        rename_chat_and_workspace(st.session_state.rename_file, sessions_folder)\n\ndef load_chat_and_workspace(file_path):\n    with open(file_path, \"r\") as f:\n        loaded_data = json.load(f)\n    st.session_state.chat_history = loaded_data.get(\"chat_history\", [])\n    st.session_state.workspace_items = loaded_data.get(\"workspace_items\", [])\n    st.session_state.total_tokens = loaded_data.get(\"total_tokens\", 0)\n    st.success(f\"Loaded {os.path.basename(file_path)}\")\n    st.rerun()\n\ndef delete_chat_and_workspace(file_path):\n    os.remove(file_path)\n    st.success(f\"File {os.path.basename(file_path)} deleted.\")\n    st.rerun()\n\ndef rename_chat_and_workspace(file_to_rename, sessions_folder):\n    current_name = os.path.splitext(file_to_rename)[0]\n    new_name = st.text_input(\"Rename file:\", value=current_name, key=\"rename_file_input\")\n    if new_name:\n        old_file_path = os.path.join(sessions_folder, file_to_rename)\n        new_file_path = os.path.join(sessions_folder, new_name + \".json\")\n        if new_file_path != old_file_path:\n            os.rename(old_file_path, new_file_path)\n            st.success(f\"File renamed to {new_name}\")\n            st.session_state.rename_file = None\n            st.cache_resource.clear()\n            st.rerun()"}
{"type": "source_file", "path": "loading_screen.py", "content": "import time\nimport os\nimport sys\nimport random\nimport threading\nimport subprocess\nimport cursor\n\ndef clear_screen():\n    os.system('cls' if os.name == 'nt' else 'clear')\n\ndef print_centered(text):\n    terminal_width = os.get_terminal_size().columns\n    print(text.center(terminal_width))\n\ndef loading_animation():\n    frames = [\n        r\"\"\"\n                                  .@#.                   \n            +=..                  -@@%..                 \n          .*@@.                  *+@%#= .                \n           *@#*+               .@:%@@** :                \n          .*@@@.=%.     .     .%.*@@@**.:                \n           =@@@@: -* .+#@-#%@=*-.@@@%*+.:                \n            *@@@@=.+#=:.     ++:*@@@**-..                \n            :%@@@@+.        .++-+@@%++.:                 \n             .#@@@+..      ::  :=@%+*.:.                 \n             . .@%#@+-+*=#+.     .-%#..                  \n              -%*=*%@@#=:          :#-.                  \n            .#@#*=---:   .+*#%%%#-  .*:                  \n             .=@@@*:    :=#@@@#@@+.  +* .                \n             .:=@@*      :%%%@@@#    .*..                \n                #@.        -*##+      =* .               \n                =.                 .  :@:.               \n               =@:-*#:            :    #= .              \n               =+@#.    ::      +-     =@:.              \n               *:+-   +--:::-+#+       .%+ .             \n                :@@@@+-@@@@@#+-         -% .             \n                  *@@#. @@@#+:   .      -@= .            \n                  :--:. =@#*+=.   .    :   :.            \n                        -@#++=::     .  :-.              \n                         @@*++==-    .-:                 \n                         :#*=-:.  :-.                    \n                         .::::::.                        \n         \"\"\",\n        r\"\"\"\n             .*##-                     -###.             \n             *+-:*                     *=.-*             \n            :+#@-.*:                 :*=-@*=:            \n            -+@@@* -+.              +=:*@@%--            \n            :+%@@@+ .+.  *%#*%@-  .*-:+@@@#=:            \n             **@@@@=  ++@@%%##*#*-#-:+@@@@+*             \n             :*#@@@@-.--#@@%###%- --+@@@@#*:             \n              :#+@@@%-.:-+%@@@@+.  .#@@@+#:              \n                =*+*-:....::::..    .++*=                \n                  *=:.               :=                  \n                 :**##+:....:...::=##++:                 \n                 =@@@@%#=--:   :-%@#%@%-                 \n                  .*%@@@#-::    *@@@#+.                  \n                  .+*@%+:::.     =%%==.                  \n                  -=---:*+#*-+*-: ...:-                  \n                  :*-=--::-*#=.   ...=:                  \n                   =+--:*-:-=. .-   -=                   \n                    =#=--=+%%*:..:-++                    \n                    =+=*#%**#*++=-.:-                    \n                   .**-=***#*+:.   =*                    \n                    -+--====-..  . :-                    \n                   .**====-:.....  =*                    \n                    =+===-::....   :=                    \n                    :--=----....  . .                    \n                         .: .                            \n\n         \"\"\",\n    ]\n    loading_text = [\n        \"Booting Up...\",\n        \"Initializing Flux Capacitor...\",\n        \"Loading AI Modules...\",\n        \"Warming Up GPUs...\",\n        \"Calibrating Quantum Sensors...\",\n        \"Generating Creative Sparks...\",\n        \"Connecting to the Matrix...\",\n        \"Downloading Latest Stuff...\",\n        \"First time?\",\n        \"Oh boy, this might take a while...\",\n        \"Getting the latest Ollama models...\",\n        \"Preparing for Awesomeness...\"\n    ]\n    dots = 0\n    max_dots = 12\n\n    while not installation_complete.is_set():\n        clear_screen()\n        print(\"\\n\" * 5)\n        print_centered(random.choice(frames))\n        print(\"\\n\" * 2)\n        print_centered(f\"{random.choice(loading_text)}{'.' * dots}\")\n        print(\"\\n\" * 5)\n        time.sleep(0.5)\n        dots = (dots + 1) % (max_dots + 1)\n\ndef run_installation(command):\n    # Hide the cursor during installation\n    cursor.hide()\n\n    # Run the command, capturing output to suppress it\n    subprocess.run(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    installation_complete.set()\n\n    # Show the cursor again after installation\n    cursor.show()\n\ndef run_installation_with_output(command):\n    # Run the command, showing output\n    subprocess.run(command, shell=True)\n    installation_complete.set()\n\ndef print_completion_message():\n    clear_screen()\n    print(\"\\n\" * 5)\n    print(\"Installation/Update Complete!\")\n    print(\"______________________________________________________\")\n\n    print(\"Built by 🟦🟦 2 Acre Studios\")\n    print(\"With thanks to:\")\n    print(\"Ollama\")\n    print(\"SerpApi\")\n    print(\"Google\")\n    print(\"Streamlit\")\n    print(\"LangChain\")\n    print(\"AutoGen\")\n    print(\"Chroma\")\n    print(\"Hugging Face\")\n    print(\"PyTorch\")\n    print(\"DuckDuckGo\")\n    print(\"Bing\")\n    print(\"And the countless developers and researchers\")\n    print(\"whose work made this possible.\")\n    print(\"______________________________________________________\")\n\n    print(\"Launching Ollama Workbench...\")\n    print(\"\\n\" * 5)\n    time.sleep(2)\n\nif __name__ == \"__main__\":\n    installation_complete = threading.Event()\n    \n    if len(sys.argv) > 1:\n        installation_command = \" \".join(sys.argv[1:])\n        \n        if \"--no-loading-screen\" in sys.argv:\n            # Run installation with output\n            run_installation_with_output(installation_command)\n        else:\n            # Run installation with loading screen\n            installation_thread = threading.Thread(target=run_installation, args=(installation_command,))\n            installation_thread.start()\n            loading_animation()\n            installation_thread.join()\n        \n        print_completion_message()\n    else:\n        print(\"Usage: python loading_screen.py <installation_command>\")\n        sys.exit(1)"}
{"type": "source_file", "path": "external_providers.py", "content": "# external_providers.py\nimport streamlit as st\nfrom ollama_utils import load_api_keys, save_api_keys\n\nADVANCED_GROQ_MODELS = [\n    \"llama-3.1-405b-reasoning\",\n    \"llama-3.1-70b-versatile\",\n    \"llama-3.1-8b-instant\",\n]\n\ndef external_providers_ui():\n    st.title(\"☁️ External Providers\")\n\n    api_keys = load_api_keys()\n\n    col1, col2 = st.columns(2)\n\n    with col1:\n        st.header(\"Search Providers\")\n        api_keys[\"serpapi_api_key\"] = st.text_input(\"SerpApi API Key\", value=api_keys.get(\"serpapi_api_key\", \"\"), type=\"password\")\n        api_keys[\"serper_api_key\"] = st.text_input(\"Serper API Key\", value=api_keys.get(\"serper_api_key\", \"\"), type=\"password\")\n        api_keys[\"google_api_key\"] = st.text_input(\"Google Custom Search API Key\", value=api_keys.get(\"google_api_key\", \"\"), type=\"password\")\n        api_keys[\"google_cse_id\"] = st.text_input(\"Google Custom Search Engine ID\", value=api_keys.get(\"google_cse_id\", \"\"), type=\"password\")\n        api_keys[\"bing_api_key\"] = st.text_input(\"Bing Search API Key\", value=api_keys.get(\"bing_api_key\", \"\"), type=\"password\")\n\n    with col2:\n        st.header(\"AI Model Providers\")\n        api_keys[\"openai_api_key\"] = st.text_input(\"OpenAI API Key\", value=api_keys.get(\"openai_api_key\", \"\"), type=\"password\")\n        api_keys[\"groq_api_key\"] = st.text_input(\"Groq API Key\", value=api_keys.get(\"groq_api_key\", \"\"), type=\"password\")\n        api_keys[\"mistral_api_key\"] = st.text_input(\"Mistral API Key\", value=api_keys.get(\"mistral_api_key\", \"\"), type=\"password\")\n        \n        # Add checkbox for advanced Groq models\n        api_keys[\"use_advanced_groq_models\"] = st.checkbox(\n            \"Enable Advanced Groq Models\",\n            value=api_keys.get(\"use_advanced_groq_models\", False),\n            help=\"Check this box if your Groq account is approved to use advanced models.\"\n        )\n\n    if st.button(\"💾 Save API Keys\"):\n        save_api_keys(api_keys)\n        st.success(\"🟢 API keys saved!\")\n\ndef get_available_groq_models(api_keys):\n    base_models = [\n        \"llama3-8b-8192\",\n        \"llama3-70b-8192\",\n        \"llama3-groq-8b-8192-tool-use-preview\",\n        \"llama3-groq-70b-8192-tool-use-preview\",\n        \"mixtral-8x7b-32768\",\n        \"gemma-7b-it\",\n        \"gemma2-9b-it\",\n    ]\n    \n    if api_keys.get(\"use_advanced_groq_models\", False):\n        return base_models + ADVANCED_GROQ_MODELS\n    else:\n        return base_models"}
{"type": "source_file", "path": "local_models.py", "content": "# local_models.py\nimport streamlit as st\nimport pandas as pd\nimport requests\nfrom datetime import datetime\nfrom ollama_utils import *\n\ndef list_local_models():\n    st.title(\"🤖 Local Ollama Models\")\n    response = requests.get(f\"{OLLAMA_URL}/tags\")\n    response.raise_for_status()\n    models = response.json().get(\"models\", [])\n    if not models:\n        st.write(\"No local models available.\")\n        return\n\n    # Prepare data for the dataframe\n    data = []\n    for model in models:\n        size_gb = model.get('size', 0) / (1024**3)  # Convert bytes to GB\n        modified_at = model.get('modified_at', 'Unknown')\n        if modified_at != 'Unknown':\n            modified_at = datetime.fromisoformat(modified_at).strftime('%Y-%m-%d %H:%M:%S')\n        data.append({\n            \"Model Name\": model['name'],\n            \"Size (GB)\": f\"{size_gb:.2f}\",\n            \"Modified At\": modified_at,\n            \"Preload\": False,\n            \"Keep-Alive\": \"\"\n        })\n\n    # Create a pandas dataframe\n    df = pd.DataFrame(data)\n\n    # Display the editable dataframe\n    edited_df = st.data_editor(\n        df,\n        hide_index=True,\n        column_config={\n            \"Model Name\": st.column_config.TextColumn(\"Model Name\", disabled=True),\n            \"Size (GB)\": st.column_config.NumberColumn(\"Size (GB)\", format=\"%.2f\", disabled=True),\n            \"Modified At\": st.column_config.TextColumn(\"Modified At\", disabled=True),\n            \"Preload\": st.column_config.CheckboxColumn(\"Preload\"),\n            \"Keep-Alive\": st.column_config.TextColumn(\"Keep-Alive\")\n        },\n        column_order=(\"Model Name\", \"Size (GB)\", \"Modified At\", \"Preload\", \"Keep-Alive\"),\n        key=\"model_editor\"\n    )\n\n    # Add Apply Changes button\n    if st.button(\"Apply Changes\"):\n        for index, row in edited_df.iterrows():\n            model_name = row[\"Model Name\"]\n            if row[\"Preload\"]:\n                preload_model(model_name)\n            if row[\"Keep-Alive\"]:\n                apply_model_keep_alive(model_name, row[\"Keep-Alive\"])\n        st.success(\"Changes applied successfully!\")"}
{"type": "source_file", "path": "main.py", "content": "# main.py\n\nimport streamlit as st\nimport threading\nfrom flask import Flask, jsonify, request\nimport os\nimport subprocess\nimport json \n\n# Set page config for wide layout\nst.set_page_config(layout=\"wide\", page_title=\"Ollama Workbench\", page_icon=\"🦙\")\n\n\nimport sys\nprint(f\"Python executable: {sys.executable}\")\ntry:\n    from streamlit_option_menu import option_menu\nexcept ImportError:\n    # Fallback implementation for streamlit_option_menu\n    def option_menu(menu_title, options, icons=None, menu_icon=None, default_index=0, styles=None):\n        import streamlit as st\n        selected = st.selectbox(\n            menu_title if menu_title else \"Menu\",\n            options,\n            index=default_index,\n        )\n        return selected\nfrom ollama_utils import *\nfrom model_tests import *\nfrom ui_elements import (\n    model_comparison_test, contextual_response_test, feature_test,\n    list_local_models, pull_models, show_model_details, remove_model_ui,\n    vision_comparison_test, chat_interface, update_models, files_tab,\n    server_configuration, server_monitoring\n)\nfrom repo_docs import main as repo_docs_main\nfrom web_to_corpus import main as web_to_corpus_main\nfrom welcome import display_welcome_message\nfrom projects import projects_main, Task\nfrom prompts import manage_prompts, get_agent_prompt, get_metacognitive_prompt, get_voice_prompt, get_identity_prompt\nfrom brainstorm import brainstorm_interface\nfrom ollama_utils import get_ollama_resource_usage\nfrom research import research_interface\nfrom enhanced_corpus import enhance_corpus_ui\nfrom build import build_interface\nfrom openai_utils import display_openai_settings, call_openai_api, set_openai_api_key\nfrom groq_utils import display_groq_settings, call_groq_api\nfrom nodes import nodes_interface  \nfrom external_providers import external_providers_ui \nfrom streamlit_extras.stylable_container import stylable_container\nfrom streamlit_javascript import st_javascript\nfrom db_init import init_db\nfrom persona_chat import persona_group_chat\nfrom persona_lab.persona_lab import persona_lab_interface\n\n# Initialize the database\ninit_db()\n\n# Global variable to store the port number\nollama_port = None \n\n# Create a native messaging host manifest \nNATIVE_HOST_MANIFEST = \"native_host_manifest.json\" \nwith open(NATIVE_HOST_MANIFEST, \"w\") as f:\n    json.dump({\n        \"name\": \"ollama_workbench_host\",\n        \"description\": \"Native messaging host for Ollama Workbench\",\n        \"path\": os.path.abspath(__file__), \n        \"type\": \"stdio\"\n    }, f, indent=4)\n\n# Custom CSS\nst.markdown(\"\"\"\n        <style>\n        body, h1, h2, h3, h4, h5, h6, p {\n            font-family: Open Sans, Helvetica, Arial, sans-serif!important;\n            font-weight: 400;\n        }\n        .app-title {\n            font-size: 40px!important;\n            font-family: Open Sans, Helvetica, Arial, sans-serif!important;\n        }\n        .app-title span {\n            color: orange;\n        }\n        .nav-link {\n            display: block;\n            color: inherit;\n            border: 0!important;\n            padding: 10px; \n            text-align: left!important;\n            cursor: pointer;\n            font-size: 16px;\n            box-sizing: border-box;\n            white-space: nowrap;\n            text-decoration: none; \n        }\n        .nav-link.active {\n            background-color: orange!important; \n            color: white;\n        }\n        .nav-button {\n            display: block;\n            color: inherit;\n            border: 0!important;\n            padding: 0px;\n            text-align: left!important;\n            cursor: pointer;\n            font-size: 16px;\n            box-sizing: border-box;\n            white-space: nowrap;\n        }\n        \n        button {\n            border: 0!important;\n            text-align: left!important;\n            justify-content: left!important;\n            white-space: nowrap;\n        }\n\n        .st-emotion-cache-0, \n        .st-emotion-cache-0 details, \n        .st-emotion-cache-0 summary {\n            border: 0!important;\n        }\n        div[data-testid=\"stVerticalBlockBorderWrapper\"] > div > div[width=\"439\"] > div[data-testid=\"stVerticalBlockBorderWrapper\"] {\n            background-color: rgb(255,255,255,.2);\n        }\n        </style>\n\"\"\", unsafe_allow_html=True)\n\n# Function to inject custom CSS\ndef inject_custom_css(css):\n    st.markdown(f'<style>{css}</style>', unsafe_allow_html=True)\n\n# JavaScript to detect theme\ntheme = st_javascript(\"\"\"\n    (function() {\n        const prefersDark = window.matchMedia('(prefers-color-scheme: dark)').matches;\n        return prefersDark ? 'dark' : 'light';\n    })();\n\"\"\")\n\n# Define CSS for light and dark modes\nlight_mode_css = \"\"\"\n/* Custom styles for light mode */\n.sidebar .css-1d391kg {\n    background-color: #fafafa;\n}\n.sidebar .css-1d391kg .nav-link {\n    color: #000000;\n}\n.sidebar .css-1d391kg .nav-link-selected {\n    background-color: #e0e0e0;\n    color: #000000;\n}\nbutton.ef3psqc13, \nbutton.ef3psqc14 {\n    background-color: #2c4755;\n    color: #FFFFFF;\n    border: solid 1px #FFF;\n}\nbutton.ef3psqc13:hover, \nbutton.ef3psqc14:hover {\n    background-color: #e16d6d;\n    color: #FFFFFF;\n}\n\"\"\"\n\ndark_mode_css = \"\"\"\n/* Custom styles for dark mode */\n.sidebar .css-1d391kg {\n    background-color: #333333;\n}\n.sidebar .css-1d391kg .nav-link {\n    color: #ffffff;\n}\n.sidebar .css-1d391kg .nav-link-selected {\n    background-color: #555555;\n    color: #ffffff;\n}\nbutton.ef3psqc13 {\n    background-color: rgb(255,255,255,.1);\n    border: 0px!important;\n}\nbutton.ef3psqc13:hover {\n    background-color: rgb(255,255,255,.05);\n}\n\"\"\"\n\n# Apply the appropriate CSS based on the detected theme\nif theme == 'dark':\n    inject_custom_css(dark_mode_css)\nelse:\n    inject_custom_css(light_mode_css)\n\n# Define constants for the sidebar sections\nSIDEBAR_SECTIONS = {\n    \"Workflow\": [\n        (\"Research\", \"Research\"),\n        (\"Brainstorm\", \"Brainstorm\"),\n        (\"Projects\", \"Projects\"),\n        (\"CEF\", \"CEF\"),\n        (\"Build\", \"Build\"),\n        (\"Prompts\", \"Prompts\"),\n    ],\n    \"Document\": [\n        (\"Repository Analyzer\", \"Repository Analyzer\"),\n        (\"Web Crawler\", \"Web Crawler\"),\n        (\"Corpus\", \"Corpus\"),\n        (\"Manage Files\", \"Files\"),\n    ],\n    \"Maintain\": [\n        (\"List Local Models\", \"List Local Models\"),\n        (\"Model Information\", \"Show Model Information\"),\n        (\"Pull a Model\", \"Pull a Model\"),\n        (\"Remove a Model\", \"Remove a Model\"),\n        (\"Update Models\", \"Update Models\"),\n        (\"Server Configuration\", \"Server Configuration\"),\n        (\"Server Monitoring\", \"Server Monitoring\"),\n        (\"External Providers\", \"External Providers\")\n    ],\n    \"Test\": [\n        (\"Model Feature Test\", \"Feature Test\"),\n        (\"Response Quality\", \"Model Comparison\"),\n        (\"Contextual Response\", \"Contextual Response\"),\n        (\"Vision Models\", \"Vision\"),\n    ],\n    \"Help\": [\n        (\"Help\", \"Help\")\n    ]\n}\n\ndef initialize_session_state():\n    \"\"\"Initialize session state variables if they don't exist.\"\"\"\n    if 'selected_test' not in st.session_state:\n        st.session_state.selected_test = \"Chat\"\n    if \"selected_models\" not in st.session_state:\n        st.session_state.selected_models = []\n    if \"selected_model\" not in st.session_state:\n        st.session_state.selected_model = None\n    if 'bm_tasks' not in st.session_state:\n        st.session_state.bm_tasks = []\n    if 'show_resource_usage' not in st.session_state:\n        st.session_state.show_resource_usage = False\n    if 'web_page_content' not in st.session_state: \n        st.session_state.web_page_content = None\n    if 'show_prompt' not in st.session_state:\n        st.session_state.show_prompt = True\n\ndef create_sidebar():\n    \"\"\"Create and populate the sidebar.\"\"\"\n    with st.sidebar:\n        st.markdown(\n            '<div style=\"text-align: left;\">'\n            '<h1 class=\"logo\" style=\"font-size: 24px; font-weight: 300;\">🦙 Ollama <span style=\"color: orange;\">Workbench</span></h1>'\n            \"</div>\",\n            unsafe_allow_html=True,\n        )\n\n        if st.session_state.get(\"show_resource_usage\", False):\n            display_resource_usage_sidebar()\n\n        # Define the main navigation menu\n        main_menu = option_menu(\n            menu_title=\"\",\n            options=[\"Chat\"] + list(SIDEBAR_SECTIONS.keys()),\n            icons=[\"chat\", \"gear\", \"folder\", \"tools\", \"clipboard-check\", \"question-circle\"],\n            menu_icon=\"cast\",\n            default_index=0,\n            styles={\n                \"container\": {\"padding\": \"0!important\"},\n                \"icon\": {\"font-size\": \"12px\"},\n                \"nav-link\": {\n                    \"font-size\": \"14px\",\n                    \"text-align\": \"left\",\n                    \"margin\": \"0px\",\n                    \"--primary-color\": \"#1976D2\",\n                    \"--hover-color\": \"#e16d6d\",\n                },\n                \"nav-link-selected\": {\"font-weight\": \"bold\"},\n            },\n        )\n\n        # Define the sub navigation menu based on the selected main menu\n        if main_menu == \"Chat\":\n            st.session_state.selected_test = \"Chat\"\n        else:\n            sub_menu = option_menu(\n                menu_title=None,\n                options=[option[1] for option in SIDEBAR_SECTIONS[main_menu]],\n                default_index=0,\n                styles={\n                    \"container\": {\"padding\": \"0!important\"},\n                    \"icon\": {\"font-size\": \"12px\"},\n                    \"nav-link\": {\n                        \"font-size\": \"14px\",\n                        \"text-align\": \"left\",\n                        \"margin\": \"0px\",\n                        \"--primary-color\": \"#1976D2\",\n                        \"--hover-color\": \"#e16d6d\",\n                    },\n                    \"nav-link-selected\": {\"font-weight\": \"bold\"},\n                },\n            )\n            st.session_state.selected_test = sub_menu\n\ndef main_content():\n    if 'bm_tasks' not in st.session_state:\n        st.session_state.bm_tasks = []\n    if st.session_state.selected_test == \"Chat\":\n        chat_interface()\n    elif st.session_state.selected_test == \"Build\":\n        build_interface()\n    elif st.session_state.selected_test == \"Research\":\n        research_interface()\n    elif st.session_state.selected_test == \"Brainstorm\":\n        brainstorm_interface()\n    elif st.session_state.selected_test == \"Projects\":\n        projects_main()\n    elif st.session_state.selected_test == \"Prompts\":\n        manage_prompts()\n    elif st.session_state.selected_test == \"CEF\":\n        nodes_interface()\n    elif st.session_state.selected_test == \"Corpus\":\n        enhance_corpus_ui()\n    elif st.session_state.selected_test == \"Files\":\n        files_tab()\n    elif st.session_state.selected_test == \"Web Crawler\":\n        web_to_corpus_main()\n    elif st.session_state.selected_test == \"Repository Analyzer\":\n        repo_docs_main()\n    elif st.session_state.selected_test == \"List Local Models\":\n        list_local_models()\n    elif st.session_state.selected_test == \"Show Model Information\":\n        show_model_details()\n    elif st.session_state.selected_test == \"Pull a Model\":\n        pull_models()\n    elif st.session_state.selected_test == \"Remove a Model\":\n        remove_model_ui()\n    elif st.session_state.selected_test == \"Update Models\":\n        update_models()\n    elif st.session_state.selected_test == \"Server Configuration\":\n        server_configuration()\n    elif st.session_state.selected_test == \"Server Monitoring\":\n        server_monitoring()\n    elif st.session_state.selected_test == \"External Providers\":\n        external_providers_ui()\n    elif st.session_state.selected_test == \"Feature Test\":\n        feature_test()\n    elif st.session_state.selected_test == \"Model Comparison\":\n        model_comparison_test()\n    elif st.session_state.selected_test == \"Contextual Response\":\n        contextual_response_test()\n    elif st.session_state.selected_test == \"Vision\":\n        vision_comparison_test()\n    elif st.session_state.selected_test == \"Help\":\n        display_welcome_message()\n    else:\n        chat_interface()\n\n# Create a Flask app for the API\napp = Flask(__name__)\n\n@app.route('/prompts')\ndef get_prompts():\n    \"\"\"Returns a JSON with all prompt types.\"\"\"\n    return jsonify({\n        \"agent\": get_agent_prompt(),\n        \"metacognitive\": get_metacognitive_prompt(),\n        \"voice\": get_voice_prompt(),\n        \"identity\": get_identity_prompt()\n    })\n\n@app.route('/openai-key')\ndef get_openai_key():\n    \"\"\"Returns the OpenAI API key.\"\"\"\n    api_keys = load_api_keys()\n    return jsonify({\"openai_api_key\": api_keys.get(\"openai_api_key\")})\n\n@app.route('/identifier')\ndef get_identifier():\n    \"\"\"Returns a unique identifier for Ollama Workbench.\"\"\"\n    return \"Ollama Workbench\" \n\n@app.route('/port')\ndef get_port():\n    \"\"\"Returns the dynamically allocated port number.\"\"\"\n    global ollama_port\n    return str(ollama_port)\n\ndef main():\n    initialize_session_state()\n    create_sidebar()\n    \n    web_page_content = st.query_params.get('web_page_content', [None])[0] \n    if web_page_content:\n        st.session_state.web_page_content = web_page_content\n\n    main_content()\n\n    # Get parameters from URL\n    web_page_url = st.query_params.get('url', None)\n    is_extension = st.query_params.get('extension', 'false').lower() == 'true'\n\n    if is_extension and web_page_url:\n        st.session_state.web_page_url = web_page_url\n        st.session_state.is_extension = is_extension\n\n# Function to send a message to the Chrome extension\ndef send_port_to_extension(port):\n    \"\"\"Sends the port number to the background script of the extension.\"\"\"\n    try:\n        # Use the 'chrome-extension' protocol to send a message to the extension\n        cmd = f'chrome-extension://{os.environ.get(\"EXTENSION_ID\", \"gddghhhklfnhijhhagfgnfiehidcdnba\")}/background.js'\n        message = {\"message\": \"ollamaPort\", \"port\": port}\n\n        # Use subprocess to send the message. Requires 'npx' which comes with Node.js.\n        process = subprocess.Popen(['npx', 'chrome-remote-interface', 'sendMessage', cmd, '--json', json.dumps(message)], \n                                    stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n        stdout, stderr = process.communicate()\n        if stderr:\n            print(f\"Error sending message to extension: {stderr.decode('utf-8')}\") \n        else:\n            print(f\"Successfully sent port number to extension: {stdout.decode('utf-8')}\")\n\n    except Exception as e:\n        print(f\"Error sending message to extension: {e}\")\n\ndef run_flask():\n    global ollama_port\n    import socket\n    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    sock.bind(('', 0))\n    ollama_port = sock.getsockname()[1] \n    sock.close()\n    app.run(port=ollama_port)\n    print(f\"Flask running on port: {ollama_port}\") \n\n    # Send the port to the extension once Flask is running\n    send_port_to_extension(ollama_port)\n\nif __name__ == \"__main__\":\n    # Start Flask before Streamlit\n    flask_thread = threading.Thread(target=run_flask)\n    flask_thread.start() \n    main() "}
{"type": "source_file", "path": "chat_interface.py", "content": "# chat_interface.py\n\nimport streamlit as st\nimport os\nimport json\nfrom datetime import datetime\nimport re\nimport ollama\nfrom ollama_utils import (\n    get_available_models, get_all_models, load_api_keys, get_token_embeddings\n)\nfrom openai_utils import call_openai_api, OPENAI_MODELS\nfrom groq_utils import get_groq_client, call_groq_api, GROQ_MODELS\nfrom mistral_utils import  call_mistral_api, MISTRAL_MODELS\nfrom prompts import (\n    get_agent_prompt, get_metacognitive_prompt, get_voice_prompt\n)\nimport tiktoken\nfrom streamlit_extras.bottom_container import bottom\nfrom enhanced_corpus import GraphRAGCorpus, OllamaEmbedder\nfrom collections import deque\nimport numpy as np\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport logging \nimport time  \nfrom tts_utils import text_to_speech, play_speech\nimport sqlite3\n\n# Setup logging\nlogging.basicConfig(\n    filename='app.log',\n    filemode='a',\n    format='%(asctime)s - %(levelname)s - %(message)s',\n    level=logging.INFO\n)\nlogger = logging.getLogger()\n\nSETTINGS_FILE = \"chat-settings.json\"\nRAGTEST_DIR = \"ragtest\"\n\n# Candidate Prompts for Instance-Adaptive Zero-Shot CoT Prompting\nCANDIDATE_PROMPTS = [\n    \"Let's think step by step.\",\n    \"Don't think. Just feel.\",\n    \"Let's solve this problem by splitting it into steps.\",\n    \"First, let's break down the problem.\",\n    \"To approach this, we'll consider each part carefully.\",\n    \"Let's analyze this systematically.\",\n    \"We'll tackle this by addressing each component individually.\",\n    \"Step one is to understand the problem fully.\",\n    \"We'll handle this by dividing it into manageable sections.\"\n]\n\nclass ModelMemoryHandler:\n    def __init__(self, model_type):\n        self.model_type = model_type\n        self.episodic_memory = EpisodicMemory()\n\n    def segment_text(self, model, text, api_keys):\n        if self.model_type == \"openai\":\n            return self.segment_text_openai(model, text, api_keys)\n        elif self.model_type == \"groq\":\n            return self.segment_text_groq(model, text, api_keys)\n        elif self.model_type == \"mistral\":\n            return self.segment_text_mistral(model, text, api_keys)\n        else:\n            return self.segment_text_ollama(model, text, api_keys)\n\n    def segment_text_openai(self, model, text, api_keys):\n        # Implementation for OpenAI models\n        pass\n\n    def segment_text_groq(self, model, text, api_keys):\n        # Implementation for Groq models\n        pass\n\n    def segment_text_mistral(self, model, text, api_keys):\n        # Implementation for Mistral models\n        pass\n\n    def segment_text_ollama(self, model, text, api_keys):\n        # Implementation for Ollama models\n        pass\n\n    def retrieve_events(self, query_embedding):\n        return self.episodic_memory.retrieve_events(query_embedding)\n\nclass EpisodicMemory:\n    def __init__(self, similarity_buffer_size: int = 5, contiguity_buffer_size: int = 3):\n        self.similarity_buffer = deque(maxlen=similarity_buffer_size)\n        self.contiguity_buffer = deque(maxlen=contiguity_buffer_size)\n        self.events = []\n        self.threshold_history = deque(maxlen=10)\n        self.min_segment_length = 20\n        self.max_segment_length = 200\n\n    def segment_text_into_events(self, model: str, text: str, threshold: float = 0.01, api_keys: dict = None):\n        \"\"\"Segments the text into events based on surprise and refines boundaries.\"\"\"\n        try:\n            # Get appropriate tokenizer\n            try:\n                tokenizer = tiktoken.encoding_for_model(model)\n            except KeyError:\n                tokenizer = tiktoken.get_encoding(\"gpt2\")\n            \n            # Tokenize text\n            tokens = tokenizer.encode(text)\n            num_tokens = len(tokens)\n            \n            # Dynamic segmentation based on text length\n            segment_size = min(max(num_tokens // 10, self.min_segment_length), self.max_segment_length)\n            surprise_indices = []\n            \n            # Get embeddings for token windows\n            embeddings = get_token_embeddings(model, text, api_keys)\n            if embeddings.ndim == 1:\n                embeddings = embeddings.reshape(1, -1)\n            \n            # Calculate surprise at each potential boundary\n            for i in range(segment_size, num_tokens - segment_size, segment_size):\n                prev_window = embeddings[max(0, i-segment_size):i]\n                next_window = embeddings[i:min(i+segment_size, num_tokens)]\n                \n                if len(prev_window) > 0 and len(next_window) > 0:\n                    prev_centroid = np.mean(prev_window, axis=0)\n                    next_centroid = np.mean(next_window, axis=0)\n                    surprise = 1 - cosine_similarity(prev_centroid.reshape(1, -1), next_centroid.reshape(1, -1))[0][0]\n                    \n                    # Adaptive thresholding\n                    current_threshold = np.mean(self.threshold_history) if self.threshold_history else threshold\n                    if surprise > current_threshold:\n                        surprise_indices.append(i)\n                        self.threshold_history.append(surprise)\n            \n            # Refine boundaries using modularity\n            if len(embeddings) > 0 and surprise_indices:\n                refined_boundaries = refine_boundaries(embeddings, surprise_indices)\n                \n                # Create events with metadata\n                self.events = []\n                start = 0\n                for end in refined_boundaries:\n                    event_text = text[start:end]\n                    event_embedding = np.mean(embeddings[start:end], axis=0)\n                    timestamp = datetime.now().isoformat()\n                    \n                    # Calculate importance score based on surprise and length\n                    importance = np.mean([\n                        1 - cosine_similarity(embeddings[max(0, start-1):start],\n                                           embeddings[start:min(start+1, len(embeddings))])[0][0]\n                        if start > 0 else 0.5\n                    ])\n                    \n                    self.events.append({\n                        'text': event_text,\n                        'embedding': event_embedding,\n                        'timestamp': timestamp,\n                        'importance': importance,\n                        'length': len(event_text)\n                    })\n                    start = end\n                \n                # Handle the last segment\n                if start < len(text):\n                    event_text = text[start:]\n                    event_embedding = np.mean(embeddings[start:], axis=0)\n                    self.events.append({\n                        'text': event_text,\n                        'embedding': event_embedding,\n                        'timestamp': datetime.now().isoformat(),\n                        'importance': 0.5,  # Default importance for last segment\n                        'length': len(event_text)\n                    })\n        \n        except Exception as e:\n            logger.error(f\"Error in segment_text_into_events: {str(e)}\")\n            # Fallback: create a single event if segmentation fails\n            if len(text) > 0:\n                self.events = [{\n                    'text': text,\n                    'embedding': np.mean(embeddings, axis=0) if len(embeddings) > 0 else np.zeros(1536),\n                    'timestamp': datetime.now().isoformat(),\n                    'importance': 0.5,\n                    'length': len(text)\n                }]\n\n    def retrieve_events(self, query_embedding: np.ndarray, top_k: int = None) -> list:\n        \"\"\"Retrieves relevant events based on the query embedding with advanced filtering.\"\"\"\n        if not self.events:\n            return []\n        \n        try:\n            query_embedding = query_embedding.reshape(1, -1)\n            event_embeddings = np.array([event['embedding'] for event in self.events])\n            \n            if event_embeddings.ndim == 1:\n                event_embeddings = event_embeddings.reshape(1, -1)\n            \n            # Calculate similarity scores\n            similarity_scores = cosine_similarity(query_embedding, event_embeddings)[0]\n            \n            # Calculate recency scores (normalized)\n            timestamps = [datetime.fromisoformat(event['timestamp']) for event in self.events]\n            max_time = max(timestamps)\n            time_diffs = [(max_time - t).total_seconds() for t in timestamps]\n            max_diff = max(time_diffs) if time_diffs else 1\n            recency_scores = [1 - (diff / max_diff) for diff in time_diffs]\n            \n            # Combine scores with weights\n            importance_scores = [event['importance'] for event in self.events]\n            final_scores = (\n                0.6 * similarity_scores +\n                0.2 * np.array(recency_scores) +\n                0.2 * np.array(importance_scores)\n            )\n            \n            # Select top events\n            if top_k is None:\n                top_k = len(self.similarity_buffer)\n            \n            top_indices = np.argsort(final_scores)[-top_k:][::-1]\n            \n            # Retrieve events and update buffers\n            retrieved_events = []\n            for idx in top_indices:\n                event = self.events[idx]\n                retrieved_events.append(event)\n                self.similarity_buffer.append(event)\n                \n                # Add contextually relevant neighboring events\n                start_idx = max(0, idx - 1)\n                end_idx = min(len(self.events), idx + 2)\n                for i in range(start_idx, end_idx):\n                    if i != idx:\n                        neighbor = self.events[i]\n                        self.contiguity_buffer.append(neighbor)\n                        if len(retrieved_events) < top_k * 2:  # Include some context but not too much\n                            retrieved_events.append(neighbor)\n            \n            return retrieved_events\n            \n        except Exception as e:\n            logger.error(f\"Error in retrieve_events: {str(e)}\")\n            return list(self.similarity_buffer) + list(self.contiguity_buffer)\n\n    def save_to_disk(self, filepath: str):\n        \"\"\"Save memory state to disk.\"\"\"\n        try:\n            save_data = {\n                'events': [{\n                    **event,\n                    'embedding': event['embedding'].tolist()  # Convert numpy array to list\n                } for event in self.events],\n                'threshold_history': list(self.threshold_history)\n            }\n            with open(filepath, 'w') as f:\n                json.dump(save_data, f)\n        except Exception as e:\n            logger.error(f\"Error saving memory to disk: {str(e)}\")\n\n    def load_from_disk(self, filepath: str):\n        \"\"\"Load memory state from disk.\"\"\"\n        try:\n            with open(filepath, 'r') as f:\n                save_data = json.load(f)\n            \n            self.events = [{\n                **event,\n                'embedding': np.array(event['embedding'])  # Convert list back to numpy array\n            } for event in save_data['events']]\n            \n            self.threshold_history = deque(save_data['threshold_history'], maxlen=10)\n        except Exception as e:\n            logger.error(f\"Error loading memory from disk: {str(e)}\")\n            # Initialize empty state if load fails\n            self.events = []\n            self.threshold_history = deque(maxlen=10)\n\ndef load_settings():\n    if os.path.exists(SETTINGS_FILE):\n        with open(SETTINGS_FILE, \"r\") as f:\n            settings = json.load(f)\n            for key, value in settings.items():\n                if value != \"None\":  # Only set non-None values\n                    st.session_state[key] = value\n    logger.info(f\"Settings loaded: {st.session_state}\")\n\ndef save_settings():\n    settings = {\n        \"selected_model\": st.session_state.selected_model,\n        \"agent_type\": st.session_state.agent_type,\n        \"metacognitive_type\": st.session_state.metacognitive_type,\n        \"voice_type\": st.session_state.voice_type,\n        \"selected_corpus\": st.session_state.selected_corpus,\n        \"temperature_slider_chat\": st.session_state.temperature_slider_chat,\n        \"max_tokens_slider_chat\": min(st.session_state.max_tokens_slider_chat, 8000),\n        \"presence_penalty_slider_chat\": st.session_state.presence_penalty_slider_chat,\n        \"frequency_penalty_slider_chat\": st.session_state.frequency_penalty_slider_chat,\n        \"episodic_memory_enabled\": st.session_state.episodic_memory_enabled,\n        \"advanced_thinking_enabled\": st.session_state.advanced_thinking_enabled,  # Ensure this line is included\n        \"thinking_steps\": st.session_state.thinking_steps,  # Added to save thinking steps\n        \"instance_adaptive_cot_enabled\": st.session_state.instance_adaptive_cot_enabled,\n        \"cot_strategy\": st.session_state.cot_strategy,\n        \"cot_threshold\": st.session_state.cot_threshold,\n        \"cot_top_n\": st.session_state.cot_top_n\n    }\n    with open(SETTINGS_FILE, \"w\") as f:\n        json.dump(settings, f)\n    logger.info(f\"Settings saved: {settings}\")\n    st.success(\"Settings saved successfully!\")\n\ndef ai_assisted_prompt_writing():\n    st.markdown(\"\"\"\n    <style>\n    .stModal > div[data-testid=\"stHorizontalBlock\"]:first-child {\n        display: none !important;\n    }\n    </style>\n    \"\"\", unsafe_allow_html=True)\n    \n    st.subheader(\"AI Prompt Writer\")\n    \n    if st.button(\"X\", key=\"close_modal\", help=\"Cancel assisted prompt writing.\"):\n        st.session_state.show_prompt_modal = False\n        st.rerun()\n    \n    user_need = st.text_input(\"What do you need help with?\")\n    if user_need:\n        prompt_suggestion = generate_prompt_suggestion(user_need)\n        if prompt_suggestion:\n            st.write(\"Suggested prompt:\")\n            edited_prompt = st.text_area(\"Edit the prompt before using it:\", value=prompt_suggestion)\n            if st.button(\"Use this prompt\"):\n                st.session_state.chat_input = edited_prompt\n                st.session_state.show_prompt_modal = False\n                st.rerun()\n        else:\n            st.warning(\"Unable to generate a prompt suggestion. Please try again or select a different model.\")\n\ndef generate_prompt_suggestion(user_need):\n    api_keys = load_api_keys()\n    model = st.session_state.selected_model\n    base_prompt = f\"Your task is to improve the user's prompt and send it to another AI agent to process. Do not respond directly to the user's response, assume whatever they give you is a prompt that needs to be improved for maximum efficiency and effectiveness. Create a prompt that will give the user the best results. Create a detailed and effective improved prompt for an AI assistant based on this user need: {user_need}\"\n    \n    # Construct the full prompt using the agent prompt builder\n    full_prompt = construct_agent_prompt(\n        st.session_state.get('agent_type', 'None'),\n        st.session_state.get('metacognitive_type', 'None'),\n        st.session_state.get('voice_type', 'None'),\n        base_prompt\n    )\n\n    try:\n        if model in OPENAI_MODELS:\n            response = call_openai_api(\n                model=model,\n                messages=[{\"role\": \"user\", \"content\": full_prompt}],\n                temperature=st.session_state.temperature_slider_chat,\n                max_tokens=min(st.session_state.max_tokens_slider_chat, 16000),\n                openai_api_key=api_keys.get(\"openai_api_key\"),\n                stream=False\n            )\n            return response\n        elif model in GROQ_MODELS:\n            response = call_groq_api(\n                model=model,\n                prompt=full_prompt,\n                temperature=st.session_state.temperature_slider_chat,\n                max_tokens=min(st.session_state.max_tokens_slider_chat, 8000),\n                groq_api_key=api_keys.get(\"groq_api_key\")\n            )\n            return response.strip()\n        elif model in MISTRAL_MODELS:\n            response = call_mistral_api(\n                model=model,\n                prompt=full_prompt,\n                temperature=st.session_state.temperature_slider_chat,\n                max_tokens=min(st.session_state.max_tokens_slider_chat, 8000),\n                mistral_api_key=api_keys.get(\"mistral_api_key\")\n            )\n            return response.strip()\n        else:\n            response = ollama.generate(\n                model=model,\n                prompt=full_prompt,\n                options={\n                    \"temperature\": st.session_state.temperature_slider_chat,\n                    \"num_predict\": min(st.session_state.max_tokens_slider_chat, 16000)\n                }\n            )\n            return response['response'].strip()\n    except Exception as e:\n        st.error(f\"An error occurred: {str(e)}\")\n        logger.error(f\"Error generating prompt suggestion: {e}\")\n        return None\n\ndef get_graphrag_context(user_input, corpus_name):\n    \"\"\"Get context from GraphRAG corpus.\"\"\"\n    try:\n        embedder = OllamaEmbedder()\n        corpus = GraphRAGCorpus.load(corpus_name, embedder)\n        results = corpus.query(user_input, n_results=3)\n        \n        context = \"\"\n        for result in results:\n            context += f\"Relevant Information (Similarity: {result['similarity']:.4f}):\\n{result['content']}\\n\\n\"\n        \n        return context.strip() if context else None\n    except Exception as e:\n        st.error(f\"Error querying GraphRAG corpus: {str(e)}\")\n        logger.error(f\"Error querying GraphRAG corpus: {e}\")\n        return None\n\ndef extract_content_blocks(text):\n    if text is None:\n        return [], []\n    \n    # Extract code blocks\n    code_blocks = re.findall(r'```[\\s\\S]*?```', text)\n    \n    # Remove code blocks from the text\n    text_without_code = re.sub(r'```[\\s\\S]*?```', '', text)\n    \n    # Extract article blocks that start with 'Title:' and continue until the next 'Title:' or the end of the text\n    article_blocks = re.findall(r'^Title:.*?(?=\\n^Title:|\\Z)', text_without_code, re.MULTILINE | re.DOTALL)\n    \n    return [block.strip('`').strip() for block in code_blocks], [block.strip() for block in article_blocks]\n\ndef construct_agent_prompt(agent_type, metacognitive_type, voice_type, selected_prompt=None):\n    \"\"\"Constructs the agent prompt based on selected types and chat mode.\"\"\"\n    prompt_parts = []\n    \n    # Only include workspace context if in code mode and workspace is enabled\n    if st.session_state.chat_mode == \"code\" and st.session_state.workspace_enabled:\n        if st.session_state.workspace_items:\n            prompt_parts.append(\"Current workspace context:\")\n            for item in st.session_state.workspace_items:\n                prompt_parts.append(f\"- {item}\")\n            prompt_parts.append(\"\")\n    \n    # Add agent type prompt if selected\n    if agent_type != \"None\":\n        agent_prompts = get_agent_prompt()\n        if agent_type in agent_prompts:\n            if isinstance(agent_prompts[agent_type], dict):\n                prompt_parts.append(agent_prompts[agent_type][\"prompt\"])\n            else:\n                prompt_parts.append(agent_prompts[agent_type])\n    \n    # Add metacognitive type prompt if selected\n    if metacognitive_type != \"None\":\n        metacog_prompts = get_metacognitive_prompt()\n        if metacognitive_type in metacog_prompts:\n            prompt_parts.append(metacog_prompts[metacognitive_type])\n    \n    # Add voice type prompt if selected\n    if voice_type != \"None\":\n        voice_prompts = get_voice_prompt()\n        if voice_type in voice_prompts:\n            prompt_parts.append(voice_prompts[voice_type])\n            \n    # Add selected prompt if provided\n    if selected_prompt:\n        prompt_parts.append(selected_prompt)\n    \n    # Return combined prompt\n    return \"\\n\\n\".join(prompt_parts) if prompt_parts else \"\"\n\ndef calculate_modularity(similarity_matrix: np.ndarray, communities: list) -> float:\n    \"\"\"Calculates the modularity of a graph given its similarity matrix and community structure.\"\"\"\n    if similarity_matrix.ndim != 2:\n        logger.error(f\"similarity_matrix should be 2D, but has {similarity_matrix.ndim} dimensions\")\n        return 0.0\n\n    m = np.sum(similarity_matrix) / 2  # Total edge weight\n    Q = 0\n    for community in communities:\n        for i in community:\n            for j in community:\n                if i >= similarity_matrix.shape[0] or j >= similarity_matrix.shape[1]:\n                    logger.error(f\"Index out of bounds. i={i}, j={j}, matrix shape={similarity_matrix.shape}\")\n                    continue\n                Q += similarity_matrix[i, j] - (np.sum(similarity_matrix[i, :]) * np.sum(similarity_matrix[:, j])) / m\n    return Q / (2 * m)\n\ndef refine_boundaries(embeddings: np.ndarray, surprise_indices: list) -> list:\n    \"\"\"Refines event boundaries using modularity.\"\"\"\n    logger.info(f\"Embeddings shape: {embeddings.shape}\")\n    logger.info(f\"Surprise indices: {surprise_indices}\")\n    \n    similarity_matrix = cosine_similarity(embeddings)\n    logger.info(f\"Similarity matrix shape: {similarity_matrix.shape}\")\n    \n    communities = []\n    start = 0\n    for end in surprise_indices:\n        communities.append(list(range(start, end)))\n        start = end\n    communities.append(list(range(start, len(embeddings))))\n    logger.info(f\"Number of communities: {len(communities)}\")\n\n    best_boundaries = surprise_indices.copy()\n    best_modularity = calculate_modularity(similarity_matrix, communities)\n    logger.info(f\"Initial modularity: {best_modularity}\")\n\n    for i in range(len(surprise_indices) - 1):\n        for j in range(surprise_indices[i] + 1, surprise_indices[i + 1]):\n            temp_boundaries = best_boundaries.copy()\n            temp_boundaries[i] = j\n            temp_communities = []\n            start = 0\n            for end in temp_boundaries:\n                temp_communities.append(list(range(start, end)))\n                start = end\n            temp_communities.append(list(range(start, len(embeddings))))\n            temp_modularity = calculate_modularity(similarity_matrix, temp_communities)\n            temp_modularity = calculate_modularity(similarity_matrix, temp_communities)\n            logger.debug(f\"Testing boundaries at step {i}, position {j}: modularity={temp_modularity}\")\n            if temp_modularity > best_modularity:\n                best_modularity = temp_modularity\n                best_boundaries = temp_boundaries.copy()\n                logger.info(f\"Updated best modularity to {best_modularity} with boundaries {best_boundaries}\")\n\n    return best_boundaries\n\ndef advanced_thinking_step(prompt: str, model: str, api_keys: dict, step: str) -> str:\n    \"\"\"Processes a single thinking step and returns the result.\"\"\"\n    try:\n        logger.info(f\"Starting thinking step: {step}\")\n        \n        # Construct the prompt for this thinking step\n        step_prompt = f\"{prompt}\\n\\nThinking step: {step}\\n\\nPlease provide your thoughts for this step.\"\n        \n        # Call the appropriate API based on the model\n        if model.startswith(\"openai/\"):\n            response = call_openai_api(\n                model,\n                step_prompt,\n                temperature=st.session_state.temperature_slider_chat,\n                max_tokens=min(st.session_state.max_tokens_slider_chat, 4000),\n                openai_api_key=api_keys.get(\"openai_api_key\")\n            )\n            return f\"**{step}**\\n\\n{response}\\n\\n\"\n        elif model.startswith(\"groq/\"):\n            response = call_groq_api(\n                model,\n                step_prompt,\n                temperature=st.session_state.temperature_slider_chat,\n                max_tokens=min(st.session_state.max_tokens_slider_chat, 8000),\n                groq_api_key=api_keys.get(\"groq_api_key\")\n            )\n            return f\"**{step}**\\n\\n{response.strip()}\\n\\n\"\n        elif model.startswith(\"mistral/\"):\n            response = call_mistral_api(\n                model,\n                step_prompt,\n                temperature=st.session_state.temperature_slider_chat,\n                max_tokens=min(st.session_state.max_tokens_slider_chat, 8000),\n                mistral_api_key=api_keys.get(\"mistral_api_key\")\n            )\n            return f\"**{step}**\\n\\n{response.strip()}\\n\\n\"\n        else:\n            response = ollama.generate(\n                model=model,\n                prompt=step_prompt,\n                options={\n                    \"temperature\": st.session_state.temperature_slider_chat,\n                    \"num_predict\": min(st.session_state.max_tokens_slider_chat, 16000)\n                }\n            )\n            return f\"**{step}**\\n\\n{response.response}\\n\\n\"\n        \n        logger.info(f\"Completed thinking step: {step}\")\n        \n    except Exception as e:\n        error_message = f\"Error during {step}: {e}\"\n        logger.error(error_message)\n        return f\"**{step}**\\n\\n{error_message}\\n\\n\"\n\ndef instance_adaptive_cot(prompt: str, model: str, api_keys: dict) -> str:\n    \"\"\"Implements Instance-Adaptive Zero-Shot CoT Prompting.\"\"\"\n    try:\n        # Generate initial rationale\n        initial_rationale = generate_rationale(prompt, \"\", model, api_keys)\n        \n        # Select the best prompt based on saliency scores\n        selected_prompt = select_best_prompt(prompt, model, api_keys)\n        \n        if selected_prompt:\n            # Construct full prompt with selected CoT prompt\n            full_prompt = f\"{selected_prompt}\\n\\nQuestion: {prompt}\\n\\nLet's approach this step-by-step:\\n1. First, {initial_rationale}\"\n        else:\n            # Fallback to default behavior with basic CoT structure\n            full_prompt = f\"Question: {prompt}\\n\\nLet's solve this step-by-step:\\n1. {initial_rationale}\"\n        \n        return full_prompt\n    except Exception as e:\n        logger.error(f\"Error in instance_adaptive_cot: {str(e)}\")\n        # Fallback to original prompt if anything fails\n        return prompt\n\ndef select_best_prompt(question: str, model: str, api_keys: dict) -> str:\n    \"\"\"Selects the best prompt from candidate prompts based on saliency scores.\"\"\"\n    saliency_scores = []\n    for candidate in CANDIDATE_PROMPTS:\n        # Generate rationale using the candidate prompt\n        rationale = generate_rationale(question, candidate, model, api_keys)\n        if not rationale:\n            saliency_scores.append(0)\n            continue\n        \n        # Calculate saliency scores\n        Iqp, Iqr, Ipr = calculate_saliency_scores(question, candidate, rationale, model, api_keys)\n        synthesized_score = synthesize_saliency_score(Iqp, Iqr, Ipr)\n        saliency_scores.append(synthesized_score)\n    \n    if not saliency_scores:\n        return None\n    \n    # Depending on the strategy, select the prompt\n    if st.session_state.cot_strategy == \"IAP-ss\":\n        for idx, score in enumerate(saliency_scores):\n            if score >= st.session_state.cot_threshold:\n                logger.info(f\"Selected prompt (IAP-ss): {CANDIDATE_PROMPTS[idx]} with score {score}\")\n                return CANDIDATE_PROMPTS[idx]\n        # If no prompt meets the threshold, return None\n        logger.info(\"No prompt met the threshold in IAP-ss strategy.\")\n        return None\n    elif st.session_state.cot_strategy == \"IAP-mv\":\n        top_n = st.session_state.cot_top_n\n        top_indices = np.argsort(saliency_scores)[-top_n:]\n        selected_prompts = [CANDIDATE_PROMPTS[idx] for idx in top_indices]\n        # Majority vote (here, we'll select the prompt with the highest score)\n        best_prompt = selected_prompts[np.argmax([saliency_scores[idx] for idx in top_indices])]\n        logger.info(f\"Selected prompt (IAP-mv): {best_prompt} with score {max([saliency_scores[idx] for idx in top_indices])}\")\n        return best_prompt\n    else:\n        return None\n\ndef generate_rationale(question: str, prompt: str, model: str, api_keys: dict) -> str:\n    \"\"\"Generates rationale using the given prompt.\"\"\"\n    full_prompt = f\"{prompt} {question}\"\n    try:\n        if model in OPENAI_MODELS:\n            response = call_openai_api(\n                model=model,\n                messages=[{\"role\": \"user\", \"content\": full_prompt}],\n                temperature=0.7,\n                max_tokens=500,\n                openai_api_key=api_keys.get(\"openai_api_key\"),\n                stream=False\n            )\n            return response\n        elif model in GROQ_MODELS:\n            response = call_groq_api(\n                model=model,\n                prompt=prompt,\n                temperature=0.7,\n                max_tokens=500,\n                groq_api_key=api_keys.get(\"groq_api_key\")\n            )\n            return response.strip()\n        elif model in MISTRAL_MODELS:\n            response = call_mistral_api(\n                model=model,\n                prompt=prompt,\n                temperature=0.7,\n                max_tokens=500,\n                mistral_api_key=api_keys.get(\"mistral_api_key\")\n            )\n            return response.strip()\n        else:\n            response = ollama.generate(\n                model=model,\n                prompt=prompt,\n                options={\n                    \"temperature\": 0.7,\n                    \"num_predict\": 500\n                }\n            )\n            return response['response'].strip()\n    except Exception as e:\n        logger.error(f\"Error generating rationale: {e}\")\n        return \"\"\n\ndef calculate_saliency_scores(question: str, prompt: str, rationale: str, model: str, api_keys: dict):\n    \"\"\"Calculates saliency scores Iqp, Iqr, Ipr.\"\"\"\n    # Placeholder implementation\n    # In a real scenario, this would involve accessing the model's attention matrices and gradients\n    # Here, we'll use dummy values for demonstration purposes\n    Iqp = np.random.rand()\n    Iqr = np.random.rand()\n    Ipr = np.random.rand()\n    logger.debug(f\"Saliency scores for prompt '{prompt}': Iqp={Iqp}, Iqr={Iqr}, Ipr={Ipr}\")\n    return Iqp, Iqr, Ipr\n\ndef synthesize_saliency_score(Iqp: float, Iqr: float, Ipr: float) -> float:\n    \"\"\"Synthesizes the saliency score based on Iqp, Iqr, Ipr.\"\"\"\n    lambda1 = 0.4\n    lambda2 = 0.3\n    lambda3 = 0.3\n    S = lambda1 * Iqp + lambda2 * Iqr + lambda3 * Ipr\n    logger.debug(f\"Synthesized saliency score: {S}\")\n    return S\n\ndef chat_interface():\n    load_settings()\n\n    # Initialize session state attributes with default values if not present\n    if \"chat_mode\" not in st.session_state:\n        st.session_state.chat_mode = \"general\"  # Can be 'general' or 'code'\n    if \"workspace_enabled\" not in st.session_state:\n        st.session_state.workspace_enabled = False\n    if \"cot_top_n\" not in st.session_state:\n        st.session_state.cot_top_n = 3  # Default value for top N prompts in IAP-mv\n    if \"cot_strategy\" not in st.session_state:\n        st.session_state.cot_strategy = \"IAP-ss\"  # Default strategy\n    if \"cot_threshold\" not in st.session_state:\n        st.session_state.cot_threshold = 0.5  # Default threshold for IAP-ss\n    if \"instance_adaptive_cot_enabled\" not in st.session_state:\n        st.session_state.instance_adaptive_cot_enabled = False  # Default to disabled\n\n    # Initialize other attributes if not present\n    if \"chat_history\" not in st.session_state:\n        st.session_state.chat_history = []\n    if \"agent_type\" not in st.session_state:\n        st.session_state.agent_type = \"None\"\n    if \"workspace_items\" not in st.session_state:\n        st.session_state.workspace_items = []\n    if \"chat_input\" not in st.session_state:\n        st.session_state.chat_input = \"\"\n    if \"suggested_prompt\" not in st.session_state:\n        st.session_state.suggested_prompt = \"\"\n    if \"show_prompt_modal\" not in st.session_state:\n        st.session_state.show_prompt_modal = False\n    if \"episodic_memory_enabled\" not in st.session_state:\n        st.session_state.episodic_memory_enabled = False\n    if \"advanced_thinking_enabled\" not in st.session_state:\n        st.session_state.advanced_thinking_enabled = False  # Initialize Advanced Thinking\n    if \"thinking_steps\" not in st.session_state:\n        st.session_state.thinking_steps = [\n            \"1. Analyzing the problem\",\n            \"2. Breaking down into subtasks\",\n            \"3. Exploring potential solutions\",\n            \"4. Evaluating approaches\",\n            \"5. Formulating a comprehensive answer\"\n        ]  # Default thinking steps\n    if \"model_memory_handler\" not in st.session_state:\n        st.session_state.model_memory_handler = ModelMemoryHandler(\"ollama\")\n    if \"groq_client\" not in st.session_state:\n        api_keys = load_api_keys()\n        groq_key = api_keys.get(\"groq_api_key\")\n        st.session_state.groq_client = get_groq_client(groq_key)\n        if not groq_key:\n            st.session_state.providers = {\"ollama\": True, \"groq\": False}\n        else:\n            st.session_state.providers = {\"ollama\": True, \"groq\": True}\n\n    # Ensure 'selected_model' is initialized with Ollama as default\n    if \"selected_model\" not in st.session_state or st.session_state.selected_model is None:\n        available_models = get_available_models()\n        if available_models:\n            # Default to an Ollama model\n            ollama_models = [m for m in available_models if not m.startswith((\"groq/\", \"openai/\", \"mistral/\"))]\n            if ollama_models:\n                st.session_state.selected_model = ollama_models[0]\n            else:\n                st.session_state.selected_model = available_models[0]\n            logger.info(f\"Initialized selected_model to default: {st.session_state.selected_model}\")\n        else:\n            st.session_state.selected_model = None\n            logger.warning(\"No available models found to initialize selected_model.\")\n    \n    st.session_state.agent_type = st.session_state.get(\"agent_type\", \"None\")\n    st.session_state.metacognitive_type = st.session_state.get(\"metacognitive_type\", \"None\")\n    st.session_state.voice_type = st.session_state.get(\"voice_type\", \"None\")\n\n    st.session_state.selected_corpus = st.session_state.get(\"selected_corpus\", \"None\")\n    st.session_state.temperature_slider_chat = st.session_state.get(\"temperature_slider_chat\", 0.7)\n    st.session_state.max_tokens_slider_chat = min(st.session_state.get(\"max_tokens_slider_chat\", 4000), 16000) \n    st.session_state.presence_penalty_slider_chat = st.session_state.get(\"presence_penalty_slider_chat\", 0.0)\n    st.session_state.frequency_penalty_slider_chat = st.session_state.get(\"frequency_penalty_slider_chat\", 0.0)\n\n    if \"total_tokens\" not in st.session_state:\n        st.session_state.total_tokens = 0\n\n    # Initialize episodic memory\n    if \"episodic_memory\" not in st.session_state:\n        st.session_state.episodic_memory = EpisodicMemory()\n\n    with st.sidebar:\n        with st.expander(\"🤖 Chat Agent Settings\", expanded=False):\n            # Add chat mode selector\n            st.session_state.chat_mode = st.selectbox(\n                \"💭 Chat Mode:\",\n                [\"general\", \"code\"],\n                index=0 if st.session_state.chat_mode == \"general\" else 1\n            )\n            \n            # Only show workspace toggle in code mode\n            if st.session_state.chat_mode == \"code\":\n                st.session_state.workspace_enabled = st.checkbox(\n                    \"Enable Workspace Context\",\n                    value=st.session_state.workspace_enabled\n                )\n            \n            # Get available models and their info\n            available_models = get_all_models()\n            \n            # Connect to the models database\n            conn = sqlite3.connect('ollama_models.db')\n            cursor = conn.cursor()\n            \n            # Get model descriptions\n            model_descriptions = {}\n            for model in available_models:\n                cursor.execute('SELECT description, capabilities FROM models WHERE model_name = ?', (model,))\n                result = cursor.fetchone()\n                if result:\n                    desc, caps = result\n                    model_descriptions[model] = f\"{desc}\\n\\nCapabilities: {caps}\" if caps else desc\n                else:\n                    model_descriptions[model] = \"An Ollama model\"\n            \n            conn.close()\n            \n            # Show model selector with descriptions\n            st.session_state.selected_model = st.selectbox(\n                \"📦 Model:\",\n                available_models,\n                index=available_models.index(st.session_state.selected_model) if st.session_state.selected_model in available_models else 0,\n                help=model_descriptions.get(st.session_state.selected_model, \"An Ollama model\")\n            )\n            agent_prompts = get_agent_prompt()\n            agent_types = [\"None\"] + list(agent_prompts.keys())\n            agent_type_descriptions = {\n                \"None\": \"No special agent behavior\",\n                **{k: v.get(\"description\", v.get(\"prompt\", \"\")[:100] + \"...\") for k, v in agent_prompts.items()}\n            }\n            st.session_state.agent_type = st.selectbox(\n                \"🧑‍🔧 Agent Type:\",\n                agent_types,\n                index=agent_types.index(st.session_state.agent_type),\n                help=agent_type_descriptions.get(st.session_state.agent_type, \"\")\n            )\n            metacognitive_types = [\"None\"] + list(get_metacognitive_prompt().keys())\n            st.session_state.metacognitive_type = st.selectbox(\"🧠 Metacognitive Type:\", metacognitive_types, index=metacognitive_types.index(st.session_state.metacognitive_type))\n            voice_options = [\"None\"] + list(get_voice_prompt().keys())\n            st.session_state.voice_type = st.selectbox(\"🗣️ Voice Type:\", voice_options, index=voice_options.index(st.session_state.voice_type) if st.session_state.voice_type in voice_options else 0)\n            corpus_options = [\"None\"] + [d for d in os.listdir(RAGTEST_DIR) if os.path.isdir(os.path.join(RAGTEST_DIR, d))]\n            st.session_state.selected_corpus = st.selectbox(\"📚 Corpus:\", corpus_options, index=corpus_options.index(st.session_state.selected_corpus) if st.session_state.selected_corpus in corpus_options else 0)\n            st.button(\"💾 Save Settings\", key=\"save_settings_general\", on_click=save_settings)\n\n        # Advanced Settings\n        with st.expander(\"🛠️ Advanced Settings\", expanded=False):\n            st.session_state.temperature_slider_chat = st.slider(\n                \"🌡️ Temperature\",\n                min_value=0.0,\n                max_value=1.0,\n                value=st.session_state.temperature_slider_chat,\n                step=0.1\n            )\n            st.session_state.max_tokens_slider_chat = st.slider(\n                \"📊 Max Tokens\",\n                min_value=1000,\n                max_value=16000,\n                value=st.session_state.max_tokens_slider_chat,\n                step=1000\n            )  # Enforce max token limit\n            st.session_state.presence_penalty_slider_chat = st.slider(\n                \"🚫 Presence Penalty\",\n                min_value=-2.0,\n                max_value=2.0,\n                value=st.session_state.presence_penalty_slider_chat,\n                step=0.1\n            )\n            st.session_state.frequency_penalty_slider_chat = st.slider(\n                \"🔁 Frequency Penalty\",\n                min_value=-2.0,\n                max_value=2.0,\n                value=st.session_state.frequency_penalty_slider_chat,\n                step=0.1\n            )\n            st.session_state.episodic_memory_enabled = st.checkbox(\n                \"Enable Episodic Memory\",\n                value=st.session_state.episodic_memory_enabled\n            )\n            st.session_state.advanced_thinking_enabled = st.checkbox(\n                \"Enable Advanced Thinking\",\n                value=st.session_state.advanced_thinking_enabled  # Added Advanced Thinking checkbox\n            )\n\n            # Instance-Adaptive Zero-Shot CoT Prompting Settings\n            st.markdown(\"### Instance-Adaptive Zero-Shot CoT Prompting\")\n            st.session_state.instance_adaptive_cot_enabled = st.checkbox(\n                \"Enable Instance-Adaptive Zero-Shot CoT Prompting\",\n                value=st.session_state.get(\"instance_adaptive_cot_enabled\", False)\n            )\n            if st.session_state.instance_adaptive_cot_enabled:\n                cot_strategies = [\"IAP-ss\", \"IAP-mv\"]\n                st.session_state.cot_strategy = st.selectbox(\n                    \"📋 CoT Prompting Strategy:\",\n                    cot_strategies,\n                    index=cot_strategies.index(st.session_state.get(\"cot_strategy\", \"IAP-ss\")) if st.session_state.get(\"cot_strategy\", \"IAP-ss\") in cot_strategies else 0\n                )\n                if st.session_state.cot_strategy == \"IAP-ss\":\n                    st.session_state.cot_threshold = st.slider(\n                        \"🔍 Saliency Score Threshold:\",\n                        min_value=0.0,\n                        max_value=1.0,\n                        value=st.session_state.get(\"cot_threshold\", 0.5),\n                        step=0.05\n                    )\n                elif st.session_state.cot_strategy == \"IAP-mv\":\n                    st.session_state.cot_top_n = st.slider(\n                        \"🔝 Number of Top Prompts:\",\n                        min_value=1,\n                        max_value=len(CANDIDATE_PROMPTS),\n                        value=st.session_state.get(\"cot_top_n\", 3),\n                        step=1\n                    )\n\n            # Configurable Thinking Steps\n            st.markdown(\"### Configurable Thinking Steps\")\n            default_steps = \"\\n\".join(st.session_state.thinking_steps)\n            new_thinking_steps = st.text_area(\n                \"Enter thinking steps (one per line):\",\n                value=default_steps,\n                height=150\n            )\n            st.session_state.thinking_steps = [step.strip() for step in new_thinking_steps.split('\\n') if step.strip()]\n            \n            st.button(\"💾 Save Settings\", key=\"save_settings_advanced\", on_click=save_settings)\n\n        with st.expander(\"📁 Saved Chats\", expanded=False):\n            manage_saved_chats()\n\n        if st.button(\"📥 Save Chat\"):\n            save_chat_and_workspace()\n\n    # Define the tabs *before* they are used\n    chat_tab, workspace_tab = st.tabs([\"💬 Chat\", \"📜 Workspace\"])\n\n    with chat_tab:\n        # Initialize placeholders for each message at the start\n        message_placeholders = []\n        for i, message in enumerate(st.session_state.chat_history):\n            placeholder = st.empty()\n            message_placeholders.append(placeholder)\n            \n            with placeholder.container():\n                with st.chat_message(message[\"role\"]):\n                    if message[\"role\"] == \"assistant\":\n                        if message.get(\"content\"):\n                            # Add TTS button first if this is the last message and not a code block\n                            code_blocks, article_blocks = extract_content_blocks(message[\"content\"])\n                            button_col, content_col = st.columns([1, 20])\n                            \n                            if not code_blocks and i == len(st.session_state.chat_history) - 1:\n                                with button_col:\n                                    if st.button(\"🔊\", key=f\"speak_button_{i}\", help=\"Click to hear this response\"):\n                                        try:\n                                            agent_prompts = get_agent_prompt()\n                                            model_voice = agent_prompts.get(st.session_state.agent_type, {}).get('model_voice', 'en-US-Wavenet-A')\n                                            speech_file = text_to_speech(message[\"content\"].strip(), voice=model_voice)\n                                            play_speech(speech_file)\n                                        except Exception as e:\n                                            st.error(f\"Error generating or playing speech: {str(e)}\")\n                            \n                            # Display the message content\n                            with content_col:\n                                for code_block in code_blocks:\n                                    st.code(code_block)\n                                non_code_parts = re.split(r'```[\\s\\S]*?```', message[\"content\"])\n                                for part in non_code_parts:\n                                    if part.strip():\n                                        st.markdown(part.strip())\n                        else:\n                            st.warning(\"This message has no content.\")\n                    else:\n                        if message.get(\"content\"):\n                            st.markdown(message[\"content\"])\n                        else:\n                            st.warning(\"This message has no content.\")\n        \n        with bottom():\n            col1, col2 = st.columns([1, 20])\n            with col1:\n                if st.button(\"✨\", key=\"prompt_helper\", help=\"Need help writing a prompt?\"):\n                    st.session_state.show_prompt_modal = True\n                    st.rerun()\n            with col2:\n                user_input = st.chat_input(\"What is up my person?\")\n\n        if st.session_state.get(\"show_prompt_modal\", False):\n            ai_assisted_prompt_writing()\n\n        if st.session_state.chat_input:\n            user_input = st.session_state.chat_input\n            st.session_state.chat_input = \"\"\n        \n        if user_input:\n            api_keys = load_api_keys()\n            st.session_state.chat_history.append({\"role\": \"user\", \"content\": user_input})\n            st.session_state.total_tokens += count_tokens(user_input)\n            logger.info(f\"User input received: {user_input}\")\n\n            agent_prompt = construct_agent_prompt(\n                st.session_state.agent_type,\n                st.session_state.metacognitive_type,\n                st.session_state.voice_type\n            )\n\n            chat_history = \"\\n\".join([f\"{msg['role'].capitalize()}: {msg['content']}\" for msg in st.session_state.chat_history[-5:]])\n\n            corpus_context = \"\"\n            if st.session_state.selected_corpus != \"None\":\n                graph_response = get_graphrag_context(user_input, st.session_state.selected_corpus)\n                if graph_response:\n                    corpus_context = f\"\\nRelevant context from the knowledge base:\\n{graph_response}\\n\"\n                else:\n                    st.warning(f\"No relevant context found in the corpus '{st.session_state.selected_corpus}'. Proceeding without additional context.\")\n                    logger.warning(f\"No relevant context found in corpus '{st.session_state.selected_corpus}'.\")\n\n            episodic_context = \"\"\n            if st.session_state.episodic_memory_enabled:\n                try:\n                    chat_history_text = \"\\n\".join([msg['content'] for msg in st.session_state.chat_history])\n                    st.session_state.model_memory_handler.segment_text(st.session_state.selected_model, chat_history_text, api_keys)\n                    query_embedding = get_token_embeddings(st.session_state.selected_model, user_input, api_keys)\n                    \n                    if query_embedding.size > 0:\n                        retrieved_events = st.session_state.model_memory_handler.retrieve_events(query_embedding)\n                        for event in retrieved_events:\n                            if event['text'] is not None:\n                                episodic_context += f\" {event['text']}\"\n                    else:\n                        st.warning(\"Failed to generate embeddings for episodic memory. Proceeding without episodic context.\")\n                        logger.warning(\"Failed to generate embeddings for episodic memory.\")\n                except Exception as e:\n                    st.error(f\"Error in episodic memory processing: {str(e)}. Proceeding without episodic context.\")\n                    logger.error(f\"Error in episodic memory processing: {e}\")\n\n            # Get webpage context from query parameters using st.query_params\n            query_params = st.query_params  \n            web_page_url = query_params.get('web_page_url', [''])[0]\n            is_extension = query_params.get('extension', ['false'])[0].lower() == 'true'\n\n            # Construct the initial prompt\n            if is_extension and web_page_url:\n                initial_prompt = f\"You are an AI assistant working within a browser extension. The user is currently on the webpage: {web_page_url}. How can I help you with information related to this page?\\n\\n\"\n            else:\n                initial_prompt = \"You are an AI assistant. How can I help you today?\\n\\n\"\n\n            # Initialize chat history if empty\n            if \"chat_history\" not in st.session_state or not st.session_state.chat_history:\n                st.session_state.chat_history = [{\"role\": \"assistant\", \"content\": initial_prompt}]\n                logger.info(\"Initialized chat history with initial prompt.\")\n\n            # Construct the final prompt\n            final_prompt = \"\"\n            \n            # Conditionally add browser extension meta-context\n            if is_extension and web_page_url:\n                final_prompt += \"You are an AI assistant working within a browser extension. You have access to the current web page's content. Please use this information to answer the user's question.\\n\\n\"\n                # Assuming web_page_content is defined elsewhere or fetched as needed\n                web_page_content = query_params.get('web_page_content', [''])[0]\n                final_prompt += f\"Webpage URL: {web_page_url}\\nWebpage Content:\\n{web_page_content}\\n\\n\" \n\n            final_prompt += f\"\"\"\n{agent_prompt}\n\nRecent conversation history:\n{chat_history}\n\n{corpus_context}\n\nEpisodic Memory Context:\n{episodic_context}\n\nHuman: {user_input}\n\nAssistant: Let me address your request based on the information provided and my capabilities.\n\"\"\"\n\n            # Apply Instance-Adaptive Zero-Shot CoT Prompting if enabled\n            if st.session_state.instance_adaptive_cot_enabled:\n                final_prompt = instance_adaptive_cot(final_prompt, st.session_state.selected_model, api_keys)\n                if final_prompt is None:\n                    final_prompt = f\"{agent_prompt}\\n\\nRecent conversation history:\\n{chat_history}\\n\\n{corpus_context}\\n\\nEpisodic Memory Context:\\n{episodic_context}\\n\\nHuman: {user_input}\\n\\nAssistant: Let me address your request based on the information provided and my capabilities.\\n\\n\"\n\n            st.session_state.total_tokens += count_tokens(final_prompt)\n            logger.info(f\"Constructed final prompt with total tokens: {st.session_state.total_tokens}\")\n\n            if st.session_state.advanced_thinking_enabled:\n                with st.sidebar.expander(\"🧠 Advanced Thinking Process\", expanded=True):\n                    thinking_placeholder = st.empty()\n                    progress_bar = st.progress(0)\n                    thinking_result_placeholder = st.empty()\n\n                    # Initialize session state for advanced thinking\n                    if \"advanced_thinking_step_index\" not in st.session_state:\n                        st.session_state.advanced_thinking_step_index = 0\n                        st.session_state.advanced_thinking_result = \"\"\n\n                    total_steps = len(st.session_state.thinking_steps)\n\n                    # Process each thinking step sequentially\n                    for step_index, step in enumerate(st.session_state.thinking_steps):\n                        # Update progress bar\n                        progress = (step_index) / total_steps\n                        progress_bar.progress(progress)\n\n                        # Display current thinking result\n                        thinking_result_placeholder.markdown(st.session_state.advanced_thinking_result)\n\n                        # Process the current step\n                        step_result = advanced_thinking_step(\n                            prompt=final_prompt,\n                            model=st.session_state.selected_model,\n                            api_keys=api_keys,\n                            step=step\n                        )\n\n                        # Append the result to the session state\n                        st.session_state.advanced_thinking_result += step_result\n\n                        # Allow the UI to update\n                        thinking_result_placeholder.markdown(st.session_state.advanced_thinking_result)\n                        progress = (step_index + 1) / total_steps\n                        progress_bar.progress(progress)\n                        time.sleep(0.1)  # Optional: Simulate delay for better visualization\n\n                    # Final update after all steps\n                    thinking_placeholder.text(\"Advanced thinking process completed.\")\n                    logger.info(\"Advanced thinking process completed.\")\n\n            with st.chat_message(\"assistant\"):\n                message_placeholder = st.empty()\n                full_response = \"\"\n\n                try:\n                    if st.session_state.selected_model in OPENAI_MODELS:\n                        response = call_openai_api(\n                            model=st.session_state.selected_model,\n                            messages=[{\"role\": \"user\", \"content\": final_prompt}],\n                            temperature=st.session_state.temperature_slider_chat,\n                            max_tokens=min(st.session_state.max_tokens_slider_chat, 16000),\n                            openai_api_key=api_keys.get(\"openai_api_key\"),\n                            stream=True\n                        )\n                        for chunk in response:\n                            if chunk.choices[0].delta.content is not None:\n                                full_response += chunk.choices[0].delta.content\n                                message_placeholder.markdown(full_response + \"▌\")\n                                st.session_state.total_tokens += count_tokens(chunk.choices[0].delta.content)\n                        message_placeholder.markdown(full_response)\n                        \n                        # Add response to history and rerun for OpenAI\n                        st.session_state.chat_history.append({\"role\": \"assistant\", \"content\": full_response})\n                        st.rerun()\n                        \n                    elif st.session_state.selected_model in GROQ_MODELS:\n                        full_response = call_groq_api(\n                            client=st.session_state.groq_client,\n                            model=st.session_state.selected_model,\n                            messages=[{\"role\": \"user\", \"content\": final_prompt}],\n                            temperature=st.session_state.temperature_slider_chat,\n                            max_tokens=min(st.session_state.max_tokens_slider_chat, 8000)\n                        )\n                        message_placeholder.markdown(full_response)\n                        \n                        # Add response to history and rerun for Groq\n                        st.session_state.chat_history.append({\"role\": \"assistant\", \"content\": full_response})\n                        st.rerun()\n                        \n                    elif st.session_state.selected_model in MISTRAL_MODELS:\n                        full_response = call_mistral_api(\n                            model=st.session_state.selected_model,\n                            prompt=final_prompt,\n                            temperature=st.session_state.temperature_slider_chat,\n                            max_tokens=min(st.session_state.max_tokens_slider_chat, 8000),\n                            mistral_api_key=api_keys.get(\"mistral_api_key\")\n                        )\n                        message_placeholder.markdown(full_response)\n                        \n                        # Add response to history and rerun for Mistral\n                        st.session_state.chat_history.append({\"role\": \"assistant\", \"content\": full_response})\n                        st.rerun()\n                        \n                    else:\n                        for response_chunk in ollama.generate(\n                            st.session_state.selected_model,\n                            final_prompt,\n                            stream=True,\n                            options={\n                                \"temperature\": st.session_state.temperature_slider_chat,\n                                \"num_predict\": min(st.session_state.max_tokens_slider_chat, 16000),\n                                \"presence_penalty\": st.session_state.presence_penalty_slider_chat,\n                                \"frequency_penalty\": st.session_state.frequency_penalty_slider_chat,\n                            }\n                        ):\n                            content = response_chunk[\"response\"]\n                            full_response += content\n                            message_placeholder.markdown(full_response + \"▌\")\n                            st.session_state.total_tokens += count_tokens(content)\n\n                        message_placeholder.markdown(full_response)\n                        \n                        # Add response to history and rerun for Ollama\n                        st.session_state.chat_history.append({\"role\": \"assistant\", \"content\": full_response})\n                        st.rerun()\n\n                except Exception as e:\n                    error_message = f\"Error: {str(e)}\"\n                    message_placeholder.error(error_message)\n                    logger.error(error_message)\n                    st.session_state.chat_history.append({\"role\": \"assistant\", \"content\": error_message})\n                    st.rerun()\n\n                st.session_state.total_tokens += count_tokens(full_response)\n                st.rerun()\n\n                # Extract and save content blocks\n                code_blocks, article_blocks = extract_content_blocks(full_response)\n\n                for code_block in code_blocks:\n                    st.session_state.workspace_items.append({\n                        \"type\": \"code\",\n                        \"content\": code_block,\n                        \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n                    })\n\n                for article_block in article_blocks:\n                    st.session_state.workspace_items.append({\n                        \"type\": \"article\",\n                        \"content\": article_block,\n                        \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n                    })\n\n                if code_blocks or article_blocks:\n                    st.success(f\"{len(code_blocks)} code block(s) and {len(article_blocks)} article(s) automatically saved to Workspace\")\n                    logger.info(f\"Saved {len(code_blocks)} code blocks and {len(article_blocks)} articles to Workspace.\")\n\n    with workspace_tab:\n        for index, item in enumerate(st.session_state.workspace_items):\n            with st.expander(f\"Item {index + 1} - {item['timestamp']}\"):\n                if item['type'] == 'code':\n                    st.code(item['content'])\n                elif item['type'] == 'article':\n                    lines = item['content'].split('\\n')\n                    st.subheader(lines[0].replace('Title:', '').strip())\n                    st.markdown('\\n'.join(lines[1:]))\n                else:\n                    st.write(item['content'])\n                if st.button(f\"Remove Item {index + 1}\"):\n                    st.session_state.workspace_items.pop(index)\n                    logger.info(f\"Removed item {index + 1} from Workspace.\")\n                    st.rerun()\n\n        new_item = st.text_area(\"Add a new item to the workspace:\", key=\"new_workspace_item\")\n        if st.button(\"✚ Add to Workspace\"):\n            if new_item:\n                st.session_state.workspace_items.append({\n                    \"type\": \"text\",\n                    \"content\": new_item,\n                    \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n                })\n                st.success(\"New item added to Workspace\")\n                logger.info(\"Added a new text item to Workspace.\")\n                st.rerun()\n\n    # Update model memory handler when model is changed\n    if st.session_state.selected_model != st.session_state.get(\"previous_model\"):\n        if st.session_state.selected_model in OPENAI_MODELS:\n            st.session_state.model_memory_handler = ModelMemoryHandler(\"openai\")\n        elif st.session_state.selected_model in GROQ_MODELS:\n            st.session_state.model_memory_handler = ModelMemoryHandler(\"groq\")\n        elif st.session_state.selected_model in MISTRAL_MODELS:\n            st.session_state.model_memory_handler = ModelMemoryHandler(\"mistral\")\n        else:\n            st.session_state.model_memory_handler = ModelMemoryHandler(\"ollama\")\n        st.session_state.previous_model = st.session_state.selected_model\n        logger.info(f\"Switched model to {st.session_state.selected_model}\")\n\ndef count_tokens(text):\n    encoding = tiktoken.get_encoding(\"cl100k_base\")\n    return len(encoding.encode(text))\n\ndef extract_code_blocks(text):\n    if text is None:\n        return [], []\n    code_blocks = re.findall(r'```[\\s\\S]*?```', text)\n    article_blocks = re.findall(r'^Title:.*?(?=\\n^Title:|\\Z)', text, re.MULTILINE | re.DOTALL)\n    return [block.strip('`').strip() for block in code_blocks], [block.strip() for block in article_blocks]\n\ndef save_chat_and_workspace():\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    default_filename = f\"{timestamp}\"\n    chat_name = st.text_input(\"Enter a name for the save:\", value=default_filename, key=\"save_chat_name\")\n    if chat_name and st.button(\"Confirm Save\"):\n        save_data = {\n            \"chat_history\": st.session_state.chat_history,\n            \"workspace_items\": st.session_state.workspace_items,\n            \"total_tokens\": st.session_state.total_tokens,\n            \"thinking_steps\": st.session_state.thinking_steps,  # Save thinking steps\n            \"instance_adaptive_cot_enabled\": st.session_state.get(\"instance_adaptive_cot_enabled\", False),\n            \"cot_strategy\": st.session_state.get(\"cot_strategy\", \"IAP-ss\"),\n            \"cot_threshold\": st.session_state.get(\"cot_threshold\", 0.5),\n            \"cot_top_n\": st.session_state.get(\"cot_top_n\", 3)\n        }\n        sessions_folder = \"sessions\"\n        if not os.path.exists(sessions_folder):\n            os.makedirs(sessions_folder)\n        file_path = os.path.join(sessions_folder, chat_name + \".json\")\n        try:\n            with open(file_path, \"w\") as f:\n                json.dump(save_data, f)\n            st.success(f\"Chat and Workspace saved to {chat_name}\")\n            logger.info(f\"Chat and Workspace saved to {chat_name}.json\")\n            st.rerun()\n        except Exception as e:\n            st.error(f\"Failed to save chat: {e}\")\n            logger.error(f\"Failed to save chat: {e}\")\n\ndef manage_saved_chats():\n    st.sidebar.subheader(\"Saved Chats and Workspaces\")\n    sessions_folder = \"sessions\"\n    if not os.path.exists(sessions_folder):\n        os.makedirs(sessions_folder)\n    saved_files = [f for f in os.listdir(sessions_folder) if f.endswith(\".json\")]\n\n    if \"rename_file\" not in st.session_state:\n        st.session_state.rename_file = None\n\n    for file in saved_files:\n        col1, col2, col3 = st.sidebar.columns([3, 1, 1])\n        with col1:\n            file_name = os.path.splitext(file)[0]\n            if st.button(file_name, key=f\"load_{file}\"):\n                load_chat_and_workspace(os.path.join(sessions_folder, file))\n        with col2:\n            if st.button(\"✏️\", key=f\"rename_{file}\"):\n                st.session_state.rename_file = file\n                st.rerun()\n        with col3:\n            if st.button(\"🗑️\", key=f\"delete_{file}\"):\n                delete_chat_and_workspace(os.path.join(sessions_folder, file))\n\n    if st.session_state.rename_file:\n        rename_chat_and_workspace(st.session_state.rename_file, sessions_folder)\n\ndef load_chat_and_workspace(file_path):\n    try:\n        with open(file_path, \"r\") as f:\n            loaded_data = json.load(f)\n        st.session_state.chat_history = loaded_data.get(\"chat_history\", [])\n        st.session_state.workspace_items = loaded_data.get(\"workspace_items\", [])\n        st.session_state.total_tokens = loaded_data.get(\"total_tokens\", 0)\n        st.session_state.thinking_steps = loaded_data.get(\"thinking_steps\", st.session_state.thinking_steps)\n        st.session_state.instance_adaptive_cot_enabled = loaded_data.get(\"instance_adaptive_cot_enabled\", False)\n        st.session_state.cot_strategy = loaded_data.get(\"cot_strategy\", \"IAP-ss\")\n        st.session_state.cot_threshold = loaded_data.get(\"cot_threshold\", 0.5)\n        st.session_state.cot_top_n = loaded_data.get(\"cot_top_n\", 3)\n        st.success(f\"Loaded {os.path.basename(file_path)}\")\n        logger.info(f\"Loaded chat and workspace from {file_path}\")\n        st.rerun()\n    except Exception as e:\n        st.error(f\"Failed to load chat: {e}\")\n        logger.error(f\"Failed to load chat from {file_path}: {e}\")\n\ndef delete_chat_and_workspace(file_path):\n    try:\n        os.remove(file_path)\n        st.success(f\"File {os.path.basename(file_path)} deleted.\")\n        logger.info(f\"Deleted file {file_path}\")\n        st.rerun()\n    except Exception as e:\n        st.error(f\"Failed to delete file: {e}\")\n        logger.error(f\"Failed to delete file {file_path}: {e}\")\n\ndef rename_chat_and_workspace(file_to_rename, sessions_folder):\n    current_name = os.path.splitext(file_to_rename)[0]\n    new_name = st.sidebar.text_input(\"Rename file:\", value=current_name, key=\"rename_file_input\")\n    if st.sidebar.button(\"Confirm Rename\"):\n        if new_name and new_name != current_name:\n            old_file_path = os.path.join(sessions_folder, file_to_rename)\n            new_file_path = os.path.join(sessions_folder, new_name + \".json\")\n            try:\n                if not os.path.exists(new_file_path):\n                    os.rename(old_file_path, new_file_path)\n                    st.sidebar.success(f\"File renamed to {new_name}\")\n                    logger.info(f\"Renamed file from {file_to_rename} to {new_name}.json\")\n                    st.session_state.rename_file = None\n                    st.rerun()\n                else:\n                    st.sidebar.error(\"A file with the new name already exists.\")\n            except Exception as e:\n                st.sidebar.error(f\"Failed to rename file: {e}\")\n                logger.error(f\"Failed to rename file from {file_to_rename} to {new_name}.json: {e}\")\n        else:\n            st.sidebar.error(\"Please enter a new name different from the current one.\")\n    \n    if st.sidebar.button(\"Cancel\"):\n        st.session_state.rename_file = None\n        st.rerun()\n\nif __name__ == \"__main__\":\n    chat_interface()\n"}
{"type": "source_file", "path": "find_imports.py", "content": "#!/usr/bin/env python\n\"\"\"\nScript to find all imports in Python files and check if they're installed.\n\"\"\"\n\nimport os\nimport re\nimport sys\nimport importlib\nimport subprocess\nfrom collections import defaultdict\n\ndef find_python_files(directory):\n    \"\"\"Find all Python files in the given directory.\"\"\"\n    python_files = []\n    for root, _, files in os.walk(directory):\n        for file in files:\n            if file.endswith('.py'):\n                python_files.append(os.path.join(root, file))\n    return python_files\n\ndef extract_imports(file_path):\n    \"\"\"Extract all import statements from a Python file.\"\"\"\n    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n        content = f.read()\n    \n    # Find all import statements\n    import_pattern = re.compile(r'^(?:from\\s+([.\\w]+)\\s+import\\s+.*|import\\s+([.\\w,\\s]+))', re.MULTILINE)\n    matches = import_pattern.findall(content)\n    \n    imports = []\n    for match in matches:\n        if match[0]:  # from X import Y\n            module = match[0].split('.')[0]\n            if module:\n                imports.append(module)\n        else:  # import X, Y, Z\n            modules = match[1].split(',')\n            for module in modules:\n                module = module.strip().split('.')[0]\n                if module:\n                    imports.append(module)\n    \n    return imports\n\ndef check_import(module_name):\n    \"\"\"Check if a module can be imported.\"\"\"\n    if not module_name:\n        return True  # Skip empty module names\n    try:\n        importlib.import_module(module_name)\n        return True\n    except ImportError:\n        return False\n\ndef map_module_to_package(module_name):\n    \"\"\"Map module name to package name for pip install.\"\"\"\n    mapping = {\n        'sklearn': 'scikit-learn',\n        'bs4': 'beautifulsoup4',\n        'PIL': 'pillow',\n        'yaml': 'pyyaml',\n        'googleapiclient': 'google-api-python-client',\n        'cv2': 'opencv-python',\n        'dotenv': 'python-dotenv',\n        'matplotlib.pyplot': 'matplotlib',\n        'IPython': 'ipython',\n        'google.auth': 'google-auth',\n        'google.oauth2': 'google-auth-oauthlib',\n        'google_auth_oauthlib': 'google-auth-oauthlib',\n        'google.cloud': 'google-cloud-storage',\n        'jwt': 'pyjwt',\n        'yaml': 'pyyaml',\n        'GPUtil': 'gputil',\n    }\n    return mapping.get(module_name, module_name)\n\ndef main():\n    \"\"\"Main function.\"\"\"\n    directory = os.getcwd()\n    python_files = find_python_files(directory)\n    \n    all_imports = set()\n    file_imports = defaultdict(set)\n    \n    print(f\"Scanning {len(python_files)} Python files for imports...\")\n    \n    for file_path in python_files:\n        try:\n            imports = extract_imports(file_path)\n            all_imports.update(imports)\n            file_imports[os.path.basename(file_path)].update(imports)\n        except Exception as e:\n            print(f\"Error processing {file_path}: {e}\")\n    \n    # Filter out standard library modules\n    standard_libs = set(sys.builtin_module_names)\n    try:\n        standard_libs.update(os.listdir(os.path.dirname(os.__file__)))\n    except (PermissionError, FileNotFoundError):\n        pass\n    \n    # Add some common standard library modules that might not be in the above list\n    standard_libs.update(['os', 'sys', 're', 'json', 'time', 'datetime', 'math', 'random', 'collections', 'typing'])\n    \n    third_party_imports = all_imports - standard_libs\n    \n    # Check which imports are missing\n    missing_imports = []\n    for module in third_party_imports:\n        if module and not check_import(module):\n            missing_imports.append(module)\n    \n    # Generate installation commands\n    if missing_imports:\n        print(\"\\nMissing imports found:\")\n        for module in sorted(missing_imports):\n            package = map_module_to_package(module)\n            print(f\"  {module} -> {package}\")\n        \n        print(\"\\nInstallation commands:\")\n        for module in sorted(missing_imports):\n            package = map_module_to_package(module)\n            print(f\"pip install {package}\")\n        \n        print(\"\\nFiles with missing imports:\")\n        for file, imports in file_imports.items():\n            missing_in_file = [imp for imp in imports if imp in missing_imports]\n            if missing_in_file:\n                print(f\"  {file}: {', '.join(missing_in_file)}\")\n    else:\n        print(\"All imports are satisfied!\")\n\nif __name__ == \"__main__\":\n    main()"}
{"type": "source_file", "path": "info_brainstorm.py", "content": "# info_brainstorm.py\nimport streamlit as st\n\ndef display_info_brainstorm():\n    with st.expander(\"❓ About Brainstorm\", expanded=False):\n        st.html(\"\"\"<img src='https://2acrestudios.com/wp-content/uploads/2024/05/00017-1652154938.png'  style='max-width: 200px;' />\"\"\")\n        st.markdown(\n        \"\"\"\n        ## How It Works\n        \n        Brainstorm is an advanced collaborative AI feature within Ollama Workbench that allows users to create, manage, and interact with multiple AI agents in a structured workflow. Here's how it works:\n\n        1. **Agent Management**: Users can create and customize multiple AI agents, each with unique characteristics such as:\n           - Name and emoji for easy identification\n           - Underlying language model\n           - Specialized roles (e.g., creative writer, code expert, analyst)\n           - Personality traits and communication styles\n           - Knowledge bases or specific data sets to draw from\n\n        2. **Workflow Creation**: Users can design workflows by selecting a sequence of agents to respond to prompts. This allows for a multi-perspective approach to problem-solving or idea generation.\n\n        3. **Interactive Sessions**: During a brainstorming session, users can input prompts or questions, and the selected agents will respond in the predetermined sequence, building upon each other's ideas and insights.\n\n        4. **Workflow Management**: Users can save, load, and modify workflows, making it easy to reuse effective agent combinations for different projects or topics.\n\n        5. **Dynamic Adjustment**: During a session, users can adjust the number of agents, change the sequence, or modify agent settings to optimize the brainstorming process.\n\n        6. **Conversation History**: The system maintains a detailed conversation history, allowing users to review and analyze the collaborative thought process.\n\n        ## Example Use Case: Product Innovation Workshop\n\n        Imagine a product development team at a tech startup using Brainstorm to generate ideas for a new smart home device.\n\n        ### Setup:\n\n        1. The team creates the following agents:\n           - 🐱 Mia the Creative: A free-thinking idea generator\n           - 🤖 Codi the Coder: An expert in IoT and embedded systems\n           - 🦉 Rev the Reviewer: A critical thinker and market analyst\n           - 🐙 Otto the Optimizer: A specialist in user experience and product optimization\n           - 🦊 Fin the Consultant: An industry expert with knowledge of current smart home trends\n\n        2. They create a workflow with the sequence: Mia → Codi → Rev → Otto → Fin\n\n        ### Brainstorming Session:\n\n        1. The team inputs the prompt: \"Propose an innovative smart home device that addresses a common household problem.\"\n\n        2. Mia the Creative suggests: \"A smart plant care system that automatically waters plants, adjusts lighting, and provides health diagnostics.\"\n\n        3. Codi the Coder builds on this: \"We can use soil moisture sensors, LED grow lights, and a Raspberry Pi to control the system. I can outline the basic architecture and components needed.\"\n\n        4. Rev the Reviewer analyzes: \"The idea has potential, but we need to consider market saturation. Let's focus on unique features like plant species recognition and customized care plans.\"\n\n        5. Otto the Optimizer suggests: \"To enhance user experience, we could add a mobile app with AI-powered plant recognition. Users can snap a photo of their plant to get instant care instructions and integration with the smart system.\"\n\n        6. Fin the Consultant provides industry context: \"This aligns well with the growing trend of indoor gardening and sustainability. To stand out, consider adding air purification features and integration with popular smart home ecosystems.\"\n\n        The team can then review the conversation history, save this workflow for future sessions, and iterate on the ideas generated. They might run additional sessions with different prompts or agent combinations to explore various aspects of the product concept.\n\n        This collaborative AI approach allows the team to rapidly generate and refine ideas, combining diverse perspectives and expertise in a structured, efficient manner.\n        \"\"\"\n        )\n        \n"}
{"type": "source_file", "path": "manage_corpus.py", "content": "# manage_corpus.py\nimport streamlit as st\nimport os\nimport shutil\nfrom langchain_community.embeddings import OllamaEmbeddings\nfrom langchain_community.vectorstores import Chroma\nfrom langchain.text_splitter import CharacterTextSplitter\nfrom langchain.docstore.document import Document\n\ndef manage_corpus():\n    st.header(\"🗂 Manage Corpus\")\n\n    # Corpus folder\n    corpus_folder = \"corpus\"\n    if not os.path.exists(corpus_folder):\n        os.makedirs(corpus_folder)\n\n    # List existing corpus\n    corpus_list = [f for f in os.listdir(corpus_folder) if os.path.isdir(os.path.join(corpus_folder, f))]\n    st.subheader(\"⚫ Existing Corpus\")\n    if corpus_list:\n        for corpus in corpus_list:\n            col1, col2, col3 = st.columns([2, 1, 1])  # Add a column for renaming\n            with col1:\n                st.write(corpus)\n            with col2:\n                if st.button(\"✏️\", key=f\"rename_corpus_{corpus}\"):\n                    st.session_state.rename_corpus = corpus\n                    st.rerun()\n            with col3:\n                if st.button(\"🗑️\", key=f\"delete_corpus_{corpus}\"):\n                    shutil.rmtree(os.path.join(corpus_folder, corpus))\n                    st.success(f\"Corpus '{corpus}' deleted.\")\n                    st.rerun()\n    else:\n        st.write(\"No existing corpus found.\")\n\n    # Handle renaming corpus\n    if \"rename_corpus\" in st.session_state and st.session_state.rename_corpus:\n        corpus_to_rename = st.session_state.rename_corpus\n        new_corpus_name = st.text_input(f\"Rename corpus '{corpus_to_rename}' to:\", value=corpus_to_rename, key=f\"rename_corpus_input_{corpus_to_rename}\")\n        if st.button(\"Confirm Rename\", key=f\"confirm_rename_{corpus_to_rename}\"):\n            if new_corpus_name:\n                os.rename(os.path.join(corpus_folder, corpus_to_rename), os.path.join(corpus_folder, new_corpus_name))\n                st.success(f\"Corpus renamed to '{new_corpus_name}'\")\n                st.session_state.rename_corpus = None\n                st.rerun()\n            else:\n                st.error(\"Please enter a new corpus name.\")\n\n    st.subheader(\"➕ Create New Corpus\")\n    # Create corpus from files\n    st.write(\"**From Files:**\")\n    files_folder = \"files\"\n    allowed_extensions = ['.json', '.txt']\n    files = [f for f in os.listdir(files_folder) if os.path.isfile(os.path.join(files_folder, f)) and os.path.splitext(f)[1].lower() in allowed_extensions]\n    selected_files = st.multiselect(\"Select files to create corpus:\", files, key=\"create_corpus_files\")\n    corpus_name = st.text_input(\"Enter a name for the corpus:\", key=\"create_corpus_name\")\n    if st.button(\"Create Corpus from Files\", key=\"create_corpus_button\"):\n        if selected_files and corpus_name:\n            create_corpus_from_files(corpus_folder, corpus_name, files_folder, selected_files)\n            st.success(f\"Corpus '{corpus_name}' created from selected files.\")\n            st.rerun()\n        else:\n            st.error(\"Please select files and enter a corpus name.\")\n\ndef create_corpus_from_files(corpus_folder, corpus_name, files_folder, selected_files):\n    corpus_path = os.path.join(corpus_folder, corpus_name)\n    os.makedirs(corpus_path, exist_ok=True)\n    \n    # Combine all selected file content into one text\n    all_text = \"\"\n    for file in selected_files:\n        file_path = os.path.join(files_folder, file)\n        with open(file_path, \"r\", encoding='utf-8') as f:\n            file_content = f.read()\n        all_text += file_content + \"\\n\\n\"\n\n    create_corpus_from_text(corpus_folder, corpus_name, all_text)\n\ndef create_corpus_from_text(corpus_folder, corpus_name, corpus_text):\n    corpus_path = os.path.join(corpus_folder, corpus_name)\n    os.makedirs(corpus_path, exist_ok=True)\n\n    # Create Langchain documents\n    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n    texts = text_splitter.split_text(corpus_text)\n    docs = [Document(page_content=t) for t in texts]\n\n    # Create and load the vector database\n    embeddings = OllamaEmbeddings()\n    db = Chroma.from_documents(docs, embeddings, persist_directory=corpus_path)\n    db.persist()"}
{"type": "source_file", "path": "files_management.py", "content": "# files_management.py\nimport streamlit as st\nimport os\n\ndef get_corpus_options():\n    corpus_folder = \"corpus\"\n    if not os.path.exists(corpus_folder):\n        os.makedirs(corpus_folder)\n    return [f for f in os.listdir(corpus_folder) if os.path.isdir(os.path.join(corpus_folder, f))]\n\ndef files_tab():\n    st.subheader(\"📂 Files\")\n    files_folder = \"files\"\n    if not os.path.exists(files_folder):\n        os.makedirs(files_folder)\n    allowed_extensions = ['.json', '.txt', '.pdf', '.gif', '.jpg', '.jpeg', '.png']\n    files = [f for f in os.listdir(files_folder) if os.path.isfile(os.path.join(files_folder, f)) and os.path.splitext(f)[1].lower() in allowed_extensions]\n\n    for file in files:\n        col1, col2, col3, col4 = st.columns([3, 1, 1, 1])\n        with col1:\n            st.write(file)\n        with col2:\n            if file.endswith('.pdf'):\n                st.button(\"📥\", key=f\"download_{file}\")\n            else:\n                st.button(\"👀\", key=f\"view_{file}\")\n        with col3:\n            if not file.endswith('.pdf'):\n                st.button(\"✏️\", key=f\"edit_{file}\")\n        with col4:\n            st.button(\"🗑️\", key=f\"delete_{file}\")\n\n    for file in files:\n        file_path = os.path.join(files_folder, file)\n        \n        if st.session_state.get(f\"view_{file}\", False):\n            try:\n                with open(file_path, \"r\", encoding='utf-8') as f:\n                    file_content = f.read()\n                st.text_area(\"File Content:\", value=file_content, height=200, key=f\"view_content_{file}\")\n            except UnicodeDecodeError:\n                st.error(f\"Unable to decode file {file}. It may be a binary file.\")\n        \n        if st.session_state.get(f\"edit_{file}\", False):\n            try:\n                with open(file_path, \"r\", encoding='utf-8') as f:\n                    file_content = f.read()\n                new_content = st.text_area(\"Edit File Content:\", value=file_content, height=200, key=f\"edit_content_{file}\")\n                if st.button(\"Save Changes\", key=f\"save_{file}\"):\n                    with open(file_path, \"w\", encoding='utf-8') as f:\n                        f.write(new_content)\n                    st.success(f\"🟢 Changes saved to {file}\")\n            except UnicodeDecodeError:\n                st.error(f\"Unable to decode file {file}. It may be a binary file.\")\n        \n        if st.session_state.get(f\"download_{file}\", False):\n            if file.endswith('.pdf'):\n                with open(file_path, \"rb\") as pdf_file:\n                    pdf_bytes = pdf_file.read()\n                st.download_button(\n                    label=\"Download PDF\",\n                    data=pdf_bytes,\n                    file_name=file,\n                    mime='application/pdf',\n                )\n            else:\n                with open(file_path, \"r\", encoding='utf-8') as f:\n                    file_content = f.read()\n                st.download_button(\n                    label=\"Download File\",\n                    data=file_content,\n                    file_name=file,\n                    mime='text/plain',\n                )\n        \n        if st.session_state.get(f\"delete_{file}\", False):\n            os.remove(file_path)\n            st.success(f\"🟢 File {file} deleted.\")\n            st.rerun()\n    \n\n   # File upload section\n    uploaded_file = st.file_uploader(\"Upload a file\", type=['txt', 'pdf', 'json', 'gif', 'jpg', 'jpeg', 'png'])\n    if uploaded_file is not None:\n        file_path = os.path.join(files_folder, uploaded_file.name)\n        with open(file_path, \"wb\") as f:\n            f.write(uploaded_file.getbuffer())\n        st.success(f\"🟢 File {uploaded_file.name} uploaded successfully!\")\n        st.rerun()"}
{"type": "source_file", "path": "global_vrm_loader.py", "content": "\"\"\"\nGlobal VRM Loader module for managing VRM model loading functionality.\nThis module provides a singleton instance for loading and managing VRM models.\n\"\"\"\n\nclass VRMLoader:\n    def __init__(self):\n        self._loaded_models = {}\n\n    def load_model(self, key, vrm_model_path):\n        \"\"\"\n        Load a VRM model from the specified path and store it with the given key.\n        \n        Args:\n            key (str): The identifier key for the model\n            vrm_model_path (str): Path to the VRM model file\n            \n        Returns:\n            bool: True if loading was successful, False otherwise\n        \"\"\"\n        try:\n            # Here you would implement the actual VRM loading logic\n            # For now, we'll just store the path as a placeholder\n            self._loaded_models[key] = vrm_model_path\n            return True\n        except Exception as e:\n            print(f\"Error loading VRM model: {e}\")\n            return False\n\n    def get_model(self, key):\n        \"\"\"\n        Retrieve a loaded VRM model by its key.\n        \n        Args:\n            key (str): The identifier key for the model\n            \n        Returns:\n            The loaded model or None if not found\n        \"\"\"\n        return self._loaded_models.get(key)\n\n    def unload_model(self, key):\n        \"\"\"\n        Unload a VRM model from memory.\n        \n        Args:\n            key (str): The identifier key for the model\n            \n        Returns:\n            bool: True if unloading was successful, False if model wasn't loaded\n        \"\"\"\n        if key in self._loaded_models:\n            del self._loaded_models[key]\n            return True\n        return False\n\n# Create a global instance\nglobal_vrm_loader = VRMLoader()\n"}
{"type": "source_file", "path": "artifact.py", "content": "# artifact.py\nimport streamlit as st\nimport json\nimport os\nimport re\nfrom datetime import datetime\nfrom ollama_utils import get_available_models, call_ollama_endpoint, save_chat_history, load_chat_history, pull_model, show_model_info, remove_model\n\n# Function to extract code blocks from text\ndef extract_code_blocks(text):\n    code_blocks = re.findall(r'```[\\s\\S]*?```', text)\n    return [block.strip('`').strip() for block in code_blocks]\n\n# Initialize session state variables\nif 'chat_history' not in st.session_state:\n    st.session_state['chat_history'] = []\nif 'artifacts' not in st.session_state:\n    st.session_state['artifacts'] = []\n\n# Streamlit app layout\nst.set_page_config(layout=\"wide\")\n\n# Sidebar for model selection and management\nwith st.sidebar:\n    st.title(\"Ollama Model Chat\")\n    models = get_available_models()\n    selected_model = st.selectbox(\"Select Model\", models)\n    \n    if st.button(\"Pull Model\"):\n        pull_model(selected_model)\n        st.success(f\"Model '{selected_model}' pulled successfully.\")\n        \n    if st.button(\"Show Model Info\"):\n        model_info = show_model_info(selected_model)\n        st.json(model_info)\n        \n    if st.button(\"Remove Model\"):\n        result = remove_model(selected_model)\n        st.write(result)\n\n# Main layout with two columns\nleft_column, right_column = st.columns(2)\n\n# Chat history on the left\nwith left_column:\n    st.header(\"Chat Stream\")\n    for chat in st.session_state['chat_history']:\n        with st.chat_message(chat[\"role\"]):\n            if chat[\"role\"] == \"assistant\":\n                code_blocks = extract_code_blocks(chat[\"content\"])\n                for code_block in code_blocks:\n                    st.code(code_block)\n                non_code_parts = re.split(r'```[\\s\\S]*?```', chat[\"content\"])\n                for part in non_code_parts:\n                    st.markdown(part.strip())\n            else:\n                st.markdown(chat[\"content\"])\n\n# Artifacts container on the right\nwith right_column:\n    st.header(\"Artifacts\")\n    for artifact in st.session_state['artifacts']:\n        st.write(artifact)\n\n# User input and response handling\nuser_input = st.text_input(\"You:\", \"\")\nif st.button(\"Send\"):\n    if user_input:\n        st.session_state['chat_history'].append({\"role\": \"user\", \"content\": user_input})\n        \n        # Display user input\n        with left_column:\n            with st.chat_message(\"user\"):\n                st.markdown(user_input)\n        \n        # Generate and display the assistant's response\n        response, _, _, _ = call_ollama_endpoint(selected_model, prompt=user_input)\n        st.session_state['chat_history'].append({\"role\": \"assistant\", \"content\": response})\n        \n        with left_column:\n            with st.chat_message(\"assistant\"):\n                code_blocks = extract_code_blocks(response)\n                for code_block in code_blocks:\n                    st.code(code_block)\n                non_code_parts = re.split(r'```[\\s\\S]*?```', response)\n                for part in non_code_parts:\n                    st.markdown(part.strip())\n        \n        # Optionally, save the final content to artifacts based on some condition\n        if \"final result\" in user_input.lower():\n            st.session_state['artifacts'].append(response)\n\n# Function to add artifacts manually\ndef add_artifact(content):\n    st.session_state['artifacts'].append(content)\n\n# Manually add an artifact\nmanual_artifact = st.text_input(\"Add Artifact Content:\", \"\")\nif st.button(\"Add Artifact\"):\n    add_artifact(manual_artifact)\n\n# Function to save chat history\nif st.button(\"Save Chat History\"):\n    save_chat_history(st.session_state['chat_history'])\n    st.success(\"Chat history saved successfully.\")\n\n# Function to load chat history\nif st.button(\"Load Chat History\"):\n    st.session_state['chat_history'] = load_chat_history(\"chat_history.json\")\n    st.success(\"Chat history loaded successfully.\")\n\nst.markdown(\n    \"\"\"\n    <style>\n    .stTextInput {\n        margin-top: 20px;\n    }\n    .stButton {\n        margin-top: 20px;\n    }\n    </style>\n    \"\"\",\n    unsafe_allow_html=True\n)\n"}
{"type": "source_file", "path": "install_streamlit_option_menu.py", "content": "#!/usr/bin/env python3\n\"\"\"\nScript to install streamlit_option_menu in the current Python environment.\nThis is useful when the application is running from a different environment\nthan the one set up by the setup scripts.\n\"\"\"\n\nimport sys\nimport subprocess\nimport importlib.util\n\ndef check_package_installed(package_name):\n    \"\"\"Check if a package is installed in the current Python environment.\"\"\"\n    spec = importlib.util.find_spec(package_name)\n    return spec is not None\n\ndef install_package(package_name, version=None):\n    \"\"\"Install a package in the current Python environment.\"\"\"\n    package_spec = package_name\n    if version:\n        package_spec = f\"{package_name}=={version}\"\n    \n    print(f\"Installing {package_spec}...\")\n    try:\n        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package_spec])\n        print(f\"Successfully installed {package_spec}\")\n        return True\n    except subprocess.CalledProcessError as e:\n        print(f\"Error installing {package_spec}: {e}\")\n        return False\n\ndef main():\n    \"\"\"Main function.\"\"\"\n    print(f\"Python executable: {sys.executable}\")\n    print(f\"Python version: {sys.version}\")\n    \n    # Check if streamlit_option_menu is already installed\n    if check_package_installed(\"streamlit_option_menu\"):\n        print(\"streamlit_option_menu is already installed\")\n        return True\n    \n    # Try to install streamlit_option_menu\n    success = install_package(\"streamlit-option-menu\", \"0.3.13\")\n    \n    if success:\n        # Verify installation\n        if check_package_installed(\"streamlit_option_menu\"):\n            print(\"Verified: streamlit_option_menu is now installed\")\n            return True\n        else:\n            print(\"Error: streamlit_option_menu was not installed correctly\")\n            return False\n    else:\n        print(\"Failed to install streamlit_option_menu\")\n        return False\n\nif __name__ == \"__main__\":\n    success = main()\n    sys.exit(0 if success else 1)"}
{"type": "source_file", "path": "agents.py", "content": "# agents.py\nimport json\nimport logging\nfrom typing import List, Dict, Tuple, Callable, Any\nfrom projects import Task\nfrom search_libraries import duckduckgo_search, google_search, serpapi_search, serper_search, bing_search\nimport ollama\nimport re\nimport spacy\nfrom langchain_community.embeddings import OllamaEmbeddings\nfrom langchain_community.vectorstores import Chroma\nfrom openai_utils import call_openai_api, OPENAI_MODELS\nfrom groq_utils import call_groq_api, GROQ_MODELS\n\n# Set up logging\nlogging.basicConfig(filename='agents.log', level=logging.INFO,\n                    format='%(asctime)s - %(levelname)s - %(message)s')\n\n# Load the spaCy language model\nnlp = spacy.load(\"en_core_web_sm\")\n\nclass Agent:\n    def __init__(self, name: str, capabilities: List[str], prompts: Dict[str, str], model: str = None, **kwargs):\n        self.name = name\n        self.capabilities = capabilities\n        self.prompts = prompts\n        self.model = model\n        self.settings = kwargs\n\n    @classmethod\n    def from_json(cls, json_path: str):\n        with open(json_path, 'r') as f:\n            data = json.load(f)\n        return cls(**data)\n\n    def cancel_task(self, task: Task):\n        \"\"\"\n        Cancels the execution of a task.\n\n        Args:\n            task: The task to cancel.\n        \"\"\"\n        print(f\"Canceling task: {task.name}\")\n        # TODO: Implement agent-specific cancellation logic\n\nclass SearchAgent(Agent):\n    def __init__(self, name: str, model: str, search_function: Callable, api_key: str = None, cse_id: str = None, prompt: str = None, role: str = None):\n        super().__init__(name=name, capabilities=[\"web_search\"], prompts={}, model=model)\n        self.search_function = search_function\n        self.api_key = api_key\n        self.cse_id = cse_id\n        self.prompt = prompt\n        self.role = role\n\n    def search(self, query: str, num_results: int = 5) -> List[Dict]:\n        try:\n            if self.search_function == google_search:\n                if self.api_key and self.cse_id:\n                    return self.search_function(query, self.api_key, self.cse_id, num_results)\n            elif self.search_function in [serpapi_search, serper_search, bing_search]:\n                if self.api_key:\n                    return self.search_function(query, self.api_key, num_results)\n            else:\n                return self.search_function(query, num_results)\n        except Exception as e:\n            logging.error(f\"Search error for agent {self.name}: {str(e)}\")\n        return []\n\n    def generate_summary(self, query: str, search_results: List[Dict]) -> str:\n        formatted_results = \"\"\n        for i, result in enumerate(search_results):\n            formatted_results += f\"[{i+1}] {result['title']}: {result['url']}\\n\"\n\n        prompt = f\"\"\"{self.prompt}\n\n        Here are some relevant search results:\n        {formatted_results}\n\n        Generate a detailed summary of the search results, addressing the following query:\n        \"{query}\"\n\n        Include relevant information from the search results and cite your sources using [n] notation.\n        \"\"\"\n\n        try:\n            if self.model in OPENAI_MODELS:\n                response = call_openai_api(self.model, [{\"role\": \"user\", \"content\": prompt}], temperature=0.7, max_tokens=1000, openai_api_key=self.api_key)\n            elif self.model in GROQ_MODELS:\n                response = call_groq_api(self.model, prompt, temperature=0.7, max_tokens=1000, groq_api_key=self.api_key)\n            else:\n                response = ollama.generate(model=self.model, prompt=prompt)\n                response = response['response']\n            return response\n        except Exception as e:\n            logging.error(f\"Summary generation error for agent {self.name}: {str(e)}\")\n            return f\"Error generating summary: {str(e)}\"\n\nclass SearchManager(Agent):\n    def __init__(self, name: str, model: str, temperature: float, max_tokens: int, api_keys: Dict[str, str]):\n        capabilities = [\"research_management\"]\n        prompts = {\n            \"agent_creation\": \"Create specialized research agents based on the given request.\",\n            \"report_compilation\": \"Compile a comprehensive report based on agent summaries.\"\n        }\n        super().__init__(name=name, capabilities=capabilities, prompts=prompts, model=model)\n        self.temperature = temperature\n        self.max_tokens = max_tokens\n        self.search_agents = {}\n        self.api_keys = api_keys\n\n    def create_search_agents(self, user_request: str, agent_model: str):\n        search_libraries = {\n            \"duckduckgo\": duckduckgo_search,\n            \"google\": google_search,\n            \"serpapi\": serpapi_search,\n            \"serper\": serper_search,\n            \"bing\": bing_search\n        }\n\n        agent_definition_prompt = f\"\"\"\n        Based on the following research request, create a team of specialized researchers:\n        \"{user_request}\"\n\n        Generate a JSON object defining 5 research agents, each specializing in a different aspect of the request.\n        The JSON object should have the following structure:\n\n        {{\n          \"agents\": [\n            {{\n              \"name\": \"Name\",\n              \"role\": \"Specific Research Focus\",\n              \"library\": \"Search Library\",\n              \"prompt\": \"Detailed instructions for the agent's research focus\"\n            }},\n            // ... more agents\n          ]\n        }}\n\n        Available search libraries: {', '.join(search_libraries.keys())}\n        Ensure each agent has a unique role and uses a different search library.\n        \"\"\"\n\n        try:\n            if self.model in OPENAI_MODELS:\n                response = call_openai_api(self.model, [{\"role\": \"user\", \"content\": agent_definition_prompt}], temperature=self.temperature, max_tokens=self.max_tokens, openai_api_key=self.api_keys.get(\"openai_api_key\"))\n            elif self.model in GROQ_MODELS:\n                response = call_groq_api(self.model, agent_definition_prompt, temperature=self.temperature, max_tokens=self.max_tokens, groq_api_key=self.api_keys.get(\"groq_api_key\"))\n            else:\n                response = ollama.generate(\n                    model=self.model,\n                    prompt=agent_definition_prompt,\n                    options={\n                        \"temperature\": self.temperature,\n                        \"max_tokens\": self.max_tokens\n                    }\n                )\n                response = response['response']\n            agent_definitions = self.parse_json_response(response)\n            if not agent_definitions:\n                raise ValueError(\"Failed to generate valid agent definitions\")\n        except Exception as e:\n            logging.error(f\"Error generating agent definitions: {str(e)}\")\n            agent_definitions = self.fallback_agent_definitions()\n\n        self.search_agents = {}\n        for agent_data in agent_definitions.get(\"agents\", []):\n            agent_name = agent_data.get(\"name\")\n            role = agent_data.get(\"role\")\n            library = agent_data.get(\"library\")\n            prompt = agent_data.get(\"prompt\")\n\n            if agent_name and role and library and prompt:\n                search_function = search_libraries.get(library)\n                if search_function:\n                    api_key = self.api_keys.get(f\"{library}_api_key\") or self.api_keys.get(\"serpapi_api_key\")\n                    cse_id = self.api_keys.get(\"google_cse_id\") if library == \"google\" else None\n                    if api_key or library == \"duckduckgo\":\n                        self.search_agents[agent_name] = SearchAgent(\n                            name=agent_name,\n                            model=agent_model,\n                            search_function=search_function,\n                            api_key=api_key,\n                            cse_id=cse_id,\n                            prompt=prompt,\n                            role=role\n                        )\n                    else:\n                        logging.warning(f\"Skipping agent {agent_name} due to missing API key for {library}\")\n                else:\n                    logging.warning(f\"Invalid search library: {library} for agent {agent_name}\")\n            else:\n                logging.warning(f\"Incomplete agent definition: {agent_data}\")\n\n    def parse_json_response(self, response: str) -> Dict[str, Any]:\n        \"\"\"\n        Attempts to parse the JSON response, handling potential errors.\n\n        Args:\n            response: The string response from the LLM.\n\n        Returns:\n            A dictionary containing the parsed JSON, or None if parsing fails.\n        \"\"\"\n        try:\n            # Remove any leading/trailing whitespace and newlines\n            response = response.strip()\n            # If the response is wrapped in ```, remove them\n            if response.startswith(\"```json\"):\n                response = response[7:]\n            if response.endswith(\"```\"):\n                response = response[:-3]\n            # Parse the JSON\n            return json.loads(response)\n        except json.JSONDecodeError as e:\n            logging.error(f\"JSON parsing error: {str(e)}\")\n            logging.error(f\"Problematic JSON: {response}\")\n            return None\n\n    def fallback_agent_definitions(self) -> Dict[str, Any]:\n        \"\"\"\n        Provides a fallback set of agent definitions if the LLM fails to generate valid JSON.\n\n        Returns:\n            A dictionary containing predefined agent definitions.\n        \"\"\"\n        return {\n            \"agents\": [\n                {\n                    \"name\": \"General Researcher\",\n                    \"role\": \"Background Research\",\n                    \"library\": \"duckduckgo\",\n                    \"prompt\": \"Conduct a general search on the topic and provide an overview of key information.\"\n                },\n                {\n                    \"name\": \"Detailed Analyst\",\n                    \"role\": \"In-depth Analysis\",\n                    \"library\": \"google\",\n                    \"prompt\": \"Perform a detailed analysis of the topic, focusing on recent developments and expert opinions.\"\n                },\n                {\n                    \"name\": \"Fact Checker\",\n                    \"role\": \"Verification\",\n                    \"library\": \"bing\",\n                    \"prompt\": \"Verify key claims and provide factual information from reputable sources.\"\n                }\n            ]\n        }\n\n    def run_research(self, user_request: str, report_length: str = \"medium\", agent_model: str = None, word_count_target: int = 1000) -> Tuple[str, List[str], List[Dict]]:\n        self.create_search_agents(user_request, agent_model or self.model)\n\n        agent_outputs = []\n        all_search_results = {}\n        citation_counter = 1\n        for agent_name, agent in self.search_agents.items():\n            logging.info(f\"{agent_name} is working...\")\n            search_results = agent.search(user_request)\n            for result in search_results:\n                all_search_results[citation_counter] = result\n                citation_counter += 1\n            summary = agent.generate_summary(user_request, search_results)\n            agent_outputs.append({\"agent\": agent_name, \"summary\": summary, \"results\": search_results})\n            logging.info(f\"{agent_name} summary: {summary}\")\n            yield f\"{agent_name} Report\", summary\n\n        formatted_summaries = \"\\n\\n\".join([f\"**{output['agent']} Summary:**\\n{output['summary']}\" for output in agent_outputs])\n        final_report_prompt = f\"\"\"You are a research manager tasked with writing a {report_length} comprehensive report on: {user_request}\n\n        Your team of specialized research agents has analyzed the topic and provided the following summaries:\n        {formatted_summaries}\n\n        Based on these summaries, generate a {report_length} report that answers the user's request.\n        Aim for approximately {word_count_target} words in your report.\n        Ensure the report is well-structured, informative, and includes citations in square brackets (e.g., [1], [2]).\n        Include the following sections in your report:\n\n        1. Introduction: Briefly introduce the topic and the purpose of the report.\n        2. Background: Provide some background information on the topic.\n        3. Analysis: Analyze the information from the agent summaries, drawing connections and insights.\n        4. Conclusion: Summarize the key findings and provide a concluding statement.\n        5. References: List the cited sources with their corresponding URLs.\n        \"\"\"\n        logging.info(\"Search Manager is generating the final report...\")\n        try:\n            if self.model in OPENAI_MODELS:\n                final_report = call_openai_api(self.model, [{\"role\": \"user\", \"content\": final_report_prompt}], temperature=self.temperature, max_tokens=self.max_tokens, openai_api_key=self.api_keys.get(\"openai_api_key\"))\n            elif self.model in GROQ_MODELS:\n                final_report = call_groq_api(self.model, final_report_prompt, temperature=self.temperature, max_tokens=self.max_tokens, groq_api_key=self.api_keys.get(\"groq_api_key\"))\n            else:\n                response = ollama.generate(\n                    model=self.model,\n                    prompt=final_report_prompt,\n                    options={\n                        \"temperature\": self.temperature,\n                        \"max_tokens\": self.max_tokens\n                    }\n                )\n                final_report = response['response']\n        except Exception as e:\n            logging.error(f\"Error generating final report: {str(e)}\")\n            final_report = f\"Error generating final report: {str(e)}\"\n\n        references = self.extract_references(final_report, all_search_results)\n        \n        yield \"Final Report\", final_report\n        yield \"References\", references\n\n        return final_report, references, agent_outputs\n\n    def extract_references(self, report: str, all_search_results: Dict[int, Dict]) -> List[str]:\n        \"\"\"Extracts references from the generated report.\"\"\"\n        citation_pattern = r\"\\[(\\d+)\\]\"\n        citations = re.findall(citation_pattern, report)\n        references = []\n        for citation in citations:\n            try:\n                index = int(citation)\n                result = all_search_results[index]\n                references.append(f\"[{citation}] {result['title']} - {result['url']}\")\n            except (ValueError, KeyError):\n                references.append(f\"Invalid citation: [{citation}]\")\n        return references"}
{"type": "source_file", "path": "autogen_compat/__init__.py", "content": "\"\"\"\nCompatibility layer for autogen imports.\nThis module redirects imports from 'autogen' to 'pyautogen'.\n\"\"\"\n\nimport sys\nimport importlib\nimport warnings\n\n# Show a warning about the compatibility layer\nwarnings.warn(\n    \"Using autogen_compat layer: 'autogen' package is not available, redirecting to 'pyautogen'\",\n    ImportWarning\n)\n\n# Try to import pyautogen\ntry:\n    import pyautogen\nexcept ImportError:\n    raise ImportError(\n        \"Neither 'autogen' nor 'pyautogen' package is installed. \"\n        \"Please install pyautogen with: pip install pyautogen>=0.2.0\"\n    )\n\n# Add the module to sys.modules\nsys.modules['autogen'] = pyautogen\n\n# Also make submodules available\nfor submodule_name in [\n    'agentchat', 'cache', 'coding', 'oai', 'token_count_utils',\n    'browser_utils', 'code_utils', 'exception_utils', 'formatting_utils',\n    'function_utils', 'graph_utils', 'retrieve_utils', 'runtime_logging', 'types'\n]:\n    try:\n        submodule = importlib.import_module(f'pyautogen.{submodule_name}')\n        sys.modules[f'autogen.{submodule_name}'] = submodule\n    except ImportError:\n        # If the submodule doesn't exist in pyautogen, just skip it\n        pass\n"}
{"type": "source_file", "path": "model_comparison.py", "content": "# model_comparison.py\nimport streamlit as st\nimport pandas as pd\nfrom ollama_utils import get_available_models, check_json_handling, check_function_calling\nfrom model_tests import performance_test\n\n@st.cache_data\ndef run_comparison(selected_models, prompt, temperature, max_tokens, presence_penalty, frequency_penalty):\n    results = performance_test(selected_models, prompt, temperature, max_tokens, presence_penalty, frequency_penalty)\n\n    # Prepare data for visualization\n    models = list(results.keys())\n    times = [results[model][1] for model in models]\n    tokens_per_second = [\n        results[model][2] / (results[model][3] / (10**9)) if results[model][2] and results[model][3] else 0\n        for model in models\n    ]\n\n    df = pd.DataFrame({\"Model\": models, \"Time (seconds)\": times, \"Tokens/second\": tokens_per_second})\n\n    return results, df, tokens_per_second, models\n\ndef model_comparison_test():\n    st.header(\"🎯 Model Comparison by Response Quality\")\n\n    available_models = get_available_models()\n\n    if \"selected_models\" not in st.session_state:\n        st.session_state.selected_models = []\n\n    selected_models = st.multiselect(\n        \"Select the models you want to compare:\",\n        available_models,\n        default=st.session_state.selected_models,\n        key=\"model_comparison_models\"\n    )\n\n    col1, col2, col3, col4 = st.columns(4)\n    with col1:\n        temperature = st.slider(\"Temperature\", min_value=0.0, max_value=1.0, value=0.5, step=0.1)\n    with col2:\n        max_tokens = st.slider(\"Max Tokens\", min_value=1000, max_value=128000, value=4000, step=1000)\n    with col3:\n        presence_penalty = st.slider(\"Presence Penalty\", min_value=-2.0, max_value=2.0, value=0.0, step=0.1)\n    with col4:\n        frequency_penalty = st.slider(\"Frequency Penalty\", min_value=-2.0, max_value=2.0, value=0.0, step=0.1)\n\n    prompt = st.text_area(\"Enter the prompt:\", value=\"Write a short story about a brave knight.\")\n\n    if st.button(label='Compare Models'):\n        if selected_models:\n            results, df, tokens_per_second, models = run_comparison(selected_models, prompt, temperature, max_tokens, presence_penalty, frequency_penalty)\n\n            st.bar_chart(df, x=\"Model\", y=[\"Time (seconds)\", \"Tokens/second\"], color=[\"#4CAF50\", \"#FFC107\"])\n\n            for model, (result, elapsed_time, eval_count, eval_duration) in results.items():\n                st.subheader(f\"Results for {model} (Time taken: {elapsed_time:.2f} seconds, Tokens/second: {tokens_per_second[models.index(model)]:.2f}):\")\n                st.write(result)\n                st.write(\"📦 JSON Handling Capability: \", \"✅\" if check_json_handling(model, temperature, max_tokens, presence_penalty, frequency_penalty) else \"❌\")\n                st.write(\"⚙️ Function Calling Capability: \", \"✅\" if check_function_calling(model, temperature, max_tokens, presence_penalty, frequency_penalty) else \"❌\")\n        else:\n            st.warning(\"Please select at least one model.\")\n"}
{"type": "source_file", "path": "editor.py", "content": "# editor.py\nimport streamlit as st\nimport os\nfrom pathlib import Path\nimport markdown\nimport bleach\nfrom pygments import highlight\nfrom pygments.lexers import get_lexer_by_name\nfrom pygments.formatters import HtmlFormatter\nimport re\n\ndef save_file(content, file_name, file_type):\n    folder = \"files\"\n    if not os.path.exists(folder):\n        os.makedirs(folder)\n    \n    file_path = os.path.join(folder, f\"{file_name}.{file_type}\")\n    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n        f.write(content)\n    return file_path\n\ndef load_file(file_path):\n    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n        return f.read()\n\ndef custom_markdown(content):\n    def code_formatter(match):\n        code = match.group(1)\n        lexer = get_lexer_by_name(\"python\", stripall=True)\n        formatter = HtmlFormatter(style=\"monokai\")\n        return highlight(code, lexer, formatter)\n\n    content = re.sub(r'```python\\n(.*?)\\n```', code_formatter, content, flags=re.DOTALL)\n    html = markdown.markdown(content, extensions=['fenced_code', 'codehilite'])\n    return bleach.clean(html, tags=['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'strong', 'em', 'u', 'ol', 'ul', 'li', 'a', 'code', 'pre', 'span'], attributes={'span': ['class']})\n\ndef insert_text(text):\n    if 'editor_content' not in st.session_state:\n        st.session_state.editor_content = \"\"\n    st.session_state.editor_content += text\n\ndef main():\n    st.title(\"Enhanced Rich Text Editor\")\n\n    # File operations\n    col1, col2, col3 = st.columns(3)\n    with col1:\n        file_name = st.text_input(\"File name\", \"document\")\n    with col2:\n        file_type = st.selectbox(\"File type\", [\"md\", \"txt\"])\n    with col3:\n        files = [f for f in os.listdir(\"files\") if f.endswith(f\".{file_type}\")]\n        selected_file = st.selectbox(\"Load file\", [\"\"] + files)\n\n    # Markdown formatting buttons\n    col1, col2, col3, col4, col5 = st.columns(5)\n    with col1:\n        if st.button(\"Bold\"):\n            insert_text(\"**Bold**\")\n    with col2:\n        if st.button(\"Italic\"):\n            insert_text(\"*Italic*\")\n    with col3:\n        if st.button(\"List Item\"):\n            insert_text(\"\\n- List item\")\n    with col4:\n        if st.button(\"Code Block\"):\n            insert_text(\"\\n```python\\n# Your code here\\n```\\n\")\n    with col5:\n        if st.button(\"Link\"):\n            insert_text(\"[Link text](https://example.com)\")\n\n    # Editor\n    if 'editor_content' not in st.session_state:\n        st.session_state.editor_content = \"\"\n    \n    editor_content = st.text_area(\"Editor\", st.session_state.editor_content, height=300, key=\"editor\")\n    st.session_state.editor_content = editor_content\n\n    # Buttons\n    col1, col2, col3 = st.columns(3)\n    with col1:\n        if st.button(\"Save\"):\n            file_path = save_file(editor_content, file_name, file_type)\n            st.success(f\"File saved: {file_path}\")\n    \n    with col2:\n        if st.button(\"Load\"):\n            if selected_file:\n                file_path = os.path.join(\"files\", selected_file)\n                st.session_state.editor_content = load_file(file_path)\n                st.experimental_rerun()\n            else:\n                st.warning(\"Please select a file to load.\")\n\n    with col3:\n        if st.button(\"Clear\"):\n            st.session_state.editor_content = \"\"\n            st.experimental_rerun()\n\n    # Preview\n    if editor_content:\n        st.subheader(\"Preview\")\n        if file_type == \"md\":\n            styled_html = f\"\"\"\n            <style>\n                {HtmlFormatter(style=\"monokai\").get_style_defs('.highlight')}\n            </style>\n            {custom_markdown(editor_content)}\n            \"\"\"\n            st.markdown(styled_html, unsafe_allow_html=True)\n        else:\n            st.text(editor_content)\n\nif __name__ == \"__main__\":\n    main()"}
{"type": "source_file", "path": "enhanced_corpus.py", "content": "# enhanced_corpus.py\n\nimport streamlit as st\nimport os\nimport logging\nimport shutil\nimport json\nimport ollama\nimport numpy as np\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport requests\nfrom bs4 import BeautifulSoup\nimport PyPDF2\nimport io\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# Constants\nRAGTEST_DIR = \"ragtest\"\nFILES_DIR = \"files\"\n\n# Ensure necessary directories exist\nos.makedirs(RAGTEST_DIR, exist_ok=True)\nos.makedirs(FILES_DIR, exist_ok=True)\n\nclass DocumentProcessor:\n    def __init__(self, chunk_size=1000, chunk_overlap=200):\n        self.chunk_size = chunk_size\n        self.chunk_overlap = chunk_overlap\n\n    def split_text(self, text):\n        chunks = []\n        for i in range(0, len(text), self.chunk_size - self.chunk_overlap):\n            chunks.append(text[i:i + self.chunk_size])\n        return chunks\n\n    def load_text_file(self, file_path):\n        with open(file_path, 'r', encoding='utf-8') as file:\n            return file.read()\n\n    def load_pdf_file(self, file_path):\n        with open(file_path, 'rb') as file:\n            pdf_reader = PyPDF2.PdfReader(file)\n            text = \"\"\n            for page in pdf_reader.pages:\n                text += page.extract_text() + \"\\n\"\n            return text\n\n    def process_file(self, file_path):\n        if file_path.lower().endswith('.pdf'):\n            text = self.load_pdf_file(file_path)\n        else:\n            text = self.load_text_file(file_path)\n        return self.split_text(text)\n\n    def process_url(self, url):\n        response = requests.get(url)\n        soup = BeautifulSoup(response.content, 'html.parser')\n        text = soup.get_text()\n        return self.split_text(text)\n\n    def process_text(self, text):\n        return self.split_text(text)\n\nclass OllamaEmbedder:\n    def __init__(self, model=\"llama2\"):\n        self.model = model\n\n    def get_embedding(self, text):\n        try:\n            response = ollama.embeddings(model=self.model, prompt=text)\n            return response['embedding']\n        except Exception as e:\n            logger.error(f\"Error getting embedding: {str(e)}\")\n            raise\n\nclass GraphRAGCorpus:\n    def __init__(self, corpus_name, embedder):\n        self.corpus_name = corpus_name\n        self.corpus_dir = os.path.join(RAGTEST_DIR, corpus_name)\n        self.embedder = embedder\n        self.documents = []\n        self.embeddings = []\n\n    def add_document(self, content, metadata=None):\n        embedding = self.embedder.get_embedding(content)\n        doc_id = len(self.documents)\n        self.documents.append({\n            \"id\": doc_id,\n            \"content\": content,\n            \"metadata\": metadata or {}\n        })\n        self.embeddings.append(embedding)\n\n    def save(self):\n        os.makedirs(self.corpus_dir, exist_ok=True)\n        \n        # Save documents\n        for doc in self.documents:\n            with open(os.path.join(self.corpus_dir, f\"doc_{doc['id']}.txt\"), \"w\", encoding='utf-8') as f:\n                f.write(doc['content'])\n\n        # Save metadata and embeddings\n        metadata = {\n            \"documents\": [{\"id\": doc[\"id\"], \"metadata\": doc[\"metadata\"]} for doc in self.documents]\n        }\n        with open(os.path.join(self.corpus_dir, \"metadata.json\"), \"w\") as f:\n            json.dump(metadata, f, indent=2)\n\n        with open(os.path.join(self.corpus_dir, \"embeddings.json\"), \"w\") as f:\n            json.dump(self.embeddings, f)\n\n    @classmethod\n    def load(cls, corpus_name, embedder):\n        corpus = cls(corpus_name, embedder)\n        corpus_dir = os.path.join(RAGTEST_DIR, corpus_name)\n\n        # Load metadata\n        with open(os.path.join(corpus_dir, \"metadata.json\"), \"r\") as f:\n            metadata = json.load(f)\n\n        # Load embeddings\n        with open(os.path.join(corpus_dir, \"embeddings.json\"), \"r\") as f:\n            corpus.embeddings = json.load(f)\n\n        # Load documents\n        for doc_meta in metadata[\"documents\"]:\n            with open(os.path.join(corpus_dir, f\"doc_{doc_meta['id']}.txt\"), \"r\", encoding='utf-8') as f:\n                content = f.read()\n            corpus.documents.append({\n                \"id\": doc_meta[\"id\"],\n                \"content\": content,\n                \"metadata\": doc_meta[\"metadata\"]\n            })\n\n        return corpus\n\n    def query(self, query_text, n_results=3):\n        query_embedding = self.embedder.get_embedding(query_text)\n        similarities = cosine_similarity([query_embedding], self.embeddings)[0]\n        top_indices = np.argsort(similarities)[-n_results:][::-1]\n        \n        results = []\n        for idx in top_indices:\n            results.append({\n                \"content\": self.documents[idx][\"content\"],\n                \"metadata\": self.documents[idx][\"metadata\"],\n                \"similarity\": similarities[idx]\n            })\n        \n        return results\n\ndef create_graphrag_corpus(corpus_name, documents):\n    embedder = OllamaEmbedder()\n    corpus = GraphRAGCorpus(corpus_name, embedder)\n\n    for i, doc in enumerate(documents):\n        corpus.add_document(doc, metadata={\"original_id\": i})\n\n    corpus.save()\n    logger.info(f\"Corpus '{corpus_name}' created successfully!\")\n\ndef process_and_save_corpus(data, corpus_name, is_url=False, is_text=False):\n    doc_processor = DocumentProcessor()\n    if is_url:\n        documents = doc_processor.process_url(data)\n    elif is_text:\n        documents = doc_processor.process_text(data)\n    else:\n        file_path = os.path.join(FILES_DIR, data.name)\n        with open(file_path, \"wb\") as f:\n            f.write(data.getbuffer())\n        documents = doc_processor.process_file(file_path)\n\n    try:\n        create_graphrag_corpus(corpus_name, documents)\n        st.success(f\"Corpus '{corpus_name}' created successfully!\")\n    except Exception as e:\n        logger.error(f\"Error creating GraphRAG corpus: {str(e)}\")\n        st.error(f\"Error creating GraphRAG corpus: {str(e)}\")\n\ndef enhance_corpus_ui():\n    st.title(\"GraphRAG Corpus Management\")\n\n    st.subheader(\"✚ Create New Corpus\")\n    tab1, tab2, tab3 = st.tabs([\"From File\", \"From URL\", \"From Text\"])\n\n    with tab1:\n        uploaded_file = st.file_uploader(\"Upload a document\", type=[\"pdf\", \"txt\"])\n        corpus_name = st.text_input(\"Enter a name for the corpus:\", key=\"create_corpus_name_file\")\n        if st.button(\"✚ Create Corpus\", key=\"create_corpus_button_file\"):\n            if uploaded_file is not None and corpus_name:\n                process_and_save_corpus(uploaded_file, corpus_name)\n            else:\n                st.error(\"Please upload a file and enter a corpus name.\")\n\n    with tab2:\n        url = st.text_input(\"Enter a URL to process\")\n        corpus_name = st.text_input(\"Enter a name for the corpus:\", key=\"create_corpus_name_url\")\n        if st.button(\"✚ Create Corpus\", key=\"create_corpus_button_url\"):\n            if url and corpus_name:\n                process_and_save_corpus(url, corpus_name, is_url=True)\n            else:\n                st.error(\"Please enter a URL and a corpus name.\")\n\n    with tab3:\n        text_input = st.text_area(\"Enter text to process\")\n        corpus_name = st.text_input(\"Enter a name for the corpus:\", key=\"create_corpus_name_text\")\n        if st.button(\"✚ Create Corpus\", key=\"create_corpus_button_text\"):\n            if text_input and corpus_name:\n                process_and_save_corpus(text_input, corpus_name, is_text=True)\n            else:\n                st.error(\"Please enter text and a corpus name.\")\n\n    st.subheader(\"📚 Existing Corpora\")\n    corpora = [d for d in os.listdir(RAGTEST_DIR) if os.path.isdir(os.path.join(RAGTEST_DIR, d))]\n    if corpora:\n        for corpus in corpora:\n            col1, col2, col3 = st.columns([2, 1, 1])\n            with col1:\n                st.write(f\"- {corpus}\")\n            with col2:\n                if st.button(\"🔍 Query\", key=f\"query_{corpus}\"):\n                    st.session_state[f\"show_query_{corpus}\"] = True\n            with col3:\n                if st.button(\"🗑️\", key=f\"delete_{corpus}\"):\n                    shutil.rmtree(os.path.join(RAGTEST_DIR, corpus))\n                    st.success(f\"Corpus '{corpus}' deleted.\")\n                    st.rerun()\n            \n            if st.session_state.get(f\"show_query_{corpus}\", False):\n                query = st.text_input(f\"Enter query for {corpus}:\")\n                if query:\n                    embedder = OllamaEmbedder()\n                    loaded_corpus = GraphRAGCorpus.load(corpus, embedder)\n                    results = loaded_corpus.query(query)\n                    st.write(\"Query Results:\")\n                    for result in results:\n                        st.write(f\"Similarity: {result['similarity']:.4f}\")\n                        st.write(result['content'])\n                        st.write(\"---\")\n    else:\n        st.write(\"No existing corpora found.\")\n\nif __name__ == \"__main__\":\n    enhance_corpus_ui()"}
{"type": "source_file", "path": "compare_ve.py", "content": "import subprocess\nimport json\nimport os\nimport streamlit as st\nimport pandas as pd\nfrom datetime import datetime\nimport ast\nimport yaml\n\ndef get_conda_list(env_name):\n    result = subprocess.run(['conda', 'list', '-n', env_name, '--json'], stdout=subprocess.PIPE, check=True)\n    packages = json.loads(result.stdout)\n    return {pkg['name']: pkg['version'] for pkg in packages}\n\ndef compare_envs(env1, env2):\n    env1_packages = get_conda_list(env1)\n    env2_packages = get_conda_list(env2)\n    \n    only_in_env1 = {pkg: ver for pkg, ver in env1_packages.items() if pkg not in env2_packages}\n    only_in_env2 = {pkg: ver for pkg, ver in env2_packages.items() if pkg not in env1_packages}\n    different_versions = {pkg: (env1_packages[pkg], env2_packages[pkg]) for pkg in env1_packages if pkg in env2_packages and env1_packages[pkg] != env2_packages[pkg]}\n    \n    return only_in_env1, only_in_env2, different_versions\n\ndef find_imports(repo_dir):\n    imports = set()\n    for root, _, files in os.walk(repo_dir):\n        for file in files:\n            if file.endswith('.py'):\n                with open(os.path.join(root, file), 'r') as f:\n                    try:\n                        tree = ast.parse(f.read())\n                        for node in ast.walk(tree):\n                            if isinstance(node, ast.Import):\n                                for n in node.names:\n                                    imports.add(n.name.split('.')[0])\n                            elif isinstance(node, ast.ImportFrom):\n                                if node.level == 0:\n                                    imports.add(node.module.split('.')[0])\n                    except Exception as e:\n                        st.warning(f\"Error parsing {file}: {str(e)}\")\n    return imports\n\ndef generate_requirements_with_pipreqs(repo_dir, output_file=\"requirements.txt\"):\n    try:\n        subprocess.run(['pipreqs', repo_dir, '--force', '--savepath', output_file], check=True)\n        with open(output_file, 'r') as f:\n            return f.read()\n    except subprocess.CalledProcessError as e:\n        return f\"Error running pipreqs: {e}\"\n    except FileNotFoundError:\n        return \"Error: pipreqs not found. Please install it using 'pip install pipreqs'.\"\n\ndef generate_perfect_requirements(repo_dir, working_env):\n    repo_imports = find_imports(repo_dir)\n    env_packages = get_conda_list(working_env)\n    required_packages = {pkg: ver for pkg, ver in env_packages.items() if pkg in repo_imports}\n    \n    for imp in repo_imports:\n        if imp not in required_packages:\n            required_packages[imp] = \"latest\"\n    \n    requirements = [f\"{pkg}=={ver}\" if ver != \"latest\" else pkg for pkg, ver in required_packages.items()]\n    return \"\\n\".join(requirements)\n\ndef save_comparison(env1, env2, only_in_env1, only_in_env2, different_versions):\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    filename = f\"comparison_{env1}_vs_{env2}_{timestamp}.json\"\n    \n    comparison_data = {\n        \"env1\": env1,\n        \"env2\": env2,\n        \"only_in_env1\": only_in_env1,\n        \"only_in_env2\": only_in_env2,\n        \"different_versions\": different_versions\n    }\n    \n    with open(filename, \"w\") as f:\n        json.dump(comparison_data, f, indent=2)\n    \n    return filename\n\ndef load_comparison(filename):\n    with open(filename, \"r\") as f:\n        return json.load(f)\n\ndef export_env_to_yaml(env_name, yaml_file):\n    subprocess.run(['conda', 'env', 'export', '-n', env_name, '-f', yaml_file], check=True)\n\ndef convert_yaml_to_requirements(yaml_file, requirements_file):\n    with open(yaml_file, 'r') as file:\n        env = yaml.safe_load(file)\n    \n    dependencies = env.get('dependencies', [])\n    \n    pip_dependencies = []\n    for dep in dependencies:\n        if isinstance(dep, str):\n            package_name = dep.split('=')[0]\n            pip_dependencies.append(package_name)\n        elif isinstance(dep, dict) and 'pip' in dep:\n            for pip_dep in dep['pip']:\n                package_name = pip_dep.split('=')[0]\n                pip_dependencies.append(package_name)\n    \n    with open(requirements_file, 'w') as file:\n        for dep in pip_dependencies:\n            file.write(dep + '\\n')\n\nst.title(\"Conda Environment Comparison and Requirements Generator\")\n\nenv1 = st.text_input(\"Enter the name of the first Conda environment:\")\nenv2 = st.text_input(\"Enter the name of the second Conda environment:\")\nrepo_dir = st.text_input(\"Enter the repository directory:\")\n\nif st.button(\"Compare Environments\", key=\"compare_envs_button\"):\n    only_in_env1, only_in_env2, different_versions = compare_envs(env1, env2)\n    \n    st.subheader(\"Packages only in Environment 1:\")\n    st.json(only_in_env1)\n    \n    st.subheader(\"Packages only in Environment 2:\")\n    st.json(only_in_env2)\n    \n    st.subheader(\"Packages with different versions:\")\n    st.json(different_versions)\n    \n    saved_filename = save_comparison(env1, env2, only_in_env1, only_in_env2, different_versions)\n    st.success(f\"Comparison saved to {saved_filename}\")\n\nst.subheader(\"Load Previous Comparison\")\nsaved_files = [f for f in os.listdir() if f.startswith(\"comparison_\") and f.endswith(\".json\")]\nselected_file = st.selectbox(\"Select a saved comparison\", saved_files)\n\nif st.button(\"Load Comparison\", key=\"load_comparison_button\"):\n    loaded_data = load_comparison(selected_file)\n    st.json(loaded_data)\n\nst.subheader(\"Generate requirements.txt\")\nrequirements_method = st.radio(\"Choose method for generating requirements.txt:\", \n                               (\"Perfect requirements\", \"Using pipreqs\"))\n\nif requirements_method == \"Perfect requirements\":\n    working_env = st.radio(\"Select the working environment:\", (env1, env2), key=\"perfect_requirements_env\")\n    if st.button(\"Generate Perfect requirements.txt\", key=\"perfect_requirements_button\"):\n        perfect_requirements = generate_perfect_requirements(repo_dir, working_env)\n        st.text_area(\"Perfect requirements.txt\", perfect_requirements, height=300)\n        \n        if st.button(\"Save Perfect requirements.txt\", key=\"save_perfect_requirements_button\"):\n            with open(\"perfect_requirements.txt\", \"w\") as f:\n                f.write(perfect_requirements)\n            st.success(\"perfect_requirements.txt saved successfully!\")\n\nelif requirements_method == \"Using pipreqs\":\n    if st.button(\"Generate requirements.txt using pipreqs\", key=\"pipreqs_requirements_button\"):\n        pipreqs_requirements = generate_requirements_with_pipreqs(repo_dir)\n        st.text_area(\"Generated requirements.txt (pipreqs)\", pipreqs_requirements, height=300)\n        \n        if st.button(\"Save pipreqs requirements.txt\", key=\"save_pipreqs_requirements_button\"):\n            with open(\"pipreqs_requirements.txt\", \"w\") as f:\n                f.write(pipreqs_requirements)\n            st.success(\"pipreqs_requirements.txt saved successfully!\")\n\nst.subheader(\"Environment Comparison Table\")\nif env1 and env2:\n    env1_packages = get_conda_list(env1)\n    env2_packages = get_conda_list(env2)\n    all_packages = sorted(set(env1_packages.keys()) | set(env2_packages.keys()))\n    \n    data = []\n    for pkg in all_packages:\n        data.append({\n            \"Package\": pkg,\n            f\"{env1} Version\": env1_packages.get(pkg, \"Not installed\"),\n            f\"{env2} Version\": env2_packages.get(pkg, \"Not installed\")\n        })\n    \n    df = pd.DataFrame(data)\n    st.dataframe(df)\n\nst.subheader(\"Environment Package Search\")\nsearch_term = st.text_input(\"Search for a package:\")\nif search_term:\n    env1_packages = get_conda_list(env1)\n    env2_packages = get_conda_list(env2)\n    \n    results = []\n    for pkg, ver in env1_packages.items():\n        if search_term.lower() in pkg.lower():\n            results.append({\"Environment\": env1, \"Package\": pkg, \"Version\": ver})\n    for pkg, ver in env2_packages.items():\n        if search_term.lower() in pkg.lower():\n            results.append({\"Environment\": env2, \"Package\": pkg, \"Version\": ver})\n    \n    if results:\n        st.table(pd.DataFrame(results))\n    else:\n        st.warning(\"No matching packages found.\")\n"}
{"type": "source_file", "path": "corpus_management.py", "content": "def get_corpus_context(corpus_file, query):\n    # Load and split the corpus file\n    files_folder = \"files\"\n    if not os.path.exists(files_folder):\n        os.makedirs(files_folder)\n    try:\n        with open(os.path.join(files_folder, corpus_file), \"r\", encoding='utf-8') as f:\n            corpus_text = f.read()\n    except UnicodeDecodeError:\n        return \"Error: Unable to decode the corpus file. Please ensure it's a text file.\"\n\n    # Add progress bar for reading the corpus\n    st.info(f\"Reading corpus file: {corpus_file}\")\n    progress_bar = st.progress(0)\n    total_chars = len(corpus_text)\n    chars_processed = 0\n\n    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n    texts = []\n    for chunk in text_splitter.split_text(corpus_text):\n        texts.append(chunk)\n        chars_processed += len(chunk)\n        progress = chars_processed / total_chars\n        progress_bar.progress(progress)\n\n    # Create Langchain documents\n    docs = [Document(page_content=t) for t in texts]\n\n    # Create and load the vector database\n    st.info(\"Creating vector database...\")\n    embeddings = OllamaEmbeddings()\n    db = Chroma.from_documents(docs, embeddings, persist_directory=\"./chroma_db\")\n    db.persist()\n\n    # Perform similarity search\n    st.info(\"Performing similarity search...\")\n    results = db.similarity_search(query, k=3)\n    st.info(\"Done!\")\n    return \"\\n\".join([doc.page_content for doc in results])\n\ndef manage_corpus():\n    st.header(\"🗂 Manage Corpus\")\n\n    # Corpus folder\n    corpus_folder = \"corpus\"\n    if not os.path.exists(corpus_folder):\n        os.makedirs(corpus_folder)\n\n    # List existing corpus\n    corpus_list = [f for f in os.listdir(corpus_folder) if os.path.isdir(os.path.join(corpus_folder, f))]\n    st.subheader(\"⚫ Existing Corpus\")\n    if corpus_list:\n        for corpus in corpus_list:\n            col1, col2, col3 = st.columns([2, 1, 1])  # Add a column for renaming\n            with col1:\n                st.write(corpus)\n            with col2:\n                if st.button(\"✏️\", key=f\"rename_corpus_{corpus}\"):\n                    st.session_state.rename_corpus = corpus\n                    st.rerun()\n            with col3:\n                if st.button(\"🗑️\", key=f\"delete_corpus_{corpus}\"):\n                    shutil.rmtree(os.path.join(corpus_folder, corpus))\n                    st.success(f\"Corpus '{corpus}' deleted.\")\n                    st.rerun()\n    else:\n        st.write(\"No existing corpus found.\")\n\n    # Handle renaming corpus\n    if \"rename_corpus\" in st.session_state and st.session_state.rename_corpus:\n        corpus_to_rename = st.session_state.rename_corpus\n        new_corpus_name = st.text_input(f\"Rename corpus '{corpus_to_rename}' to:\", value=corpus_to_rename, key=f\"rename_corpus_input_{corpus_to_rename}\")\n        if st.button(\"Confirm Rename\", key=f\"confirm_rename_{corpus_to_rename}\"):\n            if new_corpus_name:\n                os.rename(os.path.join(corpus_folder, corpus_to_rename), os.path.join(corpus_folder, new_corpus_name))\n                st.success(f\"Corpus renamed to '{new_corpus_name}'\")\n                st.session_state.rename_corpus = None\n                st.rerun()\n            else:\n                st.error(\"Please enter a new corpus name.\")\n\n    st.subheader(\"➕ Create New Corpus\")\n    # Create corpus from files\n    st.write(\"**From Files:**\")\n    files_folder = \"files\"\n    allowed_extensions = ['.json', '.txt']\n    files = [f for f in os.listdir(files_folder) if os.path.isfile(os.path.join(files_folder, f)) and os.path.splitext(f)[1].lower() in allowed_extensions]\n    selected_files = st.multiselect(\"Select files to create corpus:\", files, key=\"create_corpus_files\")\n    corpus_name = st.text_input(\"Enter a name for the corpus:\", key=\"create_corpus_name\")\n    if st.button(\"Create Corpus from Files\", key=\"create_corpus_button\"):\n        if selected_files and corpus_name:\n            create_corpus_from_files(corpus_folder, corpus_name, files_folder, selected_files)\n            st.success(f\"Corpus '{corpus_name}' created from selected files.\")\n            st.rerun()\n        else:\n            st.error(\"Please select files and enter a corpus name.\")\n\ndef create_corpus_from_files(corpus_folder, corpus_name, files_folder, selected_files):\n    corpus_path = os.path.join(corpus_folder, corpus_name)\n    os.makedirs(corpus_path, exist_ok=True)\n    \n    # Combine all selected file content into one text\n    all_text = \"\"\n    for file in selected_files:\n        file_path = os.path.join(files_folder, file)\n        with open(file_path, \"r\", encoding='utf-8') as f:\n            file_content = f.read()\n        all_text += file_content + \"\\n\\n\"\n\n    create_corpus_from_text(corpus_folder, corpus_name, all_text)\n\ndef create_corpus_from_text(corpus_folder, corpus_name, corpus_text):\n    corpus_path = os.path.join(corpus_folder, corpus_name)\n    os.makedirs(corpus_path, exist_ok=True)\n\n    # Create Langchain documents\n    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n    texts = text_splitter.split_text(corpus_text)\n    docs = [Document(page_content=t) for t in texts]\n\n    # Create and load the vector database\n    embeddings = OllamaEmbeddings()\n    db = Chroma.from_documents(docs, embeddings, persist_directory=corpus_path)\n    db.persist()\n\ndef get_corpus_context_from_db(corpus_folder, corpus_name, query):\n    corpus_path = os.path.join(corpus_folder, corpus_name)\n    embeddings = OllamaEmbeddings()\n    db = Chroma(persist_directory=corpus_path, embedding_function=embeddings)\n    results = db.similarity_search(query, k=3)\n    return \"\\n\".join([doc.page_content for doc in results])"}
{"type": "source_file", "path": "nodes-run.py", "content": "import multiprocessing\nimport streamlit as st\n\n# Function to start the Quart app\ndef start_quart_app():\n    import os\n    os.system('python nodes.py')\n\nif __name__ == \"__main__\":\n    # Start the Quart app in a separate process\n    p = multiprocessing.Process(target=start_quart_app)\n    p.start()\n\n    # Your Streamlit code goes here\n    st.title('Streamlit and Quart Integration')\n    st.write('The Quart app is running in the background.')\n\n    # Ensure the Quart process is terminated when Streamlit stops\n    p.join()"}
{"type": "source_file", "path": "extension/generate_icons.py", "content": "from PIL import Image, ImageDraw\nimport os\n\ndef generate_llama_icon(size):\n    # Create a new image with a white background\n    image = Image.new('RGBA', (size, size), (255, 255, 255, 0))\n    draw = ImageDraw.Draw(image)\n    \n    # Calculate dimensions\n    padding = size // 4\n    width = size - (2 * padding)\n    height = size - (2 * padding)\n    \n    # Draw a simple llama silhouette\n    # Body\n    body_points = [\n        (padding + width//4, padding + height//2),  # Top of body\n        (padding + width*3//4, padding + height//2),  # Back\n        (padding + width*3//4, padding + height*3//4),  # Back leg\n        (padding + width//4, padding + height*3//4),  # Front leg\n    ]\n    \n    # Neck and head\n    neck_points = [\n        (padding + width//4, padding + height//2),  # Bottom of neck\n        (padding + width//4, padding + height//4),  # Top of neck\n        (padding + width//3, padding + height//5),  # Head\n    ]\n    \n    # Draw the llama in a nice blue color\n    draw.polygon(body_points, fill=(66, 133, 244, 255))  # Google Blue\n    draw.line(neck_points, fill=(66, 133, 244, 255), width=size//8)\n    \n    # Add ear\n    ear_size = size//8\n    draw.ellipse([\n        padding + width//3 - ear_size//2,\n        padding + height//5 - ear_size//2,\n        padding + width//3 + ear_size//2,\n        padding + height//5 + ear_size//2\n    ], fill=(66, 133, 244, 255))\n    \n    return image\n\ndef main():\n    # Create icons directory if it doesn't exist\n    os.makedirs('icons', exist_ok=True)\n    \n    # Generate icons of different sizes\n    sizes = [16, 48, 128]\n    for size in sizes:\n        icon = generate_llama_icon(size)\n        icon.save(f'icons/icon{size}.png')\n        print(f'Generated icon{size}.png')\n\nif __name__ == '__main__':\n    main()\n"}
{"type": "source_file", "path": "file_management.py", "content": "# file_management.py\nimport streamlit as st\nimport os\nimport shutil\nimport re\nimport tiktoken\n\ndef count_tokens(text):\n    encoding = tiktoken.get_encoding(\"cl100k_base\")\n    return len(encoding.encode(text))\n\ndef split_file(file_path, chunk_size, include_extension):\n    file_number = 1\n    file_base, file_ext = os.path.splitext(file_path)\n    with open(file_path, 'r', encoding='utf-8') as f:\n        chunk = f.read(chunk_size)\n        while chunk:\n            if include_extension:\n                chunk_file_path = f\"{file_base}.part{file_number}{file_ext}\"\n            else:\n                chunk_file_path = f\"{file_base}.part{file_number}\"\n            with open(chunk_file_path, 'w', encoding='utf-8') as chunk_file:\n                chunk_file.write(chunk)\n            file_number += 1\n            chunk = f.read(chunk_size)\n\ndef files_tab():\n    st.title(\"📂 Files\")\n    files_folder = \"files\"\n    if not os.path.exists(files_folder):\n        os.makedirs(files_folder)\n    allowed_extensions = ['.json', '.txt', '.pdf', '.gif', '.jpg', '.jpeg', '.png', '.md']\n    files = [f for f in os.listdir(files_folder) if os.path.isfile(os.path.join(files_folder, f)) and os.path.splitext(f)[1].lower() in allowed_extensions]\n\n    for file in files:\n        col1, col2, col3, col4 = st.columns([20, 1, 1, 1])\n        with col1:\n            st.write(file)\n        with col2:\n            if file.endswith('.pdf'):\n                st.button(\"📥\", key=f\"download_{file}\")\n            else:\n                st.button(\"👀\", key=f\"view_{file}\")\n        with col3:\n            if not file.endswith('.pdf'):\n                st.button(\"✏️\", key=f\"edit_{file}\")\n        with col4:\n            st.button(\"🗑️\", key=f\"delete_{file}\")\n\n    for file in files:\n        file_path = os.path.join(files_folder, file)\n        \n        if st.session_state.get(f\"view_{file}\", False):\n            try:\n                with open(file_path, \"r\", encoding='utf-8') as f:\n                    file_content = f.read()\n                st.text_area(\"File Content:\", value=file_content, height=200, key=f\"view_content_{file}\")\n            except UnicodeDecodeError:\n                st.error(f\"Unable to decode file {file}. It may be a binary file.\")\n        \n        if st.session_state.get(f\"edit_{file}\", False):\n            try:\n                with open(file_path, \"r\", encoding='utf-8') as f:\n                    file_content = f.read()\n                new_content = st.text_area(\"Edit File Content:\", value=file_content, height=200, key=f\"edit_content_{file}\")\n                if st.button(\"Save Changes\", key=f\"save_{file}\"):\n                    with open(file_path, \"w\", encoding='utf-8') as f:\n                        f.write(new_content)\n                    st.success(f\"Changes saved to {file}\")\n            except UnicodeDecodeError:\n                st.error(f\"Unable to decode file {file}. It may be a binary file.\")\n        \n        if st.session_state.get(f\"download_{file}\", False):\n            if file.endswith('.pdf'):\n                with open(file_path, \"rb\") as pdf_file:\n                    pdf_bytes = pdf_file.read()\n                st.download_button(\n                    label=\"Download PDF\",\n                    data=pdf_bytes,\n                    file_name=file,\n                    mime='application/pdf',\n                )\n            else:\n                with open(file_path, \"r\", encoding='utf-8') as f:\n                    file_content = f.read()\n                st.download_button(\n                    label=\"Download File\",\n                    data=file_content,\n                    file_name=file,\n                    mime='text/plain',\n                )\n        \n        if st.session_state.get(f\"delete_{file}\", False):\n            os.remove(file_path)\n            st.success(f\"File {file} deleted.\")\n            st.rerun()\n\n    uploaded_file = st.file_uploader(\"Upload a file\", type=['txt', 'pdf', 'json', 'gif', 'jpg', 'jpeg', 'png', 'md'])\n    if uploaded_file is not None:\n        file_path = os.path.join(files_folder, uploaded_file.name)\n        with open(file_path, \"wb\") as f:\n            f.write(uploaded_file.getbuffer())\n        st.success(f\"File {uploaded_file.name} uploaded successfully!\")\n        st.rerun()\n\n    st.subheader(\"✂️ Split File\")\n    text_files = [f for f in files if f.endswith(('.txt', '.md'))]\n    selected_file = st.selectbox(\"Select a text file to split\", text_files)\n    chunk_size_mb = st.slider(\"Chunk Size (MB)\", 1, 100, 20)\n    chunk_size = chunk_size_mb * 1024 * 1024  # Convert MB to bytes\n    include_extension = st.checkbox(\"Include original file extension in chunk filenames\", value=True)\n    if st.button(\"✂️ Split File\"):\n        if selected_file:\n            file_path = os.path.join(files_folder, selected_file)\n            split_file(file_path, chunk_size, include_extension)\n            st.success(f\"File '{selected_file}' split into chunks.\")\n            st.rerun()\n        else:\n            st.warning(\"Please select a file to split.\")\n"}
{"type": "source_file", "path": "groq_utils.py", "content": "# groq_utils.py\nimport os\nimport json\nimport streamlit as st\nfrom groq import Groq\nfrom typing import List, Dict\ntry:\n    from sentence_transformers import SentenceTransformer\nexcept ImportError:\n    print(\"Warning: sentence_transformers package not found, using fallback implementation\")\n    # Fallback implementation for sentence_transformers\n    class SentenceTransformer:\n        def __init__(self, model_name):\n            self.model_name = model_name\n            print(f\"Warning: Using fallback SentenceTransformer with model: {model_name}\")\n            \n        def encode(self, text, **kwargs):\n            import numpy as np\n            print(f\"Warning: Using fallback encoding for text: {text[:50]}...\")\n            # Return a random embedding vector of size 384 (same as all-MiniLM-L6-v2)\n            return np.random.rand(384)\n\nGROQ_MODELS = [\n    \"llama-3.1-70b-versatile\",\n    \"llama-3.1-8b-instant\",\n    \"llama3-groq-70b-8192-tool-use-preview\",\n    \"llama3-groq-8b-8192-tool-use-preview\",\n    \"llama-guard-3-8b\",\n    \"llama3-70b-8192\",\n    \"llama3-8b-8192\",\n    \"mixtral-8x7b-32768\",\n    \"gemma-7b-it\",\n    \"gemma2-9b-it\",\n]\n\nAPI_KEYS_FILE = \"api_keys.json\"\n\n# Load the embedding model\n@st.cache_resource\ndef load_embedding_model():\n    return SentenceTransformer('all-MiniLM-L6-v2')\n\ndef load_api_keys():\n    \"\"\"Loads API keys from the JSON file.\"\"\"\n    if os.path.exists(API_KEYS_FILE):\n        with open(API_KEYS_FILE, \"r\") as f:\n            return json.load(f)\n    return {}\n\ndef save_api_keys(api_keys):\n    \"\"\"Saves API keys to the JSON file.\"\"\"\n    with open(API_KEYS_FILE, \"w\") as f:\n        json.dump(api_keys, f, indent=4)\n\ndef get_groq_client(api_key: str):\n    \"\"\"Returns a Groq client instance if API key is available, otherwise returns None.\"\"\"\n    if not api_key:\n        return None\n    try:\n        return Groq(api_key=api_key)\n    except Exception as e:\n        st.warning(\"Groq API key not configured. Some features will be limited to local models.\")\n        return None\n\ndef call_groq_api(client: Groq, model: str, messages: List[Dict[str, str]], temperature: float = 0.7, max_tokens: int = 1000) -> str:\n    \"\"\"Calls the Groq API for chat completions.\"\"\"\n    if not client:\n        return None\n    try:\n        chat_completion = client.chat.completions.create(\n            model=model,\n            messages=messages,\n            temperature=temperature,\n            max_tokens=max_tokens\n        )\n        return chat_completion.choices[0].message.content\n    except Exception as e:\n        st.warning(f\"Error calling Groq API: {str(e)}\")\n        return None\n\ndef get_local_embeddings(text: str) -> List[float]:\n    \"\"\"Generates embeddings using a local model.\"\"\"\n    model = load_embedding_model()\n    return model.encode(text).tolist()\n\ndef display_groq_settings():\n    \"\"\"Displays the Groq API key settings.\"\"\"\n    st.sidebar.subheader(\"Groq API Key\")\n    api_keys = load_api_keys()\n    groq_api_key = st.sidebar.text_input(\n        \"Enter your Groq API key:\",\n        value=api_keys.get(\"groq_api_key\", \"\"),\n        type=\"password\",\n    )\n    if st.sidebar.button(\"Save Groq API Key\"):\n        api_keys[\"groq_api_key\"] = groq_api_key\n        save_api_keys(api_keys)\n        st.success(\"Groq API key saved!\")"}
{"type": "source_file", "path": "nodes.py", "content": "# nodes.py\n\nimport json\nimport logging\nimport streamlit as st\nimport requests\nimport io\nimport base64\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom email.mime.text import MIMEText\nfrom email.mime.multipart import MIMEMultipart\nfrom typing import List, Dict, Union, Tuple\nfrom ollama_utils import get_available_models, call_ollama_endpoint, load_api_keys\nfrom openai_utils import OPENAI_MODELS, call_openai_api\nfrom groq_utils import GROQ_MODELS, call_groq_api\nfrom prompts import get_agent_prompt, get_metacognitive_prompt, get_voice_prompt, get_identity_prompt\n\n# Initialize logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# Define available node types\nAVAILABLE_NODE_TYPES = {\n    \"Input\": True,\n    \"Processing\": True,\n    \"LLM\": True,\n    \"Output\": True,\n    \"DataRetrieval\": False,  # Set to False until fully implemented\n    \"Control\": False,  # Set to False until fully implemented\n    \"Integration\": False,  # Set to False until fully implemented\n    \"Utility\": False  # Set to False until fully implemented\n}\n\n# Define color scheme and emojis\nNODE_COLORS = {\n    \"Input\": \"#90EE90\",\n    \"LLM\": \"#ADD8E6\",\n    \"Output\": \"#FFB6C1\",\n    \"Processing\": \"#FFD700\",\n    \"DataRetrieval\": \"#FFA07A\",\n    \"Control\": \"#98FB98\",\n    \"Integration\": \"#DDA0DD\",\n    \"Utility\": \"#87CEFA\"\n}\n\nNODE_EMOJIS = {\n    \"Input\": \"⤵️\",\n    \"LLM\": \"🧠\",\n    \"Output\": \"⤴️\",\n    \"Processing\": \"⚙️\",\n    \"DataRetrieval\": \"🔍\",\n    \"Control\": \"🔀\",\n    \"Integration\": \"🔌\",\n    \"Utility\": \"🛠️\"\n}\n\nclass Node:\n    \"\"\"Represents a node in the workflow.\"\"\"\n    def __init__(self, id: str, node_type: str, data: dict):\n        \"\"\"\n        Initializes a Node object.\n\n        Args:\n            id (str): The unique identifier of the node.\n            node_type (str): The type of the node (e.g., 'Input', 'LLM', 'Output').\n            data (dict): A dictionary containing the data and configuration of the node.\n        \"\"\"\n        self.id = id\n        self.type = node_type\n        self.data = data\n\n    def to_dict(self):\n        \"\"\"Converts the Node object to a dictionary.\"\"\"\n        return {\n            'id': self.id,\n            'type': self.type,\n            'data': self.data\n        }\n\n    @staticmethod\n    def from_dict(data: dict):\n        \"\"\"Creates a Node object from a dictionary.\"\"\"\n        node_type = data['type']\n        default_data = create_node(\"temp\", node_type).data\n        merged_data = {**default_data, **data.get('data', {})}\n        return Node(data['id'], data['type'], merged_data)\n\nclass Edge:\n    \"\"\"Represents an edge connecting two nodes in the workflow.\"\"\"\n    def __init__(self, id: str, source: str, target: str):\n        \"\"\"\n        Initializes an Edge object.\n\n        Args:\n            id (str): The unique identifier of the edge.\n            source (str): The ID of the source node.\n            target (str): The ID of the target node.\n        \"\"\"\n        self.id = id\n        self.source = source\n        self.target = target\n\n    def to_dict(self):\n        \"\"\"Converts the Edge object to a dictionary.\"\"\"\n        return {\n            'id': self.id,\n            'source': self.source,\n            'target': self.target\n        }\n\n    @staticmethod\n    def from_dict(data: dict):\n        \"\"\"Creates an Edge object from a dictionary.\"\"\"\n        try:\n            return Edge(data['id'], data['source'], data['target'])\n        except KeyError as e:\n            raise KeyError(f\"Missing key {e} in edge data: {data}\")\n\ndef get_all_models() -> list:\n    \"\"\"Retrieves a list of all available models.\"\"\"\n    ollama_models = get_available_models()\n    all_models = ollama_models + OPENAI_MODELS + GROQ_MODELS\n    return all_models\n\ndef call_api_and_decode_response(api_function, *args, **kwargs) -> dict:\n    \"\"\"Calls the specified API function, decodes the JSON response, and validates its structure.\"\"\"\n    try:\n        response = api_function(*args, **kwargs)\n        \n        if not response or not isinstance(response, str) or response.strip() == \"\":\n            raise ValueError(\"Received empty or invalid response from the API\")\n\n        try:\n            workflow_data = json.loads(response)\n        except json.JSONDecodeError as e:\n            logger.error(f\"JSON decoding error: {str(e)} - Response content: {response}\")\n            raise ValueError(f\"Invalid JSON response: {str(e)}\")\n        \n        if 'nodes' not in workflow_data or 'edges' not in workflow_data:\n            raise ValueError(\"Invalid JSON structure: Missing 'nodes' or 'edges'\")\n\n        for edge in workflow_data.get('edges', []):\n            if 'source' not in edge or 'target' not in edge:\n                raise ValueError(f\"Missing key 'source' or 'target' in edge data: {edge}\")\n        \n        return workflow_data\n    \n    except ValueError as e:\n        logger.error(f\"Validation error: {str(e)}\")\n        raise\n    except Exception as e:\n        logger.error(f\"Unexpected error: {str(e)}\")\n        raise\n\ndef generate_workflow(user_request: str, model: str) -> Tuple[List[Node], List[Edge]]:\n    \"\"\"Generates a workflow based on the user's request using the specified language model.\"\"\"\n    available_types = [node_type for node_type, available in AVAILABLE_NODE_TYPES.items() if available]\n    \n    prompt = f\"\"\"\n    Generate a workflow to accomplish the following task:\n    {user_request}\n\n    You can use the following node types:\n    {', '.join(available_types)}\n\n    Provide a JSON output with the following structure:\n    {{\n        \"nodes\": [\n            {{\n                \"id\": \"1\",\n                \"type\": \"Input\",\n                \"data\": {{\n                    \"content\": \"Input Node\",\n                    \"input_type\": \"Text\",\n                    \"input_text\": \"\"\n                }}\n            }},\n            ...\n        ],\n        \"edges\": [\n            {{\n                \"id\": \"1-2\",\n                \"source\": \"1\",\n                \"target\": \"2\"\n            }},\n            ...\n        ]\n    }}\n\n    Ensure that the workflow starts with an Input node and ends with an Output node.\n    Include appropriate nodes as needed, using only the node types listed above.\n    Provide detailed configurations for each node, including prompts for LLM nodes and specific settings for other node types.\n    \"\"\"\n\n    api_keys = load_api_keys()\n\n    try:\n        if model in OPENAI_MODELS:\n            response = call_openai_api(\n                model,\n                [{\"role\": \"user\", \"content\": prompt}],\n                temperature=0.7,\n                max_tokens=2000,\n                openai_api_key=api_keys.get(\"openai_api_key\")\n            )\n            # OpenAI models return the response directly, so we need to parse it\n            workflow_data = parse_openai_response(response)\n        elif model in GROQ_MODELS:\n            response = call_groq_api(\n                model,\n                prompt,\n                temperature=0.7,\n                max_tokens=2000,\n                groq_api_key=api_keys.get(\"groq_api_key\")\n            )\n            # Groq models might need similar parsing as OpenAI\n            workflow_data = parse_openai_response(response)\n        else:\n            response, _, _, _ = call_ollama_endpoint(\n                model,\n                prompt=prompt,\n                temperature=0.7,\n                max_tokens=2000\n            )\n            # Ollama models might return a different format, so we'll parse it differently\n            workflow_data = parse_ollama_response(response)\n        \n        nodes = [Node.from_dict(node_data) for node_data in workflow_data[\"nodes\"]]\n        edges = [Edge.from_dict(edge_data) for edge_data in workflow_data[\"edges\"]]\n        return nodes, edges\n    except Exception as e:\n        st.error(f\"Failed to generate a valid workflow: {str(e)}\")\n        return [], []\n\ndef parse_openai_response(response: str) -> Dict:\n    \"\"\"Parses the response from OpenAI models.\"\"\"\n    try:\n        # Find the JSON object in the response\n        json_start = response.find('{')\n        json_end = response.rfind('}') + 1\n        if json_start != -1 and json_end != -1:\n            json_str = response[json_start:json_end]\n            workflow_data = json.loads(json_str)\n            if 'nodes' not in workflow_data or 'edges' not in workflow_data:\n                raise ValueError(\"Invalid JSON structure: Missing 'nodes' or 'edges'\")\n            return workflow_data\n        else:\n            raise ValueError(\"No valid JSON object found in the response\")\n    except json.JSONDecodeError as e:\n        raise ValueError(f\"Invalid JSON in OpenAI response: {str(e)}\")\n\ndef parse_ollama_response(response: str) -> Dict:\n    \"\"\"Parses the response from Ollama models.\"\"\"\n    try:\n        # Ollama might return the entire conversation, so we need to extract the last message\n        messages = response.split('\\n\\n')\n        last_message = messages[-1]\n        \n        # Find the JSON object in the last message\n        json_start = last_message.find('{')\n        json_end = last_message.rfind('}') + 1\n        if json_start != -1 and json_end != -1:\n            json_str = last_message[json_start:json_end]\n            workflow_data = json.loads(json_str)\n            if 'nodes' not in workflow_data or 'edges' not in workflow_data:\n                raise ValueError(\"Invalid JSON structure: Missing 'nodes' or 'edges'\")\n            \n            # Ensure each edge has 'source' and 'target'\n            for edge in workflow_data['edges']:\n                if 'source' not in edge or 'target' not in edge:\n                    edge['source'] = edge.get('source', edge['id'].split('-')[0])\n                    edge['target'] = edge.get('target', edge['id'].split('-')[1])\n            \n            return workflow_data\n        else:\n            raise ValueError(\"No valid JSON object found in the Ollama response\")\n    except json.JSONDecodeError as e:\n        raise ValueError(f\"Invalid JSON in Ollama response: {str(e)}\")\n\ndef execute_workflow(nodes: List[Node], edges: List[Edge]) -> Dict[str, str]:\n    \"\"\"Executes the workflow defined by the nodes and edges.\"\"\"\n    results = {}\n    node_map = {node.id: node for node in nodes}\n    api_keys = load_api_keys()\n\n    def process_node(node_id: str) -> str:\n        \"\"\"Recursively processes a node and its inputs.\"\"\"\n        if node_id in results:\n            return results[node_id]\n\n        if node_id not in node_map:\n            logger.error(f\"Node {node_id} not found in workflow\")\n            return f\"Error: Node {node_id} not found\"\n\n        node = node_map[node_id]\n        incoming_edges = [edge for edge in edges if edge.target == node_id]\n\n        try:\n            if node.type == 'Input':\n                results[node_id] = handle_input_node(node)\n            elif node.type == 'Processing':\n                results[node_id] = handle_processing_node(node, incoming_edges, process_node)\n            elif node.type == 'LLM':\n                results[node_id] = handle_llm_node(node, incoming_edges, process_node, api_keys)\n            elif node.type == 'Output':\n                results[node_id] = handle_output_node(node, incoming_edges, process_node)\n            else:\n                logger.error(f\"Unsupported node type: {node.type}\")\n                results[node_id] = f\"Error: Unsupported node type {node.type}\"\n        except Exception as e:\n            logger.error(f\"Error processing node {node_id}: {str(e)}\")\n            results[node_id] = f\"Error processing node {node_id}: {str(e)}\"\n\n        return results[node_id]\n\n    for node in nodes:\n        process_node(node.id)\n\n    return results\n\ndef handle_input_node(node: Node) -> str:\n    \"\"\"Handles the execution of an Input node.\"\"\"\n    if node.data['input_type'] == 'Text':\n        return node.data['input_text']\n    elif node.data['input_type'] == 'File':\n        if node.data['file_upload'] is not None:\n            return node.data['file_upload'].getvalue().decode('utf-8')\n        else:\n            return \"Error: No file uploaded\"\n    elif node.data['input_type'] == 'API':\n        try:\n            response = requests.get(node.data['api_endpoint'])\n            response.raise_for_status()\n            return response.text\n        except requests.RequestException as e:\n            return f\"Error fetching API: {str(e)}\"\n    else:\n        return f\"Unknown input type: {node.data['input_type']}\"\n\ndef handle_processing_node(node: Node, incoming_edges: List[Edge], process_node) -> str:\n    \"\"\"Handles the execution of a Processing node.\"\"\"\n    if not incoming_edges:\n        return \"Error: Processing node requires input\"\n    input_data = process_node(incoming_edges[0].source)\n\n    if node.data['processing_type'] == 'Preprocessing':\n        for step in node.data['preprocessing_steps']:\n            if step == 'Tokenization':\n                input_data = input_data.split()\n            elif step == 'Lowercasing':\n                input_data = input_data.lower()\n            elif step == 'Remove Punctuation':\n                input_data = ''.join(char for char in input_data if char.isalnum() or char.isspace())\n            elif step == 'Remove Stopwords':\n                stopwords = set(['the', 'a', 'an', 'in', 'on', 'at', 'for', 'to', 'of'])\n                input_data = ' '.join([word for word in input_data.split() if word.lower() not in stopwords])\n        return ' '.join(input_data) if isinstance(input_data, list) else input_data\n\n    elif node.data['processing_type'] == 'Logic':\n        logic_type = node.data['logic_type']\n        if logic_type == 'Monte Carlo Tree Search':\n            return execute_mcts_subworkflow(node)\n        elif logic_type in ['Chain of Thought', 'Tree of Thought', 'Visualization of Thought']:\n            return apply_metacognitive_prompt(node, input_data)\n        else:\n            return f\"Unknown logic type: {logic_type}\"\n\n    elif node.data['processing_type'] == 'Vectorization':\n        return f\"Vectorized: {input_data[:50]}...\"\n    else:\n        return f\"Unknown processing type: {node.data['processing_type']}\"\n\ndef handle_llm_node(node: Node, incoming_edges: List[Edge], process_node, api_keys: Dict[str, str]) -> str:\n    \"\"\"Handles the execution of an LLM node.\"\"\"\n    if not incoming_edges:\n        return \"Error: LLM node requires input\"\n    input_text = process_node(incoming_edges[0].source)\n    prompt = node.data['prompt']\n    complete_prompt = construct_prompt(node, prompt, input_text)\n    \n    if node.data['model_name'] in OPENAI_MODELS:\n        response = call_openai_api(\n            node.data['model_name'], \n            [{\"role\": \"user\", \"content\": complete_prompt}],\n            temperature=node.data['temperature'],\n            max_tokens=node.data['max_tokens'],\n            openai_api_key=api_keys.get(\"openai_api_key\")\n        )\n    elif node.data['model_name'] in GROQ_MODELS:\n        response = call_groq_api(\n            node.data['model_name'],\n            complete_prompt,\n            temperature=node.data['temperature'],\n            max_tokens=node.data['max_tokens'],\n            groq_api_key=api_keys.get(\"groq_api_key\")\n        )\n    else:\n        response, _, _, _ = call_ollama_endpoint(\n            node.data['model_name'],\n            prompt=complete_prompt,\n            temperature=node.data['temperature'],\n            max_tokens=node.data['max_tokens'],\n            presence_penalty=node.data['presence_penalty'],\n            frequency_penalty=node.data['frequency_penalty']\n        )\n\n    # Append to conversation history\n    node.data['conversation_history'].append({\"role\": \"user\", \"content\": input_text})\n    node.data['conversation_history'].append({\"role\": \"assistant\", \"content\": response})\n\n    return response\n\ndef handle_output_node(node: Node, incoming_edges: List[Edge], process_node) -> str:\n    \"\"\"Handles the execution of an Output node.\"\"\"\n    if not incoming_edges:\n        return \"Error: Output node requires input\"\n    input_data = process_node(incoming_edges[0].source)\n    if node.data['output_type'] == 'Text':\n        return f\"{node.data['output_label']}: {input_data}\"\n    elif node.data['output_type'] == 'File':\n        return input_data  # Return the raw data for file download\n    elif node.data['output_type'] == 'Visualization':\n        plt.figure(figsize=(10, 5))\n        plt.plot([1, 2, 3, 4, 5], [1, 4, 2, 3, 5])\n        plt.title(input_data[:30])\n        buf = io.BytesIO()\n        plt.savefig(buf, format='png')\n        buf.seek(0)\n        return base64.b64encode(buf.getvalue()).decode('utf-8')\n    else:\n        return f\"Unknown output type: {node.data['output_type']}\"\n\ndef construct_prompt(node: Node, base_prompt: str, input_text: str) -> str:\n    \"\"\"Constructs the complete prompt for an LLM node.\"\"\"\n    prompt_parts = []\n    if node.data.get('agent_type', 'None') != 'None':\n        prompt_parts.append(get_agent_prompt()[node.data['agent_type']])\n    if node.data.get('metacognitive_type', 'None') != 'None':\n        prompt_parts.append(get_metacognitive_prompt()[node.data['metacognitive_type']])\n    if node.data.get('voice_type', 'None') != 'None':\n        prompt_parts.append(get_voice_prompt()[node.data['voice_type']])\n    if node.data.get('identity_type', 'None') != 'None':\n        prompt_parts.append(get_identity_prompt()[node.data['identity_type']])\n    prompt_parts.append(base_prompt)\n    prompt_parts.append(f\"User Input: {input_text}\")\n    \n    if node.data['conversation_history']:\n        history = \"\\n\".join([f\"{msg['role']}: {msg['content']}\" for msg in node.data['conversation_history'][-5:]])\n        prompt_parts.append(f\"Recent Conversation:\\n{history}\")\n    \n    return \"\\n\\n\".join(prompt_parts)\n\ndef validate_workflow(nodes: List[Node], edges: List[Edge]) -> Dict[str, Union[bool, str]]:\n    \"\"\"Validates the workflow for structural correctness.\"\"\"\n    if not any(node.type == 'Input' for node in nodes):\n        return {'valid': False, 'error': 'Workflow must have at least one Input node.'}\n\n    if not any(node.type == 'Output' for node in nodes):\n        return {'valid': False, 'error': 'Workflow must have at least one Output node.'}\n\n    node_ids = set(node.id for node in nodes)\n    connected_nodes = set()\n    for edge in edges:\n        if edge.source not in node_ids or edge.target not in node_ids:\n            return {'valid': False, 'error': f'Edge {edge.id} connects to non-existent nodes: {edge.source} -> {edge.target}'}\n        connected_nodes.add(edge.source)\n        connected_nodes.add(edge.target)\n    if node_ids != connected_nodes:\n        return {'valid': False, 'error': 'All nodes must be connected in the workflow.'}\n\n    if has_cycle(nodes, edges):\n        return {'valid': False, 'error': 'Workflow contains a cycle. It must be acyclic.'}\n\n    if not path_exists_input_to_output(nodes, edges):\n        return {'valid': False, 'error': 'There must be a path from an Input node to an Output node.'}\n\n    return {'valid': True, 'error': None}\n\ndef has_cycle(nodes: List[Node], edges: List[Edge]) -> bool:\n    \"\"\"Checks if the workflow graph contains a cycle.\"\"\"\n    graph = {node.id: set() for node in nodes}\n    for edge in edges:\n        graph[edge.source].add(edge.target)\n\n    visited = set()\n    rec_stack = set()\n\n    def is_cyclic(node_id: str) -> bool:\n        visited.add(node_id)\n        rec_stack.add(node_id)\n\n        for neighbor in graph[node_id]:\n            if neighbor not in visited:\n                if is_cyclic(neighbor):\n                    return True\n            elif neighbor in rec_stack:\n                return True\n\n        rec_stack.remove(node_id)\n        return False\n\n    for node in nodes:\n        if node.id not in visited:\n            if is_cyclic(node.id):\n                return True\n\n    return False\n\ndef path_exists_input_to_output(nodes: List[Node], edges: List[Edge]) -> bool:\n    \"\"\"Checks if there is a path from an Input node to an Output node.\"\"\"\n    graph = {node.id: set() for node in nodes}\n    for edge in edges:\n        graph[edge.source].add(edge.target)\n\n    input_nodes = [node.id for node in nodes if node.type == 'Input']\n    output_nodes = [node.id for node in nodes if node.type == 'Output']\n\n    def dfs(node_id: str, visited: set) -> bool:\n        if node_id in output_nodes:\n            return True\n        visited.add(node_id)\n        for neighbor in graph[node_id]:\n            if neighbor not in visited:\n                if dfs(neighbor, visited):\n                    return True\n        return False\n\n    for input_node in input_nodes:\n        if dfs(input_node, set()):\n            return True\n\n    return False\n\ndef execute_mcts_subworkflow(node: Node) -> str:\n    \"\"\"Executes a sub-workflow as part of MCTS logic.\"\"\"\n    # Placeholder for MCTS logic; this would involve creating and managing a sub-workflow.\n    # For demonstration purposes, we'll mock the MCTS behavior.\n    return \"MCTS decision result\"\n\ndef apply_metacognitive_prompt(node: Node, input_data: str) -> str:\n    \"\"\"Applies a metacognitive prompt for Chain of Thought, Tree of Thought, or Visualization of Thought.\"\"\"\n    metacognitive_type = node.data.get('metacognitive_type', 'None')\n    prompt = get_metacognitive_prompt()[metacognitive_type]\n    # Normally, we'd interact with the LLM API here using the prompt and input_data.\n    # For now, we'll mock the LLM response.\n    return f\"{metacognitive_type} result based on input: {input_data}\"\n\ndef render_node_settings(node: Node) -> None:\n    \"\"\"Renders the settings panel for the selected node in the Streamlit sidebar.\"\"\"\n    st.sidebar.subheader(f\"Configure {NODE_EMOJIS[node.type]} {node.type} Node {node.id}\")\n    node.data['content'] = st.sidebar.text_input(\"Node Label\", value=node.data['content'], key=f\"label_{node.id}\")\n\n    if node.type == 'Input':\n        node.data['input_type'] = st.sidebar.selectbox(\"Input Type\", [\"Text\", \"File\", \"API\"], key=f\"input_type_{node.id}\")\n        if node.data['input_type'] == 'Text':\n            node.data['input_text'] = st.sidebar.text_area(\"Input Text\", value=node.data['input_text'], key=f\"input_text_{node.id}\")\n        elif node.data['input_type'] == 'File':\n            node.data['file_upload'] = st.sidebar.file_uploader(\"Upload File\", key=f\"file_upload_{node.id}\")\n        elif node.data['input_type'] == 'API':\n            node.data['api_endpoint'] = st.sidebar.text_input(\"API Endpoint\", value=node.data['api_endpoint'], key=f\"api_endpoint_{node.id}\")\n\n    elif node.type == 'Processing':\n        node.data['processing_type'] = st.sidebar.selectbox(\"Processing Type\", [\"Preprocessing\", \"Logic\", \"Vectorization\"], key=f\"processing_type_{node.id}\")\n        if node.data['processing_type'] == 'Preprocessing':\n            node.data['preprocessing_steps'] = st.sidebar.multiselect(\"Preprocessing Steps\", [\"Tokenization\", \"Lowercasing\", \"Remove Punctuation\", \"Remove Stopwords\"], key=f\"preprocessing_steps_{node.id}\")\n        elif node.data['processing_type'] == 'Vectorization':\n            node.data['vectorization_model'] = st.sidebar.selectbox(\"Vectorization Model\", [\"Word2Vec\", \"GloVe\", \"FastText\"], key=f\"vectorization_model_{node.id}\")\n        elif node.data['processing_type'] == 'Logic':\n            metacognitive_types = list(get_metacognitive_prompt().keys())\n            logic_types = [\"Monte Carlo Tree Search\"] + metacognitive_types\n            node.data['logic_type'] = st.sidebar.selectbox(\"Logic Type\", logic_types, key=f\"logic_type_{node.id}\")\n\n    elif node.type == 'LLM':\n        all_models = get_all_models()\n        node.data['model_name'] = st.sidebar.selectbox(\n            \"Select Model\", \n            all_models, \n            index=all_models.index(node.data['model_name']) if node.data['model_name'] in all_models else 0, \n            key=f\"model_{node.id}\"\n        )\n        node.data['agent_type'] = st.sidebar.selectbox(\"Agent Type\", [\"None\"] + list(get_agent_prompt().keys()), index=([\"None\"] + list(get_agent_prompt().keys())).index(node.data['agent_type']), key=f\"agent_type_{node.id}\")\n        node.data['voice_type'] = st.sidebar.selectbox(\"Voice Type\", [\"None\"] + list(get_voice_prompt().keys()), index=([\"None\"] + list(get_voice_prompt().keys())).index(node.data['voice_type']), key=f\"voice_type_{node.id}\")\n        node.data['identity_type'] = st.sidebar.selectbox(\"Identity Type\", [\"None\"] + list(get_identity_prompt().keys()), index=([\"None\"] + list(get_identity_prompt().keys())).index(node.data['identity_type']), key=f\"identity_type_{node.id}\")\n        node.data['temperature'] = st.sidebar.slider(\"Temperature\", 0.0, 1.0, node.data['temperature'], key=f\"temperature_{node.id}\")\n        node.data['max_tokens'] = st.sidebar.slider(\"Max Tokens\", 1000, 128000, node.data['max_tokens'], step=1000, key=f\"max_tokens_{node.id}\")\n        node.data['presence_penalty'] = st.sidebar.slider(\"Presence Penalty\", -2.0, 2.0, node.data['presence_penalty'], step=0.1, key=f\"presence_penalty_{node.id}\")\n        node.data['frequency_penalty'] = st.sidebar.slider(\"Frequency Penalty\", -2.0, 2.0, node.data['frequency_penalty'], step=0.1, key=f\"frequency_penalty_{node.id}\")\n        node.data['prompt'] = st.sidebar.text_area(\"Prompt\", value=node.data['prompt'], key=f\"prompt_input_{node.id}\")\n        node.data['fine_tuning'] = st.sidebar.checkbox(\"Enable Fine-tuning\", value=node.data['fine_tuning'], key=f\"fine_tuning_{node.id}\")\n\n    elif node.type == 'Output':\n        node.data['output_type'] = st.sidebar.selectbox(\"Output Type\", [\"Text\", \"File\", \"Visualization\"], key=f\"output_type_{node.id}\")\n        node.data['output_label'] = st.sidebar.text_input(\"Output Label\", value=node.data['output_label'], key=f\"output_label_{node.id}\")\n        if node.data['output_type'] == 'Text':\n            node.data['document_format'] = st.sidebar.selectbox(\"Document Format\", [\"Text\", \"Markdown\", \"HTML\"], index=[\"Text\", \"Markdown\", \"HTML\"].index(node.data['document_format']), key=f\"document_format_{node.id}\")\n        elif node.data['output_type'] == 'File':\n            node.data['file_format'] = st.sidebar.selectbox(\"File Format\", [\"txt\", \"csv\", \"json\"], key=f\"file_format_{node.id}\")\n        elif node.data['output_type'] == 'Visualization':\n            node.data['visualization_type'] = st.sidebar.selectbox(\"Visualization Type\", [\"Bar Chart\", \"Line Chart\", \"Scatter Plot\"], key=f\"visualization_type_{node.id}\")\n\n    if st.sidebar.button(\"Update Node\", key=f\"update_node_{node.id}\"):\n        st.success(f\"Node {node.id} updated successfully!\")\n        st.rerun()\n\n\n\ndef render_workflow_canvas(nodes: List[Node], edges: List[Edge]) -> None:\n    \"\"\"Renders the workflow canvas, displaying nodes and connections.\"\"\"\n    cols = st.columns(3)\n    for i, node in enumerate(nodes):\n        with cols[i % 3]:\n            with st.container():\n                if st.button(f\"{NODE_EMOJIS[node.type]} Node {node.id}: {node.type}\", \n                             key=f\"node_button_{node.id}\",\n                             use_container_width=True):\n                    st.session_state.selected_node_id = node.id\n                    st.rerun()\n                \n                st.markdown(f\"<div style='background-color: {NODE_COLORS[node.type]}; padding: 10px; border-radius: 5px;'>\", unsafe_allow_html=True)\n                st.write(f\"**Content:** {node.data.get('content', 'No content specified')}\")\n                if node.type == 'Input':\n                    st.write(f\"**Input Type:** {node.data.get('input_type', 'Not specified')}\")\n                elif node.type == 'Processing':\n                    st.write(f\"**Processing Type:** {node.data.get('processing_type', 'Not specified')}\")\n                elif node.type == 'LLM':\n                    st.write(f\"**Model:** {node.data.get('model_name', 'Not specified')}\")\n                elif node.type == 'Output':\n                    st.write(f\"**Output Type:** {node.data.get('output_type', 'Not specified')}\")\n                st.markdown(\"</div>\", unsafe_allow_html=True)\n\n    st.subheader(\"🔀 Connections\")\n    for edge in edges:\n        st.markdown(f\"<div style='text-align: center; font-weight: bold;'>Node {edge.source} ➡️ Node {edge.target}</div>\", unsafe_allow_html=True)\n        \ndef nodes_interface() -> None:\n    \"\"\"Provides the Streamlit interface for the LLM workflow builder.\"\"\"\n    st.title(\"✳️ Compound Elemental Framework (CEF)\")\n\n    st.markdown(\"\"\"\n    <style>\n    div.stButton > button:first-child.stBtn.secondary {\n        background-color: blue;\n        color: white;\n    }\n    div.stButton > button:hover:first-child.stBtn.secondary {\n        background-color: red;\n        color: white;\n    }\n    </style>\n    \"\"\", unsafe_allow_html=True)\n    \n    if 'nodes' not in st.session_state:\n        st.session_state['nodes'] = [\n            create_node(\"1\", \"Input\"),\n            create_node(\"2\", \"LLM\"),\n            create_node(\"3\", \"Output\")\n        ]\n    if 'edges' not in st.session_state:\n        st.session_state['edges'] = [\n            Edge(\"1-2\", \"1\", \"2\"),\n            Edge(\"2-3\", \"2\", \"3\")\n        ]\n    if 'selected_node_id' not in st.session_state:\n        st.session_state.selected_node_id = None\n\n    mode = st.radio(\"Select mode:\", [\"Manual\", \"AI-Assisted\"])\n\n    if mode == \"AI-Assisted\":\n        user_request = st.text_input(\"Enter your request for the AI to generate a workflow:\")\n        workflow_model = st.selectbox(\"Select model for workflow generation:\", get_all_models())\n        if st.button(\"Generate Workflow\"):\n            with st.spinner(\"Generating workflow...\"):\n                nodes, edges = generate_workflow(user_request, workflow_model)\n                if nodes and edges:\n                    st.session_state['nodes'] = nodes\n                    st.session_state['edges'] = edges\n                    st.success(\"Workflow generated successfully!\")\n                    st.rerun()\n\n    with st.sidebar:\n        st.subheader(\"🛠️ Workflow Actions\")\n        \n        st.subheader(\"➕ Add New Node\")\n        available_node_types = [node_type for node_type, available in AVAILABLE_NODE_TYPES.items() if available]\n        node_type = st.selectbox(\"Select node type\", available_node_types)\n        if st.button(\"Add Node\", type=\"primary\"):\n            new_node_id = str(len(st.session_state['nodes']) + 1)\n            new_node = create_node(new_node_id, node_type)\n            st.session_state['nodes'].append(new_node)\n            st.rerun()\n\n        st.subheader(\"➕ Add New Edge\")\n        source = st.selectbox(\"Source Node\", [node.id for node in st.session_state['nodes']], key=\"edge_source\")\n        target = st.selectbox(\"Target Node\", [node.id for node in st.session_state['nodes']], key=\"edge_target\")\n        if st.button(\"Add Edge\", type=\"primary\"):\n            new_edge = Edge(f\"{source}-{target}\", source, target)\n            if new_edge not in st.session_state['edges']:\n                st.session_state['edges'].append(new_edge)\n                st.rerun()\n\n        st.subheader(\"⚙️ Configure Node\")\n        selected_node = next((node for node in st.session_state['nodes'] if node.id == st.session_state.selected_node_id), None)\n        if selected_node:\n            render_node_settings(selected_node)\n        else:\n            st.info(\"Select a node from the canvas to configure it.\")\n\n    render_workflow_canvas(st.session_state['nodes'], st.session_state['edges'])\n\n    st.subheader(\"🎛️ Workflow Controls\")\n    if st.button(\"▶️ Execute Workflow\", type=\"primary\"):\n        validation_result = validate_workflow(st.session_state['nodes'], st.session_state['edges'])\n        if validation_result['valid']:\n            with st.spinner(\"Executing workflow...\"):\n                results = execute_workflow(st.session_state['nodes'], st.session_state['edges'])\n            st.write(\"Workflow Execution Results:\")\n            for node_id, result in results.items():\n                node = next(node for node in st.session_state['nodes'] if node.id == node_id)\n                st.subheader(f\"{NODE_EMOJIS[node.type]} Node {node_id} ({node.type}):\")\n                if node.type == 'Output':\n                    if node.data['output_type'] == 'Text':\n                        if node.data['document_format'] == 'Markdown':\n                            st.markdown(result)\n                        elif node.data['document_format'] == 'HTML':\n                            st.components.v1.html(result, height=300)\n                        else:\n                            st.text(result)\n                    elif node.data['output_type'] == 'File':\n                        file_contents = result.encode('utf-8')\n                        st.download_button(\n                            label=\"Download Output File\",\n                            data=file_contents,\n                            file_name=f\"output.{node.data['file_format']}\",\n                            mime=f\"text/{node.data['file_format']}\"\n                        )\n                    elif node.data['output_type'] == 'Visualization':\n                        st.image(result)\n                else:\n                    st.text(result)\n        else:\n            st.error(f\"Workflow validation failed: {validation_result['error']}\")\n\n    st.html(\"<hr />\")\n    col1, col2, col3, col4 = st.columns(4)\n    with col1:\n        if st.button(\"✅ Validate Workflow\", type=\"primary\"):\n            validation_result = validate_workflow(st.session_state['nodes'], st.session_state['edges'])\n            if validation_result['valid']:\n                st.success(\"Workflow is valid! ✅\")\n            else:\n                st.error(f\"Workflow validation failed: {validation_result['error']}\")\n    with col2:\n        if st.button(\"💾 Save Workflow\", type=\"secondary\"):\n            save_workflow(st.session_state['nodes'], st.session_state['edges'])\n            st.success(\"Workflow saved successfully!\")\n    with col3:\n        if st.button(\"📂 Load Workflow\", type=\"secondary\"):\n            loaded_nodes, loaded_edges = load_workflow()\n            if loaded_nodes:\n                st.session_state['nodes'] = loaded_nodes\n                st.session_state['edges'] = loaded_edges\n                st.success(\"Workflow loaded successfully!\")\n                st.rerun()\n            else:\n                st.warning(\"No saved workflow found.\")\n    with col4:\n        if st.button(\"📤 Export as Script\", type=\"secondary\"):\n            script = export_workflow_as_script(st.session_state['nodes'], st.session_state['edges'])\n            st.download_button(\n                label=\"Download Python Script\",\n                data=script,\n                file_name=\"workflow_script.py\",\n                mime=\"text/plain\"\n            )\n\n\n    st.subheader(\"📊 Workflow Statistics\")\n    st.write(f\"Total Nodes: {len(st.session_state['nodes'])}\")\n    st.write(f\"Total Edges: {len(st.session_state['edges'])}\")\n\n    node_types = {}\n    for node in st.session_state['nodes']:\n        node_types[node.type] = node_types.get(node.type, 0) + 1\n\n    st.write(\"Node Types:\")\n    for node_type, count in node_types.items():\n        st.write(f\"  - {NODE_EMOJIS[node_type]} {node_type}: {count}\")\n\n    with st.expander(\"❓ Help\"):\n        st.markdown(\"\"\"\n        ### How to use the LLM Workflow Builder:\n        1. **Add Nodes**: Use the sidebar to add various types of nodes (Input, Processing, LLM, Output).\n        2. **Connect Nodes**: Add edges to connect nodes in the desired order.\n        3. **Configure Nodes**: Click on a node in the canvas to configure its settings in the sidebar.\n        4. **Execute Workflow**: Click the 'Execute Workflow' button to run your workflow.\n        5. **Save/Load**: Save your workflow for later use or load a previously saved workflow.\n        6. **Validate**: Use the 'Validate Workflow' button to check if your workflow is properly constructed.\n        7. **Export**: Export your workflow as a Python script for external execution.\n\n        ### AI-Assisted Mode:\n        1. Select \"AI-Assisted\" mode.\n        2. Enter your workflow request in natural language.\n        3. Choose a model for workflow generation.\n        4. Click \"Generate Workflow\" to create a workflow based on your request.\n        5. Review and modify the generated workflow as needed.\n\n        For more detailed instructions, please refer to the documentation.\n        \"\"\")\n\n    st.subheader(\"🗑️ Remove Nodes or Edges\")\n    remove_type = st.radio(\"Select what to remove:\", [\"Node\", \"Edge\"])\n    if remove_type == \"Node\":\n        node_to_remove = st.selectbox(\"Select node to remove:\", [f\"{NODE_EMOJIS[node.type]} Node {node.id}\" for node in st.session_state['nodes']])\n        if st.button(\"Remove Node\", type=\"secondary\", key=\"remove_node_button\"):\n            node_id = node_to_remove.split()[-1]\n            st.session_state['nodes'] = [node for node in st.session_state['nodes'] if node.id != node_id]\n            st.session_state['edges'] = [edge for edge in st.session_state['edges'] if edge.source != node_id and edge.target != node_id]\n            st.success(f\"Node {node_id} and its connected edges removed.\")\n            st.rerun()\n    else:\n        edge_to_remove = st.selectbox(\"Select edge to remove:\", [f\"Node {edge.source} ➡️ Node {edge.target}\" for edge in st.session_state['edges']])\n        if st.button(\"Remove Edge\", type=\"secondary\", key=\"remove_edge_button\"):\n            source, target = edge_to_remove.split(\"➡️\")\n            source = source.split()[-1]\n            target = target.split()[-1]\n            st.session_state['edges'] = [edge for edge in st.session_state['edges'] if not (edge.source == source and edge.target == target)]\n            st.success(f\"Edge {edge_to_remove} removed.\")\n            st.rerun()\n\ndef create_node(node_id: str, node_type: str) -> Node:\n    \"\"\"Creates a new node with default settings based on its type.\"\"\"\n    if not AVAILABLE_NODE_TYPES.get(node_type, False):\n        raise ValueError(f\"Node type '{node_type}' is not available or implemented.\")\n    \n    base_data = {\n        'content': f'{node_type} Node',\n    }\n    if node_type == 'Input':\n        base_data.update({\n            'input_type': 'Text',\n            'input_text': '',\n            'file_upload': None,\n            'api_endpoint': '',\n        })\n    elif node_type == 'Processing':\n        base_data.update({\n            'processing_type': 'Preprocessing',\n            'preprocessing_steps': [],\n            'vectorization_model': 'default',\n        })\n    elif node_type == 'LLM':\n        all_models = get_all_models()\n        base_data.update({\n            'model_name': all_models[0] if all_models else 'gpt-3.5-turbo',\n            'agent_type': 'None',\n            'metacognitive_type': 'None',\n            'voice_type': 'None',\n            'identity_type': 'None',\n            'temperature': 0.7,\n            'max_tokens': 4000,\n            'presence_penalty': 0.0,\n            'frequency_penalty': 0.0,\n            'prompt': 'Enter Prompt:',\n            'fine_tuning': False,\n            'conversation_history': [],\n        })\n    elif node_type == 'Output':\n        base_data.update({\n            'output_type': 'Text',\n            'output_label': 'Output:',\n            'document_format': 'Text',\n            'file_format': 'txt',\n            'visualization_type': 'None',\n        })\n    return Node(node_id, node_type, base_data)\n\ndef save_workflow(nodes: List[Node], edges: List[Edge]) -> None:\n    \"\"\"Saves the current workflow to a JSON file.\"\"\"\n    workflow_data = {\n        \"nodes\": [node.to_dict() for node in nodes],\n        \"edges\": [edge.to_dict() for edge in edges]\n    }\n    with open(\"workflow.json\", \"w\") as f:\n        json.dump(workflow_data, f, indent=2)\n\ndef load_workflow() -> Tuple[List[Node], List[Edge]]:\n    \"\"\"Loads a workflow from a JSON file.\"\"\"\n    try:\n        with open(\"workflow.json\", \"r\") as f:\n            workflow_data = json.load(f)\n        nodes = [Node.from_dict(node_data) for node_data in workflow_data[\"nodes\"]]\n        edges = [Edge.from_dict(edge_data) for edge_data in workflow_data[\"edges\"]]\n        return nodes, edges\n    except FileNotFoundError:\n        st.warning(\"No saved workflow found.\")\n        return [], []\n    except json.JSONDecodeError:\n        st.error(\"Error decoding the saved workflow. The file may be corrupted.\")\n        return [], []\n\ndef export_workflow_as_script(nodes: List[Node], edges: List[Edge]) -> str:\n    \"\"\"Exports the workflow as a Python script.\"\"\"\n    script = \"\"\"\nimport requests\nimport json\nimport logging\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\ndef execute_workflow():\n    results = {}\n    \"\"\"\n\n    for node in nodes:\n        script += f\"\\n    # Node {node.id}: {node.type}\\n\"\n        if node.type == 'Input':\n            script += f\"    results['{node.id}'] = {json.dumps(node.data['input_text'])}\\n\"\n        elif node.type == 'Processing':\n            script += f\"    input_data = results['{edges[0].source}']\\n\"\n            script += f\"    # Add processing logic here\\n\"\n            script += f\"    results['{node.id}'] = input_data  # Placeholder\\n\"\n        elif node.type == 'LLM':\n            script += f\"    input_data = results['{edges[0].source}']\\n\"\n            script += f\"    # Add LLM API call logic here\\n\"\n            script += f\"    results['{node.id}'] = 'LLM response'  # Placeholder\\n\"\n        elif node.type == 'Output':\n            script += f\"    input_data = results['{edges[0].source}']\\n\"\n            script += f\"    print(f'Output: {{input_data}}')\\n\"\n\n    script += \"\\n    return results\\n\\n\"\n    script += \"if __name__ == '__main__':\\n\"\n    script += \"    execute_workflow()\\n\"\n\n    return script\n\ndef main() -> None:\n    \"\"\"The main function to run the Streamlit app.\"\"\"\n    st.set_page_config(page_title=\"Compound Elemental Framework (CEF)\", page_icon=\"✳️\", layout=\"wide\")\n    nodes_interface()\n\nif __name__ == \"__main__\":\n    main()"}
{"type": "source_file", "path": "mistral_utils.py", "content": "# mistral_utils.py\nimport os\nimport json\nimport streamlit as st\nfrom mistralai import Mistral\nfrom typing import List, Dict, Union, AsyncGenerator\n\nMISTRAL_MODELS = [\n    \"mistral-large-latest\",\n    \"mistral-tiny\",\n    \"mistral-embed\",\n    \"pixtral-12b-2409\",\n    \"open-mistral-nemo\",\n    \"open-codestral-mamba\"\n]\n\nAPI_KEYS_FILE = \"api_keys.json\"\n\ndef load_api_keys():\n    \"\"\"Loads API keys from the JSON file.\"\"\"\n    if os.path.exists(API_KEYS_FILE):\n        with open(API_KEYS_FILE, \"r\") as f:\n            return json.load(f)\n    return {}\n\ndef save_api_keys(api_keys):\n    \"\"\"Saves API keys to the JSON file.\"\"\"\n    with open(API_KEYS_FILE, \"w\") as f:\n        json.dump(api_keys, f, indent=4)\n\ndef get_mistral_client(api_key: str = None) -> Union[Mistral, None]:\n    \"\"\"Returns a Mistral client instance if API key is available.\"\"\"\n    if not api_key:\n        api_key = load_api_keys().get('mistral_api_key')\n    if not api_key:\n        return None\n    try:\n        return Mistral(api_key=api_key)\n    except Exception as e:\n        st.warning(f\"Error initializing Mistral client: {str(e)}\")\n        return None\n\ndef call_mistral_api(\n    model: str,\n    messages: List[Dict[str, str]] = None,\n    prompt: str = None,\n    temperature: float = 0.7,\n    max_tokens: int = 1000,\n    stream: bool = False,\n    json_response: bool = False,\n    mistral_api_key: str = None\n) -> Union[str, AsyncGenerator]:\n    \"\"\"Wrapper function to call the Mistral Chat API with a unified interface.\"\"\"\n    client = get_mistral_client(mistral_api_key)\n    if not client:\n        return None\n\n    try:\n        kwargs = {\n            \"model\": model,\n            \"temperature\": temperature,\n            \"max_tokens\": max_tokens,\n        }\n        \n        # Convert prompt to messages format if needed\n        if prompt is not None:\n            kwargs[\"messages\"] = [{\"role\": \"user\", \"content\": prompt}]\n        elif messages is not None:\n            kwargs[\"messages\"] = messages\n        else:\n            raise ValueError(\"Either prompt or messages must be provided\")\n\n        if json_response:\n            kwargs[\"response_format\"] = {\"type\": \"json_object\"}\n\n        if stream:\n            response = client.chat.stream(**kwargs)\n            return response\n        else:\n            response = client.chat.complete(**kwargs)\n            return response.choices[0].message.content.strip()\n    except Exception as e:\n        st.error(f\"Error calling Mistral API: {e}\")\n        return None\n\nasync def call_mistral_api_async(\n    model: str,\n    messages: List[Dict[str, str]],\n    temperature: float = 0.7,\n    max_tokens: int = 1000,\n    json_response: bool = False,\n    mistral_api_key: str = None\n) -> AsyncGenerator:\n    \"\"\"Async wrapper function to call the Mistral Chat API.\"\"\"\n    client = get_mistral_client(mistral_api_key)\n    if not client:\n        return None\n\n    try:\n        kwargs = {\n            \"model\": model,\n            \"messages\": messages,\n            \"temperature\": temperature,\n            \"max_tokens\": max_tokens,\n        }\n        \n        if json_response:\n            kwargs[\"response_format\"] = {\"type\": \"json_object\"}\n\n        response = await client.chat.stream_async(**kwargs)\n        return response\n    except Exception as e:\n        st.error(f\"Error calling Mistral API async: {e}\")\n        return None\n\ndef call_mistral_embeddings(text: Union[str, List[str]], model: str = \"mistral-embed\", mistral_api_key: str = None):\n    \"\"\"Calls the Mistral Embeddings API.\"\"\"\n    client = get_mistral_client(mistral_api_key)\n    if not client:\n        return None\n\n    try:\n        if isinstance(text, str):\n            text = [text]\n        \n        response = client.embeddings.create(\n            model=model,\n            inputs=text\n        )\n        \n        if len(text) == 1:\n            return response.data[0].embedding\n        return [item.embedding for item in response.data]\n    except Exception as e:\n        st.error(f\"Error calling Mistral Embeddings API: {e}\")\n        return None\n\ndef display_mistral_settings():\n    \"\"\"Displays the Mistral API key settings in the sidebar.\"\"\"\n    st.sidebar.subheader(\"Mistral API Key\")\n    api_keys = load_api_keys()\n    mistral_api_key = st.sidebar.text_input(\n        \"Enter your Mistral API key:\",\n        value=api_keys.get(\"mistral_api_key\", \"\"),\n        type=\"password\",\n    )\n    if st.sidebar.button(\"Save Mistral API Key\"):\n        api_keys[\"mistral_api_key\"] = mistral_api_key\n        save_api_keys(api_keys)\n        st.success(\"Mistral API key saved!\")\n"}
{"type": "source_file", "path": "openai_utils.py", "content": "# openai_utils.py\nimport os\nimport json\nimport streamlit as st\ntry:\n    from openai import OpenAI\nexcept ImportError:\n    print(\"Warning: openai package not found, using fallback implementation\")\n    # Fallback implementation for openai\n    class OpenAI:\n        def __init__(self, api_key=None):\n            self.api_key = api_key\n            \n        class Completion:\n            @staticmethod\n            def create(*args, **kwargs):\n                return {\"choices\": [{\"text\": \"OpenAI API not available - please install the openai package\"}]}\n        \n        class ChatCompletion:\n            @staticmethod\n            def create(*args, **kwargs):\n                return {\"choices\": [{\"message\": {\"content\": \"OpenAI API not available - please install the openai package\"}}]}\n                \n        class Embedding:\n            @staticmethod\n            def create(*args, **kwargs):\n                return {\"data\": [{\"embedding\": [0.0] * 1536}]}\n                \n        completion = Completion()\n        chat = ChatCompletion()\n        embeddings = Embedding()\n\nOPENAI_MODELS = [\n    \"gpt-4-1106-preview\",\n    \"gpt-4-0125-preview\",\n    \"gpt-4-turbo\",\n    \"gpt-4o-mini\",\n    \"gpt-4o-2024-08-06\",\n    \"gpt-4o\",\n    \"gpt-4\",\n    \"gpt-4-0613\",\n    \"gpt-4-0314\",\n    \"gpt-3.5-turbo\",\n    \"gpt-3.5-turbo-1106\",\n    \"gpt-3.5-turbo-0125\",\n    \"gpt-3.5-turbo-instruct\"\n]\n\n\nAPI_KEYS_FILE = \"api_keys.json\"\n\ndef load_api_keys():\n    \"\"\"Loads API keys from the JSON file.\"\"\"\n    if os.path.exists(API_KEYS_FILE):\n        with open(API_KEYS_FILE, \"r\") as f:\n            return json.load(f)\n    return {}\n\ndef save_api_keys(api_keys):\n    \"\"\"Saves API keys to the JSON file.\"\"\"\n    with open(API_KEYS_FILE, \"w\") as f:\n        json.dump(api_keys, f, indent=4)\n\ndef set_openai_api_key(api_key):\n    \"\"\"Sets the OpenAI API key.\"\"\"\n    api_keys = load_api_keys()\n    api_keys['openai_api_key'] = api_key\n    save_api_keys(api_keys)\n    st.success(\"OpenAI API key has been set.\")\n\ndef call_openai_api(model, messages, temperature=0.7, max_tokens=1000, frequency_penalty=0.0, presence_penalty=0.0, stream=False, openai_api_key=None):\n    \"\"\"Wrapper function to call the OpenAI Chat API with a unified interface.\"\"\"\n    client = OpenAI(api_key=openai_api_key)\n    \n    try:\n        response = client.chat.completions.create(\n            model=model,\n            messages=messages,\n            temperature=temperature,\n            max_tokens=max_tokens,\n            frequency_penalty=frequency_penalty,\n            presence_penalty=presence_penalty,\n            stream=stream\n        )\n        \n        if stream:\n            return response  # Return the stream object\n        else:\n            return response.choices[0].message.content.strip()\n    except Exception as e:\n        st.error(f\"Error calling OpenAI API: {e}\")\n        return None\n\ndef call_openai_embeddings(model, input_text):\n    \"\"\"Calls the OpenAI Embeddings API.\"\"\"\n    client = OpenAI(api_key=load_api_keys().get('openai_api_key'))\n    \n    try:\n        response = client.embeddings.create(\n            model=model,\n            input=input_text\n        )\n        return response.data[0].embedding\n    except Exception as e:\n        st.error(f\"Error calling OpenAI API: {e}\")\n        return None\n\ndef display_openai_settings():\n    \"\"\"Displays the OpenAI API key settings in the sidebar.\"\"\"\n    st.sidebar.subheader(\"OpenAI API Key\")\n    api_keys = load_api_keys()\n    openai_api_key = st.sidebar.text_input(\n        \"Enter your OpenAI API key:\",\n        value=api_keys.get(\"openai_api_key\", \"\"),\n        type=\"password\",\n    )\n    if st.sidebar.button(\"Save OpenAI API Key\"):\n        set_openai_api_key(openai_api_key)"}
{"type": "source_file", "path": "ollama_chat.py", "content": "# ollama_chat.py\nimport streamlit as st\nimport streamlit.components.v1 as components\nfrom ollama_utils import call_ollama_endpoint\n\ndef ollama_chat():\n    \"\"\"Streamlit component for the Ollama Workbench chat interface.\"\"\"\n    model = st.session_state.get('model', 'mistral:instruct')\n    web_page_content = st.session_state.get('web_page_content', None)\n    web_page_url = st.session_state.get('url', None)\n\n    # Construct the initial prompt\n    initial_prompt = \"\"\n    if web_page_content:\n        initial_prompt = f\"You are an AI assistant working within a browser extension. You have access to the current web page's content. Please use this information to answer the user's question.\\n\\nWebpage URL: {web_page_url}\\nWebpage Content:\\n{web_page_content}\\n\\n\"\n    else:\n        initial_prompt = \"You are an AI assistant. How can I help you today?\\n\\n\"\n\n    # Initialize chat history\n    if \"chat_history\" not in st.session_state:\n        st.session_state.chat_history = [{\"role\": \"assistant\", \"content\": initial_prompt}]\n\n    for message in st.session_state.chat_history:\n        with st.chat_message(message[\"role\"]):\n            st.write(message[\"content\"])\n\n    user_input = st.chat_input(\"Enter your message:\")\n    if user_input:\n        st.session_state.chat_history.append({\"role\": \"user\", \"content\": user_input})\n        \n        # Construct the full prompt\n        prompt = initial_prompt + f\"User: {user_input}\\nAssistant:\"\n        \n        # Call the Ollama API\n        response, _, _, _ = call_ollama_endpoint(model, prompt=prompt, temperature=0.7, max_tokens=1000) # Hard-coded settings for now\n        \n        st.session_state.chat_history.append({\"role\": \"assistant\", \"content\": response})\n        st.experimental_rerun()\n\n# Declare the Streamlit component\ncomponents.declare_component(\n    \"ollama_chat\",\n    url=\"http://localhost:8502/chat_endpoint\"  # Hard-coded URL\n)"}
{"type": "source_file", "path": "ollama_utils.py", "content": "# ollama_utils.py\n\nimport requests\nimport json\nimport io\n\nimport streamlit as st\nimport numpy as np\nimport ollama\nimport psutil\nimport platform\nimport subprocess\nimport os\n\nfrom openai_utils import (\n    call_openai_api,\n    OPENAI_MODELS\n)\nfrom groq_utils import get_local_embeddings, GROQ_MODELS\nfrom mistral_utils import MISTRAL_MODELS\n\nAPI_KEYS_FILE = \"api_keys.json\"\nMODEL_SETTINGS_FILE = \"model_settings.json\"\n\ndef load_api_keys():\n    \"\"\"Loads API keys from the JSON file.\"\"\"\n    if os.path.exists(API_KEYS_FILE):\n        with open(API_KEYS_FILE, \"r\") as f:\n            return json.load(f)\n    return {}\n\ndef save_api_keys(api_keys):\n    \"\"\"Saves API keys to the JSON file.\"\"\"\n    with open(API_KEYS_FILE, \"w\") as f:\n        json.dump(api_keys, f, indent=4)\n\ndef load_model_settings():\n    \"\"\"Loads model settings from the JSON file.\"\"\"\n    if os.path.exists(MODEL_SETTINGS_FILE):\n        with open(MODEL_SETTINGS_FILE, \"r\") as f:\n            return json.load(f)\n    return {}\n\ndef save_model_settings(settings):\n    \"\"\"Saves model settings to the JSON file.\"\"\"\n    with open(MODEL_SETTINGS_FILE, \"w\") as f:\n        json.dump(settings, f, indent=4)\n\nOLLAMA_URL = \"http://localhost:11434/api\"\n\n@st.cache_data(ttl=0)\ndef get_ollama_resource_usage():\n    \"\"\"Gets Ollama server resource usage.\"\"\"\n    try:\n        # Check if Ollama process is running\n        for process in psutil.process_iter(['name', 'cpu_percent', 'memory_percent']):\n            if process.info['name'] == 'ollama':\n                cpu_usage = process.info['cpu_percent']\n                memory_usage = process.info['memory_percent']\n\n                # Get GPU usage if available (placeholder for now)\n                gpu_usage = \"N/A\"\n\n                # Check server responsiveness\n                response = requests.get(\"http://localhost:11434/api/tags\")\n                server_status = \"Running\" if response.status_code == 200 else \"Not Responding\"\n\n                return {\n                    \"status\": server_status,\n                    \"cpu_usage\": f\"{cpu_usage:.2f}%\",\n                    \"memory_usage\": f\"{memory_usage:.2f}%\",\n                    \"gpu_usage\": gpu_usage\n                }\n\n        return {\"status\": \"Not Running\", \"cpu_usage\": \"N/A\", \"memory_usage\": \"N/A\", \"gpu_usage\": \"N/A\"}\n    except Exception as e:\n        return {\"status\": f\"Error: {str(e)}\", \"cpu_usage\": \"N/A\", \"memory_usage\": \"N/A\", \"gpu_usage\": \"N/A\"}\n\n@st.cache_data\ndef get_available_models():\n    response = requests.get(f\"{OLLAMA_URL}/tags\")\n    response.raise_for_status()\n    models = [\n        model[\"name\"]\n        for model in response.json()[\"models\"]\n        if \"embed\" not in model[\"name\"]\n    ]\n    return models\n\ndef call_ollama_endpoint(model, prompt=None, image=None, temperature=0.5, max_tokens=150, presence_penalty=0.0, frequency_penalty=0.0, context=None, tools=None, episodic_memory=None):\n    payload = {\n        \"model\": model,\n        \"temperature\": temperature,\n        \"max_tokens\": max_tokens,\n        \"presence_penalty\": presence_penalty,\n        \"frequency_penalty\": frequency_penalty,\n        \"context\": context if context is not None else [],\n        \"tools\": tools if tools is not None else []\n    }\n    if prompt:\n        payload[\"prompt\"] = prompt\n    if image:\n        # Read image data into BytesIO\n        image_bytesio = io.BytesIO(image.read())\n\n        # Determine image format and filename\n        image_format = \"image/jpeg\" if image.type == \"image/jpeg\" else \"image/png\"\n        filename = \"image.jpg\" if image.type == \"image/jpeg\" else \"image.png\"\n\n        # Send image data using multipart/form-data\n        files = {\"file\": (filename, image_bytesio, image_format)}\n        response = requests.post(f\"{OLLAMA_URL}/generate\", data=payload, files=files, stream=True)\n    else:\n        response = requests.post(f\"{OLLAMA_URL}/generate\", json=payload, stream=True)\n    try:\n        response.raise_for_status()\n    except requests.exceptions.RequestException as e:\n        return f\"An error occurred: {str(e)}\", None, None, None\n\n    response_parts = []\n    eval_count = None\n    eval_duration = None\n    for line in response.iter_lines():\n        part = json.loads(line)\n        response_parts.append(part.get(\"response\", \"\"))\n        if part.get(\"done\", False):\n            eval_count = part.get(\"eval_count\", None)\n            eval_duration = part.get(\"eval_duration\", None)\n            break\n    return \"\".join(response_parts), part.get(\"context\", None), eval_count, eval_duration\n\ndef check_json_handling(model, temperature, max_tokens, presence_penalty, frequency_penalty):\n    prompt = \"Return the following data in JSON format: name: John, age: 30, city: New York\"\n    result, _, _, _ = call_ollama_endpoint(model, prompt=prompt, temperature=temperature, max_tokens=max_tokens, presence_penalty=presence_penalty, frequency_penalty=frequency_penalty)\n    try:\n        json.loads(result)\n        return True\n    except json.JSONDecodeError:\n        return False\n\ndef get_token_embeddings(model: str, text: str, api_keys: dict) -> np.ndarray:\n    \"\"\"Gets embeddings for each token in the text and returns a 2D array.\"\"\"\n    try:\n        if model in OPENAI_MODELS:\n            response = call_openai_api(\n                \"text-embedding-ada-002\",  # OpenAI's embedding model\n                prompt=[{\"role\": \"user\", \"content\": text}],\n                openai_api_key=api_keys.get(\"openai_api_key\"),\n                use_chat=False\n            )\n            embeddings = np.array(response['data'][0]['embedding'])\n        elif model in GROQ_MODELS:\n            embeddings = get_local_embeddings(text)\n        else:\n            response = ollama.embeddings(model=model, prompt=text)\n            embeddings = np.array(response['embedding'])\n        \n        return embeddings.reshape(1, -1)  # Ensure it's a 2D array\n    except Exception as e:\n        st.error(f\"An error occurred while getting token embeddings: {e}\")\n        return np.zeros((1, 1536))  # Return a default 2D array with 1536 features (common embedding size)\n\ndef check_function_calling(model, temperature, max_tokens, presence_penalty, frequency_penalty):\n    prompt = \"Define a function named 'add' that takes two numbers and returns their sum. Then call the function with arguments 5 and 3.\"\n    result, _, _, _ = call_ollama_endpoint(model, prompt=prompt, temperature=temperature, max_tokens=max_tokens, presence_penalty=presence_penalty, frequency_penalty=frequency_penalty)\n    return \"8\" in result\n\ndef run_tool_test(model, description, tool_description, test_function, arguments):\n    prompt = f\"Test the function: {tool_description}. Arguments: {arguments}\"\n    result, _, _, _ = call_ollama_endpoint(model, prompt=prompt)\n    return result\n\ndef pull_model(model_name):\n    payload = {\"name\": model_name, \"stream\": True}\n    response = requests.post(f\"{OLLAMA_URL}/pull\", json=payload, stream=True)\n    response.raise_for_status()\n    progress_bar = st.progress(0)\n    status_text = st.empty()\n    results = []\n    total = None\n    st.write(f\"📥 Pulling model: `{model_name}`\")\n    for line in response.iter_lines():\n        line = line.decode(\"utf-8\")\n        try:\n            data = json.loads(line)\n            \n            if \"total\" in data and \"completed\" in data:\n                total = data[\"total\"]\n                completed = data[\"completed\"]\n                progress = completed / total\n                progress_bar.progress(progress)\n                status_text.text(f\"Progress: {progress * 100:.2f}%\")\n            elif \"status\" in data:\n                if not data[\"status\"].startswith(\"pulling\"):\n                    status_text.text(data[\"status\"])\n                if data[\"status\"] == \"success\":\n                    break\n            else:\n                # Handle cases where neither \"total\" nor \"status\" is present\n                status_text.text(\"Processing...\")\n            \n            results.append(data)\n        except json.JSONDecodeError:\n            st.warning(f\"Failed to parse JSON: {line}\")\n        \n    return results\n\ndef show_model_info(model_name):\n    payload = {\"name\": model_name}\n    response = requests.post(f\"{OLLAMA_URL}/show\", json=payload)\n    response.raise_for_status()\n    return response.json()\n\ndef remove_model(model_name):\n    payload = {\"name\": model_name}\n    response = requests.delete(f\"{OLLAMA_URL}/delete\", json=payload)\n    if response.status_code == 200:\n        try:\n            return response.json()\n        except json.JSONDecodeError:\n            return {\"status\": \"success\", \"message\": f\"Model '{model_name}' removed successfully.\"}\n    else:\n        return {\"status\": \"error\", \"message\": f\"Failed to remove model '{model_name}'. Status code: {response.status_code}\"}\n\ndef save_chat_history(chat_history, filename=\"chat_history.json\"):\n    with open(filename, \"w\") as f:\n        json.dump(chat_history, f)\n\ndef load_chat_history(filename):\n    with open(filename, \"r\") as f:\n        return json.load(f)\n\ndef update_model_selection(selected_models, key):\n    st.session_state[key] = selected_models\n\ndef preload_model(model_name):\n    \"\"\"Preloads a model into memory.\"\"\"\n    try:\n        response = requests.post(f\"{OLLAMA_URL}/generate\", json={\"model\": model_name})\n        response.raise_for_status()\n        st.success(f\"Model '{model_name}' preloaded successfully!\")\n    except requests.exceptions.RequestException as e:\n        st.error(f\"Error preloading model '{model_name}': {e}\")\n\ndef stop_server():\n    \"\"\"Stops the Ollama server.\"\"\"\n    try:\n        subprocess.run([\"osascript\", \"-e\", 'tell app \"Ollama\" to quit'], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n        st.success(\"Ollama server stopped.\")\n    except Exception as e:\n        st.error(f\"Error stopping Ollama server: {e}\")\n\ndef apply_server_settings(host, origins, model_dir, global_keep_alive, max_loaded_models, num_parallel, max_queue):\n    \"\"\"Applies server settings using environment variables.\"\"\"\n    try:\n        if host:\n            os.environ[\"OLLAMA_HOST\"] = host\n        if origins:\n            formatted_origins = \" \".join([f\"http://{origin.strip()}\" for origin in origins.split(\",\")])\n            os.environ[\"OLLAMA_ORIGINS\"] = formatted_origins\n        if model_dir:\n            os.environ[\"OLLAMA_MODELS\"] = model_dir\n        if global_keep_alive:\n            os.environ[\"OLLAMA_KEEP_ALIVE\"] = global_keep_alive\n        if max_loaded_models:\n            os.environ[\"OLLAMA_MAX_LOADED_MODELS\"] = str(max_loaded_models)\n        if num_parallel:\n            os.environ[\"OLLAMA_NUM_PARALLEL\"] = str(num_parallel)\n        if max_queue:\n            os.environ[\"OLLAMA_MAX_QUEUE\"] = str(max_queue)\n        st.success(\"Server settings applied. Please restart the server.\")\n    except Exception as e:\n        st.error(f\"Error applying server settings: {e}\")\n\ndef start_server():\n    \"\"\"Starts the Ollama server.\"\"\"\n    try:\n        subprocess.Popen([\"ollama\", \"serve\"])\n        st.success(\"Ollama server started.\")\n    except Exception as e:\n        st.error(f\"Error starting Ollama server: {e}\")\n\ndef apply_model_keep_alive(model_name, keep_alive):\n    \"\"\"Applies keep-alive settings for a specific model using the ollama CLI.\"\"\"\n    try:\n        if keep_alive:\n            command = [\"ollama\", \"run\", model_name, \"\", \"--keep-alive\", keep_alive]\n            result = subprocess.run(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n\n            if result.returncode == 0:\n                st.success(f\"Keep-Alive '{keep_alive}' applied for '{model_name}'!\")\n            else:\n                st.error(f\"Error applying Keep-Alive for '{model_name}': {result.stderr.decode()}\")\n    except Exception as e:\n        st.error(f\"Error applying Keep-Alive for '{model_name}': {e}\")\n\ndef get_log_file_path():\n    \"\"\"Returns the platform-specific path to the Ollama server log file.\"\"\"\n    system = platform.system()\n    if system == \"Darwin\":\n        return os.path.expanduser(\"~/.ollama/logs/server.log\")\n    elif system == \"Linux\":\n        return \"/var/log/ollama/server.log\"  # Assuming a standard location for Linux\n    elif system == \"Windows\":\n        return os.path.join(os.environ[\"LOCALAPPDATA\"], \"Ollama\", \"server.log\")\n    else:\n        return None\n\ndef get_new_logs(last_position):\n    \"\"\"Fetches new log entries from the Ollama server log file.\"\"\"\n    log_file = get_log_file_path()\n    if not log_file:\n        return \"\", 0\n\n    try:\n        current_size = os.path.getsize(log_file)\n        if current_size > last_position:\n            with open(log_file, \"r\") as f:\n                f.seek(last_position)\n                new_logs = f.read()\n                return new_logs, current_size\n        else:\n            return \"\", last_position\n    except FileNotFoundError:\n        return \"\", 0\n\ndef view_last_logs():\n    \"\"\"Displays the last 1000 lines of the Ollama server log file.\"\"\"\n    logs = get_server_logs()\n    logs = logs[-1000:]\n    log_text = \"\".join(logs)\n    st.text_area(\"Last 1000 Lines of Server Logs\", value=log_text, height=400, key=\"last_logs_view\")\n\ndef get_server_logs():\n    \"\"\"Fetches server logs from the local Ollama log file.\"\"\"\n    system = platform.system()\n    if system == \"Darwin\":\n        log_file = os.path.expanduser(\"~/.ollama/logs/server.log\")\n    elif system == \"Linux\":\n        # Assuming systemd service\n        st.info(\"Please check the systemd journal using 'journalctl -u ollama' for logs.\")\n        return []\n    elif system == \"Windows\":\n        log_file = os.path.join(os.environ[\"LOCALAPPDATA\"], \"Ollama\", \"server.log\")\n    else:\n        st.warning(\"Unsupported operating system. Unable to fetch server logs.\")\n        return []\n\n    try:\n        with open(log_file, \"r\") as f:\n            logs = f.readlines()\n        return logs\n    except FileNotFoundError:\n        st.warning(f\"Server log file not found: {log_file}\")\n        return []\n\ndef get_resource_usage():\n    \"\"\"Fetches resource usage data from the Ollama API (placeholder).\"\"\"\n    st.info(\"Real-time resource usage monitoring is not yet supported by the Ollama API.\")\n    return {}\n\ndef generate_embeddings(model, text):\n    \"\"\"Generates embeddings for the given text using the specified model.\"\"\"\n    try:\n        if model in GROQ_MODELS:\n            # Use Groq API for embedding\n            return get_local_embeddings(text)\n        elif model in OPENAI_MODELS:\n            # Use OpenAI API for embedding\n            return call_openai_api(model, prompt=[{\"role\": \"user\", \"content\": text}], use_chat=False)\n        else:\n            # Default to Ollama API for embedding\n            response = requests.post(f\"{OLLAMA_URL}/embed\", json={\"model\": model, \"text\": text})\n            response.raise_for_status()\n            embedding_data = response.json()\n            return embedding_data[\"embedding\"], embedding_data[\"total_duration\"], embedding_data[\"load_duration\"], embedding_data[\"prompt_eval_count\"]\n    except requests.exceptions.RequestException as e:\n        st.error(f\"Error generating embeddings: {e}\")\n        return None, None, None, None\n\ndef get_all_models():\n    \"\"\"Gets all available models, including Ollama, Groq, OpenAI, and Mistral.\"\"\"\n    ollama_models = [\"🦙 Ollama Models\"] + get_available_models()\n    groq_models = [\"🚀 Groq Models\"] + GROQ_MODELS\n    openai_models = [\"🤖 OpenAI Models\"] + OPENAI_MODELS\n    mistral_models = [\"🌟 Mistral Models\"] + MISTRAL_MODELS\n    return ollama_models + groq_models + openai_models + mistral_models\n"}
{"type": "source_file", "path": "patch_main.py", "content": "#!/usr/bin/env python3\n\"\"\"\nScript to patch main.py to handle the case where streamlit_option_menu is not available.\nThis creates a fallback implementation of option_menu that uses streamlit's built-in\nselectbox component instead.\n\"\"\"\n\nimport sys\nimport os\n\ndef create_backup(file_path):\n    \"\"\"Create a backup of the file.\"\"\"\n    backup_path = f\"{file_path}.bak\"\n    if not os.path.exists(backup_path):\n        with open(file_path, 'r') as src, open(backup_path, 'w') as dst:\n            dst.write(src.read())\n        print(f\"Created backup at {backup_path}\")\n    else:\n        print(f\"Backup already exists at {backup_path}\")\n\ndef patch_main_py(file_path):\n    \"\"\"Patch main.py to handle missing streamlit_option_menu.\"\"\"\n    with open(file_path, 'r') as f:\n        content = f.read()\n    \n    # Check if the file has already been patched\n    if \"# Fallback implementation for streamlit_option_menu\" in content:\n        print(\"main.py has already been patched for streamlit_option_menu\")\n    else:\n        # Find the import line\n        import_line = \"from streamlit_option_menu import option_menu\"\n        if import_line not in content:\n            print(f\"Could not find import line: {import_line}\")\n        else:\n            # Replace the import line with the fallback implementation\n            fallback_code = \"\"\"try:\n    from streamlit_option_menu import option_menu\nexcept ImportError:\n    print(\"Warning: streamlit_option_menu not found, using fallback implementation\")\n    # Fallback implementation for streamlit_option_menu\n    def option_menu(menu_title, options, icons=None, menu_icon=None, default_index=0, styles=None):\n        import streamlit as st\n        selected = st.selectbox(\n            menu_title if menu_title else \"Menu\",\n            options,\n            index=default_index,\n        )\n        return selected\"\"\"\n            \n            content = content.replace(import_line, fallback_code)\n            print(\"Patched main.py for streamlit_option_menu\")\n    \n    # Write the patched content back to the file\n    with open(file_path, 'w') as f:\n        f.write(content)\n\ndef patch_openai_utils(file_path):\n    \"\"\"Patch openai_utils.py to handle missing openai package.\"\"\"\n    if not os.path.exists(file_path):\n        print(f\"Error: {file_path} not found\")\n        return\n    \n    # Create a backup of the file\n    create_backup(file_path)\n    \n    with open(file_path, 'r') as f:\n        content = f.read()\n    \n    # Check if the file has already been patched\n    if \"# Fallback implementation for openai\" in content:\n        print(\"openai_utils.py has already been patched\")\n        return\n    \n    # Find the import line\n    import_line = \"from openai import OpenAI\"\n    if import_line not in content:\n        print(f\"Could not find import line: {import_line}\")\n        return\n    \n    # Replace the import line with the fallback implementation\n    fallback_code = \"\"\"try:\n    from openai import OpenAI\nexcept ImportError:\n    print(\"Warning: openai package not found, using fallback implementation\")\n    # Fallback implementation for openai\n    class OpenAI:\n        def __init__(self, api_key=None):\n            self.api_key = api_key\n            \n        class Completion:\n            @staticmethod\n            def create(*args, **kwargs):\n                return {\"choices\": [{\"text\": \"OpenAI API not available - please install the openai package\"}]}\n        \n        class ChatCompletion:\n            @staticmethod\n            def create(*args, **kwargs):\n                return {\"choices\": [{\"message\": {\"content\": \"OpenAI API not available - please install the openai package\"}}]}\n                \n        class Embedding:\n            @staticmethod\n            def create(*args, **kwargs):\n                return {\"data\": [{\"embedding\": [0.0] * 1536}]}\n                \n        completion = Completion()\n        chat = ChatCompletion()\n        embeddings = Embedding()\"\"\"\n    \n    patched_content = content.replace(import_line, fallback_code)\n    \n    # Write the patched content back to the file\n    with open(file_path, 'w') as f:\n        f.write(patched_content)\n    \n    print(f\"Successfully patched {file_path} for openai\")\n\ndef patch_groq_utils(file_path):\n    \"\"\"Patch groq_utils.py to handle missing sentence_transformers package.\"\"\"\n    if not os.path.exists(file_path):\n        print(f\"Error: {file_path} not found\")\n        return\n    \n    # Create a backup of the file\n    create_backup(file_path)\n    \n    with open(file_path, 'r') as f:\n        content = f.read()\n    \n    # Check if the file has already been patched\n    if \"# Fallback implementation for sentence_transformers\" in content:\n        print(\"groq_utils.py has already been patched\")\n        return\n    \n    # Find the import line\n    import_line = \"from sentence_transformers import SentenceTransformer\"\n    if import_line not in content:\n        print(f\"Could not find import line: {import_line}\")\n        return\n    \n    # Replace the import line with the fallback implementation\n    fallback_code = \"\"\"try:\n    from sentence_transformers import SentenceTransformer\nexcept ImportError:\n    print(\"Warning: sentence_transformers package not found, using fallback implementation\")\n    # Fallback implementation for sentence_transformers\n    class SentenceTransformer:\n        def __init__(self, model_name):\n            self.model_name = model_name\n            print(f\"Warning: Using fallback SentenceTransformer with model: {model_name}\")\n            \n        def encode(self, text, **kwargs):\n            import numpy as np\n            print(f\"Warning: Using fallback encoding for text: {text[:50]}...\")\n            # Return a random embedding vector of size 384 (same as all-MiniLM-L6-v2)\n            return np.random.rand(384)\"\"\"\n    \n    patched_content = content.replace(import_line, fallback_code)\n    \n    # Write the patched content back to the file\n    with open(file_path, 'w') as f:\n        f.write(patched_content)\n    \n    print(f\"Successfully patched {file_path} for sentence_transformers\")\n\ndef patch_tts_utils(file_path):\n    \"\"\"Patch tts_utils.py to handle missing gtts package.\"\"\"\n    if not os.path.exists(file_path):\n        print(f\"Error: {file_path} not found\")\n        return\n    \n    # Create a backup of the file\n    create_backup(file_path)\n    \n    with open(file_path, 'r') as f:\n        content = f.read()\n    \n    # Check if the file has already been patched\n    if \"# Fallback implementation for gtts\" in content:\n        print(\"tts_utils.py has already been patched\")\n        return\n    \n    # Find the import line\n    import_line = \"from gtts import gTTS\"\n    if import_line not in content:\n        print(f\"Could not find import line: {import_line}\")\n        return\n    \n    # Replace the import line with the fallback implementation\n    fallback_code = \"\"\"try:\n    from gtts import gTTS\nexcept ImportError:\n    print(\"Warning: gtts package not found, using fallback implementation\")\n    # Fallback implementation for gtts\n    class gTTS:\n        def __init__(self, text=\"\", lang=\"en\", **kwargs):\n            self.text = text\n            self.lang = lang\n            print(f\"Warning: Using fallback gTTS with text: {text[:50]}...\")\n            \n        def save(self, filename):\n            print(f\"Warning: Saving fallback speech to {filename}\")\n            # Create an empty file\n            with open(filename, 'w') as f:\n                f.write(\"# This is a placeholder file created by the fallback gTTS implementation\")\n            return filename\"\"\"\n    \n    patched_content = content.replace(import_line, fallback_code)\n    \n    # Also patch pygame import\n    pygame_import = \"import pygame\"\n    if pygame_import in content:\n        pygame_fallback = \"\"\"try:\n    import pygame\nexcept ImportError:\n    print(\"Warning: pygame package not found, using fallback implementation\")\n    # Fallback implementation for pygame\n    class pygame:\n        class mixer:\n            @staticmethod\n            def init():\n                print(\"Warning: Using fallback pygame.mixer.init()\")\n                \n            @staticmethod\n            def quit():\n                print(\"Warning: Using fallback pygame.mixer.quit()\")\n                \n            class music:\n                @staticmethod\n                def load(filename):\n                    print(f\"Warning: Using fallback pygame.mixer.music.load({filename})\")\n                    \n                @staticmethod\n                def play():\n                    print(\"Warning: Using fallback pygame.mixer.music.play()\")\n                    \n                @staticmethod\n                def get_busy():\n                    # Return False to exit the playback loop immediately\n                    return False\n                    \n        class time:\n            class Clock:\n                def tick(self, framerate):\n                    pass\"\"\"\n        \n        patched_content = patched_content.replace(pygame_import, pygame_fallback)\n    \n    # Write the patched content back to the file\n    with open(file_path, 'w') as f:\n        f.write(patched_content)\n    \n    print(f\"Successfully patched {file_path} for gtts and pygame\")\n\ndef patch_brainstorm(file_path):\n    \"\"\"Patch brainstorm.py to handle missing autogen package.\"\"\"\n    if not os.path.exists(file_path):\n        print(f\"Error: {file_path} not found\")\n        return\n    \n    # Create a backup of the file\n    create_backup(file_path)\n    \n    with open(file_path, 'r') as f:\n        content = f.read()\n    \n    # Check if the file has already been patched\n    if \"# Fallback implementation for autogen\" in content:\n        print(\"brainstorm.py has already been patched\")\n        return\n    \n    # Find the import line\n    import_line = \"from autogen import ConversableAgent, UserProxyAgent, GroupChat, GroupChatManager\"\n    if import_line not in content:\n        print(f\"Could not find import line: {import_line}\")\n        return\n    \n    # Replace the import line with the fallback implementation\n    fallback_code = \"\"\"try:\n    from autogen import ConversableAgent, UserProxyAgent, GroupChat, GroupChatManager\nexcept ImportError:\n    print(\"Warning: autogen package not found, using fallback implementation\")\n    # Fallback implementation for autogen\n    class ConversableAgent:\n        def __init__(self, name, llm_config=None, human_input_mode=None, **kwargs):\n            self.name = name\n            self.llm_config = llm_config\n            print(f\"Warning: Using fallback ConversableAgent with name: {name}\")\n            \n        def generate_reply(self, messages, sender, config=None):\n            print(f\"Warning: Using fallback generate_reply for {self.name}\")\n            return f\"This is a fallback response from {self.name}. The autogen package is not installed.\"\n    \n    class UserProxyAgent(ConversableAgent):\n        def __init__(self, name, human_input_mode=None, code_execution_config=None, **kwargs):\n            super().__init__(name=name, **kwargs)\n            self.human_input_mode = human_input_mode\n            self.code_execution_config = code_execution_config\n    \n    class GroupChat:\n        def __init__(self, agents, messages=None, speaker_selection_method=None):\n            self.agents = agents\n            self.messages = messages or []\n            self.speaker_selection_method = speaker_selection_method\n    \n    class GroupChatManager:\n        def __init__(self, groupchat):\n            self.groupchat = groupchat\"\"\"\n    \n    patched_content = content.replace(import_line, fallback_code)\n    \n    # Also patch the Teachability import\n    teachability_import = \"from autogen.agentchat.contrib.capabilities.teachability import Teachability\"\n    if teachability_import in patched_content:\n        teachability_fallback = \"\"\"try:\n    from autogen.agentchat.contrib.capabilities.teachability import Teachability\nexcept ImportError:\n    print(\"Warning: autogen.agentchat.contrib.capabilities.teachability package not found, using fallback implementation\")\n    # Fallback implementation for Teachability\n    class Teachability:\n        def __init__(self, path_to_db_dir=None):\n            self.path_to_db_dir = path_to_db_dir\n            print(f\"Warning: Using fallback Teachability with path: {path_to_db_dir}\")\n            \n        def add_to_agent(self, agent):\n            print(f\"Warning: Using fallback add_to_agent for {agent.name}\")\n            pass\"\"\"\n        \n        patched_content = patched_content.replace(teachability_import, teachability_fallback)\n    \n    # Write the patched content back to the file\n    with open(file_path, 'w') as f:\n        f.write(patched_content)\n    \n    print(f\"Successfully patched {file_path} for autogen\")\n\ndef patch_web_to_corpus(file_path):\n    \"\"\"Patch web_to_corpus.py to handle missing fake_useragent package.\"\"\"\n    if not os.path.exists(file_path):\n        print(f\"Error: {file_path} not found\")\n        return\n    \n    # Create a backup of the file\n    create_backup(file_path)\n    \n    with open(file_path, 'r') as f:\n        content = f.read()\n    \n    # Check if the file has already been patched\n    if \"# Fallback implementation for fake_useragent\" in content:\n        print(\"web_to_corpus.py has already been patched\")\n        return\n    \n    # Find the import line\n    import_line = \"from fake_useragent import UserAgent\"\n    if import_line not in content:\n        print(f\"Could not find import line: {import_line}\")\n        return\n    \n    # Replace the import line with the fallback implementation\n    fallback_code = \"\"\"try:\n    from fake_useragent import UserAgent\nexcept ImportError:\n    print(\"Warning: fake_useragent package not found, using fallback implementation\")\n    # Fallback implementation for fake_useragent\n    class UserAgent:\n        def __init__(self):\n            self.user_agents = [\n                \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n                \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.1 Safari/605.1.15\",\n                \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.114 Safari/537.36\",\n                \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0\"\n            ]\n            print(\"Warning: Using fallback UserAgent\")\n            \n        @property\n        def random(self):\n            import random\n            return random.choice(self.user_agents)\"\"\"\n    \n    patched_content = content.replace(import_line, fallback_code)\n    \n    # Write the patched content back to the file\n    with open(file_path, 'w') as f:\n        f.write(patched_content)\n    \n    print(f\"Successfully patched {file_path} for fake_useragent\")\n\ndef patch_pull_model(file_path):\n    \"\"\"Patch pull_model.py to handle missing humanize package.\"\"\"\n    if not os.path.exists(file_path):\n        print(f\"Error: {file_path} not found\")\n        return\n\n    # Create a backup of the file\n    create_backup(file_path)\n\n    with open(file_path, 'r') as f:\n        content = f.read()\n\n    # Check if the file has already been patched\n    if \"# Fallback implementation for humanize\" in content:\n        print(\"pull_model.py has already been patched\")\n        return\n\n    # Find the import line\n    import_line = \"import humanize\"\n    if import_line not in content:\n        print(f\"Could not find import line: {import_line}\")\n        return\n\n    # Replace the import line with the fallback implementation\n    fallback_code = \"\"\"try:\n    import humanize\nexcept ImportError:\n    print(\"Warning: humanize package not found, using fallback implementation\")\n    # Fallback implementation for humanize\n    def format_size(size_bytes: int) -> str:\n        \\\"\\\"\\\"Format the file size in a human-readable format (fallback).\\\"\\\"\\\"\n        if size_bytes < 1024:\n            return f\"{size_bytes} B\"\n        elif size_bytes < 1024 * 1024:\n            return f\"{size_bytes / 1024:.2f} KB\"\n        elif size_bytes < 1024 * 1024 * 1024:\n            return f\"{size_bytes / (1024 * 1024):.2f} MB\"\n        else:\n            return f\"{size_bytes / (1024 * 1024 * 1024):.2f} GB\"\n    humanize = type('humanize', (object,), {'naturalsize': format_size})()\"\"\"\n\n    patched_content = content.replace(import_line, fallback_code)\n\n    # Write the patched content back to the file\n    with open(file_path, 'w') as f:\n        f.write(patched_content)\n\n    print(f\"Successfully patched {file_path} for humanize\")\n    \ndef patch_file(file_path):\n    \"\"\"Patch the file based on its name.\"\"\"\n    if os.path.basename(file_path) == \"main.py\":\n        patch_main_py(file_path)\n    elif os.path.basename(file_path) == \"openai_utils.py\":\n        patch_openai_utils(\"openai_utils.py\")\n    elif os.path.basename(file_path) == \"groq_utils.py\":\n        patch_groq_utils(\"groq_utils.py\")\n    elif os.path.basename(file_path) == \"tts_utils.py\":\n        patch_tts_utils(\"tts_utils.py\")\n    elif os.path.basename(file_path) == \"brainstorm.py\":\n        patch_brainstorm(\"brainstorm.py\")\n    elif os.path.basename(file_path) == \"web_to_corpus.py\":\n        patch_web_to_corpus(\"web_to_corpus.py\")\n    else:\n        print(f\"No specific patch available for {file_path}\")\n\ndef main():\n    \"\"\"Main function.\"\"\"\n    main_py_path = \"main.py\"\n    openai_utils_path = \"openai_utils.py\"\n    groq_utils_path = \"groq_utils.py\"\n    tts_utils_path = \"tts_utils.py\"\n    brainstorm_path = \"brainstorm.py\"\n    web_to_corpus_path = \"web_to_corpus.py\"\n    \n    if not os.path.exists(main_py_path):\n        print(f\"Error: {main_py_path} not found\")\n        return False\n    \n    # Create a backup of main.py\n    create_backup(main_py_path)\n    \n    # Patch main.py\n    patch_file(main_py_path)\n    \n    # Check if openai_utils.py exists\n    if os.path.exists(openai_utils_path):\n        # Create a backup of openai_utils.py\n        create_backup(openai_utils_path)\n        \n        # Patch openai_utils.py\n        patch_file(openai_utils_path)\n    else:\n        print(f\"Warning: {openai_utils_path} not found, skipping\")\n    \n    # Check if groq_utils.py exists\n    if os.path.exists(groq_utils_path):\n        # Create a backup of groq_utils.py\n        create_backup(groq_utils_path)\n        \n        # Patch groq_utils.py\n        patch_file(groq_utils_path)\n    else:\n        print(f\"Warning: {groq_utils_path} not found, skipping\")\n    \n    # Check if tts_utils.py exists\n    if os.path.exists(tts_utils_path):\n        # Create a backup of tts_utils.py\n        create_backup(tts_utils_path)\n        \n        # Patch tts_utils.py\n        patch_file(tts_utils_path)\n    else:\n        print(f\"Warning: {tts_utils_path} not found, skipping\")\n    \n    # Check if brainstorm.py exists\n    if os.path.exists(brainstorm_path):\n        # Create a backup of brainstorm.py\n        create_backup(brainstorm_path)\n        \n        # Patch brainstorm.py\n        patch_file(brainstorm_path)\n    else:\n        print(f\"Warning: {brainstorm_path} not found, skipping\")\n    \n    # Check if web_to_corpus.py exists\n    if os.path.exists(web_to_corpus_path):\n        # Create a backup of web_to_corpus.py\n        create_backup(web_to_corpus_path)\n        \n        # Patch web_to_corpus.py\n        patch_file(web_to_corpus_path)\n    else:\n        print(f\"Warning: {web_to_corpus_path} not found, skipping\")\n    \n    # Check if web_to_corpus.py exists\n    if os.path.exists(web_to_corpus_path):\n        # Create a backup of web_to_corpus.py\n        create_backup(web_to_corpus_path)\n        \n        # Patch web_to_corpus.py\n        patch_file(web_to_corpus_path)\n    else:\n        print(f\"Warning: {web_to_corpus_path} not found, skipping\")\n    \n    # Check if web_to_corpus.py exists\n    if os.path.exists(web_to_corpus_path):\n        # Create a backup of web_to_corpus.py\n        create_backup(web_to_corpus_path)\n        \n        # Patch web_to_corpus.py\n        patch_file(web_to_corpus_path)\n    else:\n        print(f\"Warning: {web_to_corpus_path} not found, skipping\")\n    \n    pull_model_path = \"pull_model.py\"\n    # Check if pull_model.py exists\n    if os.path.exists(pull_model_path):\n        # Create a backup of pull_model.py\n        create_backup(pull_model_path)\n\n        # Patch pull_model.py\n        patch_file(pull_model_path) # Or patch_pull_model(pull_model_path) - let's use patch_file for consistency\n    else:\n        print(f\"Warning: {pull_model_path} not found, skipping\")\n\n    print(\"\\nPatch complete. You can now run the application with:\")\n    print(\"streamlit run main.py\")\n\n    return True\n\nif __name__ == \"__main__\":\n    success = main()\n    sys.exit(0 if success else 1)"}
{"type": "source_file", "path": "check_streamlit_option_menu.py", "content": "#!/usr/bin/env python3\n\"\"\"\nDiagnostic script to check streamlit_option_menu installation.\n\"\"\"\n\nimport sys\nimport subprocess\nimport importlib.util\n\ndef check_package_installed(package_name):\n    \"\"\"Check if a package is installed in the current Python environment.\"\"\"\n    spec = importlib.util.find_spec(package_name)\n    return spec is not None\n\ndef get_package_version(package_name):\n    \"\"\"Get the installed version of a package.\"\"\"\n    try:\n        result = subprocess.run(\n            [sys.executable, \"-m\", \"pip\", \"show\", package_name],\n            capture_output=True,\n            text=True,\n            check=False\n        )\n        if result.returncode == 0:\n            for line in result.stdout.splitlines():\n                if line.startswith(\"Version:\"):\n                    return line.split(\":\", 1)[1].strip()\n        return None\n    except Exception as e:\n        print(f\"Error checking package version: {e}\")\n        return None\n\ndef main():\n    \"\"\"Run the diagnostic checks.\"\"\"\n    print(\"Checking streamlit_option_menu installation...\")\n    \n    # Check if the package is installed\n    is_installed = check_package_installed(\"streamlit_option_menu\")\n    print(f\"streamlit_option_menu is installed: {is_installed}\")\n    \n    # Check package version\n    version = get_package_version(\"streamlit_option_menu\")\n    if version:\n        print(f\"Installed version: {version}\")\n    else:\n        print(\"Could not determine installed version\")\n    \n    # Check pip list for both hyphenated and underscore versions\n    print(\"\\nChecking pip list for package names:\")\n    for package_name in [\"streamlit-option-menu\", \"streamlit_option_menu\"]:\n        result = subprocess.run(\n            [sys.executable, \"-m\", \"pip\", \"list\", \"--format=json\"],\n            capture_output=True,\n            text=True,\n            check=False\n        )\n        if result.returncode == 0:\n            import json\n            packages = json.loads(result.stdout)\n            found = False\n            for pkg in packages:\n                if pkg[\"name\"].lower() == package_name.lower():\n                    print(f\"Found in pip list: {pkg['name']} (version: {pkg['version']})\")\n                    found = True\n                    break\n            if not found:\n                print(f\"Not found in pip list: {package_name}\")\n        else:\n            print(f\"Error checking pip list: {result.stderr}\")\n    \n    # Suggest solutions\n    print(\"\\nPossible solutions:\")\n    print(\"1. Install the package manually:\")\n    print(\"   pip install streamlit-option-menu==0.3.13\")\n    print(\"2. Add the package to pyproject.toml:\")\n    print('   streamlit-option-menu = \"^0.3.13\"')\n    print(\"3. Fix the setup.sh script to ensure requirements.txt is properly processed\")\n\nif __name__ == \"__main__\":\n    main()"}
{"type": "source_file", "path": "persona_lab/__init__.py", "content": "\n"}
{"type": "source_file", "path": "persona_chat.py", "content": "import streamlit as st\nimport random\nfrom datetime import datetime\nimport json\nimport os\nfrom dataclasses import dataclass\nfrom typing import List, Dict, Optional\nimport requests\nfrom PIL import Image\nimport io\nimport base64\nimport hashlib\nfrom ollama_utils import call_ollama_endpoint\n\n@dataclass\nclass Persona:\n    name: str\n    age: int\n    nationality: str\n    occupation: str\n    background: str\n    routine: str\n    personality: str\n    skills: List[str]\n    avatar: str\n    model: str\n    temperature: float = 0.7\n    max_tokens: int = 150\n\n# Predefined occupations for quick persona generation\nSTARTER_OCCUPATIONS = [\n    \"Data Scientist\",\n    \"Software Engineer\",\n    \"Doctor\",\n    \"Artist\",\n    \"Teacher\",\n    \"Entrepreneur\",\n    \"Writer\",\n    \"Chef\"\n]\n\ndef generate_avatar(persona_name: str) -> str:\n    \"\"\"Generate a unique avatar based on the persona name.\"\"\"\n    # Create a hash of the name to ensure consistent but unique avatars\n    name_hash = hashlib.md5(persona_name.encode()).hexdigest()\n    \n    # Use the hash to generate avatar parameters\n    colors = ['FF5733', '33FF57', '3357FF', 'FF33F6', 'F6FF33']\n    color = colors[int(name_hash[:2], 16) % len(colors)]\n    \n    # Generate a unique avatar URL using DiceBear API\n    avatar_url = f\"https://api.dicebear.com/7.x/personas/svg?seed={name_hash}&backgroundColor={color}\"\n    return avatar_url\n\ndef create_persona(occupation: str = None) -> Persona:\n    \"\"\"Create a new persona with optional occupation specification.\"\"\"\n    if occupation is None:\n        occupation = random.choice(STARTER_OCCUPATIONS)\n    \n    # Use Ollama to generate persona details\n    prompt = f\"\"\"You are a creative AI assistant specializing in creating detailed, realistic personas. Generate a complete persona for a {occupation}.\n\nYour response must be a valid JSON object with exactly this format:\n{{\n    \"name\": \"<full name - be diverse and realistic>\",\n    \"age\": <number between 25-65>,\n    \"nationality\": \"<country - be diverse in your choices>\",\n    \"background\": \"<detailed 2-3 sentence professional background including education and career progression>\",\n    \"routine\": \"<detailed daily routine from morning to evening, including work and personal life>\",\n    \"personality\": \"<detailed description of personality traits, communication style, and work approach>\",\n    \"skills\": [\n        \"<specific technical skill relevant to their occupation>\",\n        \"<specific soft skill that defines their work style>\",\n        \"<unique or interesting skill that makes them stand out>\"\n    ]\n}}\n\nMake the persona feel like a real person with:\n- A coherent and believable background story\n- A realistic daily routine that matches their profession\n- Personality traits that feel authentic and three-dimensional\n- Skills that are specific and relevant to their role\n\nRespond ONLY with the JSON object, no other text.\"\"\"\n    \n    try:\n        # Use the proper call_ollama_endpoint function\n        response, _, _, _ = call_ollama_endpoint(\n            model=\"mistral:instruct\",\n            prompt=prompt,\n            temperature=0.7,\n            max_tokens=1000,\n            presence_penalty=0.3,\n            frequency_penalty=0.3\n        )\n        \n        # Clean up the response to ensure valid JSON\n        json_str = response.strip()\n        # Remove any markdown code block indicators\n        json_str = json_str.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n        \n        # Parse JSON\n        persona_data = json.loads(json_str)\n        \n        # Create and return persona\n        return Persona(\n            name=persona_data[\"name\"],\n            age=persona_data[\"age\"],\n            nationality=persona_data[\"nationality\"],\n            occupation=occupation,\n            background=persona_data[\"background\"],\n            routine=persona_data[\"routine\"],\n            personality=persona_data[\"personality\"],\n            skills=persona_data[\"skills\"],\n            avatar=generate_avatar(persona_data[\"name\"]),\n            model=\"mistral:instruct\"\n        )\n    except json.JSONDecodeError as e:\n        st.error(f\"Error parsing persona data: {str(e)}\\nResponse: {json_str}\")\n        return None\n    except KeyError as e:\n        st.error(f\"Missing required field in persona data: {str(e)}\\nResponse: {json_str}\")\n        return None\n    except Exception as e:\n        st.error(f\"Unexpected error: {str(e)}\")\n        return None\n\ndef save_personas(personas: List[Persona]):\n    \"\"\"Save personas to a JSON file.\"\"\"\n    personas_data = [vars(p) for p in personas]\n    os.makedirs(\"personas\", exist_ok=True)\n    with open(\"personas/saved_personas.json\", \"w\") as f:\n        json.dump(personas_data, f, indent=2)\n\ndef load_personas() -> List[Persona]:\n    \"\"\"Load personas from a JSON file.\"\"\"\n    try:\n        with open(\"personas/saved_personas.json\", \"r\") as f:\n            personas_data = json.load(f)\n            return [Persona(**p) for p in personas_data]\n    except FileNotFoundError:\n        return []\n\ndef persona_group_chat():\n    st.title(\"Persona Group Chat\")\n    \n    # Add link to Persona Lab\n    col1, col2 = st.columns([3, 1])\n    with col1:\n        st.markdown(\"Create conversations between AI personas with different backgrounds and personalities.\")\n    with col2:\n        if st.button(\"📝 Manage Personas\", help=\"Open Persona Lab to manage all personas\"):\n            st.session_state.selected_test = \"Persona Lab\"\n            st.rerun()\n    \n    # Initialize session state\n    if \"personas\" not in st.session_state:\n        st.session_state.personas = load_personas()\n    if \"chat_history\" not in st.session_state:\n        st.session_state.chat_history = []\n    \n    # Sidebar for persona management\n    with st.sidebar:\n        st.subheader(\"Manage Personas\")\n        \n        # Quick add buttons for starter occupations\n        st.write(\"Quick Add by Occupation:\")\n        cols = st.columns(2)\n        for i, occupation in enumerate(STARTER_OCCUPATIONS):\n            if cols[i % 2].button(occupation):\n                new_persona = create_persona(occupation)\n                if new_persona:\n                    st.session_state.personas.append(new_persona)\n                    save_personas(st.session_state.personas)\n        \n        # Manual persona creation\n        st.write(\"---\")\n        st.write(\"Custom Persona:\")\n        with st.form(\"create_persona\"):\n            name = st.text_input(\"Name\")\n            age = st.number_input(\"Age\", 25, 65, 30)\n            nationality = st.text_input(\"Nationality\")\n            occupation = st.text_input(\"Occupation\")\n            model = st.selectbox(\"Model\", [\"mistral:instruct\", \"llama2\", \"codellama\", \"neural-chat\"])\n            \n            if st.form_submit_button(\"Create Custom Persona\"):\n                if name and nationality and occupation:\n                    new_persona = Persona(\n                        name=name,\n                        age=age,\n                        nationality=nationality,\n                        occupation=occupation,\n                        background=\"\",\n                        routine=\"\",\n                        personality=\"\",\n                        skills=[],\n                        avatar=generate_avatar(name),\n                        model=model\n                    )\n                    st.session_state.personas.append(new_persona)\n                    save_personas(st.session_state.personas)\n    \n    # Main chat area\n    if not st.session_state.personas:\n        st.info(\"Add some personas using the sidebar to start the group chat!\")\n        return\n    \n    # Display current personas\n    st.subheader(\"Current Personas\")\n    cols = st.columns(len(st.session_state.personas))\n    for i, persona in enumerate(st.session_state.personas):\n        with cols[i]:\n            st.image(persona.avatar, width=100)\n            st.write(f\"**{persona.name}**\")\n            st.write(f\"*{persona.occupation}*\")\n            if st.button(f\"Remove {persona.name}\", key=f\"remove_{i}\"):\n                st.session_state.personas.pop(i)\n                save_personas(st.session_state.personas)\n                st.rerun()\n    \n    # Chat interface\n    st.write(\"---\")\n    st.subheader(\"Group Chat\")\n    \n    # Display chat history\n    for message in st.session_state.chat_history:\n        with st.chat_message(message[\"role\"], avatar=message.get(\"avatar\")):\n            st.write(f\"**{message.get('name', 'User')}:** {message['content']}\")\n    \n    # User input\n    if prompt := st.chat_input(\"Enter your message\"):\n        # Add user message to chat\n        st.session_state.chat_history.append({\n            \"role\": \"user\",\n            \"content\": prompt,\n            \"name\": \"User\"\n        })\n        \n        # Get responses from all personas\n        for persona in st.session_state.personas:\n            context = f\"\"\"You are {persona.name}, a {persona.age}-year-old {persona.nationality} {persona.occupation}.\n            Background: {persona.background}\n            Personality: {persona.personality}\n            \n            Previous conversation:\n            {chr(10).join([f\"{m['name']}: {m['content']}\" for m in st.session_state.chat_history[-5:]])}\n            \n            Respond as {persona.name}, keeping in mind your background and personality.\n            Keep the response concise (max 2-3 sentences).\n            \"\"\"\n            \n            try:\n                response, _, _, _ = call_ollama_endpoint(\n                    model=persona.model,\n                    prompt=context,\n                    temperature=persona.temperature,\n                    max_tokens=persona.max_tokens\n                )\n                \n                # Add persona's response to chat\n                st.session_state.chat_history.append({\n                    \"role\": \"assistant\",\n                    \"content\": response,\n                    \"name\": persona.name,\n                    \"avatar\": persona.avatar\n                })\n            except Exception as e:\n                st.error(f\"Error getting response from {persona.name}: {str(e)}\")\n        \n        st.rerun()\n\nif __name__ == \"__main__\":\n    persona_group_chat()\n"}
{"type": "source_file", "path": "persona_lab/persona_lab.py", "content": "import streamlit as st\nimport uuid\nimport json\nfrom datetime import datetime\nfrom typing import Optional\nfrom persona_lab.persona_model import Persona, PersonaDB\nfrom ollama_utils import call_ollama_endpoint\nfrom persona_chat import STARTER_OCCUPATIONS\n\nclass PersonaLab:\n    def __init__(self):\n        self.db = PersonaDB()\n        if \"editing_persona\" not in st.session_state:\n            st.session_state.editing_persona = None\n        if \"show_history\" not in st.session_state:\n            st.session_state.show_history = False\n        if \"current_tab\" not in st.session_state:\n            st.session_state.current_tab = \"Browse\"\n\n    def generate_persona(self, occupation: str) -> Optional[Persona]:\n        \"\"\"Generate a new persona with the given occupation.\"\"\"\n        prompt = f\"\"\"You are a creative AI assistant specializing in creating detailed, realistic personas. Generate a complete persona for a {occupation}.\n\nYour response must be a valid JSON object with exactly this format:\n{{\n    \"name\": \"<full name - be diverse and realistic>\",\n    \"age\": <number between 25-65>,\n    \"nationality\": \"<country - be diverse in your choices>\",\n    \"background\": \"<detailed 3-4 sentence professional background including education, career progression, and key achievements>\",\n    \"routine\": \"<detailed daily routine from morning to evening, including work habits, breaks, and personal life>\",\n    \"personality\": \"<detailed description of personality traits, communication style, work approach, and unique characteristics>\",\n    \"skills\": [\n        \"<specific technical skill relevant to their occupation with proficiency level>\",\n        \"<specific soft skill that defines their work style with examples>\",\n        \"<unique or interesting skill that makes them stand out with context>\"\n    ]\n}}\n\nMake the persona feel like a real person with:\n- A coherent and believable background story that shows career progression\n- A detailed daily routine that reflects their profession and personality\n- Rich personality traits that affect their work and communication style\n- Specific and measurable skills that match their experience level\n\nRespond ONLY with the JSON object, no other text.\"\"\"\n\n        try:\n            response, _, _, _ = call_ollama_endpoint(\n                model=\"mistral:instruct\",\n                prompt=prompt,\n                temperature=0.7,\n                max_tokens=1500,\n                presence_penalty=0.3,\n                frequency_penalty=0.3\n            )\n\n            # Clean and parse response\n            json_str = response.strip().replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n            persona_data = json.loads(json_str)\n\n            # Create new persona\n            persona = Persona(\n                id=str(uuid.uuid4()),\n                name=persona_data[\"name\"],\n                age=persona_data[\"age\"],\n                nationality=persona_data[\"nationality\"],\n                occupation=occupation,\n                background=persona_data[\"background\"],\n                routine=persona_data[\"routine\"],\n                personality=persona_data[\"personality\"],\n                skills=persona_data[\"skills\"],\n                avatar=self.generate_avatar(persona_data[\"name\"]),\n                model=\"mistral:instruct\",\n                created_at=datetime.now(),\n                modified_at=datetime.now(),\n                version=1,\n                generated_by=\"AI\"\n            )\n\n            # Save to database\n            if self.db.create_persona(persona):\n                return persona\n            return None\n        except Exception as e:\n            st.error(f\"Error generating persona: {str(e)}\")\n            return None\n\n    def edit_persona(self, persona_id: str):\n        \"\"\"Show the persona edit form.\"\"\"\n        persona = self.db.get_persona(persona_id)\n        if not persona:\n            st.error(\"Persona not found\")\n            return\n\n        with st.form(f\"edit_persona_{persona_id}\"):\n            col1, col2 = st.columns(2)\n            with col1:\n                name = st.text_input(\"Name\", persona.name)\n                age = st.number_input(\"Age\", 25, 65, persona.age)\n                nationality = st.text_input(\"Nationality\", persona.nationality)\n                occupation = st.text_input(\"Occupation\", persona.occupation)\n                model = st.selectbox(\"Model\", [\"mistral:instruct\", \"llama2\", \"codellama\", \"neural-chat\"], \n                                   index=[\"mistral:instruct\", \"llama2\", \"codellama\", \"neural-chat\"].index(persona.model))\n                temperature = st.slider(\"Temperature\", 0.0, 1.0, persona.temperature)\n                max_tokens = st.number_input(\"Max Tokens\", 100, 2000, persona.max_tokens)\n\n            with col2:\n                background = st.text_area(\"Background\", persona.background)\n                routine = st.text_area(\"Daily Routine\", persona.routine)\n                personality = st.text_area(\"Personality\", persona.personality)\n                skills = st.text_area(\"Skills (one per line)\", \"\\n\".join(persona.skills))\n                notes = st.text_area(\"Notes\", persona.notes)\n                tags = st.multiselect(\"Tags\", self.db.get_all_tags(), persona.tags)\n\n            if st.form_submit_button(\"Save Changes\"):\n                updated_persona = Persona(\n                    id=persona.id,\n                    name=name,\n                    age=age,\n                    nationality=nationality,\n                    occupation=occupation,\n                    background=background,\n                    routine=routine,\n                    personality=personality,\n                    skills=skills.split(\"\\n\"),\n                    avatar=persona.avatar,\n                    model=model,\n                    temperature=temperature,\n                    max_tokens=max_tokens,\n                    created_at=persona.created_at,\n                    modified_at=datetime.now(),\n                    version=persona.version,\n                    tags=tags,\n                    notes=notes,\n                    generated_by=persona.generated_by,\n                    interaction_history=persona.interaction_history,\n                    metadata=persona.metadata\n                )\n                if self.db.update_persona(updated_persona, modified_by=\"user\"):\n                    st.success(\"Changes saved successfully!\")\n                    st.rerun()\n\n    def show_persona_details(self, persona: Persona):\n        \"\"\"Show detailed view of a persona.\"\"\"\n        col1, col2 = st.columns([1, 3])\n        \n        with col1:\n            st.image(persona.avatar, width=150)\n            st.write(\"---\")\n            # Action buttons in a more compact layout\n            btn_col1, btn_col2 = st.columns(2)\n            with btn_col1:\n                if st.button(\"✏️ Edit\", key=f\"edit_{persona.id}\"):\n                    st.session_state.editing_persona = persona.id\n                if st.button(\"🗑️ Delete\", key=f\"delete_{persona.id}\", type=\"secondary\"):\n                    st.session_state.deleting_persona = persona.id\n            with btn_col2:\n                if st.button(\"📋 History\", key=f\"history_{persona.id}\"):\n                    st.session_state.show_history = persona.id\n                if st.button(\"🔄 Regen\", key=f\"regen_{persona.id}\"):\n                    st.session_state.regenerating_persona = persona.id\n\n        with col2:\n            # Header with metadata\n            st.subheader(f\"{persona.name} ({persona.age})\")\n            st.caption(f\"{persona.nationality} · {persona.occupation}\")\n            \n            # Model settings in a compact format\n            with st.expander(\"🛠️ Model Configuration\", expanded=False):\n                st.write(f\"**Model:** {persona.model}\")\n                st.write(f\"**Temperature:** {persona.temperature}\")\n                st.write(f\"**Max Tokens:** {persona.max_tokens}\")\n            \n            # Main content in tabs\n            tabs = st.tabs([\"📝 Details\", \"📅 Routine\", \"🎭 Personality\", \"🎯 Skills\"])\n            \n            with tabs[0]:\n                st.markdown(\"### Background\")\n                st.write(persona.background)\n                if persona.notes:\n                    st.markdown(\"### Notes\")\n                    st.write(persona.notes)\n                if persona.tags:\n                    st.markdown(\"### Tags\")\n                    for tag in persona.tags:\n                        st.caption(f\"#{tag}\")\n            \n            with tabs[1]:\n                st.write(persona.routine)\n            \n            with tabs[2]:\n                st.write(persona.personality)\n            \n            with tabs[3]:\n                for skill in persona.skills:\n                    st.markdown(f\"- {skill}\")\n\n        # Show edit form if this persona is being edited\n        if getattr(st.session_state, 'editing_persona', None) == persona.id:\n            st.markdown(\"---\")\n            st.subheader(\"Edit Persona\")\n            self.show_edit_form(persona)\n\n        # Show delete confirmation if this persona is being deleted\n        if getattr(st.session_state, 'deleting_persona', None) == persona.id:\n            st.markdown(\"---\")\n            st.warning(\"Are you sure you want to delete this persona?\")\n            col1, col2 = st.columns(2)\n            with col1:\n                if st.button(\"Yes, Delete\", type=\"primary\"):\n                    if self.db.delete_persona(persona.id):\n                        st.success(\"Persona deleted successfully!\")\n                        st.session_state.deleting_persona = None\n                        st.rerun()\n            with col2:\n                if st.button(\"Cancel\"):\n                    st.session_state.deleting_persona = None\n                    st.rerun()\n\n        # Show history if requested\n        if getattr(st.session_state, 'show_history', None) == persona.id:\n            st.markdown(\"---\")\n            st.subheader(\"Change History\")\n            history = self.db.get_persona_history(persona.id)\n            if not history:\n                st.info(\"No changes recorded yet\")\n            else:\n                for entry in history:\n                    with st.expander(f\"{entry.field_name} - {entry.timestamp.strftime('%Y-%m-%d %H:%M')}\"):\n                        col1, col2 = st.columns(2)\n                        with col1:\n                            st.markdown(\"**Previous Value:**\")\n                            st.text(entry.old_value)\n                        with col2:\n                            st.markdown(\"**New Value:**\")\n                            st.text(entry.new_value)\n                        st.caption(f\"Modified by: {entry.modified_by}\")\n\n    def show_edit_form(self, persona: Persona):\n        \"\"\"Show the persona edit form.\"\"\"\n        with st.form(f\"edit_persona_{persona.id}\"):\n            tabs = st.tabs([\"Basic Info\", \"Details\", \"Model Settings\", \"Tags & Notes\"])\n            \n            with tabs[0]:\n                col1, col2 = st.columns(2)\n                with col1:\n                    name = st.text_input(\"Name\", persona.name)\n                    age = st.number_input(\"Age\", 25, 65, persona.age)\n                    nationality = st.text_input(\"Nationality\", persona.nationality)\n                with col2:\n                    occupation = st.text_input(\"Occupation\", persona.occupation)\n                    avatar = st.text_input(\"Avatar URL (optional)\", persona.avatar)\n            \n            with tabs[1]:\n                background = st.text_area(\"Background\", persona.background, height=150)\n                routine = st.text_area(\"Daily Routine\", persona.routine, height=150)\n                personality = st.text_area(\"Personality\", persona.personality, height=150)\n                skills = st.text_area(\"Skills (one per line)\", \"\\n\".join(persona.skills), height=100)\n            \n            with tabs[2]:\n                col1, col2 = st.columns(2)\n                with col1:\n                    model = st.selectbox(\"Model\", \n                                       [\"mistral:instruct\", \"llama2\", \"codellama\", \"neural-chat\"],\n                                       index=[\"mistral:instruct\", \"llama2\", \"codellama\", \"neural-chat\"].index(persona.model))\n                    temperature = st.slider(\"Temperature\", 0.0, 1.0, persona.temperature)\n                with col2:\n                    max_tokens = st.number_input(\"Max Tokens\", 100, 2000, persona.max_tokens)\n            \n            with tabs[3]:\n                available_tags = self.db.get_all_tags()\n                tags = st.multiselect(\"Tags\", available_tags, persona.tags)\n                new_tag = st.text_input(\"Add New Tag\")\n                notes = st.text_area(\"Notes\", persona.notes)\n\n            col1, col2 = st.columns(2)\n            with col1:\n                if st.form_submit_button(\"Save Changes\", type=\"primary\"):\n                    # Add new tag if provided\n                    if new_tag:\n                        self.db.add_tag(new_tag)\n                        tags.append(new_tag)\n                    \n                    # Update persona\n                    updated_persona = Persona(\n                        id=persona.id,\n                        name=name,\n                        age=age,\n                        nationality=nationality,\n                        occupation=occupation,\n                        background=background,\n                        routine=routine,\n                        personality=personality,\n                        skills=skills.split(\"\\n\") if skills else [],\n                        avatar=avatar or persona.avatar,\n                        model=model,\n                        temperature=temperature,\n                        max_tokens=max_tokens,\n                        created_at=persona.created_at,\n                        modified_at=datetime.now(),\n                        version=persona.version,\n                        tags=tags,\n                        notes=notes,\n                        generated_by=persona.generated_by,\n                        interaction_history=persona.interaction_history,\n                        metadata=persona.metadata\n                    )\n                    \n                    if self.db.update_persona(updated_persona, modified_by=\"user\"):\n                        st.success(\"Changes saved successfully!\")\n                        st.session_state.editing_persona = None\n                        st.rerun()\n            with col2:\n                if st.form_submit_button(\"Cancel\", type=\"secondary\"):\n                    st.session_state.editing_persona = None\n                    st.rerun()\n\n    def delete_persona(self, persona_id: str):\n        \"\"\"Delete a persona with confirmation.\"\"\"\n        if st.button(\"Confirm Delete\"):\n            if self.db.delete_persona(persona_id):\n                st.success(\"Persona deleted successfully!\")\n                st.rerun()\n            else:\n                st.error(\"Error deleting persona\")\n\n    def show_history(self, persona_id: str):\n        \"\"\"Show the history of changes for a persona.\"\"\"\n        history = self.db.get_persona_history(persona_id)\n        if not history:\n            st.info(\"No history available for this persona\")\n            return\n\n        for entry in history:\n            with st.expander(f\"Changed {entry.field_name} on {entry.timestamp}\"):\n                st.write(\"**Old Value:**\", entry.old_value)\n                st.write(\"**New Value:**\", entry.new_value)\n                st.write(\"**Modified By:**\", entry.modified_by)\n\n    def show_interface(self):\n        \"\"\"Main interface for the Persona Lab.\"\"\"\n        st.title(\"🧪 Persona Lab\")\n        \n        # Tabs for different sections\n        tabs = st.tabs([\"Browse\", \"Create\", \"Search\", \"Analytics\"])\n        \n        with tabs[0]:  # Browse\n            personas = self.db.get_all_personas()\n            if not personas:\n                st.info(\"No personas created yet. Create one in the Create tab!\")\n            else:\n                for persona in personas:\n                    with st.expander(f\"{persona.name} - {persona.occupation}\"):\n                        self.show_persona_details(persona)\n\n        with tabs[1]:  # Create\n            st.subheader(\"Create New Persona\")\n            \n            col1, col2 = st.columns(2)\n            with col1:\n                st.write(\"Quick Add\")\n                for occupation in STARTER_OCCUPATIONS:\n                    if st.button(occupation):\n                        with st.spinner(f\"Generating {occupation.lower()} persona...\"):\n                            if self.generate_persona(occupation):\n                                st.success(f\"Created new {occupation.lower()} persona!\")\n                                st.rerun()\n            \n            with col2:\n                st.write(\"Custom Persona\")\n                with st.form(\"create_custom_persona\"):\n                    # Form fields for manual persona creation\n                    # (Similar to edit_persona form)\n                    pass\n\n        with tabs[2]:  # Search\n            st.subheader(\"Search Personas\")\n            query = st.text_input(\"Search by name, occupation, or any other field\")\n            if query:\n                results = self.db.search_personas(query)\n                if results:\n                    for persona in results:\n                        with st.expander(f\"{persona.name} - {persona.occupation}\"):\n                            self.show_persona_details(persona)\n                else:\n                    st.info(\"No matching personas found\")\n\n        with tabs[3]:  # Analytics\n            st.subheader(\"Persona Analytics\")\n            # Add visualization of persona data, trends, etc.\n            pass\n\ndef persona_lab_interface():\n    lab = PersonaLab()\n    lab.show_interface()\n"}
{"type": "source_file", "path": "remove_model.py", "content": "# remove_model.py\nimport streamlit as st\nfrom ollama_utils import *\n\ndef remove_model_ui():\n    st.header(\"🗑️ Remove an Ollama Model\")\n    \n    # Refresh available_models list\n    available_models = get_available_models()\n\n    # Initialize selected_model in session state if it doesn't exist\n    if \"selected_model\" not in st.session_state:\n        st.session_state.selected_model = available_models[0] if available_models else None\n\n    # Use a separate key for the selectbox\n    selectbox_key = \"remove_model_ui_model_selector\"\n\n    # Update selected_model when selectbox changes\n    if selectbox_key in st.session_state:\n        st.session_state.selected_model = st.session_state[selectbox_key]\n\n    selected_model = st.selectbox(\n        \"Select the model you want to remove:\", \n        available_models, \n        key=selectbox_key,\n        index=available_models.index(st.session_state.selected_model) if st.session_state.selected_model in available_models else 0\n    )\n\n    confirm_label = f\"❌ Confirm removal of model `{selected_model}`\"\n    confirm = st.checkbox(confirm_label)\n    if st.button(\"Remove Model\", key=\"remove_model\") and confirm:\n        if selected_model:\n            result = remove_model(selected_model)\n            st.write(result[\"message\"])\n\n            # Clear the cache of get_available_models\n            get_available_models.clear()\n\n            # Update the list of available models\n            st.session_state.available_models = get_available_models()\n            # Update selected_model if it was removed\n            if selected_model not in st.session_state.available_models:\n                st.session_state.selected_model = st.session_state.available_models[0] if st.session_state.available_models else None\n            st.rerun()\n        else:\n            st.error(\"Please select a model.\")"}
{"type": "source_file", "path": "pull_model.py", "content": "# pull_model.py\nimport streamlit as st\nimport requests\nimport json\nimport time\nimport humanize\n\ndef parse_model_name(input_string: str) -> str:\n    \"\"\"Parse and clean the model name from the user input.\"\"\"\n    parts = input_string.split()\n    if len(parts) > 1 and parts[0].lower() in [\"ollama\", \"run\", \"pull\"]:\n        return parts[-1]\n    return input_string.strip()\n\ndef format_size(size_bytes: int) -> str:\n    \"\"\"Format the file size in a human-readable format.\"\"\"\n    return humanize.naturalsize(size_bytes, binary=True)\n\ndef pull_model(model_name):\n    \"\"\"\n    Pull a model from the Ollama library.\n\n    Args:\n    model_name (str): The name of the model to pull.\n\n    Yields:\n    dict: Status updates during the pull process, including progress information.\n    \"\"\"\n    url = \"http://localhost:11434/api/pull\"\n    payload = {\"name\": model_name, \"stream\": True}\n    headers = {\"Content-Type\": \"application/json\"}\n\n    try:\n        with requests.post(url, json=payload, headers=headers, stream=True) as response:\n            response.raise_for_status()\n            total_size = 0\n            downloaded_size = 0\n            start_time = time.time()\n\n            for line in response.iter_lines():\n                if line:\n                    data = json.loads(line)\n                    if \"status\" in data:\n                        status = data[\"status\"]\n                        current_time = time.time()\n\n                        if status == \"pulling manifest\":\n                            yield {\"status\": \"Initializing...\", \"progress\": 0}\n                        elif \"total\" in data and \"completed\" in data:\n                            total_size = data.get(\"total\", total_size)\n                            downloaded_size = data.get(\"completed\", downloaded_size)\n                            progress = downloaded_size / total_size if total_size > 0 else 0\n\n                            elapsed_time = current_time - start_time\n                            speed = downloaded_size / elapsed_time if elapsed_time > 0 else 0\n                            remaining_time = (total_size - downloaded_size) / speed if speed > 0 else 0\n\n                            yield {\n                                \"status\": \"Downloading\",\n                                \"progress\": progress,\n                                \"details\": f\"{format_size(downloaded_size)} of {format_size(total_size)} downloaded\",\n                                \"speed\": format_size(speed) + \"/s\",\n                                \"remaining_time\": humanize.naturaldelta(remaining_time)\n                            }\n                        elif status in [\"verifying sha256 digest\", \"writing manifest\", \"removing any unused layers\"]:\n                            yield {\"status\": \"Finalizing...\", \"progress\": 1}\n                        elif status == \"success\":\n                            yield {\"status\": \"Complete\", \"progress\": 1}\n    except requests.RequestException as e:\n        yield {\"status\": f\"Error: {str(e)}\", \"progress\": 0}\n\ndef pull_models():\n    st.header(\"⬇ Pull a Model from Ollama Library\")\n    st.write(\"\"\"\n    Enter the exact name of the model you want to pull from the Ollama library. \n    You can just paste the whole model snippet from the model library page like 'ollama run llava-phi3' \n    or you can just enter the model name like 'llava-phi3' and then click 'Pull Model' to begin the download. \n    The progress of the download will be displayed below.\n    \"\"\")\n\n    col1, col2 = st.columns([4, 1], vertical_alignment=\"bottom\")\n\n    with col1:\n        model_input = st.text_input(\"Enter the name of the model you want to pull:\")\n    \n    with col2:\n        pull_button = st.button(\"Pull Model\", key=\"pull_model\")\n\n    if pull_button:\n        if model_input:\n            model_name = parse_model_name(model_input)\n            progress_bar = st.progress(0)\n            status_text = st.empty()\n            details_text = st.empty()\n            speed_text = st.empty()\n            remaining_time_text = st.empty()\n\n            for update in pull_model(model_name):\n                progress_bar.progress(int(update[\"progress\"] * 100))\n                status_text.text(update[\"status\"])\n                \n                if \"details\" in update:\n                    details_text.text(update[\"details\"])\n                \n                if \"speed\" in update:\n                    speed_text.text(f\"Speed: {update['speed']}\")\n                    remaining_time_text.text(f\"Estimated Time Remaining: {update['remaining_time']}\")\n                \n                time.sleep(0.1)  # Small delay to prevent overwhelming the UI\n\n            if update[\"status\"] == \"Complete\":\n                st.success(f\"Model '{model_name}' pulled successfully!\")\n            elif \"Error\" in update[\"status\"]:\n                st.error(update[\"status\"])\n        else:\n            st.error(\"Please enter a model name.\")\n\n    st.markdown(\"---\")\n    st.subheader(\"ℹ About Model Pulling\")\n    st.write(\"\"\"\n    Pulling a model downloads it from the Ollama library to your local machine. \n    This process may take several minutes for large models. \n    If the progress seems stuck, the download is likely still ongoing in the background.\n    \"\"\")\n\nif __name__ == \"__main__\":\n    pull_models()\n"}
{"type": "source_file", "path": "projects.py", "content": "# projects.py\nimport streamlit as st\nimport pandas as pd\nimport json\nimport os\nimport uuid\nfrom datetime import datetime\nimport base64\nfrom ollama_utils import get_available_models, call_ollama_endpoint\nfrom openai_utils import call_openai_api, OPENAI_MODELS\nfrom ollama_utils import load_api_keys\nfrom groq_utils import call_groq_api, GROQ_MODELS\nfrom prompts import get_agent_prompt, get_metacognitive_prompt, get_voice_prompt\nimport re\nfrom functools import lru_cache\n\nclass Task:\n    def __init__(self, name, description, deadline=None, priority=\"Medium\", completed=False, agent=\"None\", result=None, task_id=None):\n        self.task_id = task_id if task_id else str(uuid.uuid4())\n        self.name = name\n        self.description = description\n        self.deadline = deadline\n        self.priority = priority\n        self.completed = completed\n        self.agent = agent\n        self.result = result\n\n# Ensure the 'projects' directory exists\nif not os.path.exists('projects'):\n    os.makedirs('projects')\n\nclass DateTimeEncoder(json.JSONEncoder):\n    def default(self, obj):\n        if isinstance(obj, pd.Timestamp):\n            if pd.isna(obj):\n                return None\n            return obj.strftime('%Y-%m-%d %H:%M:%S')\n        elif isinstance(obj, Task):\n            return obj.__dict__\n        return super().default(obj)\n\ndef load_projects():\n    try:\n        with open('projects/projects.json', 'r') as f:\n            content = f.read()\n            return json.loads(content) if content else []\n    except FileNotFoundError:\n        return []\n    except json.JSONDecodeError as e:\n        st.error(f\"Error loading projects: {str(e)}. Starting with an empty project list.\")\n        return []\n\ndef save_projects(projects):\n    with open('projects/projects.json', 'w') as f:\n        json.dump(projects, f)\n\ndef load_tasks(project_name):\n    try:\n        with open(f'projects/{project_name}_tasks.json', 'r') as f:\n            content = f.read()\n            task_data = json.loads(content) if content else []\n        tasks = []\n        for data in task_data:\n            task = Task(\n                name=data['name'],\n                description=data['description'],\n                deadline=pd.to_datetime(data['deadline'], errors='coerce'),\n                priority=data['priority'],\n                completed=data['completed'],\n                agent=data['agent'],\n                result=data['result'],\n                task_id=data['task_id']\n            )\n            tasks.append(task)\n        return tasks\n    except FileNotFoundError:\n        return []\n    except json.JSONDecodeError as e:\n        st.error(f\"Error loading tasks for {project_name}: {str(e)}. Starting with an empty task list.\")\n        return []\n\n# Callback function to update task status in session state\ndef update_task_status(task_index, status, result=None):\n    if task_index < len(st.session_state.bm_tasks):\n        st.session_state.bm_tasks[task_index][\"status\"] = status\n        if result is not None:\n            st.session_state.bm_tasks[task_index][\"result\"] = result\n\ndef handle_user_input(step, task_data):\n    \"\"\"Handles user input for a specific task step.\"\"\"\n    user_input_config = step.get(\"user_input\")\n    if user_input_config:\n        input_type = user_input_config[\"type\"]\n        prompt = user_input_config[\"prompt\"]\n\n        if input_type == \"file_path\":\n            file_path = st.text_input(prompt, key=f\"user_input_{step['agent']}\")\n            if file_path:\n                task_data[\"file_path\"] = file_path\n            else:\n                st.warning(\"Please provide a file path.\")\n                return False  # Indicate that user input is not complete\n\n        elif input_type == \"options\":\n            options = user_input_config.get(\"options\", [])\n            selected_option = st.selectbox(prompt, options, key=f\"user_input_{step['agent']}\")\n            if selected_option:\n                task_data[\"selected_option\"] = selected_option\n            else:\n                st.warning(\"Please select an option.\")\n                return False  # Indicate that user input is not complete\n\n        elif input_type == \"confirmation\":\n            if not st.button(prompt, key=f\"user_input_{step['agent']}\"):\n                st.warning(\"Task skipped due to unconfirmed user input.\")\n                return False  # Indicate that user input is not complete\n\n    return True  # Indicate that user input is complete\n\ndef save_tasks(project_name, tasks):\n    tasks = [task for task in tasks if pd.notna(task.deadline)]\n    with open(f'projects/{project_name}_tasks.json', 'w') as f:\n        json.dump([task.__dict__ for task in tasks], f, cls=DateTimeEncoder)\n\ndef load_agents(project_name):\n    try:\n        with open(f'projects/{project_name}_agents.json', 'r') as f:\n            content = f.read()\n            return json.loads(content) if content else {}\n    except FileNotFoundError:\n        return {}\n    except json.JSONDecodeError as e:\n        st.error(f\"Error loading agents for {project_name}: {str(e)}. Starting with an empty agent list.\")\n        return {}\n\ndef save_agents(project_name, agents):\n    with open(f'projects/{project_name}_agents.json', 'w') as f:\n        json.dump(agents, f)\n\ndef get_corpus_options():\n    corpus_folder = \"corpus\"\n    if not os.path.exists(corpus_folder):\n        os.makedirs(corpus_folder)\n    return [\"None\"] + [f for f in os.listdir(corpus_folder) if os.path.isdir(os.path.join(corpus_folder, f))]\n\ndef get_corpus_context_from_db(corpus_folder, corpus, user_input):\n    return \"Sample context from the corpus database.\"\n\n\ndef get_all_models():\n    return get_available_models() + OPENAI_MODELS + GROQ_MODELS\n\ndef ai_agent(user_input, model, agent_type, metacognitive_type, voice_type, corpus, temperature, max_tokens, previous_responses=[]):\n    combined_prompt = \"\"\n    if agent_type != \"None\":\n        combined_prompt += get_agent_prompt()[agent_type] + \"\\n\\n\"\n    if metacognitive_type != \"None\":\n        combined_prompt += get_metacognitive_prompt()[metacognitive_type] + \"\\n\\n\"\n    if voice_type != \"None\":\n        combined_prompt += get_voice_prompt()[voice_type] + \"\\n\\n\"\n\n    if corpus != \"None\":\n        corpus_context = get_corpus_context_from_db(\"corpus\", corpus, user_input)\n        combined_prompt += f\"Context: {corpus_context}\\n\\n\"\n\n    for i, response in enumerate(previous_responses):\n        combined_prompt += f\"Response {i+1}: {response}\\n\\n\"\n\n    final_prompt = f\"{combined_prompt}User: {user_input}\"\n\n    api_keys = load_api_keys()  # Load API keys\n\n    if model in OPENAI_MODELS:\n        response = call_openai_api(model, [{\"role\": \"user\", \"content\": final_prompt}], temperature=temperature, max_tokens=max_tokens, openai_api_key=api_keys.get(\"openai_api_key\"))\n    elif model in GROQ_MODELS:\n        response = call_groq_api(model, final_prompt, temperature=temperature, max_tokens=max_tokens, groq_api_key=api_keys.get(\"groq_api_key\"))\n    else:\n        response, _, _, _ = call_ollama_endpoint(model, prompt=final_prompt, temperature=temperature, max_tokens=max_tokens)\n    \n    return response\n\ndef define_agent_block(name, agent_data=None):\n    if agent_data is None:\n        agent_data = {}\n    model = st.selectbox(f\"{name} Model\", get_all_models(), key=f\"{name}_model\", index=get_all_models().index(agent_data.get('model')) if agent_data.get('model') in get_all_models() else 0)\n    agent_type_options = [\"None\"] + list(get_agent_prompt().keys())\n    agent_type = st.selectbox(\n        f\"{name} Agent Type\",\n        agent_type_options,\n        key=f\"{name}_agent_type\",\n        index=agent_type_options.index(agent_data.get('agent_type')) if agent_data.get('agent_type') in agent_type_options else 0\n    )\n    metacognitive_options = [\"None\"] + list(get_metacognitive_prompt().keys())\n    metacognitive_type = st.selectbox(\n        f\"{name} Metacognitive Type\", \n        metacognitive_options, \n        key=f\"{name}_metacognitive_type\", \n        index=metacognitive_options.index(agent_data.get('metacognitive_type')) if agent_data.get('metacognitive_type') in metacognitive_options else 0\n    )\n    voice_options = [\"None\"] + list(get_voice_prompt().keys())\n    voice_type = st.selectbox(\n        f\"{name} Voice Type\",\n        voice_options,\n        key=f\"{name}_voice_type\",\n        index=voice_options.index(agent_data.get('voice_type')) if agent_data.get('voice_type') in voice_options else 0\n    )\n    corpus = st.selectbox(f\"{name} Corpus\", get_corpus_options(), key=f\"{name}_corpus\", index=get_corpus_options().index(agent_data.get('corpus')) if agent_data.get('corpus') in get_corpus_options() else 0)\n    temperature = st.slider(f\"{name} Temperature\", 0.0, 1.0, agent_data.get('temperature', 0.7), key=f\"{name}_temperature\")\n    max_tokens = st.slider(f\"{name} Max Tokens\", 4000, 128000, agent_data.get('max_tokens', 4000), key=f\"{name}_max_tokens\" , step=1000)\n    return {'model': model, 'agent_type': agent_type, 'metacognitive_type': metacognitive_type, 'voice_type': voice_type, 'corpus': corpus, 'temperature': temperature, 'max_tokens': max_tokens}\n\nclass ProjectManagerAgent:\n    def __init__(self, model: str, agent_type: str, temperature: float, max_tokens: int):\n        self.model = model\n        self.agent_type = agent_type\n        self.temperature = temperature\n        self.max_tokens = max_tokens\n\n    @lru_cache(maxsize=None)\n    def generate_workflow(self, user_request: str):\n        generation_prompt = f\"\"\"\n        Generate a JSON list of tasks and agents to fulfill the following user request. \n\n        User Request: {user_request}\n\n        Use a tree of thought approach to break down the user request into individual tasks. For each task, determine the most suitable agent type based on the task description and create an agent with a unique name.\n\n        Available Agent Types:\n        - Data Extractor: Extracts data from various sources, such as files or websites.\n        - Data Analyst: Analyzes data, calculates statistics, and identifies trends.\n        - Report Writer: Generates reports, summaries, and presentations based on data.\n        - Code Generator: Writes code in various programming languages based on specifications.\n        - Content Creator: Creates written content, such as articles, stories, and social media posts.\n        - Language Translator: Translates text between different languages.\n        - Chatbot: Engages in conversations and provides responses based on context.\n        - Image Generator: Creates images based on descriptions or prompts.\n        - Audio Transcriber: Transcribes audio recordings into text.\n        - Task Planner: Breaks down complex tasks into smaller, manageable steps.\n\n        JSON Output Structure:\n        {{\n            \"tasks\": [\n                {{\n                    \"name\": \"Task Name\",\n                    \"description\": \"Task Description\",\n                    \"deadline\": \"YYYY-MM-DD HH:MM:SS\",\n                    \"priority\": \"Low\", \"Medium\", or \"High\",\n                    \"completed\": false,\n                    \"agent\": \"Agent Name\",\n                    \"result\": null \n                }},\n                ...\n            ],\n            \"agents\": {{\n                \"Agent Name\": {{\n                    \"model\": \"Model Name\",\n                    \"agent_type\": \"Agent Type\",\n                    \"metacognitive_type\": \"Metacognitive Type\",\n                    \"voice_type\": \"Voice Type\",\n                    \"corpus\": \"Corpus Name\",\n                    \"temperature\": 0.7,\n                    \"max_tokens\": 4000\n                }},\n                ...\n            }}\n        }}\n        \"\"\"\n\n        api_keys = load_api_keys()  # Load API keys\n\n        if self.model in OPENAI_MODELS:\n            response = call_openai_api(self.model, [{\"role\": \"user\", \"content\": generation_prompt}], temperature=self.temperature, max_tokens=self.max_tokens, openai_api_key=api_keys.get(\"openai_api_key\"))\n        elif self.model in GROQ_MODELS:\n            response = call_groq_api(self.model, generation_prompt, temperature=self.temperature, max_tokens=self.max_tokens, groq_api_key=api_keys.get(\"groq_api_key\"))\n        else:\n            response = call_ollama_endpoint(\n                self.model,\n                prompt=generation_prompt,\n                temperature=self.temperature,\n                max_tokens=self.max_tokens\n            )[0]  # Assuming call_ollama_endpoint returns a tuple, we take the first element\n\n        generated_workflow = response\n\n        generated_workflow = generated_workflow.strip()\n        generated_workflow = generated_workflow.replace(\"'\", '\"')\n        generated_workflow = re.sub(r\"[^\\\\w\\\\s{}\\\\\\\\\\\\[\\\\]\\\":,./\\\\-+]+\", \"\", generated_workflow)\n\n        if not generated_workflow.startswith(\"{\"):\n            generated_workflow = \"{\" + generated_workflow\n        if not generated_workflow.endswith(\"}\"):\n            generated_workflow = generated_workflow + \"}\"\n\n        generated_workflow = re.sub(r\"(\\\\s*)'(\\w+)'(\\\\s*):\", r'\\1\"\\2\"\\3:', generated_workflow)\n        generated_workflow = re.sub(r\"([}\\]])(\\\\s*)(?=[{\\\\\\\\\\\\[\\\\]\\\"a-zA-Z])\", r\"\\1,\\2\", generated_workflow)\n\n        st.write(\"Generated Workflow:\", generated_workflow)\n\n        try:\n            generated_workflow = generated_workflow.strip()\n            if not generated_workflow.startswith(\"{\"):\n                generated_workflow = \"{\" + generated_workflow\n            if not generated_workflow.endswith(\"}\"):\n                generated_workflow = generated_workflow + \"}\"\n            \n            workflow_data = json.loads(generated_workflow)\n\n            tasks = []\n            for task_data in workflow_data.get('tasks', []):\n                task = Task(\n                    name=task_data.get(\"name\"),\n                    description=task_data.get(\"description\"),\n                    deadline=pd.to_datetime(task_data.get(\"deadline\"), errors='coerce'),\n                    priority=task_data.get(\"priority\", \"Medium\"),\n                    completed=task_data.get(\"completed\", False),\n                    agent=task_data.get(\"agent\"),\n                    result=task_data.get(\"result\")\n                )\n                tasks.append(task)\n\n            agents = workflow_data.get('agents', {})\n\n            return tasks, agents\n\n        except json.JSONDecodeError as e:\n            st.error(f\"Error loading or post-processing JSON: {e}\")\n            st.error(f\"Generated workflow: {generated_workflow}\")\n            return None, None\n\ndef initialize_session_state():\n    if 'projects' not in st.session_state:\n        st.session_state.projects = load_projects()\n    if 'selected_project' not in st.session_state:\n        st.session_state.selected_project = None\n    if 'tasks' not in st.session_state:\n        st.session_state.tasks = []\n    if 'agents' not in st.session_state:\n        st.session_state.agents = {}\n    if 'generated_tasks' not in st.session_state:\n        st.session_state.generated_tasks = []\n    if 'generated_agents' not in st.session_state:\n        st.session_state.generated_agents = {}\n    if 'project_manager_settings' not in st.session_state:\n        all_models = get_all_models()\n        st.session_state.project_manager_settings = {\n            'model': all_models[0] if all_models else 'gpt-3.5-turbo',  # Use the first available model or a default\n            'agent_type': 'Task Planner',\n            'temperature': 0.7,\n            'max_tokens': 4000,\n        }\n\ndef projects_main():\n    initialize_session_state()\n\n    if 'agents' not in st.session_state:\n        st.session_state.agents = {}\n\n    projects = load_projects()\n\n    st.title(\"🚀 Projects\")\n\n    # Sidebar configuration\n    with st.sidebar:\n        with st.expander(\"🤖 Project Manager Settings\", expanded=False):\n            all_models = get_all_models()  # Get all available models including Ollama, OpenAI, and Groq\n            st.session_state.project_manager_settings['model'] = st.selectbox(\n                \"Select Model for Project Manager\",\n                all_models,\n                index=all_models.index(st.session_state.project_manager_settings['model']) if st.session_state.project_manager_settings['model'] in all_models else 0\n            )\n            st.session_state.project_manager_settings['agent_type'] = \"Task Planner\"\n            st.write(f\"Agent Type: {st.session_state.project_manager_settings['agent_type']}\")\n            \n            st.session_state.project_manager_settings['temperature'] = st.slider(\"Temperature\", 0.0, 1.0, st.session_state.project_manager_settings['temperature'], step=0.1)\n            st.session_state.project_manager_settings['max_tokens'] = st.slider(\"Max Tokens\", 4000, 128000, st.session_state.project_manager_settings['max_tokens'], step=1000)\n\n    # Display task statistics\n    if projects:\n        total_tasks = completed_tasks = pending_tasks = high_priority_tasks = 0\n        for project in projects:\n            tasks = load_tasks(project)\n            total_tasks += len(tasks)\n            completed_tasks += sum(1 for task in tasks if task.completed)\n            pending_tasks += sum(1 for task in tasks if not task.completed)\n            high_priority_tasks += sum(1 for task in tasks if task.priority == 'High' and not task.completed)\n\n        col1, col2, col3, col4 = st.columns(4)\n        col1.metric(\"Total Tasks\", total_tasks)\n        col2.metric(\"Completed Tasks\", completed_tasks)\n        col3.metric(\"Pending Tasks\", pending_tasks)\n        col4.metric(\"High Priority Pending\", high_priority_tasks)\n    else:\n        st.info(\"No projects available for statistics.\")\n\n    col1, col2, col3, col4 = st.columns(4, gap=\"small\", vertical_alignment=\"bottom\")\n    \n    with col1:\n        selected_project = st.selectbox(\"🚀 Select Project\", projects)\n        if selected_project:\n            st.session_state.tasks = load_tasks(selected_project)\n            st.session_state.agents.update(load_agents(selected_project))\n\n    with col2:\n        with st.expander(\"🚀 Add New Project\"):\n            new_project = st.text_input(\"New Project Name\")\n            if st.button(\"Add Project\"):\n                if new_project and new_project not in projects:\n                    projects.append(new_project)\n                    save_projects(projects)\n                    st.success(f\"🟢 Project '{new_project}' added successfully!\")\n                elif new_project in projects:\n                    st.warning(f\"Project '{new_project}' already exists.\")\n                else:\n                    st.warning(\"Please enter a project name.\")\n\n    with col3:\n        with st.expander(\"🗑️ Delete Project\"):\n            project_to_delete = st.selectbox(\"Select Project to Delete\", projects)\n            if st.button(\"Delete Project\"):\n                if project_to_delete in projects:\n                    projects.remove(project_to_delete)\n                    task_file = f'projects/{project_to_delete}_tasks.json'\n                    agent_file = f'projects/{project_to_delete}_agents.json'\n                    if os.path.exists(task_file):\n                        os.remove(task_file)\n                    if os.path.exists(agent_file):\n                        os.remove(agent_file)\n                    save_projects(projects)\n                    st.success(f\"🟢 Project '{project_to_delete}' deleted successfully!\")\n                else:\n                    st.warning(\"Please select a valid project to delete.\")\n\n    with col4:\n        with st.expander(\"📦 Import/Export Tasks\"):\n            uploaded_file = st.file_uploader(\"Upload Tasks JSON\", type=\"json\")\n            if uploaded_file is not None:\n                try:\n                    new_tasks = json.load(uploaded_file)\n                    for task in new_tasks:\n                        task['deadline'] = pd.to_datetime(task['deadline'], errors='coerce')\n                    tasks = [Task(**task) for task in new_tasks if pd.notna(task['deadline'])]\n                    save_tasks(selected_project, tasks)\n                    st.success(\"🟢 Tasks imported successfully!\")\n                except json.JSONDecodeError:\n                    st.error(\"Error: Invalid JSON file. Please upload a valid JSON file.\")\n\n            if st.button(\"Download Tasks\"):\n                json_str = json.dumps([task.__dict__ for task in st.session_state.tasks], cls=DateTimeEncoder)\n                b64 = base64.b64encode(json_str.encode()).decode()\n                href = f'<a href=\"data:file/json;base64,{b64}\" download=\"{selected_project}_tasks.json\">Download JSON file</a>'\n                st.markdown(href, unsafe_allow_html=True)\n\n    # Option to auto-generate tasks\n    st.subheader(\"🤖 Auto-Generate Tasks\")\n    user_request = st.text_area(\"Enter your project request:\")\n\n    if selected_project:\n        if st.button(\"✅ Generate Tasks\"):\n            if user_request:\n                with st.spinner(\"Generating tasks and agents...\"):\n                    # Create a ProjectManagerAgent instance\n                    project_manager = ProjectManagerAgent(\n                        model=st.session_state.project_manager_settings['model'],\n                        agent_type=st.session_state.project_manager_settings['agent_type'],\n                        temperature=st.session_state.project_manager_settings['temperature'],\n                        max_tokens=st.session_state.project_manager_settings['max_tokens']\n                    )\n\n                    # Generate the workflow using the agent\n                    generated_tasks, generated_agents = project_manager.generate_workflow(user_request)\n\n                    if generated_tasks and generated_agents:\n                        st.session_state.tasks.extend(generated_tasks)\n                        st.session_state.agents.update(generated_agents)\n                        st.session_state.generated_tasks = generated_tasks\n                        st.session_state.generated_agents = generated_agents\n                        save_tasks(selected_project, st.session_state.tasks)\n                        save_agents(selected_project, st.session_state.agents)\n                        st.success(\"🟢 Tasks and agents generated and added to the project!\")\n                        \n                        # Display generated tasks\n                        st.subheader(\"Generated Tasks\")\n                        for task in generated_tasks:\n                            st.write(f\"Task: {task.name}\")\n                            st.write(f\"Description: {task.description}\")\n                            st.write(f\"Deadline: {task.deadline}\")\n                            st.write(f\"Priority: {task.priority}\")\n                            st.write(f\"Agent: {task.agent}\")\n                            st.write(\"---\")\n                        \n                        # Display generated agents\n                        st.subheader(\"Generated Agents\")\n                        for agent_name, agent_data in generated_agents.items():\n                            st.write(f\"Agent: {agent_name}\")\n                            st.write(f\"Model: {agent_data['model']}\")\n                            st.write(f\"Agent Type: {agent_data['agent_type']}\")\n                            st.write(\"---\")\n                    else:\n                        st.error(\"Failed to generate tasks. Please try again with a different request.\")\n\n        if selected_project:\n            # Task input form\n            with st.expander(f\"📌 Manually add a new task to {selected_project}\"):\n                task_name = st.text_input(\"Task Name\")\n                task_description = st.text_area(\"Task Description\")\n\n                col1, col2, col3 = st.columns(3)\n                with col1:\n                    task_deadline = st.date_input(\"Deadline Date\")\n                with col2:\n                    task_time = st.time_input(\"Deadline Time\")\n                with col3:\n                    task_priority = st.selectbox(\"Priority\", [\"Low\", \"Medium\", \"High\"])\n\n                agent_names = list(st.session_state.agents.keys())\n                task_agent = st.selectbox(\"AI Agent\", [\"None\"] + agent_names)\n\n                if st.button(\"📌 Add Task\"):\n                    deadline = pd.Timestamp(datetime.combine(task_deadline, task_time))\n                    if pd.notna(deadline):\n                        task = Task(\n                            name=task_name,\n                            description=task_description,\n                            deadline=deadline,\n                            priority=task_priority,\n                            completed=False,\n                            agent=task_agent,\n                            result=None\n                        )\n                        st.session_state.tasks.append(task)\n                        save_tasks(selected_project, st.session_state.tasks)\n                        st.success(\"🟢 Task added successfully!\")\n                    else:\n                        st.error(\"Invalid deadline. Please select a valid date and time.\")\n\n        # Display and manage tasks\n        st.subheader(f\"📋 Tasks for {selected_project}\")\n\n        if st.session_state.tasks:\n            df = pd.DataFrame([task.__dict__ for task in st.session_state.tasks])\n            df['deadline'] = pd.to_datetime(df['deadline'], errors='coerce')\n            df = df[['name', 'description', 'deadline', 'priority', 'completed', 'agent', 'result']]\n\n            edited_df = st.data_editor(\n                df,\n                column_config={\n                    \"completed\": st.column_config.CheckboxColumn(\n                        \"Completed\",\n                        help=\"Mark task as completed\",\n                        default=False,\n                    ),\n                    \"deadline\": st.column_config.DatetimeColumn(\n                        \"Deadline\",\n                        format=\"YYYY-MM-DD HH:mm:ss\",\n                    ),\n                },\n                hide_index=True,\n                num_rows=\"dynamic\"\n            )\n\n            updated_tasks = edited_df.to_dict('records')\n            if updated_tasks != [task.__dict__ for task in st.session_state.tasks]:\n                st.session_state.tasks = [Task(**task) for task in updated_tasks if pd.notna(task['deadline'])]\n                save_tasks(selected_project, st.session_state.tasks)\n                st.success(\"🟢 Tasks updated successfully!\")\n        else:\n            st.info(f\"No tasks found for {selected_project}. Add a task to get started!\")\n\n        # Define AI agents\n        st.subheader(\"🧑 AI Agents\")\n        agent_names = list(st.session_state.agents.keys())\n        num_agents = st.number_input(\"Number of Agents\", min_value=len(agent_names), max_value=10, value=len(agent_names))\n        \n        if num_agents > len(agent_names):\n            for i in range(len(agent_names), num_agents):\n                agent_name = f\"Agent {i+1}\"\n                st.session_state.agents[agent_name] = {'model': 'mistral:instruct', 'agent_type': 'None', 'metacognitive_type': 'None', 'voice_type': 'None', 'corpus': 'None', 'temperature': 0.7, 'max_tokens': 4000}\n                agent_names.append(agent_name)\n\n        new_agent_names = []\n        for agent_name in agent_names:\n            with st.expander(f\"🧑 {agent_name} Parameters\"):\n                new_agent_name = st.text_input(f\"{agent_name} Name\", value=agent_name, key=f\"{agent_name}_name\")\n                new_agent_names.append(new_agent_name)\n                st.session_state.agents[agent_name] = define_agent_block(agent_name, st.session_state.agents[agent_name])\n\n        if st.button(\"🧑 Save Agent Settings\"):\n            renamed_agents = {new_name: st.session_state.agents[old_name] for new_name, old_name in zip(new_agent_names, agent_names)}\n            st.session_state.agents = renamed_agents\n            save_agents(selected_project, renamed_agents)\n            st.success(\"🟢 Agent settings saved!\")\n\n        # Run AI agents on tasks\n        if st.button(\"🏃‍♂️🏃‍♀️💨 Run AI Agents on Tasks\"):\n            previous_responses = []\n            for task in st.session_state.tasks:\n                if task.agent != \"None\" and not task.completed:\n                    if task.agent in st.session_state.agents:\n                        agent_data = st.session_state.agents[task.agent]\n                        with st.spinner(f\"Running {task.agent} on task '{task.name}'...\"):\n                            result = ai_agent(task.description, **agent_data, previous_responses=previous_responses)\n                            st.write(f\"{task.agent} Output for '{task.name}': {result}\")\n                            task.result = result\n                            previous_responses.append(result)\n                    else:\n                        st.warning(f\"Agent {task.agent} not defined. Please define the agent before running.\")\n            save_tasks(selected_project, st.session_state.tasks)\n            st.success(\"🟢 AI agents completed their tasks!\")\n\ndef load_api_keys():\n    if os.path.exists(\"api_keys.json\"):\n        with open(\"api_keys.json\", \"r\") as f:\n            return json.load(f)\n    return {}\n\nif __name__ == \"__main__\":\n    projects_main()"}
{"type": "source_file", "path": "persona_lab/persona_model.py", "content": "from dataclasses import dataclass, asdict\nfrom datetime import datetime\nfrom typing import List, Dict, Optional\nimport uuid\nimport json\nimport sqlite3\nimport logging\nfrom pathlib import Path\n\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n    handlers=[\n        logging.FileHandler('persona_lab.log'),\n        logging.StreamHandler()\n    ]\n)\nlogger = logging.getLogger('persona_lab')\n\n@dataclass\nclass PersonaHistory:\n    persona_id: str\n    field_name: str\n    old_value: str\n    new_value: str\n    timestamp: datetime\n    modified_by: str\n\n@dataclass\nclass Persona:\n    id: str\n    name: str\n    age: int\n    nationality: str\n    occupation: str\n    background: str\n    routine: str\n    personality: str\n    skills: List[str]\n    avatar: str\n    model: str\n    temperature: float = 0.7\n    max_tokens: int = 150\n    created_at: datetime = None\n    modified_at: datetime = None\n    version: int = 1\n    tags: List[str] = None\n    notes: str = \"\"\n    generated_by: str = \"AI\"  # AI or Manual\n    interaction_history: List[Dict] = None\n    metadata: Dict = None\n\n    def __post_init__(self):\n        if self.created_at is None:\n            self.created_at = datetime.now()\n        if self.modified_at is None:\n            self.modified_at = self.created_at\n        if self.tags is None:\n            self.tags = []\n        if self.interaction_history is None:\n            self.interaction_history = []\n        if self.metadata is None:\n            self.metadata = {}\n\n    def to_dict(self):\n        return {\n            **asdict(self),\n            'created_at': self.created_at.isoformat(),\n            'modified_at': self.modified_at.isoformat(),\n            'skills': json.dumps(self.skills),\n            'tags': json.dumps(self.tags),\n            'interaction_history': json.dumps(self.interaction_history),\n            'metadata': json.dumps(self.metadata)\n        }\n\n    @classmethod\n    def from_dict(cls, data):\n        data = data.copy()\n        data['created_at'] = datetime.fromisoformat(data['created_at'])\n        data['modified_at'] = datetime.fromisoformat(data['modified_at'])\n        data['skills'] = json.loads(data['skills'])\n        data['tags'] = json.loads(data['tags'])\n        data['interaction_history'] = json.loads(data['interaction_history'])\n        data['metadata'] = json.loads(data['metadata'])\n        return cls(**data)\n\nclass PersonaDB:\n    def __init__(self, db_path: str = \"personas/persona_lab.db\"):\n        self.db_path = db_path\n        Path(db_path).parent.mkdir(exist_ok=True)\n        self.init_db()\n\n    def init_db(self):\n        with sqlite3.connect(self.db_path) as conn:\n            conn.execute(\"\"\"\n                CREATE TABLE IF NOT EXISTS personas (\n                    id TEXT PRIMARY KEY,\n                    name TEXT NOT NULL,\n                    age INTEGER,\n                    nationality TEXT,\n                    occupation TEXT,\n                    background TEXT,\n                    routine TEXT,\n                    personality TEXT,\n                    skills TEXT,\n                    avatar TEXT,\n                    model TEXT,\n                    temperature REAL,\n                    max_tokens INTEGER,\n                    created_at TEXT,\n                    modified_at TEXT,\n                    version INTEGER,\n                    tags TEXT,\n                    notes TEXT,\n                    generated_by TEXT,\n                    interaction_history TEXT,\n                    metadata TEXT\n                )\n            \"\"\")\n            conn.execute(\"\"\"\n                CREATE TABLE IF NOT EXISTS persona_history (\n                    id INTEGER PRIMARY KEY AUTOINCREMENT,\n                    persona_id TEXT,\n                    field_name TEXT,\n                    old_value TEXT,\n                    new_value TEXT,\n                    timestamp TEXT,\n                    modified_by TEXT,\n                    FOREIGN KEY (persona_id) REFERENCES personas (id)\n                )\n            \"\"\")\n            conn.execute(\"\"\"\n                CREATE TABLE IF NOT EXISTS persona_tags (\n                    id INTEGER PRIMARY KEY AUTOINCREMENT,\n                    name TEXT UNIQUE\n                )\n            \"\"\")\n\n    def create_persona(self, persona: Persona) -> bool:\n        try:\n            with sqlite3.connect(self.db_path) as conn:\n                conn.execute(\n                    \"\"\"\n                    INSERT INTO personas\n                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n                    \"\"\",\n                    tuple(persona.to_dict().values())\n                )\n                logger.info(f\"Created new persona: {persona.name} (ID: {persona.id})\")\n                return True\n        except Exception as e:\n            logger.error(f\"Error creating persona: {str(e)}\")\n            return False\n\n    def update_persona(self, persona: Persona, modified_by: str = \"system\") -> bool:\n        try:\n            # Get the old persona data\n            old_persona = self.get_persona(persona.id)\n            if old_persona is None:\n                return False\n\n            # Record changes in history\n            with sqlite3.connect(self.db_path) as conn:\n                old_dict = old_persona.to_dict()\n                new_dict = persona.to_dict()\n                for key in old_dict:\n                    if old_dict[key] != new_dict[key]:\n                        conn.execute(\n                            \"\"\"\n                            INSERT INTO persona_history\n                            (persona_id, field_name, old_value, new_value, timestamp, modified_by)\n                            VALUES (?, ?, ?, ?, ?, ?)\n                            \"\"\",\n                            (persona.id, key, str(old_dict[key]), str(new_dict[key]),\n                             datetime.now().isoformat(), modified_by)\n                        )\n\n                # Update the persona\n                persona.version += 1\n                persona.modified_at = datetime.now()\n                conn.execute(\n                    \"\"\"\n                    UPDATE personas\n                    SET name=?, age=?, nationality=?, occupation=?, background=?,\n                        routine=?, personality=?, skills=?, avatar=?, model=?,\n                        temperature=?, max_tokens=?, modified_at=?, version=?,\n                        tags=?, notes=?, generated_by=?, interaction_history=?,\n                        metadata=?\n                    WHERE id=?\n                    \"\"\",\n                    (*tuple(persona.to_dict().values())[1:-1], persona.id)\n                )\n                logger.info(f\"Updated persona: {persona.name} (ID: {persona.id})\")\n                return True\n        except Exception as e:\n            logger.error(f\"Error updating persona: {str(e)}\")\n            return False\n\n    def delete_persona(self, persona_id: str) -> bool:\n        try:\n            with sqlite3.connect(self.db_path) as conn:\n                conn.execute(\"DELETE FROM personas WHERE id=?\", (persona_id,))\n                conn.execute(\"DELETE FROM persona_history WHERE persona_id=?\", (persona_id,))\n                logger.info(f\"Deleted persona with ID: {persona_id}\")\n                return True\n        except Exception as e:\n            logger.error(f\"Error deleting persona: {str(e)}\")\n            return False\n\n    def get_persona(self, persona_id: str) -> Optional[Persona]:\n        try:\n            with sqlite3.connect(self.db_path) as conn:\n                cursor = conn.execute(\"SELECT * FROM personas WHERE id=?\", (persona_id,))\n                row = cursor.fetchone()\n                if row:\n                    return Persona.from_dict(dict(zip([col[0] for col in cursor.description], row)))\n                return None\n        except Exception as e:\n            logger.error(f\"Error getting persona: {str(e)}\")\n            return None\n\n    def get_all_personas(self) -> List[Persona]:\n        try:\n            with sqlite3.connect(self.db_path) as conn:\n                cursor = conn.execute(\"SELECT * FROM personas\")\n                return [\n                    Persona.from_dict(dict(zip([col[0] for col in cursor.description], row)))\n                    for row in cursor.fetchall()\n                ]\n        except Exception as e:\n            logger.error(f\"Error getting all personas: {str(e)}\")\n            return []\n\n    def get_persona_history(self, persona_id: str) -> List[PersonaHistory]:\n        try:\n            with sqlite3.connect(self.db_path) as conn:\n                cursor = conn.execute(\n                    \"SELECT * FROM persona_history WHERE persona_id=? ORDER BY timestamp DESC\",\n                    (persona_id,)\n                )\n                return [\n                    PersonaHistory(\n                        persona_id=row[1],\n                        field_name=row[2],\n                        old_value=row[3],\n                        new_value=row[4],\n                        timestamp=datetime.fromisoformat(row[5]),\n                        modified_by=row[6]\n                    )\n                    for row in cursor.fetchall()\n                ]\n        except Exception as e:\n            logger.error(f\"Error getting persona history: {str(e)}\")\n            return []\n\n    def add_tag(self, tag_name: str) -> bool:\n        try:\n            with sqlite3.connect(self.db_path) as conn:\n                conn.execute(\"INSERT OR IGNORE INTO persona_tags (name) VALUES (?)\", (tag_name,))\n                return True\n        except Exception as e:\n            logger.error(f\"Error adding tag: {str(e)}\")\n            return False\n\n    def get_all_tags(self) -> List[str]:\n        try:\n            with sqlite3.connect(self.db_path) as conn:\n                cursor = conn.execute(\"SELECT name FROM persona_tags\")\n                return [row[0] for row in cursor.fetchall()]\n        except Exception as e:\n            logger.error(f\"Error getting tags: {str(e)}\")\n            return []\n\n    def search_personas(self, query: str) -> List[Persona]:\n        try:\n            with sqlite3.connect(self.db_path) as conn:\n                cursor = conn.execute(\n                    \"\"\"\n                    SELECT * FROM personas\n                    WHERE name LIKE ? OR occupation LIKE ? OR nationality LIKE ?\n                    OR background LIKE ? OR personality LIKE ? OR notes LIKE ?\n                    \"\"\",\n                    tuple(['%' + query + '%'] * 6)\n                )\n                return [\n                    Persona.from_dict(dict(zip([col[0] for col in cursor.description], row)))\n                    for row in cursor.fetchall()\n                ]\n        except Exception as e:\n            logger.error(f\"Error searching personas: {str(e)}\")\n            return []\n"}
{"type": "source_file", "path": "research.py", "content": "# research.py\nimport streamlit as st\nimport ollama\nfrom typing import List, Dict\nimport json\nfrom langchain_community.embeddings import OllamaEmbeddings\nfrom langchain_community.vectorstores import Chroma\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.docstore.document import Document\nimport os\nimport re\nfrom agents import SearchManager, SearchAgent\nfrom search_libraries import duckduckgo_search, google_search, serpapi_search, serper_search, bing_search\nfrom ollama_utils import get_available_models\nfrom ollama_utils import load_api_keys\nfrom openai_utils import call_openai_api, OPENAI_MODELS\nfrom groq_utils import call_groq_api, GROQ_MODELS\nimport sqlite3\nfrom datetime import datetime\nfrom reportlab.lib.pagesizes import letter\nfrom reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Table, TableStyle\nfrom reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\nfrom reportlab.lib import colors\nfrom reportlab.lib.units import inch\nimport pdfkit\n\n# Create 'files' directory if it doesn't exist\nfiles_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'files')\nos.makedirs(files_dir, exist_ok=True)\n\n# Load API keys from settings file\ndef load_api_keys():\n    if os.path.exists(\"api_keys.json\"):\n        with open(\"api_keys.json\", \"r\") as f:\n            return json.load(f)\n    return {}\n\n# Save API keys to settings file\ndef save_api_keys(api_keys):\n    with open(\"api_keys.json\", \"w\") as f:\n        json.dump(api_keys, f, indent=4)\n\n# Database functions\ndef init_db():\n    conn = sqlite3.connect('research_reports.db')\n    c = conn.cursor()\n    c.execute('''CREATE TABLE IF NOT EXISTS reports\n                 (id INTEGER PRIMARY KEY AUTOINCREMENT,\n                  title TEXT,\n                  content TEXT,\n                  date TEXT)''')\n    conn.commit()\n    conn.close()\n\ndef save_report(title: str, content: str):\n    conn = sqlite3.connect('research_reports.db')\n    c = conn.cursor()\n    date = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n    c.execute(\"INSERT INTO reports (title, content, date) VALUES (?, ?, ?)\",\n              (title, content, date))\n    conn.commit()\n    conn.close()\n\ndef get_all_reports():\n    conn = sqlite3.connect('research_reports.db')\n    c = conn.cursor()\n    c.execute(\"SELECT id, title, date FROM reports\")\n    reports = c.fetchall()\n    conn.close()\n    return reports\n\ndef get_report_content(report_id: int):\n    conn = sqlite3.connect('research_reports.db')\n    c = conn.cursor()\n    c.execute(\"SELECT content FROM reports WHERE id=?\", (report_id,))\n    content = c.fetchone()[0]\n    conn.close()\n    return content\n\ndef delete_report(report_id: int):\n    conn = sqlite3.connect('research_reports.db')\n    c = conn.cursor()\n    c.execute(\"DELETE FROM reports WHERE id=?\", (report_id,))\n    conn.commit()\n    conn.close()\n\n# Export functions\ndef export_to_pdf(content: str, filename: str):\n    pdf_path = os.path.join(files_dir, filename)\n    pdf = SimpleDocTemplate(pdf_path, pagesize=letter, topMargin=0.5*inch, bottomMargin=0.5*inch)\n    styles = getSampleStyleSheet()\n    custom_style = ParagraphStyle(\n        'CustomStyle',\n        parent=styles['Normal'],\n        fontSize=10,\n        leading=14,\n        spaceBefore=6,\n        spaceAfter=6\n    )\n    title_style = ParagraphStyle(\n        'TitleStyle',\n        parent=styles['Heading1'],\n        fontSize=16,\n        leading=20,\n        spaceBefore=12,\n        spaceAfter=12\n    )\n    flowables = []\n\n    # Split content into sections\n    sections = content.split('\\n\\n')\n    for section in sections:\n        if section.startswith('Final Report:'):\n            flowables.append(Paragraph(\"Final Report\", title_style))\n            flowables.append(Paragraph(section[14:], custom_style))\n        elif section.startswith('References:'):\n            flowables.append(Paragraph(\"References\", title_style))\n            references = section[12:].split('\\n')\n            for ref in references:\n                flowables.append(Paragraph(ref, custom_style))\n        elif section.startswith('Search Results:'):\n            flowables.append(Paragraph(\"Search Results\", title_style))\n            search_results = section[16:].split('\\n\\n')\n            for result in search_results:\n                flowables.append(Paragraph(result, custom_style))\n        else:\n            flowables.append(Paragraph(section, custom_style))\n        flowables.append(Spacer(1, 12))\n\n    pdf.build(flowables)\n    return pdf_path\n\ndef export_to_txt(content: str, filename: str):\n    txt_path = os.path.join(files_dir, filename)\n    with open(txt_path, 'w', encoding='utf-8') as f:\n        f.write(content)\n    return txt_path\n\n# Load research model settings from JSON file\ndef load_research_model_settings():\n    if os.path.exists(\"research_models.json\"):\n        with open(\"research_models.json\", \"r\") as f:\n            return json.load(f)\n    return {}\n\n# Save research model settings to JSON file\ndef save_research_model_settings(settings):\n    with open(\"research_models.json\", \"w\") as f:\n        json.dump(settings, f, indent=4)\n\ndef research_interface():\n    st.title(\"🔬 Research\")\n\n    # Initialize database\n    init_db()\n\n    # Load API keys\n    api_keys = load_api_keys()\n\n    # Load research model settings\n    research_model_settings = load_research_model_settings()\n\n    # Sidebar settings\n    with st.sidebar:\n        # API Key Settings in a collapsed section\n        with st.expander(\"🔑 API Key Settings\", expanded=False):\n            api_keys[\"serpapi_api_key\"] = st.text_input(\"SerpApi API Key\", value=api_keys.get(\"serpapi_api_key\", \"\"), type=\"password\")\n            api_keys[\"serper_api_key\"] = st.text_input(\"Serper API Key\", value=api_keys.get(\"serper_api_key\", \"\"), type=\"password\")\n            api_keys[\"google_api_key\"] = st.text_input(\"Google Custom Search API Key\", value=api_keys.get(\"google_api_key\", \"\"), type=\"password\")\n            api_keys[\"google_cse_id\"] = st.text_input(\"Google Custom Search Engine ID\", value=api_keys.get(\"google_cse_id\", \"\"), type=\"password\")\n            api_keys[\"bing_api_key\"] = st.text_input(\"Bing Search API Key\", value=api_keys.get(\"bing_api_key\", \"\"), type=\"password\")\n            api_keys[\"openai_api_key\"] = st.text_input(\"OpenAI API Key\", value=api_keys.get(\"openai_api_key\", \"\"), type=\"password\")\n            api_keys[\"groq_api_key\"] = st.text_input(\"Groq API Key\", value=api_keys.get(\"groq_api_key\", \"\"), type=\"password\")\n            \n            if st.button(\"💾 Save API Keys\"):\n                save_api_keys(api_keys)\n                st.success(\"🟢 API keys saved!\")\n\n        # Model Settings in a collapsed section\n        with st.expander(\"🤖 Model Settings\", expanded=False):\n            available_models = get_available_models() + OPENAI_MODELS + GROQ_MODELS\n\n            # Load settings or defaults\n            manager_model = research_model_settings.get(\"manager_model\", available_models[0])\n            manager_temperature = research_model_settings.get(\"manager_temperature\", 0.7)\n            manager_max_tokens = research_model_settings.get(\"manager_max_tokens\", 4000)\n            agent_model = research_model_settings.get(\"agent_model\", available_models[0])\n            agent_temperature = research_model_settings.get(\"agent_temperature\", 0.7)\n            agent_max_tokens = research_model_settings.get(\"agent_max_tokens\", 4000)\n\n            # Display model selection and settings\n            manager_model = st.selectbox(\"Search Manager Model\", available_models, index=available_models.index(manager_model))\n            manager_temperature = st.slider(\"Search Manager Temperature\", 0.0, 1.0, manager_temperature, step=0.1)\n            manager_max_tokens = st.slider(\"Search Manager Max Tokens\", 1000, 128000, manager_max_tokens, step=1000)\n            agent_model = st.selectbox(\"Search Agent Model\", available_models, index=available_models.index(agent_model))\n            agent_temperature = st.slider(\"Search Agent Temperature\", 0.0, 1.0, agent_temperature, step=0.1)\n            agent_max_tokens = st.slider(\"Search Agent Max Tokens\", 1000, 128000, agent_max_tokens, step=1000)\n\n            if st.button(\"💾 Save Model Settings\"):\n                research_model_settings = {\n                    \"manager_model\": manager_model,\n                    \"manager_temperature\": manager_temperature,\n                    \"manager_max_tokens\": manager_max_tokens,\n                    \"agent_model\": agent_model,\n                    \"agent_temperature\": agent_temperature,\n                    \"agent_max_tokens\": agent_max_tokens\n                }\n                save_research_model_settings(research_model_settings)\n                st.success(\"🟢 Model settings saved!\")\n\n    # User research request\n    user_request = st.text_area(\"Enter your research request:\")\n\n    # Report length options with word count targets\n    report_lengths = {\n        \"short\": 500,\n        \"medium\": 1000,\n        \"long\": 2000\n    }\n    selected_length = st.selectbox(\"Report Length\", list(report_lengths.keys()), format_func=lambda x: f\"{x.capitalize()} (~{report_lengths[x]} words)\")\n\n    if st.button(\"🔬 Start Research\"):\n        if user_request:\n            with st.spinner(\"Initializing Search Manager...\"):\n                search_manager = SearchManager(\n                    name=\"Search Manager\",\n                    model=manager_model,\n                    temperature=manager_temperature,\n                    max_tokens=manager_max_tokens,\n                    api_keys=api_keys\n                )\n\n            with st.spinner(\"Running Research...\"):\n                final_report = \"\"\n                references = []\n                agent_outputs = []\n                for result_type, content in search_manager.run_research(user_request, selected_length, agent_model, report_lengths[selected_length]):\n                    if result_type.endswith(\"Report\") and result_type != \"Final Report\":\n                        with st.expander(result_type, expanded=False):\n                            st.write(content)\n                        agent_outputs.append({\"name\": result_type.replace(\" Report\", \"\"), \"content\": content})\n                    elif result_type == \"Final Report\":\n                        st.subheader(\"Generated Report\")\n                        st.write(content)\n                        final_report = content\n                    elif result_type == \"References\":\n                        st.subheader(\"All References\")\n                        for reference in content:\n                            st.write(reference)\n                        references = content\n\n                # After the research is complete, save the report\n                report_title = f\"Research on: {user_request[:50]}...\"  # Truncate long titles\n                full_report = f\"Final Report:\\n\\n{final_report}\\n\\nAll References:\\n\" + \"\\n\".join(references)\n                \n                # Add agent outputs to the full report\n                full_report += \"\\n\\nSearch Results:\\n\\n\"\n                for agent_output in agent_outputs:\n                    full_report += f\"{agent_output['name']}:\\n{agent_output['content']}\\n\\n\"\n                \n                save_report(report_title, full_report)\n                st.success(\"Research report saved successfully!\")\n\n        else:\n            st.error(\"Please enter a research request.\")\n\n    # Add a section for viewing saved reports\n    st.title(\"📗 Saved Reports\")\n    reports = get_all_reports()\n    for report_id, title, date in reports:\n        col1, col2, col3, col4, col5 = st.columns([12, 1, 1, 1, 1])\n        with col1:\n            st.write(f\"🟩 {title} ({date})\")\n        with col2:\n            if st.button(\"👀\", key=f\"view_{report_id}\"):\n                report_content = get_report_content(report_id)\n                st.text_area(\"Report Content\", report_content, height=300)\n        with col3:\n            if st.button(\"📕\", key=f\"pdf_{report_id}\"):\n                report_content = get_report_content(report_id)\n                pdf_file = f\"report_{report_id}.pdf\"\n                pdf_path = export_to_pdf(report_content, pdf_file)\n                with open(pdf_path, \"rb\") as f:\n                    st.download_button(\"PDF\", f, file_name=pdf_file)\n        with col4:\n            if st.button(\"📄\", key=f\"txt_{report_id}\"):\n                report_content = get_report_content(report_id)\n                txt_file = f\"report_{report_id}.txt\"\n                txt_path = export_to_txt(report_content, txt_file)\n                with open(txt_path, \"rb\") as f:\n                    st.download_button(\"TXT\", f, file_name=txt_file)\n        with col5:\n            if st.button(\"🗑️\", key=f\"delete_{report_id}\"):\n                delete_report(report_id)\n                st.rerun()  # Refresh the page to show the updated list of reports\n\nif __name__ == \"__main__\":\n    research_interface()"}
{"type": "source_file", "path": "prompts.py", "content": "# prompts.py (COMPLETE)\nimport json\nimport os\nimport streamlit as st\nfrom global_vrm_loader import global_vrm_loader\nimport base64\n\n# Directory where the script is located\nSCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))\n\ndef get_prompts_file_path(prompt_type):\n    prompts_folder = os.path.join(SCRIPT_DIR, \"prompts\")\n    if not os.path.exists(prompts_folder):\n        os.makedirs(prompts_folder)\n    return os.path.join(prompts_folder, f\"{prompt_type}_prompts.json\")\n\ndef load_prompts(prompt_type):\n    file_path = get_prompts_file_path(prompt_type)\n    if os.path.exists(file_path):\n        with open(file_path, \"r\") as f:\n            return json.load(f)\n    else:\n        return {}\n\ndef save_prompts(prompt_type, prompts):\n    file_path = get_prompts_file_path(prompt_type)\n    with open(file_path, \"w\") as f:\n        json.dump(prompts, f, indent=4)\n\ndef get_agent_prompt():\n    prompts = load_prompts(\"agent\")\n    if not prompts:\n        # Default prompts if none exist\n        prompts = {\n            \"General Assistant\": {\n                \"prompt\": \"You are a helpful AI assistant focused on general conversation and tasks. You aim to be clear, informative, and engaging while maintaining a natural conversational style.\",\n                \"model_voice\": \"en-US-Wavenet-A\"\n            },\n            \"Code Assistant\": {\n                \"prompt\": \"You are a skilled programming assistant. When discussing code, you provide clear explanations and practical examples. You help with debugging, optimization, and best practices.\",\n                \"model_voice\": \"en-US-Wavenet-B\"\n            },\n            \"Technical Writer\": {\n                \"prompt\": \"You are a technical writing assistant who helps create clear, well-structured documentation and articles. You excel at explaining complex topics in an accessible way.\",\n                \"model_voice\": \"en-US-Wavenet-C\"\n            }\n        }\n        save_prompts(\"agent\", prompts)\n    else:\n        # Create a new dictionary with updated prompts\n        updated_prompts = {}\n        for key, value in prompts.items():\n            if isinstance(value, dict):\n                if 'model_voice' not in value:\n                    value['model_voice'] = 'en-US-Wavenet-A'\n                updated_prompts[key] = value\n            else:\n                # Convert string prompts to dict format\n                updated_prompts[key] = {\n                    \"prompt\": value,\n                    \"model_voice\": \"en-US-Wavenet-A\"\n                }\n        prompts = updated_prompts\n        save_prompts(\"agent\", prompts)\n    return prompts\n\ndef get_metacognitive_prompt():\n    prompts = load_prompts(\"metacognitive\")\n    if not prompts:\n        # Default prompts if none exist\n        prompts = {\n            \"Analytical\": \"I approach problems systematically, breaking them down into smaller components and analyzing each part carefully.\",\n            \"Intuitive\": \"I combine logical analysis with intuitive understanding, considering both practical and creative solutions.\",\n            \"Collaborative\": \"I engage in a collaborative thinking process, actively involving you in the discussion and solution-finding.\"\n        }\n        save_prompts(\"metacognitive\", prompts)\n    return prompts\n\ndef get_voice_prompt():\n    prompts = load_prompts(\"voice\")\n    if not prompts:\n        # Default prompts if none exist\n        prompts = {\n            \"Professional\": \"I maintain a clear, professional tone while being approachable and helpful.\",\n            \"Casual\": \"I use a friendly, conversational tone while remaining informative and helpful.\",\n            \"Technical\": \"I use precise technical language when appropriate, but can adjust my explanations to match your expertise level.\"\n        }\n        save_prompts(\"voice\", prompts)\n    return prompts\n\ndef get_identity_prompt():\n    return load_prompts(\"identity\")\n\ndef manage_prompts():\n    st.title(\"✨ Prompts\")\n    prompt_types = [\"Agent\", \"Metacognitive\", \"Voice\", \"Identity\", \"Model Voice\"]\n    selected_prompt_type = st.selectbox(\"Select Prompt Type:\", prompt_types)\n\n    if selected_prompt_type == \"Agent\":\n        prompts = get_agent_prompt()\n        prompt_type = \"agent\"\n    elif selected_prompt_type == \"Metacognitive\":\n        prompts = get_metacognitive_prompt()\n        prompt_type = \"metacognitive\"\n    elif selected_prompt_type == \"Voice\":\n        prompts = get_voice_prompt()\n        prompt_type = \"voice\"\n    elif selected_prompt_type == \"Identity\":\n        prompts = get_identity_prompt()\n        prompt_type = \"identity\"\n    else:\n        prompts = load_prompts(\"model_voice\")\n        prompt_type = \"model_voice\"\n\n    st.markdown(\"\"\"\n        <style>\n        .stDataFrame, div[data-testid=\"stDataEditor\"] {\n            width: 100% !important;\n        }\n        </style>\n    \"\"\", unsafe_allow_html=True)\n\n    if selected_prompt_type == \"Agent\":\n        for key in list(prompts.keys()):\n            with st.expander(key):\n                if isinstance(prompts[key], str):\n                    prompts[key] = {\"prompt\": prompts[key], \"model_voice\": \"en-US-Wavenet-A\"}\n\n                prompts[key]['prompt'] = st.text_area(\"Prompt\", value=prompts[key].get('prompt', ''), key=f\"{key}_prompt\")\n                voice_options = [\"en-US-Wavenet-A\", \"en-US-Wavenet-B\", \"en-US-Wavenet-C\", \"en-US-Wavenet-D\"]\n                prompts[key]['model_voice'] = st.selectbox(\"🗣️ Model Voice:\", voice_options, index=voice_options.index(prompts[key].get('model_voice', \"en-US-Wavenet-A\")), key=f\"{key}_model_voice\")\n\n                # VRM Model Upload\n                vrm_model_file = st.file_uploader(f\"Upload VRM Model for {key}\", type=[\"vrm\"], key=f\"vrm_upload_{key}\")\n                if vrm_model_file is not None:\n                    # Save the VRM model file\n                    agent_models_dir = os.path.join(SCRIPT_DIR, \"agent_models\")\n                    if not os.path.exists(agent_models_dir):\n                        os.makedirs(agent_models_dir)\n                    vrm_model_path = os.path.join(agent_models_dir, vrm_model_file.name)\n                    with open(vrm_model_path, \"wb\") as f:\n                        f.write(vrm_model_file.getvalue())\n\n                    prompts[key]['vrm_model_path'] = vrm_model_path  # Store the path\n                    global_vrm_loader.load_model(key, vrm_model_path)\n                    st.success(\"VRM model uploaded successfully!\")\n\n        if st.button(\"Add New Agent Prompt\"):\n            new_key = f\"New Agent {len(prompts) + 1}\"\n            prompts[new_key] = {\"prompt\": \"\", \"model_voice\": \"en-US-Wavenet-A\"}\n            st.success(f\"Added new agent prompt: {new_key}\")\n            st.rerun()\n    else:\n        edited_prompts = st.data_editor(prompts, num_rows=\"dynamic\", key=f\"{selected_prompt_type}_prompts\")\n\n    if st.button(\"Save Prompts\"):\n        if selected_prompt_type == \"Agent\":\n            save_prompts(prompt_type, prompts)\n        else:\n            save_prompts(prompt_type, edited_prompts)\n        st.success(f\"{selected_prompt_type} prompts saved successfully!\")\n        st.rerun()\n\nif __name__ == \"__main__\":\n    manage_prompts()"}
{"type": "source_file", "path": "repo_docs.py", "content": "# repo_docs.py\nimport os\nimport requests\nimport json\nimport subprocess\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\nimport streamlit as st\nfrom fpdf import FPDF\nimport tempfile\nimport queue\nimport re\nfrom datetime import datetime\nfrom radon.complexity import cc_visit, cc_rank\nfrom radon.metrics import mi_visit, h_visit\nfrom flake8.api import legacy as flake8\nfrom ollama_utils import get_available_models as get_ollama_models\nfrom ollama_utils import load_api_keys\nfrom openai_utils import OPENAI_MODELS\nfrom groq_utils import GROQ_MODELS\n\n# Settings file for model settings\nMODEL_SETTINGS_FILE = \"model_settings.json\"\n\nclass PDF(FPDF):\n    def header(self):\n        self.set_font('Arial', 'B', 12)\n        self.cell(0, 10, 'Repository Analysis', 0, 1, 'C')\n\n    def chapter_title(self, title):\n        self.set_font('Arial', 'B', 12)\n        self.cell(0, 10, title.encode('latin1', 'replace').decode('latin1'), 0, 1, 'L')\n        self.ln(10)\n\n    def chapter_body(self, body):\n        self.set_font('Arial', '', 12)\n        body = body.encode('latin1', 'replace').decode('latin1')\n        self.multi_cell(0, 10, body)\n        self.ln()\n\n    def add_chapter(self, title, body):\n        self.add_page()\n        self.chapter_title(title)\n        self.chapter_body(body)\n\ndef call_ollama_endpoint(model, prompt, temperature=0.5, max_tokens=150, presence_penalty=0.0, frequency_penalty=0.0, context=None):\n    url = \"http://localhost:11434/api/generate\"\n    payload = {\n        \"model\": model,\n        \"prompt\": prompt,\n        \"temperature\": temperature,\n        \"max_tokens\": max_tokens,\n        \"presence_penalty\": presence_penalty,\n        \"frequency_penalty\": frequency_penalty,\n        \"context\": context,\n        \"stream\": True,\n    }\n    headers = {\n        \"Content-Type\": \"application/json\"\n    }\n\n    try:\n        response = requests.post(url, json=payload, headers=headers, stream=True)\n        response.raise_for_status()\n\n        full_response = \"\"\n        eval_count = 0\n        eval_duration = 0\n        for line in response.iter_lines():\n            if line:\n                decoded_line = line.decode('utf-8')\n                try:\n                    json_response = json.loads(decoded_line)\n                    if 'response' in json_response:\n                        full_response += json_response['response']\n                    if 'eval_count' in json_response:\n                        eval_count = json_response['eval_count']\n                    if 'eval_duration' in json_response:\n                        eval_duration = json_response['eval_duration']\n                except json.JSONDecodeError:\n                    print(f\"Skipping invalid JSON line: {decoded_line}\")\n\n        return full_response.strip(), None, eval_count, eval_duration\n\n    except requests.exceptions.RequestException as e:\n        print(f\"Error making request to {url}: {e}\")\n        return f\"Error calling Ollama endpoint: {str(e)}\", None, 0, 0\n\n@st.cache_data\ndef get_available_models():\n    ollama_models = get_ollama_models()\n    all_models = ollama_models + OPENAI_MODELS + GROQ_MODELS\n    return all_models\n\ndef generate_documentation_stream(file_content, task_type, model, temperature, max_tokens, repo_info=None):\n    if task_type == \"documentation\":\n        prompt = f\"\"\"\nYou are an expert in programming and technical writing. Your task is to generate comprehensive documentation and insightful commentary for the provided code. \n\nFollow these steps:\n\n1. **Overview**: Provide a high-level summary of what the code does, its purpose, its main components, and how it connects to the other scripts in this repository.\n2. **Code Walkthrough**: Go through the code section by section, explaining the functionality of each part. Highlight key functions, classes, and methods.\n3. **Best Practices**: Identify any non-idiomatic practices or areas where the code could be improved. Suggest best practices and optimizations.\n4. **Examples**: Where applicable, include example usages or scenarios that demonstrate how the code should be used.\n5. **Formatting**: Ensure that the documentation is well-organized, clearly formatted, and easy to read. Use bullet points, headers, and code blocks where necessary.\n\nGenerate a detailed and neatly formatted documentation for the following code:\n\n{file_content}\n\"\"\"\n    elif task_type == \"debug\":\n        prompt = f\"\"\"\nYou are a highly experienced code debugger with a deep understanding of best practices and coding standards. Your task is to thoroughly analyze the provided code, identify any issues, and offer recommendations for improvements. Follow these steps:\n\n1. **Identify Issues**: Go through the code line by line and identify syntax errors, logical errors, performance issues, and any non-idiomatic practices.\n2. **Provide Recommendations**: For each issue identified, provide a detailed explanation of why it is an issue and how it can be fixed or improved.\n3. **Code Examples**: Whenever possible, include code snippets to illustrate the recommended changes or improvements.\n4. **Formatting**: Ensure that your debug report is well-organized, clearly formatted, and easy to read. Use bullet points, headers, and code blocks where necessary.\n\nGenerate a comprehensive and neatly formatted debug report for the following code:\n\n{file_content}\n\"\"\"\n    elif task_type == \"readme\":\n        existing_readme = repo_info.get('existing_readme', '') if repo_info else ''\n        requirements = repo_info.get('requirements', '') if repo_info else ''\n        file_structure = '\\n'.join(repo_info.get('file_structure', [])) if repo_info else ''\n        \n        # Create a summary of key files content\n        key_files_summary = \"\"\n        for filename, content in (repo_info.get('key_files', {}) if repo_info else {}).items():\n            key_files_summary += f\"\\n### {filename}:\\n```python\\n{content[:500]}...\\n```\\n\"\n\n        if existing_readme:\n            prompt = f\"\"\"\nYou are an expert technical writer and programmer, tasked with updating an existing README.md file for a GitHub repository.\nThe repository has evolved, and the README needs to be updated to reflect the current state of the project.\n\nHere is the current state of the repository:\n\n1. Existing README.md:\n```markdown\n{existing_readme}\n```\n\n2. Current Repository Structure:\n```\n{file_structure}\n```\n\n3. Key Files:\n{key_files_summary}\n\n4. Requirements (if available):\n```\n{requirements}\n```\n\nTask: Update the README.md while maintaining its current structure and style. Make the following improvements:\n1. Update any outdated information based on the current repository structure\n2. Add any new features or components that are visible in the current codebase\n3. Update installation instructions if new dependencies are present\n4. Maintain any custom sections or formatting from the original README\n5. Ensure all links and references are still valid\n6. Add any missing but important sections that a good README should have\n\nKeep what works from the existing README, but enhance it based on the current state of the repository.\nThe output should be a complete, updated README.md file.\n\"\"\"\n        else:\n            prompt = f\"\"\"\nYou are an expert technical writer and programmer, tasked with creating a comprehensive README.md file for a GitHub repository.\nBased on the repository structure and contents, create a clear and informative README that will help users understand and use this project.\n\nRepository Analysis:\n\n1. Repository Structure:\n```\n{file_structure}\n```\n\n2. Key Files:\n{key_files_summary}\n\n3. Requirements (if available):\n```\n{requirements}\n```\n\nCreate a comprehensive README.md that includes:\n\n1. Project Title and Description\n   - Analyze the code to determine the project's main purpose\n   - Provide a clear, concise description of what the project does\n   - Highlight key features and capabilities\n\n2. Installation\n   - List all prerequisites\n   - Step-by-step installation instructions\n   - Environment setup requirements\n\n3. Usage\n   - Getting started guide\n   - Common use cases and examples\n   - Configuration options\n   - Command-line arguments (if applicable)\n\n4. Project Structure\n   - Explain the main components\n   - Describe how different parts work together\n   - Document key files and their purposes\n\n5. Dependencies\n   - List major dependencies with versions\n   - Explain why each major dependency is needed\n\n6. Contributing\n   - Guidelines for contributing\n   - Development setup\n   - Testing instructions\n\n7. License\n   - Include MIT license information\n\n8. Contact/Support\n   - How to get help\n   - Where to report issues\n\nMake the README clear, professional, and well-formatted using Markdown.\nFocus on helping users understand and use the project effectively.\n\"\"\"\n    elif task_type == \"project_summary\":\n        prompt = f\"\"\"\nYou are an expert technical writer, skilled at creating project summary documentation in Markdown format.\nCreate a comprehensive project_summary.md file for a repository containing the following files and folders.\nThe project_summary.md file will include:\n\n1. Table of Contents\n    - List all files and folders in the repository, excluding the 'files' folder.\n2. File Details\n    - For each file, provide the following information:\n        - Full Path\n        - Extension\n        - Language (if applicable)\n        - Size (in bytes)\n        - Created Date and Time\n        - Modified Date and Time\n        - Code Snippet (if applicable)\n\nINSTRUCTION: Use appropriate Markdown formatting to make the project summary visually appealing and easy to read. Here's the file and folder structure to base the project summary on:\n\n{file_content}\n\"\"\"\n    elif task_type == \"requirements\":\n        return None\n    api_keys = load_api_keys()\n    \n    if model in OPENAI_MODELS:\n        from openai_utils import call_openai_api\n        response = call_openai_api(model, [{\"role\": \"user\", \"content\": prompt}], temperature=temperature, max_tokens=max_tokens, openai_api_key=api_keys.get('openai_api_key'))\n        yield response\n    elif model in GROQ_MODELS:\n        from groq_utils import call_groq_api\n        response = call_groq_api(model, prompt, temperature=temperature, max_tokens=max_tokens, groq_api_key=api_keys.get('groq_api_key'))\n        yield response\n    else:\n        url = \"http://localhost:11434/api/generate\"\n        payload = {\n            \"model\": model,\n            \"prompt\": prompt,\n            \"temperature\": temperature,\n            \"max_tokens\": max_tokens,\n            \"stream\": True,\n        }\n        headers = {\"Content-Type\": \"application/json\"}\n\n        with requests.post(url, json=payload, headers=headers, stream=True) as response:\n            response.raise_for_status()\n            for line in response.iter_lines():\n                if line:\n                    decoded_line = line.decode('utf-8')\n                    try:\n                        json_response = json.loads(decoded_line)\n                        if 'response' in json_response:\n                            yield json_response['response']\n                    except json.JSONDecodeError:\n                        print(f\"Skipping invalid JSON line: {decoded_line}\")\n\ndef run_pylint(file_path):\n    result = subprocess.run(['pylint', file_path], capture_output=True, text=True)\n    return result.stdout\n\ndef run_phpstan(file_path):\n    result = subprocess.run(['phpstan', 'analyse', file_path], capture_output=True, text=True)\n    return result.stdout\n\ndef run_eslint(file_path):\n    result = subprocess.run(['eslint', file_path], capture_output=True, text=True)\n    return result.stdout\n\ndef get_all_code_files(root_dir, exclude_patterns):\n    code_files = []\n    for subdir, _, files in os.walk(root_dir):\n        for file in files:\n            file_path = os.path.join(subdir, file)\n            if file.endswith(('.py', '.php', '.js', '.css', '.html')):\n                try:\n                    if not any(re.search(bad_pattern, file_path) for bad_pattern in exclude_patterns):\n                        code_files.append(file_path)\n                except re.error as e:\n                    problematic_patterns = [bad_pattern for bad_pattern in exclude_patterns if re.search(bad_pattern, file_path)]\n                    st.warning(f\"Invalid exclude pattern(s): {problematic_patterns}. Error: {e}. Skipping these pattern(s).\")\n    return code_files\n\ndef get_file_info(file_path):\n    try:\n        file_stats = os.stat(file_path)\n        created_time = datetime.fromtimestamp(file_stats.st_ctime).strftime('%Y-%m-%d %H:%M:%S')\n        modified_time = datetime.fromtimestamp(file_stats.st_mtime).strftime('%Y-%m-%d %H:%M:%S')\n\n        extension = os.path.splitext(file_path)[1].lower()\n        language = {\n            '.py': \"python\",\n            '.txt': \"plaintext\",\n            '.md': \"markdown\",\n            '.json': \"json\",\n            '.sh': \"bash\",\n            '.csv': \"csv\",\n            '.php': \"php\",\n            '.js': \"javascript\",\n            '.css': \"css\",\n            '.html': \"html\",\n            '.xml': \"xml\",\n            '.yaml': \"yaml\",\n            '.yml': \"yaml\",\n            '.sql': \"sql\",\n            '.java': \"java\",\n            '.cpp': \"cpp\",\n            '.c': \"c\",\n            '.h': \"c\",\n            '.rb': \"ruby\",\n            '.go': \"go\",\n            '.rs': \"rust\",\n            '.ts': \"typescript\",\n            '.swift': \"swift\",\n            '.kt': \"kotlin\",\n            '.scala': \"scala\",\n            '.pl': \"perl\",\n            '.lua': \"lua\",\n            '.r': \"r\",\n            '.m': \"matlab\",\n            '.vb': \"vbnet\",\n            '.fs': \"fsharp\",\n            '.hs': \"haskell\",\n            '.ex': \"elixir\",\n            '.erl': \"erlang\",\n            '.clj': \"clojure\",\n            '.groovy': \"groovy\",\n            '.dart': \"dart\",\n            '.f': \"fortran\",\n            '.cob': \"cobol\",\n            '.asm': \"assembly\",\n        }.get(extension, \"unknown\")\n\n        file_info = {\n            \"Full Path\": file_path,\n            \"Extension\": extension,\n            \"Language\": language,\n            \"Size\": file_stats.st_size,\n            \"Created\": created_time,\n            \"Modified\": modified_time\n        }\n\n        # Read file content for all supported languages\n        if language != \"unknown\":\n            try:\n                with open(file_path, 'r', encoding='utf-8') as code_file:\n                    file_info['Code'] = code_file.read()\n            except UnicodeDecodeError:\n                file_info['Code'] = f\"Error reading file: UnicodeDecodeError\"\n\n        # Calculate code complexity metrics for Python files\n        if language == \"python\":\n            complexity = cc_visit(file_info['Code'])\n            maintainability = mi_visit(file_info['Code'], multi=False)\n            halstead = h_visit(file_info['Code'])\n\n            file_info['Complexity'] = {\n                'Cyclomatic Complexity': [f\"{func.name}: {func.complexity} ({cc_rank(func.complexity)})\" for func in complexity],\n                'Maintainability Index': maintainability,\n                'Halstead Metrics': halstead\n            }\n\n            # Analyze code style using flake8\n            style_guide = flake8.get_style_guide()\n            report = style_guide.check_files([file_path])\n\n            style_violations = []\n            for error in report.get_statistics(''):\n                try:\n                    if isinstance(error, str):\n                        style_violations.append(error)\n                    else:\n                        style_violations.append(f\"{error.code} ({error.text}): line {error.line}\")\n                except AttributeError:\n                    style_violations.append(f\"Unexpected error format: {error}\")\n\n            file_info['Style Violations'] = style_violations\n\n        elif language == \"php\":\n            file_info['PHPStan Report'] = run_phpstan(file_path)\n\n        elif language in [\"javascript\", \"css\"]:\n            file_info['ESLint Report'] = run_eslint(file_path)\n\n        return file_info\n    except FileNotFoundError:\n        return None\n\ndef process_file_with_updates(file_path, task_type, model, temperature, max_tokens, api_key, update_queue, progress_bar, status_text, output_area, repo_info=None):\n    try:\n        with open(file_path, 'r', encoding='utf-8') as file:\n            file_content = file.read()\n\n        # Update status\n        update_queue.put((\"status\", f\"Processing: {file_path}\"))\n        \n        # Generate documentation with real-time updates\n        documentation = \"\"\n        for chunk in generate_documentation_stream(file_content, task_type, model, temperature, max_tokens, repo_info):\n            documentation += chunk\n            update_queue.put((\"output\", documentation))\n\n        pylint_report = run_pylint(file_path) if task_type == \"debug\" and file_path.endswith('.py') else \"\"\n        phpstan_report = run_phpstan(file_path) if task_type == \"debug\" and file_path.endswith('.php') else \"\"\n        eslint_report = run_eslint(file_path) if task_type == \"debug\" and (file_path.endswith('.js') or file_path.endswith('.css')) else \"\"\n\n        return file_path, documentation, pylint_report + phpstan_report + eslint_report, file_content\n    except UnicodeDecodeError:\n        print(f\"Error reading file {file_path}: UnicodeDecodeError\")\n        return file_path, f\"Error reading file: UnicodeDecodeError\", \"\", \"\"\n\ndef analyze_repository_structure(repo_path, code_files):\n    \"\"\"Analyze the repository structure and return key information.\"\"\"\n    repo_info = {\n        'main_files': [],\n        'key_files': {},\n        'file_structure': [],\n        'existing_readme': None,\n        'existing_readme_path': None,\n        'requirements': None,\n        'setup_file': None,\n        'entry_points': []\n    }\n    \n    # First, explicitly check for README.md in the repository root\n    root_readme_path = os.path.join(repo_path, 'README.md')\n    if os.path.exists(root_readme_path):\n        try:\n            with open(root_readme_path, 'r', encoding='utf-8') as f:\n                repo_info['existing_readme'] = f.read()\n                repo_info['existing_readme_path'] = root_readme_path\n        except Exception as e:\n            print(f\"Error reading root README.md: {str(e)}\")\n    \n    # Look for key files\n    for file_path in code_files:\n        rel_path = os.path.relpath(file_path, repo_path)\n        repo_info['file_structure'].append(rel_path)\n        \n        filename = os.path.basename(file_path)\n        # Only look for README in other locations if we haven't found it in root\n        if filename.lower() == 'readme.md' and not repo_info['existing_readme']:\n            try:\n                with open(file_path, 'r', encoding='utf-8') as f:\n                    repo_info['existing_readme'] = f.read()\n                    repo_info['existing_readme_path'] = file_path\n            except Exception as e:\n                print(f\"Error reading README.md at {file_path}: {str(e)}\")\n        elif filename in ['setup.py', 'pyproject.toml']:\n            repo_info['setup_file'] = file_path\n        elif filename.endswith('requirements.txt'):\n            try:\n                with open(file_path, 'r', encoding='utf-8') as f:\n                    repo_info['requirements'] = f.read()\n            except Exception:\n                pass\n        elif filename in ['main.py', 'app.py', 'index.py']:\n            repo_info['entry_points'].append(file_path)\n            repo_info['main_files'].append(file_path)\n    \n    # Read content of main files\n    for file_path in repo_info['main_files']:\n        try:\n            with open(file_path, 'r', encoding='utf-8') as f:\n                repo_info['key_files'][os.path.basename(file_path)] = f.read()\n        except Exception:\n            continue\n    \n    return repo_info\n\ndef generate_pdf(results, output_path, task_type):\n    pdf = PDF()\n    pdf.set_left_margin(10)\n    pdf.set_right_margin(10)\n    pdf.add_page()\n\n    for file_path, documentation, pylint_report, file_content in results:\n        chapter_title = f\"File: {file_path}\"\n        if task_type == \"debug\":\n            chapter_body = f\"Pylint Report:\\n{pylint_report}\\n\\nDebug Report:\\n{documentation}\\n\\nCode:\\n{file_content}\"\n        elif task_type == \"documentation\":\n            chapter_body = f\"Documentation:\\n{documentation}\\n\\nCode:\\n{file_content}\"\n        else:  # README\n            chapter_body = documentation\n        pdf.add_chapter(chapter_title, chapter_body)\n\n    # Create 'files' directory if it doesn't exist\n    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n    pdf.output(output_path, 'F')\n\ndef generate_requirements_file(repo_path, exclude_patterns):\n    import importlib.util\n    import pkg_resources\n    from importlib.metadata import distribution, PackageNotFoundError\n    import sys\n    from pathlib import Path\n    import subprocess\n    import re\n    from collections import defaultdict\n    \n    # Categories with their common packages\n    PACKAGE_CATEGORIES = {\n        'Core Dependencies': {\n            'streamlit', 'streamlit-option-menu', 'streamlit-extras', 'streamlit-javascript',\n            'ollama', 'openai', 'streamlit-flow'\n        },\n        'Machine Learning & Data Science': {\n            'numpy', 'scipy', 'pandas', 'scikit-learn', 'torch', 'transformers',\n            'sentence-transformers', 'spacy', 'tiktoken', 'plotly', 'pydantic'\n        },\n        'Language Models & AI': {\n            'langchain', 'langchain-community', 'groq', 'autogen', 'pyautogen',\n            'mistralai'\n        },\n        'Web & API': {\n            'requests', 'httpx', 'beautifulsoup4', 'bs4', 'fake-useragent', 'flask',\n            'Flask-Cors', 'duckduckgo-search', 'google-api-python-client',\n            'serpapi', 'selenium', 'webdriver-manager', 'playwright', 'gTTS'\n        },\n        'System & Utilities': {\n            'psutil', 'GPUtil', 'rich', 'tqdm', 'humanize', 'schedule', 'cursor',\n            'pydub', 'networkx', 'bleach'\n        },\n        'Document Processing': {\n            'PyPDF2', 'fpdf', 'pdfkit', 'reportlab', 'mdutils', 'Markdown'\n        },\n        'Development & Testing': {\n            'pytest', 'pytest-html', 'flake8', 'radon', 'ruff', 'Pygments', 'PyYAML'\n        }\n    }\n    \n    def get_installed_version(package_name):\n        \"\"\"Get the installed version of a package.\"\"\"\n        try:\n            return pkg_resources.get_distribution(package_name).version\n        except pkg_resources.DistributionNotFound:\n            return None\n    \n    def normalize_package_name(name):\n        \"\"\"Normalize package names to handle different formats.\"\"\"\n        return re.sub(r'[-_.]+', '-', name).lower()\n    \n    def get_package_category(package_name):\n        \"\"\"Determine the category of a package.\"\"\"\n        normalized_name = normalize_package_name(package_name)\n        for category, packages in PACKAGE_CATEGORIES.items():\n            if any(normalize_package_name(pkg) == normalized_name for pkg in packages):\n                return category\n        return 'Utilities'  # Default category\n    \n    def get_imports_from_pipreqs():\n        \"\"\"Use pipreqs to get imports from the codebase.\"\"\"\n        try:\n            # Create a temporary requirements file\n            temp_req_file = os.path.join(repo_path, 'temp_requirements.txt')\n            \n            # Run pipreqs\n            subprocess.run([\n                'pipreqs',\n                '--savepath', temp_req_file,\n                '--force',\n                '--ignore', ','.join(exclude_patterns),\n                repo_path\n            ], capture_output=True)\n            \n            # Read the requirements\n            with open(temp_req_file, 'r') as f:\n                requirements = [line.strip() for line in f if line.strip()]\n            \n            # Clean up\n            os.remove(temp_req_file)\n            return requirements\n        except Exception as e:\n            print(f\"Error running pipreqs: {str(e)}\")\n            return []\n    \n    def get_installed_packages():\n        \"\"\"Get all installed packages in the current environment.\"\"\"\n        return {pkg.key: pkg.version for pkg in pkg_resources.working_set}\n    \n    # Get dependencies using multiple methods\n    requirements_dict = defaultdict(set)\n    \n    # Method 1: Use pipreqs to find imports\n    pipreqs_requirements = get_imports_from_pipreqs()\n    for req in pipreqs_requirements:\n        # Parse package name and version\n        match = re.match(r'([^>=<]+).*', req)\n        if match:\n            package_name = match.group(1).strip()\n            version = get_installed_version(package_name)\n            if version:\n                category = get_package_category(package_name)\n                requirements_dict[category].add(f'{package_name}=={version}')\n    \n    # Method 2: Check installed packages against our known packages\n    installed_packages = get_installed_packages()\n    for category, packages in PACKAGE_CATEGORIES.items():\n        for package in packages:\n            normalized_name = normalize_package_name(package)\n            if normalized_name in installed_packages:\n                requirements_dict[category].add(\n                    f'{package}=={installed_packages[normalized_name]}'\n                )\n    \n    # Write requirements.txt with categorized sections\n    requirements_path = os.path.join(repo_path, 'requirements.txt')\n    with open(requirements_path, 'w', encoding='utf-8') as req_file:\n        req_file.write('# Generated by Ollama Workbench Repository Analyzer\\n')\n        req_file.write('# This file contains verified package dependencies\\n\\n')\n        \n        # Write requirements in category order\n        for category in PACKAGE_CATEGORIES.keys():\n            packages = requirements_dict[category]\n            if packages:\n                req_file.write(f'# {category}\\n')\n                for package in sorted(packages):\n                    req_file.write(f'{package}\\n')\n                req_file.write('\\n')\n        \n        # Write any remaining packages in Utilities\n        other_packages = requirements_dict['Utilities']\n        if other_packages:\n            req_file.write('# Utilities\\n')\n            for package in sorted(other_packages):\n                req_file.write(f'{package}\\n')\n            req_file.write('\\n')\n    \n    return requirements_path\n\ndef generate_project_summary(repo_path, exclude_patterns):\n    # Create 'files' directory if it doesn't exist (for other outputs)\n    files_dir = os.path.join(repo_path, 'files')\n    os.makedirs(files_dir, exist_ok=True)\n\n    # Summary file path in the repository root\n    summary_path = os.path.join(repo_path, 'project_summary.md')\n    with open(summary_path, 'w', encoding='utf-8') as summary_file:\n        summary_file.write(\"--- START OF FILE project_summary.md ---\\n\\n\")\n        summary_file.write(\"# Table of Contents\\n\")\n\n        # Get all files and directories, excluding those matching the exclude patterns\n        all_files = []\n        for root, dirs, files in os.walk(repo_path):\n            dirs[:] = [d for d in dirs if d != 'files' and not any(re.search(pattern, os.path.join(root, d)) for pattern in exclude_patterns)]\n            files[:] = [f for f in files if not any(re.search(pattern, os.path.join(root, f)) for pattern in exclude_patterns)]\n            for file in files:\n                all_files.append(os.path.join(root, file))\n\n        # 1. Write README.md first\n        readme_path = os.path.join(repo_path, \"README.md\")\n        if readme_path in all_files:\n            summary_file.write(f\"- {readme_path}\\n\")\n            all_files.remove(readme_path)\n\n        # 2. Write requirements.txt second\n        requirements_path = os.path.join(repo_path, \"requirements.txt\")\n        if requirements_path in all_files:\n            summary_file.write(f\"- {requirements_path}\\n\")\n            all_files.remove(requirements_path)\n\n        # 3. Write the rest of the files in alphabetical order\n        all_files.sort()\n        for file_path in all_files:\n            summary_file.write(f\"- {file_path}\\n\")\n\n        summary_file.write(\"\\n\")\n\n        # Write file details (including README.md and requirements.txt)\n        for file_path in [readme_path, requirements_path] + all_files:\n            file_info = get_file_info(file_path)\n            if file_info:\n                write_file_details(summary_file, file_info)\n\n        summary_file.write(\"--- END OF FILE project_summary.md ---\")\n    return summary_path\n\ndef write_file_details(summary_file, file_info):\n    \"\"\"Writes the file details to the summary file.\"\"\"\n    summary_file.write(f\"## File: {file_info['Full Path']}\\n\\n\")\n    for key, value in file_info.items():\n        if key != \"Full Path\":\n            if key == \"Size\":\n                summary_file.write(f\"- {key}: {value} bytes\\n\")\n            elif key == \"Language\":\n                summary_file.write(f\"- {key}: {value}\\n\")\n                # Include content for all code and text-based file types\n                if value in (\"python\", \"php\", \"javascript\", \"css\", \"plaintext\", \"markdown\", \"bash\", \"csv\", \"json\", \"html\", \"xml\", \"yaml\", \"sql\"):\n                    summary_file.write(f\"### Code\\n\\n\")\n                    summary_file.write(f\"```{value}\\n{file_info.get('Code', 'Code not available')}\\n```\\n\\n\")\n            elif key == \"Complexity\":  # Write complexity metrics for Python files\n                summary_file.write(f\"- {key}:\\n\")\n                for metric_name, metric_value in value.items():\n                    if isinstance(metric_value, list):  # For Cyclomatic Complexity\n                        summary_file.write(f\"    - {metric_name}:\\n\")\n                        for item in metric_value:\n                            summary_file.write(f\"        - {item}\\n\")\n                    elif isinstance(metric_value, dict):  # For Halstead Metrics\n                        summary_file.write(f\"    - {metric_name}:\\n\")\n                        for sub_metric, sub_value in metric_value.items():\n                            summary_file.write(f\"        - {sub_metric}: {sub_value}\\n\")\n                    else:  # For Maintainability Index\n                        summary_file.write(f\"    - {metric_name}: {metric_value}\\n\")\n            elif key == \"Style Violations\":  # Write style violations for Python files\n                summary_file.write(f\"- {key}:\\n\")\n                for violation in value:\n                    summary_file.write(f\"    - {violation}\\n\")\n            elif key == \"PHPStan Report\":  # Write PHPStan report for PHP files\n                summary_file.write(f\"- {key}:\\n```\\n{value}\\n```\\n\")\n            elif key == \"ESLint Report\":  # Write ESLint report for JS and CSS files\n                summary_file.write(f\"- {key}:\\n```\\n{value}\\n```\\n\")\n            else:\n                summary_file.write(f\"- {key}: {value}\\n\")\n    summary_file.write(\"\\n\")\n\n# Function to load model settings\ndef load_model_settings():\n    if os.path.exists(MODEL_SETTINGS_FILE):\n        try:\n            with open(MODEL_SETTINGS_FILE, \"r\") as f:\n                settings = json.load(f)\n        except json.JSONDecodeError:\n            st.warning(\"Invalid JSON in model settings file. Loading default settings.\")\n            settings = {}\n    else:\n        settings = {}\n\n    # Set default values if keys are missing\n    default_settings = {\n        \"model\": \"mistral:instruct\",\n        \"temperature\": 0.7,\n        \"max_tokens\": 4000,\n        \"api_key\": \"\"\n    }\n\n    for key, value in default_settings.items():\n        settings.setdefault(key, value)\n\n    return settings\n\n# Function to save model settings\ndef save_model_settings(settings):\n    with open(MODEL_SETTINGS_FILE, \"w\") as f:\n        json.dump(settings, f, indent=4)\n\ndef main():\n    # Load model settings\n    model_settings = load_model_settings()\n    api_keys = load_api_keys()\n    \n    st.title(\"🔍 Repository Analyzer\")\n    st.write(\"Enter the path to your repository in the box below. Choose the task type (documentation, debug, readme, requirements, or project_summary) from the dropdown menu. Select the desired Ollama model for the task. Adjust the temperature and max tokens using the sliders. Click 'Analyze Repository' to begin. Once complete, a PDF report will be saved in the repository's 'files' folder. If you chose the 'readme' task type, a README.md file will also be created in the repository's 'files' folder. If you chose the 'project_summary' task type, a project_summary.md file will be created in the repository's 'files' folder.\")\n    \n    col1, col2 = st.columns(2)\n    with col1:\n        task_type = st.selectbox(\"Select task type\", [\"project_summary\", \"documentation\", \"debug\", \"readme\", \"requirements\"])\n    with col2:\n        repo_path = st.text_input(\"Enter the path to your repository:\")\n\n    # Input for exclude patterns\n    exclude_patterns_str = st.text_input(\"Enter file/folder patterns to exclude (comma-separated, use regex):\", \n                                        value=\".pythonlibs,.cache,.git,node_modules,__pycache__,cli,.*\\.pkl,tmp,.*\\.bin,.*\\.sqlite3,.*\\.db,.DS_Store,files,venv,.*\\.ipynb,notebooks,ragtest,LICENSE,checkpoints,.*\\.pdf,.*\\.png,.*\\.jpg,.*\\.jpeg,.*\\.gif,.*\\.csv,.*\\.docx,.*\\.zip,.*\\.eml,.*\\.json,.*\\.svg,.*\\.vue,.*\\.ogg,.*\\.eot,.*\\.ttf,.*\\.ico,.*\\.otf,.*\\.woff,.*\\.woff2,chroma_db,.pytest_cache,project_summary.md,agent_prompts,docs,__init__.py\")\n    exclude_patterns = [pattern.strip() for pattern in exclude_patterns_str.split(\",\")]\n\n    # Model Settings in a collapsed section in the sidebar\n    with st.sidebar:\n        with st.expander(\"🤖 Model Settings\", expanded=False):\n            # Model selection\n            available_models = get_available_models()\n            model_settings[\"model\"] = st.selectbox(\"Select Model\", available_models, index=available_models.index(model_settings[\"model\"]) if model_settings[\"model\"] in available_models else 0)\n\n            # API Key input (for OpenAI and Groq models)\n            if model_settings[\"model\"] in OPENAI_MODELS or model_settings[\"model\"] in GROQ_MODELS:\n                model_settings[\"api_key\"] = st.text_input(\"API Key\", value=model_settings.get(\"api_key\", \"\"), type=\"password\")\n\n            # Temperature slider\n            model_settings[\"temperature\"] = st.slider(\"Temperature\", min_value=0.0, max_value=1.0, value=model_settings[\"temperature\"], step=0.1)\n\n            # Max Tokens slider\n            model_settings[\"max_tokens\"] = st.slider(\"Max Tokens\", min_value=1000, max_value=128000, value=model_settings[\"max_tokens\"], step=1000)\n\n            # Save Settings button\n            if st.button(\"💾 Save Settings\"):\n                save_model_settings(model_settings)\n                st.success(\"Model settings saved!\")\n\n    if st.button(\"🔍 Analyze Repository\"):\n        if not repo_path or not os.path.isdir(repo_path):\n            st.error(\"Please enter a valid repository path.\")\n            return\n\n        # Create 'files' directory if it doesn't exist\n        files_dir = os.path.join(repo_path, 'files')\n        os.makedirs(files_dir, exist_ok=True)\n\n        if task_type == \"requirements\":\n            requirements_path = generate_requirements_file(repo_path, exclude_patterns)\n            st.success(f\"requirements.txt file has been created at {requirements_path}\")\n            return\n        elif task_type == \"project_summary\":\n            summary_path = generate_project_summary(repo_path, exclude_patterns)\n            st.success(f\"project_summary.md file has been created at {summary_path}\")\n            return\n\n        code_files = get_all_code_files(repo_path, exclude_patterns)\n\n        if not code_files:\n            st.warning(\"No code files found in the specified directory.\")\n            return\n\n        results = []\n        progress_bar = st.progress(0)\n        status_text = st.empty()\n        output_area = st.empty()\n\n        update_queue = queue.Queue()\n\n        def update_ui():\n            while True:\n                try:\n                    update_type, content = update_queue.get(block=False)\n                    if update_type == \"status\":\n                        status_text.text(content)\n                    elif update_type == \"output\":\n                        output_area.text(content)\n                    update_queue.task_done()\n                except queue.Empty:\n                    break\n\n        with ThreadPoolExecutor(max_workers=1) as executor:\n            api_keys = load_api_keys()\n            futures = {executor.submit(\n                process_file_with_updates, \n                file_path, \n                task_type, \n                model_settings[\"model\"], \n                model_settings[\"temperature\"], \n                model_settings[\"max_tokens\"], \n                model_settings.get(\"api_key\"),  \n                update_queue, \n                progress_bar, \n                status_text, \n                output_area\n            ): file_path for file_path in code_files}\n            \n            for i, future in enumerate(as_completed(futures)):               \n                file_path, documentation, pylint_report, file_content = future.result()\n                results.append((file_path, documentation, pylint_report, file_content))\n                progress = (i + 1) / len(code_files)\n                progress_bar.progress(progress)\n                update_ui()\n        progress_bar.empty()\n        status_text.empty()\n\n        # Generate and save PDF report in the 'files' folder\n        pdf_filename = f\"repository_{task_type}_report.pdf\"\n        pdf_path = os.path.join(files_dir, pdf_filename)\n        generate_pdf(results, pdf_path, task_type)\n        \n        st.success(f\"Analysis complete! PDF report saved as {pdf_filename} in the repository's 'files' folder.\")\n\n        if task_type == \"readme\":\n            # Analyze repository structure\n            repo_info = analyze_repository_structure(repo_path, code_files)\n            \n            # Report README status\n            if repo_info['existing_readme']:\n                st.info(f\"Found existing README.md at: {os.path.relpath(repo_info['existing_readme_path'], repo_path)}\")\n                st.write(\"Will update the existing README while preserving its structure.\")\n            else:\n                st.warning(\"No existing README.md found. Will create a new one based on repository analysis.\")\n            \n            # Process the entire repository as one\n            future = executor.submit(\n                process_file_with_updates,\n                \"README.md\",  # Just a placeholder filename\n                task_type,\n                model_settings[\"model\"],\n                model_settings[\"temperature\"],\n                model_settings[\"max_tokens\"],\n                model_settings.get(\"api_key\"),\n                update_queue,\n                progress_bar,\n                status_text,\n                output_area,\n                repo_info  # Pass repository information to the generation function\n            )\n            \n            _, readme_content, _, _ = future.result()\n            \n            # Save the README\n            if repo_info['existing_readme_path']:\n                # Create backup of existing README\n                backup_path = repo_info['existing_readme_path'] + '.backup'\n                try:\n                    with open(repo_info['existing_readme_path'], 'r', encoding='utf-8') as src:\n                        with open(backup_path, 'w', encoding='utf-8') as dst:\n                            dst.write(src.read())\n                    st.info(f\"Created backup of existing README at: {os.path.relpath(backup_path, repo_path)}\")\n                except Exception as e:\n                    st.warning(f\"Failed to create backup of existing README: {str(e)}\")\n                \n                # Update existing README\n                readme_path = repo_info['existing_readme_path']\n            else:\n                # Create new README in repository root\n                readme_path = os.path.join(repo_path, 'README.md')\n            \n            with open(readme_path, 'w', encoding='utf-8') as readme_file:\n                readme_file.write(readme_content)\n            \n            st.success(f\"README.md has been {'updated' if repo_info['existing_readme'] else 'created'} at: {os.path.relpath(readme_path, repo_path)}\")\n            st.markdown(\"## Generated README.md\")\n            st.markdown(readme_content)\n            return\n\nif __name__ == \"__main__\":\n    main()"}
