{"repo_info": {"repo_name": "DB-GPT", "repo_owner": "TsinghuaDatabaseGroup", "repo_url": "https://github.com/TsinghuaDatabaseGroup/DB-GPT"}}
{"type": "test_file", "path": "tests/agent/test_agent_function.py", "content": "import sys\nimport os\nsys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))\nfrom configs import LLM_MODELS, TEMPERATURE\nfrom server.utils import get_ChatOpenAI\nfrom langchain.chains import LLMChain\nfrom langchain.agents import LLMSingleActionAgent, AgentExecutor\nfrom server.agent.tools import tools, tool_names\nfrom langchain.memory import ConversationBufferWindowMemory\n\nmemory = ConversationBufferWindowMemory(k=5)\nmodel = get_ChatOpenAI(\n        model_name=LLM_MODELS[0],\n        temperature=TEMPERATURE,\n    )\nfrom server.agent.custom_template import CustomOutputParser, prompt\n\noutput_parser = CustomOutputParser()\nllm_chain = LLMChain(llm=model, prompt=prompt)\nagent = LLMSingleActionAgent(\n    llm_chain=llm_chain,\n    output_parser=output_parser,\n    stop=[\"\\nObservation:\"],\n    allowed_tools=tool_names\n)\n\nagent_executor = AgentExecutor.from_agent_and_tools(agent=agent, tools=tools, memory=memory, verbose=True)\n\nimport pytest\n@pytest.mark.parametrize(\"text_prompt\",\n                         [\"北京市朝阳区未来24小时天气如何？\",  # 天气功能函数\n                          \"计算 (2 + 2312312)/4 是多少？\", # 计算功能函数\n                          \"翻译这句话成中文：Life is the art of drawing sufficient conclusions form insufficient premises.\"] # 翻译功能函数\n)\ndef test_different_agent_function(text_prompt):\n    try:\n        text_answer = agent_executor.run(text_prompt)\n        assert text_answer is not None\n    except Exception as e:\n        pytest.fail(f\"agent_function failed with {text_prompt}, error: {str(e)}\")\n"}
{"type": "test_file", "path": "tests/api/test_kb_summary_api.py", "content": "import requests\nimport json\nimport sys\nfrom pathlib import Path\n\nroot_path = Path(__file__).parent.parent.parent\nsys.path.append(str(root_path))\nfrom server.utils import api_address\n\napi_base_url = api_address()\n\nkb = \"samples\"\nfile_name = \"/media/gpt4-pdf-chatbot-langchain/langchain-ChatGLM/knowledge_base/samples/content/llm/大模型技术栈-实战与应用.md\"\ndoc_ids = [\n    \"357d580f-fdf7-495c-b58b-595a398284e8\",\n    \"c7338773-2e83-4671-b237-1ad20335b0f0\",\n    \"6da613d1-327d-466f-8c1a-b32e6f461f47\"\n]\n\n\ndef test_summary_file_to_vector_store(api=\"/knowledge_base/kb_summary_api/summary_file_to_vector_store\"):\n    url = api_base_url + api\n    print(\"\\n文件摘要：\")\n    r = requests.post(url, json={\"knowledge_base_name\": kb,\n                                 \"file_name\": file_name\n                                 }, stream=True)\n    for chunk in r.iter_content(None):\n        data = json.loads(chunk)\n        assert isinstance(data, dict)\n        assert data[\"code\"] == 200\n        print(data[\"msg\"])\n\n\ndef test_summary_doc_ids_to_vector_store(api=\"/knowledge_base/kb_summary_api/summary_doc_ids_to_vector_store\"):\n    url = api_base_url + api\n    print(\"\\n文件摘要：\")\n    r = requests.post(url, json={\"knowledge_base_name\": kb,\n                                 \"doc_ids\": doc_ids\n                                 }, stream=True)\n    for chunk in r.iter_content(None):\n        data = json.loads(chunk)\n        assert isinstance(data, dict)\n        assert data[\"code\"] == 200\n        print(data)\n"}
{"type": "test_file", "path": "tests/api/test_stream_chat_api.py", "content": "import requests\nimport json\nimport sys\nfrom pathlib import Path\n\nsys.path.append(str(Path(__file__).parent.parent.parent))\nfrom configs import BING_SUBSCRIPTION_KEY\nfrom server.utils import api_address\n\nfrom pprint import pprint\n\n\napi_base_url = api_address()\n\n\ndef dump_input(d, title):\n    print(\"\\n\")\n    print(\"=\" * 30 + title + \"  input \" + \"=\"*30)\n    pprint(d)\n\n\ndef dump_output(r, title):\n    print(\"\\n\")\n    print(\"=\" * 30 + title + \"  output\" + \"=\"*30)\n    for line in r.iter_content(None, decode_unicode=True):\n        print(line, end=\"\", flush=True)\n\n\nheaders = {\n    'accept': 'application/json',\n    'Content-Type': 'application/json',\n}\n\ndata = {\n    \"query\": \"请用100字左右的文字介绍自己\",\n    \"history\": [\n        {\n            \"role\": \"user\",\n            \"content\": \"你好\"\n        },\n        {\n            \"role\": \"assistant\",\n            \"content\": \"你好，我是人工智能大模型\"\n        }\n    ],\n    \"stream\": True,\n    \"temperature\": 0.7,\n}\n\n\ndef test_chat_fastchat(api=\"/chat/fastchat\"):\n    url = f\"{api_base_url}{api}\"\n    data2 = {\n        \"stream\": True,\n        \"messages\": data[\"history\"] + [{\"role\": \"user\", \"content\": \"推荐一部科幻电影\"}]\n    }\n    dump_input(data2, api)\n    response = requests.post(url, headers=headers, json=data2, stream=True)\n    dump_output(response, api)\n    assert response.status_code == 200\n\n\ndef test_chat_chat(api=\"/chat/chat\"):\n    url = f\"{api_base_url}{api}\"\n    dump_input(data, api)\n    response = requests.post(url, headers=headers, json=data, stream=True)\n    dump_output(response, api)\n    assert response.status_code == 200\n\n\ndef test_knowledge_chat(api=\"/chat/knowledge_base_chat\"):\n    url = f\"{api_base_url}{api}\"\n    data = {\n        \"query\": \"如何提问以获得高质量答案\",\n        \"knowledge_base_name\": \"samples\",\n        \"history\": [\n            {\n                \"role\": \"user\",\n                \"content\": \"你好\"\n            },\n            {\n                \"role\": \"assistant\",\n                \"content\": \"你好，我是 ChatGLM\"\n            }\n        ],\n        \"stream\": True\n    }\n    dump_input(data, api)\n    response = requests.post(url, headers=headers, json=data, stream=True)\n    print(\"\\n\")\n    print(\"=\" * 30 + api + \"  output\" + \"=\"*30)\n    for line in response.iter_content(None, decode_unicode=True):\n        data = json.loads(line)\n        if \"answer\" in data:\n            print(data[\"answer\"], end=\"\", flush=True)\n    pprint(data)\n    assert \"docs\" in data and len(data[\"docs\"]) > 0\n    assert response.status_code == 200\n\n\ndef test_search_engine_chat(api=\"/chat/search_engine_chat\"):\n    global data\n\n    data[\"query\"] = \"室温超导最新进展是什么样？\"\n\n    url = f\"{api_base_url}{api}\"\n    for se in [\"bing\", \"duckduckgo\"]:\n        data[\"search_engine_name\"] = se\n        dump_input(data, api + f\" by {se}\")\n        response = requests.post(url, json=data, stream=True)\n        if se == \"bing\" and not BING_SUBSCRIPTION_KEY:\n            data = response.json()\n            assert data[\"code\"] == 404\n            assert data[\"msg\"] == f\"要使用Bing搜索引擎，需要设置 `BING_SUBSCRIPTION_KEY`\"\n\n        print(\"\\n\")\n        print(\"=\" * 30 + api + f\" by {se}  output\" + \"=\"*30)\n        for line in response.iter_content(None, decode_unicode=True):\n            data = json.loads(line)\n            if \"answer\" in data:\n                print(data[\"answer\"], end=\"\", flush=True)\n        assert \"docs\" in data and len(data[\"docs\"]) > 0\n        pprint(data[\"docs\"])\n        assert response.status_code == 200\n\n"}
{"type": "test_file", "path": "tests/api/test_kb_api.py", "content": "import requests\nimport json\nimport sys\nfrom pathlib import Path\n\nroot_path = Path(__file__).parent.parent.parent\nsys.path.append(str(root_path))\nfrom server.utils import api_address\nfrom configs import VECTOR_SEARCH_TOP_K\nfrom server.knowledge_base.utils import get_kb_path, get_file_path\n\nfrom pprint import pprint\n\n\napi_base_url = api_address()\n\n\nkb = \"kb_for_api_test\"\ntest_files = {\n    \"wiki/Home.MD\": get_file_path(\"samples\", \"wiki/Home.md\"),\n    \"wiki/开发环境部署.MD\": get_file_path(\"samples\", \"wiki/开发环境部署.md\"),\n    \"test_files/test.txt\": get_file_path(\"samples\", \"test_files/test.txt\"),\n}\n\nprint(\"\\n\\n直接url访问\\n\")\n\n\ndef test_delete_kb_before(api=\"/knowledge_base/delete_knowledge_base\"):\n    if not Path(get_kb_path(kb)).exists():\n        return\n\n    url = api_base_url + api\n    print(\"\\n测试知识库存在，需要删除\")\n    r = requests.post(url, json=kb)\n    data = r.json()\n    pprint(data)\n\n    # check kb not exists anymore\n    url = api_base_url + \"/knowledge_base/list_knowledge_bases\"\n    print(\"\\n获取知识库列表：\")\n    r = requests.get(url)\n    data = r.json()\n    pprint(data)\n    assert data[\"code\"] == 200\n    assert isinstance(data[\"data\"], list) and len(data[\"data\"]) > 0\n    assert kb not in data[\"data\"]\n\n\ndef test_create_kb(api=\"/knowledge_base/create_knowledge_base\"):\n    url = api_base_url + api\n\n    print(f\"\\n尝试用空名称创建知识库：\")\n    r = requests.post(url, json={\"knowledge_base_name\": \" \"})\n    data = r.json()\n    pprint(data)\n    assert data[\"code\"] == 404\n    assert data[\"msg\"] == \"知识库名称不能为空，请重新填写知识库名称\"\n\n    print(f\"\\n创建新知识库： {kb}\")\n    r = requests.post(url, json={\"knowledge_base_name\": kb})\n    data = r.json()\n    pprint(data)\n    assert data[\"code\"] == 200\n    assert data[\"msg\"] == f\"已新增知识库 {kb}\"\n\n    print(f\"\\n尝试创建同名知识库： {kb}\")\n    r = requests.post(url, json={\"knowledge_base_name\": kb})\n    data = r.json()\n    pprint(data)\n    assert data[\"code\"] == 404\n    assert data[\"msg\"] == f\"已存在同名知识库 {kb}\"\n\n\ndef test_list_kbs(api=\"/knowledge_base/list_knowledge_bases\"):\n    url = api_base_url + api\n    print(\"\\n获取知识库列表：\")\n    r = requests.get(url)\n    data = r.json()\n    pprint(data)\n    assert data[\"code\"] == 200\n    assert isinstance(data[\"data\"], list) and len(data[\"data\"]) > 0\n    assert kb in data[\"data\"]\n\n\ndef test_upload_docs(api=\"/knowledge_base/upload_docs\"):\n    url = api_base_url + api\n    files = [(\"files\", (name, open(path, \"rb\"))) for name, path in test_files.items()]\n\n    print(f\"\\n上传知识文件\")\n    data = {\"knowledge_base_name\": kb, \"override\": True}\n    r = requests.post(url, data=data, files=files)\n    data = r.json()\n    pprint(data)\n    assert data[\"code\"] == 200\n    assert len(data[\"data\"][\"failed_files\"]) == 0\n\n    print(f\"\\n尝试重新上传知识文件， 不覆盖\")\n    data = {\"knowledge_base_name\": kb, \"override\": False}\n    files = [(\"files\", (name, open(path, \"rb\"))) for name, path in test_files.items()]\n    r = requests.post(url, data=data, files=files)\n    data = r.json()\n    pprint(data)\n    assert data[\"code\"] == 200\n    assert len(data[\"data\"][\"failed_files\"]) == len(test_files)\n\n    print(f\"\\n尝试重新上传知识文件， 覆盖，自定义docs\")\n    docs = {\"FAQ.MD\": [{\"page_content\": \"custom docs\", \"metadata\": {}}]}\n    data = {\"knowledge_base_name\": kb, \"override\": True, \"docs\": json.dumps(docs)}\n    files = [(\"files\", (name, open(path, \"rb\"))) for name, path in test_files.items()]\n    r = requests.post(url, data=data, files=files)\n    data = r.json()\n    pprint(data)\n    assert data[\"code\"] == 200\n    assert len(data[\"data\"][\"failed_files\"]) == 0\n\n\ndef test_list_files(api=\"/knowledge_base/list_files\"):\n    url = api_base_url + api\n    print(\"\\n获取知识库中文件列表：\")\n    r = requests.get(url, params={\"knowledge_base_name\": kb})\n    data = r.json()\n    pprint(data)\n    assert data[\"code\"] == 200\n    assert isinstance(data[\"data\"], list)\n    for name in test_files:\n        assert name in data[\"data\"]\n\n\ndef test_search_docs(api=\"/knowledge_base/search_docs\"):\n    url = api_base_url + api\n    query = \"介绍一下langchain-chatchat项目\"\n    print(\"\\n检索知识库：\")\n    print(query)\n    r = requests.post(url, json={\"knowledge_base_name\": kb, \"query\": query})\n    data = r.json()\n    pprint(data)\n    assert isinstance(data, list) and len(data) == VECTOR_SEARCH_TOP_K\n\n\ndef test_update_info(api=\"/knowledge_base/update_info\"):\n    url = api_base_url + api\n    print(\"\\n更新知识库介绍\")\n    r = requests.post(url, json={\"knowledge_base_name\": \"samples\", \"kb_info\": \"你好\"})\n    data = r.json()\n    pprint(data)\n    assert data[\"code\"] == 200\n\ndef test_update_docs(api=\"/knowledge_base/update_docs\"):\n    url = api_base_url + api\n\n    print(f\"\\n更新知识文件\")\n    r = requests.post(url, json={\"knowledge_base_name\": kb, \"file_names\": list(test_files)})\n    data = r.json()\n    pprint(data)\n    assert data[\"code\"] == 200\n    assert len(data[\"data\"][\"failed_files\"]) == 0\n\n\ndef test_delete_docs(api=\"/knowledge_base/delete_docs\"):\n    url = api_base_url + api\n\n    print(f\"\\n删除知识文件\")\n    r = requests.post(url, json={\"knowledge_base_name\": kb, \"file_names\": list(test_files)})\n    data = r.json()\n    pprint(data)\n    assert data[\"code\"] == 200\n    assert len(data[\"data\"][\"failed_files\"]) == 0\n\n    url = api_base_url + \"/knowledge_base/search_docs\"\n    query = \"介绍一下langchain-chatchat项目\"\n    print(\"\\n尝试检索删除后的检索知识库：\")\n    print(query)\n    r = requests.post(url, json={\"knowledge_base_name\": kb, \"query\": query})\n    data = r.json()\n    pprint(data)\n    assert isinstance(data, list) and len(data) == 0\n\n\ndef test_recreate_vs(api=\"/knowledge_base/recreate_vector_store\"):\n    url = api_base_url + api\n    print(\"\\n重建知识库：\")\n    r = requests.post(url, json={\"knowledge_base_name\": kb}, stream=True)\n    for chunk in r.iter_content(None):\n        data = json.loads(chunk)\n        assert isinstance(data, dict)\n        assert data[\"code\"] == 200\n        print(data[\"msg\"])\n\n    url = api_base_url + \"/knowledge_base/search_docs\"\n    query = \"本项目支持哪些文件格式?\"\n    print(\"\\n尝试检索重建后的检索知识库：\")\n    print(query)\n    r = requests.post(url, json={\"knowledge_base_name\": kb, \"query\": query})\n    data = r.json()\n    pprint(data)\n    assert isinstance(data, list) and len(data) == VECTOR_SEARCH_TOP_K\n\n\ndef test_delete_kb_after(api=\"/knowledge_base/delete_knowledge_base\"):\n    url = api_base_url + api\n    print(\"\\n删除知识库\")\n    r = requests.post(url, json=kb)\n    data = r.json()\n    pprint(data)\n\n    # check kb not exists anymore\n    url = api_base_url + \"/knowledge_base/list_knowledge_bases\"\n    print(\"\\n获取知识库列表：\")\n    r = requests.get(url)\n    data = r.json()\n    pprint(data)\n    assert data[\"code\"] == 200\n    assert isinstance(data[\"data\"], list) and len(data[\"data\"]) > 0\n    assert kb not in data[\"data\"]\n"}
{"type": "test_file", "path": "tests/api/test_llm_api.py", "content": "import requests\nimport json\nimport sys\nfrom pathlib import Path\n\nroot_path = Path(__file__).parent.parent.parent\nsys.path.append(str(root_path))\nfrom configs.server_config import FSCHAT_MODEL_WORKERS\nfrom server.utils import api_address, get_model_worker_config\n\nfrom pprint import pprint\nimport random\nfrom typing import List\n\n\ndef get_configured_models() -> List[str]:\n    model_workers = list(FSCHAT_MODEL_WORKERS)\n    if \"default\" in model_workers:\n        model_workers.remove(\"default\")\n    return model_workers\n\n\napi_base_url = api_address()\n\n\ndef get_running_models(api=\"/llm_model/list_models\"):\n    url = api_base_url + api\n    r = requests.post(url)\n    if r.status_code == 200:\n        return r.json()[\"data\"]\n    return []\n\n\ndef test_running_models(api=\"/llm_model/list_running_models\"):\n    url = api_base_url + api\n    r = requests.post(url)\n    assert r.status_code == 200\n    print(\"\\n获取当前正在运行的模型列表：\")\n    pprint(r.json())\n    assert isinstance(r.json()[\"data\"], list)\n    assert len(r.json()[\"data\"]) > 0\n\n\n# 不建议使用stop_model功能。按现在的实现，停止了就只能手动再启动\n# def test_stop_model(api=\"/llm_model/stop\"):\n#     url = api_base_url + api\n#     r = requests.post(url, json={\"\"})\n\n\ndef test_change_model(api=\"/llm_model/change_model\"):\n    url = api_base_url + api\n\n    running_models = get_running_models()\n    assert len(running_models) > 0\n\n    model_workers = get_configured_models()\n\n    availabel_new_models = list(set(model_workers) - set(running_models))\n    assert len(availabel_new_models) > 0\n    print(availabel_new_models)\n\n    local_models = [x for x in running_models if not get_model_worker_config(x).get(\"online_api\")]\n    model_name = random.choice(local_models)\n    new_model_name = random.choice(availabel_new_models)\n    print(f\"\\n尝试将模型从 {model_name} 切换到 {new_model_name}\")\n    r = requests.post(url, json={\"model_name\": model_name, \"new_model_name\": new_model_name})\n    assert r.status_code == 200\n\n    running_models = get_running_models()\n    assert new_model_name in running_models\n"}
{"type": "test_file", "path": "tests/custom_splitter/test_different_splitter.py", "content": "import os\n\nfrom transformers import AutoTokenizer\nimport sys\n\nsys.path.append(\"../..\")\nfrom configs import (\n    CHUNK_SIZE,\n    OVERLAP_SIZE\n)\n\nfrom server.knowledge_base.utils import make_text_splitter\n\ndef text(splitter_name):\n    from langchain import document_loaders\n\n    # 使用DocumentLoader读取文件\n    filepath = \"../../knowledge_base/samples/content/test.txt\"\n    loader = document_loaders.UnstructuredFileLoader(filepath, autodetect_encoding=True)\n    docs = loader.load()\n    text_splitter = make_text_splitter(splitter_name, CHUNK_SIZE, OVERLAP_SIZE)\n    if splitter_name == \"MarkdownHeaderTextSplitter\":\n        docs = text_splitter.split_text(docs[0].page_content)\n        for doc in docs:\n            if doc.metadata:\n                doc.metadata[\"source\"] = os.path.basename(filepath)\n    else:\n        docs = text_splitter.split_documents(docs)\n    for doc in docs:\n        print(doc)\n    return docs\n\n\n\n\nimport pytest\nfrom langchain.docstore.document import Document\n\n@pytest.mark.parametrize(\"splitter_name\",\n                         [\n                             \"ChineseRecursiveTextSplitter\",\n                             \"SpacyTextSplitter\",\n                             \"RecursiveCharacterTextSplitter\",\n                             \"MarkdownHeaderTextSplitter\"\n                         ])\ndef test_different_splitter(splitter_name):\n    try:\n        docs = text(splitter_name)\n        assert isinstance(docs, list)\n        if len(docs)>0:\n            assert isinstance(docs[0], Document)\n    except Exception as e:\n        pytest.fail(f\"test_different_splitter failed with {splitter_name}, error: {str(e)}\")\n"}
{"type": "test_file", "path": "tests/api/test_server_state_api.py", "content": "import sys\nfrom pathlib import Path\nroot_path = Path(__file__).parent.parent.parent\nsys.path.append(str(root_path))\n\nfrom webui_pages.utils import ApiRequest\n\nimport pytest\nfrom pprint import pprint\nfrom typing import List\n\n\napi = ApiRequest()\n\n\ndef test_get_default_llm():\n    llm = api.get_default_llm_model()\n    \n    print(llm)\n    assert isinstance(llm, tuple)\n    assert isinstance(llm[0], str) and isinstance(llm[1], bool)\n\n\ndef test_server_configs():\n    configs = api.get_server_configs()\n    pprint(configs, depth=2)\n\n    assert isinstance(configs, dict)\n    assert len(configs) > 0\n\n\ndef test_list_search_engines():\n    engines = api.list_search_engines()\n    pprint(engines)\n\n    assert isinstance(engines, list)\n    assert len(engines) > 0\n\n\n@pytest.mark.parametrize(\"type\", [\"llm_chat\", \"agent_chat\"])\ndef test_get_prompt_template(type):\n    print(f\"prompt template for: {type}\")\n    template = api.get_prompt_template(type=type)\n\n    print(template)\n    assert isinstance(template, str)\n    assert len(template) > 0\n"}
{"type": "test_file", "path": "tests/api/test_kb_api_request.py", "content": "import requests\nimport json\nimport sys\nfrom pathlib import Path\n\nroot_path = Path(__file__).parent.parent.parent\nsys.path.append(str(root_path))\nfrom server.utils import api_address\nfrom configs import VECTOR_SEARCH_TOP_K\nfrom server.knowledge_base.utils import get_kb_path, get_file_path\nfrom webui_pages.utils import ApiRequest\n\nfrom pprint import pprint\n\n\napi_base_url = api_address()\napi: ApiRequest = ApiRequest(api_base_url)\n\n\nkb = \"kb_for_api_test\"\ntest_files = {\n    \"FAQ.MD\": str(root_path / \"docs\" / \"FAQ.MD\"),\n    \"README.MD\": str(root_path / \"README.MD\"),\n    \"test.txt\": get_file_path(\"samples\", \"test.txt\"),\n}\n\nprint(\"\\n\\nApiRquest调用\\n\")\n\n\ndef test_delete_kb_before():\n    if not Path(get_kb_path(kb)).exists():\n        return\n\n    data = api.delete_knowledge_base(kb)\n    pprint(data)\n    assert data[\"code\"] == 200\n    assert isinstance(data[\"data\"], list) and len(data[\"data\"]) > 0\n    assert kb not in data[\"data\"]\n\n\ndef test_create_kb():\n    print(f\"\\n尝试用空名称创建知识库：\")\n    data = api.create_knowledge_base(\" \")\n    pprint(data)\n    assert data[\"code\"] == 404\n    assert data[\"msg\"] == \"知识库名称不能为空，请重新填写知识库名称\"\n\n    print(f\"\\n创建新知识库： {kb}\")\n    data = api.create_knowledge_base(kb)\n    pprint(data)\n    assert data[\"code\"] == 200\n    assert data[\"msg\"] == f\"已新增知识库 {kb}\"\n\n    print(f\"\\n尝试创建同名知识库： {kb}\")\n    data = api.create_knowledge_base(kb)\n    pprint(data)\n    assert data[\"code\"] == 404\n    assert data[\"msg\"] == f\"已存在同名知识库 {kb}\"\n\n\ndef test_list_kbs():\n    data = api.list_knowledge_bases()\n    pprint(data)\n    assert isinstance(data, list) and len(data) > 0\n    assert kb in data\n\n\ndef test_upload_docs():\n    files = list(test_files.values())\n\n    print(f\"\\n上传知识文件\")\n    data = {\"knowledge_base_name\": kb, \"override\": True}\n    data = api.upload_kb_docs(files, **data)\n    pprint(data)\n    assert data[\"code\"] == 200\n    assert len(data[\"data\"][\"failed_files\"]) == 0\n\n    print(f\"\\n尝试重新上传知识文件， 不覆盖\")\n    data = {\"knowledge_base_name\": kb, \"override\": False}\n    data = api.upload_kb_docs(files, **data)\n    pprint(data)\n    assert data[\"code\"] == 200\n    assert len(data[\"data\"][\"failed_files\"]) == len(test_files)\n\n    print(f\"\\n尝试重新上传知识文件， 覆盖，自定义docs\")\n    docs = {\"FAQ.MD\": [{\"page_content\": \"custom docs\", \"metadata\": {}}]}\n    data = {\"knowledge_base_name\": kb, \"override\": True, \"docs\": docs}\n    data = api.upload_kb_docs(files, **data)\n    pprint(data)\n    assert data[\"code\"] == 200\n    assert len(data[\"data\"][\"failed_files\"]) == 0\n\n\ndef test_list_files():\n    print(\"\\n获取知识库中文件列表：\")\n    data = api.list_kb_docs(knowledge_base_name=kb)\n    pprint(data)\n    assert isinstance(data, list)\n    for name in test_files:\n        assert name in data\n\n\ndef test_search_docs():\n    query = \"介绍一下langchain-chatchat项目\"\n    print(\"\\n检索知识库：\")\n    print(query)\n    data = api.search_kb_docs(query, kb)\n    pprint(data)\n    assert isinstance(data, list) and len(data) == VECTOR_SEARCH_TOP_K\n\n\ndef test_update_docs():\n    print(f\"\\n更新知识文件\")\n    data = api.update_kb_docs(knowledge_base_name=kb, file_names=list(test_files))\n    pprint(data)\n    assert data[\"code\"] == 200\n    assert len(data[\"data\"][\"failed_files\"]) == 0\n\n\ndef test_delete_docs():\n    print(f\"\\n删除知识文件\")\n    data = api.delete_kb_docs(knowledge_base_name=kb, file_names=list(test_files))\n    pprint(data)\n    assert data[\"code\"] == 200\n    assert len(data[\"data\"][\"failed_files\"]) == 0\n\n    query = \"介绍一下langchain-chatchat项目\"\n    print(\"\\n尝试检索删除后的检索知识库：\")\n    print(query)\n    data = api.search_kb_docs(query, kb)\n    pprint(data)\n    assert isinstance(data, list) and len(data) == 0\n\n\ndef test_recreate_vs():\n    print(\"\\n重建知识库：\")\n    r = api.recreate_vector_store(kb)\n    for data in r:\n        assert isinstance(data, dict)\n        assert data[\"code\"] == 200\n        print(data[\"msg\"])\n\n    query = \"本项目支持哪些文件格式?\"\n    print(\"\\n尝试检索重建后的检索知识库：\")\n    print(query)\n    data = api.search_kb_docs(query, kb)\n    pprint(data)\n    assert isinstance(data, list) and len(data) == VECTOR_SEARCH_TOP_K\n\n\ndef test_delete_kb_after():\n    print(\"\\n删除知识库\")\n    data = api.delete_knowledge_base(kb)\n    pprint(data)\n\n    # check kb not exists anymore\n    print(\"\\n获取知识库列表：\")\n    data = api.list_knowledge_bases()\n    pprint(data)\n    assert isinstance(data, list) and len(data) > 0\n    assert kb not in data\n"}
{"type": "test_file", "path": "tests/api/test_stream_chat_api_thread.py", "content": "import requests\nimport json\nimport sys\nfrom pathlib import Path\n\nsys.path.append(str(Path(__file__).parent.parent.parent))\nfrom configs import BING_SUBSCRIPTION_KEY\nfrom server.utils import api_address\n\nfrom pprint import pprint\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\nimport time\n\n\napi_base_url = api_address()\n\n\ndef dump_input(d, title):\n    print(\"\\n\")\n    print(\"=\" * 30 + title + \"  input \" + \"=\"*30)\n    pprint(d)\n\n\ndef dump_output(r, title):\n    print(\"\\n\")\n    print(\"=\" * 30 + title + \"  output\" + \"=\"*30)\n    for line in r.iter_content(None, decode_unicode=True):\n        print(line, end=\"\", flush=True)\n\n\nheaders = {\n    'accept': 'application/json',\n    'Content-Type': 'application/json',\n}\n\n\ndef knowledge_chat(api=\"/chat/knowledge_base_chat\"):\n    url = f\"{api_base_url}{api}\"\n    data = {\n        \"query\": \"如何提问以获得高质量答案\",\n        \"knowledge_base_name\": \"samples\",\n        \"history\": [\n            {\n                \"role\": \"user\",\n                \"content\": \"你好\"\n            },\n            {\n                \"role\": \"assistant\",\n                \"content\": \"你好，我是 ChatGLM\"\n            }\n        ],\n        \"stream\": True\n    }\n    result = []\n    response = requests.post(url, headers=headers, json=data, stream=True)\n\n    for line in response.iter_content(None, decode_unicode=True):\n        data = json.loads(line)\n        result.append(data)\n    \n    return result\n\n\ndef test_thread():\n    threads = []\n    times = []\n    pool = ThreadPoolExecutor()\n    start = time.time()\n    for i in range(10):\n        t = pool.submit(knowledge_chat)\n        threads.append(t)\n    \n    for r in as_completed(threads):\n        end = time.time()\n        times.append(end - start)\n        print(\"\\nResult:\\n\")\n        pprint(r.result())\n\n    print(\"\\nTime used:\\n\")\n    for x in times:\n        print(f\"{x}\")\n"}
{"type": "test_file", "path": "tests/document_loader/test_imgloader.py", "content": "import sys\nfrom pathlib import Path\n\nroot_path = Path(__file__).parent.parent.parent\nsys.path.append(str(root_path))\nfrom pprint import pprint\n\ntest_files = {\n    \"ocr_test.jpg\": str(root_path / \"tests\" / \"samples\" / \"ocr_test.jpg\"),\n}\n\ndef test_rapidocrloader():\n    img_path = test_files[\"ocr_test.jpg\"]\n    from document_loaders import RapidOCRLoader\n\n    loader = RapidOCRLoader(img_path)\n    docs = loader.load()\n    pprint(docs)\n    assert isinstance(docs, list) and len(docs) > 0 and isinstance(docs[0].page_content, str)\n\n\n"}
{"type": "test_file", "path": "tests/kb_vector_db/__init__.py", "content": ""}
{"type": "test_file", "path": "tests/document_loader/test_pdfloader.py", "content": "import sys\nfrom pathlib import Path\n\nroot_path = Path(__file__).parent.parent.parent\nsys.path.append(str(root_path))\nfrom pprint import pprint\n\ntest_files = {\n    \"ocr_test.pdf\": str(root_path / \"tests\" / \"samples\" / \"ocr_test.pdf\"),\n}\n\ndef test_rapidocrpdfloader():\n    pdf_path = test_files[\"ocr_test.pdf\"]\n    from document_loaders import RapidOCRPDFLoader\n\n    loader = RapidOCRPDFLoader(pdf_path)\n    docs = loader.load()\n    pprint(docs)\n    assert isinstance(docs, list) and len(docs) > 0 and isinstance(docs[0].page_content, str)\n\n\n"}
{"type": "test_file", "path": "tests/test_migrate.py", "content": "from pathlib import Path\nfrom pprint import pprint\nimport os\nimport shutil\nimport sys\nroot_path = Path(__file__).parent.parent\nsys.path.append(str(root_path))\n\nfrom server.knowledge_base.kb_service.base import KBServiceFactory\nfrom server.knowledge_base.utils import get_kb_path, get_doc_path, KnowledgeFile\nfrom server.knowledge_base.migrate import folder2db, prune_db_docs, prune_folder_files\n\n\n# setup test knowledge base\nkb_name = \"test_kb_for_migrate\"\ntest_files = {\n    \"faq.md\": str(root_path / \"docs\" / \"faq.md\"),\n    \"install.md\": str(root_path / \"docs\" / \"install.md\"),\n}\n\n\nkb_path = get_kb_path(kb_name)\ndoc_path = get_doc_path(kb_name)\n\nif not os.path.isdir(doc_path):\n    os.makedirs(doc_path)\n\nfor k, v in test_files.items():\n    shutil.copy(v, os.path.join(doc_path, k))\n\n\ndef test_recreate_vs():\n    folder2db([kb_name], \"recreate_vs\")\n\n    kb = KBServiceFactory.get_service_by_name(kb_name)\n    assert kb and kb.exists()\n\n    files = kb.list_files()\n    print(files)\n    for name in test_files:\n        assert name in files\n        path = os.path.join(doc_path, name)\n\n        # list docs based on file name\n        docs = kb.list_docs(file_name=name)\n        assert len(docs) > 0\n        pprint(docs[0])\n        for doc in docs:\n            assert doc.metadata[\"source\"] == name\n\n        # list docs base on metadata\n        docs = kb.list_docs(metadata={\"source\": name})\n        assert len(docs) > 0\n\n        for doc in docs:\n            assert doc.metadata[\"source\"] == name\n\n\ndef test_increament():\n    kb = KBServiceFactory.get_service_by_name(kb_name)\n    kb.clear_vs()\n    assert kb.list_files() == []\n    assert kb.list_docs() == []\n\n    folder2db([kb_name], \"increament\")\n\n    files = kb.list_files()\n    print(files)\n    for f in test_files:\n        assert f in files\n\n        docs = kb.list_docs(file_name=f)\n        assert len(docs) > 0\n        pprint(docs[0])\n\n        for doc in docs:\n            assert doc.metadata[\"source\"] == f\n\n\ndef test_prune_db():\n    del_file, keep_file = list(test_files)[:2]\n    os.remove(os.path.join(doc_path, del_file))\n\n    prune_db_docs([kb_name])\n\n    kb = KBServiceFactory.get_service_by_name(kb_name)\n    files = kb.list_files()\n    print(files)\n    assert del_file not in files\n    assert keep_file in files\n\n    docs = kb.list_docs(file_name=del_file)\n    assert len(docs) == 0\n\n    docs = kb.list_docs(file_name=keep_file)\n    assert len(docs) > 0\n    pprint(docs[0])\n\n    shutil.copy(test_files[del_file], os.path.join(doc_path, del_file))\n\n\ndef test_prune_folder():\n    del_file, keep_file = list(test_files)[:2]\n    kb = KBServiceFactory.get_service_by_name(kb_name)\n\n    # delete docs for file\n    kb.delete_doc(KnowledgeFile(del_file, kb_name))\n    files = kb.list_files()\n    print(files)\n    assert del_file not in files\n    assert keep_file in files\n\n    docs = kb.list_docs(file_name=del_file)\n    assert len(docs) == 0\n\n    docs = kb.list_docs(file_name=keep_file)\n    assert len(docs) > 0\n\n    docs = kb.list_docs(file_name=del_file)\n    assert len(docs) == 0\n\n    assert os.path.isfile(os.path.join(doc_path, del_file))\n\n    # prune folder\n    prune_folder_files([kb_name])\n\n    # check result\n    assert not os.path.isfile(os.path.join(doc_path, del_file))\n    assert os.path.isfile(os.path.join(doc_path, keep_file))\n\n\ndef test_drop_kb():\n    kb = KBServiceFactory.get_service_by_name(kb_name)\n    kb.drop_kb()\n    assert not kb.exists()\n    assert not os.path.isdir(kb_path)\n\n    kb = KBServiceFactory.get_service_by_name(kb_name)\n    assert kb is None\n"}
{"type": "test_file", "path": "tests/test_online_api.py", "content": "import sys\nfrom pathlib import Path\nroot_path = Path(__file__).parent.parent\nsys.path.append(str(root_path))\n\nfrom configs import ONLINE_LLM_MODEL\nfrom server.model_workers.base import *\nfrom server.utils import get_model_worker_config, list_config_llm_models\nfrom pprint import pprint\nimport pytest\n\n\nworkers = []\nfor x in list_config_llm_models()[\"online\"]:\n    if x in ONLINE_LLM_MODEL and x not in workers:\n        workers.append(x)\nprint(f\"all workers to test: {workers}\")\n\n# workers = [\"fangzhou-api\"]\n\n\n@pytest.mark.parametrize(\"worker\", workers)\ndef test_chat(worker):\n    params = ApiChatParams(\n        messages = [\n            {\"role\": \"user\", \"content\": \"你是谁\"},\n        ],\n    )\n    print(f\"\\nchat with {worker} \\n\")\n\n    if worker_class := get_model_worker_config(worker).get(\"worker_class\"):\n        for x in worker_class().do_chat(params):\n            pprint(x)\n            assert isinstance(x, dict)\n            assert x[\"error_code\"] == 0\n\n\n@pytest.mark.parametrize(\"worker\", workers)\ndef test_embeddings(worker):\n    params = ApiEmbeddingsParams(\n        texts = [\n            \"LangChain-Chatchat (原 Langchain-ChatGLM): 基于 Langchain 与 ChatGLM 等大语言模型的本地知识库问答应用实现。\",\n            \"一种利用 langchain 思想实现的基于本地知识库的问答应用，目标期望建立一套对中文场景与开源模型支持友好、可离线运行的知识库问答解决方案。\",\n        ]\n    )\n\n    if worker_class := get_model_worker_config(worker).get(\"worker_class\"):\n        if worker_class.can_embedding():\n            print(f\"\\embeddings with {worker} \\n\")\n            resp = worker_class().do_embeddings(params)\n\n            pprint(resp, depth=2)\n            assert resp[\"code\"] == 200\n            assert \"data\" in resp\n            embeddings = resp[\"data\"]\n            assert isinstance(embeddings, list) and len(embeddings) > 0\n            assert isinstance(embeddings[0], list) and len(embeddings[0]) > 0\n            assert isinstance(embeddings[0][0], float)\n            print(\"向量长度：\", len(embeddings[0]))\n\n\n# @pytest.mark.parametrize(\"worker\", workers)\n# def test_completion(worker):\n#     params = ApiCompletionParams(prompt=\"五十六个民族\")\n    \n#     print(f\"\\completion with {worker} \\n\")\n\n#     worker_class = get_model_worker_config(worker)[\"worker_class\"]\n#     resp = worker_class().do_completion(params)\n#     pprint(resp)\n"}
{"type": "test_file", "path": "tests/kb_vector_db/test_milvus_db.py", "content": "from server.knowledge_base.kb_service.faiss_kb_service import FaissKBService\nfrom server.knowledge_base.kb_service.milvus_kb_service import MilvusKBService\nfrom server.knowledge_base.kb_service.pg_kb_service import PGKBService\nfrom server.knowledge_base.migrate import create_tables\nfrom server.knowledge_base.utils import KnowledgeFile\n\nkbService = MilvusKBService(\"test\")\n\ntest_kb_name = \"test\"\ntest_file_name = \"README.md\"\ntestKnowledgeFile = KnowledgeFile(test_file_name, test_kb_name)\nsearch_content = \"如何启动api服务\"\n\ndef test_init():\n    create_tables()\n\n\ndef test_create_db():\n    assert kbService.create_kb()\n\n\ndef test_add_doc():\n    assert kbService.add_doc(testKnowledgeFile)\n\n\ndef test_search_db():\n    result = kbService.search_docs(search_content)\n    assert len(result) > 0\ndef test_delete_doc():\n    assert kbService.delete_doc(testKnowledgeFile)\n\n"}
{"type": "test_file", "path": "tests/kb_vector_db/test_pg_db.py", "content": "from server.knowledge_base.kb_service.faiss_kb_service import FaissKBService\nfrom server.knowledge_base.kb_service.pg_kb_service import PGKBService\nfrom server.knowledge_base.migrate import create_tables\nfrom server.knowledge_base.utils import KnowledgeFile\n\nkbService = PGKBService(\"test\")\n\ntest_kb_name = \"test\"\ntest_file_name = \"README.md\"\ntestKnowledgeFile = KnowledgeFile(test_file_name, test_kb_name)\nsearch_content = \"如何启动api服务\"\n\n\ndef test_init():\n    create_tables()\n\n\ndef test_create_db():\n    assert kbService.create_kb()\n\n\ndef test_add_doc():\n    assert kbService.add_doc(testKnowledgeFile)\n\n\ndef test_search_db():\n    result = kbService.search_docs(search_content)\n    assert len(result) > 0\ndef test_delete_doc():\n    assert kbService.delete_doc(testKnowledgeFile)\n\n"}
{"type": "test_file", "path": "tests/kb_vector_db/test_faiss_kb.py", "content": "from server.knowledge_base.kb_service.faiss_kb_service import FaissKBService\nfrom server.knowledge_base.migrate import create_tables\nfrom server.knowledge_base.utils import KnowledgeFile\n\n\nkbService = FaissKBService(\"test\")\ntest_kb_name = \"test\"\ntest_file_name = \"README.md\"\ntestKnowledgeFile = KnowledgeFile(test_file_name, test_kb_name)\nsearch_content = \"如何启动api服务\"\n\n\ndef test_init():\n    create_tables()\n\n\ndef test_create_db():\n    assert kbService.create_kb()\n\n\ndef test_add_doc():\n    assert kbService.add_doc(testKnowledgeFile)\n\n\ndef test_search_db():\n    result = kbService.search_docs(search_content)\n    assert len(result) > 0\n\n\ndef test_delete_doc():\n    assert kbService.delete_doc(testKnowledgeFile)\n\n\ndef test_delete_db():\n    assert kbService.drop_kb()\n"}
{"type": "source_file", "path": "anomalies/add_slow_queries.py", "content": "import json\nfrom datetime import datetime\nimport re\nimport os\n\n\ndef obtain_slow_queries(diag_start_time, diag_end_time):\n    # # TODO 暂时不读取文件内容\n    # return []\n\n    try:\n        server_dir = \"/var/lib/pgsql/12/data/pg_log\"\n        # Get a list of files in the directory\n        files = os.listdir(server_dir)\n\n        # Read the contents of each file\n        slow_sqls = []\n        for filename in files:\n            if '.log' not in filename:\n                continue\n            filepath = server_dir + '/' + filename\n            \n            with open(filepath, 'r') as file:\n                try:\n                    out_date = False\n                    for line in file:\n                        \n                        if out_date == True:\n                            break\n\n                        # if the line contains the required time range\n                        if \"[]LOG:\" not in line and \"[postgres]\" not in line:\n                            words = line.split('statement: ')\n\n                            if len(words) < 2:\n                                continue\n\n                            sql = words[-1].strip().lower()\n\n                            words = words[0].split(' ')\n\n                            if len(words) < 2:\n                                continue\n\n                            sql_start_time = words[0] + ' ' + words[1]\n                            \n                            # convert the string type timestamp into value in seconds\n                            try:\n                                timestamp = datetime.strptime(sql_start_time, '%Y-%m-%d %H:%M:%S.%f')\n                                sql_start_time = (timestamp - datetime(1970, 1, 1)).total_seconds()\n\n                                if (diag_start_time <= sql_start_time and sql_start_time <= diag_end_time):\n                                    for i, word in enumerate(words):\n                                        if word == \"duration:\":\n                                            # reserve two digits\n                                            execution_seconds = round(float(words[i + 1])/1000, 2)\n                                            if \"select\" in sql:\n                                                sql = sql.strip().replace(\"\\n\", \"\").replace(\"\\t\", \"\")\n                                                dbname = re.findall(r'\\[([^]]+)\\]', words[4])[0]\n                                                slow_sqls.append({\"sql\": sql, \"dbname\": dbname, \"execution_time\": str(execution_seconds)+'s'})\n                                            break\n                                elif sql_start_time > diag_end_time:\n                                    out_date = True\n                                    break\n                            except ValueError as e:\n                                #print(f\"ValueError: {e}\")\n                                pass\n\n                except UnicodeDecodeError as e:\n                    #print(f\"UnicodeDecodeError: {e}\")\n                    pass\n\n    finally:\n        pass\n    \n    return slow_sqls\n\n# /var/lib/pgsql/12/data/pg_log\nwith open(\"testing_set_without_scripts.json\", 'r') as f:\n    anomaly_blocks = json.load(f)\n\nfor anomaly_id in anomaly_blocks:\n    # get the start  time and end time\n    anomaly_block = anomaly_blocks[anomaly_id]\n    diag_start_time = int(anomaly_block[\"start_time\"])\n    diag_end_time = int(anomaly_block[\"end_time\"])\n    if \"slow_queries\" not in anomaly_block:\n        anomaly_blocks[anomaly_id]['slow_queries'] = []\n\n    slow_queries = obtain_slow_queries(diag_start_time, diag_end_time)\n    if slow_queries != []:\n        \n        anomaly_blocks[anomaly_id]['slow_queries'].append(slow_queries)\n\nwith open(\"testing_set_without_scripts.json\", 'w') as f:\n    f.write(json.dumps(anomaly_blocks, indent=4))\n"}
{"type": "source_file", "path": "anomaly_trigger/DBException.py", "content": "import subprocess\nimport time\n\n\"\"\"\n# 安装sysbench\ncurl -s https://packagecloud.io/install/repositories/akopytov/sysbench/script.rpm.sh | sudo bash\n\nyum install sysbench\n\n\"\"\"\n\nTIMELOGPATH = str(int(time.time())) + \"_trigger_time_log.txt\"\nTIMELOG = open(TIMELOGPATH, 'w+')\n\nclass DBException:\n\n    def trigger_sysbench_exception(self):\n        \"\"\"触发异常\"\"\"\n        self.run_shell_cmd([\n            'sysbench --db-driver=pgsql --threads=90 --tables=2 --pgsql-host=xxxx --pgsql-user=xxxx --pgsql-password=xxxx --pgsql-port=5432 --pgsql-db=sysbench --time=60 --rand-type=uniform --table_size=10000000 oltp_read_only run'\n        ])\n\n    def run_shell_cmd(self, target_sql_list):\n        cmd = \"\"\n        for target_sql in target_sql_list:\n            cmd += target_sql + '; '\n\n        TIMELOG.write(str(int(time.time()))+\";\")\n        proc = subprocess.Popen(\n            cmd,\n            stdin=subprocess.PIPE,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            shell=True)\n        (stdout, stderr) = proc.communicate()\n        stdout, stderr = stdout.decode('utf-8'), stderr.decode()\n        if 'FATAL:' in stderr or 'failed to connect' in stderr:\n            print(\n                \"An error occurred while connecting to the database.\\n\" +\n                \"Details: \" +\n                stderr)\n        print(stdout)\n        TIMELOG.write(str(int(time.time()))+\"\\n\")\n        TIMELOG.flush()\n        return stdout\n\n\nif __name__ == '__main__':\n    db_exception = DBException()\n\n    # 开启异常负载\n    for i in range(5):\n        db_exception.trigger_sysbench_exception()\n\n    TIMELOG.close()\n"}
{"type": "source_file", "path": "anomaly_trigger/benchmark_job.py", "content": "import os\nimport re\nimport time\n\nimport psycopg2\n\n\nREPEATCOUNT = 1\nTIMELOGPATH = str(int(time.time())) + \"_job_trigger_time_log.txt\"\nTIMELOG = open(TIMELOGPATH, 'w+')\n\n\nclass Database():\n\n    def __init__(self):\n        self.conn = None\n        self.conn = psycopg2.connect(database='imdbload',\n                                     user='xxxx',\n                                     password='xxxx',\n                                     host='xxxx',\n                                     port=5432)\n\n    def execute_sql(self, sql):\n        fail = 1\n        cur = self.conn.cursor()\n        i = 0\n        cnt = 3\n        while fail == 1 and i < cnt:\n            try:\n                fail = 0\n                cur.execute(sql)\n            except BaseException as error:\n                fail = 1\n                print(error)\n            res = []\n            if fail == 0:\n                res = cur.fetchall()\n            i = i + 1\n        if fail == 1:\n            # print(\"SQL Execution Fatal!!\", sql)\n            return 0, ''\n        elif fail == 0:\n            return 1, res\n\n\ndef all_sql_files():\n    res_path = \"{}/join-order-benchmark-master/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    # all_file_list = list(filter(file_filter, os.listdir(res_path)))\n    # all_file_list = sorted(all_file_list, key=custom_sort)\n    all_file_list = [\n        '1a.sql', '1b.sql', '1c.sql', '1d.sql',\n        '2a.sql', '2b.sql', '2c.sql', '2d.sql',\n        '3a.sql', '3b.sql', '3c.sql',\n        '4a.sql', '4b.sql', '4c.sql',\n        '5a.sql', '5b.sql', '5c.sql',\n        '6a.sql', '6b.sql', '6c.sql', '6d.sql', '6e.sql', '6f.sql',\n        '7a.sql', '7b.sql', '7c.sql',\n        '8a.sql', '8b.sql', '8c.sql', '8d.sql',\n        '9a.sql', '9b.sql', '9c.sql', '9d.sql',\n        '10a.sql', '10b.sql', '10c.sql',\n        '11a.sql', '11b.sql', '11c.sql', '11d.sql',\n        '12a.sql', '12b.sql', '12c.sql',\n        '13a.sql', '13b.sql', '13c.sql', '13d.sql',\n        '14a.sql', '14b.sql', '14c.sql',\n        '15a.sql', '15b.sql', '15c.sql', '15d.sql',\n        '16a.sql', '16b.sql', '16c.sql', '16d.sql',\n        '17a.sql', '17b.sql', '17c.sql', '17d.sql', '17e.sql', '17f.sql',\n        '18a.sql', '18b.sql', '18c.sql',\n        '19a.sql', '19b.sql', '19c.sql', '19d.sql',\n        '20a.sql', '20b.sql', '20c.sql',\n        '21a.sql', '21b.sql', '21c.sql',\n        '22a.sql', '22b.sql', '22c.sql', '22d.sql',\n        '23a.sql', '23b.sql', '23c.sql',\n        '24a.sql', '24b.sql',\n        '25a.sql', '25b.sql', '25c.sql',\n        '26a.sql', '26b.sql', '26c.sql',\n        '27a.sql', '27b.sql', '27c.sql',\n        '28a.sql', '28b.sql', '28c.sql',\n        '29a.sql', '29b.sql', '29c.sql',\n        '30a.sql', '30b.sql', '30c.sql',\n        '31a.sql', '31b.sql', '31c.sql',\n        '32a.sql', '32b.sql',\n        '33a.sql', '33b.sql', '33c.sql']\n\n    print(all_file_list)\n    files_list = []\n    for file in all_file_list:\n        files_list.append(res_path + file)\n    return files_list\n\n\ndef custom_sort(item):\n    # 提取数字和字母部分\n    match = re.match(r'(\\d+)(\\D+)', item)\n    # 将数字部分转换为整数以进行比较\n    num_part = int(match.group(1))\n    # 返回元组以按数字和字母排序\n    return (num_part, match.group(2))\n\n\ndef file_filter(f):\n    if f[-4:] == '.sql' and 'schema' not in f and 'fkindexes' not in f:\n        return True\n    else:\n        return False\n\n\ndef get_sql_from_file(file_name):\n    file = open(file_name)\n    lines = file.readlines().copy()\n    sql = ''\n    for line in lines:\n        sql += line\n    sql = sql.replace('\\n', ' ').replace('   ', ' ').replace('  ', ' ')\n    file.close()\n    return sql\n\n\ndef test_hint_from_file(sql_file):\n    db = Database()\n    sql = get_sql_from_file(sql_file)\n    success, result_cont = db.execute_sql(sql)\n    print(success, result_cont)\n\n\ndef test_all():\n    sql_files = all_sql_files()\n\n    for sql_file in list(sql_files)[:-10]:\n        if sql_file:\n            test_hint_from_file(sql_file)\n\n\ndef test_one():\n    res_path = \"{}/join-order-benchmark-master/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    test_hint_from_file(res_path + '1a.sql')\n\n\nif __name__ == '__main__':\n    for i in range(0, REPEATCOUNT):\n        TIMELOG.write(str(int(time.time()))+\";\")\n        test_all()\n        TIMELOG.write(str(int(time.time()))+\"\\n\")\n        TIMELOG.flush()\n\n    TIMELOG.close()\n\n"}
{"type": "source_file", "path": "anomalies/process.py", "content": "import json\n\n# def extract_alert(alerts):\n\n#     alert_list = []\n#     for alert in alerts:\n#         alert_list.append(alert['alerts'][0]['labels']['alertname'])\n\n#     return str(alert_list)\n\n# with open(\"testing_set_with_workload_new.json\", 'r') as f:\n#     cases = json.load(f)\n\n# alerts = {}\n# for id in cases:\n#     # cases[id]\n#     new_alert = extract_alert(cases[id][\"alerts\"])\n#     if  new_alert not in alerts:\n#         alerts[new_alert] = [cases[id]]\n#     elif len(alerts[new_alert]) < 2:\n#         alerts[new_alert].append(cases[id])\n\n# alert_json = {}\n\n# num = 0\n# for alert in alerts:\n#     print(alert, len(alerts[alert]))\n#     for case in alerts[alert]:\n#         alert_json[str(num)] = case\n#         num += 1\n\n# with open(\"batch_testing_set.json\", 'w') as f:\n#     json.dump(alert_json, f)\n\nwith open(\"../test_cases/batch_testing_set.json\", 'r') as f:\n    dicts = json.load(f)\n\nfor dict in dicts:\n    print(dicts[dict][\"labels\"])"}
{"type": "source_file", "path": "anomaly_trigger/anomaly.py", "content": "import psycopg2\nimport sys\nfrom utils.database import DB_CONFIG, SERVER_CONFIG\nfrom utils.database import DBArgs,Database\nimport random\nimport os\nimport datetime\nimport yaml\nimport time\nimport paramiko\nfrom multiprocessing.pool import *\nimport promethues\n\ndef init():\n    # add the config\n    # config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    # with open(config_path, 'r') as config_file:\n    #     config = yaml.safe_load(config_file)\n    db_args = DBArgs(\"postgresql\", DB_CONFIG, application_name=\"anomaly\")\n    return db_args\n\n\ndef restart_init():\n    # add the config\n    db_args = DBArgs(\"postgresql\", DB_CONFIG, application_name=\"restart\")\n    return db_args\n\ndef restart():\n    db=Database(restart_init())\n    sql=\"SELECT pg_terminate_backend(pg_stat_activity.pid) FROM pg_stat_activity WHERE pg_stat_activity.application_name = 'anomaly';\"\n    db.execute_sqls(sql)\n\ndef restart_postgresql():\n\n    # 创建SSH客户端实例\n    ssh = paramiko.SSHClient()\n    ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n\n    try:\n        # 连接到远程服务器\n        ssh.connect(hostname=SERVER_CONFIG['host'], port=SERVER_CONFIG['port'], username=SERVER_CONFIG['user'], password=SERVER_CONFIG['password'])\n\n        # 执行PostgreSQL重启命令\n        # 注意：根据您服务器的配置，这些命令可能需要调整\n        stdin, stdout, stderr = ssh.exec_command(\"sudo systemctl restart postgresql-12.service\")\n        exit_status = stdout.channel.recv_exit_status()  # 阻塞直到命令执行完成\n\n        if exit_status == 0:\n            print(\"PostgreSQL服务已重启\")\n        else:\n            print(\"重启命令执行失败，错误信息：\", stderr.read().decode())\n    except Exception as e:\n        print(f\"SSH连接或命令执行出错：{e}\")\n    finally:\n        # 关闭SSH连接\n        ssh.close()\n\n# create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n# delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n# print the current time\ndef print_start_time(cmd):\n    log_file = open(\"dataset.txt\", \"a\")\n    current_time = datetime.datetime.now()\n    timestamp = current_time.timestamp()\n    inttimestamp=int(timestamp)\n    log_file.write(f\"{cmd} started at {inttimestamp}\\n\")\n    log_file.flush()\n    print(inttimestamp)\n\ndef print_end_time(cmd):\n    log_file = open(\"dataset.txt\", \"a\")\n    current_time = datetime.datetime.now()\n    timestamp = current_time.timestamp()\n    inttimestamp=int(timestamp)\n    log_file.write(f\"{cmd} ended at {inttimestamp}\\n\")\n    log_file.flush()\n    print(inttimestamp)\n\ndef write_amomaly_sql_to_file(text):\n    try:\n        with open('badsql.txt', 'a') as file:\n            file.write(f\"{text}\\n\")\n        print(\"文本已成功写入到badsql.txt文件中。\")\n    except Exception as e:\n        print(f\"写入文件时出现错误: {e}\")\n\ndef write_amomaly_sql_to_file_a_line(text):\n    try:\n        with open('badsql.txt', 'a') as file:\n            file.write(f\"{text}\\t\\t\")\n        print(\"文本已成功写入到badsql.txt文件中。\")\n    except Exception as e:\n        print(f\"写入文件时出现错误: {e}\")\n\ndef write_space():\n    try:\n        with open('badsql.txt', 'a') as file:\n            file.write(f\"\\n\")\n    except Exception as e:\n        print(f\"写入文件时出现错误: {e}\")\n\n'''insert_large_data'''\ndef insert_large_data(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    cmd=f\"python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA --threads {threads} --ncolumn {ncolumns} --nrow {nrows} --colsize {colsize}\"\n    #Delete undeleted tables\n    delete_table(table_name)\n    #create a new table\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    #insert the data\n    #insert_definitions = ', '.join(f'repeat(round(random()*999)::text,{(colsize//3)})' for i in range(ncolumns))\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    write_amomaly_sql_to_file(insert_data)\n    time.sleep(10)\n    print_start_time(cmd)\n    db.concurrent_execute_sql(threads,duration,insert_data,commit_interval=1)\n    print_end_time(cmd)\n    time.sleep(10)\n    #restaet the pg database\n    restart()\n    time.sleep(10)\n    cpu,mem=promethues.restart_decision()\n    if((cpu>50)|(mem>50)):\n        restart_postgresql()\n    \n    #delete the table\n    delete_table(table_name)\n\n\n'''missing_index'''\ndef missing_index(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    cmd=f\"python anomaly_trigger/main.py --anomaly MISSING_INDEXES --threads {threads} --ncolumn {ncolumns} --nrow {nrows} --colsize {colsize}\"\n    #create a new table\n    \n    db=Database(init())\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be selected \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.execute_sqls(insert_data)  \n\n    #select without the index\n    missing_index='select * from '+table_name+' where id='\n    write_amomaly_sql_to_file(missing_index)\n    time.sleep(10)\n    print_start_time(cmd)\n    db.concurrent_execute_sql(threads,duration,missing_index,nrows)\n    print_end_time(cmd)\n    time.sleep(10)\n    restart()\n    time.sleep(10)\n    #restaet the pg database\n    cpu,mem=promethues.restart_decision()\n    if((cpu>50)|(mem>50)):\n        restart_postgresql()\n    \n    #delete the table\n    delete_table(table_name)\n    #print the end time\n\n\n'''lock_contention'''\ndef lock_contention(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    cmd=f\"python anomaly_trigger/main.py --anomaly LOCK_CONTENTION --threads {threads} --ncolumn {ncolumns} --nrow {nrows} --colsize {colsize}\"\n    #create a new table\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    # insert some data to be updated \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n    pool = Pool(threads)\n    time.sleep(10)\n    print_start_time(cmd)\n    for _ in range(threads):\n        pool.apply_async(\n            lock, (table_name, ncolumns, colsize, duration, nrows))\n    pool.close()\n    pool.join()\n    print_end_time(cmd)\n    write_space()\n    time.sleep(10)\n    restart()\n    time.sleep(10)\n    #restaet the pg database\n    cpu,mem=promethues.restart_decision()\n    if((cpu>50)|(mem>50)):\n        restart_postgresql()\n    \n    #delete the table\n    delete_table(table_name)\n\n\n'''vacuum'''\ndef vacuum(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    cmd=f\"python anomaly_trigger/main.py --anomaly VACUUM --threads {threads} --ncolumn {ncolumns} --nrow {nrows} --colsize {colsize}\"\n    db=Database(init())\n    #create a new table\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be deleted\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n    time.sleep(10)\n    print_start_time(cmd)\n    # delete 80% of the rows\n    delete_nrows=int(nrows*0.8)\n    vacuum=f'delete from {table_name} where id < {delete_nrows};'\n    write_amomaly_sql_to_file(vacuum)\n    db.execute_sqls(vacuum)\n    print_end_time(cmd)\n    time.sleep(10)\n    # do the select , then the vacuum occurs\n    select='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,select,nrows)\n    time.sleep(10)\n    restart()\n    time.sleep(10)\n    #restaet the pg database\n    cpu,mem=promethues.restart_decision()\n    if((cpu>50)|(mem>50)):\n        restart_postgresql()\n    \n    #delete the table\n    delete_table(table_name)\n\n'''redundent_index'''\ndef redundent_index(threads,duration,ncolumns,nrows,colsize,nindex,table_name='table1'):\n    cmd=f\"python anomaly_trigger/main.py --anomaly REDUNDANT_INDEX --threads {threads} --ncolumn {ncolumns} --nrow {nrows} --colsize {colsize}\"\n    #create a new table\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    # insert some data to be updated \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n\n    #initialization of the indexes\n    nindex=int((nindex*ncolumns)/10)\n    db.build_index(table_name,nindex)\n    id_index='CREATE INDEX index_'+table_name+'_id ON '+table_name+'(id);'\n    db.execute_sqls(id_index)\n    time.sleep(10)\n    #lock_contention\n    print_start_time(cmd)\n    pool = Pool(threads)\n    for _ in range(threads):\n        pool.apply_async(\n            lock, (table_name, ncolumns, colsize, duration, nrows))\n    pool.close()\n    pool.join()\n    print_end_time(cmd)\n    time.sleep(10)\n    #drop the index\n    db.drop_index(table_name)\n    restart()\n    time.sleep(10)\n    #restaet the pg database\n    cpu,mem=promethues.restart_decision()\n    if((cpu>50)|(mem>50)):\n        restart_postgresql()\n    \n    #delete the table\n    delete_table(table_name)\n\n\n'''io_contention'''\ndef io_contention():\n    cmd=f\"python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA,IO_CONTENTION\"\n    print_start_time(cmd)\n    command = (\n    \"su - root -c 'cd /sysbench-tpcc-master; \"\n    \"./tpcc.lua --db-driver=pgsql --tables=2 --scale=3 --threads=50 --events=0 \"\n    \"--pgsql-host=xxx --pgsql-user=xxx --pgsql-password=xxx \"\n    \"--pgsql-port=xxxx --pgsql-db=tpcc --time=90 --rand-type=uniform --report-interval=10 run'\"\n    )\n    write_amomaly_sql_to_file(\"sysbench-tpcc to INSERT_LARGE_DATA, IO_CONTENTION\")\n    os.system(command)\n    print_end_time(cmd)\n    time.sleep(10)\n    restart()\n    time.sleep(10)\n    #restaet the pg database\n    cpu,mem=promethues.restart_decision()\n    if((cpu>50)|(mem>50)):\n        restart_postgresql()\n\n\n'''fetch_large_data'''\ndef fetch_large_data():\n    cmd=f\"python anomaly_trigger/main.py --anomaly FETCH_LARGE_DATA,CORRELATED_SUBQUERY\"\n    \n    try:\n        print_start_time(cmd)\n        os.system(\"python anomaly_trigger/benchmark_tpch.py\")\n        print_end_time(cmd)\n        write_amomaly_sql_to_file('''select o_orderpriority, count(*) as order_count from orders where o_orderdate >= date '1996-03-01' and o_orderdate < date '1996-03-01' + interval '3' month and exists ( select * from lineitem where l_orderkey = o_orderkey and l_commitdate < l_receiptdate ) group by o_orderpriority order by o_orderpriority LIMIT 1;''')\n        time.sleep(10)\n        restart()\n        time.sleep(10)\n    #restaet the pg database\n        cpu,mem=promethues.restart_decision()\n        if((cpu>50)|(mem>50)):\n            restart_postgresql()\n    \n    except Exception as e:\n        print(f\"exception: {e}\")\n\n\n'''cpu_contention'''\ndef cpu_contention():\n    cmd=f\"python anomaly_trigger/main.py --anomaly POOR_JOIN_PERFORMANCE,CPU_CONTENTION\"\n    try:\n        print_start_time(cmd)\n        os.system(\"python anomaly_trigger/benchmark_job.py\")\n        print_end_time(cmd)\n        write_amomaly_sql_to_file('''SELECT MIN(mc.note) AS production_note, MIN(t.title) AS movie_title,MIN(t.production_year) AS movie_year FROM company_type AS ct,info_type AS it,movie_companies AS mc,movie_info_idx AS mi_idx,title AS WHERE ct.kind = 'production companies'AND it.info = 'top 250 rank'AND mc.note NOT LIKE '%(as Metro-Goldwyn-Mayer Pictures)%' AND (mc.note LIKE '%(co-production)%'OR mc.note LIKE '%(presents)%') AND ct.id = mc.company_type_idAND t.id = mc.movie_id AND t.id = mi_idx.movie_id AND mc.movie_id = mi_idx.movie_id AND it.id = mi_idx.info_type_id;''')\n        time.sleep(10)\n        restart()\n        time.sleep(10)\n        #restaet the pg database\n        cpu,mem=promethues.restart_decision()\n        if((cpu>50)|(mem>50)):\n            restart_postgresql()\n        \n    except Exception as e:\n        print(f\"exception: {e}\")\n\n\ndef lock(table_name, ncolumns, colsize, duration, nrows):\n    args=init()\n    start = time.time()\n    #lock_contention\n    while time.time()-start < duration:\n        conn = psycopg2.connect(database=args.dbname, user=args.user, password=args.password,\n                                        host=args.host, port=args.port)\n        cur = conn.cursor()\n        #write_amomaly_sql_to_file(lock_contention)\n        while time.time()-start < duration:\n            col_name = random.randint(0, ncolumns-1)\n            row_name = random.randint(1, nrows-1)\n            lock_contention = f'update {table_name} set name{col_name}=(SELECT substr(md5(random()::text), 1, {colsize})) where id ={row_name}'\n            cur.execute(lock_contention)\n            conn.commit()\n        conn.commit()\n        conn.close()\n    write_amomaly_sql_to_file_a_line(lock_contention)\n"}
{"type": "source_file", "path": "anomaly_trigger/benchmark_tpch.py", "content": "import os\nimport re\nimport time\n\nimport psycopg2\n\n\nREPEATCOUNT = 1\nTIMELOGPATH = str(int(time.time())) + \"_tpch_trigger_time_log.txt\"\nTIMELOG = open(TIMELOGPATH, 'w+')\n\n\nclass Database():\n\n    def __init__(self):\n        self.conn = None\n        self.conn = psycopg2.connect(database='tpch',\n                                     user='xxxx',\n                                     password='xxxx',\n                                     host='xxxx',\n                                     port=5432)\n\n    def execute_sql(self, sql):\n        fail = 1\n        cur = self.conn.cursor()\n        i = 0\n        cnt = 3\n        while fail == 1 and i < cnt:\n            try:\n                fail = 0\n                cur.execute(sql)\n            except BaseException as error:\n                fail = 1\n                print(error)\n            res = []\n            if fail == 0:\n                res = cur.fetchall()\n            i = i + 1\n        if fail == 1:\n            # print(\"SQL Execution Fatal!!\", sql)\n            return 0, ''\n        elif fail == 0:\n            return 1, res\n\n\ndef all_sql_files():\n    res_path = \"{}/tpch-queries/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    # all_file_list = list(filter(file_filter, os.listdir(res_path)))\n    # all_file_list = sorted(all_file_list, key=custom_sort)\n    all_file_list = [\n        '4.explain.sql']\n\n    print(all_file_list)\n    files_list = []\n    for file in all_file_list:\n        files_list.append(res_path + file)\n    return files_list\n\n\ndef custom_sort(item):\n    # 提取数字和字母部分\n    match = re.match(r'(\\d+)(\\D+)', item)\n    # 将数字部分转换为整数以进行比较\n    num_part = int(match.group(1))\n    # 返回元组以按数字和字母排序\n    return (num_part, match.group(2))\n\n\ndef file_filter(f):\n    if f[-4:] == '.sql' and 'schema' not in f and 'fkindexes' not in f:\n        return True\n    else:\n        return False\n\n\ndef get_sql_from_file(file_name):\n    file = open(file_name)\n    lines = file.readlines().copy()\n    sql = ''\n    for line in lines:\n        sql += line\n    sql = sql.replace('\\n', ' ').replace('   ', ' ').replace('  ', ' ')\n    file.close()\n    return sql\n\n\ndef test_hint_from_file(sql_file):\n    db = Database()\n    sql = get_sql_from_file(sql_file)\n    success, result_cont = db.execute_sql(sql)\n    print(success, result_cont)\n\n\ndef test_all():\n    sql_files = all_sql_files()\n\n    for sql_file in list(sql_files):\n        if sql_file:\n            test_hint_from_file(sql_file)\n\n\ndef test_one():\n    res_path = \"{}/tpch-queries/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    test_hint_from_file(res_path + '1.explain.sql')\n\n\nif __name__ == '__main__':\n    for i in range(0, REPEATCOUNT):\n        TIMELOG.write(str(int(time.time()))+\";\")\n        test_all()\n        TIMELOG.write(str(int(time.time()))+\"\\n\")\n        TIMELOG.flush()\n\n    TIMELOG.close()\n\n"}
{"type": "source_file", "path": "anomalies/process_script.py", "content": "import json\n\n\nnew_dataset = {}\nwith open(\"testing_set/testing_set_with_workload.json\") as f:\n\n    data = json.load(f)\n\n    for key, value in data.items():\n        key_number = int(key)\n        # key_number mode 10 ~ [0, 4] \n        if key_number % 10 <=4:\n\n            alerts = value[\"alerts\"]\n            alert_info = []\n            for alert in alerts:\n                alert_info.append({\"alert_name\": alert[\"alerts\"][0]['labels']['alertname'], \"alert_level\": alert[\"alerts\"][0]['labels']['level'], \"alert_severity\": alert[\"alerts\"][0]['labels']['severity']})\n\n            new_dict = {\n                \"start_time\": value[\"start_time\"],\n                \"end_time\": value[\"end_time\"],\n                \"start_timestamp\": value[\"start_timestamp\"],\n                \"end_timestamp\": value[\"end_timestamp\"],\n                \"alerts\": alert_info,\n                \"workload\": value[\"workload\"],\n                \"dba_diag\": \"\"\n            }\n\n            new_dataset[key] = new_dict\n            \n\n# load dict in pretty format\nwith open(\"testing_set/testing_set_dba.json\", \"w\") as f:\n    json.dump(new_dataset, f, indent=4)"}
{"type": "source_file", "path": "anomaly_trigger/io.py", "content": "import os\nimport datetime\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\nif __name__ == \"__main__\":\n    print_time()\n    command = (\n    \"su - root -c 'cd /sysbench-tpcc-master; \"\n    \"./tpcc.lua --db-driver=pgsql --tables=2 --scale=3 --threads=50 --events=0 \"\n    \"--pgsql-host=xxxx --pgsql-user=xxxx --pgsql-password=xxxx \"\n    \"--pgsql-port=5432 --pgsql-db=tpcc --time=90 --rand-type=uniform --report-interval=10 run'\"\n    )\n\n    os.system(command)\n    print_time()"}
{"type": "source_file", "path": "anomaly_trigger/miss_multi.py", "content": "import psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n'''missing_index'''\ndef missing_index(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    db=Database(init())\n    # insert some data to be selected \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.execute_sqls(insert_data)  \n\n    #select without the index\n    missing_index='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,missing_index,nrows)\n\n    #print the end time\n    print_time()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 100\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = 60\n    \n    # Number of columns in the table\n    num_columns = 100\n    \n    # Number of rows to insert\n    num_rows = 37100\n    \n    # Size of each column (in characters)\n    column_size = 100\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n   \n    missing_index(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n "}
{"type": "source_file", "path": "anomaly_trigger/insert_large_data.py", "content": "import psycopg2\nimport sys\n\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\n'''insert_large_data'''\ndef insert_large_data(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n\n    print_time()\n    #Delete undeleted tables\n    delete_table(table_name)\n    #create a new table\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    #insert the data\n    #insert_definitions = ', '.join(f'repeat(round(random()*999)::text,{(colsize//3)})' for i in range(ncolumns))\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.concurrent_execute_sql(threads,duration,insert_data,commit_interval=1)\n\n    #delete the table\n    delete_table(table_name)\n    \n    #print the end time\n    print_time()\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 100\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = 60\n    \n    # Number of columns in the table\n    num_columns = 10\n    \n    # Number of rows to insert\n    num_rows = 100\n    \n    # Size of each column (in characters)\n    column_size = 200\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    insert_large_data(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n\n"}
{"type": "source_file", "path": "anomaly_trigger/createdatabase.py", "content": "import psycopg2\nfrom utils.database import DB_CONFIG, SERVER_CONFIG\n\ndef createdatabase(name):\n    # 连接到 PostgreSQL 服务器的默认数据库 \"postgres\"\n    conn = psycopg2.connect(\n        dbname=\"sysbench\",  # 连接到默认的 \"postgres\" 数据库\n        user=DB_CONFIG['user'],  # 替换为你的数据库用户名\n        password=DB_CONFIG['password'],  # 替换为你的数据库密码\n        host=DB_CONFIG['host'],  # 替换为你的数据库主机地址\n    )\n    conn.autocommit = True\n    # 创建一个数据库游标\n    cur = conn.cursor()\n\n    # 执行创建数据库的 SQL 语句\n    cur.execute(f\"CREATE DATABASE {name}\")\n\n    # 提交更改\n    conn.commit()\n\n    # 关闭游标和数据库连接\n    cur.close()\n    conn.close()\n"}
{"type": "source_file", "path": "anomaly_trigger/dataset_generation.py", "content": "import subprocess\nimport time\nimport random\nimport datetime\nimport createdatabase\nimport dropdatabase\n\ndef run_cmd(cmd_list):\n    try:\n        for cmd in cmd_list:\n            #current_datetime = datetime.datetime.now()\n            #timestamp = current_datetime.timestamp()\n            #inttimestamp=int(timestamp)\n            #log_file.write(f\"{cmd} started at {inttimestamp}\\n\")\n            #log_file.flush()\n            # 运行脚本\n            subprocess.run(cmd, shell=True)\n            # 记录结束时间\n            #current_datetime = datetime.datetime.now()\n            #end_time = datetime.datetime.strftime(current_datetime,\"%Y-%m-%d %H:%M:%S\")\n            #timestamp = current_datetime.timestamp()\n            #inttimestamp=int(timestamp)\n            #log_file.write(f\"{cmd} ended at {inttimestamp}\\n\")\n            #log_file.flush()\n            # 休息60秒钟\n            time.sleep(60)\n    except Exception as e:\n        print(f\"An error occurred: {str(e)}\")\n\n\n# 定义要运行的脚本文件\nscript = \"anomaly_trigger/main.py\"\n\n    # 循环500次\nfor i in range(500):\n        # 生成随机参数\n    Thread_1 = random.randint(50, 200)\n    Thread_2 = random.randint(50, 200)\n    Thread_3 = random.randint(50, 200)\n    Thread_4 = random.randint(50, 200)\n    Thread_5 = random.randint(50, 200)\n    ncolumns1=random.randint(5, 20)\n    ncolumns2=random.randint(20, 40)\n    ncolumns3=random.randint(50, 100)\n    ncolumns4=random.randint(5, 20)\n    ncolumns5=random.randint(50, 100)\n    ncolumns6=random.randint(50, 100)\n    nrows1=random.randint(50, 100)\n    nrows2=random.randint(50, 100)\n    nrows3=random.randint(200, 400)\n    nrows4=random.randint(2000000, 4000000)\n    nrows5=random.randint(20000, 40000)\n    nrows6=random.randint(200, 400)\n    colsize1=random.randint(20, 80)\n    colsize2=random.randint(50, 100)\n    colsize3=random.randint(50, 100)\n    colsize4=random.randint(50, 100)\n    colsize5=random.randint(50, 100)\n    colsize6=random.randint(50, 100)\n        \n        # 构建命令\n    #Highly concurrent commits   \n    cmd0 = f\"python {script} --anomaly INSERT_LARGE_DATA --threads {Thread_1} --ncolumn {ncolumns1} --colsize {colsize1} --nrow {nrows1} \"\n    #Highly concurrent inserts\n    cmd1 = f\"python {script} --anomaly INSERT_LARGE_DATA --threads {Thread_2} --ncolumn {ncolumns2} --colsize {colsize2} --nrow {nrows2}\"\n    #Lock waits\n    cmd2 = f\"python {script} --anomaly LOCK_CONTENTION --threads {Thread_3} --ncolumn {ncolumns3} --colsize {colsize3} --nrow {nrows3}\"\n    #Vacuum\n    cmd3 = f\"python {script} --anomaly VACUUM --threads {Thread_4} --ncolumn {ncolumns4} --colsize {colsize4} --nrow {nrows4}\"\n    #Missing indexes\n    cmd4 = f\"python {script} --anomaly MISSING_INDEXES --threads {Thread_5} --ncolumn {ncolumns5} --colsize {colsize5} --nrow {nrows5}\"\n    #Too many indexes\n    cmd5 = f\"python {script} --anomaly REDUNDANT_INDEX   --threads 5 --ncolumn {ncolumns6} --colsize {colsize6} --nrow {nrows6}\"\n    #INSERT_LARGE_DATA, IO_CONTENTION\n    cmd6 = f\"python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA,IO_CONTENTION\"\n    #cmd7=f\"bash stop_benchmark_tpcc.sh\"\n    #POOR_JOIN_PERFORMANCE, CPU_CONTENTION\n    cmd8 = f\"python anomaly_trigger/main.py --anomaly POOR_JOIN_PERFORMANCE,CPU_CONTENTION\"\n    #FETCH_LARGE_DATA , CORRELATED_SUBQUERY\n    cmd9 = f\"python anomaly_trigger/main.py --anomaly FETCH_LARGE_DATA,CORRELATED_SUBQUERY\"\n    #cmd10=f\"bash missing_indexes_and_vacuum.sh\"\n    #Heavy workloads  #I/O saturation\n    cmd_list=cmd_list = [cmd0,cmd1,cmd2,cmd3,cmd4,cmd5,cmd6,cmd8,cmd9]\n    #dropdatabase.dropdatabase()\n    #createdatabase.createdatabase()\n    run_cmd(cmd_list)\n    #dropdatabase.dropdatabase()\n    #run_cmd67(cmd6,cmd7)\n# 关闭日志文件\n#log_file.close()\n\n\n"}
{"type": "source_file", "path": "anomaly_trigger/multi_anomalies.py", "content": "import subprocess\nimport createdatabase\nimport dropdatabase\nimport psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\ndef run_command(command):\n    \"\"\"运行给定的命令\"\"\"\n    process = subprocess.Popen(command, shell=True)\n    return process\n\ndef main():\n    # 定义要运行的命令\n    command1 = \"python anomaly_trigger/miss_multi.py\"\n    command2 = \"python anomaly_trigger/insert_multi.py\"\n    dropdatabase.dropdatabase(\"tmp\")\n    createdatabase.createdatabase(\"tmp\")\n    # 同时启动两个进程\n    table_name=\"table1\"\n    delete_table(table_name)\n    create_table(table_name,1000, 1000)\n    process1 = run_command(command1)\n    process2 = run_command(command2)\n\n    # 等待进程完成\n    process1.wait()\n    process2.wait()\n    print(\"Both processes have finished.\")\n    dropdatabase.dropdatabase(\"tmp\")\nif __name__ == \"__main__\":\n    main()\n"}
{"type": "source_file", "path": "anomaly_trigger/change_format_with_timestamp.py", "content": "import re\nimport random\n\ninput_file_name = \"xxx\"\noutput_file_name = \"xxx\"\n\n\ntry:\n    # 打开输入文件以读取内容\n    with open(input_file_name, 'r') as input_file:\n        # 读取输入文件的所有行\n        lines = input_file.readlines()\n    print(\"已读取\")\nexcept FileNotFoundError:\n    print(f\"文件 {input_file_name} 不存在。\")\nexcept Exception as e:\n    print(f\"发生错误: {str(e)}\")\nfor line in lines:\n    # 输入的字符串\n    input_string = line\n    anomaly_match = re.search(r'--fault\\s+([^\\s]+)', input_string)\n    anomaly = str(anomaly_match.group(1)) if anomaly_match else None\n    bash_match = re.search(r'bash\\s+([^\\s]+)', input_string)\n    bash = str(bash_match.group(1)) if bash_match else None\n    pattern1 = r'started at (\\d+)'\n    timestamp_match1 = re.search(pattern1,input_string)\n    timestamp1=str(timestamp_match1.group(0)) if timestamp_match1 else \"\"\n    pattern2 = r'ended at (\\d+)'\n    timestamp_match2 = re.search(pattern2,input_string)\n    timestamp2=str(timestamp_match2.group(0)) if timestamp_match2 else \"\"\n    if anomaly==\"cpu\":\n        # 使用正则表达式提取数字\n        threads_match = re.search(r'--client_5 (\\d+)', input_string)\n        ncolumn_match = re.search(r'--ncolumns (\\d+)', input_string)\n        colsize_match = re.search(r'--colsize (\\d+)', input_string)\n        nrow_match = re.search(r'--nrows (\\d+)', input_string)\n\n        # 提取的数字存入变量\n        threads = int(threads_match.group(1)) if threads_match else None\n        ncolumn = int(ncolumn_match.group(1)) if ncolumn_match else None\n        colsize = int(colsize_match.group(1)) if colsize_match else None\n        nrow = int(nrow_match.group(1)) if nrow_match else None\n        # 打开输出文件以写入内容\n        with open(output_file_name, 'a') as output_file:\n            script=f'python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA   --threads {threads} --ncolumn {ncolumn} --colsize {colsize} --nrow {nrow} {timestamp1}{timestamp2}\\n'\n            output_file.write(script)\n    if anomaly==\"wait\":\n        # 使用正则表达式提取数字\n        threads_match = re.search(r'--client_1 (\\d+)', input_string)\n        ncolumn_match = re.search(r'--ncolumns (\\d+)', input_string)\n        colsize_match = re.search(r'--colsize (\\d+)', input_string)\n        nrow_match = re.search(r'--nrows (\\d+)', input_string)\n\n        # 提取的数字存入变量\n        threads = int(threads_match.group(1)) if threads_match else None\n        ncolumn = int(ncolumn_match.group(1)) if ncolumn_match else None\n        colsize = int(colsize_match.group(1)) if colsize_match else None\n        nrow = int(nrow_match.group(1)) if nrow_match else None\n        # 打开输出文件以写入内容\n        with open(output_file_name, 'a') as output_file:\n            script=f'python anomaly_trigger/main.py --anomaly LOCK_CONTENTION   --threads {threads} --ncolumn {ncolumn} --colsize {colsize} --nrow {nrow} {timestamp1}{timestamp2}\\n'\n            output_file.write(script)\n    if anomaly==\"vacuum\":\n        # 使用正则表达式提取数字\n        threads_match = re.search(r'--client_4 (\\d+)', input_string)\n        ncolumn_match = re.search(r'--ncolumns (\\d+)', input_string)\n        colsize_match = re.search(r'--colsize (\\d+)', input_string)\n        nrow_match = re.search(r'--nrows (\\d+)', input_string)\n\n        # 提取的数字存入变量\n        threads = int(threads_match.group(1)) if threads_match else None\n        ncolumn = int(ncolumn_match.group(1)) if ncolumn_match else None\n        colsize = int(colsize_match.group(1)) if colsize_match else None\n        nrow = int(nrow_match.group(1)) if nrow_match else None\n        # 打开输出文件以写入内容\n        with open(output_file_name, 'a') as output_file:\n            script=f'python anomaly_trigger/main.py --anomaly VACUUM   --threads {threads} --ncolumn {ncolumn} --colsize {colsize} --nrow {nrow} {timestamp1}{timestamp2}\\n'\n            output_file.write(script)\n    \n    if anomaly==\"io\":\n        # 使用正则表达式提取数字\n        threads_match = re.search(r'--client_2 (\\d+)', input_string)\n        ncolumn_match = re.search(r'--ncolumns (\\d+)', input_string)\n        colsize_match = re.search(r'--colsize (\\d+)', input_string)\n        nrow_match = re.search(r'--nrows (\\d+)', input_string)\n\n        # 提取的数字存入变量\n        threads = int(threads_match.group(1)) if threads_match else None\n        ncolumn = int(ncolumn_match.group(1)) if ncolumn_match else None\n        colsize = int(colsize_match.group(1)) if colsize_match else None\n        nrow = int(nrow_match.group(1)) if nrow_match else None\n        # 打开输出文件以写入内容\n        with open(output_file_name, 'a') as output_file:\n            script=f'python anomaly_trigger/main.py --anomaly MISSING_INDEXES    --threads {threads} --ncolumn {ncolumn} --colsize {colsize} --nrow {nrow} {timestamp1}{timestamp2}\\n'\n            output_file.write(script)\n    if bash ==\"too_many_indexes.sh\":\n        threads =random.randint(5,10)\n        ncolumn = random.randint(50,100)\n        colsize = random.randint(50,100)\n        nrow = random.randint(400000,1000000)\n        # 打开输出文件以写入内容\n        with open(output_file_name, 'a') as output_file:\n            script=f'python anomaly_trigger/main.py --anomaly REDUNDANT_INDEX   --threads {threads} --ncolumn {ncolumn} --colsize {colsize} --nrow {nrow} {timestamp1}{timestamp2}\\n'\n            output_file.write(script)\n    if bash ==\"run_benchmark_job.sh\":\n        # 打开输出文件以写入内容\n        with open(output_file_name, 'a') as output_file:\n            script=f\"python anomaly_trigger/main.py --anomaly POOR_JOIN_PERFORMANCE,CPU_CONTENTION {timestamp1}{timestamp2}\\n\"\n            output_file.write(script)\n    if bash ==\"run_benchmark_tpch.sh\":\n        # 打开输出文件以写入内容\n        with open(output_file_name, 'a') as output_file:\n            script=f\"python anomaly_trigger/main.py --anomaly FETCH_LARGE_DATA,CORRELATED_SUBQUERY {timestamp1}{timestamp2}\\n\"\n            output_file.write(script)\n    if bash ==\"run_benchmark_tpcc.sh\":\n        # 打开输出文件以写入内容\n        with open(output_file_name, 'a') as output_file:\n            script=f\"python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA,IO_CONTENTION {timestamp1}{timestamp2}\\n\"\n            output_file.write(script)"}
{"type": "source_file", "path": "anomaly_trigger/dropdatabase.py", "content": "import psycopg2\nfrom utils.database import DB_CONFIG, SERVER_CONFIG\n\ndef dropdatabase(name):\n    # 连接到 \"postgres\" 数据库\n    conn = psycopg2.connect(\n        dbname=\"sysbench\",  # 连接到默认的 \"postgres\" 数据库\n        user=DB_CONFIG['user'],  # 替换为你的数据库用户名\n        password=DB_CONFIG['password'],  # 替换为你的数据库密码\n        host=DB_CONFIG['host'],  # 替换为你的数据库主机地址\n    )\n\n    conn.autocommit = True\n    # 创建一个数据库游标\n    cur = conn.cursor()\n\n    # 删除数据库的 SQL 语句\n    drop_database_query = f\"DROP DATABASE IF EXISTS {name}\"\n\n    # 执行删除数据库的 SQL 语句\n    cur.execute(drop_database_query)\n\n    # 提交更改\n    conn.commit()\n\n    # 关闭游标和数据库连接\n    cur.close()\n    conn.close()\n"}
{"type": "source_file", "path": "anomaly_trigger/reindex.py", "content": "import psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \n    def build_index(self, table_name, idx_num):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        \n        for i in range(0, idx_num):\n            the_sql = 'CREATE INDEX index_' + table_name + '_' + str(i) + ' ON ' + table_name + '(name' + str(i) + ');'\n            print(the_sql)\n            cursor.execute(the_sql)\n\n        \n        self.conn.commit()\n        self.conn.close()\n        return\n\n\n    \n    def drop_index(self,table_name):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        cursor.execute(\"select indexname from pg_indexes where tablename='\"+table_name+\"';\")\n        idxs = cursor.fetchall()\n        for idx in idxs:\n            the_sql = 'DROP INDEX ' + idx[0] + ';'\n            cursor.execute(the_sql)\n            print(the_sql)\n        self.conn.commit()\n        self.conn.close()\n        return\n\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n'''redundent_index'''\ndef redundent_index(threads,duration,ncolumns,nrows,colsize,nindex,table_name='table1'):\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    # insert some data to be updated \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n\n    #initialization of the indexes\n    nindex=int((nindex*ncolumns)/10)\n    db.build_index(table_name,nindex)\n    id_index='CREATE INDEX index_'+table_name+'_id ON '+table_name+'(id);'\n    db.execute_sqls(id_index)\n\n    #lock_contention\n    pool = Pool(threads)\n    for _ in range(threads):\n        pool.apply_async(\n            lock, (table_name, ncolumns, colsize, duration, nrows))\n    pool.close()\n    pool.join()\n\n    #drop the index\n    db.drop_index(table_name)\n\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\n\ndef lock(table_name, ncolumns, colsize, duration, nrows):\n    args=init()\n    start = time.time()\n    #lock_contention\n    while time.time()-start < duration:\n        conn = psycopg2.connect(database=args.dbname, user=args.user, password=args.password,\n                                        host=args.host, port=args.port)\n        cur = conn.cursor()\n        while time.time()-start < duration:\n            col_name = random.randint(0, ncolumns-1)\n            row_name = random.randint(1, nrows-1)\n            lock_contention = f'update {table_name} set name{col_name}=(SELECT substr(md5(random()::text), 1, {colsize})) where id ={row_name}'\n            #db.concurrent_execute_sql(threads,duration,lock_contention,nrows)\n            cur.execute(lock_contention)\n            conn.commit()\n        conn.commit()\n        conn.close()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 100\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = 60\n    \n    # Number of columns in the table\n    num_columns = 10\n    \n    # Number of rows to insert\n    num_rows = 100\n    \n    # Size of each column (in characters)\n    column_size = 200\n    \n    # Table name\n    table_name = 'table1'\n    \n    nindex=6\n    \n    # Call the insert_large_data function\n    redundent_index(num_threads, insert_duration, num_columns, num_rows, column_size, nindex,table_name)\n    "}
{"type": "source_file", "path": "anomaly_trigger/lock.py", "content": "import psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n'''lock_contention'''\ndef lock_contention(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    # insert some data to be updated \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n    pool = Pool(threads)\n    for _ in range(threads):\n        pool.apply_async(\n            lock, (table_name, ncolumns, colsize, duration, nrows))\n    pool.close()\n    pool.join()\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\ndef lock(table_name, ncolumns, colsize, duration, nrows):\n    args=init()\n    start = time.time()\n    #lock_contention\n    while time.time()-start < duration:\n        conn = psycopg2.connect(database=args.dbname, user=args.user, password=args.password,\n                                        host=args.host, port=args.port)\n        cur = conn.cursor()\n        while time.time()-start < duration:\n            col_name = random.randint(0, ncolumns-1)\n            row_name = random.randint(1, nrows-1)\n            lock_contention = f'update {table_name} set name{col_name}=(SELECT substr(md5(random()::text), 1, {colsize})) where id ={row_name}'\n            #db.concurrent_execute_sql(threads,duration,lock_contention,nrows)\n            cur.execute(lock_contention)\n            conn.commit()\n        conn.commit()\n        conn.close()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 100\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = 60\n    \n    # Number of columns in the table\n    num_columns = 10\n    \n    # Number of rows to insert\n    num_rows = 100\n    \n    # Size of each column (in characters)\n    column_size = 200\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    lock_contention(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)"}
{"type": "source_file", "path": "anomaly_trigger/lock_multi.py", "content": "import psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n'''lock_contention'''\ndef lock_contention(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    db=Database(init())\n    # insert some data to be updated \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n    pool = Pool(threads)\n    for _ in range(threads):\n        pool.apply_async(\n            lock, (table_name, ncolumns, colsize, duration, nrows))\n    pool.close()\n    pool.join()\n    print_time()\n\ndef lock(table_name, ncolumns, colsize, duration, nrows):\n    args=init()\n    start = time.time()\n    #lock_contention\n    while time.time()-start < duration:\n        conn = psycopg2.connect(database=args.dbname, user=args.user, password=args.password,\n                                        host=args.host, port=args.port)\n        cur = conn.cursor()\n        while time.time()-start < duration:\n            col_name = random.randint(0, ncolumns-1)\n            row_name = random.randint(1, nrows-1)\n            lock_contention = f'update {table_name} set name{col_name}=(SELECT substr(md5(random()::text), 1, {colsize})) where id ={row_name}'\n            #db.concurrent_execute_sql(threads,duration,lock_contention,nrows)\n            cur.execute(lock_contention)\n            conn.commit()\n        conn.commit()\n        conn.close()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 100\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = 60\n    \n    # Number of columns in the table\n    num_columns = 10\n    \n    # Number of rows to insert\n    num_rows = 100\n    \n    # Size of each column (in characters)\n    column_size = 200\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    lock_contention(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)"}
{"type": "source_file", "path": "anomaly_trigger/main.py", "content": "import argparse\nimport anomaly\nimport createdatabase\nimport dropdatabase\n\n\n\nparser = argparse.ArgumentParser(description='Anomaly simulation tool')\nparser.add_argument('--anomaly', type=str, required=True, choices=['INSERT_LARGE_DATA', 'MISSING_INDEXES','LOCK_CONTENTION','VACUUM','REDUNDANT_INDEX','INSERT_LARGE_DATA,IO_CONTENTION',\n                                                                   'FETCH_LARGE_DATA,CORRELATED_SUBQUERY','POOR_JOIN_PERFORMANCE,CPU_CONTENTION'],\n                        help='Specify the type of anomaly to simulate')\nparser.add_argument('--threads',type=int,default=0,help='threads')\nparser.add_argument('--duration', type=int, default=60, help='duration')\nparser.add_argument('--ncolumn', type=int, default=10,help=\"number of columns\")\nparser.add_argument('--nrow', type=int, default=100,help=\"number of rows\")\nparser.add_argument('--colsize', type=int, default=200,help=\"column length\")\nparser.add_argument('--table_size', type=int, default=10,help=\"table size\")\nparser.add_argument('--table_name', type=str,default='table1', help=\"name of table to be excuted\")\nparser.add_argument('--nindex',type=int,default=5,help='index in the REDUNDANT_INDEX')\n\nargs = parser.parse_args()\n#anomaly=parser.anomaly\nthreads= args.threads\nduration = args.duration\nncolumns = args.ncolumn\nnrows = args.nrow\ncolsize = args.colsize\nnindex=args.nindex\ntable_name = args.table_name\n\n\nif args.anomaly == 'INSERT_LARGE_DATA':\n        dropdatabase.dropdatabase(\"tmp\")\n        createdatabase.createdatabase(\"tmp\")\n        anomaly.insert_large_data(threads,duration,ncolumns,nrows,colsize,table_name)\n        dropdatabase.dropdatabase(\"tmp\")\n\nelif args.anomaly == 'MISSING_INDEXES':\n        dropdatabase.dropdatabase(\"tmp\")\n        createdatabase.createdatabase(\"tmp\")\n        anomaly.missing_index(threads,duration,ncolumns, nrows, colsize,table_name)\n        dropdatabase.dropdatabase(\"tmp\")\n\nelif args.anomaly == 'LOCK_CONTENTION':\n        dropdatabase.dropdatabase(\"tmp\")\n        createdatabase.createdatabase(\"tmp\")\n        anomaly.lock_contention(threads,duration,ncolumns, nrows, colsize,table_name)\n        dropdatabase.dropdatabase(\"tmp\")\n\nelif args.anomaly == 'VACUUM':\n        dropdatabase.dropdatabase(\"tmp\")\n        createdatabase.createdatabase(\"tmp\")\n        anomaly.vacuum(threads,duration,ncolumns, nrows, colsize,table_name)\n        dropdatabase.dropdatabase(\"tmp\")\n        \nelif args.anomaly == 'REDUNDANT_INDEX':\n        dropdatabase.dropdatabase(\"tmp\")\n        createdatabase.createdatabase(\"tmp\")\n        anomaly.redundent_index(threads,duration,ncolumns,nrows,colsize,nindex,table_name)\n        dropdatabase.dropdatabase(\"tmp\") \n             \nelif args.anomaly == 'INSERT_LARGE_DATA,IO_CONTENTION':\n        anomaly.io_contention()\n            \nelif args.anomaly == 'FETCH_LARGE_DATA,CORRELATED_SUBQUERY':\n        anomaly.fetch_large_data()  \n          \nelif args.anomaly == 'POOR_JOIN_PERFORMANCE,CPU_CONTENTION':\n        anomaly.cpu_contention() \n                       \nelse:\n        print(\"Invalid --anomaly option.\")\n\n\n"}
{"type": "source_file", "path": "anomaly_trigger/insert_multi.py", "content": "import psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\n'''insert_large_data'''\ndef insert_large_data(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n\n    print_time()\n    db=Database(init())\n    #insert the data\n    #insert_definitions = ', '.join(f'repeat(round(random()*999)::text,{(colsize//3)})' for i in range(ncolumns))\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.concurrent_execute_sql(threads,duration,insert_data,commit_interval=1)\n\n    #print the end time\n    print_time()\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 100\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = 60\n    \n    # Number of columns in the table\n    num_columns = 100\n    \n    # Number of rows to insert\n    num_rows = 100\n    \n    # Size of each column (in characters)\n    column_size = 100\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    insert_large_data(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)"}
{"type": "source_file", "path": "anomaly_trigger/formalize_anomalies.py", "content": "import re\nimport datetime\n#from alert import filter_alerts_by_time\nimport json\n'''\ndef dataset_statistics():\n    max_column = 0\n    min_column = 100000000000000\n\n    keywords = [\"retail\",\n    \"online marketplace\",\n    \"online platform\",\n    \"online store\",\n    \"e-commerce\",\n    \"sales\"]\n\n    keywords = ['iot']\n\n    keywords = ['smart home']\n\n    keywords = ['financial']\n\n    keywords = ['social media']\n\n    keywords = ['file sharing']\n\n    # keyword = [\n    # \"data processing\",\n    # \"data analytics\",\n    # \"big data\",\n    # \"data-intensive\"]\n\n    anomalies = []\n\n    # read all lines in command-datasets.txt into commands in list\n    count1 = 0\n    commands = []\n    with open('command-datasets.txt', 'r') as f:\n        for line in f:\n            if ',' in line:\n                count1 += 1\n            line = line.strip()\n            commands.append(line)\n\n\n    with open('nl-datasets.txt', 'r') as f:\n        #print(\"lines:\",len(f.readlines()))\n        for i,line in enumerate(f):\n            yes = 0\n            for keyword in keywords:\n                if keyword in line.lower():\n                    yes = 1\n                    break\n\n            if yes == 1:\n\n                # write into a file named nl-datasets-iot.txt\n                # with open('nl-datasets-bi.txt', 'a') as f2:\n                #     line = line.strip()\n                #     f2.write(line+'\\n\\n')\n\n                # re: detect the \"columns\" substring in the line string and extract the string before \"columns\"\n                # before_columns = re.search(r'(.*)columns', line.lower())\n                # if before_columns is None:\n                #     continue\n                # before_columns = before_columns.group(1)\n                # before_columns = before_columns.split()\n                # column_value = before_columns[-1]\n                # # if column_value is a string of numbers\n                # if column_value.isdigit():\n                #     column_value = int(column_value)\n                #     if column_value > max_column:\n                #         max_column = column_value\n                #     if column_value < min_column:\n                #         min_column = column_value\n\n                # match anomalies\n\n                command = commands[i]\n                #import pdb; pdb.set_trace()\n\n                # command_words = command.split()[3]\n                # command_words = command_words.split(',')\n                # for word in command_words:\n                #     if word not in anomalies:\n                #         anomalies.append(word)\n                if \"ncolumn\" in command:\n                    col_num = command.split()[7]\n                    if col_num.isdigit():\n                        col_num = int(col_num)\n                        if col_num > max_column:\n                            max_column = col_num\n                        if col_num < min_column:\n                            min_column = col_num\n\n    print('num of ,:',count1)\n    print(\"max_column: \", max_column)\n    print(\"min_column: \", min_column)\n'''\ndef time_stamps():\n\n    splitting_token = \"\"\"\n\n\nimport\"\"\"\n\n    #dir_name = \"up_to_date_dataset/\"\n\n    # target_datetime = datetime.datetime(2023, 10, 10, 3, 0, 0)\n\n    alert_cnt = 0\n    commands = []\n\n    root_causes = {\n        \"INSERT_LARGE_DATA\": [\"highly concurrent commits or highly concurrent inserts\"],\n        \"LOCK_CONTENTION\": [\"highly concurrent updates\"],\n        \"VACUUM\": [\"highly deletes\"],\n        \"REDUNDANT_INDEX\": [\"too many indexes\"],\n        \"MISSING_INDEXES\": [\"missing indexes\"],\n        \"INSERT_LARGE_DATA,IO_CONTENTION\": [\"INSERT_LARGE_DATA\",\"IO_CONTENTION\"],\n        \"FETCH_LARGE_DATA,CORRELATED_SUBQUERY\": [\"FETCH_LARGE_DATA\",\"CORRELATED SUBQUERY\"],\n        \"POOR_JOIN_PERFORMANCE,CPU_CONTENTION\": [\"POOR JOIN PERFORMANCE\",\"CPU CONTENTION\"],\n    }\n\n\n    with open('m_i_naturallanguage.txt', 'r') as f:\n        # read f content into nlps in list\n        desc_blocks = f.readlines()\n\n    with open('m_i_code.txt', 'r') as f:\n        # read all the f content into nlps in a text\n        nlps = f.read()\n        code_blocks = nlps.split(splitting_token)\n        new_code_blocks = []\n        for code_block in code_blocks:            \n            new_code_blocks.append('import ' + code_block)\n        code_blocks = new_code_blocks\n\n    anomaly_jsons = []\n\n    with open('missingindex_with_timestamp.txt', 'r') as f:\n        #print(\"lines:\",len(f.readlines()))\n\n        while True:\n            line1 = f.readline()\n            line2 = f.readline()\n            \n            if not line1:\n                break\n\n            content = {\"start_time\": \"111233\",\"end_time\": \"111433\", \"start_timestamp\": \"111233\",\"end_timestamp\": \"111433\", \"alerts\": [], \"labels\":[], \"command\": \"\", \"script\": \"\", \"description\": \"\"}\n\n            #import pdb; pdb.set_trace()\n\n            timestamp = line1.split()[-1]\n            command = line1.split()[0:4]\n            command_str= ' '.join(command)\n            if command_str not in commands:\n                commands.append(command_str)\n\n            content[\"command\"] = command_str\n            content[\"start_time\"] = timestamp\n            if timestamp.isdigit():\n                timestamp = int(timestamp)\n                # convert seconds to datetime (year, month, day, hour, minute, second)\n                dt_object = datetime.datetime.fromtimestamp(timestamp)\n                # 2023-10-10 03:00:00\n                formatted_time = dt_object.strftime(\"%Y-%m-%d %H:%M:%S\")\n                content[\"start_timestamp\"] = formatted_time\n\n            timestamp = line2.split()[-1]\n            content[\"end_time\"] = timestamp\n            if timestamp.isdigit():\n                timestamp = int(timestamp)\n                # convert seconds to datetime (year, month, day, hour, minute, second)\n                dt_object = datetime.datetime.fromtimestamp(timestamp)\n                # 2023-10-10 03:00:00\n                formatted_time = dt_object.strftime(\"%Y-%m-%d %H:%M:%S\")\n                content[\"end_timestamp\"] = formatted_time\n            \n            # alerts\n            '''alerts = filter_alerts_by_time(\"alert.txt\", content[\"start_time\"], content[\"end_time\"])\n            content[\"alerts\"] = alerts\n            if alerts!=[]:\n                print(alert_cnt, alerts)\n                alert_cnt = alert_cnt + 1'''\n\n            # labels\n            for cause in root_causes:\n                if cause in command_str:\n                    content[\"labels\"] = root_causes[cause]\n\n            if len(anomaly_jsons) >= len(code_blocks):\n                break\n\n            # script\n            content[\"script\"] = code_blocks[len(anomaly_jsons)]\n                                \n            # description\n            content[\"description\"] = desc_blocks[len(anomaly_jsons)]\n\n            anomaly_jsons.append(content)\n\n\n            # record the content in well-formatted json into a file\n            with open('anomaly_jsons.txt', 'a') as f2:\n                f2.write(json.dumps(content, indent=4) + '\\n')\n\n    print(\"alert_cnt:\", alert_cnt)\n\n    return anomaly_jsons\n\nanomaly_jsons = time_stamps()"}
{"type": "source_file", "path": "anomaly_trigger/utils/database.py", "content": "import psycopg2\nimport pymysql\nimport json\nimport logging\nimport os\nfrom enum import IntEnum\nimport time\nimport random\nfrom multiprocessing.pool import *\n\nDB_CONFIG = {\n    \"dbname\":\"sysbench\",  # 连接到默认的 \"postgres\" 数据库\n    \"user\":\"test\",  # 替换为你的数据库用户名\n    \"password\":\"Test123_456\",  # 替换为你的数据库密码\n    \"host\":\"localhost\",  # 替换为你的数据库主机地址\n    \"port\": 5432,\n    # \"dbtype\": \"postgresql\"\n    }\n\nSERVER_CONFIG = {\n    \"host\": \"localhost\",\n    \"port\": 22,\n    \"user\": 'root',\n    \"password\": 'xxxx'\n}\n\n\ndef extract_node_types(json_tree):\n    node_types = []\n    \n    def traverse_tree(node):\n        if isinstance(node, dict):\n            if 'Node Type' in node:\n                node_types.append(node['Node Type'])\n            for key, value in node.items():\n                traverse_tree(value)\n        elif isinstance(node, list):\n            for item in node:\n                traverse_tree(item)\n    \n    traverse_tree(json_tree)\n    return node_types\n\nclass DataType(IntEnum):\n    VALUE = 0\n    TIME = 1\n    CHAR = 2\n\nAGGREGATE_CONSTRAINTS = {\n    DataType.VALUE.value: ['count', 'max', 'min', 'avg', 'sum'],\n    DataType.VALUE.CHAR: ['count', 'max', 'min'],\n    DataType.VALUE.TIME: ['count', 'max', 'min']\n}\n\ndef check_index_exist(cursor, index_name):\n        try:\n            cursor.execute(f\"SELECT indexname FROM pg_indexes WHERE indexname = '{index_name}';\")\n            result = cursor.fetchone()\n            return result is not None\n        except psycopg2.Error as e:\n            print(f\"Error checking index: {e}\")\n            return False\n\ndef transfer_field_type(column_type, server):\n    data_type = list()\n    if server == 'mysql':\n        data_type = [['int', 'tinyint', 'smallint', 'mediumint', 'bigint', 'float', 'double', 'decimal'],\n                     ['date', 'time', 'year', 'datetime', 'timestamp']]\n        column_type = column_type.lower().split('(')[0]\n    elif server == 'postgresql':\n        data_type = [['integer', 'numeric'],\n                     ['date']]\n\n    if column_type in data_type[0]:\n        return DataType.VALUE.value\n    elif column_type in data_type[1]:\n        return DataType.TIME.value\n    else:\n        return DataType.CHAR.value\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None,application_name=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n            self.application_name = application_name\n\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        if self.args.dbtype == 'mysql':\n                conn = pymysql.connect(\n                host=self.args.host,\n                user=self.args.user,\n                passwd=self.args.password,\n                database=self.args.dbname,\n                port=int(self.args.port),\n                charset='utf8',\n                connect_timeout=timeout,\n                read_timeout=timeout,\n                write_timeout=timeout)\n        elif self.args.dbtype == 'postgresql':\n            if timeout > 0:\n                conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port,\n                                            options='-c statement_timeout={}s'.format(timeout),\n                                            application_name=self.args.application_name)\n            else:\n                \n                conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port,\n                                            application_name=self.args.application_name)\n\n        return conn\n    '''\n    def exec_fetch(self, statement, one=True):\n        cur = self.conn.cursor()\n        cur.execute(statement)\n        if one:\n            return cur.fetchone()\n        return cur.fetchall()    \n    '''\n\n    def execute_sql(self, sql):\n        fail = 1\n        self.conn = self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        i = 0\n        cnt = 5 # retry times\n        while fail == 1 and i < cnt:\n            try:\n                fail = 0\n                # print(\"========== start execution\", i)\n                cur.execute(sql)\n            except BaseException:\n                fail = 1\n                time.sleep(1)\n            res = []\n            if fail == 0:\n                res = cur.fetchall()\n            i = i + 1\n        logging.debug('database {}, return flag {}, execute sql {}\\n'.format(self.args.dbname, 1 - fail, sql))\n\n        cur.close()\n        self.conn.close()\n\n        # print(\"========== finish time:\", time.time())\n\n        if fail == 1:\n            # raise RuntimeError(\"Database query failed\")\n            # print(\"SQL Execution Fatal!!\")\n\n            return 0, ''\n        elif fail == 0:\n            # print(\"SQL Execution Succeed!!\")\n\n            return 1, res\n\n    def pgsql_results(self, sql):\n        try:\n            #success, res = self.execute_sql('explain (FORMAT JSON, analyze) ' + sql)\n            success, res = self.execute_sql(sql)\n            #print(\"pgsql_results\", success, res)\n            if success == 1:\n                return res\n            else:\n                return \"<fail>\"\n        except Exception as error:\n            logging.error('pgsql_results Exception', error)\n            return \"<fail>\"\n\n    def pgsql_query_plan(self, sql):\n        try:\n            #success, res = self.execute_sql('explain (FORMAT JSON, analyze) ' + sql)\n\n            # sql starts with \"set\"\n            if sql.startswith(\"set\"): # hint query\n                sqls = sql.split(\";\")\n                sql = sqls[0] + ';' + ' explain (FORMAT JSON) ' + sqls[1]\n            else:\n                sql = 'explain (FORMAT JSON) ' + sql\n            \n            success, res = self.execute_sql(sql)\n            if success == 1:\n                plan = res[0][0][0]['Plan']\n                return plan\n            else:\n                logging.error('pgsql_query_plan Fails!')\n                return None\n        except Exception as error:\n            logging.error('pgsql_query_plan Exception', error)\n            return None\n\n    def query_plan_statistics(self, plan):\n        operators = extract_node_types(plan)\n\n        return plan['Total Cost'], operators\n\n\n    def pgsql_cost_estimation(self, sql):\n        try:\n            #success, res = self.execute_sql('explain (FORMAT JSON, analyze) ' + sql)\n            success, res = self.execute_sql('explain (FORMAT JSON) ' + sql)\n            if success == 1:\n                cost = res[0][0][0]['Plan']['Total Cost']\n                return cost\n            else:\n                logging.error('pgsql_cost_estimation Fails!')\n                return 0\n        except Exception as error:\n            logging.error('pgsql_cost_estimation Exception', error)\n            return 0\n\n    def pgsql_actual_time(self, sql):\n        try:\n            #success, res = self.execute_sql('explain (FORMAT JSON, analyze) ' + sql)\n            success, res = self.execute_sql('explain (FORMAT JSON, analyze) ' + sql)\n            if success == 1:\n                cost = res[0][0][0]['Plan']['Actual Total Time']\n                return cost\n            else:\n                return -1\n        except Exception as error:\n            logging.error('pgsql_actual_time Exception', error)\n            return -1\n\n    def mysql_cost_estimation(self, sql):\n        try:\n            success, res = self.execute_sql('explain format=json ' + sql)\n            if success == 1:\n                total_cost = self.get_mysql_total_cost(0, json.loads(res[0][0]))\n                return float(total_cost)\n            else:\n                return -1\n        except Exception as error:\n            logging.error('mysql_cost_estimation Exception', error)\n            return -1\n\n    def get_mysql_total_cost(self, total_cost, res):\n        if isinstance(res, dict):\n            if 'query_cost' in res.keys():\n                total_cost += float(res['query_cost'])\n            else:\n                for key in res:\n                    total_cost += self.get_mysql_total_cost(0, res[key])\n        elif isinstance(res, list):\n            for i in res:\n                total_cost += self.get_mysql_total_cost(0, i)\n\n        return total_cost\n\n    def get_tables(self):\n        if self.args.dbtype == 'mysql':\n            return self.mysql_get_tables()\n        else:\n            return self.pgsql_get_tables()\n\n    # query cost estimated by the optimizer\n    def cost_estimation(self, sql):\n        if self.args.dbtype == 'mysql':\n            return self.mysql_cost_estimation(sql)\n        else:\n            return self.pgsql_cost_estimation(sql)\n\n    def compute_table_schema(self):\n        \"\"\"\n        schema: {table_name: [field_name]}\n        :param cursor:\n        :return:\n        \"\"\"\n\n        if self.args.dbtype == 'postgresql':\n            # cur_path = os.path.abspath('.')\n            # tpath = cur_path + '/sampled_data/'+dbname+'/schema'\n            sql = 'SELECT table_name FROM information_schema.tables WHERE table_schema = \\'public\\';'\n            success, res = self.execute_sql(sql)\n            #print(\"======== tables\", res)\n            if success == 1:                \n                tables = res\n                schema = {}\n                for table_info in tables:\n                    table_name = table_info[0]\n                    sql = 'SELECT column_name, data_type FROM information_schema.columns WHERE table_name = \\'' + table_name + '\\';'\n                    success, res = self.execute_sql(sql)\n                    #print(\"======== table columns\", res)\n                    columns = res\n                    schema[table_name] = []\n                    for col in columns:\n\n                        ''' compute the distinct value ratio of the column\n                        \n                        if transfer_field_type(col[1], self.args.dbtype) == DataType.VALUE.value:\n                            sql = 'SELECT count({}) FROM {};'.format(col[0], table_name)\n                            success, res = self.execute_sql(sql)\n                            print(\"======== column rows\", res)\n                            num = res\n                            if num[0][0] != 0:\n                                schema[table_name].append(col[0])\n                        '''\n\n                        #schema[table_name].append(\"column {} is of {} type\".format(col[0], col[1]))\n                        schema[table_name].append(\"{}\".format(col[0]))\n                '''\n                with open(tpath, 'w') as f:\n                    f.write(str(schema))\n                '''\n                #print(schema)\n                return schema\n\n            else:\n                logging.error('pgsql_cost_estimation Fails!')\n                return 0\n\n    def simulate_index(self, index):\n        #table_name = index.table()\n        statement = (\n            \"SELECT * FROM hypopg_create_index(E'{}');\".format(index)\n        )\n        result = self.execute_sql(statement)\n\n        return result\n\n    def obtain_historical_queries_statistics(self, topn = 50):\n        try:\n            #success, res = self.execute_sql('explain (FORMAT JSON, analyze) ' + sql)\n            #command = \"SELECT query, calls, total_time FROM pg_stat_statements ORDER BY total_time DESC LIMIT 2;\"\n            \n            command = f\"SELECT s.query, s.calls, s.total_time, d.datname FROM pg_stat_statements s JOIN pg_database d ON s.dbid = d.oid ORDER BY s.total_time DESC LIMIT {topn};\"\n            success, res = self.execute_sql(command)\n            if success == 1:\n                slow_queries = []\n                for sql_stat in res:\n                    if not \"postgres\" in sql_stat[3]:\n                        sql_template = sql_stat[0].replace(\"\\n\", \"\").replace(\"\\t\", \"\").strip()\n                        # print(\"===== logged slow query: \", sql_template,sql_stat[1],sql_stat[2],sql_stat[3])\n                        sql_template = sql_template.lower()\n                        if \"explain\" in sql_template or \"analyze\" in sql_template:\n                            continue\n\n                        slow_queries.append({\"sql\": sql_template, \"calls\": sql_stat[1], \"total_time\": sql_stat[2], \"dbname\": sql_stat[3]})\n                        \n                return slow_queries\n            else:\n                logging.error('obtain_historical_queries_statistics Fails!')\n                return 0\n        except Exception as error:\n            logging.error('obtain_historical_queries_statistics Exception', error)\n            return 0        \n\n    def drop_simulated_index(self, oid):\n        statement = f\"select * from hypopg_drop_index({oid})\"\n        result = self.execute_sql(statement)\n\n        assert result[0] is True, f\"Could not drop simulated index with oid = {oid}.\"\n\n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \n\n        \n    def build_index(self, table_name, idx_num):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n    \n        for i in range(0, idx_num):\n            the_sql = 'CREATE INDEX index_' + table_name + '_' + str(i) + ' ON ' + table_name + '(name' + str(i) + ');'\n            print(the_sql)\n            cursor.execute(the_sql)\n\n        # 检查索引是否创建成功\n            #if check_index_exist(cursor, 'index_' + table_name + '_' + str(i)):\n                #print(f'Index index_{table_name}_{i} created successfully.')\n            #else:\n                #print(f'Failed to create index index_{table_name}_{i}.')\n    \n        self.conn.commit()\n        self.conn.close()\n        return\n\n\n    \n    def drop_index(self,table_name):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        cursor.execute(\"select indexname from pg_indexes where tablename='\"+table_name+\"';\")\n        idxs = cursor.fetchall()\n        for idx in idxs:\n            the_sql = 'DROP INDEX ' + idx[0] + ';'\n            cursor.execute(the_sql)\n            print(the_sql)\n        self.conn.commit()\n        self.conn.close()\n        return\n"}
{"type": "source_file", "path": "common/__init__.py", "content": ""}
{"type": "source_file", "path": "anomaly_trigger/miss.py", "content": "import psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n'''missing_index'''\ndef missing_index(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    db=Database(init())\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be selected \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.execute_sqls(insert_data)  \n\n    #select without the index\n    missing_index='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,missing_index,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    #print the end time\n    print_time()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 100\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = 60\n    \n    # Number of columns in the table\n    num_columns = 75\n    \n    # Number of rows to insert\n    num_rows = 37100\n    \n    # Size of each column (in characters)\n    column_size = 100\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n   \n    missing_index(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n "}
{"type": "source_file", "path": "anomaly_trigger/reindex_multi.py", "content": "import psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \n    def build_index(self, table_name, idx_num):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        \n        for i in range(0, idx_num):\n            the_sql = 'CREATE INDEX index_' + table_name + '_' + str(i) + ' ON ' + table_name + '(name' + str(i) + ');'\n            print(the_sql)\n            cursor.execute(the_sql)\n\n        \n        self.conn.commit()\n        self.conn.close()\n        return\n\n\n    \n    def drop_index(self,table_name):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        cursor.execute(\"select indexname from pg_indexes where tablename='\"+table_name+\"';\")\n        idxs = cursor.fetchall()\n        for idx in idxs:\n            the_sql = 'DROP INDEX ' + idx[0] + ';'\n            cursor.execute(the_sql)\n            print(the_sql)\n        self.conn.commit()\n        self.conn.close()\n        return\n\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n'''redundent_index'''\ndef redundent_index(threads,duration,ncolumns,nrows,colsize,nindex,table_name='table1'):\n    #create a new table\n    print_time()\n    db=Database(init())\n    # insert some data to be updated \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n\n    #initialization of the indexes\n    nindex=int((nindex*ncolumns)/10)\n    db.build_index(table_name,nindex)\n    id_index='CREATE INDEX index_'+table_name+'_id ON '+table_name+'(id);'\n    db.execute_sqls(id_index)\n\n    #lock_contention\n    pool = Pool(threads)\n    for _ in range(threads):\n        pool.apply_async(\n            lock, (table_name, ncolumns, colsize, duration, nrows))\n    pool.close()\n    pool.join()\n\n    #drop the index\n    db.drop_index(table_name)\n\n    print_time()\n\n\ndef lock(table_name, ncolumns, colsize, duration, nrows):\n    args=init()\n    start = time.time()\n    #lock_contention\n    while time.time()-start < duration:\n        conn = psycopg2.connect(database=args.dbname, user=args.user, password=args.password,\n                                        host=args.host, port=args.port)\n        cur = conn.cursor()\n        while time.time()-start < duration:\n            col_name = random.randint(0, ncolumns-1)\n            row_name = random.randint(1, nrows-1)\n            lock_contention = f'update {table_name} set name{col_name}=(SELECT substr(md5(random()::text), 1, {colsize})) where id ={row_name}'\n            #db.concurrent_execute_sql(threads,duration,lock_contention,nrows)\n            cur.execute(lock_contention)\n            conn.commit()\n        conn.commit()\n        conn.close()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 100\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = 60\n    \n    # Number of columns in the table\n    num_columns = 10\n    \n    # Number of rows to insert\n    num_rows = 100\n    \n    # Size of each column (in characters)\n    column_size = 200\n    \n    # Table name\n    table_name = 'table1'\n    \n    nindex=6\n    \n    # Call the insert_large_data function\n    redundent_index(num_threads, insert_duration, num_columns, num_rows, column_size, nindex,table_name)\n    "}
{"type": "source_file", "path": "anomaly_trigger/utils/__init__.py", "content": ""}
{"type": "source_file", "path": "anomaly_trigger/vacuum.py", "content": "import psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n'''vacuum'''\ndef vacuum(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    db=Database(init())\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be deleted\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n\n    # delete 80% of the rows\n    delete_nrows=int(nrows*0.8)\n    vacuum=f'delete from {table_name} where id < {delete_nrows};'\n    db.execute_sqls(vacuum)\n\n    # do the select , then the vacuum occurs\n    select='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,select,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 100\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = 60\n    \n    # Number of columns in the table\n    num_columns = 10\n    \n    # Number of rows to insert\n    num_rows = 100\n    \n    # Size of each column (in characters)\n    column_size = 20000\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    vacuum(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)"}
{"type": "source_file", "path": "batch_main.py", "content": "from our_argparse import args\nimport json\nimport multiprocessing as mp\nfrom math import floor\nimport jsonlines\nimport time\nimport os\nimport asyncio\nfrom pathlib import Path\n\nfrom multiagents.multiagents import MultiAgents\nfrom multiagents.tools.metrics import database_server_conf, db\nfrom multiagents.tools.metrics import get_workload_statistics, get_slow_queries, WORKLOAD_FILE_NAME, BATCH_ANOMALY_FILE_NAME\nfrom utils.server import obtain_slow_queries\n\n\nasync def main(process_id):\n    global anomaly_jsons, result_jsons\n\n    new_args = args\n\n    if process_id + 1 == process_num:\n        # [process_id*split_size:]\n        diag_ids = list(anomaly_jsons.keys())[process_id*split_size:]\n    else:\n        # process_id*split_size: (process_id+1)*split_size\n        diag_ids = list(anomaly_jsons.keys())[process_id*split_size: (process_id+1)*split_size]\n\n    for diag_id in diag_ids:\n        record_name = reports_log_dir_name + \"/diag_\"+diag_id+\".jsonl\"\n        if not os.path.exists(record_name) and diag_id not in finished_diag_ids:\n            content = anomaly_jsons[diag_id]\n            \n            # ============================= workload info =============================\n            new_args.diag_id = diag_id\n\n            # slow_queries = []\n            workload_statistics = []\n            # workload_sqls = \"\"\n            # if new_args.enable_slow_query_log == True:\n                # [slow queries] read from query logs\n                # /var/lib/pgsql/12/data/pg_log/postgresql-Mon.log\n                # slow_queries = obtain_slow_queries(database_server_conf)\n                # slow_queries = content[\"slow_queries\"] # list type\n            if new_args.enable_workload_statistics_view == True:\n                workload_statistics = db.obtain_historical_queries_statistics(topn = 50)\n            # if new_args.enable_workload_sqls == True:\n                # workload_sqls = content[\"workload\"]\n\n            with open(WORKLOAD_FILE_NAME, 'w') as f:\n                json.dump({'workload_statistics': workload_statistics}, f)\n\n            if \"alerts\" in content and content[\"alerts\"] != []:\n                new_args.alerts = content[\"alerts\"] # possibly multiple alerts for a single anomaly\n            else:\n                new_args.alerts = []\n\n            if \"labels\" in content and content[\"labels\"] != []:\n                new_args.labels = content[\"labels\"]\n            else:\n                new_args.labels = []\n            \n            new_args.start_at_seconds = content[\"start_time\"]\n            new_args.end_at_seconds = content[\"end_time\"]        \n            # =======================================================================================\n            # =======================================================================================\n\n            multi_agents = MultiAgents.from_task(new_args.agent_conf_name, new_args)\n            report, records = await multi_agents.run(new_args)\n\n            # ================== vanilla model ==================\n            # diag, labels = await multi_agents.run(new_args)\n            # result_jsons[diag_id][\"diag_\"+method] = diag\n            # result_jsons[diag_id][\"label_\"+method] = labels\n\n            # print(str(diag_id), '========== ok ================')\n            # ===================================================\n\n            # detailed log\n            with open(reports_log_dir_name + \"/diag_\"+diag_id+\".jsonl\", \"w\") as f:\n                json.dump(report, f, indent=4)\n\n            with open(log_dir_name + f\"/{str(new_args.start_at_seconds)}.jsonl\", \"w\") as f:\n                json.dump(records, f, indent=4)\n\n            # result logs\n            result_jsons[diag_id][\"diag_\"+method] = report[\"root cause\"]\n            result_jsons[diag_id][\"solution_\"+method] = report[\"solutions\"]\n            result_jsons[diag_id][\"label_\"+method] = report[\"labels\"]\n\n            print(str(diag_id), '========== ok ================')\n\ndef wrapper(i):\n    asyncio.run(main(i))\n\n# read from the anomalies with alerts. for each anomaly, \nprocess_num = 4\nresult_log_prefix = \"./alert_results/logs/\"\nlog_dir_name = result_log_prefix + \"batch_logs\"\nreports_log_dir_name = log_dir_name + \"/reports\"\n\nmethod = \"d_bot_gpt4\"\nfinished_diag_ids = [] #[\"45\",\"30\",\"15\", \"0\", \"46\", \"31\", \"1\"] #[\"2\", \"6\", \"8\", \"9\", \"10\", \"12\"] #[\"6\",\"9\",\"10\"] # finished_diag_ids = ['0', '14', '2', '25', '30', '36', '41', '49', '54', '6', '1', '15', '20', '26', '31', '37', '42', '5', '55', '60', '10', '16', '21', '27', '32', '38', '45', '50', '56', '61', '11', '17', '22', '28', '33', '39', '46', '51', '57', '7', '12', '18', '23', '29', '34', '4', '47', '52', '58', '8', '13', '19', '24', '3', '35', '40', '48', '53', '59', '9']\n\nwith open(BATCH_ANOMALY_FILE_NAME, \"r\") as f:\n    anomaly_jsons = json.load(f)\n\nresult_jsons = {}\nfor anomaly_id in anomaly_jsons:\n    anomaly = anomaly_jsons[anomaly_id]\n    alerts = anomaly[\"alerts\"]\n    alert_names = []\n    for alert in alerts:\n        if 'alerts' in alert:\n            alert_names.append(alert['alerts'][0]['labels']['alertname'])\n    result_jsons[anomaly_id] = {\"labels\": anomaly[\"labels\"], \n                                \"alerts\": alert_names,\n                                \"label_\"+method: \"\",\n                                \"diag_\"+method: \"\",\n                                \"solution_\"+method: \"\"}\n\nsplit_size = int(floor(len(anomaly_jsons) / process_num))\n\n\nif __name__=='__main__':\n\n    if not os.path.exists(reports_log_dir_name):\n        Path(reports_log_dir_name).mkdir(parents=True, exist_ok=True)\n    \n    with mp.Pool(processes=process_num) as pool:\n        pool.map(wrapper, range(process_num))\n    \n    pool.terminate()"}
{"type": "source_file", "path": "anomaly_trigger/vacuum_multi.py", "content": "import psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n'''vacuum'''\ndef vacuum(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    db=Database(init())\n    #create a new table\n    print_time()\n\n    # insert some data to be deleted\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n\n    # delete 80% of the rows\n    delete_nrows=int(nrows*0.8)\n    vacuum=f'delete from {table_name} where id < {delete_nrows};'\n    db.execute_sqls(vacuum)\n\n    # do the select , then the vacuum occurs\n    select='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,select,nrows)\n\n    print_time()\n\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 100\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = 60\n    \n    # Number of columns in the table\n    num_columns = 10\n    \n    # Number of rows to insert\n    num_rows = 100\n    \n    # Size of each column (in characters)\n    column_size = 20000\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    vacuum(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)"}
{"type": "source_file", "path": "chains/llmchain_with_history.py", "content": "from server.utils import get_ChatOpenAI\nfrom configs.model_config import LLM_MODELS, TEMPERATURE\nfrom langchain.chains import LLMChain\nfrom langchain.prompts.chat import (\n    ChatPromptTemplate,\n    HumanMessagePromptTemplate,\n)\n\nmodel = get_ChatOpenAI(model_name=LLM_MODELS[0], temperature=TEMPERATURE)\n\n\nhuman_prompt = \"{input}\"\nhuman_message_template = HumanMessagePromptTemplate.from_template(human_prompt)\n\nchat_prompt = ChatPromptTemplate.from_messages(\n    [(\"human\", \"我们来玩成语接龙，我先来，生龙活虎\"),\n     (\"ai\", \"虎头虎脑\"),\n     (\"human\", \"{input}\")])\n\n\nchain = LLMChain(prompt=chat_prompt, llm=model, verbose=True)\nprint(chain({\"input\": \"恼羞成怒\"}))"}
{"type": "source_file", "path": "anomaly_trigger/promethues.py", "content": "import requests\n\n# Prometheus的API URL\nprometheus_api_url = \"http://localhost:9090/api/v1/query\"\n\n# 查询CPU使用率的PromQL，示例仅作为参考，具体查询可能需要调整\ncpu_query = '100 - (avg by (instance) (irate(node_cpu_seconds_total{instance=\"node_exporter:9100\",mode=\"idle\"}[5m])) * 100)'\n\n# 查询内存使用量的PromQL\nmemory_query = '(node_memory_MemTotal_bytes{instance=\"node_exporter:9100\"} - node_memory_MemAvailable_bytes{instance=\"node_exporter:9100\"}) / node_memory_MemTotal_bytes{instance=\"node_exporter:9100\"} * 100'\n\n# 定义一个函数来执行Prometheus查询\ndef query_prometheus(query):\n    response = requests.get(prometheus_api_url, params={'query': query})\n    if response.status_code == 200:\n        print(response)\n        return response.json()\n    else:\n        return f\"Error: {response.status_code}\"\n\ndef restart_decision():\n    cpu_usage = query_prometheus(cpu_query)\n    memory_usage = query_prometheus(memory_query)\n\n    # 提取CPU使用率的值\n    print(\"cpu_usage: \", cpu_usage, \" memory_usage: \", memory_usage)\n    cpu_usage_value = cpu_usage['data']['result'][0]['value'][1] \n    cpu=int(float(cpu_usage_value))\n    # 提取内存使用量的值\n    memory_usage_value = memory_usage['data']['result'][0]['value'][1] \n    mem=int(float(memory_usage_value))\n    # 打印结果\n    print(\"CPU Usage:\", cpu_usage_value, \"%\")\n    print(\"Memory Usage:\", memory_usage_value, \"%\")\n\n    return cpu,mem\n\n\nrestart_decision()  \n"}
{"type": "source_file", "path": "doc2knowledge/chunk2knowledge.py", "content": "from utilss import *\nimport ast\nimport os\n\nglobal llm\n\ndef Summarize(idx, title, content, summary_min_length=400):\n    \n    return llm.Query(SUMMARIZE_PROMPT_MSG + MSG(CONTENT_TEMPLATE.format(idx=idx,title=title,content=content)))[\"content\"] if len(content)>=summary_min_length else content\n\ndef CascadingSummary(chunks_path):\n    \n    nodes = [{\n                'id': parse_id(\"0 root\"),\n                'id_str': '0',\n                'name': \"0 root\",\n                'title': 'root',\n                'content': '',\n                'father': None,\n                'children': [],\n            }]\n    for chunk_name in ListFiles(chunks_path):\n        if chunk_name.endswith('.txt'):\n            nodes.append({\n                'id': parse_id(chunk_name),\n                'id_str': '.'.join([str(x) for x in parse_id(chunk_name)]),\n                'name': chunk_name,\n                'title': chunk_name.split(' ', 1)[-1][:-4],\n                'content': read_txt(pjoin(chunks_path, chunk_name)),\n                'father': None,\n                'children': [],\n            })\n\n    nodes_mapping = {node['id_str']: node for node in nodes}\n    \n    for node1, node2 in itertools.permutations(nodes_mapping.values(), 2):\n        if is_father(node1['name'], node2['name']):\n            nodes_mapping[node1['id_str']]['children'].append(node2['id_str'])\n            nodes_mapping[node2['id_str']]['father'] = node1['id_str']\n    nodes = topo_sort(list(nodes_mapping.values()))\n    \n    nodes_mapping = {node['id_str']: node for node in nodes}\n\n    for i, v in enumerate(TQDM(nodes)):\n        children = sorted([c for c in v['children']], key=lambda x:str2id(x))\n        nodes[i]['index'] = [INDEX_TEMPLATE.format(idx=v['id_str'], title=v['title'])] + [\n            INDEX_TEMPLATE.format(idx=c, title=nodes_mapping[c]['title']) for c in children\n        ]\n        nodes[i]['full_index'] = [INDEX_TEMPLATE.format(idx=v['id_str'], title=v['title'])] + sum(\n            [nodes_mapping[c]['full_index'] for c in children], []\n        )\n        nodes[i]['content_summary'] = Summarize(\n            idx = v['id_str'],\n            title = v['title'],\n            content = v['content'],\n            summary_min_length = args.summary_min_length\n        )\n        nodes[i]['summaries'] = [CONTENT_TEMPLATE.format(idx=v['id_str'], title=v['title'], content=nodes[i]['content_summary'])] + [\n            CONTENT_TEMPLATE.format(idx=nodes_mapping[c]['id_str'], title=nodes_mapping[c]['title'], content=nodes_mapping[c]['summary']) for c in children\n        ]\n        nodes[i]['summary'] = Summarize(\n            idx = v['id_str'],\n            title = v['title'],\n            content = \"\\n\\n\".join(nodes[i]['summaries']),\n            summary_min_length = 0\n        ) if len(nodes[i]['summaries'])>1 else nodes[i]['content_summary']\n        nodes[i]['full_summary'] =  DOCUMENT_VIEW_TEMPALTE.format(\n            summaries = \"\\n\\n\".join(nodes[i]['summaries']),\n            index = \"\\n\".join(nodes[i]['full_index']),\n        )\n\n    return nodes\n\ndef count_num_tokens(messages):\n    return len(str(messages).split())\n\n\ndef ExtractKnowledge(nodes_mapping, root_index, iteration=2, iteration_gap=1, source_file='report_example',  target_file=\"extracted_knowledge.jsonl\"):\n    r = nodes_mapping[root_index]\n    source_sections = [r['name']]\n    \n    RULES_EXTRACTION_PROMPT_MSG[0]['content'] = RULES_EXTRACTION_PROMPT_MSG[0]['content'].replace('${relevant_chapters}', ' '.join(r['children']))\n\n    # relevant_nodes = ['1.5', '1.1', '1.3', '1.2', '1.4']\n    # RULES_EXTRACTION_PROMPT_MSG[0]['content'] = RULES_EXTRACTION_PROMPT_MSG[0]['content'].replace('${relevant_chapters}', str(relevant_nodes))\n\n    if os.path.exists(target_file):\n        return []\n        with open(target_file, 'r') as rf:\n            # if rf content is empty\n            if rf.read() == '':\n                knowledge_strs = {}\n            else:\n                rf.seek(0)\n                knowledge_strs = json.load(rf)    \n    else:\n        knowledge_strs = {}\n\n    messages = RULES_EXTRACTION_PROMPT_MSG + MSG(r['full_summary'])\n    messages[0]['content'] = messages[0]['content'].replace('${existing_rules}', str(knowledge_strs))\n\n    extracted_rules = []\n    used_chapters = []\n\n    # copy the values of source_sections for new_source_sections\n    new_source_sections = source_sections.copy()\n\n    while iteration:\n        print(\"The {}th iteration ...\".format(iteration))\n\n        function_response = llm.Query(messages, functions=[LOOKUP_FUNCTION, SUBMIT_RULE_FUNCTION])\n        #print(\"========================================\")\n        #print(\"Extraction Thought:\\n\" + COLOR1(\"```\\n{0}\\n```\".format(response['content'])))\n        \n        response = {\"function_call\": {}}\n        if function_response != None and \"function_call\" in function_response:\n            response[\"function_call\"][\"name\"] = function_response[\"function_call\"][\"name\"]\n            response[\"function_call\"][\"arguments\"] = function_response[\"function_call\"][\"arguments\"]\n\n        if response[\"function_call\"] != {}:\n            if response[\"function_call\"][\"name\"] == \"submit_rule\":\n                \n                print(\"Extraction Action:\", SUCCESS(\"Submitting a rule...\"))\n                try:\n                    rule = response[\"function_call\"][\"arguments\"]\n\n                    # rule = rule.replace('\"{','{')\n                    # # rule = rule.replace('\"}','}')\n                    # rule = rule.replace('}\"','}')\n                    # rule = rule.replace('\\\\\"', '\"')\n\n                    # if '['  in rule:\n                    #     rule = rule.replace('\"[', '[')\n                    #     rule = rule.replace(']\"', ']')\n                    #     rule = rule.strip()\n\n                    # if rule.startswith('{\\n  \"blocks\": \"') and rule.endswith('\"\\n}'):\n                    #     rule = rule[len('{\\n  \"blocks\": '):-len('\\n}')]\n\n                    rule = ast.literal_eval(rule)\n\n                    if 'blocks' in rule:\n                        rule = rule['blocks']\n                        if isinstance(rule, str):\n                            rule = rule.replace('\\n', '\\\\n')\n                            rule = ast.literal_eval(rule)\n\n                    if isinstance(rule, dict):\n                        rules = [rule]\n                    else:\n                        rules = rule\n\n                    if not os.path.exists(target_file):\n                        with open(target_file, 'w') as wf:\n                            wf.write('')\n                        knowledge_strs = {}\n                    else:\n                        with open(target_file, 'r') as rf:\n                            # if rf content is empty\n                            if rf.read() == '':\n                                knowledge_strs = {}\n                            else:\n                                rf.seek(0)\n                                knowledge_strs = json.load(rf)                            \n\n                    try:\n                        for rule in rules:\n                        # evaluate redundancy\n                            new_knowledge = {'source_sections':str(new_source_sections), 'rule':rule}\n                            # create target_file if not exist\n\n                            not_exist = 1\n                            for knowledge_id in knowledge_strs:\n                                exist_knowledge = str(knowledge_strs[knowledge_id])\n\n                                prompt = LOGICAL_VERIFICATION_PROMPT.format(exist_knowledge=exist_knowledge, new_knowledge=str(new_knowledge))\n                                judgment = llm.Query(MSG(prompt))[\"content\"].split('Answer:')[-1].strip().strip('.').lower().strip()\n                                if \"yes\" in judgment.lower():\n                                    not_exist = 0\n                                    break\n\n                            if not_exist:\n                                extracted_rules.append(rule)\n                                message = \"Success!\"\n                                print(COLOR1(\"```\\n{0}\\n```\".format(rule)))\n\n                                # append to the json file in pretty format\n                                knowledge_strs[str(len(knowledge_strs))] = new_knowledge\n                            else:\n                                knowledge_strs[knowledge_id] = new_knowledge\n                                print(COLOR1(\"```\\n{0}\\n```\".format(rule)))\n                                print(f\"source sections: {str(new_source_sections)}\")\n\n                                print(COLOR2(\"Redundant knowledge.\"))\n\n                            with open(target_file, 'w', encoding='utf-8') as wf:\n                                json.dump(knowledge_strs, wf, ensure_ascii=False, indent=4)\n\n                            # update the prompt (explore similar nodes from scratch)\n                            messages = RULES_EXTRACTION_PROMPT_MSG + MSG(r['full_summary'])\n                            messages[0]['content'] = messages[0]['content'].replace('${existing_rules}', str(knowledge_strs))\n\n                            for chatper in new_source_sections:\n                                if chatper not in used_chapters:\n                                    used_chapters.append(chatper)\n\n                            messages[0]['content'] = messages[0]['content'].replace('${used_chapters}', str(used_chapters))\n\n                        new_source_sections = source_sections.copy()\n                    except:\n                        rules = \"\"\n\n                except Exception as e:\n                    # import pdb; pdb.set_trace()\n                    message = \"Invaid Rule.\"\n                    print(ERROR(message))\n                    rules = \"\"\n                    iteration += 3\n                messages += [{'role': \"function\", \"name\":\"submit_rule\", 'content': str(rules)}]\n                \n            else:\n                idx = json.loads(response[\"function_call\"][\"arguments\"])[\"index\"]\n                print(\"Extraction Action:\", SUCCESS(f\"Looking up '{idx}'...\"))\n                try:\n                    assert idx in nodes_mapping\n\n                    if nodes_mapping[idx]['name'] in new_source_sections:\n                        message = f\"Redundant Source Section {nodes_mapping[idx]['name']}.\"\n                        print(ERROR(message))\n                    else:\n                        new_source_sections.append(nodes_mapping[idx]['name'])\n                        message = nodes_mapping[idx]['content']\n                        print(COLOR1(f\"Document Received.\"))\n                except:\n                    message = \"Invalid Index.\"\n                    print(ERROR(message))\n                messages += [{'role': \"function\", \"name\":\"look_up\", 'content': message}]\n            num_tokens = count_num_tokens(messages)\n            print(\"num_tokens:\", num_tokens)\n            if num_tokens > 2000:\n                break\n        iteration -= 1\n        # print(\"========================================\")\n        if iteration % 10 == 0:\n            print(WARNING(\"Waiting for next 10 iterations ... \"))\n            time.sleep(1)\n        \n    return extracted_rules\n\n\n\n\n\nif __name__==\"__main__\":\n\n    args = HeavenArguments.from_parser([\n        LiteralArgumentDescriptor(\"backend\", default=\"gpt-4\", choices=['gpt-4']),\n        StrArgumentDescriptor(\"doc\", short='d', default=\"docs/enmo/reports\"),\n        StrArgumentDescriptor(\"root_index\", short='r', default=\"0\"), # e.g., section 0\n        IntArgumentDescriptor(\"summary_min_length\", short='l', default=200),\n        IntArgumentDescriptor(\"num_iteration\", short='K', default=30),\n        IntArgumentDescriptor(\"iteration_gap\", short='T', default=1),\n        SwitchArgumentDescriptor(\"clear_cache\", short='c', default=True),\n    ])\n\n    if args.c:\n        clear_cache()\n    target_dir = \"extracted_knowledge\"\n\n    llm = LLMCore(backend=f\"openai_{args.backend}\")\n\n    # get the sections of all the files that within the args.doc directory\n    doc_section_dirs = []\n    for root, dirs, files in os.walk(args.doc):\n        for dir in dirs:\n            doc_section_dirs.append(pjoin(root, dir))\n\n\n    num_with_multi_sections = 0\n\n    for i,section_dir in enumerate(doc_section_dirs):\n        \n        print(f\"Processing {i+1}/{len(doc_section_dirs)}: {section_dir}\")\n\n        target_file = os.path.abspath(os.path.join(args.doc, f\"knowledge_from_{section_dir.split('/')[-2]}.jsonl\"))\n        if os.path.exists(target_file):\n            continue\n\n        if len(ListFiles(section_dir)) > 1:\n            num_with_multi_sections += 1\n            print(f\"Multi-sections: {section_dir}\")\n        else:\n            continue\n\n        # generate the cascading summary index\n        nodes = CascadingSummary(section_dir)\n        nodes_mapping = {node['id_str']: node for node in nodes}\n\n        if (args.root_index) not in nodes_mapping:\n            print(f\"Invalid root index: {section_dir}\")\n            continue        \n\n        extracted_knowlege_chunks = ExtractKnowledge(nodes_mapping, root_index=args.root_index, iteration=args.num_iteration, iteration_gap=args.iteration_gap, source_file=args.doc,target_file=target_file)\n    \n    print(f\"num_with_multi_sections: {num_with_multi_sections}\")"}
{"type": "source_file", "path": "doc2knowledge/doc_to_section.py", "content": "import sys\nsys.path.append(\"..\")\n\nfrom text_splitter.structured_document_splitter import StructuredDocumentSplitter, read_structured_docx\nimport os\n\nif __name__ == \"__main__\":\n\n    ROOT_DIR_NAME = \"docs/enmo/reports\"\n\n    text_splitter = StructuredDocumentSplitter(\n        keep_separator=True,\n        is_separator_regex=True,\n        chunk_size=50,\n        chunk_overlap=0,\n        section_style_prefix=[\"Heading\"]\n    )\n    \n    for root, dirs, files in os.walk(ROOT_DIR_NAME):\n        for file in files:\n            if (file.endswith(\".docx\")) and not file.startswith(\"~$\"):\n                document_path = os.path.join(root, file)\n\n                # split by section structures\n                try:\n                    read_structured_docx(root, document_path)\n                except Exception as e:\n                    import pdb; pdb.set_trace()\n                    print(\"Fail to read file: \", file)\n\n    # ls = []\n    # for file in files:\n    #     if \".doc\" in file:\n    #         try:\n    #             ls.append([file, read_structured_docx(\"../knowledge_base/yun_he_en_mo/\" + file)])\n    #         except:\n    #             print(\"Fail to read file: \", file)\n\n    # for inum, element in enumerate(ls):\n    #     print(\"\\n*************** \", element[0], \" ***************\\n\")\n    #     chunks = text_splitter._split_doc_tree(element[1])"}
{"type": "source_file", "path": "anomaly_trigger/script2code.py", "content": "import re\nwith open('missingindex2.txt', 'r') as file:\n    # 逐行读取文件并将每一行作为字符串添加到列表中\n    lines = [line.strip() for line in file]\n# 输入的字符串\nfor line in lines:\n    input_string = f\"{line}\"\n    anomaly_match = re.search(r'--anomaly\\s+([^\\s]+)', input_string)\n    anomaly = str(anomaly_match.group(1)) if anomaly_match else None\n    threads_match = re.search(r'--threads (\\d+)', input_string)\n    ncolumn_match = re.search(r'--ncolumn (\\d+)', input_string)\n    colsize_match = re.search(r'--colsize (\\d+)', input_string)\n    nrow_match = re.search(r'--nrow (\\d+)', input_string)\n    duration_match=re.search(r'--duration (\\d+)', input_string)\n        # 提取的数字存入变量\n    threads = int(threads_match.group(1)) if threads_match else None\n    ncolumn = int(ncolumn_match.group(1)) if ncolumn_match else None\n    colsize = int(colsize_match.group(1)) if colsize_match else None\n    nrow = int(nrow_match.group(1)) if nrow_match else None\n    duration=int(duration_match.group(1)) if duration_match else None\n    if anomaly == 'INSERT_LARGE_DATA':\n        # 打印提取的数字\n        code=f'''import psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n\n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{{i}} varchar({{colsize}})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {{table_name}} (id int, {{column_definitions}}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {{table_name}}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef insert_large_data(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n\n    print_time()\n    #Delete undeleted tables\n    delete_table(table_name)\n    #create a new table\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    #insert the data\n    #insert_definitions = ', '.join(f'repeat(round(random()*999)::text,{{(colsize//3)}})' for i in range(ncolumns))\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {{colsize}}))' for i in range(ncolumns))\n    insert_data=f'insert into {{table_name}} select generate_series(1,{{nrows}}),{{insert_definitions}}, now();'\n    db.concurrent_execute_sql(threads,duration,insert_data,commit_interval=1)\n\n    #delete the table\n    delete_table(table_name)\n    \n    #print the end time\n    print_time()\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = {threads}\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = {duration}\n    \n    # Number of columns in the table\n    num_columns = {ncolumn}\n    \n    # Number of rows to insert\n    num_rows = {nrow}\n    \n    # Size of each column (in characters)\n    column_size = {colsize}\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    insert_large_data(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\\n\\n\\n\\n'''\n    if anomaly == 'MISSING_INDEXES':\n        code=f'''import psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{{i}} varchar({{colsize}})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {{table_name}} (id int, {{column_definitions}}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {{table_name}}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef missing_index(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    db=Database(init())\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be selected \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {{colsize}}))' for i in range(ncolumns))\n    insert_data=f'insert into {{table_name}} select generate_series(1,{{nrows}}),{{insert_definitions}}, now();'\n    db.execute_sqls(insert_data)  \n\n    #select without the index\n    missing_index='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,missing_index,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    #print the end time\n    print_time()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = {threads}\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = {duration}\n    \n    # Number of columns in the table\n    num_columns = {ncolumn}\n    \n    # Number of rows to insert\n    num_rows = {nrow}\n    \n    # Size of each column (in characters)\n    column_size = {colsize}\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    missing_index(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\\n\\n\\n\\n'''\n\n    if anomaly == 'VACUUM':\n        code=f'''import psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{{i}} varchar({{colsize}})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {{table_name}} (id int, {{column_definitions}}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {{table_name}}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef vacuum(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    db=Database(init())\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be deleted\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {{colsize}}))' for i in range(ncolumns))\n    insert_data=f'insert into {{table_name}} select generate_series(1,{{nrows}}),{{insert_definitions}}, now();' \n    db.execute_sqls(insert_data) \n\n    # delete 80% of the rows\n    delete_nrows=int(nrows*0.8)\n    vacuum=f'delete from {{table_name}} where id < {{delete_nrows}};'\n    db.execute_sqls(vacuum)\n\n    # do the select , then the vacuum occurs\n    select='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,select,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = {threads}\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = {duration}\n    \n    # Number of columns in the table\n    num_columns = {ncolumn}\n    \n    # Number of rows to insert\n    num_rows = {nrow}\n    \n    # Size of each column (in characters)\n    column_size = {colsize}\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    vacuum(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\\n\\n\\n\\n'''\n\n    if anomaly == 'LOCK_CONTENTION':\n        code=f'''import psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{{i}} varchar({{colsize}})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {{table_name}} (id int, {{column_definitions}}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {{table_name}}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef lock_contention(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    # insert some data to be updated \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {{colsize}}))' for i in range(ncolumns))\n    insert_data=f'insert into {{table_name}} select generate_series(1,{{nrows}}),{{insert_definitions}}, now();' \n    db.execute_sqls(insert_data) \n    pool = Pool(threads)\n    for _ in range(threads):\n        pool.apply_async(\n            lock, (table_name, ncolumns, colsize, duration, nrows))\n    pool.close()\n    pool.join()\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\ndef lock(table_name, ncolumns, colsize, duration, nrows):\n    args=init()\n    start = time.time()\n    #lock_contention\n    while time.time()-start < duration:\n        conn = psycopg2.connect(database=args.dbname, user=args.user, password=args.password,\n                                        host=args.host, port=args.port)\n        cur = conn.cursor()\n        while time.time()-start < duration:\n            col_name = random.randint(0, ncolumns-1)\n            row_name = random.randint(1, nrows-1)\n            lock_contention = f'update {{table_name}} set name{{col_name}}=(SELECT substr(md5(random()::text), 1, {{colsize}})) where id ={{row_name}}'\n            #db.concurrent_execute_sql(threads,duration,lock_contention,nrows)\n            cur.execute(lock_contention)\n            conn.commit()\n        conn.commit()\n        conn.close()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = {threads}\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = {duration}\n    \n    # Number of columns in the table\n    num_columns = {ncolumn}\n    \n    # Number of rows to insert\n    num_rows = {nrow}\n    \n    # Size of each column (in characters)\n    column_size = {colsize}\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    lock_contention(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\\n\\n\\n\\n'''\n\n    if anomaly == 'REDUNDANT_INDEX':\n        code=f'''import psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \n    def build_index(self, table_name, idx_num):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        \n        for i in range(0, idx_num):\n            the_sql = 'CREATE INDEX index_' + table_name + '_' + str(i) + ' ON ' + table_name + '(name' + str(i) + ');'\n            print(the_sql)\n            cursor.execute(the_sql)\n\n        \n        self.conn.commit()\n        self.conn.close()\n        return\n\n\n    \n    def drop_index(self,table_name):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        cursor.execute(\"select indexname from pg_indexes where tablename='\"+table_name+\"';\")\n        idxs = cursor.fetchall()\n        for idx in idxs:\n            the_sql = 'DROP INDEX ' + idx[0] + ';'\n            cursor.execute(the_sql)\n            print(the_sql)\n        self.conn.commit()\n        self.conn.close()\n        return\n\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{{i}} varchar({{colsize}})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {{table_name}} (id int, {{column_definitions}}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {{table_name}}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\ndef redundent_index(threads,duration,ncolumns,nrows,colsize,nindex,table_name='table1'):\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    # insert some data to be updated \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {{colsize}}))' for i in range(ncolumns))\n    insert_data=f'insert into {{table_name}} select generate_series(1,{{nrows}}),{{insert_definitions}}, now();' \n    db.execute_sqls(insert_data) \n\n    #initialization of the indexes\n    nindex=int((nindex*ncolumns)/10)\n    db.build_index(table_name,nindex)\n    id_index='CREATE INDEX index_'+table_name+'_id ON '+table_name+'(id);'\n    db.execute_sqls(id_index)\n\n    #lock_contention\n    pool = Pool(threads)\n    for _ in range(threads):\n        pool.apply_async(\n            lock, (table_name, ncolumns, colsize, duration, nrows))\n    pool.close()\n    pool.join()\n\n    #drop the index\n    db.drop_index(table_name)\n\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\n\ndef lock(table_name, ncolumns, colsize, duration, nrows):\n    args=init()\n    start = time.time()\n    #lock_contention\n    while time.time()-start < duration:\n        conn = psycopg2.connect(database=args.dbname, user=args.user, password=args.password,\n                                        host=args.host, port=args.port)\n        cur = conn.cursor()\n        while time.time()-start < duration:\n            col_name = random.randint(0, ncolumns-1)\n            row_name = random.randint(1, nrows-1)\n            lock_contention = f'update {{table_name}} set name{{col_name}}=(SELECT substr(md5(random()::text), 1, {{colsize}})) where id ={{row_name}}'\n            #db.concurrent_execute_sql(threads,duration,lock_contention,nrows)\n            cur.execute(lock_contention)\n            conn.commit()\n        conn.commit()\n        conn.close()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = {threads}\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = {duration}\n    \n    # Number of columns in the table\n    num_columns = {ncolumn}\n    \n    # Number of rows to insert\n    num_rows = {nrow}\n    \n    # Size of each column (in characters)\n    column_size = {colsize}\n    \n    # Table name\n    table_name = 'table1'\n    \n    nindex=6\n    \n    # Call the insert_large_data function\n    redundent_index(num_threads, insert_duration, num_columns, num_rows, column_size, nindex,table_name)\\n\\n\\n\\n'''\n        \n    if anomaly == 'INSERT_LARGE_DATA,IO_CONTENTION':\n        code=f'''import os\nimport datetime\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\nif __name__ == \"__main__\":\n    print_time()\n    command = (\n    \"su - root -c 'cd /sysbench-tpcc-master; \"\n    \"./tpcc.lua --db-driver=pgsql --tables=2 --scale=3 --threads=50 --events=0 \"\n    \"--pgsql-host=xxxx --pgsql-user=xxxx --pgsql-password=xxxx \"\n    \"--pgsql-port=5432 --pgsql-db=tpcc --time=90 --rand-type=uniform --report-interval=10 run'\"\n    )\n\n    os.system(command)\n    print_time()\\n\\n\\n\\n'''\n    if anomaly == 'FETCH_LARGE_DATA,CORRELATED_SUBQUERY':\n        # 打印提取的数字\n        code=f'''import os\nimport re\nimport time\n\nimport psycopg2\n\n\nREPEATCOUNT = 1\nTIMELOGPATH = str(int(time.time())) + \"_tpch_trigger_time_log.txt\"\nTIMELOG = open(TIMELOGPATH, 'w+')\n\n\nclass Database():\n\n    def __init__(self):\n        self.conn = None\n        self.conn = psycopg2.connect(database='tpch',\n                                     user='xxxx',\n                                     password='xxxx',\n                                     host='xxxx',\n                                     port=5432)\n\n    def execute_sql(self, sql):\n        fail = 1\n        cur = self.conn.cursor()\n        i = 0\n        cnt = 3\n        while fail == 1 and i < cnt:\n            try:\n                fail = 0\n                cur.execute(sql)\n            except BaseException as error:\n                fail = 1\n                print(error)\n            res = []\n            if fail == 0:\n                res = cur.fetchall()\n            i = i + 1\n        if fail == 1:\n            # print(\"SQL Execution Fatal!!\", sql)\n            return 0, ''\n        elif fail == 0:\n            return 1, res\n\n\ndef all_sql_files():\n    res_path = \"{{}}/tpch-queries/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    # all_file_list = list(filter(file_filter, os.listdir(res_path)))\n    # all_file_list = sorted(all_file_list, key=custom_sort)\n    all_file_list = [\n        '4.explain.sql']\n\n    print(all_file_list)\n    files_list = []\n    for file in all_file_list:\n        files_list.append(res_path + file)\n    return files_list\n\n\ndef custom_sort(item):\n    # 提取数字和字母部分\n    match = re.match(r'(\\d+)(\\D+)', item)\n    # 将数字部分转换为整数以进行比较\n    num_part = int(match.group(1))\n    # 返回元组以按数字和字母排序\n    return (num_part, match.group(2))\n\n\ndef file_filter(f):\n    if f[-4:] == '.sql' and 'schema' not in f and 'fkindexes' not in f:\n        return True\n    else:\n        return False\n\n\ndef get_sql_from_file(file_name):\n    file = open(file_name)\n    lines = file.readlines().copy()\n    sql = ''\n    for line in lines:\n        sql += line\n    sql = sql.replace('\\n', ' ').replace('   ', ' ').replace('  ', ' ')\n    file.close()\n    return sql\n\n\ndef test_hint_from_file(sql_file):\n    db = Database()\n    sql = get_sql_from_file(sql_file)\n    success, result_cont = db.execute_sql(sql)\n    print(success, result_cont)\n\n\ndef test_all():\n    sql_files = all_sql_files()\n\n    for sql_file in list(sql_files):\n        if sql_file:\n            test_hint_from_file(sql_file)\n\n\ndef test_one():\n    res_path = \"{{}}/tpch-queries/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    test_hint_from_file(res_path + '1.explain.sql')\n\n\nif __name__ == '__main__':\n    for i in range(0, REPEATCOUNT):\n        TIMELOG.write(str(int(time.time()))+\";\")\n        test_all()\n        TIMELOG.write(str(int(time.time()))+\"\\n\")\n        TIMELOG.flush()\n\n    TIMELOG.close()\\n\\n\\n\\n'''\n    if anomaly == 'POOR_JOIN_PERFORMANCE,CPU_CONTENTION':\n        # 打印提取的数字\n        code=f'''import os\nimport re\nimport time\n\nimport psycopg2\n\n\nREPEATCOUNT = 1\nTIMELOGPATH = str(int(time.time())) + \"_job_trigger_time_log.txt\"\nTIMELOG = open(TIMELOGPATH, 'w+')\n\n\nclass Database():\n\n    def __init__(self):\n        self.conn = None\n        self.conn = psycopg2.connect(database='imdbload',\n                                     user='xxxx',\n                                     password='xxxx',\n                                     host='xxxx',\n                                     port=5432)\n\n    def execute_sql(self, sql):\n        fail = 1\n        cur = self.conn.cursor()\n        i = 0\n        cnt = 3\n        while fail == 1 and i < cnt:\n            try:\n                fail = 0\n                cur.execute(sql)\n            except BaseException as error:\n                fail = 1\n                print(error)\n            res = []\n            if fail == 0:\n                res = cur.fetchall()\n            i = i + 1\n        if fail == 1:\n            # print(\"SQL Execution Fatal!!\", sql)\n            return 0, ''\n        elif fail == 0:\n            return 1, res\n\n\ndef all_sql_files():\n    res_path = \"{{}}/join-order-benchmark-master/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    # all_file_list = list(filter(file_filter, os.listdir(res_path)))\n    # all_file_list = sorted(all_file_list, key=custom_sort)\n    all_file_list = [\n        '1a.sql', '1b.sql', '1c.sql', '1d.sql',\n        '2a.sql', '2b.sql', '2c.sql', '2d.sql',\n        '3a.sql', '3b.sql', '3c.sql',\n        '4a.sql', '4b.sql', '4c.sql',\n        '5a.sql', '5b.sql', '5c.sql',\n        '6a.sql', '6b.sql', '6c.sql', '6d.sql', '6e.sql', '6f.sql',\n        '7a.sql', '7b.sql', '7c.sql',\n        '8a.sql', '8b.sql', '8c.sql', '8d.sql',\n        '9a.sql', '9b.sql', '9c.sql', '9d.sql',\n        '10a.sql', '10b.sql', '10c.sql',\n        '11a.sql', '11b.sql', '11c.sql', '11d.sql',\n        '12a.sql', '12b.sql', '12c.sql',\n        '13a.sql', '13b.sql', '13c.sql', '13d.sql',\n        '14a.sql', '14b.sql', '14c.sql',\n        '15a.sql', '15b.sql', '15c.sql', '15d.sql',\n        '16a.sql', '16b.sql', '16c.sql', '16d.sql',\n        '17a.sql', '17b.sql', '17c.sql', '17d.sql', '17e.sql', '17f.sql',\n        '18a.sql', '18b.sql', '18c.sql',\n        '19a.sql', '19b.sql', '19c.sql', '19d.sql',\n        '20a.sql', '20b.sql', '20c.sql',\n        '21a.sql', '21b.sql', '21c.sql',\n        '22a.sql', '22b.sql', '22c.sql', '22d.sql',\n        '23a.sql', '23b.sql', '23c.sql',\n        '24a.sql', '24b.sql',\n        '25a.sql', '25b.sql', '25c.sql',\n        '26a.sql', '26b.sql', '26c.sql',\n        '27a.sql', '27b.sql', '27c.sql',\n        '28a.sql', '28b.sql', '28c.sql',\n        '29a.sql', '29b.sql', '29c.sql',\n        '30a.sql', '30b.sql', '30c.sql',\n        '31a.sql', '31b.sql', '31c.sql',\n        '32a.sql', '32b.sql',\n        '33a.sql', '33b.sql', '33c.sql']\n\n    print(all_file_list)\n    files_list = []\n    for file in all_file_list:\n        files_list.append(res_path + file)\n    return files_list\n\n\ndef custom_sort(item):\n    # 提取数字和字母部分\n    match = re.match(r'(\\d+)(\\D+)', item)\n    # 将数字部分转换为整数以进行比较\n    num_part = int(match.group(1))\n    # 返回元组以按数字和字母排序\n    return (num_part, match.group(2))\n\n\ndef file_filter(f):\n    if f[-4:] == '.sql' and 'schema' not in f and 'fkindexes' not in f:\n        return True\n    else:\n        return False\n\n\ndef get_sql_from_file(file_name):\n    file = open(file_name)\n    lines = file.readlines().copy()\n    sql = ''\n    for line in lines:\n        sql += line\n    sql = sql.replace('\\n', ' ').replace('   ', ' ').replace('  ', ' ')\n    file.close()\n    return sql\n\n\ndef test_hint_from_file(sql_file):\n    db = Database()\n    sql = get_sql_from_file(sql_file)\n    success, result_cont = db.execute_sql(sql)\n    print(success, result_cont)\n\n\ndef test_all():\n    sql_files = all_sql_files()\n\n    for sql_file in list(sql_files)[:-10]:\n        if sql_file:\n            test_hint_from_file(sql_file)\n\n\ndef test_one():\n    res_path = \"{{}}/join-order-benchmark-master/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    test_hint_from_file(res_path + '1a.sql')\n\n\nif __name__ == '__main__':\n    for i in range(0, REPEATCOUNT):\n        TIMELOG.write(str(int(time.time()))+\";\")\n        test_all()\n        TIMELOG.write(str(int(time.time()))+\"\\n\")\n        TIMELOG.flush()\n\n    TIMELOG.close()\n\n)\\n\\n\\n\\n'''\n    with open('m_i_code.txt', 'a') as file:\n        file.write(code)"}
{"type": "source_file", "path": "configs/__init__.py", "content": "from .basic_config import *\nfrom .model_config import *\nfrom .kb_config import *\nfrom .server_config import *\nfrom .prompt_config import *\nfrom .diagnose_config import *\n\n\nVERSION = \"v0.0.1\"\n"}
{"type": "source_file", "path": "copy_config_example.py", "content": "# 用于批量将configs下的.example文件复制并命名为.py文件\nimport os\nimport shutil\n\nif __name__ == \"__main__\":\n    files = os.listdir(\"configs\")\n\n    src_files = [os.path.join(\"configs\", file) for file in files if \".example\" in file]\n\n    for src_file in src_files:\n        tar_file = src_file.replace(\".example\", \"\")\n        shutil.copy(src_file, tar_file)\n"}
{"type": "source_file", "path": "doc2knowledge/doc2knowledge.py", "content": "from utilss import *\nimport ast\nimport sys\nsys.path.append(\"..\")\nfrom text_splitter.structured_document_splitter import StructuredDocumentSplitter, read_structured_docx, write_tree_to_files\n\ndef Initialize(doc, target_dir):\n    CreateFolder(pjoin(doc, target_dir))\n\ndef Summarize(idx, title, content, llm, summary_min_length=400):\n    \n    return llm.Query(SUMMARIZE_PROMPT_MSG + MSG(CONTENT_TEMPLATE.format(idx=idx,title=title,content=content)))[\"content\"] if len(content)>=summary_min_length else content\n\ndef CascadingSummary(args):\n    doc_path = args.doc\n    chunks_path = pjoin(doc_path, \"raw\")\n\n    nodes = [{\n        'id': parse_id(chunk_name),\n        'id_str': '.'.join([str(x) for x in parse_id(chunk_name)]),\n        'name': chunk_name,\n        'title': chunk_name.split(' ', 1)[-1][:-4],\n        'content': read_txt(pjoin(chunks_path, chunk_name)),\n        'father': None,\n        'children': [],\n    } for chunk_name in ListFiles(chunks_path) if chunk_name.endswith('.txt') or chunk_name.endswith('.doc') or chunk_name.endswith('.docx')]\n\n    nodes_mapping = {node['id_str']: node for node in nodes}\n    nodes_structure = {node['id_str']: {k:v for k,v in node.items() if k!='content'} for node in nodes}\n    \n    for node1, node2 in itertools.permutations(nodes_mapping.values(), 2):\n        if is_father(node1['name'], node2['name']):\n            nodes_mapping[node1['id_str']]['children'].append(node2['id_str'])\n            nodes_structure[node1['id_str']]['children'].append(node2['id_str'])\n            nodes_mapping[node2['id_str']]['father'] = node1['id_str']\n            nodes_structure[node2['id_str']]['father'] = node1['id_str']\n    \n    print(\"tree structure:\", nodes_structure)\n\n    nodes = topo_sort(list(nodes_mapping.values()))\n    \n    nodes_mapping = {node['id_str']: node for node in nodes}\n    \n    for i, v in enumerate(TQDM(nodes)):\n        children = sorted([c for c in v['children']], key=lambda x:str2id(x))\n        nodes[i]['index'] = [INDEX_TEMPLATE.format(idx=v['id_str'], title=v['title'])] + [\n            INDEX_TEMPLATE.format(idx=c, title=nodes_mapping[c]['title']) for c in children\n        ]\n        nodes[i]['full_index'] = [INDEX_TEMPLATE.format(idx=v['id_str'], title=v['title'])] + sum(\n            [nodes_mapping[c]['full_index'] for c in children], []\n        )\n        nodes[i]['content_summary'] = Summarize(\n            idx = v['id_str'],\n            title = v['title'],\n            content = v['content'],\n            llm = args.llm,\n            summary_min_length = args.summary_min_length\n        )\n        nodes[i]['summaries'] = [CONTENT_TEMPLATE.format(idx=v['id_str'], title=v['title'], content=nodes[i]['content_summary'])] + [\n            CONTENT_TEMPLATE.format(idx=nodes_mapping[c]['id_str'], title=nodes_mapping[c]['title'], content=nodes_mapping[c]['summary']) for c in children\n        ]\n        nodes[i]['summary'] = Summarize(\n            idx = v['id_str'],\n            title = v['title'],\n            content = \"\\n\\n\".join(nodes[i]['summaries']),\n            llm = args.llm,\n            summary_min_length = 0\n        ) if len(nodes[i]['summaries'])>1 else nodes[i]['content_summary']\n        nodes[i]['full_summary'] =  DOCUMENT_VIEW_TEMPALTE.format(\n            summaries = \"\\n\\n\".join(nodes[i]['summaries']),\n            index = \"\\n\".join(nodes[i]['full_index']),\n        )\n\n    return nodes, nodes_structure\n\ndef count_num_tokens(messages):\n    return len(str(messages).split())\n\n\ndef ExtractKnowledge(nodes_mapping, root_index, llm, iteration=2, iteration_gap=1, source_file='report_example',  target_file=\"extracted_knowledge.jsonl\"):\n    r = nodes_mapping[root_index]\n    source_sections = [r['name']]\n    \n    relevant_nodes = ['1.5', '1.1', '1.3', '1.2', '1.4']\n    #RULES_EXTRACTION_PROMPT_MSG[0]['content'] = RULES_EXTRACTION_PROMPT_MSG[0]['content'].replace('${alert_info}', ' '.join(r['children']))\n    RULES_EXTRACTION_PROMPT_MSG[0]['content'] = RULES_EXTRACTION_PROMPT_MSG[0]['content'].replace('${alert_info}', str(relevant_nodes))\n\n    messages = RULES_EXTRACTION_PROMPT_MSG + MSG(r['full_summary'])\n    extracted_rules = []\n\n    new_source_sections = source_sections\n    fail_time = 0\n    while iteration and fail_time < 10:\n\n        function_response = llm.Query(messages, functions=[LOOKUP_FUNCTION, SUBMIT_RULE_FUNCTION])\n        #print(\"========================================\")\n        #print(\"Extraction Thought:\\n\" + COLOR1(\"```\\n{0}\\n```\".format(response['content'])))\n        \n        response = {\"function_call\": {}}\n        if function_response != None and \"function_call\" in function_response:\n            response[\"function_call\"][\"name\"] = function_response[\"function_call\"][\"name\"]\n            response[\"function_call\"][\"arguments\"] = function_response[\"function_call\"][\"arguments\"]\n\n        if response[\"function_call\"] != {}:\n            if response[\"function_call\"][\"name\"] == \"submit_rule\":\n                \n                print(\"Extraction Action:\", SUCCESS(\"Submitting a rule...\"))\n                try:\n                    rule = response[\"function_call\"][\"arguments\"]\n\n                    print(\"rule:\", rule)\n\n                    # assert (rule.startswith('{\\n  \"rules\": \"') and rule.endswith('\"\\n}'))\n\n                    # rule = rule[len('{\\n  \"rules\": '):-len('\\n}')]\n                    rule = rule.replace('\"{','{')\n                    # rule = rule.replace('\"}','}')\n                    rule = rule.replace('}\"','}')\n                    rule = rule.replace('\\\\\"', '\"')\n\n                    if '['  in rule:\n                        rule = rule.replace('\"[', '[')\n                        rule = rule.replace(']\"', ']')\n                        rule = rule.strip()\n\n                    if not os.path.exists(target_file):\n                        with open(target_file, 'w') as wf:\n                            wf.write('')\n                        knowledge_strs = {}\n                    else:\n                        with open(target_file, 'r') as rf:\n                            # load json file\n                            knowledge_strs = json.load(rf)\n                    \n                    rule = ast.literal_eval(rule)\n\n                    if isinstance(rule, dict):\n                        rules = [rule]\n                    else:\n                        rules = rule\n\n                    used_rules = []\n                    for rule in rules:\n                        # evaluate redundancy                            \n                        new_knowledge = {'source_sections':str(new_source_sections), 'rule':rule}\n                        # create target_file if not exist\n\n                        not_exist = 1\n                        for knowledge_id in knowledge_strs:\n                            exist_knowledge = str(knowledge_strs[knowledge_id])\n\n                            prompt = LOGICAL_VERIFICATION_PROMPT.format(exist_knowledge=exist_knowledge, new_knowledge=str(new_knowledge))\n                            judgment = llm.Query(MSG(prompt))[\"content\"].split('Answer:')[-1].strip().strip('.').lower().strip()\n                            if \"yes\" in judgment.lower():\n                                not_exist = 0\n                                break\n\n                        if not_exist:\n                            extracted_rules.append(rule)\n                            message = \"Success!\"\n                            print(COLOR1(\"```\\n{0}\\n```\".format(rule)))\n\n                            # append to the json file in pretty format\n                            knowledge_strs[str(len(knowledge_strs))] = new_knowledge\n                            used_rules.append(rule)\n                        else:\n                            knowledge_strs[knowledge_id] = new_knowledge\n                            print(COLOR1(\"```\\n{0}\\n```\".format(rule)))\n                            print(f\"source sections: {str(new_source_sections)}\")\n\n                            print(COLOR2(\"Redundant knowledge.\"))\n\n                        with open(target_file, 'w') as wf:\n                            json.dump(knowledge_strs, wf, indent=4)\n\n                        new_source_sections = source_sections\n                \n                except:\n                    message = \"Invaid Rule.\"\n                    print(ERROR(message))\n                    rules = \"\"\n\n                if used_rules == []:\n                    fail_time += 1\n                messages += [{'role': \"function\", \"name\":\"submit_rule\", 'content': used_rules}]\n\n            else:\n                idx = json.loads(response[\"function_call\"][\"arguments\"])[\"index\"]\n                print(\"Extraction Action:\", SUCCESS(f\"Looking up '{idx}'...\"))\n                try:\n                    assert idx in nodes_mapping\n                    new_source_sections.append(nodes_mapping[idx]['name'])\n                    message = nodes_mapping[idx]['content']\n                    print(COLOR1(f\"Document Received.\"))\n                except:\n                    message = \"Invalid Index.\"\n                    print(ERROR(message))\n                messages += [{'role': \"function\", \"name\":\"look_up\", 'content': message}]\n\n            iteration -= 1\n            # print(\"========================================\")\n            if iteration:\n                print(WARNING(\"Waiting for next iteration...\"))\n                time.sleep(1)\n\n        num_tokens = count_num_tokens(messages)\n        print(\"used tokens:\", num_tokens)\n        if num_tokens > 8000:\n            break\n\n    return extracted_rules\n\n\nclass DocumentExtractionArgs:\n    def __init__(self, \n                 backend=\"openai_gpt-4\", \n                 doc=\"./docs/overall_guide\", \n                 root_index=\"1\", \n                 summary_min_length=400, \n                 num_iteration=200, \n                 iteration_gap=200, \n                 clear_cache=True):\n\n        self.backend = backend\n        self.doc = doc\n        self.root_index = root_index\n        self.summary_min_length = summary_min_length\n        self.num_iteration = num_iteration\n        self.iteration_gap = iteration_gap\n        self.clear_cache = clear_cache\n        self.llm = None\n\ndef DocumentExtraction(args):\n\n    if args.clear_cache:\n        clear_cache()\n    target_dir = \"extracted_knowledge_test\"\n\n    llm = LLMCore(backend=args.backend)\n    args.llm = llm\n\n    # First: input the document\n    Initialize(args.doc, target_dir)\n\n    # Second: generate the cascading summary index\n    nodes, nodes_structure = CascadingSummary(args)\n\n    nodes_mapping = {node['id_str']: node for node in nodes}\n\n    assert (args.root_index) in nodes_mapping, f\"Invalid root index: {args.root_index}\"\n\n    target_file = args.doc + '/' + target_dir + '/' + \"extracted_knowledge_test.jsonl\"\n    extracted_rules = ExtractKnowledge(nodes_mapping, root_index=args.root_index, llm=args.llm, iteration=args.num_iteration, iteration_gap=args.iteration_gap, source_file=args.doc,target_file=target_file)\n\ndef document_split(doc_path: str, doc_name: str):\n    text_splitter = StructuredDocumentSplitter(\n        keep_separator=True,\n        is_separator_regex=True,\n        chunk_size=50,\n        chunk_overlap=0,\n        section_style_prefix=[\"Heading\"]\n    )\n\n    # try:\n    section_list = read_structured_docx(doc_path + doc_name)\n\n    if not os.path.exists(doc_path+ \"/raw\"):\n        os.mkdir(doc_path+ \"/raw\")\n    write_tree_to_files(section_list, doc_path+ \"/raw\")\n    \n    # except:\n    #     raise Exception(\"Fail to read file: \", doc_path)\n\n\nif __name__==\"__main__\":\n\n    document_split(\"./docs/test_enmo/\", \"数据库故障处理应急方案.docx\")\n\n    args_obj = DocumentExtractionArgs(backend=\"openai_gpt-4\", doc=\"./docs/test_enmo\")\n\n    is_success = DocumentExtraction(args_obj)"}
{"type": "source_file", "path": "doc2knowledge/utilss.py", "content": "from pyheaven import *\nfrom prompts import *\n\nimport time\nfrom openai import OpenAI\nimport itertools\nimport requests\nfrom termcolor import colored\n\n# config utils\ndef get_config(key):\n    return LoadJson(\"config.json\")[key]\n\ndef get_cache(key):\n    key = key.lower().strip()\n\n    if not ExistFile(\"cache.json\"):\n        SaveJson(dict(), \"cache.json\")\n    \n    cache = LoadJson(\"cache.json\")\n    if key in cache:\n        return cache[key]\n        # crea\n    return None\n\n# cache utils\ndef update_cache(key, value, update=False):\n    key = key.lower().strip()\n    cache = LoadJson(\"cache.json\")\n    if update or (key not in cache):\n        cache[key] = value\n    SaveJson(cache, \"cache.json\")\n\ndef clear_cache():\n    SaveJson(dict(), \"cache.json\")\n\n# file utils\ndef str2id(id_str):\n    return tuple(int(j) for j in id_str.split('.'))\n\ndef parse_id(file_name):\n    return str2id(file_name.split(' ')[0])\n\ndef parse_depth(file_name):\n    return len(parse_id(file_name))\n\ndef is_father(file_name1, file_name2):\n\n    id1 = parse_id(file_name1)\n    id2 = parse_id(file_name2)\n    if id1 == id2[:-1]:\n        return True\n    elif id1 == (0,) and len(id2) == 1 and id2[0] > 0:\n        return True\n\n    return False\n\n    return parse_id(file_name1) == parse_id(file_name2)[:-1]\n\ndef id_sort(nodes, reverse=False):\n    return sorted(nodes, key=lambda x: x['id'], reverse=reverse)\n\ndef topo_sort(nodes):\n    nodes = id_sort(nodes)\n    for i, node in enumerate(nodes):\n        nodes[i]['book'] = len(node['children'])\n    nodes = {node['id_str']: node for node in nodes}\n    sorted_nodes = [nodes[key] for key in nodes if nodes[key]['book']==0]; head = 0\n    while head < len(sorted_nodes):\n        v = sorted_nodes[head]; head += 1\n        if v['father']:\n            nodes[v['father']]['book'] -= 1\n            if not nodes[v['father']]['book']:\n                sorted_nodes.append(nodes[v['father']])\n    return [{k:v for k,v in node.items() if k!='book'} for node in sorted_nodes]\n\ndef read_txt(file_path):\n    assert ExistFile(file_path), f\"File not found: {file_path}\"\n    with open(file_path, \"r\") as f:\n        return f.read().strip()\n\n# openai utils\nclass LLMCore(object):\n    def __init__(self, backend=\"openai_gpt-3.5-turbo\"):\n        self.backend = backend\n        if self.backend.startswith(\"openai_\"):\n\n            # self.config = get_config('openai-api')\n            # self.client = OpenAI(\n            #     api_key=self.config['api_key'],\n            #     organization = self.config['organization']\n            # )\n            \n            self.model = backend.split('_')[-1]\n            \n        else:\n            pass\n        \n    def Query(self, messages, temperature=0, functions=list(), retry_gap=0.1, timeout=10):\n        \n        \n        identifier = \"|\".join([self.backend, str(messages)] + ([str(functions)] if functions else []))\n        \n        #response = get_cache(identifier)\n        # if response is not None:\n        #     return response\n\n        api_key = os.environ.get(\"OPENAI_API_KEY\")\n        headers = {\n            \"Content-Type\": \"application/json\",\n            \"Authorization\": \"Bearer \" + api_key\n        }\n        url = \"https://api.aiaiapi.com/v1/chat/completions\"\n\n        cur_timeout = timeout\n        while cur_timeout>0:\n            try:\n                if functions:\n                    # assert (self.model=='gpt-4'), f\"Functions are only supported in 'gpt-4'!\"\n                    \n                    payload = {\n                        \"model\": \"gpt-4-1106-preview\", # \"gpt-4-1106-preview\", # \"gpt-3.5-turbo-16k\",#\"gpt-4-32k-0613\",\n                        \"messages\": messages,\n                        \"functions\": functions,\n                        \"temperature\": temperature,\n                    }\n\n                    response = requests.post(url, json=payload, headers=headers)\n\n\n                    # response = self.client.chat.completions.create(\n                    #     model = self.model,\n                    #     messages = messages,\n                    #     functions = functions,\n                    #     temperature = temperature,\n                    # )\n                else:\n\n                    payload = {\n                        \"model\": \"gpt-4-1106-preview\", # \"gpt-4-1106-preview\", # \"gpt-3.5-turbo-16k\",#\"gpt-4-32k-0613\",\n                        \"messages\": messages,\n                        \"temperature\": temperature,\n                    }\n\n                    response = requests.post(url, json=payload, headers=headers)\n\n                    # response = self.client.chat.completions.create(\n                    #     model = self.model,\n                    #     messages = messages,\n                    #     temperature = temperature,\n                    # )\n                \n                output = json.loads(response.text)\n                \n                if \"choices\" in output:\n                    output = output[\"choices\"][0][\"message\"]\n                    return output\n                else:\n                    if cur_timeout % timeout == 0:\n                        print(colored(\"Fail to prase the response: \" + str(response.text), \"red\"))\n\n                # response = response.choices[0].message\n\n                # update_cache(identifier, response)\n\n            except Exception as e:\n                # print(ERROR(e))\n                time.sleep(retry_gap)\n            \n            cur_timeout -= 1\n        \n        return None"}
{"type": "source_file", "path": "doc2knowledge/knowledge_clustering.py", "content": "from multiagents.llms.sentence_embedding import sentence_embedding\n\nfrom openai import OpenAI\nimport numpy as np\nimport json\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.decomposition import PCA\nfrom sklearn.decomposition import FastICA\nfrom sklearn.decomposition import TruncatedSVD\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nimport matplotlib.pyplot as plt\nimport os\nimport requests\nimport time\nimport ast\n\nexisting_knowledge_dir = './docs/extracted_knowledge_chunks'\nembedding_file_name = './embeddings_array_v2.npy'\nfile_names = os.listdir(existing_knowledge_dir)\n\ntexts = []\nlabels = []\nfor i,file_name in enumerate(file_names):\n    print(file_name)\n    if \"jsonl\" in file_name:\n        # read content split by '\\n\\n'\n        with open(existing_knowledge_dir+f'/{file_name}', 'r') as rf:\n            prefix= file_name.split('.')[0]\n            content = rf.read()\n            content = content.split('\\n\\n')\n            for text in content:\n                \n                if text == '':\n                    continue\n                labels.append(i)\n                text = text.strip()\n                # json.loads(data_string.replace(\"'\", \"\\\"\").replace(\"\\\\\\\"\", \"'\"))\n                try:\n                    text = ast.literal_eval(text)\n                except:\n                    print(f\"[invalid chunk] ({prefix})\", text)\n                    \n                if prefix not in text['name']:\n                    text['name'] = prefix + '_' + text['name']\n                texts.append(text)\n\n\nif not os.path.exists(embedding_file_name):\n\n    # Get embeddings for each text\n    embeddings = []\n    for i,text in enumerate(texts):\n        embedding = sentence_embedding(text[\"name\"])\n        embeddings.append(embedding)\n        print(f\"embedded {i} text\")\n\n    # Convert embeddings list to a NumPy array\n    embeddings_array = np.array(embeddings)\n    np.save(embedding_file_name, embeddings_array)\nelse:\n    # reload embeddings_array from file\n    embeddings_array = np.load(embedding_file_name)\n\n\nsvd = PCA(n_components=3)\nreduced_embeddings = svd.fit_transform(embeddings_array)\n\n# Plotting in 3-D\nfig = plt.figure(figsize=(10, 6))\nfig.patch.set_facecolor('none')\nax = fig.add_subplot(projection='3d')\nax.xaxis.pane.fill = False\nax.yaxis.pane.fill = False\nax.zaxis.pane.fill = False\nax.grid(True, linestyle='dotted', linewidth=0.5, color='black')\n\n\nscatter = ax.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], reduced_embeddings[:, 2], c=labels, cmap='viridis')\n\n# plt.title(\"Knowledge Clustering (Ada-002)\")\ncbar = fig.colorbar(scatter, ax=ax, shrink=0.7)\n\n#plt.colorbar(scatter)\nax.set_xlabel(\"PCA 1\")\nax.set_ylabel(\"PCA 2\")\nax.set_zlabel(\"PCA 3\")\n\nunique_labels = np.unique(labels)\nlabel_names = [\"index\", \"\", \"workload\", \"I/O\", \"writes\", \"memory\", \"\", \"CPU\", \"query\", \"\"]\n\nax.text(-0.12357282, -0.02821038, -0.08682948, \"index\", fontsize=12, weight='bold', ha='center', va='center')\nax.text(0.24026489, -0.00548978, 0.10369949, \"workload\", fontsize=12, weight='bold', ha='center', va='center')\nax.text(-0.16701542, -0.0196591 ,  0.22820786, \"I/O\", fontsize=12, weight='bold', ha='center', va='center')\nax.text(-0.14342373, -0.06689665,  0.00210631, \"writes\", fontsize=12, weight='bold', ha='center', va='center')\nax.text(-0.15936546,  0.1986972 , -0.06664728, \"memory\", fontsize=12, weight='bold', ha='center', va='center')\nax.text(-0.11849676,  0.17963724, -0.004809, \"CPU\", fontsize=12, weight='bold', ha='center', va='center')\nax.text(-0.18277633, -0.22516701, -0.21521835, \"query\", fontsize=12, weight='bold', ha='center', va='center')\n\nax.set_xlim(-0.3, 0.2)\nax.set_ylim(-0.2, 0.45)\nax.set_zlim(-0.3, 0.2)\n\n# for label in unique_labels:\n#     centroid = np.mean(reduced_embeddings[labels == label], axis=0)\n#     ax.text(centroid[0], centroid[1], centroid[2], str(label_names[int(label)]), fontsize=12, weight='bold', ha='center', va='center')\n\nplt.savefig('./knowledge_clustering_3d.png')\n\n"}
{"type": "source_file", "path": "doc2knowledge/__init__.py", "content": "from utilss import *"}
{"type": "source_file", "path": "init_database.py", "content": "import sys\nsys.path.append(\".\")\nfrom server.knowledge_base.migrate import (create_tables, reset_tables, import_from_db,\n                                           folder2db, prune_db_docs, prune_folder_files)\nfrom configs.model_config import NLTK_DATA_PATH, EMBEDDING_MODEL\nimport nltk\nnltk.data.path = [NLTK_DATA_PATH] + nltk.data.path\nfrom datetime import datetime\nimport sys\n\n\nif __name__ == \"__main__\":\n    import argparse\n    \n    parser = argparse.ArgumentParser(description=\"please specify only one operate method once time.\")\n\n    parser.add_argument(\n        \"-r\",\n        \"--recreate-vs\",\n        action=\"store_true\",\n        help=('''\n            recreate vector store.\n            use this option if you have copied document files to the content folder, but vector store has not been populated or DEFAUL_VS_TYPE/EMBEDDING_MODEL changed.\n            '''\n        )\n    )\n    \n    parser.add_argument(\n        \"--create-tables\",\n        action=\"store_true\",\n        help=(\"create empty tables if not existed\")\n    )\n    \n    parser.add_argument(\n        \"--clear-tables\",\n        action=\"store_true\",\n        help=(\"create empty tables, or drop the database tables before recreate vector stores\")\n    )\n\n    parser.add_argument(\n        \"--import-db\",\n        help=\"import tables from specified sqlite database\"\n    )\n\n    parser.add_argument(\n        \"-u\",\n        \"--update-in-db\",\n        action=\"store_true\",\n        help=('''\n            update vector store for files exist in database.\n            use this option if you want to recreate vectors for files exist in db and skip files exist in local folder only.\n            '''\n        )\n    )\n\n    parser.add_argument(\n        \"-i\",\n        \"--increament\",\n        action=\"store_true\",\n        help=('''\n            update vector store for files exist in local folder and not exist in database.\n            use this option if you want to create vectors increamentally.\n            '''\n        )\n    )\n\n    parser.add_argument(\n        \"--prune-db\",\n        action=\"store_true\",\n        help=('''\n            delete docs in database that not existed in local folder.\n            it is used to delete database docs after user deleted some doc files in file browser\n            '''\n        )\n    )\n\n    parser.add_argument(\n        \"--prune-folder\",\n        action=\"store_true\",\n        help=('''\n            delete doc files in local folder that not existed in database.\n            is is used to free local disk space by delete unused doc files.\n            '''\n        )\n    )\n\n    parser.add_argument(\n        \"-n\",\n        \"--kb-name\",\n        type=str,\n        nargs=\"+\",\n        default=[],\n        help=(\"specify knowledge base names to operate on. default is all folders exist in KB_ROOT_PATH.\")\n    )\n\n    parser.add_argument(\n        \"-e\",\n        \"--embed-model\",\n        type=str,\n        default=EMBEDDING_MODEL,\n        help=(\"specify embeddings model.\")\n    )\n\n    args = parser.parse_args()\n    start_time = datetime.now()\n    \n    if args.create_tables:\n        create_tables() # confirm tables exist\n\n    if args.clear_tables:\n        reset_tables()\n        print(\"database tables reseted\")\n\n    if args.recreate_vs:\n        create_tables()\n        print(\"recreating all vector stores\")\n        folder2db(kb_names=args.kb_name, mode=\"recreate_vs\", embed_model=args.embed_model)\n    elif args.import_db:\n        import_from_db(args.import_db)\n    elif args.update_in_db:\n        folder2db(kb_names=args.kb_name, mode=\"update_in_db\", embed_model=args.embed_model)\n    elif args.increament:\n        folder2db(kb_names=args.kb_name, mode=\"increament\", embed_model=args.embed_model)\n    elif args.prune_db:\n        prune_db_docs(args.kb_name)\n    elif args.prune_folder:\n        prune_folder_files(args.kb_name)\n\n    end_time = datetime.now()\n    print(f\"总计用时： {end_time-start_time}\")"}
{"type": "source_file", "path": "multiagents/agent_conf/__init__.py", "content": "import os\nimport yaml\n\nfrom .output_parser import DBDiag\nfrom .qwen_output_parser import QwenDBDiag"}
{"type": "source_file", "path": "document_loaders/myimgloader.py", "content": "from typing import List\nfrom langchain.document_loaders.unstructured import UnstructuredFileLoader\n\n\nclass RapidOCRLoader(UnstructuredFileLoader):\n    def _get_elements(self) -> List:\n        def img2text(filepath):\n            from rapidocr_onnxruntime import RapidOCR\n            resp = \"\"\n            ocr = RapidOCR()\n            result, _ = ocr(filepath)\n            if result:\n                ocr_result = [line[1] for line in result]\n                resp += \"\\n\".join(ocr_result)\n            return resp\n\n        text = img2text(self.file_path)\n        from unstructured.partition.text import partition_text\n        return partition_text(text=text, **self.unstructured_kwargs)\n\n\nif __name__ == \"__main__\":\n    loader = RapidOCRLoader(file_path=\"../tests/samples/ocr_test.jpg\")\n    docs = loader.load()\n    print(docs)\n"}
{"type": "source_file", "path": "doc2knowledge/our_text_embedding.py", "content": "from openai import OpenAI\nimport numpy as np\nimport json\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.decomposition import PCA\nfrom sklearn.decomposition import FastICA\nfrom sklearn.decomposition import TruncatedSVD\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nimport matplotlib.pyplot as plt\nimport os\nimport requests\nimport time\nimport ast\n\ntexts = []\n# with open('root_causes_dbmind.jsonl', 'r') as rf:\n#     # convert the file content into list format\n#     samples = json.load(rf)\n#     for sample in samples:\n#         texts.append({\"cause_name\": sample['cause_name'], \"desc\": sample['desc'], \"metrics\": sample['metrics']})\n\nwith open('./docs/case_guide/extracted_knowledge/extracted_knowledge_from_chunks.jsonl', 'r') as rf:\n    # convert the file content into list format\n    samples = json.load(rf)\n\n    for sample_id in samples:\n        texts.append({\"cause_name\": samples[str(sample_id)]['name'], \"desc\": samples[str(sample_id)]['content'], \"steps\": samples[str(sample_id)]['steps'], \"metrics\": str(samples[str(sample_id)]['metrics'])})\n\nwith open('./docs/overall_guide/extracted_knowledge/extracted_knowledge_from_chunks.jsonl', 'r') as rf:\n    # convert the file content into list format\n    samples = json.load(rf)\n    for sample_id in samples:\n        texts.append({\"cause_name\": samples[str(sample_id)]['name'], \"desc\": samples[str(sample_id)]['content'], \"steps\": samples[str(sample_id)]['steps'], \"metrics\": str(samples[str(sample_id)]['metrics'])})\n\n\nexisting_knowledge_dir = './docs/extracted_knowledge_chunks'\nfile_names = os.listdir(existing_knowledge_dir)\n\nfor i,file_name in enumerate(file_names):\n    if \"jsonl\" in file_name:\n        new_texts = []\n        with open(existing_knowledge_dir+f'/{file_name}', 'r') as rf:\n            prefix= file_name.split('.')[0]\n            content = rf.read()\n            content = content.split('\\n\\n')\n            for text in content:\n                if text == '':\n                    continue\n                \n                text = text.strip()\n                # json.loads(data_string.replace(\"'\", \"\\\"\").replace(\"\\\\\\\"\", \"'\"))\n                try:\n                    text = ast.literal_eval(text)\n                except:\n                    print(f\"[invalid chunk] ({prefix})\", text)\n                for t in texts:\n                    if text['content'] in t['desc']:\n                        new_texts.append(t)\n                        break\n            \n            # write new_texts into file in pretty format with json.dumps\n            # print(f\"prefix: {prefix}, num: {len(new_texts)}\")\n            # with open(f'./docs/knowledge_chunks/dbgpt_usage/{prefix}.jsonl', 'w') as wf:\n            #     # write in list\n            #     wf.write(json.dumps(new_texts, indent=4))\n\n\ntexts = []\nlabels = []\nfor i,file_name in enumerate(file_names):\n    print(file_name)\n    if \"jsonl\" in file_name:\n        # read content split by '\\n\\n'\n        with open(existing_knowledge_dir+f'/{file_name}', 'r') as rf:\n            prefix= file_name.split('.')[0]\n            content = rf.read()\n            content = content.split('\\n\\n')\n            for text in content:\n                \n                if text == '':\n                    continue\n                labels.append(i)\n                text = text.strip()\n                # json.loads(data_string.replace(\"'\", \"\\\"\").replace(\"\\\\\\\"\", \"'\"))\n                try:\n                    text = ast.literal_eval(text)\n                except:\n                    print(f\"[invalid chunk] ({prefix})\", text)\n                    \n                if prefix not in text['name']:\n                    text['name'] = prefix + '_' + text['name']\n                texts.append(text)\n\n## add similar text (x1)\napi_key = os.environ.get(\"OPENAI_API_KEY\")\nheaders = {\n    \"Content-Type\": \"application/json\",\n    \"Authorization\": \"Bearer \" + api_key\n}\nurl = \"https://api.aiaiapi.com/v1/chat/completions\"\n\n# if './embeddings_array.npy' not exists\n# if not os.path.exists('./embeddings_array.npy'):\n\nold_texts = []\nfor text in texts:\n    old_texts.append(text)\n\nprint(\"total num: \", len(old_texts))\n\nfor i,text in enumerate(old_texts):\n\n    prompt = f\"Given the following knowledge block:\\n{text['content']}\\n\\nGive a similar description of the knowledge block:\\n\"\n\n    message = [{'role': 'user', 'content': prompt}]\n    payload = {\n        \"model\": \"gpt-3.5-turbo-16k\", # \"gpt-4-1106-preview\", # \"gpt-3.5-turbo-16k\",#\"gpt-4-32k-0613\",\n        \"messages\": message,\n        \"temperature\": 0.5,\n        \"max_tokens\": 32,\n        \"top_p\": 1,\n        \"frequency_penalty\": 0,\n        \"presence_penalty\": 0,\n        \"stop\": [\"\\n\\n\"]\n    }\n\n    timeout=10\n    ok = 0\n    while timeout>0:\n        try:\n            response = requests.post(url, json=payload, headers=headers)\n            ok = 1\n            break\n        except Exception as e:\n            time.sleep(.01)\n            timeout -= 1\n    \n    if ok == 0:\n        raise Exception(\"Failed to get response from API!\")\n\n    new_text = {}\n    new_text['name'] = text['name']\n    new_text['content'] = json.loads(response.text)[\"choices\"][0][\"message\"][\"content\"]\n\n    labels.append(labels[texts.index(text)])\n    texts.append(text)\n\n    print(f\"enriched {i} text\")\n# import pdb; pdb.set_trace()\n\n\n# Get embeddings for each text\napi_key = os.environ.get(\"OPENAI_API_KEY\")\nheaders = {\n    \"Content-Type\": \"application/json\",\n    \"Authorization\": \"Bearer \" + api_key\n}\nurl = \"https://api.aiaiapi.com/v1/embeddings\"\n\nembeddings = []\nfor i,text in enumerate(texts):\n    payload = {\n        \"input\": [text[\"name\"]],\n        \"model\": \"text-embedding-ada-002\"        \n    }\n\n    timeout=10\n    ok = 0\n    while timeout>0:\n        try:\n            response = requests.post(url, json=payload, headers=headers)\n            ok = 1\n            break\n        except Exception as e:\n            time.sleep(.01)\n            timeout -= 1\n    \n    if ok == 0:\n        raise Exception(\"Failed to get response from API!\")\n\n    embedding = json.loads(response.text)['data'][0]['embedding']\n    embeddings.append(embedding)\n    print(f\"embedded {i} text\")\n\n# Convert embeddings list to a NumPy array\nembeddings_array = np.array(embeddings)\nnp.save('./embeddings_array.npy', embeddings_array)\n\n\n# reload embeddings_array from file\nembeddings_array = np.load('./embeddings_array.npy')\n# import pdb; pdb.set_trace()\nlabels = labels + labels\n\nsvd = PCA(n_components=3)\nreduced_embeddings = svd.fit_transform(embeddings_array)\n\n\n# plt.figure(figsize=(10, 6))\n# scatter = plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], c=labels, cmap='viridis', s=10)\n# plt.title(\"Knowledge Clustering (Ada-002)\")\n# plt.colorbar(scatter)\n# plt.xlabel(\"PCA 1\")\n# plt.ylabel(\"PCA 2\")\n# # save the figure\n# plt.savefig('./knowledge_clustering.png')\n# exit(1)\n\n# Plotting in 3-D\nfig = plt.figure(figsize=(10, 6))\nfig.patch.set_facecolor('none')\nax = fig.add_subplot(projection='3d')\nax.xaxis.pane.fill = False\nax.yaxis.pane.fill = False\nax.zaxis.pane.fill = False\nax.grid(True, linestyle='dotted', linewidth=0.5, color='black')\n\nscatter = ax.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], reduced_embeddings[:, 2], c=labels, cmap='viridis')\n\n# plt.title(\"Knowledge Clustering (Ada-002)\")\ncbar = fig.colorbar(scatter, ax=ax, shrink=0.7)\n\n#plt.colorbar(scatter)\nax.set_xlabel(\"SVD 1\")\nax.set_ylabel(\"SVD 2\")\nax.set_zlabel(\"SVD 3\")\n\nunique_labels = np.unique(labels)\nlabel_names = [\"index\", \"\", \"workload\", \"I/O\", \"writes\", \"memory\", \"\", \"CPU\", \"query\", \"\"]\n\nax.text(-0.12357282, -0.02821038, -0.08682948, \"index\", fontsize=12, weight='bold', ha='center', va='center')\nax.text(0.24026489, -0.00548978, 0.10369949, \"workload\", fontsize=12, weight='bold', ha='center', va='center')\nax.text(-0.16701542, -0.0196591 ,  0.22820786, \"I/O\", fontsize=12, weight='bold', ha='center', va='center')\nax.text(-0.14342373, -0.06689665,  0.00210631, \"writes\", fontsize=12, weight='bold', ha='center', va='center')\nax.text(-0.15936546,  0.1986972 , -0.06664728, \"memory\", fontsize=12, weight='bold', ha='center', va='center')\nax.text(-0.11849676,  0.17963724, -0.004809, \"CPU\", fontsize=12, weight='bold', ha='center', va='center')\nax.text(-0.18277633, -0.22516701, -0.21521835, \"query\", fontsize=12, weight='bold', ha='center', va='center')\n\nax.set_xlim(-0.3, 0.2)\nax.set_ylim(-0.2, 0.45)\nax.set_zlim(-0.3, 0.2)\n\n# for label in unique_labels:\n#     centroid = np.mean(reduced_embeddings[labels == label], axis=0)\n#     import pdb; pdb.set_trace()\n#     ax.text(centroid[0], centroid[1], centroid[2], str(label_names[int(label)]), fontsize=12, weight='bold', ha='center', va='center')\n\n# save the figure\nplt.savefig('./knowledge_clustering_3d.png')\nexit(1)\n\n\n# root_causes_dbmind.jsonl\n## read from the json file \ntexts = []\nwith open('root_causes_dbmind.jsonl', 'r') as rf:\n    # convert the file content into list format\n    samples = json.load(rf)\n    for sample in samples:\n        texts.append({\"name\": sample['cause_name'], \"content\": sample['desc']})\n\nwith open('./docs/case_guide/extracted_knowledge/extracted_knowledge_from_chunks.jsonl', 'r') as rf:\n    # convert the file content into list format\n    samples = json.load(rf)\n    for sample_id in samples:\n        texts.append({\"name\": samples[str(sample_id)]['name'], \"content\": samples[str(sample_id)]['content']})\n        \n\nwith open('./docs/overall_guide/extracted_knowledge/extracted_knowledge_from_chunks.jsonl', 'r') as rf:\n    # convert the file content into list format\n    samples = json.load(rf)\n    for sample_id in samples:\n        texts.append({\"name\": samples[str(sample_id)]['name'], \"content\": samples[str(sample_id)]['content']})\n\n# Your OpenAI client setup\napi_key = os.environ.get(\"OPENAI_API_KEY\")\nheaders = {\n    \"Content-Type\": \"application/json\",\n    \"Authorization\": \"Bearer \" + api_key\n}\nurl = \"https://api.aiaiapi.com/v1/embeddings\"\n\n# Get embeddings for each text\n# embeddings = []\n# for text in texts:\n#     payload = {\n#         \"input\": [text[\"name\"]],\n#         \"model\": \"text-embedding-ada-002\"        \n#     }\n\n#     timeout=10\n#     ok = 0\n#     while timeout>0:\n#         try:\n#             response = requests.post(url, json=payload, headers=headers)\n#             ok = 1\n#             break\n#         except Exception as e:\n#             time.sleep(.01)\n#             timeout -= 1\n    \n#     if ok == 0:\n#         raise Exception(\"Failed to get response from API!\")\n\n#     embedding = json.loads(response.text)['data'][0]['embedding']\n#     embeddings.append(embedding)\n\n# # Convert embeddings list to a NumPy array\n# embeddings_array = np.array(embeddings)\n\n# # save embeddings_array to file\n# np.save('./embeddings_array.npy', embeddings_array)\n\n# reload embeddings_array from file\nembeddings_array = np.load('./embeddings_array.npy')\n\n# enrich embeddings_array with more similar embeddings with noises\n# from sklearn.neighbors import NearestNeighbors\n# neigh = NearestNeighbors(n_neighbors=2)\n# neigh.fit(embeddings_array)\n# distances, indices = neigh.kneighbors(embeddings_array)\n# print(indices)\n# print(distances)\n## add more similar embeddings (for each embedding, add 10 nearby embeddings) with noises (distances) into embeddings_array\n\n# for i in range(len(indices)):\n#     for j in range(10):\n#         embeddings_array = np.append(embeddings_array, [embeddings_array[indices[i][1]] + np.random.normal(0, distances[i][1], embeddings_array[0].shape)], axis=0)\n\n'''\nvectorizer = TfidfVectorizer()\ntfidf_matrix = vectorizer.fit_transform(texts)\n\n'''\n\n# for eps in np.arange(.5, .7, 0.001):\n#     for min_samples in range(2, 5):\n#         dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n#         dbscan.fit(embeddings_array)\n#         labels = dbscan.labels_\n#         n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n#         # and the number of label -1 is lower than 50\n#         if n_clusters_ > 1 and n_clusters_ <= 8 and len(np.where(labels==-1)[0]) < 50:\n#             print(f\"eps: {eps}, min_samples: {min_samples}\")\n#             unique, counts = np.unique(labels, return_counts=True)\n#             print(dict(zip(unique, counts)))\n# exit(1)\n\n# eps: 0.53, min_samples: 4\n# eps: 0.531, min_samples: 4\n\ndbscan = DBSCAN(eps=0.5180000000000003, min_samples=3)\ndbscan.fit(embeddings_array)\nlabels = dbscan.labels_\n\n# for each cluster, save the corresponding texts within a file\nfor i in range(len(set(labels))):\n    with open(f'./docs/knowledge_chunks/cluster_{i}.jsonl', 'w') as wf:\n        for j in range(len(labels)):\n            if labels[j] == i:\n                wf.write(str(texts[j]) + '\\n\\n')\n\n# Dimensionality reduction for visualization\nsvd = PCA(n_components=3)\nreduced_embeddings = svd.fit_transform(embeddings_array)\n\n# the number of lables\nn_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n\nprint(f\"Number of clusters: {n_clusters_}\")\n\n# the number of elements in each cluster\nunique, counts = np.unique(labels, return_counts=True)\nprint(dict(zip(unique, counts)))\n\n# Plotting in 3-D\nfig = plt.figure(figsize=(10, 6))\nax = fig.add_subplot(projection='3d')\nscatter = ax.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], reduced_embeddings[:, 2], c=labels, cmap='viridis')\nplt.title(\"Knowledge Clustering (Ada-002)\")\nplt.colorbar(scatter)\nax.set_xlabel(\"PCA 1\")\nax.set_ylabel(\"PCA 2\")\nax.set_zlabel(\"PCA 3\")\n# save the figure\nplt.savefig('./knowledge_clustering_3d.png')\n\n# plt.figure(figsize=(10, 6))\n# scatter = plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], c=labels, cmap='viridis')\n# plt.title(\"Knowledge Clustering (Ada-002)\")\n# plt.colorbar(scatter)\n# plt.xlabel(\"PCA 1\")\n# plt.ylabel(\"PCA 2\")\n# # save the figure\n# plt.savefig('./knowledge_clustering.png')"}
{"type": "source_file", "path": "multiagents/agent_conf/output_parser.py", "content": "from __future__ import annotations\n\nimport re\nfrom typing import Union\nimport json\n\n# from langchain.schema import AgentAction, AgentFinish\nfrom multiagents.utils.utils import AgentAction, AgentFinish\nfrom multiagents.custom_parser import OutputParserError, output_parser_registry\nfrom multiagents.custom_parser import OutputParser\nfrom multiagents.response_formalize_scripts.combine_similar_answer import combine_similar_answers\n\n\n@output_parser_registry.register(\"agent_conf\")\nclass DBDiag(OutputParser):\n    def parse(self, output) -> Union[AgentAction, AgentFinish]:\n        # pdb.set_trace()\n        # if output is str\n        if isinstance(output, str):\n            text = output\n        else:\n            try:\n                text = output['content']\n            except:\n                raise OutputParserError(\"llm output is not str or dict\")\n        \n        cleaned_output = text.strip()\n        cleaned_output = re.sub(r\"\\n+\", \"\\n\", cleaned_output)\n\n        matches = list(re.finditer(r'\\n|\\\\n', cleaned_output))\n\n        # cleaned_output = cleaned_output.split(\"\\\\n\")\n        # if len(cleaned_output) == 1:\n        #     cleaned_output = cleaned_output[0].split(\"\\n\")\n        if len(matches) > 1:\n            raw_thought = cleaned_output[:matches[0].start()]\n            raw_action = cleaned_output[matches[0].end():matches[1].start()]\n            raw_action_input = cleaned_output[matches[1].end():]\n        else:\n            return None\n\n        cleaned_output = [raw_thought, raw_action, raw_action_input]\n\n        if not (cleaned_output[0].lower().startswith(\"thought\") and\n                cleaned_output[1].lower().startswith(\"action\") and\n                (cleaned_output[2].lower().startswith(\"action input\"))):\n            return None\n\n        # action = cleaned_output[1][len(\"Action:\") :].strip()\n        # action_input = cleaned_output[2][len(\"Action Input:\") :].strip()\n        # Extract action after \"Action:\" (case-insensitive)\n        action_split = re.split(r'(?i)^action:', cleaned_output[1], maxsplit=1)\n        action = action_split[1].strip() if len(action_split) > 1 else \"\"\n\n        # Extract action input after \"Action Input:\" (case-insensitive)\n        action_input_split = re.split(r'(?i)^action input:', cleaned_output[2], maxsplit=1)\n        action_input = action_input_split[1].strip() if len(action_input_split) > 1 else \"\"\n\n        #print(colored(\"new action\", \"red\"))\n        #print(cleaned_output)\n\n        if action.lower() in [\"speak\"]:\n\n            action_input = re.sub(r\"\\n+\", \"\\n\", action_input)\n            action_input = action_input.replace(\"\\\\\\\"\", \"\\\"\")\n\n            #action_input = action_input.split(\"\\n\")\n            try:\n                if action_input[0] == '(':\n                    action_input = action_input[1:]\n                if action_input[-1] == ')':\n                    action_input = action_input[:-1]\n                action_input = json.loads(action_input)\n            except:\n                print(\"Error in parsing diagnosis results from 'speak' action\")\n                \n                return None\n\n            action_json = {\"diagnose\": \"\", \"solution\": [], \"knowledge\": \"\"}\n            \n            for key in action_input:\n                if \"diagnose\" in key:\n                    if type(action_input[key]) == list and action_input[key] != []:\n                        action_input[key] = combine_similar_answers(action_input[key], output_format='list')\n                    elif type(action_input[key]) == str and action_input[key] != \"\":\n                        action_input[key] = combine_similar_answers(action_input[key])\n\n                    action_json[\"diagnose\"] = action_input[key]\n                elif \"solution\" in key: # list\n                    if type(action_input[key]) == list and action_input[key] != []:\n                        action_input[key] = combine_similar_answers(action_input[key], output_format='list')\n                    elif type(action_input[key]) == str and action_input[key] != \"\":\n                        action_input[key] = combine_similar_answers(action_input[key])\n                    potential_solutions = action_input[key]\n\n                    if isinstance(potential_solutions, str):\n                        potential_solutions = potential_solutions.strip()\n                        potential_solutions = re.sub(r\"\\n+\", \"\\n\", potential_solutions)\n                        potential_solutions = potential_solutions.split(\"\\n\")\n\n                    action_json[\"solution\"] = potential_solutions\n                elif \"knowledge\" in key:\n                    if type(action_input[key]) == list and action_input[key] != []:\n                        action_input[key] = combine_similar_answers(action_input[key], output_format='list')\n                    elif type(action_input[key]) == str and action_input[key] != \"\":\n                        action_input[key] = combine_similar_answers(action_input[key])\n\n                    action_json[\"knowledge\"] = action_input[key]\n\n            return AgentFinish({\"output\": action_json}, text)\n        \n        elif action == \"CallOn\":\n            return AgentFinish({\"output\": \"[CallOn] \" + action_input}, text)\n        elif action == \"RaiseHand\":\n            return AgentFinish({\"output\": \"[RaiseHand] \" + action_input}, text)\n        elif action == \"Listen\":\n            return AgentFinish({\"output\": \"\"}, text)\n        else:            \n            return AgentAction(action.lower(), action_input, text)\n\n            # p action.lower()\n            # 'obtain_start_and_end_time_of_anomaly'\n            # p action_input\n            # '{\"input\": \"1692588540, 1692588630\"}'\n\n            # p action.lower()\n            # 'whether_is_abnormal_metric'\n            # p action_input\n            # '{\"start_time\": 1692588540, \"end_time\": 1692588630, \"metric_name\": \"cpu_usage\"}'\n            # p text\n            # 'Thought: Now that I have obtained the start and end time of the anomaly, I should check whether the CPU usage is abnormal during that time period.\\nAction: whether_is_abnormal_metric\\nAction Input: {\"start_time\": 1692588540, \"end_time\": 1692588630, \"metric_name\": \"cpu_usage\"}'"}
{"type": "source_file", "path": "multiagents/__init__.py", "content": "\nfrom .agent_conf import *\nfrom .agent_conf.output_parser import DBDiag\n\n# from .agents import Agent\n\nfrom .environments import env_registry\nfrom .environments.rules import Rule\nfrom .environments.rules.order import order_registry\nfrom .environments.rules.describer import describer_registry\nfrom .environments.rules.selector import selector_registry\nfrom .environments.rules.updater import updater_registry\nfrom .environments.rules.visibility import visibility_registry\nfrom .multiagents import MultiAgents\nfrom .initialization import (\n    prepare_task_config,\n    load_agent,\n    load_environment,\n    load_tools,\n    load_llm,\n    load_memory,\n)"}
{"type": "source_file", "path": "doc2knowledge/prompts.py", "content": "LOGICAL_VERIFICATION_PROMPT = f\"\"\"Now given a new knowledge:\\n{{new_knowledge}}\\n \nDetermine whether it contains similar knowledge with an existng knowledge:\\n{{exist_knowledge}}\\n\\n \nPlease output your judgement in the following format: Output \"Answer:\" followed by a single answer \"yes\" or \"no\".\"\"\"\n\nDIAG_PROMPT = \"\"\"Here are some diagnosis knowledge blocks following the following dict format:\n```\n{\n  \"name\": \"许多死元组\",\n  \"content\": \"如果访问的表中有太多的死元组，可能会导致表膨胀并降低性能。\",\n  \"metrics\": [\"活元组\", \"死元组\", \"表大小\", \"死亡率\"],\n  \"steps\": \"对于每个访问的表，如果活元组和死元组的总数在可接受的限制范围内（1000），并且表大小不太大（50MB），则不是根本原因。否则，如果死亡率也超过了阈值（0.02），则被视为根本原因。我们建议及时清理死元组。\"\n}\n```\nNote elements in \"metrics\" should be concrete names like \"活元组\", \"死元组\", \"表大小\", \"死亡率\", etc. rather than \"metric1\", \"metric2\", etc. \"content\" should be a detailed description string. \"steps\" should be a string that contains steps to follow to diagnose the problem.\n\"\"\"\n\nLOOKUP_FUNCTION = {\n    'name': \"look_up\",\n    'description': \"Given the index of a chapter or section, return the detailed content in that chapter or section. The chapter or section may contain sub-chapters or sub-sections, keep calling the look up function on these sub-chapters or sub-sections to get more information.\",\n    'parameters': {\n        'type': \"object\",\n        'properties': {\n            'index': {\n                'type': \"string\",\n                'description': \"The chapter or section index. e.g., `1.1` or `1.4`.\",\n            },\n        },\n        'required': [\"index\"],\n    }\n}\nLOOKUP_PROMPT = f\"Remember: This is an interactive task. At any time, use `look_up()` to obtain the detailed content in a given chapter. The available chapters include ${{relevant_chapters}}\\n\"\n\nSUBMIT_RULE_FUNCTION = {\n    'name': \"submit_rule\",\n    'description': \"Submit a or multiple diagnosis knowledge blocks summarized based on the knowledge learned from the document. The diagnosis knowledge blocks should follow the dict format and all the values in the dict should be chinese.\",\n    'parameters': {\n        'type': \"object\",\n        'properties': {\n            'blocks': {\n                'type': \"string\",\n                'description': \"A list of diagnosis knowledge blocks separated by empty lines in dict format.\",\n            },\n        },\n        'required': [\"knowledges\"],\n    }\n}\nSUBMIT_RULE_PROMPT = f\"Use `submit_rule()` to submit knowledges.\\n\"\n\nDOCUMENT_TOPIC = \"query optimization\"\nDOCUMENT_PROMPT = \"Notice that the document may not be about database diagnosis, but rather contain examples that could demonstrate how to conduct database diagnosis. Pay special attention to examples in the document.\"\nSUMMARIZE_PROMPT_MSG = [{'role': \"system\", 'content': (\n    f\"Please summarize the document you are provided with in a few sentences. Please be brief.\\n\"\n    f\"Your summarization is later used as an index for others to quickly locate technical details about {DOCUMENT_TOPIC}.\\n{DOCUMENT_PROMPT}\\n\"\n)}]\n\nTASK_PROMPT = \"write an aspect of detailed diagnosis knowledge (e.g., only about high IO, only about slow queries) that can be learned from the document\"\nRULES_EXTRACTION_PROMPT_MSG = [{'role': \"system\", 'content': (\n    f\"Given a document index, please {TASK_PROMPT}.\\nTry to submit knowledge blocks in the currently reading chapter one by one in order. Each knowledge block should strictly follow the dict format (with double quotes)\\n\"\n    f\"{'Extracted knowledge blocks should follow the following format.' + ' ' + DIAG_PROMPT}\\n\"\n    f\"Do not repeatedly extract the following knowledge blocks:\\n${{existing_rules}}\\n\"\n    f\"Do not repeatedly lookup the following sub-chapters:\\n${{used_chapters}}\\n\"\n)}]\nINDEX_TEMPLATE = \"{idx} - {title}\"\nCONTENT_TEMPLATE = \"{idx} - {title}\\n{content}\"\nDOCUMENT_VIEW_TEMPALTE = \"{summaries}\\n\\nDocument Index:\\n{index}\"\n\n\ndef MSG(content, role='user'):\n    return [{'role': role, 'content': content}]"}
{"type": "source_file", "path": "document_loaders/mypdfloader.py", "content": "from typing import List\nfrom langchain.document_loaders.unstructured import UnstructuredFileLoader\nimport tqdm\n\n\nclass RapidOCRPDFLoader(UnstructuredFileLoader):\n    def _get_elements(self) -> List:\n        def pdf2text(filepath):\n            import fitz # pyMuPDF里面的fitz包，不要与pip install fitz混淆\n            from rapidocr_onnxruntime import RapidOCR\n            import numpy as np\n            ocr = RapidOCR()\n            doc = fitz.open(filepath)\n            resp = \"\"\n\n            b_unit = tqdm.tqdm(total=doc.page_count, desc=\"RapidOCRPDFLoader context page index: 0\")\n            for i, page in enumerate(doc):\n\n                # 更新描述\n                b_unit.set_description(\"RapidOCRPDFLoader context page index: {}\".format(i))\n                # 立即显示进度条更新结果\n                b_unit.refresh()\n                # TODO: 依据文本与图片顺序调整处理方式\n                text = page.get_text(\"\")\n                resp += text + \"\\n\"\n\n                img_list = page.get_images()\n                for img in img_list:\n                    pix = fitz.Pixmap(doc, img[0])\n                    img_array = np.frombuffer(pix.samples, dtype=np.uint8).reshape(pix.height, pix.width, -1)\n                    result, _ = ocr(img_array)\n                    if result:\n                        ocr_result = [line[1] for line in result]\n                        resp += \"\\n\".join(ocr_result)\n\n                # 更新进度\n                b_unit.update(1)\n            return resp\n\n        text = pdf2text(self.file_path)\n        from unstructured.partition.text import partition_text\n        return partition_text(text=text, **self.unstructured_kwargs)\n\n\nif __name__ == \"__main__\":\n    loader = RapidOCRPDFLoader(file_path=\"../tests/samples/ocr_test.pdf\")\n    docs = loader.load()\n    print(docs)\n"}
{"type": "source_file", "path": "document_loaders/__init__.py", "content": "from .mypdfloader import RapidOCRPDFLoader\nfrom .myimgloader import RapidOCRLoader"}
{"type": "source_file", "path": "document_loaders/FilteredCSVloader.py", "content": "## 指定制定列的csv文件加载器\n\nfrom langchain.document_loaders import CSVLoader\nimport csv\nfrom io import TextIOWrapper\nfrom typing import Dict, List, Optional\nfrom langchain.docstore.document import Document\nfrom langchain.document_loaders.helpers import detect_file_encodings\n\n\nclass FilteredCSVLoader(CSVLoader):\n    def __init__(\n            self,\n            file_path: str,\n            columns_to_read: List[str],\n            source_column: Optional[str] = None,\n            metadata_columns: List[str] = [],\n            csv_args: Optional[Dict] = None,\n            encoding: Optional[str] = None,\n            autodetect_encoding: bool = False,\n    ):\n        super().__init__(\n            file_path=file_path,\n            source_column=source_column,\n            metadata_columns=metadata_columns,\n            csv_args=csv_args,\n            encoding=encoding,\n            autodetect_encoding=autodetect_encoding,\n        )\n        self.columns_to_read = columns_to_read\n\n    def load(self) -> List[Document]:\n        \"\"\"Load data into document objects.\"\"\"\n\n        docs = []\n        try:\n            with open(self.file_path, newline=\"\", encoding=self.encoding) as csvfile:\n                docs = self.__read_file(csvfile)\n        except UnicodeDecodeError as e:\n            if self.autodetect_encoding:\n                detected_encodings = detect_file_encodings(self.file_path)\n                for encoding in detected_encodings:\n                    try:\n                        with open(\n                            self.file_path, newline=\"\", encoding=encoding.encoding\n                        ) as csvfile:\n                            docs = self.__read_file(csvfile)\n                            break\n                    except UnicodeDecodeError:\n                        continue\n            else:\n                raise RuntimeError(f\"Error loading {self.file_path}\") from e\n        except Exception as e:\n            raise RuntimeError(f\"Error loading {self.file_path}\") from e\n\n        return docs\n\n    def __read_file(self, csvfile: TextIOWrapper) -> List[Document]:\n        docs = []\n        csv_reader = csv.DictReader(csvfile, **self.csv_args)  # type: ignore\n        for i, row in enumerate(csv_reader):\n            if self.columns_to_read[0] in row:\n                content = row[self.columns_to_read[0]]\n                # Extract the source if available\n                source = (\n                    row.get(self.source_column, None)\n                    if self.source_column is not None\n                    else self.file_path\n                )\n                metadata = {\"source\": source, \"row\": i}\n\n                for col in self.metadata_columns:\n                    if col in row:\n                        metadata[col] = row[col]\n\n                doc = Document(page_content=content, metadata=metadata)\n                docs.append(doc)\n            else:\n                raise ValueError(f\"Column '{self.columns_to_read[0]}' not found in CSV file.\")\n\n        return docs\n"}
