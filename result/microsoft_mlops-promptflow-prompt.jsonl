{"repo_info": {"repo_name": "mlops-promptflow-prompt", "repo_owner": "microsoft", "repo_url": "https://github.com/microsoft/mlops-promptflow-prompt"}}
{"type": "test_file", "path": "tests/__init__.py", "content": ""}
{"type": "test_file", "path": "tests/test_to_delete.py", "content": "\"\"\"Fake test implementation is here to make sure that pytest is working as a part of the MLOps.\"\"\"\n\n\ndef test_to_delete():\n    \"\"\"Run a basic check.\"\"\"\n    try:\n        print(\"Hello\") is None\n    except Exception:\n        print(\"Test print function failed.\")\n        assert False\n"}
{"type": "source_file", "path": "flows/yaml_basic_flow/__init__.py", "content": ""}
{"type": "source_file", "path": "flows/yaml_basic_flow/evaluate/evaluate.py", "content": "\"\"\"Evaluation flow for yaml_basic_flow.\"\"\"\nimport argparse\nfrom pprint import pprint\nfrom promptflow.evals.evaluate import evaluate\nfrom mlops.common.config_utils import MLOpsConfig\nfrom src.evaluators.match_evaluator import MatchEvaluator\nfrom flows.yaml_basic_flow.evaluate.flow_wrapper import StandardFlowWrapper\nfrom mlops.common.naming_tools import generate_experiment_name\n\n\ndef main():\n    \"\"\"Implement parameter reading and evaluation flow.\"\"\"\n    # Config parameters\n    parser = argparse.ArgumentParser(\"config_parameters\")\n    parser.add_argument(\n        \"--environment_name\",\n        type=str,\n        required=True,\n        help=\"env_name from config.yaml\",\n    )\n    args = parser.parse_args()\n\n    mlops_config = MLOpsConfig(environment=args.environment_name)\n    flow_config = mlops_config.get_flow_config(flow_name=\"yaml_basic_flow\")\n\n    matchevaluator = MatchEvaluator()\n\n    data_eval_path = flow_config[\"eval_data_path\"]\n    flow_standard_path = flow_config[\"standard_flow_path\"]\n    aoai_deployment = flow_config[\"deployment_name\"]\n\n    aistudio_config = mlops_config.aistudio_config\n    openai_config = mlops_config.aoai_config\n\n    flow = StandardFlowWrapper(\n        flow_standard_path,\n        flow_config[\"connection_name\"],\n        aoai_deployment,\n        openai_config,\n    )\n\n    results = evaluate(\n        evaluation_name=generate_experiment_name(\"yaml_basic_flow\"),\n        data=data_eval_path,\n        target=flow,\n        evaluators={\n            \"matchevaluator\": matchevaluator,\n        },\n        evaluator_config={\n            \"matchevaluator\": {\n                \"response\": \"${target.answer}\",\n                \"ground_truth\": \"${data.results}\",\n            },\n        },\n        azure_ai_project={\n            \"subscription_id\": aistudio_config[\"subscription_id\"],\n            \"resource_group_name\": aistudio_config[\"resource_group_name\"],\n            \"project_name\": aistudio_config[\"project_name\"],\n        },\n    )\n\n    pprint(results)\n\n\nif __name__ == \"__main__\":\n    main()\n"}
{"type": "source_file", "path": "flows/chat_with_pdf/standard/chat_with_pdf/utils/__init__.py", "content": "__path__ = __import__(\"pkgutil\").extend_path(__path__, __name__)  # type: ignore\n"}
{"type": "source_file", "path": "flows/chat_with_pdf/standard/chat_with_pdf/download.py", "content": "\"\"\"Download pdfs.\"\"\"\nimport requests\nimport os\nimport re\n\nfrom utils.lock import acquire_lock\nfrom utils.logging import log\nfrom constants import PDF_DIR\n\n\ndef download(url: str) -> str:\n    \"\"\"Download a pdf file from a url and return the path to the file.\"\"\"\n    path = os.path.join(PDF_DIR, normalize_filename(url) + \".pdf\")\n    lock_path = path + \".lock\"\n\n    with acquire_lock(lock_path):\n        if os.path.exists(path):\n            log(\"Pdf already exists in \" + os.path.abspath(path))\n            return path\n\n        log(\"Downloading pdf from \" + url)\n        response = requests.get(url)\n\n        with open(path, \"wb\") as f:\n            f.write(response.content)\n\n        return path\n\n\ndef normalize_filename(filename):\n    \"\"\"Replace any invalid characters with an underscore.\"\"\"\n    return re.sub(r\"[^\\w\\-_. ]\", \"_\", filename)\n"}
{"type": "source_file", "path": "flows/chat_with_pdf/evaluate/__init__.py", "content": ""}
{"type": "source_file", "path": "flows/chat_with_pdf/standard/chat_with_pdf/qna.py", "content": "\"\"\"QNA using OpenAI chat.\"\"\"\nimport os\n\nfrom utils.oai import OAIChat\n\n\ndef qna(prompt: str, history: list):\n    \"\"\"QNA function using OpenAI chat.\"\"\"\n    max_completion_tokens = int(os.environ.get(\"MAX_COMPLETION_TOKENS\"))\n\n    chat = OAIChat()\n    stream = chat.stream(\n        messages=history + [{\"role\": \"user\", \"content\": prompt}],\n        max_tokens=max_completion_tokens,\n    )\n\n    return stream\n"}
{"type": "source_file", "path": "flows/chat_with_pdf/standard/chat_with_pdf/utils/index.py", "content": "\"\"\"Faiss index class.\"\"\"\nimport os\nfrom typing import Iterable, List, Optional\nfrom dataclasses import dataclass\nfrom faiss import Index\nimport faiss\nimport pickle\nimport numpy as np\n\nfrom .oai import OAIEmbedding as Embedding\n\n\n@dataclass\nclass SearchResultEntity:\n    \"\"\"Data class for search results.\"\"\"\n\n    text: str = None\n    vector: List[float] = None\n    score: float = None\n    original_entity: dict = None\n    metadata: dict = None\n\n\nINDEX_FILE_NAME = \"index.faiss\"\nDATA_FILE_NAME = \"index.pkl\"\n\n\nclass FAISSIndex:\n    \"\"\"Faiss index class.\"\"\"\n\n    def __init__(self, index: Index, embedding: Embedding) -> None:\n        \"\"\"Init Faiss index class.\"\"\"\n        self.index = index\n        self.docs = {}  # id -> doc, doc is (text, metadata)\n        self.embedding = embedding\n\n    def insert_batch(\n        self, texts: Iterable[str], metadatas: Optional[List[dict]] = None\n    ) -> None:\n        \"\"\"Insert batch into index.\"\"\"\n        documents = []\n        vectors = []\n        for i, text in enumerate(texts):\n            metadata = metadatas[i] if metadatas else {}\n            vector = self.embedding.generate(text)\n            documents.append((text, metadata))\n            vectors.append(vector)\n\n        self.index.add(np.array(vectors, dtype=np.float32))\n        self.docs.update(\n            {i: doc for i, doc in enumerate(documents, start=len(self.docs))}\n        )\n\n        pass\n\n    def query(self, text: str, top_k: int = 10) -> List[SearchResultEntity]:\n        \"\"\"Query index.\"\"\"\n        vector = self.embedding.generate(text)\n        scores, indices = self.index.search(np.array([vector], dtype=np.float32), top_k)\n        docs = []\n        for j, i in enumerate(indices[0]):\n            if i == -1:  # This happens when not enough docs are returned.\n                continue\n            doc = self.docs[i]\n            docs.append(\n                SearchResultEntity(text=doc[0], metadata=doc[1], score=scores[0][j])\n            )\n        return docs\n\n    def save(self, path: str) -> None:\n        \"\"\"Save index to disk.\"\"\"\n        faiss.write_index(self.index, os.path.join(path, INDEX_FILE_NAME))\n        # dump docs to pickle file\n        with open(os.path.join(path, DATA_FILE_NAME), \"wb\") as f:\n            pickle.dump(self.docs, f)\n        pass\n\n    def load(self, path: str) -> None:\n        \"\"\"Load index from disk.\"\"\"\n        self.index = faiss.read_index(os.path.join(path, INDEX_FILE_NAME))\n        with open(os.path.join(path, DATA_FILE_NAME), \"rb\") as f:\n            self.docs = pickle.load(f)\n        pass\n"}
{"type": "source_file", "path": "flows/chat_with_pdf/standard/chat_with_pdf/__init__.py", "content": "import sys\nimport os\n\nsys.path.append(os.path.dirname(os.path.abspath(__file__)))\n"}
{"type": "source_file", "path": "flows/chat_with_pdf/standard/rewrite_question_tool.py", "content": "\"\"\"Promptflow tool for re-writing question.\"\"\"\nfrom promptflow import tool\nfrom chat_with_pdf.rewrite_question import rewrite_question\n\n\n@tool\ndef rewrite_question_tool(question: str, history: list, env_ready_signal: str):\n    \"\"\"Re-write question base on chat history.\"\"\"\n    return rewrite_question(question, history)\n"}
{"type": "source_file", "path": "flows/class_plan_and_execute/standard/solver.py", "content": "\"\"\"Solver node for the plan_and_execute flow.\"\"\"\nimport os\nfrom flows.class_plan_and_execute.standard.multiprocressed_agents import (\n    MultiProcessedAssistantAgent as AssistantAgent,\n)\nfrom promptflow.tracing import trace\n\n\nclass Solver:\n    \"\"\"Solver agent.\"\"\"\n\n    def __init__(self, system_message_path: str):\n        \"\"\"Initialize the solver agent.\"\"\"\n        self.config_list = [\n            {\n                \"model\": os.getenv(\"aoai_model_gpt4\"),\n                \"api_key\": os.getenv(\"aoai_api_key\"),\n                \"base_url\": os.getenv(\"aoai_base_url\"),\n                \"api_type\": \"azure\",\n                \"api_version\": os.getenv(\"aoai_api_version\"),\n            }\n        ]\n\n        with open(system_message_path, \"r\") as file:\n            system_message = file.read()\n\n        self.solver = AssistantAgent(\n            name=\"SOLVER\",\n            description=\"An agent expert in creating a final response to the user's request.\",\n            system_message=system_message,\n            code_execution_config=False,\n            llm_config={\n                \"config_list\": self.config_list,\n                \"timeout\": 60,\n                \"cache_seed\": None,\n            },\n        )\n\n    @trace\n    def generate_response(self, question: str, results: str) -> str:\n        \"\"\"Generate a final response to the user's request.\"\"\"\n        solver_message = f\"\"\"\n        Question:\n        {question}\n\n        Step results:\n        {results}\n        \"\"\"\n\n        return self.solver.generate_reply(\n            messages=[{\"content\": solver_message, \"role\": \"user\"}]\n        )\n"}
{"type": "source_file", "path": "flows/function_basic_flow/__init__.py", "content": ""}
{"type": "source_file", "path": "flows/chat_with_pdf/standard/chat_with_pdf/utils/retry.py", "content": "\"\"\"Retry decorators.\"\"\"\nfrom typing import Tuple, Union, Optional, Type\nimport functools\nimport time\nimport random\n\n\ndef retry_and_handle_exceptions(\n    exception_to_check: Union[Type[Exception], Tuple[Type[Exception], ...]],\n    max_retries: int = 3,\n    initial_delay: float = 1,\n    exponential_base: float = 2,\n    jitter: bool = False,\n    extract_delay_from_error_message: Optional[any] = None,\n):\n    \"\"\"Create decorator to retry a function call with exponential backoff in case of specified exceptions.\"\"\"\n\n    def deco_retry(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            delay = initial_delay\n            for i in range(max_retries):\n                try:\n                    return func(*args, **kwargs)\n                except exception_to_check as e:\n                    if i == max_retries - 1:\n                        raise Exception(\n                            \"Func execution failed after {0} retries: {1}\".format(\n                                max_retries, e\n                            )\n                        )\n                    delay *= exponential_base * (1 + jitter * random.random())\n                    delay_from_error_message = None\n                    if extract_delay_from_error_message is not None:\n                        delay_from_error_message = extract_delay_from_error_message(\n                            str(e)\n                        )\n                    final_delay = (\n                        delay_from_error_message if delay_from_error_message else delay\n                    )\n                    print(\n                        \"Func execution failed. Retrying in {0} seconds: {1}\".format(\n                            final_delay, e\n                        )\n                    )\n                    time.sleep(final_delay)\n\n        return wrapper\n\n    return deco_retry\n\n\ndef retry_and_handle_exceptions_for_generator(\n    exception_to_check: Union[Type[Exception], Tuple[Type[Exception], ...]],\n    max_retries: int = 3,\n    initial_delay: float = 1,\n    exponential_base: float = 2,\n    jitter: bool = False,\n    extract_delay_from_error_message: Optional[any] = None,\n):\n    \"\"\"Create decorator to retry a generator function call with exponential backoff in case of specified exceptions.\"\"\"\n\n    def deco_retry(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            delay = initial_delay\n            for i in range(max_retries):\n                try:\n                    for value in func(*args, **kwargs):\n                        yield value\n                    break\n                except exception_to_check as e:\n                    if i == max_retries - 1:\n                        raise Exception(\n                            \"Func execution failed after {0} retries: {1}\".format(\n                                max_retries, e\n                            )\n                        )\n                    delay *= exponential_base * (1 + jitter * random.random())\n                    delay_from_error_message = None\n                    if extract_delay_from_error_message is not None:\n                        delay_from_error_message = extract_delay_from_error_message(\n                            str(e)\n                        )\n                    final_delay = (\n                        delay_from_error_message if delay_from_error_message else delay\n                    )\n                    print(\n                        \"Func execution failed. Retrying in {0} seconds: {1}\".format(\n                            final_delay, e\n                        )\n                    )\n                    time.sleep(final_delay)\n\n        return wrapper\n\n    return deco_retry\n"}
{"type": "source_file", "path": "flows/class_plan_and_execute/evaluate/evaluate.py", "content": "\"\"\"Evaluation flow for plan_and_execute flow.\"\"\"\nimport os\nimport argparse\nfrom pprint import pprint\nfrom promptflow.evals.evaluate import evaluate\nfrom promptflow.core import AzureOpenAIModelConfiguration\nfrom mlops.common.config_utils import MLOpsConfig\nfrom src.evaluators.json_evaluator import JsonEvaluator\nfrom src.evaluators.executor_evaluator import ExecutorEvaluator\nfrom promptflow.evals.evaluators import (\n    GroundednessEvaluator,\n    RelevanceEvaluator,\n    SimilarityEvaluator,\n)\nfrom flows.class_plan_and_execute.standard.plan_and_execute import PlanAndExecute\nfrom mlops.common.naming_tools import generate_experiment_name\n\n\ndef main():\n    \"\"\"Implement parameter reading and evaluation flow.\"\"\"\n    # Config parameters\n    parser = argparse.ArgumentParser(\"config_parameters\")\n    parser.add_argument(\n        \"--environment_name\",\n        type=str,\n        required=True,\n        help=\"env_name from config.yaml\",\n    )\n    args = parser.parse_args()\n\n    mlops_config = MLOpsConfig(environment=args.environment_name)\n    flow_config = mlops_config.get_flow_config(flow_name=\"class_plan_and_execute\")\n\n    json_schema_path = flow_config[\"json_schema_path\"]\n    data_eval_path = flow_config[\"eval_data_path\"]\n\n    aistudio_config = mlops_config.aistudio_config\n    openai_config = mlops_config.aoai_config\n\n    model_config = AzureOpenAIModelConfiguration(\n        azure_endpoint=openai_config[\"aoai_api_base\"],\n        api_key=openai_config[\"aoai_api_key\"],\n        api_version=openai_config[\"aoai_api_version\"],\n        azure_deployment=flow_config[\"deployment_name_gpt4\"],\n    )\n\n    os.environ[\"aoai_api_key\"] = openai_config[\"aoai_api_key\"]\n    os.environ[\"bing_api_key\"] = flow_config[\"bing_api_key\"]\n    os.environ[\"aoai_model_gpt4\"] = flow_config[\"deployment_name_gpt4\"]\n    os.environ[\"aoai_model_gpt35\"] = flow_config[\"deployment_name_gpt35\"]\n    os.environ[\"aoai_base_url\"] = openai_config[\"aoai_api_base\"]\n    os.environ[\"aoai_api_version\"] = flow_config[\"aoai_api_version\"]\n    os.environ[\"bing_endpoint\"] = flow_config[\"bing_endpoint\"]\n\n    json_evaluator = JsonEvaluator(json_schema_path)\n    executor_evaluator = ExecutorEvaluator()\n    groundedness_evaluator = GroundednessEvaluator(model_config)\n    relevance_evaluator = RelevanceEvaluator(model_config)\n    similarity_evaluator = SimilarityEvaluator(model_config)\n\n    plan_and_execute = PlanAndExecute(\n        planner_system_message_path=flow_config[\"planner_system_message_path\"],\n        solver_system_message_path=flow_config[\"solver_system_message_path\"],\n    )\n\n    results = evaluate(\n        evaluation_name=generate_experiment_name(\"plan_and_execute\"),\n        data=data_eval_path,\n        target=plan_and_execute,\n        evaluators={\n            \"json_evaluator\": json_evaluator,\n            \"executor_evaluator\": executor_evaluator,\n            \"groundedness_evaluator\": groundedness_evaluator,\n            \"relevance_evaluator\": relevance_evaluator,\n            \"similarity_evaluator\": similarity_evaluator,\n        },\n        evaluator_config={\n            \"json_evaluator\": {\"json_string\": \"${target.plan}\"},\n            \"executor_evaluator\": {\n                \"plan_steps_count\": \"${target.number_of_steps}\",\n                \"result_string\": \"${target.steps}\",\n            },\n            \"groundedness_evaluator\": {\n                \"response\": \"${target.answer}\",\n                \"context\": \"${target.steps}\",\n            },\n            \"relevance_evaluator\": {\n                \"question\": \"${data.question}\",\n                \"response\": \"${target.answer}\",\n                \"context\": \"${target.steps}\",\n            },\n            \"similarity_evaluator\": {\n                \"question\": \"${data.question}\",\n                \"answer\": \"${target.answer}\",\n                \"ground_truth\": \"${data.answer}\",\n            },\n        },\n        azure_ai_project={\n            \"subscription_id\": aistudio_config[\"subscription_id\"],\n            \"resource_group_name\": aistudio_config[\"resource_group_name\"],\n            \"project_name\": aistudio_config[\"project_name\"],\n        },\n    )\n\n    pprint(results)\n\n\nif __name__ == \"__main__\":\n    main()\n"}
{"type": "source_file", "path": "flows/class_plan_and_execute/standard/planner.py", "content": "\"\"\"Planner node for the plan_and_execute flow.\"\"\"\nimport os\nfrom flows.class_plan_and_execute.standard.multiprocressed_agents import (\n    MultiProcessedAssistantAgent as AssistantAgent,\n)\nimport json\nfrom promptflow.tracing import trace\n\n\nclass Planner:\n    \"\"\"Planner agent.\"\"\"\n\n    def __init__(self, system_message_path: str):\n        \"\"\"Initialize the planner agent.\"\"\"\n        self.config_list = [\n            {\n                \"model\": os.getenv(\"aoai_model_gpt4\"),\n                \"api_key\": os.getenv(\"aoai_api_key\"),\n                \"base_url\": os.getenv(\"aoai_base_url\"),\n                \"api_type\": \"azure\",\n                \"api_version\": os.getenv(\"aoai_api_version\"),\n            }\n        ]\n\n        with open(system_message_path, \"r\") as file:\n            system_message = file.read()\n\n        self.planner = AssistantAgent(\n            name=\"PLANNER\",\n            description=\"An agent expert in creating a step-by-step execution plan to solve the user's request.\",\n            system_message=system_message,\n            code_execution_config=False,\n            llm_config={\n                \"config_list\": self.config_list,\n                \"temperature\": 0,\n                \"timeout\": 120,\n                \"cache_seed\": None,\n            },\n        )\n\n        # register_tools(self.planner)\n\n    @trace\n    def generate_plan(self, question: str) -> str:\n        \"\"\"Generate a step-by-step execution plan to solve the user's request.\"\"\"\n        planner_reply = self.planner.generate_reply(\n            messages=[{\"content\": question, \"role\": \"user\"}]\n        )\n        planner_reply = planner_reply.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n        try:\n            plan = json.loads(planner_reply)\n        except json.JSONDecodeError:\n            plan = {}\n\n        return plan\n"}
{"type": "source_file", "path": "flows/chat_with_pdf/evaluate/flow_wrapper.py", "content": "\"\"\"Implement a wrapper for yaml based flow since it's not supported by evaluate explicitly.\"\"\"\nfrom promptflow.client import load_flow\nfrom promptflow.entities import FlowContext\nfrom promptflow.entities import AzureOpenAIConnection\nfrom promptflow.client import PFClient\n\n\nclass ChatWithPdfFlowWrapper:\n    \"\"\"Implement the flow.\"\"\"\n\n    def __init__(self, flow_config: dict, aoai_config: dict):\n        \"\"\"Initialize environment and load prompty into the memory.\"\"\"\n        connection = AzureOpenAIConnection(\n            name=flow_config[\"connection_name\"],\n            api_key=aoai_config[\"aoai_api_key\"],\n            api_base=aoai_config[\"aoai_api_base\"],\n            api_type=\"azure\",\n            api_version=aoai_config[\"aoai_api_version\"],\n        )\n\n        pf = PFClient()\n        pf.connections.create_or_update(connection)\n\n        self.flow = load_flow(flow_config[\"standard_flow_path\"])\n        self.flow.context = FlowContext(\n            connections={\"setup_env\": {\"connection\": connection}},\n            overrides={\n                \"inputs.config.default\": {\n                    \"EMBEDDING_MODEL_DEPLOYMENT_NAME\": flow_config[\n                        \"EMBEDDING_MODEL_DEPLOYMENT_NAME\"\n                    ],\n                    \"CHAT_MODEL_DEPLOYMENT_NAME\": flow_config[\n                        \"CHAT_MODEL_DEPLOYMENT_NAME\"\n                    ],\n                    \"PROMPT_TOKEN_LIMIT\": flow_config[\"PROMPT_TOKEN_LIMIT\"],\n                    \"MAX_COMPLETION_TOKENS\": flow_config[\"MAX_COMPLETION_TOKENS\"],\n                    \"VERBOSE\": flow_config[\"VERBOSE\"],\n                    \"CHUNK_SIZE\": flow_config[\"CHUNK_SIZE\"],\n                    \"CHUNK_OVERLAP\": flow_config[\"CHUNK_OVERLAP\"],\n                }\n            },\n        )\n\n    def __call__(self, *, chat_history: str, pdf_url: str, question: str, **kwargs):\n        \"\"\"Invoke the flow for a single request.\"\"\"\n        return self.flow(chat_history=chat_history, pdf_url=pdf_url, question=question)\n"}
{"type": "source_file", "path": "flows/chat_with_pdf/standard/download_tool.py", "content": "\"\"\"Promptflow tool for downloading by url.\"\"\"\nfrom promptflow import tool\nfrom chat_with_pdf.download import download\n\n\n@tool\ndef download_tool(url: str, env_ready_signal: str) -> str:\n    \"\"\"Download resource by provided url.\"\"\"\n    return download(url)\n"}
{"type": "source_file", "path": "flows/chat_with_pdf/standard/qna_tool.py", "content": "\"\"\"Promptflow tool for qna.\"\"\"\nfrom promptflow import tool\nfrom chat_with_pdf.qna import qna\n\n\n@tool\ndef qna_tool(prompt: str, history: list):\n    \"\"\"QNA using prompt on chat history.\"\"\"\n    stream = qna(prompt, convert_chat_history_to_chatml_messages(history))\n\n    answer = \"\"\n    for str in stream:\n        answer = answer + str + \"\"\n\n    return {\"answer\": answer}\n\n\ndef convert_chat_history_to_chatml_messages(history):\n    \"\"\"Convert chat hostory to messages.\"\"\"\n    messages = []\n    for item in history:\n        messages.append({\"role\": \"user\", \"content\": item[\"inputs\"][\"question\"]})\n        messages.append({\"role\": \"assistant\", \"content\": item[\"outputs\"][\"answer\"]})\n\n    return messages\n"}
{"type": "source_file", "path": "flows/plan_and_execute/standard/solver.py", "content": "\"\"\"Solver node for the plan_and_execute flow.\"\"\"\nfrom promptflow.core import tool\nfrom autogen import AssistantAgent\nfrom connection_utils import CustomConnection\n\n\n# The inputs section will change based on the arguments of the tool function, after you save the code\n# Adding type to arguments and return value will help the system show the types properly\n# Please update the function name/signature per need\n@tool\ndef solver_tool(\n    connection: CustomConnection, system_message: str, question: str, results: str\n) -> str:\n    \"\"\"Create a final response to the user's request.\"\"\"\n    config_list_gpt4 = [\n        {\n            \"model\": connection.configs[\"aoai_model_gpt4\"],\n            \"api_key\": connection.secrets[\"aoai_api_key\"],\n            \"base_url\": connection.configs[\"aoai_base_url\"],\n            \"api_type\": \"azure\",\n            \"api_version\": connection.configs[\"aoai_api_version\"],\n        }\n    ]\n\n    solver = AssistantAgent(\n        name=\"SOLVER\",\n        description=\"\"\"\n        An agent expert in creating a final response to the user's request.\n        \"\"\",\n        system_message=system_message,\n        code_execution_config=False,\n        llm_config={\"config_list\": config_list_gpt4, \"timeout\": 60, \"cache_seed\": None},\n    )\n\n    solver_message = f\"\"\"\n    Question:\n    {question}\n\n    Step results:\n    {results}\n    \"\"\"\n\n    return solver.generate_reply(messages=[{\"content\": solver_message, \"role\": \"user\"}])\n"}
{"type": "source_file", "path": "flows/chat_with_pdf/standard/setup_env.py", "content": "\"\"\"Promptflow tool to setup environment.\"\"\"\nimport os\nfrom typing import Union\n\nfrom promptflow import tool\nfrom promptflow.connections import AzureOpenAIConnection, OpenAIConnection\n\nfrom chat_with_pdf.utils.lock import acquire_lock\n\nBASE_DIR = os.path.dirname(os.path.abspath(__file__)) + \"/chat_with_pdf/\"\n\n\n@tool\ndef setup_env(connection: Union[AzureOpenAIConnection, OpenAIConnection], config: dict):\n    \"\"\"Provide setup for the environment.\"\"\"\n    if not connection or not config:\n        return\n\n    if isinstance(connection, AzureOpenAIConnection):\n        os.environ[\"OPENAI_API_TYPE\"] = \"azure\"\n        os.environ[\"OPENAI_API_BASE\"] = connection.api_base\n        os.environ[\"OPENAI_API_KEY\"] = connection.api_key\n        os.environ[\"OPENAI_API_VERSION\"] = connection.api_version\n\n    if isinstance(connection, OpenAIConnection):\n        os.environ[\"OPENAI_API_KEY\"] = connection.api_key\n        if connection.organization is not None:\n            os.environ[\"OPENAI_ORG_ID\"] = connection.organization\n\n    for key in config:\n        os.environ[key] = str(config[key])\n\n    with acquire_lock(BASE_DIR + \"create_folder.lock\"):\n        if not os.path.exists(BASE_DIR + \".pdfs\"):\n            os.mkdir(BASE_DIR + \".pdfs\")\n        if not os.path.exists(BASE_DIR + \".index/.pdfs\"):\n            os.makedirs(BASE_DIR + \".index/.pdfs\")\n\n    return \"Ready\"\n"}
{"type": "source_file", "path": "flows/chat_with_pdf/standard/chat_with_pdf/find_context.py", "content": "\"\"\"Find context in index.\"\"\"\nimport faiss\nfrom jinja2 import Environment, FileSystemLoader\nimport os\n\nfrom utils.index import FAISSIndex\nfrom utils.oai import OAIEmbedding, render_with_token_limit\nfrom utils.logging import log\n\n\ndef find_context(question: str, index_path: str):\n    \"\"\"Find context in faiss index.\"\"\"\n    index = FAISSIndex(index=faiss.IndexFlatL2(1536), embedding=OAIEmbedding())\n    index.load(path=index_path)\n    snippets = index.query(question, top_k=5)\n\n    template = Environment(\n        loader=FileSystemLoader(os.path.dirname(os.path.abspath(__file__)))\n    ).get_template(\"qna_prompt.md\")\n    token_limit = int(os.environ.get(\"PROMPT_TOKEN_LIMIT\"))\n\n    # Try to render the template with token limit and reduce snippet count if it fails\n    while True:\n        try:\n            prompt = render_with_token_limit(\n                template, token_limit, question=question, context=enumerate(snippets)\n            )\n            break\n        except ValueError:\n            snippets = snippets[:-1]\n            log(f\"Reducing snippet count to {len(snippets)} to fit token limit\")\n\n    return prompt, snippets\n"}
{"type": "source_file", "path": "flows/plan_and_execute/evaluate/evaluate.py", "content": "\"\"\"Evaluation flow for plan_and_execute flow.\"\"\"\nimport argparse\nfrom pprint import pprint\nfrom promptflow.evals.evaluate import evaluate\nfrom promptflow.core import AzureOpenAIModelConfiguration\nfrom mlops.common.config_utils import MLOpsConfig\nfrom src.evaluators.json_evaluator import JsonEvaluator\nfrom src.evaluators.executor_evaluator import ExecutorEvaluator\nfrom promptflow.evals.evaluators import (\n    GroundednessEvaluator,\n    RelevanceEvaluator,\n    SimilarityEvaluator,\n)\nfrom flows.plan_and_execute.evaluate.flow_wrapper import PlanAndExecuteFlowWrapper\nfrom mlops.common.naming_tools import generate_experiment_name\n\n\ndef main():\n    \"\"\"Implement parameter reading and evaluation flow.\"\"\"\n    # Config parameters\n    parser = argparse.ArgumentParser(\"config_parameters\")\n    parser.add_argument(\n        \"--environment_name\",\n        type=str,\n        required=True,\n        help=\"env_name from config.yaml\",\n    )\n    args = parser.parse_args()\n\n    mlops_config = MLOpsConfig(environment=args.environment_name)\n    flow_config = mlops_config.get_flow_config(flow_name=\"plan_and_execute\")\n\n    json_schema_path = flow_config[\"json_schema_path\"]\n    data_eval_path = flow_config[\"eval_data_path\"]\n    flow_standard_path = flow_config[\"standard_flow_path\"]\n\n    aistudio_config = mlops_config.aistudio_config\n    openai_config = mlops_config.aoai_config\n\n    model_config = AzureOpenAIModelConfiguration(\n        azure_endpoint=openai_config[\"aoai_api_base\"],\n        api_key=openai_config[\"aoai_api_key\"],\n        api_version=openai_config[\"aoai_api_version\"],\n        azure_deployment=flow_config[\"deployment_name_gpt4\"],\n    )\n\n    json_evaluator = JsonEvaluator(json_schema_path)\n    executor_evaluator = ExecutorEvaluator()\n    groundedness_evaluator = GroundednessEvaluator(model_config)\n    relevance_evaluator = RelevanceEvaluator(model_config)\n    similarity_evaluator = SimilarityEvaluator(model_config)\n\n    connection_secrets = {\n        \"aoai_api_key\": openai_config[\"aoai_api_key\"],\n        \"bing_api_key\": flow_config[\"bing_api_key\"],\n    }\n\n    connection_configs = {\n        \"aoai_model_gpt4\": flow_config[\"deployment_name_gpt4\"],\n        \"aoai_model_gpt35\": flow_config[\"deployment_name_gpt35\"],\n        \"aoai_base_url\": openai_config[\"aoai_api_base\"],\n        \"aoai_api_version\": flow_config[\"aoai_api_version\"],\n        \"bing_endpoint\": flow_config[\"bing_endpoint\"],\n    }\n\n    flow = PlanAndExecuteFlowWrapper(\n        flow_standard_path,\n        flow_config[\"connection_name\"],\n        connection_secrets,\n        connection_configs,\n    )\n\n    results = evaluate(\n        evaluation_name=generate_experiment_name(\"plan_and_execute\"),\n        data=data_eval_path,\n        target=flow,\n        evaluators={\n            \"json_evaluator\": json_evaluator,\n            \"executor_evaluator\": executor_evaluator,\n            \"groundedness_evaluator\": groundedness_evaluator,\n            \"relevance_evaluator\": relevance_evaluator,\n            \"similarity_evaluator\": similarity_evaluator,\n        },\n        evaluator_config={\n            \"json_evaluator\": {\"json_string\": \"${target.plan}\"},\n            \"executor_evaluator\": {\n                \"plan_steps_count\": \"${target.number_of_steps}\",\n                \"result_string\": \"${target.steps}\",\n            },\n            \"groundedness_evaluator\": {\n                \"response\": \"${target.answer}\",\n                \"context\": \"${target.steps}\",\n            },\n            \"relevance_evaluator\": {\n                \"question\": \"${data.question}\",\n                \"response\": \"${target.answer}\",\n                \"context\": \"${target.steps}\",\n            },\n            \"similarity_evaluator\": {\n                \"question\": \"${data.question}\",\n                \"answer\": \"${target.answer}\",\n                \"ground_truth\": \"${data.answer}\",\n            },\n        },\n        azure_ai_project={\n            \"subscription_id\": aistudio_config[\"subscription_id\"],\n            \"resource_group_name\": aistudio_config[\"resource_group_name\"],\n            \"project_name\": aistudio_config[\"project_name\"],\n        },\n    )\n\n    pprint(results)\n\n\nif __name__ == \"__main__\":\n    main()\n"}
{"type": "source_file", "path": "flows/plan_and_execute/evaluate/flow_wrapper.py", "content": "\"\"\"Implement a wrapper for yaml based flow since it's not supported by evaluate explicitly.\"\"\"\nfrom promptflow.client import load_flow\nfrom promptflow.entities import CustomConnection\nfrom promptflow.client import PFClient\n\n\nclass PlanAndExecuteFlowWrapper:\n    \"\"\"Implement the flow.\"\"\"\n\n    def __init__(\n        self,\n        flow_standard_path: str,\n        connection_name: str,\n        connection_secrets: dict,\n        connection_configs: dict,\n    ):\n        \"\"\"Initialize environment and load prompty into the memory.\"\"\"\n        connection = CustomConnection(\n            name=connection_name, secrets=connection_secrets, configs=connection_configs\n        )\n\n        pf = PFClient()\n        pf.connections.create_or_update(connection)\n\n        self.flow = load_flow(flow_standard_path)\n\n    def __call__(self, *, question: str, **kwargs):\n        \"\"\"Invoke the flow for a single request.\"\"\"\n        return self.flow(question=question)\n"}
{"type": "source_file", "path": "flows/__init__.py", "content": ""}
{"type": "source_file", "path": "flows/plan_and_execute/standard/connection_utils.py", "content": "\"\"\"This are helper classes to provide custom connection information in promptflow.\"\"\"\nfrom promptflow.connections import CustomStrongTypeConnection\nfrom promptflow.contracts.types import Secret\n\n\nclass CustomConnection(CustomStrongTypeConnection):\n    \"\"\"Define the custom connection keys and values.\n\n    :param aoai_api_key: The api key for Azure Open AI.\n    :type aoai_api_key: Secret\n    :param bing_api_key: The api key for the Bing Search.\n    :type bing_api_key: Secret\n    :param aoai_model_gpt4: The deployment name for the GPT-4 model.\n    :type aoai_model_gpt4: String\n    :param aoai_model_gpt35: The deployment name for the GPT-3.5 model.\n    :type aoai_model_gpt35: String\n    :param aoai_base_url: The base url for the Azure Open AI.\n    :type aoai_base_url: String\n    :param aoai_api_version: The api version for the Azure Open AI.\n    :type aoai_api_version: String\n    :param bing_endpoint: The endpoint for the Bing Search.\n    :type bing_endpoint: String\n    \"\"\"\n\n    aoai_api_key: Secret\n    bing_api_key: Secret\n    aoai_model_gpt4: str\n    aoai_model_gpt35: str\n    aoai_base_url: str\n    aoai_api_version: str\n    bing_endpoint: str\n\n\nclass ConnectionInfo(object):\n    \"\"\"Singleton class to store connection information.\"\"\"\n\n    def __new__(cls):\n        \"\"\"Store connection information.\"\"\"\n        if not hasattr(cls, \"instance\"):\n            cls.instance = super(ConnectionInfo, cls).__new__(cls)\n        return cls.instance\n"}
{"type": "source_file", "path": "flows/plan_and_execute/standard/tools.py", "content": "\"\"\"Tools definitions for AutoGen.\"\"\"\nfrom autogen import AssistantAgent, UserProxyAgent\nfrom autogen.agentchat import register_function\nfrom connection_utils import ConnectionInfo\nfrom typing_extensions import Annotated, Optional\n\ntool_descriptions = {\n    \"web_tool\": {\n        \"function\": (\n            \"Worker that searches results from the internet. Useful when you need to find short and succinct \"\n            \"answers about a specific topic.\"\n        ),\n        \"query\": \"The search query string.\",\n        \"number_of_results\": \"The number of search results to return.\",\n    },\n    \"wikipedia_tool\": {\n        \"function\": (\n            \"Worker that search for page contents from Wikipedia. Useful when you need to get holistic \"\n            \"knowledge about people, places, companies, historical events, or other subjects. You use it when you \"\n            \"already have identified the entity name, usually after searching for the entity name using web_tool.\"\n        ),\n        \"query\": \"The single person name, entity, or concept to be searched.\",\n        \"number_of_results\": \"The number of search results to return.\",\n    },\n    \"llm_tool\": {\n        \"function\": (\n            \"An agent expert in solving problems by analyzing and extracting information from the given \"\n            \"context. It should never be used to do calculations.\"\n        ),\n        \"request\": \"The request to be answered.\",\n        \"context\": \"Context with the relevant information to answer the request.\",\n    },\n    \"math_tool\": {\n        \"function\": (\n            \"A tool that can solve math problems by computing arithmetic expressions. It must be used \"\n            \"whenever you need to do calculations or solve math problems. \"\n            \"You can use it to solve simple or complex math problems.\"\n        ),\n        \"problem_description\": \"The problem to be solved.\",\n        \"context\": \"Context with the relevant information to solve the problem.\",\n    },\n}\n\n\ndef register_tools(agent):\n    \"\"\"Register tools for the agent.\"\"\"\n    for tool in tool_descriptions.keys():\n        register_function(\n            globals()[tool],\n            caller=agent,\n            executor=agent,\n            description=tool_descriptions[tool][\"function\"],\n        )\n\n\ndef llm_tool(\n    request: Annotated[str, tool_descriptions[\"llm_tool\"][\"request\"]],\n    context: Optional[Annotated[str, tool_descriptions[\"llm_tool\"][\"context\"]]] = None,\n) -> str:\n    \"\"\"Use an LLM to analyze and extract information from the given context to answer the request.\"\"\"\n    connection_info = ConnectionInfo().connection_info\n\n    try:\n        llm_assistant = AssistantAgent(\n            name=\"LLM_ASSISTANT\",\n            description=(\n                \"An agent expert in answering requests by analyzing and extracting information from the given context.\"\n            ),\n            system_message=(\n                \"Given a request and optionally some context with potentially relevant information to answer it, \"\n                \"analyze the context and extract the information needed to answer the request. \"\n                \"Then, create a sentence that answers the request. \"\n                \"You must strictly limit your response to only what was asked in the request.\"\n            ),\n            code_execution_config=False,\n            llm_config={\n                \"config_list\": [\n                    {\n                        \"model\": connection_info[\"aoai_model_gpt35\"],\n                        \"api_key\": connection_info[\"aoai_api_key\"],\n                        \"base_url\": connection_info[\"aoai_base_url\"],\n                        \"api_type\": \"azure\",\n                        \"api_version\": connection_info[\"aoai_api_version\"],\n                    }\n                ],\n                \"timeout\": 60,\n                \"temperature\": 0.3,\n                \"cache_seed\": None,\n            },\n        )\n    except Exception as e:\n        print(\"LLM_ASSISTANT error:\", e)\n        return \"\"\n\n    llm_assistant.clear_history()\n\n    message = f\"\"\"\n    Request:\n    {request}\n\n    Context:\n    {context}\n    \"\"\"\n    try:\n        reply = llm_assistant.generate_reply(\n            messages=[{\"content\": message, \"role\": \"user\"}]\n        )\n        return reply\n    except Exception as e:\n        return f\"Error: {str(e)}\"\n\n\ndef web_tool(\n    query: Annotated[str, tool_descriptions[\"web_tool\"][\"query\"]],\n    number_of_results: Optional[\n        Annotated[int, tool_descriptions[\"web_tool\"][\"number_of_results\"]]\n    ] = 3,\n) -> list:\n    \"\"\"Search results from the internet.\"\"\"\n    import requests\n    from bs4 import BeautifulSoup\n\n    connection_info = ConnectionInfo().connection_info\n\n    headers = {\"Ocp-Apim-Subscription-Key\": connection_info[\"bing_api_key\"]}\n    params = {\n        \"q\": query,\n        \"count\": number_of_results,\n        \"offset\": 0,\n        \"mkt\": \"en-US\",\n        \"safesearch\": \"Strict\",\n        \"textDecorations\": False,\n        \"textFormat\": \"HTML\",\n    }\n    response = requests.get(\n        connection_info[\"bing_endpoint\"], headers=headers, params=params\n    )\n    response.raise_for_status()\n    results = response.json()\n\n    search_results = []\n    for i in range(len(results[\"webPages\"][\"value\"])):\n        title = results[\"webPages\"][\"value\"][i][\"name\"]\n        url = results[\"webPages\"][\"value\"][i][\"url\"]\n        snippet = results[\"webPages\"][\"value\"][i][\"snippet\"]\n\n        try:\n            response = requests.get(url)\n            if response.status_code == 200:\n                soup = BeautifulSoup(response.content, \"html.parser\")\n                text = soup.get_text(separator=\" \", strip=True)\n                text = text[:5000]\n            else:\n                text = f\"Failed to fetch content, status code: {response.status_code}\"\n        except Exception as e:\n            text = f\"Error fetching the page: {str(e)}\"\n\n        search_results.append(\n            {\"title\": title, \"url\": url, \"snippet\": snippet, \"content\": text}\n        )\n\n    return llm_tool(query, search_results)\n\n\ndef wikipedia_tool(\n    query: Annotated[str, tool_descriptions[\"wikipedia_tool\"][\"query\"]],\n    number_of_results: Optional[\n        Annotated[int, tool_descriptions[\"wikipedia_tool\"][\"number_of_results\"]]\n    ] = 3,\n) -> list:\n    \"\"\"Search for page contents from Wikipedia.\"\"\"\n    import wikipedia\n\n    wikipedia.set_lang(\"en\")\n    results = wikipedia.search(query, results=number_of_results)\n\n    search_results = []\n\n    for title in results:\n        try:\n            page = wikipedia.page(title)\n            search_results.append(\n                {\"title\": page.title, \"url\": page.url, \"content\": page.content[:5000]}\n            )\n        except wikipedia.exceptions.DisambiguationError:\n            continue\n        except wikipedia.exceptions.PageError:\n            continue\n        except Exception as e:\n            search_results.append(f\"Error fetching the page: {str(e)}\")\n\n    return search_results\n\n\ndef math_tool(\n    problem_description: Annotated[\n        str, tool_descriptions[\"math_tool\"][\"problem_description\"]\n    ],\n    context: Optional[Annotated[str, tool_descriptions[\"math_tool\"][\"context\"]]] = None,\n) -> str:\n    \"\"\"Solve math problems by computing arithmetic expressions.\"\"\"\n    connection_info = ConnectionInfo().connection_info\n\n    def is_termination_msg(content):\n        have_content = content.get(\"content\", None) is not None\n        if have_content and \"TERMINATE\" in content[\"content\"]:\n            return True\n        return False\n\n    math_assistant = AssistantAgent(\n        name=\"MATH_ASSISTANT\",\n        description=\"An agent expert in solving math problems and math expressions.\",\n        system_message=(\n            \"Given a math problem and optionally some context with relevant information to solve the problem, \"\n            \"translate the math problem into an expression that can be executed using Python's numexpr library. \"\n            \"Then, use the available tool (evaluate_math_expression) to solve the expression and return the result. \"\n            \"Reply 'TERMINATE' in the end when everything is done.\"\n        ),\n        code_execution_config=False,\n        is_termination_msg=is_termination_msg,\n        llm_config={\n            \"config_list\": [\n                {\n                    \"model\": connection_info[\"aoai_model_gpt4\"],\n                    \"api_key\": connection_info[\"aoai_api_key\"],\n                    \"base_url\": connection_info[\"aoai_base_url\"],\n                    \"api_type\": \"azure\",\n                    \"api_version\": connection_info[\"aoai_api_version\"],\n                }\n            ],\n            \"timeout\": 60,\n            \"cache_seed\": None,\n        },\n    )\n\n    math_executor = UserProxyAgent(\n        name=\"TOOL_EXECUTOR\",\n        description=(\n            \"An agent that acts as a proxy for the user and executes \"\n            \"the suggested function calls from MATH_ASSISTANT.\"\n        ),\n        code_execution_config=False,\n        is_termination_msg=is_termination_msg,\n        human_input_mode=\"NEVER\",\n    )\n\n    tool_descriptions = {\n        \"evaluate_math_expression\": {\n            \"function\": \"Function to evaluate math expressions using Python's numexpr library.\",\n            \"expression\": \"The expression to be evaluated. It should be a valid numerical expression.\",\n        }\n    }\n\n    @math_executor.register_for_execution()\n    @math_assistant.register_for_llm(\n        description=tool_descriptions[\"evaluate_math_expression\"][\"function\"]\n    )\n    def evaluate_math_expression(\n        expression: Annotated[\n            str, tool_descriptions[\"evaluate_math_expression\"][\"expression\"]\n        ]\n    ) -> str:\n        import math\n        import numexpr\n        import re\n\n        try:\n            local_dict = {\"pi\": math.pi, \"e\": math.e}\n            output = str(\n                numexpr.evaluate(\n                    expression.strip(),\n                    global_dict={},  # restrict access to globals\n                    local_dict=local_dict,  # add common mathematical functions\n                )\n            )\n        except Exception as e:\n            raise ValueError(\n                f'Failed to evaluate \"{expression}\". Raised error: {repr(e)}. '\n                \"Please try again with a valid numerical expression.\"\n            )\n\n        return re.sub(r\"^\\[|\\]$\", \"\", output)\n\n    message = f\"\"\"\n    Problem:\n    {problem_description}\n\n    Context:\n    {context}\n    \"\"\"\n    math_assistant.clear_history()\n    math_executor.clear_history()\n\n    math_executor.initiate_chat(\n        message=message, recipient=math_assistant, silent=True, clear_history=True\n    )\n    result = math_executor.last_message()[\"content\"].split(\"TERMINATE\")[0].strip()\n    return result\n"}
{"type": "source_file", "path": "flows/class_plan_and_execute/standard/executor.py", "content": "\"\"\"Executor node of the plan_and_execute flow.\"\"\"\nimport os\nimport concurrent.futures\nimport json\nfrom flows.class_plan_and_execute.standard.multiprocressed_agents import (\n    MultiProcessedUserProxyAgent as UserProxyAgent,\n)\nfrom flows.class_plan_and_execute.standard.multiprocressed_agents import (\n    MultiProcessedAssistantAgent as AssistantAgent,\n)\nfrom promptflow.tracing import trace\n\n\nclass Executor:\n    \"\"\"Executor agent.\"\"\"\n\n    def __init__(self):\n        \"\"\"Initialize the executor agent.\"\"\"\n        self.config_list = [\n            {\n                \"model\": os.getenv(\"aoai_model_gpt35\"),\n                \"api_key\": os.getenv(\"aoai_api_key\"),\n                \"base_url\": os.getenv(\"aoai_base_url\"),\n                \"api_type\": \"azure\",\n                \"api_version\": os.getenv(\"aoai_api_version\"),\n            }\n        ]\n\n        self.executor = UserProxyAgent(\n            name=\"EXECUTOR\",\n            description=(\n                \"An agent that acts as a proxy for the user and executes the \"\n                \"suggested function calls from PLANNER.\"\n            ),\n            code_execution_config=False,\n            llm_config={\n                \"config_list\": self.config_list,\n                \"timeout\": 60,\n                \"cache_seed\": None,\n            },\n            human_input_mode=\"NEVER\",\n        )\n\n        # register_tools(self.executor)\n\n    def _llm_tool(self, request, context, config_list):\n        \"\"\"Define the internal auxiliary LLM agent for the executor.\"\"\"\n        llm_assistant = AssistantAgent(\n            name=\"LLM_ASSISTANT\",\n            description=(\n                \"An agent expert in answering requests by analyzing and \"\n                \"extracting information from the given context.\"\n            ),\n            system_message=(\n                \"Given a request and optionally some context with potentially \"\n                \"relevant information to answer it, analyze the context and \"\n                \"extract the information needed to answer the request. Then, \"\n                \"create a sentence that answers the request. You must strictly \"\n                \"limit your response to only what was asked in the request.\"\n            ),\n            code_execution_config=False,\n            llm_config={\n                \"config_list\": config_list,\n                \"timeout\": 60,\n                \"temperature\": 0.3,\n                \"cache_seed\": None,\n            },\n        )\n\n        llm_assistant.clear_history()\n\n        message = f\"\"\"\n        Request:\n        {request}\n\n        Context:\n        {context}\n        \"\"\"\n        try:\n            reply = llm_assistant.generate_reply(\n                messages=[{\"content\": message, \"role\": \"user\"}]\n            )\n            return reply\n        except Exception as e:\n            return f\"Error: {str(e)}\"\n\n    def _substitute_dependency(\n        self, id, original_argument_value, dependency_value, config_list\n    ):\n        \"\"\"Substitute dependencies in the execution plan.\"\"\"\n        instruction = (\n            \"Extract the entity name or fact from the dependency value in a way \"\n            \"that makes sense to use it to substitute the variable #E in the \"\n            \"original argument value. Do not include any other text in your \"\n            \"response, other than the entity name or fact extracted.\"\n        )\n\n        context = f\"\"\"\n        original argument value:\n        {original_argument_value}\n\n        dependency value:\n        {dependency_value}\n\n        extracted fact or entity:\n\n        \"\"\"\n\n        return self._llm_tool(instruction, context, config_list)\n\n    def _has_unresolved_dependencies(self, item, resolved_ids, plan_ids):\n        \"\"\"Check for unresolved dependencies in a plan step.\"\"\"\n        try:\n            args = json.loads(item[\"function\"][\"arguments\"])\n        except json.JSONDecodeError:\n            return False\n\n        for arg in args.values():\n            if isinstance(arg, str) and any(\n                ref_id\n                for ref_id in plan_ids\n                if ref_id not in resolved_ids and ref_id in arg\n            ):\n                return True\n        return False\n\n    def _submit_task(self, item_id, item, thread_executor, executor_agent, futures):\n        \"\"\"Submit a task for execution.\"\"\"\n        arguments = item[\"function\"][\"arguments\"]\n        future = thread_executor.submit(\n            executor_agent.execute_function,\n            {\"name\": item[\"function\"][\"name\"], \"arguments\": arguments},\n        )\n        futures[item_id] = future\n\n    def _update_and_submit_task(\n        self,\n        item_id,\n        item,\n        thread_executor,\n        executor_agent,\n        futures,\n        results,\n        config_list,\n    ):\n        \"\"\"Update the arguments of a task with dependency results and submit it for execution.\"\"\"\n        updated_arguments = json.loads(item[\"function\"][\"arguments\"])\n        for arg_key, arg_value in updated_arguments.items():\n            if isinstance(arg_value, str):\n                for res_id, res in results.items():\n                    if arg_key == \"context\":\n                        arg_value = arg_value.replace(res_id, res[\"content\"])\n                    else:\n                        arg_value = arg_value.replace(\n                            res_id,\n                            self._substitute_dependency(\n                                res_id, arg_value, res[\"content\"], config_list\n                            ),\n                        )\n                    updated_arguments[arg_key] = arg_value\n        future = thread_executor.submit(\n            executor_agent.execute_function,\n            {\n                \"name\": item[\"function\"][\"name\"],\n                \"arguments\": json.dumps(updated_arguments),\n            },\n        )\n        futures[item_id] = future\n\n    def _submit_ready_tasks(\n        self,\n        plan_ids,\n        resolved_ids,\n        futures,\n        results,\n        thread_executor,\n        executor_agent,\n        config_list,\n    ):\n        \"\"\"Submit plan tasks that have all dependencies resolved and are ready to be executed.\"\"\"\n        for next_item_id, next_item in plan_ids.items():\n            if (\n                next_item_id not in resolved_ids\n                and next_item_id not in futures\n                and not self._has_unresolved_dependencies(\n                    next_item, resolved_ids, plan_ids\n                )\n            ):\n                self._update_and_submit_task(\n                    next_item_id,\n                    next_item,\n                    thread_executor,\n                    executor_agent,\n                    futures,\n                    results,\n                    config_list,\n                )\n\n    def _process_done_future(\n        self,\n        future,\n        futures,\n        results,\n        resolved_ids,\n        plan_ids,\n        thread_executor,\n        executor_agent,\n        config_list,\n    ):\n        \"\"\"Process a completed future and trigger the submission of ready tasks.\"\"\"\n        item_id = next((id for id, f in futures.items() if f == future), None)\n        if item_id:\n            _, result = future.result()\n            results[item_id] = result\n            resolved_ids.add(item_id)\n            del futures[item_id]\n            self._submit_ready_tasks(\n                plan_ids,\n                resolved_ids,\n                futures,\n                results,\n                thread_executor,\n                executor_agent,\n                config_list,\n            )\n\n    @trace\n    def execute_plan_parallel(self, plan):\n        \"\"\"Execute the plan in parallel.\"\"\"\n        plan = plan[\"Functions\"]\n        plan_ids = {item[\"id\"]: item for item in plan}\n        results = {}\n        resolved_ids = set()\n        futures = {}\n\n        with concurrent.futures.ThreadPoolExecutor() as thread_executor:\n            for item_id, item in plan_ids.items():\n                if not self._has_unresolved_dependencies(item, resolved_ids, plan_ids):\n                    self._submit_task(\n                        item_id, item, thread_executor, self.executor, futures\n                    )\n\n            while futures:\n                done, _ = concurrent.futures.wait(\n                    futures.values(), return_when=concurrent.futures.FIRST_COMPLETED\n                )\n                for future in done:\n                    self._process_done_future(\n                        future,\n                        futures,\n                        results,\n                        resolved_ids,\n                        plan_ids,\n                        thread_executor,\n                        self.executor,\n                        self.config_list,\n                    )\n\n        result_str = \"\\n\".join(\n            [f\"{key} = {value['content']}\" for key, value in results.items()]\n        )\n        return result_str\n"}
{"type": "source_file", "path": "flows/class_plan_and_execute/standard/plan_and_execute.py", "content": "\"\"\"Implement plan_and_execute flow as a class.\"\"\"\nfrom flows.class_plan_and_execute.standard.planner import Planner\nfrom flows.class_plan_and_execute.standard.executor import Executor\nfrom flows.class_plan_and_execute.standard.solver import Solver\nfrom flows.class_plan_and_execute.standard.tools import (\n    tool_descriptions,\n    _web_tool,\n    _llm_tool,\n    _wikipedia_tool,\n    _math_tool,\n)\nfrom typing import Any\nfrom autogen.agentchat import register_function\nfrom promptflow.tracing import start_trace\n\n\nclass PlanAndExecute:\n    \"\"\"Implement the flow.\"\"\"\n\n    # tool wrappers for the agents\n    # need to be staticmethod to have _name attribute\n\n    @staticmethod\n    def web_tool(*args: Any, **kwargs: Any) -> Any:\n        \"\"\"Wrap the web_tool function.\"\"\"\n        return _web_tool(*args, **kwargs)\n\n    @staticmethod\n    def llm_tool(*args: Any, **kwargs: Any) -> Any:\n        \"\"\"Wrap the llm_tool function.\"\"\"\n        return _llm_tool(*args, **kwargs)\n\n    @staticmethod\n    def wikipedia_tool(*args: Any, **kwargs: Any) -> Any:\n        \"\"\"Wrap the wikipedia_tool function.\"\"\"\n        return _wikipedia_tool(*args, **kwargs)\n\n    @staticmethod\n    def math_tool(*args: Any, **kwargs: Any) -> Any:\n        \"\"\"Wrap the math_tool function.\"\"\"\n        return _math_tool(*args, **kwargs)\n\n    wrapper_mapping = {\n        \"web_tool\": web_tool,\n        \"llm_tool\": llm_tool,\n        \"wikipedia_tool\": wikipedia_tool,\n        \"math_tool\": math_tool,\n    }\n\n    def __init__(\n        self, planner_system_message_path: str, solver_system_message_path: str\n    ):\n        \"\"\"Initialize the environment.\"\"\"\n        start_trace(collection=\"plan_and_execute\")\n        self.planner = Planner(system_message_path=planner_system_message_path)\n        self.executor = Executor()\n        self.solver = Solver(system_message_path=solver_system_message_path)\n\n        self.initialized = False\n\n    def __call__(self, *, question: str, **kwargs):\n        \"\"\"Invoke the flow for a single request.\"\"\"\n        if not self.initialized:\n            for tool_name, wrapper in self.wrapper_mapping.items():\n                register_function(\n                    wrapper,\n                    caller=self.planner.planner,\n                    executor=self.executor.executor,\n                    description=tool_descriptions[tool_name][\"function\"],\n                )\n            self.initialized = True\n\n        plan = self.planner.generate_plan(question=question)\n        execution = self.executor.execute_plan_parallel(plan=plan)\n        response = self.solver.generate_response(question=question, results=execution)\n        number_of_steps = len(plan[\"Plan\"])\n\n        return {\n            \"plan\": plan,\n            \"steps\": execution,\n            \"answer\": response,\n            \"number_of_steps\": number_of_steps,\n        }\n"}
{"type": "source_file", "path": "flows/class_basic_flow/evaluate/evaluate.py", "content": "\"\"\"Evaluation flow for class_basic_flow.\"\"\"\nimport argparse\nfrom pprint import pprint\nfrom promptflow.evals.evaluate import evaluate\nfrom mlops.common.config_utils import MLOpsConfig\nfrom flows.class_basic_flow.standard.extract_entities import EntityExtraction\nfrom src.evaluators.match_evaluator import MatchEvaluator\nfrom mlops.common.naming_tools import generate_experiment_name\nfrom promptflow.client import PFClient\nfrom promptflow.entities import AzureOpenAIConnection\nfrom promptflow.core import AzureOpenAIModelConfiguration\n\n\ndef main():\n    \"\"\"Implement parameter reading and evaluation flow.\"\"\"\n    # Config parameters\n    parser = argparse.ArgumentParser(\"config_parameters\")\n    parser.add_argument(\n        \"--environment_name\",\n        type=str,\n        required=True,\n        help=\"env_name from config.yaml\",\n    )\n    args = parser.parse_args()\n\n    mlops_config = MLOpsConfig(environment=args.environment_name)\n    flow_config = mlops_config.get_flow_config(flow_name=\"class_basic_flow\")\n\n    aoai_deployment = flow_config[\"deployment_name\"]\n\n    openai_config = mlops_config.aoai_config\n\n    connection = AzureOpenAIConnection(\n        name=flow_config[\"connection_name\"],\n        api_key=openai_config[\"aoai_api_key\"],\n        api_base=openai_config[\"aoai_api_base\"],\n        api_type=\"azure\",\n        api_version=openai_config[\"aoai_api_version\"],\n    )\n\n    pf = PFClient()\n    pf.connections.create_or_update(connection)\n\n    # create the model config to be used in below flow calls\n    config = AzureOpenAIModelConfiguration(\n        connection=flow_config[\"connection_name\"], azure_deployment=aoai_deployment\n    )\n\n    # Run the flow as a basic function call with no tracing\n    obj_chat = EntityExtraction(model_config=config)\n\n    matchevaluator = MatchEvaluator()\n\n    data_eval_path = flow_config[\"eval_data_path\"]\n\n    aistudio_config = mlops_config.aistudio_config\n    print(aistudio_config[\"project_name\"])\n\n    results = evaluate(\n        evaluation_name=generate_experiment_name(\"class_basic_flow\"),\n        data=data_eval_path,\n        target=obj_chat,\n        evaluators={\n            \"matchevaluator\": matchevaluator,\n        },\n        evaluator_config={\n            \"matchevaluator\": {\n                \"response\": \"${target.answer}\",\n                \"ground_truth\": \"${data.results}\",\n            },\n        },\n        azure_ai_project={\n            \"subscription_id\": aistudio_config[\"subscription_id\"],\n            \"resource_group_name\": aistudio_config[\"resource_group_name\"],\n            \"project_name\": aistudio_config[\"project_name\"],\n        },\n    )\n\n    pprint(results)\n\n\nif __name__ == \"__main__\":\n    main()\n"}
{"type": "source_file", "path": "flows/class_plan_and_execute/standard/tools.py", "content": "\"\"\"Tools definitions for AutoGen.\"\"\"\nimport os\nfrom flows.class_plan_and_execute.standard.multiprocressed_agents import (\n    MultiProcessedAssistantAgent as AssistantAgent,\n)\nfrom flows.class_plan_and_execute.standard.multiprocressed_agents import (\n    MultiProcessedUserProxyAgent as UserProxyAgent,\n)\nfrom typing_extensions import Annotated, Optional\nfrom promptflow.tracing import trace\n\ntool_descriptions = {\n    \"web_tool\": {\n        \"function\": (\n            \"Worker that searches results from the internet. Useful when you need to find short and succinct \"\n            \"answers about a specific topic.\"\n        ),\n        \"query\": \"The search query string.\",\n        \"number_of_results\": \"The number of search results to return.\",\n    },\n    \"wikipedia_tool\": {\n        \"function\": (\n            \"Worker that search for page contents from Wikipedia. Useful when you need to get holistic \"\n            \"knowledge about people, places, companies, historical events, or other subjects. You use it when you \"\n            \"already have identified the entity name, usually after searching for the entity name using web_tool.\"\n        ),\n        \"query\": \"The single person name, entity, or concept to be searched.\",\n        \"number_of_results\": \"The number of search results to return.\",\n    },\n    \"llm_tool\": {\n        \"function\": (\n            \"An agent expert in solving problems by analyzing and extracting information from the given \"\n            \"context. It should never be used to do calculations.\"\n        ),\n        \"request\": \"The request to be answered.\",\n        \"context\": \"Context with the relevant information to answer the request.\",\n    },\n    \"math_tool\": {\n        \"function\": (\n            \"A tool that can solve math problems by computing arithmetic expressions. It must be used \"\n            \"whenever you need to do calculations or solve math problems. \"\n            \"You can use it to solve simple or complex math problems.\"\n        ),\n        \"problem_description\": \"The problem to be solved.\",\n        \"context\": \"Context with the relevant information to solve the problem.\",\n    },\n}\n\n\n@trace\ndef _llm_tool(\n    request: Annotated[str, tool_descriptions[\"llm_tool\"][\"request\"]],\n    context: Optional[Annotated[str, tool_descriptions[\"llm_tool\"][\"context\"]]] = None,\n) -> str:\n    \"\"\"Use an LLM to analyze and extract information from the given context to answer the request.\"\"\"\n    try:\n        llm_assistant = AssistantAgent(\n            name=\"LLM_ASSISTANT\",\n            description=(\n                \"An agent expert in answering requests by analyzing and extracting information from the given context.\"\n            ),\n            system_message=(\n                \"Given a request and optionally some context with potentially relevant information to answer it, \"\n                \"analyze the context and extract the information needed to answer the request. \"\n                \"Then, create a sentence that answers the request. \"\n                \"You must strictly limit your response to only what was asked in the request.\"\n            ),\n            code_execution_config=False,\n            llm_config={\n                \"config_list\": [\n                    {\n                        \"model\": os.getenv(\"aoai_model_gpt35\"),\n                        \"api_key\": os.getenv(\"aoai_api_key\"),\n                        \"base_url\": os.getenv(\"aoai_base_url\"),\n                        \"api_type\": \"azure\",\n                        \"api_version\": os.getenv(\"aoai_api_version\"),\n                    }\n                ],\n                \"timeout\": 60,\n                \"temperature\": 0.3,\n                \"cache_seed\": None,\n            },\n        )\n    except Exception as e:\n        print(\"LLM_ASSISTANT error:\", e)\n        return \"\"\n\n    llm_assistant.clear_history()\n\n    message = f\"\"\"\n    Request:\n    {request}\n\n    Context:\n    {context}\n    \"\"\"\n    try:\n        reply = llm_assistant.generate_reply(\n            messages=[{\"content\": message, \"role\": \"user\"}]\n        )\n        return reply\n    except Exception as e:\n        return f\"Error: {str(e)}\"\n\n\n@trace\ndef _web_tool(\n    query: Annotated[str, tool_descriptions[\"web_tool\"][\"query\"]],\n    number_of_results: Optional[\n        Annotated[int, tool_descriptions[\"web_tool\"][\"number_of_results\"]]\n    ] = 3,\n) -> list:\n    \"\"\"Search results from the internet.\"\"\"\n    import requests\n    from bs4 import BeautifulSoup\n\n    headers = {\"Ocp-Apim-Subscription-Key\": os.getenv(\"bing_api_key\")}\n    params = {\n        \"q\": query,\n        \"count\": number_of_results,\n        \"offset\": 0,\n        \"mkt\": \"en-US\",\n        \"safesearch\": \"Strict\",\n        \"textDecorations\": False,\n        \"textFormat\": \"HTML\",\n    }\n    response = requests.get(os.getenv(\"bing_endpoint\"), headers=headers, params=params)\n    response.raise_for_status()\n    results = response.json()\n\n    search_results = []\n    for i in range(len(results[\"webPages\"][\"value\"])):\n        title = results[\"webPages\"][\"value\"][i][\"name\"]\n        url = results[\"webPages\"][\"value\"][i][\"url\"]\n        snippet = results[\"webPages\"][\"value\"][i][\"snippet\"]\n\n        try:\n            response = requests.get(url)\n            if response.status_code == 200:\n                soup = BeautifulSoup(response.content, \"html.parser\")\n                text = soup.get_text(separator=\" \", strip=True)\n                text = text[:5000]\n            else:\n                text = f\"Failed to fetch content, status code: {response.status_code}\"\n        except Exception as e:\n            text = f\"Error fetching the page: {str(e)}\"\n\n        search_results.append(\n            {\"title\": title, \"url\": url, \"snippet\": snippet, \"content\": text}\n        )\n\n    return _llm_tool(query, search_results)\n\n\n@trace\ndef _wikipedia_tool(\n    query: Annotated[str, tool_descriptions[\"wikipedia_tool\"][\"query\"]],\n    number_of_results: Optional[\n        Annotated[int, tool_descriptions[\"wikipedia_tool\"][\"number_of_results\"]]\n    ] = 3,\n) -> list:\n    \"\"\"Search for page contents from Wikipedia.\"\"\"\n    import wikipedia\n\n    wikipedia.set_lang(\"en\")\n    results = wikipedia.search(query, results=number_of_results)\n\n    search_results = []\n\n    for title in results:\n        try:\n            page = wikipedia.page(title)\n            search_results.append(\n                {\"title\": page.title, \"url\": page.url, \"content\": page.content[:5000]}\n            )\n        except wikipedia.exceptions.DisambiguationError:\n            continue\n        except wikipedia.exceptions.PageError:\n            continue\n        except Exception as e:\n            search_results.append(f\"Error fetching the page: {str(e)}\")\n\n    return search_results\n\n\n@trace\ndef _math_tool(\n    problem_description: Annotated[\n        str, tool_descriptions[\"math_tool\"][\"problem_description\"]\n    ],\n    context: Optional[Annotated[str, tool_descriptions[\"math_tool\"][\"context\"]]] = None,\n) -> str:\n    \"\"\"Solve math problems by computing arithmetic expressions.\"\"\"\n\n    def is_termination_msg(content):\n        have_content = content.get(\"content\", None) is not None\n        if have_content and \"TERMINATE\" in content[\"content\"]:\n            return True\n        return False\n\n    math_assistant = AssistantAgent(\n        name=\"MATH_ASSISTANT\",\n        description=\"An agent expert in solving math problems and math expressions.\",\n        system_message=(\n            \"Given a math problem and optionally some context with relevant information to solve the problem, \"\n            \"translate the math problem into an expression that can be executed using Python's numexpr library. \"\n            \"Then, use the available tool (evaluate_math_expression) to solve the expression and return the result. \"\n            \"Reply 'TERMINATE' in the end when everything is done.\"\n        ),\n        code_execution_config=False,\n        is_termination_msg=is_termination_msg,\n        llm_config={\n            \"config_list\": [\n                {\n                    \"model\": os.getenv(\"aoai_model_gpt4\"),\n                    \"api_key\": os.getenv(\"aoai_api_key\"),\n                    \"base_url\": os.getenv(\"aoai_base_url\"),\n                    \"api_type\": \"azure\",\n                    \"api_version\": os.getenv(\"aoai_api_version\"),\n                }\n            ],\n            \"timeout\": 60,\n            \"cache_seed\": None,\n        },\n    )\n\n    math_executor = UserProxyAgent(\n        name=\"TOOL_EXECUTOR\",\n        description=(\n            \"An agent that acts as a proxy for the user and executes \"\n            \"the suggested function calls from MATH_ASSISTANT.\"\n        ),\n        code_execution_config=False,\n        is_termination_msg=is_termination_msg,\n        human_input_mode=\"NEVER\",\n    )\n\n    tool_descriptions = {\n        \"evaluate_math_expression\": {\n            \"function\": \"Function to evaluate math expressions using Python's numexpr library.\",\n            \"expression\": \"The expression to be evaluated. It should be a valid numerical expression.\",\n        }\n    }\n\n    @math_executor.register_for_execution()\n    @math_assistant.register_for_llm(\n        description=tool_descriptions[\"evaluate_math_expression\"][\"function\"]\n    )\n    @trace\n    def evaluate_math_expression(\n        expression: Annotated[\n            str, tool_descriptions[\"evaluate_math_expression\"][\"expression\"]\n        ]\n    ) -> str:\n        import math\n        import numexpr\n        import re\n\n        try:\n            local_dict = {\"pi\": math.pi, \"e\": math.e}\n            output = str(\n                numexpr.evaluate(\n                    expression.strip(),\n                    global_dict={},  # restrict access to globals\n                    local_dict=local_dict,  # add common mathematical functions\n                )\n            )\n        except Exception as e:\n            raise ValueError(\n                f'Failed to evaluate \"{expression}\". Raised error: {repr(e)}. '\n                \"Please try again with a valid numerical expression.\"\n            )\n\n        return re.sub(r\"^\\[|\\]$\", \"\", output)\n\n    message = f\"\"\"\n    Problem:\n    {problem_description}\n\n    Context:\n    {context}\n    \"\"\"\n    math_assistant.clear_history()\n    math_executor.clear_history()\n\n    math_executor.initiate_chat(\n        message=message, recipient=math_assistant, silent=True, clear_history=True\n    )\n    result = math_executor.last_message()[\"content\"].split(\"TERMINATE\")[0].strip()\n    return result\n"}
{"type": "source_file", "path": "flows/plan_and_execute/standard/planner.py", "content": "\"\"\"Planner node for the plan_and_execute flow.\"\"\"\nfrom promptflow.core import tool\nfrom autogen import AssistantAgent\nfrom connection_utils import CustomConnection\nfrom tools import register_tools\n\n\n# The inputs section will change based on the arguments of the tool function, after you save the code\n# Adding type to arguments and return value will help the system show the types properly\n# Please update the function name/signature per need\n@tool\ndef planner_tool(\n    connection: CustomConnection, system_message: str, question: str\n) -> str:\n    \"\"\"Generate a step-by-step execution plan to solve the user's request.\"\"\"\n    config_list_gpt4 = [\n        {\n            \"model\": connection.configs[\"aoai_model_gpt4\"],\n            \"api_key\": connection.secrets[\"aoai_api_key\"],\n            \"base_url\": connection.configs[\"aoai_base_url\"],\n            \"api_type\": \"azure\",\n            \"api_version\": connection.configs[\"aoai_api_version\"],\n        }\n    ]\n\n    planner = AssistantAgent(\n        name=\"PLANNER\",\n        description=\"\"\"\n        An agent expert in creating a step-by-step execution plan to solve the user's request.\n        \"\"\",\n        system_message=system_message,\n        code_execution_config=False,\n        llm_config={\n            \"config_list\": config_list_gpt4,\n            \"temperature\": 0,\n            \"timeout\": 120,\n            \"cache_seed\": None,\n        },\n    )\n\n    register_tools(planner)\n\n    planner_reply = planner.generate_reply(\n        messages=[{\"content\": question, \"role\": \"user\"}]\n    )\n    planner_reply = planner_reply.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n\n    return planner_reply\n"}
{"type": "source_file", "path": "flows/chat_with_pdf/standard/__init__.py", "content": "import sys\nimport os\n\n# making sure that we can import modules from a subfolder of the current folder\nsys.path.append(\n    os.path.join(os.path.dirname(os.path.abspath(__file__)), \"chat_with_pdf\")\n)\n"}
{"type": "source_file", "path": "flows/chat_with_pdf/standard/chat_with_pdf/rewrite_question.py", "content": "\"\"\"Re-write question from user.\"\"\"\nfrom jinja2 import Environment, FileSystemLoader\nimport os\nfrom utils.logging import log\nfrom utils.oai import OAIChat, render_with_token_limit\n\n\ndef rewrite_question(question: str, history: list):\n    \"\"\"Re-write question from user function.\"\"\"\n    template = Environment(\n        loader=FileSystemLoader(os.path.dirname(os.path.abspath(__file__)))\n    ).get_template(\"rewrite_question_prompt.md\")\n    token_limit = int(os.environ[\"PROMPT_TOKEN_LIMIT\"])\n    max_completion_tokens = int(os.environ[\"MAX_COMPLETION_TOKENS\"])\n\n    # Try to render the prompt with token limit and reduce the history count if it fails\n    while True:\n        try:\n            prompt = render_with_token_limit(\n                template, token_limit, question=question, history=history\n            )\n            break\n        except ValueError:\n            history = history[:-1]\n            log(f\"Reducing chat history count to {len(history)} to fit token limit\")\n\n    chat = OAIChat()\n    rewritten_question = chat.generate(\n        messages=[{\"role\": \"user\", \"content\": prompt}], max_tokens=max_completion_tokens\n    )\n    log(f\"Rewritten question: {rewritten_question}\")\n\n    return rewritten_question\n"}
{"type": "source_file", "path": "flows/chat_with_pdf/standard/chat_with_pdf/constants.py", "content": "\"\"\"Constants.\"\"\"\nimport os\n\nBASE_DIR = os.path.dirname(os.path.abspath(__file__))\nPDF_DIR = os.path.join(BASE_DIR, \".pdfs\")\nINDEX_DIR = os.path.join(BASE_DIR, \".index/.pdfs/\")\n"}
{"type": "source_file", "path": "flows/function_basic_flow/evaluate/evaluate.py", "content": "\"\"\"Evaluation flow for function_basic_flow.\"\"\"\nimport os\nimport argparse\nfrom pprint import pprint\nfrom promptflow.evals.evaluate import evaluate\nfrom mlops.common.config_utils import MLOpsConfig\nfrom flows.function_basic_flow.standard.extract_entities import extract_entity\nfrom src.evaluators.match_evaluator import MatchEvaluator\nfrom mlops.common.naming_tools import generate_experiment_name\n\n\ndef main():\n    \"\"\"Implement parameter reading and evaluation flow.\"\"\"\n    # Config parameters\n    parser = argparse.ArgumentParser(\"config_parameters\")\n    parser.add_argument(\n        \"--environment_name\",\n        type=str,\n        required=True,\n        help=\"env_name from config.yaml\",\n    )\n    args = parser.parse_args()\n\n    mlops_config = MLOpsConfig(environment=args.environment_name)\n    flow_config = mlops_config.get_flow_config(flow_name=\"function_basic_flow\")\n\n    aoai_deployment = flow_config[\"deployment_name\"]\n\n    openai_config = mlops_config.aoai_config\n\n    os.environ[\"AZURE_OPENAI_API_KEY\"] = openai_config[\"aoai_api_key\"]\n    os.environ[\"AZURE_OPENAI_API_VERSION\"] = openai_config[\"aoai_api_version\"]\n    os.environ[\"AZURE_OPENAI_DEPLOYMENT\"] = aoai_deployment\n    os.environ[\"AZURE_OPENAI_ENDPOINT\"] = openai_config[\"aoai_api_base\"]\n\n    matchevaluator = MatchEvaluator()\n\n    data_eval_path = flow_config[\"eval_data_path\"]\n\n    aistudio_config = mlops_config.aistudio_config\n    print(aistudio_config[\"project_name\"])\n\n    results = evaluate(\n        evaluation_name=generate_experiment_name(\"function_basic_flow\"),\n        data=data_eval_path,\n        target=extract_entity,\n        evaluators={\n            \"matchevaluator\": matchevaluator,\n        },\n        evaluator_config={\n            \"matchevaluator\": {\n                \"response\": \"${target.answer}\",\n                \"ground_truth\": \"${data.results}\",\n            },\n        },\n        azure_ai_project={\n            \"subscription_id\": aistudio_config[\"subscription_id\"],\n            \"resource_group_name\": aistudio_config[\"resource_group_name\"],\n            \"project_name\": aistudio_config[\"project_name\"],\n        },\n    )\n\n    pprint(results)\n\n\nif __name__ == \"__main__\":\n    main()\n"}
{"type": "source_file", "path": "flows/chat_with_pdf/standard/chat_with_pdf/utils/oai.py", "content": "\"\"\"Helper classes to work with OpeAI API.\"\"\"\nfrom typing import List\nimport openai\nfrom openai.version import VERSION as OPENAI_VERSION\nimport os\nimport tiktoken\nfrom jinja2 import Template\n\nfrom .retry import (\n    retry_and_handle_exceptions,\n    retry_and_handle_exceptions_for_generator,\n)\nfrom .logging import log\n\n\ndef extract_delay_from_rate_limit_error_msg(text):\n    \"\"\"Extract delay time from rate limit error message.\"\"\"\n    import re\n\n    pattern = r\"retry after (\\d+)\"\n    match = re.search(pattern, text)\n    if match:\n        retry_time_from_message = match.group(1)\n        return float(retry_time_from_message)\n    else:\n        return 5  # default retry time\n\n\nclass OAI:\n    \"\"\"OpenAI API client class.\"\"\"\n\n    def __init__(self):\n        \"\"\"Initialize the OAI class with API credentials and configuration.\"\"\"\n        self.check_openai_version()\n        init_params = self.get_initial_params()\n        api_type = os.environ.get(\"OPENAI_API_TYPE\")\n        self.initialize_client(api_type, init_params)\n        self.sanity_checks(api_type, init_params)\n\n    def check_openai_version(self):\n        \"\"\"Check the OpenAI package version.\"\"\"\n        if OPENAI_VERSION.startswith(\"0.\"):\n            raise Exception(\n                \"Please upgrade your OpenAI package to version >= 1.0.0 or \"\n                \"using the command: pip install --upgrade openai.\"\n            )\n\n    def get_initial_params(self):\n        \"\"\"Retrieve initial parameters from environment variables.\"\"\"\n        init_params = {}\n        if os.getenv(\"OPENAI_API_VERSION\") is not None:\n            init_params[\"api_version\"] = os.environ.get(\"OPENAI_API_VERSION\")\n        if os.getenv(\"OPENAI_ORG_ID\") is not None:\n            init_params[\"organization\"] = os.environ.get(\"OPENAI_ORG_ID\")\n        if os.getenv(\"OPENAI_API_KEY\") is None:\n            raise ValueError(\"OPENAI_API_KEY is not set in environment variables\")\n\n        init_params[\"api_key\"] = os.environ.get(\"OPENAI_API_KEY\")\n        return init_params\n\n    def sanity_checks(self, api_type, init_params):\n        \"\"\"Perform sanity checks on the initial parameters.\"\"\"\n        if api_type == \"azure\":\n            if init_params.get(\"azure_endpoint\") is None:\n                raise ValueError(\n                    \"OPENAI_API_BASE is not set in environment variables, this is required when api_type==azure\"\n                )\n            if init_params.get(\"api_version\") is None:\n                raise ValueError(\n                    \"OPENAI_API_VERSION is not set in environment variables, this is required when api_type==azure\"\n                )\n            if init_params[\"api_key\"].startswith(\"sk-\"):\n                raise ValueError(\n                    \"OPENAI_API_KEY should not start with sk- when api_type==azure, \"\n                    \"are you using openai key by mistake?\"\n                )\n\n    def initialize_client(self, api_type, init_params):\n        \"\"\"Initialize the OpenAI client.\"\"\"\n        if api_type == \"azure\":\n            from openai import AzureOpenAI as Client\n\n            init_params[\"azure_endpoint\"] = os.environ.get(\"OPENAI_API_BASE\")\n        else:\n            from openai import OpenAI as Client\n\n            if os.getenv(\"OPENAI_API_BASE\") is not None:\n                init_params[\"base_url\"] = os.environ.get(\"OPENAI_API_BASE\")\n\n        self.client = Client(**init_params)\n\n\nclass OAIChat(OAI):\n    \"\"\"OpenAI Chat API client class.\"\"\"\n\n    @retry_and_handle_exceptions(\n        exception_to_check=(\n            openai.RateLimitError,\n            openai.APIStatusError,\n            openai.APIConnectionError,\n            KeyError,\n        ),\n        max_retries=5,\n        extract_delay_from_error_message=extract_delay_from_rate_limit_error_msg,\n    )\n    def generate(self, messages: list, **kwargs) -> List[float]:\n        \"\"\"Generate a response from the chat API.\"\"\"\n        # chat api may return message with no content.\n        message = (\n            self.client.chat.completions.create(\n                model=os.environ.get(\"CHAT_MODEL_DEPLOYMENT_NAME\"),\n                messages=messages,\n                **kwargs,\n            )\n            .choices[0]\n            .message\n        )\n        return getattr(message, \"content\", \"\")\n\n    @retry_and_handle_exceptions_for_generator(\n        exception_to_check=(\n            openai.RateLimitError,\n            openai.APIStatusError,\n            openai.APIConnectionError,\n            KeyError,\n        ),\n        max_retries=5,\n        extract_delay_from_error_message=extract_delay_from_rate_limit_error_msg,\n    )\n    def stream(self, messages: list, **kwargs):\n        \"\"\"Stream a response from the chat API.\"\"\"\n        response = self.client.chat.completions.create(\n            model=os.environ.get(\"CHAT_MODEL_DEPLOYMENT_NAME\"),\n            messages=messages,\n            stream=False,\n            **kwargs,\n        )\n\n        return response.choices[0].message.content\n\n\nclass OAIEmbedding(OAI):\n    \"\"\"OpenAI Embedding API client class.\"\"\"\n\n    @retry_and_handle_exceptions(\n        exception_to_check=openai.RateLimitError,\n        max_retries=5,\n        extract_delay_from_error_message=extract_delay_from_rate_limit_error_msg,\n    )\n    def generate(self, text: str) -> List[float]:\n        \"\"\"Generate an embedding for the given text.\"\"\"\n        return (\n            self.client.embeddings.create(\n                input=text, model=os.environ.get(\"EMBEDDING_MODEL_DEPLOYMENT_NAME\")\n            )\n            .data[0]\n            .embedding\n        )\n\n\ndef count_token(text: str) -> int:\n    \"\"\"Count the number of tokens in the given text.\"\"\"\n    encoding = tiktoken.get_encoding(\"cl100k_base\")\n    return len(encoding.encode(text))\n\n\ndef render_with_token_limit(template: Template, token_limit: int, **kwargs) -> str:\n    \"\"\"Render a Jinja2 template with a token limit.\"\"\"\n    text = template.render(**kwargs)\n    token_count = count_token(text)\n    if token_count > token_limit:\n        message = f\"token count {token_count} exceeds limit {token_limit}\"\n        log(message)\n        raise ValueError(message)\n    return text\n\n\nif __name__ == \"__main__\":\n    print(count_token(\"hello world, this is impressive\"))\n"}
{"type": "source_file", "path": "flows/chat_with_pdf/evaluate/evaluate.py", "content": "\"\"\"Evaluation flow for yaml_basic_flow.\"\"\"\nimport argparse\nfrom pprint import pprint\nfrom promptflow.evals.evaluate import evaluate\nfrom mlops.common.config_utils import MLOpsConfig\nfrom promptflow.core import AzureOpenAIModelConfiguration\nfrom promptflow.evals.evaluators import RelevanceEvaluator, CoherenceEvaluator\nfrom src.evaluators.match_evaluator import MatchEvaluator\nfrom flows.chat_with_pdf.evaluate.flow_wrapper import ChatWithPdfFlowWrapper\nfrom mlops.common.naming_tools import generate_experiment_name\n\n\ndef main():\n    \"\"\"Implement parameter reading and evaluation flow.\"\"\"\n    # Config parameters\n    parser = argparse.ArgumentParser(\"config_parameters\")\n    parser.add_argument(\n        \"--environment_name\",\n        type=str,\n        required=True,\n        help=\"env_name from config.yaml\",\n    )\n    args = parser.parse_args()\n\n    mlops_config = MLOpsConfig(environment=args.environment_name)\n    flow_config = mlops_config.get_flow_config(flow_name=\"chat_with_pdf\")\n\n    model_config = AzureOpenAIModelConfiguration(\n        azure_endpoint=mlops_config.aoai_config[\"aoai_api_base\"],\n        api_key=mlops_config.aoai_config[\"aoai_api_key\"],\n        azure_deployment=flow_config[\"CHAT_MODEL_DEPLOYMENT_NAME\"],\n    )\n\n    coherence_eval = CoherenceEvaluator(model_config=model_config)\n    relevance_eval = RelevanceEvaluator(model_config=model_config)\n    match_eval = MatchEvaluator()\n\n    data_eval_path = flow_config[\"eval_data_path\"]\n\n    aistudio_config = mlops_config.aistudio_config\n    openai_config = mlops_config.aoai_config\n\n    flow = ChatWithPdfFlowWrapper(flow_config, openai_config)\n\n    results = evaluate(\n        evaluation_name=generate_experiment_name(\"chat_with_pdf\"),\n        data=data_eval_path,\n        target=flow,\n        evaluators={\n            \"match_eval\": match_eval,\n            \"coherence_eval\": coherence_eval,\n            \"relevance_eval\": relevance_eval,\n        },\n        evaluator_config={\n            \"match_eval\": {\n                \"response\": \"${target.answer}\",\n                \"ground_truth\": \"${data.groundtruth}\",\n            },\n            \"coherence_eval\": {\n                \"answer\": \"${target.answer}\",\n                \"question\": \"${data.question}\",\n            },\n            \"relevance_eval\": {\n                \"answer\": \"${target.answer}\",\n                \"context\": \"${data.context}\",\n                \"question\": \"${data.question}\",\n            },\n        },\n        azure_ai_project={\n            \"subscription_id\": aistudio_config[\"subscription_id\"],\n            \"resource_group_name\": aistudio_config[\"resource_group_name\"],\n            \"project_name\": aistudio_config[\"project_name\"],\n        },\n    )\n\n    pprint(results)\n\n\nif __name__ == \"__main__\":\n    main()\n"}
{"type": "source_file", "path": "flows/chat_with_pdf/standard/find_context_tool.py", "content": "\"\"\"Promptflow tool for finding context in index.\"\"\"\nfrom promptflow import tool\nfrom chat_with_pdf.find_context import find_context\n\n\n@tool\ndef find_context_tool(question: str, index_path: str):\n    \"\"\"Find context in the provided index.\"\"\"\n    prompt, context = find_context(question, index_path)\n\n    return {\"prompt\": prompt, \"context\": [c.text for c in context]}\n"}
{"type": "source_file", "path": "flows/class_basic_flow/standard/extract_entities.py", "content": "\"\"\"Implement PromptFlow flow as a class.\"\"\"\nimport os\nimport pathlib\nfrom typing import List\nfrom promptflow.core import Prompty, AzureOpenAIModelConfiguration\nfrom promptflow.tracing import trace\n\n\nclass EntityExtraction:\n    \"\"\"Implement the flow.\"\"\"\n\n    def __init__(self, model_config: AzureOpenAIModelConfiguration):\n        \"\"\"Initialize environment and load prompty into the memory.\"\"\"\n        rootpath = pathlib.Path(__file__).parent.resolve()\n\n        self.model_config = model_config\n\n        self.prompty = Prompty.load(\n            source=os.path.join(rootpath, \"entity_template.prompty\"),\n            model={\"configuration\": self.model_config},\n        )\n\n    @trace\n    def __call__(self, *, entity_type: str, text: str, **kwargs):\n        \"\"\"Invoke the flow for a single request.\"\"\"\n        result = self.prompty(entity_type=entity_type, text=text)\n\n        output = self.cleansing(result)\n\n        return {\"answer\": output}\n\n    def cleansing(self, entities_str: str) -> List[str]:\n        \"\"\"\n        Return a list of cleaned entities. Split, remove leading and trailing spaces/tabs/dots.\n\n        Parameters:\n            entities_str (string): a string with comma separated entities\n\n        Returns:\n            list<str>: a list of cleaned entities\n        \"\"\"\n        parts = entities_str.split(\",\")\n        cleaned_parts = [part.strip(' \\t.\"') for part in parts]\n        entities = [part for part in cleaned_parts if len(part) > 0]\n        return entities\n"}
{"type": "source_file", "path": "flows/class_plan_and_execute/standard/multiprocressed_agents.py", "content": "\"\"\"\nMulti-processed agents for the class plan and execute flow.\n\nNecessary for pickling the agents for multiprocessing when using promptflow eval.\n\"\"\"\nfrom autogen import AssistantAgent, UserProxyAgent\nfrom autogen.oai.client import OpenAIWrapper\nfrom autogen.code_utils import content_str\nfrom typing import Dict\n\n\ndef _is_default_termination_msg(message: Dict) -> bool:\n    return content_str(message.get(\"content\")) == \"TERMINATE\"\n\n\nclass MultiProcessedAssistantAgent(AssistantAgent):\n    \"\"\"AssistantAgent that can be pickled for multiprocessing.\"\"\"\n\n    def __init__(self, *args, **kwargs):\n        \"\"\"Extract is_termination_msg if provided.\"\"\"\n        is_termination_msg = kwargs.pop(\"is_termination_msg\", None)\n        super().__init__(*args, **kwargs)\n        self._is_termination_msg = (\n            is_termination_msg\n            if is_termination_msg is not None\n            else _is_default_termination_msg\n        )\n\n    def __getstate__(self):\n        \"\"\"Create a state dictionary, excluding unpickleable objects.\"\"\"\n        state = self.__dict__.copy()\n        if \"client\" in state:\n            state[\n                \"client\"\n            ] = None  # Exclude the client (and its SSLContext) from being pickled\n        return state\n\n    def __setstate__(self, state):\n        \"\"\"Restore the state.\"\"\"\n        self.__dict__.update(state)\n        # Reinitialize the client or the SSLContext\n        if self.client is None:\n            # Use actual initialization parameters for OpenAIWrapper\n            self.client = OpenAIWrapper(**self.llm_config)\n\n\nclass MultiProcessedUserProxyAgent(UserProxyAgent):\n    \"\"\"UserProxyAgent that can be pickled for multiprocessing.\"\"\"\n\n    def __init__(self, *args, **kwargs):\n        \"\"\"Extract is_termination_msg if provided.\"\"\"\n        is_termination_msg = kwargs.pop(\"is_termination_msg\", None)\n        super().__init__(*args, **kwargs)\n        self._is_termination_msg = (\n            is_termination_msg\n            if is_termination_msg is not None\n            else _is_default_termination_msg\n        )\n\n    def __getstate__(self):\n        \"\"\"Create a state dictionary, excluding unpickleable objects.\"\"\"\n        state = self.__dict__.copy()\n        if \"client\" in state:\n            state[\n                \"client\"\n            ] = None  # Exclude the client (and its SSLContext) from being pickled\n        return state\n\n    def __setstate__(self, state):\n        \"\"\"Restore the state.\"\"\"\n        self.__dict__.update(state)\n        # Reinitialize the client or the SSLContext\n        if self.client is None:\n            # Use actual initialization parameters for OpenAIWrapper\n            self.client = OpenAIWrapper(**self.llm_config)\n"}
{"type": "source_file", "path": "flows/chat_with_pdf/standard/chat_with_pdf/build_index.py", "content": "\"\"\"Build index from pdfs.\"\"\"\nimport PyPDF2\nimport faiss\nimport os\n\nfrom pathlib import Path\n\nfrom utils.oai import OAIEmbedding\nfrom utils.index import FAISSIndex\nfrom utils.logging import log\nfrom utils.lock import acquire_lock\nfrom constants import INDEX_DIR\n\n\ndef create_faiss_index(pdf_path: str) -> str:\n    \"\"\"Create faiss index from pdfs.\"\"\"\n    chunk_size = int(os.environ.get(\"CHUNK_SIZE\"))\n    chunk_overlap = int(os.environ.get(\"CHUNK_OVERLAP\"))\n    log(f\"Chunk size: {chunk_size}, chunk overlap: {chunk_overlap}\")\n\n    file_name = Path(pdf_path).name + f\".index_{chunk_size}_{chunk_overlap}\"\n    index_persistent_path = Path(INDEX_DIR) / file_name\n    index_persistent_path = index_persistent_path.resolve().as_posix()\n    lock_path = index_persistent_path + \".lock\"\n    log(\"Index path: \" + os.path.abspath(index_persistent_path))\n\n    with acquire_lock(lock_path):\n        if os.path.exists(os.path.join(index_persistent_path, \"index.faiss\")):\n            log(\"Index already exists, bypassing index creation\")\n            return index_persistent_path\n        else:\n            if not os.path.exists(index_persistent_path):\n                os.makedirs(index_persistent_path)\n\n        log(\"Building index\")\n        pdf_reader = PyPDF2.PdfReader(pdf_path)\n\n        text = \"\"\n        for page in pdf_reader.pages:\n            text += page.extract_text()\n\n        # Chunk the words into segments of X words with Y-word overlap, X=CHUNK_SIZE, Y=OVERLAP_SIZE\n        segments = split_text(text, chunk_size, chunk_overlap)\n\n        log(f\"Number of segments: {len(segments)}\")\n\n        index = FAISSIndex(index=faiss.IndexFlatL2(1536), embedding=OAIEmbedding())\n        index.insert_batch(segments)\n\n        index.save(index_persistent_path)\n\n        log(\"Index built: \" + index_persistent_path)\n        return index_persistent_path\n\n\n# Split the text into chunks with CHUNK_SIZE and CHUNK_OVERLAP as character count\ndef split_text(text, chunk_size, chunk_overlap):\n    \"\"\"Split text in chunks.\"\"\"\n    # Calculate the number of chunks\n    num_chunks = (len(text) - chunk_overlap) // (chunk_size - chunk_overlap)\n\n    # Split the text into chunks\n    chunks = []\n    for i in range(num_chunks):\n        start = i * (chunk_size - chunk_overlap)\n        end = start + chunk_size\n        chunks.append(text[start:end])\n\n    # Add the last chunk\n    chunks.append(text[num_chunks * (chunk_size - chunk_overlap):])\n\n    return chunks\n"}
{"type": "source_file", "path": "flows/chat_with_pdf/standard/build_index_tool.py", "content": "\"\"\"Promptflow Tool to build index.\"\"\"\nfrom promptflow import tool\nfrom chat_with_pdf.build_index import create_faiss_index\n\n\n@tool\ndef build_index_tool(pdf_path: str) -> str:\n    \"\"\"Build faiss index.\"\"\"\n    return create_faiss_index(pdf_path)\n"}
{"type": "source_file", "path": "flows/function_basic_flow/standard/extract_entities.py", "content": "\"\"\"function_basic_flow implementation.\"\"\"\nimport pathlib\nimport sys\nimport os\nfrom promptflow.tracing import trace\nfrom promptflow.core import Prompty\n\nsys.path.append(str(pathlib.Path(__file__).parent))\nimport cleansing  # noqa: E402\n\n\n@trace\ndef extract_entity(entity_type: str, text: str):\n    \"\"\"Implement the flow as a function.\"\"\"\n    override_model = {\n        \"configuration\": {\n            \"azure_deployment\": \"${env:AZURE_OPENAI_DEPLOYMENT}\",\n            \"api_key\": \"${env:AZURE_OPENAI_API_KEY}\",\n            \"api_version\": \"${env:AZURE_OPENAI_API_VERSION}\",\n            \"azure_endpoint\": \"${env:AZURE_OPENAI_ENDPOINT}\",\n        }\n    }\n    rootpath = pathlib.Path(__file__).parent.resolve()\n\n    prompty = Prompty.load(\n        source=os.path.join(rootpath, \"entity_template.prompty\"),\n        model=override_model,\n    )\n\n    result = prompty(entity_type=entity_type, text=text)\n\n    output = cleansing.cleansing(result)\n\n    return {\"answer\": output}\n\n\nif __name__ == \"__main__\":\n    print(\n        extract_entity(\n            \"people's full name\",\n            \"The novel 'The Great Gatsby' was written by F. Scott Fitzgerald.\",\n        )\n    )\n"}
{"type": "source_file", "path": "flows/class_basic_flow/__init__.py", "content": ""}
{"type": "source_file", "path": "flows/chat_with_pdf/standard/chat_with_pdf/utils/lock.py", "content": "\"\"\"Lock resource.\"\"\"\nimport contextlib\nimport os\nimport sys\n\nif sys.platform.startswith(\"win\"):\n    import msvcrt\nelse:\n    import fcntl\n\n\n@contextlib.contextmanager\ndef acquire_lock(filename):\n    \"\"\"Lock resource.\"\"\"\n    if not sys.platform.startswith(\"win\"):\n        with open(filename, \"a+\") as f:\n            fcntl.flock(f, fcntl.LOCK_EX)\n            yield f\n            fcntl.flock(f, fcntl.LOCK_UN)\n    else:  # Windows\n        with open(filename, \"w\") as f:\n            msvcrt.locking(f.fileno(), msvcrt.LK_LOCK, 1)\n            yield f\n            msvcrt.locking(f.fileno(), msvcrt.LK_UNLCK, 1)\n\n    try:\n        os.remove(filename)\n    except OSError:\n        pass  # best effort to remove the lock file\n"}
{"type": "source_file", "path": "flows/plan_and_execute/standard/executor.py", "content": "\"\"\"Executor node of the plan_and_execute flow.\"\"\"\n\nimport concurrent.futures\nimport json\nfrom promptflow.core import tool\nfrom autogen import UserProxyAgent, AssistantAgent\nfrom connection_utils import CustomConnection, ConnectionInfo\nfrom tools import register_tools\n\n\ndef prepare_connection_info(connection):\n    \"\"\"Prepare the connection info for the agents.\"\"\"\n    return {\n        \"aoai_model_gpt35\": connection.configs[\"aoai_model_gpt35\"],\n        \"aoai_model_gpt4\": connection.configs[\"aoai_model_gpt4\"],\n        \"aoai_api_key\": connection.secrets[\"aoai_api_key\"],\n        \"aoai_base_url\": connection.configs[\"aoai_base_url\"],\n        \"aoai_api_version\": connection.configs[\"aoai_api_version\"],\n        \"bing_api_key\": connection.secrets[\"bing_api_key\"],\n        \"bing_endpoint\": connection.configs[\"bing_endpoint\"],\n    }\n\n\ndef prepare_executor(connection_info):\n    \"\"\"Prepare the executor agent.\"\"\"\n    config_list_gpt35 = [\n        {\n            \"model\": connection_info[\"aoai_model_gpt35\"],\n            \"api_key\": connection_info[\"aoai_api_key\"],\n            \"base_url\": connection_info[\"aoai_base_url\"],\n            \"api_type\": \"azure\",\n            \"api_version\": connection_info[\"aoai_api_version\"],\n        }\n    ]\n    executor = UserProxyAgent(\n        name=\"EXECUTOR\",\n        description=(\n            \"An agent that acts as a proxy for the user and executes the \"\n            \"suggested function calls from PLANNER.\"\n        ),\n        code_execution_config=False,\n        llm_config={\n            \"config_list\": config_list_gpt35,\n            \"timeout\": 60,\n            \"cache_seed\": None,\n        },\n        human_input_mode=\"NEVER\",\n    )\n    return executor, config_list_gpt35\n\n\ndef llm_tool(request, context, config_list_gpt35):\n    \"\"\"Define the LLM agent.\"\"\"\n    llm_assistant = AssistantAgent(\n        name=\"LLM_ASSISTANT\",\n        description=(\n            \"An agent expert in answering requests by analyzing and \"\n            \"extracting information from the given context.\"\n        ),\n        system_message=(\n            \"Given a request and optionally some context with potentially \"\n            \"relevant information to answer it, analyze the context and \"\n            \"extract the information needed to answer the request. Then, \"\n            \"create a sentence that answers the request. You must strictly \"\n            \"limit your response to only what was asked in the request.\"\n        ),\n        code_execution_config=False,\n        llm_config={\n            \"config_list\": config_list_gpt35,\n            \"timeout\": 60,\n            \"temperature\": 0.3,\n            \"cache_seed\": None,\n        },\n    )\n\n    llm_assistant.clear_history()\n\n    message = f\"\"\"\n    Request:\n    {request}\n\n    Context:\n    {context}\n    \"\"\"\n    try:\n        reply = llm_assistant.generate_reply(\n            messages=[{\"content\": message, \"role\": \"user\"}]\n        )\n        return reply\n    except Exception as e:\n        return f\"Error: {str(e)}\"\n\n\ndef substitute_dependency(\n    id, original_argument_value, dependency_value, config_list_gpt35\n):\n    \"\"\"Substitute dependencies in the execution plan.\"\"\"\n    instruction = (\n        \"Extract the entity name or fact from the dependency value in a way \"\n        \"that makes sense to use it to substitute the variable #E in the \"\n        \"original argument value. Do not include any other text in your \"\n        \"response, other than the entity name or fact extracted.\"\n    )\n\n    context = f\"\"\"\n    original argument value:\n    {original_argument_value}\n\n    dependency value:\n    {dependency_value}\n\n    extracted fact or entity:\n\n    \"\"\"\n\n    return llm_tool(instruction, context, config_list_gpt35)\n\n\ndef has_unresolved_dependencies(item, resolved_ids, plan_ids):\n    \"\"\"Check for unresolved dependencies in a plan step.\"\"\"\n    try:\n        args = json.loads(item[\"function\"][\"arguments\"])\n    except json.JSONDecodeError:\n        return False\n\n    for arg in args.values():\n        if isinstance(arg, str) and any(\n            ref_id\n            for ref_id in plan_ids\n            if ref_id not in resolved_ids and ref_id in arg\n        ):\n            return True\n    return False\n\n\ndef submit_task(item_id, item, thread_executor, executor_agent, futures):\n    \"\"\"Submit a task for execution.\"\"\"\n    arguments = item[\"function\"][\"arguments\"]\n    future = thread_executor.submit(\n        executor_agent.execute_function,\n        {\"name\": item[\"function\"][\"name\"], \"arguments\": arguments},\n    )\n    futures[item_id] = future\n\n\ndef process_done_future(\n    future,\n    futures,\n    results,\n    resolved_ids,\n    plan_ids,\n    thread_executor,\n    executor_agent,\n    config_list_gpt35,\n):\n    \"\"\"Process a completed future and trigger the submission of ready tasks.\"\"\"\n    item_id = next((id for id, f in futures.items() if f == future), None)\n    if item_id:\n        _, result = future.result()\n        results[item_id] = result\n        resolved_ids.add(item_id)\n        del futures[item_id]\n        submit_ready_tasks(\n            plan_ids,\n            resolved_ids,\n            futures,\n            results,\n            thread_executor,\n            executor_agent,\n            config_list_gpt35,\n        )\n\n\ndef submit_ready_tasks(\n    plan_ids,\n    resolved_ids,\n    futures,\n    results,\n    thread_executor,\n    executor_agent,\n    config_list_gpt35,\n):\n    \"\"\"Submit plan tasks that have all dependencies resolved and are ready to be executed.\"\"\"\n    for next_item_id, next_item in plan_ids.items():\n        if (\n            next_item_id not in resolved_ids\n            and next_item_id not in futures\n            and not has_unresolved_dependencies(next_item, resolved_ids, plan_ids)\n        ):\n            update_and_submit_task(\n                next_item_id,\n                next_item,\n                thread_executor,\n                executor_agent,\n                futures,\n                results,\n                config_list_gpt35,\n            )\n\n\ndef update_and_submit_task(\n    item_id, item, thread_executor, executor_agent, futures, results, config_list_gpt35\n):\n    \"\"\"Update the arguments of a task with dependency results and submit it for execution.\"\"\"\n    updated_arguments = json.loads(item[\"function\"][\"arguments\"])\n    for arg_key, arg_value in updated_arguments.items():\n        if isinstance(arg_value, str):\n            for res_id, res in results.items():\n                if arg_key == \"context\":\n                    arg_value = arg_value.replace(res_id, res[\"content\"])\n                else:\n                    arg_value = arg_value.replace(\n                        res_id,\n                        substitute_dependency(\n                            res_id, arg_value, res[\"content\"], config_list_gpt35\n                        ),\n                    )\n                updated_arguments[arg_key] = arg_value\n    future = thread_executor.submit(\n        executor_agent.execute_function,\n        {\"name\": item[\"function\"][\"name\"], \"arguments\": json.dumps(updated_arguments)},\n    )\n    futures[item_id] = future\n\n\ndef execute_plan_parallel(plan, executor_agent, config_list_gpt35):\n    \"\"\"Execute the plan in parallel.\"\"\"\n    plan_ids = {item[\"id\"]: item for item in plan}\n    results = {}\n    resolved_ids = set()\n    futures = {}\n\n    with concurrent.futures.ThreadPoolExecutor() as thread_executor:\n        for item_id, item in plan_ids.items():\n            if not has_unresolved_dependencies(item, resolved_ids, plan_ids):\n                submit_task(item_id, item, thread_executor, executor_agent, futures)\n\n        while futures:\n            done, _ = concurrent.futures.wait(\n                futures.values(), return_when=concurrent.futures.FIRST_COMPLETED\n            )\n            for future in done:\n                process_done_future(\n                    future,\n                    futures,\n                    results,\n                    resolved_ids,\n                    plan_ids,\n                    thread_executor,\n                    executor_agent,\n                    config_list_gpt35,\n                )\n\n    result_str = \"\\n\".join(\n        [f\"{key} = {value['content']}\" for key, value in results.items()]\n    )\n    return result_str\n\n\n@tool\ndef worker_tool(connection: CustomConnection, plan: str) -> str:\n    \"\"\"Execute the plan generated by the planner node.\"\"\"\n    connection_info = prepare_connection_info(connection)\n    ConnectionInfo().connection_info = connection_info\n\n    executor, config_list_gpt35 = prepare_executor(connection_info)\n    register_tools(executor)\n\n    plan = json.loads(plan)\n    executor_reply = execute_plan_parallel(\n        plan[\"Functions\"], executor, config_list_gpt35\n    )\n    number_of_steps = len(plan[\"Plan\"])\n\n    return {\"executor_reply\": executor_reply, \"number_of_steps\": number_of_steps}\n"}
{"type": "source_file", "path": "flows/chat_with_pdf/standard/chat_with_pdf/utils/logging.py", "content": "\"\"\"Log anything.\"\"\"\nimport os\n\n\ndef log(message: str):\n    \"\"\"Log message.\"\"\"\n    verbose = os.environ.get(\"VERBOSE\", \"false\")\n    if verbose.lower() == \"true\":\n        print(message, flush=True)\n"}
{"type": "source_file", "path": "flows/yaml_basic_flow/evaluate/flow_wrapper.py", "content": "\"\"\"Implement a wrapper for yaml based flow since it's not supported by evaluate explicitly.\"\"\"\nfrom promptflow.client import load_flow\nfrom promptflow.entities import FlowContext\nfrom promptflow.entities import AzureOpenAIConnection\nfrom promptflow.client import PFClient\n\n\nclass StandardFlowWrapper:\n    \"\"\"Implement the flow.\"\"\"\n\n    def __init__(\n        self,\n        flow_standard_path: str,\n        connection_name: str,\n        aoai_deployment: str,\n        aoai_config: dict,\n    ):\n        \"\"\"Initialize environment and load prompty into the memory.\"\"\"\n        connection = AzureOpenAIConnection(\n            name=connection_name,\n            api_key=aoai_config[\"aoai_api_key\"],\n            api_base=aoai_config[\"aoai_api_base\"],\n            api_type=\"azure\",\n            api_version=aoai_config[\"aoai_api_version\"],\n        )\n\n        pf = PFClient()\n        pf.connections.create_or_update(connection)\n\n        self.flow = load_flow(flow_standard_path)\n        self.flow.context = FlowContext(\n            overrides={\"nodes.NER_LLM.inputs.deployment_name\": aoai_deployment},\n            connections={\"NER_LLM\": {\"connection\": connection}},\n        )\n\n    def __call__(self, *, entity_type: str, text: str, **kwargs):\n        \"\"\"Invoke the flow for a single request.\"\"\"\n        return self.flow(entity_type=entity_type, text=text)\n"}
{"type": "source_file", "path": "flows/function_basic_flow/standard/cleansing.py", "content": "\"\"\"This is a Prompt flow tool that we are using in standard flow.\"\"\"\nfrom typing import List\nfrom promptflow.core import tool\n\n\n@tool\ndef cleansing(entities_str: str) -> List[str]:\n    \"\"\"\n    Return a list of cleaned entities. Split, remove leading and trailing spaces/tabs/dots.\n\n    Parameters:\n      entities_str (string): a string with comma separated entities\n\n    Returns:\n        list<str>: a list of cleaned entities\n    \"\"\"\n    parts = entities_str.split(\",\")\n    cleaned_parts = [part.strip(' \\t.\"') for part in parts]\n    entities = [part for part in cleaned_parts if len(part) > 0]\n    return entities\n"}
{"type": "source_file", "path": "mlops/__init__.py", "content": ""}
{"type": "source_file", "path": "flows/yaml_basic_flow/standard/cleansing.py", "content": "\"\"\"This is a Prompt flow tool that we are using in standard flow.\"\"\"\nfrom typing import List\nfrom promptflow.core import tool\n\n\n@tool\ndef cleansing(entities_str: str) -> List[str]:\n    \"\"\"\n    Return a list of cleaned entities. Split, remove leading and trailing spaces/tabs/dots.\n\n    Parameters:\n      entities_str (string): a string with comma separated entities\n\n    Returns:\n        list<str>: a list of cleaned entities\n    \"\"\"\n    parts = entities_str.split(\",\")\n    cleaned_parts = [part.strip(' \\t.\"') for part in parts]\n    entities = [part for part in cleaned_parts if len(part) > 0]\n    return entities\n"}
