{"repo_info": {"repo_name": "coin-for-rich", "repo_owner": "coinForRich", "repo_url": "https://github.com/coinForRich/coin-for-rich"}}
{"type": "test_file", "path": "tests/__init__.py", "content": ""}
{"type": "test_file", "path": "tests/test_helpers/test_datetimehelpers.py", "content": "import pytest\nimport datetime\nimport time\nimport redis\nfrom common.config.constants import (\n    DEFAULT_DATETIME_STR_QUERY,\n    REDIS_HOST, REDIS_USER, REDIS_PASSWORD\n)\nfrom common.helpers.datetimehelpers import *\n\n\n# Fixtures\n@pytest.fixture\ndef some_date():\n    return datetime.datetime(2021, 1, 1, 0, 0, 0)\n\n@pytest.fixture\ndef some_second():\n    return 1609459200\n\n@pytest.fixture\ndef some_millisecond():\n    return 1609459200000\n\n@pytest.fixture\ndef some_str():\n    return \"2021-01-01T00:00:00\"\n\n@pytest.fixture\ndef some_Redis():\n    return redis.Redis(\n        host=REDIS_HOST,\n        username=REDIS_USER,\n        password=REDIS_PASSWORD,\n        decode_responses=True)\n\n# Tests\n@pytest.mark.beforepop\ndef test_milliseconds():\n    assert milliseconds(10.0) == 10000\n\n@pytest.mark.beforepop\ndef test_seconds():\n    assert seconds(10000.0) == 10\n\n@pytest.mark.beforepop\ndef test_microseconds_to_seconds():\n    assert microseconds_to_seconds(1000000.0) == 1\n\n@pytest.mark.beforepop\ndef test_datetime_to_seconds(some_date, some_second):\n    assert datetime_to_seconds(some_date) == float(some_second)\n\n@pytest.mark.beforepop\ndef test_datetime_to_milliseconds(some_date, some_millisecond):\n    assert datetime_to_milliseconds(some_date) == float(some_millisecond)\n\n@pytest.mark.beforepop\ndef test_milliseconds_to_datetime(some_millisecond, some_date):\n    assert milliseconds_to_datetime(some_millisecond) == some_date\n\n@pytest.mark.beforepop\ndef test_str_to_datetime(some_str, some_date):\n    assert str_to_datetime(\n        some_str, DEFAULT_DATETIME_STR_QUERY) == some_date\n\n@pytest.mark.beforepop\ndef test_datetime_to_str(some_date, some_str):\n    assert datetime_to_str(\n        some_date, DEFAULT_DATETIME_STR_QUERY) == some_str\n\n@pytest.mark.beforepop\ndef test_milliseconds_to_str(some_millisecond, some_str):\n    assert milliseconds_to_str(\n        some_millisecond, DEFAULT_DATETIME_STR_QUERY) == some_str\n\n@pytest.mark.beforepop\ndef test_str_to_milliseconds(some_str, some_millisecond):\n    assert str_to_milliseconds(\n        some_str, DEFAULT_DATETIME_STR_QUERY) == some_millisecond\n\n@pytest.mark.beforepop\ndef test_str_to_seconds(some_str, some_second):\n    assert str_to_seconds(\n        some_str, DEFAULT_DATETIME_STR_QUERY) == some_second\n\n@pytest.mark.beforepop\ndef test_list_days_fromto():\n    test_days = [\n        datetime.datetime(2021, 1, 1),\n        datetime.datetime(2021, 1, 2),\n        datetime.datetime(2021, 1, 3),\n        datetime.datetime(2021, 1, 4),\n        datetime.datetime(2021, 1, 5),\n    ]\n    \n    for i, d in enumerate(list_days_fromto(\n        datetime.datetime(2021, 1, 1),\n        datetime.datetime(2021, 1, 5)\n    )):\n        assert d == test_days[i]\n\n@pytest.mark.beforepop\ndef test_redis_time(some_Redis):\n    '''\n    See if `redis_time` is correct within 1% variance\n    '''\n    \n    start = redis_time(some_Redis)\n    time.sleep(0.25)\n    end = redis_time(some_Redis)\n    assert (end - start >= 0.2525 or end - start >= 0.2475)\n"}
{"type": "test_file", "path": "tests/test_web/test_api/test_ws.py", "content": "from fastapi.testclient import TestClient\nfrom web.main import app\n\n\nclient = TestClient(app)\n\ndef test_websocket():\n    with client.websocket_connect(\"/api/test\") as websocket:\n        data = websocket.receive_json()\n        assert data == {\"detail\": \"Hello WebSocket\"}\n"}
{"type": "test_file", "path": "tests/test_helpers/test_dbhelpers.py", "content": "import pytest\nimport logging\nimport psycopg2\nfrom psycopg2 import sql, extras\nfrom typing import Any\nfrom common.config.constants import DBCONNECTION\nfrom common.helpers.numbers import round_decimal\nfrom fetchers.config.queries import PSQL_INSERT_UPDATE_QUERY, PSQL_INSERT_IGNOREDUP_QUERY\nfrom fetchers.helpers.dbhelpers import psql_bulk_insert\n\n\n@pytest.mark.beforepop\ndef test_dbhelper():\n    def run_query(cur: Any, query: str):\n        try:\n            cur.execute(query)\n            results = cur.fetchall()\n        except Exception as exc:\n            logging.warning(exc)\n            raise exc\n        return results\n\n    conn = psycopg2.connect(DBCONNECTION)\n    cur = conn.cursor()\n    table = \"test\"\n    rows = (\n        (1, 'a', 'b', round_decimal(351235), round_decimal(3.14)),\n        (2, 'b', 'c', round_decimal(1001), round_decimal(9001)),\n        (5, 'a', 'q', round_decimal(54), round_decimal(97))\n    )\n    unique_cols = (\"id\", \"b\", \"q\")\n    update_cols = (\"c\", \"o\") # Order does not matter\n\n    query = \"select * from test where id=1;\"\n    \n    # Insert update\n    assert psql_bulk_insert(\n        conn,\n        rows,\n        table,\n        insert_update_query = PSQL_INSERT_UPDATE_QUERY,\n        unique_cols = unique_cols,\n        update_cols = update_cols,\n    ) is True\n\n    # Query from\n    results = run_query(cur, query)\n    assert results[0] == rows[0]\n    \n    # Insert ignore dups\n    assert psql_bulk_insert(\n        conn,\n        rows,\n        table,\n        insert_ignoredup_query = PSQL_INSERT_IGNOREDUP_QUERY\n    ) is True\n\n    # Query from\n    results = run_query(cur, query)\n    assert results[0] == rows[0]\n"}
{"type": "test_file", "path": "tests/test_helpers/test_numbers.py", "content": "import pytest\nfrom decimal import Decimal\nfrom common.helpers.numbers import round_decimal\n\n\n@pytest.mark.beforepop\ndef test_round_decimal():\n    assert round_decimal(Decimal(16.0/7)) == round_decimal(2.29)\n    assert round_decimal(0) == Decimal(0)\n    assert round_decimal(None) is None\n    assert round_decimal(\"1.5\") == 1.5\n"}
{"type": "test_file", "path": "tests/test_web/test_api/test_getters.py", "content": "import pytest\nfrom fastapi.testclient import TestClient\nfrom web.main import app\n\n\nclient = TestClient(app)\n\n@pytest.mark.afterpop\ndef test_get():\n    '''\n    Tests getting from Test table\n    '''\n    \n    response = client.get(\"/api/test\")\n    assert response.status_code == 200\n    \n    resp_j = response.json()[0]\n    assert (resp_j['id'] == 1) and (resp_j['b'] == \"a\") and (resp_j['q'] == \"b\")\n\n@pytest.mark.afterpop\ndef test_analytics():\n    '''\n    Tests getting from analytics APIs\n    '''\n\n    # Geometric daily return\n    response = client.get(\"/api/analytics/geodr\")\n    assert response.status_code == 200\n\n    response = client.get(\"/api/analytics/geodr?cutoff_upper_pct=20\")\n    assert response.status_code == 200\n\n    response = client.get(\"/api/analytics/geodr?cutoff_lower_pct=10\")\n    assert response.status_code == 200\n\n    response = client.get(\n        \"/api/analytics/geodr?cutoff_upper_pct=20&cutoff_lower_pct=10&limit=10\")\n    assert response.status_code == 200\n\n    # Weekly return\n    response = client.get(\"/api/analytics/wr\")\n    assert response.status_code == 200\n\n    response = client.get(\"/api/analytics/wr?cutoff_upper_pct=20\")\n    assert response.status_code == 200\n\n    response = client.get(\"/api/analytics/wr?cutoff_lower_pct=10\")\n    assert response.status_code == 200\n\n    response = client.get(\n        \"/api/analytics/wr?cutoff_upper_pct=20&cutoff_lower_pct=10&limit=10\")\n    assert response.status_code == 200\n\n    # Top 20 Volume\n    response = client.get(\"/api/analytics/top20qvlm\")\n    assert response.status_code == 200\n\n@pytest.mark.afterpop\ndef test_other_api():\n    '''\n    Tests getting from other API endpoints\n    '''\n\n    response = client.get(\"/api/symbol-exchange\")\n    assert response.status_code == 200\n"}
{"type": "test_file", "path": "tests/test_web/test_api/__init__.py", "content": ""}
{"type": "test_file", "path": "tests/test_helpers/__init__.py", "content": ""}
{"type": "source_file", "path": "celery_app/celery_main.py", "content": "# This module contains the main Celery app\n\nfrom celery import Celery\n\n\n# Celery main app\napp = Celery('Celery Coin App')\napp.config_from_object('celery_app.celery_config')\n\n# Periodic OHLCV update\n# TODO: remove this\n# app.conf.beat_schedule = {\n#     'bitfinex_ohlcv_1min': {\n#         'task': \"celery_app.celery_tasks.bitfinex_fetch_ohlcvs_mutual_basequote_1min\",\n#         'schedule': 100.0\n#     },\n#     'binance_ohlcv_1min': {\n#         'task': \"celery_app.celery_tasks.binance_fetch_ohlcvs_mutual_basequote_1min\",\n#         'schedule': 60.0\n#     },\n#     'bittrex_ohlcv_1min': {\n#         'task': \"celery_app.celery_tasks.bittrex_fetch_ohlcvs_mutual_basequote_1min\",\n#         'schedule': 120.0\n#     }\n# }\n"}
{"type": "source_file", "path": "scripts/fetchers/rest.py", "content": "# This represents commands to start REST fetching\n#   from CLI using Celery tasks\n\nimport argparse\nfrom celery_app.celery_tasks import *\n\n\n# Create the parser\narg_parser = argparse.ArgumentParser(\n    prog=\"python -m scripts.fetchers.rest\",\n    description=\"Starts a REST fetcher for an exchange using Celery\"\n)\n\n# Add the arguments\narg_parser.add_argument(\n    'action',\n    metavar='action',\n    type=str,\n    choices=[\"fetch\", \"resume\"],\n    help='fetch or resume'\n)\n\narg_parser.add_argument(\n    '--exchange',\n    metavar='exchange',\n    type=str,\n    required=True,\n    help='name of the exchange'\n)\n\narg_parser.add_argument(\n    '--start',\n    metavar='start',\n    type=str,\n    help='Start date; Must comply to this format: %%Y-%%m-%%dT%%H:%%M:%%S; \\nMust be entered if action is fetch',\n    # required=True,\n)\n\narg_parser.add_argument(\n    '--end',\n    metavar='end',\n    type=str,\n    help='End date; Must comply to this format: %%Y-%%m-%%dT%%H:%%M:%%S; \\nMust be entered if action is fetch',\n    # required=True,\n)\n\n# Execute the parse_args() method\nargs = arg_parser.parse_args()\naction = args.action\nexchange = args.exchange\nstart = args.start\nend = args.end\nif action == \"fetch\":\n    if exchange == \"bitfinex\":\n        bitfinex_fetch_ohlcvs_all_symbols.delay(start, end)\n    elif exchange == \"binance\":\n        binance_fetch_ohlcvs_all_symbols.delay(start, end)\n    elif exchange == \"bittrex\":\n        bittrex_fetch_ohlcvs_all_symbols.delay(start, end)\nelif action == \"resume\":\n    if exchange == \"bitfinex\":\n        bitfinex_resume_fetch.delay()\n    elif exchange == \"binance\":\n        binance_resume_fetch.delay()\n    elif exchange == \"bittrex\":\n        bittrex_resume_fetch.delay()\n"}
{"type": "source_file", "path": "common/config/constants.py", "content": "# This module contains constants for all apps\n\nimport os\nfrom dotenv import dotenv_values \n\n# Load env vars\nconfigs = dotenv_values(\".env\")\n\n# Common host (for fallback)\nCOMMON_HOST = configs.get('COMMON_HOST')\n\n# Postgres\nPOSTGRES_HOST = os.getenv('POSTGRES_HOST') or COMMON_HOST\nPOSTGRES_PASSWORD = os.getenv('POSTGRES_PASSWORD') or configs.get('POSTGRES_PASSWORD')\nPOSTGRES_USER = \"postgres\"\nPOSTGRES_DB = \"postgres\"\nPOSTGRES_PORT = os.getenv('POSTGRES_PORT') or configs.get('POSTGRES_PORT')\nDBCONNECTION = f\"dbname={POSTGRES_DB} user={POSTGRES_USER} password={POSTGRES_PASSWORD} host={POSTGRES_HOST} port={POSTGRES_PORT}\"\nOHLCVS_TABLE = \"ohlcvs\"\nOHLCVS_ERRORS_TABLE = \"ohlcvs_errors\"\nSYMBOL_EXCHANGE_TABLE = \"symbol_exchange\"\n\n# Redis common vars\nREDIS_HOST = os.getenv('REDIS_HOST') or COMMON_HOST\nREDIS_PASSWORD = os.getenv('REDIS_PASSWORD') or configs.get('REDIS_PASSWORD')\nREDIS_USER = \"default\"\nREDIS_PORT = os.getenv('REDIS_PORT')\nREDIS_DELIMITER = \";;\"\n\n# Celery\nCELERY_REDIS_URL = f\"redis://{REDIS_USER}:{REDIS_PASSWORD}@{REDIS_HOST}:{REDIS_PORT}/0\"\n\n# Default datetime string format when dealing with PSQL\nDEFAULT_DATETIME_STR_QUERY = \"%Y-%m-%dT%H:%M:%S\"\nDEFAULT_DATETIME_STR_RESULT = \"%Y-%m-%dT%H:%M:%S%z\"\n"}
{"type": "source_file", "path": "fetchers/utils/asyncioutils.py", "content": "# Utils for asyncio including backoffs used in fetchers\n\nfrom typing import Any\nfrom fetchers.config.constants import THROTTLER_RATE_LIMITS\n\n\n# Backoff event handlers for httpx\ndef onbackoff(details: Any):\n    '''\n    handler for backoff - on backoff event\n    '''\n\n    # Minimum throttler rate limit is 1\n    throttler = details['kwargs']['throttler']\n    throttler.period *= 2\n\n    # print(f\"Reducing throttler rate limit to: 1 request per {round(throttler.period, 2)} seconds\")\n\ndef onsuccessgiveup(details: Any):\n    '''\n    handler for backoff - on success or giveup event\n    '''\n\n    throttler = details['kwargs']['throttler']\n    exchange_name = details['kwargs']['exchange_name']\n    throttler.period = \\\n        THROTTLER_RATE_LIMITS['RATE_LIMIT_SECS_PER_MIN'] / \\\n        THROTTLER_RATE_LIMITS['RATE_LIMIT_HITS_PER_MIN'][exchange_name]\n    # print(f\"Setting throttler rate limit to: 1 request per {round(throttler.period, 2)} seconds\")\n"}
{"type": "source_file", "path": "common/helpers/datetimehelpers.py", "content": "# This module contains common datetime helpers\n\nimport datetime\nfrom typing import Generator, Union, Any\nfrom redis import Redis\n\n\ndef milliseconds(seconds: Union[float, int]) -> int:\n    '''\n    returns milliseconds from seconds\n\n    :params:\n        `seconds`: float or int of seconds\n    '''\n    return int(seconds * 1000)\n\ndef seconds(mls: Union[float, int]) -> int:\n    '''\n    returns seconds from milliseconds\n\n    :params:\n        `mls`: float or int of milliseconds\n    '''\n    return int(mls / 1000)\n\ndef microseconds_to_seconds(mic: Union[float, int]) -> float:\n    '''\n    returns seconds from `mic` microseconds\n    \n    :params:\n        `mic`: float or int of microseconds\n    '''\n    return mic / 1000000\n\ndef datetime_to_seconds(dt: datetime.datetime) -> float:\n    '''\n    converts a datetime.datetime object to seconds, represented in float\n    \n    :params:\n        `dt`: datetime object\n    '''\n    return dt.timestamp()\n\ndef datetime_to_milliseconds(dt: datetime.datetime) -> int:\n    '''\n    converts a datetime.datetime object to milliseconds, represented in int\n    \n    :params:\n        `dt`: datetime object\n    '''\n    return milliseconds(datetime_to_seconds(dt))\n\ndef milliseconds_to_datetime(mls: Union[float, int]) -> datetime.datetime:\n    '''\n    converts a millisecond timestamp into datetime object\n    \n    :params:\n        `mls`: float or int (milliseconds)\n    '''\n    return datetime.datetime.fromtimestamp(mls/1000)\n\ndef str_to_datetime(s: str, f: str) -> datetime.datetime:\n    '''\n    converts a string of format `f` to datetime obj\n    \n    :params:\n        `s`: string - representing datetime\n        `f`: string - time format\n    '''\n    return datetime.datetime.strptime(s, f)\n\ndef datetime_to_str(dt: datetime.datetime, f: str) -> str:\n    '''\n    converts a datetime object into a string of format `f`\n    \n    :params:\n        `dt`: datetime obj\n        `f`: string - time format\n    '''\n    return dt.strftime(f)\n\ndef milliseconds_to_str(mls: int, f: str) -> str:\n    '''\n    converts a millisecond timestamp into string of format `f`\n    \n    :params:\n        `mls`: int (milliseconds)\n        `f`: string - time format\n    '''\n\n    return datetime_to_str(milliseconds_to_datetime(mls), f)\n\ndef str_to_milliseconds(s: str, f: str) -> int:\n    '''\n    converts a string of format `f` to milliseconds\n    \n    :params:\n        `s`: datetime string\n        `f`: string - time format\n    '''\n\n    return datetime_to_milliseconds(str_to_datetime(s, f))\n\ndef str_to_seconds(s: str, f: str) -> int:\n    '''\n    converts a string of format `f` to seconds\n    \n    :params:\n        `s`: datetime string\n        `f`: string - time format\n    '''\n\n    return seconds(str_to_milliseconds(s, f))\n\ndef list_days_fromto(\n        start_date: datetime.datetime,\n        end_date: datetime.datetime\n    ) -> Generator[datetime.datetime, Any, Any]:\n    '''\n    generates the days between two days (inclusive)\n    \n    :params:\n        `start_date`: datetime obj\n        `end_date`: datetime obj\n    '''\n\n    for n in range((end_date - start_date).days + 1):\n        yield start_date + datetime.timedelta(days=n)\n\ndef redis_time(r: Redis) -> float:\n    '''\n    generates the time in the Redis server - in seconds,\n        including fractions of a second\n    \n    :params:\n        `r`: Redis client object\n    '''\n    \n    secs, mics = r.time()\n    return float(secs) + microseconds_to_seconds(float(mics))\n"}
{"type": "source_file", "path": "fetchers/rest/base.py", "content": "# Base for all REST fetchers\n\nimport asyncio\nimport datetime\nfrom asyncio.events import AbstractEventLoop\n\nimport httpx\nimport psycopg2\nimport redis\n\nfrom common.config.constants import \\\n    DBCONNECTION, REDIS_HOST, REDIS_PASSWORD, \\\n    REDIS_USER, SYMBOL_EXCHANGE_TABLE\nfrom common.utils.asyncioutils import aio_set_exception_handler\nfrom common.utils.logutils import create_logger\nfrom fetchers.config.constants import \\\n    HTTPX_DEFAULT_TIMEOUT, HTTPX_MAX_CONCURRENT_CONNECTIONS, \\\n    OHLCVS_FETCHING_REDIS_KEY, OHLCVS_TOFETCH_REDIS_KEY, \\\n    SYMEXCH_UNIQUE_COLUMNS, SYMEXCH_UPDATE_COLUMNS\nfrom fetchers.config.queries import \\\n    MUTUAL_BASE_QUOTE_QUERY, PSQL_INSERT_UPDATE_QUERY\nfrom fetchers.helpers.dbhelpers import psql_bulk_insert\n\n\nclass BaseOHLCVFetcher:\n    '''Base REST fetcher for all exchanges\n    '''\n\n    def __init__(self, exchange_name: str):\n        # Name, Redis to-fetch and fetching set keys\n        self.exchange_name = exchange_name\n        self.tofetch_key = OHLCVS_TOFETCH_REDIS_KEY.format(exchange=exchange_name)\n        self.fetching_key = OHLCVS_FETCHING_REDIS_KEY.format(exchange=exchange_name)\n\n        # Postgres connection\n        self.psql_conn = psycopg2.connect(DBCONNECTION)\n        self.psql_cur = self.psql_conn.cursor()\n\n        # Redis client\n        self.redis_client = redis.Redis(\n            host=REDIS_HOST,\n            username=REDIS_USER,\n            password=REDIS_PASSWORD,\n            decode_responses=True\n        )\n\n        # HTTPX limits\n        self.httpx_limits = httpx.Limits(\n            max_connections=HTTPX_MAX_CONCURRENT_CONNECTIONS[exchange_name]\n        )\n        self.httpx_timout = httpx.Timeout(HTTPX_DEFAULT_TIMEOUT)\n\n        # Redis initial feeding status\n        self.feeding = False\n\n        # Log\n        self.logger = create_logger(exchange_name)\n\n        # Symbol data\n        self.symbol_data = {}\n\n\n    def _setup_event_loop(self) -> AbstractEventLoop:\n        '''\n        Gets the event loop or resets it\n        '''\n\n        loop = asyncio.get_event_loop()\n        if loop.is_closed():\n            asyncio.set_event_loop(asyncio.new_event_loop())\n            loop = asyncio.get_event_loop()\n        aio_set_exception_handler(loop)\n        return loop\n\n    async def _fetch_ohlcvs_symbols(*args, **kwargs) -> None:\n        '''\n        Signature for _fetch_ohlcvs_symbols in child class\n        '''\n\n    async def _consume_ohlcvs_redis(self, *args, **kwargs) -> None:\n        '''\n        Signature for _consume_ohlcvs_redis in child class\n        '''\n\n    async def _resume_fetch(self, update: bool=False) -> None:\n        '''\n        Resumes fetching tasks if there're params inside Redis sets\n        '''\n\n        # Asyncio gather 1 task:\n        # - Consume from Redis to-fetch\n        await asyncio.gather(\n            self._consume_ohlcvs_redis(update)\n        )\n\n    def close_connections(self) -> None:\n        '''\n        Interface to close all connections (e.g., PSQL)\n        '''\n\n        self.psql_conn.close()\n\n    def fetch_symbol_data(self) -> None:\n        '''\n        Interface to fetch symbol data (exchange, base, quote)\n            from self.symbol_data into PSQL db\n\n        Updates is_trading status in PSQL db for existing ones\n        '''\n\n        rows = [\n            (\n                self.exchange_name,\n                bq['base_id'],\n                bq['quote_id'],\n                symbol,\n                True\n            ) for symbol, bq in self.symbol_data.items()\n        ]\n        psql_bulk_insert(\n            self.psql_conn,\n            rows,\n            SYMBOL_EXCHANGE_TABLE,\n            insert_update_query = PSQL_INSERT_UPDATE_QUERY,\n            unique_cols = SYMEXCH_UNIQUE_COLUMNS,\n            update_cols = SYMEXCH_UPDATE_COLUMNS\n        )\n\n    def get_symbols_from_exch(self, query: str) -> dict:\n        '''\n        Interface to return a dict of symbols from a pre-constructed query\n            in this form:\n                {\n                    'ETHBTC': {\n                        'base_id': 'ETH',\n                        'quote_id': 'BTC'\n                    }\n                }\n\n        The query must have a `%s` placeholder for the exchange\n\n        Primary use is to get a specific set of symbols\n        '''\n\n        self.psql_cur.execute(query, (self.exchange_name,))\n        results = self.psql_cur.fetchall()\n        ret = {}\n        for result in results:\n            ret[result[0]] = {\n                'base_id': self.symbol_data[result[0]]['base_id'],\n                'quote_id': self.symbol_data[result[0]]['quote_id']\n            }\n        return ret\n\n    def run_fetch_ohlcvs(\n        self,\n        symbols: list,\n        start_date_dt: datetime.datetime,\n        end_date_dt: datetime.datetime,\n        update: bool=False\n    ) -> None:\n        '''\n        Interface to run fetching OHLCVS for some specified symbols\n\n        :params:\n            `symbols`: list of symbol string\n            `start_date_dt`: datetime obj - for start date\n            `end_date_dt`: datetime obj - for end date\n            `update`: bool - whether to update when inserting\n                to PSQL database\n        '''\n\n        loop = self._setup_event_loop()\n        try:\n            self.logger.info(\"Run_fetch_ohlcvs: Fetching OHLCVS for indicated symbols\")\n            loop.run_until_complete(\n                self._fetch_ohlcvs_symbols(symbols, start_date_dt, end_date_dt, update)\n            )\n        finally:\n            self.logger.info(\n                \"Run_fetch_ohlcvs: Finished fetching OHLCVS for indicated symbols\")\n            loop.close()\n\n    def run_fetch_ohlcvs_all(\n        self,\n        start_date_dt: datetime.datetime,\n        end_date_dt: datetime.datetime,\n        update: bool=False\n    ) -> None:\n        '''\n        Interface to run the fetching OHLCVS for all symbols\n        \n        :params:\n            `symbols`: list of symbol string\n            `start_date_dt`: datetime obj - for start date\n            `end_date_dt`: datetime obj - for end date\n        '''\n\n        # Have to fetch symbol data first to\n        # make sure it's up-to-date\n        self.fetch_symbol_data()\n        symbols = self.symbol_data.keys()\n\n        self.run_fetch_ohlcvs(symbols, start_date_dt, end_date_dt, update)\n        self.logger.info(\"Run_fetch_ohlcvs_all: Finished fetching OHLCVS for all symbols\")\n\n    def run_fetch_ohlcvs_mutual_basequote(\n        self,\n        start_date_dt: datetime.datetime,\n        end_date_dt: datetime.datetime,\n        update: bool=False\n    ) -> None:\n        '''\n        Interface to run the fetching of the mutual base-quote symbols\n\n        :params:\n            `start_date_dt`: datetime obj\n            `end_date_dt`: datetime obj\n        '''\n        # Have to fetch symbol data first to\n        # make sure it's up-to-date\n        self.fetch_symbol_data()\n\n        symbols = self.get_symbols_from_exch(MUTUAL_BASE_QUOTE_QUERY)\n        self.run_fetch_ohlcvs(symbols.keys(), start_date_dt, end_date_dt, update)\n        self.logger.info(\n            \"Run_fetch_ohlcvs_mutual_basequote: Finished fetching OHLCVS for mutual symbols\"\n        )\n\n    def run_resume_fetch(self) -> None:\n        '''\n        Interface to run the resuming of fetching tasks\n        '''\n\n        loop = self._setup_event_loop()\n        try:\n            self.logger.info(\"Run_resume_fetch: Resuming fetching tasks from Redis sets\")\n            loop.run_until_complete(self._resume_fetch())\n        finally:\n            self.logger.info(\"Run_resume_fetch: Finished fetching OHLCVS\")\n            loop.close()\n"}
{"type": "source_file", "path": "fetchers/ws/updater.py", "content": "# This module collects websocket subbed data in Redis, from all exchanges\n#   and inserts them into PSQL database\n\nimport time\nimport redis\nimport psycopg2\nfrom typing import NoReturn\nfrom common.config.constants import (\n    REDIS_HOST, REDIS_USER,\n    REDIS_PASSWORD, REDIS_DELIMITER,\n    DBCONNECTION, OHLCVS_TABLE\n)\nfrom common.utils.logutils import create_logger\nfrom common.helpers.datetimehelpers import (\n    milliseconds_to_datetime, redis_time, milliseconds\n)\nfrom common.helpers.numbers import round_decimal\nfrom fetchers.config.constants import (\n    WS_SUB_LIST_REDIS_KEY, WS_SUB_PREFIX,\n    WS_SUB_PROCESSING_REDIS_KEY, NUM_DECIMALS\n)\nfrom fetchers.config.queries import PSQL_INSERT_IGNOREDUP_QUERY\nfrom fetchers.helpers.dbhelpers import psql_bulk_insert\nfrom fetchers.helpers.ws import (\n    make_sub_val, make_sub_redis_key, make_serve_redis_key\n)\n\n\nUPDATE_FREQUENCY_SECS = 10\nDATA_HELD_MLS_THRESHOLD = 3600000 # 1 day\n\nclass OHLCVWebsocketUpdater:\n    '''\n    Updater for OHLCVs from websockets;\n\n    This object updates/inserts OHLCV data from Redis into PSQL;\n\n    The update mechanism for now is periodic (i.e., update/insert after\n    a certain period of time)\n    '''\n\n    def __init__(\n        self, log_to_stream: bool = False, log_filename: str = None\n    ):\n        check_log_file = log_to_stream is False and log_filename is None\n        if check_log_file:\n            raise ValueError(\n                \"log_filename must be provided if not logging to stream\"\n            )\n\n        self.redis_client = redis.Redis(\n            host=REDIS_HOST,\n            username=REDIS_USER,\n            password=REDIS_PASSWORD,\n            decode_responses=True\n        )\n\n        # Set the log writing mode to 'w', because\n        # it will be too much log data if we use 'a'\n        self.logger = create_logger(\n            \"WS_fetcher_updater\",\n            stream_handler=log_to_stream,\n            log_filename=log_filename,\n            mode=\"w\"\n        )\n        self.psql_conn = psycopg2.connect(DBCONNECTION)\n\n    @classmethod\n    def make_rows_insert(\n            cls, ts: str, exch: str, base: str, quote: str, ohlcv: list\n        ) -> tuple:\n        '''\n        Makes rows to insert to PSQL db\n\n        :params:\n            `ohlcv`: list of unpacked OHLCV\n        '''\n\n        return (\n            milliseconds_to_datetime(int(ts)),\n            exch,\n            base,\n            quote,\n            round_decimal(ohlcv[1], NUM_DECIMALS),\n            round_decimal(ohlcv[2], NUM_DECIMALS),\n            round_decimal(ohlcv[3], NUM_DECIMALS),\n            round_decimal(ohlcv[4], NUM_DECIMALS),\n            round_decimal(ohlcv[5], NUM_DECIMALS)\n        )\n\n    @classmethod\n    def make_processing_val(\n            cls, exch: str, base: str, quote: str, ohlcv_str: str\n        ) -> str:\n        '''\n        Makes a serialized processing value from WS sub list\n\n        This serialized value is a temp backup in case\n            of a crash before actual OHLCV data enter PSQL db\n\n        :params:\n            `ohlcv_str`: string of packed OHLCV;\n                something like `{t}{d}{o}{d}{h}{d}{l}{d}{c}{d}{v}`\n                see: `fetchers.helpers.ws` module\n        '''\n\n        return f'{exch}{REDIS_DELIMITER}{base}{REDIS_DELIMITER}{quote}{REDIS_DELIMITER}{ohlcv_str}'\n\n    def prepare_insert(\n            self, data: dict, insert_list: list,\n            ts: str, exch: str, base: str, quote: str\n        ) -> None:\n        '''\n            Prepares rows for bulk insert\n\n            :params:\n                `data`: dict of OHLCV from WS sub list\n                `insert_list`: list of OHLCV rows to insert\n        '''\n\n        ohlcv = data[ts].split(REDIS_DELIMITER)\n        insert_list.append(\n            self.make_rows_insert(\n                ts, exch, base, quote, ohlcv\n            )\n        )\n        # Add processing ohlcv to a Redis set\n        #   in case of crash before data entering PSQL db\n        processing_val = self.make_processing_val(\n            exch,\n            base,\n            quote,\n            data[ts]\n        )\n        self.redis_client.sadd(\n            WS_SUB_PROCESSING_REDIS_KEY,\n            processing_val\n        )\n    \n    def update(self) -> NoReturn:\n        '''\n        Collects ohlcv data in ws sub Redis keys and inserts them\n            into PSQL db every `UPDATE_FREQUENCY_SECS` seconds\n        '''\n\n        try:\n            while True:\n                self.logger.info(\"Collecting subscribed OHLCV data in Redis\")\n                self.logger.info(\n                    f\"Length of WS sub list: {self.redis_client.scard(WS_SUB_LIST_REDIS_KEY)}\")\n                ohlcvs_table_insert = []\n                for key in self.redis_client.smembers(WS_SUB_LIST_REDIS_KEY):\n                    exchange, base_id, quote_id = \\\n                        key.split(WS_SUB_PREFIX)[1].split(REDIS_DELIMITER)\n                    data = self.redis_client.hgetall(key)\n                    # If no data, remove the key\n                    # Elif there is data and len data > 1,\n                    #   sort timestamps ascending, exclude the latest one,\n                    #   prepare ohlcv rows to insert\n                    #   then delete data related to inserted timestamps\n                    if not data:\n                        self.redis_client.srem(WS_SUB_LIST_REDIS_KEY, key)\n                        self.logger.info(\n                            f\"WS Fetcher Updater: Removed empty key {key} from sub list\")\n                    elif len(data) == 1:\n                        ts = list(data.keys())[0]\n                        now = milliseconds(redis_time(self.redis_client))\n                        if now - int(ts) > DATA_HELD_MLS_THRESHOLD: # more than 1 hour\n                            self.prepare_insert(\n                                data, ohlcvs_table_insert, ts,\n                                exchange, base_id, quote_id\n                            )\n                            self.redis_client.srem(WS_SUB_LIST_REDIS_KEY, key)\n                            self.redis_client.delete(key)\n                            self.logger.info(\n                                f\"WS Fetcher Updater: Key {key} has been holding data for more than 1 hour - inserting that value to PSQL db and removing key {key}\")\n                    elif len(data) > 1:\n                        ts_to_insert = sorted(data.keys())[:-1]\n                        for ts in ts_to_insert:\n                            self.prepare_insert(\n                                data, ohlcvs_table_insert, ts,\n                                exchange, base_id, quote_id\n                            )\n                        self.redis_client.hdel(key, *ts_to_insert)\n                # Bulk insert\n                try:\n                    success = psql_bulk_insert(\n                        self.psql_conn,\n                        ohlcvs_table_insert,\n                        OHLCVS_TABLE,\n                        insert_ignoredup_query = PSQL_INSERT_IGNOREDUP_QUERY\n                    )\n                    # If success, clean up\n                    # If not, unpack the values and resend them back to\n                    #   corresponding `ws_sub_redis_key`\n                    # TODO: the unpacking work should be done by a separate worker;\n                    #   maybe a cleaner? this looks messy\n                    if success:\n                        self.logger.info(\n                            f\"WS Fetcher Updater: Successfully updated OHLCV to PSQL db - {len(ohlcvs_table_insert)} rows\")\n                        self.redis_client.delete(WS_SUB_PROCESSING_REDIS_KEY)\n                    else:\n                        self.logger.warning(\n                            \"WS Fetcher Updater: Failed to update OHLCV to PSQL db - sending OHLCV values back to sub redis key\")\n                        for processing_val in \\\n                            self.redis_client.smembers(WS_SUB_PROCESSING_REDIS_KEY):\n                            exch, base, quote, \\\n                            ts, open_, high_, \\\n                            low_, close_, volume_ \\\n                                = processing_val.split(REDIS_DELIMITER)\n                            sub_val = make_sub_val(\n                                ts,\n                                open_, high_, low_, close_, volume_,\n                                REDIS_DELIMITER\n                            )\n                            ws_sub_redis_key = make_sub_redis_key(\n                                exch,\n                                base,\n                                quote,\n                                REDIS_DELIMITER\n                            )\n                            self.redis_client.sadd(\n                                WS_SUB_LIST_REDIS_KEY, ws_sub_redis_key)\n                            self.redis_client.hset(\n                                ws_sub_redis_key, ts, sub_val)\n                            self.redis_client.srem(\n                                WS_SUB_PROCESSING_REDIS_KEY,\n                                processing_val\n                            )\n                # Reconnects if connection is closed\n                except psycopg2.InterfaceError as exc:\n                    self.logger.warning(\n                        f\"WS Fetcher Updater: EXCEPTION: {exc}. Reconnecting...\")\n                    self.psql_conn = psycopg2.connect(DBCONNECTION)\n                except Exception as exc:\n                    self.logger.error(\n                        f\"WS Fetcher Updater: EXCEPTION: {exc}\")\n                    raise exc\n                time.sleep(UPDATE_FREQUENCY_SECS)\n        finally:\n            self.psql_conn.close()\n"}
{"type": "source_file", "path": "fetchers/config/queries.py", "content": "# This module contains queries related to fetching\n\n# Generic insert query that ignores unique constraints\nPSQL_INSERT_IGNOREDUP_QUERY = \"INSERT INTO {table} VALUES %s ON CONFLICT DO NOTHING;\"\n\n# Insert query into ohlcvs that updates the existing rows with new ones\n# PSQL_INSERT_UPDATE_QUERY = '''\n# INSERT INTO {table} VALUES %s\n# ON CONFLICT ({}) DO UPDATE SET ({}) = ({});\n# '''\n# Modified this query to enable single-column updates\nPSQL_INSERT_UPDATE_QUERY = '''\nINSERT INTO {table} VALUES %s\nON CONFLICT ({}) DO UPDATE SET {} = {};\n'''\n\n# Get latest timestamp for each exchange-base-quote\n#  combination\nLATEST_SYMEXCH_QUERY = '''\nselect ohlcvss.exchange, symexch.symbol, ohlcvss.time\nfrom symbol_exchange symexch,\n   lateral (\n      select time, exchange, base_id, quote_id\n      from ohlcvs\n      where ohlcvs.exchange = symexch.exchange\n         and base_id = symexch.base_id\n         and quote_id = symexch.quote_id\n      order by base_id, quote_id, time desc\n      limit 1\n   ) ohlcvss;\n'''\n\n# Get 30 sorted mutual base-quote among all exchanges (currently 3)\n#  from a materialized view `common_basequote_30`\nMUTUAL_BASE_QUOTE_QUERY = '''\nselect symexch.symbol\nfrom common_basequote_30 as cb\ninner join symbol_exchange as symexch\n   on cb.base_id = symexch.base_id\n      and cb.quote_id = symexch.quote_id\nwhere symexch.exchange = %s;\n'''\n\n# Get all symbols from an exchange\nALL_SYMBOLS_EXCHANGE_QUERY = '''\nselect symbol from symbol_exchange where exchange=%s order by base_id, quote_id;\n'''\n\n# Get timestamp gaps\nTS_GAPS_QUERY = '''\nselect row_to_json(results)\nfrom (\n   with tn as (\n      select ohlcvs.\"time\" as time,\n      lead(ohlcvs.\"time\", 1) over (\n               partition by ohlcvs.exchange, ohlcvs.base_id, ohlcvs.quote_id\n               order by ohlcvs.\"time\" asc\n         ) as next_time,\n      ohlcvs.exchange, symexch.symbol\n      from ohlcvs\n         left join symbol_exchange as symexch\n            on ohlcvs.exchange = symexch.exchange\n               and ohlcvs.base_id = symexch.base_id\n               and ohlcvs.quote_id = symexch.quote_id\n      limit 10000\n   ),\n   time_diff as (\n      select *, EXTRACT(EPOCH FROM (next_time - \"time\")) AS difference\n      from tn\n   )\n   select *\n   from time_diff\n   where difference > 60\n   order by \"time\" asc\n) results;\n'''"}
{"type": "source_file", "path": "common/helpers/numbers.py", "content": "# This module contains common number helpers\n\nfrom decimal import Decimal\nfrom typing import Union\n\n\ndef round_decimal(\n        number: Union[float, int, Decimal, str],\n        n_decimals: int=2\n    ) -> Union[Decimal, None]:\n    '''\n    Rounds a `number` to `n_decimals` decimals\n\n    If number is None, returns None\n\n    :params:\n        `number`: float, int or Decimal type or str representing float\n        `n_decimals`: number of decimals\n    '''\n\n    if number is None:\n        return None\n    return round(Decimal(number), n_decimals)\n"}
{"type": "source_file", "path": "web/db/session.py", "content": "# Backend database session\n\nfrom sqlalchemy import create_engine\nfrom sqlalchemy.orm import sessionmaker, scoped_session\nfrom sqlalchemy.ext.declarative import declarative_base\nfrom dogpile.cache.region import make_region\nfrom common.config.constants import (\n    COMMON_HOST, POSTGRES_HOST, POSTGRES_USER,\n    POSTGRES_PASSWORD, POSTGRES_DB,\n    REDIS_HOST, REDIS_PASSWORD, REDIS_PORT\n)\nfrom web.routes.api.rest.utils import caching\n\n\n# Connection and session\nSQLALCHEMY_DATABASE_URL = f\"postgresql://{POSTGRES_USER}:{POSTGRES_PASSWORD}@{POSTGRES_HOST}/{POSTGRES_DB}\"\nengine = create_engine(\n    SQLALCHEMY_DATABASE_URL\n    # connect_args={\"check_same_thread\": False}\n)\nSessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)\n\n# Caching for REST requests\nregion = make_region().configure(\n    'dogpile.cache.redis',\n    arguments = {\n        'host': REDIS_HOST,\n        'port': REDIS_PORT,\n        'password': REDIS_PASSWORD,\n        'db': 0,\n        'redis_expiration_time': 60*60*2,   # 2 hours\n        'distributed_lock': True,\n        'thread_local_lock': False\n        }\n)\nregions = {'default': region}\ncache = caching.ORMCache(regions)\ncache.listen_on_session(scoped_session(SessionLocal))\n"}
{"type": "source_file", "path": "fetchers/ws/bittrex.py", "content": "# Adapted from Bittrex Python WS client example\n#\n# Last tested 2020/09/24 on Python 3.8.5\n# Note: This file is intended solely for testing purposes and may only be used\n#   as an example to debug and compare with your code. The 3rd party libraries\n#   used in this example may not be suitable for your production use cases.\n#   You should always independently verify the security and suitability of any\n#   3rd party library used in your code.\n\n# https://github.com/slazarov/python-signalr-client\n\n\nimport hashlib\nimport hmac\nimport json\nimport logging\nimport asyncio\nimport random\nimport uuid\nimport redis\nfrom typing import Any, Iterable, NoReturn, Union\nfrom signalr_aio import Connection\nfrom base64 import b64decode\nfrom zlib import decompress, MAX_WBITS\nfrom common.config.constants import (\n    REDIS_HOST, REDIS_USER,\n    REDIS_PASSWORD, REDIS_DELIMITER,\n    DEFAULT_DATETIME_STR_RESULT\n)\nfrom common.utils.logutils import create_logger\nfrom common.helpers.datetimehelpers import str_to_milliseconds, redis_time\nfrom fetchers.config.constants import (\n    WS_SUB_REDIS_KEY, WS_SERVE_REDIS_KEY, WS_SUB_LIST_REDIS_KEY\n)\nfrom fetchers.config.queries import MUTUAL_BASE_QUOTE_QUERY\nfrom fetchers.rest.bittrex import BittrexOHLCVFetcher, EXCHANGE_NAME\nfrom fetchers.utils.exceptions import (\n    ConnectionClosed, UnsuccessfulConnection, InvalidStatusCode\n)\nfrom fetchers.helpers.ws import (\n    make_sub_val,\n    make_sub_redis_key,\n    make_serve_redis_key\n)\n\n\nURI = 'https://socket-v3.bittrex.com/signalr'\nAPI_KEY = ''\nAPI_SECRET = ''\nBACKOFF_MIN_SECS = 2.0\nBACKOFF_MAX_SECS = 60.0\n\nclass BittrexOHLCVWebsocket:\n    '''\n    Bittrex OHLCV websocket fetcher\n    '''\n\n    def __init__(\n        self, log_to_stream: bool = False, log_filename: str = None\n    ):\n        check_log_file = log_to_stream is False and log_filename is None\n        if check_log_file:\n            raise ValueError(\n                \"log_filename must be provided if not logging to stream\"\n            )\n\n        self.redis_client = redis.Redis(\n            host=REDIS_HOST,\n            username=REDIS_USER,\n            password=REDIS_PASSWORD,\n            decode_responses=True\n        )\n\n        # SignalR hub & asyncio\n        self.signalr_hub = None\n        self.asyncio_lock = asyncio.Lock()\n        self.invocation_event = None\n        self.invocation_response = None\n        self.subscription_success = False\n\n        # Rest fetcher for convenience\n        self.rest_fetcher = BittrexOHLCVFetcher()\n\n        # Latest timestamp with data\n        self.latest_ts = None\n\n        # Logging\n        self.logger = create_logger(\n            f'{EXCHANGE_NAME}_websocket',\n            stream_handler=log_to_stream,\n            log_filename=log_filename\n        )\n\n        # Backoff\n        self.backoff_delay = BACKOFF_MIN_SECS\n\n    async def _connect(self) -> None:\n        self.latest_ts = redis_time(self.redis_client)\n        connection = Connection(URI)\n        self.signalr_hub = connection.register_hub('c3')\n        connection.received += self.on_message\n        connection.error += self.on_error\n        connection.start()\n        self.logger.info('Connected')\n\n    async def _authenticate(self) -> None:\n        timestamp = str(int(redis_time(self.redis_client)) * 1000)\n        random_content = str(uuid.uuid4())\n        content = timestamp + random_content\n        signed_content = hmac.new(\n            API_SECRET.encode(), content.encode(), hashlib.sha512).hexdigest()\n\n        response = await self._invoke(\n            'Authenticate',\n            API_KEY,\n            timestamp,\n            random_content,\n            signed_content\n        )\n\n        if response['Success']:\n            self.logger.info('Authenticated')\n            self.signalr_hub.client.on('authenticationExpiring', self.on_auth_expiring)\n        else:\n            self.logger.warning('Authentication failed: ' + response['ErrorCode'])\n\n    async def _subscribe(self, symbols: Iterable, i: int = 0) -> None:\n        '''\n        Subscribes to Bittrex WS for `symbols`\n\n        :params:\n            `symbols` list of symbols\n                e.g., ['ETH-BTC', 'BTC-EUR']\n        '''\n\n        self.subscription_success = False\n\n        # self.signalr_hub.client.on('trade', on_trade)\n        self.signalr_hub.client.on('heartbeat', self.on_heartbeat)\n        self.signalr_hub.client.on('candle', self.on_candle)\n        channels = (\n            'heartbeat',\n            # 'candle_BTC-USD_MINUTE_1'\n            *(f'candle_{symbol}_MINUTE_1' for symbol in symbols)\n        )\n\n        response = await self._invoke('Subscribe', channels)\n        for c in range(len(channels)):\n            if response[c]['Success']:\n                # Only one success is enough to switch to True\n                self.subscription_success = True\n            else:\n                self.logger.error(\n                    f\"Group {i}: Subscription to {channels[c]} failed: {response[c]['ErrorCode']}\")\n                # raise UnsuccessfulConnection // not a good idea to raise here\n        if self.subscription_success:\n            self.logger.info(f\"Group {i}: Subscription successful\")\n\n    async def _invoke(self, method: str, *args) -> Union[Any, None]:\n        '''\n        Invokes a method\n\n        Default function from template\n        '''\n\n        async with self.asyncio_lock:\n            self.invocation_event = asyncio.Event()\n            self.signalr_hub.server.invoke(method, *args)\n            await self.invocation_event.wait()\n            return self.invocation_response\n\n    async def on_message(self, **msg) -> None:\n        '''\n        Action to take on message\n\n        Default function from template\n        '''\n\n        if 'R' in msg:\n            self.invocation_response = msg['R']\n            self.invocation_event.set()\n\n    async def on_error(self, msg) -> None:\n        '''\n        Action to take on error\n\n        Default function from template\n        '''\n\n        self.latest_ts = redis_time(self.redis_client)\n        self.logger.warning(msg)\n\n    async def on_heartbeat(self, msg) -> None:\n        '''\n        Action to take on heartbeat from websocket server\n\n        Default function from template\n        '''\n\n        self.latest_ts = redis_time(self.redis_client)\n        # self.logger.info('\\u2661')\n\n    async def on_auth_expiring(self, msg) -> None:\n        '''\n        Action to take on AUTH expiring\n\n        Default function from template\n        '''\n\n        self.logger.info('Authentication expiring...')\n        asyncio.create_task(self._authenticate())\n\n    async def on_trade(self, msg) -> None:\n        '''\n        Action to take on trade response\n\n        Default function from template\n        '''\n\n        self.latest_ts = redis_time(self.redis_client)\n        await self.decode_message('Trade', msg)\n\n    async def on_candle(self, msg) -> None:\n        '''\n        Action to take on candle response\n\n        Default function from template\n        '''\n\n        self.latest_ts = redis_time(self.redis_client)\n        respj = await self.decode_message('Candle', msg)\n\n        # If resp is dict, process and push to Redis\n        # Convert timestamp to milliseconds first\n        #   for conformity with the WS updater and other exchanges\n        if isinstance(respj, dict):\n            try:\n                symbol = respj['marketSymbol']\n                ohlcv = respj['delta']\n                timestamp = str_to_milliseconds(\n                    ohlcv['startsAt'], DEFAULT_DATETIME_STR_RESULT)\n                open_ = ohlcv['open']\n                high_ = ohlcv['high']\n                low_ = ohlcv['low']\n                close_ = ohlcv['close']\n                volume_ = ohlcv['volume']\n                base_id = self.rest_fetcher.symbol_data[symbol]['base_id']\n                quote_id = self.rest_fetcher.symbol_data[symbol]['quote_id']\n\n                # We make \"sub'ed value\" by serializing the price\n                #   data so it can be put into Redis\n                # Then, unique sub'ed and serve Redis keys\n                #   are created to store the above sub'ed value\n                # Also note here that the value put into the sub_\n                #   and the serve_ keys are different:\n                #   >> sub_ key is a hash with its [internal]\n                #   keys as timestamps; this is so that the updater\n                #   script can bulk-insert all the price data in\n                #   the sub_ key to PSQL later, meanwhile...\n                #   >> serve_ key is a hash with its [internal]\n                #   keys as timestamp, o, h, l, c, v; this is\n                #   because the web service (a.k.a. FastAPI) only\n                #   serves to the user [via its websocket connection]\n                #   the latest price data, meaning when any new data\n                #   comes in from the exchange's ws API,\n                #   the hash gets updated\n\n                sub_val = make_sub_val(\n                    timestamp,\n                    open_, high_, low_, close_, volume_,\n                    REDIS_DELIMITER\n                )\n                ws_sub_redis_key = make_sub_redis_key(\n                    EXCHANGE_NAME,\n                    base_id,\n                    quote_id,\n                    REDIS_DELIMITER\n                )\n                ws_serve_redis_key = make_serve_redis_key(\n                    EXCHANGE_NAME,\n                    base_id,\n                    quote_id,\n                    REDIS_DELIMITER\n                )\n\n                # Add ws sub key to set of all ws sub keys\n                # Set hash value for ws sub key\n                # Replace ws serve key hash if this timestamp\n                #   is more up-to-date\n                self.redis_client.sadd(\n                    WS_SUB_LIST_REDIS_KEY, ws_sub_redis_key\n                )\n                self.redis_client.hset(\n                    ws_sub_redis_key, timestamp, sub_val)\n                current_timestamp = self.redis_client.hget(\n                    ws_serve_redis_key, 'time')\n                if current_timestamp is None or \\\n                    timestamp >= int(current_timestamp):\n                    self.redis_client.hset(\n                        ws_serve_redis_key,\n                        mapping = {\n                            'time': timestamp,\n                            'open': open_,\n                            'high': high_,\n                            'low': low_,\n                            'close': close_,\n                            'volume': volume_\n                        }\n                    )\n            except Exception as exc:\n                self.logger.warning(\n                    f\"Bittrex WS Fethcer: EXCEPTION: {exc}\")\n\n    async def decode_message(self, title, msg) -> None:\n        '''\n        Decodes message\n\n        Default function from template\n        '''\n\n        decoded_msg = await self.process_message(msg[0])\n        return decoded_msg\n\n    async def process_message(self, message) -> None:\n        '''\n        Processes message\n\n        Default function from template\n        '''\n\n        try:\n            decompressed_msg = decompress(\n                b64decode(message, validate=True), -MAX_WBITS)\n        except SyntaxError:\n            decompressed_msg = decompress(b64decode(message, validate=True))\n        return json.loads(decompressed_msg.decode())\n\n    async def subscribe(self, symbols: Iterable) -> NoReturn:\n        '''\n        Subscribes to WS channels of `symbols`\n        '''\n\n        while True:\n            try:\n                now = redis_time(self.redis_client)\n                if self.signalr_hub is None \\\n                    or (now - self.latest_ts) > 60 \\\n                    or (not self.subscription_success):\n                    await self._connect()\n                    if API_SECRET != '':\n                        await self._authenticate()\n                    else:\n                        self.logger.info('Authentication skipped because API key was not provided')\n                    await self._subscribe(symbols)\n            # Not sure what kind of exception we will encounter\n            except (ConnectionClosed, InvalidStatusCode) as exc:\n                self.logger.warning(\n                    f\"Connection raised exception: {exc} - reconnecting...\"\n                )\n                await asyncio.sleep(min(self.backoff_delay, BACKOFF_MAX_SECS))\n                self.backoff_delay *= (1+random.random()) # add a random factor\n\n            # Sleep to release event loop\n            await asyncio.sleep(0.01)\n\n    async def all(self) -> NoReturn:\n        '''\n        Subscribes to WS channels of all symbols\n        '''\n\n        self.rest_fetcher.fetch_symbol_data()\n        symbols =  tuple(self.rest_fetcher.symbol_data.keys())\n        await asyncio.gather(self.subscribe(symbols))\n\n    async def mutual_basequote(self) -> NoReturn:\n        '''\n        Subscribes to all base-quote's that are available\n        in all exchanges\n        '''\n\n        symbols_dict = self.rest_fetcher.get_symbols_from_exch(MUTUAL_BASE_QUOTE_QUERY)\n        await asyncio.gather(self.subscribe(symbols_dict.keys()))\n\n    def run_mutual_basequote(self) -> None:\n        '''\n        API to run the `mutual base-quote` method\n        '''\n\n        # loop = asyncio.get_event_loop()\n        # if loop.is_closed():\n        #     asyncio.set_event_loop(asyncio.new_event_loop())\n        #     loop = asyncio.get_event_loop()\n        # try:\n        #     loop.create_task(self.mutual_basequote())\n        #     loop.run_forever()\n        # finally:\n        #     loop.close()\n        asyncio.run(self.mutual_basequote())\n\n    def run_all(self) -> None:\n        '''\n        API to run the `all` method\n        '''\n\n        asyncio.run(self.all())\n"}
{"type": "source_file", "path": "fetchers/config/constants.py", "content": "# This module contains constants\n\nimport signal\nfrom dotenv import dotenv_values \n\n# Load env vars\nconfigs = dotenv_values(\".env\")\n\n# HTTPX/REST\nTHROTTLER_RATE_LIMITS = {\n    'RATE_LIMIT_HITS_PER_MIN': {\n        'bittrex': 55,\n        'bitfinex': 85,\n        'binance': 1200\n    },\n    'RATE_LIMIT_SECS_PER_MIN': 60\n}\nHTTPX_MAX_CONCURRENT_CONNECTIONS = {\n    'bittrex': 100, # increased from 55\n    'bitfinex': 100, # increased from 85\n    'binance': 200 # decreased from 500\n}\nHTTPX_DEFAULT_TIMEOUT = 3.0\nHTTPX_DEFAULT_RETRIES = 12\n\n# Asyncio signals\nASYNC_SIGNALS = (signal.SIGHUP, signal.SIGTERM, signal.SIGINT)\n\n# REST/WS rate limit Redis keys\nREST_RATE_LIMIT_REDIS_KEY = \"rest_rate_limit_{exchange}\"\nWS_RATE_LIMIT_REDIS_KEY = \"ws_rate_limit_{exchange}\"\n\n# Websocket Redis keys\n# Sub is for storing temp subscribed ws data to update psql db later\n# Serve is for serving real time data to our web service\nWS_SUB_PREFIX = \"ws_sub_\"\nWS_SUB_REDIS_KEY = \"ws_sub_{exchange}{delimiter}{base_id}{delimiter}{quote_id}\"\nWS_SERVE_REDIS_KEY = \"ws_serve_{exchange}{delimiter}{base_id}{delimiter}{quote_id}\"\nWS_SUB_LIST_REDIS_KEY = \"ws_sub_list\"\nWS_SUB_PROCESSING_REDIS_KEY = \"ws_sub_processing\"\n\n# To-fetch and fetching Redis set keys\nOHLCVS_TOFETCH_REDIS_KEY = \"ohlcvs_tofetch_{exchange}\"\nOHLCVS_FETCHING_REDIS_KEY = \"ohlcvs_fetching_{exchange}\"\n\n# PSQL Constants\nOHLCV_UNIQUE_COLUMNS = (\"time\", \"exchange\", \"base_id\", \"quote_id\")\nOHLCV_UPDATE_COLUMNS = (\"open\", \"high\", \"low\", \"close\", \"volume\")\nSYMEXCH_UNIQUE_COLUMNS = (\"exchange\", \"base_id\", \"quote_id\")\nSYMEXCH_UPDATE_COLUMNS = (\"is_trading\", \"symbol\")\nNUM_DECIMALS = 4 # number of decimals\n"}
{"type": "source_file", "path": "common/__init__.py", "content": ""}
{"type": "source_file", "path": "common/utils/asyncioutils.py", "content": "# Utils for asyncio that may be used in any module\n\nimport traceback\nimport asyncio\nfrom signal import Signals\nfrom threading import Thread\nfrom typing import Any\nfrom fetchers.config.constants import ASYNC_SIGNALS\n\n\ndef aio_handle_exception(loop: asyncio.AbstractEventLoop, context: Any) -> None:\n    '''\n    Asyncio exception handler, for use with\n        `aio_shutdown` and `aio_set_exception_handler`\n\n    :params:\n        `loop`: asyncio event loop\n        `context`: ..?\n    \n    Source: https://www.roguelynn.com/words/asyncio-exception-handling/\n    '''\n\n    # context[\"message\"] will always be there;\n    # but context[\"exception\"] may not\n    msg = context.get(\"exception\", context[\"message\"])\n    print(f\"Caught exception: {msg}\")\n    print(\"Printing traceback...\")\n    traceback.print_stack()\n    print(\"Shutting down...\")\n    asyncio.create_task(aio_shutdown(loop))\n\nasync def aio_shutdown(loop: asyncio.AbstractEventLoop, signal: Signals=None) -> None:\n    '''\n    Cleans tasks tied to the service's shutdown\n    '''\n\n    if signal:\n        print(f\"Received exit signal {signal.name}...\")\n    print(\"Nacking outstanding messages\")\n    tasks = [\n        t for t in asyncio.all_tasks() if t \\\n            is not asyncio.current_task()\n    ]\n\n    [task.cancel() for task in tasks]\n\n    print(f\"Cancelling {len(tasks)} outstanding tasks\")\n    await asyncio.gather(*tasks, return_exceptions=True)\n    print(f\"Flushing metrics\")\n    loop.stop()\n\ndef aio_set_exception_handler(loop: asyncio.AbstractEventLoop) -> None:\n    '''\n    Sets exception handler for a loop\n\n    Source: https://www.roguelynn.com/words/asyncio-exception-handling/\n    '''\n\n    for s in ASYNC_SIGNALS:\n        loop.add_signal_handler(\n            s, lambda s=s: asyncio.create_task(aio_shutdown(loop, signal=s))\n        )\n    loop.set_exception_handler(aio_handle_exception)\n\n\nclass AsyncLoopThread(Thread):\n    '''\n    Async loop thread for adding coroutines to\n        a different OS thread\n\n    Source: https://stackoverflow.com/questions/34499400/how-to-add-a-coroutine-to-a-running-asyncio-loop\n    '''\n\n    def __init__(self, daemon: bool = True):\n        super().__init__(daemon=daemon)\n        self.loop = asyncio.new_event_loop()\n\n    def run(self) -> None:\n        asyncio.set_event_loop(self.loop)\n        self.loop.run_forever()\n"}
{"type": "source_file", "path": "celery_app/celery_tasks.py", "content": "# This module contains Celery tasks\n\nimport json\nimport datetime\nfrom celery_app.celery_main import app\nfrom common.config.constants import DEFAULT_DATETIME_STR_QUERY\nfrom common.helpers.datetimehelpers import str_to_datetime\nfrom fetchers.rest.bitfinex import BitfinexOHLCVFetcher\nfrom fetchers.rest.bittrex import BittrexOHLCVFetcher\nfrom fetchers.rest.binance import BinanceOHLCVFetcher\n\n\n# Fetch symbol data to get all symbols into\n#   symbol_exchange psql table\n@app.task\ndef all_fetch_symbol_data():\n    bitfinex_fetcher = BitfinexOHLCVFetcher()\n    binance_fetcher = BinanceOHLCVFetcher()\n    bittrex_fetcher = BittrexOHLCVFetcher()\n    bitfinex_fetcher.fetch_symbol_data()\n    binance_fetcher.fetch_symbol_data()\n    bittrex_fetcher.fetch_symbol_data()\n    bitfinex_fetcher.close_connections()\n    binance_fetcher.close_connections()\n    bittrex_fetcher.close_connections()\n\n# Bitfinex\n@app.task\ndef bitfinex_fetch_ohlcvs_all_symbols(start_date, end_date):\n    bitfinex_fetcher = BitfinexOHLCVFetcher()\n    # The dates need to be de-serialized\n    start_date = str_to_datetime(start_date, f=DEFAULT_DATETIME_STR_QUERY)\n    end_date = str_to_datetime(end_date, f=DEFAULT_DATETIME_STR_QUERY)\n    bitfinex_fetcher.run_fetch_ohlcvs_all(start_date, end_date)\n    bitfinex_fetcher.close_connections()\n\n@app.task\ndef bitfinex_fetch_ohlcvs_symbols(symbols, start_date, end_date):\n    '''\n    fetches ohlcvs from Bitfinex for a list of symbols\n    params (all params are str because Celery serializes args):\n        `symbols`: list of symbols\n        `start_date`: string of datetime\n        `end_date`: string of datetime\n    '''\n\n    bitfinex_fetcher = BitfinexOHLCVFetcher()\n    # Symbols need to be de-serialized\n    if isinstance(symbols, str):\n        symbols = json.loads(symbols)\n    # The dates need to be de-serialized\n    start_date = str_to_datetime(start_date, f=DEFAULT_DATETIME_STR_QUERY)\n    end_date = str_to_datetime(end_date, f=DEFAULT_DATETIME_STR_QUERY)\n    bitfinex_fetcher.run_fetch_ohlcvs(symbols, start_date, end_date)\n    bitfinex_fetcher.close_connections()\n\n@app.task\ndef bitfinex_resume_fetch():\n    bitfinex_fetcher = BitfinexOHLCVFetcher()\n    bitfinex_fetcher.run_resume_fetch()\n    bitfinex_fetcher.close_connections()\n\n@app.task\ndef bitfinex_fetch_ohlcvs_mutual_basequote(start_date, end_date):\n    bitfinex_fetcher = BitfinexOHLCVFetcher()\n    # The dates need to be de-serialized\n    start_date = str_to_datetime(start_date, f=DEFAULT_DATETIME_STR_QUERY)\n    end_date = str_to_datetime(end_date, f=DEFAULT_DATETIME_STR_QUERY)\n    print(f\"Celery: Fetching OHLCVs from {start_date} to {end_date}\")\n    bitfinex_fetcher.run_fetch_ohlcvs_mutual_basequote(start_date, end_date)\n    bitfinex_fetcher.close_connections()\n\n@app.task\ndef bitfinex_fetch_ohlcvs_mutual_basequote_1min():\n    '''\n    Periodically fetches OHLCVs on Bitfinex of mutual symbols\n        from 4 minutes before to 1 minute before\n    '''\n\n    bitfinex_fetcher = BitfinexOHLCVFetcher()\n    end = datetime.datetime.now() - datetime.timedelta(minutes=1)\n    start = end - datetime.timedelta(minutes=4)\n    print(f\"Celery: Fetching OHLCVs from {start} to {end}\")\n    bitfinex_fetcher.run_fetch_ohlcvs_mutual_basequote(start, end, update=True)\n\n# Binance\n@app.task\ndef binance_fetch_ohlcvs_all_symbols(start_date, end_date):\n    binance_fetcher = BinanceOHLCVFetcher()\n    # The dates need to be de-serialized\n    start_date = str_to_datetime(start_date, f=DEFAULT_DATETIME_STR_QUERY)\n    end_date = str_to_datetime(end_date, f=DEFAULT_DATETIME_STR_QUERY)\n    binance_fetcher.run_fetch_ohlcvs_all(start_date, end_date)\n    binance_fetcher.close_connections()\n\n@app.task\ndef binance_fetch_ohlcvs_symbols(symbols, start_date, end_date):\n    '''\n    fetches ohlcvs from Bitfinex for a list of symbols\n    params (all params are str because Celery serializes args):\n        `symbols`: list of symbols\n        `start_date`: datetime\n        `end_date`: datetime\n    '''\n\n    binance_fetcher = BinanceOHLCVFetcher()\n    # Symbols need to be de-serialized\n    if isinstance(symbols, str):\n        symbols = json.loads(symbols)\n    # The dates need to be de-serialized\n    start_date = str_to_datetime(start_date, f=DEFAULT_DATETIME_STR_QUERY)\n    end_date = str_to_datetime(end_date, f=DEFAULT_DATETIME_STR_QUERY)\n    binance_fetcher.run_fetch_ohlcvs(symbols, start_date, end_date)\n    binance_fetcher.close_connections()\n\n@app.task\ndef binance_resume_fetch():\n    binance_fetcher = BinanceOHLCVFetcher()\n    binance_fetcher.run_resume_fetch()\n    binance_fetcher.close_connections()\n\n@app.task\ndef binance_fetch_ohlcvs_mutual_basequote(start_date, end_date):\n    binance_fetcher = BinanceOHLCVFetcher()\n    # The dates need to be de-serialized\n    start_date = str_to_datetime(start_date, f=DEFAULT_DATETIME_STR_QUERY)\n    end_date = str_to_datetime(end_date, f=DEFAULT_DATETIME_STR_QUERY)\n    binance_fetcher.run_fetch_ohlcvs_mutual_basequote(start_date, end_date)\n    binance_fetcher.close_connections()\n\n@app.task\ndef binance_fetch_ohlcvs_mutual_basequote_1min():\n    '''\n    Periodically fetches OHLCVs on Binance of mutual symbols\n        from 4 minutes before to 1 minute before\n    '''\n\n    binance_fetcher = BinanceOHLCVFetcher()\n    end = datetime.datetime.now() - datetime.timedelta(minutes=1)\n    start = end - datetime.timedelta(minutes=4)\n    print(f\"Celery: Fetching OHLCVs from {start} to {end}\")\n    binance_fetcher.run_fetch_ohlcvs_mutual_basequote(start, end, update=True)\n\n# Bittrex\n@app.task\ndef bittrex_fetch_ohlcvs_all_symbols(start_date, end_date):\n    bittrex_fetcher = BittrexOHLCVFetcher()\n    # The dates need to be de-serialized\n    start_date = str_to_datetime(start_date, f=DEFAULT_DATETIME_STR_QUERY)\n    end_date = str_to_datetime(end_date, f=DEFAULT_DATETIME_STR_QUERY)\n    bittrex_fetcher.run_fetch_ohlcvs_all(start_date, end_date)\n    bittrex_fetcher.close_connections()\n\n@app.task\ndef bittrex_fetch_ohlcvs_symbols(symbols, start_date, end_date):\n    '''\n    fetches ohlcvs from Bittrex for a list of symbols\n    params (all params are str because Celery serializes args):\n        `symbols`: list of symbols\n        `start_date`: datetime\n        `end_date`: datetime\n    '''\n\n    bittrex_fetcher = BittrexOHLCVFetcher()\n    # Symbols need to be de-serialized\n    if isinstance(symbols, str):\n        symbols = json.loads(symbols)\n    # The dates need to be de-serialized\n    start_date = str_to_datetime(start_date, f=DEFAULT_DATETIME_STR_QUERY)\n    end_date = str_to_datetime(end_date, f=DEFAULT_DATETIME_STR_QUERY)\n    bittrex_fetcher.run_fetch_ohlcvs(symbols, start_date, end_date)\n    bittrex_fetcher.close_connections()\n\n@app.task\ndef bittrex_resume_fetch():\n    bittrex_fetcher = BittrexOHLCVFetcher()\n    bittrex_fetcher.run_resume_fetch()\n    bittrex_fetcher.close_connections()\n\n@app.task\ndef bittrex_fetch_ohlcvs_mutual_basequote(start_date, end_date):\n    bittrex_fetcher = BittrexOHLCVFetcher()\n    # The dates need to be de-serialized\n    start_date = str_to_datetime(start_date, f=DEFAULT_DATETIME_STR_QUERY)\n    end_date = str_to_datetime(end_date, f=DEFAULT_DATETIME_STR_QUERY)\n    bittrex_fetcher.run_fetch_ohlcvs_mutual_basequote(start_date, end_date)\n    bittrex_fetcher.close_connections()\n\n@app.task\ndef bittrex_fetch_ohlcvs_mutual_basequote_1min():\n    '''\n    Periodically fetches OHLCVs on Bittrex of mutual symbols\n        from 4 minutes before to 1 minute before\n    '''\n\n    bittrex_fetcher = BittrexOHLCVFetcher()\n    end = datetime.datetime.now() - datetime.timedelta(minutes=1)\n    start = end - datetime.timedelta(minutes=4)\n    print(f\"Celery: Fetching OHLCVs from {start} to {end}\")\n    bittrex_fetcher.run_fetch_ohlcvs_mutual_basequote(start, end, update=True)\n"}
{"type": "source_file", "path": "fetchers/rest/bitfinex.py", "content": "### This module fetches bitfinex 1-minute OHLCV data\n\nimport asyncio\nimport datetime\nfrom typing import Any, Iterable, Literal, Tuple, Union\n\nimport backoff\nimport httpx\n\nfrom common.config.constants import \\\n    OHLCVS_ERRORS_TABLE, OHLCVS_TABLE, \\\n    REDIS_DELIMITER\nfrom common.helpers.datetimehelpers import \\\n    datetime_to_milliseconds, milliseconds_to_datetime\nfrom common.helpers.numbers import round_decimal\nfrom fetchers.config.constants import \\\n    HTTPX_DEFAULT_RETRIES, OHLCV_UNIQUE_COLUMNS, \\\n    OHLCV_UPDATE_COLUMNS, REST_RATE_LIMIT_REDIS_KEY, \\\n    THROTTLER_RATE_LIMITS\nfrom fetchers.config.queries import \\\n    PSQL_INSERT_IGNOREDUP_QUERY, PSQL_INSERT_UPDATE_QUERY\nfrom fetchers.helpers.dbhelpers import psql_bulk_insert\nfrom fetchers.rest.base import BaseOHLCVFetcher\nfrom fetchers.utils.asyncioutils import onbackoff, onsuccessgiveup\nfrom fetchers.utils.exceptions import \\\n    MaximumRetriesReached, UnsuccessfulDatabaseInsert\nfrom fetchers.utils.ratelimit import GCRARateLimiter\n\nEXCHANGE_NAME = \"bitfinex\"\nBASE_CANDLE_URL = \"https://api-pub.bitfinex.com/v2/candles\"\nPAIR_EXCHANGE_URL = \"https://api-pub.bitfinex.com/v2/conf/pub:list:pair:exchange\"\nLIST_CURRENCY_URL = \"https://api-pub.bitfinex.com/v2/conf/pub:list:currency\"\nOHLCV_TIMEFRAME = \"1m\"\nOHLCV_SECTION_HIST = \"hist\"\nOHLCV_SECTION_LAST = \"last\"\nOHLCV_LIMIT = 9500\nRATE_LIMIT_HITS_PER_MIN = THROTTLER_RATE_LIMITS['RATE_LIMIT_HITS_PER_MIN'][EXCHANGE_NAME]\nRATE_LIMIT_SECS_PER_MIN = THROTTLER_RATE_LIMITS['RATE_LIMIT_SECS_PER_MIN']\nOHLCVS_CONSUME_BATCH_SIZE = 500\n\nclass BitfinexOHLCVFetcher(BaseOHLCVFetcher):\n    '''REST Fetcher for OHLCV from Bitfinex\n    '''\n    def __init__(self, *args):\n        super().__init__(*args, exchange_name = EXCHANGE_NAME)\n\n        # Rate limiter\n        self.rate_limiter = GCRARateLimiter(\n            REST_RATE_LIMIT_REDIS_KEY.format(exchange = EXCHANGE_NAME),\n            1,\n            RATE_LIMIT_SECS_PER_MIN / RATE_LIMIT_HITS_PER_MIN,\n            redis_client = self.redis_client\n        )\n\n        # Load market data\n        self._load_symbol_data()\n\n    def _load_symbol_data(self) -> None:\n        '''\n        Loads market data into a dict of this form:\n            {\n                '1INCH:USD': {\n                    'base_id': \"1INCH\",\n                    'quote_id': \"USD\"\n                },\n                'some_other_symbol': {\n                    'base_id': \"ABC\",\n                    'quote_id': \"XYZ\"\n                }\n                ...\n            }\n        \n        Saves it in `self.symbol_data`\n\n        This looks like a mess...\n        '''\n\n        # self.symbol_data = {}\n        # Only needs a temporary httpx client\n        # This code can block (non-async) as it's needed for future fetching\n        with httpx.Client(timeout=None) as client:\n            pair_ex_resp = client.get(PAIR_EXCHANGE_URL)\n            list_cur_resp = client.get(LIST_CURRENCY_URL)\n            if pair_ex_resp:\n                pair_ex = pair_ex_resp.json()[0]\n            if list_cur_resp:\n                list_cur = list_cur_resp.json()[0]\n            for symbol in sorted(pair_ex):\n                # e.g., 1INCH:USD\n                # And some extra work to extract base_id and quote_id\n                self.symbol_data[symbol] = {}\n                bq_candidates = []\n                for currency in list_cur:\n                    if \"\" in symbol.split(currency):\n                        bq_candidates.append(currency)\n                bq_candidates = sorted(bq_candidates, key=lambda x: len(x), reverse=False)\n                first = bq_candidates.pop()\n                first_split = symbol.split(first)\n                if \"\" in first_split:\n                    if first_split.index(\"\") == 0:\n                        self.symbol_data[symbol]['base_id'] = first\n                        for second in bq_candidates:\n                            if \"\" in first_split[-1].split(second):\n                                self.symbol_data[symbol]['quote_id'] = second\n                    else:\n                        self.symbol_data[symbol]['quote_id'] = first\n                        for second in bq_candidates:\n                            if \"\" in first_split[0].split(second):\n                                self.symbol_data[symbol]['base_id'] = second\n    \n    @classmethod\n    def make_tsymbol(cls, symbol: str) -> str:\n        '''\n        Returns appropriate trade symbol for bitfinex\n        \n        :params:\n            `symbol`: string (trading symbol, e.g., BTSE:USD)\n        '''\n        \n        return f't{symbol}'\n\n    @classmethod\n    def make_ohlcv_url(\n            cls,\n            time_frame: str,\n            symbol: str,\n            limit: int,\n            start_date_mls: int,\n            end_date_mls: int,\n            sort: Literal[1, -1]\n        ) -> Tuple[str, str]:\n        '''\n        Returns tuple of OHLCV url and OHLCV section\n\n        :params:\n            `time_frame`: string - time frame, e.g,, 1m\n            `symbol`: string - trading symbol, e.g., BTSE:USD\n            `limit`: int - number limit of results fetched\n            `start_date_mls`: int of milliseconds\n            `end_date_mls`: int of milliseconds\n            `sort`: int (1 or -1)\n\n        example: https://api-pub.bitfinex.com/v2/candles/trade:1m:tBTCUSD/hist?limit=10000&start=1577836800000&sort=1\n        '''\n\n        # Has to check for hist or last of OHLCV section\n        # Fetch historical data if time difference between now and start date is > 60k mls\n        delta = datetime_to_milliseconds(datetime.datetime.now()) - start_date_mls\n        symbol = cls.make_tsymbol(symbol)\n\n        if delta > 60000:\n            return (\n                f\"{BASE_CANDLE_URL}/trade:{time_frame}:{symbol}/{OHLCV_SECTION_HIST}?limit={limit}&start={start_date_mls}&end={end_date_mls}&sort={sort}\",\n                OHLCV_SECTION_HIST\n            )\n        else:\n            return (\n                f\"{BASE_CANDLE_URL}/trade:{time_frame}:{symbol}/{OHLCV_SECTION_LAST}?sort={sort}\",\n                OHLCV_SECTION_LAST\n            )\n    \n    @classmethod\n    def make_tofetch_params(\n            cls,\n            symbol: str,\n            start_date: datetime.datetime,\n            end_date: datetime.datetime,\n            time_frame: str,\n            limit: int,\n            sort: Literal[1, -1]\n        ) -> str:\n        '''\n        Makes tofetch params to feed into Redis to-fetch set\n        \n        :params:\n            `symbol`: symbol string\n            `start_date`: datetime obj\n            `end_date`: datetime obj\n            `time_frame`: string\n            `limit`: int\n            `sort`: int (1 or -1)\n        \n        Example:\n            `BTCUSD;;1000000;;2000000;;1m;;9000;;1`\n        '''\n\n        # Convert start_date and end_date to milliseconds if needed\n        if not isinstance(start_date, int):\n            start_date = datetime_to_milliseconds(start_date)\n        if not isinstance(end_date, int):\n            end_date = datetime_to_milliseconds(end_date)\n        \n        return f'{symbol}{REDIS_DELIMITER}{start_date}{REDIS_DELIMITER}{end_date}{REDIS_DELIMITER}{time_frame}{REDIS_DELIMITER}{limit}{REDIS_DELIMITER}{sort}'\n\n    @classmethod\n    def parse_ohlcvs(\n            cls,\n            ohlcvs: Iterable,\n            base_id: str,\n            quote_id: str,\n            ohlcv_section: str\n        ) -> list:\n        '''\n        Returns a list of rows of parsed ohlcvs\n        \n        Note, in the ohlcv response from Bitfinex, that:\n            - if ohlcv_section is `hist`, ohlcvs will be list of lists\n            - if ohlcv_section is `last`, ohlcvs will be a list\n        \n        :params:\n            `ohlcvs`: iterable of ohlcv data received from an API request\n            `base_id`: string\n            `quote_id`: string\n            `ohlcv_section`: string\n        '''\n\n        # Ignore ohlcvs that are empty, do not raise error,\n        #   as other errors are catched elsewhere\n        ohlcvs_table_insert = []\n        if ohlcvs:\n            if ohlcv_section == OHLCV_SECTION_HIST:\n                ohlcvs_table_insert = [\n                    (\n                        milliseconds_to_datetime(ohlcv[0]),\n                        EXCHANGE_NAME, base_id, quote_id,\n                        round_decimal(ohlcv[1]),\n                        round_decimal(ohlcv[3]),\n                        round_decimal(ohlcv[4]),\n                        round_decimal(ohlcv[2]),\n                        round_decimal(ohlcv[5])\n                    ) for ohlcv in ohlcvs\n                ]\n            else:\n                ohlcvs_table_insert = [\n                    (\n                        milliseconds_to_datetime(ohlcvs[0]),\n                        EXCHANGE_NAME, base_id, quote_id,\n                        round_decimal(ohlcvs[1]),\n                        round_decimal(ohlcvs[3]),\n                        round_decimal(ohlcvs[4]),\n                        round_decimal(ohlcvs[2]),\n                        round_decimal(ohlcvs[5])\n                    )\n                ]\n        return ohlcvs_table_insert\n        # else:\n        #     return None\n\n    @classmethod\n    def make_error_tuple(\n            cls,\n            symbol: str,\n            start_date: datetime.datetime,\n            end_date: datetime.datetime,\n            time_frame: str,\n            ohlcv_section: str,\n            resp_status_code: int,\n            exception_class: str,\n            exception_msg: str\n        ) -> tuple:\n        '''\n        Returns a list that contains: a tuple to insert into the ohlcvs error table\n        \n        :params:\n            `symbol`: string\n            `start_date`: datetime obj of start date\n            `end_date`: datetime obj of end date\n            `time_frame`: string - timeframe\n            `ohlcv_section`: string - historical or recent\n            `resp_status_code`: int - response status code\n            `exception_class`: string\n            `exception_msg`: string\n        '''\n\n        # Convert start_date and end_date to datetime obj if needed;\n        #   as timestamps in Bitfinex are in mls\n        if not isinstance(start_date, datetime.datetime):\n            start_date = milliseconds_to_datetime(start_date)\n        if not isinstance(end_date, datetime.datetime):\n            end_date = milliseconds_to_datetime(end_date)\n\n        return (\n            (EXCHANGE_NAME, symbol, start_date, end_date,\n            time_frame, ohlcv_section, resp_status_code,\n            str(exception_class),exception_msg),\n        )\n\n    @backoff.on_predicate(\n        backoff.constant,\n        lambda result: result[0] == 429,\n        max_tries=12,\n        on_backoff=onbackoff,\n        on_success=onsuccessgiveup,\n        on_giveup=onsuccessgiveup,\n        interval=RATE_LIMIT_SECS_PER_MIN\n    )\n    async def _get_ohlcv_data(\n            self,\n            ohlcv_url: str,\n            throttler: Any=None,\n            exchange_name: str=EXCHANGE_NAME\n        ) -> tuple:\n        '''\n        Gets ohlcv data based on url;\n            Also backoffs conservatively by 60 secs (not sure it works as intended..)\n        \n        Returns a tuple with:\n            - http status (None if there's none),\n            - ohlcvs (None if there's none),\n            - exception type (None if there's none),\n            - error message (None if there's none)\n\n        :params:\n            `ohlcv_url`: string - ohlcvs API url\n            `throttler`: self.rate_limiter\n            `exchange_name`: string - this exchange's name\n        '''\n        \n        retries = 0\n        while retries < HTTPX_DEFAULT_RETRIES:\n            async with self.rate_limiter:\n                try:\n                    ohlcvs_resp = await self.async_httpx_client.get(ohlcv_url)\n                    ohlcvs_resp.raise_for_status()\n                    ohlcv_data = ohlcvs_resp.json()\n                    return (\n                        ohlcvs_resp.status_code,\n                        ohlcv_data,\n                        None,\n                        None\n                    )\n                except httpx.HTTPStatusError as exc:\n                    resp_status_code = exc.response.status_code\n                    return (\n                        resp_status_code,\n                        None,\n                        type(exc),\n                        f'EXCEPTION: Response status code: {resp_status_code} while requesting {exc.request.url}'\n                    )\n                except httpx.TimeoutException as exc:\n                    await asyncio.sleep(1) # for now just 1 sec\n                except Exception as exc:\n                    return (\n                        None,\n                        None,\n                        type(exc),\n                        f'EXCEPTION: Request error while requesting {ohlcv_url}'\n                    )\n            retries += 1\n        return (\n            None,\n            None,\n            MaximumRetriesReached,\n            f'EXCEPTION: Maximum retries reached while requesting {ohlcv_url}'\n        )\n\n    async def _get_and_parse_ohlcv(\n            self,\n            params: str,\n            update: bool=False\n        ) -> Union[str, None]:\n        '''\n        Gets and parses ohlcvs from consumed params\n\n        :params:\n            `params`: params consumed from Redis to-fetch set\n        '''\n          \n        # Extract params\n        params_split = params.split(REDIS_DELIMITER)\n        symbol = params_split[0]\n        start_date_mls = int(params_split[1])\n        end_date_mls = int(params_split[2])\n        time_frame = params_split[3]\n        limit = params_split[4]\n        sort = params_split[5]\n\n        # Construct url and fetch\n        base_id = self.symbol_data[symbol]['base_id']\n        quote_id = self.symbol_data[symbol]['quote_id']\n\n        ohlcv_url, ohlcv_section = self.make_ohlcv_url(\n            time_frame, symbol, limit, start_date_mls, end_date_mls, sort\n        )\n        ohlcv_result = await self._get_ohlcv_data(\n            ohlcv_url, throttler=self.rate_limiter, exchange_name=self.exchange_name\n        )\n        resp_status_code = ohlcv_result[0]\n        ohlcvs = ohlcv_result[1]\n        exc_type = ohlcv_result[2]\n        exception_msg = ohlcv_result[3]\n\n        # If exc_type is None (meaning no exception), process;\n        #   Else, process the error\n        # Why increment start_date_mls by 60000 * OHLCV_LIMIT:\n        #   Because in each request we fetch at least `OHLCV_LIMIT`\n        #   transaction-minutes. Thus, the next transaction-minute must\n        #   be at least 60000 * OHLCV_LIMIT milliseconds away\n        # Copy to PSQL if parsed successfully\n        # Perform update if `update` is True\n        # Get the latest date in OHLCVS list,\n        #   if latest date > start_date, update start_date\n        # Finally, remove params only in 2 cases:\n        #   - insert is successful\n        #   - empty ohlcvs from API\n        if exc_type is None:\n            try:\n                ohlcvs_parsed = self.parse_ohlcvs(\n                    ohlcvs, base_id, quote_id, ohlcv_section)\n                if ohlcvs_parsed:\n                    insert_success = False\n                    if update:\n                        insert_success = psql_bulk_insert(\n                            self.psql_conn,\n                            ohlcvs_parsed,\n                            OHLCVS_TABLE,\n                            insert_update_query = PSQL_INSERT_UPDATE_QUERY,\n                            unique_cols = OHLCV_UNIQUE_COLUMNS,\n                            update_cols = OHLCV_UPDATE_COLUMNS\n                        )\n                    else:\n                        insert_success = psql_bulk_insert(\n                            self.psql_conn,\n                            ohlcvs_parsed,\n                            OHLCVS_TABLE,\n                            insert_ignoredup_query = PSQL_INSERT_IGNOREDUP_QUERY\n                        )\n                    \n                    ohlcvs_last_date = datetime_to_milliseconds(ohlcvs_parsed[-1][0])\n                    if ohlcvs_last_date > start_date_mls:\n                        start_date_mls = ohlcvs_last_date\n                    else:\n                        start_date_mls += (60000 * OHLCV_LIMIT)\n\n                    # Comment this out - not needed atm\n                    # if insert_success:\n                    #     self.redis_client.srem(self.fetching_key, params)\n                    if not insert_success:\n                        exc_type = UnsuccessfulDatabaseInsert\n                        exception_msg = \"EXCEPTION: Unsuccessful database insert\"\n                        error_tuple = self.make_error_tuple(\n                            symbol, start_date_mls, end_date_mls, time_frame, ohlcv_section, resp_status_code, exc_type, exception_msg\n                        )\n                        psql_bulk_insert(\n                            self.psql_conn,\n                            error_tuple,\n                            OHLCVS_ERRORS_TABLE,\n                            insert_ignoredup_query = PSQL_INSERT_IGNOREDUP_QUERY\n                        )\n                else:\n                    start_date_mls += (60000 * OHLCV_LIMIT)\n                    # self.redis_client.srem(self.fetching_key, params) # not needed atm\n            \n            except Exception as exc:\n                exc_type = type(exc)\n                exception_msg = f'EXCEPTION: Error while processing ohlcv response: {exc}'\n                self.logger.warning(exception_msg)\n                error_tuple = self.make_error_tuple(\n                    symbol, start_date_mls, end_date_mls, time_frame, ohlcv_section, resp_status_code, exc_type, exception_msg\n                )\n                psql_bulk_insert(\n                    self.psql_conn,\n                    error_tuple,\n                    OHLCVS_ERRORS_TABLE,\n                    insert_ignoredup_query = PSQL_INSERT_IGNOREDUP_QUERY\n                )\n                start_date_mls += (60000 * OHLCV_LIMIT)\n        else:\n            self.logger.warning(exception_msg)\n            error_tuple = self.make_error_tuple(\n                symbol, start_date_mls, end_date_mls, time_frame, ohlcv_section, resp_status_code, exc_type, exception_msg\n            )\n            psql_bulk_insert(\n                self.psql_conn,\n                error_tuple,\n                OHLCVS_ERRORS_TABLE,\n                insert_ignoredup_query = PSQL_INSERT_IGNOREDUP_QUERY\n            )\n            start_date_mls += (60000 * OHLCV_LIMIT)\n        \n        # PSQL Commit\n        self.psql_conn.commit()\n\n        # Also make more params for to-fetch set\n        if start_date_mls < end_date_mls:\n            return self.make_tofetch_params(\n                symbol, start_date_mls, end_date_mls, time_frame, limit, sort\n            )\n        else:\n            return None\n\n    async def _init_tofetch_redis(\n            self,\n            symbols: Iterable,\n            start_date: datetime.datetime,\n            end_date: datetime.datetime,\n            time_frame: str,\n            limit: int,\n            sort: Literal[1, -1]\n        ) -> None:\n        '''\n        Initializes feeding params to Redis to-fetch set\n        \n        :params:\n            `symbols`: iterable of symbols\n            `start_date`: datetime obj\n            `end_date`: datetime obj\n            `time_frame`: string\n            `limit`: int\n            `sort`: int (1 or -1)\n        \n        Feeds the following information:\n            - key: `self.tofetch_key`\n            - value: `symbol;;start_date_mls;;end_date_mls;;time_frame;;limit;;sort`\n        \n        example:\n            `BTCUSD;;1000000;;2000000;;1m;;9000;;1`\n        '''\n\n        # Set feeding status\n        # Convert datetime with tzinfo to non-tzinfo, if any\n        self.feeding = True\n        start_date = start_date.replace(tzinfo=None)\n        end_date = end_date.replace(tzinfo=None)\n        start_date_mls = datetime_to_milliseconds(start_date)\n        end_date_mls = datetime_to_milliseconds(end_date)\n\n        # The maximum list of symbols is short enough to feed\n        #   to Redis in a batch (~300 symbols total as of June 2021)\n        # Finally reset feeding status\n        params_list = [\n            self.make_tofetch_params(\n                symbol, start_date_mls, end_date_mls, time_frame, limit, sort\n            ) for symbol in symbols\n        ]\n        self.redis_client.sadd(self.tofetch_key, *params_list)\n        self.feeding = False\n        self.logger.info(\"Redis: Successfully initialized feeding params\")\n\n    async def _consume_ohlcvs_redis(self, update: bool=False) -> None:\n        '''\n        Consumes OHLCV parameters from Redis to-fetch set\n        '''\n\n        # When start, move all [existing] params from fetching set to to-fetch set\n        # Keep looping and processing in batch if either:\n        #   - self.feeding or\n        #   - there are elements in to-fetch set or fetching set\n        fetching_params = self.redis_client.spop(\n            self.fetching_key,\n            self.redis_client.scard(self.fetching_key)\n        )\n        if fetching_params:\n            self.redis_client.sadd(\n                self.tofetch_key, *fetching_params\n            )\n        \n        # Pop a batch of size `rate_limit` from Redis to-fetch set,\n        #   send it to Redis fetching set\n        # Add params in params list to Redis fetching set\n        # New to-fetch params with new start dates will be results\n        #   of `get_parse_tasks`\n        #   Add these params to Redis to-fetch set, if not None\n        # Finally, remove params list from Redis fetching set\n        async with httpx.AsyncClient(\n            timeout=self.httpx_timout, limits=self.httpx_limits) as client:\n            self.async_httpx_client = client\n            while self.feeding or \\\n                self.redis_client.scard(self.tofetch_key) > 0 \\\n                or self.redis_client.scard(self.fetching_key) > 0:\n                params_list = self.redis_client.spop(\n                    self.tofetch_key, OHLCVS_CONSUME_BATCH_SIZE\n                )\n                if params_list:\n                    self.redis_client.sadd(self.fetching_key, *params_list)\n                    get_parse_tasks = [\n                        self._get_and_parse_ohlcv(params, update) for params in params_list\n                    ]\n                    task_results = await asyncio.gather(*get_parse_tasks)\n                    new_tofetch_params = [\n                        params for params in task_results if params is not None\n                    ]\n                    if new_tofetch_params:\n                        self.logger.info(\n                            \"Redis: Adding more params to to-fetch with new start dates\")\n                        self.redis_client.sadd(\n                            self.tofetch_key, *new_tofetch_params)\n\n                    self.redis_client.srem(self.fetching_key, *params_list)\n    \n    async def _fetch_ohlcvs_symbols(\n            self,\n            symbols: list,\n            start_date_dt: datetime.datetime,\n            end_date_dt: datetime.datetime,\n            update: bool=False\n        ) -> None:\n        '''\n        Function to fetch OHLCVs of symbols\n        \n        :params:\n            `symbols`: list of symbol string\n            `start_date_dt`: datetime obj - for start date\n            `end_date_dt`: datetime obj - for end date\n            `update`: boolean - whether to update when inserting\n                to PSQL db\n        '''\n\n        # Set feeding status so the consume\n        # function does not close immediately\n        self.feeding = True\n\n        # Asyncio gather 2 tasks:\n        # - Init to-fetch\n        # - Consume from Redis to-fetch\n        await asyncio.gather(\n            self._init_tofetch_redis(\n                symbols, start_date_dt, end_date_dt, OHLCV_TIMEFRAME, OHLCV_LIMIT, 1\n            ),\n            self._consume_ohlcvs_redis(update)\n        )\n"}
{"type": "source_file", "path": "common/utils/logutils.py", "content": "import logging\nimport inspect\n\n\ndef create_logger(\n    logger_name: str,\n    log_level: int = logging.INFO,\n    stream_handler: bool = True,\n    log_filename: str = None,\n    **kwargs\n):\n    '''\n    Creates a Logger with default level at INFO;\n    The default handler is StreamHandler - the only other option is FileHandler\n\n    :params:\n        @log_filename: full path to log filename\n        @kwargs: additional keyword arguments\n    '''\n\n    check_file_handler = \\\n        (stream_handler is False or stream_handler is None) and log_filename is None\n    if check_file_handler:\n        raise ValueError(\n            \"log_filename must be provided if stream handler is not selected\"\n        )\n\n    logger = logging.getLogger(logger_name)\n    logger.setLevel(log_level)\n\n    if stream_handler:\n        log_handler = logging.StreamHandler()\n    else:\n        # Look into the kwargs provided above, and grab any arg\n        # that FileHandler object uses\n        file_handler_kws = {\n            key: value for key, value in kwargs.items()\n            if key in (inspect.getfullargspec(logging.FileHandler).args)\n        }\n        log_handler = logging.FileHandler(log_filename, **file_handler_kws)\n    log_handler.setLevel(log_level)\n    log_formatter = logging.Formatter(\n        '%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n    log_handler.setFormatter(log_formatter)\n\n    logger.addHandler(log_handler)\n    return logger\n"}
{"type": "source_file", "path": "web/config/constants.py", "content": "PSQL_BATCHSIZE = 10000\nOHLCV_INTERVALS = [\n    '1m', '5m', '15m', '30m', '1h', '3h', '6h', '12h', '1D', '7D', '14D', '1M'\n]\nWS_SEND_EVENT_TYPES = [\"subscribe\", \"unsubscribe\"]\n"}
{"type": "source_file", "path": "fetchers/rest/santiment.py", "content": "# This module contains fetching for santiment[bitfinex] API\n\nBASE_PUB_API = \"https://api-pub.bitfinex.com/v2/dir/query\"\n"}
{"type": "source_file", "path": "fetchers/ws/binance.py", "content": "### This module uses websocket to fetch binance 1-minute OHLCV data in real time\n\nimport random\nimport asyncio\nimport json\nimport redis\nimport websockets\nfrom typing import Iterable, NoReturn\nfrom common.config.constants import (\n    REDIS_HOST, REDIS_USER,\n    REDIS_PASSWORD, REDIS_DELIMITER\n)\nfrom common.utils.logutils import create_logger\nfrom fetchers.config.constants import WS_SUB_LIST_REDIS_KEY\nfrom fetchers.config.queries import MUTUAL_BASE_QUOTE_QUERY\nfrom fetchers.rest.binance import BinanceOHLCVFetcher, EXCHANGE_NAME\nfrom fetchers.utils.exceptions import (\n    UnsuccessfulConnection, ConnectionClosed, InvalidStatusCode\n)\nfrom fetchers.helpers.ws import (\n    make_sub_val,\n    make_sub_redis_key,\n    make_serve_redis_key\n)\n\n\n# Binance only allows up to 1024 subscriptions per ws connection\n#   However, so far only a max value of 200 works...\nURI = \"wss://stream.binance.com:9443/ws\"\nMAX_SUB_PER_CONN = 200\nBACKOFF_MIN_SECS = 2.0\nBACKOFF_MAX_SECS = 60.0\n\nclass BinanceOHLCVWebsocket:\n    '''\n    Binance OHLCV websocket fetcher\n    '''\n\n    def __init__(\n        self, log_to_stream: bool = False, log_filename: str = None\n    ):\n        check_log_file = log_to_stream is False and log_filename is None\n        if check_log_file:\n            raise ValueError(\n                \"log_filename must be provided if not logging to stream\"\n            )\n\n        self.redis_client = redis.Redis(\n            host=REDIS_HOST,\n            username=REDIS_USER,\n            password=REDIS_PASSWORD,\n            decode_responses=True\n        )\n\n        # Rest fetcher for convenience\n        self.rest_fetcher = BinanceOHLCVFetcher()\n\n        # Logging\n        self.logger = create_logger(\n            f'{EXCHANGE_NAME}_websocket',\n            stream_handler=log_to_stream,\n            log_filename=log_filename\n        )\n\n        # Backoff\n        # This backoff delay will be increased\n        #   when a connection is unsuccessful\n        self.backoff_delay = BACKOFF_MIN_SECS\n\n    async def subscribe(self, symbols: Iterable, i: int = 0) -> NoReturn:\n        '''\n        Subscribes to Binance WS for `symbols`\n\n        :params:\n            `symbols`: list of symbols (not tsymbol)\n                e.g., [ETHBTC]\n        '''\n\n        while True:\n            try:\n                async with websockets.connect(URI) as ws:\n                    # Binance requires WS symbols to be lowercase\n                    params = [\n                        f'{symbol.lower()}@kline_1m'\n                        for symbol in symbols\n                    ]\n                    await ws.send(json.dumps(\n                        {\n                            \"method\": \"SUBSCRIBE\",\n                            \"params\": params,\n                            \"id\": i\n                        })\n                    )\n                    self.logger.info(f\"Connection {i}: Successful\")\n                    self.backoff_delay = BACKOFF_MIN_SECS\n                    while True:\n                        resp = await ws.recv()\n                        respj = json.loads(resp)\n\n                        if isinstance(respj, dict):\n                            if 'result' in respj:\n                                if respj['result'] is not None:\n                                    raise UnsuccessfulConnection\n                            else:\n                                try:\n                                    symbol = respj['s']\n                                    timestamp = int(respj['k']['t'])\n                                    open_ = respj['k']['o']\n                                    high_ = respj['k']['h']\n                                    low_ = respj['k']['l']\n                                    close_ = respj['k']['c']\n                                    volume_ = respj['k']['v']\n                                    base_id = self.rest_fetcher.symbol_data[symbol]['base_id']\n                                    quote_id = self.rest_fetcher.symbol_data[symbol]['quote_id']\n\n                                    # We make \"sub'ed value\" by serializing the price\n                                    #   data so it can be put into Redis\n                                    # Then, unique sub'ed and serve Redis keys\n                                    #   are created to store the above sub'ed value\n                                    # Also note here that the value put into the sub_\n                                    #   and the serve_ keys are different:\n                                    #   >> sub_ key is a hash with its [internal]\n                                    #   keys as timestamps; this is so that the updater\n                                    #   script can bulk-insert all the price data in\n                                    #   the sub_ key to PSQL later, meanwhile...\n                                    #   >> serve_ key is a hash with its [internal]\n                                    #   keys as timestamp, o, h, l, c, v; this is\n                                    #   because the web service (a.k.a. FastAPI) only\n                                    #   serves to the user [via its websocket connection]\n                                    #   the latest price data, meaning when any new data\n                                    #   comes in from the exchange's ws API,\n                                    #   the hash gets updated\n\n                                    sub_val = make_sub_val(\n                                        timestamp,\n                                        open_, high_, low_, close_, volume_,\n                                        REDIS_DELIMITER\n                                    )\n                                    ws_sub_redis_key = make_sub_redis_key(\n                                        EXCHANGE_NAME,\n                                        base_id,\n                                        quote_id,\n                                        REDIS_DELIMITER\n                                    )\n                                    ws_serve_redis_key = make_serve_redis_key(\n                                        EXCHANGE_NAME,\n                                        base_id,\n                                        quote_id,\n                                        REDIS_DELIMITER\n                                    )\n\n                                    # Add ws sub key to set of all ws sub keys\n                                    # Set hash value for ws sub key\n                                    # Replace ws serve key hash if this timestamp\n                                    #   is more up-to-date\n                                    self.redis_client.sadd(\n                                        WS_SUB_LIST_REDIS_KEY, ws_sub_redis_key)\n                                    self.redis_client.hset(\n                                        ws_sub_redis_key, timestamp, sub_val)\n                                    current_timestamp = self.redis_client.hget(\n                                        ws_serve_redis_key, 'time')\n                                    if current_timestamp is None or \\\n                                        timestamp >= int(current_timestamp):\n                                        self.redis_client.hset(\n                                            ws_serve_redis_key,\n                                            mapping = {\n                                                'time': timestamp,\n                                                'open': open_,\n                                                'high': high_,\n                                                'low': low_,\n                                                'close': close_,\n                                                'volume': volume_\n                                            }\n                                        )\n                                except Exception as exc:\n                                    self.logger.warning(\n                                        f\"Binance WS Fetcher: EXCEPTION: {exc}\")\n\n                        # Sleep to release event loop\n                        await asyncio.sleep(0.01)\n            except (ConnectionClosed, InvalidStatusCode) as exc:\n                self.logger.warning(\n                    f\"Connection {i} raised exception: {exc} - reconnecting...\"\n                )\n                await asyncio.sleep(min(self.backoff_delay, BACKOFF_MAX_SECS))\n                self.backoff_delay *= (1+random.random()) # add a random factor\n\n    async def mutual_basequote(self) -> None:\n        '''\n        Subscribes to all base-quote's that are available\n        in all exchanges\n        '''\n\n        symbols_dict = self.rest_fetcher.get_symbols_from_exch(MUTUAL_BASE_QUOTE_QUERY)\n        self.rest_fetcher.close_connections()\n        await asyncio.gather(self.subscribe(symbols_dict.keys()))\n\n    async def all(self) -> None:\n        '''\n        Subscribes to WS channels of all symbols\n        '''\n\n        self.rest_fetcher.fetch_symbol_data()\n        symbols =  tuple(self.rest_fetcher.symbol_data.keys())\n\n        # Subscribe to `MAX_SUB_PER_CONN` per connection (e.g., 200)\n        await asyncio.gather(\n            *(\n                self.subscribe(symbols[i:i+MAX_SUB_PER_CONN], int(i/MAX_SUB_PER_CONN))\n                    for i in range(0, len(symbols), MAX_SUB_PER_CONN)\n            )\n        )\n\n    def run_mutual_basequote(self) -> None:\n        '''\n        API to run the `mutual base-quote` method\n        '''\n\n        asyncio.run(self.mutual_basequote())\n\n    def run_all(self) -> None:\n        '''\n        API to run the `all` method\n        '''\n\n        asyncio.run(self.all())\n"}
{"type": "source_file", "path": "scripts/fetchers/ws.py", "content": "import argparse\nfrom fetchers.ws.bitfinex import BitfinexOHLCVWebsocket\nfrom fetchers.ws.binance import BinanceOHLCVWebsocket\nfrom fetchers.ws.bittrex import BittrexOHLCVWebsocket\nfrom fetchers.ws.updater import OHLCVWebsocketUpdater\n\n\n# Create the parser\narg_parser = argparse.ArgumentParser(\n    prog=\"python -m scripts.fetchers.ws\",\n    description=\"Starts a websocket fetcher for an exchange or an updater\"\n)\n\n# Add the arguments\narg_parser.add_argument(\n    'action',\n    metavar='action',\n    type=str,\n    choices=[\"fetch\", \"update\"],\n    help='fetch (for an exchange) or update (collect fetched data to db)'\n)\n\narg_parser.add_argument(\n    '--exchange',\n    metavar='exchange',\n    type=str,\n    help='name of the exchange; Only needed if action is fetch'\n)\n\narg_parser.add_argument(\n    '--log_filename',\n    metavar='log_filename',\n    type=str,\n    help='full path to the log filename'\n)\n\n# Execute the parse_args() method\nargs = arg_parser.parse_args()\naction = args.action\nexchange = args.exchange\nlog_filename = args.log_filename\nif action == \"fetch\":\n    if exchange == \"bitfinex\":\n        ws = BitfinexOHLCVWebsocket(log_filename=log_filename)\n    elif exchange == \"binance\":\n        ws = BinanceOHLCVWebsocket(log_filename=log_filename)\n    elif exchange == \"bittrex\":\n        ws = BittrexOHLCVWebsocket(log_filename=log_filename)\n    ws.run_all()\nelif action == \"update\":\n    ws = OHLCVWebsocketUpdater(log_filename=log_filename)\n    ws.update()\n"}
{"type": "source_file", "path": "celery_app/celery_config.py", "content": "# This module contains settings for Celery app\n\nfrom common.config.constants import CELERY_REDIS_URL\n\n# Broker settings, trying Redis\nbroker_url = CELERY_REDIS_URL\n\n# List of modules to import when the Celery worker starts\ninclude = ['celery_app.celery_tasks']\n\n# Using the database to store task state\nresult_backend = CELERY_REDIS_URL\n\n# Enabling worker pool restart\nworker_pool_restarts = True\n\n# Routing\ntask_routes = {\n    'celery_app.celery_tasks.bitfinex_fetch_ohlcvs_all_symbols': {\n        'queue': 'bitfinex_rest'\n    },\n    'celery_app.celery_tasks.bitfinex_fetch_ohlcvs_symbols': {\n        'queue': 'bitfinex_rest'\n    },\n    'celery_app.celery_tasks.bitfinex_resume_fetch': {\n        'queue': 'bitfinex_rest'\n    },\n    'celery_app.celery_tasks.bitfinex_fetch_ohlcvs_mutual_basequote': {\n        'queue': 'bitfinex_rest'\n    },\n    'celery_app.celery_tasks.bitfinex_fetch_ohlcvs_mutual_basequote_1min': {\n        'queue': 'bitfinex_rest'\n    },\n    'celery_app.celery_tasks.binance_fetch_ohlcvs_all_symbols': {\n        'queue': 'binance_rest'\n    },\n    'celery_app.celery_tasks.binance_fetch_ohlcvs_symbols': {\n        'queue': 'binance_rest'\n    },\n    'celery_app.celery_tasks.binance_resume_fetch': {\n        'queue': 'binance_rest'\n    },\n    'celery_app.celery_tasks.binance_fetch_ohlcvs_mutual_basequote': {\n        'queue': 'binance_rest'\n    },\n    'celery_app.celery_tasks.binance_fetch_ohlcvs_mutual_basequote_1min': {\n        'queue': 'binance_rest'   \n    },\n    'celery_app.celery_tasks.bittrex_fetch_ohlcvs_all_symbols': {\n        'queue': 'bittrex_rest'\n    },\n    'celery_app.celery_tasks.bittrex_fetch_ohlcvs_symbols': {\n        'queue': 'bittrex_rest'\n    },\n    'celery_app.celery_tasks.bittrex_resume_fetch': {\n        'queue': 'bittrex_rest'\n    },\n    'celery_app.celery_tasks.bittrex_fetch_ohlcvs_mutual_basequote': {\n        'queue': 'bittrex_rest'\n    },\n    'celery_app.celery_tasks.bittrex_fetch_ohlcvs_mutual_basequote_1min': {\n        'queue': 'bittrex_rest'\n    },\n    'celery_app.celery_tasks.all_fetch_symbol_data': {\n        'queue': 'all_rest'\n    }\n}"}
{"type": "source_file", "path": "fetchers/utils/exceptions.py", "content": "from websockets.exceptions import (\n    ConnectionClosed,\n    ConnectionClosedOK,\n    InvalidStatusCode\n)\n\n\nclass UnsuccessfulConnection(Exception):\n    '''\n    Unsuccessful connection to an endpoint (e.g., ws)\n    '''\n    pass\n\nclass MaximumRetriesReached(Exception):\n    '''\n    Maximum retries reached while fetching an endpoint\n    '''\n    pass\n\nclass UnsuccessfulDatabaseInsert(Exception):\n    '''\n    Unsuccessful insert to db\n    '''\n    pass\n"}
{"type": "source_file", "path": "fetchers/utils/ratelimit.py", "content": "import redis\nimport asyncio\nimport random\nimport time\nfrom redis.exceptions import LockError\nfrom common.config.constants import REDIS_HOST, REDIS_USER, REDIS_PASSWORD\nfrom common.helpers.datetimehelpers import microseconds_to_seconds\nfrom fetchers.config.constants import REST_RATE_LIMIT_REDIS_KEY\n\n\nLOCK_TIMEOUT_SECS = 5\n\n\nclass GCRARateLimiter:\n    '''\n    Client-side request rate-limiter using the GCRA algorithm with Redis\n\n    Applicable for multiple instances of a requesting object (e.g., a fetcher)\n        sharing the same Redis rate-limiter key\n\n    See GCRA explanation: https://blog.ian.stapletoncordas.co/2018/12/understanding-generic-cell-rate-limiting.html\n    '''\n    \n    def __init__(\n        self,\n        rate_limit_key: str,\n        rate_limit: float,\n        period: float,\n        redis_client: redis.Redis = None\n    ):\n        '''\n        :params:\n            `rate_limit_key`: unique key for this rate limiter\n            `rate_limit`:\n            `period`:\n            `redis_client`:\n        '''\n    \n        if not redis_client:\n            redis_client = redis.Redis(\n                host=REDIS_HOST,\n                username=REDIS_USER,\n                password=REDIS_PASSWORD,\n                decode_responses=True\n            )\n        self.redis_client = redis_client\n        self.key = rate_limit_key\n        self.rate_limit = rate_limit\n        self.period = period\n        self.increment = self.period / self.rate_limit\n   \n    def _is_limited(self):\n        '''\n        Checks if the requesting function is rate-limited\n\n        Source: https://dev.to/astagi/rate-limiting-using-python-and-redis-58gk\n        '''\n\n        secs, mics = self.redis_client.time()\n        t = int(secs) + microseconds_to_seconds(float(mics))\n        try:\n            with self.redis_client.lock(\n                f'lock:{self.key}',\n                timeout=LOCK_TIMEOUT_SECS,\n                blocking_timeout=0.01\n            ) as lock:\n                self.redis_client.setnx(self.key, t)\n                tat = max(float(self.redis_client.get(self.key)), t)\n                allowed_at = tat + self.increment - self.period\n                if t >= allowed_at:\n                    new_tat = tat + self.increment\n                    self.redis_client.set(self.key, new_tat)\n                    return (False, None)\n                return (True, allowed_at - t)\n        except LockError:\n            return (True, self.increment)\n        except Exception as exc:\n            print(f\"GCRARateLimiter: EXCEPTION: {exc}\")\n\n    async def wait(self):\n        '''\n        API call to wait until the requesting function is not rate-limited\n        '''\n\n        while True:\n            limited, retry_after = self._is_limited()\n            if not limited:\n                break\n            await asyncio.sleep(retry_after)\n        \n    async def __aenter__(self):\n        await self.wait()\n\n    async def __aexit__(self, exc_type, exc, tb):\n        pass\n\nclass AsyncThrottler:\n    '''\n    An asyncio throttler using Redis\n\n    Based on: https://github.com/hallazzang/asyncio-throttle\n    '''\n    \n    def __init__(\n        self,\n        rate_limit_key: str,\n        rate_limit: int,\n        period: float,\n        retry_interval: float = 0.01,\n        redis_client: redis.Redis = None\n    ):\n        if not redis_client:\n            redis_client = redis.Redis(\n                host=REDIS_HOST,\n                username=REDIS_USER,\n                password=REDIS_PASSWORD,\n                decode_responses=True\n            )\n        self.redis_client = redis_client\n        self.key = rate_limit_key\n        self.rate_limit = rate_limit\n        self.period = period\n        self.increment = period / rate_limit\n        self.retry_interval = retry_interval\n\n    def flush(self):\n        secs, mics = self.redis_client.time()\n        now = int(secs) + microseconds_to_seconds(float(mics))\n        try:\n            with self.redis_client.lock(\n                f'lock:{self.key}',\n                timeout=LOCK_TIMEOUT_SECS,\n                blocking_timeout=0.01\n            ) as lock:\n                while int(self.redis_client.llen(self.key)) > 0:\n                    if now - float(self.redis_client.lindex(self.key, 0)) > self.period:\n                        self.redis_client.lpop(self.key)\n                    else:\n                        break\n        except Exception:\n            pass\n\n    async def acquire(self):\n        while True:\n            self.flush()\n            if int(self.redis_client.llen(self.key)) < self.rate_limit:\n                break\n            await asyncio.sleep(self.retry_interval)\n        \n        secs, mics = self.redis_client.time()\n        now = int(secs) + microseconds_to_seconds(float(mics))\n        self.redis_client.rpush(self.key, now)\n\n    async def __aenter__(self):\n        await self.acquire()\n\n    async def __aexit__(self, exc_type, exc, tb):\n        pass\n"}
{"type": "source_file", "path": "fetchers/rest/binance.py", "content": "### This module fetches binance 1-minute OHLCV data\n\nimport asyncio\nimport datetime\nimport logging\nimport random\nimport time\nfrom typing import Iterable, Tuple, Union\n\nimport httpx\nimport redis\nfrom redis.exceptions import LockError\n\nfrom common.config.constants import \\\n    OHLCVS_ERRORS_TABLE, OHLCVS_TABLE, \\\n    REDIS_DELIMITER, REDIS_HOST, \\\n    REDIS_PASSWORD, REDIS_USER\nfrom common.helpers.datetimehelpers import \\\n    datetime_to_milliseconds, \\\n    milliseconds_to_datetime, redis_time\nfrom common.helpers.numbers import round_decimal\nfrom fetchers.config.constants import \\\n    HTTPX_DEFAULT_RETRIES, OHLCV_UNIQUE_COLUMNS, \\\n    OHLCV_UPDATE_COLUMNS, REST_RATE_LIMIT_REDIS_KEY, \\\n    THROTTLER_RATE_LIMITS\nfrom fetchers.config.queries import \\\n    PSQL_INSERT_IGNOREDUP_QUERY, PSQL_INSERT_UPDATE_QUERY\nfrom fetchers.helpers.dbhelpers import psql_bulk_insert\nfrom fetchers.rest.base import BaseOHLCVFetcher\nfrom fetchers.utils.exceptions import \\\n    MaximumRetriesReached, UnsuccessfulDatabaseInsert\nfrom fetchers.utils.ratelimit import GCRARateLimiter\n\nURL = \"https://api.binance.com/api/v3/klines?symbol=BTCTUSD&interval=1m&startTime=1357020000000&limit=1000\"\n\nEXCHANGE_NAME = \"binance\"\nBASE_URL = \"https://api.binance.com/api/v3\"\nBASE_URL_1 = \"https://api1.binance.com/api/v3\"\nBASE_URL_2 = \"https://api2.binance.com/api/v3\"\nBASE_URL_3 = \"https://api3.binance.com/api/v3\"\nOHLCV_TIMEFRAME = \"1m\"\nOHLCV_LIMIT = 1000\nDEFAULT_WEIGHT_LIMIT = 1200\nRATE_LIMIT_HITS_PER_MIN = THROTTLER_RATE_LIMITS['RATE_LIMIT_HITS_PER_MIN'][EXCHANGE_NAME]\nRATE_LIMIT_SECS_PER_MIN = THROTTLER_RATE_LIMITS['RATE_LIMIT_SECS_PER_MIN']\nBACKOFF_STT_REDIS = \"backoff_stt_binance\" # Common Backoff status\nBACKOFF_URL_REDIS = \"backoff_url_binance\" # Common Backoff url\nBACKOFF_TIME_REDIS = \"backoff_time_binance\" # Common Backoff time\nBACKOFF_DUR_REDIS = \"backoff_dur_binance\" # Common Backoff duration\n\nOHLCVS_CONSUME_BATCH_SIZE = 500\n# At httpx concurrent limit of 200, lag bug seems to be gone\n\nLOCK_TIMEOUT_SECS = 5\n\nclass RequestWeightManager:\n    '''\n    Request weight manager specifically for Binance\n\n    Uses Redis to manage request weight\n    '''\n    \n    def __init__(\n        self,\n        weight_limit: int,\n        period: int,\n        redis_client: redis.Redis=None\n    ):\n        self.full_weight_limit = weight_limit\n        self.period = period\n        self.key_ts = f'weight_limit_timestamp_{EXCHANGE_NAME}'\n        self.key_rw = f'weight_limit_value_{EXCHANGE_NAME}'\n\n        # Redis client\n        if not redis_client:\n            redis_client = redis.Redis(\n                host=REDIS_HOST,\n                username=REDIS_USER,\n                password=REDIS_PASSWORD,\n                decode_responses=True\n            )\n        self.redis_client = redis_client\n    \n    def _is_enough(self, weight: int) -> tuple:\n        '''\n        Checks if the request weight poll has enough for\n            operation with `weight`\n        '''\n        \n        # logging.info(f\"Current request weight is {self.redis_client.get(self.key_rw)}\")\n        now = redis_time(self.redis_client)\n\n        try:\n            with self.redis_client.lock(\n                f'lock:{self.key_ts}',\n                timeout=LOCK_TIMEOUT_SECS,\n                blocking_timeout=0.01\n            ) as lock:\n                # Initialize\n                self.redis_client.setnx(self.key_ts, now)\n                self.redis_client.setnx(self.key_rw, self.full_weight_limit)\n                \n                # Reset timestamp and request weight if `period` of time\n                #   has passed since last timestamp\n                if now - float(self.redis_client.get(self.key_ts)) > self.period:\n                    self.redis_client.set(self.key_ts, now)\n                    self.redis_client.set(self.key_rw, self.full_weight_limit)\n                \n                # Check if there is enough weight\n                request_weight = int(self.redis_client.get(self.key_rw))\n                if request_weight >= weight:\n                    self.redis_client.decrby(self.key_rw, weight)\n                    return (True, None)\n                else:\n                    return (\n                        False,\n                        self.period - (now - float(self.redis_client.get(self.key_ts)))\n                    )\n        except LockError:\n            return (\n                False,\n                self.period - (now - float(self.redis_client.get(self.key_ts)))\n            )\n        except Exception as exc:\n            logging.warning(f\"RequestWeightManager: EXCEPTION: {exc}\")\n\n    def _wait(self, weight: int) -> None:\n        while True:\n            enough, wait_time = self._is_enough(weight)\n            if enough:\n                break\n            time.sleep(wait_time)\n\n    async def _await(self, weight: int) -> None:\n        while True:\n            enough, wait_time = self._is_enough(weight)\n            if enough:\n                break\n            await asyncio.sleep(wait_time)\n\n    def check(self, weight: int) -> None:\n        '''\n        To be inserted at the very beginning of a request function\n            that costs `weight` weight units (non-async)\n        '''\n\n        self._wait(weight)\n    \n    async def acheck(self, weight: int) -> None:\n        '''\n        To be inserted at the very beginning of a request function\n            that costs `weight` weight units\n        '''\n\n        await self._await(weight)\n\n\nclass BinanceOHLCVFetcher(BaseOHLCVFetcher):\n    '''REST Fetcher for OHLCV from Binance\n    '''\n    def __init__(self, *args):\n        super().__init__(*args, exchange_name = EXCHANGE_NAME)\n\n        # Request weight manager\n        self.rw_manager = RequestWeightManager(\n            DEFAULT_WEIGHT_LIMIT,\n            RATE_LIMIT_SECS_PER_MIN,\n            self.redis_client\n        )\n\n        # Rate limiter\n        self.rate_limiter = GCRARateLimiter(\n            REST_RATE_LIMIT_REDIS_KEY.format(exchange = EXCHANGE_NAME),\n            1,\n            RATE_LIMIT_SECS_PER_MIN / RATE_LIMIT_HITS_PER_MIN,\n            redis_client = self.redis_client\n        )\n\n        # Load market data\n        self._load_symbol_data()\n\n    def _load_symbol_data(self) -> None:\n        '''\n        Loads market data into a dict of this form:\n            {\n                'ETHBTC': {\n                    'base_id': \"ETH\",\n                    'quote_id': \"BTC\"\n                },\n                'some_other_symbol': {\n                    'base_id': \"ABC\",\n                    'quote_id': \"XYZ\"\n                }\n                ...\n            }\n        \n        Saves it in self.symbol_data\n        '''\n\n        # Only needs a temporary httpx client\n        # This code can block (non-async) as it's needed for future fetching\n        # Only processes trading symbols\n        # self.symbol_data = {}\n        with httpx.Client(timeout=None) as client:\n            self.rw_manager.check(10)\n            exch_info_resp = client.get(f'{BASE_URL}/exchangeInfo') \\\n                or client.get(f'{BASE_URL_1}/exchangeInfo') \\\n                or client.get(f'{BASE_URL_2}/exchangeInfo') \\\n                or client.get(f'{BASE_URL_3}/exchangeInfo')\n            if exch_info_resp:\n                symbol_info = exch_info_resp.json()['symbols']\n            for symbol_dict in symbol_info:\n                if symbol_dict['status'] == \"TRADING\":\n                    self.symbol_data[symbol_dict['symbol']] = {\n                        'base_id': symbol_dict['baseAsset'],\n                        'quote_id': symbol_dict['quoteAsset']\n                    }\n\n    @classmethod\n    def make_ohlcv_url(\n            cls,\n            interval: str,\n            symbol: str,\n            limit: int,\n            start_date_mls: int\n        ) -> Tuple[str, str, str, str]:\n        '''\n        Returns tuple of OHLCV url options\n\n        :params:\n            `interval`: string - interval, e.g., 1m\n            `symbol`: string - trading symbol, e.g., BTCTUSD\n            `limit`: int - number limit of results fetched\n            `start_date_mls`: int - datetime obj converted into milliseconds\n\n        Note that binance does not distinguish historical url or not\n\n        example: https://api.binance.com/api/v3/klines?symbol=BTCTUSD&interval=1m&startTime=1357020000000&limit=1000\n        '''\n\n        return (\n            f\"{BASE_URL}/klines?symbol={symbol}&interval={interval}&startTime={start_date_mls}&limit={limit}\",\n            f\"{BASE_URL_1}/klines?symbol={symbol}&interval={interval}&startTime={start_date_mls}&limit={limit}\",\n            f\"{BASE_URL_2}/klines?symbol={symbol}&interval={interval}&startTime={start_date_mls}&limit={limit}\",\n            f\"{BASE_URL_3}/klines?symbol={symbol}&interval={interval}&startTime={start_date_mls}&limit={limit}\"\n        )\n    \n    @classmethod\n    def make_tofetch_params(\n            cls,\n            symbol: str,\n            start_date_mls: int,\n            end_date_mls: int,\n            interval: str,\n            limit: int\n        ) -> str:\n        '''\n        Makes tofetch params to feed into Redis to-fetch set\n        \n        :params:\n            `symbol`: string - symbol\n            `start_date_mls`: int - datetime in millisecs\n            `end_date_mls`: int - datetime in millisecs\n            `interval`: string - time interval\n            `limit`: int - limit of data points\n        example:\n            'BTCTUSD;;1000000;;2000000;;1m;;1000'\n        '''\n\n        return f'{symbol}{REDIS_DELIMITER}{start_date_mls}{REDIS_DELIMITER}{end_date_mls}{REDIS_DELIMITER}{interval}{REDIS_DELIMITER}{limit}'\n\n    @classmethod\n    def parse_ohlcvs(\n            cls,\n            ohlcvs: Iterable,\n            base_id: str,\n            quote_id: str\n        ) -> list:\n        '''\n        Returns a list of rows of parsed ohlcvs\n        \n        :params:\n            `ohlcvs`: iterable of ohlcv dicts (returned from request)\n            `base_id`: string\n            `quote_id`: string\n        '''\n\n        # Ignore ohlcvs that are empty, do not raise error,\n        #   as other errors are catched elsewhere\n        ohlcvs_table_insert = []\n        if ohlcvs:\n            ohlcvs_table_insert = [\n                (\n                    milliseconds_to_datetime(ohlcv[0]),\n                    EXCHANGE_NAME, base_id, quote_id,\n                    round_decimal(ohlcv[1]),\n                    round_decimal(ohlcv[2]),\n                    round_decimal(ohlcv[3]),\n                    round_decimal(ohlcv[4]),\n                    round_decimal(ohlcv[5])\n                ) for ohlcv in ohlcvs\n            ]\n        return ohlcvs_table_insert\n        # else:\n        #     return None\n\n    @classmethod\n    def make_error_tuple(\n            cls,\n            symbol: str,\n            start_date: datetime.datetime,\n            end_date: datetime.datetime,\n            interval: str,\n            resp_status_code: int,\n            exception_class: str,\n            exception_msg: str,\n            ohlcv_section: str=None\n        ) -> tuple:\n        '''\n        Returns a list that contains: a tuple to insert into the ohlcvs error table\n        \n        :params:\n            `symbol`: string\n            `start_date`: datetime obj of start date\n            `end_date`: datetime obj of end date\n            `interval`: string\n            `resp_status_code`: int - response status code\n            `exception_class`: string\n            `exception_msg`: string\n            `ohlcv_section`: default None\n        '''\n\n        # Convert start_date and end_date to datetime obj if needed;\n        # Because timestamps in binance are in mls\n        if not isinstance(start_date, datetime.datetime):\n            start_date = milliseconds_to_datetime(start_date)\n        if not isinstance(end_date, datetime.datetime):\n            end_date = milliseconds_to_datetime(end_date)\n\n        return (\n            (EXCHANGE_NAME, symbol, start_date, end_date,\n            interval, ohlcv_section, resp_status_code,\n            str(exception_class),exception_msg),\n        )\n\n    def _reset_backoff(self):\n        '''\n        Resets Redis backoff attributes\n        '''\n\n        self.redis_client.delete(\n            BACKOFF_STT_REDIS,\n            BACKOFF_URL_REDIS,\n            BACKOFF_TIME_REDIS,\n            BACKOFF_DUR_REDIS\n        )\n\n    async def _get_ohlcv_data(self, ohlcv_url: str) -> tuple:\n        '''\n        Gets ohlcv data based on url;\n            Also prepares to backoff before making request\n        \n        Returns a tuple with:\n            - http status (None if there's none),\n            - ohlcvs (None if there's none),\n            - exception type (None if there's none),\n            - error message (None if there's none)\n        \n        :params:\n            `ohlcv_url`: string - ohlcv API url\n            `throttler`: asyncio throttler obj\n        '''\n        \n        retries = 0\n        while retries < HTTPX_DEFAULT_RETRIES:\n            await self.rw_manager.acheck(1)\n            backoff_stt = self.redis_client.get(BACKOFF_STT_REDIS)\n            backoff_url = self.redis_client.get(BACKOFF_URL_REDIS)\n            backoff_duration = self.redis_client.get(BACKOFF_DUR_REDIS)\n            backoff_time = self.redis_client.get(BACKOFF_TIME_REDIS)\n            if (backoff_stt != \"429\" and backoff_stt != \"418\") \\\n                or ohlcv_url == backoff_url:\n                async with self.rate_limiter:\n                    try:\n                        ohlcvs_resp = await self.async_httpx_client.get(ohlcv_url)\n                        ohlcvs_resp.raise_for_status()\n                        self._reset_backoff()\n                        ohlcv_data = ohlcvs_resp.json()\n                        return (\n                            ohlcvs_resp.status_code,\n                            ohlcv_data,\n                            None,\n                            None\n                        )\n                    except httpx.HTTPStatusError as exc:\n                        resp_status_code = exc.response.status_code\n                        if resp_status_code == 429 or resp_status_code == 418:\n                            retry_after = exc.response.headers['Retry-After']\n\n                            self.redis_client.set(BACKOFF_STT_REDIS, resp_status_code)\n                            self.redis_client.set(BACKOFF_URL_REDIS, ohlcv_url)\n                            self.redis_client.set(BACKOFF_DUR_REDIS, retry_after)\n                            self.redis_client.set(\n                                BACKOFF_TIME_REDIS, redis_time(self.redis_client)\n                            )\n\n                            self.logger.info(f\"get_ohlcv_data: Backing off...\")\n                            await asyncio.sleep(float(retry_after))\n                        else:\n                            self._reset_backoff()\n                            return (\n                                resp_status_code,\n                                None,\n                                type(exc),\n                                f'EXCEPTION: Response status code: {resp_status_code} while requesting {exc.request.url}'\n                            )\n                    except httpx.TimeoutException as exc:\n                        await asyncio.sleep(1) # for now just 1 sec\n                    except Exception as exc:\n                        self._reset_backoff()\n                        return (\n                            None,\n                            None,\n                            type(exc),\n                            f'EXCEPTION: Request error while requesting {ohlcv_url}'\n                        )\n            else:\n                self.logger.info(\"get_ohlcv_data: Backing off...\")\n                if backoff_duration and backoff_time:\n                    await asyncio.sleep(\n                        min(float(backoff_duration) - (redis_time(self.redis_client) - float(backoff_time)) + 10 * random.random(), RATE_LIMIT_SECS_PER_MIN)\n                    )\n                else:\n                    await asyncio.sleep(RATE_LIMIT_SECS_PER_MIN)\n            retries += 1\n        self._reset_backoff()\n        return (\n            None,\n            None,\n            MaximumRetriesReached,\n            f'EXCEPTION: Maximum retries reached while requesting {ohlcv_url}'\n        )\n\n    async def _get_and_parse_ohlcv(\n            self,\n            params: str,\n            update: bool=False\n        ) -> Union[str, None]:\n        '''\n        Gets and parses ohlcvs from consumed params\n\n        :params:\n            `params`: params consumed from Redis to-fetch set\n        '''\n          \n        # Extract params\n        params_split = params.split(REDIS_DELIMITER)\n        symbol = params_split[0]\n        start_date_mls = int(params_split[1])\n        end_date_mls = int(params_split[2])\n        interval = params_split[3]\n        limit = params_split[4]\n\n        # Construct url and fetch\n        #   Also try out all url options\n        base_id = self.symbol_data[symbol]['base_id']\n        quote_id = self.symbol_data[symbol]['quote_id']\n\n        ohlcv_urls = self.make_ohlcv_url(\n            interval, symbol, limit, start_date_mls\n        )\n        ohlcv_result = await self._get_ohlcv_data(\n            ohlcv_urls[0]\n        ) \\\n        or await self._get_ohlcv_data(\n            ohlcv_urls[1]\n        ) \\\n        or await self._get_ohlcv_data(\n            ohlcv_urls[2]\n        ) \\\n        or await self._get_ohlcv_data(\n            ohlcv_urls[3]\n        )\n\n        # if ohlcv_result:\n        # what the heck? why need this condition check?\n\n        resp_status_code = ohlcv_result[0]\n        ohlcvs = ohlcv_result[1]\n        exc_type = ohlcv_result[2]\n        exception_msg = ohlcv_result[3]\n\n        # If exc_type is None (meaning no exception), process;\n        #   Else, process the error\n        # Why increment start_date_mls by 60000 * OHLCV_LIMIT:\n        #   Because in each request we fetch at least `OHLCV_LIMIT`\n        #   transaction-minutes. Thus, the next transaction-minute must\n        #   be at least 60000 * OHLCV_LIMIT milliseconds away\n        # Finally, remove params only in 2 cases:\n        #   - insert is successful\n        #   - empty ohlcvs from API\n        if exc_type is None:\n            try:\n                # Copy to PSQL if parsed successfully\n                # Get the latest date in OHLCVS list,\n                #   if latest date > start_date, update start_date\n                ohlcvs_parsed = self.parse_ohlcvs(ohlcvs, base_id, quote_id)\n                if ohlcvs_parsed:\n                    insert_success = False\n                    if update:\n                        insert_success = psql_bulk_insert(\n                            self.psql_conn,\n                            ohlcvs_parsed,\n                            OHLCVS_TABLE,\n                            insert_update_query = PSQL_INSERT_UPDATE_QUERY,\n                            unique_cols = OHLCV_UNIQUE_COLUMNS,\n                            update_cols = OHLCV_UPDATE_COLUMNS\n                        )\n                    else:\n                        insert_success = psql_bulk_insert(\n                            self.psql_conn,\n                            ohlcvs_parsed,\n                            OHLCVS_TABLE,\n                            insert_ignoredup_query = PSQL_INSERT_IGNOREDUP_QUERY\n                        )\n                    \n                    ohlcvs_last_date = datetime_to_milliseconds(ohlcvs_parsed[-1][0])\n                    if ohlcvs_last_date > start_date_mls:\n                        start_date_mls = ohlcvs_last_date\n                    else:\n                        start_date_mls += (60000 * OHLCV_LIMIT)\n\n                    # Comment this out - not needed atm\n                    # if insert_success:\n                    #     self.redis_client.srem(self.fetching_key, params)\n                    # Honestly this part sucks...\n                    if not insert_success:\n                        exc_type = UnsuccessfulDatabaseInsert\n                        exception_msg = \"EXCEPTION: Unsuccessful database insert\"\n                        error_tuple = self.make_error_tuple(\n                            symbol, start_date_mls, end_date_mls, interval,\n                            resp_status_code, exc_type, exception_msg\n                        )\n                        psql_bulk_insert(\n                            self.psql_conn,\n                            error_tuple,\n                            OHLCVS_ERRORS_TABLE,\n                            insert_ignoredup_query = PSQL_INSERT_IGNOREDUP_QUERY\n                        )\n                else:\n                    start_date_mls += (60000 * OHLCV_LIMIT)\n                    # self.redis_client.srem(self.fetching_key, params) # not needed atm\n            except Exception as exc:\n                exc_type = type(exc)\n                exception_msg = f'EXCEPTION: Error while processing ohlcv response: {exc}'\n                self.logger.warning(exception_msg)\n                error_tuple = self.make_error_tuple(\n                    symbol, start_date_mls, end_date_mls, interval,\n                    resp_status_code, exc_type, exception_msg\n                )\n                psql_bulk_insert(\n                    self.psql_conn,\n                    error_tuple,\n                    OHLCVS_ERRORS_TABLE,\n                    insert_ignoredup_query = PSQL_INSERT_IGNOREDUP_QUERY\n                )\n                start_date_mls += (60000 * OHLCV_LIMIT)\n        else:\n            self.logger.warning(exception_msg)\n            error_tuple = self.make_error_tuple(\n                symbol, start_date_mls, end_date_mls, interval,\n                resp_status_code, exc_type, exception_msg\n            )\n            psql_bulk_insert(\n                self.psql_conn,\n                error_tuple,\n                OHLCVS_ERRORS_TABLE,\n                insert_ignoredup_query = PSQL_INSERT_IGNOREDUP_QUERY\n            )\n            start_date_mls += (60000 * OHLCV_LIMIT)\n        \n        # PSQL Commit\n        self.psql_conn.commit()\n        \n        # what the heck? why need this condition check?\n        # else:\n        #     start_date_mls += (60000 * OHLCV_LIMIT)\n\n        # Also make more params for to-fetch set\n        if start_date_mls < end_date_mls:\n            return self.make_tofetch_params(\n                symbol, start_date_mls, end_date_mls, interval, limit\n            )\n        else:\n            return None\n\n    async def _init_tofetch_redis(\n            self,\n            symbols: Iterable,\n            start_date: datetime.datetime,\n            end_date: datetime.datetime,\n            interval: str,\n            limit: int\n        ) -> None:\n        '''\n        Initializes feeding params to Redis to-fetch set\n        \n        :params:\n            `symbols`: iterable of symbols\n            `start_date`: datetime obj\n            `end_date`: datetime obj\n            `interval`: string\n            `limit`: int\n        \n        Feeds the following information:\n            - key: `self.tofetch_key`\n            - value: `symbol;;start_date_mls;;end_date_mls;;time_frame;;limit;;sort`\n        \n        example:\n            `BTCTUSD;;1000000;;2000000;;1m;;1000`\n        '''\n\n        # Set feeding status\n        # Convert datetime with tzinfo to non-tzinfo, if any\n        self.feeding = True\n        start_date = start_date.replace(tzinfo=None)\n        end_date = end_date.replace(tzinfo=None)\n        start_date_mls = datetime_to_milliseconds(start_date)\n        end_date_mls = datetime_to_milliseconds(end_date)\n\n        # The maximum list of symbols is short enough to feed\n        #   to Redis in a batch (~1200 symbols total as of June 2021)\n        # Finally reset feeding status\n        params_list = [\n            self.make_tofetch_params(\n                symbol, start_date_mls, end_date_mls, interval, limit\n            ) for symbol in symbols\n        ]\n        self.redis_client.sadd(self.tofetch_key, *params_list)\n        self.feeding = False\n        self.logger.info(\"Redis: Successfully initialized feeding params\")\n\n    async def _consume_ohlcvs_redis(self, update: bool=False) -> None:\n        '''\n        Consumes OHLCV parameters from the Redis to-fetch set\n        '''\n\n        # When start, move all [existing] params from fetching set to to-fetch set\n        # Keep looping and processing in batch if either:\n        # - self.feeding or\n        # - there are elements in to-fetch set or fetching set\n        fetching_params = self.redis_client.spop(\n            self.fetching_key,\n            self.redis_client.scard(self.fetching_key)\n        )\n        if fetching_params:\n            self.redis_client.sadd(\n                self.tofetch_key, *fetching_params\n            )\n        async with httpx.AsyncClient(\n            timeout=self.httpx_timout, limits=self.httpx_limits) as client:\n            self.async_httpx_client = client\n            while self.feeding or \\\n                self.redis_client.scard(self.tofetch_key) > 0 \\\n                or self.redis_client.scard(self.fetching_key) > 0:\n                # Pop a batch of size `rate_limit` from Redis to-fetch set,\n                #   send it to Redis fetching set\n                # Add params in params list to Redis fetching set\n                # New to-fetch params with new start dates will be results\n                #   of `get_parse_tasks`\n                #   Add these params to Redis to-fetch set, if not None\n                # Finally, remove params list from Redis fetching set\n                params_list = self.redis_client.spop(\n                    self.tofetch_key, OHLCVS_CONSUME_BATCH_SIZE\n                )\n                if params_list:\n                    self.redis_client.sadd(self.fetching_key, *params_list)\n                    get_parse_tasks = [\n                        self._get_and_parse_ohlcv(params, update) for params in params_list\n                    ]\n                    task_results = await asyncio.gather(*get_parse_tasks)\n                    new_tofetch_params = [\n                        params for params in task_results if params is not None\n                    ]\n                    if new_tofetch_params:\n                        self.logger.info(\n                            \"Redis: Adding more params to to-fetch with new start dates\")\n                        self.redis_client.sadd(\n                            self.tofetch_key, *new_tofetch_params)\n                       \n                    self.redis_client.srem(self.fetching_key, *params_list)\n    \n    async def _fetch_ohlcvs_symbols(\n            self,\n            symbols: list,\n            start_date_dt: datetime.datetime,\n            end_date_dt: datetime.datetime,\n            update: bool=False\n        ) -> None:\n        '''\n        Function to fetch OHLCVs of symbols\n        \n        :params:\n            `symbols`: list of symbol string\n            `start_date_dt`: datetime obj - for start date\n            `end_date_dt`: datetime obj - for end date\n            `update`: boolean - whether to update when inserting\n                to PSQL db\n        '''\n\n        # Set feeding status so the consume\n        # function does not close immediately\n        self.feeding = True\n\n        # Asyncio gather 2 tasks:\n        # - Init to-fetch\n        # - Consume from Redis to-fetch\n        await asyncio.gather(\n            self._init_tofetch_redis(\n                symbols, start_date_dt, end_date_dt, OHLCV_TIMEFRAME, OHLCV_LIMIT\n            ),\n            self._consume_ohlcvs_redis(update)\n        )\n"}
{"type": "source_file", "path": "fetchers/ws/bitfinex.py", "content": "### This module uses websocket to fetch bitfinex 1-minute OHLCV data in real time\n\nimport sys\nimport random\nimport logging\nimport asyncio\nimport json\nimport redis\nimport websockets\nfrom typing import Any, Iterable, NoReturn\nfrom common.config.constants import (\n    REDIS_HOST, REDIS_USER,\n    REDIS_PASSWORD, REDIS_DELIMITER\n)\nfrom common.utils.logutils import create_logger\nfrom common.utils.asyncioutils import AsyncLoopThread\nfrom fetchers.config.constants import (\n    WS_SUB_REDIS_KEY, WS_SERVE_REDIS_KEY,\n    WS_SUB_LIST_REDIS_KEY, WS_RATE_LIMIT_REDIS_KEY\n)\nfrom fetchers.config.queries import MUTUAL_BASE_QUOTE_QUERY\nfrom fetchers.rest.bitfinex import BitfinexOHLCVFetcher, EXCHANGE_NAME\nfrom fetchers.utils.ratelimit import AsyncThrottler\nfrom fetchers.utils.exceptions import (\n    UnsuccessfulConnection, ConnectionClosed, InvalidStatusCode\n)\nfrom fetchers.helpers.ws import (\n    make_sub_val,\n    make_sub_redis_key,\n    make_serve_redis_key\n)\n\n\n# Bitfinex only allows up to 30 subscriptions per ws connection\nURI = \"wss://api-pub.bitfinex.com/ws/2\"\nMAX_SUB_PER_CONN = 25\nBACKOFF_MIN_SECS = 2.0\nBACKOFF_MAX_SECS = 60.0\n\nclass BitfinexOHLCVWebsocket:\n    '''\n    Bitfinex OHLCV websocket fetcher\n    '''\n\n    def __init__(\n        self, log_to_stream: bool = False, log_filename: str = None\n    ):\n        check_log_file = log_to_stream is False and log_filename is None\n        if check_log_file:\n            raise ValueError(\n                \"log_filename must be provided if not logging to stream\"\n            )\n\n        self.redis_client = redis.Redis(\n            host=REDIS_HOST,\n            username=REDIS_USER,\n            password=REDIS_PASSWORD,\n            decode_responses=True\n        )\n        # Mapping from ws_symbol to symbol\n        #   and mapping from channel ID to symbol\n        self.wssymbol_mapping = {}\n        self.chanid_mapping = {}\n\n        # Rest fetcher for convenience\n        self.rest_fetcher = BitfinexOHLCVFetcher()\n\n        # Logging\n        self.logger = create_logger(\n            f'{EXCHANGE_NAME}_websocket',\n            stream_handler=log_to_stream,\n            log_filename=log_filename\n        )\n\n        # Rate limit manager\n        # Limit to attempt to connect every 3 secs\n        self.rate_limiter = AsyncThrottler(\n            WS_RATE_LIMIT_REDIS_KEY.format(exchange = EXCHANGE_NAME),\n            1,\n            3,\n            redis_client = self.redis_client\n        )\n\n        # Backoff\n        self.backoff_delay = BACKOFF_MIN_SECS\n\n        # Loop\n        # self.loop_handler = AsyncLoopThread(daemon=None)\n        # self.loop_handler.start()\n\n    async def subscribe_one(\n        self,\n        symbol: str,\n        ws_client: Any\n    ) -> None:\n        '''\n        Connects to WS endpoint for a symbol\n\n        :params:\n            `symbol`: string of symbol\n            `ws_client`: websockets client obj\n        '''\n\n        tsymbol = self.rest_fetcher.make_tsymbol(symbol)\n        ws_symbol = f\"trade:1m:{tsymbol}\"\n        self.wssymbol_mapping[ws_symbol] = symbol\n        msg = {'event': 'subscribe',  'channel': 'candles', 'key': ws_symbol}\n        await ws_client.send(json.dumps(msg))\n\n    async def subscribe(self, symbols: Iterable, i: int = 0) -> NoReturn:\n        '''\n        Subscribes to Bitfinex WS for `symbols`\n\n        :params:\n            `symbols` list of symbols\n                e.g., ['ETHBTC', 'BTCEUR']\n        '''\n\n        while True:\n            try:\n                # Delay before making a connection\n                # async with self.rate_limiter:\n                async with websockets.connect(URI, ping_interval=10) as ws:\n                    await asyncio.gather(\n                        *(self.subscribe_one(symbol, ws) for symbol in symbols))\n                    self.logger.info(f\"Connection {i}: Successful\")\n                    self.backoff_delay = BACKOFF_MIN_SECS\n                    while True:\n                        resp = await ws.recv()\n                        respj = json.loads(resp)\n\n                        # If resp is dict, find the symbol using wssymbol_mapping\n                        #   and then map chanID to found symbol\n                        # If resp is list, make sure its length is 6\n                        #   and use the mappings to find symbol and push to Redis\n                        if isinstance(respj, dict):\n                            if 'event' in respj:\n                                if respj['event'] == \"subscribed\":\n                                    symbol = self.wssymbol_mapping[respj['key']]\n                                    self.chanid_mapping[respj['chanId']] = symbol\n                                elif respj['event'] == \"error\":\n                                    self.logger.error(\n                                        f\"Connection {i}: Subscription failed, raising exception\")\n                                    raise UnsuccessfulConnection           \n                        elif isinstance(respj, list):\n                            if len(respj) == 2 and len(respj[1]) == 6:\n                                try:\n                                    symbol = self.chanid_mapping[respj[0]]\n                                    timestamp = int(respj[1][0])\n                                    open_ = respj[1][1]\n                                    high_ = respj[1][3]\n                                    low_ = respj[1][4]\n                                    close_ = respj[1][2]\n                                    volume_ = respj[1][5]\n                                    base_id = self.rest_fetcher.symbol_data[symbol]['base_id']\n                                    quote_id = self.rest_fetcher.symbol_data[symbol]['quote_id']\n\n                                    # We make \"sub'ed value\" by serializing the price\n                                    #   data so it can be put into Redis\n                                    # Then, unique sub'ed and serve Redis keys\n                                    #   are created to store the above sub'ed value\n                                    # Also note here that the value put into the sub_\n                                    #   and the serve_ keys are different:\n                                    #   >> sub_ key is a hash with its [internal]\n                                    #   keys as timestamps; this is so that the updater\n                                    #   script can bulk-insert all the price data in\n                                    #   the sub_ key to PSQL later, meanwhile...\n                                    #   >> serve_ key is a hash with its [internal]\n                                    #   keys as timestamp, o, h, l, c, v; this is\n                                    #   because the web service (a.k.a. FastAPI) only\n                                    #   serves to the user [via its websocket connection]\n                                    #   the latest price data, meaning when any new data\n                                    #   comes in from the exchange's ws API,\n                                    #   the hash gets updated\n\n                                    sub_val = make_sub_val(\n                                        timestamp,\n                                        open_, high_, low_, close_, volume_,\n                                        REDIS_DELIMITER\n                                    )\n                                    ws_sub_redis_key = make_sub_redis_key(\n                                        EXCHANGE_NAME,\n                                        base_id,\n                                        quote_id,\n                                        REDIS_DELIMITER\n                                    )\n                                    ws_serve_redis_key = make_serve_redis_key(\n                                        EXCHANGE_NAME,\n                                        base_id,\n                                        quote_id,\n                                        REDIS_DELIMITER\n                                    )\n\n                                    # Add ws sub key to set of all ws sub keys\n                                    # Set hash value for ws sub key\n                                    # Replace ws serve key hash if this timestamp\n                                    #   is more up-to-date\n                                    self.redis_client.sadd(\n                                        WS_SUB_LIST_REDIS_KEY, ws_sub_redis_key)\n                                    self.redis_client.hset(\n                                        ws_sub_redis_key, timestamp, sub_val)\n                                    current_timestamp = self.redis_client.hget(\n                                        ws_serve_redis_key,\n                                        'time')\n                                    if current_timestamp is None or \\\n                                        timestamp >= int(current_timestamp):\n                                        self.redis_client.hset(\n                                            ws_serve_redis_key,\n                                            mapping = {\n                                                'time': timestamp,\n                                                'open': open_,\n                                                'high': high_,\n                                                'low': low_,\n                                                'close': close_,\n                                                'volume': volume_\n                                            }\n                                        )\n                                except Exception as exc:\n                                    self.logger.warning(\n                                        f\"Bitfinex WS Fetcher: EXCEPTION: {exc}\")\n\n                        # Sleep to release event loop\n                        await asyncio.sleep(0.01)\n            except (ConnectionClosed, InvalidStatusCode) as exc:\n                self.logger.warning(\n                    f\"Connection {i} raised exception: {exc} - reconnecting...\"\n                )\n                await asyncio.sleep(min(self.backoff_delay, BACKOFF_MAX_SECS))\n                self.backoff_delay *= (1+random.random()) # add a random factor\n\n    async def mutual_basequote(self) -> None:\n        '''\n        Subscribes to all base-quote's that are available\n        in all exchanges\n        '''\n\n        symbols_dict = self.rest_fetcher.get_symbols_from_exch(MUTUAL_BASE_QUOTE_QUERY)\n        self.rest_fetcher.close_connections()\n        await asyncio.gather(self.subscribe(symbols_dict.keys()))\n\n    async def all(self) -> None:\n        '''\n        Subscribes to WS channels of all symbols\n        '''\n\n        self.rest_fetcher.fetch_symbol_data()\n        symbols =  tuple(self.rest_fetcher.symbol_data.keys())\n\n        # TODO: probably running in different threads is not needed\n        # for i in range(0, len(symbols), MAX_SUB_PER_CONN):\n        #     asyncio.run_coroutine_threadsafe(\n        #         self.subscribe(symbols[i:i+MAX_SUB_PER_CONN], i),\n        #         # self.coroutine(5, i),\n        #         self.loop_handler.loop\n        #     )\n\n        # Subscribe to `MAX_SUB_PER_CONN` per connection (e.g., 30)\n        await asyncio.gather(\n            *(\n                self.subscribe(symbols[i:i+MAX_SUB_PER_CONN], int(i/MAX_SUB_PER_CONN))\n                    for i in range(0, len(symbols), MAX_SUB_PER_CONN)\n            )\n        )\n\n    def run_mutual_basequote(self) -> None:\n        '''\n        API to run the `mutual base-quote` method\n        '''\n\n        asyncio.run(self.mutual_basequote())\n\n    def run_all(self) -> None:\n        '''\n        API to run the `all` method\n        '''\n\n        asyncio.run(self.all())\n"}
{"type": "source_file", "path": "scripts/fetchers/__init__.py", "content": ""}
{"type": "source_file", "path": "web/db/base.py", "content": "# Base class for database models\n\nfrom sqlalchemy.ext.declarative import declarative_base\n\nBase = declarative_base()\nmetadata = Base.metadata\n"}
{"type": "source_file", "path": "common/helpers/dbhelpers.py", "content": "# This module contains common db helpers\n\nimport json\n\n\ndef redis_pipe_rpush(redis_client, key, vals, serialize=False):\n    '''\n    Redis serialize (optional) and rpush using pipeline\n    params:\n        `redis_client`: Redis client obj\n        `key`: string - Redis key\n        `vals`: iterable of serialized values\n    '''\n\n    with redis_client.pipeline() as pipe:\n        for val in vals:\n            if serialize:\n                pipe.rpush(key, json.dumps(val))\n            else:\n                pipe.rpush(key, val)\n        pipe.execute()\n\ndef redis_pipe_sadd(redis_client, key, vals, serialize=False):\n    '''\n    Redis serialize (optional) and rpush using pipeline\n    params:\n        `redis_client`: Redis client obj\n        `key`: string - Redis key\n        `vals`: iterable of serialized values\n    '''\n\n    with redis_client.pipeline() as pipe:\n        for val in vals:\n            if serialize:\n                pipe.sadd(key, json.dumps(val))\n            else:\n                pipe.sadd(key, val)\n        pipe.execute()\n"}
{"type": "source_file", "path": "fetchers/rest/bittrex.py", "content": "# This module fetches bittrex 1-minute OHLCV data\n\nimport asyncio\nimport datetime\nfrom typing import Any, Iterable\n\nimport backoff\nimport httpx\n\nfrom common.config.constants import \\\n    DEFAULT_DATETIME_STR_QUERY, \\\n    OHLCVS_ERRORS_TABLE, OHLCVS_TABLE, \\\n    REDIS_DELIMITER\nfrom common.helpers.datetimehelpers import \\\n    datetime_to_str, list_days_fromto, str_to_datetime\nfrom common.helpers.numbers import round_decimal\nfrom fetchers.config.constants import \\\n    HTTPX_DEFAULT_RETRIES, OHLCV_UNIQUE_COLUMNS, \\\n    OHLCV_UPDATE_COLUMNS, REST_RATE_LIMIT_REDIS_KEY, \\\n    THROTTLER_RATE_LIMITS\nfrom fetchers.config.queries import \\\n    PSQL_INSERT_IGNOREDUP_QUERY, PSQL_INSERT_UPDATE_QUERY\nfrom fetchers.helpers.dbhelpers import psql_bulk_insert\nfrom fetchers.rest.base import BaseOHLCVFetcher\nfrom fetchers.utils.asyncioutils import onbackoff, onsuccessgiveup\nfrom fetchers.utils.exceptions import \\\n    MaximumRetriesReached, UnsuccessfulDatabaseInsert\nfrom fetchers.utils.ratelimit import GCRARateLimiter\n\n# Bittrex returns:\n#   1-min or 5-min time windows in a period of 1 day\n#   1-hour time windows in a period of 31 days\n#   1-day time windows in a period of 366 days\nEXCHANGE_NAME = \"bittrex\"\nBASE_URL = \"https://api.bittrex.com/v3\"\nMARKET_URL = \"https://api.bittrex.com/v3/markets\"\nOHLCV_INTERVALS = [\"MINUTE_1\", \"MINUTE_5\", \"HOUR_1\", \"DAY_1\"]\nDAYDELTAS = {\"MINUTE_1\": 1, \"MINUTE_5\": 1, \"HOUR_1\": 31, \"DAY_1\": 366}\nOHLCV_INTERVAL = \"MINUTE_1\"\nOHLCV_SECTION_HIST = \"historical\"\nRATE_LIMIT_HITS_PER_MIN = THROTTLER_RATE_LIMITS['RATE_LIMIT_HITS_PER_MIN'][EXCHANGE_NAME]\nRATE_LIMIT_SECS_PER_MIN = THROTTLER_RATE_LIMITS['RATE_LIMIT_SECS_PER_MIN']\nOHLCVS_BITTREX_TOFETCH_REDIS = \"ohlcvs_tofetch_bittrex\"\nOHLCVS_BITTREX_FETCHING_REDIS = \"ohlcvs_fetching_bittrex\"\nDATETIME_STR_FORMAT = \"%Y-%m-%dT%H:%M:%S\"\nOHLCVS_CONSUME_BATCH_SIZE = 100\n\nclass BittrexOHLCVFetcher(BaseOHLCVFetcher):\n    '''REST Fetcher for OHLCV from Bittrex\n    '''\n    def __init__(self, *args):\n        super().__init__(*args, exchange_name = EXCHANGE_NAME)\n\n        # Rate limiter\n        self.rate_limiter = GCRARateLimiter(\n            REST_RATE_LIMIT_REDIS_KEY.format(exchange = EXCHANGE_NAME),\n            1,\n            RATE_LIMIT_SECS_PER_MIN / RATE_LIMIT_HITS_PER_MIN,\n            redis_client = self.redis_client\n        )\n\n        # Load market data\n        self._load_symbol_data()\n\n    def _load_symbol_data(self) -> None:\n        '''\n        Loads market data into a dict of this form:\n            {\n                '1INCH-USD': {\n                    'base_id': \"1INCH\",\n                    'quote_id': \"USD\"\n                },\n                'some_other_symbol': {\n                    'base_id': \"ABC\",\n                    'quote_id': \"XYZ\"\n                }\n                ...\n            }\n        \n        Saves it in self.symbol_data\n        '''\n\n        # self.symbol_data = {}\n        # Only needs a temporary client\n        # This code can block (non-async) as it's needed for future fetching\n        with httpx.Client(timeout=None) as client:\n            markets_resp = client.get(MARKET_URL)\n            market_data = markets_resp.json()\n            for symbol_data in market_data:\n                symbol = symbol_data['symbol']\n                base_id = symbol_data['baseCurrencySymbol'].upper()\n                quote_id = symbol_data['quoteCurrencySymbol'].upper()\n                self.symbol_data[symbol] = {\n                    'base_id': base_id,\n                    'quote_id': quote_id\n                }\n\n    @classmethod\n    def make_ohlcv_url(\n            cls,\n            symbol: str,\n            interval: str,\n            start_date: datetime.datetime\n        ) -> tuple:\n        '''\n        Returns tuple of string of OHLCV url and historical indicator\n        \n        :params:\n            `symbol`: string - symbol\n            `interval`: string - interval type (see INTERVALS)\n            `start_date`: datetime object\n        \n        example: https://api.bittrex.com/v3/markets/1INCH-USD/candles/MINUTE_1/historical/2019/01/01\n        '''\n        \n        # Has to check for hist or recent of OHLCV historical param\n        # Fetch historical data if time difference between now and start date is > 1 day\n        delta = datetime.datetime.now() - start_date\n        historical = 0\n        if delta.days > 1:\n            historical = OHLCV_SECTION_HIST\n\n        if historical == OHLCV_SECTION_HIST:\n            if interval == \"MINUTE_1\" or interval == \"MINUTE_5\":\n                return (\n                    f'{BASE_URL}/markets/{symbol}/candles/{interval}/{historical}/{start_date.year}/{start_date.month}/{start_date.day}',\n                    historical\n                )\n            elif interval == \"HOUR_1\":\n                return (\n                    f'{BASE_URL}/markets/{symbol}/candles/{interval}/{historical}/{start_date.year}/{start_date.month}',\n                    historical\n                )\n            elif interval == \"DAY_1\":\n                return (\n                    f'{BASE_URL}/markets/{symbol}/candles/{interval}/{historical}/{start_date.year}',\n                    historical\n                )\n        return (\n            f\"{BASE_URL}/markets/{symbol}/candles/{interval}/recent\",\n            historical\n        )\n\n    @classmethod\n    def make_tofetch_params(\n            cls,\n            symbol: str,\n            start_date: str,\n            end_date: str,\n            interval: str\n        ) -> str:\n        '''\n        Makes tofetch params to feed into Redis to-fetch set\n        \n        :params:\n            `symbol`: symbol string\n            `start_date`: string representing datetime\n                that complies to `DEFAULT_DATETIME_STR_QUERY`\n            `end_date`: string representing datetime\n                that complies to `DEFAULT_DATETIME_STR_QUERY`\n            `interval`: string\n        \n        example:\n            `BTC-USD;;2021-06-16T00:00:00;;2021-06-17T00:00:00;;MINUTE_1`\n        '''\n\n        return f'{symbol}{REDIS_DELIMITER}{start_date}{REDIS_DELIMITER}{end_date}{REDIS_DELIMITER}{interval}'\n\n    @classmethod\n    def parse_ohlcvs(\n            cls,\n            ohlcvs: Iterable,\n            base_id: str,\n            quote_id: str\n        ) -> list:\n        '''\n        Returns a list of rows of parsed ohlcvs\n        \n        :params:\n            `ohlcvs`: iterable of ohlcv dicts (returned from request)\n            `base_id`: string\n            `quote_id`: string\n        '''\n\n        # Ignore ohlcvs that are empty, do not raise error,\n        #   as other errors are catched elsewhere\n        ohlcvs_table_insert = []\n        if ohlcvs:\n            ohlcvs_table_insert = [\n                (\n                    ohlcv['startsAt'],\n                    EXCHANGE_NAME, base_id, quote_id,\n                    round_decimal(ohlcv['open']),\n                    round_decimal(ohlcv['high']),\n                    round_decimal(ohlcv['low']),\n                    round_decimal(ohlcv['close']),\n                    round_decimal(ohlcv['volume'])\n                ) for ohlcv in ohlcvs\n            ]\n        return ohlcvs_table_insert\n    \n    @classmethod\n    def make_error_tuple(\n            cls,\n            symbol: str,\n            start_date: datetime.datetime,\n            end_date: datetime.datetime,\n            interval: str,\n            historical: str,\n            resp_status_code: int,\n            exception_class: str,\n            exception_msg: str\n        ) -> tuple:\n        '''\n        Returns a list that contains: a tuple to insert into the ohlcvs error table\n\n        :params:\n            `symbol`: string\n            `start_date`: datetime obj of start date\n            `end_date`: datetime obj of end date\n            `interval`: string - timeframe; e.g., MINUTE_1\n            `historical`: string - historical or not\n            `resp_status_code`: int - response status code\n            `exception_class`: string\n            `exception_msg`: string\n        '''\n        \n        return (\n            (EXCHANGE_NAME, symbol, start_date, end_date,\n            interval, historical, resp_status_code,\n            str(exception_class),exception_msg),\n        )\n\n    @backoff.on_predicate(\n        backoff.constant,\n        lambda result: result[0] == 429,\n        max_tries=12,\n        on_backoff=onbackoff,\n        on_success=onsuccessgiveup,\n        on_giveup=onsuccessgiveup,\n        interval=RATE_LIMIT_SECS_PER_MIN\n    )\n    async def _get_ohlcv_data(\n            self,\n            ohlcv_url: str,\n            throttler: Any=None,\n            exchange_name: str=EXCHANGE_NAME\n        ) -> tuple:\n        '''\n        Gets ohlcv data based on url;\n            Also backoffs conservatively by 60 secs\n        \n        Returns a tuple with:\n            - http status (None if there's none),\n            - ohlcvs (None if there's none),\n            - exception type (None if there's none),\n            - error message (None if there's none)\n\n        :params:\n            `ohlcv_url`: string - ohlcvs API url\n            `throttler`: self.rate_limiter\n            `exchange_name`: string - this exchange's name\n        '''\n\n        retries = 0\n        while retries < HTTPX_DEFAULT_RETRIES:\n            async with self.rate_limiter:\n                try:\n                    ohlcvs_resp = await self.async_httpx_client.get(ohlcv_url)\n                    ohlcvs_resp.raise_for_status()\n                    ohlcv_data = ohlcvs_resp.json()\n                    return (\n                        ohlcvs_resp.status_code,\n                        ohlcv_data,\n                        None,\n                        None\n                    )\n                except httpx.HTTPStatusError as exc:\n                    resp_status_code = exc.response.status_code\n                    return (\n                        resp_status_code,\n                        None,\n                        type(exc),\n                        f'EXCEPTION: Response status code: {resp_status_code} while requesting {exc.request.url}'\n                    )\n                except httpx.TimeoutException as exc:\n                    await asyncio.sleep(1) # for now just 1 sec\n                except Exception as exc:\n                    return (\n                        None,\n                        None,\n                        type(exc),\n                        f'EXCEPTION: Request error while requesting {ohlcv_url}'\n                    )\n            retries += 1\n        return (\n            None,\n            None,\n            MaximumRetriesReached,\n            f'EXCEPTION: Maximum retries reached while requesting {ohlcv_url}'\n        )\n\n    async def _get_and_parse_ohlcv(\n            self,\n            params: str,\n            update: bool=False\n        ) -> None:\n        '''\n        Gets and parses ohlcvs from consumed params\n        \n        :params:\n            `params`: params consumed from Redis to-fetch set\n        '''\n\n        # Extract params\n        params_split = params.split(REDIS_DELIMITER)\n        symbol = params_split[0]\n        start_date = str_to_datetime(params_split[1], DEFAULT_DATETIME_STR_QUERY)\n        end_date = str_to_datetime(params_split[2], DEFAULT_DATETIME_STR_QUERY)\n        interval = params_split[3]\n\n        # Construct url and fetch\n        base_id = self.symbol_data[symbol]['base_id']\n        quote_id = self.symbol_data[symbol]['quote_id']\n\n        ohlcv_url, historical = self.make_ohlcv_url(\n            symbol, interval, start_date\n        )\n        ohlcv_result = await self._get_ohlcv_data(\n            ohlcv_url, throttler=self.rate_limiter, exchange_name=self.exchange_name\n        )\n        resp_status_code = ohlcv_result[0]\n        ohlcvs = ohlcv_result[1]\n        exc_type = ohlcv_result[2]\n        exception_msg = ohlcv_result[3]\n\n        # If exc_type is None (meaning no exception), process;\n        #   Else, process the error\n        # Finally, remove params only in 2 cases:\n        #   - insert is successful\n        #   - empty ohlcvs from API\n        if exc_type is None:\n            try:\n                # Copy to PSQL if parsed successfully\n                # Get the latest date in OHLCVS list,\n                #   if latest date > start_date, update start_date\n                ohlcvs_parsed = self.parse_ohlcvs(ohlcvs, base_id, quote_id)\n                if ohlcvs_parsed:\n                    insert_success = False\n                    if update:\n                        insert_success = psql_bulk_insert(\n                            self.psql_conn,\n                            ohlcvs_parsed,\n                            OHLCVS_TABLE,\n                            insert_update_query = PSQL_INSERT_UPDATE_QUERY,\n                            unique_cols = OHLCV_UNIQUE_COLUMNS,\n                            update_cols = OHLCV_UPDATE_COLUMNS\n                        )\n                    else:\n                        insert_success = psql_bulk_insert(\n                            self.psql_conn,\n                            ohlcvs_parsed,\n                            OHLCVS_TABLE,\n                            insert_ignoredup_query = PSQL_INSERT_IGNOREDUP_QUERY\n                        )\n\n                    # Comment this out - not needed atm\n                    # if insert_success:\n                    #         self.redis_client.srem(self.fetching_key, params)\n                    if not insert_success:\n                        exc_type = UnsuccessfulDatabaseInsert\n                        exception_msg = \"EXCEPTION: Unsuccessful database insert\"\n                        error_tuple = self.make_error_tuple(\n                            symbol, start_date, end_date, interval, historical,\n                            resp_status_code, exc_type, exception_msg\n                        )\n                        psql_bulk_insert(\n                            self.psql_conn,\n                            error_tuple,\n                            OHLCVS_ERRORS_TABLE,\n                            insert_ignoredup_query = PSQL_INSERT_IGNOREDUP_QUERY\n                        )\n                # else:\n                    # self.redis_client.srem(self.fetching_key, params) # not needed atm\n            except Exception as exc:\n                exc_type = type(exc)\n                exception_msg = f'EXCEPTION: Error while processing ohlcv response: {exc}'\n                self.logger.warning(exception_msg)\n                error_tuple = self.make_error_tuple(\n                    symbol, start_date, end_date, interval, historical,\n                    resp_status_code, exc_type, exception_msg\n                )\n                psql_bulk_insert(\n                    self.psql_conn,\n                    error_tuple,\n                    OHLCVS_ERRORS_TABLE,\n                    insert_ignoredup_query = PSQL_INSERT_IGNOREDUP_QUERY\n                )\n        else:\n            self.logger.warning(exception_msg)\n            error_tuple = self.make_error_tuple(\n                symbol, start_date, end_date, interval, historical,\n                resp_status_code, exc_type, exception_msg\n            )\n            psql_bulk_insert(\n                    self.psql_conn,\n                    error_tuple,\n                    OHLCVS_ERRORS_TABLE,\n                    insert_ignoredup_query = PSQL_INSERT_IGNOREDUP_QUERY\n            )\n        \n        # PSQL Commit\n        self.psql_conn.commit()\n\n    async def _init_tofetch_redis(\n            self,\n            symbols: Iterable,\n            start_date: datetime.datetime,\n            end_date: datetime.datetime,\n            interval: str\n        ) -> None:\n        '''\n        Initializes feeding params to Redis to-fetch set\n        \n        :params:\n            `symbols`: iterable of symbols\n            `start_date`: datetime obj\n            `end_date`: datetime obj\n            `interval`: string\n        \n        Feeds the following information:\n            - key: `self.tofetch_key`\n            - value: `symbol;;interval;;historical;;start_date_str;;end_date_str`\n        \n        example:\n            `BTC-USD;;2021-06-16T00:00:00;;2021-06-17T00:00:00;;MINUTE_1`\n        '''\n\n        # Set feeding status\n        # Convert datetime with tzinfo to non-tzinfo, if any\n        # Also format end_date according to DEFAULT_DATETIME_STR_QUERY\n        self.feeding = True\n        start_date = start_date.replace(tzinfo=None)\n        end_date = end_date.replace(tzinfo=None)\n        end_date_fmted = datetime_to_str(end_date, DEFAULT_DATETIME_STR_QUERY)\n\n        # Initial feed params to Redis set\n        # Keep looping until start_date = end_date\n        # while start_date < end_date:\n        #     self.sadd_tofetch_redis(symbol, start_date, end_date, interval)\n        #     start_date += datetime.timedelta(days=DAYDELTAS[interval])\n        # Finally reset feeding status\n        for date in list_days_fromto(start_date, end_date):\n            date_fmted = datetime_to_str(date, DEFAULT_DATETIME_STR_QUERY)\n            params_list = [\n                self.make_tofetch_params(\n                    symbol, date_fmted, end_date_fmted, interval\n                ) for symbol in symbols\n            ]\n            self.redis_client.sadd(OHLCVS_BITTREX_TOFETCH_REDIS, *params_list)\n            \n            # Asyncio sleep to release event loop for the consume-ohlcvs task\n            await asyncio.sleep(\n                RATE_LIMIT_SECS_PER_MIN / RATE_LIMIT_HITS_PER_MIN\n            )\n        self.feeding = False\n        self.logger.info(\"Redis: Successfully initialized feeding params\")\n\n    async def _consume_ohlcvs_redis(self, update: bool=False) -> None:\n        '''\n        Consumes OHLCV parameters from the Redis to-fetch set\n        '''\n\n        # When start, move all params from fetching set to to-fetch set\n        # Only create http client when consuming, hence the context manager\n        # Keep looping if either:\n        # - self.feeding or\n        # - there are elements in to-fetch set or fetching set\n        fetching_params = self.redis_client.spop(\n            OHLCVS_BITTREX_FETCHING_REDIS,\n            self.redis_client.scard(OHLCVS_BITTREX_FETCHING_REDIS)\n        )\n        if fetching_params:\n            self.redis_client.sadd(\n                OHLCVS_BITTREX_TOFETCH_REDIS, *fetching_params\n            )\n        async with httpx.AsyncClient(\n            timeout=self.httpx_timout, limits=self.httpx_limits) as client:\n            self.async_httpx_client = client\n            while self.feeding or \\\n                self.redis_client.scard(OHLCVS_BITTREX_TOFETCH_REDIS) > 0 \\\n                    or self.redis_client.scard(OHLCVS_BITTREX_FETCHING_REDIS) > 0:\n                # Pop a batch of size `rate_limit` from Redis to-fetch set,\n                #   send it to Redis fetching set\n                # Add params in params list to Redis fetching set\n                # New to-fetch params with new start dates will be results\n                #   of `get_parse_tasks`\n                #   Add these params to Redis to-fetch set, if not None\n                # Finally, remove params list from Redis fetching set\n                    params_list = self.redis_client.spop(\n                        OHLCVS_BITTREX_TOFETCH_REDIS, OHLCVS_CONSUME_BATCH_SIZE\n                    )\n                    if params_list:\n                        self.redis_client.sadd(OHLCVS_BITTREX_FETCHING_REDIS, *params_list)\n                        get_parse_tasks = [\n                            self._get_and_parse_ohlcv(params, update) for params in params_list\n                        ]\n                        await asyncio.gather(*get_parse_tasks)\n\n                        self.redis_client.srem(self.fetching_key, *params_list)\n\n    async def _fetch_ohlcvs_symbols(\n            self,\n            symbols: list,\n            start_date_dt: datetime.datetime,\n            end_date_dt: datetime.datetime,\n            update: bool=False\n        ) -> None:\n        '''\n        Function to get OHLCVs of symbols\n        \n        :params:\n            `symbols`: list of symbol string\n            `start_date_dt`: datetime obj - for start date\n            `end_date_dt`: datetime obj - for end date\n            `update`: boolean - whether to update when inserting\n                to PSQL db\n        '''\n\n        # Set feeding status so the consume\n        # function does not close immediately\n        self.feeding = True\n\n        # Asyncio gather 2 tasks:\n        # - Init to-fetch\n        # - Consume from Redis to-fetch\n        await asyncio.gather(\n            self._init_tofetch_redis(\n                symbols, start_date_dt, end_date_dt, OHLCV_INTERVAL\n            ),\n            self._consume_ohlcvs_redis(update)\n        )\n"}
{"type": "source_file", "path": "fetchers/helpers/ws.py", "content": "# Helpers for WS fetchers\n\nfrom fetchers.config.constants import WS_SUB_REDIS_KEY, WS_SERVE_REDIS_KEY\n\n\ndef make_sub_val(t, o, h, l, c, v, d) -> str:\n    '''\n    Serializes OHLCV into sub value\n\n    For use of the WS updater\n\n    :params:\n        `t`: timestamp\n        `d`: delimiter\n    '''\n\n    return f'{t}{d}{o}{d}{h}{d}{l}{d}{c}{d}{v}'\n\ndef make_sub_redis_key(exch: str, base: str, quote: str, delimiter: str) -> str:\n    '''\n    Makes sub Redis key for the e-b-q combination\n\n    Can be of use when a query is submitted to view chart\n        of a symbol (i.e., e-b-q combination);\n        We can then indicate if the symbol is actively traded\n\n    :params:\n        `exch`: exchange name\n        `base`: base id\n        `quote`: quote id\n    '''\n\n    return WS_SUB_REDIS_KEY.format(\n        exchange = exch,\n        base_id = base,\n        quote_id = quote,\n        delimiter = delimiter\n    )\n\n# TODO: Rename this function to make_serve_redis_key\ndef make_serve_redis_key(exch: str, base: str, quote: str, delimiter: str) -> str:\n    '''\n    Makes send/serve Redis key for the e-b-q combination\n\n    Can be of use for the web service to grab real-time price\n        data of a symbol (i.e., e-b-q combination)\n\n    :params:\n        `exch`: exchange name\n        `base`: base id\n        `quote`: quote id\n    '''\n\n    return WS_SERVE_REDIS_KEY.format(\n        exchange = exch,\n        base_id = base,\n        quote_id = quote,\n        delimiter = delimiter\n    )\n"}
{"type": "source_file", "path": "web/main.py", "content": "import redis\nfrom fastapi import FastAPI\nfrom fastapi.staticfiles import StaticFiles\nfrom web.routes.api.ws.utils.connections import WSConnectionManager\nfrom web.routes.api.api import api_router\nfrom web.routes.views import views_router\n# from web.db.base import metadata\n# from web.db.session import engine\n\n# metadata.create_all(bind=engine)\napp = FastAPI(openapi_url=\"/api/openapi.json\")\napp.mount(\"/src\", StaticFiles(directory=\"web/src\"), name=\"src\")\napp.include_router(api_router, prefix=\"/api\")\napp.include_router(views_router, prefix=\"/view\")\nws_manager = WSConnectionManager()\n\n\n\n\n\n# @app.get(\"/testws\", response_class=HTMLResponse)\n# async def testws(\n#         request: Request,\n#         exchange: str,\n#         base_id: str,\n#         quote_id: str\n#     ):\n#     return templates.TemplateResponse(\n#         \"viewsymbol.html\", {\n#             \"request\": request,\n#             \"exchange\": exchange,\n#             \"base_id\": base_id,\n#             \"quote_id\": quote_id\n#         }\n#     )\n\n# @app.get(\"/analytics\", response_class=HTMLResponse)\n# async def analytics(request: Request):\n#     return templates.TemplateResponse(\n#         \"analytics.html\", {\n#             \"request\": request\n#         })\n"}
{"type": "source_file", "path": "fetchers/helpers/dbhelpers.py", "content": "# This module contains db helpers\n\nimport sys\nimport csv\nimport logging\nimport psycopg2\nfrom psycopg2 import sql, extras\nfrom typing import Iterable\nfrom io import StringIO\n\n\ndef log_psycopg2_exc(err: Exception) -> None:\n    '''\n    Logs details about a psycopg2 Exception\n    '''\n    \n    # get details about the exception\n    err_type, err_obj, traceback = sys.exc_info()\n\n    # get the line number when exception occured\n    line_num = traceback.tb_lineno\n\n    # print the connect() error\n    logging.error(\"\\npsycopg2 ERROR:\", err, \"on line number:\", line_num)\n    logging.error(\"psycopg2 traceback:\", traceback, \"-- type:\", err_type)\n\n    # psycopg2 extensions.Diagnostics object attribute\n    # print (\"\\nextensions.Diagnostics:\", err.diag)\n\n    # print the pgcode and pgerror exceptions\n    logging.error(\"pgerror:\", err.pgerror)\n    logging.error(\"pgcode:\", err.pgcode, \"\\n\")\n\ndef psql_bulk_insert(\n        conn,\n        rows: Iterable,\n        table: str,\n        insert_update_query: str = None,\n        insert_ignoredup_query: str = None,\n        unique_cols: tuple = None,\n        update_cols: tuple = None,\n        cursor = None\n    ) -> bool:\n    '''\n    Bulk inserts `rows` to `table` using StringIO and CSV;\n        \n    On conflict, either ignores or updates new values;\n        \n    Also uses `page_size` of 1000 for inserting;\n        \n    Returns a boolean value indicating whether insert is successful\n\n    :params:\n        `conn`: psycopg2 conn obj\n        `rows`: iterable of tuples\n        `table`: string - table name\n        `insert_update_query`: string - insert-update query to `table`,\\n\n                in case the copy method fails; this query must have a\\n\n                `{table}` placeholder, 3 placeholders for unique columns,\\n\n                update columns, and \"excluded\" columns\n        `insert_ignoredup_query`: string - insert-ignoredup query to `table`,\\n\n                in case the copy method fails; this query must have a `{table}` placeholder\n        `unique_cols`: tuple of strings with column names where unique constraint exists\n        `update_cols`: tuple of strings with column names to update data on\n        `cursor`: psycopg2 cursor obj (optional)\n\n    Note: `insert_update_query` is prioritized over `insert_ignoredup_query` if both are entered\n    '''\n\n    if insert_update_query is None and insert_ignoredup_query is None:\n        # Raise exception immediately\n        raise ValueError(\n            \"PSQL Bulk Insert: Either insert-update query or insert-ignoredup query must be provided\"\n        )\n    else:\n        if not cursor:\n            cursor = conn.cursor()\n        try:\n            buffer = StringIO()\n            writer = csv.writer(buffer)\n            writer.writerows(rows)\n            buffer.seek(0)\n            cursor.copy_from(buffer, table, sep=\",\", null=\"\")\n            conn.commit()\n            logging.info(f'PSQL Bulk Insert: Successfully copied rows to table {table}')\n            return True\n        except psycopg2.IntegrityError:\n            conn.rollback()\n            if insert_update_query is not None:\n                logging.info(\n                    f\"PSQL Bulk Insert: Performing insert with update to table {table}\"\n                )\n                ex_cols = (\"excluded\" for _ in update_cols)\n\n                if len(update_cols) > 1:\n                    insert_query = sql.SQL(insert_update_query).format(\n                        sql.SQL(\", \").join(map(sql.Identifier, unique_cols)),\n                        sql.SQL(\"(\") + sql.SQL(\", \").join(map(sql.Identifier, update_cols)) + sql.SQL(\")\"),\n                        sql.SQL(\"(\") + sql.SQL(\", \").join(map(sql.Identifier, ex_cols, update_cols)) + sql.SQL(\")\"),\n                        table = sql.Identifier(table)\n                    )\n                else:\n                    # This should enable single-column updates\n                    insert_query = sql.SQL(insert_update_query).format(\n                        sql.SQL(\", \").join(map(sql.Identifier, unique_cols)),\n                        sql.SQL(\", \").join(map(sql.Identifier, update_cols)),\n                        sql.SQL(\", \").join(map(sql.Identifier, ex_cols, update_cols)),\n                        table = sql.Identifier(table)\n                    )\n\n            elif insert_ignoredup_query is not None:\n                logging.info(\n                    f\"PSQL Bulk Insert: Performing insert without update to table {table}\"\n                )\n                insert_query = sql.SQL(insert_ignoredup_query).format(\n                    table = sql.Identifier(table)\n                )\n            extras.execute_values(cursor, insert_query, rows, page_size=1000)\n            conn.commit()\n            logging.info(f'PSQL Bulk Insert: Successfully inserted rows to table {table}')\n            return True\n        # Is it fine to catch ANY exception?\n        except Exception as exc:\n            conn.rollback()\n            logging.warning(f'PSQL Bulk Insert: EXCEPTION: \\n')\n            log_psycopg2_exc(exc)\n            return False\n            # Not want to raise exception here\n            #   because this function is used in mass-fetching\n            # raise exc\n        finally:\n            cursor.close()\n\ndef psql_query_format(query, *args):\n    '''\n    Returns a formatted SQL query in\n        psycopg2 format\n    :params:\n        `query`: string - a query with **only**\n            positional placeholders (i.e., `{}`)\n        `*args`: any - position arguments to\n            put into `query`\n    '''\n    \n    return sql.SQL(query).format(\n        *(sql.Identifier(arg) for arg in args)\n    )\n\n"}
{"type": "source_file", "path": "web/models/ohlcvs.py", "content": "# Models for OHLCV\n\nfrom sqlalchemy import (\n    Column, DateTime, ForeignKeyConstraint,\n    Index, Numeric, String, Table\n)\nfrom sqlalchemy.orm import relationship\nfrom common.config.constants import OHLCVS_TABLE\nfrom web.db.base import Base, metadata\n\n\nclass Ohlcv(Base):\n    __tablename__ = OHLCVS_TABLE\n    __table_args__ = (\n        ForeignKeyConstraint(['exchange', 'base_id', 'quote_id'], ['symbol_exchange.exchange', 'symbol_exchange.base_id', 'symbol_exchange.quote_id'], ondelete='CASCADE'),\n        Index('ohlcvs_time_idx', 'time'),\n        Index('ohlcvs_exch_time_idx', 'exchange', 'time'),\n        Index('ohlcvs_base_quote_time_idx', 'base_id', 'quote_id', 'time')\n    )\n\n    time = Column(DateTime(True), primary_key=True, nullable=False, index=True)\n    exchange = Column(String(100), primary_key=True, nullable=False)\n    base_id = Column(String(20), primary_key=True, nullable=False)\n    quote_id = Column(String(20), primary_key=True, nullable=False)\n    open = Column(Numeric, nullable=False)\n    high = Column(Numeric, nullable=False)\n    low = Column(Numeric, nullable=False)\n    close = Column(Numeric, nullable=False)\n    volume = Column(Numeric, nullable=False)\n\n    symbol_exchange = relationship('SymbolExchange')\n\nt_common_basequote_30 = Table(\n    'common_basequote_30', metadata,\n    Column('base_id', String(20)),\n    Column('quote_id', String(20))\n)\n\nt_ohlcvs_summary_daily = Table(\n    'ohlcvs_summary_daily', metadata,\n    Column('bucket', DateTime(True)),\n    Column('exchange', String(100)),\n    Column('base_id', String(20)),\n    Column('quote_id', String(20)),\n    Column('open', Numeric),\n    Column('high', Numeric),\n    Column('low', Numeric),\n    Column('close', Numeric),\n    Column('volume', Numeric)\n)\n\nt_ohlcvs_summary_5min = Table(\n    'ohlcvs_summary_5min', metadata,\n    Column('bucket', DateTime(True)),\n    Column('exchange', String(100)),\n    Column('base_id', String(20)),\n    Column('quote_id', String(20)),\n    Column('open', Numeric),\n    Column('high', Numeric),\n    Column('low', Numeric),\n    Column('close', Numeric),\n    Column('volume', Numeric)\n)\n\nt_ohlcvs_summary_15min = Table(\n    'ohlcvs_summary_15min', metadata,\n    Column('bucket', DateTime(True)),\n    Column('exchange', String(100)),\n    Column('base_id', String(20)),\n    Column('quote_id', String(20)),\n    Column('open', Numeric),\n    Column('high', Numeric),\n    Column('low', Numeric),\n    Column('close', Numeric),\n    Column('volume', Numeric)\n)\n\nt_ohlcvs_summary_30min = Table(\n    'ohlcvs_summary_30min', metadata,\n    Column('bucket', DateTime(True)),\n    Column('exchange', String(100)),\n    Column('base_id', String(20)),\n    Column('quote_id', String(20)),\n    Column('open', Numeric),\n    Column('high', Numeric),\n    Column('low', Numeric),\n    Column('close', Numeric),\n    Column('volume', Numeric)\n)\n\nt_ohlcvs_summary_1hour = Table(\n    'ohlcvs_summary_1hour', metadata,\n    Column('bucket', DateTime(True)),\n    Column('exchange', String(100)),\n    Column('base_id', String(20)),\n    Column('quote_id', String(20)),\n    Column('open', Numeric),\n    Column('high', Numeric),\n    Column('low', Numeric),\n    Column('close', Numeric),\n    Column('volume', Numeric)\n)\n\nt_ohlcvs_summary_6hour = Table(\n    'ohlcvs_summary_6hour', metadata,\n    Column('bucket', DateTime(True)),\n    Column('exchange', String(100)),\n    Column('base_id', String(20)),\n    Column('quote_id', String(20)),\n    Column('open', Numeric),\n    Column('high', Numeric),\n    Column('low', Numeric),\n    Column('close', Numeric),\n    Column('volume', Numeric)\n)\n\nt_ohlcvs_summary_12hour = Table(\n    'ohlcvs_summary_12hour', metadata,\n    Column('bucket', DateTime(True)),\n    Column('exchange', String(100)),\n    Column('base_id', String(20)),\n    Column('quote_id', String(20)),\n    Column('open', Numeric),\n    Column('high', Numeric),\n    Column('low', Numeric),\n    Column('close', Numeric),\n    Column('volume', Numeric)\n)\n\nt_ohlcvs_summary_7day = Table(\n    'ohlcvs_summary_7day', metadata,\n    Column('bucket', DateTime(True)),\n    Column('exchange', String(100)),\n    Column('base_id', String(20)),\n    Column('quote_id', String(20)),\n    Column('open', Numeric),\n    Column('high', Numeric),\n    Column('low', Numeric),\n    Column('close', Numeric),\n    Column('volume', Numeric)\n)\n"}
{"type": "source_file", "path": "web/routes/api/rest/utils/caching.py", "content": "from dogpile.cache.api import NO_VALUE\nfrom sqlalchemy import event\nfrom sqlalchemy.orm import loading\nfrom sqlalchemy.orm.interfaces import UserDefinedOption\n\n\n# Source code: https://docs.sqlalchemy.org/en/14/orm/examples.html#module-examples.dogpile_caching\n\nclass ORMCache():\n    '''\n    An add-on for an ORM :class:`.Session` optionally loads full results\n        from a dogpile cache region.\n\n    '''\n\n    def __init__(self, regions):\n        self.cache_regions = regions\n        self._statement_cache = {}\n\n    def listen_on_session(self, session_factory):\n        event.listen(session_factory, \"do_orm_execute\", self._do_orm_execute)\n\n    def _do_orm_execute(self, orm_context):\n        for opt in orm_context.user_defined_options:\n            if isinstance(opt, RelationshipCache):\n                opt = opt._process_orm_context(orm_context)\n                if opt is None:\n                    continue\n\n            if isinstance(opt, FromCache):\n                dogpile_region = self.cache_regions[opt.region]\n\n                our_cache_key = opt._generate_cache_key(\n                    orm_context.statement, orm_context.parameters, self\n                )\n\n                if opt.ignore_expiration:\n                    cached_value = dogpile_region.get(\n                        our_cache_key,\n                        expiration_time=opt.expiration_time,\n                        ignore_expiration=opt.ignore_expiration,\n                    )\n                else:\n\n                    def createfunc():\n                        return orm_context.invoke_statement().freeze()\n\n                    cached_value = dogpile_region.get_or_create(\n                        our_cache_key,\n                        createfunc,\n                        expiration_time=opt.expiration_time,\n                    )\n\n                if cached_value is NO_VALUE:\n                    # keyerror?   this is bigger than a keyerror...\n                    raise KeyError()\n\n                orm_result = loading.merge_frozen_result(\n                    orm_context.session,\n                    orm_context.statement,\n                    cached_value,\n                    load=False,\n                )\n                return orm_result()\n        else:\n            return None\n\n    def invalidate(self, statement, parameters, opt):\n        '''\n        Invalidate the cache value represented by a statement\n        '''\n\n        statement = statement.__clause_element__()\n\n        dogpile_region = self.cache_regions[opt.region]\n\n        cache_key = opt._generate_cache_key(statement, parameters, self)\n\n        dogpile_region.delete(cache_key)\n\n\nclass FromCache(UserDefinedOption):\n    '''\n    Specifies that a Query should load results from a cache\n    '''\n\n    propagate_to_loaders = False\n\n    def __init__(\n        self,\n        region=\"default\",\n        cache_key=None,\n        expiration_time=None,\n        ignore_expiration=False,\n    ):\n        '''\n        Construct a new FromCache.\n\n        :param region: the cache region.  Should be a\n         region configured in the dictionary of dogpile\n         regions.\n\n        :param cache_key: optional.  A string cache key\n         that will serve as the key to the query.   Use this\n         if your query has a huge amount of parameters (such\n         as when using in_()) which correspond more simply to\n         some other identifier.\n        '''\n\n        self.region = region\n        self.cache_key = cache_key\n        self.expiration_time = expiration_time\n        self.ignore_expiration = ignore_expiration\n\n    def _gen_cache_key(self, anon_map, bindparams):\n        return None\n\n    def _generate_cache_key(self, statement, parameters, orm_cache):\n        statement_cache_key = statement._generate_cache_key()\n\n        key = statement_cache_key.to_offline_string(\n            orm_cache._statement_cache, statement, parameters\n        ) + repr(self.cache_key)\n        # print(\"here's our key...%s\" % key)\n        return key\n\n\nclass RelationshipCache(FromCache):\n    '''\n    Specifies that a Query as called within a \"lazy load\"\n        should load results from a cache\n    '''\n\n    propagate_to_loaders = True\n\n    def __init__(\n        self,\n        attribute,\n        region=\"default\",\n        cache_key=None,\n        expiration_time=None,\n        ignore_expiration=False,\n    ):\n        \"\"\"Construct a new RelationshipCache.\n\n        :param attribute: A Class.attribute which\n         indicates a particular class relationship() whose\n         lazy loader should be pulled from the cache.\n\n        :param region: name of the cache region.\n\n        :param cache_key: optional.  A string cache key\n         that will serve as the key to the query, bypassing\n         the usual means of forming a key from the Query itself.\n\n        \"\"\"\n        self.region = region\n        self.cache_key = cache_key\n        self.expiration_time = expiration_time\n        self.ignore_expiration = ignore_expiration\n        self._relationship_options = {\n            (attribute.property.parent.class_, attribute.property.key): self\n        }\n\n    def _process_orm_context(self, orm_context):\n        current_path = orm_context.loader_strategy_path\n\n        if current_path:\n            mapper, prop = current_path[-2:]\n            key = prop.key\n\n            for cls in mapper.class_.__mro__:\n                if (cls, key) in self._relationship_options:\n                    relationship_option = self._relationship_options[\n                        (cls, key)\n                    ]\n                    return relationship_option\n\n    def and_(self, option):\n        \"\"\"Chain another RelationshipCache option to this one.\n\n        While many RelationshipCache objects can be specified on a single\n        Query separately, chaining them together allows for a more efficient\n        lookup during load.\n\n        \"\"\"\n        self._relationship_options.update(option._relationship_options)\n        return self\n"}
{"type": "source_file", "path": "web/routes/api/rest/utils/readers.py", "content": "# Get functions from database, independent of backend path operations\n\nimport datetime\nfrom typing import List, Optional, Union\nfrom sqlalchemy import func, literal, column\nfrom sqlalchemy.orm import Session\nfrom sqlalchemy.dialects.postgresql import INTERVAL\nfrom sqlalchemy.sql.functions import concat\nfrom common.helpers.datetimehelpers import milliseconds_to_datetime\nfrom web import models\nfrom web.config.constants import OHLCV_INTERVALS\nfrom web.routes.api.rest.utils import caching, parsers\n\n\ndef read_test(db: Session) -> List[models.TestTable]:\n    '''\n    Reads a row from `test` table\n    '''\n    \n    return db.query(models.TestTable) \\\n        .order_by(models.TestTable.id).limit(1).all()\n\ndef read_symbol_exchange(db: Session) -> List[models.SymbolExchange]:\n    '''\n    Reads all rows from `symbol_exchange` database table\n    '''\n\n    return db.query(models.SymbolExchange) \\\n        .filter(models.SymbolExchange.is_trading == True) \\\n        .order_by(models.SymbolExchange.exchange.asc()).all()\n\ndef read_geodr(\n        db: Session,\n        cutoff_upper_pct: Union[int, None],\n        cutoff_lower_pct: Union[int, None],\n        limit: int = 500\n    ) -> list:\n    '''\n    Reads all rows from `geo_daily_return` database table\n\n    If limit == -1, returns all symbols\n    '''\n\n    if cutoff_upper_pct and cutoff_lower_pct:\n        fromdb = db.query(models.geo_daily_return) \\\n            .filter(\n                models.geo_daily_return.c.daily_return_pct \\\n                    < cutoff_upper_pct,\n                models.geo_daily_return.c.daily_return_pct \\\n                    > cutoff_lower_pct)\n    elif cutoff_upper_pct:\n        fromdb = db.query(models.geo_daily_return) \\\n            .filter(\n                models.geo_daily_return.c.daily_return_pct \\\n                    < cutoff_upper_pct)\n    elif cutoff_lower_pct:\n        fromdb = db.query(models.geo_daily_return) \\\n            .filter(\n                models.geo_daily_return.c.daily_return_pct \\\n                    > cutoff_lower_pct)\n    else:\n        fromdb = db.query(models.geo_daily_return)\n\n    if limit == -1:\n        return fromdb \\\n        .order_by(models.geo_daily_return.c.daily_return_pct.desc()).all()\n    return fromdb \\\n        .order_by(models.geo_daily_return.c.daily_return_pct.desc()) \\\n        .limit(limit).all()\n\ndef read_wr(\n        db: Session,\n        cutoff_upper_pct: Union[int, None],\n        cutoff_lower_pct: Union[int, None],\n        limit: int = 500\n    ) -> list:\n    '''\n    Reads all rows from `weekly_return` database table\n\n    :params:\n        `limit`: limit number of rows\n        `cutoff_upper_pct`: upper cutoff for extreme values\n        `cutoff_lower_pct`: lower cutoff for extreme values\n\n    If `limit` == -1, returns all symbols\n    \n    Default limit of 500 rows\n    '''\n\n    if cutoff_upper_pct and cutoff_lower_pct:\n        fromdb = db.query(models.weekly_return) \\\n            .filter(\n                models.weekly_return.c.weekly_return_pct \\\n                    < cutoff_upper_pct,\n                models.weekly_return.c.weekly_return_pct \\\n                    > cutoff_lower_pct)\n    elif cutoff_upper_pct:\n        fromdb = db.query(models.weekly_return) \\\n            .filter(\n                models.weekly_return.c.weekly_return_pct \\\n                    < cutoff_upper_pct)\n    elif cutoff_lower_pct:\n        fromdb = db.query(models.weekly_return) \\\n            .filter(\n                models.weekly_return.c.weekly_return_pct \\\n                    > cutoff_lower_pct)\n    else:\n        fromdb = db.query(models.weekly_return)\n    \n    \n    if limit == -1:\n        return fromdb \\\n            .order_by(models.weekly_return.c.weekly_return_pct.desc()).all()\n    return fromdb \\\n        .order_by(models.weekly_return.c.weekly_return_pct.desc()) \\\n        .limit(limit).all()\n\ndef read_top20qvlm(db: Session) -> list:\n    '''\n    Reads all rows from `top_20_quoted_vol` database table\n    '''\n\n    return db.query(models.top_20_quoted_vol) \\\n        .order_by(models.top_20_quoted_vol.c.total_volume.desc()).all()\n\ndef read_ohlcvs(\n        db: Session,\n        exchange: str,\n        base_id: str,\n        quote_id: str,\n        interval: str,\n        start: Optional[int] = None,\n        end: Optional[int] = None,\n        limit: Optional[int] = 500,\n        empty_ts: Optional[bool] = False,\n        results_mls: Optional[bool] = True,\n    ) -> list:\n    '''\n    Reads rows from `ohlcvs` database table,\n        based on `exchange`, `base_id`, `quote_id`\n        and timestamps between `start` and `end` (inclusive)\n    \n    Limits to a maximum of 500 data points\n    \n    Outputs rows with timestamp by ascending order\n    \n    Outputs rows with timestamp in seconds\n    \n    :params:\n        `db`: sqlalchemy Session obj\n        `exchange`: str - exchange name\n        `base_id`: str - base id\n        `quote_id`: str - quote id\n        `interval`: str - time interval of candles\n            enum: within the `OHLCV_INTERVALS` list\n        `start`: datetime obj or int - start time (must have same type as `end`)\n        `end`: datetime obj or int - end time (must have same type as `start`)\n        `limit`: maximum number of data points to return\n        `empty_ts`: show timestamps with empty ohlcv values by filling them with the averages of the ohlcv values from resulted rows from database - this is for a more \"correct\" chart;\n            If this parameter is `true`, results are sorted in ascending order, so that the real-time chart engine runs without errors\n        `results_mls`: bool if result timestamps are in milliseconds;\n            If true, result timestamps are in millieconds;\n            If false, result timestamps are in seconds\n    \n    If `interval` == `1m`: get directly from ohlcv table\n    '''\n\n    limit = min(limit, 500)\n    if end:\n        end = milliseconds_to_datetime(end).replace(second=0, microsecond=0)\n    else:\n        end = (\n            datetime.datetime.now() - datetime.timedelta(minutes = 1)\n        ).replace(second=0, microsecond=0)\n    if start:\n        start = milliseconds_to_datetime(start).replace(second=0, microsecond=0)\n\n    # Return ohlcv from different tables based on interval\n    ohlcvs = []\n    if interval not in OHLCV_INTERVALS:\n        return ohlcvs\n    # Get max. latest XXXX rows from psql db\n    elif interval == \"1m\":\n        if start:\n            fromdb = db.query(models.Ohlcv) \\\n                .options(caching.FromCache(\"default\")) \\\n                .filter(\n                    models.Ohlcv.exchange == exchange,\n                    models.Ohlcv.base_id == base_id,\n                    models.Ohlcv.quote_id == quote_id,\n                    models.Ohlcv.time >= start,\n                    models.Ohlcv.time <= end\n                ) \\\n                .order_by(models.Ohlcv.time.desc()) \\\n                .limit(limit) \\\n                # .subquery()\n        else:\n            fromdb = db.query(models.Ohlcv) \\\n                .options(caching.FromCache(\"default\")) \\\n                .filter(\n                    models.Ohlcv.exchange == exchange,\n                    models.Ohlcv.base_id == base_id,\n                    models.Ohlcv.quote_id == quote_id,\n                    models.Ohlcv.time <= end\n                ) \\\n                .order_by(models.Ohlcv.time.desc()) \\\n                .limit(limit) \\\n                # .subquery()\n        \n        if empty_ts:\n            # Convert fromdb\n            fromdb = fromdb.subquery()\n\n            # Dummy timestamps to fill empty timestamps from db\n            ts = func.generate_series(\n                    func.min(fromdb.c.time),\n                    end,\n                    func.cast(concat(1, ' MINUTE'), INTERVAL)\n                ).label('time')\n            \n            dseries = db.query(\n                ts,\n                func.avg(fromdb.c.open).label('open'),\n                func.avg(fromdb.c.high).label('high'),\n                func.avg(fromdb.c.low).label('low'),\n                func.avg(fromdb.c.close).label('close'),\n                literal(0).label('volume')) \\\n                .order_by(ts.desc()) \\\n                .limit(limit) \\\n                .subquery()\n\n            # Join the dummy timestamps with fromdb\n            result = db.query(\n                dseries.c.time,\n                func.coalesce(fromdb.c.open, dseries.c.open).label('open'),\n                func.coalesce(fromdb.c.high, dseries.c.high).label('high'),\n                func.coalesce(fromdb.c.low, dseries.c.low).label('low'),\n                func.coalesce(fromdb.c.close, dseries.c.close).label('close'),\n                func.coalesce(fromdb.c.volume, dseries.c.volume).label('volume')\n            ) \\\n            .join(fromdb, dseries.c.time == fromdb.c.time, isouter = True) \\\n            .order_by(dseries.c.time.asc()) \\\n            .limit(limit) \\\n            .all()\n        else:\n            result = fromdb.all()\n\n    else:\n        # Choose summary table according to `interval`\n        if interval == \"5m\":\n            # if not start:\n            #     start = end - datetime.timedelta(minutes = limit * 5)\n            table = models.t_ohlcvs_summary_5min\n            conc_tup = (5, ' MINUTES')\n        elif interval == \"15m\":\n            # if not start:\n            #     start = end - datetime.timedelta(minutes = limit * 15)\n            table = models.t_ohlcvs_summary_15min\n            conc_tup = (15, ' MINUTES')\n        elif interval == \"30m\":\n            # if not start:\n            #     start = end - datetime.timedelta(minutes = limit * 30)\n            table = models.t_ohlcvs_summary_30min\n            conc_tup = (30, ' MINUTES')\n        elif interval == \"1h\":\n            # if not start:\n            #     start = end - datetime.timedelta(hours = limit * 1)\n            table = models.t_ohlcvs_summary_1hour\n            conc_tup = (1, ' HOUR')\n        elif interval == \"6h\":\n            # if not start:\n            #     start = end - datetime.timedelta(hours = limit * 6)\n            table = models.t_ohlcvs_summary_6hour\n            conc_tup = (6, ' HOURS')\n        elif interval == \"12h\":\n            # if not start:\n            #     start = end - datetime.timedelta(hours = limit * 12)\n            table = models.t_ohlcvs_summary_12hour\n            conc_tup = (12, ' HOURS')\n        elif interval == \"1D\":\n            # if not start:\n            #     start = end - datetime.timedelta(days = limit * 1)\n            table = models.t_ohlcvs_summary_daily\n            conc_tup = (1, ' DAY')\n        elif interval == \"7D\":\n            # if not start:\n            #     start = end - datetime.timedelta(days = limit * 7)\n            table = models.t_ohlcvs_summary_7day\n            conc_tup = (7, ' DAYS')\n\n        if start:\n            fromdb = db.query(\n                table.c.bucket.label('time'),\n                table.c.open.label('open'),\n                table.c.high.label('high'),\n                table.c.low.label('low'),\n                table.c.close.label('close'),\n                table.c.volume.label('volume')) \\\n                .options(caching.FromCache(\"default\")) \\\n                .filter(\n                    table.c.exchange == exchange,\n                    table.c.base_id == base_id,\n                    table.c.quote_id == quote_id,\n                    table.c.bucket >= start,\n                    table.c.bucket <= end\n                ) \\\n                .order_by(table.c.bucket.desc()) \\\n                .limit(limit) \\\n                # .subquery()\n        else:\n            fromdb = db.query(\n                table.c.bucket.label('time'),\n                table.c.open.label('open'),\n                table.c.high.label('high'),\n                table.c.low.label('low'),\n                table.c.close.label('close'),\n                table.c.volume.label('volume')) \\\n                .options(caching.FromCache(\"default\")) \\\n                .filter(\n                    table.c.exchange == exchange,\n                    table.c.base_id == base_id,\n                    table.c.quote_id == quote_id,\n                    table.c.bucket <= end\n                ) \\\n                .order_by(table.c.bucket.desc()) \\\n                .limit(limit) \\\n                # .subquery()\n\n        if empty_ts:\n            # Convert fromdb\n            fromdb = fromdb.subquery()\n\n            # Dummy timestamps to fill empty timestamps from db\n            ts = func.generate_series(\n                    func.min(fromdb.c.time),\n                    end,\n                    func.cast(concat(conc_tup[0], conc_tup[1]), INTERVAL)\n                ).label('time')\n\n            dseries = db.query(\n                ts,\n                func.avg(fromdb.c.open).label('open'),\n                func.avg(fromdb.c.high).label('high'),\n                func.avg(fromdb.c.low).label('low'),\n                func.avg(fromdb.c.close).label('close'),\n                literal(0).label('volume')) \\\n                .order_by(ts.desc()) \\\n                .limit(limit) \\\n                .subquery()\n\n            # Join the dummy timestamps with fromdb\n            result = db.query(\n                dseries.c.time,\n                func.coalesce(fromdb.c.open, dseries.c.open).label('open'),\n                func.coalesce(fromdb.c.high, dseries.c.high).label('high'),\n                func.coalesce(fromdb.c.low, dseries.c.low).label('low'),\n                func.coalesce(fromdb.c.close, dseries.c.close).label('close'),\n                func.coalesce(fromdb.c.volume, dseries.c.volume).label('volume')) \\\n                .join(fromdb, dseries.c.time == fromdb.c.time, isouter=True) \\\n                .order_by(dseries.c.time.asc()) \\\n                .limit(limit) \\\n                .all()\n        else:\n            result = fromdb.all()\n    \n    # Parse ohlcvs\n    ohlcvs = parsers.parse_ohlcv(result, results_mls)\n    return ohlcvs\n"}
{"type": "source_file", "path": "web/models/ohlcvs_errors.py", "content": "from sqlalchemy import (\n    Column, DateTime, ForeignKeyConstraint, Index, Numeric,\n    SmallInteger, String, Table, Text\n)\nfrom sqlalchemy.orm import relationship\nfrom common.config.constants import (\n    OHLCVS_TABLE, OHLCVS_ERRORS_TABLE, SYMBOL_EXCHANGE_TABLE\n)\nfrom web.db.base import Base, metadata\n\n\nclass OhlcvsError(Base):\n    __tablename__ = OHLCVS_ERRORS_TABLE\n\n    exchange = Column(String(100), primary_key=True, nullable=False)\n    symbol = Column(String(20), primary_key=True, nullable=False)\n    start_date = Column(DateTime(True), primary_key=True, nullable=False)\n    end_date = Column(DateTime(True), primary_key=True, nullable=False)\n    time_frame = Column(String(10), primary_key=True, nullable=False)\n    ohlcv_section = Column(String(30))\n    resp_status_code = Column(SmallInteger)\n    exception_class = Column(Text, primary_key=True, nullable=False)\n    exception_message = Column(Text)\n"}
{"type": "source_file", "path": "web/routes/api/api.py", "content": "from fastapi import APIRouter\nfrom web.routes.api.rest import (\n    ohlcvs as ohlcvs_rest, symexch, test as test_rest, analytics\n)\nfrom web.routes.api.ws import ohlcvs as ohlcvs_ws, test as test_ws\n\napi_router = APIRouter()\napi_router.include_router(ohlcvs_rest.router, tags=[\"api_ohlcvs_rest\"])\napi_router.include_router(symexch.router, tags=[\"api_symexch\"])\napi_router.include_router(test_rest.router, tags=[\"api_test_rest\"])\napi_router.include_router(ohlcvs_ws.router, tags=[\"api_ohlcvs_ws\"])\napi_router.include_router(analytics.router, tags=[\"api_analytics\"])\napi_router.include_router(test_ws.router, tags=[\"api_test_ws\"])\n"}
{"type": "source_file", "path": "web/routes/api/rest/analytics.py", "content": "# Backend REST API endpoint for analytics views\n\nfrom typing import Union, Optional\nfrom sqlalchemy.orm import Session\nfrom fastapi import APIRouter, Depends\nfrom web.routes.api.deps import get_db\nfrom web.routes.api.rest.utils import readers\n\n\nrouter = APIRouter(prefix=\"/analytics\")\n\n@router.get(\"/geodr\", name=\"get_geodr\")\nasync def get_geodr(\n        db: Session = Depends(get_db),\n        cutoff_upper_pct: Optional[Union[int, None]] = 10000,\n        cutoff_lower_pct: Optional[Union[int, None]] = 0,\n        limit: int = 500\n    ):\n    '''\n    Gets geometric daily return\n\n    :params:\n        `limit`: limit number of rows\n        `cutoff_upper_pct`: upper cutoff for extreme values\n        `cutoff_lower_pct`: lower cutoff for extreme values\n    \n    If limit == -1, returns all symbols\n\n    Default limit of 500 rows\n    '''\n    \n    return readers.read_geodr(db, cutoff_upper_pct, cutoff_lower_pct, limit)\n\n@router.get(\"/top20qvlm\", name=\"get_top20qvlm\")\nasync def get_top20qvlm(db: Session = Depends(get_db)):\n    return readers.read_top20qvlm(db)\n\n@router.get(\"/wr\", name=\"get_wr\")\nasync def get_wr(\n        db: Session = Depends(get_db),\n        cutoff_upper_pct: Optional[Union[int, None]] = 10000,\n        cutoff_lower_pct: Optional[Union[int, None]] = 0,\n        limit: int = 500\n    ):\n    '''\n    Gets weekly return\n\n    :params:\n        `limit`: limit number of rows\n        `cutoff_upper_pct`: upper cutoff for extreme values\n        `cutoff_lower_pct`: lower cutoff for extreme values\n\n    If `limit` == -1, returns all symbols\n    \n    Default limit of 500 rows\n    '''\n\n    return readers.read_wr(db, cutoff_upper_pct, cutoff_lower_pct, limit)\n"}
{"type": "source_file", "path": "web/routes/api/ws/utils/connections.py", "content": "# Backend WS API connection utils\n\nfrom typing import List\nfrom fastapi import WebSocket\n\n\nclass WSConnectionManager:\n    '''\n    Websocket connection manager\n    '''\n\n    def __init__(self):\n        self.active_connections: List[WebSocket] = []\n\n    async def connect(self, websocket: WebSocket):\n        await websocket.accept()\n        self.active_connections.append(websocket)\n\n    def disconnect(self, websocket: WebSocket):\n        self.active_connections.remove(websocket)\n\n    # async def send_personal_message(self, message: str, websocket: WebSocket):\n    #     await websocket.send_text(message)\n\n    # async def broadcast(self, message: str):\n    #     for connection in self.active_connections:\n    #         await connection.send_text(message)\n"}
{"type": "source_file", "path": "web/routes/api/rest/utils/parsers.py", "content": "import logging\nfrom common.helpers.numbers import round_decimal\nfrom common.helpers.datetimehelpers import (\n    datetime_to_seconds,\n    datetime_to_milliseconds\n)\n\n\ndef parse_ohlcv(ohlcvs: list, mls: bool) -> list:\n    '''\n    Parses OHLCV received from API endpoint\n        for web chart view or something else\n    \n    :params:\n        `ohlcvs`: list - OHLCVs received\n        `mls`: bool - whether to convert timestamps to milliseconds\n            if true: convert to milliseconds\n            if false: convert to seconds\n    '''\n\n    ret = []\n    default_decimals = 4\n    if ohlcvs:\n        try:\n            # ohlcv.sort(key = lambda x: x.time)\n            ret = [\n                {\n                    'time': int(datetime_to_milliseconds(o.time)) \\\n                        if mls else int(datetime_to_seconds(o.time)),\n                    'open': round_decimal(o.open, default_decimals),\n                    'high': round_decimal(o.high, default_decimals),\n                    'low': round_decimal(o.low, default_decimals),\n                    'close': round_decimal(o.close, default_decimals),\n                    'volume': round_decimal(o.volume, default_decimals)\n                }\n                for o in ohlcvs\n            ]\n        except TypeError as exc:\n            logging.warning(f\"parse_ohlcv: EXCEPTION: {exc}\")\n    return ret\n"}
{"type": "source_file", "path": "web/routes/api/rest/ohlcvs.py", "content": "# Backend REST API endpoint for OHLCV\n\nfrom typing import Optional\nfrom sqlalchemy.orm import Session\nfrom fastapi import APIRouter, Depends\nfrom web.routes.api.deps import get_db\nfrom web.routes.api.rest.utils import readers\n\n\nrouter = APIRouter()\n\n@router.get(\"/ohlcvs\", name=\"get_ohlcvs\")\nasync def get_ohlcvs(\n        exchange: str,\n        base_id: str,\n        quote_id: str,\n        interval: str,\n        start: Optional[int] = None,\n        end: Optional[int] = None,\n        limit: Optional[int] = 500,\n        empty_ts: Optional[bool] = False,\n        results_mls: Optional[bool] = True,\n        db: Session = Depends(get_db)\n    ) -> list:\n    '''\n    API to read OHLCV from database, max 500 data points\n    \n    Can be used for charting\n    \n    :params:\n        `interval`: str - time interval of candles\n            enum:\n            \n                - `1m` for 1 minute\n                - `5m` for 5 minutes\n                - `15m` for 15 minutes\n                - `30m` for 30 minutes\n                - `1h` for 1 hour\n                - `3h` for 3 hours\n                - `6h` for 6 hours\n                - `12h` for 12 hours\n                - `1D` for 1 day\n                - `7D` for 7 days\n                - `14D` for 14 days\n                - `1M` for 1 month \n\n        `results_mls`: bool if result timestamps are in milliseconds\n            if true, result timestamps are in millieconds\n            if false, result timestamps are in seconds\n\n    example of output:\n        ```\n        [{'time': 1629296700000, 'open': 2619.4, 'high': 2619.4, 'low': 2619.4, 'close': 2619.4, 'volume': 0.02230204}, ...]\n        ```\n    '''\n\n    return readers.read_ohlcvs(\n        db, exchange, base_id, quote_id,\n        interval, start, end, limit, empty_ts, results_mls\n    )"}
{"type": "source_file", "path": "web/routes/api/rest/symexch.py", "content": "# Backend API endpoint for OHLCV\n\nfrom typing import Optional\nfrom sqlalchemy.orm import Session\nfrom fastapi import APIRouter, Depends\nfrom fastapi.exceptions import HTTPException\nfrom web.routes.api.deps import get_db\nfrom web.routes.api.rest.utils import readers\n\n\nrouter = APIRouter()\n\n@router.get(\"/symbol-exchange\", name=\"get_symbol_exchange\")\nasync def get_symbol_exchange(\n        db: Session = Depends(get_db)\n    ):\n    '''\n    API to read symbol-exchanges from database\n    '''\n\n    return readers.read_symbol_exchange(db)\n"}
{"type": "source_file", "path": "web/models/analytics.py", "content": "# Tables models for analytic views created in Postgres db\n\nfrom sqlalchemy import (\n    Column, DateTime, Index,\n    Numeric, String, Table, BigInteger\n)\nfrom web.db.base import metadata\n\n# Geometric average daily return\ngeo_daily_return = Table(\n    'geo_daily_return', metadata,\n    Column('exchange', String(100)),\n    Column('base_id', String(20)),\n    Column('quote_id', String(20)),\n    Column('daily_return_pct', Numeric),\n    Index('geo_dr_idx', 'exchange', 'base_id', 'quote_id')\n)\n\n# Top 10 hot commodities(bases)\ntop_20_quoted_vol = Table(\n    'top_20_quoted_vol', metadata,\n    Column('bqgrp', String()),\n    Column('total_volume', Numeric),\n    Index('top_20_qvlm_idx', 'bqgrp')\n)\n\n# Weekly return\nweekly_return = Table(\n    'weekly_return', metadata,\n    Column('time', DateTime(True)),\n    Column('exchange', String(100)),\n    Column('base_id', String(20)),\n    Column('quote_id', String(20)),\n    Column('weekly_return_pct', Numeric),\n    Index('wr_idx', 'exchange', 'base_id', 'quote_id', 'time')\n)\n"}
{"type": "source_file", "path": "web/routes/api/deps.py", "content": "# Dependencies for backend API\n\nimport redis\nfrom typing import Generator\nfrom web.db.session import SessionLocal\nfrom common.config.constants import REDIS_HOST, REDIS_USER, REDIS_PASSWORD\n\n\ndef get_db() -> Generator:\n    db = SessionLocal()\n    try:\n        yield db\n    finally:\n        db.close()\n\ndef get_redis() -> Generator:\n    redis_client = redis.Redis(\n        host=REDIS_HOST,\n        username=REDIS_USER,\n        password=REDIS_PASSWORD,\n        decode_responses=True\n    )\n    try:\n        yield redis_client\n    finally:\n        redis_client.close()\n"}
{"type": "source_file", "path": "web/models/symexch.py", "content": "from sqlalchemy import (\n    Column, Index, String, Text, Boolean\n)\nfrom sqlalchemy.orm import relationship\nfrom common.config.constants import (\n    OHLCVS_TABLE, OHLCVS_ERRORS_TABLE, SYMBOL_EXCHANGE_TABLE\n)\nfrom web.db.base import Base, metadata\n\n\nclass SymbolExchange(Base):\n    __tablename__ = SYMBOL_EXCHANGE_TABLE\n    __table_args__ = (\n        Index('symexch_exch_idx', 'exchange'),\n        Index('symexch_base_idx', 'base_id'),\n        Index('symexch_quote_idx', 'quote_id')\n    )\n\n    exchange = Column(String(100), primary_key=True, nullable=False, index=True)\n    base_id = Column(String(20), primary_key=True, nullable=False, index=True)\n    quote_id = Column(String(20), primary_key=True, nullable=False, index=True)\n    symbol = Column(String(40), nullable=False)\n    is_trading = Column(Boolean, nullable=False)\n"}
{"type": "source_file", "path": "web/models/__init__.py", "content": "from .ohlcvs import (\n    Ohlcv, t_common_basequote_30, t_ohlcvs_summary_daily,\n    t_ohlcvs_summary_5min, t_ohlcvs_summary_15min, t_ohlcvs_summary_30min,\n    t_ohlcvs_summary_1hour, t_ohlcvs_summary_6hour, t_ohlcvs_summary_12hour,\n    t_ohlcvs_summary_7day\n)\nfrom .symexch import SymbolExchange\nfrom .ohlcvs_errors import OhlcvsError\nfrom .testtable import TestTable\nfrom .analytics import geo_daily_return, top_20_quoted_vol, weekly_return\n"}
{"type": "source_file", "path": "web/routes/api/ws/ohlcvs.py", "content": "# Backend WS API endpoint for OHLCV\n\nimport logging\nfrom redis import Redis\nfrom sqlalchemy.orm import Session\nfrom fastapi import APIRouter, Depends, WebSocket\nfrom starlette.websockets import WebSocketDisconnect\nfrom web.config.constants import WS_SEND_EVENT_TYPES\nfrom web.routes.api.deps import get_db, get_redis\nfrom web.routes.api.ws.utils.senders import WSSender\nfrom web.routes.api.ws.utils.connections import WSConnectionManager\n\n\nrouter = APIRouter()\nws_manager = WSConnectionManager()\n\n@router.websocket(\"/ohlcvs\")\nasync def ws_ohlcvs(\n        websocket: WebSocket,\n        redis_client: Redis = Depends(get_redis),\n        db: Session = Depends(get_db)\n    ):\n\n    await ws_manager.connect(websocket)\n    ws_sender = WSSender(ws_manager, websocket, redis_client, db)\n    try:\n        while True:\n            input = await websocket.receive_json()\n            if input:\n                try:\n                    event_type = input['event_type']\n                    data_type = input['data_type']\n                    exchange = input['exchange']\n                    base_id = input['base_id']\n                    quote_id = input['quote_id']\n                    interval = input['interval']\n                    if event_type not in WS_SEND_EVENT_TYPES:\n                        await websocket.send_json({\n                            'detail': \"event_type must be subscribe or unsubscribe\"\n                        })\n                    elif event_type == \"subscribe\":\n                        mls = input['mls'] # only subsribe requires mls\n                        if data_type == \"ohlcv\":\n                            ws_sender.send_ohlcv(\n                                exchange, base_id, quote_id, interval, mls)\n                    elif event_type == \"unsubscribe\":\n                        if data_type == \"ohlcv\":\n                            ws_sender.stopsend_ohlcv(\n                                exchange, base_id, quote_id, interval)\n                            await websocket.send_json({\n                                'detail': \\\n                                    f\"successfully unsubscribed from {exchange}_{base_id}_{quote_id}_{interval}\"})\n                except Exception as exc:\n                    logging.error(f'Websocket OHLCVS: EXCEPTION: {exc}')\n    except WebSocketDisconnect:\n        logging.warning(f\"Websocket OHLCVS: {websocket} disconnected\")\n        ws_manager.disconnect(websocket)\n"}
{"type": "source_file", "path": "web/routes/api/ws/utils/parsers.py", "content": "from common.helpers.datetimehelpers import seconds\n\ndef parse_ohlcv(data: dict, mls: bool=True) -> dict:\n    '''\n    JSON-serializes a single OHLCV datum\n\n    The `time` attribute of the datum must represent a millisecond\n        It can be of type `str` or `int`\n\n    :params:\n        `data`: dict - a single OHLCV datum\n        `mls`: bool - whether return timestamp in milliseconds\n            or seconds\n    '''\n    \n    return {\n        'time': int(data['time']) \\\n            if mls else seconds(int(data['time'])),\n        'open': float(data['open']),\n        'high': float(data['high']),\n        'low': float(data['low']),\n        'close': float(data['close']),\n        'volume': float(data['volume'])\n    }\n"}
{"type": "source_file", "path": "web/routes/api/ws/utils/senders.py", "content": "# Backend WS API senders utils\n\nimport asyncio\nimport logging\nfrom redis import Redis\nfrom typing import List\nfrom sqlalchemy.orm import Session\nfrom fastapi import WebSocket\nfrom common.config.constants import REDIS_DELIMITER\nfrom common.utils.asyncioutils import AsyncLoopThread\nfrom fetchers.helpers.ws import make_serve_redis_key\nfrom web.config.constants import OHLCV_INTERVALS\nfrom web.routes.api.rest.utils.readers import read_ohlcvs\nfrom web.routes.api.ws.utils.parsers import parse_ohlcv\nfrom web.routes.api.ws.utils.connections import WSConnectionManager\n\n\nclass WSSender:\n    '''\n    Websocket sender for clients that view chart\n    '''\n\n    def __init__(\n            self,\n            ws_manager: WSConnectionManager,\n            ws: WebSocket,\n            redis_client: Redis,\n            db: Session\n        ):\n        self.loop_handler = AsyncLoopThread()\n        self.loop_handler.start()\n        self.ws_manager = ws_manager\n        self.ws = ws\n        self.redis_client = redis_client\n        self.db = db\n        self.serving_ids: List[str] = []\n        \n    async def _send_ohlcv(\n            self,\n            exchange: str,\n            base_id: str,\n            quote_id: str,\n            interval: str,\n            mls: bool = True\n        ) -> None:\n        '''\n        Private coroutine that sends OHLCV from Redis hash or REST API\n        \n        Sends OHLCV data every XXXX seconds\n        \n        :params:\n            `mls`: boolean\n                if True: sends timestamps in milliseconds\n                if False: sends timestamps in seconds\n            \n        '''\n\n        if interval not in OHLCV_INTERVALS:\n            await self.ws.send_json({\n                'message': \"interval must be in the determined list\"\n            })\n        \n        # TODO: This serving_id is not unique among different users/clients\n        serving_id = f'ohlcv_{exchange}_{base_id}_{quote_id}_{interval}'\n        self.serving_ids.append(serving_id)\n        \n        while self.ws in self.ws_manager.active_connections and \\\n            serving_id in self.serving_ids:\n            # If `interval` == \"1m\", use \"fresh\" data from Redis,\n            #   otherwise use data from REST API\n            # Sleep between messages according to `interval` as well\n            # TODO: add heartbeat\n            if interval == \"1m\":\n                ws_send_redis_key = make_serve_redis_key(\n                    exchange, base_id, quote_id, REDIS_DELIMITER\n                )\n                data = self.redis_client.hgetall(ws_send_redis_key)\n                if data:\n                    try:\n                        data = parse_ohlcv(data, mls)\n                        await self.ws.send_json(data)\n                    except Exception as exc:\n                        logging.warning(f\"Send OHLCV: EXCEPTION: {exc}\")    \n                await asyncio.sleep(1)\n            else:\n                data = read_ohlcvs(\n                    db = self.db,\n                    exchange = exchange,\n                    base_id = base_id, \n                    quote_id = quote_id,\n                    interval = interval,\n                    limit = 1,\n                    empty_ts = False,\n                    results_mls = True,\n                )\n\n                if data:\n                    try:\n                        data = parse_ohlcv(data[0], mls)\n                        await self.ws.send_json(data)\n                        # logging.info(f\"Send OHLCV: Sending {data}\")\n                    except Exception as exc:\n                        logging.warning(f\"EXCEPTION!! {exc}\")\n\n                if interval == \"5m\":\n                    await asyncio.sleep(5)\n                elif interval == \"15m\":\n                    await asyncio.sleep(15)\n                elif interval == \"30m\":\n                    await asyncio.sleep(30)\n                elif interval == \"1h\":\n                    await asyncio.sleep(60)\n                elif interval == \"6h\":\n                    await asyncio.sleep(360)\n                elif interval == \"12h\":\n                    await asyncio.sleep(720)\n                elif interval == \"1D\":\n                    await asyncio.sleep(1440)\n                elif interval == \"7D\":\n                    await asyncio.sleep(10080)\n   \n    async def _stopsend_ohlcv(\n            self, exchange: str, base_id: str, quote_id: str, interval: str\n        ) -> None:\n        '''\n        Private coroutine that stops serving OHLCV\n            for `exchange`, `base_id`, quote_id`, `interval`\n        '''\n        \n        if interval not in OHLCV_INTERVALS:\n            await self.ws.send_json({\n                'detail': \"interval must be in the determined list\"\n            })\n            return\n        serving_id = f'ohlcv_{exchange}_{base_id}_{quote_id}_{interval}'\n        self.serving_ids.remove(serving_id)\n\n    def send_ohlcv(\n            self,\n            exchange: str,\n            base_id: str,\n            quote_id: str,\n            interval: str,\n            mls: bool = True\n        ) -> None:\n        '''\n        Websocket API for sending OHLCV\n        '''\n\n        asyncio.run_coroutine_threadsafe(\n            self._send_ohlcv(\n                exchange, base_id, quote_id, interval, mls\n            ),\n            self.loop_handler.loop)\n\n    def stopsend_ohlcv(\n            self, exchange: str, base_id: str, quote_id: str, interval: str\n        ) -> None:\n        '''\n        Websocket API for stopping sending OHLCV\n        '''\n        \n        asyncio.run_coroutine_threadsafe(\n            self._stopsend_ohlcv(\n                exchange, base_id, quote_id, interval\n            ),\n            self.loop_handler.loop)\n"}
