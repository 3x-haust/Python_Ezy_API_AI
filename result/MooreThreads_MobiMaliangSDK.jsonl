{"repo_info": {"repo_name": "MobiMaliangSDK", "repo_owner": "MooreThreads", "repo_url": "https://github.com/MooreThreads/MobiMaliangSDK"}}
{"type": "source_file", "path": "examples/controlnets/example_mlsd2img.py", "content": "import torch\nfrom PIL import Image\nimport os\nimport sys\nimport argparse\n\n__dir__ = os.path.dirname(os.path.abspath(__file__))\nsys.path.append(os.path.dirname(os.path.dirname(__dir__)))\nfrom modules.shared import SDModel, Controlnet, device\nfrom modules.wrappers import MLSDdetectorWrapper\nfrom modules.controlnet.mlsd2img import mlsd2img\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument(\n        \"--checkpoint_path\",\n        default=None,\n        type=str,\n        required=True,\n        help=\"Path to the stable diffusion model.\",\n    )\n\n    parser.add_argument(\n        \"--mlsd_controlnet_path\",\n        default=None,\n        type=str,\n        required=True,\n        help=\"Path to the mlsd controlnet model.\",\n    )\n    \n    parser.add_argument(\n        \"--mlsd_processor_model_path\",\n        default=None,\n        type=str,\n        required=True,\n        help=\"Path to the mlsd processor model.\",\n    )\n\n    parser.add_argument(\n        \"--image_path\", default=None, type=str, required=True, help=\"The image path\"\n    )\n\n    parser.add_argument(\n        \"--prompt\",\n        default=None,\n        type=str,\n        required=True,\n        help=\"Describe the image content with text\",\n    )\n\n    parser.add_argument(\n        \"--output_path\",\n        default=\"./\",\n        type=str,\n        required=True,\n        help=\"Result folder path\",\n    )\n\n    parser.add_argument(\n        \"--negative_prompt\",\n        default=None,\n        type=str,\n        required=False,\n        help=\"Content which you don't want to show up with text\",\n    )\n\n    parser.add_argument(\n        \"--diffusers_format\",\n        action=\"store_true\",\n        required=False,\n        help=\"Model files in diffusers format\",\n    )\n    parser.add_argument(\n        \"--seed\", default=2023, required=False, type=int, help=\"Random seed\"\n    )\n\n    args = parser.parse_args()\n\n    image = Image.open(args.image_path).convert(\"RGB\").resize((512, 512))\n    sd_model = SDModel(device=device)\n    controlnet_model = Controlnet(device=sd_model.device)\n    processor = MLSDdetectorWrapper(device=\"cpu\")\n    sd_model.load_models(args.checkpoint_path, diffusers_format=args.diffusers_format)\n    controlnet_model.load_models(args.mlsd_controlnet_path, diffusers_format=True)\n    processor.load_models(args.mlsd_processor_model_path)\n\n    images, _, mlsd_image = mlsd2img(\n        sd_model,\n        controlnet_model,\n        processor,\n        ori_image=image,\n        prompt=args.prompt,\n        negative_prompt=args.negative_prompt,\n        seed=args.seed,\n    )\n\n    os.makedirs(args.output_path, exist_ok=True)\n    images[0].save(os.path.join(args.output_path, \"mlsd2img.png\"))\n    mlsd_image.save(os.path.join(args.output_path, \"mlsd.png\"))\n\n# python examples/controlnets/example_mlsd2img.py --checkpoint_path models/Stable-diffusion/v1-5-pruned-emaonly.ckpt --mlsd_controlnet_path models/controlnet/mlsd_v11 --mlsd_processor_model_path models/controlnet/annotators --image_path data/person.jpg --prompt \"a pretty girl\" --output_path outputs\n"}
{"type": "source_file", "path": "examples/controlnets/example_scribble2img.py", "content": "import torch\nfrom PIL import Image\nimport os\nimport sys\nimport argparse\n\n__dir__ = os.path.dirname(os.path.abspath(__file__))\nsys.path.append(os.path.dirname(os.path.dirname(__dir__)))\nfrom modules.shared import SDModel, Controlnet, device\nfrom modules.wrappers import HEDdetectorWrapper\nfrom modules.controlnet.scribble2img import scribble2img\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument(\n        \"--checkpoint_path\",\n        default=None,\n        type=str,\n        required=True,\n        help=\"Path to the stable diffusion model.\",\n    )\n\n    parser.add_argument(\n        \"--scribble_controlnet_path\",\n        default=None,\n        type=str,\n        required=True,\n        help=\"Path to the scribble controlnet model.\",\n    )\n    \n    parser.add_argument(\n        \"--scribble_processor_model_path\",\n        default=None,\n        type=str,\n        required=True,\n        help=\"Path to the scribble processor model.\",\n    )\n\n    parser.add_argument(\n        \"--image_path\", default=None, type=str, required=True, help=\"The image path\"\n    )\n\n    parser.add_argument(\n        \"--prompt\",\n        default=None,\n        type=str,\n        required=True,\n        help=\"Describe the image content with text\",\n    )\n\n    parser.add_argument(\n        \"--output_path\",\n        default=\"./\",\n        type=str,\n        required=True,\n        help=\"Result folder path\",\n    )\n\n    parser.add_argument(\n        \"--negative_prompt\",\n        default=None,\n        type=str,\n        required=False,\n        help=\"Content which you don't want to show up with text\",\n    )\n\n    parser.add_argument(\n        \"--diffusers_format\",\n        action=\"store_true\",\n        required=False,\n        help=\"Model files in diffusers format\",\n    )\n    parser.add_argument(\n        \"--seed\", default=2023, required=False, type=int, help=\"Random seed\"\n    )\n\n    args = parser.parse_args()\n\n    image = Image.open(args.image_path).convert(\"RGB\").resize((512, 512))\n    sd_model = SDModel(device=device)\n    controlnet_model = Controlnet(device=sd_model.device)\n    processor = HEDdetectorWrapper(device=sd_model.device)\n    sd_model.load_models(args.checkpoint_path, diffusers_format=args.diffusers_format)\n    controlnet_model.load_models(args.scribble_controlnet_path, diffusers_format=True)\n    processor.load_models(args.scribble_processor_model_path)\n\n    images, _, scribble_image = scribble2img(\n        sd_model,\n        controlnet_model,\n        processor,\n        ori_image=image,\n        prompt=args.prompt,\n        negative_prompt=args.negative_prompt,\n        seed=args.seed,\n    )\n\n    os.makedirs(args.output_path, exist_ok=True)\n    images[0].save(os.path.join(args.output_path, \"scribble2img.png\"))\n    scribble_image.save(os.path.join(args.output_path, \"scribble.png\"))\n\n# python examples/controlnets/example_scribble2img.py --checkpoint_path models/Stable-diffusion/v1-5-pruned-emaonly.ckpt --scribble_controlnet_path models/controlnet/scribble_v11 --scribble_processor_model_path models/controlnet/annotators --image_path data/person.jpg --prompt \"a pretty girl\" --output_path outputs"}
{"type": "source_file", "path": "examples/controlnets/example_normalbae2img.py", "content": "import torch\nfrom PIL import Image\nimport os\nimport sys\nimport argparse\n\n__dir__ = os.path.dirname(os.path.abspath(__file__))\nsys.path.append(os.path.dirname(os.path.dirname(__dir__)))\nfrom modules.shared import SDModel, Controlnet, device\nfrom modules.wrappers import NormalBaeDetectorWrapper\nfrom modules.controlnet.normalbae2img import normalbae2img\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument(\n        \"--checkpoint_path\",\n        default=None,\n        type=str,\n        required=True,\n        help=\"Path to the stable diffusion model.\",\n    )\n\n    parser.add_argument(\n        \"--normalbae_controlnet_path\",\n        default=None,\n        type=str,\n        required=True,\n        help=\"Path to the normalbae controlnet model.\",\n    )\n    \n    parser.add_argument(\n        \"--normalbae_processor_model_path\",\n        default=None,\n        type=str,\n        required=True,\n        help=\"Path to the normalbae processor model.\",\n    )\n\n    parser.add_argument(\n        \"--image_path\", default=None, type=str, required=True, help=\"The image path\"\n    )\n\n    parser.add_argument(\n        \"--prompt\",\n        default=None,\n        type=str,\n        required=True,\n        help=\"Describe the image content with text\",\n    )\n\n    parser.add_argument(\n        \"--output_path\",\n        default=\"./\",\n        type=str,\n        required=True,\n        help=\"Result folder path\",\n    )\n\n    parser.add_argument(\n        \"--negative_prompt\",\n        default=None,\n        type=str,\n        required=False,\n        help=\"Content which you don't want to show up with text\",\n    )\n\n    parser.add_argument(\n        \"--diffusers_format\",\n        action=\"store_true\",\n        required=False,\n        help=\"Model files in diffusers format\",\n    )\n    parser.add_argument(\n        \"--seed\", default=2023, required=False, type=int, help=\"Random seed\"\n    )\n\n    args = parser.parse_args()\n\n    image = Image.open(args.image_path).convert(\"RGB\").resize((512, 512))\n    sd_model = SDModel(device=device)\n    controlnet_model = Controlnet(device=sd_model.device)\n    processor = NormalBaeDetectorWrapper(\"cpu\")\n    sd_model.load_models(args.checkpoint_path, diffusers_format=args.diffusers_format)\n    controlnet_model.load_models(args.normalbae_controlnet_path, diffusers_format=True)\n    processor.load_models(args.normalbae_processor_model_path)\n\n    images, _, normalbae_image = normalbae2img(\n        sd_model,\n        controlnet_model,\n        processor,\n        ori_image=image,\n        prompt=args.prompt,\n        negative_prompt=args.negative_prompt,\n        seed=args.seed,\n    )\n\n    os.makedirs(args.output_path, exist_ok=True)\n    images[0].save(os.path.join(args.output_path, \"normalbae2img.png\"))\n    normalbae_image.save(os.path.join(args.output_path, \"normalbae.png\"))\n\n# python examples/controlnets/example_normalbae2img.py --checkpoint_path models/Stable-diffusion/v1-5-pruned-emaonly.ckpt --normalbae_controlnet_path models/controlnet/normalbae_v11 --normalbae_processor_model_path models/controlnet/annotators --image_path data/person.jpg --prompt \"a pretty girl\" --output_path outputs"}
{"type": "source_file", "path": "examples/controlnets/example_ip2p2img.py", "content": "import torch\nfrom PIL import Image\nimport os\nimport sys\nimport argparse\n\n__dir__ = os.path.dirname(os.path.abspath(__file__))\nsys.path.append(os.path.dirname(os.path.dirname(__dir__)))\nfrom modules.shared import SDModel, Controlnet, device\nfrom modules.controlnet.ip2p2img import ip2p2imgdevice\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument(\n        \"--checkpoint_path\",\n        default=None,\n        type=str,\n        required=True,\n        help=\"Path to the stable diffusion model.\",\n    )\n\n    parser.add_argument(\n        \"--ip2p_controlnet_path\",\n        default=None,\n        type=str,\n        required=True,\n        help=\"Path to the ip2p controlnet model.\",\n    )\n\n    parser.add_argument(\n        \"--image_path\", default=None, type=str, required=True, help=\"The image path\"\n    )\n\n    parser.add_argument(\n        \"--prompt\",\n        default=None,\n        type=str,\n        required=True,\n        help=\"Describe the image content with text\",\n    )\n\n    parser.add_argument(\n        \"--output_path\",\n        default=\"./\",\n        type=str,\n        required=True,\n        help=\"Result folder path\",\n    )\n\n    parser.add_argument(\n        \"--negative_prompt\",\n        default=None,\n        type=str,\n        required=False,\n        help=\"Content which you don't want to show up with text\",\n    )\n\n    parser.add_argument(\n        \"--diffusers_format\",\n        action=\"store_true\",\n        required=False,\n        help=\"Model files in diffusers format\",\n    )\n    parser.add_argument(\n        \"--seed\", default=2023, required=False, type=int, help=\"Random seed\"\n    )\n\n    args = parser.parse_args()\n\n    image = Image.open(args.image_path).convert(\"RGB\").resize((512, 512))\n    sd_model = SDModel(device=device)\n    controlnet_model = Controlnet(device=sd_model.device)\n    sd_model.load_models(args.checkpoint_path, diffusers_format=args.diffusers_format)\n    controlnet_model.load_models(args.ip2p_controlnet_path, diffusers_format=True)\n\n    images, _, ip2p_image = ip2p2img(\n        sd_model,\n        controlnet_model,\n        ori_image=image,\n        prompt=args.prompt,\n        negative_prompt=args.negative_prompt,\n        seed=args.seed,\n    )\n\n    os.makedirs(args.output_path, exist_ok=True)\n    images[0].save(os.path.join(args.output_path, \"ip2p2img.png\"))\n    ip2p_image.save(os.path.join(args.output_path, \"ip2p.png\"))\n    \n# python examples/controlnets/example_ip2p2img.py --checkpoint_path models/Stable-diffusion/v1-5-pruned-emaonly.ckpt --ip2p_controlnet_path models/controlnet/ip2p_v11 --image_path data/person.jpg --prompt \"a pretty girl\" --output_path outputs\n    "}
{"type": "source_file", "path": "examples/controlnets/example_pose2img.py", "content": "import torch\nfrom PIL import Image\nimport os\nimport sys\nimport argparse\n\n__dir__ = os.path.dirname(os.path.abspath(__file__))\nsys.path.append(os.path.dirname(os.path.dirname(__dir__)))\nfrom modules.shared import SDModel, Controlnet, device\nfrom modules.wrappers import OpenposeDetectorWrapper\nfrom modules.controlnet.pose2img import pose2img\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument(\n        \"--checkpoint_path\",\n        default=None,\n        type=str,\n        required=True,\n        help=\"Path to the stable diffusion model.\",\n    )\n\n    parser.add_argument(\n        \"--openpose_controlnet_path\",\n        default=None,\n        type=str,\n        required=True,\n        help=\"Path to the openpose controlnet model.\",\n    )\n    \n    parser.add_argument(\n        \"--openpose_processor_model_path\",\n        default=None,\n        type=str,\n        required=True,\n        help=\"Path to the openpose processor model.\",\n    )\n\n    parser.add_argument(\n        \"--image_path\", default=None, type=str, required=True, help=\"The image path\"\n    )\n\n    parser.add_argument(\n        \"--prompt\",\n        default=None,\n        type=str,\n        required=True,\n        help=\"Describe the image content with text\",\n    )\n\n    parser.add_argument(\n        \"--output_path\",\n        default=\"./\",\n        type=str,\n        required=True,\n        help=\"Result folder path\",\n    )\n\n    parser.add_argument(\n        \"--negative_prompt\",\n        default=None,\n        type=str,\n        required=False,\n        help=\"Content which you don't want to show up with text\",\n    )\n\n    parser.add_argument(\n        \"--diffusers_format\",\n        action=\"store_true\",\n        required=False,\n        help=\"Model files in diffusers format\",\n    )\n    parser.add_argument(\n        \"--seed\", default=2023, required=False, type=int, help=\"Random seed\"\n    )\n\n    args = parser.parse_args()\n\n    image = Image.open(args.image_path).convert(\"RGB\").resize((512, 512))\n    sd_model = SDModel(device=device)\n    controlnet_model = Controlnet(device=sd_model.device)\n    processor = OpenposeDetectorWrapper(sd_model.device)\n    sd_model.load_models(args.checkpoint_path, diffusers_format=args.diffusers_format)\n    controlnet_model.load_models(args.openpose_controlnet_path, diffusers_format=True)\n    processor.load_models(args.openpose_processor_model_path)\n\n    images, _, openpose_image = pose2img(\n        sd_model,\n        controlnet_model,\n        processor,\n        ori_image=image,\n        prompt=args.prompt,\n        negative_prompt=args.negative_prompt,\n        seed=args.seed,\n    )\n\n    os.makedirs(args.output_path, exist_ok=True)\n    images[0].save(os.path.join(args.output_path, \"openpose2img.png\"))\n    openpose_image.save(os.path.join(args.output_path, \"openpose.png\"))\n\n# python examples/controlnets/example_pose2img.py --checkpoint_path models/Stable-diffusion/v1-5-pruned-emaonly.ckpt --openpose_controlnet_path models/controlnet/openpose_v11 --openpose_processor_model_path models/controlnet/annotators --image_path data/person.jpg --prompt \"a pretty girl\" --output_path outputs"}
{"type": "source_file", "path": "examples/controlnets/example_tile2img.py", "content": "import torch\nfrom PIL import Image\nimport os\nimport sys\nimport argparse\n\n__dir__ = os.path.dirname(os.path.abspath(__file__))\nsys.path.append(os.path.dirname(os.path.dirname(__dir__)))\nfrom modules.shared import SDModel, Controlnet, device\nfrom modules.wrappers import ResizeForConditionalImageWrapper\nfrom modules.controlnet.tile2img import tile2img\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument(\n        \"--checkpoint_path\",\n        default=None,\n        type=str,\n        required=True,\n        help=\"Path to the stable diffusion model.\",\n    )\n\n    parser.add_argument(\n        \"--tile_controlnet_path\",\n        default=None,\n        type=str,\n        required=True,\n        help=\"Path to the tile controlnet model.\",\n    )\n\n    parser.add_argument(\n        \"--image_path\", default=None, type=str, required=True, help=\"The image path\"\n    )\n\n    parser.add_argument(\n        \"--prompt\",\n        default=None,\n        type=str,\n        required=True,\n        help=\"Describe the image content with text\",\n    )\n\n    parser.add_argument(\n        \"--output_path\",\n        default=\"./\",\n        type=str,\n        required=True,\n        help=\"Result folder path\",\n    )\n\n    parser.add_argument(\n        \"--negative_prompt\",\n        default=None,\n        type=str,\n        required=False,\n        help=\"Content which you don't want to show up with text\",\n    )\n\n    parser.add_argument(\n        \"--diffusers_format\",\n        action=\"store_true\",\n        required=False,\n        help=\"Model files in diffusers format\",\n    )\n    parser.add_argument(\n        \"--seed\", default=2023, required=False, type=int, help=\"Random seed\"\n    )\n\n    args = parser.parse_args()\n\n    image = Image.open(args.image_path).convert(\"RGB\").resize((512, 512))\n    sd_model = SDModel(device=device)\n    controlnet_model = Controlnet(device=sd_model.device)\n    processor = ResizeForConditionalImageWrapper()\n    sd_model.load_models(args.checkpoint_path, diffusers_format=args.diffusers_format)\n    controlnet_model.load_models(args.tile_controlnet_path, diffusers_format=True)\n\n    images, _, tile_image = tile2img(\n        sd_model,\n        controlnet_model,\n        processor,\n        ori_image=image,\n        prompt=args.prompt,\n        negative_prompt=args.negative_prompt,\n        seed=args.seed,\n    )\n\n    os.makedirs(args.output_path, exist_ok=True)\n    images[0].save(os.path.join(args.output_path, \"tile2img.png\"))\n    tile_image.save(os.path.join(args.output_path, \"tile.png\"))\n    \n    # python examples/controlnets/example_tile2img.py --checkpoint_path models/Stable-diffusion/v1-5-pruned-emaonly.ckpt --tile_controlnet_path models/controlnet/tile_v11 --image_path data/person.jpg --prompt \"a pretty girl\" --output_path outputs"}
{"type": "source_file", "path": "backend/server.py", "content": "import torch\nimport uvicorn\nimport os\nimport sys\nimport time\nfrom fastapi import File, UploadFile, Request, FastAPI\nfrom fastapi import Request\nimport json\n\n__dir__ = os.path.dirname(os.path.abspath(__file__))\nsys.path.append(os.path.join(os.path.dirname(__dir__)))\n\nfrom modules.shared import SDModel, device\nfrom modules.basic.txt2img import txt2img\n\nfrom modules.tools.image_utils import image2base64, base642image\nfrom modules.tools.loggers import server_logger\n\nMODEL_DIR = \"models\"\nBASE_MODEL_DIR = os.path.join(MODEL_DIR, \"Stable-diffusion/ml-person-1.2.safetensors\")\nLORA_PATH = os.path.join(MODEL_DIR, \"models/lora_models/blindbox_v1_mix.safetensors\")\n\nsd_model = SDModel(device=device, requires_safety_checker=False)\nsd_model.load_models(BASE_MODEL_DIR, diffusers_format=False)\napp = FastAPI()\n\n@app.post(\"/text2image\")\nasync def text2image(request: Request):\n    try:\n        params = await request.json()\n        prompt = params.pop(\"prompt\", \"masterpiece, best quaility\")\n        seed = params.pop(\"seed\", 0)\n        images, status = txt2img(\n            sd_model,\n            prompt=prompt,\n            seed=seed,\n            **params\n        )\n        decoded_images = [image2base64(image) for image in images]\n        rsp = {\"images\": decoded_images, \"message\":\"ok\"}\n        \n    except Exception as e:\n        server_logger.error(\" ------ {}\".format(e))\n        rsp = {\"images\": [], \"message\": f\"err: {e}\"}\n    \n    return rsp\n\n@app.post(\"/lora2image\")\nasync def lora2image(request: Request):\n    try:\n        params = await request.json()\n        lora_path = params.pop(\"lora_path\", LORA_PATH)\n        sd_model.load_lora(lora_path, alpha=1)\n        assert lora_path is not None\n        \n        prompt = params.pop(\"prompt\", \"masterpiece, best quaility\")\n        seed = params.pop(\"seed\", 0)\n        images, status  = txt2img(\n            sd_model,\n            prompt,\n            seed=seed,\n            lora_model_name_or_path_or_dict=LORA_PATH,\n            lora_alpha=0.75,\n            **params\n        )\n        decoded_images = [image2base64(image) for image in images]\n        \n        rsp = {\"images\": decoded_images, \"message\":\"ok\"}\n    except Exception as e:\n        server_logger.error(\" ------ {}\".format(e))\n        rsp = {\"images\": [], \"message\": f\"err: {e}\"}\n    return rsp\n\nif __name__ == '__main__':\n    uvicorn.run(app, port=1001, host=\"0.0.0.0\")"}
{"type": "source_file", "path": "examples/controlnets/example_shuffle2img.py", "content": "import torch\nfrom PIL import Image\nimport os\nimport sys\nimport argparse\n\n__dir__ = os.path.dirname(os.path.abspath(__file__))\nsys.path.append(os.path.dirname(os.path.dirname(__dir__)))\nfrom modules.shared import SDModel, Controlnet, device\nfrom modules.wrappers import ContentShuffleDetectorWrapper\nfrom modules.controlnet.shuffle2img import shuffle2img\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument(\n        \"--checkpoint_path\",\n        default=None,\n        type=str,\n        required=True,\n        help=\"Path to the stable diffusion model.\",\n    )\n\n    parser.add_argument(\n        \"--shuffle_controlnet_path\",\n        default=None,\n        type=str,\n        required=True,\n        help=\"Path to the shuffle controlnet model.\",\n    )\n\n    parser.add_argument(\n        \"--image_path\", default=None, type=str, required=True, help=\"The image path\"\n    )\n\n    parser.add_argument(\n        \"--prompt\",\n        default=None,\n        type=str,\n        required=True,\n        help=\"Describe the image content with text\",\n    )\n\n    parser.add_argument(\n        \"--output_path\",\n        default=\"./\",\n        type=str,\n        required=True,\n        help=\"Result folder path\",\n    )\n\n    parser.add_argument(\n        \"--negative_prompt\",\n        default=None,\n        type=str,\n        required=False,\n        help=\"Content which you don't want to show up with text\",\n    )\n\n    parser.add_argument(\n        \"--diffusers_format\",\n        action=\"store_true\",\n        required=False,\n        help=\"Model files in diffusers format\",\n    )\n    parser.add_argument(\n        \"--seed\", default=2023, required=False, type=int, help=\"Random seed\"\n    )\n\n    args = parser.parse_args()\n\n    image = Image.open(args.image_path).convert(\"RGB\").resize((512, 512))\n    sd_model = SDModel(device=device)\n    controlnet_model = Controlnet(device=sd_model.device)\n    processor = ContentShuffleDetectorWrapper()\n    sd_model.load_models(args.checkpoint_path, diffusers_format=args.diffusers_format)\n    controlnet_model.load_models(args.shuffle_controlnet_path, diffusers_format=True)\n\n    images, _, shuffle_image = shuffle2img(\n        sd_model,\n        controlnet_model,\n        processor,\n        ori_image=image,\n        prompt=args.prompt,\n        negative_prompt=args.negative_prompt,\n        seed=args.seed,\n    )\n\n    os.makedirs(args.output_path, exist_ok=True)\n    images[0].save(os.path.join(args.output_path, \"shuffle2img.png\"))\n    shuffle_image.save(os.path.join(args.output_path, \"shuffle.png\"))\n    \n    # python examples/controlnets/example_shuffle2img.py --checkpoint_path models/Stable-diffusion/v1-5-pruned-emaonly.ckpt --shuffle_controlnet_path models/controlnet/shuffle_v11 --image_path data/person.jpg --prompt \"a pretty girl\" --output_path outputs"}
{"type": "source_file", "path": "examples/controlnets/example_canny2img.py", "content": "import torch\nfrom PIL import Image\nimport os\nimport sys\nimport argparse\n\n__dir__ = os.path.dirname(os.path.abspath(__file__))\nsys.path.append(os.path.dirname(os.path.dirname(__dir__)))\nfrom modules.shared import SDModel, Controlnet, device\nfrom modules.wrappers import CannyDetectorWrapper\nfrom modules.controlnet.canny2img import canny2img\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument(\n        \"--checkpoint_path\",\n        default=None,\n        type=str,\n        required=True,\n        help=\"Path to the stable diffusion model.\",\n    )\n\n    parser.add_argument(\n        \"--canny_controlnet_path\",\n        default=None,\n        type=str,\n        required=True,\n        help=\"Path to the canny controlnet model.\",\n    )\n\n    parser.add_argument(\n        \"--image_path\", default=None, type=str, required=True, help=\"The image path\"\n    )\n\n    parser.add_argument(\n        \"--prompt\",\n        default=None,\n        type=str,\n        required=True,\n        help=\"Describe the image content with text\",\n    )\n\n    parser.add_argument(\n        \"--output_path\",\n        default=\"./\",\n        type=str,\n        required=True,\n        help=\"Result folder path\",\n    )\n\n    parser.add_argument(\n        \"--negative_prompt\",\n        default=None,\n        type=str,\n        required=False,\n        help=\"Content which you don't want to show up with text\",\n    )\n\n    parser.add_argument(\n        \"--diffusers_format\",\n        action=\"store_true\",\n        required=False,\n        help=\"Model files in diffusers format\",\n    )\n    parser.add_argument(\n        \"--seed\", default=2023, required=False, type=int, help=\"Random seed\"\n    )\n\n    args = parser.parse_args()\n\n    image = Image.open(args.image_path).convert(\"RGB\").resize((512, 512))\n    sd_model = SDModel(device=device)\n    controlnet_model = Controlnet(device=sd_model.device)\n    processor = CannyDetectorWrapper()\n    sd_model.load_models(args.checkpoint_path, diffusers_format=args.diffusers_format)\n    controlnet_model.load_models(args.canny_controlnet_path, diffusers_format=True)\n\n    images, _, canny_image = canny2img(\n        sd_model,\n        controlnet_model,\n        processor,\n        ori_image=image,\n        prompt=args.prompt,\n        negative_prompt=args.negative_prompt,\n        seed=args.seed,\n    )\n\n    os.makedirs(args.output_path, exist_ok=True)\n    images[0].save(os.path.join(args.output_path, \"canny2img.png\"))\n    canny_image.save(os.path.join(args.output_path, \"canny.png\"))\n# python examples/controlnets/example_shuffle2img.py --checkpoint_path models/Stable-diffusion/v1-5-pruned-emaonly.ckpt --shuffle_controlnet_path models/controlnet/shuffle_v11 --image_path data/person.jpg --prompt \"a pretty girl\" --output_path outputs"}
{"type": "source_file", "path": "examples/controlnets/example_softedge2img.py", "content": "import torch\nfrom PIL import Image\nimport os\nimport sys\nimport argparse\n\n__dir__ = os.path.dirname(os.path.abspath(__file__))\nsys.path.append(os.path.dirname(os.path.dirname(__dir__)))\nfrom modules.shared import SDModel, Controlnet, device\nfrom modules.wrappers import PidiNetDetectorWrapper\nfrom modules.controlnet.softedge2img import softedge2img\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument(\n        \"--checkpoint_path\",\n        default=None,\n        type=str,\n        required=True,\n        help=\"Path to the stable diffusion model.\",\n    )\n\n    parser.add_argument(\n        \"--softedge_controlnet_path\",\n        default=None,\n        type=str,\n        required=True,\n        help=\"Path to the softedge controlnet model.\",\n    )\n    \n    parser.add_argument(\n        \"--softedge_processor_model_path\",\n        default=None,\n        type=str,\n        required=True,\n        help=\"Path to the softedge processor model.\",\n    )\n\n    parser.add_argument(\n        \"--image_path\", default=None, type=str, required=True, help=\"The image path\"\n    )\n\n    parser.add_argument(\n        \"--prompt\",\n        default=None,\n        type=str,\n        required=True,\n        help=\"Describe the image content with text\",\n    )\n\n    parser.add_argument(\n        \"--output_path\",\n        default=\"./\",\n        type=str,\n        required=True,\n        help=\"Result folder path\",\n    )\n\n    parser.add_argument(\n        \"--negative_prompt\",\n        default=None,\n        type=str,\n        required=False,\n        help=\"Content which you don't want to show up with text\",\n    )\n\n    parser.add_argument(\n        \"--diffusers_format\",\n        action=\"store_true\",\n        required=False,\n        help=\"Model files in diffusers format\",\n    )\n    parser.add_argument(\n        \"--seed\", default=2023, required=False, type=int, help=\"Random seed\"\n    )\n\n    args = parser.parse_args()\n\n    image = Image.open(args.image_path).convert(\"RGB\").resize((512, 512))\n    sd_model = SDModel(device=device)\n    controlnet_model = Controlnet(device=sd_model.device)\n    processor = PidiNetDetectorWrapper(device=\"cpu\")\n    sd_model.load_models(args.checkpoint_path, diffusers_format=args.diffusers_format)\n    controlnet_model.load_models(args.softedge_controlnet_path, diffusers_format=True)\n    processor.load_models(args.softedge_processor_model_path)\n\n    images, _, softedge_image = softedge2img(\n        sd_model,\n        controlnet_model,\n        processor,\n        ori_image=image,\n        prompt=args.prompt,\n        negative_prompt=args.negative_prompt,\n        seed=args.seed,\n    )\n\n    os.makedirs(args.output_path, exist_ok=True)\n    images[0].save(os.path.join(args.output_path, \"softedge2img.png\"))\n    softedge_image.save(os.path.join(args.output_path, \"softedge.png\"))\n\n# python examples/controlnets/example_softedge2img.py --checkpoint_path models/Stable-diffusion/v1-5-pruned-emaonly.ckpt --softedge_controlnet_path models/controlnet/softedge_v11 --softedge_processor_model_path models/controlnet/annotators --image_path data/person.jpg --prompt \"a pretty girl\" --output_path outputs"}
{"type": "source_file", "path": "examples/controlnets/example_lineart2img.py", "content": "import torch\nfrom PIL import Image\nimport os\nimport sys\nimport argparse\n\n__dir__ = os.path.dirname(os.path.abspath(__file__))\nsys.path.append(os.path.dirname(os.path.dirname(__dir__)))\nfrom modules.shared import SDModel, Controlnet, device\nfrom modules.wrappers import LineartDetectorWrapper\nfrom modules.controlnet.lineart2img import lineart2img\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument(\n        \"--checkpoint_path\",\n        default=None,\n        type=str,\n        required=True,\n        help=\"Path to the stable diffusion model.\",\n    )\n\n    parser.add_argument(\n        \"--lineart_controlnet_path\",\n        default=None,\n        type=str,\n        required=True,\n        help=\"Path to the lineart controlnet model.\",\n    )\n    \n    parser.add_argument(\n        \"--lineart_processor_model_path\",\n        default=None,\n        type=str,\n        required=True,\n        help=\"Path to the lineart processor model.\",\n    )\n\n    parser.add_argument(\n        \"--image_path\", default=None, type=str, required=True, help=\"The image path\"\n    )\n\n    parser.add_argument(\n        \"--prompt\",\n        default=None,\n        type=str,\n        required=True,\n        help=\"Describe the image content with text\",\n    )\n\n    parser.add_argument(\n        \"--output_path\",\n        default=\"./\",\n        type=str,\n        required=True,\n        help=\"Result folder path\",\n    )\n\n    parser.add_argument(\n        \"--negative_prompt\",\n        default=None,\n        type=str,\n        required=False,\n        help=\"Content which you don't want to show up with text\",\n    )\n\n    parser.add_argument(\n        \"--diffusers_format\",\n        action=\"store_true\",\n        required=False,\n        help=\"Model files in diffusers format\",\n    )\n    parser.add_argument(\n        \"--seed\", default=2023, required=False, type=int, help=\"Random seed\"\n    )\n\n    args = parser.parse_args()\n\n    image = Image.open(args.image_path).convert(\"RGB\").resize((512, 512))\n    sd_model = SDModel(device=device)\n    controlnet_model = Controlnet(device=sd_model.device)\n    processor = LineartDetectorWrapper(device=sd_model.device)\n    sd_model.load_models(args.checkpoint_path, diffusers_format=args.diffusers_format)\n    controlnet_model.load_models(args.lineart_controlnet_path, diffusers_format=True)\n    processor.load_models(args.lineart_processor_model_path)\n\n    images, _, lineart_image = lineart2img(\n        sd_model,\n        controlnet_model,\n        processor,\n        ori_image=image,\n        prompt=args.prompt,\n        negative_prompt=args.negative_prompt,\n        seed=args.seed,\n    )\n\n    os.makedirs(args.output_path, exist_ok=True)\n    images[0].save(os.path.join(args.output_path, \"lineart2img.png\"))\n    lineart_image.save(os.path.join(args.output_path, \"lineart.png\"))\n\n# python examples/controlnets/example_lineart2img.py --checkpoint_path models/Stable-diffusion/v1-5-pruned-emaonly.ckpt --lineart_controlnet_path models/controlnet/lineart_v11 --lineart_processor_model_path models/controlnet/annotators --image_path data/person.jpg --prompt \"a pretty girl\" --output_path outputs\n"}
{"type": "source_file", "path": "backend/agents/txt2img.py", "content": "import requests\nimport json\nimport os\nimport sys\nfrom PIL import Image\n\n__dir__ = os.path.dirname(os.path.abspath(__file__))\nsys.path.append(os.path.join(os.path.dirname(__dir__)))\n\nfrom modules.tools.image_utils import image2base64, base642image\n\ndata = {\n    \"prompt\": \"a pretty puppy\", \n    \"negative_prompt\": \"ugly, low quaility, blur\",\n    \"seed\": 2023\n}\n# fill in your ip address and port like 0.0.0.0:1001\nresponse = requests.post('http://x.x.x.x:x/text2image', json=data)\nresult = json.loads(response.content.decode(\"utf-8\"))\nbase64_str = result[\"images\"][0]\nimage = base642image(base64_str)\nimage.save(\"outputs/agent_txt2img.jpg\")"}
{"type": "source_file", "path": "examples/controlnets/example_inpaint2img.py", "content": "import torch\nfrom PIL import Image\nimport os\nimport sys\nimport argparse\n\n__dir__ = os.path.dirname(os.path.abspath(__file__))\nsys.path.append(os.path.dirname(os.path.dirname(__dir__)))\nfrom modules.shared import SDModel, Controlnet, device\nfrom modules.wrappers import InpaintingConditionalImageWrapper\nfrom modules.controlnet.inpaint2img import inpaint2img\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument(\n        \"--checkpoint_path\",\n        default=None,\n        type=str,\n        required=True,\n        help=\"Path to the stable diffusion model.\",\n    )\n\n    parser.add_argument(\n        \"--inpainting_controlnet_path\",\n        default=None,\n        type=str,\n        required=True,\n        help=\"Path to the inpainting controlnet model.\",\n    )\n\n    parser.add_argument(\n        \"--image_path\", default=None, type=str, required=True, help=\"The image path\"\n    )\n    \n    parser.add_argument(\n        \"--mask_image_path\", default=None, type=str, required=True, help=\"The mask image path\"\n    )\n\n    parser.add_argument(\n        \"--prompt\",\n        default=None,\n        type=str,\n        required=True,\n        help=\"Describe the image content with text\",\n    )\n\n    parser.add_argument(\n        \"--output_path\",\n        default=\"./\",\n        type=str,\n        required=True,\n        help=\"Result folder path\",\n    )\n\n    parser.add_argument(\n        \"--negative_prompt\",\n        default=None,\n        type=str,\n        required=False,\n        help=\"Content which you don't want to show up with text\",\n    )\n\n    parser.add_argument(\n        \"--diffusers_format\",\n        action=\"store_true\",\n        required=False,\n        help=\"Model files in diffusers format\",\n    )\n    parser.add_argument(\n        \"--seed\", default=2023, required=False, type=int, help=\"Random seed\"\n    )\n\n    args = parser.parse_args()\n\n    image = Image.open(args.image_path).convert(\"RGB\").resize((512, 512))\n    mask_iamge = Image.open(args.mask_image_path).convert(\"RGB\").resize((512, 512))\n    sd_model = SDModel(device=device)\n    controlnet_model = Controlnet(device=sd_model.device)\n    processor = InpaintingConditionalImageWrapper()\n    sd_model.load_models(args.checkpoint_path, diffusers_format=args.diffusers_format)\n    controlnet_model.load_models(args.inpainting_controlnet_path, diffusers_format=True)\n\n    images, _, inpainting_image = inpaint2img(\n        sd_model,\n        controlnet_model,\n        processor,\n        ori_image=image,\n        mask_image=mask_iamge,\n        prompt=args.prompt,\n        negative_prompt=args.negative_prompt,\n        seed=args.seed,\n    )\n\n    os.makedirs(args.output_path, exist_ok=True)\n    images[0].save(os.path.join(args.output_path, \"inpaint2img.png\"))\n    inpainting_image.save(os.path.join(args.output_path, \"inpaint.png\"))\n    \n# python examples/controlnets/example_inpaint2img.py --checkpoint_path models/Stable-diffusion/v1-5-pruned-emaonly.ckpt --inpainting_controlnet_path models/controlnet/inpaint_v11 --image_path data/original.png --mask_image_path data/mask.png  --prompt \"a pretty rabbit\" --output_path outputs"}
{"type": "source_file", "path": "examples/example_reference2img.py", "content": "import torch\nfrom PIL import Image\nimport os\nimport sys\nimport argparse\n\n__dir__ = os.path.dirname(os.path.abspath(__file__))\nsys.path.append(os.path.dirname(__dir__))\nfrom modules.shared import SDModel, device\nfrom modules.reference.reference import reference2img\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument(\n        \"--checkpoint_path\",\n        default=None,\n        type=str,\n        required=True,\n        help=\"Path to the stable diffusion model.\",\n    )\n\n    parser.add_argument(\n        \"--ref_image_path\",\n        default=None,\n        type=str,\n        required=True,\n        help=\"The reference image path\",\n    )\n\n    parser.add_argument(\n        \"--prompt\",\n        default=None,\n        type=str,\n        required=True,\n        help=\"Describe the image content with text\",\n    )\n\n    parser.add_argument(\n        \"--output_path\",\n        default=\"./\",\n        type=str,\n        required=True,\n        help=\"Result folder path\",\n    )\n\n    parser.add_argument(\n        \"--negative_prompt\",\n        default=None,\n        type=str,\n        required=False,\n        help=\"Content which you don't want to show up with text\",\n    )\n\n    parser.add_argument(\n        \"--diffusers_format\",\n        action=\"store_true\",\n        required=False,\n        help=\"Model files in diffusers format\",\n    )\n    parser.add_argument(\n        \"--seed\", default=2023, required=False, type=int, help=\"Random seed\"\n    )\n\n    args = parser.parse_args()\n\n    ref_image = Image.open(args.ref_image_path).convert(\"RGB\").resize((512, 512))\n    sd_model = SDModel(device=device)\n    sd_model.load_models(args.checkpoint_path, diffusers_format=args.diffusers_format)\n    images, _ = reference2img(\n        sd_model,\n        ref_image=ref_image,\n        prompt=args.prompt,\n        negative_prompt=args.negative_prompt,\n        seed=args.seed,\n    )\n\n    os.makedirs(args.output_path, exist_ok=True)\n    images[0].save(os.path.join(args.output_path, \"reference2img.png\"))\n"}
{"type": "source_file", "path": "modules/controlnet/tile2img.py", "content": "import torch\nfrom typing import Any, Callable, Dict, List, Optional, Union\nfrom PIL import Image\nimport os\nimport sys\n\n__dir__ = os.path.dirname(os.path.abspath(__file__))\nsys.path.append(os.path.dirname(os.path.dirname(__dir__)))\nfrom modules.shared import SDModel, Controlnet\nfrom modules.controlnet.controlnet_img2img import controlnet_img2img\nfrom modules.wrappers import ResizeForConditionalImageWrapper\n\n\ndef tile2img(\n    sd_model: SDModel,\n    controlnet_model: Controlnet,\n    processor: ResizeForConditionalImageWrapper,\n    ori_image: Image,\n    is_tile_image: bool = False,\n    down_sampling_ratio: float = 8.0,\n    prompt: Union[str, List[str]] = None,\n    height: Optional[int] = None,\n    width: Optional[int] = None,\n    strength: float = 1.0,\n    num_inference_steps: int = 20,\n    guidance_scale: float = 7.5,\n    negative_prompt: Optional[Union[str, List[str]]] = None,\n    num_images_per_prompt: Optional[int] = 1,\n    eta: float = 0.0,\n    seed: Optional[Union[int, List[int]]] = None,\n    latents: Optional[torch.FloatTensor] = None,\n    prompt_embeds: Optional[torch.FloatTensor] = None,\n    negative_prompt_embeds: Optional[torch.FloatTensor] = None,\n    output_type: Optional[str] = \"pil\",\n    return_dict: bool = True,\n    callback: Optional[Callable[[int, int, torch.FloatTensor], None]] = None,\n    callback_steps: int = 1,\n    cross_attention_kwargs: Optional[Dict[str, Any]] = None,\n    controlnet_conditioning_scale: Union[float, List[float]] = 0.8,\n    guess_mode: bool = False,\n    control_guidance_start: Union[float, List[float]] = 0.0,\n    control_guidance_end: Union[float, List[float]] = 1.0,\n    lora_model_name_or_path_or_dict: Optional[Union[str, List[str]]] = None,\n    lora_alpha: Optional[float] = 0.75,\n    text_inversion_model_name_or_path: Optional[\n        Union[str, List[str], Dict[str, torch.Tensor], List[Dict[str, torch.Tensor]]]\n    ] = None,\n):\n\n    if is_tile_image:\n        tile_image = ori_image\n    else:\n        tile_image = processor(\n            input_image=ori_image, down_sampling_ratio=down_sampling_ratio\n        )\n\n    images, nsfw_content_detected = controlnet_img2img(\n        sd_model=sd_model,\n        controlnet_model=controlnet_model,\n        prompt=prompt,\n        image=tile_image,\n        control_image=tile_image,\n        height=height,\n        width=width,\n        strength=strength,\n        num_inference_steps=num_inference_steps,\n        guidance_scale=guidance_scale,\n        negative_prompt=negative_prompt,\n        num_images_per_prompt=num_images_per_prompt,\n        eta=eta,\n        seed=seed,\n        latents=latents,\n        prompt_embeds=prompt_embeds,\n        negative_prompt_embeds=negative_prompt_embeds,\n        output_type=output_type,\n        return_dict=return_dict,\n        callback=callback,\n        callback_steps=callback_steps,\n        cross_attention_kwargs=cross_attention_kwargs,\n        controlnet_conditioning_scale=controlnet_conditioning_scale,\n        guess_mode=guess_mode,\n        control_guidance_start=control_guidance_start,\n        control_guidance_end=control_guidance_end,\n        lora_model_name_or_path_or_dict=lora_model_name_or_path_or_dict,\n        lora_alpha=lora_alpha,\n        text_inversion_model_name_or_path=text_inversion_model_name_or_path,\n    )\n\n    return images, nsfw_content_detected, tile_image\n"}
{"type": "source_file", "path": "examples/example_super_resolution.py", "content": "import torch\nimport os\nimport sys\nfrom PIL import Image\nimport argparse\n\n__dir__ = os.path.dirname(os.path.abspath(__file__))\nsys.path.append(os.path.dirname(__dir__))\nfrom modules.wrappers import RealESRGANWrapper\nfrom modules.shared import device\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument(\n        \"--checkpoint_path\",\n        default=None,\n        type=str,\n        required=True,\n        help=\"Path to super resolution model.\",\n    )\n\n    parser.add_argument(\n        \"--image_path\",\n        default=None,\n        type=str,\n        required=True,\n        help=\"The input image path\",\n    )\n\n    parser.add_argument(\n        \"--output_path\",\n        default=\"./\",\n        type=str,\n        required=True,\n        help=\"Result folder path\",\n    )\n\n    args = parser.parse_args()\n\n    realesrgan = RealESRGANWrapper(device=device)\n    realesrgan.load_models(model_path=args.checkpoint_path)\n    image = Image.open(args.image_path).convert(\"RGB\")\n    result = realesrgan(image)\n\n    os.makedirs(args.output_path, exist_ok=True)\n    result.save(os.path.join(args.output_path, \"sr_img.png\"))\n"}
{"type": "source_file", "path": "modules/basic/img2img.py", "content": "import torch\nfrom diffusers import StableDiffusionImg2ImgPipeline\nfrom diffusers.utils import is_accelerate_available, is_accelerate_version\nfrom typing import Any, Callable, Dict, List, Optional, Union\nimport gc\nimport PIL\nimport numpy as np\nimport os\nimport sys\n\n__dir__ = os.path.dirname(os.path.abspath(__file__))\nsys.path.append(os.path.dirname(__dir__))\nfrom shared import SDModel, MTGPU_DETECTION\n\n\ndef img2img(\n    sd_model: SDModel,\n    prompt: Union[str, List[str]] = None,\n    image: Union[\n        torch.FloatTensor,\n        PIL.Image.Image,\n        np.ndarray,\n        List[torch.FloatTensor],\n        List[PIL.Image.Image],\n        List[np.ndarray],\n    ] = None,\n    strength: float = 0.8,\n    num_inference_steps: Optional[int] = 20,\n    guidance_scale: Optional[float] = 7.5,\n    negative_prompt: Optional[Union[str, List[str]]] = None,\n    num_images_per_prompt: Optional[int] = 1,\n    eta: Optional[float] = 0.0,\n    seed: Optional[Union[int, List[int]]] = None,\n    prompt_embeds: Optional[torch.FloatTensor] = None,\n    negative_prompt_embeds: Optional[torch.FloatTensor] = None,\n    output_type: Optional[str] = \"pil\",\n    return_dict: bool = True,\n    callback: Optional[Callable[[int, int, torch.FloatTensor], None]] = None,\n    callback_steps: int = 1,\n    cross_attention_kwargs: Optional[Dict[str, Any]] = None,\n    lora_model_name_or_path_or_dict: Optional[Union[str, List[str]]] = None,\n    lora_alpha: Optional[int] = 0.75,\n    text_inversion_model_name_or_path: Optional[\n        Union[str, List[str], Dict[str, torch.Tensor], List[Dict[str, torch.Tensor]]]\n    ] = None,\n    fast_mode: bool = False,\n):\n    torch.musa.empty_cache() if MTGPU_DETECTION else torch.cuda.empty_cache() \n    gc.collect()\n\n    img2img_pipeline = StableDiffusionImg2ImgPipeline(\n        vae=sd_model.vae,\n        text_encoder=sd_model.text_encoder,\n        tokenizer=sd_model.tokenizer,\n        unet=sd_model.unet,\n        scheduler=sd_model.scheduler,\n        safety_checker=sd_model.safety_checker,\n        feature_extractor=sd_model.feature_extractor,\n        requires_safety_checker=sd_model.requires_safety_checker,\n    )\n\n    img2img_pipeline.unet.oom = False\n    \n    if lora_model_name_or_path_or_dict is not None:\n        if not isinstance(lora_model_name_or_path_or_dict, list):\n            lora_model_name_or_path_or_dict = [lora_model_name_or_path_or_dict]\n        for lora_weight in lora_model_name_or_path_or_dict:\n            sd_model.load_lora(lora_weight, alpha=lora_alpha)\n\n    if text_inversion_model_name_or_path is not None:\n        if not isinstance(text_inversion_model_name_or_path, list):\n            text_inversion_model_name_or_path = [text_inversion_model_name_or_path]\n        for text_inversion_weight in text_inversion_model_name_or_path:\n            img2img_pipeline.load_textual_inversion(text_inversion_weight)\n\n    generator = None\n    if seed is not None:\n        if not isinstance(seed, list):\n            seed = [seed]\n        generator = [\n            torch.Generator(sd_model.device).manual_seed(single_seed)\n            for single_seed in seed\n        ]\n\n    sd_model.unet.oom = False\n    \n    results = img2img_pipeline(\n        prompt=prompt,\n        image=image,\n        strength=strength,\n        num_inference_steps=num_inference_steps,\n        guidance_scale=guidance_scale,\n        negative_prompt=negative_prompt,\n        num_images_per_prompt=num_images_per_prompt,\n        eta=eta,\n        generator=generator,\n        prompt_embeds=prompt_embeds,\n        negative_prompt_embeds=negative_prompt_embeds,\n        output_type=output_type,\n        return_dict=return_dict,\n        callback=callback,\n        callback_steps=callback_steps,\n        cross_attention_kwargs=cross_attention_kwargs,\n    )\n    images = results.images\n    nsfw_content_detected = results.nsfw_content_detected\n\n    torch.musa.empty_cache() if MTGPU_DETECTION else torch.cuda.empty_cache()     \n    gc.collect()\n    \n    if (\n        sd_model.enable_offload_cpu\n        and is_accelerate_available()\n        and is_accelerate_version(\">=\", \"0.14.0\")\n    ):\n        img2img_pipeline.enable_model_cpu_offload()\n\n    if lora_model_name_or_path_or_dict is not None:\n        for lora_weight in lora_model_name_or_path_or_dict:\n            sd_model.offload_lora(lora_weight, alpha=lora_alpha)\n\n    return images, nsfw_content_detected\n"}
{"type": "source_file", "path": "modules/controlnet/tilehd2img.py", "content": "import torch\nfrom typing import Any, Callable, Dict, List, Optional, Union\nfrom PIL import Image\nimport os\nimport sys\n\n__dir__ = os.path.dirname(os.path.abspath(__file__))\nsys.path.append(os.path.dirname(os.path.dirname(__dir__)))\nfrom modules.shared import SDModel, Controlnet\nfrom modules.controlnet.controlnet_tileimg2img import controlnet_tileimg2img\nfrom modules.wrappers import ZoomConditionalImageWrapper\n\n\ndef tilehd2img(\n    sd_model: SDModel,\n    controlnet_model: Controlnet,\n    processor: ZoomConditionalImageWrapper,\n    ori_image: Image,\n    is_zoom_image: bool = False,\n    up_sampling_ratio: float = 2.0,\n    prompt: Union[str, List[str]] = None,\n    height: Optional[int] = None,\n    width: Optional[int] = None,\n    strength: float = 0.35,\n    num_inference_steps: int = 20,\n    guidance_scale: float = 7.5,\n    negative_prompt: Optional[Union[str, List[str]]] = None,\n    num_images_per_prompt: Optional[int] = 1,\n    eta: float = 0.0,\n    seed: Optional[Union[int, List[int]]] = None,\n    latents: Optional[torch.FloatTensor] = None,\n    prompt_embeds: Optional[torch.FloatTensor] = None,\n    negative_prompt_embeds: Optional[torch.FloatTensor] = None,\n    output_type: Optional[str] = \"pil\",\n    return_dict: bool = True,\n    callback: Optional[Callable[[int, int, torch.FloatTensor], None]] = None,\n    callback_steps: int = 1,\n    cross_attention_kwargs: Optional[Dict[str, Any]] = None,\n    controlnet_conditioning_scale: Union[float, List[float]] = 0.8,\n    guess_mode: bool = False,\n    control_guidance_start: Union[float, List[float]] = 0.0,\n    control_guidance_end: Union[float, List[float]] = 1.0,\n    window_size: int = 80,\n    lora_model_name_or_path_or_dict: Optional[Union[str, List[str]]] = None,\n    lora_alpha: Optional[float] = 0.75,\n    text_inversion_model_name_or_path: Optional[\n        Union[str, List[str], Dict[str, torch.Tensor], List[Dict[str, torch.Tensor]]]\n    ] = None,\n):\n\n    if is_zoom_image:\n        zoom_image = ori_image\n    else:\n        zoom_image = processor(\n            input_image=ori_image, up_sampling_ratio=up_sampling_ratio\n        )\n\n    images, nsfw_content_detected = controlnet_tileimg2img(\n        sd_model=sd_model,\n        controlnet_model=controlnet_model,\n        prompt=prompt,\n        image=zoom_image,\n        control_image=zoom_image,\n        height=height,\n        width=width,\n        strength=strength,\n        num_inference_steps=num_inference_steps,\n        guidance_scale=guidance_scale,\n        negative_prompt=negative_prompt,\n        num_images_per_prompt=num_images_per_prompt,\n        eta=eta,\n        seed=seed,\n        latents=latents,\n        prompt_embeds=prompt_embeds,\n        negative_prompt_embeds=negative_prompt_embeds,\n        output_type=output_type,\n        return_dict=return_dict,\n        callback=callback,\n        callback_steps=callback_steps,\n        cross_attention_kwargs=cross_attention_kwargs,\n        controlnet_conditioning_scale=controlnet_conditioning_scale,\n        guess_mode=guess_mode,\n        control_guidance_start=control_guidance_start,\n        control_guidance_end=control_guidance_end,\n        window_size=window_size,\n        lora_model_name_or_path_or_dict=lora_model_name_or_path_or_dict,\n        lora_alpha=lora_alpha,\n        text_inversion_model_name_or_path=text_inversion_model_name_or_path,\n    )\n\n    return images, nsfw_content_detected, zoom_image\n"}
{"type": "source_file", "path": "examples/example_img2img.py", "content": "import torch\nfrom PIL import Image\nimport os\nimport sys\nimport argparse\n\n__dir__ = os.path.dirname(os.path.abspath(__file__))\nsys.path.append(os.path.dirname(__dir__))\nfrom modules.shared import SDModel, device\nfrom modules.basic.img2img import img2img\n    \nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument(\n        \"--checkpoint_path\",\n        default=None,\n        type=str,\n        required=True,\n        help=\"Path to the stable diffusion model.\",\n    )\n\n    parser.add_argument(\n        \"--image_path\",\n        default=None,\n        type=str,\n        required=True,\n        help=\"The initial image path\",\n    )\n\n    parser.add_argument(\n        \"--prompt\",\n        default=None,\n        type=str,\n        required=True,\n        help=\"Describe the image content with text\",\n    )\n\n    parser.add_argument(\n        \"--output_path\",\n        default=\"./\",\n        type=str,\n        required=True,\n        help=\"Result folder path\",\n    )\n\n    parser.add_argument(\n        \"--negative_prompt\",\n        default=None,\n        type=str,\n        required=False,\n        help=\"Content which you don't want to show up with text\",\n    )\n\n    parser.add_argument(\n        \"--diffusers_format\",\n        action=\"store_true\",\n        required=False,\n        help=\"Model files in diffusers format\",\n    )\n    parser.add_argument(\n        \"--seed\", default=2023, required=False, type=int, help=\"Random seed\"\n    )\n\n    args = parser.parse_args()\n\n    image = Image.open(args.image_path).convert(\"RGB\").resize((512, 512))\n    sd_model = SDModel(device=device)\n    sd_model.load_models(args.checkpoint_path, diffusers_format=args.diffusers_format)\n\n    images, _ = img2img(\n        sd_model,\n        image=image,\n        prompt=args.prompt,\n        negative_prompt=args.negative_prompt,\n        seed=args.seed,\n    )\n\n    os.makedirs(args.output_path, exist_ok=True)\n    images[0].save(os.path.join(args.output_path, \"img2img.png\"))\n"}
{"type": "source_file", "path": "modules/basic/txt2img.py", "content": "import torch\nfrom diffusers import StableDiffusionPipeline\nfrom diffusers.utils import is_accelerate_available, is_accelerate_version\nfrom typing import Any, Callable, Dict, List, Optional, Union\nimport gc\nimport os\nimport sys\n\n__dir__ = os.path.dirname(os.path.abspath(__file__))\nsys.path.append(os.path.dirname(__dir__))\nfrom shared import SDModel, MTGPU_DETECTION\n\n\ndef txt2img(\n    sd_model: SDModel,\n    prompt: Union[str, List[str]] = None,\n    height: Optional[int] = None,\n    width: Optional[int] = None,\n    num_inference_steps: int = 20,\n    guidance_scale: float = 7.5,\n    negative_prompt: Optional[Union[str, List[str]]] = None,\n    num_images_per_prompt: Optional[int] = 1,\n    eta: float = 0.0,\n    seed: Optional[Union[int, List[int]]] = None,\n    latents: Optional[torch.FloatTensor] = None,\n    prompt_embeds: Optional[torch.FloatTensor] = None,\n    negative_prompt_embeds: Optional[torch.FloatTensor] = None,\n    output_type: Optional[str] = \"pil\",\n    return_dict: bool = True,\n    callback: Optional[Callable[[int, int, torch.FloatTensor], None]] = None,\n    callback_steps: int = 1,\n    cross_attention_kwargs: Optional[Dict[str, Any]] = None,\n    guidance_rescale: float = 0.0,\n    lora_model_name_or_path_or_dict: Optional[Union[str, List[str]]] = None,\n    lora_alpha: Optional[float] = 0.75,\n    text_inversion_model_name_or_path: Optional[\n        Union[str, List[str], Dict[str, torch.Tensor], List[Dict[str, torch.Tensor]]]\n    ] = None,\n    fast_mode: bool = False,\n):\n\n    torch.musa.empty_cache() if MTGPU_DETECTION else torch.cuda.empty_cache()    \n    gc.collect()\n\n    txt2img_pipeline = StableDiffusionPipeline(\n        vae=sd_model.vae,\n        text_encoder=sd_model.text_encoder,\n        tokenizer=sd_model.tokenizer,\n        unet=sd_model.unet,\n        scheduler=sd_model.fast_scheduler if fast_mode else sd_model.scheduler,\n        safety_checker=sd_model.safety_checker,\n        feature_extractor=sd_model.feature_extractor,\n        requires_safety_checker=sd_model.requires_safety_checker,\n    )\n    \n    txt2img_pipeline.unet.oom = False\n\n    if lora_model_name_or_path_or_dict is not None:\n        if not isinstance(lora_model_name_or_path_or_dict, list):\n            lora_model_name_or_path_or_dict = [lora_model_name_or_path_or_dict]\n        for lora_weight in lora_model_name_or_path_or_dict:\n            sd_model.load_lora(lora_weight, alpha=lora_alpha)\n\n    if text_inversion_model_name_or_path is not None:\n        if not isinstance(text_inversion_model_name_or_path, list):\n            text_inversion_model_name_or_path = [text_inversion_model_name_or_path]\n        for text_inversion_weight in text_inversion_model_name_or_path:\n            txt2img_pipeline.load_textual_inversion(text_inversion_weight)\n\n    generator = None\n    if seed is not None:\n        if not isinstance(seed, list):\n            seed = [seed]\n        generator = [\n            torch.Generator(sd_model.device).manual_seed(single_seed)\n            for single_seed in seed\n        ]\n\n    sd_model.unet.oom = False\n    \n    results = txt2img_pipeline(\n        prompt=prompt,\n        height=height,\n        width=width,\n        num_inference_steps=num_inference_steps,\n        guidance_scale=guidance_scale,\n        negative_prompt=negative_prompt,\n        num_images_per_prompt=num_images_per_prompt,\n        eta=eta,\n        generator=generator,\n        latents=latents,\n        prompt_embeds=prompt_embeds,\n        negative_prompt_embeds=negative_prompt_embeds,\n        output_type=output_type,\n        return_dict=return_dict,\n        callback=callback,\n        callback_steps=callback_steps,\n        cross_attention_kwargs=cross_attention_kwargs,\n        guidance_rescale=guidance_rescale,\n    )\n    images = results.images\n    nsfw_content_detected = results.nsfw_content_detected\n    \n    torch.musa.empty_cache() if MTGPU_DETECTION else torch.cuda.empty_cache()    \n    gc.collect()\n\n    if (\n        sd_model.enable_offload_cpu\n        and is_accelerate_available()\n        and is_accelerate_version(\">=\", \"0.14.0\")\n    ):\n        txt2img_pipeline.enable_model_cpu_offload()\n\n    if lora_model_name_or_path_or_dict is not None:\n        for lora_weight in lora_model_name_or_path_or_dict:\n            sd_model.offload_lora(lora_weight, alpha=lora_alpha)\n\n    return images, nsfw_content_detected\n"}
{"type": "source_file", "path": "modules/hijack/__init__.py", "content": "from .cpu_offload import (\n    replace_sequential_cpu_offload_from_cuda_to_musa,\n    replace_model_cpu_offload_from_cuda_to_musa,\n)\nfrom .safety_checker_remove_black_image import (\n    replace_safety_checker_forward,\n    replace_safety_checker_postprocess,\n)\nfrom .encode_prompt import replace_encode_prompt\nfrom .convert_ckpt_to_diffusers import replace_convert_controlnet_checkpoint\nfrom .pipeline_stable_diffusion import replace_pipeline_stable_diffusion_call\nfrom .load_from_single_file import replace_from_single_file"}
{"type": "source_file", "path": "examples/example_tagger.py", "content": "\nimport os\nimport sys\nimport torch\nfrom PIL import Image\nimport argparse\n\n__dir__ = os.path.dirname(os.path.abspath(__file__))\nsys.path.append(os.path.dirname(__dir__))\nfrom modules.wrappers import DeepDanbooruWrapper\nfrom modules.shared import device\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument(\n        \"--checkpoint_path\",\n        default=None,\n        type=str,\n        required=True,\n        help=\"Path to super resolution model.\",\n    )\n\n    parser.add_argument(\n        \"--image_path\",\n        default=None,\n        type=str,\n        required=True,\n        help=\"The input image path\",\n    )\n\n    args = parser.parse_args()\n\n    tagger = DeepDanbooruWrapper(device=device)\n    tagger.load_models(model_path=args.checkpoint_path)\n    image = Image.open(args.image_path).convert(\"RGB\")\n    result = tagger(image)\n    print(result)\n"}
{"type": "source_file", "path": "modules/controlnet/canny2img.py", "content": "import torch\nfrom typing import Any, Callable, Dict, List, Optional, Union\nfrom PIL import Image\nimport os\nimport sys\n\n__dir__ = os.path.dirname(os.path.abspath(__file__))\nsys.path.append(os.path.dirname(os.path.dirname(__dir__)))\nfrom modules.shared import SDModel, Controlnet\nfrom modules.controlnet.controlnet_txt2img import controlnet_txt2img\nfrom modules.controlnet.controlnet_img2img import controlnet_img2img\nfrom modules.wrappers import CannyDetectorWrapper\n\n\ndef canny2img(\n    sd_model: SDModel,\n    controlnet_model: Controlnet,\n    processor: CannyDetectorWrapper,\n    ori_image: Image,\n    aux_image: Image = None,\n    is_canny_image: bool = False,\n    low_threshold: int = 100,\n    high_threshold: int = 200,\n    prompt: Union[str, List[str]] = None,\n    height: Optional[int] = None,\n    width: Optional[int] = None,\n    strength: float = 0.8,\n    num_inference_steps: int = 20,\n    guidance_scale: float = 7.5,\n    negative_prompt: Optional[Union[str, List[str]]] = None,\n    num_images_per_prompt: Optional[int] = 1,\n    eta: float = 0.0,\n    seed: Optional[Union[int, List[int]]] = None,\n    latents: Optional[torch.FloatTensor] = None,\n    prompt_embeds: Optional[torch.FloatTensor] = None,\n    negative_prompt_embeds: Optional[torch.FloatTensor] = None,\n    output_type: Optional[str] = \"pil\",\n    return_dict: bool = True,\n    callback: Optional[Callable[[int, int, torch.FloatTensor], None]] = None,\n    callback_steps: int = 1,\n    cross_attention_kwargs: Optional[Dict[str, Any]] = None,\n    controlnet_conditioning_scale: Union[float, List[float]] = 0.8,\n    guess_mode: bool = False,\n    control_guidance_start: Union[float, List[float]] = 0.0,\n    control_guidance_end: Union[float, List[float]] = 1.0,\n    lora_model_name_or_path_or_dict: Optional[Union[str, List[str]]] = None,\n    lora_alpha: Optional[float] = 0.75,\n    text_inversion_model_name_or_path: Optional[\n        Union[str, List[str], Dict[str, torch.Tensor], List[Dict[str, torch.Tensor]]]\n    ] = None,\n    fast_mode = False\n):\n\n    if is_canny_image:\n        canny_image = ori_image\n    else:\n        canny_image = processor(\n            input_image=ori_image,\n            low_threshold=low_threshold,\n            high_threshold=high_threshold,\n        )\n\n    if aux_image is None:\n        images, nsfw_content_detected = controlnet_txt2img(\n            sd_model=sd_model,\n            controlnet_model=controlnet_model,\n            prompt=prompt,\n            image=canny_image,\n            height=height,\n            width=width,\n            num_inference_steps=num_inference_steps,\n            guidance_scale=guidance_scale,\n            negative_prompt=negative_prompt,\n            num_images_per_prompt=num_images_per_prompt,\n            eta=eta,\n            seed=seed,\n            latents=latents,\n            prompt_embeds=prompt_embeds,\n            negative_prompt_embeds=negative_prompt_embeds,\n            output_type=output_type,\n            return_dict=return_dict,\n            callback=callback,\n            callback_steps=callback_steps,\n            cross_attention_kwargs=cross_attention_kwargs,\n            controlnet_conditioning_scale=controlnet_conditioning_scale,\n            guess_mode=guess_mode,\n            control_guidance_start=control_guidance_start,\n            control_guidance_end=control_guidance_end,\n            lora_model_name_or_path_or_dict=lora_model_name_or_path_or_dict,\n            lora_alpha=lora_alpha,\n            text_inversion_model_name_or_path=text_inversion_model_name_or_path,\n            fast_mode=fast_mode\n        )\n    else:\n        images, nsfw_content_detected = controlnet_img2img(\n            sd_model=sd_model,\n            controlnet_model=controlnet_model,\n            prompt=prompt,\n            image=aux_image,\n            control_image=canny_image,\n            height=height,\n            width=width,\n            strength=strength,\n            num_inference_steps=num_inference_steps,\n            guidance_scale=guidance_scale,\n            negative_prompt=negative_prompt,\n            num_images_per_prompt=num_images_per_prompt,\n            eta=eta,\n            seed=seed,\n            latents=latents,\n            prompt_embeds=prompt_embeds,\n            negative_prompt_embeds=negative_prompt_embeds,\n            output_type=output_type,\n            return_dict=return_dict,\n            callback=callback,\n            callback_steps=callback_steps,\n            cross_attention_kwargs=cross_attention_kwargs,\n            controlnet_conditioning_scale=controlnet_conditioning_scale,\n            guess_mode=guess_mode,\n            control_guidance_start=control_guidance_start,\n            control_guidance_end=control_guidance_end,\n            lora_model_name_or_path_or_dict=lora_model_name_or_path_or_dict,\n            lora_alpha=lora_alpha,\n            text_inversion_model_name_or_path=text_inversion_model_name_or_path,\n        )\n\n    return images, nsfw_content_detected, canny_image\n"}
{"type": "source_file", "path": "models/controlnet/openpose_v11/controlnet_utils.py", "content": "def ade_palette():\n    \"\"\"ADE20K palette that maps each class to RGB values.\"\"\"\n    return [[120, 120, 120], [180, 120, 120], [6, 230, 230], [80, 50, 50],\n            [4, 200, 3], [120, 120, 80], [140, 140, 140], [204, 5, 255],\n            [230, 230, 230], [4, 250, 7], [224, 5, 255], [235, 255, 7],\n            [150, 5, 61], [120, 120, 70], [8, 255, 51], [255, 6, 82],\n            [143, 255, 140], [204, 255, 4], [255, 51, 7], [204, 70, 3],\n            [0, 102, 200], [61, 230, 250], [255, 6, 51], [11, 102, 255],\n            [255, 7, 71], [255, 9, 224], [9, 7, 230], [220, 220, 220],\n            [255, 9, 92], [112, 9, 255], [8, 255, 214], [7, 255, 224],\n            [255, 184, 6], [10, 255, 71], [255, 41, 10], [7, 255, 255],\n            [224, 255, 8], [102, 8, 255], [255, 61, 6], [255, 194, 7],\n            [255, 122, 8], [0, 255, 20], [255, 8, 41], [255, 5, 153],\n            [6, 51, 255], [235, 12, 255], [160, 150, 20], [0, 163, 255],\n            [140, 140, 140], [250, 10, 15], [20, 255, 0], [31, 255, 0],\n            [255, 31, 0], [255, 224, 0], [153, 255, 0], [0, 0, 255],\n            [255, 71, 0], [0, 235, 255], [0, 173, 255], [31, 0, 255],\n            [11, 200, 200], [255, 82, 0], [0, 255, 245], [0, 61, 255],\n            [0, 255, 112], [0, 255, 133], [255, 0, 0], [255, 163, 0],\n            [255, 102, 0], [194, 255, 0], [0, 143, 255], [51, 255, 0],\n            [0, 82, 255], [0, 255, 41], [0, 255, 173], [10, 0, 255],\n            [173, 255, 0], [0, 255, 153], [255, 92, 0], [255, 0, 255],\n            [255, 0, 245], [255, 0, 102], [255, 173, 0], [255, 0, 20],\n            [255, 184, 184], [0, 31, 255], [0, 255, 61], [0, 71, 255],\n            [255, 0, 204], [0, 255, 194], [0, 255, 82], [0, 10, 255],\n            [0, 112, 255], [51, 0, 255], [0, 194, 255], [0, 122, 255],\n            [0, 255, 163], [255, 153, 0], [0, 255, 10], [255, 112, 0],\n            [143, 255, 0], [82, 0, 255], [163, 255, 0], [255, 235, 0],\n            [8, 184, 170], [133, 0, 255], [0, 255, 92], [184, 0, 255],\n            [255, 0, 31], [0, 184, 255], [0, 214, 255], [255, 0, 112],\n            [92, 255, 0], [0, 224, 255], [112, 224, 255], [70, 184, 160],\n            [163, 0, 255], [153, 0, 255], [71, 255, 0], [255, 0, 163],\n            [255, 204, 0], [255, 0, 143], [0, 255, 235], [133, 255, 0],\n            [255, 0, 235], [245, 0, 255], [255, 0, 122], [255, 245, 0],\n            [10, 190, 212], [214, 255, 0], [0, 204, 255], [20, 0, 255],\n            [255, 255, 0], [0, 153, 255], [0, 41, 255], [0, 255, 204],\n            [41, 0, 255], [41, 255, 0], [173, 0, 255], [0, 245, 255],\n            [71, 0, 255], [122, 0, 255], [0, 255, 184], [0, 92, 255],\n            [184, 255, 0], [0, 133, 255], [255, 214, 0], [25, 194, 194],\n            [102, 255, 0], [92, 0, 255]]\n"}
{"type": "source_file", "path": "modules/hijack/convert_ckpt_to_diffusers.py", "content": "import diffusers\nfrom diffusers.pipelines.stable_diffusion.convert_from_ckpt import (\n    create_unet_diffusers_config,\n    convert_ldm_unet_checkpoint,\n)\nfrom diffusers import ControlNetModel\n\n\ndef replace_convert_controlnet_checkpoint():\n    def convert_controlnet_checkpoint(\n        checkpoint,\n        original_config,\n        checkpoint_path,\n        image_size,\n        upcast_attention,\n        extract_ema,\n        use_linear_projection=None,\n        cross_attention_dim=None,\n    ):\n        ctrlnet_config = create_unet_diffusers_config(\n            original_config, image_size=image_size, controlnet=True\n        )\n        ctrlnet_config[\"upcast_attention\"] = upcast_attention\n\n        ctrlnet_config.pop(\"sample_size\")\n\n        if use_linear_projection is not None:\n            ctrlnet_config[\"use_linear_projection\"] = use_linear_projection\n\n        if cross_attention_dim is not None:\n            ctrlnet_config[\"cross_attention_dim\"] = cross_attention_dim\n\n        # Some controlnet ckpt files are distributed independently from the rest of the\n        # model components i.e. https://huggingface.co/thibaud/controlnet-sd21/\n        if \"time_embed.0.weight\" in checkpoint:\n            skip_extract_state_dict = True\n        else:\n            skip_extract_state_dict = False\n\n        converted_ctrl_checkpoint = convert_ldm_unet_checkpoint(\n            checkpoint,\n            ctrlnet_config,\n            path=checkpoint_path,\n            extract_ema=extract_ema,\n            controlnet=True,\n            skip_extract_state_dict=skip_extract_state_dict,\n        )\n        ctrlnet_config.pop(\"addition_embed_type\")\n        ctrlnet_config.pop(\"addition_time_embed_dim\")\n        ctrlnet_config.pop(\"transformer_layers_per_block\")\n        controlnet_model = ControlNetModel(**ctrlnet_config)\n\n        controlnet_model.load_state_dict(converted_ctrl_checkpoint)\n\n        return controlnet_model\n\n    diffusers.pipelines.stable_diffusion.convert_from_ckpt.convert_controlnet_checkpoint = (\n        convert_controlnet_checkpoint\n    )\n"}
{"type": "source_file", "path": "modules/hijack/cpu_offload.py", "content": "import torch\nimport diffusers\nfrom diffusers.utils import is_accelerate_available, is_accelerate_version\n\ntry:    \n    import torch_musa\n    MTGPU_DETECTION = True\nexcept:\n    print(\"We cannot import torch_musa\")\n    MTGPU_DETECTION = False\n        \n\ndef replace_sequential_cpu_offload_from_cuda_to_musa():\n    def enable_sequential_cpu_offload(self, gpu_id=0):\n        r\"\"\"\n        Offloads all models to CPU using accelerate, significantly reducing memory usage. When called, unet,\n        text_encoder, vae and safety checker have their state dicts saved to CPU and then are moved to a\n        `torch.device('meta') and loaded to GPU only when their specific submodule has its `forward` method called.\n        Note that offloading happens on a submodule basis. Memory savings are higher than with\n        `enable_model_cpu_offload`, but performance is lower.\n        \"\"\"\n        if is_accelerate_available() and is_accelerate_version(\">=\", \"0.14.0\"):\n            from accelerate import cpu_offload\n        else:\n            raise ImportError(\n                \"`enable_sequential_cpu_offload` requires `accelerate v0.14.0` or higher\"\n            )\n        if MTGPU_DETECTION:\n            device = torch.device(f\"musa:{gpu_id}\")\n        else:\n            device = torch.device(f\"cuda:{gpu_id}\") if torch.cuda.is_available() else torch.device(\"cpu\")\n\n        if self.device.type != \"cpu\":\n            self.to(\"cpu\", silence_dtype_warnings=True)\n            torch.musa.empty_cache() if MTGPU_DETECTION else torch.cuda.empty_cache()  # otherwise we don't see the memory savings (but they probably exist)\n\n        for cpu_offloaded_model in [self.unet, self.text_encoder, self.vae]:\n            cpu_offload(cpu_offloaded_model, device)\n\n        if self.safety_checker is not None:\n            cpu_offload(\n                self.safety_checker, execution_device=device, offload_buffers=True\n            )\n\n    diffusers.StableDiffusionPipeline.enable_sequential_cpu_offload = (\n        enable_sequential_cpu_offload\n    )\n    diffusers.StableDiffusionImg2ImgPipeline.enable_sequential_cpu_offload = (\n        enable_sequential_cpu_offload\n    )\n    diffusers.StableDiffusionControlNetPipeline.enable_sequential_cpu_offload = (\n        enable_sequential_cpu_offload\n    )\n    diffusers.StableDiffusionControlNetImg2ImgPipeline.enable_sequential_cpu_offload = (\n        enable_sequential_cpu_offload\n    )\n    diffusers.StableDiffusionControlNetInpaintPipeline.enable_sequential_cpu_offload = (\n        enable_sequential_cpu_offload\n    )\n\n\ndef replace_model_cpu_offload_from_cuda_to_musa():\n    def enable_model_cpu_offload(self, gpu_id=0):\n        r\"\"\"\n        Offloads all models to CPU using accelerate, reducing memory usage with a low impact on performance. Compared\n        to `enable_sequential_cpu_offload`, this method moves one whole model at a time to the GPU when its `forward`\n        method is called, and the model remains in GPU until the next model runs. Memory savings are lower than with\n        `enable_sequential_cpu_offload`, but performance is much better due to the iterative execution of the `unet`.\n        \"\"\"\n        if is_accelerate_available() and is_accelerate_version(\">=\", \"0.17.0.dev0\"):\n            from accelerate import cpu_offload_with_hook\n        else:\n            raise ImportError(\n                \"`enable_model_cpu_offload` requires `accelerate v0.17.0` or higher.\"\n            )\n\n        device = torch.device(f\"musa:{gpu_id}\")\n\n        if self.device.type != \"cpu\":\n            self.to(\"cpu\", silence_dtype_warnings=True)\n            torch.musa.empty_cache()  # otherwise we don't see the memory savings (but they probably exist)\n\n        hook = None\n        for cpu_offloaded_model in [self.text_encoder, self.unet, self.vae]:\n            _, hook = cpu_offload_with_hook(\n                cpu_offloaded_model, device, prev_module_hook=hook\n            )\n\n        if self.safety_checker is not None:\n            _, hook = cpu_offload_with_hook(\n                self.safety_checker, device, prev_module_hook=hook\n            )\n\n        # We'll offload the last model manually.\n        self.final_offload_hook = hook\n\n    diffusers.StableDiffusionPipeline.enable_model_cpu_offload = (\n        enable_model_cpu_offload\n    )\n    diffusers.StableDiffusionImg2ImgPipeline.enable_model_cpu_offload = (\n        enable_model_cpu_offload\n    )\n    diffusers.StableDiffusionControlNetPipeline.enable_model_cpu_offload = (\n        enable_model_cpu_offload\n    )\n    diffusers.StableDiffusionControlNetImg2ImgPipeline.enable_model_cpu_offload = (\n        enable_model_cpu_offload\n    )\n    diffusers.StableDiffusionControlNetInpaintPipeline.enable_model_cpu_offload = (\n        enable_model_cpu_offload\n    )\n"}
{"type": "source_file", "path": "examples/example_txt2img.py", "content": "import torch\nimport os\nimport sys\nimport argparse\n\n__dir__ = os.path.dirname(os.path.abspath(__file__))\nsys.path.append(os.path.dirname(__dir__))\nfrom modules.shared import SDModel, device\nfrom modules.basic.txt2img import txt2img\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument(\n        \"--checkpoint_path\",\n        default=None,\n        type=str,\n        required=True,\n        help=\"Path to the stable diffusion model.\",\n    )\n\n    parser.add_argument(\n        \"--prompt\",\n        default=None,\n        type=str,\n        required=True,\n        help=\"Describe the image content with text\",\n    )\n\n    parser.add_argument(\n        \"--output_path\",\n        default=\"./\",\n        type=str,\n        required=True,\n        help=\"Result folder path\",\n    )\n\n    parser.add_argument(\n        \"--negative_prompt\",\n        default=None,\n        type=str,\n        required=False,\n        help=\"Content which you don't want to show up with text\",\n    )\n\n    parser.add_argument(\n        \"--diffusers_format\",\n        action=\"store_true\",\n        required=False,\n        help=\"Model files in diffusers format\",\n    )\n\n    parser.add_argument(\n        \"--seed\", default=2023, required=False, type=int, help=\"Random seed\"\n    )\n\n    args = parser.parse_args()\n\n    sd_model = SDModel(device=device, requires_safety_checker=False)\n    sd_model.load_models(args.checkpoint_path, diffusers_format=args.diffusers_format)\n\n    images, status = txt2img(\n        sd_model,\n        prompt=args.prompt,\n        negative_prompt=args.negative_prompt,\n        seed=args.seed,\n    )\n\n    os.makedirs(args.output_path, exist_ok=True)\n    images[0].save(os.path.join(args.output_path, \"txt2img.png\"))\n"}
{"type": "source_file", "path": "modules/controlnet/controlnet_img2img.py", "content": "import torch\nfrom diffusers import StableDiffusionControlNetImg2ImgPipeline\nfrom diffusers.utils import is_accelerate_available, is_accelerate_version\nfrom typing import Any, Callable, Dict, List, Optional, Union\nimport gc\nimport PIL\nimport numpy as np\nimport os\nimport sys\n\n__dir__ = os.path.dirname(os.path.abspath(__file__))\nsys.path.append(os.path.dirname(__dir__))\nfrom shared import SDModel, Controlnet, MTGPU_DETECTION\n\n\ndef controlnet_img2img(\n    sd_model: SDModel,\n    controlnet_model: Controlnet,\n    prompt: Union[str, List[str]] = None,\n    image: Union[\n        torch.FloatTensor,\n        PIL.Image.Image,\n        np.ndarray,\n        List[torch.FloatTensor],\n        List[PIL.Image.Image],\n        List[np.ndarray],\n    ] = None,\n    control_image: Union[\n        torch.FloatTensor,\n        PIL.Image.Image,\n        np.ndarray,\n        List[torch.FloatTensor],\n        List[PIL.Image.Image],\n        List[np.ndarray],\n    ] = None,\n    height: Optional[int] = None,\n    width: Optional[int] = None,\n    strength: float = 0.8,\n    num_inference_steps: int = 20,\n    guidance_scale: float = 7.5,\n    negative_prompt: Optional[Union[str, List[str]]] = None,\n    num_images_per_prompt: Optional[int] = 1,\n    eta: float = 0.0,\n    seed: Optional[Union[int, List[int]]] = None,\n    latents: Optional[torch.FloatTensor] = None,\n    prompt_embeds: Optional[torch.FloatTensor] = None,\n    negative_prompt_embeds: Optional[torch.FloatTensor] = None,\n    output_type: Optional[str] = \"pil\",\n    return_dict: bool = True,\n    callback: Optional[Callable[[int, int, torch.FloatTensor], None]] = None,\n    callback_steps: int = 1,\n    cross_attention_kwargs: Optional[Dict[str, Any]] = None,\n    controlnet_conditioning_scale: Union[float, List[float]] = 0.8,\n    guess_mode: bool = False,\n    control_guidance_start: Union[float, List[float]] = 0.0,\n    control_guidance_end: Union[float, List[float]] = 1.0,\n    lora_model_name_or_path_or_dict: Optional[Union[str, List[str]]] = None,\n    lora_alpha: Optional[float] = 0.75,\n    text_inversion_model_name_or_path: Optional[\n        Union[str, List[str], Dict[str, torch.Tensor], List[Dict[str, torch.Tensor]]]\n    ] = None,\n    fast_mode: bool = False,\n):\n\n    torch.musa.empty_cache() if MTGPU_DETECTION else torch.cuda.empty_cache()   \n    gc.collect()\n\n    controlnet_img2img_pipeline = StableDiffusionControlNetImg2ImgPipeline(\n        vae=sd_model.vae,\n        text_encoder=sd_model.text_encoder,\n        tokenizer=sd_model.tokenizer,\n        unet=sd_model.unet,\n        controlnet=controlnet_model.controlnet,\n        scheduler=sd_model.scheduler,\n        safety_checker=sd_model.safety_checker,\n        feature_extractor=sd_model.feature_extractor,\n        requires_safety_checker=sd_model.requires_safety_checker,\n    )\n    \n    controlnet_img2img_pipeline.unet.oom = False\n\n    if lora_model_name_or_path_or_dict is not None:\n        if not isinstance(lora_model_name_or_path_or_dict, list):\n            lora_model_name_or_path_or_dict = [lora_model_name_or_path_or_dict]\n        for lora_weight in lora_model_name_or_path_or_dict:\n            sd_model.load_lora(lora_weight, alpha=lora_alpha)\n\n    if text_inversion_model_name_or_path is not None:\n        if not isinstance(text_inversion_model_name_or_path, list):\n            text_inversion_model_name_or_path = [text_inversion_model_name_or_path]\n        for text_inversion_weight in text_inversion_model_name_or_path:\n            controlnet_img2img_pipeline.load_textual_inversion(text_inversion_weight)\n\n    generator = None\n    if seed is not None:\n        if not isinstance(seed, list):\n            seed = [seed]\n        generator = [\n            torch.Generator(sd_model.device).manual_seed(single_seed)\n            for single_seed in seed\n        ]\n\n    sd_model.unet.oom = False\n    \n    results = controlnet_img2img_pipeline(\n        prompt=prompt,\n        image=image,\n        control_image=control_image,\n        height=height,\n        width=width,\n        strength=strength,\n        num_inference_steps=num_inference_steps,\n        guidance_scale=guidance_scale,\n        negative_prompt=negative_prompt,\n        num_images_per_prompt=num_images_per_prompt,\n        eta=eta,\n        generator=generator,\n        latents=latents,\n        prompt_embeds=prompt_embeds,\n        negative_prompt_embeds=negative_prompt_embeds,\n        output_type=output_type,\n        return_dict=return_dict,\n        callback=callback,\n        callback_steps=callback_steps,\n        cross_attention_kwargs=cross_attention_kwargs,\n        controlnet_conditioning_scale=controlnet_conditioning_scale,\n        guess_mode=guess_mode,\n        control_guidance_start=control_guidance_start,\n        control_guidance_end=control_guidance_end,\n    )\n    images = results.images\n    nsfw_content_detected = results.nsfw_content_detected\n\n    torch.musa.empty_cache() if MTGPU_DETECTION else torch.cuda.empty_cache()    \n    gc.collect()\n    \n    if (\n        sd_model.enable_offload_cpu\n        and is_accelerate_available()\n        and is_accelerate_version(\">=\", \"0.14.0\")\n    ):\n        controlnet_img2img_pipeline.enable_model_cpu_offload()\n\n    if lora_model_name_or_path_or_dict is not None:\n        for lora_weight in lora_model_name_or_path_or_dict:\n            sd_model.offload_lora(lora_weight, alpha=lora_alpha)\n\n    return images, nsfw_content_detected\n"}
{"type": "source_file", "path": "examples/example_textinversion2img.py", "content": "import torch\nimport os\nimport sys\nimport argparse\n\n__dir__ = os.path.dirname(os.path.abspath(__file__))\nsys.path.append(os.path.dirname(__dir__))\nfrom modules.shared import SDModel, device\nfrom modules.basic.txt2img import txt2img\n    \nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument(\n        \"--checkpoint_path\",\n        default=None,\n        type=str,\n        required=True,\n        help=\"Path to the stable diffusion model.\",\n    )\n\n    parser.add_argument(\n        \"--text_inversion_path\",\n        default=None,\n        type=str,\n        required=True,\n        help=\"Path to the text inventrual model.\",\n    )\n\n    parser.add_argument(\n        \"--prompt\",\n        default=None,\n        type=str,\n        required=True,\n        help=\"Describe the image content with text\",\n    )\n\n    parser.add_argument(\n        \"--output_path\",\n        default=\"./\",\n        type=str,\n        required=True,\n        help=\"Result folder path\",\n    )\n\n    parser.add_argument(\n        \"--negative_prompt\",\n        default=None,\n        type=str,\n        required=False,\n        help=\"Content which you don't want to show up with text\",\n    )\n\n    parser.add_argument(\n        \"--diffusers_format\",\n        action=\"store_true\",\n        required=False,\n        help=\"Model files in diffusers format\",\n    )\n    parser.add_argument(\n        \"--seed\", default=2023, required=False, type=int, help=\"Random seed\"\n    )\n\n    args = parser.parse_args()\n\n    sd_model = SDModel(device=device)\n    sd_model.load_models(args.checkpoint_path, diffusers_format=args.diffusers_format)\n\n    image, status = txt2img(\n        sd_model,\n        prompt=args.prompt,\n        negative_prompt=args.negative_prompt,\n        seed=args.seed,\n        text_inversion_model_name_or_path=args.text_inversion_path,\n    )\n\n    os.makedirs(args.output_path, exist_ok=True)\n    image[0].save(os.path.join(args.output_path, \"textinventual2img.png\"))\n"}
{"type": "source_file", "path": "examples/example_translator.py", "content": "\nimport os\nimport sys\nimport argparse\n\n__dir__ = os.path.dirname(os.path.abspath(__file__))\nsys.path.append(os.path.dirname(__dir__))\nfrom modules.wrappers import Translator\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument(\n        \"--checkpoint_path\",\n        default=None,\n        type=str,\n        required=True,\n        help=\"Path to the translator model.\",\n    )\n\n    parser.add_argument(\n        \"--chinese_text\",\n        default=None,\n        type=str,\n        required=True,\n        help=\"Chinese sentence(s) or phrase(s) or word(s)\",\n    )\n\n    args = parser.parse_args()\n\n    tagger = Translator(device=\"cpu\")\n    tagger.load_models(model_path=args.checkpoint_path)\n    english = tagger(args.chinese_text)\n    print(english)\n"}
{"type": "source_file", "path": "modules/hijack/encode_prompt.py", "content": "import torch\nimport diffusers\nfrom diffusers.loaders import (\n    FromSingleFileMixin,\n    LoraLoaderMixin,\n    TextualInversionLoaderMixin,\n)\nfrom diffusers.utils import logging\nfrom typing import Any, Callable, Dict, List, Optional, Union\nimport os\nimport sys\n\n__dir__ = os.path.dirname(os.path.abspath(__file__))\nsys.path.append(os.path.dirname(os.path.dirname(__dir__)))\nfrom modules.tools import prompt_parser\n\nlogger = logging.get_logger(__name__)  # pylint: disable=invalid-name\n\n\ndef replace_encode_prompt():\n    def _encode_prompt(\n        self,\n        prompt,\n        device,\n        num_images_per_prompt,\n        do_classifier_free_guidance,\n        negative_prompt=None,\n        prompt_embeds: Optional[torch.FloatTensor] = None,\n        negative_prompt_embeds: Optional[torch.FloatTensor] = None,\n        lora_scale: Optional[float] = None,\n    ):\n        r\"\"\"\n        Encodes the prompt into text encoder hidden states.\n\n        Args:\n             prompt (`str` or `List[str]`, *optional*):\n                prompt to be encoded\n            device: (`torch.device`):\n                torch device\n            num_images_per_prompt (`int`):\n                number of images that should be generated per prompt\n            do_classifier_free_guidance (`bool`):\n                whether to use classifier free guidance or not\n            negative_prompt (`str` or `List[str]`, *optional*):\n                The prompt or prompts not to guide the image generation. If not defined, one has to pass\n                `negative_prompt_embeds` instead. Ignored when not using guidance (i.e., ignored if `guidance_scale` is\n                less than `1`).\n            prompt_embeds (`torch.FloatTensor`, *optional*):\n                Pre-generated text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not\n                provided, text embeddings will be generated from `prompt` input argument.\n            negative_prompt_embeds (`torch.FloatTensor`, *optional*):\n                Pre-generated negative text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt\n                weighting. If not provided, negative_prompt_embeds will be generated from `negative_prompt` input\n                argument.\n            lora_scale (`float`, *optional*):\n                A lora scale that will be applied to all LoRA layers of the text encoder if LoRA layers are loaded.\n        \"\"\"\n        # set lora scale so that monkey patched LoRA\n        # function of text encoder can correctly access it\n        if lora_scale is not None and isinstance(self, LoraLoaderMixin):\n            self._lora_scale = lora_scale\n\n        if prompt is not None and isinstance(prompt, str):\n            batch_size = 1\n        elif prompt is not None and isinstance(prompt, list):\n            batch_size = len(prompt)\n        else:\n            batch_size = prompt_embeds.shape[0]\n\n        if prompt_embeds is None:\n            # textual inversion: procecss multi-vector tokens if necessary\n            if isinstance(self, TextualInversionLoaderMixin):\n                prompt = self.maybe_convert_prompt(prompt, self.tokenizer)\n\n        # get unconditional embeddings for classifier free guidance\n        if negative_prompt_embeds is None:\n            uncond_tokens: List[str]\n            if negative_prompt is None:\n                uncond_tokens = [\"\"] * batch_size\n            elif prompt is not None and type(prompt) is not type(negative_prompt):\n                raise TypeError(\n                    f\"`negative_prompt` should be the same type to `prompt`, but got {type(negative_prompt)} !=\"\n                    f\" {type(prompt)}.\"\n                )\n            elif isinstance(negative_prompt, str):\n                uncond_tokens = [negative_prompt]\n            elif batch_size != len(negative_prompt):\n                raise ValueError(\n                    f\"`negative_prompt`: {negative_prompt} has batch size {len(negative_prompt)}, but `prompt`:\"\n                    f\" {prompt} has batch size {batch_size}. Please make sure that passed `negative_prompt` matches\"\n                    \" the batch size of `prompt`.\"\n                )\n            else:\n                uncond_tokens = negative_prompt\n\n            # textual inversion: procecss multi-vector tokens if necessary\n            if isinstance(self, TextualInversionLoaderMixin):\n                uncond_tokens = self.maybe_convert_prompt(uncond_tokens, self.tokenizer)\n\n        prompt_embeds, negative_prompt_embeds = prompt_parser.get_weighted_text_embeddings(\n            pipe=self,\n            prompt=prompt,\n            uncond_prompt=uncond_tokens,\n            max_embeddings_multiples=3,\n        )\n\n        prompt_embeds = prompt_embeds.to(dtype=self.text_encoder.dtype, device=device)\n\n        bs_embed, seq_len, _ = prompt_embeds.shape\n        # duplicate text embeddings for each generation per prompt, using mps friendly method\n        prompt_embeds = prompt_embeds.repeat(1, num_images_per_prompt, 1)\n        prompt_embeds = prompt_embeds.view(\n            bs_embed * num_images_per_prompt, seq_len, -1\n        )\n\n        if do_classifier_free_guidance:\n            # duplicate unconditional embeddings for each generation per prompt, using mps friendly method\n            seq_len = negative_prompt_embeds.shape[1]\n\n            negative_prompt_embeds = negative_prompt_embeds.to(\n                dtype=self.text_encoder.dtype, device=device\n            )\n\n            negative_prompt_embeds = negative_prompt_embeds.repeat(\n                1, num_images_per_prompt, 1\n            )\n            negative_prompt_embeds = negative_prompt_embeds.view(\n                batch_size * num_images_per_prompt, seq_len, -1\n            )\n\n            # For classifier free guidance, we need to do two forward passes.\n            # Here we concatenate the unconditional and text embeddings into a single batch\n            # to avoid doing two forward passes\n            prompt_embeds = torch.cat([negative_prompt_embeds, prompt_embeds])\n\n        return prompt_embeds\n\n    diffusers.StableDiffusionPipeline._encode_prompt = _encode_prompt\n    diffusers.StableDiffusionImg2ImgPipeline._encode_prompt = _encode_prompt\n    diffusers.StableDiffusionControlNetPipeline._encode_prompt = _encode_prompt\n    diffusers.StableDiffusionControlNetImg2ImgPipeline._encode_prompt = _encode_prompt\n    diffusers.StableDiffusionControlNetInpaintPipeline._encode_prompt = _encode_prompt\n    try:\n        from modules.pipelines.controlnet_tile_hd import (\n            StableDiffusionControlNetTileImg2ImgPipeline,\n        )\n\n        StableDiffusionControlNetTileImg2ImgPipeline._encode_prompt = _encode_prompt\n    except:\n        print(\n            \"can't replace the StableDiffusionControlNetTileImg2ImgPipeline._encode_prompt with default _encode_prompt\"\n        )\n"}
{"type": "source_file", "path": "modules/controlnet/controlnet_inpaint.py", "content": "import torch\nfrom diffusers import StableDiffusionControlNetInpaintPipeline\nfrom diffusers.utils import is_accelerate_available, is_accelerate_version\nfrom typing import Any, Callable, Dict, List, Optional, Union\nimport gc\nimport PIL\nimport numpy as np\nimport os\nimport sys\n\n__dir__ = os.path.dirname(os.path.abspath(__file__))\nsys.path.append(os.path.dirname(__dir__))\nfrom shared import SDModel, Controlnet, MTGPU_DETECTION\n\n\ndef controlnet_inpaint(\n    sd_model: SDModel,\n    controlnet_model: Controlnet,\n    prompt: Union[str, List[str]] = None,\n    image: Union[\n        torch.FloatTensor,\n        PIL.Image.Image,\n        np.ndarray,\n        List[torch.FloatTensor],\n        List[PIL.Image.Image],\n        List[np.ndarray],\n    ] = None,\n    mask_image: Union[torch.Tensor, PIL.Image.Image] = None,\n    control_image: Union[\n        torch.FloatTensor,\n        PIL.Image.Image,\n        np.ndarray,\n        List[torch.FloatTensor],\n        List[PIL.Image.Image],\n        List[np.ndarray],\n    ] = None,\n    height: Optional[int] = None,\n    width: Optional[int] = None,\n    strength: float = 0.8,\n    num_inference_steps: int = 20,\n    guidance_scale: float = 7.5,\n    negative_prompt: Optional[Union[str, List[str]]] = None,\n    num_images_per_prompt: Optional[int] = 1,\n    eta: float = 0.0,\n    seed: Optional[Union[int, List[int]]] = None,\n    latents: Optional[torch.FloatTensor] = None,\n    prompt_embeds: Optional[torch.FloatTensor] = None,\n    negative_prompt_embeds: Optional[torch.FloatTensor] = None,\n    output_type: Optional[str] = \"pil\",\n    return_dict: bool = True,\n    callback: Optional[Callable[[int, int, torch.FloatTensor], None]] = None,\n    callback_steps: int = 1,\n    cross_attention_kwargs: Optional[Dict[str, Any]] = None,\n    controlnet_conditioning_scale: Union[float, List[float]] = 0.8,\n    guess_mode: bool = False,\n    control_guidance_start: Union[float, List[float]] = 0.0,\n    control_guidance_end: Union[float, List[float]] = 1.0,\n    lora_model_name_or_path_or_dict: Optional[Union[str, List[str]]] = None,\n    lora_alpha: Optional[float] = 0.75,\n    text_inversion_model_name_or_path: Optional[\n        Union[str, List[str], Dict[str, torch.Tensor], List[Dict[str, torch.Tensor]]]\n    ] = None,\n    fast_mode: bool = False,\n):\n\n    torch.musa.empty_cache() if MTGPU_DETECTION else torch.cuda.empty_cache()    \n    gc.collect()\n\n    controlnet_inpaint_pipeline = StableDiffusionControlNetInpaintPipeline(\n        vae=sd_model.vae,\n        text_encoder=sd_model.text_encoder,\n        tokenizer=sd_model.tokenizer,\n        unet=sd_model.unet,\n        controlnet=controlnet_model.controlnet,\n        scheduler=sd_model.scheduler,\n        safety_checker=sd_model.safety_checker,\n        feature_extractor=sd_model.feature_extractor,\n        requires_safety_checker=sd_model.requires_safety_checker,\n    )\n\n    controlnet_inpaint_pipeline.unet.oom = False\n    \n    if lora_model_name_or_path_or_dict is not None:\n        if not isinstance(lora_model_name_or_path_or_dict, list):\n            lora_model_name_or_path_or_dict = [lora_model_name_or_path_or_dict]\n        for lora_weight in lora_model_name_or_path_or_dict:\n            sd_model.load_lora(lora_weight, alpha=lora_alpha)\n\n    if text_inversion_model_name_or_path is not None:\n        if not isinstance(text_inversion_model_name_or_path, list):\n            text_inversion_model_name_or_path = [text_inversion_model_name_or_path]\n        for text_inversion_weight in text_inversion_model_name_or_path:\n            controlnet_inpaint_pipeline.load_textual_inversion(text_inversion_weight)\n\n    generator = None\n    if seed is not None:\n        if not isinstance(seed, list):\n            seed = [seed]\n        generator = [\n            torch.Generator(sd_model.device).manual_seed(single_seed)\n            for single_seed in seed\n        ]\n\n    sd_model.unet.oom = False\n    \n    results = controlnet_inpaint_pipeline(\n        prompt=prompt,\n        image=image,\n        mask_image=mask_image,\n        control_image=control_image,\n        height=height,\n        width=width,\n        strength=strength,\n        num_inference_steps=num_inference_steps,\n        guidance_scale=guidance_scale,\n        negative_prompt=negative_prompt,\n        num_images_per_prompt=num_images_per_prompt,\n        eta=eta,\n        generator=generator,\n        latents=latents,\n        prompt_embeds=prompt_embeds,\n        negative_prompt_embeds=negative_prompt_embeds,\n        output_type=output_type,\n        return_dict=return_dict,\n        callback=callback,\n        callback_steps=callback_steps,\n        cross_attention_kwargs=cross_attention_kwargs,\n        controlnet_conditioning_scale=controlnet_conditioning_scale,\n        guess_mode=guess_mode,\n        control_guidance_start=control_guidance_start,\n        control_guidance_end=control_guidance_end,\n    )\n    images = results.images\n    nsfw_content_detected = results.nsfw_content_detected\n\n    torch.musa.empty_cache() if MTGPU_DETECTION else torch.cuda.empty_cache()    \n    gc.collect()\n    \n    if (\n        sd_model.enable_offload_cpu\n        and is_accelerate_available()\n        and is_accelerate_version(\">=\", \"0.14.0\")\n    ):\n        controlnet_inpaint_pipeline.enable_model_cpu_offload()\n\n    if lora_model_name_or_path_or_dict is not None:\n        for lora_weight in lora_model_name_or_path_or_dict:\n            sd_model.offload_lora(lora_weight, alpha=lora_alpha)\n\n    return images, nsfw_content_detected\n"}
{"type": "source_file", "path": "modules/hijack/load_from_single_file.py", "content": "import diffusers\nfrom pathlib import Path\nfrom diffusers.utils import (\n    DIFFUSERS_CACHE,\n    HF_HUB_OFFLINE,\n    is_safetensors_available,\n)\n\nfrom huggingface_hub import hf_hub_download\ndef replace_from_single_file():\n    @classmethod\n    def from_single_file(cls, pretrained_model_link_or_path, **kwargs):\n        r\"\"\"\n        Instantiate a [`DiffusionPipeline`] from pretrained pipeline weights saved in the `.ckpt` format. The pipeline\n        is set in evaluation mode (`model.eval()`) by default.\n\n        Parameters:\n            pretrained_model_link_or_path (`str` or `os.PathLike`, *optional*):\n                Can be either:\n                    - A link to the `.ckpt` file (for example\n                      `\"https://huggingface.co/<repo_id>/blob/main/<path_to_file>.ckpt\"`) on the Hub.\n                    - A path to a *file* containing all pipeline weights.\n            torch_dtype (`str` or `torch.dtype`, *optional*):\n                Override the default `torch.dtype` and load the model with another dtype. If `\"auto\"` is passed, the\n                dtype is automatically derived from the model's weights.\n            force_download (`bool`, *optional*, defaults to `False`):\n                Whether or not to force the (re-)download of the model weights and configuration files, overriding the\n                cached versions if they exist.\n            cache_dir (`Union[str, os.PathLike]`, *optional*):\n                Path to a directory where a downloaded pretrained model configuration is cached if the standard cache\n                is not used.\n            resume_download (`bool`, *optional*, defaults to `False`):\n                Whether or not to resume downloading the model weights and configuration files. If set to `False`, any\n                incompletely downloaded files are deleted.\n            proxies (`Dict[str, str]`, *optional*):\n                A dictionary of proxy servers to use by protocol or endpoint, for example, `{'http': 'foo.bar:3128',\n                'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.\n            local_files_only (`bool`, *optional*, defaults to `False`):\n                Whether to only load local model weights and configuration files or not. If set to True, the model\n                won't be downloaded from the Hub.\n            use_auth_token (`str` or *bool*, *optional*):\n                The token to use as HTTP bearer authorization for remote files. If `True`, the token generated from\n                `diffusers-cli login` (stored in `~/.huggingface`) is used.\n            revision (`str`, *optional*, defaults to `\"main\"`):\n                The specific model version to use. It can be a branch name, a tag name, a commit id, or any identifier\n                allowed by Git.\n            use_safetensors (`bool`, *optional*, defaults to `None`):\n                If set to `None`, the safetensors weights are downloaded if they're available **and** if the\n                safetensors library is installed. If set to `True`, the model is forcibly loaded from safetensors\n                weights. If set to `False`, safetensors weights are not loaded.\n            extract_ema (`bool`, *optional*, defaults to `False`):\n                Whether to extract the EMA weights or not. Pass `True` to extract the EMA weights which usually yield\n                higher quality images for inference. Non-EMA weights are usually better to continue finetuning.\n            upcast_attention (`bool`, *optional*, defaults to `None`):\n                Whether the attention computation should always be upcasted.\n            image_size (`int`, *optional*, defaults to 512):\n                The image size the model was trained on. Use 512 for all Stable Diffusion v1 models and the Stable\n                Diffusion v2 base model. Use 768 for Stable Diffusion v2.\n            prediction_type (`str`, *optional*):\n                The prediction type the model was trained on. Use `'epsilon'` for all Stable Diffusion v1 models and\n                the Stable Diffusion v2 base model. Use `'v_prediction'` for Stable Diffusion v2.\n            num_in_channels (`int`, *optional*, defaults to `None`):\n                The number of input channels. If `None`, it will be automatically inferred.\n            scheduler_type (`str`, *optional*, defaults to `\"pndm\"`):\n                Type of scheduler to use. Should be one of `[\"pndm\", \"lms\", \"heun\", \"euler\", \"euler-ancestral\", \"dpm\",\n                \"ddim\"]`.\n            load_safety_checker (`bool`, *optional*, defaults to `True`):\n                Whether to load the safety checker or not.\n            text_encoder (`CLIPTextModel`, *optional*, defaults to `None`):\n                An instance of\n                [CLIP](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModel) to use,\n                specifically the [clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14)\n                variant. If this parameter is `None`, the function will load a new instance of [CLIP] by itself, if\n                needed.\n            tokenizer (`CLIPTokenizer`, *optional*, defaults to `None`):\n                An instance of\n                [CLIPTokenizer](https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer)\n                to use. If this parameter is `None`, the function will load a new instance of [CLIPTokenizer] by\n                itself, if needed.\n            kwargs (remaining dictionary of keyword arguments, *optional*):\n                Can be used to overwrite load and saveable variables (for example the pipeline components of the\n                specific pipeline class). The overwritten components are directly passed to the pipelines `__init__`\n                method. See example below for more information.\n\n        Examples:\n\n        ```py\n        >>> from diffusers import StableDiffusionPipeline\n\n        >>> # Download pipeline from huggingface.co and cache.\n        >>> pipeline = StableDiffusionPipeline.from_single_file(\n        ...     \"https://huggingface.co/WarriorMama777/OrangeMixs/blob/main/Models/AbyssOrangeMix/AbyssOrangeMix.safetensors\"\n        ... )\n\n        >>> # Download pipeline from local file\n        >>> # file is downloaded under ./v1-5-pruned-emaonly.ckpt\n        >>> pipeline = StableDiffusionPipeline.from_single_file(\"./v1-5-pruned-emaonly\")\n\n        >>> # Enable float16 and move to GPU\n        >>> pipeline = StableDiffusionPipeline.from_single_file(\n        ...     \"https://huggingface.co/runwayml/stable-diffusion-v1-5/blob/main/v1-5-pruned-emaonly.ckpt\",\n        ...     torch_dtype=torch.float16,\n        ... )\n        >>> pipeline.to(\"cuda\")\n        ```\n        \"\"\"\n        # import here to avoid circular dependency\n        from diffusers.pipelines.stable_diffusion.convert_from_ckpt import download_from_original_stable_diffusion_ckpt\n\n        cache_dir = kwargs.pop(\"cache_dir\", DIFFUSERS_CACHE)\n        original_config_file = kwargs.pop(\"original_config_file\", None)\n        resume_download = kwargs.pop(\"resume_download\", False)\n        force_download = kwargs.pop(\"force_download\", False)\n        proxies = kwargs.pop(\"proxies\", None)\n        local_files_only = kwargs.pop(\"local_files_only\", HF_HUB_OFFLINE)\n        use_auth_token = kwargs.pop(\"use_auth_token\", None)\n        revision = kwargs.pop(\"revision\", None)\n        extract_ema = kwargs.pop(\"extract_ema\", False)\n        image_size = kwargs.pop(\"image_size\", None)\n        scheduler_type = kwargs.pop(\"scheduler_type\", \"pndm\")\n        num_in_channels = kwargs.pop(\"num_in_channels\", None)\n        upcast_attention = kwargs.pop(\"upcast_attention\", None)\n        load_safety_checker = kwargs.pop(\"load_safety_checker\", True)\n        prediction_type = kwargs.pop(\"prediction_type\", None)\n        text_encoder = kwargs.pop(\"text_encoder\", None)\n        tokenizer = kwargs.pop(\"tokenizer\", None)\n\n        torch_dtype = kwargs.pop(\"torch_dtype\", None)\n\n        use_safetensors = kwargs.pop(\"use_safetensors\", None if is_safetensors_available() else False)\n\n        pipeline_name = cls.__name__\n        file_extension = pretrained_model_link_or_path.rsplit(\".\", 1)[-1]\n        from_safetensors = file_extension == \"safetensors\"\n\n        if from_safetensors and use_safetensors is False:\n            raise ValueError(\"Make sure to install `safetensors` with `pip install safetensors`.\")\n\n        # TODO: For now we only support stable diffusion\n        stable_unclip = None\n        model_type = None\n        controlnet = False\n\n        if pipeline_name == \"StableDiffusionControlNetPipeline\":\n            # Model type will be inferred from the checkpoint.\n            controlnet = True\n        elif \"StableDiffusion\" in pipeline_name:\n            # Model type will be inferred from the checkpoint.\n            pass\n        elif pipeline_name == \"StableUnCLIPPipeline\":\n            model_type = \"FrozenOpenCLIPEmbedder\"\n            stable_unclip = \"txt2img\"\n        elif pipeline_name == \"StableUnCLIPImg2ImgPipeline\":\n            model_type = \"FrozenOpenCLIPEmbedder\"\n            stable_unclip = \"img2img\"\n        elif pipeline_name == \"PaintByExamplePipeline\":\n            model_type = \"PaintByExample\"\n        elif pipeline_name == \"LDMTextToImagePipeline\":\n            model_type = \"LDMTextToImage\"\n        else:\n            raise ValueError(f\"Unhandled pipeline class: {pipeline_name}\")\n\n        # remove huggingface url\n        for prefix in [\"https://huggingface.co/\", \"huggingface.co/\", \"hf.co/\", \"https://hf.co/\"]:\n            if pretrained_model_link_or_path.startswith(prefix):\n                pretrained_model_link_or_path = pretrained_model_link_or_path[len(prefix) :]\n\n        # Code based on diffusers.pipelines.pipeline_utils.DiffusionPipeline.from_pretrained\n        ckpt_path = Path(pretrained_model_link_or_path)\n        if not ckpt_path.is_file():\n            # get repo_id and (potentially nested) file path of ckpt in repo\n            repo_id = \"/\".join(ckpt_path.parts[:2])\n            file_path = \"/\".join(ckpt_path.parts[2:])\n\n            if file_path.startswith(\"blob/\"):\n                file_path = file_path[len(\"blob/\") :]\n\n            if file_path.startswith(\"main/\"):\n                file_path = file_path[len(\"main/\") :]\n\n            pretrained_model_link_or_path = hf_hub_download(\n                repo_id,\n                filename=file_path,\n                cache_dir=cache_dir,\n                resume_download=resume_download,\n                proxies=proxies,\n                local_files_only=local_files_only,\n                use_auth_token=use_auth_token,\n                revision=revision,\n                force_download=force_download,\n            )\n        pipe = download_from_original_stable_diffusion_ckpt(\n            pretrained_model_link_or_path,\n            original_config_file=original_config_file,\n            pipeline_class=cls,\n            model_type=model_type,\n            stable_unclip=stable_unclip,\n            controlnet=controlnet,\n            from_safetensors=from_safetensors,\n            extract_ema=extract_ema,\n            image_size=image_size,\n            scheduler_type=scheduler_type,\n            num_in_channels=num_in_channels,\n            upcast_attention=upcast_attention,\n            load_safety_checker=load_safety_checker,\n            prediction_type=prediction_type,\n            text_encoder=text_encoder,\n            tokenizer=tokenizer,\n        )\n\n        if torch_dtype is not None:\n            pipe.to(torch_dtype=torch_dtype)\n\n        return pipe\n    diffusers.loaders.FromSingleFileMixin.from_single_file = from_single_file\n\n    \n"}
{"type": "source_file", "path": "modules/controlnet/controlnet_tileimg2img.py", "content": "import torch\nfrom diffusers.utils import is_accelerate_available, is_accelerate_version\nfrom typing import Any, Callable, Dict, List, Optional, Union\nimport gc\nimport PIL\nimport numpy as np\nimport os\nimport sys\n\n__dir__ = os.path.dirname(os.path.abspath(__file__))\nsys.path.append(os.path.dirname(__dir__))\nfrom shared import SDModel, Controlnet, MTGPU_DETECTION\nfrom modules.pipelines.controlnet_tile_hd import (\n    StableDiffusionControlNetTileImg2ImgPipeline,\n)\n\n#  prompt  image to image \n\ndef controlnet_tileimg2img(\n    sd_model: SDModel,\n    controlnet_model: Controlnet,\n    prompt: Union[str, List[str]] = None,\n    image: Union[\n        torch.FloatTensor,\n        PIL.Image.Image,\n        np.ndarray,\n        List[torch.FloatTensor],\n        List[PIL.Image.Image],\n        List[np.ndarray],\n    ] = None,\n    control_image: Union[\n        torch.FloatTensor,\n        PIL.Image.Image,\n        np.ndarray,\n        List[torch.FloatTensor],\n        List[PIL.Image.Image],\n        List[np.ndarray],\n    ] = None,\n    height: Optional[int] = None,\n    width: Optional[int] = None,\n    strength: float = 0.35,\n    num_inference_steps: int = 20,\n    guidance_scale: float = 7.5,\n    negative_prompt: Optional[Union[str, List[str]]] = None,\n    num_images_per_prompt: Optional[int] = 1,\n    eta: float = 0.0,\n    seed: Optional[Union[int, List[int]]] = None,\n    latents: Optional[torch.FloatTensor] = None,\n    prompt_embeds: Optional[torch.FloatTensor] = None,\n    negative_prompt_embeds: Optional[torch.FloatTensor] = None,\n    output_type: Optional[str] = \"pil\",\n    return_dict: bool = True,\n    callback: Optional[Callable[[int, int, torch.FloatTensor], None]] = None,\n    callback_steps: int = 1,\n    cross_attention_kwargs: Optional[Dict[str, Any]] = None,\n    controlnet_conditioning_scale: Union[float, List[float]] = 0.8,\n    guess_mode: bool = False,\n    control_guidance_start: Union[float, List[float]] = 0.0,\n    control_guidance_end: Union[float, List[float]] = 1.0,\n    window_size: int = 80,\n    lora_model_name_or_path_or_dict: Optional[Union[str, List[str]]] = None,\n    lora_alpha: Optional[float] = 0.75,\n    text_inversion_model_name_or_path: Optional[\n        Union[str, List[str], Dict[str, torch.Tensor], List[Dict[str, torch.Tensor]]]\n    ] = None,\n    fast_mode: bool = False,\n):\n\n    torch.musa.empty_cache() if MTGPU_DETECTION else torch.cuda.empty_cache()     \n    gc.collect()\n\n    tiled_controlnet_img2img_pipeline = StableDiffusionControlNetTileImg2ImgPipeline(\n        vae=sd_model.vae,\n        text_encoder=sd_model.text_encoder,\n        tokenizer=sd_model.tokenizer,\n        unet=sd_model.unet.unet,\n        controlnet=controlnet_model.controlnet,\n        scheduler=sd_model.scheduler,\n        safety_checker=sd_model.safety_checker,\n        feature_extractor=sd_model.feature_extractor,\n        requires_safety_checker=sd_model.requires_safety_checker,\n    )\n    \n    tiled_controlnet_img2img_pipeline.enable_attention_slicing()\n\n    if lora_model_name_or_path_or_dict is not None:\n        if not isinstance(lora_model_name_or_path_or_dict, list):\n            lora_model_name_or_path_or_dict = [lora_model_name_or_path_or_dict]\n        for lora_weight in lora_model_name_or_path_or_dict:\n            sd_model.load_lora(lora_weight, alpha=lora_alpha)\n\n    if text_inversion_model_name_or_path is not None:\n        if not isinstance(text_inversion_model_name_or_path, list):\n            text_inversion_model_name_or_path = [text_inversion_model_name_or_path]\n        for text_inversion_weight in text_inversion_model_name_or_path:\n            tiled_controlnet_img2img_pipeline.load_textual_inversion(\n                text_inversion_weight\n            )\n\n    generator = None\n    if seed is not None:\n        if not isinstance(seed, list):\n            seed = [seed]\n        generator = [\n            torch.Generator(sd_model.device).manual_seed(single_seed)\n            for single_seed in seed\n        ]\n\n    sd_model.unet.oom = False\n    \n    results = tiled_controlnet_img2img_pipeline(\n        prompt=prompt,\n        image=image,\n        control_image=control_image,\n        height=height,\n        width=width,\n        strength=strength,\n        num_inference_steps=num_inference_steps,\n        guidance_scale=guidance_scale,\n        negative_prompt=negative_prompt,\n        num_images_per_prompt=num_images_per_prompt,\n        eta=eta,\n        generator=generator,\n        latents=latents,\n        prompt_embeds=prompt_embeds,\n        negative_prompt_embeds=negative_prompt_embeds,\n        output_type=output_type,\n        return_dict=return_dict,\n        callback=callback,\n        callback_steps=callback_steps,\n        cross_attention_kwargs=cross_attention_kwargs,\n        controlnet_conditioning_scale=controlnet_conditioning_scale,\n        guess_mode=guess_mode,\n        control_guidance_start=control_guidance_start,\n        control_guidance_end=control_guidance_end,\n        window_size=window_size,\n    )\n    images = results.images\n    nsfw_content_detected = results.nsfw_content_detected\n\n    torch.musa.empty_cache() if MTGPU_DETECTION else torch.cuda.empty_cache()    \n    gc.collect()\n    \n    if (\n        sd_model.enable_offload_cpu\n        and is_accelerate_available()\n        and is_accelerate_version(\">=\", \"0.14.0\")\n    ):\n        tiled_controlnet_img2img_pipeline.enable_model_cpu_offload()\n\n    if lora_model_name_or_path_or_dict is not None:\n        for lora_weight in lora_model_name_or_path_or_dict:\n            sd_model.offload_lora(lora_weight, alpha=lora_alpha)\n\n    return images, nsfw_content_detected\n"}
{"type": "source_file", "path": "modules/hijack/pipeline_stable_diffusion.py", "content": "# Copyright 2023 The HuggingFace Team. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport inspect\nimport warnings\nfrom typing import Any, Callable, Dict, List, Optional, Union\n\nimport torch\nfrom packaging import version\nfrom transformers import CLIPImageProcessor, CLIPTextModel, CLIPTokenizer\n\nimport diffusers\nfrom diffusers.configuration_utils import FrozenDict\nfrom diffusers.image_processor import VaeImageProcessor\nfrom diffusers.loaders import (\n    FromSingleFileMixin,\n    LoraLoaderMixin,\n    TextualInversionLoaderMixin,\n)\nfrom diffusers.models import AutoencoderKL, UNet2DConditionModel\nfrom diffusers.schedulers import KarrasDiffusionSchedulers\nfrom diffusers.utils import (\n    deprecate,\n    is_accelerate_available,\n    is_accelerate_version,\n    logging,\n    randn_tensor,\n    replace_example_docstring,\n)\nfrom diffusers.pipelines.pipeline_utils import DiffusionPipeline\nfrom diffusers.pipelines.stable_diffusion import StableDiffusionPipelineOutput\nfrom diffusers.pipelines.stable_diffusion.safety_checker import (\n    StableDiffusionSafetyChecker,\n)\n\nlogger = logging.get_logger(__name__)  # pylint: disable=invalid-name\n\nEXAMPLE_DOC_STRING = \"\"\"\n    Examples:\n        ```py\n        >>> import torch\n        >>> from diffusers import StableDiffusionPipeline\n\n        >>> pipe = StableDiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16)\n        >>> pipe = pipe.to(\"cuda\")\n\n        >>> prompt = \"a photo of an astronaut riding a horse on mars\"\n        >>> image = pipe(prompt).images[0]\n        ```\n\"\"\"\n\n\ndef replace_pipeline_stable_diffusion_call():\n    @torch.no_grad()\n    @replace_example_docstring(EXAMPLE_DOC_STRING)\n    def __call__(\n        self,\n        prompt: Union[str, List[str]] = None,\n        height: Optional[int] = None,\n        width: Optional[int] = None,\n        num_inference_steps: int = 50,\n        guidance_scale: float = 7.5,\n        negative_prompt: Optional[Union[str, List[str]]] = None,\n        num_images_per_prompt: Optional[int] = 1,\n        eta: float = 0.0,\n        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n        latents: Optional[torch.FloatTensor] = None,\n        prompt_embeds: Optional[torch.FloatTensor] = None,\n        negative_prompt_embeds: Optional[torch.FloatTensor] = None,\n        output_type: Optional[str] = \"pil\",\n        return_dict: bool = True,\n        callback: Optional[Callable[[int, int, torch.FloatTensor], None]] = None,\n        callback_steps: int = 1,\n        cross_attention_kwargs: Optional[Dict[str, Any]] = None,\n        guidance_rescale: float = 0.0,\n    ):\n        r\"\"\"\n        Function invoked when calling the pipeline for generation.\n\n        Args:\n            prompt (`str` or `List[str]`, *optional*):\n                The prompt or prompts to guide the image generation. If not defined, one has to pass `prompt_embeds`.\n                instead.\n            height (`int`, *optional*, defaults to self.unet.config.sample_size * self.vae_scale_factor):\n                The height in pixels of the generated image.\n            width (`int`, *optional*, defaults to self.unet.config.sample_size * self.vae_scale_factor):\n                The width in pixels of the generated image.\n            num_inference_steps (`int`, *optional*, defaults to 50):\n                The number of denoising steps. More denoising steps usually lead to a higher quality image at the\n                expense of slower inference.\n            guidance_scale (`float`, *optional*, defaults to 7.5):\n                Guidance scale as defined in [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598).\n                `guidance_scale` is defined as `w` of equation 2. of [Imagen\n                Paper](https://arxiv.org/pdf/2205.11487.pdf). Guidance scale is enabled by setting `guidance_scale >\n                1`. Higher guidance scale encourages to generate images that are closely linked to the text `prompt`,\n                usually at the expense of lower image quality.\n            negative_prompt (`str` or `List[str]`, *optional*):\n                The prompt or prompts not to guide the image generation. If not defined, one has to pass\n                `negative_prompt_embeds` instead. Ignored when not using guidance (i.e., ignored if `guidance_scale` is\n                less than `1`).\n            num_images_per_prompt (`int`, *optional*, defaults to 1):\n                The number of images to generate per prompt.\n            eta (`float`, *optional*, defaults to 0.0):\n                Corresponds to parameter eta () in the DDIM paper: https://arxiv.org/abs/2010.02502. Only applies to\n                [`schedulers.DDIMScheduler`], will be ignored for others.\n            generator (`torch.Generator` or `List[torch.Generator]`, *optional*):\n                One or a list of [torch generator(s)](https://pytorch.org/docs/stable/generated/torch.Generator.html)\n                to make generation deterministic.\n            latents (`torch.FloatTensor`, *optional*):\n                Pre-generated noisy latents, sampled from a Gaussian distribution, to be used as inputs for image\n                generation. Can be used to tweak the same generation with different prompts. If not provided, a latents\n                tensor will ge generated by sampling using the supplied random `generator`.\n            prompt_embeds (`torch.FloatTensor`, *optional*):\n                Pre-generated text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not\n                provided, text embeddings will be generated from `prompt` input argument.\n            negative_prompt_embeds (`torch.FloatTensor`, *optional*):\n                Pre-generated negative text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt\n                weighting. If not provided, negative_prompt_embeds will be generated from `negative_prompt` input\n                argument.\n            output_type (`str`, *optional*, defaults to `\"pil\"`):\n                The output format of the generate image. Choose between\n                [PIL](https://pillow.readthedocs.io/en/stable/): `PIL.Image.Image` or `np.array`.\n            return_dict (`bool`, *optional*, defaults to `True`):\n                Whether or not to return a [`~pipelines.stable_diffusion.StableDiffusionPipelineOutput`] instead of a\n                plain tuple.\n            callback (`Callable`, *optional*):\n                A function that will be called every `callback_steps` steps during inference. The function will be\n                called with the following arguments: `callback(step: int, timestep: int, latents: torch.FloatTensor)`.\n            callback_steps (`int`, *optional*, defaults to 1):\n                The frequency at which the `callback` function will be called. If not specified, the callback will be\n                called at every step.\n            cross_attention_kwargs (`dict`, *optional*):\n                A kwargs dictionary that if specified is passed along to the `AttentionProcessor` as defined under\n                `self.processor` in\n                [diffusers.cross_attention](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/cross_attention.py).\n            guidance_rescale (`float`, *optional*, defaults to 0.7):\n                Guidance rescale factor proposed by [Common Diffusion Noise Schedules and Sample Steps are\n                Flawed](https://arxiv.org/pdf/2305.08891.pdf) `guidance_scale` is defined as `` in equation 16. of\n                [Common Diffusion Noise Schedules and Sample Steps are Flawed](https://arxiv.org/pdf/2305.08891.pdf).\n                Guidance rescale factor should fix overexposure when using zero terminal SNR.\n\n        Examples:\n\n        Returns:\n            [`~pipelines.stable_diffusion.StableDiffusionPipelineOutput`] or `tuple`:\n            [`~pipelines.stable_diffusion.StableDiffusionPipelineOutput`] if `return_dict` is True, otherwise a `tuple.\n            When returning a tuple, the first element is a list with the generated images, and the second element is a\n            list of `bool`s denoting whether the corresponding generated image likely represents \"not-safe-for-work\"\n            (nsfw) content, according to the `safety_checker`.\n        \"\"\"\n        # 0. Default height and width to unet\n        height = height or self.unet.config.sample_size * self.vae_scale_factor\n        width = width or self.unet.config.sample_size * self.vae_scale_factor\n\n        # 1. Check inputs. Raise error if not correct\n        self.check_inputs(\n            prompt,\n            height,\n            width,\n            callback_steps,\n            negative_prompt,\n            prompt_embeds,\n            negative_prompt_embeds,\n        )\n\n        # 2. Define call parameters\n        if prompt is not None and isinstance(prompt, str):\n            batch_size = 1\n        elif prompt is not None and isinstance(prompt, list):\n            batch_size = len(prompt)\n        else:\n            batch_size = prompt_embeds.shape[0]\n\n        device = self._execution_device\n        # here `guidance_scale` is defined analog to the guidance weight `w` of equation (2)\n        # of the Imagen paper: https://arxiv.org/pdf/2205.11487.pdf . `guidance_scale = 1`\n        # corresponds to doing no classifier free guidance.\n        do_classifier_free_guidance = guidance_scale > 1.0\n\n        # 3. Encode input prompt\n        text_encoder_lora_scale = (\n            cross_attention_kwargs.get(\"scale\", None)\n            if cross_attention_kwargs is not None\n            else None\n        )\n        prompt_embeds = self._encode_prompt(\n            prompt,\n            device,\n            num_images_per_prompt,\n            do_classifier_free_guidance,\n            negative_prompt,\n            prompt_embeds=prompt_embeds,\n            negative_prompt_embeds=negative_prompt_embeds,\n            lora_scale=text_encoder_lora_scale,\n        )\n\n        # 4. Prepare timesteps\n        self.scheduler.set_timesteps(num_inference_steps, device=device)\n        timesteps = self.scheduler.timesteps\n\n        # 5. Prepare latent variables\n        num_channels_latents = self.unet.config.in_channels\n        latents = self.prepare_latents(\n            batch_size * num_images_per_prompt,\n            num_channels_latents,\n            height,\n            width,\n            prompt_embeds.dtype,\n            device,\n            generator,\n            latents,\n        )\n\n        # 6. Prepare extra step kwargs. TODO: Logic should ideally just be moved out of the pipeline\n        extra_step_kwargs = self.prepare_extra_step_kwargs(generator, eta)\n\n        # 7. Denoising loop\n        will_oom = False\n        num_warmup_steps = len(timesteps) - num_inference_steps * self.scheduler.order\n        with self.progress_bar(total=num_inference_steps) as progress_bar:\n            for i, t in enumerate(timesteps):\n                # expand the latents if we are doing classifier free guidance\n                latent_model_input = (\n                    torch.cat([latents] * 2) if do_classifier_free_guidance else latents\n                )\n                latent_model_input = self.scheduler.scale_model_input(\n                    latent_model_input, t\n                )\n\n                # predict the noise residual\n                if will_oom:\n                    noise_pred_uncond = self.unet(\n                        latent_model_input[:1],\n                        t,\n                        encoder_hidden_states=prompt_embeds[:1],\n                        cross_attention_kwargs=cross_attention_kwargs,\n                        return_dict=False,\n                    )[0]\n                    noise_pred_text = self.unet(\n                        latent_model_input[1:],\n                        t,\n                        encoder_hidden_states=prompt_embeds[1:],\n                        cross_attention_kwargs=cross_attention_kwargs,\n                        return_dict=False,\n                    )[0]\n                    noise_pred = torch.cat([noise_pred_uncond, noise_pred_text])\n                else:\n                    try:\n                        noise_pred = self.unet(\n                            latent_model_input,\n                            t,\n                            encoder_hidden_states=prompt_embeds,\n                            cross_attention_kwargs=cross_attention_kwargs,\n                            return_dict=False,\n                        )[0]\n                    except:\n                        will_oom = True\n\n                        noise_pred_uncond = self.unet(\n                            latent_model_input[:1],\n                            t,\n                            encoder_hidden_states=prompt_embeds[:1],\n                            cross_attention_kwargs=cross_attention_kwargs,\n                            return_dict=False,\n                        )[0]\n                        noise_pred_text = self.unet(\n                            latent_model_input[1:],\n                            t,\n                            encoder_hidden_states=prompt_embeds[1:],\n                            cross_attention_kwargs=cross_attention_kwargs,\n                            return_dict=False,\n                        )[0]\n                        noise_pred = torch.cat([noise_pred_uncond, noise_pred_text])\n\n                # perform guidance\n                if do_classifier_free_guidance:\n                    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n                    noise_pred = noise_pred_uncond + guidance_scale * (\n                        noise_pred_text - noise_pred_uncond\n                    )\n\n                if do_classifier_free_guidance and guidance_rescale > 0.0:\n                    # Based on 3.4. in https://arxiv.org/pdf/2305.08891.pdf\n                    noise_pred = rescale_noise_cfg(\n                        noise_pred, noise_pred_text, guidance_rescale=guidance_rescale\n                    )\n\n                # compute the previous noisy sample x_t -> x_t-1\n                latents = self.scheduler.step(\n                    noise_pred, t, latents, **extra_step_kwargs, return_dict=False\n                )[0]\n\n                # call the callback, if provided\n                if i == len(timesteps) - 1 or (\n                    (i + 1) > num_warmup_steps and (i + 1) % self.scheduler.order == 0\n                ):\n                    progress_bar.update()\n                    if callback is not None and i % callback_steps == 0:\n                        callback(i, t, latents)\n\n        if not output_type == \"latent\":\n            image = self.vae.decode(\n                latents / self.vae.config.scaling_factor, return_dict=False\n            )[0]\n            image, has_nsfw_concept = self.run_safety_checker(\n                image, device, prompt_embeds.dtype\n            )\n        else:\n            image = latents\n            has_nsfw_concept = None\n\n        if has_nsfw_concept is None:\n            do_denormalize = [True] * image.shape[0]\n        else:\n            do_denormalize = [not has_nsfw for has_nsfw in has_nsfw_concept]\n\n        image = self.image_processor.postprocess(\n            image, output_type=output_type, do_denormalize=do_denormalize\n        )\n\n        # Offload last model to CPU\n        if hasattr(self, \"final_offload_hook\") and self.final_offload_hook is not None:\n            self.final_offload_hook.offload()\n\n        if not return_dict:\n            return (image, has_nsfw_concept)\n\n        return StableDiffusionPipelineOutput(\n            images=image, nsfw_content_detected=has_nsfw_concept\n        )\n\n    diffusers.StableDiffusionPipeline.__call__ = __call__\n"}
{"type": "source_file", "path": "modules/controlnet/controlnet_txt2img.py", "content": "import torch\nfrom diffusers import StableDiffusionControlNetPipeline\nfrom diffusers.utils import is_accelerate_available, is_accelerate_version\nfrom typing import Any, Callable, Dict, List, Optional, Union\nimport gc\nimport PIL\nimport numpy as np\nimport os\nimport sys\n\n__dir__ = os.path.dirname(os.path.abspath(__file__))\nsys.path.append(os.path.dirname(__dir__))\nfrom shared import SDModel, Controlnet, MTGPU_DETECTION\n\n\ndef controlnet_txt2img(\n    sd_model: SDModel,\n    controlnet_model: Controlnet,\n    prompt: Union[str, List[str]] = None,\n    image: Union[\n        torch.FloatTensor,\n        PIL.Image.Image,\n        np.ndarray,\n        List[torch.FloatTensor],\n        List[PIL.Image.Image],\n        List[np.ndarray],\n    ] = None,\n    height: Optional[int] = None,\n    width: Optional[int] = None,\n    num_inference_steps: int = 20,\n    guidance_scale: float = 7.5,\n    negative_prompt: Optional[Union[str, List[str]]] = None,\n    num_images_per_prompt: Optional[int] = 1,\n    eta: float = 0.0,\n    seed: Optional[Union[int, List[int]]] = None,\n    latents: Optional[torch.FloatTensor] = None,\n    prompt_embeds: Optional[torch.FloatTensor] = None,\n    negative_prompt_embeds: Optional[torch.FloatTensor] = None,\n    output_type: Optional[str] = \"pil\",\n    return_dict: bool = True,\n    callback: Optional[Callable[[int, int, torch.FloatTensor], None]] = None,\n    callback_steps: int = 1,\n    cross_attention_kwargs: Optional[Dict[str, Any]] = None,\n    controlnet_conditioning_scale: Union[float, List[float]] = 1.0,\n    guess_mode: bool = False,\n    control_guidance_start: Union[float, List[float]] = 0.0,\n    control_guidance_end: Union[float, List[float]] = 1.0,\n    lora_model_name_or_path_or_dict: Optional[Union[str, List[str]]] = None,\n    lora_alpha: Optional[float] = 0.75,\n    text_inversion_model_name_or_path: Optional[\n        Union[str, List[str], Dict[str, torch.Tensor], List[Dict[str, torch.Tensor]]]\n    ] = None,\n    fast_mode: bool = False,\n):\n\n    torch.musa.empty_cache() if MTGPU_DETECTION else torch.cuda.empty_cache()    \n    gc.collect()\n\n    controlnet_txt2img_pipeline = StableDiffusionControlNetPipeline(\n        vae=sd_model.vae,\n        text_encoder=sd_model.text_encoder,\n        tokenizer=sd_model.tokenizer,\n        unet=sd_model.unet,\n        controlnet=controlnet_model.controlnet,\n        scheduler=sd_model.fast_scheduler if fast_mode else sd_model.scheduler,\n        safety_checker=sd_model.safety_checker,\n        feature_extractor=sd_model.feature_extractor,\n        requires_safety_checker=sd_model.requires_safety_checker,\n    )\n    \n    controlnet_txt2img_pipeline.unet.oom = False\n\n    if lora_model_name_or_path_or_dict is not None:\n        if not isinstance(lora_model_name_or_path_or_dict, list):\n            lora_model_name_or_path_or_dict = [lora_model_name_or_path_or_dict]\n        for lora_weight in lora_model_name_or_path_or_dict:\n            sd_model.load_lora(lora_weight, alpha=lora_alpha)\n\n    if text_inversion_model_name_or_path is not None:\n        if not isinstance(text_inversion_model_name_or_path, list):\n            text_inversion_model_name_or_path = [text_inversion_model_name_or_path]\n        for text_inversion_weight in text_inversion_model_name_or_path:\n            controlnet_txt2img_pipeline.load_textual_inversion(text_inversion_weight)\n\n    generator = None\n    if seed is not None:\n        if not isinstance(seed, list):\n            seed = [seed]\n        generator = [\n            torch.Generator(sd_model.device).manual_seed(single_seed)\n            for single_seed in seed\n        ]\n    \n    sd_model.unet.oom = False\n\n    results = controlnet_txt2img_pipeline(\n        prompt=prompt,\n        image=image,\n        height=height,\n        width=width,\n        num_inference_steps=num_inference_steps,\n        guidance_scale=guidance_scale,\n        negative_prompt=negative_prompt,\n        num_images_per_prompt=num_images_per_prompt,\n        eta=eta,\n        generator=generator,\n        latents=latents,\n        prompt_embeds=prompt_embeds,\n        negative_prompt_embeds=negative_prompt_embeds,\n        output_type=output_type,\n        return_dict=return_dict,\n        callback=callback,\n        callback_steps=callback_steps,\n        cross_attention_kwargs=cross_attention_kwargs,\n        controlnet_conditioning_scale=controlnet_conditioning_scale,\n        guess_mode=guess_mode,\n        control_guidance_start=control_guidance_start,\n        control_guidance_end=control_guidance_end,\n    )\n    images = results.images\n    nsfw_content_detected = results.nsfw_content_detected\n\n    torch.musa.empty_cache() if MTGPU_DETECTION else torch.cuda.empty_cache()    \n    gc.collect()\n    \n    if (\n        sd_model.enable_offload_cpu\n        and is_accelerate_available()\n        and is_accelerate_version(\">=\", \"0.14.0\")\n    ):\n        controlnet_txt2img_pipeline.enable_model_cpu_offload()\n\n    if lora_model_name_or_path_or_dict is not None:\n        for lora_weight in lora_model_name_or_path_or_dict:\n            sd_model.offload_lora(lora_weight, alpha=lora_alpha)\n\n    return images, nsfw_content_detected\n"}
{"type": "source_file", "path": "examples/example_lora2img.py", "content": "import torch\nimport os\nimport sys\nimport argparse\n\n__dir__ = os.path.dirname(os.path.abspath(__file__))\nsys.path.append(os.path.dirname(__dir__))\nfrom modules.shared import SDModel, device\nfrom modules.basic.txt2img import txt2img\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument(\n        \"--checkpoint_path\",\n        default=None,\n        type=str,\n        required=True,\n        help=\"Path to the stable diffusion model.\",\n    )\n\n    parser.add_argument(\n        \"--lora_path\",\n        default=None,\n        type=str,\n        required=True,\n        help=\"Path to the Lora model.\",\n    )\n\n    parser.add_argument(\n        \"--prompt\",\n        default=None,\n        type=str,\n        required=True,\n        help=\"Describe the image content with text\",\n    )\n\n    parser.add_argument(\n        \"--output_path\",\n        default=\"./\",\n        type=str,\n        required=True,\n        help=\"Result folder path\",\n    )\n\n    parser.add_argument(\n        \"--lora_alpha\", default=0.75, required=False, type=float, help=\"Lora weight\"\n    )\n\n    parser.add_argument(\n        \"--negative_prompt\",\n        default=None,\n        type=str,\n        required=False,\n        help=\"Content which you don't want to show up with text\",\n    )\n\n    parser.add_argument(\n        \"--diffusers_format\",\n        action=\"store_true\",\n        required=False,\n        help=\"Model files in diffusers format\",\n    )\n    parser.add_argument(\n        \"--seed\", default=2023, required=False, type=int, help=\"Random seed\"\n    )\n\n    args = parser.parse_args()\n\n    sd_model = SDModel(device=device)\n    sd_model.load_models(args.checkpoint_path, diffusers_format=args.diffusers_format)\n\n    image, status = txt2img(\n        sd_model,\n        prompt=args.prompt,\n        negative_prompt=args.negative_prompt,\n        seed=args.seed,\n        lora_model_name_or_path_or_dict=args.lora_path,\n        lora_alpha=args.lora_alpha,\n    )\n\n    os.makedirs(args.output_path, exist_ok=True)\n    image[0].save(os.path.join(args.output_path, \"lora2img.png\"))\n"}
{"type": "source_file", "path": "modules/controlnet/inpaint2img.py", "content": "import torch\nimport numpy as np\nfrom typing import Any, Callable, Dict, List, Optional, Union\nfrom PIL import Image\nimport os\nimport sys\n\n__dir__ = os.path.dirname(os.path.abspath(__file__))\nsys.path.append(os.path.dirname(os.path.dirname(__dir__)))\nfrom modules.shared import SDModel, Controlnet\nfrom modules.controlnet.controlnet_inpaint import controlnet_inpaint\nfrom modules.wrappers import InpaintingConditionalImageWrapper\n\n\ndef inpaint2img(\n    sd_model: SDModel,\n    controlnet_model: Controlnet,\n    processor: InpaintingConditionalImageWrapper,\n    ori_image: Image,\n    mask_image: Image,\n    prompt: Union[str, List[str]] = None,\n    height: Optional[int] = None,\n    width: Optional[int] = None,\n    strength: float = 0.8,\n    num_inference_steps: int = 20,\n    guidance_scale: float = 7.5,\n    negative_prompt: Optional[Union[str, List[str]]] = None,\n    num_images_per_prompt: Optional[int] = 1,\n    eta: float = 0.0,\n    seed: Optional[Union[int, List[int]]] = None,\n    latents: Optional[torch.FloatTensor] = None,\n    prompt_embeds: Optional[torch.FloatTensor] = None,\n    negative_prompt_embeds: Optional[torch.FloatTensor] = None,\n    output_type: Optional[str] = \"pil\",\n    return_dict: bool = True,\n    callback: Optional[Callable[[int, int, torch.FloatTensor], None]] = None,\n    callback_steps: int = 1,\n    cross_attention_kwargs: Optional[Dict[str, Any]] = None,\n    controlnet_conditioning_scale: Union[float, List[float]] = 0.8,\n    guess_mode: bool = False,\n    control_guidance_start: Union[float, List[float]] = 0.0,\n    control_guidance_end: Union[float, List[float]] = 1.0,\n    lora_model_name_or_path_or_dict: Optional[Union[str, List[str]]] = None,\n    lora_alpha: Optional[float] = 0.75,\n    text_inversion_model_name_or_path: Optional[\n        Union[str, List[str], Dict[str, torch.Tensor], List[Dict[str, torch.Tensor]]]\n    ] = None,\n):\n\n    inpaint_image = processor(input_image=ori_image, mask_image=mask_image)\n\n    images, nsfw_content_detected = controlnet_inpaint(\n        sd_model=sd_model,\n        controlnet_model=controlnet_model,\n        prompt=prompt,\n        image=ori_image,\n        mask_image=mask_image,\n        control_image=inpaint_image,\n        height=height,\n        width=width,\n        strength=strength,\n        num_inference_steps=num_inference_steps,\n        guidance_scale=guidance_scale,\n        negative_prompt=negative_prompt,\n        num_images_per_prompt=num_images_per_prompt,\n        eta=eta,\n        seed=seed,\n        latents=latents,\n        prompt_embeds=prompt_embeds,\n        negative_prompt_embeds=negative_prompt_embeds,\n        output_type=output_type,\n        return_dict=return_dict,\n        callback=callback,\n        callback_steps=callback_steps,\n        cross_attention_kwargs=cross_attention_kwargs,\n        controlnet_conditioning_scale=controlnet_conditioning_scale,\n        guess_mode=guess_mode,\n        control_guidance_start=control_guidance_start,\n        control_guidance_end=control_guidance_end,\n        lora_model_name_or_path_or_dict=lora_model_name_or_path_or_dict,\n        lora_alpha=lora_alpha,\n        text_inversion_model_name_or_path=text_inversion_model_name_or_path,\n    )\n\n    inpaint_image = inpaint_image.numpy()[0].transpose(1, 2, 0)\n    inpaint_image = np.clip((inpaint_image * 255.0), 0, 255).astype(np.uint8)\n    inpaint_image = Image.fromarray(inpaint_image)\n\n    return images, nsfw_content_detected, inpaint_image\n"}
{"type": "source_file", "path": "modules/controlnet/ip2p2img.py", "content": "import torch\nfrom typing import Any, Callable, Dict, List, Optional, Union\nfrom PIL import Image\nimport os\nimport sys\n\n__dir__ = os.path.dirname(os.path.abspath(__file__))\nsys.path.append(os.path.dirname(os.path.dirname(__dir__)))\nfrom modules.shared import SDModel, Controlnet\nfrom modules.controlnet.controlnet_txt2img import controlnet_txt2img\nfrom modules.controlnet.controlnet_img2img import controlnet_img2img\n\n\ndef ip2p2img(\n    sd_model: SDModel,\n    controlnet_model: Controlnet,\n    ori_image: Image,\n    aux_image: Image = None,\n    prompt: Union[str, List[str]] = None,\n    height: Optional[int] = None,\n    width: Optional[int] = None,\n    strength: float = 0.8,\n    num_inference_steps: int = 20,\n    guidance_scale: float = 7.5,\n    negative_prompt: Optional[Union[str, List[str]]] = None,\n    num_images_per_prompt: Optional[int] = 1,\n    eta: float = 0.0,\n    seed: Optional[Union[int, List[int]]] = None,\n    latents: Optional[torch.FloatTensor] = None,\n    prompt_embeds: Optional[torch.FloatTensor] = None,\n    negative_prompt_embeds: Optional[torch.FloatTensor] = None,\n    output_type: Optional[str] = \"pil\",\n    return_dict: bool = True,\n    callback: Optional[Callable[[int, int, torch.FloatTensor], None]] = None,\n    callback_steps: int = 1,\n    cross_attention_kwargs: Optional[Dict[str, Any]] = None,\n    controlnet_conditioning_scale: Union[float, List[float]] = 0.8,\n    guess_mode: bool = False,\n    control_guidance_start: Union[float, List[float]] = 0.0,\n    control_guidance_end: Union[float, List[float]] = 1.0,\n    lora_model_name_or_path_or_dict: Optional[Union[str, List[str]]] = None,\n    lora_alpha: Optional[float] = 0.75,\n    text_inversion_model_name_or_path: Optional[\n        Union[str, List[str], Dict[str, torch.Tensor], List[Dict[str, torch.Tensor]]]\n    ] = None,\n    fast_mode = False\n):\n\n    ip2p_image = ori_image\n\n    if aux_image is None:\n        images, nsfw_content_detected = controlnet_txt2img(\n            sd_model=sd_model,\n            controlnet_model=controlnet_model,\n            prompt=prompt,\n            image=ip2p_image,\n            height=height,\n            width=width,\n            num_inference_steps=num_inference_steps,\n            guidance_scale=guidance_scale,\n            negative_prompt=negative_prompt,\n            num_images_per_prompt=num_images_per_prompt,\n            eta=eta,\n            seed=seed,\n            latents=latents,\n            prompt_embeds=prompt_embeds,\n            negative_prompt_embeds=negative_prompt_embeds,\n            output_type=output_type,\n            return_dict=return_dict,\n            callback=callback,\n            callback_steps=callback_steps,\n            cross_attention_kwargs=cross_attention_kwargs,\n            controlnet_conditioning_scale=controlnet_conditioning_scale,\n            guess_mode=guess_mode,\n            control_guidance_start=control_guidance_start,\n            control_guidance_end=control_guidance_end,\n            lora_model_name_or_path_or_dict=lora_model_name_or_path_or_dict,\n            lora_alpha=lora_alpha,\n            text_inversion_model_name_or_path=text_inversion_model_name_or_path,\n            fast_mode=fast_mode\n        )\n    else:\n        images, nsfw_content_detected = controlnet_img2img(\n            sd_model=sd_model,\n            controlnet_model=controlnet_model,\n            prompt=prompt,\n            image=aux_image,\n            control_image=ip2p_image,\n            height=height,\n            width=width,\n            strength=strength,\n            num_inference_steps=num_inference_steps,\n            guidance_scale=guidance_scale,\n            negative_prompt=negative_prompt,\n            num_images_per_prompt=num_images_per_prompt,\n            eta=eta,\n            seed=seed,\n            latents=latents,\n            prompt_embeds=prompt_embeds,\n            negative_prompt_embeds=negative_prompt_embeds,\n            output_type=output_type,\n            return_dict=return_dict,\n            callback=callback,\n            callback_steps=callback_steps,\n            cross_attention_kwargs=cross_attention_kwargs,\n            controlnet_conditioning_scale=controlnet_conditioning_scale,\n            guess_mode=guess_mode,\n            control_guidance_start=control_guidance_start,\n            control_guidance_end=control_guidance_end,\n            lora_model_name_or_path_or_dict=lora_model_name_or_path_or_dict,\n            lora_alpha=lora_alpha,\n            text_inversion_model_name_or_path=text_inversion_model_name_or_path,\n        )\n\n    return images, nsfw_content_detected, ip2p_image\n"}
{"type": "source_file", "path": "frontend/config.py", "content": "NEED_SAFETY = False\nSAMPLER = \"DDIM\"\nMAX_SIZE = 832\nDEBUG = True\nFREE_HW = True\n"}
{"type": "source_file", "path": "modules/__init__.py", "content": ""}
{"type": "source_file", "path": "frontend/main.py", "content": "# -*- coding: UTF-8 -*-\n\n# WebUI\n# , , , tab, tab, tab\n\nimport numpy as np\nimport streamlit as st\nfrom PIL import Image, ImageFilter, PngImagePlugin\nfrom glob import glob\nfrom tqdm import tqdm\nimport pickle\nimport os\nimport cv2\nimport random\nimport time\nimport sys\nfrom io import BytesIO\nfrom config import *\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nos.environ[\"PYTORCH_MUSA_ALLOC_CONF\"] = \"max_split_size_mb:32\"\n\n__dir__ = os.path.dirname(os.path.abspath(__file__))\nsys.path.append(os.path.dirname(__dir__))\n\nimport torch\nimport torch.nn.functional as F\nfrom safetensors.torch import load_file\nfrom diffusers import logging\nlogging.set_verbosity_error()\n\nfrom modules.shared import SDModel, Controlnet, MTGPU_DETECTION, device\nfrom modules.basic.txt2img import txt2img\nfrom modules.basic.img2img import img2img\nfrom modules.controlnet.canny2img import canny2img\nfrom modules.controlnet.pose2img import pose2img\nfrom modules.controlnet.tilehd2img import tilehd2img\nfrom modules.controlnet.mlsd2img import mlsd2img\nfrom modules.tools.image_utils import read_img_as_base64, read_png_info\nfrom modules.tools.prompt_parser import get_lora_from_prompt, contain_zh\nfrom modules.tools.prompt_engineering import get_enhancer, get_artists, get_neg_prompt\nfrom modules.wrappers import *\n\nfrom transformers import CLIPModel\n\n##### maliang #####\n\nargs, btns = {}, {}\n\nargs[\"save_dir\"] = os.path.join(\"frontend\", \"output\")\nos.makedirs(args[\"save_dir\"], exist_ok=True)\n\nargs[\"sd_ckpts\"] = sorted(glob(os.path.join(\"models\", \"Stable-diffusion\", \"*\")))\nargs[\"sd_ckpts\"] = [path.split(os.sep)[-1] for path in args[\"sd_ckpts\"]]\n\nargs[\"text_embeddings\"] = sorted(glob(os.path.join(\"models\", \"embeddings\", \"*.pt\")))\n\nargs[\"lora_names\"] = sorted(glob(os.path.join(\"models\", \"lora\", \"*.safetensors\")))\nargs[\"lora_names\"] = [\n    \"<lora:\" + path.split(os.sep)[-1][:-len(\".safetensors\")] + \":0.8>\"\n    for path in args[\"lora_names\"]\n]\nargs[\"loras\"] = {}\n\nrandom_prompt_path = np.random.choice(\n    glob(os.path.join(\"assets\", \"random_prompts\", \"*\"))\n)\nwith open(random_prompt_path, \"r\") as fr:\n    lines = fr.readlines()\n    args[\"random_prompts\"] = [line.rstrip(\"\\n\") for line in lines]\n\nargs[\"sampler\"] = SAMPLER\n\nargs[\"loaded_loras\"] = os.path.join(\"models\", \"loaded_loras\")\nos.makedirs(args[\"loaded_loras\"], exist_ok=True)\nos.makedirs(os.path.join(args[\"loaded_loras\"], \"fixed\"), exist_ok=True)\nos.makedirs(os.path.join(args[\"loaded_loras\"], \"dynamic\"), exist_ok=True)\nloaded_ckpt_path = os.path.join(args[\"loaded_loras\"], \"loaded_ckpt.txt\")\n\n\n@st.cache_resource(max_entries=1)\ndef load_ckpt():\n    ml = SDModel(device=device, requires_safety_checker=NEED_SAFETY)\n\n    path = os.path.join(\"models\", \"Stable-diffusion\", st.session_state.ckpt_name)\n    ml.load_models(path, diffusers_format=os.path.isdir(path))\n\n    _, _ = txt2img(\n        sd_model=ml,\n        prompt=\"\",\n        height=256,\n        width=256,\n        num_inference_steps=1,\n        guidance_scale=7.5,\n        negative_prompt=None,\n        num_images_per_prompt=1,\n        seed=1234,\n        callback=None,\n        text_inversion_model_name_or_path=args[\"text_embeddings\"],\n        fast_mode=False,\n    )\n\n    with open(loaded_ckpt_path, \"w\") as fw:\n        fw.write(st.session_state.ckpt_name)\n\n    return ml\n\ndef switch_ckpt():\n    ml.offload_models()\n\n    path = os.path.join(\"models\", \"Stable-diffusion\", st.session_state.ckpt_name)\n    ml.load_models(path, diffusers_format=os.path.isdir(path))\n\n    _, _ = txt2img(\n        sd_model=ml,\n        prompt=\"\",\n        height=256,\n        width=256,\n        num_inference_steps=1,\n        guidance_scale=7.5,\n        negative_prompt=None,\n        num_images_per_prompt=1,\n        seed=1234,\n        callback=None,\n        text_inversion_model_name_or_path=args[\"text_embeddings\"],\n        fast_mode=False,\n    )\n\n    with open(loaded_ckpt_path, \"w\") as fw:\n        fw.write(st.session_state.ckpt_name)\n\n@st.cache_resource()\ndef load_controlnet(control_type):\n    net = Controlnet(device=device)\n\n    if control_type == \"canny\":\n        processor = CannyDetectorWrapper()\n        net.load_models(\n            os.path.join(\"models\", \"controlnet\", \"canny_v11\"), diffusers_format=True\n        )\n\n    elif control_type == \"pose\":\n        processor = OpenposeDetectorWrapper(device=device)\n        processor.load_models(os.path.join(\"models\", \"controlnet\", \"annotators\"))\n        net.load_models(\n            os.path.join(\"models\", \"controlnet\", \"openpose_v11\"), diffusers_format=True\n        )\n\n    elif control_type == \"tile\":\n        processor = ZoomConditionalImageWrapper()\n        net.load_models(\n            os.path.join(\"models\", \"controlnet\", \"tile_v11\"), diffusers_format=True\n        )\n\n    elif control_type == 'mlsd':\n        processor = MLSDdetectorWrapper(device=\"cpu\")\n        processor.load_models(os.path.join(\"models\", \"controlnet\", \"annotators\"))\n        net.load_models(\n            os.path.join(\"models\", \"controlnet\", \"mlsd_v11\"), diffusers_format=True\n        )\n\n    return processor, net\n\n\nmlcn_canny, mlcn_pose, mlcn_tile, mlcn_mlsd = None, None, None, None\n\n\n@st.cache_resource()\ndef load_safetensors(safetensors_name):\n    sf = load_file(safetensors_name)\n    return sf\n\n\n@st.cache_resource()\ndef load_translator():\n    ml_translator = Translator(device=\"cpu\")\n    ml_translator.load_models(os.path.join(\"models\", \"tools\", \"zh2en\"))\n    return ml_translator\n\n\nml_translator = None\n\n\n@st.cache_resource()\ndef load_sr():\n    ml_sr = RealESRGANWrapper(device=device)\n    ml_sr.load_models(os.path.join(\"models\", \"tools\", \"RealESRGAN_x4plus.pth\"))\n    return ml_sr\n\n\nml_sr = None\n\n\n@st.cache_resource()\ndef load_tagger():\n    ml_tagger = DeepDanbooruWrapper(device=device)\n    ml_tagger.load_models(os.path.join(\"models\", \"tools\", \"model-resnet_custom_v3.pt\"))\n    return ml_tagger\n\n\nml_tagger = None\n\n\n@st.cache_resource()\ndef load_enhancer():\n    with open(os.path.join(\"assets\", \"enhancer.pkl\"), \"rb\") as fr:\n        data_enhancer = pickle.load(fr)\n\n    with open(os.path.join(\"assets\", \"artist.txt\"), \"r\") as fr:\n        lines = fr.readlines()\n        artists_list = [line.rstrip(\"\\n\").split(\"^\")[0] for line in lines]\n\n    return [data_enhancer, artists_list]\n\n\nml_enhancer = load_enhancer()\n\n\n@st.cache_resource\ndef load_dynamic_loras():\n    dynamic_loras_dir = os.path.join(\"models\", \"tools\", \"dynamic_loras\")\n\n    if os.path.exists(dynamic_loras_dir):\n        ml_clip = CLIPModel.from_pretrained(os.path.join(\"models\", \"tools\", \"clip-vit-large-patch14\"))\n        ml_cc = np.load(os.path.join(dynamic_loras_dir, \"centroids.npy\"))\n        ml_loras_paths = sorted(glob(os.path.join(dynamic_loras_dir, \"lora\", \"*.safetensors\")))\n    else:\n        ml_clip, ml_cc, ml_loras_paths = None, None, None\n\n    return ml_clip, ml_cc, ml_loras_paths\n\nml_clip, ml_cc, ml_loras_paths = load_dynamic_loras()\n\n\n##### style #####\n\nst.markdown(\n    \"\"\"\n    <style type='text/css'>\n    .css-pxxe24 {\n        visibility: hidden;\n    }\n    div[data-testid='stTickBar'] {\n        display: none;\n    }\n    div.block-container {\n        padding-bottom: 50px;\n    }\n\n    section[data-testid='stSidebar'] {\n        width: 25% !important;\n        min-width: 356px !important;\n    }\n    section[data-testid='stSidebar'] div.block-container {\n        padding-bottom: 0px;\n    }\n    section[data-testid='stSidebar'] > div:first-child > div:nth-child(1) {\n        display: none;\n    }\n    section[data-testid='stSidebar'] > div:first-child > div:nth-child(2) {\n        padding: 1.8rem;\n    }\n    section[data-testid='stSidebar'] > div:first-child > div:nth-child(2) > div:first-child > div:first-child > div:first-child > div:nth-child(1) p {\n        margin-top: -10px;\n    }\n    section[data-testid='stSidebar'] > div:first-child > div:nth-child(2) > div:first-child > div:first-child > div:first-child > div:nth-child(1) div[role='progressbar'] {\n        margin-top: 10px;\n        margin-bottom: 20px;\n    }\n    section[data-testid='stSidebar'] > div:first-child > div:nth-child(2) > div:first-child > div:first-child > div:first-child > div:nth-child(2),\n    section[data-testid='stSidebar'] > div:first-child > div:nth-child(2) > div:first-child > div:first-child > div:first-child > div:nth-child(3) {\n\n    }\n    section[data-testid='stSidebar'] > div:first-child > div:nth-child(2) > div:first-child > div:first-child > div:first-child > div:nth-child(2) > div[data-testid='stVerticalBlock'] div[data-testid='stImage'],\n    section[data-testid='stSidebar'] > div:first-child > div:nth-child(2) > div:first-child > div:first-child > div:first-child > div:nth-child(3) > div[data-testid='stVerticalBlock'] div[data-testid='stImage'] {\n        width: 100%;\n    }\n    section[data-testid='stSidebar'] > div:first-child > div:nth-child(2) > div:first-child > div:first-child > div:first-child > div:nth-child(2) > div[data-testid='stVerticalBlock'] button[title='View fullscreen'],\n    section[data-testid='stSidebar'] > div:first-child > div:nth-child(2) > div:first-child > div:first-child > div:first-child > div:nth-child(3) > div[data-testid='stVerticalBlock'] button[title='View fullscreen'] {\n        display: none;\n    }\n    section[data-testid='stSidebar'] > div:first-child > div:nth-child(2) > div:first-child > div:first-child > div:first-child > div:nth-child(2) > div[data-testid='stVerticalBlock'] button[kind='secondary'],\n    section[data-testid='stSidebar'] > div:first-child > div:nth-child(2) > div:first-child > div:first-child > div:first-child > div:nth-child(3) > div[data-testid='stVerticalBlock'] button[kind='secondary'],\n    div.st-bc > div:nth-child(5) div.stDownloadButton button[kind='secondary'] {\n        position: absolute;\n        top: -60px;\n        right: 7px;\n        background-color: rgba(13, 17, 23, 0.4);\n        width: 80px;\n        border-radius: 4px;\n    }\n    section[data-testid='stSidebar'] > div:first-child > div:nth-child(2) > div:first-child > div:first-child > div:first-child > div:nth-child(2) > div[data-testid='stVerticalBlock'] button[kind='secondary']:hover,\n    section[data-testid='stSidebar'] > div:first-child > div:nth-child(2) > div:first-child > div:first-child > div:first-child > div:nth-child(3) > div[data-testid='stVerticalBlock'] button[kind='secondary']:hover,\n    div.st-bc > div:nth-child(5) div.stDownloadButton button[kind='secondary']:hover {\n        color: rgb(250, 250, 250);\n        background-color: #FF671D;\n        background-opacity: 1;\n    }\n\n\n    section.main > div.block-container > div:first-child > div > div.element-container button[title='View fullscreen'] {\n        display: none;\n    }\n    section.main > div.block-container > div:first-child > div > div.element-container img {\n        width: 50%;\n        margin-left: 30%;\n        margin-bottom: 15px;\n    }\n    div.st-bc > div:nth-child(1) div[data-baseweb='tab-list'] button:first-child {\n        margin-left: 33%;\n        margin-right: 4%;\n    }\n    div.st-bc > div:nth-child(1) div[data-baseweb='tab-list'] button:nth-child(2) {\n        margin-right: 4%;\n    }\n\n\n    div.st-bc > div:nth-child(3) > div > div > div:nth-child(2) button {\n        width: 100%;\n    }\n    div.st-bc > div:nth-child(3) > div > div > div:nth-child(2) > div:nth-child(3) button {\n        border-color: #FF671D;\n    }\n    div.st-bc > div:nth-child(3) > div > div > div:nth-child(2) > div:nth-child(3) button:hover {\n        color: rgb(250, 250, 250);\n        background-color: #FF671D;\n    }\n    div.st-bc > div:nth-child(3) button[title='View fullscreen'] {\n        display: none;\n    }\n\n\n    .image_cell {\n        width: 160px;\n        margin: 10px 20px 0px 0px;\n        border-radius: 6px;\n        border: 2px solid #0E1117;\n        opacity: 0.7;\n        display: inline-block;\n        position: relative;\n    }\n    div.st-bc > div:nth-child(4) div[data-testid='stHorizontalBlock'] div[data-testid='stVerticalBlock'] div[data-testid='stVerticalBlock']:hover .image_cell {\n        opacity: 1;\n        border-color: #FF671D;\n        cursor: pointer;\n    }\n    .image_cell img {\n        width:100%;\n        border-radius:4px;\n    }\n    div.st-bc > div:nth-child(4) div[data-testid='stHorizontalBlock'] div[data-testid='stVerticalBlock'] div[data-testid='stVerticalBlock'] {\n        margin-bottom: -80px;\n    }\n    div.st-bc > div:nth-child(4) div[data-testid='stHorizontalBlock'] div[data-testid='stVerticalBlock'] div[data-testid='stVerticalBlock'] div.element-container {\n        margin-bottom: -16px;\n    }\n    div.st-bc > div:nth-child(4) div.stButton button[kind='secondary'] {\n        position: relative;\n        background-color: rgba(13, 17, 23, 0.4);\n        opacity: 0;\n        border-radius: 4px;\n        padding: 0 10px 0 10px;\n    }\n    div.st-bc > div:nth-child(4) div[data-testid='stHorizontalBlock'] div[data-testid='stVerticalBlock'] div[data-testid='stVerticalBlock'] div.element-container:nth-child(1) {\n        margin-bottom: -24px;\n    }\n    div.st-bc > div:nth-child(4) div[data-testid='stHorizontalBlock'] div[data-testid='stVerticalBlock'] div[data-testid='stVerticalBlock'] div.element-container:nth-child(2) div.stButton button[kind='secondary'] {\n        right: -7px;\n        top: -22px;\n    }\n    div.st-bc > div:nth-child(4) div[data-testid='stHorizontalBlock'] div[data-testid='stVerticalBlock'] div[data-testid='stVerticalBlock'] div.element-container:nth-child(3) div.stButton button[kind='secondary'] {\n        right: -57px;\n        top: -60px;\n        visibility: hidden;\n    }\n    div.st-bc > div:nth-child(4) div[data-testid='stHorizontalBlock'] div[data-testid='stVerticalBlock'] div[data-testid='stVerticalBlock'] div.element-container:nth-child(4) div.stButton button[kind='secondary'] {\n        right: -107px;\n        top: -98px;\n    }\n    div.st-bc > div:nth-child(4) div[data-testid='stHorizontalBlock'] div[data-testid='stVerticalBlock'] div[data-testid='stVerticalBlock']:hover div.stButton button[kind='secondary'] {\n        opacity: 1;\n    }\n    div.st-bc > div:nth-child(4) div.stButton button[kind='secondary']:hover {\n        color: rgb(250, 250, 250);\n        background-color: #FF671D;\n        background-opacity: 1;\n    }\n    div.st-bc > div:nth-child(4) div.stButton button[kind='secondary'] p {\n        font-size: 12px;\n    }\n    div.st-bc > div:nth-child(4) > div > div > div:nth-child(2) {\n        margin-top: 20px;\n    }\n    div.st-bc > div:nth-child(4) > div > div > div:nth-child(2) > div:nth-child(3) p {\n        margin-top: 7px;\n    }\n    \n\n    div.st-bc > div:nth-child(5) div[data-testid='stExpander']:nth-child(1) div.element-container:nth-child(3),\n    div.st-bc > div:nth-child(5) div[data-testid='stExpander']:nth-child(1) div.element-container:nth-child(5) {\n        margin-bottom: -16px;\n    }\n\n    button[title=\"View fullscreen\"] {\n        right: 2px;\n        top: 2px;\n        color: #FFF;\n        background-color: #FF671D;\n    }\n\n    footer {\n        text-align: center;\n    }\n    </style>\n    \"\"\",\n    unsafe_allow_html=True,\n)\n\n##### header #####\n\nst.image(Image.open(os.path.join(\"assets\", \"imgs\", \"banner.png\")), caption=\"\")\n\ntab1, tab2, tab3 = st.tabs([\"\\t:pencil2:\", \"\\t:city_sunrise:\", \"\\t:lollipop:\"])\n\n\n#####  #####\n\nwith tab1:\n    prompt_container = st.container()\n    col1, col2, col3 = st.columns([4, 1, 1])\n\n    with col1:\n        args[\"negative_prompt\"] = st.text_input(\n            label=\"\",\n            value=\"\",\n            placeholder=\"\",\n            label_visibility=\"collapsed\",\n        )\n\n    with col2:\n        btns[\"choose_random_prompt\"] = st.button(\"\", on_click=None)\n        if btns[\"choose_random_prompt\"]:\n            st.session_state.random_prompt = np.random.choice(args[\"random_prompts\"])\n\n    with col3:\n        btns[\"start_generation\"] = st.button(\" \", on_click=None)\n\n    with prompt_container:\n        args[\"prompt\"] = st.text_area(\n            label=\"\",\n            value=st.session_state.random_prompt\n            if \"random_prompt\" in st.session_state\n            else \"\",\n            placeholder=\"\",\n            label_visibility=\"collapsed\",\n            height=160,\n        )\n\n    with st.container():\n        col1, col2 = st.columns([1, 1])\n        with col1:\n            if os.path.exists(loaded_ckpt_path):\n                with open(loaded_ckpt_path, \"r\") as fr:\n                    lines = fr.readlines()\n\n                    if len(lines) > 0:\n                        line = lines[0]\n                    else:\n                        line = args[\"sd_ckpts\"][0] \n            else:\n                line = args[\"sd_ckpts\"][0]\n\n            args[\"model\"] = st.selectbox(\n                label=\"\", options=args[\"sd_ckpts\"], index=args[\"sd_ckpts\"].index(line), on_change=switch_ckpt, key=\"ckpt_name\"\n            )\n            ml = load_ckpt()\n\n            if FREE_HW:\n                args[\"height\"] = st.slider(label=\"\", min_value=384, max_value=MAX_SIZE, value=512, step=64)\n                st.write('\\n')\n\n        with col2:\n            args[\"num_images\"] = st.number_input(\n                label=\"\", min_value=1, max_value=4, value=1, step=1, on_change=None\n            )\n\n            if FREE_HW:\n                args[\"width\"] = st.slider(label=\"\", min_value=384, max_value=MAX_SIZE, value=512, step=64)\n                st.write('\\n')\n\n        if not FREE_HW:\n            args[\"hw\"] = st.radio(\n                \" ( x )\",\n                (\"512 x 512\", \"768 x 512\", \"512 x 768\", \"576 x 576\", \"640 x 640\"),\n                index=0,\n                horizontal=True,\n            )\n            args[\"hw\"] = args[\"hw\"].split(\" x \")\n            args[\"height\"] = int(args[\"hw\"][0])\n            args[\"width\"] = int(args[\"hw\"][1])\n\n    col1, col2 = st.columns([1, 2])\n    with col1:\n        with st.expander(\"****\\t:rocket:\", expanded=False):\n            args[\"steps\"] = st.slider(\n                label=\"20\", min_value=20, max_value=30, value=20, step=2\n            )\n            st.write(\"\\n\")\n            args[\"cfg\"] = st.slider(\n                label=\"7.5\", min_value=5.0, max_value=10.0, value=7.5, step=0.5\n            )\n            st.write(\"\\n\")\n\n            if \"last_seed\" not in st.session_state:\n                st.session_state.last_seed = \"-1\"\n\n            seed_radio = st.radio(\"\", (\"\", \"\"), index=0, horizontal=True)\n            args[\"seed\"] = st.text_input(\n                \"-1\",\n                value=\"-1\" if seed_radio == \"\" else st.session_state.last_seed,\n                on_change=None,\n                placeholder=\"\",\n            )\n            args[\"use_enhance\"] = st.checkbox(\"\", value=True)\n            args[\"use_negative_enhance\"] = st.checkbox(\"\", value=True)\n            args[\"use_fast_mode\"] = st.checkbox(\"\", value=True)\n\n    with col2:\n        with st.expander(\"****\\t:cityscape:\", expanded=False):\n            args[\"init_image\"] = st.file_uploader(\n                \"\", type=[\"png\", \"jpg\", \"jpeg\"], on_change=None\n            )\n\n            if args[\"init_image\"] is not None:\n                args[\"init_image\"] = np.asarray(\n                    bytearray(args[\"init_image\"].read()), dtype=np.uint8\n                )\n                args[\"init_image\"] = cv2.imdecode(args[\"init_image\"], 1)[:, :, ::-1]\n                args[\"init_image\"] = Image.fromarray(\n                    args[\"init_image\"].astype(\"uint8\")\n                ).convert(\"RGB\")\n\n                w, h = args[\"init_image\"].size\n                if max(w, h) > MAX_SIZE:\n                    if h > w:\n                        args[\"init_image\"] = args[\"init_image\"].resize(\n                            (int(MAX_SIZE * w / h), MAX_SIZE)\n                        )\n                    else:\n                        args[\"init_image\"] = args[\"init_image\"].resize(\n                            (MAX_SIZE, int(MAX_SIZE * h / w))\n                        )\n\n                args[\"use_init_image\"] = True\n                st.image(args[\"init_image\"])\n\n                args[\"denoising_strength\"] = st.slider(\n                    label=\"\", min_value=0.1, max_value=1.0, value=0.8, step=0.1\n                )\n                st.write(\"\\n\")\n            else:\n                args[\"use_init_image\"] = False\n\n            args[\"controlnet_type\"] = st.selectbox(\n                label=\"\", options=[\"\", \"\", \"\", \"\"], index=0, on_change=None\n            )\n\n            if args[\"controlnet_type\"] != \"\":\n                args[\"controlnet_image\"] = st.file_uploader(\n                    \"\", type=[\"png\", \"jpg\", \"jpeg\"], on_change=None\n                )\n\n                if args[\"controlnet_image\"] is not None:\n                    args[\"controlnet_image\"] = np.asarray(\n                        bytearray(args[\"controlnet_image\"].read()), dtype=np.uint8\n                    )\n                    args[\"controlnet_image\"] = cv2.imdecode(\n                        args[\"controlnet_image\"], 1\n                    )[:, :, ::-1]\n                    args[\"controlnet_image\"] = Image.fromarray(\n                        args[\"controlnet_image\"].astype(\"uint8\")\n                    ).convert(\"RGB\")\n                    st.image(args[\"controlnet_image\"])\n\n    with st.expander(\"**LORA**\\t:fire:\", expanded=False):\n        st.caption(\"lora\")\n        for lora in args[\"lora_names\"]:\n            st.code(lora)\n\n        args['use_dynamic_lora'] = st.checkbox('', value=True)\n        if args['use_dynamic_lora']:\n            args['dynamic_mode'] = st.radio('', ('', '', '', ''), index=0, horizontal=True, label_visibility='collapsed')\n\nskip_layers_none = []\nskip_layers_some = [\n    \"lora_unet_mid_block_attentions_0\",\n    \"lora_unet_up_blocks_1_attentions_0\",\n    \"lora_unet_up_blocks_1_attentions_1\",\n    \"lora_unet_up_blocks_1_attentions_2\",\n]\nskip_layers_more = [\n    \"lora_unet_down_blocks_2_attentions_0\",\n    \"lora_unet_down_blocks_2_attentions_1\",\n    \"lora_unet_mid_block_attentions_0\",\n    \"lora_unet_up_blocks_1_attentions_0\",\n    \"lora_unet_up_blocks_1_attentions_1\",\n    \"lora_unet_up_blocks_1_attentions_2\",\n    \"lora_unet_up_blocks_2_attentions_0\",\n    \"lora_unet_up_blocks_2_attentions_1\",\n    \"lora_unet_up_blocks_2_attentions_2\",\n]\n\nif args[\"use_dynamic_lora\"]:\n    if args[\"dynamic_mode\"] == \"\":\n        if args[\"model\"].find(\"ml-person\") >= 0:\n            dynamic_skip_layers = skip_layers_more\n        elif args[\"model\"].find(\"ml-2.5D\") >= 0:\n            dynamic_skip_layers = skip_layers_some\n        else:\n            dynamic_skip_layers = skip_layers_none\n    elif args[\"dynamic_mode\"] == \"\":\n        dynamic_skip_layers = skip_layers_none\n    elif args[\"dynamic_mode\"] == \"\":\n        dynamic_skip_layers = skip_layers_some\n    elif args[\"dynamic_mode\"] == \"\":\n        dynamic_skip_layers = skip_layers_more\n\n##### sidebar #####\n\n\ndef progress_callback(step, timestep, image, total_timestep=1000):\n    timestep = timestep.to(\"cpu\")\n    percentage = (total_timestep - timestep + 1) / total_timestep\n    pr_bar.progress(\n        int(percentage * 100), text=\":smiley:\\t{:.2f}%\".format(percentage * 100)\n    )\n    return step, timestep, image\n\n\ndef get_dynamic_lora_from_prompt(prompt, N=3):\n    if ml_clip is None:\n        return []\n\n    inputs = ml.tokenizer([prompt], padding=True, return_tensors='pt')\n    if inputs['input_ids'].shape[1] > 77:\n        sos = torch.tensor(49406) # inputs['input_ids'][0][0]\n        eos = torch.tensor(49407) # inputs['input_ids'][0][-1]\n        inputs['input_ids'] = inputs['input_ids'][:, :77]\n        inputs['input_ids'][:, -1] = eos\n        inputs['attention_mask'] = inputs['attention_mask'][:, :77]\n        inputs['attention_mask'][:, -1] = 0\n\n    text_features = ml_clip.get_text_features(**inputs).data.numpy()\n\n    similarity = F.cosine_similarity(torch.tensor(text_features), torch.tensor(ml_cc), dim=1).data.numpy()\n    index = np.argsort(similarity)[::-1]\n\n    idx = index[:N]\n    alphas = [similarity[i] for i in idx]\n\n    start, end = 0.2, 0.6\n    alphas = [(a - alphas[-1]) / (alphas[0] - alphas[-1]) * (end - start) + start for a in alphas]\n\n    s = np.sum(alphas)\n    alphas = [round(r / s, 3) for r in alphas]\n\n    return [[ml_loras_paths[idx[i]], alphas[i]] for i in range(len(idx))]\n\n\ndef load_loras(loras, dynamic=False, skip_layers=[]):\n    for lora in loras:\n        name, alpha = lora\n\n        if dynamic:\n            lora_name = name.split(os.sep)[-1][:-len(\".safetensors\")]\n            lora_path = os.path.join(args[\"loaded_loras\"], \"dynamic\", f\"{lora_name}.pkl\")\n\n            if not os.path.exists(lora_path):\n                ml.load_lora(\n                    name, alpha=alpha, is_path=True, skip_layers=skip_layers\n                )\n\n                with open(lora_path, \"wb\") as fw:\n                    pickle.dump([alpha, skip_layers], fw)\n        else:\n            path = os.path.join(\"models\", \"lora\", name + \".safetensors\")\n            lora_path = os.path.join(args[\"loaded_loras\"], \"fixed\", f\"{name}.pkl\")\n            \n            if os.path.exists(path):\n                if name not in args[\"loras\"]:\n                    args[\"loras\"][name] = load_safetensors(path)\n\n                if not os.path.exists(lora_path):\n                    ml.load_lora(\n                        args[\"loras\"][name],\n                        alpha=alpha,\n                        is_path=False,\n                        skip_layers=skip_layers,\n                    )\n\n                    with open(lora_path, \"wb\") as fw:\n                        pickle.dump(alpha, fw)\n\n\ndef offload_loras(loras, dynamic=False, skip_layers=[]):\n    for lora in loras:\n        name, alpha = lora\n\n        if dynamic:\n            lora_name = name.split(os.sep)[-1][:-len(\".safetensors\")]\n            lora_path = os.path.join(args[\"loaded_loras\"], \"dynamic\", f\"{lora_name}.pkl\")\n\n            if os.path.exists(lora_path):\n                ml.offload_lora(\n                    name, alpha=alpha, is_path=True, skip_layers=skip_layers\n                )\n                os.system(f\"rm {lora_path}\")\n        else:\n            path = os.path.join(\"models\", \"lora\", name + \".safetensors\")\n            lora_path = os.path.join(args[\"loaded_loras\"], \"fixed\", f\"{name}.pkl\")\n            \n            if os.path.exists(path):\n                if name not in args[\"loras\"]:\n                    args[\"loras\"][name] = load_safetensors(path)\n                \n                if os.path.exists(lora_path):\n                    ml.offload_lora(\n                        args[\"loras\"][name],\n                        alpha=alpha,\n                        is_path=False,\n                        skip_layers=skip_layers,\n                    )\n                    os.system(f\"rm {lora_path}\")\n\n\ndef enhance_prompt(prompt, seed, use_artist=False):\n    enhancer = get_enhancer(ml_enhancer[0], seed)\n    prompt += \", \" + enhancer\n\n    if use_artist:\n        artist = get_artists(ml_enhancer[1], seed)\n        if artist:\n            prompt += \", (\" + artist + \":0.6)\"\n\n    return prompt\n\n\ndef enhance_negative_prompt(neg):\n    safety_neg_prompt, neg_embeddings_prompt, quality_neg_prompt = get_neg_prompt()\n\n    if args[\"model\"].find(\"ml-general\") >= 0:\n        neg_embeddings_prompt = \"\"\n\n    if neg == \"\":\n        neg = safety_neg_prompt + neg_embeddings_prompt + quality_neg_prompt\n    else:\n        neg = (\n            safety_neg_prompt + neg + \", \" + neg_embeddings_prompt + quality_neg_prompt\n        )\n    return neg\n\n\nwith st.sidebar:\n    msg = st.container()\n    msg.write(\" \")\n\n    res = st.container()\n\n    if btns[\"start_generation\"]:\n        if args[\"prompt\"] == \"\":\n            msg.write(\":slightly_frowning_face:\\t\")\n        else:\n            pr_bar = msg.progress(0, text=\":smiley:\\t\")\n\n            if contain_zh(args[\"prompt\"]):\n                if ml_translator is None:\n                    ml_translator = load_translator()\n\n                args[\"prompt\"] = ml_translator(args[\"prompt\"])\n\n            if contain_zh(args[\"negative_prompt\"]):\n                args[\"negative_prompt\"] = ml_translator(args[\"negative_prompt\"])\n\n            if args[\"seed\"] in [\"-1\", \"0\"]:\n                use_seed = random.randrange(4294967294)\n            else:\n                try:\n                    use_seed = int(args[\"seed\"])\n                except:\n                    use_seed = random.randrange(4294967294)\n\n            st.session_state.last_seed = str(use_seed)\n            st.session_state.last_images = []\n\n            # lora\n            fixed_loras, prompt = get_lora_from_prompt(args[\"prompt\"])\n            load_loras(fixed_loras)\n\n            if args['use_dynamic_lora']:\n                dynamic_loras = get_dynamic_lora_from_prompt(prompt)\n                load_loras(dynamic_loras, dynamic=True, skip_layers=dynamic_skip_layers)\n\n            negative_prompt = enhance_negative_prompt(args[\"negative_prompt\"]) if args[\"use_negative_enhance\"] else args[\"negative_prompt\"]\n\n            ori_prompt = prompt\n            for i in range(args[\"num_images\"]):\n                if args[\"use_enhance\"]:\n                    prompt = enhance_prompt(ori_prompt, use_seed + i)\n\n                if args[\"controlnet_type\"] == \"\" or args[\"controlnet_image\"] is None:\n                    if not args[\"use_init_image\"]:\n                        images, status = txt2img(\n                            sd_model=ml,\n                            prompt=prompt,\n                            height=args[\"height\"],\n                            width=args[\"width\"],\n                            num_inference_steps=args[\"steps\"],\n                            guidance_scale=args[\"cfg\"],\n                            negative_prompt=negative_prompt,\n                            num_images_per_prompt=1,\n                            seed=use_seed + i,\n                            callback=progress_callback,\n                            fast_mode=args[\"use_fast_mode\"],\n                        )\n                    else:\n                        images, status = img2img(\n                            sd_model=ml,\n                            prompt=prompt,\n                            image=args[\"init_image\"],\n                            strength=args[\"denoising_strength\"],\n                            num_inference_steps=args[\"steps\"],\n                            guidance_scale=args[\"cfg\"],\n                            negative_prompt=negative_prompt,\n                            num_images_per_prompt=1,\n                            seed=use_seed + i,\n                            callback=progress_callback,\n                        )\n\n                    anno_image = None\n\n                else:\n                    if args[\"use_init_image\"]:\n                        w, h = args[\"init_image\"].size\n                        if w == args[\"width\"] and h == args[\"height\"]:\n                            also_use_init_image = True\n                        else:\n                            also_use_init_image = False\n                    else:\n                        also_use_init_image = False\n\n                    if args[\"controlnet_type\"] == \"\":\n                        if mlcn_canny is None:\n                            mlcn_canny = load_controlnet(\"canny\")\n\n                        mlcn_canny[1].controlnet = mlcn_canny[1].controlnet.to(device)\n\n                        images, status, anno_image = canny2img(\n                            sd_model=ml,\n                            controlnet_model=mlcn_canny[1],\n                            processor=mlcn_canny[0],\n                            ori_image=args[\"controlnet_image\"],\n                            aux_image=args[\"init_image\"]\n                            if also_use_init_image\n                            else None,\n                            is_canny_image=False,\n                            prompt=prompt,\n                            height=args[\"height\"],\n                            width=args[\"width\"],\n                            strength=1.0,\n                            num_inference_steps=args[\"steps\"],\n                            guidance_scale=args[\"cfg\"],\n                            negative_prompt=negative_prompt,\n                            num_images_per_prompt=1,\n                            seed=use_seed + i,\n                            callback=progress_callback,\n                            fast_mode=args[\"use_fast_mode\"],\n                        )\n\n                        mlcn_canny[1].controlnet = mlcn_canny[1].controlnet.to(\"cpu\")\n\n                    elif args[\"controlnet_type\"] == \"\":\n                        if mlcn_pose is None:\n                            mlcn_pose = load_controlnet(\"pose\")\n\n                        mlcn_pose[1].controlnet = mlcn_pose[1].controlnet.to(device)\n\n                        images, status, anno_image = pose2img(\n                            sd_model=ml,\n                            controlnet_model=mlcn_pose[1],\n                            processor=mlcn_pose[0],\n                            ori_image=args[\"controlnet_image\"],\n                            aux_image=args[\"init_image\"]\n                            if also_use_init_image\n                            else None,\n                            is_pose_image=False,\n                            prompt=prompt,\n                            height=args[\"height\"],\n                            width=args[\"width\"],\n                            strength=1.0,\n                            num_inference_steps=args[\"steps\"],\n                            guidance_scale=args[\"cfg\"],\n                            negative_prompt=negative_prompt,\n                            num_images_per_prompt=1,\n                            seed=use_seed + i,\n                            callback=progress_callback,\n                            fast_mode=args[\"use_fast_mode\"],\n                        )\n\n                        mlcn_pose[1].controlnet = mlcn_pose[1].controlnet.to(\"cpu\")\n\n                    elif args[\"controlnet_type\"] == \"\":\n                        if mlcn_mlsd is None:\n                            mlcn_mlsd = load_controlnet(\"mlsd\")\n\n                        mlcn_mlsd[1].controlnet = mlcn_mlsd[1].controlnet.to(device)\n\n                        images, status, anno_image = mlsd2img(\n                            sd_model=ml,\n                            controlnet_model=mlcn_mlsd[1],\n                            processor=mlcn_mlsd[0],\n                            ori_image=args[\"controlnet_image\"],\n                            aux_image=args[\"init_image\"]\n                            if also_use_init_image\n                            else None,\n                            is_mlsd_image=False,\n                            prompt=prompt,\n                            height=args[\"height\"],\n                            width=args[\"width\"],\n                            strength=1.0,\n                            num_inference_steps=args[\"steps\"],\n                            guidance_scale=args[\"cfg\"],\n                            negative_prompt=negative_prompt,\n                            num_images_per_prompt=1,\n                            seed=use_seed + i,\n                            callback=progress_callback,\n                            fast_mode=args[\"use_fast_mode\"],\n                        )\n\n                        mlcn_mlsd[1].controlnet = mlcn_mlsd[1].controlnet.to(\"cpu\")\n\n                image = images[0]\n\n                # nsfw\n                if status:\n                    image = image.filter(\n                        ImageFilter.GaussianBlur(\n                            radius=max(args[\"height\"], args[\"width\"]) // 8\n                        )\n                    )\n\n                res.image(image)\n\n                save_name = (\n                    str(int(time.time()))\n                    + \"-\"\n                    + str(np.random.randint(0, 10e4)).zfill(4)\n                    + \".png\"\n                )\n\n                pnginfo_data = PngImagePlugin.PngInfo()\n                value = args[\"prompt\"] + \"\\n\"\n                value += \"Negative prompt: \" + args[\"negative_prompt\"] + \"\\n\"\n                value += (\n                    \"Steps: \"\n                    + str(args[\"steps\"])\n                    + \", Sampler: \"\n                    + args[\"sampler\"]\n                    + \", \"\n                )\n                value += \"CFG scale: \" + str(args[\"cfg\"]) + \", \"\n                value += \"Seed: \" + str(use_seed + i) + \", \"\n                value += (\n                    \"Size: \" + str(args[\"width\"]) + \"x\" + str(args[\"height\"]) + \", \"\n                )\n                value += \"Model: \" + args[\"model\"]\n                pnginfo = {\"parameters\": value}\n\n                for k, v in pnginfo.items():\n                    pnginfo_data.add_text(k, str(v))\n\n                save_path = os.path.join(args[\"save_dir\"], save_name)\n                image.save(save_path, pnginfo=pnginfo_data)\n                st.session_state.last_images.append([image.copy(), save_name])\n\n                w, h = image.size\n                image.thumbnail((w // 2, h // 2))\n                image.save(\n                    os.path.join(args[\"save_dir\"], save_name.rstrip(\".png\") + \".jpg\")\n                )\n\n                with open(save_path, \"rb\") as fr:\n                    res.download_button(\n                        label=\"\", data=fr, file_name=save_name, mime=\"image/png\"\n                    )\n\n            if anno_image is not None:\n                res.image(anno_image)\n\n            pr_bar.empty()\n\n    elif \"last_images\" in st.session_state:\n        pr_bar = msg.progress(0, text=\":smiley:\\t\")\n        pr_bar.empty()\n\n        last_images = st.session_state.last_images\n\n        for item in last_images:\n            image, save_name = item\n\n            res.image(image)\n\n            save_path = os.path.join(args[\"save_dir\"], save_name)\n\n            if os.path.exists(save_path):\n                with open(save_path, \"rb\") as fr:\n                    res.download_button(\n                        label=\"\", data=fr, file_name=save_name, mime=\"image/png\"\n                    )\n            else:\n                buffered = BytesIO()\n                image.save(buffered, format=\"png\")\n                res.download_button(\n                    label=\"\",\n                    data=buffered.getvalue(),\n                    file_name=save_name,\n                    mime=\"image/png\",\n                )\n\n#####  #####\n\nwith tab2:\n    paths = sorted(glob(os.path.join(args[\"save_dir\"], \"*.jpg\")))[::-1]\n\n    num_image_per_page = 20\n    num_of_columns = 4\n    total_page = len(paths) // num_image_per_page\n    if len(paths) % num_image_per_page > 0:\n        total_page += 1\n\n    gallery = st.container()\n    col1, col2, col3, col4 = gallery.columns([1, 1, 1, 1])\n    columns = [col1, col2, col3, col4]\n\n    if total_page > 1:\n        col1, col2, col3 = st.columns([3, 4, 4])\n        with col2:\n            args[\"current_page\"] = st.number_input(\n                label=\"\",\n                min_value=1,\n                max_value=total_page,\n                value=1,\n                step=1,\n                on_change=None,\n                label_visibility=\"collapsed\",\n            )\n        with col3:\n            st.write(f\" {total_page} \")\n    else:\n        args[\"current_page\"] = 1\n\n    for i, path in enumerate(\n        paths[\n            (args[\"current_page\"] - 1)\n            * num_image_per_page : args[\"current_page\"]\n            * num_image_per_page\n        ]\n    ):\n        img = read_img_as_base64(path)\n\n        funcs = columns[i % num_of_columns].container()\n\n        funcs.write(\n            f\"\"\"\n            <div class='image_cell'>\n                <img src='{img}'>\n            </div>\n            \"\"\",\n            unsafe_allow_html=True,\n        )\n\n        name = path.split(os.sep)[-1].rstrip(\".jpg\")\n\n        tiled_btn = funcs.button(\"\", key=f\"hd_{name}\", on_click=None)\n\n        if tiled_btn:\n            pr_bar = msg.progress(0, text=\":smiley:\\t\")\n\n            st.session_state.last_images = []\n\n            save_name = name + \"_tiled.png\"\n            save_path = os.path.join(args[\"save_dir\"], save_name)\n\n            image = Image.open(os.path.join(args[\"save_dir\"], f\"{name}.png\"))\n            params = read_png_info(image)\n\n            # lora\n            fixed_loras, prompt = get_lora_from_prompt(params[\"prompt\"])\n            load_loras(fixed_loras)\n\n            if args['use_dynamic_lora']:\n                dynamic_loras = get_dynamic_lora_from_prompt(prompt)\n                load_loras(dynamic_loras, dynamic=True, skip_layers=dynamic_skip_layers)\n\n            negative_prompt = enhance_negative_prompt(params[\"negative_prompt\"]) if args[\"use_negative_enhance\"] else params[\"negative_prompt\"]\n\n            if args[\"use_enhance\"]:\n                prompt = enhance_prompt(prompt, params[\"seed\"])\n\n            if mlcn_tile is None:\n                mlcn_tile = load_controlnet(\"tile\")\n\n            mlcn_tile[1].controlnet = mlcn_tile[1].controlnet.to(mlcn_tile[1].device)\n\n            images, status, anno_image = tilehd2img(\n                sd_model=ml,\n                controlnet_model=mlcn_tile[1],\n                processor=mlcn_tile[0],\n                ori_image=image,\n                is_zoom_image=False,\n                up_sampling_ratio=2,\n                prompt=prompt,\n                # height=params[\"height\"],\n                # width=params[\"width\"],\n                strength=0.3,\n                num_inference_steps=params[\"steps\"],\n                guidance_scale=params[\"cfg\"],\n                negative_prompt=negative_prompt,\n                num_images_per_prompt=1,\n                seed=params[\"seed\"],\n                callback=progress_callback,\n            )\n\n            mlcn_tile[1].controlnet = mlcn_tile[1].controlnet.to(\"cpu\")\n\n            image = images[0]\n\n            # nsfw\n            if status:\n                image = image.filter(\n                    ImageFilter.GaussianBlur(\n                        radius=max(params[\"height\"], params[\"width\"]) // 8\n                    )\n                )\n\n            image.save(save_path)\n            st.session_state.last_images.append([image.copy(), save_name])\n\n            res.image(image)\n\n            with open(save_path, 'rb') as fr:\n                res.download_button(label='', data=fr, key=save_name, file_name=save_name, mime='image/png')\n\n            pr_bar.empty()\n            st.experimental_rerun()\n\n        face_btn = funcs.button(\"\", key=f\"face_{name}\", on_click=None)\n\n        del_btn = funcs.button(\"\", key=f\"del_{name}\", on_click=None)\n        if del_btn:\n            del_path = os.path.join(args[\"save_dir\"], name)\n            os.system(f\"rm {del_path}.png\")\n            os.system(f\"rm {del_path}.jpg\")\n\n            del_hd_path = f\"{del_path}_tiled.png\"\n            if os.path.exists(del_hd_path):\n                os.system(f\"rm {del_hd_path}\")\n\n            st.experimental_rerun()\n\n#####  #####\n\n\ndef record_download_hd():\n    st.session_state.hd_image_has_download = True\n\n\nwith tab3:\n    with st.expander(\"**X4**\\t:rainbow:\", expanded=True):\n        args[\"hd_image\"] = st.file_uploader(\n            \"\",\n            key=\"upload_hd_image\",\n            type=[\"png\", \"jpg\", \"jpeg\"],\n            on_change=None,\n            label_visibility=\"collapsed\",\n        )\n        if args[\"hd_image\"] is not None:\n            filename = args[\"hd_image\"].name\n            args[\"hd_image\"] = np.asarray(\n                bytearray(args[\"hd_image\"].read()), dtype=np.uint8\n            )\n            args[\"hd_image\"] = cv2.imdecode(args[\"hd_image\"], 1)[:, :, ::-1]\n            args[\"hd_image\"] = Image.fromarray(\n                args[\"hd_image\"].astype(\"uint8\")\n            ).convert(\"RGB\")\n\n            if (\n                \"last_hd_image_name\" in st.session_state\n                and st.session_state.last_hd_image_name == filename\n            ):\n                hd_image = st.session_state.last_hd_image\n            else:\n                with st.spinner(text=\":smiley:\\t\"):\n                    if ml_sr is None:\n                        ml_sr = load_sr()\n\n                    hd_image = ml_sr(args[\"hd_image\"])\n                    st.session_state.last_hd_image = hd_image\n                    st.session_state.last_hd_image_name = filename\n                    st.session_state.hd_image_has_download = False\n\n            if (\n                \"hd_image_has_download\" in st.session_state\n                and not st.session_state.hd_image_has_download\n            ):\n                args[\"hd_image\"] = args[\"hd_image\"].resize(hd_image.size)\n                combine = np.concatenate(\n                    [np.array(args[\"hd_image\"]), np.array(hd_image)], 1\n                )\n                combine = Image.fromarray(combine.astype(\"uint8\")).convert(\"RGB\")\n                st.image(combine)\n\n                buf = BytesIO()\n                hd_image.save(buf, format=\"PNG\")\n                save_name = \"hd_\" + filename[: filename.rfind(\".\")] + \".png\"\n                st.download_button(\n                    label=\"\",\n                    data=buf.getvalue(),\n                    file_name=save_name,\n                    mime=\"image/png\",\n                    on_click=record_download_hd,\n                )\n\n        else:\n            st.session_state.last_hd_image = None\n            st.session_state.last_hd_image_name = \"\"\n            st.session_state.hd_image_has_download = False\n\n    with st.expander(\"****\\t:pencil2:\", expanded=True):\n        args[\"tag_image\"] = st.file_uploader(\n            \"\",\n            key=\"upload_tag_image\",\n            type=[\"png\", \"jpg\", \"jpeg\"],\n            on_change=None,\n            label_visibility=\"collapsed\",\n        )\n        if args[\"tag_image\"] is not None:\n            args[\"tag_image\"] = np.asarray(\n                bytearray(args[\"tag_image\"].read()), dtype=np.uint8\n            )\n            args[\"tag_image\"] = cv2.imdecode(args[\"tag_image\"], 1)[:, :, ::-1]\n            args[\"tag_image\"] = Image.fromarray(\n                args[\"tag_image\"].astype(\"uint8\")\n            ).convert(\"RGB\")\n\n            if ml_tagger is None:\n                ml_tagger = load_tagger()\n\n            caption = ml_tagger(args[\"tag_image\"])\n\n            st.code(caption)\n\n# lora\npaths = sorted(glob(os.path.join(args[\"loaded_loras\"], \"fixed\", \"*\")))\nfixed_loras = []\nfor path in paths:\n    with open(path, \"rb\") as fr:\n        alpha = pickle.load(fr)\n        fixed_loras.append([path.split(os.sep)[-1][:-len(\".pkl\")], alpha])\noffload_loras(fixed_loras)\n\npaths = sorted(glob(os.path.join(args[\"loaded_loras\"], \"dynamic\", \"*\")))\ndynamic_loras = []\nskip_layers = []\nfor path in paths:\n    with open(path, 'rb') as fr:\n        alpha, skip_layers = pickle.load(fr)\n        dynamic_loras.append([os.path.join(\"models\", \"tools\", \"dynamic_loras\", \"lora\",\n            path.split(os.sep)[-1][:-len('.pkl')] + \".safetensors\"), alpha])\n\noffload_loras(dynamic_loras, dynamic=True, skip_layers=skip_layers)"}
{"type": "source_file", "path": "modules/controlnet/pose2img.py", "content": "import torch\nfrom typing import Any, Callable, Dict, List, Optional, Union\nfrom PIL import Image\nimport os\nimport sys\n\n__dir__ = os.path.dirname(os.path.abspath(__file__))\nsys.path.append(os.path.dirname(os.path.dirname(__dir__)))\nfrom modules.shared import SDModel, Controlnet\nfrom modules.controlnet.controlnet_txt2img import controlnet_txt2img\nfrom modules.controlnet.controlnet_img2img import controlnet_img2img\nfrom modules.wrappers import OpenposeDetectorWrapper\n\n\ndef pose2img(\n    sd_model: SDModel,\n    controlnet_model: Controlnet,\n    processor: OpenposeDetectorWrapper,\n    ori_image: Image,\n    aux_image: Image = None,\n    is_pose_image: bool = False,\n    include_hand: bool = True,\n    include_face: bool = True,\n    prompt: Union[str, List[str]] = None,\n    height: Optional[int] = None,\n    width: Optional[int] = None,\n    strength: float = 0.8,\n    num_inference_steps: int = 20,\n    guidance_scale: float = 7.5,\n    negative_prompt: Optional[Union[str, List[str]]] = None,\n    num_images_per_prompt: Optional[int] = 1,\n    eta: float = 0.0,\n    seed: Optional[Union[int, List[int]]] = None,\n    latents: Optional[torch.FloatTensor] = None,\n    prompt_embeds: Optional[torch.FloatTensor] = None,\n    negative_prompt_embeds: Optional[torch.FloatTensor] = None,\n    output_type: Optional[str] = \"pil\",\n    return_dict: bool = True,\n    callback: Optional[Callable[[int, int, torch.FloatTensor], None]] = None,\n    callback_steps: int = 1,\n    cross_attention_kwargs: Optional[Dict[str, Any]] = None,\n    controlnet_conditioning_scale: Union[float, List[float]] = 0.8,\n    guess_mode: bool = False,\n    control_guidance_start: Union[float, List[float]] = 0.0,\n    control_guidance_end: Union[float, List[float]] = 1.0,\n    lora_model_name_or_path_or_dict: Optional[Union[str, List[str]]] = None,\n    lora_alpha: Optional[float] = 0.75,\n    text_inversion_model_name_or_path: Optional[\n        Union[str, List[str], Dict[str, torch.Tensor], List[Dict[str, torch.Tensor]]]\n    ] = None,\n    fast_mode = False\n):\n\n    if is_pose_image:\n        pose_image = ori_image\n    else:\n        pose_image = processor(\n            input_image=ori_image, include_hand=include_hand, include_face=include_face\n        )\n\n    if aux_image is None:\n        images, nsfw_content_detected = controlnet_txt2img(\n            sd_model=sd_model,\n            controlnet_model=controlnet_model,\n            prompt=prompt,\n            image=pose_image,\n            height=height,\n            width=width,\n            num_inference_steps=num_inference_steps,\n            guidance_scale=guidance_scale,\n            negative_prompt=negative_prompt,\n            num_images_per_prompt=num_images_per_prompt,\n            eta=eta,\n            seed=seed,\n            latents=latents,\n            prompt_embeds=prompt_embeds,\n            negative_prompt_embeds=negative_prompt_embeds,\n            output_type=output_type,\n            return_dict=return_dict,\n            callback=callback,\n            callback_steps=callback_steps,\n            cross_attention_kwargs=cross_attention_kwargs,\n            controlnet_conditioning_scale=controlnet_conditioning_scale,\n            guess_mode=guess_mode,\n            control_guidance_start=control_guidance_start,\n            control_guidance_end=control_guidance_end,\n            lora_model_name_or_path_or_dict=lora_model_name_or_path_or_dict,\n            lora_alpha=lora_alpha,\n            text_inversion_model_name_or_path=text_inversion_model_name_or_path,\n            fast_mode=fast_mode\n        )\n    else:\n        images, nsfw_content_detected = controlnet_img2img(\n            sd_model=sd_model,\n            controlnet_model=controlnet_model,\n            prompt=prompt,\n            image=aux_image,\n            control_image=pose_image,\n            height=height,\n            width=width,\n            strength=strength,\n            num_inference_steps=num_inference_steps,\n            guidance_scale=guidance_scale,\n            negative_prompt=negative_prompt,\n            num_images_per_prompt=num_images_per_prompt,\n            eta=eta,\n            seed=seed,\n            latents=latents,\n            prompt_embeds=prompt_embeds,\n            negative_prompt_embeds=negative_prompt_embeds,\n            output_type=output_type,\n            return_dict=return_dict,\n            callback=callback,\n            callback_steps=callback_steps,\n            cross_attention_kwargs=cross_attention_kwargs,\n            controlnet_conditioning_scale=controlnet_conditioning_scale,\n            guess_mode=guess_mode,\n            control_guidance_start=control_guidance_start,\n            control_guidance_end=control_guidance_end,\n            lora_model_name_or_path_or_dict=lora_model_name_or_path_or_dict,\n            lora_alpha=lora_alpha,\n            text_inversion_model_name_or_path=text_inversion_model_name_or_path,\n        )\n\n    return images, nsfw_content_detected, pose_image\n"}
{"type": "source_file", "path": "modules/controlnet/lineart2img.py", "content": "import torch\nfrom typing import Any, Callable, Dict, List, Optional, Union\nfrom PIL import Image\nimport os\nimport sys\n\n__dir__ = os.path.dirname(os.path.abspath(__file__))\nsys.path.append(os.path.dirname(os.path.dirname(__dir__)))\nfrom modules.shared import SDModel, Controlnet\nfrom modules.controlnet.controlnet_txt2img import controlnet_txt2img\nfrom modules.controlnet.controlnet_img2img import controlnet_img2img\nfrom modules.wrappers import LineartDetectorWrapper\n\n\ndef lineart2img(\n    sd_model: SDModel,\n    controlnet_model: Controlnet,\n    processor: LineartDetectorWrapper,\n    ori_image: Image,\n    aux_image: Image = None,\n    is_lineart_image: bool = False,\n    coarse: bool = False,\n    detect_resolution: int = 512,\n    image_resolution: int = 512,\n    lineart_output_type: str = \"pil\",\n    prompt: Union[str, List[str]] = None,\n    height: Optional[int] = None,\n    width: Optional[int] = None,\n    strength: float = 0.8,\n    num_inference_steps: int = 20,\n    guidance_scale: float = 7.5,\n    negative_prompt: Optional[Union[str, List[str]]] = None,\n    num_images_per_prompt: Optional[int] = 1,\n    eta: float = 0.0,\n    seed: Optional[Union[int, List[int]]] = None,\n    latents: Optional[torch.FloatTensor] = None,\n    prompt_embeds: Optional[torch.FloatTensor] = None,\n    negative_prompt_embeds: Optional[torch.FloatTensor] = None,\n    output_type: Optional[str] = \"pil\",\n    return_dict: bool = True,\n    callback: Optional[Callable[[int, int, torch.FloatTensor], None]] = None,\n    callback_steps: int = 1,\n    cross_attention_kwargs: Optional[Dict[str, Any]] = None,\n    controlnet_conditioning_scale: Union[float, List[float]] = 0.8,\n    guess_mode: bool = False,\n    control_guidance_start: Union[float, List[float]] = 0.0,\n    control_guidance_end: Union[float, List[float]] = 1.0,\n    lora_model_name_or_path_or_dict: Optional[Union[str, List[str]]] = None,\n    lora_alpha: Optional[float] = 0.75,\n    text_inversion_model_name_or_path: Optional[\n        Union[str, List[str], Dict[str, torch.Tensor], List[Dict[str, torch.Tensor]]]\n    ] = None,\n    fast_mode = False\n):\n\n    if is_lineart_image:\n        lineart_image = ori_image\n    else:\n        lineart_image = processor(\n            input_image=ori_image,\n            coarse=coarse,\n            detect_resolution=detect_resolution,\n            image_resolution=image_resolution,\n            output_type=lineart_output_type,\n        )\n\n    if aux_image is None:\n        images, nsfw_content_detected = controlnet_txt2img(\n            sd_model=sd_model,\n            controlnet_model=controlnet_model,\n            prompt=prompt,\n            image=lineart_image,\n            height=height,\n            width=width,\n            num_inference_steps=num_inference_steps,\n            guidance_scale=guidance_scale,\n            negative_prompt=negative_prompt,\n            num_images_per_prompt=num_images_per_prompt,\n            eta=eta,\n            seed=seed,\n            latents=latents,\n            prompt_embeds=prompt_embeds,\n            negative_prompt_embeds=negative_prompt_embeds,\n            output_type=output_type,\n            return_dict=return_dict,\n            callback=callback,\n            callback_steps=callback_steps,\n            cross_attention_kwargs=cross_attention_kwargs,\n            controlnet_conditioning_scale=controlnet_conditioning_scale,\n            guess_mode=guess_mode,\n            control_guidance_start=control_guidance_start,\n            control_guidance_end=control_guidance_end,\n            lora_model_name_or_path_or_dict=lora_model_name_or_path_or_dict,\n            lora_alpha=lora_alpha,\n            text_inversion_model_name_or_path=text_inversion_model_name_or_path,\n            fast_mode=fast_mode\n        )\n    else:\n        images, nsfw_content_detected = controlnet_img2img(\n            sd_model=sd_model,\n            controlnet_model=controlnet_model,\n            prompt=prompt,\n            image=aux_image,\n            control_image=lineart_image,\n            height=height,\n            width=width,\n            strength=strength,\n            num_inference_steps=num_inference_steps,\n            guidance_scale=guidance_scale,\n            negative_prompt=negative_prompt,\n            num_images_per_prompt=num_images_per_prompt,\n            eta=eta,\n            seed=seed,\n            latents=latents,\n            prompt_embeds=prompt_embeds,\n            negative_prompt_embeds=negative_prompt_embeds,\n            output_type=output_type,\n            return_dict=return_dict,\n            callback=callback,\n            callback_steps=callback_steps,\n            cross_attention_kwargs=cross_attention_kwargs,\n            controlnet_conditioning_scale=controlnet_conditioning_scale,\n            guess_mode=guess_mode,\n            control_guidance_start=control_guidance_start,\n            control_guidance_end=control_guidance_end,\n            lora_model_name_or_path_or_dict=lora_model_name_or_path_or_dict,\n            lora_alpha=lora_alpha,\n            text_inversion_model_name_or_path=text_inversion_model_name_or_path,\n        )\n\n    return images, nsfw_content_detected, lineart_image\n"}
{"type": "source_file", "path": "modules/controlnet/scribble2img.py", "content": "import torch\nfrom typing import Any, Callable, Dict, List, Optional, Union\nfrom PIL import Image\nimport os\nimport sys\n\n__dir__ = os.path.dirname(os.path.abspath(__file__))\nsys.path.append(os.path.dirname(os.path.dirname(__dir__)))\nfrom modules.shared import SDModel, Controlnet\nfrom modules.controlnet.controlnet_txt2img import controlnet_txt2img\nfrom modules.controlnet.controlnet_img2img import controlnet_img2img\nfrom modules.wrappers import HEDdetectorWrapper\n\n\ndef scribble2img(\n    sd_model: SDModel,\n    controlnet_model: Controlnet,\n    processor: HEDdetectorWrapper,\n    ori_image: Image,\n    aux_image: Image = None,\n    is_scribble_image: bool = False,\n    detect_resolution: int = 512,\n    image_resolution: int = 512,\n    safe: bool = False,\n    scribble_output_type: str = \"pil\",\n    scribble: bool = False,\n    prompt: Union[str, List[str]] = None,\n    height: Optional[int] = None,\n    width: Optional[int] = None,\n    strength: float = 0.8,\n    num_inference_steps: int = 20,\n    guidance_scale: float = 7.5,\n    negative_prompt: Optional[Union[str, List[str]]] = None,\n    num_images_per_prompt: Optional[int] = 1,\n    eta: float = 0.0,\n    seed: Optional[Union[int, List[int]]] = None,\n    latents: Optional[torch.FloatTensor] = None,\n    prompt_embeds: Optional[torch.FloatTensor] = None,\n    negative_prompt_embeds: Optional[torch.FloatTensor] = None,\n    output_type: Optional[str] = \"pil\",\n    return_dict: bool = True,\n    callback: Optional[Callable[[int, int, torch.FloatTensor], None]] = None,\n    callback_steps: int = 1,\n    cross_attention_kwargs: Optional[Dict[str, Any]] = None,\n    controlnet_conditioning_scale: Union[float, List[float]] = 0.8,\n    guess_mode: bool = False,\n    control_guidance_start: Union[float, List[float]] = 0.0,\n    control_guidance_end: Union[float, List[float]] = 1.0,\n    lora_model_name_or_path_or_dict: Optional[Union[str, List[str]]] = None,\n    lora_alpha: Optional[float] = 0.75,\n    text_inversion_model_name_or_path: Optional[\n        Union[str, List[str], Dict[str, torch.Tensor], List[Dict[str, torch.Tensor]]]\n    ] = None,\n    fast_mode = False\n):\n\n    if is_scribble_image:\n        scribble_image = ori_image\n    else:\n        scribble_image = processor(\n            input_image=ori_image,\n            detect_resolution=detect_resolution,\n            image_resolution=image_resolution,\n            output_type=scribble_output_type,\n            safe=safe,\n            scribble=scribble,\n        )\n\n    if aux_image is None:\n        images, nsfw_content_detected = controlnet_txt2img(\n            sd_model=sd_model,\n            controlnet_model=controlnet_model,\n            prompt=prompt,\n            image=scribble_image,\n            height=height,\n            width=width,\n            num_inference_steps=num_inference_steps,\n            guidance_scale=guidance_scale,\n            negative_prompt=negative_prompt,\n            num_images_per_prompt=num_images_per_prompt,\n            eta=eta,\n            seed=seed,\n            latents=latents,\n            prompt_embeds=prompt_embeds,\n            negative_prompt_embeds=negative_prompt_embeds,\n            output_type=output_type,\n            return_dict=return_dict,\n            callback=callback,\n            callback_steps=callback_steps,\n            cross_attention_kwargs=cross_attention_kwargs,\n            controlnet_conditioning_scale=controlnet_conditioning_scale,\n            guess_mode=guess_mode,\n            control_guidance_start=control_guidance_start,\n            control_guidance_end=control_guidance_end,\n            lora_model_name_or_path_or_dict=lora_model_name_or_path_or_dict,\n            lora_alpha=lora_alpha,\n            text_inversion_model_name_or_path=text_inversion_model_name_or_path,\n            fast_mode=fast_mode\n        )\n    else:\n        images, nsfw_content_detected = controlnet_img2img(\n            sd_model=sd_model,\n            controlnet_model=controlnet_model,\n            prompt=prompt,\n            image=aux_image,\n            control_image=scribble_image,\n            height=height,\n            width=width,\n            strength=strength,\n            num_inference_steps=num_inference_steps,\n            guidance_scale=guidance_scale,\n            negative_prompt=negative_prompt,\n            num_images_per_prompt=num_images_per_prompt,\n            eta=eta,\n            seed=seed,\n            latents=latents,\n            prompt_embeds=prompt_embeds,\n            negative_prompt_embeds=negative_prompt_embeds,\n            output_type=output_type,\n            return_dict=return_dict,\n            callback=callback,\n            callback_steps=callback_steps,\n            cross_attention_kwargs=cross_attention_kwargs,\n            controlnet_conditioning_scale=controlnet_conditioning_scale,\n            guess_mode=guess_mode,\n            control_guidance_start=control_guidance_start,\n            control_guidance_end=control_guidance_end,\n            lora_model_name_or_path_or_dict=lora_model_name_or_path_or_dict,\n            lora_alpha=lora_alpha,\n            text_inversion_model_name_or_path=text_inversion_model_name_or_path,\n        )\n\n    return images, nsfw_content_detected, scribble_image\n"}
{"type": "source_file", "path": "modules/controlnet/softedge2img.py", "content": "import torch\nfrom typing import Any, Callable, Dict, List, Optional, Union\nfrom PIL import Image\nimport os\nimport sys\n\n__dir__ = os.path.dirname(os.path.abspath(__file__))\nsys.path.append(os.path.dirname(os.path.dirname(__dir__)))\nfrom modules.shared import SDModel, Controlnet\nfrom modules.controlnet.controlnet_txt2img import controlnet_txt2img\nfrom modules.controlnet.controlnet_img2img import controlnet_img2img\nfrom modules.wrappers import PidiNetDetectorWrapper\n\n\ndef softedge2img(\n    sd_model: SDModel,\n    controlnet_model: Controlnet,\n    processor: PidiNetDetectorWrapper,\n    ori_image: Image,\n    aux_image: Image = None,\n    is_softedge_image: bool = False,\n    detect_resolution: int = 512,\n    image_resolution: int = 512,\n    safe: bool = False,\n    softedge_output_type: str = \"pil\",\n    scribble: bool = False,\n    apply_filter: bool = False,\n    prompt: Union[str, List[str]] = None,\n    height: Optional[int] = None,\n    width: Optional[int] = None,\n    strength: float = 0.8,\n    num_inference_steps: int = 20,\n    guidance_scale: float = 7.5,\n    negative_prompt: Optional[Union[str, List[str]]] = None,\n    num_images_per_prompt: Optional[int] = 1,\n    eta: float = 0.0,\n    seed: Optional[Union[int, List[int]]] = None,\n    latents: Optional[torch.FloatTensor] = None,\n    prompt_embeds: Optional[torch.FloatTensor] = None,\n    negative_prompt_embeds: Optional[torch.FloatTensor] = None,\n    output_type: Optional[str] = \"pil\",\n    return_dict: bool = True,\n    callback: Optional[Callable[[int, int, torch.FloatTensor], None]] = None,\n    callback_steps: int = 1,\n    cross_attention_kwargs: Optional[Dict[str, Any]] = None,\n    controlnet_conditioning_scale: Union[float, List[float]] = 0.8,\n    guess_mode: bool = False,\n    control_guidance_start: Union[float, List[float]] = 0.0,\n    control_guidance_end: Union[float, List[float]] = 1.0,\n    lora_model_name_or_path_or_dict: Optional[Union[str, List[str]]] = None,\n    lora_alpha: Optional[float] = 0.75,\n    text_inversion_model_name_or_path: Optional[\n        Union[str, List[str], Dict[str, torch.Tensor], List[Dict[str, torch.Tensor]]]\n    ] = None,\n    fast_mode = False\n):\n\n    if is_softedge_image:\n        softedge_image = ori_image\n    else:\n        softedge_image = processor(\n            input_image=ori_image,\n            detect_resolution=detect_resolution,\n            image_resolution=image_resolution,\n            output_type=softedge_output_type,\n            safe=safe,\n            scribble=scribble,\n            apply_filter=apply_filter,\n        )\n\n    if aux_image is None:\n        images, nsfw_content_detected = controlnet_txt2img(\n            sd_model=sd_model,\n            controlnet_model=controlnet_model,\n            prompt=prompt,\n            image=softedge_image,\n            height=height,\n            width=width,\n            num_inference_steps=num_inference_steps,\n            guidance_scale=guidance_scale,\n            negative_prompt=negative_prompt,\n            num_images_per_prompt=num_images_per_prompt,\n            eta=eta,\n            seed=seed,\n            latents=latents,\n            prompt_embeds=prompt_embeds,\n            negative_prompt_embeds=negative_prompt_embeds,\n            output_type=output_type,\n            return_dict=return_dict,\n            callback=callback,\n            callback_steps=callback_steps,\n            cross_attention_kwargs=cross_attention_kwargs,\n            controlnet_conditioning_scale=controlnet_conditioning_scale,\n            guess_mode=guess_mode,\n            control_guidance_start=control_guidance_start,\n            control_guidance_end=control_guidance_end,\n            lora_model_name_or_path_or_dict=lora_model_name_or_path_or_dict,\n            lora_alpha=lora_alpha,\n            text_inversion_model_name_or_path=text_inversion_model_name_or_path,\n            fast_mode=fast_mode\n        )\n    else:\n        images, nsfw_content_detected = controlnet_img2img(\n            sd_model=sd_model,\n            controlnet_model=controlnet_model,\n            prompt=prompt,\n            image=aux_image,\n            control_image=softedge_image,\n            height=height,\n            width=width,\n            strength=strength,\n            num_inference_steps=num_inference_steps,\n            guidance_scale=guidance_scale,\n            negative_prompt=negative_prompt,\n            num_images_per_prompt=num_images_per_prompt,\n            eta=eta,\n            seed=seed,\n            latents=latents,\n            prompt_embeds=prompt_embeds,\n            negative_prompt_embeds=negative_prompt_embeds,\n            output_type=output_type,\n            return_dict=return_dict,\n            callback=callback,\n            callback_steps=callback_steps,\n            cross_attention_kwargs=cross_attention_kwargs,\n            controlnet_conditioning_scale=controlnet_conditioning_scale,\n            guess_mode=guess_mode,\n            control_guidance_start=control_guidance_start,\n            control_guidance_end=control_guidance_end,\n            lora_model_name_or_path_or_dict=lora_model_name_or_path_or_dict,\n            lora_alpha=lora_alpha,\n            text_inversion_model_name_or_path=text_inversion_model_name_or_path,\n        )\n\n    return images, nsfw_content_detected, softedge_image\n"}
{"type": "source_file", "path": "modules/controlnet/shuffle2img.py", "content": "import torch\nfrom typing import Any, Callable, Dict, List, Optional, Union\nfrom PIL import Image\nimport os\nimport sys\n\n__dir__ = os.path.dirname(os.path.abspath(__file__))\nsys.path.append(os.path.dirname(os.path.dirname(__dir__)))\nfrom modules.shared import SDModel, Controlnet\nfrom modules.controlnet.controlnet_txt2img import controlnet_txt2img\nfrom modules.controlnet.controlnet_img2img import controlnet_img2img\nfrom modules.wrappers import ContentShuffleDetectorWrapper\n\n\ndef shuffle2img(\n    sd_model: SDModel,\n    controlnet_model: Controlnet,\n    processor: ContentShuffleDetectorWrapper,\n    ori_image: Image,\n    aux_image: Image = None,\n    is_shuffle_image: bool = False,\n    detect_resolution: int = 512,\n    image_resolution: int = 512,\n    shuffle_output_type: str = \"pil\",\n    prompt: Union[str, List[str]] = None,\n    height: Optional[int] = None,\n    width: Optional[int] = None,\n    strength: float = 0.8,\n    num_inference_steps: int = 20,\n    guidance_scale: float = 7.5,\n    negative_prompt: Optional[Union[str, List[str]]] = None,\n    num_images_per_prompt: Optional[int] = 1,\n    eta: float = 0.0,\n    seed: Optional[Union[int, List[int]]] = None,\n    latents: Optional[torch.FloatTensor] = None,\n    prompt_embeds: Optional[torch.FloatTensor] = None,\n    negative_prompt_embeds: Optional[torch.FloatTensor] = None,\n    output_type: Optional[str] = \"pil\",\n    return_dict: bool = True,\n    callback: Optional[Callable[[int, int, torch.FloatTensor], None]] = None,\n    callback_steps: int = 1,\n    cross_attention_kwargs: Optional[Dict[str, Any]] = None,\n    controlnet_conditioning_scale: Union[float, List[float]] = 0.8,\n    guess_mode: bool = False,\n    control_guidance_start: Union[float, List[float]] = 0.0,\n    control_guidance_end: Union[float, List[float]] = 1.0,\n    lora_model_name_or_path_or_dict: Optional[Union[str, List[str]]] = None,\n    lora_alpha: Optional[float] = 0.75,\n    text_inversion_model_name_or_path: Optional[\n        Union[str, List[str], Dict[str, torch.Tensor], List[Dict[str, torch.Tensor]]]\n    ] = None,\n    fast_mode = False\n):\n\n    if is_shuffle_image:\n        shuffle_image = ori_image\n    else:\n        shuffle_image = processor(\n            input_image=ori_image,\n            detect_resolution=detect_resolution,\n            image_resolution=image_resolution,\n            output_type=shuffle_output_type,\n        )\n\n    if aux_image is None:\n        images, nsfw_content_detected = controlnet_txt2img(\n            sd_model=sd_model,\n            controlnet_model=controlnet_model,\n            prompt=prompt,\n            image=shuffle_image,\n            height=height,\n            width=width,\n            num_inference_steps=num_inference_steps,\n            guidance_scale=guidance_scale,\n            negative_prompt=negative_prompt,\n            num_images_per_prompt=num_images_per_prompt,\n            eta=eta,\n            seed=seed,\n            latents=latents,\n            prompt_embeds=prompt_embeds,\n            negative_prompt_embeds=negative_prompt_embeds,\n            output_type=output_type,\n            return_dict=return_dict,\n            callback=callback,\n            callback_steps=callback_steps,\n            cross_attention_kwargs=cross_attention_kwargs,\n            controlnet_conditioning_scale=controlnet_conditioning_scale,\n            guess_mode=guess_mode,\n            control_guidance_start=control_guidance_start,\n            control_guidance_end=control_guidance_end,\n            lora_model_name_or_path_or_dict=lora_model_name_or_path_or_dict,\n            lora_alpha=lora_alpha,\n            text_inversion_model_name_or_path=text_inversion_model_name_or_path,\n            fast_mode=fast_mode\n        )\n    else:\n        images, nsfw_content_detected = controlnet_img2img(\n            sd_model=sd_model,\n            controlnet_model=controlnet_model,\n            prompt=prompt,\n            image=aux_image,\n            control_image=shuffle_image,\n            height=height,\n            width=width,\n            strength=strength,\n            num_inference_steps=num_inference_steps,\n            guidance_scale=guidance_scale,\n            negative_prompt=negative_prompt,\n            num_images_per_prompt=num_images_per_prompt,\n            eta=eta,\n            seed=seed,\n            latents=latents,\n            prompt_embeds=prompt_embeds,\n            negative_prompt_embeds=negative_prompt_embeds,\n            output_type=output_type,\n            return_dict=return_dict,\n            callback=callback,\n            callback_steps=callback_steps,\n            cross_attention_kwargs=cross_attention_kwargs,\n            controlnet_conditioning_scale=controlnet_conditioning_scale,\n            guess_mode=guess_mode,\n            control_guidance_start=control_guidance_start,\n            control_guidance_end=control_guidance_end,\n            lora_model_name_or_path_or_dict=lora_model_name_or_path_or_dict,\n            lora_alpha=lora_alpha,\n            text_inversion_model_name_or_path=text_inversion_model_name_or_path,\n        )\n\n    return images, nsfw_content_detected, shuffle_image\n"}
{"type": "source_file", "path": "examples/controlnets/example_tilehd2img.py", "content": "import torch\nfrom PIL import Image\nimport os\nimport sys\nimport argparse\n\n__dir__ = os.path.dirname(os.path.abspath(__file__))\nsys.path.append(os.path.dirname(os.path.dirname(__dir__)))\nfrom modules.shared import SDModel, Controlnet, device\nfrom modules.wrappers import ZoomConditionalImageWrapper\nfrom modules.controlnet.tilehd2img import tilehd2img\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument(\n        \"--checkpoint_path\",\n        default=None,\n        type=str,\n        required=True,\n        help=\"Path to the stable diffusion model.\",\n    )\n\n    parser.add_argument(\n        \"--tile_controlnet_path\",\n        default=None,\n        type=str,\n        required=True,\n        help=\"Path to the tile controlnet model.\",\n    )\n\n    parser.add_argument(\n        \"--image_path\", default=None, type=str, required=True, help=\"The image path\"\n    )\n\n    parser.add_argument(\n        \"--prompt\",\n        default=None,\n        type=str,\n        required=True,\n        help=\"Describe the image content with text\",\n    )\n\n    parser.add_argument(\n        \"--output_path\",\n        default=\"./\",\n        type=str,\n        required=True,\n        help=\"Result folder path\",\n    )\n\n    parser.add_argument(\n        \"--negative_prompt\",\n        default=None,\n        type=str,\n        required=False,\n        help=\"Content which you don't want to show up with text\",\n    )\n\n    parser.add_argument(\n        \"--diffusers_format\",\n        action=\"store_true\",\n        required=False,\n        help=\"Model files in diffusers format\",\n    )\n    parser.add_argument(\n        \"--seed\", default=2023, required=False, type=int, help=\"Random seed\"\n    )\n\n    args = parser.parse_args()\n\n    image = Image.open(args.image_path).convert(\"RGB\").resize((512, 512))\n    sd_model = SDModel(device=device)\n    controlnet_model = Controlnet(device=sd_model.device)\n    processor = ZoomConditionalImageWrapper()\n    sd_model.load_models(args.checkpoint_path, diffusers_format=args.diffusers_format)\n    controlnet_model.load_models(args.tile_controlnet_path, diffusers_format=True)\n    \n    images, _, zoom_image = tilehd2img(\n        sd_model, \n        controlnet_model, \n        processor, \n        prompt=args.prompt, \n        negative_prompt=args.negative_prompt,\n        ori_image=image, \n        up_sampling_ratio=2,\n        window_size=80,\n        strength=0.35,\n    )\n    images[0].save(\"outputs/tilehd2img.png\")\n    zoom_image.save(\"outputs/zoom.png\")\n    \n    # python examples/controlnets/example_tilehd2img.py --checkpoint_path models/Stable-diffusion/v1-5-pruned-emaonly.ckpt --tile_controlnet_path models/controlnet/tile_v11 --image_path data/person.jpg --prompt \"(best quality, ultra detailed, master piece, sharp:1.3)\" --output_path outputs\n"}
{"type": "source_file", "path": "modules/controlnet/mlsd2img.py", "content": "import torch\nfrom typing import Any, Callable, Dict, List, Optional, Union\nfrom PIL import Image\nimport os\nimport sys\n\n__dir__ = os.path.dirname(os.path.abspath(__file__))\nsys.path.append(os.path.dirname(os.path.dirname(__dir__)))\nfrom modules.shared import SDModel, Controlnet\nfrom modules.controlnet.controlnet_txt2img import controlnet_txt2img\nfrom modules.controlnet.controlnet_img2img import controlnet_img2img\nfrom modules.wrappers import MLSDdetectorWrapper\n\n\ndef mlsd2img(\n    sd_model: SDModel,\n    controlnet_model: Controlnet,\n    processor: MLSDdetectorWrapper,\n    ori_image: Image,\n    aux_image: Image = None,\n    is_mlsd_image: bool = False,\n    thr_v: float = 0.1,\n    thr_d: float = 0.1,\n    detect_resolution: int = 512,\n    image_resolution: int = 512,\n    mlsd_output_type: str = \"pil\",\n    prompt: Union[str, List[str]] = None,\n    height: Optional[int] = None,\n    width: Optional[int] = None,\n    strength: float = 0.8,\n    num_inference_steps: int = 20,\n    guidance_scale: float = 7.5,\n    negative_prompt: Optional[Union[str, List[str]]] = None,\n    num_images_per_prompt: Optional[int] = 1,\n    eta: float = 0.0,\n    seed: Optional[Union[int, List[int]]] = None,\n    latents: Optional[torch.FloatTensor] = None,\n    prompt_embeds: Optional[torch.FloatTensor] = None,\n    negative_prompt_embeds: Optional[torch.FloatTensor] = None,\n    output_type: Optional[str] = \"pil\",\n    return_dict: bool = True,\n    callback: Optional[Callable[[int, int, torch.FloatTensor], None]] = None,\n    callback_steps: int = 1,\n    cross_attention_kwargs: Optional[Dict[str, Any]] = None,\n    controlnet_conditioning_scale: Union[float, List[float]] = 0.8,\n    guess_mode: bool = False,\n    control_guidance_start: Union[float, List[float]] = 0.0,\n    control_guidance_end: Union[float, List[float]] = 1.0,\n    lora_model_name_or_path_or_dict: Optional[Union[str, List[str]]] = None,\n    lora_alpha: Optional[float] = 0.75,\n    text_inversion_model_name_or_path: Optional[\n        Union[str, List[str], Dict[str, torch.Tensor], List[Dict[str, torch.Tensor]]]\n    ] = None,\n    fast_mode = False\n):\n\n    if is_mlsd_image:\n        mlsd_image = ori_image\n    else:\n        mlsd_image = processor(\n            input_image=ori_image,\n            thr_v=thr_v,\n            thr_d=thr_d,\n            detect_resolution=detect_resolution,\n            image_resolution=image_resolution,\n            output_type=mlsd_output_type,\n        )\n\n    if aux_image is None:\n        images, nsfw_content_detected = controlnet_txt2img(\n            sd_model=sd_model,\n            controlnet_model=controlnet_model,\n            prompt=prompt,\n            image=mlsd_image,\n            height=height,\n            width=width,\n            num_inference_steps=num_inference_steps,\n            guidance_scale=guidance_scale,\n            negative_prompt=negative_prompt,\n            num_images_per_prompt=num_images_per_prompt,\n            eta=eta,\n            seed=seed,\n            latents=latents,\n            prompt_embeds=prompt_embeds,\n            negative_prompt_embeds=negative_prompt_embeds,\n            output_type=output_type,\n            return_dict=return_dict,\n            callback=callback,\n            callback_steps=callback_steps,\n            cross_attention_kwargs=cross_attention_kwargs,\n            controlnet_conditioning_scale=controlnet_conditioning_scale,\n            guess_mode=guess_mode,\n            control_guidance_start=control_guidance_start,\n            control_guidance_end=control_guidance_end,\n            lora_model_name_or_path_or_dict=lora_model_name_or_path_or_dict,\n            lora_alpha=lora_alpha,\n            text_inversion_model_name_or_path=text_inversion_model_name_or_path,\n            fast_mode=fast_mode\n        )\n    else:\n        images, nsfw_content_detected = controlnet_img2img(\n            sd_model=sd_model,\n            controlnet_model=controlnet_model,\n            prompt=prompt,\n            image=aux_image,\n            control_image=mlsd_image,\n            height=height,\n            width=width,\n            strength=strength,\n            num_inference_steps=num_inference_steps,\n            guidance_scale=guidance_scale,\n            negative_prompt=negative_prompt,\n            num_images_per_prompt=num_images_per_prompt,\n            eta=eta,\n            seed=seed,\n            latents=latents,\n            prompt_embeds=prompt_embeds,\n            negative_prompt_embeds=negative_prompt_embeds,\n            output_type=output_type,\n            return_dict=return_dict,\n            callback=callback,\n            callback_steps=callback_steps,\n            cross_attention_kwargs=cross_attention_kwargs,\n            controlnet_conditioning_scale=controlnet_conditioning_scale,\n            guess_mode=guess_mode,\n            control_guidance_start=control_guidance_start,\n            control_guidance_end=control_guidance_end,\n            lora_model_name_or_path_or_dict=lora_model_name_or_path_or_dict,\n            lora_alpha=lora_alpha,\n            text_inversion_model_name_or_path=text_inversion_model_name_or_path,\n        )\n\n    return images, nsfw_content_detected, mlsd_image\n"}
{"type": "source_file", "path": "modules/controlnet/normalbae2img.py", "content": "import torch\nfrom typing import Any, Callable, Dict, List, Optional, Union\nfrom PIL import Image\nimport os\nimport sys\n\n__dir__ = os.path.dirname(os.path.abspath(__file__))\nsys.path.append(os.path.dirname(os.path.dirname(__dir__)))\nfrom modules.shared import SDModel, Controlnet\nfrom modules.controlnet.controlnet_txt2img import controlnet_txt2img\nfrom modules.controlnet.controlnet_img2img import controlnet_img2img\nfrom modules.wrappers import NormalBaeDetectorWrapper\n\n\ndef normalbae2img(\n    sd_model: SDModel,\n    controlnet_model: Controlnet,\n    processor: NormalBaeDetectorWrapper,\n    ori_image: Image,\n    aux_image: Image = None,\n    is_normalbae_image: bool = False,\n    detect_resolution: int = 512,\n    image_resolution: int = 512,\n    normalbae_output_type: str = \"pil\",\n    prompt: Union[str, List[str]] = None,\n    height: Optional[int] = None,\n    width: Optional[int] = None,\n    strength: float = 0.8,\n    num_inference_steps: int = 20,\n    guidance_scale: float = 7.5,\n    negative_prompt: Optional[Union[str, List[str]]] = None,\n    num_images_per_prompt: Optional[int] = 1,\n    eta: float = 0.0,\n    seed: Optional[Union[int, List[int]]] = None,\n    latents: Optional[torch.FloatTensor] = None,\n    prompt_embeds: Optional[torch.FloatTensor] = None,\n    negative_prompt_embeds: Optional[torch.FloatTensor] = None,\n    output_type: Optional[str] = \"pil\",\n    return_dict: bool = True,\n    callback: Optional[Callable[[int, int, torch.FloatTensor], None]] = None,\n    callback_steps: int = 1,\n    cross_attention_kwargs: Optional[Dict[str, Any]] = None,\n    controlnet_conditioning_scale: Union[float, List[float]] = 0.8,\n    guess_mode: bool = False,\n    control_guidance_start: Union[float, List[float]] = 0.0,\n    control_guidance_end: Union[float, List[float]] = 1.0,\n    lora_model_name_or_path_or_dict: Optional[Union[str, List[str]]] = None,\n    lora_alpha: Optional[float] = 0.75,\n    text_inversion_model_name_or_path: Optional[\n        Union[str, List[str], Dict[str, torch.Tensor], List[Dict[str, torch.Tensor]]]\n    ] = None,\n    fast_mode = False\n):\n\n    if is_normalbae_image:\n        normalbae_image = ori_image\n    else:\n        normalbae_image = processor(\n            input_image=ori_image,\n            detect_resolution=detect_resolution,\n            image_resolution=image_resolution,\n            output_type=normalbae_output_type,\n        )\n\n    if aux_image is None:\n        images, nsfw_content_detected = controlnet_txt2img(\n            sd_model=sd_model,\n            controlnet_model=controlnet_model,\n            prompt=prompt,\n            image=normalbae_image,\n            height=height,\n            width=width,\n            num_inference_steps=num_inference_steps,\n            guidance_scale=guidance_scale,\n            negative_prompt=negative_prompt,\n            num_images_per_prompt=num_images_per_prompt,\n            eta=eta,\n            seed=seed,\n            latents=latents,\n            prompt_embeds=prompt_embeds,\n            negative_prompt_embeds=negative_prompt_embeds,\n            output_type=output_type,\n            return_dict=return_dict,\n            callback=callback,\n            callback_steps=callback_steps,\n            cross_attention_kwargs=cross_attention_kwargs,\n            controlnet_conditioning_scale=controlnet_conditioning_scale,\n            guess_mode=guess_mode,\n            control_guidance_start=control_guidance_start,\n            control_guidance_end=control_guidance_end,\n            lora_model_name_or_path_or_dict=lora_model_name_or_path_or_dict,\n            lora_alpha=lora_alpha,\n            text_inversion_model_name_or_path=text_inversion_model_name_or_path,\n            fast_mode=fast_mode\n        )\n    else:\n        images, nsfw_content_detected = controlnet_img2img(\n            sd_model=sd_model,\n            controlnet_model=controlnet_model,\n            prompt=prompt,\n            image=aux_image,\n            control_image=normalbae_image,\n            height=height,\n            width=width,\n            strength=strength,\n            num_inference_steps=num_inference_steps,\n            guidance_scale=guidance_scale,\n            negative_prompt=negative_prompt,\n            num_images_per_prompt=num_images_per_prompt,\n            eta=eta,\n            seed=seed,\n            latents=latents,\n            prompt_embeds=prompt_embeds,\n            negative_prompt_embeds=negative_prompt_embeds,\n            output_type=output_type,\n            return_dict=return_dict,\n            callback=callback,\n            callback_steps=callback_steps,\n            cross_attention_kwargs=cross_attention_kwargs,\n            controlnet_conditioning_scale=controlnet_conditioning_scale,\n            guess_mode=guess_mode,\n            control_guidance_start=control_guidance_start,\n            control_guidance_end=control_guidance_end,\n            lora_model_name_or_path_or_dict=lora_model_name_or_path_or_dict,\n            lora_alpha=lora_alpha,\n            text_inversion_model_name_or_path=text_inversion_model_name_or_path,\n        )\n\n    return images, nsfw_content_detected, normalbae_image\n"}
