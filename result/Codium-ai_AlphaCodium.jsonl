{"repo_info": {"repo_name": "AlphaCodium", "repo_owner": "Codium-ai", "repo_url": "https://github.com/Codium-ai/AlphaCodium"}}
{"type": "test_file", "path": "tests/alpha_codium/code_contests/__init__.py", "content": ""}
{"type": "test_file", "path": "tests/alpha_codium/code_contests/eval/__init__.py", "content": ""}
{"type": "test_file", "path": "tests/__init__.py", "content": ""}
{"type": "test_file", "path": "tests/alpha_codium/__init__.py", "content": ""}
{"type": "test_file", "path": "tests/alpha_codium/code_contests/eval/test_local_exec.py", "content": "import ast\nimport inspect\nimport io\nimport math\nimport os\nimport sys\nimport tempfile\nfrom contextlib import contextmanager\nfrom functools import partial\nfrom typing import Callable, List\n\nimport pytest as pytest\nfrom pysnooper import snoop\n\nfrom alpha_codium.code_contests.eval.local_exec import MultiTestResult, ProgramStatus, execute_candidate_code\nfrom alpha_codium.code_contests.eval.tracer import snooper_kwargs\n\ntimeout = 3\n\n\n@contextmanager\ndef mock_input_output(mock_input_value):\n    new_out = io.StringIO()\n    new_in = io.StringIO(mock_input_value + '\\n')\n\n    old_out = sys.stdout\n    old_in = sys.stdin\n\n    sys.stdout = new_out\n    sys.stdin = new_in\n\n    yield new_out\n\n    sys.stdout = old_out\n    sys.stdin = old_in\n\n\nclass SandboxCaseContainer:\n\n    def __init__(self, f: Callable):\n        self.f = f\n\n    def execute_as_string(self, input: str, sandbox=False):\n        return self.execute_as_str_inner([input], trace=False, sandbox=sandbox)\n\n    def execute_as_string_with_tracing(self, input: str, sandbox=False):\n        return self.execute_as_str_inner([input], trace=True, sandbox=sandbox)\n\n    def execute_as_str_inner(self, inputs: List[str], trace=False, sandbox=False):\n        check_program = self.get_body()\n        f = partial(execute_candidate_code, candidate=check_program, inputs=inputs, test_id=self.f.__name__,\n                    timeout=timeout, sandbox=sandbox, snoop=trace)\n        if sandbox:\n            with tempfile.TemporaryDirectory() as temp_dir:\n                os.chdir(temp_dir)\n                result = f()\n        else:\n            result = f()\n\n        return result\n\n    def get_body(self):\n        function_body = inspect.getsource(self.f)\n        func_ast = ast.parse(function_body)\n        func_def = [node for node in ast.walk(func_ast) if isinstance(node, ast.FunctionDef)][0]\n        body = func_def.body\n        lines = [ast.unparse(node).strip() for node in body]\n        result = \"\\n\".join(lines).strip()\n        print(result)\n        return result\n\n\ndef io_solution():\n    x = input()\n    print(x)\n\n\ndef one_level_and_loop_solution():\n    def my_func(val):\n        for i in range(val):\n            print(i)\n\n    x = int(input())\n    my_func(x)\n\n\ndef multi_level_and_loop_solution():\n    def printer_inner(val):\n        print(val)\n\n    def printer(val):\n        print(\"p\")\n        printer_inner(val)\n\n    def my_func(val):\n        for i in range(val):\n            printer(i)\n\n    x = int(input())\n    my_func(x)\n\n\ndef recursion_solution():\n    def fibonacci(n):\n        if n <= 0:\n            return 0\n        elif n == 1:\n            return 1\n        else:\n            return fibonacci(n - 1) + fibonacci(n - 2)\n\n    x = int(input())\n    fib = fibonacci(x)\n    print(fib)\n\n\ndef timeout_solution():\n    def sleeper(timeout):\n        import time\n        print(f\"sleeping for {timeout + 1}\")\n        time.sleep(timeout + 1)\n\n    timeout = int(input())\n    sleeper(timeout)\n\n\ndef exception_solution():\n    def excepter(n):\n        raise ValueError(f\"test run cannot accept {n}\")\n\n    x = int(input())\n    excepter(x)\n\n\ndef bad_import_solution():\n    print(math.sqrt(int(input())))\n\n\ntest_data = [\n    (io_solution, 'hello', 'hello'),  # (function, input, expected output)\n    (one_level_and_loop_solution, '4', '0\\n1\\n2\\n3'),\n    (multi_level_and_loop_solution, '4', 'p\\n0\\np\\n1\\np\\n2\\np\\n3'),\n    (recursion_solution, '4', '3'),\n]\n\nrun_types = ['regular', 'regular_with_tracing', 'as_string', 'as_string_with_tracing']\n\n\ndef data_id(test_case):\n    f, input_, output_ = test_case\n    return f\"{f.__name__}-{hash(str(input_) + str(output_))}\"\n\nsandbox_ids=[\"not-sandboxed\", \"sandboxed\"]\n\n\n@pytest.mark.parametrize(\"run_type\", run_types)\n@pytest.mark.parametrize(\"sandbox\", [False, True], ids=sandbox_ids)\n@pytest.mark.parametrize(\"func, input, expected\", test_data, ids=[data_id(case) for case in test_data])\ndef test_happy_paths(monkeypatch, func, input, expected, run_type, sandbox):\n    def assert_passed_and_output(expected, result: MultiTestResult):\n        assert len(result.test_results) == 1\n        my_result = result.test_results[0]\n        assert my_result.stdout == expected\n        assert my_result.stderr == ''\n        print(\"trace\\n\")\n        print(my_result.trace)\n\n    my_case = SandboxCaseContainer(func)\n    if 'regular' in run_type:\n        with mock_input_output(input) as captured_output:\n            if 'regular_with_tracing' == run_type:\n                with snoop(**snooper_kwargs):\n                    my_case.f()\n            else:\n                my_case.f()\n            result = captured_output.getvalue().strip()\n            assert expected == result\n\n    elif run_type == 'as_string':\n        res = my_case.execute_as_string(input, sandbox=sandbox)\n        assert_passed_and_output(expected, res)\n\n    elif run_type == 'as_string_with_tracing':\n        res = my_case.execute_as_string_with_tracing(input, sandbox=sandbox)\n        assert_passed_and_output(expected, res)\n\n\ntest_exception_data = [\n    (timeout_solution, str(timeout), ProgramStatus.kTimeout, ''),\n    (exception_solution, '1', ProgramStatus.kFailed, 'test run cannot accept 1'),\n    (bad_import_solution, '1', ProgramStatus.kFailed, \"NameError: name 'math' is not defined\"),\n]\n\ndef exception_data_id(test_case):\n    f, input_, status, _ = test_case\n    return f\"{f.__name__}-{str(status)}-{hash(input_)}\"\n\n@pytest.mark.parametrize(\"run_type\", run_types)\n@pytest.mark.parametrize(\"sandbox\", [False, True], ids=sandbox_ids)\n@pytest.mark.parametrize(\"func, input, status, error_string\", test_exception_data,\n                         ids=[exception_data_id(case) for case in test_exception_data])\ndef test_runtime_issues(monkeypatch, func, input, status, error_string, run_type, sandbox):\n    def assert_status_and_error(result: MultiTestResult, status, err):\n        assert len(result.test_results) == 1\n        my_result = result.test_results[0]\n        assert my_result.program_status == status\n        assert err in my_result.sandbox_result\n        print(\"trace\")\n        print(my_result.trace)\n        print(\"=============\")\n        print(\"stack trace\")\n        print(my_result.sandbox_result)\n\n    my_case = SandboxCaseContainer(func)\n\n    if run_type == 'as_string':\n        res = my_case.execute_as_string(input)\n        assert_status_and_error(res, status, error_string)\n\n    elif run_type == 'as_string_with_tracing':\n        res = my_case.execute_as_string_with_tracing(input)\n        assert_status_and_error(res, status, error_string)\n\n\nif __name__ == '__main__':\n    timeout_solution()\n"}
{"type": "source_file", "path": "alpha_codium/evaluate_dataset.py", "content": "import argparse\nimport json\nfrom collections import OrderedDict\n\nfrom alpha_codium.code_contests.data.provider import CodeContestDataProvider\nfrom alpha_codium.log import get_logger\n\nlogger = get_logger(__name__)\n\ndef evaluate_dataset_solution(dataset_name='valid_and_test_processed',\n                               split_name='test',\n                               solution_path_database='valid_database_solution.json'):\n    \"\"\"\n    Evaluate the performance of dataset solutions.\n\n    Args:\n        dataset_name (str, optional): The name of the dataset. Defaults to 'valid_and_test_processed'.\n        split_name (str, optional): The name of the split. Defaults to 'test'.\n        solution_path_database (str, optional): The path to the solution database file. Defaults to 'valid_database_solution.json'.\n    \"\"\"\n\n    # Load the dataset and solution database\n    data_provider = CodeContestDataProvider(dataset_location=dataset_name)\n    ds = data_provider.dataset[split_name]\n    with open(solution_path_database, 'r') as f:\n        database_solutions = json.load(f)\n        database_solutions[split_name] = OrderedDict(\n            sorted(database_solutions[split_name].items(), key=lambda x: int(x[0])))\n\n    # Initialize counters for passed and failed problems\n    total_passed = 0\n    total_failed = 0\n\n    # Iterate over the solutions in the database\n    for sol in database_solutions[split_name]:\n        try:\n            key_str = sol\n            key_int = int(key_str)\n            problem = ds[key_int]\n            if problem.get('is_valid_problem', True) is False:\n                print(f\"problem {key_int} is not valid\")\n                continue\n            solution = database_solutions[split_name][sol]\n            passed_current = -1\n\n            # scanning the iterations\n            v_iter =[v for v in solution.values() if (v is not None and 'solution' in v)]\n            for v in v_iter:\n                if not v:\n                    continue\n                test_failed_generate = v['test_failed_generate']\n                test_failed_private = v['test_failed_private']\n                test_passed_generate = v['test_passed_generate']\n                test_passed_private = v['test_passed_private']\n                if 'test_timeout_generate' in v:\n                    test_timeout_generate = v['test_timeout_generate']\n                    test_timeout_private = v['test_timeout_private']\n                else:\n                    test_timeout_generate = 0\n                    test_timeout_private = 0\n\n                if ((test_failed_generate + test_timeout_generate + test_failed_private + test_timeout_private) == 0 and\n                        (test_passed_generate + test_passed_private) > 0):\n                    print(f\"problem {key_int} passed all tests\")\n                    passed_current=1\n                    break\n                else:\n                    passed_current = 0\n            if passed_current == 1:\n                total_passed += 1\n            elif passed_current == 0:\n                total_failed += 1\n        except Exception as e:\n            print(f\"Error: {e}\")\n            pass\n\n    # Print the total number of passed and failed problems\n    print(f\"total_passed: {total_passed}, total_failed: {total_failed}\")\n\n    # Calculate the pass rate\n    pass_rate = total_passed / (total_passed + total_failed)\n    print(f\"pass rate: {pass_rate}\")\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\"--dataset_name\", type=str, default=\"valid_and_test_processed\")\nparser.add_argument(\"--split_name\", type=str, default=\"valid\")\nparser.add_argument(\"--database_solution_path\", type=str, default=\"./gpt_3_solution_database_valid.json\")\n\nif __name__ == \"__main__\":\n    args = parser.parse_args()\n    evaluate_dataset_solution(dataset_name=args.dataset_name,\n                              split_name=args.split_name,\n                              solution_path_database=args.database_solution_path)\n"}
{"type": "source_file", "path": "alpha_codium/gen/coding_competitor.py", "content": "import asyncio\nimport logging\nimport os\nfrom jinja2 import Environment, StrictUndefined\n\nfrom alpha_codium.code_contests.data.provider import CodeContestDataProvider\nfrom alpha_codium.gen.stages.run_baseline import run_baseline\nfrom alpha_codium.gen.stages.run_choose_best_solution import run_choose_best_solution\nfrom alpha_codium.gen.stages.run_evaluate_all_ai_tests import run_evaluate_all_ai_tests\nfrom alpha_codium.gen.stages.run_evaluate_public_tests import run_evaluate_public_tests\nfrom alpha_codium.gen.stages.run_generate_ai_test import run_generate_ai_tests\nfrom alpha_codium.gen.stages.run_generate_possible_solutions import run_generate_possible_solutions\nfrom alpha_codium.gen.stages.run_self_reflect import run_self_reflect\nfrom alpha_codium.gen.stages.run_initial_code_generation import run_initial_code_generation\nfrom alpha_codium.gen.stages.utils import set_configurations\nfrom alpha_codium.gen.utils import evaluate_solution_on_subset\nfrom alpha_codium.llm.ai_handler import AiHandler\nfrom alpha_codium.log import get_logger\nfrom alpha_codium.settings.config_loader import get_settings\n\n\nclass CodeContestsCompetitor:\n    def __init__(self):\n        self.prompt = {}\n        for set in get_settings():\n            if 'code_contests_prompt' in set.lower():\n                self.prompt[set.lower()] = get_settings()[set]\n        self.ai_handler = AiHandler()\n\n    def render(self, problem_json, prompt: str):\n        environment = Environment(undefined=StrictUndefined)\n        environment.globals[\"zip\"] = zip\n        environment.globals[\"enumerate\"] = enumerate\n        sys_prompt = environment.from_string(self.prompt[prompt].system).render(problem_json)\n        usr_prompt = environment.from_string(self.prompt[prompt].user).render(problem_json)\n        if hasattr(self.prompt[prompt], 'temperature'):\n            temperature = self.prompt[prompt].temperature\n        else:\n            temperature = 0.2\n        if hasattr(self.prompt[prompt], 'frequency_penalty'):\n            frequency_penalty = self.prompt[prompt].frequency_penalty\n        else:\n            frequency_penalty = None\n        return sys_prompt, usr_prompt, temperature, frequency_penalty\n\n    async def _run(self, model, problem, prompt:str = \"code_contests_prompt_reflect\"):\n        system_prompt, user_prompt, temperature, frequency_penalty = self.render(problem, prompt)\n\n        if frequency_penalty == None:\n            frequency_penalty = get_settings().get(\"config.frequency_penalty\")\n\n        response, finish_reason = await self.ai_handler.chat_completion(\n            model=model, system=system_prompt, user=user_prompt,\n            temperature=temperature, frequency_penalty=frequency_penalty,\n        )\n        return response, finish_reason\n\n    async def run(self, problem, iteration=0, logger_ext=None):\n        if logger_ext:\n            logger = logger_ext\n        else:\n            logger = get_logger(__name__)\n        logger.info(f\"Running code contests competitor, model {get_settings().config['model']}\")\n\n        try:\n            if get_settings().get(\"solve.use_baseline\", False):\n                problem['code_recent_solution'] = await run_baseline(self, problem)\n            else:\n                # configurations\n                problem = set_configurations(problem, iteration)\n\n                # self-reflect\n                problem = await run_self_reflect(self, problem)\n\n                # generate solutions\n                problem = await run_generate_possible_solutions(self, problem)\n\n                # choose best solution\n                problem = await run_choose_best_solution(self, problem)\n\n                # generate ai tests\n                problem = await run_generate_ai_tests(self, problem)\n\n                # initial code generation\n                problem = await run_initial_code_generation(self, problem)\n\n                # evaluate on public tests\n                problem = await run_evaluate_public_tests(self, problem)\n\n                # evaluate on ai tests\n                problem = await run_evaluate_all_ai_tests(self, problem)\n\n            return problem['code_recent_solution']\n        except Exception as e:\n            logging.error(f\"Error: {e}\")\n            return \"\"\n\n    def solve_problem_in_dataset(self, example, iteration=0, logger_ext=None):\n        problem = {k: example.get(k) for k in [\"name\", \"description\", 'public_tests']}\n        prediction = asyncio.run(self.run(problem=problem, iteration=iteration, logger_ext=logger_ext))\n        return prediction\n\n\ndef solve_problem(dataset_name,\n                  split_name=\"valid\",\n                  problem_name=\"\",\n                  problem_number=0):\n\n    # load dataset\n    logger = get_logger(__name__)\n    data_provider = CodeContestDataProvider(dataset_location=dataset_name)\n    if problem_number and problem_name:\n        logger.info(f\"problem_number and problem_name are both specified, using problem_name\")\n    if not problem_name and problem_number:\n        problem_name = data_provider.dataset[split_name][int(problem_number)]['name']\n        logger.info(f\"problem_name: {problem_name}\")\n\n    # find problem\n    problem = data_provider.find_problem(ds=data_provider.dataset, problem_name=problem_name, split_name=split_name)\n    logger.info(f\"problem['name']: {problem['name']}\")\n\n    # # check if problem is valid (at least one of the provided solutions actually passes the generated tests)\n    # if not problem.get('is_valid_problem', True):\n    #     logger.info(f\"problem['is_valid_problem'] == False, skipping\")\n    #     return None, None\n\n    # evaluate prev solutions\n    evaluate_prev_solutions = get_settings().get(\"dataset.evaluate_prev_solutions\", False)\n    if evaluate_prev_solutions:\n        try:\n            if not problem['solutions']['solution']:\n                logger.info(\"No public solutions for this problem\")\n            found_solution = False\n            for index_published, sol_published in enumerate(problem['solutions']['solution']):\n                if 'python' not in problem['solutions']['language'][index_published].lower():\n                    found_solution = True\n                    continue\n                logger.info(f\"evaluating public solution {index_published} on private tests...\")\n                test_results, test_passed_private, test_failed_private, test_timeout_private \\\n                    = evaluate_solution_on_subset('private_tests', problem, sol_published, silent=True)\n                logger.info(f\"evaluating public solution {index_published} on generated tests...\")\n                test_results, test_passed_generate, test_failed_generate, test_timeout_generate = (\n                    evaluate_solution_on_subset('generated_tests', problem, sol_published, silent=True))\n\n                if (test_failed_private == test_failed_generate == test_timeout_private == test_timeout_generate == 0) \\\n                        and test_passed_private + test_passed_generate > 0:\n                    logger.info(f\"sol_published index {index_published} passed all tests:\\n{sol_published}\")\n                    found_solution = True\n                    break\n\n            if not found_solution:\n                logger.info(f\"None of the public solutions passed all tests\")\n        except Exception as e:\n            logger.error(f\"Error evaluating public solutions: {e}\")\n            pass\n\n\n    return solve_my_problem(problem)\n\n\ndef solve_my_problem(problem):\n\n    base_path = os.getcwd()\n    logger = get_logger(__name__)\n\n    solver = CodeContestsCompetitor()\n    os.chdir(base_path)\n    solution = solver.solve_problem_in_dataset(problem)\n    logger.info(f\"testing solution on private tests with prediction:\\n{solution}\")\n\n    logger.info(f\"evaluating solution on public tests...\")\n    test_results, test_passed_public, test_failed_public, test_timeout_public = evaluate_solution_on_subset('public_tests',\n                                                                                                       problem,\n                                                                                                       solution,\n                                                                                                       silent=True)\n\n\n    logger.info(f\"evaluating solution on private tests...\")\n    test_results, test_passed_private, test_failed_private, test_timeout_private = evaluate_solution_on_subset('private_tests',\n                                                                                                       problem,\n                                                                                                       solution,\n                                                                                                       silent=True)\n\n    logger.info(f\"evaluating solution on generated tests...\")\n    test_results, test_passed_generate, test_failed_generate, test_timeout_generate = evaluate_solution_on_subset(\n        'generated_tests', problem, solution, silent=True)\n\n    logger.info(f\"\\ntest_passed_generate: {test_passed_generate}, test_passed_private: {test_passed_private}, test_passed_public: {test_passed_public}\"\n                f\"\\ntest_failed_generate: {test_failed_generate}, test_failed_private: {test_failed_private}, test_failed_public: {test_failed_public}\"\n                f\"\\ntest_timeout_generate: {test_timeout_generate}, test_timeout_private: {test_timeout_private}, test_timeout_public: {test_timeout_public}\")\n\n    return solution, test_results\n"}
{"type": "source_file", "path": "alpha_codium/__init__.py", "content": "import os\nimport random\n\nimport numpy as np\n\n\ndef set_all_seeds(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n\n    try:\n        import tensorflow as tf\n        tf.random.set_seed(seed)\n    except ImportError:\n        pass\n\n    try:\n        import torch\n        torch.manual_seed(seed)\n        if torch.cuda.is_available():\n            torch.cuda.manual_seed(seed)\n            torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n    except ImportError:\n        pass\n\nset_all_seeds(1337)"}
{"type": "source_file", "path": "alpha_codium/gen/__init__.py", "content": ""}
{"type": "source_file", "path": "alpha_codium/log/__init__.py", "content": "import json\nimport logging\nimport sys\nfrom enum import Enum\n\nfrom loguru import logger\n\n\nclass LoggingFormat(str, Enum):\n    CONSOLE = \"CONSOLE\"\n    JSON = \"JSON\"\n\n\ndef json_format(record: dict) -> str:\n    return record[\"message\"]\n\n\ndef setup_logger(logger_path: str = \"./example.log\",\n                 level: str = \"INFO\",\n                 fmt: LoggingFormat = LoggingFormat.CONSOLE):\n    level: int = logging.getLevelName(level.upper())\n    if type(level) is not int:\n        level = logging.INFO\n\n    fileHandler = logging.FileHandler(logger_path, mode='w')\n\n    if fmt == LoggingFormat.JSON:\n        logger.remove(None)\n        logger.add(\n            sys.stdout,\n            level=level,\n            format=\"{message}\",\n            colorize=False,\n            serialize=True,\n        )\n    elif fmt == LoggingFormat.CONSOLE:\n        logger.remove(None)\n        logger.add(sys.stdout, level=level, colorize=True)\n        logger.add(fileHandler, level=logging.DEBUG)\n\n    return logger\n\n\ndef get_logger(*args, **kwargs):\n    return logger\n"}
{"type": "source_file", "path": "alpha_codium/gen/stages/run_initial_code_generation.py", "content": "import copy\nimport logging\n\nfrom alpha_codium.settings.config_loader import get_settings\nfrom alpha_codium.gen.stages.run_initial_solve import run_initial_solve\nfrom alpha_codium.gen.stages.run_tests import run_tests\nfrom alpha_codium.log import get_logger\n\nlogger = get_logger(__name__)\n\n\nasync def run_initial_code_generation(self, problem):\n    counter_retry = 0\n    while True:\n        try:\n            logger.info(\"--run initial code generation stage--\")\n\n            max_attempts = get_settings().get('initial_code_generation.max_attempts', 5)\n            counter = 0\n\n            # set the public tests as input\n            test_input = problem['public_tests']['input']\n            test_output = problem['public_tests']['output']\n\n            # generate an initial code, using the top solution from the previous stage\n            problem = await run_initial_solve(self, problem)\n\n            # run the solution on the selected tests\n            problem, passed_tests, non_empty_output, error_str, trace_str, tests_timeout, d_tot \\\n                = run_tests(self, problem, counter, test_input, test_output)\n\n            best_solution = copy.deepcopy(problem['code_recent_solution'])\n            best_d = float('inf') # distance to the correct solution\n\n            # set the distance to the correct solution\n            if -1 < d_tot < best_d:\n                best_solution = copy.deepcopy(problem['code_recent_solution'])\n                best_d = d_tot\n\n            while not passed_tests:\n                counter += 1\n                if counter > max_attempts:\n                    logger.error(f\"Failed to pass tests after {counter - 1} attempts. exiting the stage\")\n                    break\n\n                s_best_solution_original = problem['s_best_solution']\n                if counter > 1 and 's_possible_solutions' in problem:\n                    # give two attempts to the highest ranked solution\n                    problem['s_best_solution'] = problem['s_possible_solutions'][\n                        counter % len(problem['s_possible_solutions'])]\n                problem = await run_initial_solve(self, problem)\n                problem['s_best_solution'] = s_best_solution_original\n\n                problem, passed_tests, non_empty_output, error_str, trace_str, tests_timeout, d_tot \\\n                    = run_tests(self, problem, counter, test_input, test_output)\n\n                if passed_tests:\n                    logger.info(f\"Passed tests after {counter} attempts\")\n                    break\n                else:\n                    logger.info(f\"Failed to pass tests after {counter} attempts, d: {d_tot}, best_d so far: {best_d}\")\n\n                # save the best solution so far\n                if -1 < d_tot < best_d:\n                    best_solution = copy.deepcopy(problem['code_recent_solution'])\n                    best_d = d_tot\n\n            # set the best solution\n            if not passed_tests and best_d < float('inf'):\n                logger.error(f'Reverting to best solution so far, d_tot: {best_d}')\n                problem['code_recent_solution'] = best_solution\n\n            return problem\n        except Exception as e:\n            logging.error(f\"'initial code generation' stage, counter_retry {counter_retry}, Error: {e}\")\n            counter_retry += 1\n            if counter_retry > 2:\n                raise e\n"}
{"type": "source_file", "path": "alpha_codium/gen/utils.py", "content": "import re\nfrom typing import List\n\nimport yaml\n\nfrom alpha_codium.code_contests.eval.code_test_runners import eval_solution\nfrom alpha_codium.settings.config_loader import get_settings\nfrom alpha_codium.log import get_logger\n\nlogger = get_logger(__name__)\n\n\ndef clip_string(s: str, max_lines: int = None):\n    lines = s.split(\"\\n\")\n    if max_lines is not None and 0 < max_lines < len(lines):\n        logger.debug(f\"clipping string from {len(lines)} to {max_lines}\")\n        half_lines = int(max_lines / 2)\n        lines = (\n                lines[:half_lines] +\n                [f\"\\n.... {len(lines) - max_lines} omitted lines ....\\n\"] +\n                lines[-half_lines:]\n        )\n        return \"\\n\".join(lines)\n    else:\n        return s\n\n\ndef render_trace(trace_data):\n    if not trace_data:\n        return ''\n\n    max_trace_lines = get_settings().code_tester.get(\"max_trace_lines\")\n    trace_data = clip_string(trace_data, max_trace_lines)\n    return trace_data\n\n\ndef postprocess_response(response):\n    response = str(response)\n    if response.endswith(\"stop\"):\n        response = response[:-4]\n    pattern = r'```\\w*\\n(.*?)```'\n    matches = re.findall(pattern, response, re.DOTALL)\n    if matches:\n        response = matches[0]\n    return response\n\n\ndef evaluate_solution_on_subset(evaluation_test_type, problem, solution, silent=False, break_on_timeout=True):\n    # evaluate solution\n    test_results = None\n    if evaluation_test_type:\n        test_results = eval_solution(evaluation_test_type=evaluation_test_type, example=problem, prediction=solution,\n                                     silent=silent, break_on_timeout=break_on_timeout)\n\n    if test_results[1] == []:\n        if not silent:\n            logger.info(\"=====================================\")\n            logger.info(\"No tests\")\n            logger.info(\"=====================================\")\n        return test_results, 0, 0, 0\n\n    if (hasattr(test_results[1], 'compilation_result') and\n            test_results[1].compilation_result.program_status.name == 'kTimeout'):\n        if not silent:\n            logger.info(\"=====================================\")\n            logger.info(\"Timeout\")\n            logger.info(\"=====================================\")\n        return test_results, 0, 0, len(test_results[0])\n\n    test_passed = 0\n    test_failed = 0\n    test_timeout = 0\n    if not problem[evaluation_test_type]['input']:\n        logger.info(f\"No {evaluation_test_type} for this problem\")\n    else:\n        for test in test_results[1].test_results:\n            if (hasattr(test, 'program_status') and test.program_status.name == 'kTimeout'):\n                test_timeout += 1\n            elif not test.passed:\n                test_failed += 1\n            else:\n                test_passed += 1\n        if not silent:\n            logger.info(\"=====================================\")\n            logger.info(f\"test_passed: {test_passed}, test_failed: {test_failed}, test_timeout: {test_timeout}\")\n            logger.info(\"=====================================\")\n\n    return test_results, test_passed, test_failed, test_timeout\n\n\ndef evaluate_on_private_tests(evaluation_test_type, problem, solution, silent=True):\n    # evaluate solution\n    test_results = None\n    if evaluation_test_type:\n        test_results = eval_solution(evaluation_test_type=evaluation_test_type, example=problem, prediction=solution, silent=silent)\n\n    test_passed = 0\n    test_failed = 0\n    test_timeout = 0\n\n    if not test_results[1]:\n        logger.info(\"No tests were run\")\n        return test_results, 0, 0\n\n    for test in test_results[1].test_results:\n        if test.program_status.name=='kTimeout':\n            test_timeout += 1\n        elif not test.passed:\n            test_failed += 1\n        else:\n            test_passed += 1\n\n\n    logger.info(\"=====================================\")\n    logger.info(f\"test_passed: {test_passed}, test_failed: {test_failed}, test_timeout: {test_timeout}\")\n    logger.info(\"=====================================\")\n\n    return test_results, test_passed, test_failed, test_timeout\n\n\ndef load_yaml(response_text: str, keys_fix_yaml: List[str] = []) -> dict:\n    response_text = response_text.rstrip(\"` \\n\")\n    response_text = response_text.removeprefix('```yaml').rstrip('`')\n    try:\n        data = yaml.safe_load(response_text)\n    except Exception as e:\n        data = try_fix_yaml(response_text, keys_fix_yaml=keys_fix_yaml)\n        if not data:\n            get_logger().info(f\"Failed to parse AI YAML prediction: {e}\")\n    return data\n\n\ndef try_fix_yaml(response_text: str, keys_fix_yaml: List[str] = []) -> dict:\n    response_text_lines = response_text.split('\\n')\n\n    keys = keys_fix_yaml\n    response_text_lines_copy = response_text_lines.copy()\n    for i in range(0, len(response_text_lines_copy)):\n        for key in keys:\n            if response_text_lines_copy[i].strip().startswith(key) and not '|' in response_text_lines_copy[i]:\n                response_text_lines_copy[i] = response_text_lines_copy[i].replace(f'{key}',\n                                                                                  f'{key} |-\\n        ')\n    try:\n        data = yaml.safe_load('\\n'.join(response_text_lines_copy))\n        get_logger().info(f\"Successfully parsed AI prediction after adding |-\\n\")\n        return data\n    except:\n        raise \"yaml parsing error\""}
{"type": "source_file", "path": "alpha_codium/settings/config_loader.py", "content": "import pathlib\nfrom os import listdir\nfrom os.path import abspath, dirname, join, isfile\nimport glob\n\nfrom dynaconf import Dynaconf\n\nPR_AGENT_TOML_KEY = \"pr-agent\"\n\ncurrent_dir = dirname(abspath(__file__))\n# setting_dir = join(current_dir, \"settings\")\nsetting_dir = current_dir\n\n\n\ntoml_files = list(pathlib.Path(join(setting_dir)).glob('*.toml')) # includes hidden files\nglobal_settings = Dynaconf(\n    envvar_prefix=False,\n    merge_enabled=True,\n    settings_files=toml_files,\n)\n\n\ndef get_settings():\n    return global_settings\n"}
{"type": "source_file", "path": "alpha_codium/gen/stages/indirect/run_fix_self_reflect.py", "content": "import copy\nimport functools\nimport logging\nimport yaml\n\nfrom alpha_codium.settings.config_loader import get_settings\nfrom alpha_codium.gen.utils import postprocess_response\nfrom alpha_codium.llm.ai_invoker import send_inference\nfrom alpha_codium.log import get_logger\n\nlogger = get_logger(__name__)\n\n\nasync def run_validate_self_reflect(self, problem):\n    try:\n        logger.info(\"--validate reflection stage--\")\n        f = functools.partial(self._run, problem=problem, prompt=\"code_contests_prompts_validate_reflection\")\n\n        # inference\n        response_validate_reflect, _ = await send_inference(f)\n        response_validate_reflect = response_validate_reflect.rstrip(\"` \\n\")\n        if response_validate_reflect.startswith(\"```yaml\"):\n            response_validate_reflect = response_validate_reflect[8:]\n        try:\n            response_validate_reflect_yaml = yaml.safe_load(response_validate_reflect)\n        except yaml.YAMLError:\n            response_validate_reflect = postprocess_response(response_validate_reflect)  # try to include only the yaml part\n            response_validate_reflect_yaml = yaml.safe_load(response_validate_reflect)\n\n        # check number of tests\n        actual_number_of_tests = len(problem['public_tests']['input'])\n        calculated_number_of_tests = len(response_validate_reflect_yaml['fixed_tests_explanations'])\n        if actual_number_of_tests != calculated_number_of_tests:\n            raise (f\"Error: number of tests in validate self-reflection ({calculated_number_of_tests}) \"\n                   f\"does not match the actual number of tests ({actual_number_of_tests})\")\n\n        problem['response_validate_self_reflect'] = response_validate_reflect\n        problem['tests_explanations'] = response_validate_reflect_yaml['fixed_tests_explanations']\n        problem['tests_explanations_str'] = response_validate_reflect.split('tests_explanations:')[1]\n\n        # re-order the public tests from easiest to hardest\n        problem['public_tests']['original'] = copy.deepcopy(problem['public_tests'])\n        problem['public_tests']['input'] = [t['input'] for t in problem['tests_explanations']]\n        problem['public_tests']['output'] = [t['output'] for t in problem['tests_explanations']]\n        problem['public_tests']['explanation'] = [t['explanation'] for t in problem['tests_explanations']]\n\n        return problem\n    except Exception as e:\n        logging.error(f\"Failed 'run_validate_self_reflect', Error: {e}\")\n        return problem\n"}
{"type": "source_file", "path": "alpha_codium/gen/stages/utils.py", "content": "from alpha_codium.log import get_logger\n\nlogger = get_logger(__name__)\n\n\ndef set_configurations(problem, iteration=0):\n    # configurations\n    problem = {k: problem.get(k) for k in [\"name\", \"description\", \"public_tests\"]}\n    problem['iteration'] = iteration\n\n    # initialize passed tests field\n    problem['passed_tests'] = {}\n    problem['passed_tests']['inputs'] = []\n    problem['passed_tests']['outputs'] = []\n\n    # shorter description, without the input-output examples\n    if '\\nExample\\n' in problem['description']:\n        problem['description_short'] = problem['description'].split('\\nExample\\n')[0].strip()\n    elif '\\nExamples\\n' in problem['description']:\n        problem['description_short'] = problem['description'].split('\\nExamples\\n')[0].strip()\n    else:\n        logger.info(f\"could not split description to short description, description: {problem['description']}\")\n        problem['description_short'] = problem['description']\n    return problem"}
{"type": "source_file", "path": "alpha_codium/gen/stages/run_initial_solve.py", "content": "import functools\nimport logging\nfrom alpha_codium.llm.ai_invoker import send_inference\nfrom alpha_codium.log import get_logger\nfrom alpha_codium.settings.config_loader import get_settings\n\nlogger = get_logger(__name__)\n\n\nasync def run_initial_solve(self, problem):\n    counter_retry = 0\n    while True:\n        try:\n            logger.info(\"--initial solve stage--\")\n\n            f = functools.partial(self._run, problem=problem, prompt=choose_prompt())\n            response_solve, _ = await send_inference(f)\n\n            # clean up the response\n            response_solve = response_solve.rstrip(\"` \\n\")\n            if response_solve.startswith(\"```python\"):\n                response_solve = response_solve[10:]\n            elif response_solve.startswith(\"python\"):\n                response_solve = response_solve[6:]\n\n            # save the response\n            problem['code_recent_solution'] = response_solve\n            problem['code_prev_solution'] = response_solve\n            return problem\n        except Exception as e:\n            logging.error(f\"'initial solve' stage, counter_retry {counter_retry}, Error: {e}\")\n            counter_retry += 1\n            if counter_retry > 2:\n                raise e\n\ndef choose_prompt():\n    if get_settings().get(\"solve.use_direct_solutions\", False):\n        return \"code_contests_prompts_solve_direct\"\n    else:\n        return \"code_contests_prompts_solve\""}
{"type": "source_file", "path": "alpha_codium/gen/generators.py", "content": "import asyncio\nimport functools\n\nfrom alpha_codium.llm.ai_handler import AiHandler\nfrom alpha_codium.llm.ai_invoker import send_inference\n\n\nclass SimplePrompt:\n    def __init__(self, system_prompt=\"\", temperature=0.2, frequency_penalty=0):\n        self.system_prompt = system_prompt\n        self.temperature = temperature\n        self.frequency_penalty = frequency_penalty\n        self.ai_handler = AiHandler()\n\n    async def _run(self, model, user_prompt):\n        response, finish_reason = await self.ai_handler.chat_completion(\n            model=model,\n            temperature=self.temperature,\n            frequency_penalty=self.frequency_penalty,\n            system=self.system_prompt,\n            user=user_prompt,\n        )\n        return response\n\n    async def run(self, user_prompt):\n        f = functools.partial(self._run, user_prompt=user_prompt)\n        response = await send_inference(f)\n        return response\n\n\nif __name__ == \"__main__\":\n    p = SimplePrompt()\n    asyncio.run(p.run(\"what is the capital city of Israel\"))\n"}
{"type": "source_file", "path": "alpha_codium/solve_problem.py", "content": "import argparse\n\nfrom alpha_codium.gen.coding_competitor import solve_problem\nfrom alpha_codium.log import setup_logger\nfrom alpha_codium.settings.config_loader import get_settings\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\"--dataset_name\", type=str, default=\"valid_and_test_processed\")\nparser.add_argument(\"--split_name\", type=str, default=\"valid\")\nparser.add_argument(\"--problem_number\", type=int, default=0)\nparser.add_argument(\"--problem_name\", type=str, default=\"\")\n\nif __name__ == \"__main__\":\n    args = parser.parse_args()\n    setup_logger()\n    solve_problem(dataset_name=args.dataset_name,\n                  split_name=args.split_name,\n                  problem_number=args.problem_number,\n                  problem_name=args.problem_name)\n"}
{"type": "source_file", "path": "alpha_codium/gen/stages/run_self_reflect.py", "content": "import functools\nimport logging\nimport yaml\n\nfrom alpha_codium.gen.stages.indirect.run_fix_self_reflect import run_validate_self_reflect\nfrom alpha_codium.settings.config_loader import get_settings\nfrom alpha_codium.gen.utils import postprocess_response\nfrom alpha_codium.llm.ai_invoker import send_inference\nfrom alpha_codium.log import get_logger\n\nlogger = get_logger(__name__)\n\n\nasync def run_self_reflect(self, problem):\n    counter_retry = 0\n    while True:\n        try:\n            logger.info(\"--reflection stage--\")\n\n            # get settings\n            validate_self_reflection = get_settings().get('self_reflection.validate_self_reflection', False)\n            actual_number_of_tests = len(problem['public_tests']['input'])\n            problem['actual_number_of_tests'] = actual_number_of_tests\n            f = functools.partial(self._run, problem=problem, prompt=\"code_contests_prompt_reflect\")\n\n            # inference\n            response_reflect, _ = await send_inference(f)\n            response_reflect = response_reflect.rstrip(\"` \\n\")\n            if response_reflect.startswith(\"```yaml\"):\n                response_reflect = response_reflect[8:]\n            try:\n                response_reflect_yaml = yaml.safe_load(response_reflect)\n            except yaml.YAMLError:\n                response_reflect = postprocess_response(response_reflect)  # try to include only the yaml part\n                response_reflect_yaml = yaml.safe_load(response_reflect)\n\n            # check number of tests\n            actual_number_of_tests = len(problem['public_tests']['input'])\n            calculated_number_of_tests = len(response_reflect_yaml['tests_explanations'])\n            if actual_number_of_tests != calculated_number_of_tests:\n                raise (f\"Error: number of tests in self-reflection ({calculated_number_of_tests}) \"\n                       f\"does not match the actual number of tests ({actual_number_of_tests})\")\n            problem['response_reflect'] = response_reflect\n            try:\n                problem['self_reflection'] = '- ' + '\\n- '.join(response_reflect_yaml['self_reflection'])\n                if problem['self_reflection'].startswith('- - '):\n                    problem['self_reflection'] = problem['self_reflection'][2:]\n            except:\n                problem['self_reflection'] = response_reflect_yaml['self_reflection']\n            problem['tests_explanations'] = response_reflect_yaml['tests_explanations']\n            problem['tests_explanations_str'] = response_reflect.split('tests_explanations:')[1]\n\n            # double validation self-reflection\n            if validate_self_reflection:\n                problem = await run_validate_self_reflect(self, problem)\n\n            for s in problem['tests_explanations']:\n                s['input'] = s['input'].replace('\\\\n', '\\n')\n                s['output'] = s['output'].replace('\\\\n', '\\n')\n                s['explanation'] = s['explanation'].replace('\\\\n', '\\n')\n\n            return problem\n        except Exception as e:\n            logging.error(f\"'run_self_reflect' stage, counter_retry {counter_retry}, Error: {e}\")\n            counter_retry += 1\n            if counter_retry > 2:\n                raise e\n"}
{"type": "source_file", "path": "alpha_codium/litellm/proxy/_types.py", "content": "import enum\nimport json\nimport os\nimport sys\nimport uuid\nfrom dataclasses import fields\nfrom datetime import datetime\nfrom typing import TYPE_CHECKING, Any, Dict, List, Literal, Optional, TypedDict, Union\n\nfrom pydantic import BaseModel, Extra, Field\ntry:\n    from pydantic import model_validator, Json, ConfigDict\nexcept ImportError:\n    model_validator = None\n    Json = None\n    ConfigDict = None\n\nfrom typing_extensions import Annotated\n\nfrom litellm.types.router import UpdateRouterConfig\nfrom litellm.types.utils import ProviderField\n\nif TYPE_CHECKING:\n    from opentelemetry.trace import Span as _Span\n\n    Span = _Span\nelse:\n    Span = Any\n\n\nclass LitellmUserRoles(str, enum.Enum):\n    \"\"\"\n    Admin Roles:\n    PROXY_ADMIN: admin over the platform\n    PROXY_ADMIN_VIEW_ONLY: can login, view all own keys, view all spend\n\n    Internal User Roles:\n    INTERNAL_USER: can login, view/create/delete their own keys, view their spend\n    INTERNAL_USER_VIEW_ONLY: can login, view their own keys, view their own spend\n\n\n    Team Roles:\n    TEAM: used for JWT auth\n\n\n    Customer Roles:\n    CUSTOMER: External users -> these are customers\n\n    \"\"\"\n\n    # Admin Roles\n    PROXY_ADMIN = \"proxy_admin\"\n    PROXY_ADMIN_VIEW_ONLY = \"proxy_admin_viewer\"\n\n    # Internal User Roles\n    INTERNAL_USER = \"internal_user\"\n    INTERNAL_USER_VIEW_ONLY = \"internal_user_viewer\"\n\n    # Team Roles\n    TEAM = \"team\"\n\n    # Customer Roles - External users of proxy\n    CUSTOMER = \"customer\"\n\n    def __str__(self):\n        return str(self.value)\n\n    @property\n    def description(self):\n        \"\"\"\n        Descriptions for the enum values\n        \"\"\"\n        descriptions = {\n            \"proxy_admin\": \"admin over litellm proxy, has all permissions\",\n            \"proxy_admin_viewer\": \"view all keys, view all spend\",\n            \"internal_user\": \"view/create/delete their own keys, view their own spend\",\n            \"internal_user_viewer\": \"view their own keys, view their own spend\",\n            \"team\": \"team scope used for JWT auth\",\n            \"customer\": \"customer\",\n        }\n        return descriptions.get(self.value, \"\")\n\n    @property\n    def ui_label(self):\n        \"\"\"\n        UI labels for the enum values\n        \"\"\"\n        ui_labels = {\n            \"proxy_admin\": \"Admin (All Permissions)\",\n            \"proxy_admin_viewer\": \"Admin (View Only)\",\n            \"internal_user\": \"Internal User (Create/Delete/View)\",\n            \"internal_user_viewer\": \"Internal User (View Only)\",\n            \"team\": \"Team\",\n            \"customer\": \"Customer\",\n        }\n        return ui_labels.get(self.value, \"\")\n\n\nclass LitellmTableNames(str, enum.Enum):\n    \"\"\"\n    Enum for Table Names used by LiteLLM\n    \"\"\"\n\n    TEAM_TABLE_NAME: str = \"LiteLLM_TeamTable\"\n    USER_TABLE_NAME: str = \"LiteLLM_UserTable\"\n    KEY_TABLE_NAME: str = \"LiteLLM_VerificationToken\"\n    PROXY_MODEL_TABLE_NAME: str = \"LiteLLM_ModelTable\"\n\n\nAlertType = Literal[\n    \"llm_exceptions\",\n    \"llm_too_slow\",\n    \"llm_requests_hanging\",\n    \"budget_alerts\",\n    \"db_exceptions\",\n    \"daily_reports\",\n    \"spend_reports\",\n    \"cooldown_deployment\",\n    \"new_model_added\",\n    \"outage_alerts\",\n    \"region_outage_alerts\",\n]\n\n\ndef hash_token(token: str):\n    import hashlib\n\n    # Hash the string using SHA-256\n    hashed_token = hashlib.sha256(token.encode()).hexdigest()\n\n    return hashed_token\n\n\nclass LiteLLMBase(BaseModel):\n    \"\"\"\n    Implements default functions, all pydantic objects should have.\n    \"\"\"\n\n    def json(self, **kwargs):\n        try:\n            return self.model_dump(**kwargs)  # noqa\n        except Exception as e:\n            # if using pydantic v1\n            return self.dict(**kwargs)\n\n    def fields_set(self):\n        try:\n            return self.model_fields_set  # noqa\n        except:\n            # if using pydantic v1\n            return self.__fields_set__\n\n    model_config = ConfigDict(protected_namespaces=())\n\n\nclass LiteLLM_UpperboundKeyGenerateParams(LiteLLMBase):\n    \"\"\"\n    Set default upperbound to max budget a key called via `/key/generate` can be.\n    \"\"\"\n\n    max_budget: Optional[float] = None\n    budget_duration: Optional[str] = None\n    max_parallel_requests: Optional[int] = None\n    tpm_limit: Optional[int] = None\n    rpm_limit: Optional[int] = None\n\n\nclass LiteLLMRoutes(enum.Enum):\n    openai_route_names: List = [\n        \"chat_completion\",\n        \"completion\",\n        \"embeddings\",\n        \"image_generation\",\n        \"audio_transcriptions\",\n        \"moderations\",\n        \"model_list\",  # OpenAI /v1/models route\n    ]\n    openai_routes: List = [\n        # chat completions\n        \"/engines/{model}/chat/completions\",\n        \"/openai/deployments/{model}/chat/completions\",\n        \"/chat/completions\",\n        \"/v1/chat/completions\",\n        # completions\n        \"/engines/{model}/completions\",\n        \"/openai/deployments/{model}/completions\",\n        \"/completions\",\n        \"/v1/completions\",\n        # embeddings\n        \"/engines/{model}/embeddings\",\n        \"/openai/deployments/{model}/embeddings\",\n        \"/embeddings\",\n        \"/v1/embeddings\",\n        # image generation\n        \"/images/generations\",\n        \"/v1/images/generations\",\n        # audio transcription\n        \"/audio/transcriptions\",\n        \"/v1/audio/transcriptions\",\n        # audio Speech\n        \"/audio/speech\",\n        \"/v1/audio/speech\",\n        # moderations\n        \"/moderations\",\n        \"/v1/moderations\",\n        # batches\n        \"/v1/batches\",\n        \"/batches\",\n        \"/v1/batches{batch_id}\",\n        \"/batches{batch_id}\",\n        # files\n        \"/v1/files\",\n        \"/files\",\n        \"/v1/files/{file_id}\",\n        \"/files/{file_id}\",\n        \"/v1/files/{file_id}/content\",\n        \"/files/{file_id}/content\",\n        # assistants-related routes\n        \"/assistants\",\n        \"/v1/assistants\",\n        \"/v1/assistants/{assistant_id}\",\n        \"/assistants/{assistant_id}\",\n        \"/threads\",\n        \"/v1/threads\",\n        \"/threads/{thread_id}\",\n        \"/v1/threads/{thread_id}\",\n        \"/threads/{thread_id}/messages\",\n        \"/v1/threads/{thread_id}/messages\",\n        \"/threads/{thread_id}/runs\",\n        \"/v1/threads/{thread_id}/runs\",\n        # models\n        \"/models\",\n        \"/v1/models\",\n        # token counter\n        \"/utils/token_counter\",\n    ]\n\n    anthropic_routes: List = [\n        \"/v1/messages\",\n    ]\n\n    info_routes: List = [\n        \"/key/info\",\n        \"/team/info\",\n        \"/team/list\",\n        \"/user/info\",\n        \"/model/info\",\n        \"/v2/model/info\",\n        \"/v2/key/info\",\n        \"/model_group/info\",\n        \"/health\",\n    ]\n\n    # NOTE: ROUTES ONLY FOR MASTER KEY - only the Master Key should be able to Reset Spend\n    master_key_only_routes: List = [\n        \"/global/spend/reset\",\n    ]\n\n    sso_only_routes: List = [\n        \"/key/generate\",\n        \"/key/update\",\n        \"/key/delete\",\n        \"/global/spend/logs\",\n        \"/global/predict/spend/logs\",\n        \"/sso/get/logout_url\",\n    ]\n\n    management_routes: List = [  # key\n        \"/key/generate\",\n        \"/key/update\",\n        \"/key/delete\",\n        \"/key/info\",\n        # user\n        \"/user/new\",\n        \"/user/update\",\n        \"/user/delete\",\n        \"/user/info\",\n        # team\n        \"/team/new\",\n        \"/team/update\",\n        \"/team/delete\",\n        \"/team/list\",\n        \"/team/info\",\n        \"/team/block\",\n        \"/team/unblock\",\n        # model\n        \"/model/new\",\n        \"/model/update\",\n        \"/model/delete\",\n        \"/model/info\",\n    ]\n\n    spend_tracking_routes: List = [\n        # spend\n        \"/spend/keys\",\n        \"/spend/users\",\n        \"/spend/tags\",\n        \"/spend/calculate\",\n        \"/spend/logs\",\n    ]\n\n    global_spend_tracking_routes: List = [\n        # global spend\n        \"/global/spend/logs\",\n        \"/global/spend\",\n        \"/global/spend/keys\",\n        \"/global/spend/teams\",\n        \"/global/spend/end_users\",\n        \"/global/spend/models\",\n        \"/global/predict/spend/logs\",\n        \"/global/spend/report\",\n    ]\n\n    public_routes: List = [\n        \"/routes\",\n        \"/\",\n        \"/health/liveliness\",\n        \"/health/liveness\",\n        \"/health/readiness\",\n        \"/test\",\n        \"/config/yaml\",\n        \"/metrics\",\n    ]\n\n    internal_user_routes: List = (\n        [\n            \"/key/generate\",\n            \"/key/update\",\n            \"/key/delete\",\n            \"/key/info\",\n        ]\n        + spend_tracking_routes\n        + sso_only_routes\n    )\n\n\n# class LiteLLMAllowedRoutes(LiteLLMBase):\n#     \"\"\"\n#     Defines allowed routes based on key type.\n\n#     Types = [\"admin\", \"team\", \"user\", \"unmapped\"]\n#     \"\"\"\n\n#     admin_allowed_routes: List[\n#         Literal[\"openai_routes\", \"info_routes\", \"management_routes\", \"spend_tracking_routes\", \"global_spend_tracking_routes\"]\n#     ] = [\"management_routes\"]\n\n\nclass LiteLLM_JWTAuth(LiteLLMBase):\n    \"\"\"\n    A class to define the roles and permissions for a LiteLLM Proxy w/ JWT Auth.\n\n    Attributes:\n    - admin_jwt_scope: The JWT scope required for proxy admin roles.\n    - admin_allowed_routes: list of allowed routes for proxy admin roles.\n    - team_jwt_scope: The JWT scope required for proxy team roles.\n    - team_id_jwt_field: The field in the JWT token that stores the team ID. Default - `client_id`.\n    - team_allowed_routes: list of allowed routes for proxy team roles.\n    - user_id_jwt_field: The field in the JWT token that stores the user id (maps to `LiteLLMUserTable`). Use this for internal employees.\n    - end_user_id_jwt_field: The field in the JWT token that stores the end-user ID (maps to `LiteLLMEndUserTable`). Turn this off by setting to `None`. Enables end-user cost tracking. Use this for external customers.\n    - public_key_ttl: Default - 600s. TTL for caching public JWT keys.\n\n    See `auth_checks.py` for the specific routes\n    \"\"\"\n\n    admin_jwt_scope: str = \"litellm_proxy_admin\"\n    admin_allowed_routes: List[\n        Literal[\n            \"openai_routes\",\n            \"info_routes\",\n            \"management_routes\",\n            \"spend_tracking_routes\",\n            \"global_spend_tracking_routes\",\n        ]\n    ] = [\n        \"management_routes\",\n        \"spend_tracking_routes\",\n        \"global_spend_tracking_routes\",\n        \"info_routes\",\n    ]\n    team_id_jwt_field: Optional[str] = None\n    team_allowed_routes: List[\n        Literal[\"openai_routes\", \"info_routes\", \"management_routes\"]\n    ] = [\"openai_routes\", \"info_routes\"]\n    team_id_default: Optional[str] = Field(\n        default=None,\n        description=\"If no team_id given, default permissions/spend-tracking to this team.s\",\n    )\n    org_id_jwt_field: Optional[str] = None\n    user_id_jwt_field: Optional[str] = None\n    user_id_upsert: bool = Field(\n        default=False, description=\"If user doesn't exist, upsert them into the db.\"\n    )\n    end_user_id_jwt_field: Optional[str] = None\n    public_key_ttl: float = 600\n\n    def __init__(self, **kwargs: Any) -> None:\n        # get the attribute names for this Pydantic model\n        allowed_keys = self.__annotations__.keys()\n\n        invalid_keys = set(kwargs.keys()) - allowed_keys\n\n        if invalid_keys:\n            raise ValueError(\n                f\"Invalid arguments provided: {', '.join(invalid_keys)}. Allowed arguments are: {', '.join(allowed_keys)}.\"\n            )\n\n        super().__init__(**kwargs)\n\n\nclass LiteLLMPromptInjectionParams(LiteLLMBase):\n    heuristics_check: bool = False\n    vector_db_check: bool = False\n    llm_api_check: bool = False\n    llm_api_name: Optional[str] = None\n    llm_api_system_prompt: Optional[str] = None\n    llm_api_fail_call_string: Optional[str] = None\n    reject_as_response: Optional[bool] = Field(\n        default=False,\n        description=\"Return rejected request error message as a string to the user. Default behaviour is to raise an exception.\",\n    )\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def check_llm_api_params(cls, values):\n        llm_api_check = values.get(\"llm_api_check\")\n        if llm_api_check is True:\n            if \"llm_api_name\" not in values or not values[\"llm_api_name\"]:\n                raise ValueError(\n                    \"If llm_api_check is set to True, llm_api_name must be provided\"\n                )\n            if (\n                \"llm_api_system_prompt\" not in values\n                or not values[\"llm_api_system_prompt\"]\n            ):\n                raise ValueError(\n                    \"If llm_api_check is set to True, llm_api_system_prompt must be provided\"\n                )\n            if (\n                \"llm_api_fail_call_string\" not in values\n                or not values[\"llm_api_fail_call_string\"]\n            ):\n                raise ValueError(\n                    \"If llm_api_check is set to True, llm_api_fail_call_string must be provided\"\n                )\n        return values\n\n\n######### Request Class Definition ######\nclass ProxyChatCompletionRequest(LiteLLMBase):\n    model: str\n    messages: List[Dict[str, str]]\n    temperature: Optional[float] = None\n    top_p: Optional[float] = None\n    n: Optional[int] = None\n    stream: Optional[bool] = None\n    stop: Optional[List[str]] = None\n    max_tokens: Optional[int] = None\n    presence_penalty: Optional[float] = None\n    frequency_penalty: Optional[float] = None\n    logit_bias: Optional[Dict[str, float]] = None\n    user: Optional[str] = None\n    response_format: Optional[Dict[str, str]] = None\n    seed: Optional[int] = None\n    tools: Optional[List[str]] = None\n    tool_choice: Optional[str] = None\n    functions: Optional[List[str]] = None  # soon to be deprecated\n    function_call: Optional[str] = None  # soon to be deprecated\n\n    # Optional LiteLLM params\n    caching: Optional[bool] = None\n    api_base: Optional[str] = None\n    api_version: Optional[str] = None\n    api_key: Optional[str] = None\n    num_retries: Optional[int] = None\n    context_window_fallback_dict: Optional[Dict[str, str]] = None\n    fallbacks: Optional[List[str]] = None\n    metadata: Optional[Dict[str, str]] = {}\n    deployment_id: Optional[str] = None\n    request_timeout: Optional[int] = None\n\n    model_config = ConfigDict(\n        extra=\"allow\"\n    )  # allow params not defined here, these fall in litellm.completion(**kwargs)\n\n\nclass ModelInfoDelete(LiteLLMBase):\n    id: str\n\n\nclass ModelInfo(LiteLLMBase):\n    id: Optional[str]\n    mode: Optional[Literal[\"embedding\", \"chat\", \"completion\"]]\n    input_cost_per_token: Optional[float] = 0.0\n    output_cost_per_token: Optional[float] = 0.0\n    max_tokens: Optional[int] = 2048  # assume 2048 if not set\n\n    # for azure models we need users to specify the base model, one azure you can call deployments - azure/my-random-model\n    # we look up the base model in model_prices_and_context_window.json\n    base_model: Optional[\n        Literal[\n            \"gpt-4-1106-preview\",\n            \"gpt-4-32k\",\n            \"gpt-4\",\n            \"gpt-3.5-turbo-16k\",\n            \"gpt-3.5-turbo\",\n            \"text-embedding-ada-002\",\n        ]\n    ]\n\n    model_config = ConfigDict(protected_namespaces=(), extra=\"allow\")\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def set_model_info(cls, values):\n        if values.get(\"id\") is None:\n            values.update({\"id\": str(uuid.uuid4())})\n        if values.get(\"mode\") is None:\n            values.update({\"mode\": None})\n        if values.get(\"input_cost_per_token\") is None:\n            values.update({\"input_cost_per_token\": None})\n        if values.get(\"output_cost_per_token\") is None:\n            values.update({\"output_cost_per_token\": None})\n        if values.get(\"max_tokens\") is None:\n            values.update({\"max_tokens\": None})\n        if values.get(\"base_model\") is None:\n            values.update({\"base_model\": None})\n        return values\n\n\nclass ProviderInfo(LiteLLMBase):\n    name: str\n    fields: List[ProviderField]\n\n\nclass BlockUsers(LiteLLMBase):\n    user_ids: List[str]  # required\n\n\nclass ModelParams(LiteLLMBase):\n    model_name: str\n    litellm_params: dict\n    model_info: ModelInfo\n\n    model_config = ConfigDict(protected_namespaces=())\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def set_model_info(cls, values):\n        if values.get(\"model_info\") is None:\n            values.update({\"model_info\": ModelInfo()})\n        return values\n\n\nclass GenerateRequestBase(LiteLLMBase):\n    \"\"\"\n    Overlapping schema between key and user generate/update requests\n    \"\"\"\n\n    models: Optional[list] = []\n    spend: Optional[float] = 0\n    max_budget: Optional[float] = None\n    user_id: Optional[str] = None\n    team_id: Optional[str] = None\n    max_parallel_requests: Optional[int] = None\n    metadata: Optional[dict] = {}\n    tpm_limit: Optional[int] = None\n    rpm_limit: Optional[int] = None\n    budget_duration: Optional[str] = None\n    allowed_cache_controls: Optional[list] = []\n    soft_budget: Optional[float] = None\n\n\nclass GenerateKeyRequest(GenerateRequestBase):\n    key_alias: Optional[str] = None\n    duration: Optional[str] = None\n    aliases: Optional[dict] = {}\n    config: Optional[dict] = {}\n    permissions: Optional[dict] = {}\n    model_max_budget: Optional[dict] = (\n        {}\n    )  # {\"gpt-4\": 5.0, \"gpt-3.5-turbo\": 5.0}, defaults to {}\n\n    model_config = ConfigDict(protected_namespaces=())\n    send_invite_email: Optional[bool] = None\n\n\nclass GenerateKeyResponse(GenerateKeyRequest):\n    key: str\n    key_name: Optional[str] = None\n    expires: Optional[datetime]\n    user_id: Optional[str] = None\n    token_id: Optional[str] = None\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def set_model_info(cls, values):\n        if values.get(\"token\") is not None:\n            values.update({\"key\": values.get(\"token\")})\n        dict_fields = [\n            \"metadata\",\n            \"aliases\",\n            \"config\",\n            \"permissions\",\n            \"model_max_budget\",\n        ]\n        for field in dict_fields:\n            value = values.get(field)\n            if value is not None and isinstance(value, str):\n                try:\n                    values[field] = json.loads(value)\n                except json.JSONDecodeError:\n                    raise ValueError(f\"Field {field} should be a valid dictionary\")\n\n        return values\n\n\nclass UpdateKeyRequest(GenerateKeyRequest):\n    # Note: the defaults of all Params here MUST BE NONE\n    # else they will get overwritten\n    key: str\n    duration: Optional[str] = None\n    spend: Optional[float] = None\n    metadata: Optional[dict] = None\n\n\nclass KeyRequest(LiteLLMBase):\n    keys: List[str]\n\n\nclass LiteLLM_ModelTable(LiteLLMBase):\n    model_aliases: Optional[str] = None  # json dump the dict\n    created_by: str\n    updated_by: str\n\n    model_config = ConfigDict(protected_namespaces=())\n\n\nclass NewUserRequest(GenerateKeyRequest):\n    max_budget: Optional[float] = None\n    user_email: Optional[str] = None\n    user_role: Optional[\n        Literal[\n            LitellmUserRoles.PROXY_ADMIN,\n            LitellmUserRoles.PROXY_ADMIN_VIEW_ONLY,\n            LitellmUserRoles.INTERNAL_USER,\n            LitellmUserRoles.INTERNAL_USER_VIEW_ONLY,\n            LitellmUserRoles.TEAM,\n            LitellmUserRoles.CUSTOMER,\n        ]\n    ] = None\n    teams: Optional[list] = None\n    organization_id: Optional[str] = None\n    auto_create_key: bool = (\n        True  # flag used for returning a key as part of the /user/new response\n    )\n    send_invite_email: Optional[bool] = None\n\n\nclass NewUserResponse(GenerateKeyResponse):\n    max_budget: Optional[float] = None\n    user_email: Optional[str] = None\n    user_role: Optional[\n        Literal[\n            LitellmUserRoles.PROXY_ADMIN,\n            LitellmUserRoles.PROXY_ADMIN_VIEW_ONLY,\n            LitellmUserRoles.INTERNAL_USER,\n            LitellmUserRoles.INTERNAL_USER_VIEW_ONLY,\n            LitellmUserRoles.TEAM,\n            LitellmUserRoles.CUSTOMER,\n        ]\n    ] = None\n    teams: Optional[list] = None\n    organization_id: Optional[str] = None\n\n\nclass UpdateUserRequest(GenerateRequestBase):\n    # Note: the defaults of all Params here MUST BE NONE\n    # else they will get overwritten\n    user_id: Optional[str] = None\n    password: Optional[str] = None\n    user_email: Optional[str] = None\n    spend: Optional[float] = None\n    metadata: Optional[dict] = None\n    user_role: Optional[\n        Literal[\n            LitellmUserRoles.PROXY_ADMIN,\n            LitellmUserRoles.PROXY_ADMIN_VIEW_ONLY,\n            LitellmUserRoles.INTERNAL_USER,\n            LitellmUserRoles.INTERNAL_USER_VIEW_ONLY,\n            LitellmUserRoles.TEAM,\n            LitellmUserRoles.CUSTOMER,\n        ]\n    ] = None\n    max_budget: Optional[float] = None\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def check_user_info(cls, values):\n        if values.get(\"user_id\") is None and values.get(\"user_email\") is None:\n            raise ValueError(\"Either user id or user email must be provided\")\n        return values\n\n\nclass DeleteUserRequest(LiteLLMBase):\n    user_ids: List[str]  # required\n\n\nclass NewCustomerRequest(LiteLLMBase):\n    \"\"\"\n    Create a new customer, allocate a budget to them\n    \"\"\"\n\n    user_id: str\n    alias: Optional[str] = None  # human-friendly alias\n    blocked: bool = False  # allow/disallow requests for this end-user\n    max_budget: Optional[float] = None\n    budget_id: Optional[str] = None  # give either a budget_id or max_budget\n    allowed_model_region: Optional[Literal[\"eu\"]] = (\n        None  # require all user requests to use models in this specific region\n    )\n    default_model: Optional[str] = (\n        None  # if no equivalent model in allowed region - default all requests to this model\n    )\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def check_user_info(cls, values):\n        if values.get(\"max_budget\") is not None and values.get(\"budget_id\") is not None:\n            raise ValueError(\"Set either 'max_budget' or 'budget_id', not both.\")\n\n        return values\n\n\nclass UpdateCustomerRequest(LiteLLMBase):\n    \"\"\"\n    Update a Customer, use this to update customer budgets etc\n\n    \"\"\"\n\n    user_id: str\n    alias: Optional[str] = None  # human-friendly alias\n    blocked: bool = False  # allow/disallow requests for this end-user\n    max_budget: Optional[float] = None\n    budget_id: Optional[str] = None  # give either a budget_id or max_budget\n    allowed_model_region: Optional[Literal[\"eu\"]] = (\n        None  # require all user requests to use models in this specific region\n    )\n    default_model: Optional[str] = (\n        None  # if no equivalent model in allowed region - default all requests to this model\n    )\n\n\nclass DeleteCustomerRequest(LiteLLMBase):\n    \"\"\"\n    Delete multiple Customers\n    \"\"\"\n\n    user_ids: List[str]\n\n\nclass Member(LiteLLMBase):\n    role: Literal[\"admin\", \"user\"]\n    user_id: Optional[str] = None\n    user_email: Optional[str] = None\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def check_user_info(cls, values):\n        if not isinstance(values, dict):\n            raise ValueError(\"input needs to be a dictionary\")\n        if values.get(\"user_id\") is None and values.get(\"user_email\") is None:\n            raise ValueError(\"Either user id or user email must be provided\")\n        return values\n\n\nclass TeamBase(LiteLLMBase):\n    team_alias: Optional[str] = None\n    team_id: Optional[str] = None\n    organization_id: Optional[str] = None\n    admins: list = []\n    members: list = []\n    members_with_roles: List[Member] = []\n    metadata: Optional[dict] = None\n    tpm_limit: Optional[int] = None\n    rpm_limit: Optional[int] = None\n\n    # Budget fields\n    max_budget: Optional[float] = None\n    budget_duration: Optional[str] = None\n\n    models: list = []\n    blocked: bool = False\n\n\nclass NewTeamRequest(TeamBase):\n    model_aliases: Optional[dict] = None\n\n    model_config = ConfigDict(protected_namespaces=())\n\n\nclass GlobalEndUsersSpend(LiteLLMBase):\n    api_key: Optional[str] = None\n    startTime: Optional[datetime] = None\n    endTime: Optional[datetime] = None\n\n\nclass TeamMemberAddRequest(LiteLLMBase):\n    team_id: str\n    member: Union[List[Member], Member]\n    max_budget_in_team: Optional[float] = None  # Users max budget within the team\n\n    def __init__(self, **data):\n        member_data = data.get(\"member\")\n        if isinstance(member_data, list):\n            # If member is a list of dictionaries, convert each dictionary to a Member object\n            members = [Member(**item) for item in member_data]\n            # Replace member_data with the list of Member objects\n            data[\"member\"] = members\n        elif isinstance(member_data, dict):\n            # If member is a dictionary, convert it to a single Member object\n            member = Member(**member_data)\n            # Replace member_data with the single Member object\n            data[\"member\"] = member\n        # Call the superclass __init__ method to initialize the object\n        super().__init__(**data)\n\n\nclass TeamMemberDeleteRequest(LiteLLMBase):\n    team_id: str\n    user_id: Optional[str] = None\n    user_email: Optional[str] = None\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def check_user_info(cls, values):\n        if values.get(\"user_id\") is None and values.get(\"user_email\") is None:\n            raise ValueError(\"Either user id or user email must be provided\")\n        return values\n\n\nclass UpdateTeamRequest(LiteLLMBase):\n    \"\"\"\n    UpdateTeamRequest, used by /team/update when you need to update a team\n\n    team_id: str\n    team_alias: Optional[str] = None\n    organization_id: Optional[str] = None\n    metadata: Optional[dict] = None\n    tpm_limit: Optional[int] = None\n    rpm_limit: Optional[int] = None\n    max_budget: Optional[float] = None\n    models: Optional[list] = None\n    blocked: Optional[bool] = None\n    budget_duration: Optional[str] = None\n    \"\"\"\n\n    team_id: str  # required\n    team_alias: Optional[str] = None\n    organization_id: Optional[str] = None\n    metadata: Optional[dict] = None\n    tpm_limit: Optional[int] = None\n    rpm_limit: Optional[int] = None\n    max_budget: Optional[float] = None\n    models: Optional[list] = None\n    blocked: Optional[bool] = None\n    budget_duration: Optional[str] = None\n\n\nclass ResetTeamBudgetRequest(LiteLLMBase):\n    \"\"\"\n    internal type used to reset the budget on a team\n    used by reset_budget()\n\n    team_id: str\n    spend: float\n    budget_reset_at: datetime\n    \"\"\"\n\n    team_id: str\n    spend: float\n    budget_reset_at: datetime\n    updated_at: datetime\n\n\nclass DeleteTeamRequest(LiteLLMBase):\n    team_ids: List[str]  # required\n\n\nclass BlockTeamRequest(LiteLLMBase):\n    team_id: str  # required\n\n\nclass AddTeamCallback(LiteLLMBase):\n    callback_name: str\n    callback_type: Literal[\"success\", \"failure\", \"success_and_failure\"]\n    # for now - only supported for langfuse\n    callback_vars: Dict[\n        Literal[\"langfuse_public_key\", \"langfuse_secret_key\", \"langfuse_host\"], str\n    ]\n\n\nclass TeamCallbackMetadata(LiteLLMBase):\n    success_callback: Optional[List[str]] = []\n    failure_callback: Optional[List[str]] = []\n    # for now - only supported for langfuse\n    callback_vars: Optional[\n        Dict[\n            Literal[\"langfuse_public_key\", \"langfuse_secret_key\", \"langfuse_host\"], str\n        ]\n    ] = {}\n\n\nclass LiteLLM_TeamTable(TeamBase):\n    spend: Optional[float] = None\n    max_parallel_requests: Optional[int] = None\n    budget_duration: Optional[str] = None\n    budget_reset_at: Optional[datetime] = None\n    model_id: Optional[int] = None\n    last_refreshed_at: Optional[float] = None\n\n    model_config = ConfigDict(protected_namespaces=())\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def set_model_info(cls, values):\n        dict_fields = [\n            \"metadata\",\n            \"aliases\",\n            \"config\",\n            \"permissions\",\n            \"model_max_budget\",\n            \"model_aliases\",\n        ]\n        for field in dict_fields:\n            value = values.get(field)\n            if value is not None and isinstance(value, str):\n                try:\n                    values[field] = json.loads(value)\n                except json.JSONDecodeError:\n                    raise ValueError(f\"Field {field} should be a valid dictionary\")\n\n        return values\n\n\nclass TeamRequest(LiteLLMBase):\n    teams: List[str]\n\n\nclass LiteLLM_BudgetTable(LiteLLMBase):\n    \"\"\"Represents user-controllable params for a LiteLLM_BudgetTable record\"\"\"\n\n    soft_budget: Optional[float] = None\n    max_budget: Optional[float] = None\n    max_parallel_requests: Optional[int] = None\n    tpm_limit: Optional[int] = None\n    rpm_limit: Optional[int] = None\n    model_max_budget: Optional[dict] = None\n    budget_duration: Optional[str] = None\n\n    model_config = ConfigDict(protected_namespaces=())\n\n\nclass LiteLLM_TeamMemberTable(LiteLLM_BudgetTable):\n    \"\"\"\n    Used to track spend of a user_id within a team_id\n    \"\"\"\n\n    spend: Optional[float] = None\n    user_id: Optional[str] = None\n    team_id: Optional[str] = None\n    budget_id: Optional[str] = None\n\n    model_config = ConfigDict(protected_namespaces=())\n\n\nclass NewOrganizationRequest(LiteLLM_BudgetTable):\n    organization_id: Optional[str] = None\n    organization_alias: str\n    models: List = []\n    budget_id: Optional[str] = None\n\n\nclass LiteLLM_OrganizationTable(LiteLLMBase):\n    \"\"\"Represents user-controllable params for a LiteLLM_OrganizationTable record\"\"\"\n\n    organization_id: Optional[str] = None\n    organization_alias: Optional[str] = None\n    budget_id: str\n    metadata: Optional[dict] = None\n    models: List[str]\n    created_by: str\n    updated_by: str\n\n\nclass NewOrganizationResponse(LiteLLM_OrganizationTable):\n    organization_id: str\n    created_at: datetime\n    updated_at: datetime\n\n\nclass OrganizationRequest(LiteLLMBase):\n    organizations: List[str]\n\n\nclass BudgetNew(LiteLLMBase):\n    budget_id: str = Field(default=None, description=\"The unique budget id.\")\n    max_budget: Optional[float] = Field(\n        default=None,\n        description=\"Requests will fail if this budget (in USD) is exceeded.\",\n    )\n    soft_budget: Optional[float] = Field(\n        default=None,\n        description=\"Requests will NOT fail if this is exceeded. Will fire alerting though.\",\n    )\n    max_parallel_requests: Optional[int] = Field(\n        default=None, description=\"Max concurrent requests allowed for this budget id.\"\n    )\n    tpm_limit: Optional[int] = Field(\n        default=None, description=\"Max tokens per minute, allowed for this budget id.\"\n    )\n    rpm_limit: Optional[int] = Field(\n        default=None, description=\"Max requests per minute, allowed for this budget id.\"\n    )\n    budget_duration: Optional[str] = Field(\n        default=None,\n        description=\"Max duration budget should be set for (e.g. '1hr', '1d', '28d')\",\n    )\n\n\nclass BudgetRequest(LiteLLMBase):\n    budgets: List[str]\n\n\nclass BudgetDeleteRequest(LiteLLMBase):\n    id: str\n\n\nclass KeyManagementSystem(enum.Enum):\n    GOOGLE_KMS = \"google_kms\"\n    AZURE_KEY_VAULT = \"azure_key_vault\"\n    AWS_SECRET_MANAGER = \"aws_secret_manager\"\n    LOCAL = \"local\"\n    AWS_KMS = \"aws_kms\"\n\n\nclass KeyManagementSettings(LiteLLMBase):\n    hosted_keys: List\n\n\nclass TeamDefaultSettings(LiteLLMBase):\n    team_id: str\n\n    model_config = ConfigDict(\n        extra=\"allow\"\n    )  # allow params not defined here, these fall in litellm.completion(**kwargs)\n\n\nclass DynamoDBArgs(LiteLLMBase):\n    billing_mode: Literal[\"PROVISIONED_THROUGHPUT\", \"PAY_PER_REQUEST\"]\n    read_capacity_units: Optional[int] = None\n    write_capacity_units: Optional[int] = None\n    ssl_verify: Optional[bool] = None\n    region_name: str\n    user_table_name: str = \"LiteLLM_UserTable\"\n    key_table_name: str = \"LiteLLM_VerificationToken\"\n    config_table_name: str = \"LiteLLM_Config\"\n    spend_table_name: str = \"LiteLLM_SpendLogs\"\n    aws_role_name: Optional[str] = None\n    aws_session_name: Optional[str] = None\n    aws_web_identity_token: Optional[str] = None\n    aws_provider_id: Optional[str] = None\n    aws_policy_arns: Optional[List[str]] = None\n    aws_policy: Optional[str] = None\n    aws_duration_seconds: Optional[int] = None\n    assume_role_aws_role_name: Optional[str] = None\n    assume_role_aws_session_name: Optional[str] = None\n\n\nclass ConfigFieldUpdate(LiteLLMBase):\n    field_name: str\n    field_value: Any\n    config_type: Literal[\"general_settings\"]\n\n\nclass ConfigFieldDelete(LiteLLMBase):\n    config_type: Literal[\"general_settings\"]\n    field_name: str\n\n\nclass ConfigList(LiteLLMBase):\n    field_name: str\n    field_type: str\n    field_description: str\n    field_value: Any\n    stored_in_db: Optional[bool]\n    field_default_value: Any\n    premium_field: bool = False\n\n\nclass ConfigGeneralSettings(LiteLLMBase):\n    \"\"\"\n    Documents all the fields supported by `general_settings` in config.yaml\n    \"\"\"\n\n    completion_model: Optional[str] = Field(\n        None, description=\"proxy level default model for all chat completion calls\"\n    )\n    key_management_system: Optional[KeyManagementSystem] = Field(\n        None, description=\"key manager to load keys from / decrypt keys with\"\n    )\n    use_google_kms: Optional[bool] = Field(\n        None, description=\"decrypt keys with google kms\"\n    )\n    use_azure_key_vault: Optional[bool] = Field(\n        None, description=\"load keys from azure key vault\"\n    )\n    master_key: Optional[str] = Field(\n        None, description=\"require a key for all calls to proxy\"\n    )\n    database_url: Optional[str] = Field(\n        None,\n        description=\"connect to a postgres db - needed for generating temporary keys + tracking spend / key\",\n    )\n    database_connection_pool_limit: Optional[int] = Field(\n        100,\n        description=\"default connection pool for prisma client connecting to postgres db\",\n    )\n    database_connection_timeout: Optional[float] = Field(\n        60, description=\"default timeout for a connection to the database\"\n    )\n    database_type: Optional[Literal[\"dynamo_db\"]] = Field(\n        None, description=\"to use dynamodb instead of postgres db\"\n    )\n    database_args: Optional[DynamoDBArgs] = Field(\n        None,\n        description=\"custom args for instantiating dynamodb client - e.g. billing provision\",\n    )\n    otel: Optional[bool] = Field(\n        None,\n        description=\"[BETA] OpenTelemetry support - this might change, use with caution.\",\n    )\n    custom_auth: Optional[str] = Field(\n        None,\n        description=\"override user_api_key_auth with your own auth script - https://docs.litellm.ai/docs/proxy/virtual_keys#custom-auth\",\n    )\n    max_parallel_requests: Optional[int] = Field(\n        None,\n        description=\"maximum parallel requests for each api key\",\n    )\n    global_max_parallel_requests: Optional[int] = Field(\n        None, description=\"global max parallel requests to allow for a proxy instance.\"\n    )\n    max_request_size_mb: Optional[int] = Field(\n        None,\n        description=\"max request size in MB, if a request is larger than this size it will be rejected\",\n    )\n    max_response_size_mb: Optional[int] = Field(\n        None,\n        description=\"max response size in MB, if a response is larger than this size it will be rejected\",\n    )\n    infer_model_from_keys: Optional[bool] = Field(\n        None,\n        description=\"for `/models` endpoint, infers available model based on environment keys (e.g. OPENAI_API_KEY)\",\n    )\n    background_health_checks: Optional[bool] = Field(\n        None, description=\"run health checks in background\"\n    )\n    health_check_interval: int = Field(\n        300, description=\"background health check interval in seconds\"\n    )\n    alerting: Optional[List] = Field(\n        None,\n        description=\"List of alerting integrations. Today, just slack - `alerting: ['slack']`\",\n    )\n    alert_types: Optional[List[AlertType]] = Field(\n        None,\n        description=\"List of alerting types. By default it is all alerts\",\n    )\n    alert_to_webhook_url: Optional[Dict] = Field(\n        None,\n        description=\"Mapping of alert type to webhook url. e.g. `alert_to_webhook_url: {'budget_alerts': 'https://hooks.slack.com/services/T00000000/B00000000/XXXXXXXXXXXXXXXXXXXXXXXX'}`\",\n    )\n    alerting_args: Optional[Dict] = Field(\n        None, description=\"Controllable params for slack alerting - e.g. ttl in cache.\"\n    )\n    alerting_threshold: Optional[int] = Field(\n        None,\n        description=\"sends alerts if requests hang for 5min+\",\n    )\n    ui_access_mode: Optional[Literal[\"admin_only\", \"all\"]] = Field(\n        \"all\", description=\"Control access to the Proxy UI\"\n    )\n    allowed_routes: Optional[List] = Field(\n        None, description=\"Proxy API Endpoints you want users to be able to access\"\n    )\n    enable_public_model_hub: bool = Field(\n        default=False,\n        description=\"Public model hub for users to see what models they have access to, supported openai params, etc.\",\n    )\n\n\nclass ConfigYAML(LiteLLMBase):\n    \"\"\"\n    Documents all the fields supported by the config.yaml\n    \"\"\"\n\n    environment_variables: Optional[dict] = Field(\n        None,\n        description=\"Object to pass in additional environment variables via POST request\",\n    )\n    model_list: Optional[List[ModelParams]] = Field(\n        None,\n        description=\"List of supported models on the server, with model-specific configs\",\n    )\n    litellm_settings: Optional[dict] = Field(\n        None,\n        description=\"litellm Module settings. See __init__.py for all, example litellm.drop_params=True, litellm.set_verbose=True, litellm.api_base, litellm.cache\",\n    )\n    general_settings: Optional[ConfigGeneralSettings] = None\n    router_settings: Optional[UpdateRouterConfig] = Field(\n        None,\n        description=\"litellm router object settings. See router.py __init__ for all, example router.num_retries=5, router.timeout=5, router.max_retries=5, router.retry_after=5\",\n    )\n\n    model_config = ConfigDict(protected_namespaces=())\n\n\nclass LiteLLM_VerificationToken(LiteLLMBase):\n    token: Optional[str] = None\n    key_name: Optional[str] = None\n    key_alias: Optional[str] = None\n    spend: float = 0.0\n    max_budget: Optional[float] = None\n    expires: Optional[str] = None\n    models: List = []\n    aliases: Dict = {}\n    config: Dict = {}\n    user_id: Optional[str] = None\n    team_id: Optional[str] = None\n    max_parallel_requests: Optional[int] = None\n    metadata: Dict = {}\n    tpm_limit: Optional[int] = None\n    rpm_limit: Optional[int] = None\n    budget_duration: Optional[str] = None\n    budget_reset_at: Optional[datetime] = None\n    allowed_cache_controls: Optional[list] = []\n    permissions: Dict = {}\n    model_spend: Dict = {}\n    model_max_budget: Dict = {}\n    soft_budget_cooldown: bool = False\n    litellm_budget_table: Optional[dict] = None\n\n    org_id: Optional[str] = None  # org id for a given key\n\n    model_config = ConfigDict(protected_namespaces=())\n\n\nclass LiteLLM_VerificationTokenView(LiteLLM_VerificationToken):\n    \"\"\"\n    Combined view of litellm verification token + litellm team table (select values)\n    \"\"\"\n\n    team_spend: Optional[float] = None\n    team_alias: Optional[str] = None\n    team_tpm_limit: Optional[int] = None\n    team_rpm_limit: Optional[int] = None\n    team_max_budget: Optional[float] = None\n    team_models: List = []\n    team_blocked: bool = False\n    soft_budget: Optional[float] = None\n    team_model_aliases: Optional[Dict] = None\n    team_member_spend: Optional[float] = None\n    team_metadata: Optional[Dict] = None\n\n    # End User Params\n    end_user_id: Optional[str] = None\n    end_user_tpm_limit: Optional[int] = None\n    end_user_rpm_limit: Optional[int] = None\n    end_user_max_budget: Optional[float] = None\n\n    # Time stamps\n    last_refreshed_at: Optional[float] = None  # last time joint view was pulled from db\n\n\nclass UserAPIKeyAuth(\n    LiteLLM_VerificationTokenView\n):  # the expected response object for user api key auth\n    \"\"\"\n    Return the row in the db\n    \"\"\"\n\n    api_key: Optional[str] = None\n    user_role: Optional[\n        Literal[\n            LitellmUserRoles.PROXY_ADMIN,\n            LitellmUserRoles.PROXY_ADMIN_VIEW_ONLY,\n            LitellmUserRoles.INTERNAL_USER,\n            LitellmUserRoles.INTERNAL_USER_VIEW_ONLY,\n            LitellmUserRoles.TEAM,\n            LitellmUserRoles.CUSTOMER,\n        ]\n    ] = None\n    allowed_model_region: Optional[Literal[\"eu\"]] = None\n    parent_otel_span: Optional[Span] = None\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def check_api_key(cls, values):\n        if values.get(\"api_key\") is not None:\n            values.update({\"token\": hash_token(values.get(\"api_key\"))})\n            if isinstance(values.get(\"api_key\"), str) and values.get(\n                \"api_key\"\n            ).startswith(\"sk-\"):\n                values.update({\"api_key\": hash_token(values.get(\"api_key\"))})\n        return values\n\n    class Config:\n        arbitrary_types_allowed = True\n\n\nclass LiteLLM_Config(LiteLLMBase):\n    param_name: str\n    param_value: Dict\n\n\nclass LiteLLM_UserTable(LiteLLMBase):\n    user_id: str\n    max_budget: Optional[float]\n    spend: float = 0.0\n    model_max_budget: Optional[Dict] = {}\n    model_spend: Optional[Dict] = {}\n    user_email: Optional[str]\n    models: list = []\n    tpm_limit: Optional[int] = None\n    rpm_limit: Optional[int] = None\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def set_model_info(cls, values):\n        if values.get(\"spend\") is None:\n            values.update({\"spend\": 0.0})\n        if values.get(\"models\") is None:\n            values.update({\"models\": []})\n        return values\n\n    model_config = ConfigDict(protected_namespaces=())\n\n\nclass LiteLLM_EndUserTable(LiteLLMBase):\n    user_id: str\n    blocked: bool\n    alias: Optional[str] = None\n    spend: float = 0.0\n    allowed_model_region: Optional[Literal[\"eu\"]] = None\n    default_model: Optional[str] = None\n    litellm_budget_table: Optional[LiteLLM_BudgetTable] = None\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def set_model_info(cls, values):\n        if values.get(\"spend\") is None:\n            values.update({\"spend\": 0.0})\n        return values\n\n    model_config = ConfigDict(protected_namespaces=())\n\n\nclass LiteLLM_SpendLogs(LiteLLMBase):\n    request_id: str\n    api_key: str\n    model: Optional[str] = \"\"\n    api_base: Optional[str] = \"\"\n    call_type: str\n    spend: Optional[float] = 0.0\n    total_tokens: Optional[int] = 0\n    prompt_tokens: Optional[int] = 0\n    completion_tokens: Optional[int] = 0\n    startTime: Union[str, datetime, None]\n    endTime: Union[str, datetime, None]\n    user: Optional[str] = \"\"\n    metadata: Optional[Json] = {}\n    cache_hit: Optional[str] = \"False\"\n    cache_key: Optional[str] = None\n    request_tags: Optional[Json] = None\n    requester_ip_address: Optional[str] = None\n\n\nclass LiteLLM_ErrorLogs(LiteLLMBase):\n    request_id: Optional[str] = str(uuid.uuid4())\n    api_base: Optional[str] = \"\"\n    model_group: Optional[str] = \"\"\n    litellm_model_name: Optional[str] = \"\"\n    model_id: Optional[str] = \"\"\n    request_kwargs: Optional[dict] = {}\n    exception_type: Optional[str] = \"\"\n    status_code: Optional[str] = \"\"\n    exception_string: Optional[str] = \"\"\n    startTime: Union[str, datetime, None]\n    endTime: Union[str, datetime, None]\n\n\nclass LiteLLM_AuditLogs(LiteLLMBase):\n    id: str\n    updated_at: datetime\n    changed_by: str\n    changed_by_api_key: Optional[str] = None\n    action: Literal[\"created\", \"updated\", \"deleted\"]\n    table_name: Literal[\n        LitellmTableNames.TEAM_TABLE_NAME,\n        LitellmTableNames.USER_TABLE_NAME,\n        LitellmTableNames.KEY_TABLE_NAME,\n        LitellmTableNames.PROXY_MODEL_TABLE_NAME,\n    ]\n    object_id: str\n    before_value: Optional[Json] = None\n    updated_values: Optional[Json] = None\n\n\nclass LiteLLM_SpendLogs_ResponseObject(LiteLLMBase):\n    response: Optional[List[Union[LiteLLM_SpendLogs, Any]]] = None\n\n\nclass TokenCountRequest(LiteLLMBase):\n    model: str\n    prompt: Optional[str] = None\n    messages: Optional[List[dict]] = None\n\n\nclass TokenCountResponse(LiteLLMBase):\n    total_tokens: int\n    request_model: str\n    model_used: str\n    tokenizer_type: str\n\n\nclass CallInfo(LiteLLMBase):\n    \"\"\"Used for slack budget alerting\"\"\"\n\n    spend: float\n    max_budget: Optional[float] = None\n    token: Optional[str] = Field(default=None, description=\"Hashed value of that key\")\n    customer_id: Optional[str] = None\n    user_id: Optional[str] = None\n    team_id: Optional[str] = None\n    team_alias: Optional[str] = None\n    user_email: Optional[str] = None\n    key_alias: Optional[str] = None\n    projected_exceeded_date: Optional[str] = None\n    projected_spend: Optional[float] = None\n\n\nclass WebhookEvent(CallInfo):\n    event: Literal[\n        \"budget_crossed\",\n        \"threshold_crossed\",\n        \"projected_limit_exceeded\",\n        \"key_created\",\n        \"internal_user_created\",\n        \"spend_tracked\",\n    ]\n    event_group: Literal[\"internal_user\", \"key\", \"team\", \"proxy\", \"customer\"]\n    event_message: str  # human-readable description of event\n\n\nclass SpecialModelNames(enum.Enum):\n    all_team_models = \"all-team-models\"\n    all_proxy_models = \"all-proxy-models\"\n\n\nclass InvitationNew(LiteLLMBase):\n    user_id: str\n\n\nclass InvitationUpdate(LiteLLMBase):\n    invitation_id: str\n    is_accepted: bool\n\n\nclass InvitationDelete(LiteLLMBase):\n    invitation_id: str\n\n\nclass InvitationModel(LiteLLMBase):\n    id: str\n    user_id: str\n    is_accepted: bool\n    accepted_at: Optional[datetime]\n    expires_at: datetime\n    created_at: datetime\n    created_by: str\n    updated_at: datetime\n    updated_by: str\n\n\nclass InvitationClaim(LiteLLMBase):\n    invitation_link: str\n    user_id: str\n    password: str\n\n\nclass ConfigFieldInfo(LiteLLMBase):\n    field_name: str\n    field_value: Any\n\n\nclass CallbackOnUI(LiteLLMBase):\n    litellm_callback_name: str\n    litellm_callback_params: Optional[list]\n    ui_callback_name: str\n\n\nclass AllCallbacks(LiteLLMBase):\n    langfuse: CallbackOnUI = CallbackOnUI(\n        litellm_callback_name=\"langfuse\",\n        ui_callback_name=\"Langfuse\",\n        litellm_callback_params=[\n            \"LANGFUSE_PUBLIC_KEY\",\n            \"LANGFUSE_SECRET_KEY\",\n        ],\n    )\n\n    otel: CallbackOnUI = CallbackOnUI(\n        litellm_callback_name=\"otel\",\n        ui_callback_name=\"OpenTelemetry\",\n        litellm_callback_params=[\n            \"OTEL_EXPORTER\",\n            \"OTEL_ENDPOINT\",\n            \"OTEL_HEADERS\",\n        ],\n    )\n\n    s3: CallbackOnUI = CallbackOnUI(\n        litellm_callback_name=\"s3\",\n        ui_callback_name=\"s3 Bucket (AWS)\",\n        litellm_callback_params=[\n            \"AWS_ACCESS_KEY_ID\",\n            \"AWS_SECRET_ACCESS_KEY\",\n            \"AWS_REGION_NAME\",\n        ],\n    )\n\n    openmeter: CallbackOnUI = CallbackOnUI(\n        litellm_callback_name=\"openmeter\",\n        ui_callback_name=\"OpenMeter\",\n        litellm_callback_params=[\n            \"OPENMETER_API_ENDPOINT\",\n            \"OPENMETER_API_KEY\",\n        ],\n    )\n\n    custom_callback_api: CallbackOnUI = CallbackOnUI(\n        litellm_callback_name=\"custom_callback_api\",\n        litellm_callback_params=[\"GENERIC_LOGGER_ENDPOINT\"],\n        ui_callback_name=\"Custom Callback API\",\n    )\n\n    datadog: CallbackOnUI = CallbackOnUI(\n        litellm_callback_name=\"datadog\",\n        litellm_callback_params=[\"DD_API_KEY\", \"DD_SITE\"],\n        ui_callback_name=\"Datadog\",\n    )\n\n\nclass SpendLogsMetadata(TypedDict):\n    \"\"\"\n    Specific metadata k,v pairs logged to spendlogs for easier cost tracking\n    \"\"\"\n\n    user_api_key: Optional[str]\n    user_api_key_alias: Optional[str]\n    user_api_key_team_id: Optional[str]\n    user_api_key_user_id: Optional[str]\n    user_api_key_team_alias: Optional[str]\n    spend_logs_metadata: Optional[\n        dict\n    ]  # special param to log k,v pairs to spendlogs for a call\n    requester_ip_address: Optional[str]\n\n\nclass SpendLogsPayload(TypedDict):\n    request_id: str\n    call_type: str\n    api_key: str\n    spend: float\n    total_tokens: int\n    prompt_tokens: int\n    completion_tokens: int\n    startTime: datetime\n    endTime: datetime\n    completionStartTime: Optional[datetime]\n    model: str\n    model_id: Optional[str]\n    model_group: Optional[str]\n    api_base: str\n    user: str\n    metadata: str  # json str\n    cache_hit: str\n    cache_key: str\n    request_tags: str  # json str\n    team_id: Optional[str]\n    end_user: Optional[str]\n    requester_ip_address: Optional[str]\n\n\nclass SpanAttributes(str, enum.Enum):\n    # Note: We've taken this from opentelemetry-semantic-conventions-ai\n    # I chose to not add a new dependency to litellm for this\n\n    # Semantic Conventions for LLM requests, this needs to be removed after\n    # OpenTelemetry Semantic Conventions support Gen AI.\n    # Issue at https://github.com/open-telemetry/opentelemetry-python/issues/3868\n    # Refer to https://github.com/open-telemetry/semantic-conventions/blob/main/docs/gen-ai/llm-spans.md\n\n    LLM_SYSTEM = \"gen_ai.system\"\n    LLM_REQUEST_MODEL = \"gen_ai.request.model\"\n    LLM_REQUEST_MAX_TOKENS = \"gen_ai.request.max_tokens\"\n    LLM_REQUEST_TEMPERATURE = \"gen_ai.request.temperature\"\n    LLM_REQUEST_TOP_P = \"gen_ai.request.top_p\"\n    LLM_PROMPTS = \"gen_ai.prompt\"\n    LLM_COMPLETIONS = \"gen_ai.completion\"\n    LLM_RESPONSE_MODEL = \"gen_ai.response.model\"\n    LLM_USAGE_COMPLETION_TOKENS = \"gen_ai.usage.completion_tokens\"\n    LLM_USAGE_PROMPT_TOKENS = \"gen_ai.usage.prompt_tokens\"\n    LLM_TOKEN_TYPE = \"gen_ai.token.type\"\n    # To be added\n    # LLM_RESPONSE_FINISH_REASON = \"gen_ai.response.finish_reasons\"\n    # LLM_RESPONSE_ID = \"gen_ai.response.id\"\n\n    # LLM\n    LLM_REQUEST_TYPE = \"llm.request.type\"\n    LLM_USAGE_TOTAL_TOKENS = \"llm.usage.total_tokens\"\n    LLM_USAGE_TOKEN_TYPE = \"llm.usage.token_type\"\n    LLM_USER = \"llm.user\"\n    LLM_HEADERS = \"llm.headers\"\n    LLM_TOP_K = \"llm.top_k\"\n    LLM_IS_STREAMING = \"llm.is_streaming\"\n    LLM_FREQUENCY_PENALTY = \"llm.frequency_penalty\"\n    LLM_PRESENCE_PENALTY = \"llm.presence_penalty\"\n    LLM_CHAT_STOP_SEQUENCES = \"llm.chat.stop_sequences\"\n    LLM_REQUEST_FUNCTIONS = \"llm.request.functions\"\n    LLM_REQUEST_REPETITION_PENALTY = \"llm.request.repetition_penalty\"\n    LLM_RESPONSE_FINISH_REASON = \"llm.response.finish_reason\"\n    LLM_RESPONSE_STOP_REASON = \"llm.response.stop_reason\"\n    LLM_CONTENT_COMPLETION_CHUNK = \"llm.content.completion.chunk\"\n\n    # OpenAI\n    LLM_OPENAI_RESPONSE_SYSTEM_FINGERPRINT = \"gen_ai.openai.system_fingerprint\"\n    LLM_OPENAI_API_BASE = \"gen_ai.openai.api_base\"\n    LLM_OPENAI_API_VERSION = \"gen_ai.openai.api_version\"\n    LLM_OPENAI_API_TYPE = \"gen_ai.openai.api_type\"\n\n\nclass ManagementEndpointLoggingPayload(LiteLLMBase):\n    route: str\n    request_data: dict\n    response: Optional[dict] = None\n    exception: Optional[Any] = None\n    start_time: Optional[datetime] = None\n    end_time: Optional[datetime] = None\n\n\nclass ProxyException(Exception):\n    # NOTE: DO NOT MODIFY THIS\n    # This is used to map exactly to OPENAI Exceptions\n    def __init__(\n        self,\n        message: str,\n        type: str,\n        param: Optional[str],\n        code: Optional[int],\n        headers: Optional[Dict[str, str]] = None,\n    ):\n        self.message = message\n        self.type = type\n        self.param = param\n        self.code = code\n        if headers is not None:\n            for k, v in headers.items():\n                if not isinstance(v, str):\n                    headers[k] = str(v)\n        self.headers = headers or {}\n\n        # rules for proxyExceptions\n        # Litellm router.py returns \"No healthy deployment available\" when there are no deployments available\n        # Should map to 429 errors https://github.com/BerriAI/litellm/issues/2487\n        if (\n            \"No healthy deployment available\" in self.message\n            or \"No deployments available\" in self.message\n        ):\n            self.code = 429\n\n    def to_dict(self) -> dict:\n        \"\"\"Converts the ProxyException instance to a dictionary.\"\"\"\n        return {\n            \"message\": self.message,\n            \"type\": self.type,\n            \"param\": self.param,\n            \"code\": self.code,\n        }\n\n\nclass CommonProxyErrors(str, enum.Enum):\n    db_not_connected_error = \"DB not connected\"\n    no_llm_router = \"No models configured on proxy\"\n    not_allowed_access = \"Admin-only endpoint. Not allowed to access this.\"\n    not_premium_user = \"You must be a LiteLLM Enterprise user to use this feature. If you have a license please set `LITELLM_LICENSE` in your env. If you want to obtain a license meet with us here: https://calendly.com/d/4mp-gd3-k5k/litellm-1-1-onboarding-chat\"\n\n\nclass SpendCalculateRequest(LiteLLMBase):\n    model: Optional[str] = None\n    messages: Optional[List] = None\n    completion_response: Optional[dict] = None\n\n\nclass ProxyErrorTypes(str, enum.Enum):\n    budget_exceeded = \"budget_exceeded\"\n    expired_key = \"expired_key\"\n    auth_error = \"auth_error\"\n    internal_server_error = \"internal_server_error\"\n    bad_request_error = \"bad_request_error\"\n"}
{"type": "source_file", "path": "alpha_codium/llm/ai_handler.py", "content": "import logging\nimport os\n\nimport litellm\nimport openai\nfrom aiolimiter import AsyncLimiter\nfrom litellm import acompletion\nfrom litellm import RateLimitError\nfrom litellm.exceptions import APIError\n# from openai.error import APIError, RateLimitError, Timeout, TryAgain\nfrom retry import retry\n\nfrom alpha_codium.settings.config_loader import get_settings\nfrom alpha_codium.log import get_logger\n\nlogger = get_logger(__name__)\nOPENAI_RETRIES = 5\n\n\nclass AiHandler:\n    \"\"\"\n    This class handles interactions with the OpenAI API for chat completions.\n    It initializes the API key and other settings from a configuration file,\n    and provides a method for performing chat completions using the OpenAI ChatCompletion API.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Initializes the OpenAI API key and other settings from a configuration file.\n        Raises a ValueError if the OpenAI key is missing.\n        \"\"\"\n        self.limiter = AsyncLimiter(get_settings().config.max_requests_per_minute)\n        try:\n            if \"gpt\" in get_settings().get(\"config.model\").lower():\n                openai.api_key = get_settings().openai.key\n                litellm.openai_key = get_settings().openai.key\n            self.azure = False\n            if \"deepseek\" in get_settings().get(\"config.model\"):\n                litellm.register_prompt_template(\n                    model=\"huggingface/deepseek-ai/deepseek-coder-33b-instruct\",\n                    roles={\n                        \"system\": {\n                            \"pre_message\": \"\",\n                            \"post_message\": \"\\n\"\n                        },\n                        \"user\": {\n                            \"pre_message\": \"### Instruction:\\n\",\n                            \"post_message\": \"\\n### Response:\\n\"\n                        },\n                    },\n\n                )\n        except AttributeError as e:\n            raise ValueError(\"OpenAI key is required\") from e\n\n    @property\n    def deployment_id(self):\n        \"\"\"\n        Returns the deployment ID for the OpenAI API.\n        \"\"\"\n        return get_settings().get(\"OPENAI.DEPLOYMENT_ID\", None)\n\n    @retry(\n        exceptions=(AttributeError, RateLimitError),\n        tries=OPENAI_RETRIES,\n        delay=2,\n        backoff=2,\n        jitter=(1, 3),\n    )\n    async def chat_completion(\n            self, model: str,\n            system: str,\n            user: str,\n            temperature: float = 0.2,\n            frequency_penalty: float = 0.0,\n    ):\n        try:\n            deployment_id = self.deployment_id\n            if get_settings().config.verbosity_level >= 2:\n                logging.debug(\n                    f\"Generating completion with {model}\"\n                    f\"{(' from deployment ' + deployment_id) if deployment_id else ''}\"\n                )\n\n            async with self.limiter:\n                logger.info(\"-----------------\")\n                logger.info(\"Running inference ...\")\n                logger.debug(f\"system:\\n{system}\")\n                logger.debug(f\"user:\\n{user}\")\n                if \"deepseek\" in get_settings().get(\"config.model\"):\n                    response = await acompletion(\n                        model=\"huggingface/deepseek-ai/deepseek-coder-33b-instruct\",\n                        messages=[\n                            {\"role\": \"system\", \"content\": system},\n                            {\"role\": \"user\", \"content\": user},\n                        ],\n                        api_base=get_settings().get(\"config.model\"),\n                        temperature=temperature,\n                        repetition_penalty=frequency_penalty+1, # the scale of TGI is different from OpenAI\n                        force_timeout=get_settings().config.ai_timeout,\n                        max_tokens=2000,\n                        stop=['<|EOT|>'],\n                    )\n                    response[\"choices\"][0][\"message\"][\"content\"] = response[\"choices\"][0][\"message\"][\"content\"].rstrip()\n                    if response[\"choices\"][0][\"message\"][\"content\"].endswith(\"<|EOT|>\"):\n                        response[\"choices\"][0][\"message\"][\"content\"] = response[\"choices\"][0][\"message\"][\"content\"][:-7]\n                else:\n                    response = await acompletion(\n                        model=model,\n                        deployment_id=deployment_id,\n                        messages=[\n                            {\"role\": \"system\", \"content\": system},\n                            {\"role\": \"user\", \"content\": user},\n                        ],\n                        temperature=temperature,\n                        frequency_penalty=frequency_penalty,\n                        force_timeout=get_settings().config.ai_timeout,\n                    )\n        except (APIError) as e:\n            logging.error(\"Error during OpenAI inference\")\n            raise\n        except RateLimitError as e:\n            logging.error(\"Rate limit error during OpenAI inference\")\n            raise\n        except Exception as e:\n            logging.error(\"Unknown error during OpenAI inference: \", e)\n            raise APIError from e\n        if response is None or len(response[\"choices\"]) == 0:\n            raise APIError\n        resp = response[\"choices\"][0][\"message\"][\"content\"]\n        finish_reason = response[\"choices\"][0][\"finish_reason\"]\n        logger.debug(f\"response:\\n{resp}\")\n        logger.info('done')\n        logger.info(\"-----------------\")\n        return resp, finish_reason\n"}
{"type": "source_file", "path": "alpha_codium/llm/token_handler.py", "content": "from jinja2 import Environment, StrictUndefined\nfrom tiktoken import encoding_for_model, get_encoding\n\nfrom alpha_codium.settings.config_loader import get_settings\n\n\ndef get_token_encoder():\n    return (\n        encoding_for_model(get_settings().config.model)\n        if \"gpt\" in get_settings().config.model\n        else get_encoding(\"cl100k_base\")\n    )\n\n\nclass TokenHandler:\n    \"\"\"\n    A class for handling tokens in the context of a pull request.\n\n    Attributes:\n    - encoder: An object of the encoding_for_model class from the tiktoken module. Used to encode strings and count the\n      number of tokens in them.\n    - limit: The maximum number of tokens allowed for the given model, as defined in the MAX_TOKENS dictionary in the\n      pr_agent.algo module.\n    - prompt_tokens: The number of tokens in the system and user strings, as calculated by the _get_system_user_tokens\n      method.\n    \"\"\"\n\n    def __init__(self, message=None, vars: dict = {}, system=\"\", user=\"\"):  # noqa: B006\n        \"\"\"\n        Initializes the TokenHandler object.\n\n        Args:\n        - pr: The pull request object.\n        - vars: A dictionary of variables.\n        - system: The system string.\n        - user: The user string.\n        \"\"\"\n        self.encoder = get_token_encoder()\n        if message is not None:\n            self.prompt_tokens = self._get_system_user_tokens(\n                message, self.encoder, vars, system, user\n            )\n\n    def _get_system_user_tokens(self, message, encoder, vars: dict, system, user):\n        \"\"\"\n        Calculates the number of tokens in the system and user strings.\n\n        Args:\n        - message: The pull request object.\n        - encoder: An object of the encoding_for_model class from the tiktoken module.\n        - vars: A dictionary of variables.\n        - system: The system string.\n        - user: The user string.\n\n        Returns:\n        The sum of the number of tokens in the system and user strings.\n        \"\"\"\n        environment = Environment(undefined=StrictUndefined)\n        system_prompt = environment.from_string(system).render(vars)\n        user_prompt = environment.from_string(user).render(vars)\n        system_prompt_tokens = len(encoder.encode(system_prompt))\n        user_prompt_tokens = len(encoder.encode(user_prompt))\n        return system_prompt_tokens + user_prompt_tokens\n\n    def count_tokens(self, patch: str) -> int:\n        \"\"\"\n        Counts the number of tokens in a given patch string.\n\n        Args:\n        - patch: The patch string.\n\n        Returns:\n        The number of tokens in the patch string.\n        \"\"\"\n        return len(self.encoder.encode(patch, disallowed_special=()))\n"}
{"type": "source_file", "path": "alpha_codium/gen/stages/run_baseline.py", "content": "import functools\nimport logging\nfrom alpha_codium.gen.utils import postprocess_response\nfrom alpha_codium.llm.ai_invoker import send_inference\nfrom alpha_codium.log import get_logger\n\nlogger = get_logger(__name__)\n\n\nasync def run_baseline(self, problem):\n    try:\n        logging.info(\"Using baseline prompt\")\n        f = functools.partial(self._run, problem=problem, prompt=\"code_contests_prompts_baseline\")\n        response_baseline, _ = await send_inference(f)\n        recent_solution =  postprocess_response(response_baseline)\n        return recent_solution\n    except Exception as e:\n        logging.error(f\"Error: {e}\")\n        exit(-1)\n"}
{"type": "source_file", "path": "alpha_codium/gen/stages/run_generate_possible_solutions.py", "content": "import copy\nimport functools\nimport logging\nimport yaml\n\nfrom alpha_codium.gen.utils import load_yaml\nfrom alpha_codium.settings.config_loader import get_settings\nfrom alpha_codium.llm.ai_invoker import send_inference\nfrom alpha_codium.log import get_logger\n\nlogger = get_logger(__name__)\n\n\nasync def run_generate_possible_solutions(self, problem):\n    counter_retry = 0\n    while True:\n        try:\n            logger.info(\"--generate possible solutions stage--\")\n            if get_settings().get(\"solve.use_direct_solutions\", False):\n                return problem\n\n            # get settings\n            problem['max_num_of_possible_solutions'] = get_settings().get('possible_solutions.max_num_of_possible_solutions')\n            problem['use_test_explanations_possible_solutions'] = get_settings().get('possible_solutions.use_test_explanations')\n            f = functools.partial(self._run, problem=problem, prompt=\"code_contests_prompt_generate_possible_solutions\")\n\n            # inference\n            response_possible_solutions, _ = await send_inference(f)\n            response_possible_solutions_yaml = load_yaml(response_possible_solutions)\n\n            if get_settings().get('possible_solutions.remove_bruce_force_solutions'):\n                for i, s in enumerate(response_possible_solutions_yaml['possible_solutions']):\n                    if 'brute' in s['name'].lower():\n                        response_possible_solutions_yaml['possible_solutions'].pop(i)\n                        response_possible_solutions = yaml.dump(response_possible_solutions_yaml, sort_keys=False, line_break=\"\\n\")\n                        break\n            problem['s_possible_solutions'] = response_possible_solutions_yaml['possible_solutions']\n            problem['s_possible_solutions_str'] = response_possible_solutions.split('possible_solutions:')[1].strip()\n\n            return problem\n        except Exception as e:\n            logging.error(f\"'possible solutions' stage, counter_retry {counter_retry}, Error: {e}\")\n            counter_retry += 1\n            if counter_retry > 2:\n                raise e\n"}
{"type": "source_file", "path": "alpha_codium/llm/__init__.py", "content": ""}
{"type": "source_file", "path": "alpha_codium/solve_dataset.py", "content": "import argparse\n\nfrom alpha_codium.gen.dataset_solver import solve_dataset\nfrom alpha_codium.log import get_logger, setup_logger\n\nlogger = get_logger(__name__)\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\"--dataset_name\", type=str, default=\"valid_and_test_processed\")\nparser.add_argument(\"--split_name\", type=str, default=\"valid\")\nparser.add_argument(\"--database_solution_path\", type=str, default=\"\")\nif __name__ == \"__main__\":\n    args = parser.parse_args()\n    setup_logger()\n\n    # set default database_solution_path\n    args.database_solution_path = args.database_solution_path\n    if not args.database_solution_path:\n        args.database_solution_path = f\"./{args.dataset_name}_{args.split_name}_solution_database.json\"\n        logger.info(f\"args.database_solution_path: {args.database_solution_path}\")\n\n    solve_dataset(dataset_name=args.dataset_name,\n                  split_name=args.split_name,\n                  database_solution_path=args.database_solution_path)\n"}
{"type": "source_file", "path": "alpha_codium/gen/stages/run_choose_best_solution.py", "content": "import functools\nimport logging\nfrom alpha_codium.llm.ai_invoker import send_inference\nfrom alpha_codium.log import get_logger\nfrom alpha_codium.gen.utils import load_yaml\nfrom alpha_codium.settings.config_loader import get_settings\n\nlogger = get_logger(__name__)\n\n\nasync def run_choose_best_solution(self, problem):\n    counter_retry = 0\n    while True:\n        try:\n            logger.info(\"--choose best solution stage--\")\n\n            # get settings\n            f = functools.partial(self._run, problem=problem, prompt=choose_prompt())\n\n            # inference\n            response_best_solution, _ = await send_inference(f)\n            response_best_solution_yaml = load_yaml(response_best_solution,\n                                                    keys_fix_yaml=[\"name:\", \"content:\", \"why:\", \"- \"])\n\n            # update best solution\n            problem['s_best_solution'] = response_best_solution\n            if 's_possible_solutions' in problem:\n                problem['s_other_solutions'] = []\n                for solution in problem['s_possible_solutions']:\n                    if solution['name'] != response_best_solution_yaml['name']:\n                        problem['s_other_solutions'].append(solution)\n\n            return problem\n        except Exception as e:\n            logging.error(f\"'run_choose_best_solution' stage, counter_retry {counter_retry}, Error: {e}\")\n            counter_retry += 1\n            if counter_retry > 2:\n                raise e\n\n\ndef choose_prompt():\n    if get_settings().get(\"solve.use_direct_solutions\", False):\n        return \"code_contests_prompts_choose_best_solution_direct\"\n    else:\n        return \"code_contests_prompts_choose_best_solution\""}
{"type": "source_file", "path": "alpha_codium/gen/dataset_solver.py", "content": "import json\nimport os\nimport shutil\nfrom collections import OrderedDict\n\nfrom alpha_codium.code_contests.data.provider import CodeContestDataProvider\nfrom alpha_codium.gen.coding_competitor import CodeContestsCompetitor\nfrom alpha_codium.gen.utils import evaluate_solution_on_subset\nfrom alpha_codium.log import setup_logger, get_logger\nfrom alpha_codium.settings.config_loader import get_settings\n\n\ndef solve_dataset(dataset_name='valid_and_test_processed',\n                  split_name='valid',\n                  database_solution_path='solution_database.json'):\n\n    # load dataset\n    data_provider = CodeContestDataProvider(dataset_location=dataset_name)\n    setting = get_settings()\n    num_problems = len(data_provider.dataset[split_name])\n    base_path = os.getcwd()\n    setting.solve.reduce_verbose = True\n\n    ## load previous solution-database if exists\n    try:\n        with open(database_solution_path, 'r') as f:\n            database = json.load(f)\n            database[split_name] = OrderedDict(sorted(database[split_name].items(), key=lambda x: int(x[0])))\n    except:\n        print(f\"Failed to load database from {database_solution_path}\")\n        database = {split_name: {}}\n\n    # iterate on problems\n    for problem_number in range(0, num_problems):\n\n        # skip if already ran\n        logger = setup_logger()\n\n        num_iterations =  setting.get(\"dataset.num_iterations\", 1)\n        prev = database[split_name].get(str(problem_number), {}).get(f'iteration_{num_iterations-1}', {})\n        if not ((prev == {}) or (prev is None)):\n            print(f\"problem_number {problem_number} already ran\")\n            continue\n\n        # check if problem is valid (at least one of the provided solutions actually passes the generated tests)\n        if data_provider.dataset[split_name][problem_number].get('is_valid_problem', True) is False:\n            logger.info(f\"problem {problem_number} is not valid\")\n            continue\n\n        os.chdir(base_path)\n        logger.info(f\"problem_number: {problem_number}\")\n        problem_name = data_provider.dataset[split_name][int(problem_number)]['name']\n        logger.info(f\"problem_name: {problem_name}\")\n        problem = data_provider.find_problem(ds=data_provider.dataset, problem_name=problem_name, split_name=split_name)\n        logger.info(f\"problem['cf_tags']: {problem['cf_tags']}\")\n\n        # solve problem\n        problem_database = {problem_number: {}}\n        solver = CodeContestsCompetitor()\n        for iteration in range(setting.get(\"dataset.num_iterations\", 1)):\n            it_str = f\"iteration_{iteration}\"\n            problem_database[problem_number][it_str] = {}\n\n            # skip if iteration already ran\n            prev_iter = database[split_name].get(str(problem_number), {}).get(it_str, {})\n            if not ((prev_iter == {}) or (prev_iter is None)):\n                print(f\"prev_iter {iteration} already ran\")\n                problem_database[problem_number][it_str] = prev_iter\n                if is_solved(prev_iter):\n                    logger.info(f\"codium solved problem {problem_number} in iteration {iteration}\")\n                    break\n                continue\n\n            # solve problem\n            solution = solver.solve_problem_in_dataset(problem, iteration, logger)\n\n            logger.info(f\"solution code:\\n{solution}\")\n            if not solution:\n                logger.info(f\"Failed to solve problem {problem_number} in iteration {iteration}\")\n                continue\n            logger.info(f\"Evaluating solution on public tests...\")\n            test_results, test_passed_public, test_failed_public, test_timeout_public = evaluate_solution_on_subset(\n                'public_tests', problem, solution, silent=True)\n\n            logger.info(f\"evaluating solution on private tests...\")\n            test_results, test_passed_private, test_failed_private, test_timeout_private = evaluate_solution_on_subset(\n                'private_tests', problem, solution, silent=True)\n\n            logger.info(f\"evaluating solution on generated tests...\")\n            test_results, test_passed_generate, test_failed_generate, test_timeout_generate = evaluate_solution_on_subset(\n                'generated_tests', problem, solution, silent=True)\n\n            logger.info(\n                f\"\\ntest_passed_public: {test_passed_public}, test_failed_public: {test_failed_public}, test_timeout_public: {test_timeout_public}\\n\"\n                f\"test_passed_private: {test_passed_private}, test_failed_private: {test_failed_private}, test_timeout_private: {test_timeout_private}\\n\"\n                f\"test_passed_generate: {test_passed_generate}, test_failed_generate: {test_failed_generate}, test_timeout_generate: {test_timeout_generate}\\n\")\n\n            problem_database[problem_number][it_str]['solution'] = solution\n            problem_database[problem_number][it_str]['test_passed_private'] = test_passed_private\n            problem_database[problem_number][it_str]['test_failed_private'] = test_failed_private\n            problem_database[problem_number][it_str]['test_timeout_private'] = test_timeout_private\n            problem_database[problem_number][it_str]['test_passed_generate'] = test_passed_generate\n            problem_database[problem_number][it_str]['test_failed_generate'] = test_failed_generate\n            problem_database[problem_number][it_str]['test_timeout_generate'] = test_timeout_generate\n            problem_database[problem_number][it_str]['test_passed_public'] = test_passed_public\n            problem_database[problem_number][it_str]['test_failed_public'] = test_failed_public\n            problem_database[problem_number][it_str]['test_timeout_public'] = test_timeout_public\n            os.chdir(base_path)\n            if is_solved(problem_database[problem_number][it_str]):\n                logger.info(f\"codium solved problem {problem_number} in iteration {iteration}\")\n                break\n            else:\n                logger.info(f\"codium failed to solve problem {problem_number} in iteration {iteration}\")\n        database[split_name][problem_number] = problem_database[problem_number]\n        os.chdir(base_path)\n        with open(database_solution_path, 'w') as f:\n            json.dump(database, f)\n\n\ndef is_solved(s):\n    if s['test_failed_private'] == 0 and s['test_failed_generate'] == 0 and \\\n            s['test_timeout_private'] == 0 and s['test_timeout_generate'] == 0 and \\\n            (s['test_passed_private'] + s['test_passed_generate']) > 0:\n        return True\n    else:\n        return False\n"}
{"type": "source_file", "path": "alpha_codium/llm/ai_invoker.py", "content": "import logging\nimport traceback\nfrom typing import Callable, List\n\nfrom alpha_codium.settings.config_loader import get_settings\n\n\nasync def send_inference(f: Callable):\n    all_models = _get_all_models()\n    all_deployments = _get_all_deployments(all_models)\n    # try each (model, deployment_id) pair until one is successful, otherwise raise exception\n    for i, (model, deployment_id) in enumerate(zip(all_models, all_deployments)):\n        try:\n            get_settings().set(\"openai.deployment_id\", deployment_id)\n            return await f(model)\n        except Exception:\n            logging.warning(\n                f\"Failed to generate prediction with {model}\"\n                f\"{(' from deployment ' + deployment_id) if deployment_id else ''}: \"\n                f\"{traceback.format_exc()}\"\n            )\n            if i == len(all_models) - 1:  # If it's the last iteration\n                raise  # Re-raise the last exception\n\n\ndef _get_all_models() -> List[str]:\n    model = get_settings().config.model\n    fallback_models = get_settings().config.fallback_models\n    if not isinstance(fallback_models, list):\n        fallback_models = [m.strip() for m in fallback_models.split(\",\")]\n    all_models = [model] + fallback_models\n    return all_models\n\n\ndef _get_all_deployments(all_models: List[str]) -> List[str]:\n    deployment_id = get_settings().get(\"openai.deployment_id\", None)\n    fallback_deployments = get_settings().get(\"openai.fallback_deployments\", [])\n    if not isinstance(fallback_deployments, list) and fallback_deployments:\n        fallback_deployments = [d.strip() for d in fallback_deployments.split(\",\")]\n    if fallback_deployments:\n        all_deployments = [deployment_id] + fallback_deployments\n        if len(all_deployments) < len(all_models):\n            raise ValueError(\n                f\"The number of deployments ({len(all_deployments)}) \"\n                f\"is less than the number of models ({len(all_models)})\"\n            )\n    else:\n        all_deployments = [deployment_id] * len(all_models)\n    return all_deployments\n"}
{"type": "source_file", "path": "alpha_codium/solve_my_problem.py", "content": "import argparse\nimport json\n\nfrom alpha_codium.gen.coding_competitor import solve_problem, solve_my_problem\nfrom alpha_codium.log import setup_logger\nfrom alpha_codium.settings.config_loader import get_settings\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\"--my_problem_json_file\", type=str, default=\"my_problem_example.json\")\n\nif __name__ == \"__main__\":\n    args = parser.parse_args()\n    setup_logger()\n    \n    with open(args.my_problem_json_file, \"r\") as my_problem:\n        solve_my_problem(json.load(my_problem))\n"}
